***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.125', 'DATASET.NOISE_TYPE', 'asym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.125/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.125
  NOISE_TYPE: asym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.125/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.125/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.1620, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 508
clean true:498
clean false:10
clean_rate:0.9803149606299213
noisy true:67
noisy false:2561
after delete: len(clean_dataset) 508
after delete: len(noisy_dataset) 2628
epoch [1/200] batch [5/15] time 0.520 (0.553) data 0.390 (0.405) loss_x loss_x 2.6855 (2.8895) acc_x 50.0000 (41.2500) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [10/15] time 0.475 (0.522) data 0.345 (0.383) loss_x loss_x 2.4102 (2.7420) acc_x 56.2500 (44.6875) lr 1.0000e-05 eta 0:00:02
epoch [1/200] batch [15/15] time 0.393 (0.531) data 0.264 (0.395) loss_x loss_x 2.4355 (2.6374) acc_x 50.0000 (44.5833) lr 1.0000e-05 eta 0:00:00
epoch [1/200] batch [5/82] time 0.486 (0.506) data 0.356 (0.370) loss_u loss_u 0.9282 (0.9261) acc_u 25.0000 (13.1250) lr 1.0000e-05 eta 0:00:38
epoch [1/200] batch [10/82] time 0.402 (0.500) data 0.272 (0.366) loss_u loss_u 0.9204 (0.9238) acc_u 15.6250 (14.6875) lr 1.0000e-05 eta 0:00:36
epoch [1/200] batch [15/82] time 0.500 (0.495) data 0.369 (0.361) loss_u loss_u 0.8989 (0.9205) acc_u 18.7500 (15.0000) lr 1.0000e-05 eta 0:00:33
epoch [1/200] batch [20/82] time 0.521 (0.488) data 0.392 (0.355) loss_u loss_u 0.8833 (0.9141) acc_u 21.8750 (16.2500) lr 1.0000e-05 eta 0:00:30
epoch [1/200] batch [25/82] time 0.435 (0.482) data 0.305 (0.349) loss_u loss_u 0.9077 (0.9121) acc_u 18.7500 (16.7500) lr 1.0000e-05 eta 0:00:27
epoch [1/200] batch [30/82] time 0.358 (0.484) data 0.228 (0.352) loss_u loss_u 0.7856 (0.9041) acc_u 37.5000 (18.2292) lr 1.0000e-05 eta 0:00:25
epoch [1/200] batch [35/82] time 0.403 (0.482) data 0.274 (0.350) loss_u loss_u 0.9160 (0.8975) acc_u 12.5000 (19.1964) lr 1.0000e-05 eta 0:00:22
epoch [1/200] batch [40/82] time 0.426 (0.476) data 0.297 (0.344) loss_u loss_u 0.8169 (0.8935) acc_u 25.0000 (19.2188) lr 1.0000e-05 eta 0:00:19
epoch [1/200] batch [45/82] time 0.420 (0.472) data 0.291 (0.340) loss_u loss_u 0.9185 (0.8921) acc_u 12.5000 (19.2361) lr 1.0000e-05 eta 0:00:17
epoch [1/200] batch [50/82] time 0.470 (0.473) data 0.340 (0.342) loss_u loss_u 0.8652 (0.8903) acc_u 15.6250 (19.3750) lr 1.0000e-05 eta 0:00:15
epoch [1/200] batch [55/82] time 0.352 (0.466) data 0.221 (0.334) loss_u loss_u 0.8931 (0.8903) acc_u 18.7500 (19.3750) lr 1.0000e-05 eta 0:00:12
epoch [1/200] batch [60/82] time 0.476 (0.462) data 0.346 (0.331) loss_u loss_u 0.8579 (0.8890) acc_u 21.8750 (19.5833) lr 1.0000e-05 eta 0:00:10
epoch [1/200] batch [65/82] time 0.435 (0.457) data 0.305 (0.326) loss_u loss_u 0.8945 (0.8886) acc_u 15.6250 (19.4712) lr 1.0000e-05 eta 0:00:07
epoch [1/200] batch [70/82] time 0.449 (0.460) data 0.319 (0.329) loss_u loss_u 0.8970 (0.8873) acc_u 6.2500 (19.4196) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [75/82] time 0.380 (0.458) data 0.250 (0.327) loss_u loss_u 0.9023 (0.8865) acc_u 12.5000 (19.5000) lr 1.0000e-05 eta 0:00:03
epoch [1/200] batch [80/82] time 0.482 (0.458) data 0.351 (0.327) loss_u loss_u 0.9268 (0.8855) acc_u 9.3750 (19.7266) lr 1.0000e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2066
confident_label rate tensor(0.3061, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 960
clean true:945
clean false:15
clean_rate:0.984375
noisy true:125
noisy false:2051
after delete: len(clean_dataset) 960
after delete: len(noisy_dataset) 2176
epoch [2/200] batch [5/30] time 0.641 (0.505) data 0.511 (0.375) loss_x loss_x 1.7754 (1.8303) acc_x 62.5000 (56.2500) lr 2.0000e-03 eta 0:00:12
epoch [2/200] batch [10/30] time 0.482 (0.452) data 0.352 (0.322) loss_x loss_x 1.7432 (1.6812) acc_x 50.0000 (57.5000) lr 2.0000e-03 eta 0:00:09
epoch [2/200] batch [15/30] time 0.651 (0.468) data 0.521 (0.338) loss_x loss_x 2.5723 (1.7355) acc_x 31.2500 (54.7917) lr 2.0000e-03 eta 0:00:07
epoch [2/200] batch [20/30] time 0.425 (0.471) data 0.296 (0.341) loss_x loss_x 2.0312 (1.7318) acc_x 40.6250 (54.0625) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [25/30] time 0.625 (0.476) data 0.495 (0.346) loss_x loss_x 2.2402 (1.7462) acc_x 56.2500 (54.7500) lr 2.0000e-03 eta 0:00:02
epoch [2/200] batch [30/30] time 0.433 (0.478) data 0.303 (0.348) loss_x loss_x 1.3467 (1.7694) acc_x 71.8750 (54.2708) lr 2.0000e-03 eta 0:00:00
epoch [2/200] batch [5/68] time 0.407 (0.470) data 0.276 (0.340) loss_u loss_u 0.8174 (0.8528) acc_u 21.8750 (21.2500) lr 2.0000e-03 eta 0:00:29
epoch [2/200] batch [10/68] time 0.384 (0.461) data 0.254 (0.331) loss_u loss_u 0.7920 (0.8502) acc_u 37.5000 (20.9375) lr 2.0000e-03 eta 0:00:26
epoch [2/200] batch [15/68] time 0.395 (0.463) data 0.264 (0.333) loss_u loss_u 0.8340 (0.8388) acc_u 18.7500 (22.9167) lr 2.0000e-03 eta 0:00:24
epoch [2/200] batch [20/68] time 0.479 (0.461) data 0.348 (0.330) loss_u loss_u 0.7739 (0.8342) acc_u 34.3750 (22.6562) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [25/68] time 0.763 (0.464) data 0.633 (0.334) loss_u loss_u 0.8359 (0.8329) acc_u 25.0000 (22.0000) lr 2.0000e-03 eta 0:00:19
epoch [2/200] batch [30/68] time 0.520 (0.463) data 0.390 (0.332) loss_u loss_u 0.7612 (0.8297) acc_u 34.3750 (22.1875) lr 2.0000e-03 eta 0:00:17
epoch [2/200] batch [35/68] time 0.465 (0.458) data 0.334 (0.328) loss_u loss_u 0.7700 (0.8246) acc_u 31.2500 (23.1250) lr 2.0000e-03 eta 0:00:15
epoch [2/200] batch [40/68] time 0.432 (0.457) data 0.300 (0.327) loss_u loss_u 0.7188 (0.8163) acc_u 34.3750 (24.0625) lr 2.0000e-03 eta 0:00:12
epoch [2/200] batch [45/68] time 0.455 (0.456) data 0.325 (0.326) loss_u loss_u 0.7612 (0.8136) acc_u 25.0000 (24.2361) lr 2.0000e-03 eta 0:00:10
epoch [2/200] batch [50/68] time 0.476 (0.456) data 0.346 (0.325) loss_u loss_u 0.8203 (0.8124) acc_u 28.1250 (24.3750) lr 2.0000e-03 eta 0:00:08
epoch [2/200] batch [55/68] time 0.361 (0.454) data 0.231 (0.323) loss_u loss_u 0.8384 (0.8101) acc_u 25.0000 (24.7159) lr 2.0000e-03 eta 0:00:05
epoch [2/200] batch [60/68] time 0.406 (0.453) data 0.276 (0.322) loss_u loss_u 0.7554 (0.8100) acc_u 31.2500 (24.7917) lr 2.0000e-03 eta 0:00:03
epoch [2/200] batch [65/68] time 0.574 (0.455) data 0.444 (0.324) loss_u loss_u 0.8174 (0.8091) acc_u 31.2500 (25.2885) lr 2.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1821
confident_label rate tensor(0.3747, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1175
clean true:1159
clean false:16
clean_rate:0.9863829787234043
noisy true:156
noisy false:1805
after delete: len(clean_dataset) 1175
after delete: len(noisy_dataset) 1961
epoch [3/200] batch [5/36] time 0.489 (0.491) data 0.358 (0.360) loss_x loss_x 1.7998 (1.4805) acc_x 53.1250 (61.2500) lr 1.9999e-03 eta 0:00:15
epoch [3/200] batch [10/36] time 0.620 (0.499) data 0.490 (0.368) loss_x loss_x 1.6553 (1.5320) acc_x 43.7500 (57.1875) lr 1.9999e-03 eta 0:00:12
epoch [3/200] batch [15/36] time 0.606 (0.484) data 0.476 (0.353) loss_x loss_x 1.4014 (1.5266) acc_x 62.5000 (59.5833) lr 1.9999e-03 eta 0:00:10
epoch [3/200] batch [20/36] time 0.582 (0.494) data 0.452 (0.363) loss_x loss_x 1.4053 (1.5326) acc_x 59.3750 (59.5312) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [25/36] time 0.381 (0.479) data 0.251 (0.348) loss_x loss_x 1.3027 (1.5345) acc_x 62.5000 (58.8750) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [30/36] time 0.440 (0.470) data 0.309 (0.340) loss_x loss_x 1.4150 (1.5213) acc_x 59.3750 (58.6458) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [35/36] time 0.380 (0.465) data 0.250 (0.335) loss_x loss_x 1.6621 (1.5501) acc_x 53.1250 (58.3036) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/61] time 0.387 (0.460) data 0.256 (0.329) loss_u loss_u 0.8169 (0.8335) acc_u 25.0000 (20.6250) lr 1.9999e-03 eta 0:00:25
epoch [3/200] batch [10/61] time 0.429 (0.454) data 0.298 (0.323) loss_u loss_u 0.8306 (0.8323) acc_u 18.7500 (19.3750) lr 1.9999e-03 eta 0:00:23
epoch [3/200] batch [15/61] time 0.366 (0.447) data 0.236 (0.317) loss_u loss_u 0.8315 (0.8370) acc_u 25.0000 (20.6250) lr 1.9999e-03 eta 0:00:20
epoch [3/200] batch [20/61] time 0.459 (0.451) data 0.328 (0.320) loss_u loss_u 0.8037 (0.8381) acc_u 18.7500 (20.3125) lr 1.9999e-03 eta 0:00:18
epoch [3/200] batch [25/61] time 0.461 (0.450) data 0.330 (0.319) loss_u loss_u 0.7656 (0.8365) acc_u 37.5000 (20.8750) lr 1.9999e-03 eta 0:00:16
epoch [3/200] batch [30/61] time 0.376 (0.449) data 0.246 (0.318) loss_u loss_u 0.8638 (0.8311) acc_u 18.7500 (21.8750) lr 1.9999e-03 eta 0:00:13
epoch [3/200] batch [35/61] time 0.565 (0.448) data 0.435 (0.318) loss_u loss_u 0.8525 (0.8296) acc_u 12.5000 (21.7857) lr 1.9999e-03 eta 0:00:11
epoch [3/200] batch [40/61] time 0.433 (0.447) data 0.302 (0.316) loss_u loss_u 0.8086 (0.8290) acc_u 31.2500 (22.1094) lr 1.9999e-03 eta 0:00:09
epoch [3/200] batch [45/61] time 0.521 (0.443) data 0.390 (0.313) loss_u loss_u 0.7866 (0.8291) acc_u 28.1250 (22.0833) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [50/61] time 0.415 (0.443) data 0.283 (0.312) loss_u loss_u 0.8262 (0.8298) acc_u 18.7500 (21.9375) lr 1.9999e-03 eta 0:00:04
epoch [3/200] batch [55/61] time 0.463 (0.441) data 0.331 (0.310) loss_u loss_u 0.7695 (0.8300) acc_u 31.2500 (22.1591) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [60/61] time 0.473 (0.444) data 0.342 (0.313) loss_u loss_u 0.8975 (0.8314) acc_u 12.5000 (21.9271) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1854
confident_label rate tensor(0.3702, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1161
clean true:1143
clean false:18
clean_rate:0.9844961240310077
noisy true:139
noisy false:1836
after delete: len(clean_dataset) 1161
after delete: len(noisy_dataset) 1975
epoch [4/200] batch [5/36] time 0.449 (0.470) data 0.319 (0.340) loss_x loss_x 0.8579 (1.6103) acc_x 87.5000 (58.7500) lr 1.9995e-03 eta 0:00:14
epoch [4/200] batch [10/36] time 0.409 (0.471) data 0.280 (0.341) loss_x loss_x 1.6191 (1.7011) acc_x 62.5000 (56.5625) lr 1.9995e-03 eta 0:00:12
epoch [4/200] batch [15/36] time 0.407 (0.453) data 0.277 (0.323) loss_x loss_x 1.0244 (1.6541) acc_x 65.6250 (57.2917) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [20/36] time 0.410 (0.442) data 0.280 (0.312) loss_x loss_x 1.7090 (1.6174) acc_x 53.1250 (57.9688) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [25/36] time 0.497 (0.445) data 0.367 (0.315) loss_x loss_x 1.8486 (1.5960) acc_x 50.0000 (58.3750) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [30/36] time 0.567 (0.451) data 0.436 (0.321) loss_x loss_x 1.9932 (1.5840) acc_x 50.0000 (58.5417) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [35/36] time 0.439 (0.458) data 0.309 (0.328) loss_x loss_x 1.0449 (1.5630) acc_x 71.8750 (59.0179) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/61] time 0.500 (0.454) data 0.369 (0.324) loss_u loss_u 0.7314 (0.8046) acc_u 28.1250 (26.2500) lr 1.9995e-03 eta 0:00:25
epoch [4/200] batch [10/61] time 0.395 (0.449) data 0.264 (0.319) loss_u loss_u 0.7856 (0.8062) acc_u 31.2500 (24.6875) lr 1.9995e-03 eta 0:00:22
epoch [4/200] batch [15/61] time 0.354 (0.449) data 0.223 (0.319) loss_u loss_u 0.8530 (0.8121) acc_u 18.7500 (25.0000) lr 1.9995e-03 eta 0:00:20
epoch [4/200] batch [20/61] time 0.437 (0.443) data 0.306 (0.313) loss_u loss_u 0.8423 (0.8133) acc_u 18.7500 (24.6875) lr 1.9995e-03 eta 0:00:18
epoch [4/200] batch [25/61] time 0.297 (0.439) data 0.166 (0.309) loss_u loss_u 0.8364 (0.8158) acc_u 18.7500 (24.2500) lr 1.9995e-03 eta 0:00:15
epoch [4/200] batch [30/61] time 0.505 (0.444) data 0.374 (0.314) loss_u loss_u 0.7983 (0.8178) acc_u 31.2500 (23.8542) lr 1.9995e-03 eta 0:00:13
epoch [4/200] batch [35/61] time 0.401 (0.441) data 0.271 (0.311) loss_u loss_u 0.7881 (0.8157) acc_u 25.0000 (23.9286) lr 1.9995e-03 eta 0:00:11
epoch [4/200] batch [40/61] time 0.365 (0.441) data 0.234 (0.311) loss_u loss_u 0.8003 (0.8152) acc_u 31.2500 (24.1406) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [45/61] time 0.377 (0.437) data 0.245 (0.307) loss_u loss_u 0.8267 (0.8173) acc_u 34.3750 (24.0972) lr 1.9995e-03 eta 0:00:06
epoch [4/200] batch [50/61] time 0.451 (0.437) data 0.319 (0.307) loss_u loss_u 0.8120 (0.8123) acc_u 25.0000 (24.5625) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [55/61] time 0.466 (0.438) data 0.335 (0.307) loss_u loss_u 0.8203 (0.8126) acc_u 15.6250 (24.4886) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [60/61] time 0.465 (0.439) data 0.334 (0.309) loss_u loss_u 0.7998 (0.8120) acc_u 21.8750 (24.3229) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1799
confident_label rate tensor(0.3795, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1190
clean true:1175
clean false:15
clean_rate:0.9873949579831933
noisy true:162
noisy false:1784
after delete: len(clean_dataset) 1190
after delete: len(noisy_dataset) 1946
epoch [5/200] batch [5/37] time 0.454 (0.452) data 0.324 (0.322) loss_x loss_x 1.4268 (1.6924) acc_x 62.5000 (56.2500) lr 1.9989e-03 eta 0:00:14
epoch [5/200] batch [10/37] time 0.465 (0.494) data 0.335 (0.364) loss_x loss_x 2.3105 (1.6354) acc_x 50.0000 (57.1875) lr 1.9989e-03 eta 0:00:13
epoch [5/200] batch [15/37] time 0.426 (0.484) data 0.296 (0.354) loss_x loss_x 1.3115 (1.5741) acc_x 71.8750 (58.9583) lr 1.9989e-03 eta 0:00:10
epoch [5/200] batch [20/37] time 0.351 (0.481) data 0.221 (0.350) loss_x loss_x 2.0996 (1.6216) acc_x 37.5000 (57.0312) lr 1.9989e-03 eta 0:00:08
epoch [5/200] batch [25/37] time 0.407 (0.473) data 0.277 (0.343) loss_x loss_x 1.5361 (1.5832) acc_x 56.2500 (57.6250) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [30/37] time 0.541 (0.467) data 0.411 (0.337) loss_x loss_x 1.5098 (1.5949) acc_x 71.8750 (57.7083) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [35/37] time 0.510 (0.467) data 0.380 (0.337) loss_x loss_x 1.9365 (1.6193) acc_x 40.6250 (56.8750) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/60] time 0.639 (0.464) data 0.509 (0.333) loss_u loss_u 0.8184 (0.8230) acc_u 28.1250 (25.0000) lr 1.9989e-03 eta 0:00:25
epoch [5/200] batch [10/60] time 0.348 (0.457) data 0.218 (0.327) loss_u loss_u 0.8389 (0.8242) acc_u 21.8750 (25.0000) lr 1.9989e-03 eta 0:00:22
epoch [5/200] batch [15/60] time 0.430 (0.453) data 0.300 (0.323) loss_u loss_u 0.8457 (0.8161) acc_u 25.0000 (27.2917) lr 1.9989e-03 eta 0:00:20
epoch [5/200] batch [20/60] time 0.425 (0.450) data 0.294 (0.320) loss_u loss_u 0.7861 (0.8055) acc_u 31.2500 (28.5938) lr 1.9989e-03 eta 0:00:18
epoch [5/200] batch [25/60] time 0.419 (0.450) data 0.288 (0.319) loss_u loss_u 0.7979 (0.8072) acc_u 28.1250 (27.7500) lr 1.9989e-03 eta 0:00:15
epoch [5/200] batch [30/60] time 0.367 (0.448) data 0.236 (0.318) loss_u loss_u 0.8047 (0.8027) acc_u 21.8750 (27.3958) lr 1.9989e-03 eta 0:00:13
epoch [5/200] batch [35/60] time 0.495 (0.445) data 0.366 (0.314) loss_u loss_u 0.8125 (0.8040) acc_u 25.0000 (27.1429) lr 1.9989e-03 eta 0:00:11
epoch [5/200] batch [40/60] time 0.319 (0.440) data 0.188 (0.310) loss_u loss_u 0.8066 (0.8056) acc_u 25.0000 (26.4844) lr 1.9989e-03 eta 0:00:08
epoch [5/200] batch [45/60] time 0.493 (0.440) data 0.363 (0.309) loss_u loss_u 0.7642 (0.8058) acc_u 31.2500 (26.4583) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [50/60] time 0.600 (0.439) data 0.470 (0.309) loss_u loss_u 0.8281 (0.8068) acc_u 18.7500 (26.1250) lr 1.9989e-03 eta 0:00:04
epoch [5/200] batch [55/60] time 0.344 (0.437) data 0.215 (0.307) loss_u loss_u 0.7729 (0.8088) acc_u 34.3750 (25.8523) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [60/60] time 0.396 (0.435) data 0.267 (0.304) loss_u loss_u 0.8184 (0.8108) acc_u 21.8750 (25.5208) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1794
confident_label rate tensor(0.3820, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1198
clean true:1189
clean false:9
clean_rate:0.9924874791318865
noisy true:153
noisy false:1785
after delete: len(clean_dataset) 1198
after delete: len(noisy_dataset) 1938
epoch [6/200] batch [5/37] time 0.512 (0.493) data 0.383 (0.363) loss_x loss_x 1.6602 (1.5695) acc_x 46.8750 (60.0000) lr 1.9980e-03 eta 0:00:15
epoch [6/200] batch [10/37] time 0.393 (0.461) data 0.263 (0.331) loss_x loss_x 1.7979 (1.5741) acc_x 46.8750 (59.6875) lr 1.9980e-03 eta 0:00:12
epoch [6/200] batch [15/37] time 0.586 (0.456) data 0.455 (0.326) loss_x loss_x 1.6621 (1.5656) acc_x 65.6250 (60.6250) lr 1.9980e-03 eta 0:00:10
epoch [6/200] batch [20/37] time 0.479 (0.457) data 0.349 (0.327) loss_x loss_x 1.7148 (1.5801) acc_x 59.3750 (59.8438) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [25/37] time 0.424 (0.450) data 0.295 (0.320) loss_x loss_x 1.5898 (1.5845) acc_x 59.3750 (59.5000) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [30/37] time 0.390 (0.453) data 0.259 (0.323) loss_x loss_x 1.5498 (1.5602) acc_x 65.6250 (60.2083) lr 1.9980e-03 eta 0:00:03
epoch [6/200] batch [35/37] time 0.429 (0.450) data 0.299 (0.320) loss_x loss_x 1.3594 (1.5670) acc_x 59.3750 (60.0893) lr 1.9980e-03 eta 0:00:00
epoch [6/200] batch [5/60] time 0.484 (0.445) data 0.353 (0.315) loss_u loss_u 0.7822 (0.8197) acc_u 28.1250 (25.0000) lr 1.9980e-03 eta 0:00:24
epoch [6/200] batch [10/60] time 0.542 (0.448) data 0.412 (0.318) loss_u loss_u 0.7417 (0.8093) acc_u 31.2500 (27.1875) lr 1.9980e-03 eta 0:00:22
epoch [6/200] batch [15/60] time 0.396 (0.447) data 0.266 (0.317) loss_u loss_u 0.8252 (0.8165) acc_u 21.8750 (25.4167) lr 1.9980e-03 eta 0:00:20
epoch [6/200] batch [20/60] time 0.462 (0.440) data 0.332 (0.310) loss_u loss_u 0.7905 (0.8088) acc_u 31.2500 (27.0312) lr 1.9980e-03 eta 0:00:17
epoch [6/200] batch [25/60] time 0.389 (0.438) data 0.258 (0.308) loss_u loss_u 0.8345 (0.8178) acc_u 18.7500 (25.0000) lr 1.9980e-03 eta 0:00:15
epoch [6/200] batch [30/60] time 0.401 (0.435) data 0.271 (0.305) loss_u loss_u 0.8521 (0.8186) acc_u 28.1250 (25.7292) lr 1.9980e-03 eta 0:00:13
epoch [6/200] batch [35/60] time 0.372 (0.431) data 0.241 (0.301) loss_u loss_u 0.8389 (0.8222) acc_u 18.7500 (24.9107) lr 1.9980e-03 eta 0:00:10
epoch [6/200] batch [40/60] time 0.353 (0.430) data 0.223 (0.300) loss_u loss_u 0.8149 (0.8190) acc_u 21.8750 (24.5312) lr 1.9980e-03 eta 0:00:08
epoch [6/200] batch [45/60] time 0.384 (0.430) data 0.253 (0.300) loss_u loss_u 0.7764 (0.8173) acc_u 25.0000 (24.4444) lr 1.9980e-03 eta 0:00:06
epoch [6/200] batch [50/60] time 0.591 (0.429) data 0.462 (0.299) loss_u loss_u 0.8828 (0.8163) acc_u 18.7500 (24.6875) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [55/60] time 0.311 (0.428) data 0.181 (0.298) loss_u loss_u 0.7412 (0.8133) acc_u 25.0000 (25.0000) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [60/60] time 0.446 (0.432) data 0.315 (0.302) loss_u loss_u 0.8389 (0.8140) acc_u 25.0000 (25.1562) lr 1.9980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1769
confident_label rate tensor(0.3929, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1232
clean true:1219
clean false:13
clean_rate:0.989448051948052
noisy true:148
noisy false:1756
after delete: len(clean_dataset) 1232
after delete: len(noisy_dataset) 1904
epoch [7/200] batch [5/38] time 0.486 (0.449) data 0.357 (0.319) loss_x loss_x 1.7432 (1.4590) acc_x 56.2500 (65.6250) lr 1.9969e-03 eta 0:00:14
epoch [7/200] batch [10/38] time 0.418 (0.427) data 0.287 (0.297) loss_x loss_x 1.4932 (1.5587) acc_x 56.2500 (60.3125) lr 1.9969e-03 eta 0:00:11
epoch [7/200] batch [15/38] time 0.442 (0.434) data 0.313 (0.304) loss_x loss_x 2.1777 (1.6403) acc_x 43.7500 (57.5000) lr 1.9969e-03 eta 0:00:09
epoch [7/200] batch [20/38] time 0.442 (0.451) data 0.313 (0.322) loss_x loss_x 1.5596 (1.5979) acc_x 56.2500 (57.9688) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [25/38] time 0.477 (0.463) data 0.347 (0.333) loss_x loss_x 1.3789 (1.5963) acc_x 56.2500 (57.3750) lr 1.9969e-03 eta 0:00:06
epoch [7/200] batch [30/38] time 0.497 (0.456) data 0.368 (0.327) loss_x loss_x 1.0938 (1.5667) acc_x 78.1250 (58.5417) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [35/38] time 0.378 (0.450) data 0.248 (0.320) loss_x loss_x 2.0684 (1.5823) acc_x 62.5000 (58.7500) lr 1.9969e-03 eta 0:00:01
epoch [7/200] batch [5/59] time 0.416 (0.441) data 0.286 (0.311) loss_u loss_u 0.8540 (0.8319) acc_u 18.7500 (21.2500) lr 1.9969e-03 eta 0:00:23
epoch [7/200] batch [10/59] time 0.480 (0.443) data 0.350 (0.314) loss_u loss_u 0.8823 (0.8247) acc_u 6.2500 (21.5625) lr 1.9969e-03 eta 0:00:21
epoch [7/200] batch [15/59] time 0.392 (0.444) data 0.261 (0.314) loss_u loss_u 0.7817 (0.8201) acc_u 25.0000 (23.3333) lr 1.9969e-03 eta 0:00:19
epoch [7/200] batch [20/59] time 0.335 (0.438) data 0.204 (0.308) loss_u loss_u 0.9009 (0.8191) acc_u 6.2500 (23.4375) lr 1.9969e-03 eta 0:00:17
epoch [7/200] batch [25/59] time 0.353 (0.438) data 0.223 (0.308) loss_u loss_u 0.8740 (0.8145) acc_u 21.8750 (24.7500) lr 1.9969e-03 eta 0:00:14
epoch [7/200] batch [30/59] time 0.352 (0.436) data 0.221 (0.306) loss_u loss_u 0.8052 (0.8143) acc_u 25.0000 (24.4792) lr 1.9969e-03 eta 0:00:12
epoch [7/200] batch [35/59] time 0.395 (0.438) data 0.264 (0.308) loss_u loss_u 0.8340 (0.8110) acc_u 21.8750 (25.0893) lr 1.9969e-03 eta 0:00:10
epoch [7/200] batch [40/59] time 0.391 (0.437) data 0.261 (0.307) loss_u loss_u 0.8843 (0.8126) acc_u 9.3750 (24.7656) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [45/59] time 0.324 (0.434) data 0.193 (0.304) loss_u loss_u 0.8145 (0.8123) acc_u 25.0000 (25.0000) lr 1.9969e-03 eta 0:00:06
epoch [7/200] batch [50/59] time 0.671 (0.436) data 0.540 (0.306) loss_u loss_u 0.7944 (0.8133) acc_u 31.2500 (24.8750) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [55/59] time 0.316 (0.434) data 0.186 (0.304) loss_u loss_u 0.8081 (0.8154) acc_u 21.8750 (24.1477) lr 1.9969e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1799
confident_label rate tensor(0.3868, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1213
clean true:1191
clean false:22
clean_rate:0.9818631492168178
noisy true:146
noisy false:1777
after delete: len(clean_dataset) 1213
after delete: len(noisy_dataset) 1923
epoch [8/200] batch [5/37] time 0.600 (0.541) data 0.470 (0.411) loss_x loss_x 1.6953 (1.3945) acc_x 62.5000 (61.8750) lr 1.9956e-03 eta 0:00:17
epoch [8/200] batch [10/37] time 0.365 (0.490) data 0.236 (0.360) loss_x loss_x 1.1572 (1.4414) acc_x 81.2500 (62.1875) lr 1.9956e-03 eta 0:00:13
epoch [8/200] batch [15/37] time 0.425 (0.464) data 0.294 (0.334) loss_x loss_x 1.6318 (1.4665) acc_x 65.6250 (62.2917) lr 1.9956e-03 eta 0:00:10
epoch [8/200] batch [20/37] time 0.453 (0.454) data 0.324 (0.324) loss_x loss_x 1.0996 (1.5073) acc_x 71.8750 (62.0312) lr 1.9956e-03 eta 0:00:07
epoch [8/200] batch [25/37] time 0.409 (0.450) data 0.280 (0.320) loss_x loss_x 1.2861 (1.5178) acc_x 71.8750 (61.1250) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [30/37] time 0.388 (0.440) data 0.258 (0.310) loss_x loss_x 1.3975 (1.4880) acc_x 62.5000 (62.1875) lr 1.9956e-03 eta 0:00:03
epoch [8/200] batch [35/37] time 0.507 (0.454) data 0.378 (0.324) loss_x loss_x 1.8877 (1.5098) acc_x 50.0000 (62.2321) lr 1.9956e-03 eta 0:00:00
epoch [8/200] batch [5/60] time 0.343 (0.442) data 0.212 (0.312) loss_u loss_u 0.8066 (0.7992) acc_u 18.7500 (23.7500) lr 1.9956e-03 eta 0:00:24
epoch [8/200] batch [10/60] time 0.415 (0.433) data 0.284 (0.303) loss_u loss_u 0.8325 (0.7987) acc_u 18.7500 (24.0625) lr 1.9956e-03 eta 0:00:21
epoch [8/200] batch [15/60] time 0.368 (0.429) data 0.239 (0.299) loss_u loss_u 0.7896 (0.8055) acc_u 31.2500 (23.9583) lr 1.9956e-03 eta 0:00:19
epoch [8/200] batch [20/60] time 0.342 (0.426) data 0.212 (0.296) loss_u loss_u 0.8979 (0.8132) acc_u 15.6250 (23.5938) lr 1.9956e-03 eta 0:00:17
epoch [8/200] batch [25/60] time 0.368 (0.426) data 0.238 (0.296) loss_u loss_u 0.8853 (0.8191) acc_u 6.2500 (22.5000) lr 1.9956e-03 eta 0:00:14
epoch [8/200] batch [30/60] time 0.378 (0.427) data 0.247 (0.298) loss_u loss_u 0.7686 (0.8184) acc_u 37.5000 (22.6042) lr 1.9956e-03 eta 0:00:12
epoch [8/200] batch [35/60] time 0.426 (0.430) data 0.295 (0.300) loss_u loss_u 0.7661 (0.8183) acc_u 37.5000 (22.9464) lr 1.9956e-03 eta 0:00:10
epoch [8/200] batch [40/60] time 0.505 (0.435) data 0.374 (0.305) loss_u loss_u 0.8530 (0.8165) acc_u 18.7500 (23.1250) lr 1.9956e-03 eta 0:00:08
epoch [8/200] batch [45/60] time 0.361 (0.433) data 0.232 (0.303) loss_u loss_u 0.8579 (0.8194) acc_u 21.8750 (23.0556) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [50/60] time 0.401 (0.433) data 0.271 (0.303) loss_u loss_u 0.8599 (0.8176) acc_u 18.7500 (23.6250) lr 1.9956e-03 eta 0:00:04
epoch [8/200] batch [55/60] time 0.411 (0.434) data 0.279 (0.304) loss_u loss_u 0.7534 (0.8142) acc_u 40.6250 (24.2045) lr 1.9956e-03 eta 0:00:02
epoch [8/200] batch [60/60] time 0.456 (0.432) data 0.325 (0.302) loss_u loss_u 0.8452 (0.8155) acc_u 21.8750 (24.3229) lr 1.9956e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1723
confident_label rate tensor(0.4078, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1279
clean true:1262
clean false:17
clean_rate:0.9867083659108679
noisy true:151
noisy false:1706
after delete: len(clean_dataset) 1279
after delete: len(noisy_dataset) 1857
epoch [9/200] batch [5/39] time 0.475 (0.489) data 0.345 (0.359) loss_x loss_x 1.3574 (1.3994) acc_x 53.1250 (58.7500) lr 1.9940e-03 eta 0:00:16
epoch [9/200] batch [10/39] time 0.445 (0.472) data 0.316 (0.342) loss_x loss_x 1.2725 (1.4280) acc_x 53.1250 (58.7500) lr 1.9940e-03 eta 0:00:13
epoch [9/200] batch [15/39] time 0.528 (0.477) data 0.399 (0.347) loss_x loss_x 1.4121 (1.4245) acc_x 65.6250 (60.4167) lr 1.9940e-03 eta 0:00:11
epoch [9/200] batch [20/39] time 0.592 (0.483) data 0.462 (0.353) loss_x loss_x 1.3730 (1.4591) acc_x 65.6250 (60.1562) lr 1.9940e-03 eta 0:00:09
epoch [9/200] batch [25/39] time 0.384 (0.469) data 0.255 (0.340) loss_x loss_x 1.4629 (1.4530) acc_x 65.6250 (60.0000) lr 1.9940e-03 eta 0:00:06
epoch [9/200] batch [30/39] time 0.507 (0.468) data 0.377 (0.339) loss_x loss_x 1.5195 (1.4463) acc_x 56.2500 (60.7292) lr 1.9940e-03 eta 0:00:04
epoch [9/200] batch [35/39] time 0.470 (0.470) data 0.340 (0.341) loss_x loss_x 1.4277 (1.4627) acc_x 65.6250 (60.0893) lr 1.9940e-03 eta 0:00:01
epoch [9/200] batch [5/58] time 0.372 (0.455) data 0.241 (0.325) loss_u loss_u 0.8804 (0.8310) acc_u 12.5000 (23.7500) lr 1.9940e-03 eta 0:00:24
epoch [9/200] batch [10/58] time 0.328 (0.448) data 0.197 (0.318) loss_u loss_u 0.8438 (0.8358) acc_u 12.5000 (21.8750) lr 1.9940e-03 eta 0:00:21
epoch [9/200] batch [15/58] time 0.477 (0.447) data 0.346 (0.317) loss_u loss_u 0.8735 (0.8358) acc_u 15.6250 (21.2500) lr 1.9940e-03 eta 0:00:19
epoch [9/200] batch [20/58] time 0.451 (0.447) data 0.321 (0.317) loss_u loss_u 0.8364 (0.8391) acc_u 18.7500 (20.4688) lr 1.9940e-03 eta 0:00:16
epoch [9/200] batch [25/58] time 0.338 (0.442) data 0.208 (0.312) loss_u loss_u 0.9043 (0.8378) acc_u 9.3750 (20.7500) lr 1.9940e-03 eta 0:00:14
epoch [9/200] batch [30/58] time 0.421 (0.438) data 0.290 (0.309) loss_u loss_u 0.9141 (0.8396) acc_u 15.6250 (20.7292) lr 1.9940e-03 eta 0:00:12
epoch [9/200] batch [35/58] time 0.389 (0.437) data 0.259 (0.307) loss_u loss_u 0.8096 (0.8327) acc_u 25.0000 (21.2500) lr 1.9940e-03 eta 0:00:10
epoch [9/200] batch [40/58] time 0.479 (0.438) data 0.348 (0.308) loss_u loss_u 0.7603 (0.8320) acc_u 34.3750 (21.6406) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [45/58] time 0.411 (0.437) data 0.280 (0.307) loss_u loss_u 0.8960 (0.8326) acc_u 12.5000 (21.5972) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [50/58] time 0.355 (0.435) data 0.224 (0.305) loss_u loss_u 0.8286 (0.8291) acc_u 21.8750 (22.3750) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [55/58] time 0.487 (0.435) data 0.356 (0.305) loss_u loss_u 0.6904 (0.8253) acc_u 53.1250 (22.8977) lr 1.9940e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1732
confident_label rate tensor(0.4056, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1272
clean true:1254
clean false:18
clean_rate:0.9858490566037735
noisy true:150
noisy false:1714
after delete: len(clean_dataset) 1272
after delete: len(noisy_dataset) 1864
epoch [10/200] batch [5/39] time 0.484 (0.453) data 0.355 (0.323) loss_x loss_x 1.2109 (1.3297) acc_x 71.8750 (66.2500) lr 1.9921e-03 eta 0:00:15
epoch [10/200] batch [10/39] time 0.543 (0.447) data 0.413 (0.317) loss_x loss_x 1.7383 (1.3289) acc_x 56.2500 (65.0000) lr 1.9921e-03 eta 0:00:12
epoch [10/200] batch [15/39] time 0.482 (0.471) data 0.353 (0.342) loss_x loss_x 2.1035 (1.4276) acc_x 62.5000 (63.3333) lr 1.9921e-03 eta 0:00:11
epoch [10/200] batch [20/39] time 0.434 (0.462) data 0.304 (0.332) loss_x loss_x 1.6553 (1.4585) acc_x 50.0000 (61.5625) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [25/39] time 0.505 (0.461) data 0.376 (0.331) loss_x loss_x 1.2266 (1.4394) acc_x 62.5000 (61.5000) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [30/39] time 0.375 (0.455) data 0.244 (0.325) loss_x loss_x 1.7334 (1.4468) acc_x 53.1250 (61.5625) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [35/39] time 0.403 (0.458) data 0.273 (0.328) loss_x loss_x 1.5674 (1.4561) acc_x 56.2500 (61.4286) lr 1.9921e-03 eta 0:00:01
epoch [10/200] batch [5/58] time 0.456 (0.459) data 0.325 (0.329) loss_u loss_u 0.8530 (0.8362) acc_u 25.0000 (20.6250) lr 1.9921e-03 eta 0:00:24
epoch [10/200] batch [10/58] time 0.367 (0.453) data 0.236 (0.323) loss_u loss_u 0.7134 (0.8281) acc_u 34.3750 (20.6250) lr 1.9921e-03 eta 0:00:21
epoch [10/200] batch [15/58] time 0.372 (0.448) data 0.241 (0.318) loss_u loss_u 0.7510 (0.8172) acc_u 40.6250 (23.3333) lr 1.9921e-03 eta 0:00:19
epoch [10/200] batch [20/58] time 0.410 (0.447) data 0.279 (0.317) loss_u loss_u 0.7754 (0.8201) acc_u 31.2500 (22.1875) lr 1.9921e-03 eta 0:00:16
epoch [10/200] batch [25/58] time 0.406 (0.446) data 0.275 (0.315) loss_u loss_u 0.8354 (0.8163) acc_u 15.6250 (23.1250) lr 1.9921e-03 eta 0:00:14
epoch [10/200] batch [30/58] time 0.355 (0.446) data 0.224 (0.316) loss_u loss_u 0.7588 (0.8147) acc_u 31.2500 (23.4375) lr 1.9921e-03 eta 0:00:12
epoch [10/200] batch [35/58] time 0.287 (0.440) data 0.156 (0.310) loss_u loss_u 0.8159 (0.8155) acc_u 25.0000 (23.2143) lr 1.9921e-03 eta 0:00:10
epoch [10/200] batch [40/58] time 0.401 (0.437) data 0.270 (0.307) loss_u loss_u 0.8511 (0.8156) acc_u 18.7500 (23.5156) lr 1.9921e-03 eta 0:00:07
epoch [10/200] batch [45/58] time 0.410 (0.438) data 0.279 (0.308) loss_u loss_u 0.8184 (0.8118) acc_u 21.8750 (24.3056) lr 1.9921e-03 eta 0:00:05
epoch [10/200] batch [50/58] time 0.580 (0.440) data 0.449 (0.310) loss_u loss_u 0.7656 (0.8110) acc_u 28.1250 (24.6875) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [55/58] time 0.349 (0.438) data 0.218 (0.307) loss_u loss_u 0.8706 (0.8116) acc_u 12.5000 (24.4886) lr 1.9921e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1783
confident_label rate tensor(0.3887, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1219
clean true:1204
clean false:15
clean_rate:0.9876948318293683
noisy true:149
noisy false:1768
after delete: len(clean_dataset) 1219
after delete: len(noisy_dataset) 1917
epoch [11/200] batch [5/38] time 0.517 (0.435) data 0.387 (0.305) loss_x loss_x 1.1797 (1.2844) acc_x 75.0000 (68.7500) lr 1.9900e-03 eta 0:00:14
epoch [11/200] batch [10/38] time 0.449 (0.448) data 0.320 (0.318) loss_x loss_x 1.5859 (1.4063) acc_x 56.2500 (63.1250) lr 1.9900e-03 eta 0:00:12
epoch [11/200] batch [15/38] time 0.383 (0.479) data 0.253 (0.349) loss_x loss_x 1.5039 (1.4594) acc_x 56.2500 (62.2917) lr 1.9900e-03 eta 0:00:11
epoch [11/200] batch [20/38] time 0.355 (0.474) data 0.225 (0.344) loss_x loss_x 1.6455 (1.4387) acc_x 56.2500 (62.9688) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [25/38] time 0.427 (0.476) data 0.297 (0.346) loss_x loss_x 1.4219 (1.4831) acc_x 71.8750 (61.7500) lr 1.9900e-03 eta 0:00:06
epoch [11/200] batch [30/38] time 0.435 (0.474) data 0.305 (0.344) loss_x loss_x 2.1777 (1.5015) acc_x 50.0000 (61.7708) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [35/38] time 0.510 (0.473) data 0.379 (0.342) loss_x loss_x 1.5762 (1.5065) acc_x 59.3750 (61.8750) lr 1.9900e-03 eta 0:00:01
epoch [11/200] batch [5/59] time 0.369 (0.462) data 0.238 (0.332) loss_u loss_u 0.8003 (0.7922) acc_u 25.0000 (25.6250) lr 1.9900e-03 eta 0:00:24
epoch [11/200] batch [10/59] time 0.436 (0.461) data 0.305 (0.331) loss_u loss_u 0.8726 (0.8048) acc_u 12.5000 (22.1875) lr 1.9900e-03 eta 0:00:22
epoch [11/200] batch [15/59] time 0.430 (0.455) data 0.299 (0.324) loss_u loss_u 0.8091 (0.8058) acc_u 28.1250 (23.9583) lr 1.9900e-03 eta 0:00:20
epoch [11/200] batch [20/59] time 0.625 (0.453) data 0.494 (0.323) loss_u loss_u 0.7944 (0.8043) acc_u 31.2500 (24.8438) lr 1.9900e-03 eta 0:00:17
epoch [11/200] batch [25/59] time 0.482 (0.454) data 0.350 (0.323) loss_u loss_u 0.7656 (0.8046) acc_u 34.3750 (25.3750) lr 1.9900e-03 eta 0:00:15
epoch [11/200] batch [30/59] time 0.409 (0.455) data 0.278 (0.325) loss_u loss_u 0.8433 (0.8046) acc_u 15.6250 (25.6250) lr 1.9900e-03 eta 0:00:13
epoch [11/200] batch [35/59] time 0.317 (0.450) data 0.186 (0.319) loss_u loss_u 0.7964 (0.8006) acc_u 25.0000 (26.1607) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [40/59] time 0.407 (0.450) data 0.276 (0.319) loss_u loss_u 0.8813 (0.8000) acc_u 12.5000 (26.0156) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [45/59] time 0.358 (0.448) data 0.228 (0.317) loss_u loss_u 0.8589 (0.8024) acc_u 28.1250 (25.7639) lr 1.9900e-03 eta 0:00:06
epoch [11/200] batch [50/59] time 0.411 (0.448) data 0.280 (0.317) loss_u loss_u 0.8350 (0.8052) acc_u 15.6250 (24.8125) lr 1.9900e-03 eta 0:00:04
epoch [11/200] batch [55/59] time 0.371 (0.445) data 0.240 (0.314) loss_u loss_u 0.8628 (0.8062) acc_u 12.5000 (24.8864) lr 1.9900e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1734
confident_label rate tensor(0.4059, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1273
clean true:1256
clean false:17
clean_rate:0.9866457187745483
noisy true:146
noisy false:1717
after delete: len(clean_dataset) 1273
after delete: len(noisy_dataset) 1863
epoch [12/200] batch [5/39] time 0.433 (0.391) data 0.303 (0.261) loss_x loss_x 1.7979 (1.5008) acc_x 59.3750 (60.6250) lr 1.9877e-03 eta 0:00:13
epoch [12/200] batch [10/39] time 0.464 (0.407) data 0.334 (0.277) loss_x loss_x 1.9326 (1.4877) acc_x 56.2500 (60.0000) lr 1.9877e-03 eta 0:00:11
epoch [12/200] batch [15/39] time 0.556 (0.427) data 0.426 (0.297) loss_x loss_x 0.7549 (1.5013) acc_x 87.5000 (60.4167) lr 1.9877e-03 eta 0:00:10
epoch [12/200] batch [20/39] time 0.399 (0.450) data 0.269 (0.320) loss_x loss_x 1.2139 (1.4729) acc_x 59.3750 (61.8750) lr 1.9877e-03 eta 0:00:08
epoch [12/200] batch [25/39] time 0.512 (0.448) data 0.382 (0.318) loss_x loss_x 0.8506 (1.4654) acc_x 71.8750 (62.0000) lr 1.9877e-03 eta 0:00:06
epoch [12/200] batch [30/39] time 0.405 (0.449) data 0.275 (0.319) loss_x loss_x 1.1367 (1.4891) acc_x 62.5000 (61.6667) lr 1.9877e-03 eta 0:00:04
epoch [12/200] batch [35/39] time 0.440 (0.455) data 0.310 (0.324) loss_x loss_x 1.3965 (1.4871) acc_x 56.2500 (61.4286) lr 1.9877e-03 eta 0:00:01
epoch [12/200] batch [5/58] time 0.412 (0.455) data 0.281 (0.324) loss_u loss_u 0.7529 (0.7921) acc_u 31.2500 (26.8750) lr 1.9877e-03 eta 0:00:24
epoch [12/200] batch [10/58] time 0.524 (0.453) data 0.393 (0.323) loss_u loss_u 0.7837 (0.7975) acc_u 31.2500 (26.8750) lr 1.9877e-03 eta 0:00:21
epoch [12/200] batch [15/58] time 0.417 (0.450) data 0.286 (0.320) loss_u loss_u 0.8726 (0.8075) acc_u 28.1250 (26.2500) lr 1.9877e-03 eta 0:00:19
epoch [12/200] batch [20/58] time 0.497 (0.445) data 0.366 (0.315) loss_u loss_u 0.9194 (0.8098) acc_u 9.3750 (25.6250) lr 1.9877e-03 eta 0:00:16
epoch [12/200] batch [25/58] time 0.370 (0.443) data 0.239 (0.312) loss_u loss_u 0.7837 (0.8142) acc_u 31.2500 (25.0000) lr 1.9877e-03 eta 0:00:14
epoch [12/200] batch [30/58] time 0.351 (0.440) data 0.221 (0.309) loss_u loss_u 0.7695 (0.8108) acc_u 34.3750 (25.4167) lr 1.9877e-03 eta 0:00:12
epoch [12/200] batch [35/58] time 0.366 (0.445) data 0.235 (0.314) loss_u loss_u 0.7969 (0.8114) acc_u 25.0000 (25.0000) lr 1.9877e-03 eta 0:00:10
epoch [12/200] batch [40/58] time 0.406 (0.442) data 0.275 (0.311) loss_u loss_u 0.8140 (0.8114) acc_u 21.8750 (24.6875) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [45/58] time 0.373 (0.439) data 0.242 (0.308) loss_u loss_u 0.8252 (0.8160) acc_u 28.1250 (24.0278) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [50/58] time 0.361 (0.437) data 0.230 (0.307) loss_u loss_u 0.8193 (0.8182) acc_u 28.1250 (23.9375) lr 1.9877e-03 eta 0:00:03
epoch [12/200] batch [55/58] time 0.443 (0.435) data 0.312 (0.305) loss_u loss_u 0.8726 (0.8217) acc_u 15.6250 (23.4659) lr 1.9877e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1702
confident_label rate tensor(0.4094, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1284
clean true:1272
clean false:12
clean_rate:0.9906542056074766
noisy true:162
noisy false:1690
after delete: len(clean_dataset) 1284
after delete: len(noisy_dataset) 1852
epoch [13/200] batch [5/40] time 0.437 (0.460) data 0.306 (0.330) loss_x loss_x 1.2090 (1.4736) acc_x 75.0000 (64.3750) lr 1.9851e-03 eta 0:00:16
epoch [13/200] batch [10/40] time 0.340 (0.431) data 0.210 (0.301) loss_x loss_x 2.0332 (1.4602) acc_x 56.2500 (65.6250) lr 1.9851e-03 eta 0:00:12
epoch [13/200] batch [15/40] time 0.521 (0.450) data 0.391 (0.320) loss_x loss_x 1.4893 (1.4960) acc_x 65.6250 (64.1667) lr 1.9851e-03 eta 0:00:11
epoch [13/200] batch [20/40] time 0.463 (0.448) data 0.333 (0.318) loss_x loss_x 1.5400 (1.5278) acc_x 53.1250 (62.3438) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [25/40] time 0.505 (0.455) data 0.374 (0.325) loss_x loss_x 1.5234 (1.5254) acc_x 62.5000 (61.6250) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [30/40] time 0.503 (0.455) data 0.373 (0.325) loss_x loss_x 1.1143 (1.5148) acc_x 75.0000 (62.0833) lr 1.9851e-03 eta 0:00:04
epoch [13/200] batch [35/40] time 0.491 (0.464) data 0.361 (0.334) loss_x loss_x 1.3164 (1.4992) acc_x 71.8750 (62.4107) lr 1.9851e-03 eta 0:00:02
epoch [13/200] batch [40/40] time 0.407 (0.463) data 0.277 (0.332) loss_x loss_x 1.2979 (1.4746) acc_x 50.0000 (62.2656) lr 1.9851e-03 eta 0:00:00
epoch [13/200] batch [5/57] time 0.384 (0.457) data 0.253 (0.327) loss_u loss_u 0.7554 (0.7719) acc_u 37.5000 (28.7500) lr 1.9851e-03 eta 0:00:23
epoch [13/200] batch [10/57] time 0.430 (0.451) data 0.298 (0.320) loss_u loss_u 0.7915 (0.7942) acc_u 28.1250 (25.6250) lr 1.9851e-03 eta 0:00:21
epoch [13/200] batch [15/57] time 0.410 (0.446) data 0.279 (0.315) loss_u loss_u 0.8457 (0.7979) acc_u 18.7500 (25.0000) lr 1.9851e-03 eta 0:00:18
epoch [13/200] batch [20/57] time 0.372 (0.443) data 0.242 (0.313) loss_u loss_u 0.8223 (0.8007) acc_u 21.8750 (25.7812) lr 1.9851e-03 eta 0:00:16
epoch [13/200] batch [25/57] time 0.609 (0.442) data 0.478 (0.311) loss_u loss_u 0.7441 (0.8027) acc_u 31.2500 (25.1250) lr 1.9851e-03 eta 0:00:14
epoch [13/200] batch [30/57] time 0.432 (0.442) data 0.302 (0.311) loss_u loss_u 0.7949 (0.8074) acc_u 21.8750 (24.6875) lr 1.9851e-03 eta 0:00:11
epoch [13/200] batch [35/57] time 0.355 (0.441) data 0.224 (0.310) loss_u loss_u 0.8301 (0.8044) acc_u 31.2500 (25.3571) lr 1.9851e-03 eta 0:00:09
epoch [13/200] batch [40/57] time 0.385 (0.442) data 0.254 (0.311) loss_u loss_u 0.9097 (0.8091) acc_u 12.5000 (24.6875) lr 1.9851e-03 eta 0:00:07
epoch [13/200] batch [45/57] time 0.442 (0.442) data 0.311 (0.311) loss_u loss_u 0.8228 (0.8091) acc_u 21.8750 (24.7222) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [50/57] time 0.314 (0.437) data 0.183 (0.307) loss_u loss_u 0.8081 (0.8066) acc_u 21.8750 (25.3125) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [55/57] time 0.480 (0.438) data 0.349 (0.307) loss_u loss_u 0.8301 (0.8058) acc_u 28.1250 (25.5682) lr 1.9851e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1745
confident_label rate tensor(0.3951, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1239
clean true:1227
clean false:12
clean_rate:0.9903147699757869
noisy true:164
noisy false:1733
after delete: len(clean_dataset) 1239
after delete: len(noisy_dataset) 1897
epoch [14/200] batch [5/38] time 0.406 (0.494) data 0.276 (0.363) loss_x loss_x 1.7744 (1.6701) acc_x 56.2500 (58.7500) lr 1.9823e-03 eta 0:00:16
epoch [14/200] batch [10/38] time 0.509 (0.477) data 0.379 (0.346) loss_x loss_x 1.5918 (1.6308) acc_x 56.2500 (58.4375) lr 1.9823e-03 eta 0:00:13
epoch [14/200] batch [15/38] time 0.373 (0.472) data 0.243 (0.342) loss_x loss_x 1.4131 (1.5040) acc_x 71.8750 (61.4583) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [20/38] time 0.516 (0.471) data 0.386 (0.341) loss_x loss_x 1.3525 (1.4654) acc_x 81.2500 (62.6562) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [25/38] time 0.438 (0.468) data 0.308 (0.338) loss_x loss_x 1.0488 (1.4091) acc_x 65.6250 (63.2500) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [30/38] time 0.419 (0.466) data 0.288 (0.335) loss_x loss_x 1.3887 (1.4170) acc_x 59.3750 (63.6458) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [35/38] time 0.453 (0.463) data 0.323 (0.333) loss_x loss_x 1.3682 (1.4584) acc_x 62.5000 (62.7679) lr 1.9823e-03 eta 0:00:01
epoch [14/200] batch [5/59] time 0.386 (0.459) data 0.255 (0.329) loss_u loss_u 0.7881 (0.8033) acc_u 31.2500 (22.5000) lr 1.9823e-03 eta 0:00:24
epoch [14/200] batch [10/59] time 0.583 (0.454) data 0.453 (0.323) loss_u loss_u 0.8657 (0.8259) acc_u 12.5000 (20.0000) lr 1.9823e-03 eta 0:00:22
epoch [14/200] batch [15/59] time 0.526 (0.458) data 0.396 (0.328) loss_u loss_u 0.7559 (0.8243) acc_u 31.2500 (20.0000) lr 1.9823e-03 eta 0:00:20
epoch [14/200] batch [20/59] time 0.330 (0.459) data 0.200 (0.328) loss_u loss_u 0.8335 (0.8222) acc_u 15.6250 (20.6250) lr 1.9823e-03 eta 0:00:17
epoch [14/200] batch [25/59] time 0.428 (0.452) data 0.298 (0.322) loss_u loss_u 0.7480 (0.8135) acc_u 43.7500 (22.3750) lr 1.9823e-03 eta 0:00:15
epoch [14/200] batch [30/59] time 0.391 (0.448) data 0.260 (0.318) loss_u loss_u 0.8198 (0.8171) acc_u 18.7500 (22.0833) lr 1.9823e-03 eta 0:00:13
epoch [14/200] batch [35/59] time 0.396 (0.446) data 0.265 (0.315) loss_u loss_u 0.8198 (0.8160) acc_u 18.7500 (22.8571) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [40/59] time 0.407 (0.442) data 0.276 (0.311) loss_u loss_u 0.7866 (0.8148) acc_u 25.0000 (22.8125) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [45/59] time 0.385 (0.442) data 0.254 (0.311) loss_u loss_u 0.8071 (0.8155) acc_u 21.8750 (22.8472) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [50/59] time 0.419 (0.442) data 0.288 (0.311) loss_u loss_u 0.8853 (0.8157) acc_u 12.5000 (22.7500) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [55/59] time 0.416 (0.439) data 0.285 (0.308) loss_u loss_u 0.8535 (0.8114) acc_u 15.6250 (23.5227) lr 1.9823e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1694
confident_label rate tensor(0.4126, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1294
clean true:1283
clean false:11
clean_rate:0.991499227202473
noisy true:159
noisy false:1683
after delete: len(clean_dataset) 1294
after delete: len(noisy_dataset) 1842
epoch [15/200] batch [5/40] time 0.397 (0.428) data 0.267 (0.298) loss_x loss_x 1.1270 (1.3594) acc_x 62.5000 (63.1250) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [10/40] time 0.409 (0.437) data 0.278 (0.307) loss_x loss_x 1.1455 (1.3123) acc_x 78.1250 (65.3125) lr 1.9792e-03 eta 0:00:13
epoch [15/200] batch [15/40] time 0.441 (0.450) data 0.311 (0.320) loss_x loss_x 1.1729 (1.3245) acc_x 62.5000 (64.3750) lr 1.9792e-03 eta 0:00:11
epoch [15/200] batch [20/40] time 0.445 (0.466) data 0.313 (0.335) loss_x loss_x 1.1953 (1.3588) acc_x 75.0000 (64.3750) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [25/40] time 0.492 (0.462) data 0.362 (0.331) loss_x loss_x 1.7236 (1.3893) acc_x 62.5000 (63.1250) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [30/40] time 0.419 (0.456) data 0.289 (0.325) loss_x loss_x 1.3027 (1.4057) acc_x 56.2500 (61.9792) lr 1.9792e-03 eta 0:00:04
epoch [15/200] batch [35/40] time 0.333 (0.449) data 0.203 (0.318) loss_x loss_x 1.3174 (1.3907) acc_x 68.7500 (62.6786) lr 1.9792e-03 eta 0:00:02
epoch [15/200] batch [40/40] time 0.382 (0.451) data 0.253 (0.320) loss_x loss_x 1.3232 (1.3905) acc_x 65.6250 (62.8125) lr 1.9792e-03 eta 0:00:00
epoch [15/200] batch [5/57] time 0.453 (0.459) data 0.322 (0.328) loss_u loss_u 0.7646 (0.7963) acc_u 31.2500 (25.6250) lr 1.9792e-03 eta 0:00:23
epoch [15/200] batch [10/57] time 0.577 (0.458) data 0.446 (0.327) loss_u loss_u 0.8535 (0.8294) acc_u 18.7500 (21.8750) lr 1.9792e-03 eta 0:00:21
epoch [15/200] batch [15/57] time 0.448 (0.454) data 0.317 (0.323) loss_u loss_u 0.7427 (0.8187) acc_u 37.5000 (23.7500) lr 1.9792e-03 eta 0:00:19
epoch [15/200] batch [20/57] time 0.488 (0.452) data 0.358 (0.321) loss_u loss_u 0.7583 (0.8204) acc_u 46.8750 (24.5312) lr 1.9792e-03 eta 0:00:16
epoch [15/200] batch [25/57] time 0.342 (0.448) data 0.211 (0.318) loss_u loss_u 0.8555 (0.8262) acc_u 18.7500 (23.3750) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [30/57] time 0.483 (0.446) data 0.353 (0.316) loss_u loss_u 0.8579 (0.8253) acc_u 15.6250 (23.1250) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [35/57] time 0.422 (0.444) data 0.291 (0.314) loss_u loss_u 0.8413 (0.8233) acc_u 25.0000 (23.5714) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [40/57] time 0.539 (0.442) data 0.408 (0.312) loss_u loss_u 0.7437 (0.8183) acc_u 37.5000 (24.5312) lr 1.9792e-03 eta 0:00:07
epoch [15/200] batch [45/57] time 0.418 (0.440) data 0.288 (0.310) loss_u loss_u 0.7544 (0.8168) acc_u 28.1250 (24.3750) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [50/57] time 0.523 (0.438) data 0.391 (0.308) loss_u loss_u 0.7402 (0.8150) acc_u 31.2500 (24.6250) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [55/57] time 0.384 (0.437) data 0.253 (0.306) loss_u loss_u 0.7949 (0.8131) acc_u 18.7500 (24.7159) lr 1.9792e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1657
confident_label rate tensor(0.4206, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1319
clean true:1306
clean false:13
clean_rate:0.9901440485216073
noisy true:173
noisy false:1644
after delete: len(clean_dataset) 1319
after delete: len(noisy_dataset) 1817
epoch [16/200] batch [5/41] time 0.429 (0.528) data 0.300 (0.399) loss_x loss_x 1.7061 (1.2711) acc_x 65.6250 (71.2500) lr 1.9759e-03 eta 0:00:19
epoch [16/200] batch [10/41] time 0.445 (0.483) data 0.315 (0.354) loss_x loss_x 1.5898 (1.3114) acc_x 68.7500 (68.4375) lr 1.9759e-03 eta 0:00:14
epoch [16/200] batch [15/41] time 0.399 (0.471) data 0.269 (0.341) loss_x loss_x 1.7539 (1.3794) acc_x 50.0000 (65.4167) lr 1.9759e-03 eta 0:00:12
epoch [16/200] batch [20/41] time 0.458 (0.481) data 0.328 (0.351) loss_x loss_x 1.5186 (1.3827) acc_x 65.6250 (64.3750) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [25/41] time 0.422 (0.470) data 0.292 (0.340) loss_x loss_x 1.4668 (1.3875) acc_x 56.2500 (64.5000) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [30/41] time 0.382 (0.460) data 0.253 (0.330) loss_x loss_x 1.5205 (1.4210) acc_x 62.5000 (63.8542) lr 1.9759e-03 eta 0:00:05
epoch [16/200] batch [35/41] time 0.459 (0.462) data 0.330 (0.332) loss_x loss_x 1.3564 (1.4198) acc_x 62.5000 (64.1964) lr 1.9759e-03 eta 0:00:02
epoch [16/200] batch [40/41] time 0.348 (0.452) data 0.218 (0.322) loss_x loss_x 2.2070 (1.4283) acc_x 31.2500 (63.2031) lr 1.9759e-03 eta 0:00:00
epoch [16/200] batch [5/56] time 0.385 (0.447) data 0.254 (0.317) loss_u loss_u 0.8169 (0.8381) acc_u 34.3750 (23.1250) lr 1.9759e-03 eta 0:00:22
epoch [16/200] batch [10/56] time 0.502 (0.447) data 0.370 (0.317) loss_u loss_u 0.7686 (0.7978) acc_u 25.0000 (26.2500) lr 1.9759e-03 eta 0:00:20
epoch [16/200] batch [15/56] time 0.480 (0.446) data 0.349 (0.316) loss_u loss_u 0.7295 (0.8024) acc_u 46.8750 (26.8750) lr 1.9759e-03 eta 0:00:18
epoch [16/200] batch [20/56] time 0.330 (0.446) data 0.199 (0.316) loss_u loss_u 0.8174 (0.8017) acc_u 25.0000 (26.8750) lr 1.9759e-03 eta 0:00:16
epoch [16/200] batch [25/56] time 0.331 (0.443) data 0.200 (0.312) loss_u loss_u 0.8667 (0.8008) acc_u 21.8750 (26.7500) lr 1.9759e-03 eta 0:00:13
epoch [16/200] batch [30/56] time 0.455 (0.442) data 0.324 (0.311) loss_u loss_u 0.7202 (0.7982) acc_u 40.6250 (27.0833) lr 1.9759e-03 eta 0:00:11
epoch [16/200] batch [35/56] time 0.446 (0.442) data 0.315 (0.312) loss_u loss_u 0.7944 (0.7982) acc_u 31.2500 (27.2321) lr 1.9759e-03 eta 0:00:09
epoch [16/200] batch [40/56] time 0.377 (0.439) data 0.246 (0.309) loss_u loss_u 0.7803 (0.7989) acc_u 18.7500 (27.1875) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [45/56] time 0.381 (0.437) data 0.250 (0.306) loss_u loss_u 0.8164 (0.8014) acc_u 21.8750 (26.4583) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [50/56] time 0.418 (0.438) data 0.287 (0.307) loss_u loss_u 0.7959 (0.8022) acc_u 28.1250 (26.1875) lr 1.9759e-03 eta 0:00:02
epoch [16/200] batch [55/56] time 0.332 (0.433) data 0.201 (0.302) loss_u loss_u 0.8481 (0.8035) acc_u 15.6250 (25.9659) lr 1.9759e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1702
confident_label rate tensor(0.4088, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1282
clean true:1271
clean false:11
clean_rate:0.9914196567862714
noisy true:163
noisy false:1691
after delete: len(clean_dataset) 1282
after delete: len(noisy_dataset) 1854
epoch [17/200] batch [5/40] time 0.392 (0.461) data 0.262 (0.331) loss_x loss_x 1.6553 (1.4217) acc_x 56.2500 (61.8750) lr 1.9724e-03 eta 0:00:16
epoch [17/200] batch [10/40] time 0.482 (0.479) data 0.352 (0.348) loss_x loss_x 1.4443 (1.3326) acc_x 65.6250 (64.3750) lr 1.9724e-03 eta 0:00:14
epoch [17/200] batch [15/40] time 0.481 (0.467) data 0.349 (0.336) loss_x loss_x 1.7070 (1.3792) acc_x 62.5000 (63.3333) lr 1.9724e-03 eta 0:00:11
epoch [17/200] batch [20/40] time 0.394 (0.464) data 0.264 (0.333) loss_x loss_x 2.2695 (1.4304) acc_x 50.0000 (62.1875) lr 1.9724e-03 eta 0:00:09
epoch [17/200] batch [25/40] time 0.637 (0.470) data 0.507 (0.340) loss_x loss_x 1.9668 (1.4693) acc_x 53.1250 (62.6250) lr 1.9724e-03 eta 0:00:07
epoch [17/200] batch [30/40] time 0.468 (0.459) data 0.337 (0.328) loss_x loss_x 1.2246 (1.4671) acc_x 65.6250 (62.6042) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [35/40] time 0.378 (0.457) data 0.247 (0.327) loss_x loss_x 1.4150 (1.4783) acc_x 56.2500 (61.6071) lr 1.9724e-03 eta 0:00:02
epoch [17/200] batch [40/40] time 0.435 (0.462) data 0.304 (0.331) loss_x loss_x 1.6064 (1.4739) acc_x 65.6250 (61.7188) lr 1.9724e-03 eta 0:00:00
epoch [17/200] batch [5/57] time 0.328 (0.460) data 0.198 (0.329) loss_u loss_u 0.6992 (0.7711) acc_u 40.6250 (31.8750) lr 1.9724e-03 eta 0:00:23
epoch [17/200] batch [10/57] time 0.460 (0.455) data 0.329 (0.324) loss_u loss_u 0.8745 (0.7985) acc_u 15.6250 (26.8750) lr 1.9724e-03 eta 0:00:21
epoch [17/200] batch [15/57] time 0.421 (0.448) data 0.291 (0.317) loss_u loss_u 0.7920 (0.7918) acc_u 28.1250 (28.7500) lr 1.9724e-03 eta 0:00:18
epoch [17/200] batch [20/57] time 0.467 (0.445) data 0.336 (0.314) loss_u loss_u 0.8164 (0.8029) acc_u 21.8750 (27.3438) lr 1.9724e-03 eta 0:00:16
epoch [17/200] batch [25/57] time 0.417 (0.444) data 0.286 (0.313) loss_u loss_u 0.7178 (0.7978) acc_u 40.6250 (28.8750) lr 1.9724e-03 eta 0:00:14
epoch [17/200] batch [30/57] time 0.372 (0.440) data 0.241 (0.310) loss_u loss_u 0.7607 (0.7958) acc_u 31.2500 (28.3333) lr 1.9724e-03 eta 0:00:11
epoch [17/200] batch [35/57] time 0.364 (0.439) data 0.233 (0.309) loss_u loss_u 0.7739 (0.7972) acc_u 31.2500 (28.1250) lr 1.9724e-03 eta 0:00:09
epoch [17/200] batch [40/57] time 0.372 (0.435) data 0.241 (0.304) loss_u loss_u 0.7876 (0.7978) acc_u 25.0000 (27.4219) lr 1.9724e-03 eta 0:00:07
epoch [17/200] batch [45/57] time 0.368 (0.434) data 0.238 (0.303) loss_u loss_u 0.8066 (0.7958) acc_u 31.2500 (27.3611) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [50/57] time 0.310 (0.433) data 0.180 (0.302) loss_u loss_u 0.8228 (0.7949) acc_u 21.8750 (27.3750) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [55/57] time 0.643 (0.435) data 0.512 (0.304) loss_u loss_u 0.7871 (0.7954) acc_u 25.0000 (27.1591) lr 1.9724e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1635
confident_label rate tensor(0.4314, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1353
clean true:1336
clean false:17
clean_rate:0.9874353288987435
noisy true:165
noisy false:1618
after delete: len(clean_dataset) 1353
after delete: len(noisy_dataset) 1783
epoch [18/200] batch [5/42] time 0.415 (0.484) data 0.286 (0.354) loss_x loss_x 1.4688 (1.4943) acc_x 62.5000 (60.0000) lr 1.9686e-03 eta 0:00:17
epoch [18/200] batch [10/42] time 0.431 (0.460) data 0.301 (0.330) loss_x loss_x 1.6211 (1.4742) acc_x 50.0000 (58.7500) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [15/42] time 0.376 (0.454) data 0.245 (0.324) loss_x loss_x 1.5635 (1.4932) acc_x 53.1250 (59.5833) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [20/42] time 0.505 (0.463) data 0.375 (0.333) loss_x loss_x 0.7949 (1.4563) acc_x 87.5000 (62.3438) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [25/42] time 0.402 (0.463) data 0.272 (0.332) loss_x loss_x 1.4062 (1.4024) acc_x 56.2500 (63.6250) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [30/42] time 0.380 (0.463) data 0.250 (0.333) loss_x loss_x 1.1758 (1.4240) acc_x 65.6250 (62.9167) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [35/42] time 0.339 (0.458) data 0.209 (0.328) loss_x loss_x 1.6279 (1.4492) acc_x 62.5000 (62.7679) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [40/42] time 0.443 (0.454) data 0.313 (0.323) loss_x loss_x 1.4121 (1.4615) acc_x 59.3750 (61.8750) lr 1.9686e-03 eta 0:00:00
epoch [18/200] batch [5/55] time 0.369 (0.447) data 0.238 (0.317) loss_u loss_u 0.8843 (0.8282) acc_u 21.8750 (26.2500) lr 1.9686e-03 eta 0:00:22
epoch [18/200] batch [10/55] time 0.433 (0.447) data 0.302 (0.316) loss_u loss_u 0.8101 (0.8139) acc_u 31.2500 (26.8750) lr 1.9686e-03 eta 0:00:20
epoch [18/200] batch [15/55] time 0.491 (0.448) data 0.360 (0.318) loss_u loss_u 0.7871 (0.8194) acc_u 28.1250 (24.3750) lr 1.9686e-03 eta 0:00:17
epoch [18/200] batch [20/55] time 0.341 (0.446) data 0.211 (0.316) loss_u loss_u 0.8999 (0.8145) acc_u 6.2500 (23.5938) lr 1.9686e-03 eta 0:00:15
epoch [18/200] batch [25/55] time 0.353 (0.445) data 0.223 (0.315) loss_u loss_u 0.6978 (0.8081) acc_u 37.5000 (24.5000) lr 1.9686e-03 eta 0:00:13
epoch [18/200] batch [30/55] time 0.387 (0.442) data 0.256 (0.312) loss_u loss_u 0.8711 (0.8074) acc_u 15.6250 (24.3750) lr 1.9686e-03 eta 0:00:11
epoch [18/200] batch [35/55] time 0.489 (0.443) data 0.358 (0.313) loss_u loss_u 0.7031 (0.8005) acc_u 37.5000 (25.2679) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [40/55] time 0.427 (0.442) data 0.296 (0.311) loss_u loss_u 0.8716 (0.8024) acc_u 18.7500 (25.0000) lr 1.9686e-03 eta 0:00:06
epoch [18/200] batch [45/55] time 0.396 (0.440) data 0.265 (0.310) loss_u loss_u 0.7979 (0.8022) acc_u 28.1250 (25.2778) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [50/55] time 0.360 (0.439) data 0.229 (0.309) loss_u loss_u 0.8340 (0.8017) acc_u 15.6250 (25.2500) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [55/55] time 0.400 (0.437) data 0.269 (0.306) loss_u loss_u 0.7646 (0.8015) acc_u 25.0000 (25.3409) lr 1.9686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1588
confident_label rate tensor(0.4439, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1392
clean true:1380
clean false:12
clean_rate:0.9913793103448276
noisy true:168
noisy false:1576
after delete: len(clean_dataset) 1392
after delete: len(noisy_dataset) 1744
epoch [19/200] batch [5/43] time 0.381 (0.456) data 0.251 (0.326) loss_x loss_x 1.6484 (1.2749) acc_x 59.3750 (68.7500) lr 1.9646e-03 eta 0:00:17
epoch [19/200] batch [10/43] time 0.534 (0.487) data 0.401 (0.357) loss_x loss_x 1.4648 (1.3871) acc_x 71.8750 (66.8750) lr 1.9646e-03 eta 0:00:16
epoch [19/200] batch [15/43] time 0.618 (0.524) data 0.488 (0.393) loss_x loss_x 1.4434 (1.4157) acc_x 65.6250 (63.5417) lr 1.9646e-03 eta 0:00:14
epoch [19/200] batch [20/43] time 0.461 (0.498) data 0.332 (0.368) loss_x loss_x 1.4023 (1.3919) acc_x 50.0000 (64.2188) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [25/43] time 0.393 (0.481) data 0.263 (0.350) loss_x loss_x 1.5049 (1.4297) acc_x 62.5000 (62.8750) lr 1.9646e-03 eta 0:00:08
epoch [19/200] batch [30/43] time 0.582 (0.486) data 0.452 (0.356) loss_x loss_x 1.6016 (1.4349) acc_x 62.5000 (62.6042) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [35/43] time 0.450 (0.484) data 0.319 (0.353) loss_x loss_x 1.0566 (1.4364) acc_x 75.0000 (62.5000) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [40/43] time 0.388 (0.476) data 0.258 (0.346) loss_x loss_x 1.1689 (1.4237) acc_x 78.1250 (63.2031) lr 1.9646e-03 eta 0:00:01
epoch [19/200] batch [5/54] time 0.391 (0.472) data 0.260 (0.342) loss_u loss_u 0.8633 (0.8107) acc_u 21.8750 (24.3750) lr 1.9646e-03 eta 0:00:23
epoch [19/200] batch [10/54] time 0.387 (0.469) data 0.256 (0.339) loss_u loss_u 0.7866 (0.8087) acc_u 28.1250 (26.5625) lr 1.9646e-03 eta 0:00:20
epoch [19/200] batch [15/54] time 0.532 (0.466) data 0.400 (0.335) loss_u loss_u 0.8184 (0.8086) acc_u 25.0000 (26.6667) lr 1.9646e-03 eta 0:00:18
epoch [19/200] batch [20/54] time 0.363 (0.459) data 0.232 (0.328) loss_u loss_u 0.8071 (0.8154) acc_u 18.7500 (24.6875) lr 1.9646e-03 eta 0:00:15
epoch [19/200] batch [25/54] time 0.341 (0.455) data 0.210 (0.325) loss_u loss_u 0.9180 (0.8149) acc_u 9.3750 (24.5000) lr 1.9646e-03 eta 0:00:13
epoch [19/200] batch [30/54] time 0.431 (0.452) data 0.300 (0.321) loss_u loss_u 0.8125 (0.8128) acc_u 15.6250 (24.1667) lr 1.9646e-03 eta 0:00:10
epoch [19/200] batch [35/54] time 0.297 (0.448) data 0.165 (0.318) loss_u loss_u 0.8477 (0.8106) acc_u 15.6250 (24.3750) lr 1.9646e-03 eta 0:00:08
epoch [19/200] batch [40/54] time 0.318 (0.445) data 0.187 (0.314) loss_u loss_u 0.8174 (0.8138) acc_u 21.8750 (23.5938) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [45/54] time 0.361 (0.441) data 0.230 (0.310) loss_u loss_u 0.7705 (0.8114) acc_u 25.0000 (23.9583) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [50/54] time 0.377 (0.442) data 0.246 (0.311) loss_u loss_u 0.8306 (0.8133) acc_u 21.8750 (23.7500) lr 1.9646e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1634
confident_label rate tensor(0.4298, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1348
clean true:1336
clean false:12
clean_rate:0.9910979228486647
noisy true:166
noisy false:1622
after delete: len(clean_dataset) 1348
after delete: len(noisy_dataset) 1788
epoch [20/200] batch [5/42] time 0.498 (0.452) data 0.367 (0.322) loss_x loss_x 1.6221 (1.3822) acc_x 71.8750 (65.0000) lr 1.9603e-03 eta 0:00:16
epoch [20/200] batch [10/42] time 0.430 (0.457) data 0.299 (0.326) loss_x loss_x 0.7666 (1.3032) acc_x 84.3750 (67.1875) lr 1.9603e-03 eta 0:00:14
epoch [20/200] batch [15/42] time 0.488 (0.465) data 0.358 (0.335) loss_x loss_x 0.9360 (1.3232) acc_x 81.2500 (65.2083) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [20/42] time 0.503 (0.457) data 0.372 (0.326) loss_x loss_x 1.3057 (1.3053) acc_x 71.8750 (65.1562) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [25/42] time 0.453 (0.453) data 0.323 (0.323) loss_x loss_x 0.9009 (1.3567) acc_x 71.8750 (63.5000) lr 1.9603e-03 eta 0:00:07
epoch [20/200] batch [30/42] time 0.382 (0.451) data 0.251 (0.320) loss_x loss_x 1.8125 (1.3945) acc_x 53.1250 (63.1250) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [35/42] time 0.660 (0.456) data 0.529 (0.325) loss_x loss_x 1.5195 (1.3789) acc_x 59.3750 (63.2143) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [40/42] time 0.515 (0.453) data 0.385 (0.323) loss_x loss_x 1.6719 (1.3701) acc_x 59.3750 (63.9062) lr 1.9603e-03 eta 0:00:00
epoch [20/200] batch [5/55] time 0.360 (0.447) data 0.230 (0.316) loss_u loss_u 0.7563 (0.7988) acc_u 25.0000 (23.7500) lr 1.9603e-03 eta 0:00:22
epoch [20/200] batch [10/55] time 0.335 (0.443) data 0.204 (0.312) loss_u loss_u 0.8403 (0.7972) acc_u 21.8750 (25.3125) lr 1.9603e-03 eta 0:00:19
epoch [20/200] batch [15/55] time 0.383 (0.443) data 0.253 (0.313) loss_u loss_u 0.8281 (0.8030) acc_u 18.7500 (24.3750) lr 1.9603e-03 eta 0:00:17
epoch [20/200] batch [20/55] time 0.350 (0.441) data 0.219 (0.311) loss_u loss_u 0.9062 (0.8122) acc_u 12.5000 (23.4375) lr 1.9603e-03 eta 0:00:15
epoch [20/200] batch [25/55] time 0.517 (0.439) data 0.387 (0.308) loss_u loss_u 0.7026 (0.8058) acc_u 43.7500 (24.2500) lr 1.9603e-03 eta 0:00:13
epoch [20/200] batch [30/55] time 0.382 (0.437) data 0.251 (0.306) loss_u loss_u 0.7769 (0.7964) acc_u 25.0000 (25.5208) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [35/55] time 0.345 (0.436) data 0.215 (0.306) loss_u loss_u 0.8408 (0.7982) acc_u 21.8750 (25.6250) lr 1.9603e-03 eta 0:00:08
epoch [20/200] batch [40/55] time 0.515 (0.437) data 0.385 (0.307) loss_u loss_u 0.8086 (0.8006) acc_u 18.7500 (25.0000) lr 1.9603e-03 eta 0:00:06
epoch [20/200] batch [45/55] time 0.460 (0.439) data 0.329 (0.309) loss_u loss_u 0.8081 (0.7959) acc_u 28.1250 (25.6250) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [50/55] time 0.344 (0.437) data 0.214 (0.307) loss_u loss_u 0.8081 (0.7959) acc_u 28.1250 (25.7500) lr 1.9603e-03 eta 0:00:02
epoch [20/200] batch [55/55] time 0.547 (0.438) data 0.416 (0.308) loss_u loss_u 0.8071 (0.7959) acc_u 18.7500 (25.5682) lr 1.9603e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1665
confident_label rate tensor(0.4190, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1314
clean true:1306
clean false:8
clean_rate:0.9939117199391172
noisy true:165
noisy false:1657
after delete: len(clean_dataset) 1314
after delete: len(noisy_dataset) 1822
epoch [21/200] batch [5/41] time 0.428 (0.453) data 0.298 (0.323) loss_x loss_x 1.9258 (1.3176) acc_x 56.2500 (66.8750) lr 1.9558e-03 eta 0:00:16
epoch [21/200] batch [10/41] time 0.504 (0.466) data 0.373 (0.336) loss_x loss_x 1.8633 (1.3705) acc_x 65.6250 (67.5000) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [15/41] time 0.504 (0.481) data 0.374 (0.351) loss_x loss_x 1.2900 (1.3822) acc_x 65.6250 (66.6667) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [20/41] time 0.515 (0.473) data 0.385 (0.343) loss_x loss_x 1.3613 (1.4047) acc_x 62.5000 (65.3125) lr 1.9558e-03 eta 0:00:09
epoch [21/200] batch [25/41] time 0.532 (0.466) data 0.401 (0.336) loss_x loss_x 1.0176 (1.3718) acc_x 75.0000 (66.1250) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [30/41] time 0.372 (0.455) data 0.242 (0.324) loss_x loss_x 1.2959 (1.3972) acc_x 59.3750 (65.0000) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [35/41] time 0.579 (0.458) data 0.449 (0.328) loss_x loss_x 1.4092 (1.3886) acc_x 53.1250 (64.9107) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [40/41] time 0.474 (0.465) data 0.344 (0.335) loss_x loss_x 1.2041 (1.3828) acc_x 71.8750 (65.5469) lr 1.9558e-03 eta 0:00:00
epoch [21/200] batch [5/56] time 0.344 (0.459) data 0.213 (0.328) loss_u loss_u 0.7715 (0.7994) acc_u 25.0000 (27.5000) lr 1.9558e-03 eta 0:00:23
epoch [21/200] batch [10/56] time 0.339 (0.454) data 0.209 (0.324) loss_u loss_u 0.7983 (0.7890) acc_u 25.0000 (28.4375) lr 1.9558e-03 eta 0:00:20
epoch [21/200] batch [15/56] time 0.406 (0.452) data 0.276 (0.322) loss_u loss_u 0.7065 (0.7856) acc_u 40.6250 (28.1250) lr 1.9558e-03 eta 0:00:18
epoch [21/200] batch [20/56] time 0.502 (0.452) data 0.372 (0.322) loss_u loss_u 0.7871 (0.7801) acc_u 28.1250 (29.0625) lr 1.9558e-03 eta 0:00:16
epoch [21/200] batch [25/56] time 0.450 (0.449) data 0.320 (0.319) loss_u loss_u 0.8262 (0.7821) acc_u 31.2500 (29.2500) lr 1.9558e-03 eta 0:00:13
epoch [21/200] batch [30/56] time 0.480 (0.450) data 0.349 (0.319) loss_u loss_u 0.8525 (0.7857) acc_u 18.7500 (28.2292) lr 1.9558e-03 eta 0:00:11
epoch [21/200] batch [35/56] time 0.380 (0.448) data 0.248 (0.317) loss_u loss_u 0.8193 (0.7850) acc_u 21.8750 (27.6786) lr 1.9558e-03 eta 0:00:09
epoch [21/200] batch [40/56] time 0.521 (0.446) data 0.389 (0.316) loss_u loss_u 0.8311 (0.7915) acc_u 31.2500 (27.5000) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [45/56] time 0.417 (0.443) data 0.287 (0.313) loss_u loss_u 0.8467 (0.7952) acc_u 18.7500 (27.2917) lr 1.9558e-03 eta 0:00:04
epoch [21/200] batch [50/56] time 0.366 (0.441) data 0.234 (0.311) loss_u loss_u 0.8081 (0.7964) acc_u 25.0000 (27.1875) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [55/56] time 0.402 (0.438) data 0.271 (0.308) loss_u loss_u 0.7358 (0.7923) acc_u 34.3750 (27.7273) lr 1.9558e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1598
confident_label rate tensor(0.4397, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1379
clean true:1367
clean false:12
clean_rate:0.9912980420594634
noisy true:171
noisy false:1586
after delete: len(clean_dataset) 1379
after delete: len(noisy_dataset) 1757
epoch [22/200] batch [5/43] time 0.363 (0.426) data 0.233 (0.295) loss_x loss_x 0.9058 (1.1646) acc_x 71.8750 (70.0000) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [10/43] time 0.383 (0.447) data 0.252 (0.317) loss_x loss_x 1.2969 (1.2253) acc_x 59.3750 (68.4375) lr 1.9511e-03 eta 0:00:14
epoch [22/200] batch [15/43] time 0.519 (0.451) data 0.388 (0.321) loss_x loss_x 1.5186 (1.2823) acc_x 46.8750 (66.8750) lr 1.9511e-03 eta 0:00:12
epoch [22/200] batch [20/43] time 0.462 (0.469) data 0.331 (0.338) loss_x loss_x 0.9858 (1.2624) acc_x 71.8750 (67.1875) lr 1.9511e-03 eta 0:00:10
epoch [22/200] batch [25/43] time 0.433 (0.467) data 0.302 (0.336) loss_x loss_x 1.1855 (1.2606) acc_x 75.0000 (67.7500) lr 1.9511e-03 eta 0:00:08
epoch [22/200] batch [30/43] time 0.452 (0.464) data 0.321 (0.334) loss_x loss_x 1.8867 (1.3047) acc_x 56.2500 (66.8750) lr 1.9511e-03 eta 0:00:06
epoch [22/200] batch [35/43] time 0.407 (0.456) data 0.277 (0.326) loss_x loss_x 1.6592 (1.3171) acc_x 50.0000 (66.5179) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [40/43] time 0.490 (0.455) data 0.360 (0.324) loss_x loss_x 1.1748 (1.3358) acc_x 81.2500 (66.2500) lr 1.9511e-03 eta 0:00:01
epoch [22/200] batch [5/54] time 0.378 (0.450) data 0.247 (0.320) loss_u loss_u 0.7710 (0.7846) acc_u 28.1250 (30.0000) lr 1.9511e-03 eta 0:00:22
epoch [22/200] batch [10/54] time 0.439 (0.450) data 0.308 (0.320) loss_u loss_u 0.8379 (0.7885) acc_u 18.7500 (28.7500) lr 1.9511e-03 eta 0:00:19
epoch [22/200] batch [15/54] time 0.448 (0.453) data 0.317 (0.323) loss_u loss_u 0.7695 (0.7882) acc_u 28.1250 (28.3333) lr 1.9511e-03 eta 0:00:17
epoch [22/200] batch [20/54] time 0.325 (0.448) data 0.194 (0.317) loss_u loss_u 0.8813 (0.7897) acc_u 9.3750 (27.3438) lr 1.9511e-03 eta 0:00:15
epoch [22/200] batch [25/54] time 0.359 (0.446) data 0.229 (0.315) loss_u loss_u 0.8662 (0.7964) acc_u 18.7500 (26.7500) lr 1.9511e-03 eta 0:00:12
epoch [22/200] batch [30/54] time 0.515 (0.449) data 0.384 (0.318) loss_u loss_u 0.8447 (0.8023) acc_u 21.8750 (26.0417) lr 1.9511e-03 eta 0:00:10
epoch [22/200] batch [35/54] time 0.337 (0.445) data 0.207 (0.314) loss_u loss_u 0.8037 (0.8077) acc_u 31.2500 (24.9107) lr 1.9511e-03 eta 0:00:08
epoch [22/200] batch [40/54] time 0.416 (0.442) data 0.285 (0.311) loss_u loss_u 0.7695 (0.8049) acc_u 28.1250 (25.1562) lr 1.9511e-03 eta 0:00:06
epoch [22/200] batch [45/54] time 0.371 (0.439) data 0.240 (0.308) loss_u loss_u 0.7979 (0.8096) acc_u 28.1250 (24.3750) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [50/54] time 0.425 (0.438) data 0.294 (0.307) loss_u loss_u 0.7549 (0.8080) acc_u 31.2500 (24.5625) lr 1.9511e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1599
confident_label rate tensor(0.4372, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1371
clean true:1353
clean false:18
clean_rate:0.986870897155361
noisy true:184
noisy false:1581
after delete: len(clean_dataset) 1371
after delete: len(noisy_dataset) 1765
epoch [23/200] batch [5/42] time 0.424 (0.445) data 0.294 (0.315) loss_x loss_x 1.6523 (1.4865) acc_x 59.3750 (64.3750) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [10/42] time 0.386 (0.446) data 0.256 (0.316) loss_x loss_x 1.2334 (1.4184) acc_x 65.6250 (65.6250) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [15/42] time 0.369 (0.454) data 0.239 (0.324) loss_x loss_x 1.5059 (1.4534) acc_x 71.8750 (66.0417) lr 1.9461e-03 eta 0:00:12
epoch [23/200] batch [20/42] time 0.441 (0.456) data 0.311 (0.325) loss_x loss_x 1.7402 (1.4741) acc_x 59.3750 (64.6875) lr 1.9461e-03 eta 0:00:10
epoch [23/200] batch [25/42] time 0.353 (0.457) data 0.222 (0.327) loss_x loss_x 1.5811 (1.4756) acc_x 65.6250 (64.5000) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [30/42] time 0.454 (0.457) data 0.324 (0.327) loss_x loss_x 1.3799 (1.4306) acc_x 68.7500 (65.4167) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [35/42] time 0.435 (0.449) data 0.305 (0.318) loss_x loss_x 1.3633 (1.4359) acc_x 56.2500 (64.5536) lr 1.9461e-03 eta 0:00:03
epoch [23/200] batch [40/42] time 0.542 (0.448) data 0.412 (0.318) loss_x loss_x 1.2490 (1.4469) acc_x 65.6250 (64.0625) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/55] time 0.347 (0.447) data 0.216 (0.317) loss_u loss_u 0.8135 (0.8197) acc_u 21.8750 (21.2500) lr 1.9461e-03 eta 0:00:22
epoch [23/200] batch [10/55] time 0.356 (0.443) data 0.226 (0.312) loss_u loss_u 0.7568 (0.8093) acc_u 34.3750 (23.4375) lr 1.9461e-03 eta 0:00:19
epoch [23/200] batch [15/55] time 0.532 (0.439) data 0.401 (0.308) loss_u loss_u 0.7466 (0.8121) acc_u 31.2500 (22.2917) lr 1.9461e-03 eta 0:00:17
epoch [23/200] batch [20/55] time 0.564 (0.439) data 0.433 (0.309) loss_u loss_u 0.8037 (0.8078) acc_u 21.8750 (22.6562) lr 1.9461e-03 eta 0:00:15
epoch [23/200] batch [25/55] time 0.376 (0.440) data 0.245 (0.309) loss_u loss_u 0.8813 (0.8063) acc_u 15.6250 (23.7500) lr 1.9461e-03 eta 0:00:13
epoch [23/200] batch [30/55] time 0.373 (0.436) data 0.243 (0.305) loss_u loss_u 0.7554 (0.7996) acc_u 37.5000 (25.0000) lr 1.9461e-03 eta 0:00:10
epoch [23/200] batch [35/55] time 0.492 (0.434) data 0.361 (0.304) loss_u loss_u 0.6982 (0.8030) acc_u 34.3750 (24.1964) lr 1.9461e-03 eta 0:00:08
epoch [23/200] batch [40/55] time 0.352 (0.435) data 0.220 (0.304) loss_u loss_u 0.8252 (0.8014) acc_u 21.8750 (24.6875) lr 1.9461e-03 eta 0:00:06
epoch [23/200] batch [45/55] time 0.384 (0.433) data 0.253 (0.302) loss_u loss_u 0.8330 (0.8019) acc_u 12.5000 (24.4444) lr 1.9461e-03 eta 0:00:04
epoch [23/200] batch [50/55] time 0.662 (0.438) data 0.531 (0.307) loss_u loss_u 0.7646 (0.8001) acc_u 31.2500 (24.7500) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [55/55] time 0.343 (0.435) data 0.212 (0.304) loss_u loss_u 0.7578 (0.7990) acc_u 34.3750 (24.9432) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1620
confident_label rate tensor(0.4394, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1378
clean true:1368
clean false:10
clean_rate:0.9927431059506531
noisy true:148
noisy false:1610
after delete: len(clean_dataset) 1378
after delete: len(noisy_dataset) 1758
epoch [24/200] batch [5/43] time 0.494 (0.463) data 0.363 (0.333) loss_x loss_x 1.5791 (1.3021) acc_x 62.5000 (63.7500) lr 1.9409e-03 eta 0:00:17
epoch [24/200] batch [10/43] time 0.357 (0.458) data 0.227 (0.328) loss_x loss_x 1.3555 (1.2905) acc_x 65.6250 (65.3125) lr 1.9409e-03 eta 0:00:15
epoch [24/200] batch [15/43] time 0.370 (0.452) data 0.241 (0.322) loss_x loss_x 1.6816 (1.3493) acc_x 56.2500 (62.0833) lr 1.9409e-03 eta 0:00:12
epoch [24/200] batch [20/43] time 0.440 (0.449) data 0.310 (0.319) loss_x loss_x 1.8271 (1.3754) acc_x 62.5000 (62.0312) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [25/43] time 0.427 (0.449) data 0.298 (0.319) loss_x loss_x 1.9404 (1.4146) acc_x 37.5000 (60.8750) lr 1.9409e-03 eta 0:00:08
epoch [24/200] batch [30/43] time 0.491 (0.443) data 0.361 (0.314) loss_x loss_x 1.2979 (1.4399) acc_x 65.6250 (60.5208) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [35/43] time 0.415 (0.440) data 0.285 (0.310) loss_x loss_x 1.6230 (1.4369) acc_x 59.3750 (61.6964) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [40/43] time 0.451 (0.443) data 0.321 (0.313) loss_x loss_x 1.3516 (1.4374) acc_x 62.5000 (61.8750) lr 1.9409e-03 eta 0:00:01
epoch [24/200] batch [5/54] time 0.401 (0.439) data 0.271 (0.309) loss_u loss_u 0.7725 (0.7779) acc_u 25.0000 (30.6250) lr 1.9409e-03 eta 0:00:21
epoch [24/200] batch [10/54] time 0.401 (0.438) data 0.270 (0.308) loss_u loss_u 0.8320 (0.7825) acc_u 21.8750 (28.7500) lr 1.9409e-03 eta 0:00:19
epoch [24/200] batch [15/54] time 0.368 (0.434) data 0.238 (0.304) loss_u loss_u 0.7393 (0.7842) acc_u 31.2500 (28.3333) lr 1.9409e-03 eta 0:00:16
epoch [24/200] batch [20/54] time 0.341 (0.436) data 0.210 (0.306) loss_u loss_u 0.7671 (0.7803) acc_u 31.2500 (28.4375) lr 1.9409e-03 eta 0:00:14
epoch [24/200] batch [25/54] time 0.396 (0.437) data 0.265 (0.307) loss_u loss_u 0.7280 (0.7812) acc_u 31.2500 (27.8750) lr 1.9409e-03 eta 0:00:12
epoch [24/200] batch [30/54] time 0.555 (0.441) data 0.422 (0.310) loss_u loss_u 0.6738 (0.7826) acc_u 53.1250 (28.1250) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [35/54] time 0.298 (0.440) data 0.167 (0.310) loss_u loss_u 0.7759 (0.7866) acc_u 31.2500 (28.0357) lr 1.9409e-03 eta 0:00:08
epoch [24/200] batch [40/54] time 0.453 (0.441) data 0.322 (0.311) loss_u loss_u 0.7856 (0.7898) acc_u 31.2500 (27.5781) lr 1.9409e-03 eta 0:00:06
epoch [24/200] batch [45/54] time 0.383 (0.443) data 0.252 (0.313) loss_u loss_u 0.8018 (0.7899) acc_u 25.0000 (27.9167) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [50/54] time 0.408 (0.443) data 0.278 (0.313) loss_u loss_u 0.8003 (0.7931) acc_u 28.1250 (27.5625) lr 1.9409e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1604
confident_label rate tensor(0.4420, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1386
clean true:1369
clean false:17
clean_rate:0.9877344877344877
noisy true:163
noisy false:1587
after delete: len(clean_dataset) 1386
after delete: len(noisy_dataset) 1750
epoch [25/200] batch [5/43] time 0.455 (0.434) data 0.324 (0.304) loss_x loss_x 1.5938 (1.4451) acc_x 65.6250 (62.5000) lr 1.9354e-03 eta 0:00:16
epoch [25/200] batch [10/43] time 0.392 (0.430) data 0.261 (0.299) loss_x loss_x 1.5078 (1.4735) acc_x 62.5000 (60.0000) lr 1.9354e-03 eta 0:00:14
epoch [25/200] batch [15/43] time 0.418 (0.444) data 0.288 (0.314) loss_x loss_x 1.3135 (1.4226) acc_x 62.5000 (62.2917) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [20/43] time 0.445 (0.444) data 0.315 (0.313) loss_x loss_x 1.3506 (1.4193) acc_x 71.8750 (62.9688) lr 1.9354e-03 eta 0:00:10
epoch [25/200] batch [25/43] time 0.446 (0.446) data 0.316 (0.316) loss_x loss_x 1.6055 (1.4402) acc_x 59.3750 (62.6250) lr 1.9354e-03 eta 0:00:08
epoch [25/200] batch [30/43] time 0.559 (0.450) data 0.428 (0.320) loss_x loss_x 1.4658 (1.4529) acc_x 62.5000 (62.7083) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [35/43] time 0.475 (0.448) data 0.345 (0.318) loss_x loss_x 2.0195 (1.4675) acc_x 46.8750 (62.5000) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [40/43] time 0.430 (0.452) data 0.299 (0.321) loss_x loss_x 1.2148 (1.4680) acc_x 68.7500 (62.8125) lr 1.9354e-03 eta 0:00:01
epoch [25/200] batch [5/54] time 0.432 (0.449) data 0.301 (0.319) loss_u loss_u 0.7212 (0.7805) acc_u 37.5000 (27.5000) lr 1.9354e-03 eta 0:00:21
epoch [25/200] batch [10/54] time 0.667 (0.448) data 0.535 (0.318) loss_u loss_u 0.8345 (0.7920) acc_u 25.0000 (25.6250) lr 1.9354e-03 eta 0:00:19
epoch [25/200] batch [15/54] time 0.384 (0.450) data 0.253 (0.320) loss_u loss_u 0.8154 (0.7917) acc_u 18.7500 (26.2500) lr 1.9354e-03 eta 0:00:17
epoch [25/200] batch [20/54] time 0.332 (0.444) data 0.201 (0.313) loss_u loss_u 0.8008 (0.7956) acc_u 28.1250 (26.2500) lr 1.9354e-03 eta 0:00:15
epoch [25/200] batch [25/54] time 0.542 (0.442) data 0.411 (0.312) loss_u loss_u 0.7764 (0.7960) acc_u 31.2500 (26.1250) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [30/54] time 0.584 (0.444) data 0.453 (0.314) loss_u loss_u 0.8252 (0.7964) acc_u 25.0000 (26.3542) lr 1.9354e-03 eta 0:00:10
epoch [25/200] batch [35/54] time 0.444 (0.441) data 0.313 (0.311) loss_u loss_u 0.7622 (0.7947) acc_u 34.3750 (26.3393) lr 1.9354e-03 eta 0:00:08
epoch [25/200] batch [40/54] time 0.551 (0.442) data 0.420 (0.312) loss_u loss_u 0.8296 (0.7960) acc_u 18.7500 (26.1719) lr 1.9354e-03 eta 0:00:06
epoch [25/200] batch [45/54] time 0.484 (0.441) data 0.354 (0.311) loss_u loss_u 0.8096 (0.7953) acc_u 21.8750 (26.1806) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [50/54] time 0.494 (0.439) data 0.363 (0.308) loss_u loss_u 0.7764 (0.7955) acc_u 28.1250 (26.1250) lr 1.9354e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1602
confident_label rate tensor(0.4391, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1377
clean true:1361
clean false:16
clean_rate:0.9883805374001452
noisy true:173
noisy false:1586
after delete: len(clean_dataset) 1377
after delete: len(noisy_dataset) 1759
epoch [26/200] batch [5/43] time 0.393 (0.411) data 0.262 (0.281) loss_x loss_x 1.5098 (1.3845) acc_x 59.3750 (64.3750) lr 1.9298e-03 eta 0:00:15
epoch [26/200] batch [10/43] time 0.441 (0.434) data 0.310 (0.303) loss_x loss_x 1.0205 (1.3452) acc_x 71.8750 (65.0000) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [15/43] time 0.418 (0.453) data 0.287 (0.323) loss_x loss_x 1.2246 (1.2792) acc_x 68.7500 (67.0833) lr 1.9298e-03 eta 0:00:12
epoch [26/200] batch [20/43] time 0.525 (0.454) data 0.394 (0.323) loss_x loss_x 1.7803 (1.2792) acc_x 53.1250 (67.3438) lr 1.9298e-03 eta 0:00:10
epoch [26/200] batch [25/43] time 0.607 (0.462) data 0.477 (0.331) loss_x loss_x 1.4141 (1.3061) acc_x 65.6250 (66.6250) lr 1.9298e-03 eta 0:00:08
epoch [26/200] batch [30/43] time 0.514 (0.464) data 0.383 (0.333) loss_x loss_x 1.4385 (1.3146) acc_x 62.5000 (66.6667) lr 1.9298e-03 eta 0:00:06
epoch [26/200] batch [35/43] time 0.479 (0.463) data 0.349 (0.332) loss_x loss_x 1.3438 (1.3218) acc_x 59.3750 (66.5179) lr 1.9298e-03 eta 0:00:03
epoch [26/200] batch [40/43] time 0.347 (0.459) data 0.217 (0.328) loss_x loss_x 1.5654 (1.3336) acc_x 68.7500 (66.4062) lr 1.9298e-03 eta 0:00:01
epoch [26/200] batch [5/54] time 0.346 (0.454) data 0.215 (0.323) loss_u loss_u 0.8604 (0.8095) acc_u 21.8750 (27.5000) lr 1.9298e-03 eta 0:00:22
epoch [26/200] batch [10/54] time 0.371 (0.452) data 0.240 (0.321) loss_u loss_u 0.7769 (0.8020) acc_u 25.0000 (26.2500) lr 1.9298e-03 eta 0:00:19
epoch [26/200] batch [15/54] time 0.436 (0.449) data 0.304 (0.318) loss_u loss_u 0.7773 (0.8103) acc_u 31.2500 (25.6250) lr 1.9298e-03 eta 0:00:17
epoch [26/200] batch [20/54] time 0.440 (0.450) data 0.309 (0.319) loss_u loss_u 0.6250 (0.7980) acc_u 56.2500 (27.6562) lr 1.9298e-03 eta 0:00:15
epoch [26/200] batch [25/54] time 0.367 (0.445) data 0.237 (0.314) loss_u loss_u 0.8125 (0.7966) acc_u 25.0000 (28.1250) lr 1.9298e-03 eta 0:00:12
epoch [26/200] batch [30/54] time 0.369 (0.441) data 0.237 (0.311) loss_u loss_u 0.7119 (0.7958) acc_u 34.3750 (28.1250) lr 1.9298e-03 eta 0:00:10
epoch [26/200] batch [35/54] time 0.414 (0.441) data 0.283 (0.310) loss_u loss_u 0.7612 (0.7918) acc_u 21.8750 (28.3036) lr 1.9298e-03 eta 0:00:08
epoch [26/200] batch [40/54] time 0.503 (0.442) data 0.372 (0.311) loss_u loss_u 0.7876 (0.7874) acc_u 28.1250 (28.9062) lr 1.9298e-03 eta 0:00:06
epoch [26/200] batch [45/54] time 0.378 (0.442) data 0.247 (0.312) loss_u loss_u 0.8462 (0.7858) acc_u 12.5000 (28.7500) lr 1.9298e-03 eta 0:00:03
epoch [26/200] batch [50/54] time 0.558 (0.444) data 0.427 (0.313) loss_u loss_u 0.7461 (0.7828) acc_u 34.3750 (28.9375) lr 1.9298e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1595
confident_label rate tensor(0.4413, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1384
clean true:1368
clean false:16
clean_rate:0.9884393063583815
noisy true:173
noisy false:1579
after delete: len(clean_dataset) 1384
after delete: len(noisy_dataset) 1752
epoch [27/200] batch [5/43] time 0.400 (0.447) data 0.270 (0.317) loss_x loss_x 1.5557 (1.2934) acc_x 53.1250 (66.2500) lr 1.9239e-03 eta 0:00:16
epoch [27/200] batch [10/43] time 0.481 (0.467) data 0.351 (0.337) loss_x loss_x 1.0947 (1.2118) acc_x 75.0000 (67.8125) lr 1.9239e-03 eta 0:00:15
epoch [27/200] batch [15/43] time 0.488 (0.452) data 0.358 (0.322) loss_x loss_x 1.2793 (1.2364) acc_x 62.5000 (66.0417) lr 1.9239e-03 eta 0:00:12
epoch [27/200] batch [20/43] time 0.477 (0.449) data 0.348 (0.318) loss_x loss_x 1.2373 (1.2611) acc_x 75.0000 (67.3438) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [25/43] time 0.375 (0.447) data 0.245 (0.317) loss_x loss_x 1.3760 (1.3290) acc_x 62.5000 (65.3750) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [30/43] time 0.475 (0.441) data 0.345 (0.311) loss_x loss_x 0.9966 (1.3177) acc_x 75.0000 (65.0000) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [35/43] time 0.415 (0.448) data 0.285 (0.318) loss_x loss_x 1.3066 (1.3260) acc_x 65.6250 (65.3571) lr 1.9239e-03 eta 0:00:03
epoch [27/200] batch [40/43] time 0.418 (0.448) data 0.289 (0.318) loss_x loss_x 0.8682 (1.3005) acc_x 84.3750 (65.9375) lr 1.9239e-03 eta 0:00:01
epoch [27/200] batch [5/54] time 0.359 (0.448) data 0.229 (0.318) loss_u loss_u 0.7959 (0.7985) acc_u 21.8750 (28.1250) lr 1.9239e-03 eta 0:00:21
epoch [27/200] batch [10/54] time 0.328 (0.444) data 0.197 (0.314) loss_u loss_u 0.7832 (0.8003) acc_u 25.0000 (27.5000) lr 1.9239e-03 eta 0:00:19
epoch [27/200] batch [15/54] time 0.411 (0.448) data 0.281 (0.318) loss_u loss_u 0.7939 (0.7984) acc_u 28.1250 (27.2917) lr 1.9239e-03 eta 0:00:17
epoch [27/200] batch [20/54] time 0.455 (0.445) data 0.324 (0.315) loss_u loss_u 0.7500 (0.7960) acc_u 28.1250 (27.1875) lr 1.9239e-03 eta 0:00:15
epoch [27/200] batch [25/54] time 0.451 (0.448) data 0.320 (0.318) loss_u loss_u 0.7910 (0.8008) acc_u 25.0000 (26.7500) lr 1.9239e-03 eta 0:00:12
epoch [27/200] batch [30/54] time 0.346 (0.449) data 0.215 (0.318) loss_u loss_u 0.7686 (0.8006) acc_u 34.3750 (27.5000) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [35/54] time 0.355 (0.445) data 0.224 (0.314) loss_u loss_u 0.7793 (0.7951) acc_u 21.8750 (27.7679) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [40/54] time 0.565 (0.444) data 0.433 (0.313) loss_u loss_u 0.7197 (0.7909) acc_u 34.3750 (28.4375) lr 1.9239e-03 eta 0:00:06
epoch [27/200] batch [45/54] time 0.503 (0.441) data 0.370 (0.311) loss_u loss_u 0.8184 (0.7918) acc_u 15.6250 (28.0556) lr 1.9239e-03 eta 0:00:03
epoch [27/200] batch [50/54] time 0.384 (0.440) data 0.253 (0.310) loss_u loss_u 0.6890 (0.7870) acc_u 46.8750 (28.6250) lr 1.9239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1553
confident_label rate tensor(0.4522, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1418
clean true:1406
clean false:12
clean_rate:0.9915373765867419
noisy true:177
noisy false:1541
after delete: len(clean_dataset) 1418
after delete: len(noisy_dataset) 1718
epoch [28/200] batch [5/44] time 0.481 (0.432) data 0.351 (0.303) loss_x loss_x 1.0098 (1.4209) acc_x 81.2500 (65.6250) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [10/44] time 0.361 (0.434) data 0.232 (0.305) loss_x loss_x 0.9263 (1.3393) acc_x 68.7500 (65.9375) lr 1.9178e-03 eta 0:00:14
epoch [28/200] batch [15/44] time 0.542 (0.442) data 0.412 (0.312) loss_x loss_x 1.7324 (1.3325) acc_x 59.3750 (67.7083) lr 1.9178e-03 eta 0:00:12
epoch [28/200] batch [20/44] time 0.368 (0.443) data 0.238 (0.313) loss_x loss_x 1.3223 (1.3531) acc_x 68.7500 (65.7812) lr 1.9178e-03 eta 0:00:10
epoch [28/200] batch [25/44] time 0.511 (0.447) data 0.381 (0.317) loss_x loss_x 1.8018 (1.3669) acc_x 59.3750 (66.2500) lr 1.9178e-03 eta 0:00:08
epoch [28/200] batch [30/44] time 0.412 (0.447) data 0.282 (0.317) loss_x loss_x 0.9844 (1.3539) acc_x 65.6250 (66.0417) lr 1.9178e-03 eta 0:00:06
epoch [28/200] batch [35/44] time 0.506 (0.450) data 0.376 (0.320) loss_x loss_x 1.4131 (1.3384) acc_x 56.2500 (66.3393) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [40/44] time 0.397 (0.454) data 0.267 (0.324) loss_x loss_x 1.7051 (1.3580) acc_x 50.0000 (65.7031) lr 1.9178e-03 eta 0:00:01
epoch [28/200] batch [5/53] time 0.407 (0.457) data 0.277 (0.327) loss_u loss_u 0.8340 (0.7981) acc_u 25.0000 (24.3750) lr 1.9178e-03 eta 0:00:21
epoch [28/200] batch [10/53] time 0.372 (0.456) data 0.242 (0.326) loss_u loss_u 0.7656 (0.7937) acc_u 37.5000 (27.1875) lr 1.9178e-03 eta 0:00:19
epoch [28/200] batch [15/53] time 0.410 (0.452) data 0.279 (0.322) loss_u loss_u 0.7344 (0.7915) acc_u 40.6250 (27.7083) lr 1.9178e-03 eta 0:00:17
epoch [28/200] batch [20/53] time 0.390 (0.450) data 0.259 (0.320) loss_u loss_u 0.7407 (0.7790) acc_u 34.3750 (28.9062) lr 1.9178e-03 eta 0:00:14
epoch [28/200] batch [25/53] time 0.478 (0.448) data 0.347 (0.318) loss_u loss_u 0.8066 (0.7825) acc_u 28.1250 (28.3750) lr 1.9178e-03 eta 0:00:12
epoch [28/200] batch [30/53] time 0.332 (0.442) data 0.201 (0.312) loss_u loss_u 0.8760 (0.7844) acc_u 21.8750 (28.1250) lr 1.9178e-03 eta 0:00:10
epoch [28/200] batch [35/53] time 0.378 (0.443) data 0.247 (0.312) loss_u loss_u 0.7666 (0.7868) acc_u 21.8750 (27.7679) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [40/53] time 0.320 (0.437) data 0.188 (0.306) loss_u loss_u 0.7949 (0.7892) acc_u 28.1250 (27.5000) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [45/53] time 0.367 (0.439) data 0.236 (0.308) loss_u loss_u 0.7710 (0.7900) acc_u 28.1250 (27.2917) lr 1.9178e-03 eta 0:00:03
epoch [28/200] batch [50/53] time 0.406 (0.438) data 0.275 (0.307) loss_u loss_u 0.7378 (0.7886) acc_u 31.2500 (27.4375) lr 1.9178e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1555
confident_label rate tensor(0.4566, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1432
clean true:1418
clean false:14
clean_rate:0.9902234636871509
noisy true:163
noisy false:1541
after delete: len(clean_dataset) 1432
after delete: len(noisy_dataset) 1704
epoch [29/200] batch [5/44] time 0.455 (0.492) data 0.325 (0.361) loss_x loss_x 1.0195 (1.1363) acc_x 75.0000 (65.6250) lr 1.9114e-03 eta 0:00:19
epoch [29/200] batch [10/44] time 0.497 (0.514) data 0.368 (0.384) loss_x loss_x 1.3018 (1.3020) acc_x 65.6250 (63.4375) lr 1.9114e-03 eta 0:00:17
epoch [29/200] batch [15/44] time 0.408 (0.484) data 0.278 (0.354) loss_x loss_x 1.3955 (1.2739) acc_x 59.3750 (65.0000) lr 1.9114e-03 eta 0:00:14
epoch [29/200] batch [20/44] time 0.414 (0.479) data 0.284 (0.348) loss_x loss_x 1.2822 (1.2662) acc_x 65.6250 (65.6250) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [25/44] time 0.401 (0.475) data 0.271 (0.344) loss_x loss_x 1.3838 (1.2526) acc_x 68.7500 (66.0000) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [30/44] time 0.382 (0.471) data 0.251 (0.341) loss_x loss_x 1.2695 (1.2743) acc_x 71.8750 (66.2500) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [35/44] time 0.397 (0.461) data 0.267 (0.331) loss_x loss_x 1.6787 (1.2934) acc_x 62.5000 (66.0714) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [40/44] time 0.556 (0.460) data 0.426 (0.330) loss_x loss_x 1.3184 (1.3096) acc_x 65.6250 (65.7031) lr 1.9114e-03 eta 0:00:01
epoch [29/200] batch [5/53] time 0.437 (0.455) data 0.306 (0.325) loss_u loss_u 0.7939 (0.8064) acc_u 28.1250 (27.5000) lr 1.9114e-03 eta 0:00:21
epoch [29/200] batch [10/53] time 0.385 (0.453) data 0.254 (0.323) loss_u loss_u 0.8022 (0.7819) acc_u 25.0000 (30.3125) lr 1.9114e-03 eta 0:00:19
epoch [29/200] batch [15/53] time 0.487 (0.453) data 0.356 (0.323) loss_u loss_u 0.8008 (0.7882) acc_u 28.1250 (28.3333) lr 1.9114e-03 eta 0:00:17
epoch [29/200] batch [20/53] time 0.394 (0.450) data 0.263 (0.320) loss_u loss_u 0.8242 (0.7883) acc_u 25.0000 (28.4375) lr 1.9114e-03 eta 0:00:14
epoch [29/200] batch [25/53] time 0.438 (0.448) data 0.307 (0.318) loss_u loss_u 0.7881 (0.7908) acc_u 21.8750 (27.3750) lr 1.9114e-03 eta 0:00:12
epoch [29/200] batch [30/53] time 0.345 (0.445) data 0.214 (0.314) loss_u loss_u 0.7451 (0.7921) acc_u 31.2500 (27.1875) lr 1.9114e-03 eta 0:00:10
epoch [29/200] batch [35/53] time 0.357 (0.442) data 0.226 (0.312) loss_u loss_u 0.8013 (0.7951) acc_u 28.1250 (27.2321) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [40/53] time 0.363 (0.439) data 0.232 (0.309) loss_u loss_u 0.8237 (0.7974) acc_u 25.0000 (26.8750) lr 1.9114e-03 eta 0:00:05
epoch [29/200] batch [45/53] time 0.335 (0.439) data 0.204 (0.308) loss_u loss_u 0.7627 (0.8007) acc_u 31.2500 (26.4583) lr 1.9114e-03 eta 0:00:03
epoch [29/200] batch [50/53] time 0.499 (0.441) data 0.368 (0.310) loss_u loss_u 0.8184 (0.8001) acc_u 18.7500 (26.5625) lr 1.9114e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1567
confident_label rate tensor(0.4506, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1413
clean true:1399
clean false:14
clean_rate:0.9900920028308563
noisy true:170
noisy false:1553
after delete: len(clean_dataset) 1413
after delete: len(noisy_dataset) 1723
epoch [30/200] batch [5/44] time 0.451 (0.468) data 0.321 (0.338) loss_x loss_x 1.6826 (1.5480) acc_x 59.3750 (63.7500) lr 1.9048e-03 eta 0:00:18
epoch [30/200] batch [10/44] time 0.423 (0.459) data 0.293 (0.329) loss_x loss_x 1.2881 (1.4199) acc_x 68.7500 (68.1250) lr 1.9048e-03 eta 0:00:15
epoch [30/200] batch [15/44] time 0.483 (0.458) data 0.353 (0.328) loss_x loss_x 1.1660 (1.4498) acc_x 71.8750 (67.5000) lr 1.9048e-03 eta 0:00:13
epoch [30/200] batch [20/44] time 0.443 (0.452) data 0.313 (0.321) loss_x loss_x 1.4453 (1.4190) acc_x 65.6250 (67.1875) lr 1.9048e-03 eta 0:00:10
epoch [30/200] batch [25/44] time 0.456 (0.466) data 0.327 (0.335) loss_x loss_x 1.6250 (1.4840) acc_x 59.3750 (65.1250) lr 1.9048e-03 eta 0:00:08
epoch [30/200] batch [30/44] time 0.484 (0.468) data 0.354 (0.338) loss_x loss_x 1.1748 (1.4289) acc_x 68.7500 (65.6250) lr 1.9048e-03 eta 0:00:06
epoch [30/200] batch [35/44] time 0.460 (0.466) data 0.331 (0.336) loss_x loss_x 0.8477 (1.4017) acc_x 78.1250 (66.1607) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [40/44] time 0.416 (0.467) data 0.285 (0.337) loss_x loss_x 0.8105 (1.3842) acc_x 84.3750 (66.3281) lr 1.9048e-03 eta 0:00:01
epoch [30/200] batch [5/53] time 0.420 (0.459) data 0.289 (0.328) loss_u loss_u 0.8242 (0.8040) acc_u 25.0000 (28.1250) lr 1.9048e-03 eta 0:00:22
epoch [30/200] batch [10/53] time 0.483 (0.458) data 0.354 (0.328) loss_u loss_u 0.7593 (0.8089) acc_u 37.5000 (25.9375) lr 1.9048e-03 eta 0:00:19
epoch [30/200] batch [15/53] time 0.433 (0.456) data 0.302 (0.325) loss_u loss_u 0.8325 (0.8017) acc_u 15.6250 (26.0417) lr 1.9048e-03 eta 0:00:17
epoch [30/200] batch [20/53] time 0.421 (0.451) data 0.291 (0.321) loss_u loss_u 0.8862 (0.8046) acc_u 15.6250 (26.0938) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [25/53] time 0.449 (0.447) data 0.319 (0.317) loss_u loss_u 0.8379 (0.8019) acc_u 21.8750 (26.1250) lr 1.9048e-03 eta 0:00:12
epoch [30/200] batch [30/53] time 0.345 (0.448) data 0.216 (0.317) loss_u loss_u 0.7983 (0.7953) acc_u 18.7500 (26.8750) lr 1.9048e-03 eta 0:00:10
epoch [30/200] batch [35/53] time 0.336 (0.446) data 0.206 (0.315) loss_u loss_u 0.6997 (0.7934) acc_u 40.6250 (27.4107) lr 1.9048e-03 eta 0:00:08
epoch [30/200] batch [40/53] time 0.416 (0.445) data 0.286 (0.314) loss_u loss_u 0.7949 (0.7940) acc_u 25.0000 (26.8750) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [45/53] time 0.393 (0.441) data 0.262 (0.311) loss_u loss_u 0.8677 (0.7991) acc_u 18.7500 (26.6667) lr 1.9048e-03 eta 0:00:03
epoch [30/200] batch [50/53] time 0.360 (0.439) data 0.230 (0.308) loss_u loss_u 0.7739 (0.7943) acc_u 40.6250 (27.4375) lr 1.9048e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1541
confident_label rate tensor(0.4544, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1425
clean true:1412
clean false:13
clean_rate:0.9908771929824561
noisy true:183
noisy false:1528
after delete: len(clean_dataset) 1425
after delete: len(noisy_dataset) 1711
epoch [31/200] batch [5/44] time 0.456 (0.495) data 0.326 (0.366) loss_x loss_x 1.0293 (1.0752) acc_x 71.8750 (70.6250) lr 1.8980e-03 eta 0:00:19
epoch [31/200] batch [10/44] time 0.552 (0.477) data 0.423 (0.347) loss_x loss_x 1.1250 (1.2689) acc_x 71.8750 (68.7500) lr 1.8980e-03 eta 0:00:16
epoch [31/200] batch [15/44] time 0.411 (0.483) data 0.281 (0.353) loss_x loss_x 1.3965 (1.2714) acc_x 62.5000 (68.1250) lr 1.8980e-03 eta 0:00:14
epoch [31/200] batch [20/44] time 0.431 (0.467) data 0.300 (0.337) loss_x loss_x 1.4697 (1.2524) acc_x 65.6250 (68.2812) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [25/44] time 0.350 (0.458) data 0.219 (0.328) loss_x loss_x 1.5566 (1.3054) acc_x 75.0000 (67.0000) lr 1.8980e-03 eta 0:00:08
epoch [31/200] batch [30/44] time 0.354 (0.458) data 0.224 (0.327) loss_x loss_x 1.4727 (1.2807) acc_x 65.6250 (67.2917) lr 1.8980e-03 eta 0:00:06
epoch [31/200] batch [35/44] time 0.502 (0.462) data 0.373 (0.332) loss_x loss_x 1.3008 (1.2958) acc_x 65.6250 (67.0536) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [40/44] time 0.377 (0.463) data 0.246 (0.333) loss_x loss_x 2.0078 (1.3320) acc_x 37.5000 (65.9375) lr 1.8980e-03 eta 0:00:01
epoch [31/200] batch [5/53] time 0.432 (0.463) data 0.301 (0.332) loss_u loss_u 0.7891 (0.7985) acc_u 31.2500 (25.6250) lr 1.8980e-03 eta 0:00:22
epoch [31/200] batch [10/53] time 0.430 (0.465) data 0.300 (0.335) loss_u loss_u 0.7764 (0.7911) acc_u 28.1250 (27.1875) lr 1.8980e-03 eta 0:00:19
epoch [31/200] batch [15/53] time 0.413 (0.458) data 0.283 (0.328) loss_u loss_u 0.8506 (0.7965) acc_u 21.8750 (27.2917) lr 1.8980e-03 eta 0:00:17
epoch [31/200] batch [20/53] time 0.327 (0.453) data 0.196 (0.323) loss_u loss_u 0.7529 (0.7932) acc_u 31.2500 (27.6562) lr 1.8980e-03 eta 0:00:14
epoch [31/200] batch [25/53] time 0.562 (0.453) data 0.432 (0.323) loss_u loss_u 0.7720 (0.7949) acc_u 25.0000 (26.8750) lr 1.8980e-03 eta 0:00:12
epoch [31/200] batch [30/53] time 0.552 (0.454) data 0.421 (0.323) loss_u loss_u 0.7798 (0.7927) acc_u 28.1250 (26.7708) lr 1.8980e-03 eta 0:00:10
epoch [31/200] batch [35/53] time 0.346 (0.452) data 0.216 (0.321) loss_u loss_u 0.8198 (0.7963) acc_u 28.1250 (26.2500) lr 1.8980e-03 eta 0:00:08
epoch [31/200] batch [40/53] time 0.400 (0.450) data 0.269 (0.320) loss_u loss_u 0.7881 (0.7898) acc_u 25.0000 (27.1875) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [45/53] time 0.359 (0.449) data 0.227 (0.319) loss_u loss_u 0.8374 (0.7899) acc_u 25.0000 (27.2222) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [50/53] time 0.371 (0.449) data 0.241 (0.318) loss_u loss_u 0.8184 (0.7914) acc_u 28.1250 (27.1250) lr 1.8980e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1540
confident_label rate tensor(0.4598, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1442
clean true:1424
clean false:18
clean_rate:0.9875173370319001
noisy true:172
noisy false:1522
after delete: len(clean_dataset) 1442
after delete: len(noisy_dataset) 1694
epoch [32/200] batch [5/45] time 0.411 (0.422) data 0.281 (0.292) loss_x loss_x 1.0947 (1.3799) acc_x 68.7500 (65.6250) lr 1.8910e-03 eta 0:00:16
epoch [32/200] batch [10/45] time 0.445 (0.427) data 0.315 (0.297) loss_x loss_x 1.1445 (1.4314) acc_x 68.7500 (63.7500) lr 1.8910e-03 eta 0:00:14
epoch [32/200] batch [15/45] time 0.378 (0.429) data 0.249 (0.300) loss_x loss_x 1.0771 (1.3580) acc_x 68.7500 (66.0417) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [20/45] time 0.442 (0.444) data 0.312 (0.314) loss_x loss_x 1.5215 (1.3801) acc_x 68.7500 (65.3125) lr 1.8910e-03 eta 0:00:11
epoch [32/200] batch [25/45] time 0.596 (0.467) data 0.466 (0.337) loss_x loss_x 1.1777 (1.3357) acc_x 75.0000 (66.3750) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [30/45] time 0.476 (0.467) data 0.345 (0.337) loss_x loss_x 0.9165 (1.3202) acc_x 78.1250 (66.9792) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [35/45] time 0.496 (0.462) data 0.366 (0.332) loss_x loss_x 1.3867 (1.3127) acc_x 62.5000 (67.2321) lr 1.8910e-03 eta 0:00:04
epoch [32/200] batch [40/45] time 0.481 (0.462) data 0.349 (0.332) loss_x loss_x 1.6494 (1.3234) acc_x 65.6250 (67.0312) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [45/45] time 0.369 (0.461) data 0.239 (0.331) loss_x loss_x 1.3262 (1.3207) acc_x 65.6250 (67.5694) lr 1.8910e-03 eta 0:00:00
epoch [32/200] batch [5/52] time 0.419 (0.456) data 0.289 (0.326) loss_u loss_u 0.8521 (0.7851) acc_u 21.8750 (28.1250) lr 1.8910e-03 eta 0:00:21
epoch [32/200] batch [10/52] time 0.369 (0.449) data 0.240 (0.319) loss_u loss_u 0.8320 (0.7757) acc_u 15.6250 (27.1875) lr 1.8910e-03 eta 0:00:18
epoch [32/200] batch [15/52] time 0.363 (0.444) data 0.232 (0.314) loss_u loss_u 0.8403 (0.7966) acc_u 15.6250 (25.6250) lr 1.8910e-03 eta 0:00:16
epoch [32/200] batch [20/52] time 0.509 (0.447) data 0.378 (0.317) loss_u loss_u 0.7598 (0.7971) acc_u 34.3750 (26.0938) lr 1.8910e-03 eta 0:00:14
epoch [32/200] batch [25/52] time 0.420 (0.446) data 0.291 (0.315) loss_u loss_u 0.8662 (0.7939) acc_u 12.5000 (26.3750) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [30/52] time 0.648 (0.448) data 0.518 (0.318) loss_u loss_u 0.8013 (0.7940) acc_u 28.1250 (26.5625) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [35/52] time 0.617 (0.449) data 0.487 (0.319) loss_u loss_u 0.8188 (0.7990) acc_u 25.0000 (26.0714) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [40/52] time 0.334 (0.447) data 0.203 (0.317) loss_u loss_u 0.7690 (0.7967) acc_u 34.3750 (26.3281) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [45/52] time 0.599 (0.447) data 0.468 (0.317) loss_u loss_u 0.8242 (0.7974) acc_u 28.1250 (26.3889) lr 1.8910e-03 eta 0:00:03
epoch [32/200] batch [50/52] time 0.423 (0.443) data 0.294 (0.313) loss_u loss_u 0.8467 (0.7981) acc_u 21.8750 (26.5625) lr 1.8910e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1569
confident_label rate tensor(0.4499, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1411
clean true:1396
clean false:15
clean_rate:0.9893692416725727
noisy true:171
noisy false:1554
after delete: len(clean_dataset) 1411
after delete: len(noisy_dataset) 1725
epoch [33/200] batch [5/44] time 0.596 (0.549) data 0.466 (0.419) loss_x loss_x 1.2227 (1.3461) acc_x 62.5000 (64.3750) lr 1.8838e-03 eta 0:00:21
epoch [33/200] batch [10/44] time 0.475 (0.478) data 0.345 (0.348) loss_x loss_x 1.0615 (1.3255) acc_x 75.0000 (64.6875) lr 1.8838e-03 eta 0:00:16
epoch [33/200] batch [15/44] time 0.415 (0.467) data 0.283 (0.337) loss_x loss_x 1.0420 (1.3246) acc_x 65.6250 (64.7917) lr 1.8838e-03 eta 0:00:13
epoch [33/200] batch [20/44] time 0.366 (0.464) data 0.236 (0.333) loss_x loss_x 1.1182 (1.3325) acc_x 65.6250 (63.4375) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [25/44] time 0.460 (0.473) data 0.331 (0.343) loss_x loss_x 1.3750 (1.3143) acc_x 59.3750 (64.5000) lr 1.8838e-03 eta 0:00:08
epoch [33/200] batch [30/44] time 0.651 (0.477) data 0.520 (0.347) loss_x loss_x 1.3740 (1.3462) acc_x 65.6250 (64.7917) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [35/44] time 0.426 (0.472) data 0.296 (0.341) loss_x loss_x 1.6631 (1.3718) acc_x 56.2500 (64.1071) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [40/44] time 0.367 (0.462) data 0.237 (0.332) loss_x loss_x 1.3750 (1.3674) acc_x 53.1250 (64.3750) lr 1.8838e-03 eta 0:00:01
epoch [33/200] batch [5/53] time 0.452 (0.456) data 0.322 (0.326) loss_u loss_u 0.7554 (0.8128) acc_u 28.1250 (23.1250) lr 1.8838e-03 eta 0:00:21
epoch [33/200] batch [10/53] time 0.384 (0.453) data 0.254 (0.323) loss_u loss_u 0.7817 (0.8060) acc_u 28.1250 (23.7500) lr 1.8838e-03 eta 0:00:19
epoch [33/200] batch [15/53] time 0.447 (0.451) data 0.316 (0.321) loss_u loss_u 0.8276 (0.7985) acc_u 25.0000 (26.4583) lr 1.8838e-03 eta 0:00:17
epoch [33/200] batch [20/53] time 0.442 (0.446) data 0.312 (0.316) loss_u loss_u 0.7539 (0.7978) acc_u 34.3750 (25.7812) lr 1.8838e-03 eta 0:00:14
epoch [33/200] batch [25/53] time 0.389 (0.446) data 0.257 (0.316) loss_u loss_u 0.7231 (0.7956) acc_u 34.3750 (25.8750) lr 1.8838e-03 eta 0:00:12
epoch [33/200] batch [30/53] time 0.392 (0.444) data 0.261 (0.314) loss_u loss_u 0.7153 (0.7883) acc_u 37.5000 (27.2917) lr 1.8838e-03 eta 0:00:10
epoch [33/200] batch [35/53] time 0.402 (0.444) data 0.272 (0.314) loss_u loss_u 0.7324 (0.7841) acc_u 40.6250 (28.1250) lr 1.8838e-03 eta 0:00:07
epoch [33/200] batch [40/53] time 0.427 (0.443) data 0.297 (0.312) loss_u loss_u 0.8027 (0.7838) acc_u 28.1250 (28.5938) lr 1.8838e-03 eta 0:00:05
epoch [33/200] batch [45/53] time 0.443 (0.440) data 0.311 (0.310) loss_u loss_u 0.7368 (0.7839) acc_u 34.3750 (28.6806) lr 1.8838e-03 eta 0:00:03
epoch [33/200] batch [50/53] time 0.368 (0.439) data 0.237 (0.309) loss_u loss_u 0.7642 (0.7828) acc_u 25.0000 (28.6875) lr 1.8838e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1519
confident_label rate tensor(0.4662, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1462
clean true:1445
clean false:17
clean_rate:0.9883720930232558
noisy true:172
noisy false:1502
after delete: len(clean_dataset) 1462
after delete: len(noisy_dataset) 1674
epoch [34/200] batch [5/45] time 0.331 (0.448) data 0.200 (0.318) loss_x loss_x 1.1729 (1.2196) acc_x 78.1250 (70.0000) lr 1.8763e-03 eta 0:00:17
epoch [34/200] batch [10/45] time 0.469 (0.475) data 0.338 (0.344) loss_x loss_x 0.9385 (1.1892) acc_x 81.2500 (69.6875) lr 1.8763e-03 eta 0:00:16
epoch [34/200] batch [15/45] time 0.566 (0.471) data 0.436 (0.340) loss_x loss_x 1.4658 (1.2393) acc_x 65.6250 (68.5417) lr 1.8763e-03 eta 0:00:14
epoch [34/200] batch [20/45] time 0.391 (0.462) data 0.261 (0.331) loss_x loss_x 1.4561 (1.2490) acc_x 53.1250 (68.2812) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [25/45] time 0.452 (0.477) data 0.322 (0.346) loss_x loss_x 2.0312 (1.3221) acc_x 43.7500 (66.8750) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [30/45] time 0.444 (0.470) data 0.314 (0.340) loss_x loss_x 1.0459 (1.3067) acc_x 62.5000 (67.0833) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [35/45] time 0.506 (0.477) data 0.377 (0.347) loss_x loss_x 0.7178 (1.2675) acc_x 71.8750 (68.2143) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [40/45] time 0.455 (0.478) data 0.324 (0.347) loss_x loss_x 0.9395 (1.2853) acc_x 75.0000 (67.8125) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [45/45] time 0.420 (0.471) data 0.290 (0.341) loss_x loss_x 1.9707 (1.3071) acc_x 43.7500 (67.0833) lr 1.8763e-03 eta 0:00:00
epoch [34/200] batch [5/52] time 0.439 (0.472) data 0.308 (0.342) loss_u loss_u 0.7896 (0.7988) acc_u 28.1250 (25.6250) lr 1.8763e-03 eta 0:00:22
epoch [34/200] batch [10/52] time 0.372 (0.462) data 0.241 (0.331) loss_u loss_u 0.7061 (0.7684) acc_u 31.2500 (29.6875) lr 1.8763e-03 eta 0:00:19
epoch [34/200] batch [15/52] time 0.340 (0.455) data 0.209 (0.324) loss_u loss_u 0.7861 (0.7847) acc_u 31.2500 (27.9167) lr 1.8763e-03 eta 0:00:16
epoch [34/200] batch [20/52] time 0.563 (0.452) data 0.432 (0.321) loss_u loss_u 0.8442 (0.7858) acc_u 15.6250 (27.9688) lr 1.8763e-03 eta 0:00:14
epoch [34/200] batch [25/52] time 0.389 (0.452) data 0.257 (0.322) loss_u loss_u 0.8101 (0.7924) acc_u 18.7500 (27.0000) lr 1.8763e-03 eta 0:00:12
epoch [34/200] batch [30/52] time 0.469 (0.450) data 0.338 (0.320) loss_u loss_u 0.8330 (0.7968) acc_u 18.7500 (26.0417) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [35/52] time 0.372 (0.449) data 0.241 (0.318) loss_u loss_u 0.7812 (0.7964) acc_u 21.8750 (25.9821) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [40/52] time 0.543 (0.449) data 0.412 (0.318) loss_u loss_u 0.7773 (0.7937) acc_u 25.0000 (26.1719) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [45/52] time 0.467 (0.450) data 0.336 (0.319) loss_u loss_u 0.8130 (0.7931) acc_u 28.1250 (26.0417) lr 1.8763e-03 eta 0:00:03
epoch [34/200] batch [50/52] time 0.410 (0.448) data 0.278 (0.318) loss_u loss_u 0.7158 (0.7902) acc_u 34.3750 (26.6250) lr 1.8763e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1525
confident_label rate tensor(0.4643, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1456
clean true:1440
clean false:16
clean_rate:0.989010989010989
noisy true:171
noisy false:1509
after delete: len(clean_dataset) 1456
after delete: len(noisy_dataset) 1680
epoch [35/200] batch [5/45] time 0.365 (0.420) data 0.236 (0.290) loss_x loss_x 1.3242 (1.2695) acc_x 71.8750 (69.3750) lr 1.8686e-03 eta 0:00:16
epoch [35/200] batch [10/45] time 0.445 (0.444) data 0.314 (0.314) loss_x loss_x 1.3115 (1.1485) acc_x 62.5000 (70.3125) lr 1.8686e-03 eta 0:00:15
epoch [35/200] batch [15/45] time 0.582 (0.448) data 0.451 (0.318) loss_x loss_x 1.0215 (1.1683) acc_x 71.8750 (70.0000) lr 1.8686e-03 eta 0:00:13
epoch [35/200] batch [20/45] time 0.471 (0.441) data 0.341 (0.311) loss_x loss_x 1.0312 (1.1703) acc_x 81.2500 (70.7812) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [25/45] time 0.488 (0.439) data 0.358 (0.309) loss_x loss_x 1.5928 (1.1813) acc_x 62.5000 (70.7500) lr 1.8686e-03 eta 0:00:08
epoch [35/200] batch [30/45] time 0.539 (0.450) data 0.409 (0.320) loss_x loss_x 1.4150 (1.1861) acc_x 62.5000 (70.9375) lr 1.8686e-03 eta 0:00:06
epoch [35/200] batch [35/45] time 0.354 (0.452) data 0.223 (0.322) loss_x loss_x 1.2227 (1.2104) acc_x 78.1250 (69.8214) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [40/45] time 0.495 (0.454) data 0.364 (0.324) loss_x loss_x 1.0840 (1.2315) acc_x 68.7500 (69.4531) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [45/45] time 0.664 (0.456) data 0.534 (0.326) loss_x loss_x 1.4258 (1.2518) acc_x 65.6250 (68.7500) lr 1.8686e-03 eta 0:00:00
epoch [35/200] batch [5/52] time 0.384 (0.448) data 0.253 (0.318) loss_u loss_u 0.7954 (0.7820) acc_u 25.0000 (25.6250) lr 1.8686e-03 eta 0:00:21
epoch [35/200] batch [10/52] time 0.457 (0.454) data 0.326 (0.324) loss_u loss_u 0.7866 (0.7849) acc_u 21.8750 (26.2500) lr 1.8686e-03 eta 0:00:19
epoch [35/200] batch [15/52] time 0.618 (0.455) data 0.487 (0.324) loss_u loss_u 0.8218 (0.7916) acc_u 15.6250 (24.5833) lr 1.8686e-03 eta 0:00:16
epoch [35/200] batch [20/52] time 0.451 (0.451) data 0.320 (0.320) loss_u loss_u 0.8398 (0.7999) acc_u 18.7500 (23.9062) lr 1.8686e-03 eta 0:00:14
epoch [35/200] batch [25/52] time 0.416 (0.450) data 0.285 (0.319) loss_u loss_u 0.8257 (0.7944) acc_u 25.0000 (25.1250) lr 1.8686e-03 eta 0:00:12
epoch [35/200] batch [30/52] time 0.516 (0.451) data 0.385 (0.320) loss_u loss_u 0.7998 (0.7946) acc_u 31.2500 (25.6250) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [35/52] time 0.416 (0.449) data 0.285 (0.318) loss_u loss_u 0.8369 (0.7929) acc_u 28.1250 (26.6071) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [40/52] time 0.565 (0.451) data 0.434 (0.320) loss_u loss_u 0.7993 (0.7923) acc_u 15.6250 (26.4844) lr 1.8686e-03 eta 0:00:05
epoch [35/200] batch [45/52] time 0.451 (0.450) data 0.320 (0.319) loss_u loss_u 0.7505 (0.7942) acc_u 28.1250 (26.3889) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [50/52] time 0.497 (0.447) data 0.366 (0.317) loss_u loss_u 0.7607 (0.7963) acc_u 28.1250 (26.1875) lr 1.8686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1515
confident_label rate tensor(0.4646, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1457
clean true:1442
clean false:15
clean_rate:0.9897048730267674
noisy true:179
noisy false:1500
after delete: len(clean_dataset) 1457
after delete: len(noisy_dataset) 1679
epoch [36/200] batch [5/45] time 0.553 (0.467) data 0.422 (0.336) loss_x loss_x 1.0469 (1.2279) acc_x 71.8750 (66.8750) lr 1.8607e-03 eta 0:00:18
epoch [36/200] batch [10/45] time 0.420 (0.445) data 0.291 (0.315) loss_x loss_x 0.9648 (1.2354) acc_x 65.6250 (66.5625) lr 1.8607e-03 eta 0:00:15
epoch [36/200] batch [15/45] time 0.579 (0.465) data 0.447 (0.335) loss_x loss_x 0.7847 (1.1785) acc_x 68.7500 (68.1250) lr 1.8607e-03 eta 0:00:13
epoch [36/200] batch [20/45] time 0.419 (0.464) data 0.289 (0.334) loss_x loss_x 2.0684 (1.2561) acc_x 53.1250 (67.3438) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [25/45] time 0.451 (0.455) data 0.320 (0.325) loss_x loss_x 1.1982 (1.2547) acc_x 75.0000 (67.8750) lr 1.8607e-03 eta 0:00:09
epoch [36/200] batch [30/45] time 0.500 (0.458) data 0.370 (0.328) loss_x loss_x 1.0850 (1.2546) acc_x 75.0000 (67.8125) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [35/45] time 0.479 (0.471) data 0.349 (0.341) loss_x loss_x 0.9424 (1.2659) acc_x 71.8750 (67.6786) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [40/45] time 0.393 (0.465) data 0.263 (0.335) loss_x loss_x 0.7544 (1.2450) acc_x 75.0000 (67.7344) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [45/45] time 0.401 (0.465) data 0.270 (0.335) loss_x loss_x 1.2666 (1.2507) acc_x 65.6250 (67.5000) lr 1.8607e-03 eta 0:00:00
epoch [36/200] batch [5/52] time 0.464 (0.458) data 0.333 (0.328) loss_u loss_u 0.7856 (0.7764) acc_u 21.8750 (25.6250) lr 1.8607e-03 eta 0:00:21
epoch [36/200] batch [10/52] time 0.424 (0.456) data 0.293 (0.325) loss_u loss_u 0.7461 (0.7645) acc_u 28.1250 (28.7500) lr 1.8607e-03 eta 0:00:19
epoch [36/200] batch [15/52] time 0.511 (0.449) data 0.380 (0.318) loss_u loss_u 0.7744 (0.7812) acc_u 25.0000 (26.4583) lr 1.8607e-03 eta 0:00:16
epoch [36/200] batch [20/52] time 0.340 (0.446) data 0.210 (0.315) loss_u loss_u 0.7520 (0.7774) acc_u 37.5000 (26.8750) lr 1.8607e-03 eta 0:00:14
epoch [36/200] batch [25/52] time 0.425 (0.447) data 0.295 (0.317) loss_u loss_u 0.8335 (0.7845) acc_u 15.6250 (26.5000) lr 1.8607e-03 eta 0:00:12
epoch [36/200] batch [30/52] time 0.422 (0.449) data 0.291 (0.319) loss_u loss_u 0.8262 (0.7815) acc_u 18.7500 (27.3958) lr 1.8607e-03 eta 0:00:09
epoch [36/200] batch [35/52] time 0.378 (0.446) data 0.247 (0.315) loss_u loss_u 0.8452 (0.7827) acc_u 18.7500 (27.2321) lr 1.8607e-03 eta 0:00:07
epoch [36/200] batch [40/52] time 0.511 (0.444) data 0.380 (0.313) loss_u loss_u 0.7812 (0.7818) acc_u 25.0000 (27.3438) lr 1.8607e-03 eta 0:00:05
epoch [36/200] batch [45/52] time 0.420 (0.442) data 0.290 (0.311) loss_u loss_u 0.7593 (0.7832) acc_u 28.1250 (27.3611) lr 1.8607e-03 eta 0:00:03
epoch [36/200] batch [50/52] time 0.487 (0.442) data 0.358 (0.312) loss_u loss_u 0.8257 (0.7819) acc_u 21.8750 (27.6875) lr 1.8607e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1466
confident_label rate tensor(0.4818, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1511
clean true:1495
clean false:16
clean_rate:0.9894109861019192
noisy true:175
noisy false:1450
after delete: len(clean_dataset) 1511
after delete: len(noisy_dataset) 1625
epoch [37/200] batch [5/47] time 0.403 (0.462) data 0.273 (0.331) loss_x loss_x 1.4980 (1.1368) acc_x 59.3750 (69.3750) lr 1.8526e-03 eta 0:00:19
epoch [37/200] batch [10/47] time 0.471 (0.462) data 0.342 (0.332) loss_x loss_x 1.4492 (1.1943) acc_x 68.7500 (69.0625) lr 1.8526e-03 eta 0:00:17
epoch [37/200] batch [15/47] time 0.491 (0.487) data 0.360 (0.357) loss_x loss_x 1.0244 (1.2360) acc_x 71.8750 (67.7083) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [20/47] time 0.400 (0.501) data 0.269 (0.371) loss_x loss_x 1.6523 (1.2406) acc_x 65.6250 (68.2812) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [25/47] time 0.413 (0.487) data 0.283 (0.357) loss_x loss_x 0.9165 (1.2296) acc_x 78.1250 (68.5000) lr 1.8526e-03 eta 0:00:10
epoch [37/200] batch [30/47] time 0.410 (0.481) data 0.281 (0.351) loss_x loss_x 2.0391 (1.2814) acc_x 50.0000 (67.0833) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [35/47] time 0.389 (0.470) data 0.259 (0.340) loss_x loss_x 0.9717 (1.2609) acc_x 75.0000 (67.2321) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [40/47] time 0.421 (0.464) data 0.291 (0.334) loss_x loss_x 2.1797 (1.2853) acc_x 53.1250 (66.9531) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [45/47] time 0.598 (0.466) data 0.468 (0.336) loss_x loss_x 1.4189 (1.2991) acc_x 50.0000 (66.3889) lr 1.8526e-03 eta 0:00:00
epoch [37/200] batch [5/50] time 0.440 (0.460) data 0.310 (0.330) loss_u loss_u 0.8423 (0.7949) acc_u 18.7500 (25.6250) lr 1.8526e-03 eta 0:00:20
epoch [37/200] batch [10/50] time 0.382 (0.457) data 0.252 (0.327) loss_u loss_u 0.8223 (0.7865) acc_u 18.7500 (26.5625) lr 1.8526e-03 eta 0:00:18
epoch [37/200] batch [15/50] time 0.359 (0.454) data 0.228 (0.324) loss_u loss_u 0.8315 (0.7990) acc_u 25.0000 (26.0417) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [20/50] time 0.369 (0.448) data 0.239 (0.318) loss_u loss_u 0.7036 (0.7927) acc_u 34.3750 (26.7188) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [25/50] time 0.449 (0.446) data 0.317 (0.316) loss_u loss_u 0.7310 (0.7954) acc_u 37.5000 (26.5000) lr 1.8526e-03 eta 0:00:11
epoch [37/200] batch [30/50] time 0.295 (0.443) data 0.164 (0.313) loss_u loss_u 0.7915 (0.7928) acc_u 31.2500 (26.5625) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [35/50] time 0.363 (0.442) data 0.232 (0.312) loss_u loss_u 0.7266 (0.7981) acc_u 34.3750 (25.8929) lr 1.8526e-03 eta 0:00:06
epoch [37/200] batch [40/50] time 0.360 (0.444) data 0.229 (0.313) loss_u loss_u 0.7769 (0.8002) acc_u 28.1250 (25.5469) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [45/50] time 0.380 (0.441) data 0.249 (0.310) loss_u loss_u 0.7871 (0.8007) acc_u 28.1250 (25.5556) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [50/50] time 0.408 (0.441) data 0.277 (0.311) loss_u loss_u 0.8286 (0.8002) acc_u 15.6250 (25.5625) lr 1.8526e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1495
confident_label rate tensor(0.4774, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1497
clean true:1481
clean false:16
clean_rate:0.9893119572478289
noisy true:160
noisy false:1479
after delete: len(clean_dataset) 1497
after delete: len(noisy_dataset) 1639
epoch [38/200] batch [5/46] time 0.634 (0.444) data 0.501 (0.313) loss_x loss_x 1.1074 (1.3210) acc_x 71.8750 (66.2500) lr 1.8443e-03 eta 0:00:18
epoch [38/200] batch [10/46] time 0.429 (0.458) data 0.298 (0.327) loss_x loss_x 1.5498 (1.4525) acc_x 50.0000 (63.4375) lr 1.8443e-03 eta 0:00:16
epoch [38/200] batch [15/46] time 0.419 (0.446) data 0.289 (0.315) loss_x loss_x 1.6211 (1.4737) acc_x 65.6250 (63.1250) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [20/46] time 0.475 (0.448) data 0.345 (0.317) loss_x loss_x 1.4297 (1.4168) acc_x 56.2500 (62.5000) lr 1.8443e-03 eta 0:00:11
epoch [38/200] batch [25/46] time 0.526 (0.460) data 0.396 (0.330) loss_x loss_x 1.4131 (1.4023) acc_x 65.6250 (62.5000) lr 1.8443e-03 eta 0:00:09
epoch [38/200] batch [30/46] time 0.390 (0.465) data 0.260 (0.334) loss_x loss_x 1.0010 (1.4059) acc_x 78.1250 (63.2292) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [35/46] time 0.487 (0.461) data 0.357 (0.330) loss_x loss_x 1.6875 (1.4102) acc_x 65.6250 (63.2143) lr 1.8443e-03 eta 0:00:05
epoch [38/200] batch [40/46] time 0.524 (0.459) data 0.394 (0.328) loss_x loss_x 1.6650 (1.4127) acc_x 62.5000 (63.8281) lr 1.8443e-03 eta 0:00:02
epoch [38/200] batch [45/46] time 0.477 (0.452) data 0.346 (0.322) loss_x loss_x 1.7998 (1.4039) acc_x 56.2500 (63.9583) lr 1.8443e-03 eta 0:00:00
epoch [38/200] batch [5/51] time 0.460 (0.450) data 0.329 (0.320) loss_u loss_u 0.8008 (0.7521) acc_u 18.7500 (30.6250) lr 1.8443e-03 eta 0:00:20
epoch [38/200] batch [10/51] time 0.344 (0.447) data 0.213 (0.317) loss_u loss_u 0.8398 (0.7697) acc_u 15.6250 (28.4375) lr 1.8443e-03 eta 0:00:18
epoch [38/200] batch [15/51] time 0.494 (0.448) data 0.363 (0.317) loss_u loss_u 0.8740 (0.7887) acc_u 12.5000 (25.6250) lr 1.8443e-03 eta 0:00:16
epoch [38/200] batch [20/51] time 0.343 (0.442) data 0.212 (0.312) loss_u loss_u 0.8149 (0.7938) acc_u 28.1250 (25.6250) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [25/51] time 0.438 (0.441) data 0.307 (0.310) loss_u loss_u 0.8032 (0.7959) acc_u 21.8750 (25.0000) lr 1.8443e-03 eta 0:00:11
epoch [38/200] batch [30/51] time 0.452 (0.442) data 0.321 (0.312) loss_u loss_u 0.7773 (0.7872) acc_u 28.1250 (26.8750) lr 1.8443e-03 eta 0:00:09
epoch [38/200] batch [35/51] time 0.476 (0.439) data 0.342 (0.309) loss_u loss_u 0.8237 (0.7886) acc_u 18.7500 (26.6964) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [40/51] time 0.382 (0.441) data 0.251 (0.310) loss_u loss_u 0.8208 (0.7910) acc_u 18.7500 (26.3281) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [45/51] time 0.296 (0.442) data 0.165 (0.312) loss_u loss_u 0.7788 (0.7915) acc_u 28.1250 (26.4583) lr 1.8443e-03 eta 0:00:02
epoch [38/200] batch [50/51] time 0.377 (0.441) data 0.246 (0.310) loss_u loss_u 0.8110 (0.7936) acc_u 34.3750 (26.3750) lr 1.8443e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1566
confident_label rate tensor(0.4528, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1420
clean true:1405
clean false:15
clean_rate:0.9894366197183099
noisy true:165
noisy false:1551
after delete: len(clean_dataset) 1420
after delete: len(noisy_dataset) 1716
epoch [39/200] batch [5/44] time 0.550 (0.497) data 0.420 (0.366) loss_x loss_x 1.8096 (1.2437) acc_x 56.2500 (68.7500) lr 1.8358e-03 eta 0:00:19
epoch [39/200] batch [10/44] time 0.527 (0.494) data 0.396 (0.363) loss_x loss_x 0.7817 (1.2444) acc_x 81.2500 (68.4375) lr 1.8358e-03 eta 0:00:16
epoch [39/200] batch [15/44] time 0.481 (0.475) data 0.351 (0.345) loss_x loss_x 1.1426 (1.2544) acc_x 75.0000 (69.1667) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [20/44] time 0.612 (0.489) data 0.481 (0.359) loss_x loss_x 1.2646 (1.2630) acc_x 59.3750 (68.4375) lr 1.8358e-03 eta 0:00:11
epoch [39/200] batch [25/44] time 0.421 (0.485) data 0.290 (0.354) loss_x loss_x 1.0010 (1.2473) acc_x 71.8750 (68.6250) lr 1.8358e-03 eta 0:00:09
epoch [39/200] batch [30/44] time 0.439 (0.483) data 0.309 (0.352) loss_x loss_x 1.3320 (1.2498) acc_x 65.6250 (68.5417) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [35/44] time 0.394 (0.477) data 0.263 (0.346) loss_x loss_x 1.3105 (1.2651) acc_x 62.5000 (67.5000) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [40/44] time 0.459 (0.477) data 0.328 (0.346) loss_x loss_x 1.7451 (1.3218) acc_x 62.5000 (66.4062) lr 1.8358e-03 eta 0:00:01
epoch [39/200] batch [5/53] time 0.327 (0.464) data 0.196 (0.334) loss_u loss_u 0.6558 (0.7331) acc_u 43.7500 (32.5000) lr 1.8358e-03 eta 0:00:22
epoch [39/200] batch [10/53] time 0.544 (0.467) data 0.413 (0.336) loss_u loss_u 0.7852 (0.7638) acc_u 34.3750 (29.6875) lr 1.8358e-03 eta 0:00:20
epoch [39/200] batch [15/53] time 0.468 (0.463) data 0.337 (0.332) loss_u loss_u 0.8154 (0.7793) acc_u 21.8750 (27.2917) lr 1.8358e-03 eta 0:00:17
epoch [39/200] batch [20/53] time 0.356 (0.456) data 0.225 (0.326) loss_u loss_u 0.8516 (0.7814) acc_u 12.5000 (27.0312) lr 1.8358e-03 eta 0:00:15
epoch [39/200] batch [25/53] time 0.398 (0.453) data 0.267 (0.322) loss_u loss_u 0.8008 (0.7779) acc_u 21.8750 (28.0000) lr 1.8358e-03 eta 0:00:12
epoch [39/200] batch [30/53] time 0.480 (0.450) data 0.349 (0.319) loss_u loss_u 0.8188 (0.7828) acc_u 21.8750 (27.5000) lr 1.8358e-03 eta 0:00:10
epoch [39/200] batch [35/53] time 0.692 (0.452) data 0.561 (0.321) loss_u loss_u 0.8633 (0.7884) acc_u 12.5000 (26.7857) lr 1.8358e-03 eta 0:00:08
epoch [39/200] batch [40/53] time 0.346 (0.450) data 0.215 (0.319) loss_u loss_u 0.8306 (0.7863) acc_u 15.6250 (26.7188) lr 1.8358e-03 eta 0:00:05
epoch [39/200] batch [45/53] time 0.409 (0.448) data 0.278 (0.317) loss_u loss_u 0.7861 (0.7860) acc_u 34.3750 (27.0833) lr 1.8358e-03 eta 0:00:03
epoch [39/200] batch [50/53] time 0.383 (0.446) data 0.252 (0.315) loss_u loss_u 0.7622 (0.7851) acc_u 21.8750 (27.0000) lr 1.8358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1495
confident_label rate tensor(0.4732, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1484
clean true:1465
clean false:19
clean_rate:0.9871967654986523
noisy true:176
noisy false:1476
after delete: len(clean_dataset) 1484
after delete: len(noisy_dataset) 1652
epoch [40/200] batch [5/46] time 0.466 (0.497) data 0.335 (0.366) loss_x loss_x 1.1807 (1.0266) acc_x 68.7500 (74.3750) lr 1.8271e-03 eta 0:00:20
epoch [40/200] batch [10/46] time 0.438 (0.474) data 0.307 (0.344) loss_x loss_x 1.2178 (1.2416) acc_x 62.5000 (66.2500) lr 1.8271e-03 eta 0:00:17
epoch [40/200] batch [15/46] time 0.438 (0.474) data 0.308 (0.344) loss_x loss_x 1.3369 (1.2546) acc_x 62.5000 (66.0417) lr 1.8271e-03 eta 0:00:14
epoch [40/200] batch [20/46] time 0.442 (0.478) data 0.311 (0.347) loss_x loss_x 1.7393 (1.2645) acc_x 46.8750 (65.9375) lr 1.8271e-03 eta 0:00:12
epoch [40/200] batch [25/46] time 0.565 (0.478) data 0.435 (0.348) loss_x loss_x 1.3047 (1.2886) acc_x 65.6250 (65.7500) lr 1.8271e-03 eta 0:00:10
epoch [40/200] batch [30/46] time 0.462 (0.476) data 0.332 (0.346) loss_x loss_x 1.5928 (1.2624) acc_x 53.1250 (65.9375) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [35/46] time 0.528 (0.476) data 0.398 (0.345) loss_x loss_x 1.1143 (1.2779) acc_x 71.8750 (65.8036) lr 1.8271e-03 eta 0:00:05
epoch [40/200] batch [40/46] time 0.355 (0.473) data 0.224 (0.342) loss_x loss_x 1.2402 (1.2783) acc_x 62.5000 (66.0156) lr 1.8271e-03 eta 0:00:02
epoch [40/200] batch [45/46] time 0.448 (0.469) data 0.318 (0.338) loss_x loss_x 1.5342 (1.2741) acc_x 59.3750 (66.0417) lr 1.8271e-03 eta 0:00:00
epoch [40/200] batch [5/51] time 0.492 (0.466) data 0.361 (0.336) loss_u loss_u 0.8066 (0.7961) acc_u 31.2500 (30.0000) lr 1.8271e-03 eta 0:00:21
epoch [40/200] batch [10/51] time 0.446 (0.461) data 0.315 (0.331) loss_u loss_u 0.8643 (0.7902) acc_u 18.7500 (29.0625) lr 1.8271e-03 eta 0:00:18
epoch [40/200] batch [15/51] time 0.375 (0.456) data 0.244 (0.325) loss_u loss_u 0.8237 (0.8066) acc_u 18.7500 (25.8333) lr 1.8271e-03 eta 0:00:16
epoch [40/200] batch [20/51] time 0.386 (0.450) data 0.255 (0.319) loss_u loss_u 0.8252 (0.8064) acc_u 25.0000 (26.7188) lr 1.8271e-03 eta 0:00:13
epoch [40/200] batch [25/51] time 0.416 (0.449) data 0.286 (0.319) loss_u loss_u 0.8198 (0.7970) acc_u 18.7500 (27.5000) lr 1.8271e-03 eta 0:00:11
epoch [40/200] batch [30/51] time 0.771 (0.450) data 0.642 (0.320) loss_u loss_u 0.8643 (0.7952) acc_u 15.6250 (27.3958) lr 1.8271e-03 eta 0:00:09
epoch [40/200] batch [35/51] time 0.441 (0.450) data 0.311 (0.319) loss_u loss_u 0.7788 (0.7934) acc_u 31.2500 (27.7679) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [40/51] time 0.365 (0.448) data 0.235 (0.317) loss_u loss_u 0.8018 (0.7942) acc_u 25.0000 (27.6562) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [45/51] time 0.399 (0.447) data 0.269 (0.317) loss_u loss_u 0.7246 (0.7953) acc_u 40.6250 (27.4306) lr 1.8271e-03 eta 0:00:02
epoch [40/200] batch [50/51] time 0.426 (0.447) data 0.294 (0.316) loss_u loss_u 0.8120 (0.7945) acc_u 15.6250 (27.3750) lr 1.8271e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1477
confident_label rate tensor(0.4818, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1511
clean true:1494
clean false:17
clean_rate:0.9887491727332892
noisy true:165
noisy false:1460
after delete: len(clean_dataset) 1511
after delete: len(noisy_dataset) 1625
epoch [41/200] batch [5/47] time 0.522 (0.508) data 0.392 (0.379) loss_x loss_x 1.1729 (1.0235) acc_x 68.7500 (75.0000) lr 1.8181e-03 eta 0:00:21
epoch [41/200] batch [10/47] time 0.462 (0.483) data 0.332 (0.353) loss_x loss_x 1.4287 (1.0425) acc_x 62.5000 (74.6875) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [15/47] time 0.458 (0.476) data 0.328 (0.346) loss_x loss_x 1.7373 (1.1240) acc_x 65.6250 (71.2500) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [20/47] time 0.567 (0.492) data 0.437 (0.362) loss_x loss_x 1.2334 (1.1889) acc_x 68.7500 (70.0000) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [25/47] time 0.482 (0.488) data 0.352 (0.358) loss_x loss_x 1.6279 (1.2416) acc_x 56.2500 (68.7500) lr 1.8181e-03 eta 0:00:10
epoch [41/200] batch [30/47] time 0.427 (0.478) data 0.297 (0.348) loss_x loss_x 1.0449 (1.2558) acc_x 71.8750 (68.0208) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [35/47] time 0.415 (0.470) data 0.285 (0.340) loss_x loss_x 1.1289 (1.2452) acc_x 65.6250 (68.0357) lr 1.8181e-03 eta 0:00:05
epoch [41/200] batch [40/47] time 0.472 (0.463) data 0.342 (0.332) loss_x loss_x 1.1816 (1.2448) acc_x 71.8750 (67.8906) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [45/47] time 0.484 (0.460) data 0.354 (0.330) loss_x loss_x 1.4209 (1.2456) acc_x 65.6250 (68.1944) lr 1.8181e-03 eta 0:00:00
epoch [41/200] batch [5/50] time 0.485 (0.453) data 0.354 (0.323) loss_u loss_u 0.7329 (0.7479) acc_u 31.2500 (30.0000) lr 1.8181e-03 eta 0:00:20
epoch [41/200] batch [10/50] time 0.374 (0.450) data 0.243 (0.320) loss_u loss_u 0.8574 (0.7740) acc_u 18.7500 (27.8125) lr 1.8181e-03 eta 0:00:18
epoch [41/200] batch [15/50] time 0.457 (0.447) data 0.326 (0.317) loss_u loss_u 0.8330 (0.7939) acc_u 15.6250 (24.7917) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [20/50] time 0.454 (0.444) data 0.323 (0.313) loss_u loss_u 0.8706 (0.7908) acc_u 15.6250 (26.2500) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [25/50] time 0.529 (0.442) data 0.398 (0.311) loss_u loss_u 0.7607 (0.7892) acc_u 25.0000 (27.0000) lr 1.8181e-03 eta 0:00:11
epoch [41/200] batch [30/50] time 0.340 (0.442) data 0.209 (0.311) loss_u loss_u 0.7715 (0.7885) acc_u 21.8750 (26.7708) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [35/50] time 0.407 (0.445) data 0.277 (0.315) loss_u loss_u 0.7324 (0.7833) acc_u 40.6250 (27.2321) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [40/50] time 0.731 (0.445) data 0.600 (0.314) loss_u loss_u 0.7627 (0.7870) acc_u 31.2500 (26.9531) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [45/50] time 0.368 (0.441) data 0.237 (0.310) loss_u loss_u 0.8345 (0.7868) acc_u 31.2500 (27.4306) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [50/50] time 0.373 (0.441) data 0.243 (0.311) loss_u loss_u 0.8086 (0.7890) acc_u 25.0000 (27.3125) lr 1.8181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1525
confident_label rate tensor(0.4646, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1457
clean true:1439
clean false:18
clean_rate:0.9876458476321208
noisy true:172
noisy false:1507
after delete: len(clean_dataset) 1457
after delete: len(noisy_dataset) 1679
epoch [42/200] batch [5/45] time 0.509 (0.436) data 0.378 (0.305) loss_x loss_x 1.2754 (1.2176) acc_x 75.0000 (69.3750) lr 1.8090e-03 eta 0:00:17
epoch [42/200] batch [10/45] time 0.478 (0.442) data 0.347 (0.312) loss_x loss_x 1.3428 (1.2681) acc_x 75.0000 (70.3125) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [15/45] time 0.466 (0.440) data 0.336 (0.310) loss_x loss_x 1.0547 (1.2359) acc_x 75.0000 (69.7917) lr 1.8090e-03 eta 0:00:13
epoch [42/200] batch [20/45] time 0.440 (0.444) data 0.310 (0.314) loss_x loss_x 0.8379 (1.2376) acc_x 71.8750 (69.3750) lr 1.8090e-03 eta 0:00:11
epoch [42/200] batch [25/45] time 0.601 (0.453) data 0.471 (0.322) loss_x loss_x 1.4805 (1.2582) acc_x 65.6250 (68.3750) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [30/45] time 0.541 (0.456) data 0.411 (0.325) loss_x loss_x 0.8770 (1.2410) acc_x 75.0000 (68.7500) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [35/45] time 0.500 (0.460) data 0.370 (0.329) loss_x loss_x 1.4385 (1.2660) acc_x 68.7500 (68.5714) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [40/45] time 0.473 (0.459) data 0.342 (0.329) loss_x loss_x 1.3027 (1.2892) acc_x 71.8750 (68.0469) lr 1.8090e-03 eta 0:00:02
epoch [42/200] batch [45/45] time 0.517 (0.463) data 0.387 (0.333) loss_x loss_x 0.8809 (1.2965) acc_x 68.7500 (67.7778) lr 1.8090e-03 eta 0:00:00
epoch [42/200] batch [5/52] time 0.398 (0.459) data 0.267 (0.329) loss_u loss_u 0.7656 (0.7622) acc_u 28.1250 (28.7500) lr 1.8090e-03 eta 0:00:21
epoch [42/200] batch [10/52] time 0.356 (0.456) data 0.225 (0.325) loss_u loss_u 0.7949 (0.7871) acc_u 25.0000 (26.2500) lr 1.8090e-03 eta 0:00:19
epoch [42/200] batch [15/52] time 0.342 (0.450) data 0.211 (0.320) loss_u loss_u 0.7280 (0.7856) acc_u 31.2500 (26.2500) lr 1.8090e-03 eta 0:00:16
epoch [42/200] batch [20/52] time 0.393 (0.448) data 0.263 (0.317) loss_u loss_u 0.8252 (0.7857) acc_u 15.6250 (26.7188) lr 1.8090e-03 eta 0:00:14
epoch [42/200] batch [25/52] time 0.379 (0.452) data 0.249 (0.321) loss_u loss_u 0.7783 (0.7874) acc_u 40.6250 (26.7500) lr 1.8090e-03 eta 0:00:12
epoch [42/200] batch [30/52] time 0.420 (0.450) data 0.289 (0.319) loss_u loss_u 0.6836 (0.7885) acc_u 34.3750 (26.8750) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [35/52] time 0.537 (0.450) data 0.406 (0.319) loss_u loss_u 0.7417 (0.7850) acc_u 37.5000 (27.9464) lr 1.8090e-03 eta 0:00:07
epoch [42/200] batch [40/52] time 0.440 (0.447) data 0.309 (0.317) loss_u loss_u 0.7769 (0.7833) acc_u 28.1250 (28.1250) lr 1.8090e-03 eta 0:00:05
epoch [42/200] batch [45/52] time 0.435 (0.447) data 0.304 (0.317) loss_u loss_u 0.8003 (0.7813) acc_u 28.1250 (28.3333) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [50/52] time 0.354 (0.443) data 0.223 (0.313) loss_u loss_u 0.8218 (0.7849) acc_u 28.1250 (27.9375) lr 1.8090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1493
confident_label rate tensor(0.4786, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1501
clean true:1482
clean false:19
clean_rate:0.9873417721518988
noisy true:161
noisy false:1474
after delete: len(clean_dataset) 1501
after delete: len(noisy_dataset) 1635
epoch [43/200] batch [5/46] time 0.396 (0.408) data 0.266 (0.278) loss_x loss_x 0.9824 (1.0644) acc_x 81.2500 (70.6250) lr 1.7997e-03 eta 0:00:16
epoch [43/200] batch [10/46] time 0.528 (0.440) data 0.398 (0.310) loss_x loss_x 1.1797 (1.0830) acc_x 81.2500 (71.8750) lr 1.7997e-03 eta 0:00:15
epoch [43/200] batch [15/46] time 0.604 (0.451) data 0.474 (0.322) loss_x loss_x 1.0527 (1.0702) acc_x 68.7500 (72.9167) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [20/46] time 0.426 (0.458) data 0.297 (0.329) loss_x loss_x 1.5635 (1.0922) acc_x 62.5000 (72.8125) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [25/46] time 0.448 (0.461) data 0.318 (0.331) loss_x loss_x 1.3760 (1.1175) acc_x 62.5000 (71.7500) lr 1.7997e-03 eta 0:00:09
epoch [43/200] batch [30/46] time 0.425 (0.459) data 0.296 (0.330) loss_x loss_x 1.1123 (1.1574) acc_x 71.8750 (70.9375) lr 1.7997e-03 eta 0:00:07
epoch [43/200] batch [35/46] time 0.461 (0.455) data 0.331 (0.325) loss_x loss_x 1.2734 (1.1712) acc_x 75.0000 (70.3571) lr 1.7997e-03 eta 0:00:05
epoch [43/200] batch [40/46] time 0.441 (0.453) data 0.312 (0.324) loss_x loss_x 0.8682 (1.1709) acc_x 75.0000 (70.1562) lr 1.7997e-03 eta 0:00:02
epoch [43/200] batch [45/46] time 0.453 (0.450) data 0.321 (0.321) loss_x loss_x 1.1719 (1.1877) acc_x 68.7500 (70.2083) lr 1.7997e-03 eta 0:00:00
epoch [43/200] batch [5/51] time 0.321 (0.445) data 0.190 (0.315) loss_u loss_u 0.7612 (0.7626) acc_u 37.5000 (30.6250) lr 1.7997e-03 eta 0:00:20
epoch [43/200] batch [10/51] time 0.532 (0.447) data 0.401 (0.317) loss_u loss_u 0.7910 (0.7764) acc_u 25.0000 (27.8125) lr 1.7997e-03 eta 0:00:18
epoch [43/200] batch [15/51] time 0.336 (0.446) data 0.205 (0.316) loss_u loss_u 0.8750 (0.7773) acc_u 12.5000 (27.9167) lr 1.7997e-03 eta 0:00:16
epoch [43/200] batch [20/51] time 0.387 (0.442) data 0.256 (0.312) loss_u loss_u 0.8184 (0.7787) acc_u 21.8750 (27.5000) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [25/51] time 0.417 (0.439) data 0.286 (0.309) loss_u loss_u 0.7451 (0.7808) acc_u 28.1250 (26.8750) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [30/51] time 0.442 (0.437) data 0.311 (0.307) loss_u loss_u 0.8477 (0.7830) acc_u 12.5000 (26.7708) lr 1.7997e-03 eta 0:00:09
epoch [43/200] batch [35/51] time 0.608 (0.440) data 0.477 (0.310) loss_u loss_u 0.8530 (0.7849) acc_u 18.7500 (26.6071) lr 1.7997e-03 eta 0:00:07
epoch [43/200] batch [40/51] time 0.472 (0.443) data 0.341 (0.313) loss_u loss_u 0.7422 (0.7820) acc_u 31.2500 (26.9531) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [45/51] time 0.331 (0.442) data 0.200 (0.312) loss_u loss_u 0.8765 (0.7815) acc_u 12.5000 (27.2222) lr 1.7997e-03 eta 0:00:02
epoch [43/200] batch [50/51] time 0.366 (0.442) data 0.236 (0.311) loss_u loss_u 0.7603 (0.7845) acc_u 21.8750 (26.8750) lr 1.7997e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1490
confident_label rate tensor(0.4758, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1492
clean true:1471
clean false:21
clean_rate:0.9859249329758714
noisy true:175
noisy false:1469
after delete: len(clean_dataset) 1492
after delete: len(noisy_dataset) 1644
epoch [44/200] batch [5/46] time 0.499 (0.425) data 0.369 (0.294) loss_x loss_x 1.2090 (1.1725) acc_x 68.7500 (73.1250) lr 1.7902e-03 eta 0:00:17
epoch [44/200] batch [10/46] time 0.468 (0.417) data 0.338 (0.287) loss_x loss_x 1.3799 (1.1210) acc_x 75.0000 (72.8125) lr 1.7902e-03 eta 0:00:15
epoch [44/200] batch [15/46] time 0.632 (0.445) data 0.500 (0.314) loss_x loss_x 1.5801 (1.2097) acc_x 65.6250 (69.1667) lr 1.7902e-03 eta 0:00:13
epoch [44/200] batch [20/46] time 0.373 (0.452) data 0.242 (0.322) loss_x loss_x 1.6533 (1.2183) acc_x 62.5000 (69.0625) lr 1.7902e-03 eta 0:00:11
epoch [44/200] batch [25/46] time 0.411 (0.452) data 0.280 (0.322) loss_x loss_x 0.9536 (1.2079) acc_x 81.2500 (68.8750) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [30/46] time 0.362 (0.452) data 0.232 (0.321) loss_x loss_x 1.1484 (1.2523) acc_x 78.1250 (67.9167) lr 1.7902e-03 eta 0:00:07
epoch [44/200] batch [35/46] time 0.370 (0.444) data 0.240 (0.313) loss_x loss_x 1.1885 (1.2448) acc_x 68.7500 (67.8571) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [40/46] time 0.536 (0.449) data 0.406 (0.319) loss_x loss_x 1.2031 (1.2498) acc_x 65.6250 (67.6562) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [45/46] time 0.403 (0.448) data 0.273 (0.317) loss_x loss_x 1.1309 (1.2639) acc_x 75.0000 (67.0833) lr 1.7902e-03 eta 0:00:00
epoch [44/200] batch [5/51] time 0.428 (0.442) data 0.296 (0.311) loss_u loss_u 0.7637 (0.8012) acc_u 28.1250 (28.1250) lr 1.7902e-03 eta 0:00:20
epoch [44/200] batch [10/51] time 0.445 (0.439) data 0.314 (0.308) loss_u loss_u 0.7466 (0.7723) acc_u 37.5000 (31.8750) lr 1.7902e-03 eta 0:00:17
epoch [44/200] batch [15/51] time 0.375 (0.438) data 0.244 (0.307) loss_u loss_u 0.8604 (0.7930) acc_u 21.8750 (27.9167) lr 1.7902e-03 eta 0:00:15
epoch [44/200] batch [20/51] time 0.569 (0.441) data 0.438 (0.310) loss_u loss_u 0.8291 (0.7933) acc_u 15.6250 (26.7188) lr 1.7902e-03 eta 0:00:13
epoch [44/200] batch [25/51] time 0.336 (0.439) data 0.207 (0.309) loss_u loss_u 0.7515 (0.7872) acc_u 31.2500 (27.2500) lr 1.7902e-03 eta 0:00:11
epoch [44/200] batch [30/51] time 0.351 (0.437) data 0.221 (0.307) loss_u loss_u 0.7588 (0.7881) acc_u 37.5000 (27.5000) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [35/51] time 0.474 (0.435) data 0.343 (0.305) loss_u loss_u 0.8315 (0.7904) acc_u 18.7500 (26.9643) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [40/51] time 0.520 (0.436) data 0.389 (0.306) loss_u loss_u 0.7739 (0.7902) acc_u 25.0000 (27.0312) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [45/51] time 0.411 (0.438) data 0.278 (0.307) loss_u loss_u 0.8066 (0.7903) acc_u 28.1250 (26.8056) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [50/51] time 0.371 (0.440) data 0.240 (0.309) loss_u loss_u 0.7104 (0.7855) acc_u 37.5000 (27.7500) lr 1.7902e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1462
confident_label rate tensor(0.4828, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1514
clean true:1503
clean false:11
clean_rate:0.9927344782034346
noisy true:171
noisy false:1451
after delete: len(clean_dataset) 1514
after delete: len(noisy_dataset) 1622
epoch [45/200] batch [5/47] time 0.432 (0.501) data 0.301 (0.371) loss_x loss_x 1.5176 (1.2610) acc_x 56.2500 (65.6250) lr 1.7804e-03 eta 0:00:21
epoch [45/200] batch [10/47] time 0.406 (0.485) data 0.275 (0.354) loss_x loss_x 1.6504 (1.2674) acc_x 59.3750 (67.5000) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [15/47] time 0.536 (0.482) data 0.405 (0.351) loss_x loss_x 1.2959 (1.2797) acc_x 68.7500 (67.2917) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [20/47] time 0.395 (0.476) data 0.265 (0.346) loss_x loss_x 1.3057 (1.2282) acc_x 65.6250 (68.4375) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [25/47] time 0.517 (0.465) data 0.386 (0.335) loss_x loss_x 1.0078 (1.2255) acc_x 65.6250 (68.0000) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [30/47] time 0.512 (0.460) data 0.382 (0.329) loss_x loss_x 1.1836 (1.2716) acc_x 68.7500 (67.3958) lr 1.7804e-03 eta 0:00:07
epoch [45/200] batch [35/47] time 0.414 (0.455) data 0.284 (0.325) loss_x loss_x 1.4717 (1.3075) acc_x 68.7500 (66.6964) lr 1.7804e-03 eta 0:00:05
epoch [45/200] batch [40/47] time 0.531 (0.456) data 0.401 (0.326) loss_x loss_x 1.5068 (1.3061) acc_x 62.5000 (66.7969) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [45/47] time 0.654 (0.460) data 0.524 (0.329) loss_x loss_x 1.6523 (1.3127) acc_x 62.5000 (66.5278) lr 1.7804e-03 eta 0:00:00
epoch [45/200] batch [5/50] time 0.382 (0.463) data 0.251 (0.333) loss_u loss_u 0.7544 (0.7638) acc_u 28.1250 (30.0000) lr 1.7804e-03 eta 0:00:20
epoch [45/200] batch [10/50] time 0.359 (0.456) data 0.230 (0.326) loss_u loss_u 0.8115 (0.7973) acc_u 21.8750 (25.0000) lr 1.7804e-03 eta 0:00:18
epoch [45/200] batch [15/50] time 0.435 (0.457) data 0.305 (0.327) loss_u loss_u 0.7764 (0.7923) acc_u 28.1250 (26.8750) lr 1.7804e-03 eta 0:00:16
epoch [45/200] batch [20/50] time 0.500 (0.457) data 0.369 (0.327) loss_u loss_u 0.6948 (0.7796) acc_u 37.5000 (28.2812) lr 1.7804e-03 eta 0:00:13
epoch [45/200] batch [25/50] time 0.339 (0.453) data 0.209 (0.323) loss_u loss_u 0.6963 (0.7788) acc_u 34.3750 (28.5000) lr 1.7804e-03 eta 0:00:11
epoch [45/200] batch [30/50] time 0.383 (0.450) data 0.254 (0.320) loss_u loss_u 0.8276 (0.7819) acc_u 15.6250 (27.6042) lr 1.7804e-03 eta 0:00:09
epoch [45/200] batch [35/50] time 0.344 (0.448) data 0.213 (0.318) loss_u loss_u 0.7588 (0.7813) acc_u 28.1250 (27.8571) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [40/50] time 0.400 (0.447) data 0.270 (0.317) loss_u loss_u 0.8789 (0.7859) acc_u 12.5000 (27.3438) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [45/50] time 0.460 (0.447) data 0.329 (0.317) loss_u loss_u 0.7734 (0.7845) acc_u 31.2500 (27.5000) lr 1.7804e-03 eta 0:00:02
epoch [45/200] batch [50/50] time 0.324 (0.444) data 0.194 (0.313) loss_u loss_u 0.8228 (0.7829) acc_u 31.2500 (27.8750) lr 1.7804e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1468
confident_label rate tensor(0.4828, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1514
clean true:1493
clean false:21
clean_rate:0.9861294583883752
noisy true:175
noisy false:1447
after delete: len(clean_dataset) 1514
after delete: len(noisy_dataset) 1622
epoch [46/200] batch [5/47] time 0.383 (0.460) data 0.252 (0.327) loss_x loss_x 1.3154 (1.2740) acc_x 62.5000 (65.0000) lr 1.7705e-03 eta 0:00:19
epoch [46/200] batch [10/47] time 0.344 (0.427) data 0.213 (0.295) loss_x loss_x 1.2461 (1.2790) acc_x 65.6250 (65.3125) lr 1.7705e-03 eta 0:00:15
epoch [46/200] batch [15/47] time 0.444 (0.453) data 0.313 (0.322) loss_x loss_x 1.0225 (1.2994) acc_x 71.8750 (65.2083) lr 1.7705e-03 eta 0:00:14
epoch [46/200] batch [20/47] time 0.431 (0.454) data 0.301 (0.323) loss_x loss_x 0.7217 (1.2738) acc_x 78.1250 (65.6250) lr 1.7705e-03 eta 0:00:12
epoch [46/200] batch [25/47] time 0.448 (0.452) data 0.317 (0.321) loss_x loss_x 1.7012 (1.2433) acc_x 50.0000 (66.5000) lr 1.7705e-03 eta 0:00:09
epoch [46/200] batch [30/47] time 0.546 (0.456) data 0.416 (0.325) loss_x loss_x 1.3135 (1.2423) acc_x 53.1250 (66.5625) lr 1.7705e-03 eta 0:00:07
epoch [46/200] batch [35/47] time 0.486 (0.460) data 0.355 (0.329) loss_x loss_x 1.0879 (1.2360) acc_x 71.8750 (67.2321) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [40/47] time 0.558 (0.458) data 0.428 (0.327) loss_x loss_x 2.4023 (1.2861) acc_x 46.8750 (66.4062) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [45/47] time 0.553 (0.459) data 0.423 (0.328) loss_x loss_x 1.4033 (1.2903) acc_x 65.6250 (66.1111) lr 1.7705e-03 eta 0:00:00
epoch [46/200] batch [5/50] time 0.398 (0.457) data 0.267 (0.326) loss_u loss_u 0.8008 (0.7615) acc_u 28.1250 (31.2500) lr 1.7705e-03 eta 0:00:20
epoch [46/200] batch [10/50] time 0.454 (0.453) data 0.324 (0.322) loss_u loss_u 0.8174 (0.7662) acc_u 21.8750 (30.6250) lr 1.7705e-03 eta 0:00:18
epoch [46/200] batch [15/50] time 0.570 (0.452) data 0.440 (0.321) loss_u loss_u 0.7251 (0.7643) acc_u 37.5000 (31.6667) lr 1.7705e-03 eta 0:00:15
epoch [46/200] batch [20/50] time 0.500 (0.450) data 0.369 (0.320) loss_u loss_u 0.7891 (0.7761) acc_u 25.0000 (29.5312) lr 1.7705e-03 eta 0:00:13
epoch [46/200] batch [25/50] time 0.356 (0.446) data 0.225 (0.316) loss_u loss_u 0.7153 (0.7802) acc_u 31.2500 (29.1250) lr 1.7705e-03 eta 0:00:11
epoch [46/200] batch [30/50] time 0.640 (0.448) data 0.508 (0.317) loss_u loss_u 0.6616 (0.7749) acc_u 43.7500 (29.5833) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [35/50] time 0.437 (0.447) data 0.306 (0.317) loss_u loss_u 0.8154 (0.7751) acc_u 21.8750 (29.1964) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [40/50] time 0.451 (0.444) data 0.320 (0.313) loss_u loss_u 0.7988 (0.7816) acc_u 31.2500 (28.3594) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [45/50] time 0.474 (0.443) data 0.343 (0.312) loss_u loss_u 0.7603 (0.7783) acc_u 37.5000 (28.8889) lr 1.7705e-03 eta 0:00:02
epoch [46/200] batch [50/50] time 0.334 (0.443) data 0.203 (0.313) loss_u loss_u 0.8203 (0.7787) acc_u 25.0000 (28.9375) lr 1.7705e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1472
confident_label rate tensor(0.4809, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1508
clean true:1488
clean false:20
clean_rate:0.986737400530504
noisy true:176
noisy false:1452
after delete: len(clean_dataset) 1508
after delete: len(noisy_dataset) 1628
epoch [47/200] batch [5/47] time 0.507 (0.446) data 0.377 (0.315) loss_x loss_x 0.9878 (1.3323) acc_x 87.5000 (66.2500) lr 1.7604e-03 eta 0:00:18
epoch [47/200] batch [10/47] time 0.457 (0.463) data 0.327 (0.333) loss_x loss_x 1.6816 (1.4443) acc_x 65.6250 (63.1250) lr 1.7604e-03 eta 0:00:17
epoch [47/200] batch [15/47] time 0.411 (0.452) data 0.281 (0.321) loss_x loss_x 1.2695 (1.3761) acc_x 65.6250 (65.2083) lr 1.7604e-03 eta 0:00:14
epoch [47/200] batch [20/47] time 0.490 (0.444) data 0.360 (0.314) loss_x loss_x 1.5762 (1.3731) acc_x 59.3750 (65.6250) lr 1.7604e-03 eta 0:00:11
epoch [47/200] batch [25/47] time 0.555 (0.453) data 0.425 (0.322) loss_x loss_x 1.3936 (1.3311) acc_x 68.7500 (66.8750) lr 1.7604e-03 eta 0:00:09
epoch [47/200] batch [30/47] time 0.506 (0.452) data 0.375 (0.322) loss_x loss_x 0.8115 (1.2899) acc_x 75.0000 (67.2917) lr 1.7604e-03 eta 0:00:07
epoch [47/200] batch [35/47] time 0.452 (0.449) data 0.321 (0.318) loss_x loss_x 1.1865 (1.2931) acc_x 65.6250 (67.2321) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [40/47] time 0.623 (0.452) data 0.492 (0.322) loss_x loss_x 1.3711 (1.3046) acc_x 68.7500 (67.0312) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [45/47] time 0.423 (0.450) data 0.293 (0.320) loss_x loss_x 1.3027 (1.2947) acc_x 68.7500 (66.7361) lr 1.7604e-03 eta 0:00:00
epoch [47/200] batch [5/50] time 0.519 (0.455) data 0.389 (0.324) loss_u loss_u 0.7988 (0.7662) acc_u 25.0000 (30.6250) lr 1.7604e-03 eta 0:00:20
epoch [47/200] batch [10/50] time 0.358 (0.453) data 0.227 (0.322) loss_u loss_u 0.7974 (0.7795) acc_u 21.8750 (28.1250) lr 1.7604e-03 eta 0:00:18
epoch [47/200] batch [15/50] time 0.421 (0.448) data 0.289 (0.318) loss_u loss_u 0.7246 (0.7889) acc_u 40.6250 (27.9167) lr 1.7604e-03 eta 0:00:15
epoch [47/200] batch [20/50] time 0.355 (0.442) data 0.223 (0.312) loss_u loss_u 0.7812 (0.7823) acc_u 25.0000 (28.9062) lr 1.7604e-03 eta 0:00:13
epoch [47/200] batch [25/50] time 0.368 (0.442) data 0.237 (0.311) loss_u loss_u 0.7705 (0.7811) acc_u 28.1250 (28.8750) lr 1.7604e-03 eta 0:00:11
epoch [47/200] batch [30/50] time 0.381 (0.441) data 0.250 (0.310) loss_u loss_u 0.7061 (0.7788) acc_u 37.5000 (29.6875) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [35/50] time 0.425 (0.441) data 0.293 (0.310) loss_u loss_u 0.7798 (0.7776) acc_u 21.8750 (29.0179) lr 1.7604e-03 eta 0:00:06
epoch [47/200] batch [40/50] time 0.451 (0.443) data 0.320 (0.312) loss_u loss_u 0.8091 (0.7831) acc_u 25.0000 (28.0469) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [45/50] time 0.444 (0.442) data 0.313 (0.311) loss_u loss_u 0.7163 (0.7768) acc_u 40.6250 (28.9583) lr 1.7604e-03 eta 0:00:02
epoch [47/200] batch [50/50] time 0.393 (0.444) data 0.262 (0.313) loss_u loss_u 0.8013 (0.7773) acc_u 28.1250 (28.8125) lr 1.7604e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1485
confident_label rate tensor(0.4770, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1496
clean true:1476
clean false:20
clean_rate:0.9866310160427807
noisy true:175
noisy false:1465
after delete: len(clean_dataset) 1496
after delete: len(noisy_dataset) 1640
epoch [48/200] batch [5/46] time 0.465 (0.468) data 0.334 (0.338) loss_x loss_x 1.2822 (1.2902) acc_x 68.7500 (66.2500) lr 1.7501e-03 eta 0:00:19
epoch [48/200] batch [10/46] time 0.445 (0.455) data 0.315 (0.325) loss_x loss_x 0.9873 (1.1905) acc_x 78.1250 (70.0000) lr 1.7501e-03 eta 0:00:16
epoch [48/200] batch [15/46] time 0.557 (0.474) data 0.427 (0.344) loss_x loss_x 1.4492 (1.2012) acc_x 68.7500 (69.1667) lr 1.7501e-03 eta 0:00:14
epoch [48/200] batch [20/46] time 0.436 (0.475) data 0.306 (0.345) loss_x loss_x 0.8398 (1.1942) acc_x 78.1250 (69.6875) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [25/46] time 0.542 (0.473) data 0.412 (0.343) loss_x loss_x 0.8193 (1.2005) acc_x 84.3750 (69.6250) lr 1.7501e-03 eta 0:00:09
epoch [48/200] batch [30/46] time 0.473 (0.474) data 0.343 (0.343) loss_x loss_x 1.1377 (1.2218) acc_x 68.7500 (69.0625) lr 1.7501e-03 eta 0:00:07
epoch [48/200] batch [35/46] time 0.397 (0.472) data 0.266 (0.342) loss_x loss_x 1.3535 (1.2448) acc_x 68.7500 (68.8393) lr 1.7501e-03 eta 0:00:05
epoch [48/200] batch [40/46] time 0.471 (0.469) data 0.340 (0.339) loss_x loss_x 1.0273 (1.2307) acc_x 75.0000 (69.3750) lr 1.7501e-03 eta 0:00:02
epoch [48/200] batch [45/46] time 0.558 (0.469) data 0.428 (0.338) loss_x loss_x 1.0479 (1.2203) acc_x 68.7500 (69.3056) lr 1.7501e-03 eta 0:00:00
epoch [48/200] batch [5/51] time 0.475 (0.462) data 0.344 (0.331) loss_u loss_u 0.7793 (0.7676) acc_u 21.8750 (27.5000) lr 1.7501e-03 eta 0:00:21
epoch [48/200] batch [10/51] time 0.452 (0.461) data 0.321 (0.330) loss_u loss_u 0.8003 (0.7726) acc_u 18.7500 (26.8750) lr 1.7501e-03 eta 0:00:18
epoch [48/200] batch [15/51] time 0.403 (0.460) data 0.269 (0.330) loss_u loss_u 0.7769 (0.7722) acc_u 28.1250 (27.5000) lr 1.7501e-03 eta 0:00:16
epoch [48/200] batch [20/51] time 0.491 (0.456) data 0.360 (0.325) loss_u loss_u 0.7920 (0.7779) acc_u 25.0000 (27.1875) lr 1.7501e-03 eta 0:00:14
epoch [48/200] batch [25/51] time 0.538 (0.458) data 0.407 (0.327) loss_u loss_u 0.7334 (0.7772) acc_u 37.5000 (27.5000) lr 1.7501e-03 eta 0:00:11
epoch [48/200] batch [30/51] time 0.418 (0.453) data 0.287 (0.323) loss_u loss_u 0.6875 (0.7763) acc_u 37.5000 (27.5000) lr 1.7501e-03 eta 0:00:09
epoch [48/200] batch [35/51] time 0.372 (0.450) data 0.241 (0.319) loss_u loss_u 0.6846 (0.7724) acc_u 40.6250 (28.1250) lr 1.7501e-03 eta 0:00:07
epoch [48/200] batch [40/51] time 0.406 (0.448) data 0.275 (0.317) loss_u loss_u 0.7896 (0.7729) acc_u 31.2500 (27.9688) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [45/51] time 0.449 (0.445) data 0.318 (0.314) loss_u loss_u 0.8608 (0.7743) acc_u 18.7500 (27.8472) lr 1.7501e-03 eta 0:00:02
epoch [48/200] batch [50/51] time 0.334 (0.442) data 0.203 (0.311) loss_u loss_u 0.7212 (0.7747) acc_u 40.6250 (27.7500) lr 1.7501e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1448
confident_label rate tensor(0.4869, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1527
clean true:1510
clean false:17
clean_rate:0.9888670595939751
noisy true:178
noisy false:1431
after delete: len(clean_dataset) 1527
after delete: len(noisy_dataset) 1609
epoch [49/200] batch [5/47] time 0.387 (0.394) data 0.256 (0.263) loss_x loss_x 1.2656 (1.4168) acc_x 59.3750 (61.8750) lr 1.7396e-03 eta 0:00:16
epoch [49/200] batch [10/47] time 0.387 (0.426) data 0.257 (0.296) loss_x loss_x 1.1143 (1.3309) acc_x 71.8750 (65.0000) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [15/47] time 0.381 (0.423) data 0.250 (0.293) loss_x loss_x 1.4453 (1.3671) acc_x 62.5000 (66.0417) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [20/47] time 0.457 (0.429) data 0.324 (0.299) loss_x loss_x 1.3486 (1.3501) acc_x 59.3750 (66.2500) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [25/47] time 0.471 (0.438) data 0.341 (0.308) loss_x loss_x 1.1045 (1.3560) acc_x 75.0000 (65.5000) lr 1.7396e-03 eta 0:00:09
epoch [49/200] batch [30/47] time 0.402 (0.436) data 0.272 (0.306) loss_x loss_x 1.1631 (1.3384) acc_x 53.1250 (65.5208) lr 1.7396e-03 eta 0:00:07
epoch [49/200] batch [35/47] time 0.387 (0.437) data 0.256 (0.307) loss_x loss_x 0.8105 (1.3386) acc_x 78.1250 (65.7143) lr 1.7396e-03 eta 0:00:05
epoch [49/200] batch [40/47] time 0.426 (0.439) data 0.296 (0.308) loss_x loss_x 1.0645 (1.2933) acc_x 78.1250 (66.7969) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [45/47] time 0.364 (0.446) data 0.234 (0.316) loss_x loss_x 0.9648 (1.2762) acc_x 78.1250 (67.1528) lr 1.7396e-03 eta 0:00:00
epoch [49/200] batch [5/50] time 0.605 (0.446) data 0.475 (0.316) loss_u loss_u 0.7681 (0.7898) acc_u 37.5000 (30.0000) lr 1.7396e-03 eta 0:00:20
epoch [49/200] batch [10/50] time 0.512 (0.449) data 0.381 (0.318) loss_u loss_u 0.7393 (0.7680) acc_u 25.0000 (30.3125) lr 1.7396e-03 eta 0:00:17
epoch [49/200] batch [15/50] time 0.561 (0.448) data 0.430 (0.317) loss_u loss_u 0.7778 (0.7762) acc_u 25.0000 (28.7500) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [20/50] time 0.323 (0.446) data 0.192 (0.315) loss_u loss_u 0.8281 (0.7809) acc_u 28.1250 (28.2812) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [25/50] time 0.445 (0.443) data 0.315 (0.312) loss_u loss_u 0.8545 (0.7828) acc_u 15.6250 (27.7500) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [30/50] time 0.426 (0.441) data 0.295 (0.310) loss_u loss_u 0.7949 (0.7876) acc_u 28.1250 (27.2917) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [35/50] time 0.458 (0.443) data 0.326 (0.312) loss_u loss_u 0.8608 (0.7863) acc_u 15.6250 (27.4107) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [40/50] time 0.392 (0.444) data 0.261 (0.313) loss_u loss_u 0.7993 (0.7912) acc_u 31.2500 (26.5625) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [45/50] time 0.419 (0.443) data 0.288 (0.313) loss_u loss_u 0.7827 (0.7911) acc_u 28.1250 (26.7361) lr 1.7396e-03 eta 0:00:02
epoch [49/200] batch [50/50] time 0.491 (0.440) data 0.360 (0.310) loss_u loss_u 0.8623 (0.7891) acc_u 15.6250 (27.0000) lr 1.7396e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1451
confident_label rate tensor(0.4869, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1527
clean true:1505
clean false:22
clean_rate:0.9855926653569089
noisy true:180
noisy false:1429
after delete: len(clean_dataset) 1527
after delete: len(noisy_dataset) 1609
epoch [50/200] batch [5/47] time 0.415 (0.453) data 0.286 (0.323) loss_x loss_x 1.1289 (1.1766) acc_x 71.8750 (69.3750) lr 1.7290e-03 eta 0:00:19
epoch [50/200] batch [10/47] time 0.431 (0.458) data 0.302 (0.327) loss_x loss_x 0.9243 (1.1603) acc_x 84.3750 (70.0000) lr 1.7290e-03 eta 0:00:16
epoch [50/200] batch [15/47] time 0.516 (0.458) data 0.386 (0.327) loss_x loss_x 1.3301 (1.1697) acc_x 71.8750 (71.8750) lr 1.7290e-03 eta 0:00:14
epoch [50/200] batch [20/47] time 0.351 (0.450) data 0.220 (0.319) loss_x loss_x 1.6377 (1.1936) acc_x 50.0000 (70.1562) lr 1.7290e-03 eta 0:00:12
epoch [50/200] batch [25/47] time 0.490 (0.459) data 0.360 (0.329) loss_x loss_x 0.7891 (1.1896) acc_x 81.2500 (70.2500) lr 1.7290e-03 eta 0:00:10
epoch [50/200] batch [30/47] time 0.423 (0.463) data 0.292 (0.333) loss_x loss_x 0.6987 (1.1777) acc_x 90.6250 (70.3125) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [35/47] time 0.555 (0.461) data 0.426 (0.330) loss_x loss_x 0.8511 (1.2317) acc_x 75.0000 (69.6429) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [40/47] time 0.379 (0.458) data 0.249 (0.328) loss_x loss_x 1.3457 (1.2526) acc_x 59.3750 (68.9844) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [45/47] time 0.485 (0.462) data 0.356 (0.332) loss_x loss_x 1.5322 (1.2695) acc_x 68.7500 (68.4722) lr 1.7290e-03 eta 0:00:00
epoch [50/200] batch [5/50] time 0.471 (0.459) data 0.341 (0.329) loss_u loss_u 0.7983 (0.7724) acc_u 18.7500 (24.3750) lr 1.7290e-03 eta 0:00:20
epoch [50/200] batch [10/50] time 0.647 (0.458) data 0.516 (0.327) loss_u loss_u 0.7656 (0.7767) acc_u 28.1250 (25.6250) lr 1.7290e-03 eta 0:00:18
epoch [50/200] batch [15/50] time 0.423 (0.454) data 0.293 (0.323) loss_u loss_u 0.7466 (0.7765) acc_u 28.1250 (26.8750) lr 1.7290e-03 eta 0:00:15
epoch [50/200] batch [20/50] time 0.514 (0.451) data 0.385 (0.321) loss_u loss_u 0.7583 (0.7721) acc_u 34.3750 (27.9688) lr 1.7290e-03 eta 0:00:13
epoch [50/200] batch [25/50] time 0.425 (0.446) data 0.295 (0.316) loss_u loss_u 0.7437 (0.7706) acc_u 34.3750 (28.7500) lr 1.7290e-03 eta 0:00:11
epoch [50/200] batch [30/50] time 0.488 (0.448) data 0.358 (0.318) loss_u loss_u 0.7744 (0.7801) acc_u 31.2500 (27.6042) lr 1.7290e-03 eta 0:00:08
epoch [50/200] batch [35/50] time 0.367 (0.444) data 0.237 (0.314) loss_u loss_u 0.8652 (0.7867) acc_u 21.8750 (26.8750) lr 1.7290e-03 eta 0:00:06
epoch [50/200] batch [40/50] time 0.361 (0.441) data 0.231 (0.311) loss_u loss_u 0.7402 (0.7863) acc_u 34.3750 (27.0312) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [45/50] time 0.546 (0.440) data 0.414 (0.310) loss_u loss_u 0.7695 (0.7876) acc_u 31.2500 (27.0139) lr 1.7290e-03 eta 0:00:02
epoch [50/200] batch [50/50] time 0.454 (0.440) data 0.323 (0.309) loss_u loss_u 0.8086 (0.7837) acc_u 28.1250 (27.5625) lr 1.7290e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1460
confident_label rate tensor(0.4818, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1511
clean true:1494
clean false:17
clean_rate:0.9887491727332892
noisy true:182
noisy false:1443
after delete: len(clean_dataset) 1511
after delete: len(noisy_dataset) 1625
epoch [51/200] batch [5/47] time 0.475 (0.451) data 0.345 (0.322) loss_x loss_x 0.5918 (0.9927) acc_x 84.3750 (74.3750) lr 1.7181e-03 eta 0:00:18
epoch [51/200] batch [10/47] time 0.501 (0.485) data 0.371 (0.355) loss_x loss_x 1.0732 (1.1261) acc_x 71.8750 (71.5625) lr 1.7181e-03 eta 0:00:17
epoch [51/200] batch [15/47] time 0.446 (0.473) data 0.316 (0.343) loss_x loss_x 1.1445 (1.1662) acc_x 59.3750 (70.8333) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [20/47] time 0.521 (0.471) data 0.390 (0.341) loss_x loss_x 1.3672 (1.1909) acc_x 62.5000 (70.3125) lr 1.7181e-03 eta 0:00:12
epoch [51/200] batch [25/47] time 0.361 (0.461) data 0.231 (0.331) loss_x loss_x 1.0322 (1.1481) acc_x 71.8750 (71.6250) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [30/47] time 0.441 (0.454) data 0.311 (0.324) loss_x loss_x 1.1250 (1.1815) acc_x 75.0000 (70.6250) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [35/47] time 0.640 (0.458) data 0.510 (0.328) loss_x loss_x 0.9141 (1.2026) acc_x 78.1250 (70.0893) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [40/47] time 0.443 (0.457) data 0.313 (0.327) loss_x loss_x 1.2705 (1.2139) acc_x 71.8750 (69.0625) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [45/47] time 0.445 (0.454) data 0.314 (0.324) loss_x loss_x 1.6611 (1.2146) acc_x 59.3750 (68.6806) lr 1.7181e-03 eta 0:00:00
epoch [51/200] batch [5/50] time 0.450 (0.451) data 0.320 (0.321) loss_u loss_u 0.8711 (0.8032) acc_u 12.5000 (25.0000) lr 1.7181e-03 eta 0:00:20
epoch [51/200] batch [10/50] time 0.387 (0.450) data 0.257 (0.320) loss_u loss_u 0.7290 (0.7923) acc_u 34.3750 (26.8750) lr 1.7181e-03 eta 0:00:17
epoch [51/200] batch [15/50] time 0.539 (0.447) data 0.409 (0.318) loss_u loss_u 0.7363 (0.7796) acc_u 34.3750 (28.5417) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [20/50] time 0.387 (0.445) data 0.258 (0.315) loss_u loss_u 0.8628 (0.7850) acc_u 12.5000 (27.8125) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [25/50] time 0.428 (0.447) data 0.298 (0.317) loss_u loss_u 0.8003 (0.7846) acc_u 31.2500 (28.3750) lr 1.7181e-03 eta 0:00:11
epoch [51/200] batch [30/50] time 0.380 (0.444) data 0.251 (0.314) loss_u loss_u 0.7793 (0.7837) acc_u 21.8750 (28.7500) lr 1.7181e-03 eta 0:00:08
epoch [51/200] batch [35/50] time 0.378 (0.446) data 0.247 (0.316) loss_u loss_u 0.7285 (0.7793) acc_u 43.7500 (29.2857) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [40/50] time 0.325 (0.445) data 0.194 (0.315) loss_u loss_u 0.8228 (0.7818) acc_u 18.7500 (28.9844) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [45/50] time 0.424 (0.444) data 0.294 (0.314) loss_u loss_u 0.8257 (0.7813) acc_u 21.8750 (29.2361) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [50/50] time 0.352 (0.446) data 0.221 (0.316) loss_u loss_u 0.7026 (0.7814) acc_u 34.3750 (28.9375) lr 1.7181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1515
confident_label rate tensor(0.4688, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1470
clean true:1455
clean false:15
clean_rate:0.9897959183673469
noisy true:166
noisy false:1500
after delete: len(clean_dataset) 1470
after delete: len(noisy_dataset) 1666
epoch [52/200] batch [5/45] time 0.405 (0.443) data 0.274 (0.313) loss_x loss_x 1.0625 (1.1802) acc_x 62.5000 (71.2500) lr 1.7071e-03 eta 0:00:17
epoch [52/200] batch [10/45] time 0.470 (0.458) data 0.340 (0.328) loss_x loss_x 1.1338 (1.1233) acc_x 68.7500 (70.9375) lr 1.7071e-03 eta 0:00:16
epoch [52/200] batch [15/45] time 0.385 (0.439) data 0.255 (0.309) loss_x loss_x 1.2920 (1.1525) acc_x 65.6250 (69.3750) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [20/45] time 0.490 (0.449) data 0.360 (0.319) loss_x loss_x 1.1738 (1.1430) acc_x 71.8750 (70.0000) lr 1.7071e-03 eta 0:00:11
epoch [52/200] batch [25/45] time 0.486 (0.454) data 0.355 (0.324) loss_x loss_x 1.0332 (1.1876) acc_x 71.8750 (69.1250) lr 1.7071e-03 eta 0:00:09
epoch [52/200] batch [30/45] time 0.402 (0.451) data 0.272 (0.321) loss_x loss_x 1.9619 (1.1896) acc_x 62.5000 (69.4792) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [35/45] time 0.439 (0.451) data 0.309 (0.321) loss_x loss_x 1.1729 (1.1898) acc_x 68.7500 (69.4643) lr 1.7071e-03 eta 0:00:04
epoch [52/200] batch [40/45] time 0.459 (0.454) data 0.329 (0.324) loss_x loss_x 0.9531 (1.1926) acc_x 78.1250 (69.2188) lr 1.7071e-03 eta 0:00:02
epoch [52/200] batch [45/45] time 0.385 (0.453) data 0.254 (0.323) loss_x loss_x 1.4395 (1.2022) acc_x 62.5000 (68.4722) lr 1.7071e-03 eta 0:00:00
epoch [52/200] batch [5/52] time 0.415 (0.450) data 0.284 (0.320) loss_u loss_u 0.7319 (0.7908) acc_u 34.3750 (24.3750) lr 1.7071e-03 eta 0:00:21
epoch [52/200] batch [10/52] time 0.399 (0.448) data 0.268 (0.318) loss_u loss_u 0.6772 (0.7661) acc_u 37.5000 (29.6875) lr 1.7071e-03 eta 0:00:18
epoch [52/200] batch [15/52] time 0.592 (0.446) data 0.461 (0.316) loss_u loss_u 0.8052 (0.7749) acc_u 28.1250 (29.7917) lr 1.7071e-03 eta 0:00:16
epoch [52/200] batch [20/52] time 0.693 (0.451) data 0.562 (0.320) loss_u loss_u 0.7549 (0.7757) acc_u 25.0000 (29.0625) lr 1.7071e-03 eta 0:00:14
epoch [52/200] batch [25/52] time 0.361 (0.449) data 0.230 (0.319) loss_u loss_u 0.7783 (0.7783) acc_u 28.1250 (28.5000) lr 1.7071e-03 eta 0:00:12
epoch [52/200] batch [30/52] time 0.552 (0.452) data 0.420 (0.321) loss_u loss_u 0.8350 (0.7786) acc_u 15.6250 (28.4375) lr 1.7071e-03 eta 0:00:09
epoch [52/200] batch [35/52] time 0.377 (0.452) data 0.246 (0.321) loss_u loss_u 0.6606 (0.7724) acc_u 46.8750 (29.3750) lr 1.7071e-03 eta 0:00:07
epoch [52/200] batch [40/52] time 0.433 (0.449) data 0.302 (0.319) loss_u loss_u 0.6646 (0.7655) acc_u 50.0000 (29.9219) lr 1.7071e-03 eta 0:00:05
epoch [52/200] batch [45/52] time 0.550 (0.447) data 0.418 (0.317) loss_u loss_u 0.7974 (0.7643) acc_u 28.1250 (30.1389) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [50/52] time 0.483 (0.446) data 0.351 (0.316) loss_u loss_u 0.8218 (0.7679) acc_u 18.7500 (29.5000) lr 1.7071e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1477
confident_label rate tensor(0.4774, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1497
clean true:1481
clean false:16
clean_rate:0.9893119572478289
noisy true:178
noisy false:1461
after delete: len(clean_dataset) 1497
after delete: len(noisy_dataset) 1639
epoch [53/200] batch [5/46] time 0.481 (0.451) data 0.351 (0.320) loss_x loss_x 1.2881 (1.2432) acc_x 65.6250 (70.6250) lr 1.6959e-03 eta 0:00:18
epoch [53/200] batch [10/46] time 0.506 (0.477) data 0.373 (0.346) loss_x loss_x 1.1680 (1.1842) acc_x 71.8750 (70.9375) lr 1.6959e-03 eta 0:00:17
epoch [53/200] batch [15/46] time 0.455 (0.474) data 0.325 (0.343) loss_x loss_x 1.3125 (1.2048) acc_x 71.8750 (69.5833) lr 1.6959e-03 eta 0:00:14
epoch [53/200] batch [20/46] time 0.410 (0.459) data 0.280 (0.328) loss_x loss_x 1.0771 (1.2466) acc_x 71.8750 (69.2188) lr 1.6959e-03 eta 0:00:11
epoch [53/200] batch [25/46] time 0.388 (0.453) data 0.259 (0.323) loss_x loss_x 0.9004 (1.2302) acc_x 68.7500 (69.1250) lr 1.6959e-03 eta 0:00:09
epoch [53/200] batch [30/46] time 0.464 (0.447) data 0.334 (0.317) loss_x loss_x 1.3789 (1.2334) acc_x 68.7500 (68.9583) lr 1.6959e-03 eta 0:00:07
epoch [53/200] batch [35/46] time 0.360 (0.451) data 0.231 (0.321) loss_x loss_x 1.0410 (1.2241) acc_x 68.7500 (68.5714) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [40/46] time 0.412 (0.450) data 0.282 (0.320) loss_x loss_x 1.3457 (1.2194) acc_x 68.7500 (69.0625) lr 1.6959e-03 eta 0:00:02
epoch [53/200] batch [45/46] time 0.471 (0.456) data 0.341 (0.326) loss_x loss_x 0.8398 (1.2130) acc_x 71.8750 (69.2361) lr 1.6959e-03 eta 0:00:00
epoch [53/200] batch [5/51] time 0.538 (0.460) data 0.409 (0.330) loss_u loss_u 0.8027 (0.7757) acc_u 21.8750 (27.5000) lr 1.6959e-03 eta 0:00:21
epoch [53/200] batch [10/51] time 0.478 (0.464) data 0.348 (0.334) loss_u loss_u 0.7817 (0.7794) acc_u 31.2500 (26.2500) lr 1.6959e-03 eta 0:00:19
epoch [53/200] batch [15/51] time 0.351 (0.463) data 0.220 (0.333) loss_u loss_u 0.8643 (0.7773) acc_u 18.7500 (27.2917) lr 1.6959e-03 eta 0:00:16
epoch [53/200] batch [20/51] time 0.375 (0.463) data 0.244 (0.333) loss_u loss_u 0.7397 (0.7755) acc_u 34.3750 (28.1250) lr 1.6959e-03 eta 0:00:14
epoch [53/200] batch [25/51] time 0.426 (0.460) data 0.295 (0.330) loss_u loss_u 0.8105 (0.7766) acc_u 25.0000 (28.2500) lr 1.6959e-03 eta 0:00:11
epoch [53/200] batch [30/51] time 0.444 (0.455) data 0.313 (0.325) loss_u loss_u 0.8193 (0.7801) acc_u 21.8750 (27.9167) lr 1.6959e-03 eta 0:00:09
epoch [53/200] batch [35/51] time 0.404 (0.452) data 0.273 (0.322) loss_u loss_u 0.8267 (0.7850) acc_u 18.7500 (27.2321) lr 1.6959e-03 eta 0:00:07
epoch [53/200] batch [40/51] time 0.420 (0.450) data 0.286 (0.320) loss_u loss_u 0.7729 (0.7862) acc_u 31.2500 (27.4219) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [45/51] time 0.492 (0.448) data 0.361 (0.318) loss_u loss_u 0.7446 (0.7891) acc_u 31.2500 (26.8056) lr 1.6959e-03 eta 0:00:02
epoch [53/200] batch [50/51] time 0.424 (0.444) data 0.292 (0.313) loss_u loss_u 0.7974 (0.7905) acc_u 25.0000 (26.9375) lr 1.6959e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1483
confident_label rate tensor(0.4805, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1507
clean true:1489
clean false:18
clean_rate:0.9880557398805574
noisy true:164
noisy false:1465
after delete: len(clean_dataset) 1507
after delete: len(noisy_dataset) 1629
epoch [54/200] batch [5/47] time 0.463 (0.425) data 0.333 (0.294) loss_x loss_x 0.9277 (1.3289) acc_x 75.0000 (62.5000) lr 1.6845e-03 eta 0:00:17
epoch [54/200] batch [10/47] time 0.427 (0.423) data 0.296 (0.293) loss_x loss_x 1.1309 (1.2256) acc_x 65.6250 (67.1875) lr 1.6845e-03 eta 0:00:15
epoch [54/200] batch [15/47] time 0.520 (0.431) data 0.389 (0.301) loss_x loss_x 1.0459 (1.1821) acc_x 71.8750 (68.3333) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [20/47] time 0.396 (0.430) data 0.265 (0.300) loss_x loss_x 1.5283 (1.2403) acc_x 62.5000 (67.8125) lr 1.6845e-03 eta 0:00:11
epoch [54/200] batch [25/47] time 0.497 (0.436) data 0.366 (0.306) loss_x loss_x 1.1699 (1.1929) acc_x 68.7500 (68.7500) lr 1.6845e-03 eta 0:00:09
epoch [54/200] batch [30/47] time 0.479 (0.434) data 0.349 (0.303) loss_x loss_x 1.0625 (1.2068) acc_x 68.7500 (68.2292) lr 1.6845e-03 eta 0:00:07
epoch [54/200] batch [35/47] time 0.402 (0.438) data 0.272 (0.308) loss_x loss_x 0.9038 (1.2072) acc_x 71.8750 (68.0357) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [40/47] time 0.402 (0.441) data 0.272 (0.311) loss_x loss_x 1.1836 (1.2056) acc_x 68.7500 (68.0469) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [45/47] time 0.411 (0.445) data 0.280 (0.314) loss_x loss_x 1.1768 (1.2068) acc_x 56.2500 (67.5694) lr 1.6845e-03 eta 0:00:00
epoch [54/200] batch [5/50] time 0.423 (0.445) data 0.292 (0.315) loss_u loss_u 0.7637 (0.7812) acc_u 34.3750 (28.7500) lr 1.6845e-03 eta 0:00:20
epoch [54/200] batch [10/50] time 0.447 (0.446) data 0.316 (0.315) loss_u loss_u 0.8501 (0.7856) acc_u 12.5000 (28.1250) lr 1.6845e-03 eta 0:00:17
epoch [54/200] batch [15/50] time 0.398 (0.446) data 0.267 (0.315) loss_u loss_u 0.7588 (0.7877) acc_u 28.1250 (27.2917) lr 1.6845e-03 eta 0:00:15
epoch [54/200] batch [20/50] time 0.367 (0.446) data 0.237 (0.316) loss_u loss_u 0.8076 (0.7889) acc_u 25.0000 (27.8125) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [25/50] time 0.404 (0.444) data 0.273 (0.313) loss_u loss_u 0.7622 (0.7845) acc_u 31.2500 (27.8750) lr 1.6845e-03 eta 0:00:11
epoch [54/200] batch [30/50] time 0.506 (0.442) data 0.375 (0.312) loss_u loss_u 0.7593 (0.7806) acc_u 28.1250 (28.6458) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [35/50] time 0.462 (0.441) data 0.331 (0.310) loss_u loss_u 0.7417 (0.7755) acc_u 34.3750 (29.4643) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [40/50] time 0.439 (0.439) data 0.307 (0.308) loss_u loss_u 0.8281 (0.7760) acc_u 18.7500 (29.3750) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [45/50] time 0.380 (0.443) data 0.249 (0.312) loss_u loss_u 0.7690 (0.7752) acc_u 25.0000 (29.4444) lr 1.6845e-03 eta 0:00:02
epoch [54/200] batch [50/50] time 0.375 (0.441) data 0.244 (0.310) loss_u loss_u 0.8164 (0.7763) acc_u 21.8750 (29.5000) lr 1.6845e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1466
confident_label rate tensor(0.4857, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1523
clean true:1504
clean false:19
clean_rate:0.9875246224556796
noisy true:166
noisy false:1447
after delete: len(clean_dataset) 1523
after delete: len(noisy_dataset) 1613
epoch [55/200] batch [5/47] time 0.548 (0.510) data 0.418 (0.379) loss_x loss_x 1.3105 (1.3020) acc_x 68.7500 (68.1250) lr 1.6730e-03 eta 0:00:21
epoch [55/200] batch [10/47] time 0.451 (0.499) data 0.320 (0.368) loss_x loss_x 1.1504 (1.2548) acc_x 68.7500 (70.6250) lr 1.6730e-03 eta 0:00:18
epoch [55/200] batch [15/47] time 0.510 (0.484) data 0.379 (0.354) loss_x loss_x 1.1348 (1.1764) acc_x 75.0000 (72.7083) lr 1.6730e-03 eta 0:00:15
epoch [55/200] batch [20/47] time 0.395 (0.473) data 0.264 (0.342) loss_x loss_x 1.4072 (1.2498) acc_x 59.3750 (69.2188) lr 1.6730e-03 eta 0:00:12
epoch [55/200] batch [25/47] time 0.396 (0.474) data 0.266 (0.343) loss_x loss_x 1.7559 (1.2726) acc_x 62.5000 (68.1250) lr 1.6730e-03 eta 0:00:10
epoch [55/200] batch [30/47] time 0.489 (0.471) data 0.358 (0.340) loss_x loss_x 1.2412 (1.2818) acc_x 62.5000 (67.5000) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [35/47] time 0.480 (0.476) data 0.350 (0.345) loss_x loss_x 1.0352 (1.2698) acc_x 71.8750 (67.8571) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [40/47] time 0.413 (0.470) data 0.282 (0.340) loss_x loss_x 1.4385 (1.2498) acc_x 56.2500 (68.1250) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [45/47] time 0.522 (0.468) data 0.391 (0.337) loss_x loss_x 1.3281 (1.2341) acc_x 62.5000 (68.7500) lr 1.6730e-03 eta 0:00:00
epoch [55/200] batch [5/50] time 0.359 (0.465) data 0.229 (0.334) loss_u loss_u 0.8481 (0.8114) acc_u 15.6250 (20.6250) lr 1.6730e-03 eta 0:00:20
epoch [55/200] batch [10/50] time 0.404 (0.459) data 0.274 (0.328) loss_u loss_u 0.7197 (0.7830) acc_u 34.3750 (26.5625) lr 1.6730e-03 eta 0:00:18
epoch [55/200] batch [15/50] time 0.372 (0.458) data 0.242 (0.327) loss_u loss_u 0.8545 (0.7962) acc_u 12.5000 (23.9583) lr 1.6730e-03 eta 0:00:16
epoch [55/200] batch [20/50] time 0.400 (0.452) data 0.270 (0.321) loss_u loss_u 0.7578 (0.7893) acc_u 37.5000 (26.4062) lr 1.6730e-03 eta 0:00:13
epoch [55/200] batch [25/50] time 0.410 (0.451) data 0.279 (0.321) loss_u loss_u 0.7432 (0.7842) acc_u 31.2500 (26.7500) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [30/50] time 0.470 (0.449) data 0.340 (0.319) loss_u loss_u 0.7793 (0.7840) acc_u 28.1250 (26.6667) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [35/50] time 0.655 (0.451) data 0.524 (0.321) loss_u loss_u 0.8613 (0.7888) acc_u 15.6250 (26.1607) lr 1.6730e-03 eta 0:00:06
epoch [55/200] batch [40/50] time 0.406 (0.448) data 0.274 (0.318) loss_u loss_u 0.8291 (0.7890) acc_u 25.0000 (26.2500) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [45/50] time 0.556 (0.449) data 0.425 (0.318) loss_u loss_u 0.7671 (0.7810) acc_u 28.1250 (27.2917) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [50/50] time 0.385 (0.447) data 0.254 (0.317) loss_u loss_u 0.7729 (0.7798) acc_u 28.1250 (27.6875) lr 1.6730e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1456
confident_label rate tensor(0.4847, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1520
clean true:1502
clean false:18
clean_rate:0.9881578947368421
noisy true:178
noisy false:1438
after delete: len(clean_dataset) 1520
after delete: len(noisy_dataset) 1616
epoch [56/200] batch [5/47] time 0.464 (0.422) data 0.334 (0.292) loss_x loss_x 1.3115 (1.3555) acc_x 62.5000 (65.0000) lr 1.6613e-03 eta 0:00:17
epoch [56/200] batch [10/47] time 0.429 (0.431) data 0.300 (0.302) loss_x loss_x 1.1523 (1.3015) acc_x 71.8750 (64.6875) lr 1.6613e-03 eta 0:00:15
epoch [56/200] batch [15/47] time 0.426 (0.448) data 0.296 (0.318) loss_x loss_x 1.3271 (1.2812) acc_x 68.7500 (66.6667) lr 1.6613e-03 eta 0:00:14
epoch [56/200] batch [20/47] time 0.371 (0.449) data 0.240 (0.319) loss_x loss_x 1.3301 (1.2165) acc_x 68.7500 (68.2812) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [25/47] time 0.395 (0.441) data 0.265 (0.311) loss_x loss_x 0.9355 (1.2179) acc_x 81.2500 (68.1250) lr 1.6613e-03 eta 0:00:09
epoch [56/200] batch [30/47] time 0.477 (0.448) data 0.347 (0.318) loss_x loss_x 1.1475 (1.2027) acc_x 68.7500 (68.2292) lr 1.6613e-03 eta 0:00:07
epoch [56/200] batch [35/47] time 0.596 (0.455) data 0.466 (0.325) loss_x loss_x 1.0225 (1.2315) acc_x 75.0000 (67.3214) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [40/47] time 0.421 (0.465) data 0.291 (0.334) loss_x loss_x 0.8838 (1.2121) acc_x 68.7500 (67.6562) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [45/47] time 0.375 (0.459) data 0.245 (0.329) loss_x loss_x 1.3281 (1.2187) acc_x 68.7500 (67.8472) lr 1.6613e-03 eta 0:00:00
epoch [56/200] batch [5/50] time 0.495 (0.453) data 0.364 (0.323) loss_u loss_u 0.7705 (0.7555) acc_u 28.1250 (29.3750) lr 1.6613e-03 eta 0:00:20
epoch [56/200] batch [10/50] time 0.370 (0.452) data 0.239 (0.321) loss_u loss_u 0.7407 (0.7393) acc_u 28.1250 (32.1875) lr 1.6613e-03 eta 0:00:18
epoch [56/200] batch [15/50] time 0.339 (0.449) data 0.207 (0.319) loss_u loss_u 0.7412 (0.7547) acc_u 31.2500 (29.7917) lr 1.6613e-03 eta 0:00:15
epoch [56/200] batch [20/50] time 0.412 (0.448) data 0.281 (0.318) loss_u loss_u 0.7920 (0.7567) acc_u 28.1250 (29.8438) lr 1.6613e-03 eta 0:00:13
epoch [56/200] batch [25/50] time 0.314 (0.446) data 0.183 (0.315) loss_u loss_u 0.7725 (0.7652) acc_u 28.1250 (28.7500) lr 1.6613e-03 eta 0:00:11
epoch [56/200] batch [30/50] time 0.522 (0.450) data 0.391 (0.319) loss_u loss_u 0.7188 (0.7683) acc_u 37.5000 (28.6458) lr 1.6613e-03 eta 0:00:08
epoch [56/200] batch [35/50] time 0.426 (0.446) data 0.295 (0.315) loss_u loss_u 0.8247 (0.7738) acc_u 25.0000 (28.1250) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [40/50] time 0.356 (0.443) data 0.225 (0.313) loss_u loss_u 0.8164 (0.7687) acc_u 28.1250 (28.6719) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [45/50] time 0.385 (0.440) data 0.254 (0.310) loss_u loss_u 0.6831 (0.7651) acc_u 40.6250 (29.0278) lr 1.6613e-03 eta 0:00:02
epoch [56/200] batch [50/50] time 0.465 (0.439) data 0.335 (0.309) loss_u loss_u 0.8555 (0.7654) acc_u 12.5000 (29.3125) lr 1.6613e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1461
confident_label rate tensor(0.4831, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1515
clean true:1500
clean false:15
clean_rate:0.9900990099009901
noisy true:175
noisy false:1446
after delete: len(clean_dataset) 1515
after delete: len(noisy_dataset) 1621
epoch [57/200] batch [5/47] time 0.468 (0.438) data 0.337 (0.307) loss_x loss_x 1.1719 (1.0293) acc_x 68.7500 (73.7500) lr 1.6494e-03 eta 0:00:18
epoch [57/200] batch [10/47] time 0.494 (0.503) data 0.364 (0.373) loss_x loss_x 1.3076 (1.1226) acc_x 71.8750 (70.9375) lr 1.6494e-03 eta 0:00:18
epoch [57/200] batch [15/47] time 0.511 (0.490) data 0.381 (0.360) loss_x loss_x 0.9321 (1.1632) acc_x 71.8750 (70.2083) lr 1.6494e-03 eta 0:00:15
epoch [57/200] batch [20/47] time 0.380 (0.493) data 0.250 (0.363) loss_x loss_x 0.9106 (1.1463) acc_x 75.0000 (70.4688) lr 1.6494e-03 eta 0:00:13
epoch [57/200] batch [25/47] time 0.432 (0.488) data 0.302 (0.358) loss_x loss_x 1.2275 (1.1345) acc_x 68.7500 (70.7500) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [30/47] time 0.453 (0.485) data 0.322 (0.355) loss_x loss_x 1.2100 (1.1204) acc_x 59.3750 (71.3542) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [35/47] time 0.402 (0.473) data 0.272 (0.343) loss_x loss_x 1.4990 (1.1424) acc_x 71.8750 (70.7143) lr 1.6494e-03 eta 0:00:05
epoch [57/200] batch [40/47] time 0.454 (0.474) data 0.323 (0.344) loss_x loss_x 1.3604 (1.1485) acc_x 62.5000 (70.5469) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [45/47] time 0.497 (0.468) data 0.366 (0.337) loss_x loss_x 1.0049 (1.1580) acc_x 75.0000 (70.1389) lr 1.6494e-03 eta 0:00:00
epoch [57/200] batch [5/50] time 0.298 (0.461) data 0.167 (0.331) loss_u loss_u 0.7700 (0.8103) acc_u 25.0000 (22.5000) lr 1.6494e-03 eta 0:00:20
epoch [57/200] batch [10/50] time 0.413 (0.455) data 0.282 (0.325) loss_u loss_u 0.8120 (0.7653) acc_u 18.7500 (28.7500) lr 1.6494e-03 eta 0:00:18
epoch [57/200] batch [15/50] time 0.471 (0.453) data 0.340 (0.323) loss_u loss_u 0.7979 (0.7859) acc_u 37.5000 (27.0833) lr 1.6494e-03 eta 0:00:15
epoch [57/200] batch [20/50] time 0.375 (0.455) data 0.244 (0.324) loss_u loss_u 0.7822 (0.7825) acc_u 25.0000 (28.4375) lr 1.6494e-03 eta 0:00:13
epoch [57/200] batch [25/50] time 0.434 (0.452) data 0.301 (0.322) loss_u loss_u 0.8501 (0.7793) acc_u 12.5000 (29.1250) lr 1.6494e-03 eta 0:00:11
epoch [57/200] batch [30/50] time 0.440 (0.451) data 0.309 (0.321) loss_u loss_u 0.7427 (0.7790) acc_u 31.2500 (29.3750) lr 1.6494e-03 eta 0:00:09
epoch [57/200] batch [35/50] time 0.369 (0.447) data 0.238 (0.316) loss_u loss_u 0.8462 (0.7788) acc_u 21.8750 (29.4643) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [40/50] time 0.427 (0.444) data 0.295 (0.313) loss_u loss_u 0.7622 (0.7793) acc_u 31.2500 (28.9844) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [45/50] time 0.415 (0.443) data 0.284 (0.312) loss_u loss_u 0.6909 (0.7820) acc_u 40.6250 (28.6111) lr 1.6494e-03 eta 0:00:02
epoch [57/200] batch [50/50] time 0.375 (0.442) data 0.244 (0.311) loss_u loss_u 0.7305 (0.7791) acc_u 28.1250 (28.8125) lr 1.6494e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1446
confident_label rate tensor(0.4841, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1518
clean true:1508
clean false:10
clean_rate:0.9934123847167325
noisy true:182
noisy false:1436
after delete: len(clean_dataset) 1518
after delete: len(noisy_dataset) 1618
epoch [58/200] batch [5/47] time 0.450 (0.518) data 0.320 (0.389) loss_x loss_x 1.1875 (1.2820) acc_x 78.1250 (73.1250) lr 1.6374e-03 eta 0:00:21
epoch [58/200] batch [10/47] time 0.373 (0.472) data 0.244 (0.342) loss_x loss_x 1.1621 (1.1809) acc_x 68.7500 (72.8125) lr 1.6374e-03 eta 0:00:17
epoch [58/200] batch [15/47] time 0.374 (0.465) data 0.245 (0.335) loss_x loss_x 1.6885 (1.1822) acc_x 62.5000 (71.8750) lr 1.6374e-03 eta 0:00:14
epoch [58/200] batch [20/47] time 0.455 (0.466) data 0.325 (0.336) loss_x loss_x 2.0000 (1.1985) acc_x 50.0000 (70.7812) lr 1.6374e-03 eta 0:00:12
epoch [58/200] batch [25/47] time 0.430 (0.460) data 0.300 (0.330) loss_x loss_x 1.2588 (1.2252) acc_x 59.3750 (69.6250) lr 1.6374e-03 eta 0:00:10
epoch [58/200] batch [30/47] time 0.378 (0.451) data 0.248 (0.321) loss_x loss_x 1.3330 (1.2278) acc_x 75.0000 (69.5833) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [35/47] time 0.427 (0.451) data 0.297 (0.321) loss_x loss_x 1.2285 (1.2420) acc_x 65.6250 (69.2857) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [40/47] time 0.361 (0.447) data 0.231 (0.317) loss_x loss_x 1.7646 (1.2701) acc_x 56.2500 (68.5156) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [45/47] time 0.410 (0.444) data 0.280 (0.314) loss_x loss_x 1.3652 (1.2575) acc_x 59.3750 (68.5417) lr 1.6374e-03 eta 0:00:00
epoch [58/200] batch [5/50] time 0.381 (0.445) data 0.251 (0.315) loss_u loss_u 0.8169 (0.7822) acc_u 28.1250 (27.5000) lr 1.6374e-03 eta 0:00:20
epoch [58/200] batch [10/50] time 0.559 (0.450) data 0.429 (0.320) loss_u loss_u 0.7949 (0.7662) acc_u 28.1250 (31.2500) lr 1.6374e-03 eta 0:00:18
epoch [58/200] batch [15/50] time 0.550 (0.448) data 0.419 (0.318) loss_u loss_u 0.7437 (0.7696) acc_u 34.3750 (30.2083) lr 1.6374e-03 eta 0:00:15
epoch [58/200] batch [20/50] time 0.394 (0.445) data 0.263 (0.315) loss_u loss_u 0.7769 (0.7698) acc_u 28.1250 (29.5312) lr 1.6374e-03 eta 0:00:13
epoch [58/200] batch [25/50] time 0.390 (0.443) data 0.259 (0.313) loss_u loss_u 0.7461 (0.7711) acc_u 31.2500 (29.7500) lr 1.6374e-03 eta 0:00:11
epoch [58/200] batch [30/50] time 0.457 (0.443) data 0.325 (0.312) loss_u loss_u 0.8291 (0.7743) acc_u 21.8750 (29.2708) lr 1.6374e-03 eta 0:00:08
epoch [58/200] batch [35/50] time 0.388 (0.440) data 0.258 (0.310) loss_u loss_u 0.8784 (0.7787) acc_u 9.3750 (28.3929) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [40/50] time 0.409 (0.437) data 0.279 (0.307) loss_u loss_u 0.8242 (0.7749) acc_u 21.8750 (28.8281) lr 1.6374e-03 eta 0:00:04
epoch [58/200] batch [45/50] time 0.429 (0.436) data 0.298 (0.305) loss_u loss_u 0.6978 (0.7692) acc_u 46.8750 (29.6528) lr 1.6374e-03 eta 0:00:02
epoch [58/200] batch [50/50] time 0.344 (0.434) data 0.214 (0.304) loss_u loss_u 0.7207 (0.7707) acc_u 34.3750 (29.2500) lr 1.6374e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1430
confident_label rate tensor(0.4959, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1555
clean true:1536
clean false:19
clean_rate:0.9877813504823151
noisy true:170
noisy false:1411
after delete: len(clean_dataset) 1555
after delete: len(noisy_dataset) 1581
epoch [59/200] batch [5/48] time 0.469 (0.437) data 0.339 (0.307) loss_x loss_x 1.3799 (1.2047) acc_x 71.8750 (70.0000) lr 1.6252e-03 eta 0:00:18
epoch [59/200] batch [10/48] time 0.533 (0.481) data 0.404 (0.351) loss_x loss_x 1.2207 (1.1600) acc_x 65.6250 (69.3750) lr 1.6252e-03 eta 0:00:18
epoch [59/200] batch [15/48] time 0.593 (0.500) data 0.463 (0.370) loss_x loss_x 0.7725 (1.1554) acc_x 78.1250 (69.3750) lr 1.6252e-03 eta 0:00:16
epoch [59/200] batch [20/48] time 0.597 (0.482) data 0.467 (0.352) loss_x loss_x 0.9712 (1.1548) acc_x 71.8750 (69.2188) lr 1.6252e-03 eta 0:00:13
epoch [59/200] batch [25/48] time 0.551 (0.487) data 0.421 (0.357) loss_x loss_x 1.4082 (1.1720) acc_x 62.5000 (68.7500) lr 1.6252e-03 eta 0:00:11
epoch [59/200] batch [30/48] time 0.487 (0.484) data 0.357 (0.354) loss_x loss_x 1.1924 (1.1777) acc_x 71.8750 (68.4375) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [35/48] time 0.507 (0.482) data 0.376 (0.352) loss_x loss_x 1.1162 (1.1964) acc_x 81.2500 (68.7500) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [40/48] time 0.422 (0.478) data 0.292 (0.347) loss_x loss_x 1.2822 (1.2094) acc_x 68.7500 (68.2031) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [45/48] time 0.364 (0.471) data 0.234 (0.340) loss_x loss_x 1.4707 (1.2350) acc_x 68.7500 (67.8472) lr 1.6252e-03 eta 0:00:01
epoch [59/200] batch [5/49] time 0.498 (0.464) data 0.367 (0.334) loss_u loss_u 0.8179 (0.7900) acc_u 18.7500 (28.1250) lr 1.6252e-03 eta 0:00:20
epoch [59/200] batch [10/49] time 0.312 (0.457) data 0.180 (0.326) loss_u loss_u 0.8384 (0.7925) acc_u 21.8750 (26.8750) lr 1.6252e-03 eta 0:00:17
epoch [59/200] batch [15/49] time 0.351 (0.456) data 0.220 (0.326) loss_u loss_u 0.6909 (0.7841) acc_u 37.5000 (26.8750) lr 1.6252e-03 eta 0:00:15
epoch [59/200] batch [20/49] time 0.381 (0.454) data 0.250 (0.323) loss_u loss_u 0.7886 (0.7902) acc_u 28.1250 (26.5625) lr 1.6252e-03 eta 0:00:13
epoch [59/200] batch [25/49] time 0.364 (0.451) data 0.233 (0.321) loss_u loss_u 0.7368 (0.7835) acc_u 34.3750 (27.5000) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [30/49] time 0.373 (0.451) data 0.242 (0.320) loss_u loss_u 0.8188 (0.7845) acc_u 18.7500 (27.2917) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [35/49] time 0.421 (0.448) data 0.290 (0.317) loss_u loss_u 0.7998 (0.7839) acc_u 28.1250 (27.4107) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [40/49] time 0.339 (0.442) data 0.208 (0.312) loss_u loss_u 0.7554 (0.7839) acc_u 31.2500 (27.3438) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [45/49] time 0.501 (0.441) data 0.370 (0.310) loss_u loss_u 0.7866 (0.7804) acc_u 21.8750 (27.5000) lr 1.6252e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1435
confident_label rate tensor(0.4885, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1532
clean true:1518
clean false:14
clean_rate:0.9908616187989556
noisy true:183
noisy false:1421
after delete: len(clean_dataset) 1532
after delete: len(noisy_dataset) 1604
epoch [60/200] batch [5/47] time 0.379 (0.406) data 0.249 (0.276) loss_x loss_x 1.2451 (1.1487) acc_x 68.7500 (68.7500) lr 1.6129e-03 eta 0:00:17
epoch [60/200] batch [10/47] time 0.461 (0.412) data 0.330 (0.281) loss_x loss_x 1.0840 (1.2001) acc_x 75.0000 (68.1250) lr 1.6129e-03 eta 0:00:15
epoch [60/200] batch [15/47] time 0.454 (0.427) data 0.325 (0.297) loss_x loss_x 0.9614 (1.2179) acc_x 78.1250 (68.7500) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [20/47] time 0.434 (0.437) data 0.304 (0.307) loss_x loss_x 0.8809 (1.2334) acc_x 84.3750 (68.9062) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [25/47] time 0.459 (0.445) data 0.329 (0.315) loss_x loss_x 0.8726 (1.2163) acc_x 75.0000 (69.0000) lr 1.6129e-03 eta 0:00:09
epoch [60/200] batch [30/47] time 0.407 (0.437) data 0.277 (0.307) loss_x loss_x 2.1777 (1.2579) acc_x 53.1250 (68.5417) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [35/47] time 0.493 (0.440) data 0.363 (0.309) loss_x loss_x 0.9292 (1.2484) acc_x 81.2500 (68.8393) lr 1.6129e-03 eta 0:00:05
epoch [60/200] batch [40/47] time 0.598 (0.443) data 0.468 (0.313) loss_x loss_x 1.5537 (1.2351) acc_x 68.7500 (69.0625) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [45/47] time 0.449 (0.444) data 0.318 (0.314) loss_x loss_x 1.2451 (1.2307) acc_x 65.6250 (68.6111) lr 1.6129e-03 eta 0:00:00
epoch [60/200] batch [5/50] time 0.346 (0.443) data 0.215 (0.312) loss_u loss_u 0.8604 (0.8162) acc_u 15.6250 (25.6250) lr 1.6129e-03 eta 0:00:19
epoch [60/200] batch [10/50] time 0.359 (0.444) data 0.227 (0.313) loss_u loss_u 0.8716 (0.8250) acc_u 12.5000 (23.1250) lr 1.6129e-03 eta 0:00:17
epoch [60/200] batch [15/50] time 0.436 (0.439) data 0.305 (0.309) loss_u loss_u 0.8110 (0.8105) acc_u 21.8750 (24.7917) lr 1.6129e-03 eta 0:00:15
epoch [60/200] batch [20/50] time 0.437 (0.437) data 0.306 (0.307) loss_u loss_u 0.7388 (0.7973) acc_u 31.2500 (25.7812) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [25/50] time 0.508 (0.440) data 0.377 (0.310) loss_u loss_u 0.7622 (0.7978) acc_u 28.1250 (25.2500) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [30/50] time 0.414 (0.445) data 0.283 (0.314) loss_u loss_u 0.7461 (0.7923) acc_u 31.2500 (26.0417) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [35/50] time 0.419 (0.446) data 0.288 (0.315) loss_u loss_u 0.7241 (0.7849) acc_u 31.2500 (27.1429) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [40/50] time 0.782 (0.450) data 0.653 (0.319) loss_u loss_u 0.7041 (0.7795) acc_u 40.6250 (27.5781) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [45/50] time 0.400 (0.450) data 0.269 (0.319) loss_u loss_u 0.8271 (0.7828) acc_u 21.8750 (27.0833) lr 1.6129e-03 eta 0:00:02
epoch [60/200] batch [50/50] time 0.367 (0.448) data 0.236 (0.317) loss_u loss_u 0.7720 (0.7841) acc_u 31.2500 (27.0625) lr 1.6129e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1500
confident_label rate tensor(0.4745, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1488
clean true:1470
clean false:18
clean_rate:0.9879032258064516
noisy true:166
noisy false:1482
after delete: len(clean_dataset) 1488
after delete: len(noisy_dataset) 1648
epoch [61/200] batch [5/46] time 0.462 (0.521) data 0.332 (0.391) loss_x loss_x 1.1250 (1.4237) acc_x 71.8750 (64.3750) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [10/46] time 0.365 (0.494) data 0.234 (0.363) loss_x loss_x 1.3008 (1.2440) acc_x 53.1250 (66.5625) lr 1.6004e-03 eta 0:00:17
epoch [61/200] batch [15/46] time 0.470 (0.486) data 0.340 (0.356) loss_x loss_x 1.4395 (1.2517) acc_x 62.5000 (67.9167) lr 1.6004e-03 eta 0:00:15
epoch [61/200] batch [20/46] time 0.517 (0.478) data 0.388 (0.348) loss_x loss_x 0.8730 (1.1900) acc_x 65.6250 (68.5938) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [25/46] time 0.419 (0.480) data 0.289 (0.350) loss_x loss_x 1.1406 (1.2103) acc_x 75.0000 (68.0000) lr 1.6004e-03 eta 0:00:10
epoch [61/200] batch [30/46] time 0.483 (0.471) data 0.353 (0.341) loss_x loss_x 1.0771 (1.2295) acc_x 65.6250 (67.9167) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [35/46] time 0.487 (0.466) data 0.357 (0.335) loss_x loss_x 1.1357 (1.2235) acc_x 75.0000 (68.2143) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [40/46] time 0.478 (0.467) data 0.346 (0.336) loss_x loss_x 0.9634 (1.2109) acc_x 81.2500 (68.6719) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [45/46] time 0.363 (0.459) data 0.233 (0.329) loss_x loss_x 0.9458 (1.2282) acc_x 81.2500 (68.4722) lr 1.6004e-03 eta 0:00:00
epoch [61/200] batch [5/51] time 0.685 (0.461) data 0.554 (0.330) loss_u loss_u 0.6836 (0.7437) acc_u 40.6250 (35.0000) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [10/51] time 0.455 (0.458) data 0.323 (0.327) loss_u loss_u 0.8423 (0.7494) acc_u 18.7500 (32.1875) lr 1.6004e-03 eta 0:00:18
epoch [61/200] batch [15/51] time 0.357 (0.454) data 0.228 (0.324) loss_u loss_u 0.7739 (0.7531) acc_u 28.1250 (31.2500) lr 1.6004e-03 eta 0:00:16
epoch [61/200] batch [20/51] time 0.403 (0.453) data 0.274 (0.322) loss_u loss_u 0.7715 (0.7572) acc_u 21.8750 (30.1562) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [25/51] time 0.688 (0.457) data 0.557 (0.327) loss_u loss_u 0.7568 (0.7542) acc_u 31.2500 (30.6250) lr 1.6004e-03 eta 0:00:11
epoch [61/200] batch [30/51] time 0.365 (0.454) data 0.234 (0.324) loss_u loss_u 0.7529 (0.7569) acc_u 25.0000 (30.9375) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [35/51] time 0.422 (0.452) data 0.291 (0.321) loss_u loss_u 0.8130 (0.7608) acc_u 25.0000 (30.7143) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [40/51] time 0.410 (0.449) data 0.279 (0.318) loss_u loss_u 0.7627 (0.7600) acc_u 25.0000 (30.4688) lr 1.6004e-03 eta 0:00:04
epoch [61/200] batch [45/51] time 0.334 (0.445) data 0.203 (0.314) loss_u loss_u 0.7539 (0.7571) acc_u 34.3750 (30.7639) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [50/51] time 0.368 (0.441) data 0.237 (0.311) loss_u loss_u 0.8335 (0.7574) acc_u 15.6250 (30.5625) lr 1.6004e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.4987, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1564
clean true:1546
clean false:18
clean_rate:0.9884910485933504
noisy true:170
noisy false:1402
after delete: len(clean_dataset) 1564
after delete: len(noisy_dataset) 1572
epoch [62/200] batch [5/48] time 0.486 (0.484) data 0.356 (0.354) loss_x loss_x 1.1445 (1.0810) acc_x 75.0000 (71.2500) lr 1.5878e-03 eta 0:00:20
epoch [62/200] batch [10/48] time 0.528 (0.452) data 0.397 (0.322) loss_x loss_x 0.8877 (1.1546) acc_x 81.2500 (70.6250) lr 1.5878e-03 eta 0:00:17
epoch [62/200] batch [15/48] time 0.517 (0.469) data 0.388 (0.339) loss_x loss_x 1.1289 (1.1432) acc_x 71.8750 (70.0000) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [20/48] time 0.455 (0.458) data 0.325 (0.328) loss_x loss_x 1.6025 (1.1771) acc_x 59.3750 (68.4375) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [25/48] time 0.462 (0.457) data 0.332 (0.327) loss_x loss_x 1.7285 (1.2053) acc_x 50.0000 (68.0000) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [30/48] time 0.411 (0.464) data 0.282 (0.334) loss_x loss_x 0.8989 (1.1781) acc_x 71.8750 (68.6458) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [35/48] time 0.453 (0.462) data 0.324 (0.332) loss_x loss_x 1.5488 (1.2009) acc_x 62.5000 (68.3036) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [40/48] time 0.433 (0.452) data 0.304 (0.322) loss_x loss_x 1.5498 (1.2166) acc_x 59.3750 (67.5781) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [45/48] time 0.526 (0.458) data 0.396 (0.328) loss_x loss_x 1.4238 (1.2313) acc_x 59.3750 (67.2917) lr 1.5878e-03 eta 0:00:01
epoch [62/200] batch [5/49] time 0.498 (0.460) data 0.368 (0.330) loss_u loss_u 0.7529 (0.7560) acc_u 25.0000 (30.6250) lr 1.5878e-03 eta 0:00:20
epoch [62/200] batch [10/49] time 0.366 (0.453) data 0.235 (0.323) loss_u loss_u 0.6899 (0.7417) acc_u 34.3750 (31.2500) lr 1.5878e-03 eta 0:00:17
epoch [62/200] batch [15/49] time 0.446 (0.451) data 0.315 (0.321) loss_u loss_u 0.7485 (0.7495) acc_u 28.1250 (29.7917) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [20/49] time 0.442 (0.447) data 0.312 (0.317) loss_u loss_u 0.6997 (0.7603) acc_u 43.7500 (29.3750) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [25/49] time 0.445 (0.445) data 0.314 (0.315) loss_u loss_u 0.8516 (0.7659) acc_u 21.8750 (28.6250) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [30/49] time 0.392 (0.443) data 0.261 (0.313) loss_u loss_u 0.7808 (0.7653) acc_u 31.2500 (29.0625) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [35/49] time 0.426 (0.444) data 0.295 (0.313) loss_u loss_u 0.8262 (0.7685) acc_u 21.8750 (29.0179) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [40/49] time 0.401 (0.443) data 0.270 (0.313) loss_u loss_u 0.7979 (0.7674) acc_u 25.0000 (29.0625) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [45/49] time 0.598 (0.446) data 0.467 (0.316) loss_u loss_u 0.8525 (0.7695) acc_u 25.0000 (28.8194) lr 1.5878e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1445
confident_label rate tensor(0.4885, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1532
clean true:1514
clean false:18
clean_rate:0.9882506527415144
noisy true:177
noisy false:1427
after delete: len(clean_dataset) 1532
after delete: len(noisy_dataset) 1604
epoch [63/200] batch [5/47] time 0.411 (0.488) data 0.281 (0.358) loss_x loss_x 1.6934 (1.2594) acc_x 56.2500 (70.0000) lr 1.5750e-03 eta 0:00:20
epoch [63/200] batch [10/47] time 0.380 (0.458) data 0.251 (0.328) loss_x loss_x 1.0195 (1.2646) acc_x 75.0000 (68.7500) lr 1.5750e-03 eta 0:00:16
epoch [63/200] batch [15/47] time 0.389 (0.445) data 0.259 (0.314) loss_x loss_x 1.0586 (1.1948) acc_x 71.8750 (69.3750) lr 1.5750e-03 eta 0:00:14
epoch [63/200] batch [20/47] time 0.460 (0.445) data 0.330 (0.315) loss_x loss_x 1.0801 (1.2361) acc_x 53.1250 (67.6562) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [25/47] time 0.408 (0.445) data 0.276 (0.315) loss_x loss_x 0.8745 (1.2221) acc_x 84.3750 (68.1250) lr 1.5750e-03 eta 0:00:09
epoch [63/200] batch [30/47] time 0.454 (0.459) data 0.324 (0.329) loss_x loss_x 1.1797 (1.1985) acc_x 68.7500 (69.0625) lr 1.5750e-03 eta 0:00:07
epoch [63/200] batch [35/47] time 0.402 (0.459) data 0.272 (0.329) loss_x loss_x 0.8120 (1.2160) acc_x 78.1250 (68.5714) lr 1.5750e-03 eta 0:00:05
epoch [63/200] batch [40/47] time 0.406 (0.457) data 0.276 (0.327) loss_x loss_x 1.3398 (1.2137) acc_x 65.6250 (68.3594) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [45/47] time 0.573 (0.459) data 0.443 (0.328) loss_x loss_x 1.1104 (1.2337) acc_x 59.3750 (67.9861) lr 1.5750e-03 eta 0:00:00
epoch [63/200] batch [5/50] time 0.533 (0.456) data 0.402 (0.326) loss_u loss_u 0.7524 (0.7687) acc_u 34.3750 (30.6250) lr 1.5750e-03 eta 0:00:20
epoch [63/200] batch [10/50] time 0.501 (0.455) data 0.370 (0.325) loss_u loss_u 0.7930 (0.7678) acc_u 34.3750 (31.5625) lr 1.5750e-03 eta 0:00:18
epoch [63/200] batch [15/50] time 0.397 (0.452) data 0.265 (0.321) loss_u loss_u 0.7876 (0.7788) acc_u 25.0000 (30.2083) lr 1.5750e-03 eta 0:00:15
epoch [63/200] batch [20/50] time 0.626 (0.452) data 0.495 (0.321) loss_u loss_u 0.7461 (0.7834) acc_u 40.6250 (29.6875) lr 1.5750e-03 eta 0:00:13
epoch [63/200] batch [25/50] time 0.370 (0.449) data 0.239 (0.318) loss_u loss_u 0.6934 (0.7844) acc_u 31.2500 (28.7500) lr 1.5750e-03 eta 0:00:11
epoch [63/200] batch [30/50] time 0.408 (0.447) data 0.277 (0.316) loss_u loss_u 0.7832 (0.7808) acc_u 28.1250 (28.7500) lr 1.5750e-03 eta 0:00:08
epoch [63/200] batch [35/50] time 0.420 (0.446) data 0.288 (0.315) loss_u loss_u 0.7451 (0.7759) acc_u 34.3750 (29.3750) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [40/50] time 0.417 (0.445) data 0.286 (0.315) loss_u loss_u 0.7568 (0.7808) acc_u 28.1250 (28.2812) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [45/50] time 0.459 (0.444) data 0.328 (0.313) loss_u loss_u 0.7354 (0.7785) acc_u 37.5000 (28.6111) lr 1.5750e-03 eta 0:00:02
epoch [63/200] batch [50/50] time 0.454 (0.440) data 0.322 (0.310) loss_u loss_u 0.7529 (0.7732) acc_u 34.3750 (29.1875) lr 1.5750e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.4933, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1547
clean true:1530
clean false:17
clean_rate:0.989010989010989
noisy true:165
noisy false:1424
after delete: len(clean_dataset) 1547
after delete: len(noisy_dataset) 1589
epoch [64/200] batch [5/48] time 0.442 (0.504) data 0.311 (0.373) loss_x loss_x 1.3262 (1.1355) acc_x 71.8750 (73.1250) lr 1.5621e-03 eta 0:00:21
epoch [64/200] batch [10/48] time 0.424 (0.454) data 0.294 (0.323) loss_x loss_x 1.0039 (1.0556) acc_x 68.7500 (74.6875) lr 1.5621e-03 eta 0:00:17
epoch [64/200] batch [15/48] time 0.428 (0.452) data 0.298 (0.321) loss_x loss_x 1.6455 (1.1498) acc_x 59.3750 (72.7083) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [20/48] time 0.493 (0.449) data 0.363 (0.318) loss_x loss_x 1.0381 (1.1286) acc_x 75.0000 (72.9688) lr 1.5621e-03 eta 0:00:12
epoch [64/200] batch [25/48] time 0.430 (0.458) data 0.300 (0.327) loss_x loss_x 1.2520 (1.1668) acc_x 68.7500 (71.3750) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [30/48] time 0.361 (0.450) data 0.231 (0.320) loss_x loss_x 1.4668 (1.1817) acc_x 65.6250 (70.8333) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [35/48] time 0.416 (0.451) data 0.286 (0.321) loss_x loss_x 1.4102 (1.1955) acc_x 68.7500 (70.1786) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [40/48] time 0.609 (0.454) data 0.479 (0.323) loss_x loss_x 1.0664 (1.1940) acc_x 75.0000 (69.9219) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [45/48] time 0.437 (0.455) data 0.307 (0.325) loss_x loss_x 1.3848 (1.1854) acc_x 68.7500 (70.1389) lr 1.5621e-03 eta 0:00:01
epoch [64/200] batch [5/49] time 0.418 (0.445) data 0.287 (0.315) loss_u loss_u 0.8652 (0.8355) acc_u 12.5000 (19.3750) lr 1.5621e-03 eta 0:00:19
epoch [64/200] batch [10/49] time 0.435 (0.451) data 0.302 (0.320) loss_u loss_u 0.7695 (0.7913) acc_u 34.3750 (25.6250) lr 1.5621e-03 eta 0:00:17
epoch [64/200] batch [15/49] time 0.509 (0.450) data 0.378 (0.320) loss_u loss_u 0.7729 (0.7886) acc_u 31.2500 (26.4583) lr 1.5621e-03 eta 0:00:15
epoch [64/200] batch [20/49] time 0.411 (0.450) data 0.280 (0.320) loss_u loss_u 0.7471 (0.7785) acc_u 37.5000 (28.7500) lr 1.5621e-03 eta 0:00:13
epoch [64/200] batch [25/49] time 0.428 (0.450) data 0.297 (0.320) loss_u loss_u 0.7852 (0.7715) acc_u 28.1250 (29.5000) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [30/49] time 0.402 (0.447) data 0.271 (0.316) loss_u loss_u 0.7568 (0.7684) acc_u 31.2500 (30.2083) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [35/49] time 0.422 (0.445) data 0.291 (0.314) loss_u loss_u 0.8350 (0.7703) acc_u 28.1250 (29.7321) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [40/49] time 0.359 (0.442) data 0.228 (0.312) loss_u loss_u 0.7485 (0.7692) acc_u 31.2500 (29.5312) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [45/49] time 0.377 (0.440) data 0.246 (0.310) loss_u loss_u 0.7915 (0.7708) acc_u 28.1250 (29.5833) lr 1.5621e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1431
confident_label rate tensor(0.4923, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1544
clean true:1524
clean false:20
clean_rate:0.9870466321243523
noisy true:181
noisy false:1411
after delete: len(clean_dataset) 1544
after delete: len(noisy_dataset) 1592
epoch [65/200] batch [5/48] time 0.430 (0.482) data 0.301 (0.352) loss_x loss_x 1.3125 (1.0895) acc_x 71.8750 (74.3750) lr 1.5490e-03 eta 0:00:20
epoch [65/200] batch [10/48] time 0.435 (0.448) data 0.304 (0.318) loss_x loss_x 0.8638 (1.0647) acc_x 78.1250 (72.8125) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [15/48] time 0.550 (0.452) data 0.417 (0.322) loss_x loss_x 1.5996 (1.1632) acc_x 68.7500 (71.8750) lr 1.5490e-03 eta 0:00:14
epoch [65/200] batch [20/48] time 0.369 (0.456) data 0.240 (0.325) loss_x loss_x 0.7993 (1.2000) acc_x 78.1250 (70.7812) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [25/48] time 0.450 (0.454) data 0.320 (0.324) loss_x loss_x 0.9663 (1.1959) acc_x 68.7500 (70.8750) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [30/48] time 0.330 (0.454) data 0.200 (0.324) loss_x loss_x 1.1631 (1.2271) acc_x 78.1250 (70.4167) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [35/48] time 0.453 (0.455) data 0.323 (0.325) loss_x loss_x 1.4238 (1.2345) acc_x 56.2500 (69.7321) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [40/48] time 0.432 (0.460) data 0.302 (0.330) loss_x loss_x 1.2266 (1.2276) acc_x 78.1250 (70.0781) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [45/48] time 0.323 (0.450) data 0.194 (0.320) loss_x loss_x 1.2393 (1.2219) acc_x 71.8750 (70.0694) lr 1.5490e-03 eta 0:00:01
epoch [65/200] batch [5/49] time 0.450 (0.450) data 0.320 (0.320) loss_u loss_u 0.9009 (0.7799) acc_u 9.3750 (28.7500) lr 1.5490e-03 eta 0:00:19
epoch [65/200] batch [10/49] time 0.402 (0.453) data 0.271 (0.323) loss_u loss_u 0.8818 (0.8031) acc_u 15.6250 (26.2500) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [15/49] time 0.357 (0.453) data 0.227 (0.323) loss_u loss_u 0.8047 (0.7967) acc_u 25.0000 (26.2500) lr 1.5490e-03 eta 0:00:15
epoch [65/200] batch [20/49] time 0.428 (0.448) data 0.297 (0.318) loss_u loss_u 0.8018 (0.8120) acc_u 25.0000 (23.7500) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [25/49] time 0.363 (0.444) data 0.232 (0.314) loss_u loss_u 0.8516 (0.8047) acc_u 18.7500 (24.8750) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [30/49] time 0.405 (0.443) data 0.274 (0.313) loss_u loss_u 0.6943 (0.8004) acc_u 43.7500 (25.9375) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [35/49] time 0.427 (0.441) data 0.297 (0.311) loss_u loss_u 0.7290 (0.7990) acc_u 37.5000 (26.2500) lr 1.5490e-03 eta 0:00:06
epoch [65/200] batch [40/49] time 0.387 (0.439) data 0.256 (0.309) loss_u loss_u 0.8164 (0.7922) acc_u 25.0000 (27.1875) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [45/49] time 0.398 (0.441) data 0.269 (0.311) loss_u loss_u 0.8018 (0.7879) acc_u 31.2500 (27.7778) lr 1.5490e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1429
confident_label rate tensor(0.4971, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1559
clean true:1539
clean false:20
clean_rate:0.9871712636305324
noisy true:168
noisy false:1409
after delete: len(clean_dataset) 1559
after delete: len(noisy_dataset) 1577
epoch [66/200] batch [5/48] time 0.433 (0.459) data 0.302 (0.328) loss_x loss_x 1.1328 (1.1847) acc_x 71.8750 (71.2500) lr 1.5358e-03 eta 0:00:19
epoch [66/200] batch [10/48] time 0.369 (0.427) data 0.239 (0.297) loss_x loss_x 1.2705 (1.2032) acc_x 71.8750 (70.6250) lr 1.5358e-03 eta 0:00:16
epoch [66/200] batch [15/48] time 0.383 (0.415) data 0.253 (0.285) loss_x loss_x 1.2588 (1.1615) acc_x 59.3750 (69.5833) lr 1.5358e-03 eta 0:00:13
epoch [66/200] batch [20/48] time 0.408 (0.417) data 0.278 (0.287) loss_x loss_x 1.2402 (1.1892) acc_x 71.8750 (69.5312) lr 1.5358e-03 eta 0:00:11
epoch [66/200] batch [25/48] time 0.333 (0.428) data 0.202 (0.298) loss_x loss_x 1.1973 (1.2005) acc_x 68.7500 (68.6250) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [30/48] time 0.364 (0.429) data 0.234 (0.299) loss_x loss_x 1.0039 (1.1926) acc_x 71.8750 (68.6458) lr 1.5358e-03 eta 0:00:07
epoch [66/200] batch [35/48] time 0.460 (0.443) data 0.330 (0.313) loss_x loss_x 0.7183 (1.1550) acc_x 81.2500 (69.4643) lr 1.5358e-03 eta 0:00:05
epoch [66/200] batch [40/48] time 0.474 (0.442) data 0.344 (0.312) loss_x loss_x 0.9438 (1.1730) acc_x 71.8750 (69.1406) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [45/48] time 0.410 (0.445) data 0.280 (0.315) loss_x loss_x 1.5400 (1.1921) acc_x 53.1250 (68.4028) lr 1.5358e-03 eta 0:00:01
epoch [66/200] batch [5/49] time 0.356 (0.441) data 0.225 (0.311) loss_u loss_u 0.7100 (0.7613) acc_u 43.7500 (30.0000) lr 1.5358e-03 eta 0:00:19
epoch [66/200] batch [10/49] time 0.515 (0.443) data 0.384 (0.313) loss_u loss_u 0.7451 (0.7726) acc_u 40.6250 (27.5000) lr 1.5358e-03 eta 0:00:17
epoch [66/200] batch [15/49] time 0.402 (0.442) data 0.271 (0.312) loss_u loss_u 0.7773 (0.7652) acc_u 31.2500 (28.9583) lr 1.5358e-03 eta 0:00:15
epoch [66/200] batch [20/49] time 0.370 (0.449) data 0.239 (0.319) loss_u loss_u 0.8716 (0.7777) acc_u 21.8750 (28.1250) lr 1.5358e-03 eta 0:00:13
epoch [66/200] batch [25/49] time 0.372 (0.446) data 0.241 (0.316) loss_u loss_u 0.7954 (0.7853) acc_u 25.0000 (27.1250) lr 1.5358e-03 eta 0:00:10
epoch [66/200] batch [30/49] time 0.438 (0.446) data 0.307 (0.315) loss_u loss_u 0.7485 (0.7871) acc_u 31.2500 (27.1875) lr 1.5358e-03 eta 0:00:08
epoch [66/200] batch [35/49] time 0.416 (0.444) data 0.284 (0.313) loss_u loss_u 0.8018 (0.7869) acc_u 25.0000 (26.7857) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [40/49] time 0.334 (0.442) data 0.203 (0.312) loss_u loss_u 0.7686 (0.7792) acc_u 31.2500 (28.1250) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [45/49] time 0.372 (0.440) data 0.241 (0.309) loss_u loss_u 0.7441 (0.7799) acc_u 28.1250 (27.8472) lr 1.5358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1430
confident_label rate tensor(0.4978, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1561
clean true:1544
clean false:17
clean_rate:0.9891095451633568
noisy true:162
noisy false:1413
after delete: len(clean_dataset) 1561
after delete: len(noisy_dataset) 1575
epoch [67/200] batch [5/48] time 0.532 (0.453) data 0.402 (0.323) loss_x loss_x 0.9839 (1.2343) acc_x 75.0000 (68.7500) lr 1.5225e-03 eta 0:00:19
epoch [67/200] batch [10/48] time 0.488 (0.473) data 0.358 (0.342) loss_x loss_x 1.2793 (1.1745) acc_x 71.8750 (72.1875) lr 1.5225e-03 eta 0:00:17
epoch [67/200] batch [15/48] time 0.400 (0.481) data 0.270 (0.351) loss_x loss_x 1.3545 (1.1779) acc_x 71.8750 (71.4583) lr 1.5225e-03 eta 0:00:15
epoch [67/200] batch [20/48] time 0.420 (0.474) data 0.289 (0.344) loss_x loss_x 1.5381 (1.2145) acc_x 62.5000 (70.7812) lr 1.5225e-03 eta 0:00:13
epoch [67/200] batch [25/48] time 0.421 (0.475) data 0.291 (0.345) loss_x loss_x 1.2070 (1.1950) acc_x 71.8750 (70.6250) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [30/48] time 0.395 (0.466) data 0.265 (0.335) loss_x loss_x 1.1992 (1.2205) acc_x 56.2500 (69.3750) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [35/48] time 0.483 (0.457) data 0.353 (0.327) loss_x loss_x 1.4766 (1.2269) acc_x 62.5000 (69.1964) lr 1.5225e-03 eta 0:00:05
epoch [67/200] batch [40/48] time 0.402 (0.461) data 0.271 (0.331) loss_x loss_x 1.4404 (1.1945) acc_x 59.3750 (69.5312) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [45/48] time 0.556 (0.464) data 0.425 (0.334) loss_x loss_x 1.4150 (1.2000) acc_x 65.6250 (69.4444) lr 1.5225e-03 eta 0:00:01
epoch [67/200] batch [5/49] time 0.400 (0.464) data 0.269 (0.334) loss_u loss_u 0.8228 (0.7738) acc_u 21.8750 (26.8750) lr 1.5225e-03 eta 0:00:20
epoch [67/200] batch [10/49] time 0.395 (0.460) data 0.265 (0.329) loss_u loss_u 0.8662 (0.7905) acc_u 21.8750 (27.1875) lr 1.5225e-03 eta 0:00:17
epoch [67/200] batch [15/49] time 0.368 (0.456) data 0.237 (0.326) loss_u loss_u 0.7935 (0.7882) acc_u 21.8750 (26.2500) lr 1.5225e-03 eta 0:00:15
epoch [67/200] batch [20/49] time 0.522 (0.458) data 0.392 (0.327) loss_u loss_u 0.7822 (0.7860) acc_u 25.0000 (26.8750) lr 1.5225e-03 eta 0:00:13
epoch [67/200] batch [25/49] time 0.365 (0.455) data 0.234 (0.325) loss_u loss_u 0.8164 (0.7795) acc_u 25.0000 (27.8750) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [30/49] time 0.770 (0.459) data 0.639 (0.329) loss_u loss_u 0.7769 (0.7764) acc_u 25.0000 (27.9167) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [35/49] time 0.356 (0.459) data 0.225 (0.328) loss_u loss_u 0.8159 (0.7750) acc_u 25.0000 (28.1250) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [40/49] time 0.433 (0.457) data 0.301 (0.326) loss_u loss_u 0.7212 (0.7686) acc_u 25.0000 (28.5938) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [45/49] time 0.374 (0.457) data 0.243 (0.327) loss_u loss_u 0.7202 (0.7701) acc_u 34.3750 (28.5417) lr 1.5225e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1450
confident_label rate tensor(0.4898, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1536
clean true:1519
clean false:17
clean_rate:0.9889322916666666
noisy true:167
noisy false:1433
after delete: len(clean_dataset) 1536
after delete: len(noisy_dataset) 1600
epoch [68/200] batch [5/48] time 0.352 (0.426) data 0.222 (0.295) loss_x loss_x 1.4229 (1.2975) acc_x 59.3750 (68.7500) lr 1.5090e-03 eta 0:00:18
epoch [68/200] batch [10/48] time 0.424 (0.410) data 0.293 (0.280) loss_x loss_x 1.1201 (1.2688) acc_x 81.2500 (71.2500) lr 1.5090e-03 eta 0:00:15
epoch [68/200] batch [15/48] time 0.648 (0.437) data 0.518 (0.307) loss_x loss_x 1.3945 (1.2704) acc_x 62.5000 (69.7917) lr 1.5090e-03 eta 0:00:14
epoch [68/200] batch [20/48] time 0.511 (0.439) data 0.381 (0.309) loss_x loss_x 0.7910 (1.2305) acc_x 81.2500 (70.6250) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [25/48] time 0.359 (0.443) data 0.229 (0.313) loss_x loss_x 1.0488 (1.1955) acc_x 68.7500 (71.3750) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [30/48] time 0.363 (0.443) data 0.233 (0.312) loss_x loss_x 1.0869 (1.2131) acc_x 68.7500 (70.5208) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [35/48] time 0.312 (0.442) data 0.182 (0.312) loss_x loss_x 1.3008 (1.2031) acc_x 75.0000 (71.0714) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [40/48] time 0.370 (0.443) data 0.239 (0.313) loss_x loss_x 1.6777 (1.2528) acc_x 50.0000 (70.0000) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [45/48] time 0.359 (0.446) data 0.228 (0.315) loss_x loss_x 0.8867 (1.2369) acc_x 68.7500 (70.0694) lr 1.5090e-03 eta 0:00:01
epoch [68/200] batch [5/50] time 0.355 (0.442) data 0.224 (0.311) loss_u loss_u 0.7275 (0.7698) acc_u 37.5000 (28.1250) lr 1.5090e-03 eta 0:00:19
epoch [68/200] batch [10/50] time 0.494 (0.438) data 0.363 (0.308) loss_u loss_u 0.8208 (0.7787) acc_u 21.8750 (27.5000) lr 1.5090e-03 eta 0:00:17
epoch [68/200] batch [15/50] time 0.414 (0.443) data 0.283 (0.312) loss_u loss_u 0.8447 (0.7839) acc_u 21.8750 (27.9167) lr 1.5090e-03 eta 0:00:15
epoch [68/200] batch [20/50] time 0.410 (0.440) data 0.279 (0.310) loss_u loss_u 0.6821 (0.7770) acc_u 40.6250 (28.5938) lr 1.5090e-03 eta 0:00:13
epoch [68/200] batch [25/50] time 0.330 (0.437) data 0.199 (0.307) loss_u loss_u 0.9277 (0.7851) acc_u 6.2500 (27.3750) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [30/50] time 0.411 (0.437) data 0.280 (0.306) loss_u loss_u 0.7891 (0.7844) acc_u 25.0000 (27.3958) lr 1.5090e-03 eta 0:00:08
epoch [68/200] batch [35/50] time 0.537 (0.440) data 0.405 (0.309) loss_u loss_u 0.8511 (0.7787) acc_u 18.7500 (28.0357) lr 1.5090e-03 eta 0:00:06
epoch [68/200] batch [40/50] time 0.350 (0.441) data 0.218 (0.311) loss_u loss_u 0.8149 (0.7795) acc_u 15.6250 (27.6562) lr 1.5090e-03 eta 0:00:04
epoch [68/200] batch [45/50] time 0.414 (0.443) data 0.283 (0.312) loss_u loss_u 0.7456 (0.7739) acc_u 34.3750 (28.3333) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [50/50] time 0.400 (0.440) data 0.269 (0.310) loss_u loss_u 0.8101 (0.7725) acc_u 25.0000 (28.5625) lr 1.5090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1418
confident_label rate tensor(0.4971, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1559
clean true:1542
clean false:17
clean_rate:0.9890955740859525
noisy true:176
noisy false:1401
after delete: len(clean_dataset) 1559
after delete: len(noisy_dataset) 1577
epoch [69/200] batch [5/48] time 0.452 (0.502) data 0.321 (0.372) loss_x loss_x 0.9004 (1.0940) acc_x 75.0000 (75.0000) lr 1.4955e-03 eta 0:00:21
epoch [69/200] batch [10/48] time 0.373 (0.459) data 0.243 (0.328) loss_x loss_x 1.7148 (1.2297) acc_x 62.5000 (70.6250) lr 1.4955e-03 eta 0:00:17
epoch [69/200] batch [15/48] time 0.482 (0.472) data 0.351 (0.342) loss_x loss_x 1.3066 (1.1965) acc_x 62.5000 (70.0000) lr 1.4955e-03 eta 0:00:15
epoch [69/200] batch [20/48] time 0.401 (0.468) data 0.270 (0.337) loss_x loss_x 1.1494 (1.2019) acc_x 75.0000 (70.0000) lr 1.4955e-03 eta 0:00:13
epoch [69/200] batch [25/48] time 0.391 (0.464) data 0.261 (0.334) loss_x loss_x 1.6777 (1.1904) acc_x 46.8750 (69.6250) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [30/48] time 0.389 (0.459) data 0.259 (0.328) loss_x loss_x 0.8159 (1.2118) acc_x 75.0000 (68.6458) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [35/48] time 0.534 (0.462) data 0.404 (0.331) loss_x loss_x 1.4180 (1.2156) acc_x 62.5000 (68.8393) lr 1.4955e-03 eta 0:00:06
epoch [69/200] batch [40/48] time 0.348 (0.457) data 0.217 (0.326) loss_x loss_x 1.5771 (1.2298) acc_x 56.2500 (68.6719) lr 1.4955e-03 eta 0:00:03
epoch [69/200] batch [45/48] time 0.469 (0.457) data 0.338 (0.326) loss_x loss_x 0.8765 (1.2256) acc_x 75.0000 (68.4028) lr 1.4955e-03 eta 0:00:01
epoch [69/200] batch [5/49] time 0.388 (0.456) data 0.257 (0.326) loss_u loss_u 0.7964 (0.7614) acc_u 21.8750 (33.1250) lr 1.4955e-03 eta 0:00:20
epoch [69/200] batch [10/49] time 0.365 (0.453) data 0.234 (0.322) loss_u loss_u 0.7568 (0.7518) acc_u 34.3750 (35.0000) lr 1.4955e-03 eta 0:00:17
epoch [69/200] batch [15/49] time 0.407 (0.447) data 0.276 (0.316) loss_u loss_u 0.7920 (0.7517) acc_u 25.0000 (33.3333) lr 1.4955e-03 eta 0:00:15
epoch [69/200] batch [20/49] time 0.472 (0.445) data 0.340 (0.314) loss_u loss_u 0.7876 (0.7562) acc_u 25.0000 (31.7188) lr 1.4955e-03 eta 0:00:12
epoch [69/200] batch [25/49] time 0.425 (0.442) data 0.294 (0.312) loss_u loss_u 0.7568 (0.7546) acc_u 31.2500 (31.6250) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [30/49] time 0.383 (0.441) data 0.252 (0.311) loss_u loss_u 0.7441 (0.7596) acc_u 37.5000 (31.2500) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [35/49] time 0.448 (0.439) data 0.316 (0.309) loss_u loss_u 0.7510 (0.7597) acc_u 40.6250 (31.2500) lr 1.4955e-03 eta 0:00:06
epoch [69/200] batch [40/49] time 0.445 (0.440) data 0.314 (0.309) loss_u loss_u 0.6816 (0.7614) acc_u 40.6250 (31.0156) lr 1.4955e-03 eta 0:00:03
epoch [69/200] batch [45/49] time 0.368 (0.441) data 0.238 (0.310) loss_u loss_u 0.8608 (0.7685) acc_u 15.6250 (29.5833) lr 1.4955e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1417
confident_label rate tensor(0.4974, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1560
clean true:1544
clean false:16
clean_rate:0.9897435897435898
noisy true:175
noisy false:1401
after delete: len(clean_dataset) 1560
after delete: len(noisy_dataset) 1576
epoch [70/200] batch [5/48] time 0.562 (0.495) data 0.432 (0.365) loss_x loss_x 1.1504 (1.2054) acc_x 75.0000 (71.8750) lr 1.4818e-03 eta 0:00:21
epoch [70/200] batch [10/48] time 0.405 (0.459) data 0.276 (0.330) loss_x loss_x 1.2939 (1.2720) acc_x 65.6250 (68.4375) lr 1.4818e-03 eta 0:00:17
epoch [70/200] batch [15/48] time 0.416 (0.442) data 0.287 (0.313) loss_x loss_x 0.9785 (1.2441) acc_x 75.0000 (69.1667) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [20/48] time 0.456 (0.449) data 0.327 (0.319) loss_x loss_x 1.4023 (1.2632) acc_x 75.0000 (69.5312) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [25/48] time 0.451 (0.449) data 0.322 (0.320) loss_x loss_x 1.2588 (1.2321) acc_x 59.3750 (69.7500) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [30/48] time 0.471 (0.447) data 0.342 (0.318) loss_x loss_x 0.5518 (1.2315) acc_x 93.7500 (69.7917) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [35/48] time 0.453 (0.455) data 0.324 (0.325) loss_x loss_x 1.1279 (1.2110) acc_x 62.5000 (70.0000) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [40/48] time 0.441 (0.455) data 0.311 (0.326) loss_x loss_x 0.9526 (1.2066) acc_x 75.0000 (69.8438) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [45/48] time 0.421 (0.455) data 0.290 (0.325) loss_x loss_x 1.0410 (1.1937) acc_x 62.5000 (70.1389) lr 1.4818e-03 eta 0:00:01
epoch [70/200] batch [5/49] time 0.311 (0.448) data 0.180 (0.318) loss_u loss_u 0.8428 (0.7697) acc_u 25.0000 (29.3750) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [10/49] time 0.332 (0.447) data 0.201 (0.317) loss_u loss_u 0.8115 (0.7806) acc_u 28.1250 (28.7500) lr 1.4818e-03 eta 0:00:17
epoch [70/200] batch [15/49] time 0.376 (0.442) data 0.245 (0.312) loss_u loss_u 0.7842 (0.7736) acc_u 25.0000 (29.7917) lr 1.4818e-03 eta 0:00:15
epoch [70/200] batch [20/49] time 0.396 (0.436) data 0.265 (0.306) loss_u loss_u 0.7539 (0.7696) acc_u 37.5000 (30.0000) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [25/49] time 0.408 (0.433) data 0.277 (0.303) loss_u loss_u 0.7231 (0.7690) acc_u 31.2500 (29.8750) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [30/49] time 0.374 (0.432) data 0.243 (0.302) loss_u loss_u 0.8511 (0.7722) acc_u 18.7500 (29.6875) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [35/49] time 0.422 (0.433) data 0.291 (0.303) loss_u loss_u 0.7754 (0.7742) acc_u 21.8750 (29.1071) lr 1.4818e-03 eta 0:00:06
epoch [70/200] batch [40/49] time 0.417 (0.436) data 0.286 (0.305) loss_u loss_u 0.7290 (0.7733) acc_u 40.6250 (29.1406) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [45/49] time 0.448 (0.435) data 0.317 (0.304) loss_u loss_u 0.7891 (0.7735) acc_u 31.2500 (29.0278) lr 1.4818e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1403
confident_label rate tensor(0.5035, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1579
clean true:1564
clean false:15
clean_rate:0.9905003166561115
noisy true:169
noisy false:1388
after delete: len(clean_dataset) 1579
after delete: len(noisy_dataset) 1557
epoch [71/200] batch [5/49] time 0.418 (0.456) data 0.288 (0.325) loss_x loss_x 1.3955 (1.3715) acc_x 75.0000 (68.1250) lr 1.4679e-03 eta 0:00:20
epoch [71/200] batch [10/49] time 0.391 (0.434) data 0.261 (0.304) loss_x loss_x 0.7305 (1.1950) acc_x 78.1250 (70.6250) lr 1.4679e-03 eta 0:00:16
epoch [71/200] batch [15/49] time 0.434 (0.431) data 0.304 (0.301) loss_x loss_x 1.3896 (1.2194) acc_x 68.7500 (69.7917) lr 1.4679e-03 eta 0:00:14
epoch [71/200] batch [20/49] time 0.469 (0.436) data 0.339 (0.306) loss_x loss_x 0.8091 (1.1899) acc_x 81.2500 (70.0000) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [25/49] time 0.536 (0.448) data 0.406 (0.318) loss_x loss_x 1.2373 (1.1746) acc_x 78.1250 (71.2500) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [30/49] time 0.419 (0.446) data 0.290 (0.316) loss_x loss_x 1.0801 (1.1669) acc_x 65.6250 (70.8333) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [35/49] time 0.341 (0.447) data 0.211 (0.317) loss_x loss_x 1.3359 (1.1606) acc_x 62.5000 (70.8929) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [40/49] time 0.501 (0.448) data 0.371 (0.318) loss_x loss_x 1.3408 (1.1550) acc_x 68.7500 (71.0938) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [45/49] time 0.410 (0.449) data 0.280 (0.319) loss_x loss_x 1.3086 (1.1451) acc_x 62.5000 (71.2500) lr 1.4679e-03 eta 0:00:01
epoch [71/200] batch [5/48] time 0.387 (0.448) data 0.256 (0.318) loss_u loss_u 0.7407 (0.7380) acc_u 28.1250 (32.5000) lr 1.4679e-03 eta 0:00:19
epoch [71/200] batch [10/48] time 0.534 (0.451) data 0.403 (0.321) loss_u loss_u 0.8145 (0.7711) acc_u 31.2500 (29.0625) lr 1.4679e-03 eta 0:00:17
epoch [71/200] batch [15/48] time 0.329 (0.452) data 0.198 (0.321) loss_u loss_u 0.7773 (0.7747) acc_u 31.2500 (28.7500) lr 1.4679e-03 eta 0:00:14
epoch [71/200] batch [20/48] time 0.449 (0.451) data 0.318 (0.321) loss_u loss_u 0.6953 (0.7729) acc_u 40.6250 (29.2188) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [25/48] time 0.390 (0.447) data 0.259 (0.316) loss_u loss_u 0.8228 (0.7693) acc_u 18.7500 (29.7500) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [30/48] time 0.376 (0.442) data 0.245 (0.312) loss_u loss_u 0.7095 (0.7681) acc_u 31.2500 (29.5833) lr 1.4679e-03 eta 0:00:07
epoch [71/200] batch [35/48] time 0.409 (0.438) data 0.278 (0.308) loss_u loss_u 0.8174 (0.7708) acc_u 18.7500 (28.4821) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [40/48] time 0.329 (0.435) data 0.198 (0.304) loss_u loss_u 0.8706 (0.7736) acc_u 15.6250 (28.3594) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [45/48] time 0.352 (0.434) data 0.221 (0.303) loss_u loss_u 0.7480 (0.7709) acc_u 43.7500 (28.8889) lr 1.4679e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1397
confident_label rate tensor(0.5010, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1571
clean true:1554
clean false:17
clean_rate:0.9891788669637174
noisy true:185
noisy false:1380
after delete: len(clean_dataset) 1571
after delete: len(noisy_dataset) 1565
epoch [72/200] batch [5/49] time 0.433 (0.450) data 0.302 (0.320) loss_x loss_x 1.0391 (1.1416) acc_x 68.7500 (73.1250) lr 1.4540e-03 eta 0:00:19
epoch [72/200] batch [10/49] time 0.509 (0.460) data 0.379 (0.330) loss_x loss_x 0.9634 (1.2526) acc_x 75.0000 (69.3750) lr 1.4540e-03 eta 0:00:17
epoch [72/200] batch [15/49] time 0.522 (0.460) data 0.391 (0.329) loss_x loss_x 0.9570 (1.1464) acc_x 75.0000 (71.6667) lr 1.4540e-03 eta 0:00:15
epoch [72/200] batch [20/49] time 0.339 (0.454) data 0.209 (0.324) loss_x loss_x 1.1064 (1.1344) acc_x 65.6250 (72.1875) lr 1.4540e-03 eta 0:00:13
epoch [72/200] batch [25/49] time 0.521 (0.450) data 0.390 (0.319) loss_x loss_x 1.7021 (1.1686) acc_x 62.5000 (70.6250) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [30/49] time 0.452 (0.443) data 0.321 (0.313) loss_x loss_x 1.1992 (1.1659) acc_x 71.8750 (70.9375) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [35/49] time 0.548 (0.449) data 0.418 (0.319) loss_x loss_x 1.6953 (1.1617) acc_x 53.1250 (70.6250) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [40/49] time 0.409 (0.451) data 0.278 (0.321) loss_x loss_x 1.2686 (1.1546) acc_x 71.8750 (71.1719) lr 1.4540e-03 eta 0:00:04
epoch [72/200] batch [45/49] time 0.400 (0.453) data 0.270 (0.323) loss_x loss_x 1.5684 (1.1594) acc_x 59.3750 (70.7639) lr 1.4540e-03 eta 0:00:01
epoch [72/200] batch [5/48] time 0.775 (0.454) data 0.642 (0.324) loss_u loss_u 0.7051 (0.7857) acc_u 37.5000 (30.0000) lr 1.4540e-03 eta 0:00:19
epoch [72/200] batch [10/48] time 0.499 (0.454) data 0.368 (0.323) loss_u loss_u 0.7983 (0.7759) acc_u 28.1250 (31.2500) lr 1.4540e-03 eta 0:00:17
epoch [72/200] batch [15/48] time 0.405 (0.449) data 0.274 (0.319) loss_u loss_u 0.8257 (0.7885) acc_u 15.6250 (28.1250) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [20/48] time 0.488 (0.449) data 0.357 (0.319) loss_u loss_u 0.8711 (0.7866) acc_u 21.8750 (28.7500) lr 1.4540e-03 eta 0:00:12
epoch [72/200] batch [25/48] time 0.418 (0.445) data 0.287 (0.314) loss_u loss_u 0.7461 (0.7828) acc_u 31.2500 (29.3750) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [30/48] time 0.423 (0.442) data 0.292 (0.312) loss_u loss_u 0.7456 (0.7787) acc_u 28.1250 (29.1667) lr 1.4540e-03 eta 0:00:07
epoch [72/200] batch [35/48] time 0.504 (0.444) data 0.373 (0.313) loss_u loss_u 0.7686 (0.7767) acc_u 34.3750 (29.7321) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [40/48] time 0.398 (0.443) data 0.267 (0.312) loss_u loss_u 0.7334 (0.7740) acc_u 28.1250 (29.6875) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [45/48] time 0.421 (0.442) data 0.290 (0.311) loss_u loss_u 0.8042 (0.7739) acc_u 25.0000 (29.7917) lr 1.4540e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1414
confident_label rate tensor(0.4952, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1553
clean true:1535
clean false:18
clean_rate:0.9884095299420477
noisy true:187
noisy false:1396
after delete: len(clean_dataset) 1553
after delete: len(noisy_dataset) 1583
epoch [73/200] batch [5/48] time 0.465 (0.460) data 0.335 (0.329) loss_x loss_x 0.6899 (1.0376) acc_x 87.5000 (71.8750) lr 1.4399e-03 eta 0:00:19
epoch [73/200] batch [10/48] time 0.545 (0.464) data 0.415 (0.334) loss_x loss_x 1.4180 (1.1295) acc_x 50.0000 (70.0000) lr 1.4399e-03 eta 0:00:17
epoch [73/200] batch [15/48] time 0.413 (0.452) data 0.282 (0.321) loss_x loss_x 1.3076 (1.1749) acc_x 71.8750 (70.2083) lr 1.4399e-03 eta 0:00:14
epoch [73/200] batch [20/48] time 0.477 (0.456) data 0.347 (0.326) loss_x loss_x 1.3857 (1.1717) acc_x 59.3750 (69.2188) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [25/48] time 0.531 (0.451) data 0.400 (0.321) loss_x loss_x 0.5767 (1.1525) acc_x 90.6250 (70.2500) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [30/48] time 0.527 (0.448) data 0.396 (0.318) loss_x loss_x 1.1396 (1.1465) acc_x 62.5000 (70.0000) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [35/48] time 0.486 (0.446) data 0.356 (0.316) loss_x loss_x 1.2217 (1.1547) acc_x 68.7500 (70.1786) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [40/48] time 0.420 (0.445) data 0.290 (0.314) loss_x loss_x 1.1729 (1.1864) acc_x 65.6250 (69.0625) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [45/48] time 0.527 (0.449) data 0.397 (0.318) loss_x loss_x 1.0527 (1.1865) acc_x 71.8750 (69.0972) lr 1.4399e-03 eta 0:00:01
epoch [73/200] batch [5/49] time 0.463 (0.444) data 0.332 (0.314) loss_u loss_u 0.7847 (0.7378) acc_u 21.8750 (31.2500) lr 1.4399e-03 eta 0:00:19
epoch [73/200] batch [10/49] time 0.424 (0.446) data 0.293 (0.316) loss_u loss_u 0.8828 (0.7508) acc_u 12.5000 (29.3750) lr 1.4399e-03 eta 0:00:17
epoch [73/200] batch [15/49] time 0.446 (0.446) data 0.315 (0.315) loss_u loss_u 0.7319 (0.7507) acc_u 37.5000 (30.6250) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [20/49] time 0.496 (0.451) data 0.365 (0.320) loss_u loss_u 0.7993 (0.7579) acc_u 21.8750 (29.6875) lr 1.4399e-03 eta 0:00:13
epoch [73/200] batch [25/49] time 0.397 (0.451) data 0.266 (0.321) loss_u loss_u 0.7910 (0.7629) acc_u 31.2500 (28.8750) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [30/49] time 0.409 (0.450) data 0.279 (0.319) loss_u loss_u 0.7520 (0.7630) acc_u 34.3750 (29.2708) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [35/49] time 0.432 (0.446) data 0.301 (0.315) loss_u loss_u 0.8770 (0.7652) acc_u 12.5000 (29.1071) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [40/49] time 0.385 (0.442) data 0.255 (0.311) loss_u loss_u 0.8413 (0.7733) acc_u 12.5000 (28.1250) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [45/49] time 0.373 (0.442) data 0.243 (0.312) loss_u loss_u 0.7744 (0.7734) acc_u 28.1250 (27.9861) lr 1.4399e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.4946, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1551
clean true:1538
clean false:13
clean_rate:0.9916183107672469
noisy true:164
noisy false:1421
after delete: len(clean_dataset) 1551
after delete: len(noisy_dataset) 1585
epoch [74/200] batch [5/48] time 0.494 (0.471) data 0.363 (0.341) loss_x loss_x 1.0322 (1.0189) acc_x 65.6250 (70.6250) lr 1.4258e-03 eta 0:00:20
epoch [74/200] batch [10/48] time 0.492 (0.478) data 0.361 (0.348) loss_x loss_x 1.0361 (1.0449) acc_x 81.2500 (73.7500) lr 1.4258e-03 eta 0:00:18
epoch [74/200] batch [15/48] time 0.317 (0.471) data 0.187 (0.341) loss_x loss_x 1.3857 (1.1325) acc_x 65.6250 (71.0417) lr 1.4258e-03 eta 0:00:15
epoch [74/200] batch [20/48] time 0.514 (0.475) data 0.385 (0.345) loss_x loss_x 1.4355 (1.1739) acc_x 75.0000 (70.0000) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [25/48] time 0.415 (0.458) data 0.286 (0.328) loss_x loss_x 1.2627 (1.2122) acc_x 59.3750 (69.0000) lr 1.4258e-03 eta 0:00:10
epoch [74/200] batch [30/48] time 0.556 (0.465) data 0.426 (0.335) loss_x loss_x 1.4170 (1.2165) acc_x 50.0000 (68.4375) lr 1.4258e-03 eta 0:00:08
epoch [74/200] batch [35/48] time 0.391 (0.459) data 0.262 (0.330) loss_x loss_x 1.6670 (1.2372) acc_x 59.3750 (67.6786) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [40/48] time 0.462 (0.457) data 0.332 (0.327) loss_x loss_x 0.8130 (1.2394) acc_x 84.3750 (68.2031) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [45/48] time 0.415 (0.454) data 0.285 (0.324) loss_x loss_x 0.7769 (1.2464) acc_x 84.3750 (68.4028) lr 1.4258e-03 eta 0:00:01
epoch [74/200] batch [5/49] time 0.616 (0.461) data 0.485 (0.331) loss_u loss_u 0.7739 (0.7686) acc_u 28.1250 (27.5000) lr 1.4258e-03 eta 0:00:20
epoch [74/200] batch [10/49] time 0.388 (0.462) data 0.257 (0.331) loss_u loss_u 0.8018 (0.7544) acc_u 21.8750 (28.7500) lr 1.4258e-03 eta 0:00:18
epoch [74/200] batch [15/49] time 0.390 (0.458) data 0.258 (0.328) loss_u loss_u 0.7729 (0.7553) acc_u 25.0000 (28.1250) lr 1.4258e-03 eta 0:00:15
epoch [74/200] batch [20/49] time 0.440 (0.455) data 0.309 (0.324) loss_u loss_u 0.8340 (0.7707) acc_u 21.8750 (26.8750) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [25/49] time 0.413 (0.450) data 0.283 (0.320) loss_u loss_u 0.7402 (0.7709) acc_u 28.1250 (27.0000) lr 1.4258e-03 eta 0:00:10
epoch [74/200] batch [30/49] time 0.365 (0.446) data 0.234 (0.316) loss_u loss_u 0.8276 (0.7722) acc_u 25.0000 (27.1875) lr 1.4258e-03 eta 0:00:08
epoch [74/200] batch [35/49] time 0.481 (0.445) data 0.350 (0.315) loss_u loss_u 0.7632 (0.7724) acc_u 34.3750 (27.2321) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [40/49] time 0.583 (0.445) data 0.453 (0.315) loss_u loss_u 0.7637 (0.7771) acc_u 34.3750 (26.9531) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [45/49] time 0.385 (0.443) data 0.254 (0.313) loss_u loss_u 0.7373 (0.7741) acc_u 34.3750 (27.5000) lr 1.4258e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1431
confident_label rate tensor(0.4949, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1552
clean true:1536
clean false:16
clean_rate:0.9896907216494846
noisy true:169
noisy false:1415
after delete: len(clean_dataset) 1552
after delete: len(noisy_dataset) 1584
epoch [75/200] batch [5/48] time 0.397 (0.452) data 0.266 (0.322) loss_x loss_x 0.9590 (1.0556) acc_x 78.1250 (74.3750) lr 1.4115e-03 eta 0:00:19
epoch [75/200] batch [10/48] time 0.410 (0.449) data 0.279 (0.318) loss_x loss_x 0.8750 (1.0891) acc_x 75.0000 (74.0625) lr 1.4115e-03 eta 0:00:17
epoch [75/200] batch [15/48] time 0.631 (0.459) data 0.500 (0.328) loss_x loss_x 1.2285 (1.1247) acc_x 62.5000 (72.5000) lr 1.4115e-03 eta 0:00:15
epoch [75/200] batch [20/48] time 0.459 (0.463) data 0.328 (0.333) loss_x loss_x 1.2939 (1.1201) acc_x 71.8750 (70.9375) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [25/48] time 0.675 (0.465) data 0.544 (0.334) loss_x loss_x 1.3359 (1.1602) acc_x 65.6250 (69.8750) lr 1.4115e-03 eta 0:00:10
epoch [75/200] batch [30/48] time 0.454 (0.463) data 0.323 (0.333) loss_x loss_x 0.8682 (1.1502) acc_x 84.3750 (69.2708) lr 1.4115e-03 eta 0:00:08
epoch [75/200] batch [35/48] time 0.447 (0.456) data 0.317 (0.325) loss_x loss_x 1.6230 (1.1698) acc_x 53.1250 (68.5714) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [40/48] time 0.401 (0.452) data 0.271 (0.322) loss_x loss_x 1.1660 (1.1572) acc_x 78.1250 (69.1406) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [45/48] time 0.386 (0.450) data 0.256 (0.320) loss_x loss_x 0.8726 (1.1523) acc_x 84.3750 (69.8611) lr 1.4115e-03 eta 0:00:01
epoch [75/200] batch [5/49] time 0.571 (0.458) data 0.440 (0.327) loss_u loss_u 0.8247 (0.7633) acc_u 18.7500 (30.0000) lr 1.4115e-03 eta 0:00:20
epoch [75/200] batch [10/49] time 0.302 (0.456) data 0.171 (0.325) loss_u loss_u 0.7354 (0.7602) acc_u 37.5000 (30.3125) lr 1.4115e-03 eta 0:00:17
epoch [75/200] batch [15/49] time 0.319 (0.453) data 0.187 (0.323) loss_u loss_u 0.7417 (0.7755) acc_u 37.5000 (28.1250) lr 1.4115e-03 eta 0:00:15
epoch [75/200] batch [20/49] time 0.393 (0.449) data 0.262 (0.318) loss_u loss_u 0.8149 (0.7783) acc_u 18.7500 (27.9688) lr 1.4115e-03 eta 0:00:13
epoch [75/200] batch [25/49] time 0.375 (0.445) data 0.244 (0.314) loss_u loss_u 0.7461 (0.7720) acc_u 37.5000 (29.1250) lr 1.4115e-03 eta 0:00:10
epoch [75/200] batch [30/49] time 0.423 (0.444) data 0.292 (0.313) loss_u loss_u 0.7710 (0.7680) acc_u 28.1250 (29.3750) lr 1.4115e-03 eta 0:00:08
epoch [75/200] batch [35/49] time 0.320 (0.440) data 0.189 (0.310) loss_u loss_u 0.7949 (0.7755) acc_u 28.1250 (28.8393) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [40/49] time 0.420 (0.437) data 0.289 (0.306) loss_u loss_u 0.7573 (0.7780) acc_u 34.3750 (28.5156) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [45/49] time 0.538 (0.440) data 0.407 (0.309) loss_u loss_u 0.7769 (0.7779) acc_u 25.0000 (28.4028) lr 1.4115e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1390
confident_label rate tensor(0.5035, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1579
clean true:1566
clean false:13
clean_rate:0.9917669411019633
noisy true:180
noisy false:1377
after delete: len(clean_dataset) 1579
after delete: len(noisy_dataset) 1557
epoch [76/200] batch [5/49] time 0.398 (0.500) data 0.268 (0.370) loss_x loss_x 1.1826 (1.1175) acc_x 62.5000 (73.1250) lr 1.3971e-03 eta 0:00:21
epoch [76/200] batch [10/49] time 0.473 (0.490) data 0.344 (0.361) loss_x loss_x 0.9873 (1.0311) acc_x 75.0000 (74.3750) lr 1.3971e-03 eta 0:00:19
epoch [76/200] batch [15/49] time 0.502 (0.466) data 0.372 (0.337) loss_x loss_x 1.1514 (1.0799) acc_x 81.2500 (73.9583) lr 1.3971e-03 eta 0:00:15
epoch [76/200] batch [20/49] time 0.372 (0.456) data 0.242 (0.326) loss_x loss_x 0.7612 (1.0831) acc_x 75.0000 (72.8125) lr 1.3971e-03 eta 0:00:13
epoch [76/200] batch [25/49] time 0.403 (0.452) data 0.272 (0.322) loss_x loss_x 1.2793 (1.0776) acc_x 62.5000 (72.5000) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [30/49] time 0.439 (0.443) data 0.310 (0.313) loss_x loss_x 1.2656 (1.1149) acc_x 65.6250 (71.1458) lr 1.3971e-03 eta 0:00:08
epoch [76/200] batch [35/49] time 0.480 (0.447) data 0.350 (0.317) loss_x loss_x 1.4824 (1.1648) acc_x 68.7500 (70.5357) lr 1.3971e-03 eta 0:00:06
epoch [76/200] batch [40/49] time 0.480 (0.446) data 0.351 (0.316) loss_x loss_x 1.2031 (1.1711) acc_x 62.5000 (70.2344) lr 1.3971e-03 eta 0:00:04
epoch [76/200] batch [45/49] time 0.453 (0.449) data 0.323 (0.319) loss_x loss_x 1.1777 (1.1849) acc_x 71.8750 (69.8611) lr 1.3971e-03 eta 0:00:01
epoch [76/200] batch [5/48] time 0.537 (0.453) data 0.407 (0.323) loss_u loss_u 0.8154 (0.7779) acc_u 25.0000 (26.2500) lr 1.3971e-03 eta 0:00:19
epoch [76/200] batch [10/48] time 0.518 (0.453) data 0.387 (0.323) loss_u loss_u 0.7993 (0.7721) acc_u 25.0000 (26.8750) lr 1.3971e-03 eta 0:00:17
epoch [76/200] batch [15/48] time 0.506 (0.451) data 0.374 (0.321) loss_u loss_u 0.7280 (0.7582) acc_u 37.5000 (28.9583) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [20/48] time 0.423 (0.450) data 0.293 (0.320) loss_u loss_u 0.7754 (0.7586) acc_u 28.1250 (29.5312) lr 1.3971e-03 eta 0:00:12
epoch [76/200] batch [25/48] time 0.371 (0.446) data 0.241 (0.316) loss_u loss_u 0.7471 (0.7556) acc_u 37.5000 (30.6250) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [30/48] time 0.571 (0.446) data 0.439 (0.316) loss_u loss_u 0.7764 (0.7570) acc_u 28.1250 (30.5208) lr 1.3971e-03 eta 0:00:08
epoch [76/200] batch [35/48] time 0.477 (0.446) data 0.346 (0.315) loss_u loss_u 0.7524 (0.7604) acc_u 28.1250 (30.1786) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [40/48] time 0.405 (0.444) data 0.273 (0.313) loss_u loss_u 0.7041 (0.7615) acc_u 40.6250 (29.9219) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [45/48] time 0.355 (0.441) data 0.223 (0.311) loss_u loss_u 0.7070 (0.7623) acc_u 34.3750 (29.8611) lr 1.3971e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1427
confident_label rate tensor(0.4901, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1537
clean true:1523
clean false:14
clean_rate:0.9908913467794405
noisy true:186
noisy false:1413
after delete: len(clean_dataset) 1537
after delete: len(noisy_dataset) 1599
epoch [77/200] batch [5/48] time 0.481 (0.518) data 0.351 (0.387) loss_x loss_x 1.0625 (1.2069) acc_x 78.1250 (73.1250) lr 1.3827e-03 eta 0:00:22
epoch [77/200] batch [10/48] time 0.462 (0.463) data 0.333 (0.333) loss_x loss_x 0.7412 (1.1270) acc_x 81.2500 (70.6250) lr 1.3827e-03 eta 0:00:17
epoch [77/200] batch [15/48] time 0.381 (0.459) data 0.250 (0.329) loss_x loss_x 0.7583 (1.0442) acc_x 68.7500 (70.8333) lr 1.3827e-03 eta 0:00:15
epoch [77/200] batch [20/48] time 0.481 (0.454) data 0.350 (0.324) loss_x loss_x 1.6631 (1.1345) acc_x 46.8750 (69.0625) lr 1.3827e-03 eta 0:00:12
epoch [77/200] batch [25/48] time 0.418 (0.449) data 0.287 (0.319) loss_x loss_x 0.9370 (1.1349) acc_x 71.8750 (69.5000) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [30/48] time 0.461 (0.449) data 0.331 (0.318) loss_x loss_x 0.8765 (1.1155) acc_x 75.0000 (69.8958) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [35/48] time 0.490 (0.448) data 0.359 (0.318) loss_x loss_x 1.5498 (1.1356) acc_x 56.2500 (69.5536) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [40/48] time 0.517 (0.452) data 0.386 (0.321) loss_x loss_x 1.4678 (1.1624) acc_x 59.3750 (68.8281) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [45/48] time 0.520 (0.457) data 0.390 (0.327) loss_x loss_x 0.8862 (1.1480) acc_x 81.2500 (69.6528) lr 1.3827e-03 eta 0:00:01
epoch [77/200] batch [5/49] time 0.711 (0.458) data 0.580 (0.327) loss_u loss_u 0.7764 (0.7949) acc_u 18.7500 (24.3750) lr 1.3827e-03 eta 0:00:20
epoch [77/200] batch [10/49] time 0.350 (0.452) data 0.219 (0.321) loss_u loss_u 0.8062 (0.7715) acc_u 25.0000 (28.7500) lr 1.3827e-03 eta 0:00:17
epoch [77/200] batch [15/49] time 0.372 (0.447) data 0.241 (0.317) loss_u loss_u 0.8091 (0.7675) acc_u 21.8750 (28.7500) lr 1.3827e-03 eta 0:00:15
epoch [77/200] batch [20/49] time 0.375 (0.442) data 0.245 (0.311) loss_u loss_u 0.8774 (0.7723) acc_u 12.5000 (28.7500) lr 1.3827e-03 eta 0:00:12
epoch [77/200] batch [25/49] time 0.494 (0.442) data 0.362 (0.311) loss_u loss_u 0.7373 (0.7747) acc_u 37.5000 (28.8750) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [30/49] time 0.306 (0.441) data 0.175 (0.311) loss_u loss_u 0.8730 (0.7746) acc_u 12.5000 (28.7500) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [35/49] time 0.425 (0.443) data 0.294 (0.313) loss_u loss_u 0.8237 (0.7758) acc_u 25.0000 (28.6607) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [40/49] time 0.523 (0.445) data 0.391 (0.315) loss_u loss_u 0.7026 (0.7728) acc_u 40.6250 (28.9844) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [45/49] time 0.337 (0.441) data 0.206 (0.310) loss_u loss_u 0.8789 (0.7731) acc_u 15.6250 (28.9583) lr 1.3827e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1344
confident_label rate tensor(0.5246, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1645
clean true:1623
clean false:22
clean_rate:0.9866261398176291
noisy true:169
noisy false:1322
after delete: len(clean_dataset) 1645
after delete: len(noisy_dataset) 1491
epoch [78/200] batch [5/51] time 0.534 (0.499) data 0.404 (0.369) loss_x loss_x 0.8662 (1.0689) acc_x 78.1250 (74.3750) lr 1.3681e-03 eta 0:00:22
epoch [78/200] batch [10/51] time 0.392 (0.480) data 0.262 (0.350) loss_x loss_x 1.4219 (1.2301) acc_x 62.5000 (69.6875) lr 1.3681e-03 eta 0:00:19
epoch [78/200] batch [15/51] time 0.440 (0.459) data 0.309 (0.329) loss_x loss_x 1.0781 (1.2514) acc_x 75.0000 (70.0000) lr 1.3681e-03 eta 0:00:16
epoch [78/200] batch [20/51] time 0.468 (0.444) data 0.338 (0.313) loss_x loss_x 1.0664 (1.2681) acc_x 75.0000 (69.2188) lr 1.3681e-03 eta 0:00:13
epoch [78/200] batch [25/51] time 0.540 (0.453) data 0.411 (0.322) loss_x loss_x 0.9702 (1.2666) acc_x 75.0000 (69.3750) lr 1.3681e-03 eta 0:00:11
epoch [78/200] batch [30/51] time 0.395 (0.450) data 0.266 (0.320) loss_x loss_x 0.8301 (1.2086) acc_x 75.0000 (70.8333) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [35/51] time 0.578 (0.452) data 0.448 (0.322) loss_x loss_x 1.1328 (1.2219) acc_x 68.7500 (69.8214) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [40/51] time 0.427 (0.457) data 0.297 (0.327) loss_x loss_x 1.1309 (1.2222) acc_x 56.2500 (69.4531) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [45/51] time 0.440 (0.457) data 0.310 (0.326) loss_x loss_x 0.6558 (1.2068) acc_x 84.3750 (69.8611) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [50/51] time 0.546 (0.458) data 0.416 (0.328) loss_x loss_x 1.1328 (1.1893) acc_x 68.7500 (70.1250) lr 1.3681e-03 eta 0:00:00
epoch [78/200] batch [5/46] time 0.418 (0.454) data 0.287 (0.324) loss_u loss_u 0.7012 (0.7378) acc_u 34.3750 (35.0000) lr 1.3681e-03 eta 0:00:18
epoch [78/200] batch [10/46] time 0.411 (0.453) data 0.281 (0.322) loss_u loss_u 0.7114 (0.7369) acc_u 34.3750 (33.4375) lr 1.3681e-03 eta 0:00:16
epoch [78/200] batch [15/46] time 0.337 (0.449) data 0.207 (0.319) loss_u loss_u 0.8188 (0.7589) acc_u 21.8750 (30.2083) lr 1.3681e-03 eta 0:00:13
epoch [78/200] batch [20/46] time 0.408 (0.449) data 0.277 (0.319) loss_u loss_u 0.7607 (0.7697) acc_u 31.2500 (29.0625) lr 1.3681e-03 eta 0:00:11
epoch [78/200] batch [25/46] time 0.347 (0.445) data 0.216 (0.314) loss_u loss_u 0.8281 (0.7762) acc_u 18.7500 (28.3750) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [30/46] time 0.393 (0.442) data 0.263 (0.311) loss_u loss_u 0.7817 (0.7730) acc_u 25.0000 (28.7500) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [35/46] time 0.439 (0.441) data 0.308 (0.310) loss_u loss_u 0.6953 (0.7703) acc_u 40.6250 (29.1964) lr 1.3681e-03 eta 0:00:04
epoch [78/200] batch [40/46] time 0.408 (0.438) data 0.277 (0.307) loss_u loss_u 0.7607 (0.7673) acc_u 31.2500 (29.6094) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [45/46] time 0.375 (0.438) data 0.244 (0.308) loss_u loss_u 0.7856 (0.7681) acc_u 28.1250 (29.2361) lr 1.3681e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1384
confident_label rate tensor(0.5105, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1601
clean true:1584
clean false:17
clean_rate:0.9893816364772018
noisy true:168
noisy false:1367
after delete: len(clean_dataset) 1601
after delete: len(noisy_dataset) 1535
epoch [79/200] batch [5/50] time 0.389 (0.405) data 0.258 (0.275) loss_x loss_x 1.1758 (1.1613) acc_x 71.8750 (70.6250) lr 1.3535e-03 eta 0:00:18
epoch [79/200] batch [10/50] time 0.500 (0.422) data 0.370 (0.292) loss_x loss_x 1.2490 (1.2868) acc_x 75.0000 (68.1250) lr 1.3535e-03 eta 0:00:16
epoch [79/200] batch [15/50] time 0.523 (0.427) data 0.393 (0.296) loss_x loss_x 1.2988 (1.2673) acc_x 65.6250 (68.9583) lr 1.3535e-03 eta 0:00:14
epoch [79/200] batch [20/50] time 0.609 (0.440) data 0.478 (0.309) loss_x loss_x 0.9961 (1.2486) acc_x 78.1250 (69.2188) lr 1.3535e-03 eta 0:00:13
epoch [79/200] batch [25/50] time 0.347 (0.439) data 0.217 (0.308) loss_x loss_x 1.3867 (1.2246) acc_x 71.8750 (70.2500) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [30/50] time 0.397 (0.439) data 0.267 (0.308) loss_x loss_x 1.7920 (1.2479) acc_x 59.3750 (69.1667) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [35/50] time 0.381 (0.443) data 0.251 (0.313) loss_x loss_x 0.9531 (1.2444) acc_x 71.8750 (69.3750) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [40/50] time 0.353 (0.449) data 0.223 (0.318) loss_x loss_x 1.3018 (1.2403) acc_x 68.7500 (69.2188) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [45/50] time 0.463 (0.448) data 0.333 (0.317) loss_x loss_x 0.8228 (1.2235) acc_x 84.3750 (69.5139) lr 1.3535e-03 eta 0:00:02
epoch [79/200] batch [50/50] time 0.362 (0.449) data 0.232 (0.319) loss_x loss_x 0.9668 (1.2233) acc_x 71.8750 (69.5625) lr 1.3535e-03 eta 0:00:00
epoch [79/200] batch [5/47] time 0.433 (0.445) data 0.302 (0.314) loss_u loss_u 0.7256 (0.7611) acc_u 31.2500 (28.1250) lr 1.3535e-03 eta 0:00:18
epoch [79/200] batch [10/47] time 0.632 (0.450) data 0.501 (0.319) loss_u loss_u 0.7651 (0.7664) acc_u 37.5000 (30.3125) lr 1.3535e-03 eta 0:00:16
epoch [79/200] batch [15/47] time 0.539 (0.449) data 0.408 (0.318) loss_u loss_u 0.7588 (0.7646) acc_u 31.2500 (29.3750) lr 1.3535e-03 eta 0:00:14
epoch [79/200] batch [20/47] time 0.482 (0.447) data 0.348 (0.316) loss_u loss_u 0.7622 (0.7689) acc_u 25.0000 (28.5938) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [25/47] time 0.455 (0.449) data 0.324 (0.318) loss_u loss_u 0.7065 (0.7606) acc_u 37.5000 (29.3750) lr 1.3535e-03 eta 0:00:09
epoch [79/200] batch [30/47] time 0.419 (0.446) data 0.288 (0.315) loss_u loss_u 0.7773 (0.7681) acc_u 28.1250 (28.5417) lr 1.3535e-03 eta 0:00:07
epoch [79/200] batch [35/47] time 0.455 (0.443) data 0.324 (0.313) loss_u loss_u 0.6816 (0.7664) acc_u 37.5000 (28.7500) lr 1.3535e-03 eta 0:00:05
epoch [79/200] batch [40/47] time 0.388 (0.442) data 0.257 (0.311) loss_u loss_u 0.6948 (0.7672) acc_u 40.6250 (28.9062) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [45/47] time 0.359 (0.438) data 0.228 (0.307) loss_u loss_u 0.7529 (0.7706) acc_u 28.1250 (28.0556) lr 1.3535e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1419
confident_label rate tensor(0.4933, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1547
clean true:1532
clean false:15
clean_rate:0.9903038138332256
noisy true:185
noisy false:1404
after delete: len(clean_dataset) 1547
after delete: len(noisy_dataset) 1589
epoch [80/200] batch [5/48] time 0.389 (0.491) data 0.259 (0.360) loss_x loss_x 1.1963 (1.1475) acc_x 68.7500 (71.2500) lr 1.3387e-03 eta 0:00:21
epoch [80/200] batch [10/48] time 0.608 (0.494) data 0.477 (0.363) loss_x loss_x 1.0020 (1.1511) acc_x 68.7500 (70.9375) lr 1.3387e-03 eta 0:00:18
epoch [80/200] batch [15/48] time 0.435 (0.489) data 0.305 (0.358) loss_x loss_x 1.0654 (1.1414) acc_x 71.8750 (69.7917) lr 1.3387e-03 eta 0:00:16
epoch [80/200] batch [20/48] time 0.532 (0.481) data 0.401 (0.350) loss_x loss_x 1.2168 (1.1395) acc_x 68.7500 (69.5312) lr 1.3387e-03 eta 0:00:13
epoch [80/200] batch [25/48] time 0.402 (0.459) data 0.272 (0.328) loss_x loss_x 1.3105 (1.1419) acc_x 68.7500 (69.7500) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [30/48] time 0.479 (0.461) data 0.349 (0.330) loss_x loss_x 0.7515 (1.1550) acc_x 81.2500 (69.2708) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [35/48] time 0.357 (0.453) data 0.227 (0.323) loss_x loss_x 1.2676 (1.1563) acc_x 65.6250 (69.1964) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [40/48] time 0.525 (0.459) data 0.395 (0.329) loss_x loss_x 1.4902 (1.1569) acc_x 62.5000 (69.4531) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [45/48] time 0.387 (0.458) data 0.256 (0.327) loss_x loss_x 1.1436 (1.1700) acc_x 78.1250 (69.5139) lr 1.3387e-03 eta 0:00:01
epoch [80/200] batch [5/49] time 0.526 (0.450) data 0.395 (0.320) loss_u loss_u 0.8013 (0.7720) acc_u 25.0000 (26.8750) lr 1.3387e-03 eta 0:00:19
epoch [80/200] batch [10/49] time 0.319 (0.446) data 0.188 (0.316) loss_u loss_u 0.7622 (0.7665) acc_u 31.2500 (29.6875) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [15/49] time 0.479 (0.445) data 0.348 (0.315) loss_u loss_u 0.7734 (0.7633) acc_u 31.2500 (29.3750) lr 1.3387e-03 eta 0:00:15
epoch [80/200] batch [20/49] time 0.361 (0.447) data 0.231 (0.316) loss_u loss_u 0.7949 (0.7603) acc_u 25.0000 (30.0000) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [25/49] time 0.610 (0.450) data 0.479 (0.319) loss_u loss_u 0.8149 (0.7669) acc_u 21.8750 (29.0000) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [30/49] time 0.371 (0.447) data 0.240 (0.316) loss_u loss_u 0.7871 (0.7647) acc_u 40.6250 (29.5833) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [35/49] time 0.388 (0.444) data 0.257 (0.314) loss_u loss_u 0.7178 (0.7643) acc_u 37.5000 (29.6429) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [40/49] time 0.361 (0.443) data 0.230 (0.312) loss_u loss_u 0.6494 (0.7652) acc_u 37.5000 (29.3750) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [45/49] time 0.344 (0.440) data 0.213 (0.310) loss_u loss_u 0.8223 (0.7694) acc_u 21.8750 (28.6111) lr 1.3387e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1391
confident_label rate tensor(0.5045, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1582
clean true:1560
clean false:22
clean_rate:0.9860935524652339
noisy true:185
noisy false:1369
after delete: len(clean_dataset) 1582
after delete: len(noisy_dataset) 1554
epoch [81/200] batch [5/49] time 0.489 (0.454) data 0.358 (0.323) loss_x loss_x 1.3691 (1.2431) acc_x 59.3750 (70.0000) lr 1.3239e-03 eta 0:00:19
epoch [81/200] batch [10/49] time 0.380 (0.426) data 0.249 (0.296) loss_x loss_x 0.7690 (1.1147) acc_x 90.6250 (74.0625) lr 1.3239e-03 eta 0:00:16
epoch [81/200] batch [15/49] time 0.416 (0.420) data 0.285 (0.289) loss_x loss_x 1.4688 (1.1339) acc_x 62.5000 (70.8333) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [20/49] time 0.404 (0.418) data 0.274 (0.288) loss_x loss_x 0.8140 (1.1359) acc_x 78.1250 (69.3750) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [25/49] time 0.508 (0.437) data 0.378 (0.306) loss_x loss_x 0.9717 (1.1714) acc_x 78.1250 (70.5000) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [30/49] time 0.429 (0.441) data 0.299 (0.311) loss_x loss_x 1.1387 (1.1817) acc_x 71.8750 (70.5208) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [35/49] time 0.420 (0.443) data 0.290 (0.313) loss_x loss_x 1.1807 (1.1795) acc_x 68.7500 (70.6250) lr 1.3239e-03 eta 0:00:06
epoch [81/200] batch [40/49] time 0.556 (0.448) data 0.427 (0.318) loss_x loss_x 1.0361 (1.1908) acc_x 65.6250 (70.5469) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [45/49] time 0.461 (0.447) data 0.331 (0.317) loss_x loss_x 1.2559 (1.1982) acc_x 71.8750 (70.2083) lr 1.3239e-03 eta 0:00:01
epoch [81/200] batch [5/48] time 0.699 (0.459) data 0.567 (0.329) loss_u loss_u 0.8311 (0.7314) acc_u 21.8750 (34.3750) lr 1.3239e-03 eta 0:00:19
epoch [81/200] batch [10/48] time 0.466 (0.460) data 0.330 (0.329) loss_u loss_u 0.7993 (0.7330) acc_u 31.2500 (35.3125) lr 1.3239e-03 eta 0:00:17
epoch [81/200] batch [15/48] time 0.386 (0.453) data 0.255 (0.322) loss_u loss_u 0.7163 (0.7394) acc_u 34.3750 (33.9583) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [20/48] time 0.413 (0.451) data 0.281 (0.320) loss_u loss_u 0.8511 (0.7470) acc_u 18.7500 (33.2812) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [25/48] time 0.487 (0.452) data 0.355 (0.321) loss_u loss_u 0.6929 (0.7539) acc_u 37.5000 (32.2500) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [30/48] time 0.347 (0.450) data 0.216 (0.319) loss_u loss_u 0.8735 (0.7598) acc_u 25.0000 (31.7708) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [35/48] time 0.294 (0.447) data 0.163 (0.316) loss_u loss_u 0.7485 (0.7621) acc_u 34.3750 (30.9821) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [40/48] time 0.438 (0.444) data 0.307 (0.313) loss_u loss_u 0.7124 (0.7613) acc_u 34.3750 (30.9375) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [45/48] time 0.438 (0.442) data 0.306 (0.311) loss_u loss_u 0.7632 (0.7640) acc_u 25.0000 (30.2778) lr 1.3239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1375
confident_label rate tensor(0.5124, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1607
clean true:1588
clean false:19
clean_rate:0.9881767268201618
noisy true:173
noisy false:1356
after delete: len(clean_dataset) 1607
after delete: len(noisy_dataset) 1529
epoch [82/200] batch [5/50] time 0.523 (0.451) data 0.393 (0.321) loss_x loss_x 0.6367 (1.1158) acc_x 81.2500 (70.0000) lr 1.3090e-03 eta 0:00:20
epoch [82/200] batch [10/50] time 0.458 (0.427) data 0.328 (0.297) loss_x loss_x 1.6904 (1.2352) acc_x 53.1250 (69.6875) lr 1.3090e-03 eta 0:00:17
epoch [82/200] batch [15/50] time 0.483 (0.438) data 0.352 (0.307) loss_x loss_x 1.3701 (1.2863) acc_x 59.3750 (68.1250) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [20/50] time 0.410 (0.439) data 0.279 (0.309) loss_x loss_x 1.4717 (1.2716) acc_x 50.0000 (67.9688) lr 1.3090e-03 eta 0:00:13
epoch [82/200] batch [25/50] time 0.606 (0.450) data 0.476 (0.319) loss_x loss_x 0.9966 (1.2533) acc_x 71.8750 (68.0000) lr 1.3090e-03 eta 0:00:11
epoch [82/200] batch [30/50] time 0.438 (0.451) data 0.307 (0.321) loss_x loss_x 1.1074 (1.2595) acc_x 71.8750 (68.0208) lr 1.3090e-03 eta 0:00:09
epoch [82/200] batch [35/50] time 0.339 (0.446) data 0.208 (0.316) loss_x loss_x 0.6538 (1.2397) acc_x 93.7500 (68.5714) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [40/50] time 0.471 (0.444) data 0.341 (0.313) loss_x loss_x 1.2305 (1.2397) acc_x 65.6250 (68.7500) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [45/50] time 0.464 (0.444) data 0.334 (0.314) loss_x loss_x 0.7876 (1.2458) acc_x 84.3750 (68.9583) lr 1.3090e-03 eta 0:00:02
epoch [82/200] batch [50/50] time 0.407 (0.445) data 0.277 (0.314) loss_x loss_x 0.7227 (1.2357) acc_x 84.3750 (68.8750) lr 1.3090e-03 eta 0:00:00
epoch [82/200] batch [5/47] time 0.350 (0.449) data 0.219 (0.318) loss_u loss_u 0.6592 (0.7522) acc_u 40.6250 (30.6250) lr 1.3090e-03 eta 0:00:18
epoch [82/200] batch [10/47] time 0.473 (0.448) data 0.341 (0.317) loss_u loss_u 0.7544 (0.7658) acc_u 31.2500 (30.6250) lr 1.3090e-03 eta 0:00:16
epoch [82/200] batch [15/47] time 0.360 (0.446) data 0.229 (0.316) loss_u loss_u 0.8057 (0.7810) acc_u 21.8750 (28.9583) lr 1.3090e-03 eta 0:00:14
epoch [82/200] batch [20/47] time 0.452 (0.446) data 0.321 (0.315) loss_u loss_u 0.7046 (0.7812) acc_u 34.3750 (28.4375) lr 1.3090e-03 eta 0:00:12
epoch [82/200] batch [25/47] time 0.358 (0.443) data 0.227 (0.312) loss_u loss_u 0.6582 (0.7654) acc_u 43.7500 (30.3750) lr 1.3090e-03 eta 0:00:09
epoch [82/200] batch [30/47] time 0.389 (0.439) data 0.258 (0.308) loss_u loss_u 0.8218 (0.7672) acc_u 31.2500 (30.2083) lr 1.3090e-03 eta 0:00:07
epoch [82/200] batch [35/47] time 0.396 (0.438) data 0.266 (0.307) loss_u loss_u 0.8330 (0.7679) acc_u 18.7500 (29.9107) lr 1.3090e-03 eta 0:00:05
epoch [82/200] batch [40/47] time 0.477 (0.437) data 0.346 (0.307) loss_u loss_u 0.8569 (0.7729) acc_u 15.6250 (29.0625) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [45/47] time 0.585 (0.443) data 0.454 (0.312) loss_u loss_u 0.7705 (0.7706) acc_u 34.3750 (29.2361) lr 1.3090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1394
confident_label rate tensor(0.5022, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1575
clean true:1562
clean false:13
clean_rate:0.9917460317460317
noisy true:180
noisy false:1381
after delete: len(clean_dataset) 1575
after delete: len(noisy_dataset) 1561
epoch [83/200] batch [5/49] time 0.426 (0.417) data 0.295 (0.287) loss_x loss_x 1.2510 (1.3652) acc_x 71.8750 (65.6250) lr 1.2940e-03 eta 0:00:18
epoch [83/200] batch [10/49] time 0.414 (0.424) data 0.285 (0.294) loss_x loss_x 1.0225 (1.2413) acc_x 68.7500 (70.0000) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [15/49] time 0.461 (0.431) data 0.330 (0.301) loss_x loss_x 1.3545 (1.2464) acc_x 62.5000 (69.3750) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [20/49] time 0.380 (0.456) data 0.249 (0.326) loss_x loss_x 0.8394 (1.2216) acc_x 78.1250 (69.8438) lr 1.2940e-03 eta 0:00:13
epoch [83/200] batch [25/49] time 0.446 (0.460) data 0.316 (0.330) loss_x loss_x 1.1006 (1.2083) acc_x 71.8750 (69.8750) lr 1.2940e-03 eta 0:00:11
epoch [83/200] batch [30/49] time 0.466 (0.465) data 0.337 (0.334) loss_x loss_x 0.9927 (1.2034) acc_x 75.0000 (69.5833) lr 1.2940e-03 eta 0:00:08
epoch [83/200] batch [35/49] time 0.409 (0.457) data 0.279 (0.327) loss_x loss_x 1.1631 (1.2040) acc_x 65.6250 (68.8393) lr 1.2940e-03 eta 0:00:06
epoch [83/200] batch [40/49] time 0.430 (0.458) data 0.299 (0.328) loss_x loss_x 1.1875 (1.2208) acc_x 59.3750 (68.3594) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [45/49] time 0.489 (0.454) data 0.358 (0.324) loss_x loss_x 1.2393 (1.2107) acc_x 68.7500 (68.9583) lr 1.2940e-03 eta 0:00:01
epoch [83/200] batch [5/48] time 0.443 (0.454) data 0.312 (0.323) loss_u loss_u 0.7349 (0.7523) acc_u 37.5000 (29.3750) lr 1.2940e-03 eta 0:00:19
epoch [83/200] batch [10/48] time 0.398 (0.453) data 0.267 (0.323) loss_u loss_u 0.7715 (0.7391) acc_u 40.6250 (31.8750) lr 1.2940e-03 eta 0:00:17
epoch [83/200] batch [15/48] time 0.364 (0.451) data 0.233 (0.321) loss_u loss_u 0.8452 (0.7515) acc_u 21.8750 (30.2083) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [20/48] time 0.417 (0.450) data 0.286 (0.320) loss_u loss_u 0.6841 (0.7519) acc_u 40.6250 (29.6875) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [25/48] time 0.384 (0.448) data 0.253 (0.317) loss_u loss_u 0.7866 (0.7597) acc_u 25.0000 (28.6250) lr 1.2940e-03 eta 0:00:10
epoch [83/200] batch [30/48] time 0.344 (0.443) data 0.213 (0.313) loss_u loss_u 0.8271 (0.7676) acc_u 25.0000 (28.1250) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [35/48] time 0.533 (0.446) data 0.402 (0.316) loss_u loss_u 0.7686 (0.7723) acc_u 34.3750 (28.0357) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [40/48] time 0.411 (0.448) data 0.280 (0.318) loss_u loss_u 0.7412 (0.7695) acc_u 34.3750 (28.4375) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [45/48] time 0.450 (0.449) data 0.319 (0.318) loss_u loss_u 0.8169 (0.7723) acc_u 18.7500 (28.0556) lr 1.2940e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1399
confident_label rate tensor(0.5051, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1584
clean true:1569
clean false:15
clean_rate:0.990530303030303
noisy true:168
noisy false:1384
after delete: len(clean_dataset) 1584
after delete: len(noisy_dataset) 1552
epoch [84/200] batch [5/49] time 0.453 (0.447) data 0.323 (0.317) loss_x loss_x 1.0732 (1.2184) acc_x 65.6250 (70.0000) lr 1.2790e-03 eta 0:00:19
epoch [84/200] batch [10/49] time 0.363 (0.445) data 0.233 (0.315) loss_x loss_x 1.1016 (1.1880) acc_x 75.0000 (69.3750) lr 1.2790e-03 eta 0:00:17
epoch [84/200] batch [15/49] time 0.440 (0.456) data 0.310 (0.326) loss_x loss_x 0.8169 (1.1793) acc_x 84.3750 (70.4167) lr 1.2790e-03 eta 0:00:15
epoch [84/200] batch [20/49] time 0.433 (0.448) data 0.302 (0.317) loss_x loss_x 0.7876 (1.1322) acc_x 84.3750 (71.4062) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [25/49] time 0.407 (0.452) data 0.276 (0.322) loss_x loss_x 1.0781 (1.1713) acc_x 75.0000 (70.6250) lr 1.2790e-03 eta 0:00:10
epoch [84/200] batch [30/49] time 0.554 (0.460) data 0.423 (0.329) loss_x loss_x 1.3389 (1.1792) acc_x 68.7500 (70.6250) lr 1.2790e-03 eta 0:00:08
epoch [84/200] batch [35/49] time 0.467 (0.462) data 0.337 (0.332) loss_x loss_x 1.5439 (1.1737) acc_x 53.1250 (69.8214) lr 1.2790e-03 eta 0:00:06
epoch [84/200] batch [40/49] time 0.388 (0.462) data 0.258 (0.331) loss_x loss_x 1.3467 (1.1926) acc_x 56.2500 (69.2969) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [45/49] time 0.397 (0.457) data 0.266 (0.326) loss_x loss_x 0.7144 (1.1801) acc_x 78.1250 (69.5139) lr 1.2790e-03 eta 0:00:01
epoch [84/200] batch [5/48] time 0.444 (0.453) data 0.314 (0.323) loss_u loss_u 0.8555 (0.7585) acc_u 18.7500 (30.6250) lr 1.2790e-03 eta 0:00:19
epoch [84/200] batch [10/48] time 0.402 (0.447) data 0.271 (0.317) loss_u loss_u 0.7100 (0.7536) acc_u 34.3750 (29.3750) lr 1.2790e-03 eta 0:00:16
epoch [84/200] batch [15/48] time 0.405 (0.446) data 0.274 (0.316) loss_u loss_u 0.6514 (0.7535) acc_u 53.1250 (30.8333) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [20/48] time 0.380 (0.447) data 0.249 (0.317) loss_u loss_u 0.7788 (0.7674) acc_u 31.2500 (29.2188) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [25/48] time 0.387 (0.444) data 0.256 (0.314) loss_u loss_u 0.7412 (0.7648) acc_u 28.1250 (29.5000) lr 1.2790e-03 eta 0:00:10
epoch [84/200] batch [30/48] time 0.366 (0.440) data 0.235 (0.310) loss_u loss_u 0.6807 (0.7635) acc_u 40.6250 (29.7917) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [35/48] time 0.476 (0.440) data 0.346 (0.310) loss_u loss_u 0.7578 (0.7644) acc_u 34.3750 (30.0893) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [40/48] time 0.444 (0.439) data 0.313 (0.309) loss_u loss_u 0.7593 (0.7678) acc_u 25.0000 (29.6875) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [45/48] time 0.585 (0.440) data 0.454 (0.309) loss_u loss_u 0.7188 (0.7662) acc_u 31.2500 (29.6528) lr 1.2790e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.4971, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1559
clean true:1547
clean false:12
clean_rate:0.9923027581783195
noisy true:176
noisy false:1401
after delete: len(clean_dataset) 1559
after delete: len(noisy_dataset) 1577
epoch [85/200] batch [5/48] time 0.469 (0.494) data 0.339 (0.364) loss_x loss_x 1.1992 (1.1887) acc_x 62.5000 (68.1250) lr 1.2639e-03 eta 0:00:21
epoch [85/200] batch [10/48] time 0.400 (0.483) data 0.270 (0.353) loss_x loss_x 1.1904 (1.1002) acc_x 71.8750 (71.2500) lr 1.2639e-03 eta 0:00:18
epoch [85/200] batch [15/48] time 0.388 (0.478) data 0.258 (0.348) loss_x loss_x 1.1436 (1.0842) acc_x 71.8750 (71.4583) lr 1.2639e-03 eta 0:00:15
epoch [85/200] batch [20/48] time 0.470 (0.481) data 0.341 (0.351) loss_x loss_x 0.8394 (1.0755) acc_x 81.2500 (72.8125) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [25/48] time 0.400 (0.481) data 0.270 (0.351) loss_x loss_x 0.9321 (1.0704) acc_x 78.1250 (72.1250) lr 1.2639e-03 eta 0:00:11
epoch [85/200] batch [30/48] time 0.465 (0.476) data 0.336 (0.346) loss_x loss_x 1.4717 (1.0822) acc_x 71.8750 (71.5625) lr 1.2639e-03 eta 0:00:08
epoch [85/200] batch [35/48] time 0.458 (0.476) data 0.327 (0.346) loss_x loss_x 1.6211 (1.0885) acc_x 75.0000 (71.6964) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [40/48] time 0.459 (0.474) data 0.329 (0.343) loss_x loss_x 1.3350 (1.1175) acc_x 71.8750 (71.0938) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [45/48] time 0.448 (0.471) data 0.318 (0.341) loss_x loss_x 1.5557 (1.1280) acc_x 71.8750 (70.8333) lr 1.2639e-03 eta 0:00:01
epoch [85/200] batch [5/49] time 0.513 (0.472) data 0.382 (0.341) loss_u loss_u 0.7998 (0.7628) acc_u 25.0000 (31.8750) lr 1.2639e-03 eta 0:00:20
epoch [85/200] batch [10/49] time 0.367 (0.464) data 0.236 (0.334) loss_u loss_u 0.8174 (0.7733) acc_u 21.8750 (27.8125) lr 1.2639e-03 eta 0:00:18
epoch [85/200] batch [15/49] time 0.457 (0.459) data 0.326 (0.329) loss_u loss_u 0.7422 (0.7674) acc_u 40.6250 (28.7500) lr 1.2639e-03 eta 0:00:15
epoch [85/200] batch [20/49] time 0.600 (0.458) data 0.469 (0.328) loss_u loss_u 0.7725 (0.7710) acc_u 28.1250 (28.4375) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [25/49] time 0.391 (0.456) data 0.260 (0.325) loss_u loss_u 0.7886 (0.7697) acc_u 28.1250 (29.0000) lr 1.2639e-03 eta 0:00:10
epoch [85/200] batch [30/49] time 0.388 (0.454) data 0.257 (0.324) loss_u loss_u 0.6509 (0.7676) acc_u 43.7500 (29.0625) lr 1.2639e-03 eta 0:00:08
epoch [85/200] batch [35/49] time 0.438 (0.451) data 0.307 (0.321) loss_u loss_u 0.8101 (0.7674) acc_u 18.7500 (28.9286) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [40/49] time 0.349 (0.447) data 0.218 (0.316) loss_u loss_u 0.8774 (0.7702) acc_u 15.6250 (28.6719) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [45/49] time 0.435 (0.447) data 0.304 (0.316) loss_u loss_u 0.7456 (0.7714) acc_u 28.1250 (28.4722) lr 1.2639e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1409
confident_label rate tensor(0.4959, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1555
clean true:1540
clean false:15
clean_rate:0.9903536977491961
noisy true:187
noisy false:1394
after delete: len(clean_dataset) 1555
after delete: len(noisy_dataset) 1581
epoch [86/200] batch [5/48] time 0.475 (0.465) data 0.345 (0.334) loss_x loss_x 1.0615 (1.1578) acc_x 81.2500 (72.5000) lr 1.2487e-03 eta 0:00:19
epoch [86/200] batch [10/48] time 0.427 (0.463) data 0.296 (0.332) loss_x loss_x 0.5244 (1.1167) acc_x 87.5000 (71.2500) lr 1.2487e-03 eta 0:00:17
epoch [86/200] batch [15/48] time 0.377 (0.476) data 0.246 (0.345) loss_x loss_x 1.0146 (1.0590) acc_x 68.7500 (73.5417) lr 1.2487e-03 eta 0:00:15
epoch [86/200] batch [20/48] time 0.387 (0.460) data 0.256 (0.329) loss_x loss_x 0.5708 (1.0125) acc_x 90.6250 (75.6250) lr 1.2487e-03 eta 0:00:12
epoch [86/200] batch [25/48] time 0.404 (0.461) data 0.273 (0.331) loss_x loss_x 1.3916 (1.0598) acc_x 65.6250 (74.2500) lr 1.2487e-03 eta 0:00:10
epoch [86/200] batch [30/48] time 0.446 (0.470) data 0.316 (0.340) loss_x loss_x 1.2080 (1.0702) acc_x 75.0000 (74.3750) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [35/48] time 0.364 (0.464) data 0.233 (0.334) loss_x loss_x 1.4697 (1.1104) acc_x 68.7500 (73.9286) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [40/48] time 0.419 (0.457) data 0.288 (0.326) loss_x loss_x 1.1230 (1.1168) acc_x 68.7500 (73.2812) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [45/48] time 0.434 (0.457) data 0.304 (0.327) loss_x loss_x 1.6963 (1.1330) acc_x 59.3750 (72.7083) lr 1.2487e-03 eta 0:00:01
epoch [86/200] batch [5/49] time 0.359 (0.452) data 0.228 (0.322) loss_u loss_u 0.6895 (0.7610) acc_u 40.6250 (33.7500) lr 1.2487e-03 eta 0:00:19
epoch [86/200] batch [10/49] time 0.334 (0.447) data 0.203 (0.317) loss_u loss_u 0.8105 (0.7489) acc_u 25.0000 (33.4375) lr 1.2487e-03 eta 0:00:17
epoch [86/200] batch [15/49] time 0.411 (0.445) data 0.280 (0.314) loss_u loss_u 0.8696 (0.7627) acc_u 21.8750 (32.0833) lr 1.2487e-03 eta 0:00:15
epoch [86/200] batch [20/49] time 0.462 (0.446) data 0.331 (0.315) loss_u loss_u 0.7485 (0.7629) acc_u 40.6250 (31.8750) lr 1.2487e-03 eta 0:00:12
epoch [86/200] batch [25/49] time 0.346 (0.451) data 0.214 (0.320) loss_u loss_u 0.7710 (0.7643) acc_u 31.2500 (31.2500) lr 1.2487e-03 eta 0:00:10
epoch [86/200] batch [30/49] time 0.431 (0.450) data 0.300 (0.319) loss_u loss_u 0.7515 (0.7635) acc_u 31.2500 (31.5625) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [35/49] time 0.414 (0.448) data 0.283 (0.317) loss_u loss_u 0.7505 (0.7670) acc_u 34.3750 (30.6250) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [40/49] time 0.395 (0.446) data 0.264 (0.315) loss_u loss_u 0.8457 (0.7706) acc_u 18.7500 (30.1562) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [45/49] time 0.472 (0.443) data 0.341 (0.312) loss_u loss_u 0.7422 (0.7713) acc_u 31.2500 (29.7222) lr 1.2487e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1391
confident_label rate tensor(0.5054, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1585
clean true:1567
clean false:18
clean_rate:0.9886435331230284
noisy true:178
noisy false:1373
after delete: len(clean_dataset) 1585
after delete: len(noisy_dataset) 1551
epoch [87/200] batch [5/49] time 0.436 (0.437) data 0.306 (0.306) loss_x loss_x 1.5449 (1.2727) acc_x 65.6250 (70.0000) lr 1.2334e-03 eta 0:00:19
epoch [87/200] batch [10/49] time 0.591 (0.462) data 0.461 (0.332) loss_x loss_x 1.4453 (1.1852) acc_x 56.2500 (68.7500) lr 1.2334e-03 eta 0:00:18
epoch [87/200] batch [15/49] time 0.408 (0.458) data 0.278 (0.327) loss_x loss_x 1.5254 (1.2097) acc_x 65.6250 (69.1667) lr 1.2334e-03 eta 0:00:15
epoch [87/200] batch [20/49] time 0.506 (0.458) data 0.375 (0.328) loss_x loss_x 1.1855 (1.1909) acc_x 65.6250 (70.0000) lr 1.2334e-03 eta 0:00:13
epoch [87/200] batch [25/49] time 0.461 (0.459) data 0.332 (0.328) loss_x loss_x 0.9458 (1.1138) acc_x 71.8750 (72.3750) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [30/49] time 0.465 (0.456) data 0.336 (0.326) loss_x loss_x 0.9209 (1.1049) acc_x 78.1250 (72.7083) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [35/49] time 0.464 (0.460) data 0.334 (0.330) loss_x loss_x 1.2959 (1.1266) acc_x 65.6250 (71.8750) lr 1.2334e-03 eta 0:00:06
epoch [87/200] batch [40/49] time 0.424 (0.451) data 0.294 (0.321) loss_x loss_x 0.7583 (1.1304) acc_x 78.1250 (72.0312) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [45/49] time 0.534 (0.453) data 0.404 (0.323) loss_x loss_x 1.3955 (1.1557) acc_x 62.5000 (71.2500) lr 1.2334e-03 eta 0:00:01
epoch [87/200] batch [5/48] time 0.385 (0.454) data 0.255 (0.323) loss_u loss_u 0.7192 (0.7786) acc_u 37.5000 (30.6250) lr 1.2334e-03 eta 0:00:19
epoch [87/200] batch [10/48] time 0.337 (0.446) data 0.206 (0.316) loss_u loss_u 0.7759 (0.7646) acc_u 31.2500 (32.1875) lr 1.2334e-03 eta 0:00:16
epoch [87/200] batch [15/48] time 0.479 (0.442) data 0.348 (0.312) loss_u loss_u 0.7104 (0.7546) acc_u 34.3750 (31.8750) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [20/48] time 0.549 (0.444) data 0.418 (0.314) loss_u loss_u 0.8110 (0.7518) acc_u 28.1250 (32.1875) lr 1.2334e-03 eta 0:00:12
epoch [87/200] batch [25/48] time 0.450 (0.445) data 0.318 (0.315) loss_u loss_u 0.7007 (0.7517) acc_u 37.5000 (32.3750) lr 1.2334e-03 eta 0:00:10
epoch [87/200] batch [30/48] time 0.386 (0.440) data 0.254 (0.309) loss_u loss_u 0.8013 (0.7527) acc_u 28.1250 (32.2917) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [35/48] time 0.471 (0.438) data 0.337 (0.308) loss_u loss_u 0.7329 (0.7572) acc_u 40.6250 (31.9643) lr 1.2334e-03 eta 0:00:05
epoch [87/200] batch [40/48] time 0.474 (0.440) data 0.343 (0.309) loss_u loss_u 0.7871 (0.7576) acc_u 25.0000 (31.8750) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [45/48] time 0.448 (0.439) data 0.317 (0.308) loss_u loss_u 0.7930 (0.7583) acc_u 21.8750 (31.3194) lr 1.2334e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1380
confident_label rate tensor(0.5105, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1601
clean true:1586
clean false:15
clean_rate:0.9906308557151781
noisy true:170
noisy false:1365
after delete: len(clean_dataset) 1601
after delete: len(noisy_dataset) 1535
epoch [88/200] batch [5/50] time 0.439 (0.449) data 0.309 (0.319) loss_x loss_x 0.5601 (0.9900) acc_x 84.3750 (73.1250) lr 1.2181e-03 eta 0:00:20
epoch [88/200] batch [10/50] time 0.456 (0.460) data 0.326 (0.329) loss_x loss_x 1.3945 (1.0292) acc_x 53.1250 (71.2500) lr 1.2181e-03 eta 0:00:18
epoch [88/200] batch [15/50] time 0.566 (0.461) data 0.435 (0.331) loss_x loss_x 1.1191 (1.1106) acc_x 68.7500 (68.9583) lr 1.2181e-03 eta 0:00:16
epoch [88/200] batch [20/50] time 0.371 (0.457) data 0.241 (0.327) loss_x loss_x 0.8813 (1.1348) acc_x 78.1250 (68.9062) lr 1.2181e-03 eta 0:00:13
epoch [88/200] batch [25/50] time 0.486 (0.456) data 0.356 (0.326) loss_x loss_x 1.5859 (1.1844) acc_x 59.3750 (68.6250) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [30/50] time 0.329 (0.455) data 0.199 (0.324) loss_x loss_x 0.9224 (1.1970) acc_x 84.3750 (68.7500) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [35/50] time 0.440 (0.451) data 0.310 (0.321) loss_x loss_x 1.7705 (1.2184) acc_x 62.5000 (68.3929) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [40/50] time 0.485 (0.459) data 0.353 (0.328) loss_x loss_x 1.2002 (1.2031) acc_x 75.0000 (68.9062) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [45/50] time 0.461 (0.461) data 0.331 (0.331) loss_x loss_x 1.6992 (1.2253) acc_x 53.1250 (68.2639) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [50/50] time 0.460 (0.457) data 0.329 (0.326) loss_x loss_x 1.1094 (1.2179) acc_x 71.8750 (68.2500) lr 1.2181e-03 eta 0:00:00
epoch [88/200] batch [5/47] time 0.362 (0.451) data 0.231 (0.320) loss_u loss_u 0.7725 (0.7716) acc_u 40.6250 (31.2500) lr 1.2181e-03 eta 0:00:18
epoch [88/200] batch [10/47] time 0.552 (0.451) data 0.421 (0.320) loss_u loss_u 0.8467 (0.7674) acc_u 21.8750 (31.8750) lr 1.2181e-03 eta 0:00:16
epoch [88/200] batch [15/47] time 0.392 (0.449) data 0.261 (0.318) loss_u loss_u 0.8115 (0.7717) acc_u 28.1250 (30.2083) lr 1.2181e-03 eta 0:00:14
epoch [88/200] batch [20/47] time 0.373 (0.447) data 0.242 (0.316) loss_u loss_u 0.7559 (0.7730) acc_u 34.3750 (29.8438) lr 1.2181e-03 eta 0:00:12
epoch [88/200] batch [25/47] time 0.383 (0.443) data 0.252 (0.313) loss_u loss_u 0.7451 (0.7704) acc_u 31.2500 (30.6250) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [30/47] time 0.363 (0.441) data 0.232 (0.311) loss_u loss_u 0.7764 (0.7663) acc_u 34.3750 (30.9375) lr 1.2181e-03 eta 0:00:07
epoch [88/200] batch [35/47] time 0.383 (0.440) data 0.252 (0.309) loss_u loss_u 0.7529 (0.7676) acc_u 28.1250 (30.3571) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [40/47] time 0.515 (0.441) data 0.384 (0.310) loss_u loss_u 0.8291 (0.7692) acc_u 18.7500 (29.8438) lr 1.2181e-03 eta 0:00:03
epoch [88/200] batch [45/47] time 0.369 (0.440) data 0.237 (0.309) loss_u loss_u 0.5967 (0.7639) acc_u 56.2500 (30.6250) lr 1.2181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1430
confident_label rate tensor(0.4933, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1547
clean true:1533
clean false:14
clean_rate:0.9909502262443439
noisy true:173
noisy false:1416
after delete: len(clean_dataset) 1547
after delete: len(noisy_dataset) 1589
epoch [89/200] batch [5/48] time 0.476 (0.438) data 0.347 (0.308) loss_x loss_x 1.1357 (1.0387) acc_x 68.7500 (71.8750) lr 1.2028e-03 eta 0:00:18
epoch [89/200] batch [10/48] time 0.390 (0.451) data 0.259 (0.321) loss_x loss_x 0.6538 (1.0893) acc_x 81.2500 (70.9375) lr 1.2028e-03 eta 0:00:17
epoch [89/200] batch [15/48] time 0.698 (0.460) data 0.568 (0.330) loss_x loss_x 1.1748 (1.1451) acc_x 71.8750 (70.0000) lr 1.2028e-03 eta 0:00:15
epoch [89/200] batch [20/48] time 0.364 (0.453) data 0.235 (0.323) loss_x loss_x 0.6533 (1.1012) acc_x 84.3750 (71.7188) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [25/48] time 0.525 (0.459) data 0.395 (0.328) loss_x loss_x 0.8467 (1.1312) acc_x 84.3750 (71.7500) lr 1.2028e-03 eta 0:00:10
epoch [89/200] batch [30/48] time 0.434 (0.455) data 0.303 (0.324) loss_x loss_x 0.7295 (1.0982) acc_x 78.1250 (72.8125) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [35/48] time 0.526 (0.461) data 0.396 (0.331) loss_x loss_x 1.0176 (1.1117) acc_x 78.1250 (72.7679) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [40/48] time 0.419 (0.458) data 0.289 (0.327) loss_x loss_x 1.0732 (1.1149) acc_x 62.5000 (72.5000) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [45/48] time 0.503 (0.465) data 0.372 (0.334) loss_x loss_x 1.0098 (1.0959) acc_x 62.5000 (72.6389) lr 1.2028e-03 eta 0:00:01
epoch [89/200] batch [5/49] time 0.312 (0.464) data 0.181 (0.334) loss_u loss_u 0.7876 (0.7704) acc_u 31.2500 (31.2500) lr 1.2028e-03 eta 0:00:20
epoch [89/200] batch [10/49] time 0.445 (0.463) data 0.314 (0.332) loss_u loss_u 0.6709 (0.7548) acc_u 43.7500 (31.5625) lr 1.2028e-03 eta 0:00:18
epoch [89/200] batch [15/49] time 0.524 (0.461) data 0.393 (0.330) loss_u loss_u 0.6167 (0.7344) acc_u 43.7500 (34.3750) lr 1.2028e-03 eta 0:00:15
epoch [89/200] batch [20/49] time 0.354 (0.457) data 0.222 (0.326) loss_u loss_u 0.7744 (0.7377) acc_u 31.2500 (34.0625) lr 1.2028e-03 eta 0:00:13
epoch [89/200] batch [25/49] time 0.325 (0.453) data 0.194 (0.322) loss_u loss_u 0.8262 (0.7481) acc_u 28.1250 (32.3750) lr 1.2028e-03 eta 0:00:10
epoch [89/200] batch [30/49] time 0.464 (0.448) data 0.332 (0.318) loss_u loss_u 0.6265 (0.7476) acc_u 40.6250 (31.5625) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [35/49] time 0.389 (0.447) data 0.258 (0.316) loss_u loss_u 0.7363 (0.7484) acc_u 28.1250 (31.6964) lr 1.2028e-03 eta 0:00:06
epoch [89/200] batch [40/49] time 0.356 (0.447) data 0.225 (0.316) loss_u loss_u 0.7734 (0.7461) acc_u 37.5000 (32.1094) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [45/49] time 0.365 (0.445) data 0.234 (0.314) loss_u loss_u 0.7505 (0.7475) acc_u 31.2500 (31.8750) lr 1.2028e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1371
confident_label rate tensor(0.5150, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1615
clean true:1598
clean false:17
clean_rate:0.9894736842105263
noisy true:167
noisy false:1354
after delete: len(clean_dataset) 1615
after delete: len(noisy_dataset) 1521
epoch [90/200] batch [5/50] time 0.485 (0.431) data 0.354 (0.300) loss_x loss_x 1.2051 (1.1519) acc_x 71.8750 (71.2500) lr 1.1874e-03 eta 0:00:19
epoch [90/200] batch [10/50] time 0.391 (0.441) data 0.261 (0.311) loss_x loss_x 0.9248 (1.0891) acc_x 68.7500 (71.8750) lr 1.1874e-03 eta 0:00:17
epoch [90/200] batch [15/50] time 0.493 (0.453) data 0.363 (0.322) loss_x loss_x 1.0352 (1.1060) acc_x 75.0000 (71.8750) lr 1.1874e-03 eta 0:00:15
epoch [90/200] batch [20/50] time 0.582 (0.455) data 0.451 (0.324) loss_x loss_x 1.3555 (1.1631) acc_x 68.7500 (70.7812) lr 1.1874e-03 eta 0:00:13
epoch [90/200] batch [25/50] time 0.489 (0.450) data 0.359 (0.320) loss_x loss_x 1.2891 (1.1800) acc_x 71.8750 (69.8750) lr 1.1874e-03 eta 0:00:11
epoch [90/200] batch [30/50] time 0.588 (0.466) data 0.458 (0.335) loss_x loss_x 1.2637 (1.1928) acc_x 65.6250 (69.7917) lr 1.1874e-03 eta 0:00:09
epoch [90/200] batch [35/50] time 0.476 (0.462) data 0.346 (0.332) loss_x loss_x 0.6738 (1.1919) acc_x 87.5000 (69.8214) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [40/50] time 0.403 (0.456) data 0.273 (0.325) loss_x loss_x 1.0742 (1.1895) acc_x 75.0000 (69.7656) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [45/50] time 0.405 (0.454) data 0.274 (0.323) loss_x loss_x 1.8662 (1.1891) acc_x 56.2500 (70.0694) lr 1.1874e-03 eta 0:00:02
epoch [90/200] batch [50/50] time 0.480 (0.452) data 0.350 (0.321) loss_x loss_x 1.1025 (1.2176) acc_x 71.8750 (69.8125) lr 1.1874e-03 eta 0:00:00
epoch [90/200] batch [5/47] time 0.376 (0.449) data 0.245 (0.319) loss_u loss_u 0.7607 (0.7702) acc_u 28.1250 (28.1250) lr 1.1874e-03 eta 0:00:18
epoch [90/200] batch [10/47] time 0.519 (0.448) data 0.389 (0.317) loss_u loss_u 0.8188 (0.7514) acc_u 25.0000 (30.9375) lr 1.1874e-03 eta 0:00:16
epoch [90/200] batch [15/47] time 0.591 (0.447) data 0.461 (0.317) loss_u loss_u 0.8403 (0.7582) acc_u 15.6250 (29.1667) lr 1.1874e-03 eta 0:00:14
epoch [90/200] batch [20/47] time 0.368 (0.446) data 0.238 (0.315) loss_u loss_u 0.8276 (0.7719) acc_u 21.8750 (27.9688) lr 1.1874e-03 eta 0:00:12
epoch [90/200] batch [25/47] time 0.390 (0.443) data 0.259 (0.312) loss_u loss_u 0.7456 (0.7589) acc_u 31.2500 (29.5000) lr 1.1874e-03 eta 0:00:09
epoch [90/200] batch [30/47] time 0.421 (0.441) data 0.291 (0.310) loss_u loss_u 0.7632 (0.7657) acc_u 34.3750 (28.8542) lr 1.1874e-03 eta 0:00:07
epoch [90/200] batch [35/47] time 0.442 (0.442) data 0.311 (0.311) loss_u loss_u 0.7236 (0.7614) acc_u 31.2500 (29.6429) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [40/47] time 0.355 (0.439) data 0.224 (0.309) loss_u loss_u 0.7612 (0.7616) acc_u 28.1250 (29.2969) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [45/47] time 0.490 (0.439) data 0.359 (0.309) loss_u loss_u 0.6919 (0.7603) acc_u 34.3750 (29.5139) lr 1.1874e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1398
confident_label rate tensor(0.5032, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1578
clean true:1562
clean false:16
clean_rate:0.9898605830164765
noisy true:176
noisy false:1382
after delete: len(clean_dataset) 1578
after delete: len(noisy_dataset) 1558
epoch [91/200] batch [5/49] time 0.365 (0.453) data 0.234 (0.322) loss_x loss_x 2.1309 (1.2632) acc_x 62.5000 (74.3750) lr 1.1719e-03 eta 0:00:19
epoch [91/200] batch [10/49] time 0.396 (0.468) data 0.266 (0.337) loss_x loss_x 1.1045 (1.1253) acc_x 68.7500 (74.6875) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [15/49] time 0.562 (0.475) data 0.431 (0.344) loss_x loss_x 0.9990 (1.0810) acc_x 75.0000 (75.2083) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [20/49] time 0.481 (0.471) data 0.351 (0.340) loss_x loss_x 1.2148 (1.0806) acc_x 65.6250 (74.8438) lr 1.1719e-03 eta 0:00:13
epoch [91/200] batch [25/49] time 0.467 (0.460) data 0.336 (0.330) loss_x loss_x 0.8032 (1.0723) acc_x 75.0000 (74.5000) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [30/49] time 0.342 (0.452) data 0.212 (0.321) loss_x loss_x 1.2920 (1.1078) acc_x 56.2500 (73.0208) lr 1.1719e-03 eta 0:00:08
epoch [91/200] batch [35/49] time 0.461 (0.453) data 0.330 (0.322) loss_x loss_x 0.7588 (1.1157) acc_x 84.3750 (73.0357) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [40/49] time 0.426 (0.451) data 0.295 (0.320) loss_x loss_x 1.2197 (1.1382) acc_x 65.6250 (72.3438) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [45/49] time 0.519 (0.455) data 0.389 (0.324) loss_x loss_x 1.2734 (1.1405) acc_x 71.8750 (71.7361) lr 1.1719e-03 eta 0:00:01
epoch [91/200] batch [5/48] time 0.415 (0.447) data 0.283 (0.316) loss_u loss_u 0.8037 (0.7560) acc_u 28.1250 (33.1250) lr 1.1719e-03 eta 0:00:19
epoch [91/200] batch [10/48] time 0.433 (0.447) data 0.302 (0.316) loss_u loss_u 0.8677 (0.7660) acc_u 18.7500 (30.0000) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [15/48] time 0.382 (0.447) data 0.251 (0.317) loss_u loss_u 0.6543 (0.7611) acc_u 46.8750 (30.8333) lr 1.1719e-03 eta 0:00:14
epoch [91/200] batch [20/48] time 0.333 (0.449) data 0.202 (0.318) loss_u loss_u 0.7378 (0.7549) acc_u 28.1250 (31.4062) lr 1.1719e-03 eta 0:00:12
epoch [91/200] batch [25/48] time 0.418 (0.446) data 0.287 (0.315) loss_u loss_u 0.7930 (0.7602) acc_u 34.3750 (31.0000) lr 1.1719e-03 eta 0:00:10
epoch [91/200] batch [30/48] time 0.302 (0.441) data 0.171 (0.311) loss_u loss_u 0.7666 (0.7613) acc_u 28.1250 (30.8333) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [35/48] time 0.433 (0.442) data 0.302 (0.311) loss_u loss_u 0.7729 (0.7666) acc_u 21.8750 (29.7321) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [40/48] time 0.551 (0.445) data 0.420 (0.314) loss_u loss_u 0.7686 (0.7665) acc_u 25.0000 (29.2969) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [45/48] time 0.482 (0.444) data 0.351 (0.314) loss_u loss_u 0.8477 (0.7672) acc_u 21.8750 (29.3750) lr 1.1719e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1338
confident_label rate tensor(0.5201, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1631
clean true:1615
clean false:16
clean_rate:0.9901900674432863
noisy true:183
noisy false:1322
after delete: len(clean_dataset) 1631
after delete: len(noisy_dataset) 1505
epoch [92/200] batch [5/50] time 0.705 (0.510) data 0.574 (0.379) loss_x loss_x 1.0342 (1.1922) acc_x 68.7500 (72.5000) lr 1.1564e-03 eta 0:00:22
epoch [92/200] batch [10/50] time 0.547 (0.480) data 0.416 (0.349) loss_x loss_x 1.0840 (1.1468) acc_x 65.6250 (70.3125) lr 1.1564e-03 eta 0:00:19
epoch [92/200] batch [15/50] time 0.517 (0.480) data 0.386 (0.350) loss_x loss_x 1.1602 (1.1418) acc_x 71.8750 (68.9583) lr 1.1564e-03 eta 0:00:16
epoch [92/200] batch [20/50] time 0.553 (0.473) data 0.423 (0.343) loss_x loss_x 1.2881 (1.1049) acc_x 62.5000 (70.3125) lr 1.1564e-03 eta 0:00:14
epoch [92/200] batch [25/50] time 0.374 (0.463) data 0.243 (0.333) loss_x loss_x 1.6133 (1.1231) acc_x 56.2500 (70.1250) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [30/50] time 0.438 (0.462) data 0.308 (0.332) loss_x loss_x 1.1309 (1.1167) acc_x 75.0000 (70.9375) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [35/50] time 0.462 (0.459) data 0.331 (0.328) loss_x loss_x 1.6494 (1.1499) acc_x 65.6250 (70.5357) lr 1.1564e-03 eta 0:00:06
epoch [92/200] batch [40/50] time 0.482 (0.455) data 0.352 (0.325) loss_x loss_x 1.4316 (1.1791) acc_x 62.5000 (69.6875) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [45/50] time 0.435 (0.456) data 0.305 (0.325) loss_x loss_x 1.0830 (1.1885) acc_x 65.6250 (69.5139) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [50/50] time 0.428 (0.457) data 0.297 (0.326) loss_x loss_x 1.1689 (1.1856) acc_x 71.8750 (69.5000) lr 1.1564e-03 eta 0:00:00
epoch [92/200] batch [5/47] time 0.404 (0.461) data 0.272 (0.330) loss_u loss_u 0.8833 (0.8130) acc_u 9.3750 (22.5000) lr 1.1564e-03 eta 0:00:19
epoch [92/200] batch [10/47] time 0.431 (0.461) data 0.299 (0.330) loss_u loss_u 0.7959 (0.8101) acc_u 28.1250 (24.0625) lr 1.1564e-03 eta 0:00:17
epoch [92/200] batch [15/47] time 0.379 (0.454) data 0.248 (0.323) loss_u loss_u 0.7837 (0.8023) acc_u 34.3750 (25.8333) lr 1.1564e-03 eta 0:00:14
epoch [92/200] batch [20/47] time 0.457 (0.451) data 0.326 (0.321) loss_u loss_u 0.8091 (0.7989) acc_u 25.0000 (26.0938) lr 1.1564e-03 eta 0:00:12
epoch [92/200] batch [25/47] time 0.354 (0.447) data 0.223 (0.316) loss_u loss_u 0.7466 (0.7922) acc_u 31.2500 (27.0000) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [30/47] time 0.485 (0.444) data 0.354 (0.313) loss_u loss_u 0.6323 (0.7872) acc_u 46.8750 (27.2917) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [35/47] time 0.452 (0.443) data 0.322 (0.312) loss_u loss_u 0.8062 (0.7836) acc_u 25.0000 (27.6786) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [40/47] time 0.395 (0.440) data 0.264 (0.310) loss_u loss_u 0.7842 (0.7811) acc_u 37.5000 (28.3594) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [45/47] time 0.541 (0.442) data 0.409 (0.311) loss_u loss_u 0.8301 (0.7771) acc_u 18.7500 (29.0972) lr 1.1564e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1342
confident_label rate tensor(0.5188, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1627
clean true:1612
clean false:15
clean_rate:0.990780577750461
noisy true:182
noisy false:1327
after delete: len(clean_dataset) 1627
after delete: len(noisy_dataset) 1509
epoch [93/200] batch [5/50] time 0.416 (0.435) data 0.286 (0.304) loss_x loss_x 1.4082 (1.2553) acc_x 68.7500 (68.1250) lr 1.1409e-03 eta 0:00:19
epoch [93/200] batch [10/50] time 0.537 (0.461) data 0.407 (0.330) loss_x loss_x 1.4170 (1.3188) acc_x 59.3750 (65.0000) lr 1.1409e-03 eta 0:00:18
epoch [93/200] batch [15/50] time 0.507 (0.446) data 0.376 (0.315) loss_x loss_x 1.3633 (1.3399) acc_x 56.2500 (64.5833) lr 1.1409e-03 eta 0:00:15
epoch [93/200] batch [20/50] time 0.372 (0.446) data 0.242 (0.315) loss_x loss_x 0.8857 (1.2800) acc_x 78.1250 (67.1875) lr 1.1409e-03 eta 0:00:13
epoch [93/200] batch [25/50] time 0.521 (0.450) data 0.390 (0.320) loss_x loss_x 1.5107 (1.3052) acc_x 59.3750 (65.6250) lr 1.1409e-03 eta 0:00:11
epoch [93/200] batch [30/50] time 0.478 (0.450) data 0.348 (0.319) loss_x loss_x 0.9917 (1.2924) acc_x 78.1250 (65.7292) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [35/50] time 0.434 (0.445) data 0.304 (0.314) loss_x loss_x 0.8789 (1.2610) acc_x 84.3750 (66.4286) lr 1.1409e-03 eta 0:00:06
epoch [93/200] batch [40/50] time 0.436 (0.446) data 0.305 (0.316) loss_x loss_x 1.1123 (1.2529) acc_x 75.0000 (67.0312) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [45/50] time 0.472 (0.446) data 0.342 (0.316) loss_x loss_x 1.1973 (1.2450) acc_x 75.0000 (67.8472) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [50/50] time 0.373 (0.451) data 0.242 (0.321) loss_x loss_x 1.2305 (1.2424) acc_x 65.6250 (67.6875) lr 1.1409e-03 eta 0:00:00
epoch [93/200] batch [5/47] time 0.412 (0.450) data 0.281 (0.319) loss_u loss_u 0.7173 (0.8229) acc_u 40.6250 (23.7500) lr 1.1409e-03 eta 0:00:18
epoch [93/200] batch [10/47] time 0.330 (0.445) data 0.200 (0.315) loss_u loss_u 0.7114 (0.7791) acc_u 34.3750 (29.0625) lr 1.1409e-03 eta 0:00:16
epoch [93/200] batch [15/47] time 0.404 (0.451) data 0.272 (0.320) loss_u loss_u 0.7822 (0.7716) acc_u 25.0000 (30.4167) lr 1.1409e-03 eta 0:00:14
epoch [93/200] batch [20/47] time 0.511 (0.453) data 0.380 (0.322) loss_u loss_u 0.7095 (0.7649) acc_u 40.6250 (31.4062) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [25/47] time 0.414 (0.453) data 0.282 (0.322) loss_u loss_u 0.8496 (0.7669) acc_u 12.5000 (31.1250) lr 1.1409e-03 eta 0:00:09
epoch [93/200] batch [30/47] time 0.560 (0.453) data 0.428 (0.322) loss_u loss_u 0.7705 (0.7741) acc_u 28.1250 (29.4792) lr 1.1409e-03 eta 0:00:07
epoch [93/200] batch [35/47] time 0.450 (0.451) data 0.319 (0.321) loss_u loss_u 0.7900 (0.7766) acc_u 28.1250 (29.3750) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [40/47] time 0.415 (0.453) data 0.284 (0.322) loss_u loss_u 0.7461 (0.7766) acc_u 34.3750 (29.6094) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [45/47] time 0.346 (0.449) data 0.216 (0.318) loss_u loss_u 0.7451 (0.7744) acc_u 31.2500 (29.7917) lr 1.1409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1373
confident_label rate tensor(0.5143, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1613
clean true:1599
clean false:14
clean_rate:0.9913205207687539
noisy true:164
noisy false:1359
after delete: len(clean_dataset) 1613
after delete: len(noisy_dataset) 1523
epoch [94/200] batch [5/50] time 0.506 (0.435) data 0.376 (0.304) loss_x loss_x 1.3174 (1.1986) acc_x 65.6250 (67.5000) lr 1.1253e-03 eta 0:00:19
epoch [94/200] batch [10/50] time 0.423 (0.439) data 0.293 (0.309) loss_x loss_x 1.6084 (1.2298) acc_x 62.5000 (67.5000) lr 1.1253e-03 eta 0:00:17
epoch [94/200] batch [15/50] time 0.450 (0.439) data 0.320 (0.308) loss_x loss_x 1.0977 (1.2257) acc_x 68.7500 (68.5417) lr 1.1253e-03 eta 0:00:15
epoch [94/200] batch [20/50] time 0.453 (0.439) data 0.322 (0.308) loss_x loss_x 1.7930 (1.2161) acc_x 65.6250 (69.2188) lr 1.1253e-03 eta 0:00:13
epoch [94/200] batch [25/50] time 0.431 (0.442) data 0.300 (0.312) loss_x loss_x 1.1660 (1.2301) acc_x 65.6250 (69.1250) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [30/50] time 0.518 (0.453) data 0.388 (0.322) loss_x loss_x 1.2256 (1.2205) acc_x 68.7500 (69.3750) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [35/50] time 0.504 (0.452) data 0.373 (0.322) loss_x loss_x 1.3047 (1.1790) acc_x 68.7500 (70.4464) lr 1.1253e-03 eta 0:00:06
epoch [94/200] batch [40/50] time 0.417 (0.447) data 0.287 (0.317) loss_x loss_x 0.9546 (1.1587) acc_x 75.0000 (70.7031) lr 1.1253e-03 eta 0:00:04
epoch [94/200] batch [45/50] time 0.458 (0.453) data 0.327 (0.322) loss_x loss_x 0.8716 (1.1613) acc_x 75.0000 (70.7639) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [50/50] time 0.487 (0.447) data 0.356 (0.317) loss_x loss_x 1.4707 (1.1855) acc_x 53.1250 (70.1250) lr 1.1253e-03 eta 0:00:00
epoch [94/200] batch [5/47] time 0.433 (0.450) data 0.302 (0.320) loss_u loss_u 0.7681 (0.7222) acc_u 34.3750 (35.0000) lr 1.1253e-03 eta 0:00:18
epoch [94/200] batch [10/47] time 0.358 (0.445) data 0.227 (0.315) loss_u loss_u 0.8540 (0.7584) acc_u 21.8750 (30.6250) lr 1.1253e-03 eta 0:00:16
epoch [94/200] batch [15/47] time 0.345 (0.442) data 0.213 (0.312) loss_u loss_u 0.8940 (0.7732) acc_u 18.7500 (30.0000) lr 1.1253e-03 eta 0:00:14
epoch [94/200] batch [20/47] time 0.425 (0.442) data 0.294 (0.311) loss_u loss_u 0.7456 (0.7753) acc_u 28.1250 (28.7500) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [25/47] time 0.481 (0.442) data 0.349 (0.311) loss_u loss_u 0.7690 (0.7790) acc_u 25.0000 (28.8750) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [30/47] time 0.640 (0.445) data 0.509 (0.314) loss_u loss_u 0.7852 (0.7766) acc_u 28.1250 (29.1667) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [35/47] time 0.343 (0.442) data 0.212 (0.311) loss_u loss_u 0.7681 (0.7710) acc_u 31.2500 (30.0000) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [40/47] time 0.366 (0.440) data 0.235 (0.310) loss_u loss_u 0.7437 (0.7656) acc_u 28.1250 (30.6250) lr 1.1253e-03 eta 0:00:03
epoch [94/200] batch [45/47] time 0.471 (0.439) data 0.340 (0.308) loss_u loss_u 0.8193 (0.7677) acc_u 21.8750 (30.0000) lr 1.1253e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1356
confident_label rate tensor(0.5166, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1620
clean true:1601
clean false:19
clean_rate:0.9882716049382716
noisy true:179
noisy false:1337
after delete: len(clean_dataset) 1620
after delete: len(noisy_dataset) 1516
epoch [95/200] batch [5/50] time 0.360 (0.406) data 0.228 (0.275) loss_x loss_x 1.5254 (1.3365) acc_x 68.7500 (70.0000) lr 1.1097e-03 eta 0:00:18
epoch [95/200] batch [10/50] time 0.601 (0.443) data 0.470 (0.312) loss_x loss_x 1.5693 (1.2082) acc_x 75.0000 (73.4375) lr 1.1097e-03 eta 0:00:17
epoch [95/200] batch [15/50] time 0.382 (0.450) data 0.251 (0.319) loss_x loss_x 0.9565 (1.1899) acc_x 78.1250 (72.9167) lr 1.1097e-03 eta 0:00:15
epoch [95/200] batch [20/50] time 0.362 (0.438) data 0.232 (0.308) loss_x loss_x 0.9243 (1.1767) acc_x 75.0000 (72.0312) lr 1.1097e-03 eta 0:00:13
epoch [95/200] batch [25/50] time 0.441 (0.448) data 0.310 (0.317) loss_x loss_x 0.7397 (1.1238) acc_x 81.2500 (73.3750) lr 1.1097e-03 eta 0:00:11
epoch [95/200] batch [30/50] time 0.500 (0.451) data 0.369 (0.320) loss_x loss_x 0.7427 (1.1121) acc_x 81.2500 (72.9167) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [35/50] time 0.401 (0.457) data 0.270 (0.327) loss_x loss_x 0.6689 (1.1179) acc_x 81.2500 (72.4107) lr 1.1097e-03 eta 0:00:06
epoch [95/200] batch [40/50] time 0.419 (0.461) data 0.289 (0.330) loss_x loss_x 1.1582 (1.1403) acc_x 68.7500 (72.1094) lr 1.1097e-03 eta 0:00:04
epoch [95/200] batch [45/50] time 0.554 (0.459) data 0.424 (0.329) loss_x loss_x 0.9946 (1.1556) acc_x 71.8750 (71.8750) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [50/50] time 0.434 (0.454) data 0.303 (0.323) loss_x loss_x 1.1699 (1.1568) acc_x 68.7500 (71.6250) lr 1.1097e-03 eta 0:00:00
epoch [95/200] batch [5/47] time 0.500 (0.451) data 0.369 (0.321) loss_u loss_u 0.8057 (0.7367) acc_u 25.0000 (32.5000) lr 1.1097e-03 eta 0:00:18
epoch [95/200] batch [10/47] time 0.476 (0.453) data 0.345 (0.323) loss_u loss_u 0.7446 (0.7484) acc_u 28.1250 (31.5625) lr 1.1097e-03 eta 0:00:16
epoch [95/200] batch [15/47] time 0.432 (0.453) data 0.301 (0.322) loss_u loss_u 0.7900 (0.7497) acc_u 28.1250 (31.2500) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [20/47] time 0.448 (0.453) data 0.317 (0.322) loss_u loss_u 0.7202 (0.7549) acc_u 37.5000 (30.3125) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [25/47] time 0.398 (0.454) data 0.267 (0.324) loss_u loss_u 0.8311 (0.7594) acc_u 21.8750 (29.6250) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [30/47] time 0.580 (0.454) data 0.446 (0.323) loss_u loss_u 0.8081 (0.7552) acc_u 21.8750 (30.0000) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [35/47] time 0.618 (0.458) data 0.487 (0.327) loss_u loss_u 0.7739 (0.7586) acc_u 21.8750 (29.8214) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [40/47] time 0.469 (0.454) data 0.338 (0.323) loss_u loss_u 0.8135 (0.7595) acc_u 28.1250 (30.0781) lr 1.1097e-03 eta 0:00:03
epoch [95/200] batch [45/47] time 0.379 (0.452) data 0.248 (0.321) loss_u loss_u 0.7832 (0.7588) acc_u 28.1250 (30.1389) lr 1.1097e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1389
confident_label rate tensor(0.5041, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1581
clean true:1565
clean false:16
clean_rate:0.9898798228969007
noisy true:182
noisy false:1373
after delete: len(clean_dataset) 1581
after delete: len(noisy_dataset) 1555
epoch [96/200] batch [5/49] time 0.492 (0.442) data 0.362 (0.311) loss_x loss_x 1.3213 (1.2590) acc_x 71.8750 (71.2500) lr 1.0941e-03 eta 0:00:19
epoch [96/200] batch [10/49] time 0.418 (0.439) data 0.288 (0.309) loss_x loss_x 1.0400 (1.2622) acc_x 68.7500 (68.7500) lr 1.0941e-03 eta 0:00:17
epoch [96/200] batch [15/49] time 0.638 (0.474) data 0.507 (0.343) loss_x loss_x 1.6045 (1.2223) acc_x 65.6250 (69.5833) lr 1.0941e-03 eta 0:00:16
epoch [96/200] batch [20/49] time 0.390 (0.464) data 0.260 (0.334) loss_x loss_x 1.7354 (1.2476) acc_x 65.6250 (69.0625) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [25/49] time 0.366 (0.452) data 0.236 (0.321) loss_x loss_x 1.6162 (1.2247) acc_x 53.1250 (69.2500) lr 1.0941e-03 eta 0:00:10
epoch [96/200] batch [30/49] time 0.421 (0.457) data 0.291 (0.326) loss_x loss_x 1.4531 (1.2378) acc_x 59.3750 (68.8542) lr 1.0941e-03 eta 0:00:08
epoch [96/200] batch [35/49] time 0.447 (0.458) data 0.317 (0.328) loss_x loss_x 0.8989 (1.2277) acc_x 71.8750 (68.4821) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [40/49] time 0.303 (0.446) data 0.174 (0.316) loss_x loss_x 0.7686 (1.1981) acc_x 81.2500 (69.5312) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [45/49] time 0.441 (0.455) data 0.311 (0.324) loss_x loss_x 1.6875 (1.1947) acc_x 65.6250 (70.0000) lr 1.0941e-03 eta 0:00:01
epoch [96/200] batch [5/48] time 0.556 (0.454) data 0.426 (0.323) loss_u loss_u 0.7305 (0.7812) acc_u 28.1250 (25.0000) lr 1.0941e-03 eta 0:00:19
epoch [96/200] batch [10/48] time 0.356 (0.449) data 0.224 (0.319) loss_u loss_u 0.7666 (0.7833) acc_u 28.1250 (25.0000) lr 1.0941e-03 eta 0:00:17
epoch [96/200] batch [15/48] time 0.464 (0.446) data 0.334 (0.315) loss_u loss_u 0.5791 (0.7520) acc_u 53.1250 (29.7917) lr 1.0941e-03 eta 0:00:14
epoch [96/200] batch [20/48] time 0.328 (0.445) data 0.198 (0.315) loss_u loss_u 0.6636 (0.7464) acc_u 46.8750 (30.4688) lr 1.0941e-03 eta 0:00:12
epoch [96/200] batch [25/48] time 0.376 (0.441) data 0.244 (0.310) loss_u loss_u 0.8101 (0.7459) acc_u 21.8750 (30.8750) lr 1.0941e-03 eta 0:00:10
epoch [96/200] batch [30/48] time 0.417 (0.439) data 0.286 (0.308) loss_u loss_u 0.7764 (0.7532) acc_u 34.3750 (30.5208) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [35/48] time 0.392 (0.440) data 0.262 (0.310) loss_u loss_u 0.7446 (0.7528) acc_u 31.2500 (30.8036) lr 1.0941e-03 eta 0:00:05
epoch [96/200] batch [40/48] time 0.413 (0.440) data 0.282 (0.310) loss_u loss_u 0.7930 (0.7561) acc_u 28.1250 (30.7031) lr 1.0941e-03 eta 0:00:03
epoch [96/200] batch [45/48] time 0.363 (0.440) data 0.233 (0.309) loss_u loss_u 0.7783 (0.7586) acc_u 28.1250 (30.1389) lr 1.0941e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1361
confident_label rate tensor(0.5179, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1624
clean true:1612
clean false:12
clean_rate:0.9926108374384236
noisy true:163
noisy false:1349
after delete: len(clean_dataset) 1624
after delete: len(noisy_dataset) 1512
epoch [97/200] batch [5/50] time 0.460 (0.450) data 0.329 (0.320) loss_x loss_x 0.8032 (1.0341) acc_x 84.3750 (76.2500) lr 1.0785e-03 eta 0:00:20
epoch [97/200] batch [10/50] time 0.385 (0.448) data 0.255 (0.318) loss_x loss_x 0.8008 (1.0656) acc_x 90.6250 (73.1250) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [15/50] time 0.430 (0.440) data 0.300 (0.310) loss_x loss_x 1.3145 (1.0653) acc_x 65.6250 (73.7500) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [20/50] time 0.490 (0.443) data 0.359 (0.313) loss_x loss_x 1.1172 (1.0783) acc_x 65.6250 (73.2812) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [25/50] time 0.463 (0.448) data 0.332 (0.318) loss_x loss_x 1.0127 (1.0829) acc_x 78.1250 (73.3750) lr 1.0785e-03 eta 0:00:11
epoch [97/200] batch [30/50] time 0.489 (0.451) data 0.358 (0.320) loss_x loss_x 1.3965 (1.1028) acc_x 46.8750 (71.9792) lr 1.0785e-03 eta 0:00:09
epoch [97/200] batch [35/50] time 0.383 (0.447) data 0.252 (0.316) loss_x loss_x 0.8252 (1.1461) acc_x 78.1250 (71.0714) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [40/50] time 0.449 (0.445) data 0.319 (0.315) loss_x loss_x 0.7983 (1.1407) acc_x 78.1250 (71.0938) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [45/50] time 0.419 (0.440) data 0.288 (0.309) loss_x loss_x 0.9473 (1.1269) acc_x 75.0000 (71.5278) lr 1.0785e-03 eta 0:00:02
epoch [97/200] batch [50/50] time 0.448 (0.441) data 0.317 (0.310) loss_x loss_x 1.5596 (1.1199) acc_x 56.2500 (71.5625) lr 1.0785e-03 eta 0:00:00
epoch [97/200] batch [5/47] time 0.426 (0.442) data 0.294 (0.311) loss_u loss_u 0.7515 (0.7498) acc_u 34.3750 (31.2500) lr 1.0785e-03 eta 0:00:18
epoch [97/200] batch [10/47] time 0.567 (0.445) data 0.436 (0.314) loss_u loss_u 0.7344 (0.7621) acc_u 31.2500 (28.7500) lr 1.0785e-03 eta 0:00:16
epoch [97/200] batch [15/47] time 0.367 (0.442) data 0.237 (0.312) loss_u loss_u 0.7490 (0.7593) acc_u 31.2500 (29.5833) lr 1.0785e-03 eta 0:00:14
epoch [97/200] batch [20/47] time 0.443 (0.439) data 0.312 (0.309) loss_u loss_u 0.7329 (0.7674) acc_u 43.7500 (29.6875) lr 1.0785e-03 eta 0:00:11
epoch [97/200] batch [25/47] time 0.598 (0.439) data 0.468 (0.308) loss_u loss_u 0.7207 (0.7642) acc_u 43.7500 (30.6250) lr 1.0785e-03 eta 0:00:09
epoch [97/200] batch [30/47] time 0.435 (0.441) data 0.304 (0.311) loss_u loss_u 0.7646 (0.7698) acc_u 28.1250 (29.7917) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [35/47] time 0.574 (0.442) data 0.444 (0.312) loss_u loss_u 0.6489 (0.7616) acc_u 46.8750 (30.6250) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [40/47] time 0.477 (0.441) data 0.347 (0.310) loss_u loss_u 0.8164 (0.7649) acc_u 28.1250 (30.3125) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [45/47] time 0.371 (0.438) data 0.242 (0.308) loss_u loss_u 0.7202 (0.7653) acc_u 37.5000 (30.2778) lr 1.0785e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1328
confident_label rate tensor(0.5214, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1635
clean true:1623
clean false:12
clean_rate:0.9926605504587156
noisy true:185
noisy false:1316
after delete: len(clean_dataset) 1635
after delete: len(noisy_dataset) 1501
epoch [98/200] batch [5/51] time 0.614 (0.477) data 0.484 (0.347) loss_x loss_x 1.0195 (1.1726) acc_x 75.0000 (71.2500) lr 1.0628e-03 eta 0:00:21
epoch [98/200] batch [10/51] time 0.364 (0.453) data 0.234 (0.322) loss_x loss_x 1.9277 (1.2504) acc_x 56.2500 (70.0000) lr 1.0628e-03 eta 0:00:18
epoch [98/200] batch [15/51] time 0.468 (0.444) data 0.338 (0.314) loss_x loss_x 1.2861 (1.2917) acc_x 65.6250 (68.1250) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [20/51] time 0.347 (0.443) data 0.216 (0.312) loss_x loss_x 0.7822 (1.2234) acc_x 71.8750 (69.5312) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [25/51] time 0.422 (0.443) data 0.291 (0.313) loss_x loss_x 1.0840 (1.2255) acc_x 68.7500 (69.1250) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [30/51] time 0.542 (0.447) data 0.412 (0.317) loss_x loss_x 1.0576 (1.1898) acc_x 75.0000 (69.7917) lr 1.0628e-03 eta 0:00:09
epoch [98/200] batch [35/51] time 0.405 (0.449) data 0.275 (0.319) loss_x loss_x 1.1504 (1.1730) acc_x 75.0000 (70.5357) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [40/51] time 0.607 (0.451) data 0.476 (0.320) loss_x loss_x 0.6392 (1.1459) acc_x 78.1250 (70.5469) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [45/51] time 0.433 (0.448) data 0.302 (0.317) loss_x loss_x 1.4062 (1.1557) acc_x 68.7500 (70.6250) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [50/51] time 0.421 (0.449) data 0.291 (0.318) loss_x loss_x 1.1123 (1.1524) acc_x 81.2500 (71.0625) lr 1.0628e-03 eta 0:00:00
epoch [98/200] batch [5/46] time 0.356 (0.451) data 0.227 (0.321) loss_u loss_u 0.7646 (0.7597) acc_u 25.0000 (28.7500) lr 1.0628e-03 eta 0:00:18
epoch [98/200] batch [10/46] time 0.380 (0.450) data 0.249 (0.319) loss_u loss_u 0.8257 (0.7833) acc_u 34.3750 (27.8125) lr 1.0628e-03 eta 0:00:16
epoch [98/200] batch [15/46] time 0.473 (0.449) data 0.341 (0.319) loss_u loss_u 0.8154 (0.7802) acc_u 25.0000 (27.2917) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [20/46] time 0.463 (0.446) data 0.333 (0.315) loss_u loss_u 0.8101 (0.7788) acc_u 18.7500 (27.5000) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [25/46] time 0.430 (0.442) data 0.299 (0.312) loss_u loss_u 0.6670 (0.7707) acc_u 37.5000 (28.8750) lr 1.0628e-03 eta 0:00:09
epoch [98/200] batch [30/46] time 0.447 (0.447) data 0.315 (0.316) loss_u loss_u 0.7061 (0.7673) acc_u 37.5000 (29.2708) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [35/46] time 0.416 (0.445) data 0.285 (0.315) loss_u loss_u 0.7061 (0.7672) acc_u 34.3750 (29.1964) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [40/46] time 0.409 (0.444) data 0.278 (0.313) loss_u loss_u 0.7842 (0.7675) acc_u 28.1250 (29.2969) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [45/46] time 0.450 (0.441) data 0.319 (0.311) loss_u loss_u 0.7192 (0.7669) acc_u 40.6250 (29.6528) lr 1.0628e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1359
confident_label rate tensor(0.5163, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1619
clean true:1595
clean false:24
clean_rate:0.9851760345892526
noisy true:182
noisy false:1335
after delete: len(clean_dataset) 1619
after delete: len(noisy_dataset) 1517
epoch [99/200] batch [5/50] time 0.451 (0.444) data 0.320 (0.313) loss_x loss_x 1.6309 (1.2217) acc_x 59.3750 (71.2500) lr 1.0471e-03 eta 0:00:19
epoch [99/200] batch [10/50] time 0.458 (0.450) data 0.327 (0.319) loss_x loss_x 1.1436 (1.1724) acc_x 65.6250 (68.7500) lr 1.0471e-03 eta 0:00:17
epoch [99/200] batch [15/50] time 0.523 (0.466) data 0.393 (0.335) loss_x loss_x 1.1982 (1.1307) acc_x 75.0000 (70.6250) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [20/50] time 0.517 (0.469) data 0.387 (0.338) loss_x loss_x 0.9795 (1.1257) acc_x 71.8750 (71.2500) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [25/50] time 0.381 (0.451) data 0.250 (0.321) loss_x loss_x 1.8252 (1.1505) acc_x 56.2500 (70.7500) lr 1.0471e-03 eta 0:00:11
epoch [99/200] batch [30/50] time 0.521 (0.454) data 0.390 (0.324) loss_x loss_x 0.7832 (1.1351) acc_x 75.0000 (70.5208) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [35/50] time 0.414 (0.456) data 0.284 (0.325) loss_x loss_x 1.0010 (1.1125) acc_x 75.0000 (70.7143) lr 1.0471e-03 eta 0:00:06
epoch [99/200] batch [40/50] time 0.486 (0.454) data 0.356 (0.323) loss_x loss_x 0.6475 (1.0873) acc_x 84.3750 (71.4844) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [45/50] time 0.455 (0.452) data 0.325 (0.322) loss_x loss_x 0.8296 (1.0760) acc_x 84.3750 (72.1528) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [50/50] time 0.441 (0.451) data 0.310 (0.320) loss_x loss_x 1.0508 (1.0814) acc_x 68.7500 (71.8125) lr 1.0471e-03 eta 0:00:00
epoch [99/200] batch [5/47] time 0.363 (0.445) data 0.233 (0.315) loss_u loss_u 0.8032 (0.7583) acc_u 21.8750 (28.7500) lr 1.0471e-03 eta 0:00:18
epoch [99/200] batch [10/47] time 0.407 (0.440) data 0.275 (0.310) loss_u loss_u 0.8428 (0.7505) acc_u 18.7500 (29.6875) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [15/47] time 0.494 (0.446) data 0.363 (0.315) loss_u loss_u 0.7744 (0.7653) acc_u 34.3750 (29.7917) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [20/47] time 0.390 (0.446) data 0.259 (0.315) loss_u loss_u 0.8110 (0.7623) acc_u 31.2500 (30.7812) lr 1.0471e-03 eta 0:00:12
epoch [99/200] batch [25/47] time 0.507 (0.442) data 0.376 (0.311) loss_u loss_u 0.8174 (0.7677) acc_u 18.7500 (29.6250) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [30/47] time 0.354 (0.442) data 0.223 (0.312) loss_u loss_u 0.7798 (0.7659) acc_u 34.3750 (29.8958) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [35/47] time 0.465 (0.440) data 0.334 (0.309) loss_u loss_u 0.7090 (0.7626) acc_u 37.5000 (30.5357) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [40/47] time 0.500 (0.439) data 0.369 (0.308) loss_u loss_u 0.7681 (0.7638) acc_u 31.2500 (30.3125) lr 1.0471e-03 eta 0:00:03
epoch [99/200] batch [45/47] time 0.410 (0.437) data 0.279 (0.307) loss_u loss_u 0.7822 (0.7634) acc_u 21.8750 (30.1389) lr 1.0471e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1364
confident_label rate tensor(0.5159, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1618
clean true:1598
clean false:20
clean_rate:0.9876390605686032
noisy true:174
noisy false:1344
all clean rate:  [0.9803149606299213, 0.984375, 0.9863829787234043, 0.9844961240310077, 0.9873949579831933, 0.9924874791318865, 0.989448051948052, 0.9818631492168178, 0.9867083659108679, 0.9858490566037735, 0.9876948318293683, 0.9866457187745483, 0.9906542056074766, 0.9903147699757869, 0.991499227202473, 0.9901440485216073, 0.9914196567862714, 0.9874353288987435, 0.9913793103448276, 0.9910979228486647, 0.9939117199391172, 0.9912980420594634, 0.986870897155361, 0.9927431059506531, 0.9877344877344877, 0.9883805374001452, 0.9884393063583815, 0.9915373765867419, 0.9902234636871509, 0.9900920028308563, 0.9908771929824561, 0.9875173370319001, 0.9893692416725727, 0.9883720930232558, 0.989010989010989, 0.9897048730267674, 0.9894109861019192, 0.9893119572478289, 0.9894366197183099, 0.9871967654986523, 0.9887491727332892, 0.9876458476321208, 0.9873417721518988, 0.9859249329758714, 0.9927344782034346, 0.9861294583883752, 0.986737400530504, 0.9866310160427807, 0.9888670595939751, 0.9855926653569089, 0.9887491727332892, 0.9897959183673469, 0.9893119572478289, 0.9880557398805574, 0.9875246224556796, 0.9881578947368421, 0.9900990099009901, 0.9934123847167325, 0.9877813504823151, 0.9908616187989556, 0.9879032258064516, 0.9884910485933504, 0.9882506527415144, 0.989010989010989, 0.9870466321243523, 0.9871712636305324, 0.9891095451633568, 0.9889322916666666, 0.9890955740859525, 0.9897435897435898, 0.9905003166561115, 0.9891788669637174, 0.9884095299420477, 0.9916183107672469, 0.9896907216494846, 0.9917669411019633, 0.9908913467794405, 0.9866261398176291, 0.9893816364772018, 0.9903038138332256, 0.9860935524652339, 0.9881767268201618, 0.9917460317460317, 0.990530303030303, 0.9923027581783195, 0.9903536977491961, 0.9886435331230284, 0.9906308557151781, 0.9909502262443439, 0.9894736842105263, 0.9898605830164765, 0.9901900674432863, 0.990780577750461, 0.9913205207687539, 0.9882716049382716, 0.9898798228969007, 0.9926108374384236, 0.9926605504587156, 0.9851760345892526, 0.9876390605686032]
after delete: len(clean_dataset) 1618
after delete: len(noisy_dataset) 1518
epoch [100/200] batch [5/50] time 0.466 (0.437) data 0.335 (0.307) loss_x loss_x 0.9902 (1.1079) acc_x 78.1250 (73.1250) lr 1.0314e-03 eta 0:00:19
epoch [100/200] batch [10/50] time 0.416 (0.445) data 0.286 (0.314) loss_x loss_x 0.5215 (1.0456) acc_x 81.2500 (74.0625) lr 1.0314e-03 eta 0:00:17
epoch [100/200] batch [15/50] time 0.398 (0.437) data 0.268 (0.306) loss_x loss_x 0.8608 (1.0449) acc_x 78.1250 (74.5833) lr 1.0314e-03 eta 0:00:15
epoch [100/200] batch [20/50] time 0.427 (0.435) data 0.297 (0.304) loss_x loss_x 1.8311 (1.1171) acc_x 56.2500 (72.5000) lr 1.0314e-03 eta 0:00:13
epoch [100/200] batch [25/50] time 0.463 (0.434) data 0.332 (0.304) loss_x loss_x 1.5762 (1.1439) acc_x 62.5000 (71.7500) lr 1.0314e-03 eta 0:00:10
epoch [100/200] batch [30/50] time 0.382 (0.445) data 0.252 (0.315) loss_x loss_x 0.7188 (1.1148) acc_x 81.2500 (72.0833) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [35/50] time 0.357 (0.445) data 0.227 (0.314) loss_x loss_x 1.1309 (1.1370) acc_x 68.7500 (71.6071) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [40/50] time 0.395 (0.439) data 0.265 (0.309) loss_x loss_x 1.0273 (1.1214) acc_x 75.0000 (72.1094) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [45/50] time 0.491 (0.448) data 0.361 (0.317) loss_x loss_x 1.7773 (1.1634) acc_x 56.2500 (70.8333) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [50/50] time 0.480 (0.447) data 0.349 (0.317) loss_x loss_x 0.8623 (1.1480) acc_x 84.3750 (71.1250) lr 1.0314e-03 eta 0:00:00
epoch [100/200] batch [5/47] time 0.438 (0.446) data 0.307 (0.316) loss_u loss_u 0.7422 (0.7907) acc_u 25.0000 (26.8750) lr 1.0314e-03 eta 0:00:18
epoch [100/200] batch [10/47] time 0.345 (0.440) data 0.214 (0.309) loss_u loss_u 0.6016 (0.7604) acc_u 50.0000 (30.9375) lr 1.0314e-03 eta 0:00:16
epoch [100/200] batch [15/47] time 0.374 (0.438) data 0.244 (0.308) loss_u loss_u 0.7017 (0.7549) acc_u 40.6250 (31.6667) lr 1.0314e-03 eta 0:00:14
epoch [100/200] batch [20/47] time 0.425 (0.438) data 0.294 (0.307) loss_u loss_u 0.7637 (0.7590) acc_u 34.3750 (31.0938) lr 1.0314e-03 eta 0:00:11
epoch [100/200] batch [25/47] time 0.459 (0.439) data 0.328 (0.308) loss_u loss_u 0.8013 (0.7629) acc_u 31.2500 (30.8750) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [30/47] time 0.374 (0.437) data 0.243 (0.306) loss_u loss_u 0.8433 (0.7641) acc_u 18.7500 (30.8333) lr 1.0314e-03 eta 0:00:07
epoch [100/200] batch [35/47] time 0.481 (0.439) data 0.350 (0.309) loss_u loss_u 0.7710 (0.7688) acc_u 25.0000 (29.9107) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [40/47] time 0.548 (0.444) data 0.417 (0.313) loss_u loss_u 0.6958 (0.7689) acc_u 40.6250 (30.1562) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [45/47] time 0.370 (0.441) data 0.239 (0.310) loss_u loss_u 0.7583 (0.7712) acc_u 34.3750 (29.7222) lr 1.0314e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1392
confident_label rate tensor(0.5038, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1580
clean true:1564
clean false:16
clean_rate:0.9898734177215189
noisy true:180
noisy false:1376
after delete: len(clean_dataset) 1580
after delete: len(noisy_dataset) 1556
epoch [101/200] batch [5/49] time 0.485 (0.420) data 0.355 (0.290) loss_x loss_x 1.1318 (1.0589) acc_x 71.8750 (75.0000) lr 1.0157e-03 eta 0:00:18
epoch [101/200] batch [10/49] time 0.385 (0.411) data 0.255 (0.282) loss_x loss_x 1.0195 (1.0727) acc_x 78.1250 (73.4375) lr 1.0157e-03 eta 0:00:16
epoch [101/200] batch [15/49] time 0.351 (0.424) data 0.221 (0.294) loss_x loss_x 1.1230 (1.1393) acc_x 59.3750 (71.2500) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [20/49] time 0.429 (0.434) data 0.298 (0.304) loss_x loss_x 1.6660 (1.1221) acc_x 59.3750 (72.0312) lr 1.0157e-03 eta 0:00:12
epoch [101/200] batch [25/49] time 0.438 (0.445) data 0.308 (0.314) loss_x loss_x 1.3604 (1.1929) acc_x 62.5000 (70.6250) lr 1.0157e-03 eta 0:00:10
epoch [101/200] batch [30/49] time 0.492 (0.454) data 0.361 (0.324) loss_x loss_x 1.3613 (1.1993) acc_x 56.2500 (70.2083) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [35/49] time 0.399 (0.455) data 0.269 (0.325) loss_x loss_x 1.1846 (1.1743) acc_x 68.7500 (70.2679) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [40/49] time 0.446 (0.456) data 0.316 (0.326) loss_x loss_x 1.2705 (1.1816) acc_x 71.8750 (70.3125) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [45/49] time 0.473 (0.457) data 0.344 (0.327) loss_x loss_x 1.0127 (1.1679) acc_x 59.3750 (70.3472) lr 1.0157e-03 eta 0:00:01
epoch [101/200] batch [5/48] time 0.483 (0.451) data 0.353 (0.321) loss_u loss_u 0.7969 (0.7926) acc_u 31.2500 (31.2500) lr 1.0157e-03 eta 0:00:19
epoch [101/200] batch [10/48] time 0.442 (0.453) data 0.313 (0.323) loss_u loss_u 0.7329 (0.7681) acc_u 40.6250 (31.8750) lr 1.0157e-03 eta 0:00:17
epoch [101/200] batch [15/48] time 0.363 (0.446) data 0.233 (0.316) loss_u loss_u 0.8359 (0.7630) acc_u 15.6250 (31.4583) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [20/48] time 0.472 (0.442) data 0.342 (0.312) loss_u loss_u 0.7534 (0.7780) acc_u 34.3750 (29.8438) lr 1.0157e-03 eta 0:00:12
epoch [101/200] batch [25/48] time 0.350 (0.441) data 0.220 (0.311) loss_u loss_u 0.7896 (0.7802) acc_u 25.0000 (29.8750) lr 1.0157e-03 eta 0:00:10
epoch [101/200] batch [30/48] time 0.386 (0.436) data 0.256 (0.306) loss_u loss_u 0.7910 (0.7758) acc_u 18.7500 (30.0000) lr 1.0157e-03 eta 0:00:07
epoch [101/200] batch [35/48] time 0.388 (0.436) data 0.257 (0.306) loss_u loss_u 0.8301 (0.7756) acc_u 21.8750 (29.6429) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [40/48] time 0.434 (0.437) data 0.302 (0.307) loss_u loss_u 0.6948 (0.7698) acc_u 37.5000 (30.3125) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [45/48] time 0.386 (0.435) data 0.255 (0.305) loss_u loss_u 0.6772 (0.7692) acc_u 53.1250 (30.4861) lr 1.0157e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1358
confident_label rate tensor(0.5198, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1630
clean true:1609
clean false:21
clean_rate:0.9871165644171779
noisy true:169
noisy false:1337
after delete: len(clean_dataset) 1630
after delete: len(noisy_dataset) 1506
epoch [102/200] batch [5/50] time 0.464 (0.500) data 0.334 (0.369) loss_x loss_x 0.8877 (0.9880) acc_x 84.3750 (76.8750) lr 1.0000e-03 eta 0:00:22
epoch [102/200] batch [10/50] time 0.432 (0.471) data 0.302 (0.340) loss_x loss_x 1.6982 (1.1108) acc_x 65.6250 (72.8125) lr 1.0000e-03 eta 0:00:18
epoch [102/200] batch [15/50] time 0.404 (0.452) data 0.274 (0.321) loss_x loss_x 0.7847 (1.0941) acc_x 81.2500 (73.5417) lr 1.0000e-03 eta 0:00:15
epoch [102/200] batch [20/50] time 0.425 (0.462) data 0.295 (0.331) loss_x loss_x 1.4824 (1.1061) acc_x 53.1250 (72.5000) lr 1.0000e-03 eta 0:00:13
epoch [102/200] batch [25/50] time 0.444 (0.451) data 0.314 (0.321) loss_x loss_x 0.9434 (1.0933) acc_x 75.0000 (73.0000) lr 1.0000e-03 eta 0:00:11
epoch [102/200] batch [30/50] time 0.493 (0.451) data 0.363 (0.320) loss_x loss_x 1.1211 (1.1151) acc_x 68.7500 (71.8750) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [35/50] time 0.392 (0.448) data 0.262 (0.317) loss_x loss_x 1.6680 (1.1236) acc_x 68.7500 (71.6964) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [40/50] time 0.433 (0.447) data 0.302 (0.317) loss_x loss_x 1.4678 (1.1488) acc_x 62.5000 (71.0156) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [45/50] time 0.380 (0.445) data 0.249 (0.314) loss_x loss_x 1.1152 (1.1404) acc_x 71.8750 (71.3889) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [50/50] time 0.374 (0.442) data 0.244 (0.312) loss_x loss_x 1.0967 (1.1371) acc_x 75.0000 (71.0625) lr 1.0000e-03 eta 0:00:00
epoch [102/200] batch [5/47] time 0.417 (0.440) data 0.286 (0.309) loss_u loss_u 0.7124 (0.7545) acc_u 34.3750 (29.3750) lr 1.0000e-03 eta 0:00:18
epoch [102/200] batch [10/47] time 0.354 (0.437) data 0.223 (0.306) loss_u loss_u 0.7637 (0.7609) acc_u 34.3750 (28.7500) lr 1.0000e-03 eta 0:00:16
epoch [102/200] batch [15/47] time 0.549 (0.440) data 0.418 (0.309) loss_u loss_u 0.8047 (0.7688) acc_u 25.0000 (27.7083) lr 1.0000e-03 eta 0:00:14
epoch [102/200] batch [20/47] time 0.412 (0.439) data 0.281 (0.308) loss_u loss_u 0.7969 (0.7649) acc_u 21.8750 (28.1250) lr 1.0000e-03 eta 0:00:11
epoch [102/200] batch [25/47] time 0.361 (0.439) data 0.229 (0.308) loss_u loss_u 0.7969 (0.7722) acc_u 31.2500 (27.3750) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [30/47] time 0.385 (0.440) data 0.254 (0.310) loss_u loss_u 0.7344 (0.7738) acc_u 34.3750 (27.5000) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [35/47] time 0.487 (0.440) data 0.356 (0.309) loss_u loss_u 0.7949 (0.7730) acc_u 28.1250 (27.6786) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [40/47] time 0.553 (0.438) data 0.422 (0.307) loss_u loss_u 0.7866 (0.7745) acc_u 28.1250 (27.4219) lr 1.0000e-03 eta 0:00:03
epoch [102/200] batch [45/47] time 0.579 (0.439) data 0.447 (0.308) loss_u loss_u 0.7471 (0.7718) acc_u 34.3750 (27.7083) lr 1.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1336
confident_label rate tensor(0.5281, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1656
clean true:1632
clean false:24
clean_rate:0.9855072463768116
noisy true:168
noisy false:1312
after delete: len(clean_dataset) 1656
after delete: len(noisy_dataset) 1480
epoch [103/200] batch [5/51] time 0.433 (0.491) data 0.302 (0.360) loss_x loss_x 0.6812 (0.9326) acc_x 78.1250 (73.1250) lr 9.8429e-04 eta 0:00:22
epoch [103/200] batch [10/51] time 0.407 (0.472) data 0.276 (0.341) loss_x loss_x 1.5107 (1.1034) acc_x 56.2500 (70.6250) lr 9.8429e-04 eta 0:00:19
epoch [103/200] batch [15/51] time 0.457 (0.467) data 0.326 (0.336) loss_x loss_x 1.6055 (1.1535) acc_x 56.2500 (69.7917) lr 9.8429e-04 eta 0:00:16
epoch [103/200] batch [20/51] time 0.449 (0.452) data 0.318 (0.321) loss_x loss_x 1.5576 (1.1305) acc_x 59.3750 (70.1562) lr 9.8429e-04 eta 0:00:13
epoch [103/200] batch [25/51] time 0.552 (0.454) data 0.421 (0.323) loss_x loss_x 0.6172 (1.1403) acc_x 84.3750 (70.3750) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [30/51] time 0.399 (0.455) data 0.268 (0.325) loss_x loss_x 1.4629 (1.1624) acc_x 59.3750 (69.7917) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [35/51] time 0.500 (0.453) data 0.369 (0.323) loss_x loss_x 1.1436 (1.1839) acc_x 71.8750 (69.5536) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [40/51] time 0.498 (0.456) data 0.367 (0.325) loss_x loss_x 1.2334 (1.1728) acc_x 71.8750 (70.0000) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [45/51] time 0.488 (0.461) data 0.357 (0.331) loss_x loss_x 1.3945 (1.1678) acc_x 65.6250 (70.4861) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [50/51] time 0.365 (0.458) data 0.234 (0.327) loss_x loss_x 1.1504 (1.1732) acc_x 68.7500 (70.0000) lr 9.8429e-04 eta 0:00:00
epoch [103/200] batch [5/46] time 0.337 (0.451) data 0.206 (0.320) loss_u loss_u 0.7178 (0.7405) acc_u 34.3750 (36.2500) lr 9.8429e-04 eta 0:00:18
epoch [103/200] batch [10/46] time 0.394 (0.447) data 0.262 (0.316) loss_u loss_u 0.7598 (0.7518) acc_u 31.2500 (32.1875) lr 9.8429e-04 eta 0:00:16
epoch [103/200] batch [15/46] time 0.518 (0.453) data 0.387 (0.322) loss_u loss_u 0.7119 (0.7588) acc_u 43.7500 (31.0417) lr 9.8429e-04 eta 0:00:14
epoch [103/200] batch [20/46] time 0.332 (0.448) data 0.200 (0.317) loss_u loss_u 0.7256 (0.7608) acc_u 37.5000 (30.1562) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [25/46] time 0.408 (0.444) data 0.277 (0.313) loss_u loss_u 0.8647 (0.7686) acc_u 12.5000 (29.1250) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [30/46] time 0.439 (0.444) data 0.307 (0.313) loss_u loss_u 0.7944 (0.7767) acc_u 28.1250 (28.0208) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [35/46] time 0.429 (0.443) data 0.297 (0.312) loss_u loss_u 0.7676 (0.7694) acc_u 34.3750 (29.1964) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [40/46] time 0.686 (0.444) data 0.556 (0.313) loss_u loss_u 0.7363 (0.7725) acc_u 37.5000 (28.7500) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [45/46] time 0.350 (0.445) data 0.219 (0.313) loss_u loss_u 0.7646 (0.7759) acc_u 28.1250 (28.1944) lr 9.8429e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1381
confident_label rate tensor(0.5102, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1600
clean true:1584
clean false:16
clean_rate:0.99
noisy true:171
noisy false:1365
after delete: len(clean_dataset) 1600
after delete: len(noisy_dataset) 1536
epoch [104/200] batch [5/50] time 0.505 (0.488) data 0.374 (0.358) loss_x loss_x 0.9922 (1.1689) acc_x 71.8750 (70.6250) lr 9.6859e-04 eta 0:00:21
epoch [104/200] batch [10/50] time 0.479 (0.469) data 0.349 (0.338) loss_x loss_x 0.8149 (1.1571) acc_x 81.2500 (71.2500) lr 9.6859e-04 eta 0:00:18
epoch [104/200] batch [15/50] time 0.499 (0.471) data 0.368 (0.341) loss_x loss_x 0.6245 (1.1677) acc_x 78.1250 (70.4167) lr 9.6859e-04 eta 0:00:16
epoch [104/200] batch [20/50] time 0.426 (0.474) data 0.296 (0.344) loss_x loss_x 1.3398 (1.1768) acc_x 68.7500 (70.1562) lr 9.6859e-04 eta 0:00:14
epoch [104/200] batch [25/50] time 0.429 (0.469) data 0.298 (0.339) loss_x loss_x 1.5371 (1.1621) acc_x 59.3750 (70.1250) lr 9.6859e-04 eta 0:00:11
epoch [104/200] batch [30/50] time 0.417 (0.474) data 0.287 (0.344) loss_x loss_x 1.2949 (1.1425) acc_x 53.1250 (70.4167) lr 9.6859e-04 eta 0:00:09
epoch [104/200] batch [35/50] time 0.408 (0.467) data 0.278 (0.336) loss_x loss_x 1.2803 (1.1184) acc_x 62.5000 (70.8929) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [40/50] time 0.376 (0.470) data 0.246 (0.340) loss_x loss_x 1.1641 (1.1278) acc_x 68.7500 (70.5469) lr 9.6859e-04 eta 0:00:04
epoch [104/200] batch [45/50] time 0.441 (0.465) data 0.311 (0.335) loss_x loss_x 1.0713 (1.1216) acc_x 81.2500 (70.8333) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [50/50] time 0.383 (0.468) data 0.253 (0.338) loss_x loss_x 1.3672 (1.1200) acc_x 62.5000 (70.6875) lr 9.6859e-04 eta 0:00:00
epoch [104/200] batch [5/48] time 0.348 (0.470) data 0.216 (0.340) loss_u loss_u 0.7656 (0.7853) acc_u 34.3750 (28.1250) lr 9.6859e-04 eta 0:00:20
epoch [104/200] batch [10/48] time 0.439 (0.466) data 0.308 (0.336) loss_u loss_u 0.8008 (0.7907) acc_u 21.8750 (26.8750) lr 9.6859e-04 eta 0:00:17
epoch [104/200] batch [15/48] time 0.360 (0.458) data 0.229 (0.328) loss_u loss_u 0.7158 (0.7816) acc_u 34.3750 (27.7083) lr 9.6859e-04 eta 0:00:15
epoch [104/200] batch [20/48] time 0.380 (0.457) data 0.249 (0.326) loss_u loss_u 0.8154 (0.7790) acc_u 21.8750 (28.5938) lr 9.6859e-04 eta 0:00:12
epoch [104/200] batch [25/48] time 0.357 (0.456) data 0.225 (0.325) loss_u loss_u 0.6768 (0.7792) acc_u 43.7500 (28.0000) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [30/48] time 0.356 (0.452) data 0.225 (0.321) loss_u loss_u 0.8613 (0.7731) acc_u 15.6250 (29.0625) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [35/48] time 0.349 (0.451) data 0.218 (0.321) loss_u loss_u 0.8271 (0.7662) acc_u 21.8750 (30.2679) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [40/48] time 0.420 (0.448) data 0.289 (0.318) loss_u loss_u 0.7437 (0.7604) acc_u 28.1250 (30.7812) lr 9.6859e-04 eta 0:00:03
epoch [104/200] batch [45/48] time 0.430 (0.449) data 0.299 (0.318) loss_u loss_u 0.8179 (0.7596) acc_u 25.0000 (30.9028) lr 9.6859e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1350
confident_label rate tensor(0.5182, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1625
clean true:1604
clean false:21
clean_rate:0.9870769230769231
noisy true:182
noisy false:1329
after delete: len(clean_dataset) 1625
after delete: len(noisy_dataset) 1511
epoch [105/200] batch [5/50] time 0.445 (0.419) data 0.315 (0.288) loss_x loss_x 0.8188 (1.1081) acc_x 75.0000 (72.5000) lr 9.5289e-04 eta 0:00:18
epoch [105/200] batch [10/50] time 0.412 (0.423) data 0.282 (0.292) loss_x loss_x 1.5176 (1.1161) acc_x 56.2500 (71.2500) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [15/50] time 0.513 (0.429) data 0.382 (0.298) loss_x loss_x 1.2549 (1.1738) acc_x 75.0000 (70.6250) lr 9.5289e-04 eta 0:00:15
epoch [105/200] batch [20/50] time 0.429 (0.423) data 0.299 (0.292) loss_x loss_x 1.5859 (1.1758) acc_x 68.7500 (70.1562) lr 9.5289e-04 eta 0:00:12
epoch [105/200] batch [25/50] time 0.520 (0.433) data 0.389 (0.303) loss_x loss_x 1.2979 (1.2035) acc_x 65.6250 (68.8750) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [30/50] time 0.528 (0.438) data 0.397 (0.307) loss_x loss_x 1.1865 (1.1790) acc_x 75.0000 (70.3125) lr 9.5289e-04 eta 0:00:08
epoch [105/200] batch [35/50] time 0.403 (0.447) data 0.273 (0.316) loss_x loss_x 0.9917 (1.1862) acc_x 84.3750 (70.6250) lr 9.5289e-04 eta 0:00:06
epoch [105/200] batch [40/50] time 0.490 (0.448) data 0.359 (0.317) loss_x loss_x 1.1025 (1.1761) acc_x 75.0000 (71.1719) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [45/50] time 0.479 (0.445) data 0.349 (0.315) loss_x loss_x 1.5615 (1.1884) acc_x 65.6250 (70.7639) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [50/50] time 0.557 (0.450) data 0.426 (0.319) loss_x loss_x 0.6650 (1.1613) acc_x 78.1250 (71.3750) lr 9.5289e-04 eta 0:00:00
epoch [105/200] batch [5/47] time 0.472 (0.447) data 0.341 (0.316) loss_u loss_u 0.8408 (0.7699) acc_u 15.6250 (30.0000) lr 9.5289e-04 eta 0:00:18
epoch [105/200] batch [10/47] time 0.459 (0.448) data 0.328 (0.318) loss_u loss_u 0.7363 (0.7763) acc_u 31.2500 (27.5000) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [15/47] time 0.408 (0.445) data 0.276 (0.314) loss_u loss_u 0.7295 (0.7573) acc_u 34.3750 (30.4167) lr 9.5289e-04 eta 0:00:14
epoch [105/200] batch [20/47] time 0.408 (0.443) data 0.275 (0.312) loss_u loss_u 0.7939 (0.7741) acc_u 21.8750 (27.9688) lr 9.5289e-04 eta 0:00:11
epoch [105/200] batch [25/47] time 0.404 (0.441) data 0.273 (0.311) loss_u loss_u 0.8286 (0.7734) acc_u 21.8750 (28.1250) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [30/47] time 0.362 (0.442) data 0.231 (0.312) loss_u loss_u 0.7334 (0.7678) acc_u 34.3750 (29.0625) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [35/47] time 0.405 (0.444) data 0.274 (0.313) loss_u loss_u 0.7188 (0.7677) acc_u 34.3750 (29.4643) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [40/47] time 0.485 (0.441) data 0.353 (0.310) loss_u loss_u 0.7759 (0.7671) acc_u 34.3750 (30.0781) lr 9.5289e-04 eta 0:00:03
epoch [105/200] batch [45/47] time 0.454 (0.442) data 0.322 (0.311) loss_u loss_u 0.7563 (0.7666) acc_u 31.2500 (30.0000) lr 9.5289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1343
confident_label rate tensor(0.5252, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1647
clean true:1629
clean false:18
clean_rate:0.9890710382513661
noisy true:164
noisy false:1325
after delete: len(clean_dataset) 1647
after delete: len(noisy_dataset) 1489
epoch [106/200] batch [5/51] time 0.641 (0.526) data 0.510 (0.395) loss_x loss_x 1.7305 (1.0889) acc_x 65.6250 (77.5000) lr 9.3721e-04 eta 0:00:24
epoch [106/200] batch [10/51] time 0.391 (0.469) data 0.261 (0.339) loss_x loss_x 1.5146 (1.0877) acc_x 50.0000 (76.2500) lr 9.3721e-04 eta 0:00:19
epoch [106/200] batch [15/51] time 0.497 (0.462) data 0.367 (0.332) loss_x loss_x 1.4688 (1.1361) acc_x 59.3750 (73.7500) lr 9.3721e-04 eta 0:00:16
epoch [106/200] batch [20/51] time 0.483 (0.464) data 0.352 (0.334) loss_x loss_x 0.6274 (1.1274) acc_x 84.3750 (73.7500) lr 9.3721e-04 eta 0:00:14
epoch [106/200] batch [25/51] time 0.470 (0.465) data 0.339 (0.335) loss_x loss_x 1.1650 (1.1025) acc_x 75.0000 (73.7500) lr 9.3721e-04 eta 0:00:12
epoch [106/200] batch [30/51] time 0.383 (0.454) data 0.253 (0.323) loss_x loss_x 1.2861 (1.1291) acc_x 62.5000 (72.9167) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [35/51] time 0.389 (0.455) data 0.259 (0.324) loss_x loss_x 1.0137 (1.1130) acc_x 65.6250 (73.0357) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [40/51] time 0.529 (0.456) data 0.399 (0.326) loss_x loss_x 1.6211 (1.1250) acc_x 53.1250 (72.1875) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [45/51] time 0.416 (0.453) data 0.286 (0.323) loss_x loss_x 0.8477 (1.1222) acc_x 68.7500 (71.5278) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [50/51] time 0.559 (0.454) data 0.429 (0.324) loss_x loss_x 1.1475 (1.1137) acc_x 68.7500 (72.0000) lr 9.3721e-04 eta 0:00:00
epoch [106/200] batch [5/46] time 0.430 (0.454) data 0.299 (0.324) loss_u loss_u 0.6958 (0.7536) acc_u 46.8750 (30.0000) lr 9.3721e-04 eta 0:00:18
epoch [106/200] batch [10/46] time 0.413 (0.454) data 0.281 (0.323) loss_u loss_u 0.7241 (0.7514) acc_u 43.7500 (30.9375) lr 9.3721e-04 eta 0:00:16
epoch [106/200] batch [15/46] time 0.651 (0.456) data 0.520 (0.325) loss_u loss_u 0.8481 (0.7747) acc_u 25.0000 (27.9167) lr 9.3721e-04 eta 0:00:14
epoch [106/200] batch [20/46] time 0.399 (0.452) data 0.265 (0.321) loss_u loss_u 0.7373 (0.7740) acc_u 31.2500 (28.2812) lr 9.3721e-04 eta 0:00:11
epoch [106/200] batch [25/46] time 0.364 (0.451) data 0.233 (0.320) loss_u loss_u 0.8340 (0.7841) acc_u 15.6250 (26.6250) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [30/46] time 0.506 (0.451) data 0.374 (0.320) loss_u loss_u 0.7075 (0.7778) acc_u 37.5000 (27.2917) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [35/46] time 0.438 (0.448) data 0.307 (0.317) loss_u loss_u 0.7666 (0.7730) acc_u 25.0000 (28.1250) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [40/46] time 0.412 (0.447) data 0.281 (0.316) loss_u loss_u 0.6680 (0.7715) acc_u 37.5000 (27.7344) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [45/46] time 0.352 (0.445) data 0.220 (0.314) loss_u loss_u 0.8115 (0.7709) acc_u 25.0000 (27.7778) lr 9.3721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1331
confident_label rate tensor(0.5217, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1636
clean true:1620
clean false:16
clean_rate:0.9902200488997555
noisy true:185
noisy false:1315
after delete: len(clean_dataset) 1636
after delete: len(noisy_dataset) 1500
epoch [107/200] batch [5/51] time 0.451 (0.478) data 0.320 (0.347) loss_x loss_x 1.0732 (0.9558) acc_x 81.2500 (80.0000) lr 9.2154e-04 eta 0:00:21
epoch [107/200] batch [10/51] time 0.621 (0.491) data 0.490 (0.360) loss_x loss_x 1.3936 (1.0091) acc_x 65.6250 (76.8750) lr 9.2154e-04 eta 0:00:20
epoch [107/200] batch [15/51] time 0.371 (0.473) data 0.240 (0.343) loss_x loss_x 1.0361 (1.0470) acc_x 71.8750 (75.4167) lr 9.2154e-04 eta 0:00:17
epoch [107/200] batch [20/51] time 0.477 (0.463) data 0.347 (0.332) loss_x loss_x 1.6465 (1.0942) acc_x 50.0000 (73.5938) lr 9.2154e-04 eta 0:00:14
epoch [107/200] batch [25/51] time 0.423 (0.460) data 0.293 (0.330) loss_x loss_x 1.6484 (1.1714) acc_x 62.5000 (71.2500) lr 9.2154e-04 eta 0:00:11
epoch [107/200] batch [30/51] time 0.405 (0.451) data 0.275 (0.320) loss_x loss_x 0.7480 (1.1672) acc_x 87.5000 (71.6667) lr 9.2154e-04 eta 0:00:09
epoch [107/200] batch [35/51] time 0.405 (0.442) data 0.274 (0.312) loss_x loss_x 1.5303 (1.1962) acc_x 65.6250 (71.1607) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [40/51] time 0.518 (0.444) data 0.387 (0.313) loss_x loss_x 1.1904 (1.1803) acc_x 71.8750 (71.7969) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [45/51] time 0.481 (0.447) data 0.351 (0.316) loss_x loss_x 0.6011 (1.1679) acc_x 84.3750 (71.8750) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [50/51] time 0.441 (0.448) data 0.310 (0.318) loss_x loss_x 0.7622 (1.1547) acc_x 81.2500 (71.8125) lr 9.2154e-04 eta 0:00:00
epoch [107/200] batch [5/46] time 0.341 (0.441) data 0.210 (0.310) loss_u loss_u 0.7534 (0.7708) acc_u 31.2500 (26.8750) lr 9.2154e-04 eta 0:00:18
epoch [107/200] batch [10/46] time 0.509 (0.442) data 0.378 (0.312) loss_u loss_u 0.7412 (0.7695) acc_u 25.0000 (26.5625) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [15/46] time 0.414 (0.444) data 0.283 (0.313) loss_u loss_u 0.7700 (0.7717) acc_u 25.0000 (26.8750) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [20/46] time 0.417 (0.447) data 0.287 (0.316) loss_u loss_u 0.8330 (0.7698) acc_u 15.6250 (27.5000) lr 9.2154e-04 eta 0:00:11
epoch [107/200] batch [25/46] time 0.319 (0.447) data 0.188 (0.316) loss_u loss_u 0.8706 (0.7685) acc_u 12.5000 (28.0000) lr 9.2154e-04 eta 0:00:09
epoch [107/200] batch [30/46] time 0.470 (0.446) data 0.340 (0.315) loss_u loss_u 0.8052 (0.7663) acc_u 21.8750 (28.3333) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [35/46] time 0.389 (0.443) data 0.258 (0.313) loss_u loss_u 0.7729 (0.7656) acc_u 31.2500 (28.3929) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [40/46] time 0.416 (0.448) data 0.284 (0.317) loss_u loss_u 0.7788 (0.7644) acc_u 18.7500 (28.5938) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [45/46] time 0.470 (0.446) data 0.339 (0.315) loss_u loss_u 0.7983 (0.7666) acc_u 21.8750 (28.1944) lr 9.2154e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1367
confident_label rate tensor(0.5150, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1615
clean true:1593
clean false:22
clean_rate:0.9863777089783282
noisy true:176
noisy false:1345
after delete: len(clean_dataset) 1615
after delete: len(noisy_dataset) 1521
epoch [108/200] batch [5/50] time 0.512 (0.445) data 0.381 (0.314) loss_x loss_x 1.0352 (1.1998) acc_x 75.0000 (71.2500) lr 9.0589e-04 eta 0:00:20
epoch [108/200] batch [10/50] time 0.377 (0.455) data 0.247 (0.324) loss_x loss_x 1.4473 (1.1378) acc_x 56.2500 (71.5625) lr 9.0589e-04 eta 0:00:18
epoch [108/200] batch [15/50] time 0.391 (0.449) data 0.260 (0.318) loss_x loss_x 1.3652 (1.1388) acc_x 65.6250 (71.0417) lr 9.0589e-04 eta 0:00:15
epoch [108/200] batch [20/50] time 0.476 (0.446) data 0.346 (0.315) loss_x loss_x 1.4160 (1.2342) acc_x 62.5000 (68.4375) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [25/50] time 0.453 (0.451) data 0.322 (0.320) loss_x loss_x 0.8843 (1.1711) acc_x 75.0000 (69.6250) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [30/50] time 0.651 (0.450) data 0.520 (0.320) loss_x loss_x 1.2568 (1.1739) acc_x 78.1250 (69.6875) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [35/50] time 0.405 (0.457) data 0.275 (0.327) loss_x loss_x 1.1543 (1.1692) acc_x 75.0000 (70.3571) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [40/50] time 0.476 (0.454) data 0.346 (0.324) loss_x loss_x 0.8394 (1.1486) acc_x 71.8750 (70.6250) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [45/50] time 0.440 (0.451) data 0.309 (0.320) loss_x loss_x 0.9619 (1.1493) acc_x 65.6250 (70.4167) lr 9.0589e-04 eta 0:00:02
epoch [108/200] batch [50/50] time 0.491 (0.451) data 0.361 (0.321) loss_x loss_x 0.9766 (1.1469) acc_x 71.8750 (70.5625) lr 9.0589e-04 eta 0:00:00
epoch [108/200] batch [5/47] time 0.482 (0.459) data 0.349 (0.328) loss_u loss_u 0.7622 (0.7406) acc_u 28.1250 (33.7500) lr 9.0589e-04 eta 0:00:19
epoch [108/200] batch [10/47] time 0.440 (0.455) data 0.308 (0.324) loss_u loss_u 0.7944 (0.7540) acc_u 25.0000 (30.9375) lr 9.0589e-04 eta 0:00:16
epoch [108/200] batch [15/47] time 0.386 (0.451) data 0.255 (0.320) loss_u loss_u 0.8110 (0.7609) acc_u 21.8750 (29.7917) lr 9.0589e-04 eta 0:00:14
epoch [108/200] batch [20/47] time 0.446 (0.451) data 0.314 (0.320) loss_u loss_u 0.8442 (0.7639) acc_u 18.7500 (29.8438) lr 9.0589e-04 eta 0:00:12
epoch [108/200] batch [25/47] time 0.375 (0.451) data 0.244 (0.320) loss_u loss_u 0.7422 (0.7604) acc_u 43.7500 (31.2500) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [30/47] time 0.531 (0.450) data 0.400 (0.319) loss_u loss_u 0.7334 (0.7558) acc_u 34.3750 (32.0833) lr 9.0589e-04 eta 0:00:07
epoch [108/200] batch [35/47] time 0.407 (0.448) data 0.276 (0.317) loss_u loss_u 0.7803 (0.7574) acc_u 34.3750 (31.8750) lr 9.0589e-04 eta 0:00:05
epoch [108/200] batch [40/47] time 0.335 (0.443) data 0.204 (0.313) loss_u loss_u 0.8213 (0.7557) acc_u 25.0000 (32.1094) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [45/47] time 0.384 (0.440) data 0.253 (0.309) loss_u loss_u 0.7505 (0.7558) acc_u 31.2500 (32.0139) lr 9.0589e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1338
confident_label rate tensor(0.5242, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1644
clean true:1627
clean false:17
clean_rate:0.9896593673965937
noisy true:171
noisy false:1321
after delete: len(clean_dataset) 1644
after delete: len(noisy_dataset) 1492
epoch [109/200] batch [5/51] time 0.391 (0.446) data 0.261 (0.316) loss_x loss_x 0.8677 (1.0453) acc_x 68.7500 (73.1250) lr 8.9027e-04 eta 0:00:20
epoch [109/200] batch [10/51] time 0.348 (0.444) data 0.219 (0.314) loss_x loss_x 1.3770 (1.1728) acc_x 62.5000 (70.6250) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [15/51] time 0.404 (0.463) data 0.272 (0.333) loss_x loss_x 0.8755 (1.1039) acc_x 75.0000 (72.7083) lr 8.9027e-04 eta 0:00:16
epoch [109/200] batch [20/51] time 0.473 (0.454) data 0.343 (0.323) loss_x loss_x 1.4189 (1.1482) acc_x 68.7500 (72.1875) lr 8.9027e-04 eta 0:00:14
epoch [109/200] batch [25/51] time 0.531 (0.447) data 0.401 (0.317) loss_x loss_x 0.8569 (1.1316) acc_x 78.1250 (72.5000) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [30/51] time 0.363 (0.448) data 0.234 (0.318) loss_x loss_x 1.2168 (1.1580) acc_x 62.5000 (71.8750) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [35/51] time 0.584 (0.456) data 0.453 (0.326) loss_x loss_x 1.3223 (1.2132) acc_x 62.5000 (70.1786) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [40/51] time 0.509 (0.457) data 0.379 (0.327) loss_x loss_x 0.9995 (1.1936) acc_x 78.1250 (71.0938) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [45/51] time 0.459 (0.457) data 0.329 (0.327) loss_x loss_x 0.7017 (1.1977) acc_x 84.3750 (70.9722) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [50/51] time 0.461 (0.457) data 0.332 (0.327) loss_x loss_x 0.9629 (1.1902) acc_x 75.0000 (70.9375) lr 8.9027e-04 eta 0:00:00
epoch [109/200] batch [5/46] time 0.427 (0.448) data 0.296 (0.318) loss_u loss_u 0.8232 (0.7607) acc_u 15.6250 (28.1250) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [10/46] time 0.427 (0.442) data 0.296 (0.312) loss_u loss_u 0.7739 (0.7603) acc_u 28.1250 (29.3750) lr 8.9027e-04 eta 0:00:15
epoch [109/200] batch [15/46] time 0.474 (0.442) data 0.343 (0.312) loss_u loss_u 0.7700 (0.7611) acc_u 21.8750 (28.5417) lr 8.9027e-04 eta 0:00:13
epoch [109/200] batch [20/46] time 0.449 (0.442) data 0.319 (0.312) loss_u loss_u 0.7275 (0.7615) acc_u 37.5000 (28.7500) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [25/46] time 0.397 (0.438) data 0.266 (0.307) loss_u loss_u 0.7861 (0.7616) acc_u 25.0000 (29.2500) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [30/46] time 0.484 (0.437) data 0.353 (0.307) loss_u loss_u 0.7109 (0.7612) acc_u 37.5000 (29.2708) lr 8.9027e-04 eta 0:00:06
epoch [109/200] batch [35/46] time 0.467 (0.439) data 0.336 (0.308) loss_u loss_u 0.7480 (0.7649) acc_u 31.2500 (29.0179) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [40/46] time 0.355 (0.439) data 0.224 (0.309) loss_u loss_u 0.7017 (0.7641) acc_u 37.5000 (29.3750) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [45/46] time 0.542 (0.439) data 0.411 (0.309) loss_u loss_u 0.8203 (0.7629) acc_u 25.0000 (29.7222) lr 8.9027e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1322
confident_label rate tensor(0.5277, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1655
clean true:1632
clean false:23
clean_rate:0.9861027190332327
noisy true:182
noisy false:1299
after delete: len(clean_dataset) 1655
after delete: len(noisy_dataset) 1481
epoch [110/200] batch [5/51] time 0.403 (0.421) data 0.273 (0.291) loss_x loss_x 0.5986 (1.1410) acc_x 84.3750 (72.5000) lr 8.7467e-04 eta 0:00:19
epoch [110/200] batch [10/51] time 0.707 (0.466) data 0.576 (0.335) loss_x loss_x 0.8296 (1.0801) acc_x 75.0000 (72.5000) lr 8.7467e-04 eta 0:00:19
epoch [110/200] batch [15/51] time 0.420 (0.446) data 0.290 (0.315) loss_x loss_x 0.8774 (1.1115) acc_x 84.3750 (72.0833) lr 8.7467e-04 eta 0:00:16
epoch [110/200] batch [20/51] time 0.425 (0.442) data 0.295 (0.311) loss_x loss_x 1.0615 (1.1286) acc_x 78.1250 (71.7188) lr 8.7467e-04 eta 0:00:13
epoch [110/200] batch [25/51] time 0.359 (0.443) data 0.229 (0.313) loss_x loss_x 1.3477 (1.1300) acc_x 68.7500 (71.8750) lr 8.7467e-04 eta 0:00:11
epoch [110/200] batch [30/51] time 0.409 (0.441) data 0.279 (0.310) loss_x loss_x 1.0469 (1.1187) acc_x 71.8750 (72.5000) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [35/51] time 0.467 (0.441) data 0.336 (0.310) loss_x loss_x 1.4893 (1.1338) acc_x 62.5000 (72.0536) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [40/51] time 0.423 (0.441) data 0.292 (0.311) loss_x loss_x 1.4033 (1.1589) acc_x 56.2500 (71.1719) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [45/51] time 0.422 (0.452) data 0.292 (0.321) loss_x loss_x 0.9565 (1.1450) acc_x 68.7500 (71.4583) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [50/51] time 0.509 (0.453) data 0.379 (0.323) loss_x loss_x 1.6426 (1.1460) acc_x 65.6250 (70.9375) lr 8.7467e-04 eta 0:00:00
epoch [110/200] batch [5/46] time 0.400 (0.447) data 0.269 (0.317) loss_u loss_u 0.7637 (0.7967) acc_u 37.5000 (27.5000) lr 8.7467e-04 eta 0:00:18
epoch [110/200] batch [10/46] time 0.513 (0.455) data 0.382 (0.324) loss_u loss_u 0.5645 (0.7859) acc_u 56.2500 (28.7500) lr 8.7467e-04 eta 0:00:16
epoch [110/200] batch [15/46] time 0.436 (0.455) data 0.305 (0.324) loss_u loss_u 0.7974 (0.7931) acc_u 21.8750 (26.2500) lr 8.7467e-04 eta 0:00:14
epoch [110/200] batch [20/46] time 0.361 (0.452) data 0.230 (0.321) loss_u loss_u 0.8154 (0.7928) acc_u 25.0000 (26.2500) lr 8.7467e-04 eta 0:00:11
epoch [110/200] batch [25/46] time 0.405 (0.448) data 0.273 (0.317) loss_u loss_u 0.6499 (0.7750) acc_u 46.8750 (28.3750) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [30/46] time 0.314 (0.445) data 0.183 (0.314) loss_u loss_u 0.7109 (0.7735) acc_u 31.2500 (28.6458) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [35/46] time 0.397 (0.443) data 0.266 (0.312) loss_u loss_u 0.8164 (0.7730) acc_u 25.0000 (28.7500) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [40/46] time 0.456 (0.445) data 0.325 (0.314) loss_u loss_u 0.7080 (0.7694) acc_u 37.5000 (28.7500) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [45/46] time 0.360 (0.441) data 0.228 (0.310) loss_u loss_u 0.7271 (0.7702) acc_u 34.3750 (29.0278) lr 8.7467e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1307
confident_label rate tensor(0.5252, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1647
clean true:1630
clean false:17
clean_rate:0.9896782027929569
noisy true:199
noisy false:1290
after delete: len(clean_dataset) 1647
after delete: len(noisy_dataset) 1489
epoch [111/200] batch [5/51] time 0.494 (0.479) data 0.363 (0.349) loss_x loss_x 1.2285 (1.1252) acc_x 68.7500 (71.8750) lr 8.5910e-04 eta 0:00:22
epoch [111/200] batch [10/51] time 0.423 (0.458) data 0.293 (0.328) loss_x loss_x 1.0850 (1.0950) acc_x 65.6250 (72.1875) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [15/51] time 0.454 (0.444) data 0.324 (0.314) loss_x loss_x 1.0508 (1.0761) acc_x 71.8750 (72.9167) lr 8.5910e-04 eta 0:00:15
epoch [111/200] batch [20/51] time 0.482 (0.456) data 0.352 (0.326) loss_x loss_x 1.0244 (1.0563) acc_x 65.6250 (71.5625) lr 8.5910e-04 eta 0:00:14
epoch [111/200] batch [25/51] time 0.445 (0.451) data 0.315 (0.321) loss_x loss_x 0.9155 (1.0672) acc_x 78.1250 (71.7500) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [30/51] time 0.464 (0.457) data 0.334 (0.326) loss_x loss_x 1.0225 (1.0676) acc_x 71.8750 (71.4583) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [35/51] time 0.589 (0.461) data 0.458 (0.330) loss_x loss_x 0.9048 (1.0805) acc_x 78.1250 (71.4286) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [40/51] time 0.366 (0.463) data 0.235 (0.333) loss_x loss_x 1.2178 (1.1093) acc_x 62.5000 (70.7031) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [45/51] time 0.493 (0.456) data 0.363 (0.326) loss_x loss_x 0.8369 (1.1108) acc_x 81.2500 (71.3194) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [50/51] time 0.530 (0.458) data 0.399 (0.328) loss_x loss_x 1.2148 (1.1208) acc_x 68.7500 (71.2500) lr 8.5910e-04 eta 0:00:00
epoch [111/200] batch [5/46] time 0.403 (0.453) data 0.271 (0.323) loss_u loss_u 0.7695 (0.7799) acc_u 28.1250 (26.8750) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [10/46] time 0.453 (0.450) data 0.322 (0.320) loss_u loss_u 0.7656 (0.7647) acc_u 31.2500 (28.7500) lr 8.5910e-04 eta 0:00:16
epoch [111/200] batch [15/46] time 0.422 (0.448) data 0.291 (0.317) loss_u loss_u 0.8413 (0.7589) acc_u 12.5000 (28.5417) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [20/46] time 0.411 (0.444) data 0.281 (0.314) loss_u loss_u 0.8350 (0.7698) acc_u 18.7500 (27.6562) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [25/46] time 0.375 (0.444) data 0.243 (0.314) loss_u loss_u 0.7861 (0.7712) acc_u 28.1250 (27.2500) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [30/46] time 0.592 (0.444) data 0.460 (0.314) loss_u loss_u 0.6880 (0.7649) acc_u 31.2500 (28.8542) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [35/46] time 0.395 (0.443) data 0.264 (0.312) loss_u loss_u 0.6885 (0.7603) acc_u 40.6250 (29.5536) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [40/46] time 0.304 (0.440) data 0.173 (0.310) loss_u loss_u 0.7090 (0.7574) acc_u 40.6250 (29.9219) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [45/46] time 0.424 (0.442) data 0.293 (0.311) loss_u loss_u 0.7671 (0.7575) acc_u 37.5000 (30.2083) lr 8.5910e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1347
confident_label rate tensor(0.5210, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1634
clean true:1617
clean false:17
clean_rate:0.9895960832313342
noisy true:172
noisy false:1330
after delete: len(clean_dataset) 1634
after delete: len(noisy_dataset) 1502
epoch [112/200] batch [5/51] time 0.451 (0.397) data 0.322 (0.267) loss_x loss_x 1.2715 (1.3335) acc_x 65.6250 (71.8750) lr 8.4357e-04 eta 0:00:18
epoch [112/200] batch [10/51] time 0.525 (0.448) data 0.396 (0.319) loss_x loss_x 1.1670 (1.2395) acc_x 62.5000 (69.3750) lr 8.4357e-04 eta 0:00:18
epoch [112/200] batch [15/51] time 0.412 (0.431) data 0.282 (0.301) loss_x loss_x 1.0078 (1.2126) acc_x 78.1250 (71.6667) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [20/51] time 0.646 (0.440) data 0.517 (0.311) loss_x loss_x 0.9980 (1.1781) acc_x 71.8750 (71.0938) lr 8.4357e-04 eta 0:00:13
epoch [112/200] batch [25/51] time 0.391 (0.447) data 0.261 (0.317) loss_x loss_x 1.2373 (1.1566) acc_x 62.5000 (71.5000) lr 8.4357e-04 eta 0:00:11
epoch [112/200] batch [30/51] time 0.475 (0.456) data 0.345 (0.326) loss_x loss_x 1.0596 (1.1549) acc_x 68.7500 (71.1458) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [35/51] time 0.391 (0.451) data 0.261 (0.321) loss_x loss_x 0.7671 (1.1223) acc_x 84.3750 (72.3214) lr 8.4357e-04 eta 0:00:07
epoch [112/200] batch [40/51] time 0.524 (0.451) data 0.393 (0.321) loss_x loss_x 1.1201 (1.1053) acc_x 65.6250 (72.6562) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [45/51] time 0.413 (0.452) data 0.282 (0.322) loss_x loss_x 1.4072 (1.1145) acc_x 75.0000 (72.5694) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [50/51] time 0.416 (0.450) data 0.286 (0.321) loss_x loss_x 1.4443 (1.1349) acc_x 53.1250 (72.0000) lr 8.4357e-04 eta 0:00:00
epoch [112/200] batch [5/46] time 0.385 (0.449) data 0.254 (0.319) loss_u loss_u 0.7524 (0.7739) acc_u 34.3750 (28.7500) lr 8.4357e-04 eta 0:00:18
epoch [112/200] batch [10/46] time 0.369 (0.445) data 0.238 (0.315) loss_u loss_u 0.8096 (0.7789) acc_u 21.8750 (27.5000) lr 8.4357e-04 eta 0:00:16
epoch [112/200] batch [15/46] time 0.431 (0.449) data 0.299 (0.318) loss_u loss_u 0.6904 (0.7688) acc_u 37.5000 (29.1667) lr 8.4357e-04 eta 0:00:13
epoch [112/200] batch [20/46] time 0.401 (0.445) data 0.270 (0.315) loss_u loss_u 0.7334 (0.7619) acc_u 40.6250 (30.9375) lr 8.4357e-04 eta 0:00:11
epoch [112/200] batch [25/46] time 0.363 (0.443) data 0.232 (0.313) loss_u loss_u 0.8306 (0.7665) acc_u 21.8750 (29.8750) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [30/46] time 0.410 (0.440) data 0.279 (0.310) loss_u loss_u 0.7852 (0.7663) acc_u 25.0000 (29.8958) lr 8.4357e-04 eta 0:00:07
epoch [112/200] batch [35/46] time 0.351 (0.438) data 0.220 (0.308) loss_u loss_u 0.6460 (0.7606) acc_u 37.5000 (30.3571) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [40/46] time 0.415 (0.436) data 0.284 (0.306) loss_u loss_u 0.8091 (0.7652) acc_u 28.1250 (29.9219) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [45/46] time 0.451 (0.436) data 0.319 (0.306) loss_u loss_u 0.6577 (0.7642) acc_u 40.6250 (30.1389) lr 8.4357e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1347
confident_label rate tensor(0.5191, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1628
clean true:1609
clean false:19
clean_rate:0.9883292383292384
noisy true:180
noisy false:1328
after delete: len(clean_dataset) 1628
after delete: len(noisy_dataset) 1508
epoch [113/200] batch [5/50] time 0.581 (0.475) data 0.451 (0.344) loss_x loss_x 1.0625 (1.2326) acc_x 75.0000 (68.1250) lr 8.2807e-04 eta 0:00:21
epoch [113/200] batch [10/50] time 0.483 (0.470) data 0.354 (0.340) loss_x loss_x 1.5654 (1.1695) acc_x 62.5000 (70.0000) lr 8.2807e-04 eta 0:00:18
epoch [113/200] batch [15/50] time 0.340 (0.445) data 0.209 (0.315) loss_x loss_x 1.2461 (1.1551) acc_x 68.7500 (70.8333) lr 8.2807e-04 eta 0:00:15
epoch [113/200] batch [20/50] time 0.668 (0.445) data 0.538 (0.315) loss_x loss_x 1.0186 (1.1560) acc_x 75.0000 (71.0938) lr 8.2807e-04 eta 0:00:13
epoch [113/200] batch [25/50] time 0.542 (0.442) data 0.413 (0.312) loss_x loss_x 1.3398 (1.1682) acc_x 65.6250 (70.8750) lr 8.2807e-04 eta 0:00:11
epoch [113/200] batch [30/50] time 0.579 (0.453) data 0.449 (0.323) loss_x loss_x 1.1113 (1.1531) acc_x 71.8750 (70.9375) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [35/50] time 0.359 (0.453) data 0.229 (0.323) loss_x loss_x 1.1045 (1.1499) acc_x 75.0000 (70.8929) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [40/50] time 0.486 (0.455) data 0.356 (0.325) loss_x loss_x 1.6777 (1.1456) acc_x 62.5000 (71.0156) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [45/50] time 0.418 (0.452) data 0.288 (0.322) loss_x loss_x 1.1855 (1.1482) acc_x 68.7500 (70.6250) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [50/50] time 0.485 (0.455) data 0.354 (0.325) loss_x loss_x 1.4355 (1.1402) acc_x 68.7500 (71.1250) lr 8.2807e-04 eta 0:00:00
epoch [113/200] batch [5/47] time 0.418 (0.458) data 0.287 (0.327) loss_u loss_u 0.7515 (0.7343) acc_u 40.6250 (38.1250) lr 8.2807e-04 eta 0:00:19
epoch [113/200] batch [10/47] time 0.355 (0.462) data 0.224 (0.331) loss_u loss_u 0.8267 (0.7723) acc_u 21.8750 (30.9375) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [15/47] time 0.333 (0.455) data 0.203 (0.324) loss_u loss_u 0.8423 (0.7685) acc_u 18.7500 (31.6667) lr 8.2807e-04 eta 0:00:14
epoch [113/200] batch [20/47] time 0.369 (0.448) data 0.238 (0.318) loss_u loss_u 0.7441 (0.7670) acc_u 37.5000 (31.5625) lr 8.2807e-04 eta 0:00:12
epoch [113/200] batch [25/47] time 0.425 (0.446) data 0.294 (0.316) loss_u loss_u 0.7104 (0.7634) acc_u 46.8750 (31.6250) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [30/47] time 0.429 (0.446) data 0.298 (0.316) loss_u loss_u 0.7720 (0.7553) acc_u 31.2500 (32.3958) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [35/47] time 0.318 (0.444) data 0.187 (0.314) loss_u loss_u 0.8247 (0.7572) acc_u 21.8750 (31.7857) lr 8.2807e-04 eta 0:00:05
epoch [113/200] batch [40/47] time 0.355 (0.442) data 0.224 (0.312) loss_u loss_u 0.7827 (0.7594) acc_u 34.3750 (31.0938) lr 8.2807e-04 eta 0:00:03
epoch [113/200] batch [45/47] time 0.325 (0.441) data 0.194 (0.310) loss_u loss_u 0.7007 (0.7548) acc_u 34.3750 (31.3194) lr 8.2807e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1352
confident_label rate tensor(0.5214, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1635
clean true:1612
clean false:23
clean_rate:0.9859327217125382
noisy true:172
noisy false:1329
after delete: len(clean_dataset) 1635
after delete: len(noisy_dataset) 1501
epoch [114/200] batch [5/51] time 0.495 (0.514) data 0.364 (0.383) loss_x loss_x 0.9551 (1.0065) acc_x 78.1250 (76.2500) lr 8.1262e-04 eta 0:00:23
epoch [114/200] batch [10/51] time 0.357 (0.479) data 0.227 (0.348) loss_x loss_x 1.1045 (1.0194) acc_x 75.0000 (74.6875) lr 8.1262e-04 eta 0:00:19
epoch [114/200] batch [15/51] time 0.362 (0.454) data 0.233 (0.324) loss_x loss_x 1.2461 (1.0999) acc_x 71.8750 (72.2917) lr 8.1262e-04 eta 0:00:16
epoch [114/200] batch [20/51] time 0.417 (0.461) data 0.286 (0.330) loss_x loss_x 1.4365 (1.0694) acc_x 59.3750 (71.7188) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [25/51] time 0.420 (0.444) data 0.290 (0.314) loss_x loss_x 1.4941 (1.1181) acc_x 62.5000 (71.1250) lr 8.1262e-04 eta 0:00:11
epoch [114/200] batch [30/51] time 0.517 (0.456) data 0.386 (0.326) loss_x loss_x 1.1133 (1.0919) acc_x 65.6250 (71.6667) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [35/51] time 0.474 (0.450) data 0.343 (0.320) loss_x loss_x 1.0791 (1.1019) acc_x 68.7500 (71.4286) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [40/51] time 0.491 (0.447) data 0.362 (0.317) loss_x loss_x 1.0186 (1.0904) acc_x 78.1250 (71.3281) lr 8.1262e-04 eta 0:00:04
epoch [114/200] batch [45/51] time 0.477 (0.453) data 0.348 (0.323) loss_x loss_x 0.6875 (1.0829) acc_x 84.3750 (71.9444) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [50/51] time 0.407 (0.455) data 0.276 (0.325) loss_x loss_x 0.6270 (1.0640) acc_x 84.3750 (72.7500) lr 8.1262e-04 eta 0:00:00
epoch [114/200] batch [5/46] time 0.518 (0.454) data 0.387 (0.324) loss_u loss_u 0.7686 (0.7691) acc_u 28.1250 (30.6250) lr 8.1262e-04 eta 0:00:18
epoch [114/200] batch [10/46] time 0.409 (0.454) data 0.278 (0.324) loss_u loss_u 0.8125 (0.7617) acc_u 28.1250 (31.2500) lr 8.1262e-04 eta 0:00:16
epoch [114/200] batch [15/46] time 0.457 (0.452) data 0.326 (0.322) loss_u loss_u 0.7939 (0.7543) acc_u 25.0000 (32.5000) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [20/46] time 0.458 (0.447) data 0.328 (0.316) loss_u loss_u 0.7891 (0.7547) acc_u 25.0000 (32.1875) lr 8.1262e-04 eta 0:00:11
epoch [114/200] batch [25/46] time 0.530 (0.446) data 0.399 (0.316) loss_u loss_u 0.7505 (0.7500) acc_u 37.5000 (32.7500) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [30/46] time 0.419 (0.448) data 0.288 (0.318) loss_u loss_u 0.7827 (0.7558) acc_u 28.1250 (31.6667) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [35/46] time 0.537 (0.447) data 0.406 (0.316) loss_u loss_u 0.8115 (0.7601) acc_u 31.2500 (30.8929) lr 8.1262e-04 eta 0:00:04
epoch [114/200] batch [40/46] time 0.424 (0.444) data 0.294 (0.314) loss_u loss_u 0.6997 (0.7594) acc_u 34.3750 (30.8594) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [45/46] time 0.461 (0.445) data 0.330 (0.314) loss_u loss_u 0.7979 (0.7629) acc_u 28.1250 (30.4167) lr 8.1262e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1336
confident_label rate tensor(0.5271, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1653
clean true:1633
clean false:20
clean_rate:0.9879007864488808
noisy true:167
noisy false:1316
after delete: len(clean_dataset) 1653
after delete: len(noisy_dataset) 1483
epoch [115/200] batch [5/51] time 0.419 (0.466) data 0.290 (0.336) loss_x loss_x 1.7305 (1.2529) acc_x 53.1250 (63.7500) lr 7.9721e-04 eta 0:00:21
epoch [115/200] batch [10/51] time 0.474 (0.440) data 0.344 (0.310) loss_x loss_x 0.8833 (1.1474) acc_x 78.1250 (68.4375) lr 7.9721e-04 eta 0:00:18
epoch [115/200] batch [15/51] time 0.491 (0.448) data 0.361 (0.317) loss_x loss_x 1.5371 (1.2022) acc_x 68.7500 (67.7083) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [20/51] time 0.505 (0.447) data 0.375 (0.317) loss_x loss_x 1.2480 (1.1930) acc_x 68.7500 (68.5938) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [25/51] time 0.404 (0.444) data 0.274 (0.313) loss_x loss_x 1.0752 (1.1575) acc_x 71.8750 (69.8750) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [30/51] time 0.418 (0.440) data 0.288 (0.310) loss_x loss_x 0.8110 (1.1235) acc_x 78.1250 (70.7292) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [35/51] time 0.395 (0.440) data 0.264 (0.309) loss_x loss_x 1.1357 (1.1151) acc_x 65.6250 (71.0714) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [40/51] time 0.766 (0.451) data 0.635 (0.321) loss_x loss_x 0.9312 (1.0912) acc_x 78.1250 (71.5625) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [45/51] time 0.476 (0.453) data 0.346 (0.322) loss_x loss_x 1.3008 (1.1072) acc_x 62.5000 (71.2500) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [50/51] time 0.369 (0.452) data 0.238 (0.321) loss_x loss_x 1.6504 (1.1036) acc_x 62.5000 (71.3750) lr 7.9721e-04 eta 0:00:00
epoch [115/200] batch [5/46] time 0.331 (0.449) data 0.200 (0.318) loss_u loss_u 0.7832 (0.7669) acc_u 21.8750 (28.7500) lr 7.9721e-04 eta 0:00:18
epoch [115/200] batch [10/46] time 0.358 (0.442) data 0.227 (0.312) loss_u loss_u 0.7095 (0.7587) acc_u 37.5000 (29.3750) lr 7.9721e-04 eta 0:00:15
epoch [115/200] batch [15/46] time 0.385 (0.442) data 0.254 (0.312) loss_u loss_u 0.8091 (0.7589) acc_u 25.0000 (30.2083) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [20/46] time 0.626 (0.442) data 0.497 (0.311) loss_u loss_u 0.8340 (0.7713) acc_u 21.8750 (27.9688) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [25/46] time 0.575 (0.445) data 0.445 (0.315) loss_u loss_u 0.7549 (0.7677) acc_u 31.2500 (28.2500) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [30/46] time 0.451 (0.445) data 0.320 (0.315) loss_u loss_u 0.7466 (0.7594) acc_u 34.3750 (29.5833) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [35/46] time 0.499 (0.444) data 0.368 (0.314) loss_u loss_u 0.7896 (0.7556) acc_u 31.2500 (30.0893) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [40/46] time 0.350 (0.442) data 0.219 (0.311) loss_u loss_u 0.8286 (0.7603) acc_u 21.8750 (29.9219) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [45/46] time 0.363 (0.439) data 0.233 (0.309) loss_u loss_u 0.7700 (0.7615) acc_u 28.1250 (29.7222) lr 7.9721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1302
confident_label rate tensor(0.5389, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1690
clean true:1667
clean false:23
clean_rate:0.9863905325443787
noisy true:167
noisy false:1279
after delete: len(clean_dataset) 1690
after delete: len(noisy_dataset) 1446
epoch [116/200] batch [5/52] time 0.367 (0.453) data 0.237 (0.323) loss_x loss_x 1.1953 (1.1308) acc_x 62.5000 (70.0000) lr 7.8186e-04 eta 0:00:21
epoch [116/200] batch [10/52] time 0.578 (0.493) data 0.448 (0.363) loss_x loss_x 1.4160 (1.2004) acc_x 59.3750 (67.1875) lr 7.8186e-04 eta 0:00:20
epoch [116/200] batch [15/52] time 0.451 (0.479) data 0.321 (0.348) loss_x loss_x 1.0361 (1.0850) acc_x 71.8750 (71.0417) lr 7.8186e-04 eta 0:00:17
epoch [116/200] batch [20/52] time 0.484 (0.465) data 0.353 (0.335) loss_x loss_x 0.7900 (1.0628) acc_x 75.0000 (72.1875) lr 7.8186e-04 eta 0:00:14
epoch [116/200] batch [25/52] time 0.440 (0.464) data 0.310 (0.334) loss_x loss_x 0.9580 (1.0629) acc_x 68.7500 (71.5000) lr 7.8186e-04 eta 0:00:12
epoch [116/200] batch [30/52] time 0.573 (0.469) data 0.442 (0.339) loss_x loss_x 1.1836 (1.1075) acc_x 68.7500 (71.0417) lr 7.8186e-04 eta 0:00:10
epoch [116/200] batch [35/52] time 0.541 (0.468) data 0.411 (0.338) loss_x loss_x 1.6650 (1.1094) acc_x 59.3750 (71.2500) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [40/52] time 0.536 (0.464) data 0.406 (0.334) loss_x loss_x 1.1396 (1.1228) acc_x 71.8750 (71.0938) lr 7.8186e-04 eta 0:00:05
epoch [116/200] batch [45/52] time 0.432 (0.465) data 0.302 (0.335) loss_x loss_x 1.0986 (1.1206) acc_x 71.8750 (71.2500) lr 7.8186e-04 eta 0:00:03
epoch [116/200] batch [50/52] time 0.427 (0.461) data 0.296 (0.331) loss_x loss_x 1.3291 (1.1287) acc_x 78.1250 (71.2500) lr 7.8186e-04 eta 0:00:00
epoch [116/200] batch [5/45] time 0.517 (0.457) data 0.386 (0.326) loss_u loss_u 0.7671 (0.7690) acc_u 25.0000 (29.3750) lr 7.8186e-04 eta 0:00:18
epoch [116/200] batch [10/45] time 0.341 (0.454) data 0.209 (0.323) loss_u loss_u 0.7773 (0.7693) acc_u 21.8750 (28.1250) lr 7.8186e-04 eta 0:00:15
epoch [116/200] batch [15/45] time 0.340 (0.449) data 0.209 (0.318) loss_u loss_u 0.8540 (0.7701) acc_u 15.6250 (27.0833) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [20/45] time 0.397 (0.449) data 0.266 (0.318) loss_u loss_u 0.8125 (0.7769) acc_u 25.0000 (26.5625) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [25/45] time 0.384 (0.444) data 0.254 (0.313) loss_u loss_u 0.8066 (0.7781) acc_u 25.0000 (26.5000) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [30/45] time 0.411 (0.445) data 0.280 (0.314) loss_u loss_u 0.8110 (0.7708) acc_u 21.8750 (27.2917) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [35/45] time 0.412 (0.443) data 0.281 (0.312) loss_u loss_u 0.8223 (0.7711) acc_u 25.0000 (27.3214) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [40/45] time 0.440 (0.442) data 0.309 (0.311) loss_u loss_u 0.7690 (0.7705) acc_u 28.1250 (27.5000) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [45/45] time 0.390 (0.443) data 0.259 (0.312) loss_u loss_u 0.7842 (0.7738) acc_u 25.0000 (27.7083) lr 7.8186e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1384
confident_label rate tensor(0.5124, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1607
clean true:1586
clean false:21
clean_rate:0.9869321717485999
noisy true:166
noisy false:1363
after delete: len(clean_dataset) 1607
after delete: len(noisy_dataset) 1529
epoch [117/200] batch [5/50] time 0.454 (0.430) data 0.324 (0.300) loss_x loss_x 1.5146 (1.1211) acc_x 65.6250 (69.3750) lr 7.6655e-04 eta 0:00:19
epoch [117/200] batch [10/50] time 0.480 (0.436) data 0.350 (0.306) loss_x loss_x 1.0352 (1.0860) acc_x 75.0000 (71.5625) lr 7.6655e-04 eta 0:00:17
epoch [117/200] batch [15/50] time 0.423 (0.433) data 0.294 (0.303) loss_x loss_x 1.8398 (1.2118) acc_x 56.2500 (69.1667) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [20/50] time 0.483 (0.426) data 0.354 (0.296) loss_x loss_x 0.9346 (1.1834) acc_x 71.8750 (69.8438) lr 7.6655e-04 eta 0:00:12
epoch [117/200] batch [25/50] time 0.514 (0.433) data 0.384 (0.303) loss_x loss_x 0.8428 (1.1492) acc_x 87.5000 (71.0000) lr 7.6655e-04 eta 0:00:10
epoch [117/200] batch [30/50] time 0.436 (0.432) data 0.307 (0.302) loss_x loss_x 1.4590 (1.1513) acc_x 62.5000 (70.8333) lr 7.6655e-04 eta 0:00:08
epoch [117/200] batch [35/50] time 0.378 (0.426) data 0.248 (0.296) loss_x loss_x 1.3184 (1.1900) acc_x 68.7500 (69.9107) lr 7.6655e-04 eta 0:00:06
epoch [117/200] batch [40/50] time 0.487 (0.434) data 0.356 (0.304) loss_x loss_x 1.5068 (1.1697) acc_x 65.6250 (70.4688) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [45/50] time 0.523 (0.446) data 0.393 (0.316) loss_x loss_x 1.5742 (1.1810) acc_x 65.6250 (70.3472) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [50/50] time 0.498 (0.448) data 0.368 (0.318) loss_x loss_x 0.9512 (1.1785) acc_x 75.0000 (70.1250) lr 7.6655e-04 eta 0:00:00
epoch [117/200] batch [5/47] time 0.383 (0.447) data 0.252 (0.317) loss_u loss_u 0.7554 (0.7143) acc_u 28.1250 (34.3750) lr 7.6655e-04 eta 0:00:18
epoch [117/200] batch [10/47] time 0.333 (0.443) data 0.202 (0.312) loss_u loss_u 0.7129 (0.7299) acc_u 31.2500 (33.7500) lr 7.6655e-04 eta 0:00:16
epoch [117/200] batch [15/47] time 0.397 (0.443) data 0.266 (0.313) loss_u loss_u 0.6592 (0.7179) acc_u 40.6250 (35.4167) lr 7.6655e-04 eta 0:00:14
epoch [117/200] batch [20/47] time 0.482 (0.442) data 0.351 (0.311) loss_u loss_u 0.7158 (0.7284) acc_u 31.2500 (34.3750) lr 7.6655e-04 eta 0:00:11
epoch [117/200] batch [25/47] time 0.388 (0.439) data 0.257 (0.309) loss_u loss_u 0.7905 (0.7436) acc_u 21.8750 (31.7500) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [30/47] time 0.370 (0.439) data 0.239 (0.309) loss_u loss_u 0.8213 (0.7546) acc_u 21.8750 (30.1042) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [35/47] time 0.430 (0.438) data 0.299 (0.308) loss_u loss_u 0.6904 (0.7570) acc_u 37.5000 (29.5536) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [40/47] time 0.326 (0.437) data 0.195 (0.306) loss_u loss_u 0.7305 (0.7531) acc_u 34.3750 (29.7656) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [45/47] time 0.408 (0.437) data 0.277 (0.306) loss_u loss_u 0.6030 (0.7492) acc_u 43.7500 (30.4167) lr 7.6655e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1372
confident_label rate tensor(0.5143, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1613
clean true:1594
clean false:19
clean_rate:0.9882207067575945
noisy true:170
noisy false:1353
after delete: len(clean_dataset) 1613
after delete: len(noisy_dataset) 1523
epoch [118/200] batch [5/50] time 0.464 (0.446) data 0.333 (0.315) loss_x loss_x 1.0010 (1.2655) acc_x 84.3750 (73.1250) lr 7.5131e-04 eta 0:00:20
epoch [118/200] batch [10/50] time 0.390 (0.466) data 0.260 (0.335) loss_x loss_x 0.7686 (1.1017) acc_x 81.2500 (75.3125) lr 7.5131e-04 eta 0:00:18
epoch [118/200] batch [15/50] time 0.496 (0.469) data 0.365 (0.338) loss_x loss_x 1.1338 (1.1313) acc_x 65.6250 (72.2917) lr 7.5131e-04 eta 0:00:16
epoch [118/200] batch [20/50] time 0.425 (0.452) data 0.294 (0.321) loss_x loss_x 1.2598 (1.1132) acc_x 78.1250 (72.6562) lr 7.5131e-04 eta 0:00:13
epoch [118/200] batch [25/50] time 0.537 (0.463) data 0.407 (0.332) loss_x loss_x 0.9995 (1.0887) acc_x 71.8750 (72.7500) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [30/50] time 0.445 (0.451) data 0.314 (0.321) loss_x loss_x 1.3359 (1.1255) acc_x 68.7500 (71.7708) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [35/50] time 0.454 (0.451) data 0.324 (0.320) loss_x loss_x 1.2129 (1.1429) acc_x 62.5000 (71.1607) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [40/50] time 0.408 (0.453) data 0.277 (0.322) loss_x loss_x 1.0000 (1.1278) acc_x 81.2500 (71.7969) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [45/50] time 0.408 (0.450) data 0.279 (0.319) loss_x loss_x 1.2256 (1.1271) acc_x 75.0000 (71.9444) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [50/50] time 0.443 (0.450) data 0.313 (0.320) loss_x loss_x 1.4600 (1.1426) acc_x 62.5000 (71.7500) lr 7.5131e-04 eta 0:00:00
epoch [118/200] batch [5/47] time 0.411 (0.450) data 0.280 (0.319) loss_u loss_u 0.7905 (0.7860) acc_u 28.1250 (29.3750) lr 7.5131e-04 eta 0:00:18
epoch [118/200] batch [10/47] time 0.423 (0.446) data 0.292 (0.315) loss_u loss_u 0.7280 (0.7810) acc_u 28.1250 (27.1875) lr 7.5131e-04 eta 0:00:16
epoch [118/200] batch [15/47] time 0.581 (0.444) data 0.449 (0.314) loss_u loss_u 0.7734 (0.7797) acc_u 25.0000 (27.5000) lr 7.5131e-04 eta 0:00:14
epoch [118/200] batch [20/47] time 0.490 (0.443) data 0.359 (0.313) loss_u loss_u 0.7471 (0.7545) acc_u 31.2500 (30.1562) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [25/47] time 0.409 (0.445) data 0.277 (0.314) loss_u loss_u 0.7939 (0.7589) acc_u 18.7500 (28.8750) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [30/47] time 0.489 (0.444) data 0.357 (0.314) loss_u loss_u 0.7305 (0.7568) acc_u 28.1250 (29.1667) lr 7.5131e-04 eta 0:00:07
epoch [118/200] batch [35/47] time 0.432 (0.443) data 0.301 (0.312) loss_u loss_u 0.8564 (0.7602) acc_u 18.7500 (29.1964) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [40/47] time 0.451 (0.442) data 0.321 (0.311) loss_u loss_u 0.7246 (0.7593) acc_u 37.5000 (29.4531) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [45/47] time 0.391 (0.440) data 0.262 (0.309) loss_u loss_u 0.7568 (0.7588) acc_u 31.2500 (29.7222) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1389
confident_label rate tensor(0.5121, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1606
clean true:1584
clean false:22
clean_rate:0.9863013698630136
noisy true:163
noisy false:1367
after delete: len(clean_dataset) 1606
after delete: len(noisy_dataset) 1530
epoch [119/200] batch [5/50] time 0.434 (0.455) data 0.304 (0.325) loss_x loss_x 1.5127 (1.1010) acc_x 65.6250 (73.1250) lr 7.3613e-04 eta 0:00:20
epoch [119/200] batch [10/50] time 0.417 (0.458) data 0.286 (0.327) loss_x loss_x 0.6694 (1.0132) acc_x 87.5000 (75.0000) lr 7.3613e-04 eta 0:00:18
epoch [119/200] batch [15/50] time 0.393 (0.450) data 0.263 (0.320) loss_x loss_x 1.0820 (1.0309) acc_x 75.0000 (74.1667) lr 7.3613e-04 eta 0:00:15
epoch [119/200] batch [20/50] time 0.452 (0.440) data 0.321 (0.310) loss_x loss_x 1.4023 (1.0772) acc_x 56.2500 (72.5000) lr 7.3613e-04 eta 0:00:13
epoch [119/200] batch [25/50] time 0.403 (0.453) data 0.272 (0.322) loss_x loss_x 0.9375 (1.0588) acc_x 78.1250 (73.1250) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [30/50] time 0.375 (0.450) data 0.244 (0.319) loss_x loss_x 1.0312 (1.0815) acc_x 68.7500 (72.1875) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [35/50] time 0.566 (0.450) data 0.436 (0.320) loss_x loss_x 1.5273 (1.0672) acc_x 62.5000 (72.3214) lr 7.3613e-04 eta 0:00:06
epoch [119/200] batch [40/50] time 0.569 (0.448) data 0.439 (0.318) loss_x loss_x 1.4912 (1.0963) acc_x 68.7500 (71.9531) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [45/50] time 0.445 (0.446) data 0.314 (0.315) loss_x loss_x 1.0176 (1.1072) acc_x 65.6250 (71.3889) lr 7.3613e-04 eta 0:00:02
epoch [119/200] batch [50/50] time 0.456 (0.449) data 0.326 (0.318) loss_x loss_x 1.3262 (1.1185) acc_x 65.6250 (71.2500) lr 7.3613e-04 eta 0:00:00
epoch [119/200] batch [5/47] time 0.528 (0.448) data 0.397 (0.317) loss_u loss_u 0.7798 (0.7430) acc_u 28.1250 (32.5000) lr 7.3613e-04 eta 0:00:18
epoch [119/200] batch [10/47] time 0.345 (0.443) data 0.213 (0.313) loss_u loss_u 0.6841 (0.7413) acc_u 37.5000 (30.6250) lr 7.3613e-04 eta 0:00:16
epoch [119/200] batch [15/47] time 0.447 (0.440) data 0.316 (0.309) loss_u loss_u 0.7969 (0.7369) acc_u 25.0000 (31.6667) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [20/47] time 0.416 (0.439) data 0.285 (0.309) loss_u loss_u 0.8213 (0.7469) acc_u 18.7500 (29.8438) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [25/47] time 0.479 (0.445) data 0.348 (0.314) loss_u loss_u 0.7920 (0.7527) acc_u 28.1250 (29.5000) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [30/47] time 0.414 (0.444) data 0.283 (0.313) loss_u loss_u 0.7744 (0.7508) acc_u 31.2500 (30.3125) lr 7.3613e-04 eta 0:00:07
epoch [119/200] batch [35/47] time 0.483 (0.442) data 0.351 (0.312) loss_u loss_u 0.7500 (0.7526) acc_u 34.3750 (30.4464) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [40/47] time 0.995 (0.448) data 0.864 (0.317) loss_u loss_u 0.7344 (0.7541) acc_u 28.1250 (30.3125) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [45/47] time 0.385 (0.447) data 0.255 (0.316) loss_u loss_u 0.7524 (0.7515) acc_u 28.1250 (30.5556) lr 7.3613e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1375
confident_label rate tensor(0.5166, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1620
clean true:1597
clean false:23
clean_rate:0.9858024691358025
noisy true:164
noisy false:1352
after delete: len(clean_dataset) 1620
after delete: len(noisy_dataset) 1516
epoch [120/200] batch [5/50] time 0.502 (0.436) data 0.371 (0.306) loss_x loss_x 0.6826 (1.1338) acc_x 87.5000 (68.7500) lr 7.2101e-04 eta 0:00:19
epoch [120/200] batch [10/50] time 0.504 (0.431) data 0.374 (0.301) loss_x loss_x 0.8521 (1.1509) acc_x 75.0000 (69.3750) lr 7.2101e-04 eta 0:00:17
epoch [120/200] batch [15/50] time 0.493 (0.449) data 0.363 (0.319) loss_x loss_x 1.1133 (1.1469) acc_x 71.8750 (70.6250) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [20/50] time 0.415 (0.450) data 0.285 (0.320) loss_x loss_x 1.2275 (1.1327) acc_x 71.8750 (70.7812) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [25/50] time 0.490 (0.461) data 0.359 (0.331) loss_x loss_x 1.2197 (1.1274) acc_x 71.8750 (70.7500) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [30/50] time 0.570 (0.467) data 0.440 (0.337) loss_x loss_x 0.8213 (1.1162) acc_x 75.0000 (71.2500) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [35/50] time 0.432 (0.461) data 0.302 (0.331) loss_x loss_x 1.2959 (1.1305) acc_x 65.6250 (71.2500) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [40/50] time 0.477 (0.459) data 0.346 (0.329) loss_x loss_x 1.4658 (1.1455) acc_x 71.8750 (71.4062) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [45/50] time 0.539 (0.461) data 0.408 (0.331) loss_x loss_x 1.0791 (1.1470) acc_x 68.7500 (70.9722) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [50/50] time 0.411 (0.461) data 0.280 (0.331) loss_x loss_x 0.6050 (1.1532) acc_x 81.2500 (70.5625) lr 7.2101e-04 eta 0:00:00
epoch [120/200] batch [5/47] time 0.382 (0.459) data 0.251 (0.328) loss_u loss_u 0.6924 (0.7385) acc_u 43.7500 (30.6250) lr 7.2101e-04 eta 0:00:19
epoch [120/200] batch [10/47] time 0.376 (0.456) data 0.245 (0.326) loss_u loss_u 0.8135 (0.7398) acc_u 21.8750 (29.6875) lr 7.2101e-04 eta 0:00:16
epoch [120/200] batch [15/47] time 0.476 (0.454) data 0.346 (0.324) loss_u loss_u 0.6982 (0.7389) acc_u 46.8750 (31.0417) lr 7.2101e-04 eta 0:00:14
epoch [120/200] batch [20/47] time 0.501 (0.458) data 0.370 (0.328) loss_u loss_u 0.7549 (0.7539) acc_u 28.1250 (29.8438) lr 7.2101e-04 eta 0:00:12
epoch [120/200] batch [25/47] time 0.384 (0.453) data 0.254 (0.323) loss_u loss_u 0.7031 (0.7555) acc_u 40.6250 (30.0000) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [30/47] time 0.424 (0.453) data 0.294 (0.322) loss_u loss_u 0.7642 (0.7578) acc_u 25.0000 (29.3750) lr 7.2101e-04 eta 0:00:07
epoch [120/200] batch [35/47] time 0.363 (0.451) data 0.232 (0.321) loss_u loss_u 0.7612 (0.7567) acc_u 34.3750 (30.0893) lr 7.2101e-04 eta 0:00:05
epoch [120/200] batch [40/47] time 0.365 (0.450) data 0.234 (0.320) loss_u loss_u 0.8613 (0.7629) acc_u 12.5000 (28.9844) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [45/47] time 0.479 (0.449) data 0.348 (0.318) loss_u loss_u 0.7373 (0.7607) acc_u 25.0000 (29.2361) lr 7.2101e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1370
confident_label rate tensor(0.5073, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1591
clean true:1572
clean false:19
clean_rate:0.9880578252671276
noisy true:194
noisy false:1351
after delete: len(clean_dataset) 1591
after delete: len(noisy_dataset) 1545
epoch [121/200] batch [5/49] time 0.444 (0.468) data 0.314 (0.338) loss_x loss_x 1.1514 (1.0033) acc_x 65.6250 (75.0000) lr 7.0596e-04 eta 0:00:20
epoch [121/200] batch [10/49] time 0.457 (0.462) data 0.326 (0.331) loss_x loss_x 1.2100 (1.0625) acc_x 68.7500 (72.8125) lr 7.0596e-04 eta 0:00:17
epoch [121/200] batch [15/49] time 0.381 (0.440) data 0.251 (0.310) loss_x loss_x 1.4502 (1.0599) acc_x 68.7500 (73.7500) lr 7.0596e-04 eta 0:00:14
epoch [121/200] batch [20/49] time 0.442 (0.442) data 0.313 (0.312) loss_x loss_x 1.1006 (1.0302) acc_x 71.8750 (74.5312) lr 7.0596e-04 eta 0:00:12
epoch [121/200] batch [25/49] time 0.388 (0.446) data 0.258 (0.316) loss_x loss_x 1.3652 (1.0339) acc_x 71.8750 (74.7500) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [30/49] time 0.351 (0.440) data 0.222 (0.310) loss_x loss_x 1.2939 (1.0610) acc_x 62.5000 (73.6458) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [35/49] time 0.460 (0.436) data 0.330 (0.306) loss_x loss_x 1.0566 (1.0634) acc_x 75.0000 (73.3929) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [40/49] time 0.508 (0.441) data 0.378 (0.311) loss_x loss_x 0.9023 (1.0866) acc_x 78.1250 (73.1250) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [45/49] time 0.598 (0.446) data 0.468 (0.316) loss_x loss_x 1.1338 (1.0821) acc_x 71.8750 (72.9861) lr 7.0596e-04 eta 0:00:01
epoch [121/200] batch [5/48] time 0.542 (0.450) data 0.411 (0.320) loss_u loss_u 0.7822 (0.7483) acc_u 25.0000 (33.7500) lr 7.0596e-04 eta 0:00:19
epoch [121/200] batch [10/48] time 0.392 (0.448) data 0.263 (0.318) loss_u loss_u 0.7510 (0.7645) acc_u 31.2500 (30.6250) lr 7.0596e-04 eta 0:00:17
epoch [121/200] batch [15/48] time 0.354 (0.446) data 0.223 (0.315) loss_u loss_u 0.8081 (0.7622) acc_u 28.1250 (30.8333) lr 7.0596e-04 eta 0:00:14
epoch [121/200] batch [20/48] time 0.529 (0.443) data 0.398 (0.313) loss_u loss_u 0.6709 (0.7578) acc_u 43.7500 (30.9375) lr 7.0596e-04 eta 0:00:12
epoch [121/200] batch [25/48] time 0.398 (0.442) data 0.267 (0.312) loss_u loss_u 0.7866 (0.7557) acc_u 28.1250 (30.7500) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [30/48] time 0.559 (0.445) data 0.428 (0.315) loss_u loss_u 0.8159 (0.7546) acc_u 21.8750 (30.4167) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [35/48] time 0.472 (0.443) data 0.341 (0.313) loss_u loss_u 0.7354 (0.7522) acc_u 34.3750 (31.0714) lr 7.0596e-04 eta 0:00:05
epoch [121/200] batch [40/48] time 0.475 (0.443) data 0.344 (0.312) loss_u loss_u 0.7622 (0.7503) acc_u 18.7500 (31.1719) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [45/48] time 0.478 (0.444) data 0.348 (0.313) loss_u loss_u 0.7979 (0.7493) acc_u 28.1250 (31.5972) lr 7.0596e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1321
confident_label rate tensor(0.5277, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1655
clean true:1637
clean false:18
clean_rate:0.9891238670694864
noisy true:178
noisy false:1303
after delete: len(clean_dataset) 1655
after delete: len(noisy_dataset) 1481
epoch [122/200] batch [5/51] time 0.490 (0.458) data 0.360 (0.327) loss_x loss_x 0.6406 (0.9943) acc_x 84.3750 (75.0000) lr 6.9098e-04 eta 0:00:21
epoch [122/200] batch [10/51] time 0.410 (0.439) data 0.279 (0.308) loss_x loss_x 0.9053 (0.9536) acc_x 75.0000 (75.3125) lr 6.9098e-04 eta 0:00:17
epoch [122/200] batch [15/51] time 0.498 (0.461) data 0.367 (0.331) loss_x loss_x 1.2939 (1.0297) acc_x 75.0000 (73.1250) lr 6.9098e-04 eta 0:00:16
epoch [122/200] batch [20/51] time 0.429 (0.456) data 0.299 (0.325) loss_x loss_x 0.8774 (1.0574) acc_x 81.2500 (73.2812) lr 6.9098e-04 eta 0:00:14
epoch [122/200] batch [25/51] time 0.613 (0.454) data 0.482 (0.324) loss_x loss_x 0.9546 (1.0379) acc_x 81.2500 (74.3750) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [30/51] time 0.402 (0.451) data 0.271 (0.321) loss_x loss_x 0.8608 (1.0354) acc_x 78.1250 (74.8958) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [35/51] time 0.400 (0.453) data 0.270 (0.323) loss_x loss_x 1.0811 (1.0718) acc_x 81.2500 (73.9286) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [40/51] time 0.496 (0.452) data 0.365 (0.322) loss_x loss_x 1.5430 (1.0971) acc_x 56.2500 (72.8125) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [45/51] time 0.480 (0.460) data 0.350 (0.330) loss_x loss_x 1.3164 (1.0728) acc_x 65.6250 (73.0556) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [50/51] time 0.407 (0.462) data 0.276 (0.332) loss_x loss_x 0.7578 (1.0650) acc_x 84.3750 (73.0625) lr 6.9098e-04 eta 0:00:00
epoch [122/200] batch [5/46] time 0.589 (0.463) data 0.459 (0.332) loss_u loss_u 0.7368 (0.7731) acc_u 37.5000 (30.6250) lr 6.9098e-04 eta 0:00:18
epoch [122/200] batch [10/46] time 0.414 (0.461) data 0.284 (0.331) loss_u loss_u 0.6729 (0.7598) acc_u 43.7500 (30.0000) lr 6.9098e-04 eta 0:00:16
epoch [122/200] batch [15/46] time 0.386 (0.454) data 0.256 (0.324) loss_u loss_u 0.8340 (0.7717) acc_u 28.1250 (28.9583) lr 6.9098e-04 eta 0:00:14
epoch [122/200] batch [20/46] time 0.329 (0.450) data 0.198 (0.320) loss_u loss_u 0.8545 (0.7728) acc_u 18.7500 (28.7500) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [25/46] time 0.322 (0.451) data 0.191 (0.321) loss_u loss_u 0.7129 (0.7658) acc_u 40.6250 (30.1250) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [30/46] time 0.426 (0.447) data 0.294 (0.316) loss_u loss_u 0.7065 (0.7645) acc_u 37.5000 (30.0000) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [35/46] time 0.435 (0.446) data 0.304 (0.316) loss_u loss_u 0.7603 (0.7627) acc_u 28.1250 (29.7321) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [40/46] time 0.458 (0.445) data 0.327 (0.315) loss_u loss_u 0.8154 (0.7666) acc_u 18.7500 (29.0625) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [45/46] time 0.358 (0.444) data 0.227 (0.313) loss_u loss_u 0.7529 (0.7634) acc_u 31.2500 (29.4444) lr 6.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1348
confident_label rate tensor(0.5210, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1634
clean true:1618
clean false:16
clean_rate:0.9902080783353733
noisy true:170
noisy false:1332
after delete: len(clean_dataset) 1634
after delete: len(noisy_dataset) 1502
epoch [123/200] batch [5/51] time 0.604 (0.473) data 0.474 (0.342) loss_x loss_x 0.8853 (1.1401) acc_x 84.3750 (73.7500) lr 6.7608e-04 eta 0:00:21
epoch [123/200] batch [10/51] time 0.406 (0.449) data 0.276 (0.318) loss_x loss_x 0.7109 (1.0892) acc_x 81.2500 (73.7500) lr 6.7608e-04 eta 0:00:18
epoch [123/200] batch [15/51] time 0.378 (0.438) data 0.248 (0.308) loss_x loss_x 1.1494 (1.0624) acc_x 68.7500 (73.5417) lr 6.7608e-04 eta 0:00:15
epoch [123/200] batch [20/51] time 0.468 (0.445) data 0.338 (0.315) loss_x loss_x 0.8613 (1.0609) acc_x 78.1250 (73.4375) lr 6.7608e-04 eta 0:00:13
epoch [123/200] batch [25/51] time 0.459 (0.455) data 0.330 (0.325) loss_x loss_x 1.3926 (1.0999) acc_x 68.7500 (71.7500) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [30/51] time 0.406 (0.449) data 0.276 (0.319) loss_x loss_x 1.2451 (1.0991) acc_x 65.6250 (71.9792) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [35/51] time 0.472 (0.453) data 0.341 (0.323) loss_x loss_x 1.5000 (1.1424) acc_x 68.7500 (71.1607) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [40/51] time 0.526 (0.457) data 0.395 (0.327) loss_x loss_x 1.2852 (1.1216) acc_x 81.2500 (72.2656) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [45/51] time 0.452 (0.457) data 0.320 (0.327) loss_x loss_x 1.3760 (1.1438) acc_x 71.8750 (71.5972) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [50/51] time 0.500 (0.458) data 0.369 (0.328) loss_x loss_x 1.0918 (1.1619) acc_x 65.6250 (70.7500) lr 6.7608e-04 eta 0:00:00
epoch [123/200] batch [5/46] time 0.400 (0.453) data 0.269 (0.323) loss_u loss_u 0.7881 (0.7491) acc_u 28.1250 (30.0000) lr 6.7608e-04 eta 0:00:18
epoch [123/200] batch [10/46] time 0.429 (0.451) data 0.298 (0.320) loss_u loss_u 0.7485 (0.7419) acc_u 31.2500 (32.5000) lr 6.7608e-04 eta 0:00:16
epoch [123/200] batch [15/46] time 0.352 (0.452) data 0.221 (0.322) loss_u loss_u 0.8110 (0.7530) acc_u 21.8750 (30.6250) lr 6.7608e-04 eta 0:00:14
epoch [123/200] batch [20/46] time 0.425 (0.451) data 0.294 (0.320) loss_u loss_u 0.7432 (0.7603) acc_u 25.0000 (30.0000) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [25/46] time 0.361 (0.448) data 0.229 (0.317) loss_u loss_u 0.7617 (0.7615) acc_u 43.7500 (30.5000) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [30/46] time 0.420 (0.444) data 0.289 (0.314) loss_u loss_u 0.8057 (0.7604) acc_u 25.0000 (30.6250) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [35/46] time 0.553 (0.444) data 0.423 (0.313) loss_u loss_u 0.7231 (0.7586) acc_u 31.2500 (30.3571) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [40/46] time 0.443 (0.443) data 0.313 (0.312) loss_u loss_u 0.7871 (0.7549) acc_u 21.8750 (30.7812) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [45/46] time 0.517 (0.441) data 0.386 (0.310) loss_u loss_u 0.7437 (0.7599) acc_u 31.2500 (30.2778) lr 6.7608e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1342
confident_label rate tensor(0.5201, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1631
clean true:1612
clean false:19
clean_rate:0.9883507050889025
noisy true:182
noisy false:1323
after delete: len(clean_dataset) 1631
after delete: len(noisy_dataset) 1505
epoch [124/200] batch [5/50] time 0.499 (0.466) data 0.368 (0.335) loss_x loss_x 1.3896 (1.1301) acc_x 68.7500 (71.8750) lr 6.6126e-04 eta 0:00:20
epoch [124/200] batch [10/50] time 0.489 (0.460) data 0.359 (0.330) loss_x loss_x 0.9585 (1.0862) acc_x 71.8750 (71.8750) lr 6.6126e-04 eta 0:00:18
epoch [124/200] batch [15/50] time 0.469 (0.449) data 0.339 (0.318) loss_x loss_x 1.2617 (1.0525) acc_x 59.3750 (71.4583) lr 6.6126e-04 eta 0:00:15
epoch [124/200] batch [20/50] time 0.401 (0.459) data 0.270 (0.329) loss_x loss_x 0.9111 (1.0409) acc_x 81.2500 (72.5000) lr 6.6126e-04 eta 0:00:13
epoch [124/200] batch [25/50] time 0.502 (0.471) data 0.371 (0.341) loss_x loss_x 1.5059 (1.0871) acc_x 68.7500 (72.5000) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [30/50] time 0.460 (0.470) data 0.329 (0.340) loss_x loss_x 1.0059 (1.1299) acc_x 71.8750 (71.5625) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [35/50] time 0.610 (0.481) data 0.480 (0.350) loss_x loss_x 0.8940 (1.0969) acc_x 84.3750 (73.0357) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [40/50] time 0.467 (0.479) data 0.337 (0.349) loss_x loss_x 1.1738 (1.1128) acc_x 68.7500 (72.4219) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [45/50] time 0.426 (0.473) data 0.296 (0.343) loss_x loss_x 1.0117 (1.1149) acc_x 78.1250 (72.2222) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [50/50] time 0.378 (0.474) data 0.247 (0.344) loss_x loss_x 0.6943 (1.1232) acc_x 87.5000 (72.0625) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/47] time 0.379 (0.469) data 0.248 (0.339) loss_u loss_u 0.7544 (0.7778) acc_u 28.1250 (29.3750) lr 6.6126e-04 eta 0:00:19
epoch [124/200] batch [10/47] time 0.405 (0.465) data 0.274 (0.334) loss_u loss_u 0.7188 (0.7641) acc_u 34.3750 (30.3125) lr 6.6126e-04 eta 0:00:17
epoch [124/200] batch [15/47] time 0.431 (0.460) data 0.300 (0.329) loss_u loss_u 0.8125 (0.7634) acc_u 21.8750 (30.2083) lr 6.6126e-04 eta 0:00:14
epoch [124/200] batch [20/47] time 0.392 (0.457) data 0.261 (0.327) loss_u loss_u 0.7959 (0.7609) acc_u 28.1250 (31.2500) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [25/47] time 0.516 (0.454) data 0.385 (0.323) loss_u loss_u 0.7085 (0.7554) acc_u 37.5000 (31.3750) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [30/47] time 0.352 (0.451) data 0.220 (0.320) loss_u loss_u 0.7534 (0.7563) acc_u 31.2500 (30.9375) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [35/47] time 0.399 (0.447) data 0.268 (0.317) loss_u loss_u 0.6753 (0.7556) acc_u 37.5000 (30.7143) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [40/47] time 0.571 (0.450) data 0.440 (0.319) loss_u loss_u 0.6167 (0.7537) acc_u 40.6250 (30.7812) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [45/47] time 0.530 (0.448) data 0.399 (0.318) loss_u loss_u 0.7373 (0.7559) acc_u 28.1250 (30.4167) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1320
confident_label rate tensor(0.5274, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1654
clean true:1637
clean false:17
clean_rate:0.9897218863361548
noisy true:179
noisy false:1303
after delete: len(clean_dataset) 1654
after delete: len(noisy_dataset) 1482
epoch [125/200] batch [5/51] time 0.466 (0.441) data 0.335 (0.311) loss_x loss_x 1.4297 (1.3170) acc_x 65.6250 (70.0000) lr 6.4653e-04 eta 0:00:20
epoch [125/200] batch [10/51] time 0.474 (0.441) data 0.343 (0.311) loss_x loss_x 0.6763 (1.2215) acc_x 81.2500 (70.9375) lr 6.4653e-04 eta 0:00:18
epoch [125/200] batch [15/51] time 0.599 (0.450) data 0.468 (0.319) loss_x loss_x 1.0498 (1.1740) acc_x 71.8750 (72.2917) lr 6.4653e-04 eta 0:00:16
epoch [125/200] batch [20/51] time 0.456 (0.464) data 0.325 (0.333) loss_x loss_x 1.1533 (1.1798) acc_x 62.5000 (70.6250) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [25/51] time 0.386 (0.451) data 0.255 (0.321) loss_x loss_x 0.8071 (1.1547) acc_x 78.1250 (71.2500) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [30/51] time 0.433 (0.446) data 0.302 (0.315) loss_x loss_x 1.1436 (1.1922) acc_x 78.1250 (70.6250) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [35/51] time 0.454 (0.444) data 0.323 (0.313) loss_x loss_x 1.0635 (1.1676) acc_x 71.8750 (71.0714) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [40/51] time 0.488 (0.445) data 0.358 (0.314) loss_x loss_x 1.0938 (1.1478) acc_x 75.0000 (71.3281) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [45/51] time 0.488 (0.448) data 0.357 (0.318) loss_x loss_x 0.8530 (1.1405) acc_x 78.1250 (71.2500) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [50/51] time 0.423 (0.451) data 0.292 (0.320) loss_x loss_x 0.8418 (1.1418) acc_x 81.2500 (70.5625) lr 6.4653e-04 eta 0:00:00
epoch [125/200] batch [5/46] time 0.439 (0.445) data 0.308 (0.315) loss_u loss_u 0.7109 (0.7335) acc_u 37.5000 (35.0000) lr 6.4653e-04 eta 0:00:18
epoch [125/200] batch [10/46] time 0.415 (0.440) data 0.284 (0.310) loss_u loss_u 0.7212 (0.7520) acc_u 34.3750 (31.2500) lr 6.4653e-04 eta 0:00:15
epoch [125/200] batch [15/46] time 0.439 (0.440) data 0.308 (0.309) loss_u loss_u 0.7222 (0.7541) acc_u 40.6250 (31.4583) lr 6.4653e-04 eta 0:00:13
epoch [125/200] batch [20/46] time 0.361 (0.439) data 0.230 (0.308) loss_u loss_u 0.8169 (0.7529) acc_u 18.7500 (30.9375) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [25/46] time 0.357 (0.436) data 0.226 (0.305) loss_u loss_u 0.7373 (0.7566) acc_u 40.6250 (30.3750) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [30/46] time 0.386 (0.439) data 0.255 (0.308) loss_u loss_u 0.7852 (0.7582) acc_u 18.7500 (30.0000) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [35/46] time 0.489 (0.438) data 0.357 (0.307) loss_u loss_u 0.8467 (0.7631) acc_u 12.5000 (29.3750) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [40/46] time 0.439 (0.437) data 0.308 (0.306) loss_u loss_u 0.7456 (0.7621) acc_u 34.3750 (29.6875) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [45/46] time 0.362 (0.436) data 0.231 (0.305) loss_u loss_u 0.7612 (0.7649) acc_u 28.1250 (29.2361) lr 6.4653e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1362
confident_label rate tensor(0.5188, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1627
clean true:1609
clean false:18
clean_rate:0.9889366933005531
noisy true:165
noisy false:1344
after delete: len(clean_dataset) 1627
after delete: len(noisy_dataset) 1509
epoch [126/200] batch [5/50] time 0.629 (0.516) data 0.499 (0.386) loss_x loss_x 1.5557 (1.1334) acc_x 65.6250 (73.7500) lr 6.3188e-04 eta 0:00:23
epoch [126/200] batch [10/50] time 0.548 (0.492) data 0.418 (0.362) loss_x loss_x 0.9438 (1.0472) acc_x 75.0000 (75.0000) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [15/50] time 0.392 (0.471) data 0.262 (0.341) loss_x loss_x 1.3926 (1.0675) acc_x 62.5000 (75.0000) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [20/50] time 0.488 (0.467) data 0.358 (0.337) loss_x loss_x 1.3145 (1.1253) acc_x 59.3750 (72.1875) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [25/50] time 0.453 (0.474) data 0.322 (0.343) loss_x loss_x 1.6221 (1.1324) acc_x 62.5000 (72.5000) lr 6.3188e-04 eta 0:00:11
epoch [126/200] batch [30/50] time 0.531 (0.468) data 0.401 (0.338) loss_x loss_x 1.1914 (1.1176) acc_x 65.6250 (72.3958) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [35/50] time 0.426 (0.468) data 0.296 (0.337) loss_x loss_x 1.4053 (1.1441) acc_x 71.8750 (71.9643) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [40/50] time 0.470 (0.469) data 0.340 (0.338) loss_x loss_x 0.9849 (1.1452) acc_x 65.6250 (71.5625) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [45/50] time 0.413 (0.466) data 0.283 (0.336) loss_x loss_x 0.8677 (1.1294) acc_x 78.1250 (72.0833) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [50/50] time 0.416 (0.458) data 0.285 (0.328) loss_x loss_x 1.6826 (1.1615) acc_x 59.3750 (71.6875) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/47] time 0.481 (0.453) data 0.349 (0.322) loss_u loss_u 0.7847 (0.7169) acc_u 25.0000 (36.8750) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [10/47] time 0.451 (0.451) data 0.320 (0.321) loss_u loss_u 0.8232 (0.7394) acc_u 21.8750 (34.3750) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [15/47] time 0.451 (0.446) data 0.320 (0.315) loss_u loss_u 0.6567 (0.7396) acc_u 46.8750 (33.5417) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [20/47] time 0.359 (0.440) data 0.228 (0.309) loss_u loss_u 0.7539 (0.7356) acc_u 31.2500 (34.2188) lr 6.3188e-04 eta 0:00:11
epoch [126/200] batch [25/47] time 0.387 (0.442) data 0.256 (0.311) loss_u loss_u 0.8110 (0.7403) acc_u 18.7500 (33.5000) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [30/47] time 0.360 (0.440) data 0.229 (0.309) loss_u loss_u 0.7261 (0.7473) acc_u 28.1250 (32.1875) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [35/47] time 0.440 (0.439) data 0.309 (0.308) loss_u loss_u 0.8027 (0.7519) acc_u 21.8750 (31.5179) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [40/47] time 0.439 (0.438) data 0.308 (0.308) loss_u loss_u 0.8188 (0.7571) acc_u 21.8750 (30.7031) lr 6.3188e-04 eta 0:00:03
epoch [126/200] batch [45/47] time 0.405 (0.437) data 0.274 (0.306) loss_u loss_u 0.6816 (0.7572) acc_u 37.5000 (30.6250) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1290
confident_label rate tensor(0.5364, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1682
clean true:1662
clean false:20
clean_rate:0.9881093935790726
noisy true:184
noisy false:1270
after delete: len(clean_dataset) 1682
after delete: len(noisy_dataset) 1454
epoch [127/200] batch [5/52] time 0.435 (0.428) data 0.304 (0.298) loss_x loss_x 0.7734 (1.0362) acc_x 71.8750 (73.7500) lr 6.1732e-04 eta 0:00:20
epoch [127/200] batch [10/52] time 0.378 (0.448) data 0.248 (0.318) loss_x loss_x 0.8950 (0.9469) acc_x 78.1250 (76.2500) lr 6.1732e-04 eta 0:00:18
epoch [127/200] batch [15/52] time 0.514 (0.445) data 0.385 (0.315) loss_x loss_x 0.6836 (1.0154) acc_x 81.2500 (75.0000) lr 6.1732e-04 eta 0:00:16
epoch [127/200] batch [20/52] time 0.508 (0.443) data 0.378 (0.313) loss_x loss_x 0.9595 (1.0388) acc_x 71.8750 (73.7500) lr 6.1732e-04 eta 0:00:14
epoch [127/200] batch [25/52] time 0.415 (0.443) data 0.285 (0.313) loss_x loss_x 1.5449 (1.0977) acc_x 65.6250 (72.5000) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [30/52] time 0.418 (0.445) data 0.288 (0.315) loss_x loss_x 1.0732 (1.1094) acc_x 75.0000 (72.1875) lr 6.1732e-04 eta 0:00:09
epoch [127/200] batch [35/52] time 0.550 (0.450) data 0.420 (0.320) loss_x loss_x 1.0645 (1.1004) acc_x 62.5000 (71.6964) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [40/52] time 0.570 (0.451) data 0.440 (0.321) loss_x loss_x 1.9424 (1.1473) acc_x 50.0000 (71.0938) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [45/52] time 0.322 (0.446) data 0.191 (0.316) loss_x loss_x 0.8130 (1.1488) acc_x 81.2500 (71.0417) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [50/52] time 0.412 (0.447) data 0.283 (0.317) loss_x loss_x 0.9224 (1.1396) acc_x 71.8750 (71.1250) lr 6.1732e-04 eta 0:00:00
epoch [127/200] batch [5/45] time 0.324 (0.442) data 0.193 (0.311) loss_u loss_u 0.7012 (0.7779) acc_u 34.3750 (27.5000) lr 6.1732e-04 eta 0:00:17
epoch [127/200] batch [10/45] time 0.358 (0.438) data 0.228 (0.308) loss_u loss_u 0.6328 (0.7714) acc_u 46.8750 (29.3750) lr 6.1732e-04 eta 0:00:15
epoch [127/200] batch [15/45] time 0.432 (0.436) data 0.303 (0.306) loss_u loss_u 0.7642 (0.7750) acc_u 25.0000 (27.9167) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [20/45] time 0.352 (0.435) data 0.220 (0.305) loss_u loss_u 0.7026 (0.7708) acc_u 34.3750 (28.5938) lr 6.1732e-04 eta 0:00:10
epoch [127/200] batch [25/45] time 0.353 (0.436) data 0.223 (0.305) loss_u loss_u 0.7026 (0.7645) acc_u 43.7500 (29.8750) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [30/45] time 0.367 (0.436) data 0.236 (0.305) loss_u loss_u 0.8057 (0.7682) acc_u 15.6250 (29.0625) lr 6.1732e-04 eta 0:00:06
epoch [127/200] batch [35/45] time 0.329 (0.436) data 0.198 (0.306) loss_u loss_u 0.7778 (0.7664) acc_u 34.3750 (29.6429) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [40/45] time 0.354 (0.435) data 0.223 (0.304) loss_u loss_u 0.7227 (0.7641) acc_u 34.3750 (29.6875) lr 6.1732e-04 eta 0:00:02
epoch [127/200] batch [45/45] time 0.541 (0.436) data 0.410 (0.306) loss_u loss_u 0.8130 (0.7646) acc_u 25.0000 (29.7222) lr 6.1732e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1331
confident_label rate tensor(0.5236, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1642
clean true:1629
clean false:13
clean_rate:0.9920828258221681
noisy true:176
noisy false:1318
after delete: len(clean_dataset) 1642
after delete: len(noisy_dataset) 1494
epoch [128/200] batch [5/51] time 0.501 (0.474) data 0.372 (0.343) loss_x loss_x 1.1270 (1.1018) acc_x 59.3750 (69.3750) lr 6.0285e-04 eta 0:00:21
epoch [128/200] batch [10/51] time 0.388 (0.454) data 0.258 (0.324) loss_x loss_x 1.1113 (1.0873) acc_x 75.0000 (70.9375) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [15/51] time 0.511 (0.457) data 0.382 (0.328) loss_x loss_x 1.1650 (1.1497) acc_x 75.0000 (70.6250) lr 6.0285e-04 eta 0:00:16
epoch [128/200] batch [20/51] time 0.391 (0.460) data 0.261 (0.330) loss_x loss_x 1.7705 (1.1756) acc_x 59.3750 (71.0938) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [25/51] time 0.385 (0.456) data 0.254 (0.326) loss_x loss_x 0.5952 (1.1607) acc_x 78.1250 (71.1250) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [30/51] time 0.485 (0.454) data 0.355 (0.324) loss_x loss_x 1.0400 (1.1251) acc_x 75.0000 (71.7708) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [35/51] time 0.469 (0.455) data 0.339 (0.325) loss_x loss_x 1.1396 (1.1354) acc_x 71.8750 (70.9821) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [40/51] time 0.471 (0.459) data 0.341 (0.329) loss_x loss_x 0.6797 (1.1442) acc_x 78.1250 (70.9375) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [45/51] time 0.610 (0.461) data 0.479 (0.331) loss_x loss_x 1.5508 (1.1379) acc_x 53.1250 (71.3889) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [50/51] time 0.570 (0.463) data 0.439 (0.333) loss_x loss_x 1.0225 (1.1388) acc_x 81.2500 (71.5625) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/46] time 0.344 (0.459) data 0.214 (0.328) loss_u loss_u 0.7344 (0.7543) acc_u 31.2500 (28.7500) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [10/46] time 0.370 (0.455) data 0.239 (0.324) loss_u loss_u 0.6797 (0.7637) acc_u 43.7500 (30.6250) lr 6.0285e-04 eta 0:00:16
epoch [128/200] batch [15/46] time 0.455 (0.453) data 0.324 (0.323) loss_u loss_u 0.7476 (0.7652) acc_u 31.2500 (31.0417) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [20/46] time 0.402 (0.449) data 0.271 (0.318) loss_u loss_u 0.7798 (0.7653) acc_u 31.2500 (31.2500) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [25/46] time 0.349 (0.448) data 0.218 (0.317) loss_u loss_u 0.7432 (0.7663) acc_u 37.5000 (31.2500) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [30/46] time 0.386 (0.446) data 0.255 (0.316) loss_u loss_u 0.8145 (0.7651) acc_u 18.7500 (30.8333) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [35/46] time 0.425 (0.444) data 0.294 (0.314) loss_u loss_u 0.7358 (0.7646) acc_u 31.2500 (31.0714) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [40/46] time 0.370 (0.446) data 0.239 (0.315) loss_u loss_u 0.7607 (0.7630) acc_u 34.3750 (31.0156) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [45/46] time 0.398 (0.443) data 0.267 (0.312) loss_u loss_u 0.8013 (0.7621) acc_u 28.1250 (31.5278) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1318
confident_label rate tensor(0.5277, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1655
clean true:1638
clean false:17
clean_rate:0.9897280966767371
noisy true:180
noisy false:1301
after delete: len(clean_dataset) 1655
after delete: len(noisy_dataset) 1481
epoch [129/200] batch [5/51] time 0.430 (0.429) data 0.300 (0.298) loss_x loss_x 0.7939 (1.0589) acc_x 81.2500 (72.5000) lr 5.8849e-04 eta 0:00:19
epoch [129/200] batch [10/51] time 0.657 (0.444) data 0.527 (0.314) loss_x loss_x 1.3516 (1.1090) acc_x 59.3750 (70.6250) lr 5.8849e-04 eta 0:00:18
epoch [129/200] batch [15/51] time 0.415 (0.446) data 0.284 (0.315) loss_x loss_x 0.5664 (1.0979) acc_x 81.2500 (71.4583) lr 5.8849e-04 eta 0:00:16
epoch [129/200] batch [20/51] time 0.455 (0.453) data 0.324 (0.322) loss_x loss_x 0.7651 (1.0812) acc_x 81.2500 (72.8125) lr 5.8849e-04 eta 0:00:14
epoch [129/200] batch [25/51] time 0.607 (0.457) data 0.477 (0.326) loss_x loss_x 1.5645 (1.1189) acc_x 62.5000 (72.2500) lr 5.8849e-04 eta 0:00:11
epoch [129/200] batch [30/51] time 0.613 (0.464) data 0.482 (0.333) loss_x loss_x 0.7964 (1.0691) acc_x 81.2500 (73.2292) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [35/51] time 0.365 (0.455) data 0.235 (0.324) loss_x loss_x 0.8799 (1.0602) acc_x 87.5000 (73.6607) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [40/51] time 0.379 (0.458) data 0.249 (0.327) loss_x loss_x 1.0244 (1.0601) acc_x 68.7500 (72.8125) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [45/51] time 0.379 (0.455) data 0.248 (0.324) loss_x loss_x 0.9727 (1.0624) acc_x 71.8750 (72.5000) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [50/51] time 0.433 (0.453) data 0.304 (0.323) loss_x loss_x 1.2744 (1.0607) acc_x 65.6250 (72.6250) lr 5.8849e-04 eta 0:00:00
epoch [129/200] batch [5/46] time 0.355 (0.448) data 0.224 (0.317) loss_u loss_u 0.7534 (0.7645) acc_u 37.5000 (33.1250) lr 5.8849e-04 eta 0:00:18
epoch [129/200] batch [10/46] time 0.454 (0.442) data 0.323 (0.311) loss_u loss_u 0.7915 (0.7657) acc_u 31.2500 (31.2500) lr 5.8849e-04 eta 0:00:15
epoch [129/200] batch [15/46] time 0.449 (0.444) data 0.318 (0.313) loss_u loss_u 0.7446 (0.7667) acc_u 34.3750 (31.4583) lr 5.8849e-04 eta 0:00:13
epoch [129/200] batch [20/46] time 0.498 (0.443) data 0.367 (0.313) loss_u loss_u 0.8096 (0.7617) acc_u 18.7500 (31.7188) lr 5.8849e-04 eta 0:00:11
epoch [129/200] batch [25/46] time 0.498 (0.442) data 0.367 (0.311) loss_u loss_u 0.7539 (0.7628) acc_u 31.2500 (31.0000) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [30/46] time 0.507 (0.441) data 0.376 (0.310) loss_u loss_u 0.8003 (0.7624) acc_u 31.2500 (31.4583) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [35/46] time 0.375 (0.441) data 0.244 (0.310) loss_u loss_u 0.7842 (0.7632) acc_u 25.0000 (30.9821) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [40/46] time 0.470 (0.439) data 0.340 (0.308) loss_u loss_u 0.6577 (0.7607) acc_u 46.8750 (30.9375) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [45/46] time 0.524 (0.441) data 0.393 (0.310) loss_u loss_u 0.8433 (0.7644) acc_u 18.7500 (30.1389) lr 5.8849e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1330
confident_label rate tensor(0.5284, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1657
clean true:1636
clean false:21
clean_rate:0.9873264936632469
noisy true:170
noisy false:1309
after delete: len(clean_dataset) 1657
after delete: len(noisy_dataset) 1479
epoch [130/200] batch [5/51] time 0.638 (0.499) data 0.507 (0.368) loss_x loss_x 1.0488 (1.0811) acc_x 81.2500 (78.7500) lr 5.7422e-04 eta 0:00:22
epoch [130/200] batch [10/51] time 0.446 (0.466) data 0.315 (0.336) loss_x loss_x 1.0566 (1.1408) acc_x 75.0000 (74.3750) lr 5.7422e-04 eta 0:00:19
epoch [130/200] batch [15/51] time 0.530 (0.467) data 0.400 (0.336) loss_x loss_x 1.4033 (1.1709) acc_x 71.8750 (72.7083) lr 5.7422e-04 eta 0:00:16
epoch [130/200] batch [20/51] time 0.499 (0.475) data 0.369 (0.344) loss_x loss_x 1.2246 (1.1633) acc_x 81.2500 (73.1250) lr 5.7422e-04 eta 0:00:14
epoch [130/200] batch [25/51] time 0.449 (0.471) data 0.319 (0.341) loss_x loss_x 0.8887 (1.1472) acc_x 75.0000 (72.6250) lr 5.7422e-04 eta 0:00:12
epoch [130/200] batch [30/51] time 0.354 (0.466) data 0.224 (0.336) loss_x loss_x 0.9751 (1.1245) acc_x 78.1250 (72.8125) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [35/51] time 0.528 (0.465) data 0.398 (0.335) loss_x loss_x 1.1807 (1.1730) acc_x 65.6250 (72.0536) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [40/51] time 0.335 (0.458) data 0.205 (0.328) loss_x loss_x 1.0283 (1.1628) acc_x 68.7500 (72.1875) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [45/51] time 0.539 (0.456) data 0.409 (0.326) loss_x loss_x 1.3125 (1.1746) acc_x 68.7500 (72.2222) lr 5.7422e-04 eta 0:00:02
epoch [130/200] batch [50/51] time 0.413 (0.452) data 0.283 (0.321) loss_x loss_x 0.9326 (1.1914) acc_x 65.6250 (71.3125) lr 5.7422e-04 eta 0:00:00
epoch [130/200] batch [5/46] time 0.414 (0.450) data 0.283 (0.320) loss_u loss_u 0.8530 (0.7792) acc_u 18.7500 (26.8750) lr 5.7422e-04 eta 0:00:18
epoch [130/200] batch [10/46] time 0.368 (0.445) data 0.236 (0.315) loss_u loss_u 0.8066 (0.7825) acc_u 21.8750 (27.1875) lr 5.7422e-04 eta 0:00:16
epoch [130/200] batch [15/46] time 0.359 (0.441) data 0.229 (0.310) loss_u loss_u 0.7148 (0.7703) acc_u 40.6250 (29.7917) lr 5.7422e-04 eta 0:00:13
epoch [130/200] batch [20/46] time 0.345 (0.437) data 0.214 (0.306) loss_u loss_u 0.8193 (0.7661) acc_u 18.7500 (29.3750) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [25/46] time 0.480 (0.439) data 0.349 (0.308) loss_u loss_u 0.7007 (0.7571) acc_u 34.3750 (29.8750) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [30/46] time 0.428 (0.439) data 0.297 (0.308) loss_u loss_u 0.7759 (0.7553) acc_u 28.1250 (30.1042) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [35/46] time 0.434 (0.437) data 0.303 (0.306) loss_u loss_u 0.7632 (0.7588) acc_u 28.1250 (29.8214) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [40/46] time 0.491 (0.436) data 0.360 (0.306) loss_u loss_u 0.7300 (0.7552) acc_u 31.2500 (30.3906) lr 5.7422e-04 eta 0:00:02
epoch [130/200] batch [45/46] time 0.438 (0.439) data 0.307 (0.308) loss_u loss_u 0.7417 (0.7549) acc_u 34.3750 (30.4167) lr 5.7422e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1295
confident_label rate tensor(0.5325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1670
clean true:1652
clean false:18
clean_rate:0.9892215568862276
noisy true:189
noisy false:1277
after delete: len(clean_dataset) 1670
after delete: len(noisy_dataset) 1466
epoch [131/200] batch [5/52] time 0.443 (0.461) data 0.313 (0.331) loss_x loss_x 1.0654 (1.0487) acc_x 75.0000 (75.6250) lr 5.6006e-04 eta 0:00:21
epoch [131/200] batch [10/52] time 0.606 (0.460) data 0.476 (0.329) loss_x loss_x 1.1426 (1.1212) acc_x 78.1250 (72.5000) lr 5.6006e-04 eta 0:00:19
epoch [131/200] batch [15/52] time 0.496 (0.472) data 0.365 (0.341) loss_x loss_x 0.9277 (1.0824) acc_x 84.3750 (72.7083) lr 5.6006e-04 eta 0:00:17
epoch [131/200] batch [20/52] time 0.354 (0.465) data 0.223 (0.334) loss_x loss_x 1.2393 (1.0940) acc_x 68.7500 (72.3438) lr 5.6006e-04 eta 0:00:14
epoch [131/200] batch [25/52] time 0.357 (0.457) data 0.227 (0.327) loss_x loss_x 0.9609 (1.0892) acc_x 68.7500 (71.2500) lr 5.6006e-04 eta 0:00:12
epoch [131/200] batch [30/52] time 0.352 (0.452) data 0.222 (0.321) loss_x loss_x 1.1328 (1.1165) acc_x 71.8750 (70.6250) lr 5.6006e-04 eta 0:00:09
epoch [131/200] batch [35/52] time 0.444 (0.454) data 0.314 (0.324) loss_x loss_x 0.9136 (1.1189) acc_x 75.0000 (70.5357) lr 5.6006e-04 eta 0:00:07
epoch [131/200] batch [40/52] time 0.409 (0.453) data 0.278 (0.323) loss_x loss_x 0.7222 (1.1287) acc_x 84.3750 (70.5469) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [45/52] time 0.391 (0.451) data 0.261 (0.321) loss_x loss_x 0.7646 (1.1333) acc_x 84.3750 (70.8333) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [50/52] time 0.417 (0.448) data 0.287 (0.318) loss_x loss_x 0.9990 (1.1599) acc_x 75.0000 (70.1250) lr 5.6006e-04 eta 0:00:00
epoch [131/200] batch [5/45] time 0.382 (0.446) data 0.251 (0.316) loss_u loss_u 0.7524 (0.7662) acc_u 31.2500 (28.7500) lr 5.6006e-04 eta 0:00:17
epoch [131/200] batch [10/45] time 0.342 (0.444) data 0.211 (0.313) loss_u loss_u 0.8472 (0.7857) acc_u 18.7500 (26.2500) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [15/45] time 0.366 (0.441) data 0.235 (0.311) loss_u loss_u 0.7261 (0.7697) acc_u 34.3750 (28.7500) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [20/45] time 0.460 (0.439) data 0.329 (0.308) loss_u loss_u 0.6851 (0.7577) acc_u 37.5000 (30.9375) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [25/45] time 0.477 (0.438) data 0.346 (0.308) loss_u loss_u 0.6646 (0.7503) acc_u 43.7500 (32.2500) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [30/45] time 0.758 (0.444) data 0.627 (0.313) loss_u loss_u 0.8628 (0.7542) acc_u 15.6250 (31.5625) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [35/45] time 0.356 (0.442) data 0.225 (0.311) loss_u loss_u 0.8433 (0.7526) acc_u 18.7500 (31.6071) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [40/45] time 0.488 (0.441) data 0.356 (0.310) loss_u loss_u 0.6924 (0.7516) acc_u 43.7500 (31.7969) lr 5.6006e-04 eta 0:00:02
epoch [131/200] batch [45/45] time 0.325 (0.438) data 0.194 (0.307) loss_u loss_u 0.7568 (0.7513) acc_u 31.2500 (31.7361) lr 5.6006e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1303
confident_label rate tensor(0.5364, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1682
clean true:1664
clean false:18
clean_rate:0.9892984542211652
noisy true:169
noisy false:1285
after delete: len(clean_dataset) 1682
after delete: len(noisy_dataset) 1454
epoch [132/200] batch [5/52] time 0.360 (0.378) data 0.229 (0.247) loss_x loss_x 0.9766 (1.3117) acc_x 84.3750 (73.7500) lr 5.4601e-04 eta 0:00:17
epoch [132/200] batch [10/52] time 0.422 (0.406) data 0.291 (0.276) loss_x loss_x 1.1895 (1.2635) acc_x 68.7500 (73.7500) lr 5.4601e-04 eta 0:00:17
epoch [132/200] batch [15/52] time 0.458 (0.429) data 0.326 (0.299) loss_x loss_x 0.7886 (1.2370) acc_x 78.1250 (71.8750) lr 5.4601e-04 eta 0:00:15
epoch [132/200] batch [20/52] time 0.455 (0.430) data 0.325 (0.300) loss_x loss_x 0.8936 (1.2075) acc_x 81.2500 (71.8750) lr 5.4601e-04 eta 0:00:13
epoch [132/200] batch [25/52] time 0.414 (0.438) data 0.284 (0.308) loss_x loss_x 0.9893 (1.1822) acc_x 71.8750 (71.5000) lr 5.4601e-04 eta 0:00:11
epoch [132/200] batch [30/52] time 0.564 (0.445) data 0.433 (0.315) loss_x loss_x 1.2529 (1.1866) acc_x 68.7500 (71.2500) lr 5.4601e-04 eta 0:00:09
epoch [132/200] batch [35/52] time 0.381 (0.442) data 0.251 (0.311) loss_x loss_x 0.7266 (1.1665) acc_x 81.2500 (71.5179) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [40/52] time 0.530 (0.449) data 0.400 (0.319) loss_x loss_x 1.1113 (1.1567) acc_x 78.1250 (71.5625) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [45/52] time 0.609 (0.452) data 0.478 (0.322) loss_x loss_x 0.8911 (1.1581) acc_x 75.0000 (71.2500) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [50/52] time 0.332 (0.451) data 0.202 (0.321) loss_x loss_x 1.2832 (1.1639) acc_x 65.6250 (70.9375) lr 5.4601e-04 eta 0:00:00
epoch [132/200] batch [5/45] time 0.393 (0.458) data 0.264 (0.327) loss_u loss_u 0.6748 (0.7136) acc_u 46.8750 (37.5000) lr 5.4601e-04 eta 0:00:18
epoch [132/200] batch [10/45] time 0.469 (0.455) data 0.339 (0.325) loss_u loss_u 0.8921 (0.7479) acc_u 6.2500 (31.8750) lr 5.4601e-04 eta 0:00:15
epoch [132/200] batch [15/45] time 0.351 (0.451) data 0.220 (0.321) loss_u loss_u 0.7275 (0.7497) acc_u 34.3750 (30.8333) lr 5.4601e-04 eta 0:00:13
epoch [132/200] batch [20/45] time 0.273 (0.446) data 0.142 (0.316) loss_u loss_u 0.7368 (0.7543) acc_u 28.1250 (30.4688) lr 5.4601e-04 eta 0:00:11
epoch [132/200] batch [25/45] time 0.514 (0.444) data 0.382 (0.314) loss_u loss_u 0.6968 (0.7555) acc_u 40.6250 (30.6250) lr 5.4601e-04 eta 0:00:08
epoch [132/200] batch [30/45] time 0.308 (0.442) data 0.177 (0.311) loss_u loss_u 0.8003 (0.7610) acc_u 18.7500 (30.3125) lr 5.4601e-04 eta 0:00:06
epoch [132/200] batch [35/45] time 0.425 (0.439) data 0.295 (0.308) loss_u loss_u 0.8203 (0.7634) acc_u 25.0000 (30.1786) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [40/45] time 0.376 (0.440) data 0.244 (0.309) loss_u loss_u 0.7061 (0.7572) acc_u 34.3750 (30.9375) lr 5.4601e-04 eta 0:00:02
epoch [132/200] batch [45/45] time 0.462 (0.439) data 0.331 (0.308) loss_u loss_u 0.7266 (0.7572) acc_u 37.5000 (31.1111) lr 5.4601e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1366
confident_label rate tensor(0.5156, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1617
clean true:1596
clean false:21
clean_rate:0.987012987012987
noisy true:174
noisy false:1345
after delete: len(clean_dataset) 1617
after delete: len(noisy_dataset) 1519
epoch [133/200] batch [5/50] time 0.465 (0.466) data 0.335 (0.336) loss_x loss_x 0.9883 (0.9210) acc_x 81.2500 (78.7500) lr 5.3207e-04 eta 0:00:20
epoch [133/200] batch [10/50] time 0.446 (0.457) data 0.315 (0.326) loss_x loss_x 0.8193 (0.9843) acc_x 81.2500 (76.8750) lr 5.3207e-04 eta 0:00:18
epoch [133/200] batch [15/50] time 0.521 (0.459) data 0.390 (0.329) loss_x loss_x 1.0098 (0.9843) acc_x 71.8750 (76.4583) lr 5.3207e-04 eta 0:00:16
epoch [133/200] batch [20/50] time 0.423 (0.475) data 0.293 (0.344) loss_x loss_x 0.9126 (1.0151) acc_x 78.1250 (74.8438) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [25/50] time 0.382 (0.469) data 0.252 (0.339) loss_x loss_x 1.1875 (1.0378) acc_x 75.0000 (74.2500) lr 5.3207e-04 eta 0:00:11
epoch [133/200] batch [30/50] time 0.392 (0.464) data 0.262 (0.334) loss_x loss_x 1.0371 (1.0541) acc_x 68.7500 (73.4375) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [35/50] time 0.464 (0.462) data 0.334 (0.331) loss_x loss_x 1.1064 (1.0564) acc_x 75.0000 (73.1250) lr 5.3207e-04 eta 0:00:06
epoch [133/200] batch [40/50] time 0.349 (0.464) data 0.218 (0.333) loss_x loss_x 0.8457 (1.0862) acc_x 78.1250 (72.6562) lr 5.3207e-04 eta 0:00:04
epoch [133/200] batch [45/50] time 0.439 (0.462) data 0.308 (0.331) loss_x loss_x 1.6348 (1.1149) acc_x 65.6250 (72.0833) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [50/50] time 0.442 (0.458) data 0.311 (0.327) loss_x loss_x 1.0957 (1.1007) acc_x 75.0000 (72.4375) lr 5.3207e-04 eta 0:00:00
epoch [133/200] batch [5/47] time 0.399 (0.453) data 0.268 (0.323) loss_u loss_u 0.7090 (0.7296) acc_u 31.2500 (31.2500) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [10/47] time 0.342 (0.450) data 0.211 (0.319) loss_u loss_u 0.7764 (0.7531) acc_u 34.3750 (29.6875) lr 5.3207e-04 eta 0:00:16
epoch [133/200] batch [15/47] time 0.435 (0.445) data 0.304 (0.314) loss_u loss_u 0.6885 (0.7442) acc_u 43.7500 (31.8750) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [20/47] time 0.469 (0.444) data 0.338 (0.313) loss_u loss_u 0.7944 (0.7522) acc_u 25.0000 (30.7812) lr 5.3207e-04 eta 0:00:11
epoch [133/200] batch [25/47] time 0.553 (0.440) data 0.422 (0.309) loss_u loss_u 0.6753 (0.7486) acc_u 40.6250 (31.1250) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [30/47] time 0.431 (0.438) data 0.300 (0.308) loss_u loss_u 0.7817 (0.7503) acc_u 28.1250 (30.8333) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [35/47] time 0.368 (0.437) data 0.237 (0.306) loss_u loss_u 0.7920 (0.7557) acc_u 25.0000 (30.1786) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [40/47] time 0.352 (0.437) data 0.220 (0.306) loss_u loss_u 0.7124 (0.7514) acc_u 34.3750 (30.7812) lr 5.3207e-04 eta 0:00:03
epoch [133/200] batch [45/47] time 0.413 (0.438) data 0.282 (0.307) loss_u loss_u 0.7437 (0.7532) acc_u 28.1250 (30.6944) lr 5.3207e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1317
confident_label rate tensor(0.5287, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1658
clean true:1636
clean false:22
clean_rate:0.9867310012062727
noisy true:183
noisy false:1295
after delete: len(clean_dataset) 1658
after delete: len(noisy_dataset) 1478
epoch [134/200] batch [5/51] time 0.480 (0.425) data 0.349 (0.294) loss_x loss_x 0.9424 (1.0318) acc_x 78.1250 (71.8750) lr 5.1825e-04 eta 0:00:19
epoch [134/200] batch [10/51] time 0.418 (0.472) data 0.288 (0.342) loss_x loss_x 0.9399 (1.0481) acc_x 81.2500 (72.1875) lr 5.1825e-04 eta 0:00:19
epoch [134/200] batch [15/51] time 0.539 (0.466) data 0.409 (0.336) loss_x loss_x 1.5254 (1.1147) acc_x 68.7500 (71.4583) lr 5.1825e-04 eta 0:00:16
epoch [134/200] batch [20/51] time 0.465 (0.465) data 0.334 (0.334) loss_x loss_x 0.8657 (1.1302) acc_x 75.0000 (71.4062) lr 5.1825e-04 eta 0:00:14
epoch [134/200] batch [25/51] time 0.461 (0.462) data 0.330 (0.331) loss_x loss_x 0.5298 (1.1017) acc_x 93.7500 (72.5000) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [30/51] time 0.457 (0.455) data 0.326 (0.324) loss_x loss_x 1.3750 (1.1085) acc_x 62.5000 (72.1875) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [35/51] time 0.441 (0.449) data 0.311 (0.318) loss_x loss_x 1.3721 (1.1014) acc_x 68.7500 (72.4107) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [40/51] time 0.492 (0.446) data 0.361 (0.315) loss_x loss_x 1.6523 (1.0989) acc_x 59.3750 (72.3438) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [45/51] time 0.380 (0.447) data 0.250 (0.317) loss_x loss_x 1.3164 (1.1017) acc_x 56.2500 (71.5972) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [50/51] time 0.376 (0.443) data 0.246 (0.312) loss_x loss_x 0.8306 (1.0920) acc_x 75.0000 (71.5000) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/46] time 0.401 (0.438) data 0.270 (0.308) loss_u loss_u 0.6675 (0.7583) acc_u 40.6250 (32.5000) lr 5.1825e-04 eta 0:00:17
epoch [134/200] batch [10/46] time 0.596 (0.438) data 0.464 (0.307) loss_u loss_u 0.7236 (0.7580) acc_u 34.3750 (32.1875) lr 5.1825e-04 eta 0:00:15
epoch [134/200] batch [15/46] time 0.402 (0.439) data 0.271 (0.308) loss_u loss_u 0.7358 (0.7405) acc_u 34.3750 (33.5417) lr 5.1825e-04 eta 0:00:13
epoch [134/200] batch [20/46] time 0.463 (0.442) data 0.331 (0.311) loss_u loss_u 0.7949 (0.7532) acc_u 21.8750 (31.4062) lr 5.1825e-04 eta 0:00:11
epoch [134/200] batch [25/46] time 0.500 (0.440) data 0.369 (0.310) loss_u loss_u 0.7646 (0.7553) acc_u 28.1250 (30.8750) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [30/46] time 0.488 (0.442) data 0.357 (0.311) loss_u loss_u 0.8057 (0.7594) acc_u 25.0000 (30.2083) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [35/46] time 0.406 (0.439) data 0.275 (0.308) loss_u loss_u 0.8716 (0.7619) acc_u 12.5000 (30.4464) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [40/46] time 0.362 (0.436) data 0.231 (0.305) loss_u loss_u 0.7754 (0.7638) acc_u 31.2500 (30.3906) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [45/46] time 0.530 (0.440) data 0.399 (0.309) loss_u loss_u 0.7729 (0.7648) acc_u 34.3750 (30.1389) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1307
confident_label rate tensor(0.5325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1670
clean true:1651
clean false:19
clean_rate:0.988622754491018
noisy true:178
noisy false:1288
after delete: len(clean_dataset) 1670
after delete: len(noisy_dataset) 1466
epoch [135/200] batch [5/52] time 0.521 (0.441) data 0.391 (0.311) loss_x loss_x 0.9985 (1.0055) acc_x 68.7500 (74.3750) lr 5.0454e-04 eta 0:00:20
epoch [135/200] batch [10/52] time 0.435 (0.454) data 0.305 (0.324) loss_x loss_x 0.8882 (1.0128) acc_x 71.8750 (74.0625) lr 5.0454e-04 eta 0:00:19
epoch [135/200] batch [15/52] time 0.413 (0.433) data 0.283 (0.302) loss_x loss_x 1.0146 (0.9923) acc_x 78.1250 (73.9583) lr 5.0454e-04 eta 0:00:16
epoch [135/200] batch [20/52] time 0.499 (0.455) data 0.368 (0.325) loss_x loss_x 1.4121 (1.0219) acc_x 62.5000 (72.6562) lr 5.0454e-04 eta 0:00:14
epoch [135/200] batch [25/52] time 0.383 (0.457) data 0.252 (0.327) loss_x loss_x 0.8203 (1.0393) acc_x 81.2500 (73.1250) lr 5.0454e-04 eta 0:00:12
epoch [135/200] batch [30/52] time 0.516 (0.467) data 0.386 (0.337) loss_x loss_x 1.4854 (1.0384) acc_x 71.8750 (73.6458) lr 5.0454e-04 eta 0:00:10
epoch [135/200] batch [35/52] time 0.403 (0.461) data 0.273 (0.331) loss_x loss_x 1.2266 (1.0333) acc_x 68.7500 (73.5714) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [40/52] time 0.490 (0.468) data 0.360 (0.338) loss_x loss_x 0.8252 (1.0427) acc_x 81.2500 (73.5156) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [45/52] time 0.391 (0.464) data 0.260 (0.333) loss_x loss_x 1.1494 (1.0545) acc_x 62.5000 (72.9861) lr 5.0454e-04 eta 0:00:03
epoch [135/200] batch [50/52] time 0.371 (0.459) data 0.241 (0.329) loss_x loss_x 1.1680 (1.0459) acc_x 75.0000 (73.6250) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/45] time 0.412 (0.452) data 0.281 (0.321) loss_u loss_u 0.7881 (0.7841) acc_u 25.0000 (26.8750) lr 5.0454e-04 eta 0:00:18
epoch [135/200] batch [10/45] time 0.531 (0.456) data 0.400 (0.326) loss_u loss_u 0.7104 (0.7878) acc_u 31.2500 (26.2500) lr 5.0454e-04 eta 0:00:15
epoch [135/200] batch [15/45] time 0.478 (0.453) data 0.347 (0.323) loss_u loss_u 0.6592 (0.7758) acc_u 43.7500 (28.7500) lr 5.0454e-04 eta 0:00:13
epoch [135/200] batch [20/45] time 0.501 (0.451) data 0.370 (0.321) loss_u loss_u 0.7222 (0.7729) acc_u 31.2500 (28.7500) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [25/45] time 0.482 (0.450) data 0.351 (0.319) loss_u loss_u 0.6743 (0.7741) acc_u 37.5000 (29.0000) lr 5.0454e-04 eta 0:00:08
epoch [135/200] batch [30/45] time 0.399 (0.448) data 0.268 (0.317) loss_u loss_u 0.6455 (0.7762) acc_u 40.6250 (28.5417) lr 5.0454e-04 eta 0:00:06
epoch [135/200] batch [35/45] time 0.479 (0.448) data 0.348 (0.317) loss_u loss_u 0.7456 (0.7684) acc_u 28.1250 (29.1964) lr 5.0454e-04 eta 0:00:04
epoch [135/200] batch [40/45] time 0.363 (0.443) data 0.231 (0.312) loss_u loss_u 0.6816 (0.7641) acc_u 34.3750 (29.5312) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [45/45] time 0.509 (0.442) data 0.377 (0.311) loss_u loss_u 0.8013 (0.7678) acc_u 18.7500 (28.6806) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1295
confident_label rate tensor(0.5395, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1692
clean true:1673
clean false:19
clean_rate:0.9887706855791962
noisy true:168
noisy false:1276
after delete: len(clean_dataset) 1692
after delete: len(noisy_dataset) 1444
epoch [136/200] batch [5/52] time 0.588 (0.492) data 0.458 (0.361) loss_x loss_x 0.9644 (1.0798) acc_x 81.2500 (70.6250) lr 4.9096e-04 eta 0:00:23
epoch [136/200] batch [10/52] time 0.500 (0.474) data 0.370 (0.343) loss_x loss_x 0.6582 (0.9929) acc_x 84.3750 (74.0625) lr 4.9096e-04 eta 0:00:19
epoch [136/200] batch [15/52] time 0.412 (0.459) data 0.281 (0.329) loss_x loss_x 1.0918 (1.0170) acc_x 68.7500 (72.2917) lr 4.9096e-04 eta 0:00:16
epoch [136/200] batch [20/52] time 0.443 (0.459) data 0.312 (0.329) loss_x loss_x 1.7705 (1.0667) acc_x 53.1250 (71.5625) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [25/52] time 0.398 (0.463) data 0.267 (0.332) loss_x loss_x 0.8120 (1.0375) acc_x 81.2500 (73.0000) lr 4.9096e-04 eta 0:00:12
epoch [136/200] batch [30/52] time 0.380 (0.455) data 0.250 (0.325) loss_x loss_x 0.7881 (1.0550) acc_x 84.3750 (73.1250) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [35/52] time 0.414 (0.452) data 0.284 (0.322) loss_x loss_x 0.9712 (1.0415) acc_x 75.0000 (73.2143) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [40/52] time 0.554 (0.457) data 0.423 (0.326) loss_x loss_x 0.7700 (1.0608) acc_x 84.3750 (73.2812) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [45/52] time 0.328 (0.455) data 0.198 (0.325) loss_x loss_x 0.8188 (1.0682) acc_x 75.0000 (72.7083) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [50/52] time 0.415 (0.451) data 0.285 (0.321) loss_x loss_x 1.1865 (1.0711) acc_x 62.5000 (72.2500) lr 4.9096e-04 eta 0:00:00
epoch [136/200] batch [5/45] time 0.421 (0.455) data 0.291 (0.325) loss_u loss_u 0.7607 (0.7750) acc_u 25.0000 (27.5000) lr 4.9096e-04 eta 0:00:18
epoch [136/200] batch [10/45] time 0.416 (0.448) data 0.286 (0.317) loss_u loss_u 0.7563 (0.7685) acc_u 25.0000 (30.0000) lr 4.9096e-04 eta 0:00:15
epoch [136/200] batch [15/45] time 0.385 (0.443) data 0.253 (0.312) loss_u loss_u 0.7578 (0.7814) acc_u 34.3750 (28.5417) lr 4.9096e-04 eta 0:00:13
epoch [136/200] batch [20/45] time 0.368 (0.440) data 0.237 (0.309) loss_u loss_u 0.8052 (0.7847) acc_u 18.7500 (27.6562) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [25/45] time 0.467 (0.440) data 0.336 (0.309) loss_u loss_u 0.7231 (0.7792) acc_u 34.3750 (28.1250) lr 4.9096e-04 eta 0:00:08
epoch [136/200] batch [30/45] time 0.432 (0.441) data 0.301 (0.310) loss_u loss_u 0.7646 (0.7745) acc_u 31.2500 (29.1667) lr 4.9096e-04 eta 0:00:06
epoch [136/200] batch [35/45] time 0.445 (0.439) data 0.314 (0.308) loss_u loss_u 0.7007 (0.7717) acc_u 34.3750 (29.5536) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [40/45] time 0.415 (0.441) data 0.283 (0.310) loss_u loss_u 0.6709 (0.7584) acc_u 43.7500 (31.2500) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [45/45] time 0.504 (0.441) data 0.373 (0.310) loss_u loss_u 0.6919 (0.7586) acc_u 34.3750 (30.9722) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1336
confident_label rate tensor(0.5220, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1637
clean true:1618
clean false:19
clean_rate:0.9883934025656689
noisy true:182
noisy false:1317
after delete: len(clean_dataset) 1637
after delete: len(noisy_dataset) 1499
epoch [137/200] batch [5/51] time 0.411 (0.457) data 0.281 (0.326) loss_x loss_x 1.2988 (1.2514) acc_x 59.3750 (66.8750) lr 4.7750e-04 eta 0:00:21
epoch [137/200] batch [10/51] time 0.454 (0.467) data 0.324 (0.337) loss_x loss_x 0.6479 (1.1563) acc_x 81.2500 (69.6875) lr 4.7750e-04 eta 0:00:19
epoch [137/200] batch [15/51] time 0.507 (0.473) data 0.377 (0.343) loss_x loss_x 0.8159 (1.0980) acc_x 71.8750 (70.8333) lr 4.7750e-04 eta 0:00:17
epoch [137/200] batch [20/51] time 0.435 (0.475) data 0.305 (0.345) loss_x loss_x 1.2070 (1.0854) acc_x 71.8750 (71.2500) lr 4.7750e-04 eta 0:00:14
epoch [137/200] batch [25/51] time 0.471 (0.466) data 0.341 (0.336) loss_x loss_x 1.4043 (1.1213) acc_x 71.8750 (71.6250) lr 4.7750e-04 eta 0:00:12
epoch [137/200] batch [30/51] time 0.452 (0.475) data 0.322 (0.344) loss_x loss_x 0.8940 (1.0875) acc_x 81.2500 (72.2917) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [35/51] time 0.355 (0.464) data 0.225 (0.333) loss_x loss_x 1.3457 (1.0663) acc_x 65.6250 (72.7679) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [40/51] time 0.450 (0.458) data 0.320 (0.328) loss_x loss_x 0.7529 (1.0667) acc_x 84.3750 (72.7344) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [45/51] time 0.380 (0.456) data 0.250 (0.326) loss_x loss_x 1.2119 (1.0515) acc_x 75.0000 (73.4028) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [50/51] time 0.468 (0.457) data 0.337 (0.327) loss_x loss_x 1.3418 (1.0766) acc_x 68.7500 (72.6875) lr 4.7750e-04 eta 0:00:00
epoch [137/200] batch [5/46] time 0.586 (0.458) data 0.454 (0.327) loss_u loss_u 0.7539 (0.7927) acc_u 31.2500 (23.7500) lr 4.7750e-04 eta 0:00:18
epoch [137/200] batch [10/46] time 0.320 (0.452) data 0.189 (0.322) loss_u loss_u 0.7031 (0.7769) acc_u 43.7500 (26.2500) lr 4.7750e-04 eta 0:00:16
epoch [137/200] batch [15/46] time 0.354 (0.448) data 0.223 (0.317) loss_u loss_u 0.7280 (0.7583) acc_u 34.3750 (30.0000) lr 4.7750e-04 eta 0:00:13
epoch [137/200] batch [20/46] time 0.357 (0.444) data 0.226 (0.313) loss_u loss_u 0.7627 (0.7585) acc_u 34.3750 (30.3125) lr 4.7750e-04 eta 0:00:11
epoch [137/200] batch [25/46] time 0.332 (0.443) data 0.201 (0.313) loss_u loss_u 0.7925 (0.7651) acc_u 28.1250 (29.7500) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [30/46] time 0.385 (0.442) data 0.254 (0.311) loss_u loss_u 0.6807 (0.7605) acc_u 31.2500 (29.7917) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [35/46] time 0.377 (0.440) data 0.246 (0.310) loss_u loss_u 0.7446 (0.7606) acc_u 25.0000 (29.3750) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [40/46] time 0.409 (0.437) data 0.277 (0.306) loss_u loss_u 0.6401 (0.7585) acc_u 46.8750 (29.6875) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [45/46] time 0.397 (0.437) data 0.266 (0.306) loss_u loss_u 0.7427 (0.7579) acc_u 25.0000 (29.5833) lr 4.7750e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1247
confident_label rate tensor(0.5494, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1723
clean true:1704
clean false:19
clean_rate:0.9889727219965178
noisy true:185
noisy false:1228
after delete: len(clean_dataset) 1723
after delete: len(noisy_dataset) 1413
epoch [138/200] batch [5/53] time 0.465 (0.439) data 0.334 (0.309) loss_x loss_x 1.1816 (1.2984) acc_x 71.8750 (66.2500) lr 4.6417e-04 eta 0:00:21
epoch [138/200] batch [10/53] time 0.481 (0.454) data 0.351 (0.324) loss_x loss_x 1.1729 (1.1780) acc_x 71.8750 (68.7500) lr 4.6417e-04 eta 0:00:19
epoch [138/200] batch [15/53] time 0.397 (0.451) data 0.266 (0.320) loss_x loss_x 1.1904 (1.2484) acc_x 56.2500 (67.5000) lr 4.6417e-04 eta 0:00:17
epoch [138/200] batch [20/53] time 0.396 (0.447) data 0.266 (0.317) loss_x loss_x 0.6455 (1.1360) acc_x 81.2500 (70.6250) lr 4.6417e-04 eta 0:00:14
epoch [138/200] batch [25/53] time 0.538 (0.454) data 0.408 (0.323) loss_x loss_x 1.7715 (1.1803) acc_x 62.5000 (69.6250) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [30/53] time 0.408 (0.453) data 0.278 (0.323) loss_x loss_x 0.8003 (1.1616) acc_x 75.0000 (70.0000) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [35/53] time 0.409 (0.446) data 0.279 (0.315) loss_x loss_x 0.9434 (1.1320) acc_x 78.1250 (70.7143) lr 4.6417e-04 eta 0:00:08
epoch [138/200] batch [40/53] time 0.383 (0.443) data 0.252 (0.313) loss_x loss_x 0.9658 (1.1407) acc_x 71.8750 (70.9375) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [45/53] time 0.383 (0.443) data 0.252 (0.312) loss_x loss_x 1.2852 (1.1874) acc_x 75.0000 (69.8611) lr 4.6417e-04 eta 0:00:03
epoch [138/200] batch [50/53] time 0.472 (0.447) data 0.341 (0.317) loss_x loss_x 0.9355 (1.1867) acc_x 75.0000 (70.0625) lr 4.6417e-04 eta 0:00:01
epoch [138/200] batch [5/44] time 0.454 (0.447) data 0.323 (0.317) loss_u loss_u 0.8560 (0.7787) acc_u 18.7500 (28.7500) lr 4.6417e-04 eta 0:00:17
epoch [138/200] batch [10/44] time 0.310 (0.441) data 0.179 (0.310) loss_u loss_u 0.7783 (0.7815) acc_u 34.3750 (28.4375) lr 4.6417e-04 eta 0:00:14
epoch [138/200] batch [15/44] time 0.377 (0.440) data 0.246 (0.309) loss_u loss_u 0.7695 (0.7719) acc_u 25.0000 (28.9583) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [20/44] time 0.433 (0.441) data 0.302 (0.310) loss_u loss_u 0.6875 (0.7637) acc_u 37.5000 (29.3750) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [25/44] time 0.360 (0.438) data 0.229 (0.308) loss_u loss_u 0.7939 (0.7691) acc_u 28.1250 (28.7500) lr 4.6417e-04 eta 0:00:08
epoch [138/200] batch [30/44] time 0.351 (0.439) data 0.220 (0.308) loss_u loss_u 0.7144 (0.7669) acc_u 34.3750 (29.1667) lr 4.6417e-04 eta 0:00:06
epoch [138/200] batch [35/44] time 0.407 (0.438) data 0.276 (0.307) loss_u loss_u 0.8062 (0.7702) acc_u 34.3750 (29.0179) lr 4.6417e-04 eta 0:00:03
epoch [138/200] batch [40/44] time 0.390 (0.438) data 0.259 (0.307) loss_u loss_u 0.6406 (0.7614) acc_u 37.5000 (30.0781) lr 4.6417e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1330
confident_label rate tensor(0.5306, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1664
clean true:1642
clean false:22
clean_rate:0.9867788461538461
noisy true:164
noisy false:1308
after delete: len(clean_dataset) 1664
after delete: len(noisy_dataset) 1472
epoch [139/200] batch [5/52] time 0.336 (0.453) data 0.206 (0.324) loss_x loss_x 1.2900 (1.2170) acc_x 65.6250 (68.1250) lr 4.5098e-04 eta 0:00:21
epoch [139/200] batch [10/52] time 0.495 (0.463) data 0.366 (0.333) loss_x loss_x 0.7842 (1.1021) acc_x 71.8750 (71.5625) lr 4.5098e-04 eta 0:00:19
epoch [139/200] batch [15/52] time 0.470 (0.471) data 0.340 (0.341) loss_x loss_x 0.9160 (1.0515) acc_x 87.5000 (73.9583) lr 4.5098e-04 eta 0:00:17
epoch [139/200] batch [20/52] time 0.544 (0.465) data 0.413 (0.335) loss_x loss_x 1.7207 (1.1416) acc_x 62.5000 (72.1875) lr 4.5098e-04 eta 0:00:14
epoch [139/200] batch [25/52] time 0.408 (0.458) data 0.277 (0.328) loss_x loss_x 1.6084 (1.1282) acc_x 56.2500 (71.3750) lr 4.5098e-04 eta 0:00:12
epoch [139/200] batch [30/52] time 0.587 (0.465) data 0.457 (0.335) loss_x loss_x 1.0840 (1.1107) acc_x 75.0000 (71.8750) lr 4.5098e-04 eta 0:00:10
epoch [139/200] batch [35/52] time 0.513 (0.466) data 0.383 (0.336) loss_x loss_x 0.8545 (1.1096) acc_x 78.1250 (72.2321) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [40/52] time 0.429 (0.461) data 0.299 (0.331) loss_x loss_x 0.7695 (1.1003) acc_x 84.3750 (72.5000) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [45/52] time 0.345 (0.455) data 0.215 (0.325) loss_x loss_x 1.2695 (1.1143) acc_x 75.0000 (72.2222) lr 4.5098e-04 eta 0:00:03
epoch [139/200] batch [50/52] time 0.314 (0.447) data 0.184 (0.317) loss_x loss_x 1.5381 (1.1216) acc_x 62.5000 (71.6250) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/46] time 0.378 (0.443) data 0.246 (0.313) loss_u loss_u 0.7524 (0.7507) acc_u 28.1250 (33.1250) lr 4.5098e-04 eta 0:00:18
epoch [139/200] batch [10/46] time 0.432 (0.441) data 0.302 (0.311) loss_u loss_u 0.7378 (0.7427) acc_u 31.2500 (34.0625) lr 4.5098e-04 eta 0:00:15
epoch [139/200] batch [15/46] time 0.501 (0.441) data 0.370 (0.311) loss_u loss_u 0.7173 (0.7505) acc_u 31.2500 (32.0833) lr 4.5098e-04 eta 0:00:13
epoch [139/200] batch [20/46] time 0.412 (0.442) data 0.281 (0.312) loss_u loss_u 0.7588 (0.7430) acc_u 28.1250 (33.1250) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [25/46] time 0.459 (0.443) data 0.328 (0.313) loss_u loss_u 0.7583 (0.7438) acc_u 31.2500 (32.8750) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [30/46] time 0.394 (0.442) data 0.263 (0.312) loss_u loss_u 0.8472 (0.7512) acc_u 21.8750 (31.8750) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [35/46] time 0.383 (0.442) data 0.252 (0.312) loss_u loss_u 0.7842 (0.7507) acc_u 31.2500 (31.8750) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [40/46] time 0.368 (0.442) data 0.237 (0.312) loss_u loss_u 0.8096 (0.7504) acc_u 21.8750 (31.4844) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [45/46] time 0.330 (0.440) data 0.200 (0.310) loss_u loss_u 0.8462 (0.7515) acc_u 21.8750 (31.2500) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1349
confident_label rate tensor(0.5204, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1632
clean true:1611
clean false:21
clean_rate:0.9871323529411765
noisy true:176
noisy false:1328
after delete: len(clean_dataset) 1632
after delete: len(noisy_dataset) 1504
epoch [140/200] batch [5/51] time 0.484 (0.434) data 0.354 (0.303) loss_x loss_x 0.8672 (1.1171) acc_x 84.3750 (71.2500) lr 4.3792e-04 eta 0:00:19
epoch [140/200] batch [10/51] time 0.455 (0.437) data 0.325 (0.307) loss_x loss_x 1.2930 (1.1078) acc_x 68.7500 (70.9375) lr 4.3792e-04 eta 0:00:17
epoch [140/200] batch [15/51] time 0.402 (0.436) data 0.271 (0.306) loss_x loss_x 0.9829 (1.1859) acc_x 71.8750 (68.3333) lr 4.3792e-04 eta 0:00:15
epoch [140/200] batch [20/51] time 0.343 (0.440) data 0.213 (0.309) loss_x loss_x 0.7607 (1.1591) acc_x 87.5000 (70.1562) lr 4.3792e-04 eta 0:00:13
epoch [140/200] batch [25/51] time 0.481 (0.446) data 0.350 (0.315) loss_x loss_x 0.9648 (1.1008) acc_x 81.2500 (72.1250) lr 4.3792e-04 eta 0:00:11
epoch [140/200] batch [30/51] time 0.489 (0.446) data 0.360 (0.315) loss_x loss_x 0.8364 (1.0587) acc_x 78.1250 (73.2292) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [35/51] time 0.466 (0.443) data 0.336 (0.313) loss_x loss_x 1.0264 (1.0515) acc_x 68.7500 (73.2143) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [40/51] time 0.563 (0.444) data 0.432 (0.313) loss_x loss_x 1.3047 (1.0592) acc_x 62.5000 (72.9688) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [45/51] time 0.566 (0.444) data 0.435 (0.313) loss_x loss_x 1.2627 (1.0782) acc_x 75.0000 (72.9167) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [50/51] time 0.370 (0.441) data 0.240 (0.310) loss_x loss_x 1.2969 (1.1052) acc_x 65.6250 (72.5000) lr 4.3792e-04 eta 0:00:00
epoch [140/200] batch [5/47] time 0.395 (0.437) data 0.264 (0.307) loss_u loss_u 0.7739 (0.7553) acc_u 31.2500 (30.0000) lr 4.3792e-04 eta 0:00:18
epoch [140/200] batch [10/47] time 0.465 (0.439) data 0.334 (0.308) loss_u loss_u 0.6865 (0.7529) acc_u 34.3750 (29.6875) lr 4.3792e-04 eta 0:00:16
epoch [140/200] batch [15/47] time 0.372 (0.439) data 0.240 (0.308) loss_u loss_u 0.8184 (0.7601) acc_u 25.0000 (29.5833) lr 4.3792e-04 eta 0:00:14
epoch [140/200] batch [20/47] time 0.310 (0.438) data 0.179 (0.307) loss_u loss_u 0.7993 (0.7535) acc_u 25.0000 (30.1562) lr 4.3792e-04 eta 0:00:11
epoch [140/200] batch [25/47] time 0.392 (0.435) data 0.261 (0.304) loss_u loss_u 0.7153 (0.7509) acc_u 40.6250 (30.8750) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [30/47] time 0.388 (0.433) data 0.257 (0.303) loss_u loss_u 0.6953 (0.7570) acc_u 43.7500 (30.4167) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [35/47] time 0.464 (0.432) data 0.333 (0.301) loss_u loss_u 0.7622 (0.7523) acc_u 28.1250 (31.1607) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [40/47] time 0.395 (0.437) data 0.262 (0.307) loss_u loss_u 0.7300 (0.7556) acc_u 34.3750 (30.6250) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [45/47] time 0.559 (0.438) data 0.428 (0.307) loss_u loss_u 0.7500 (0.7570) acc_u 34.3750 (30.6250) lr 4.3792e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1290
confident_label rate tensor(0.5399, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1693
clean true:1671
clean false:22
clean_rate:0.987005316007088
noisy true:175
noisy false:1268
after delete: len(clean_dataset) 1693
after delete: len(noisy_dataset) 1443
epoch [141/200] batch [5/52] time 0.485 (0.480) data 0.355 (0.350) loss_x loss_x 0.5244 (1.0064) acc_x 87.5000 (75.0000) lr 4.2499e-04 eta 0:00:22
epoch [141/200] batch [10/52] time 0.498 (0.458) data 0.368 (0.327) loss_x loss_x 1.7100 (1.1462) acc_x 62.5000 (71.2500) lr 4.2499e-04 eta 0:00:19
epoch [141/200] batch [15/52] time 0.476 (0.458) data 0.346 (0.328) loss_x loss_x 1.1572 (1.1513) acc_x 68.7500 (70.2083) lr 4.2499e-04 eta 0:00:16
epoch [141/200] batch [20/52] time 0.357 (0.444) data 0.226 (0.314) loss_x loss_x 1.2559 (1.1017) acc_x 68.7500 (72.5000) lr 4.2499e-04 eta 0:00:14
epoch [141/200] batch [25/52] time 0.398 (0.444) data 0.268 (0.314) loss_x loss_x 1.4365 (1.0876) acc_x 59.3750 (72.7500) lr 4.2499e-04 eta 0:00:11
epoch [141/200] batch [30/52] time 0.324 (0.438) data 0.194 (0.308) loss_x loss_x 0.8135 (1.0932) acc_x 81.2500 (72.8125) lr 4.2499e-04 eta 0:00:09
epoch [141/200] batch [35/52] time 0.749 (0.445) data 0.619 (0.315) loss_x loss_x 1.1025 (1.1311) acc_x 68.7500 (71.6964) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [40/52] time 0.452 (0.446) data 0.321 (0.316) loss_x loss_x 0.7378 (1.1247) acc_x 78.1250 (71.6406) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [45/52] time 0.394 (0.442) data 0.264 (0.312) loss_x loss_x 1.2373 (1.1234) acc_x 62.5000 (71.4583) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [50/52] time 0.448 (0.446) data 0.318 (0.316) loss_x loss_x 1.0625 (1.1310) acc_x 75.0000 (71.1875) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/45] time 0.359 (0.441) data 0.228 (0.310) loss_u loss_u 0.7563 (0.7603) acc_u 28.1250 (29.3750) lr 4.2499e-04 eta 0:00:17
epoch [141/200] batch [10/45] time 0.414 (0.438) data 0.282 (0.308) loss_u loss_u 0.6543 (0.7741) acc_u 46.8750 (27.8125) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [15/45] time 0.597 (0.437) data 0.466 (0.306) loss_u loss_u 0.7637 (0.7640) acc_u 34.3750 (29.7917) lr 4.2499e-04 eta 0:00:13
epoch [141/200] batch [20/45] time 0.396 (0.435) data 0.264 (0.305) loss_u loss_u 0.8984 (0.7676) acc_u 6.2500 (28.4375) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [25/45] time 0.330 (0.437) data 0.199 (0.306) loss_u loss_u 0.8042 (0.7573) acc_u 25.0000 (30.1250) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [30/45] time 0.420 (0.434) data 0.289 (0.303) loss_u loss_u 0.7729 (0.7616) acc_u 31.2500 (29.6875) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [35/45] time 0.477 (0.437) data 0.345 (0.306) loss_u loss_u 0.7432 (0.7603) acc_u 34.3750 (29.6429) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [40/45] time 0.472 (0.438) data 0.341 (0.307) loss_u loss_u 0.8345 (0.7613) acc_u 21.8750 (29.2969) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [45/45] time 0.413 (0.437) data 0.282 (0.306) loss_u loss_u 0.7207 (0.7630) acc_u 34.3750 (29.2361) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1321
confident_label rate tensor(0.5312, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1666
clean true:1646
clean false:20
clean_rate:0.9879951980792316
noisy true:169
noisy false:1301
after delete: len(clean_dataset) 1666
after delete: len(noisy_dataset) 1470
epoch [142/200] batch [5/52] time 0.487 (0.432) data 0.356 (0.302) loss_x loss_x 0.9956 (1.0092) acc_x 81.2500 (75.6250) lr 4.1221e-04 eta 0:00:20
epoch [142/200] batch [10/52] time 0.410 (0.430) data 0.280 (0.300) loss_x loss_x 0.9951 (0.9511) acc_x 68.7500 (75.3125) lr 4.1221e-04 eta 0:00:18
epoch [142/200] batch [15/52] time 0.412 (0.434) data 0.282 (0.304) loss_x loss_x 0.8379 (0.9808) acc_x 78.1250 (74.5833) lr 4.1221e-04 eta 0:00:16
epoch [142/200] batch [20/52] time 0.335 (0.425) data 0.205 (0.295) loss_x loss_x 1.1777 (0.9637) acc_x 71.8750 (75.3125) lr 4.1221e-04 eta 0:00:13
epoch [142/200] batch [25/52] time 0.356 (0.426) data 0.226 (0.296) loss_x loss_x 1.0576 (0.9658) acc_x 71.8750 (75.8750) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [30/52] time 0.401 (0.426) data 0.270 (0.296) loss_x loss_x 1.1465 (1.0407) acc_x 65.6250 (74.1667) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [35/52] time 0.470 (0.435) data 0.339 (0.305) loss_x loss_x 0.8521 (1.0469) acc_x 84.3750 (74.0179) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [40/52] time 0.461 (0.443) data 0.330 (0.312) loss_x loss_x 1.2148 (1.0290) acc_x 71.8750 (74.6094) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [45/52] time 0.454 (0.448) data 0.323 (0.318) loss_x loss_x 0.9878 (1.0428) acc_x 81.2500 (74.3056) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [50/52] time 0.441 (0.447) data 0.311 (0.317) loss_x loss_x 1.9766 (1.0572) acc_x 53.1250 (74.0000) lr 4.1221e-04 eta 0:00:00
epoch [142/200] batch [5/45] time 0.493 (0.451) data 0.363 (0.321) loss_u loss_u 0.7759 (0.7474) acc_u 25.0000 (31.8750) lr 4.1221e-04 eta 0:00:18
epoch [142/200] batch [10/45] time 0.560 (0.447) data 0.430 (0.316) loss_u loss_u 0.8413 (0.7479) acc_u 25.0000 (32.5000) lr 4.1221e-04 eta 0:00:15
epoch [142/200] batch [15/45] time 0.382 (0.448) data 0.251 (0.318) loss_u loss_u 0.7905 (0.7532) acc_u 25.0000 (31.8750) lr 4.1221e-04 eta 0:00:13
epoch [142/200] batch [20/45] time 0.381 (0.446) data 0.250 (0.316) loss_u loss_u 0.7256 (0.7548) acc_u 31.2500 (30.6250) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [25/45] time 0.541 (0.445) data 0.410 (0.314) loss_u loss_u 0.8340 (0.7586) acc_u 15.6250 (30.1250) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [30/45] time 0.475 (0.444) data 0.344 (0.313) loss_u loss_u 0.8496 (0.7635) acc_u 18.7500 (29.5833) lr 4.1221e-04 eta 0:00:06
epoch [142/200] batch [35/45] time 0.517 (0.442) data 0.385 (0.312) loss_u loss_u 0.7476 (0.7600) acc_u 34.3750 (30.4464) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [40/45] time 0.402 (0.439) data 0.271 (0.309) loss_u loss_u 0.7378 (0.7581) acc_u 37.5000 (30.5469) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [45/45] time 0.509 (0.439) data 0.378 (0.309) loss_u loss_u 0.6753 (0.7513) acc_u 37.5000 (31.5278) lr 4.1221e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1306
confident_label rate tensor(0.5297, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1661
clean true:1647
clean false:14
clean_rate:0.99157134256472
noisy true:183
noisy false:1292
after delete: len(clean_dataset) 1661
after delete: len(noisy_dataset) 1475
epoch [143/200] batch [5/51] time 0.417 (0.433) data 0.287 (0.303) loss_x loss_x 1.1758 (1.4219) acc_x 75.0000 (66.8750) lr 3.9958e-04 eta 0:00:19
epoch [143/200] batch [10/51] time 0.356 (0.436) data 0.226 (0.306) loss_x loss_x 1.1523 (1.2757) acc_x 71.8750 (69.0625) lr 3.9958e-04 eta 0:00:17
epoch [143/200] batch [15/51] time 0.457 (0.443) data 0.326 (0.313) loss_x loss_x 0.7866 (1.1622) acc_x 84.3750 (72.0833) lr 3.9958e-04 eta 0:00:15
epoch [143/200] batch [20/51] time 0.418 (0.444) data 0.288 (0.314) loss_x loss_x 1.0889 (1.1026) acc_x 68.7500 (72.9688) lr 3.9958e-04 eta 0:00:13
epoch [143/200] batch [25/51] time 0.505 (0.447) data 0.374 (0.317) loss_x loss_x 0.9609 (1.0921) acc_x 78.1250 (72.8750) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [30/51] time 0.569 (0.448) data 0.438 (0.318) loss_x loss_x 1.7090 (1.1102) acc_x 59.3750 (72.6042) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [35/51] time 0.546 (0.449) data 0.415 (0.318) loss_x loss_x 1.0488 (1.1414) acc_x 75.0000 (72.4107) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [40/51] time 0.520 (0.456) data 0.389 (0.326) loss_x loss_x 0.9399 (1.0979) acc_x 84.3750 (73.5938) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [45/51] time 0.436 (0.458) data 0.305 (0.328) loss_x loss_x 1.3115 (1.0836) acc_x 71.8750 (73.8194) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [50/51] time 0.506 (0.458) data 0.374 (0.328) loss_x loss_x 1.3945 (1.0895) acc_x 62.5000 (73.5000) lr 3.9958e-04 eta 0:00:00
epoch [143/200] batch [5/46] time 0.404 (0.457) data 0.273 (0.327) loss_u loss_u 0.7349 (0.7335) acc_u 34.3750 (34.3750) lr 3.9958e-04 eta 0:00:18
epoch [143/200] batch [10/46] time 0.423 (0.460) data 0.292 (0.330) loss_u loss_u 0.7876 (0.7385) acc_u 28.1250 (32.1875) lr 3.9958e-04 eta 0:00:16
epoch [143/200] batch [15/46] time 0.355 (0.454) data 0.223 (0.323) loss_u loss_u 0.6162 (0.7415) acc_u 50.0000 (32.2917) lr 3.9958e-04 eta 0:00:14
epoch [143/200] batch [20/46] time 0.366 (0.449) data 0.235 (0.318) loss_u loss_u 0.7720 (0.7440) acc_u 31.2500 (32.1875) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [25/46] time 0.338 (0.445) data 0.207 (0.314) loss_u loss_u 0.7388 (0.7483) acc_u 40.6250 (31.7500) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [30/46] time 0.386 (0.443) data 0.255 (0.312) loss_u loss_u 0.7144 (0.7424) acc_u 31.2500 (31.8750) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [35/46] time 0.373 (0.440) data 0.242 (0.310) loss_u loss_u 0.7920 (0.7443) acc_u 28.1250 (31.5179) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [40/46] time 0.518 (0.439) data 0.387 (0.308) loss_u loss_u 0.7422 (0.7429) acc_u 37.5000 (32.1094) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [45/46] time 0.446 (0.439) data 0.315 (0.308) loss_u loss_u 0.6484 (0.7451) acc_u 46.8750 (31.8056) lr 3.9958e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1346
confident_label rate tensor(0.5233, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1641
clean true:1625
clean false:16
clean_rate:0.9902498476538696
noisy true:165
noisy false:1330
after delete: len(clean_dataset) 1641
after delete: len(noisy_dataset) 1495
epoch [144/200] batch [5/51] time 0.430 (0.436) data 0.299 (0.306) loss_x loss_x 1.0586 (1.0516) acc_x 71.8750 (74.3750) lr 3.8709e-04 eta 0:00:20
epoch [144/200] batch [10/51] time 0.430 (0.446) data 0.299 (0.315) loss_x loss_x 1.3945 (1.0646) acc_x 59.3750 (73.1250) lr 3.8709e-04 eta 0:00:18
epoch [144/200] batch [15/51] time 0.498 (0.470) data 0.368 (0.340) loss_x loss_x 1.0977 (1.0630) acc_x 68.7500 (74.1667) lr 3.8709e-04 eta 0:00:16
epoch [144/200] batch [20/51] time 0.462 (0.464) data 0.331 (0.334) loss_x loss_x 0.8115 (1.0945) acc_x 84.3750 (73.9062) lr 3.8709e-04 eta 0:00:14
epoch [144/200] batch [25/51] time 0.441 (0.462) data 0.311 (0.331) loss_x loss_x 0.8848 (1.0672) acc_x 78.1250 (73.8750) lr 3.8709e-04 eta 0:00:12
epoch [144/200] batch [30/51] time 0.432 (0.465) data 0.301 (0.335) loss_x loss_x 1.4854 (1.0532) acc_x 68.7500 (74.5833) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [35/51] time 0.450 (0.466) data 0.320 (0.335) loss_x loss_x 0.8628 (1.0337) acc_x 71.8750 (74.5536) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [40/51] time 0.456 (0.460) data 0.325 (0.329) loss_x loss_x 1.1260 (1.0340) acc_x 59.3750 (74.3750) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [45/51] time 0.388 (0.460) data 0.258 (0.329) loss_x loss_x 0.8735 (1.0389) acc_x 78.1250 (73.9583) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [50/51] time 0.333 (0.453) data 0.203 (0.322) loss_x loss_x 1.0117 (1.0397) acc_x 71.8750 (73.9375) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/46] time 0.500 (0.457) data 0.369 (0.327) loss_u loss_u 0.7720 (0.7663) acc_u 21.8750 (25.6250) lr 3.8709e-04 eta 0:00:18
epoch [144/200] batch [10/46] time 0.362 (0.454) data 0.231 (0.324) loss_u loss_u 0.7266 (0.7567) acc_u 34.3750 (28.1250) lr 3.8709e-04 eta 0:00:16
epoch [144/200] batch [15/46] time 0.400 (0.450) data 0.269 (0.320) loss_u loss_u 0.7373 (0.7547) acc_u 28.1250 (29.3750) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [20/46] time 0.333 (0.447) data 0.201 (0.316) loss_u loss_u 0.9023 (0.7607) acc_u 6.2500 (28.7500) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [25/46] time 0.370 (0.445) data 0.239 (0.314) loss_u loss_u 0.7314 (0.7552) acc_u 40.6250 (30.0000) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [30/46] time 0.451 (0.444) data 0.319 (0.314) loss_u loss_u 0.7764 (0.7463) acc_u 31.2500 (31.4583) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [35/46] time 0.429 (0.445) data 0.298 (0.314) loss_u loss_u 0.7134 (0.7445) acc_u 40.6250 (31.8750) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [40/46] time 0.519 (0.446) data 0.388 (0.316) loss_u loss_u 0.8018 (0.7478) acc_u 21.8750 (30.7031) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [45/46] time 0.329 (0.444) data 0.198 (0.313) loss_u loss_u 0.7065 (0.7443) acc_u 37.5000 (31.3889) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1351
confident_label rate tensor(0.5201, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1631
clean true:1608
clean false:23
clean_rate:0.9858982219497241
noisy true:177
noisy false:1328
after delete: len(clean_dataset) 1631
after delete: len(noisy_dataset) 1505
epoch [145/200] batch [5/50] time 0.408 (0.474) data 0.278 (0.344) loss_x loss_x 0.8623 (0.9537) acc_x 75.0000 (76.2500) lr 3.7476e-04 eta 0:00:21
epoch [145/200] batch [10/50] time 0.448 (0.483) data 0.317 (0.352) loss_x loss_x 0.8896 (0.8966) acc_x 71.8750 (78.4375) lr 3.7476e-04 eta 0:00:19
epoch [145/200] batch [15/50] time 0.408 (0.480) data 0.278 (0.350) loss_x loss_x 0.8184 (0.9837) acc_x 81.2500 (76.4583) lr 3.7476e-04 eta 0:00:16
epoch [145/200] batch [20/50] time 0.434 (0.461) data 0.303 (0.331) loss_x loss_x 0.7017 (0.9764) acc_x 81.2500 (76.2500) lr 3.7476e-04 eta 0:00:13
epoch [145/200] batch [25/50] time 0.382 (0.457) data 0.253 (0.327) loss_x loss_x 0.9595 (0.9802) acc_x 78.1250 (76.1250) lr 3.7476e-04 eta 0:00:11
epoch [145/200] batch [30/50] time 0.576 (0.465) data 0.445 (0.335) loss_x loss_x 1.0293 (0.9821) acc_x 81.2500 (76.0417) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [35/50] time 0.355 (0.460) data 0.225 (0.330) loss_x loss_x 1.4531 (1.0067) acc_x 59.3750 (75.1786) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [40/50] time 0.428 (0.460) data 0.297 (0.330) loss_x loss_x 1.3193 (1.0135) acc_x 62.5000 (75.1562) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [45/50] time 0.410 (0.462) data 0.280 (0.332) loss_x loss_x 0.7988 (1.0093) acc_x 71.8750 (75.2778) lr 3.7476e-04 eta 0:00:02
epoch [145/200] batch [50/50] time 0.383 (0.459) data 0.254 (0.328) loss_x loss_x 1.4180 (1.0257) acc_x 71.8750 (75.3125) lr 3.7476e-04 eta 0:00:00
epoch [145/200] batch [5/47] time 0.444 (0.456) data 0.314 (0.325) loss_u loss_u 0.7065 (0.7429) acc_u 34.3750 (34.3750) lr 3.7476e-04 eta 0:00:19
epoch [145/200] batch [10/47] time 0.433 (0.454) data 0.302 (0.324) loss_u loss_u 0.6953 (0.7382) acc_u 40.6250 (34.6875) lr 3.7476e-04 eta 0:00:16
epoch [145/200] batch [15/47] time 0.419 (0.452) data 0.288 (0.321) loss_u loss_u 0.7261 (0.7316) acc_u 28.1250 (34.3750) lr 3.7476e-04 eta 0:00:14
epoch [145/200] batch [20/47] time 0.352 (0.449) data 0.222 (0.319) loss_u loss_u 0.6846 (0.7337) acc_u 37.5000 (35.0000) lr 3.7476e-04 eta 0:00:12
epoch [145/200] batch [25/47] time 0.412 (0.447) data 0.281 (0.317) loss_u loss_u 0.8203 (0.7452) acc_u 18.7500 (33.1250) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [30/47] time 0.445 (0.444) data 0.312 (0.313) loss_u loss_u 0.8203 (0.7484) acc_u 15.6250 (31.9792) lr 3.7476e-04 eta 0:00:07
epoch [145/200] batch [35/47] time 0.377 (0.443) data 0.246 (0.313) loss_u loss_u 0.6802 (0.7483) acc_u 50.0000 (32.1429) lr 3.7476e-04 eta 0:00:05
epoch [145/200] batch [40/47] time 0.482 (0.447) data 0.351 (0.317) loss_u loss_u 0.7598 (0.7480) acc_u 37.5000 (32.3438) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [45/47] time 0.449 (0.445) data 0.319 (0.315) loss_u loss_u 0.7261 (0.7527) acc_u 43.7500 (31.8056) lr 3.7476e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1305
confident_label rate tensor(0.5325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1670
clean true:1650
clean false:20
clean_rate:0.9880239520958084
noisy true:181
noisy false:1285
after delete: len(clean_dataset) 1670
after delete: len(noisy_dataset) 1466
epoch [146/200] batch [5/52] time 0.390 (0.461) data 0.260 (0.330) loss_x loss_x 1.0947 (1.3291) acc_x 75.0000 (68.1250) lr 3.6258e-04 eta 0:00:21
epoch [146/200] batch [10/52] time 0.404 (0.456) data 0.273 (0.325) loss_x loss_x 0.6719 (1.2104) acc_x 84.3750 (70.9375) lr 3.6258e-04 eta 0:00:19
epoch [146/200] batch [15/52] time 0.520 (0.452) data 0.390 (0.322) loss_x loss_x 0.9121 (1.1976) acc_x 71.8750 (71.0417) lr 3.6258e-04 eta 0:00:16
epoch [146/200] batch [20/52] time 0.494 (0.447) data 0.364 (0.316) loss_x loss_x 0.8291 (1.1759) acc_x 78.1250 (71.7188) lr 3.6258e-04 eta 0:00:14
epoch [146/200] batch [25/52] time 0.434 (0.446) data 0.303 (0.315) loss_x loss_x 0.9995 (1.1548) acc_x 71.8750 (72.1250) lr 3.6258e-04 eta 0:00:12
epoch [146/200] batch [30/52] time 0.436 (0.449) data 0.306 (0.319) loss_x loss_x 0.9458 (1.1222) acc_x 75.0000 (72.7083) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [35/52] time 0.416 (0.451) data 0.285 (0.320) loss_x loss_x 1.0869 (1.0999) acc_x 81.2500 (73.0357) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [40/52] time 0.417 (0.449) data 0.287 (0.318) loss_x loss_x 1.1445 (1.1031) acc_x 71.8750 (73.1250) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [45/52] time 0.545 (0.448) data 0.415 (0.318) loss_x loss_x 1.4570 (1.0958) acc_x 62.5000 (73.4722) lr 3.6258e-04 eta 0:00:03
epoch [146/200] batch [50/52] time 0.476 (0.450) data 0.346 (0.320) loss_x loss_x 1.3955 (1.1044) acc_x 62.5000 (73.0000) lr 3.6258e-04 eta 0:00:00
epoch [146/200] batch [5/45] time 0.406 (0.448) data 0.275 (0.317) loss_u loss_u 0.8096 (0.7864) acc_u 25.0000 (30.0000) lr 3.6258e-04 eta 0:00:17
epoch [146/200] batch [10/45] time 0.452 (0.448) data 0.321 (0.318) loss_u loss_u 0.6655 (0.7784) acc_u 46.8750 (29.3750) lr 3.6258e-04 eta 0:00:15
epoch [146/200] batch [15/45] time 0.432 (0.450) data 0.300 (0.319) loss_u loss_u 0.7568 (0.7640) acc_u 31.2500 (29.3750) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [20/45] time 0.422 (0.447) data 0.291 (0.316) loss_u loss_u 0.8203 (0.7659) acc_u 28.1250 (29.0625) lr 3.6258e-04 eta 0:00:11
epoch [146/200] batch [25/45] time 0.386 (0.448) data 0.255 (0.317) loss_u loss_u 0.7461 (0.7574) acc_u 25.0000 (29.7500) lr 3.6258e-04 eta 0:00:08
epoch [146/200] batch [30/45] time 0.421 (0.444) data 0.290 (0.313) loss_u loss_u 0.7178 (0.7566) acc_u 34.3750 (29.5833) lr 3.6258e-04 eta 0:00:06
epoch [146/200] batch [35/45] time 0.385 (0.443) data 0.254 (0.313) loss_u loss_u 0.7290 (0.7579) acc_u 46.8750 (30.1786) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [40/45] time 0.314 (0.439) data 0.183 (0.309) loss_u loss_u 0.7295 (0.7578) acc_u 31.2500 (29.8438) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [45/45] time 0.445 (0.442) data 0.314 (0.311) loss_u loss_u 0.7534 (0.7595) acc_u 31.2500 (29.3750) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1352
confident_label rate tensor(0.5147, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1614
clean true:1596
clean false:18
clean_rate:0.9888475836431226
noisy true:188
noisy false:1334
after delete: len(clean_dataset) 1614
after delete: len(noisy_dataset) 1522
epoch [147/200] batch [5/50] time 0.510 (0.508) data 0.379 (0.377) loss_x loss_x 1.0137 (1.0827) acc_x 78.1250 (73.1250) lr 3.5055e-04 eta 0:00:22
epoch [147/200] batch [10/50] time 0.434 (0.496) data 0.303 (0.366) loss_x loss_x 1.4160 (1.1184) acc_x 68.7500 (72.1875) lr 3.5055e-04 eta 0:00:19
epoch [147/200] batch [15/50] time 0.550 (0.485) data 0.420 (0.355) loss_x loss_x 0.8965 (1.0189) acc_x 75.0000 (74.5833) lr 3.5055e-04 eta 0:00:16
epoch [147/200] batch [20/50] time 0.688 (0.484) data 0.558 (0.353) loss_x loss_x 0.9453 (0.9637) acc_x 75.0000 (75.0000) lr 3.5055e-04 eta 0:00:14
epoch [147/200] batch [25/50] time 0.464 (0.469) data 0.334 (0.339) loss_x loss_x 1.8662 (1.0180) acc_x 59.3750 (74.1250) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [30/50] time 0.527 (0.470) data 0.397 (0.340) loss_x loss_x 0.5288 (0.9934) acc_x 87.5000 (74.2708) lr 3.5055e-04 eta 0:00:09
epoch [147/200] batch [35/50] time 0.505 (0.464) data 0.375 (0.334) loss_x loss_x 0.9473 (1.0083) acc_x 84.3750 (74.1071) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [40/50] time 0.436 (0.459) data 0.306 (0.329) loss_x loss_x 0.8413 (0.9984) acc_x 81.2500 (74.6094) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [45/50] time 0.412 (0.453) data 0.282 (0.323) loss_x loss_x 1.3203 (1.0253) acc_x 75.0000 (74.0972) lr 3.5055e-04 eta 0:00:02
epoch [147/200] batch [50/50] time 0.424 (0.447) data 0.294 (0.317) loss_x loss_x 0.7314 (1.0249) acc_x 78.1250 (73.9375) lr 3.5055e-04 eta 0:00:00
epoch [147/200] batch [5/47] time 0.426 (0.449) data 0.296 (0.318) loss_u loss_u 0.8037 (0.7648) acc_u 28.1250 (29.3750) lr 3.5055e-04 eta 0:00:18
epoch [147/200] batch [10/47] time 0.477 (0.445) data 0.347 (0.315) loss_u loss_u 0.8423 (0.7543) acc_u 18.7500 (29.6875) lr 3.5055e-04 eta 0:00:16
epoch [147/200] batch [15/47] time 0.424 (0.449) data 0.295 (0.319) loss_u loss_u 0.7217 (0.7530) acc_u 34.3750 (29.5833) lr 3.5055e-04 eta 0:00:14
epoch [147/200] batch [20/47] time 0.394 (0.447) data 0.264 (0.317) loss_u loss_u 0.7612 (0.7499) acc_u 28.1250 (29.8438) lr 3.5055e-04 eta 0:00:12
epoch [147/200] batch [25/47] time 0.513 (0.443) data 0.381 (0.313) loss_u loss_u 0.7788 (0.7570) acc_u 25.0000 (29.6250) lr 3.5055e-04 eta 0:00:09
epoch [147/200] batch [30/47] time 0.427 (0.443) data 0.295 (0.312) loss_u loss_u 0.7456 (0.7561) acc_u 40.6250 (30.2083) lr 3.5055e-04 eta 0:00:07
epoch [147/200] batch [35/47] time 0.406 (0.446) data 0.276 (0.316) loss_u loss_u 0.7959 (0.7516) acc_u 28.1250 (31.0714) lr 3.5055e-04 eta 0:00:05
epoch [147/200] batch [40/47] time 0.508 (0.445) data 0.377 (0.314) loss_u loss_u 0.7227 (0.7508) acc_u 28.1250 (30.9375) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [45/47] time 0.389 (0.444) data 0.257 (0.313) loss_u loss_u 0.8525 (0.7514) acc_u 28.1250 (31.4583) lr 3.5055e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1287
confident_label rate tensor(0.5373, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1685
clean true:1668
clean false:17
clean_rate:0.9899109792284867
noisy true:181
noisy false:1270
after delete: len(clean_dataset) 1685
after delete: len(noisy_dataset) 1451
epoch [148/200] batch [5/52] time 0.391 (0.451) data 0.261 (0.320) loss_x loss_x 1.0254 (1.1748) acc_x 75.0000 (70.6250) lr 3.3869e-04 eta 0:00:21
epoch [148/200] batch [10/52] time 0.510 (0.448) data 0.380 (0.318) loss_x loss_x 1.3652 (1.3059) acc_x 68.7500 (68.4375) lr 3.3869e-04 eta 0:00:18
epoch [148/200] batch [15/52] time 0.446 (0.448) data 0.316 (0.317) loss_x loss_x 0.9565 (1.2134) acc_x 71.8750 (69.5833) lr 3.3869e-04 eta 0:00:16
epoch [148/200] batch [20/52] time 0.546 (0.455) data 0.416 (0.325) loss_x loss_x 1.1768 (1.1947) acc_x 75.0000 (69.8438) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [25/52] time 0.438 (0.453) data 0.307 (0.322) loss_x loss_x 1.4365 (1.2090) acc_x 50.0000 (68.7500) lr 3.3869e-04 eta 0:00:12
epoch [148/200] batch [30/52] time 0.464 (0.447) data 0.334 (0.317) loss_x loss_x 1.1680 (1.1806) acc_x 78.1250 (70.3125) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [35/52] time 0.480 (0.446) data 0.349 (0.316) loss_x loss_x 1.5195 (1.1869) acc_x 62.5000 (70.0893) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [40/52] time 0.453 (0.449) data 0.322 (0.319) loss_x loss_x 1.5146 (1.1873) acc_x 59.3750 (69.8438) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [45/52] time 0.441 (0.448) data 0.311 (0.318) loss_x loss_x 1.2168 (1.1687) acc_x 78.1250 (70.4167) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [50/52] time 0.424 (0.448) data 0.293 (0.318) loss_x loss_x 0.8774 (1.1603) acc_x 71.8750 (70.7500) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/45] time 0.426 (0.440) data 0.295 (0.310) loss_u loss_u 0.8462 (0.7705) acc_u 25.0000 (31.8750) lr 3.3869e-04 eta 0:00:17
epoch [148/200] batch [10/45] time 0.432 (0.439) data 0.301 (0.308) loss_u loss_u 0.7427 (0.7703) acc_u 37.5000 (30.3125) lr 3.3869e-04 eta 0:00:15
epoch [148/200] batch [15/45] time 0.450 (0.441) data 0.318 (0.311) loss_u loss_u 0.8281 (0.7725) acc_u 25.0000 (30.2083) lr 3.3869e-04 eta 0:00:13
epoch [148/200] batch [20/45] time 0.412 (0.443) data 0.280 (0.312) loss_u loss_u 0.7080 (0.7703) acc_u 50.0000 (30.4688) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [25/45] time 0.420 (0.441) data 0.288 (0.311) loss_u loss_u 0.7173 (0.7696) acc_u 31.2500 (29.5000) lr 3.3869e-04 eta 0:00:08
epoch [148/200] batch [30/45] time 0.393 (0.440) data 0.262 (0.309) loss_u loss_u 0.7866 (0.7688) acc_u 25.0000 (29.1667) lr 3.3869e-04 eta 0:00:06
epoch [148/200] batch [35/45] time 0.481 (0.439) data 0.350 (0.308) loss_u loss_u 0.8242 (0.7658) acc_u 18.7500 (29.6429) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [40/45] time 0.375 (0.435) data 0.244 (0.304) loss_u loss_u 0.7261 (0.7598) acc_u 37.5000 (30.1562) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [45/45] time 0.386 (0.434) data 0.255 (0.303) loss_u loss_u 0.7310 (0.7592) acc_u 34.3750 (30.6250) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1336
confident_label rate tensor(0.5195, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1629
clean true:1609
clean false:20
clean_rate:0.9877225291589933
noisy true:191
noisy false:1316
after delete: len(clean_dataset) 1629
after delete: len(noisy_dataset) 1507
epoch [149/200] batch [5/50] time 0.483 (0.503) data 0.352 (0.372) loss_x loss_x 1.4336 (1.0491) acc_x 62.5000 (71.8750) lr 3.2699e-04 eta 0:00:22
epoch [149/200] batch [10/50] time 0.521 (0.482) data 0.391 (0.351) loss_x loss_x 1.1895 (1.0530) acc_x 71.8750 (74.3750) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [15/50] time 0.419 (0.457) data 0.288 (0.326) loss_x loss_x 2.0840 (1.1407) acc_x 53.1250 (71.8750) lr 3.2699e-04 eta 0:00:16
epoch [149/200] batch [20/50] time 0.470 (0.451) data 0.340 (0.320) loss_x loss_x 1.1807 (1.1779) acc_x 78.1250 (71.8750) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [25/50] time 0.479 (0.457) data 0.348 (0.327) loss_x loss_x 1.2266 (1.1309) acc_x 65.6250 (72.3750) lr 3.2699e-04 eta 0:00:11
epoch [149/200] batch [30/50] time 0.479 (0.453) data 0.349 (0.322) loss_x loss_x 0.9414 (1.1247) acc_x 81.2500 (73.0208) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [35/50] time 0.516 (0.451) data 0.385 (0.320) loss_x loss_x 1.1602 (1.1206) acc_x 84.3750 (72.8571) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [40/50] time 0.372 (0.451) data 0.242 (0.320) loss_x loss_x 1.5156 (1.1384) acc_x 71.8750 (72.5000) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [45/50] time 0.394 (0.446) data 0.263 (0.316) loss_x loss_x 1.2393 (1.1305) acc_x 68.7500 (72.5000) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [50/50] time 0.439 (0.457) data 0.308 (0.326) loss_x loss_x 0.5752 (1.1346) acc_x 84.3750 (72.3750) lr 3.2699e-04 eta 0:00:00
epoch [149/200] batch [5/47] time 0.614 (0.458) data 0.483 (0.327) loss_u loss_u 0.7437 (0.7502) acc_u 28.1250 (30.6250) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [10/47] time 0.417 (0.456) data 0.285 (0.325) loss_u loss_u 0.7505 (0.7693) acc_u 34.3750 (28.4375) lr 3.2699e-04 eta 0:00:16
epoch [149/200] batch [15/47] time 0.294 (0.449) data 0.162 (0.318) loss_u loss_u 0.8184 (0.7575) acc_u 21.8750 (30.4167) lr 3.2699e-04 eta 0:00:14
epoch [149/200] batch [20/47] time 0.426 (0.447) data 0.294 (0.316) loss_u loss_u 0.8091 (0.7557) acc_u 18.7500 (30.3125) lr 3.2699e-04 eta 0:00:12
epoch [149/200] batch [25/47] time 0.445 (0.445) data 0.313 (0.314) loss_u loss_u 0.6963 (0.7515) acc_u 37.5000 (30.1250) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [30/47] time 0.423 (0.445) data 0.291 (0.314) loss_u loss_u 0.7256 (0.7523) acc_u 25.0000 (30.1042) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [35/47] time 0.473 (0.444) data 0.342 (0.313) loss_u loss_u 0.7407 (0.7472) acc_u 34.3750 (30.7143) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [40/47] time 0.396 (0.442) data 0.265 (0.311) loss_u loss_u 0.6992 (0.7465) acc_u 43.7500 (31.2500) lr 3.2699e-04 eta 0:00:03
epoch [149/200] batch [45/47] time 0.489 (0.443) data 0.358 (0.312) loss_u loss_u 0.8110 (0.7498) acc_u 21.8750 (30.9722) lr 3.2699e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1293
confident_label rate tensor(0.5351, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1678
clean true:1663
clean false:15
clean_rate:0.9910607866507747
noisy true:180
noisy false:1278
after delete: len(clean_dataset) 1678
after delete: len(noisy_dataset) 1458
epoch [150/200] batch [5/52] time 0.515 (0.504) data 0.384 (0.373) loss_x loss_x 1.4404 (1.3873) acc_x 65.6250 (68.1250) lr 3.1545e-04 eta 0:00:23
epoch [150/200] batch [10/52] time 0.474 (0.466) data 0.344 (0.335) loss_x loss_x 0.6060 (1.2294) acc_x 87.5000 (70.9375) lr 3.1545e-04 eta 0:00:19
epoch [150/200] batch [15/52] time 0.463 (0.453) data 0.332 (0.323) loss_x loss_x 0.7622 (1.1462) acc_x 81.2500 (72.5000) lr 3.1545e-04 eta 0:00:16
epoch [150/200] batch [20/52] time 0.498 (0.457) data 0.368 (0.327) loss_x loss_x 0.8032 (1.1655) acc_x 75.0000 (71.2500) lr 3.1545e-04 eta 0:00:14
epoch [150/200] batch [25/52] time 0.560 (0.474) data 0.429 (0.343) loss_x loss_x 0.9487 (1.1276) acc_x 71.8750 (72.6250) lr 3.1545e-04 eta 0:00:12
epoch [150/200] batch [30/52] time 0.377 (0.469) data 0.246 (0.338) loss_x loss_x 1.8691 (1.1514) acc_x 62.5000 (72.2917) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [35/52] time 0.363 (0.463) data 0.232 (0.332) loss_x loss_x 1.5625 (1.1493) acc_x 68.7500 (72.4107) lr 3.1545e-04 eta 0:00:07
epoch [150/200] batch [40/52] time 0.443 (0.460) data 0.313 (0.329) loss_x loss_x 1.5801 (1.1657) acc_x 53.1250 (71.9531) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [45/52] time 0.441 (0.460) data 0.311 (0.329) loss_x loss_x 0.6172 (1.1557) acc_x 81.2500 (72.2222) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [50/52] time 0.467 (0.460) data 0.338 (0.330) loss_x loss_x 0.6929 (1.1513) acc_x 78.1250 (71.8125) lr 3.1545e-04 eta 0:00:00
epoch [150/200] batch [5/45] time 0.345 (0.457) data 0.214 (0.326) loss_u loss_u 0.6680 (0.7535) acc_u 37.5000 (29.3750) lr 3.1545e-04 eta 0:00:18
epoch [150/200] batch [10/45] time 0.340 (0.452) data 0.209 (0.321) loss_u loss_u 0.6670 (0.7525) acc_u 46.8750 (30.0000) lr 3.1545e-04 eta 0:00:15
epoch [150/200] batch [15/45] time 0.398 (0.446) data 0.268 (0.316) loss_u loss_u 0.7510 (0.7493) acc_u 34.3750 (30.6250) lr 3.1545e-04 eta 0:00:13
epoch [150/200] batch [20/45] time 0.412 (0.444) data 0.280 (0.314) loss_u loss_u 0.8081 (0.7550) acc_u 21.8750 (30.1562) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [25/45] time 0.623 (0.441) data 0.491 (0.310) loss_u loss_u 0.8193 (0.7599) acc_u 21.8750 (30.1250) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [30/45] time 0.359 (0.440) data 0.226 (0.309) loss_u loss_u 0.7754 (0.7642) acc_u 28.1250 (29.2708) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [35/45] time 0.400 (0.437) data 0.269 (0.307) loss_u loss_u 0.7261 (0.7581) acc_u 25.0000 (29.5536) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [40/45] time 0.349 (0.437) data 0.217 (0.307) loss_u loss_u 0.7236 (0.7605) acc_u 40.6250 (29.5312) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [45/45] time 0.434 (0.437) data 0.303 (0.306) loss_u loss_u 0.7217 (0.7539) acc_u 34.3750 (30.1389) lr 3.1545e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1292
confident_label rate tensor(0.5364, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1682
clean true:1660
clean false:22
clean_rate:0.9869203329369798
noisy true:184
noisy false:1270
after delete: len(clean_dataset) 1682
after delete: len(noisy_dataset) 1454
epoch [151/200] batch [5/52] time 0.506 (0.430) data 0.376 (0.299) loss_x loss_x 1.3105 (1.1477) acc_x 62.5000 (67.5000) lr 3.0409e-04 eta 0:00:20
epoch [151/200] batch [10/52] time 0.395 (0.433) data 0.264 (0.303) loss_x loss_x 0.8091 (1.1359) acc_x 81.2500 (70.6250) lr 3.0409e-04 eta 0:00:18
epoch [151/200] batch [15/52] time 0.437 (0.438) data 0.307 (0.308) loss_x loss_x 1.1963 (1.1113) acc_x 75.0000 (73.5417) lr 3.0409e-04 eta 0:00:16
epoch [151/200] batch [20/52] time 0.499 (0.449) data 0.368 (0.319) loss_x loss_x 0.9287 (1.0958) acc_x 71.8750 (72.8125) lr 3.0409e-04 eta 0:00:14
epoch [151/200] batch [25/52] time 0.406 (0.441) data 0.276 (0.311) loss_x loss_x 0.8389 (1.0750) acc_x 81.2500 (73.1250) lr 3.0409e-04 eta 0:00:11
epoch [151/200] batch [30/52] time 0.404 (0.436) data 0.274 (0.306) loss_x loss_x 0.9346 (1.1210) acc_x 78.1250 (72.0833) lr 3.0409e-04 eta 0:00:09
epoch [151/200] batch [35/52] time 0.408 (0.442) data 0.278 (0.311) loss_x loss_x 1.1016 (1.1049) acc_x 71.8750 (72.7679) lr 3.0409e-04 eta 0:00:07
epoch [151/200] batch [40/52] time 0.335 (0.440) data 0.204 (0.310) loss_x loss_x 0.9253 (1.0934) acc_x 68.7500 (72.5781) lr 3.0409e-04 eta 0:00:05
epoch [151/200] batch [45/52] time 0.520 (0.437) data 0.389 (0.307) loss_x loss_x 0.9795 (1.1049) acc_x 75.0000 (72.5694) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [50/52] time 0.538 (0.441) data 0.407 (0.310) loss_x loss_x 1.2432 (1.0886) acc_x 53.1250 (72.7500) lr 3.0409e-04 eta 0:00:00
epoch [151/200] batch [5/45] time 0.388 (0.438) data 0.257 (0.308) loss_u loss_u 0.8071 (0.7788) acc_u 25.0000 (26.2500) lr 3.0409e-04 eta 0:00:17
epoch [151/200] batch [10/45] time 0.453 (0.444) data 0.322 (0.313) loss_u loss_u 0.7822 (0.7902) acc_u 31.2500 (25.9375) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [15/45] time 0.406 (0.442) data 0.274 (0.311) loss_u loss_u 0.7837 (0.7818) acc_u 21.8750 (27.0833) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [20/45] time 0.388 (0.440) data 0.256 (0.309) loss_u loss_u 0.7183 (0.7659) acc_u 43.7500 (30.3125) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [25/45] time 0.467 (0.442) data 0.336 (0.312) loss_u loss_u 0.7368 (0.7636) acc_u 31.2500 (30.1250) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [30/45] time 0.587 (0.442) data 0.456 (0.312) loss_u loss_u 0.7778 (0.7636) acc_u 34.3750 (30.0000) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [35/45] time 0.400 (0.441) data 0.269 (0.310) loss_u loss_u 0.7593 (0.7592) acc_u 37.5000 (30.7143) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [40/45] time 0.374 (0.441) data 0.242 (0.310) loss_u loss_u 0.7666 (0.7619) acc_u 31.2500 (30.3125) lr 3.0409e-04 eta 0:00:02
epoch [151/200] batch [45/45] time 0.393 (0.441) data 0.262 (0.310) loss_u loss_u 0.6211 (0.7576) acc_u 40.6250 (30.6250) lr 3.0409e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1294
confident_label rate tensor(0.5354, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1679
clean true:1663
clean false:16
clean_rate:0.9904705181655747
noisy true:179
noisy false:1278
after delete: len(clean_dataset) 1679
after delete: len(noisy_dataset) 1457
epoch [152/200] batch [5/52] time 0.388 (0.493) data 0.258 (0.363) loss_x loss_x 0.8027 (0.9076) acc_x 87.5000 (81.8750) lr 2.9289e-04 eta 0:00:23
epoch [152/200] batch [10/52] time 0.459 (0.456) data 0.329 (0.325) loss_x loss_x 1.0586 (1.0019) acc_x 78.1250 (77.1875) lr 2.9289e-04 eta 0:00:19
epoch [152/200] batch [15/52] time 0.428 (0.462) data 0.298 (0.331) loss_x loss_x 0.9258 (0.9609) acc_x 71.8750 (76.8750) lr 2.9289e-04 eta 0:00:17
epoch [152/200] batch [20/52] time 0.433 (0.466) data 0.302 (0.336) loss_x loss_x 1.1035 (1.0228) acc_x 71.8750 (75.3125) lr 2.9289e-04 eta 0:00:14
epoch [152/200] batch [25/52] time 0.568 (0.468) data 0.436 (0.338) loss_x loss_x 1.2832 (1.0593) acc_x 71.8750 (75.0000) lr 2.9289e-04 eta 0:00:12
epoch [152/200] batch [30/52] time 0.428 (0.465) data 0.298 (0.335) loss_x loss_x 1.2158 (1.0257) acc_x 71.8750 (75.7292) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [35/52] time 0.542 (0.463) data 0.411 (0.332) loss_x loss_x 1.0381 (1.0187) acc_x 75.0000 (75.8929) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [40/52] time 0.440 (0.453) data 0.310 (0.322) loss_x loss_x 1.0459 (1.0321) acc_x 59.3750 (75.0781) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [45/52] time 0.550 (0.463) data 0.420 (0.333) loss_x loss_x 1.8926 (1.0495) acc_x 40.6250 (73.8889) lr 2.9289e-04 eta 0:00:03
epoch [152/200] batch [50/52] time 0.495 (0.464) data 0.364 (0.333) loss_x loss_x 0.7900 (1.0411) acc_x 75.0000 (74.0000) lr 2.9289e-04 eta 0:00:00
epoch [152/200] batch [5/45] time 0.361 (0.459) data 0.230 (0.328) loss_u loss_u 0.8066 (0.7737) acc_u 18.7500 (25.6250) lr 2.9289e-04 eta 0:00:18
epoch [152/200] batch [10/45] time 0.377 (0.455) data 0.247 (0.325) loss_u loss_u 0.8345 (0.7699) acc_u 25.0000 (26.5625) lr 2.9289e-04 eta 0:00:15
epoch [152/200] batch [15/45] time 0.367 (0.456) data 0.237 (0.325) loss_u loss_u 0.7134 (0.7593) acc_u 34.3750 (29.3750) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [20/45] time 0.378 (0.455) data 0.247 (0.324) loss_u loss_u 0.7837 (0.7650) acc_u 21.8750 (28.2812) lr 2.9289e-04 eta 0:00:11
epoch [152/200] batch [25/45] time 0.584 (0.454) data 0.453 (0.323) loss_u loss_u 0.7607 (0.7619) acc_u 28.1250 (28.6250) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [30/45] time 0.369 (0.450) data 0.238 (0.320) loss_u loss_u 0.6782 (0.7610) acc_u 43.7500 (28.5417) lr 2.9289e-04 eta 0:00:06
epoch [152/200] batch [35/45] time 0.381 (0.446) data 0.250 (0.315) loss_u loss_u 0.7383 (0.7631) acc_u 31.2500 (28.5714) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [40/45] time 0.338 (0.442) data 0.207 (0.311) loss_u loss_u 0.7827 (0.7605) acc_u 28.1250 (28.9844) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [45/45] time 0.401 (0.441) data 0.270 (0.310) loss_u loss_u 0.6221 (0.7581) acc_u 43.7500 (29.0278) lr 2.9289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1285
confident_label rate tensor(0.5332, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1672
clean true:1662
clean false:10
clean_rate:0.9940191387559809
noisy true:189
noisy false:1275
after delete: len(clean_dataset) 1672
after delete: len(noisy_dataset) 1464
epoch [153/200] batch [5/52] time 0.471 (0.470) data 0.341 (0.340) loss_x loss_x 1.2236 (1.0470) acc_x 75.0000 (75.0000) lr 2.8187e-04 eta 0:00:22
epoch [153/200] batch [10/52] time 0.579 (0.455) data 0.449 (0.325) loss_x loss_x 1.0850 (1.1298) acc_x 71.8750 (70.6250) lr 2.8187e-04 eta 0:00:19
epoch [153/200] batch [15/52] time 0.433 (0.450) data 0.303 (0.320) loss_x loss_x 1.0723 (1.1427) acc_x 68.7500 (69.3750) lr 2.8187e-04 eta 0:00:16
epoch [153/200] batch [20/52] time 0.409 (0.453) data 0.279 (0.323) loss_x loss_x 1.2412 (1.1440) acc_x 71.8750 (69.5312) lr 2.8187e-04 eta 0:00:14
epoch [153/200] batch [25/52] time 0.396 (0.448) data 0.265 (0.317) loss_x loss_x 0.5737 (1.1363) acc_x 90.6250 (70.0000) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [30/52] time 0.479 (0.453) data 0.349 (0.323) loss_x loss_x 1.4385 (1.1406) acc_x 68.7500 (70.8333) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [35/52] time 0.419 (0.453) data 0.289 (0.323) loss_x loss_x 0.6982 (1.1184) acc_x 84.3750 (71.7857) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [40/52] time 0.465 (0.454) data 0.335 (0.324) loss_x loss_x 0.9111 (1.1151) acc_x 75.0000 (71.4844) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [45/52] time 0.506 (0.453) data 0.376 (0.323) loss_x loss_x 1.8789 (1.1450) acc_x 65.6250 (71.1111) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [50/52] time 0.355 (0.452) data 0.225 (0.321) loss_x loss_x 0.9736 (1.1072) acc_x 75.0000 (72.1875) lr 2.8187e-04 eta 0:00:00
epoch [153/200] batch [5/45] time 0.420 (0.445) data 0.290 (0.315) loss_u loss_u 0.7427 (0.7616) acc_u 34.3750 (31.2500) lr 2.8187e-04 eta 0:00:17
epoch [153/200] batch [10/45] time 0.420 (0.449) data 0.287 (0.318) loss_u loss_u 0.7075 (0.7688) acc_u 37.5000 (30.3125) lr 2.8187e-04 eta 0:00:15
epoch [153/200] batch [15/45] time 0.524 (0.451) data 0.393 (0.320) loss_u loss_u 0.6851 (0.7511) acc_u 37.5000 (32.0833) lr 2.8187e-04 eta 0:00:13
epoch [153/200] batch [20/45] time 0.439 (0.455) data 0.308 (0.324) loss_u loss_u 0.7622 (0.7504) acc_u 31.2500 (32.0312) lr 2.8187e-04 eta 0:00:11
epoch [153/200] batch [25/45] time 0.515 (0.456) data 0.384 (0.325) loss_u loss_u 0.7573 (0.7499) acc_u 28.1250 (31.6250) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [30/45] time 0.429 (0.451) data 0.298 (0.321) loss_u loss_u 0.7471 (0.7545) acc_u 31.2500 (30.8333) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [35/45] time 0.445 (0.451) data 0.313 (0.320) loss_u loss_u 0.7251 (0.7525) acc_u 34.3750 (31.0714) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [40/45] time 0.381 (0.448) data 0.249 (0.317) loss_u loss_u 0.7188 (0.7493) acc_u 34.3750 (31.7188) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [45/45] time 0.406 (0.445) data 0.275 (0.314) loss_u loss_u 0.7544 (0.7484) acc_u 25.0000 (31.4583) lr 2.8187e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1340
confident_label rate tensor(0.5239, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1643
clean true:1619
clean false:24
clean_rate:0.985392574558734
noisy true:177
noisy false:1316
after delete: len(clean_dataset) 1643
after delete: len(noisy_dataset) 1493
epoch [154/200] batch [5/51] time 0.484 (0.421) data 0.353 (0.290) loss_x loss_x 0.5986 (1.0814) acc_x 81.2500 (71.2500) lr 2.7103e-04 eta 0:00:19
epoch [154/200] batch [10/51] time 0.440 (0.432) data 0.309 (0.302) loss_x loss_x 1.2188 (1.1016) acc_x 71.8750 (73.4375) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [15/51] time 0.436 (0.435) data 0.305 (0.304) loss_x loss_x 1.0859 (1.0861) acc_x 71.8750 (73.7500) lr 2.7103e-04 eta 0:00:15
epoch [154/200] batch [20/51] time 0.502 (0.443) data 0.372 (0.312) loss_x loss_x 0.6387 (1.0072) acc_x 81.2500 (75.3125) lr 2.7103e-04 eta 0:00:13
epoch [154/200] batch [25/51] time 0.558 (0.447) data 0.428 (0.316) loss_x loss_x 0.8516 (0.9876) acc_x 75.0000 (75.3750) lr 2.7103e-04 eta 0:00:11
epoch [154/200] batch [30/51] time 0.557 (0.455) data 0.426 (0.324) loss_x loss_x 1.5010 (1.0482) acc_x 65.6250 (74.3750) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [35/51] time 0.449 (0.454) data 0.318 (0.323) loss_x loss_x 1.5459 (1.0611) acc_x 62.5000 (74.0179) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [40/51] time 0.393 (0.451) data 0.262 (0.320) loss_x loss_x 0.9575 (1.0765) acc_x 75.0000 (73.9844) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [45/51] time 0.597 (0.457) data 0.465 (0.326) loss_x loss_x 1.2051 (1.1079) acc_x 75.0000 (72.7778) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [50/51] time 0.375 (0.454) data 0.243 (0.323) loss_x loss_x 0.8599 (1.1284) acc_x 84.3750 (72.5000) lr 2.7103e-04 eta 0:00:00
epoch [154/200] batch [5/46] time 0.438 (0.451) data 0.306 (0.320) loss_u loss_u 0.7329 (0.7586) acc_u 37.5000 (30.0000) lr 2.7103e-04 eta 0:00:18
epoch [154/200] batch [10/46] time 0.629 (0.450) data 0.497 (0.319) loss_u loss_u 0.8071 (0.7732) acc_u 21.8750 (27.5000) lr 2.7103e-04 eta 0:00:16
epoch [154/200] batch [15/46] time 0.497 (0.453) data 0.366 (0.322) loss_u loss_u 0.7324 (0.7685) acc_u 34.3750 (28.7500) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [20/46] time 0.397 (0.451) data 0.265 (0.320) loss_u loss_u 0.7422 (0.7660) acc_u 34.3750 (29.0625) lr 2.7103e-04 eta 0:00:11
epoch [154/200] batch [25/46] time 0.338 (0.448) data 0.207 (0.317) loss_u loss_u 0.7622 (0.7646) acc_u 28.1250 (29.8750) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [30/46] time 0.406 (0.449) data 0.275 (0.317) loss_u loss_u 0.7466 (0.7667) acc_u 34.3750 (29.2708) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [35/46] time 0.467 (0.449) data 0.335 (0.318) loss_u loss_u 0.7666 (0.7646) acc_u 28.1250 (29.6429) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [40/46] time 0.456 (0.448) data 0.324 (0.316) loss_u loss_u 0.7949 (0.7628) acc_u 21.8750 (29.5312) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [45/46] time 0.368 (0.449) data 0.238 (0.317) loss_u loss_u 0.7402 (0.7590) acc_u 28.1250 (29.6528) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1328
confident_label rate tensor(0.5284, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1657
clean true:1633
clean false:24
clean_rate:0.9855159927579964
noisy true:175
noisy false:1304
after delete: len(clean_dataset) 1657
after delete: len(noisy_dataset) 1479
epoch [155/200] batch [5/51] time 0.365 (0.423) data 0.236 (0.293) loss_x loss_x 1.2930 (1.1998) acc_x 65.6250 (63.7500) lr 2.6037e-04 eta 0:00:19
epoch [155/200] batch [10/51] time 0.408 (0.432) data 0.277 (0.302) loss_x loss_x 0.9990 (1.1180) acc_x 71.8750 (69.6875) lr 2.6037e-04 eta 0:00:17
epoch [155/200] batch [15/51] time 0.575 (0.444) data 0.445 (0.314) loss_x loss_x 0.8550 (1.1254) acc_x 81.2500 (71.0417) lr 2.6037e-04 eta 0:00:15
epoch [155/200] batch [20/51] time 0.559 (0.443) data 0.428 (0.313) loss_x loss_x 1.0752 (1.1595) acc_x 71.8750 (70.6250) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [25/51] time 0.364 (0.439) data 0.234 (0.309) loss_x loss_x 1.0186 (1.1161) acc_x 68.7500 (72.1250) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [30/51] time 0.399 (0.441) data 0.269 (0.310) loss_x loss_x 1.2676 (1.0775) acc_x 71.8750 (73.2292) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [35/51] time 0.392 (0.441) data 0.262 (0.311) loss_x loss_x 1.0674 (1.0726) acc_x 68.7500 (72.8571) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [40/51] time 0.366 (0.437) data 0.236 (0.307) loss_x loss_x 1.3672 (1.1101) acc_x 68.7500 (72.1875) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [45/51] time 0.414 (0.438) data 0.284 (0.308) loss_x loss_x 1.7070 (1.1355) acc_x 65.6250 (71.6667) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [50/51] time 0.403 (0.441) data 0.273 (0.311) loss_x loss_x 0.8262 (1.1144) acc_x 84.3750 (72.3125) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/46] time 0.611 (0.440) data 0.481 (0.310) loss_u loss_u 0.7515 (0.7197) acc_u 34.3750 (38.1250) lr 2.6037e-04 eta 0:00:18
epoch [155/200] batch [10/46] time 0.508 (0.439) data 0.377 (0.309) loss_u loss_u 0.7344 (0.7308) acc_u 40.6250 (35.3125) lr 2.6037e-04 eta 0:00:15
epoch [155/200] batch [15/46] time 0.586 (0.440) data 0.455 (0.310) loss_u loss_u 0.7314 (0.7423) acc_u 34.3750 (32.5000) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [20/46] time 0.405 (0.438) data 0.274 (0.308) loss_u loss_u 0.7861 (0.7487) acc_u 25.0000 (31.5625) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [25/46] time 0.458 (0.437) data 0.328 (0.307) loss_u loss_u 0.7891 (0.7515) acc_u 18.7500 (30.7500) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [30/46] time 0.444 (0.437) data 0.313 (0.307) loss_u loss_u 0.7632 (0.7539) acc_u 25.0000 (30.1042) lr 2.6037e-04 eta 0:00:06
epoch [155/200] batch [35/46] time 0.389 (0.439) data 0.259 (0.308) loss_u loss_u 0.7158 (0.7516) acc_u 37.5000 (30.6250) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [40/46] time 0.410 (0.438) data 0.278 (0.308) loss_u loss_u 0.7964 (0.7566) acc_u 25.0000 (30.2344) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [45/46] time 0.359 (0.437) data 0.228 (0.306) loss_u loss_u 0.7056 (0.7547) acc_u 40.6250 (30.6250) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1301
confident_label rate tensor(0.5335, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1673
clean true:1655
clean false:18
clean_rate:0.9892408846383742
noisy true:180
noisy false:1283
after delete: len(clean_dataset) 1673
after delete: len(noisy_dataset) 1463
epoch [156/200] batch [5/52] time 0.366 (0.449) data 0.236 (0.319) loss_x loss_x 1.2754 (1.0332) acc_x 71.8750 (77.5000) lr 2.4989e-04 eta 0:00:21
epoch [156/200] batch [10/52] time 0.368 (0.447) data 0.238 (0.317) loss_x loss_x 0.9390 (1.0827) acc_x 78.1250 (75.6250) lr 2.4989e-04 eta 0:00:18
epoch [156/200] batch [15/52] time 0.436 (0.443) data 0.306 (0.313) loss_x loss_x 0.9272 (1.0965) acc_x 71.8750 (73.3333) lr 2.4989e-04 eta 0:00:16
epoch [156/200] batch [20/52] time 0.545 (0.448) data 0.414 (0.317) loss_x loss_x 1.1475 (1.1316) acc_x 68.7500 (72.1875) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [25/52] time 0.602 (0.455) data 0.471 (0.324) loss_x loss_x 0.8989 (1.1118) acc_x 68.7500 (72.2500) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [30/52] time 0.453 (0.449) data 0.323 (0.318) loss_x loss_x 0.7456 (1.1092) acc_x 84.3750 (72.2917) lr 2.4989e-04 eta 0:00:09
epoch [156/200] batch [35/52] time 0.463 (0.451) data 0.332 (0.320) loss_x loss_x 0.6973 (1.1050) acc_x 84.3750 (72.5893) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [40/52] time 0.484 (0.448) data 0.354 (0.318) loss_x loss_x 1.3809 (1.1074) acc_x 71.8750 (72.7344) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [45/52] time 0.504 (0.446) data 0.373 (0.315) loss_x loss_x 1.4092 (1.1148) acc_x 65.6250 (72.8472) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [50/52] time 0.415 (0.447) data 0.284 (0.316) loss_x loss_x 1.0918 (1.1118) acc_x 65.6250 (72.9375) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/45] time 0.342 (0.441) data 0.211 (0.310) loss_u loss_u 0.8257 (0.7532) acc_u 25.0000 (29.3750) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [10/45] time 0.340 (0.437) data 0.209 (0.306) loss_u loss_u 0.6997 (0.7456) acc_u 43.7500 (31.2500) lr 2.4989e-04 eta 0:00:15
epoch [156/200] batch [15/45] time 0.436 (0.437) data 0.305 (0.306) loss_u loss_u 0.7588 (0.7483) acc_u 31.2500 (30.2083) lr 2.4989e-04 eta 0:00:13
epoch [156/200] batch [20/45] time 0.403 (0.434) data 0.270 (0.303) loss_u loss_u 0.7578 (0.7514) acc_u 34.3750 (30.4688) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [25/45] time 0.428 (0.437) data 0.297 (0.307) loss_u loss_u 0.8120 (0.7546) acc_u 21.8750 (30.6250) lr 2.4989e-04 eta 0:00:08
epoch [156/200] batch [30/45] time 0.457 (0.436) data 0.326 (0.305) loss_u loss_u 0.7446 (0.7516) acc_u 31.2500 (31.2500) lr 2.4989e-04 eta 0:00:06
epoch [156/200] batch [35/45] time 0.550 (0.437) data 0.419 (0.306) loss_u loss_u 0.7930 (0.7508) acc_u 21.8750 (31.5179) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [40/45] time 0.488 (0.436) data 0.357 (0.305) loss_u loss_u 0.7480 (0.7432) acc_u 31.2500 (32.5000) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [45/45] time 0.362 (0.436) data 0.231 (0.305) loss_u loss_u 0.6660 (0.7415) acc_u 34.3750 (32.7778) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1310
confident_label rate tensor(0.5351, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1678
clean true:1653
clean false:25
clean_rate:0.9851013110846245
noisy true:173
noisy false:1285
after delete: len(clean_dataset) 1678
after delete: len(noisy_dataset) 1458
epoch [157/200] batch [5/52] time 0.449 (0.444) data 0.318 (0.313) loss_x loss_x 0.8188 (0.9144) acc_x 75.0000 (75.6250) lr 2.3959e-04 eta 0:00:20
epoch [157/200] batch [10/52] time 0.457 (0.441) data 0.327 (0.311) loss_x loss_x 0.7749 (0.9787) acc_x 81.2500 (74.6875) lr 2.3959e-04 eta 0:00:18
epoch [157/200] batch [15/52] time 0.449 (0.439) data 0.319 (0.308) loss_x loss_x 1.0625 (1.0252) acc_x 75.0000 (73.7500) lr 2.3959e-04 eta 0:00:16
epoch [157/200] batch [20/52] time 0.564 (0.452) data 0.434 (0.322) loss_x loss_x 0.9756 (1.0073) acc_x 75.0000 (75.4688) lr 2.3959e-04 eta 0:00:14
epoch [157/200] batch [25/52] time 0.506 (0.452) data 0.376 (0.322) loss_x loss_x 1.3027 (1.0153) acc_x 65.6250 (75.0000) lr 2.3959e-04 eta 0:00:12
epoch [157/200] batch [30/52] time 0.448 (0.451) data 0.318 (0.321) loss_x loss_x 0.8794 (1.0107) acc_x 81.2500 (75.6250) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [35/52] time 0.494 (0.445) data 0.364 (0.315) loss_x loss_x 1.0938 (1.0330) acc_x 75.0000 (74.5536) lr 2.3959e-04 eta 0:00:07
epoch [157/200] batch [40/52] time 0.365 (0.445) data 0.235 (0.315) loss_x loss_x 1.5400 (1.0655) acc_x 50.0000 (73.2812) lr 2.3959e-04 eta 0:00:05
epoch [157/200] batch [45/52] time 0.381 (0.450) data 0.250 (0.319) loss_x loss_x 0.9541 (1.0567) acc_x 75.0000 (73.4722) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [50/52] time 0.347 (0.447) data 0.216 (0.316) loss_x loss_x 1.2354 (1.0953) acc_x 62.5000 (72.8750) lr 2.3959e-04 eta 0:00:00
epoch [157/200] batch [5/45] time 0.375 (0.446) data 0.244 (0.316) loss_u loss_u 0.6475 (0.7242) acc_u 40.6250 (37.5000) lr 2.3959e-04 eta 0:00:17
epoch [157/200] batch [10/45] time 0.377 (0.442) data 0.246 (0.312) loss_u loss_u 0.6792 (0.7242) acc_u 34.3750 (35.0000) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [15/45] time 0.542 (0.440) data 0.411 (0.310) loss_u loss_u 0.8125 (0.7356) acc_u 31.2500 (34.1667) lr 2.3959e-04 eta 0:00:13
epoch [157/200] batch [20/45] time 0.684 (0.445) data 0.553 (0.314) loss_u loss_u 0.7720 (0.7358) acc_u 28.1250 (34.2188) lr 2.3959e-04 eta 0:00:11
epoch [157/200] batch [25/45] time 0.382 (0.442) data 0.251 (0.312) loss_u loss_u 0.7490 (0.7421) acc_u 28.1250 (33.0000) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [30/45] time 0.391 (0.442) data 0.260 (0.311) loss_u loss_u 0.7710 (0.7418) acc_u 25.0000 (32.7083) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [35/45] time 0.464 (0.441) data 0.333 (0.310) loss_u loss_u 0.8389 (0.7421) acc_u 21.8750 (32.7679) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [40/45] time 0.504 (0.439) data 0.373 (0.308) loss_u loss_u 0.8066 (0.7426) acc_u 28.1250 (33.1250) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [45/45] time 0.506 (0.438) data 0.374 (0.307) loss_u loss_u 0.6987 (0.7410) acc_u 37.5000 (33.1944) lr 2.3959e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1297
confident_label rate tensor(0.5341, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1675
clean true:1655
clean false:20
clean_rate:0.9880597014925373
noisy true:184
noisy false:1277
after delete: len(clean_dataset) 1675
after delete: len(noisy_dataset) 1461
epoch [158/200] batch [5/52] time 0.429 (0.440) data 0.299 (0.310) loss_x loss_x 1.6113 (1.0904) acc_x 65.6250 (72.5000) lr 2.2949e-04 eta 0:00:20
epoch [158/200] batch [10/52] time 0.403 (0.438) data 0.273 (0.307) loss_x loss_x 1.0332 (1.0785) acc_x 78.1250 (72.8125) lr 2.2949e-04 eta 0:00:18
epoch [158/200] batch [15/52] time 0.515 (0.447) data 0.385 (0.316) loss_x loss_x 1.0371 (1.1053) acc_x 75.0000 (72.2917) lr 2.2949e-04 eta 0:00:16
epoch [158/200] batch [20/52] time 0.372 (0.441) data 0.242 (0.311) loss_x loss_x 1.1729 (1.0948) acc_x 62.5000 (71.4062) lr 2.2949e-04 eta 0:00:14
epoch [158/200] batch [25/52] time 0.465 (0.443) data 0.335 (0.313) loss_x loss_x 0.8408 (1.0836) acc_x 75.0000 (71.6250) lr 2.2949e-04 eta 0:00:11
epoch [158/200] batch [30/52] time 0.482 (0.439) data 0.351 (0.309) loss_x loss_x 2.0977 (1.0968) acc_x 56.2500 (71.8750) lr 2.2949e-04 eta 0:00:09
epoch [158/200] batch [35/52] time 0.379 (0.442) data 0.249 (0.311) loss_x loss_x 0.8521 (1.0773) acc_x 81.2500 (72.5893) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [40/52] time 0.385 (0.440) data 0.255 (0.309) loss_x loss_x 0.7959 (1.0556) acc_x 78.1250 (73.1250) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [45/52] time 0.471 (0.442) data 0.341 (0.312) loss_x loss_x 0.8208 (1.0607) acc_x 84.3750 (73.1944) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [50/52] time 0.482 (0.443) data 0.352 (0.313) loss_x loss_x 0.9800 (1.0779) acc_x 75.0000 (73.1250) lr 2.2949e-04 eta 0:00:00
epoch [158/200] batch [5/45] time 0.395 (0.440) data 0.262 (0.309) loss_u loss_u 0.8057 (0.7657) acc_u 28.1250 (28.7500) lr 2.2949e-04 eta 0:00:17
epoch [158/200] batch [10/45] time 0.438 (0.439) data 0.307 (0.308) loss_u loss_u 0.6309 (0.7273) acc_u 43.7500 (34.3750) lr 2.2949e-04 eta 0:00:15
epoch [158/200] batch [15/45] time 0.409 (0.440) data 0.278 (0.310) loss_u loss_u 0.7495 (0.7292) acc_u 31.2500 (33.1250) lr 2.2949e-04 eta 0:00:13
epoch [158/200] batch [20/45] time 0.372 (0.439) data 0.241 (0.309) loss_u loss_u 0.7944 (0.7429) acc_u 28.1250 (31.7188) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [25/45] time 0.450 (0.441) data 0.320 (0.311) loss_u loss_u 0.7646 (0.7505) acc_u 31.2500 (30.6250) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [30/45] time 0.480 (0.441) data 0.349 (0.310) loss_u loss_u 0.6851 (0.7495) acc_u 46.8750 (31.7708) lr 2.2949e-04 eta 0:00:06
epoch [158/200] batch [35/45] time 0.500 (0.443) data 0.368 (0.313) loss_u loss_u 0.6587 (0.7498) acc_u 43.7500 (31.7857) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [40/45] time 0.323 (0.440) data 0.192 (0.309) loss_u loss_u 0.7896 (0.7530) acc_u 31.2500 (31.3281) lr 2.2949e-04 eta 0:00:02
epoch [158/200] batch [45/45] time 0.407 (0.440) data 0.277 (0.309) loss_u loss_u 0.7412 (0.7529) acc_u 31.2500 (31.0417) lr 2.2949e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1319
confident_label rate tensor(0.5322, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1669
clean true:1644
clean false:25
clean_rate:0.9850209706411025
noisy true:173
noisy false:1294
after delete: len(clean_dataset) 1669
after delete: len(noisy_dataset) 1467
epoch [159/200] batch [5/52] time 0.566 (0.448) data 0.436 (0.317) loss_x loss_x 1.1045 (1.0936) acc_x 62.5000 (66.2500) lr 2.1957e-04 eta 0:00:21
epoch [159/200] batch [10/52] time 0.425 (0.445) data 0.294 (0.314) loss_x loss_x 1.4258 (1.1134) acc_x 68.7500 (70.0000) lr 2.1957e-04 eta 0:00:18
epoch [159/200] batch [15/52] time 0.592 (0.458) data 0.461 (0.327) loss_x loss_x 0.9004 (1.0623) acc_x 78.1250 (72.0833) lr 2.1957e-04 eta 0:00:16
epoch [159/200] batch [20/52] time 0.467 (0.451) data 0.337 (0.321) loss_x loss_x 0.9653 (1.1055) acc_x 78.1250 (71.8750) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [25/52] time 0.414 (0.448) data 0.283 (0.317) loss_x loss_x 0.9209 (1.0665) acc_x 71.8750 (72.5000) lr 2.1957e-04 eta 0:00:12
epoch [159/200] batch [30/52] time 0.500 (0.454) data 0.369 (0.324) loss_x loss_x 1.1660 (1.0406) acc_x 71.8750 (73.2292) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [35/52] time 0.403 (0.458) data 0.272 (0.327) loss_x loss_x 1.0723 (1.0586) acc_x 81.2500 (73.1250) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [40/52] time 0.479 (0.458) data 0.349 (0.328) loss_x loss_x 0.7383 (1.0462) acc_x 78.1250 (73.8281) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [45/52] time 0.404 (0.455) data 0.274 (0.324) loss_x loss_x 0.9390 (1.0399) acc_x 81.2500 (74.0972) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [50/52] time 0.376 (0.451) data 0.246 (0.320) loss_x loss_x 0.9775 (1.0343) acc_x 71.8750 (74.2500) lr 2.1957e-04 eta 0:00:00
epoch [159/200] batch [5/45] time 0.597 (0.451) data 0.468 (0.321) loss_u loss_u 0.7573 (0.7968) acc_u 31.2500 (25.6250) lr 2.1957e-04 eta 0:00:18
epoch [159/200] batch [10/45] time 0.499 (0.450) data 0.367 (0.319) loss_u loss_u 0.7832 (0.7673) acc_u 31.2500 (30.0000) lr 2.1957e-04 eta 0:00:15
epoch [159/200] batch [15/45] time 0.451 (0.446) data 0.320 (0.315) loss_u loss_u 0.6445 (0.7581) acc_u 50.0000 (30.6250) lr 2.1957e-04 eta 0:00:13
epoch [159/200] batch [20/45] time 0.474 (0.444) data 0.343 (0.314) loss_u loss_u 0.7026 (0.7529) acc_u 37.5000 (31.4062) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [25/45] time 0.414 (0.442) data 0.284 (0.311) loss_u loss_u 0.7612 (0.7553) acc_u 31.2500 (30.7500) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [30/45] time 0.387 (0.439) data 0.250 (0.309) loss_u loss_u 0.7139 (0.7577) acc_u 43.7500 (30.7292) lr 2.1957e-04 eta 0:00:06
epoch [159/200] batch [35/45] time 0.355 (0.439) data 0.224 (0.308) loss_u loss_u 0.7036 (0.7542) acc_u 40.6250 (31.6071) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [40/45] time 0.417 (0.441) data 0.286 (0.310) loss_u loss_u 0.6836 (0.7461) acc_u 46.8750 (32.7344) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [45/45] time 0.421 (0.439) data 0.290 (0.308) loss_u loss_u 0.8442 (0.7486) acc_u 18.7500 (32.3611) lr 2.1957e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1292
confident_label rate tensor(0.5411, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1697
clean true:1674
clean false:23
clean_rate:0.986446670595168
noisy true:170
noisy false:1269
after delete: len(clean_dataset) 1697
after delete: len(noisy_dataset) 1439
epoch [160/200] batch [5/53] time 0.462 (0.475) data 0.332 (0.344) loss_x loss_x 0.9258 (0.8642) acc_x 71.8750 (78.1250) lr 2.0984e-04 eta 0:00:22
epoch [160/200] batch [10/53] time 0.325 (0.464) data 0.195 (0.333) loss_x loss_x 1.2168 (1.0301) acc_x 71.8750 (76.5625) lr 2.0984e-04 eta 0:00:19
epoch [160/200] batch [15/53] time 0.424 (0.461) data 0.294 (0.330) loss_x loss_x 0.7593 (1.0199) acc_x 87.5000 (75.2083) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [20/53] time 0.377 (0.451) data 0.247 (0.320) loss_x loss_x 1.2656 (1.0583) acc_x 65.6250 (73.2812) lr 2.0984e-04 eta 0:00:14
epoch [160/200] batch [25/53] time 0.381 (0.444) data 0.251 (0.314) loss_x loss_x 0.8091 (1.0689) acc_x 81.2500 (72.8750) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [30/53] time 0.385 (0.439) data 0.255 (0.309) loss_x loss_x 1.2305 (1.0739) acc_x 62.5000 (72.7083) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [35/53] time 0.508 (0.445) data 0.377 (0.315) loss_x loss_x 0.9355 (1.0734) acc_x 78.1250 (72.8571) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [40/53] time 0.584 (0.447) data 0.454 (0.317) loss_x loss_x 1.0127 (1.0322) acc_x 75.0000 (73.8281) lr 2.0984e-04 eta 0:00:05
epoch [160/200] batch [45/53] time 0.479 (0.452) data 0.349 (0.322) loss_x loss_x 1.5498 (1.0435) acc_x 68.7500 (73.8889) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [50/53] time 0.420 (0.456) data 0.290 (0.326) loss_x loss_x 1.1289 (1.0496) acc_x 71.8750 (73.3750) lr 2.0984e-04 eta 0:00:01
epoch [160/200] batch [5/44] time 0.302 (0.455) data 0.171 (0.324) loss_u loss_u 0.6895 (0.7370) acc_u 43.7500 (34.3750) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [10/44] time 0.380 (0.452) data 0.249 (0.321) loss_u loss_u 0.7773 (0.7517) acc_u 28.1250 (31.5625) lr 2.0984e-04 eta 0:00:15
epoch [160/200] batch [15/44] time 0.400 (0.448) data 0.270 (0.318) loss_u loss_u 0.7671 (0.7530) acc_u 31.2500 (31.6667) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [20/44] time 0.378 (0.446) data 0.248 (0.315) loss_u loss_u 0.7354 (0.7547) acc_u 31.2500 (31.0938) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [25/44] time 0.350 (0.443) data 0.220 (0.312) loss_u loss_u 0.7998 (0.7571) acc_u 31.2500 (30.5000) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [30/44] time 0.448 (0.441) data 0.317 (0.311) loss_u loss_u 0.7671 (0.7541) acc_u 34.3750 (30.9375) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [35/44] time 0.493 (0.440) data 0.362 (0.309) loss_u loss_u 0.7295 (0.7520) acc_u 37.5000 (30.9821) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [40/44] time 0.398 (0.441) data 0.267 (0.310) loss_u loss_u 0.8345 (0.7498) acc_u 18.7500 (31.3281) lr 2.0984e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1288
confident_label rate tensor(0.5392, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1691
clean true:1667
clean false:24
clean_rate:0.9858072146658782
noisy true:181
noisy false:1264
after delete: len(clean_dataset) 1691
after delete: len(noisy_dataset) 1445
epoch [161/200] batch [5/52] time 0.390 (0.521) data 0.260 (0.390) loss_x loss_x 1.1445 (1.1053) acc_x 68.7500 (74.3750) lr 2.0032e-04 eta 0:00:24
epoch [161/200] batch [10/52] time 0.425 (0.478) data 0.295 (0.348) loss_x loss_x 0.8594 (1.0928) acc_x 78.1250 (72.1875) lr 2.0032e-04 eta 0:00:20
epoch [161/200] batch [15/52] time 0.422 (0.490) data 0.291 (0.360) loss_x loss_x 1.0098 (1.1215) acc_x 81.2500 (72.5000) lr 2.0032e-04 eta 0:00:18
epoch [161/200] batch [20/52] time 0.450 (0.489) data 0.320 (0.359) loss_x loss_x 0.9580 (1.0843) acc_x 75.0000 (73.1250) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [25/52] time 0.455 (0.477) data 0.325 (0.347) loss_x loss_x 1.4072 (1.1067) acc_x 68.7500 (72.6250) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [30/52] time 0.340 (0.468) data 0.210 (0.338) loss_x loss_x 1.4092 (1.0876) acc_x 68.7500 (72.9167) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [35/52] time 0.401 (0.458) data 0.271 (0.328) loss_x loss_x 1.0488 (1.0982) acc_x 65.6250 (72.8571) lr 2.0032e-04 eta 0:00:07
epoch [161/200] batch [40/52] time 0.498 (0.461) data 0.368 (0.330) loss_x loss_x 1.5684 (1.1152) acc_x 56.2500 (72.6562) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [45/52] time 0.426 (0.458) data 0.296 (0.328) loss_x loss_x 0.7822 (1.0878) acc_x 81.2500 (73.2639) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [50/52] time 0.436 (0.459) data 0.306 (0.329) loss_x loss_x 1.2539 (1.1140) acc_x 68.7500 (72.8750) lr 2.0032e-04 eta 0:00:00
epoch [161/200] batch [5/45] time 0.643 (0.459) data 0.512 (0.329) loss_u loss_u 0.7661 (0.7792) acc_u 25.0000 (26.2500) lr 2.0032e-04 eta 0:00:18
epoch [161/200] batch [10/45] time 0.479 (0.454) data 0.348 (0.324) loss_u loss_u 0.6392 (0.7531) acc_u 40.6250 (31.2500) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [15/45] time 0.375 (0.452) data 0.245 (0.322) loss_u loss_u 0.7568 (0.7565) acc_u 28.1250 (30.2083) lr 2.0032e-04 eta 0:00:13
epoch [161/200] batch [20/45] time 0.350 (0.452) data 0.219 (0.322) loss_u loss_u 0.8057 (0.7625) acc_u 18.7500 (29.3750) lr 2.0032e-04 eta 0:00:11
epoch [161/200] batch [25/45] time 0.470 (0.450) data 0.337 (0.319) loss_u loss_u 0.6826 (0.7621) acc_u 37.5000 (29.7500) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [30/45] time 0.402 (0.448) data 0.271 (0.317) loss_u loss_u 0.6685 (0.7577) acc_u 46.8750 (30.5208) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [35/45] time 0.374 (0.445) data 0.243 (0.315) loss_u loss_u 0.8184 (0.7597) acc_u 25.0000 (30.8036) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [40/45] time 0.378 (0.445) data 0.247 (0.314) loss_u loss_u 0.7461 (0.7603) acc_u 31.2500 (30.3125) lr 2.0032e-04 eta 0:00:02
epoch [161/200] batch [45/45] time 0.423 (0.443) data 0.292 (0.312) loss_u loss_u 0.7817 (0.7597) acc_u 34.3750 (30.5556) lr 2.0032e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1317
confident_label rate tensor(0.5344, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1676
clean true:1652
clean false:24
clean_rate:0.9856801909307876
noisy true:167
noisy false:1293
after delete: len(clean_dataset) 1676
after delete: len(noisy_dataset) 1460
epoch [162/200] batch [5/52] time 0.369 (0.430) data 0.240 (0.300) loss_x loss_x 1.3857 (1.1657) acc_x 53.1250 (69.3750) lr 1.9098e-04 eta 0:00:20
epoch [162/200] batch [10/52] time 0.523 (0.436) data 0.392 (0.306) loss_x loss_x 1.3994 (1.1856) acc_x 65.6250 (70.9375) lr 1.9098e-04 eta 0:00:18
epoch [162/200] batch [15/52] time 0.550 (0.449) data 0.419 (0.319) loss_x loss_x 0.7837 (1.1081) acc_x 81.2500 (72.5000) lr 1.9098e-04 eta 0:00:16
epoch [162/200] batch [20/52] time 0.446 (0.444) data 0.315 (0.314) loss_x loss_x 1.4307 (1.1487) acc_x 56.2500 (71.5625) lr 1.9098e-04 eta 0:00:14
epoch [162/200] batch [25/52] time 0.574 (0.446) data 0.444 (0.315) loss_x loss_x 1.1396 (1.1411) acc_x 68.7500 (72.5000) lr 1.9098e-04 eta 0:00:12
epoch [162/200] batch [30/52] time 0.410 (0.451) data 0.279 (0.321) loss_x loss_x 1.9170 (1.1431) acc_x 59.3750 (72.9167) lr 1.9098e-04 eta 0:00:09
epoch [162/200] batch [35/52] time 0.492 (0.454) data 0.362 (0.324) loss_x loss_x 1.0547 (1.1133) acc_x 81.2500 (73.5714) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [40/52] time 0.492 (0.450) data 0.362 (0.320) loss_x loss_x 0.7983 (1.1209) acc_x 84.3750 (73.2031) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [45/52] time 0.426 (0.449) data 0.296 (0.319) loss_x loss_x 0.8955 (1.1291) acc_x 84.3750 (72.9861) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [50/52] time 0.556 (0.451) data 0.426 (0.320) loss_x loss_x 0.8188 (1.1286) acc_x 81.2500 (73.0000) lr 1.9098e-04 eta 0:00:00
epoch [162/200] batch [5/45] time 0.466 (0.444) data 0.335 (0.313) loss_u loss_u 0.6206 (0.7161) acc_u 46.8750 (34.3750) lr 1.9098e-04 eta 0:00:17
epoch [162/200] batch [10/45] time 0.405 (0.440) data 0.274 (0.309) loss_u loss_u 0.8276 (0.7556) acc_u 15.6250 (30.3125) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [15/45] time 0.503 (0.438) data 0.372 (0.308) loss_u loss_u 0.7988 (0.7492) acc_u 25.0000 (30.2083) lr 1.9098e-04 eta 0:00:13
epoch [162/200] batch [20/45] time 0.366 (0.435) data 0.235 (0.304) loss_u loss_u 0.7500 (0.7529) acc_u 21.8750 (29.5312) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [25/45] time 0.691 (0.440) data 0.560 (0.309) loss_u loss_u 0.7744 (0.7575) acc_u 21.8750 (28.6250) lr 1.9098e-04 eta 0:00:08
epoch [162/200] batch [30/45] time 0.401 (0.439) data 0.269 (0.308) loss_u loss_u 0.7046 (0.7577) acc_u 43.7500 (28.8542) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [35/45] time 0.388 (0.437) data 0.257 (0.307) loss_u loss_u 0.7603 (0.7592) acc_u 28.1250 (28.8393) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [40/45] time 0.356 (0.437) data 0.226 (0.306) loss_u loss_u 0.8232 (0.7623) acc_u 18.7500 (28.6719) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [45/45] time 0.399 (0.434) data 0.268 (0.303) loss_u loss_u 0.7632 (0.7613) acc_u 28.1250 (28.8889) lr 1.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1261
confident_label rate tensor(0.5421, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1700
clean true:1686
clean false:14
clean_rate:0.991764705882353
noisy true:189
noisy false:1247
after delete: len(clean_dataset) 1700
after delete: len(noisy_dataset) 1436
epoch [163/200] batch [5/53] time 0.398 (0.436) data 0.268 (0.305) loss_x loss_x 1.0850 (1.0746) acc_x 68.7500 (73.1250) lr 1.8185e-04 eta 0:00:20
epoch [163/200] batch [10/53] time 0.561 (0.479) data 0.431 (0.348) loss_x loss_x 1.0576 (0.9725) acc_x 75.0000 (76.8750) lr 1.8185e-04 eta 0:00:20
epoch [163/200] batch [15/53] time 0.511 (0.476) data 0.381 (0.346) loss_x loss_x 0.6338 (1.0114) acc_x 87.5000 (76.4583) lr 1.8185e-04 eta 0:00:18
epoch [163/200] batch [20/53] time 0.329 (0.479) data 0.199 (0.349) loss_x loss_x 1.1074 (1.0198) acc_x 78.1250 (76.4062) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [25/53] time 0.549 (0.485) data 0.418 (0.355) loss_x loss_x 0.8501 (1.0265) acc_x 78.1250 (75.8750) lr 1.8185e-04 eta 0:00:13
epoch [163/200] batch [30/53] time 0.388 (0.478) data 0.258 (0.348) loss_x loss_x 1.0557 (1.0405) acc_x 68.7500 (75.5208) lr 1.8185e-04 eta 0:00:11
epoch [163/200] batch [35/53] time 0.552 (0.470) data 0.421 (0.339) loss_x loss_x 1.0293 (1.0445) acc_x 84.3750 (75.4464) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [40/53] time 0.535 (0.467) data 0.404 (0.337) loss_x loss_x 1.0566 (1.0368) acc_x 71.8750 (75.1562) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [45/53] time 0.439 (0.459) data 0.308 (0.329) loss_x loss_x 1.1191 (1.0208) acc_x 71.8750 (75.4167) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [50/53] time 0.509 (0.456) data 0.377 (0.326) loss_x loss_x 0.8682 (1.0305) acc_x 78.1250 (74.7500) lr 1.8185e-04 eta 0:00:01
epoch [163/200] batch [5/44] time 0.373 (0.452) data 0.242 (0.321) loss_u loss_u 0.7500 (0.7893) acc_u 31.2500 (25.0000) lr 1.8185e-04 eta 0:00:17
epoch [163/200] batch [10/44] time 0.584 (0.452) data 0.453 (0.322) loss_u loss_u 0.7578 (0.7819) acc_u 31.2500 (25.9375) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [15/44] time 0.647 (0.453) data 0.516 (0.323) loss_u loss_u 0.7622 (0.7768) acc_u 21.8750 (25.8333) lr 1.8185e-04 eta 0:00:13
epoch [163/200] batch [20/44] time 0.419 (0.449) data 0.287 (0.319) loss_u loss_u 0.8096 (0.7698) acc_u 21.8750 (26.7188) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [25/44] time 0.344 (0.447) data 0.213 (0.316) loss_u loss_u 0.6768 (0.7611) acc_u 40.6250 (28.5000) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [30/44] time 0.367 (0.440) data 0.236 (0.309) loss_u loss_u 0.8364 (0.7667) acc_u 18.7500 (28.5417) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [35/44] time 0.406 (0.440) data 0.275 (0.310) loss_u loss_u 0.7354 (0.7670) acc_u 31.2500 (28.4821) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [40/44] time 0.371 (0.441) data 0.240 (0.311) loss_u loss_u 0.7515 (0.7603) acc_u 34.3750 (29.4531) lr 1.8185e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1273
confident_label rate tensor(0.5440, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1706
clean true:1679
clean false:27
clean_rate:0.9841735052754983
noisy true:184
noisy false:1246
after delete: len(clean_dataset) 1706
after delete: len(noisy_dataset) 1430
epoch [164/200] batch [5/53] time 0.393 (0.426) data 0.263 (0.295) loss_x loss_x 1.0137 (0.8686) acc_x 75.0000 (79.3750) lr 1.7292e-04 eta 0:00:20
epoch [164/200] batch [10/53] time 0.380 (0.413) data 0.249 (0.283) loss_x loss_x 1.0264 (0.9497) acc_x 71.8750 (76.8750) lr 1.7292e-04 eta 0:00:17
epoch [164/200] batch [15/53] time 0.426 (0.422) data 0.296 (0.291) loss_x loss_x 1.0781 (0.9375) acc_x 68.7500 (77.5000) lr 1.7292e-04 eta 0:00:16
epoch [164/200] batch [20/53] time 0.536 (0.439) data 0.406 (0.309) loss_x loss_x 0.7754 (0.9608) acc_x 81.2500 (76.4062) lr 1.7292e-04 eta 0:00:14
epoch [164/200] batch [25/53] time 0.428 (0.445) data 0.296 (0.314) loss_x loss_x 0.9878 (1.0030) acc_x 81.2500 (75.8750) lr 1.7292e-04 eta 0:00:12
epoch [164/200] batch [30/53] time 0.372 (0.436) data 0.242 (0.305) loss_x loss_x 1.3164 (1.0069) acc_x 62.5000 (75.4167) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [35/53] time 0.541 (0.432) data 0.410 (0.302) loss_x loss_x 0.7607 (1.0020) acc_x 81.2500 (74.3750) lr 1.7292e-04 eta 0:00:07
epoch [164/200] batch [40/53] time 0.494 (0.435) data 0.364 (0.304) loss_x loss_x 1.4424 (1.0413) acc_x 68.7500 (73.6719) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [45/53] time 0.324 (0.431) data 0.193 (0.301) loss_x loss_x 1.5713 (1.0572) acc_x 68.7500 (73.1250) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [50/53] time 0.480 (0.435) data 0.350 (0.304) loss_x loss_x 1.3984 (1.0679) acc_x 75.0000 (73.1875) lr 1.7292e-04 eta 0:00:01
epoch [164/200] batch [5/44] time 0.488 (0.434) data 0.358 (0.304) loss_u loss_u 0.6479 (0.7413) acc_u 46.8750 (32.5000) lr 1.7292e-04 eta 0:00:16
epoch [164/200] batch [10/44] time 0.336 (0.432) data 0.205 (0.302) loss_u loss_u 0.7998 (0.7441) acc_u 21.8750 (29.6875) lr 1.7292e-04 eta 0:00:14
epoch [164/200] batch [15/44] time 0.549 (0.432) data 0.419 (0.302) loss_u loss_u 0.7842 (0.7420) acc_u 25.0000 (30.2083) lr 1.7292e-04 eta 0:00:12
epoch [164/200] batch [20/44] time 0.388 (0.432) data 0.256 (0.302) loss_u loss_u 0.8257 (0.7355) acc_u 28.1250 (31.7188) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [25/44] time 0.400 (0.435) data 0.268 (0.304) loss_u loss_u 0.7935 (0.7420) acc_u 21.8750 (30.8750) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [30/44] time 0.490 (0.433) data 0.360 (0.303) loss_u loss_u 0.7695 (0.7439) acc_u 25.0000 (30.6250) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [35/44] time 0.347 (0.435) data 0.216 (0.304) loss_u loss_u 0.8354 (0.7470) acc_u 15.6250 (30.2679) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [40/44] time 0.530 (0.438) data 0.400 (0.307) loss_u loss_u 0.7783 (0.7526) acc_u 28.1250 (29.6094) lr 1.7292e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1243
confident_label rate tensor(0.5504, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1726
clean true:1703
clean false:23
clean_rate:0.9866743916570104
noisy true:190
noisy false:1220
after delete: len(clean_dataset) 1726
after delete: len(noisy_dataset) 1410
epoch [165/200] batch [5/53] time 0.402 (0.453) data 0.271 (0.323) loss_x loss_x 1.4482 (1.2007) acc_x 75.0000 (71.8750) lr 1.6419e-04 eta 0:00:21
epoch [165/200] batch [10/53] time 0.384 (0.439) data 0.253 (0.309) loss_x loss_x 1.5293 (1.2315) acc_x 65.6250 (70.0000) lr 1.6419e-04 eta 0:00:18
epoch [165/200] batch [15/53] time 0.639 (0.449) data 0.508 (0.319) loss_x loss_x 0.9014 (1.1297) acc_x 84.3750 (72.0833) lr 1.6419e-04 eta 0:00:17
epoch [165/200] batch [20/53] time 0.382 (0.453) data 0.252 (0.323) loss_x loss_x 1.4043 (1.1506) acc_x 56.2500 (71.5625) lr 1.6419e-04 eta 0:00:14
epoch [165/200] batch [25/53] time 0.426 (0.454) data 0.296 (0.323) loss_x loss_x 1.4609 (1.1223) acc_x 56.2500 (72.0000) lr 1.6419e-04 eta 0:00:12
epoch [165/200] batch [30/53] time 0.430 (0.458) data 0.299 (0.327) loss_x loss_x 0.9902 (1.1104) acc_x 75.0000 (72.1875) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [35/53] time 0.458 (0.454) data 0.328 (0.324) loss_x loss_x 1.0176 (1.1068) acc_x 78.1250 (72.3214) lr 1.6419e-04 eta 0:00:08
epoch [165/200] batch [40/53] time 0.571 (0.457) data 0.440 (0.327) loss_x loss_x 0.9067 (1.1081) acc_x 81.2500 (72.2656) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [45/53] time 0.592 (0.456) data 0.462 (0.326) loss_x loss_x 1.1865 (1.1057) acc_x 75.0000 (72.3611) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [50/53] time 0.307 (0.449) data 0.177 (0.319) loss_x loss_x 1.0156 (1.1131) acc_x 71.8750 (72.3125) lr 1.6419e-04 eta 0:00:01
epoch [165/200] batch [5/44] time 0.340 (0.444) data 0.209 (0.314) loss_u loss_u 0.8730 (0.7788) acc_u 15.6250 (30.0000) lr 1.6419e-04 eta 0:00:17
epoch [165/200] batch [10/44] time 0.483 (0.443) data 0.349 (0.312) loss_u loss_u 0.8096 (0.7771) acc_u 25.0000 (29.0625) lr 1.6419e-04 eta 0:00:15
epoch [165/200] batch [15/44] time 0.419 (0.443) data 0.287 (0.313) loss_u loss_u 0.8291 (0.7738) acc_u 18.7500 (29.3750) lr 1.6419e-04 eta 0:00:12
epoch [165/200] batch [20/44] time 0.455 (0.442) data 0.324 (0.311) loss_u loss_u 0.6592 (0.7601) acc_u 37.5000 (30.3125) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [25/44] time 0.369 (0.438) data 0.237 (0.308) loss_u loss_u 0.6470 (0.7503) acc_u 50.0000 (31.8750) lr 1.6419e-04 eta 0:00:08
epoch [165/200] batch [30/44] time 0.385 (0.436) data 0.254 (0.305) loss_u loss_u 0.7944 (0.7563) acc_u 28.1250 (31.1458) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [35/44] time 0.379 (0.436) data 0.248 (0.305) loss_u loss_u 0.8276 (0.7517) acc_u 18.7500 (31.4286) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [40/44] time 0.526 (0.441) data 0.395 (0.311) loss_u loss_u 0.8301 (0.7536) acc_u 21.8750 (31.3281) lr 1.6419e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1318
confident_label rate tensor(0.5268, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1652
clean true:1628
clean false:24
clean_rate:0.9854721549636803
noisy true:190
noisy false:1294
after delete: len(clean_dataset) 1652
after delete: len(noisy_dataset) 1484
epoch [166/200] batch [5/51] time 0.413 (0.436) data 0.283 (0.305) loss_x loss_x 0.8301 (1.0243) acc_x 78.1250 (76.8750) lr 1.5567e-04 eta 0:00:20
epoch [166/200] batch [10/51] time 0.421 (0.425) data 0.290 (0.295) loss_x loss_x 0.7524 (1.0802) acc_x 75.0000 (75.9375) lr 1.5567e-04 eta 0:00:17
epoch [166/200] batch [15/51] time 0.472 (0.448) data 0.341 (0.318) loss_x loss_x 0.9873 (1.0850) acc_x 84.3750 (76.2500) lr 1.5567e-04 eta 0:00:16
epoch [166/200] batch [20/51] time 0.492 (0.459) data 0.359 (0.329) loss_x loss_x 1.5234 (1.0510) acc_x 68.7500 (75.9375) lr 1.5567e-04 eta 0:00:14
epoch [166/200] batch [25/51] time 0.425 (0.453) data 0.294 (0.322) loss_x loss_x 0.8374 (1.0440) acc_x 75.0000 (75.1250) lr 1.5567e-04 eta 0:00:11
epoch [166/200] batch [30/51] time 0.411 (0.444) data 0.281 (0.313) loss_x loss_x 0.7612 (1.0720) acc_x 87.5000 (74.7917) lr 1.5567e-04 eta 0:00:09
epoch [166/200] batch [35/51] time 0.481 (0.451) data 0.351 (0.321) loss_x loss_x 1.0195 (1.0758) acc_x 78.1250 (74.6429) lr 1.5567e-04 eta 0:00:07
epoch [166/200] batch [40/51] time 0.422 (0.448) data 0.291 (0.318) loss_x loss_x 0.5889 (1.0711) acc_x 87.5000 (74.8438) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [45/51] time 0.495 (0.448) data 0.365 (0.317) loss_x loss_x 1.5322 (1.0875) acc_x 65.6250 (74.4444) lr 1.5567e-04 eta 0:00:02
epoch [166/200] batch [50/51] time 0.421 (0.450) data 0.290 (0.320) loss_x loss_x 0.7388 (1.0640) acc_x 81.2500 (74.9375) lr 1.5567e-04 eta 0:00:00
epoch [166/200] batch [5/46] time 0.419 (0.450) data 0.288 (0.320) loss_u loss_u 0.7285 (0.7734) acc_u 34.3750 (28.7500) lr 1.5567e-04 eta 0:00:18
epoch [166/200] batch [10/46] time 0.329 (0.447) data 0.198 (0.317) loss_u loss_u 0.6934 (0.7548) acc_u 43.7500 (32.5000) lr 1.5567e-04 eta 0:00:16
epoch [166/200] batch [15/46] time 0.530 (0.445) data 0.399 (0.315) loss_u loss_u 0.6846 (0.7445) acc_u 37.5000 (33.3333) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [20/46] time 0.414 (0.441) data 0.283 (0.310) loss_u loss_u 0.7397 (0.7399) acc_u 37.5000 (33.9062) lr 1.5567e-04 eta 0:00:11
epoch [166/200] batch [25/46] time 0.345 (0.441) data 0.214 (0.310) loss_u loss_u 0.7798 (0.7474) acc_u 34.3750 (33.2500) lr 1.5567e-04 eta 0:00:09
epoch [166/200] batch [30/46] time 0.378 (0.439) data 0.246 (0.309) loss_u loss_u 0.7036 (0.7412) acc_u 31.2500 (33.7500) lr 1.5567e-04 eta 0:00:07
epoch [166/200] batch [35/46] time 0.523 (0.440) data 0.391 (0.309) loss_u loss_u 0.6973 (0.7466) acc_u 34.3750 (33.3036) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [40/46] time 0.488 (0.438) data 0.357 (0.307) loss_u loss_u 0.7134 (0.7477) acc_u 40.6250 (32.8906) lr 1.5567e-04 eta 0:00:02
epoch [166/200] batch [45/46] time 0.368 (0.436) data 0.237 (0.306) loss_u loss_u 0.7168 (0.7503) acc_u 34.3750 (32.6389) lr 1.5567e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1324
confident_label rate tensor(0.5312, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1666
clean true:1643
clean false:23
clean_rate:0.9861944777911165
noisy true:169
noisy false:1301
after delete: len(clean_dataset) 1666
after delete: len(noisy_dataset) 1470
epoch [167/200] batch [5/52] time 0.497 (0.530) data 0.367 (0.399) loss_x loss_x 1.0830 (0.9546) acc_x 71.8750 (79.3750) lr 1.4736e-04 eta 0:00:24
epoch [167/200] batch [10/52] time 0.503 (0.495) data 0.372 (0.365) loss_x loss_x 0.9170 (0.9874) acc_x 78.1250 (77.5000) lr 1.4736e-04 eta 0:00:20
epoch [167/200] batch [15/52] time 0.509 (0.478) data 0.378 (0.348) loss_x loss_x 0.6914 (0.9102) acc_x 81.2500 (77.9167) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [20/52] time 0.345 (0.463) data 0.215 (0.332) loss_x loss_x 1.1279 (0.9584) acc_x 71.8750 (75.9375) lr 1.4736e-04 eta 0:00:14
epoch [167/200] batch [25/52] time 0.579 (0.464) data 0.448 (0.333) loss_x loss_x 1.3428 (0.9649) acc_x 71.8750 (76.1250) lr 1.4736e-04 eta 0:00:12
epoch [167/200] batch [30/52] time 0.399 (0.460) data 0.269 (0.330) loss_x loss_x 0.7915 (0.9917) acc_x 81.2500 (75.5208) lr 1.4736e-04 eta 0:00:10
epoch [167/200] batch [35/52] time 0.391 (0.458) data 0.261 (0.328) loss_x loss_x 1.3828 (1.0166) acc_x 68.7500 (75.1786) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [40/52] time 0.406 (0.457) data 0.276 (0.326) loss_x loss_x 1.4248 (1.0307) acc_x 71.8750 (75.0781) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [45/52] time 0.407 (0.453) data 0.277 (0.323) loss_x loss_x 1.0840 (1.0016) acc_x 75.0000 (75.9028) lr 1.4736e-04 eta 0:00:03
epoch [167/200] batch [50/52] time 0.375 (0.449) data 0.244 (0.319) loss_x loss_x 1.2061 (1.0053) acc_x 65.6250 (75.2500) lr 1.4736e-04 eta 0:00:00
epoch [167/200] batch [5/45] time 0.509 (0.448) data 0.378 (0.318) loss_u loss_u 0.7251 (0.7146) acc_u 34.3750 (38.7500) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [10/45] time 0.319 (0.449) data 0.188 (0.318) loss_u loss_u 0.7920 (0.7257) acc_u 34.3750 (37.5000) lr 1.4736e-04 eta 0:00:15
epoch [167/200] batch [15/45] time 0.545 (0.446) data 0.414 (0.316) loss_u loss_u 0.8047 (0.7427) acc_u 28.1250 (35.0000) lr 1.4736e-04 eta 0:00:13
epoch [167/200] batch [20/45] time 0.415 (0.442) data 0.284 (0.312) loss_u loss_u 0.6221 (0.7398) acc_u 50.0000 (35.3125) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [25/45] time 0.390 (0.441) data 0.259 (0.310) loss_u loss_u 0.7124 (0.7386) acc_u 31.2500 (34.2500) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [30/45] time 0.481 (0.440) data 0.350 (0.309) loss_u loss_u 0.8452 (0.7455) acc_u 15.6250 (33.2292) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [35/45] time 0.410 (0.440) data 0.279 (0.309) loss_u loss_u 0.7598 (0.7422) acc_u 31.2500 (33.8393) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [40/45] time 0.400 (0.440) data 0.269 (0.310) loss_u loss_u 0.6006 (0.7439) acc_u 56.2500 (33.5156) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [45/45] time 0.417 (0.439) data 0.286 (0.309) loss_u loss_u 0.7852 (0.7433) acc_u 25.0000 (33.1250) lr 1.4736e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1331
confident_label rate tensor(0.5306, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1664
clean true:1643
clean false:21
clean_rate:0.9873798076923077
noisy true:162
noisy false:1310
after delete: len(clean_dataset) 1664
after delete: len(noisy_dataset) 1472
epoch [168/200] batch [5/52] time 0.432 (0.416) data 0.302 (0.285) loss_x loss_x 0.7202 (1.0850) acc_x 84.3750 (72.5000) lr 1.3926e-04 eta 0:00:19
epoch [168/200] batch [10/52] time 0.415 (0.428) data 0.285 (0.298) loss_x loss_x 1.1162 (1.0400) acc_x 75.0000 (74.3750) lr 1.3926e-04 eta 0:00:17
epoch [168/200] batch [15/52] time 0.421 (0.462) data 0.291 (0.331) loss_x loss_x 1.0000 (0.9637) acc_x 75.0000 (75.8333) lr 1.3926e-04 eta 0:00:17
epoch [168/200] batch [20/52] time 0.325 (0.459) data 0.194 (0.329) loss_x loss_x 1.0078 (0.9890) acc_x 78.1250 (75.7812) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [25/52] time 0.420 (0.450) data 0.290 (0.319) loss_x loss_x 1.0430 (1.0399) acc_x 75.0000 (74.0000) lr 1.3926e-04 eta 0:00:12
epoch [168/200] batch [30/52] time 0.479 (0.455) data 0.349 (0.324) loss_x loss_x 1.1289 (1.0782) acc_x 78.1250 (73.1250) lr 1.3926e-04 eta 0:00:10
epoch [168/200] batch [35/52] time 0.500 (0.455) data 0.370 (0.325) loss_x loss_x 1.1914 (1.0470) acc_x 71.8750 (73.8393) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [40/52] time 0.458 (0.450) data 0.328 (0.319) loss_x loss_x 0.7329 (1.0106) acc_x 81.2500 (74.6094) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [45/52] time 0.464 (0.456) data 0.333 (0.325) loss_x loss_x 1.2686 (1.0262) acc_x 68.7500 (74.2361) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [50/52] time 0.469 (0.452) data 0.338 (0.322) loss_x loss_x 1.0586 (1.0371) acc_x 75.0000 (73.8750) lr 1.3926e-04 eta 0:00:00
epoch [168/200] batch [5/46] time 0.302 (0.448) data 0.171 (0.317) loss_u loss_u 0.7642 (0.7242) acc_u 31.2500 (35.0000) lr 1.3926e-04 eta 0:00:18
epoch [168/200] batch [10/46] time 0.449 (0.450) data 0.318 (0.320) loss_u loss_u 0.8008 (0.7288) acc_u 28.1250 (35.0000) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [15/46] time 0.476 (0.450) data 0.345 (0.319) loss_u loss_u 0.7935 (0.7346) acc_u 21.8750 (33.7500) lr 1.3926e-04 eta 0:00:13
epoch [168/200] batch [20/46] time 0.372 (0.446) data 0.241 (0.315) loss_u loss_u 0.6880 (0.7260) acc_u 37.5000 (34.6875) lr 1.3926e-04 eta 0:00:11
epoch [168/200] batch [25/46] time 0.346 (0.444) data 0.215 (0.313) loss_u loss_u 0.6738 (0.7286) acc_u 40.6250 (34.6250) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [30/46] time 0.437 (0.444) data 0.306 (0.314) loss_u loss_u 0.6221 (0.7284) acc_u 46.8750 (34.7917) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [35/46] time 0.519 (0.444) data 0.387 (0.314) loss_u loss_u 0.7827 (0.7339) acc_u 25.0000 (33.7500) lr 1.3926e-04 eta 0:00:04
epoch [168/200] batch [40/46] time 0.390 (0.445) data 0.258 (0.314) loss_u loss_u 0.7075 (0.7361) acc_u 37.5000 (33.5938) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [45/46] time 0.416 (0.442) data 0.285 (0.311) loss_u loss_u 0.8354 (0.7386) acc_u 15.6250 (33.1944) lr 1.3926e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1240
confident_label rate tensor(0.5494, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1723
clean true:1707
clean false:16
clean_rate:0.9907138711549622
noisy true:189
noisy false:1224
after delete: len(clean_dataset) 1723
after delete: len(noisy_dataset) 1413
epoch [169/200] batch [5/53] time 0.460 (0.429) data 0.329 (0.298) loss_x loss_x 0.7646 (0.8550) acc_x 71.8750 (76.2500) lr 1.3137e-04 eta 0:00:20
epoch [169/200] batch [10/53] time 0.372 (0.422) data 0.241 (0.292) loss_x loss_x 1.1895 (0.9731) acc_x 78.1250 (75.0000) lr 1.3137e-04 eta 0:00:18
epoch [169/200] batch [15/53] time 0.503 (0.419) data 0.373 (0.288) loss_x loss_x 1.4795 (1.0709) acc_x 59.3750 (72.7083) lr 1.3137e-04 eta 0:00:15
epoch [169/200] batch [20/53] time 0.426 (0.429) data 0.295 (0.298) loss_x loss_x 0.9966 (1.0082) acc_x 71.8750 (74.2188) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [25/53] time 0.398 (0.424) data 0.267 (0.293) loss_x loss_x 1.1719 (0.9954) acc_x 81.2500 (74.7500) lr 1.3137e-04 eta 0:00:11
epoch [169/200] batch [30/53] time 0.458 (0.423) data 0.327 (0.293) loss_x loss_x 1.3320 (0.9903) acc_x 65.6250 (74.3750) lr 1.3137e-04 eta 0:00:09
epoch [169/200] batch [35/53] time 0.475 (0.435) data 0.344 (0.304) loss_x loss_x 0.9688 (0.9758) acc_x 75.0000 (74.6429) lr 1.3137e-04 eta 0:00:07
epoch [169/200] batch [40/53] time 0.391 (0.433) data 0.261 (0.302) loss_x loss_x 1.2734 (0.9705) acc_x 75.0000 (75.0781) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [45/53] time 0.555 (0.437) data 0.423 (0.307) loss_x loss_x 1.4473 (1.0040) acc_x 62.5000 (74.3056) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [50/53] time 0.454 (0.443) data 0.323 (0.313) loss_x loss_x 1.1680 (1.0415) acc_x 65.6250 (73.5000) lr 1.3137e-04 eta 0:00:01
epoch [169/200] batch [5/44] time 0.570 (0.447) data 0.439 (0.316) loss_u loss_u 0.7988 (0.7916) acc_u 25.0000 (26.2500) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [10/44] time 0.382 (0.442) data 0.251 (0.311) loss_u loss_u 0.7559 (0.7804) acc_u 34.3750 (27.8125) lr 1.3137e-04 eta 0:00:15
epoch [169/200] batch [15/44] time 0.409 (0.443) data 0.278 (0.313) loss_u loss_u 0.7051 (0.7608) acc_u 40.6250 (29.7917) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [20/44] time 0.314 (0.442) data 0.183 (0.312) loss_u loss_u 0.7520 (0.7648) acc_u 28.1250 (29.8438) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [25/44] time 0.555 (0.443) data 0.424 (0.312) loss_u loss_u 0.7417 (0.7613) acc_u 28.1250 (30.0000) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [30/44] time 0.348 (0.440) data 0.217 (0.309) loss_u loss_u 0.8174 (0.7632) acc_u 31.2500 (30.1042) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [35/44] time 0.449 (0.441) data 0.318 (0.310) loss_u loss_u 0.7119 (0.7581) acc_u 31.2500 (30.7143) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [40/44] time 0.348 (0.440) data 0.217 (0.309) loss_u loss_u 0.7622 (0.7594) acc_u 34.3750 (30.7812) lr 1.3137e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1323
confident_label rate tensor(0.5316, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1667
clean true:1645
clean false:22
clean_rate:0.9868026394721056
noisy true:168
noisy false:1301
after delete: len(clean_dataset) 1667
after delete: len(noisy_dataset) 1469
epoch [170/200] batch [5/52] time 0.449 (0.499) data 0.319 (0.368) loss_x loss_x 1.2979 (1.0137) acc_x 68.7500 (75.0000) lr 1.2369e-04 eta 0:00:23
epoch [170/200] batch [10/52] time 0.510 (0.476) data 0.380 (0.345) loss_x loss_x 1.0645 (1.0095) acc_x 75.0000 (75.3125) lr 1.2369e-04 eta 0:00:19
epoch [170/200] batch [15/52] time 0.388 (0.454) data 0.258 (0.323) loss_x loss_x 1.1084 (1.0333) acc_x 78.1250 (74.1667) lr 1.2369e-04 eta 0:00:16
epoch [170/200] batch [20/52] time 0.428 (0.456) data 0.297 (0.325) loss_x loss_x 1.3740 (1.0498) acc_x 75.0000 (74.2188) lr 1.2369e-04 eta 0:00:14
epoch [170/200] batch [25/52] time 0.558 (0.471) data 0.428 (0.340) loss_x loss_x 0.8071 (1.0580) acc_x 71.8750 (73.8750) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [30/52] time 0.450 (0.466) data 0.320 (0.335) loss_x loss_x 1.2510 (1.0657) acc_x 75.0000 (73.2292) lr 1.2369e-04 eta 0:00:10
epoch [170/200] batch [35/52] time 0.413 (0.465) data 0.282 (0.334) loss_x loss_x 1.1416 (1.0765) acc_x 71.8750 (72.9464) lr 1.2369e-04 eta 0:00:07
epoch [170/200] batch [40/52] time 0.520 (0.458) data 0.389 (0.327) loss_x loss_x 0.9287 (1.0595) acc_x 75.0000 (73.5156) lr 1.2369e-04 eta 0:00:05
epoch [170/200] batch [45/52] time 0.473 (0.458) data 0.343 (0.328) loss_x loss_x 0.7354 (1.0725) acc_x 84.3750 (72.7083) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [50/52] time 0.415 (0.460) data 0.285 (0.329) loss_x loss_x 1.1631 (1.0844) acc_x 71.8750 (72.3125) lr 1.2369e-04 eta 0:00:00
epoch [170/200] batch [5/45] time 0.421 (0.453) data 0.290 (0.322) loss_u loss_u 0.8135 (0.7394) acc_u 18.7500 (31.2500) lr 1.2369e-04 eta 0:00:18
epoch [170/200] batch [10/45] time 0.463 (0.451) data 0.332 (0.321) loss_u loss_u 0.7998 (0.7541) acc_u 28.1250 (30.9375) lr 1.2369e-04 eta 0:00:15
epoch [170/200] batch [15/45] time 0.428 (0.450) data 0.297 (0.320) loss_u loss_u 0.8511 (0.7611) acc_u 18.7500 (30.2083) lr 1.2369e-04 eta 0:00:13
epoch [170/200] batch [20/45] time 0.362 (0.451) data 0.231 (0.320) loss_u loss_u 0.8145 (0.7495) acc_u 25.0000 (31.4062) lr 1.2369e-04 eta 0:00:11
epoch [170/200] batch [25/45] time 0.362 (0.451) data 0.230 (0.320) loss_u loss_u 0.8066 (0.7518) acc_u 28.1250 (31.6250) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [30/45] time 0.365 (0.450) data 0.234 (0.319) loss_u loss_u 0.7236 (0.7466) acc_u 34.3750 (31.7708) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [35/45] time 0.406 (0.447) data 0.275 (0.317) loss_u loss_u 0.7520 (0.7503) acc_u 25.0000 (30.9821) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [40/45] time 0.342 (0.445) data 0.211 (0.314) loss_u loss_u 0.8018 (0.7509) acc_u 25.0000 (31.0156) lr 1.2369e-04 eta 0:00:02
epoch [170/200] batch [45/45] time 0.332 (0.442) data 0.201 (0.311) loss_u loss_u 0.7642 (0.7518) acc_u 25.0000 (30.5556) lr 1.2369e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1304
confident_label rate tensor(0.5348, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1677
clean true:1656
clean false:21
clean_rate:0.9874776386404294
noisy true:176
noisy false:1283
after delete: len(clean_dataset) 1677
after delete: len(noisy_dataset) 1459
epoch [171/200] batch [5/52] time 0.597 (0.481) data 0.467 (0.350) loss_x loss_x 0.8730 (0.9133) acc_x 78.1250 (76.8750) lr 1.1623e-04 eta 0:00:22
epoch [171/200] batch [10/52] time 0.433 (0.447) data 0.303 (0.316) loss_x loss_x 1.4775 (1.0777) acc_x 56.2500 (71.5625) lr 1.1623e-04 eta 0:00:18
epoch [171/200] batch [15/52] time 0.417 (0.464) data 0.287 (0.333) loss_x loss_x 0.9814 (1.0488) acc_x 84.3750 (75.0000) lr 1.1623e-04 eta 0:00:17
epoch [171/200] batch [20/52] time 0.414 (0.450) data 0.284 (0.320) loss_x loss_x 1.2002 (1.1381) acc_x 75.0000 (72.6562) lr 1.1623e-04 eta 0:00:14
epoch [171/200] batch [25/52] time 0.470 (0.459) data 0.339 (0.329) loss_x loss_x 1.0332 (1.1282) acc_x 68.7500 (72.8750) lr 1.1623e-04 eta 0:00:12
epoch [171/200] batch [30/52] time 0.457 (0.457) data 0.327 (0.326) loss_x loss_x 0.8657 (1.1096) acc_x 81.2500 (73.1250) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [35/52] time 0.607 (0.459) data 0.475 (0.329) loss_x loss_x 1.1631 (1.1264) acc_x 65.6250 (72.4107) lr 1.1623e-04 eta 0:00:07
epoch [171/200] batch [40/52] time 0.554 (0.465) data 0.423 (0.334) loss_x loss_x 0.5742 (1.0894) acc_x 90.6250 (73.7500) lr 1.1623e-04 eta 0:00:05
epoch [171/200] batch [45/52] time 0.510 (0.464) data 0.379 (0.334) loss_x loss_x 1.2480 (1.0741) acc_x 71.8750 (73.9583) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [50/52] time 0.497 (0.462) data 0.366 (0.331) loss_x loss_x 0.6421 (1.0558) acc_x 81.2500 (74.2500) lr 1.1623e-04 eta 0:00:00
epoch [171/200] batch [5/45] time 0.372 (0.456) data 0.241 (0.325) loss_u loss_u 0.7056 (0.7266) acc_u 31.2500 (31.2500) lr 1.1623e-04 eta 0:00:18
epoch [171/200] batch [10/45] time 0.478 (0.452) data 0.347 (0.321) loss_u loss_u 0.6489 (0.7208) acc_u 40.6250 (32.5000) lr 1.1623e-04 eta 0:00:15
epoch [171/200] batch [15/45] time 0.374 (0.449) data 0.242 (0.318) loss_u loss_u 0.8843 (0.7377) acc_u 12.5000 (30.8333) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [20/45] time 0.407 (0.452) data 0.276 (0.321) loss_u loss_u 0.7622 (0.7501) acc_u 34.3750 (29.6875) lr 1.1623e-04 eta 0:00:11
epoch [171/200] batch [25/45] time 0.407 (0.448) data 0.276 (0.318) loss_u loss_u 0.8271 (0.7514) acc_u 25.0000 (29.5000) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [30/45] time 0.405 (0.447) data 0.274 (0.316) loss_u loss_u 0.6943 (0.7497) acc_u 34.3750 (29.5833) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [35/45] time 0.367 (0.446) data 0.236 (0.315) loss_u loss_u 0.7393 (0.7456) acc_u 31.2500 (30.0000) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [40/45] time 0.344 (0.447) data 0.213 (0.316) loss_u loss_u 0.7036 (0.7422) acc_u 40.6250 (31.0156) lr 1.1623e-04 eta 0:00:02
epoch [171/200] batch [45/45] time 0.417 (0.444) data 0.286 (0.314) loss_u loss_u 0.7930 (0.7408) acc_u 21.8750 (31.2500) lr 1.1623e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1318
confident_label rate tensor(0.5335, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1673
clean true:1647
clean false:26
clean_rate:0.9844590555887627
noisy true:171
noisy false:1292
after delete: len(clean_dataset) 1673
after delete: len(noisy_dataset) 1463
epoch [172/200] batch [5/52] time 0.387 (0.445) data 0.257 (0.315) loss_x loss_x 0.6646 (0.9614) acc_x 84.3750 (76.2500) lr 1.0899e-04 eta 0:00:20
epoch [172/200] batch [10/52] time 0.704 (0.444) data 0.573 (0.313) loss_x loss_x 1.6543 (1.0521) acc_x 56.2500 (71.5625) lr 1.0899e-04 eta 0:00:18
epoch [172/200] batch [15/52] time 0.402 (0.452) data 0.272 (0.322) loss_x loss_x 1.2266 (1.0152) acc_x 71.8750 (73.7500) lr 1.0899e-04 eta 0:00:16
epoch [172/200] batch [20/52] time 0.460 (0.443) data 0.329 (0.312) loss_x loss_x 1.1045 (1.0396) acc_x 78.1250 (73.5938) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [25/52] time 0.505 (0.450) data 0.375 (0.320) loss_x loss_x 0.8076 (1.0410) acc_x 81.2500 (74.1250) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [30/52] time 0.382 (0.456) data 0.251 (0.325) loss_x loss_x 0.9600 (1.0168) acc_x 78.1250 (74.3750) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [35/52] time 0.443 (0.459) data 0.312 (0.328) loss_x loss_x 1.0557 (1.0330) acc_x 78.1250 (74.5536) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [40/52] time 0.389 (0.455) data 0.259 (0.324) loss_x loss_x 0.9336 (1.0247) acc_x 71.8750 (74.5312) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [45/52] time 0.467 (0.452) data 0.337 (0.322) loss_x loss_x 0.8442 (1.0136) acc_x 78.1250 (74.8611) lr 1.0899e-04 eta 0:00:03
epoch [172/200] batch [50/52] time 0.402 (0.457) data 0.271 (0.327) loss_x loss_x 0.9282 (1.0117) acc_x 75.0000 (74.6250) lr 1.0899e-04 eta 0:00:00
epoch [172/200] batch [5/45] time 0.541 (0.458) data 0.410 (0.327) loss_u loss_u 0.7026 (0.7141) acc_u 37.5000 (33.1250) lr 1.0899e-04 eta 0:00:18
epoch [172/200] batch [10/45] time 0.415 (0.453) data 0.283 (0.322) loss_u loss_u 0.7144 (0.7557) acc_u 37.5000 (28.7500) lr 1.0899e-04 eta 0:00:15
epoch [172/200] batch [15/45] time 0.403 (0.453) data 0.271 (0.322) loss_u loss_u 0.7280 (0.7537) acc_u 31.2500 (29.1667) lr 1.0899e-04 eta 0:00:13
epoch [172/200] batch [20/45] time 0.437 (0.448) data 0.306 (0.317) loss_u loss_u 0.7700 (0.7616) acc_u 28.1250 (27.8125) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [25/45] time 0.429 (0.449) data 0.298 (0.318) loss_u loss_u 0.8379 (0.7541) acc_u 18.7500 (28.7500) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [30/45] time 0.565 (0.449) data 0.435 (0.318) loss_u loss_u 0.7168 (0.7491) acc_u 31.2500 (29.4792) lr 1.0899e-04 eta 0:00:06
epoch [172/200] batch [35/45] time 0.341 (0.447) data 0.209 (0.316) loss_u loss_u 0.7588 (0.7491) acc_u 28.1250 (29.3750) lr 1.0899e-04 eta 0:00:04
epoch [172/200] batch [40/45] time 0.412 (0.445) data 0.280 (0.314) loss_u loss_u 0.7314 (0.7475) acc_u 37.5000 (29.7656) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [45/45] time 0.380 (0.443) data 0.249 (0.312) loss_u loss_u 0.7109 (0.7487) acc_u 40.6250 (30.0000) lr 1.0899e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1304
confident_label rate tensor(0.5344, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1676
clean true:1658
clean false:18
clean_rate:0.9892601431980907
noisy true:174
noisy false:1286
after delete: len(clean_dataset) 1676
after delete: len(noisy_dataset) 1460
epoch [173/200] batch [5/52] time 0.496 (0.536) data 0.366 (0.405) loss_x loss_x 0.9741 (1.1690) acc_x 75.0000 (71.2500) lr 1.0197e-04 eta 0:00:25
epoch [173/200] batch [10/52] time 0.433 (0.491) data 0.302 (0.360) loss_x loss_x 1.8174 (1.2256) acc_x 50.0000 (67.8125) lr 1.0197e-04 eta 0:00:20
epoch [173/200] batch [15/52] time 0.542 (0.496) data 0.411 (0.366) loss_x loss_x 0.6987 (1.0915) acc_x 78.1250 (71.0417) lr 1.0197e-04 eta 0:00:18
epoch [173/200] batch [20/52] time 0.440 (0.495) data 0.310 (0.365) loss_x loss_x 0.9927 (1.1079) acc_x 65.6250 (70.6250) lr 1.0197e-04 eta 0:00:15
epoch [173/200] batch [25/52] time 0.402 (0.486) data 0.271 (0.355) loss_x loss_x 0.7202 (1.0719) acc_x 84.3750 (72.3750) lr 1.0197e-04 eta 0:00:13
epoch [173/200] batch [30/52] time 0.380 (0.474) data 0.249 (0.344) loss_x loss_x 0.9136 (1.0775) acc_x 81.2500 (72.6042) lr 1.0197e-04 eta 0:00:10
epoch [173/200] batch [35/52] time 0.409 (0.467) data 0.279 (0.336) loss_x loss_x 1.2725 (1.0738) acc_x 68.7500 (73.1250) lr 1.0197e-04 eta 0:00:07
epoch [173/200] batch [40/52] time 0.427 (0.460) data 0.296 (0.330) loss_x loss_x 1.0059 (1.0854) acc_x 68.7500 (72.5781) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [45/52] time 0.398 (0.463) data 0.267 (0.332) loss_x loss_x 0.9849 (1.0940) acc_x 81.2500 (72.2917) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [50/52] time 0.404 (0.462) data 0.273 (0.331) loss_x loss_x 0.9937 (1.0932) acc_x 84.3750 (72.6250) lr 1.0197e-04 eta 0:00:00
epoch [173/200] batch [5/45] time 0.366 (0.457) data 0.234 (0.327) loss_u loss_u 0.6772 (0.7119) acc_u 53.1250 (39.3750) lr 1.0197e-04 eta 0:00:18
epoch [173/200] batch [10/45] time 0.435 (0.456) data 0.304 (0.326) loss_u loss_u 0.7061 (0.7403) acc_u 34.3750 (32.8125) lr 1.0197e-04 eta 0:00:15
epoch [173/200] batch [15/45] time 0.378 (0.451) data 0.245 (0.320) loss_u loss_u 0.8462 (0.7434) acc_u 15.6250 (31.2500) lr 1.0197e-04 eta 0:00:13
epoch [173/200] batch [20/45] time 0.374 (0.446) data 0.242 (0.316) loss_u loss_u 0.7202 (0.7348) acc_u 37.5000 (32.9688) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [25/45] time 0.386 (0.448) data 0.255 (0.317) loss_u loss_u 0.8789 (0.7473) acc_u 15.6250 (31.2500) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [30/45] time 0.397 (0.445) data 0.266 (0.315) loss_u loss_u 0.7651 (0.7478) acc_u 31.2500 (31.1458) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [35/45] time 0.397 (0.445) data 0.267 (0.314) loss_u loss_u 0.7197 (0.7450) acc_u 43.7500 (31.7857) lr 1.0197e-04 eta 0:00:04
epoch [173/200] batch [40/45] time 0.434 (0.445) data 0.302 (0.314) loss_u loss_u 0.7983 (0.7500) acc_u 28.1250 (31.0938) lr 1.0197e-04 eta 0:00:02
epoch [173/200] batch [45/45] time 0.352 (0.445) data 0.221 (0.314) loss_u loss_u 0.8149 (0.7507) acc_u 28.1250 (31.4583) lr 1.0197e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1285
confident_label rate tensor(0.5399, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1693
clean true:1672
clean false:21
clean_rate:0.9875959834613113
noisy true:179
noisy false:1264
after delete: len(clean_dataset) 1693
after delete: len(noisy_dataset) 1443
epoch [174/200] batch [5/52] time 0.383 (0.444) data 0.253 (0.314) loss_x loss_x 0.9521 (1.0657) acc_x 68.7500 (72.5000) lr 9.5173e-05 eta 0:00:20
epoch [174/200] batch [10/52] time 0.432 (0.454) data 0.302 (0.324) loss_x loss_x 1.5078 (1.1088) acc_x 56.2500 (72.1875) lr 9.5173e-05 eta 0:00:19
epoch [174/200] batch [15/52] time 0.520 (0.454) data 0.389 (0.324) loss_x loss_x 0.8379 (1.0707) acc_x 84.3750 (73.1250) lr 9.5173e-05 eta 0:00:16
epoch [174/200] batch [20/52] time 0.460 (0.444) data 0.329 (0.314) loss_x loss_x 0.8550 (1.0833) acc_x 78.1250 (72.1875) lr 9.5173e-05 eta 0:00:14
epoch [174/200] batch [25/52] time 0.361 (0.437) data 0.231 (0.307) loss_x loss_x 1.0693 (1.0692) acc_x 68.7500 (72.1250) lr 9.5173e-05 eta 0:00:11
epoch [174/200] batch [30/52] time 0.501 (0.444) data 0.371 (0.314) loss_x loss_x 1.4873 (1.1026) acc_x 62.5000 (71.7708) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [35/52] time 0.408 (0.450) data 0.278 (0.319) loss_x loss_x 1.3008 (1.1390) acc_x 78.1250 (71.3393) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [40/52] time 0.438 (0.447) data 0.308 (0.317) loss_x loss_x 0.8130 (1.1127) acc_x 78.1250 (72.0312) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [45/52] time 0.388 (0.442) data 0.257 (0.312) loss_x loss_x 1.1553 (1.0872) acc_x 68.7500 (72.3611) lr 9.5173e-05 eta 0:00:03
epoch [174/200] batch [50/52] time 0.409 (0.442) data 0.279 (0.312) loss_x loss_x 0.6240 (1.0756) acc_x 87.5000 (73.0000) lr 9.5173e-05 eta 0:00:00
epoch [174/200] batch [5/45] time 0.431 (0.444) data 0.300 (0.314) loss_u loss_u 0.7593 (0.7628) acc_u 31.2500 (28.7500) lr 9.5173e-05 eta 0:00:17
epoch [174/200] batch [10/45] time 0.462 (0.444) data 0.330 (0.314) loss_u loss_u 0.7041 (0.7365) acc_u 34.3750 (33.4375) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [15/45] time 0.463 (0.441) data 0.332 (0.311) loss_u loss_u 0.7095 (0.7349) acc_u 40.6250 (33.3333) lr 9.5173e-05 eta 0:00:13
epoch [174/200] batch [20/45] time 0.436 (0.443) data 0.305 (0.312) loss_u loss_u 0.7803 (0.7390) acc_u 37.5000 (32.8125) lr 9.5173e-05 eta 0:00:11
epoch [174/200] batch [25/45] time 0.437 (0.443) data 0.306 (0.312) loss_u loss_u 0.7573 (0.7400) acc_u 31.2500 (32.6250) lr 9.5173e-05 eta 0:00:08
epoch [174/200] batch [30/45] time 0.460 (0.441) data 0.329 (0.310) loss_u loss_u 0.7783 (0.7486) acc_u 21.8750 (31.1458) lr 9.5173e-05 eta 0:00:06
epoch [174/200] batch [35/45] time 0.547 (0.442) data 0.417 (0.311) loss_u loss_u 0.7129 (0.7543) acc_u 31.2500 (30.3571) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [40/45] time 0.416 (0.439) data 0.285 (0.308) loss_u loss_u 0.7886 (0.7546) acc_u 34.3750 (30.4688) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [45/45] time 0.461 (0.442) data 0.330 (0.311) loss_u loss_u 0.7651 (0.7573) acc_u 28.1250 (29.9306) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1272
confident_label rate tensor(0.5446, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1708
clean true:1689
clean false:19
clean_rate:0.9888758782201406
noisy true:175
noisy false:1253
after delete: len(clean_dataset) 1708
after delete: len(noisy_dataset) 1428
epoch [175/200] batch [5/53] time 0.499 (0.441) data 0.369 (0.311) loss_x loss_x 0.8281 (1.1076) acc_x 78.1250 (70.6250) lr 8.8597e-05 eta 0:00:21
epoch [175/200] batch [10/53] time 0.428 (0.439) data 0.298 (0.308) loss_x loss_x 1.3740 (1.0803) acc_x 68.7500 (70.9375) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [15/53] time 0.439 (0.450) data 0.309 (0.320) loss_x loss_x 0.7573 (1.0774) acc_x 78.1250 (72.0833) lr 8.8597e-05 eta 0:00:17
epoch [175/200] batch [20/53] time 0.545 (0.455) data 0.415 (0.324) loss_x loss_x 1.4258 (1.0588) acc_x 75.0000 (73.9062) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [25/53] time 0.377 (0.453) data 0.246 (0.322) loss_x loss_x 0.8496 (1.0807) acc_x 84.3750 (74.1250) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [30/53] time 0.407 (0.453) data 0.277 (0.323) loss_x loss_x 0.6978 (1.0545) acc_x 87.5000 (74.6875) lr 8.8597e-05 eta 0:00:10
epoch [175/200] batch [35/53] time 0.454 (0.451) data 0.324 (0.320) loss_x loss_x 0.9688 (1.0530) acc_x 78.1250 (74.6429) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [40/53] time 0.377 (0.450) data 0.248 (0.319) loss_x loss_x 1.2197 (1.0541) acc_x 71.8750 (74.3750) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [45/53] time 0.377 (0.449) data 0.247 (0.318) loss_x loss_x 0.8071 (1.0392) acc_x 81.2500 (74.7222) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [50/53] time 0.528 (0.447) data 0.398 (0.317) loss_x loss_x 1.5088 (1.0572) acc_x 65.6250 (74.4375) lr 8.8597e-05 eta 0:00:01
epoch [175/200] batch [5/44] time 0.424 (0.449) data 0.293 (0.319) loss_u loss_u 0.7803 (0.7799) acc_u 28.1250 (26.8750) lr 8.8597e-05 eta 0:00:17
epoch [175/200] batch [10/44] time 0.410 (0.444) data 0.281 (0.314) loss_u loss_u 0.8174 (0.7739) acc_u 18.7500 (27.5000) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [15/44] time 0.452 (0.445) data 0.321 (0.315) loss_u loss_u 0.8018 (0.7568) acc_u 28.1250 (30.2083) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [20/44] time 0.443 (0.441) data 0.312 (0.311) loss_u loss_u 0.7705 (0.7616) acc_u 28.1250 (29.6875) lr 8.8597e-05 eta 0:00:10
epoch [175/200] batch [25/44] time 0.402 (0.437) data 0.271 (0.307) loss_u loss_u 0.7891 (0.7612) acc_u 25.0000 (29.8750) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [30/44] time 0.414 (0.435) data 0.283 (0.304) loss_u loss_u 0.6846 (0.7486) acc_u 40.6250 (31.7708) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [35/44] time 0.384 (0.436) data 0.253 (0.305) loss_u loss_u 0.8101 (0.7456) acc_u 18.7500 (32.1429) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [40/44] time 0.384 (0.434) data 0.253 (0.304) loss_u loss_u 0.8394 (0.7535) acc_u 15.6250 (30.9375) lr 8.8597e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1301
confident_label rate tensor(0.5322, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1669
clean true:1649
clean false:20
clean_rate:0.988016776512882
noisy true:186
noisy false:1281
after delete: len(clean_dataset) 1669
after delete: len(noisy_dataset) 1467
epoch [176/200] batch [5/52] time 0.426 (0.433) data 0.296 (0.302) loss_x loss_x 0.9575 (0.8463) acc_x 78.1250 (81.2500) lr 8.2245e-05 eta 0:00:20
epoch [176/200] batch [10/52] time 0.491 (0.426) data 0.360 (0.295) loss_x loss_x 1.0557 (0.9832) acc_x 78.1250 (78.4375) lr 8.2245e-05 eta 0:00:17
epoch [176/200] batch [15/52] time 0.500 (0.429) data 0.370 (0.298) loss_x loss_x 0.8794 (1.0391) acc_x 81.2500 (75.4167) lr 8.2245e-05 eta 0:00:15
epoch [176/200] batch [20/52] time 0.465 (0.445) data 0.334 (0.314) loss_x loss_x 1.4287 (1.0617) acc_x 68.7500 (74.3750) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [25/52] time 0.496 (0.458) data 0.366 (0.327) loss_x loss_x 0.6543 (1.0514) acc_x 81.2500 (74.5000) lr 8.2245e-05 eta 0:00:12
epoch [176/200] batch [30/52] time 0.374 (0.452) data 0.243 (0.321) loss_x loss_x 0.9033 (1.0222) acc_x 75.0000 (75.1042) lr 8.2245e-05 eta 0:00:09
epoch [176/200] batch [35/52] time 0.493 (0.454) data 0.362 (0.323) loss_x loss_x 0.8169 (1.0315) acc_x 75.0000 (74.5536) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [40/52] time 0.513 (0.452) data 0.383 (0.321) loss_x loss_x 1.1689 (1.0471) acc_x 81.2500 (73.9062) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [45/52] time 0.542 (0.457) data 0.412 (0.327) loss_x loss_x 1.2314 (1.0532) acc_x 59.3750 (73.6111) lr 8.2245e-05 eta 0:00:03
epoch [176/200] batch [50/52] time 0.557 (0.460) data 0.426 (0.330) loss_x loss_x 1.2803 (1.0608) acc_x 65.6250 (73.5625) lr 8.2245e-05 eta 0:00:00
epoch [176/200] batch [5/45] time 0.329 (0.456) data 0.198 (0.326) loss_u loss_u 0.7046 (0.7441) acc_u 40.6250 (34.3750) lr 8.2245e-05 eta 0:00:18
epoch [176/200] batch [10/45] time 0.350 (0.451) data 0.219 (0.321) loss_u loss_u 0.7256 (0.7623) acc_u 28.1250 (29.3750) lr 8.2245e-05 eta 0:00:15
epoch [176/200] batch [15/45] time 0.740 (0.454) data 0.609 (0.324) loss_u loss_u 0.8442 (0.7678) acc_u 21.8750 (28.9583) lr 8.2245e-05 eta 0:00:13
epoch [176/200] batch [20/45] time 0.322 (0.450) data 0.191 (0.319) loss_u loss_u 0.7080 (0.7626) acc_u 40.6250 (30.0000) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [25/45] time 0.496 (0.448) data 0.365 (0.317) loss_u loss_u 0.7656 (0.7561) acc_u 28.1250 (30.5000) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [30/45] time 0.380 (0.445) data 0.249 (0.314) loss_u loss_u 0.7876 (0.7586) acc_u 25.0000 (29.8958) lr 8.2245e-05 eta 0:00:06
epoch [176/200] batch [35/45] time 0.440 (0.445) data 0.308 (0.314) loss_u loss_u 0.6841 (0.7581) acc_u 37.5000 (30.0893) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [40/45] time 0.397 (0.442) data 0.266 (0.311) loss_u loss_u 0.7427 (0.7592) acc_u 34.3750 (29.9219) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [45/45] time 0.373 (0.440) data 0.242 (0.309) loss_u loss_u 0.7329 (0.7554) acc_u 37.5000 (30.7639) lr 8.2245e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1296
confident_label rate tensor(0.5367, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1683
clean true:1662
clean false:21
clean_rate:0.9875222816399287
noisy true:178
noisy false:1275
after delete: len(clean_dataset) 1683
after delete: len(noisy_dataset) 1453
epoch [177/200] batch [5/52] time 0.385 (0.495) data 0.255 (0.365) loss_x loss_x 1.1328 (1.0014) acc_x 65.6250 (70.6250) lr 7.6120e-05 eta 0:00:23
epoch [177/200] batch [10/52] time 0.375 (0.482) data 0.245 (0.351) loss_x loss_x 1.6006 (0.9973) acc_x 71.8750 (73.4375) lr 7.6120e-05 eta 0:00:20
epoch [177/200] batch [15/52] time 0.451 (0.456) data 0.321 (0.326) loss_x loss_x 0.9468 (1.0666) acc_x 84.3750 (72.9167) lr 7.6120e-05 eta 0:00:16
epoch [177/200] batch [20/52] time 0.371 (0.440) data 0.240 (0.309) loss_x loss_x 0.7837 (1.1002) acc_x 84.3750 (72.5000) lr 7.6120e-05 eta 0:00:14
epoch [177/200] batch [25/52] time 0.349 (0.442) data 0.219 (0.312) loss_x loss_x 0.9414 (1.0783) acc_x 75.0000 (73.0000) lr 7.6120e-05 eta 0:00:11
epoch [177/200] batch [30/52] time 0.455 (0.438) data 0.324 (0.308) loss_x loss_x 1.1367 (1.0680) acc_x 75.0000 (73.6458) lr 7.6120e-05 eta 0:00:09
epoch [177/200] batch [35/52] time 0.395 (0.445) data 0.265 (0.314) loss_x loss_x 1.2295 (1.0996) acc_x 59.3750 (73.2143) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [40/52] time 0.590 (0.446) data 0.459 (0.316) loss_x loss_x 0.9312 (1.0973) acc_x 62.5000 (72.4219) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [45/52] time 0.529 (0.445) data 0.399 (0.315) loss_x loss_x 0.9526 (1.0917) acc_x 75.0000 (72.5000) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [50/52] time 0.449 (0.446) data 0.318 (0.316) loss_x loss_x 1.0996 (1.0868) acc_x 71.8750 (72.6875) lr 7.6120e-05 eta 0:00:00
epoch [177/200] batch [5/45] time 0.496 (0.448) data 0.365 (0.317) loss_u loss_u 0.7075 (0.7542) acc_u 40.6250 (33.7500) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [10/45] time 0.387 (0.445) data 0.256 (0.315) loss_u loss_u 0.6392 (0.7390) acc_u 43.7500 (33.7500) lr 7.6120e-05 eta 0:00:15
epoch [177/200] batch [15/45] time 0.474 (0.445) data 0.342 (0.315) loss_u loss_u 0.7827 (0.7447) acc_u 28.1250 (32.5000) lr 7.6120e-05 eta 0:00:13
epoch [177/200] batch [20/45] time 0.375 (0.443) data 0.244 (0.313) loss_u loss_u 0.7412 (0.7525) acc_u 31.2500 (31.4062) lr 7.6120e-05 eta 0:00:11
epoch [177/200] batch [25/45] time 0.429 (0.445) data 0.298 (0.315) loss_u loss_u 0.7676 (0.7596) acc_u 31.2500 (30.5000) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [30/45] time 0.419 (0.443) data 0.287 (0.312) loss_u loss_u 0.7842 (0.7607) acc_u 31.2500 (31.0417) lr 7.6120e-05 eta 0:00:06
epoch [177/200] batch [35/45] time 0.312 (0.439) data 0.180 (0.308) loss_u loss_u 0.7222 (0.7582) acc_u 37.5000 (31.0714) lr 7.6120e-05 eta 0:00:04
epoch [177/200] batch [40/45] time 0.426 (0.436) data 0.295 (0.305) loss_u loss_u 0.7827 (0.7565) acc_u 25.0000 (31.1719) lr 7.6120e-05 eta 0:00:02
epoch [177/200] batch [45/45] time 0.495 (0.435) data 0.364 (0.305) loss_u loss_u 0.7734 (0.7578) acc_u 21.8750 (30.5556) lr 7.6120e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1237
confident_label rate tensor(0.5539, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1737
clean true:1718
clean false:19
clean_rate:0.9890616004605642
noisy true:181
noisy false:1218
after delete: len(clean_dataset) 1737
after delete: len(noisy_dataset) 1399
epoch [178/200] batch [5/54] time 0.383 (0.412) data 0.253 (0.282) loss_x loss_x 1.4160 (1.1744) acc_x 71.8750 (70.6250) lr 7.0224e-05 eta 0:00:20
epoch [178/200] batch [10/54] time 0.412 (0.417) data 0.281 (0.287) loss_x loss_x 0.5532 (1.1009) acc_x 84.3750 (72.8125) lr 7.0224e-05 eta 0:00:18
epoch [178/200] batch [15/54] time 0.521 (0.434) data 0.391 (0.304) loss_x loss_x 0.7671 (1.0624) acc_x 78.1250 (73.7500) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [20/54] time 0.500 (0.433) data 0.369 (0.303) loss_x loss_x 1.4473 (1.0768) acc_x 71.8750 (73.5938) lr 7.0224e-05 eta 0:00:14
epoch [178/200] batch [25/54] time 0.510 (0.443) data 0.380 (0.313) loss_x loss_x 0.9531 (1.0925) acc_x 68.7500 (73.5000) lr 7.0224e-05 eta 0:00:12
epoch [178/200] batch [30/54] time 0.362 (0.441) data 0.232 (0.310) loss_x loss_x 1.1201 (1.0616) acc_x 65.6250 (74.3750) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [35/54] time 0.513 (0.448) data 0.383 (0.318) loss_x loss_x 1.0430 (1.0392) acc_x 68.7500 (74.3750) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [40/54] time 0.430 (0.442) data 0.300 (0.312) loss_x loss_x 0.9888 (1.0371) acc_x 75.0000 (74.9219) lr 7.0224e-05 eta 0:00:06
epoch [178/200] batch [45/54] time 0.422 (0.441) data 0.292 (0.311) loss_x loss_x 0.8232 (1.0607) acc_x 78.1250 (73.8889) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [50/54] time 0.395 (0.442) data 0.265 (0.312) loss_x loss_x 1.1836 (1.0764) acc_x 68.7500 (73.6250) lr 7.0224e-05 eta 0:00:01
epoch [178/200] batch [5/43] time 0.394 (0.442) data 0.263 (0.312) loss_u loss_u 0.7876 (0.8033) acc_u 31.2500 (25.0000) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [10/43] time 0.461 (0.443) data 0.330 (0.313) loss_u loss_u 0.6978 (0.7736) acc_u 43.7500 (29.6875) lr 7.0224e-05 eta 0:00:14
epoch [178/200] batch [15/43] time 0.606 (0.446) data 0.474 (0.315) loss_u loss_u 0.6733 (0.7514) acc_u 43.7500 (31.6667) lr 7.0224e-05 eta 0:00:12
epoch [178/200] batch [20/43] time 0.395 (0.443) data 0.264 (0.313) loss_u loss_u 0.6597 (0.7458) acc_u 43.7500 (31.7188) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [25/43] time 0.336 (0.438) data 0.205 (0.308) loss_u loss_u 0.8076 (0.7512) acc_u 21.8750 (31.0000) lr 7.0224e-05 eta 0:00:07
epoch [178/200] batch [30/43] time 0.431 (0.437) data 0.300 (0.307) loss_u loss_u 0.6416 (0.7505) acc_u 40.6250 (30.8333) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [35/43] time 0.431 (0.437) data 0.300 (0.306) loss_u loss_u 0.8032 (0.7535) acc_u 25.0000 (30.2679) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [40/43] time 0.435 (0.436) data 0.304 (0.305) loss_u loss_u 0.7139 (0.7527) acc_u 37.5000 (30.5469) lr 7.0224e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1256
confident_label rate tensor(0.5466, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1714
clean true:1692
clean false:22
clean_rate:0.9871645274212368
noisy true:188
noisy false:1234
after delete: len(clean_dataset) 1714
after delete: len(noisy_dataset) 1422
epoch [179/200] batch [5/53] time 0.497 (0.445) data 0.367 (0.314) loss_x loss_x 0.9258 (1.2826) acc_x 75.0000 (68.1250) lr 6.4556e-05 eta 0:00:21
epoch [179/200] batch [10/53] time 0.500 (0.468) data 0.369 (0.338) loss_x loss_x 0.8770 (1.1183) acc_x 78.1250 (70.9375) lr 6.4556e-05 eta 0:00:20
epoch [179/200] batch [15/53] time 0.566 (0.460) data 0.435 (0.330) loss_x loss_x 1.0039 (1.1000) acc_x 68.7500 (71.2500) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [20/53] time 0.396 (0.455) data 0.265 (0.325) loss_x loss_x 1.5703 (1.1103) acc_x 65.6250 (72.1875) lr 6.4556e-05 eta 0:00:15
epoch [179/200] batch [25/53] time 0.457 (0.449) data 0.327 (0.319) loss_x loss_x 1.0293 (1.0894) acc_x 68.7500 (72.5000) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [30/53] time 0.464 (0.447) data 0.333 (0.317) loss_x loss_x 1.0557 (1.0809) acc_x 78.1250 (72.8125) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [35/53] time 0.474 (0.449) data 0.344 (0.318) loss_x loss_x 1.0332 (1.0744) acc_x 68.7500 (72.5893) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [40/53] time 0.478 (0.451) data 0.347 (0.321) loss_x loss_x 1.2920 (1.0607) acc_x 68.7500 (73.1250) lr 6.4556e-05 eta 0:00:05
epoch [179/200] batch [45/53] time 0.361 (0.456) data 0.231 (0.326) loss_x loss_x 0.7822 (1.0513) acc_x 81.2500 (73.3333) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [50/53] time 0.440 (0.455) data 0.309 (0.325) loss_x loss_x 1.1309 (1.0648) acc_x 78.1250 (72.8750) lr 6.4556e-05 eta 0:00:01
epoch [179/200] batch [5/44] time 0.551 (0.452) data 0.420 (0.321) loss_u loss_u 0.7896 (0.7187) acc_u 25.0000 (34.3750) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [10/44] time 0.493 (0.448) data 0.361 (0.318) loss_u loss_u 0.6982 (0.7137) acc_u 34.3750 (34.3750) lr 6.4556e-05 eta 0:00:15
epoch [179/200] batch [15/44] time 0.459 (0.446) data 0.328 (0.316) loss_u loss_u 0.7090 (0.7269) acc_u 31.2500 (33.3333) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [20/44] time 0.384 (0.442) data 0.253 (0.312) loss_u loss_u 0.7373 (0.7305) acc_u 31.2500 (33.1250) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [25/44] time 0.508 (0.441) data 0.377 (0.310) loss_u loss_u 0.7456 (0.7300) acc_u 31.2500 (33.0000) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [30/44] time 0.376 (0.442) data 0.245 (0.311) loss_u loss_u 0.7690 (0.7270) acc_u 28.1250 (33.3333) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [35/44] time 0.373 (0.442) data 0.241 (0.312) loss_u loss_u 0.7778 (0.7310) acc_u 28.1250 (33.1250) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [40/44] time 0.410 (0.440) data 0.279 (0.309) loss_u loss_u 0.8032 (0.7366) acc_u 21.8750 (32.4219) lr 6.4556e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1261
confident_label rate tensor(0.5510, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1728
clean true:1700
clean false:28
clean_rate:0.9837962962962963
noisy true:175
noisy false:1233
after delete: len(clean_dataset) 1728
after delete: len(noisy_dataset) 1408
epoch [180/200] batch [5/54] time 0.516 (0.447) data 0.385 (0.316) loss_x loss_x 1.3145 (1.1819) acc_x 68.7500 (69.3750) lr 5.9119e-05 eta 0:00:21
epoch [180/200] batch [10/54] time 0.468 (0.457) data 0.338 (0.326) loss_x loss_x 1.3799 (1.0862) acc_x 56.2500 (71.2500) lr 5.9119e-05 eta 0:00:20
epoch [180/200] batch [15/54] time 0.457 (0.443) data 0.326 (0.313) loss_x loss_x 0.7690 (1.1251) acc_x 81.2500 (71.0417) lr 5.9119e-05 eta 0:00:17
epoch [180/200] batch [20/54] time 0.396 (0.439) data 0.265 (0.309) loss_x loss_x 0.9038 (1.1038) acc_x 81.2500 (72.5000) lr 5.9119e-05 eta 0:00:14
epoch [180/200] batch [25/54] time 0.519 (0.440) data 0.388 (0.310) loss_x loss_x 0.8286 (1.1162) acc_x 78.1250 (71.8750) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [30/54] time 0.424 (0.452) data 0.294 (0.322) loss_x loss_x 0.8735 (1.0988) acc_x 78.1250 (72.2917) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [35/54] time 0.370 (0.449) data 0.239 (0.319) loss_x loss_x 0.9141 (1.0959) acc_x 81.2500 (72.5000) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [40/54] time 0.430 (0.452) data 0.299 (0.321) loss_x loss_x 1.4170 (1.0993) acc_x 56.2500 (72.2656) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [45/54] time 0.336 (0.446) data 0.206 (0.316) loss_x loss_x 0.9902 (1.0959) acc_x 81.2500 (72.6389) lr 5.9119e-05 eta 0:00:04
epoch [180/200] batch [50/54] time 0.458 (0.449) data 0.327 (0.318) loss_x loss_x 1.0957 (1.0787) acc_x 75.0000 (73.1250) lr 5.9119e-05 eta 0:00:01
epoch [180/200] batch [5/44] time 0.410 (0.449) data 0.279 (0.318) loss_u loss_u 0.8057 (0.7718) acc_u 25.0000 (28.7500) lr 5.9119e-05 eta 0:00:17
epoch [180/200] batch [10/44] time 0.593 (0.450) data 0.462 (0.320) loss_u loss_u 0.6255 (0.7525) acc_u 46.8750 (30.3125) lr 5.9119e-05 eta 0:00:15
epoch [180/200] batch [15/44] time 0.512 (0.449) data 0.382 (0.318) loss_u loss_u 0.7051 (0.7601) acc_u 37.5000 (28.7500) lr 5.9119e-05 eta 0:00:13
epoch [180/200] batch [20/44] time 0.362 (0.446) data 0.231 (0.315) loss_u loss_u 0.7085 (0.7651) acc_u 31.2500 (27.8125) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [25/44] time 0.352 (0.442) data 0.221 (0.312) loss_u loss_u 0.8901 (0.7626) acc_u 15.6250 (27.6250) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [30/44] time 0.401 (0.441) data 0.270 (0.310) loss_u loss_u 0.8335 (0.7658) acc_u 21.8750 (27.7083) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [35/44] time 0.492 (0.440) data 0.361 (0.309) loss_u loss_u 0.7490 (0.7628) acc_u 28.1250 (28.5714) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [40/44] time 0.755 (0.443) data 0.624 (0.312) loss_u loss_u 0.8516 (0.7652) acc_u 12.5000 (28.2812) lr 5.9119e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1301
confident_label rate tensor(0.5300, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1662
clean true:1652
clean false:10
clean_rate:0.9939831528279182
noisy true:183
noisy false:1291
after delete: len(clean_dataset) 1662
after delete: len(noisy_dataset) 1474
epoch [181/200] batch [5/51] time 0.435 (0.487) data 0.304 (0.355) loss_x loss_x 1.1748 (1.3816) acc_x 81.2500 (68.7500) lr 5.3915e-05 eta 0:00:22
epoch [181/200] batch [10/51] time 0.540 (0.485) data 0.410 (0.354) loss_x loss_x 1.3574 (1.3195) acc_x 62.5000 (70.6250) lr 5.3915e-05 eta 0:00:19
epoch [181/200] batch [15/51] time 0.476 (0.468) data 0.345 (0.337) loss_x loss_x 0.8848 (1.2046) acc_x 84.3750 (72.2917) lr 5.3915e-05 eta 0:00:16
epoch [181/200] batch [20/51] time 0.387 (0.451) data 0.256 (0.320) loss_x loss_x 1.4688 (1.2350) acc_x 59.3750 (69.2188) lr 5.3915e-05 eta 0:00:13
epoch [181/200] batch [25/51] time 0.615 (0.463) data 0.485 (0.332) loss_x loss_x 0.7983 (1.1911) acc_x 71.8750 (70.2500) lr 5.3915e-05 eta 0:00:12
epoch [181/200] batch [30/51] time 0.418 (0.467) data 0.287 (0.336) loss_x loss_x 1.0303 (1.1311) acc_x 71.8750 (71.6667) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [35/51] time 0.426 (0.468) data 0.296 (0.337) loss_x loss_x 1.1494 (1.1147) acc_x 71.8750 (72.2321) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [40/51] time 0.442 (0.463) data 0.311 (0.332) loss_x loss_x 1.1875 (1.1202) acc_x 78.1250 (72.1875) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [45/51] time 0.432 (0.460) data 0.302 (0.330) loss_x loss_x 0.6821 (1.0933) acc_x 87.5000 (72.5000) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [50/51] time 0.389 (0.458) data 0.259 (0.328) loss_x loss_x 1.0322 (1.0843) acc_x 81.2500 (72.8125) lr 5.3915e-05 eta 0:00:00
epoch [181/200] batch [5/46] time 0.437 (0.452) data 0.306 (0.321) loss_u loss_u 0.6484 (0.7316) acc_u 43.7500 (33.1250) lr 5.3915e-05 eta 0:00:18
epoch [181/200] batch [10/46] time 0.431 (0.453) data 0.300 (0.322) loss_u loss_u 0.6719 (0.7064) acc_u 46.8750 (36.8750) lr 5.3915e-05 eta 0:00:16
epoch [181/200] batch [15/46] time 0.364 (0.448) data 0.233 (0.317) loss_u loss_u 0.7261 (0.7114) acc_u 37.5000 (37.0833) lr 5.3915e-05 eta 0:00:13
epoch [181/200] batch [20/46] time 0.527 (0.449) data 0.396 (0.318) loss_u loss_u 0.7188 (0.7093) acc_u 31.2500 (36.8750) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [25/46] time 0.419 (0.445) data 0.290 (0.314) loss_u loss_u 0.7144 (0.7079) acc_u 34.3750 (37.5000) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [30/46] time 0.397 (0.444) data 0.267 (0.314) loss_u loss_u 0.6636 (0.7127) acc_u 34.3750 (36.4583) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [35/46] time 0.490 (0.441) data 0.359 (0.311) loss_u loss_u 0.7559 (0.7179) acc_u 28.1250 (35.6250) lr 5.3915e-05 eta 0:00:04
epoch [181/200] batch [40/46] time 0.401 (0.440) data 0.270 (0.310) loss_u loss_u 0.7803 (0.7215) acc_u 31.2500 (35.6250) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [45/46] time 0.335 (0.439) data 0.204 (0.308) loss_u loss_u 0.7344 (0.7244) acc_u 28.1250 (34.8611) lr 5.3915e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1285
confident_label rate tensor(0.5379, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1687
clean true:1666
clean false:21
clean_rate:0.9875518672199171
noisy true:185
noisy false:1264
after delete: len(clean_dataset) 1687
after delete: len(noisy_dataset) 1449
epoch [182/200] batch [5/52] time 0.408 (0.463) data 0.277 (0.333) loss_x loss_x 1.0186 (1.0931) acc_x 84.3750 (76.2500) lr 4.8943e-05 eta 0:00:21
epoch [182/200] batch [10/52] time 0.359 (0.441) data 0.229 (0.311) loss_x loss_x 1.1631 (1.0792) acc_x 68.7500 (75.0000) lr 4.8943e-05 eta 0:00:18
epoch [182/200] batch [15/52] time 0.499 (0.445) data 0.369 (0.314) loss_x loss_x 1.0332 (1.0509) acc_x 68.7500 (75.4167) lr 4.8943e-05 eta 0:00:16
epoch [182/200] batch [20/52] time 0.459 (0.436) data 0.329 (0.305) loss_x loss_x 1.0488 (1.0291) acc_x 75.0000 (74.8438) lr 4.8943e-05 eta 0:00:13
epoch [182/200] batch [25/52] time 0.425 (0.434) data 0.294 (0.304) loss_x loss_x 1.0977 (1.0294) acc_x 75.0000 (74.8750) lr 4.8943e-05 eta 0:00:11
epoch [182/200] batch [30/52] time 0.481 (0.441) data 0.350 (0.311) loss_x loss_x 0.9199 (1.0009) acc_x 75.0000 (75.5208) lr 4.8943e-05 eta 0:00:09
epoch [182/200] batch [35/52] time 0.708 (0.449) data 0.577 (0.319) loss_x loss_x 0.8496 (1.0066) acc_x 78.1250 (75.8036) lr 4.8943e-05 eta 0:00:07
epoch [182/200] batch [40/52] time 0.357 (0.454) data 0.226 (0.324) loss_x loss_x 1.3232 (1.0264) acc_x 68.7500 (75.3125) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [45/52] time 0.494 (0.458) data 0.364 (0.328) loss_x loss_x 1.3418 (1.0335) acc_x 59.3750 (74.5833) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [50/52] time 0.453 (0.456) data 0.323 (0.326) loss_x loss_x 1.0479 (1.0534) acc_x 71.8750 (73.9375) lr 4.8943e-05 eta 0:00:00
epoch [182/200] batch [5/45] time 0.539 (0.453) data 0.408 (0.322) loss_u loss_u 0.6592 (0.7143) acc_u 40.6250 (33.7500) lr 4.8943e-05 eta 0:00:18
epoch [182/200] batch [10/45] time 0.409 (0.449) data 0.278 (0.318) loss_u loss_u 0.6680 (0.7229) acc_u 40.6250 (34.6875) lr 4.8943e-05 eta 0:00:15
epoch [182/200] batch [15/45] time 0.410 (0.445) data 0.279 (0.315) loss_u loss_u 0.7554 (0.7368) acc_u 28.1250 (32.0833) lr 4.8943e-05 eta 0:00:13
epoch [182/200] batch [20/45] time 0.342 (0.448) data 0.210 (0.317) loss_u loss_u 0.8350 (0.7511) acc_u 12.5000 (30.7812) lr 4.8943e-05 eta 0:00:11
epoch [182/200] batch [25/45] time 0.473 (0.445) data 0.342 (0.315) loss_u loss_u 0.8271 (0.7529) acc_u 18.7500 (29.8750) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [30/45] time 0.420 (0.445) data 0.289 (0.314) loss_u loss_u 0.6855 (0.7514) acc_u 31.2500 (30.5208) lr 4.8943e-05 eta 0:00:06
epoch [182/200] batch [35/45] time 0.437 (0.443) data 0.306 (0.313) loss_u loss_u 0.8115 (0.7550) acc_u 21.8750 (30.2679) lr 4.8943e-05 eta 0:00:04
epoch [182/200] batch [40/45] time 0.447 (0.441) data 0.316 (0.311) loss_u loss_u 0.7373 (0.7525) acc_u 34.3750 (30.8594) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [45/45] time 0.426 (0.440) data 0.294 (0.309) loss_u loss_u 0.7583 (0.7523) acc_u 28.1250 (30.7639) lr 4.8943e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1314
confident_label rate tensor(0.5322, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1669
clean true:1642
clean false:27
clean_rate:0.9838226482923906
noisy true:180
noisy false:1287
after delete: len(clean_dataset) 1669
after delete: len(noisy_dataset) 1467
epoch [183/200] batch [5/52] time 0.433 (0.432) data 0.302 (0.300) loss_x loss_x 1.0986 (1.0678) acc_x 68.7500 (71.2500) lr 4.4207e-05 eta 0:00:20
epoch [183/200] batch [10/52] time 0.440 (0.469) data 0.310 (0.338) loss_x loss_x 0.7427 (1.0505) acc_x 84.3750 (71.5625) lr 4.4207e-05 eta 0:00:19
epoch [183/200] batch [15/52] time 0.420 (0.470) data 0.290 (0.339) loss_x loss_x 1.4844 (1.0685) acc_x 56.2500 (72.2917) lr 4.4207e-05 eta 0:00:17
epoch [183/200] batch [20/52] time 0.520 (0.473) data 0.390 (0.342) loss_x loss_x 1.2451 (1.0652) acc_x 65.6250 (72.0312) lr 4.4207e-05 eta 0:00:15
epoch [183/200] batch [25/52] time 0.349 (0.460) data 0.219 (0.330) loss_x loss_x 0.8081 (1.0480) acc_x 84.3750 (72.8750) lr 4.4207e-05 eta 0:00:12
epoch [183/200] batch [30/52] time 0.388 (0.446) data 0.258 (0.316) loss_x loss_x 0.9751 (1.0510) acc_x 75.0000 (73.0208) lr 4.4207e-05 eta 0:00:09
epoch [183/200] batch [35/52] time 0.452 (0.443) data 0.321 (0.313) loss_x loss_x 1.0146 (1.0367) acc_x 71.8750 (73.1250) lr 4.4207e-05 eta 0:00:07
epoch [183/200] batch [40/52] time 0.412 (0.447) data 0.281 (0.317) loss_x loss_x 1.3730 (1.0368) acc_x 68.7500 (72.9688) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [45/52] time 0.445 (0.453) data 0.314 (0.323) loss_x loss_x 0.7891 (1.0402) acc_x 75.0000 (73.0556) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [50/52] time 0.448 (0.448) data 0.317 (0.318) loss_x loss_x 0.9272 (1.0463) acc_x 71.8750 (72.8125) lr 4.4207e-05 eta 0:00:00
epoch [183/200] batch [5/45] time 0.346 (0.449) data 0.215 (0.318) loss_u loss_u 0.6421 (0.7273) acc_u 40.6250 (32.5000) lr 4.4207e-05 eta 0:00:17
epoch [183/200] batch [10/45] time 0.479 (0.448) data 0.348 (0.317) loss_u loss_u 0.7881 (0.7172) acc_u 28.1250 (34.0625) lr 4.4207e-05 eta 0:00:15
epoch [183/200] batch [15/45] time 0.393 (0.445) data 0.261 (0.314) loss_u loss_u 0.6318 (0.7084) acc_u 53.1250 (36.0417) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [20/45] time 0.408 (0.442) data 0.277 (0.311) loss_u loss_u 0.6616 (0.7123) acc_u 34.3750 (35.4688) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [25/45] time 0.477 (0.440) data 0.346 (0.309) loss_u loss_u 0.6650 (0.7182) acc_u 40.6250 (35.0000) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [30/45] time 0.479 (0.439) data 0.348 (0.308) loss_u loss_u 0.8218 (0.7287) acc_u 18.7500 (33.3333) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [35/45] time 0.338 (0.441) data 0.207 (0.310) loss_u loss_u 0.7534 (0.7340) acc_u 34.3750 (32.5893) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [40/45] time 0.556 (0.442) data 0.425 (0.311) loss_u loss_u 0.6924 (0.7313) acc_u 37.5000 (32.9688) lr 4.4207e-05 eta 0:00:02
epoch [183/200] batch [45/45] time 0.397 (0.439) data 0.266 (0.308) loss_u loss_u 0.7588 (0.7349) acc_u 34.3750 (32.6389) lr 4.4207e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1277
confident_label rate tensor(0.5399, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1693
clean true:1677
clean false:16
clean_rate:0.9905493207324276
noisy true:182
noisy false:1261
after delete: len(clean_dataset) 1693
after delete: len(noisy_dataset) 1443
epoch [184/200] batch [5/52] time 0.380 (0.380) data 0.249 (0.250) loss_x loss_x 1.2793 (1.1019) acc_x 62.5000 (73.7500) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [10/52] time 0.430 (0.415) data 0.300 (0.285) loss_x loss_x 1.0781 (1.0035) acc_x 78.1250 (75.9375) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [15/52] time 0.442 (0.436) data 0.312 (0.306) loss_x loss_x 0.6162 (0.9972) acc_x 81.2500 (75.4167) lr 3.9706e-05 eta 0:00:16
epoch [184/200] batch [20/52] time 0.430 (0.441) data 0.300 (0.311) loss_x loss_x 1.1270 (1.0144) acc_x 78.1250 (74.8438) lr 3.9706e-05 eta 0:00:14
epoch [184/200] batch [25/52] time 0.599 (0.441) data 0.469 (0.310) loss_x loss_x 0.8931 (1.0042) acc_x 81.2500 (75.5000) lr 3.9706e-05 eta 0:00:11
epoch [184/200] batch [30/52] time 0.439 (0.448) data 0.308 (0.317) loss_x loss_x 0.7710 (1.0274) acc_x 75.0000 (75.0000) lr 3.9706e-05 eta 0:00:09
epoch [184/200] batch [35/52] time 0.428 (0.450) data 0.298 (0.319) loss_x loss_x 0.8975 (1.0328) acc_x 71.8750 (74.8214) lr 3.9706e-05 eta 0:00:07
epoch [184/200] batch [40/52] time 0.461 (0.449) data 0.331 (0.319) loss_x loss_x 0.9937 (1.0525) acc_x 71.8750 (74.3750) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [45/52] time 0.476 (0.450) data 0.346 (0.320) loss_x loss_x 1.8740 (1.0921) acc_x 53.1250 (73.4028) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [50/52] time 0.452 (0.455) data 0.321 (0.325) loss_x loss_x 1.2666 (1.0850) acc_x 65.6250 (73.3750) lr 3.9706e-05 eta 0:00:00
epoch [184/200] batch [5/45] time 0.426 (0.449) data 0.295 (0.319) loss_u loss_u 0.8018 (0.7424) acc_u 21.8750 (30.6250) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [10/45] time 0.414 (0.447) data 0.283 (0.317) loss_u loss_u 0.8242 (0.7453) acc_u 21.8750 (30.6250) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [15/45] time 0.369 (0.445) data 0.238 (0.314) loss_u loss_u 0.6724 (0.7396) acc_u 43.7500 (31.6667) lr 3.9706e-05 eta 0:00:13
epoch [184/200] batch [20/45] time 0.394 (0.443) data 0.262 (0.312) loss_u loss_u 0.8643 (0.7482) acc_u 15.6250 (30.3125) lr 3.9706e-05 eta 0:00:11
epoch [184/200] batch [25/45] time 0.321 (0.441) data 0.191 (0.310) loss_u loss_u 0.7383 (0.7479) acc_u 34.3750 (30.1250) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [30/45] time 0.588 (0.443) data 0.456 (0.312) loss_u loss_u 0.8062 (0.7490) acc_u 31.2500 (30.8333) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [35/45] time 0.377 (0.442) data 0.244 (0.311) loss_u loss_u 0.9102 (0.7442) acc_u 3.1250 (31.4286) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [40/45] time 0.558 (0.444) data 0.427 (0.313) loss_u loss_u 0.5483 (0.7427) acc_u 59.3750 (31.8750) lr 3.9706e-05 eta 0:00:02
epoch [184/200] batch [45/45] time 0.498 (0.443) data 0.367 (0.312) loss_u loss_u 0.8193 (0.7433) acc_u 25.0000 (31.6667) lr 3.9706e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1288
confident_label rate tensor(0.5383, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1688
clean true:1668
clean false:20
clean_rate:0.9881516587677726
noisy true:180
noisy false:1268
after delete: len(clean_dataset) 1688
after delete: len(noisy_dataset) 1448
epoch [185/200] batch [5/52] time 0.433 (0.432) data 0.303 (0.301) loss_x loss_x 1.1650 (1.0970) acc_x 68.7500 (71.2500) lr 3.5443e-05 eta 0:00:20
epoch [185/200] batch [10/52] time 0.517 (0.448) data 0.386 (0.318) loss_x loss_x 1.2305 (1.1105) acc_x 71.8750 (72.1875) lr 3.5443e-05 eta 0:00:18
epoch [185/200] batch [15/52] time 0.445 (0.448) data 0.315 (0.317) loss_x loss_x 0.7231 (1.0612) acc_x 87.5000 (71.6667) lr 3.5443e-05 eta 0:00:16
epoch [185/200] batch [20/52] time 0.531 (0.447) data 0.401 (0.316) loss_x loss_x 0.7354 (1.0464) acc_x 81.2500 (72.5000) lr 3.5443e-05 eta 0:00:14
epoch [185/200] batch [25/52] time 0.502 (0.450) data 0.372 (0.319) loss_x loss_x 0.8091 (1.0580) acc_x 87.5000 (72.5000) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [30/52] time 0.410 (0.443) data 0.279 (0.313) loss_x loss_x 0.7573 (1.0794) acc_x 75.0000 (71.6667) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [35/52] time 0.441 (0.446) data 0.311 (0.315) loss_x loss_x 1.1807 (1.0799) acc_x 62.5000 (71.7857) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [40/52] time 0.507 (0.446) data 0.377 (0.315) loss_x loss_x 1.2197 (1.0725) acc_x 59.3750 (71.9531) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [45/52] time 0.577 (0.444) data 0.448 (0.313) loss_x loss_x 1.0977 (1.0786) acc_x 71.8750 (71.7361) lr 3.5443e-05 eta 0:00:03
epoch [185/200] batch [50/52] time 0.457 (0.444) data 0.327 (0.314) loss_x loss_x 1.2441 (1.0921) acc_x 65.6250 (71.4375) lr 3.5443e-05 eta 0:00:00
epoch [185/200] batch [5/45] time 0.435 (0.445) data 0.304 (0.315) loss_u loss_u 0.8208 (0.7776) acc_u 21.8750 (25.0000) lr 3.5443e-05 eta 0:00:17
epoch [185/200] batch [10/45] time 0.365 (0.442) data 0.234 (0.311) loss_u loss_u 0.7676 (0.7719) acc_u 31.2500 (27.1875) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [15/45] time 0.445 (0.441) data 0.314 (0.310) loss_u loss_u 0.7891 (0.7705) acc_u 31.2500 (27.7083) lr 3.5443e-05 eta 0:00:13
epoch [185/200] batch [20/45] time 0.479 (0.439) data 0.347 (0.309) loss_u loss_u 0.5815 (0.7520) acc_u 50.0000 (30.3125) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [25/45] time 0.409 (0.441) data 0.279 (0.310) loss_u loss_u 0.7451 (0.7554) acc_u 28.1250 (29.8750) lr 3.5443e-05 eta 0:00:08
epoch [185/200] batch [30/45] time 0.354 (0.440) data 0.223 (0.309) loss_u loss_u 0.7246 (0.7556) acc_u 31.2500 (29.8958) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [35/45] time 0.374 (0.440) data 0.243 (0.309) loss_u loss_u 0.8330 (0.7613) acc_u 21.8750 (29.2857) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [40/45] time 0.491 (0.439) data 0.360 (0.308) loss_u loss_u 0.7534 (0.7595) acc_u 25.0000 (29.6875) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [45/45] time 0.344 (0.437) data 0.213 (0.306) loss_u loss_u 0.7539 (0.7520) acc_u 37.5000 (30.9028) lr 3.5443e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1272
confident_label rate tensor(0.5437, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1705
clean true:1682
clean false:23
clean_rate:0.9865102639296187
noisy true:182
noisy false:1249
after delete: len(clean_dataset) 1705
after delete: len(noisy_dataset) 1431
epoch [186/200] batch [5/53] time 0.409 (0.459) data 0.278 (0.329) loss_x loss_x 0.8579 (1.0276) acc_x 78.1250 (75.0000) lr 3.1417e-05 eta 0:00:22
epoch [186/200] batch [10/53] time 0.373 (0.438) data 0.242 (0.307) loss_x loss_x 0.9780 (0.9360) acc_x 78.1250 (76.5625) lr 3.1417e-05 eta 0:00:18
epoch [186/200] batch [15/53] time 0.456 (0.454) data 0.325 (0.324) loss_x loss_x 0.8477 (0.9986) acc_x 75.0000 (75.4167) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [20/53] time 0.534 (0.451) data 0.403 (0.320) loss_x loss_x 1.1650 (1.0062) acc_x 71.8750 (74.8438) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [25/53] time 0.404 (0.448) data 0.274 (0.317) loss_x loss_x 0.5820 (0.9735) acc_x 87.5000 (75.0000) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [30/53] time 0.494 (0.452) data 0.364 (0.322) loss_x loss_x 1.5430 (1.0000) acc_x 62.5000 (74.7917) lr 3.1417e-05 eta 0:00:10
epoch [186/200] batch [35/53] time 0.454 (0.453) data 0.324 (0.323) loss_x loss_x 1.2109 (1.0004) acc_x 65.6250 (73.9286) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [40/53] time 0.577 (0.455) data 0.447 (0.325) loss_x loss_x 0.4910 (1.0027) acc_x 87.5000 (74.0625) lr 3.1417e-05 eta 0:00:05
epoch [186/200] batch [45/53] time 0.424 (0.457) data 0.294 (0.327) loss_x loss_x 0.9082 (1.0270) acc_x 65.6250 (73.4722) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [50/53] time 0.433 (0.460) data 0.302 (0.330) loss_x loss_x 1.2549 (1.0225) acc_x 65.6250 (73.6875) lr 3.1417e-05 eta 0:00:01
epoch [186/200] batch [5/44] time 0.478 (0.458) data 0.347 (0.327) loss_u loss_u 0.7734 (0.7471) acc_u 40.6250 (31.8750) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [10/44] time 0.426 (0.455) data 0.295 (0.325) loss_u loss_u 0.7925 (0.7442) acc_u 28.1250 (32.5000) lr 3.1417e-05 eta 0:00:15
epoch [186/200] batch [15/44] time 0.401 (0.452) data 0.270 (0.321) loss_u loss_u 0.8018 (0.7484) acc_u 31.2500 (31.0417) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [20/44] time 0.363 (0.453) data 0.232 (0.322) loss_u loss_u 0.7144 (0.7492) acc_u 34.3750 (30.4688) lr 3.1417e-05 eta 0:00:10
epoch [186/200] batch [25/44] time 0.359 (0.453) data 0.228 (0.322) loss_u loss_u 0.7686 (0.7407) acc_u 28.1250 (31.5000) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [30/44] time 0.367 (0.449) data 0.236 (0.319) loss_u loss_u 0.7397 (0.7456) acc_u 28.1250 (31.1458) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [35/44] time 0.480 (0.448) data 0.350 (0.317) loss_u loss_u 0.6802 (0.7434) acc_u 46.8750 (31.7857) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [40/44] time 0.397 (0.446) data 0.267 (0.315) loss_u loss_u 0.8291 (0.7465) acc_u 15.6250 (31.4844) lr 3.1417e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1298
confident_label rate tensor(0.5370, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1684
clean true:1657
clean false:27
clean_rate:0.9839667458432304
noisy true:181
noisy false:1271
after delete: len(clean_dataset) 1684
after delete: len(noisy_dataset) 1452
epoch [187/200] batch [5/52] time 0.511 (0.411) data 0.381 (0.281) loss_x loss_x 1.3760 (1.1570) acc_x 68.7500 (73.7500) lr 2.7630e-05 eta 0:00:19
epoch [187/200] batch [10/52] time 0.459 (0.437) data 0.329 (0.306) loss_x loss_x 1.2090 (1.1255) acc_x 71.8750 (73.4375) lr 2.7630e-05 eta 0:00:18
epoch [187/200] batch [15/52] time 0.466 (0.451) data 0.336 (0.321) loss_x loss_x 1.1660 (1.1859) acc_x 71.8750 (72.0833) lr 2.7630e-05 eta 0:00:16
epoch [187/200] batch [20/52] time 0.369 (0.442) data 0.238 (0.311) loss_x loss_x 1.5361 (1.1499) acc_x 56.2500 (72.5000) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [25/52] time 0.410 (0.439) data 0.279 (0.308) loss_x loss_x 1.0586 (1.1389) acc_x 78.1250 (72.2500) lr 2.7630e-05 eta 0:00:11
epoch [187/200] batch [30/52] time 0.548 (0.447) data 0.417 (0.316) loss_x loss_x 1.3105 (1.1215) acc_x 59.3750 (72.6042) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [35/52] time 0.550 (0.456) data 0.420 (0.325) loss_x loss_x 1.0107 (1.0905) acc_x 78.1250 (73.3036) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [40/52] time 0.375 (0.452) data 0.244 (0.321) loss_x loss_x 0.9937 (1.0971) acc_x 78.1250 (73.0469) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [45/52] time 0.629 (0.455) data 0.499 (0.325) loss_x loss_x 1.0869 (1.0909) acc_x 62.5000 (72.6389) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [50/52] time 0.478 (0.456) data 0.348 (0.326) loss_x loss_x 0.6475 (1.0894) acc_x 81.2500 (72.6250) lr 2.7630e-05 eta 0:00:00
epoch [187/200] batch [5/45] time 0.454 (0.451) data 0.322 (0.320) loss_u loss_u 0.7158 (0.7238) acc_u 31.2500 (34.3750) lr 2.7630e-05 eta 0:00:18
epoch [187/200] batch [10/45] time 0.434 (0.452) data 0.303 (0.322) loss_u loss_u 0.7202 (0.7518) acc_u 34.3750 (30.3125) lr 2.7630e-05 eta 0:00:15
epoch [187/200] batch [15/45] time 0.371 (0.449) data 0.240 (0.318) loss_u loss_u 0.8525 (0.7618) acc_u 18.7500 (29.1667) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [20/45] time 0.351 (0.449) data 0.220 (0.318) loss_u loss_u 0.7197 (0.7571) acc_u 37.5000 (29.6875) lr 2.7630e-05 eta 0:00:11
epoch [187/200] batch [25/45] time 0.345 (0.446) data 0.215 (0.315) loss_u loss_u 0.7690 (0.7625) acc_u 34.3750 (29.3750) lr 2.7630e-05 eta 0:00:08
epoch [187/200] batch [30/45] time 0.349 (0.443) data 0.218 (0.312) loss_u loss_u 0.7412 (0.7622) acc_u 31.2500 (29.8958) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [35/45] time 0.411 (0.441) data 0.280 (0.310) loss_u loss_u 0.7427 (0.7618) acc_u 34.3750 (30.0893) lr 2.7630e-05 eta 0:00:04
epoch [187/200] batch [40/45] time 0.392 (0.439) data 0.260 (0.309) loss_u loss_u 0.7129 (0.7571) acc_u 37.5000 (30.4688) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [45/45] time 0.345 (0.439) data 0.215 (0.308) loss_u loss_u 0.7207 (0.7555) acc_u 28.1250 (30.4167) lr 2.7630e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1308
confident_label rate tensor(0.5341, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1675
clean true:1649
clean false:26
clean_rate:0.9844776119402985
noisy true:179
noisy false:1282
after delete: len(clean_dataset) 1675
after delete: len(noisy_dataset) 1461
epoch [188/200] batch [5/52] time 0.668 (0.472) data 0.538 (0.342) loss_x loss_x 1.1826 (1.0127) acc_x 71.8750 (75.6250) lr 2.4083e-05 eta 0:00:22
epoch [188/200] batch [10/52] time 0.380 (0.471) data 0.250 (0.341) loss_x loss_x 1.5742 (1.0366) acc_x 68.7500 (75.6250) lr 2.4083e-05 eta 0:00:19
epoch [188/200] batch [15/52] time 0.501 (0.463) data 0.371 (0.333) loss_x loss_x 0.7896 (1.0004) acc_x 78.1250 (76.6667) lr 2.4083e-05 eta 0:00:17
epoch [188/200] batch [20/52] time 0.578 (0.468) data 0.447 (0.338) loss_x loss_x 0.9990 (0.9852) acc_x 71.8750 (76.5625) lr 2.4083e-05 eta 0:00:14
epoch [188/200] batch [25/52] time 0.460 (0.466) data 0.329 (0.335) loss_x loss_x 0.9683 (0.9914) acc_x 78.1250 (75.8750) lr 2.4083e-05 eta 0:00:12
epoch [188/200] batch [30/52] time 0.377 (0.462) data 0.247 (0.331) loss_x loss_x 0.8008 (1.0044) acc_x 81.2500 (75.9375) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [35/52] time 0.450 (0.462) data 0.319 (0.332) loss_x loss_x 1.0938 (1.0161) acc_x 68.7500 (74.9107) lr 2.4083e-05 eta 0:00:07
epoch [188/200] batch [40/52] time 0.394 (0.460) data 0.264 (0.330) loss_x loss_x 0.9824 (1.0063) acc_x 71.8750 (75.2344) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [45/52] time 0.478 (0.457) data 0.348 (0.326) loss_x loss_x 1.0391 (1.0380) acc_x 71.8750 (74.0278) lr 2.4083e-05 eta 0:00:03
epoch [188/200] batch [50/52] time 0.416 (0.457) data 0.285 (0.326) loss_x loss_x 0.7861 (1.0300) acc_x 78.1250 (74.1250) lr 2.4083e-05 eta 0:00:00
epoch [188/200] batch [5/45] time 0.448 (0.455) data 0.316 (0.324) loss_u loss_u 0.7856 (0.7338) acc_u 25.0000 (36.2500) lr 2.4083e-05 eta 0:00:18
epoch [188/200] batch [10/45] time 0.363 (0.450) data 0.233 (0.319) loss_u loss_u 0.7002 (0.7440) acc_u 40.6250 (34.3750) lr 2.4083e-05 eta 0:00:15
epoch [188/200] batch [15/45] time 0.496 (0.450) data 0.365 (0.319) loss_u loss_u 0.7212 (0.7459) acc_u 34.3750 (34.7917) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [20/45] time 0.351 (0.450) data 0.220 (0.319) loss_u loss_u 0.8174 (0.7482) acc_u 25.0000 (33.7500) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [25/45] time 0.459 (0.447) data 0.328 (0.316) loss_u loss_u 0.8223 (0.7496) acc_u 18.7500 (33.1250) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [30/45] time 0.677 (0.446) data 0.546 (0.315) loss_u loss_u 0.7227 (0.7488) acc_u 28.1250 (33.2292) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [35/45] time 0.392 (0.441) data 0.261 (0.311) loss_u loss_u 0.7388 (0.7484) acc_u 34.3750 (33.0357) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [40/45] time 0.495 (0.441) data 0.364 (0.310) loss_u loss_u 0.8237 (0.7547) acc_u 25.0000 (32.2656) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [45/45] time 0.336 (0.442) data 0.205 (0.311) loss_u loss_u 0.8462 (0.7524) acc_u 18.7500 (32.2917) lr 2.4083e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1281
confident_label rate tensor(0.5402, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1694
clean true:1674
clean false:20
clean_rate:0.9881936245572609
noisy true:181
noisy false:1261
after delete: len(clean_dataset) 1694
after delete: len(noisy_dataset) 1442
epoch [189/200] batch [5/52] time 0.436 (0.427) data 0.305 (0.297) loss_x loss_x 0.9028 (1.1167) acc_x 78.1250 (68.1250) lr 2.0777e-05 eta 0:00:20
epoch [189/200] batch [10/52] time 0.473 (0.447) data 0.342 (0.316) loss_x loss_x 0.7910 (1.0156) acc_x 81.2500 (72.5000) lr 2.0777e-05 eta 0:00:18
epoch [189/200] batch [15/52] time 0.449 (0.444) data 0.319 (0.314) loss_x loss_x 0.9844 (1.0130) acc_x 81.2500 (72.9167) lr 2.0777e-05 eta 0:00:16
epoch [189/200] batch [20/52] time 0.426 (0.445) data 0.295 (0.315) loss_x loss_x 0.8760 (0.9897) acc_x 75.0000 (73.9062) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [25/52] time 0.434 (0.441) data 0.303 (0.310) loss_x loss_x 1.0996 (0.9958) acc_x 81.2500 (74.3750) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [30/52] time 0.716 (0.445) data 0.585 (0.315) loss_x loss_x 0.7549 (0.9706) acc_x 78.1250 (75.1042) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [35/52] time 0.545 (0.445) data 0.415 (0.315) loss_x loss_x 1.0088 (0.9687) acc_x 84.3750 (75.5357) lr 2.0777e-05 eta 0:00:07
epoch [189/200] batch [40/52] time 0.448 (0.449) data 0.318 (0.319) loss_x loss_x 0.8916 (0.9833) acc_x 78.1250 (75.1562) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [45/52] time 0.391 (0.447) data 0.261 (0.317) loss_x loss_x 0.7153 (0.9691) acc_x 84.3750 (75.6944) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [50/52] time 0.335 (0.444) data 0.205 (0.313) loss_x loss_x 1.3076 (0.9611) acc_x 65.6250 (75.4375) lr 2.0777e-05 eta 0:00:00
epoch [189/200] batch [5/45] time 0.431 (0.441) data 0.299 (0.310) loss_u loss_u 0.8350 (0.7185) acc_u 15.6250 (35.6250) lr 2.0777e-05 eta 0:00:17
epoch [189/200] batch [10/45] time 0.494 (0.440) data 0.363 (0.309) loss_u loss_u 0.7568 (0.7372) acc_u 28.1250 (32.8125) lr 2.0777e-05 eta 0:00:15
epoch [189/200] batch [15/45] time 0.476 (0.444) data 0.345 (0.314) loss_u loss_u 0.7891 (0.7480) acc_u 25.0000 (31.4583) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [20/45] time 0.389 (0.442) data 0.258 (0.311) loss_u loss_u 0.7241 (0.7515) acc_u 34.3750 (30.9375) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [25/45] time 0.445 (0.443) data 0.314 (0.312) loss_u loss_u 0.7695 (0.7521) acc_u 31.2500 (31.5000) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [30/45] time 0.427 (0.441) data 0.296 (0.310) loss_u loss_u 0.7847 (0.7523) acc_u 28.1250 (32.1875) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [35/45] time 0.392 (0.439) data 0.260 (0.308) loss_u loss_u 0.6196 (0.7442) acc_u 50.0000 (32.8571) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [40/45] time 0.387 (0.438) data 0.256 (0.308) loss_u loss_u 0.7275 (0.7454) acc_u 37.5000 (32.6562) lr 2.0777e-05 eta 0:00:02
epoch [189/200] batch [45/45] time 0.435 (0.438) data 0.304 (0.308) loss_u loss_u 0.7642 (0.7466) acc_u 28.1250 (32.0139) lr 2.0777e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1260
confident_label rate tensor(0.5475, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1717
clean true:1696
clean false:21
clean_rate:0.9877693651718114
noisy true:180
noisy false:1239
after delete: len(clean_dataset) 1717
after delete: len(noisy_dataset) 1419
epoch [190/200] batch [5/53] time 0.458 (0.427) data 0.328 (0.297) loss_x loss_x 0.9014 (1.0548) acc_x 84.3750 (76.8750) lr 1.7713e-05 eta 0:00:20
epoch [190/200] batch [10/53] time 0.479 (0.456) data 0.349 (0.326) loss_x loss_x 1.3281 (0.9549) acc_x 71.8750 (77.1875) lr 1.7713e-05 eta 0:00:19
epoch [190/200] batch [15/53] time 0.398 (0.466) data 0.268 (0.336) loss_x loss_x 0.9702 (0.9331) acc_x 71.8750 (77.0833) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [20/53] time 0.431 (0.453) data 0.300 (0.322) loss_x loss_x 0.8071 (0.8963) acc_x 78.1250 (77.5000) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [25/53] time 0.460 (0.450) data 0.330 (0.320) loss_x loss_x 1.0273 (0.9932) acc_x 75.0000 (75.3750) lr 1.7713e-05 eta 0:00:12
epoch [190/200] batch [30/53] time 0.546 (0.451) data 0.415 (0.321) loss_x loss_x 1.3203 (0.9862) acc_x 75.0000 (75.4167) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [35/53] time 0.517 (0.454) data 0.386 (0.323) loss_x loss_x 1.4248 (1.0168) acc_x 65.6250 (74.7321) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [40/53] time 0.481 (0.455) data 0.351 (0.324) loss_x loss_x 1.1650 (1.0113) acc_x 68.7500 (74.6094) lr 1.7713e-05 eta 0:00:05
epoch [190/200] batch [45/53] time 0.425 (0.451) data 0.294 (0.321) loss_x loss_x 0.8452 (1.0295) acc_x 84.3750 (74.5139) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [50/53] time 0.447 (0.451) data 0.317 (0.321) loss_x loss_x 1.0410 (1.0385) acc_x 78.1250 (74.3125) lr 1.7713e-05 eta 0:00:01
epoch [190/200] batch [5/44] time 0.398 (0.451) data 0.267 (0.321) loss_u loss_u 0.7798 (0.7514) acc_u 34.3750 (31.8750) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [10/44] time 0.393 (0.446) data 0.262 (0.316) loss_u loss_u 0.6899 (0.7458) acc_u 34.3750 (31.5625) lr 1.7713e-05 eta 0:00:15
epoch [190/200] batch [15/44] time 0.442 (0.449) data 0.311 (0.319) loss_u loss_u 0.7944 (0.7531) acc_u 28.1250 (31.6667) lr 1.7713e-05 eta 0:00:13
epoch [190/200] batch [20/44] time 0.352 (0.445) data 0.221 (0.314) loss_u loss_u 0.8623 (0.7616) acc_u 15.6250 (30.3125) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [25/44] time 0.431 (0.443) data 0.300 (0.313) loss_u loss_u 0.8296 (0.7593) acc_u 21.8750 (29.8750) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [30/44] time 0.463 (0.441) data 0.332 (0.311) loss_u loss_u 0.8018 (0.7618) acc_u 25.0000 (29.4792) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [35/44] time 0.600 (0.441) data 0.466 (0.311) loss_u loss_u 0.7354 (0.7605) acc_u 37.5000 (29.9107) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [40/44] time 0.356 (0.438) data 0.225 (0.307) loss_u loss_u 0.8076 (0.7631) acc_u 28.1250 (29.5312) lr 1.7713e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1274
confident_label rate tensor(0.5418, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1699
clean true:1679
clean false:20
clean_rate:0.9882283696291937
noisy true:183
noisy false:1254
after delete: len(clean_dataset) 1699
after delete: len(noisy_dataset) 1437
epoch [191/200] batch [5/53] time 0.328 (0.447) data 0.198 (0.317) loss_x loss_x 1.1914 (1.0573) acc_x 68.7500 (73.1250) lr 1.4891e-05 eta 0:00:21
epoch [191/200] batch [10/53] time 0.534 (0.460) data 0.404 (0.330) loss_x loss_x 0.9858 (1.0445) acc_x 71.8750 (74.0625) lr 1.4891e-05 eta 0:00:19
epoch [191/200] batch [15/53] time 0.479 (0.464) data 0.348 (0.333) loss_x loss_x 0.9561 (1.1215) acc_x 78.1250 (72.9167) lr 1.4891e-05 eta 0:00:17
epoch [191/200] batch [20/53] time 0.665 (0.462) data 0.535 (0.331) loss_x loss_x 0.6733 (1.0923) acc_x 84.3750 (73.4375) lr 1.4891e-05 eta 0:00:15
epoch [191/200] batch [25/53] time 0.436 (0.460) data 0.306 (0.330) loss_x loss_x 0.7495 (1.0694) acc_x 84.3750 (73.8750) lr 1.4891e-05 eta 0:00:12
epoch [191/200] batch [30/53] time 0.605 (0.466) data 0.475 (0.335) loss_x loss_x 0.5093 (1.0359) acc_x 93.7500 (74.7917) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [35/53] time 0.439 (0.458) data 0.309 (0.328) loss_x loss_x 0.9780 (1.0494) acc_x 81.2500 (74.4643) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [40/53] time 0.399 (0.452) data 0.269 (0.321) loss_x loss_x 0.8604 (1.0454) acc_x 68.7500 (74.6094) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [45/53] time 0.406 (0.454) data 0.275 (0.324) loss_x loss_x 1.0674 (1.0460) acc_x 78.1250 (74.4444) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [50/53] time 0.444 (0.447) data 0.313 (0.316) loss_x loss_x 1.0586 (1.0559) acc_x 71.8750 (74.2500) lr 1.4891e-05 eta 0:00:01
epoch [191/200] batch [5/44] time 0.407 (0.441) data 0.276 (0.310) loss_u loss_u 0.8291 (0.7963) acc_u 25.0000 (26.8750) lr 1.4891e-05 eta 0:00:17
epoch [191/200] batch [10/44] time 0.483 (0.439) data 0.352 (0.309) loss_u loss_u 0.7559 (0.7831) acc_u 40.6250 (28.7500) lr 1.4891e-05 eta 0:00:14
epoch [191/200] batch [15/44] time 0.430 (0.437) data 0.299 (0.306) loss_u loss_u 0.7583 (0.7776) acc_u 31.2500 (28.9583) lr 1.4891e-05 eta 0:00:12
epoch [191/200] batch [20/44] time 0.376 (0.435) data 0.245 (0.304) loss_u loss_u 0.7144 (0.7641) acc_u 34.3750 (30.4688) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [25/44] time 0.439 (0.435) data 0.308 (0.304) loss_u loss_u 0.7686 (0.7611) acc_u 31.2500 (30.5000) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [30/44] time 0.345 (0.432) data 0.214 (0.302) loss_u loss_u 0.7417 (0.7534) acc_u 34.3750 (31.3542) lr 1.4891e-05 eta 0:00:06
epoch [191/200] batch [35/44] time 0.406 (0.434) data 0.275 (0.304) loss_u loss_u 0.8042 (0.7585) acc_u 25.0000 (30.8929) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [40/44] time 0.450 (0.434) data 0.319 (0.303) loss_u loss_u 0.7300 (0.7560) acc_u 37.5000 (31.2500) lr 1.4891e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1293
confident_label rate tensor(0.5405, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1695
clean true:1674
clean false:21
clean_rate:0.9876106194690265
noisy true:169
noisy false:1272
after delete: len(clean_dataset) 1695
after delete: len(noisy_dataset) 1441
epoch [192/200] batch [5/52] time 0.444 (0.465) data 0.313 (0.334) loss_x loss_x 1.0420 (0.8216) acc_x 62.5000 (75.6250) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [10/52] time 0.571 (0.479) data 0.440 (0.348) loss_x loss_x 0.9287 (0.9862) acc_x 71.8750 (73.7500) lr 1.2312e-05 eta 0:00:20
epoch [192/200] batch [15/52] time 0.397 (0.480) data 0.267 (0.350) loss_x loss_x 1.5576 (1.0530) acc_x 53.1250 (71.2500) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [20/52] time 0.513 (0.476) data 0.382 (0.345) loss_x loss_x 0.8696 (1.0145) acc_x 84.3750 (72.6562) lr 1.2312e-05 eta 0:00:15
epoch [192/200] batch [25/52] time 0.381 (0.472) data 0.251 (0.342) loss_x loss_x 1.4238 (1.0645) acc_x 65.6250 (72.1250) lr 1.2312e-05 eta 0:00:12
epoch [192/200] batch [30/52] time 0.547 (0.476) data 0.417 (0.345) loss_x loss_x 0.8599 (1.0352) acc_x 87.5000 (73.6458) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [35/52] time 0.486 (0.473) data 0.356 (0.342) loss_x loss_x 1.2959 (1.0410) acc_x 65.6250 (73.3036) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [40/52] time 0.476 (0.466) data 0.346 (0.335) loss_x loss_x 1.1348 (1.0500) acc_x 78.1250 (73.2031) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [45/52] time 0.415 (0.461) data 0.285 (0.330) loss_x loss_x 0.5933 (1.0283) acc_x 87.5000 (73.6806) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [50/52] time 0.442 (0.458) data 0.312 (0.327) loss_x loss_x 1.2334 (1.0154) acc_x 75.0000 (74.0625) lr 1.2312e-05 eta 0:00:00
epoch [192/200] batch [5/45] time 0.370 (0.454) data 0.239 (0.324) loss_u loss_u 0.8130 (0.7874) acc_u 15.6250 (23.7500) lr 1.2312e-05 eta 0:00:18
epoch [192/200] batch [10/45] time 0.326 (0.449) data 0.193 (0.319) loss_u loss_u 0.8257 (0.7772) acc_u 25.0000 (26.8750) lr 1.2312e-05 eta 0:00:15
epoch [192/200] batch [15/45] time 0.388 (0.446) data 0.257 (0.315) loss_u loss_u 0.7578 (0.7738) acc_u 34.3750 (27.9167) lr 1.2312e-05 eta 0:00:13
epoch [192/200] batch [20/45] time 0.387 (0.447) data 0.256 (0.316) loss_u loss_u 0.7065 (0.7584) acc_u 34.3750 (29.6875) lr 1.2312e-05 eta 0:00:11
epoch [192/200] batch [25/45] time 0.408 (0.445) data 0.275 (0.314) loss_u loss_u 0.7266 (0.7551) acc_u 31.2500 (30.3750) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [30/45] time 0.560 (0.444) data 0.429 (0.314) loss_u loss_u 0.7847 (0.7523) acc_u 28.1250 (30.6250) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [35/45] time 0.398 (0.442) data 0.267 (0.311) loss_u loss_u 0.6919 (0.7528) acc_u 46.8750 (30.8036) lr 1.2312e-05 eta 0:00:04
epoch [192/200] batch [40/45] time 0.375 (0.441) data 0.244 (0.310) loss_u loss_u 0.7227 (0.7481) acc_u 40.6250 (31.6406) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [45/45] time 0.377 (0.440) data 0.246 (0.309) loss_u loss_u 0.7573 (0.7534) acc_u 31.2500 (30.9722) lr 1.2312e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1298
confident_label rate tensor(0.5335, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1673
clean true:1654
clean false:19
clean_rate:0.9886431560071728
noisy true:184
noisy false:1279
after delete: len(clean_dataset) 1673
after delete: len(noisy_dataset) 1463
epoch [193/200] batch [5/52] time 0.499 (0.421) data 0.369 (0.290) loss_x loss_x 0.7993 (0.9231) acc_x 75.0000 (71.8750) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [10/52] time 0.523 (0.456) data 0.392 (0.326) loss_x loss_x 0.5469 (0.9775) acc_x 81.2500 (72.8125) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [15/52] time 0.602 (0.458) data 0.472 (0.328) loss_x loss_x 1.1738 (0.9743) acc_x 65.6250 (73.5417) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [20/52] time 0.416 (0.448) data 0.285 (0.318) loss_x loss_x 1.0996 (0.9790) acc_x 78.1250 (74.2188) lr 9.9763e-06 eta 0:00:14
epoch [193/200] batch [25/52] time 0.443 (0.452) data 0.312 (0.322) loss_x loss_x 1.5400 (0.9964) acc_x 68.7500 (74.7500) lr 9.9763e-06 eta 0:00:12
epoch [193/200] batch [30/52] time 0.418 (0.450) data 0.287 (0.319) loss_x loss_x 0.7622 (0.9806) acc_x 81.2500 (74.6875) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [35/52] time 0.399 (0.456) data 0.269 (0.326) loss_x loss_x 1.2637 (0.9676) acc_x 75.0000 (75.2679) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [40/52] time 0.517 (0.455) data 0.387 (0.325) loss_x loss_x 0.9536 (0.9606) acc_x 75.0000 (75.7031) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [45/52] time 0.373 (0.453) data 0.243 (0.323) loss_x loss_x 1.1064 (0.9719) acc_x 75.0000 (75.5556) lr 9.9763e-06 eta 0:00:03
epoch [193/200] batch [50/52] time 0.500 (0.453) data 0.369 (0.322) loss_x loss_x 1.2598 (0.9738) acc_x 65.6250 (75.2500) lr 9.9763e-06 eta 0:00:00
epoch [193/200] batch [5/45] time 0.366 (0.448) data 0.235 (0.318) loss_u loss_u 0.7764 (0.7450) acc_u 28.1250 (31.2500) lr 9.9763e-06 eta 0:00:17
epoch [193/200] batch [10/45] time 0.319 (0.445) data 0.188 (0.314) loss_u loss_u 0.5825 (0.7271) acc_u 46.8750 (32.1875) lr 9.9763e-06 eta 0:00:15
epoch [193/200] batch [15/45] time 0.487 (0.444) data 0.356 (0.313) loss_u loss_u 0.7437 (0.7391) acc_u 37.5000 (32.0833) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [20/45] time 0.353 (0.442) data 0.221 (0.311) loss_u loss_u 0.7700 (0.7472) acc_u 31.2500 (31.5625) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [25/45] time 0.348 (0.442) data 0.217 (0.311) loss_u loss_u 0.7617 (0.7483) acc_u 31.2500 (31.5000) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [30/45] time 0.389 (0.438) data 0.258 (0.307) loss_u loss_u 0.8091 (0.7499) acc_u 18.7500 (31.4583) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [35/45] time 0.494 (0.438) data 0.363 (0.308) loss_u loss_u 0.8271 (0.7485) acc_u 21.8750 (31.3393) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [40/45] time 0.428 (0.440) data 0.296 (0.309) loss_u loss_u 0.7798 (0.7486) acc_u 28.1250 (31.1719) lr 9.9763e-06 eta 0:00:02
epoch [193/200] batch [45/45] time 0.383 (0.438) data 0.252 (0.308) loss_u loss_u 0.7695 (0.7474) acc_u 28.1250 (31.6667) lr 9.9763e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1271
confident_label rate tensor(0.5462, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1713
clean true:1696
clean false:17
clean_rate:0.9900758902510216
noisy true:169
noisy false:1254
after delete: len(clean_dataset) 1713
after delete: len(noisy_dataset) 1423
epoch [194/200] batch [5/53] time 0.459 (0.463) data 0.328 (0.333) loss_x loss_x 1.0283 (1.0218) acc_x 68.7500 (70.6250) lr 7.8853e-06 eta 0:00:22
epoch [194/200] batch [10/53] time 0.345 (0.436) data 0.214 (0.305) loss_x loss_x 0.8159 (1.0146) acc_x 81.2500 (72.5000) lr 7.8853e-06 eta 0:00:18
epoch [194/200] batch [15/53] time 0.741 (0.466) data 0.610 (0.335) loss_x loss_x 0.8457 (0.9351) acc_x 75.0000 (75.0000) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [20/53] time 0.469 (0.460) data 0.339 (0.330) loss_x loss_x 1.3428 (0.9345) acc_x 59.3750 (74.2188) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [25/53] time 0.449 (0.449) data 0.317 (0.319) loss_x loss_x 0.7642 (0.9568) acc_x 81.2500 (73.7500) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [30/53] time 0.508 (0.445) data 0.377 (0.315) loss_x loss_x 1.0352 (0.9613) acc_x 78.1250 (74.0625) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [35/53] time 0.393 (0.443) data 0.263 (0.313) loss_x loss_x 0.9771 (0.9762) acc_x 71.8750 (74.1964) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [40/53] time 0.386 (0.443) data 0.256 (0.312) loss_x loss_x 0.9634 (0.9830) acc_x 81.2500 (74.2969) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [45/53] time 0.384 (0.440) data 0.254 (0.310) loss_x loss_x 1.0938 (0.9846) acc_x 75.0000 (74.3056) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [50/53] time 0.399 (0.441) data 0.269 (0.311) loss_x loss_x 0.7446 (0.9689) acc_x 75.0000 (74.6250) lr 7.8853e-06 eta 0:00:01
epoch [194/200] batch [5/44] time 0.447 (0.443) data 0.315 (0.312) loss_u loss_u 0.7363 (0.7425) acc_u 31.2500 (30.0000) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [10/44] time 0.537 (0.445) data 0.406 (0.314) loss_u loss_u 0.7192 (0.7432) acc_u 40.6250 (30.9375) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [15/44] time 0.399 (0.443) data 0.268 (0.312) loss_u loss_u 0.7764 (0.7441) acc_u 28.1250 (30.6250) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [20/44] time 0.457 (0.447) data 0.325 (0.316) loss_u loss_u 0.7368 (0.7512) acc_u 34.3750 (30.4688) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [25/44] time 0.495 (0.446) data 0.364 (0.315) loss_u loss_u 0.6904 (0.7475) acc_u 37.5000 (30.7500) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [30/44] time 0.401 (0.446) data 0.270 (0.315) loss_u loss_u 0.7646 (0.7484) acc_u 34.3750 (31.0417) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [35/44] time 0.398 (0.445) data 0.266 (0.314) loss_u loss_u 0.8643 (0.7531) acc_u 18.7500 (30.1786) lr 7.8853e-06 eta 0:00:04
epoch [194/200] batch [40/44] time 0.449 (0.445) data 0.318 (0.314) loss_u loss_u 0.8081 (0.7550) acc_u 21.8750 (29.9219) lr 7.8853e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1269
confident_label rate tensor(0.5440, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1706
clean true:1683
clean false:23
clean_rate:0.9865181711606096
noisy true:184
noisy false:1246
after delete: len(clean_dataset) 1706
after delete: len(noisy_dataset) 1430
epoch [195/200] batch [5/53] time 0.431 (0.483) data 0.301 (0.352) loss_x loss_x 1.1973 (1.0619) acc_x 68.7500 (70.0000) lr 6.0390e-06 eta 0:00:23
epoch [195/200] batch [10/53] time 0.413 (0.456) data 0.282 (0.325) loss_x loss_x 1.1162 (1.0911) acc_x 75.0000 (70.6250) lr 6.0390e-06 eta 0:00:19
epoch [195/200] batch [15/53] time 0.411 (0.460) data 0.280 (0.329) loss_x loss_x 0.8438 (1.0234) acc_x 81.2500 (72.7083) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [20/53] time 0.401 (0.453) data 0.270 (0.322) loss_x loss_x 1.2568 (1.0646) acc_x 71.8750 (72.8125) lr 6.0390e-06 eta 0:00:14
epoch [195/200] batch [25/53] time 0.422 (0.457) data 0.292 (0.327) loss_x loss_x 1.6465 (1.0999) acc_x 59.3750 (71.6250) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [30/53] time 0.449 (0.458) data 0.318 (0.328) loss_x loss_x 0.9658 (1.0649) acc_x 71.8750 (72.3958) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [35/53] time 0.424 (0.461) data 0.294 (0.330) loss_x loss_x 0.9331 (1.0384) acc_x 75.0000 (73.2143) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [40/53] time 0.400 (0.461) data 0.270 (0.330) loss_x loss_x 0.7656 (1.0302) acc_x 81.2500 (73.3594) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [45/53] time 0.518 (0.457) data 0.388 (0.327) loss_x loss_x 1.1494 (1.0182) acc_x 68.7500 (73.8889) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [50/53] time 0.383 (0.453) data 0.253 (0.322) loss_x loss_x 1.0645 (1.0143) acc_x 75.0000 (74.0625) lr 6.0390e-06 eta 0:00:01
epoch [195/200] batch [5/44] time 0.368 (0.450) data 0.237 (0.320) loss_u loss_u 0.7485 (0.7421) acc_u 31.2500 (30.6250) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [10/44] time 0.353 (0.445) data 0.221 (0.315) loss_u loss_u 0.7896 (0.7409) acc_u 25.0000 (30.6250) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [15/44] time 0.360 (0.441) data 0.229 (0.310) loss_u loss_u 0.7046 (0.7495) acc_u 37.5000 (30.2083) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [20/44] time 0.433 (0.441) data 0.302 (0.311) loss_u loss_u 0.8013 (0.7620) acc_u 21.8750 (28.5938) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [25/44] time 0.375 (0.443) data 0.244 (0.312) loss_u loss_u 0.8691 (0.7679) acc_u 18.7500 (28.6250) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [30/44] time 0.530 (0.443) data 0.399 (0.313) loss_u loss_u 0.7749 (0.7603) acc_u 28.1250 (29.3750) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [35/44] time 0.386 (0.444) data 0.255 (0.313) loss_u loss_u 0.6816 (0.7559) acc_u 34.3750 (30.0000) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [40/44] time 0.391 (0.442) data 0.260 (0.311) loss_u loss_u 0.6704 (0.7546) acc_u 37.5000 (30.3125) lr 6.0390e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1308
confident_label rate tensor(0.5322, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1669
clean true:1648
clean false:21
clean_rate:0.987417615338526
noisy true:180
noisy false:1287
after delete: len(clean_dataset) 1669
after delete: len(noisy_dataset) 1467
epoch [196/200] batch [5/52] time 0.493 (0.507) data 0.364 (0.377) loss_x loss_x 1.4512 (1.0286) acc_x 53.1250 (72.5000) lr 4.4380e-06 eta 0:00:23
epoch [196/200] batch [10/52] time 0.405 (0.469) data 0.276 (0.340) loss_x loss_x 0.9609 (1.0480) acc_x 78.1250 (76.5625) lr 4.4380e-06 eta 0:00:19
epoch [196/200] batch [15/52] time 0.455 (0.464) data 0.326 (0.334) loss_x loss_x 0.7925 (1.0823) acc_x 81.2500 (75.4167) lr 4.4380e-06 eta 0:00:17
epoch [196/200] batch [20/52] time 0.401 (0.463) data 0.270 (0.333) loss_x loss_x 0.9595 (1.0384) acc_x 75.0000 (76.0938) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [25/52] time 0.421 (0.454) data 0.291 (0.324) loss_x loss_x 1.1611 (1.0740) acc_x 75.0000 (75.2500) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [30/52] time 0.391 (0.453) data 0.261 (0.323) loss_x loss_x 0.9072 (1.0597) acc_x 75.0000 (74.7917) lr 4.4380e-06 eta 0:00:09
epoch [196/200] batch [35/52] time 0.357 (0.452) data 0.226 (0.322) loss_x loss_x 0.9492 (1.0344) acc_x 78.1250 (74.8214) lr 4.4380e-06 eta 0:00:07
epoch [196/200] batch [40/52] time 0.451 (0.452) data 0.320 (0.322) loss_x loss_x 1.1152 (1.0186) acc_x 71.8750 (75.0000) lr 4.4380e-06 eta 0:00:05
epoch [196/200] batch [45/52] time 0.358 (0.448) data 0.228 (0.318) loss_x loss_x 0.9194 (1.0437) acc_x 78.1250 (74.3056) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [50/52] time 0.388 (0.446) data 0.258 (0.316) loss_x loss_x 1.1592 (1.0574) acc_x 71.8750 (74.1875) lr 4.4380e-06 eta 0:00:00
epoch [196/200] batch [5/45] time 0.433 (0.447) data 0.302 (0.317) loss_u loss_u 0.6646 (0.6965) acc_u 37.5000 (36.2500) lr 4.4380e-06 eta 0:00:17
epoch [196/200] batch [10/45] time 0.441 (0.446) data 0.310 (0.316) loss_u loss_u 0.7563 (0.7229) acc_u 31.2500 (34.6875) lr 4.4380e-06 eta 0:00:15
epoch [196/200] batch [15/45] time 0.464 (0.444) data 0.333 (0.314) loss_u loss_u 0.9043 (0.7346) acc_u 9.3750 (32.9167) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [20/45] time 0.578 (0.443) data 0.446 (0.313) loss_u loss_u 0.7466 (0.7303) acc_u 31.2500 (33.1250) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [25/45] time 0.577 (0.444) data 0.446 (0.314) loss_u loss_u 0.8179 (0.7425) acc_u 21.8750 (31.7500) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [30/45] time 0.427 (0.447) data 0.295 (0.317) loss_u loss_u 0.7539 (0.7467) acc_u 28.1250 (31.4583) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [35/45] time 0.492 (0.445) data 0.361 (0.314) loss_u loss_u 0.7471 (0.7448) acc_u 37.5000 (32.1429) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [40/45] time 0.368 (0.442) data 0.237 (0.311) loss_u loss_u 0.7710 (0.7454) acc_u 25.0000 (32.0312) lr 4.4380e-06 eta 0:00:02
epoch [196/200] batch [45/45] time 0.473 (0.442) data 0.342 (0.312) loss_u loss_u 0.7222 (0.7434) acc_u 37.5000 (32.2917) lr 4.4380e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1273
confident_label rate tensor(0.5389, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1690
clean true:1673
clean false:17
clean_rate:0.9899408284023669
noisy true:190
noisy false:1256
after delete: len(clean_dataset) 1690
after delete: len(noisy_dataset) 1446
epoch [197/200] batch [5/52] time 0.387 (0.409) data 0.257 (0.278) loss_x loss_x 0.9385 (1.0150) acc_x 81.2500 (77.5000) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [10/52] time 0.462 (0.418) data 0.330 (0.287) loss_x loss_x 0.7012 (1.0035) acc_x 81.2500 (77.1875) lr 3.0827e-06 eta 0:00:17
epoch [197/200] batch [15/52] time 0.439 (0.435) data 0.309 (0.304) loss_x loss_x 0.8218 (1.0251) acc_x 84.3750 (75.2083) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [20/52] time 0.373 (0.426) data 0.243 (0.296) loss_x loss_x 1.0029 (1.0437) acc_x 84.3750 (75.4688) lr 3.0827e-06 eta 0:00:13
epoch [197/200] batch [25/52] time 0.482 (0.432) data 0.352 (0.302) loss_x loss_x 1.3164 (1.0288) acc_x 65.6250 (75.6250) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [30/52] time 0.503 (0.430) data 0.373 (0.300) loss_x loss_x 1.1494 (1.0583) acc_x 71.8750 (75.2083) lr 3.0827e-06 eta 0:00:09
epoch [197/200] batch [35/52] time 0.330 (0.426) data 0.199 (0.295) loss_x loss_x 0.8452 (1.0454) acc_x 71.8750 (74.9107) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [40/52] time 0.591 (0.431) data 0.460 (0.301) loss_x loss_x 1.1553 (1.0612) acc_x 71.8750 (74.0625) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [45/52] time 0.468 (0.443) data 0.337 (0.312) loss_x loss_x 0.7549 (1.0385) acc_x 81.2500 (74.5139) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [50/52] time 0.369 (0.449) data 0.239 (0.318) loss_x loss_x 1.2461 (1.0470) acc_x 65.6250 (74.0000) lr 3.0827e-06 eta 0:00:00
epoch [197/200] batch [5/45] time 0.322 (0.444) data 0.191 (0.314) loss_u loss_u 0.8550 (0.7762) acc_u 12.5000 (27.5000) lr 3.0827e-06 eta 0:00:17
epoch [197/200] batch [10/45] time 0.440 (0.441) data 0.309 (0.310) loss_u loss_u 0.8667 (0.7698) acc_u 9.3750 (28.1250) lr 3.0827e-06 eta 0:00:15
epoch [197/200] batch [15/45] time 0.470 (0.439) data 0.339 (0.309) loss_u loss_u 0.6885 (0.7457) acc_u 37.5000 (30.4167) lr 3.0827e-06 eta 0:00:13
epoch [197/200] batch [20/45] time 0.376 (0.437) data 0.246 (0.306) loss_u loss_u 0.6616 (0.7339) acc_u 46.8750 (32.5000) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [25/45] time 0.439 (0.435) data 0.308 (0.305) loss_u loss_u 0.7764 (0.7354) acc_u 25.0000 (31.8750) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [30/45] time 0.548 (0.438) data 0.417 (0.307) loss_u loss_u 0.7212 (0.7317) acc_u 37.5000 (32.7083) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [35/45] time 0.431 (0.435) data 0.300 (0.305) loss_u loss_u 0.6958 (0.7298) acc_u 43.7500 (33.1250) lr 3.0827e-06 eta 0:00:04
epoch [197/200] batch [40/45] time 0.427 (0.438) data 0.296 (0.308) loss_u loss_u 0.6860 (0.7288) acc_u 37.5000 (33.2812) lr 3.0827e-06 eta 0:00:02
epoch [197/200] batch [45/45] time 0.336 (0.438) data 0.205 (0.307) loss_u loss_u 0.8447 (0.7330) acc_u 18.7500 (33.2639) lr 3.0827e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1277
confident_label rate tensor(0.5399, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1693
clean true:1669
clean false:24
clean_rate:0.9858239810986414
noisy true:190
noisy false:1253
after delete: len(clean_dataset) 1693
after delete: len(noisy_dataset) 1443
epoch [198/200] batch [5/52] time 0.474 (0.421) data 0.344 (0.291) loss_x loss_x 1.1992 (1.0188) acc_x 65.6250 (73.7500) lr 1.9733e-06 eta 0:00:19
epoch [198/200] batch [10/52] time 0.358 (0.414) data 0.228 (0.284) loss_x loss_x 1.2227 (1.0205) acc_x 68.7500 (73.1250) lr 1.9733e-06 eta 0:00:17
epoch [198/200] batch [15/52] time 0.433 (0.427) data 0.302 (0.297) loss_x loss_x 0.8760 (1.0668) acc_x 75.0000 (72.5000) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [20/52] time 0.591 (0.435) data 0.460 (0.305) loss_x loss_x 0.8032 (1.0420) acc_x 78.1250 (73.2812) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [25/52] time 0.444 (0.440) data 0.313 (0.309) loss_x loss_x 1.0732 (1.0551) acc_x 68.7500 (73.5000) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [30/52] time 0.395 (0.440) data 0.265 (0.310) loss_x loss_x 1.2363 (1.0525) acc_x 68.7500 (73.3333) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [35/52] time 0.397 (0.440) data 0.267 (0.309) loss_x loss_x 1.3506 (1.0771) acc_x 68.7500 (72.6786) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [40/52] time 0.481 (0.445) data 0.351 (0.314) loss_x loss_x 0.5815 (1.0547) acc_x 78.1250 (72.8906) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [45/52] time 0.461 (0.449) data 0.331 (0.318) loss_x loss_x 1.0713 (1.0716) acc_x 78.1250 (72.9167) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [50/52] time 0.455 (0.444) data 0.324 (0.314) loss_x loss_x 1.1270 (1.0748) acc_x 75.0000 (73.1250) lr 1.9733e-06 eta 0:00:00
epoch [198/200] batch [5/45] time 0.369 (0.441) data 0.238 (0.311) loss_u loss_u 0.6650 (0.7176) acc_u 34.3750 (31.8750) lr 1.9733e-06 eta 0:00:17
epoch [198/200] batch [10/45] time 0.397 (0.440) data 0.265 (0.309) loss_u loss_u 0.7368 (0.7339) acc_u 28.1250 (31.5625) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [15/45] time 0.359 (0.439) data 0.228 (0.308) loss_u loss_u 0.6597 (0.7303) acc_u 40.6250 (32.7083) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [20/45] time 0.371 (0.437) data 0.239 (0.307) loss_u loss_u 0.7329 (0.7440) acc_u 31.2500 (31.5625) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [25/45] time 0.737 (0.443) data 0.605 (0.312) loss_u loss_u 0.7261 (0.7345) acc_u 31.2500 (32.5000) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [30/45] time 0.403 (0.440) data 0.272 (0.309) loss_u loss_u 0.7051 (0.7352) acc_u 34.3750 (32.6042) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [35/45] time 0.307 (0.440) data 0.175 (0.309) loss_u loss_u 0.6997 (0.7352) acc_u 40.6250 (32.6786) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [40/45] time 0.423 (0.440) data 0.292 (0.309) loss_u loss_u 0.7012 (0.7383) acc_u 43.7500 (32.5000) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [45/45] time 0.458 (0.440) data 0.327 (0.309) loss_u loss_u 0.6421 (0.7329) acc_u 40.6250 (33.1944) lr 1.9733e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1254
confident_label rate tensor(0.5466, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1714
clean true:1699
clean false:15
clean_rate:0.9912485414235706
noisy true:183
noisy false:1239
after delete: len(clean_dataset) 1714
after delete: len(noisy_dataset) 1422
epoch [199/200] batch [5/53] time 0.522 (0.480) data 0.392 (0.349) loss_x loss_x 1.2354 (1.1084) acc_x 68.7500 (71.2500) lr 1.1101e-06 eta 0:00:23
epoch [199/200] batch [10/53] time 0.436 (0.464) data 0.305 (0.334) loss_x loss_x 1.1064 (1.1558) acc_x 81.2500 (71.2500) lr 1.1101e-06 eta 0:00:19
epoch [199/200] batch [15/53] time 0.576 (0.465) data 0.445 (0.335) loss_x loss_x 0.7534 (1.1408) acc_x 71.8750 (72.0833) lr 1.1101e-06 eta 0:00:17
epoch [199/200] batch [20/53] time 0.490 (0.464) data 0.360 (0.334) loss_x loss_x 0.4905 (1.1281) acc_x 90.6250 (71.8750) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [25/53] time 0.459 (0.470) data 0.329 (0.339) loss_x loss_x 0.8384 (1.0847) acc_x 81.2500 (73.0000) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [30/53] time 0.369 (0.465) data 0.238 (0.334) loss_x loss_x 1.4141 (1.0733) acc_x 65.6250 (73.1250) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [35/53] time 0.444 (0.460) data 0.313 (0.330) loss_x loss_x 1.3623 (1.0728) acc_x 65.6250 (73.1250) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [40/53] time 0.561 (0.461) data 0.430 (0.331) loss_x loss_x 1.2227 (1.0724) acc_x 68.7500 (73.0469) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [45/53] time 0.391 (0.455) data 0.260 (0.325) loss_x loss_x 1.0811 (1.0796) acc_x 65.6250 (72.9861) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [50/53] time 0.369 (0.452) data 0.238 (0.322) loss_x loss_x 0.7783 (1.0776) acc_x 68.7500 (72.8750) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [5/44] time 0.425 (0.456) data 0.294 (0.326) loss_u loss_u 0.7983 (0.7693) acc_u 21.8750 (25.0000) lr 1.1101e-06 eta 0:00:17
epoch [199/200] batch [10/44] time 0.443 (0.455) data 0.311 (0.324) loss_u loss_u 0.8472 (0.7607) acc_u 15.6250 (27.8125) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [15/44] time 0.356 (0.452) data 0.224 (0.322) loss_u loss_u 0.7085 (0.7618) acc_u 34.3750 (28.5417) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [20/44] time 0.353 (0.451) data 0.222 (0.320) loss_u loss_u 0.7256 (0.7519) acc_u 34.3750 (29.5312) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [25/44] time 0.360 (0.446) data 0.229 (0.316) loss_u loss_u 0.7476 (0.7559) acc_u 28.1250 (28.5000) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [30/44] time 0.333 (0.441) data 0.203 (0.310) loss_u loss_u 0.7407 (0.7576) acc_u 37.5000 (29.1667) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [35/44] time 0.366 (0.439) data 0.235 (0.308) loss_u loss_u 0.7539 (0.7540) acc_u 31.2500 (29.7321) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [40/44] time 0.491 (0.440) data 0.360 (0.309) loss_u loss_u 0.6201 (0.7478) acc_u 50.0000 (30.7812) lr 1.1101e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1299
confident_label rate tensor(0.5325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1670
clean true:1651
clean false:19
clean_rate:0.988622754491018
noisy true:186
noisy false:1280
after delete: len(clean_dataset) 1670
after delete: len(noisy_dataset) 1466
epoch [200/200] batch [5/52] time 0.568 (0.525) data 0.438 (0.394) loss_x loss_x 0.7241 (0.9563) acc_x 81.2500 (75.0000) lr 4.9344e-07 eta 0:00:24
epoch [200/200] batch [10/52] time 0.375 (0.506) data 0.244 (0.375) loss_x loss_x 0.6680 (0.8479) acc_x 81.2500 (76.2500) lr 4.9344e-07 eta 0:00:21
epoch [200/200] batch [15/52] time 0.374 (0.496) data 0.244 (0.365) loss_x loss_x 1.0547 (0.9792) acc_x 75.0000 (74.7917) lr 4.9344e-07 eta 0:00:18
epoch [200/200] batch [20/52] time 0.423 (0.492) data 0.292 (0.361) loss_x loss_x 1.0928 (0.9775) acc_x 71.8750 (75.0000) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [25/52] time 0.424 (0.487) data 0.293 (0.356) loss_x loss_x 1.0605 (1.0045) acc_x 78.1250 (75.0000) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [30/52] time 0.443 (0.477) data 0.312 (0.346) loss_x loss_x 1.3730 (1.0396) acc_x 65.6250 (74.4792) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [35/52] time 0.379 (0.468) data 0.249 (0.337) loss_x loss_x 1.6211 (1.0729) acc_x 68.7500 (73.9286) lr 4.9344e-07 eta 0:00:07
epoch [200/200] batch [40/52] time 0.383 (0.462) data 0.253 (0.331) loss_x loss_x 0.8091 (1.0475) acc_x 78.1250 (74.3750) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [45/52] time 0.556 (0.463) data 0.426 (0.332) loss_x loss_x 1.5732 (1.0594) acc_x 62.5000 (73.8194) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [50/52] time 0.413 (0.456) data 0.282 (0.325) loss_x loss_x 0.8877 (1.0597) acc_x 78.1250 (73.6250) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [5/45] time 0.463 (0.453) data 0.332 (0.322) loss_u loss_u 0.8008 (0.7569) acc_u 21.8750 (29.3750) lr 4.9344e-07 eta 0:00:18
epoch [200/200] batch [10/45] time 0.509 (0.458) data 0.378 (0.327) loss_u loss_u 0.7642 (0.7250) acc_u 31.2500 (30.9375) lr 4.9344e-07 eta 0:00:16
epoch [200/200] batch [15/45] time 0.319 (0.452) data 0.190 (0.321) loss_u loss_u 0.7544 (0.7382) acc_u 31.2500 (30.4167) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [20/45] time 0.430 (0.453) data 0.298 (0.323) loss_u loss_u 0.7222 (0.7370) acc_u 43.7500 (32.3438) lr 4.9344e-07 eta 0:00:11
epoch [200/200] batch [25/45] time 0.395 (0.450) data 0.265 (0.319) loss_u loss_u 0.8193 (0.7491) acc_u 21.8750 (31.3750) lr 4.9344e-07 eta 0:00:09
epoch [200/200] batch [30/45] time 0.304 (0.444) data 0.172 (0.314) loss_u loss_u 0.7993 (0.7481) acc_u 15.6250 (31.2500) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [35/45] time 0.397 (0.443) data 0.266 (0.312) loss_u loss_u 0.7944 (0.7509) acc_u 25.0000 (30.8036) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [40/45] time 0.565 (0.442) data 0.435 (0.311) loss_u loss_u 0.7461 (0.7539) acc_u 31.2500 (30.1562) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [45/45] time 0.449 (0.442) data 0.318 (0.312) loss_u loss_u 0.7100 (0.7542) acc_u 34.3750 (29.8611) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.125/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 5,597
* accuracy: 69.6%
* error: 30.4%
* macro_f1: 68.3%
Elapsed: 5:05:46
