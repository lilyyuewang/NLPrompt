***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.25', 'DATASET.NOISE_TYPE', 'sym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.25/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.25
  NOISE_TYPE: sym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.25/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.25/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2568
confident_label rate tensor(0.1381, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 433
clean true:432
clean false:1
clean_rate:0.9976905311778291
noisy true:136
noisy false:2567
after delete: len(clean_dataset) 433
after delete: len(noisy_dataset) 2703
epoch [1/200] batch [5/13] time 0.522 (0.516) data 0.392 (0.361) loss_x loss_x 3.0117 (3.0992) acc_x 34.3750 (35.0000) lr 1.0000e-05 eta 0:00:04
epoch [1/200] batch [10/13] time 0.515 (0.539) data 0.385 (0.396) loss_x loss_x 2.3223 (2.8176) acc_x 46.8750 (42.1875) lr 1.0000e-05 eta 0:00:01
epoch [1/200] batch [5/84] time 0.371 (0.518) data 0.240 (0.380) loss_u loss_u 0.9565 (0.9620) acc_u 12.5000 (10.0000) lr 1.0000e-05 eta 0:00:40
epoch [1/200] batch [10/84] time 0.484 (0.521) data 0.353 (0.384) loss_u loss_u 0.9487 (0.9485) acc_u 12.5000 (12.1875) lr 1.0000e-05 eta 0:00:38
epoch [1/200] batch [15/84] time 0.420 (0.503) data 0.290 (0.367) loss_u loss_u 0.9453 (0.9436) acc_u 6.2500 (11.6667) lr 1.0000e-05 eta 0:00:34
epoch [1/200] batch [20/84] time 0.356 (0.501) data 0.226 (0.365) loss_u loss_u 0.9600 (0.9380) acc_u 3.1250 (11.8750) lr 1.0000e-05 eta 0:00:32
epoch [1/200] batch [25/84] time 0.435 (0.492) data 0.304 (0.357) loss_u loss_u 0.9321 (0.9334) acc_u 12.5000 (11.6250) lr 1.0000e-05 eta 0:00:29
epoch [1/200] batch [30/84] time 0.382 (0.481) data 0.253 (0.347) loss_u loss_u 0.9170 (0.9308) acc_u 9.3750 (11.9792) lr 1.0000e-05 eta 0:00:25
epoch [1/200] batch [35/84] time 0.413 (0.482) data 0.282 (0.348) loss_u loss_u 0.9048 (0.9294) acc_u 6.2500 (11.7857) lr 1.0000e-05 eta 0:00:23
epoch [1/200] batch [40/84] time 0.484 (0.483) data 0.353 (0.350) loss_u loss_u 0.9097 (0.9272) acc_u 12.5000 (12.0312) lr 1.0000e-05 eta 0:00:21
epoch [1/200] batch [45/84] time 0.371 (0.485) data 0.241 (0.352) loss_u loss_u 0.9233 (0.9246) acc_u 12.5000 (12.5000) lr 1.0000e-05 eta 0:00:18
epoch [1/200] batch [50/84] time 0.377 (0.478) data 0.246 (0.345) loss_u loss_u 0.9316 (0.9255) acc_u 15.6250 (12.6250) lr 1.0000e-05 eta 0:00:16
epoch [1/200] batch [55/84] time 0.446 (0.474) data 0.314 (0.341) loss_u loss_u 0.9131 (0.9244) acc_u 12.5000 (12.7273) lr 1.0000e-05 eta 0:00:13
epoch [1/200] batch [60/84] time 0.460 (0.471) data 0.330 (0.339) loss_u loss_u 0.8486 (0.9215) acc_u 28.1250 (13.1771) lr 1.0000e-05 eta 0:00:11
epoch [1/200] batch [65/84] time 0.461 (0.469) data 0.331 (0.336) loss_u loss_u 0.9009 (0.9213) acc_u 18.7500 (13.4615) lr 1.0000e-05 eta 0:00:08
epoch [1/200] batch [70/84] time 0.480 (0.468) data 0.349 (0.336) loss_u loss_u 0.9272 (0.9189) acc_u 15.6250 (13.8839) lr 1.0000e-05 eta 0:00:06
epoch [1/200] batch [75/84] time 0.546 (0.470) data 0.415 (0.338) loss_u loss_u 0.9111 (0.9189) acc_u 12.5000 (13.7500) lr 1.0000e-05 eta 0:00:04
epoch [1/200] batch [80/84] time 0.442 (0.471) data 0.311 (0.339) loss_u loss_u 0.9390 (0.9180) acc_u 6.2500 (13.7500) lr 1.0000e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2140
confident_label rate tensor(0.2408, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 755
clean true:752
clean false:3
clean_rate:0.9960264900662251
noisy true:244
noisy false:2137
after delete: len(clean_dataset) 755
after delete: len(noisy_dataset) 2381
epoch [2/200] batch [5/23] time 0.531 (0.484) data 0.400 (0.354) loss_x loss_x 2.0000 (2.2496) acc_x 46.8750 (46.2500) lr 2.0000e-03 eta 0:00:08
epoch [2/200] batch [10/23] time 0.461 (0.467) data 0.331 (0.337) loss_x loss_x 1.7871 (2.1093) acc_x 50.0000 (48.1250) lr 2.0000e-03 eta 0:00:06
epoch [2/200] batch [15/23] time 0.379 (0.454) data 0.249 (0.324) loss_x loss_x 1.9072 (2.0417) acc_x 53.1250 (50.2083) lr 2.0000e-03 eta 0:00:03
epoch [2/200] batch [20/23] time 0.479 (0.459) data 0.349 (0.329) loss_x loss_x 2.8965 (2.0635) acc_x 34.3750 (49.5312) lr 2.0000e-03 eta 0:00:01
epoch [2/200] batch [5/74] time 0.397 (0.467) data 0.267 (0.337) loss_u loss_u 0.7886 (0.8526) acc_u 34.3750 (25.6250) lr 2.0000e-03 eta 0:00:32
epoch [2/200] batch [10/74] time 0.466 (0.466) data 0.336 (0.336) loss_u loss_u 0.8911 (0.8635) acc_u 18.7500 (21.8750) lr 2.0000e-03 eta 0:00:29
epoch [2/200] batch [15/74] time 0.362 (0.460) data 0.232 (0.330) loss_u loss_u 0.8550 (0.8625) acc_u 21.8750 (20.6250) lr 2.0000e-03 eta 0:00:27
epoch [2/200] batch [20/74] time 0.484 (0.458) data 0.353 (0.328) loss_u loss_u 0.8828 (0.8588) acc_u 18.7500 (21.0938) lr 2.0000e-03 eta 0:00:24
epoch [2/200] batch [25/74] time 0.433 (0.458) data 0.302 (0.328) loss_u loss_u 0.7998 (0.8583) acc_u 28.1250 (20.8750) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [30/74] time 0.433 (0.457) data 0.301 (0.327) loss_u loss_u 0.8647 (0.8556) acc_u 12.5000 (20.6250) lr 2.0000e-03 eta 0:00:20
epoch [2/200] batch [35/74] time 0.682 (0.461) data 0.549 (0.331) loss_u loss_u 0.8628 (0.8563) acc_u 28.1250 (20.4464) lr 2.0000e-03 eta 0:00:17
epoch [2/200] batch [40/74] time 0.381 (0.461) data 0.250 (0.331) loss_u loss_u 0.8984 (0.8601) acc_u 15.6250 (20.0000) lr 2.0000e-03 eta 0:00:15
epoch [2/200] batch [45/74] time 0.417 (0.460) data 0.287 (0.329) loss_u loss_u 0.9043 (0.8585) acc_u 12.5000 (20.1389) lr 2.0000e-03 eta 0:00:13
epoch [2/200] batch [50/74] time 0.441 (0.455) data 0.311 (0.325) loss_u loss_u 0.8574 (0.8592) acc_u 12.5000 (19.8125) lr 2.0000e-03 eta 0:00:10
epoch [2/200] batch [55/74] time 0.417 (0.454) data 0.287 (0.324) loss_u loss_u 0.9253 (0.8580) acc_u 9.3750 (20.0568) lr 2.0000e-03 eta 0:00:08
epoch [2/200] batch [60/74] time 0.497 (0.452) data 0.366 (0.321) loss_u loss_u 0.8604 (0.8568) acc_u 15.6250 (20.0521) lr 2.0000e-03 eta 0:00:06
epoch [2/200] batch [65/74] time 0.571 (0.452) data 0.441 (0.322) loss_u loss_u 0.8940 (0.8577) acc_u 15.6250 (19.9038) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [70/74] time 0.395 (0.450) data 0.264 (0.320) loss_u loss_u 0.9009 (0.8582) acc_u 12.5000 (20.0446) lr 2.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1885
confident_label rate tensor(0.3004, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 942
clean true:941
clean false:1
clean_rate:0.9989384288747346
noisy true:310
noisy false:1884
after delete: len(clean_dataset) 942
after delete: len(noisy_dataset) 2194
epoch [3/200] batch [5/29] time 0.533 (0.463) data 0.404 (0.333) loss_x loss_x 1.4277 (1.4832) acc_x 46.8750 (55.6250) lr 1.9999e-03 eta 0:00:11
epoch [3/200] batch [10/29] time 0.553 (0.457) data 0.422 (0.327) loss_x loss_x 1.7090 (1.6145) acc_x 59.3750 (56.8750) lr 1.9999e-03 eta 0:00:08
epoch [3/200] batch [15/29] time 0.525 (0.476) data 0.395 (0.346) loss_x loss_x 1.1562 (1.5667) acc_x 71.8750 (59.1667) lr 1.9999e-03 eta 0:00:06
epoch [3/200] batch [20/29] time 0.428 (0.460) data 0.299 (0.330) loss_x loss_x 1.0020 (1.5477) acc_x 71.8750 (60.3125) lr 1.9999e-03 eta 0:00:04
epoch [3/200] batch [25/29] time 0.542 (0.465) data 0.410 (0.335) loss_x loss_x 1.2920 (1.5454) acc_x 59.3750 (59.2500) lr 1.9999e-03 eta 0:00:01
epoch [3/200] batch [5/68] time 0.486 (0.469) data 0.355 (0.339) loss_u loss_u 0.8848 (0.8708) acc_u 25.0000 (15.0000) lr 1.9999e-03 eta 0:00:29
epoch [3/200] batch [10/68] time 0.437 (0.465) data 0.306 (0.335) loss_u loss_u 0.8848 (0.8763) acc_u 15.6250 (15.6250) lr 1.9999e-03 eta 0:00:26
epoch [3/200] batch [15/68] time 0.462 (0.461) data 0.331 (0.331) loss_u loss_u 0.8579 (0.8722) acc_u 15.6250 (17.0833) lr 1.9999e-03 eta 0:00:24
epoch [3/200] batch [20/68] time 0.392 (0.454) data 0.261 (0.324) loss_u loss_u 0.8447 (0.8665) acc_u 25.0000 (18.1250) lr 1.9999e-03 eta 0:00:21
epoch [3/200] batch [25/68] time 0.481 (0.449) data 0.349 (0.318) loss_u loss_u 0.8325 (0.8630) acc_u 21.8750 (18.1250) lr 1.9999e-03 eta 0:00:19
epoch [3/200] batch [30/68] time 0.399 (0.449) data 0.268 (0.318) loss_u loss_u 0.8491 (0.8666) acc_u 21.8750 (17.8125) lr 1.9999e-03 eta 0:00:17
epoch [3/200] batch [35/68] time 0.597 (0.448) data 0.466 (0.317) loss_u loss_u 0.8799 (0.8586) acc_u 12.5000 (18.8393) lr 1.9999e-03 eta 0:00:14
epoch [3/200] batch [40/68] time 0.449 (0.451) data 0.319 (0.321) loss_u loss_u 0.9224 (0.8594) acc_u 12.5000 (18.8281) lr 1.9999e-03 eta 0:00:12
epoch [3/200] batch [45/68] time 0.392 (0.451) data 0.263 (0.320) loss_u loss_u 0.8662 (0.8612) acc_u 21.8750 (18.7500) lr 1.9999e-03 eta 0:00:10
epoch [3/200] batch [50/68] time 0.351 (0.447) data 0.221 (0.317) loss_u loss_u 0.8931 (0.8625) acc_u 21.8750 (18.8125) lr 1.9999e-03 eta 0:00:08
epoch [3/200] batch [55/68] time 0.558 (0.446) data 0.428 (0.315) loss_u loss_u 0.8853 (0.8635) acc_u 12.5000 (18.6364) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [60/68] time 0.595 (0.445) data 0.463 (0.314) loss_u loss_u 0.8843 (0.8630) acc_u 9.3750 (18.5938) lr 1.9999e-03 eta 0:00:03
epoch [3/200] batch [65/68] time 0.471 (0.447) data 0.340 (0.316) loss_u loss_u 0.8477 (0.8652) acc_u 25.0000 (18.3173) lr 1.9999e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1853
confident_label rate tensor(0.3106, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 974
clean true:974
clean false:0
clean_rate:1.0
noisy true:309
noisy false:1853
after delete: len(clean_dataset) 974
after delete: len(noisy_dataset) 2162
epoch [4/200] batch [5/30] time 0.470 (0.453) data 0.340 (0.323) loss_x loss_x 1.6338 (1.7609) acc_x 59.3750 (57.5000) lr 1.9995e-03 eta 0:00:11
epoch [4/200] batch [10/30] time 0.548 (0.476) data 0.418 (0.345) loss_x loss_x 1.3477 (1.7141) acc_x 62.5000 (55.3125) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [15/30] time 0.453 (0.464) data 0.323 (0.334) loss_x loss_x 1.4854 (1.6662) acc_x 68.7500 (57.7083) lr 1.9995e-03 eta 0:00:06
epoch [4/200] batch [20/30] time 0.438 (0.459) data 0.308 (0.329) loss_x loss_x 1.3750 (1.6094) acc_x 59.3750 (58.5938) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [25/30] time 0.466 (0.460) data 0.336 (0.330) loss_x loss_x 1.7129 (1.6204) acc_x 50.0000 (58.2500) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [30/30] time 0.616 (0.461) data 0.486 (0.331) loss_x loss_x 1.7900 (1.6110) acc_x 56.2500 (58.6458) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/67] time 0.365 (0.454) data 0.234 (0.324) loss_u loss_u 0.8657 (0.8549) acc_u 18.7500 (16.8750) lr 1.9995e-03 eta 0:00:28
epoch [4/200] batch [10/67] time 0.300 (0.451) data 0.170 (0.321) loss_u loss_u 0.8765 (0.8651) acc_u 9.3750 (15.9375) lr 1.9995e-03 eta 0:00:25
epoch [4/200] batch [15/67] time 0.525 (0.449) data 0.394 (0.319) loss_u loss_u 0.8848 (0.8615) acc_u 12.5000 (17.2917) lr 1.9995e-03 eta 0:00:23
epoch [4/200] batch [20/67] time 0.478 (0.454) data 0.347 (0.324) loss_u loss_u 0.8252 (0.8669) acc_u 15.6250 (16.2500) lr 1.9995e-03 eta 0:00:21
epoch [4/200] batch [25/67] time 0.412 (0.449) data 0.282 (0.319) loss_u loss_u 0.8403 (0.8654) acc_u 25.0000 (17.1250) lr 1.9995e-03 eta 0:00:18
epoch [4/200] batch [30/67] time 0.379 (0.453) data 0.248 (0.322) loss_u loss_u 0.8340 (0.8690) acc_u 21.8750 (16.2500) lr 1.9995e-03 eta 0:00:16
epoch [4/200] batch [35/67] time 0.381 (0.453) data 0.248 (0.323) loss_u loss_u 0.9370 (0.8646) acc_u 9.3750 (16.7857) lr 1.9995e-03 eta 0:00:14
epoch [4/200] batch [40/67] time 0.598 (0.457) data 0.465 (0.326) loss_u loss_u 0.8647 (0.8642) acc_u 15.6250 (17.4219) lr 1.9995e-03 eta 0:00:12
epoch [4/200] batch [45/67] time 0.354 (0.453) data 0.222 (0.323) loss_u loss_u 0.8794 (0.8641) acc_u 18.7500 (17.3611) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [50/67] time 0.409 (0.452) data 0.277 (0.322) loss_u loss_u 0.7754 (0.8629) acc_u 31.2500 (17.5000) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [55/67] time 0.469 (0.455) data 0.338 (0.324) loss_u loss_u 0.8525 (0.8618) acc_u 12.5000 (17.4432) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [60/67] time 0.408 (0.451) data 0.276 (0.320) loss_u loss_u 0.9053 (0.8639) acc_u 12.5000 (17.0312) lr 1.9995e-03 eta 0:00:03
epoch [4/200] batch [65/67] time 0.499 (0.452) data 0.366 (0.321) loss_u loss_u 0.8657 (0.8668) acc_u 25.0000 (16.9231) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1860
confident_label rate tensor(0.3058, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 959
clean true:959
clean false:0
clean_rate:1.0
noisy true:317
noisy false:1860
after delete: len(clean_dataset) 959
after delete: len(noisy_dataset) 2177
epoch [5/200] batch [5/29] time 0.514 (0.449) data 0.384 (0.318) loss_x loss_x 1.7080 (1.6236) acc_x 62.5000 (58.1250) lr 1.9989e-03 eta 0:00:10
epoch [5/200] batch [10/29] time 0.380 (0.457) data 0.250 (0.326) loss_x loss_x 1.9580 (1.6345) acc_x 43.7500 (58.7500) lr 1.9989e-03 eta 0:00:08
epoch [5/200] batch [15/29] time 0.468 (0.455) data 0.338 (0.324) loss_x loss_x 2.3750 (1.6289) acc_x 46.8750 (58.1250) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [20/29] time 0.395 (0.461) data 0.266 (0.331) loss_x loss_x 1.3164 (1.5645) acc_x 75.0000 (60.0000) lr 1.9989e-03 eta 0:00:04
epoch [5/200] batch [25/29] time 0.554 (0.457) data 0.424 (0.327) loss_x loss_x 1.4170 (1.5550) acc_x 62.5000 (59.8750) lr 1.9989e-03 eta 0:00:01
epoch [5/200] batch [5/68] time 0.365 (0.450) data 0.235 (0.319) loss_u loss_u 0.8779 (0.8645) acc_u 9.3750 (17.5000) lr 1.9989e-03 eta 0:00:28
epoch [5/200] batch [10/68] time 0.418 (0.444) data 0.287 (0.314) loss_u loss_u 0.8911 (0.8753) acc_u 15.6250 (15.3125) lr 1.9989e-03 eta 0:00:25
epoch [5/200] batch [15/68] time 0.442 (0.443) data 0.311 (0.312) loss_u loss_u 0.7988 (0.8665) acc_u 25.0000 (16.0417) lr 1.9989e-03 eta 0:00:23
epoch [5/200] batch [20/68] time 0.376 (0.440) data 0.245 (0.309) loss_u loss_u 0.8555 (0.8640) acc_u 18.7500 (16.8750) lr 1.9989e-03 eta 0:00:21
epoch [5/200] batch [25/68] time 0.406 (0.436) data 0.273 (0.305) loss_u loss_u 0.8926 (0.8653) acc_u 21.8750 (17.1250) lr 1.9989e-03 eta 0:00:18
epoch [5/200] batch [30/68] time 0.463 (0.433) data 0.332 (0.302) loss_u loss_u 0.8140 (0.8667) acc_u 25.0000 (16.5625) lr 1.9989e-03 eta 0:00:16
epoch [5/200] batch [35/68] time 0.370 (0.437) data 0.239 (0.306) loss_u loss_u 0.8647 (0.8686) acc_u 15.6250 (16.2500) lr 1.9989e-03 eta 0:00:14
epoch [5/200] batch [40/68] time 0.354 (0.436) data 0.222 (0.305) loss_u loss_u 0.8442 (0.8670) acc_u 25.0000 (16.4844) lr 1.9989e-03 eta 0:00:12
epoch [5/200] batch [45/68] time 0.630 (0.440) data 0.499 (0.309) loss_u loss_u 0.8994 (0.8665) acc_u 12.5000 (16.6667) lr 1.9989e-03 eta 0:00:10
epoch [5/200] batch [50/68] time 0.423 (0.439) data 0.292 (0.308) loss_u loss_u 0.8052 (0.8602) acc_u 37.5000 (17.9375) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [55/68] time 0.459 (0.436) data 0.328 (0.306) loss_u loss_u 0.9214 (0.8622) acc_u 9.3750 (17.7273) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [60/68] time 0.411 (0.436) data 0.281 (0.305) loss_u loss_u 0.8379 (0.8633) acc_u 18.7500 (17.5521) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [65/68] time 0.535 (0.439) data 0.404 (0.308) loss_u loss_u 0.8950 (0.8624) acc_u 15.6250 (17.7404) lr 1.9989e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1824
confident_label rate tensor(0.3195, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1002
clean true:999
clean false:3
clean_rate:0.9970059880239521
noisy true:313
noisy false:1821
after delete: len(clean_dataset) 1002
after delete: len(noisy_dataset) 2134
epoch [6/200] batch [5/31] time 0.516 (0.449) data 0.386 (0.319) loss_x loss_x 1.4414 (1.3439) acc_x 56.2500 (62.5000) lr 1.9980e-03 eta 0:00:11
epoch [6/200] batch [10/31] time 0.412 (0.457) data 0.282 (0.327) loss_x loss_x 1.7344 (1.5144) acc_x 50.0000 (58.7500) lr 1.9980e-03 eta 0:00:09
epoch [6/200] batch [15/31] time 0.457 (0.468) data 0.326 (0.338) loss_x loss_x 1.4053 (1.5047) acc_x 65.6250 (59.7917) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [20/31] time 0.476 (0.466) data 0.346 (0.336) loss_x loss_x 1.5801 (1.5351) acc_x 68.7500 (60.3125) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [25/31] time 0.510 (0.473) data 0.380 (0.343) loss_x loss_x 1.7285 (1.5479) acc_x 59.3750 (60.8750) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [30/31] time 0.479 (0.473) data 0.350 (0.343) loss_x loss_x 1.7129 (1.5780) acc_x 46.8750 (60.5208) lr 1.9980e-03 eta 0:00:00
epoch [6/200] batch [5/66] time 0.445 (0.467) data 0.314 (0.337) loss_u loss_u 0.8433 (0.8796) acc_u 25.0000 (15.6250) lr 1.9980e-03 eta 0:00:28
epoch [6/200] batch [10/66] time 0.384 (0.461) data 0.255 (0.331) loss_u loss_u 0.8682 (0.8853) acc_u 15.6250 (14.6875) lr 1.9980e-03 eta 0:00:25
epoch [6/200] batch [15/66] time 0.478 (0.458) data 0.349 (0.328) loss_u loss_u 0.9087 (0.8782) acc_u 15.6250 (16.0417) lr 1.9980e-03 eta 0:00:23
epoch [6/200] batch [20/66] time 0.376 (0.451) data 0.246 (0.321) loss_u loss_u 0.8804 (0.8751) acc_u 15.6250 (16.7188) lr 1.9980e-03 eta 0:00:20
epoch [6/200] batch [25/66] time 0.358 (0.447) data 0.228 (0.317) loss_u loss_u 0.8091 (0.8707) acc_u 28.1250 (17.0000) lr 1.9980e-03 eta 0:00:18
epoch [6/200] batch [30/66] time 0.383 (0.444) data 0.252 (0.314) loss_u loss_u 0.8281 (0.8663) acc_u 28.1250 (18.1250) lr 1.9980e-03 eta 0:00:15
epoch [6/200] batch [35/66] time 0.449 (0.447) data 0.318 (0.317) loss_u loss_u 0.8652 (0.8636) acc_u 15.6250 (18.2143) lr 1.9980e-03 eta 0:00:13
epoch [6/200] batch [40/66] time 0.481 (0.449) data 0.350 (0.319) loss_u loss_u 0.9287 (0.8635) acc_u 9.3750 (18.0469) lr 1.9980e-03 eta 0:00:11
epoch [6/200] batch [45/66] time 0.532 (0.451) data 0.401 (0.320) loss_u loss_u 0.8545 (0.8667) acc_u 21.8750 (17.8472) lr 1.9980e-03 eta 0:00:09
epoch [6/200] batch [50/66] time 0.371 (0.449) data 0.240 (0.318) loss_u loss_u 0.8447 (0.8662) acc_u 21.8750 (17.8750) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [55/66] time 0.325 (0.443) data 0.194 (0.313) loss_u loss_u 0.8345 (0.8675) acc_u 18.7500 (17.7841) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [60/66] time 0.377 (0.443) data 0.246 (0.313) loss_u loss_u 0.8408 (0.8658) acc_u 21.8750 (17.9688) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [65/66] time 0.569 (0.442) data 0.438 (0.312) loss_u loss_u 0.8628 (0.8651) acc_u 21.8750 (18.0769) lr 1.9980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1801
confident_label rate tensor(0.3272, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1026
clean true:1025
clean false:1
clean_rate:0.9990253411306043
noisy true:310
noisy false:1800
after delete: len(clean_dataset) 1026
after delete: len(noisy_dataset) 2110
epoch [7/200] batch [5/32] time 0.450 (0.393) data 0.320 (0.263) loss_x loss_x 1.4502 (1.5311) acc_x 68.7500 (65.6250) lr 1.9969e-03 eta 0:00:10
epoch [7/200] batch [10/32] time 0.397 (0.414) data 0.267 (0.284) loss_x loss_x 1.7002 (1.6368) acc_x 56.2500 (59.6875) lr 1.9969e-03 eta 0:00:09
epoch [7/200] batch [15/32] time 0.367 (0.416) data 0.237 (0.286) loss_x loss_x 1.3760 (1.5564) acc_x 59.3750 (62.5000) lr 1.9969e-03 eta 0:00:07
epoch [7/200] batch [20/32] time 0.417 (0.412) data 0.287 (0.282) loss_x loss_x 1.6162 (1.6105) acc_x 59.3750 (61.2500) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [25/32] time 0.589 (0.438) data 0.458 (0.307) loss_x loss_x 1.6924 (1.5679) acc_x 50.0000 (61.6250) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [30/32] time 0.474 (0.435) data 0.344 (0.305) loss_x loss_x 1.3730 (1.5369) acc_x 65.6250 (61.1458) lr 1.9969e-03 eta 0:00:00
epoch [7/200] batch [5/65] time 0.517 (0.437) data 0.386 (0.306) loss_u loss_u 0.8735 (0.8309) acc_u 18.7500 (23.1250) lr 1.9969e-03 eta 0:00:26
epoch [7/200] batch [10/65] time 0.355 (0.438) data 0.224 (0.308) loss_u loss_u 0.9429 (0.8440) acc_u 3.1250 (20.0000) lr 1.9969e-03 eta 0:00:24
epoch [7/200] batch [15/65] time 0.357 (0.432) data 0.226 (0.301) loss_u loss_u 0.8442 (0.8564) acc_u 18.7500 (18.9583) lr 1.9969e-03 eta 0:00:21
epoch [7/200] batch [20/65] time 0.528 (0.431) data 0.397 (0.300) loss_u loss_u 0.8789 (0.8590) acc_u 12.5000 (18.2812) lr 1.9969e-03 eta 0:00:19
epoch [7/200] batch [25/65] time 0.404 (0.436) data 0.272 (0.306) loss_u loss_u 0.8535 (0.8583) acc_u 21.8750 (18.1250) lr 1.9969e-03 eta 0:00:17
epoch [7/200] batch [30/65] time 0.350 (0.433) data 0.220 (0.302) loss_u loss_u 0.8687 (0.8554) acc_u 15.6250 (18.6458) lr 1.9969e-03 eta 0:00:15
epoch [7/200] batch [35/65] time 0.386 (0.432) data 0.255 (0.302) loss_u loss_u 0.9175 (0.8600) acc_u 3.1250 (18.0357) lr 1.9969e-03 eta 0:00:12
epoch [7/200] batch [40/65] time 0.347 (0.432) data 0.216 (0.301) loss_u loss_u 0.9087 (0.8617) acc_u 12.5000 (18.2031) lr 1.9969e-03 eta 0:00:10
epoch [7/200] batch [45/65] time 0.385 (0.433) data 0.255 (0.302) loss_u loss_u 0.8682 (0.8621) acc_u 9.3750 (17.8472) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [50/65] time 0.811 (0.436) data 0.681 (0.305) loss_u loss_u 0.8604 (0.8624) acc_u 21.8750 (18.3750) lr 1.9969e-03 eta 0:00:06
epoch [7/200] batch [55/65] time 0.580 (0.438) data 0.449 (0.307) loss_u loss_u 0.8735 (0.8645) acc_u 12.5000 (17.8977) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [60/65] time 0.465 (0.438) data 0.334 (0.307) loss_u loss_u 0.8608 (0.8610) acc_u 18.7500 (18.4375) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [65/65] time 0.463 (0.437) data 0.332 (0.306) loss_u loss_u 0.8911 (0.8618) acc_u 15.6250 (18.2692) lr 1.9969e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1782
confident_label rate tensor(0.3294, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1033
clean true:1033
clean false:0
clean_rate:1.0
noisy true:321
noisy false:1782
after delete: len(clean_dataset) 1033
after delete: len(noisy_dataset) 2103
epoch [8/200] batch [5/32] time 0.392 (0.437) data 0.262 (0.306) loss_x loss_x 1.5254 (1.5123) acc_x 56.2500 (62.5000) lr 1.9956e-03 eta 0:00:11
epoch [8/200] batch [10/32] time 0.528 (0.450) data 0.398 (0.319) loss_x loss_x 1.3691 (1.4872) acc_x 75.0000 (60.9375) lr 1.9956e-03 eta 0:00:09
epoch [8/200] batch [15/32] time 0.734 (0.460) data 0.602 (0.329) loss_x loss_x 1.4102 (1.5255) acc_x 68.7500 (62.0833) lr 1.9956e-03 eta 0:00:07
epoch [8/200] batch [20/32] time 0.348 (0.460) data 0.218 (0.329) loss_x loss_x 1.9248 (1.5233) acc_x 50.0000 (61.8750) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [25/32] time 0.490 (0.462) data 0.359 (0.332) loss_x loss_x 1.4414 (1.5060) acc_x 68.7500 (61.1250) lr 1.9956e-03 eta 0:00:03
epoch [8/200] batch [30/32] time 0.484 (0.459) data 0.354 (0.328) loss_x loss_x 1.5605 (1.5256) acc_x 59.3750 (61.5625) lr 1.9956e-03 eta 0:00:00
epoch [8/200] batch [5/65] time 0.451 (0.462) data 0.322 (0.332) loss_u loss_u 0.8706 (0.8661) acc_u 21.8750 (15.0000) lr 1.9956e-03 eta 0:00:27
epoch [8/200] batch [10/65] time 0.376 (0.454) data 0.246 (0.324) loss_u loss_u 0.8735 (0.8588) acc_u 21.8750 (17.8125) lr 1.9956e-03 eta 0:00:24
epoch [8/200] batch [15/65] time 0.425 (0.450) data 0.293 (0.319) loss_u loss_u 0.9106 (0.8642) acc_u 15.6250 (17.9167) lr 1.9956e-03 eta 0:00:22
epoch [8/200] batch [20/65] time 0.446 (0.446) data 0.316 (0.316) loss_u loss_u 0.8408 (0.8686) acc_u 18.7500 (16.7188) lr 1.9956e-03 eta 0:00:20
epoch [8/200] batch [25/65] time 0.303 (0.443) data 0.173 (0.313) loss_u loss_u 0.8657 (0.8688) acc_u 18.7500 (17.2500) lr 1.9956e-03 eta 0:00:17
epoch [8/200] batch [30/65] time 0.365 (0.443) data 0.234 (0.313) loss_u loss_u 0.9033 (0.8695) acc_u 9.3750 (16.9792) lr 1.9956e-03 eta 0:00:15
epoch [8/200] batch [35/65] time 0.431 (0.442) data 0.299 (0.311) loss_u loss_u 0.7910 (0.8606) acc_u 25.0000 (18.3929) lr 1.9956e-03 eta 0:00:13
epoch [8/200] batch [40/65] time 0.480 (0.442) data 0.349 (0.311) loss_u loss_u 0.8418 (0.8630) acc_u 25.0000 (18.3594) lr 1.9956e-03 eta 0:00:11
epoch [8/200] batch [45/65] time 0.423 (0.438) data 0.292 (0.307) loss_u loss_u 0.8672 (0.8656) acc_u 15.6250 (18.1250) lr 1.9956e-03 eta 0:00:08
epoch [8/200] batch [50/65] time 0.430 (0.434) data 0.301 (0.304) loss_u loss_u 0.9009 (0.8676) acc_u 15.6250 (18.0625) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [55/65] time 0.544 (0.436) data 0.413 (0.305) loss_u loss_u 0.7871 (0.8656) acc_u 28.1250 (18.2955) lr 1.9956e-03 eta 0:00:04
epoch [8/200] batch [60/65] time 0.475 (0.436) data 0.344 (0.306) loss_u loss_u 0.9077 (0.8665) acc_u 9.3750 (18.2292) lr 1.9956e-03 eta 0:00:02
epoch [8/200] batch [65/65] time 0.343 (0.437) data 0.212 (0.307) loss_u loss_u 0.8618 (0.8656) acc_u 21.8750 (18.5096) lr 1.9956e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1801
confident_label rate tensor(0.3268, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1025
clean true:1022
clean false:3
clean_rate:0.9970731707317073
noisy true:313
noisy false:1798
after delete: len(clean_dataset) 1025
after delete: len(noisy_dataset) 2111
epoch [9/200] batch [5/32] time 0.398 (0.434) data 0.267 (0.303) loss_x loss_x 1.3838 (1.4113) acc_x 65.6250 (63.1250) lr 1.9940e-03 eta 0:00:11
epoch [9/200] batch [10/32] time 0.472 (0.450) data 0.341 (0.319) loss_x loss_x 1.4688 (1.4668) acc_x 56.2500 (64.3750) lr 1.9940e-03 eta 0:00:09
epoch [9/200] batch [15/32] time 0.540 (0.454) data 0.409 (0.324) loss_x loss_x 1.6514 (1.4470) acc_x 50.0000 (65.4167) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [20/32] time 0.501 (0.461) data 0.369 (0.331) loss_x loss_x 1.5898 (1.4389) acc_x 56.2500 (65.0000) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [25/32] time 0.386 (0.465) data 0.256 (0.334) loss_x loss_x 1.9385 (1.4334) acc_x 40.6250 (64.7500) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [30/32] time 0.458 (0.461) data 0.327 (0.331) loss_x loss_x 1.9268 (1.4576) acc_x 53.1250 (63.8542) lr 1.9940e-03 eta 0:00:00
epoch [9/200] batch [5/65] time 0.438 (0.455) data 0.306 (0.324) loss_u loss_u 0.9014 (0.8715) acc_u 12.5000 (16.8750) lr 1.9940e-03 eta 0:00:27
epoch [9/200] batch [10/65] time 0.425 (0.449) data 0.294 (0.319) loss_u loss_u 0.8989 (0.8578) acc_u 12.5000 (18.4375) lr 1.9940e-03 eta 0:00:24
epoch [9/200] batch [15/65] time 0.495 (0.454) data 0.365 (0.323) loss_u loss_u 0.8599 (0.8681) acc_u 12.5000 (17.5000) lr 1.9940e-03 eta 0:00:22
epoch [9/200] batch [20/65] time 0.401 (0.447) data 0.270 (0.317) loss_u loss_u 0.7866 (0.8642) acc_u 25.0000 (17.9688) lr 1.9940e-03 eta 0:00:20
epoch [9/200] batch [25/65] time 0.446 (0.445) data 0.315 (0.315) loss_u loss_u 0.8496 (0.8612) acc_u 21.8750 (18.8750) lr 1.9940e-03 eta 0:00:17
epoch [9/200] batch [30/65] time 0.477 (0.448) data 0.345 (0.317) loss_u loss_u 0.8672 (0.8618) acc_u 18.7500 (18.8542) lr 1.9940e-03 eta 0:00:15
epoch [9/200] batch [35/65] time 0.499 (0.450) data 0.366 (0.319) loss_u loss_u 0.9043 (0.8652) acc_u 18.7500 (18.3036) lr 1.9940e-03 eta 0:00:13
epoch [9/200] batch [40/65] time 0.477 (0.449) data 0.345 (0.318) loss_u loss_u 0.8076 (0.8650) acc_u 25.0000 (18.3594) lr 1.9940e-03 eta 0:00:11
epoch [9/200] batch [45/65] time 0.429 (0.447) data 0.298 (0.316) loss_u loss_u 0.8750 (0.8665) acc_u 12.5000 (17.8472) lr 1.9940e-03 eta 0:00:08
epoch [9/200] batch [50/65] time 0.394 (0.448) data 0.264 (0.317) loss_u loss_u 0.7979 (0.8647) acc_u 28.1250 (18.0625) lr 1.9940e-03 eta 0:00:06
epoch [9/200] batch [55/65] time 0.416 (0.448) data 0.286 (0.317) loss_u loss_u 0.9141 (0.8660) acc_u 18.7500 (17.8977) lr 1.9940e-03 eta 0:00:04
epoch [9/200] batch [60/65] time 0.439 (0.445) data 0.308 (0.314) loss_u loss_u 0.8867 (0.8649) acc_u 12.5000 (17.8125) lr 1.9940e-03 eta 0:00:02
epoch [9/200] batch [65/65] time 0.303 (0.441) data 0.173 (0.310) loss_u loss_u 0.8706 (0.8659) acc_u 18.7500 (17.7885) lr 1.9940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1764
confident_label rate tensor(0.3329, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1044
clean true:1042
clean false:2
clean_rate:0.9980842911877394
noisy true:330
noisy false:1762
after delete: len(clean_dataset) 1044
after delete: len(noisy_dataset) 2092
epoch [10/200] batch [5/32] time 0.519 (0.484) data 0.389 (0.354) loss_x loss_x 1.1807 (1.3781) acc_x 65.6250 (64.3750) lr 1.9921e-03 eta 0:00:13
epoch [10/200] batch [10/32] time 0.420 (0.486) data 0.290 (0.356) loss_x loss_x 1.0840 (1.3492) acc_x 62.5000 (63.4375) lr 1.9921e-03 eta 0:00:10
epoch [10/200] batch [15/32] time 0.409 (0.481) data 0.279 (0.351) loss_x loss_x 1.6279 (1.4359) acc_x 59.3750 (62.0833) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [20/32] time 0.482 (0.480) data 0.351 (0.350) loss_x loss_x 1.4570 (1.4628) acc_x 53.1250 (61.5625) lr 1.9921e-03 eta 0:00:05
epoch [10/200] batch [25/32] time 0.376 (0.486) data 0.246 (0.356) loss_x loss_x 1.4336 (1.4808) acc_x 56.2500 (60.8750) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [30/32] time 0.388 (0.472) data 0.258 (0.342) loss_x loss_x 1.5732 (1.4756) acc_x 59.3750 (60.7292) lr 1.9921e-03 eta 0:00:00
epoch [10/200] batch [5/65] time 0.351 (0.456) data 0.221 (0.326) loss_u loss_u 0.8833 (0.8636) acc_u 12.5000 (16.2500) lr 1.9921e-03 eta 0:00:27
epoch [10/200] batch [10/65] time 0.356 (0.453) data 0.224 (0.323) loss_u loss_u 0.7676 (0.8460) acc_u 25.0000 (18.7500) lr 1.9921e-03 eta 0:00:24
epoch [10/200] batch [15/65] time 0.633 (0.456) data 0.502 (0.326) loss_u loss_u 0.8052 (0.8501) acc_u 28.1250 (18.7500) lr 1.9921e-03 eta 0:00:22
epoch [10/200] batch [20/65] time 0.342 (0.450) data 0.211 (0.320) loss_u loss_u 0.7725 (0.8505) acc_u 28.1250 (18.9062) lr 1.9921e-03 eta 0:00:20
epoch [10/200] batch [25/65] time 0.409 (0.447) data 0.279 (0.317) loss_u loss_u 0.8647 (0.8555) acc_u 12.5000 (18.1250) lr 1.9921e-03 eta 0:00:17
epoch [10/200] batch [30/65] time 0.506 (0.446) data 0.375 (0.316) loss_u loss_u 0.8628 (0.8540) acc_u 18.7500 (18.8542) lr 1.9921e-03 eta 0:00:15
epoch [10/200] batch [35/65] time 0.427 (0.446) data 0.295 (0.315) loss_u loss_u 0.7998 (0.8543) acc_u 34.3750 (19.0179) lr 1.9921e-03 eta 0:00:13
epoch [10/200] batch [40/65] time 0.434 (0.446) data 0.303 (0.316) loss_u loss_u 0.9453 (0.8603) acc_u 3.1250 (18.2031) lr 1.9921e-03 eta 0:00:11
epoch [10/200] batch [45/65] time 0.401 (0.444) data 0.272 (0.313) loss_u loss_u 0.8560 (0.8602) acc_u 21.8750 (18.4028) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [50/65] time 0.401 (0.446) data 0.269 (0.315) loss_u loss_u 0.8408 (0.8582) acc_u 15.6250 (18.3750) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [55/65] time 0.430 (0.447) data 0.299 (0.317) loss_u loss_u 0.7603 (0.8553) acc_u 25.0000 (18.7500) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [60/65] time 0.441 (0.444) data 0.310 (0.314) loss_u loss_u 0.8794 (0.8566) acc_u 12.5000 (18.6458) lr 1.9921e-03 eta 0:00:02
epoch [10/200] batch [65/65] time 0.444 (0.442) data 0.314 (0.312) loss_u loss_u 0.8057 (0.8572) acc_u 21.8750 (18.4135) lr 1.9921e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1784
confident_label rate tensor(0.3297, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1034
clean true:1033
clean false:1
clean_rate:0.9990328820116054
noisy true:319
noisy false:1783
after delete: len(clean_dataset) 1034
after delete: len(noisy_dataset) 2102
epoch [11/200] batch [5/32] time 0.494 (0.448) data 0.363 (0.318) loss_x loss_x 1.0078 (1.4621) acc_x 71.8750 (63.1250) lr 1.9900e-03 eta 0:00:12
epoch [11/200] batch [10/32] time 0.635 (0.467) data 0.505 (0.337) loss_x loss_x 2.0410 (1.6995) acc_x 53.1250 (57.8125) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [15/32] time 0.456 (0.473) data 0.326 (0.343) loss_x loss_x 1.3818 (1.6187) acc_x 59.3750 (58.1250) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [20/32] time 0.596 (0.473) data 0.465 (0.343) loss_x loss_x 1.7217 (1.5813) acc_x 59.3750 (57.6562) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [25/32] time 0.543 (0.469) data 0.413 (0.339) loss_x loss_x 1.4258 (1.5714) acc_x 71.8750 (59.0000) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [30/32] time 0.462 (0.473) data 0.332 (0.343) loss_x loss_x 1.2939 (1.5298) acc_x 62.5000 (60.1042) lr 1.9900e-03 eta 0:00:00
epoch [11/200] batch [5/65] time 0.455 (0.472) data 0.325 (0.342) loss_u loss_u 0.8452 (0.8226) acc_u 21.8750 (22.5000) lr 1.9900e-03 eta 0:00:28
epoch [11/200] batch [10/65] time 0.449 (0.470) data 0.318 (0.340) loss_u loss_u 0.8740 (0.8519) acc_u 25.0000 (19.6875) lr 1.9900e-03 eta 0:00:25
epoch [11/200] batch [15/65] time 0.479 (0.467) data 0.347 (0.337) loss_u loss_u 0.9072 (0.8555) acc_u 15.6250 (18.7500) lr 1.9900e-03 eta 0:00:23
epoch [11/200] batch [20/65] time 0.358 (0.464) data 0.226 (0.334) loss_u loss_u 0.7979 (0.8511) acc_u 28.1250 (19.8438) lr 1.9900e-03 eta 0:00:20
epoch [11/200] batch [25/65] time 0.369 (0.458) data 0.238 (0.327) loss_u loss_u 0.8838 (0.8550) acc_u 18.7500 (19.1250) lr 1.9900e-03 eta 0:00:18
epoch [11/200] batch [30/65] time 0.415 (0.458) data 0.284 (0.328) loss_u loss_u 0.8213 (0.8537) acc_u 25.0000 (18.7500) lr 1.9900e-03 eta 0:00:16
epoch [11/200] batch [35/65] time 0.398 (0.454) data 0.267 (0.323) loss_u loss_u 0.7939 (0.8508) acc_u 31.2500 (19.2857) lr 1.9900e-03 eta 0:00:13
epoch [11/200] batch [40/65] time 0.414 (0.447) data 0.284 (0.317) loss_u loss_u 0.8979 (0.8517) acc_u 3.1250 (18.9062) lr 1.9900e-03 eta 0:00:11
epoch [11/200] batch [45/65] time 0.297 (0.445) data 0.167 (0.314) loss_u loss_u 0.9019 (0.8548) acc_u 9.3750 (18.5417) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [50/65] time 0.531 (0.446) data 0.399 (0.315) loss_u loss_u 0.9087 (0.8550) acc_u 9.3750 (18.1250) lr 1.9900e-03 eta 0:00:06
epoch [11/200] batch [55/65] time 0.428 (0.444) data 0.297 (0.313) loss_u loss_u 0.8496 (0.8569) acc_u 15.6250 (17.8977) lr 1.9900e-03 eta 0:00:04
epoch [11/200] batch [60/65] time 0.450 (0.445) data 0.320 (0.315) loss_u loss_u 0.8086 (0.8565) acc_u 21.8750 (18.0208) lr 1.9900e-03 eta 0:00:02
epoch [11/200] batch [65/65] time 0.365 (0.445) data 0.234 (0.314) loss_u loss_u 0.8936 (0.8573) acc_u 12.5000 (18.0769) lr 1.9900e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1822
confident_label rate tensor(0.3221, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1010
clean true:1009
clean false:1
clean_rate:0.999009900990099
noisy true:305
noisy false:1821
after delete: len(clean_dataset) 1010
after delete: len(noisy_dataset) 2126
epoch [12/200] batch [5/31] time 0.589 (0.505) data 0.458 (0.374) loss_x loss_x 0.6865 (1.3591) acc_x 84.3750 (66.8750) lr 1.9877e-03 eta 0:00:13
epoch [12/200] batch [10/31] time 0.532 (0.483) data 0.401 (0.352) loss_x loss_x 1.5996 (1.4753) acc_x 62.5000 (63.4375) lr 1.9877e-03 eta 0:00:10
epoch [12/200] batch [15/31] time 0.421 (0.474) data 0.290 (0.343) loss_x loss_x 1.5996 (1.5308) acc_x 53.1250 (61.2500) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [20/31] time 0.498 (0.472) data 0.368 (0.342) loss_x loss_x 1.3564 (1.4981) acc_x 71.8750 (62.1875) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [25/31] time 0.376 (0.478) data 0.246 (0.347) loss_x loss_x 1.0830 (1.5089) acc_x 75.0000 (61.6250) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [30/31] time 0.415 (0.470) data 0.284 (0.339) loss_x loss_x 1.3477 (1.5220) acc_x 65.6250 (61.3542) lr 1.9877e-03 eta 0:00:00
epoch [12/200] batch [5/66] time 0.316 (0.464) data 0.186 (0.333) loss_u loss_u 0.8584 (0.8290) acc_u 12.5000 (23.1250) lr 1.9877e-03 eta 0:00:28
epoch [12/200] batch [10/66] time 0.481 (0.456) data 0.351 (0.326) loss_u loss_u 0.8804 (0.8515) acc_u 18.7500 (17.8125) lr 1.9877e-03 eta 0:00:25
epoch [12/200] batch [15/66] time 0.372 (0.454) data 0.242 (0.324) loss_u loss_u 0.8486 (0.8491) acc_u 25.0000 (18.9583) lr 1.9877e-03 eta 0:00:23
epoch [12/200] batch [20/66] time 0.464 (0.458) data 0.332 (0.327) loss_u loss_u 0.8623 (0.8511) acc_u 21.8750 (19.2188) lr 1.9877e-03 eta 0:00:21
epoch [12/200] batch [25/66] time 0.407 (0.453) data 0.277 (0.322) loss_u loss_u 0.8745 (0.8533) acc_u 25.0000 (19.1250) lr 1.9877e-03 eta 0:00:18
epoch [12/200] batch [30/66] time 0.459 (0.449) data 0.328 (0.318) loss_u loss_u 0.7822 (0.8510) acc_u 31.2500 (19.6875) lr 1.9877e-03 eta 0:00:16
epoch [12/200] batch [35/66] time 0.411 (0.447) data 0.280 (0.316) loss_u loss_u 0.8672 (0.8522) acc_u 15.6250 (19.1071) lr 1.9877e-03 eta 0:00:13
epoch [12/200] batch [40/66] time 0.398 (0.446) data 0.263 (0.315) loss_u loss_u 0.8379 (0.8511) acc_u 18.7500 (19.1406) lr 1.9877e-03 eta 0:00:11
epoch [12/200] batch [45/66] time 0.533 (0.449) data 0.402 (0.318) loss_u loss_u 0.8145 (0.8490) acc_u 18.7500 (19.2361) lr 1.9877e-03 eta 0:00:09
epoch [12/200] batch [50/66] time 0.386 (0.446) data 0.256 (0.315) loss_u loss_u 0.8408 (0.8473) acc_u 18.7500 (19.5000) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [55/66] time 0.446 (0.443) data 0.315 (0.312) loss_u loss_u 0.8730 (0.8486) acc_u 15.6250 (19.4318) lr 1.9877e-03 eta 0:00:04
epoch [12/200] batch [60/66] time 0.394 (0.445) data 0.264 (0.314) loss_u loss_u 0.8975 (0.8504) acc_u 12.5000 (19.2708) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [65/66] time 0.510 (0.443) data 0.379 (0.313) loss_u loss_u 0.9082 (0.8518) acc_u 9.3750 (19.0385) lr 1.9877e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1756
confident_label rate tensor(0.3367, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1056
clean true:1053
clean false:3
clean_rate:0.9971590909090909
noisy true:327
noisy false:1753
after delete: len(clean_dataset) 1056
after delete: len(noisy_dataset) 2080
epoch [13/200] batch [5/33] time 0.392 (0.445) data 0.263 (0.316) loss_x loss_x 1.6318 (1.4869) acc_x 59.3750 (60.0000) lr 1.9851e-03 eta 0:00:12
epoch [13/200] batch [10/33] time 0.485 (0.451) data 0.356 (0.322) loss_x loss_x 1.0811 (1.3861) acc_x 68.7500 (62.1875) lr 1.9851e-03 eta 0:00:10
epoch [13/200] batch [15/33] time 0.528 (0.464) data 0.397 (0.334) loss_x loss_x 1.3770 (1.4151) acc_x 62.5000 (62.7083) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [20/33] time 0.504 (0.460) data 0.374 (0.330) loss_x loss_x 1.5215 (1.4529) acc_x 53.1250 (62.0312) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [25/33] time 0.534 (0.455) data 0.403 (0.325) loss_x loss_x 1.3848 (1.4573) acc_x 59.3750 (61.3750) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [30/33] time 0.550 (0.460) data 0.420 (0.329) loss_x loss_x 1.6924 (1.4844) acc_x 62.5000 (61.7708) lr 1.9851e-03 eta 0:00:01
epoch [13/200] batch [5/65] time 0.483 (0.448) data 0.352 (0.317) loss_u loss_u 0.8896 (0.8630) acc_u 25.0000 (19.3750) lr 1.9851e-03 eta 0:00:26
epoch [13/200] batch [10/65] time 0.472 (0.446) data 0.341 (0.316) loss_u loss_u 0.8608 (0.8594) acc_u 18.7500 (18.4375) lr 1.9851e-03 eta 0:00:24
epoch [13/200] batch [15/65] time 0.436 (0.445) data 0.305 (0.314) loss_u loss_u 0.7993 (0.8481) acc_u 25.0000 (20.0000) lr 1.9851e-03 eta 0:00:22
epoch [13/200] batch [20/65] time 0.469 (0.443) data 0.339 (0.312) loss_u loss_u 0.7041 (0.8461) acc_u 37.5000 (19.6875) lr 1.9851e-03 eta 0:00:19
epoch [13/200] batch [25/65] time 0.457 (0.447) data 0.325 (0.316) loss_u loss_u 0.8833 (0.8515) acc_u 15.6250 (19.5000) lr 1.9851e-03 eta 0:00:17
epoch [13/200] batch [30/65] time 0.431 (0.447) data 0.300 (0.316) loss_u loss_u 0.7915 (0.8516) acc_u 21.8750 (19.5833) lr 1.9851e-03 eta 0:00:15
epoch [13/200] batch [35/65] time 0.521 (0.448) data 0.390 (0.317) loss_u loss_u 0.8721 (0.8548) acc_u 21.8750 (19.4643) lr 1.9851e-03 eta 0:00:13
epoch [13/200] batch [40/65] time 0.389 (0.443) data 0.258 (0.313) loss_u loss_u 0.8687 (0.8549) acc_u 15.6250 (19.0625) lr 1.9851e-03 eta 0:00:11
epoch [13/200] batch [45/65] time 0.413 (0.442) data 0.283 (0.311) loss_u loss_u 0.8696 (0.8557) acc_u 18.7500 (18.8194) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [50/65] time 0.423 (0.442) data 0.292 (0.311) loss_u loss_u 0.8901 (0.8562) acc_u 15.6250 (18.5625) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [55/65] time 0.483 (0.440) data 0.353 (0.309) loss_u loss_u 0.8530 (0.8590) acc_u 15.6250 (18.1250) lr 1.9851e-03 eta 0:00:04
epoch [13/200] batch [60/65] time 0.376 (0.437) data 0.245 (0.306) loss_u loss_u 0.8237 (0.8582) acc_u 21.8750 (18.2292) lr 1.9851e-03 eta 0:00:02
epoch [13/200] batch [65/65] time 0.409 (0.438) data 0.277 (0.308) loss_u loss_u 0.8154 (0.8564) acc_u 18.7500 (18.5577) lr 1.9851e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1721
confident_label rate tensor(0.3559, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1116
clean true:1112
clean false:4
clean_rate:0.996415770609319
noisy true:303
noisy false:1717
after delete: len(clean_dataset) 1116
after delete: len(noisy_dataset) 2020
epoch [14/200] batch [5/34] time 0.443 (0.479) data 0.313 (0.349) loss_x loss_x 1.6084 (1.8303) acc_x 53.1250 (56.8750) lr 1.9823e-03 eta 0:00:13
epoch [14/200] batch [10/34] time 0.421 (0.480) data 0.289 (0.349) loss_x loss_x 1.2344 (1.7475) acc_x 68.7500 (57.1875) lr 1.9823e-03 eta 0:00:11
epoch [14/200] batch [15/34] time 0.556 (0.494) data 0.424 (0.362) loss_x loss_x 1.1445 (1.6518) acc_x 65.6250 (58.3333) lr 1.9823e-03 eta 0:00:09
epoch [14/200] batch [20/34] time 0.542 (0.495) data 0.410 (0.364) loss_x loss_x 1.4990 (1.6187) acc_x 62.5000 (58.9062) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [25/34] time 0.392 (0.498) data 0.261 (0.366) loss_x loss_x 1.2539 (1.5651) acc_x 65.6250 (60.3750) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [30/34] time 0.494 (0.497) data 0.362 (0.366) loss_x loss_x 1.3838 (1.5366) acc_x 56.2500 (61.0417) lr 1.9823e-03 eta 0:00:01
epoch [14/200] batch [5/63] time 0.500 (0.488) data 0.368 (0.356) loss_u loss_u 0.8364 (0.8749) acc_u 15.6250 (16.2500) lr 1.9823e-03 eta 0:00:28
epoch [14/200] batch [10/63] time 0.478 (0.484) data 0.347 (0.352) loss_u loss_u 0.8521 (0.8771) acc_u 15.6250 (16.5625) lr 1.9823e-03 eta 0:00:25
epoch [14/200] batch [15/63] time 0.384 (0.472) data 0.253 (0.340) loss_u loss_u 0.8901 (0.8783) acc_u 21.8750 (16.4583) lr 1.9823e-03 eta 0:00:22
epoch [14/200] batch [20/63] time 0.340 (0.464) data 0.208 (0.332) loss_u loss_u 0.9302 (0.8821) acc_u 9.3750 (15.7812) lr 1.9823e-03 eta 0:00:19
epoch [14/200] batch [25/63] time 0.509 (0.460) data 0.377 (0.329) loss_u loss_u 0.8916 (0.8815) acc_u 15.6250 (16.1250) lr 1.9823e-03 eta 0:00:17
epoch [14/200] batch [30/63] time 0.365 (0.456) data 0.234 (0.324) loss_u loss_u 0.8652 (0.8761) acc_u 25.0000 (16.9792) lr 1.9823e-03 eta 0:00:15
epoch [14/200] batch [35/63] time 0.513 (0.463) data 0.381 (0.331) loss_u loss_u 0.9028 (0.8751) acc_u 12.5000 (17.3214) lr 1.9823e-03 eta 0:00:12
epoch [14/200] batch [40/63] time 0.412 (0.469) data 0.280 (0.337) loss_u loss_u 0.9160 (0.8713) acc_u 6.2500 (17.6562) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [45/63] time 0.423 (0.467) data 0.292 (0.336) loss_u loss_u 0.8613 (0.8721) acc_u 15.6250 (17.2222) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [50/63] time 0.447 (0.465) data 0.316 (0.334) loss_u loss_u 0.8228 (0.8722) acc_u 15.6250 (16.7500) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [55/63] time 0.364 (0.463) data 0.231 (0.331) loss_u loss_u 0.8623 (0.8687) acc_u 18.7500 (17.2727) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [60/63] time 0.338 (0.461) data 0.205 (0.330) loss_u loss_u 0.8999 (0.8694) acc_u 9.3750 (16.9271) lr 1.9823e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1753
confident_label rate tensor(0.3380, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1060
clean true:1058
clean false:2
clean_rate:0.9981132075471698
noisy true:325
noisy false:1751
after delete: len(clean_dataset) 1060
after delete: len(noisy_dataset) 2076
epoch [15/200] batch [5/33] time 0.581 (0.473) data 0.450 (0.343) loss_x loss_x 1.6074 (1.4957) acc_x 46.8750 (59.3750) lr 1.9792e-03 eta 0:00:13
epoch [15/200] batch [10/33] time 0.536 (0.495) data 0.405 (0.364) loss_x loss_x 1.7295 (1.4740) acc_x 56.2500 (61.5625) lr 1.9792e-03 eta 0:00:11
epoch [15/200] batch [15/33] time 0.409 (0.470) data 0.280 (0.340) loss_x loss_x 1.5176 (1.4535) acc_x 65.6250 (62.0833) lr 1.9792e-03 eta 0:00:08
epoch [15/200] batch [20/33] time 0.364 (0.461) data 0.233 (0.331) loss_x loss_x 1.5488 (1.4455) acc_x 59.3750 (62.3438) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [25/33] time 0.610 (0.457) data 0.479 (0.326) loss_x loss_x 1.0918 (1.4152) acc_x 78.1250 (64.0000) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [30/33] time 0.386 (0.452) data 0.256 (0.322) loss_x loss_x 1.9561 (1.4225) acc_x 43.7500 (63.5417) lr 1.9792e-03 eta 0:00:01
epoch [15/200] batch [5/64] time 0.424 (0.457) data 0.293 (0.327) loss_u loss_u 0.8892 (0.8719) acc_u 12.5000 (18.1250) lr 1.9792e-03 eta 0:00:26
epoch [15/200] batch [10/64] time 0.417 (0.461) data 0.287 (0.331) loss_u loss_u 0.8579 (0.8634) acc_u 15.6250 (17.5000) lr 1.9792e-03 eta 0:00:24
epoch [15/200] batch [15/64] time 0.350 (0.453) data 0.219 (0.323) loss_u loss_u 0.9170 (0.8701) acc_u 12.5000 (17.2917) lr 1.9792e-03 eta 0:00:22
epoch [15/200] batch [20/64] time 0.412 (0.457) data 0.281 (0.326) loss_u loss_u 0.8569 (0.8608) acc_u 28.1250 (19.2188) lr 1.9792e-03 eta 0:00:20
epoch [15/200] batch [25/64] time 0.395 (0.454) data 0.264 (0.323) loss_u loss_u 0.9077 (0.8607) acc_u 12.5000 (19.0000) lr 1.9792e-03 eta 0:00:17
epoch [15/200] batch [30/64] time 0.390 (0.453) data 0.259 (0.323) loss_u loss_u 0.7632 (0.8555) acc_u 31.2500 (19.2708) lr 1.9792e-03 eta 0:00:15
epoch [15/200] batch [35/64] time 0.454 (0.447) data 0.323 (0.316) loss_u loss_u 0.7964 (0.8567) acc_u 25.0000 (18.9286) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [40/64] time 0.432 (0.447) data 0.301 (0.317) loss_u loss_u 0.8857 (0.8589) acc_u 15.6250 (18.5156) lr 1.9792e-03 eta 0:00:10
epoch [15/200] batch [45/64] time 0.335 (0.444) data 0.204 (0.313) loss_u loss_u 0.8931 (0.8633) acc_u 18.7500 (18.2639) lr 1.9792e-03 eta 0:00:08
epoch [15/200] batch [50/64] time 0.383 (0.442) data 0.252 (0.311) loss_u loss_u 0.8120 (0.8602) acc_u 25.0000 (18.5000) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [55/64] time 0.660 (0.443) data 0.530 (0.313) loss_u loss_u 0.8989 (0.8600) acc_u 9.3750 (18.5795) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [60/64] time 0.477 (0.442) data 0.346 (0.311) loss_u loss_u 0.9155 (0.8613) acc_u 12.5000 (18.5938) lr 1.9792e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1707
confident_label rate tensor(0.3552, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1114
clean true:1112
clean false:2
clean_rate:0.9982046678635548
noisy true:317
noisy false:1705
after delete: len(clean_dataset) 1114
after delete: len(noisy_dataset) 2022
epoch [16/200] batch [5/34] time 0.420 (0.461) data 0.290 (0.331) loss_x loss_x 1.6816 (1.5436) acc_x 50.0000 (57.5000) lr 1.9759e-03 eta 0:00:13
epoch [16/200] batch [10/34] time 0.682 (0.495) data 0.551 (0.364) loss_x loss_x 1.1758 (1.3358) acc_x 68.7500 (63.1250) lr 1.9759e-03 eta 0:00:11
epoch [16/200] batch [15/34] time 0.436 (0.494) data 0.304 (0.362) loss_x loss_x 1.6895 (1.4728) acc_x 50.0000 (59.3750) lr 1.9759e-03 eta 0:00:09
epoch [16/200] batch [20/34] time 0.417 (0.496) data 0.287 (0.365) loss_x loss_x 1.7344 (1.4001) acc_x 59.3750 (62.0312) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [25/34] time 0.500 (0.486) data 0.370 (0.355) loss_x loss_x 1.5107 (1.4357) acc_x 65.6250 (61.8750) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [30/34] time 0.432 (0.474) data 0.302 (0.343) loss_x loss_x 1.5078 (1.4414) acc_x 56.2500 (61.5625) lr 1.9759e-03 eta 0:00:01
epoch [16/200] batch [5/63] time 0.392 (0.463) data 0.260 (0.332) loss_u loss_u 0.8335 (0.8607) acc_u 28.1250 (20.0000) lr 1.9759e-03 eta 0:00:26
epoch [16/200] batch [10/63] time 0.363 (0.458) data 0.232 (0.328) loss_u loss_u 0.8467 (0.8584) acc_u 25.0000 (21.5625) lr 1.9759e-03 eta 0:00:24
epoch [16/200] batch [15/63] time 0.424 (0.453) data 0.294 (0.322) loss_u loss_u 0.8555 (0.8508) acc_u 18.7500 (21.6667) lr 1.9759e-03 eta 0:00:21
epoch [16/200] batch [20/63] time 0.432 (0.449) data 0.301 (0.319) loss_u loss_u 0.7764 (0.8503) acc_u 21.8750 (21.2500) lr 1.9759e-03 eta 0:00:19
epoch [16/200] batch [25/63] time 0.393 (0.445) data 0.262 (0.315) loss_u loss_u 0.8750 (0.8552) acc_u 15.6250 (20.3750) lr 1.9759e-03 eta 0:00:16
epoch [16/200] batch [30/63] time 0.579 (0.446) data 0.448 (0.316) loss_u loss_u 0.8716 (0.8593) acc_u 18.7500 (19.2708) lr 1.9759e-03 eta 0:00:14
epoch [16/200] batch [35/63] time 0.364 (0.444) data 0.233 (0.314) loss_u loss_u 0.8721 (0.8576) acc_u 12.5000 (19.4643) lr 1.9759e-03 eta 0:00:12
epoch [16/200] batch [40/63] time 0.401 (0.444) data 0.270 (0.313) loss_u loss_u 0.8647 (0.8588) acc_u 18.7500 (19.0625) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [45/63] time 0.399 (0.444) data 0.268 (0.314) loss_u loss_u 0.8169 (0.8587) acc_u 21.8750 (18.6806) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [50/63] time 0.643 (0.447) data 0.512 (0.317) loss_u loss_u 0.8276 (0.8595) acc_u 21.8750 (18.5000) lr 1.9759e-03 eta 0:00:05
epoch [16/200] batch [55/63] time 0.649 (0.450) data 0.519 (0.319) loss_u loss_u 0.8018 (0.8565) acc_u 21.8750 (18.9773) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [60/63] time 0.338 (0.450) data 0.207 (0.320) loss_u loss_u 0.8960 (0.8592) acc_u 9.3750 (18.5417) lr 1.9759e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1745
confident_label rate tensor(0.3418, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1072
clean true:1070
clean false:2
clean_rate:0.9981343283582089
noisy true:321
noisy false:1743
after delete: len(clean_dataset) 1072
after delete: len(noisy_dataset) 2064
epoch [17/200] batch [5/33] time 0.442 (0.466) data 0.311 (0.335) loss_x loss_x 1.0771 (1.2975) acc_x 71.8750 (65.0000) lr 1.9724e-03 eta 0:00:13
epoch [17/200] batch [10/33] time 0.361 (0.437) data 0.230 (0.306) loss_x loss_x 1.5459 (1.4169) acc_x 56.2500 (63.1250) lr 1.9724e-03 eta 0:00:10
epoch [17/200] batch [15/33] time 0.443 (0.441) data 0.313 (0.310) loss_x loss_x 1.7988 (1.4535) acc_x 59.3750 (62.7083) lr 1.9724e-03 eta 0:00:07
epoch [17/200] batch [20/33] time 0.594 (0.452) data 0.464 (0.321) loss_x loss_x 1.5117 (1.4416) acc_x 59.3750 (63.1250) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [25/33] time 0.378 (0.449) data 0.248 (0.319) loss_x loss_x 1.3408 (1.4223) acc_x 56.2500 (63.1250) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [30/33] time 0.429 (0.453) data 0.298 (0.322) loss_x loss_x 2.0117 (1.4337) acc_x 37.5000 (62.6042) lr 1.9724e-03 eta 0:00:01
epoch [17/200] batch [5/64] time 0.372 (0.453) data 0.241 (0.322) loss_u loss_u 0.8604 (0.8504) acc_u 9.3750 (18.1250) lr 1.9724e-03 eta 0:00:26
epoch [17/200] batch [10/64] time 0.397 (0.451) data 0.266 (0.320) loss_u loss_u 0.8457 (0.8514) acc_u 18.7500 (17.8125) lr 1.9724e-03 eta 0:00:24
epoch [17/200] batch [15/64] time 0.377 (0.444) data 0.246 (0.313) loss_u loss_u 0.8643 (0.8559) acc_u 21.8750 (18.1250) lr 1.9724e-03 eta 0:00:21
epoch [17/200] batch [20/64] time 0.420 (0.442) data 0.289 (0.311) loss_u loss_u 0.8633 (0.8514) acc_u 28.1250 (19.5312) lr 1.9724e-03 eta 0:00:19
epoch [17/200] batch [25/64] time 0.417 (0.438) data 0.286 (0.308) loss_u loss_u 0.8691 (0.8566) acc_u 15.6250 (19.0000) lr 1.9724e-03 eta 0:00:17
epoch [17/200] batch [30/64] time 0.496 (0.442) data 0.365 (0.311) loss_u loss_u 0.9185 (0.8573) acc_u 12.5000 (18.7500) lr 1.9724e-03 eta 0:00:15
epoch [17/200] batch [35/64] time 0.477 (0.442) data 0.343 (0.311) loss_u loss_u 0.8716 (0.8557) acc_u 12.5000 (18.6607) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [40/64] time 0.422 (0.443) data 0.291 (0.312) loss_u loss_u 0.8208 (0.8552) acc_u 18.7500 (18.5938) lr 1.9724e-03 eta 0:00:10
epoch [17/200] batch [45/64] time 0.407 (0.445) data 0.275 (0.314) loss_u loss_u 0.8076 (0.8527) acc_u 28.1250 (19.3056) lr 1.9724e-03 eta 0:00:08
epoch [17/200] batch [50/64] time 0.474 (0.444) data 0.343 (0.313) loss_u loss_u 0.8604 (0.8511) acc_u 18.7500 (19.4375) lr 1.9724e-03 eta 0:00:06
epoch [17/200] batch [55/64] time 0.450 (0.445) data 0.319 (0.314) loss_u loss_u 0.8589 (0.8507) acc_u 18.7500 (19.6023) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [60/64] time 0.443 (0.446) data 0.312 (0.315) loss_u loss_u 0.8140 (0.8519) acc_u 31.2500 (19.6354) lr 1.9724e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1701
confident_label rate tensor(0.3517, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1103
clean true:1102
clean false:1
clean_rate:0.99909338168631
noisy true:333
noisy false:1700
after delete: len(clean_dataset) 1103
after delete: len(noisy_dataset) 2033
epoch [18/200] batch [5/34] time 0.381 (0.441) data 0.250 (0.310) loss_x loss_x 1.8926 (1.4686) acc_x 53.1250 (61.2500) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [10/34] time 0.445 (0.445) data 0.315 (0.314) loss_x loss_x 1.2852 (1.3579) acc_x 71.8750 (63.4375) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [15/34] time 0.519 (0.452) data 0.389 (0.322) loss_x loss_x 1.3662 (1.4015) acc_x 65.6250 (62.7083) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [20/34] time 0.339 (0.455) data 0.209 (0.324) loss_x loss_x 1.5801 (1.4102) acc_x 56.2500 (62.6562) lr 1.9686e-03 eta 0:00:06
epoch [18/200] batch [25/34] time 0.466 (0.463) data 0.337 (0.333) loss_x loss_x 1.1230 (1.4225) acc_x 71.8750 (62.6250) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [30/34] time 0.457 (0.470) data 0.328 (0.340) loss_x loss_x 1.6191 (1.4170) acc_x 56.2500 (62.6042) lr 1.9686e-03 eta 0:00:01
epoch [18/200] batch [5/63] time 0.328 (0.457) data 0.197 (0.326) loss_u loss_u 0.8071 (0.8646) acc_u 21.8750 (15.0000) lr 1.9686e-03 eta 0:00:26
epoch [18/200] batch [10/63] time 0.406 (0.449) data 0.275 (0.319) loss_u loss_u 0.8745 (0.8577) acc_u 12.5000 (17.8125) lr 1.9686e-03 eta 0:00:23
epoch [18/200] batch [15/63] time 0.381 (0.441) data 0.249 (0.310) loss_u loss_u 0.8398 (0.8631) acc_u 21.8750 (17.5000) lr 1.9686e-03 eta 0:00:21
epoch [18/200] batch [20/63] time 0.422 (0.437) data 0.292 (0.306) loss_u loss_u 0.8584 (0.8541) acc_u 28.1250 (19.2188) lr 1.9686e-03 eta 0:00:18
epoch [18/200] batch [25/63] time 0.384 (0.438) data 0.253 (0.307) loss_u loss_u 0.8452 (0.8552) acc_u 15.6250 (18.3750) lr 1.9686e-03 eta 0:00:16
epoch [18/200] batch [30/63] time 0.409 (0.439) data 0.278 (0.308) loss_u loss_u 0.9072 (0.8604) acc_u 12.5000 (17.5000) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [35/63] time 0.365 (0.436) data 0.234 (0.306) loss_u loss_u 0.8242 (0.8578) acc_u 25.0000 (18.0357) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [40/63] time 0.514 (0.439) data 0.380 (0.308) loss_u loss_u 0.8784 (0.8581) acc_u 12.5000 (18.1250) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [45/63] time 0.624 (0.441) data 0.493 (0.310) loss_u loss_u 0.8496 (0.8563) acc_u 21.8750 (18.3333) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [50/63] time 0.490 (0.444) data 0.359 (0.313) loss_u loss_u 0.8887 (0.8568) acc_u 18.7500 (18.4375) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [55/63] time 0.479 (0.445) data 0.348 (0.314) loss_u loss_u 0.8242 (0.8549) acc_u 21.8750 (18.6932) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [60/63] time 0.350 (0.446) data 0.218 (0.315) loss_u loss_u 0.8589 (0.8546) acc_u 18.7500 (18.7500) lr 1.9686e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1698
confident_label rate tensor(0.3546, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1112
clean true:1108
clean false:4
clean_rate:0.9964028776978417
noisy true:330
noisy false:1694
after delete: len(clean_dataset) 1112
after delete: len(noisy_dataset) 2024
epoch [19/200] batch [5/34] time 0.385 (0.441) data 0.255 (0.311) loss_x loss_x 1.2412 (1.2842) acc_x 65.6250 (67.5000) lr 1.9646e-03 eta 0:00:12
epoch [19/200] batch [10/34] time 0.453 (0.482) data 0.323 (0.351) loss_x loss_x 1.5332 (1.5271) acc_x 59.3750 (63.7500) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [15/34] time 0.370 (0.467) data 0.238 (0.337) loss_x loss_x 1.6924 (1.5432) acc_x 56.2500 (61.6667) lr 1.9646e-03 eta 0:00:08
epoch [19/200] batch [20/34] time 0.389 (0.470) data 0.258 (0.340) loss_x loss_x 1.7256 (1.4842) acc_x 62.5000 (62.6562) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [25/34] time 0.448 (0.481) data 0.318 (0.350) loss_x loss_x 1.2236 (1.4740) acc_x 68.7500 (62.6250) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [30/34] time 0.530 (0.470) data 0.400 (0.340) loss_x loss_x 1.7051 (1.4781) acc_x 53.1250 (62.0833) lr 1.9646e-03 eta 0:00:01
epoch [19/200] batch [5/63] time 0.566 (0.461) data 0.436 (0.330) loss_u loss_u 0.8623 (0.8695) acc_u 15.6250 (18.7500) lr 1.9646e-03 eta 0:00:26
epoch [19/200] batch [10/63] time 0.376 (0.452) data 0.246 (0.321) loss_u loss_u 0.8486 (0.8730) acc_u 18.7500 (16.5625) lr 1.9646e-03 eta 0:00:23
epoch [19/200] batch [15/63] time 0.382 (0.447) data 0.251 (0.317) loss_u loss_u 0.7754 (0.8667) acc_u 34.3750 (17.7083) lr 1.9646e-03 eta 0:00:21
epoch [19/200] batch [20/63] time 0.377 (0.443) data 0.247 (0.313) loss_u loss_u 0.8926 (0.8729) acc_u 15.6250 (16.8750) lr 1.9646e-03 eta 0:00:19
epoch [19/200] batch [25/63] time 0.498 (0.446) data 0.368 (0.316) loss_u loss_u 0.8252 (0.8687) acc_u 18.7500 (17.6250) lr 1.9646e-03 eta 0:00:16
epoch [19/200] batch [30/63] time 0.556 (0.446) data 0.426 (0.315) loss_u loss_u 0.8364 (0.8630) acc_u 21.8750 (18.6458) lr 1.9646e-03 eta 0:00:14
epoch [19/200] batch [35/63] time 0.368 (0.441) data 0.238 (0.311) loss_u loss_u 0.9199 (0.8624) acc_u 6.2500 (18.4821) lr 1.9646e-03 eta 0:00:12
epoch [19/200] batch [40/63] time 0.476 (0.442) data 0.346 (0.312) loss_u loss_u 0.7964 (0.8565) acc_u 25.0000 (19.2188) lr 1.9646e-03 eta 0:00:10
epoch [19/200] batch [45/63] time 0.367 (0.442) data 0.237 (0.312) loss_u loss_u 0.8857 (0.8616) acc_u 18.7500 (18.5417) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [50/63] time 0.507 (0.443) data 0.376 (0.313) loss_u loss_u 0.9102 (0.8606) acc_u 9.3750 (18.3750) lr 1.9646e-03 eta 0:00:05
epoch [19/200] batch [55/63] time 0.519 (0.443) data 0.387 (0.313) loss_u loss_u 0.8608 (0.8585) acc_u 21.8750 (18.6932) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [60/63] time 0.414 (0.442) data 0.283 (0.311) loss_u loss_u 0.8418 (0.8595) acc_u 28.1250 (18.6979) lr 1.9646e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1731
confident_label rate tensor(0.3438, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1078
clean true:1077
clean false:1
clean_rate:0.9990723562152134
noisy true:328
noisy false:1730
after delete: len(clean_dataset) 1078
after delete: len(noisy_dataset) 2058
epoch [20/200] batch [5/33] time 0.431 (0.432) data 0.300 (0.301) loss_x loss_x 1.1016 (1.4680) acc_x 68.7500 (64.3750) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [10/33] time 0.409 (0.446) data 0.279 (0.315) loss_x loss_x 0.9917 (1.4500) acc_x 68.7500 (63.4375) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [15/33] time 0.380 (0.457) data 0.250 (0.326) loss_x loss_x 1.8203 (1.4134) acc_x 56.2500 (65.0000) lr 1.9603e-03 eta 0:00:08
epoch [20/200] batch [20/33] time 0.398 (0.458) data 0.268 (0.328) loss_x loss_x 1.6250 (1.4587) acc_x 56.2500 (63.7500) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [25/33] time 0.469 (0.452) data 0.339 (0.322) loss_x loss_x 1.3398 (1.4621) acc_x 59.3750 (62.6250) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [30/33] time 0.378 (0.460) data 0.248 (0.329) loss_x loss_x 2.0684 (1.5070) acc_x 68.7500 (61.7708) lr 1.9603e-03 eta 0:00:01
epoch [20/200] batch [5/64] time 0.379 (0.461) data 0.248 (0.331) loss_u loss_u 0.9102 (0.8624) acc_u 9.3750 (16.8750) lr 1.9603e-03 eta 0:00:27
epoch [20/200] batch [10/64] time 0.389 (0.459) data 0.258 (0.328) loss_u loss_u 0.7759 (0.8448) acc_u 31.2500 (19.6875) lr 1.9603e-03 eta 0:00:24
epoch [20/200] batch [15/64] time 0.464 (0.454) data 0.333 (0.323) loss_u loss_u 0.9141 (0.8538) acc_u 12.5000 (19.1667) lr 1.9603e-03 eta 0:00:22
epoch [20/200] batch [20/64] time 0.364 (0.447) data 0.233 (0.316) loss_u loss_u 0.8042 (0.8517) acc_u 25.0000 (19.0625) lr 1.9603e-03 eta 0:00:19
epoch [20/200] batch [25/64] time 0.353 (0.448) data 0.220 (0.318) loss_u loss_u 0.8984 (0.8522) acc_u 12.5000 (19.1250) lr 1.9603e-03 eta 0:00:17
epoch [20/200] batch [30/64] time 0.504 (0.451) data 0.372 (0.320) loss_u loss_u 0.9224 (0.8574) acc_u 15.6250 (18.5417) lr 1.9603e-03 eta 0:00:15
epoch [20/200] batch [35/64] time 0.418 (0.452) data 0.287 (0.321) loss_u loss_u 0.8413 (0.8564) acc_u 31.2500 (18.8393) lr 1.9603e-03 eta 0:00:13
epoch [20/200] batch [40/64] time 0.427 (0.452) data 0.296 (0.321) loss_u loss_u 0.8179 (0.8511) acc_u 21.8750 (19.3750) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [45/64] time 0.394 (0.452) data 0.263 (0.321) loss_u loss_u 0.8921 (0.8528) acc_u 18.7500 (19.2361) lr 1.9603e-03 eta 0:00:08
epoch [20/200] batch [50/64] time 0.382 (0.452) data 0.251 (0.321) loss_u loss_u 0.8501 (0.8506) acc_u 21.8750 (19.7500) lr 1.9603e-03 eta 0:00:06
epoch [20/200] batch [55/64] time 0.433 (0.450) data 0.302 (0.319) loss_u loss_u 0.8672 (0.8500) acc_u 18.7500 (19.9432) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [60/64] time 0.427 (0.447) data 0.296 (0.316) loss_u loss_u 0.7998 (0.8502) acc_u 25.0000 (20.0521) lr 1.9603e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1723
confident_label rate tensor(0.3431, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1076
clean true:1074
clean false:2
clean_rate:0.9981412639405205
noisy true:339
noisy false:1721
after delete: len(clean_dataset) 1076
after delete: len(noisy_dataset) 2060
epoch [21/200] batch [5/33] time 0.443 (0.501) data 0.311 (0.370) loss_x loss_x 1.3984 (1.2354) acc_x 68.7500 (68.1250) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [10/33] time 0.484 (0.483) data 0.353 (0.351) loss_x loss_x 1.0996 (1.3257) acc_x 62.5000 (65.0000) lr 1.9558e-03 eta 0:00:11
epoch [21/200] batch [15/33] time 0.383 (0.458) data 0.253 (0.327) loss_x loss_x 1.7236 (1.3845) acc_x 50.0000 (63.3333) lr 1.9558e-03 eta 0:00:08
epoch [21/200] batch [20/33] time 0.511 (0.470) data 0.381 (0.340) loss_x loss_x 1.3311 (1.4091) acc_x 62.5000 (63.2812) lr 1.9558e-03 eta 0:00:06
epoch [21/200] batch [25/33] time 0.368 (0.468) data 0.237 (0.337) loss_x loss_x 2.1738 (1.4076) acc_x 40.6250 (63.2500) lr 1.9558e-03 eta 0:00:03
epoch [21/200] batch [30/33] time 0.527 (0.469) data 0.397 (0.339) loss_x loss_x 2.1367 (1.3812) acc_x 46.8750 (64.2708) lr 1.9558e-03 eta 0:00:01
epoch [21/200] batch [5/64] time 0.400 (0.459) data 0.269 (0.328) loss_u loss_u 0.7700 (0.8416) acc_u 28.1250 (19.3750) lr 1.9558e-03 eta 0:00:27
epoch [21/200] batch [10/64] time 0.464 (0.456) data 0.331 (0.325) loss_u loss_u 0.8330 (0.8415) acc_u 25.0000 (20.0000) lr 1.9558e-03 eta 0:00:24
epoch [21/200] batch [15/64] time 0.383 (0.461) data 0.251 (0.330) loss_u loss_u 0.8921 (0.8595) acc_u 12.5000 (17.7083) lr 1.9558e-03 eta 0:00:22
epoch [21/200] batch [20/64] time 0.434 (0.457) data 0.302 (0.326) loss_u loss_u 0.8379 (0.8524) acc_u 28.1250 (19.0625) lr 1.9558e-03 eta 0:00:20
epoch [21/200] batch [25/64] time 0.505 (0.455) data 0.373 (0.323) loss_u loss_u 0.8369 (0.8534) acc_u 18.7500 (18.6250) lr 1.9558e-03 eta 0:00:17
epoch [21/200] batch [30/64] time 0.524 (0.460) data 0.392 (0.329) loss_u loss_u 0.9077 (0.8553) acc_u 12.5000 (18.4375) lr 1.9558e-03 eta 0:00:15
epoch [21/200] batch [35/64] time 0.454 (0.459) data 0.323 (0.327) loss_u loss_u 0.8110 (0.8511) acc_u 31.2500 (19.3750) lr 1.9558e-03 eta 0:00:13
epoch [21/200] batch [40/64] time 0.535 (0.458) data 0.405 (0.326) loss_u loss_u 0.8213 (0.8518) acc_u 31.2500 (19.3750) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [45/64] time 0.368 (0.459) data 0.237 (0.328) loss_u loss_u 0.8867 (0.8541) acc_u 18.7500 (19.3056) lr 1.9558e-03 eta 0:00:08
epoch [21/200] batch [50/64] time 0.340 (0.458) data 0.210 (0.327) loss_u loss_u 0.8950 (0.8551) acc_u 15.6250 (19.5000) lr 1.9558e-03 eta 0:00:06
epoch [21/200] batch [55/64] time 0.379 (0.456) data 0.249 (0.325) loss_u loss_u 0.8682 (0.8557) acc_u 21.8750 (19.7159) lr 1.9558e-03 eta 0:00:04
epoch [21/200] batch [60/64] time 0.662 (0.455) data 0.531 (0.324) loss_u loss_u 0.8613 (0.8556) acc_u 25.0000 (19.6875) lr 1.9558e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1634
confident_label rate tensor(0.3712, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1164
clean true:1159
clean false:5
clean_rate:0.9957044673539519
noisy true:343
noisy false:1629
after delete: len(clean_dataset) 1164
after delete: len(noisy_dataset) 1972
epoch [22/200] batch [5/36] time 0.493 (0.485) data 0.363 (0.354) loss_x loss_x 1.2686 (1.1065) acc_x 65.6250 (70.6250) lr 1.9511e-03 eta 0:00:15
epoch [22/200] batch [10/36] time 0.521 (0.478) data 0.391 (0.348) loss_x loss_x 1.4639 (1.2593) acc_x 65.6250 (68.7500) lr 1.9511e-03 eta 0:00:12
epoch [22/200] batch [15/36] time 0.504 (0.469) data 0.374 (0.338) loss_x loss_x 1.5420 (1.2881) acc_x 56.2500 (66.6667) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [20/36] time 0.472 (0.482) data 0.341 (0.351) loss_x loss_x 1.1299 (1.2849) acc_x 75.0000 (66.0938) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [25/36] time 0.747 (0.490) data 0.614 (0.359) loss_x loss_x 1.7236 (1.3127) acc_x 59.3750 (65.2500) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [30/36] time 0.563 (0.491) data 0.433 (0.360) loss_x loss_x 1.6514 (1.3433) acc_x 56.2500 (64.2708) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [35/36] time 0.499 (0.493) data 0.366 (0.362) loss_x loss_x 1.1738 (1.3083) acc_x 62.5000 (65.3571) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/61] time 0.489 (0.483) data 0.358 (0.352) loss_u loss_u 0.8184 (0.8391) acc_u 25.0000 (21.2500) lr 1.9511e-03 eta 0:00:27
epoch [22/200] batch [10/61] time 0.407 (0.482) data 0.276 (0.351) loss_u loss_u 0.8794 (0.8444) acc_u 15.6250 (20.6250) lr 1.9511e-03 eta 0:00:24
epoch [22/200] batch [15/61] time 0.435 (0.478) data 0.304 (0.347) loss_u loss_u 0.8970 (0.8539) acc_u 9.3750 (18.9583) lr 1.9511e-03 eta 0:00:21
epoch [22/200] batch [20/61] time 0.438 (0.476) data 0.307 (0.344) loss_u loss_u 0.8442 (0.8519) acc_u 25.0000 (20.0000) lr 1.9511e-03 eta 0:00:19
epoch [22/200] batch [25/61] time 0.428 (0.470) data 0.295 (0.339) loss_u loss_u 0.8311 (0.8526) acc_u 25.0000 (20.0000) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [30/61] time 0.469 (0.472) data 0.331 (0.341) loss_u loss_u 0.9146 (0.8555) acc_u 9.3750 (19.2708) lr 1.9511e-03 eta 0:00:14
epoch [22/200] batch [35/61] time 0.366 (0.471) data 0.233 (0.339) loss_u loss_u 0.8677 (0.8600) acc_u 21.8750 (18.7500) lr 1.9511e-03 eta 0:00:12
epoch [22/200] batch [40/61] time 0.441 (0.470) data 0.309 (0.338) loss_u loss_u 0.9097 (0.8585) acc_u 15.6250 (19.1406) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [45/61] time 0.399 (0.467) data 0.268 (0.335) loss_u loss_u 0.8442 (0.8589) acc_u 18.7500 (18.8194) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [50/61] time 0.486 (0.464) data 0.355 (0.333) loss_u loss_u 0.7769 (0.8595) acc_u 25.0000 (18.4375) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [55/61] time 0.346 (0.460) data 0.215 (0.328) loss_u loss_u 0.9326 (0.8604) acc_u 6.2500 (18.4091) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [60/61] time 0.415 (0.461) data 0.284 (0.330) loss_u loss_u 0.9028 (0.8598) acc_u 6.2500 (18.4375) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1673
confident_label rate tensor(0.3635, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1140
clean true:1139
clean false:1
clean_rate:0.9991228070175439
noisy true:324
noisy false:1672
after delete: len(clean_dataset) 1140
after delete: len(noisy_dataset) 1996
epoch [23/200] batch [5/35] time 0.427 (0.490) data 0.297 (0.359) loss_x loss_x 1.5742 (1.3830) acc_x 59.3750 (63.1250) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [10/35] time 0.589 (0.465) data 0.459 (0.334) loss_x loss_x 1.9160 (1.3597) acc_x 53.1250 (64.0625) lr 1.9461e-03 eta 0:00:11
epoch [23/200] batch [15/35] time 0.513 (0.462) data 0.383 (0.332) loss_x loss_x 1.7363 (1.4284) acc_x 62.5000 (62.5000) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [20/35] time 0.386 (0.470) data 0.256 (0.340) loss_x loss_x 1.3223 (1.4285) acc_x 62.5000 (61.7188) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [25/35] time 0.395 (0.467) data 0.265 (0.337) loss_x loss_x 1.5996 (1.4363) acc_x 62.5000 (61.2500) lr 1.9461e-03 eta 0:00:04
epoch [23/200] batch [30/35] time 0.433 (0.470) data 0.302 (0.339) loss_x loss_x 1.3721 (1.4504) acc_x 62.5000 (61.2500) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [35/35] time 0.498 (0.468) data 0.367 (0.337) loss_x loss_x 1.4873 (1.4680) acc_x 53.1250 (60.8036) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/62] time 0.461 (0.459) data 0.331 (0.329) loss_u loss_u 0.8887 (0.8772) acc_u 12.5000 (15.6250) lr 1.9461e-03 eta 0:00:26
epoch [23/200] batch [10/62] time 0.326 (0.453) data 0.194 (0.322) loss_u loss_u 0.7900 (0.8708) acc_u 31.2500 (17.5000) lr 1.9461e-03 eta 0:00:23
epoch [23/200] batch [15/62] time 0.620 (0.450) data 0.489 (0.320) loss_u loss_u 0.8984 (0.8731) acc_u 12.5000 (16.8750) lr 1.9461e-03 eta 0:00:21
epoch [23/200] batch [20/62] time 0.494 (0.450) data 0.363 (0.319) loss_u loss_u 0.8389 (0.8678) acc_u 21.8750 (18.1250) lr 1.9461e-03 eta 0:00:18
epoch [23/200] batch [25/62] time 0.546 (0.450) data 0.414 (0.319) loss_u loss_u 0.8560 (0.8684) acc_u 15.6250 (17.3750) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [30/62] time 0.411 (0.449) data 0.280 (0.318) loss_u loss_u 0.8052 (0.8650) acc_u 25.0000 (17.9167) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [35/62] time 0.334 (0.448) data 0.203 (0.317) loss_u loss_u 0.7681 (0.8612) acc_u 34.3750 (18.7500) lr 1.9461e-03 eta 0:00:12
epoch [23/200] batch [40/62] time 0.364 (0.445) data 0.233 (0.314) loss_u loss_u 0.7832 (0.8576) acc_u 31.2500 (19.2969) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [45/62] time 0.357 (0.443) data 0.226 (0.312) loss_u loss_u 0.8379 (0.8525) acc_u 15.6250 (19.4444) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [50/62] time 0.596 (0.443) data 0.464 (0.312) loss_u loss_u 0.7378 (0.8518) acc_u 37.5000 (20.0000) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [55/62] time 0.355 (0.443) data 0.224 (0.312) loss_u loss_u 0.7158 (0.8509) acc_u 34.3750 (19.9432) lr 1.9461e-03 eta 0:00:03
epoch [23/200] batch [60/62] time 0.474 (0.444) data 0.341 (0.313) loss_u loss_u 0.7808 (0.8511) acc_u 34.3750 (20.1042) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1689
confident_label rate tensor(0.3587, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1125
clean true:1124
clean false:1
clean_rate:0.9991111111111111
noisy true:323
noisy false:1688
after delete: len(clean_dataset) 1125
after delete: len(noisy_dataset) 2011
epoch [24/200] batch [5/35] time 0.392 (0.449) data 0.262 (0.319) loss_x loss_x 1.3701 (1.5213) acc_x 65.6250 (64.3750) lr 1.9409e-03 eta 0:00:13
epoch [24/200] batch [10/35] time 0.523 (0.446) data 0.393 (0.316) loss_x loss_x 1.5820 (1.4163) acc_x 59.3750 (67.1875) lr 1.9409e-03 eta 0:00:11
epoch [24/200] batch [15/35] time 0.497 (0.447) data 0.367 (0.317) loss_x loss_x 1.0645 (1.4398) acc_x 78.1250 (65.4167) lr 1.9409e-03 eta 0:00:08
epoch [24/200] batch [20/35] time 0.421 (0.454) data 0.290 (0.323) loss_x loss_x 1.7314 (1.4643) acc_x 59.3750 (64.3750) lr 1.9409e-03 eta 0:00:06
epoch [24/200] batch [25/35] time 0.459 (0.458) data 0.328 (0.327) loss_x loss_x 1.8584 (1.4648) acc_x 50.0000 (64.5000) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [30/35] time 0.628 (0.464) data 0.498 (0.333) loss_x loss_x 1.2109 (1.4192) acc_x 75.0000 (65.5208) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [35/35] time 0.523 (0.460) data 0.392 (0.329) loss_x loss_x 1.3438 (1.4205) acc_x 62.5000 (65.3571) lr 1.9409e-03 eta 0:00:00
epoch [24/200] batch [5/62] time 0.431 (0.461) data 0.300 (0.330) loss_u loss_u 0.8101 (0.8256) acc_u 25.0000 (21.2500) lr 1.9409e-03 eta 0:00:26
epoch [24/200] batch [10/62] time 0.376 (0.455) data 0.245 (0.325) loss_u loss_u 0.7891 (0.8299) acc_u 31.2500 (21.8750) lr 1.9409e-03 eta 0:00:23
epoch [24/200] batch [15/62] time 0.418 (0.461) data 0.287 (0.331) loss_u loss_u 0.7788 (0.8300) acc_u 31.2500 (22.9167) lr 1.9409e-03 eta 0:00:21
epoch [24/200] batch [20/62] time 0.337 (0.457) data 0.206 (0.326) loss_u loss_u 0.8906 (0.8373) acc_u 6.2500 (21.7188) lr 1.9409e-03 eta 0:00:19
epoch [24/200] batch [25/62] time 0.492 (0.455) data 0.361 (0.324) loss_u loss_u 0.8652 (0.8428) acc_u 18.7500 (21.2500) lr 1.9409e-03 eta 0:00:16
epoch [24/200] batch [30/62] time 0.448 (0.451) data 0.317 (0.320) loss_u loss_u 0.8125 (0.8434) acc_u 34.3750 (21.1458) lr 1.9409e-03 eta 0:00:14
epoch [24/200] batch [35/62] time 0.421 (0.450) data 0.290 (0.319) loss_u loss_u 0.9097 (0.8457) acc_u 9.3750 (20.8036) lr 1.9409e-03 eta 0:00:12
epoch [24/200] batch [40/62] time 0.361 (0.445) data 0.230 (0.314) loss_u loss_u 0.8438 (0.8458) acc_u 21.8750 (21.1719) lr 1.9409e-03 eta 0:00:09
epoch [24/200] batch [45/62] time 0.420 (0.444) data 0.289 (0.313) loss_u loss_u 0.8433 (0.8505) acc_u 21.8750 (20.3472) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [50/62] time 0.401 (0.443) data 0.270 (0.312) loss_u loss_u 0.8267 (0.8483) acc_u 21.8750 (20.6250) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [55/62] time 0.501 (0.443) data 0.368 (0.312) loss_u loss_u 0.8809 (0.8472) acc_u 21.8750 (20.8523) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [60/62] time 0.403 (0.446) data 0.272 (0.315) loss_u loss_u 0.9111 (0.8493) acc_u 9.3750 (20.5208) lr 1.9409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1686
confident_label rate tensor(0.3607, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1131
clean true:1129
clean false:2
clean_rate:0.9982316534040672
noisy true:321
noisy false:1684
after delete: len(clean_dataset) 1131
after delete: len(noisy_dataset) 2005
epoch [25/200] batch [5/35] time 0.512 (0.483) data 0.380 (0.352) loss_x loss_x 2.1992 (1.3967) acc_x 50.0000 (65.0000) lr 1.9354e-03 eta 0:00:14
epoch [25/200] batch [10/35] time 0.471 (0.474) data 0.341 (0.343) loss_x loss_x 1.3906 (1.4525) acc_x 62.5000 (63.7500) lr 1.9354e-03 eta 0:00:11
epoch [25/200] batch [15/35] time 0.425 (0.459) data 0.294 (0.328) loss_x loss_x 0.9316 (1.4311) acc_x 75.0000 (64.3750) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [20/35] time 0.480 (0.460) data 0.349 (0.329) loss_x loss_x 1.6465 (1.3707) acc_x 53.1250 (64.8438) lr 1.9354e-03 eta 0:00:06
epoch [25/200] batch [25/35] time 0.429 (0.469) data 0.298 (0.338) loss_x loss_x 2.0234 (1.3954) acc_x 43.7500 (64.3750) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [30/35] time 0.467 (0.461) data 0.336 (0.330) loss_x loss_x 1.5605 (1.4101) acc_x 53.1250 (63.0208) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [35/35] time 0.438 (0.458) data 0.307 (0.327) loss_x loss_x 1.5068 (1.4518) acc_x 62.5000 (62.5893) lr 1.9354e-03 eta 0:00:00
epoch [25/200] batch [5/62] time 0.441 (0.453) data 0.309 (0.322) loss_u loss_u 0.9185 (0.8769) acc_u 9.3750 (13.7500) lr 1.9354e-03 eta 0:00:25
epoch [25/200] batch [10/62] time 0.458 (0.459) data 0.327 (0.328) loss_u loss_u 0.8779 (0.8570) acc_u 12.5000 (16.5625) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [15/62] time 0.509 (0.459) data 0.377 (0.328) loss_u loss_u 0.8535 (0.8570) acc_u 15.6250 (16.8750) lr 1.9354e-03 eta 0:00:21
epoch [25/200] batch [20/62] time 0.395 (0.452) data 0.264 (0.321) loss_u loss_u 0.8843 (0.8562) acc_u 12.5000 (17.3438) lr 1.9354e-03 eta 0:00:19
epoch [25/200] batch [25/62] time 0.361 (0.451) data 0.229 (0.319) loss_u loss_u 0.8521 (0.8632) acc_u 15.6250 (16.5000) lr 1.9354e-03 eta 0:00:16
epoch [25/200] batch [30/62] time 0.584 (0.455) data 0.451 (0.323) loss_u loss_u 0.8750 (0.8614) acc_u 15.6250 (16.6667) lr 1.9354e-03 eta 0:00:14
epoch [25/200] batch [35/62] time 0.455 (0.456) data 0.324 (0.324) loss_u loss_u 0.8340 (0.8565) acc_u 21.8750 (17.8571) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [40/62] time 0.444 (0.452) data 0.313 (0.321) loss_u loss_u 0.8540 (0.8582) acc_u 18.7500 (17.8125) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [45/62] time 0.407 (0.451) data 0.273 (0.320) loss_u loss_u 0.9307 (0.8605) acc_u 12.5000 (17.5000) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [50/62] time 0.319 (0.451) data 0.187 (0.320) loss_u loss_u 0.8711 (0.8604) acc_u 12.5000 (17.7500) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [55/62] time 0.483 (0.449) data 0.351 (0.317) loss_u loss_u 0.8359 (0.8599) acc_u 21.8750 (17.8409) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [60/62] time 0.526 (0.449) data 0.394 (0.318) loss_u loss_u 0.8921 (0.8605) acc_u 15.6250 (17.8646) lr 1.9354e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1665
confident_label rate tensor(0.3651, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1145
clean true:1144
clean false:1
clean_rate:0.9991266375545852
noisy true:327
noisy false:1664
after delete: len(clean_dataset) 1145
after delete: len(noisy_dataset) 1991
epoch [26/200] batch [5/35] time 0.437 (0.485) data 0.307 (0.354) loss_x loss_x 1.7549 (1.6770) acc_x 56.2500 (53.7500) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [10/35] time 0.505 (0.462) data 0.375 (0.331) loss_x loss_x 1.5615 (1.5843) acc_x 56.2500 (56.5625) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [15/35] time 0.423 (0.473) data 0.293 (0.342) loss_x loss_x 0.8838 (1.5053) acc_x 78.1250 (60.0000) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [20/35] time 0.455 (0.461) data 0.326 (0.330) loss_x loss_x 0.9736 (1.4683) acc_x 75.0000 (61.2500) lr 1.9298e-03 eta 0:00:06
epoch [26/200] batch [25/35] time 0.417 (0.454) data 0.286 (0.323) loss_x loss_x 1.0732 (1.4423) acc_x 62.5000 (60.8750) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [30/35] time 0.544 (0.455) data 0.414 (0.325) loss_x loss_x 1.9512 (1.4190) acc_x 53.1250 (61.1458) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [35/35] time 0.416 (0.454) data 0.286 (0.324) loss_x loss_x 1.4766 (1.4281) acc_x 53.1250 (60.3571) lr 1.9298e-03 eta 0:00:00
epoch [26/200] batch [5/62] time 0.499 (0.454) data 0.368 (0.323) loss_u loss_u 0.7920 (0.8542) acc_u 34.3750 (19.3750) lr 1.9298e-03 eta 0:00:25
epoch [26/200] batch [10/62] time 0.584 (0.460) data 0.453 (0.330) loss_u loss_u 0.8657 (0.8453) acc_u 15.6250 (20.3125) lr 1.9298e-03 eta 0:00:23
epoch [26/200] batch [15/62] time 0.334 (0.454) data 0.202 (0.323) loss_u loss_u 0.8662 (0.8505) acc_u 15.6250 (18.7500) lr 1.9298e-03 eta 0:00:21
epoch [26/200] batch [20/62] time 0.415 (0.453) data 0.283 (0.322) loss_u loss_u 0.9004 (0.8559) acc_u 12.5000 (18.4375) lr 1.9298e-03 eta 0:00:19
epoch [26/200] batch [25/62] time 0.396 (0.449) data 0.266 (0.318) loss_u loss_u 0.8911 (0.8543) acc_u 12.5000 (18.6250) lr 1.9298e-03 eta 0:00:16
epoch [26/200] batch [30/62] time 0.381 (0.449) data 0.249 (0.318) loss_u loss_u 0.8667 (0.8548) acc_u 18.7500 (18.4375) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [35/62] time 0.431 (0.454) data 0.299 (0.324) loss_u loss_u 0.8525 (0.8524) acc_u 18.7500 (19.0179) lr 1.9298e-03 eta 0:00:12
epoch [26/200] batch [40/62] time 0.489 (0.454) data 0.357 (0.323) loss_u loss_u 0.8486 (0.8512) acc_u 12.5000 (19.1406) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [45/62] time 0.455 (0.454) data 0.323 (0.323) loss_u loss_u 0.8813 (0.8521) acc_u 18.7500 (19.1667) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [50/62] time 0.511 (0.457) data 0.379 (0.325) loss_u loss_u 0.8579 (0.8544) acc_u 18.7500 (19.0000) lr 1.9298e-03 eta 0:00:05
epoch [26/200] batch [55/62] time 0.403 (0.453) data 0.271 (0.322) loss_u loss_u 0.8633 (0.8568) acc_u 18.7500 (18.5795) lr 1.9298e-03 eta 0:00:03
epoch [26/200] batch [60/62] time 0.634 (0.455) data 0.504 (0.324) loss_u loss_u 0.8169 (0.8561) acc_u 28.1250 (18.8021) lr 1.9298e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1637
confident_label rate tensor(0.3756, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1178
clean true:1177
clean false:1
clean_rate:0.9991511035653651
noisy true:322
noisy false:1636
after delete: len(clean_dataset) 1178
after delete: len(noisy_dataset) 1958
epoch [27/200] batch [5/36] time 0.518 (0.488) data 0.387 (0.358) loss_x loss_x 1.4678 (1.3998) acc_x 68.7500 (69.3750) lr 1.9239e-03 eta 0:00:15
epoch [27/200] batch [10/36] time 0.476 (0.474) data 0.345 (0.344) loss_x loss_x 1.3164 (1.5548) acc_x 68.7500 (63.4375) lr 1.9239e-03 eta 0:00:12
epoch [27/200] batch [15/36] time 0.584 (0.497) data 0.454 (0.366) loss_x loss_x 1.3154 (1.4100) acc_x 75.0000 (64.5833) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [20/36] time 0.565 (0.483) data 0.433 (0.352) loss_x loss_x 1.1777 (1.4124) acc_x 81.2500 (65.0000) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [25/36] time 0.493 (0.489) data 0.362 (0.358) loss_x loss_x 0.9985 (1.4354) acc_x 71.8750 (63.3750) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [30/36] time 0.526 (0.492) data 0.396 (0.361) loss_x loss_x 1.1816 (1.4271) acc_x 65.6250 (63.8542) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [35/36] time 0.514 (0.481) data 0.383 (0.350) loss_x loss_x 1.9453 (1.4375) acc_x 53.1250 (63.2143) lr 1.9239e-03 eta 0:00:00
epoch [27/200] batch [5/61] time 0.499 (0.476) data 0.368 (0.345) loss_u loss_u 0.8110 (0.8517) acc_u 21.8750 (18.7500) lr 1.9239e-03 eta 0:00:26
epoch [27/200] batch [10/61] time 0.496 (0.472) data 0.365 (0.341) loss_u loss_u 0.8750 (0.8497) acc_u 12.5000 (17.8125) lr 1.9239e-03 eta 0:00:24
epoch [27/200] batch [15/61] time 0.389 (0.464) data 0.258 (0.333) loss_u loss_u 0.8560 (0.8507) acc_u 15.6250 (17.7083) lr 1.9239e-03 eta 0:00:21
epoch [27/200] batch [20/61] time 0.433 (0.460) data 0.301 (0.329) loss_u loss_u 0.8423 (0.8502) acc_u 21.8750 (17.8125) lr 1.9239e-03 eta 0:00:18
epoch [27/200] batch [25/61] time 0.402 (0.457) data 0.271 (0.326) loss_u loss_u 0.8848 (0.8495) acc_u 15.6250 (18.0000) lr 1.9239e-03 eta 0:00:16
epoch [27/200] batch [30/61] time 0.400 (0.459) data 0.268 (0.327) loss_u loss_u 0.8662 (0.8562) acc_u 15.6250 (17.3958) lr 1.9239e-03 eta 0:00:14
epoch [27/200] batch [35/61] time 0.504 (0.463) data 0.373 (0.332) loss_u loss_u 0.8267 (0.8548) acc_u 25.0000 (17.6786) lr 1.9239e-03 eta 0:00:12
epoch [27/200] batch [40/61] time 0.429 (0.457) data 0.298 (0.326) loss_u loss_u 0.8198 (0.8557) acc_u 25.0000 (17.8125) lr 1.9239e-03 eta 0:00:09
epoch [27/200] batch [45/61] time 0.478 (0.455) data 0.347 (0.324) loss_u loss_u 0.8223 (0.8546) acc_u 21.8750 (18.2639) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [50/61] time 0.490 (0.456) data 0.357 (0.325) loss_u loss_u 0.8013 (0.8534) acc_u 25.0000 (18.5625) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [55/61] time 0.441 (0.456) data 0.310 (0.324) loss_u loss_u 0.8906 (0.8550) acc_u 12.5000 (18.4091) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [60/61] time 0.332 (0.452) data 0.201 (0.321) loss_u loss_u 0.8564 (0.8547) acc_u 21.8750 (18.3333) lr 1.9239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1686
confident_label rate tensor(0.3591, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1126
clean true:1122
clean false:4
clean_rate:0.9964476021314387
noisy true:328
noisy false:1682
after delete: len(clean_dataset) 1126
after delete: len(noisy_dataset) 2010
epoch [28/200] batch [5/35] time 0.457 (0.515) data 0.326 (0.384) loss_x loss_x 0.9414 (1.2431) acc_x 75.0000 (66.8750) lr 1.9178e-03 eta 0:00:15
epoch [28/200] batch [10/35] time 0.494 (0.507) data 0.363 (0.376) loss_x loss_x 1.6592 (1.3634) acc_x 59.3750 (65.0000) lr 1.9178e-03 eta 0:00:12
epoch [28/200] batch [15/35] time 0.504 (0.484) data 0.374 (0.353) loss_x loss_x 1.6143 (1.4837) acc_x 56.2500 (63.5417) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [20/35] time 0.408 (0.470) data 0.277 (0.339) loss_x loss_x 1.0615 (1.4570) acc_x 71.8750 (63.4375) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [25/35] time 0.474 (0.459) data 0.344 (0.328) loss_x loss_x 1.8369 (1.4841) acc_x 50.0000 (61.7500) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [30/35] time 0.542 (0.465) data 0.412 (0.335) loss_x loss_x 1.3984 (1.4485) acc_x 56.2500 (62.0833) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [35/35] time 0.525 (0.470) data 0.395 (0.340) loss_x loss_x 1.0557 (1.4090) acc_x 78.1250 (63.2143) lr 1.9178e-03 eta 0:00:00
epoch [28/200] batch [5/62] time 0.378 (0.460) data 0.247 (0.330) loss_u loss_u 0.8867 (0.8414) acc_u 12.5000 (21.2500) lr 1.9178e-03 eta 0:00:26
epoch [28/200] batch [10/62] time 0.413 (0.460) data 0.282 (0.329) loss_u loss_u 0.8457 (0.8496) acc_u 21.8750 (18.7500) lr 1.9178e-03 eta 0:00:23
epoch [28/200] batch [15/62] time 0.491 (0.453) data 0.359 (0.322) loss_u loss_u 0.8857 (0.8530) acc_u 12.5000 (18.7500) lr 1.9178e-03 eta 0:00:21
epoch [28/200] batch [20/62] time 0.355 (0.453) data 0.224 (0.322) loss_u loss_u 0.8877 (0.8547) acc_u 18.7500 (19.2188) lr 1.9178e-03 eta 0:00:19
epoch [28/200] batch [25/62] time 0.487 (0.452) data 0.356 (0.321) loss_u loss_u 0.8662 (0.8534) acc_u 18.7500 (19.6250) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [30/62] time 0.579 (0.454) data 0.448 (0.323) loss_u loss_u 0.7998 (0.8518) acc_u 25.0000 (19.6875) lr 1.9178e-03 eta 0:00:14
epoch [28/200] batch [35/62] time 0.541 (0.451) data 0.410 (0.320) loss_u loss_u 0.8584 (0.8544) acc_u 15.6250 (19.3750) lr 1.9178e-03 eta 0:00:12
epoch [28/200] batch [40/62] time 0.449 (0.456) data 0.318 (0.325) loss_u loss_u 0.8457 (0.8518) acc_u 21.8750 (19.9219) lr 1.9178e-03 eta 0:00:10
epoch [28/200] batch [45/62] time 0.377 (0.454) data 0.246 (0.323) loss_u loss_u 0.8667 (0.8519) acc_u 12.5000 (19.7917) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [50/62] time 0.332 (0.452) data 0.201 (0.321) loss_u loss_u 0.8809 (0.8544) acc_u 12.5000 (19.2500) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [55/62] time 0.364 (0.450) data 0.234 (0.319) loss_u loss_u 0.8569 (0.8528) acc_u 12.5000 (19.6591) lr 1.9178e-03 eta 0:00:03
epoch [28/200] batch [60/62] time 0.382 (0.448) data 0.251 (0.317) loss_u loss_u 0.8770 (0.8552) acc_u 15.6250 (19.2188) lr 1.9178e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1675
confident_label rate tensor(0.3568, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1119
clean true:1117
clean false:2
clean_rate:0.998212689901698
noisy true:344
noisy false:1673
after delete: len(clean_dataset) 1119
after delete: len(noisy_dataset) 2017
epoch [29/200] batch [5/34] time 0.448 (0.481) data 0.318 (0.350) loss_x loss_x 1.3350 (1.4055) acc_x 62.5000 (66.2500) lr 1.9114e-03 eta 0:00:13
epoch [29/200] batch [10/34] time 0.386 (0.456) data 0.255 (0.326) loss_x loss_x 1.2812 (1.3217) acc_x 62.5000 (69.3750) lr 1.9114e-03 eta 0:00:10
epoch [29/200] batch [15/34] time 0.451 (0.460) data 0.321 (0.330) loss_x loss_x 1.5537 (1.2822) acc_x 65.6250 (70.0000) lr 1.9114e-03 eta 0:00:08
epoch [29/200] batch [20/34] time 0.393 (0.460) data 0.261 (0.330) loss_x loss_x 1.6348 (1.3063) acc_x 62.5000 (67.3438) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [25/34] time 0.633 (0.483) data 0.502 (0.353) loss_x loss_x 2.0293 (1.3302) acc_x 43.7500 (66.6250) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [30/34] time 0.423 (0.475) data 0.293 (0.344) loss_x loss_x 1.1582 (1.2937) acc_x 71.8750 (67.7083) lr 1.9114e-03 eta 0:00:01
epoch [29/200] batch [5/63] time 0.428 (0.464) data 0.297 (0.333) loss_u loss_u 0.8193 (0.8290) acc_u 28.1250 (23.1250) lr 1.9114e-03 eta 0:00:26
epoch [29/200] batch [10/63] time 0.412 (0.466) data 0.280 (0.336) loss_u loss_u 0.8667 (0.8549) acc_u 15.6250 (18.7500) lr 1.9114e-03 eta 0:00:24
epoch [29/200] batch [15/63] time 0.469 (0.466) data 0.336 (0.336) loss_u loss_u 0.8052 (0.8523) acc_u 25.0000 (19.1667) lr 1.9114e-03 eta 0:00:22
epoch [29/200] batch [20/63] time 0.396 (0.462) data 0.265 (0.332) loss_u loss_u 0.8706 (0.8526) acc_u 18.7500 (19.5312) lr 1.9114e-03 eta 0:00:19
epoch [29/200] batch [25/63] time 0.476 (0.458) data 0.345 (0.327) loss_u loss_u 0.8545 (0.8489) acc_u 18.7500 (20.0000) lr 1.9114e-03 eta 0:00:17
epoch [29/200] batch [30/63] time 0.355 (0.452) data 0.224 (0.322) loss_u loss_u 0.8394 (0.8524) acc_u 21.8750 (19.4792) lr 1.9114e-03 eta 0:00:14
epoch [29/200] batch [35/63] time 0.388 (0.450) data 0.256 (0.319) loss_u loss_u 0.8740 (0.8529) acc_u 15.6250 (19.3750) lr 1.9114e-03 eta 0:00:12
epoch [29/200] batch [40/63] time 0.369 (0.445) data 0.238 (0.315) loss_u loss_u 0.8306 (0.8513) acc_u 25.0000 (19.5312) lr 1.9114e-03 eta 0:00:10
epoch [29/200] batch [45/63] time 0.417 (0.440) data 0.285 (0.310) loss_u loss_u 0.7788 (0.8502) acc_u 34.3750 (19.5833) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [50/63] time 0.380 (0.441) data 0.249 (0.310) loss_u loss_u 0.8325 (0.8497) acc_u 25.0000 (19.5625) lr 1.9114e-03 eta 0:00:05
epoch [29/200] batch [55/63] time 0.566 (0.445) data 0.434 (0.314) loss_u loss_u 0.8555 (0.8496) acc_u 21.8750 (19.4886) lr 1.9114e-03 eta 0:00:03
epoch [29/200] batch [60/63] time 0.557 (0.445) data 0.425 (0.314) loss_u loss_u 0.8906 (0.8498) acc_u 12.5000 (19.4271) lr 1.9114e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1632
confident_label rate tensor(0.3718, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1166
clean true:1163
clean false:3
clean_rate:0.9974271012006861
noisy true:341
noisy false:1629
after delete: len(clean_dataset) 1166
after delete: len(noisy_dataset) 1970
epoch [30/200] batch [5/36] time 0.525 (0.463) data 0.395 (0.333) loss_x loss_x 1.4619 (1.3547) acc_x 75.0000 (66.2500) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [10/36] time 0.424 (0.473) data 0.293 (0.343) loss_x loss_x 1.4131 (1.3749) acc_x 59.3750 (63.7500) lr 1.9048e-03 eta 0:00:12
epoch [30/200] batch [15/36] time 0.415 (0.479) data 0.285 (0.348) loss_x loss_x 1.9365 (1.4306) acc_x 53.1250 (63.1250) lr 1.9048e-03 eta 0:00:10
epoch [30/200] batch [20/36] time 0.386 (0.467) data 0.255 (0.337) loss_x loss_x 1.8105 (1.4170) acc_x 46.8750 (62.8125) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [25/36] time 0.381 (0.457) data 0.251 (0.326) loss_x loss_x 1.7812 (1.4378) acc_x 40.6250 (62.0000) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [30/36] time 0.540 (0.460) data 0.409 (0.330) loss_x loss_x 1.6602 (1.4450) acc_x 62.5000 (62.2917) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [35/36] time 0.490 (0.464) data 0.359 (0.333) loss_x loss_x 1.5625 (1.4573) acc_x 53.1250 (62.1429) lr 1.9048e-03 eta 0:00:00
epoch [30/200] batch [5/61] time 0.396 (0.469) data 0.264 (0.338) loss_u loss_u 0.8350 (0.8451) acc_u 18.7500 (18.1250) lr 1.9048e-03 eta 0:00:26
epoch [30/200] batch [10/61] time 0.374 (0.469) data 0.243 (0.338) loss_u loss_u 0.8037 (0.8231) acc_u 25.0000 (23.1250) lr 1.9048e-03 eta 0:00:23
epoch [30/200] batch [15/61] time 0.419 (0.465) data 0.288 (0.334) loss_u loss_u 0.8257 (0.8234) acc_u 25.0000 (23.9583) lr 1.9048e-03 eta 0:00:21
epoch [30/200] batch [20/61] time 0.529 (0.464) data 0.398 (0.333) loss_u loss_u 0.7817 (0.8216) acc_u 28.1250 (23.9062) lr 1.9048e-03 eta 0:00:19
epoch [30/200] batch [25/61] time 0.409 (0.462) data 0.278 (0.331) loss_u loss_u 0.9199 (0.8230) acc_u 12.5000 (23.2500) lr 1.9048e-03 eta 0:00:16
epoch [30/200] batch [30/61] time 0.434 (0.459) data 0.303 (0.328) loss_u loss_u 0.8926 (0.8312) acc_u 12.5000 (22.2917) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [35/61] time 0.387 (0.463) data 0.255 (0.332) loss_u loss_u 0.8872 (0.8375) acc_u 15.6250 (21.8750) lr 1.9048e-03 eta 0:00:12
epoch [30/200] batch [40/61] time 0.349 (0.459) data 0.219 (0.328) loss_u loss_u 0.8521 (0.8404) acc_u 21.8750 (21.3281) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [45/61] time 0.392 (0.455) data 0.261 (0.324) loss_u loss_u 0.8862 (0.8448) acc_u 15.6250 (20.5556) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [50/61] time 0.431 (0.456) data 0.299 (0.325) loss_u loss_u 0.7856 (0.8450) acc_u 25.0000 (20.3750) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [55/61] time 0.342 (0.455) data 0.210 (0.324) loss_u loss_u 0.8643 (0.8449) acc_u 18.7500 (20.3977) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [60/61] time 0.370 (0.453) data 0.239 (0.322) loss_u loss_u 0.8164 (0.8439) acc_u 25.0000 (20.6250) lr 1.9048e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1657
confident_label rate tensor(0.3673, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1152
clean true:1151
clean false:1
clean_rate:0.9991319444444444
noisy true:328
noisy false:1656
after delete: len(clean_dataset) 1152
after delete: len(noisy_dataset) 1984
epoch [31/200] batch [5/36] time 0.541 (0.494) data 0.411 (0.364) loss_x loss_x 2.0449 (1.5725) acc_x 46.8750 (59.3750) lr 1.8980e-03 eta 0:00:15
epoch [31/200] batch [10/36] time 0.662 (0.476) data 0.532 (0.346) loss_x loss_x 1.3711 (1.4635) acc_x 62.5000 (62.5000) lr 1.8980e-03 eta 0:00:12
epoch [31/200] batch [15/36] time 0.500 (0.476) data 0.370 (0.345) loss_x loss_x 0.8955 (1.4122) acc_x 84.3750 (63.9583) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [20/36] time 0.392 (0.492) data 0.260 (0.362) loss_x loss_x 1.3516 (1.3847) acc_x 78.1250 (64.5312) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [25/36] time 0.399 (0.484) data 0.268 (0.354) loss_x loss_x 1.3066 (1.3531) acc_x 75.0000 (65.5000) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [30/36] time 0.451 (0.477) data 0.320 (0.346) loss_x loss_x 1.2207 (1.3666) acc_x 75.0000 (65.7292) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [35/36] time 0.372 (0.473) data 0.241 (0.342) loss_x loss_x 1.4697 (1.3892) acc_x 65.6250 (65.4464) lr 1.8980e-03 eta 0:00:00
epoch [31/200] batch [5/62] time 0.460 (0.470) data 0.329 (0.339) loss_u loss_u 0.8540 (0.8342) acc_u 21.8750 (21.2500) lr 1.8980e-03 eta 0:00:26
epoch [31/200] batch [10/62] time 0.509 (0.466) data 0.377 (0.335) loss_u loss_u 0.8643 (0.8336) acc_u 15.6250 (20.0000) lr 1.8980e-03 eta 0:00:24
epoch [31/200] batch [15/62] time 0.359 (0.468) data 0.227 (0.337) loss_u loss_u 0.8804 (0.8391) acc_u 12.5000 (19.7917) lr 1.8980e-03 eta 0:00:21
epoch [31/200] batch [20/62] time 0.424 (0.467) data 0.292 (0.335) loss_u loss_u 0.9238 (0.8493) acc_u 9.3750 (18.7500) lr 1.8980e-03 eta 0:00:19
epoch [31/200] batch [25/62] time 0.384 (0.463) data 0.252 (0.332) loss_u loss_u 0.8608 (0.8481) acc_u 15.6250 (19.3750) lr 1.8980e-03 eta 0:00:17
epoch [31/200] batch [30/62] time 0.357 (0.467) data 0.224 (0.336) loss_u loss_u 0.8525 (0.8518) acc_u 21.8750 (18.7500) lr 1.8980e-03 eta 0:00:14
epoch [31/200] batch [35/62] time 0.389 (0.469) data 0.255 (0.338) loss_u loss_u 0.7529 (0.8534) acc_u 31.2500 (18.2143) lr 1.8980e-03 eta 0:00:12
epoch [31/200] batch [40/62] time 0.421 (0.472) data 0.290 (0.340) loss_u loss_u 0.9321 (0.8544) acc_u 6.2500 (18.2812) lr 1.8980e-03 eta 0:00:10
epoch [31/200] batch [45/62] time 0.496 (0.469) data 0.364 (0.337) loss_u loss_u 0.8560 (0.8572) acc_u 9.3750 (17.5694) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [50/62] time 0.415 (0.466) data 0.283 (0.335) loss_u loss_u 0.9028 (0.8585) acc_u 9.3750 (17.4375) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [55/62] time 0.458 (0.466) data 0.325 (0.334) loss_u loss_u 0.8501 (0.8566) acc_u 25.0000 (17.9545) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [60/62] time 0.459 (0.465) data 0.327 (0.333) loss_u loss_u 0.7979 (0.8569) acc_u 25.0000 (17.9688) lr 1.8980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1644
confident_label rate tensor(0.3718, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1166
clean true:1164
clean false:2
clean_rate:0.9982847341337907
noisy true:328
noisy false:1642
after delete: len(clean_dataset) 1166
after delete: len(noisy_dataset) 1970
epoch [32/200] batch [5/36] time 0.444 (0.405) data 0.314 (0.275) loss_x loss_x 1.7998 (1.7395) acc_x 59.3750 (57.5000) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [10/36] time 0.516 (0.439) data 0.386 (0.309) loss_x loss_x 1.0303 (1.4608) acc_x 71.8750 (62.1875) lr 1.8910e-03 eta 0:00:11
epoch [32/200] batch [15/36] time 0.468 (0.452) data 0.338 (0.322) loss_x loss_x 1.6367 (1.4356) acc_x 62.5000 (61.8750) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [20/36] time 0.397 (0.470) data 0.267 (0.339) loss_x loss_x 1.1865 (1.4080) acc_x 71.8750 (62.9688) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [25/36] time 0.417 (0.469) data 0.287 (0.339) loss_x loss_x 1.8154 (1.3921) acc_x 50.0000 (63.7500) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [30/36] time 0.405 (0.458) data 0.276 (0.327) loss_x loss_x 1.1436 (1.3875) acc_x 68.7500 (64.0625) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [35/36] time 0.533 (0.466) data 0.404 (0.336) loss_x loss_x 1.4365 (1.3779) acc_x 62.5000 (63.9286) lr 1.8910e-03 eta 0:00:00
epoch [32/200] batch [5/61] time 0.402 (0.457) data 0.271 (0.326) loss_u loss_u 0.8799 (0.8642) acc_u 15.6250 (18.7500) lr 1.8910e-03 eta 0:00:25
epoch [32/200] batch [10/61] time 0.388 (0.454) data 0.259 (0.324) loss_u loss_u 0.8345 (0.8622) acc_u 25.0000 (19.0625) lr 1.8910e-03 eta 0:00:23
epoch [32/200] batch [15/61] time 0.325 (0.450) data 0.194 (0.319) loss_u loss_u 0.7446 (0.8528) acc_u 28.1250 (19.7917) lr 1.8910e-03 eta 0:00:20
epoch [32/200] batch [20/61] time 0.522 (0.451) data 0.392 (0.321) loss_u loss_u 0.8789 (0.8561) acc_u 12.5000 (18.2812) lr 1.8910e-03 eta 0:00:18
epoch [32/200] batch [25/61] time 0.655 (0.460) data 0.524 (0.330) loss_u loss_u 0.8413 (0.8543) acc_u 21.8750 (18.7500) lr 1.8910e-03 eta 0:00:16
epoch [32/200] batch [30/61] time 0.434 (0.465) data 0.303 (0.335) loss_u loss_u 0.8701 (0.8579) acc_u 12.5000 (18.2292) lr 1.8910e-03 eta 0:00:14
epoch [32/200] batch [35/61] time 0.437 (0.464) data 0.305 (0.333) loss_u loss_u 0.8657 (0.8587) acc_u 15.6250 (18.0357) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [40/61] time 0.464 (0.462) data 0.333 (0.332) loss_u loss_u 0.9067 (0.8548) acc_u 12.5000 (18.3594) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [45/61] time 0.540 (0.464) data 0.409 (0.333) loss_u loss_u 0.8872 (0.8526) acc_u 15.6250 (18.8889) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [50/61] time 0.451 (0.460) data 0.320 (0.329) loss_u loss_u 0.8013 (0.8510) acc_u 28.1250 (19.2500) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [55/61] time 0.367 (0.458) data 0.235 (0.327) loss_u loss_u 0.9009 (0.8517) acc_u 12.5000 (18.9205) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [60/61] time 0.458 (0.460) data 0.327 (0.329) loss_u loss_u 0.7910 (0.8523) acc_u 34.3750 (19.0625) lr 1.8910e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1669
confident_label rate tensor(0.3635, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1140
clean true:1137
clean false:3
clean_rate:0.9973684210526316
noisy true:330
noisy false:1666
after delete: len(clean_dataset) 1140
after delete: len(noisy_dataset) 1996
epoch [33/200] batch [5/35] time 0.497 (0.473) data 0.368 (0.343) loss_x loss_x 1.1348 (1.3088) acc_x 62.5000 (66.2500) lr 1.8838e-03 eta 0:00:14
epoch [33/200] batch [10/35] time 0.346 (0.446) data 0.215 (0.316) loss_x loss_x 1.6572 (1.3980) acc_x 65.6250 (66.2500) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [15/35] time 0.430 (0.469) data 0.300 (0.338) loss_x loss_x 0.8145 (1.3080) acc_x 81.2500 (68.3333) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [20/35] time 0.370 (0.457) data 0.239 (0.327) loss_x loss_x 1.1270 (1.2729) acc_x 68.7500 (67.9688) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [25/35] time 0.525 (0.458) data 0.395 (0.327) loss_x loss_x 2.2129 (1.2992) acc_x 46.8750 (66.6250) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [30/35] time 0.535 (0.467) data 0.403 (0.336) loss_x loss_x 1.5088 (1.3277) acc_x 62.5000 (65.7292) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [35/35] time 0.395 (0.468) data 0.264 (0.337) loss_x loss_x 1.3604 (1.3474) acc_x 71.8750 (65.4464) lr 1.8838e-03 eta 0:00:00
epoch [33/200] batch [5/62] time 0.408 (0.462) data 0.278 (0.331) loss_u loss_u 0.9189 (0.8574) acc_u 9.3750 (17.5000) lr 1.8838e-03 eta 0:00:26
epoch [33/200] batch [10/62] time 0.600 (0.462) data 0.470 (0.331) loss_u loss_u 0.8872 (0.8405) acc_u 15.6250 (20.9375) lr 1.8838e-03 eta 0:00:24
epoch [33/200] batch [15/62] time 0.348 (0.460) data 0.218 (0.330) loss_u loss_u 0.7944 (0.8339) acc_u 21.8750 (21.2500) lr 1.8838e-03 eta 0:00:21
epoch [33/200] batch [20/62] time 0.438 (0.457) data 0.307 (0.326) loss_u loss_u 0.7603 (0.8309) acc_u 37.5000 (22.0312) lr 1.8838e-03 eta 0:00:19
epoch [33/200] batch [25/62] time 0.434 (0.453) data 0.304 (0.323) loss_u loss_u 0.8330 (0.8282) acc_u 18.7500 (22.3750) lr 1.8838e-03 eta 0:00:16
epoch [33/200] batch [30/62] time 0.437 (0.453) data 0.306 (0.322) loss_u loss_u 0.7974 (0.8295) acc_u 28.1250 (22.1875) lr 1.8838e-03 eta 0:00:14
epoch [33/200] batch [35/62] time 0.406 (0.454) data 0.275 (0.323) loss_u loss_u 0.7559 (0.8273) acc_u 37.5000 (22.5000) lr 1.8838e-03 eta 0:00:12
epoch [33/200] batch [40/62] time 0.474 (0.453) data 0.344 (0.322) loss_u loss_u 0.9058 (0.8351) acc_u 15.6250 (21.3281) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [45/62] time 0.398 (0.451) data 0.267 (0.320) loss_u loss_u 0.7974 (0.8379) acc_u 21.8750 (20.6944) lr 1.8838e-03 eta 0:00:07
epoch [33/200] batch [50/62] time 0.426 (0.451) data 0.295 (0.320) loss_u loss_u 0.7959 (0.8396) acc_u 21.8750 (20.6250) lr 1.8838e-03 eta 0:00:05
epoch [33/200] batch [55/62] time 0.357 (0.450) data 0.225 (0.319) loss_u loss_u 0.8872 (0.8388) acc_u 12.5000 (21.0227) lr 1.8838e-03 eta 0:00:03
epoch [33/200] batch [60/62] time 0.397 (0.451) data 0.265 (0.320) loss_u loss_u 0.8350 (0.8395) acc_u 21.8750 (20.9375) lr 1.8838e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1636
confident_label rate tensor(0.3811, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1195
clean true:1191
clean false:4
clean_rate:0.9966527196652719
noisy true:309
noisy false:1632
after delete: len(clean_dataset) 1195
after delete: len(noisy_dataset) 1941
epoch [34/200] batch [5/37] time 0.503 (0.487) data 0.374 (0.357) loss_x loss_x 1.1543 (1.4387) acc_x 75.0000 (60.6250) lr 1.8763e-03 eta 0:00:15
epoch [34/200] batch [10/37] time 0.426 (0.483) data 0.296 (0.353) loss_x loss_x 1.4365 (1.3857) acc_x 56.2500 (62.1875) lr 1.8763e-03 eta 0:00:13
epoch [34/200] batch [15/37] time 0.394 (0.462) data 0.264 (0.332) loss_x loss_x 1.9600 (1.4534) acc_x 65.6250 (61.4583) lr 1.8763e-03 eta 0:00:10
epoch [34/200] batch [20/37] time 0.390 (0.450) data 0.259 (0.320) loss_x loss_x 1.8359 (1.4589) acc_x 53.1250 (61.5625) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [25/37] time 0.531 (0.461) data 0.401 (0.331) loss_x loss_x 1.2627 (1.4492) acc_x 62.5000 (62.0000) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [30/37] time 0.521 (0.458) data 0.391 (0.328) loss_x loss_x 1.0801 (1.4391) acc_x 71.8750 (62.3958) lr 1.8763e-03 eta 0:00:03
epoch [34/200] batch [35/37] time 0.401 (0.455) data 0.270 (0.325) loss_x loss_x 1.2539 (1.4051) acc_x 62.5000 (63.1250) lr 1.8763e-03 eta 0:00:00
epoch [34/200] batch [5/60] time 0.421 (0.463) data 0.289 (0.333) loss_u loss_u 0.8862 (0.8367) acc_u 12.5000 (21.8750) lr 1.8763e-03 eta 0:00:25
epoch [34/200] batch [10/60] time 0.422 (0.459) data 0.291 (0.329) loss_u loss_u 0.8569 (0.8302) acc_u 18.7500 (22.5000) lr 1.8763e-03 eta 0:00:22
epoch [34/200] batch [15/60] time 0.495 (0.457) data 0.364 (0.326) loss_u loss_u 0.8594 (0.8363) acc_u 15.6250 (22.0833) lr 1.8763e-03 eta 0:00:20
epoch [34/200] batch [20/60] time 0.495 (0.456) data 0.364 (0.326) loss_u loss_u 0.8608 (0.8319) acc_u 28.1250 (22.3438) lr 1.8763e-03 eta 0:00:18
epoch [34/200] batch [25/60] time 0.552 (0.457) data 0.420 (0.327) loss_u loss_u 0.8999 (0.8388) acc_u 12.5000 (21.5000) lr 1.8763e-03 eta 0:00:16
epoch [34/200] batch [30/60] time 0.445 (0.455) data 0.314 (0.324) loss_u loss_u 0.8560 (0.8402) acc_u 18.7500 (21.0417) lr 1.8763e-03 eta 0:00:13
epoch [34/200] batch [35/60] time 0.397 (0.453) data 0.266 (0.322) loss_u loss_u 0.8760 (0.8449) acc_u 15.6250 (20.0893) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [40/60] time 0.391 (0.454) data 0.260 (0.323) loss_u loss_u 0.8145 (0.8443) acc_u 25.0000 (20.0781) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [45/60] time 0.488 (0.452) data 0.357 (0.322) loss_u loss_u 0.8911 (0.8451) acc_u 21.8750 (20.2778) lr 1.8763e-03 eta 0:00:06
epoch [34/200] batch [50/60] time 0.358 (0.453) data 0.227 (0.322) loss_u loss_u 0.8130 (0.8454) acc_u 21.8750 (20.1875) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [55/60] time 0.589 (0.451) data 0.458 (0.320) loss_u loss_u 0.8662 (0.8472) acc_u 15.6250 (19.8295) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [60/60] time 0.476 (0.450) data 0.345 (0.319) loss_u loss_u 0.8105 (0.8441) acc_u 25.0000 (20.3646) lr 1.8763e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1613
confident_label rate tensor(0.3756, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1178
clean true:1175
clean false:3
clean_rate:0.9974533106960951
noisy true:348
noisy false:1610
after delete: len(clean_dataset) 1178
after delete: len(noisy_dataset) 1958
epoch [35/200] batch [5/36] time 0.393 (0.413) data 0.262 (0.282) loss_x loss_x 1.9375 (1.3925) acc_x 50.0000 (65.0000) lr 1.8686e-03 eta 0:00:12
epoch [35/200] batch [10/36] time 0.432 (0.424) data 0.302 (0.293) loss_x loss_x 1.2178 (1.3735) acc_x 78.1250 (65.6250) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [15/36] time 0.625 (0.457) data 0.495 (0.327) loss_x loss_x 1.3242 (1.2756) acc_x 62.5000 (67.0833) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [20/36] time 0.451 (0.467) data 0.319 (0.336) loss_x loss_x 1.1963 (1.2449) acc_x 68.7500 (68.5938) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [25/36] time 0.577 (0.471) data 0.445 (0.340) loss_x loss_x 1.3496 (1.2662) acc_x 62.5000 (67.1250) lr 1.8686e-03 eta 0:00:05
epoch [35/200] batch [30/36] time 0.505 (0.468) data 0.375 (0.337) loss_x loss_x 1.2402 (1.2641) acc_x 71.8750 (67.9167) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [35/36] time 0.418 (0.460) data 0.288 (0.330) loss_x loss_x 1.5156 (1.2904) acc_x 68.7500 (67.1429) lr 1.8686e-03 eta 0:00:00
epoch [35/200] batch [5/61] time 0.523 (0.453) data 0.391 (0.322) loss_u loss_u 0.9282 (0.8857) acc_u 9.3750 (14.3750) lr 1.8686e-03 eta 0:00:25
epoch [35/200] batch [10/61] time 0.549 (0.464) data 0.418 (0.333) loss_u loss_u 0.8325 (0.8676) acc_u 21.8750 (17.8125) lr 1.8686e-03 eta 0:00:23
epoch [35/200] batch [15/61] time 0.549 (0.463) data 0.417 (0.332) loss_u loss_u 0.8115 (0.8590) acc_u 21.8750 (18.9583) lr 1.8686e-03 eta 0:00:21
epoch [35/200] batch [20/61] time 0.454 (0.459) data 0.323 (0.328) loss_u loss_u 0.8613 (0.8634) acc_u 28.1250 (19.0625) lr 1.8686e-03 eta 0:00:18
epoch [35/200] batch [25/61] time 0.393 (0.454) data 0.261 (0.323) loss_u loss_u 0.7959 (0.8580) acc_u 31.2500 (19.7500) lr 1.8686e-03 eta 0:00:16
epoch [35/200] batch [30/61] time 0.436 (0.456) data 0.305 (0.325) loss_u loss_u 0.8232 (0.8538) acc_u 18.7500 (19.8958) lr 1.8686e-03 eta 0:00:14
epoch [35/200] batch [35/61] time 0.458 (0.454) data 0.327 (0.323) loss_u loss_u 0.8721 (0.8567) acc_u 18.7500 (19.6429) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [40/61] time 0.456 (0.454) data 0.325 (0.323) loss_u loss_u 0.8179 (0.8543) acc_u 21.8750 (19.9219) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [45/61] time 0.538 (0.454) data 0.407 (0.323) loss_u loss_u 0.8794 (0.8548) acc_u 21.8750 (20.3472) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [50/61] time 0.491 (0.449) data 0.360 (0.318) loss_u loss_u 0.8037 (0.8550) acc_u 25.0000 (19.8750) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [55/61] time 0.407 (0.452) data 0.276 (0.321) loss_u loss_u 0.9307 (0.8593) acc_u 12.5000 (19.2614) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [60/61] time 0.406 (0.451) data 0.275 (0.320) loss_u loss_u 0.9097 (0.8546) acc_u 9.3750 (19.8958) lr 1.8686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1612
confident_label rate tensor(0.3753, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1177
clean true:1175
clean false:2
clean_rate:0.9983007646559049
noisy true:349
noisy false:1610
after delete: len(clean_dataset) 1177
after delete: len(noisy_dataset) 1959
epoch [36/200] batch [5/36] time 0.521 (0.456) data 0.390 (0.325) loss_x loss_x 1.2754 (1.1899) acc_x 65.6250 (65.6250) lr 1.8607e-03 eta 0:00:14
epoch [36/200] batch [10/36] time 0.527 (0.480) data 0.396 (0.349) loss_x loss_x 0.8784 (1.2025) acc_x 78.1250 (67.5000) lr 1.8607e-03 eta 0:00:12
epoch [36/200] batch [15/36] time 0.465 (0.490) data 0.335 (0.359) loss_x loss_x 1.7705 (1.4163) acc_x 56.2500 (63.9583) lr 1.8607e-03 eta 0:00:10
epoch [36/200] batch [20/36] time 0.409 (0.468) data 0.279 (0.337) loss_x loss_x 1.8672 (1.4288) acc_x 46.8750 (63.2812) lr 1.8607e-03 eta 0:00:07
epoch [36/200] batch [25/36] time 0.453 (0.452) data 0.323 (0.321) loss_x loss_x 1.4775 (1.4391) acc_x 68.7500 (62.8750) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [30/36] time 0.335 (0.446) data 0.204 (0.315) loss_x loss_x 1.1670 (1.4020) acc_x 78.1250 (63.5417) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [35/36] time 0.594 (0.454) data 0.463 (0.324) loss_x loss_x 1.7607 (1.3907) acc_x 59.3750 (64.2857) lr 1.8607e-03 eta 0:00:00
epoch [36/200] batch [5/61] time 0.508 (0.460) data 0.378 (0.329) loss_u loss_u 0.7969 (0.8382) acc_u 18.7500 (18.7500) lr 1.8607e-03 eta 0:00:25
epoch [36/200] batch [10/61] time 0.467 (0.458) data 0.337 (0.328) loss_u loss_u 0.8823 (0.8359) acc_u 12.5000 (20.0000) lr 1.8607e-03 eta 0:00:23
epoch [36/200] batch [15/61] time 0.433 (0.456) data 0.302 (0.325) loss_u loss_u 0.8452 (0.8444) acc_u 18.7500 (18.5417) lr 1.8607e-03 eta 0:00:20
epoch [36/200] batch [20/61] time 0.402 (0.452) data 0.269 (0.322) loss_u loss_u 0.8716 (0.8454) acc_u 15.6250 (19.2188) lr 1.8607e-03 eta 0:00:18
epoch [36/200] batch [25/61] time 0.438 (0.450) data 0.307 (0.319) loss_u loss_u 0.8599 (0.8504) acc_u 18.7500 (18.7500) lr 1.8607e-03 eta 0:00:16
epoch [36/200] batch [30/61] time 0.419 (0.449) data 0.285 (0.318) loss_u loss_u 0.8721 (0.8512) acc_u 18.7500 (18.8542) lr 1.8607e-03 eta 0:00:13
epoch [36/200] batch [35/61] time 0.433 (0.448) data 0.301 (0.317) loss_u loss_u 0.8252 (0.8502) acc_u 25.0000 (19.2857) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [40/61] time 0.476 (0.449) data 0.344 (0.318) loss_u loss_u 0.8584 (0.8463) acc_u 18.7500 (20.1562) lr 1.8607e-03 eta 0:00:09
epoch [36/200] batch [45/61] time 0.428 (0.449) data 0.298 (0.318) loss_u loss_u 0.8369 (0.8477) acc_u 25.0000 (20.0000) lr 1.8607e-03 eta 0:00:07
epoch [36/200] batch [50/61] time 0.397 (0.450) data 0.267 (0.319) loss_u loss_u 0.8662 (0.8456) acc_u 15.6250 (20.3125) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [55/61] time 0.395 (0.451) data 0.265 (0.320) loss_u loss_u 0.8335 (0.8469) acc_u 18.7500 (20.0000) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [60/61] time 0.433 (0.449) data 0.304 (0.318) loss_u loss_u 0.7568 (0.8470) acc_u 25.0000 (19.9479) lr 1.8607e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1618
confident_label rate tensor(0.3760, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1179
clean true:1179
clean false:0
clean_rate:1.0
noisy true:339
noisy false:1618
after delete: len(clean_dataset) 1179
after delete: len(noisy_dataset) 1957
epoch [37/200] batch [5/36] time 0.361 (0.415) data 0.230 (0.284) loss_x loss_x 1.3584 (1.4088) acc_x 65.6250 (65.6250) lr 1.8526e-03 eta 0:00:12
epoch [37/200] batch [10/36] time 0.466 (0.431) data 0.334 (0.300) loss_x loss_x 1.0010 (1.3018) acc_x 71.8750 (66.8750) lr 1.8526e-03 eta 0:00:11
epoch [37/200] batch [15/36] time 0.408 (0.427) data 0.277 (0.296) loss_x loss_x 1.1182 (1.3513) acc_x 68.7500 (64.7917) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [20/36] time 0.419 (0.449) data 0.288 (0.318) loss_x loss_x 1.2832 (1.3595) acc_x 75.0000 (64.0625) lr 1.8526e-03 eta 0:00:07
epoch [37/200] batch [25/36] time 0.454 (0.451) data 0.324 (0.320) loss_x loss_x 1.3721 (1.3511) acc_x 62.5000 (63.6250) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [30/36] time 0.385 (0.454) data 0.255 (0.324) loss_x loss_x 1.5830 (1.3386) acc_x 59.3750 (64.0625) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [35/36] time 0.434 (0.448) data 0.303 (0.317) loss_x loss_x 1.3896 (1.3520) acc_x 59.3750 (63.3929) lr 1.8526e-03 eta 0:00:00
epoch [37/200] batch [5/61] time 0.433 (0.447) data 0.302 (0.316) loss_u loss_u 0.8638 (0.8526) acc_u 25.0000 (20.6250) lr 1.8526e-03 eta 0:00:25
epoch [37/200] batch [10/61] time 0.509 (0.446) data 0.377 (0.316) loss_u loss_u 0.8315 (0.8315) acc_u 28.1250 (24.6875) lr 1.8526e-03 eta 0:00:22
epoch [37/200] batch [15/61] time 0.401 (0.448) data 0.268 (0.317) loss_u loss_u 0.9209 (0.8441) acc_u 12.5000 (21.4583) lr 1.8526e-03 eta 0:00:20
epoch [37/200] batch [20/61] time 0.478 (0.451) data 0.346 (0.320) loss_u loss_u 0.8257 (0.8416) acc_u 21.8750 (21.0938) lr 1.8526e-03 eta 0:00:18
epoch [37/200] batch [25/61] time 0.404 (0.452) data 0.273 (0.321) loss_u loss_u 0.8857 (0.8492) acc_u 18.7500 (20.5000) lr 1.8526e-03 eta 0:00:16
epoch [37/200] batch [30/61] time 0.741 (0.463) data 0.610 (0.332) loss_u loss_u 0.7979 (0.8465) acc_u 25.0000 (20.3125) lr 1.8526e-03 eta 0:00:14
epoch [37/200] batch [35/61] time 0.493 (0.461) data 0.360 (0.329) loss_u loss_u 0.9253 (0.8514) acc_u 12.5000 (19.6429) lr 1.8526e-03 eta 0:00:11
epoch [37/200] batch [40/61] time 0.463 (0.469) data 0.330 (0.338) loss_u loss_u 0.9189 (0.8510) acc_u 9.3750 (19.6875) lr 1.8526e-03 eta 0:00:09
epoch [37/200] batch [45/61] time 0.378 (0.466) data 0.245 (0.334) loss_u loss_u 0.8501 (0.8517) acc_u 18.7500 (19.5139) lr 1.8526e-03 eta 0:00:07
epoch [37/200] batch [50/61] time 0.383 (0.465) data 0.250 (0.334) loss_u loss_u 0.8281 (0.8524) acc_u 18.7500 (19.4375) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [55/61] time 0.476 (0.465) data 0.346 (0.334) loss_u loss_u 0.8125 (0.8484) acc_u 28.1250 (20.0000) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [60/61] time 0.405 (0.464) data 0.273 (0.332) loss_u loss_u 0.8540 (0.8483) acc_u 21.8750 (19.8958) lr 1.8526e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1633
confident_label rate tensor(0.3667, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1150
clean true:1149
clean false:1
clean_rate:0.9991304347826087
noisy true:354
noisy false:1632
after delete: len(clean_dataset) 1150
after delete: len(noisy_dataset) 1986
epoch [38/200] batch [5/35] time 0.464 (0.512) data 0.333 (0.381) loss_x loss_x 1.0088 (1.0426) acc_x 62.5000 (73.7500) lr 1.8443e-03 eta 0:00:15
epoch [38/200] batch [10/35] time 0.532 (0.520) data 0.401 (0.388) loss_x loss_x 1.7744 (1.1544) acc_x 53.1250 (69.6875) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [15/35] time 0.530 (0.508) data 0.399 (0.377) loss_x loss_x 0.7627 (1.1508) acc_x 81.2500 (70.8333) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [20/35] time 0.376 (0.497) data 0.244 (0.365) loss_x loss_x 0.9395 (1.2074) acc_x 78.1250 (69.6875) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [25/35] time 0.433 (0.489) data 0.302 (0.358) loss_x loss_x 1.0781 (1.2465) acc_x 65.6250 (67.3750) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [30/35] time 0.490 (0.489) data 0.358 (0.357) loss_x loss_x 1.6250 (1.2482) acc_x 59.3750 (68.1250) lr 1.8443e-03 eta 0:00:02
epoch [38/200] batch [35/35] time 0.459 (0.484) data 0.327 (0.352) loss_x loss_x 1.5381 (1.2521) acc_x 50.0000 (67.9464) lr 1.8443e-03 eta 0:00:00
epoch [38/200] batch [5/62] time 0.349 (0.478) data 0.218 (0.346) loss_u loss_u 0.8726 (0.8208) acc_u 21.8750 (23.1250) lr 1.8443e-03 eta 0:00:27
epoch [38/200] batch [10/62] time 0.373 (0.474) data 0.242 (0.342) loss_u loss_u 0.7896 (0.8242) acc_u 34.3750 (24.3750) lr 1.8443e-03 eta 0:00:24
epoch [38/200] batch [15/62] time 0.424 (0.466) data 0.293 (0.334) loss_u loss_u 0.9033 (0.8356) acc_u 15.6250 (22.9167) lr 1.8443e-03 eta 0:00:21
epoch [38/200] batch [20/62] time 0.486 (0.462) data 0.354 (0.331) loss_u loss_u 0.8550 (0.8344) acc_u 21.8750 (23.5938) lr 1.8443e-03 eta 0:00:19
epoch [38/200] batch [25/62] time 0.387 (0.456) data 0.256 (0.324) loss_u loss_u 0.8164 (0.8357) acc_u 21.8750 (23.0000) lr 1.8443e-03 eta 0:00:16
epoch [38/200] batch [30/62] time 0.404 (0.454) data 0.273 (0.323) loss_u loss_u 0.9028 (0.8417) acc_u 9.3750 (21.7708) lr 1.8443e-03 eta 0:00:14
epoch [38/200] batch [35/62] time 0.521 (0.458) data 0.390 (0.327) loss_u loss_u 0.8857 (0.8432) acc_u 15.6250 (21.6071) lr 1.8443e-03 eta 0:00:12
epoch [38/200] batch [40/62] time 0.538 (0.456) data 0.406 (0.325) loss_u loss_u 0.7715 (0.8385) acc_u 28.1250 (21.9531) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [45/62] time 0.432 (0.451) data 0.301 (0.319) loss_u loss_u 0.8867 (0.8395) acc_u 15.6250 (21.7361) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [50/62] time 0.489 (0.454) data 0.354 (0.323) loss_u loss_u 0.7759 (0.8399) acc_u 28.1250 (21.8125) lr 1.8443e-03 eta 0:00:05
epoch [38/200] batch [55/62] time 0.461 (0.459) data 0.331 (0.327) loss_u loss_u 0.8242 (0.8413) acc_u 25.0000 (21.5341) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [60/62] time 0.352 (0.455) data 0.221 (0.324) loss_u loss_u 0.8530 (0.8434) acc_u 21.8750 (21.0417) lr 1.8443e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1628
confident_label rate tensor(0.3689, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1157
clean true:1156
clean false:1
clean_rate:0.9991356957649092
noisy true:352
noisy false:1627
after delete: len(clean_dataset) 1157
after delete: len(noisy_dataset) 1979
epoch [39/200] batch [5/36] time 0.557 (0.464) data 0.427 (0.333) loss_x loss_x 1.3594 (1.3897) acc_x 75.0000 (63.7500) lr 1.8358e-03 eta 0:00:14
epoch [39/200] batch [10/36] time 0.431 (0.466) data 0.302 (0.335) loss_x loss_x 1.1553 (1.3019) acc_x 68.7500 (65.3125) lr 1.8358e-03 eta 0:00:12
epoch [39/200] batch [15/36] time 0.468 (0.466) data 0.338 (0.336) loss_x loss_x 0.9829 (1.2552) acc_x 75.0000 (67.2917) lr 1.8358e-03 eta 0:00:09
epoch [39/200] batch [20/36] time 0.394 (0.468) data 0.264 (0.338) loss_x loss_x 1.8398 (1.3043) acc_x 59.3750 (67.3438) lr 1.8358e-03 eta 0:00:07
epoch [39/200] batch [25/36] time 0.532 (0.466) data 0.402 (0.336) loss_x loss_x 1.2852 (1.3482) acc_x 71.8750 (67.5000) lr 1.8358e-03 eta 0:00:05
epoch [39/200] batch [30/36] time 0.371 (0.457) data 0.241 (0.327) loss_x loss_x 1.8086 (1.3563) acc_x 53.1250 (67.5000) lr 1.8358e-03 eta 0:00:02
epoch [39/200] batch [35/36] time 0.363 (0.452) data 0.233 (0.322) loss_x loss_x 2.2773 (1.4059) acc_x 53.1250 (67.0536) lr 1.8358e-03 eta 0:00:00
epoch [39/200] batch [5/61] time 0.399 (0.454) data 0.268 (0.324) loss_u loss_u 0.8301 (0.8490) acc_u 25.0000 (20.6250) lr 1.8358e-03 eta 0:00:25
epoch [39/200] batch [10/61] time 0.495 (0.458) data 0.364 (0.328) loss_u loss_u 0.8755 (0.8405) acc_u 15.6250 (20.6250) lr 1.8358e-03 eta 0:00:23
epoch [39/200] batch [15/61] time 0.334 (0.454) data 0.204 (0.323) loss_u loss_u 0.8696 (0.8425) acc_u 12.5000 (20.2083) lr 1.8358e-03 eta 0:00:20
epoch [39/200] batch [20/61] time 0.407 (0.457) data 0.277 (0.326) loss_u loss_u 0.9175 (0.8413) acc_u 15.6250 (20.7812) lr 1.8358e-03 eta 0:00:18
epoch [39/200] batch [25/61] time 0.366 (0.451) data 0.235 (0.321) loss_u loss_u 0.8564 (0.8464) acc_u 18.7500 (20.7500) lr 1.8358e-03 eta 0:00:16
epoch [39/200] batch [30/61] time 0.365 (0.452) data 0.235 (0.321) loss_u loss_u 0.8594 (0.8451) acc_u 18.7500 (20.5208) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [35/61] time 0.409 (0.448) data 0.278 (0.317) loss_u loss_u 0.8320 (0.8402) acc_u 28.1250 (21.5179) lr 1.8358e-03 eta 0:00:11
epoch [39/200] batch [40/61] time 0.335 (0.445) data 0.205 (0.315) loss_u loss_u 0.8394 (0.8366) acc_u 18.7500 (22.1875) lr 1.8358e-03 eta 0:00:09
epoch [39/200] batch [45/61] time 0.671 (0.449) data 0.542 (0.318) loss_u loss_u 0.8979 (0.8398) acc_u 12.5000 (21.8056) lr 1.8358e-03 eta 0:00:07
epoch [39/200] batch [50/61] time 0.359 (0.445) data 0.229 (0.314) loss_u loss_u 0.7866 (0.8394) acc_u 25.0000 (21.6250) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [55/61] time 0.398 (0.443) data 0.267 (0.313) loss_u loss_u 0.8364 (0.8409) acc_u 18.7500 (21.2500) lr 1.8358e-03 eta 0:00:02
epoch [39/200] batch [60/61] time 0.449 (0.444) data 0.318 (0.314) loss_u loss_u 0.8179 (0.8411) acc_u 31.2500 (21.4062) lr 1.8358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1611
confident_label rate tensor(0.3728, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1169
clean true:1166
clean false:3
clean_rate:0.9974337040205303
noisy true:359
noisy false:1608
after delete: len(clean_dataset) 1169
after delete: len(noisy_dataset) 1967
epoch [40/200] batch [5/36] time 0.468 (0.502) data 0.338 (0.372) loss_x loss_x 1.6719 (1.5427) acc_x 65.6250 (61.8750) lr 1.8271e-03 eta 0:00:15
epoch [40/200] batch [10/36] time 0.430 (0.487) data 0.299 (0.357) loss_x loss_x 1.1465 (1.3695) acc_x 68.7500 (65.3125) lr 1.8271e-03 eta 0:00:12
epoch [40/200] batch [15/36] time 0.422 (0.468) data 0.291 (0.338) loss_x loss_x 0.7656 (1.3019) acc_x 84.3750 (67.5000) lr 1.8271e-03 eta 0:00:09
epoch [40/200] batch [20/36] time 0.485 (0.465) data 0.355 (0.335) loss_x loss_x 1.3955 (1.3081) acc_x 56.2500 (66.0938) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [25/36] time 0.398 (0.457) data 0.268 (0.326) loss_x loss_x 1.1602 (1.3513) acc_x 62.5000 (65.5000) lr 1.8271e-03 eta 0:00:05
epoch [40/200] batch [30/36] time 0.343 (0.445) data 0.213 (0.315) loss_x loss_x 2.0762 (1.3471) acc_x 53.1250 (66.3542) lr 1.8271e-03 eta 0:00:02
epoch [40/200] batch [35/36] time 0.393 (0.442) data 0.262 (0.312) loss_x loss_x 1.8027 (1.3600) acc_x 62.5000 (66.0714) lr 1.8271e-03 eta 0:00:00
epoch [40/200] batch [5/61] time 0.382 (0.445) data 0.251 (0.314) loss_u loss_u 0.7925 (0.8579) acc_u 28.1250 (18.7500) lr 1.8271e-03 eta 0:00:24
epoch [40/200] batch [10/61] time 0.521 (0.451) data 0.389 (0.320) loss_u loss_u 0.6689 (0.8432) acc_u 40.6250 (20.0000) lr 1.8271e-03 eta 0:00:22
epoch [40/200] batch [15/61] time 0.472 (0.445) data 0.341 (0.315) loss_u loss_u 0.7822 (0.8435) acc_u 28.1250 (20.2083) lr 1.8271e-03 eta 0:00:20
epoch [40/200] batch [20/61] time 0.483 (0.446) data 0.352 (0.315) loss_u loss_u 0.8389 (0.8479) acc_u 28.1250 (20.1562) lr 1.8271e-03 eta 0:00:18
epoch [40/200] batch [25/61] time 0.453 (0.448) data 0.322 (0.318) loss_u loss_u 0.8076 (0.8494) acc_u 28.1250 (20.2500) lr 1.8271e-03 eta 0:00:16
epoch [40/200] batch [30/61] time 0.336 (0.445) data 0.205 (0.314) loss_u loss_u 0.8232 (0.8505) acc_u 21.8750 (20.0000) lr 1.8271e-03 eta 0:00:13
epoch [40/200] batch [35/61] time 0.364 (0.446) data 0.233 (0.315) loss_u loss_u 0.8462 (0.8502) acc_u 18.7500 (20.1786) lr 1.8271e-03 eta 0:00:11
epoch [40/200] batch [40/61] time 0.360 (0.444) data 0.229 (0.313) loss_u loss_u 0.8481 (0.8504) acc_u 18.7500 (19.9219) lr 1.8271e-03 eta 0:00:09
epoch [40/200] batch [45/61] time 0.333 (0.443) data 0.203 (0.313) loss_u loss_u 0.8735 (0.8504) acc_u 18.7500 (19.8611) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [50/61] time 0.321 (0.438) data 0.191 (0.308) loss_u loss_u 0.8511 (0.8525) acc_u 21.8750 (19.5000) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [55/61] time 0.443 (0.438) data 0.313 (0.308) loss_u loss_u 0.8154 (0.8504) acc_u 31.2500 (19.8864) lr 1.8271e-03 eta 0:00:02
epoch [40/200] batch [60/61] time 0.395 (0.438) data 0.266 (0.307) loss_u loss_u 0.8364 (0.8509) acc_u 28.1250 (20.0521) lr 1.8271e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1625
confident_label rate tensor(0.3737, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1172
clean true:1171
clean false:1
clean_rate:0.9991467576791809
noisy true:340
noisy false:1624
after delete: len(clean_dataset) 1172
after delete: len(noisy_dataset) 1964
epoch [41/200] batch [5/36] time 0.666 (0.498) data 0.536 (0.367) loss_x loss_x 1.2324 (1.1726) acc_x 71.8750 (66.8750) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [10/36] time 0.412 (0.476) data 0.282 (0.346) loss_x loss_x 1.2891 (1.2077) acc_x 75.0000 (69.3750) lr 1.8181e-03 eta 0:00:12
epoch [41/200] batch [15/36] time 0.554 (0.467) data 0.424 (0.337) loss_x loss_x 1.4639 (1.2405) acc_x 59.3750 (69.3750) lr 1.8181e-03 eta 0:00:09
epoch [41/200] batch [20/36] time 0.471 (0.478) data 0.342 (0.348) loss_x loss_x 1.1562 (1.2478) acc_x 68.7500 (69.8438) lr 1.8181e-03 eta 0:00:07
epoch [41/200] batch [25/36] time 0.441 (0.464) data 0.310 (0.334) loss_x loss_x 1.3623 (1.2494) acc_x 65.6250 (69.8750) lr 1.8181e-03 eta 0:00:05
epoch [41/200] batch [30/36] time 0.465 (0.457) data 0.334 (0.327) loss_x loss_x 1.5342 (1.2810) acc_x 53.1250 (68.8542) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [35/36] time 0.532 (0.461) data 0.402 (0.330) loss_x loss_x 1.2988 (1.2719) acc_x 59.3750 (69.0179) lr 1.8181e-03 eta 0:00:00
epoch [41/200] batch [5/61] time 0.375 (0.455) data 0.245 (0.325) loss_u loss_u 0.8423 (0.8693) acc_u 18.7500 (15.6250) lr 1.8181e-03 eta 0:00:25
epoch [41/200] batch [10/61] time 0.390 (0.453) data 0.260 (0.323) loss_u loss_u 0.8271 (0.8519) acc_u 25.0000 (19.3750) lr 1.8181e-03 eta 0:00:23
epoch [41/200] batch [15/61] time 0.816 (0.460) data 0.685 (0.330) loss_u loss_u 0.8589 (0.8531) acc_u 18.7500 (19.1667) lr 1.8181e-03 eta 0:00:21
epoch [41/200] batch [20/61] time 0.495 (0.453) data 0.364 (0.323) loss_u loss_u 0.8027 (0.8525) acc_u 28.1250 (19.0625) lr 1.8181e-03 eta 0:00:18
epoch [41/200] batch [25/61] time 0.361 (0.455) data 0.230 (0.324) loss_u loss_u 0.7969 (0.8542) acc_u 21.8750 (18.8750) lr 1.8181e-03 eta 0:00:16
epoch [41/200] batch [30/61] time 0.411 (0.450) data 0.280 (0.320) loss_u loss_u 0.8584 (0.8509) acc_u 21.8750 (19.0625) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [35/61] time 0.563 (0.450) data 0.433 (0.320) loss_u loss_u 0.9189 (0.8537) acc_u 6.2500 (18.6607) lr 1.8181e-03 eta 0:00:11
epoch [41/200] batch [40/61] time 0.446 (0.449) data 0.317 (0.318) loss_u loss_u 0.9019 (0.8577) acc_u 15.6250 (18.7500) lr 1.8181e-03 eta 0:00:09
epoch [41/200] batch [45/61] time 0.462 (0.452) data 0.331 (0.322) loss_u loss_u 0.8579 (0.8571) acc_u 18.7500 (18.8194) lr 1.8181e-03 eta 0:00:07
epoch [41/200] batch [50/61] time 0.375 (0.453) data 0.244 (0.323) loss_u loss_u 0.7847 (0.8543) acc_u 28.1250 (19.2500) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [55/61] time 0.430 (0.451) data 0.298 (0.321) loss_u loss_u 0.8467 (0.8545) acc_u 18.7500 (19.3182) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [60/61] time 0.468 (0.450) data 0.337 (0.319) loss_u loss_u 0.8750 (0.8544) acc_u 15.6250 (19.2708) lr 1.8181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1575
confident_label rate tensor(0.3897, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1222
clean true:1220
clean false:2
clean_rate:0.9983633387888707
noisy true:341
noisy false:1573
after delete: len(clean_dataset) 1222
after delete: len(noisy_dataset) 1914
epoch [42/200] batch [5/38] time 0.391 (0.438) data 0.261 (0.308) loss_x loss_x 1.8311 (1.4945) acc_x 50.0000 (64.3750) lr 1.8090e-03 eta 0:00:14
epoch [42/200] batch [10/38] time 0.403 (0.432) data 0.272 (0.302) loss_x loss_x 1.1787 (1.4254) acc_x 65.6250 (63.4375) lr 1.8090e-03 eta 0:00:12
epoch [42/200] batch [15/38] time 0.390 (0.413) data 0.260 (0.283) loss_x loss_x 1.7861 (1.4354) acc_x 62.5000 (62.2917) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [20/38] time 0.508 (0.423) data 0.379 (0.293) loss_x loss_x 1.4170 (1.4722) acc_x 50.0000 (60.4688) lr 1.8090e-03 eta 0:00:07
epoch [42/200] batch [25/38] time 0.444 (0.449) data 0.314 (0.319) loss_x loss_x 1.1992 (1.4230) acc_x 53.1250 (61.1250) lr 1.8090e-03 eta 0:00:05
epoch [42/200] batch [30/38] time 0.512 (0.454) data 0.382 (0.324) loss_x loss_x 1.1992 (1.3952) acc_x 68.7500 (61.7708) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [35/38] time 0.396 (0.456) data 0.265 (0.326) loss_x loss_x 1.5732 (1.3872) acc_x 62.5000 (62.2321) lr 1.8090e-03 eta 0:00:01
epoch [42/200] batch [5/59] time 0.450 (0.453) data 0.319 (0.322) loss_u loss_u 0.8701 (0.8944) acc_u 12.5000 (13.1250) lr 1.8090e-03 eta 0:00:24
epoch [42/200] batch [10/59] time 0.499 (0.456) data 0.367 (0.325) loss_u loss_u 0.8774 (0.8831) acc_u 18.7500 (13.7500) lr 1.8090e-03 eta 0:00:22
epoch [42/200] batch [15/59] time 0.413 (0.453) data 0.281 (0.322) loss_u loss_u 0.8447 (0.8783) acc_u 15.6250 (15.4167) lr 1.8090e-03 eta 0:00:19
epoch [42/200] batch [20/59] time 0.421 (0.447) data 0.290 (0.317) loss_u loss_u 0.8091 (0.8622) acc_u 25.0000 (18.5938) lr 1.8090e-03 eta 0:00:17
epoch [42/200] batch [25/59] time 0.379 (0.444) data 0.247 (0.313) loss_u loss_u 0.9062 (0.8640) acc_u 12.5000 (18.3750) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [30/59] time 0.553 (0.449) data 0.420 (0.319) loss_u loss_u 0.9038 (0.8593) acc_u 18.7500 (19.1667) lr 1.8090e-03 eta 0:00:13
epoch [42/200] batch [35/59] time 0.590 (0.452) data 0.459 (0.321) loss_u loss_u 0.8184 (0.8620) acc_u 28.1250 (18.7500) lr 1.8090e-03 eta 0:00:10
epoch [42/200] batch [40/59] time 0.488 (0.453) data 0.358 (0.322) loss_u loss_u 0.8364 (0.8550) acc_u 25.0000 (19.6875) lr 1.8090e-03 eta 0:00:08
epoch [42/200] batch [45/59] time 0.557 (0.458) data 0.426 (0.327) loss_u loss_u 0.7671 (0.8535) acc_u 34.3750 (19.9306) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [50/59] time 0.437 (0.461) data 0.307 (0.330) loss_u loss_u 0.8579 (0.8543) acc_u 18.7500 (19.6875) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [55/59] time 0.758 (0.464) data 0.627 (0.333) loss_u loss_u 0.8862 (0.8521) acc_u 15.6250 (19.7159) lr 1.8090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1581
confident_label rate tensor(0.3858, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1210
clean true:1209
clean false:1
clean_rate:0.9991735537190083
noisy true:346
noisy false:1580
after delete: len(clean_dataset) 1210
after delete: len(noisy_dataset) 1926
epoch [43/200] batch [5/37] time 0.549 (0.499) data 0.418 (0.368) loss_x loss_x 1.3408 (1.2966) acc_x 62.5000 (68.7500) lr 1.7997e-03 eta 0:00:15
epoch [43/200] batch [10/37] time 0.416 (0.466) data 0.285 (0.335) loss_x loss_x 1.3672 (1.3482) acc_x 68.7500 (67.1875) lr 1.7997e-03 eta 0:00:12
epoch [43/200] batch [15/37] time 0.410 (0.455) data 0.280 (0.324) loss_x loss_x 1.3945 (1.4270) acc_x 68.7500 (65.4167) lr 1.7997e-03 eta 0:00:10
epoch [43/200] batch [20/37] time 0.423 (0.451) data 0.288 (0.320) loss_x loss_x 1.6562 (1.4453) acc_x 56.2500 (63.7500) lr 1.7997e-03 eta 0:00:07
epoch [43/200] batch [25/37] time 0.556 (0.463) data 0.423 (0.332) loss_x loss_x 1.3799 (1.4650) acc_x 71.8750 (62.8750) lr 1.7997e-03 eta 0:00:05
epoch [43/200] batch [30/37] time 0.567 (0.485) data 0.435 (0.354) loss_x loss_x 1.6514 (1.4423) acc_x 59.3750 (63.3333) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [35/37] time 0.458 (0.481) data 0.325 (0.349) loss_x loss_x 1.4326 (1.4072) acc_x 62.5000 (64.3750) lr 1.7997e-03 eta 0:00:00
epoch [43/200] batch [5/60] time 0.589 (0.481) data 0.456 (0.350) loss_u loss_u 0.7949 (0.8272) acc_u 25.0000 (23.7500) lr 1.7997e-03 eta 0:00:26
epoch [43/200] batch [10/60] time 0.451 (0.478) data 0.320 (0.346) loss_u loss_u 0.8926 (0.8361) acc_u 15.6250 (22.1875) lr 1.7997e-03 eta 0:00:23
epoch [43/200] batch [15/60] time 0.426 (0.471) data 0.294 (0.340) loss_u loss_u 0.7886 (0.8389) acc_u 28.1250 (21.6667) lr 1.7997e-03 eta 0:00:21
epoch [43/200] batch [20/60] time 0.563 (0.467) data 0.431 (0.335) loss_u loss_u 0.7969 (0.8361) acc_u 28.1250 (21.7188) lr 1.7997e-03 eta 0:00:18
epoch [43/200] batch [25/60] time 0.540 (0.467) data 0.409 (0.335) loss_u loss_u 0.8750 (0.8434) acc_u 12.5000 (20.7500) lr 1.7997e-03 eta 0:00:16
epoch [43/200] batch [30/60] time 0.381 (0.467) data 0.250 (0.336) loss_u loss_u 0.8408 (0.8440) acc_u 25.0000 (20.8333) lr 1.7997e-03 eta 0:00:14
epoch [43/200] batch [35/60] time 0.393 (0.464) data 0.262 (0.333) loss_u loss_u 0.7812 (0.8447) acc_u 31.2500 (20.7143) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [40/60] time 0.393 (0.464) data 0.262 (0.333) loss_u loss_u 0.8999 (0.8461) acc_u 9.3750 (20.5469) lr 1.7997e-03 eta 0:00:09
epoch [43/200] batch [45/60] time 0.411 (0.463) data 0.281 (0.331) loss_u loss_u 0.8696 (0.8472) acc_u 25.0000 (20.3472) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [50/60] time 0.454 (0.460) data 0.324 (0.329) loss_u loss_u 0.9009 (0.8493) acc_u 12.5000 (20.0625) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [55/60] time 0.474 (0.461) data 0.344 (0.329) loss_u loss_u 0.8335 (0.8493) acc_u 25.0000 (20.2273) lr 1.7997e-03 eta 0:00:02
epoch [43/200] batch [60/60] time 0.395 (0.460) data 0.264 (0.329) loss_u loss_u 0.8047 (0.8485) acc_u 18.7500 (20.0521) lr 1.7997e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1636
confident_label rate tensor(0.3721, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1167
clean true:1163
clean false:4
clean_rate:0.9965724078834619
noisy true:337
noisy false:1632
after delete: len(clean_dataset) 1167
after delete: len(noisy_dataset) 1969
epoch [44/200] batch [5/36] time 0.528 (0.498) data 0.398 (0.366) loss_x loss_x 1.5000 (1.4162) acc_x 68.7500 (65.6250) lr 1.7902e-03 eta 0:00:15
epoch [44/200] batch [10/36] time 0.420 (0.498) data 0.290 (0.366) loss_x loss_x 1.7988 (1.4404) acc_x 59.3750 (64.6875) lr 1.7902e-03 eta 0:00:12
epoch [44/200] batch [15/36] time 0.460 (0.465) data 0.329 (0.334) loss_x loss_x 1.3242 (1.4730) acc_x 62.5000 (62.5000) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [20/36] time 0.511 (0.466) data 0.381 (0.335) loss_x loss_x 1.0332 (1.3941) acc_x 71.8750 (65.1562) lr 1.7902e-03 eta 0:00:07
epoch [44/200] batch [25/36] time 0.398 (0.453) data 0.266 (0.322) loss_x loss_x 1.1377 (1.3614) acc_x 71.8750 (66.3750) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [30/36] time 0.638 (0.459) data 0.507 (0.328) loss_x loss_x 1.4404 (1.3932) acc_x 68.7500 (66.1458) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [35/36] time 0.447 (0.457) data 0.317 (0.326) loss_x loss_x 1.0566 (1.3605) acc_x 75.0000 (66.6964) lr 1.7902e-03 eta 0:00:00
epoch [44/200] batch [5/61] time 0.499 (0.452) data 0.369 (0.321) loss_u loss_u 0.8530 (0.8372) acc_u 21.8750 (26.2500) lr 1.7902e-03 eta 0:00:25
epoch [44/200] batch [10/61] time 0.494 (0.452) data 0.363 (0.321) loss_u loss_u 0.8379 (0.8513) acc_u 21.8750 (21.8750) lr 1.7902e-03 eta 0:00:23
epoch [44/200] batch [15/61] time 0.483 (0.450) data 0.353 (0.319) loss_u loss_u 0.7847 (0.8433) acc_u 25.0000 (21.8750) lr 1.7902e-03 eta 0:00:20
epoch [44/200] batch [20/61] time 0.470 (0.455) data 0.339 (0.324) loss_u loss_u 0.7769 (0.8315) acc_u 31.2500 (22.8125) lr 1.7902e-03 eta 0:00:18
epoch [44/200] batch [25/61] time 0.559 (0.459) data 0.428 (0.328) loss_u loss_u 0.8550 (0.8365) acc_u 18.7500 (22.2500) lr 1.7902e-03 eta 0:00:16
epoch [44/200] batch [30/61] time 0.494 (0.455) data 0.363 (0.324) loss_u loss_u 0.7764 (0.8300) acc_u 28.1250 (22.9167) lr 1.7902e-03 eta 0:00:14
epoch [44/200] batch [35/61] time 0.404 (0.451) data 0.274 (0.320) loss_u loss_u 0.9038 (0.8331) acc_u 9.3750 (22.4107) lr 1.7902e-03 eta 0:00:11
epoch [44/200] batch [40/61] time 0.369 (0.449) data 0.239 (0.318) loss_u loss_u 0.8970 (0.8355) acc_u 9.3750 (21.7969) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [45/61] time 0.318 (0.446) data 0.188 (0.315) loss_u loss_u 0.8989 (0.8363) acc_u 15.6250 (21.5278) lr 1.7902e-03 eta 0:00:07
epoch [44/200] batch [50/61] time 0.457 (0.447) data 0.327 (0.316) loss_u loss_u 0.7988 (0.8383) acc_u 28.1250 (21.2500) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [55/61] time 0.433 (0.447) data 0.301 (0.316) loss_u loss_u 0.8945 (0.8391) acc_u 15.6250 (21.3068) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [60/61] time 0.737 (0.449) data 0.607 (0.318) loss_u loss_u 0.8013 (0.8414) acc_u 21.8750 (20.9896) lr 1.7902e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1583
confident_label rate tensor(0.3846, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1206
clean true:1203
clean false:3
clean_rate:0.9975124378109452
noisy true:350
noisy false:1580
after delete: len(clean_dataset) 1206
after delete: len(noisy_dataset) 1930
epoch [45/200] batch [5/37] time 0.475 (0.497) data 0.345 (0.367) loss_x loss_x 1.1182 (1.3775) acc_x 78.1250 (69.3750) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [10/37] time 0.352 (0.475) data 0.222 (0.345) loss_x loss_x 1.3945 (1.3407) acc_x 62.5000 (68.1250) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [15/37] time 0.477 (0.469) data 0.347 (0.339) loss_x loss_x 1.4121 (1.3302) acc_x 62.5000 (68.1250) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [20/37] time 0.523 (0.464) data 0.393 (0.334) loss_x loss_x 1.3994 (1.3231) acc_x 53.1250 (66.4062) lr 1.7804e-03 eta 0:00:07
epoch [45/200] batch [25/37] time 0.351 (0.454) data 0.220 (0.324) loss_x loss_x 1.1162 (1.3153) acc_x 65.6250 (66.1250) lr 1.7804e-03 eta 0:00:05
epoch [45/200] batch [30/37] time 0.418 (0.456) data 0.288 (0.326) loss_x loss_x 1.3291 (1.3284) acc_x 68.7500 (65.8333) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [35/37] time 0.379 (0.453) data 0.248 (0.322) loss_x loss_x 1.5713 (1.3322) acc_x 56.2500 (66.6964) lr 1.7804e-03 eta 0:00:00
epoch [45/200] batch [5/60] time 0.394 (0.458) data 0.263 (0.328) loss_u loss_u 0.8770 (0.8470) acc_u 12.5000 (19.3750) lr 1.7804e-03 eta 0:00:25
epoch [45/200] batch [10/60] time 0.408 (0.454) data 0.278 (0.323) loss_u loss_u 0.8804 (0.8508) acc_u 18.7500 (19.3750) lr 1.7804e-03 eta 0:00:22
epoch [45/200] batch [15/60] time 0.454 (0.454) data 0.322 (0.324) loss_u loss_u 0.7466 (0.8381) acc_u 31.2500 (20.0000) lr 1.7804e-03 eta 0:00:20
epoch [45/200] batch [20/60] time 0.379 (0.453) data 0.249 (0.323) loss_u loss_u 0.8296 (0.8463) acc_u 25.0000 (18.9062) lr 1.7804e-03 eta 0:00:18
epoch [45/200] batch [25/60] time 0.420 (0.449) data 0.289 (0.318) loss_u loss_u 0.8447 (0.8452) acc_u 25.0000 (19.3750) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [30/60] time 0.418 (0.450) data 0.287 (0.320) loss_u loss_u 0.8120 (0.8435) acc_u 25.0000 (19.5833) lr 1.7804e-03 eta 0:00:13
epoch [45/200] batch [35/60] time 0.378 (0.446) data 0.248 (0.316) loss_u loss_u 0.8936 (0.8507) acc_u 9.3750 (18.4821) lr 1.7804e-03 eta 0:00:11
epoch [45/200] batch [40/60] time 0.518 (0.446) data 0.387 (0.315) loss_u loss_u 0.8955 (0.8489) acc_u 12.5000 (18.8281) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [45/60] time 0.348 (0.446) data 0.216 (0.316) loss_u loss_u 0.7495 (0.8473) acc_u 31.2500 (19.1667) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [50/60] time 0.374 (0.446) data 0.243 (0.315) loss_u loss_u 0.8970 (0.8479) acc_u 12.5000 (19.1875) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [55/60] time 0.410 (0.443) data 0.279 (0.313) loss_u loss_u 0.8560 (0.8478) acc_u 18.7500 (19.0909) lr 1.7804e-03 eta 0:00:02
epoch [45/200] batch [60/60] time 0.439 (0.442) data 0.308 (0.311) loss_u loss_u 0.8008 (0.8482) acc_u 28.1250 (19.3229) lr 1.7804e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1620
confident_label rate tensor(0.3817, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1197
clean true:1194
clean false:3
clean_rate:0.9974937343358395
noisy true:322
noisy false:1617
after delete: len(clean_dataset) 1197
after delete: len(noisy_dataset) 1939
epoch [46/200] batch [5/37] time 0.349 (0.428) data 0.218 (0.298) loss_x loss_x 1.8125 (1.4061) acc_x 53.1250 (60.6250) lr 1.7705e-03 eta 0:00:13
epoch [46/200] batch [10/37] time 0.397 (0.449) data 0.267 (0.319) loss_x loss_x 1.5068 (1.3417) acc_x 71.8750 (63.7500) lr 1.7705e-03 eta 0:00:12
epoch [46/200] batch [15/37] time 0.397 (0.435) data 0.267 (0.304) loss_x loss_x 1.6240 (1.3570) acc_x 56.2500 (63.9583) lr 1.7705e-03 eta 0:00:09
epoch [46/200] batch [20/37] time 0.399 (0.428) data 0.268 (0.297) loss_x loss_x 1.7949 (1.3759) acc_x 43.7500 (62.6562) lr 1.7705e-03 eta 0:00:07
epoch [46/200] batch [25/37] time 0.565 (0.439) data 0.435 (0.309) loss_x loss_x 1.5752 (1.3977) acc_x 59.3750 (62.7500) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [30/37] time 0.416 (0.436) data 0.285 (0.306) loss_x loss_x 1.5439 (1.3859) acc_x 68.7500 (63.6458) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [35/37] time 0.552 (0.437) data 0.422 (0.306) loss_x loss_x 1.8086 (1.3833) acc_x 56.2500 (63.9286) lr 1.7705e-03 eta 0:00:00
epoch [46/200] batch [5/60] time 0.405 (0.432) data 0.273 (0.302) loss_u loss_u 0.8794 (0.8791) acc_u 21.8750 (16.2500) lr 1.7705e-03 eta 0:00:23
epoch [46/200] batch [10/60] time 0.576 (0.433) data 0.445 (0.303) loss_u loss_u 0.8936 (0.8833) acc_u 15.6250 (14.6875) lr 1.7705e-03 eta 0:00:21
epoch [46/200] batch [15/60] time 0.461 (0.433) data 0.330 (0.303) loss_u loss_u 0.8965 (0.8812) acc_u 18.7500 (16.2500) lr 1.7705e-03 eta 0:00:19
epoch [46/200] batch [20/60] time 0.485 (0.438) data 0.354 (0.308) loss_u loss_u 0.8882 (0.8759) acc_u 9.3750 (16.0938) lr 1.7705e-03 eta 0:00:17
epoch [46/200] batch [25/60] time 0.335 (0.436) data 0.203 (0.305) loss_u loss_u 0.9028 (0.8768) acc_u 12.5000 (16.3750) lr 1.7705e-03 eta 0:00:15
epoch [46/200] batch [30/60] time 0.420 (0.439) data 0.289 (0.308) loss_u loss_u 0.8662 (0.8717) acc_u 18.7500 (17.1875) lr 1.7705e-03 eta 0:00:13
epoch [46/200] batch [35/60] time 0.638 (0.441) data 0.506 (0.310) loss_u loss_u 0.8813 (0.8692) acc_u 18.7500 (17.5000) lr 1.7705e-03 eta 0:00:11
epoch [46/200] batch [40/60] time 0.746 (0.445) data 0.614 (0.314) loss_u loss_u 0.7715 (0.8654) acc_u 28.1250 (17.5781) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [45/60] time 0.407 (0.442) data 0.276 (0.311) loss_u loss_u 0.9019 (0.8609) acc_u 12.5000 (18.2639) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [50/60] time 0.429 (0.442) data 0.298 (0.312) loss_u loss_u 0.8574 (0.8595) acc_u 18.7500 (18.4375) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [55/60] time 0.571 (0.442) data 0.440 (0.311) loss_u loss_u 0.8340 (0.8593) acc_u 18.7500 (18.5227) lr 1.7705e-03 eta 0:00:02
epoch [46/200] batch [60/60] time 0.363 (0.440) data 0.232 (0.309) loss_u loss_u 0.8281 (0.8596) acc_u 12.5000 (18.4375) lr 1.7705e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1583
confident_label rate tensor(0.3842, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1205
clean true:1201
clean false:4
clean_rate:0.9966804979253112
noisy true:352
noisy false:1579
after delete: len(clean_dataset) 1205
after delete: len(noisy_dataset) 1931
epoch [47/200] batch [5/37] time 0.542 (0.471) data 0.411 (0.339) loss_x loss_x 0.9351 (1.4603) acc_x 68.7500 (65.0000) lr 1.7604e-03 eta 0:00:15
epoch [47/200] batch [10/37] time 0.452 (0.463) data 0.320 (0.332) loss_x loss_x 1.3730 (1.3188) acc_x 62.5000 (64.6875) lr 1.7604e-03 eta 0:00:12
epoch [47/200] batch [15/37] time 0.434 (0.463) data 0.303 (0.332) loss_x loss_x 1.1826 (1.3157) acc_x 68.7500 (65.4167) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [20/37] time 0.369 (0.455) data 0.238 (0.324) loss_x loss_x 1.2139 (1.3212) acc_x 68.7500 (66.0938) lr 1.7604e-03 eta 0:00:07
epoch [47/200] batch [25/37] time 0.531 (0.458) data 0.399 (0.327) loss_x loss_x 0.9272 (1.3319) acc_x 81.2500 (65.3750) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [30/37] time 0.502 (0.463) data 0.372 (0.332) loss_x loss_x 1.3252 (1.3299) acc_x 68.7500 (65.8333) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [35/37] time 0.446 (0.453) data 0.315 (0.322) loss_x loss_x 1.0391 (1.3255) acc_x 75.0000 (66.5179) lr 1.7604e-03 eta 0:00:00
epoch [47/200] batch [5/60] time 0.419 (0.455) data 0.289 (0.324) loss_u loss_u 0.8525 (0.8703) acc_u 21.8750 (18.1250) lr 1.7604e-03 eta 0:00:25
epoch [47/200] batch [10/60] time 0.399 (0.458) data 0.269 (0.328) loss_u loss_u 0.8081 (0.8583) acc_u 28.1250 (19.6875) lr 1.7604e-03 eta 0:00:22
epoch [47/200] batch [15/60] time 0.580 (0.462) data 0.448 (0.331) loss_u loss_u 0.8540 (0.8526) acc_u 21.8750 (20.4167) lr 1.7604e-03 eta 0:00:20
epoch [47/200] batch [20/60] time 0.474 (0.457) data 0.343 (0.326) loss_u loss_u 0.8164 (0.8410) acc_u 25.0000 (21.8750) lr 1.7604e-03 eta 0:00:18
epoch [47/200] batch [25/60] time 0.455 (0.454) data 0.323 (0.323) loss_u loss_u 0.8906 (0.8421) acc_u 9.3750 (21.3750) lr 1.7604e-03 eta 0:00:15
epoch [47/200] batch [30/60] time 0.545 (0.456) data 0.413 (0.325) loss_u loss_u 0.9111 (0.8450) acc_u 12.5000 (21.1458) lr 1.7604e-03 eta 0:00:13
epoch [47/200] batch [35/60] time 0.365 (0.455) data 0.234 (0.324) loss_u loss_u 0.8516 (0.8435) acc_u 21.8750 (21.1607) lr 1.7604e-03 eta 0:00:11
epoch [47/200] batch [40/60] time 0.422 (0.457) data 0.291 (0.326) loss_u loss_u 0.8105 (0.8404) acc_u 31.2500 (22.1094) lr 1.7604e-03 eta 0:00:09
epoch [47/200] batch [45/60] time 0.357 (0.452) data 0.226 (0.321) loss_u loss_u 0.8120 (0.8430) acc_u 25.0000 (21.5972) lr 1.7604e-03 eta 0:00:06
epoch [47/200] batch [50/60] time 0.327 (0.449) data 0.197 (0.318) loss_u loss_u 0.9561 (0.8464) acc_u 9.3750 (21.1875) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [55/60] time 0.471 (0.450) data 0.340 (0.319) loss_u loss_u 0.8579 (0.8460) acc_u 15.6250 (21.0795) lr 1.7604e-03 eta 0:00:02
epoch [47/200] batch [60/60] time 0.453 (0.450) data 0.322 (0.319) loss_u loss_u 0.8125 (0.8432) acc_u 18.7500 (21.4062) lr 1.7604e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1587
confident_label rate tensor(0.3858, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1210
clean true:1206
clean false:4
clean_rate:0.996694214876033
noisy true:343
noisy false:1583
after delete: len(clean_dataset) 1210
after delete: len(noisy_dataset) 1926
epoch [48/200] batch [5/37] time 0.453 (0.439) data 0.323 (0.308) loss_x loss_x 0.6626 (1.1263) acc_x 90.6250 (72.5000) lr 1.7501e-03 eta 0:00:14
epoch [48/200] batch [10/37] time 0.442 (0.460) data 0.311 (0.329) loss_x loss_x 1.4521 (1.2121) acc_x 71.8750 (70.9375) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [15/37] time 0.442 (0.439) data 0.311 (0.308) loss_x loss_x 1.4902 (1.2604) acc_x 62.5000 (68.7500) lr 1.7501e-03 eta 0:00:09
epoch [48/200] batch [20/37] time 0.414 (0.443) data 0.283 (0.312) loss_x loss_x 1.0146 (1.2493) acc_x 71.8750 (69.0625) lr 1.7501e-03 eta 0:00:07
epoch [48/200] batch [25/37] time 0.417 (0.440) data 0.286 (0.310) loss_x loss_x 1.4160 (1.2638) acc_x 59.3750 (68.1250) lr 1.7501e-03 eta 0:00:05
epoch [48/200] batch [30/37] time 0.439 (0.439) data 0.309 (0.309) loss_x loss_x 1.3652 (1.3162) acc_x 62.5000 (66.9792) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [35/37] time 0.600 (0.451) data 0.469 (0.320) loss_x loss_x 1.2432 (1.3502) acc_x 62.5000 (65.8036) lr 1.7501e-03 eta 0:00:00
epoch [48/200] batch [5/60] time 0.457 (0.443) data 0.326 (0.313) loss_u loss_u 0.8691 (0.8684) acc_u 18.7500 (16.8750) lr 1.7501e-03 eta 0:00:24
epoch [48/200] batch [10/60] time 0.366 (0.449) data 0.235 (0.319) loss_u loss_u 0.8647 (0.8547) acc_u 18.7500 (19.3750) lr 1.7501e-03 eta 0:00:22
epoch [48/200] batch [15/60] time 0.482 (0.447) data 0.351 (0.317) loss_u loss_u 0.8066 (0.8516) acc_u 25.0000 (19.1667) lr 1.7501e-03 eta 0:00:20
epoch [48/200] batch [20/60] time 0.383 (0.450) data 0.253 (0.320) loss_u loss_u 0.8745 (0.8543) acc_u 12.5000 (18.5938) lr 1.7501e-03 eta 0:00:17
epoch [48/200] batch [25/60] time 0.455 (0.450) data 0.324 (0.319) loss_u loss_u 0.8916 (0.8569) acc_u 6.2500 (18.3750) lr 1.7501e-03 eta 0:00:15
epoch [48/200] batch [30/60] time 0.527 (0.450) data 0.396 (0.319) loss_u loss_u 0.8706 (0.8556) acc_u 12.5000 (18.5417) lr 1.7501e-03 eta 0:00:13
epoch [48/200] batch [35/60] time 0.408 (0.448) data 0.278 (0.317) loss_u loss_u 0.7998 (0.8566) acc_u 25.0000 (18.8393) lr 1.7501e-03 eta 0:00:11
epoch [48/200] batch [40/60] time 0.407 (0.444) data 0.275 (0.313) loss_u loss_u 0.8525 (0.8571) acc_u 15.6250 (18.8281) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [45/60] time 0.502 (0.443) data 0.371 (0.313) loss_u loss_u 0.7598 (0.8524) acc_u 34.3750 (19.5139) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [50/60] time 0.363 (0.441) data 0.231 (0.311) loss_u loss_u 0.9585 (0.8528) acc_u 0.0000 (19.3750) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [55/60] time 0.358 (0.438) data 0.228 (0.308) loss_u loss_u 0.8223 (0.8506) acc_u 21.8750 (19.7159) lr 1.7501e-03 eta 0:00:02
epoch [48/200] batch [60/60] time 0.421 (0.437) data 0.291 (0.307) loss_u loss_u 0.8125 (0.8508) acc_u 25.0000 (19.5312) lr 1.7501e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1601
confident_label rate tensor(0.3874, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1215
clean true:1212
clean false:3
clean_rate:0.9975308641975309
noisy true:323
noisy false:1598
after delete: len(clean_dataset) 1215
after delete: len(noisy_dataset) 1921
epoch [49/200] batch [5/37] time 0.410 (0.406) data 0.280 (0.276) loss_x loss_x 1.0889 (1.3123) acc_x 71.8750 (65.6250) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [10/37] time 0.490 (0.438) data 0.360 (0.307) loss_x loss_x 1.2080 (1.3209) acc_x 68.7500 (65.6250) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [15/37] time 0.491 (0.445) data 0.360 (0.315) loss_x loss_x 1.3301 (1.2683) acc_x 65.6250 (67.7083) lr 1.7396e-03 eta 0:00:09
epoch [49/200] batch [20/37] time 0.390 (0.458) data 0.259 (0.327) loss_x loss_x 1.1904 (1.2446) acc_x 68.7500 (67.9688) lr 1.7396e-03 eta 0:00:07
epoch [49/200] batch [25/37] time 0.498 (0.457) data 0.368 (0.326) loss_x loss_x 1.1328 (1.2170) acc_x 71.8750 (68.2500) lr 1.7396e-03 eta 0:00:05
epoch [49/200] batch [30/37] time 0.429 (0.457) data 0.298 (0.326) loss_x loss_x 1.5996 (1.2305) acc_x 56.2500 (67.7083) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [35/37] time 0.370 (0.451) data 0.239 (0.320) loss_x loss_x 1.4385 (1.2547) acc_x 68.7500 (67.7679) lr 1.7396e-03 eta 0:00:00
epoch [49/200] batch [5/60] time 0.498 (0.451) data 0.367 (0.320) loss_u loss_u 0.7798 (0.8334) acc_u 31.2500 (23.1250) lr 1.7396e-03 eta 0:00:24
epoch [49/200] batch [10/60] time 0.477 (0.452) data 0.346 (0.321) loss_u loss_u 0.8042 (0.8328) acc_u 28.1250 (22.5000) lr 1.7396e-03 eta 0:00:22
epoch [49/200] batch [15/60] time 0.752 (0.458) data 0.621 (0.327) loss_u loss_u 0.8472 (0.8425) acc_u 25.0000 (21.0417) lr 1.7396e-03 eta 0:00:20
epoch [49/200] batch [20/60] time 0.415 (0.455) data 0.284 (0.324) loss_u loss_u 0.8057 (0.8509) acc_u 21.8750 (19.5312) lr 1.7396e-03 eta 0:00:18
epoch [49/200] batch [25/60] time 0.492 (0.458) data 0.359 (0.327) loss_u loss_u 0.8145 (0.8449) acc_u 25.0000 (20.2500) lr 1.7396e-03 eta 0:00:16
epoch [49/200] batch [30/60] time 0.507 (0.462) data 0.376 (0.331) loss_u loss_u 0.8730 (0.8429) acc_u 12.5000 (20.5208) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [35/60] time 0.492 (0.459) data 0.360 (0.327) loss_u loss_u 0.8271 (0.8460) acc_u 15.6250 (19.2857) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [40/60] time 0.393 (0.459) data 0.262 (0.328) loss_u loss_u 0.8350 (0.8489) acc_u 28.1250 (19.0625) lr 1.7396e-03 eta 0:00:09
epoch [49/200] batch [45/60] time 0.445 (0.458) data 0.314 (0.327) loss_u loss_u 0.8599 (0.8486) acc_u 18.7500 (19.4444) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [50/60] time 0.386 (0.456) data 0.254 (0.325) loss_u loss_u 0.7378 (0.8441) acc_u 31.2500 (20.0000) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [55/60] time 0.626 (0.458) data 0.494 (0.327) loss_u loss_u 0.8965 (0.8467) acc_u 12.5000 (19.6591) lr 1.7396e-03 eta 0:00:02
epoch [49/200] batch [60/60] time 0.339 (0.456) data 0.207 (0.325) loss_u loss_u 0.8687 (0.8464) acc_u 18.7500 (19.7917) lr 1.7396e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1619
confident_label rate tensor(0.3772, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1183
clean true:1182
clean false:1
clean_rate:0.9991546914623838
noisy true:335
noisy false:1618
after delete: len(clean_dataset) 1183
after delete: len(noisy_dataset) 1953
epoch [50/200] batch [5/36] time 0.671 (0.553) data 0.540 (0.422) loss_x loss_x 1.4941 (1.6035) acc_x 65.6250 (59.3750) lr 1.7290e-03 eta 0:00:17
epoch [50/200] batch [10/36] time 0.397 (0.486) data 0.266 (0.355) loss_x loss_x 0.6274 (1.2711) acc_x 78.1250 (67.1875) lr 1.7290e-03 eta 0:00:12
epoch [50/200] batch [15/36] time 0.460 (0.470) data 0.329 (0.339) loss_x loss_x 0.9746 (1.2937) acc_x 78.1250 (67.0833) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [20/36] time 0.521 (0.462) data 0.390 (0.331) loss_x loss_x 1.2549 (1.3009) acc_x 71.8750 (68.1250) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [25/36] time 0.483 (0.455) data 0.352 (0.324) loss_x loss_x 1.6709 (1.2980) acc_x 62.5000 (68.3750) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [30/36] time 0.396 (0.450) data 0.265 (0.320) loss_x loss_x 2.3652 (1.3483) acc_x 46.8750 (67.7083) lr 1.7290e-03 eta 0:00:02
epoch [50/200] batch [35/36] time 0.370 (0.446) data 0.239 (0.315) loss_x loss_x 0.9468 (1.3406) acc_x 71.8750 (67.6786) lr 1.7290e-03 eta 0:00:00
epoch [50/200] batch [5/61] time 0.476 (0.447) data 0.344 (0.317) loss_u loss_u 0.8843 (0.8581) acc_u 15.6250 (17.5000) lr 1.7290e-03 eta 0:00:25
epoch [50/200] batch [10/61] time 0.366 (0.442) data 0.235 (0.312) loss_u loss_u 0.8403 (0.8473) acc_u 21.8750 (20.6250) lr 1.7290e-03 eta 0:00:22
epoch [50/200] batch [15/61] time 0.525 (0.443) data 0.394 (0.312) loss_u loss_u 0.8369 (0.8435) acc_u 21.8750 (21.4583) lr 1.7290e-03 eta 0:00:20
epoch [50/200] batch [20/61] time 0.330 (0.440) data 0.199 (0.309) loss_u loss_u 0.8975 (0.8484) acc_u 12.5000 (20.6250) lr 1.7290e-03 eta 0:00:18
epoch [50/200] batch [25/61] time 0.386 (0.442) data 0.255 (0.311) loss_u loss_u 0.8687 (0.8462) acc_u 15.6250 (20.5000) lr 1.7290e-03 eta 0:00:15
epoch [50/200] batch [30/61] time 0.494 (0.441) data 0.363 (0.310) loss_u loss_u 0.8833 (0.8463) acc_u 9.3750 (20.4167) lr 1.7290e-03 eta 0:00:13
epoch [50/200] batch [35/61] time 0.549 (0.443) data 0.417 (0.312) loss_u loss_u 0.8745 (0.8450) acc_u 18.7500 (20.3571) lr 1.7290e-03 eta 0:00:11
epoch [50/200] batch [40/61] time 0.349 (0.444) data 0.218 (0.313) loss_u loss_u 0.8525 (0.8447) acc_u 25.0000 (20.5469) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [45/61] time 0.544 (0.446) data 0.413 (0.315) loss_u loss_u 0.8442 (0.8477) acc_u 18.7500 (20.1389) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [50/61] time 0.437 (0.445) data 0.306 (0.314) loss_u loss_u 0.9482 (0.8495) acc_u 6.2500 (19.8750) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [55/61] time 0.345 (0.444) data 0.213 (0.313) loss_u loss_u 0.8594 (0.8475) acc_u 15.6250 (20.2273) lr 1.7290e-03 eta 0:00:02
epoch [50/200] batch [60/61] time 0.351 (0.443) data 0.220 (0.312) loss_u loss_u 0.8408 (0.8455) acc_u 15.6250 (20.4688) lr 1.7290e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1552
confident_label rate tensor(0.3922, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1230
clean true:1228
clean false:2
clean_rate:0.9983739837398374
noisy true:356
noisy false:1550
after delete: len(clean_dataset) 1230
after delete: len(noisy_dataset) 1906
epoch [51/200] batch [5/38] time 0.613 (0.476) data 0.482 (0.346) loss_x loss_x 1.4561 (1.2359) acc_x 59.3750 (70.0000) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [10/38] time 0.497 (0.477) data 0.367 (0.346) loss_x loss_x 1.2422 (1.2301) acc_x 65.6250 (69.3750) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [15/38] time 0.553 (0.476) data 0.423 (0.345) loss_x loss_x 1.7119 (1.2838) acc_x 59.3750 (68.5417) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [20/38] time 0.559 (0.489) data 0.429 (0.358) loss_x loss_x 1.0352 (1.2581) acc_x 71.8750 (69.0625) lr 1.7181e-03 eta 0:00:08
epoch [51/200] batch [25/38] time 0.454 (0.478) data 0.323 (0.348) loss_x loss_x 1.7812 (1.2472) acc_x 65.6250 (69.7500) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [30/38] time 0.384 (0.471) data 0.254 (0.340) loss_x loss_x 1.5303 (1.2785) acc_x 53.1250 (68.1250) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [35/38] time 0.494 (0.467) data 0.363 (0.336) loss_x loss_x 1.2100 (1.2985) acc_x 71.8750 (67.4107) lr 1.7181e-03 eta 0:00:01
epoch [51/200] batch [5/59] time 0.550 (0.467) data 0.418 (0.336) loss_u loss_u 0.8882 (0.8685) acc_u 18.7500 (16.8750) lr 1.7181e-03 eta 0:00:25
epoch [51/200] batch [10/59] time 0.393 (0.463) data 0.260 (0.332) loss_u loss_u 0.7842 (0.8512) acc_u 28.1250 (19.3750) lr 1.7181e-03 eta 0:00:22
epoch [51/200] batch [15/59] time 0.560 (0.464) data 0.428 (0.333) loss_u loss_u 0.8628 (0.8490) acc_u 15.6250 (20.2083) lr 1.7181e-03 eta 0:00:20
epoch [51/200] batch [20/59] time 0.420 (0.462) data 0.288 (0.331) loss_u loss_u 0.8184 (0.8455) acc_u 21.8750 (20.3125) lr 1.7181e-03 eta 0:00:18
epoch [51/200] batch [25/59] time 0.356 (0.460) data 0.224 (0.328) loss_u loss_u 0.9229 (0.8514) acc_u 9.3750 (19.2500) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [30/59] time 0.475 (0.456) data 0.342 (0.325) loss_u loss_u 0.8286 (0.8499) acc_u 25.0000 (19.6875) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [35/59] time 0.419 (0.454) data 0.286 (0.323) loss_u loss_u 0.8687 (0.8553) acc_u 15.6250 (18.6607) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [40/59] time 0.364 (0.452) data 0.232 (0.321) loss_u loss_u 0.8359 (0.8553) acc_u 18.7500 (18.5156) lr 1.7181e-03 eta 0:00:08
epoch [51/200] batch [45/59] time 0.528 (0.457) data 0.393 (0.325) loss_u loss_u 0.8115 (0.8529) acc_u 21.8750 (18.8889) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [50/59] time 0.690 (0.457) data 0.557 (0.326) loss_u loss_u 0.8486 (0.8491) acc_u 21.8750 (19.1875) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [55/59] time 0.469 (0.458) data 0.337 (0.327) loss_u loss_u 0.9082 (0.8515) acc_u 12.5000 (19.3750) lr 1.7181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1561
confident_label rate tensor(0.3922, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1230
clean true:1229
clean false:1
clean_rate:0.9991869918699187
noisy true:346
noisy false:1560
after delete: len(clean_dataset) 1230
after delete: len(noisy_dataset) 1906
epoch [52/200] batch [5/38] time 0.579 (0.510) data 0.449 (0.379) loss_x loss_x 1.5928 (1.6328) acc_x 71.8750 (66.2500) lr 1.7071e-03 eta 0:00:16
epoch [52/200] batch [10/38] time 0.449 (0.483) data 0.319 (0.352) loss_x loss_x 1.0244 (1.4700) acc_x 65.6250 (65.6250) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [15/38] time 0.356 (0.449) data 0.225 (0.318) loss_x loss_x 1.5381 (1.4267) acc_x 59.3750 (65.0000) lr 1.7071e-03 eta 0:00:10
epoch [52/200] batch [20/38] time 0.332 (0.452) data 0.203 (0.321) loss_x loss_x 0.9087 (1.3710) acc_x 75.0000 (65.1562) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [25/38] time 0.491 (0.453) data 0.361 (0.323) loss_x loss_x 0.8027 (1.3357) acc_x 71.8750 (66.0000) lr 1.7071e-03 eta 0:00:05
epoch [52/200] batch [30/38] time 0.388 (0.458) data 0.257 (0.328) loss_x loss_x 1.2949 (1.3360) acc_x 62.5000 (66.0417) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [35/38] time 0.513 (0.457) data 0.382 (0.326) loss_x loss_x 1.0762 (1.3229) acc_x 68.7500 (66.2500) lr 1.7071e-03 eta 0:00:01
epoch [52/200] batch [5/59] time 0.483 (0.459) data 0.351 (0.328) loss_u loss_u 0.8638 (0.8298) acc_u 18.7500 (20.6250) lr 1.7071e-03 eta 0:00:24
epoch [52/200] batch [10/59] time 0.356 (0.457) data 0.223 (0.326) loss_u loss_u 0.8359 (0.8460) acc_u 21.8750 (19.6875) lr 1.7071e-03 eta 0:00:22
epoch [52/200] batch [15/59] time 0.525 (0.466) data 0.393 (0.335) loss_u loss_u 0.7158 (0.8496) acc_u 34.3750 (18.7500) lr 1.7071e-03 eta 0:00:20
epoch [52/200] batch [20/59] time 0.390 (0.461) data 0.259 (0.330) loss_u loss_u 0.7983 (0.8499) acc_u 25.0000 (18.9062) lr 1.7071e-03 eta 0:00:17
epoch [52/200] batch [25/59] time 0.424 (0.463) data 0.292 (0.332) loss_u loss_u 0.8013 (0.8445) acc_u 25.0000 (19.5000) lr 1.7071e-03 eta 0:00:15
epoch [52/200] batch [30/59] time 0.362 (0.458) data 0.231 (0.327) loss_u loss_u 0.9214 (0.8463) acc_u 12.5000 (19.2708) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [35/59] time 0.375 (0.454) data 0.244 (0.323) loss_u loss_u 0.7666 (0.8384) acc_u 31.2500 (20.2679) lr 1.7071e-03 eta 0:00:10
epoch [52/200] batch [40/59] time 0.450 (0.450) data 0.320 (0.319) loss_u loss_u 0.7910 (0.8317) acc_u 25.0000 (21.4062) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [45/59] time 0.584 (0.448) data 0.453 (0.317) loss_u loss_u 0.8174 (0.8338) acc_u 21.8750 (20.9722) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [50/59] time 0.413 (0.448) data 0.282 (0.317) loss_u loss_u 0.8354 (0.8348) acc_u 25.0000 (21.0625) lr 1.7071e-03 eta 0:00:04
epoch [52/200] batch [55/59] time 0.457 (0.445) data 0.326 (0.314) loss_u loss_u 0.8818 (0.8363) acc_u 18.7500 (21.0795) lr 1.7071e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1588
confident_label rate tensor(0.3916, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1228
clean true:1223
clean false:5
clean_rate:0.995928338762215
noisy true:325
noisy false:1583
after delete: len(clean_dataset) 1228
after delete: len(noisy_dataset) 1908
epoch [53/200] batch [5/38] time 0.481 (0.459) data 0.351 (0.328) loss_x loss_x 0.9736 (1.3596) acc_x 84.3750 (68.1250) lr 1.6959e-03 eta 0:00:15
epoch [53/200] batch [10/38] time 0.406 (0.445) data 0.276 (0.315) loss_x loss_x 0.9932 (1.3488) acc_x 78.1250 (65.9375) lr 1.6959e-03 eta 0:00:12
epoch [53/200] batch [15/38] time 0.584 (0.464) data 0.453 (0.333) loss_x loss_x 1.3105 (1.3658) acc_x 71.8750 (66.6667) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [20/38] time 0.488 (0.466) data 0.357 (0.336) loss_x loss_x 1.3037 (1.3963) acc_x 65.6250 (65.6250) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [25/38] time 0.452 (0.473) data 0.322 (0.342) loss_x loss_x 1.5488 (1.3422) acc_x 56.2500 (66.8750) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [30/38] time 0.547 (0.460) data 0.417 (0.330) loss_x loss_x 1.3984 (1.3527) acc_x 71.8750 (66.8750) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [35/38] time 0.474 (0.457) data 0.344 (0.327) loss_x loss_x 0.8315 (1.3424) acc_x 81.2500 (66.6964) lr 1.6959e-03 eta 0:00:01
epoch [53/200] batch [5/59] time 0.350 (0.458) data 0.220 (0.327) loss_u loss_u 0.8164 (0.8398) acc_u 28.1250 (23.1250) lr 1.6959e-03 eta 0:00:24
epoch [53/200] batch [10/59] time 0.467 (0.456) data 0.336 (0.325) loss_u loss_u 0.8960 (0.8368) acc_u 21.8750 (23.1250) lr 1.6959e-03 eta 0:00:22
epoch [53/200] batch [15/59] time 0.389 (0.453) data 0.258 (0.323) loss_u loss_u 0.9014 (0.8412) acc_u 18.7500 (22.7083) lr 1.6959e-03 eta 0:00:19
epoch [53/200] batch [20/59] time 0.467 (0.449) data 0.336 (0.318) loss_u loss_u 0.8540 (0.8433) acc_u 15.6250 (21.7188) lr 1.6959e-03 eta 0:00:17
epoch [53/200] batch [25/59] time 0.406 (0.449) data 0.275 (0.318) loss_u loss_u 0.8696 (0.8460) acc_u 18.7500 (21.5000) lr 1.6959e-03 eta 0:00:15
epoch [53/200] batch [30/59] time 0.621 (0.446) data 0.490 (0.315) loss_u loss_u 0.8818 (0.8488) acc_u 15.6250 (20.7292) lr 1.6959e-03 eta 0:00:12
epoch [53/200] batch [35/59] time 0.311 (0.447) data 0.180 (0.316) loss_u loss_u 0.8638 (0.8490) acc_u 18.7500 (20.8036) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [40/59] time 0.546 (0.448) data 0.415 (0.317) loss_u loss_u 0.8076 (0.8464) acc_u 25.0000 (20.9375) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [45/59] time 0.522 (0.450) data 0.391 (0.319) loss_u loss_u 0.8350 (0.8458) acc_u 21.8750 (20.9028) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [50/59] time 0.354 (0.448) data 0.222 (0.317) loss_u loss_u 0.8257 (0.8485) acc_u 25.0000 (20.5000) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [55/59] time 0.431 (0.445) data 0.300 (0.314) loss_u loss_u 0.8711 (0.8456) acc_u 21.8750 (20.9091) lr 1.6959e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1610
confident_label rate tensor(0.3817, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1197
clean true:1195
clean false:2
clean_rate:0.9983291562238931
noisy true:331
noisy false:1608
after delete: len(clean_dataset) 1197
after delete: len(noisy_dataset) 1939
epoch [54/200] batch [5/37] time 0.395 (0.430) data 0.265 (0.300) loss_x loss_x 1.3320 (1.2523) acc_x 75.0000 (71.2500) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [10/37] time 0.314 (0.436) data 0.184 (0.305) loss_x loss_x 1.7314 (1.2773) acc_x 65.6250 (68.7500) lr 1.6845e-03 eta 0:00:11
epoch [54/200] batch [15/37] time 0.374 (0.447) data 0.243 (0.316) loss_x loss_x 1.3516 (1.3292) acc_x 62.5000 (66.6667) lr 1.6845e-03 eta 0:00:09
epoch [54/200] batch [20/37] time 0.381 (0.439) data 0.250 (0.308) loss_x loss_x 1.0947 (1.3005) acc_x 75.0000 (66.7188) lr 1.6845e-03 eta 0:00:07
epoch [54/200] batch [25/37] time 0.521 (0.444) data 0.391 (0.313) loss_x loss_x 0.8979 (1.3007) acc_x 71.8750 (66.1250) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [30/37] time 0.461 (0.453) data 0.331 (0.322) loss_x loss_x 1.2822 (1.2770) acc_x 56.2500 (66.9792) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [35/37] time 0.325 (0.445) data 0.194 (0.314) loss_x loss_x 1.7900 (1.2912) acc_x 62.5000 (67.0536) lr 1.6845e-03 eta 0:00:00
epoch [54/200] batch [5/60] time 0.437 (0.448) data 0.301 (0.317) loss_u loss_u 0.8501 (0.8442) acc_u 25.0000 (21.2500) lr 1.6845e-03 eta 0:00:24
epoch [54/200] batch [10/60] time 0.422 (0.448) data 0.289 (0.317) loss_u loss_u 0.8076 (0.8343) acc_u 25.0000 (23.4375) lr 1.6845e-03 eta 0:00:22
epoch [54/200] batch [15/60] time 0.378 (0.447) data 0.247 (0.316) loss_u loss_u 0.8296 (0.8493) acc_u 18.7500 (19.7917) lr 1.6845e-03 eta 0:00:20
epoch [54/200] batch [20/60] time 0.534 (0.449) data 0.403 (0.317) loss_u loss_u 0.8311 (0.8465) acc_u 28.1250 (20.7812) lr 1.6845e-03 eta 0:00:17
epoch [54/200] batch [25/60] time 0.339 (0.443) data 0.207 (0.312) loss_u loss_u 0.8140 (0.8447) acc_u 28.1250 (21.3750) lr 1.6845e-03 eta 0:00:15
epoch [54/200] batch [30/60] time 0.368 (0.443) data 0.237 (0.311) loss_u loss_u 0.8994 (0.8412) acc_u 9.3750 (21.0417) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [35/60] time 0.371 (0.442) data 0.240 (0.311) loss_u loss_u 0.7773 (0.8382) acc_u 28.1250 (21.1607) lr 1.6845e-03 eta 0:00:11
epoch [54/200] batch [40/60] time 0.344 (0.442) data 0.214 (0.311) loss_u loss_u 0.8579 (0.8369) acc_u 18.7500 (21.0156) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [45/60] time 0.362 (0.440) data 0.231 (0.309) loss_u loss_u 0.8804 (0.8401) acc_u 12.5000 (20.2778) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [50/60] time 0.601 (0.443) data 0.470 (0.312) loss_u loss_u 0.8115 (0.8346) acc_u 31.2500 (21.3125) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [55/60] time 0.352 (0.445) data 0.221 (0.314) loss_u loss_u 0.8267 (0.8361) acc_u 15.6250 (21.3068) lr 1.6845e-03 eta 0:00:02
epoch [54/200] batch [60/60] time 0.405 (0.443) data 0.274 (0.312) loss_u loss_u 0.8101 (0.8356) acc_u 25.0000 (21.4062) lr 1.6845e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1598
confident_label rate tensor(0.3776, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1184
clean true:1181
clean false:3
clean_rate:0.9974662162162162
noisy true:357
noisy false:1595
after delete: len(clean_dataset) 1184
after delete: len(noisy_dataset) 1952
epoch [55/200] batch [5/37] time 0.460 (0.490) data 0.330 (0.360) loss_x loss_x 1.4180 (1.4186) acc_x 62.5000 (65.6250) lr 1.6730e-03 eta 0:00:15
epoch [55/200] batch [10/37] time 0.472 (0.473) data 0.342 (0.343) loss_x loss_x 1.0850 (1.3075) acc_x 75.0000 (66.2500) lr 1.6730e-03 eta 0:00:12
epoch [55/200] batch [15/37] time 0.460 (0.473) data 0.329 (0.343) loss_x loss_x 1.0225 (1.3924) acc_x 71.8750 (64.3750) lr 1.6730e-03 eta 0:00:10
epoch [55/200] batch [20/37] time 0.493 (0.475) data 0.362 (0.344) loss_x loss_x 0.9966 (1.3510) acc_x 68.7500 (65.1562) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [25/37] time 0.358 (0.465) data 0.228 (0.335) loss_x loss_x 0.9829 (1.3218) acc_x 62.5000 (65.3750) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [30/37] time 0.380 (0.457) data 0.250 (0.326) loss_x loss_x 1.2607 (1.3064) acc_x 75.0000 (65.7292) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [35/37] time 0.503 (0.460) data 0.373 (0.330) loss_x loss_x 0.7944 (1.2837) acc_x 75.0000 (66.2500) lr 1.6730e-03 eta 0:00:00
epoch [55/200] batch [5/61] time 0.446 (0.463) data 0.314 (0.332) loss_u loss_u 0.8013 (0.8017) acc_u 25.0000 (26.8750) lr 1.6730e-03 eta 0:00:25
epoch [55/200] batch [10/61] time 0.519 (0.462) data 0.387 (0.331) loss_u loss_u 0.8325 (0.8202) acc_u 28.1250 (24.3750) lr 1.6730e-03 eta 0:00:23
epoch [55/200] batch [15/61] time 0.420 (0.454) data 0.288 (0.323) loss_u loss_u 0.8569 (0.8364) acc_u 21.8750 (22.0833) lr 1.6730e-03 eta 0:00:20
epoch [55/200] batch [20/61] time 0.405 (0.455) data 0.274 (0.324) loss_u loss_u 0.7803 (0.8324) acc_u 31.2500 (22.9688) lr 1.6730e-03 eta 0:00:18
epoch [55/200] batch [25/61] time 0.398 (0.451) data 0.267 (0.320) loss_u loss_u 0.7954 (0.8299) acc_u 31.2500 (23.2500) lr 1.6730e-03 eta 0:00:16
epoch [55/200] batch [30/61] time 0.434 (0.449) data 0.302 (0.318) loss_u loss_u 0.7158 (0.8317) acc_u 34.3750 (22.7083) lr 1.6730e-03 eta 0:00:13
epoch [55/200] batch [35/61] time 0.399 (0.446) data 0.266 (0.315) loss_u loss_u 0.8525 (0.8368) acc_u 15.6250 (21.6964) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [40/61] time 0.427 (0.450) data 0.296 (0.319) loss_u loss_u 0.8818 (0.8421) acc_u 9.3750 (20.9375) lr 1.6730e-03 eta 0:00:09
epoch [55/200] batch [45/61] time 0.516 (0.450) data 0.385 (0.319) loss_u loss_u 0.8257 (0.8407) acc_u 21.8750 (21.0417) lr 1.6730e-03 eta 0:00:07
epoch [55/200] batch [50/61] time 0.466 (0.450) data 0.334 (0.319) loss_u loss_u 0.8003 (0.8415) acc_u 21.8750 (20.9375) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [55/61] time 0.442 (0.447) data 0.311 (0.316) loss_u loss_u 0.8345 (0.8400) acc_u 21.8750 (21.1364) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [60/61] time 0.368 (0.445) data 0.238 (0.315) loss_u loss_u 0.8574 (0.8401) acc_u 12.5000 (20.9375) lr 1.6730e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1563
confident_label rate tensor(0.3970, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1245
clean true:1242
clean false:3
clean_rate:0.9975903614457832
noisy true:331
noisy false:1560
after delete: len(clean_dataset) 1245
after delete: len(noisy_dataset) 1891
epoch [56/200] batch [5/38] time 0.437 (0.477) data 0.307 (0.347) loss_x loss_x 0.9072 (1.2178) acc_x 81.2500 (72.5000) lr 1.6613e-03 eta 0:00:15
epoch [56/200] batch [10/38] time 0.407 (0.442) data 0.277 (0.312) loss_x loss_x 1.9648 (1.3095) acc_x 53.1250 (67.5000) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [15/38] time 0.374 (0.437) data 0.244 (0.307) loss_x loss_x 1.4980 (1.2919) acc_x 68.7500 (66.8750) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [20/38] time 0.542 (0.449) data 0.412 (0.319) loss_x loss_x 1.1523 (1.2560) acc_x 68.7500 (66.4062) lr 1.6613e-03 eta 0:00:08
epoch [56/200] batch [25/38] time 0.384 (0.458) data 0.254 (0.328) loss_x loss_x 1.5361 (1.2718) acc_x 53.1250 (65.7500) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [30/38] time 0.394 (0.464) data 0.263 (0.333) loss_x loss_x 2.1465 (1.3091) acc_x 50.0000 (65.1042) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [35/38] time 0.418 (0.463) data 0.288 (0.333) loss_x loss_x 1.6348 (1.3254) acc_x 50.0000 (63.8393) lr 1.6613e-03 eta 0:00:01
epoch [56/200] batch [5/59] time 0.385 (0.461) data 0.252 (0.330) loss_u loss_u 0.9336 (0.8624) acc_u 9.3750 (18.7500) lr 1.6613e-03 eta 0:00:24
epoch [56/200] batch [10/59] time 0.677 (0.466) data 0.545 (0.336) loss_u loss_u 0.8931 (0.8576) acc_u 9.3750 (18.1250) lr 1.6613e-03 eta 0:00:22
epoch [56/200] batch [15/59] time 0.367 (0.462) data 0.236 (0.331) loss_u loss_u 0.8027 (0.8526) acc_u 28.1250 (20.0000) lr 1.6613e-03 eta 0:00:20
epoch [56/200] batch [20/59] time 0.420 (0.460) data 0.289 (0.329) loss_u loss_u 0.8394 (0.8606) acc_u 25.0000 (18.7500) lr 1.6613e-03 eta 0:00:17
epoch [56/200] batch [25/59] time 0.458 (0.460) data 0.327 (0.329) loss_u loss_u 0.8818 (0.8537) acc_u 12.5000 (19.2500) lr 1.6613e-03 eta 0:00:15
epoch [56/200] batch [30/59] time 0.572 (0.459) data 0.441 (0.328) loss_u loss_u 0.8521 (0.8506) acc_u 18.7500 (19.7917) lr 1.6613e-03 eta 0:00:13
epoch [56/200] batch [35/59] time 0.436 (0.456) data 0.305 (0.325) loss_u loss_u 0.8125 (0.8487) acc_u 25.0000 (19.7321) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [40/59] time 0.424 (0.453) data 0.292 (0.322) loss_u loss_u 0.8462 (0.8488) acc_u 21.8750 (19.9219) lr 1.6613e-03 eta 0:00:08
epoch [56/200] batch [45/59] time 0.380 (0.449) data 0.249 (0.318) loss_u loss_u 0.8638 (0.8507) acc_u 18.7500 (19.9306) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [50/59] time 0.465 (0.452) data 0.334 (0.321) loss_u loss_u 0.7783 (0.8475) acc_u 21.8750 (20.0625) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [55/59] time 0.359 (0.452) data 0.228 (0.321) loss_u loss_u 0.8232 (0.8483) acc_u 21.8750 (19.8295) lr 1.6613e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1569
confident_label rate tensor(0.3906, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1225
clean true:1220
clean false:5
clean_rate:0.9959183673469387
noisy true:347
noisy false:1564
after delete: len(clean_dataset) 1225
after delete: len(noisy_dataset) 1911
epoch [57/200] batch [5/38] time 0.414 (0.457) data 0.283 (0.327) loss_x loss_x 1.6240 (1.4170) acc_x 56.2500 (64.3750) lr 1.6494e-03 eta 0:00:15
epoch [57/200] batch [10/38] time 0.427 (0.437) data 0.297 (0.306) loss_x loss_x 1.2783 (1.3380) acc_x 65.6250 (66.2500) lr 1.6494e-03 eta 0:00:12
epoch [57/200] batch [15/38] time 0.524 (0.464) data 0.393 (0.334) loss_x loss_x 1.1650 (1.2850) acc_x 68.7500 (68.1250) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [20/38] time 0.415 (0.461) data 0.284 (0.330) loss_x loss_x 1.0498 (1.3059) acc_x 81.2500 (67.0312) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [25/38] time 0.385 (0.466) data 0.253 (0.335) loss_x loss_x 1.5977 (1.3045) acc_x 59.3750 (66.5000) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [30/38] time 0.396 (0.457) data 0.265 (0.327) loss_x loss_x 1.0225 (1.3340) acc_x 75.0000 (66.3542) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [35/38] time 0.434 (0.458) data 0.303 (0.327) loss_x loss_x 0.8389 (1.3313) acc_x 81.2500 (66.6964) lr 1.6494e-03 eta 0:00:01
epoch [57/200] batch [5/59] time 0.731 (0.461) data 0.599 (0.330) loss_u loss_u 0.8384 (0.8414) acc_u 15.6250 (21.2500) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [10/59] time 0.477 (0.460) data 0.345 (0.329) loss_u loss_u 0.8159 (0.8441) acc_u 28.1250 (20.9375) lr 1.6494e-03 eta 0:00:22
epoch [57/200] batch [15/59] time 0.439 (0.457) data 0.308 (0.327) loss_u loss_u 0.8594 (0.8473) acc_u 9.3750 (19.1667) lr 1.6494e-03 eta 0:00:20
epoch [57/200] batch [20/59] time 0.465 (0.456) data 0.333 (0.325) loss_u loss_u 0.8359 (0.8448) acc_u 15.6250 (19.6875) lr 1.6494e-03 eta 0:00:17
epoch [57/200] batch [25/59] time 0.562 (0.459) data 0.431 (0.328) loss_u loss_u 0.7979 (0.8401) acc_u 25.0000 (20.8750) lr 1.6494e-03 eta 0:00:15
epoch [57/200] batch [30/59] time 0.419 (0.461) data 0.287 (0.330) loss_u loss_u 0.8540 (0.8418) acc_u 25.0000 (20.4167) lr 1.6494e-03 eta 0:00:13
epoch [57/200] batch [35/59] time 0.367 (0.458) data 0.237 (0.327) loss_u loss_u 0.8994 (0.8425) acc_u 9.3750 (20.3571) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [40/59] time 0.511 (0.457) data 0.380 (0.326) loss_u loss_u 0.9014 (0.8431) acc_u 12.5000 (20.2344) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [45/59] time 0.368 (0.451) data 0.237 (0.320) loss_u loss_u 0.7471 (0.8431) acc_u 37.5000 (20.2083) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [50/59] time 0.387 (0.450) data 0.256 (0.318) loss_u loss_u 0.8818 (0.8458) acc_u 18.7500 (19.9375) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [55/59] time 0.506 (0.453) data 0.375 (0.321) loss_u loss_u 0.8052 (0.8457) acc_u 25.0000 (20.1136) lr 1.6494e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1577
confident_label rate tensor(0.3858, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1210
clean true:1207
clean false:3
clean_rate:0.9975206611570248
noisy true:352
noisy false:1574
after delete: len(clean_dataset) 1210
after delete: len(noisy_dataset) 1926
epoch [58/200] batch [5/37] time 0.446 (0.427) data 0.316 (0.297) loss_x loss_x 1.0869 (1.1894) acc_x 65.6250 (67.5000) lr 1.6374e-03 eta 0:00:13
epoch [58/200] batch [10/37] time 0.567 (0.464) data 0.438 (0.334) loss_x loss_x 1.2461 (1.2500) acc_x 59.3750 (66.8750) lr 1.6374e-03 eta 0:00:12
epoch [58/200] batch [15/37] time 0.392 (0.449) data 0.263 (0.319) loss_x loss_x 0.8467 (1.3143) acc_x 78.1250 (65.4167) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [20/37] time 0.469 (0.453) data 0.339 (0.323) loss_x loss_x 2.0234 (1.3469) acc_x 50.0000 (65.4688) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [25/37] time 0.612 (0.449) data 0.482 (0.319) loss_x loss_x 1.1064 (1.2960) acc_x 68.7500 (67.1250) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [30/37] time 0.566 (0.453) data 0.435 (0.322) loss_x loss_x 0.9541 (1.2846) acc_x 78.1250 (67.0833) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [35/37] time 0.476 (0.450) data 0.344 (0.320) loss_x loss_x 1.1094 (1.2838) acc_x 68.7500 (66.6071) lr 1.6374e-03 eta 0:00:00
epoch [58/200] batch [5/60] time 0.530 (0.453) data 0.398 (0.323) loss_u loss_u 0.8374 (0.8469) acc_u 31.2500 (21.8750) lr 1.6374e-03 eta 0:00:24
epoch [58/200] batch [10/60] time 0.337 (0.449) data 0.204 (0.319) loss_u loss_u 0.7876 (0.8364) acc_u 31.2500 (23.4375) lr 1.6374e-03 eta 0:00:22
epoch [58/200] batch [15/60] time 0.450 (0.450) data 0.318 (0.319) loss_u loss_u 0.8154 (0.8468) acc_u 25.0000 (21.2500) lr 1.6374e-03 eta 0:00:20
epoch [58/200] batch [20/60] time 0.438 (0.446) data 0.306 (0.315) loss_u loss_u 0.8784 (0.8455) acc_u 18.7500 (21.4062) lr 1.6374e-03 eta 0:00:17
epoch [58/200] batch [25/60] time 0.462 (0.456) data 0.331 (0.325) loss_u loss_u 0.8140 (0.8448) acc_u 28.1250 (21.7500) lr 1.6374e-03 eta 0:00:15
epoch [58/200] batch [30/60] time 0.395 (0.452) data 0.263 (0.321) loss_u loss_u 0.8657 (0.8454) acc_u 15.6250 (21.2500) lr 1.6374e-03 eta 0:00:13
epoch [58/200] batch [35/60] time 0.411 (0.453) data 0.280 (0.322) loss_u loss_u 0.8223 (0.8430) acc_u 31.2500 (21.6071) lr 1.6374e-03 eta 0:00:11
epoch [58/200] batch [40/60] time 0.317 (0.450) data 0.187 (0.319) loss_u loss_u 0.8760 (0.8449) acc_u 15.6250 (21.0938) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [45/60] time 0.386 (0.450) data 0.256 (0.319) loss_u loss_u 0.8916 (0.8468) acc_u 12.5000 (20.8333) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [50/60] time 0.494 (0.449) data 0.364 (0.318) loss_u loss_u 0.8350 (0.8462) acc_u 18.7500 (20.6875) lr 1.6374e-03 eta 0:00:04
epoch [58/200] batch [55/60] time 0.417 (0.447) data 0.285 (0.316) loss_u loss_u 0.7573 (0.8425) acc_u 31.2500 (21.1932) lr 1.6374e-03 eta 0:00:02
epoch [58/200] batch [60/60] time 0.321 (0.446) data 0.190 (0.315) loss_u loss_u 0.8457 (0.8418) acc_u 15.6250 (21.1458) lr 1.6374e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1617
confident_label rate tensor(0.3798, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1191
clean true:1189
clean false:2
clean_rate:0.998320738874895
noisy true:330
noisy false:1615
after delete: len(clean_dataset) 1191
after delete: len(noisy_dataset) 1945
epoch [59/200] batch [5/37] time 0.522 (0.480) data 0.391 (0.349) loss_x loss_x 1.2705 (1.3470) acc_x 65.6250 (66.2500) lr 1.6252e-03 eta 0:00:15
epoch [59/200] batch [10/37] time 0.378 (0.463) data 0.247 (0.332) loss_x loss_x 1.1963 (1.3120) acc_x 71.8750 (65.9375) lr 1.6252e-03 eta 0:00:12
epoch [59/200] batch [15/37] time 0.362 (0.457) data 0.232 (0.327) loss_x loss_x 0.8179 (1.2742) acc_x 84.3750 (67.0833) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [20/37] time 0.497 (0.449) data 0.367 (0.318) loss_x loss_x 1.1035 (1.2868) acc_x 71.8750 (66.7188) lr 1.6252e-03 eta 0:00:07
epoch [59/200] batch [25/37] time 0.396 (0.459) data 0.266 (0.328) loss_x loss_x 1.4268 (1.2916) acc_x 62.5000 (66.7500) lr 1.6252e-03 eta 0:00:05
epoch [59/200] batch [30/37] time 0.436 (0.457) data 0.305 (0.326) loss_x loss_x 1.2402 (1.2895) acc_x 71.8750 (67.0833) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [35/37] time 0.385 (0.454) data 0.254 (0.323) loss_x loss_x 1.5742 (1.2950) acc_x 62.5000 (67.1429) lr 1.6252e-03 eta 0:00:00
epoch [59/200] batch [5/60] time 0.376 (0.451) data 0.245 (0.321) loss_u loss_u 0.9082 (0.8714) acc_u 9.3750 (15.6250) lr 1.6252e-03 eta 0:00:24
epoch [59/200] batch [10/60] time 0.420 (0.451) data 0.289 (0.320) loss_u loss_u 0.9053 (0.8375) acc_u 12.5000 (20.6250) lr 1.6252e-03 eta 0:00:22
epoch [59/200] batch [15/60] time 0.423 (0.451) data 0.293 (0.320) loss_u loss_u 0.8613 (0.8363) acc_u 9.3750 (20.6250) lr 1.6252e-03 eta 0:00:20
epoch [59/200] batch [20/60] time 0.458 (0.449) data 0.327 (0.318) loss_u loss_u 0.7930 (0.8262) acc_u 21.8750 (22.1875) lr 1.6252e-03 eta 0:00:17
epoch [59/200] batch [25/60] time 0.435 (0.448) data 0.304 (0.318) loss_u loss_u 0.8315 (0.8293) acc_u 18.7500 (21.3750) lr 1.6252e-03 eta 0:00:15
epoch [59/200] batch [30/60] time 0.431 (0.451) data 0.301 (0.320) loss_u loss_u 0.8525 (0.8307) acc_u 28.1250 (22.1875) lr 1.6252e-03 eta 0:00:13
epoch [59/200] batch [35/60] time 0.516 (0.451) data 0.386 (0.320) loss_u loss_u 0.8291 (0.8330) acc_u 25.0000 (21.9643) lr 1.6252e-03 eta 0:00:11
epoch [59/200] batch [40/60] time 0.441 (0.448) data 0.310 (0.318) loss_u loss_u 0.8887 (0.8347) acc_u 15.6250 (21.7188) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [45/60] time 0.418 (0.448) data 0.288 (0.317) loss_u loss_u 0.8032 (0.8358) acc_u 28.1250 (21.6667) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [50/60] time 0.331 (0.448) data 0.201 (0.317) loss_u loss_u 0.8228 (0.8342) acc_u 21.8750 (21.6875) lr 1.6252e-03 eta 0:00:04
epoch [59/200] batch [55/60] time 0.356 (0.446) data 0.225 (0.315) loss_u loss_u 0.7852 (0.8347) acc_u 28.1250 (21.4773) lr 1.6252e-03 eta 0:00:02
epoch [59/200] batch [60/60] time 0.516 (0.446) data 0.384 (0.315) loss_u loss_u 0.9189 (0.8355) acc_u 9.3750 (21.3542) lr 1.6252e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1558
confident_label rate tensor(0.3948, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1238
clean true:1232
clean false:6
clean_rate:0.9951534733441034
noisy true:346
noisy false:1552
after delete: len(clean_dataset) 1238
after delete: len(noisy_dataset) 1898
epoch [60/200] batch [5/38] time 0.465 (0.494) data 0.335 (0.364) loss_x loss_x 1.5469 (1.4869) acc_x 59.3750 (65.0000) lr 1.6129e-03 eta 0:00:16
epoch [60/200] batch [10/38] time 0.500 (0.475) data 0.368 (0.345) loss_x loss_x 1.2256 (1.3081) acc_x 71.8750 (68.7500) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [15/38] time 0.530 (0.482) data 0.399 (0.352) loss_x loss_x 1.2666 (1.3453) acc_x 62.5000 (66.2500) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [20/38] time 0.525 (0.474) data 0.394 (0.344) loss_x loss_x 1.1367 (1.3224) acc_x 68.7500 (67.0312) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [25/38] time 0.383 (0.463) data 0.252 (0.333) loss_x loss_x 1.3291 (1.2807) acc_x 68.7500 (68.0000) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [30/38] time 0.391 (0.454) data 0.260 (0.324) loss_x loss_x 1.0996 (1.2698) acc_x 75.0000 (68.2292) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [35/38] time 0.392 (0.457) data 0.262 (0.327) loss_x loss_x 1.4072 (1.2596) acc_x 71.8750 (68.8393) lr 1.6129e-03 eta 0:00:01
epoch [60/200] batch [5/59] time 0.482 (0.454) data 0.350 (0.323) loss_u loss_u 0.7163 (0.8445) acc_u 31.2500 (18.7500) lr 1.6129e-03 eta 0:00:24
epoch [60/200] batch [10/59] time 0.341 (0.451) data 0.208 (0.320) loss_u loss_u 0.8149 (0.8399) acc_u 28.1250 (19.6875) lr 1.6129e-03 eta 0:00:22
epoch [60/200] batch [15/59] time 0.451 (0.455) data 0.319 (0.324) loss_u loss_u 0.8467 (0.8364) acc_u 21.8750 (21.0417) lr 1.6129e-03 eta 0:00:20
epoch [60/200] batch [20/59] time 0.424 (0.452) data 0.293 (0.321) loss_u loss_u 0.8257 (0.8341) acc_u 28.1250 (21.8750) lr 1.6129e-03 eta 0:00:17
epoch [60/200] batch [25/59] time 0.441 (0.452) data 0.310 (0.322) loss_u loss_u 0.7617 (0.8316) acc_u 34.3750 (21.7500) lr 1.6129e-03 eta 0:00:15
epoch [60/200] batch [30/59] time 0.629 (0.452) data 0.497 (0.321) loss_u loss_u 0.8164 (0.8318) acc_u 25.0000 (21.6667) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [35/59] time 0.409 (0.451) data 0.278 (0.320) loss_u loss_u 0.7852 (0.8347) acc_u 28.1250 (21.3393) lr 1.6129e-03 eta 0:00:10
epoch [60/200] batch [40/59] time 0.495 (0.446) data 0.363 (0.315) loss_u loss_u 0.8652 (0.8384) acc_u 18.7500 (20.7812) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [45/59] time 0.408 (0.447) data 0.276 (0.316) loss_u loss_u 0.8867 (0.8413) acc_u 15.6250 (20.2778) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [50/59] time 0.449 (0.443) data 0.318 (0.312) loss_u loss_u 0.8472 (0.8438) acc_u 25.0000 (20.0625) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [55/59] time 0.403 (0.441) data 0.271 (0.310) loss_u loss_u 0.9082 (0.8483) acc_u 12.5000 (19.5455) lr 1.6129e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1613
confident_label rate tensor(0.3785, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1187
clean true:1184
clean false:3
clean_rate:0.9974726200505476
noisy true:339
noisy false:1610
after delete: len(clean_dataset) 1187
after delete: len(noisy_dataset) 1949
epoch [61/200] batch [5/37] time 0.388 (0.465) data 0.257 (0.334) loss_x loss_x 1.6680 (1.3113) acc_x 68.7500 (63.1250) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [10/37] time 0.423 (0.467) data 0.292 (0.335) loss_x loss_x 1.2461 (1.2847) acc_x 65.6250 (63.7500) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [15/37] time 0.382 (0.477) data 0.251 (0.346) loss_x loss_x 1.0889 (1.2433) acc_x 65.6250 (65.6250) lr 1.6004e-03 eta 0:00:10
epoch [61/200] batch [20/37] time 0.459 (0.493) data 0.328 (0.362) loss_x loss_x 1.6816 (1.2608) acc_x 68.7500 (66.4062) lr 1.6004e-03 eta 0:00:08
epoch [61/200] batch [25/37] time 0.443 (0.490) data 0.311 (0.360) loss_x loss_x 1.3564 (1.2771) acc_x 65.6250 (66.3750) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [30/37] time 0.356 (0.482) data 0.225 (0.351) loss_x loss_x 1.3818 (1.2703) acc_x 68.7500 (66.9792) lr 1.6004e-03 eta 0:00:03
epoch [61/200] batch [35/37] time 0.520 (0.482) data 0.389 (0.351) loss_x loss_x 0.9282 (1.2635) acc_x 78.1250 (67.5000) lr 1.6004e-03 eta 0:00:00
epoch [61/200] batch [5/60] time 0.592 (0.480) data 0.460 (0.349) loss_u loss_u 0.8774 (0.8226) acc_u 12.5000 (21.8750) lr 1.6004e-03 eta 0:00:26
epoch [61/200] batch [10/60] time 0.386 (0.483) data 0.251 (0.352) loss_u loss_u 0.7461 (0.8094) acc_u 31.2500 (23.4375) lr 1.6004e-03 eta 0:00:24
epoch [61/200] batch [15/60] time 0.780 (0.494) data 0.648 (0.362) loss_u loss_u 0.8350 (0.8199) acc_u 25.0000 (22.2917) lr 1.6004e-03 eta 0:00:22
epoch [61/200] batch [20/60] time 0.347 (0.491) data 0.214 (0.359) loss_u loss_u 0.8706 (0.8338) acc_u 25.0000 (21.0938) lr 1.6004e-03 eta 0:00:19
epoch [61/200] batch [25/60] time 0.553 (0.486) data 0.420 (0.354) loss_u loss_u 0.7759 (0.8352) acc_u 31.2500 (20.8750) lr 1.6004e-03 eta 0:00:16
epoch [61/200] batch [30/60] time 0.394 (0.482) data 0.262 (0.350) loss_u loss_u 0.8730 (0.8389) acc_u 15.6250 (20.3125) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [35/60] time 0.434 (0.479) data 0.302 (0.348) loss_u loss_u 0.8105 (0.8299) acc_u 25.0000 (21.6964) lr 1.6004e-03 eta 0:00:11
epoch [61/200] batch [40/60] time 0.525 (0.477) data 0.393 (0.346) loss_u loss_u 0.9365 (0.8347) acc_u 3.1250 (21.0156) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [45/60] time 0.612 (0.475) data 0.481 (0.344) loss_u loss_u 0.8472 (0.8354) acc_u 15.6250 (20.8333) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [50/60] time 0.690 (0.474) data 0.558 (0.342) loss_u loss_u 0.8721 (0.8364) acc_u 18.7500 (20.5625) lr 1.6004e-03 eta 0:00:04
epoch [61/200] batch [55/60] time 0.338 (0.471) data 0.206 (0.340) loss_u loss_u 0.7734 (0.8343) acc_u 34.3750 (21.1932) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [60/60] time 0.364 (0.468) data 0.233 (0.337) loss_u loss_u 0.8770 (0.8356) acc_u 25.0000 (21.1979) lr 1.6004e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1558
confident_label rate tensor(0.3967, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1244
clean true:1241
clean false:3
clean_rate:0.997588424437299
noisy true:337
noisy false:1555
after delete: len(clean_dataset) 1244
after delete: len(noisy_dataset) 1892
epoch [62/200] batch [5/38] time 0.443 (0.456) data 0.313 (0.326) loss_x loss_x 1.0000 (1.0826) acc_x 81.2500 (71.2500) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [10/38] time 0.431 (0.461) data 0.300 (0.331) loss_x loss_x 0.8726 (1.1077) acc_x 81.2500 (73.1250) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [15/38] time 0.462 (0.453) data 0.332 (0.323) loss_x loss_x 1.3281 (1.1798) acc_x 65.6250 (69.7917) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [20/38] time 0.391 (0.444) data 0.260 (0.314) loss_x loss_x 1.2676 (1.1925) acc_x 65.6250 (69.2188) lr 1.5878e-03 eta 0:00:07
epoch [62/200] batch [25/38] time 0.440 (0.446) data 0.309 (0.315) loss_x loss_x 1.2207 (1.1976) acc_x 59.3750 (67.5000) lr 1.5878e-03 eta 0:00:05
epoch [62/200] batch [30/38] time 0.415 (0.446) data 0.285 (0.316) loss_x loss_x 1.2617 (1.2042) acc_x 71.8750 (67.7083) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [35/38] time 0.377 (0.443) data 0.248 (0.313) loss_x loss_x 1.3330 (1.2587) acc_x 65.6250 (66.6071) lr 1.5878e-03 eta 0:00:01
epoch [62/200] batch [5/59] time 0.395 (0.443) data 0.264 (0.313) loss_u loss_u 0.8652 (0.8415) acc_u 18.7500 (21.2500) lr 1.5878e-03 eta 0:00:23
epoch [62/200] batch [10/59] time 0.446 (0.440) data 0.314 (0.310) loss_u loss_u 0.8286 (0.8344) acc_u 25.0000 (21.2500) lr 1.5878e-03 eta 0:00:21
epoch [62/200] batch [15/59] time 0.596 (0.447) data 0.465 (0.316) loss_u loss_u 0.8604 (0.8438) acc_u 18.7500 (20.2083) lr 1.5878e-03 eta 0:00:19
epoch [62/200] batch [20/59] time 0.365 (0.447) data 0.234 (0.316) loss_u loss_u 0.8701 (0.8474) acc_u 9.3750 (19.5312) lr 1.5878e-03 eta 0:00:17
epoch [62/200] batch [25/59] time 0.341 (0.446) data 0.210 (0.315) loss_u loss_u 0.9131 (0.8540) acc_u 9.3750 (18.3750) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [30/59] time 0.480 (0.446) data 0.349 (0.316) loss_u loss_u 0.9062 (0.8506) acc_u 12.5000 (19.0625) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [35/59] time 0.386 (0.449) data 0.254 (0.318) loss_u loss_u 0.8447 (0.8524) acc_u 15.6250 (18.3929) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [40/59] time 0.411 (0.446) data 0.280 (0.315) loss_u loss_u 0.8599 (0.8490) acc_u 18.7500 (18.9844) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [45/59] time 0.421 (0.444) data 0.290 (0.313) loss_u loss_u 0.8262 (0.8475) acc_u 21.8750 (19.3056) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [50/59] time 0.439 (0.442) data 0.308 (0.311) loss_u loss_u 0.8389 (0.8451) acc_u 21.8750 (19.8750) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [55/59] time 0.387 (0.445) data 0.255 (0.314) loss_u loss_u 0.8433 (0.8431) acc_u 18.7500 (20.0568) lr 1.5878e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1565
confident_label rate tensor(0.3951, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1239
clean true:1235
clean false:4
clean_rate:0.9967715899919289
noisy true:336
noisy false:1561
after delete: len(clean_dataset) 1239
after delete: len(noisy_dataset) 1897
epoch [63/200] batch [5/38] time 0.401 (0.430) data 0.271 (0.300) loss_x loss_x 1.4824 (1.2034) acc_x 65.6250 (71.8750) lr 1.5750e-03 eta 0:00:14
epoch [63/200] batch [10/38] time 0.523 (0.463) data 0.393 (0.333) loss_x loss_x 1.3203 (1.2108) acc_x 65.6250 (71.5625) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [15/38] time 0.358 (0.447) data 0.228 (0.317) loss_x loss_x 1.4688 (1.2368) acc_x 68.7500 (69.3750) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [20/38] time 0.418 (0.443) data 0.288 (0.313) loss_x loss_x 1.2061 (1.2055) acc_x 65.6250 (70.3125) lr 1.5750e-03 eta 0:00:07
epoch [63/200] batch [25/38] time 0.484 (0.445) data 0.354 (0.315) loss_x loss_x 1.0820 (1.2148) acc_x 75.0000 (69.8750) lr 1.5750e-03 eta 0:00:05
epoch [63/200] batch [30/38] time 0.413 (0.437) data 0.284 (0.307) loss_x loss_x 1.5654 (1.2148) acc_x 56.2500 (69.8958) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [35/38] time 0.422 (0.442) data 0.292 (0.312) loss_x loss_x 1.1514 (1.2375) acc_x 68.7500 (69.0179) lr 1.5750e-03 eta 0:00:01
epoch [63/200] batch [5/59] time 0.444 (0.437) data 0.313 (0.307) loss_u loss_u 0.7690 (0.8303) acc_u 28.1250 (22.5000) lr 1.5750e-03 eta 0:00:23
epoch [63/200] batch [10/59] time 0.400 (0.434) data 0.268 (0.303) loss_u loss_u 0.8999 (0.8427) acc_u 9.3750 (21.8750) lr 1.5750e-03 eta 0:00:21
epoch [63/200] batch [15/59] time 0.472 (0.434) data 0.340 (0.304) loss_u loss_u 0.8857 (0.8356) acc_u 18.7500 (22.7083) lr 1.5750e-03 eta 0:00:19
epoch [63/200] batch [20/59] time 0.516 (0.442) data 0.385 (0.311) loss_u loss_u 0.8364 (0.8365) acc_u 18.7500 (22.1875) lr 1.5750e-03 eta 0:00:17
epoch [63/200] batch [25/59] time 0.398 (0.443) data 0.266 (0.312) loss_u loss_u 0.8491 (0.8377) acc_u 12.5000 (21.5000) lr 1.5750e-03 eta 0:00:15
epoch [63/200] batch [30/59] time 0.495 (0.446) data 0.364 (0.316) loss_u loss_u 0.8281 (0.8374) acc_u 28.1250 (21.1458) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [35/59] time 0.584 (0.445) data 0.452 (0.314) loss_u loss_u 0.8818 (0.8408) acc_u 12.5000 (20.8036) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [40/59] time 0.444 (0.443) data 0.314 (0.312) loss_u loss_u 0.8530 (0.8444) acc_u 15.6250 (20.2344) lr 1.5750e-03 eta 0:00:08
epoch [63/200] batch [45/59] time 0.469 (0.444) data 0.338 (0.313) loss_u loss_u 0.8350 (0.8438) acc_u 21.8750 (20.4861) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [50/59] time 0.412 (0.442) data 0.281 (0.311) loss_u loss_u 0.7466 (0.8399) acc_u 31.2500 (20.8125) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [55/59] time 0.377 (0.442) data 0.244 (0.311) loss_u loss_u 0.7842 (0.8401) acc_u 25.0000 (20.7386) lr 1.5750e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1600
confident_label rate tensor(0.3865, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1212
clean true:1209
clean false:3
clean_rate:0.9975247524752475
noisy true:327
noisy false:1597
after delete: len(clean_dataset) 1212
after delete: len(noisy_dataset) 1924
epoch [64/200] batch [5/37] time 0.486 (0.448) data 0.355 (0.318) loss_x loss_x 1.5029 (1.3359) acc_x 62.5000 (67.5000) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [10/37] time 0.499 (0.451) data 0.368 (0.321) loss_x loss_x 1.0225 (1.1939) acc_x 81.2500 (69.6875) lr 1.5621e-03 eta 0:00:12
epoch [64/200] batch [15/37] time 0.387 (0.463) data 0.256 (0.332) loss_x loss_x 1.1299 (1.2079) acc_x 62.5000 (68.7500) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [20/37] time 0.404 (0.467) data 0.273 (0.336) loss_x loss_x 1.5498 (1.2521) acc_x 62.5000 (67.9688) lr 1.5621e-03 eta 0:00:07
epoch [64/200] batch [25/37] time 0.362 (0.457) data 0.232 (0.327) loss_x loss_x 1.3564 (1.2341) acc_x 71.8750 (69.2500) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [30/37] time 0.590 (0.463) data 0.460 (0.332) loss_x loss_x 1.0518 (1.2141) acc_x 75.0000 (69.5833) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [35/37] time 0.392 (0.452) data 0.261 (0.322) loss_x loss_x 1.9404 (1.2273) acc_x 53.1250 (69.1964) lr 1.5621e-03 eta 0:00:00
epoch [64/200] batch [5/60] time 0.404 (0.459) data 0.273 (0.328) loss_u loss_u 0.9209 (0.8730) acc_u 15.6250 (16.2500) lr 1.5621e-03 eta 0:00:25
epoch [64/200] batch [10/60] time 0.400 (0.454) data 0.268 (0.323) loss_u loss_u 0.8989 (0.8563) acc_u 12.5000 (19.3750) lr 1.5621e-03 eta 0:00:22
epoch [64/200] batch [15/60] time 0.451 (0.451) data 0.319 (0.320) loss_u loss_u 0.8604 (0.8558) acc_u 21.8750 (19.7917) lr 1.5621e-03 eta 0:00:20
epoch [64/200] batch [20/60] time 0.366 (0.446) data 0.235 (0.315) loss_u loss_u 0.8369 (0.8550) acc_u 21.8750 (19.6875) lr 1.5621e-03 eta 0:00:17
epoch [64/200] batch [25/60] time 0.488 (0.447) data 0.357 (0.316) loss_u loss_u 0.7754 (0.8462) acc_u 21.8750 (21.0000) lr 1.5621e-03 eta 0:00:15
epoch [64/200] batch [30/60] time 0.441 (0.445) data 0.309 (0.314) loss_u loss_u 0.9194 (0.8466) acc_u 9.3750 (20.6250) lr 1.5621e-03 eta 0:00:13
epoch [64/200] batch [35/60] time 0.385 (0.444) data 0.254 (0.313) loss_u loss_u 0.9043 (0.8438) acc_u 9.3750 (20.5357) lr 1.5621e-03 eta 0:00:11
epoch [64/200] batch [40/60] time 0.452 (0.446) data 0.320 (0.315) loss_u loss_u 0.8921 (0.8427) acc_u 18.7500 (20.5469) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [45/60] time 0.682 (0.448) data 0.550 (0.317) loss_u loss_u 0.8530 (0.8462) acc_u 15.6250 (19.8611) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [50/60] time 0.414 (0.448) data 0.282 (0.317) loss_u loss_u 0.8638 (0.8481) acc_u 15.6250 (19.5625) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [55/60] time 0.405 (0.448) data 0.273 (0.316) loss_u loss_u 0.8940 (0.8468) acc_u 12.5000 (20.0568) lr 1.5621e-03 eta 0:00:02
epoch [64/200] batch [60/60] time 0.422 (0.449) data 0.291 (0.317) loss_u loss_u 0.8208 (0.8449) acc_u 18.7500 (20.2604) lr 1.5621e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1579
confident_label rate tensor(0.3874, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1215
clean true:1214
clean false:1
clean_rate:0.9991769547325103
noisy true:343
noisy false:1578
after delete: len(clean_dataset) 1215
after delete: len(noisy_dataset) 1921
epoch [65/200] batch [5/37] time 0.391 (0.435) data 0.261 (0.304) loss_x loss_x 1.4238 (1.5297) acc_x 62.5000 (62.5000) lr 1.5490e-03 eta 0:00:13
epoch [65/200] batch [10/37] time 0.432 (0.448) data 0.301 (0.318) loss_x loss_x 1.1826 (1.4104) acc_x 68.7500 (65.3125) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [15/37] time 0.447 (0.460) data 0.316 (0.329) loss_x loss_x 0.8179 (1.3506) acc_x 78.1250 (65.6250) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [20/37] time 0.507 (0.454) data 0.377 (0.323) loss_x loss_x 1.1592 (1.3246) acc_x 75.0000 (67.5000) lr 1.5490e-03 eta 0:00:07
epoch [65/200] batch [25/37] time 0.461 (0.451) data 0.330 (0.321) loss_x loss_x 1.5303 (1.3815) acc_x 62.5000 (65.7500) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [30/37] time 0.476 (0.454) data 0.346 (0.323) loss_x loss_x 0.6953 (1.3440) acc_x 81.2500 (66.4583) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [35/37] time 0.454 (0.447) data 0.323 (0.316) loss_x loss_x 1.4502 (1.3335) acc_x 59.3750 (66.6964) lr 1.5490e-03 eta 0:00:00
epoch [65/200] batch [5/60] time 0.444 (0.455) data 0.309 (0.324) loss_u loss_u 0.8149 (0.7975) acc_u 21.8750 (26.2500) lr 1.5490e-03 eta 0:00:25
epoch [65/200] batch [10/60] time 0.552 (0.456) data 0.421 (0.325) loss_u loss_u 0.8354 (0.8117) acc_u 21.8750 (23.7500) lr 1.5490e-03 eta 0:00:22
epoch [65/200] batch [15/60] time 0.367 (0.455) data 0.236 (0.324) loss_u loss_u 0.8555 (0.8262) acc_u 18.7500 (22.2917) lr 1.5490e-03 eta 0:00:20
epoch [65/200] batch [20/60] time 0.607 (0.454) data 0.476 (0.323) loss_u loss_u 0.8472 (0.8412) acc_u 15.6250 (20.7812) lr 1.5490e-03 eta 0:00:18
epoch [65/200] batch [25/60] time 0.700 (0.453) data 0.569 (0.322) loss_u loss_u 0.8315 (0.8450) acc_u 21.8750 (20.1250) lr 1.5490e-03 eta 0:00:15
epoch [65/200] batch [30/60] time 0.385 (0.449) data 0.254 (0.318) loss_u loss_u 0.8223 (0.8433) acc_u 21.8750 (20.5208) lr 1.5490e-03 eta 0:00:13
epoch [65/200] batch [35/60] time 0.429 (0.444) data 0.298 (0.313) loss_u loss_u 0.9038 (0.8454) acc_u 15.6250 (20.5357) lr 1.5490e-03 eta 0:00:11
epoch [65/200] batch [40/60] time 0.370 (0.448) data 0.239 (0.317) loss_u loss_u 0.8384 (0.8443) acc_u 21.8750 (20.7031) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [45/60] time 0.325 (0.446) data 0.194 (0.315) loss_u loss_u 0.8218 (0.8419) acc_u 21.8750 (21.1806) lr 1.5490e-03 eta 0:00:06
epoch [65/200] batch [50/60] time 0.348 (0.444) data 0.217 (0.313) loss_u loss_u 0.8506 (0.8396) acc_u 18.7500 (21.5625) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [55/60] time 0.424 (0.442) data 0.294 (0.312) loss_u loss_u 0.8447 (0.8405) acc_u 18.7500 (21.4205) lr 1.5490e-03 eta 0:00:02
epoch [65/200] batch [60/60] time 0.514 (0.445) data 0.383 (0.314) loss_u loss_u 0.8120 (0.8378) acc_u 21.8750 (21.7188) lr 1.5490e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1592
confident_label rate tensor(0.3925, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1231
clean true:1228
clean false:3
clean_rate:0.9975629569455727
noisy true:316
noisy false:1589
after delete: len(clean_dataset) 1231
after delete: len(noisy_dataset) 1905
epoch [66/200] batch [5/38] time 0.571 (0.453) data 0.441 (0.322) loss_x loss_x 1.9268 (1.2870) acc_x 46.8750 (67.5000) lr 1.5358e-03 eta 0:00:14
epoch [66/200] batch [10/38] time 0.409 (0.437) data 0.279 (0.306) loss_x loss_x 0.8330 (1.1208) acc_x 75.0000 (71.2500) lr 1.5358e-03 eta 0:00:12
epoch [66/200] batch [15/38] time 0.390 (0.427) data 0.260 (0.296) loss_x loss_x 1.1514 (1.1340) acc_x 71.8750 (70.8333) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [20/38] time 0.444 (0.430) data 0.313 (0.299) loss_x loss_x 1.2607 (1.1614) acc_x 71.8750 (71.8750) lr 1.5358e-03 eta 0:00:07
epoch [66/200] batch [25/38] time 0.476 (0.432) data 0.345 (0.301) loss_x loss_x 1.7441 (1.1608) acc_x 56.2500 (72.0000) lr 1.5358e-03 eta 0:00:05
epoch [66/200] batch [30/38] time 0.397 (0.434) data 0.266 (0.304) loss_x loss_x 1.1201 (1.1611) acc_x 78.1250 (71.5625) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [35/38] time 0.413 (0.436) data 0.282 (0.305) loss_x loss_x 1.7051 (1.2109) acc_x 62.5000 (69.8214) lr 1.5358e-03 eta 0:00:01
epoch [66/200] batch [5/59] time 0.401 (0.433) data 0.270 (0.302) loss_u loss_u 0.8374 (0.8231) acc_u 21.8750 (23.7500) lr 1.5358e-03 eta 0:00:23
epoch [66/200] batch [10/59] time 0.402 (0.438) data 0.271 (0.308) loss_u loss_u 0.8198 (0.8418) acc_u 21.8750 (20.6250) lr 1.5358e-03 eta 0:00:21
epoch [66/200] batch [15/59] time 0.638 (0.443) data 0.506 (0.312) loss_u loss_u 0.9028 (0.8460) acc_u 12.5000 (20.2083) lr 1.5358e-03 eta 0:00:19
epoch [66/200] batch [20/59] time 0.426 (0.439) data 0.294 (0.308) loss_u loss_u 0.8335 (0.8443) acc_u 21.8750 (20.4688) lr 1.5358e-03 eta 0:00:17
epoch [66/200] batch [25/59] time 0.433 (0.437) data 0.301 (0.306) loss_u loss_u 0.7993 (0.8438) acc_u 25.0000 (20.2500) lr 1.5358e-03 eta 0:00:14
epoch [66/200] batch [30/59] time 0.402 (0.435) data 0.271 (0.304) loss_u loss_u 0.8740 (0.8474) acc_u 18.7500 (19.8958) lr 1.5358e-03 eta 0:00:12
epoch [66/200] batch [35/59] time 0.661 (0.437) data 0.529 (0.306) loss_u loss_u 0.8477 (0.8488) acc_u 18.7500 (19.7321) lr 1.5358e-03 eta 0:00:10
epoch [66/200] batch [40/59] time 0.728 (0.442) data 0.597 (0.311) loss_u loss_u 0.7935 (0.8451) acc_u 28.1250 (20.2344) lr 1.5358e-03 eta 0:00:08
epoch [66/200] batch [45/59] time 0.498 (0.443) data 0.366 (0.312) loss_u loss_u 0.8052 (0.8458) acc_u 28.1250 (19.9306) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [50/59] time 0.404 (0.441) data 0.273 (0.310) loss_u loss_u 0.9131 (0.8492) acc_u 12.5000 (19.6250) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [55/59] time 0.358 (0.440) data 0.227 (0.309) loss_u loss_u 0.7661 (0.8434) acc_u 25.0000 (20.5682) lr 1.5358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1562
confident_label rate tensor(0.3881, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1217
clean true:1214
clean false:3
clean_rate:0.9975349219391948
noisy true:360
noisy false:1559
after delete: len(clean_dataset) 1217
after delete: len(noisy_dataset) 1919
epoch [67/200] batch [5/38] time 0.517 (0.449) data 0.387 (0.318) loss_x loss_x 1.3750 (1.2994) acc_x 56.2500 (66.2500) lr 1.5225e-03 eta 0:00:14
epoch [67/200] batch [10/38] time 0.506 (0.462) data 0.376 (0.332) loss_x loss_x 1.7471 (1.3241) acc_x 65.6250 (68.4375) lr 1.5225e-03 eta 0:00:12
epoch [67/200] batch [15/38] time 0.480 (0.448) data 0.349 (0.317) loss_x loss_x 1.1357 (1.2803) acc_x 75.0000 (68.9583) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [20/38] time 0.423 (0.445) data 0.293 (0.315) loss_x loss_x 1.6260 (1.2985) acc_x 62.5000 (67.6562) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [25/38] time 0.459 (0.448) data 0.328 (0.318) loss_x loss_x 1.3789 (1.3154) acc_x 59.3750 (66.6250) lr 1.5225e-03 eta 0:00:05
epoch [67/200] batch [30/38] time 0.427 (0.455) data 0.296 (0.325) loss_x loss_x 1.4023 (1.3306) acc_x 65.6250 (66.3542) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [35/38] time 0.425 (0.460) data 0.295 (0.329) loss_x loss_x 1.1094 (1.3249) acc_x 78.1250 (66.6071) lr 1.5225e-03 eta 0:00:01
epoch [67/200] batch [5/59] time 0.475 (0.458) data 0.344 (0.327) loss_u loss_u 0.8379 (0.8324) acc_u 18.7500 (20.6250) lr 1.5225e-03 eta 0:00:24
epoch [67/200] batch [10/59] time 0.338 (0.450) data 0.207 (0.319) loss_u loss_u 0.8550 (0.8386) acc_u 21.8750 (21.2500) lr 1.5225e-03 eta 0:00:22
epoch [67/200] batch [15/59] time 0.526 (0.447) data 0.395 (0.316) loss_u loss_u 0.8477 (0.8390) acc_u 28.1250 (21.2500) lr 1.5225e-03 eta 0:00:19
epoch [67/200] batch [20/59] time 0.355 (0.450) data 0.223 (0.319) loss_u loss_u 0.8511 (0.8364) acc_u 18.7500 (20.6250) lr 1.5225e-03 eta 0:00:17
epoch [67/200] batch [25/59] time 0.575 (0.449) data 0.444 (0.318) loss_u loss_u 0.8369 (0.8332) acc_u 18.7500 (21.3750) lr 1.5225e-03 eta 0:00:15
epoch [67/200] batch [30/59] time 0.462 (0.445) data 0.331 (0.314) loss_u loss_u 0.8325 (0.8339) acc_u 18.7500 (21.4583) lr 1.5225e-03 eta 0:00:12
epoch [67/200] batch [35/59] time 0.491 (0.449) data 0.360 (0.318) loss_u loss_u 0.8608 (0.8350) acc_u 21.8750 (21.5179) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [40/59] time 0.350 (0.446) data 0.219 (0.315) loss_u loss_u 0.9067 (0.8369) acc_u 15.6250 (21.4062) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [45/59] time 0.598 (0.446) data 0.467 (0.315) loss_u loss_u 0.8032 (0.8331) acc_u 25.0000 (21.8750) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [50/59] time 0.360 (0.448) data 0.229 (0.317) loss_u loss_u 0.8306 (0.8326) acc_u 18.7500 (21.9375) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [55/59] time 0.396 (0.446) data 0.266 (0.315) loss_u loss_u 0.7959 (0.8361) acc_u 25.0000 (21.3636) lr 1.5225e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1562
confident_label rate tensor(0.3951, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1239
clean true:1236
clean false:3
clean_rate:0.9975786924939467
noisy true:338
noisy false:1559
after delete: len(clean_dataset) 1239
after delete: len(noisy_dataset) 1897
epoch [68/200] batch [5/38] time 0.462 (0.463) data 0.332 (0.333) loss_x loss_x 1.0254 (1.1885) acc_x 75.0000 (66.8750) lr 1.5090e-03 eta 0:00:15
epoch [68/200] batch [10/38] time 0.569 (0.455) data 0.440 (0.325) loss_x loss_x 1.2939 (1.2788) acc_x 75.0000 (68.4375) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [15/38] time 0.661 (0.456) data 0.531 (0.326) loss_x loss_x 0.8047 (1.2370) acc_x 78.1250 (68.9583) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [20/38] time 0.412 (0.447) data 0.281 (0.317) loss_x loss_x 1.0801 (1.2130) acc_x 75.0000 (70.3125) lr 1.5090e-03 eta 0:00:08
epoch [68/200] batch [25/38] time 0.560 (0.443) data 0.430 (0.313) loss_x loss_x 1.2090 (1.2072) acc_x 71.8750 (70.2500) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [30/38] time 0.416 (0.446) data 0.285 (0.316) loss_x loss_x 1.9189 (1.2821) acc_x 46.8750 (68.5417) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [35/38] time 0.451 (0.449) data 0.320 (0.319) loss_x loss_x 0.9780 (1.2505) acc_x 71.8750 (69.1964) lr 1.5090e-03 eta 0:00:01
epoch [68/200] batch [5/59] time 0.439 (0.448) data 0.308 (0.317) loss_u loss_u 0.8921 (0.8378) acc_u 15.6250 (21.8750) lr 1.5090e-03 eta 0:00:24
epoch [68/200] batch [10/59] time 0.347 (0.441) data 0.217 (0.310) loss_u loss_u 0.8301 (0.8315) acc_u 21.8750 (21.5625) lr 1.5090e-03 eta 0:00:21
epoch [68/200] batch [15/59] time 0.397 (0.440) data 0.266 (0.309) loss_u loss_u 0.8926 (0.8367) acc_u 15.6250 (21.4583) lr 1.5090e-03 eta 0:00:19
epoch [68/200] batch [20/59] time 0.428 (0.439) data 0.297 (0.309) loss_u loss_u 0.9497 (0.8425) acc_u 0.0000 (20.7812) lr 1.5090e-03 eta 0:00:17
epoch [68/200] batch [25/59] time 0.393 (0.437) data 0.262 (0.306) loss_u loss_u 0.8589 (0.8415) acc_u 15.6250 (20.5000) lr 1.5090e-03 eta 0:00:14
epoch [68/200] batch [30/59] time 0.486 (0.436) data 0.356 (0.306) loss_u loss_u 0.7905 (0.8376) acc_u 21.8750 (20.7292) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [35/59] time 0.452 (0.434) data 0.321 (0.304) loss_u loss_u 0.8867 (0.8420) acc_u 15.6250 (20.2679) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [40/59] time 0.442 (0.436) data 0.311 (0.306) loss_u loss_u 0.8623 (0.8397) acc_u 18.7500 (20.7812) lr 1.5090e-03 eta 0:00:08
epoch [68/200] batch [45/59] time 0.402 (0.439) data 0.270 (0.309) loss_u loss_u 0.9092 (0.8422) acc_u 12.5000 (20.6944) lr 1.5090e-03 eta 0:00:06
epoch [68/200] batch [50/59] time 0.509 (0.439) data 0.372 (0.308) loss_u loss_u 0.8477 (0.8454) acc_u 21.8750 (20.4375) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [55/59] time 0.466 (0.438) data 0.335 (0.307) loss_u loss_u 0.8198 (0.8472) acc_u 25.0000 (20.1705) lr 1.5090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1546
confident_label rate tensor(0.3948, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1238
clean true:1237
clean false:1
clean_rate:0.9991922455573505
noisy true:353
noisy false:1545
after delete: len(clean_dataset) 1238
after delete: len(noisy_dataset) 1898
epoch [69/200] batch [5/38] time 0.478 (0.456) data 0.347 (0.325) loss_x loss_x 1.4473 (1.1698) acc_x 59.3750 (69.3750) lr 1.4955e-03 eta 0:00:15
epoch [69/200] batch [10/38] time 0.452 (0.457) data 0.322 (0.327) loss_x loss_x 1.2559 (1.2938) acc_x 68.7500 (67.1875) lr 1.4955e-03 eta 0:00:12
epoch [69/200] batch [15/38] time 0.382 (0.454) data 0.251 (0.323) loss_x loss_x 1.4688 (1.2506) acc_x 68.7500 (67.9167) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [20/38] time 0.479 (0.456) data 0.348 (0.325) loss_x loss_x 1.1602 (1.2519) acc_x 65.6250 (67.8125) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [25/38] time 0.423 (0.448) data 0.292 (0.317) loss_x loss_x 1.9697 (1.2425) acc_x 56.2500 (68.3750) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [30/38] time 0.521 (0.451) data 0.390 (0.320) loss_x loss_x 1.3262 (1.2490) acc_x 68.7500 (68.2292) lr 1.4955e-03 eta 0:00:03
epoch [69/200] batch [35/38] time 0.418 (0.445) data 0.288 (0.314) loss_x loss_x 1.4033 (1.2408) acc_x 68.7500 (68.8393) lr 1.4955e-03 eta 0:00:01
epoch [69/200] batch [5/59] time 0.364 (0.440) data 0.233 (0.309) loss_u loss_u 0.7373 (0.8299) acc_u 37.5000 (24.3750) lr 1.4955e-03 eta 0:00:23
epoch [69/200] batch [10/59] time 0.478 (0.449) data 0.345 (0.318) loss_u loss_u 0.8984 (0.8403) acc_u 9.3750 (21.2500) lr 1.4955e-03 eta 0:00:21
epoch [69/200] batch [15/59] time 0.474 (0.450) data 0.343 (0.319) loss_u loss_u 0.9072 (0.8388) acc_u 12.5000 (21.2500) lr 1.4955e-03 eta 0:00:19
epoch [69/200] batch [20/59] time 0.429 (0.448) data 0.298 (0.318) loss_u loss_u 0.7720 (0.8359) acc_u 28.1250 (21.0938) lr 1.4955e-03 eta 0:00:17
epoch [69/200] batch [25/59] time 0.499 (0.450) data 0.367 (0.319) loss_u loss_u 0.8325 (0.8375) acc_u 18.7500 (20.7500) lr 1.4955e-03 eta 0:00:15
epoch [69/200] batch [30/59] time 0.476 (0.451) data 0.345 (0.320) loss_u loss_u 0.8633 (0.8380) acc_u 18.7500 (20.9375) lr 1.4955e-03 eta 0:00:13
epoch [69/200] batch [35/59] time 0.360 (0.452) data 0.229 (0.321) loss_u loss_u 0.8159 (0.8368) acc_u 21.8750 (20.9821) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [40/59] time 0.518 (0.452) data 0.382 (0.321) loss_u loss_u 0.8374 (0.8374) acc_u 18.7500 (21.0156) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [45/59] time 0.464 (0.451) data 0.332 (0.320) loss_u loss_u 0.7129 (0.8343) acc_u 34.3750 (21.3889) lr 1.4955e-03 eta 0:00:06
epoch [69/200] batch [50/59] time 0.336 (0.448) data 0.206 (0.317) loss_u loss_u 0.9048 (0.8378) acc_u 12.5000 (20.8750) lr 1.4955e-03 eta 0:00:04
epoch [69/200] batch [55/59] time 0.506 (0.447) data 0.374 (0.316) loss_u loss_u 0.8096 (0.8395) acc_u 21.8750 (20.7955) lr 1.4955e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1544
confident_label rate tensor(0.4011, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1258
clean true:1256
clean false:2
clean_rate:0.9984101748807631
noisy true:336
noisy false:1542
after delete: len(clean_dataset) 1258
after delete: len(noisy_dataset) 1878
epoch [70/200] batch [5/39] time 0.417 (0.422) data 0.286 (0.290) loss_x loss_x 0.8242 (1.3465) acc_x 84.3750 (69.3750) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [10/39] time 0.426 (0.429) data 0.296 (0.298) loss_x loss_x 1.2930 (1.2628) acc_x 75.0000 (70.0000) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [15/39] time 0.491 (0.451) data 0.361 (0.320) loss_x loss_x 0.7603 (1.2355) acc_x 87.5000 (69.5833) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [20/39] time 0.426 (0.450) data 0.295 (0.319) loss_x loss_x 1.1777 (1.2765) acc_x 62.5000 (67.3438) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [25/39] time 0.487 (0.458) data 0.355 (0.327) loss_x loss_x 1.0020 (1.2272) acc_x 65.6250 (67.5000) lr 1.4818e-03 eta 0:00:06
epoch [70/200] batch [30/39] time 0.520 (0.466) data 0.390 (0.335) loss_x loss_x 1.5430 (1.2343) acc_x 59.3750 (68.0208) lr 1.4818e-03 eta 0:00:04
epoch [70/200] batch [35/39] time 0.409 (0.465) data 0.278 (0.334) loss_x loss_x 1.2666 (1.2077) acc_x 68.7500 (68.8393) lr 1.4818e-03 eta 0:00:01
epoch [70/200] batch [5/58] time 0.425 (0.461) data 0.294 (0.330) loss_u loss_u 0.7788 (0.8792) acc_u 31.2500 (17.5000) lr 1.4818e-03 eta 0:00:24
epoch [70/200] batch [10/58] time 0.353 (0.459) data 0.221 (0.329) loss_u loss_u 0.8716 (0.8650) acc_u 21.8750 (20.3125) lr 1.4818e-03 eta 0:00:22
epoch [70/200] batch [15/58] time 0.457 (0.456) data 0.326 (0.325) loss_u loss_u 0.8237 (0.8480) acc_u 28.1250 (21.8750) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [20/58] time 0.507 (0.454) data 0.375 (0.323) loss_u loss_u 0.7930 (0.8492) acc_u 28.1250 (21.0938) lr 1.4818e-03 eta 0:00:17
epoch [70/200] batch [25/58] time 0.546 (0.453) data 0.414 (0.322) loss_u loss_u 0.8364 (0.8500) acc_u 18.7500 (20.1250) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [30/58] time 0.417 (0.453) data 0.286 (0.322) loss_u loss_u 0.8687 (0.8492) acc_u 15.6250 (20.4167) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [35/58] time 0.452 (0.450) data 0.321 (0.319) loss_u loss_u 0.8936 (0.8495) acc_u 9.3750 (20.1786) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [40/58] time 0.366 (0.449) data 0.235 (0.318) loss_u loss_u 0.8589 (0.8490) acc_u 15.6250 (20.0000) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [45/58] time 0.451 (0.446) data 0.320 (0.315) loss_u loss_u 0.8989 (0.8509) acc_u 6.2500 (19.5139) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [50/58] time 0.374 (0.443) data 0.243 (0.312) loss_u loss_u 0.8296 (0.8513) acc_u 21.8750 (19.4375) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [55/58] time 0.366 (0.441) data 0.234 (0.309) loss_u loss_u 0.7222 (0.8479) acc_u 34.3750 (19.8864) lr 1.4818e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1572
confident_label rate tensor(0.3938, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1235
clean true:1231
clean false:4
clean_rate:0.9967611336032388
noisy true:333
noisy false:1568
after delete: len(clean_dataset) 1235
after delete: len(noisy_dataset) 1901
epoch [71/200] batch [5/38] time 0.388 (0.459) data 0.258 (0.328) loss_x loss_x 1.5195 (1.3611) acc_x 59.3750 (64.3750) lr 1.4679e-03 eta 0:00:15
epoch [71/200] batch [10/38] time 0.491 (0.453) data 0.360 (0.322) loss_x loss_x 1.2969 (1.3694) acc_x 59.3750 (64.6875) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [15/38] time 0.491 (0.454) data 0.360 (0.324) loss_x loss_x 1.8184 (1.3924) acc_x 65.6250 (66.0417) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [20/38] time 0.544 (0.461) data 0.414 (0.330) loss_x loss_x 1.7979 (1.4109) acc_x 56.2500 (65.0000) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [25/38] time 0.459 (0.454) data 0.329 (0.324) loss_x loss_x 0.9961 (1.3612) acc_x 78.1250 (66.3750) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [30/38] time 0.384 (0.445) data 0.253 (0.315) loss_x loss_x 0.7803 (1.3408) acc_x 78.1250 (66.9792) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [35/38] time 0.404 (0.444) data 0.273 (0.314) loss_x loss_x 1.1055 (1.3279) acc_x 75.0000 (67.2321) lr 1.4679e-03 eta 0:00:01
epoch [71/200] batch [5/59] time 0.477 (0.441) data 0.346 (0.310) loss_u loss_u 0.8296 (0.8629) acc_u 21.8750 (20.0000) lr 1.4679e-03 eta 0:00:23
epoch [71/200] batch [10/59] time 0.496 (0.446) data 0.365 (0.316) loss_u loss_u 0.8574 (0.8477) acc_u 18.7500 (20.3125) lr 1.4679e-03 eta 0:00:21
epoch [71/200] batch [15/59] time 0.444 (0.447) data 0.312 (0.317) loss_u loss_u 0.8452 (0.8394) acc_u 25.0000 (21.0417) lr 1.4679e-03 eta 0:00:19
epoch [71/200] batch [20/59] time 0.400 (0.445) data 0.269 (0.314) loss_u loss_u 0.8013 (0.8330) acc_u 28.1250 (22.1875) lr 1.4679e-03 eta 0:00:17
epoch [71/200] batch [25/59] time 0.348 (0.444) data 0.217 (0.313) loss_u loss_u 0.7925 (0.8310) acc_u 21.8750 (22.1250) lr 1.4679e-03 eta 0:00:15
epoch [71/200] batch [30/59] time 0.369 (0.444) data 0.237 (0.313) loss_u loss_u 0.8096 (0.8299) acc_u 21.8750 (22.0833) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [35/59] time 0.422 (0.446) data 0.291 (0.315) loss_u loss_u 0.8799 (0.8295) acc_u 9.3750 (22.2321) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [40/59] time 0.440 (0.450) data 0.308 (0.320) loss_u loss_u 0.8018 (0.8307) acc_u 18.7500 (21.9531) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [45/59] time 0.332 (0.452) data 0.200 (0.321) loss_u loss_u 0.7773 (0.8260) acc_u 31.2500 (22.6389) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [50/59] time 0.397 (0.450) data 0.265 (0.319) loss_u loss_u 0.8740 (0.8289) acc_u 15.6250 (22.3125) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [55/59] time 0.497 (0.449) data 0.365 (0.318) loss_u loss_u 0.8306 (0.8329) acc_u 21.8750 (21.7045) lr 1.4679e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1548
confident_label rate tensor(0.4027, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1263
clean true:1261
clean false:2
clean_rate:0.9984164687252574
noisy true:327
noisy false:1546
after delete: len(clean_dataset) 1263
after delete: len(noisy_dataset) 1873
epoch [72/200] batch [5/39] time 0.422 (0.463) data 0.292 (0.332) loss_x loss_x 0.8540 (1.2159) acc_x 71.8750 (67.5000) lr 1.4540e-03 eta 0:00:15
epoch [72/200] batch [10/39] time 0.416 (0.480) data 0.286 (0.349) loss_x loss_x 1.9775 (1.3276) acc_x 62.5000 (66.2500) lr 1.4540e-03 eta 0:00:13
epoch [72/200] batch [15/39] time 0.390 (0.479) data 0.259 (0.348) loss_x loss_x 0.8091 (1.2036) acc_x 81.2500 (69.1667) lr 1.4540e-03 eta 0:00:11
epoch [72/200] batch [20/39] time 0.512 (0.471) data 0.382 (0.340) loss_x loss_x 0.9937 (1.2102) acc_x 75.0000 (68.2812) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [25/39] time 0.591 (0.467) data 0.460 (0.336) loss_x loss_x 1.4355 (1.2471) acc_x 56.2500 (67.2500) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [30/39] time 0.378 (0.460) data 0.248 (0.330) loss_x loss_x 1.3711 (1.2203) acc_x 59.3750 (68.0208) lr 1.4540e-03 eta 0:00:04
epoch [72/200] batch [35/39] time 0.422 (0.455) data 0.293 (0.325) loss_x loss_x 1.5391 (1.2371) acc_x 68.7500 (68.3929) lr 1.4540e-03 eta 0:00:01
epoch [72/200] batch [5/58] time 0.442 (0.451) data 0.311 (0.320) loss_u loss_u 0.8789 (0.8475) acc_u 21.8750 (19.3750) lr 1.4540e-03 eta 0:00:23
epoch [72/200] batch [10/58] time 0.376 (0.446) data 0.245 (0.315) loss_u loss_u 0.8647 (0.8543) acc_u 18.7500 (18.7500) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [15/58] time 0.352 (0.445) data 0.221 (0.314) loss_u loss_u 0.8428 (0.8479) acc_u 18.7500 (19.5833) lr 1.4540e-03 eta 0:00:19
epoch [72/200] batch [20/58] time 0.353 (0.444) data 0.222 (0.313) loss_u loss_u 0.8687 (0.8464) acc_u 15.6250 (20.3125) lr 1.4540e-03 eta 0:00:16
epoch [72/200] batch [25/58] time 0.488 (0.442) data 0.358 (0.311) loss_u loss_u 0.7495 (0.8408) acc_u 31.2500 (21.2500) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [30/58] time 0.529 (0.438) data 0.399 (0.308) loss_u loss_u 0.8105 (0.8364) acc_u 25.0000 (21.8750) lr 1.4540e-03 eta 0:00:12
epoch [72/200] batch [35/58] time 0.386 (0.437) data 0.255 (0.306) loss_u loss_u 0.8501 (0.8357) acc_u 15.6250 (21.8750) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [40/58] time 0.429 (0.437) data 0.299 (0.306) loss_u loss_u 0.7944 (0.8368) acc_u 31.2500 (21.7188) lr 1.4540e-03 eta 0:00:07
epoch [72/200] batch [45/58] time 0.555 (0.439) data 0.421 (0.308) loss_u loss_u 0.8994 (0.8382) acc_u 12.5000 (21.5278) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [50/58] time 0.507 (0.439) data 0.376 (0.308) loss_u loss_u 0.8286 (0.8380) acc_u 15.6250 (21.4375) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [55/58] time 0.442 (0.443) data 0.312 (0.312) loss_u loss_u 0.9146 (0.8369) acc_u 15.6250 (21.8750) lr 1.4540e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1528
confident_label rate tensor(0.4037, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1266
clean true:1264
clean false:2
clean_rate:0.9984202211690363
noisy true:344
noisy false:1526
after delete: len(clean_dataset) 1266
after delete: len(noisy_dataset) 1870
epoch [73/200] batch [5/39] time 0.513 (0.488) data 0.383 (0.358) loss_x loss_x 1.2559 (1.2070) acc_x 65.6250 (71.2500) lr 1.4399e-03 eta 0:00:16
epoch [73/200] batch [10/39] time 0.574 (0.466) data 0.444 (0.335) loss_x loss_x 1.4502 (1.2353) acc_x 59.3750 (68.1250) lr 1.4399e-03 eta 0:00:13
epoch [73/200] batch [15/39] time 0.518 (0.462) data 0.388 (0.331) loss_x loss_x 1.3457 (1.2693) acc_x 68.7500 (66.6667) lr 1.4399e-03 eta 0:00:11
epoch [73/200] batch [20/39] time 0.406 (0.458) data 0.275 (0.328) loss_x loss_x 0.8896 (1.2386) acc_x 81.2500 (67.3438) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [25/39] time 0.351 (0.452) data 0.221 (0.321) loss_x loss_x 1.7393 (1.3257) acc_x 53.1250 (66.3750) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [30/39] time 0.394 (0.455) data 0.263 (0.324) loss_x loss_x 1.1035 (1.3324) acc_x 71.8750 (66.2500) lr 1.4399e-03 eta 0:00:04
epoch [73/200] batch [35/39] time 0.568 (0.454) data 0.439 (0.324) loss_x loss_x 1.2266 (1.3120) acc_x 71.8750 (66.8750) lr 1.4399e-03 eta 0:00:01
epoch [73/200] batch [5/58] time 0.341 (0.456) data 0.210 (0.326) loss_u loss_u 0.8931 (0.8779) acc_u 12.5000 (15.6250) lr 1.4399e-03 eta 0:00:24
epoch [73/200] batch [10/58] time 0.611 (0.460) data 0.478 (0.329) loss_u loss_u 0.8628 (0.8671) acc_u 6.2500 (15.6250) lr 1.4399e-03 eta 0:00:22
epoch [73/200] batch [15/58] time 0.479 (0.458) data 0.346 (0.328) loss_u loss_u 0.8818 (0.8686) acc_u 18.7500 (15.6250) lr 1.4399e-03 eta 0:00:19
epoch [73/200] batch [20/58] time 0.372 (0.460) data 0.240 (0.329) loss_u loss_u 0.8359 (0.8530) acc_u 25.0000 (18.2812) lr 1.4399e-03 eta 0:00:17
epoch [73/200] batch [25/58] time 0.425 (0.461) data 0.292 (0.330) loss_u loss_u 0.8228 (0.8518) acc_u 25.0000 (18.8750) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [30/58] time 0.543 (0.460) data 0.411 (0.328) loss_u loss_u 0.7456 (0.8480) acc_u 25.0000 (18.7500) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [35/58] time 0.531 (0.459) data 0.399 (0.328) loss_u loss_u 0.8013 (0.8482) acc_u 25.0000 (18.9286) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [40/58] time 0.557 (0.460) data 0.424 (0.328) loss_u loss_u 0.8408 (0.8489) acc_u 18.7500 (18.7500) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [45/58] time 0.460 (0.461) data 0.328 (0.330) loss_u loss_u 0.7627 (0.8494) acc_u 34.3750 (18.9583) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [50/58] time 0.460 (0.458) data 0.329 (0.327) loss_u loss_u 0.8926 (0.8509) acc_u 9.3750 (18.6875) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [55/58] time 0.543 (0.461) data 0.411 (0.329) loss_u loss_u 0.8931 (0.8502) acc_u 18.7500 (19.1477) lr 1.4399e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1540
confident_label rate tensor(0.3957, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1241
clean true:1239
clean false:2
clean_rate:0.9983883964544722
noisy true:357
noisy false:1538
after delete: len(clean_dataset) 1241
after delete: len(noisy_dataset) 1895
epoch [74/200] batch [5/38] time 0.537 (0.502) data 0.405 (0.371) loss_x loss_x 1.2578 (1.2549) acc_x 56.2500 (68.7500) lr 1.4258e-03 eta 0:00:16
epoch [74/200] batch [10/38] time 0.489 (0.477) data 0.357 (0.345) loss_x loss_x 1.2295 (1.2293) acc_x 78.1250 (69.3750) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [15/38] time 0.466 (0.464) data 0.334 (0.332) loss_x loss_x 1.3203 (1.2314) acc_x 68.7500 (69.3750) lr 1.4258e-03 eta 0:00:10
epoch [74/200] batch [20/38] time 0.466 (0.469) data 0.334 (0.337) loss_x loss_x 1.0723 (1.2034) acc_x 65.6250 (70.6250) lr 1.4258e-03 eta 0:00:08
epoch [74/200] batch [25/38] time 0.442 (0.471) data 0.311 (0.339) loss_x loss_x 1.4023 (1.2394) acc_x 71.8750 (69.8750) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [30/38] time 0.509 (0.467) data 0.378 (0.336) loss_x loss_x 1.1875 (1.2150) acc_x 68.7500 (70.7292) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [35/38] time 0.403 (0.461) data 0.272 (0.330) loss_x loss_x 1.4180 (1.2492) acc_x 65.6250 (69.8214) lr 1.4258e-03 eta 0:00:01
epoch [74/200] batch [5/59] time 0.365 (0.462) data 0.233 (0.331) loss_u loss_u 0.8052 (0.8365) acc_u 21.8750 (22.5000) lr 1.4258e-03 eta 0:00:24
epoch [74/200] batch [10/59] time 0.470 (0.465) data 0.339 (0.334) loss_u loss_u 0.7871 (0.8361) acc_u 28.1250 (22.1875) lr 1.4258e-03 eta 0:00:22
epoch [74/200] batch [15/59] time 0.431 (0.458) data 0.300 (0.326) loss_u loss_u 0.7700 (0.8389) acc_u 31.2500 (21.4583) lr 1.4258e-03 eta 0:00:20
epoch [74/200] batch [20/59] time 0.342 (0.461) data 0.211 (0.329) loss_u loss_u 0.8931 (0.8451) acc_u 18.7500 (21.0938) lr 1.4258e-03 eta 0:00:17
epoch [74/200] batch [25/59] time 0.462 (0.459) data 0.331 (0.327) loss_u loss_u 0.9204 (0.8508) acc_u 9.3750 (19.6250) lr 1.4258e-03 eta 0:00:15
epoch [74/200] batch [30/59] time 0.387 (0.455) data 0.256 (0.323) loss_u loss_u 0.7725 (0.8468) acc_u 31.2500 (20.3125) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [35/59] time 0.461 (0.454) data 0.330 (0.322) loss_u loss_u 0.8325 (0.8399) acc_u 18.7500 (21.1607) lr 1.4258e-03 eta 0:00:10
epoch [74/200] batch [40/59] time 0.462 (0.451) data 0.330 (0.320) loss_u loss_u 0.8120 (0.8365) acc_u 28.1250 (21.4844) lr 1.4258e-03 eta 0:00:08
epoch [74/200] batch [45/59] time 0.458 (0.452) data 0.326 (0.321) loss_u loss_u 0.8262 (0.8335) acc_u 21.8750 (21.8056) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [50/59] time 0.610 (0.451) data 0.479 (0.320) loss_u loss_u 0.7891 (0.8346) acc_u 25.0000 (21.5625) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [55/59] time 0.399 (0.450) data 0.267 (0.318) loss_u loss_u 0.8184 (0.8346) acc_u 18.7500 (21.5341) lr 1.4258e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1546
confident_label rate tensor(0.3983, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1249
clean true:1246
clean false:3
clean_rate:0.9975980784627703
noisy true:344
noisy false:1543
after delete: len(clean_dataset) 1249
after delete: len(noisy_dataset) 1887
epoch [75/200] batch [5/39] time 0.419 (0.421) data 0.289 (0.290) loss_x loss_x 0.9878 (1.1961) acc_x 75.0000 (71.2500) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [10/39] time 0.403 (0.430) data 0.273 (0.299) loss_x loss_x 2.2051 (1.3196) acc_x 50.0000 (67.1875) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [15/39] time 0.407 (0.432) data 0.276 (0.301) loss_x loss_x 1.4121 (1.3601) acc_x 68.7500 (66.8750) lr 1.4115e-03 eta 0:00:10
epoch [75/200] batch [20/39] time 0.460 (0.436) data 0.330 (0.306) loss_x loss_x 1.0049 (1.3242) acc_x 75.0000 (67.5000) lr 1.4115e-03 eta 0:00:08
epoch [75/200] batch [25/39] time 0.426 (0.430) data 0.295 (0.299) loss_x loss_x 1.3945 (1.3065) acc_x 75.0000 (68.1250) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [30/39] time 0.552 (0.439) data 0.421 (0.308) loss_x loss_x 1.3584 (1.2821) acc_x 71.8750 (68.9583) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [35/39] time 0.469 (0.432) data 0.338 (0.301) loss_x loss_x 1.1387 (1.2522) acc_x 68.7500 (69.5536) lr 1.4115e-03 eta 0:00:01
epoch [75/200] batch [5/58] time 0.472 (0.434) data 0.341 (0.303) loss_u loss_u 0.7524 (0.8123) acc_u 34.3750 (23.7500) lr 1.4115e-03 eta 0:00:22
epoch [75/200] batch [10/58] time 0.458 (0.434) data 0.327 (0.304) loss_u loss_u 0.7788 (0.8229) acc_u 31.2500 (22.8125) lr 1.4115e-03 eta 0:00:20
epoch [75/200] batch [15/58] time 0.428 (0.430) data 0.296 (0.300) loss_u loss_u 0.8838 (0.8244) acc_u 12.5000 (22.0833) lr 1.4115e-03 eta 0:00:18
epoch [75/200] batch [20/58] time 0.462 (0.438) data 0.331 (0.307) loss_u loss_u 0.8979 (0.8329) acc_u 12.5000 (21.2500) lr 1.4115e-03 eta 0:00:16
epoch [75/200] batch [25/58] time 0.423 (0.438) data 0.291 (0.307) loss_u loss_u 0.8960 (0.8353) acc_u 12.5000 (21.2500) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [30/58] time 0.340 (0.437) data 0.208 (0.306) loss_u loss_u 0.8418 (0.8356) acc_u 21.8750 (21.0417) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [35/58] time 0.477 (0.440) data 0.345 (0.309) loss_u loss_u 0.8413 (0.8351) acc_u 18.7500 (21.3393) lr 1.4115e-03 eta 0:00:10
epoch [75/200] batch [40/58] time 0.354 (0.438) data 0.221 (0.307) loss_u loss_u 0.8398 (0.8364) acc_u 25.0000 (21.2500) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [45/58] time 0.412 (0.442) data 0.280 (0.311) loss_u loss_u 0.8584 (0.8418) acc_u 21.8750 (20.6944) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [50/58] time 0.376 (0.444) data 0.245 (0.313) loss_u loss_u 0.8418 (0.8422) acc_u 21.8750 (20.4375) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [55/58] time 0.429 (0.443) data 0.298 (0.311) loss_u loss_u 0.9180 (0.8410) acc_u 9.3750 (20.4545) lr 1.4115e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1551
confident_label rate tensor(0.4002, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1255
clean true:1251
clean false:4
clean_rate:0.9968127490039841
noisy true:334
noisy false:1547
after delete: len(clean_dataset) 1255
after delete: len(noisy_dataset) 1881
epoch [76/200] batch [5/39] time 0.378 (0.438) data 0.248 (0.308) loss_x loss_x 1.0000 (1.4752) acc_x 68.7500 (62.5000) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [10/39] time 0.361 (0.451) data 0.231 (0.320) loss_x loss_x 1.1699 (1.2885) acc_x 71.8750 (66.5625) lr 1.3971e-03 eta 0:00:13
epoch [76/200] batch [15/39] time 0.448 (0.451) data 0.318 (0.320) loss_x loss_x 1.1084 (1.2707) acc_x 65.6250 (66.6667) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [20/39] time 0.422 (0.446) data 0.292 (0.315) loss_x loss_x 0.7930 (1.2365) acc_x 81.2500 (67.0312) lr 1.3971e-03 eta 0:00:08
epoch [76/200] batch [25/39] time 0.433 (0.444) data 0.303 (0.314) loss_x loss_x 1.3242 (1.2641) acc_x 71.8750 (66.5000) lr 1.3971e-03 eta 0:00:06
epoch [76/200] batch [30/39] time 0.579 (0.441) data 0.449 (0.311) loss_x loss_x 0.9829 (1.2909) acc_x 68.7500 (66.1458) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [35/39] time 0.527 (0.449) data 0.392 (0.318) loss_x loss_x 1.1201 (1.2972) acc_x 68.7500 (66.0714) lr 1.3971e-03 eta 0:00:01
epoch [76/200] batch [5/58] time 0.356 (0.441) data 0.224 (0.310) loss_u loss_u 0.8125 (0.8582) acc_u 21.8750 (16.8750) lr 1.3971e-03 eta 0:00:23
epoch [76/200] batch [10/58] time 0.466 (0.441) data 0.334 (0.310) loss_u loss_u 0.8403 (0.8495) acc_u 28.1250 (20.9375) lr 1.3971e-03 eta 0:00:21
epoch [76/200] batch [15/58] time 0.415 (0.446) data 0.284 (0.315) loss_u loss_u 0.8403 (0.8400) acc_u 15.6250 (21.8750) lr 1.3971e-03 eta 0:00:19
epoch [76/200] batch [20/58] time 0.344 (0.440) data 0.213 (0.308) loss_u loss_u 0.8682 (0.8489) acc_u 15.6250 (20.3125) lr 1.3971e-03 eta 0:00:16
epoch [76/200] batch [25/58] time 0.434 (0.440) data 0.302 (0.309) loss_u loss_u 0.8462 (0.8436) acc_u 21.8750 (21.0000) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [30/58] time 0.394 (0.439) data 0.263 (0.308) loss_u loss_u 0.8462 (0.8430) acc_u 21.8750 (21.5625) lr 1.3971e-03 eta 0:00:12
epoch [76/200] batch [35/58] time 0.343 (0.438) data 0.211 (0.307) loss_u loss_u 0.8228 (0.8386) acc_u 21.8750 (21.8750) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [40/58] time 0.541 (0.441) data 0.410 (0.310) loss_u loss_u 0.7656 (0.8369) acc_u 37.5000 (22.0312) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [45/58] time 0.454 (0.441) data 0.323 (0.310) loss_u loss_u 0.8042 (0.8364) acc_u 21.8750 (21.8750) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [50/58] time 0.613 (0.442) data 0.483 (0.311) loss_u loss_u 0.7925 (0.8349) acc_u 28.1250 (22.2500) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [55/58] time 0.454 (0.445) data 0.323 (0.314) loss_u loss_u 0.8618 (0.8370) acc_u 15.6250 (21.8182) lr 1.3971e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1561
confident_label rate tensor(0.3976, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1247
clean true:1245
clean false:2
clean_rate:0.9983961507618284
noisy true:330
noisy false:1559
after delete: len(clean_dataset) 1247
after delete: len(noisy_dataset) 1889
epoch [77/200] batch [5/38] time 0.439 (0.439) data 0.309 (0.309) loss_x loss_x 1.0674 (1.2626) acc_x 62.5000 (68.7500) lr 1.3827e-03 eta 0:00:14
epoch [77/200] batch [10/38] time 0.593 (0.477) data 0.462 (0.347) loss_x loss_x 1.0723 (1.1886) acc_x 68.7500 (68.7500) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [15/38] time 0.364 (0.467) data 0.233 (0.336) loss_x loss_x 1.4951 (1.2342) acc_x 68.7500 (68.9583) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [20/38] time 0.527 (0.455) data 0.396 (0.324) loss_x loss_x 1.5303 (1.2918) acc_x 71.8750 (68.5938) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [25/38] time 0.516 (0.458) data 0.383 (0.327) loss_x loss_x 1.1670 (1.2913) acc_x 65.6250 (68.1250) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [30/38] time 0.513 (0.463) data 0.381 (0.332) loss_x loss_x 1.5547 (1.3063) acc_x 68.7500 (67.7083) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [35/38] time 0.606 (0.470) data 0.474 (0.339) loss_x loss_x 2.2500 (1.3533) acc_x 43.7500 (66.7857) lr 1.3827e-03 eta 0:00:01
epoch [77/200] batch [5/59] time 0.474 (0.464) data 0.343 (0.333) loss_u loss_u 0.7710 (0.8444) acc_u 28.1250 (20.0000) lr 1.3827e-03 eta 0:00:25
epoch [77/200] batch [10/59] time 0.369 (0.462) data 0.238 (0.331) loss_u loss_u 0.8691 (0.8466) acc_u 18.7500 (20.0000) lr 1.3827e-03 eta 0:00:22
epoch [77/200] batch [15/59] time 0.361 (0.454) data 0.230 (0.323) loss_u loss_u 0.8672 (0.8494) acc_u 15.6250 (19.1667) lr 1.3827e-03 eta 0:00:19
epoch [77/200] batch [20/59] time 0.510 (0.455) data 0.378 (0.324) loss_u loss_u 0.8672 (0.8492) acc_u 25.0000 (19.3750) lr 1.3827e-03 eta 0:00:17
epoch [77/200] batch [25/59] time 0.408 (0.456) data 0.277 (0.325) loss_u loss_u 0.9121 (0.8518) acc_u 6.2500 (18.6250) lr 1.3827e-03 eta 0:00:15
epoch [77/200] batch [30/59] time 0.492 (0.459) data 0.359 (0.328) loss_u loss_u 0.8945 (0.8482) acc_u 15.6250 (18.9583) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [35/59] time 0.510 (0.458) data 0.379 (0.327) loss_u loss_u 0.8359 (0.8432) acc_u 18.7500 (19.6429) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [40/59] time 0.434 (0.457) data 0.302 (0.325) loss_u loss_u 0.8384 (0.8416) acc_u 25.0000 (20.3125) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [45/59] time 0.391 (0.454) data 0.259 (0.323) loss_u loss_u 0.8633 (0.8416) acc_u 18.7500 (20.2083) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [50/59] time 0.394 (0.452) data 0.264 (0.321) loss_u loss_u 0.8672 (0.8398) acc_u 12.5000 (20.5000) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [55/59] time 0.369 (0.452) data 0.238 (0.320) loss_u loss_u 0.7939 (0.8387) acc_u 21.8750 (20.3409) lr 1.3827e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1540
confident_label rate tensor(0.4069, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1276
clean true:1275
clean false:1
clean_rate:0.9992163009404389
noisy true:321
noisy false:1539
after delete: len(clean_dataset) 1276
after delete: len(noisy_dataset) 1860
epoch [78/200] batch [5/39] time 0.346 (0.417) data 0.216 (0.286) loss_x loss_x 1.2070 (1.2168) acc_x 68.7500 (70.6250) lr 1.3681e-03 eta 0:00:14
epoch [78/200] batch [10/39] time 0.437 (0.435) data 0.306 (0.305) loss_x loss_x 1.4609 (1.2617) acc_x 62.5000 (70.0000) lr 1.3681e-03 eta 0:00:12
epoch [78/200] batch [15/39] time 0.435 (0.447) data 0.304 (0.317) loss_x loss_x 0.9438 (1.2129) acc_x 68.7500 (69.5833) lr 1.3681e-03 eta 0:00:10
epoch [78/200] batch [20/39] time 0.586 (0.451) data 0.455 (0.320) loss_x loss_x 0.9512 (1.2446) acc_x 71.8750 (68.2812) lr 1.3681e-03 eta 0:00:08
epoch [78/200] batch [25/39] time 0.379 (0.457) data 0.248 (0.326) loss_x loss_x 1.4688 (1.2897) acc_x 53.1250 (66.8750) lr 1.3681e-03 eta 0:00:06
epoch [78/200] batch [30/39] time 0.482 (0.467) data 0.352 (0.336) loss_x loss_x 1.2812 (1.2855) acc_x 75.0000 (67.1875) lr 1.3681e-03 eta 0:00:04
epoch [78/200] batch [35/39] time 0.636 (0.467) data 0.504 (0.336) loss_x loss_x 0.8506 (1.3101) acc_x 81.2500 (66.8750) lr 1.3681e-03 eta 0:00:01
epoch [78/200] batch [5/58] time 0.455 (0.482) data 0.322 (0.351) loss_u loss_u 0.9111 (0.8723) acc_u 9.3750 (16.2500) lr 1.3681e-03 eta 0:00:25
epoch [78/200] batch [10/58] time 0.407 (0.479) data 0.275 (0.347) loss_u loss_u 0.8286 (0.8594) acc_u 25.0000 (19.6875) lr 1.3681e-03 eta 0:00:22
epoch [78/200] batch [15/58] time 0.360 (0.477) data 0.228 (0.345) loss_u loss_u 0.8701 (0.8503) acc_u 18.7500 (20.6250) lr 1.3681e-03 eta 0:00:20
epoch [78/200] batch [20/58] time 0.460 (0.477) data 0.328 (0.345) loss_u loss_u 0.9038 (0.8458) acc_u 15.6250 (20.9375) lr 1.3681e-03 eta 0:00:18
epoch [78/200] batch [25/58] time 0.382 (0.475) data 0.250 (0.344) loss_u loss_u 0.8989 (0.8488) acc_u 21.8750 (20.8750) lr 1.3681e-03 eta 0:00:15
epoch [78/200] batch [30/58] time 0.550 (0.473) data 0.418 (0.342) loss_u loss_u 0.7617 (0.8461) acc_u 28.1250 (21.1458) lr 1.3681e-03 eta 0:00:13
epoch [78/200] batch [35/58] time 0.477 (0.472) data 0.345 (0.340) loss_u loss_u 0.8198 (0.8441) acc_u 28.1250 (21.2500) lr 1.3681e-03 eta 0:00:10
epoch [78/200] batch [40/58] time 0.534 (0.473) data 0.402 (0.342) loss_u loss_u 0.8589 (0.8440) acc_u 21.8750 (21.4062) lr 1.3681e-03 eta 0:00:08
epoch [78/200] batch [45/58] time 0.412 (0.472) data 0.280 (0.340) loss_u loss_u 0.9521 (0.8455) acc_u 3.1250 (21.3194) lr 1.3681e-03 eta 0:00:06
epoch [78/200] batch [50/58] time 0.455 (0.477) data 0.324 (0.345) loss_u loss_u 0.8252 (0.8417) acc_u 28.1250 (21.6875) lr 1.3681e-03 eta 0:00:03
epoch [78/200] batch [55/58] time 0.467 (0.475) data 0.330 (0.343) loss_u loss_u 0.8696 (0.8410) acc_u 18.7500 (21.7614) lr 1.3681e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1544
confident_label rate tensor(0.3945, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1237
clean true:1235
clean false:2
clean_rate:0.9983831851253031
noisy true:357
noisy false:1542
after delete: len(clean_dataset) 1237
after delete: len(noisy_dataset) 1899
epoch [79/200] batch [5/38] time 0.408 (0.423) data 0.277 (0.292) loss_x loss_x 1.5420 (1.4980) acc_x 59.3750 (65.6250) lr 1.3535e-03 eta 0:00:13
epoch [79/200] batch [10/38] time 0.401 (0.443) data 0.270 (0.312) loss_x loss_x 0.9292 (1.4076) acc_x 81.2500 (69.0625) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [15/38] time 0.516 (0.455) data 0.387 (0.324) loss_x loss_x 1.4131 (1.4027) acc_x 62.5000 (68.7500) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [20/38] time 0.393 (0.445) data 0.263 (0.314) loss_x loss_x 1.5811 (1.4036) acc_x 65.6250 (67.6562) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [25/38] time 0.390 (0.444) data 0.260 (0.314) loss_x loss_x 1.0400 (1.3345) acc_x 78.1250 (69.0000) lr 1.3535e-03 eta 0:00:05
epoch [79/200] batch [30/38] time 0.463 (0.449) data 0.332 (0.319) loss_x loss_x 1.1377 (1.3261) acc_x 65.6250 (68.8542) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [35/38] time 0.436 (0.451) data 0.306 (0.320) loss_x loss_x 1.1758 (1.3424) acc_x 71.8750 (67.5893) lr 1.3535e-03 eta 0:00:01
epoch [79/200] batch [5/59] time 0.349 (0.447) data 0.218 (0.317) loss_u loss_u 0.8853 (0.8265) acc_u 15.6250 (20.6250) lr 1.3535e-03 eta 0:00:24
epoch [79/200] batch [10/59] time 0.466 (0.448) data 0.334 (0.318) loss_u loss_u 0.8496 (0.8385) acc_u 21.8750 (20.6250) lr 1.3535e-03 eta 0:00:21
epoch [79/200] batch [15/59] time 0.408 (0.449) data 0.278 (0.318) loss_u loss_u 0.7627 (0.8297) acc_u 28.1250 (21.8750) lr 1.3535e-03 eta 0:00:19
epoch [79/200] batch [20/59] time 0.380 (0.447) data 0.249 (0.316) loss_u loss_u 0.8784 (0.8272) acc_u 18.7500 (22.0312) lr 1.3535e-03 eta 0:00:17
epoch [79/200] batch [25/59] time 0.407 (0.447) data 0.276 (0.316) loss_u loss_u 0.8286 (0.8284) acc_u 21.8750 (22.0000) lr 1.3535e-03 eta 0:00:15
epoch [79/200] batch [30/59] time 0.381 (0.446) data 0.249 (0.315) loss_u loss_u 0.8081 (0.8280) acc_u 21.8750 (21.6667) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [35/59] time 0.512 (0.449) data 0.379 (0.318) loss_u loss_u 0.8765 (0.8308) acc_u 18.7500 (21.6071) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [40/59] time 0.526 (0.450) data 0.393 (0.319) loss_u loss_u 0.7935 (0.8317) acc_u 31.2500 (21.6406) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [45/59] time 0.546 (0.450) data 0.414 (0.319) loss_u loss_u 0.8452 (0.8341) acc_u 18.7500 (21.4583) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [50/59] time 0.747 (0.454) data 0.615 (0.323) loss_u loss_u 0.7808 (0.8311) acc_u 31.2500 (21.5625) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [55/59] time 0.731 (0.457) data 0.597 (0.326) loss_u loss_u 0.8623 (0.8326) acc_u 21.8750 (21.5909) lr 1.3535e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1549
confident_label rate tensor(0.3970, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1245
clean true:1245
clean false:0
clean_rate:1.0
noisy true:342
noisy false:1549
after delete: len(clean_dataset) 1245
after delete: len(noisy_dataset) 1891
epoch [80/200] batch [5/38] time 0.456 (0.530) data 0.325 (0.399) loss_x loss_x 1.1377 (1.1211) acc_x 65.6250 (69.3750) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [10/38] time 0.542 (0.512) data 0.411 (0.382) loss_x loss_x 1.2529 (1.2535) acc_x 71.8750 (66.5625) lr 1.3387e-03 eta 0:00:14
epoch [80/200] batch [15/38] time 0.358 (0.483) data 0.228 (0.352) loss_x loss_x 1.6826 (1.3736) acc_x 59.3750 (64.5833) lr 1.3387e-03 eta 0:00:11
epoch [80/200] batch [20/38] time 0.476 (0.490) data 0.347 (0.359) loss_x loss_x 1.3281 (1.3134) acc_x 62.5000 (66.4062) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [25/38] time 0.315 (0.474) data 0.185 (0.343) loss_x loss_x 1.5088 (1.2922) acc_x 65.6250 (67.1250) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [30/38] time 0.388 (0.465) data 0.257 (0.334) loss_x loss_x 1.5625 (1.3089) acc_x 59.3750 (66.7708) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [35/38] time 0.401 (0.460) data 0.271 (0.330) loss_x loss_x 1.1973 (1.3229) acc_x 62.5000 (66.1607) lr 1.3387e-03 eta 0:00:01
epoch [80/200] batch [5/59] time 0.452 (0.450) data 0.320 (0.320) loss_u loss_u 0.8447 (0.8303) acc_u 15.6250 (22.5000) lr 1.3387e-03 eta 0:00:24
epoch [80/200] batch [10/59] time 0.403 (0.449) data 0.271 (0.318) loss_u loss_u 0.8335 (0.8218) acc_u 25.0000 (23.4375) lr 1.3387e-03 eta 0:00:21
epoch [80/200] batch [15/59] time 0.700 (0.457) data 0.569 (0.326) loss_u loss_u 0.9009 (0.8327) acc_u 15.6250 (21.6667) lr 1.3387e-03 eta 0:00:20
epoch [80/200] batch [20/59] time 0.435 (0.453) data 0.304 (0.322) loss_u loss_u 0.8657 (0.8376) acc_u 12.5000 (21.0938) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [25/59] time 0.470 (0.452) data 0.339 (0.321) loss_u loss_u 0.7451 (0.8355) acc_u 31.2500 (21.1250) lr 1.3387e-03 eta 0:00:15
epoch [80/200] batch [30/59] time 0.490 (0.449) data 0.358 (0.318) loss_u loss_u 0.9102 (0.8446) acc_u 21.8750 (20.7292) lr 1.3387e-03 eta 0:00:13
epoch [80/200] batch [35/59] time 0.485 (0.448) data 0.354 (0.317) loss_u loss_u 0.8472 (0.8431) acc_u 15.6250 (20.8929) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [40/59] time 0.398 (0.444) data 0.267 (0.313) loss_u loss_u 0.8042 (0.8427) acc_u 28.1250 (20.9375) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [45/59] time 0.442 (0.445) data 0.311 (0.314) loss_u loss_u 0.9258 (0.8459) acc_u 12.5000 (20.6944) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [50/59] time 0.388 (0.442) data 0.257 (0.311) loss_u loss_u 0.8584 (0.8502) acc_u 18.7500 (19.9375) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [55/59] time 0.706 (0.445) data 0.575 (0.314) loss_u loss_u 0.7935 (0.8480) acc_u 25.0000 (20.2273) lr 1.3387e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1512
confident_label rate tensor(0.4110, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1289
clean true:1285
clean false:4
clean_rate:0.9968968192397207
noisy true:339
noisy false:1508
after delete: len(clean_dataset) 1289
after delete: len(noisy_dataset) 1847
epoch [81/200] batch [5/40] time 0.723 (0.534) data 0.589 (0.401) loss_x loss_x 1.0938 (1.3652) acc_x 81.2500 (69.3750) lr 1.3239e-03 eta 0:00:18
epoch [81/200] batch [10/40] time 0.490 (0.529) data 0.358 (0.397) loss_x loss_x 1.0322 (1.3320) acc_x 68.7500 (68.4375) lr 1.3239e-03 eta 0:00:15
epoch [81/200] batch [15/40] time 0.393 (0.513) data 0.262 (0.382) loss_x loss_x 1.0703 (1.2793) acc_x 68.7500 (68.7500) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [20/40] time 0.461 (0.486) data 0.328 (0.355) loss_x loss_x 1.2227 (1.2905) acc_x 71.8750 (68.7500) lr 1.3239e-03 eta 0:00:09
epoch [81/200] batch [25/40] time 0.347 (0.484) data 0.216 (0.352) loss_x loss_x 1.6748 (1.2955) acc_x 59.3750 (68.2500) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [30/40] time 0.469 (0.487) data 0.337 (0.356) loss_x loss_x 0.9556 (1.2921) acc_x 78.1250 (68.1250) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [35/40] time 0.413 (0.486) data 0.281 (0.354) loss_x loss_x 1.0078 (1.2835) acc_x 75.0000 (68.0357) lr 1.3239e-03 eta 0:00:02
epoch [81/200] batch [40/40] time 0.386 (0.485) data 0.255 (0.353) loss_x loss_x 1.3291 (1.2811) acc_x 68.7500 (67.8906) lr 1.3239e-03 eta 0:00:00
epoch [81/200] batch [5/57] time 0.449 (0.479) data 0.317 (0.348) loss_u loss_u 0.8486 (0.8521) acc_u 25.0000 (20.6250) lr 1.3239e-03 eta 0:00:24
epoch [81/200] batch [10/57] time 0.459 (0.480) data 0.326 (0.348) loss_u loss_u 0.8179 (0.8511) acc_u 28.1250 (20.6250) lr 1.3239e-03 eta 0:00:22
epoch [81/200] batch [15/57] time 0.341 (0.479) data 0.208 (0.348) loss_u loss_u 0.7935 (0.8335) acc_u 21.8750 (21.6667) lr 1.3239e-03 eta 0:00:20
epoch [81/200] batch [20/57] time 0.338 (0.474) data 0.205 (0.342) loss_u loss_u 0.9189 (0.8465) acc_u 12.5000 (20.0000) lr 1.3239e-03 eta 0:00:17
epoch [81/200] batch [25/57] time 0.566 (0.477) data 0.434 (0.345) loss_u loss_u 0.7544 (0.8475) acc_u 40.6250 (19.8750) lr 1.3239e-03 eta 0:00:15
epoch [81/200] batch [30/57] time 0.388 (0.472) data 0.256 (0.340) loss_u loss_u 0.7656 (0.8418) acc_u 28.1250 (20.1042) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [35/57] time 0.584 (0.477) data 0.451 (0.345) loss_u loss_u 0.7544 (0.8408) acc_u 34.3750 (20.4464) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [40/57] time 0.503 (0.487) data 0.371 (0.355) loss_u loss_u 0.8540 (0.8419) acc_u 18.7500 (20.3906) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [45/57] time 0.534 (0.487) data 0.402 (0.355) loss_u loss_u 0.8281 (0.8410) acc_u 28.1250 (20.4861) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [50/57] time 0.547 (0.488) data 0.412 (0.356) loss_u loss_u 0.8779 (0.8422) acc_u 21.8750 (20.4375) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [55/57] time 0.387 (0.485) data 0.256 (0.353) loss_u loss_u 0.8267 (0.8418) acc_u 25.0000 (20.5114) lr 1.3239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1531
confident_label rate tensor(0.4015, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1259
clean true:1257
clean false:2
clean_rate:0.9984114376489277
noisy true:348
noisy false:1529
after delete: len(clean_dataset) 1259
after delete: len(noisy_dataset) 1877
epoch [82/200] batch [5/39] time 0.343 (0.416) data 0.213 (0.285) loss_x loss_x 1.8545 (1.4379) acc_x 59.3750 (64.3750) lr 1.3090e-03 eta 0:00:14
epoch [82/200] batch [10/39] time 0.578 (0.459) data 0.448 (0.328) loss_x loss_x 1.0322 (1.2821) acc_x 62.5000 (67.1875) lr 1.3090e-03 eta 0:00:13
epoch [82/200] batch [15/39] time 0.529 (0.473) data 0.398 (0.342) loss_x loss_x 1.4600 (1.2968) acc_x 71.8750 (67.7083) lr 1.3090e-03 eta 0:00:11
epoch [82/200] batch [20/39] time 0.406 (0.465) data 0.276 (0.335) loss_x loss_x 1.3779 (1.2574) acc_x 62.5000 (68.2812) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [25/39] time 0.461 (0.464) data 0.331 (0.334) loss_x loss_x 1.1074 (1.2812) acc_x 68.7500 (66.6250) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [30/39] time 0.434 (0.474) data 0.304 (0.343) loss_x loss_x 0.9761 (1.2867) acc_x 78.1250 (66.6667) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [35/39] time 0.467 (0.474) data 0.336 (0.343) loss_x loss_x 1.3320 (1.2966) acc_x 68.7500 (66.1607) lr 1.3090e-03 eta 0:00:01
epoch [82/200] batch [5/58] time 0.384 (0.471) data 0.252 (0.340) loss_u loss_u 0.7891 (0.8354) acc_u 25.0000 (20.6250) lr 1.3090e-03 eta 0:00:24
epoch [82/200] batch [10/58] time 0.394 (0.467) data 0.263 (0.335) loss_u loss_u 0.9126 (0.8402) acc_u 12.5000 (20.9375) lr 1.3090e-03 eta 0:00:22
epoch [82/200] batch [15/58] time 0.464 (0.463) data 0.333 (0.332) loss_u loss_u 0.8086 (0.8243) acc_u 28.1250 (22.5000) lr 1.3090e-03 eta 0:00:19
epoch [82/200] batch [20/58] time 0.454 (0.461) data 0.322 (0.329) loss_u loss_u 0.8525 (0.8196) acc_u 18.7500 (23.1250) lr 1.3090e-03 eta 0:00:17
epoch [82/200] batch [25/58] time 0.386 (0.458) data 0.254 (0.327) loss_u loss_u 0.8730 (0.8285) acc_u 15.6250 (21.7500) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [30/58] time 0.540 (0.460) data 0.408 (0.328) loss_u loss_u 0.9189 (0.8346) acc_u 6.2500 (21.2500) lr 1.3090e-03 eta 0:00:12
epoch [82/200] batch [35/58] time 0.358 (0.458) data 0.229 (0.327) loss_u loss_u 0.7554 (0.8283) acc_u 31.2500 (22.0536) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [40/58] time 0.416 (0.456) data 0.284 (0.325) loss_u loss_u 0.8423 (0.8315) acc_u 25.0000 (21.7969) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [45/58] time 0.453 (0.458) data 0.322 (0.327) loss_u loss_u 0.8447 (0.8331) acc_u 15.6250 (21.3889) lr 1.3090e-03 eta 0:00:05
epoch [82/200] batch [50/58] time 0.515 (0.456) data 0.384 (0.325) loss_u loss_u 0.9180 (0.8376) acc_u 9.3750 (20.9375) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [55/58] time 0.393 (0.454) data 0.261 (0.323) loss_u loss_u 0.8770 (0.8394) acc_u 18.7500 (20.7386) lr 1.3090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1511
confident_label rate tensor(0.4101, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1286
clean true:1282
clean false:4
clean_rate:0.9968895800933126
noisy true:343
noisy false:1507
after delete: len(clean_dataset) 1286
after delete: len(noisy_dataset) 1850
epoch [83/200] batch [5/40] time 0.379 (0.438) data 0.249 (0.307) loss_x loss_x 1.1357 (1.3174) acc_x 68.7500 (68.7500) lr 1.2940e-03 eta 0:00:15
epoch [83/200] batch [10/40] time 0.398 (0.458) data 0.268 (0.328) loss_x loss_x 0.8325 (1.2753) acc_x 81.2500 (70.9375) lr 1.2940e-03 eta 0:00:13
epoch [83/200] batch [15/40] time 0.460 (0.451) data 0.329 (0.321) loss_x loss_x 1.0410 (1.2394) acc_x 65.6250 (69.5833) lr 1.2940e-03 eta 0:00:11
epoch [83/200] batch [20/40] time 0.439 (0.448) data 0.308 (0.318) loss_x loss_x 1.4990 (1.2456) acc_x 56.2500 (68.7500) lr 1.2940e-03 eta 0:00:08
epoch [83/200] batch [25/40] time 0.428 (0.449) data 0.298 (0.318) loss_x loss_x 0.9463 (1.2444) acc_x 75.0000 (68.7500) lr 1.2940e-03 eta 0:00:06
epoch [83/200] batch [30/40] time 0.426 (0.444) data 0.295 (0.313) loss_x loss_x 1.3535 (1.2311) acc_x 56.2500 (68.7500) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [35/40] time 0.349 (0.440) data 0.218 (0.309) loss_x loss_x 1.6719 (1.2547) acc_x 62.5000 (68.5714) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [40/40] time 0.474 (0.441) data 0.344 (0.311) loss_x loss_x 1.0996 (1.2813) acc_x 78.1250 (67.8125) lr 1.2940e-03 eta 0:00:00
epoch [83/200] batch [5/57] time 0.439 (0.440) data 0.308 (0.310) loss_u loss_u 0.8716 (0.8295) acc_u 12.5000 (19.3750) lr 1.2940e-03 eta 0:00:22
epoch [83/200] batch [10/57] time 0.625 (0.446) data 0.493 (0.315) loss_u loss_u 0.7407 (0.8331) acc_u 37.5000 (20.3125) lr 1.2940e-03 eta 0:00:20
epoch [83/200] batch [15/57] time 0.473 (0.446) data 0.341 (0.315) loss_u loss_u 0.8774 (0.8364) acc_u 12.5000 (20.2083) lr 1.2940e-03 eta 0:00:18
epoch [83/200] batch [20/57] time 0.430 (0.446) data 0.298 (0.315) loss_u loss_u 0.9248 (0.8417) acc_u 9.3750 (18.7500) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [25/57] time 0.526 (0.452) data 0.394 (0.321) loss_u loss_u 0.9326 (0.8452) acc_u 6.2500 (18.6250) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [30/57] time 0.368 (0.453) data 0.237 (0.322) loss_u loss_u 0.8457 (0.8387) acc_u 15.6250 (19.6875) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [35/57] time 0.313 (0.451) data 0.182 (0.320) loss_u loss_u 0.9004 (0.8432) acc_u 12.5000 (19.7321) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [40/57] time 0.412 (0.450) data 0.281 (0.318) loss_u loss_u 0.7651 (0.8434) acc_u 34.3750 (19.8438) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [45/57] time 0.623 (0.452) data 0.492 (0.321) loss_u loss_u 0.8149 (0.8395) acc_u 21.8750 (20.2778) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [50/57] time 0.451 (0.452) data 0.321 (0.321) loss_u loss_u 0.8462 (0.8349) acc_u 18.7500 (21.0625) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [55/57] time 0.319 (0.448) data 0.188 (0.317) loss_u loss_u 0.8320 (0.8364) acc_u 28.1250 (21.1932) lr 1.2940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1486
confident_label rate tensor(0.4114, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1290
clean true:1289
clean false:1
clean_rate:0.9992248062015504
noisy true:361
noisy false:1485
after delete: len(clean_dataset) 1290
after delete: len(noisy_dataset) 1846
epoch [84/200] batch [5/40] time 0.483 (0.505) data 0.353 (0.374) loss_x loss_x 1.5898 (1.2069) acc_x 59.3750 (66.2500) lr 1.2790e-03 eta 0:00:17
epoch [84/200] batch [10/40] time 0.422 (0.489) data 0.292 (0.359) loss_x loss_x 1.6611 (1.2239) acc_x 65.6250 (68.4375) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [15/40] time 0.533 (0.484) data 0.403 (0.354) loss_x loss_x 1.1699 (1.2011) acc_x 71.8750 (70.0000) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [20/40] time 0.459 (0.479) data 0.330 (0.349) loss_x loss_x 1.0225 (1.2213) acc_x 75.0000 (69.3750) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [25/40] time 0.424 (0.468) data 0.294 (0.338) loss_x loss_x 1.7031 (1.2434) acc_x 59.3750 (69.0000) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [30/40] time 0.543 (0.464) data 0.414 (0.334) loss_x loss_x 0.8872 (1.1998) acc_x 81.2500 (69.8958) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [35/40] time 0.372 (0.455) data 0.242 (0.325) loss_x loss_x 1.4355 (1.1908) acc_x 71.8750 (69.9107) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [40/40] time 0.416 (0.451) data 0.285 (0.321) loss_x loss_x 1.7695 (1.2261) acc_x 56.2500 (69.0625) lr 1.2790e-03 eta 0:00:00
epoch [84/200] batch [5/57] time 0.475 (0.452) data 0.343 (0.321) loss_u loss_u 0.7285 (0.8023) acc_u 31.2500 (25.6250) lr 1.2790e-03 eta 0:00:23
epoch [84/200] batch [10/57] time 0.401 (0.445) data 0.271 (0.315) loss_u loss_u 0.8276 (0.8236) acc_u 25.0000 (22.8125) lr 1.2790e-03 eta 0:00:20
epoch [84/200] batch [15/57] time 0.461 (0.443) data 0.329 (0.313) loss_u loss_u 0.8633 (0.8319) acc_u 18.7500 (21.2500) lr 1.2790e-03 eta 0:00:18
epoch [84/200] batch [20/57] time 0.406 (0.442) data 0.274 (0.311) loss_u loss_u 0.8843 (0.8378) acc_u 12.5000 (20.3125) lr 1.2790e-03 eta 0:00:16
epoch [84/200] batch [25/57] time 0.433 (0.445) data 0.302 (0.315) loss_u loss_u 0.9004 (0.8438) acc_u 12.5000 (19.6250) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [30/57] time 0.460 (0.444) data 0.330 (0.314) loss_u loss_u 0.8315 (0.8440) acc_u 28.1250 (19.8958) lr 1.2790e-03 eta 0:00:11
epoch [84/200] batch [35/57] time 0.491 (0.445) data 0.360 (0.315) loss_u loss_u 0.8916 (0.8478) acc_u 15.6250 (19.6429) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [40/57] time 0.448 (0.443) data 0.317 (0.312) loss_u loss_u 0.7969 (0.8454) acc_u 28.1250 (20.0000) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [45/57] time 0.372 (0.446) data 0.241 (0.316) loss_u loss_u 0.9209 (0.8459) acc_u 9.3750 (19.9306) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [50/57] time 0.439 (0.447) data 0.308 (0.316) loss_u loss_u 0.8071 (0.8432) acc_u 28.1250 (20.4375) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [55/57] time 0.425 (0.446) data 0.294 (0.316) loss_u loss_u 0.7939 (0.8417) acc_u 25.0000 (20.3977) lr 1.2790e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1503
confident_label rate tensor(0.4123, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1293
clean true:1291
clean false:2
clean_rate:0.9984532095901005
noisy true:342
noisy false:1501
after delete: len(clean_dataset) 1293
after delete: len(noisy_dataset) 1843
epoch [85/200] batch [5/40] time 0.435 (0.441) data 0.305 (0.310) loss_x loss_x 1.7451 (1.5232) acc_x 62.5000 (60.6250) lr 1.2639e-03 eta 0:00:15
epoch [85/200] batch [10/40] time 0.453 (0.441) data 0.322 (0.311) loss_x loss_x 1.5488 (1.4138) acc_x 46.8750 (61.5625) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [15/40] time 0.427 (0.439) data 0.297 (0.309) loss_x loss_x 1.0479 (1.3533) acc_x 78.1250 (65.0000) lr 1.2639e-03 eta 0:00:10
epoch [85/200] batch [20/40] time 0.436 (0.434) data 0.306 (0.303) loss_x loss_x 1.4521 (1.3524) acc_x 56.2500 (65.0000) lr 1.2639e-03 eta 0:00:08
epoch [85/200] batch [25/40] time 0.513 (0.440) data 0.383 (0.310) loss_x loss_x 1.2695 (1.3155) acc_x 68.7500 (65.5000) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [30/40] time 0.486 (0.442) data 0.355 (0.312) loss_x loss_x 0.9937 (1.2878) acc_x 81.2500 (66.5625) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [35/40] time 0.343 (0.445) data 0.213 (0.315) loss_x loss_x 1.4209 (1.2988) acc_x 56.2500 (66.4286) lr 1.2639e-03 eta 0:00:02
epoch [85/200] batch [40/40] time 0.394 (0.445) data 0.264 (0.315) loss_x loss_x 1.3613 (1.2965) acc_x 65.6250 (66.7969) lr 1.2639e-03 eta 0:00:00
epoch [85/200] batch [5/57] time 0.557 (0.446) data 0.425 (0.315) loss_u loss_u 0.8003 (0.7961) acc_u 28.1250 (26.8750) lr 1.2639e-03 eta 0:00:23
epoch [85/200] batch [10/57] time 0.445 (0.448) data 0.313 (0.318) loss_u loss_u 0.8936 (0.8251) acc_u 15.6250 (22.8125) lr 1.2639e-03 eta 0:00:21
epoch [85/200] batch [15/57] time 0.411 (0.449) data 0.280 (0.318) loss_u loss_u 0.6968 (0.8225) acc_u 40.6250 (22.9167) lr 1.2639e-03 eta 0:00:18
epoch [85/200] batch [20/57] time 0.508 (0.446) data 0.377 (0.316) loss_u loss_u 0.7627 (0.8228) acc_u 34.3750 (22.9688) lr 1.2639e-03 eta 0:00:16
epoch [85/200] batch [25/57] time 0.364 (0.445) data 0.233 (0.315) loss_u loss_u 0.8086 (0.8247) acc_u 21.8750 (23.0000) lr 1.2639e-03 eta 0:00:14
epoch [85/200] batch [30/57] time 0.433 (0.446) data 0.302 (0.315) loss_u loss_u 0.8940 (0.8294) acc_u 12.5000 (21.9792) lr 1.2639e-03 eta 0:00:12
epoch [85/200] batch [35/57] time 0.444 (0.447) data 0.314 (0.316) loss_u loss_u 0.7769 (0.8276) acc_u 28.1250 (22.1429) lr 1.2639e-03 eta 0:00:09
epoch [85/200] batch [40/57] time 0.351 (0.444) data 0.221 (0.314) loss_u loss_u 0.9028 (0.8302) acc_u 15.6250 (22.0312) lr 1.2639e-03 eta 0:00:07
epoch [85/200] batch [45/57] time 0.388 (0.445) data 0.258 (0.314) loss_u loss_u 0.8452 (0.8330) acc_u 25.0000 (21.7361) lr 1.2639e-03 eta 0:00:05
epoch [85/200] batch [50/57] time 0.449 (0.445) data 0.319 (0.314) loss_u loss_u 0.8921 (0.8354) acc_u 15.6250 (21.3125) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [55/57] time 0.347 (0.443) data 0.217 (0.312) loss_u loss_u 0.9458 (0.8375) acc_u 3.1250 (20.7955) lr 1.2639e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1565
confident_label rate tensor(0.3941, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1236
clean true:1233
clean false:3
clean_rate:0.9975728155339806
noisy true:338
noisy false:1562
after delete: len(clean_dataset) 1236
after delete: len(noisy_dataset) 1900
epoch [86/200] batch [5/38] time 0.505 (0.485) data 0.373 (0.354) loss_x loss_x 1.4141 (1.1445) acc_x 71.8750 (76.8750) lr 1.2487e-03 eta 0:00:15
epoch [86/200] batch [10/38] time 0.471 (0.490) data 0.340 (0.359) loss_x loss_x 0.7700 (1.1414) acc_x 78.1250 (74.3750) lr 1.2487e-03 eta 0:00:13
epoch [86/200] batch [15/38] time 0.430 (0.481) data 0.299 (0.351) loss_x loss_x 0.9038 (1.1582) acc_x 68.7500 (71.4583) lr 1.2487e-03 eta 0:00:11
epoch [86/200] batch [20/38] time 0.329 (0.468) data 0.198 (0.338) loss_x loss_x 1.4326 (1.2562) acc_x 68.7500 (69.2188) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [25/38] time 0.414 (0.465) data 0.284 (0.335) loss_x loss_x 1.5039 (1.2523) acc_x 62.5000 (69.6250) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [30/38] time 0.438 (0.467) data 0.307 (0.337) loss_x loss_x 1.7168 (1.2735) acc_x 50.0000 (69.3750) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [35/38] time 0.498 (0.464) data 0.367 (0.334) loss_x loss_x 1.3408 (1.2767) acc_x 68.7500 (68.7500) lr 1.2487e-03 eta 0:00:01
epoch [86/200] batch [5/59] time 0.358 (0.458) data 0.226 (0.327) loss_u loss_u 0.7632 (0.8486) acc_u 31.2500 (19.3750) lr 1.2487e-03 eta 0:00:24
epoch [86/200] batch [10/59] time 0.439 (0.454) data 0.307 (0.323) loss_u loss_u 0.6997 (0.8272) acc_u 37.5000 (22.1875) lr 1.2487e-03 eta 0:00:22
epoch [86/200] batch [15/59] time 0.414 (0.452) data 0.283 (0.322) loss_u loss_u 0.8564 (0.8391) acc_u 18.7500 (20.8333) lr 1.2487e-03 eta 0:00:19
epoch [86/200] batch [20/59] time 0.445 (0.451) data 0.314 (0.321) loss_u loss_u 0.8740 (0.8290) acc_u 12.5000 (22.1875) lr 1.2487e-03 eta 0:00:17
epoch [86/200] batch [25/59] time 0.503 (0.451) data 0.372 (0.320) loss_u loss_u 0.8154 (0.8308) acc_u 18.7500 (21.8750) lr 1.2487e-03 eta 0:00:15
epoch [86/200] batch [30/59] time 0.480 (0.450) data 0.348 (0.319) loss_u loss_u 0.7812 (0.8268) acc_u 34.3750 (22.8125) lr 1.2487e-03 eta 0:00:13
epoch [86/200] batch [35/59] time 0.312 (0.446) data 0.179 (0.315) loss_u loss_u 0.8901 (0.8307) acc_u 12.5000 (22.3214) lr 1.2487e-03 eta 0:00:10
epoch [86/200] batch [40/59] time 0.418 (0.446) data 0.285 (0.315) loss_u loss_u 0.8613 (0.8294) acc_u 15.6250 (22.2656) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [45/59] time 0.369 (0.447) data 0.237 (0.316) loss_u loss_u 0.7197 (0.8222) acc_u 31.2500 (22.8472) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [50/59] time 0.316 (0.449) data 0.184 (0.318) loss_u loss_u 0.9067 (0.8236) acc_u 9.3750 (22.6875) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [55/59] time 0.495 (0.449) data 0.365 (0.318) loss_u loss_u 0.8696 (0.8268) acc_u 15.6250 (21.9886) lr 1.2487e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1533
confident_label rate tensor(0.4043, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1268
clean true:1264
clean false:4
clean_rate:0.9968454258675079
noisy true:339
noisy false:1529
after delete: len(clean_dataset) 1268
after delete: len(noisy_dataset) 1868
epoch [87/200] batch [5/39] time 0.347 (0.417) data 0.217 (0.286) loss_x loss_x 1.3242 (1.0262) acc_x 62.5000 (75.6250) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [10/39] time 0.416 (0.440) data 0.286 (0.309) loss_x loss_x 0.9912 (1.0894) acc_x 71.8750 (72.8125) lr 1.2334e-03 eta 0:00:12
epoch [87/200] batch [15/39] time 0.412 (0.437) data 0.282 (0.306) loss_x loss_x 1.0615 (1.2090) acc_x 75.0000 (70.2083) lr 1.2334e-03 eta 0:00:10
epoch [87/200] batch [20/39] time 0.461 (0.441) data 0.331 (0.310) loss_x loss_x 1.6943 (1.2086) acc_x 62.5000 (70.3125) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [25/39] time 0.479 (0.449) data 0.349 (0.319) loss_x loss_x 0.8594 (1.1911) acc_x 75.0000 (70.6250) lr 1.2334e-03 eta 0:00:06
epoch [87/200] batch [30/39] time 0.432 (0.459) data 0.302 (0.328) loss_x loss_x 1.3750 (1.2014) acc_x 68.7500 (70.9375) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [35/39] time 0.326 (0.461) data 0.196 (0.331) loss_x loss_x 1.7559 (1.2226) acc_x 56.2500 (70.1786) lr 1.2334e-03 eta 0:00:01
epoch [87/200] batch [5/58] time 0.490 (0.455) data 0.359 (0.325) loss_u loss_u 0.7935 (0.8211) acc_u 21.8750 (20.6250) lr 1.2334e-03 eta 0:00:24
epoch [87/200] batch [10/58] time 0.449 (0.456) data 0.318 (0.325) loss_u loss_u 0.8296 (0.8296) acc_u 21.8750 (20.3125) lr 1.2334e-03 eta 0:00:21
epoch [87/200] batch [15/58] time 0.494 (0.458) data 0.363 (0.327) loss_u loss_u 0.8257 (0.8333) acc_u 18.7500 (19.5833) lr 1.2334e-03 eta 0:00:19
epoch [87/200] batch [20/58] time 0.385 (0.455) data 0.254 (0.325) loss_u loss_u 0.7549 (0.8251) acc_u 37.5000 (21.5625) lr 1.2334e-03 eta 0:00:17
epoch [87/200] batch [25/58] time 0.340 (0.450) data 0.209 (0.319) loss_u loss_u 0.8813 (0.8306) acc_u 12.5000 (21.0000) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [30/58] time 0.509 (0.448) data 0.379 (0.317) loss_u loss_u 0.8086 (0.8289) acc_u 21.8750 (21.4583) lr 1.2334e-03 eta 0:00:12
epoch [87/200] batch [35/58] time 0.412 (0.445) data 0.281 (0.315) loss_u loss_u 0.9014 (0.8329) acc_u 3.1250 (20.8036) lr 1.2334e-03 eta 0:00:10
epoch [87/200] batch [40/58] time 0.323 (0.447) data 0.192 (0.316) loss_u loss_u 0.8164 (0.8365) acc_u 31.2500 (20.7031) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [45/58] time 0.429 (0.445) data 0.297 (0.315) loss_u loss_u 0.7471 (0.8342) acc_u 31.2500 (20.9722) lr 1.2334e-03 eta 0:00:05
epoch [87/200] batch [50/58] time 0.375 (0.449) data 0.244 (0.318) loss_u loss_u 0.7954 (0.8346) acc_u 28.1250 (21.3125) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [55/58] time 0.398 (0.446) data 0.265 (0.315) loss_u loss_u 0.7949 (0.8318) acc_u 31.2500 (21.8182) lr 1.2334e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1540
confident_label rate tensor(0.4031, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1264
clean true:1263
clean false:1
clean_rate:0.9992088607594937
noisy true:333
noisy false:1539
after delete: len(clean_dataset) 1264
after delete: len(noisy_dataset) 1872
epoch [88/200] batch [5/39] time 0.448 (0.455) data 0.317 (0.324) loss_x loss_x 1.5430 (1.1573) acc_x 65.6250 (71.2500) lr 1.2181e-03 eta 0:00:15
epoch [88/200] batch [10/39] time 0.440 (0.454) data 0.309 (0.324) loss_x loss_x 0.7437 (1.2152) acc_x 78.1250 (70.0000) lr 1.2181e-03 eta 0:00:13
epoch [88/200] batch [15/39] time 0.456 (0.449) data 0.324 (0.318) loss_x loss_x 1.2646 (1.2694) acc_x 71.8750 (69.1667) lr 1.2181e-03 eta 0:00:10
epoch [88/200] batch [20/39] time 0.427 (0.451) data 0.296 (0.320) loss_x loss_x 1.4346 (1.2194) acc_x 59.3750 (70.7812) lr 1.2181e-03 eta 0:00:08
epoch [88/200] batch [25/39] time 0.479 (0.441) data 0.348 (0.311) loss_x loss_x 0.6201 (1.1910) acc_x 87.5000 (71.7500) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [30/39] time 0.669 (0.449) data 0.539 (0.318) loss_x loss_x 1.4678 (1.2455) acc_x 68.7500 (70.6250) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [35/39] time 0.533 (0.453) data 0.403 (0.323) loss_x loss_x 1.0645 (1.2627) acc_x 65.6250 (69.4643) lr 1.2181e-03 eta 0:00:01
epoch [88/200] batch [5/58] time 0.410 (0.455) data 0.278 (0.324) loss_u loss_u 0.8696 (0.8387) acc_u 18.7500 (23.7500) lr 1.2181e-03 eta 0:00:24
epoch [88/200] batch [10/58] time 0.386 (0.449) data 0.254 (0.318) loss_u loss_u 0.8604 (0.8470) acc_u 12.5000 (20.0000) lr 1.2181e-03 eta 0:00:21
epoch [88/200] batch [15/58] time 0.968 (0.461) data 0.836 (0.330) loss_u loss_u 0.7900 (0.8363) acc_u 28.1250 (21.6667) lr 1.2181e-03 eta 0:00:19
epoch [88/200] batch [20/58] time 0.466 (0.460) data 0.334 (0.329) loss_u loss_u 0.8008 (0.8332) acc_u 21.8750 (22.3438) lr 1.2181e-03 eta 0:00:17
epoch [88/200] batch [25/58] time 0.458 (0.456) data 0.326 (0.325) loss_u loss_u 0.8813 (0.8351) acc_u 15.6250 (22.1250) lr 1.2181e-03 eta 0:00:15
epoch [88/200] batch [30/58] time 0.374 (0.451) data 0.244 (0.320) loss_u loss_u 0.8984 (0.8406) acc_u 18.7500 (21.6667) lr 1.2181e-03 eta 0:00:12
epoch [88/200] batch [35/58] time 0.584 (0.452) data 0.452 (0.321) loss_u loss_u 0.9097 (0.8447) acc_u 6.2500 (20.8929) lr 1.2181e-03 eta 0:00:10
epoch [88/200] batch [40/58] time 0.428 (0.454) data 0.297 (0.323) loss_u loss_u 0.9312 (0.8461) acc_u 6.2500 (21.0156) lr 1.2181e-03 eta 0:00:08
epoch [88/200] batch [45/58] time 0.425 (0.456) data 0.293 (0.325) loss_u loss_u 0.7764 (0.8469) acc_u 21.8750 (20.5556) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [50/58] time 0.378 (0.453) data 0.246 (0.322) loss_u loss_u 0.8672 (0.8459) acc_u 15.6250 (20.5000) lr 1.2181e-03 eta 0:00:03
epoch [88/200] batch [55/58] time 0.388 (0.453) data 0.257 (0.321) loss_u loss_u 0.7896 (0.8457) acc_u 25.0000 (20.1705) lr 1.2181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1506
confident_label rate tensor(0.4136, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1297
clean true:1295
clean false:2
clean_rate:0.9984579799537394
noisy true:335
noisy false:1504
after delete: len(clean_dataset) 1297
after delete: len(noisy_dataset) 1839
epoch [89/200] batch [5/40] time 0.450 (0.412) data 0.320 (0.282) loss_x loss_x 1.3818 (1.4314) acc_x 62.5000 (69.3750) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [10/40] time 0.485 (0.443) data 0.355 (0.313) loss_x loss_x 1.0908 (1.2871) acc_x 71.8750 (70.0000) lr 1.2028e-03 eta 0:00:13
epoch [89/200] batch [15/40] time 0.450 (0.452) data 0.321 (0.322) loss_x loss_x 1.7236 (1.3730) acc_x 59.3750 (67.5000) lr 1.2028e-03 eta 0:00:11
epoch [89/200] batch [20/40] time 0.416 (0.446) data 0.287 (0.316) loss_x loss_x 1.2334 (1.3915) acc_x 59.3750 (67.1875) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [25/40] time 0.436 (0.439) data 0.306 (0.309) loss_x loss_x 1.9766 (1.4110) acc_x 56.2500 (66.8750) lr 1.2028e-03 eta 0:00:06
epoch [89/200] batch [30/40] time 0.412 (0.439) data 0.282 (0.309) loss_x loss_x 0.9277 (1.3770) acc_x 71.8750 (67.0833) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [35/40] time 0.500 (0.442) data 0.369 (0.312) loss_x loss_x 1.4131 (1.3574) acc_x 62.5000 (67.0536) lr 1.2028e-03 eta 0:00:02
epoch [89/200] batch [40/40] time 0.514 (0.445) data 0.384 (0.315) loss_x loss_x 1.3164 (1.3569) acc_x 65.6250 (67.1094) lr 1.2028e-03 eta 0:00:00
epoch [89/200] batch [5/57] time 0.325 (0.439) data 0.194 (0.309) loss_u loss_u 0.8086 (0.8452) acc_u 25.0000 (18.7500) lr 1.2028e-03 eta 0:00:22
epoch [89/200] batch [10/57] time 0.651 (0.443) data 0.520 (0.313) loss_u loss_u 0.9009 (0.8293) acc_u 12.5000 (21.5625) lr 1.2028e-03 eta 0:00:20
epoch [89/200] batch [15/57] time 0.506 (0.444) data 0.374 (0.314) loss_u loss_u 0.8057 (0.8206) acc_u 25.0000 (22.9167) lr 1.2028e-03 eta 0:00:18
epoch [89/200] batch [20/57] time 0.499 (0.442) data 0.367 (0.312) loss_u loss_u 0.8687 (0.8345) acc_u 18.7500 (21.0938) lr 1.2028e-03 eta 0:00:16
epoch [89/200] batch [25/57] time 0.443 (0.441) data 0.312 (0.310) loss_u loss_u 0.8789 (0.8344) acc_u 18.7500 (21.1250) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [30/57] time 0.418 (0.446) data 0.287 (0.315) loss_u loss_u 0.7856 (0.8374) acc_u 31.2500 (20.7292) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [35/57] time 0.432 (0.444) data 0.300 (0.313) loss_u loss_u 0.7954 (0.8363) acc_u 18.7500 (20.6250) lr 1.2028e-03 eta 0:00:09
epoch [89/200] batch [40/57] time 0.365 (0.438) data 0.233 (0.308) loss_u loss_u 0.9360 (0.8387) acc_u 6.2500 (20.3125) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [45/57] time 0.424 (0.437) data 0.293 (0.306) loss_u loss_u 0.8799 (0.8416) acc_u 15.6250 (19.7222) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [50/57] time 0.437 (0.442) data 0.306 (0.311) loss_u loss_u 0.8994 (0.8447) acc_u 6.2500 (19.2500) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [55/57] time 0.456 (0.443) data 0.325 (0.312) loss_u loss_u 0.8320 (0.8439) acc_u 25.0000 (19.4318) lr 1.2028e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.4043, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1268
clean true:1265
clean false:3
clean_rate:0.9976340694006309
noisy true:354
noisy false:1514
after delete: len(clean_dataset) 1268
after delete: len(noisy_dataset) 1868
epoch [90/200] batch [5/39] time 0.558 (0.487) data 0.427 (0.356) loss_x loss_x 1.1650 (1.0571) acc_x 78.1250 (75.0000) lr 1.1874e-03 eta 0:00:16
epoch [90/200] batch [10/39] time 0.559 (0.488) data 0.429 (0.357) loss_x loss_x 0.9165 (1.1405) acc_x 68.7500 (73.4375) lr 1.1874e-03 eta 0:00:14
epoch [90/200] batch [15/39] time 0.492 (0.477) data 0.362 (0.347) loss_x loss_x 1.3232 (1.1558) acc_x 53.1250 (70.6250) lr 1.1874e-03 eta 0:00:11
epoch [90/200] batch [20/39] time 0.710 (0.482) data 0.579 (0.351) loss_x loss_x 1.1621 (1.1921) acc_x 65.6250 (69.6875) lr 1.1874e-03 eta 0:00:09
epoch [90/200] batch [25/39] time 0.441 (0.475) data 0.311 (0.344) loss_x loss_x 1.3594 (1.2335) acc_x 68.7500 (69.3750) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [30/39] time 0.405 (0.467) data 0.275 (0.336) loss_x loss_x 1.1162 (1.2123) acc_x 71.8750 (69.8958) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [35/39] time 0.438 (0.463) data 0.308 (0.333) loss_x loss_x 1.5068 (1.2354) acc_x 65.6250 (69.0179) lr 1.1874e-03 eta 0:00:01
epoch [90/200] batch [5/58] time 0.463 (0.457) data 0.332 (0.327) loss_u loss_u 0.8369 (0.8167) acc_u 21.8750 (23.1250) lr 1.1874e-03 eta 0:00:24
epoch [90/200] batch [10/58] time 0.401 (0.458) data 0.270 (0.327) loss_u loss_u 0.8457 (0.8073) acc_u 18.7500 (24.3750) lr 1.1874e-03 eta 0:00:21
epoch [90/200] batch [15/58] time 0.526 (0.457) data 0.396 (0.326) loss_u loss_u 0.8584 (0.8170) acc_u 18.7500 (23.5417) lr 1.1874e-03 eta 0:00:19
epoch [90/200] batch [20/58] time 0.495 (0.455) data 0.365 (0.325) loss_u loss_u 0.8491 (0.8279) acc_u 21.8750 (22.0312) lr 1.1874e-03 eta 0:00:17
epoch [90/200] batch [25/58] time 0.371 (0.456) data 0.240 (0.325) loss_u loss_u 0.8809 (0.8313) acc_u 15.6250 (21.5000) lr 1.1874e-03 eta 0:00:15
epoch [90/200] batch [30/58] time 0.369 (0.453) data 0.238 (0.322) loss_u loss_u 0.8501 (0.8287) acc_u 18.7500 (22.0833) lr 1.1874e-03 eta 0:00:12
epoch [90/200] batch [35/58] time 0.472 (0.450) data 0.342 (0.320) loss_u loss_u 0.8135 (0.8289) acc_u 21.8750 (21.8750) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [40/58] time 0.384 (0.446) data 0.254 (0.315) loss_u loss_u 0.9189 (0.8345) acc_u 6.2500 (21.0938) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [45/58] time 0.550 (0.447) data 0.418 (0.316) loss_u loss_u 0.8408 (0.8364) acc_u 18.7500 (20.8333) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [50/58] time 0.440 (0.445) data 0.309 (0.315) loss_u loss_u 0.8354 (0.8339) acc_u 18.7500 (21.2500) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [55/58] time 0.382 (0.444) data 0.251 (0.313) loss_u loss_u 0.8599 (0.8359) acc_u 18.7500 (21.1364) lr 1.1874e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1505
confident_label rate tensor(0.4117, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1291
clean true:1288
clean false:3
clean_rate:0.9976762199845082
noisy true:343
noisy false:1502
after delete: len(clean_dataset) 1291
after delete: len(noisy_dataset) 1845
epoch [91/200] batch [5/40] time 0.442 (0.428) data 0.311 (0.297) loss_x loss_x 1.2314 (1.5090) acc_x 68.7500 (64.3750) lr 1.1719e-03 eta 0:00:14
epoch [91/200] batch [10/40] time 0.455 (0.426) data 0.324 (0.295) loss_x loss_x 1.9766 (1.4072) acc_x 53.1250 (65.9375) lr 1.1719e-03 eta 0:00:12
epoch [91/200] batch [15/40] time 0.415 (0.444) data 0.284 (0.314) loss_x loss_x 1.5293 (1.3316) acc_x 68.7500 (67.0833) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [20/40] time 0.484 (0.442) data 0.351 (0.311) loss_x loss_x 1.2852 (1.2777) acc_x 68.7500 (68.4375) lr 1.1719e-03 eta 0:00:08
epoch [91/200] batch [25/40] time 0.443 (0.441) data 0.313 (0.310) loss_x loss_x 1.3135 (1.2626) acc_x 59.3750 (68.0000) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [30/40] time 0.416 (0.441) data 0.286 (0.310) loss_x loss_x 1.0977 (1.2437) acc_x 75.0000 (68.3333) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [35/40] time 0.389 (0.433) data 0.258 (0.303) loss_x loss_x 1.4824 (1.2305) acc_x 62.5000 (68.4821) lr 1.1719e-03 eta 0:00:02
epoch [91/200] batch [40/40] time 0.406 (0.430) data 0.276 (0.300) loss_x loss_x 0.6460 (1.2228) acc_x 78.1250 (68.2812) lr 1.1719e-03 eta 0:00:00
epoch [91/200] batch [5/57] time 0.412 (0.428) data 0.280 (0.298) loss_u loss_u 0.8750 (0.8143) acc_u 21.8750 (25.0000) lr 1.1719e-03 eta 0:00:22
epoch [91/200] batch [10/57] time 0.506 (0.429) data 0.375 (0.298) loss_u loss_u 0.9111 (0.8303) acc_u 9.3750 (22.1875) lr 1.1719e-03 eta 0:00:20
epoch [91/200] batch [15/57] time 0.573 (0.432) data 0.442 (0.302) loss_u loss_u 0.7842 (0.8194) acc_u 28.1250 (24.3750) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [20/57] time 0.365 (0.430) data 0.234 (0.299) loss_u loss_u 0.8784 (0.8325) acc_u 12.5000 (22.5000) lr 1.1719e-03 eta 0:00:15
epoch [91/200] batch [25/57] time 0.429 (0.428) data 0.298 (0.297) loss_u loss_u 0.9111 (0.8346) acc_u 6.2500 (21.7500) lr 1.1719e-03 eta 0:00:13
epoch [91/200] batch [30/57] time 0.494 (0.428) data 0.362 (0.297) loss_u loss_u 0.8008 (0.8377) acc_u 25.0000 (21.2500) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [35/57] time 0.530 (0.429) data 0.400 (0.298) loss_u loss_u 0.8306 (0.8398) acc_u 18.7500 (20.8929) lr 1.1719e-03 eta 0:00:09
epoch [91/200] batch [40/57] time 0.517 (0.430) data 0.386 (0.299) loss_u loss_u 0.8589 (0.8352) acc_u 18.7500 (21.2500) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [45/57] time 0.351 (0.432) data 0.220 (0.301) loss_u loss_u 0.8218 (0.8381) acc_u 21.8750 (20.5556) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [50/57] time 0.504 (0.437) data 0.372 (0.306) loss_u loss_u 0.8281 (0.8409) acc_u 31.2500 (20.5625) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [55/57] time 0.431 (0.436) data 0.299 (0.306) loss_u loss_u 0.8398 (0.8423) acc_u 18.7500 (20.2273) lr 1.1719e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1510
confident_label rate tensor(0.4082, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1280
clean true:1277
clean false:3
clean_rate:0.99765625
noisy true:349
noisy false:1507
after delete: len(clean_dataset) 1280
after delete: len(noisy_dataset) 1856
epoch [92/200] batch [5/40] time 0.439 (0.439) data 0.308 (0.308) loss_x loss_x 1.7178 (1.2593) acc_x 68.7500 (71.2500) lr 1.1564e-03 eta 0:00:15
epoch [92/200] batch [10/40] time 0.474 (0.446) data 0.344 (0.316) loss_x loss_x 1.5703 (1.3874) acc_x 62.5000 (69.0625) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [15/40] time 0.461 (0.450) data 0.331 (0.320) loss_x loss_x 0.8574 (1.2515) acc_x 71.8750 (70.2083) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [20/40] time 0.368 (0.456) data 0.238 (0.326) loss_x loss_x 0.9707 (1.2494) acc_x 71.8750 (69.2188) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [25/40] time 0.569 (0.467) data 0.439 (0.337) loss_x loss_x 1.0635 (1.2249) acc_x 62.5000 (69.3750) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [30/40] time 0.406 (0.461) data 0.275 (0.331) loss_x loss_x 1.0752 (1.2308) acc_x 78.1250 (69.6875) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [35/40] time 0.444 (0.456) data 0.313 (0.326) loss_x loss_x 1.7158 (1.2383) acc_x 65.6250 (69.3750) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [40/40] time 0.431 (0.457) data 0.301 (0.326) loss_x loss_x 1.1377 (1.2469) acc_x 75.0000 (69.2188) lr 1.1564e-03 eta 0:00:00
epoch [92/200] batch [5/58] time 0.382 (0.456) data 0.251 (0.326) loss_u loss_u 0.7544 (0.8283) acc_u 28.1250 (20.6250) lr 1.1564e-03 eta 0:00:24
epoch [92/200] batch [10/58] time 0.435 (0.457) data 0.304 (0.326) loss_u loss_u 0.8213 (0.8310) acc_u 28.1250 (22.5000) lr 1.1564e-03 eta 0:00:21
epoch [92/200] batch [15/58] time 0.458 (0.458) data 0.327 (0.327) loss_u loss_u 0.8066 (0.8474) acc_u 28.1250 (19.5833) lr 1.1564e-03 eta 0:00:19
epoch [92/200] batch [20/58] time 0.386 (0.458) data 0.255 (0.327) loss_u loss_u 0.9282 (0.8475) acc_u 12.5000 (19.8438) lr 1.1564e-03 eta 0:00:17
epoch [92/200] batch [25/58] time 0.418 (0.460) data 0.287 (0.330) loss_u loss_u 0.9141 (0.8469) acc_u 9.3750 (19.3750) lr 1.1564e-03 eta 0:00:15
epoch [92/200] batch [30/58] time 0.421 (0.458) data 0.289 (0.328) loss_u loss_u 0.7959 (0.8431) acc_u 25.0000 (20.0000) lr 1.1564e-03 eta 0:00:12
epoch [92/200] batch [35/58] time 0.421 (0.457) data 0.290 (0.326) loss_u loss_u 0.8213 (0.8394) acc_u 25.0000 (20.6250) lr 1.1564e-03 eta 0:00:10
epoch [92/200] batch [40/58] time 0.517 (0.455) data 0.386 (0.324) loss_u loss_u 0.7847 (0.8365) acc_u 31.2500 (21.2500) lr 1.1564e-03 eta 0:00:08
epoch [92/200] batch [45/58] time 0.373 (0.453) data 0.241 (0.322) loss_u loss_u 0.8838 (0.8359) acc_u 12.5000 (21.0417) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [50/58] time 0.488 (0.452) data 0.357 (0.321) loss_u loss_u 0.8379 (0.8360) acc_u 21.8750 (20.8750) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [55/58] time 0.436 (0.452) data 0.305 (0.321) loss_u loss_u 0.7544 (0.8327) acc_u 31.2500 (21.3068) lr 1.1564e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1568
confident_label rate tensor(0.3919, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1229
clean true:1225
clean false:4
clean_rate:0.9967453213995118
noisy true:343
noisy false:1564
after delete: len(clean_dataset) 1229
after delete: len(noisy_dataset) 1907
epoch [93/200] batch [5/38] time 0.552 (0.462) data 0.421 (0.332) loss_x loss_x 0.8315 (1.1634) acc_x 78.1250 (66.2500) lr 1.1409e-03 eta 0:00:15
epoch [93/200] batch [10/38] time 0.477 (0.455) data 0.347 (0.325) loss_x loss_x 0.8765 (1.2155) acc_x 81.2500 (65.9375) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [15/38] time 0.443 (0.449) data 0.313 (0.319) loss_x loss_x 1.1230 (1.1977) acc_x 65.6250 (66.2500) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [20/38] time 0.430 (0.453) data 0.300 (0.322) loss_x loss_x 1.2090 (1.2125) acc_x 65.6250 (66.4062) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [25/38] time 0.468 (0.459) data 0.337 (0.329) loss_x loss_x 1.0225 (1.2127) acc_x 78.1250 (66.8750) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [30/38] time 0.423 (0.458) data 0.293 (0.328) loss_x loss_x 1.5654 (1.2306) acc_x 59.3750 (66.6667) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [35/38] time 0.377 (0.453) data 0.247 (0.322) loss_x loss_x 1.0029 (1.2422) acc_x 78.1250 (67.2321) lr 1.1409e-03 eta 0:00:01
epoch [93/200] batch [5/59] time 0.617 (0.445) data 0.486 (0.314) loss_u loss_u 0.8784 (0.8370) acc_u 15.6250 (17.5000) lr 1.1409e-03 eta 0:00:24
epoch [93/200] batch [10/59] time 0.425 (0.442) data 0.294 (0.312) loss_u loss_u 0.8936 (0.8337) acc_u 12.5000 (19.0625) lr 1.1409e-03 eta 0:00:21
epoch [93/200] batch [15/59] time 0.506 (0.445) data 0.375 (0.315) loss_u loss_u 0.8149 (0.8426) acc_u 25.0000 (19.1667) lr 1.1409e-03 eta 0:00:19
epoch [93/200] batch [20/59] time 0.464 (0.443) data 0.333 (0.312) loss_u loss_u 0.8496 (0.8336) acc_u 15.6250 (20.3125) lr 1.1409e-03 eta 0:00:17
epoch [93/200] batch [25/59] time 0.576 (0.447) data 0.445 (0.317) loss_u loss_u 0.8027 (0.8375) acc_u 28.1250 (20.0000) lr 1.1409e-03 eta 0:00:15
epoch [93/200] batch [30/59] time 0.531 (0.450) data 0.399 (0.319) loss_u loss_u 0.7617 (0.8358) acc_u 28.1250 (20.4167) lr 1.1409e-03 eta 0:00:13
epoch [93/200] batch [35/59] time 0.396 (0.449) data 0.265 (0.318) loss_u loss_u 0.7886 (0.8310) acc_u 25.0000 (21.1607) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [40/59] time 0.392 (0.447) data 0.261 (0.317) loss_u loss_u 0.8281 (0.8310) acc_u 18.7500 (21.0156) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [45/59] time 0.508 (0.447) data 0.377 (0.316) loss_u loss_u 0.8003 (0.8318) acc_u 25.0000 (21.1111) lr 1.1409e-03 eta 0:00:06
epoch [93/200] batch [50/59] time 0.556 (0.449) data 0.422 (0.318) loss_u loss_u 0.8535 (0.8344) acc_u 18.7500 (20.8125) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [55/59] time 0.616 (0.450) data 0.483 (0.319) loss_u loss_u 0.9058 (0.8342) acc_u 15.6250 (20.7386) lr 1.1409e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1503
confident_label rate tensor(0.4184, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1312
clean true:1307
clean false:5
clean_rate:0.9961890243902439
noisy true:326
noisy false:1498
after delete: len(clean_dataset) 1312
after delete: len(noisy_dataset) 1824
epoch [94/200] batch [5/41] time 0.571 (0.495) data 0.442 (0.365) loss_x loss_x 1.3232 (1.0824) acc_x 59.3750 (70.6250) lr 1.1253e-03 eta 0:00:17
epoch [94/200] batch [10/41] time 0.369 (0.461) data 0.239 (0.331) loss_x loss_x 1.3184 (1.2388) acc_x 68.7500 (67.8125) lr 1.1253e-03 eta 0:00:14
epoch [94/200] batch [15/41] time 0.579 (0.478) data 0.448 (0.348) loss_x loss_x 1.1445 (1.2566) acc_x 68.7500 (67.7083) lr 1.1253e-03 eta 0:00:12
epoch [94/200] batch [20/41] time 0.486 (0.462) data 0.356 (0.332) loss_x loss_x 1.0752 (1.2404) acc_x 68.7500 (67.5000) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [25/41] time 0.438 (0.460) data 0.308 (0.329) loss_x loss_x 1.4639 (1.2511) acc_x 65.6250 (68.5000) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [30/41] time 0.379 (0.465) data 0.249 (0.335) loss_x loss_x 0.9609 (1.2322) acc_x 75.0000 (69.0625) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [35/41] time 0.434 (0.457) data 0.304 (0.327) loss_x loss_x 1.2852 (1.2510) acc_x 65.6250 (68.5714) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [40/41] time 0.393 (0.453) data 0.263 (0.323) loss_x loss_x 1.3623 (1.2433) acc_x 62.5000 (68.7500) lr 1.1253e-03 eta 0:00:00
epoch [94/200] batch [5/57] time 0.493 (0.453) data 0.362 (0.323) loss_u loss_u 0.8442 (0.8285) acc_u 18.7500 (23.1250) lr 1.1253e-03 eta 0:00:23
epoch [94/200] batch [10/57] time 0.353 (0.449) data 0.222 (0.319) loss_u loss_u 0.8569 (0.8383) acc_u 18.7500 (21.8750) lr 1.1253e-03 eta 0:00:21
epoch [94/200] batch [15/57] time 0.504 (0.449) data 0.373 (0.319) loss_u loss_u 0.8638 (0.8412) acc_u 12.5000 (19.7917) lr 1.1253e-03 eta 0:00:18
epoch [94/200] batch [20/57] time 0.469 (0.448) data 0.339 (0.317) loss_u loss_u 0.8828 (0.8403) acc_u 15.6250 (20.4688) lr 1.1253e-03 eta 0:00:16
epoch [94/200] batch [25/57] time 0.566 (0.447) data 0.435 (0.316) loss_u loss_u 0.8257 (0.8458) acc_u 21.8750 (19.6250) lr 1.1253e-03 eta 0:00:14
epoch [94/200] batch [30/57] time 0.314 (0.442) data 0.182 (0.312) loss_u loss_u 0.7856 (0.8453) acc_u 28.1250 (19.6875) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [35/57] time 0.360 (0.442) data 0.229 (0.311) loss_u loss_u 0.8291 (0.8456) acc_u 18.7500 (19.6429) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [40/57] time 0.788 (0.447) data 0.658 (0.316) loss_u loss_u 0.9175 (0.8458) acc_u 15.6250 (19.4531) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [45/57] time 0.422 (0.447) data 0.292 (0.317) loss_u loss_u 0.8555 (0.8446) acc_u 15.6250 (19.5139) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [50/57] time 0.384 (0.446) data 0.254 (0.315) loss_u loss_u 0.8101 (0.8429) acc_u 31.2500 (20.1250) lr 1.1253e-03 eta 0:00:03
epoch [94/200] batch [55/57] time 0.361 (0.446) data 0.231 (0.315) loss_u loss_u 0.9429 (0.8442) acc_u 6.2500 (19.7727) lr 1.1253e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1479
confident_label rate tensor(0.4165, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1306
clean true:1303
clean false:3
clean_rate:0.9977029096477795
noisy true:354
noisy false:1476
after delete: len(clean_dataset) 1306
after delete: len(noisy_dataset) 1830
epoch [95/200] batch [5/40] time 0.408 (0.415) data 0.276 (0.284) loss_x loss_x 1.8623 (1.5461) acc_x 59.3750 (61.2500) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [10/40] time 0.481 (0.454) data 0.352 (0.323) loss_x loss_x 1.2383 (1.4242) acc_x 71.8750 (65.9375) lr 1.1097e-03 eta 0:00:13
epoch [95/200] batch [15/40] time 0.508 (0.465) data 0.378 (0.334) loss_x loss_x 1.2520 (1.4441) acc_x 59.3750 (65.0000) lr 1.1097e-03 eta 0:00:11
epoch [95/200] batch [20/40] time 0.411 (0.461) data 0.281 (0.331) loss_x loss_x 0.7349 (1.3144) acc_x 81.2500 (68.2812) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [25/40] time 0.424 (0.457) data 0.294 (0.326) loss_x loss_x 1.6064 (1.3178) acc_x 59.3750 (68.1250) lr 1.1097e-03 eta 0:00:06
epoch [95/200] batch [30/40] time 0.364 (0.445) data 0.234 (0.315) loss_x loss_x 1.3750 (1.3384) acc_x 62.5000 (67.2917) lr 1.1097e-03 eta 0:00:04
epoch [95/200] batch [35/40] time 0.524 (0.450) data 0.391 (0.319) loss_x loss_x 1.3018 (1.3057) acc_x 65.6250 (67.7679) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [40/40] time 0.516 (0.453) data 0.386 (0.323) loss_x loss_x 1.1631 (1.3207) acc_x 75.0000 (67.6562) lr 1.1097e-03 eta 0:00:00
epoch [95/200] batch [5/57] time 0.355 (0.455) data 0.222 (0.324) loss_u loss_u 0.8403 (0.8555) acc_u 18.7500 (18.7500) lr 1.1097e-03 eta 0:00:23
epoch [95/200] batch [10/57] time 0.465 (0.458) data 0.334 (0.327) loss_u loss_u 0.7910 (0.8468) acc_u 25.0000 (20.6250) lr 1.1097e-03 eta 0:00:21
epoch [95/200] batch [15/57] time 0.394 (0.454) data 0.263 (0.323) loss_u loss_u 0.8579 (0.8443) acc_u 21.8750 (21.6667) lr 1.1097e-03 eta 0:00:19
epoch [95/200] batch [20/57] time 0.528 (0.454) data 0.398 (0.323) loss_u loss_u 0.8062 (0.8392) acc_u 25.0000 (21.7188) lr 1.1097e-03 eta 0:00:16
epoch [95/200] batch [25/57] time 0.432 (0.453) data 0.301 (0.323) loss_u loss_u 0.9043 (0.8365) acc_u 15.6250 (22.5000) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [30/57] time 0.374 (0.454) data 0.243 (0.324) loss_u loss_u 0.8359 (0.8376) acc_u 25.0000 (22.0833) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [35/57] time 0.490 (0.453) data 0.359 (0.323) loss_u loss_u 0.8335 (0.8367) acc_u 25.0000 (22.4107) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [40/57] time 0.371 (0.450) data 0.239 (0.319) loss_u loss_u 0.8838 (0.8352) acc_u 9.3750 (22.1875) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [45/57] time 0.337 (0.450) data 0.206 (0.319) loss_u loss_u 0.8760 (0.8394) acc_u 12.5000 (21.5278) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [50/57] time 0.415 (0.449) data 0.284 (0.318) loss_u loss_u 0.8940 (0.8432) acc_u 12.5000 (21.0000) lr 1.1097e-03 eta 0:00:03
epoch [95/200] batch [55/57] time 0.420 (0.447) data 0.289 (0.316) loss_u loss_u 0.8628 (0.8436) acc_u 21.8750 (21.1364) lr 1.1097e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1498
confident_label rate tensor(0.4101, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1286
clean true:1285
clean false:1
clean_rate:0.9992223950233281
noisy true:353
noisy false:1497
after delete: len(clean_dataset) 1286
after delete: len(noisy_dataset) 1850
epoch [96/200] batch [5/40] time 0.404 (0.420) data 0.274 (0.289) loss_x loss_x 1.2881 (1.4855) acc_x 71.8750 (66.8750) lr 1.0941e-03 eta 0:00:14
epoch [96/200] batch [10/40] time 0.368 (0.440) data 0.237 (0.309) loss_x loss_x 1.3154 (1.3185) acc_x 71.8750 (70.0000) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [15/40] time 0.422 (0.429) data 0.292 (0.299) loss_x loss_x 1.3691 (1.2732) acc_x 62.5000 (69.3750) lr 1.0941e-03 eta 0:00:10
epoch [96/200] batch [20/40] time 0.374 (0.439) data 0.243 (0.309) loss_x loss_x 1.4932 (1.2504) acc_x 68.7500 (70.3125) lr 1.0941e-03 eta 0:00:08
epoch [96/200] batch [25/40] time 0.407 (0.435) data 0.277 (0.304) loss_x loss_x 1.7158 (1.2411) acc_x 50.0000 (70.1250) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [30/40] time 0.385 (0.431) data 0.256 (0.301) loss_x loss_x 1.1123 (1.2435) acc_x 71.8750 (70.0000) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [35/40] time 0.410 (0.430) data 0.281 (0.300) loss_x loss_x 1.1113 (1.2348) acc_x 71.8750 (70.1786) lr 1.0941e-03 eta 0:00:02
epoch [96/200] batch [40/40] time 0.521 (0.439) data 0.392 (0.309) loss_x loss_x 1.2002 (1.2042) acc_x 62.5000 (70.4688) lr 1.0941e-03 eta 0:00:00
epoch [96/200] batch [5/57] time 0.574 (0.448) data 0.443 (0.318) loss_u loss_u 0.8633 (0.8241) acc_u 21.8750 (25.6250) lr 1.0941e-03 eta 0:00:23
epoch [96/200] batch [10/57] time 0.380 (0.447) data 0.248 (0.316) loss_u loss_u 0.9194 (0.8361) acc_u 12.5000 (23.1250) lr 1.0941e-03 eta 0:00:21
epoch [96/200] batch [15/57] time 0.391 (0.444) data 0.261 (0.313) loss_u loss_u 0.7827 (0.8314) acc_u 28.1250 (22.0833) lr 1.0941e-03 eta 0:00:18
epoch [96/200] batch [20/57] time 0.419 (0.442) data 0.289 (0.312) loss_u loss_u 0.8750 (0.8407) acc_u 12.5000 (20.0000) lr 1.0941e-03 eta 0:00:16
epoch [96/200] batch [25/57] time 0.468 (0.440) data 0.337 (0.309) loss_u loss_u 0.8193 (0.8439) acc_u 15.6250 (19.6250) lr 1.0941e-03 eta 0:00:14
epoch [96/200] batch [30/57] time 0.483 (0.444) data 0.352 (0.314) loss_u loss_u 0.7920 (0.8344) acc_u 28.1250 (20.4167) lr 1.0941e-03 eta 0:00:11
epoch [96/200] batch [35/57] time 0.467 (0.443) data 0.336 (0.313) loss_u loss_u 0.8281 (0.8337) acc_u 21.8750 (20.4464) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [40/57] time 0.537 (0.444) data 0.406 (0.313) loss_u loss_u 0.8091 (0.8338) acc_u 21.8750 (20.3906) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [45/57] time 0.448 (0.444) data 0.317 (0.313) loss_u loss_u 0.8955 (0.8330) acc_u 12.5000 (20.5556) lr 1.0941e-03 eta 0:00:05
epoch [96/200] batch [50/57] time 0.399 (0.445) data 0.267 (0.314) loss_u loss_u 0.8882 (0.8311) acc_u 12.5000 (20.8125) lr 1.0941e-03 eta 0:00:03
epoch [96/200] batch [55/57] time 0.457 (0.445) data 0.326 (0.314) loss_u loss_u 0.9438 (0.8312) acc_u 6.2500 (20.7955) lr 1.0941e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1513
confident_label rate tensor(0.4072, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1277
clean true:1276
clean false:1
clean_rate:0.9992169146436961
noisy true:347
noisy false:1512
after delete: len(clean_dataset) 1277
after delete: len(noisy_dataset) 1859
epoch [97/200] batch [5/39] time 0.474 (0.454) data 0.343 (0.323) loss_x loss_x 0.8921 (1.0382) acc_x 71.8750 (71.8750) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [10/39] time 0.389 (0.449) data 0.259 (0.319) loss_x loss_x 0.7041 (1.0490) acc_x 87.5000 (74.0625) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [15/39] time 0.363 (0.445) data 0.233 (0.314) loss_x loss_x 0.9849 (1.1352) acc_x 78.1250 (71.6667) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [20/39] time 0.390 (0.443) data 0.260 (0.312) loss_x loss_x 1.2373 (1.1858) acc_x 71.8750 (71.5625) lr 1.0785e-03 eta 0:00:08
epoch [97/200] batch [25/39] time 0.540 (0.454) data 0.409 (0.323) loss_x loss_x 1.3652 (1.2109) acc_x 62.5000 (70.3750) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [30/39] time 0.410 (0.463) data 0.278 (0.332) loss_x loss_x 1.2686 (1.2132) acc_x 68.7500 (70.5208) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [35/39] time 0.417 (0.458) data 0.286 (0.327) loss_x loss_x 0.6016 (1.2051) acc_x 87.5000 (70.6250) lr 1.0785e-03 eta 0:00:01
epoch [97/200] batch [5/58] time 0.358 (0.448) data 0.226 (0.317) loss_u loss_u 0.8149 (0.8268) acc_u 18.7500 (22.5000) lr 1.0785e-03 eta 0:00:23
epoch [97/200] batch [10/58] time 0.530 (0.450) data 0.398 (0.319) loss_u loss_u 0.8613 (0.8216) acc_u 15.6250 (23.4375) lr 1.0785e-03 eta 0:00:21
epoch [97/200] batch [15/58] time 0.467 (0.446) data 0.335 (0.315) loss_u loss_u 0.8354 (0.8326) acc_u 18.7500 (21.8750) lr 1.0785e-03 eta 0:00:19
epoch [97/200] batch [20/58] time 0.547 (0.449) data 0.415 (0.318) loss_u loss_u 0.8223 (0.8339) acc_u 28.1250 (21.7188) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [25/58] time 0.455 (0.450) data 0.322 (0.319) loss_u loss_u 0.8584 (0.8369) acc_u 21.8750 (21.5000) lr 1.0785e-03 eta 0:00:14
epoch [97/200] batch [30/58] time 0.406 (0.453) data 0.274 (0.322) loss_u loss_u 0.8154 (0.8358) acc_u 21.8750 (21.2500) lr 1.0785e-03 eta 0:00:12
epoch [97/200] batch [35/58] time 0.486 (0.454) data 0.354 (0.323) loss_u loss_u 0.7373 (0.8309) acc_u 34.3750 (21.8750) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [40/58] time 0.430 (0.449) data 0.299 (0.318) loss_u loss_u 0.9492 (0.8352) acc_u 3.1250 (21.4844) lr 1.0785e-03 eta 0:00:08
epoch [97/200] batch [45/58] time 0.389 (0.450) data 0.257 (0.319) loss_u loss_u 0.8408 (0.8344) acc_u 21.8750 (21.5278) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [50/58] time 0.427 (0.449) data 0.295 (0.317) loss_u loss_u 0.8784 (0.8373) acc_u 12.5000 (20.9375) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [55/58] time 0.322 (0.448) data 0.191 (0.316) loss_u loss_u 0.7817 (0.8357) acc_u 28.1250 (21.0227) lr 1.0785e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1507
confident_label rate tensor(0.4085, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1281
clean true:1280
clean false:1
clean_rate:0.9992193598750976
noisy true:349
noisy false:1506
after delete: len(clean_dataset) 1281
after delete: len(noisy_dataset) 1855
epoch [98/200] batch [5/40] time 0.460 (0.485) data 0.330 (0.355) loss_x loss_x 1.2129 (1.3802) acc_x 68.7500 (68.1250) lr 1.0628e-03 eta 0:00:16
epoch [98/200] batch [10/40] time 0.397 (0.439) data 0.267 (0.309) loss_x loss_x 1.1299 (1.2500) acc_x 75.0000 (70.0000) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [15/40] time 0.364 (0.437) data 0.234 (0.307) loss_x loss_x 1.0518 (1.2677) acc_x 84.3750 (70.8333) lr 1.0628e-03 eta 0:00:10
epoch [98/200] batch [20/40] time 0.421 (0.442) data 0.291 (0.311) loss_x loss_x 0.7334 (1.2303) acc_x 81.2500 (72.1875) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [25/40] time 0.422 (0.442) data 0.291 (0.312) loss_x loss_x 1.6367 (1.2603) acc_x 59.3750 (71.5000) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [30/40] time 0.425 (0.442) data 0.294 (0.312) loss_x loss_x 0.9116 (1.2457) acc_x 78.1250 (70.7292) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [35/40] time 0.538 (0.450) data 0.407 (0.320) loss_x loss_x 1.6924 (1.2532) acc_x 59.3750 (70.5357) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [40/40] time 0.419 (0.450) data 0.289 (0.319) loss_x loss_x 1.0156 (1.2431) acc_x 75.0000 (70.5469) lr 1.0628e-03 eta 0:00:00
epoch [98/200] batch [5/57] time 0.388 (0.444) data 0.256 (0.314) loss_u loss_u 0.8071 (0.7922) acc_u 25.0000 (26.8750) lr 1.0628e-03 eta 0:00:23
epoch [98/200] batch [10/57] time 0.443 (0.442) data 0.311 (0.312) loss_u loss_u 0.8701 (0.8134) acc_u 18.7500 (25.3125) lr 1.0628e-03 eta 0:00:20
epoch [98/200] batch [15/57] time 0.362 (0.434) data 0.230 (0.304) loss_u loss_u 0.8379 (0.8281) acc_u 21.8750 (22.7083) lr 1.0628e-03 eta 0:00:18
epoch [98/200] batch [20/57] time 0.350 (0.432) data 0.219 (0.302) loss_u loss_u 0.9312 (0.8336) acc_u 9.3750 (21.4062) lr 1.0628e-03 eta 0:00:16
epoch [98/200] batch [25/57] time 0.415 (0.430) data 0.284 (0.300) loss_u loss_u 0.8467 (0.8376) acc_u 21.8750 (21.5000) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [30/57] time 0.452 (0.431) data 0.321 (0.301) loss_u loss_u 0.8213 (0.8346) acc_u 25.0000 (21.7708) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [35/57] time 0.370 (0.431) data 0.239 (0.300) loss_u loss_u 0.8315 (0.8301) acc_u 21.8750 (22.5000) lr 1.0628e-03 eta 0:00:09
epoch [98/200] batch [40/57] time 0.556 (0.438) data 0.424 (0.307) loss_u loss_u 0.8965 (0.8364) acc_u 15.6250 (21.4062) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [45/57] time 0.506 (0.439) data 0.376 (0.309) loss_u loss_u 0.8516 (0.8360) acc_u 12.5000 (21.2500) lr 1.0628e-03 eta 0:00:05
epoch [98/200] batch [50/57] time 0.596 (0.441) data 0.465 (0.310) loss_u loss_u 0.8638 (0.8368) acc_u 15.6250 (21.0000) lr 1.0628e-03 eta 0:00:03
epoch [98/200] batch [55/57] time 0.486 (0.442) data 0.354 (0.311) loss_u loss_u 0.8252 (0.8340) acc_u 21.8750 (21.3068) lr 1.0628e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1499
confident_label rate tensor(0.4136, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1297
clean true:1294
clean false:3
clean_rate:0.9976869699306091
noisy true:343
noisy false:1496
after delete: len(clean_dataset) 1297
after delete: len(noisy_dataset) 1839
epoch [99/200] batch [5/40] time 0.464 (0.481) data 0.335 (0.351) loss_x loss_x 1.5547 (1.2209) acc_x 56.2500 (71.8750) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [10/40] time 0.458 (0.469) data 0.327 (0.339) loss_x loss_x 0.9727 (1.1567) acc_x 71.8750 (70.0000) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [15/40] time 0.548 (0.460) data 0.416 (0.330) loss_x loss_x 1.2041 (1.1301) acc_x 78.1250 (70.8333) lr 1.0471e-03 eta 0:00:11
epoch [99/200] batch [20/40] time 0.477 (0.471) data 0.346 (0.340) loss_x loss_x 1.2188 (1.1508) acc_x 75.0000 (71.2500) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [25/40] time 0.371 (0.457) data 0.241 (0.327) loss_x loss_x 1.2363 (1.1629) acc_x 68.7500 (71.5000) lr 1.0471e-03 eta 0:00:06
epoch [99/200] batch [30/40] time 0.345 (0.451) data 0.214 (0.320) loss_x loss_x 1.1074 (1.1792) acc_x 68.7500 (70.8333) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [35/40] time 0.416 (0.452) data 0.285 (0.322) loss_x loss_x 1.3008 (1.2098) acc_x 59.3750 (69.7321) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [40/40] time 0.434 (0.450) data 0.303 (0.320) loss_x loss_x 1.6172 (1.2086) acc_x 62.5000 (69.4531) lr 1.0471e-03 eta 0:00:00
epoch [99/200] batch [5/57] time 0.437 (0.449) data 0.306 (0.318) loss_u loss_u 0.7397 (0.8263) acc_u 34.3750 (20.6250) lr 1.0471e-03 eta 0:00:23
epoch [99/200] batch [10/57] time 0.465 (0.445) data 0.333 (0.314) loss_u loss_u 0.8052 (0.8400) acc_u 25.0000 (19.0625) lr 1.0471e-03 eta 0:00:20
epoch [99/200] batch [15/57] time 0.365 (0.442) data 0.232 (0.311) loss_u loss_u 0.8491 (0.8340) acc_u 18.7500 (20.2083) lr 1.0471e-03 eta 0:00:18
epoch [99/200] batch [20/57] time 0.348 (0.437) data 0.217 (0.306) loss_u loss_u 0.8169 (0.8286) acc_u 18.7500 (21.8750) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [25/57] time 0.370 (0.433) data 0.239 (0.302) loss_u loss_u 0.8599 (0.8347) acc_u 12.5000 (21.1250) lr 1.0471e-03 eta 0:00:13
epoch [99/200] batch [30/57] time 0.363 (0.435) data 0.232 (0.304) loss_u loss_u 0.8579 (0.8311) acc_u 12.5000 (21.5625) lr 1.0471e-03 eta 0:00:11
epoch [99/200] batch [35/57] time 0.421 (0.436) data 0.290 (0.305) loss_u loss_u 0.7681 (0.8285) acc_u 31.2500 (22.1429) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [40/57] time 0.542 (0.439) data 0.411 (0.308) loss_u loss_u 0.8193 (0.8292) acc_u 21.8750 (22.2656) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [45/57] time 0.386 (0.441) data 0.254 (0.310) loss_u loss_u 0.8296 (0.8295) acc_u 18.7500 (21.8750) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [50/57] time 0.434 (0.439) data 0.303 (0.308) loss_u loss_u 0.8423 (0.8304) acc_u 25.0000 (21.9375) lr 1.0471e-03 eta 0:00:03
epoch [99/200] batch [55/57] time 0.519 (0.440) data 0.388 (0.310) loss_u loss_u 0.7905 (0.8301) acc_u 31.2500 (21.9318) lr 1.0471e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1484
confident_label rate tensor(0.4190, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1314
clean true:1313
clean false:1
clean_rate:0.9992389649923896
noisy true:339
noisy false:1483
all clean rate:  [0.9976905311778291, 0.9960264900662251, 0.9989384288747346, 1.0, 1.0, 0.9970059880239521, 0.9990253411306043, 1.0, 0.9970731707317073, 0.9980842911877394, 0.9990328820116054, 0.999009900990099, 0.9971590909090909, 0.996415770609319, 0.9981132075471698, 0.9982046678635548, 0.9981343283582089, 0.99909338168631, 0.9964028776978417, 0.9990723562152134, 0.9981412639405205, 0.9957044673539519, 0.9991228070175439, 0.9991111111111111, 0.9982316534040672, 0.9991266375545852, 0.9991511035653651, 0.9964476021314387, 0.998212689901698, 0.9974271012006861, 0.9991319444444444, 0.9982847341337907, 0.9973684210526316, 0.9966527196652719, 0.9974533106960951, 0.9983007646559049, 1.0, 0.9991304347826087, 0.9991356957649092, 0.9974337040205303, 0.9991467576791809, 0.9983633387888707, 0.9991735537190083, 0.9965724078834619, 0.9975124378109452, 0.9974937343358395, 0.9966804979253112, 0.996694214876033, 0.9975308641975309, 0.9991546914623838, 0.9983739837398374, 0.9991869918699187, 0.995928338762215, 0.9983291562238931, 0.9974662162162162, 0.9975903614457832, 0.9959183673469387, 0.9975206611570248, 0.998320738874895, 0.9951534733441034, 0.9974726200505476, 0.997588424437299, 0.9967715899919289, 0.9975247524752475, 0.9991769547325103, 0.9975629569455727, 0.9975349219391948, 0.9975786924939467, 0.9991922455573505, 0.9984101748807631, 0.9967611336032388, 0.9984164687252574, 0.9984202211690363, 0.9983883964544722, 0.9975980784627703, 0.9968127490039841, 0.9983961507618284, 0.9992163009404389, 0.9983831851253031, 1.0, 0.9968968192397207, 0.9984114376489277, 0.9968895800933126, 0.9992248062015504, 0.9984532095901005, 0.9975728155339806, 0.9968454258675079, 0.9992088607594937, 0.9984579799537394, 0.9976340694006309, 0.9976762199845082, 0.99765625, 0.9967453213995118, 0.9961890243902439, 0.9977029096477795, 0.9992223950233281, 0.9992169146436961, 0.9992193598750976, 0.9976869699306091, 0.9992389649923896]
after delete: len(clean_dataset) 1314
after delete: len(noisy_dataset) 1822
epoch [100/200] batch [5/41] time 0.530 (0.477) data 0.400 (0.346) loss_x loss_x 1.0371 (1.3218) acc_x 75.0000 (65.6250) lr 1.0314e-03 eta 0:00:17
epoch [100/200] batch [10/41] time 0.428 (0.461) data 0.298 (0.331) loss_x loss_x 1.1191 (1.1336) acc_x 71.8750 (71.2500) lr 1.0314e-03 eta 0:00:14
epoch [100/200] batch [15/41] time 0.406 (0.452) data 0.275 (0.322) loss_x loss_x 1.3535 (1.1784) acc_x 59.3750 (69.1667) lr 1.0314e-03 eta 0:00:11
epoch [100/200] batch [20/41] time 0.517 (0.458) data 0.386 (0.328) loss_x loss_x 1.5195 (1.1726) acc_x 59.3750 (69.8438) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [25/41] time 0.391 (0.458) data 0.261 (0.327) loss_x loss_x 1.1143 (1.1596) acc_x 71.8750 (70.3750) lr 1.0314e-03 eta 0:00:07
epoch [100/200] batch [30/41] time 0.438 (0.456) data 0.307 (0.325) loss_x loss_x 1.2363 (1.1541) acc_x 71.8750 (70.4167) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [35/41] time 0.548 (0.458) data 0.417 (0.327) loss_x loss_x 1.4951 (1.1775) acc_x 68.7500 (69.4643) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [40/41] time 0.448 (0.462) data 0.317 (0.331) loss_x loss_x 1.5439 (1.1771) acc_x 65.6250 (69.9219) lr 1.0314e-03 eta 0:00:00
epoch [100/200] batch [5/56] time 0.324 (0.461) data 0.193 (0.330) loss_u loss_u 0.8267 (0.8355) acc_u 15.6250 (20.0000) lr 1.0314e-03 eta 0:00:23
epoch [100/200] batch [10/56] time 0.558 (0.460) data 0.426 (0.329) loss_u loss_u 0.7393 (0.8156) acc_u 31.2500 (21.5625) lr 1.0314e-03 eta 0:00:21
epoch [100/200] batch [15/56] time 0.477 (0.462) data 0.344 (0.331) loss_u loss_u 0.8398 (0.8319) acc_u 25.0000 (20.6250) lr 1.0314e-03 eta 0:00:18
epoch [100/200] batch [20/56] time 0.533 (0.467) data 0.401 (0.336) loss_u loss_u 0.8818 (0.8353) acc_u 18.7500 (20.6250) lr 1.0314e-03 eta 0:00:16
epoch [100/200] batch [25/56] time 0.383 (0.467) data 0.250 (0.336) loss_u loss_u 0.8730 (0.8375) acc_u 12.5000 (20.2500) lr 1.0314e-03 eta 0:00:14
epoch [100/200] batch [30/56] time 0.490 (0.464) data 0.358 (0.332) loss_u loss_u 0.8555 (0.8347) acc_u 18.7500 (20.5208) lr 1.0314e-03 eta 0:00:12
epoch [100/200] batch [35/56] time 0.431 (0.463) data 0.300 (0.332) loss_u loss_u 0.8716 (0.8363) acc_u 15.6250 (20.4464) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [40/56] time 0.352 (0.459) data 0.221 (0.328) loss_u loss_u 0.8638 (0.8367) acc_u 15.6250 (20.3906) lr 1.0314e-03 eta 0:00:07
epoch [100/200] batch [45/56] time 0.375 (0.458) data 0.242 (0.327) loss_u loss_u 0.7690 (0.8348) acc_u 31.2500 (20.6250) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [50/56] time 0.441 (0.459) data 0.309 (0.328) loss_u loss_u 0.8589 (0.8354) acc_u 21.8750 (20.7500) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [55/56] time 0.450 (0.458) data 0.319 (0.326) loss_u loss_u 0.8857 (0.8360) acc_u 15.6250 (20.7386) lr 1.0314e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1496
confident_label rate tensor(0.4145, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1300
clean true:1298
clean false:2
clean_rate:0.9984615384615385
noisy true:342
noisy false:1494
after delete: len(clean_dataset) 1300
after delete: len(noisy_dataset) 1836
epoch [101/200] batch [5/40] time 0.416 (0.422) data 0.286 (0.292) loss_x loss_x 1.2100 (1.1803) acc_x 62.5000 (71.8750) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [10/40] time 0.446 (0.420) data 0.317 (0.291) loss_x loss_x 1.3457 (1.2316) acc_x 75.0000 (69.6875) lr 1.0157e-03 eta 0:00:12
epoch [101/200] batch [15/40] time 0.423 (0.433) data 0.294 (0.304) loss_x loss_x 1.1533 (1.2002) acc_x 62.5000 (69.5833) lr 1.0157e-03 eta 0:00:10
epoch [101/200] batch [20/40] time 0.350 (0.439) data 0.220 (0.309) loss_x loss_x 1.0420 (1.1763) acc_x 75.0000 (70.4688) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [25/40] time 0.393 (0.438) data 0.263 (0.308) loss_x loss_x 1.3105 (1.1817) acc_x 71.8750 (70.3750) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [30/40] time 0.360 (0.434) data 0.231 (0.304) loss_x loss_x 1.0176 (1.1817) acc_x 65.6250 (69.8958) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [35/40] time 0.507 (0.442) data 0.377 (0.312) loss_x loss_x 1.0547 (1.1942) acc_x 81.2500 (69.8214) lr 1.0157e-03 eta 0:00:02
epoch [101/200] batch [40/40] time 0.426 (0.445) data 0.296 (0.315) loss_x loss_x 0.8066 (1.1716) acc_x 87.5000 (70.3906) lr 1.0157e-03 eta 0:00:00
epoch [101/200] batch [5/57] time 0.397 (0.443) data 0.267 (0.313) loss_u loss_u 0.8706 (0.8356) acc_u 12.5000 (19.3750) lr 1.0157e-03 eta 0:00:23
epoch [101/200] batch [10/57] time 0.338 (0.442) data 0.207 (0.312) loss_u loss_u 0.8955 (0.8413) acc_u 12.5000 (18.4375) lr 1.0157e-03 eta 0:00:20
epoch [101/200] batch [15/57] time 0.511 (0.439) data 0.380 (0.309) loss_u loss_u 0.8311 (0.8329) acc_u 18.7500 (21.6667) lr 1.0157e-03 eta 0:00:18
epoch [101/200] batch [20/57] time 0.492 (0.441) data 0.362 (0.311) loss_u loss_u 0.7842 (0.8340) acc_u 21.8750 (21.0938) lr 1.0157e-03 eta 0:00:16
epoch [101/200] batch [25/57] time 0.380 (0.444) data 0.249 (0.314) loss_u loss_u 0.7954 (0.8281) acc_u 25.0000 (21.8750) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [30/57] time 0.372 (0.441) data 0.241 (0.311) loss_u loss_u 0.8218 (0.8279) acc_u 21.8750 (22.1875) lr 1.0157e-03 eta 0:00:11
epoch [101/200] batch [35/57] time 0.340 (0.436) data 0.210 (0.306) loss_u loss_u 0.7866 (0.8298) acc_u 21.8750 (21.7857) lr 1.0157e-03 eta 0:00:09
epoch [101/200] batch [40/57] time 0.504 (0.436) data 0.374 (0.306) loss_u loss_u 0.8584 (0.8325) acc_u 18.7500 (21.7188) lr 1.0157e-03 eta 0:00:07
epoch [101/200] batch [45/57] time 0.557 (0.436) data 0.427 (0.306) loss_u loss_u 0.9194 (0.8350) acc_u 9.3750 (21.4583) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [50/57] time 0.474 (0.436) data 0.342 (0.306) loss_u loss_u 0.8169 (0.8375) acc_u 21.8750 (21.1875) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [55/57] time 0.446 (0.437) data 0.314 (0.306) loss_u loss_u 0.8374 (0.8374) acc_u 15.6250 (21.1932) lr 1.0157e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1480
confident_label rate tensor(0.4142, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1299
clean true:1299
clean false:0
clean_rate:1.0
noisy true:357
noisy false:1480
after delete: len(clean_dataset) 1299
after delete: len(noisy_dataset) 1837
epoch [102/200] batch [5/40] time 0.622 (0.472) data 0.492 (0.342) loss_x loss_x 1.1230 (1.0049) acc_x 75.0000 (73.7500) lr 1.0000e-03 eta 0:00:16
epoch [102/200] batch [10/40] time 0.370 (0.445) data 0.240 (0.315) loss_x loss_x 1.6504 (1.1015) acc_x 50.0000 (71.5625) lr 1.0000e-03 eta 0:00:13
epoch [102/200] batch [15/40] time 0.433 (0.433) data 0.302 (0.303) loss_x loss_x 1.1172 (1.0938) acc_x 68.7500 (71.2500) lr 1.0000e-03 eta 0:00:10
epoch [102/200] batch [20/40] time 0.556 (0.446) data 0.425 (0.316) loss_x loss_x 0.8247 (1.1230) acc_x 81.2500 (70.7812) lr 1.0000e-03 eta 0:00:08
epoch [102/200] batch [25/40] time 0.380 (0.445) data 0.250 (0.315) loss_x loss_x 1.4727 (1.1363) acc_x 65.6250 (70.2500) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [30/40] time 0.440 (0.443) data 0.309 (0.313) loss_x loss_x 1.7217 (1.1952) acc_x 65.6250 (69.2708) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [35/40] time 0.498 (0.445) data 0.368 (0.315) loss_x loss_x 1.3506 (1.1849) acc_x 62.5000 (69.4643) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [40/40] time 0.396 (0.446) data 0.266 (0.316) loss_x loss_x 1.2236 (1.1842) acc_x 62.5000 (69.2969) lr 1.0000e-03 eta 0:00:00
epoch [102/200] batch [5/57] time 0.437 (0.445) data 0.305 (0.315) loss_u loss_u 0.8774 (0.8538) acc_u 18.7500 (21.8750) lr 1.0000e-03 eta 0:00:23
epoch [102/200] batch [10/57] time 0.440 (0.445) data 0.307 (0.315) loss_u loss_u 0.8237 (0.8541) acc_u 18.7500 (20.0000) lr 1.0000e-03 eta 0:00:20
epoch [102/200] batch [15/57] time 0.317 (0.447) data 0.187 (0.317) loss_u loss_u 0.7686 (0.8345) acc_u 28.1250 (21.2500) lr 1.0000e-03 eta 0:00:18
epoch [102/200] batch [20/57] time 0.515 (0.449) data 0.385 (0.319) loss_u loss_u 0.9077 (0.8344) acc_u 9.3750 (21.4062) lr 1.0000e-03 eta 0:00:16
epoch [102/200] batch [25/57] time 0.547 (0.450) data 0.417 (0.320) loss_u loss_u 0.9170 (0.8392) acc_u 9.3750 (20.5000) lr 1.0000e-03 eta 0:00:14
epoch [102/200] batch [30/57] time 0.407 (0.449) data 0.277 (0.318) loss_u loss_u 0.8574 (0.8366) acc_u 21.8750 (21.0417) lr 1.0000e-03 eta 0:00:12
epoch [102/200] batch [35/57] time 0.430 (0.455) data 0.298 (0.324) loss_u loss_u 0.8906 (0.8417) acc_u 12.5000 (20.0893) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [40/57] time 0.372 (0.449) data 0.241 (0.318) loss_u loss_u 0.8950 (0.8447) acc_u 9.3750 (19.6875) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [45/57] time 0.402 (0.447) data 0.270 (0.316) loss_u loss_u 0.9639 (0.8447) acc_u 6.2500 (19.5833) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [50/57] time 0.302 (0.442) data 0.171 (0.311) loss_u loss_u 0.8052 (0.8459) acc_u 25.0000 (19.4375) lr 1.0000e-03 eta 0:00:03
epoch [102/200] batch [55/57] time 0.463 (0.447) data 0.329 (0.316) loss_u loss_u 0.8076 (0.8404) acc_u 21.8750 (20.2273) lr 1.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1507
confident_label rate tensor(0.4091, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1283
clean true:1283
clean false:0
clean_rate:1.0
noisy true:346
noisy false:1507
after delete: len(clean_dataset) 1283
after delete: len(noisy_dataset) 1853
epoch [103/200] batch [5/40] time 0.352 (0.384) data 0.222 (0.254) loss_x loss_x 1.3438 (1.3316) acc_x 68.7500 (68.1250) lr 9.8429e-04 eta 0:00:13
epoch [103/200] batch [10/40] time 0.377 (0.395) data 0.247 (0.265) loss_x loss_x 1.6787 (1.3605) acc_x 65.6250 (65.9375) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [15/40] time 0.411 (0.421) data 0.281 (0.290) loss_x loss_x 1.6729 (1.3549) acc_x 62.5000 (67.5000) lr 9.8429e-04 eta 0:00:10
epoch [103/200] batch [20/40] time 0.595 (0.430) data 0.464 (0.300) loss_x loss_x 1.3145 (1.3216) acc_x 71.8750 (68.9062) lr 9.8429e-04 eta 0:00:08
epoch [103/200] batch [25/40] time 0.469 (0.436) data 0.339 (0.306) loss_x loss_x 1.2900 (1.2789) acc_x 62.5000 (69.6250) lr 9.8429e-04 eta 0:00:06
epoch [103/200] batch [30/40] time 0.407 (0.437) data 0.277 (0.306) loss_x loss_x 1.2812 (1.2629) acc_x 68.7500 (69.8958) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [35/40] time 0.448 (0.439) data 0.317 (0.309) loss_x loss_x 1.1377 (1.2614) acc_x 71.8750 (69.7321) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [40/40] time 0.558 (0.444) data 0.427 (0.314) loss_x loss_x 1.2588 (1.2600) acc_x 65.6250 (69.3750) lr 9.8429e-04 eta 0:00:00
epoch [103/200] batch [5/57] time 0.453 (0.438) data 0.322 (0.308) loss_u loss_u 0.8818 (0.8351) acc_u 15.6250 (20.0000) lr 9.8429e-04 eta 0:00:22
epoch [103/200] batch [10/57] time 0.507 (0.444) data 0.374 (0.314) loss_u loss_u 0.7993 (0.8374) acc_u 31.2500 (22.1875) lr 9.8429e-04 eta 0:00:20
epoch [103/200] batch [15/57] time 0.388 (0.438) data 0.256 (0.308) loss_u loss_u 0.8706 (0.8468) acc_u 9.3750 (20.4167) lr 9.8429e-04 eta 0:00:18
epoch [103/200] batch [20/57] time 0.378 (0.440) data 0.246 (0.309) loss_u loss_u 0.8716 (0.8501) acc_u 15.6250 (19.6875) lr 9.8429e-04 eta 0:00:16
epoch [103/200] batch [25/57] time 0.454 (0.442) data 0.323 (0.311) loss_u loss_u 0.8672 (0.8437) acc_u 12.5000 (20.0000) lr 9.8429e-04 eta 0:00:14
epoch [103/200] batch [30/57] time 0.602 (0.449) data 0.471 (0.318) loss_u loss_u 0.7847 (0.8442) acc_u 28.1250 (20.0000) lr 9.8429e-04 eta 0:00:12
epoch [103/200] batch [35/57] time 0.534 (0.448) data 0.403 (0.317) loss_u loss_u 0.8735 (0.8418) acc_u 15.6250 (20.0893) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [40/57] time 0.391 (0.447) data 0.259 (0.316) loss_u loss_u 0.8403 (0.8375) acc_u 18.7500 (21.0156) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [45/57] time 0.434 (0.447) data 0.303 (0.316) loss_u loss_u 0.8745 (0.8389) acc_u 15.6250 (20.8333) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [50/57] time 0.420 (0.445) data 0.288 (0.314) loss_u loss_u 0.8374 (0.8386) acc_u 15.6250 (20.7500) lr 9.8429e-04 eta 0:00:03
epoch [103/200] batch [55/57] time 0.400 (0.445) data 0.269 (0.314) loss_u loss_u 0.8218 (0.8399) acc_u 25.0000 (20.8523) lr 9.8429e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1500
confident_label rate tensor(0.4069, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1276
clean true:1276
clean false:0
clean_rate:1.0
noisy true:360
noisy false:1500
after delete: len(clean_dataset) 1276
after delete: len(noisy_dataset) 1860
epoch [104/200] batch [5/39] time 0.619 (0.482) data 0.488 (0.351) loss_x loss_x 1.1992 (1.1851) acc_x 75.0000 (74.3750) lr 9.6859e-04 eta 0:00:16
epoch [104/200] batch [10/39] time 0.460 (0.502) data 0.331 (0.371) loss_x loss_x 0.9321 (1.1751) acc_x 75.0000 (70.9375) lr 9.6859e-04 eta 0:00:14
epoch [104/200] batch [15/39] time 0.506 (0.480) data 0.376 (0.350) loss_x loss_x 0.8755 (1.1636) acc_x 78.1250 (70.2083) lr 9.6859e-04 eta 0:00:11
epoch [104/200] batch [20/39] time 0.347 (0.472) data 0.218 (0.342) loss_x loss_x 1.1436 (1.1576) acc_x 71.8750 (70.6250) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [25/39] time 0.411 (0.465) data 0.281 (0.335) loss_x loss_x 1.4824 (1.1394) acc_x 59.3750 (71.1250) lr 9.6859e-04 eta 0:00:06
epoch [104/200] batch [30/39] time 0.475 (0.455) data 0.344 (0.325) loss_x loss_x 1.0371 (1.1211) acc_x 59.3750 (71.2500) lr 9.6859e-04 eta 0:00:04
epoch [104/200] batch [35/39] time 0.417 (0.452) data 0.286 (0.321) loss_x loss_x 1.1816 (1.1394) acc_x 65.6250 (70.9821) lr 9.6859e-04 eta 0:00:01
epoch [104/200] batch [5/58] time 0.725 (0.464) data 0.594 (0.334) loss_u loss_u 0.7578 (0.8352) acc_u 28.1250 (21.2500) lr 9.6859e-04 eta 0:00:24
epoch [104/200] batch [10/58] time 0.434 (0.469) data 0.301 (0.339) loss_u loss_u 0.8848 (0.8634) acc_u 18.7500 (17.8125) lr 9.6859e-04 eta 0:00:22
epoch [104/200] batch [15/58] time 0.485 (0.471) data 0.352 (0.340) loss_u loss_u 0.8867 (0.8588) acc_u 15.6250 (18.5417) lr 9.6859e-04 eta 0:00:20
epoch [104/200] batch [20/58] time 0.436 (0.472) data 0.303 (0.341) loss_u loss_u 0.8560 (0.8544) acc_u 18.7500 (18.9062) lr 9.6859e-04 eta 0:00:17
epoch [104/200] batch [25/58] time 0.432 (0.470) data 0.301 (0.339) loss_u loss_u 0.7803 (0.8452) acc_u 28.1250 (20.1250) lr 9.6859e-04 eta 0:00:15
epoch [104/200] batch [30/58] time 0.399 (0.473) data 0.268 (0.342) loss_u loss_u 0.8755 (0.8363) acc_u 15.6250 (20.9375) lr 9.6859e-04 eta 0:00:13
epoch [104/200] batch [35/58] time 0.383 (0.472) data 0.250 (0.340) loss_u loss_u 0.8477 (0.8428) acc_u 21.8750 (20.0000) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [40/58] time 0.555 (0.471) data 0.423 (0.340) loss_u loss_u 0.7979 (0.8339) acc_u 25.0000 (21.6406) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [45/58] time 0.361 (0.468) data 0.230 (0.336) loss_u loss_u 0.8921 (0.8360) acc_u 6.2500 (21.3889) lr 9.6859e-04 eta 0:00:06
epoch [104/200] batch [50/58] time 0.469 (0.464) data 0.338 (0.333) loss_u loss_u 0.8750 (0.8328) acc_u 21.8750 (21.8125) lr 9.6859e-04 eta 0:00:03
epoch [104/200] batch [55/58] time 0.680 (0.464) data 0.548 (0.333) loss_u loss_u 0.8584 (0.8316) acc_u 25.0000 (22.2159) lr 9.6859e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1488
confident_label rate tensor(0.4193, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1315
clean true:1311
clean false:4
clean_rate:0.996958174904943
noisy true:337
noisy false:1484
after delete: len(clean_dataset) 1315
after delete: len(noisy_dataset) 1821
epoch [105/200] batch [5/41] time 0.484 (0.505) data 0.353 (0.373) loss_x loss_x 0.9722 (1.0358) acc_x 81.2500 (76.2500) lr 9.5289e-04 eta 0:00:18
epoch [105/200] batch [10/41] time 0.431 (0.525) data 0.299 (0.393) loss_x loss_x 1.3438 (1.2286) acc_x 68.7500 (72.1875) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [15/41] time 0.536 (0.523) data 0.403 (0.391) loss_x loss_x 1.1904 (1.2340) acc_x 75.0000 (71.0417) lr 9.5289e-04 eta 0:00:13
epoch [105/200] batch [20/41] time 0.538 (0.512) data 0.406 (0.380) loss_x loss_x 0.8569 (1.2499) acc_x 81.2500 (71.4062) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [25/41] time 0.447 (0.502) data 0.315 (0.370) loss_x loss_x 1.3350 (1.2898) acc_x 71.8750 (70.5000) lr 9.5289e-04 eta 0:00:08
epoch [105/200] batch [30/41] time 0.375 (0.491) data 0.244 (0.360) loss_x loss_x 1.0996 (1.2495) acc_x 65.6250 (70.2083) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [35/41] time 0.372 (0.482) data 0.241 (0.350) loss_x loss_x 1.3984 (1.2702) acc_x 56.2500 (68.8393) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [40/41] time 0.500 (0.477) data 0.369 (0.345) loss_x loss_x 1.2949 (1.2636) acc_x 62.5000 (68.4375) lr 9.5289e-04 eta 0:00:00
epoch [105/200] batch [5/56] time 0.436 (0.470) data 0.305 (0.339) loss_u loss_u 0.8740 (0.8303) acc_u 15.6250 (23.7500) lr 9.5289e-04 eta 0:00:23
epoch [105/200] batch [10/56] time 0.730 (0.470) data 0.598 (0.338) loss_u loss_u 0.8228 (0.8417) acc_u 18.7500 (20.6250) lr 9.5289e-04 eta 0:00:21
epoch [105/200] batch [15/56] time 0.435 (0.470) data 0.304 (0.339) loss_u loss_u 0.8555 (0.8395) acc_u 28.1250 (21.0417) lr 9.5289e-04 eta 0:00:19
epoch [105/200] batch [20/56] time 0.425 (0.466) data 0.293 (0.335) loss_u loss_u 0.7773 (0.8375) acc_u 31.2500 (21.5625) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [25/56] time 0.668 (0.469) data 0.537 (0.338) loss_u loss_u 0.7363 (0.8361) acc_u 31.2500 (21.3750) lr 9.5289e-04 eta 0:00:14
epoch [105/200] batch [30/56] time 0.439 (0.468) data 0.309 (0.336) loss_u loss_u 0.8730 (0.8420) acc_u 15.6250 (20.3125) lr 9.5289e-04 eta 0:00:12
epoch [105/200] batch [35/56] time 0.355 (0.463) data 0.223 (0.331) loss_u loss_u 0.8413 (0.8439) acc_u 25.0000 (20.1786) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [40/56] time 0.342 (0.464) data 0.211 (0.333) loss_u loss_u 0.7212 (0.8417) acc_u 34.3750 (20.6250) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [45/56] time 0.404 (0.461) data 0.271 (0.330) loss_u loss_u 0.8120 (0.8409) acc_u 25.0000 (20.8333) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [50/56] time 0.643 (0.463) data 0.511 (0.331) loss_u loss_u 0.8921 (0.8446) acc_u 12.5000 (20.1250) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [55/56] time 0.505 (0.462) data 0.372 (0.330) loss_u loss_u 0.8677 (0.8456) acc_u 18.7500 (19.8295) lr 9.5289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1490
confident_label rate tensor(0.4155, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1303
clean true:1301
clean false:2
clean_rate:0.9984650805832693
noisy true:345
noisy false:1488
after delete: len(clean_dataset) 1303
after delete: len(noisy_dataset) 1833
epoch [106/200] batch [5/40] time 0.398 (0.396) data 0.268 (0.266) loss_x loss_x 1.3076 (1.4018) acc_x 71.8750 (64.3750) lr 9.3721e-04 eta 0:00:13
epoch [106/200] batch [10/40] time 0.451 (0.433) data 0.321 (0.304) loss_x loss_x 1.2803 (1.2157) acc_x 59.3750 (68.4375) lr 9.3721e-04 eta 0:00:13
epoch [106/200] batch [15/40] time 0.564 (0.431) data 0.435 (0.301) loss_x loss_x 0.9492 (1.2580) acc_x 71.8750 (67.5000) lr 9.3721e-04 eta 0:00:10
epoch [106/200] batch [20/40] time 0.532 (0.437) data 0.402 (0.307) loss_x loss_x 0.9111 (1.2113) acc_x 71.8750 (67.6562) lr 9.3721e-04 eta 0:00:08
epoch [106/200] batch [25/40] time 0.433 (0.443) data 0.302 (0.313) loss_x loss_x 0.8701 (1.1960) acc_x 78.1250 (68.3750) lr 9.3721e-04 eta 0:00:06
epoch [106/200] batch [30/40] time 0.502 (0.439) data 0.372 (0.309) loss_x loss_x 1.0840 (1.2041) acc_x 68.7500 (67.6042) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [35/40] time 0.507 (0.444) data 0.376 (0.314) loss_x loss_x 0.9907 (1.1735) acc_x 78.1250 (68.5714) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [40/40] time 0.466 (0.441) data 0.336 (0.311) loss_x loss_x 1.1201 (1.1942) acc_x 78.1250 (68.2031) lr 9.3721e-04 eta 0:00:00
epoch [106/200] batch [5/57] time 0.403 (0.439) data 0.271 (0.308) loss_u loss_u 0.8677 (0.7784) acc_u 12.5000 (30.6250) lr 9.3721e-04 eta 0:00:22
epoch [106/200] batch [10/57] time 0.519 (0.440) data 0.383 (0.310) loss_u loss_u 0.8066 (0.8174) acc_u 28.1250 (24.6875) lr 9.3721e-04 eta 0:00:20
epoch [106/200] batch [15/57] time 0.420 (0.445) data 0.289 (0.314) loss_u loss_u 0.9375 (0.8233) acc_u 9.3750 (23.5417) lr 9.3721e-04 eta 0:00:18
epoch [106/200] batch [20/57] time 0.411 (0.446) data 0.278 (0.315) loss_u loss_u 0.7720 (0.8297) acc_u 34.3750 (22.8125) lr 9.3721e-04 eta 0:00:16
epoch [106/200] batch [25/57] time 0.490 (0.451) data 0.359 (0.320) loss_u loss_u 0.8657 (0.8326) acc_u 12.5000 (21.8750) lr 9.3721e-04 eta 0:00:14
epoch [106/200] batch [30/57] time 0.384 (0.448) data 0.252 (0.317) loss_u loss_u 0.9214 (0.8350) acc_u 6.2500 (21.4583) lr 9.3721e-04 eta 0:00:12
epoch [106/200] batch [35/57] time 0.320 (0.448) data 0.188 (0.317) loss_u loss_u 0.8604 (0.8370) acc_u 15.6250 (21.0714) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [40/57] time 0.497 (0.450) data 0.365 (0.319) loss_u loss_u 0.8794 (0.8416) acc_u 18.7500 (20.3125) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [45/57] time 0.461 (0.453) data 0.330 (0.321) loss_u loss_u 0.8066 (0.8413) acc_u 21.8750 (20.4167) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [50/57] time 0.497 (0.457) data 0.365 (0.326) loss_u loss_u 0.8706 (0.8437) acc_u 18.7500 (20.2500) lr 9.3721e-04 eta 0:00:03
epoch [106/200] batch [55/57] time 0.435 (0.455) data 0.302 (0.324) loss_u loss_u 0.8174 (0.8428) acc_u 28.1250 (20.5114) lr 9.3721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1488
confident_label rate tensor(0.4139, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1298
clean true:1295
clean false:3
clean_rate:0.99768875192604
noisy true:353
noisy false:1485
after delete: len(clean_dataset) 1298
after delete: len(noisy_dataset) 1838
epoch [107/200] batch [5/40] time 0.561 (0.546) data 0.430 (0.415) loss_x loss_x 0.8569 (1.1046) acc_x 68.7500 (69.3750) lr 9.2154e-04 eta 0:00:19
epoch [107/200] batch [10/40] time 0.547 (0.511) data 0.415 (0.380) loss_x loss_x 0.8672 (1.1962) acc_x 81.2500 (68.4375) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [15/40] time 0.570 (0.501) data 0.438 (0.370) loss_x loss_x 1.5674 (1.2270) acc_x 59.3750 (67.7083) lr 9.2154e-04 eta 0:00:12
epoch [107/200] batch [20/40] time 0.517 (0.492) data 0.381 (0.360) loss_x loss_x 1.3301 (1.1799) acc_x 68.7500 (69.2188) lr 9.2154e-04 eta 0:00:09
epoch [107/200] batch [25/40] time 0.422 (0.492) data 0.290 (0.361) loss_x loss_x 0.9878 (1.1568) acc_x 71.8750 (69.8750) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [30/40] time 0.440 (0.484) data 0.309 (0.353) loss_x loss_x 1.2275 (1.1714) acc_x 65.6250 (69.3750) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [35/40] time 0.432 (0.480) data 0.301 (0.349) loss_x loss_x 1.3174 (1.1923) acc_x 62.5000 (69.1964) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [40/40] time 0.436 (0.477) data 0.305 (0.346) loss_x loss_x 1.6104 (1.1887) acc_x 56.2500 (69.1406) lr 9.2154e-04 eta 0:00:00
epoch [107/200] batch [5/57] time 0.335 (0.469) data 0.204 (0.338) loss_u loss_u 0.7744 (0.8432) acc_u 25.0000 (20.0000) lr 9.2154e-04 eta 0:00:24
epoch [107/200] batch [10/57] time 0.498 (0.472) data 0.366 (0.341) loss_u loss_u 0.7656 (0.8267) acc_u 31.2500 (23.4375) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [15/57] time 0.622 (0.475) data 0.491 (0.344) loss_u loss_u 0.8018 (0.8211) acc_u 25.0000 (23.7500) lr 9.2154e-04 eta 0:00:19
epoch [107/200] batch [20/57] time 0.537 (0.480) data 0.404 (0.348) loss_u loss_u 0.8247 (0.8247) acc_u 28.1250 (23.2812) lr 9.2154e-04 eta 0:00:17
epoch [107/200] batch [25/57] time 0.422 (0.479) data 0.291 (0.347) loss_u loss_u 0.8179 (0.8322) acc_u 25.0000 (22.0000) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [30/57] time 0.383 (0.474) data 0.251 (0.342) loss_u loss_u 0.8350 (0.8343) acc_u 25.0000 (21.6667) lr 9.2154e-04 eta 0:00:12
epoch [107/200] batch [35/57] time 0.399 (0.470) data 0.268 (0.339) loss_u loss_u 0.8413 (0.8347) acc_u 21.8750 (21.4286) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [40/57] time 0.385 (0.470) data 0.254 (0.338) loss_u loss_u 0.7720 (0.8317) acc_u 25.0000 (21.7188) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [45/57] time 0.453 (0.468) data 0.320 (0.336) loss_u loss_u 0.8281 (0.8311) acc_u 25.0000 (21.8056) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [50/57] time 0.541 (0.464) data 0.409 (0.333) loss_u loss_u 0.7969 (0.8296) acc_u 25.0000 (21.8750) lr 9.2154e-04 eta 0:00:03
epoch [107/200] batch [55/57] time 0.465 (0.467) data 0.334 (0.335) loss_u loss_u 0.7656 (0.8297) acc_u 21.8750 (21.5341) lr 9.2154e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1470
confident_label rate tensor(0.4177, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1310
clean true:1309
clean false:1
clean_rate:0.999236641221374
noisy true:357
noisy false:1469
after delete: len(clean_dataset) 1310
after delete: len(noisy_dataset) 1826
epoch [108/200] batch [5/40] time 0.422 (0.435) data 0.291 (0.303) loss_x loss_x 1.3125 (1.3228) acc_x 56.2500 (65.6250) lr 9.0589e-04 eta 0:00:15
epoch [108/200] batch [10/40] time 0.453 (0.447) data 0.322 (0.316) loss_x loss_x 1.2031 (1.2789) acc_x 71.8750 (68.1250) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [15/40] time 0.569 (0.456) data 0.437 (0.324) loss_x loss_x 1.5068 (1.3211) acc_x 65.6250 (67.5000) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [20/40] time 0.457 (0.471) data 0.320 (0.339) loss_x loss_x 1.1924 (1.3466) acc_x 62.5000 (66.7188) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [25/40] time 0.458 (0.476) data 0.326 (0.345) loss_x loss_x 1.4746 (1.3315) acc_x 62.5000 (67.2500) lr 9.0589e-04 eta 0:00:07
epoch [108/200] batch [30/40] time 0.396 (0.472) data 0.264 (0.340) loss_x loss_x 1.6855 (1.3530) acc_x 59.3750 (66.1458) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [35/40] time 0.486 (0.489) data 0.354 (0.357) loss_x loss_x 1.1270 (1.3358) acc_x 68.7500 (66.6071) lr 9.0589e-04 eta 0:00:02
epoch [108/200] batch [40/40] time 0.432 (0.486) data 0.302 (0.354) loss_x loss_x 1.1816 (1.3037) acc_x 68.7500 (67.7344) lr 9.0589e-04 eta 0:00:00
epoch [108/200] batch [5/57] time 0.387 (0.479) data 0.255 (0.348) loss_u loss_u 0.7842 (0.8083) acc_u 28.1250 (25.6250) lr 9.0589e-04 eta 0:00:24
epoch [108/200] batch [10/57] time 0.451 (0.473) data 0.319 (0.342) loss_u loss_u 0.8770 (0.8162) acc_u 15.6250 (24.3750) lr 9.0589e-04 eta 0:00:22
epoch [108/200] batch [15/57] time 0.491 (0.471) data 0.359 (0.339) loss_u loss_u 0.8384 (0.8298) acc_u 25.0000 (21.8750) lr 9.0589e-04 eta 0:00:19
epoch [108/200] batch [20/57] time 0.418 (0.469) data 0.286 (0.338) loss_u loss_u 0.8301 (0.8334) acc_u 18.7500 (21.7188) lr 9.0589e-04 eta 0:00:17
epoch [108/200] batch [25/57] time 0.365 (0.465) data 0.233 (0.333) loss_u loss_u 0.7300 (0.8268) acc_u 28.1250 (22.5000) lr 9.0589e-04 eta 0:00:14
epoch [108/200] batch [30/57] time 0.471 (0.461) data 0.341 (0.330) loss_u loss_u 0.9053 (0.8310) acc_u 9.3750 (21.9792) lr 9.0589e-04 eta 0:00:12
epoch [108/200] batch [35/57] time 0.375 (0.458) data 0.244 (0.326) loss_u loss_u 0.9229 (0.8366) acc_u 6.2500 (21.1607) lr 9.0589e-04 eta 0:00:10
epoch [108/200] batch [40/57] time 0.469 (0.457) data 0.337 (0.326) loss_u loss_u 0.7861 (0.8337) acc_u 18.7500 (21.1719) lr 9.0589e-04 eta 0:00:07
epoch [108/200] batch [45/57] time 0.443 (0.457) data 0.312 (0.326) loss_u loss_u 0.9155 (0.8350) acc_u 9.3750 (21.1111) lr 9.0589e-04 eta 0:00:05
epoch [108/200] batch [50/57] time 0.553 (0.460) data 0.422 (0.328) loss_u loss_u 0.8408 (0.8367) acc_u 15.6250 (20.7500) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [55/57] time 0.590 (0.463) data 0.459 (0.331) loss_u loss_u 0.8857 (0.8373) acc_u 15.6250 (20.8523) lr 9.0589e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1460
confident_label rate tensor(0.4228, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1326
clean true:1324
clean false:2
clean_rate:0.9984917043740573
noisy true:352
noisy false:1458
after delete: len(clean_dataset) 1326
after delete: len(noisy_dataset) 1810
epoch [109/200] batch [5/41] time 0.434 (0.453) data 0.304 (0.323) loss_x loss_x 1.2158 (1.0169) acc_x 71.8750 (75.6250) lr 8.9027e-04 eta 0:00:16
epoch [109/200] batch [10/41] time 0.531 (0.469) data 0.400 (0.338) loss_x loss_x 1.1367 (1.1615) acc_x 71.8750 (71.2500) lr 8.9027e-04 eta 0:00:14
epoch [109/200] batch [15/41] time 0.553 (0.473) data 0.421 (0.342) loss_x loss_x 0.8350 (1.1743) acc_x 78.1250 (69.7917) lr 8.9027e-04 eta 0:00:12
epoch [109/200] batch [20/41] time 0.456 (0.469) data 0.325 (0.338) loss_x loss_x 1.0605 (1.1697) acc_x 78.1250 (69.5312) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [25/41] time 0.399 (0.457) data 0.267 (0.326) loss_x loss_x 1.0078 (1.1918) acc_x 78.1250 (68.6250) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [30/41] time 0.495 (0.464) data 0.364 (0.333) loss_x loss_x 0.9976 (1.1995) acc_x 62.5000 (68.1250) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [35/41] time 0.491 (0.467) data 0.360 (0.336) loss_x loss_x 1.3809 (1.2349) acc_x 59.3750 (67.9464) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [40/41] time 0.374 (0.458) data 0.243 (0.327) loss_x loss_x 1.4229 (1.2531) acc_x 68.7500 (67.7344) lr 8.9027e-04 eta 0:00:00
epoch [109/200] batch [5/56] time 0.553 (0.463) data 0.422 (0.332) loss_u loss_u 0.8125 (0.8327) acc_u 25.0000 (22.5000) lr 8.9027e-04 eta 0:00:23
epoch [109/200] batch [10/56] time 0.396 (0.459) data 0.265 (0.328) loss_u loss_u 0.8428 (0.8411) acc_u 21.8750 (21.8750) lr 8.9027e-04 eta 0:00:21
epoch [109/200] batch [15/56] time 0.334 (0.454) data 0.202 (0.323) loss_u loss_u 0.8354 (0.8346) acc_u 25.0000 (23.3333) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [20/56] time 0.446 (0.450) data 0.316 (0.319) loss_u loss_u 0.8379 (0.8317) acc_u 21.8750 (23.1250) lr 8.9027e-04 eta 0:00:16
epoch [109/200] batch [25/56] time 0.482 (0.450) data 0.351 (0.319) loss_u loss_u 0.7539 (0.8288) acc_u 25.0000 (22.6250) lr 8.9027e-04 eta 0:00:13
epoch [109/200] batch [30/56] time 0.589 (0.452) data 0.457 (0.321) loss_u loss_u 0.7827 (0.8309) acc_u 21.8750 (21.8750) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [35/56] time 0.329 (0.450) data 0.198 (0.319) loss_u loss_u 0.8315 (0.8351) acc_u 18.7500 (21.1607) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [40/56] time 0.383 (0.449) data 0.251 (0.318) loss_u loss_u 0.8457 (0.8355) acc_u 18.7500 (20.9375) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [45/56] time 0.375 (0.446) data 0.243 (0.315) loss_u loss_u 0.8687 (0.8366) acc_u 15.6250 (20.8333) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [50/56] time 0.450 (0.444) data 0.319 (0.313) loss_u loss_u 0.8066 (0.8323) acc_u 28.1250 (21.6250) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [55/56] time 0.609 (0.448) data 0.476 (0.317) loss_u loss_u 0.8530 (0.8330) acc_u 21.8750 (21.4205) lr 8.9027e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1481
confident_label rate tensor(0.4161, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1305
clean true:1301
clean false:4
clean_rate:0.9969348659003832
noisy true:354
noisy false:1477
after delete: len(clean_dataset) 1305
after delete: len(noisy_dataset) 1831
epoch [110/200] batch [5/40] time 0.384 (0.419) data 0.253 (0.289) loss_x loss_x 1.1465 (1.2783) acc_x 68.7500 (70.0000) lr 8.7467e-04 eta 0:00:14
epoch [110/200] batch [10/40] time 0.404 (0.440) data 0.274 (0.310) loss_x loss_x 1.1514 (1.2841) acc_x 71.8750 (71.5625) lr 8.7467e-04 eta 0:00:13
epoch [110/200] batch [15/40] time 0.573 (0.469) data 0.442 (0.339) loss_x loss_x 0.4675 (1.2102) acc_x 93.7500 (72.2917) lr 8.7467e-04 eta 0:00:11
epoch [110/200] batch [20/40] time 0.461 (0.476) data 0.330 (0.346) loss_x loss_x 1.0566 (1.2159) acc_x 68.7500 (71.0938) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [25/40] time 0.433 (0.476) data 0.301 (0.345) loss_x loss_x 1.1787 (1.2640) acc_x 75.0000 (70.2500) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [30/40] time 0.453 (0.478) data 0.322 (0.347) loss_x loss_x 0.8799 (1.2478) acc_x 75.0000 (70.4167) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [35/40] time 0.481 (0.487) data 0.349 (0.356) loss_x loss_x 1.1416 (1.2239) acc_x 65.6250 (70.8036) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [40/40] time 0.410 (0.483) data 0.278 (0.352) loss_x loss_x 1.9707 (1.2697) acc_x 56.2500 (68.9844) lr 8.7467e-04 eta 0:00:00
epoch [110/200] batch [5/57] time 0.456 (0.483) data 0.323 (0.351) loss_u loss_u 0.8003 (0.8147) acc_u 31.2500 (26.2500) lr 8.7467e-04 eta 0:00:25
epoch [110/200] batch [10/57] time 0.572 (0.493) data 0.441 (0.362) loss_u loss_u 0.8838 (0.8463) acc_u 15.6250 (21.2500) lr 8.7467e-04 eta 0:00:23
epoch [110/200] batch [15/57] time 0.425 (0.494) data 0.292 (0.363) loss_u loss_u 0.8657 (0.8592) acc_u 18.7500 (18.9583) lr 8.7467e-04 eta 0:00:20
epoch [110/200] batch [20/57] time 0.462 (0.490) data 0.330 (0.359) loss_u loss_u 0.7725 (0.8509) acc_u 28.1250 (19.6875) lr 8.7467e-04 eta 0:00:18
epoch [110/200] batch [25/57] time 0.368 (0.485) data 0.236 (0.354) loss_u loss_u 0.9282 (0.8531) acc_u 6.2500 (19.1250) lr 8.7467e-04 eta 0:00:15
epoch [110/200] batch [30/57] time 0.368 (0.480) data 0.237 (0.348) loss_u loss_u 0.7905 (0.8477) acc_u 25.0000 (19.8958) lr 8.7467e-04 eta 0:00:12
epoch [110/200] batch [35/57] time 0.400 (0.478) data 0.269 (0.347) loss_u loss_u 0.8418 (0.8453) acc_u 18.7500 (20.1786) lr 8.7467e-04 eta 0:00:10
epoch [110/200] batch [40/57] time 0.397 (0.475) data 0.265 (0.343) loss_u loss_u 0.8394 (0.8449) acc_u 18.7500 (20.3906) lr 8.7467e-04 eta 0:00:08
epoch [110/200] batch [45/57] time 0.503 (0.471) data 0.371 (0.339) loss_u loss_u 0.8008 (0.8447) acc_u 21.8750 (20.2083) lr 8.7467e-04 eta 0:00:05
epoch [110/200] batch [50/57] time 0.402 (0.471) data 0.269 (0.339) loss_u loss_u 0.8647 (0.8438) acc_u 15.6250 (20.3750) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [55/57] time 0.461 (0.474) data 0.329 (0.342) loss_u loss_u 0.9082 (0.8481) acc_u 15.6250 (19.7727) lr 8.7467e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1433
confident_label rate tensor(0.4267, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1338
clean true:1334
clean false:4
clean_rate:0.9970104633781763
noisy true:369
noisy false:1429
after delete: len(clean_dataset) 1338
after delete: len(noisy_dataset) 1798
epoch [111/200] batch [5/41] time 0.581 (0.484) data 0.449 (0.353) loss_x loss_x 1.1250 (1.2660) acc_x 78.1250 (74.3750) lr 8.5910e-04 eta 0:00:17
epoch [111/200] batch [10/41] time 0.438 (0.464) data 0.307 (0.333) loss_x loss_x 0.9189 (1.1555) acc_x 78.1250 (72.8125) lr 8.5910e-04 eta 0:00:14
epoch [111/200] batch [15/41] time 0.463 (0.471) data 0.332 (0.340) loss_x loss_x 1.0312 (1.1611) acc_x 62.5000 (71.4583) lr 8.5910e-04 eta 0:00:12
epoch [111/200] batch [20/41] time 0.504 (0.476) data 0.372 (0.345) loss_x loss_x 1.6797 (1.1589) acc_x 59.3750 (71.0938) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [25/41] time 0.494 (0.472) data 0.363 (0.341) loss_x loss_x 1.1914 (1.1774) acc_x 71.8750 (71.2500) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [30/41] time 0.443 (0.467) data 0.311 (0.336) loss_x loss_x 1.1309 (1.2110) acc_x 68.7500 (70.5208) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [35/41] time 0.542 (0.467) data 0.411 (0.336) loss_x loss_x 1.1484 (1.1683) acc_x 68.7500 (71.2500) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [40/41] time 0.568 (0.470) data 0.437 (0.338) loss_x loss_x 1.0908 (1.1803) acc_x 65.6250 (70.6250) lr 8.5910e-04 eta 0:00:00
epoch [111/200] batch [5/56] time 0.387 (0.468) data 0.256 (0.337) loss_u loss_u 0.8452 (0.8450) acc_u 15.6250 (20.0000) lr 8.5910e-04 eta 0:00:23
epoch [111/200] batch [10/56] time 0.573 (0.465) data 0.440 (0.333) loss_u loss_u 0.8945 (0.8459) acc_u 15.6250 (21.2500) lr 8.5910e-04 eta 0:00:21
epoch [111/200] batch [15/56] time 0.431 (0.464) data 0.299 (0.333) loss_u loss_u 0.8472 (0.8476) acc_u 15.6250 (20.4167) lr 8.5910e-04 eta 0:00:19
epoch [111/200] batch [20/56] time 0.319 (0.463) data 0.187 (0.331) loss_u loss_u 0.8301 (0.8471) acc_u 25.0000 (20.7812) lr 8.5910e-04 eta 0:00:16
epoch [111/200] batch [25/56] time 0.406 (0.460) data 0.275 (0.329) loss_u loss_u 0.8726 (0.8429) acc_u 12.5000 (21.0000) lr 8.5910e-04 eta 0:00:14
epoch [111/200] batch [30/56] time 0.404 (0.458) data 0.273 (0.326) loss_u loss_u 0.8286 (0.8410) acc_u 18.7500 (21.0417) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [35/56] time 0.598 (0.463) data 0.466 (0.331) loss_u loss_u 0.7974 (0.8360) acc_u 31.2500 (21.6964) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [40/56] time 0.481 (0.463) data 0.348 (0.331) loss_u loss_u 0.8262 (0.8357) acc_u 21.8750 (21.4062) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [45/56] time 0.470 (0.464) data 0.339 (0.332) loss_u loss_u 0.8540 (0.8362) acc_u 15.6250 (21.3194) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [50/56] time 0.416 (0.465) data 0.284 (0.333) loss_u loss_u 0.8218 (0.8357) acc_u 25.0000 (21.5000) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [55/56] time 0.620 (0.466) data 0.488 (0.334) loss_u loss_u 0.8140 (0.8360) acc_u 25.0000 (21.3636) lr 8.5910e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1502
confident_label rate tensor(0.4126, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1294
clean true:1291
clean false:3
clean_rate:0.9976816074188563
noisy true:343
noisy false:1499
after delete: len(clean_dataset) 1294
after delete: len(noisy_dataset) 1842
epoch [112/200] batch [5/40] time 0.454 (0.448) data 0.323 (0.316) loss_x loss_x 1.3164 (1.2841) acc_x 65.6250 (68.1250) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [10/40] time 0.485 (0.462) data 0.353 (0.330) loss_x loss_x 1.4492 (1.1951) acc_x 59.3750 (67.8125) lr 8.4357e-04 eta 0:00:13
epoch [112/200] batch [15/40] time 0.533 (0.465) data 0.401 (0.334) loss_x loss_x 1.6133 (1.1826) acc_x 62.5000 (69.7917) lr 8.4357e-04 eta 0:00:11
epoch [112/200] batch [20/40] time 0.420 (0.465) data 0.289 (0.333) loss_x loss_x 1.6660 (1.2036) acc_x 53.1250 (69.6875) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [25/40] time 0.357 (0.474) data 0.226 (0.343) loss_x loss_x 1.3984 (1.2081) acc_x 56.2500 (69.5000) lr 8.4357e-04 eta 0:00:07
epoch [112/200] batch [30/40] time 0.531 (0.473) data 0.399 (0.341) loss_x loss_x 1.2754 (1.2001) acc_x 71.8750 (70.3125) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [35/40] time 0.562 (0.474) data 0.431 (0.342) loss_x loss_x 0.9521 (1.1876) acc_x 68.7500 (70.4464) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [40/40] time 0.501 (0.480) data 0.370 (0.349) loss_x loss_x 0.8735 (1.1414) acc_x 87.5000 (72.0312) lr 8.4357e-04 eta 0:00:00
epoch [112/200] batch [5/57] time 0.641 (0.479) data 0.509 (0.347) loss_u loss_u 0.8252 (0.8561) acc_u 25.0000 (20.0000) lr 8.4357e-04 eta 0:00:24
epoch [112/200] batch [10/57] time 0.427 (0.474) data 0.295 (0.342) loss_u loss_u 0.7520 (0.8364) acc_u 28.1250 (20.6250) lr 8.4357e-04 eta 0:00:22
epoch [112/200] batch [15/57] time 0.531 (0.475) data 0.399 (0.343) loss_u loss_u 0.8130 (0.8363) acc_u 28.1250 (20.4167) lr 8.4357e-04 eta 0:00:19
epoch [112/200] batch [20/57] time 0.517 (0.470) data 0.386 (0.338) loss_u loss_u 0.7490 (0.8332) acc_u 34.3750 (21.2500) lr 8.4357e-04 eta 0:00:17
epoch [112/200] batch [25/57] time 0.573 (0.477) data 0.441 (0.345) loss_u loss_u 0.7500 (0.8283) acc_u 28.1250 (21.8750) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [30/57] time 0.567 (0.478) data 0.435 (0.346) loss_u loss_u 0.8550 (0.8297) acc_u 18.7500 (21.9792) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [35/57] time 0.427 (0.476) data 0.295 (0.344) loss_u loss_u 0.8579 (0.8321) acc_u 15.6250 (21.8750) lr 8.4357e-04 eta 0:00:10
epoch [112/200] batch [40/57] time 0.432 (0.476) data 0.300 (0.344) loss_u loss_u 0.8369 (0.8341) acc_u 21.8750 (21.7188) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [45/57] time 0.440 (0.472) data 0.308 (0.341) loss_u loss_u 0.7988 (0.8301) acc_u 25.0000 (22.3611) lr 8.4357e-04 eta 0:00:05
epoch [112/200] batch [50/57] time 0.579 (0.472) data 0.447 (0.341) loss_u loss_u 0.8516 (0.8332) acc_u 18.7500 (21.8750) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [55/57] time 0.396 (0.469) data 0.264 (0.338) loss_u loss_u 0.8462 (0.8322) acc_u 18.7500 (21.8182) lr 8.4357e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1483
confident_label rate tensor(0.4126, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1294
clean true:1291
clean false:3
clean_rate:0.9976816074188563
noisy true:362
noisy false:1480
after delete: len(clean_dataset) 1294
after delete: len(noisy_dataset) 1842
epoch [113/200] batch [5/40] time 0.482 (0.486) data 0.352 (0.355) loss_x loss_x 1.2725 (1.1834) acc_x 68.7500 (72.5000) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [10/40] time 0.461 (0.498) data 0.331 (0.367) loss_x loss_x 1.3477 (1.2365) acc_x 65.6250 (71.8750) lr 8.2807e-04 eta 0:00:14
epoch [113/200] batch [15/40] time 0.439 (0.489) data 0.308 (0.359) loss_x loss_x 1.3945 (1.2839) acc_x 62.5000 (70.2083) lr 8.2807e-04 eta 0:00:12
epoch [113/200] batch [20/40] time 0.434 (0.471) data 0.303 (0.340) loss_x loss_x 0.9209 (1.2921) acc_x 78.1250 (69.2188) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [25/40] time 0.368 (0.461) data 0.237 (0.331) loss_x loss_x 1.5176 (1.2721) acc_x 65.6250 (69.2500) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [30/40] time 0.405 (0.455) data 0.274 (0.324) loss_x loss_x 1.5234 (1.2647) acc_x 56.2500 (69.2708) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [35/40] time 0.476 (0.459) data 0.345 (0.328) loss_x loss_x 1.1436 (1.2680) acc_x 78.1250 (69.5536) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [40/40] time 0.534 (0.467) data 0.402 (0.337) loss_x loss_x 0.8413 (1.2443) acc_x 75.0000 (69.6875) lr 8.2807e-04 eta 0:00:00
epoch [113/200] batch [5/57] time 0.373 (0.462) data 0.241 (0.331) loss_u loss_u 0.8794 (0.8539) acc_u 15.6250 (18.7500) lr 8.2807e-04 eta 0:00:24
epoch [113/200] batch [10/57] time 0.327 (0.458) data 0.195 (0.327) loss_u loss_u 0.8711 (0.8475) acc_u 12.5000 (18.1250) lr 8.2807e-04 eta 0:00:21
epoch [113/200] batch [15/57] time 0.356 (0.456) data 0.225 (0.325) loss_u loss_u 0.8613 (0.8559) acc_u 18.7500 (17.0833) lr 8.2807e-04 eta 0:00:19
epoch [113/200] batch [20/57] time 0.505 (0.457) data 0.373 (0.326) loss_u loss_u 0.8354 (0.8451) acc_u 18.7500 (19.2188) lr 8.2807e-04 eta 0:00:16
epoch [113/200] batch [25/57] time 0.635 (0.467) data 0.503 (0.335) loss_u loss_u 0.8066 (0.8453) acc_u 21.8750 (19.0000) lr 8.2807e-04 eta 0:00:14
epoch [113/200] batch [30/57] time 0.453 (0.467) data 0.322 (0.336) loss_u loss_u 0.8301 (0.8346) acc_u 25.0000 (20.3125) lr 8.2807e-04 eta 0:00:12
epoch [113/200] batch [35/57] time 0.522 (0.468) data 0.390 (0.336) loss_u loss_u 0.8154 (0.8362) acc_u 21.8750 (20.5357) lr 8.2807e-04 eta 0:00:10
epoch [113/200] batch [40/57] time 0.545 (0.468) data 0.413 (0.337) loss_u loss_u 0.8076 (0.8334) acc_u 18.7500 (20.5469) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [45/57] time 0.428 (0.466) data 0.295 (0.335) loss_u loss_u 0.7520 (0.8299) acc_u 31.2500 (21.0417) lr 8.2807e-04 eta 0:00:05
epoch [113/200] batch [50/57] time 0.541 (0.465) data 0.408 (0.333) loss_u loss_u 0.8340 (0.8281) acc_u 21.8750 (21.3750) lr 8.2807e-04 eta 0:00:03
epoch [113/200] batch [55/57] time 0.504 (0.468) data 0.372 (0.337) loss_u loss_u 0.8237 (0.8266) acc_u 28.1250 (21.5909) lr 8.2807e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1473
confident_label rate tensor(0.4193, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1315
clean true:1313
clean false:2
clean_rate:0.9984790874524715
noisy true:350
noisy false:1471
after delete: len(clean_dataset) 1315
after delete: len(noisy_dataset) 1821
epoch [114/200] batch [5/41] time 0.526 (0.455) data 0.396 (0.324) loss_x loss_x 1.3311 (1.3836) acc_x 62.5000 (62.5000) lr 8.1262e-04 eta 0:00:16
epoch [114/200] batch [10/41] time 0.437 (0.453) data 0.307 (0.322) loss_x loss_x 1.4707 (1.3008) acc_x 65.6250 (66.5625) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [15/41] time 0.477 (0.461) data 0.346 (0.330) loss_x loss_x 1.0918 (1.2976) acc_x 71.8750 (67.2917) lr 8.1262e-04 eta 0:00:11
epoch [114/200] batch [20/41] time 0.438 (0.465) data 0.307 (0.334) loss_x loss_x 1.1729 (1.2431) acc_x 78.1250 (69.0625) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [25/41] time 0.458 (0.471) data 0.326 (0.340) loss_x loss_x 0.8716 (1.1846) acc_x 84.3750 (70.5000) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [30/41] time 0.402 (0.459) data 0.270 (0.327) loss_x loss_x 1.5771 (1.2007) acc_x 53.1250 (70.3125) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [35/41] time 0.479 (0.454) data 0.348 (0.323) loss_x loss_x 1.4004 (1.2037) acc_x 56.2500 (69.1964) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [40/41] time 0.432 (0.452) data 0.301 (0.321) loss_x loss_x 1.6084 (1.2178) acc_x 62.5000 (68.8281) lr 8.1262e-04 eta 0:00:00
epoch [114/200] batch [5/56] time 0.482 (0.463) data 0.350 (0.332) loss_u loss_u 0.7861 (0.8258) acc_u 21.8750 (19.3750) lr 8.1262e-04 eta 0:00:23
epoch [114/200] batch [10/56] time 0.542 (0.468) data 0.410 (0.336) loss_u loss_u 0.8037 (0.8297) acc_u 21.8750 (20.6250) lr 8.1262e-04 eta 0:00:21
epoch [114/200] batch [15/56] time 0.517 (0.465) data 0.385 (0.334) loss_u loss_u 0.8530 (0.8309) acc_u 21.8750 (21.8750) lr 8.1262e-04 eta 0:00:19
epoch [114/200] batch [20/56] time 0.412 (0.463) data 0.281 (0.331) loss_u loss_u 0.8384 (0.8270) acc_u 25.0000 (22.5000) lr 8.1262e-04 eta 0:00:16
epoch [114/200] batch [25/56] time 0.507 (0.464) data 0.375 (0.333) loss_u loss_u 0.8662 (0.8356) acc_u 15.6250 (21.6250) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [30/56] time 0.382 (0.463) data 0.251 (0.332) loss_u loss_u 0.9160 (0.8385) acc_u 9.3750 (21.0417) lr 8.1262e-04 eta 0:00:12
epoch [114/200] batch [35/56] time 0.442 (0.463) data 0.310 (0.332) loss_u loss_u 0.8569 (0.8402) acc_u 21.8750 (20.7143) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [40/56] time 0.543 (0.465) data 0.411 (0.333) loss_u loss_u 0.6504 (0.8360) acc_u 40.6250 (21.2500) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [45/56] time 0.504 (0.466) data 0.372 (0.334) loss_u loss_u 0.8516 (0.8364) acc_u 15.6250 (20.9722) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [50/56] time 0.410 (0.461) data 0.278 (0.329) loss_u loss_u 0.7827 (0.8359) acc_u 31.2500 (21.1250) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [55/56] time 0.425 (0.463) data 0.293 (0.332) loss_u loss_u 0.8530 (0.8377) acc_u 21.8750 (20.9091) lr 8.1262e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1501
confident_label rate tensor(0.4117, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1291
clean true:1289
clean false:2
clean_rate:0.9984508133230054
noisy true:346
noisy false:1499
after delete: len(clean_dataset) 1291
after delete: len(noisy_dataset) 1845
epoch [115/200] batch [5/40] time 0.426 (0.464) data 0.296 (0.333) loss_x loss_x 1.3682 (1.2787) acc_x 68.7500 (65.6250) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [10/40] time 0.455 (0.455) data 0.323 (0.324) loss_x loss_x 1.4014 (1.2989) acc_x 68.7500 (65.0000) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [15/40] time 0.526 (0.476) data 0.395 (0.345) loss_x loss_x 1.4658 (1.3391) acc_x 65.6250 (65.8333) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [20/40] time 0.417 (0.464) data 0.286 (0.333) loss_x loss_x 1.4668 (1.3346) acc_x 65.6250 (65.4688) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [25/40] time 0.398 (0.461) data 0.267 (0.330) loss_x loss_x 1.3721 (1.2769) acc_x 65.6250 (67.0000) lr 7.9721e-04 eta 0:00:06
epoch [115/200] batch [30/40] time 0.466 (0.462) data 0.334 (0.331) loss_x loss_x 0.9795 (1.2497) acc_x 78.1250 (68.3333) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [35/40] time 0.680 (0.472) data 0.549 (0.340) loss_x loss_x 1.1162 (1.2578) acc_x 65.6250 (67.8571) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [40/40] time 0.435 (0.466) data 0.304 (0.335) loss_x loss_x 0.9512 (1.2677) acc_x 75.0000 (67.8125) lr 7.9721e-04 eta 0:00:00
epoch [115/200] batch [5/57] time 0.551 (0.465) data 0.419 (0.334) loss_u loss_u 0.9043 (0.8456) acc_u 18.7500 (21.2500) lr 7.9721e-04 eta 0:00:24
epoch [115/200] batch [10/57] time 0.503 (0.466) data 0.371 (0.335) loss_u loss_u 0.8242 (0.8324) acc_u 25.0000 (22.5000) lr 7.9721e-04 eta 0:00:21
epoch [115/200] batch [15/57] time 0.406 (0.464) data 0.275 (0.332) loss_u loss_u 0.7690 (0.8328) acc_u 21.8750 (22.0833) lr 7.9721e-04 eta 0:00:19
epoch [115/200] batch [20/57] time 0.362 (0.463) data 0.231 (0.331) loss_u loss_u 0.8965 (0.8389) acc_u 9.3750 (20.1562) lr 7.9721e-04 eta 0:00:17
epoch [115/200] batch [25/57] time 0.451 (0.464) data 0.319 (0.332) loss_u loss_u 0.8696 (0.8336) acc_u 15.6250 (21.0000) lr 7.9721e-04 eta 0:00:14
epoch [115/200] batch [30/57] time 0.476 (0.469) data 0.343 (0.338) loss_u loss_u 0.8413 (0.8339) acc_u 18.7500 (21.3542) lr 7.9721e-04 eta 0:00:12
epoch [115/200] batch [35/57] time 0.452 (0.471) data 0.318 (0.339) loss_u loss_u 0.8286 (0.8334) acc_u 25.0000 (21.4286) lr 7.9721e-04 eta 0:00:10
epoch [115/200] batch [40/57] time 0.532 (0.470) data 0.399 (0.338) loss_u loss_u 0.8286 (0.8312) acc_u 25.0000 (22.0312) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [45/57] time 0.382 (0.467) data 0.250 (0.336) loss_u loss_u 0.8223 (0.8311) acc_u 25.0000 (22.3611) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [50/57] time 0.492 (0.467) data 0.360 (0.335) loss_u loss_u 0.8608 (0.8315) acc_u 15.6250 (22.2500) lr 7.9721e-04 eta 0:00:03
epoch [115/200] batch [55/57] time 0.523 (0.468) data 0.391 (0.336) loss_u loss_u 0.7124 (0.8277) acc_u 37.5000 (22.8409) lr 7.9721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1474
confident_label rate tensor(0.4187, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1313
clean true:1310
clean false:3
clean_rate:0.9977151561309977
noisy true:352
noisy false:1471
after delete: len(clean_dataset) 1313
after delete: len(noisy_dataset) 1823
epoch [116/200] batch [5/41] time 0.529 (0.466) data 0.397 (0.335) loss_x loss_x 0.9736 (1.0386) acc_x 75.0000 (72.5000) lr 7.8186e-04 eta 0:00:16
epoch [116/200] batch [10/41] time 0.431 (0.449) data 0.300 (0.318) loss_x loss_x 0.8599 (1.2002) acc_x 68.7500 (67.5000) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [15/41] time 0.396 (0.449) data 0.266 (0.318) loss_x loss_x 1.3340 (1.2017) acc_x 62.5000 (67.0833) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [20/41] time 0.518 (0.465) data 0.387 (0.334) loss_x loss_x 0.9214 (1.1592) acc_x 75.0000 (69.0625) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [25/41] time 0.393 (0.470) data 0.262 (0.339) loss_x loss_x 1.4941 (1.1724) acc_x 59.3750 (69.0000) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [30/41] time 0.441 (0.472) data 0.310 (0.341) loss_x loss_x 0.7583 (1.1732) acc_x 75.0000 (69.2708) lr 7.8186e-04 eta 0:00:05
epoch [116/200] batch [35/41] time 0.414 (0.468) data 0.282 (0.336) loss_x loss_x 1.7988 (1.1804) acc_x 53.1250 (68.7500) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [40/41] time 0.494 (0.468) data 0.363 (0.337) loss_x loss_x 1.6055 (1.2004) acc_x 62.5000 (68.5156) lr 7.8186e-04 eta 0:00:00
epoch [116/200] batch [5/56] time 0.552 (0.473) data 0.419 (0.342) loss_u loss_u 0.7954 (0.8231) acc_u 21.8750 (21.2500) lr 7.8186e-04 eta 0:00:24
epoch [116/200] batch [10/56] time 0.680 (0.478) data 0.548 (0.347) loss_u loss_u 0.8247 (0.8251) acc_u 28.1250 (21.8750) lr 7.8186e-04 eta 0:00:21
epoch [116/200] batch [15/56] time 0.473 (0.482) data 0.341 (0.351) loss_u loss_u 0.7593 (0.8268) acc_u 31.2500 (22.5000) lr 7.8186e-04 eta 0:00:19
epoch [116/200] batch [20/56] time 0.449 (0.479) data 0.317 (0.347) loss_u loss_u 0.8105 (0.8204) acc_u 25.0000 (22.9688) lr 7.8186e-04 eta 0:00:17
epoch [116/200] batch [25/56] time 0.471 (0.480) data 0.339 (0.349) loss_u loss_u 0.8193 (0.8227) acc_u 25.0000 (22.7500) lr 7.8186e-04 eta 0:00:14
epoch [116/200] batch [30/56] time 0.560 (0.483) data 0.429 (0.351) loss_u loss_u 0.9258 (0.8340) acc_u 12.5000 (21.6667) lr 7.8186e-04 eta 0:00:12
epoch [116/200] batch [35/56] time 0.615 (0.483) data 0.480 (0.352) loss_u loss_u 0.8750 (0.8360) acc_u 15.6250 (21.6071) lr 7.8186e-04 eta 0:00:10
epoch [116/200] batch [40/56] time 0.425 (0.484) data 0.293 (0.352) loss_u loss_u 0.8115 (0.8337) acc_u 28.1250 (22.3438) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [45/56] time 0.425 (0.480) data 0.293 (0.349) loss_u loss_u 0.8726 (0.8385) acc_u 15.6250 (21.3194) lr 7.8186e-04 eta 0:00:05
epoch [116/200] batch [50/56] time 0.383 (0.476) data 0.252 (0.344) loss_u loss_u 0.7598 (0.8345) acc_u 34.3750 (21.6875) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [55/56] time 0.404 (0.474) data 0.273 (0.342) loss_u loss_u 0.8374 (0.8343) acc_u 25.0000 (21.6477) lr 7.8186e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1439
confident_label rate tensor(0.4289, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1345
clean true:1342
clean false:3
clean_rate:0.9977695167286246
noisy true:355
noisy false:1436
after delete: len(clean_dataset) 1345
after delete: len(noisy_dataset) 1791
epoch [117/200] batch [5/42] time 0.460 (0.516) data 0.329 (0.385) loss_x loss_x 1.8896 (1.3807) acc_x 62.5000 (67.5000) lr 7.6655e-04 eta 0:00:19
epoch [117/200] batch [10/42] time 0.475 (0.488) data 0.344 (0.357) loss_x loss_x 1.3857 (1.3850) acc_x 65.6250 (65.0000) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [15/42] time 0.380 (0.448) data 0.249 (0.318) loss_x loss_x 1.3662 (1.3591) acc_x 59.3750 (65.4167) lr 7.6655e-04 eta 0:00:12
epoch [117/200] batch [20/42] time 0.498 (0.444) data 0.367 (0.313) loss_x loss_x 1.3389 (1.3315) acc_x 65.6250 (65.7812) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [25/42] time 0.364 (0.445) data 0.233 (0.314) loss_x loss_x 0.9985 (1.3427) acc_x 87.5000 (66.1250) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [30/42] time 0.506 (0.453) data 0.375 (0.322) loss_x loss_x 1.3066 (1.2970) acc_x 68.7500 (66.7708) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [35/42] time 0.460 (0.453) data 0.329 (0.322) loss_x loss_x 1.5195 (1.3222) acc_x 62.5000 (66.5179) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [40/42] time 0.320 (0.454) data 0.189 (0.323) loss_x loss_x 1.8984 (1.3459) acc_x 59.3750 (65.9375) lr 7.6655e-04 eta 0:00:00
epoch [117/200] batch [5/55] time 0.454 (0.460) data 0.322 (0.328) loss_u loss_u 0.7090 (0.8153) acc_u 31.2500 (23.7500) lr 7.6655e-04 eta 0:00:22
epoch [117/200] batch [10/55] time 0.369 (0.456) data 0.237 (0.325) loss_u loss_u 0.8667 (0.8273) acc_u 21.8750 (23.1250) lr 7.6655e-04 eta 0:00:20
epoch [117/200] batch [15/55] time 0.567 (0.458) data 0.436 (0.327) loss_u loss_u 0.8774 (0.8312) acc_u 12.5000 (21.8750) lr 7.6655e-04 eta 0:00:18
epoch [117/200] batch [20/55] time 0.368 (0.455) data 0.236 (0.324) loss_u loss_u 0.9180 (0.8368) acc_u 9.3750 (21.0938) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [25/55] time 0.457 (0.459) data 0.325 (0.328) loss_u loss_u 0.8828 (0.8317) acc_u 12.5000 (22.1250) lr 7.6655e-04 eta 0:00:13
epoch [117/200] batch [30/55] time 0.579 (0.459) data 0.448 (0.327) loss_u loss_u 0.7920 (0.8311) acc_u 31.2500 (21.8750) lr 7.6655e-04 eta 0:00:11
epoch [117/200] batch [35/55] time 0.616 (0.460) data 0.483 (0.329) loss_u loss_u 0.8599 (0.8350) acc_u 18.7500 (21.0714) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [40/55] time 0.388 (0.460) data 0.256 (0.328) loss_u loss_u 0.8560 (0.8397) acc_u 21.8750 (20.6250) lr 7.6655e-04 eta 0:00:06
epoch [117/200] batch [45/55] time 0.533 (0.463) data 0.401 (0.331) loss_u loss_u 0.8496 (0.8418) acc_u 21.8750 (20.4167) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [50/55] time 0.473 (0.466) data 0.341 (0.334) loss_u loss_u 0.7480 (0.8386) acc_u 34.3750 (20.8750) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [55/55] time 0.539 (0.469) data 0.407 (0.338) loss_u loss_u 0.8223 (0.8360) acc_u 25.0000 (21.5341) lr 7.6655e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1449
confident_label rate tensor(0.4286, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1344
clean true:1343
clean false:1
clean_rate:0.9992559523809523
noisy true:344
noisy false:1448
after delete: len(clean_dataset) 1344
after delete: len(noisy_dataset) 1792
epoch [118/200] batch [5/42] time 0.654 (0.519) data 0.523 (0.388) loss_x loss_x 1.2383 (1.1635) acc_x 65.6250 (71.8750) lr 7.5131e-04 eta 0:00:19
epoch [118/200] batch [10/42] time 0.368 (0.487) data 0.237 (0.356) loss_x loss_x 1.0391 (1.1253) acc_x 68.7500 (71.5625) lr 7.5131e-04 eta 0:00:15
epoch [118/200] batch [15/42] time 0.695 (0.479) data 0.564 (0.348) loss_x loss_x 0.4583 (1.0799) acc_x 93.7500 (72.9167) lr 7.5131e-04 eta 0:00:12
epoch [118/200] batch [20/42] time 0.461 (0.476) data 0.327 (0.345) loss_x loss_x 1.2754 (1.1162) acc_x 71.8750 (72.5000) lr 7.5131e-04 eta 0:00:10
epoch [118/200] batch [25/42] time 0.504 (0.468) data 0.373 (0.336) loss_x loss_x 0.9575 (1.1398) acc_x 68.7500 (71.7500) lr 7.5131e-04 eta 0:00:07
epoch [118/200] batch [30/42] time 0.428 (0.472) data 0.297 (0.341) loss_x loss_x 1.5742 (1.1979) acc_x 62.5000 (69.8958) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [35/42] time 0.469 (0.479) data 0.337 (0.348) loss_x loss_x 1.0713 (1.2244) acc_x 71.8750 (69.6429) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [40/42] time 0.480 (0.478) data 0.349 (0.347) loss_x loss_x 1.1660 (1.2191) acc_x 65.6250 (69.7656) lr 7.5131e-04 eta 0:00:00
epoch [118/200] batch [5/56] time 0.514 (0.477) data 0.382 (0.345) loss_u loss_u 0.7695 (0.8134) acc_u 25.0000 (21.8750) lr 7.5131e-04 eta 0:00:24
epoch [118/200] batch [10/56] time 0.560 (0.478) data 0.428 (0.346) loss_u loss_u 0.8584 (0.8362) acc_u 18.7500 (19.0625) lr 7.5131e-04 eta 0:00:21
epoch [118/200] batch [15/56] time 0.339 (0.473) data 0.207 (0.341) loss_u loss_u 0.8818 (0.8409) acc_u 9.3750 (18.9583) lr 7.5131e-04 eta 0:00:19
epoch [118/200] batch [20/56] time 0.390 (0.468) data 0.259 (0.337) loss_u loss_u 0.8169 (0.8474) acc_u 21.8750 (18.7500) lr 7.5131e-04 eta 0:00:16
epoch [118/200] batch [25/56] time 0.344 (0.465) data 0.213 (0.333) loss_u loss_u 0.9028 (0.8437) acc_u 9.3750 (19.3750) lr 7.5131e-04 eta 0:00:14
epoch [118/200] batch [30/56] time 0.471 (0.464) data 0.340 (0.333) loss_u loss_u 0.8286 (0.8440) acc_u 25.0000 (19.7917) lr 7.5131e-04 eta 0:00:12
epoch [118/200] batch [35/56] time 0.454 (0.466) data 0.322 (0.335) loss_u loss_u 0.8237 (0.8405) acc_u 21.8750 (20.4464) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [40/56] time 0.400 (0.468) data 0.267 (0.336) loss_u loss_u 0.8926 (0.8432) acc_u 12.5000 (20.0781) lr 7.5131e-04 eta 0:00:07
epoch [118/200] batch [45/56] time 0.512 (0.469) data 0.381 (0.337) loss_u loss_u 0.8989 (0.8443) acc_u 12.5000 (20.0000) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [50/56] time 0.421 (0.470) data 0.289 (0.338) loss_u loss_u 0.8608 (0.8426) acc_u 15.6250 (20.0000) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [55/56] time 0.430 (0.472) data 0.299 (0.341) loss_u loss_u 0.8823 (0.8417) acc_u 15.6250 (20.2273) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1450
confident_label rate tensor(0.4228, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1326
clean true:1323
clean false:3
clean_rate:0.997737556561086
noisy true:363
noisy false:1447
after delete: len(clean_dataset) 1326
after delete: len(noisy_dataset) 1810
epoch [119/200] batch [5/41] time 0.410 (0.468) data 0.278 (0.337) loss_x loss_x 1.1006 (1.0391) acc_x 68.7500 (70.6250) lr 7.3613e-04 eta 0:00:16
epoch [119/200] batch [10/41] time 0.458 (0.444) data 0.327 (0.313) loss_x loss_x 0.9409 (1.1798) acc_x 78.1250 (69.3750) lr 7.3613e-04 eta 0:00:13
epoch [119/200] batch [15/41] time 0.510 (0.459) data 0.379 (0.328) loss_x loss_x 0.7188 (1.1556) acc_x 78.1250 (69.5833) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [20/41] time 0.407 (0.458) data 0.276 (0.327) loss_x loss_x 1.0605 (1.1131) acc_x 75.0000 (70.4688) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [25/41] time 0.368 (0.452) data 0.237 (0.321) loss_x loss_x 0.8716 (1.0941) acc_x 81.2500 (71.2500) lr 7.3613e-04 eta 0:00:07
epoch [119/200] batch [30/41] time 0.477 (0.453) data 0.346 (0.322) loss_x loss_x 1.0488 (1.0830) acc_x 71.8750 (70.9375) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [35/41] time 0.462 (0.456) data 0.331 (0.325) loss_x loss_x 0.9023 (1.0839) acc_x 81.2500 (71.4286) lr 7.3613e-04 eta 0:00:02
epoch [119/200] batch [40/41] time 0.527 (0.452) data 0.396 (0.321) loss_x loss_x 1.4531 (1.1093) acc_x 78.1250 (71.1719) lr 7.3613e-04 eta 0:00:00
epoch [119/200] batch [5/56] time 0.585 (0.460) data 0.453 (0.328) loss_u loss_u 0.8901 (0.8078) acc_u 12.5000 (27.5000) lr 7.3613e-04 eta 0:00:23
epoch [119/200] batch [10/56] time 0.454 (0.463) data 0.323 (0.332) loss_u loss_u 0.8330 (0.8104) acc_u 21.8750 (25.3125) lr 7.3613e-04 eta 0:00:21
epoch [119/200] batch [15/56] time 0.438 (0.464) data 0.307 (0.333) loss_u loss_u 0.7983 (0.8192) acc_u 25.0000 (23.7500) lr 7.3613e-04 eta 0:00:19
epoch [119/200] batch [20/56] time 0.410 (0.459) data 0.279 (0.327) loss_u loss_u 0.9028 (0.8331) acc_u 15.6250 (22.0312) lr 7.3613e-04 eta 0:00:16
epoch [119/200] batch [25/56] time 0.364 (0.454) data 0.232 (0.323) loss_u loss_u 0.9062 (0.8411) acc_u 9.3750 (20.6250) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [30/56] time 0.446 (0.453) data 0.314 (0.322) loss_u loss_u 0.8140 (0.8411) acc_u 25.0000 (20.2083) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [35/56] time 0.520 (0.454) data 0.388 (0.323) loss_u loss_u 0.8203 (0.8353) acc_u 25.0000 (20.8036) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [40/56] time 0.466 (0.455) data 0.333 (0.323) loss_u loss_u 0.8154 (0.8331) acc_u 21.8750 (21.2500) lr 7.3613e-04 eta 0:00:07
epoch [119/200] batch [45/56] time 0.382 (0.458) data 0.250 (0.327) loss_u loss_u 0.8848 (0.8342) acc_u 18.7500 (21.1111) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [50/56] time 0.507 (0.457) data 0.376 (0.326) loss_u loss_u 0.8267 (0.8350) acc_u 21.8750 (21.2500) lr 7.3613e-04 eta 0:00:02
epoch [119/200] batch [55/56] time 0.551 (0.459) data 0.418 (0.328) loss_u loss_u 0.8530 (0.8365) acc_u 18.7500 (21.0227) lr 7.3613e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1460
confident_label rate tensor(0.4270, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1339
clean true:1339
clean false:0
clean_rate:1.0
noisy true:337
noisy false:1460
after delete: len(clean_dataset) 1339
after delete: len(noisy_dataset) 1797
epoch [120/200] batch [5/41] time 0.434 (0.428) data 0.303 (0.296) loss_x loss_x 0.9268 (1.1795) acc_x 75.0000 (66.8750) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [10/41] time 0.480 (0.447) data 0.350 (0.316) loss_x loss_x 0.8350 (1.1916) acc_x 78.1250 (69.6875) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [15/41] time 0.472 (0.458) data 0.341 (0.327) loss_x loss_x 1.0615 (1.1803) acc_x 71.8750 (68.7500) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [20/41] time 0.403 (0.459) data 0.272 (0.328) loss_x loss_x 1.3789 (1.1749) acc_x 62.5000 (68.2812) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [25/41] time 0.425 (0.455) data 0.295 (0.324) loss_x loss_x 1.5781 (1.2012) acc_x 68.7500 (68.5000) lr 7.2101e-04 eta 0:00:07
epoch [120/200] batch [30/41] time 0.441 (0.460) data 0.310 (0.329) loss_x loss_x 1.7568 (1.2322) acc_x 56.2500 (67.6042) lr 7.2101e-04 eta 0:00:05
epoch [120/200] batch [35/41] time 0.487 (0.465) data 0.357 (0.334) loss_x loss_x 1.0400 (1.2339) acc_x 81.2500 (67.5893) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [40/41] time 0.392 (0.468) data 0.261 (0.337) loss_x loss_x 1.4492 (1.2482) acc_x 62.5000 (67.1875) lr 7.2101e-04 eta 0:00:00
epoch [120/200] batch [5/56] time 0.690 (0.474) data 0.559 (0.344) loss_u loss_u 0.8101 (0.8363) acc_u 18.7500 (20.0000) lr 7.2101e-04 eta 0:00:24
epoch [120/200] batch [10/56] time 0.408 (0.470) data 0.277 (0.339) loss_u loss_u 0.7686 (0.8376) acc_u 31.2500 (20.0000) lr 7.2101e-04 eta 0:00:21
epoch [120/200] batch [15/56] time 0.424 (0.465) data 0.293 (0.334) loss_u loss_u 0.7471 (0.8305) acc_u 28.1250 (20.4167) lr 7.2101e-04 eta 0:00:19
epoch [120/200] batch [20/56] time 0.366 (0.463) data 0.234 (0.332) loss_u loss_u 0.8120 (0.8327) acc_u 31.2500 (20.6250) lr 7.2101e-04 eta 0:00:16
epoch [120/200] batch [25/56] time 0.590 (0.463) data 0.459 (0.332) loss_u loss_u 0.8398 (0.8358) acc_u 12.5000 (20.1250) lr 7.2101e-04 eta 0:00:14
epoch [120/200] batch [30/56] time 0.369 (0.458) data 0.238 (0.327) loss_u loss_u 0.8730 (0.8371) acc_u 12.5000 (20.3125) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [35/56] time 0.294 (0.454) data 0.164 (0.323) loss_u loss_u 0.8384 (0.8375) acc_u 18.7500 (20.2679) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [40/56] time 0.442 (0.454) data 0.312 (0.323) loss_u loss_u 0.7334 (0.8358) acc_u 40.6250 (20.6250) lr 7.2101e-04 eta 0:00:07
epoch [120/200] batch [45/56] time 0.453 (0.452) data 0.322 (0.321) loss_u loss_u 0.8433 (0.8327) acc_u 18.7500 (20.8333) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [50/56] time 0.603 (0.453) data 0.472 (0.322) loss_u loss_u 0.8403 (0.8312) acc_u 25.0000 (21.1875) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [55/56] time 0.449 (0.453) data 0.317 (0.322) loss_u loss_u 0.8457 (0.8341) acc_u 21.8750 (20.9659) lr 7.2101e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1483
confident_label rate tensor(0.4165, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1306
clean true:1305
clean false:1
clean_rate:0.9992343032159265
noisy true:348
noisy false:1482
after delete: len(clean_dataset) 1306
after delete: len(noisy_dataset) 1830
epoch [121/200] batch [5/40] time 0.545 (0.453) data 0.415 (0.322) loss_x loss_x 0.9375 (1.2574) acc_x 68.7500 (66.2500) lr 7.0596e-04 eta 0:00:15
epoch [121/200] batch [10/40] time 0.425 (0.439) data 0.294 (0.308) loss_x loss_x 0.7144 (1.1552) acc_x 78.1250 (68.4375) lr 7.0596e-04 eta 0:00:13
epoch [121/200] batch [15/40] time 0.381 (0.436) data 0.250 (0.306) loss_x loss_x 1.2393 (1.1685) acc_x 62.5000 (67.7083) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [20/40] time 0.485 (0.444) data 0.355 (0.314) loss_x loss_x 1.1279 (1.1297) acc_x 78.1250 (69.5312) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [25/40] time 0.547 (0.449) data 0.416 (0.319) loss_x loss_x 0.9189 (1.1278) acc_x 75.0000 (69.5000) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [30/40] time 0.460 (0.453) data 0.329 (0.322) loss_x loss_x 0.6538 (1.1385) acc_x 84.3750 (69.8958) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [35/40] time 0.483 (0.463) data 0.353 (0.332) loss_x loss_x 0.7168 (1.1489) acc_x 84.3750 (69.6429) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [40/40] time 0.353 (0.457) data 0.222 (0.326) loss_x loss_x 0.8052 (1.1476) acc_x 81.2500 (69.7656) lr 7.0596e-04 eta 0:00:00
epoch [121/200] batch [5/57] time 0.523 (0.454) data 0.389 (0.323) loss_u loss_u 0.8579 (0.8592) acc_u 25.0000 (20.6250) lr 7.0596e-04 eta 0:00:23
epoch [121/200] batch [10/57] time 0.527 (0.462) data 0.395 (0.331) loss_u loss_u 0.7715 (0.8334) acc_u 28.1250 (23.4375) lr 7.0596e-04 eta 0:00:21
epoch [121/200] batch [15/57] time 0.400 (0.461) data 0.269 (0.330) loss_u loss_u 0.8350 (0.8218) acc_u 15.6250 (24.7917) lr 7.0596e-04 eta 0:00:19
epoch [121/200] batch [20/57] time 0.395 (0.458) data 0.263 (0.327) loss_u loss_u 0.7700 (0.8187) acc_u 28.1250 (24.3750) lr 7.0596e-04 eta 0:00:16
epoch [121/200] batch [25/57] time 0.433 (0.460) data 0.301 (0.328) loss_u loss_u 0.8271 (0.8218) acc_u 21.8750 (23.3750) lr 7.0596e-04 eta 0:00:14
epoch [121/200] batch [30/57] time 0.402 (0.457) data 0.270 (0.326) loss_u loss_u 0.8301 (0.8205) acc_u 21.8750 (23.5417) lr 7.0596e-04 eta 0:00:12
epoch [121/200] batch [35/57] time 0.367 (0.456) data 0.235 (0.325) loss_u loss_u 0.7930 (0.8234) acc_u 28.1250 (23.3929) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [40/57] time 0.446 (0.457) data 0.315 (0.326) loss_u loss_u 0.8223 (0.8223) acc_u 15.6250 (23.1250) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [45/57] time 0.388 (0.459) data 0.255 (0.328) loss_u loss_u 0.9268 (0.8274) acc_u 12.5000 (22.7083) lr 7.0596e-04 eta 0:00:05
epoch [121/200] batch [50/57] time 0.451 (0.462) data 0.318 (0.330) loss_u loss_u 0.7812 (0.8266) acc_u 25.0000 (22.7500) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [55/57] time 0.377 (0.460) data 0.245 (0.329) loss_u loss_u 0.7959 (0.8258) acc_u 31.2500 (23.0682) lr 7.0596e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1473
confident_label rate tensor(0.4203, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1318
clean true:1311
clean false:7
clean_rate:0.9946889226100152
noisy true:352
noisy false:1466
after delete: len(clean_dataset) 1318
after delete: len(noisy_dataset) 1818
epoch [122/200] batch [5/41] time 0.470 (0.531) data 0.340 (0.401) loss_x loss_x 0.5928 (1.0411) acc_x 84.3750 (71.8750) lr 6.9098e-04 eta 0:00:19
epoch [122/200] batch [10/41] time 0.555 (0.507) data 0.425 (0.377) loss_x loss_x 0.9258 (1.0624) acc_x 65.6250 (72.1875) lr 6.9098e-04 eta 0:00:15
epoch [122/200] batch [15/41] time 0.466 (0.502) data 0.336 (0.372) loss_x loss_x 1.5312 (1.0704) acc_x 68.7500 (72.0833) lr 6.9098e-04 eta 0:00:13
epoch [122/200] batch [20/41] time 0.544 (0.495) data 0.414 (0.364) loss_x loss_x 0.9922 (1.0755) acc_x 71.8750 (72.5000) lr 6.9098e-04 eta 0:00:10
epoch [122/200] batch [25/41] time 0.480 (0.497) data 0.350 (0.366) loss_x loss_x 0.5415 (1.1268) acc_x 90.6250 (72.0000) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [30/41] time 0.485 (0.493) data 0.354 (0.363) loss_x loss_x 0.7583 (1.1224) acc_x 84.3750 (72.1875) lr 6.9098e-04 eta 0:00:05
epoch [122/200] batch [35/41] time 0.508 (0.485) data 0.376 (0.354) loss_x loss_x 1.2510 (1.1215) acc_x 62.5000 (72.0536) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [40/41] time 0.327 (0.475) data 0.196 (0.344) loss_x loss_x 1.1729 (1.1366) acc_x 68.7500 (72.0312) lr 6.9098e-04 eta 0:00:00
epoch [122/200] batch [5/56] time 0.459 (0.472) data 0.327 (0.341) loss_u loss_u 0.7720 (0.8368) acc_u 34.3750 (21.8750) lr 6.9098e-04 eta 0:00:24
epoch [122/200] batch [10/56] time 0.454 (0.467) data 0.322 (0.337) loss_u loss_u 0.8682 (0.8388) acc_u 15.6250 (20.6250) lr 6.9098e-04 eta 0:00:21
epoch [122/200] batch [15/56] time 0.404 (0.465) data 0.273 (0.334) loss_u loss_u 0.8511 (0.8528) acc_u 21.8750 (18.9583) lr 6.9098e-04 eta 0:00:19
epoch [122/200] batch [20/56] time 0.506 (0.467) data 0.375 (0.336) loss_u loss_u 0.8457 (0.8449) acc_u 21.8750 (19.8438) lr 6.9098e-04 eta 0:00:16
epoch [122/200] batch [25/56] time 0.661 (0.475) data 0.528 (0.344) loss_u loss_u 0.7798 (0.8392) acc_u 25.0000 (20.3750) lr 6.9098e-04 eta 0:00:14
epoch [122/200] batch [30/56] time 0.504 (0.478) data 0.373 (0.347) loss_u loss_u 0.8369 (0.8337) acc_u 18.7500 (21.0417) lr 6.9098e-04 eta 0:00:12
epoch [122/200] batch [35/56] time 0.385 (0.475) data 0.252 (0.344) loss_u loss_u 0.8613 (0.8348) acc_u 18.7500 (21.0714) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [40/56] time 0.522 (0.479) data 0.389 (0.347) loss_u loss_u 0.8530 (0.8362) acc_u 21.8750 (20.7031) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [45/56] time 0.429 (0.478) data 0.296 (0.346) loss_u loss_u 0.8057 (0.8328) acc_u 25.0000 (21.3889) lr 6.9098e-04 eta 0:00:05
epoch [122/200] batch [50/56] time 0.395 (0.476) data 0.263 (0.344) loss_u loss_u 0.8120 (0.8317) acc_u 21.8750 (21.3750) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [55/56] time 0.415 (0.474) data 0.283 (0.342) loss_u loss_u 0.8472 (0.8331) acc_u 15.6250 (21.0795) lr 6.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1492
confident_label rate tensor(0.4082, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1280
clean true:1279
clean false:1
clean_rate:0.99921875
noisy true:365
noisy false:1491
after delete: len(clean_dataset) 1280
after delete: len(noisy_dataset) 1856
epoch [123/200] batch [5/40] time 0.498 (0.470) data 0.366 (0.338) loss_x loss_x 1.4316 (1.1159) acc_x 62.5000 (71.8750) lr 6.7608e-04 eta 0:00:16
epoch [123/200] batch [10/40] time 0.459 (0.461) data 0.327 (0.330) loss_x loss_x 1.2676 (1.1998) acc_x 56.2500 (66.8750) lr 6.7608e-04 eta 0:00:13
epoch [123/200] batch [15/40] time 0.535 (0.454) data 0.402 (0.323) loss_x loss_x 1.3174 (1.1790) acc_x 62.5000 (67.5000) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [20/40] time 0.436 (0.459) data 0.305 (0.328) loss_x loss_x 1.3291 (1.1625) acc_x 75.0000 (67.9688) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [25/40] time 0.420 (0.462) data 0.291 (0.331) loss_x loss_x 1.0371 (1.1145) acc_x 78.1250 (69.5000) lr 6.7608e-04 eta 0:00:06
epoch [123/200] batch [30/40] time 0.455 (0.467) data 0.323 (0.336) loss_x loss_x 0.7700 (1.1049) acc_x 81.2500 (69.7917) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [35/40] time 0.496 (0.470) data 0.366 (0.339) loss_x loss_x 1.1494 (1.1265) acc_x 68.7500 (69.3750) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [40/40] time 0.490 (0.469) data 0.360 (0.339) loss_x loss_x 1.5547 (1.1448) acc_x 65.6250 (69.6094) lr 6.7608e-04 eta 0:00:00
epoch [123/200] batch [5/58] time 0.443 (0.465) data 0.312 (0.334) loss_u loss_u 0.7959 (0.8050) acc_u 34.3750 (29.3750) lr 6.7608e-04 eta 0:00:24
epoch [123/200] batch [10/58] time 0.326 (0.461) data 0.196 (0.330) loss_u loss_u 0.9116 (0.8264) acc_u 15.6250 (25.3125) lr 6.7608e-04 eta 0:00:22
epoch [123/200] batch [15/58] time 0.408 (0.458) data 0.277 (0.327) loss_u loss_u 0.8691 (0.8149) acc_u 15.6250 (25.8333) lr 6.7608e-04 eta 0:00:19
epoch [123/200] batch [20/58] time 0.419 (0.457) data 0.288 (0.327) loss_u loss_u 0.7505 (0.8123) acc_u 28.1250 (25.1562) lr 6.7608e-04 eta 0:00:17
epoch [123/200] batch [25/58] time 0.475 (0.458) data 0.343 (0.327) loss_u loss_u 0.7534 (0.8187) acc_u 28.1250 (23.8750) lr 6.7608e-04 eta 0:00:15
epoch [123/200] batch [30/58] time 0.481 (0.458) data 0.350 (0.327) loss_u loss_u 0.9111 (0.8242) acc_u 12.5000 (23.2292) lr 6.7608e-04 eta 0:00:12
epoch [123/200] batch [35/58] time 0.413 (0.458) data 0.282 (0.326) loss_u loss_u 0.8218 (0.8287) acc_u 18.7500 (22.4107) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [40/58] time 0.537 (0.460) data 0.406 (0.328) loss_u loss_u 0.8359 (0.8272) acc_u 15.6250 (22.1094) lr 6.7608e-04 eta 0:00:08
epoch [123/200] batch [45/58] time 0.430 (0.462) data 0.299 (0.331) loss_u loss_u 0.8730 (0.8265) acc_u 18.7500 (22.3611) lr 6.7608e-04 eta 0:00:06
epoch [123/200] batch [50/58] time 0.375 (0.461) data 0.243 (0.330) loss_u loss_u 0.7871 (0.8273) acc_u 31.2500 (22.3750) lr 6.7608e-04 eta 0:00:03
epoch [123/200] batch [55/58] time 0.419 (0.464) data 0.288 (0.333) loss_u loss_u 0.8716 (0.8287) acc_u 15.6250 (22.1023) lr 6.7608e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1447
confident_label rate tensor(0.4279, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1342
clean true:1341
clean false:1
clean_rate:0.9992548435171386
noisy true:348
noisy false:1446
after delete: len(clean_dataset) 1342
after delete: len(noisy_dataset) 1794
epoch [124/200] batch [5/41] time 0.498 (0.482) data 0.367 (0.350) loss_x loss_x 0.9937 (1.2221) acc_x 75.0000 (70.6250) lr 6.6126e-04 eta 0:00:17
epoch [124/200] batch [10/41] time 0.518 (0.481) data 0.386 (0.350) loss_x loss_x 0.8398 (1.1896) acc_x 84.3750 (70.6250) lr 6.6126e-04 eta 0:00:14
epoch [124/200] batch [15/41] time 0.559 (0.494) data 0.429 (0.363) loss_x loss_x 1.0674 (1.1989) acc_x 65.6250 (70.4167) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [20/41] time 0.635 (0.494) data 0.504 (0.363) loss_x loss_x 0.5952 (1.1411) acc_x 84.3750 (70.9375) lr 6.6126e-04 eta 0:00:10
epoch [124/200] batch [25/41] time 0.475 (0.485) data 0.344 (0.355) loss_x loss_x 0.9082 (1.1522) acc_x 65.6250 (70.5000) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [30/41] time 0.472 (0.498) data 0.341 (0.367) loss_x loss_x 1.5674 (1.2032) acc_x 62.5000 (69.4792) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [35/41] time 0.523 (0.496) data 0.392 (0.365) loss_x loss_x 0.8599 (1.1863) acc_x 78.1250 (69.6429) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [40/41] time 0.487 (0.491) data 0.356 (0.360) loss_x loss_x 1.0732 (1.1817) acc_x 75.0000 (69.3750) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/56] time 0.436 (0.492) data 0.305 (0.361) loss_u loss_u 0.8872 (0.8376) acc_u 15.6250 (20.6250) lr 6.6126e-04 eta 0:00:25
epoch [124/200] batch [10/56] time 0.423 (0.492) data 0.293 (0.361) loss_u loss_u 0.6934 (0.8246) acc_u 40.6250 (21.8750) lr 6.6126e-04 eta 0:00:22
epoch [124/200] batch [15/56] time 0.390 (0.486) data 0.260 (0.355) loss_u loss_u 0.8228 (0.8386) acc_u 18.7500 (19.7917) lr 6.6126e-04 eta 0:00:19
epoch [124/200] batch [20/56] time 0.419 (0.483) data 0.288 (0.352) loss_u loss_u 0.8936 (0.8427) acc_u 12.5000 (19.3750) lr 6.6126e-04 eta 0:00:17
epoch [124/200] batch [25/56] time 0.724 (0.492) data 0.591 (0.360) loss_u loss_u 0.9189 (0.8463) acc_u 15.6250 (19.0000) lr 6.6126e-04 eta 0:00:15
epoch [124/200] batch [30/56] time 0.459 (0.492) data 0.328 (0.361) loss_u loss_u 0.8257 (0.8424) acc_u 21.8750 (20.2083) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [35/56] time 0.428 (0.488) data 0.296 (0.357) loss_u loss_u 0.8145 (0.8399) acc_u 25.0000 (20.9821) lr 6.6126e-04 eta 0:00:10
epoch [124/200] batch [40/56] time 0.401 (0.483) data 0.269 (0.352) loss_u loss_u 0.8608 (0.8378) acc_u 18.7500 (20.9375) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [45/56] time 0.419 (0.481) data 0.288 (0.350) loss_u loss_u 0.8584 (0.8418) acc_u 18.7500 (20.6250) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [50/56] time 0.437 (0.477) data 0.306 (0.346) loss_u loss_u 0.8599 (0.8437) acc_u 21.8750 (20.2500) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [55/56] time 0.352 (0.474) data 0.221 (0.343) loss_u loss_u 0.8687 (0.8405) acc_u 18.7500 (20.5682) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1445
confident_label rate tensor(0.4228, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1326
clean true:1325
clean false:1
clean_rate:0.9992458521870287
noisy true:366
noisy false:1444
after delete: len(clean_dataset) 1326
after delete: len(noisy_dataset) 1810
epoch [125/200] batch [5/41] time 0.393 (0.470) data 0.263 (0.339) loss_x loss_x 0.9795 (1.1232) acc_x 78.1250 (69.3750) lr 6.4653e-04 eta 0:00:16
epoch [125/200] batch [10/41] time 0.353 (0.465) data 0.222 (0.334) loss_x loss_x 1.6367 (1.2267) acc_x 68.7500 (67.8125) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [15/41] time 0.439 (0.485) data 0.309 (0.355) loss_x loss_x 1.2871 (1.2453) acc_x 59.3750 (66.8750) lr 6.4653e-04 eta 0:00:12
epoch [125/200] batch [20/41] time 0.438 (0.493) data 0.307 (0.363) loss_x loss_x 1.5908 (1.2864) acc_x 65.6250 (67.1875) lr 6.4653e-04 eta 0:00:10
epoch [125/200] batch [25/41] time 0.387 (0.482) data 0.256 (0.351) loss_x loss_x 1.2764 (1.2565) acc_x 62.5000 (67.0000) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [30/41] time 0.426 (0.475) data 0.295 (0.344) loss_x loss_x 1.8604 (1.2873) acc_x 71.8750 (66.7708) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [35/41] time 0.408 (0.475) data 0.277 (0.344) loss_x loss_x 1.2910 (1.3182) acc_x 62.5000 (65.6250) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [40/41] time 0.539 (0.478) data 0.407 (0.347) loss_x loss_x 1.1680 (1.3014) acc_x 71.8750 (66.5625) lr 6.4653e-04 eta 0:00:00
epoch [125/200] batch [5/56] time 0.397 (0.475) data 0.265 (0.344) loss_u loss_u 0.8594 (0.8933) acc_u 18.7500 (15.0000) lr 6.4653e-04 eta 0:00:24
epoch [125/200] batch [10/56] time 0.447 (0.478) data 0.315 (0.347) loss_u loss_u 0.8750 (0.8726) acc_u 21.8750 (17.8125) lr 6.4653e-04 eta 0:00:21
epoch [125/200] batch [15/56] time 0.411 (0.474) data 0.279 (0.343) loss_u loss_u 0.7764 (0.8502) acc_u 31.2500 (20.6250) lr 6.4653e-04 eta 0:00:19
epoch [125/200] batch [20/56] time 0.680 (0.476) data 0.548 (0.345) loss_u loss_u 0.8379 (0.8431) acc_u 25.0000 (21.0938) lr 6.4653e-04 eta 0:00:17
epoch [125/200] batch [25/56] time 0.421 (0.474) data 0.289 (0.343) loss_u loss_u 0.7412 (0.8395) acc_u 34.3750 (21.6250) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [30/56] time 0.443 (0.472) data 0.311 (0.340) loss_u loss_u 0.8369 (0.8400) acc_u 18.7500 (21.1458) lr 6.4653e-04 eta 0:00:12
epoch [125/200] batch [35/56] time 0.548 (0.469) data 0.415 (0.337) loss_u loss_u 0.7681 (0.8322) acc_u 31.2500 (21.9643) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [40/56] time 0.429 (0.469) data 0.298 (0.337) loss_u loss_u 0.8418 (0.8312) acc_u 28.1250 (22.3438) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [45/56] time 0.446 (0.467) data 0.314 (0.336) loss_u loss_u 0.8237 (0.8279) acc_u 25.0000 (22.7778) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [50/56] time 0.525 (0.468) data 0.394 (0.337) loss_u loss_u 0.8423 (0.8282) acc_u 28.1250 (22.6875) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [55/56] time 0.348 (0.468) data 0.217 (0.337) loss_u loss_u 0.8740 (0.8316) acc_u 12.5000 (21.9886) lr 6.4653e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1472
confident_label rate tensor(0.4225, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1325
clean true:1323
clean false:2
clean_rate:0.9984905660377359
noisy true:341
noisy false:1470
after delete: len(clean_dataset) 1325
after delete: len(noisy_dataset) 1811
epoch [126/200] batch [5/41] time 0.527 (0.493) data 0.396 (0.361) loss_x loss_x 0.8354 (1.2505) acc_x 81.2500 (63.7500) lr 6.3188e-04 eta 0:00:17
epoch [126/200] batch [10/41] time 0.417 (0.457) data 0.286 (0.326) loss_x loss_x 1.0430 (1.2092) acc_x 75.0000 (68.1250) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [15/41] time 0.456 (0.463) data 0.325 (0.332) loss_x loss_x 1.2881 (1.1993) acc_x 65.6250 (69.1667) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [20/41] time 0.407 (0.458) data 0.277 (0.327) loss_x loss_x 1.5039 (1.2046) acc_x 59.3750 (67.9688) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [25/41] time 0.419 (0.450) data 0.288 (0.319) loss_x loss_x 1.3096 (1.1741) acc_x 68.7500 (69.1250) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [30/41] time 0.507 (0.457) data 0.377 (0.326) loss_x loss_x 1.4873 (1.1661) acc_x 59.3750 (69.8958) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [35/41] time 0.480 (0.460) data 0.348 (0.329) loss_x loss_x 0.9736 (1.1897) acc_x 68.7500 (69.1964) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [40/41] time 0.475 (0.458) data 0.343 (0.328) loss_x loss_x 1.5156 (1.2004) acc_x 53.1250 (68.6719) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/56] time 0.589 (0.471) data 0.457 (0.340) loss_u loss_u 0.9233 (0.8641) acc_u 9.3750 (16.2500) lr 6.3188e-04 eta 0:00:24
epoch [126/200] batch [10/56] time 0.388 (0.466) data 0.256 (0.335) loss_u loss_u 0.8193 (0.8541) acc_u 25.0000 (18.1250) lr 6.3188e-04 eta 0:00:21
epoch [126/200] batch [15/56] time 0.415 (0.472) data 0.283 (0.340) loss_u loss_u 0.8203 (0.8493) acc_u 18.7500 (18.1250) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [20/56] time 0.354 (0.472) data 0.222 (0.341) loss_u loss_u 0.9170 (0.8386) acc_u 9.3750 (20.0000) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [25/56] time 0.392 (0.467) data 0.260 (0.335) loss_u loss_u 0.8398 (0.8367) acc_u 21.8750 (20.6250) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [30/56] time 0.509 (0.466) data 0.377 (0.334) loss_u loss_u 0.9023 (0.8410) acc_u 9.3750 (20.1042) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [35/56] time 0.410 (0.461) data 0.278 (0.330) loss_u loss_u 0.7920 (0.8376) acc_u 21.8750 (20.0000) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [40/56] time 0.419 (0.462) data 0.288 (0.330) loss_u loss_u 0.8691 (0.8380) acc_u 15.6250 (20.0781) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [45/56] time 0.475 (0.462) data 0.343 (0.331) loss_u loss_u 0.9014 (0.8336) acc_u 12.5000 (20.7639) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [50/56] time 0.463 (0.466) data 0.331 (0.334) loss_u loss_u 0.8091 (0.8287) acc_u 21.8750 (21.3750) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [55/56] time 0.458 (0.465) data 0.326 (0.334) loss_u loss_u 0.9043 (0.8330) acc_u 12.5000 (20.9091) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1445
confident_label rate tensor(0.4318, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1354
clean true:1352
clean false:2
clean_rate:0.9985228951255539
noisy true:339
noisy false:1443
after delete: len(clean_dataset) 1354
after delete: len(noisy_dataset) 1782
epoch [127/200] batch [5/42] time 0.424 (0.440) data 0.293 (0.309) loss_x loss_x 1.3320 (1.1938) acc_x 71.8750 (70.0000) lr 6.1732e-04 eta 0:00:16
epoch [127/200] batch [10/42] time 0.435 (0.439) data 0.304 (0.308) loss_x loss_x 0.7266 (1.1732) acc_x 71.8750 (70.9375) lr 6.1732e-04 eta 0:00:14
epoch [127/200] batch [15/42] time 0.431 (0.445) data 0.301 (0.314) loss_x loss_x 0.9087 (1.2244) acc_x 81.2500 (70.2083) lr 6.1732e-04 eta 0:00:12
epoch [127/200] batch [20/42] time 0.404 (0.452) data 0.273 (0.321) loss_x loss_x 1.0039 (1.1562) acc_x 65.6250 (71.0938) lr 6.1732e-04 eta 0:00:09
epoch [127/200] batch [25/42] time 0.404 (0.454) data 0.273 (0.323) loss_x loss_x 1.4736 (1.1645) acc_x 68.7500 (71.2500) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [30/42] time 0.433 (0.464) data 0.303 (0.333) loss_x loss_x 1.4795 (1.1707) acc_x 68.7500 (70.9375) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [35/42] time 0.471 (0.466) data 0.340 (0.335) loss_x loss_x 1.4814 (1.1547) acc_x 62.5000 (71.3393) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [40/42] time 0.547 (0.467) data 0.416 (0.336) loss_x loss_x 1.3232 (1.1703) acc_x 65.6250 (70.8594) lr 6.1732e-04 eta 0:00:00
epoch [127/200] batch [5/55] time 0.692 (0.472) data 0.562 (0.342) loss_u loss_u 0.8379 (0.8656) acc_u 25.0000 (18.7500) lr 6.1732e-04 eta 0:00:23
epoch [127/200] batch [10/55] time 0.451 (0.471) data 0.320 (0.340) loss_u loss_u 0.8140 (0.8527) acc_u 28.1250 (19.3750) lr 6.1732e-04 eta 0:00:21
epoch [127/200] batch [15/55] time 0.526 (0.469) data 0.395 (0.338) loss_u loss_u 0.8125 (0.8485) acc_u 21.8750 (19.5833) lr 6.1732e-04 eta 0:00:18
epoch [127/200] batch [20/55] time 0.355 (0.465) data 0.223 (0.334) loss_u loss_u 0.8511 (0.8515) acc_u 18.7500 (19.5312) lr 6.1732e-04 eta 0:00:16
epoch [127/200] batch [25/55] time 0.506 (0.463) data 0.374 (0.332) loss_u loss_u 0.7627 (0.8500) acc_u 25.0000 (19.5000) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [30/55] time 0.558 (0.464) data 0.428 (0.333) loss_u loss_u 0.8320 (0.8444) acc_u 21.8750 (20.1042) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [35/55] time 0.559 (0.465) data 0.427 (0.334) loss_u loss_u 0.8687 (0.8428) acc_u 15.6250 (20.1786) lr 6.1732e-04 eta 0:00:09
epoch [127/200] batch [40/55] time 0.581 (0.468) data 0.449 (0.337) loss_u loss_u 0.9019 (0.8380) acc_u 12.5000 (20.7031) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [45/55] time 0.635 (0.469) data 0.503 (0.338) loss_u loss_u 0.8691 (0.8390) acc_u 12.5000 (20.4167) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [50/55] time 0.390 (0.466) data 0.259 (0.335) loss_u loss_u 0.8193 (0.8373) acc_u 21.8750 (20.8125) lr 6.1732e-04 eta 0:00:02
epoch [127/200] batch [55/55] time 0.734 (0.470) data 0.602 (0.339) loss_u loss_u 0.7393 (0.8350) acc_u 31.2500 (21.1364) lr 6.1732e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1450
confident_label rate tensor(0.4254, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1334
clean true:1334
clean false:0
clean_rate:1.0
noisy true:352
noisy false:1450
after delete: len(clean_dataset) 1334
after delete: len(noisy_dataset) 1802
epoch [128/200] batch [5/41] time 0.443 (0.555) data 0.313 (0.424) loss_x loss_x 1.2363 (1.1544) acc_x 56.2500 (68.1250) lr 6.0285e-04 eta 0:00:19
epoch [128/200] batch [10/41] time 0.413 (0.515) data 0.281 (0.383) loss_x loss_x 1.1602 (1.2160) acc_x 71.8750 (67.5000) lr 6.0285e-04 eta 0:00:15
epoch [128/200] batch [15/41] time 0.381 (0.505) data 0.250 (0.374) loss_x loss_x 1.4707 (1.2301) acc_x 59.3750 (66.4583) lr 6.0285e-04 eta 0:00:13
epoch [128/200] batch [20/41] time 0.367 (0.483) data 0.237 (0.352) loss_x loss_x 1.2803 (1.2246) acc_x 62.5000 (66.7188) lr 6.0285e-04 eta 0:00:10
epoch [128/200] batch [25/41] time 0.452 (0.477) data 0.321 (0.346) loss_x loss_x 1.0400 (1.2083) acc_x 75.0000 (67.1250) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [30/41] time 0.431 (0.482) data 0.300 (0.351) loss_x loss_x 1.0488 (1.2046) acc_x 78.1250 (67.5000) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [35/41] time 0.447 (0.477) data 0.315 (0.346) loss_x loss_x 1.5830 (1.1933) acc_x 68.7500 (68.0357) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [40/41] time 0.408 (0.470) data 0.277 (0.339) loss_x loss_x 0.9375 (1.1916) acc_x 84.3750 (68.1250) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/56] time 0.589 (0.472) data 0.458 (0.341) loss_u loss_u 0.7905 (0.7909) acc_u 31.2500 (27.5000) lr 6.0285e-04 eta 0:00:24
epoch [128/200] batch [10/56] time 0.525 (0.473) data 0.395 (0.341) loss_u loss_u 0.8667 (0.8215) acc_u 12.5000 (23.1250) lr 6.0285e-04 eta 0:00:21
epoch [128/200] batch [15/56] time 0.484 (0.472) data 0.352 (0.341) loss_u loss_u 0.8906 (0.8443) acc_u 12.5000 (20.0000) lr 6.0285e-04 eta 0:00:19
epoch [128/200] batch [20/56] time 0.567 (0.475) data 0.435 (0.343) loss_u loss_u 0.8311 (0.8427) acc_u 18.7500 (20.0000) lr 6.0285e-04 eta 0:00:17
epoch [128/200] batch [25/56] time 0.381 (0.469) data 0.249 (0.338) loss_u loss_u 0.8315 (0.8360) acc_u 18.7500 (20.2500) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [30/56] time 0.488 (0.466) data 0.357 (0.335) loss_u loss_u 0.7988 (0.8332) acc_u 15.6250 (20.3125) lr 6.0285e-04 eta 0:00:12
epoch [128/200] batch [35/56] time 0.478 (0.466) data 0.345 (0.335) loss_u loss_u 0.8394 (0.8296) acc_u 18.7500 (20.9821) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [40/56] time 0.720 (0.469) data 0.586 (0.338) loss_u loss_u 0.8330 (0.8296) acc_u 18.7500 (21.0156) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [45/56] time 0.479 (0.470) data 0.344 (0.339) loss_u loss_u 0.8149 (0.8295) acc_u 31.2500 (21.0417) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [50/56] time 0.565 (0.474) data 0.433 (0.343) loss_u loss_u 0.7979 (0.8311) acc_u 31.2500 (21.2500) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [55/56] time 0.489 (0.476) data 0.357 (0.344) loss_u loss_u 0.8374 (0.8316) acc_u 18.7500 (21.1364) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1438
confident_label rate tensor(0.4283, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1343
clean true:1339
clean false:4
clean_rate:0.9970215934475056
noisy true:359
noisy false:1434
after delete: len(clean_dataset) 1343
after delete: len(noisy_dataset) 1793
epoch [129/200] batch [5/41] time 0.424 (0.446) data 0.295 (0.315) loss_x loss_x 0.9473 (1.1086) acc_x 71.8750 (70.0000) lr 5.8849e-04 eta 0:00:16
epoch [129/200] batch [10/41] time 0.471 (0.461) data 0.340 (0.330) loss_x loss_x 1.4736 (1.2465) acc_x 71.8750 (70.3125) lr 5.8849e-04 eta 0:00:14
epoch [129/200] batch [15/41] time 0.457 (0.467) data 0.326 (0.337) loss_x loss_x 1.1055 (1.2320) acc_x 59.3750 (70.4167) lr 5.8849e-04 eta 0:00:12
epoch [129/200] batch [20/41] time 0.450 (0.464) data 0.321 (0.333) loss_x loss_x 1.2002 (1.1769) acc_x 78.1250 (71.8750) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [25/41] time 0.389 (0.455) data 0.259 (0.324) loss_x loss_x 0.6865 (1.1488) acc_x 84.3750 (72.1250) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [30/41] time 0.397 (0.454) data 0.266 (0.323) loss_x loss_x 0.8232 (1.1447) acc_x 87.5000 (71.4583) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [35/41] time 0.457 (0.460) data 0.326 (0.329) loss_x loss_x 1.1465 (1.1789) acc_x 71.8750 (70.7143) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [40/41] time 0.710 (0.472) data 0.578 (0.341) loss_x loss_x 0.8594 (1.1991) acc_x 84.3750 (70.2344) lr 5.8849e-04 eta 0:00:00
epoch [129/200] batch [5/56] time 0.477 (0.470) data 0.345 (0.339) loss_u loss_u 0.8574 (0.8517) acc_u 12.5000 (17.5000) lr 5.8849e-04 eta 0:00:23
epoch [129/200] batch [10/56] time 0.384 (0.467) data 0.253 (0.336) loss_u loss_u 0.7959 (0.8359) acc_u 21.8750 (21.2500) lr 5.8849e-04 eta 0:00:21
epoch [129/200] batch [15/56] time 0.441 (0.465) data 0.311 (0.334) loss_u loss_u 0.8149 (0.8427) acc_u 28.1250 (20.2083) lr 5.8849e-04 eta 0:00:19
epoch [129/200] batch [20/56] time 0.555 (0.465) data 0.423 (0.334) loss_u loss_u 0.8281 (0.8443) acc_u 21.8750 (20.6250) lr 5.8849e-04 eta 0:00:16
epoch [129/200] batch [25/56] time 0.426 (0.465) data 0.294 (0.334) loss_u loss_u 0.7930 (0.8384) acc_u 28.1250 (21.1250) lr 5.8849e-04 eta 0:00:14
epoch [129/200] batch [30/56] time 0.581 (0.468) data 0.449 (0.336) loss_u loss_u 0.8936 (0.8362) acc_u 18.7500 (21.7708) lr 5.8849e-04 eta 0:00:12
epoch [129/200] batch [35/56] time 0.418 (0.466) data 0.286 (0.335) loss_u loss_u 0.8647 (0.8358) acc_u 15.6250 (21.8750) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [40/56] time 0.332 (0.464) data 0.200 (0.333) loss_u loss_u 0.9424 (0.8402) acc_u 6.2500 (21.0156) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [45/56] time 0.431 (0.462) data 0.299 (0.331) loss_u loss_u 0.8413 (0.8359) acc_u 18.7500 (21.6667) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [50/56] time 0.688 (0.466) data 0.555 (0.335) loss_u loss_u 0.9141 (0.8386) acc_u 15.6250 (21.5000) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [55/56] time 0.524 (0.469) data 0.392 (0.337) loss_u loss_u 0.6851 (0.8331) acc_u 40.6250 (22.3864) lr 5.8849e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1486
confident_label rate tensor(0.4184, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1312
clean true:1310
clean false:2
clean_rate:0.9984756097560976
noisy true:340
noisy false:1484
after delete: len(clean_dataset) 1312
after delete: len(noisy_dataset) 1824
epoch [130/200] batch [5/41] time 0.510 (0.450) data 0.379 (0.319) loss_x loss_x 1.0029 (1.0419) acc_x 75.0000 (71.8750) lr 5.7422e-04 eta 0:00:16
epoch [130/200] batch [10/41] time 0.370 (0.448) data 0.239 (0.316) loss_x loss_x 1.2324 (1.0324) acc_x 71.8750 (73.7500) lr 5.7422e-04 eta 0:00:13
epoch [130/200] batch [15/41] time 0.434 (0.452) data 0.303 (0.321) loss_x loss_x 1.2764 (1.1736) acc_x 75.0000 (70.4167) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [20/41] time 0.642 (0.474) data 0.512 (0.343) loss_x loss_x 1.7451 (1.2384) acc_x 56.2500 (69.2188) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [25/41] time 0.609 (0.487) data 0.478 (0.356) loss_x loss_x 1.3311 (1.2449) acc_x 71.8750 (69.7500) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [30/41] time 0.516 (0.484) data 0.385 (0.353) loss_x loss_x 1.1260 (1.2299) acc_x 75.0000 (70.0000) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [35/41] time 0.412 (0.477) data 0.282 (0.346) loss_x loss_x 0.7065 (1.2128) acc_x 71.8750 (70.4464) lr 5.7422e-04 eta 0:00:02
epoch [130/200] batch [40/41] time 0.465 (0.471) data 0.333 (0.341) loss_x loss_x 1.1348 (1.2052) acc_x 71.8750 (70.4688) lr 5.7422e-04 eta 0:00:00
epoch [130/200] batch [5/57] time 0.409 (0.480) data 0.279 (0.349) loss_u loss_u 0.7769 (0.8043) acc_u 28.1250 (28.1250) lr 5.7422e-04 eta 0:00:24
epoch [130/200] batch [10/57] time 0.379 (0.474) data 0.248 (0.343) loss_u loss_u 0.8545 (0.8249) acc_u 15.6250 (24.0625) lr 5.7422e-04 eta 0:00:22
epoch [130/200] batch [15/57] time 0.422 (0.474) data 0.290 (0.343) loss_u loss_u 0.7095 (0.8138) acc_u 37.5000 (24.5833) lr 5.7422e-04 eta 0:00:19
epoch [130/200] batch [20/57] time 0.353 (0.468) data 0.220 (0.337) loss_u loss_u 0.8110 (0.8233) acc_u 28.1250 (22.3438) lr 5.7422e-04 eta 0:00:17
epoch [130/200] batch [25/57] time 0.402 (0.465) data 0.272 (0.334) loss_u loss_u 0.8770 (0.8277) acc_u 12.5000 (21.5000) lr 5.7422e-04 eta 0:00:14
epoch [130/200] batch [30/57] time 0.369 (0.467) data 0.238 (0.336) loss_u loss_u 0.8892 (0.8257) acc_u 12.5000 (21.9792) lr 5.7422e-04 eta 0:00:12
epoch [130/200] batch [35/57] time 0.525 (0.465) data 0.394 (0.334) loss_u loss_u 0.7622 (0.8243) acc_u 34.3750 (22.2321) lr 5.7422e-04 eta 0:00:10
epoch [130/200] batch [40/57] time 0.299 (0.461) data 0.168 (0.330) loss_u loss_u 0.9150 (0.8270) acc_u 9.3750 (22.1875) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [45/57] time 0.314 (0.462) data 0.183 (0.331) loss_u loss_u 0.8433 (0.8258) acc_u 21.8750 (22.4306) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [50/57] time 0.423 (0.462) data 0.290 (0.331) loss_u loss_u 0.8125 (0.8267) acc_u 21.8750 (22.2500) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [55/57] time 0.419 (0.461) data 0.287 (0.330) loss_u loss_u 0.7817 (0.8260) acc_u 25.0000 (22.3864) lr 5.7422e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1446
confident_label rate tensor(0.4340, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1361
clean true:1358
clean false:3
clean_rate:0.9977957384276267
noisy true:332
noisy false:1443
after delete: len(clean_dataset) 1361
after delete: len(noisy_dataset) 1775
epoch [131/200] batch [5/42] time 0.620 (0.450) data 0.488 (0.319) loss_x loss_x 1.0723 (1.1163) acc_x 71.8750 (68.7500) lr 5.6006e-04 eta 0:00:16
epoch [131/200] batch [10/42] time 0.548 (0.479) data 0.417 (0.348) loss_x loss_x 1.1074 (1.1750) acc_x 65.6250 (69.3750) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [15/42] time 0.428 (0.494) data 0.296 (0.364) loss_x loss_x 0.7207 (1.1838) acc_x 84.3750 (70.2083) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [20/42] time 0.461 (0.495) data 0.330 (0.364) loss_x loss_x 1.0400 (1.2278) acc_x 78.1250 (69.3750) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [25/42] time 0.479 (0.490) data 0.349 (0.359) loss_x loss_x 1.0752 (1.2153) acc_x 78.1250 (70.0000) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [30/42] time 0.504 (0.493) data 0.374 (0.363) loss_x loss_x 0.9092 (1.2113) acc_x 78.1250 (70.6250) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [35/42] time 0.443 (0.495) data 0.311 (0.364) loss_x loss_x 1.1035 (1.2146) acc_x 71.8750 (70.0000) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [40/42] time 0.435 (0.489) data 0.306 (0.358) loss_x loss_x 1.8047 (1.2317) acc_x 56.2500 (69.7656) lr 5.6006e-04 eta 0:00:00
epoch [131/200] batch [5/55] time 0.418 (0.486) data 0.287 (0.355) loss_u loss_u 0.8223 (0.8046) acc_u 28.1250 (28.7500) lr 5.6006e-04 eta 0:00:24
epoch [131/200] batch [10/55] time 0.357 (0.481) data 0.225 (0.350) loss_u loss_u 0.8184 (0.8198) acc_u 28.1250 (25.3125) lr 5.6006e-04 eta 0:00:21
epoch [131/200] batch [15/55] time 0.411 (0.476) data 0.277 (0.345) loss_u loss_u 0.9023 (0.8279) acc_u 12.5000 (23.1250) lr 5.6006e-04 eta 0:00:19
epoch [131/200] batch [20/55] time 0.439 (0.473) data 0.307 (0.342) loss_u loss_u 0.8735 (0.8333) acc_u 18.7500 (22.3438) lr 5.6006e-04 eta 0:00:16
epoch [131/200] batch [25/55] time 0.335 (0.470) data 0.203 (0.339) loss_u loss_u 0.8413 (0.8354) acc_u 21.8750 (21.3750) lr 5.6006e-04 eta 0:00:14
epoch [131/200] batch [30/55] time 0.477 (0.472) data 0.346 (0.341) loss_u loss_u 0.8271 (0.8348) acc_u 28.1250 (21.4583) lr 5.6006e-04 eta 0:00:11
epoch [131/200] batch [35/55] time 0.430 (0.471) data 0.298 (0.340) loss_u loss_u 0.7588 (0.8284) acc_u 28.1250 (21.7857) lr 5.6006e-04 eta 0:00:09
epoch [131/200] batch [40/55] time 0.633 (0.469) data 0.501 (0.338) loss_u loss_u 0.8174 (0.8291) acc_u 25.0000 (21.8750) lr 5.6006e-04 eta 0:00:07
epoch [131/200] batch [45/55] time 0.439 (0.469) data 0.305 (0.338) loss_u loss_u 0.8491 (0.8264) acc_u 25.0000 (22.0833) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [50/55] time 0.532 (0.470) data 0.400 (0.338) loss_u loss_u 0.7515 (0.8228) acc_u 37.5000 (22.4375) lr 5.6006e-04 eta 0:00:02
epoch [131/200] batch [55/55] time 0.525 (0.472) data 0.393 (0.340) loss_u loss_u 0.8730 (0.8270) acc_u 21.8750 (21.8182) lr 5.6006e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1428
confident_label rate tensor(0.4321, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1355
clean true:1351
clean false:4
clean_rate:0.9970479704797048
noisy true:357
noisy false:1424
after delete: len(clean_dataset) 1355
after delete: len(noisy_dataset) 1781
epoch [132/200] batch [5/42] time 0.438 (0.464) data 0.308 (0.332) loss_x loss_x 1.3232 (1.1970) acc_x 75.0000 (71.8750) lr 5.4601e-04 eta 0:00:17
epoch [132/200] batch [10/42] time 0.383 (0.462) data 0.252 (0.331) loss_x loss_x 0.8096 (1.3126) acc_x 84.3750 (71.8750) lr 5.4601e-04 eta 0:00:14
epoch [132/200] batch [15/42] time 0.468 (0.473) data 0.338 (0.343) loss_x loss_x 1.4443 (1.2765) acc_x 62.5000 (70.0000) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [20/42] time 0.421 (0.470) data 0.291 (0.340) loss_x loss_x 1.1855 (1.2621) acc_x 65.6250 (69.0625) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [25/42] time 0.604 (0.470) data 0.474 (0.339) loss_x loss_x 1.0537 (1.2128) acc_x 68.7500 (70.5000) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [30/42] time 0.485 (0.476) data 0.354 (0.345) loss_x loss_x 1.3320 (1.2171) acc_x 62.5000 (70.2083) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [35/42] time 0.472 (0.475) data 0.341 (0.344) loss_x loss_x 1.3994 (1.2090) acc_x 65.6250 (70.1786) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [40/42] time 0.482 (0.483) data 0.351 (0.353) loss_x loss_x 1.0029 (1.2009) acc_x 78.1250 (70.3125) lr 5.4601e-04 eta 0:00:00
epoch [132/200] batch [5/55] time 0.544 (0.479) data 0.412 (0.348) loss_u loss_u 0.8398 (0.8416) acc_u 15.6250 (15.0000) lr 5.4601e-04 eta 0:00:23
epoch [132/200] batch [10/55] time 0.500 (0.474) data 0.368 (0.343) loss_u loss_u 0.8745 (0.8408) acc_u 18.7500 (17.5000) lr 5.4601e-04 eta 0:00:21
epoch [132/200] batch [15/55] time 0.428 (0.473) data 0.296 (0.342) loss_u loss_u 0.8398 (0.8450) acc_u 21.8750 (18.3333) lr 5.4601e-04 eta 0:00:18
epoch [132/200] batch [20/55] time 0.404 (0.467) data 0.273 (0.336) loss_u loss_u 0.9004 (0.8501) acc_u 9.3750 (17.9688) lr 5.4601e-04 eta 0:00:16
epoch [132/200] batch [25/55] time 0.495 (0.464) data 0.362 (0.333) loss_u loss_u 0.8657 (0.8424) acc_u 18.7500 (19.2500) lr 5.4601e-04 eta 0:00:13
epoch [132/200] batch [30/55] time 0.386 (0.464) data 0.255 (0.333) loss_u loss_u 0.8716 (0.8452) acc_u 15.6250 (19.1667) lr 5.4601e-04 eta 0:00:11
epoch [132/200] batch [35/55] time 0.541 (0.465) data 0.410 (0.334) loss_u loss_u 0.7944 (0.8399) acc_u 40.6250 (20.2679) lr 5.4601e-04 eta 0:00:09
epoch [132/200] batch [40/55] time 0.531 (0.466) data 0.400 (0.335) loss_u loss_u 0.8633 (0.8429) acc_u 18.7500 (20.0000) lr 5.4601e-04 eta 0:00:06
epoch [132/200] batch [45/55] time 0.439 (0.466) data 0.308 (0.335) loss_u loss_u 0.8496 (0.8421) acc_u 21.8750 (20.3472) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [50/55] time 0.399 (0.463) data 0.267 (0.332) loss_u loss_u 0.8677 (0.8407) acc_u 18.7500 (20.4375) lr 5.4601e-04 eta 0:00:02
epoch [132/200] batch [55/55] time 0.616 (0.469) data 0.485 (0.338) loss_u loss_u 0.8062 (0.8399) acc_u 18.7500 (20.3977) lr 5.4601e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1456
confident_label rate tensor(0.4273, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1340
clean true:1339
clean false:1
clean_rate:0.9992537313432835
noisy true:341
noisy false:1455
after delete: len(clean_dataset) 1340
after delete: len(noisy_dataset) 1796
epoch [133/200] batch [5/41] time 0.406 (0.460) data 0.276 (0.329) loss_x loss_x 1.1602 (1.2811) acc_x 71.8750 (66.8750) lr 5.3207e-04 eta 0:00:16
epoch [133/200] batch [10/41] time 0.431 (0.492) data 0.300 (0.362) loss_x loss_x 1.4404 (1.2452) acc_x 68.7500 (68.4375) lr 5.3207e-04 eta 0:00:15
epoch [133/200] batch [15/41] time 0.553 (0.474) data 0.423 (0.344) loss_x loss_x 0.9180 (1.2365) acc_x 78.1250 (68.5417) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [20/41] time 0.489 (0.475) data 0.358 (0.344) loss_x loss_x 1.0508 (1.1891) acc_x 78.1250 (70.0000) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [25/41] time 0.427 (0.477) data 0.297 (0.347) loss_x loss_x 1.2568 (1.2250) acc_x 75.0000 (68.8750) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [30/41] time 0.358 (0.471) data 0.227 (0.340) loss_x loss_x 1.1904 (1.1981) acc_x 68.7500 (69.7917) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [35/41] time 0.464 (0.472) data 0.334 (0.341) loss_x loss_x 1.2832 (1.1878) acc_x 68.7500 (70.2679) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [40/41] time 0.480 (0.472) data 0.348 (0.341) loss_x loss_x 1.0186 (1.1723) acc_x 71.8750 (70.7812) lr 5.3207e-04 eta 0:00:00
epoch [133/200] batch [5/56] time 0.389 (0.472) data 0.257 (0.341) loss_u loss_u 0.9160 (0.8560) acc_u 6.2500 (18.1250) lr 5.3207e-04 eta 0:00:24
epoch [133/200] batch [10/56] time 0.345 (0.465) data 0.214 (0.335) loss_u loss_u 0.8379 (0.8535) acc_u 18.7500 (19.6875) lr 5.3207e-04 eta 0:00:21
epoch [133/200] batch [15/56] time 0.373 (0.471) data 0.243 (0.340) loss_u loss_u 0.7793 (0.8383) acc_u 25.0000 (21.4583) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [20/56] time 0.327 (0.467) data 0.197 (0.336) loss_u loss_u 0.8423 (0.8309) acc_u 25.0000 (21.7188) lr 5.3207e-04 eta 0:00:16
epoch [133/200] batch [25/56] time 0.593 (0.469) data 0.463 (0.339) loss_u loss_u 0.8965 (0.8335) acc_u 15.6250 (21.8750) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [30/56] time 0.569 (0.472) data 0.436 (0.341) loss_u loss_u 0.7974 (0.8281) acc_u 21.8750 (22.2917) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [35/56] time 0.545 (0.471) data 0.412 (0.340) loss_u loss_u 0.7881 (0.8297) acc_u 25.0000 (22.1429) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [40/56] time 0.572 (0.472) data 0.441 (0.341) loss_u loss_u 0.8237 (0.8266) acc_u 25.0000 (22.6562) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [45/56] time 0.528 (0.472) data 0.397 (0.341) loss_u loss_u 0.7842 (0.8258) acc_u 25.0000 (22.5694) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [50/56] time 0.542 (0.471) data 0.410 (0.340) loss_u loss_u 0.8101 (0.8261) acc_u 12.5000 (22.1250) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [55/56] time 0.589 (0.470) data 0.458 (0.339) loss_u loss_u 0.8286 (0.8288) acc_u 25.0000 (21.8182) lr 5.3207e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1444
confident_label rate tensor(0.4283, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1343
clean true:1339
clean false:4
clean_rate:0.9970215934475056
noisy true:353
noisy false:1440
after delete: len(clean_dataset) 1343
after delete: len(noisy_dataset) 1793
epoch [134/200] batch [5/41] time 0.382 (0.431) data 0.252 (0.300) loss_x loss_x 1.3477 (1.2242) acc_x 65.6250 (66.8750) lr 5.1825e-04 eta 0:00:15
epoch [134/200] batch [10/41] time 0.441 (0.442) data 0.310 (0.311) loss_x loss_x 1.9434 (1.3352) acc_x 59.3750 (67.1875) lr 5.1825e-04 eta 0:00:13
epoch [134/200] batch [15/41] time 0.514 (0.475) data 0.381 (0.344) loss_x loss_x 0.5483 (1.2365) acc_x 87.5000 (69.7917) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [20/41] time 0.410 (0.467) data 0.280 (0.336) loss_x loss_x 1.3340 (1.2801) acc_x 81.2500 (70.1562) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [25/41] time 0.581 (0.466) data 0.450 (0.336) loss_x loss_x 1.4775 (1.2497) acc_x 68.7500 (70.6250) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [30/41] time 0.343 (0.460) data 0.212 (0.329) loss_x loss_x 0.9712 (1.2321) acc_x 78.1250 (71.1458) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [35/41] time 0.515 (0.455) data 0.385 (0.325) loss_x loss_x 1.1543 (1.2017) acc_x 75.0000 (71.5179) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [40/41] time 0.399 (0.452) data 0.269 (0.321) loss_x loss_x 1.0811 (1.2291) acc_x 62.5000 (70.7031) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/56] time 0.425 (0.457) data 0.294 (0.326) loss_u loss_u 0.8013 (0.8340) acc_u 25.0000 (20.6250) lr 5.1825e-04 eta 0:00:23
epoch [134/200] batch [10/56] time 0.365 (0.454) data 0.234 (0.323) loss_u loss_u 0.8550 (0.8344) acc_u 15.6250 (20.9375) lr 5.1825e-04 eta 0:00:20
epoch [134/200] batch [15/56] time 0.445 (0.453) data 0.313 (0.322) loss_u loss_u 0.7520 (0.8338) acc_u 31.2500 (21.0417) lr 5.1825e-04 eta 0:00:18
epoch [134/200] batch [20/56] time 0.424 (0.455) data 0.293 (0.324) loss_u loss_u 0.7959 (0.8264) acc_u 25.0000 (21.8750) lr 5.1825e-04 eta 0:00:16
epoch [134/200] batch [25/56] time 0.442 (0.461) data 0.310 (0.330) loss_u loss_u 0.7812 (0.8229) acc_u 25.0000 (22.5000) lr 5.1825e-04 eta 0:00:14
epoch [134/200] batch [30/56] time 0.504 (0.457) data 0.372 (0.326) loss_u loss_u 0.7852 (0.8256) acc_u 25.0000 (22.0833) lr 5.1825e-04 eta 0:00:11
epoch [134/200] batch [35/56] time 0.383 (0.456) data 0.252 (0.325) loss_u loss_u 0.7100 (0.8247) acc_u 37.5000 (22.0536) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [40/56] time 0.491 (0.455) data 0.360 (0.324) loss_u loss_u 0.8394 (0.8230) acc_u 18.7500 (22.4219) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [45/56] time 0.407 (0.454) data 0.277 (0.323) loss_u loss_u 0.8599 (0.8283) acc_u 15.6250 (21.5278) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [50/56] time 0.519 (0.454) data 0.388 (0.323) loss_u loss_u 0.8975 (0.8308) acc_u 15.6250 (21.2500) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [55/56] time 0.594 (0.460) data 0.459 (0.329) loss_u loss_u 0.7695 (0.8325) acc_u 28.1250 (21.0795) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1461
confident_label rate tensor(0.4247, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1332
clean true:1331
clean false:1
clean_rate:0.9992492492492493
noisy true:344
noisy false:1460
after delete: len(clean_dataset) 1332
after delete: len(noisy_dataset) 1804
epoch [135/200] batch [5/41] time 0.489 (0.448) data 0.360 (0.318) loss_x loss_x 1.4443 (1.2090) acc_x 68.7500 (70.0000) lr 5.0454e-04 eta 0:00:16
epoch [135/200] batch [10/41] time 0.440 (0.449) data 0.310 (0.319) loss_x loss_x 1.3984 (1.2527) acc_x 68.7500 (69.3750) lr 5.0454e-04 eta 0:00:13
epoch [135/200] batch [15/41] time 0.469 (0.448) data 0.337 (0.318) loss_x loss_x 0.9092 (1.2181) acc_x 71.8750 (68.7500) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [20/41] time 0.629 (0.465) data 0.499 (0.334) loss_x loss_x 0.8652 (1.1943) acc_x 81.2500 (69.6875) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [25/41] time 0.499 (0.463) data 0.368 (0.332) loss_x loss_x 0.7944 (1.1555) acc_x 78.1250 (70.1250) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [30/41] time 0.482 (0.467) data 0.352 (0.337) loss_x loss_x 1.1768 (1.1644) acc_x 78.1250 (70.3125) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [35/41] time 0.482 (0.465) data 0.350 (0.334) loss_x loss_x 1.4307 (1.1582) acc_x 71.8750 (70.2679) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [40/41] time 0.472 (0.458) data 0.342 (0.327) loss_x loss_x 1.7549 (1.1677) acc_x 56.2500 (69.6875) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/56] time 0.396 (0.452) data 0.264 (0.321) loss_u loss_u 0.8843 (0.8383) acc_u 15.6250 (20.0000) lr 5.0454e-04 eta 0:00:23
epoch [135/200] batch [10/56] time 0.375 (0.451) data 0.244 (0.321) loss_u loss_u 0.8491 (0.8367) acc_u 15.6250 (20.0000) lr 5.0454e-04 eta 0:00:20
epoch [135/200] batch [15/56] time 0.472 (0.452) data 0.341 (0.322) loss_u loss_u 0.6978 (0.8253) acc_u 34.3750 (20.4167) lr 5.0454e-04 eta 0:00:18
epoch [135/200] batch [20/56] time 0.425 (0.458) data 0.294 (0.327) loss_u loss_u 0.8110 (0.8309) acc_u 25.0000 (19.8438) lr 5.0454e-04 eta 0:00:16
epoch [135/200] batch [25/56] time 0.652 (0.461) data 0.520 (0.330) loss_u loss_u 0.8662 (0.8305) acc_u 15.6250 (20.1250) lr 5.0454e-04 eta 0:00:14
epoch [135/200] batch [30/56] time 0.491 (0.461) data 0.360 (0.330) loss_u loss_u 0.7593 (0.8281) acc_u 31.2500 (20.8333) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [35/56] time 0.710 (0.463) data 0.579 (0.332) loss_u loss_u 0.7583 (0.8284) acc_u 37.5000 (21.2500) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [40/56] time 0.571 (0.466) data 0.440 (0.335) loss_u loss_u 0.7339 (0.8304) acc_u 31.2500 (20.7812) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [45/56] time 0.505 (0.465) data 0.375 (0.334) loss_u loss_u 0.8350 (0.8291) acc_u 18.7500 (20.7639) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [50/56] time 0.428 (0.463) data 0.297 (0.332) loss_u loss_u 0.8022 (0.8271) acc_u 28.1250 (21.1875) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [55/56] time 0.690 (0.467) data 0.559 (0.336) loss_u loss_u 0.7075 (0.8253) acc_u 37.5000 (21.7045) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1424
confident_label rate tensor(0.4346, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1363
clean true:1363
clean false:0
clean_rate:1.0
noisy true:349
noisy false:1424
after delete: len(clean_dataset) 1363
after delete: len(noisy_dataset) 1773
epoch [136/200] batch [5/42] time 0.564 (0.493) data 0.434 (0.363) loss_x loss_x 1.0059 (1.2271) acc_x 75.0000 (71.2500) lr 4.9096e-04 eta 0:00:18
epoch [136/200] batch [10/42] time 0.448 (0.497) data 0.317 (0.367) loss_x loss_x 1.0732 (1.1359) acc_x 62.5000 (69.6875) lr 4.9096e-04 eta 0:00:15
epoch [136/200] batch [15/42] time 0.460 (0.486) data 0.329 (0.355) loss_x loss_x 1.2197 (1.1090) acc_x 68.7500 (70.8333) lr 4.9096e-04 eta 0:00:13
epoch [136/200] batch [20/42] time 0.413 (0.474) data 0.283 (0.344) loss_x loss_x 1.1924 (1.1369) acc_x 71.8750 (70.1562) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [25/42] time 0.477 (0.476) data 0.346 (0.346) loss_x loss_x 1.5176 (1.1432) acc_x 62.5000 (70.2500) lr 4.9096e-04 eta 0:00:08
epoch [136/200] batch [30/42] time 0.623 (0.477) data 0.492 (0.347) loss_x loss_x 0.9375 (1.1735) acc_x 84.3750 (70.4167) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [35/42] time 0.368 (0.478) data 0.238 (0.348) loss_x loss_x 1.2393 (1.1766) acc_x 65.6250 (70.1786) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [40/42] time 0.478 (0.480) data 0.348 (0.349) loss_x loss_x 0.9604 (1.1846) acc_x 71.8750 (69.7656) lr 4.9096e-04 eta 0:00:00
epoch [136/200] batch [5/55] time 0.359 (0.476) data 0.228 (0.346) loss_u loss_u 0.8965 (0.8407) acc_u 18.7500 (21.8750) lr 4.9096e-04 eta 0:00:23
epoch [136/200] batch [10/55] time 0.452 (0.476) data 0.318 (0.345) loss_u loss_u 0.8813 (0.8471) acc_u 15.6250 (22.1875) lr 4.9096e-04 eta 0:00:21
epoch [136/200] batch [15/55] time 0.382 (0.476) data 0.251 (0.345) loss_u loss_u 0.8350 (0.8461) acc_u 18.7500 (20.8333) lr 4.9096e-04 eta 0:00:19
epoch [136/200] batch [20/55] time 0.416 (0.472) data 0.285 (0.342) loss_u loss_u 0.7363 (0.8336) acc_u 34.3750 (21.8750) lr 4.9096e-04 eta 0:00:16
epoch [136/200] batch [25/55] time 0.438 (0.470) data 0.307 (0.339) loss_u loss_u 0.7705 (0.8294) acc_u 31.2500 (22.6250) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [30/55] time 0.520 (0.471) data 0.390 (0.340) loss_u loss_u 0.9038 (0.8289) acc_u 6.2500 (22.7083) lr 4.9096e-04 eta 0:00:11
epoch [136/200] batch [35/55] time 0.434 (0.471) data 0.303 (0.340) loss_u loss_u 0.8608 (0.8336) acc_u 9.3750 (21.5179) lr 4.9096e-04 eta 0:00:09
epoch [136/200] batch [40/55] time 0.410 (0.471) data 0.280 (0.340) loss_u loss_u 0.8818 (0.8351) acc_u 12.5000 (21.4062) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [45/55] time 0.389 (0.472) data 0.258 (0.342) loss_u loss_u 0.8262 (0.8379) acc_u 21.8750 (20.8333) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [50/55] time 0.627 (0.473) data 0.496 (0.343) loss_u loss_u 0.7944 (0.8397) acc_u 18.7500 (20.4375) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [55/55] time 0.449 (0.470) data 0.317 (0.339) loss_u loss_u 0.8208 (0.8373) acc_u 21.8750 (20.7386) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1442
confident_label rate tensor(0.4295, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1347
clean true:1343
clean false:4
clean_rate:0.9970304380103935
noisy true:351
noisy false:1438
after delete: len(clean_dataset) 1347
after delete: len(noisy_dataset) 1789
epoch [137/200] batch [5/42] time 0.530 (0.488) data 0.399 (0.357) loss_x loss_x 1.4814 (1.2375) acc_x 62.5000 (68.1250) lr 4.7750e-04 eta 0:00:18
epoch [137/200] batch [10/42] time 0.448 (0.466) data 0.318 (0.335) loss_x loss_x 1.6582 (1.2219) acc_x 62.5000 (69.0625) lr 4.7750e-04 eta 0:00:14
epoch [137/200] batch [15/42] time 0.367 (0.459) data 0.237 (0.329) loss_x loss_x 1.1621 (1.1711) acc_x 59.3750 (69.3750) lr 4.7750e-04 eta 0:00:12
epoch [137/200] batch [20/42] time 0.517 (0.458) data 0.387 (0.328) loss_x loss_x 1.3008 (1.2125) acc_x 65.6250 (68.5938) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [25/42] time 0.409 (0.454) data 0.279 (0.323) loss_x loss_x 0.9824 (1.1719) acc_x 75.0000 (70.0000) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [30/42] time 0.387 (0.463) data 0.256 (0.332) loss_x loss_x 0.9443 (1.1424) acc_x 75.0000 (70.3125) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [35/42] time 0.378 (0.461) data 0.248 (0.331) loss_x loss_x 1.6836 (1.1660) acc_x 56.2500 (70.0893) lr 4.7750e-04 eta 0:00:03
epoch [137/200] batch [40/42] time 0.468 (0.465) data 0.338 (0.335) loss_x loss_x 1.0469 (1.1606) acc_x 71.8750 (70.0781) lr 4.7750e-04 eta 0:00:00
epoch [137/200] batch [5/55] time 0.790 (0.475) data 0.660 (0.345) loss_u loss_u 0.8638 (0.8482) acc_u 18.7500 (17.5000) lr 4.7750e-04 eta 0:00:23
epoch [137/200] batch [10/55] time 0.392 (0.471) data 0.260 (0.341) loss_u loss_u 0.8809 (0.8383) acc_u 12.5000 (18.7500) lr 4.7750e-04 eta 0:00:21
epoch [137/200] batch [15/55] time 0.426 (0.473) data 0.295 (0.342) loss_u loss_u 0.8042 (0.8290) acc_u 25.0000 (20.6250) lr 4.7750e-04 eta 0:00:18
epoch [137/200] batch [20/55] time 0.567 (0.468) data 0.436 (0.338) loss_u loss_u 0.8296 (0.8252) acc_u 21.8750 (21.7188) lr 4.7750e-04 eta 0:00:16
epoch [137/200] batch [25/55] time 0.378 (0.465) data 0.247 (0.335) loss_u loss_u 0.8770 (0.8349) acc_u 12.5000 (20.1250) lr 4.7750e-04 eta 0:00:13
epoch [137/200] batch [30/55] time 0.359 (0.463) data 0.228 (0.332) loss_u loss_u 0.8540 (0.8335) acc_u 18.7500 (20.3125) lr 4.7750e-04 eta 0:00:11
epoch [137/200] batch [35/55] time 0.440 (0.459) data 0.309 (0.328) loss_u loss_u 0.9106 (0.8347) acc_u 12.5000 (20.2679) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [40/55] time 0.463 (0.458) data 0.332 (0.327) loss_u loss_u 0.7886 (0.8327) acc_u 28.1250 (20.4688) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [45/55] time 0.532 (0.459) data 0.401 (0.328) loss_u loss_u 0.8262 (0.8331) acc_u 21.8750 (20.5556) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [50/55] time 0.453 (0.459) data 0.321 (0.328) loss_u loss_u 0.8394 (0.8304) acc_u 21.8750 (21.1875) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [55/55] time 0.692 (0.458) data 0.560 (0.327) loss_u loss_u 0.8408 (0.8339) acc_u 15.6250 (20.6250) lr 4.7750e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1471
confident_label rate tensor(0.4235, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1328
clean true:1327
clean false:1
clean_rate:0.9992469879518072
noisy true:338
noisy false:1470
after delete: len(clean_dataset) 1328
after delete: len(noisy_dataset) 1808
epoch [138/200] batch [5/41] time 0.473 (0.453) data 0.342 (0.322) loss_x loss_x 0.8242 (1.2336) acc_x 78.1250 (70.0000) lr 4.6417e-04 eta 0:00:16
epoch [138/200] batch [10/41] time 0.474 (0.484) data 0.344 (0.353) loss_x loss_x 1.3359 (1.3013) acc_x 68.7500 (66.8750) lr 4.6417e-04 eta 0:00:15
epoch [138/200] batch [15/41] time 0.507 (0.465) data 0.377 (0.335) loss_x loss_x 1.2588 (1.2456) acc_x 65.6250 (68.1250) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [20/41] time 0.473 (0.453) data 0.342 (0.322) loss_x loss_x 1.0625 (1.2592) acc_x 75.0000 (68.2812) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [25/41] time 0.563 (0.452) data 0.432 (0.321) loss_x loss_x 1.1035 (1.2114) acc_x 71.8750 (68.8750) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [30/41] time 0.448 (0.456) data 0.317 (0.325) loss_x loss_x 1.1953 (1.2462) acc_x 65.6250 (68.3333) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [35/41] time 0.586 (0.462) data 0.455 (0.331) loss_x loss_x 1.2285 (1.2424) acc_x 65.6250 (68.4821) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [40/41] time 0.461 (0.463) data 0.331 (0.333) loss_x loss_x 1.0859 (1.2276) acc_x 78.1250 (68.5938) lr 4.6417e-04 eta 0:00:00
epoch [138/200] batch [5/56] time 0.491 (0.467) data 0.360 (0.337) loss_u loss_u 0.8403 (0.8105) acc_u 21.8750 (27.5000) lr 4.6417e-04 eta 0:00:23
epoch [138/200] batch [10/56] time 0.422 (0.467) data 0.291 (0.336) loss_u loss_u 0.8682 (0.8248) acc_u 12.5000 (23.1250) lr 4.6417e-04 eta 0:00:21
epoch [138/200] batch [15/56] time 0.507 (0.464) data 0.375 (0.334) loss_u loss_u 0.7397 (0.8252) acc_u 37.5000 (22.9167) lr 4.6417e-04 eta 0:00:19
epoch [138/200] batch [20/56] time 0.718 (0.475) data 0.586 (0.344) loss_u loss_u 0.8247 (0.8343) acc_u 18.7500 (21.7188) lr 4.6417e-04 eta 0:00:17
epoch [138/200] batch [25/56] time 0.441 (0.473) data 0.310 (0.342) loss_u loss_u 0.8472 (0.8317) acc_u 15.6250 (21.5000) lr 4.6417e-04 eta 0:00:14
epoch [138/200] batch [30/56] time 0.310 (0.470) data 0.178 (0.339) loss_u loss_u 0.7695 (0.8304) acc_u 25.0000 (21.8750) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [35/56] time 0.343 (0.464) data 0.212 (0.333) loss_u loss_u 0.9209 (0.8361) acc_u 15.6250 (21.1607) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [40/56] time 0.443 (0.462) data 0.313 (0.331) loss_u loss_u 0.8062 (0.8373) acc_u 25.0000 (21.0156) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [45/56] time 0.382 (0.459) data 0.252 (0.328) loss_u loss_u 0.8320 (0.8361) acc_u 25.0000 (21.0417) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [50/56] time 0.513 (0.461) data 0.381 (0.330) loss_u loss_u 0.8101 (0.8325) acc_u 28.1250 (21.7500) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [55/56] time 0.434 (0.462) data 0.303 (0.331) loss_u loss_u 0.8447 (0.8334) acc_u 15.6250 (21.6477) lr 4.6417e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1456
confident_label rate tensor(0.4257, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1335
clean true:1328
clean false:7
clean_rate:0.9947565543071161
noisy true:352
noisy false:1449
after delete: len(clean_dataset) 1335
after delete: len(noisy_dataset) 1801
epoch [139/200] batch [5/41] time 0.535 (0.528) data 0.405 (0.395) loss_x loss_x 1.0723 (1.0957) acc_x 71.8750 (71.2500) lr 4.5098e-04 eta 0:00:19
epoch [139/200] batch [10/41] time 0.560 (0.498) data 0.427 (0.366) loss_x loss_x 1.3682 (1.1523) acc_x 62.5000 (68.4375) lr 4.5098e-04 eta 0:00:15
epoch [139/200] batch [15/41] time 0.408 (0.492) data 0.278 (0.361) loss_x loss_x 1.1846 (1.1585) acc_x 65.6250 (69.7917) lr 4.5098e-04 eta 0:00:12
epoch [139/200] batch [20/41] time 0.420 (0.476) data 0.289 (0.345) loss_x loss_x 1.6719 (1.1990) acc_x 62.5000 (69.3750) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [25/41] time 0.537 (0.473) data 0.405 (0.342) loss_x loss_x 1.3613 (1.2097) acc_x 65.6250 (68.8750) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [30/41] time 0.448 (0.468) data 0.318 (0.337) loss_x loss_x 1.2607 (1.1823) acc_x 62.5000 (69.3750) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [35/41] time 0.330 (0.463) data 0.200 (0.332) loss_x loss_x 1.9014 (1.2102) acc_x 46.8750 (68.5714) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [40/41] time 0.393 (0.455) data 0.262 (0.324) loss_x loss_x 1.1816 (1.2200) acc_x 78.1250 (69.2188) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/56] time 0.569 (0.463) data 0.439 (0.332) loss_u loss_u 0.8149 (0.8407) acc_u 25.0000 (18.7500) lr 4.5098e-04 eta 0:00:23
epoch [139/200] batch [10/56] time 0.391 (0.456) data 0.260 (0.325) loss_u loss_u 0.7954 (0.8366) acc_u 25.0000 (20.3125) lr 4.5098e-04 eta 0:00:20
epoch [139/200] batch [15/56] time 0.415 (0.457) data 0.284 (0.326) loss_u loss_u 0.8984 (0.8448) acc_u 12.5000 (18.9583) lr 4.5098e-04 eta 0:00:18
epoch [139/200] batch [20/56] time 0.360 (0.452) data 0.230 (0.321) loss_u loss_u 0.8281 (0.8358) acc_u 25.0000 (21.2500) lr 4.5098e-04 eta 0:00:16
epoch [139/200] batch [25/56] time 0.441 (0.451) data 0.311 (0.320) loss_u loss_u 0.8848 (0.8374) acc_u 12.5000 (20.7500) lr 4.5098e-04 eta 0:00:13
epoch [139/200] batch [30/56] time 0.465 (0.456) data 0.335 (0.325) loss_u loss_u 0.7744 (0.8368) acc_u 34.3750 (21.2500) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [35/56] time 0.368 (0.453) data 0.238 (0.322) loss_u loss_u 0.8198 (0.8379) acc_u 31.2500 (21.4286) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [40/56] time 0.426 (0.455) data 0.295 (0.324) loss_u loss_u 0.8931 (0.8387) acc_u 12.5000 (21.3281) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [45/56] time 0.409 (0.455) data 0.278 (0.324) loss_u loss_u 0.8052 (0.8314) acc_u 18.7500 (22.1528) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [50/56] time 0.475 (0.456) data 0.344 (0.325) loss_u loss_u 0.7930 (0.8316) acc_u 25.0000 (21.5625) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [55/56] time 0.423 (0.457) data 0.291 (0.326) loss_u loss_u 0.7876 (0.8302) acc_u 21.8750 (21.4773) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1412
confident_label rate tensor(0.4362, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1368
clean true:1366
clean false:2
clean_rate:0.9985380116959064
noisy true:358
noisy false:1410
after delete: len(clean_dataset) 1368
after delete: len(noisy_dataset) 1768
epoch [140/200] batch [5/42] time 0.501 (0.438) data 0.370 (0.307) loss_x loss_x 0.7749 (1.2446) acc_x 81.2500 (68.1250) lr 4.3792e-04 eta 0:00:16
epoch [140/200] batch [10/42] time 0.438 (0.456) data 0.307 (0.324) loss_x loss_x 1.5186 (1.2900) acc_x 59.3750 (67.1875) lr 4.3792e-04 eta 0:00:14
epoch [140/200] batch [15/42] time 0.447 (0.451) data 0.316 (0.320) loss_x loss_x 1.5986 (1.3332) acc_x 59.3750 (64.7917) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [20/42] time 0.446 (0.447) data 0.316 (0.316) loss_x loss_x 1.7803 (1.3384) acc_x 50.0000 (64.8438) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [25/42] time 0.459 (0.449) data 0.329 (0.318) loss_x loss_x 1.1504 (1.2850) acc_x 71.8750 (66.5000) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [30/42] time 0.314 (0.448) data 0.184 (0.318) loss_x loss_x 1.1201 (1.2507) acc_x 78.1250 (67.9167) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [35/42] time 0.461 (0.453) data 0.331 (0.322) loss_x loss_x 1.1504 (1.2131) acc_x 68.7500 (68.8393) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [40/42] time 0.601 (0.465) data 0.471 (0.334) loss_x loss_x 1.2324 (1.2098) acc_x 68.7500 (69.2969) lr 4.3792e-04 eta 0:00:00
epoch [140/200] batch [5/55] time 0.563 (0.468) data 0.433 (0.337) loss_u loss_u 0.8501 (0.8131) acc_u 18.7500 (23.1250) lr 4.3792e-04 eta 0:00:23
epoch [140/200] batch [10/55] time 0.379 (0.461) data 0.248 (0.330) loss_u loss_u 0.7905 (0.8057) acc_u 25.0000 (25.0000) lr 4.3792e-04 eta 0:00:20
epoch [140/200] batch [15/55] time 0.375 (0.461) data 0.244 (0.331) loss_u loss_u 0.8306 (0.8220) acc_u 21.8750 (23.1250) lr 4.3792e-04 eta 0:00:18
epoch [140/200] batch [20/55] time 0.518 (0.461) data 0.387 (0.330) loss_u loss_u 0.8281 (0.8369) acc_u 18.7500 (21.0938) lr 4.3792e-04 eta 0:00:16
epoch [140/200] batch [25/55] time 0.381 (0.459) data 0.251 (0.329) loss_u loss_u 0.8433 (0.8331) acc_u 18.7500 (21.1250) lr 4.3792e-04 eta 0:00:13
epoch [140/200] batch [30/55] time 0.531 (0.462) data 0.399 (0.331) loss_u loss_u 0.8447 (0.8352) acc_u 12.5000 (20.8333) lr 4.3792e-04 eta 0:00:11
epoch [140/200] batch [35/55] time 0.446 (0.463) data 0.315 (0.332) loss_u loss_u 0.6885 (0.8245) acc_u 43.7500 (22.6786) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [40/55] time 0.430 (0.461) data 0.299 (0.330) loss_u loss_u 0.9165 (0.8289) acc_u 9.3750 (21.9531) lr 4.3792e-04 eta 0:00:06
epoch [140/200] batch [45/55] time 0.359 (0.458) data 0.227 (0.327) loss_u loss_u 0.7646 (0.8278) acc_u 31.2500 (22.2222) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [50/55] time 0.369 (0.457) data 0.237 (0.326) loss_u loss_u 0.9048 (0.8300) acc_u 9.3750 (21.6250) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [55/55] time 0.448 (0.458) data 0.318 (0.327) loss_u loss_u 0.8555 (0.8336) acc_u 18.7500 (21.1932) lr 4.3792e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1448
confident_label rate tensor(0.4267, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1338
clean true:1337
clean false:1
clean_rate:0.9992526158445441
noisy true:351
noisy false:1447
after delete: len(clean_dataset) 1338
after delete: len(noisy_dataset) 1798
epoch [141/200] batch [5/41] time 0.488 (0.474) data 0.356 (0.342) loss_x loss_x 0.9839 (1.2146) acc_x 75.0000 (71.2500) lr 4.2499e-04 eta 0:00:17
epoch [141/200] batch [10/41] time 0.635 (0.502) data 0.504 (0.370) loss_x loss_x 1.3779 (1.3083) acc_x 56.2500 (66.5625) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [15/41] time 0.502 (0.498) data 0.371 (0.367) loss_x loss_x 0.8135 (1.1894) acc_x 78.1250 (68.5417) lr 4.2499e-04 eta 0:00:12
epoch [141/200] batch [20/41] time 0.492 (0.474) data 0.361 (0.342) loss_x loss_x 1.0469 (1.2110) acc_x 75.0000 (69.0625) lr 4.2499e-04 eta 0:00:09
epoch [141/200] batch [25/41] time 0.485 (0.475) data 0.354 (0.343) loss_x loss_x 1.0996 (1.1847) acc_x 71.8750 (69.6250) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [30/41] time 0.441 (0.466) data 0.310 (0.335) loss_x loss_x 1.1836 (1.1729) acc_x 75.0000 (70.4167) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [35/41] time 0.628 (0.471) data 0.497 (0.340) loss_x loss_x 1.6416 (1.1833) acc_x 65.6250 (70.5357) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [40/41] time 0.360 (0.459) data 0.230 (0.328) loss_x loss_x 1.2246 (1.1948) acc_x 65.6250 (70.1562) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/56] time 0.501 (0.451) data 0.370 (0.320) loss_u loss_u 0.8472 (0.8477) acc_u 21.8750 (20.6250) lr 4.2499e-04 eta 0:00:23
epoch [141/200] batch [10/56] time 0.466 (0.453) data 0.334 (0.322) loss_u loss_u 0.8335 (0.8348) acc_u 21.8750 (20.9375) lr 4.2499e-04 eta 0:00:20
epoch [141/200] batch [15/56] time 0.639 (0.459) data 0.509 (0.328) loss_u loss_u 0.8525 (0.8451) acc_u 21.8750 (20.4167) lr 4.2499e-04 eta 0:00:18
epoch [141/200] batch [20/56] time 0.421 (0.457) data 0.290 (0.326) loss_u loss_u 0.8135 (0.8447) acc_u 25.0000 (20.0000) lr 4.2499e-04 eta 0:00:16
epoch [141/200] batch [25/56] time 0.387 (0.457) data 0.256 (0.326) loss_u loss_u 0.8789 (0.8392) acc_u 12.5000 (20.5000) lr 4.2499e-04 eta 0:00:14
epoch [141/200] batch [30/56] time 0.519 (0.459) data 0.389 (0.328) loss_u loss_u 0.8013 (0.8378) acc_u 21.8750 (20.8333) lr 4.2499e-04 eta 0:00:11
epoch [141/200] batch [35/56] time 0.617 (0.461) data 0.486 (0.330) loss_u loss_u 0.8452 (0.8371) acc_u 18.7500 (20.8036) lr 4.2499e-04 eta 0:00:09
epoch [141/200] batch [40/56] time 0.549 (0.464) data 0.418 (0.333) loss_u loss_u 0.8550 (0.8353) acc_u 18.7500 (21.0938) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [45/56] time 0.592 (0.463) data 0.461 (0.332) loss_u loss_u 0.8682 (0.8370) acc_u 15.6250 (20.9722) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [50/56] time 0.527 (0.463) data 0.396 (0.332) loss_u loss_u 0.8169 (0.8346) acc_u 21.8750 (21.2500) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [55/56] time 0.424 (0.461) data 0.292 (0.330) loss_u loss_u 0.6895 (0.8303) acc_u 40.6250 (21.7045) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1464
confident_label rate tensor(0.4232, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1327
clean true:1325
clean false:2
clean_rate:0.9984928409947249
noisy true:347
noisy false:1462
after delete: len(clean_dataset) 1327
after delete: len(noisy_dataset) 1809
epoch [142/200] batch [5/41] time 0.450 (0.473) data 0.319 (0.341) loss_x loss_x 0.8188 (1.1702) acc_x 71.8750 (71.8750) lr 4.1221e-04 eta 0:00:17
epoch [142/200] batch [10/41] time 0.470 (0.465) data 0.340 (0.334) loss_x loss_x 1.1191 (1.2009) acc_x 75.0000 (70.6250) lr 4.1221e-04 eta 0:00:14
epoch [142/200] batch [15/41] time 0.509 (0.459) data 0.377 (0.328) loss_x loss_x 1.6211 (1.2269) acc_x 65.6250 (69.7917) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [20/41] time 0.432 (0.465) data 0.302 (0.334) loss_x loss_x 0.8252 (1.1828) acc_x 84.3750 (70.7812) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [25/41] time 0.446 (0.459) data 0.316 (0.328) loss_x loss_x 0.9229 (1.1409) acc_x 78.1250 (71.6250) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [30/41] time 0.481 (0.470) data 0.351 (0.339) loss_x loss_x 0.9863 (1.1526) acc_x 78.1250 (71.3542) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [35/41] time 0.520 (0.470) data 0.389 (0.339) loss_x loss_x 1.5967 (1.1635) acc_x 56.2500 (71.3393) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [40/41] time 0.487 (0.469) data 0.357 (0.338) loss_x loss_x 1.0508 (1.1769) acc_x 65.6250 (70.9375) lr 4.1221e-04 eta 0:00:00
epoch [142/200] batch [5/56] time 0.507 (0.471) data 0.376 (0.340) loss_u loss_u 0.7583 (0.8350) acc_u 34.3750 (19.3750) lr 4.1221e-04 eta 0:00:24
epoch [142/200] batch [10/56] time 0.546 (0.473) data 0.415 (0.342) loss_u loss_u 0.7720 (0.8296) acc_u 21.8750 (20.6250) lr 4.1221e-04 eta 0:00:21
epoch [142/200] batch [15/56] time 0.447 (0.471) data 0.316 (0.340) loss_u loss_u 0.7817 (0.8278) acc_u 28.1250 (21.6667) lr 4.1221e-04 eta 0:00:19
epoch [142/200] batch [20/56] time 0.389 (0.471) data 0.259 (0.340) loss_u loss_u 0.8135 (0.8277) acc_u 25.0000 (21.7188) lr 4.1221e-04 eta 0:00:16
epoch [142/200] batch [25/56] time 0.711 (0.469) data 0.580 (0.338) loss_u loss_u 0.8584 (0.8272) acc_u 18.7500 (22.0000) lr 4.1221e-04 eta 0:00:14
epoch [142/200] batch [30/56] time 0.388 (0.470) data 0.256 (0.339) loss_u loss_u 0.8291 (0.8295) acc_u 18.7500 (21.9792) lr 4.1221e-04 eta 0:00:12
epoch [142/200] batch [35/56] time 0.377 (0.467) data 0.247 (0.336) loss_u loss_u 0.8315 (0.8289) acc_u 21.8750 (22.2321) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [40/56] time 0.465 (0.467) data 0.334 (0.336) loss_u loss_u 0.7319 (0.8291) acc_u 31.2500 (21.8750) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [45/56] time 0.343 (0.463) data 0.212 (0.332) loss_u loss_u 0.7246 (0.8293) acc_u 34.3750 (21.8750) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [50/56] time 0.503 (0.463) data 0.372 (0.332) loss_u loss_u 0.8091 (0.8256) acc_u 28.1250 (22.1875) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [55/56] time 0.478 (0.461) data 0.347 (0.330) loss_u loss_u 0.8623 (0.8271) acc_u 15.6250 (21.8182) lr 4.1221e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1453
confident_label rate tensor(0.4200, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1317
clean true:1315
clean false:2
clean_rate:0.9984813971146546
noisy true:368
noisy false:1451
after delete: len(clean_dataset) 1317
after delete: len(noisy_dataset) 1819
epoch [143/200] batch [5/41] time 0.557 (0.504) data 0.425 (0.373) loss_x loss_x 1.0518 (1.1076) acc_x 68.7500 (71.2500) lr 3.9958e-04 eta 0:00:18
epoch [143/200] batch [10/41] time 0.477 (0.487) data 0.346 (0.356) loss_x loss_x 0.8486 (1.1018) acc_x 84.3750 (72.1875) lr 3.9958e-04 eta 0:00:15
epoch [143/200] batch [15/41] time 0.426 (0.478) data 0.296 (0.347) loss_x loss_x 0.9224 (1.1316) acc_x 75.0000 (71.8750) lr 3.9958e-04 eta 0:00:12
epoch [143/200] batch [20/41] time 0.458 (0.483) data 0.328 (0.353) loss_x loss_x 1.5566 (1.1498) acc_x 68.7500 (71.0938) lr 3.9958e-04 eta 0:00:10
epoch [143/200] batch [25/41] time 0.543 (0.483) data 0.413 (0.352) loss_x loss_x 0.9346 (1.1581) acc_x 71.8750 (71.5000) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [30/41] time 0.497 (0.478) data 0.367 (0.348) loss_x loss_x 1.3418 (1.1640) acc_x 65.6250 (70.9375) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [35/41] time 0.567 (0.486) data 0.437 (0.356) loss_x loss_x 0.5454 (1.1582) acc_x 87.5000 (71.4286) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [40/41] time 0.434 (0.480) data 0.303 (0.349) loss_x loss_x 1.6143 (1.1791) acc_x 53.1250 (70.9375) lr 3.9958e-04 eta 0:00:00
epoch [143/200] batch [5/56] time 0.438 (0.474) data 0.307 (0.343) loss_u loss_u 0.7700 (0.7852) acc_u 25.0000 (26.2500) lr 3.9958e-04 eta 0:00:24
epoch [143/200] batch [10/56] time 0.428 (0.469) data 0.296 (0.338) loss_u loss_u 0.7993 (0.8009) acc_u 25.0000 (23.4375) lr 3.9958e-04 eta 0:00:21
epoch [143/200] batch [15/56] time 0.407 (0.468) data 0.276 (0.337) loss_u loss_u 0.8501 (0.8149) acc_u 25.0000 (23.3333) lr 3.9958e-04 eta 0:00:19
epoch [143/200] batch [20/56] time 0.718 (0.472) data 0.587 (0.341) loss_u loss_u 0.7319 (0.8093) acc_u 28.1250 (23.4375) lr 3.9958e-04 eta 0:00:16
epoch [143/200] batch [25/56] time 0.525 (0.470) data 0.393 (0.339) loss_u loss_u 0.8154 (0.8143) acc_u 21.8750 (23.2500) lr 3.9958e-04 eta 0:00:14
epoch [143/200] batch [30/56] time 0.432 (0.470) data 0.300 (0.340) loss_u loss_u 0.7559 (0.8141) acc_u 31.2500 (23.3333) lr 3.9958e-04 eta 0:00:12
epoch [143/200] batch [35/56] time 0.452 (0.470) data 0.322 (0.339) loss_u loss_u 0.8354 (0.8196) acc_u 21.8750 (22.9464) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [40/56] time 0.488 (0.469) data 0.356 (0.338) loss_u loss_u 0.8511 (0.8202) acc_u 18.7500 (22.9688) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [45/56] time 0.530 (0.468) data 0.397 (0.337) loss_u loss_u 0.8301 (0.8195) acc_u 18.7500 (22.9167) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [50/56] time 0.342 (0.466) data 0.211 (0.335) loss_u loss_u 0.9077 (0.8239) acc_u 12.5000 (22.5625) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [55/56] time 0.483 (0.464) data 0.351 (0.333) loss_u loss_u 0.8623 (0.8278) acc_u 21.8750 (22.1591) lr 3.9958e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1480
confident_label rate tensor(0.4209, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1320
clean true:1319
clean false:1
clean_rate:0.9992424242424243
noisy true:337
noisy false:1479
after delete: len(clean_dataset) 1320
after delete: len(noisy_dataset) 1816
epoch [144/200] batch [5/41] time 0.587 (0.466) data 0.456 (0.334) loss_x loss_x 1.0166 (1.0895) acc_x 78.1250 (73.1250) lr 3.8709e-04 eta 0:00:16
epoch [144/200] batch [10/41] time 0.430 (0.460) data 0.300 (0.329) loss_x loss_x 0.9062 (1.1318) acc_x 75.0000 (72.5000) lr 3.8709e-04 eta 0:00:14
epoch [144/200] batch [15/41] time 0.509 (0.477) data 0.378 (0.346) loss_x loss_x 0.8784 (1.0840) acc_x 75.0000 (73.1250) lr 3.8709e-04 eta 0:00:12
epoch [144/200] batch [20/41] time 0.413 (0.466) data 0.283 (0.335) loss_x loss_x 1.1982 (1.1346) acc_x 68.7500 (71.7188) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [25/41] time 0.507 (0.469) data 0.376 (0.338) loss_x loss_x 0.9971 (1.1721) acc_x 81.2500 (71.5000) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [30/41] time 0.443 (0.472) data 0.313 (0.341) loss_x loss_x 0.7490 (1.1227) acc_x 87.5000 (73.0208) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [35/41] time 0.437 (0.482) data 0.307 (0.351) loss_x loss_x 0.8843 (1.1014) acc_x 75.0000 (73.3036) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [40/41] time 0.556 (0.478) data 0.426 (0.348) loss_x loss_x 0.9561 (1.1196) acc_x 81.2500 (72.7344) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/56] time 0.419 (0.478) data 0.288 (0.347) loss_u loss_u 0.7505 (0.8344) acc_u 28.1250 (18.1250) lr 3.8709e-04 eta 0:00:24
epoch [144/200] batch [10/56] time 0.451 (0.472) data 0.319 (0.341) loss_u loss_u 0.8564 (0.8294) acc_u 21.8750 (20.9375) lr 3.8709e-04 eta 0:00:21
epoch [144/200] batch [15/56] time 0.423 (0.469) data 0.292 (0.338) loss_u loss_u 0.7158 (0.8241) acc_u 37.5000 (23.1250) lr 3.8709e-04 eta 0:00:19
epoch [144/200] batch [20/56] time 0.548 (0.473) data 0.418 (0.343) loss_u loss_u 0.8267 (0.8231) acc_u 15.6250 (22.8125) lr 3.8709e-04 eta 0:00:17
epoch [144/200] batch [25/56] time 0.456 (0.473) data 0.326 (0.342) loss_u loss_u 0.7764 (0.8184) acc_u 25.0000 (23.0000) lr 3.8709e-04 eta 0:00:14
epoch [144/200] batch [30/56] time 0.566 (0.471) data 0.435 (0.340) loss_u loss_u 0.7695 (0.8168) acc_u 25.0000 (23.2292) lr 3.8709e-04 eta 0:00:12
epoch [144/200] batch [35/56] time 0.551 (0.467) data 0.419 (0.336) loss_u loss_u 0.7881 (0.8173) acc_u 28.1250 (23.2143) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [40/56] time 0.383 (0.465) data 0.253 (0.334) loss_u loss_u 0.8354 (0.8205) acc_u 18.7500 (22.6562) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [45/56] time 0.434 (0.465) data 0.304 (0.334) loss_u loss_u 0.8203 (0.8217) acc_u 25.0000 (22.2917) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [50/56] time 0.384 (0.463) data 0.252 (0.332) loss_u loss_u 0.7603 (0.8221) acc_u 21.8750 (22.0625) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [55/56] time 0.475 (0.465) data 0.344 (0.334) loss_u loss_u 0.8467 (0.8224) acc_u 21.8750 (21.9886) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1500
confident_label rate tensor(0.4174, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1309
clean true:1308
clean false:1
clean_rate:0.9992360580595875
noisy true:328
noisy false:1499
after delete: len(clean_dataset) 1309
after delete: len(noisy_dataset) 1827
epoch [145/200] batch [5/40] time 0.428 (0.463) data 0.297 (0.332) loss_x loss_x 0.9697 (1.0989) acc_x 78.1250 (70.6250) lr 3.7476e-04 eta 0:00:16
epoch [145/200] batch [10/40] time 0.422 (0.447) data 0.291 (0.316) loss_x loss_x 1.1875 (1.0754) acc_x 65.6250 (70.9375) lr 3.7476e-04 eta 0:00:13
epoch [145/200] batch [15/40] time 0.578 (0.462) data 0.447 (0.331) loss_x loss_x 0.9668 (1.0115) acc_x 71.8750 (73.1250) lr 3.7476e-04 eta 0:00:11
epoch [145/200] batch [20/40] time 0.474 (0.457) data 0.344 (0.326) loss_x loss_x 1.2295 (1.0508) acc_x 71.8750 (72.9688) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [25/40] time 0.520 (0.461) data 0.389 (0.331) loss_x loss_x 1.3232 (1.0594) acc_x 62.5000 (73.2500) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [30/40] time 0.501 (0.471) data 0.371 (0.340) loss_x loss_x 1.3740 (1.0952) acc_x 68.7500 (72.6042) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [35/40] time 0.503 (0.466) data 0.373 (0.335) loss_x loss_x 1.3125 (1.0710) acc_x 71.8750 (73.2143) lr 3.7476e-04 eta 0:00:02
epoch [145/200] batch [40/40] time 0.388 (0.465) data 0.257 (0.335) loss_x loss_x 1.3857 (1.0673) acc_x 68.7500 (73.5156) lr 3.7476e-04 eta 0:00:00
epoch [145/200] batch [5/57] time 0.572 (0.467) data 0.441 (0.336) loss_u loss_u 0.8320 (0.8284) acc_u 18.7500 (21.2500) lr 3.7476e-04 eta 0:00:24
epoch [145/200] batch [10/57] time 0.425 (0.470) data 0.294 (0.339) loss_u loss_u 0.8594 (0.8401) acc_u 18.7500 (20.9375) lr 3.7476e-04 eta 0:00:22
epoch [145/200] batch [15/57] time 0.357 (0.464) data 0.226 (0.333) loss_u loss_u 0.7368 (0.8216) acc_u 31.2500 (22.5000) lr 3.7476e-04 eta 0:00:19
epoch [145/200] batch [20/57] time 0.556 (0.467) data 0.425 (0.337) loss_u loss_u 0.8818 (0.8198) acc_u 18.7500 (23.2812) lr 3.7476e-04 eta 0:00:17
epoch [145/200] batch [25/57] time 0.529 (0.464) data 0.397 (0.333) loss_u loss_u 0.7749 (0.8239) acc_u 34.3750 (22.8750) lr 3.7476e-04 eta 0:00:14
epoch [145/200] batch [30/57] time 0.543 (0.461) data 0.412 (0.330) loss_u loss_u 0.8359 (0.8234) acc_u 18.7500 (23.0208) lr 3.7476e-04 eta 0:00:12
epoch [145/200] batch [35/57] time 0.632 (0.465) data 0.500 (0.334) loss_u loss_u 0.7480 (0.8195) acc_u 34.3750 (23.3036) lr 3.7476e-04 eta 0:00:10
epoch [145/200] batch [40/57] time 0.390 (0.462) data 0.259 (0.331) loss_u loss_u 0.8169 (0.8203) acc_u 15.6250 (22.8906) lr 3.7476e-04 eta 0:00:07
epoch [145/200] batch [45/57] time 0.583 (0.464) data 0.452 (0.333) loss_u loss_u 0.8340 (0.8200) acc_u 18.7500 (22.9167) lr 3.7476e-04 eta 0:00:05
epoch [145/200] batch [50/57] time 0.509 (0.462) data 0.377 (0.331) loss_u loss_u 0.8257 (0.8217) acc_u 21.8750 (22.8125) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [55/57] time 0.421 (0.462) data 0.290 (0.331) loss_u loss_u 0.9365 (0.8260) acc_u 9.3750 (22.4432) lr 3.7476e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1482
confident_label rate tensor(0.4209, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1320
clean true:1320
clean false:0
clean_rate:1.0
noisy true:334
noisy false:1482
after delete: len(clean_dataset) 1320
after delete: len(noisy_dataset) 1816
epoch [146/200] batch [5/41] time 0.591 (0.491) data 0.460 (0.360) loss_x loss_x 0.7700 (1.1326) acc_x 78.1250 (73.7500) lr 3.6258e-04 eta 0:00:17
epoch [146/200] batch [10/41] time 0.507 (0.483) data 0.376 (0.352) loss_x loss_x 1.0273 (1.1175) acc_x 78.1250 (71.8750) lr 3.6258e-04 eta 0:00:14
epoch [146/200] batch [15/41] time 0.403 (0.481) data 0.273 (0.351) loss_x loss_x 1.0117 (1.0806) acc_x 68.7500 (71.8750) lr 3.6258e-04 eta 0:00:12
epoch [146/200] batch [20/41] time 0.498 (0.479) data 0.367 (0.348) loss_x loss_x 1.3350 (1.0942) acc_x 75.0000 (73.2812) lr 3.6258e-04 eta 0:00:10
epoch [146/200] batch [25/41] time 0.421 (0.471) data 0.290 (0.340) loss_x loss_x 1.0303 (1.1293) acc_x 71.8750 (72.6250) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [30/41] time 0.516 (0.468) data 0.386 (0.337) loss_x loss_x 1.3447 (1.1538) acc_x 62.5000 (71.9792) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [35/41] time 0.398 (0.468) data 0.267 (0.337) loss_x loss_x 1.6357 (1.1765) acc_x 65.6250 (71.1607) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [40/41] time 0.551 (0.468) data 0.421 (0.337) loss_x loss_x 0.9609 (1.1477) acc_x 71.8750 (71.2500) lr 3.6258e-04 eta 0:00:00
epoch [146/200] batch [5/56] time 0.465 (0.465) data 0.332 (0.334) loss_u loss_u 0.8809 (0.8030) acc_u 15.6250 (25.0000) lr 3.6258e-04 eta 0:00:23
epoch [146/200] batch [10/56] time 0.434 (0.468) data 0.302 (0.338) loss_u loss_u 0.8286 (0.8288) acc_u 18.7500 (21.8750) lr 3.6258e-04 eta 0:00:21
epoch [146/200] batch [15/56] time 0.389 (0.463) data 0.259 (0.332) loss_u loss_u 0.8447 (0.8350) acc_u 25.0000 (21.0417) lr 3.6258e-04 eta 0:00:18
epoch [146/200] batch [20/56] time 0.401 (0.459) data 0.270 (0.328) loss_u loss_u 0.8594 (0.8352) acc_u 18.7500 (20.4688) lr 3.6258e-04 eta 0:00:16
epoch [146/200] batch [25/56] time 0.414 (0.455) data 0.284 (0.325) loss_u loss_u 0.8140 (0.8361) acc_u 21.8750 (20.0000) lr 3.6258e-04 eta 0:00:14
epoch [146/200] batch [30/56] time 0.632 (0.464) data 0.502 (0.334) loss_u loss_u 0.8457 (0.8433) acc_u 21.8750 (19.3750) lr 3.6258e-04 eta 0:00:12
epoch [146/200] batch [35/56] time 0.466 (0.465) data 0.334 (0.334) loss_u loss_u 0.8223 (0.8431) acc_u 25.0000 (19.7321) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [40/56] time 0.437 (0.463) data 0.306 (0.333) loss_u loss_u 0.8042 (0.8393) acc_u 28.1250 (20.4688) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [45/56] time 0.487 (0.462) data 0.356 (0.331) loss_u loss_u 0.8291 (0.8412) acc_u 25.0000 (20.4167) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [50/56] time 0.528 (0.464) data 0.397 (0.333) loss_u loss_u 0.7979 (0.8420) acc_u 28.1250 (20.4375) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [55/56] time 0.629 (0.467) data 0.498 (0.336) loss_u loss_u 0.8218 (0.8370) acc_u 21.8750 (21.0795) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1453
confident_label rate tensor(0.4292, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1346
clean true:1342
clean false:4
clean_rate:0.9970282317979198
noisy true:341
noisy false:1449
after delete: len(clean_dataset) 1346
after delete: len(noisy_dataset) 1790
epoch [147/200] batch [5/42] time 0.342 (0.411) data 0.211 (0.280) loss_x loss_x 0.9526 (1.3077) acc_x 81.2500 (68.1250) lr 3.5055e-04 eta 0:00:15
epoch [147/200] batch [10/42] time 0.457 (0.440) data 0.326 (0.309) loss_x loss_x 0.8594 (1.2426) acc_x 75.0000 (69.6875) lr 3.5055e-04 eta 0:00:14
epoch [147/200] batch [15/42] time 0.497 (0.460) data 0.366 (0.329) loss_x loss_x 1.0088 (1.1296) acc_x 71.8750 (71.2500) lr 3.5055e-04 eta 0:00:12
epoch [147/200] batch [20/42] time 0.443 (0.454) data 0.313 (0.323) loss_x loss_x 1.0713 (1.1788) acc_x 78.1250 (70.3125) lr 3.5055e-04 eta 0:00:09
epoch [147/200] batch [25/42] time 0.389 (0.457) data 0.258 (0.326) loss_x loss_x 0.9839 (1.1818) acc_x 75.0000 (69.8750) lr 3.5055e-04 eta 0:00:07
epoch [147/200] batch [30/42] time 0.415 (0.454) data 0.284 (0.323) loss_x loss_x 1.3613 (1.1675) acc_x 68.7500 (70.4167) lr 3.5055e-04 eta 0:00:05
epoch [147/200] batch [35/42] time 0.450 (0.454) data 0.319 (0.323) loss_x loss_x 1.3281 (1.1447) acc_x 65.6250 (70.9821) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [40/42] time 0.545 (0.456) data 0.415 (0.325) loss_x loss_x 1.2803 (1.1614) acc_x 65.6250 (71.2500) lr 3.5055e-04 eta 0:00:00
epoch [147/200] batch [5/55] time 0.378 (0.453) data 0.247 (0.322) loss_u loss_u 0.8262 (0.8026) acc_u 21.8750 (25.0000) lr 3.5055e-04 eta 0:00:22
epoch [147/200] batch [10/55] time 0.437 (0.450) data 0.306 (0.319) loss_u loss_u 0.7310 (0.8161) acc_u 31.2500 (23.1250) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [15/55] time 0.574 (0.452) data 0.441 (0.321) loss_u loss_u 0.7969 (0.8165) acc_u 25.0000 (23.7500) lr 3.5055e-04 eta 0:00:18
epoch [147/200] batch [20/55] time 0.384 (0.453) data 0.251 (0.322) loss_u loss_u 0.8379 (0.8206) acc_u 18.7500 (22.6562) lr 3.5055e-04 eta 0:00:15
epoch [147/200] batch [25/55] time 0.431 (0.457) data 0.300 (0.326) loss_u loss_u 0.7866 (0.8175) acc_u 31.2500 (23.3750) lr 3.5055e-04 eta 0:00:13
epoch [147/200] batch [30/55] time 0.486 (0.459) data 0.355 (0.328) loss_u loss_u 0.8394 (0.8160) acc_u 21.8750 (23.7500) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [35/55] time 0.379 (0.457) data 0.247 (0.326) loss_u loss_u 0.9238 (0.8217) acc_u 3.1250 (22.8571) lr 3.5055e-04 eta 0:00:09
epoch [147/200] batch [40/55] time 0.415 (0.454) data 0.284 (0.323) loss_u loss_u 0.7939 (0.8235) acc_u 25.0000 (22.5000) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [45/55] time 0.440 (0.458) data 0.309 (0.327) loss_u loss_u 0.9102 (0.8257) acc_u 12.5000 (22.0833) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [50/55] time 0.452 (0.461) data 0.320 (0.330) loss_u loss_u 0.8037 (0.8283) acc_u 28.1250 (21.6250) lr 3.5055e-04 eta 0:00:02
epoch [147/200] batch [55/55] time 0.575 (0.462) data 0.443 (0.331) loss_u loss_u 0.8403 (0.8317) acc_u 21.8750 (21.2500) lr 3.5055e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1430
confident_label rate tensor(0.4346, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1363
clean true:1361
clean false:2
clean_rate:0.9985326485693323
noisy true:345
noisy false:1428
after delete: len(clean_dataset) 1363
after delete: len(noisy_dataset) 1773
epoch [148/200] batch [5/42] time 0.441 (0.489) data 0.311 (0.359) loss_x loss_x 1.0732 (1.2046) acc_x 71.8750 (72.5000) lr 3.3869e-04 eta 0:00:18
epoch [148/200] batch [10/42] time 0.516 (0.473) data 0.385 (0.342) loss_x loss_x 1.1494 (1.1458) acc_x 71.8750 (73.7500) lr 3.3869e-04 eta 0:00:15
epoch [148/200] batch [15/42] time 0.391 (0.473) data 0.260 (0.342) loss_x loss_x 1.7236 (1.1876) acc_x 65.6250 (72.5000) lr 3.3869e-04 eta 0:00:12
epoch [148/200] batch [20/42] time 0.461 (0.469) data 0.331 (0.339) loss_x loss_x 0.9116 (1.1843) acc_x 75.0000 (71.8750) lr 3.3869e-04 eta 0:00:10
epoch [148/200] batch [25/42] time 0.460 (0.460) data 0.330 (0.329) loss_x loss_x 1.4473 (1.1900) acc_x 68.7500 (71.2500) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [30/42] time 0.407 (0.470) data 0.277 (0.340) loss_x loss_x 1.1768 (1.1604) acc_x 65.6250 (72.2917) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [35/42] time 0.418 (0.469) data 0.288 (0.338) loss_x loss_x 1.5684 (1.1549) acc_x 65.6250 (72.1429) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [40/42] time 0.350 (0.467) data 0.220 (0.336) loss_x loss_x 1.1387 (1.1643) acc_x 78.1250 (72.2656) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/55] time 0.562 (0.468) data 0.432 (0.338) loss_u loss_u 0.8857 (0.8384) acc_u 18.7500 (23.7500) lr 3.3869e-04 eta 0:00:23
epoch [148/200] batch [10/55] time 0.411 (0.469) data 0.280 (0.339) loss_u loss_u 0.8623 (0.8505) acc_u 12.5000 (20.9375) lr 3.3869e-04 eta 0:00:21
epoch [148/200] batch [15/55] time 0.455 (0.469) data 0.323 (0.338) loss_u loss_u 0.8726 (0.8408) acc_u 18.7500 (21.4583) lr 3.3869e-04 eta 0:00:18
epoch [148/200] batch [20/55] time 0.499 (0.469) data 0.367 (0.338) loss_u loss_u 0.8047 (0.8344) acc_u 25.0000 (22.9688) lr 3.3869e-04 eta 0:00:16
epoch [148/200] batch [25/55] time 0.474 (0.470) data 0.343 (0.339) loss_u loss_u 0.8018 (0.8321) acc_u 21.8750 (22.6250) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [30/55] time 0.458 (0.472) data 0.326 (0.341) loss_u loss_u 0.8369 (0.8336) acc_u 21.8750 (22.0833) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [35/55] time 0.423 (0.470) data 0.292 (0.340) loss_u loss_u 0.6543 (0.8284) acc_u 46.8750 (22.5000) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [40/55] time 0.513 (0.471) data 0.381 (0.340) loss_u loss_u 0.8091 (0.8317) acc_u 25.0000 (21.8750) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [45/55] time 0.457 (0.468) data 0.325 (0.337) loss_u loss_u 0.8618 (0.8332) acc_u 15.6250 (21.5278) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [50/55] time 0.448 (0.468) data 0.318 (0.337) loss_u loss_u 0.7783 (0.8321) acc_u 21.8750 (21.4375) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [55/55] time 0.416 (0.470) data 0.286 (0.339) loss_u loss_u 0.8403 (0.8305) acc_u 25.0000 (21.8182) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1462
confident_label rate tensor(0.4247, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1332
clean true:1328
clean false:4
clean_rate:0.996996996996997
noisy true:346
noisy false:1458
after delete: len(clean_dataset) 1332
after delete: len(noisy_dataset) 1804
epoch [149/200] batch [5/41] time 0.551 (0.463) data 0.420 (0.332) loss_x loss_x 1.2959 (1.1804) acc_x 78.1250 (75.0000) lr 3.2699e-04 eta 0:00:16
epoch [149/200] batch [10/41] time 0.427 (0.449) data 0.296 (0.318) loss_x loss_x 0.8438 (1.1442) acc_x 68.7500 (72.1875) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [15/41] time 0.440 (0.452) data 0.308 (0.320) loss_x loss_x 1.6006 (1.1564) acc_x 71.8750 (71.4583) lr 3.2699e-04 eta 0:00:11
epoch [149/200] batch [20/41] time 0.459 (0.465) data 0.327 (0.334) loss_x loss_x 1.0645 (1.1336) acc_x 75.0000 (72.1875) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [25/41] time 0.448 (0.462) data 0.318 (0.331) loss_x loss_x 1.1338 (1.1459) acc_x 71.8750 (72.1250) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [30/41] time 0.435 (0.460) data 0.305 (0.329) loss_x loss_x 1.2314 (1.1357) acc_x 68.7500 (72.3958) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [35/41] time 0.576 (0.468) data 0.445 (0.337) loss_x loss_x 1.2119 (1.1736) acc_x 68.7500 (71.6964) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [40/41] time 0.417 (0.468) data 0.286 (0.337) loss_x loss_x 1.1582 (1.1850) acc_x 68.7500 (71.4844) lr 3.2699e-04 eta 0:00:00
epoch [149/200] batch [5/56] time 0.396 (0.464) data 0.265 (0.333) loss_u loss_u 0.9478 (0.8563) acc_u 3.1250 (18.1250) lr 3.2699e-04 eta 0:00:23
epoch [149/200] batch [10/56] time 0.391 (0.461) data 0.260 (0.330) loss_u loss_u 0.7690 (0.8294) acc_u 21.8750 (20.6250) lr 3.2699e-04 eta 0:00:21
epoch [149/200] batch [15/56] time 0.495 (0.468) data 0.363 (0.337) loss_u loss_u 0.8467 (0.8299) acc_u 25.0000 (20.6250) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [20/56] time 0.416 (0.468) data 0.284 (0.337) loss_u loss_u 0.9058 (0.8324) acc_u 15.6250 (20.7812) lr 3.2699e-04 eta 0:00:16
epoch [149/200] batch [25/56] time 0.381 (0.473) data 0.250 (0.342) loss_u loss_u 0.8320 (0.8367) acc_u 18.7500 (20.5000) lr 3.2699e-04 eta 0:00:14
epoch [149/200] batch [30/56] time 0.480 (0.469) data 0.349 (0.338) loss_u loss_u 0.8384 (0.8355) acc_u 21.8750 (21.1458) lr 3.2699e-04 eta 0:00:12
epoch [149/200] batch [35/56] time 0.389 (0.469) data 0.258 (0.338) loss_u loss_u 0.8110 (0.8320) acc_u 28.1250 (21.4286) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [40/56] time 0.417 (0.468) data 0.286 (0.337) loss_u loss_u 0.7964 (0.8292) acc_u 21.8750 (21.6406) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [45/56] time 0.353 (0.469) data 0.222 (0.338) loss_u loss_u 0.9224 (0.8308) acc_u 6.2500 (21.3194) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [50/56] time 0.480 (0.471) data 0.349 (0.340) loss_u loss_u 0.7388 (0.8309) acc_u 37.5000 (21.5000) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [55/56] time 0.653 (0.474) data 0.521 (0.343) loss_u loss_u 0.8267 (0.8302) acc_u 25.0000 (21.4205) lr 3.2699e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1430
confident_label rate tensor(0.4324, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1356
clean true:1351
clean false:5
clean_rate:0.9963126843657817
noisy true:355
noisy false:1425
after delete: len(clean_dataset) 1356
after delete: len(noisy_dataset) 1780
epoch [150/200] batch [5/42] time 0.403 (0.503) data 0.273 (0.373) loss_x loss_x 1.2822 (1.1183) acc_x 75.0000 (71.8750) lr 3.1545e-04 eta 0:00:18
epoch [150/200] batch [10/42] time 0.488 (0.487) data 0.358 (0.357) loss_x loss_x 1.0742 (1.1273) acc_x 71.8750 (72.5000) lr 3.1545e-04 eta 0:00:15
epoch [150/200] batch [15/42] time 0.403 (0.478) data 0.273 (0.348) loss_x loss_x 1.2939 (1.1697) acc_x 65.6250 (71.2500) lr 3.1545e-04 eta 0:00:12
epoch [150/200] batch [20/42] time 0.485 (0.478) data 0.355 (0.348) loss_x loss_x 1.2656 (1.1453) acc_x 68.7500 (71.2500) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [25/42] time 0.354 (0.467) data 0.224 (0.337) loss_x loss_x 1.3291 (1.1425) acc_x 71.8750 (71.2500) lr 3.1545e-04 eta 0:00:07
epoch [150/200] batch [30/42] time 0.454 (0.463) data 0.323 (0.333) loss_x loss_x 1.3340 (1.1591) acc_x 68.7500 (70.7292) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [35/42] time 0.443 (0.463) data 0.313 (0.333) loss_x loss_x 1.2295 (1.1548) acc_x 68.7500 (70.7143) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [40/42] time 0.565 (0.463) data 0.433 (0.332) loss_x loss_x 0.9131 (1.1507) acc_x 81.2500 (70.8594) lr 3.1545e-04 eta 0:00:00
epoch [150/200] batch [5/55] time 0.384 (0.460) data 0.253 (0.330) loss_u loss_u 0.8188 (0.8042) acc_u 21.8750 (23.7500) lr 3.1545e-04 eta 0:00:23
epoch [150/200] batch [10/55] time 0.583 (0.467) data 0.452 (0.336) loss_u loss_u 0.8003 (0.8200) acc_u 18.7500 (21.5625) lr 3.1545e-04 eta 0:00:21
epoch [150/200] batch [15/55] time 0.415 (0.463) data 0.284 (0.333) loss_u loss_u 0.8242 (0.8125) acc_u 21.8750 (22.2917) lr 3.1545e-04 eta 0:00:18
epoch [150/200] batch [20/55] time 0.378 (0.459) data 0.248 (0.329) loss_u loss_u 0.8604 (0.8135) acc_u 18.7500 (22.6562) lr 3.1545e-04 eta 0:00:16
epoch [150/200] batch [25/55] time 0.396 (0.461) data 0.265 (0.331) loss_u loss_u 0.8804 (0.8181) acc_u 15.6250 (22.1250) lr 3.1545e-04 eta 0:00:13
epoch [150/200] batch [30/55] time 0.388 (0.457) data 0.256 (0.327) loss_u loss_u 0.8613 (0.8243) acc_u 12.5000 (21.6667) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [35/55] time 0.537 (0.456) data 0.405 (0.325) loss_u loss_u 0.8232 (0.8292) acc_u 18.7500 (20.7143) lr 3.1545e-04 eta 0:00:09
epoch [150/200] batch [40/55] time 0.465 (0.455) data 0.332 (0.325) loss_u loss_u 0.8467 (0.8336) acc_u 25.0000 (20.6250) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [45/55] time 0.532 (0.460) data 0.401 (0.329) loss_u loss_u 0.8101 (0.8324) acc_u 21.8750 (20.6944) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [50/55] time 0.567 (0.462) data 0.435 (0.331) loss_u loss_u 0.8027 (0.8333) acc_u 28.1250 (20.8125) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [55/55] time 0.520 (0.463) data 0.389 (0.332) loss_u loss_u 0.8608 (0.8332) acc_u 21.8750 (21.0227) lr 3.1545e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1448
confident_label rate tensor(0.4308, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1351
clean true:1349
clean false:2
clean_rate:0.9985196150999259
noisy true:339
noisy false:1446
after delete: len(clean_dataset) 1351
after delete: len(noisy_dataset) 1785
epoch [151/200] batch [5/42] time 0.428 (0.423) data 0.298 (0.293) loss_x loss_x 0.9995 (1.1523) acc_x 68.7500 (71.8750) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [10/42] time 0.438 (0.426) data 0.308 (0.295) loss_x loss_x 1.3164 (1.1718) acc_x 62.5000 (72.8125) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [15/42] time 0.473 (0.439) data 0.343 (0.308) loss_x loss_x 1.7695 (1.1639) acc_x 59.3750 (71.4583) lr 3.0409e-04 eta 0:00:11
epoch [151/200] batch [20/42] time 0.431 (0.443) data 0.300 (0.312) loss_x loss_x 1.1689 (1.1163) acc_x 71.8750 (72.0312) lr 3.0409e-04 eta 0:00:09
epoch [151/200] batch [25/42] time 0.499 (0.458) data 0.369 (0.327) loss_x loss_x 0.9507 (1.1247) acc_x 75.0000 (71.8750) lr 3.0409e-04 eta 0:00:07
epoch [151/200] batch [30/42] time 0.442 (0.452) data 0.311 (0.321) loss_x loss_x 0.8154 (1.1129) acc_x 78.1250 (71.2500) lr 3.0409e-04 eta 0:00:05
epoch [151/200] batch [35/42] time 0.554 (0.458) data 0.423 (0.327) loss_x loss_x 1.3086 (1.1274) acc_x 56.2500 (70.6250) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [40/42] time 0.574 (0.463) data 0.443 (0.333) loss_x loss_x 0.9922 (1.1162) acc_x 71.8750 (70.9375) lr 3.0409e-04 eta 0:00:00
epoch [151/200] batch [5/55] time 0.451 (0.463) data 0.321 (0.332) loss_u loss_u 0.7778 (0.8070) acc_u 28.1250 (26.2500) lr 3.0409e-04 eta 0:00:23
epoch [151/200] batch [10/55] time 0.403 (0.467) data 0.272 (0.336) loss_u loss_u 0.8916 (0.8337) acc_u 12.5000 (23.1250) lr 3.0409e-04 eta 0:00:21
epoch [151/200] batch [15/55] time 0.661 (0.468) data 0.531 (0.337) loss_u loss_u 0.8813 (0.8312) acc_u 15.6250 (22.7083) lr 3.0409e-04 eta 0:00:18
epoch [151/200] batch [20/55] time 0.581 (0.466) data 0.450 (0.335) loss_u loss_u 0.8018 (0.8340) acc_u 21.8750 (21.5625) lr 3.0409e-04 eta 0:00:16
epoch [151/200] batch [25/55] time 0.458 (0.466) data 0.326 (0.336) loss_u loss_u 0.8374 (0.8277) acc_u 21.8750 (22.2500) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [30/55] time 0.396 (0.465) data 0.264 (0.334) loss_u loss_u 0.8804 (0.8285) acc_u 15.6250 (22.2917) lr 3.0409e-04 eta 0:00:11
epoch [151/200] batch [35/55] time 0.406 (0.462) data 0.275 (0.331) loss_u loss_u 0.8618 (0.8285) acc_u 15.6250 (22.0536) lr 3.0409e-04 eta 0:00:09
epoch [151/200] batch [40/55] time 0.472 (0.464) data 0.342 (0.333) loss_u loss_u 0.8501 (0.8299) acc_u 15.6250 (21.7969) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [45/55] time 0.630 (0.469) data 0.499 (0.338) loss_u loss_u 0.8188 (0.8278) acc_u 21.8750 (21.8750) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [50/55] time 0.360 (0.468) data 0.229 (0.337) loss_u loss_u 0.7798 (0.8276) acc_u 21.8750 (21.8750) lr 3.0409e-04 eta 0:00:02
epoch [151/200] batch [55/55] time 0.446 (0.465) data 0.315 (0.334) loss_u loss_u 0.7837 (0.8282) acc_u 21.8750 (21.8182) lr 3.0409e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1421
confident_label rate tensor(0.4292, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1346
clean true:1342
clean false:4
clean_rate:0.9970282317979198
noisy true:373
noisy false:1417
after delete: len(clean_dataset) 1346
after delete: len(noisy_dataset) 1790
epoch [152/200] batch [5/42] time 0.673 (0.535) data 0.542 (0.404) loss_x loss_x 0.9746 (1.0708) acc_x 71.8750 (75.0000) lr 2.9289e-04 eta 0:00:19
epoch [152/200] batch [10/42] time 0.554 (0.519) data 0.424 (0.388) loss_x loss_x 0.9800 (1.1052) acc_x 81.2500 (74.6875) lr 2.9289e-04 eta 0:00:16
epoch [152/200] batch [15/42] time 0.393 (0.493) data 0.262 (0.362) loss_x loss_x 0.9746 (1.0409) acc_x 68.7500 (75.0000) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [20/42] time 0.485 (0.478) data 0.355 (0.347) loss_x loss_x 0.9014 (1.0851) acc_x 71.8750 (73.9062) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [25/42] time 0.440 (0.468) data 0.309 (0.337) loss_x loss_x 0.9160 (1.0665) acc_x 81.2500 (73.8750) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [30/42] time 0.381 (0.461) data 0.251 (0.330) loss_x loss_x 1.1465 (1.1172) acc_x 71.8750 (72.6042) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [35/42] time 0.337 (0.451) data 0.207 (0.320) loss_x loss_x 1.9424 (1.1420) acc_x 59.3750 (72.0536) lr 2.9289e-04 eta 0:00:03
epoch [152/200] batch [40/42] time 0.453 (0.452) data 0.323 (0.322) loss_x loss_x 1.3809 (1.1236) acc_x 62.5000 (72.3438) lr 2.9289e-04 eta 0:00:00
epoch [152/200] batch [5/55] time 0.479 (0.459) data 0.348 (0.328) loss_u loss_u 0.8066 (0.8699) acc_u 18.7500 (16.2500) lr 2.9289e-04 eta 0:00:22
epoch [152/200] batch [10/55] time 0.466 (0.460) data 0.335 (0.329) loss_u loss_u 0.8374 (0.8417) acc_u 21.8750 (20.6250) lr 2.9289e-04 eta 0:00:20
epoch [152/200] batch [15/55] time 0.770 (0.470) data 0.639 (0.339) loss_u loss_u 0.8462 (0.8315) acc_u 12.5000 (20.4167) lr 2.9289e-04 eta 0:00:18
epoch [152/200] batch [20/55] time 0.374 (0.469) data 0.243 (0.338) loss_u loss_u 0.8276 (0.8270) acc_u 21.8750 (21.4062) lr 2.9289e-04 eta 0:00:16
epoch [152/200] batch [25/55] time 0.453 (0.470) data 0.322 (0.339) loss_u loss_u 0.8022 (0.8245) acc_u 25.0000 (21.6250) lr 2.9289e-04 eta 0:00:14
epoch [152/200] batch [30/55] time 0.365 (0.465) data 0.234 (0.335) loss_u loss_u 0.8398 (0.8232) acc_u 21.8750 (21.8750) lr 2.9289e-04 eta 0:00:11
epoch [152/200] batch [35/55] time 0.676 (0.468) data 0.545 (0.337) loss_u loss_u 0.8652 (0.8268) acc_u 18.7500 (21.6071) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [40/55] time 0.451 (0.464) data 0.320 (0.333) loss_u loss_u 0.8008 (0.8255) acc_u 21.8750 (21.7969) lr 2.9289e-04 eta 0:00:06
epoch [152/200] batch [45/55] time 0.432 (0.463) data 0.301 (0.332) loss_u loss_u 0.8042 (0.8297) acc_u 31.2500 (21.3889) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [50/55] time 0.409 (0.461) data 0.279 (0.331) loss_u loss_u 0.7695 (0.8239) acc_u 28.1250 (22.1250) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [55/55] time 0.356 (0.458) data 0.225 (0.327) loss_u loss_u 0.8882 (0.8270) acc_u 15.6250 (21.5909) lr 2.9289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1428
confident_label rate tensor(0.4372, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1371
clean true:1368
clean false:3
clean_rate:0.9978118161925602
noisy true:340
noisy false:1425
after delete: len(clean_dataset) 1371
after delete: len(noisy_dataset) 1765
epoch [153/200] batch [5/42] time 0.526 (0.428) data 0.394 (0.297) loss_x loss_x 1.0010 (1.1801) acc_x 68.7500 (66.2500) lr 2.8187e-04 eta 0:00:15
epoch [153/200] batch [10/42] time 0.486 (0.454) data 0.355 (0.324) loss_x loss_x 1.0967 (1.1190) acc_x 75.0000 (72.5000) lr 2.8187e-04 eta 0:00:14
epoch [153/200] batch [15/42] time 0.460 (0.454) data 0.329 (0.324) loss_x loss_x 0.7490 (1.0878) acc_x 75.0000 (73.3333) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [20/42] time 0.493 (0.453) data 0.362 (0.322) loss_x loss_x 1.1777 (1.1331) acc_x 71.8750 (72.8125) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [25/42] time 0.464 (0.456) data 0.332 (0.325) loss_x loss_x 0.9858 (1.1258) acc_x 75.0000 (72.7500) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [30/42] time 0.406 (0.454) data 0.275 (0.323) loss_x loss_x 1.6689 (1.1304) acc_x 62.5000 (72.7083) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [35/42] time 0.575 (0.459) data 0.444 (0.328) loss_x loss_x 1.2188 (1.1221) acc_x 68.7500 (72.3214) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [40/42] time 0.729 (0.465) data 0.599 (0.335) loss_x loss_x 0.7920 (1.1111) acc_x 84.3750 (73.0469) lr 2.8187e-04 eta 0:00:00
epoch [153/200] batch [5/55] time 0.575 (0.466) data 0.444 (0.335) loss_u loss_u 0.7720 (0.8118) acc_u 28.1250 (24.3750) lr 2.8187e-04 eta 0:00:23
epoch [153/200] batch [10/55] time 0.606 (0.471) data 0.475 (0.340) loss_u loss_u 0.7612 (0.8172) acc_u 25.0000 (23.1250) lr 2.8187e-04 eta 0:00:21
epoch [153/200] batch [15/55] time 0.449 (0.464) data 0.317 (0.333) loss_u loss_u 0.8784 (0.8249) acc_u 12.5000 (22.2917) lr 2.8187e-04 eta 0:00:18
epoch [153/200] batch [20/55] time 0.519 (0.465) data 0.387 (0.334) loss_u loss_u 0.8604 (0.8324) acc_u 12.5000 (20.6250) lr 2.8187e-04 eta 0:00:16
epoch [153/200] batch [25/55] time 0.490 (0.465) data 0.358 (0.333) loss_u loss_u 0.7529 (0.8288) acc_u 31.2500 (21.2500) lr 2.8187e-04 eta 0:00:13
epoch [153/200] batch [30/55] time 0.398 (0.470) data 0.267 (0.338) loss_u loss_u 0.8228 (0.8331) acc_u 25.0000 (20.9375) lr 2.8187e-04 eta 0:00:11
epoch [153/200] batch [35/55] time 0.467 (0.467) data 0.336 (0.336) loss_u loss_u 0.8525 (0.8327) acc_u 18.7500 (21.5179) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [40/55] time 0.426 (0.462) data 0.296 (0.331) loss_u loss_u 0.8374 (0.8313) acc_u 21.8750 (21.7188) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [45/55] time 0.427 (0.466) data 0.297 (0.335) loss_u loss_u 0.8428 (0.8342) acc_u 15.6250 (21.4583) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [50/55] time 0.676 (0.469) data 0.546 (0.338) loss_u loss_u 0.7734 (0.8341) acc_u 28.1250 (21.4375) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [55/55] time 0.409 (0.468) data 0.279 (0.337) loss_u loss_u 0.8848 (0.8358) acc_u 15.6250 (21.0227) lr 2.8187e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1452
confident_label rate tensor(0.4321, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1355
clean true:1354
clean false:1
clean_rate:0.9992619926199262
noisy true:330
noisy false:1451
after delete: len(clean_dataset) 1355
after delete: len(noisy_dataset) 1781
epoch [154/200] batch [5/42] time 0.518 (0.465) data 0.387 (0.334) loss_x loss_x 1.1709 (1.1065) acc_x 71.8750 (74.3750) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [10/42] time 0.413 (0.458) data 0.281 (0.327) loss_x loss_x 1.2979 (1.1618) acc_x 68.7500 (71.8750) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [15/42] time 0.533 (0.467) data 0.403 (0.336) loss_x loss_x 1.0498 (1.1271) acc_x 75.0000 (72.7083) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [20/42] time 0.459 (0.451) data 0.328 (0.320) loss_x loss_x 1.2344 (1.1375) acc_x 68.7500 (72.8125) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [25/42] time 0.357 (0.449) data 0.226 (0.318) loss_x loss_x 1.3877 (1.1708) acc_x 71.8750 (70.8750) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [30/42] time 0.560 (0.471) data 0.429 (0.340) loss_x loss_x 1.3203 (1.1544) acc_x 68.7500 (71.2500) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [35/42] time 0.362 (0.469) data 0.231 (0.339) loss_x loss_x 1.6699 (1.1412) acc_x 65.6250 (71.9643) lr 2.7103e-04 eta 0:00:03
epoch [154/200] batch [40/42] time 0.417 (0.469) data 0.286 (0.338) loss_x loss_x 1.3975 (1.1592) acc_x 59.3750 (71.7188) lr 2.7103e-04 eta 0:00:00
epoch [154/200] batch [5/55] time 0.497 (0.480) data 0.364 (0.349) loss_u loss_u 0.8716 (0.8696) acc_u 18.7500 (15.0000) lr 2.7103e-04 eta 0:00:23
epoch [154/200] batch [10/55] time 0.537 (0.477) data 0.405 (0.346) loss_u loss_u 0.8223 (0.8736) acc_u 18.7500 (14.3750) lr 2.7103e-04 eta 0:00:21
epoch [154/200] batch [15/55] time 0.409 (0.475) data 0.278 (0.344) loss_u loss_u 0.7827 (0.8618) acc_u 28.1250 (16.4583) lr 2.7103e-04 eta 0:00:18
epoch [154/200] batch [20/55] time 0.587 (0.476) data 0.456 (0.346) loss_u loss_u 0.7920 (0.8582) acc_u 25.0000 (16.7188) lr 2.7103e-04 eta 0:00:16
epoch [154/200] batch [25/55] time 0.489 (0.478) data 0.358 (0.347) loss_u loss_u 0.8462 (0.8552) acc_u 15.6250 (17.2500) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [30/55] time 0.554 (0.479) data 0.423 (0.348) loss_u loss_u 0.8662 (0.8528) acc_u 21.8750 (18.0208) lr 2.7103e-04 eta 0:00:11
epoch [154/200] batch [35/55] time 0.476 (0.480) data 0.345 (0.349) loss_u loss_u 0.9419 (0.8531) acc_u 9.3750 (18.2143) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [40/55] time 0.430 (0.475) data 0.298 (0.344) loss_u loss_u 0.9004 (0.8510) acc_u 9.3750 (18.5156) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [45/55] time 0.409 (0.471) data 0.277 (0.340) loss_u loss_u 0.7847 (0.8487) acc_u 25.0000 (18.9583) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [50/55] time 0.359 (0.469) data 0.227 (0.338) loss_u loss_u 0.8208 (0.8492) acc_u 21.8750 (18.8750) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [55/55] time 0.737 (0.468) data 0.606 (0.337) loss_u loss_u 0.8936 (0.8460) acc_u 12.5000 (19.1477) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1425
confident_label rate tensor(0.4330, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1358
clean true:1356
clean false:2
clean_rate:0.9985272459499264
noisy true:355
noisy false:1423
after delete: len(clean_dataset) 1358
after delete: len(noisy_dataset) 1778
epoch [155/200] batch [5/42] time 0.594 (0.498) data 0.464 (0.367) loss_x loss_x 1.2021 (1.1374) acc_x 78.1250 (73.7500) lr 2.6037e-04 eta 0:00:18
epoch [155/200] batch [10/42] time 0.520 (0.469) data 0.389 (0.338) loss_x loss_x 0.6831 (1.2018) acc_x 87.5000 (71.8750) lr 2.6037e-04 eta 0:00:14
epoch [155/200] batch [15/42] time 0.493 (0.483) data 0.361 (0.352) loss_x loss_x 0.8262 (1.1945) acc_x 81.2500 (70.4167) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [20/42] time 0.420 (0.480) data 0.290 (0.349) loss_x loss_x 1.4463 (1.1902) acc_x 65.6250 (70.6250) lr 2.6037e-04 eta 0:00:10
epoch [155/200] batch [25/42] time 0.443 (0.483) data 0.313 (0.352) loss_x loss_x 1.2871 (1.1749) acc_x 68.7500 (71.0000) lr 2.6037e-04 eta 0:00:08
epoch [155/200] batch [30/42] time 0.449 (0.478) data 0.318 (0.347) loss_x loss_x 1.3320 (1.1849) acc_x 62.5000 (70.8333) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [35/42] time 0.445 (0.477) data 0.315 (0.346) loss_x loss_x 0.8452 (1.1717) acc_x 81.2500 (70.4464) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [40/42] time 0.365 (0.478) data 0.235 (0.347) loss_x loss_x 1.0752 (1.1614) acc_x 65.6250 (70.6250) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/55] time 0.378 (0.479) data 0.247 (0.348) loss_u loss_u 0.8169 (0.8241) acc_u 18.7500 (21.8750) lr 2.6037e-04 eta 0:00:23
epoch [155/200] batch [10/55] time 0.478 (0.479) data 0.347 (0.348) loss_u loss_u 0.8237 (0.8344) acc_u 21.8750 (20.6250) lr 2.6037e-04 eta 0:00:21
epoch [155/200] batch [15/55] time 0.455 (0.478) data 0.324 (0.347) loss_u loss_u 0.8589 (0.8363) acc_u 18.7500 (19.7917) lr 2.6037e-04 eta 0:00:19
epoch [155/200] batch [20/55] time 0.395 (0.475) data 0.265 (0.344) loss_u loss_u 0.8799 (0.8399) acc_u 18.7500 (19.3750) lr 2.6037e-04 eta 0:00:16
epoch [155/200] batch [25/55] time 0.391 (0.470) data 0.259 (0.339) loss_u loss_u 0.8306 (0.8371) acc_u 18.7500 (19.8750) lr 2.6037e-04 eta 0:00:14
epoch [155/200] batch [30/55] time 0.472 (0.472) data 0.340 (0.341) loss_u loss_u 0.7847 (0.8344) acc_u 21.8750 (20.2083) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [35/55] time 0.364 (0.471) data 0.232 (0.340) loss_u loss_u 0.7886 (0.8349) acc_u 25.0000 (20.4464) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [40/55] time 0.444 (0.469) data 0.314 (0.338) loss_u loss_u 0.8813 (0.8328) acc_u 18.7500 (20.9375) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [45/55] time 0.735 (0.473) data 0.603 (0.342) loss_u loss_u 0.7437 (0.8299) acc_u 37.5000 (21.5972) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [50/55] time 0.450 (0.470) data 0.319 (0.339) loss_u loss_u 0.7861 (0.8299) acc_u 31.2500 (21.6875) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [55/55] time 0.434 (0.471) data 0.302 (0.340) loss_u loss_u 0.8325 (0.8301) acc_u 21.8750 (21.5341) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1443
confident_label rate tensor(0.4286, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1344
clean true:1342
clean false:2
clean_rate:0.9985119047619048
noisy true:351
noisy false:1441
after delete: len(clean_dataset) 1344
after delete: len(noisy_dataset) 1792
epoch [156/200] batch [5/42] time 0.534 (0.471) data 0.402 (0.340) loss_x loss_x 1.3965 (1.4217) acc_x 65.6250 (69.3750) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [10/42] time 0.648 (0.473) data 0.514 (0.342) loss_x loss_x 1.7842 (1.4960) acc_x 65.6250 (67.1875) lr 2.4989e-04 eta 0:00:15
epoch [156/200] batch [15/42] time 0.471 (0.494) data 0.337 (0.362) loss_x loss_x 0.9829 (1.3417) acc_x 75.0000 (69.5833) lr 2.4989e-04 eta 0:00:13
epoch [156/200] batch [20/42] time 0.451 (0.508) data 0.320 (0.376) loss_x loss_x 0.8013 (1.2707) acc_x 78.1250 (70.4688) lr 2.4989e-04 eta 0:00:11
epoch [156/200] batch [25/42] time 0.481 (0.509) data 0.349 (0.377) loss_x loss_x 0.8672 (1.2719) acc_x 68.7500 (69.0000) lr 2.4989e-04 eta 0:00:08
epoch [156/200] batch [30/42] time 0.426 (0.503) data 0.295 (0.372) loss_x loss_x 1.0879 (1.2470) acc_x 65.6250 (69.3750) lr 2.4989e-04 eta 0:00:06
epoch [156/200] batch [35/42] time 0.459 (0.500) data 0.329 (0.368) loss_x loss_x 1.8730 (1.2371) acc_x 53.1250 (69.8214) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [40/42] time 0.421 (0.494) data 0.291 (0.363) loss_x loss_x 1.8086 (1.2275) acc_x 53.1250 (69.7656) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/56] time 0.541 (0.493) data 0.410 (0.362) loss_u loss_u 0.8433 (0.8259) acc_u 18.7500 (21.8750) lr 2.4989e-04 eta 0:00:25
epoch [156/200] batch [10/56] time 0.456 (0.490) data 0.325 (0.359) loss_u loss_u 0.8042 (0.8376) acc_u 21.8750 (20.9375) lr 2.4989e-04 eta 0:00:22
epoch [156/200] batch [15/56] time 0.330 (0.486) data 0.198 (0.354) loss_u loss_u 0.8442 (0.8411) acc_u 21.8750 (20.6250) lr 2.4989e-04 eta 0:00:19
epoch [156/200] batch [20/56] time 0.516 (0.483) data 0.385 (0.352) loss_u loss_u 0.9062 (0.8412) acc_u 12.5000 (20.4688) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [25/56] time 0.420 (0.481) data 0.287 (0.349) loss_u loss_u 0.7524 (0.8284) acc_u 28.1250 (22.0000) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [30/56] time 0.435 (0.486) data 0.302 (0.354) loss_u loss_u 0.7744 (0.8250) acc_u 34.3750 (22.6042) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [35/56] time 0.386 (0.487) data 0.254 (0.355) loss_u loss_u 0.8452 (0.8284) acc_u 18.7500 (21.9643) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [40/56] time 0.371 (0.485) data 0.239 (0.354) loss_u loss_u 0.8599 (0.8336) acc_u 15.6250 (20.9375) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [45/56] time 0.466 (0.480) data 0.335 (0.349) loss_u loss_u 0.7827 (0.8338) acc_u 28.1250 (21.3889) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [50/56] time 0.411 (0.477) data 0.280 (0.345) loss_u loss_u 0.8003 (0.8304) acc_u 25.0000 (21.8750) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [55/56] time 0.379 (0.476) data 0.248 (0.345) loss_u loss_u 0.7686 (0.8260) acc_u 31.2500 (22.3864) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1465
confident_label rate tensor(0.4251, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1333
clean true:1331
clean false:2
clean_rate:0.9984996249062266
noisy true:340
noisy false:1463
after delete: len(clean_dataset) 1333
after delete: len(noisy_dataset) 1803
epoch [157/200] batch [5/41] time 0.624 (0.498) data 0.492 (0.367) loss_x loss_x 1.1514 (1.1994) acc_x 78.1250 (73.1250) lr 2.3959e-04 eta 0:00:17
epoch [157/200] batch [10/41] time 0.427 (0.484) data 0.296 (0.354) loss_x loss_x 1.3418 (1.2327) acc_x 75.0000 (72.5000) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [15/41] time 0.331 (0.470) data 0.201 (0.340) loss_x loss_x 1.1426 (1.1613) acc_x 62.5000 (73.1250) lr 2.3959e-04 eta 0:00:12
epoch [157/200] batch [20/41] time 0.553 (0.463) data 0.422 (0.333) loss_x loss_x 1.0137 (1.2066) acc_x 75.0000 (72.6562) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [25/41] time 0.461 (0.460) data 0.332 (0.329) loss_x loss_x 1.0498 (1.1881) acc_x 75.0000 (72.2500) lr 2.3959e-04 eta 0:00:07
epoch [157/200] batch [30/41] time 0.431 (0.465) data 0.301 (0.334) loss_x loss_x 1.0361 (1.1844) acc_x 87.5000 (72.8125) lr 2.3959e-04 eta 0:00:05
epoch [157/200] batch [35/41] time 0.405 (0.466) data 0.275 (0.335) loss_x loss_x 1.2695 (1.1670) acc_x 65.6250 (73.3036) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [40/41] time 0.487 (0.467) data 0.357 (0.336) loss_x loss_x 1.4131 (1.1649) acc_x 59.3750 (72.8906) lr 2.3959e-04 eta 0:00:00
epoch [157/200] batch [5/56] time 0.628 (0.479) data 0.497 (0.349) loss_u loss_u 0.9077 (0.8454) acc_u 12.5000 (21.8750) lr 2.3959e-04 eta 0:00:24
epoch [157/200] batch [10/56] time 0.381 (0.474) data 0.248 (0.343) loss_u loss_u 0.8687 (0.8518) acc_u 15.6250 (20.3125) lr 2.3959e-04 eta 0:00:21
epoch [157/200] batch [15/56] time 0.389 (0.472) data 0.257 (0.341) loss_u loss_u 0.8271 (0.8383) acc_u 25.0000 (21.8750) lr 2.3959e-04 eta 0:00:19
epoch [157/200] batch [20/56] time 0.421 (0.477) data 0.289 (0.346) loss_u loss_u 0.9258 (0.8434) acc_u 6.2500 (20.6250) lr 2.3959e-04 eta 0:00:17
epoch [157/200] batch [25/56] time 0.537 (0.478) data 0.404 (0.348) loss_u loss_u 0.8760 (0.8452) acc_u 12.5000 (20.1250) lr 2.3959e-04 eta 0:00:14
epoch [157/200] batch [30/56] time 0.465 (0.477) data 0.334 (0.346) loss_u loss_u 0.8374 (0.8443) acc_u 18.7500 (20.1042) lr 2.3959e-04 eta 0:00:12
epoch [157/200] batch [35/56] time 0.406 (0.475) data 0.274 (0.344) loss_u loss_u 0.8169 (0.8408) acc_u 18.7500 (20.8036) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [40/56] time 0.322 (0.469) data 0.190 (0.338) loss_u loss_u 0.8916 (0.8374) acc_u 12.5000 (21.1719) lr 2.3959e-04 eta 0:00:07
epoch [157/200] batch [45/56] time 0.490 (0.471) data 0.360 (0.340) loss_u loss_u 0.8032 (0.8357) acc_u 18.7500 (21.2500) lr 2.3959e-04 eta 0:00:05
epoch [157/200] batch [50/56] time 0.367 (0.469) data 0.234 (0.337) loss_u loss_u 0.8027 (0.8344) acc_u 28.1250 (21.3125) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [55/56] time 0.439 (0.470) data 0.307 (0.339) loss_u loss_u 0.7817 (0.8313) acc_u 25.0000 (21.8182) lr 2.3959e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1460
confident_label rate tensor(0.4244, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1331
clean true:1329
clean false:2
clean_rate:0.9984973703981969
noisy true:347
noisy false:1458
after delete: len(clean_dataset) 1331
after delete: len(noisy_dataset) 1805
epoch [158/200] batch [5/41] time 0.581 (0.504) data 0.450 (0.374) loss_x loss_x 1.3789 (1.2274) acc_x 71.8750 (74.3750) lr 2.2949e-04 eta 0:00:18
epoch [158/200] batch [10/41] time 0.536 (0.524) data 0.404 (0.393) loss_x loss_x 1.0713 (1.1127) acc_x 65.6250 (74.3750) lr 2.2949e-04 eta 0:00:16
epoch [158/200] batch [15/41] time 0.540 (0.499) data 0.410 (0.368) loss_x loss_x 1.3477 (1.1521) acc_x 62.5000 (72.7083) lr 2.2949e-04 eta 0:00:12
epoch [158/200] batch [20/41] time 0.440 (0.485) data 0.310 (0.355) loss_x loss_x 0.8940 (1.1445) acc_x 87.5000 (73.2812) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [25/41] time 0.542 (0.488) data 0.411 (0.357) loss_x loss_x 1.2559 (1.1419) acc_x 65.6250 (72.7500) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [30/41] time 0.563 (0.495) data 0.432 (0.364) loss_x loss_x 1.4561 (1.1571) acc_x 62.5000 (71.9792) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [35/41] time 0.505 (0.491) data 0.374 (0.360) loss_x loss_x 0.8760 (1.1722) acc_x 78.1250 (71.7857) lr 2.2949e-04 eta 0:00:02
epoch [158/200] batch [40/41] time 0.442 (0.490) data 0.312 (0.359) loss_x loss_x 0.7886 (1.1469) acc_x 81.2500 (71.4062) lr 2.2949e-04 eta 0:00:00
epoch [158/200] batch [5/56] time 0.394 (0.482) data 0.263 (0.351) loss_u loss_u 0.9082 (0.8109) acc_u 9.3750 (24.3750) lr 2.2949e-04 eta 0:00:24
epoch [158/200] batch [10/56] time 0.612 (0.484) data 0.480 (0.353) loss_u loss_u 0.8574 (0.8176) acc_u 21.8750 (24.0625) lr 2.2949e-04 eta 0:00:22
epoch [158/200] batch [15/56] time 0.407 (0.481) data 0.275 (0.350) loss_u loss_u 0.8545 (0.8302) acc_u 15.6250 (22.2917) lr 2.2949e-04 eta 0:00:19
epoch [158/200] batch [20/56] time 0.447 (0.482) data 0.315 (0.351) loss_u loss_u 0.8838 (0.8410) acc_u 9.3750 (20.4688) lr 2.2949e-04 eta 0:00:17
epoch [158/200] batch [25/56] time 0.408 (0.481) data 0.276 (0.350) loss_u loss_u 0.8218 (0.8390) acc_u 21.8750 (20.6250) lr 2.2949e-04 eta 0:00:14
epoch [158/200] batch [30/56] time 0.573 (0.483) data 0.441 (0.352) loss_u loss_u 0.7881 (0.8293) acc_u 25.0000 (21.5625) lr 2.2949e-04 eta 0:00:12
epoch [158/200] batch [35/56] time 0.379 (0.481) data 0.247 (0.350) loss_u loss_u 0.6597 (0.8230) acc_u 43.7500 (22.6786) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [40/56] time 0.469 (0.479) data 0.337 (0.348) loss_u loss_u 0.8848 (0.8223) acc_u 15.6250 (22.6562) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [45/56] time 0.359 (0.477) data 0.228 (0.346) loss_u loss_u 0.8853 (0.8246) acc_u 12.5000 (22.1528) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [50/56] time 0.385 (0.474) data 0.253 (0.343) loss_u loss_u 0.8545 (0.8290) acc_u 15.6250 (21.3750) lr 2.2949e-04 eta 0:00:02
epoch [158/200] batch [55/56] time 0.458 (0.471) data 0.328 (0.340) loss_u loss_u 0.7183 (0.8270) acc_u 37.5000 (21.3636) lr 2.2949e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1435
confident_label rate tensor(0.4314, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1353
clean true:1352
clean false:1
clean_rate:0.9992609016999261
noisy true:349
noisy false:1434
after delete: len(clean_dataset) 1353
after delete: len(noisy_dataset) 1783
epoch [159/200] batch [5/42] time 0.394 (0.450) data 0.263 (0.319) loss_x loss_x 1.3916 (1.1078) acc_x 62.5000 (72.5000) lr 2.1957e-04 eta 0:00:16
epoch [159/200] batch [10/42] time 0.502 (0.464) data 0.371 (0.333) loss_x loss_x 0.9375 (1.1746) acc_x 78.1250 (71.5625) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [15/42] time 0.500 (0.471) data 0.369 (0.340) loss_x loss_x 1.6924 (1.1412) acc_x 59.3750 (72.2917) lr 2.1957e-04 eta 0:00:12
epoch [159/200] batch [20/42] time 0.472 (0.463) data 0.342 (0.332) loss_x loss_x 0.8906 (1.1104) acc_x 78.1250 (72.5000) lr 2.1957e-04 eta 0:00:10
epoch [159/200] batch [25/42] time 0.539 (0.472) data 0.408 (0.341) loss_x loss_x 1.3457 (1.1416) acc_x 68.7500 (71.3750) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [30/42] time 0.498 (0.477) data 0.367 (0.346) loss_x loss_x 1.7041 (1.1429) acc_x 59.3750 (71.3542) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [35/42] time 0.493 (0.483) data 0.363 (0.352) loss_x loss_x 1.5430 (1.1804) acc_x 68.7500 (71.1607) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [40/42] time 0.399 (0.476) data 0.268 (0.345) loss_x loss_x 1.0596 (1.1458) acc_x 71.8750 (71.9531) lr 2.1957e-04 eta 0:00:00
epoch [159/200] batch [5/55] time 0.521 (0.483) data 0.389 (0.352) loss_u loss_u 0.8364 (0.8159) acc_u 15.6250 (21.2500) lr 2.1957e-04 eta 0:00:24
epoch [159/200] batch [10/55] time 0.483 (0.482) data 0.353 (0.351) loss_u loss_u 0.8936 (0.8213) acc_u 9.3750 (20.9375) lr 2.1957e-04 eta 0:00:21
epoch [159/200] batch [15/55] time 0.444 (0.482) data 0.312 (0.350) loss_u loss_u 0.8364 (0.8178) acc_u 21.8750 (21.6667) lr 2.1957e-04 eta 0:00:19
epoch [159/200] batch [20/55] time 0.420 (0.483) data 0.290 (0.351) loss_u loss_u 0.7676 (0.8163) acc_u 25.0000 (21.7188) lr 2.1957e-04 eta 0:00:16
epoch [159/200] batch [25/55] time 0.439 (0.487) data 0.308 (0.356) loss_u loss_u 0.8359 (0.8215) acc_u 18.7500 (21.2500) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [30/55] time 0.530 (0.486) data 0.398 (0.355) loss_u loss_u 0.8633 (0.8252) acc_u 15.6250 (20.9375) lr 2.1957e-04 eta 0:00:12
epoch [159/200] batch [35/55] time 0.349 (0.483) data 0.217 (0.352) loss_u loss_u 0.8286 (0.8251) acc_u 21.8750 (20.8929) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [40/55] time 0.378 (0.478) data 0.247 (0.347) loss_u loss_u 0.8052 (0.8263) acc_u 21.8750 (21.0938) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [45/55] time 0.558 (0.479) data 0.428 (0.348) loss_u loss_u 0.8433 (0.8290) acc_u 18.7500 (20.9028) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [50/55] time 0.422 (0.477) data 0.292 (0.346) loss_u loss_u 0.8540 (0.8271) acc_u 18.7500 (21.4375) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [55/55] time 0.505 (0.476) data 0.374 (0.345) loss_u loss_u 0.8735 (0.8278) acc_u 15.6250 (21.5341) lr 2.1957e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1456
confident_label rate tensor(0.4235, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1328
clean true:1327
clean false:1
clean_rate:0.9992469879518072
noisy true:353
noisy false:1455
after delete: len(clean_dataset) 1328
after delete: len(noisy_dataset) 1808
epoch [160/200] batch [5/41] time 0.544 (0.573) data 0.413 (0.443) loss_x loss_x 1.3584 (1.2917) acc_x 71.8750 (70.0000) lr 2.0984e-04 eta 0:00:20
epoch [160/200] batch [10/41] time 0.413 (0.533) data 0.283 (0.403) loss_x loss_x 1.2705 (1.3330) acc_x 68.7500 (68.1250) lr 2.0984e-04 eta 0:00:16
epoch [160/200] batch [15/41] time 0.395 (0.508) data 0.265 (0.378) loss_x loss_x 1.2666 (1.2281) acc_x 68.7500 (70.2083) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [20/41] time 0.470 (0.495) data 0.339 (0.365) loss_x loss_x 1.5420 (1.1791) acc_x 56.2500 (70.0000) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [25/41] time 0.424 (0.485) data 0.294 (0.354) loss_x loss_x 1.0176 (1.1398) acc_x 75.0000 (71.2500) lr 2.0984e-04 eta 0:00:07
epoch [160/200] batch [30/41] time 0.535 (0.478) data 0.404 (0.348) loss_x loss_x 1.1387 (1.1117) acc_x 71.8750 (71.6667) lr 2.0984e-04 eta 0:00:05
epoch [160/200] batch [35/41] time 0.477 (0.468) data 0.345 (0.337) loss_x loss_x 1.0879 (1.0969) acc_x 65.6250 (71.0714) lr 2.0984e-04 eta 0:00:02
epoch [160/200] batch [40/41] time 0.491 (0.468) data 0.360 (0.337) loss_x loss_x 1.2021 (1.1095) acc_x 71.8750 (70.9375) lr 2.0984e-04 eta 0:00:00
epoch [160/200] batch [5/56] time 0.616 (0.489) data 0.483 (0.358) loss_u loss_u 0.7510 (0.8243) acc_u 37.5000 (22.5000) lr 2.0984e-04 eta 0:00:24
epoch [160/200] batch [10/56] time 0.468 (0.489) data 0.336 (0.358) loss_u loss_u 0.9150 (0.8208) acc_u 9.3750 (23.1250) lr 2.0984e-04 eta 0:00:22
epoch [160/200] batch [15/56] time 0.507 (0.484) data 0.375 (0.353) loss_u loss_u 0.8481 (0.8216) acc_u 25.0000 (24.1667) lr 2.0984e-04 eta 0:00:19
epoch [160/200] batch [20/56] time 0.453 (0.483) data 0.322 (0.352) loss_u loss_u 0.8848 (0.8243) acc_u 9.3750 (23.4375) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [25/56] time 0.476 (0.480) data 0.344 (0.349) loss_u loss_u 0.7661 (0.8206) acc_u 31.2500 (24.0000) lr 2.0984e-04 eta 0:00:14
epoch [160/200] batch [30/56] time 0.476 (0.478) data 0.345 (0.347) loss_u loss_u 0.9209 (0.8299) acc_u 15.6250 (22.9167) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [35/56] time 0.539 (0.474) data 0.408 (0.343) loss_u loss_u 0.7788 (0.8294) acc_u 28.1250 (23.1250) lr 2.0984e-04 eta 0:00:09
epoch [160/200] batch [40/56] time 0.384 (0.474) data 0.253 (0.343) loss_u loss_u 0.7935 (0.8292) acc_u 21.8750 (22.8125) lr 2.0984e-04 eta 0:00:07
epoch [160/200] batch [45/56] time 0.479 (0.472) data 0.349 (0.341) loss_u loss_u 0.8521 (0.8320) acc_u 18.7500 (22.2917) lr 2.0984e-04 eta 0:00:05
epoch [160/200] batch [50/56] time 0.345 (0.474) data 0.215 (0.343) loss_u loss_u 0.8486 (0.8326) acc_u 18.7500 (22.1250) lr 2.0984e-04 eta 0:00:02
epoch [160/200] batch [55/56] time 0.426 (0.473) data 0.296 (0.342) loss_u loss_u 0.7783 (0.8299) acc_u 25.0000 (22.1023) lr 2.0984e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.4334, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1359
clean true:1355
clean false:4
clean_rate:0.9970566593083149
noisy true:361
noisy false:1416
after delete: len(clean_dataset) 1359
after delete: len(noisy_dataset) 1777
epoch [161/200] batch [5/42] time 0.447 (0.485) data 0.317 (0.354) loss_x loss_x 1.3896 (1.3733) acc_x 68.7500 (70.6250) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [10/42] time 0.386 (0.474) data 0.256 (0.343) loss_x loss_x 0.7603 (1.1752) acc_x 81.2500 (72.8125) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [15/42] time 0.520 (0.483) data 0.388 (0.352) loss_x loss_x 1.2139 (1.1249) acc_x 71.8750 (72.7083) lr 2.0032e-04 eta 0:00:13
epoch [161/200] batch [20/42] time 0.531 (0.486) data 0.401 (0.355) loss_x loss_x 1.3027 (1.1561) acc_x 68.7500 (72.1875) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [25/42] time 0.323 (0.472) data 0.192 (0.341) loss_x loss_x 1.5518 (1.1734) acc_x 65.6250 (71.2500) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [30/42] time 0.466 (0.471) data 0.335 (0.340) loss_x loss_x 1.1475 (1.1964) acc_x 71.8750 (70.4167) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [35/42] time 0.418 (0.465) data 0.287 (0.335) loss_x loss_x 1.1748 (1.1959) acc_x 65.6250 (70.0000) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [40/42] time 0.456 (0.466) data 0.325 (0.335) loss_x loss_x 1.0762 (1.2049) acc_x 71.8750 (69.4531) lr 2.0032e-04 eta 0:00:00
epoch [161/200] batch [5/55] time 0.406 (0.461) data 0.275 (0.330) loss_u loss_u 0.7700 (0.8077) acc_u 28.1250 (23.7500) lr 2.0032e-04 eta 0:00:23
epoch [161/200] batch [10/55] time 0.385 (0.459) data 0.254 (0.328) loss_u loss_u 0.9199 (0.8288) acc_u 6.2500 (20.9375) lr 2.0032e-04 eta 0:00:20
epoch [161/200] batch [15/55] time 0.466 (0.459) data 0.335 (0.328) loss_u loss_u 0.8921 (0.8335) acc_u 12.5000 (20.4167) lr 2.0032e-04 eta 0:00:18
epoch [161/200] batch [20/55] time 0.493 (0.460) data 0.362 (0.329) loss_u loss_u 0.8101 (0.8300) acc_u 25.0000 (21.2500) lr 2.0032e-04 eta 0:00:16
epoch [161/200] batch [25/55] time 0.396 (0.467) data 0.265 (0.336) loss_u loss_u 0.8408 (0.8355) acc_u 21.8750 (20.7500) lr 2.0032e-04 eta 0:00:14
epoch [161/200] batch [30/55] time 0.442 (0.466) data 0.311 (0.335) loss_u loss_u 0.8892 (0.8328) acc_u 9.3750 (21.4583) lr 2.0032e-04 eta 0:00:11
epoch [161/200] batch [35/55] time 0.439 (0.461) data 0.308 (0.330) loss_u loss_u 0.8428 (0.8301) acc_u 18.7500 (21.6964) lr 2.0032e-04 eta 0:00:09
epoch [161/200] batch [40/55] time 0.368 (0.459) data 0.238 (0.328) loss_u loss_u 0.8809 (0.8320) acc_u 12.5000 (21.2500) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [45/55] time 0.591 (0.459) data 0.459 (0.328) loss_u loss_u 0.7432 (0.8265) acc_u 34.3750 (22.0833) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [50/55] time 0.446 (0.455) data 0.314 (0.324) loss_u loss_u 0.7422 (0.8266) acc_u 37.5000 (22.3125) lr 2.0032e-04 eta 0:00:02
epoch [161/200] batch [55/55] time 0.535 (0.454) data 0.405 (0.323) loss_u loss_u 0.8950 (0.8321) acc_u 15.6250 (21.6477) lr 2.0032e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1438
confident_label rate tensor(0.4247, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1332
clean true:1332
clean false:0
clean_rate:1.0
noisy true:366
noisy false:1438
after delete: len(clean_dataset) 1332
after delete: len(noisy_dataset) 1804
epoch [162/200] batch [5/41] time 0.392 (0.508) data 0.261 (0.377) loss_x loss_x 0.8179 (1.0887) acc_x 78.1250 (70.0000) lr 1.9098e-04 eta 0:00:18
epoch [162/200] batch [10/41] time 0.527 (0.489) data 0.396 (0.358) loss_x loss_x 1.2070 (1.1412) acc_x 78.1250 (71.2500) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [15/41] time 0.512 (0.479) data 0.381 (0.349) loss_x loss_x 0.6387 (1.0906) acc_x 84.3750 (72.5000) lr 1.9098e-04 eta 0:00:12
epoch [162/200] batch [20/41] time 0.408 (0.480) data 0.278 (0.349) loss_x loss_x 1.6318 (1.1382) acc_x 50.0000 (70.7812) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [25/41] time 0.527 (0.473) data 0.395 (0.342) loss_x loss_x 1.6406 (1.1927) acc_x 65.6250 (69.6250) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [30/41] time 0.395 (0.472) data 0.264 (0.341) loss_x loss_x 1.2324 (1.1910) acc_x 65.6250 (69.4792) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [35/41] time 0.491 (0.471) data 0.361 (0.340) loss_x loss_x 1.5771 (1.1919) acc_x 65.6250 (69.6429) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [40/41] time 0.545 (0.467) data 0.415 (0.336) loss_x loss_x 0.9458 (1.1717) acc_x 68.7500 (70.0781) lr 1.9098e-04 eta 0:00:00
epoch [162/200] batch [5/56] time 0.404 (0.464) data 0.272 (0.333) loss_u loss_u 0.8813 (0.8404) acc_u 15.6250 (21.2500) lr 1.9098e-04 eta 0:00:23
epoch [162/200] batch [10/56] time 0.522 (0.462) data 0.390 (0.331) loss_u loss_u 0.8130 (0.8478) acc_u 21.8750 (20.6250) lr 1.9098e-04 eta 0:00:21
epoch [162/200] batch [15/56] time 0.543 (0.463) data 0.412 (0.332) loss_u loss_u 0.8750 (0.8553) acc_u 12.5000 (18.9583) lr 1.9098e-04 eta 0:00:18
epoch [162/200] batch [20/56] time 0.564 (0.463) data 0.434 (0.332) loss_u loss_u 0.8330 (0.8546) acc_u 21.8750 (19.5312) lr 1.9098e-04 eta 0:00:16
epoch [162/200] batch [25/56] time 0.457 (0.460) data 0.326 (0.329) loss_u loss_u 0.7925 (0.8475) acc_u 21.8750 (20.0000) lr 1.9098e-04 eta 0:00:14
epoch [162/200] batch [30/56] time 0.372 (0.458) data 0.242 (0.327) loss_u loss_u 0.7798 (0.8444) acc_u 31.2500 (20.4167) lr 1.9098e-04 eta 0:00:11
epoch [162/200] batch [35/56] time 0.437 (0.460) data 0.306 (0.329) loss_u loss_u 0.8726 (0.8416) acc_u 18.7500 (20.9821) lr 1.9098e-04 eta 0:00:09
epoch [162/200] batch [40/56] time 0.392 (0.458) data 0.260 (0.327) loss_u loss_u 0.7783 (0.8364) acc_u 25.0000 (21.5625) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [45/56] time 0.402 (0.456) data 0.270 (0.325) loss_u loss_u 0.8809 (0.8376) acc_u 18.7500 (21.6667) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [50/56] time 0.379 (0.455) data 0.248 (0.324) loss_u loss_u 0.7700 (0.8314) acc_u 28.1250 (22.3750) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [55/56] time 0.655 (0.459) data 0.524 (0.327) loss_u loss_u 0.8628 (0.8312) acc_u 18.7500 (22.3864) lr 1.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1377
confident_label rate tensor(0.4413, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1384
clean true:1380
clean false:4
clean_rate:0.9971098265895953
noisy true:379
noisy false:1373
after delete: len(clean_dataset) 1384
after delete: len(noisy_dataset) 1752
epoch [163/200] batch [5/43] time 0.499 (0.522) data 0.368 (0.391) loss_x loss_x 1.4844 (1.2854) acc_x 71.8750 (66.2500) lr 1.8185e-04 eta 0:00:19
epoch [163/200] batch [10/43] time 0.448 (0.481) data 0.318 (0.350) loss_x loss_x 1.4268 (1.1996) acc_x 56.2500 (69.0625) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [15/43] time 0.413 (0.459) data 0.283 (0.329) loss_x loss_x 1.4775 (1.2906) acc_x 68.7500 (68.1250) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [20/43] time 0.482 (0.455) data 0.352 (0.325) loss_x loss_x 0.8232 (1.2401) acc_x 84.3750 (68.2812) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [25/43] time 0.452 (0.455) data 0.322 (0.325) loss_x loss_x 1.1006 (1.2182) acc_x 71.8750 (69.5000) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [30/43] time 0.416 (0.451) data 0.285 (0.321) loss_x loss_x 0.6230 (1.1702) acc_x 84.3750 (70.4167) lr 1.8185e-04 eta 0:00:05
epoch [163/200] batch [35/43] time 0.500 (0.453) data 0.369 (0.323) loss_x loss_x 0.9790 (1.1690) acc_x 81.2500 (70.7143) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [40/43] time 0.437 (0.459) data 0.306 (0.328) loss_x loss_x 0.7510 (1.1372) acc_x 81.2500 (71.2500) lr 1.8185e-04 eta 0:00:01
epoch [163/200] batch [5/54] time 0.475 (0.464) data 0.344 (0.334) loss_u loss_u 0.7930 (0.8319) acc_u 25.0000 (21.8750) lr 1.8185e-04 eta 0:00:22
epoch [163/200] batch [10/54] time 0.495 (0.464) data 0.363 (0.333) loss_u loss_u 0.8584 (0.8300) acc_u 15.6250 (22.1875) lr 1.8185e-04 eta 0:00:20
epoch [163/200] batch [15/54] time 0.525 (0.464) data 0.395 (0.333) loss_u loss_u 0.7729 (0.8188) acc_u 28.1250 (23.5417) lr 1.8185e-04 eta 0:00:18
epoch [163/200] batch [20/54] time 0.389 (0.461) data 0.257 (0.331) loss_u loss_u 0.8037 (0.8066) acc_u 21.8750 (24.6875) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [25/54] time 0.751 (0.471) data 0.619 (0.340) loss_u loss_u 0.7739 (0.8174) acc_u 28.1250 (23.1250) lr 1.8185e-04 eta 0:00:13
epoch [163/200] batch [30/54] time 0.391 (0.468) data 0.261 (0.337) loss_u loss_u 0.8535 (0.8234) acc_u 15.6250 (22.7083) lr 1.8185e-04 eta 0:00:11
epoch [163/200] batch [35/54] time 0.414 (0.467) data 0.282 (0.336) loss_u loss_u 0.7930 (0.8275) acc_u 21.8750 (22.1429) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [40/54] time 0.492 (0.467) data 0.360 (0.336) loss_u loss_u 0.8125 (0.8288) acc_u 21.8750 (22.2656) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [45/54] time 0.351 (0.469) data 0.219 (0.338) loss_u loss_u 0.8618 (0.8318) acc_u 15.6250 (21.7361) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [50/54] time 0.489 (0.471) data 0.358 (0.340) loss_u loss_u 0.7651 (0.8265) acc_u 28.1250 (22.2500) lr 1.8185e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1397
confident_label rate tensor(0.4385, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1375
clean true:1373
clean false:2
clean_rate:0.9985454545454545
noisy true:366
noisy false:1395
after delete: len(clean_dataset) 1375
after delete: len(noisy_dataset) 1761
epoch [164/200] batch [5/42] time 0.381 (0.456) data 0.251 (0.326) loss_x loss_x 1.0342 (1.2598) acc_x 81.2500 (71.8750) lr 1.7292e-04 eta 0:00:16
epoch [164/200] batch [10/42] time 0.401 (0.472) data 0.269 (0.340) loss_x loss_x 0.8032 (1.1247) acc_x 75.0000 (73.1250) lr 1.7292e-04 eta 0:00:15
epoch [164/200] batch [15/42] time 0.550 (0.470) data 0.418 (0.339) loss_x loss_x 1.2559 (1.1222) acc_x 71.8750 (73.7500) lr 1.7292e-04 eta 0:00:12
epoch [164/200] batch [20/42] time 0.435 (0.481) data 0.305 (0.350) loss_x loss_x 0.8940 (1.1574) acc_x 84.3750 (71.7188) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [25/42] time 0.436 (0.467) data 0.304 (0.336) loss_x loss_x 0.6987 (1.1526) acc_x 75.0000 (71.2500) lr 1.7292e-04 eta 0:00:07
epoch [164/200] batch [30/42] time 0.587 (0.477) data 0.457 (0.346) loss_x loss_x 1.1377 (1.1286) acc_x 68.7500 (71.7708) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [35/42] time 0.443 (0.477) data 0.313 (0.346) loss_x loss_x 0.7041 (1.1322) acc_x 81.2500 (71.8750) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [40/42] time 0.446 (0.472) data 0.316 (0.341) loss_x loss_x 0.9321 (1.1279) acc_x 81.2500 (72.2656) lr 1.7292e-04 eta 0:00:00
epoch [164/200] batch [5/55] time 0.489 (0.480) data 0.359 (0.349) loss_u loss_u 0.8530 (0.8205) acc_u 18.7500 (22.5000) lr 1.7292e-04 eta 0:00:23
epoch [164/200] batch [10/55] time 0.421 (0.469) data 0.290 (0.338) loss_u loss_u 0.8394 (0.8344) acc_u 15.6250 (20.9375) lr 1.7292e-04 eta 0:00:21
epoch [164/200] batch [15/55] time 0.391 (0.469) data 0.260 (0.338) loss_u loss_u 0.7900 (0.8388) acc_u 25.0000 (20.6250) lr 1.7292e-04 eta 0:00:18
epoch [164/200] batch [20/55] time 0.423 (0.469) data 0.291 (0.338) loss_u loss_u 0.8735 (0.8385) acc_u 15.6250 (20.4688) lr 1.7292e-04 eta 0:00:16
epoch [164/200] batch [25/55] time 0.455 (0.469) data 0.324 (0.338) loss_u loss_u 0.8247 (0.8409) acc_u 21.8750 (20.5000) lr 1.7292e-04 eta 0:00:14
epoch [164/200] batch [30/55] time 0.467 (0.468) data 0.337 (0.337) loss_u loss_u 0.8823 (0.8353) acc_u 15.6250 (21.7708) lr 1.7292e-04 eta 0:00:11
epoch [164/200] batch [35/55] time 0.381 (0.469) data 0.249 (0.338) loss_u loss_u 0.9014 (0.8358) acc_u 6.2500 (21.3393) lr 1.7292e-04 eta 0:00:09
epoch [164/200] batch [40/55] time 0.357 (0.470) data 0.226 (0.339) loss_u loss_u 0.7778 (0.8357) acc_u 31.2500 (21.0156) lr 1.7292e-04 eta 0:00:07
epoch [164/200] batch [45/55] time 0.590 (0.470) data 0.459 (0.339) loss_u loss_u 0.8188 (0.8340) acc_u 21.8750 (21.3889) lr 1.7292e-04 eta 0:00:04
epoch [164/200] batch [50/55] time 0.389 (0.472) data 0.259 (0.341) loss_u loss_u 0.8452 (0.8327) acc_u 21.8750 (21.4375) lr 1.7292e-04 eta 0:00:02
epoch [164/200] batch [55/55] time 0.506 (0.470) data 0.377 (0.339) loss_u loss_u 0.6914 (0.8304) acc_u 43.7500 (21.8750) lr 1.7292e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.4292, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1346
clean true:1342
clean false:4
clean_rate:0.9970282317979198
noisy true:360
noisy false:1430
after delete: len(clean_dataset) 1346
after delete: len(noisy_dataset) 1790
epoch [165/200] batch [5/42] time 0.475 (0.452) data 0.345 (0.322) loss_x loss_x 1.1514 (1.2146) acc_x 71.8750 (69.3750) lr 1.6419e-04 eta 0:00:16
epoch [165/200] batch [10/42] time 0.492 (0.453) data 0.362 (0.322) loss_x loss_x 1.3643 (1.1884) acc_x 68.7500 (72.1875) lr 1.6419e-04 eta 0:00:14
epoch [165/200] batch [15/42] time 0.576 (0.468) data 0.446 (0.337) loss_x loss_x 1.1465 (1.1702) acc_x 75.0000 (71.8750) lr 1.6419e-04 eta 0:00:12
epoch [165/200] batch [20/42] time 0.506 (0.466) data 0.375 (0.335) loss_x loss_x 0.8965 (1.2059) acc_x 71.8750 (71.2500) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [25/42] time 0.514 (0.466) data 0.384 (0.335) loss_x loss_x 0.8945 (1.1380) acc_x 78.1250 (72.3750) lr 1.6419e-04 eta 0:00:07
epoch [165/200] batch [30/42] time 0.416 (0.459) data 0.286 (0.328) loss_x loss_x 0.9160 (1.1539) acc_x 78.1250 (71.9792) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [35/42] time 0.308 (0.462) data 0.177 (0.331) loss_x loss_x 1.2969 (1.1687) acc_x 68.7500 (71.8750) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [40/42] time 0.470 (0.461) data 0.339 (0.330) loss_x loss_x 1.1543 (1.1588) acc_x 78.1250 (72.1094) lr 1.6419e-04 eta 0:00:00
epoch [165/200] batch [5/55] time 0.405 (0.463) data 0.275 (0.332) loss_u loss_u 0.9150 (0.8166) acc_u 12.5000 (23.7500) lr 1.6419e-04 eta 0:00:23
epoch [165/200] batch [10/55] time 0.817 (0.468) data 0.686 (0.337) loss_u loss_u 0.8281 (0.8239) acc_u 15.6250 (21.2500) lr 1.6419e-04 eta 0:00:21
epoch [165/200] batch [15/55] time 0.368 (0.471) data 0.237 (0.341) loss_u loss_u 0.8218 (0.8246) acc_u 15.6250 (21.2500) lr 1.6419e-04 eta 0:00:18
epoch [165/200] batch [20/55] time 0.428 (0.469) data 0.297 (0.338) loss_u loss_u 0.8799 (0.8293) acc_u 18.7500 (20.7812) lr 1.6419e-04 eta 0:00:16
epoch [165/200] batch [25/55] time 0.389 (0.465) data 0.257 (0.334) loss_u loss_u 0.8970 (0.8317) acc_u 12.5000 (20.8750) lr 1.6419e-04 eta 0:00:13
epoch [165/200] batch [30/55] time 0.404 (0.461) data 0.274 (0.330) loss_u loss_u 0.9150 (0.8349) acc_u 6.2500 (20.6250) lr 1.6419e-04 eta 0:00:11
epoch [165/200] batch [35/55] time 0.512 (0.465) data 0.381 (0.334) loss_u loss_u 0.7817 (0.8308) acc_u 21.8750 (20.8929) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [40/55] time 0.469 (0.462) data 0.339 (0.331) loss_u loss_u 0.8247 (0.8326) acc_u 25.0000 (21.0156) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [45/55] time 0.348 (0.462) data 0.217 (0.331) loss_u loss_u 0.8335 (0.8296) acc_u 21.8750 (21.5278) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [50/55] time 0.421 (0.463) data 0.289 (0.332) loss_u loss_u 0.9160 (0.8286) acc_u 12.5000 (21.7500) lr 1.6419e-04 eta 0:00:02
epoch [165/200] batch [55/55] time 0.378 (0.461) data 0.248 (0.330) loss_u loss_u 0.8848 (0.8307) acc_u 15.6250 (21.7614) lr 1.6419e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1416
confident_label rate tensor(0.4330, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1358
clean true:1356
clean false:2
clean_rate:0.9985272459499264
noisy true:364
noisy false:1414
after delete: len(clean_dataset) 1358
after delete: len(noisy_dataset) 1778
epoch [166/200] batch [5/42] time 0.335 (0.472) data 0.203 (0.341) loss_x loss_x 1.3125 (1.1900) acc_x 65.6250 (70.0000) lr 1.5567e-04 eta 0:00:17
epoch [166/200] batch [10/42] time 0.389 (0.459) data 0.259 (0.328) loss_x loss_x 1.1318 (1.2063) acc_x 78.1250 (70.0000) lr 1.5567e-04 eta 0:00:14
epoch [166/200] batch [15/42] time 0.419 (0.468) data 0.288 (0.338) loss_x loss_x 1.4893 (1.1951) acc_x 62.5000 (69.7917) lr 1.5567e-04 eta 0:00:12
epoch [166/200] batch [20/42] time 0.451 (0.455) data 0.320 (0.325) loss_x loss_x 0.8252 (1.1858) acc_x 84.3750 (70.9375) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [25/42] time 0.537 (0.459) data 0.406 (0.328) loss_x loss_x 0.9170 (1.1734) acc_x 71.8750 (70.8750) lr 1.5567e-04 eta 0:00:07
epoch [166/200] batch [30/42] time 0.394 (0.459) data 0.264 (0.328) loss_x loss_x 0.9536 (1.1679) acc_x 75.0000 (71.5625) lr 1.5567e-04 eta 0:00:05
epoch [166/200] batch [35/42] time 0.403 (0.463) data 0.270 (0.332) loss_x loss_x 1.1484 (1.1328) acc_x 65.6250 (72.3214) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [40/42] time 0.440 (0.459) data 0.310 (0.328) loss_x loss_x 0.7642 (1.1249) acc_x 71.8750 (72.4219) lr 1.5567e-04 eta 0:00:00
epoch [166/200] batch [5/55] time 0.429 (0.461) data 0.298 (0.331) loss_u loss_u 0.7954 (0.8316) acc_u 28.1250 (21.2500) lr 1.5567e-04 eta 0:00:23
epoch [166/200] batch [10/55] time 0.531 (0.466) data 0.399 (0.335) loss_u loss_u 0.7461 (0.8216) acc_u 34.3750 (22.8125) lr 1.5567e-04 eta 0:00:20
epoch [166/200] batch [15/55] time 0.365 (0.466) data 0.235 (0.336) loss_u loss_u 0.8125 (0.8268) acc_u 18.7500 (21.6667) lr 1.5567e-04 eta 0:00:18
epoch [166/200] batch [20/55] time 0.464 (0.466) data 0.334 (0.336) loss_u loss_u 0.8794 (0.8223) acc_u 12.5000 (22.3438) lr 1.5567e-04 eta 0:00:16
epoch [166/200] batch [25/55] time 0.445 (0.466) data 0.314 (0.335) loss_u loss_u 0.8091 (0.8230) acc_u 25.0000 (22.3750) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [30/55] time 0.424 (0.471) data 0.293 (0.340) loss_u loss_u 0.9082 (0.8234) acc_u 9.3750 (22.1875) lr 1.5567e-04 eta 0:00:11
epoch [166/200] batch [35/55] time 0.423 (0.467) data 0.292 (0.336) loss_u loss_u 0.7705 (0.8219) acc_u 28.1250 (22.3214) lr 1.5567e-04 eta 0:00:09
epoch [166/200] batch [40/55] time 0.504 (0.469) data 0.372 (0.338) loss_u loss_u 0.8643 (0.8223) acc_u 15.6250 (22.2656) lr 1.5567e-04 eta 0:00:07
epoch [166/200] batch [45/55] time 0.514 (0.469) data 0.383 (0.338) loss_u loss_u 0.9497 (0.8267) acc_u 6.2500 (21.7361) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [50/55] time 0.608 (0.472) data 0.477 (0.341) loss_u loss_u 0.8008 (0.8253) acc_u 21.8750 (21.8125) lr 1.5567e-04 eta 0:00:02
epoch [166/200] batch [55/55] time 0.363 (0.471) data 0.231 (0.340) loss_u loss_u 0.9033 (0.8305) acc_u 12.5000 (21.2500) lr 1.5567e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1414
confident_label rate tensor(0.4381, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1374
clean true:1372
clean false:2
clean_rate:0.9985443959243085
noisy true:350
noisy false:1412
after delete: len(clean_dataset) 1374
after delete: len(noisy_dataset) 1762
epoch [167/200] batch [5/42] time 0.448 (0.481) data 0.318 (0.350) loss_x loss_x 0.8545 (1.1774) acc_x 75.0000 (71.2500) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [10/42] time 0.479 (0.472) data 0.348 (0.342) loss_x loss_x 1.3496 (1.1550) acc_x 71.8750 (71.8750) lr 1.4736e-04 eta 0:00:15
epoch [167/200] batch [15/42] time 0.438 (0.472) data 0.307 (0.342) loss_x loss_x 1.1211 (1.1478) acc_x 71.8750 (72.0833) lr 1.4736e-04 eta 0:00:12
epoch [167/200] batch [20/42] time 0.449 (0.474) data 0.318 (0.343) loss_x loss_x 1.1523 (1.1522) acc_x 68.7500 (71.7188) lr 1.4736e-04 eta 0:00:10
epoch [167/200] batch [25/42] time 0.464 (0.499) data 0.333 (0.368) loss_x loss_x 0.9951 (1.1275) acc_x 71.8750 (72.6250) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [30/42] time 0.439 (0.502) data 0.308 (0.372) loss_x loss_x 1.2197 (1.1340) acc_x 75.0000 (72.2917) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [35/42] time 0.506 (0.504) data 0.375 (0.374) loss_x loss_x 1.2998 (1.1556) acc_x 68.7500 (71.6071) lr 1.4736e-04 eta 0:00:03
epoch [167/200] batch [40/42] time 0.358 (0.496) data 0.227 (0.366) loss_x loss_x 1.0615 (1.1463) acc_x 75.0000 (71.7969) lr 1.4736e-04 eta 0:00:00
epoch [167/200] batch [5/55] time 0.480 (0.482) data 0.348 (0.351) loss_u loss_u 0.7769 (0.8186) acc_u 28.1250 (23.1250) lr 1.4736e-04 eta 0:00:24
epoch [167/200] batch [10/55] time 0.443 (0.476) data 0.312 (0.346) loss_u loss_u 0.8516 (0.8375) acc_u 18.7500 (21.5625) lr 1.4736e-04 eta 0:00:21
epoch [167/200] batch [15/55] time 0.393 (0.474) data 0.261 (0.343) loss_u loss_u 0.7109 (0.8192) acc_u 34.3750 (22.7083) lr 1.4736e-04 eta 0:00:18
epoch [167/200] batch [20/55] time 0.387 (0.479) data 0.257 (0.348) loss_u loss_u 0.8198 (0.8269) acc_u 18.7500 (21.0938) lr 1.4736e-04 eta 0:00:16
epoch [167/200] batch [25/55] time 0.399 (0.479) data 0.268 (0.349) loss_u loss_u 0.8208 (0.8211) acc_u 28.1250 (21.3750) lr 1.4736e-04 eta 0:00:14
epoch [167/200] batch [30/55] time 0.401 (0.477) data 0.269 (0.346) loss_u loss_u 0.7407 (0.8208) acc_u 31.2500 (21.2500) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [35/55] time 0.483 (0.476) data 0.352 (0.345) loss_u loss_u 0.8945 (0.8239) acc_u 15.6250 (21.3393) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [40/55] time 0.370 (0.478) data 0.238 (0.347) loss_u loss_u 0.8433 (0.8247) acc_u 21.8750 (21.3281) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [45/55] time 0.421 (0.477) data 0.290 (0.346) loss_u loss_u 0.8486 (0.8265) acc_u 21.8750 (21.2500) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [50/55] time 0.322 (0.474) data 0.191 (0.343) loss_u loss_u 0.8184 (0.8291) acc_u 25.0000 (21.1875) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [55/55] time 0.434 (0.470) data 0.302 (0.339) loss_u loss_u 0.8145 (0.8316) acc_u 18.7500 (20.8523) lr 1.4736e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1432
confident_label rate tensor(0.4334, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1359
clean true:1358
clean false:1
clean_rate:0.9992641648270787
noisy true:346
noisy false:1431
after delete: len(clean_dataset) 1359
after delete: len(noisy_dataset) 1777
epoch [168/200] batch [5/42] time 0.432 (0.458) data 0.301 (0.327) loss_x loss_x 1.4854 (1.0833) acc_x 65.6250 (68.7500) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [10/42] time 0.357 (0.440) data 0.227 (0.310) loss_x loss_x 0.9067 (1.0225) acc_x 65.6250 (70.9375) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [15/42] time 0.442 (0.449) data 0.312 (0.318) loss_x loss_x 0.9307 (1.1118) acc_x 78.1250 (69.7917) lr 1.3926e-04 eta 0:00:12
epoch [168/200] batch [20/42] time 0.444 (0.453) data 0.312 (0.323) loss_x loss_x 1.1172 (1.1380) acc_x 81.2500 (70.3125) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [25/42] time 0.381 (0.449) data 0.250 (0.318) loss_x loss_x 1.0459 (1.0996) acc_x 78.1250 (71.7500) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [30/42] time 0.434 (0.454) data 0.303 (0.323) loss_x loss_x 1.0801 (1.0901) acc_x 71.8750 (72.0833) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [35/42] time 0.471 (0.459) data 0.341 (0.328) loss_x loss_x 1.1201 (1.0773) acc_x 65.6250 (72.2321) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [40/42] time 0.527 (0.460) data 0.396 (0.330) loss_x loss_x 1.0127 (1.0850) acc_x 71.8750 (72.2656) lr 1.3926e-04 eta 0:00:00
epoch [168/200] batch [5/55] time 0.554 (0.471) data 0.423 (0.340) loss_u loss_u 0.7783 (0.8646) acc_u 31.2500 (16.8750) lr 1.3926e-04 eta 0:00:23
epoch [168/200] batch [10/55] time 0.445 (0.466) data 0.313 (0.336) loss_u loss_u 0.8179 (0.8532) acc_u 21.8750 (17.8125) lr 1.3926e-04 eta 0:00:20
epoch [168/200] batch [15/55] time 0.406 (0.463) data 0.274 (0.332) loss_u loss_u 0.8301 (0.8382) acc_u 25.0000 (20.4167) lr 1.3926e-04 eta 0:00:18
epoch [168/200] batch [20/55] time 0.403 (0.462) data 0.270 (0.332) loss_u loss_u 0.8115 (0.8356) acc_u 28.1250 (21.0938) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [25/55] time 0.457 (0.463) data 0.325 (0.332) loss_u loss_u 0.9048 (0.8367) acc_u 15.6250 (21.1250) lr 1.3926e-04 eta 0:00:13
epoch [168/200] batch [30/55] time 0.438 (0.462) data 0.307 (0.331) loss_u loss_u 0.8101 (0.8378) acc_u 25.0000 (21.0417) lr 1.3926e-04 eta 0:00:11
epoch [168/200] batch [35/55] time 0.365 (0.464) data 0.233 (0.334) loss_u loss_u 0.8706 (0.8316) acc_u 15.6250 (22.2321) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [40/55] time 0.380 (0.464) data 0.248 (0.333) loss_u loss_u 0.8774 (0.8333) acc_u 18.7500 (21.7969) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [45/55] time 0.429 (0.463) data 0.297 (0.332) loss_u loss_u 0.8115 (0.8334) acc_u 25.0000 (21.5972) lr 1.3926e-04 eta 0:00:04
epoch [168/200] batch [50/55] time 0.414 (0.464) data 0.283 (0.333) loss_u loss_u 0.8086 (0.8338) acc_u 21.8750 (21.6250) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [55/55] time 0.432 (0.465) data 0.302 (0.334) loss_u loss_u 0.7222 (0.8315) acc_u 34.3750 (21.8182) lr 1.3926e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1437
confident_label rate tensor(0.4349, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1364
clean true:1361
clean false:3
clean_rate:0.9978005865102639
noisy true:338
noisy false:1434
after delete: len(clean_dataset) 1364
after delete: len(noisy_dataset) 1772
epoch [169/200] batch [5/42] time 0.414 (0.504) data 0.283 (0.372) loss_x loss_x 1.3711 (1.1429) acc_x 65.6250 (70.0000) lr 1.3137e-04 eta 0:00:18
epoch [169/200] batch [10/42] time 0.502 (0.514) data 0.371 (0.383) loss_x loss_x 1.2363 (1.0420) acc_x 75.0000 (71.8750) lr 1.3137e-04 eta 0:00:16
epoch [169/200] batch [15/42] time 0.682 (0.532) data 0.550 (0.401) loss_x loss_x 0.8662 (1.0988) acc_x 81.2500 (71.8750) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [20/42] time 0.437 (0.506) data 0.306 (0.375) loss_x loss_x 1.0195 (1.1339) acc_x 68.7500 (71.0938) lr 1.3137e-04 eta 0:00:11
epoch [169/200] batch [25/42] time 0.570 (0.497) data 0.439 (0.366) loss_x loss_x 1.3916 (1.1791) acc_x 71.8750 (71.1250) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [30/42] time 0.391 (0.486) data 0.260 (0.354) loss_x loss_x 0.9434 (1.1654) acc_x 78.1250 (70.9375) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [35/42] time 0.492 (0.484) data 0.362 (0.353) loss_x loss_x 1.0811 (1.1266) acc_x 65.6250 (71.9643) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [40/42] time 0.373 (0.477) data 0.243 (0.346) loss_x loss_x 0.9912 (1.1272) acc_x 78.1250 (71.7188) lr 1.3137e-04 eta 0:00:00
epoch [169/200] batch [5/55] time 0.477 (0.475) data 0.345 (0.344) loss_u loss_u 0.7983 (0.8276) acc_u 25.0000 (22.5000) lr 1.3137e-04 eta 0:00:23
epoch [169/200] batch [10/55] time 0.536 (0.474) data 0.405 (0.343) loss_u loss_u 0.7881 (0.8232) acc_u 28.1250 (21.8750) lr 1.3137e-04 eta 0:00:21
epoch [169/200] batch [15/55] time 0.616 (0.476) data 0.486 (0.345) loss_u loss_u 0.8550 (0.8338) acc_u 18.7500 (21.0417) lr 1.3137e-04 eta 0:00:19
epoch [169/200] batch [20/55] time 0.469 (0.475) data 0.338 (0.344) loss_u loss_u 0.8511 (0.8348) acc_u 18.7500 (20.4688) lr 1.3137e-04 eta 0:00:16
epoch [169/200] batch [25/55] time 0.348 (0.471) data 0.218 (0.340) loss_u loss_u 0.8140 (0.8393) acc_u 25.0000 (19.7500) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [30/55] time 0.664 (0.475) data 0.533 (0.344) loss_u loss_u 0.8745 (0.8406) acc_u 12.5000 (19.6875) lr 1.3137e-04 eta 0:00:11
epoch [169/200] batch [35/55] time 0.384 (0.479) data 0.253 (0.348) loss_u loss_u 0.8389 (0.8371) acc_u 21.8750 (20.4464) lr 1.3137e-04 eta 0:00:09
epoch [169/200] batch [40/55] time 0.398 (0.478) data 0.266 (0.346) loss_u loss_u 0.7598 (0.8319) acc_u 28.1250 (21.0156) lr 1.3137e-04 eta 0:00:07
epoch [169/200] batch [45/55] time 0.495 (0.477) data 0.364 (0.346) loss_u loss_u 0.7310 (0.8306) acc_u 37.5000 (21.3194) lr 1.3137e-04 eta 0:00:04
epoch [169/200] batch [50/55] time 0.443 (0.478) data 0.312 (0.346) loss_u loss_u 0.8047 (0.8286) acc_u 25.0000 (21.3125) lr 1.3137e-04 eta 0:00:02
epoch [169/200] batch [55/55] time 0.334 (0.478) data 0.204 (0.346) loss_u loss_u 0.7905 (0.8296) acc_u 25.0000 (21.3636) lr 1.3137e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.4289, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1345
clean true:1343
clean false:2
clean_rate:0.9985130111524163
noisy true:359
noisy false:1432
after delete: len(clean_dataset) 1345
after delete: len(noisy_dataset) 1791
epoch [170/200] batch [5/42] time 0.432 (0.492) data 0.301 (0.361) loss_x loss_x 1.2158 (1.0747) acc_x 62.5000 (68.7500) lr 1.2369e-04 eta 0:00:18
epoch [170/200] batch [10/42] time 0.488 (0.497) data 0.358 (0.366) loss_x loss_x 1.1328 (1.1100) acc_x 75.0000 (71.2500) lr 1.2369e-04 eta 0:00:15
epoch [170/200] batch [15/42] time 0.515 (0.488) data 0.384 (0.358) loss_x loss_x 1.2783 (1.0665) acc_x 68.7500 (72.5000) lr 1.2369e-04 eta 0:00:13
epoch [170/200] batch [20/42] time 0.571 (0.492) data 0.440 (0.361) loss_x loss_x 1.0791 (1.0204) acc_x 68.7500 (73.1250) lr 1.2369e-04 eta 0:00:10
epoch [170/200] batch [25/42] time 0.468 (0.501) data 0.338 (0.370) loss_x loss_x 1.1035 (1.0439) acc_x 75.0000 (73.5000) lr 1.2369e-04 eta 0:00:08
epoch [170/200] batch [30/42] time 0.446 (0.490) data 0.314 (0.359) loss_x loss_x 0.7466 (1.0484) acc_x 78.1250 (73.4375) lr 1.2369e-04 eta 0:00:05
epoch [170/200] batch [35/42] time 0.517 (0.489) data 0.386 (0.358) loss_x loss_x 1.6787 (1.0826) acc_x 71.8750 (73.2143) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [40/42] time 0.519 (0.488) data 0.389 (0.358) loss_x loss_x 1.7715 (1.1216) acc_x 62.5000 (72.7344) lr 1.2369e-04 eta 0:00:00
epoch [170/200] batch [5/55] time 0.538 (0.488) data 0.406 (0.357) loss_u loss_u 0.7930 (0.8059) acc_u 25.0000 (26.2500) lr 1.2369e-04 eta 0:00:24
epoch [170/200] batch [10/55] time 0.442 (0.482) data 0.311 (0.351) loss_u loss_u 0.8076 (0.8297) acc_u 28.1250 (22.8125) lr 1.2369e-04 eta 0:00:21
epoch [170/200] batch [15/55] time 0.404 (0.479) data 0.273 (0.348) loss_u loss_u 0.8735 (0.8398) acc_u 9.3750 (20.2083) lr 1.2369e-04 eta 0:00:19
epoch [170/200] batch [20/55] time 0.366 (0.474) data 0.234 (0.343) loss_u loss_u 0.8218 (0.8350) acc_u 25.0000 (20.6250) lr 1.2369e-04 eta 0:00:16
epoch [170/200] batch [25/55] time 0.406 (0.472) data 0.274 (0.341) loss_u loss_u 0.8174 (0.8370) acc_u 18.7500 (20.5000) lr 1.2369e-04 eta 0:00:14
epoch [170/200] batch [30/55] time 0.367 (0.468) data 0.236 (0.337) loss_u loss_u 0.8110 (0.8454) acc_u 25.0000 (19.3750) lr 1.2369e-04 eta 0:00:11
epoch [170/200] batch [35/55] time 0.370 (0.465) data 0.239 (0.334) loss_u loss_u 0.7017 (0.8349) acc_u 31.2500 (20.4464) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [40/55] time 0.409 (0.465) data 0.276 (0.334) loss_u loss_u 0.8086 (0.8319) acc_u 18.7500 (20.9375) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [45/55] time 0.431 (0.465) data 0.299 (0.334) loss_u loss_u 0.8770 (0.8302) acc_u 18.7500 (21.5972) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [50/55] time 0.565 (0.464) data 0.435 (0.333) loss_u loss_u 0.8423 (0.8312) acc_u 12.5000 (21.5000) lr 1.2369e-04 eta 0:00:02
epoch [170/200] batch [55/55] time 0.514 (0.469) data 0.382 (0.337) loss_u loss_u 0.8262 (0.8355) acc_u 25.0000 (20.9659) lr 1.2369e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1442
confident_label rate tensor(0.4302, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1349
clean true:1346
clean false:3
clean_rate:0.9977761304670126
noisy true:348
noisy false:1439
after delete: len(clean_dataset) 1349
after delete: len(noisy_dataset) 1787
epoch [171/200] batch [5/42] time 0.423 (0.475) data 0.293 (0.345) loss_x loss_x 0.8525 (0.8902) acc_x 75.0000 (76.8750) lr 1.1623e-04 eta 0:00:17
epoch [171/200] batch [10/42] time 0.572 (0.481) data 0.442 (0.350) loss_x loss_x 0.9536 (0.9735) acc_x 71.8750 (74.0625) lr 1.1623e-04 eta 0:00:15
epoch [171/200] batch [15/42] time 0.501 (0.476) data 0.369 (0.345) loss_x loss_x 1.1875 (1.0101) acc_x 68.7500 (73.1250) lr 1.1623e-04 eta 0:00:12
epoch [171/200] batch [20/42] time 0.488 (0.476) data 0.356 (0.346) loss_x loss_x 1.2295 (1.0014) acc_x 68.7500 (73.7500) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [25/42] time 0.436 (0.470) data 0.305 (0.339) loss_x loss_x 0.9717 (0.9874) acc_x 81.2500 (74.7500) lr 1.1623e-04 eta 0:00:07
epoch [171/200] batch [30/42] time 0.411 (0.466) data 0.280 (0.335) loss_x loss_x 1.3848 (1.0313) acc_x 62.5000 (73.9583) lr 1.1623e-04 eta 0:00:05
epoch [171/200] batch [35/42] time 0.537 (0.471) data 0.407 (0.340) loss_x loss_x 0.9482 (1.0438) acc_x 68.7500 (73.7500) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [40/42] time 0.491 (0.469) data 0.358 (0.338) loss_x loss_x 0.9683 (1.0626) acc_x 75.0000 (73.3594) lr 1.1623e-04 eta 0:00:00
epoch [171/200] batch [5/55] time 0.438 (0.475) data 0.308 (0.344) loss_u loss_u 0.8354 (0.8344) acc_u 21.8750 (20.0000) lr 1.1623e-04 eta 0:00:23
epoch [171/200] batch [10/55] time 0.557 (0.475) data 0.426 (0.344) loss_u loss_u 0.7349 (0.7990) acc_u 37.5000 (25.9375) lr 1.1623e-04 eta 0:00:21
epoch [171/200] batch [15/55] time 0.356 (0.479) data 0.224 (0.348) loss_u loss_u 0.8467 (0.8030) acc_u 25.0000 (25.8333) lr 1.1623e-04 eta 0:00:19
epoch [171/200] batch [20/55] time 0.478 (0.481) data 0.347 (0.350) loss_u loss_u 0.8516 (0.8124) acc_u 15.6250 (24.3750) lr 1.1623e-04 eta 0:00:16
epoch [171/200] batch [25/55] time 0.428 (0.476) data 0.297 (0.345) loss_u loss_u 0.7734 (0.8152) acc_u 34.3750 (24.5000) lr 1.1623e-04 eta 0:00:14
epoch [171/200] batch [30/55] time 0.481 (0.475) data 0.349 (0.344) loss_u loss_u 0.8188 (0.8159) acc_u 25.0000 (23.9583) lr 1.1623e-04 eta 0:00:11
epoch [171/200] batch [35/55] time 0.499 (0.474) data 0.367 (0.343) loss_u loss_u 0.9009 (0.8194) acc_u 12.5000 (23.5714) lr 1.1623e-04 eta 0:00:09
epoch [171/200] batch [40/55] time 0.475 (0.475) data 0.343 (0.344) loss_u loss_u 0.8223 (0.8220) acc_u 25.0000 (23.4375) lr 1.1623e-04 eta 0:00:07
epoch [171/200] batch [45/55] time 0.552 (0.479) data 0.422 (0.348) loss_u loss_u 0.8569 (0.8237) acc_u 15.6250 (22.9167) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [50/55] time 0.405 (0.477) data 0.274 (0.346) loss_u loss_u 0.8247 (0.8269) acc_u 18.7500 (22.5000) lr 1.1623e-04 eta 0:00:02
epoch [171/200] batch [55/55] time 0.473 (0.475) data 0.341 (0.344) loss_u loss_u 0.8262 (0.8255) acc_u 21.8750 (22.7273) lr 1.1623e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1481
confident_label rate tensor(0.4212, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1321
clean true:1320
clean false:1
clean_rate:0.9992429977289932
noisy true:335
noisy false:1480
after delete: len(clean_dataset) 1321
after delete: len(noisy_dataset) 1815
epoch [172/200] batch [5/41] time 0.513 (0.487) data 0.382 (0.356) loss_x loss_x 0.8901 (1.1552) acc_x 71.8750 (71.2500) lr 1.0899e-04 eta 0:00:17
epoch [172/200] batch [10/41] time 0.470 (0.481) data 0.340 (0.350) loss_x loss_x 1.4355 (1.1853) acc_x 62.5000 (70.3125) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [15/41] time 0.591 (0.499) data 0.458 (0.368) loss_x loss_x 0.7593 (1.0800) acc_x 75.0000 (73.3333) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [20/41] time 0.402 (0.484) data 0.269 (0.352) loss_x loss_x 0.9697 (1.1011) acc_x 84.3750 (72.9688) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [25/41] time 0.600 (0.491) data 0.467 (0.359) loss_x loss_x 1.0332 (1.1073) acc_x 71.8750 (72.6250) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [30/41] time 0.476 (0.488) data 0.343 (0.356) loss_x loss_x 1.8340 (1.1098) acc_x 46.8750 (72.2917) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [35/41] time 0.599 (0.486) data 0.467 (0.353) loss_x loss_x 0.8564 (1.1241) acc_x 81.2500 (71.8750) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [40/41] time 0.480 (0.479) data 0.349 (0.346) loss_x loss_x 0.8975 (1.1251) acc_x 81.2500 (72.1875) lr 1.0899e-04 eta 0:00:00
epoch [172/200] batch [5/56] time 0.371 (0.477) data 0.239 (0.344) loss_u loss_u 0.8765 (0.8205) acc_u 9.3750 (23.7500) lr 1.0899e-04 eta 0:00:24
epoch [172/200] batch [10/56] time 0.526 (0.476) data 0.395 (0.343) loss_u loss_u 0.8613 (0.8295) acc_u 15.6250 (21.5625) lr 1.0899e-04 eta 0:00:21
epoch [172/200] batch [15/56] time 0.455 (0.475) data 0.324 (0.343) loss_u loss_u 0.8667 (0.8297) acc_u 25.0000 (21.4583) lr 1.0899e-04 eta 0:00:19
epoch [172/200] batch [20/56] time 0.548 (0.475) data 0.417 (0.343) loss_u loss_u 0.7705 (0.8224) acc_u 28.1250 (21.8750) lr 1.0899e-04 eta 0:00:17
epoch [172/200] batch [25/56] time 0.499 (0.478) data 0.367 (0.346) loss_u loss_u 0.8545 (0.8258) acc_u 18.7500 (21.3750) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [30/56] time 0.447 (0.483) data 0.315 (0.351) loss_u loss_u 0.7251 (0.8239) acc_u 31.2500 (21.6667) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [35/56] time 0.365 (0.481) data 0.235 (0.349) loss_u loss_u 0.8247 (0.8220) acc_u 21.8750 (21.8750) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [40/56] time 0.531 (0.483) data 0.401 (0.351) loss_u loss_u 0.7480 (0.8199) acc_u 31.2500 (22.4219) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [45/56] time 0.378 (0.482) data 0.246 (0.350) loss_u loss_u 0.7603 (0.8172) acc_u 37.5000 (23.2639) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [50/56] time 0.463 (0.480) data 0.332 (0.348) loss_u loss_u 0.7529 (0.8188) acc_u 31.2500 (23.1250) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [55/56] time 0.361 (0.477) data 0.230 (0.345) loss_u loss_u 0.8823 (0.8187) acc_u 18.7500 (23.1818) lr 1.0899e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1398
confident_label rate tensor(0.4346, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1363
clean true:1362
clean false:1
clean_rate:0.9992663242846662
noisy true:376
noisy false:1397
after delete: len(clean_dataset) 1363
after delete: len(noisy_dataset) 1773
epoch [173/200] batch [5/42] time 0.397 (0.432) data 0.267 (0.302) loss_x loss_x 1.0859 (1.1097) acc_x 68.7500 (70.0000) lr 1.0197e-04 eta 0:00:16
epoch [173/200] batch [10/42] time 0.484 (0.474) data 0.353 (0.344) loss_x loss_x 0.9985 (1.0854) acc_x 75.0000 (71.5625) lr 1.0197e-04 eta 0:00:15
epoch [173/200] batch [15/42] time 0.470 (0.468) data 0.338 (0.337) loss_x loss_x 1.4395 (1.1553) acc_x 71.8750 (71.2500) lr 1.0197e-04 eta 0:00:12
epoch [173/200] batch [20/42] time 0.428 (0.469) data 0.297 (0.338) loss_x loss_x 1.6357 (1.1792) acc_x 62.5000 (70.3125) lr 1.0197e-04 eta 0:00:10
epoch [173/200] batch [25/42] time 0.462 (0.471) data 0.330 (0.340) loss_x loss_x 1.3721 (1.1496) acc_x 62.5000 (70.8750) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [30/42] time 0.626 (0.476) data 0.495 (0.344) loss_x loss_x 1.1729 (1.1766) acc_x 71.8750 (70.5208) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [35/42] time 0.398 (0.469) data 0.267 (0.337) loss_x loss_x 1.0693 (1.1591) acc_x 71.8750 (70.8036) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [40/42] time 0.643 (0.476) data 0.512 (0.344) loss_x loss_x 1.3096 (1.1648) acc_x 62.5000 (70.4688) lr 1.0197e-04 eta 0:00:00
epoch [173/200] batch [5/55] time 0.386 (0.473) data 0.255 (0.341) loss_u loss_u 0.7402 (0.7956) acc_u 31.2500 (26.2500) lr 1.0197e-04 eta 0:00:23
epoch [173/200] batch [10/55] time 0.397 (0.468) data 0.267 (0.337) loss_u loss_u 0.7588 (0.8095) acc_u 31.2500 (25.3125) lr 1.0197e-04 eta 0:00:21
epoch [173/200] batch [15/55] time 0.428 (0.471) data 0.297 (0.340) loss_u loss_u 0.6851 (0.8116) acc_u 40.6250 (24.3750) lr 1.0197e-04 eta 0:00:18
epoch [173/200] batch [20/55] time 0.416 (0.467) data 0.285 (0.336) loss_u loss_u 0.8774 (0.8217) acc_u 15.6250 (23.1250) lr 1.0197e-04 eta 0:00:16
epoch [173/200] batch [25/55] time 0.436 (0.463) data 0.306 (0.331) loss_u loss_u 0.8545 (0.8157) acc_u 15.6250 (23.8750) lr 1.0197e-04 eta 0:00:13
epoch [173/200] batch [30/55] time 0.386 (0.459) data 0.256 (0.328) loss_u loss_u 0.8867 (0.8205) acc_u 15.6250 (23.7500) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [35/55] time 0.368 (0.455) data 0.237 (0.324) loss_u loss_u 0.8193 (0.8223) acc_u 21.8750 (23.0357) lr 1.0197e-04 eta 0:00:09
epoch [173/200] batch [40/55] time 0.529 (0.454) data 0.399 (0.323) loss_u loss_u 0.7524 (0.8206) acc_u 31.2500 (23.5156) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [45/55] time 0.458 (0.454) data 0.327 (0.322) loss_u loss_u 0.8213 (0.8201) acc_u 21.8750 (23.4028) lr 1.0197e-04 eta 0:00:04
epoch [173/200] batch [50/55] time 0.452 (0.456) data 0.321 (0.325) loss_u loss_u 0.8848 (0.8207) acc_u 12.5000 (23.3750) lr 1.0197e-04 eta 0:00:02
epoch [173/200] batch [55/55] time 0.648 (0.462) data 0.517 (0.330) loss_u loss_u 0.8110 (0.8205) acc_u 31.2500 (23.5795) lr 1.0197e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1463
confident_label rate tensor(0.4222, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1324
clean true:1324
clean false:0
clean_rate:1.0
noisy true:349
noisy false:1463
after delete: len(clean_dataset) 1324
after delete: len(noisy_dataset) 1812
epoch [174/200] batch [5/41] time 0.477 (0.451) data 0.347 (0.320) loss_x loss_x 1.5908 (1.1875) acc_x 53.1250 (65.0000) lr 9.5173e-05 eta 0:00:16
epoch [174/200] batch [10/41] time 0.500 (0.500) data 0.368 (0.370) loss_x loss_x 1.1289 (1.1273) acc_x 68.7500 (69.0625) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [15/41] time 0.533 (0.504) data 0.402 (0.373) loss_x loss_x 1.4688 (1.1438) acc_x 65.6250 (70.0000) lr 9.5173e-05 eta 0:00:13
epoch [174/200] batch [20/41] time 0.427 (0.485) data 0.297 (0.354) loss_x loss_x 1.2686 (1.1250) acc_x 65.6250 (71.4062) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [25/41] time 0.391 (0.480) data 0.261 (0.349) loss_x loss_x 1.2051 (1.1222) acc_x 71.8750 (71.7500) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [30/41] time 0.513 (0.480) data 0.383 (0.349) loss_x loss_x 0.6250 (1.1357) acc_x 84.3750 (71.7708) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [35/41] time 0.464 (0.483) data 0.333 (0.352) loss_x loss_x 1.1914 (1.1242) acc_x 59.3750 (71.9643) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [40/41] time 0.383 (0.477) data 0.253 (0.346) loss_x loss_x 1.2305 (1.1519) acc_x 75.0000 (71.3281) lr 9.5173e-05 eta 0:00:00
epoch [174/200] batch [5/56] time 0.435 (0.478) data 0.305 (0.347) loss_u loss_u 0.7769 (0.8175) acc_u 34.3750 (26.2500) lr 9.5173e-05 eta 0:00:24
epoch [174/200] batch [10/56] time 0.525 (0.487) data 0.395 (0.357) loss_u loss_u 0.8477 (0.8222) acc_u 21.8750 (23.7500) lr 9.5173e-05 eta 0:00:22
epoch [174/200] batch [15/56] time 0.414 (0.482) data 0.283 (0.351) loss_u loss_u 0.7529 (0.8152) acc_u 37.5000 (25.0000) lr 9.5173e-05 eta 0:00:19
epoch [174/200] batch [20/56] time 0.365 (0.478) data 0.233 (0.347) loss_u loss_u 0.7998 (0.8227) acc_u 25.0000 (23.2812) lr 9.5173e-05 eta 0:00:17
epoch [174/200] batch [25/56] time 0.415 (0.475) data 0.283 (0.344) loss_u loss_u 0.7803 (0.8091) acc_u 37.5000 (25.6250) lr 9.5173e-05 eta 0:00:14
epoch [174/200] batch [30/56] time 0.421 (0.472) data 0.290 (0.341) loss_u loss_u 0.8369 (0.8114) acc_u 18.7500 (25.0000) lr 9.5173e-05 eta 0:00:12
epoch [174/200] batch [35/56] time 0.433 (0.470) data 0.302 (0.339) loss_u loss_u 0.7690 (0.8131) acc_u 28.1250 (24.4643) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [40/56] time 0.461 (0.466) data 0.331 (0.335) loss_u loss_u 0.8638 (0.8153) acc_u 15.6250 (23.9062) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [45/56] time 0.384 (0.463) data 0.253 (0.332) loss_u loss_u 0.9033 (0.8190) acc_u 9.3750 (23.2639) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [50/56] time 0.512 (0.466) data 0.381 (0.335) loss_u loss_u 0.7778 (0.8189) acc_u 28.1250 (23.3125) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [55/56] time 0.492 (0.471) data 0.361 (0.339) loss_u loss_u 0.8442 (0.8193) acc_u 21.8750 (23.0682) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1401
confident_label rate tensor(0.4429, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1389
clean true:1386
clean false:3
clean_rate:0.9978401727861771
noisy true:349
noisy false:1398
after delete: len(clean_dataset) 1389
after delete: len(noisy_dataset) 1747
epoch [175/200] batch [5/43] time 0.442 (0.457) data 0.310 (0.326) loss_x loss_x 0.8086 (0.9888) acc_x 87.5000 (73.7500) lr 8.8597e-05 eta 0:00:17
epoch [175/200] batch [10/43] time 0.504 (0.449) data 0.373 (0.318) loss_x loss_x 0.6650 (0.9957) acc_x 84.3750 (75.6250) lr 8.8597e-05 eta 0:00:14
epoch [175/200] batch [15/43] time 0.500 (0.463) data 0.369 (0.332) loss_x loss_x 1.3418 (1.0454) acc_x 62.5000 (74.1667) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [20/43] time 0.451 (0.466) data 0.319 (0.334) loss_x loss_x 1.4619 (1.0838) acc_x 65.6250 (73.1250) lr 8.8597e-05 eta 0:00:10
epoch [175/200] batch [25/43] time 0.464 (0.464) data 0.334 (0.332) loss_x loss_x 0.8628 (1.0979) acc_x 81.2500 (73.5000) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [30/43] time 0.358 (0.464) data 0.227 (0.333) loss_x loss_x 1.2891 (1.0923) acc_x 75.0000 (73.6458) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [35/43] time 0.626 (0.470) data 0.495 (0.339) loss_x loss_x 1.2109 (1.1269) acc_x 62.5000 (72.5000) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [40/43] time 0.512 (0.468) data 0.382 (0.337) loss_x loss_x 1.4639 (1.1419) acc_x 56.2500 (71.9531) lr 8.8597e-05 eta 0:00:01
epoch [175/200] batch [5/54] time 0.399 (0.468) data 0.268 (0.337) loss_u loss_u 0.7759 (0.8079) acc_u 31.2500 (25.0000) lr 8.8597e-05 eta 0:00:22
epoch [175/200] batch [10/54] time 0.485 (0.467) data 0.354 (0.336) loss_u loss_u 0.9111 (0.8270) acc_u 12.5000 (22.1875) lr 8.8597e-05 eta 0:00:20
epoch [175/200] batch [15/54] time 0.701 (0.468) data 0.570 (0.337) loss_u loss_u 0.8647 (0.8239) acc_u 18.7500 (22.7083) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [20/54] time 0.429 (0.467) data 0.299 (0.336) loss_u loss_u 0.7700 (0.8155) acc_u 34.3750 (23.7500) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [25/54] time 0.724 (0.469) data 0.591 (0.338) loss_u loss_u 0.8447 (0.8145) acc_u 18.7500 (23.5000) lr 8.8597e-05 eta 0:00:13
epoch [175/200] batch [30/54] time 0.545 (0.469) data 0.414 (0.338) loss_u loss_u 0.7617 (0.8096) acc_u 34.3750 (23.9583) lr 8.8597e-05 eta 0:00:11
epoch [175/200] batch [35/54] time 0.599 (0.468) data 0.468 (0.336) loss_u loss_u 0.7812 (0.8141) acc_u 21.8750 (23.0357) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [40/54] time 0.442 (0.469) data 0.311 (0.338) loss_u loss_u 0.8359 (0.8179) acc_u 18.7500 (22.3438) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [45/54] time 0.391 (0.465) data 0.260 (0.334) loss_u loss_u 0.8574 (0.8162) acc_u 21.8750 (23.0556) lr 8.8597e-05 eta 0:00:04
epoch [175/200] batch [50/54] time 0.478 (0.464) data 0.347 (0.333) loss_u loss_u 0.7842 (0.8182) acc_u 28.1250 (22.7500) lr 8.8597e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1406
confident_label rate tensor(0.4356, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1366
clean true:1364
clean false:2
clean_rate:0.9985358711566618
noisy true:366
noisy false:1404
after delete: len(clean_dataset) 1366
after delete: len(noisy_dataset) 1770
epoch [176/200] batch [5/42] time 0.397 (0.479) data 0.266 (0.348) loss_x loss_x 1.6758 (1.2373) acc_x 53.1250 (68.1250) lr 8.2245e-05 eta 0:00:17
epoch [176/200] batch [10/42] time 0.487 (0.477) data 0.356 (0.346) loss_x loss_x 1.2070 (1.2923) acc_x 65.6250 (68.1250) lr 8.2245e-05 eta 0:00:15
epoch [176/200] batch [15/42] time 0.532 (0.490) data 0.400 (0.359) loss_x loss_x 1.0596 (1.2769) acc_x 68.7500 (69.1667) lr 8.2245e-05 eta 0:00:13
epoch [176/200] batch [20/42] time 0.527 (0.503) data 0.396 (0.372) loss_x loss_x 1.4424 (1.2885) acc_x 62.5000 (68.7500) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [25/42] time 0.358 (0.497) data 0.227 (0.366) loss_x loss_x 0.6543 (1.2474) acc_x 84.3750 (69.2500) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [30/42] time 0.488 (0.493) data 0.358 (0.362) loss_x loss_x 0.7798 (1.1905) acc_x 75.0000 (70.7292) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [35/42] time 0.406 (0.496) data 0.275 (0.365) loss_x loss_x 0.9185 (1.1733) acc_x 81.2500 (71.7857) lr 8.2245e-05 eta 0:00:03
epoch [176/200] batch [40/42] time 0.455 (0.488) data 0.324 (0.357) loss_x loss_x 1.4824 (1.1833) acc_x 68.7500 (71.7188) lr 8.2245e-05 eta 0:00:00
epoch [176/200] batch [5/55] time 0.538 (0.486) data 0.406 (0.355) loss_u loss_u 0.7769 (0.7908) acc_u 31.2500 (28.1250) lr 8.2245e-05 eta 0:00:24
epoch [176/200] batch [10/55] time 0.435 (0.481) data 0.303 (0.350) loss_u loss_u 0.8418 (0.8081) acc_u 21.8750 (25.0000) lr 8.2245e-05 eta 0:00:21
epoch [176/200] batch [15/55] time 0.410 (0.483) data 0.279 (0.352) loss_u loss_u 0.9082 (0.8235) acc_u 12.5000 (21.6667) lr 8.2245e-05 eta 0:00:19
epoch [176/200] batch [20/55] time 0.388 (0.481) data 0.257 (0.350) loss_u loss_u 0.9058 (0.8252) acc_u 9.3750 (21.4062) lr 8.2245e-05 eta 0:00:16
epoch [176/200] batch [25/55] time 0.349 (0.477) data 0.217 (0.346) loss_u loss_u 0.7817 (0.8244) acc_u 28.1250 (21.8750) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [30/55] time 0.402 (0.474) data 0.270 (0.343) loss_u loss_u 0.9243 (0.8313) acc_u 9.3750 (21.1458) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [35/55] time 0.561 (0.475) data 0.427 (0.343) loss_u loss_u 0.8975 (0.8328) acc_u 15.6250 (21.1607) lr 8.2245e-05 eta 0:00:09
epoch [176/200] batch [40/55] time 0.396 (0.473) data 0.264 (0.342) loss_u loss_u 0.7847 (0.8289) acc_u 25.0000 (21.5625) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [45/55] time 0.370 (0.468) data 0.238 (0.337) loss_u loss_u 0.8647 (0.8276) acc_u 12.5000 (21.8056) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [50/55] time 0.592 (0.466) data 0.460 (0.335) loss_u loss_u 0.8325 (0.8275) acc_u 18.7500 (21.6875) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [55/55] time 0.392 (0.467) data 0.261 (0.335) loss_u loss_u 0.7261 (0.8259) acc_u 40.6250 (22.1591) lr 8.2245e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1421
confident_label rate tensor(0.4334, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1359
clean true:1357
clean false:2
clean_rate:0.9985283296541575
noisy true:358
noisy false:1419
after delete: len(clean_dataset) 1359
after delete: len(noisy_dataset) 1777
epoch [177/200] batch [5/42] time 0.388 (0.460) data 0.258 (0.329) loss_x loss_x 1.2402 (1.2132) acc_x 71.8750 (73.1250) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [10/42] time 0.477 (0.446) data 0.346 (0.315) loss_x loss_x 1.2734 (1.2501) acc_x 71.8750 (73.1250) lr 7.6120e-05 eta 0:00:14
epoch [177/200] batch [15/42] time 0.411 (0.465) data 0.281 (0.334) loss_x loss_x 1.2373 (1.1813) acc_x 62.5000 (72.7083) lr 7.6120e-05 eta 0:00:12
epoch [177/200] batch [20/42] time 0.478 (0.472) data 0.348 (0.341) loss_x loss_x 1.0771 (1.1742) acc_x 75.0000 (73.5938) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [25/42] time 0.510 (0.463) data 0.380 (0.332) loss_x loss_x 1.1152 (1.1485) acc_x 71.8750 (73.7500) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [30/42] time 0.534 (0.467) data 0.403 (0.336) loss_x loss_x 0.9238 (1.1329) acc_x 81.2500 (73.7500) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [35/42] time 0.469 (0.465) data 0.339 (0.334) loss_x loss_x 0.9424 (1.1368) acc_x 68.7500 (72.9464) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [40/42] time 0.559 (0.471) data 0.428 (0.340) loss_x loss_x 1.4482 (1.1334) acc_x 68.7500 (73.1250) lr 7.6120e-05 eta 0:00:00
epoch [177/200] batch [5/55] time 0.438 (0.470) data 0.307 (0.339) loss_u loss_u 0.7456 (0.8162) acc_u 34.3750 (22.5000) lr 7.6120e-05 eta 0:00:23
epoch [177/200] batch [10/55] time 0.403 (0.465) data 0.273 (0.335) loss_u loss_u 0.8203 (0.8295) acc_u 18.7500 (20.0000) lr 7.6120e-05 eta 0:00:20
epoch [177/200] batch [15/55] time 0.445 (0.459) data 0.314 (0.328) loss_u loss_u 0.7271 (0.8182) acc_u 31.2500 (22.2917) lr 7.6120e-05 eta 0:00:18
epoch [177/200] batch [20/55] time 0.324 (0.459) data 0.192 (0.329) loss_u loss_u 0.8882 (0.8287) acc_u 12.5000 (20.4688) lr 7.6120e-05 eta 0:00:16
epoch [177/200] batch [25/55] time 0.536 (0.464) data 0.400 (0.333) loss_u loss_u 0.8101 (0.8246) acc_u 25.0000 (21.2500) lr 7.6120e-05 eta 0:00:13
epoch [177/200] batch [30/55] time 0.356 (0.466) data 0.225 (0.335) loss_u loss_u 0.8745 (0.8264) acc_u 15.6250 (21.1458) lr 7.6120e-05 eta 0:00:11
epoch [177/200] batch [35/55] time 0.640 (0.466) data 0.508 (0.335) loss_u loss_u 0.8799 (0.8320) acc_u 12.5000 (20.5357) lr 7.6120e-05 eta 0:00:09
epoch [177/200] batch [40/55] time 0.455 (0.465) data 0.323 (0.334) loss_u loss_u 0.8022 (0.8315) acc_u 31.2500 (20.7031) lr 7.6120e-05 eta 0:00:06
epoch [177/200] batch [45/55] time 0.469 (0.463) data 0.339 (0.332) loss_u loss_u 0.8462 (0.8305) acc_u 21.8750 (20.9028) lr 7.6120e-05 eta 0:00:04
epoch [177/200] batch [50/55] time 0.407 (0.463) data 0.277 (0.333) loss_u loss_u 0.8638 (0.8315) acc_u 15.6250 (20.6250) lr 7.6120e-05 eta 0:00:02
epoch [177/200] batch [55/55] time 0.455 (0.469) data 0.323 (0.338) loss_u loss_u 0.8105 (0.8304) acc_u 25.0000 (20.6250) lr 7.6120e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1400
confident_label rate tensor(0.4375, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1372
clean true:1371
clean false:1
clean_rate:0.999271137026239
noisy true:365
noisy false:1399
after delete: len(clean_dataset) 1372
after delete: len(noisy_dataset) 1764
epoch [178/200] batch [5/42] time 0.472 (0.483) data 0.342 (0.353) loss_x loss_x 0.6831 (0.9132) acc_x 84.3750 (75.6250) lr 7.0224e-05 eta 0:00:17
epoch [178/200] batch [10/42] time 0.596 (0.510) data 0.465 (0.379) loss_x loss_x 1.5596 (1.0600) acc_x 65.6250 (74.3750) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [15/42] time 0.402 (0.492) data 0.271 (0.361) loss_x loss_x 0.7817 (1.0155) acc_x 84.3750 (75.6250) lr 7.0224e-05 eta 0:00:13
epoch [178/200] batch [20/42] time 0.606 (0.505) data 0.475 (0.374) loss_x loss_x 1.3252 (1.0868) acc_x 65.6250 (73.1250) lr 7.0224e-05 eta 0:00:11
epoch [178/200] batch [25/42] time 0.489 (0.503) data 0.358 (0.372) loss_x loss_x 1.3008 (1.1087) acc_x 59.3750 (72.3750) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [30/42] time 0.434 (0.489) data 0.302 (0.358) loss_x loss_x 1.0176 (1.1232) acc_x 75.0000 (72.1875) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [35/42] time 0.471 (0.491) data 0.340 (0.360) loss_x loss_x 1.5000 (1.1292) acc_x 65.6250 (71.7857) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [40/42] time 0.551 (0.486) data 0.421 (0.355) loss_x loss_x 1.2021 (1.1341) acc_x 81.2500 (72.2656) lr 7.0224e-05 eta 0:00:00
epoch [178/200] batch [5/55] time 0.463 (0.484) data 0.332 (0.354) loss_u loss_u 0.7744 (0.7858) acc_u 28.1250 (26.2500) lr 7.0224e-05 eta 0:00:24
epoch [178/200] batch [10/55] time 0.449 (0.483) data 0.317 (0.352) loss_u loss_u 0.8398 (0.8111) acc_u 18.7500 (22.5000) lr 7.0224e-05 eta 0:00:21
epoch [178/200] batch [15/55] time 0.499 (0.480) data 0.367 (0.349) loss_u loss_u 0.9087 (0.8153) acc_u 12.5000 (22.0833) lr 7.0224e-05 eta 0:00:19
epoch [178/200] batch [20/55] time 0.433 (0.475) data 0.302 (0.344) loss_u loss_u 0.8032 (0.8221) acc_u 25.0000 (21.8750) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [25/55] time 0.557 (0.475) data 0.426 (0.344) loss_u loss_u 0.7598 (0.8187) acc_u 34.3750 (22.3750) lr 7.0224e-05 eta 0:00:14
epoch [178/200] batch [30/55] time 0.370 (0.472) data 0.239 (0.341) loss_u loss_u 0.7646 (0.8199) acc_u 28.1250 (21.7708) lr 7.0224e-05 eta 0:00:11
epoch [178/200] batch [35/55] time 0.440 (0.471) data 0.309 (0.340) loss_u loss_u 0.8438 (0.8214) acc_u 18.7500 (21.8750) lr 7.0224e-05 eta 0:00:09
epoch [178/200] batch [40/55] time 0.420 (0.469) data 0.289 (0.338) loss_u loss_u 0.9253 (0.8221) acc_u 6.2500 (21.6406) lr 7.0224e-05 eta 0:00:07
epoch [178/200] batch [45/55] time 0.429 (0.471) data 0.297 (0.340) loss_u loss_u 0.8672 (0.8223) acc_u 18.7500 (21.8750) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [50/55] time 0.441 (0.470) data 0.309 (0.339) loss_u loss_u 0.8564 (0.8236) acc_u 15.6250 (21.9375) lr 7.0224e-05 eta 0:00:02
epoch [178/200] batch [55/55] time 0.508 (0.467) data 0.377 (0.336) loss_u loss_u 0.9062 (0.8252) acc_u 18.7500 (21.9318) lr 7.0224e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1391
confident_label rate tensor(0.4445, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1394
clean true:1391
clean false:3
clean_rate:0.9978479196556671
noisy true:354
noisy false:1388
after delete: len(clean_dataset) 1394
after delete: len(noisy_dataset) 1742
epoch [179/200] batch [5/43] time 0.538 (0.469) data 0.407 (0.338) loss_x loss_x 1.0938 (1.1861) acc_x 71.8750 (71.2500) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [10/43] time 0.396 (0.453) data 0.265 (0.322) loss_x loss_x 1.0479 (1.1651) acc_x 78.1250 (72.1875) lr 6.4556e-05 eta 0:00:14
epoch [179/200] batch [15/43] time 0.385 (0.461) data 0.254 (0.330) loss_x loss_x 1.6133 (1.1689) acc_x 62.5000 (72.9167) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [20/43] time 0.459 (0.456) data 0.328 (0.325) loss_x loss_x 0.8174 (1.1287) acc_x 81.2500 (73.1250) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [25/43] time 0.459 (0.459) data 0.328 (0.328) loss_x loss_x 0.9590 (1.1011) acc_x 75.0000 (73.6250) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [30/43] time 0.412 (0.470) data 0.280 (0.340) loss_x loss_x 1.3115 (1.0926) acc_x 75.0000 (73.9583) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [35/43] time 0.486 (0.474) data 0.356 (0.344) loss_x loss_x 1.5342 (1.1182) acc_x 59.3750 (72.9464) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [40/43] time 0.505 (0.471) data 0.375 (0.340) loss_x loss_x 1.3535 (1.1241) acc_x 62.5000 (72.6562) lr 6.4556e-05 eta 0:00:01
epoch [179/200] batch [5/54] time 0.417 (0.461) data 0.285 (0.330) loss_u loss_u 0.8647 (0.8433) acc_u 18.7500 (21.2500) lr 6.4556e-05 eta 0:00:22
epoch [179/200] batch [10/54] time 0.443 (0.459) data 0.312 (0.329) loss_u loss_u 0.8301 (0.8377) acc_u 18.7500 (21.2500) lr 6.4556e-05 eta 0:00:20
epoch [179/200] batch [15/54] time 0.488 (0.460) data 0.357 (0.329) loss_u loss_u 0.8833 (0.8321) acc_u 15.6250 (21.4583) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [20/54] time 0.409 (0.458) data 0.279 (0.327) loss_u loss_u 0.8281 (0.8231) acc_u 21.8750 (22.0312) lr 6.4556e-05 eta 0:00:15
epoch [179/200] batch [25/54] time 0.366 (0.465) data 0.235 (0.334) loss_u loss_u 0.8696 (0.8241) acc_u 21.8750 (22.1250) lr 6.4556e-05 eta 0:00:13
epoch [179/200] batch [30/54] time 0.466 (0.466) data 0.334 (0.335) loss_u loss_u 0.8467 (0.8244) acc_u 18.7500 (21.8750) lr 6.4556e-05 eta 0:00:11
epoch [179/200] batch [35/54] time 0.414 (0.466) data 0.283 (0.335) loss_u loss_u 0.8174 (0.8266) acc_u 25.0000 (21.2500) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [40/54] time 0.317 (0.462) data 0.185 (0.331) loss_u loss_u 0.8677 (0.8307) acc_u 15.6250 (20.8594) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [45/54] time 0.437 (0.459) data 0.306 (0.328) loss_u loss_u 0.8755 (0.8322) acc_u 15.6250 (20.7639) lr 6.4556e-05 eta 0:00:04
epoch [179/200] batch [50/54] time 0.376 (0.456) data 0.244 (0.325) loss_u loss_u 0.8164 (0.8345) acc_u 25.0000 (20.6250) lr 6.4556e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1426
confident_label rate tensor(0.4330, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1358
clean true:1356
clean false:2
clean_rate:0.9985272459499264
noisy true:354
noisy false:1424
after delete: len(clean_dataset) 1358
after delete: len(noisy_dataset) 1778
epoch [180/200] batch [5/42] time 0.598 (0.520) data 0.466 (0.390) loss_x loss_x 1.3154 (1.1781) acc_x 65.6250 (69.3750) lr 5.9119e-05 eta 0:00:19
epoch [180/200] batch [10/42] time 0.440 (0.484) data 0.310 (0.354) loss_x loss_x 1.1289 (1.1104) acc_x 68.7500 (71.5625) lr 5.9119e-05 eta 0:00:15
epoch [180/200] batch [15/42] time 0.391 (0.460) data 0.259 (0.330) loss_x loss_x 1.3291 (1.1326) acc_x 56.2500 (69.3750) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [20/42] time 0.582 (0.461) data 0.451 (0.331) loss_x loss_x 0.6025 (1.1247) acc_x 93.7500 (71.2500) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [25/42] time 0.455 (0.481) data 0.324 (0.351) loss_x loss_x 0.9399 (1.1127) acc_x 81.2500 (72.0000) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [30/42] time 0.466 (0.480) data 0.335 (0.349) loss_x loss_x 1.4961 (1.1673) acc_x 62.5000 (71.1458) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [35/42] time 0.396 (0.476) data 0.266 (0.345) loss_x loss_x 1.3223 (1.1583) acc_x 71.8750 (71.5179) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [40/42] time 0.373 (0.468) data 0.242 (0.337) loss_x loss_x 1.4834 (1.1563) acc_x 59.3750 (71.5625) lr 5.9119e-05 eta 0:00:00
epoch [180/200] batch [5/55] time 0.411 (0.464) data 0.278 (0.333) loss_u loss_u 0.8315 (0.8354) acc_u 25.0000 (20.0000) lr 5.9119e-05 eta 0:00:23
epoch [180/200] batch [10/55] time 0.426 (0.464) data 0.295 (0.333) loss_u loss_u 0.8770 (0.8416) acc_u 15.6250 (20.3125) lr 5.9119e-05 eta 0:00:20
epoch [180/200] batch [15/55] time 0.559 (0.465) data 0.428 (0.334) loss_u loss_u 0.6538 (0.8267) acc_u 43.7500 (22.2917) lr 5.9119e-05 eta 0:00:18
epoch [180/200] batch [20/55] time 0.383 (0.464) data 0.252 (0.334) loss_u loss_u 0.8120 (0.8238) acc_u 12.5000 (21.2500) lr 5.9119e-05 eta 0:00:16
epoch [180/200] batch [25/55] time 0.430 (0.465) data 0.299 (0.334) loss_u loss_u 0.8438 (0.8251) acc_u 21.8750 (21.6250) lr 5.9119e-05 eta 0:00:13
epoch [180/200] batch [30/55] time 0.487 (0.466) data 0.355 (0.336) loss_u loss_u 0.8608 (0.8299) acc_u 15.6250 (21.0417) lr 5.9119e-05 eta 0:00:11
epoch [180/200] batch [35/55] time 0.475 (0.466) data 0.344 (0.335) loss_u loss_u 0.7949 (0.8255) acc_u 25.0000 (21.3393) lr 5.9119e-05 eta 0:00:09
epoch [180/200] batch [40/55] time 0.402 (0.463) data 0.270 (0.332) loss_u loss_u 0.9043 (0.8242) acc_u 12.5000 (21.8750) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [45/55] time 0.604 (0.463) data 0.473 (0.332) loss_u loss_u 0.7480 (0.8245) acc_u 31.2500 (21.7361) lr 5.9119e-05 eta 0:00:04
epoch [180/200] batch [50/55] time 0.616 (0.464) data 0.485 (0.333) loss_u loss_u 0.9126 (0.8276) acc_u 15.6250 (21.5000) lr 5.9119e-05 eta 0:00:02
epoch [180/200] batch [55/55] time 0.478 (0.463) data 0.348 (0.332) loss_u loss_u 0.7842 (0.8285) acc_u 31.2500 (21.5909) lr 5.9119e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1439
confident_label rate tensor(0.4263, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1337
clean true:1335
clean false:2
clean_rate:0.9985041136873598
noisy true:362
noisy false:1437
after delete: len(clean_dataset) 1337
after delete: len(noisy_dataset) 1799
epoch [181/200] batch [5/41] time 0.413 (0.442) data 0.283 (0.311) loss_x loss_x 1.4902 (1.2797) acc_x 62.5000 (66.8750) lr 5.3915e-05 eta 0:00:15
epoch [181/200] batch [10/41] time 0.517 (0.451) data 0.386 (0.320) loss_x loss_x 0.9365 (1.1632) acc_x 78.1250 (70.9375) lr 5.3915e-05 eta 0:00:13
epoch [181/200] batch [15/41] time 0.406 (0.442) data 0.275 (0.311) loss_x loss_x 1.1562 (1.1188) acc_x 71.8750 (71.4583) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [20/41] time 0.545 (0.451) data 0.414 (0.320) loss_x loss_x 1.2539 (1.1424) acc_x 65.6250 (71.0938) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [25/41] time 0.573 (0.462) data 0.443 (0.331) loss_x loss_x 0.9009 (1.1205) acc_x 84.3750 (71.7500) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [30/41] time 0.435 (0.458) data 0.304 (0.327) loss_x loss_x 1.2500 (1.1262) acc_x 68.7500 (71.5625) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [35/41] time 0.522 (0.458) data 0.391 (0.327) loss_x loss_x 0.8301 (1.1474) acc_x 81.2500 (71.4286) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [40/41] time 0.392 (0.460) data 0.262 (0.330) loss_x loss_x 1.2256 (1.1625) acc_x 78.1250 (71.4844) lr 5.3915e-05 eta 0:00:00
epoch [181/200] batch [5/56] time 0.479 (0.459) data 0.348 (0.329) loss_u loss_u 0.7959 (0.8511) acc_u 21.8750 (16.2500) lr 5.3915e-05 eta 0:00:23
epoch [181/200] batch [10/56] time 0.414 (0.460) data 0.283 (0.329) loss_u loss_u 0.8662 (0.8362) acc_u 18.7500 (19.6875) lr 5.3915e-05 eta 0:00:21
epoch [181/200] batch [15/56] time 0.467 (0.464) data 0.336 (0.334) loss_u loss_u 0.7441 (0.8269) acc_u 34.3750 (22.0833) lr 5.3915e-05 eta 0:00:19
epoch [181/200] batch [20/56] time 0.396 (0.459) data 0.266 (0.329) loss_u loss_u 0.7939 (0.8280) acc_u 28.1250 (21.8750) lr 5.3915e-05 eta 0:00:16
epoch [181/200] batch [25/56] time 0.383 (0.464) data 0.252 (0.333) loss_u loss_u 0.8594 (0.8301) acc_u 12.5000 (20.7500) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [30/56] time 0.427 (0.462) data 0.295 (0.332) loss_u loss_u 0.8398 (0.8248) acc_u 25.0000 (21.5625) lr 5.3915e-05 eta 0:00:12
epoch [181/200] batch [35/56] time 0.553 (0.463) data 0.421 (0.332) loss_u loss_u 0.8174 (0.8303) acc_u 18.7500 (20.5357) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [40/56] time 0.529 (0.462) data 0.398 (0.331) loss_u loss_u 0.9009 (0.8314) acc_u 15.6250 (20.7031) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [45/56] time 0.502 (0.465) data 0.372 (0.334) loss_u loss_u 0.7534 (0.8306) acc_u 31.2500 (20.9722) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [50/56] time 0.516 (0.465) data 0.384 (0.334) loss_u loss_u 0.7280 (0.8204) acc_u 34.3750 (22.2500) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [55/56] time 0.440 (0.465) data 0.309 (0.334) loss_u loss_u 0.8057 (0.8201) acc_u 28.1250 (22.4432) lr 5.3915e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1455
confident_label rate tensor(0.4254, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1334
clean true:1331
clean false:3
clean_rate:0.9977511244377811
noisy true:350
noisy false:1452
after delete: len(clean_dataset) 1334
after delete: len(noisy_dataset) 1802
epoch [182/200] batch [5/41] time 0.613 (0.498) data 0.481 (0.367) loss_x loss_x 0.8882 (1.2425) acc_x 75.0000 (66.8750) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [10/41] time 0.387 (0.467) data 0.256 (0.336) loss_x loss_x 1.1006 (1.2292) acc_x 75.0000 (69.6875) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [15/41] time 0.432 (0.481) data 0.302 (0.350) loss_x loss_x 0.6255 (1.1433) acc_x 81.2500 (71.2500) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [20/41] time 0.382 (0.489) data 0.252 (0.358) loss_x loss_x 0.8467 (1.1189) acc_x 78.1250 (72.3438) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [25/41] time 0.532 (0.483) data 0.401 (0.352) loss_x loss_x 1.7920 (1.1216) acc_x 62.5000 (72.6250) lr 4.8943e-05 eta 0:00:07
epoch [182/200] batch [30/41] time 0.390 (0.470) data 0.260 (0.339) loss_x loss_x 1.2207 (1.1273) acc_x 62.5000 (72.2917) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [35/41] time 0.623 (0.475) data 0.493 (0.345) loss_x loss_x 1.3135 (1.1202) acc_x 68.7500 (72.0536) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [40/41] time 0.422 (0.479) data 0.292 (0.348) loss_x loss_x 1.2441 (1.1519) acc_x 65.6250 (71.3281) lr 4.8943e-05 eta 0:00:00
epoch [182/200] batch [5/56] time 0.344 (0.480) data 0.213 (0.349) loss_u loss_u 0.8374 (0.8319) acc_u 15.6250 (21.2500) lr 4.8943e-05 eta 0:00:24
epoch [182/200] batch [10/56] time 0.580 (0.487) data 0.448 (0.356) loss_u loss_u 0.8970 (0.8444) acc_u 12.5000 (18.7500) lr 4.8943e-05 eta 0:00:22
epoch [182/200] batch [15/56] time 0.432 (0.481) data 0.300 (0.350) loss_u loss_u 0.8345 (0.8406) acc_u 21.8750 (19.1667) lr 4.8943e-05 eta 0:00:19
epoch [182/200] batch [20/56] time 0.487 (0.482) data 0.356 (0.351) loss_u loss_u 0.8696 (0.8420) acc_u 18.7500 (19.2188) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [25/56] time 0.440 (0.478) data 0.310 (0.347) loss_u loss_u 0.8384 (0.8428) acc_u 21.8750 (19.5000) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [30/56] time 0.450 (0.473) data 0.318 (0.342) loss_u loss_u 0.7593 (0.8324) acc_u 31.2500 (20.9375) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [35/56] time 0.378 (0.469) data 0.246 (0.338) loss_u loss_u 0.7974 (0.8321) acc_u 28.1250 (21.3393) lr 4.8943e-05 eta 0:00:09
epoch [182/200] batch [40/56] time 0.570 (0.469) data 0.438 (0.338) loss_u loss_u 0.8472 (0.8309) acc_u 18.7500 (21.4062) lr 4.8943e-05 eta 0:00:07
epoch [182/200] batch [45/56] time 0.423 (0.466) data 0.291 (0.335) loss_u loss_u 0.8779 (0.8324) acc_u 15.6250 (21.0417) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [50/56] time 0.757 (0.470) data 0.625 (0.339) loss_u loss_u 0.8379 (0.8331) acc_u 25.0000 (21.0000) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [55/56] time 0.449 (0.468) data 0.317 (0.337) loss_u loss_u 0.8120 (0.8353) acc_u 21.8750 (20.9091) lr 4.8943e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1408
confident_label rate tensor(0.4378, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1373
clean true:1371
clean false:2
clean_rate:0.9985433357611071
noisy true:357
noisy false:1406
after delete: len(clean_dataset) 1373
after delete: len(noisy_dataset) 1763
epoch [183/200] batch [5/42] time 0.446 (0.467) data 0.315 (0.336) loss_x loss_x 1.1387 (1.0119) acc_x 68.7500 (72.5000) lr 4.4207e-05 eta 0:00:17
epoch [183/200] batch [10/42] time 0.440 (0.455) data 0.308 (0.324) loss_x loss_x 1.1982 (1.2331) acc_x 65.6250 (69.0625) lr 4.4207e-05 eta 0:00:14
epoch [183/200] batch [15/42] time 0.592 (0.466) data 0.461 (0.335) loss_x loss_x 1.1865 (1.2593) acc_x 59.3750 (67.9167) lr 4.4207e-05 eta 0:00:12
epoch [183/200] batch [20/42] time 0.418 (0.468) data 0.288 (0.337) loss_x loss_x 1.1025 (1.2177) acc_x 71.8750 (69.0625) lr 4.4207e-05 eta 0:00:10
epoch [183/200] batch [25/42] time 0.443 (0.460) data 0.312 (0.329) loss_x loss_x 1.4336 (1.2074) acc_x 71.8750 (69.0000) lr 4.4207e-05 eta 0:00:07
epoch [183/200] batch [30/42] time 0.404 (0.457) data 0.273 (0.326) loss_x loss_x 1.1328 (1.1703) acc_x 68.7500 (70.3125) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [35/42] time 0.509 (0.462) data 0.378 (0.331) loss_x loss_x 0.9155 (1.1384) acc_x 78.1250 (71.4286) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [40/42] time 0.455 (0.460) data 0.325 (0.329) loss_x loss_x 1.4395 (1.1591) acc_x 71.8750 (71.0938) lr 4.4207e-05 eta 0:00:00
epoch [183/200] batch [5/55] time 0.544 (0.459) data 0.412 (0.328) loss_u loss_u 0.8906 (0.8454) acc_u 9.3750 (19.3750) lr 4.4207e-05 eta 0:00:22
epoch [183/200] batch [10/55] time 0.413 (0.456) data 0.280 (0.325) loss_u loss_u 0.7856 (0.8367) acc_u 21.8750 (20.0000) lr 4.4207e-05 eta 0:00:20
epoch [183/200] batch [15/55] time 0.637 (0.461) data 0.506 (0.330) loss_u loss_u 0.8657 (0.8519) acc_u 15.6250 (18.3333) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [20/55] time 0.393 (0.468) data 0.261 (0.337) loss_u loss_u 0.8291 (0.8388) acc_u 18.7500 (19.5312) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [25/55] time 0.490 (0.466) data 0.358 (0.334) loss_u loss_u 0.7754 (0.8347) acc_u 31.2500 (20.3750) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [30/55] time 0.406 (0.465) data 0.274 (0.333) loss_u loss_u 0.8901 (0.8324) acc_u 9.3750 (20.6250) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [35/55] time 0.382 (0.466) data 0.250 (0.334) loss_u loss_u 0.8125 (0.8378) acc_u 31.2500 (20.2679) lr 4.4207e-05 eta 0:00:09
epoch [183/200] batch [40/55] time 0.491 (0.467) data 0.360 (0.336) loss_u loss_u 0.8442 (0.8312) acc_u 18.7500 (20.6250) lr 4.4207e-05 eta 0:00:07
epoch [183/200] batch [45/55] time 0.338 (0.467) data 0.206 (0.336) loss_u loss_u 0.8979 (0.8334) acc_u 15.6250 (20.3472) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [50/55] time 0.439 (0.463) data 0.307 (0.332) loss_u loss_u 0.6660 (0.8316) acc_u 43.7500 (20.8125) lr 4.4207e-05 eta 0:00:02
epoch [183/200] batch [55/55] time 0.402 (0.460) data 0.270 (0.329) loss_u loss_u 0.8989 (0.8321) acc_u 12.5000 (20.6250) lr 4.4207e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1421
confident_label rate tensor(0.4394, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1378
clean true:1375
clean false:3
clean_rate:0.997822931785196
noisy true:340
noisy false:1418
after delete: len(clean_dataset) 1378
after delete: len(noisy_dataset) 1758
epoch [184/200] batch [5/43] time 0.462 (0.491) data 0.331 (0.361) loss_x loss_x 0.6685 (0.9755) acc_x 81.2500 (73.1250) lr 3.9706e-05 eta 0:00:18
epoch [184/200] batch [10/43] time 0.403 (0.469) data 0.272 (0.338) loss_x loss_x 0.8730 (1.0746) acc_x 75.0000 (70.3125) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [15/43] time 0.409 (0.446) data 0.277 (0.315) loss_x loss_x 1.3652 (1.1570) acc_x 62.5000 (69.1667) lr 3.9706e-05 eta 0:00:12
epoch [184/200] batch [20/43] time 0.598 (0.451) data 0.467 (0.319) loss_x loss_x 0.7163 (1.1697) acc_x 81.2500 (70.0000) lr 3.9706e-05 eta 0:00:10
epoch [184/200] batch [25/43] time 0.310 (0.457) data 0.178 (0.326) loss_x loss_x 1.6523 (1.1917) acc_x 50.0000 (69.3750) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [30/43] time 0.552 (0.456) data 0.421 (0.324) loss_x loss_x 1.5732 (1.2042) acc_x 50.0000 (68.7500) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [35/43] time 0.507 (0.465) data 0.376 (0.334) loss_x loss_x 1.4043 (1.2079) acc_x 68.7500 (69.0179) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [40/43] time 0.503 (0.462) data 0.372 (0.331) loss_x loss_x 1.2725 (1.2139) acc_x 75.0000 (69.0625) lr 3.9706e-05 eta 0:00:01
epoch [184/200] batch [5/54] time 0.390 (0.452) data 0.258 (0.320) loss_u loss_u 0.8325 (0.8001) acc_u 28.1250 (26.8750) lr 3.9706e-05 eta 0:00:22
epoch [184/200] batch [10/54] time 0.520 (0.452) data 0.389 (0.321) loss_u loss_u 0.7803 (0.8067) acc_u 34.3750 (26.2500) lr 3.9706e-05 eta 0:00:19
epoch [184/200] batch [15/54] time 0.397 (0.450) data 0.265 (0.318) loss_u loss_u 0.8408 (0.8115) acc_u 25.0000 (25.2083) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [20/54] time 0.405 (0.450) data 0.273 (0.319) loss_u loss_u 0.8271 (0.8115) acc_u 21.8750 (24.5312) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [25/54] time 0.419 (0.450) data 0.286 (0.319) loss_u loss_u 0.9224 (0.8181) acc_u 9.3750 (23.0000) lr 3.9706e-05 eta 0:00:13
epoch [184/200] batch [30/54] time 0.452 (0.450) data 0.319 (0.318) loss_u loss_u 0.8379 (0.8207) acc_u 21.8750 (22.9167) lr 3.9706e-05 eta 0:00:10
epoch [184/200] batch [35/54] time 0.338 (0.450) data 0.206 (0.319) loss_u loss_u 0.8657 (0.8226) acc_u 21.8750 (22.5893) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [40/54] time 0.459 (0.449) data 0.328 (0.318) loss_u loss_u 0.7866 (0.8196) acc_u 31.2500 (22.8906) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [45/54] time 0.350 (0.449) data 0.219 (0.317) loss_u loss_u 0.8550 (0.8220) acc_u 15.6250 (22.4306) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [50/54] time 0.456 (0.450) data 0.324 (0.318) loss_u loss_u 0.7817 (0.8265) acc_u 25.0000 (21.8750) lr 3.9706e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1426
confident_label rate tensor(0.4279, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1342
clean true:1342
clean false:0
clean_rate:1.0
noisy true:368
noisy false:1426
after delete: len(clean_dataset) 1342
after delete: len(noisy_dataset) 1794
epoch [185/200] batch [5/41] time 0.408 (0.417) data 0.278 (0.286) loss_x loss_x 1.0840 (1.2207) acc_x 84.3750 (66.8750) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [10/41] time 0.486 (0.436) data 0.355 (0.305) loss_x loss_x 0.8574 (1.1600) acc_x 81.2500 (69.0625) lr 3.5443e-05 eta 0:00:13
epoch [185/200] batch [15/41] time 0.365 (0.449) data 0.234 (0.318) loss_x loss_x 0.7939 (1.0898) acc_x 78.1250 (70.0000) lr 3.5443e-05 eta 0:00:11
epoch [185/200] batch [20/41] time 0.420 (0.456) data 0.290 (0.325) loss_x loss_x 0.9185 (1.0930) acc_x 71.8750 (70.1562) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [25/41] time 0.533 (0.463) data 0.401 (0.332) loss_x loss_x 1.2217 (1.1104) acc_x 75.0000 (70.2500) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [30/41] time 0.416 (0.461) data 0.285 (0.330) loss_x loss_x 0.8838 (1.1201) acc_x 78.1250 (70.2083) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [35/41] time 0.386 (0.457) data 0.256 (0.326) loss_x loss_x 1.4668 (1.1313) acc_x 59.3750 (69.9107) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [40/41] time 0.515 (0.464) data 0.385 (0.332) loss_x loss_x 1.0215 (1.1256) acc_x 81.2500 (70.5469) lr 3.5443e-05 eta 0:00:00
epoch [185/200] batch [5/56] time 0.426 (0.463) data 0.295 (0.332) loss_u loss_u 0.8179 (0.8462) acc_u 21.8750 (20.0000) lr 3.5443e-05 eta 0:00:23
epoch [185/200] batch [10/56] time 0.393 (0.464) data 0.262 (0.333) loss_u loss_u 0.8184 (0.8419) acc_u 21.8750 (18.7500) lr 3.5443e-05 eta 0:00:21
epoch [185/200] batch [15/56] time 0.350 (0.460) data 0.219 (0.329) loss_u loss_u 0.8291 (0.8419) acc_u 21.8750 (19.7917) lr 3.5443e-05 eta 0:00:18
epoch [185/200] batch [20/56] time 0.550 (0.459) data 0.419 (0.328) loss_u loss_u 0.8086 (0.8365) acc_u 25.0000 (20.3125) lr 3.5443e-05 eta 0:00:16
epoch [185/200] batch [25/56] time 0.413 (0.460) data 0.281 (0.329) loss_u loss_u 0.8140 (0.8442) acc_u 25.0000 (19.8750) lr 3.5443e-05 eta 0:00:14
epoch [185/200] batch [30/56] time 0.388 (0.460) data 0.257 (0.329) loss_u loss_u 0.7832 (0.8373) acc_u 31.2500 (20.8333) lr 3.5443e-05 eta 0:00:11
epoch [185/200] batch [35/56] time 0.476 (0.460) data 0.345 (0.329) loss_u loss_u 0.7983 (0.8340) acc_u 21.8750 (21.4286) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [40/56] time 0.433 (0.459) data 0.302 (0.328) loss_u loss_u 0.7979 (0.8295) acc_u 28.1250 (22.1875) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [45/56] time 0.489 (0.459) data 0.359 (0.328) loss_u loss_u 0.7744 (0.8270) acc_u 21.8750 (22.3611) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [50/56] time 0.534 (0.459) data 0.402 (0.327) loss_u loss_u 0.8594 (0.8264) acc_u 18.7500 (22.1875) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [55/56] time 0.448 (0.461) data 0.317 (0.330) loss_u loss_u 0.7256 (0.8235) acc_u 37.5000 (22.5000) lr 3.5443e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1377
confident_label rate tensor(0.4452, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1396
clean true:1394
clean false:2
clean_rate:0.998567335243553
noisy true:365
noisy false:1375
after delete: len(clean_dataset) 1396
after delete: len(noisy_dataset) 1740
epoch [186/200] batch [5/43] time 0.489 (0.468) data 0.359 (0.338) loss_x loss_x 0.7891 (1.1131) acc_x 81.2500 (73.1250) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [10/43] time 0.495 (0.478) data 0.364 (0.348) loss_x loss_x 0.9653 (1.1154) acc_x 78.1250 (72.1875) lr 3.1417e-05 eta 0:00:15
epoch [186/200] batch [15/43] time 0.545 (0.492) data 0.411 (0.360) loss_x loss_x 1.0273 (1.0800) acc_x 68.7500 (72.5000) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [20/43] time 0.453 (0.506) data 0.323 (0.374) loss_x loss_x 0.8740 (1.0683) acc_x 71.8750 (72.9688) lr 3.1417e-05 eta 0:00:11
epoch [186/200] batch [25/43] time 0.401 (0.492) data 0.271 (0.360) loss_x loss_x 0.9775 (1.0639) acc_x 75.0000 (73.0000) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [30/43] time 0.401 (0.483) data 0.271 (0.352) loss_x loss_x 0.8042 (1.0571) acc_x 78.1250 (73.1250) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [35/43] time 0.493 (0.480) data 0.363 (0.349) loss_x loss_x 1.4150 (1.0861) acc_x 62.5000 (72.7679) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [40/43] time 0.455 (0.484) data 0.324 (0.353) loss_x loss_x 0.7007 (1.0699) acc_x 81.2500 (72.8125) lr 3.1417e-05 eta 0:00:01
epoch [186/200] batch [5/54] time 0.455 (0.485) data 0.324 (0.354) loss_u loss_u 0.8481 (0.8418) acc_u 21.8750 (23.1250) lr 3.1417e-05 eta 0:00:23
epoch [186/200] batch [10/54] time 0.409 (0.483) data 0.277 (0.352) loss_u loss_u 0.7837 (0.8532) acc_u 25.0000 (20.3125) lr 3.1417e-05 eta 0:00:21
epoch [186/200] batch [15/54] time 0.369 (0.487) data 0.238 (0.356) loss_u loss_u 0.8887 (0.8554) acc_u 12.5000 (19.5833) lr 3.1417e-05 eta 0:00:19
epoch [186/200] batch [20/54] time 0.445 (0.483) data 0.313 (0.352) loss_u loss_u 0.7896 (0.8392) acc_u 25.0000 (21.2500) lr 3.1417e-05 eta 0:00:16
epoch [186/200] batch [25/54] time 0.444 (0.481) data 0.312 (0.350) loss_u loss_u 0.7832 (0.8330) acc_u 31.2500 (22.1250) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [30/54] time 0.479 (0.480) data 0.347 (0.349) loss_u loss_u 0.9419 (0.8347) acc_u 6.2500 (21.8750) lr 3.1417e-05 eta 0:00:11
epoch [186/200] batch [35/54] time 0.401 (0.478) data 0.269 (0.347) loss_u loss_u 0.7979 (0.8317) acc_u 25.0000 (21.9643) lr 3.1417e-05 eta 0:00:09
epoch [186/200] batch [40/54] time 0.783 (0.481) data 0.651 (0.350) loss_u loss_u 0.8198 (0.8284) acc_u 21.8750 (22.5000) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [45/54] time 0.434 (0.479) data 0.303 (0.348) loss_u loss_u 0.8921 (0.8282) acc_u 9.3750 (22.3611) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [50/54] time 0.648 (0.481) data 0.517 (0.350) loss_u loss_u 0.8745 (0.8302) acc_u 18.7500 (22.0625) lr 3.1417e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.4263, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1337
clean true:1334
clean false:3
clean_rate:0.9977561705310396
noisy true:361
noisy false:1438
after delete: len(clean_dataset) 1337
after delete: len(noisy_dataset) 1799
epoch [187/200] batch [5/41] time 0.578 (0.494) data 0.447 (0.364) loss_x loss_x 1.2061 (1.0846) acc_x 65.6250 (71.8750) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [10/41] time 0.553 (0.484) data 0.422 (0.354) loss_x loss_x 0.9746 (1.0380) acc_x 71.8750 (74.6875) lr 2.7630e-05 eta 0:00:15
epoch [187/200] batch [15/41] time 0.628 (0.499) data 0.497 (0.368) loss_x loss_x 1.1680 (1.0964) acc_x 78.1250 (73.1250) lr 2.7630e-05 eta 0:00:12
epoch [187/200] batch [20/41] time 0.457 (0.497) data 0.327 (0.366) loss_x loss_x 1.3076 (1.1564) acc_x 62.5000 (71.7188) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [25/41] time 0.451 (0.485) data 0.320 (0.355) loss_x loss_x 1.1650 (1.1445) acc_x 75.0000 (72.2500) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [30/41] time 0.477 (0.479) data 0.346 (0.348) loss_x loss_x 1.4092 (1.1843) acc_x 68.7500 (71.1458) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [35/41] time 0.430 (0.475) data 0.299 (0.345) loss_x loss_x 1.0479 (1.1833) acc_x 75.0000 (71.4286) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [40/41] time 0.451 (0.482) data 0.319 (0.351) loss_x loss_x 1.4961 (1.1725) acc_x 65.6250 (71.3281) lr 2.7630e-05 eta 0:00:00
epoch [187/200] batch [5/56] time 0.471 (0.482) data 0.340 (0.351) loss_u loss_u 0.8311 (0.8257) acc_u 21.8750 (23.7500) lr 2.7630e-05 eta 0:00:24
epoch [187/200] batch [10/56] time 0.386 (0.477) data 0.255 (0.345) loss_u loss_u 0.8374 (0.8178) acc_u 15.6250 (24.0625) lr 2.7630e-05 eta 0:00:21
epoch [187/200] batch [15/56] time 0.525 (0.473) data 0.393 (0.342) loss_u loss_u 0.7783 (0.8224) acc_u 28.1250 (23.7500) lr 2.7630e-05 eta 0:00:19
epoch [187/200] batch [20/56] time 0.510 (0.474) data 0.380 (0.343) loss_u loss_u 0.8989 (0.8350) acc_u 12.5000 (21.7188) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [25/56] time 0.477 (0.472) data 0.345 (0.341) loss_u loss_u 0.8110 (0.8326) acc_u 21.8750 (21.8750) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [30/56] time 0.637 (0.482) data 0.506 (0.351) loss_u loss_u 0.8706 (0.8375) acc_u 12.5000 (21.0417) lr 2.7630e-05 eta 0:00:12
epoch [187/200] batch [35/56] time 0.493 (0.477) data 0.361 (0.346) loss_u loss_u 0.8511 (0.8360) acc_u 18.7500 (21.2500) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [40/56] time 0.535 (0.478) data 0.404 (0.347) loss_u loss_u 0.8115 (0.8332) acc_u 25.0000 (21.5625) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [45/56] time 0.362 (0.477) data 0.230 (0.345) loss_u loss_u 0.8198 (0.8330) acc_u 25.0000 (21.7361) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [50/56] time 0.439 (0.480) data 0.308 (0.349) loss_u loss_u 0.8438 (0.8300) acc_u 18.7500 (22.1250) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [55/56] time 0.462 (0.476) data 0.329 (0.345) loss_u loss_u 0.8105 (0.8263) acc_u 18.7500 (22.2159) lr 2.7630e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1459
confident_label rate tensor(0.4254, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1334
clean true:1333
clean false:1
clean_rate:0.9992503748125937
noisy true:344
noisy false:1458
after delete: len(clean_dataset) 1334
after delete: len(noisy_dataset) 1802
epoch [188/200] batch [5/41] time 0.551 (0.449) data 0.420 (0.318) loss_x loss_x 1.3125 (1.0002) acc_x 75.0000 (78.7500) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [10/41] time 0.443 (0.432) data 0.312 (0.301) loss_x loss_x 1.1445 (1.0499) acc_x 78.1250 (75.6250) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [15/41] time 0.549 (0.442) data 0.417 (0.311) loss_x loss_x 1.4023 (1.0368) acc_x 59.3750 (74.3750) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [20/41] time 0.510 (0.464) data 0.377 (0.333) loss_x loss_x 0.8931 (1.0556) acc_x 75.0000 (74.0625) lr 2.4083e-05 eta 0:00:09
epoch [188/200] batch [25/41] time 0.602 (0.468) data 0.471 (0.337) loss_x loss_x 1.4434 (1.0726) acc_x 59.3750 (74.1250) lr 2.4083e-05 eta 0:00:07
epoch [188/200] batch [30/41] time 0.371 (0.465) data 0.239 (0.334) loss_x loss_x 0.7129 (1.0753) acc_x 84.3750 (74.3750) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [35/41] time 0.449 (0.465) data 0.318 (0.334) loss_x loss_x 0.9482 (1.0823) acc_x 78.1250 (74.0179) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [40/41] time 0.524 (0.462) data 0.394 (0.331) loss_x loss_x 0.8101 (1.0781) acc_x 75.0000 (74.2969) lr 2.4083e-05 eta 0:00:00
epoch [188/200] batch [5/56] time 0.476 (0.471) data 0.345 (0.340) loss_u loss_u 0.9238 (0.8589) acc_u 15.6250 (19.3750) lr 2.4083e-05 eta 0:00:24
epoch [188/200] batch [10/56] time 0.342 (0.475) data 0.211 (0.344) loss_u loss_u 0.8062 (0.8482) acc_u 21.8750 (20.6250) lr 2.4083e-05 eta 0:00:21
epoch [188/200] batch [15/56] time 0.423 (0.467) data 0.292 (0.336) loss_u loss_u 0.9111 (0.8351) acc_u 15.6250 (21.8750) lr 2.4083e-05 eta 0:00:19
epoch [188/200] batch [20/56] time 0.621 (0.470) data 0.490 (0.339) loss_u loss_u 0.8291 (0.8368) acc_u 21.8750 (21.2500) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [25/56] time 0.424 (0.469) data 0.292 (0.338) loss_u loss_u 0.8135 (0.8346) acc_u 25.0000 (21.3750) lr 2.4083e-05 eta 0:00:14
epoch [188/200] batch [30/56] time 0.441 (0.469) data 0.310 (0.338) loss_u loss_u 0.7319 (0.8288) acc_u 28.1250 (21.9792) lr 2.4083e-05 eta 0:00:12
epoch [188/200] batch [35/56] time 0.426 (0.466) data 0.295 (0.335) loss_u loss_u 0.8062 (0.8297) acc_u 25.0000 (21.9643) lr 2.4083e-05 eta 0:00:09
epoch [188/200] batch [40/56] time 0.530 (0.465) data 0.399 (0.334) loss_u loss_u 0.8760 (0.8274) acc_u 12.5000 (22.1875) lr 2.4083e-05 eta 0:00:07
epoch [188/200] batch [45/56] time 0.477 (0.463) data 0.346 (0.332) loss_u loss_u 0.8711 (0.8269) acc_u 12.5000 (22.2222) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [50/56] time 0.540 (0.463) data 0.409 (0.332) loss_u loss_u 0.8008 (0.8272) acc_u 25.0000 (22.0000) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [55/56] time 0.422 (0.462) data 0.290 (0.331) loss_u loss_u 0.9097 (0.8289) acc_u 12.5000 (21.5909) lr 2.4083e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1453
confident_label rate tensor(0.4244, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1331
clean true:1330
clean false:1
clean_rate:0.9992486851990984
noisy true:353
noisy false:1452
after delete: len(clean_dataset) 1331
after delete: len(noisy_dataset) 1805
epoch [189/200] batch [5/41] time 0.370 (0.437) data 0.239 (0.306) loss_x loss_x 0.9570 (1.1883) acc_x 75.0000 (70.6250) lr 2.0777e-05 eta 0:00:15
epoch [189/200] batch [10/41] time 0.412 (0.436) data 0.282 (0.305) loss_x loss_x 1.3301 (1.1540) acc_x 75.0000 (71.8750) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [15/41] time 0.482 (0.451) data 0.351 (0.320) loss_x loss_x 1.9414 (1.2297) acc_x 56.2500 (71.4583) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [20/41] time 0.518 (0.448) data 0.386 (0.317) loss_x loss_x 1.1211 (1.1686) acc_x 68.7500 (72.0312) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [25/41] time 0.473 (0.448) data 0.342 (0.317) loss_x loss_x 0.9521 (1.1459) acc_x 75.0000 (71.3750) lr 2.0777e-05 eta 0:00:07
epoch [189/200] batch [30/41] time 0.344 (0.446) data 0.214 (0.315) loss_x loss_x 1.4590 (1.1414) acc_x 59.3750 (71.1458) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [35/41] time 0.650 (0.454) data 0.519 (0.323) loss_x loss_x 0.9604 (1.1157) acc_x 65.6250 (71.4286) lr 2.0777e-05 eta 0:00:02
epoch [189/200] batch [40/41] time 0.434 (0.453) data 0.304 (0.322) loss_x loss_x 1.0654 (1.1223) acc_x 81.2500 (71.5625) lr 2.0777e-05 eta 0:00:00
epoch [189/200] batch [5/56] time 0.466 (0.465) data 0.335 (0.334) loss_u loss_u 0.7773 (0.8243) acc_u 34.3750 (23.7500) lr 2.0777e-05 eta 0:00:23
epoch [189/200] batch [10/56] time 0.385 (0.460) data 0.255 (0.329) loss_u loss_u 0.8345 (0.8254) acc_u 21.8750 (23.7500) lr 2.0777e-05 eta 0:00:21
epoch [189/200] batch [15/56] time 0.603 (0.460) data 0.472 (0.330) loss_u loss_u 0.8052 (0.8247) acc_u 18.7500 (22.5000) lr 2.0777e-05 eta 0:00:18
epoch [189/200] batch [20/56] time 0.463 (0.458) data 0.332 (0.327) loss_u loss_u 0.7993 (0.8175) acc_u 21.8750 (23.2812) lr 2.0777e-05 eta 0:00:16
epoch [189/200] batch [25/56] time 0.368 (0.453) data 0.236 (0.322) loss_u loss_u 0.7734 (0.8129) acc_u 25.0000 (23.7500) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [30/56] time 0.565 (0.454) data 0.433 (0.323) loss_u loss_u 0.8462 (0.8178) acc_u 18.7500 (23.1250) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [35/56] time 0.416 (0.453) data 0.284 (0.322) loss_u loss_u 0.8672 (0.8232) acc_u 21.8750 (22.5000) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [40/56] time 0.466 (0.454) data 0.335 (0.323) loss_u loss_u 0.8540 (0.8232) acc_u 15.6250 (22.4219) lr 2.0777e-05 eta 0:00:07
epoch [189/200] batch [45/56] time 0.490 (0.456) data 0.359 (0.325) loss_u loss_u 0.8394 (0.8260) acc_u 25.0000 (22.1528) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [50/56] time 0.413 (0.452) data 0.282 (0.321) loss_u loss_u 0.8188 (0.8283) acc_u 21.8750 (21.9375) lr 2.0777e-05 eta 0:00:02
epoch [189/200] batch [55/56] time 0.405 (0.452) data 0.274 (0.321) loss_u loss_u 0.7935 (0.8251) acc_u 25.0000 (22.3295) lr 2.0777e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1464
confident_label rate tensor(0.4190, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1314
clean true:1313
clean false:1
clean_rate:0.9992389649923896
noisy true:359
noisy false:1463
after delete: len(clean_dataset) 1314
after delete: len(noisy_dataset) 1822
epoch [190/200] batch [5/41] time 0.414 (0.475) data 0.283 (0.344) loss_x loss_x 1.0469 (1.3430) acc_x 68.7500 (70.0000) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [10/41] time 0.438 (0.457) data 0.307 (0.326) loss_x loss_x 1.6221 (1.2980) acc_x 56.2500 (70.9375) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [15/41] time 0.447 (0.471) data 0.316 (0.340) loss_x loss_x 1.4131 (1.2430) acc_x 71.8750 (72.9167) lr 1.7713e-05 eta 0:00:12
epoch [190/200] batch [20/41] time 0.421 (0.465) data 0.291 (0.335) loss_x loss_x 1.0869 (1.2242) acc_x 68.7500 (72.9688) lr 1.7713e-05 eta 0:00:09
epoch [190/200] batch [25/41] time 0.515 (0.456) data 0.385 (0.325) loss_x loss_x 0.8433 (1.2022) acc_x 87.5000 (72.8750) lr 1.7713e-05 eta 0:00:07
epoch [190/200] batch [30/41] time 0.423 (0.463) data 0.292 (0.332) loss_x loss_x 1.2080 (1.1877) acc_x 68.7500 (72.8125) lr 1.7713e-05 eta 0:00:05
epoch [190/200] batch [35/41] time 0.503 (0.464) data 0.373 (0.334) loss_x loss_x 1.4453 (1.1470) acc_x 62.5000 (73.3036) lr 1.7713e-05 eta 0:00:02
epoch [190/200] batch [40/41] time 0.440 (0.461) data 0.309 (0.330) loss_x loss_x 1.3174 (1.1499) acc_x 62.5000 (72.8906) lr 1.7713e-05 eta 0:00:00
epoch [190/200] batch [5/56] time 0.434 (0.460) data 0.303 (0.329) loss_u loss_u 0.7959 (0.8219) acc_u 25.0000 (18.7500) lr 1.7713e-05 eta 0:00:23
epoch [190/200] batch [10/56] time 0.445 (0.458) data 0.314 (0.327) loss_u loss_u 0.7681 (0.8039) acc_u 37.5000 (22.5000) lr 1.7713e-05 eta 0:00:21
epoch [190/200] batch [15/56] time 0.413 (0.456) data 0.283 (0.325) loss_u loss_u 0.8306 (0.8111) acc_u 18.7500 (22.7083) lr 1.7713e-05 eta 0:00:18
epoch [190/200] batch [20/56] time 0.539 (0.456) data 0.407 (0.325) loss_u loss_u 0.7935 (0.8041) acc_u 25.0000 (23.7500) lr 1.7713e-05 eta 0:00:16
epoch [190/200] batch [25/56] time 0.460 (0.454) data 0.329 (0.323) loss_u loss_u 0.8877 (0.8062) acc_u 12.5000 (23.3750) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [30/56] time 0.375 (0.451) data 0.244 (0.320) loss_u loss_u 0.8853 (0.8120) acc_u 12.5000 (23.1250) lr 1.7713e-05 eta 0:00:11
epoch [190/200] batch [35/56] time 0.441 (0.451) data 0.310 (0.320) loss_u loss_u 0.7954 (0.8154) acc_u 28.1250 (22.9464) lr 1.7713e-05 eta 0:00:09
epoch [190/200] batch [40/56] time 0.391 (0.452) data 0.258 (0.321) loss_u loss_u 0.7837 (0.8166) acc_u 21.8750 (22.8125) lr 1.7713e-05 eta 0:00:07
epoch [190/200] batch [45/56] time 0.417 (0.452) data 0.286 (0.321) loss_u loss_u 0.8955 (0.8178) acc_u 15.6250 (22.7778) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [50/56] time 0.357 (0.453) data 0.227 (0.322) loss_u loss_u 0.9155 (0.8190) acc_u 9.3750 (22.2500) lr 1.7713e-05 eta 0:00:02
epoch [190/200] batch [55/56] time 0.504 (0.453) data 0.374 (0.322) loss_u loss_u 0.7876 (0.8203) acc_u 28.1250 (22.0455) lr 1.7713e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1436
confident_label rate tensor(0.4349, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1364
clean true:1362
clean false:2
clean_rate:0.998533724340176
noisy true:338
noisy false:1434
after delete: len(clean_dataset) 1364
after delete: len(noisy_dataset) 1772
epoch [191/200] batch [5/42] time 0.415 (0.491) data 0.284 (0.360) loss_x loss_x 1.4502 (1.1173) acc_x 68.7500 (71.8750) lr 1.4891e-05 eta 0:00:18
epoch [191/200] batch [10/42] time 0.494 (0.481) data 0.364 (0.350) loss_x loss_x 1.2217 (1.0771) acc_x 75.0000 (75.0000) lr 1.4891e-05 eta 0:00:15
epoch [191/200] batch [15/42] time 0.523 (0.480) data 0.391 (0.349) loss_x loss_x 1.0889 (1.0767) acc_x 71.8750 (74.5833) lr 1.4891e-05 eta 0:00:12
epoch [191/200] batch [20/42] time 0.515 (0.468) data 0.384 (0.337) loss_x loss_x 1.3633 (1.1231) acc_x 62.5000 (72.9688) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [25/42] time 0.533 (0.469) data 0.403 (0.339) loss_x loss_x 1.2656 (1.1361) acc_x 65.6250 (72.6250) lr 1.4891e-05 eta 0:00:07
epoch [191/200] batch [30/42] time 0.405 (0.462) data 0.274 (0.331) loss_x loss_x 0.8140 (1.1562) acc_x 78.1250 (72.7083) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [35/42] time 0.430 (0.467) data 0.300 (0.336) loss_x loss_x 1.2246 (1.1645) acc_x 65.6250 (72.1429) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [40/42] time 0.501 (0.469) data 0.370 (0.339) loss_x loss_x 1.0400 (1.1480) acc_x 68.7500 (71.9531) lr 1.4891e-05 eta 0:00:00
epoch [191/200] batch [5/55] time 0.433 (0.476) data 0.302 (0.345) loss_u loss_u 0.8447 (0.8377) acc_u 21.8750 (21.8750) lr 1.4891e-05 eta 0:00:23
epoch [191/200] batch [10/55] time 0.485 (0.476) data 0.353 (0.345) loss_u loss_u 0.8691 (0.8325) acc_u 18.7500 (22.5000) lr 1.4891e-05 eta 0:00:21
epoch [191/200] batch [15/55] time 0.382 (0.473) data 0.251 (0.342) loss_u loss_u 0.8691 (0.8310) acc_u 15.6250 (21.4583) lr 1.4891e-05 eta 0:00:18
epoch [191/200] batch [20/55] time 0.424 (0.470) data 0.292 (0.339) loss_u loss_u 0.8203 (0.8316) acc_u 21.8750 (21.0938) lr 1.4891e-05 eta 0:00:16
epoch [191/200] batch [25/55] time 0.358 (0.466) data 0.227 (0.335) loss_u loss_u 0.8394 (0.8339) acc_u 21.8750 (21.2500) lr 1.4891e-05 eta 0:00:13
epoch [191/200] batch [30/55] time 0.388 (0.465) data 0.256 (0.334) loss_u loss_u 0.8066 (0.8326) acc_u 18.7500 (21.3542) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [35/55] time 0.505 (0.463) data 0.373 (0.332) loss_u loss_u 0.8066 (0.8314) acc_u 21.8750 (21.6071) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [40/55] time 0.373 (0.463) data 0.242 (0.332) loss_u loss_u 0.7866 (0.8278) acc_u 28.1250 (22.3438) lr 1.4891e-05 eta 0:00:06
epoch [191/200] batch [45/55] time 0.405 (0.460) data 0.273 (0.329) loss_u loss_u 0.7627 (0.8269) acc_u 28.1250 (22.5000) lr 1.4891e-05 eta 0:00:04
epoch [191/200] batch [50/55] time 0.467 (0.461) data 0.337 (0.330) loss_u loss_u 0.7173 (0.8278) acc_u 31.2500 (22.2500) lr 1.4891e-05 eta 0:00:02
epoch [191/200] batch [55/55] time 0.418 (0.463) data 0.286 (0.332) loss_u loss_u 0.8164 (0.8280) acc_u 25.0000 (22.2159) lr 1.4891e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1414
confident_label rate tensor(0.4372, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1371
clean true:1370
clean false:1
clean_rate:0.9992706053975201
noisy true:352
noisy false:1413
after delete: len(clean_dataset) 1371
after delete: len(noisy_dataset) 1765
epoch [192/200] batch [5/42] time 0.446 (0.481) data 0.315 (0.350) loss_x loss_x 1.3096 (1.4477) acc_x 65.6250 (65.0000) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [10/42] time 0.545 (0.471) data 0.414 (0.340) loss_x loss_x 1.3418 (1.2703) acc_x 59.3750 (69.3750) lr 1.2312e-05 eta 0:00:15
epoch [192/200] batch [15/42] time 0.506 (0.478) data 0.375 (0.347) loss_x loss_x 1.4404 (1.2620) acc_x 62.5000 (68.1250) lr 1.2312e-05 eta 0:00:12
epoch [192/200] batch [20/42] time 0.471 (0.473) data 0.341 (0.342) loss_x loss_x 1.2002 (1.2025) acc_x 75.0000 (70.4688) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [25/42] time 0.474 (0.470) data 0.344 (0.339) loss_x loss_x 1.4297 (1.2157) acc_x 65.6250 (70.0000) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [30/42] time 0.503 (0.467) data 0.373 (0.336) loss_x loss_x 1.2588 (1.2428) acc_x 75.0000 (69.5833) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [35/42] time 0.519 (0.475) data 0.389 (0.344) loss_x loss_x 1.1484 (1.2339) acc_x 75.0000 (70.2679) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [40/42] time 0.417 (0.474) data 0.286 (0.344) loss_x loss_x 1.1592 (1.2182) acc_x 65.6250 (70.3125) lr 1.2312e-05 eta 0:00:00
epoch [192/200] batch [5/55] time 0.672 (0.476) data 0.541 (0.346) loss_u loss_u 0.7812 (0.8102) acc_u 31.2500 (24.3750) lr 1.2312e-05 eta 0:00:23
epoch [192/200] batch [10/55] time 0.356 (0.473) data 0.225 (0.342) loss_u loss_u 0.8564 (0.8283) acc_u 15.6250 (22.5000) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [15/55] time 0.368 (0.470) data 0.235 (0.340) loss_u loss_u 0.7607 (0.8215) acc_u 31.2500 (23.3333) lr 1.2312e-05 eta 0:00:18
epoch [192/200] batch [20/55] time 0.428 (0.464) data 0.297 (0.333) loss_u loss_u 0.7559 (0.8200) acc_u 28.1250 (23.2812) lr 1.2312e-05 eta 0:00:16
epoch [192/200] batch [25/55] time 0.396 (0.462) data 0.264 (0.331) loss_u loss_u 0.8643 (0.8292) acc_u 15.6250 (21.7500) lr 1.2312e-05 eta 0:00:13
epoch [192/200] batch [30/55] time 0.418 (0.469) data 0.286 (0.338) loss_u loss_u 0.8301 (0.8249) acc_u 28.1250 (22.7083) lr 1.2312e-05 eta 0:00:11
epoch [192/200] batch [35/55] time 0.359 (0.467) data 0.228 (0.336) loss_u loss_u 0.8530 (0.8295) acc_u 18.7500 (21.8750) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [40/55] time 0.489 (0.463) data 0.357 (0.333) loss_u loss_u 0.7695 (0.8260) acc_u 34.3750 (22.3438) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [45/55] time 0.470 (0.461) data 0.338 (0.330) loss_u loss_u 0.7510 (0.8254) acc_u 31.2500 (22.4306) lr 1.2312e-05 eta 0:00:04
epoch [192/200] batch [50/55] time 0.368 (0.459) data 0.237 (0.328) loss_u loss_u 0.8853 (0.8251) acc_u 15.6250 (22.3125) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [55/55] time 0.453 (0.458) data 0.322 (0.327) loss_u loss_u 0.8398 (0.8265) acc_u 15.6250 (21.8750) lr 1.2312e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1421
confident_label rate tensor(0.4353, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1365
clean true:1363
clean false:2
clean_rate:0.9985347985347985
noisy true:352
noisy false:1419
after delete: len(clean_dataset) 1365
after delete: len(noisy_dataset) 1771
epoch [193/200] batch [5/42] time 0.468 (0.469) data 0.338 (0.338) loss_x loss_x 0.9175 (0.9983) acc_x 71.8750 (77.5000) lr 9.9763e-06 eta 0:00:17
epoch [193/200] batch [10/42] time 0.432 (0.449) data 0.303 (0.319) loss_x loss_x 1.4814 (1.1156) acc_x 50.0000 (70.9375) lr 9.9763e-06 eta 0:00:14
epoch [193/200] batch [15/42] time 0.458 (0.455) data 0.328 (0.324) loss_x loss_x 1.3711 (1.1534) acc_x 59.3750 (69.5833) lr 9.9763e-06 eta 0:00:12
epoch [193/200] batch [20/42] time 0.475 (0.453) data 0.343 (0.322) loss_x loss_x 0.7124 (1.1242) acc_x 78.1250 (70.3125) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [25/42] time 0.419 (0.457) data 0.288 (0.326) loss_x loss_x 1.1289 (1.1195) acc_x 71.8750 (70.2500) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [30/42] time 0.399 (0.463) data 0.269 (0.333) loss_x loss_x 0.9536 (1.0812) acc_x 68.7500 (71.3542) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [35/42] time 0.398 (0.460) data 0.267 (0.329) loss_x loss_x 1.3428 (1.0910) acc_x 78.1250 (71.6071) lr 9.9763e-06 eta 0:00:03
epoch [193/200] batch [40/42] time 0.446 (0.458) data 0.315 (0.328) loss_x loss_x 1.2051 (1.0932) acc_x 81.2500 (71.9531) lr 9.9763e-06 eta 0:00:00
epoch [193/200] batch [5/55] time 0.682 (0.462) data 0.551 (0.331) loss_u loss_u 0.7881 (0.8127) acc_u 25.0000 (25.6250) lr 9.9763e-06 eta 0:00:23
epoch [193/200] batch [10/55] time 0.459 (0.462) data 0.328 (0.331) loss_u loss_u 0.8398 (0.8007) acc_u 18.7500 (26.8750) lr 9.9763e-06 eta 0:00:20
epoch [193/200] batch [15/55] time 0.391 (0.462) data 0.261 (0.331) loss_u loss_u 0.8418 (0.8156) acc_u 21.8750 (24.7917) lr 9.9763e-06 eta 0:00:18
epoch [193/200] batch [20/55] time 0.531 (0.465) data 0.399 (0.334) loss_u loss_u 0.7568 (0.8237) acc_u 34.3750 (23.5938) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [25/55] time 0.636 (0.462) data 0.505 (0.332) loss_u loss_u 0.7490 (0.8239) acc_u 28.1250 (23.2500) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [30/55] time 0.358 (0.460) data 0.227 (0.329) loss_u loss_u 0.7686 (0.8232) acc_u 28.1250 (22.9167) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [35/55] time 0.523 (0.464) data 0.391 (0.333) loss_u loss_u 0.8091 (0.8235) acc_u 15.6250 (22.7679) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [40/55] time 0.484 (0.464) data 0.353 (0.333) loss_u loss_u 0.8232 (0.8241) acc_u 21.8750 (22.5781) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [45/55] time 0.524 (0.461) data 0.393 (0.331) loss_u loss_u 0.8154 (0.8254) acc_u 15.6250 (22.0833) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [50/55] time 0.476 (0.460) data 0.344 (0.330) loss_u loss_u 0.8770 (0.8252) acc_u 15.6250 (22.0625) lr 9.9763e-06 eta 0:00:02
epoch [193/200] batch [55/55] time 0.417 (0.459) data 0.284 (0.328) loss_u loss_u 0.8530 (0.8238) acc_u 18.7500 (22.1591) lr 9.9763e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.4334, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1359
clean true:1357
clean false:2
clean_rate:0.9985283296541575
noisy true:345
noisy false:1432
after delete: len(clean_dataset) 1359
after delete: len(noisy_dataset) 1777
epoch [194/200] batch [5/42] time 0.370 (0.417) data 0.240 (0.286) loss_x loss_x 1.0898 (1.2756) acc_x 71.8750 (69.3750) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [10/42] time 0.454 (0.423) data 0.324 (0.293) loss_x loss_x 0.9507 (1.1052) acc_x 84.3750 (72.5000) lr 7.8853e-06 eta 0:00:13
epoch [194/200] batch [15/42] time 0.577 (0.458) data 0.446 (0.328) loss_x loss_x 1.3477 (1.1399) acc_x 71.8750 (72.2917) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [20/42] time 0.514 (0.461) data 0.383 (0.331) loss_x loss_x 1.1777 (1.1685) acc_x 65.6250 (72.0312) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [25/42] time 0.606 (0.466) data 0.476 (0.336) loss_x loss_x 0.7441 (1.1071) acc_x 90.6250 (74.1250) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [30/42] time 0.441 (0.466) data 0.310 (0.335) loss_x loss_x 0.9863 (1.1410) acc_x 75.0000 (73.9583) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [35/42] time 0.445 (0.464) data 0.314 (0.333) loss_x loss_x 1.0244 (1.1033) acc_x 71.8750 (74.2857) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [40/42] time 0.486 (0.468) data 0.356 (0.337) loss_x loss_x 1.5869 (1.1272) acc_x 65.6250 (73.8281) lr 7.8853e-06 eta 0:00:00
epoch [194/200] batch [5/55] time 0.411 (0.472) data 0.279 (0.342) loss_u loss_u 0.7739 (0.8683) acc_u 31.2500 (18.1250) lr 7.8853e-06 eta 0:00:23
epoch [194/200] batch [10/55] time 0.392 (0.470) data 0.261 (0.339) loss_u loss_u 0.8306 (0.8473) acc_u 15.6250 (18.4375) lr 7.8853e-06 eta 0:00:21
epoch [194/200] batch [15/55] time 0.558 (0.475) data 0.426 (0.344) loss_u loss_u 0.8296 (0.8331) acc_u 21.8750 (20.8333) lr 7.8853e-06 eta 0:00:18
epoch [194/200] batch [20/55] time 0.461 (0.480) data 0.330 (0.349) loss_u loss_u 0.8530 (0.8325) acc_u 18.7500 (20.7812) lr 7.8853e-06 eta 0:00:16
epoch [194/200] batch [25/55] time 0.352 (0.477) data 0.221 (0.346) loss_u loss_u 0.7466 (0.8330) acc_u 25.0000 (20.7500) lr 7.8853e-06 eta 0:00:14
epoch [194/200] batch [30/55] time 0.499 (0.477) data 0.367 (0.346) loss_u loss_u 0.8081 (0.8357) acc_u 21.8750 (20.3125) lr 7.8853e-06 eta 0:00:11
epoch [194/200] batch [35/55] time 0.562 (0.476) data 0.431 (0.345) loss_u loss_u 0.7964 (0.8327) acc_u 21.8750 (20.6250) lr 7.8853e-06 eta 0:00:09
epoch [194/200] batch [40/55] time 0.395 (0.471) data 0.263 (0.340) loss_u loss_u 0.8779 (0.8317) acc_u 15.6250 (20.7812) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [45/55] time 0.362 (0.469) data 0.231 (0.338) loss_u loss_u 0.7363 (0.8257) acc_u 34.3750 (21.7361) lr 7.8853e-06 eta 0:00:04
epoch [194/200] batch [50/55] time 0.410 (0.470) data 0.280 (0.339) loss_u loss_u 0.7939 (0.8217) acc_u 34.3750 (22.1875) lr 7.8853e-06 eta 0:00:02
epoch [194/200] batch [55/55] time 0.446 (0.468) data 0.314 (0.337) loss_u loss_u 0.8555 (0.8229) acc_u 25.0000 (22.2727) lr 7.8853e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1450
confident_label rate tensor(0.4260, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1336
clean true:1335
clean false:1
clean_rate:0.999251497005988
noisy true:351
noisy false:1449
after delete: len(clean_dataset) 1336
after delete: len(noisy_dataset) 1800
epoch [195/200] batch [5/41] time 0.455 (0.429) data 0.325 (0.299) loss_x loss_x 0.8062 (0.9836) acc_x 78.1250 (76.8750) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [10/41] time 0.509 (0.443) data 0.378 (0.312) loss_x loss_x 1.1328 (1.0649) acc_x 68.7500 (73.7500) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [15/41] time 0.575 (0.479) data 0.444 (0.348) loss_x loss_x 0.9673 (1.0146) acc_x 71.8750 (73.9583) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [20/41] time 0.376 (0.467) data 0.245 (0.336) loss_x loss_x 1.2363 (1.0050) acc_x 71.8750 (74.3750) lr 6.0390e-06 eta 0:00:09
epoch [195/200] batch [25/41] time 0.473 (0.463) data 0.342 (0.332) loss_x loss_x 1.1025 (1.0429) acc_x 75.0000 (74.3750) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [30/41] time 0.372 (0.461) data 0.242 (0.330) loss_x loss_x 1.1768 (1.0337) acc_x 71.8750 (74.5833) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [35/41] time 0.563 (0.466) data 0.432 (0.335) loss_x loss_x 1.4053 (1.0460) acc_x 56.2500 (74.0179) lr 6.0390e-06 eta 0:00:02
epoch [195/200] batch [40/41] time 0.468 (0.472) data 0.337 (0.341) loss_x loss_x 1.3613 (1.0576) acc_x 56.2500 (72.7344) lr 6.0390e-06 eta 0:00:00
epoch [195/200] batch [5/56] time 0.416 (0.466) data 0.285 (0.336) loss_u loss_u 0.7808 (0.8437) acc_u 25.0000 (17.5000) lr 6.0390e-06 eta 0:00:23
epoch [195/200] batch [10/56] time 0.400 (0.464) data 0.268 (0.333) loss_u loss_u 0.7832 (0.8186) acc_u 25.0000 (22.1875) lr 6.0390e-06 eta 0:00:21
epoch [195/200] batch [15/56] time 0.411 (0.460) data 0.280 (0.329) loss_u loss_u 0.8315 (0.8242) acc_u 21.8750 (22.5000) lr 6.0390e-06 eta 0:00:18
epoch [195/200] batch [20/56] time 0.455 (0.462) data 0.325 (0.331) loss_u loss_u 0.7822 (0.8205) acc_u 31.2500 (22.5000) lr 6.0390e-06 eta 0:00:16
epoch [195/200] batch [25/56] time 0.466 (0.465) data 0.335 (0.334) loss_u loss_u 0.8081 (0.8187) acc_u 28.1250 (22.6250) lr 6.0390e-06 eta 0:00:14
epoch [195/200] batch [30/56] time 0.461 (0.464) data 0.330 (0.333) loss_u loss_u 0.8379 (0.8237) acc_u 18.7500 (21.9792) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [35/56] time 0.388 (0.465) data 0.257 (0.334) loss_u loss_u 0.7896 (0.8212) acc_u 28.1250 (22.1429) lr 6.0390e-06 eta 0:00:09
epoch [195/200] batch [40/56] time 0.359 (0.466) data 0.227 (0.335) loss_u loss_u 0.8101 (0.8192) acc_u 25.0000 (22.6562) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [45/56] time 0.463 (0.470) data 0.333 (0.339) loss_u loss_u 0.8711 (0.8225) acc_u 15.6250 (22.1528) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [50/56] time 0.526 (0.468) data 0.394 (0.337) loss_u loss_u 0.8682 (0.8215) acc_u 15.6250 (22.2500) lr 6.0390e-06 eta 0:00:02
epoch [195/200] batch [55/56] time 0.495 (0.465) data 0.365 (0.334) loss_u loss_u 0.8320 (0.8174) acc_u 18.7500 (22.8977) lr 6.0390e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.4302, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1349
clean true:1347
clean false:2
clean_rate:0.9985174203113417
noisy true:348
noisy false:1439
after delete: len(clean_dataset) 1349
after delete: len(noisy_dataset) 1787
epoch [196/200] batch [5/42] time 0.464 (0.466) data 0.334 (0.335) loss_x loss_x 1.2695 (1.1265) acc_x 71.8750 (70.6250) lr 4.4380e-06 eta 0:00:17
epoch [196/200] batch [10/42] time 0.424 (0.462) data 0.293 (0.331) loss_x loss_x 1.0488 (1.1138) acc_x 75.0000 (73.4375) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [15/42] time 0.517 (0.459) data 0.387 (0.328) loss_x loss_x 1.1084 (1.1031) acc_x 71.8750 (73.1250) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [20/42] time 0.458 (0.466) data 0.327 (0.335) loss_x loss_x 1.0566 (1.1463) acc_x 78.1250 (71.8750) lr 4.4380e-06 eta 0:00:10
epoch [196/200] batch [25/42] time 0.615 (0.477) data 0.483 (0.346) loss_x loss_x 1.4199 (1.1443) acc_x 71.8750 (72.7500) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [30/42] time 0.372 (0.476) data 0.241 (0.344) loss_x loss_x 1.0293 (1.1184) acc_x 71.8750 (73.4375) lr 4.4380e-06 eta 0:00:05
epoch [196/200] batch [35/42] time 0.462 (0.474) data 0.331 (0.342) loss_x loss_x 1.0215 (1.1385) acc_x 78.1250 (73.1250) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [40/42] time 0.580 (0.475) data 0.449 (0.343) loss_x loss_x 0.8809 (1.1243) acc_x 75.0000 (73.0469) lr 4.4380e-06 eta 0:00:00
epoch [196/200] batch [5/55] time 0.388 (0.478) data 0.258 (0.347) loss_u loss_u 0.8608 (0.8490) acc_u 21.8750 (23.1250) lr 4.4380e-06 eta 0:00:23
epoch [196/200] batch [10/55] time 0.385 (0.476) data 0.255 (0.345) loss_u loss_u 0.8506 (0.8392) acc_u 15.6250 (21.5625) lr 4.4380e-06 eta 0:00:21
epoch [196/200] batch [15/55] time 0.355 (0.468) data 0.224 (0.337) loss_u loss_u 0.8633 (0.8338) acc_u 21.8750 (21.8750) lr 4.4380e-06 eta 0:00:18
epoch [196/200] batch [20/55] time 0.398 (0.466) data 0.268 (0.335) loss_u loss_u 0.7607 (0.8280) acc_u 31.2500 (22.8125) lr 4.4380e-06 eta 0:00:16
epoch [196/200] batch [25/55] time 0.365 (0.461) data 0.234 (0.330) loss_u loss_u 0.9136 (0.8350) acc_u 6.2500 (21.6250) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [30/55] time 0.517 (0.462) data 0.386 (0.331) loss_u loss_u 0.7852 (0.8335) acc_u 28.1250 (21.3542) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [35/55] time 0.586 (0.464) data 0.455 (0.333) loss_u loss_u 0.8931 (0.8303) acc_u 12.5000 (21.8750) lr 4.4380e-06 eta 0:00:09
epoch [196/200] batch [40/55] time 0.525 (0.463) data 0.394 (0.332) loss_u loss_u 0.8662 (0.8284) acc_u 18.7500 (21.7969) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [45/55] time 0.845 (0.468) data 0.715 (0.337) loss_u loss_u 0.9126 (0.8292) acc_u 9.3750 (21.5972) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [50/55] time 0.455 (0.465) data 0.324 (0.334) loss_u loss_u 0.8809 (0.8290) acc_u 15.6250 (21.7500) lr 4.4380e-06 eta 0:00:02
epoch [196/200] batch [55/55] time 0.365 (0.462) data 0.233 (0.331) loss_u loss_u 0.8350 (0.8278) acc_u 21.8750 (21.8750) lr 4.4380e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1448
confident_label rate tensor(0.4232, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1327
clean true:1324
clean false:3
clean_rate:0.9977392614920875
noisy true:364
noisy false:1445
after delete: len(clean_dataset) 1327
after delete: len(noisy_dataset) 1809
epoch [197/200] batch [5/41] time 0.620 (0.531) data 0.488 (0.400) loss_x loss_x 0.9180 (0.8514) acc_x 75.0000 (80.0000) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [10/41] time 0.549 (0.490) data 0.418 (0.360) loss_x loss_x 1.0215 (0.9661) acc_x 78.1250 (76.5625) lr 3.0827e-06 eta 0:00:15
epoch [197/200] batch [15/41] time 0.570 (0.484) data 0.439 (0.354) loss_x loss_x 1.2793 (1.0016) acc_x 59.3750 (75.6250) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [20/41] time 0.470 (0.492) data 0.339 (0.362) loss_x loss_x 0.7876 (1.0383) acc_x 75.0000 (75.0000) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [25/41] time 0.371 (0.488) data 0.240 (0.357) loss_x loss_x 0.8105 (1.0392) acc_x 75.0000 (75.1250) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [30/41] time 0.432 (0.491) data 0.301 (0.360) loss_x loss_x 1.1787 (1.0650) acc_x 62.5000 (74.3750) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [35/41] time 0.526 (0.488) data 0.394 (0.357) loss_x loss_x 0.7197 (1.0625) acc_x 81.2500 (74.0179) lr 3.0827e-06 eta 0:00:02
epoch [197/200] batch [40/41] time 0.432 (0.489) data 0.301 (0.358) loss_x loss_x 1.6123 (1.1131) acc_x 62.5000 (72.7344) lr 3.0827e-06 eta 0:00:00
epoch [197/200] batch [5/56] time 0.386 (0.481) data 0.255 (0.350) loss_u loss_u 0.9370 (0.8342) acc_u 6.2500 (20.6250) lr 3.0827e-06 eta 0:00:24
epoch [197/200] batch [10/56] time 0.381 (0.477) data 0.249 (0.346) loss_u loss_u 0.8638 (0.8437) acc_u 21.8750 (21.5625) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [15/56] time 0.443 (0.477) data 0.311 (0.346) loss_u loss_u 0.8608 (0.8306) acc_u 18.7500 (22.0833) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [20/56] time 0.354 (0.471) data 0.223 (0.340) loss_u loss_u 0.9077 (0.8354) acc_u 15.6250 (21.4062) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [25/56] time 0.543 (0.468) data 0.411 (0.337) loss_u loss_u 0.8774 (0.8319) acc_u 15.6250 (21.7500) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [30/56] time 0.681 (0.470) data 0.550 (0.339) loss_u loss_u 0.8428 (0.8292) acc_u 15.6250 (22.1875) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [35/56] time 0.504 (0.471) data 0.373 (0.340) loss_u loss_u 0.7583 (0.8324) acc_u 28.1250 (21.4286) lr 3.0827e-06 eta 0:00:09
epoch [197/200] batch [40/56] time 0.562 (0.470) data 0.431 (0.338) loss_u loss_u 0.7778 (0.8313) acc_u 28.1250 (21.6406) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [45/56] time 0.428 (0.468) data 0.297 (0.337) loss_u loss_u 0.8789 (0.8320) acc_u 12.5000 (21.2500) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [50/56] time 0.480 (0.469) data 0.349 (0.338) loss_u loss_u 0.7534 (0.8265) acc_u 31.2500 (22.1875) lr 3.0827e-06 eta 0:00:02
epoch [197/200] batch [55/56] time 0.464 (0.468) data 0.333 (0.337) loss_u loss_u 0.7983 (0.8259) acc_u 25.0000 (22.1591) lr 3.0827e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1458
confident_label rate tensor(0.4260, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1336
clean true:1334
clean false:2
clean_rate:0.9985029940119761
noisy true:344
noisy false:1456
after delete: len(clean_dataset) 1336
after delete: len(noisy_dataset) 1800
epoch [198/200] batch [5/41] time 0.444 (0.436) data 0.314 (0.307) loss_x loss_x 1.6816 (1.4354) acc_x 62.5000 (66.8750) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [10/41] time 0.398 (0.446) data 0.268 (0.316) loss_x loss_x 0.9932 (1.2081) acc_x 75.0000 (71.2500) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [15/41] time 0.466 (0.452) data 0.335 (0.322) loss_x loss_x 0.7881 (1.1400) acc_x 84.3750 (72.9167) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [20/41] time 0.558 (0.469) data 0.428 (0.339) loss_x loss_x 0.5913 (1.1161) acc_x 78.1250 (73.1250) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [25/41] time 0.399 (0.453) data 0.269 (0.322) loss_x loss_x 1.0908 (1.1174) acc_x 71.8750 (72.7500) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [30/41] time 0.450 (0.446) data 0.320 (0.316) loss_x loss_x 1.7461 (1.1288) acc_x 59.3750 (73.1250) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [35/41] time 0.507 (0.451) data 0.377 (0.321) loss_x loss_x 1.1914 (1.1074) acc_x 68.7500 (73.3929) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [40/41] time 0.451 (0.448) data 0.320 (0.318) loss_x loss_x 0.6973 (1.1114) acc_x 81.2500 (72.7344) lr 1.9733e-06 eta 0:00:00
epoch [198/200] batch [5/56] time 0.364 (0.444) data 0.232 (0.314) loss_u loss_u 0.7822 (0.8296) acc_u 31.2500 (22.5000) lr 1.9733e-06 eta 0:00:22
epoch [198/200] batch [10/56] time 0.377 (0.442) data 0.247 (0.311) loss_u loss_u 0.8354 (0.8225) acc_u 21.8750 (23.1250) lr 1.9733e-06 eta 0:00:20
epoch [198/200] batch [15/56] time 0.494 (0.444) data 0.363 (0.314) loss_u loss_u 0.8657 (0.8204) acc_u 15.6250 (23.1250) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [20/56] time 0.580 (0.444) data 0.449 (0.314) loss_u loss_u 0.8320 (0.8255) acc_u 25.0000 (22.1875) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [25/56] time 0.402 (0.443) data 0.271 (0.312) loss_u loss_u 0.8701 (0.8268) acc_u 18.7500 (22.8750) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [30/56] time 0.591 (0.446) data 0.458 (0.315) loss_u loss_u 0.8374 (0.8204) acc_u 25.0000 (23.5417) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [35/56] time 0.450 (0.450) data 0.318 (0.319) loss_u loss_u 0.8052 (0.8214) acc_u 21.8750 (22.9464) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [40/56] time 0.724 (0.455) data 0.592 (0.324) loss_u loss_u 0.7979 (0.8211) acc_u 18.7500 (23.1250) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [45/56] time 0.446 (0.454) data 0.315 (0.323) loss_u loss_u 0.7246 (0.8184) acc_u 34.3750 (23.7500) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [50/56] time 0.351 (0.454) data 0.220 (0.324) loss_u loss_u 0.8696 (0.8180) acc_u 21.8750 (23.9375) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [55/56] time 0.687 (0.455) data 0.556 (0.324) loss_u loss_u 0.8384 (0.8166) acc_u 18.7500 (23.9773) lr 1.9733e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1408
confident_label rate tensor(0.4362, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1368
clean true:1364
clean false:4
clean_rate:0.9970760233918129
noisy true:364
noisy false:1404
after delete: len(clean_dataset) 1368
after delete: len(noisy_dataset) 1768
epoch [199/200] batch [5/42] time 0.516 (0.512) data 0.385 (0.381) loss_x loss_x 1.0820 (0.9734) acc_x 71.8750 (75.0000) lr 1.1101e-06 eta 0:00:18
epoch [199/200] batch [10/42] time 0.510 (0.484) data 0.379 (0.353) loss_x loss_x 1.1016 (1.1270) acc_x 62.5000 (71.2500) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [15/42] time 0.397 (0.485) data 0.265 (0.354) loss_x loss_x 1.0859 (1.1805) acc_x 62.5000 (68.9583) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [20/42] time 0.440 (0.481) data 0.310 (0.349) loss_x loss_x 1.0186 (1.1508) acc_x 68.7500 (69.5312) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [25/42] time 0.376 (0.476) data 0.244 (0.344) loss_x loss_x 1.1445 (1.1267) acc_x 71.8750 (70.3750) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [30/42] time 0.379 (0.471) data 0.248 (0.340) loss_x loss_x 1.2051 (1.1271) acc_x 75.0000 (70.6250) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [35/42] time 0.352 (0.471) data 0.221 (0.339) loss_x loss_x 1.1533 (1.1343) acc_x 65.6250 (70.4464) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [40/42] time 0.581 (0.476) data 0.451 (0.344) loss_x loss_x 1.4287 (1.1395) acc_x 75.0000 (70.8594) lr 1.1101e-06 eta 0:00:00
epoch [199/200] batch [5/55] time 0.348 (0.473) data 0.216 (0.341) loss_u loss_u 0.8306 (0.8561) acc_u 15.6250 (18.1250) lr 1.1101e-06 eta 0:00:23
epoch [199/200] batch [10/55] time 0.374 (0.466) data 0.242 (0.334) loss_u loss_u 0.7666 (0.8266) acc_u 31.2500 (22.5000) lr 1.1101e-06 eta 0:00:20
epoch [199/200] batch [15/55] time 0.446 (0.466) data 0.315 (0.335) loss_u loss_u 0.8262 (0.8154) acc_u 25.0000 (24.1667) lr 1.1101e-06 eta 0:00:18
epoch [199/200] batch [20/55] time 0.508 (0.468) data 0.377 (0.336) loss_u loss_u 0.7725 (0.8157) acc_u 25.0000 (22.9688) lr 1.1101e-06 eta 0:00:16
epoch [199/200] batch [25/55] time 0.647 (0.472) data 0.515 (0.340) loss_u loss_u 0.7842 (0.8221) acc_u 28.1250 (22.2500) lr 1.1101e-06 eta 0:00:14
epoch [199/200] batch [30/55] time 0.421 (0.469) data 0.290 (0.338) loss_u loss_u 0.8994 (0.8243) acc_u 15.6250 (21.8750) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [35/55] time 0.656 (0.469) data 0.525 (0.338) loss_u loss_u 0.8315 (0.8254) acc_u 21.8750 (21.7857) lr 1.1101e-06 eta 0:00:09
epoch [199/200] batch [40/55] time 0.501 (0.465) data 0.370 (0.334) loss_u loss_u 0.8774 (0.8211) acc_u 15.6250 (22.0312) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [45/55] time 0.557 (0.466) data 0.426 (0.334) loss_u loss_u 0.8765 (0.8270) acc_u 12.5000 (21.1806) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [50/55] time 0.416 (0.464) data 0.285 (0.333) loss_u loss_u 0.8120 (0.8278) acc_u 25.0000 (21.0625) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [55/55] time 0.419 (0.465) data 0.287 (0.333) loss_u loss_u 0.8223 (0.8281) acc_u 25.0000 (20.9091) lr 1.1101e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1406
confident_label rate tensor(0.4391, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1377
clean true:1374
clean false:3
clean_rate:0.9978213507625272
noisy true:356
noisy false:1403
after delete: len(clean_dataset) 1377
after delete: len(noisy_dataset) 1759
epoch [200/200] batch [5/43] time 0.445 (0.452) data 0.314 (0.321) loss_x loss_x 1.1348 (1.0137) acc_x 65.6250 (70.0000) lr 4.9344e-07 eta 0:00:17
epoch [200/200] batch [10/43] time 0.428 (0.492) data 0.297 (0.362) loss_x loss_x 0.9404 (1.0984) acc_x 78.1250 (69.6875) lr 4.9344e-07 eta 0:00:16
epoch [200/200] batch [15/43] time 0.347 (0.469) data 0.216 (0.339) loss_x loss_x 1.0752 (1.0263) acc_x 81.2500 (73.3333) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [20/43] time 0.462 (0.468) data 0.331 (0.337) loss_x loss_x 0.8335 (1.0168) acc_x 81.2500 (73.7500) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [25/43] time 0.405 (0.459) data 0.274 (0.329) loss_x loss_x 1.3047 (1.0262) acc_x 65.6250 (73.3750) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [30/43] time 0.432 (0.451) data 0.301 (0.321) loss_x loss_x 0.6660 (0.9867) acc_x 84.3750 (74.2708) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [35/43] time 0.639 (0.459) data 0.506 (0.329) loss_x loss_x 1.3506 (1.0127) acc_x 65.6250 (73.8393) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [40/43] time 0.586 (0.462) data 0.455 (0.331) loss_x loss_x 0.7964 (1.0472) acc_x 81.2500 (73.1250) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [5/54] time 0.431 (0.463) data 0.299 (0.332) loss_u loss_u 0.8613 (0.8394) acc_u 18.7500 (19.3750) lr 4.9344e-07 eta 0:00:22
epoch [200/200] batch [10/54] time 0.339 (0.459) data 0.208 (0.329) loss_u loss_u 0.7686 (0.8293) acc_u 31.2500 (21.2500) lr 4.9344e-07 eta 0:00:20
epoch [200/200] batch [15/54] time 0.422 (0.463) data 0.291 (0.332) loss_u loss_u 0.8657 (0.8277) acc_u 15.6250 (21.2500) lr 4.9344e-07 eta 0:00:18
epoch [200/200] batch [20/54] time 0.500 (0.460) data 0.368 (0.328) loss_u loss_u 0.8247 (0.8352) acc_u 18.7500 (20.9375) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [25/54] time 0.348 (0.455) data 0.216 (0.324) loss_u loss_u 0.8340 (0.8389) acc_u 15.6250 (20.0000) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [30/54] time 0.502 (0.456) data 0.370 (0.325) loss_u loss_u 0.7393 (0.8334) acc_u 28.1250 (20.6250) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [35/54] time 0.465 (0.456) data 0.334 (0.324) loss_u loss_u 0.8516 (0.8356) acc_u 18.7500 (20.2679) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [40/54] time 0.381 (0.453) data 0.249 (0.322) loss_u loss_u 0.8335 (0.8354) acc_u 21.8750 (20.0000) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [45/54] time 0.530 (0.453) data 0.398 (0.321) loss_u loss_u 0.8628 (0.8365) acc_u 15.6250 (19.7917) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [50/54] time 0.465 (0.451) data 0.333 (0.320) loss_u loss_u 0.8848 (0.8367) acc_u 15.6250 (19.8750) lr 4.9344e-07 eta 0:00:01
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.25/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 5,417
* accuracy: 67.4%
* error: 32.6%
* macro_f1: 66.1%
Elapsed: 5:03:47
