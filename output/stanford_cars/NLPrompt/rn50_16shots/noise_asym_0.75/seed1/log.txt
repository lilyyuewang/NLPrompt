***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.75', 'DATASET.NOISE_TYPE', 'asym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.75/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.75
  NOISE_TYPE: asym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.75/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.75/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.0705, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 221
clean true:154
clean false:67
clean_rate:0.6968325791855203
noisy true:411
noisy false:2504
after delete: len(clean_dataset) 221
after delete: len(noisy_dataset) 2915
epoch [1/200] batch [5/6] time 0.550 (0.531) data 0.420 (0.378) loss_x loss_x 2.7227 (3.1707) acc_x 40.6250 (33.1250) lr 1.0000e-05 eta 0:00:00
epoch [1/200] batch [5/91] time 0.578 (0.505) data 0.447 (0.362) loss_u loss_u 0.9790 (0.9807) acc_u 3.1250 (3.1250) lr 1.0000e-05 eta 0:00:43
epoch [1/200] batch [10/91] time 0.345 (0.487) data 0.215 (0.348) loss_u loss_u 0.9780 (0.9806) acc_u 3.1250 (2.8125) lr 1.0000e-05 eta 0:00:39
epoch [1/200] batch [15/91] time 0.458 (0.491) data 0.328 (0.354) loss_u loss_u 0.9795 (0.9768) acc_u 0.0000 (3.3333) lr 1.0000e-05 eta 0:00:37
epoch [1/200] batch [20/91] time 0.444 (0.496) data 0.313 (0.361) loss_u loss_u 0.9648 (0.9755) acc_u 12.5000 (4.2188) lr 1.0000e-05 eta 0:00:35
epoch [1/200] batch [25/91] time 0.779 (0.506) data 0.648 (0.371) loss_u loss_u 0.9741 (0.9734) acc_u 3.1250 (4.5000) lr 1.0000e-05 eta 0:00:33
epoch [1/200] batch [30/91] time 0.411 (0.494) data 0.279 (0.360) loss_u loss_u 0.9785 (0.9737) acc_u 0.0000 (4.3750) lr 1.0000e-05 eta 0:00:30
epoch [1/200] batch [35/91] time 0.399 (0.496) data 0.267 (0.362) loss_u loss_u 0.9683 (0.9726) acc_u 12.5000 (4.5536) lr 1.0000e-05 eta 0:00:27
epoch [1/200] batch [40/91] time 0.638 (0.494) data 0.507 (0.361) loss_u loss_u 0.9517 (0.9725) acc_u 12.5000 (4.6094) lr 1.0000e-05 eta 0:00:25
epoch [1/200] batch [45/91] time 0.483 (0.489) data 0.353 (0.355) loss_u loss_u 0.9634 (0.9719) acc_u 6.2500 (4.5833) lr 1.0000e-05 eta 0:00:22
epoch [1/200] batch [50/91] time 0.664 (0.492) data 0.532 (0.359) loss_u loss_u 0.9727 (0.9710) acc_u 3.1250 (4.6875) lr 1.0000e-05 eta 0:00:20
epoch [1/200] batch [55/91] time 0.526 (0.496) data 0.394 (0.363) loss_u loss_u 0.9702 (0.9716) acc_u 3.1250 (4.4318) lr 1.0000e-05 eta 0:00:17
epoch [1/200] batch [60/91] time 0.521 (0.495) data 0.391 (0.362) loss_u loss_u 0.9619 (0.9720) acc_u 6.2500 (4.3750) lr 1.0000e-05 eta 0:00:15
epoch [1/200] batch [65/91] time 0.424 (0.491) data 0.293 (0.358) loss_u loss_u 0.9619 (0.9715) acc_u 6.2500 (4.5673) lr 1.0000e-05 eta 0:00:12
epoch [1/200] batch [70/91] time 0.366 (0.488) data 0.235 (0.356) loss_u loss_u 0.9795 (0.9722) acc_u 0.0000 (4.3304) lr 1.0000e-05 eta 0:00:10
epoch [1/200] batch [75/91] time 0.535 (0.485) data 0.403 (0.353) loss_u loss_u 0.9873 (0.9727) acc_u 0.0000 (4.1250) lr 1.0000e-05 eta 0:00:07
epoch [1/200] batch [80/91] time 0.515 (0.484) data 0.384 (0.352) loss_u loss_u 0.9595 (0.9723) acc_u 6.2500 (4.2578) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [85/91] time 0.612 (0.484) data 0.477 (0.352) loss_u loss_u 0.9878 (0.9728) acc_u 0.0000 (4.0809) lr 1.0000e-05 eta 0:00:02
epoch [1/200] batch [90/91] time 0.470 (0.482) data 0.339 (0.350) loss_u loss_u 0.9829 (0.9730) acc_u 3.1250 (4.1319) lr 1.0000e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2375
confident_label rate tensor(0.0826, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 259
clean true:183
clean false:76
clean_rate:0.7065637065637066
noisy true:578
noisy false:2299
after delete: len(clean_dataset) 259
after delete: len(noisy_dataset) 2877
epoch [2/200] batch [5/8] time 0.526 (0.543) data 0.395 (0.404) loss_x loss_x 2.2227 (2.3598) acc_x 31.2500 (36.8750) lr 2.0000e-03 eta 0:00:01
epoch [2/200] batch [5/89] time 0.422 (0.501) data 0.290 (0.366) loss_u loss_u 0.9424 (0.9445) acc_u 9.3750 (6.8750) lr 2.0000e-03 eta 0:00:42
epoch [2/200] batch [10/89] time 0.461 (0.494) data 0.331 (0.360) loss_u loss_u 0.9077 (0.9355) acc_u 9.3750 (9.0625) lr 2.0000e-03 eta 0:00:39
epoch [2/200] batch [15/89] time 0.437 (0.491) data 0.306 (0.358) loss_u loss_u 0.9370 (0.9390) acc_u 9.3750 (8.3333) lr 2.0000e-03 eta 0:00:36
epoch [2/200] batch [20/89] time 0.329 (0.476) data 0.197 (0.343) loss_u loss_u 0.9185 (0.9380) acc_u 12.5000 (8.2812) lr 2.0000e-03 eta 0:00:32
epoch [2/200] batch [25/89] time 0.499 (0.471) data 0.368 (0.338) loss_u loss_u 0.8911 (0.9349) acc_u 12.5000 (8.5000) lr 2.0000e-03 eta 0:00:30
epoch [2/200] batch [30/89] time 0.510 (0.471) data 0.378 (0.338) loss_u loss_u 0.9604 (0.9314) acc_u 6.2500 (8.7500) lr 2.0000e-03 eta 0:00:27
epoch [2/200] batch [35/89] time 0.444 (0.468) data 0.312 (0.335) loss_u loss_u 0.8979 (0.9301) acc_u 9.3750 (8.9286) lr 2.0000e-03 eta 0:00:25
epoch [2/200] batch [40/89] time 0.601 (0.469) data 0.469 (0.336) loss_u loss_u 0.9419 (0.9296) acc_u 6.2500 (8.8281) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [45/89] time 0.370 (0.470) data 0.239 (0.337) loss_u loss_u 0.8940 (0.9270) acc_u 12.5000 (9.1667) lr 2.0000e-03 eta 0:00:20
epoch [2/200] batch [50/89] time 0.398 (0.464) data 0.266 (0.331) loss_u loss_u 0.9170 (0.9269) acc_u 12.5000 (9.1875) lr 2.0000e-03 eta 0:00:18
epoch [2/200] batch [55/89] time 0.421 (0.462) data 0.290 (0.330) loss_u loss_u 0.9116 (0.9233) acc_u 12.5000 (9.6591) lr 2.0000e-03 eta 0:00:15
epoch [2/200] batch [60/89] time 0.421 (0.458) data 0.289 (0.325) loss_u loss_u 0.8994 (0.9212) acc_u 9.3750 (9.8438) lr 2.0000e-03 eta 0:00:13
epoch [2/200] batch [65/89] time 0.535 (0.455) data 0.403 (0.323) loss_u loss_u 0.9155 (0.9219) acc_u 12.5000 (9.7115) lr 2.0000e-03 eta 0:00:10
epoch [2/200] batch [70/89] time 0.532 (0.453) data 0.400 (0.321) loss_u loss_u 0.8604 (0.9209) acc_u 15.6250 (9.7768) lr 2.0000e-03 eta 0:00:08
epoch [2/200] batch [75/89] time 0.359 (0.455) data 0.228 (0.323) loss_u loss_u 0.9058 (0.9204) acc_u 9.3750 (9.7917) lr 2.0000e-03 eta 0:00:06
epoch [2/200] batch [80/89] time 0.402 (0.451) data 0.272 (0.319) loss_u loss_u 0.9106 (0.9209) acc_u 15.6250 (9.8047) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [85/89] time 0.541 (0.453) data 0.409 (0.321) loss_u loss_u 0.8887 (0.9212) acc_u 15.6250 (9.7794) lr 2.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1930
confident_label rate tensor(0.1253, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 393
clean true:305
clean false:88
clean_rate:0.7760814249363868
noisy true:901
noisy false:1842
after delete: len(clean_dataset) 393
after delete: len(noisy_dataset) 2743
epoch [3/200] batch [5/12] time 0.401 (0.485) data 0.271 (0.355) loss_x loss_x 1.6816 (1.6418) acc_x 53.1250 (58.7500) lr 1.9999e-03 eta 0:00:03
epoch [3/200] batch [10/12] time 0.419 (0.458) data 0.288 (0.327) loss_x loss_x 1.9375 (1.7971) acc_x 46.8750 (54.0625) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/85] time 0.446 (0.457) data 0.315 (0.326) loss_u loss_u 0.8970 (0.9306) acc_u 15.6250 (7.5000) lr 1.9999e-03 eta 0:00:36
epoch [3/200] batch [10/85] time 0.707 (0.477) data 0.576 (0.346) loss_u loss_u 0.9263 (0.9407) acc_u 9.3750 (7.8125) lr 1.9999e-03 eta 0:00:35
epoch [3/200] batch [15/85] time 0.448 (0.475) data 0.316 (0.344) loss_u loss_u 0.9194 (0.9428) acc_u 9.3750 (7.2917) lr 1.9999e-03 eta 0:00:33
epoch [3/200] batch [20/85] time 0.470 (0.471) data 0.339 (0.340) loss_u loss_u 0.9302 (0.9422) acc_u 3.1250 (7.0312) lr 1.9999e-03 eta 0:00:30
epoch [3/200] batch [25/85] time 0.427 (0.468) data 0.297 (0.337) loss_u loss_u 0.8975 (0.9399) acc_u 9.3750 (7.3750) lr 1.9999e-03 eta 0:00:28
epoch [3/200] batch [30/85] time 0.333 (0.462) data 0.202 (0.331) loss_u loss_u 0.9541 (0.9414) acc_u 3.1250 (6.8750) lr 1.9999e-03 eta 0:00:25
epoch [3/200] batch [35/85] time 0.391 (0.459) data 0.261 (0.328) loss_u loss_u 0.9258 (0.9374) acc_u 3.1250 (7.5000) lr 1.9999e-03 eta 0:00:22
epoch [3/200] batch [40/85] time 0.369 (0.454) data 0.238 (0.323) loss_u loss_u 0.8384 (0.9360) acc_u 28.1250 (7.7344) lr 1.9999e-03 eta 0:00:20
epoch [3/200] batch [45/85] time 0.410 (0.453) data 0.278 (0.322) loss_u loss_u 0.9121 (0.9345) acc_u 12.5000 (7.8472) lr 1.9999e-03 eta 0:00:18
epoch [3/200] batch [50/85] time 0.374 (0.453) data 0.244 (0.321) loss_u loss_u 0.9692 (0.9351) acc_u 3.1250 (7.6875) lr 1.9999e-03 eta 0:00:15
epoch [3/200] batch [55/85] time 0.478 (0.451) data 0.346 (0.320) loss_u loss_u 0.9141 (0.9340) acc_u 12.5000 (8.0114) lr 1.9999e-03 eta 0:00:13
epoch [3/200] batch [60/85] time 0.665 (0.453) data 0.533 (0.321) loss_u loss_u 0.9580 (0.9335) acc_u 6.2500 (8.1771) lr 1.9999e-03 eta 0:00:11
epoch [3/200] batch [65/85] time 0.406 (0.452) data 0.275 (0.320) loss_u loss_u 0.9243 (0.9330) acc_u 6.2500 (8.2212) lr 1.9999e-03 eta 0:00:09
epoch [3/200] batch [70/85] time 0.619 (0.452) data 0.488 (0.321) loss_u loss_u 0.9048 (0.9335) acc_u 9.3750 (8.1250) lr 1.9999e-03 eta 0:00:06
epoch [3/200] batch [75/85] time 0.456 (0.452) data 0.324 (0.321) loss_u loss_u 0.9702 (0.9343) acc_u 6.2500 (7.9583) lr 1.9999e-03 eta 0:00:04
epoch [3/200] batch [80/85] time 0.393 (0.451) data 0.261 (0.320) loss_u loss_u 0.8896 (0.9337) acc_u 21.8750 (8.1250) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [85/85] time 0.497 (0.454) data 0.364 (0.323) loss_u loss_u 0.8862 (0.9328) acc_u 18.7500 (8.3088) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1916
confident_label rate tensor(0.1355, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 425
clean true:310
clean false:115
clean_rate:0.7294117647058823
noisy true:910
noisy false:1801
after delete: len(clean_dataset) 425
after delete: len(noisy_dataset) 2711
epoch [4/200] batch [5/13] time 0.637 (0.489) data 0.506 (0.358) loss_x loss_x 1.8633 (1.9760) acc_x 56.2500 (46.8750) lr 1.9995e-03 eta 0:00:03
epoch [4/200] batch [10/13] time 0.378 (0.496) data 0.247 (0.365) loss_x loss_x 1.8418 (1.8258) acc_x 50.0000 (49.3750) lr 1.9995e-03 eta 0:00:01
epoch [4/200] batch [5/84] time 0.478 (0.471) data 0.347 (0.340) loss_u loss_u 0.9556 (0.9391) acc_u 6.2500 (6.2500) lr 1.9995e-03 eta 0:00:37
epoch [4/200] batch [10/84] time 0.612 (0.466) data 0.480 (0.335) loss_u loss_u 0.9648 (0.9371) acc_u 6.2500 (7.5000) lr 1.9995e-03 eta 0:00:34
epoch [4/200] batch [15/84] time 0.480 (0.476) data 0.348 (0.345) loss_u loss_u 0.9395 (0.9368) acc_u 3.1250 (7.2917) lr 1.9995e-03 eta 0:00:32
epoch [4/200] batch [20/84] time 0.380 (0.465) data 0.248 (0.334) loss_u loss_u 0.9277 (0.9399) acc_u 6.2500 (6.8750) lr 1.9995e-03 eta 0:00:29
epoch [4/200] batch [25/84] time 0.361 (0.456) data 0.229 (0.325) loss_u loss_u 0.9404 (0.9371) acc_u 6.2500 (6.7500) lr 1.9995e-03 eta 0:00:26
epoch [4/200] batch [30/84] time 0.602 (0.455) data 0.470 (0.323) loss_u loss_u 0.9526 (0.9378) acc_u 6.2500 (6.5625) lr 1.9995e-03 eta 0:00:24
epoch [4/200] batch [35/84] time 0.443 (0.459) data 0.311 (0.328) loss_u loss_u 0.9336 (0.9362) acc_u 6.2500 (6.9643) lr 1.9995e-03 eta 0:00:22
epoch [4/200] batch [40/84] time 0.452 (0.458) data 0.320 (0.326) loss_u loss_u 0.9380 (0.9352) acc_u 3.1250 (7.1094) lr 1.9995e-03 eta 0:00:20
epoch [4/200] batch [45/84] time 0.335 (0.458) data 0.203 (0.326) loss_u loss_u 0.9648 (0.9369) acc_u 9.3750 (7.0139) lr 1.9995e-03 eta 0:00:17
epoch [4/200] batch [50/84] time 0.451 (0.455) data 0.319 (0.323) loss_u loss_u 0.9346 (0.9384) acc_u 9.3750 (6.8750) lr 1.9995e-03 eta 0:00:15
epoch [4/200] batch [55/84] time 0.461 (0.453) data 0.328 (0.321) loss_u loss_u 0.9385 (0.9363) acc_u 6.2500 (7.1023) lr 1.9995e-03 eta 0:00:13
epoch [4/200] batch [60/84] time 0.343 (0.456) data 0.212 (0.324) loss_u loss_u 0.9497 (0.9357) acc_u 0.0000 (7.2917) lr 1.9995e-03 eta 0:00:10
epoch [4/200] batch [65/84] time 0.579 (0.453) data 0.447 (0.322) loss_u loss_u 0.9390 (0.9360) acc_u 3.1250 (7.2115) lr 1.9995e-03 eta 0:00:08
epoch [4/200] batch [70/84] time 0.477 (0.453) data 0.345 (0.321) loss_u loss_u 0.9185 (0.9357) acc_u 9.3750 (7.1875) lr 1.9995e-03 eta 0:00:06
epoch [4/200] batch [75/84] time 0.481 (0.456) data 0.349 (0.325) loss_u loss_u 0.9741 (0.9361) acc_u 3.1250 (7.1667) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [80/84] time 0.375 (0.455) data 0.244 (0.323) loss_u loss_u 0.9644 (0.9362) acc_u 3.1250 (7.3047) lr 1.9995e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1921
confident_label rate tensor(0.1311, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 411
clean true:297
clean false:114
clean_rate:0.7226277372262774
noisy true:918
noisy false:1807
after delete: len(clean_dataset) 411
after delete: len(noisy_dataset) 2725
epoch [5/200] batch [5/12] time 0.475 (0.526) data 0.344 (0.395) loss_x loss_x 1.6660 (1.5424) acc_x 46.8750 (52.5000) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [10/12] time 0.392 (0.496) data 0.261 (0.365) loss_x loss_x 1.8994 (1.6448) acc_x 40.6250 (50.6250) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/85] time 0.365 (0.469) data 0.235 (0.339) loss_u loss_u 0.9624 (0.9595) acc_u 3.1250 (3.7500) lr 1.9989e-03 eta 0:00:37
epoch [5/200] batch [10/85] time 0.449 (0.457) data 0.318 (0.326) loss_u loss_u 0.8638 (0.9385) acc_u 12.5000 (6.2500) lr 1.9989e-03 eta 0:00:34
epoch [5/200] batch [15/85] time 0.380 (0.446) data 0.250 (0.315) loss_u loss_u 0.9102 (0.9437) acc_u 9.3750 (5.6250) lr 1.9989e-03 eta 0:00:31
epoch [5/200] batch [20/85] time 0.364 (0.436) data 0.234 (0.306) loss_u loss_u 0.9429 (0.9377) acc_u 6.2500 (6.0938) lr 1.9989e-03 eta 0:00:28
epoch [5/200] batch [25/85] time 0.615 (0.451) data 0.484 (0.320) loss_u loss_u 0.8838 (0.9392) acc_u 15.6250 (6.2500) lr 1.9989e-03 eta 0:00:27
epoch [5/200] batch [30/85] time 0.371 (0.450) data 0.239 (0.319) loss_u loss_u 0.9697 (0.9419) acc_u 0.0000 (6.1458) lr 1.9989e-03 eta 0:00:24
epoch [5/200] batch [35/85] time 0.580 (0.454) data 0.448 (0.323) loss_u loss_u 0.9028 (0.9401) acc_u 12.5000 (6.2500) lr 1.9989e-03 eta 0:00:22
epoch [5/200] batch [40/85] time 0.466 (0.453) data 0.335 (0.322) loss_u loss_u 0.9155 (0.9391) acc_u 12.5000 (6.4844) lr 1.9989e-03 eta 0:00:20
epoch [5/200] batch [45/85] time 0.424 (0.450) data 0.292 (0.319) loss_u loss_u 0.9463 (0.9399) acc_u 6.2500 (6.4583) lr 1.9989e-03 eta 0:00:17
epoch [5/200] batch [50/85] time 0.583 (0.452) data 0.452 (0.321) loss_u loss_u 0.9312 (0.9367) acc_u 12.5000 (7.0625) lr 1.9989e-03 eta 0:00:15
epoch [5/200] batch [55/85] time 0.353 (0.449) data 0.221 (0.318) loss_u loss_u 0.8872 (0.9351) acc_u 9.3750 (7.1023) lr 1.9989e-03 eta 0:00:13
epoch [5/200] batch [60/85] time 0.483 (0.449) data 0.351 (0.318) loss_u loss_u 0.9414 (0.9366) acc_u 6.2500 (6.9792) lr 1.9989e-03 eta 0:00:11
epoch [5/200] batch [65/85] time 0.462 (0.446) data 0.330 (0.315) loss_u loss_u 0.9565 (0.9368) acc_u 3.1250 (6.8750) lr 1.9989e-03 eta 0:00:08
epoch [5/200] batch [70/85] time 0.467 (0.446) data 0.336 (0.315) loss_u loss_u 0.8994 (0.9352) acc_u 12.5000 (7.1429) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [75/85] time 0.464 (0.445) data 0.332 (0.314) loss_u loss_u 0.9302 (0.9346) acc_u 12.5000 (7.2500) lr 1.9989e-03 eta 0:00:04
epoch [5/200] batch [80/85] time 0.421 (0.446) data 0.290 (0.315) loss_u loss_u 0.9395 (0.9340) acc_u 9.3750 (7.3047) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [85/85] time 0.430 (0.446) data 0.298 (0.315) loss_u loss_u 0.9473 (0.9340) acc_u 6.2500 (7.3529) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1911
confident_label rate tensor(0.1393, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 437
clean true:323
clean false:114
clean_rate:0.7391304347826086
noisy true:902
noisy false:1797
after delete: len(clean_dataset) 437
after delete: len(noisy_dataset) 2699
epoch [6/200] batch [5/13] time 0.573 (0.500) data 0.442 (0.369) loss_x loss_x 1.8496 (1.6723) acc_x 46.8750 (56.8750) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [10/13] time 0.425 (0.472) data 0.295 (0.341) loss_x loss_x 1.7754 (1.7006) acc_x 50.0000 (55.3125) lr 1.9980e-03 eta 0:00:01
epoch [6/200] batch [5/84] time 0.419 (0.454) data 0.288 (0.323) loss_u loss_u 0.9243 (0.9349) acc_u 6.2500 (6.2500) lr 1.9980e-03 eta 0:00:35
epoch [6/200] batch [10/84] time 0.421 (0.457) data 0.290 (0.326) loss_u loss_u 0.9209 (0.9277) acc_u 9.3750 (8.4375) lr 1.9980e-03 eta 0:00:33
epoch [6/200] batch [15/84] time 0.490 (0.454) data 0.359 (0.323) loss_u loss_u 0.8813 (0.9252) acc_u 21.8750 (8.9583) lr 1.9980e-03 eta 0:00:31
epoch [6/200] batch [20/84] time 0.535 (0.456) data 0.403 (0.325) loss_u loss_u 0.9697 (0.9253) acc_u 0.0000 (9.2188) lr 1.9980e-03 eta 0:00:29
epoch [6/200] batch [25/84] time 0.439 (0.452) data 0.310 (0.321) loss_u loss_u 0.8945 (0.9282) acc_u 9.3750 (8.6250) lr 1.9980e-03 eta 0:00:26
epoch [6/200] batch [30/84] time 0.450 (0.452) data 0.318 (0.321) loss_u loss_u 0.9263 (0.9276) acc_u 6.2500 (8.8542) lr 1.9980e-03 eta 0:00:24
epoch [6/200] batch [35/84] time 0.363 (0.448) data 0.233 (0.317) loss_u loss_u 0.9556 (0.9277) acc_u 6.2500 (9.1071) lr 1.9980e-03 eta 0:00:21
epoch [6/200] batch [40/84] time 0.392 (0.448) data 0.260 (0.317) loss_u loss_u 0.9551 (0.9295) acc_u 6.2500 (8.5156) lr 1.9980e-03 eta 0:00:19
epoch [6/200] batch [45/84] time 0.365 (0.446) data 0.233 (0.315) loss_u loss_u 0.9238 (0.9284) acc_u 12.5000 (8.7500) lr 1.9980e-03 eta 0:00:17
epoch [6/200] batch [50/84] time 0.480 (0.445) data 0.349 (0.314) loss_u loss_u 0.9307 (0.9284) acc_u 6.2500 (8.6875) lr 1.9980e-03 eta 0:00:15
epoch [6/200] batch [55/84] time 0.461 (0.446) data 0.328 (0.315) loss_u loss_u 0.9644 (0.9298) acc_u 3.1250 (8.5227) lr 1.9980e-03 eta 0:00:12
epoch [6/200] batch [60/84] time 0.312 (0.446) data 0.182 (0.315) loss_u loss_u 0.8901 (0.9286) acc_u 15.6250 (8.6979) lr 1.9980e-03 eta 0:00:10
epoch [6/200] batch [65/84] time 0.623 (0.451) data 0.491 (0.320) loss_u loss_u 0.9336 (0.9291) acc_u 12.5000 (8.7500) lr 1.9980e-03 eta 0:00:08
epoch [6/200] batch [70/84] time 0.452 (0.453) data 0.321 (0.322) loss_u loss_u 0.9424 (0.9294) acc_u 6.2500 (8.6161) lr 1.9980e-03 eta 0:00:06
epoch [6/200] batch [75/84] time 0.444 (0.451) data 0.313 (0.320) loss_u loss_u 0.8721 (0.9285) acc_u 15.6250 (8.7500) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [80/84] time 0.447 (0.452) data 0.316 (0.321) loss_u loss_u 0.9468 (0.9283) acc_u 3.1250 (8.8281) lr 1.9980e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1929
confident_label rate tensor(0.1371, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 430
clean true:325
clean false:105
clean_rate:0.7558139534883721
noisy true:882
noisy false:1824
after delete: len(clean_dataset) 430
after delete: len(noisy_dataset) 2706
epoch [7/200] batch [5/13] time 0.436 (0.447) data 0.305 (0.315) loss_x loss_x 2.3379 (1.8693) acc_x 43.7500 (49.3750) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [10/13] time 0.444 (0.454) data 0.313 (0.323) loss_x loss_x 1.8018 (1.7950) acc_x 53.1250 (51.5625) lr 1.9969e-03 eta 0:00:01
epoch [7/200] batch [5/84] time 0.538 (0.448) data 0.407 (0.317) loss_u loss_u 0.9375 (0.9373) acc_u 6.2500 (6.8750) lr 1.9969e-03 eta 0:00:35
epoch [7/200] batch [10/84] time 0.367 (0.443) data 0.236 (0.312) loss_u loss_u 0.9409 (0.9417) acc_u 0.0000 (5.6250) lr 1.9969e-03 eta 0:00:32
epoch [7/200] batch [15/84] time 0.569 (0.455) data 0.439 (0.324) loss_u loss_u 0.9155 (0.9432) acc_u 12.5000 (6.2500) lr 1.9969e-03 eta 0:00:31
epoch [7/200] batch [20/84] time 0.571 (0.463) data 0.441 (0.332) loss_u loss_u 0.8838 (0.9363) acc_u 21.8750 (7.3438) lr 1.9969e-03 eta 0:00:29
epoch [7/200] batch [25/84] time 0.404 (0.457) data 0.273 (0.327) loss_u loss_u 0.9160 (0.9354) acc_u 12.5000 (7.6250) lr 1.9969e-03 eta 0:00:26
epoch [7/200] batch [30/84] time 0.465 (0.455) data 0.333 (0.324) loss_u loss_u 0.9619 (0.9349) acc_u 3.1250 (7.6042) lr 1.9969e-03 eta 0:00:24
epoch [7/200] batch [35/84] time 0.515 (0.455) data 0.384 (0.324) loss_u loss_u 0.9629 (0.9345) acc_u 3.1250 (7.7679) lr 1.9969e-03 eta 0:00:22
epoch [7/200] batch [40/84] time 0.415 (0.455) data 0.282 (0.324) loss_u loss_u 0.9312 (0.9354) acc_u 12.5000 (7.7344) lr 1.9969e-03 eta 0:00:20
epoch [7/200] batch [45/84] time 0.339 (0.454) data 0.207 (0.323) loss_u loss_u 0.9263 (0.9354) acc_u 6.2500 (7.5694) lr 1.9969e-03 eta 0:00:17
epoch [7/200] batch [50/84] time 0.443 (0.452) data 0.311 (0.321) loss_u loss_u 0.9307 (0.9341) acc_u 9.3750 (7.8750) lr 1.9969e-03 eta 0:00:15
epoch [7/200] batch [55/84] time 0.489 (0.452) data 0.356 (0.321) loss_u loss_u 0.9517 (0.9362) acc_u 6.2500 (7.7273) lr 1.9969e-03 eta 0:00:13
epoch [7/200] batch [60/84] time 0.428 (0.451) data 0.296 (0.320) loss_u loss_u 0.8916 (0.9339) acc_u 21.8750 (8.3333) lr 1.9969e-03 eta 0:00:10
epoch [7/200] batch [65/84] time 0.450 (0.449) data 0.316 (0.318) loss_u loss_u 0.9062 (0.9331) acc_u 9.3750 (8.4135) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [70/84] time 0.678 (0.450) data 0.547 (0.318) loss_u loss_u 0.8633 (0.9318) acc_u 18.7500 (8.6607) lr 1.9969e-03 eta 0:00:06
epoch [7/200] batch [75/84] time 0.453 (0.450) data 0.323 (0.319) loss_u loss_u 0.9551 (0.9324) acc_u 9.3750 (8.5833) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [80/84] time 0.477 (0.450) data 0.346 (0.318) loss_u loss_u 0.8794 (0.9324) acc_u 18.7500 (8.4766) lr 1.9969e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1897
confident_label rate tensor(0.1355, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 425
clean true:317
clean false:108
clean_rate:0.7458823529411764
noisy true:922
noisy false:1789
after delete: len(clean_dataset) 425
after delete: len(noisy_dataset) 2711
epoch [8/200] batch [5/13] time 0.468 (0.493) data 0.338 (0.362) loss_x loss_x 1.7305 (1.8572) acc_x 46.8750 (54.3750) lr 1.9956e-03 eta 0:00:03
epoch [8/200] batch [10/13] time 0.463 (0.460) data 0.334 (0.329) loss_x loss_x 1.8350 (1.8151) acc_x 59.3750 (53.4375) lr 1.9956e-03 eta 0:00:01
epoch [8/200] batch [5/84] time 0.500 (0.471) data 0.370 (0.340) loss_u loss_u 0.9521 (0.9348) acc_u 6.2500 (6.8750) lr 1.9956e-03 eta 0:00:37
epoch [8/200] batch [10/84] time 0.508 (0.469) data 0.376 (0.338) loss_u loss_u 0.9326 (0.9350) acc_u 6.2500 (7.1875) lr 1.9956e-03 eta 0:00:34
epoch [8/200] batch [15/84] time 0.390 (0.467) data 0.260 (0.336) loss_u loss_u 0.8892 (0.9232) acc_u 15.6250 (8.9583) lr 1.9956e-03 eta 0:00:32
epoch [8/200] batch [20/84] time 0.375 (0.462) data 0.244 (0.331) loss_u loss_u 0.9058 (0.9180) acc_u 12.5000 (9.6875) lr 1.9956e-03 eta 0:00:29
epoch [8/200] batch [25/84] time 0.352 (0.456) data 0.221 (0.325) loss_u loss_u 0.9238 (0.9205) acc_u 9.3750 (9.5000) lr 1.9956e-03 eta 0:00:26
epoch [8/200] batch [30/84] time 0.408 (0.452) data 0.277 (0.321) loss_u loss_u 0.9688 (0.9254) acc_u 3.1250 (8.6458) lr 1.9956e-03 eta 0:00:24
epoch [8/200] batch [35/84] time 0.407 (0.455) data 0.276 (0.324) loss_u loss_u 0.9678 (0.9266) acc_u 3.1250 (8.4821) lr 1.9956e-03 eta 0:00:22
epoch [8/200] batch [40/84] time 0.366 (0.452) data 0.235 (0.321) loss_u loss_u 0.8682 (0.9237) acc_u 12.5000 (8.8281) lr 1.9956e-03 eta 0:00:19
epoch [8/200] batch [45/84] time 0.472 (0.450) data 0.341 (0.319) loss_u loss_u 0.8633 (0.9221) acc_u 15.6250 (9.6528) lr 1.9956e-03 eta 0:00:17
epoch [8/200] batch [50/84] time 0.443 (0.449) data 0.311 (0.318) loss_u loss_u 0.9424 (0.9234) acc_u 9.3750 (9.5000) lr 1.9956e-03 eta 0:00:15
epoch [8/200] batch [55/84] time 0.323 (0.449) data 0.191 (0.317) loss_u loss_u 0.9595 (0.9241) acc_u 3.1250 (9.5455) lr 1.9956e-03 eta 0:00:13
epoch [8/200] batch [60/84] time 0.561 (0.447) data 0.430 (0.316) loss_u loss_u 0.9175 (0.9249) acc_u 9.3750 (9.4792) lr 1.9956e-03 eta 0:00:10
epoch [8/200] batch [65/84] time 0.486 (0.446) data 0.355 (0.315) loss_u loss_u 0.9238 (0.9245) acc_u 9.3750 (9.5673) lr 1.9956e-03 eta 0:00:08
epoch [8/200] batch [70/84] time 0.481 (0.448) data 0.350 (0.317) loss_u loss_u 0.9268 (0.9231) acc_u 12.5000 (9.7768) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [75/84] time 0.433 (0.452) data 0.301 (0.321) loss_u loss_u 0.9590 (0.9221) acc_u 3.1250 (9.7917) lr 1.9956e-03 eta 0:00:04
epoch [8/200] batch [80/84] time 0.402 (0.449) data 0.272 (0.318) loss_u loss_u 0.8989 (0.9216) acc_u 15.6250 (9.9219) lr 1.9956e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1927
confident_label rate tensor(0.1419, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 445
clean true:317
clean false:128
clean_rate:0.7123595505617978
noisy true:892
noisy false:1799
after delete: len(clean_dataset) 445
after delete: len(noisy_dataset) 2691
epoch [9/200] batch [5/13] time 0.429 (0.441) data 0.298 (0.310) loss_x loss_x 2.0586 (1.6791) acc_x 46.8750 (55.6250) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [10/13] time 0.441 (0.432) data 0.310 (0.302) loss_x loss_x 1.5498 (1.6639) acc_x 56.2500 (55.0000) lr 1.9940e-03 eta 0:00:01
epoch [9/200] batch [5/84] time 0.407 (0.457) data 0.276 (0.326) loss_u loss_u 0.9404 (0.9444) acc_u 6.2500 (5.0000) lr 1.9940e-03 eta 0:00:36
epoch [9/200] batch [10/84] time 0.593 (0.459) data 0.463 (0.328) loss_u loss_u 0.9648 (0.9301) acc_u 3.1250 (8.1250) lr 1.9940e-03 eta 0:00:33
epoch [9/200] batch [15/84] time 0.473 (0.454) data 0.343 (0.323) loss_u loss_u 0.8882 (0.9267) acc_u 21.8750 (8.7500) lr 1.9940e-03 eta 0:00:31
epoch [9/200] batch [20/84] time 0.419 (0.450) data 0.287 (0.319) loss_u loss_u 0.9492 (0.9259) acc_u 6.2500 (9.2188) lr 1.9940e-03 eta 0:00:28
epoch [9/200] batch [25/84] time 0.600 (0.453) data 0.469 (0.322) loss_u loss_u 0.9346 (0.9288) acc_u 9.3750 (9.0000) lr 1.9940e-03 eta 0:00:26
epoch [9/200] batch [30/84] time 0.414 (0.451) data 0.283 (0.320) loss_u loss_u 0.9570 (0.9259) acc_u 3.1250 (9.3750) lr 1.9940e-03 eta 0:00:24
epoch [9/200] batch [35/84] time 0.459 (0.450) data 0.328 (0.319) loss_u loss_u 0.8979 (0.9256) acc_u 12.5000 (9.2857) lr 1.9940e-03 eta 0:00:22
epoch [9/200] batch [40/84] time 0.502 (0.457) data 0.371 (0.326) loss_u loss_u 0.9355 (0.9251) acc_u 12.5000 (9.5312) lr 1.9940e-03 eta 0:00:20
epoch [9/200] batch [45/84] time 0.444 (0.460) data 0.313 (0.329) loss_u loss_u 0.9380 (0.9266) acc_u 6.2500 (9.1667) lr 1.9940e-03 eta 0:00:17
epoch [9/200] batch [50/84] time 0.439 (0.461) data 0.307 (0.330) loss_u loss_u 0.9097 (0.9284) acc_u 12.5000 (8.8750) lr 1.9940e-03 eta 0:00:15
epoch [9/200] batch [55/84] time 0.487 (0.463) data 0.357 (0.332) loss_u loss_u 0.9150 (0.9293) acc_u 9.3750 (8.5795) lr 1.9940e-03 eta 0:00:13
epoch [9/200] batch [60/84] time 0.478 (0.466) data 0.346 (0.334) loss_u loss_u 0.9170 (0.9287) acc_u 9.3750 (8.6458) lr 1.9940e-03 eta 0:00:11
epoch [9/200] batch [65/84] time 0.393 (0.468) data 0.262 (0.337) loss_u loss_u 0.9492 (0.9297) acc_u 6.2500 (8.6538) lr 1.9940e-03 eta 0:00:08
epoch [9/200] batch [70/84] time 0.407 (0.464) data 0.275 (0.333) loss_u loss_u 0.9189 (0.9303) acc_u 9.3750 (8.5268) lr 1.9940e-03 eta 0:00:06
epoch [9/200] batch [75/84] time 0.421 (0.463) data 0.290 (0.331) loss_u loss_u 0.9487 (0.9309) acc_u 6.2500 (8.4167) lr 1.9940e-03 eta 0:00:04
epoch [9/200] batch [80/84] time 0.357 (0.460) data 0.225 (0.329) loss_u loss_u 0.9521 (0.9317) acc_u 3.1250 (8.1641) lr 1.9940e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1868
confident_label rate tensor(0.1387, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 435
clean true:319
clean false:116
clean_rate:0.7333333333333333
noisy true:949
noisy false:1752
after delete: len(clean_dataset) 435
after delete: len(noisy_dataset) 2701
epoch [10/200] batch [5/13] time 0.386 (0.463) data 0.256 (0.332) loss_x loss_x 1.5400 (1.7645) acc_x 50.0000 (50.6250) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [10/13] time 0.533 (0.463) data 0.403 (0.333) loss_x loss_x 1.5459 (1.7071) acc_x 40.6250 (51.2500) lr 1.9921e-03 eta 0:00:01
epoch [10/200] batch [5/84] time 0.392 (0.453) data 0.261 (0.322) loss_u loss_u 0.9609 (0.9317) acc_u 0.0000 (6.2500) lr 1.9921e-03 eta 0:00:35
epoch [10/200] batch [10/84] time 0.427 (0.440) data 0.297 (0.309) loss_u loss_u 0.9526 (0.9313) acc_u 6.2500 (7.5000) lr 1.9921e-03 eta 0:00:32
epoch [10/200] batch [15/84] time 0.357 (0.431) data 0.227 (0.300) loss_u loss_u 0.9141 (0.9297) acc_u 6.2500 (8.1250) lr 1.9921e-03 eta 0:00:29
epoch [10/200] batch [20/84] time 0.459 (0.429) data 0.328 (0.298) loss_u loss_u 0.9272 (0.9225) acc_u 9.3750 (9.2188) lr 1.9921e-03 eta 0:00:27
epoch [10/200] batch [25/84] time 0.555 (0.433) data 0.425 (0.303) loss_u loss_u 0.9155 (0.9250) acc_u 12.5000 (9.0000) lr 1.9921e-03 eta 0:00:25
epoch [10/200] batch [30/84] time 0.621 (0.437) data 0.489 (0.306) loss_u loss_u 0.9434 (0.9258) acc_u 3.1250 (8.9583) lr 1.9921e-03 eta 0:00:23
epoch [10/200] batch [35/84] time 0.348 (0.437) data 0.217 (0.306) loss_u loss_u 0.9023 (0.9257) acc_u 18.7500 (9.2857) lr 1.9921e-03 eta 0:00:21
epoch [10/200] batch [40/84] time 0.497 (0.438) data 0.366 (0.307) loss_u loss_u 0.9443 (0.9239) acc_u 9.3750 (9.9219) lr 1.9921e-03 eta 0:00:19
epoch [10/200] batch [45/84] time 0.425 (0.441) data 0.294 (0.310) loss_u loss_u 0.9600 (0.9257) acc_u 9.3750 (9.5833) lr 1.9921e-03 eta 0:00:17
epoch [10/200] batch [50/84] time 0.485 (0.446) data 0.355 (0.315) loss_u loss_u 0.9170 (0.9241) acc_u 9.3750 (9.5625) lr 1.9921e-03 eta 0:00:15
epoch [10/200] batch [55/84] time 0.438 (0.443) data 0.307 (0.312) loss_u loss_u 0.9478 (0.9248) acc_u 9.3750 (9.3750) lr 1.9921e-03 eta 0:00:12
epoch [10/200] batch [60/84] time 0.334 (0.446) data 0.203 (0.315) loss_u loss_u 0.9507 (0.9243) acc_u 6.2500 (9.4792) lr 1.9921e-03 eta 0:00:10
epoch [10/200] batch [65/84] time 0.406 (0.445) data 0.275 (0.314) loss_u loss_u 0.9517 (0.9249) acc_u 12.5000 (9.5673) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [70/84] time 0.381 (0.444) data 0.248 (0.313) loss_u loss_u 0.9365 (0.9268) acc_u 9.3750 (9.3304) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [75/84] time 0.480 (0.445) data 0.348 (0.314) loss_u loss_u 0.8936 (0.9251) acc_u 9.3750 (9.6250) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [80/84] time 0.431 (0.443) data 0.298 (0.312) loss_u loss_u 0.8413 (0.9236) acc_u 21.8750 (9.8438) lr 1.9921e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1897
confident_label rate tensor(0.1505, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 472
clean true:338
clean false:134
clean_rate:0.7161016949152542
noisy true:901
noisy false:1763
after delete: len(clean_dataset) 472
after delete: len(noisy_dataset) 2664
epoch [11/200] batch [5/14] time 0.433 (0.441) data 0.303 (0.310) loss_x loss_x 1.9385 (1.7383) acc_x 43.7500 (55.0000) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [10/14] time 0.413 (0.437) data 0.282 (0.306) loss_x loss_x 1.7998 (1.7224) acc_x 50.0000 (55.9375) lr 1.9900e-03 eta 0:00:01
epoch [11/200] batch [5/83] time 0.395 (0.450) data 0.264 (0.319) loss_u loss_u 0.9028 (0.9234) acc_u 18.7500 (10.6250) lr 1.9900e-03 eta 0:00:35
epoch [11/200] batch [10/83] time 0.477 (0.458) data 0.344 (0.326) loss_u loss_u 0.9678 (0.9369) acc_u 3.1250 (9.0625) lr 1.9900e-03 eta 0:00:33
epoch [11/200] batch [15/83] time 0.395 (0.453) data 0.264 (0.322) loss_u loss_u 0.9312 (0.9347) acc_u 12.5000 (9.1667) lr 1.9900e-03 eta 0:00:30
epoch [11/200] batch [20/83] time 0.346 (0.456) data 0.216 (0.325) loss_u loss_u 0.9355 (0.9329) acc_u 6.2500 (9.5312) lr 1.9900e-03 eta 0:00:28
epoch [11/200] batch [25/83] time 0.503 (0.451) data 0.371 (0.319) loss_u loss_u 0.9268 (0.9344) acc_u 12.5000 (9.2500) lr 1.9900e-03 eta 0:00:26
epoch [11/200] batch [30/83] time 0.507 (0.460) data 0.376 (0.329) loss_u loss_u 0.8828 (0.9329) acc_u 12.5000 (9.1667) lr 1.9900e-03 eta 0:00:24
epoch [11/200] batch [35/83] time 0.511 (0.466) data 0.381 (0.335) loss_u loss_u 0.9067 (0.9278) acc_u 12.5000 (9.7321) lr 1.9900e-03 eta 0:00:22
epoch [11/200] batch [40/83] time 0.385 (0.463) data 0.254 (0.332) loss_u loss_u 0.9019 (0.9277) acc_u 18.7500 (9.6875) lr 1.9900e-03 eta 0:00:19
epoch [11/200] batch [45/83] time 0.466 (0.460) data 0.334 (0.329) loss_u loss_u 0.9546 (0.9289) acc_u 6.2500 (9.5833) lr 1.9900e-03 eta 0:00:17
epoch [11/200] batch [50/83] time 0.416 (0.457) data 0.285 (0.326) loss_u loss_u 0.9097 (0.9297) acc_u 15.6250 (9.5000) lr 1.9900e-03 eta 0:00:15
epoch [11/200] batch [55/83] time 0.452 (0.458) data 0.320 (0.326) loss_u loss_u 0.9751 (0.9299) acc_u 0.0000 (9.5455) lr 1.9900e-03 eta 0:00:12
epoch [11/200] batch [60/83] time 0.404 (0.457) data 0.272 (0.326) loss_u loss_u 0.9272 (0.9276) acc_u 9.3750 (9.7917) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [65/83] time 0.458 (0.454) data 0.326 (0.323) loss_u loss_u 0.9214 (0.9269) acc_u 9.3750 (9.8077) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [70/83] time 0.394 (0.453) data 0.262 (0.321) loss_u loss_u 0.8984 (0.9268) acc_u 12.5000 (9.8661) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [75/83] time 0.308 (0.452) data 0.177 (0.320) loss_u loss_u 0.9336 (0.9275) acc_u 9.3750 (9.7083) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [80/83] time 0.402 (0.450) data 0.270 (0.319) loss_u loss_u 0.9097 (0.9263) acc_u 12.5000 (9.8828) lr 1.9900e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1910
confident_label rate tensor(0.1543, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 484
clean true:333
clean false:151
clean_rate:0.6880165289256198
noisy true:893
noisy false:1759
after delete: len(clean_dataset) 484
after delete: len(noisy_dataset) 2652
epoch [12/200] batch [5/15] time 0.517 (0.542) data 0.386 (0.411) loss_x loss_x 2.0312 (1.7633) acc_x 46.8750 (50.0000) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [10/15] time 0.425 (0.486) data 0.295 (0.355) loss_x loss_x 1.5938 (1.6775) acc_x 56.2500 (51.8750) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [15/15] time 0.421 (0.480) data 0.290 (0.349) loss_x loss_x 1.8145 (1.7285) acc_x 50.0000 (52.2917) lr 1.9877e-03 eta 0:00:00
epoch [12/200] batch [5/82] time 0.415 (0.468) data 0.285 (0.338) loss_u loss_u 0.9316 (0.9313) acc_u 9.3750 (6.8750) lr 1.9877e-03 eta 0:00:36
epoch [12/200] batch [10/82] time 0.492 (0.466) data 0.361 (0.335) loss_u loss_u 0.9302 (0.9327) acc_u 12.5000 (8.4375) lr 1.9877e-03 eta 0:00:33
epoch [12/200] batch [15/82] time 0.431 (0.461) data 0.300 (0.330) loss_u loss_u 0.9390 (0.9301) acc_u 6.2500 (8.7500) lr 1.9877e-03 eta 0:00:30
epoch [12/200] batch [20/82] time 0.381 (0.454) data 0.250 (0.323) loss_u loss_u 0.9287 (0.9310) acc_u 6.2500 (8.9062) lr 1.9877e-03 eta 0:00:28
epoch [12/200] batch [25/82] time 0.354 (0.450) data 0.223 (0.319) loss_u loss_u 0.9375 (0.9351) acc_u 9.3750 (8.2500) lr 1.9877e-03 eta 0:00:25
epoch [12/200] batch [30/82] time 0.431 (0.443) data 0.300 (0.313) loss_u loss_u 0.9312 (0.9360) acc_u 6.2500 (7.9167) lr 1.9877e-03 eta 0:00:23
epoch [12/200] batch [35/82] time 0.560 (0.445) data 0.429 (0.314) loss_u loss_u 0.8926 (0.9330) acc_u 9.3750 (8.3036) lr 1.9877e-03 eta 0:00:20
epoch [12/200] batch [40/82] time 0.392 (0.448) data 0.261 (0.317) loss_u loss_u 0.9077 (0.9315) acc_u 12.5000 (8.6719) lr 1.9877e-03 eta 0:00:18
epoch [12/200] batch [45/82] time 0.404 (0.446) data 0.273 (0.315) loss_u loss_u 0.9497 (0.9312) acc_u 3.1250 (8.8889) lr 1.9877e-03 eta 0:00:16
epoch [12/200] batch [50/82] time 0.451 (0.442) data 0.320 (0.311) loss_u loss_u 0.9561 (0.9322) acc_u 3.1250 (8.6250) lr 1.9877e-03 eta 0:00:14
epoch [12/200] batch [55/82] time 0.480 (0.442) data 0.348 (0.311) loss_u loss_u 0.8506 (0.9304) acc_u 21.8750 (8.8636) lr 1.9877e-03 eta 0:00:11
epoch [12/200] batch [60/82] time 0.363 (0.439) data 0.233 (0.308) loss_u loss_u 0.9023 (0.9300) acc_u 12.5000 (8.9583) lr 1.9877e-03 eta 0:00:09
epoch [12/200] batch [65/82] time 0.516 (0.437) data 0.386 (0.306) loss_u loss_u 0.9204 (0.9313) acc_u 12.5000 (8.6058) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [70/82] time 0.514 (0.442) data 0.382 (0.311) loss_u loss_u 0.9541 (0.9307) acc_u 6.2500 (8.7500) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [75/82] time 0.502 (0.443) data 0.371 (0.312) loss_u loss_u 0.9521 (0.9299) acc_u 6.2500 (8.8750) lr 1.9877e-03 eta 0:00:03
epoch [12/200] batch [80/82] time 0.526 (0.444) data 0.395 (0.313) loss_u loss_u 0.9121 (0.9285) acc_u 12.5000 (9.0625) lr 1.9877e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2008
confident_label rate tensor(0.1416, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 444
clean true:305
clean false:139
clean_rate:0.6869369369369369
noisy true:823
noisy false:1869
after delete: len(clean_dataset) 444
after delete: len(noisy_dataset) 2692
epoch [13/200] batch [5/13] time 0.583 (0.521) data 0.453 (0.390) loss_x loss_x 2.1016 (1.5762) acc_x 50.0000 (59.3750) lr 1.9851e-03 eta 0:00:04
epoch [13/200] batch [10/13] time 0.523 (0.486) data 0.393 (0.356) loss_x loss_x 1.7256 (1.5025) acc_x 62.5000 (59.6875) lr 1.9851e-03 eta 0:00:01
epoch [13/200] batch [5/84] time 0.530 (0.486) data 0.399 (0.355) loss_u loss_u 0.9189 (0.9172) acc_u 12.5000 (10.6250) lr 1.9851e-03 eta 0:00:38
epoch [13/200] batch [10/84] time 0.476 (0.476) data 0.345 (0.345) loss_u loss_u 0.9453 (0.9181) acc_u 9.3750 (11.5625) lr 1.9851e-03 eta 0:00:35
epoch [13/200] batch [15/84] time 0.346 (0.464) data 0.214 (0.333) loss_u loss_u 0.9468 (0.9244) acc_u 12.5000 (11.0417) lr 1.9851e-03 eta 0:00:31
epoch [13/200] batch [20/84] time 0.596 (0.465) data 0.464 (0.334) loss_u loss_u 0.9121 (0.9244) acc_u 12.5000 (10.4688) lr 1.9851e-03 eta 0:00:29
epoch [13/200] batch [25/84] time 0.420 (0.456) data 0.288 (0.325) loss_u loss_u 0.9033 (0.9250) acc_u 9.3750 (10.5000) lr 1.9851e-03 eta 0:00:26
epoch [13/200] batch [30/84] time 0.421 (0.453) data 0.290 (0.322) loss_u loss_u 0.8311 (0.9237) acc_u 18.7500 (10.3125) lr 1.9851e-03 eta 0:00:24
epoch [13/200] batch [35/84] time 0.543 (0.454) data 0.412 (0.323) loss_u loss_u 0.9243 (0.9260) acc_u 9.3750 (9.7321) lr 1.9851e-03 eta 0:00:22
epoch [13/200] batch [40/84] time 0.670 (0.462) data 0.539 (0.331) loss_u loss_u 0.9082 (0.9276) acc_u 9.3750 (9.2969) lr 1.9851e-03 eta 0:00:20
epoch [13/200] batch [45/84] time 0.398 (0.459) data 0.267 (0.328) loss_u loss_u 0.8970 (0.9277) acc_u 15.6250 (9.2361) lr 1.9851e-03 eta 0:00:17
epoch [13/200] batch [50/84] time 0.526 (0.456) data 0.396 (0.325) loss_u loss_u 0.9419 (0.9269) acc_u 6.2500 (9.3750) lr 1.9851e-03 eta 0:00:15
epoch [13/200] batch [55/84] time 0.360 (0.455) data 0.229 (0.324) loss_u loss_u 0.9263 (0.9260) acc_u 6.2500 (9.4318) lr 1.9851e-03 eta 0:00:13
epoch [13/200] batch [60/84] time 0.542 (0.453) data 0.412 (0.322) loss_u loss_u 0.8711 (0.9251) acc_u 15.6250 (9.4271) lr 1.9851e-03 eta 0:00:10
epoch [13/200] batch [65/84] time 0.490 (0.453) data 0.358 (0.322) loss_u loss_u 0.9521 (0.9266) acc_u 6.2500 (9.2308) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [70/84] time 0.557 (0.451) data 0.427 (0.320) loss_u loss_u 0.9082 (0.9272) acc_u 18.7500 (9.2411) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [75/84] time 0.457 (0.452) data 0.327 (0.321) loss_u loss_u 0.9014 (0.9264) acc_u 9.3750 (9.3333) lr 1.9851e-03 eta 0:00:04
epoch [13/200] batch [80/84] time 0.459 (0.454) data 0.327 (0.323) loss_u loss_u 0.9053 (0.9264) acc_u 15.6250 (9.2969) lr 1.9851e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1933
confident_label rate tensor(0.1454, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 456
clean true:319
clean false:137
clean_rate:0.6995614035087719
noisy true:884
noisy false:1796
after delete: len(clean_dataset) 456
after delete: len(noisy_dataset) 2680
epoch [14/200] batch [5/14] time 0.506 (0.522) data 0.375 (0.390) loss_x loss_x 1.7842 (1.6041) acc_x 43.7500 (56.8750) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [10/14] time 0.449 (0.514) data 0.318 (0.382) loss_x loss_x 1.5000 (1.5497) acc_x 46.8750 (57.1875) lr 1.9823e-03 eta 0:00:02
epoch [14/200] batch [5/83] time 0.422 (0.467) data 0.289 (0.335) loss_u loss_u 0.9277 (0.9152) acc_u 6.2500 (10.6250) lr 1.9823e-03 eta 0:00:36
epoch [14/200] batch [10/83] time 0.583 (0.479) data 0.450 (0.347) loss_u loss_u 0.9282 (0.9230) acc_u 12.5000 (10.0000) lr 1.9823e-03 eta 0:00:34
epoch [14/200] batch [15/83] time 0.311 (0.472) data 0.180 (0.341) loss_u loss_u 0.9409 (0.9273) acc_u 6.2500 (9.5833) lr 1.9823e-03 eta 0:00:32
epoch [14/200] batch [20/83] time 0.321 (0.462) data 0.189 (0.330) loss_u loss_u 0.9331 (0.9271) acc_u 6.2500 (9.3750) lr 1.9823e-03 eta 0:00:29
epoch [14/200] batch [25/83] time 0.527 (0.466) data 0.396 (0.335) loss_u loss_u 0.9346 (0.9265) acc_u 6.2500 (9.1250) lr 1.9823e-03 eta 0:00:27
epoch [14/200] batch [30/83] time 0.405 (0.463) data 0.273 (0.332) loss_u loss_u 0.9644 (0.9296) acc_u 0.0000 (8.7500) lr 1.9823e-03 eta 0:00:24
epoch [14/200] batch [35/83] time 0.427 (0.459) data 0.295 (0.327) loss_u loss_u 0.9009 (0.9296) acc_u 15.6250 (8.9286) lr 1.9823e-03 eta 0:00:22
epoch [14/200] batch [40/83] time 0.368 (0.456) data 0.237 (0.324) loss_u loss_u 0.9507 (0.9267) acc_u 3.1250 (9.1406) lr 1.9823e-03 eta 0:00:19
epoch [14/200] batch [45/83] time 0.330 (0.450) data 0.200 (0.318) loss_u loss_u 0.8926 (0.9252) acc_u 18.7500 (9.7222) lr 1.9823e-03 eta 0:00:17
epoch [14/200] batch [50/83] time 0.501 (0.454) data 0.370 (0.322) loss_u loss_u 0.9409 (0.9250) acc_u 6.2500 (9.7500) lr 1.9823e-03 eta 0:00:14
epoch [14/200] batch [55/83] time 0.423 (0.450) data 0.292 (0.319) loss_u loss_u 0.9502 (0.9247) acc_u 3.1250 (9.7727) lr 1.9823e-03 eta 0:00:12
epoch [14/200] batch [60/83] time 0.330 (0.449) data 0.200 (0.318) loss_u loss_u 0.9150 (0.9229) acc_u 9.3750 (9.9479) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [65/83] time 0.480 (0.453) data 0.350 (0.322) loss_u loss_u 0.9058 (0.9216) acc_u 6.2500 (9.9038) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [70/83] time 0.479 (0.453) data 0.348 (0.322) loss_u loss_u 0.8906 (0.9215) acc_u 18.7500 (9.8661) lr 1.9823e-03 eta 0:00:05
epoch [14/200] batch [75/83] time 0.347 (0.449) data 0.216 (0.318) loss_u loss_u 0.9263 (0.9210) acc_u 6.2500 (9.8750) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [80/83] time 0.466 (0.447) data 0.334 (0.315) loss_u loss_u 0.9712 (0.9209) acc_u 0.0000 (9.9609) lr 1.9823e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1955
confident_label rate tensor(0.1534, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 481
clean true:320
clean false:161
clean_rate:0.6652806652806653
noisy true:861
noisy false:1794
after delete: len(clean_dataset) 481
after delete: len(noisy_dataset) 2655
epoch [15/200] batch [5/15] time 0.472 (0.511) data 0.341 (0.380) loss_x loss_x 1.4111 (1.5178) acc_x 62.5000 (58.1250) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [10/15] time 0.467 (0.477) data 0.336 (0.346) loss_x loss_x 1.3584 (1.5318) acc_x 59.3750 (58.1250) lr 1.9792e-03 eta 0:00:02
epoch [15/200] batch [15/15] time 0.406 (0.460) data 0.275 (0.329) loss_x loss_x 2.1504 (1.5518) acc_x 46.8750 (59.3750) lr 1.9792e-03 eta 0:00:00
epoch [15/200] batch [5/82] time 0.412 (0.467) data 0.281 (0.336) loss_u loss_u 0.8872 (0.9109) acc_u 18.7500 (13.7500) lr 1.9792e-03 eta 0:00:35
epoch [15/200] batch [10/82] time 0.506 (0.459) data 0.374 (0.327) loss_u loss_u 0.9077 (0.9178) acc_u 9.3750 (10.6250) lr 1.9792e-03 eta 0:00:33
epoch [15/200] batch [15/82] time 0.475 (0.457) data 0.344 (0.326) loss_u loss_u 0.9575 (0.9239) acc_u 0.0000 (9.5833) lr 1.9792e-03 eta 0:00:30
epoch [15/200] batch [20/82] time 0.407 (0.470) data 0.275 (0.338) loss_u loss_u 0.9180 (0.9239) acc_u 9.3750 (9.6875) lr 1.9792e-03 eta 0:00:29
epoch [15/200] batch [25/82] time 0.431 (0.470) data 0.299 (0.339) loss_u loss_u 0.9307 (0.9245) acc_u 6.2500 (9.2500) lr 1.9792e-03 eta 0:00:26
epoch [15/200] batch [30/82] time 0.409 (0.468) data 0.276 (0.337) loss_u loss_u 0.8652 (0.9205) acc_u 15.6250 (10.0000) lr 1.9792e-03 eta 0:00:24
epoch [15/200] batch [35/82] time 0.450 (0.463) data 0.319 (0.332) loss_u loss_u 0.9395 (0.9235) acc_u 9.3750 (9.2857) lr 1.9792e-03 eta 0:00:21
epoch [15/200] batch [40/82] time 0.381 (0.459) data 0.250 (0.327) loss_u loss_u 0.9331 (0.9213) acc_u 9.3750 (9.8438) lr 1.9792e-03 eta 0:00:19
epoch [15/200] batch [45/82] time 0.370 (0.463) data 0.238 (0.332) loss_u loss_u 0.8809 (0.9216) acc_u 15.6250 (9.9306) lr 1.9792e-03 eta 0:00:17
epoch [15/200] batch [50/82] time 0.559 (0.465) data 0.427 (0.333) loss_u loss_u 0.9404 (0.9227) acc_u 9.3750 (9.8750) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [55/82] time 0.510 (0.464) data 0.379 (0.333) loss_u loss_u 0.9658 (0.9236) acc_u 3.1250 (9.8295) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [60/82] time 0.563 (0.464) data 0.431 (0.332) loss_u loss_u 0.9434 (0.9239) acc_u 12.5000 (9.7917) lr 1.9792e-03 eta 0:00:10
epoch [15/200] batch [65/82] time 0.656 (0.466) data 0.524 (0.335) loss_u loss_u 0.9507 (0.9249) acc_u 6.2500 (9.6154) lr 1.9792e-03 eta 0:00:07
epoch [15/200] batch [70/82] time 0.353 (0.469) data 0.223 (0.337) loss_u loss_u 0.9321 (0.9249) acc_u 3.1250 (9.5536) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [75/82] time 0.416 (0.465) data 0.285 (0.334) loss_u loss_u 0.8955 (0.9247) acc_u 12.5000 (9.6667) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [80/82] time 0.343 (0.463) data 0.211 (0.332) loss_u loss_u 0.9385 (0.9245) acc_u 6.2500 (9.8047) lr 1.9792e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1870
confident_label rate tensor(0.1527, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 479
clean true:337
clean false:142
clean_rate:0.7035490605427975
noisy true:929
noisy false:1728
after delete: len(clean_dataset) 479
after delete: len(noisy_dataset) 2657
epoch [16/200] batch [5/14] time 0.466 (0.516) data 0.334 (0.384) loss_x loss_x 2.0879 (1.5391) acc_x 50.0000 (61.8750) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [10/14] time 0.435 (0.500) data 0.302 (0.368) loss_x loss_x 1.1279 (1.5246) acc_x 81.2500 (64.6875) lr 1.9759e-03 eta 0:00:02
epoch [16/200] batch [5/83] time 0.432 (0.489) data 0.300 (0.357) loss_u loss_u 0.9741 (0.9370) acc_u 3.1250 (6.2500) lr 1.9759e-03 eta 0:00:38
epoch [16/200] batch [10/83] time 0.430 (0.479) data 0.297 (0.347) loss_u loss_u 0.8779 (0.9224) acc_u 21.8750 (9.6875) lr 1.9759e-03 eta 0:00:34
epoch [16/200] batch [15/83] time 0.469 (0.473) data 0.337 (0.341) loss_u loss_u 0.9326 (0.9248) acc_u 15.6250 (9.3750) lr 1.9759e-03 eta 0:00:32
epoch [16/200] batch [20/83] time 0.385 (0.473) data 0.254 (0.341) loss_u loss_u 0.9121 (0.9256) acc_u 12.5000 (9.2188) lr 1.9759e-03 eta 0:00:29
epoch [16/200] batch [25/83] time 0.454 (0.470) data 0.323 (0.338) loss_u loss_u 0.9014 (0.9238) acc_u 15.6250 (10.0000) lr 1.9759e-03 eta 0:00:27
epoch [16/200] batch [30/83] time 0.364 (0.464) data 0.233 (0.332) loss_u loss_u 0.9263 (0.9257) acc_u 9.3750 (9.5833) lr 1.9759e-03 eta 0:00:24
epoch [16/200] batch [35/83] time 0.452 (0.462) data 0.319 (0.330) loss_u loss_u 0.8584 (0.9249) acc_u 25.0000 (10.0893) lr 1.9759e-03 eta 0:00:22
epoch [16/200] batch [40/83] time 0.581 (0.470) data 0.447 (0.338) loss_u loss_u 0.8887 (0.9203) acc_u 18.7500 (10.7031) lr 1.9759e-03 eta 0:00:20
epoch [16/200] batch [45/83] time 0.425 (0.470) data 0.294 (0.339) loss_u loss_u 0.9761 (0.9216) acc_u 3.1250 (10.4861) lr 1.9759e-03 eta 0:00:17
epoch [16/200] batch [50/83] time 0.481 (0.467) data 0.350 (0.335) loss_u loss_u 0.9126 (0.9215) acc_u 15.6250 (10.5000) lr 1.9759e-03 eta 0:00:15
epoch [16/200] batch [55/83] time 0.461 (0.464) data 0.330 (0.332) loss_u loss_u 0.9502 (0.9230) acc_u 3.1250 (10.3977) lr 1.9759e-03 eta 0:00:13
epoch [16/200] batch [60/83] time 0.566 (0.463) data 0.434 (0.332) loss_u loss_u 0.9287 (0.9237) acc_u 6.2500 (10.3125) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [65/83] time 0.467 (0.463) data 0.336 (0.331) loss_u loss_u 0.9463 (0.9233) acc_u 3.1250 (10.3846) lr 1.9759e-03 eta 0:00:08
epoch [16/200] batch [70/83] time 0.422 (0.460) data 0.291 (0.328) loss_u loss_u 0.9194 (0.9214) acc_u 9.3750 (10.5804) lr 1.9759e-03 eta 0:00:05
epoch [16/200] batch [75/83] time 0.385 (0.459) data 0.254 (0.328) loss_u loss_u 0.8931 (0.9226) acc_u 15.6250 (10.3750) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [80/83] time 0.587 (0.457) data 0.456 (0.326) loss_u loss_u 0.9419 (0.9227) acc_u 6.2500 (10.3125) lr 1.9759e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2012
confident_label rate tensor(0.1575, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 494
clean true:315
clean false:179
clean_rate:0.6376518218623481
noisy true:809
noisy false:1833
after delete: len(clean_dataset) 494
after delete: len(noisy_dataset) 2642
epoch [17/200] batch [5/15] time 0.421 (0.482) data 0.290 (0.352) loss_x loss_x 1.3311 (1.2711) acc_x 59.3750 (63.7500) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [10/15] time 0.526 (0.500) data 0.395 (0.369) loss_x loss_x 1.4014 (1.4837) acc_x 59.3750 (59.0625) lr 1.9724e-03 eta 0:00:02
epoch [17/200] batch [15/15] time 0.468 (0.485) data 0.337 (0.355) loss_x loss_x 1.6533 (1.5639) acc_x 56.2500 (57.5000) lr 1.9724e-03 eta 0:00:00
epoch [17/200] batch [5/82] time 0.469 (0.477) data 0.336 (0.345) loss_u loss_u 0.9023 (0.9333) acc_u 12.5000 (6.8750) lr 1.9724e-03 eta 0:00:36
epoch [17/200] batch [10/82] time 0.444 (0.477) data 0.312 (0.345) loss_u loss_u 0.9463 (0.9266) acc_u 9.3750 (8.4375) lr 1.9724e-03 eta 0:00:34
epoch [17/200] batch [15/82] time 0.388 (0.478) data 0.256 (0.346) loss_u loss_u 0.8662 (0.9194) acc_u 18.7500 (9.3750) lr 1.9724e-03 eta 0:00:32
epoch [17/200] batch [20/82] time 0.365 (0.478) data 0.233 (0.346) loss_u loss_u 0.9668 (0.9267) acc_u 0.0000 (8.4375) lr 1.9724e-03 eta 0:00:29
epoch [17/200] batch [25/82] time 0.523 (0.472) data 0.391 (0.341) loss_u loss_u 0.9326 (0.9297) acc_u 6.2500 (7.8750) lr 1.9724e-03 eta 0:00:26
epoch [17/200] batch [30/82] time 0.433 (0.467) data 0.302 (0.335) loss_u loss_u 0.9663 (0.9301) acc_u 6.2500 (8.0208) lr 1.9724e-03 eta 0:00:24
epoch [17/200] batch [35/82] time 0.553 (0.467) data 0.421 (0.335) loss_u loss_u 0.9722 (0.9310) acc_u 3.1250 (8.1250) lr 1.9724e-03 eta 0:00:21
epoch [17/200] batch [40/82] time 0.446 (0.465) data 0.315 (0.333) loss_u loss_u 0.9048 (0.9297) acc_u 12.5000 (8.5938) lr 1.9724e-03 eta 0:00:19
epoch [17/200] batch [45/82] time 0.408 (0.464) data 0.276 (0.332) loss_u loss_u 0.9556 (0.9276) acc_u 6.2500 (8.8889) lr 1.9724e-03 eta 0:00:17
epoch [17/200] batch [50/82] time 0.468 (0.461) data 0.336 (0.329) loss_u loss_u 0.9385 (0.9279) acc_u 3.1250 (8.8125) lr 1.9724e-03 eta 0:00:14
epoch [17/200] batch [55/82] time 0.539 (0.462) data 0.409 (0.330) loss_u loss_u 0.9604 (0.9295) acc_u 6.2500 (8.5227) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [60/82] time 0.488 (0.464) data 0.354 (0.332) loss_u loss_u 0.9312 (0.9271) acc_u 12.5000 (9.0625) lr 1.9724e-03 eta 0:00:10
epoch [17/200] batch [65/82] time 0.399 (0.463) data 0.267 (0.331) loss_u loss_u 0.9429 (0.9278) acc_u 6.2500 (8.9904) lr 1.9724e-03 eta 0:00:07
epoch [17/200] batch [70/82] time 0.520 (0.464) data 0.389 (0.333) loss_u loss_u 0.9844 (0.9264) acc_u 0.0000 (9.1518) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [75/82] time 0.494 (0.465) data 0.363 (0.333) loss_u loss_u 0.9380 (0.9246) acc_u 3.1250 (9.4167) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [80/82] time 0.373 (0.463) data 0.241 (0.332) loss_u loss_u 0.9653 (0.9248) acc_u 0.0000 (9.3750) lr 1.9724e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1961
confident_label rate tensor(0.1588, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 498
clean true:328
clean false:170
clean_rate:0.6586345381526104
noisy true:847
noisy false:1791
after delete: len(clean_dataset) 498
after delete: len(noisy_dataset) 2638
epoch [18/200] batch [5/15] time 0.643 (0.456) data 0.512 (0.326) loss_x loss_x 0.9204 (1.4257) acc_x 78.1250 (63.1250) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [10/15] time 0.564 (0.449) data 0.434 (0.319) loss_x loss_x 1.2119 (1.5239) acc_x 68.7500 (60.0000) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [15/15] time 0.578 (0.468) data 0.447 (0.337) loss_x loss_x 1.9297 (1.5565) acc_x 43.7500 (58.7500) lr 1.9686e-03 eta 0:00:00
epoch [18/200] batch [5/82] time 0.384 (0.473) data 0.233 (0.340) loss_u loss_u 0.8989 (0.9104) acc_u 9.3750 (10.6250) lr 1.9686e-03 eta 0:00:36
epoch [18/200] batch [10/82] time 0.524 (0.462) data 0.393 (0.329) loss_u loss_u 0.9146 (0.9229) acc_u 12.5000 (10.3125) lr 1.9686e-03 eta 0:00:33
epoch [18/200] batch [15/82] time 0.324 (0.461) data 0.192 (0.329) loss_u loss_u 0.9502 (0.9255) acc_u 9.3750 (10.0000) lr 1.9686e-03 eta 0:00:30
epoch [18/200] batch [20/82] time 0.560 (0.471) data 0.430 (0.339) loss_u loss_u 0.9414 (0.9284) acc_u 6.2500 (9.3750) lr 1.9686e-03 eta 0:00:29
epoch [18/200] batch [25/82] time 0.434 (0.464) data 0.303 (0.332) loss_u loss_u 0.9365 (0.9277) acc_u 3.1250 (9.0000) lr 1.9686e-03 eta 0:00:26
epoch [18/200] batch [30/82] time 0.469 (0.461) data 0.338 (0.330) loss_u loss_u 0.9214 (0.9278) acc_u 9.3750 (9.1667) lr 1.9686e-03 eta 0:00:23
epoch [18/200] batch [35/82] time 0.439 (0.461) data 0.308 (0.329) loss_u loss_u 0.9634 (0.9287) acc_u 3.1250 (8.8393) lr 1.9686e-03 eta 0:00:21
epoch [18/200] batch [40/82] time 0.346 (0.462) data 0.215 (0.330) loss_u loss_u 0.9595 (0.9262) acc_u 6.2500 (9.1406) lr 1.9686e-03 eta 0:00:19
epoch [18/200] batch [45/82] time 0.442 (0.458) data 0.310 (0.326) loss_u loss_u 0.9185 (0.9257) acc_u 9.3750 (9.1667) lr 1.9686e-03 eta 0:00:16
epoch [18/200] batch [50/82] time 0.493 (0.460) data 0.362 (0.329) loss_u loss_u 0.9683 (0.9248) acc_u 3.1250 (9.4375) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [55/82] time 0.552 (0.460) data 0.420 (0.329) loss_u loss_u 0.9307 (0.9248) acc_u 6.2500 (9.5455) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [60/82] time 0.374 (0.458) data 0.243 (0.326) loss_u loss_u 0.9038 (0.9258) acc_u 15.6250 (9.3229) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [65/82] time 0.351 (0.458) data 0.220 (0.327) loss_u loss_u 0.8931 (0.9264) acc_u 12.5000 (9.2308) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [70/82] time 0.502 (0.458) data 0.371 (0.327) loss_u loss_u 0.9033 (0.9265) acc_u 12.5000 (9.2411) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [75/82] time 0.434 (0.456) data 0.302 (0.325) loss_u loss_u 0.9136 (0.9266) acc_u 15.6250 (9.1250) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [80/82] time 0.474 (0.458) data 0.342 (0.326) loss_u loss_u 0.9028 (0.9257) acc_u 9.3750 (9.1406) lr 1.9686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1916
confident_label rate tensor(0.1696, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 532
clean true:346
clean false:186
clean_rate:0.650375939849624
noisy true:874
noisy false:1730
after delete: len(clean_dataset) 532
after delete: len(noisy_dataset) 2604
epoch [19/200] batch [5/16] time 0.365 (0.436) data 0.235 (0.306) loss_x loss_x 2.1777 (1.7410) acc_x 43.7500 (54.3750) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [10/16] time 0.479 (0.466) data 0.347 (0.334) loss_x loss_x 1.4004 (1.6231) acc_x 71.8750 (56.5625) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [15/16] time 0.411 (0.460) data 0.280 (0.329) loss_x loss_x 1.3184 (1.6394) acc_x 62.5000 (56.0417) lr 1.9646e-03 eta 0:00:00
epoch [19/200] batch [5/81] time 0.403 (0.449) data 0.272 (0.317) loss_u loss_u 0.8472 (0.8930) acc_u 18.7500 (11.8750) lr 1.9646e-03 eta 0:00:34
epoch [19/200] batch [10/81] time 0.343 (0.447) data 0.211 (0.315) loss_u loss_u 0.9258 (0.9105) acc_u 12.5000 (10.9375) lr 1.9646e-03 eta 0:00:31
epoch [19/200] batch [15/81] time 0.362 (0.448) data 0.232 (0.316) loss_u loss_u 0.8809 (0.9167) acc_u 15.6250 (10.2083) lr 1.9646e-03 eta 0:00:29
epoch [19/200] batch [20/81] time 0.530 (0.453) data 0.399 (0.322) loss_u loss_u 0.9077 (0.9140) acc_u 9.3750 (10.7812) lr 1.9646e-03 eta 0:00:27
epoch [19/200] batch [25/81] time 0.385 (0.450) data 0.252 (0.319) loss_u loss_u 0.8823 (0.9148) acc_u 18.7500 (10.6250) lr 1.9646e-03 eta 0:00:25
epoch [19/200] batch [30/81] time 0.424 (0.451) data 0.293 (0.320) loss_u loss_u 0.9556 (0.9169) acc_u 3.1250 (10.4167) lr 1.9646e-03 eta 0:00:23
epoch [19/200] batch [35/81] time 0.406 (0.449) data 0.276 (0.317) loss_u loss_u 0.9839 (0.9224) acc_u 0.0000 (9.5536) lr 1.9646e-03 eta 0:00:20
epoch [19/200] batch [40/81] time 0.402 (0.449) data 0.271 (0.317) loss_u loss_u 0.9155 (0.9231) acc_u 18.7500 (9.7656) lr 1.9646e-03 eta 0:00:18
epoch [19/200] batch [45/81] time 0.482 (0.446) data 0.351 (0.315) loss_u loss_u 0.8774 (0.9237) acc_u 18.7500 (9.7917) lr 1.9646e-03 eta 0:00:16
epoch [19/200] batch [50/81] time 0.398 (0.449) data 0.265 (0.317) loss_u loss_u 0.8838 (0.9230) acc_u 21.8750 (10.2500) lr 1.9646e-03 eta 0:00:13
epoch [19/200] batch [55/81] time 0.607 (0.454) data 0.477 (0.323) loss_u loss_u 0.9219 (0.9222) acc_u 9.3750 (10.3409) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [60/81] time 0.449 (0.455) data 0.317 (0.324) loss_u loss_u 0.9478 (0.9223) acc_u 6.2500 (10.5208) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [65/81] time 0.457 (0.455) data 0.325 (0.323) loss_u loss_u 0.9102 (0.9228) acc_u 9.3750 (10.3365) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [70/81] time 0.404 (0.451) data 0.272 (0.320) loss_u loss_u 0.9009 (0.9226) acc_u 18.7500 (10.3571) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [75/81] time 0.450 (0.453) data 0.320 (0.321) loss_u loss_u 0.9326 (0.9232) acc_u 6.2500 (10.1667) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [80/81] time 0.482 (0.456) data 0.350 (0.325) loss_u loss_u 0.8691 (0.9225) acc_u 15.6250 (10.3125) lr 1.9646e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2011
confident_label rate tensor(0.1591, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 499
clean true:318
clean false:181
clean_rate:0.6372745490981964
noisy true:807
noisy false:1830
after delete: len(clean_dataset) 499
after delete: len(noisy_dataset) 2637
epoch [20/200] batch [5/15] time 0.496 (0.475) data 0.364 (0.343) loss_x loss_x 1.6592 (1.2728) acc_x 40.6250 (63.7500) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [10/15] time 0.497 (0.508) data 0.366 (0.377) loss_x loss_x 1.2344 (1.5271) acc_x 65.6250 (58.4375) lr 1.9603e-03 eta 0:00:02
epoch [20/200] batch [15/15] time 0.463 (0.484) data 0.331 (0.352) loss_x loss_x 1.9277 (1.5756) acc_x 50.0000 (57.5000) lr 1.9603e-03 eta 0:00:00
epoch [20/200] batch [5/82] time 0.410 (0.488) data 0.279 (0.357) loss_u loss_u 0.9438 (0.9256) acc_u 6.2500 (9.3750) lr 1.9603e-03 eta 0:00:37
epoch [20/200] batch [10/82] time 0.579 (0.483) data 0.447 (0.352) loss_u loss_u 0.9150 (0.9222) acc_u 12.5000 (10.0000) lr 1.9603e-03 eta 0:00:34
epoch [20/200] batch [15/82] time 0.441 (0.473) data 0.309 (0.341) loss_u loss_u 0.9067 (0.9227) acc_u 15.6250 (9.5833) lr 1.9603e-03 eta 0:00:31
epoch [20/200] batch [20/82] time 0.436 (0.470) data 0.303 (0.339) loss_u loss_u 0.9604 (0.9234) acc_u 3.1250 (9.2188) lr 1.9603e-03 eta 0:00:29
epoch [20/200] batch [25/82] time 0.412 (0.468) data 0.281 (0.337) loss_u loss_u 0.9512 (0.9240) acc_u 6.2500 (9.0000) lr 1.9603e-03 eta 0:00:26
epoch [20/200] batch [30/82] time 0.434 (0.464) data 0.303 (0.333) loss_u loss_u 0.9053 (0.9194) acc_u 12.5000 (9.6875) lr 1.9603e-03 eta 0:00:24
epoch [20/200] batch [35/82] time 0.509 (0.467) data 0.378 (0.335) loss_u loss_u 0.8823 (0.9158) acc_u 12.5000 (10.1786) lr 1.9603e-03 eta 0:00:21
epoch [20/200] batch [40/82] time 0.492 (0.461) data 0.361 (0.330) loss_u loss_u 0.9492 (0.9184) acc_u 3.1250 (9.7656) lr 1.9603e-03 eta 0:00:19
epoch [20/200] batch [45/82] time 0.386 (0.461) data 0.255 (0.329) loss_u loss_u 0.8765 (0.9166) acc_u 18.7500 (10.2778) lr 1.9603e-03 eta 0:00:17
epoch [20/200] batch [50/82] time 0.574 (0.460) data 0.443 (0.328) loss_u loss_u 0.9648 (0.9188) acc_u 3.1250 (10.0625) lr 1.9603e-03 eta 0:00:14
epoch [20/200] batch [55/82] time 0.439 (0.457) data 0.309 (0.325) loss_u loss_u 0.9131 (0.9172) acc_u 12.5000 (10.3977) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [60/82] time 0.538 (0.459) data 0.407 (0.328) loss_u loss_u 0.9395 (0.9169) acc_u 9.3750 (10.6250) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [65/82] time 0.579 (0.459) data 0.447 (0.328) loss_u loss_u 0.9453 (0.9176) acc_u 6.2500 (10.4808) lr 1.9603e-03 eta 0:00:07
epoch [20/200] batch [70/82] time 0.404 (0.462) data 0.273 (0.331) loss_u loss_u 0.9097 (0.9186) acc_u 6.2500 (10.2232) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [75/82] time 0.493 (0.463) data 0.360 (0.332) loss_u loss_u 0.9180 (0.9191) acc_u 9.3750 (10.1667) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [80/82] time 0.485 (0.460) data 0.354 (0.329) loss_u loss_u 0.9570 (0.9202) acc_u 3.1250 (9.9219) lr 1.9603e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1942
confident_label rate tensor(0.1617, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 507
clean true:334
clean false:173
clean_rate:0.6587771203155819
noisy true:860
noisy false:1769
after delete: len(clean_dataset) 507
after delete: len(noisy_dataset) 2629
epoch [21/200] batch [5/15] time 0.430 (0.531) data 0.300 (0.400) loss_x loss_x 2.1777 (1.7271) acc_x 46.8750 (52.5000) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [10/15] time 0.343 (0.469) data 0.213 (0.339) loss_x loss_x 1.3037 (1.5968) acc_x 56.2500 (55.3125) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [15/15] time 0.414 (0.460) data 0.283 (0.330) loss_x loss_x 1.5303 (1.5402) acc_x 56.2500 (56.0417) lr 1.9558e-03 eta 0:00:00
epoch [21/200] batch [5/82] time 0.432 (0.447) data 0.299 (0.316) loss_u loss_u 0.9199 (0.9343) acc_u 6.2500 (6.2500) lr 1.9558e-03 eta 0:00:34
epoch [21/200] batch [10/82] time 0.410 (0.436) data 0.279 (0.304) loss_u loss_u 0.9526 (0.9267) acc_u 6.2500 (7.8125) lr 1.9558e-03 eta 0:00:31
epoch [21/200] batch [15/82] time 0.393 (0.434) data 0.262 (0.303) loss_u loss_u 0.9507 (0.9250) acc_u 6.2500 (8.9583) lr 1.9558e-03 eta 0:00:29
epoch [21/200] batch [20/82] time 0.401 (0.449) data 0.270 (0.318) loss_u loss_u 0.9795 (0.9290) acc_u 0.0000 (8.1250) lr 1.9558e-03 eta 0:00:27
epoch [21/200] batch [25/82] time 0.406 (0.454) data 0.275 (0.322) loss_u loss_u 0.9043 (0.9262) acc_u 12.5000 (8.6250) lr 1.9558e-03 eta 0:00:25
epoch [21/200] batch [30/82] time 0.480 (0.458) data 0.348 (0.326) loss_u loss_u 0.9287 (0.9251) acc_u 6.2500 (9.0625) lr 1.9558e-03 eta 0:00:23
epoch [21/200] batch [35/82] time 0.424 (0.454) data 0.292 (0.323) loss_u loss_u 0.9014 (0.9245) acc_u 12.5000 (9.0179) lr 1.9558e-03 eta 0:00:21
epoch [21/200] batch [40/82] time 0.499 (0.455) data 0.368 (0.324) loss_u loss_u 0.9136 (0.9257) acc_u 12.5000 (9.0625) lr 1.9558e-03 eta 0:00:19
epoch [21/200] batch [45/82] time 0.446 (0.455) data 0.315 (0.324) loss_u loss_u 0.9297 (0.9265) acc_u 9.3750 (9.0278) lr 1.9558e-03 eta 0:00:16
epoch [21/200] batch [50/82] time 0.435 (0.457) data 0.303 (0.325) loss_u loss_u 0.9160 (0.9279) acc_u 15.6250 (8.8750) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [55/82] time 0.591 (0.460) data 0.459 (0.329) loss_u loss_u 0.9326 (0.9282) acc_u 6.2500 (8.8068) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [60/82] time 0.405 (0.458) data 0.274 (0.327) loss_u loss_u 0.9302 (0.9275) acc_u 6.2500 (8.9062) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [65/82] time 0.478 (0.462) data 0.347 (0.331) loss_u loss_u 0.9282 (0.9268) acc_u 6.2500 (9.0385) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [70/82] time 0.570 (0.467) data 0.437 (0.336) loss_u loss_u 0.8682 (0.9247) acc_u 25.0000 (9.4196) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [75/82] time 0.422 (0.467) data 0.291 (0.336) loss_u loss_u 0.8652 (0.9238) acc_u 21.8750 (9.5833) lr 1.9558e-03 eta 0:00:03
epoch [21/200] batch [80/82] time 0.436 (0.470) data 0.307 (0.338) loss_u loss_u 0.9326 (0.9231) acc_u 9.3750 (9.5703) lr 1.9558e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1965
confident_label rate tensor(0.1722, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 540
clean true:347
clean false:193
clean_rate:0.6425925925925926
noisy true:824
noisy false:1772
after delete: len(clean_dataset) 540
after delete: len(noisy_dataset) 2596
epoch [22/200] batch [5/16] time 0.566 (0.519) data 0.436 (0.389) loss_x loss_x 1.7471 (1.4775) acc_x 50.0000 (59.3750) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [10/16] time 0.531 (0.514) data 0.400 (0.383) loss_x loss_x 1.4355 (1.4448) acc_x 62.5000 (59.6875) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [15/16] time 0.488 (0.508) data 0.358 (0.378) loss_x loss_x 1.8945 (1.5022) acc_x 56.2500 (58.5417) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/81] time 0.500 (0.494) data 0.368 (0.363) loss_u loss_u 0.8623 (0.9215) acc_u 18.7500 (9.3750) lr 1.9511e-03 eta 0:00:37
epoch [22/200] batch [10/81] time 0.553 (0.490) data 0.420 (0.359) loss_u loss_u 0.9282 (0.9246) acc_u 12.5000 (10.0000) lr 1.9511e-03 eta 0:00:34
epoch [22/200] batch [15/81] time 0.566 (0.487) data 0.435 (0.356) loss_u loss_u 0.8921 (0.9249) acc_u 21.8750 (10.2083) lr 1.9511e-03 eta 0:00:32
epoch [22/200] batch [20/81] time 0.549 (0.483) data 0.416 (0.351) loss_u loss_u 0.9023 (0.9323) acc_u 15.6250 (9.0625) lr 1.9511e-03 eta 0:00:29
epoch [22/200] batch [25/81] time 0.423 (0.481) data 0.291 (0.349) loss_u loss_u 0.9404 (0.9287) acc_u 6.2500 (9.3750) lr 1.9511e-03 eta 0:00:26
epoch [22/200] batch [30/81] time 0.391 (0.476) data 0.261 (0.345) loss_u loss_u 0.9048 (0.9255) acc_u 15.6250 (9.8958) lr 1.9511e-03 eta 0:00:24
epoch [22/200] batch [35/81] time 0.797 (0.479) data 0.665 (0.348) loss_u loss_u 0.9263 (0.9266) acc_u 6.2500 (9.4643) lr 1.9511e-03 eta 0:00:22
epoch [22/200] batch [40/81] time 0.471 (0.476) data 0.340 (0.345) loss_u loss_u 0.8955 (0.9240) acc_u 9.3750 (9.7656) lr 1.9511e-03 eta 0:00:19
epoch [22/200] batch [45/81] time 0.550 (0.471) data 0.419 (0.340) loss_u loss_u 0.9121 (0.9241) acc_u 12.5000 (9.7917) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [50/81] time 0.366 (0.465) data 0.235 (0.334) loss_u loss_u 0.9497 (0.9230) acc_u 6.2500 (10.0000) lr 1.9511e-03 eta 0:00:14
epoch [22/200] batch [55/81] time 0.411 (0.465) data 0.280 (0.334) loss_u loss_u 0.9663 (0.9255) acc_u 6.2500 (9.5455) lr 1.9511e-03 eta 0:00:12
epoch [22/200] batch [60/81] time 0.444 (0.465) data 0.311 (0.334) loss_u loss_u 0.9399 (0.9257) acc_u 9.3750 (9.6875) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [65/81] time 0.434 (0.465) data 0.302 (0.334) loss_u loss_u 0.9272 (0.9252) acc_u 6.2500 (9.8558) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [70/81] time 0.532 (0.464) data 0.401 (0.333) loss_u loss_u 0.8926 (0.9244) acc_u 15.6250 (9.8661) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [75/81] time 0.412 (0.462) data 0.280 (0.331) loss_u loss_u 0.9541 (0.9256) acc_u 9.3750 (9.7917) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [80/81] time 0.526 (0.463) data 0.396 (0.332) loss_u loss_u 0.9194 (0.9259) acc_u 9.3750 (9.5703) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1959
confident_label rate tensor(0.1591, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 499
clean true:335
clean false:164
clean_rate:0.6713426853707415
noisy true:842
noisy false:1795
after delete: len(clean_dataset) 499
after delete: len(noisy_dataset) 2637
epoch [23/200] batch [5/15] time 0.423 (0.433) data 0.292 (0.302) loss_x loss_x 1.5723 (1.3227) acc_x 59.3750 (63.7500) lr 1.9461e-03 eta 0:00:04
epoch [23/200] batch [10/15] time 0.498 (0.446) data 0.367 (0.315) loss_x loss_x 1.4512 (1.4366) acc_x 56.2500 (61.8750) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [15/15] time 0.412 (0.455) data 0.282 (0.324) loss_x loss_x 2.0391 (1.4977) acc_x 50.0000 (60.4167) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/82] time 0.410 (0.447) data 0.278 (0.316) loss_u loss_u 0.9517 (0.9396) acc_u 6.2500 (5.6250) lr 1.9461e-03 eta 0:00:34
epoch [23/200] batch [10/82] time 0.540 (0.449) data 0.410 (0.318) loss_u loss_u 0.9536 (0.9375) acc_u 3.1250 (5.9375) lr 1.9461e-03 eta 0:00:32
epoch [23/200] batch [15/82] time 0.471 (0.448) data 0.340 (0.317) loss_u loss_u 0.8779 (0.9306) acc_u 12.5000 (7.9167) lr 1.9461e-03 eta 0:00:30
epoch [23/200] batch [20/82] time 0.453 (0.452) data 0.323 (0.321) loss_u loss_u 0.9248 (0.9305) acc_u 15.6250 (7.6562) lr 1.9461e-03 eta 0:00:28
epoch [23/200] batch [25/82] time 0.475 (0.449) data 0.344 (0.318) loss_u loss_u 0.9438 (0.9319) acc_u 3.1250 (7.2500) lr 1.9461e-03 eta 0:00:25
epoch [23/200] batch [30/82] time 0.291 (0.445) data 0.160 (0.314) loss_u loss_u 0.9331 (0.9282) acc_u 9.3750 (8.0208) lr 1.9461e-03 eta 0:00:23
epoch [23/200] batch [35/82] time 0.495 (0.443) data 0.364 (0.312) loss_u loss_u 0.9399 (0.9287) acc_u 6.2500 (8.0357) lr 1.9461e-03 eta 0:00:20
epoch [23/200] batch [40/82] time 0.406 (0.445) data 0.275 (0.314) loss_u loss_u 0.9663 (0.9288) acc_u 6.2500 (8.5156) lr 1.9461e-03 eta 0:00:18
epoch [23/200] batch [45/82] time 0.524 (0.449) data 0.393 (0.318) loss_u loss_u 0.9185 (0.9287) acc_u 12.5000 (8.7500) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [50/82] time 0.510 (0.450) data 0.378 (0.319) loss_u loss_u 0.9258 (0.9282) acc_u 9.3750 (8.8125) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [55/82] time 0.442 (0.451) data 0.310 (0.320) loss_u loss_u 0.8633 (0.9265) acc_u 18.7500 (9.2045) lr 1.9461e-03 eta 0:00:12
epoch [23/200] batch [60/82] time 0.408 (0.450) data 0.277 (0.319) loss_u loss_u 0.8911 (0.9258) acc_u 15.6250 (9.3229) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [65/82] time 0.531 (0.452) data 0.400 (0.321) loss_u loss_u 0.9072 (0.9235) acc_u 12.5000 (9.6635) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [70/82] time 0.432 (0.453) data 0.301 (0.321) loss_u loss_u 0.9243 (0.9247) acc_u 12.5000 (9.5982) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [75/82] time 0.536 (0.456) data 0.405 (0.325) loss_u loss_u 0.8823 (0.9233) acc_u 18.7500 (9.9167) lr 1.9461e-03 eta 0:00:03
epoch [23/200] batch [80/82] time 0.555 (0.455) data 0.424 (0.324) loss_u loss_u 0.8740 (0.9222) acc_u 12.5000 (10.0781) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1979
confident_label rate tensor(0.1770, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 555
clean true:352
clean false:203
clean_rate:0.6342342342342342
noisy true:805
noisy false:1776
after delete: len(clean_dataset) 555
after delete: len(noisy_dataset) 2581
epoch [24/200] batch [5/17] time 0.437 (0.475) data 0.307 (0.345) loss_x loss_x 1.6240 (1.5334) acc_x 53.1250 (58.1250) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [10/17] time 0.569 (0.459) data 0.438 (0.328) loss_x loss_x 1.5635 (1.5480) acc_x 43.7500 (56.8750) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [15/17] time 0.406 (0.455) data 0.276 (0.325) loss_x loss_x 1.9795 (1.5417) acc_x 43.7500 (57.2917) lr 1.9409e-03 eta 0:00:00
epoch [24/200] batch [5/80] time 0.344 (0.438) data 0.211 (0.307) loss_u loss_u 0.9536 (0.9172) acc_u 6.2500 (11.8750) lr 1.9409e-03 eta 0:00:32
epoch [24/200] batch [10/80] time 0.518 (0.439) data 0.386 (0.308) loss_u loss_u 0.9043 (0.9165) acc_u 9.3750 (10.9375) lr 1.9409e-03 eta 0:00:30
epoch [24/200] batch [15/80] time 0.446 (0.443) data 0.315 (0.311) loss_u loss_u 0.8882 (0.9191) acc_u 15.6250 (10.4167) lr 1.9409e-03 eta 0:00:28
epoch [24/200] batch [20/80] time 0.481 (0.450) data 0.350 (0.319) loss_u loss_u 0.8818 (0.9160) acc_u 12.5000 (10.6250) lr 1.9409e-03 eta 0:00:27
epoch [24/200] batch [25/80] time 0.440 (0.446) data 0.310 (0.314) loss_u loss_u 0.9209 (0.9208) acc_u 9.3750 (9.8750) lr 1.9409e-03 eta 0:00:24
epoch [24/200] batch [30/80] time 0.416 (0.447) data 0.284 (0.316) loss_u loss_u 0.9346 (0.9221) acc_u 6.2500 (9.6875) lr 1.9409e-03 eta 0:00:22
epoch [24/200] batch [35/80] time 0.524 (0.450) data 0.393 (0.319) loss_u loss_u 0.9409 (0.9262) acc_u 6.2500 (9.4643) lr 1.9409e-03 eta 0:00:20
epoch [24/200] batch [40/80] time 0.409 (0.457) data 0.277 (0.326) loss_u loss_u 0.8999 (0.9256) acc_u 12.5000 (9.5312) lr 1.9409e-03 eta 0:00:18
epoch [24/200] batch [45/80] time 0.505 (0.457) data 0.374 (0.326) loss_u loss_u 0.9355 (0.9238) acc_u 6.2500 (9.5833) lr 1.9409e-03 eta 0:00:15
epoch [24/200] batch [50/80] time 0.453 (0.458) data 0.321 (0.327) loss_u loss_u 0.8579 (0.9220) acc_u 25.0000 (10.0625) lr 1.9409e-03 eta 0:00:13
epoch [24/200] batch [55/80] time 0.523 (0.455) data 0.392 (0.324) loss_u loss_u 0.9492 (0.9224) acc_u 9.3750 (10.2841) lr 1.9409e-03 eta 0:00:11
epoch [24/200] batch [60/80] time 0.453 (0.453) data 0.321 (0.322) loss_u loss_u 0.9233 (0.9234) acc_u 9.3750 (10.1042) lr 1.9409e-03 eta 0:00:09
epoch [24/200] batch [65/80] time 0.495 (0.453) data 0.362 (0.321) loss_u loss_u 0.9424 (0.9220) acc_u 6.2500 (10.3846) lr 1.9409e-03 eta 0:00:06
epoch [24/200] batch [70/80] time 0.383 (0.449) data 0.250 (0.318) loss_u loss_u 0.9795 (0.9240) acc_u 3.1250 (10.1339) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [75/80] time 0.540 (0.450) data 0.409 (0.318) loss_u loss_u 0.9341 (0.9239) acc_u 12.5000 (10.1667) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [80/80] time 0.576 (0.454) data 0.445 (0.323) loss_u loss_u 0.9736 (0.9252) acc_u 3.1250 (10.0000) lr 1.9409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2044
confident_label rate tensor(0.1626, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 510
clean true:309
clean false:201
clean_rate:0.6058823529411764
noisy true:783
noisy false:1843
after delete: len(clean_dataset) 510
after delete: len(noisy_dataset) 2626
epoch [25/200] batch [5/15] time 0.482 (0.478) data 0.352 (0.347) loss_x loss_x 1.4424 (1.6576) acc_x 62.5000 (61.2500) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [10/15] time 0.441 (0.463) data 0.310 (0.332) loss_x loss_x 1.5605 (1.6520) acc_x 59.3750 (59.6875) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [15/15] time 0.395 (0.451) data 0.265 (0.320) loss_x loss_x 1.1611 (1.5874) acc_x 65.6250 (61.0417) lr 1.9354e-03 eta 0:00:00
epoch [25/200] batch [5/82] time 0.433 (0.448) data 0.302 (0.318) loss_u loss_u 0.9072 (0.8893) acc_u 9.3750 (14.3750) lr 1.9354e-03 eta 0:00:34
epoch [25/200] batch [10/82] time 0.386 (0.462) data 0.255 (0.331) loss_u loss_u 0.9429 (0.9100) acc_u 12.5000 (11.8750) lr 1.9354e-03 eta 0:00:33
epoch [25/200] batch [15/82] time 0.330 (0.456) data 0.200 (0.325) loss_u loss_u 0.9287 (0.9169) acc_u 9.3750 (10.8333) lr 1.9354e-03 eta 0:00:30
epoch [25/200] batch [20/82] time 0.403 (0.454) data 0.272 (0.323) loss_u loss_u 0.9346 (0.9159) acc_u 9.3750 (11.0938) lr 1.9354e-03 eta 0:00:28
epoch [25/200] batch [25/82] time 0.428 (0.447) data 0.297 (0.316) loss_u loss_u 0.9092 (0.9172) acc_u 9.3750 (10.6250) lr 1.9354e-03 eta 0:00:25
epoch [25/200] batch [30/82] time 0.442 (0.448) data 0.311 (0.317) loss_u loss_u 0.9111 (0.9191) acc_u 18.7500 (10.6250) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [35/82] time 0.374 (0.448) data 0.244 (0.317) loss_u loss_u 0.9546 (0.9197) acc_u 9.3750 (10.8929) lr 1.9354e-03 eta 0:00:21
epoch [25/200] batch [40/82] time 0.427 (0.447) data 0.296 (0.316) loss_u loss_u 0.9434 (0.9207) acc_u 6.2500 (10.8594) lr 1.9354e-03 eta 0:00:18
epoch [25/200] batch [45/82] time 0.403 (0.445) data 0.271 (0.314) loss_u loss_u 0.9404 (0.9227) acc_u 6.2500 (10.3472) lr 1.9354e-03 eta 0:00:16
epoch [25/200] batch [50/82] time 0.455 (0.448) data 0.324 (0.317) loss_u loss_u 0.9468 (0.9214) acc_u 3.1250 (10.2500) lr 1.9354e-03 eta 0:00:14
epoch [25/200] batch [55/82] time 0.387 (0.449) data 0.257 (0.318) loss_u loss_u 0.9517 (0.9225) acc_u 6.2500 (10.1136) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [60/82] time 0.506 (0.447) data 0.376 (0.316) loss_u loss_u 0.9468 (0.9230) acc_u 6.2500 (10.0521) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [65/82] time 0.605 (0.450) data 0.474 (0.319) loss_u loss_u 0.9375 (0.9228) acc_u 9.3750 (10.0962) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [70/82] time 0.605 (0.449) data 0.474 (0.318) loss_u loss_u 0.8984 (0.9234) acc_u 15.6250 (10.0893) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [75/82] time 0.423 (0.453) data 0.292 (0.322) loss_u loss_u 0.9468 (0.9238) acc_u 6.2500 (9.9583) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [80/82] time 0.492 (0.454) data 0.360 (0.323) loss_u loss_u 0.9165 (0.9228) acc_u 12.5000 (10.1562) lr 1.9354e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2009
confident_label rate tensor(0.1668, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 523
clean true:330
clean false:193
clean_rate:0.6309751434034416
noisy true:797
noisy false:1816
after delete: len(clean_dataset) 523
after delete: len(noisy_dataset) 2613
epoch [26/200] batch [5/16] time 0.480 (0.424) data 0.350 (0.293) loss_x loss_x 1.9502 (1.5566) acc_x 59.3750 (61.8750) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [10/16] time 0.608 (0.482) data 0.477 (0.351) loss_x loss_x 1.3418 (1.6168) acc_x 68.7500 (61.8750) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [15/16] time 0.465 (0.485) data 0.334 (0.354) loss_x loss_x 1.8105 (1.5472) acc_x 62.5000 (63.1250) lr 1.9298e-03 eta 0:00:00
epoch [26/200] batch [5/81] time 0.493 (0.478) data 0.361 (0.348) loss_u loss_u 0.8721 (0.9056) acc_u 21.8750 (11.8750) lr 1.9298e-03 eta 0:00:36
epoch [26/200] batch [10/81] time 0.473 (0.472) data 0.341 (0.341) loss_u loss_u 0.9429 (0.9228) acc_u 6.2500 (9.0625) lr 1.9298e-03 eta 0:00:33
epoch [26/200] batch [15/81] time 0.360 (0.471) data 0.228 (0.339) loss_u loss_u 0.9346 (0.9271) acc_u 9.3750 (8.9583) lr 1.9298e-03 eta 0:00:31
epoch [26/200] batch [20/81] time 0.457 (0.473) data 0.326 (0.341) loss_u loss_u 0.9634 (0.9201) acc_u 3.1250 (10.1562) lr 1.9298e-03 eta 0:00:28
epoch [26/200] batch [25/81] time 0.442 (0.469) data 0.312 (0.338) loss_u loss_u 0.9053 (0.9193) acc_u 9.3750 (9.8750) lr 1.9298e-03 eta 0:00:26
epoch [26/200] batch [30/81] time 0.568 (0.477) data 0.438 (0.346) loss_u loss_u 0.9644 (0.9187) acc_u 3.1250 (10.2083) lr 1.9298e-03 eta 0:00:24
epoch [26/200] batch [35/81] time 0.444 (0.475) data 0.313 (0.344) loss_u loss_u 0.8477 (0.9170) acc_u 18.7500 (10.5357) lr 1.9298e-03 eta 0:00:21
epoch [26/200] batch [40/81] time 0.568 (0.476) data 0.436 (0.345) loss_u loss_u 0.9502 (0.9178) acc_u 3.1250 (10.3125) lr 1.9298e-03 eta 0:00:19
epoch [26/200] batch [45/81] time 0.487 (0.476) data 0.356 (0.345) loss_u loss_u 0.9038 (0.9200) acc_u 18.7500 (10.0694) lr 1.9298e-03 eta 0:00:17
epoch [26/200] batch [50/81] time 0.386 (0.472) data 0.256 (0.341) loss_u loss_u 0.8574 (0.9167) acc_u 18.7500 (10.9375) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [55/81] time 0.425 (0.470) data 0.294 (0.339) loss_u loss_u 0.9268 (0.9159) acc_u 6.2500 (10.7955) lr 1.9298e-03 eta 0:00:12
epoch [26/200] batch [60/81] time 0.366 (0.466) data 0.236 (0.335) loss_u loss_u 0.9282 (0.9162) acc_u 12.5000 (10.8854) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [65/81] time 0.544 (0.464) data 0.413 (0.333) loss_u loss_u 0.8696 (0.9142) acc_u 12.5000 (11.1058) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [70/81] time 0.365 (0.460) data 0.234 (0.329) loss_u loss_u 0.8955 (0.9141) acc_u 15.6250 (11.2946) lr 1.9298e-03 eta 0:00:05
epoch [26/200] batch [75/81] time 0.460 (0.459) data 0.328 (0.328) loss_u loss_u 0.9316 (0.9151) acc_u 9.3750 (11.0833) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [80/81] time 0.423 (0.460) data 0.293 (0.329) loss_u loss_u 0.9067 (0.9158) acc_u 6.2500 (10.8203) lr 1.9298e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2017
confident_label rate tensor(0.1709, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 536
clean true:315
clean false:221
clean_rate:0.5876865671641791
noisy true:804
noisy false:1796
after delete: len(clean_dataset) 536
after delete: len(noisy_dataset) 2600
epoch [27/200] batch [5/16] time 0.385 (0.499) data 0.253 (0.368) loss_x loss_x 1.3623 (1.4296) acc_x 56.2500 (61.8750) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [10/16] time 0.401 (0.509) data 0.270 (0.378) loss_x loss_x 1.4902 (1.4352) acc_x 68.7500 (60.6250) lr 1.9239e-03 eta 0:00:03
epoch [27/200] batch [15/16] time 0.401 (0.502) data 0.270 (0.371) loss_x loss_x 1.7725 (1.4659) acc_x 59.3750 (61.0417) lr 1.9239e-03 eta 0:00:00
epoch [27/200] batch [5/81] time 0.506 (0.483) data 0.375 (0.352) loss_u loss_u 0.9604 (0.9241) acc_u 0.0000 (10.0000) lr 1.9239e-03 eta 0:00:36
epoch [27/200] batch [10/81] time 0.505 (0.479) data 0.375 (0.348) loss_u loss_u 0.9224 (0.9245) acc_u 9.3750 (9.6875) lr 1.9239e-03 eta 0:00:33
epoch [27/200] batch [15/81] time 0.480 (0.473) data 0.350 (0.342) loss_u loss_u 0.9673 (0.9279) acc_u 6.2500 (9.7917) lr 1.9239e-03 eta 0:00:31
epoch [27/200] batch [20/81] time 0.480 (0.474) data 0.349 (0.343) loss_u loss_u 0.9302 (0.9257) acc_u 9.3750 (9.8438) lr 1.9239e-03 eta 0:00:28
epoch [27/200] batch [25/81] time 0.442 (0.472) data 0.312 (0.341) loss_u loss_u 0.9019 (0.9207) acc_u 12.5000 (10.3750) lr 1.9239e-03 eta 0:00:26
epoch [27/200] batch [30/81] time 0.414 (0.467) data 0.283 (0.336) loss_u loss_u 0.9390 (0.9212) acc_u 9.3750 (10.8333) lr 1.9239e-03 eta 0:00:23
epoch [27/200] batch [35/81] time 0.439 (0.467) data 0.308 (0.336) loss_u loss_u 0.9448 (0.9188) acc_u 12.5000 (11.2500) lr 1.9239e-03 eta 0:00:21
epoch [27/200] batch [40/81] time 0.465 (0.464) data 0.335 (0.333) loss_u loss_u 0.8604 (0.9168) acc_u 15.6250 (11.1719) lr 1.9239e-03 eta 0:00:19
epoch [27/200] batch [45/81] time 0.406 (0.466) data 0.274 (0.335) loss_u loss_u 0.9634 (0.9179) acc_u 9.3750 (10.9722) lr 1.9239e-03 eta 0:00:16
epoch [27/200] batch [50/81] time 0.490 (0.464) data 0.358 (0.333) loss_u loss_u 0.9160 (0.9180) acc_u 9.3750 (10.7500) lr 1.9239e-03 eta 0:00:14
epoch [27/200] batch [55/81] time 0.410 (0.463) data 0.280 (0.332) loss_u loss_u 0.9067 (0.9177) acc_u 9.3750 (10.6818) lr 1.9239e-03 eta 0:00:12
epoch [27/200] batch [60/81] time 0.483 (0.463) data 0.353 (0.332) loss_u loss_u 0.9185 (0.9167) acc_u 9.3750 (10.6250) lr 1.9239e-03 eta 0:00:09
epoch [27/200] batch [65/81] time 0.447 (0.459) data 0.314 (0.328) loss_u loss_u 0.9604 (0.9192) acc_u 0.0000 (10.2885) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [70/81] time 0.615 (0.464) data 0.485 (0.333) loss_u loss_u 0.9292 (0.9195) acc_u 15.6250 (10.4464) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [75/81] time 0.563 (0.469) data 0.431 (0.338) loss_u loss_u 0.9282 (0.9204) acc_u 6.2500 (10.2917) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [80/81] time 0.397 (0.468) data 0.266 (0.337) loss_u loss_u 0.9570 (0.9211) acc_u 6.2500 (10.1953) lr 1.9239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2032
confident_label rate tensor(0.1783, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 559
clean true:326
clean false:233
clean_rate:0.5831842576028623
noisy true:778
noisy false:1799
after delete: len(clean_dataset) 559
after delete: len(noisy_dataset) 2577
epoch [28/200] batch [5/17] time 0.543 (0.493) data 0.413 (0.362) loss_x loss_x 1.5605 (1.7063) acc_x 50.0000 (52.5000) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [10/17] time 0.485 (0.481) data 0.354 (0.350) loss_x loss_x 1.1865 (1.5585) acc_x 59.3750 (58.4375) lr 1.9178e-03 eta 0:00:03
epoch [28/200] batch [15/17] time 0.409 (0.470) data 0.279 (0.340) loss_x loss_x 1.5166 (1.6077) acc_x 62.5000 (57.5000) lr 1.9178e-03 eta 0:00:00
epoch [28/200] batch [5/80] time 0.484 (0.462) data 0.353 (0.331) loss_u loss_u 0.9429 (0.9266) acc_u 6.2500 (10.0000) lr 1.9178e-03 eta 0:00:34
epoch [28/200] batch [10/80] time 0.436 (0.452) data 0.304 (0.321) loss_u loss_u 0.9429 (0.9332) acc_u 6.2500 (8.7500) lr 1.9178e-03 eta 0:00:31
epoch [28/200] batch [15/80] time 0.506 (0.457) data 0.375 (0.326) loss_u loss_u 0.9263 (0.9313) acc_u 6.2500 (8.5417) lr 1.9178e-03 eta 0:00:29
epoch [28/200] batch [20/80] time 0.461 (0.450) data 0.329 (0.319) loss_u loss_u 0.9131 (0.9315) acc_u 12.5000 (9.2188) lr 1.9178e-03 eta 0:00:27
epoch [28/200] batch [25/80] time 0.520 (0.459) data 0.389 (0.328) loss_u loss_u 0.9077 (0.9302) acc_u 12.5000 (9.2500) lr 1.9178e-03 eta 0:00:25
epoch [28/200] batch [30/80] time 0.594 (0.466) data 0.464 (0.335) loss_u loss_u 0.8950 (0.9266) acc_u 9.3750 (9.7917) lr 1.9178e-03 eta 0:00:23
epoch [28/200] batch [35/80] time 0.471 (0.468) data 0.340 (0.337) loss_u loss_u 0.9375 (0.9266) acc_u 6.2500 (9.7321) lr 1.9178e-03 eta 0:00:21
epoch [28/200] batch [40/80] time 0.392 (0.464) data 0.260 (0.333) loss_u loss_u 0.9009 (0.9234) acc_u 15.6250 (10.4688) lr 1.9178e-03 eta 0:00:18
epoch [28/200] batch [45/80] time 0.482 (0.468) data 0.351 (0.337) loss_u loss_u 0.9492 (0.9218) acc_u 3.1250 (10.4861) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [50/80] time 0.451 (0.464) data 0.320 (0.333) loss_u loss_u 0.8789 (0.9216) acc_u 15.6250 (10.6875) lr 1.9178e-03 eta 0:00:13
epoch [28/200] batch [55/80] time 0.431 (0.461) data 0.300 (0.329) loss_u loss_u 0.9390 (0.9216) acc_u 6.2500 (10.5682) lr 1.9178e-03 eta 0:00:11
epoch [28/200] batch [60/80] time 0.531 (0.462) data 0.400 (0.331) loss_u loss_u 0.8916 (0.9201) acc_u 9.3750 (10.6250) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [65/80] time 0.418 (0.464) data 0.287 (0.333) loss_u loss_u 0.9131 (0.9204) acc_u 6.2500 (10.5288) lr 1.9178e-03 eta 0:00:06
epoch [28/200] batch [70/80] time 0.371 (0.462) data 0.239 (0.331) loss_u loss_u 0.9551 (0.9188) acc_u 9.3750 (10.6696) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [75/80] time 0.372 (0.463) data 0.240 (0.331) loss_u loss_u 0.9341 (0.9191) acc_u 6.2500 (10.6250) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [80/80] time 0.401 (0.460) data 0.271 (0.329) loss_u loss_u 0.8711 (0.9191) acc_u 21.8750 (10.7031) lr 1.9178e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1969
confident_label rate tensor(0.1754, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 550
clean true:347
clean false:203
clean_rate:0.6309090909090909
noisy true:820
noisy false:1766
after delete: len(clean_dataset) 550
after delete: len(noisy_dataset) 2586
epoch [29/200] batch [5/17] time 0.502 (0.511) data 0.371 (0.380) loss_x loss_x 1.6240 (1.5188) acc_x 62.5000 (63.1250) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [10/17] time 0.421 (0.487) data 0.290 (0.356) loss_x loss_x 1.4453 (1.5596) acc_x 56.2500 (60.9375) lr 1.9114e-03 eta 0:00:03
epoch [29/200] batch [15/17] time 0.379 (0.483) data 0.248 (0.352) loss_x loss_x 1.4619 (1.5125) acc_x 71.8750 (62.2917) lr 1.9114e-03 eta 0:00:00
epoch [29/200] batch [5/80] time 0.537 (0.485) data 0.406 (0.355) loss_u loss_u 0.9082 (0.9170) acc_u 12.5000 (9.3750) lr 1.9114e-03 eta 0:00:36
epoch [29/200] batch [10/80] time 0.452 (0.487) data 0.321 (0.356) loss_u loss_u 0.8491 (0.9112) acc_u 15.6250 (10.3125) lr 1.9114e-03 eta 0:00:34
epoch [29/200] batch [15/80] time 0.353 (0.478) data 0.222 (0.348) loss_u loss_u 0.9375 (0.9217) acc_u 9.3750 (9.5833) lr 1.9114e-03 eta 0:00:31
epoch [29/200] batch [20/80] time 0.467 (0.473) data 0.336 (0.342) loss_u loss_u 0.9155 (0.9188) acc_u 15.6250 (10.3125) lr 1.9114e-03 eta 0:00:28
epoch [29/200] batch [25/80] time 0.501 (0.471) data 0.370 (0.340) loss_u loss_u 0.9341 (0.9203) acc_u 15.6250 (10.1250) lr 1.9114e-03 eta 0:00:25
epoch [29/200] batch [30/80] time 0.459 (0.469) data 0.325 (0.338) loss_u loss_u 0.9365 (0.9219) acc_u 6.2500 (9.6875) lr 1.9114e-03 eta 0:00:23
epoch [29/200] batch [35/80] time 0.348 (0.470) data 0.216 (0.339) loss_u loss_u 0.9570 (0.9221) acc_u 6.2500 (9.9107) lr 1.9114e-03 eta 0:00:21
epoch [29/200] batch [40/80] time 0.561 (0.473) data 0.430 (0.342) loss_u loss_u 0.9106 (0.9203) acc_u 12.5000 (10.2344) lr 1.9114e-03 eta 0:00:18
epoch [29/200] batch [45/80] time 0.482 (0.472) data 0.350 (0.341) loss_u loss_u 0.8896 (0.9207) acc_u 18.7500 (10.4861) lr 1.9114e-03 eta 0:00:16
epoch [29/200] batch [50/80] time 0.548 (0.471) data 0.418 (0.340) loss_u loss_u 0.8379 (0.9184) acc_u 21.8750 (10.9375) lr 1.9114e-03 eta 0:00:14
epoch [29/200] batch [55/80] time 0.548 (0.472) data 0.418 (0.341) loss_u loss_u 0.9111 (0.9182) acc_u 9.3750 (11.0795) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [60/80] time 0.441 (0.472) data 0.309 (0.341) loss_u loss_u 0.9160 (0.9186) acc_u 6.2500 (10.8854) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [65/80] time 0.438 (0.470) data 0.307 (0.339) loss_u loss_u 0.9614 (0.9190) acc_u 6.2500 (10.7212) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [70/80] time 0.498 (0.468) data 0.366 (0.337) loss_u loss_u 0.9087 (0.9193) acc_u 15.6250 (10.7589) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [75/80] time 0.434 (0.466) data 0.302 (0.335) loss_u loss_u 0.8809 (0.9194) acc_u 15.6250 (10.7083) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [80/80] time 0.409 (0.466) data 0.278 (0.335) loss_u loss_u 0.9102 (0.9202) acc_u 9.3750 (10.4297) lr 1.9114e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2034
confident_label rate tensor(0.1741, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 546
clean true:313
clean false:233
clean_rate:0.5732600732600732
noisy true:789
noisy false:1801
after delete: len(clean_dataset) 546
after delete: len(noisy_dataset) 2590
epoch [30/200] batch [5/17] time 0.467 (0.443) data 0.336 (0.312) loss_x loss_x 1.4023 (1.4396) acc_x 68.7500 (62.5000) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [10/17] time 0.389 (0.473) data 0.259 (0.342) loss_x loss_x 1.4600 (1.5794) acc_x 68.7500 (60.3125) lr 1.9048e-03 eta 0:00:03
epoch [30/200] batch [15/17] time 0.395 (0.459) data 0.265 (0.328) loss_x loss_x 0.9751 (1.5506) acc_x 75.0000 (60.4167) lr 1.9048e-03 eta 0:00:00
epoch [30/200] batch [5/80] time 0.429 (0.445) data 0.296 (0.314) loss_u loss_u 0.9492 (0.8995) acc_u 6.2500 (11.8750) lr 1.9048e-03 eta 0:00:33
epoch [30/200] batch [10/80] time 0.471 (0.444) data 0.339 (0.313) loss_u loss_u 0.8438 (0.8980) acc_u 21.8750 (12.1875) lr 1.9048e-03 eta 0:00:31
epoch [30/200] batch [15/80] time 0.683 (0.447) data 0.552 (0.316) loss_u loss_u 0.9346 (0.9122) acc_u 6.2500 (10.8333) lr 1.9048e-03 eta 0:00:29
epoch [30/200] batch [20/80] time 0.451 (0.442) data 0.319 (0.311) loss_u loss_u 0.9375 (0.9179) acc_u 9.3750 (10.0000) lr 1.9048e-03 eta 0:00:26
epoch [30/200] batch [25/80] time 0.469 (0.441) data 0.335 (0.310) loss_u loss_u 0.8906 (0.9203) acc_u 18.7500 (9.7500) lr 1.9048e-03 eta 0:00:24
epoch [30/200] batch [30/80] time 0.484 (0.443) data 0.353 (0.311) loss_u loss_u 0.8965 (0.9201) acc_u 15.6250 (9.8958) lr 1.9048e-03 eta 0:00:22
epoch [30/200] batch [35/80] time 0.436 (0.447) data 0.304 (0.316) loss_u loss_u 0.9214 (0.9177) acc_u 6.2500 (10.3571) lr 1.9048e-03 eta 0:00:20
epoch [30/200] batch [40/80] time 0.744 (0.456) data 0.613 (0.324) loss_u loss_u 0.9712 (0.9213) acc_u 3.1250 (9.8438) lr 1.9048e-03 eta 0:00:18
epoch [30/200] batch [45/80] time 0.346 (0.452) data 0.214 (0.321) loss_u loss_u 0.9453 (0.9213) acc_u 3.1250 (9.7917) lr 1.9048e-03 eta 0:00:15
epoch [30/200] batch [50/80] time 0.411 (0.451) data 0.280 (0.319) loss_u loss_u 0.8848 (0.9193) acc_u 18.7500 (10.3750) lr 1.9048e-03 eta 0:00:13
epoch [30/200] batch [55/80] time 0.308 (0.449) data 0.176 (0.317) loss_u loss_u 0.9531 (0.9218) acc_u 3.1250 (9.8295) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [60/80] time 0.570 (0.450) data 0.438 (0.319) loss_u loss_u 0.9780 (0.9240) acc_u 3.1250 (9.5312) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [65/80] time 0.364 (0.453) data 0.232 (0.322) loss_u loss_u 0.9077 (0.9221) acc_u 9.3750 (9.9519) lr 1.9048e-03 eta 0:00:06
epoch [30/200] batch [70/80] time 0.580 (0.453) data 0.448 (0.321) loss_u loss_u 0.9072 (0.9219) acc_u 12.5000 (9.9107) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [75/80] time 0.363 (0.453) data 0.231 (0.322) loss_u loss_u 0.9546 (0.9230) acc_u 6.2500 (9.8333) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [80/80] time 0.548 (0.455) data 0.414 (0.323) loss_u loss_u 0.9023 (0.9235) acc_u 9.3750 (9.6875) lr 1.9048e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2021
confident_label rate tensor(0.1776, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 557
clean true:327
clean false:230
clean_rate:0.5870736086175943
noisy true:788
noisy false:1791
after delete: len(clean_dataset) 557
after delete: len(noisy_dataset) 2579
epoch [31/200] batch [5/17] time 0.454 (0.487) data 0.324 (0.357) loss_x loss_x 1.1797 (1.4754) acc_x 59.3750 (63.1250) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [10/17] time 0.634 (0.511) data 0.503 (0.380) loss_x loss_x 1.4258 (1.3864) acc_x 56.2500 (63.7500) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [15/17] time 0.567 (0.491) data 0.436 (0.360) loss_x loss_x 1.2891 (1.4452) acc_x 78.1250 (62.9167) lr 1.8980e-03 eta 0:00:00
epoch [31/200] batch [5/80] time 0.427 (0.486) data 0.295 (0.355) loss_u loss_u 0.9097 (0.9159) acc_u 15.6250 (10.6250) lr 1.8980e-03 eta 0:00:36
epoch [31/200] batch [10/80] time 0.360 (0.476) data 0.228 (0.345) loss_u loss_u 0.9058 (0.9058) acc_u 9.3750 (12.1875) lr 1.8980e-03 eta 0:00:33
epoch [31/200] batch [15/80] time 0.530 (0.482) data 0.400 (0.351) loss_u loss_u 0.9072 (0.9071) acc_u 9.3750 (11.6667) lr 1.8980e-03 eta 0:00:31
epoch [31/200] batch [20/80] time 0.415 (0.476) data 0.283 (0.345) loss_u loss_u 0.9434 (0.9125) acc_u 9.3750 (11.0938) lr 1.8980e-03 eta 0:00:28
epoch [31/200] batch [25/80] time 0.413 (0.480) data 0.283 (0.349) loss_u loss_u 0.9688 (0.9173) acc_u 0.0000 (10.6250) lr 1.8980e-03 eta 0:00:26
epoch [31/200] batch [30/80] time 0.390 (0.476) data 0.260 (0.345) loss_u loss_u 0.9048 (0.9198) acc_u 12.5000 (10.5208) lr 1.8980e-03 eta 0:00:23
epoch [31/200] batch [35/80] time 0.485 (0.477) data 0.354 (0.346) loss_u loss_u 0.8955 (0.9191) acc_u 15.6250 (10.5357) lr 1.8980e-03 eta 0:00:21
epoch [31/200] batch [40/80] time 0.352 (0.475) data 0.221 (0.344) loss_u loss_u 0.9331 (0.9212) acc_u 9.3750 (10.3125) lr 1.8980e-03 eta 0:00:19
epoch [31/200] batch [45/80] time 0.453 (0.478) data 0.321 (0.347) loss_u loss_u 0.9521 (0.9200) acc_u 6.2500 (10.2778) lr 1.8980e-03 eta 0:00:16
epoch [31/200] batch [50/80] time 0.499 (0.477) data 0.368 (0.345) loss_u loss_u 0.9326 (0.9197) acc_u 9.3750 (10.4375) lr 1.8980e-03 eta 0:00:14
epoch [31/200] batch [55/80] time 0.502 (0.475) data 0.372 (0.344) loss_u loss_u 0.8726 (0.9205) acc_u 18.7500 (10.3977) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [60/80] time 0.657 (0.477) data 0.526 (0.346) loss_u loss_u 0.8979 (0.9210) acc_u 12.5000 (10.2604) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [65/80] time 0.474 (0.474) data 0.341 (0.343) loss_u loss_u 0.9492 (0.9218) acc_u 3.1250 (10.0962) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [70/80] time 0.411 (0.471) data 0.280 (0.340) loss_u loss_u 0.8921 (0.9220) acc_u 12.5000 (10.0893) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [75/80] time 0.510 (0.478) data 0.378 (0.347) loss_u loss_u 0.9473 (0.9210) acc_u 3.1250 (10.2083) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [80/80] time 0.345 (0.476) data 0.215 (0.344) loss_u loss_u 0.9307 (0.9216) acc_u 6.2500 (10.1172) lr 1.8980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2032
confident_label rate tensor(0.1716, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 538
clean true:317
clean false:221
clean_rate:0.5892193308550185
noisy true:787
noisy false:1811
after delete: len(clean_dataset) 538
after delete: len(noisy_dataset) 2598
epoch [32/200] batch [5/16] time 0.392 (0.409) data 0.262 (0.279) loss_x loss_x 1.8477 (1.6805) acc_x 46.8750 (53.1250) lr 1.8910e-03 eta 0:00:04
epoch [32/200] batch [10/16] time 0.416 (0.430) data 0.285 (0.299) loss_x loss_x 1.8789 (1.5336) acc_x 46.8750 (59.0625) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [15/16] time 0.538 (0.456) data 0.407 (0.325) loss_x loss_x 1.4473 (1.5013) acc_x 59.3750 (59.3750) lr 1.8910e-03 eta 0:00:00
epoch [32/200] batch [5/81] time 0.485 (0.459) data 0.354 (0.328) loss_u loss_u 0.9604 (0.9357) acc_u 6.2500 (10.0000) lr 1.8910e-03 eta 0:00:34
epoch [32/200] batch [10/81] time 0.392 (0.458) data 0.259 (0.327) loss_u loss_u 0.8286 (0.9167) acc_u 21.8750 (12.8125) lr 1.8910e-03 eta 0:00:32
epoch [32/200] batch [15/81] time 0.596 (0.468) data 0.465 (0.337) loss_u loss_u 0.8438 (0.9119) acc_u 21.8750 (12.9167) lr 1.8910e-03 eta 0:00:30
epoch [32/200] batch [20/81] time 0.418 (0.461) data 0.286 (0.330) loss_u loss_u 0.8315 (0.9060) acc_u 25.0000 (13.1250) lr 1.8910e-03 eta 0:00:28
epoch [32/200] batch [25/81] time 0.425 (0.466) data 0.295 (0.335) loss_u loss_u 0.9385 (0.9080) acc_u 6.2500 (12.5000) lr 1.8910e-03 eta 0:00:26
epoch [32/200] batch [30/81] time 0.520 (0.471) data 0.389 (0.340) loss_u loss_u 0.9390 (0.9074) acc_u 3.1250 (12.3958) lr 1.8910e-03 eta 0:00:24
epoch [32/200] batch [35/81] time 0.441 (0.465) data 0.310 (0.334) loss_u loss_u 0.9395 (0.9116) acc_u 9.3750 (11.8750) lr 1.8910e-03 eta 0:00:21
epoch [32/200] batch [40/81] time 0.403 (0.463) data 0.272 (0.332) loss_u loss_u 0.8940 (0.9121) acc_u 12.5000 (11.6406) lr 1.8910e-03 eta 0:00:18
epoch [32/200] batch [45/81] time 0.363 (0.462) data 0.232 (0.331) loss_u loss_u 0.9102 (0.9106) acc_u 12.5000 (11.8056) lr 1.8910e-03 eta 0:00:16
epoch [32/200] batch [50/81] time 0.541 (0.462) data 0.411 (0.331) loss_u loss_u 0.9023 (0.9122) acc_u 9.3750 (11.3750) lr 1.8910e-03 eta 0:00:14
epoch [32/200] batch [55/81] time 0.412 (0.460) data 0.281 (0.329) loss_u loss_u 0.9546 (0.9128) acc_u 6.2500 (11.3636) lr 1.8910e-03 eta 0:00:11
epoch [32/200] batch [60/81] time 0.435 (0.460) data 0.304 (0.329) loss_u loss_u 0.9248 (0.9115) acc_u 6.2500 (11.4583) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [65/81] time 0.447 (0.459) data 0.315 (0.328) loss_u loss_u 0.8047 (0.9088) acc_u 21.8750 (11.9231) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [70/81] time 0.405 (0.457) data 0.273 (0.326) loss_u loss_u 0.9038 (0.9092) acc_u 15.6250 (11.7857) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [75/81] time 0.685 (0.457) data 0.553 (0.326) loss_u loss_u 0.9399 (0.9102) acc_u 9.3750 (11.7500) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [80/81] time 0.380 (0.454) data 0.249 (0.323) loss_u loss_u 0.9263 (0.9103) acc_u 6.2500 (11.6797) lr 1.8910e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2026
confident_label rate tensor(0.1916, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 601
clean true:343
clean false:258
clean_rate:0.5707154742096506
noisy true:767
noisy false:1768
after delete: len(clean_dataset) 601
after delete: len(noisy_dataset) 2535
epoch [33/200] batch [5/18] time 0.432 (0.461) data 0.302 (0.331) loss_x loss_x 1.5146 (1.6281) acc_x 56.2500 (59.3750) lr 1.8838e-03 eta 0:00:05
epoch [33/200] batch [10/18] time 0.542 (0.492) data 0.411 (0.362) loss_x loss_x 1.7490 (1.5538) acc_x 46.8750 (60.3125) lr 1.8838e-03 eta 0:00:03
epoch [33/200] batch [15/18] time 0.345 (0.490) data 0.214 (0.359) loss_x loss_x 1.4287 (1.5799) acc_x 62.5000 (60.4167) lr 1.8838e-03 eta 0:00:01
epoch [33/200] batch [5/79] time 0.548 (0.493) data 0.417 (0.362) loss_u loss_u 0.9043 (0.9174) acc_u 12.5000 (8.1250) lr 1.8838e-03 eta 0:00:36
epoch [33/200] batch [10/79] time 0.431 (0.495) data 0.299 (0.364) loss_u loss_u 0.8887 (0.9108) acc_u 12.5000 (9.3750) lr 1.8838e-03 eta 0:00:34
epoch [33/200] batch [15/79] time 0.423 (0.486) data 0.292 (0.356) loss_u loss_u 0.8574 (0.9144) acc_u 21.8750 (9.7917) lr 1.8838e-03 eta 0:00:31
epoch [33/200] batch [20/79] time 0.738 (0.492) data 0.606 (0.361) loss_u loss_u 0.8628 (0.9115) acc_u 18.7500 (10.3125) lr 1.8838e-03 eta 0:00:29
epoch [33/200] batch [25/79] time 0.591 (0.497) data 0.459 (0.366) loss_u loss_u 0.9170 (0.9128) acc_u 9.3750 (10.1250) lr 1.8838e-03 eta 0:00:26
epoch [33/200] batch [30/79] time 0.524 (0.495) data 0.389 (0.364) loss_u loss_u 0.9570 (0.9132) acc_u 0.0000 (10.1042) lr 1.8838e-03 eta 0:00:24
epoch [33/200] batch [35/79] time 0.641 (0.491) data 0.507 (0.360) loss_u loss_u 0.9473 (0.9139) acc_u 3.1250 (10.1786) lr 1.8838e-03 eta 0:00:21
epoch [33/200] batch [40/79] time 0.431 (0.486) data 0.297 (0.355) loss_u loss_u 0.9385 (0.9122) acc_u 6.2500 (10.5469) lr 1.8838e-03 eta 0:00:18
epoch [33/200] batch [45/79] time 0.558 (0.486) data 0.424 (0.354) loss_u loss_u 0.9243 (0.9143) acc_u 6.2500 (10.4167) lr 1.8838e-03 eta 0:00:16
epoch [33/200] batch [50/79] time 0.474 (0.483) data 0.342 (0.351) loss_u loss_u 0.8555 (0.9133) acc_u 21.8750 (10.6875) lr 1.8838e-03 eta 0:00:14
epoch [33/200] batch [55/79] time 0.552 (0.478) data 0.419 (0.346) loss_u loss_u 0.9233 (0.9156) acc_u 9.3750 (10.4545) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [60/79] time 0.441 (0.474) data 0.310 (0.342) loss_u loss_u 0.8779 (0.9161) acc_u 21.8750 (10.4688) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [65/79] time 0.483 (0.474) data 0.350 (0.342) loss_u loss_u 0.9209 (0.9168) acc_u 9.3750 (10.2885) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [70/79] time 0.423 (0.471) data 0.291 (0.339) loss_u loss_u 0.9141 (0.9180) acc_u 9.3750 (10.1339) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [75/79] time 0.336 (0.466) data 0.205 (0.334) loss_u loss_u 0.9092 (0.9166) acc_u 9.3750 (10.4167) lr 1.8838e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2050
confident_label rate tensor(0.1875, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 588
clean true:329
clean false:259
clean_rate:0.5595238095238095
noisy true:757
noisy false:1791
after delete: len(clean_dataset) 588
after delete: len(noisy_dataset) 2548
epoch [34/200] batch [5/18] time 0.583 (0.524) data 0.452 (0.393) loss_x loss_x 1.6240 (1.2979) acc_x 56.2500 (67.5000) lr 1.8763e-03 eta 0:00:06
epoch [34/200] batch [10/18] time 0.371 (0.513) data 0.241 (0.382) loss_x loss_x 1.2246 (1.4419) acc_x 71.8750 (64.3750) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [15/18] time 0.391 (0.481) data 0.261 (0.350) loss_x loss_x 1.6846 (1.5554) acc_x 68.7500 (62.2917) lr 1.8763e-03 eta 0:00:01
epoch [34/200] batch [5/79] time 0.528 (0.488) data 0.397 (0.357) loss_u loss_u 0.9316 (0.9393) acc_u 6.2500 (6.2500) lr 1.8763e-03 eta 0:00:36
epoch [34/200] batch [10/79] time 0.469 (0.477) data 0.337 (0.346) loss_u loss_u 0.9312 (0.9460) acc_u 9.3750 (6.2500) lr 1.8763e-03 eta 0:00:32
epoch [34/200] batch [15/79] time 0.379 (0.469) data 0.247 (0.338) loss_u loss_u 0.8550 (0.9354) acc_u 18.7500 (7.9167) lr 1.8763e-03 eta 0:00:30
epoch [34/200] batch [20/79] time 0.388 (0.460) data 0.256 (0.329) loss_u loss_u 0.9307 (0.9300) acc_u 12.5000 (8.7500) lr 1.8763e-03 eta 0:00:27
epoch [34/200] batch [25/79] time 0.395 (0.463) data 0.263 (0.332) loss_u loss_u 0.9453 (0.9273) acc_u 6.2500 (9.3750) lr 1.8763e-03 eta 0:00:25
epoch [34/200] batch [30/79] time 0.392 (0.460) data 0.261 (0.329) loss_u loss_u 0.9468 (0.9259) acc_u 9.3750 (9.4792) lr 1.8763e-03 eta 0:00:22
epoch [34/200] batch [35/79] time 0.395 (0.453) data 0.264 (0.322) loss_u loss_u 0.8994 (0.9240) acc_u 15.6250 (9.8214) lr 1.8763e-03 eta 0:00:19
epoch [34/200] batch [40/79] time 0.437 (0.452) data 0.305 (0.321) loss_u loss_u 0.8828 (0.9256) acc_u 9.3750 (9.2969) lr 1.8763e-03 eta 0:00:17
epoch [34/200] batch [45/79] time 0.495 (0.457) data 0.365 (0.326) loss_u loss_u 0.9321 (0.9239) acc_u 9.3750 (9.6528) lr 1.8763e-03 eta 0:00:15
epoch [34/200] batch [50/79] time 0.393 (0.458) data 0.263 (0.327) loss_u loss_u 0.9458 (0.9231) acc_u 6.2500 (9.5625) lr 1.8763e-03 eta 0:00:13
epoch [34/200] batch [55/79] time 0.700 (0.459) data 0.569 (0.328) loss_u loss_u 0.9473 (0.9249) acc_u 6.2500 (9.4886) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [60/79] time 0.486 (0.460) data 0.354 (0.329) loss_u loss_u 0.9634 (0.9269) acc_u 0.0000 (9.1667) lr 1.8763e-03 eta 0:00:08
epoch [34/200] batch [65/79] time 0.571 (0.465) data 0.440 (0.334) loss_u loss_u 0.9214 (0.9270) acc_u 9.3750 (9.0865) lr 1.8763e-03 eta 0:00:06
epoch [34/200] batch [70/79] time 0.453 (0.462) data 0.321 (0.331) loss_u loss_u 0.7681 (0.9241) acc_u 31.2500 (9.5536) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [75/79] time 0.392 (0.459) data 0.261 (0.328) loss_u loss_u 0.9429 (0.9230) acc_u 3.1250 (9.7500) lr 1.8763e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2014
confident_label rate tensor(0.1837, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 576
clean true:335
clean false:241
clean_rate:0.5815972222222222
noisy true:787
noisy false:1773
after delete: len(clean_dataset) 576
after delete: len(noisy_dataset) 2560
epoch [35/200] batch [5/18] time 0.493 (0.447) data 0.362 (0.317) loss_x loss_x 1.2773 (1.3551) acc_x 59.3750 (65.0000) lr 1.8686e-03 eta 0:00:05
epoch [35/200] batch [10/18] time 0.533 (0.457) data 0.403 (0.327) loss_x loss_x 0.9844 (1.3067) acc_x 78.1250 (67.1875) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [15/18] time 0.340 (0.441) data 0.209 (0.310) loss_x loss_x 1.8887 (1.3988) acc_x 46.8750 (63.3333) lr 1.8686e-03 eta 0:00:01
epoch [35/200] batch [5/80] time 0.513 (0.456) data 0.381 (0.325) loss_u loss_u 0.8857 (0.9189) acc_u 18.7500 (11.2500) lr 1.8686e-03 eta 0:00:34
epoch [35/200] batch [10/80] time 0.332 (0.450) data 0.202 (0.319) loss_u loss_u 0.9209 (0.9289) acc_u 6.2500 (10.3125) lr 1.8686e-03 eta 0:00:31
epoch [35/200] batch [15/80] time 0.497 (0.450) data 0.365 (0.320) loss_u loss_u 0.8979 (0.9350) acc_u 15.6250 (9.1667) lr 1.8686e-03 eta 0:00:29
epoch [35/200] batch [20/80] time 0.416 (0.449) data 0.283 (0.318) loss_u loss_u 0.8628 (0.9269) acc_u 18.7500 (10.1562) lr 1.8686e-03 eta 0:00:26
epoch [35/200] batch [25/80] time 0.576 (0.461) data 0.445 (0.330) loss_u loss_u 0.9150 (0.9252) acc_u 12.5000 (10.5000) lr 1.8686e-03 eta 0:00:25
epoch [35/200] batch [30/80] time 0.512 (0.462) data 0.381 (0.331) loss_u loss_u 0.9165 (0.9235) acc_u 6.2500 (10.4167) lr 1.8686e-03 eta 0:00:23
epoch [35/200] batch [35/80] time 0.560 (0.463) data 0.429 (0.332) loss_u loss_u 0.9561 (0.9256) acc_u 6.2500 (10.0000) lr 1.8686e-03 eta 0:00:20
epoch [35/200] batch [40/80] time 0.478 (0.463) data 0.345 (0.331) loss_u loss_u 0.9136 (0.9274) acc_u 6.2500 (9.4531) lr 1.8686e-03 eta 0:00:18
epoch [35/200] batch [45/80] time 0.472 (0.469) data 0.341 (0.338) loss_u loss_u 0.9370 (0.9258) acc_u 9.3750 (9.5833) lr 1.8686e-03 eta 0:00:16
epoch [35/200] batch [50/80] time 0.381 (0.473) data 0.249 (0.342) loss_u loss_u 0.8989 (0.9249) acc_u 15.6250 (9.8125) lr 1.8686e-03 eta 0:00:14
epoch [35/200] batch [55/80] time 0.552 (0.473) data 0.421 (0.342) loss_u loss_u 0.8774 (0.9234) acc_u 18.7500 (10.0000) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [60/80] time 0.576 (0.474) data 0.445 (0.343) loss_u loss_u 0.9736 (0.9230) acc_u 3.1250 (10.2083) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [65/80] time 0.430 (0.471) data 0.298 (0.340) loss_u loss_u 0.9146 (0.9218) acc_u 12.5000 (10.3365) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [70/80] time 0.458 (0.468) data 0.326 (0.337) loss_u loss_u 0.8979 (0.9220) acc_u 9.3750 (10.2232) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [75/80] time 0.468 (0.467) data 0.338 (0.336) loss_u loss_u 0.9282 (0.9219) acc_u 6.2500 (10.1250) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [80/80] time 0.446 (0.465) data 0.315 (0.334) loss_u loss_u 0.8911 (0.9208) acc_u 12.5000 (10.2734) lr 1.8686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2047
confident_label rate tensor(0.1913, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 600
clean true:345
clean false:255
clean_rate:0.575
noisy true:744
noisy false:1792
after delete: len(clean_dataset) 600
after delete: len(noisy_dataset) 2536
epoch [36/200] batch [5/18] time 0.465 (0.484) data 0.335 (0.354) loss_x loss_x 1.3604 (1.2760) acc_x 62.5000 (64.3750) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [10/18] time 0.469 (0.463) data 0.339 (0.332) loss_x loss_x 1.5908 (1.3946) acc_x 62.5000 (60.9375) lr 1.8607e-03 eta 0:00:03
epoch [36/200] batch [15/18] time 0.457 (0.455) data 0.327 (0.324) loss_x loss_x 1.6289 (1.4328) acc_x 65.6250 (61.8750) lr 1.8607e-03 eta 0:00:01
epoch [36/200] batch [5/79] time 0.649 (0.461) data 0.518 (0.331) loss_u loss_u 0.9399 (0.9409) acc_u 6.2500 (8.1250) lr 1.8607e-03 eta 0:00:34
epoch [36/200] batch [10/79] time 0.387 (0.448) data 0.254 (0.317) loss_u loss_u 0.8853 (0.9138) acc_u 9.3750 (10.9375) lr 1.8607e-03 eta 0:00:30
epoch [36/200] batch [15/79] time 0.345 (0.449) data 0.213 (0.318) loss_u loss_u 0.9561 (0.9173) acc_u 9.3750 (10.2083) lr 1.8607e-03 eta 0:00:28
epoch [36/200] batch [20/79] time 0.419 (0.450) data 0.288 (0.319) loss_u loss_u 0.8975 (0.9212) acc_u 15.6250 (10.0000) lr 1.8607e-03 eta 0:00:26
epoch [36/200] batch [25/79] time 0.412 (0.453) data 0.279 (0.322) loss_u loss_u 0.9224 (0.9219) acc_u 9.3750 (9.8750) lr 1.8607e-03 eta 0:00:24
epoch [36/200] batch [30/79] time 0.437 (0.453) data 0.306 (0.322) loss_u loss_u 0.8784 (0.9206) acc_u 15.6250 (10.0000) lr 1.8607e-03 eta 0:00:22
epoch [36/200] batch [35/79] time 0.454 (0.456) data 0.323 (0.325) loss_u loss_u 0.9531 (0.9184) acc_u 6.2500 (10.4464) lr 1.8607e-03 eta 0:00:20
epoch [36/200] batch [40/79] time 0.589 (0.457) data 0.457 (0.325) loss_u loss_u 0.9580 (0.9168) acc_u 9.3750 (10.8594) lr 1.8607e-03 eta 0:00:17
epoch [36/200] batch [45/79] time 0.443 (0.458) data 0.312 (0.327) loss_u loss_u 0.9609 (0.9191) acc_u 9.3750 (10.4861) lr 1.8607e-03 eta 0:00:15
epoch [36/200] batch [50/79] time 0.576 (0.463) data 0.445 (0.332) loss_u loss_u 0.9355 (0.9181) acc_u 9.3750 (10.5625) lr 1.8607e-03 eta 0:00:13
epoch [36/200] batch [55/79] time 0.543 (0.465) data 0.412 (0.334) loss_u loss_u 0.9502 (0.9187) acc_u 6.2500 (10.5114) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [60/79] time 0.880 (0.467) data 0.748 (0.336) loss_u loss_u 0.8789 (0.9180) acc_u 15.6250 (10.7292) lr 1.8607e-03 eta 0:00:08
epoch [36/200] batch [65/79] time 0.483 (0.468) data 0.352 (0.337) loss_u loss_u 0.8989 (0.9157) acc_u 12.5000 (11.1538) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [70/79] time 0.604 (0.467) data 0.473 (0.335) loss_u loss_u 0.9160 (0.9155) acc_u 12.5000 (11.2500) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [75/79] time 0.431 (0.468) data 0.299 (0.337) loss_u loss_u 0.9507 (0.9170) acc_u 6.2500 (11.0000) lr 1.8607e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2048
confident_label rate tensor(0.1983, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 622
clean true:345
clean false:277
clean_rate:0.5546623794212219
noisy true:743
noisy false:1771
after delete: len(clean_dataset) 622
after delete: len(noisy_dataset) 2514
epoch [37/200] batch [5/19] time 0.427 (0.466) data 0.295 (0.335) loss_x loss_x 1.2988 (1.5195) acc_x 68.7500 (61.8750) lr 1.8526e-03 eta 0:00:06
epoch [37/200] batch [10/19] time 0.496 (0.452) data 0.366 (0.322) loss_x loss_x 1.4980 (1.4944) acc_x 59.3750 (62.8125) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [15/19] time 0.476 (0.447) data 0.346 (0.316) loss_x loss_x 2.0820 (1.5024) acc_x 56.2500 (61.8750) lr 1.8526e-03 eta 0:00:01
epoch [37/200] batch [5/78] time 0.588 (0.480) data 0.456 (0.349) loss_u loss_u 0.9077 (0.9267) acc_u 9.3750 (8.7500) lr 1.8526e-03 eta 0:00:35
epoch [37/200] batch [10/78] time 0.413 (0.460) data 0.283 (0.329) loss_u loss_u 0.9434 (0.9135) acc_u 6.2500 (10.9375) lr 1.8526e-03 eta 0:00:31
epoch [37/200] batch [15/78] time 0.431 (0.466) data 0.299 (0.335) loss_u loss_u 0.9463 (0.9139) acc_u 6.2500 (10.6250) lr 1.8526e-03 eta 0:00:29
epoch [37/200] batch [20/78] time 0.341 (0.459) data 0.209 (0.327) loss_u loss_u 0.9282 (0.9189) acc_u 6.2500 (9.6875) lr 1.8526e-03 eta 0:00:26
epoch [37/200] batch [25/78] time 0.552 (0.464) data 0.421 (0.333) loss_u loss_u 0.9121 (0.9189) acc_u 9.3750 (9.7500) lr 1.8526e-03 eta 0:00:24
epoch [37/200] batch [30/78] time 0.371 (0.461) data 0.239 (0.330) loss_u loss_u 0.9282 (0.9195) acc_u 9.3750 (9.7917) lr 1.8526e-03 eta 0:00:22
epoch [37/200] batch [35/78] time 0.475 (0.465) data 0.345 (0.334) loss_u loss_u 0.9233 (0.9209) acc_u 6.2500 (9.6429) lr 1.8526e-03 eta 0:00:19
epoch [37/200] batch [40/78] time 0.380 (0.466) data 0.250 (0.335) loss_u loss_u 0.9683 (0.9222) acc_u 3.1250 (9.5312) lr 1.8526e-03 eta 0:00:17
epoch [37/200] batch [45/78] time 0.390 (0.462) data 0.258 (0.331) loss_u loss_u 0.9351 (0.9217) acc_u 6.2500 (9.6528) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [50/78] time 0.517 (0.463) data 0.385 (0.332) loss_u loss_u 0.9717 (0.9237) acc_u 3.1250 (9.3750) lr 1.8526e-03 eta 0:00:12
epoch [37/200] batch [55/78] time 0.398 (0.465) data 0.265 (0.334) loss_u loss_u 0.9712 (0.9253) acc_u 9.3750 (9.1477) lr 1.8526e-03 eta 0:00:10
epoch [37/200] batch [60/78] time 0.380 (0.460) data 0.248 (0.329) loss_u loss_u 0.9434 (0.9261) acc_u 6.2500 (8.9583) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [65/78] time 0.404 (0.459) data 0.273 (0.328) loss_u loss_u 0.9272 (0.9233) acc_u 6.2500 (9.6635) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [70/78] time 0.496 (0.462) data 0.365 (0.331) loss_u loss_u 0.9761 (0.9241) acc_u 6.2500 (9.6875) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [75/78] time 0.417 (0.463) data 0.285 (0.331) loss_u loss_u 0.9443 (0.9237) acc_u 6.2500 (9.6250) lr 1.8526e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2084
confident_label rate tensor(0.1872, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 587
clean true:325
clean false:262
clean_rate:0.5536626916524702
noisy true:727
noisy false:1822
after delete: len(clean_dataset) 587
after delete: len(noisy_dataset) 2549
epoch [38/200] batch [5/18] time 0.531 (0.459) data 0.401 (0.329) loss_x loss_x 1.4951 (1.6379) acc_x 50.0000 (55.0000) lr 1.8443e-03 eta 0:00:05
epoch [38/200] batch [10/18] time 0.442 (0.449) data 0.311 (0.319) loss_x loss_x 1.7441 (1.5639) acc_x 53.1250 (58.4375) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [15/18] time 0.426 (0.445) data 0.295 (0.315) loss_x loss_x 1.3496 (1.5186) acc_x 68.7500 (60.2083) lr 1.8443e-03 eta 0:00:01
epoch [38/200] batch [5/79] time 0.498 (0.440) data 0.367 (0.309) loss_u loss_u 0.9224 (0.9196) acc_u 6.2500 (10.6250) lr 1.8443e-03 eta 0:00:32
epoch [38/200] batch [10/79] time 0.610 (0.440) data 0.479 (0.310) loss_u loss_u 0.9180 (0.9156) acc_u 15.6250 (10.9375) lr 1.8443e-03 eta 0:00:30
epoch [38/200] batch [15/79] time 0.464 (0.442) data 0.332 (0.311) loss_u loss_u 0.8472 (0.9157) acc_u 15.6250 (10.4167) lr 1.8443e-03 eta 0:00:28
epoch [38/200] batch [20/79] time 0.511 (0.441) data 0.380 (0.310) loss_u loss_u 0.9575 (0.9196) acc_u 3.1250 (9.6875) lr 1.8443e-03 eta 0:00:26
epoch [38/200] batch [25/79] time 0.442 (0.446) data 0.312 (0.315) loss_u loss_u 0.9160 (0.9206) acc_u 6.2500 (9.6250) lr 1.8443e-03 eta 0:00:24
epoch [38/200] batch [30/79] time 0.540 (0.451) data 0.408 (0.320) loss_u loss_u 0.9551 (0.9207) acc_u 6.2500 (9.8958) lr 1.8443e-03 eta 0:00:22
epoch [38/200] batch [35/79] time 0.424 (0.456) data 0.292 (0.325) loss_u loss_u 0.8984 (0.9240) acc_u 12.5000 (9.5536) lr 1.8443e-03 eta 0:00:20
epoch [38/200] batch [40/79] time 0.407 (0.454) data 0.276 (0.323) loss_u loss_u 0.9248 (0.9245) acc_u 18.7500 (9.7656) lr 1.8443e-03 eta 0:00:17
epoch [38/200] batch [45/79] time 0.435 (0.458) data 0.303 (0.327) loss_u loss_u 0.8506 (0.9232) acc_u 18.7500 (9.7222) lr 1.8443e-03 eta 0:00:15
epoch [38/200] batch [50/79] time 0.351 (0.457) data 0.219 (0.325) loss_u loss_u 0.9404 (0.9239) acc_u 9.3750 (9.8125) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [55/79] time 0.557 (0.456) data 0.425 (0.325) loss_u loss_u 0.9512 (0.9236) acc_u 3.1250 (9.9432) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [60/79] time 0.521 (0.460) data 0.389 (0.329) loss_u loss_u 0.8862 (0.9224) acc_u 9.3750 (10.0521) lr 1.8443e-03 eta 0:00:08
epoch [38/200] batch [65/79] time 0.389 (0.460) data 0.257 (0.329) loss_u loss_u 0.9473 (0.9229) acc_u 9.3750 (10.0962) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [70/79] time 0.432 (0.463) data 0.302 (0.332) loss_u loss_u 0.9194 (0.9218) acc_u 9.3750 (10.3125) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [75/79] time 0.521 (0.465) data 0.389 (0.333) loss_u loss_u 0.9380 (0.9212) acc_u 3.1250 (10.2500) lr 1.8443e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2036
confident_label rate tensor(0.1897, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 595
clean true:336
clean false:259
clean_rate:0.5647058823529412
noisy true:764
noisy false:1777
after delete: len(clean_dataset) 595
after delete: len(noisy_dataset) 2541
epoch [39/200] batch [5/18] time 0.493 (0.520) data 0.362 (0.389) loss_x loss_x 2.4453 (1.7902) acc_x 43.7500 (58.7500) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [10/18] time 0.395 (0.506) data 0.265 (0.375) loss_x loss_x 1.1328 (1.5722) acc_x 59.3750 (61.8750) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [15/18] time 0.504 (0.489) data 0.374 (0.359) loss_x loss_x 1.4971 (1.5183) acc_x 62.5000 (60.8333) lr 1.8358e-03 eta 0:00:01
epoch [39/200] batch [5/79] time 0.664 (0.476) data 0.532 (0.345) loss_u loss_u 0.8584 (0.9016) acc_u 18.7500 (13.1250) lr 1.8358e-03 eta 0:00:35
epoch [39/200] batch [10/79] time 0.427 (0.470) data 0.296 (0.339) loss_u loss_u 0.9219 (0.9208) acc_u 12.5000 (10.6250) lr 1.8358e-03 eta 0:00:32
epoch [39/200] batch [15/79] time 0.401 (0.466) data 0.269 (0.334) loss_u loss_u 0.9077 (0.9191) acc_u 12.5000 (11.0417) lr 1.8358e-03 eta 0:00:29
epoch [39/200] batch [20/79] time 0.553 (0.470) data 0.422 (0.339) loss_u loss_u 0.8535 (0.9214) acc_u 25.0000 (11.0938) lr 1.8358e-03 eta 0:00:27
epoch [39/200] batch [25/79] time 0.500 (0.478) data 0.368 (0.347) loss_u loss_u 0.8276 (0.9165) acc_u 21.8750 (11.6250) lr 1.8358e-03 eta 0:00:25
epoch [39/200] batch [30/79] time 0.420 (0.478) data 0.289 (0.347) loss_u loss_u 0.9438 (0.9156) acc_u 9.3750 (11.5625) lr 1.8358e-03 eta 0:00:23
epoch [39/200] batch [35/79] time 0.424 (0.478) data 0.293 (0.347) loss_u loss_u 0.9014 (0.9159) acc_u 15.6250 (11.5179) lr 1.8358e-03 eta 0:00:21
epoch [39/200] batch [40/79] time 0.449 (0.480) data 0.318 (0.349) loss_u loss_u 0.9209 (0.9173) acc_u 12.5000 (10.9375) lr 1.8358e-03 eta 0:00:18
epoch [39/200] batch [45/79] time 0.397 (0.480) data 0.266 (0.349) loss_u loss_u 0.9297 (0.9191) acc_u 9.3750 (10.4861) lr 1.8358e-03 eta 0:00:16
epoch [39/200] batch [50/79] time 0.513 (0.481) data 0.381 (0.349) loss_u loss_u 0.9321 (0.9188) acc_u 6.2500 (10.5000) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [55/79] time 0.399 (0.478) data 0.268 (0.347) loss_u loss_u 0.9067 (0.9195) acc_u 15.6250 (10.5114) lr 1.8358e-03 eta 0:00:11
epoch [39/200] batch [60/79] time 0.422 (0.476) data 0.290 (0.345) loss_u loss_u 0.9434 (0.9200) acc_u 6.2500 (10.3646) lr 1.8358e-03 eta 0:00:09
epoch [39/200] batch [65/79] time 0.356 (0.474) data 0.225 (0.342) loss_u loss_u 0.8857 (0.9209) acc_u 12.5000 (10.1923) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [70/79] time 0.560 (0.474) data 0.428 (0.342) loss_u loss_u 0.9287 (0.9209) acc_u 9.3750 (10.0000) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [75/79] time 0.405 (0.471) data 0.275 (0.339) loss_u loss_u 0.8838 (0.9199) acc_u 12.5000 (10.0417) lr 1.8358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2028
confident_label rate tensor(0.1939, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 608
clean true:339
clean false:269
clean_rate:0.5575657894736842
noisy true:769
noisy false:1759
after delete: len(clean_dataset) 608
after delete: len(noisy_dataset) 2528
epoch [40/200] batch [5/19] time 0.569 (0.486) data 0.438 (0.356) loss_x loss_x 1.3828 (1.3543) acc_x 68.7500 (63.1250) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [10/19] time 0.426 (0.503) data 0.296 (0.372) loss_x loss_x 1.5059 (1.4667) acc_x 59.3750 (63.4375) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [15/19] time 0.383 (0.471) data 0.253 (0.340) loss_x loss_x 1.2119 (1.4617) acc_x 71.8750 (63.9583) lr 1.8271e-03 eta 0:00:01
epoch [40/200] batch [5/79] time 0.520 (0.462) data 0.389 (0.331) loss_u loss_u 0.9102 (0.9104) acc_u 12.5000 (12.5000) lr 1.8271e-03 eta 0:00:34
epoch [40/200] batch [10/79] time 0.391 (0.455) data 0.259 (0.324) loss_u loss_u 0.9229 (0.9177) acc_u 12.5000 (11.5625) lr 1.8271e-03 eta 0:00:31
epoch [40/200] batch [15/79] time 0.412 (0.459) data 0.282 (0.328) loss_u loss_u 0.9346 (0.9234) acc_u 9.3750 (10.6250) lr 1.8271e-03 eta 0:00:29
epoch [40/200] batch [20/79] time 0.404 (0.455) data 0.270 (0.323) loss_u loss_u 0.9165 (0.9245) acc_u 12.5000 (10.3125) lr 1.8271e-03 eta 0:00:26
epoch [40/200] batch [25/79] time 0.495 (0.457) data 0.364 (0.325) loss_u loss_u 0.8877 (0.9216) acc_u 15.6250 (10.2500) lr 1.8271e-03 eta 0:00:24
epoch [40/200] batch [30/79] time 0.682 (0.463) data 0.550 (0.331) loss_u loss_u 0.9131 (0.9230) acc_u 15.6250 (10.1042) lr 1.8271e-03 eta 0:00:22
epoch [40/200] batch [35/79] time 0.470 (0.465) data 0.338 (0.334) loss_u loss_u 0.8921 (0.9229) acc_u 12.5000 (10.0893) lr 1.8271e-03 eta 0:00:20
epoch [40/200] batch [40/79] time 0.416 (0.462) data 0.284 (0.331) loss_u loss_u 0.8862 (0.9234) acc_u 18.7500 (10.0781) lr 1.8271e-03 eta 0:00:18
epoch [40/200] batch [45/79] time 0.369 (0.465) data 0.238 (0.334) loss_u loss_u 0.9131 (0.9218) acc_u 9.3750 (10.3472) lr 1.8271e-03 eta 0:00:15
epoch [40/200] batch [50/79] time 0.404 (0.463) data 0.272 (0.332) loss_u loss_u 0.9590 (0.9219) acc_u 3.1250 (10.1250) lr 1.8271e-03 eta 0:00:13
epoch [40/200] batch [55/79] time 0.517 (0.461) data 0.385 (0.330) loss_u loss_u 0.9209 (0.9217) acc_u 6.2500 (10.0568) lr 1.8271e-03 eta 0:00:11
epoch [40/200] batch [60/79] time 0.491 (0.463) data 0.360 (0.332) loss_u loss_u 0.9390 (0.9191) acc_u 12.5000 (10.4167) lr 1.8271e-03 eta 0:00:08
epoch [40/200] batch [65/79] time 0.496 (0.459) data 0.364 (0.328) loss_u loss_u 0.9658 (0.9196) acc_u 3.1250 (10.3846) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [70/79] time 0.376 (0.464) data 0.245 (0.333) loss_u loss_u 0.9253 (0.9198) acc_u 15.6250 (10.5357) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [75/79] time 0.392 (0.463) data 0.261 (0.331) loss_u loss_u 0.9297 (0.9187) acc_u 6.2500 (10.6250) lr 1.8271e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2052
confident_label rate tensor(0.1952, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 612
clean true:328
clean false:284
clean_rate:0.5359477124183006
noisy true:756
noisy false:1768
after delete: len(clean_dataset) 612
after delete: len(noisy_dataset) 2524
epoch [41/200] batch [5/19] time 0.456 (0.502) data 0.326 (0.371) loss_x loss_x 1.4385 (1.6496) acc_x 65.6250 (56.8750) lr 1.8181e-03 eta 0:00:07
epoch [41/200] batch [10/19] time 0.469 (0.477) data 0.339 (0.347) loss_x loss_x 1.6777 (1.6338) acc_x 53.1250 (59.3750) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [15/19] time 0.415 (0.482) data 0.284 (0.351) loss_x loss_x 0.9697 (1.5440) acc_x 68.7500 (60.6250) lr 1.8181e-03 eta 0:00:01
epoch [41/200] batch [5/78] time 0.370 (0.469) data 0.239 (0.339) loss_u loss_u 0.8916 (0.9081) acc_u 15.6250 (11.8750) lr 1.8181e-03 eta 0:00:34
epoch [41/200] batch [10/78] time 0.457 (0.461) data 0.326 (0.330) loss_u loss_u 0.9399 (0.9240) acc_u 9.3750 (9.6875) lr 1.8181e-03 eta 0:00:31
epoch [41/200] batch [15/78] time 0.360 (0.462) data 0.228 (0.331) loss_u loss_u 0.9458 (0.9220) acc_u 6.2500 (8.9583) lr 1.8181e-03 eta 0:00:29
epoch [41/200] batch [20/78] time 0.349 (0.467) data 0.217 (0.336) loss_u loss_u 0.9033 (0.9120) acc_u 12.5000 (10.7812) lr 1.8181e-03 eta 0:00:27
epoch [41/200] batch [25/78] time 0.386 (0.464) data 0.254 (0.333) loss_u loss_u 0.9019 (0.9127) acc_u 12.5000 (10.7500) lr 1.8181e-03 eta 0:00:24
epoch [41/200] batch [30/78] time 0.410 (0.461) data 0.278 (0.330) loss_u loss_u 0.9888 (0.9158) acc_u 0.0000 (10.7292) lr 1.8181e-03 eta 0:00:22
epoch [41/200] batch [35/78] time 0.499 (0.464) data 0.367 (0.333) loss_u loss_u 0.9082 (0.9155) acc_u 12.5000 (10.8929) lr 1.8181e-03 eta 0:00:19
epoch [41/200] batch [40/78] time 0.445 (0.469) data 0.314 (0.338) loss_u loss_u 0.8369 (0.9143) acc_u 18.7500 (11.0938) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [45/78] time 0.468 (0.468) data 0.335 (0.337) loss_u loss_u 0.9556 (0.9167) acc_u 3.1250 (10.6944) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [50/78] time 0.458 (0.470) data 0.327 (0.338) loss_u loss_u 0.8857 (0.9156) acc_u 15.6250 (10.6875) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [55/78] time 0.420 (0.468) data 0.288 (0.336) loss_u loss_u 0.9067 (0.9165) acc_u 9.3750 (10.5114) lr 1.8181e-03 eta 0:00:10
epoch [41/200] batch [60/78] time 0.542 (0.472) data 0.410 (0.341) loss_u loss_u 0.9390 (0.9161) acc_u 3.1250 (10.4688) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [65/78] time 0.461 (0.472) data 0.329 (0.340) loss_u loss_u 0.8916 (0.9145) acc_u 18.7500 (10.8173) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [70/78] time 0.459 (0.471) data 0.328 (0.340) loss_u loss_u 0.8740 (0.9145) acc_u 18.7500 (11.0714) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [75/78] time 0.422 (0.473) data 0.291 (0.342) loss_u loss_u 0.9077 (0.9150) acc_u 9.3750 (11.0417) lr 1.8181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2042
confident_label rate tensor(0.2034, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 638
clean true:352
clean false:286
clean_rate:0.5517241379310345
noisy true:742
noisy false:1756
after delete: len(clean_dataset) 638
after delete: len(noisy_dataset) 2498
epoch [42/200] batch [5/19] time 0.387 (0.449) data 0.256 (0.318) loss_x loss_x 1.2500 (1.5096) acc_x 59.3750 (59.3750) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [10/19] time 0.486 (0.473) data 0.355 (0.342) loss_x loss_x 1.0908 (1.4356) acc_x 68.7500 (62.5000) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [15/19] time 0.440 (0.486) data 0.309 (0.355) loss_x loss_x 1.4600 (1.4601) acc_x 62.5000 (62.2917) lr 1.8090e-03 eta 0:00:01
epoch [42/200] batch [5/78] time 0.513 (0.489) data 0.381 (0.358) loss_u loss_u 0.9585 (0.9077) acc_u 3.1250 (13.1250) lr 1.8090e-03 eta 0:00:35
epoch [42/200] batch [10/78] time 0.483 (0.484) data 0.352 (0.353) loss_u loss_u 0.9243 (0.9121) acc_u 9.3750 (11.5625) lr 1.8090e-03 eta 0:00:32
epoch [42/200] batch [15/78] time 0.397 (0.482) data 0.265 (0.351) loss_u loss_u 0.9414 (0.9190) acc_u 9.3750 (10.2083) lr 1.8090e-03 eta 0:00:30
epoch [42/200] batch [20/78] time 0.378 (0.481) data 0.247 (0.350) loss_u loss_u 0.9419 (0.9249) acc_u 3.1250 (9.0625) lr 1.8090e-03 eta 0:00:27
epoch [42/200] batch [25/78] time 0.350 (0.470) data 0.219 (0.338) loss_u loss_u 0.9282 (0.9216) acc_u 6.2500 (9.6250) lr 1.8090e-03 eta 0:00:24
epoch [42/200] batch [30/78] time 0.471 (0.467) data 0.341 (0.336) loss_u loss_u 0.8823 (0.9189) acc_u 18.7500 (10.1042) lr 1.8090e-03 eta 0:00:22
epoch [42/200] batch [35/78] time 0.465 (0.468) data 0.334 (0.337) loss_u loss_u 0.9185 (0.9187) acc_u 12.5000 (10.1786) lr 1.8090e-03 eta 0:00:20
epoch [42/200] batch [40/78] time 0.519 (0.463) data 0.388 (0.332) loss_u loss_u 0.9297 (0.9176) acc_u 9.3750 (10.1562) lr 1.8090e-03 eta 0:00:17
epoch [42/200] batch [45/78] time 0.389 (0.459) data 0.258 (0.328) loss_u loss_u 0.9546 (0.9180) acc_u 6.2500 (10.0000) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [50/78] time 0.550 (0.460) data 0.418 (0.328) loss_u loss_u 0.9326 (0.9179) acc_u 9.3750 (10.1250) lr 1.8090e-03 eta 0:00:12
epoch [42/200] batch [55/78] time 0.430 (0.464) data 0.298 (0.332) loss_u loss_u 0.8970 (0.9188) acc_u 9.3750 (10.1705) lr 1.8090e-03 eta 0:00:10
epoch [42/200] batch [60/78] time 0.425 (0.463) data 0.294 (0.332) loss_u loss_u 0.9707 (0.9214) acc_u 0.0000 (9.8438) lr 1.8090e-03 eta 0:00:08
epoch [42/200] batch [65/78] time 0.500 (0.463) data 0.368 (0.332) loss_u loss_u 0.9326 (0.9225) acc_u 6.2500 (9.5673) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [70/78] time 0.522 (0.467) data 0.389 (0.335) loss_u loss_u 0.9082 (0.9221) acc_u 15.6250 (9.7321) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [75/78] time 0.569 (0.468) data 0.437 (0.336) loss_u loss_u 0.9194 (0.9228) acc_u 12.5000 (9.6667) lr 1.8090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2057
confident_label rate tensor(0.1840, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 577
clean true:323
clean false:254
clean_rate:0.5597920277296361
noisy true:756
noisy false:1803
after delete: len(clean_dataset) 577
after delete: len(noisy_dataset) 2559
epoch [43/200] batch [5/18] time 0.370 (0.446) data 0.239 (0.315) loss_x loss_x 1.6807 (1.4682) acc_x 56.2500 (61.2500) lr 1.7997e-03 eta 0:00:05
epoch [43/200] batch [10/18] time 0.449 (0.463) data 0.318 (0.332) loss_x loss_x 1.3760 (1.3991) acc_x 56.2500 (60.3125) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [15/18] time 0.474 (0.454) data 0.343 (0.323) loss_x loss_x 1.3164 (1.3423) acc_x 71.8750 (62.0833) lr 1.7997e-03 eta 0:00:01
epoch [43/200] batch [5/79] time 0.438 (0.451) data 0.307 (0.320) loss_u loss_u 0.9517 (0.9154) acc_u 3.1250 (10.6250) lr 1.7997e-03 eta 0:00:33
epoch [43/200] batch [10/79] time 0.438 (0.449) data 0.307 (0.318) loss_u loss_u 0.9033 (0.9167) acc_u 12.5000 (10.6250) lr 1.7997e-03 eta 0:00:30
epoch [43/200] batch [15/79] time 0.380 (0.439) data 0.248 (0.308) loss_u loss_u 0.9268 (0.9189) acc_u 9.3750 (10.6250) lr 1.7997e-03 eta 0:00:28
epoch [43/200] batch [20/79] time 0.548 (0.449) data 0.417 (0.318) loss_u loss_u 0.8540 (0.9165) acc_u 15.6250 (11.0938) lr 1.7997e-03 eta 0:00:26
epoch [43/200] batch [25/79] time 0.452 (0.450) data 0.321 (0.319) loss_u loss_u 0.9375 (0.9149) acc_u 9.3750 (11.5000) lr 1.7997e-03 eta 0:00:24
epoch [43/200] batch [30/79] time 0.437 (0.452) data 0.306 (0.321) loss_u loss_u 0.9507 (0.9172) acc_u 6.2500 (10.9375) lr 1.7997e-03 eta 0:00:22
epoch [43/200] batch [35/79] time 0.480 (0.457) data 0.349 (0.325) loss_u loss_u 0.8984 (0.9134) acc_u 15.6250 (11.2500) lr 1.7997e-03 eta 0:00:20
epoch [43/200] batch [40/79] time 0.413 (0.453) data 0.281 (0.322) loss_u loss_u 0.8794 (0.9146) acc_u 18.7500 (11.0938) lr 1.7997e-03 eta 0:00:17
epoch [43/200] batch [45/79] time 0.423 (0.450) data 0.292 (0.319) loss_u loss_u 0.8706 (0.9117) acc_u 15.6250 (11.3889) lr 1.7997e-03 eta 0:00:15
epoch [43/200] batch [50/79] time 0.432 (0.448) data 0.301 (0.317) loss_u loss_u 0.9038 (0.9119) acc_u 12.5000 (11.4375) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [55/79] time 0.390 (0.446) data 0.258 (0.315) loss_u loss_u 0.9829 (0.9131) acc_u 3.1250 (11.3068) lr 1.7997e-03 eta 0:00:10
epoch [43/200] batch [60/79] time 0.485 (0.447) data 0.354 (0.316) loss_u loss_u 0.8682 (0.9124) acc_u 12.5000 (11.3542) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [65/79] time 0.456 (0.448) data 0.324 (0.317) loss_u loss_u 0.9609 (0.9133) acc_u 3.1250 (11.2019) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [70/79] time 0.391 (0.447) data 0.260 (0.315) loss_u loss_u 0.9399 (0.9140) acc_u 6.2500 (11.0714) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [75/79] time 0.483 (0.449) data 0.352 (0.317) loss_u loss_u 0.8857 (0.9124) acc_u 15.6250 (11.2917) lr 1.7997e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2063
confident_label rate tensor(0.1980, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 621
clean true:340
clean false:281
clean_rate:0.5475040257648953
noisy true:733
noisy false:1782
after delete: len(clean_dataset) 621
after delete: len(noisy_dataset) 2515
epoch [44/200] batch [5/19] time 0.465 (0.439) data 0.335 (0.308) loss_x loss_x 1.5254 (1.4793) acc_x 68.7500 (67.5000) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [10/19] time 0.454 (0.472) data 0.323 (0.341) loss_x loss_x 1.6074 (1.5587) acc_x 65.6250 (64.3750) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [15/19] time 0.463 (0.489) data 0.332 (0.358) loss_x loss_x 2.3262 (1.6017) acc_x 56.2500 (63.9583) lr 1.7902e-03 eta 0:00:01
epoch [44/200] batch [5/78] time 0.436 (0.477) data 0.304 (0.346) loss_u loss_u 0.8877 (0.9084) acc_u 15.6250 (12.5000) lr 1.7902e-03 eta 0:00:34
epoch [44/200] batch [10/78] time 0.483 (0.472) data 0.351 (0.341) loss_u loss_u 0.9502 (0.9073) acc_u 0.0000 (11.2500) lr 1.7902e-03 eta 0:00:32
epoch [44/200] batch [15/78] time 0.402 (0.472) data 0.270 (0.340) loss_u loss_u 0.9336 (0.9081) acc_u 9.3750 (11.4583) lr 1.7902e-03 eta 0:00:29
epoch [44/200] batch [20/78] time 0.473 (0.470) data 0.338 (0.338) loss_u loss_u 0.9136 (0.9119) acc_u 6.2500 (10.3125) lr 1.7902e-03 eta 0:00:27
epoch [44/200] batch [25/78] time 0.439 (0.468) data 0.307 (0.336) loss_u loss_u 0.9331 (0.9141) acc_u 6.2500 (10.2500) lr 1.7902e-03 eta 0:00:24
epoch [44/200] batch [30/78] time 0.456 (0.470) data 0.326 (0.339) loss_u loss_u 0.9146 (0.9135) acc_u 12.5000 (10.6250) lr 1.7902e-03 eta 0:00:22
epoch [44/200] batch [35/78] time 0.368 (0.467) data 0.236 (0.336) loss_u loss_u 0.9473 (0.9150) acc_u 9.3750 (10.4464) lr 1.7902e-03 eta 0:00:20
epoch [44/200] batch [40/78] time 0.469 (0.464) data 0.338 (0.333) loss_u loss_u 0.8481 (0.9147) acc_u 25.0000 (10.6250) lr 1.7902e-03 eta 0:00:17
epoch [44/200] batch [45/78] time 0.384 (0.464) data 0.252 (0.332) loss_u loss_u 0.8975 (0.9143) acc_u 9.3750 (10.7639) lr 1.7902e-03 eta 0:00:15
epoch [44/200] batch [50/78] time 0.472 (0.464) data 0.340 (0.332) loss_u loss_u 0.9170 (0.9132) acc_u 9.3750 (11.0000) lr 1.7902e-03 eta 0:00:12
epoch [44/200] batch [55/78] time 0.412 (0.460) data 0.281 (0.328) loss_u loss_u 0.9268 (0.9134) acc_u 9.3750 (10.8523) lr 1.7902e-03 eta 0:00:10
epoch [44/200] batch [60/78] time 0.438 (0.459) data 0.306 (0.328) loss_u loss_u 0.9409 (0.9144) acc_u 6.2500 (10.6771) lr 1.7902e-03 eta 0:00:08
epoch [44/200] batch [65/78] time 0.579 (0.458) data 0.448 (0.326) loss_u loss_u 0.9556 (0.9158) acc_u 6.2500 (10.5288) lr 1.7902e-03 eta 0:00:05
epoch [44/200] batch [70/78] time 0.510 (0.457) data 0.378 (0.326) loss_u loss_u 0.8906 (0.9162) acc_u 12.5000 (10.5804) lr 1.7902e-03 eta 0:00:03
epoch [44/200] batch [75/78] time 0.350 (0.455) data 0.218 (0.324) loss_u loss_u 0.8701 (0.9163) acc_u 12.5000 (10.6667) lr 1.7902e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2072
confident_label rate tensor(0.1961, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 615
clean true:326
clean false:289
clean_rate:0.5300813008130081
noisy true:738
noisy false:1783
after delete: len(clean_dataset) 615
after delete: len(noisy_dataset) 2521
epoch [45/200] batch [5/19] time 0.504 (0.485) data 0.373 (0.354) loss_x loss_x 1.9053 (1.4364) acc_x 56.2500 (66.8750) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [10/19] time 0.461 (0.481) data 0.330 (0.350) loss_x loss_x 1.9658 (1.5753) acc_x 59.3750 (62.8125) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [15/19] time 0.372 (0.458) data 0.241 (0.327) loss_x loss_x 1.5996 (1.5724) acc_x 53.1250 (61.6667) lr 1.7804e-03 eta 0:00:01
epoch [45/200] batch [5/78] time 0.407 (0.480) data 0.275 (0.349) loss_u loss_u 0.8403 (0.9058) acc_u 21.8750 (12.5000) lr 1.7804e-03 eta 0:00:35
epoch [45/200] batch [10/78] time 0.423 (0.473) data 0.291 (0.342) loss_u loss_u 0.8979 (0.9159) acc_u 9.3750 (10.6250) lr 1.7804e-03 eta 0:00:32
epoch [45/200] batch [15/78] time 0.400 (0.469) data 0.268 (0.338) loss_u loss_u 0.9761 (0.9182) acc_u 3.1250 (10.4167) lr 1.7804e-03 eta 0:00:29
epoch [45/200] batch [20/78] time 0.503 (0.467) data 0.372 (0.336) loss_u loss_u 0.9053 (0.9223) acc_u 6.2500 (9.2188) lr 1.7804e-03 eta 0:00:27
epoch [45/200] batch [25/78] time 0.516 (0.477) data 0.385 (0.345) loss_u loss_u 0.9507 (0.9230) acc_u 6.2500 (9.3750) lr 1.7804e-03 eta 0:00:25
epoch [45/200] batch [30/78] time 0.465 (0.473) data 0.334 (0.342) loss_u loss_u 0.9639 (0.9215) acc_u 3.1250 (9.5833) lr 1.7804e-03 eta 0:00:22
epoch [45/200] batch [35/78] time 0.680 (0.474) data 0.549 (0.343) loss_u loss_u 0.8452 (0.9158) acc_u 15.6250 (10.3571) lr 1.7804e-03 eta 0:00:20
epoch [45/200] batch [40/78] time 0.422 (0.471) data 0.290 (0.340) loss_u loss_u 0.9243 (0.9171) acc_u 9.3750 (10.2344) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [45/78] time 0.474 (0.470) data 0.343 (0.339) loss_u loss_u 0.8691 (0.9165) acc_u 21.8750 (10.3472) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [50/78] time 0.391 (0.465) data 0.260 (0.333) loss_u loss_u 0.9072 (0.9168) acc_u 12.5000 (10.5000) lr 1.7804e-03 eta 0:00:13
epoch [45/200] batch [55/78] time 0.523 (0.467) data 0.391 (0.335) loss_u loss_u 0.8428 (0.9158) acc_u 15.6250 (10.6818) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [60/78] time 0.457 (0.469) data 0.325 (0.338) loss_u loss_u 0.8984 (0.9163) acc_u 12.5000 (10.6771) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [65/78] time 0.479 (0.468) data 0.348 (0.336) loss_u loss_u 0.9204 (0.9165) acc_u 15.6250 (10.7212) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [70/78] time 0.491 (0.469) data 0.360 (0.338) loss_u loss_u 0.9038 (0.9160) acc_u 15.6250 (10.8036) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [75/78] time 0.453 (0.467) data 0.322 (0.336) loss_u loss_u 0.8877 (0.9157) acc_u 18.7500 (10.8750) lr 1.7804e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2030
confident_label rate tensor(0.2006, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 629
clean true:337
clean false:292
clean_rate:0.5357710651828299
noisy true:769
noisy false:1738
after delete: len(clean_dataset) 629
after delete: len(noisy_dataset) 2507
epoch [46/200] batch [5/19] time 0.428 (0.452) data 0.297 (0.321) loss_x loss_x 1.2773 (1.5617) acc_x 56.2500 (55.6250) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [10/19] time 0.545 (0.478) data 0.414 (0.347) loss_x loss_x 2.0195 (1.5420) acc_x 56.2500 (60.3125) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [15/19] time 0.474 (0.469) data 0.343 (0.338) loss_x loss_x 1.2451 (1.4605) acc_x 59.3750 (61.2500) lr 1.7705e-03 eta 0:00:01
epoch [46/200] batch [5/78] time 0.443 (0.462) data 0.311 (0.331) loss_u loss_u 0.8955 (0.9130) acc_u 15.6250 (10.6250) lr 1.7705e-03 eta 0:00:33
epoch [46/200] batch [10/78] time 0.536 (0.455) data 0.404 (0.324) loss_u loss_u 0.8799 (0.9173) acc_u 15.6250 (10.3125) lr 1.7705e-03 eta 0:00:30
epoch [46/200] batch [15/78] time 0.469 (0.453) data 0.339 (0.322) loss_u loss_u 0.9048 (0.9174) acc_u 15.6250 (10.8333) lr 1.7705e-03 eta 0:00:28
epoch [46/200] batch [20/78] time 0.782 (0.460) data 0.649 (0.329) loss_u loss_u 0.9663 (0.9175) acc_u 0.0000 (10.4688) lr 1.7705e-03 eta 0:00:26
epoch [46/200] batch [25/78] time 0.428 (0.460) data 0.296 (0.329) loss_u loss_u 0.9785 (0.9177) acc_u 0.0000 (10.2500) lr 1.7705e-03 eta 0:00:24
epoch [46/200] batch [30/78] time 0.465 (0.461) data 0.334 (0.330) loss_u loss_u 0.9429 (0.9169) acc_u 6.2500 (10.6250) lr 1.7705e-03 eta 0:00:22
epoch [46/200] batch [35/78] time 0.373 (0.460) data 0.243 (0.329) loss_u loss_u 0.9243 (0.9193) acc_u 6.2500 (10.1786) lr 1.7705e-03 eta 0:00:19
epoch [46/200] batch [40/78] time 0.414 (0.456) data 0.283 (0.325) loss_u loss_u 0.9375 (0.9187) acc_u 9.3750 (10.0781) lr 1.7705e-03 eta 0:00:17
epoch [46/200] batch [45/78] time 0.403 (0.453) data 0.273 (0.322) loss_u loss_u 0.9165 (0.9183) acc_u 6.2500 (10.0694) lr 1.7705e-03 eta 0:00:14
epoch [46/200] batch [50/78] time 0.420 (0.457) data 0.288 (0.326) loss_u loss_u 0.8555 (0.9156) acc_u 18.7500 (10.6250) lr 1.7705e-03 eta 0:00:12
epoch [46/200] batch [55/78] time 0.336 (0.457) data 0.201 (0.325) loss_u loss_u 0.8364 (0.9145) acc_u 28.1250 (11.0227) lr 1.7705e-03 eta 0:00:10
epoch [46/200] batch [60/78] time 0.405 (0.460) data 0.271 (0.329) loss_u loss_u 0.8979 (0.9146) acc_u 12.5000 (10.9896) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [65/78] time 0.345 (0.459) data 0.213 (0.328) loss_u loss_u 0.8882 (0.9131) acc_u 15.6250 (11.2019) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [70/78] time 0.487 (0.457) data 0.355 (0.326) loss_u loss_u 0.9150 (0.9132) acc_u 9.3750 (11.0268) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [75/78] time 0.416 (0.456) data 0.284 (0.325) loss_u loss_u 0.9253 (0.9125) acc_u 15.6250 (11.2083) lr 1.7705e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2025
confident_label rate tensor(0.1955, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 613
clean true:329
clean false:284
clean_rate:0.5367047308319739
noisy true:782
noisy false:1741
after delete: len(clean_dataset) 613
after delete: len(noisy_dataset) 2523
epoch [47/200] batch [5/19] time 0.482 (0.531) data 0.351 (0.399) loss_x loss_x 1.0869 (1.6201) acc_x 75.0000 (61.2500) lr 1.7604e-03 eta 0:00:07
epoch [47/200] batch [10/19] time 0.433 (0.489) data 0.302 (0.358) loss_x loss_x 1.9453 (1.5970) acc_x 50.0000 (59.0625) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [15/19] time 0.453 (0.476) data 0.322 (0.345) loss_x loss_x 1.3965 (1.5456) acc_x 68.7500 (59.7917) lr 1.7604e-03 eta 0:00:01
epoch [47/200] batch [5/78] time 0.571 (0.480) data 0.440 (0.349) loss_u loss_u 0.8877 (0.9108) acc_u 15.6250 (10.6250) lr 1.7604e-03 eta 0:00:35
epoch [47/200] batch [10/78] time 0.436 (0.477) data 0.305 (0.346) loss_u loss_u 0.8179 (0.9089) acc_u 31.2500 (11.5625) lr 1.7604e-03 eta 0:00:32
epoch [47/200] batch [15/78] time 0.415 (0.471) data 0.283 (0.340) loss_u loss_u 0.9370 (0.9138) acc_u 9.3750 (10.6250) lr 1.7604e-03 eta 0:00:29
epoch [47/200] batch [20/78] time 0.448 (0.469) data 0.316 (0.338) loss_u loss_u 0.9399 (0.9069) acc_u 6.2500 (11.0938) lr 1.7604e-03 eta 0:00:27
epoch [47/200] batch [25/78] time 0.423 (0.465) data 0.291 (0.334) loss_u loss_u 0.8594 (0.9091) acc_u 18.7500 (10.7500) lr 1.7604e-03 eta 0:00:24
epoch [47/200] batch [30/78] time 0.560 (0.464) data 0.429 (0.333) loss_u loss_u 0.9004 (0.9087) acc_u 12.5000 (11.2500) lr 1.7604e-03 eta 0:00:22
epoch [47/200] batch [35/78] time 0.491 (0.461) data 0.359 (0.329) loss_u loss_u 0.9678 (0.9133) acc_u 3.1250 (10.6250) lr 1.7604e-03 eta 0:00:19
epoch [47/200] batch [40/78] time 0.452 (0.456) data 0.319 (0.325) loss_u loss_u 0.9048 (0.9124) acc_u 9.3750 (10.7031) lr 1.7604e-03 eta 0:00:17
epoch [47/200] batch [45/78] time 0.401 (0.457) data 0.270 (0.326) loss_u loss_u 0.9648 (0.9126) acc_u 6.2500 (10.7639) lr 1.7604e-03 eta 0:00:15
epoch [47/200] batch [50/78] time 0.522 (0.453) data 0.390 (0.322) loss_u loss_u 0.9658 (0.9145) acc_u 0.0000 (10.4375) lr 1.7604e-03 eta 0:00:12
epoch [47/200] batch [55/78] time 0.503 (0.452) data 0.372 (0.320) loss_u loss_u 0.9316 (0.9140) acc_u 12.5000 (10.7386) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [60/78] time 0.416 (0.449) data 0.284 (0.317) loss_u loss_u 0.8647 (0.9130) acc_u 18.7500 (10.9896) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [65/78] time 0.478 (0.450) data 0.347 (0.318) loss_u loss_u 0.9756 (0.9131) acc_u 3.1250 (10.9615) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [70/78] time 0.376 (0.447) data 0.245 (0.315) loss_u loss_u 0.9326 (0.9133) acc_u 9.3750 (11.0268) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [75/78] time 0.351 (0.450) data 0.221 (0.318) loss_u loss_u 0.9463 (0.9137) acc_u 6.2500 (10.9167) lr 1.7604e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2030
confident_label rate tensor(0.1990, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 624
clean true:340
clean false:284
clean_rate:0.5448717948717948
noisy true:766
noisy false:1746
after delete: len(clean_dataset) 624
after delete: len(noisy_dataset) 2512
epoch [48/200] batch [5/19] time 0.356 (0.474) data 0.225 (0.343) loss_x loss_x 1.4482 (1.4830) acc_x 75.0000 (63.1250) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [10/19] time 0.455 (0.469) data 0.324 (0.338) loss_x loss_x 1.1465 (1.3746) acc_x 75.0000 (63.7500) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [15/19] time 0.484 (0.471) data 0.354 (0.340) loss_x loss_x 1.4521 (1.4233) acc_x 56.2500 (62.0833) lr 1.7501e-03 eta 0:00:01
epoch [48/200] batch [5/78] time 0.600 (0.475) data 0.470 (0.344) loss_u loss_u 0.9160 (0.9038) acc_u 12.5000 (11.8750) lr 1.7501e-03 eta 0:00:34
epoch [48/200] batch [10/78] time 0.288 (0.468) data 0.157 (0.338) loss_u loss_u 0.9629 (0.9057) acc_u 9.3750 (12.5000) lr 1.7501e-03 eta 0:00:31
epoch [48/200] batch [15/78] time 0.391 (0.455) data 0.260 (0.324) loss_u loss_u 0.9458 (0.9080) acc_u 9.3750 (12.7083) lr 1.7501e-03 eta 0:00:28
epoch [48/200] batch [20/78] time 0.493 (0.456) data 0.361 (0.325) loss_u loss_u 0.9414 (0.9150) acc_u 6.2500 (11.8750) lr 1.7501e-03 eta 0:00:26
epoch [48/200] batch [25/78] time 0.431 (0.455) data 0.299 (0.324) loss_u loss_u 0.8994 (0.9121) acc_u 15.6250 (12.5000) lr 1.7501e-03 eta 0:00:24
epoch [48/200] batch [30/78] time 0.373 (0.451) data 0.242 (0.320) loss_u loss_u 0.9165 (0.9103) acc_u 9.3750 (12.3958) lr 1.7501e-03 eta 0:00:21
epoch [48/200] batch [35/78] time 0.403 (0.446) data 0.271 (0.315) loss_u loss_u 0.9121 (0.9107) acc_u 15.6250 (12.1429) lr 1.7501e-03 eta 0:00:19
epoch [48/200] batch [40/78] time 0.537 (0.446) data 0.405 (0.315) loss_u loss_u 0.9189 (0.9145) acc_u 9.3750 (11.3281) lr 1.7501e-03 eta 0:00:16
epoch [48/200] batch [45/78] time 0.407 (0.451) data 0.276 (0.320) loss_u loss_u 0.8916 (0.9159) acc_u 15.6250 (10.9722) lr 1.7501e-03 eta 0:00:14
epoch [48/200] batch [50/78] time 0.514 (0.451) data 0.382 (0.320) loss_u loss_u 0.9512 (0.9165) acc_u 3.1250 (10.8125) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [55/78] time 0.375 (0.456) data 0.242 (0.325) loss_u loss_u 0.8931 (0.9167) acc_u 15.6250 (10.7386) lr 1.7501e-03 eta 0:00:10
epoch [48/200] batch [60/78] time 0.361 (0.456) data 0.231 (0.325) loss_u loss_u 0.8945 (0.9142) acc_u 12.5000 (11.0417) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [65/78] time 0.415 (0.456) data 0.284 (0.325) loss_u loss_u 0.8638 (0.9136) acc_u 18.7500 (10.9615) lr 1.7501e-03 eta 0:00:05
epoch [48/200] batch [70/78] time 0.515 (0.454) data 0.385 (0.322) loss_u loss_u 0.9331 (0.9139) acc_u 9.3750 (10.9375) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [75/78] time 0.473 (0.452) data 0.342 (0.321) loss_u loss_u 0.9233 (0.9150) acc_u 12.5000 (10.7917) lr 1.7501e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2020
confident_label rate tensor(0.1971, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 618
clean true:331
clean false:287
clean_rate:0.5355987055016181
noisy true:785
noisy false:1733
after delete: len(clean_dataset) 618
after delete: len(noisy_dataset) 2518
epoch [49/200] batch [5/19] time 0.423 (0.445) data 0.292 (0.314) loss_x loss_x 1.2822 (1.3489) acc_x 75.0000 (68.1250) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [10/19] time 0.418 (0.453) data 0.288 (0.322) loss_x loss_x 1.5635 (1.3813) acc_x 56.2500 (65.3125) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [15/19] time 0.609 (0.475) data 0.476 (0.344) loss_x loss_x 1.7871 (1.4808) acc_x 53.1250 (63.3333) lr 1.7396e-03 eta 0:00:01
epoch [49/200] batch [5/78] time 0.601 (0.478) data 0.470 (0.347) loss_u loss_u 0.8481 (0.8996) acc_u 18.7500 (11.8750) lr 1.7396e-03 eta 0:00:34
epoch [49/200] batch [10/78] time 0.453 (0.471) data 0.322 (0.340) loss_u loss_u 0.9282 (0.9074) acc_u 6.2500 (11.8750) lr 1.7396e-03 eta 0:00:32
epoch [49/200] batch [15/78] time 0.414 (0.475) data 0.283 (0.344) loss_u loss_u 0.9541 (0.9146) acc_u 6.2500 (11.0417) lr 1.7396e-03 eta 0:00:29
epoch [49/200] batch [20/78] time 0.459 (0.473) data 0.328 (0.342) loss_u loss_u 0.9316 (0.9152) acc_u 9.3750 (10.6250) lr 1.7396e-03 eta 0:00:27
epoch [49/200] batch [25/78] time 0.569 (0.480) data 0.437 (0.349) loss_u loss_u 0.9102 (0.9118) acc_u 9.3750 (11.1250) lr 1.7396e-03 eta 0:00:25
epoch [49/200] batch [30/78] time 0.463 (0.478) data 0.332 (0.347) loss_u loss_u 0.8970 (0.9112) acc_u 12.5000 (11.5625) lr 1.7396e-03 eta 0:00:22
epoch [49/200] batch [35/78] time 0.492 (0.474) data 0.361 (0.343) loss_u loss_u 0.9365 (0.9124) acc_u 3.1250 (11.3393) lr 1.7396e-03 eta 0:00:20
epoch [49/200] batch [40/78] time 0.414 (0.481) data 0.283 (0.350) loss_u loss_u 0.9219 (0.9133) acc_u 9.3750 (11.4062) lr 1.7396e-03 eta 0:00:18
epoch [49/200] batch [45/78] time 0.506 (0.479) data 0.375 (0.348) loss_u loss_u 0.8784 (0.9131) acc_u 12.5000 (11.1111) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [50/78] time 0.450 (0.478) data 0.319 (0.346) loss_u loss_u 0.8677 (0.9116) acc_u 18.7500 (11.5000) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [55/78] time 0.444 (0.474) data 0.313 (0.343) loss_u loss_u 0.9189 (0.9100) acc_u 6.2500 (11.4773) lr 1.7396e-03 eta 0:00:10
epoch [49/200] batch [60/78] time 0.514 (0.474) data 0.383 (0.343) loss_u loss_u 0.8950 (0.9089) acc_u 15.6250 (11.7188) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [65/78] time 0.487 (0.472) data 0.356 (0.341) loss_u loss_u 0.9160 (0.9099) acc_u 15.6250 (11.7788) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [70/78] time 0.402 (0.473) data 0.272 (0.341) loss_u loss_u 0.9067 (0.9092) acc_u 12.5000 (11.8304) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [75/78] time 0.494 (0.471) data 0.363 (0.340) loss_u loss_u 0.9482 (0.9103) acc_u 3.1250 (11.6250) lr 1.7396e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2009
confident_label rate tensor(0.2006, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 629
clean true:345
clean false:284
clean_rate:0.548489666136725
noisy true:782
noisy false:1725
after delete: len(clean_dataset) 629
after delete: len(noisy_dataset) 2507
epoch [50/200] batch [5/19] time 0.386 (0.444) data 0.256 (0.314) loss_x loss_x 1.9531 (1.5113) acc_x 59.3750 (61.2500) lr 1.7290e-03 eta 0:00:06
epoch [50/200] batch [10/19] time 0.407 (0.435) data 0.276 (0.304) loss_x loss_x 1.5137 (1.4168) acc_x 62.5000 (62.8125) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [15/19] time 0.410 (0.444) data 0.278 (0.314) loss_x loss_x 1.3584 (1.3799) acc_x 68.7500 (63.5417) lr 1.7290e-03 eta 0:00:01
epoch [50/200] batch [5/78] time 0.381 (0.439) data 0.249 (0.308) loss_u loss_u 0.9082 (0.8974) acc_u 12.5000 (15.0000) lr 1.7290e-03 eta 0:00:32
epoch [50/200] batch [10/78] time 0.522 (0.436) data 0.392 (0.305) loss_u loss_u 0.9053 (0.9030) acc_u 9.3750 (14.0625) lr 1.7290e-03 eta 0:00:29
epoch [50/200] batch [15/78] time 0.350 (0.435) data 0.217 (0.304) loss_u loss_u 0.8525 (0.9075) acc_u 21.8750 (13.3333) lr 1.7290e-03 eta 0:00:27
epoch [50/200] batch [20/78] time 0.492 (0.437) data 0.360 (0.306) loss_u loss_u 0.9370 (0.9056) acc_u 9.3750 (13.5938) lr 1.7290e-03 eta 0:00:25
epoch [50/200] batch [25/78] time 0.407 (0.437) data 0.275 (0.306) loss_u loss_u 0.9360 (0.9108) acc_u 3.1250 (12.6250) lr 1.7290e-03 eta 0:00:23
epoch [50/200] batch [30/78] time 0.393 (0.442) data 0.262 (0.311) loss_u loss_u 0.9458 (0.9132) acc_u 3.1250 (12.1875) lr 1.7290e-03 eta 0:00:21
epoch [50/200] batch [35/78] time 0.326 (0.439) data 0.195 (0.308) loss_u loss_u 0.9326 (0.9140) acc_u 6.2500 (11.8750) lr 1.7290e-03 eta 0:00:18
epoch [50/200] batch [40/78] time 0.492 (0.442) data 0.361 (0.311) loss_u loss_u 0.9062 (0.9126) acc_u 9.3750 (11.7969) lr 1.7290e-03 eta 0:00:16
epoch [50/200] batch [45/78] time 0.367 (0.447) data 0.236 (0.316) loss_u loss_u 0.8472 (0.9111) acc_u 21.8750 (12.0833) lr 1.7290e-03 eta 0:00:14
epoch [50/200] batch [50/78] time 0.557 (0.447) data 0.426 (0.316) loss_u loss_u 0.9429 (0.9135) acc_u 9.3750 (11.7500) lr 1.7290e-03 eta 0:00:12
epoch [50/200] batch [55/78] time 0.348 (0.448) data 0.216 (0.317) loss_u loss_u 0.8315 (0.9116) acc_u 28.1250 (11.9886) lr 1.7290e-03 eta 0:00:10
epoch [50/200] batch [60/78] time 0.448 (0.450) data 0.318 (0.319) loss_u loss_u 0.8755 (0.9115) acc_u 12.5000 (11.9792) lr 1.7290e-03 eta 0:00:08
epoch [50/200] batch [65/78] time 0.353 (0.448) data 0.221 (0.317) loss_u loss_u 0.9487 (0.9123) acc_u 6.2500 (11.7308) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [70/78] time 0.526 (0.454) data 0.394 (0.323) loss_u loss_u 0.8652 (0.9108) acc_u 18.7500 (11.7411) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [75/78] time 0.403 (0.453) data 0.273 (0.322) loss_u loss_u 0.9199 (0.9110) acc_u 12.5000 (11.7500) lr 1.7290e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2043
confident_label rate tensor(0.2089, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 655
clean true:348
clean false:307
clean_rate:0.5312977099236641
noisy true:745
noisy false:1736
after delete: len(clean_dataset) 655
after delete: len(noisy_dataset) 2481
epoch [51/200] batch [5/20] time 0.500 (0.485) data 0.370 (0.354) loss_x loss_x 1.9707 (1.5461) acc_x 53.1250 (63.7500) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [10/20] time 0.533 (0.503) data 0.403 (0.373) loss_x loss_x 1.2334 (1.4191) acc_x 62.5000 (62.1875) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [15/20] time 0.567 (0.499) data 0.437 (0.369) loss_x loss_x 1.6035 (1.5220) acc_x 50.0000 (58.9583) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [20/20] time 0.440 (0.487) data 0.310 (0.357) loss_x loss_x 1.3066 (1.4962) acc_x 56.2500 (58.9062) lr 1.7181e-03 eta 0:00:00
epoch [51/200] batch [5/77] time 0.545 (0.477) data 0.414 (0.346) loss_u loss_u 0.9175 (0.8817) acc_u 12.5000 (16.2500) lr 1.7181e-03 eta 0:00:34
epoch [51/200] batch [10/77] time 0.330 (0.469) data 0.198 (0.338) loss_u loss_u 0.9512 (0.8971) acc_u 9.3750 (15.3125) lr 1.7181e-03 eta 0:00:31
epoch [51/200] batch [15/77] time 0.377 (0.459) data 0.245 (0.328) loss_u loss_u 0.9316 (0.9054) acc_u 9.3750 (13.7500) lr 1.7181e-03 eta 0:00:28
epoch [51/200] batch [20/77] time 0.454 (0.461) data 0.322 (0.330) loss_u loss_u 0.9082 (0.9125) acc_u 12.5000 (12.3438) lr 1.7181e-03 eta 0:00:26
epoch [51/200] batch [25/77] time 0.437 (0.459) data 0.307 (0.328) loss_u loss_u 0.8975 (0.9151) acc_u 21.8750 (12.0000) lr 1.7181e-03 eta 0:00:23
epoch [51/200] batch [30/77] time 0.391 (0.460) data 0.261 (0.329) loss_u loss_u 0.9551 (0.9138) acc_u 3.1250 (11.7708) lr 1.7181e-03 eta 0:00:21
epoch [51/200] batch [35/77] time 0.542 (0.467) data 0.411 (0.336) loss_u loss_u 0.9268 (0.9148) acc_u 9.3750 (11.5179) lr 1.7181e-03 eta 0:00:19
epoch [51/200] batch [40/77] time 0.552 (0.466) data 0.421 (0.336) loss_u loss_u 0.9258 (0.9151) acc_u 12.5000 (11.4844) lr 1.7181e-03 eta 0:00:17
epoch [51/200] batch [45/77] time 0.587 (0.465) data 0.455 (0.334) loss_u loss_u 0.8940 (0.9158) acc_u 15.6250 (11.3889) lr 1.7181e-03 eta 0:00:14
epoch [51/200] batch [50/77] time 0.405 (0.465) data 0.273 (0.334) loss_u loss_u 0.8467 (0.9143) acc_u 28.1250 (11.6250) lr 1.7181e-03 eta 0:00:12
epoch [51/200] batch [55/77] time 0.480 (0.462) data 0.348 (0.331) loss_u loss_u 0.9092 (0.9141) acc_u 18.7500 (11.5341) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [60/77] time 0.500 (0.461) data 0.369 (0.330) loss_u loss_u 0.9395 (0.9128) acc_u 6.2500 (11.6146) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [65/77] time 0.487 (0.459) data 0.356 (0.328) loss_u loss_u 0.8799 (0.9110) acc_u 18.7500 (11.7788) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [70/77] time 0.454 (0.459) data 0.322 (0.328) loss_u loss_u 0.9072 (0.9110) acc_u 6.2500 (11.6964) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [75/77] time 0.340 (0.458) data 0.209 (0.327) loss_u loss_u 0.9531 (0.9127) acc_u 0.0000 (11.4167) lr 1.7181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2075
confident_label rate tensor(0.2025, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 635
clean true:338
clean false:297
clean_rate:0.5322834645669291
noisy true:723
noisy false:1778
after delete: len(clean_dataset) 635
after delete: len(noisy_dataset) 2501
epoch [52/200] batch [5/19] time 0.377 (0.439) data 0.247 (0.308) loss_x loss_x 1.9883 (1.5646) acc_x 59.3750 (61.8750) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [10/19] time 0.457 (0.433) data 0.326 (0.303) loss_x loss_x 1.5742 (1.4796) acc_x 62.5000 (62.8125) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [15/19] time 0.576 (0.443) data 0.445 (0.312) loss_x loss_x 1.6191 (1.4990) acc_x 53.1250 (61.0417) lr 1.7071e-03 eta 0:00:01
epoch [52/200] batch [5/78] time 0.408 (0.446) data 0.277 (0.315) loss_u loss_u 0.9229 (0.9370) acc_u 6.2500 (6.8750) lr 1.7071e-03 eta 0:00:32
epoch [52/200] batch [10/78] time 0.446 (0.443) data 0.313 (0.312) loss_u loss_u 0.9355 (0.9205) acc_u 6.2500 (8.7500) lr 1.7071e-03 eta 0:00:30
epoch [52/200] batch [15/78] time 0.549 (0.454) data 0.416 (0.323) loss_u loss_u 0.8857 (0.9116) acc_u 15.6250 (11.0417) lr 1.7071e-03 eta 0:00:28
epoch [52/200] batch [20/78] time 0.450 (0.454) data 0.319 (0.323) loss_u loss_u 0.9028 (0.9141) acc_u 9.3750 (11.0938) lr 1.7071e-03 eta 0:00:26
epoch [52/200] batch [25/78] time 0.456 (0.462) data 0.325 (0.331) loss_u loss_u 0.9297 (0.9184) acc_u 9.3750 (10.8750) lr 1.7071e-03 eta 0:00:24
epoch [52/200] batch [30/78] time 0.405 (0.469) data 0.274 (0.338) loss_u loss_u 0.8960 (0.9117) acc_u 9.3750 (11.6667) lr 1.7071e-03 eta 0:00:22
epoch [52/200] batch [35/78] time 0.475 (0.472) data 0.343 (0.341) loss_u loss_u 0.9492 (0.9116) acc_u 6.2500 (11.4286) lr 1.7071e-03 eta 0:00:20
epoch [52/200] batch [40/78] time 0.390 (0.471) data 0.259 (0.340) loss_u loss_u 0.9287 (0.9116) acc_u 9.3750 (11.6406) lr 1.7071e-03 eta 0:00:17
epoch [52/200] batch [45/78] time 0.552 (0.470) data 0.421 (0.338) loss_u loss_u 0.9546 (0.9127) acc_u 6.2500 (11.3889) lr 1.7071e-03 eta 0:00:15
epoch [52/200] batch [50/78] time 0.563 (0.470) data 0.432 (0.339) loss_u loss_u 0.8765 (0.9131) acc_u 18.7500 (11.3125) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [55/78] time 0.679 (0.473) data 0.549 (0.342) loss_u loss_u 0.8950 (0.9136) acc_u 12.5000 (11.1932) lr 1.7071e-03 eta 0:00:10
epoch [52/200] batch [60/78] time 0.412 (0.470) data 0.281 (0.338) loss_u loss_u 0.8599 (0.9117) acc_u 12.5000 (11.4583) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [65/78] time 0.584 (0.472) data 0.452 (0.341) loss_u loss_u 0.9507 (0.9125) acc_u 6.2500 (11.2981) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [70/78] time 0.400 (0.471) data 0.269 (0.339) loss_u loss_u 0.8853 (0.9114) acc_u 9.3750 (11.3393) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [75/78] time 0.472 (0.468) data 0.341 (0.337) loss_u loss_u 0.9067 (0.9102) acc_u 12.5000 (11.4583) lr 1.7071e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2044
confident_label rate tensor(0.2018, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 633
clean true:338
clean false:295
clean_rate:0.5339652448657188
noisy true:754
noisy false:1749
after delete: len(clean_dataset) 633
after delete: len(noisy_dataset) 2503
epoch [53/200] batch [5/19] time 0.548 (0.460) data 0.418 (0.330) loss_x loss_x 1.4395 (1.4885) acc_x 71.8750 (63.1250) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [10/19] time 0.378 (0.450) data 0.247 (0.320) loss_x loss_x 1.4395 (1.4490) acc_x 68.7500 (63.4375) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [15/19] time 0.533 (0.452) data 0.403 (0.322) loss_x loss_x 1.9365 (1.3963) acc_x 53.1250 (64.7917) lr 1.6959e-03 eta 0:00:01
epoch [53/200] batch [5/78] time 0.473 (0.450) data 0.342 (0.319) loss_u loss_u 0.9375 (0.9431) acc_u 9.3750 (6.8750) lr 1.6959e-03 eta 0:00:32
epoch [53/200] batch [10/78] time 0.459 (0.452) data 0.327 (0.321) loss_u loss_u 0.8706 (0.9069) acc_u 15.6250 (11.5625) lr 1.6959e-03 eta 0:00:30
epoch [53/200] batch [15/78] time 0.645 (0.459) data 0.514 (0.328) loss_u loss_u 0.9751 (0.9034) acc_u 3.1250 (13.1250) lr 1.6959e-03 eta 0:00:28
epoch [53/200] batch [20/78] time 0.493 (0.453) data 0.361 (0.322) loss_u loss_u 0.9775 (0.9153) acc_u 3.1250 (11.7188) lr 1.6959e-03 eta 0:00:26
epoch [53/200] batch [25/78] time 0.696 (0.459) data 0.565 (0.328) loss_u loss_u 0.9414 (0.9118) acc_u 6.2500 (12.2500) lr 1.6959e-03 eta 0:00:24
epoch [53/200] batch [30/78] time 0.409 (0.457) data 0.278 (0.326) loss_u loss_u 0.9624 (0.9169) acc_u 3.1250 (11.2500) lr 1.6959e-03 eta 0:00:21
epoch [53/200] batch [35/78] time 0.511 (0.454) data 0.379 (0.323) loss_u loss_u 0.8911 (0.9159) acc_u 15.6250 (11.2500) lr 1.6959e-03 eta 0:00:19
epoch [53/200] batch [40/78] time 0.494 (0.453) data 0.362 (0.322) loss_u loss_u 0.9009 (0.9156) acc_u 6.2500 (11.0156) lr 1.6959e-03 eta 0:00:17
epoch [53/200] batch [45/78] time 0.508 (0.455) data 0.376 (0.324) loss_u loss_u 0.8330 (0.9156) acc_u 18.7500 (10.9722) lr 1.6959e-03 eta 0:00:15
epoch [53/200] batch [50/78] time 0.373 (0.452) data 0.241 (0.321) loss_u loss_u 0.9004 (0.9153) acc_u 18.7500 (10.9375) lr 1.6959e-03 eta 0:00:12
epoch [53/200] batch [55/78] time 0.618 (0.455) data 0.487 (0.324) loss_u loss_u 0.8945 (0.9150) acc_u 12.5000 (11.2500) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [60/78] time 0.373 (0.454) data 0.241 (0.323) loss_u loss_u 0.8667 (0.9129) acc_u 18.7500 (11.3542) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [65/78] time 0.443 (0.454) data 0.311 (0.323) loss_u loss_u 0.9307 (0.9119) acc_u 9.3750 (11.4904) lr 1.6959e-03 eta 0:00:05
epoch [53/200] batch [70/78] time 0.535 (0.454) data 0.404 (0.322) loss_u loss_u 0.9482 (0.9125) acc_u 3.1250 (11.3839) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [75/78] time 0.424 (0.451) data 0.292 (0.320) loss_u loss_u 0.8926 (0.9133) acc_u 12.5000 (11.2500) lr 1.6959e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2112
confident_label rate tensor(0.2034, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 638
clean true:320
clean false:318
clean_rate:0.5015673981191222
noisy true:704
noisy false:1794
after delete: len(clean_dataset) 638
after delete: len(noisy_dataset) 2498
epoch [54/200] batch [5/19] time 0.494 (0.442) data 0.363 (0.311) loss_x loss_x 1.1465 (1.2770) acc_x 65.6250 (65.6250) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [10/19] time 0.462 (0.478) data 0.331 (0.347) loss_x loss_x 1.4482 (1.3788) acc_x 62.5000 (65.3125) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [15/19] time 0.393 (0.489) data 0.263 (0.358) loss_x loss_x 1.3135 (1.3305) acc_x 68.7500 (65.2083) lr 1.6845e-03 eta 0:00:01
epoch [54/200] batch [5/78] time 0.398 (0.466) data 0.267 (0.335) loss_u loss_u 0.8857 (0.9009) acc_u 12.5000 (10.0000) lr 1.6845e-03 eta 0:00:34
epoch [54/200] batch [10/78] time 0.385 (0.457) data 0.253 (0.326) loss_u loss_u 0.8745 (0.9073) acc_u 15.6250 (9.0625) lr 1.6845e-03 eta 0:00:31
epoch [54/200] batch [15/78] time 0.444 (0.453) data 0.312 (0.322) loss_u loss_u 0.9409 (0.9108) acc_u 9.3750 (9.7917) lr 1.6845e-03 eta 0:00:28
epoch [54/200] batch [20/78] time 0.523 (0.465) data 0.391 (0.333) loss_u loss_u 0.9033 (0.9089) acc_u 15.6250 (10.7812) lr 1.6845e-03 eta 0:00:26
epoch [54/200] batch [25/78] time 0.554 (0.468) data 0.422 (0.337) loss_u loss_u 0.8813 (0.9119) acc_u 15.6250 (10.6250) lr 1.6845e-03 eta 0:00:24
epoch [54/200] batch [30/78] time 0.494 (0.467) data 0.362 (0.335) loss_u loss_u 0.9375 (0.9154) acc_u 9.3750 (10.1042) lr 1.6845e-03 eta 0:00:22
epoch [54/200] batch [35/78] time 0.570 (0.470) data 0.439 (0.339) loss_u loss_u 0.9785 (0.9159) acc_u 0.0000 (10.2679) lr 1.6845e-03 eta 0:00:20
epoch [54/200] batch [40/78] time 0.421 (0.469) data 0.288 (0.338) loss_u loss_u 0.9536 (0.9145) acc_u 6.2500 (10.4688) lr 1.6845e-03 eta 0:00:17
epoch [54/200] batch [45/78] time 0.497 (0.464) data 0.365 (0.333) loss_u loss_u 0.8867 (0.9156) acc_u 18.7500 (10.4167) lr 1.6845e-03 eta 0:00:15
epoch [54/200] batch [50/78] time 0.470 (0.464) data 0.338 (0.333) loss_u loss_u 0.9507 (0.9170) acc_u 3.1250 (10.2500) lr 1.6845e-03 eta 0:00:12
epoch [54/200] batch [55/78] time 0.368 (0.462) data 0.237 (0.331) loss_u loss_u 0.9443 (0.9172) acc_u 6.2500 (10.2841) lr 1.6845e-03 eta 0:00:10
epoch [54/200] batch [60/78] time 0.387 (0.458) data 0.256 (0.327) loss_u loss_u 0.8804 (0.9164) acc_u 15.6250 (10.6771) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [65/78] time 0.633 (0.461) data 0.501 (0.330) loss_u loss_u 0.9443 (0.9155) acc_u 3.1250 (10.9615) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [70/78] time 0.415 (0.460) data 0.284 (0.329) loss_u loss_u 0.8594 (0.9135) acc_u 15.6250 (11.1161) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [75/78] time 0.413 (0.462) data 0.283 (0.330) loss_u loss_u 0.8711 (0.9112) acc_u 18.7500 (11.2917) lr 1.6845e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2067
confident_label rate tensor(0.2073, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 650
clean true:338
clean false:312
clean_rate:0.52
noisy true:731
noisy false:1755
after delete: len(clean_dataset) 650
after delete: len(noisy_dataset) 2486
epoch [55/200] batch [5/20] time 0.508 (0.488) data 0.378 (0.357) loss_x loss_x 1.3105 (1.3604) acc_x 68.7500 (63.7500) lr 1.6730e-03 eta 0:00:07
epoch [55/200] batch [10/20] time 0.404 (0.441) data 0.273 (0.311) loss_x loss_x 2.0703 (1.4877) acc_x 46.8750 (61.5625) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [15/20] time 0.456 (0.449) data 0.326 (0.318) loss_x loss_x 2.0293 (1.5538) acc_x 56.2500 (60.0000) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [20/20] time 0.421 (0.454) data 0.291 (0.323) loss_x loss_x 1.4775 (1.5293) acc_x 59.3750 (60.6250) lr 1.6730e-03 eta 0:00:00
epoch [55/200] batch [5/77] time 0.457 (0.444) data 0.325 (0.314) loss_u loss_u 0.9038 (0.9166) acc_u 12.5000 (8.7500) lr 1.6730e-03 eta 0:00:31
epoch [55/200] batch [10/77] time 0.464 (0.453) data 0.332 (0.322) loss_u loss_u 0.8364 (0.9051) acc_u 25.0000 (11.8750) lr 1.6730e-03 eta 0:00:30
epoch [55/200] batch [15/77] time 0.422 (0.450) data 0.291 (0.319) loss_u loss_u 0.8887 (0.9048) acc_u 12.5000 (11.4583) lr 1.6730e-03 eta 0:00:27
epoch [55/200] batch [20/77] time 0.469 (0.452) data 0.339 (0.321) loss_u loss_u 0.9038 (0.9050) acc_u 15.6250 (11.5625) lr 1.6730e-03 eta 0:00:25
epoch [55/200] batch [25/77] time 0.369 (0.445) data 0.238 (0.315) loss_u loss_u 0.8716 (0.9037) acc_u 18.7500 (12.1250) lr 1.6730e-03 eta 0:00:23
epoch [55/200] batch [30/77] time 0.384 (0.445) data 0.253 (0.314) loss_u loss_u 0.9097 (0.9034) acc_u 6.2500 (12.3958) lr 1.6730e-03 eta 0:00:20
epoch [55/200] batch [35/77] time 0.491 (0.445) data 0.359 (0.314) loss_u loss_u 0.9043 (0.9045) acc_u 15.6250 (12.1429) lr 1.6730e-03 eta 0:00:18
epoch [55/200] batch [40/77] time 0.395 (0.445) data 0.263 (0.314) loss_u loss_u 0.9321 (0.9103) acc_u 6.2500 (11.3281) lr 1.6730e-03 eta 0:00:16
epoch [55/200] batch [45/77] time 0.400 (0.446) data 0.269 (0.314) loss_u loss_u 0.9277 (0.9069) acc_u 6.2500 (11.5278) lr 1.6730e-03 eta 0:00:14
epoch [55/200] batch [50/77] time 0.473 (0.445) data 0.342 (0.314) loss_u loss_u 0.9282 (0.9073) acc_u 6.2500 (11.5000) lr 1.6730e-03 eta 0:00:12
epoch [55/200] batch [55/77] time 0.429 (0.450) data 0.298 (0.319) loss_u loss_u 0.9209 (0.9076) acc_u 15.6250 (11.6477) lr 1.6730e-03 eta 0:00:09
epoch [55/200] batch [60/77] time 0.453 (0.452) data 0.320 (0.321) loss_u loss_u 0.9688 (0.9078) acc_u 6.2500 (11.6667) lr 1.6730e-03 eta 0:00:07
epoch [55/200] batch [65/77] time 0.429 (0.452) data 0.297 (0.321) loss_u loss_u 0.9468 (0.9101) acc_u 9.3750 (11.3942) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [70/77] time 0.433 (0.454) data 0.301 (0.322) loss_u loss_u 0.9312 (0.9108) acc_u 9.3750 (11.2500) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [75/77] time 0.529 (0.453) data 0.398 (0.322) loss_u loss_u 0.9146 (0.9109) acc_u 15.6250 (11.4167) lr 1.6730e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2050
confident_label rate tensor(0.2060, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 646
clean true:349
clean false:297
clean_rate:0.5402476780185759
noisy true:737
noisy false:1753
after delete: len(clean_dataset) 646
after delete: len(noisy_dataset) 2490
epoch [56/200] batch [5/20] time 0.537 (0.456) data 0.407 (0.325) loss_x loss_x 1.4941 (1.5613) acc_x 56.2500 (56.8750) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [10/20] time 0.584 (0.455) data 0.453 (0.324) loss_x loss_x 1.8789 (1.5246) acc_x 50.0000 (58.7500) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [15/20] time 0.605 (0.468) data 0.474 (0.337) loss_x loss_x 1.0566 (1.4406) acc_x 71.8750 (61.2500) lr 1.6613e-03 eta 0:00:02
epoch [56/200] batch [20/20] time 0.439 (0.466) data 0.308 (0.335) loss_x loss_x 1.6465 (1.4825) acc_x 50.0000 (59.2188) lr 1.6613e-03 eta 0:00:00
epoch [56/200] batch [5/77] time 0.497 (0.476) data 0.366 (0.345) loss_u loss_u 0.8916 (0.9094) acc_u 12.5000 (14.3750) lr 1.6613e-03 eta 0:00:34
epoch [56/200] batch [10/77] time 0.398 (0.468) data 0.266 (0.337) loss_u loss_u 0.9365 (0.9137) acc_u 12.5000 (13.4375) lr 1.6613e-03 eta 0:00:31
epoch [56/200] batch [15/77] time 0.367 (0.466) data 0.235 (0.335) loss_u loss_u 0.9146 (0.9173) acc_u 12.5000 (12.5000) lr 1.6613e-03 eta 0:00:28
epoch [56/200] batch [20/77] time 0.410 (0.461) data 0.279 (0.330) loss_u loss_u 0.9121 (0.9171) acc_u 9.3750 (12.1875) lr 1.6613e-03 eta 0:00:26
epoch [56/200] batch [25/77] time 0.369 (0.461) data 0.238 (0.329) loss_u loss_u 0.9419 (0.9141) acc_u 9.3750 (12.5000) lr 1.6613e-03 eta 0:00:23
epoch [56/200] batch [30/77] time 0.426 (0.464) data 0.296 (0.333) loss_u loss_u 0.8677 (0.9110) acc_u 15.6250 (12.8125) lr 1.6613e-03 eta 0:00:21
epoch [56/200] batch [35/77] time 0.442 (0.461) data 0.311 (0.330) loss_u loss_u 0.9092 (0.9106) acc_u 12.5000 (12.7679) lr 1.6613e-03 eta 0:00:19
epoch [56/200] batch [40/77] time 0.625 (0.461) data 0.493 (0.330) loss_u loss_u 0.9380 (0.9108) acc_u 12.5000 (12.8906) lr 1.6613e-03 eta 0:00:17
epoch [56/200] batch [45/77] time 0.452 (0.463) data 0.320 (0.332) loss_u loss_u 0.8433 (0.9074) acc_u 21.8750 (13.0556) lr 1.6613e-03 eta 0:00:14
epoch [56/200] batch [50/77] time 0.602 (0.463) data 0.472 (0.332) loss_u loss_u 0.8828 (0.9094) acc_u 18.7500 (12.6875) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [55/77] time 0.619 (0.464) data 0.489 (0.333) loss_u loss_u 0.8359 (0.9077) acc_u 18.7500 (12.8977) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [60/77] time 0.414 (0.463) data 0.283 (0.331) loss_u loss_u 0.8770 (0.9062) acc_u 12.5000 (12.9167) lr 1.6613e-03 eta 0:00:07
epoch [56/200] batch [65/77] time 0.483 (0.464) data 0.352 (0.333) loss_u loss_u 0.9219 (0.9100) acc_u 9.3750 (12.3077) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [70/77] time 0.400 (0.464) data 0.268 (0.333) loss_u loss_u 0.8848 (0.9085) acc_u 18.7500 (12.6786) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [75/77] time 0.606 (0.467) data 0.475 (0.335) loss_u loss_u 0.8687 (0.9086) acc_u 21.8750 (12.6250) lr 1.6613e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2081
confident_label rate tensor(0.2108, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 661
clean true:338
clean false:323
clean_rate:0.5113464447806354
noisy true:717
noisy false:1758
after delete: len(clean_dataset) 661
after delete: len(noisy_dataset) 2475
epoch [57/200] batch [5/20] time 0.377 (0.448) data 0.247 (0.317) loss_x loss_x 1.6133 (1.3809) acc_x 43.7500 (58.1250) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [10/20] time 0.434 (0.454) data 0.304 (0.323) loss_x loss_x 1.2568 (1.4096) acc_x 62.5000 (59.3750) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [15/20] time 0.684 (0.473) data 0.552 (0.342) loss_x loss_x 1.5078 (1.4092) acc_x 43.7500 (59.3750) lr 1.6494e-03 eta 0:00:02
epoch [57/200] batch [20/20] time 0.526 (0.468) data 0.394 (0.337) loss_x loss_x 1.6748 (1.3933) acc_x 59.3750 (60.6250) lr 1.6494e-03 eta 0:00:00
epoch [57/200] batch [5/77] time 0.441 (0.465) data 0.309 (0.334) loss_u loss_u 0.9556 (0.8940) acc_u 3.1250 (11.8750) lr 1.6494e-03 eta 0:00:33
epoch [57/200] batch [10/77] time 0.465 (0.465) data 0.335 (0.333) loss_u loss_u 0.9185 (0.9121) acc_u 12.5000 (10.3125) lr 1.6494e-03 eta 0:00:31
epoch [57/200] batch [15/77] time 0.544 (0.462) data 0.412 (0.331) loss_u loss_u 0.9229 (0.9176) acc_u 3.1250 (8.7500) lr 1.6494e-03 eta 0:00:28
epoch [57/200] batch [20/77] time 0.428 (0.463) data 0.297 (0.332) loss_u loss_u 0.8970 (0.9111) acc_u 12.5000 (10.1562) lr 1.6494e-03 eta 0:00:26
epoch [57/200] batch [25/77] time 0.532 (0.464) data 0.400 (0.333) loss_u loss_u 0.9478 (0.9156) acc_u 6.2500 (9.7500) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [30/77] time 0.382 (0.463) data 0.251 (0.331) loss_u loss_u 0.9321 (0.9137) acc_u 6.2500 (10.4167) lr 1.6494e-03 eta 0:00:21
epoch [57/200] batch [35/77] time 0.467 (0.458) data 0.336 (0.327) loss_u loss_u 0.9019 (0.9148) acc_u 9.3750 (10.4464) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [40/77] time 0.568 (0.459) data 0.437 (0.327) loss_u loss_u 0.9233 (0.9130) acc_u 12.5000 (10.9375) lr 1.6494e-03 eta 0:00:16
epoch [57/200] batch [45/77] time 0.452 (0.458) data 0.321 (0.327) loss_u loss_u 0.9360 (0.9120) acc_u 9.3750 (10.9028) lr 1.6494e-03 eta 0:00:14
epoch [57/200] batch [50/77] time 0.504 (0.458) data 0.373 (0.327) loss_u loss_u 0.9204 (0.9129) acc_u 12.5000 (10.8125) lr 1.6494e-03 eta 0:00:12
epoch [57/200] batch [55/77] time 0.368 (0.456) data 0.237 (0.324) loss_u loss_u 0.9014 (0.9106) acc_u 12.5000 (11.1932) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [60/77] time 0.294 (0.452) data 0.164 (0.321) loss_u loss_u 0.9062 (0.9102) acc_u 9.3750 (11.3542) lr 1.6494e-03 eta 0:00:07
epoch [57/200] batch [65/77] time 0.360 (0.452) data 0.229 (0.321) loss_u loss_u 0.9209 (0.9104) acc_u 6.2500 (11.3462) lr 1.6494e-03 eta 0:00:05
epoch [57/200] batch [70/77] time 0.589 (0.452) data 0.457 (0.321) loss_u loss_u 0.9292 (0.9111) acc_u 6.2500 (11.1161) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [75/77] time 0.525 (0.455) data 0.392 (0.323) loss_u loss_u 0.8892 (0.9100) acc_u 9.3750 (11.2500) lr 1.6494e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2087
confident_label rate tensor(0.2063, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 647
clean true:333
clean false:314
clean_rate:0.5146831530139103
noisy true:716
noisy false:1773
after delete: len(clean_dataset) 647
after delete: len(noisy_dataset) 2489
epoch [58/200] batch [5/20] time 0.458 (0.469) data 0.326 (0.338) loss_x loss_x 1.2715 (1.3351) acc_x 65.6250 (65.6250) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [10/20] time 0.549 (0.480) data 0.418 (0.349) loss_x loss_x 0.9307 (1.3595) acc_x 71.8750 (65.3125) lr 1.6374e-03 eta 0:00:04
epoch [58/200] batch [15/20] time 0.602 (0.478) data 0.470 (0.347) loss_x loss_x 1.3525 (1.3241) acc_x 53.1250 (63.9583) lr 1.6374e-03 eta 0:00:02
epoch [58/200] batch [20/20] time 0.651 (0.487) data 0.521 (0.356) loss_x loss_x 2.1992 (1.4031) acc_x 56.2500 (62.6562) lr 1.6374e-03 eta 0:00:00
epoch [58/200] batch [5/77] time 0.485 (0.482) data 0.354 (0.351) loss_u loss_u 0.9214 (0.8799) acc_u 9.3750 (16.8750) lr 1.6374e-03 eta 0:00:34
epoch [58/200] batch [10/77] time 0.553 (0.482) data 0.421 (0.351) loss_u loss_u 0.9092 (0.9016) acc_u 12.5000 (12.8125) lr 1.6374e-03 eta 0:00:32
epoch [58/200] batch [15/77] time 0.378 (0.469) data 0.246 (0.338) loss_u loss_u 0.8599 (0.8990) acc_u 15.6250 (13.1250) lr 1.6374e-03 eta 0:00:29
epoch [58/200] batch [20/77] time 0.472 (0.467) data 0.340 (0.336) loss_u loss_u 0.8979 (0.8991) acc_u 12.5000 (13.1250) lr 1.6374e-03 eta 0:00:26
epoch [58/200] batch [25/77] time 0.520 (0.470) data 0.389 (0.339) loss_u loss_u 0.9370 (0.9035) acc_u 9.3750 (12.7500) lr 1.6374e-03 eta 0:00:24
epoch [58/200] batch [30/77] time 0.424 (0.470) data 0.293 (0.339) loss_u loss_u 0.9268 (0.9069) acc_u 9.3750 (12.1875) lr 1.6374e-03 eta 0:00:22
epoch [58/200] batch [35/77] time 0.486 (0.472) data 0.356 (0.341) loss_u loss_u 0.9097 (0.9089) acc_u 15.6250 (11.9643) lr 1.6374e-03 eta 0:00:19
epoch [58/200] batch [40/77] time 0.405 (0.469) data 0.274 (0.338) loss_u loss_u 0.9214 (0.9103) acc_u 12.5000 (11.9531) lr 1.6374e-03 eta 0:00:17
epoch [58/200] batch [45/77] time 0.458 (0.465) data 0.327 (0.334) loss_u loss_u 0.9482 (0.9124) acc_u 3.1250 (11.7361) lr 1.6374e-03 eta 0:00:14
epoch [58/200] batch [50/77] time 0.418 (0.464) data 0.287 (0.333) loss_u loss_u 0.8872 (0.9104) acc_u 12.5000 (12.0000) lr 1.6374e-03 eta 0:00:12
epoch [58/200] batch [55/77] time 0.438 (0.464) data 0.307 (0.333) loss_u loss_u 0.9248 (0.9115) acc_u 9.3750 (11.8182) lr 1.6374e-03 eta 0:00:10
epoch [58/200] batch [60/77] time 0.713 (0.467) data 0.583 (0.336) loss_u loss_u 0.8774 (0.9109) acc_u 21.8750 (11.8229) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [65/77] time 0.483 (0.465) data 0.353 (0.334) loss_u loss_u 0.8989 (0.9099) acc_u 12.5000 (11.8750) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [70/77] time 0.434 (0.466) data 0.303 (0.335) loss_u loss_u 0.9009 (0.9107) acc_u 9.3750 (11.7411) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [75/77] time 0.346 (0.465) data 0.214 (0.334) loss_u loss_u 0.9634 (0.9113) acc_u 3.1250 (11.6667) lr 1.6374e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2061
confident_label rate tensor(0.2092, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 656
clean true:342
clean false:314
clean_rate:0.5213414634146342
noisy true:733
noisy false:1747
after delete: len(clean_dataset) 656
after delete: len(noisy_dataset) 2480
epoch [59/200] batch [5/20] time 0.506 (0.454) data 0.374 (0.323) loss_x loss_x 1.6328 (1.3992) acc_x 53.1250 (61.2500) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [10/20] time 0.545 (0.493) data 0.415 (0.362) loss_x loss_x 1.1934 (1.4246) acc_x 59.3750 (60.9375) lr 1.6252e-03 eta 0:00:04
epoch [59/200] batch [15/20] time 0.477 (0.483) data 0.346 (0.352) loss_x loss_x 1.3389 (1.4329) acc_x 62.5000 (62.0833) lr 1.6252e-03 eta 0:00:02
epoch [59/200] batch [20/20] time 0.445 (0.474) data 0.314 (0.343) loss_x loss_x 1.2139 (1.4043) acc_x 59.3750 (62.6562) lr 1.6252e-03 eta 0:00:00
epoch [59/200] batch [5/77] time 0.389 (0.466) data 0.257 (0.335) loss_u loss_u 0.9438 (0.9261) acc_u 3.1250 (6.8750) lr 1.6252e-03 eta 0:00:33
epoch [59/200] batch [10/77] time 0.417 (0.471) data 0.284 (0.340) loss_u loss_u 0.9233 (0.9225) acc_u 15.6250 (8.1250) lr 1.6252e-03 eta 0:00:31
epoch [59/200] batch [15/77] time 0.358 (0.463) data 0.226 (0.332) loss_u loss_u 0.9526 (0.9190) acc_u 3.1250 (8.7500) lr 1.6252e-03 eta 0:00:28
epoch [59/200] batch [20/77] time 0.378 (0.470) data 0.247 (0.338) loss_u loss_u 0.9521 (0.9152) acc_u 6.2500 (9.3750) lr 1.6252e-03 eta 0:00:26
epoch [59/200] batch [25/77] time 0.369 (0.463) data 0.237 (0.332) loss_u loss_u 0.9312 (0.9142) acc_u 9.3750 (10.0000) lr 1.6252e-03 eta 0:00:24
epoch [59/200] batch [30/77] time 0.540 (0.465) data 0.408 (0.334) loss_u loss_u 0.9028 (0.9112) acc_u 12.5000 (10.3125) lr 1.6252e-03 eta 0:00:21
epoch [59/200] batch [35/77] time 0.495 (0.470) data 0.364 (0.338) loss_u loss_u 0.9312 (0.9081) acc_u 6.2500 (10.7143) lr 1.6252e-03 eta 0:00:19
epoch [59/200] batch [40/77] time 0.392 (0.470) data 0.260 (0.339) loss_u loss_u 0.9507 (0.9090) acc_u 9.3750 (10.7812) lr 1.6252e-03 eta 0:00:17
epoch [59/200] batch [45/77] time 0.379 (0.468) data 0.248 (0.336) loss_u loss_u 0.9082 (0.9087) acc_u 12.5000 (10.7639) lr 1.6252e-03 eta 0:00:14
epoch [59/200] batch [50/77] time 0.452 (0.467) data 0.322 (0.335) loss_u loss_u 0.9224 (0.9076) acc_u 9.3750 (11.0000) lr 1.6252e-03 eta 0:00:12
epoch [59/200] batch [55/77] time 0.494 (0.469) data 0.362 (0.338) loss_u loss_u 0.9072 (0.9057) acc_u 9.3750 (11.5341) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [60/77] time 0.687 (0.472) data 0.556 (0.340) loss_u loss_u 0.9302 (0.9074) acc_u 6.2500 (11.3021) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [65/77] time 0.522 (0.473) data 0.391 (0.341) loss_u loss_u 0.8853 (0.9075) acc_u 12.5000 (11.2981) lr 1.6252e-03 eta 0:00:05
epoch [59/200] batch [70/77] time 0.338 (0.472) data 0.206 (0.341) loss_u loss_u 0.9653 (0.9093) acc_u 6.2500 (11.0714) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [75/77] time 0.422 (0.470) data 0.291 (0.338) loss_u loss_u 0.9253 (0.9099) acc_u 12.5000 (11.1667) lr 1.6252e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2038
confident_label rate tensor(0.2162, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 678
clean true:348
clean false:330
clean_rate:0.5132743362831859
noisy true:750
noisy false:1708
after delete: len(clean_dataset) 678
after delete: len(noisy_dataset) 2458
epoch [60/200] batch [5/21] time 0.483 (0.463) data 0.352 (0.332) loss_x loss_x 1.9346 (1.5059) acc_x 46.8750 (62.5000) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [10/21] time 0.632 (0.462) data 0.501 (0.332) loss_x loss_x 1.2705 (1.4752) acc_x 62.5000 (62.5000) lr 1.6129e-03 eta 0:00:05
epoch [60/200] batch [15/21] time 0.456 (0.465) data 0.326 (0.334) loss_x loss_x 2.1426 (1.5072) acc_x 50.0000 (61.4583) lr 1.6129e-03 eta 0:00:02
epoch [60/200] batch [20/21] time 0.447 (0.464) data 0.317 (0.334) loss_x loss_x 1.1152 (1.5361) acc_x 71.8750 (60.9375) lr 1.6129e-03 eta 0:00:00
epoch [60/200] batch [5/76] time 0.466 (0.454) data 0.336 (0.323) loss_u loss_u 0.9448 (0.9134) acc_u 6.2500 (11.8750) lr 1.6129e-03 eta 0:00:32
epoch [60/200] batch [10/76] time 0.480 (0.451) data 0.348 (0.320) loss_u loss_u 0.8521 (0.8967) acc_u 25.0000 (15.0000) lr 1.6129e-03 eta 0:00:29
epoch [60/200] batch [15/76] time 0.395 (0.447) data 0.264 (0.316) loss_u loss_u 0.9438 (0.9049) acc_u 6.2500 (13.5417) lr 1.6129e-03 eta 0:00:27
epoch [60/200] batch [20/76] time 0.385 (0.448) data 0.254 (0.317) loss_u loss_u 0.9033 (0.9066) acc_u 12.5000 (12.5000) lr 1.6129e-03 eta 0:00:25
epoch [60/200] batch [25/76] time 0.671 (0.448) data 0.541 (0.317) loss_u loss_u 0.9355 (0.9009) acc_u 6.2500 (13.2500) lr 1.6129e-03 eta 0:00:22
epoch [60/200] batch [30/76] time 0.395 (0.450) data 0.263 (0.318) loss_u loss_u 0.9302 (0.8976) acc_u 9.3750 (13.5417) lr 1.6129e-03 eta 0:00:20
epoch [60/200] batch [35/76] time 0.639 (0.448) data 0.508 (0.317) loss_u loss_u 0.8340 (0.8988) acc_u 21.8750 (13.4821) lr 1.6129e-03 eta 0:00:18
epoch [60/200] batch [40/76] time 0.497 (0.445) data 0.365 (0.314) loss_u loss_u 0.9434 (0.9023) acc_u 3.1250 (12.9688) lr 1.6129e-03 eta 0:00:16
epoch [60/200] batch [45/76] time 0.414 (0.444) data 0.283 (0.313) loss_u loss_u 0.9556 (0.9033) acc_u 0.0000 (12.5694) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [50/76] time 0.392 (0.451) data 0.260 (0.320) loss_u loss_u 0.8882 (0.9042) acc_u 15.6250 (12.4375) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [55/76] time 0.409 (0.448) data 0.278 (0.317) loss_u loss_u 0.8716 (0.9045) acc_u 21.8750 (12.5568) lr 1.6129e-03 eta 0:00:09
epoch [60/200] batch [60/76] time 0.355 (0.447) data 0.224 (0.316) loss_u loss_u 0.9375 (0.9055) acc_u 9.3750 (12.3438) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [65/76] time 0.399 (0.447) data 0.266 (0.316) loss_u loss_u 0.9062 (0.9064) acc_u 12.5000 (12.2115) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [70/76] time 0.576 (0.452) data 0.444 (0.321) loss_u loss_u 0.8770 (0.9048) acc_u 18.7500 (12.4107) lr 1.6129e-03 eta 0:00:02
epoch [60/200] batch [75/76] time 0.477 (0.456) data 0.346 (0.325) loss_u loss_u 0.9482 (0.9057) acc_u 6.2500 (12.2083) lr 1.6129e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2116
confident_label rate tensor(0.2172, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 681
clean true:336
clean false:345
clean_rate:0.4933920704845815
noisy true:684
noisy false:1771
after delete: len(clean_dataset) 681
after delete: len(noisy_dataset) 2455
epoch [61/200] batch [5/21] time 0.430 (0.533) data 0.300 (0.402) loss_x loss_x 1.4805 (1.5578) acc_x 65.6250 (58.1250) lr 1.6004e-03 eta 0:00:08
epoch [61/200] batch [10/21] time 0.471 (0.507) data 0.341 (0.376) loss_x loss_x 1.4482 (1.4774) acc_x 53.1250 (60.0000) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [15/21] time 0.551 (0.496) data 0.419 (0.365) loss_x loss_x 1.2119 (1.4424) acc_x 56.2500 (62.5000) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [20/21] time 0.415 (0.487) data 0.284 (0.356) loss_x loss_x 1.4697 (1.4329) acc_x 59.3750 (62.9688) lr 1.6004e-03 eta 0:00:00
epoch [61/200] batch [5/76] time 0.421 (0.470) data 0.290 (0.339) loss_u loss_u 0.8877 (0.9186) acc_u 18.7500 (13.1250) lr 1.6004e-03 eta 0:00:33
epoch [61/200] batch [10/76] time 0.370 (0.462) data 0.238 (0.331) loss_u loss_u 0.9087 (0.9250) acc_u 12.5000 (11.5625) lr 1.6004e-03 eta 0:00:30
epoch [61/200] batch [15/76] time 0.461 (0.464) data 0.329 (0.333) loss_u loss_u 0.9146 (0.9203) acc_u 12.5000 (12.7083) lr 1.6004e-03 eta 0:00:28
epoch [61/200] batch [20/76] time 0.490 (0.463) data 0.360 (0.332) loss_u loss_u 0.8374 (0.9085) acc_u 25.0000 (14.2188) lr 1.6004e-03 eta 0:00:25
epoch [61/200] batch [25/76] time 0.392 (0.460) data 0.261 (0.328) loss_u loss_u 0.9053 (0.9138) acc_u 6.2500 (12.5000) lr 1.6004e-03 eta 0:00:23
epoch [61/200] batch [30/76] time 0.423 (0.460) data 0.290 (0.328) loss_u loss_u 0.9321 (0.9139) acc_u 6.2500 (11.9792) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [35/76] time 0.430 (0.463) data 0.298 (0.332) loss_u loss_u 0.8926 (0.9168) acc_u 15.6250 (11.5179) lr 1.6004e-03 eta 0:00:18
epoch [61/200] batch [40/76] time 0.509 (0.467) data 0.377 (0.336) loss_u loss_u 0.9106 (0.9155) acc_u 9.3750 (11.7188) lr 1.6004e-03 eta 0:00:16
epoch [61/200] batch [45/76] time 0.401 (0.467) data 0.270 (0.335) loss_u loss_u 0.8545 (0.9156) acc_u 15.6250 (11.5278) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [50/76] time 0.548 (0.468) data 0.417 (0.336) loss_u loss_u 0.8735 (0.9124) acc_u 12.5000 (11.6250) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [55/76] time 0.500 (0.469) data 0.369 (0.338) loss_u loss_u 0.9092 (0.9138) acc_u 12.5000 (11.4205) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [60/76] time 0.567 (0.474) data 0.436 (0.343) loss_u loss_u 0.9004 (0.9112) acc_u 15.6250 (11.6667) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [65/76] time 0.531 (0.473) data 0.399 (0.342) loss_u loss_u 0.9077 (0.9128) acc_u 15.6250 (11.4904) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [70/76] time 0.366 (0.472) data 0.236 (0.340) loss_u loss_u 0.8721 (0.9108) acc_u 15.6250 (11.8750) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [75/76] time 0.363 (0.470) data 0.231 (0.339) loss_u loss_u 0.8813 (0.9116) acc_u 18.7500 (11.7500) lr 1.6004e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2144
confident_label rate tensor(0.2044, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 641
clean true:305
clean false:336
clean_rate:0.47581903276131043
noisy true:687
noisy false:1808
after delete: len(clean_dataset) 641
after delete: len(noisy_dataset) 2495
epoch [62/200] batch [5/20] time 0.594 (0.506) data 0.463 (0.375) loss_x loss_x 1.5410 (1.4316) acc_x 59.3750 (63.1250) lr 1.5878e-03 eta 0:00:07
epoch [62/200] batch [10/20] time 0.438 (0.453) data 0.307 (0.322) loss_x loss_x 1.0605 (1.4320) acc_x 68.7500 (64.6875) lr 1.5878e-03 eta 0:00:04
epoch [62/200] batch [15/20] time 0.560 (0.463) data 0.430 (0.332) loss_x loss_x 1.1670 (1.3567) acc_x 68.7500 (66.4583) lr 1.5878e-03 eta 0:00:02
epoch [62/200] batch [20/20] time 0.587 (0.466) data 0.456 (0.336) loss_x loss_x 1.5293 (1.4184) acc_x 59.3750 (64.0625) lr 1.5878e-03 eta 0:00:00
epoch [62/200] batch [5/77] time 0.500 (0.461) data 0.369 (0.330) loss_u loss_u 0.8696 (0.8910) acc_u 15.6250 (13.7500) lr 1.5878e-03 eta 0:00:33
epoch [62/200] batch [10/77] time 0.424 (0.463) data 0.291 (0.332) loss_u loss_u 0.8789 (0.8941) acc_u 15.6250 (12.5000) lr 1.5878e-03 eta 0:00:31
epoch [62/200] batch [15/77] time 0.449 (0.477) data 0.317 (0.346) loss_u loss_u 0.9189 (0.8950) acc_u 12.5000 (12.9167) lr 1.5878e-03 eta 0:00:29
epoch [62/200] batch [20/77] time 0.344 (0.471) data 0.212 (0.339) loss_u loss_u 0.8916 (0.8985) acc_u 15.6250 (13.2812) lr 1.5878e-03 eta 0:00:26
epoch [62/200] batch [25/77] time 0.573 (0.475) data 0.441 (0.344) loss_u loss_u 0.8721 (0.9017) acc_u 15.6250 (12.8750) lr 1.5878e-03 eta 0:00:24
epoch [62/200] batch [30/77] time 0.354 (0.466) data 0.224 (0.335) loss_u loss_u 0.9014 (0.9031) acc_u 12.5000 (12.5000) lr 1.5878e-03 eta 0:00:21
epoch [62/200] batch [35/77] time 0.364 (0.460) data 0.233 (0.329) loss_u loss_u 0.9194 (0.9064) acc_u 9.3750 (12.0536) lr 1.5878e-03 eta 0:00:19
epoch [62/200] batch [40/77] time 0.460 (0.457) data 0.328 (0.326) loss_u loss_u 0.8647 (0.9058) acc_u 15.6250 (11.7969) lr 1.5878e-03 eta 0:00:16
epoch [62/200] batch [45/77] time 0.397 (0.455) data 0.265 (0.324) loss_u loss_u 0.9600 (0.9067) acc_u 9.3750 (12.0833) lr 1.5878e-03 eta 0:00:14
epoch [62/200] batch [50/77] time 0.339 (0.453) data 0.208 (0.322) loss_u loss_u 0.9473 (0.9056) acc_u 6.2500 (12.4375) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [55/77] time 0.352 (0.455) data 0.220 (0.324) loss_u loss_u 0.9199 (0.9059) acc_u 9.3750 (12.3295) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [60/77] time 0.590 (0.458) data 0.458 (0.327) loss_u loss_u 0.8271 (0.9050) acc_u 21.8750 (12.5521) lr 1.5878e-03 eta 0:00:07
epoch [62/200] batch [65/77] time 0.590 (0.460) data 0.458 (0.329) loss_u loss_u 0.8765 (0.9035) acc_u 12.5000 (12.6442) lr 1.5878e-03 eta 0:00:05
epoch [62/200] batch [70/77] time 0.434 (0.458) data 0.302 (0.327) loss_u loss_u 0.8647 (0.9037) acc_u 18.7500 (12.4554) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [75/77] time 0.445 (0.458) data 0.313 (0.326) loss_u loss_u 0.8340 (0.9027) acc_u 18.7500 (12.5000) lr 1.5878e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2033
confident_label rate tensor(0.2098, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 658
clean true:343
clean false:315
clean_rate:0.5212765957446809
noisy true:760
noisy false:1718
after delete: len(clean_dataset) 658
after delete: len(noisy_dataset) 2478
epoch [63/200] batch [5/20] time 0.454 (0.417) data 0.324 (0.286) loss_x loss_x 1.5391 (1.2464) acc_x 56.2500 (65.6250) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [10/20] time 0.609 (0.455) data 0.478 (0.324) loss_x loss_x 0.8013 (1.2767) acc_x 75.0000 (65.9375) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [15/20] time 0.525 (0.448) data 0.395 (0.317) loss_x loss_x 1.2832 (1.3458) acc_x 62.5000 (65.4167) lr 1.5750e-03 eta 0:00:02
epoch [63/200] batch [20/20] time 0.574 (0.459) data 0.443 (0.328) loss_x loss_x 1.7002 (1.3635) acc_x 53.1250 (63.7500) lr 1.5750e-03 eta 0:00:00
epoch [63/200] batch [5/77] time 0.397 (0.460) data 0.265 (0.329) loss_u loss_u 0.9268 (0.9137) acc_u 6.2500 (10.6250) lr 1.5750e-03 eta 0:00:33
epoch [63/200] batch [10/77] time 0.392 (0.460) data 0.260 (0.329) loss_u loss_u 0.8770 (0.9056) acc_u 12.5000 (12.1875) lr 1.5750e-03 eta 0:00:30
epoch [63/200] batch [15/77] time 0.381 (0.463) data 0.250 (0.331) loss_u loss_u 0.8613 (0.9086) acc_u 18.7500 (11.8750) lr 1.5750e-03 eta 0:00:28
epoch [63/200] batch [20/77] time 0.584 (0.460) data 0.453 (0.329) loss_u loss_u 0.9497 (0.9066) acc_u 9.3750 (12.6562) lr 1.5750e-03 eta 0:00:26
epoch [63/200] batch [25/77] time 0.450 (0.464) data 0.319 (0.333) loss_u loss_u 0.8882 (0.9052) acc_u 15.6250 (12.6250) lr 1.5750e-03 eta 0:00:24
epoch [63/200] batch [30/77] time 0.647 (0.472) data 0.515 (0.341) loss_u loss_u 0.9487 (0.9068) acc_u 9.3750 (12.5000) lr 1.5750e-03 eta 0:00:22
epoch [63/200] batch [35/77] time 0.439 (0.470) data 0.307 (0.339) loss_u loss_u 0.8813 (0.9047) acc_u 15.6250 (12.6786) lr 1.5750e-03 eta 0:00:19
epoch [63/200] batch [40/77] time 0.465 (0.470) data 0.333 (0.339) loss_u loss_u 0.9453 (0.9037) acc_u 9.3750 (12.8125) lr 1.5750e-03 eta 0:00:17
epoch [63/200] batch [45/77] time 0.562 (0.470) data 0.430 (0.338) loss_u loss_u 0.9189 (0.9016) acc_u 9.3750 (13.1250) lr 1.5750e-03 eta 0:00:15
epoch [63/200] batch [50/77] time 0.430 (0.468) data 0.298 (0.336) loss_u loss_u 0.8428 (0.8991) acc_u 15.6250 (13.1875) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [55/77] time 0.598 (0.469) data 0.467 (0.337) loss_u loss_u 0.8633 (0.8977) acc_u 15.6250 (13.3523) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [60/77] time 0.370 (0.464) data 0.239 (0.333) loss_u loss_u 0.9375 (0.9005) acc_u 3.1250 (12.7604) lr 1.5750e-03 eta 0:00:07
epoch [63/200] batch [65/77] time 0.475 (0.466) data 0.344 (0.334) loss_u loss_u 0.9292 (0.9011) acc_u 12.5000 (12.6442) lr 1.5750e-03 eta 0:00:05
epoch [63/200] batch [70/77] time 0.539 (0.465) data 0.408 (0.334) loss_u loss_u 0.9062 (0.9027) acc_u 12.5000 (12.5000) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [75/77] time 0.459 (0.464) data 0.329 (0.333) loss_u loss_u 0.9253 (0.9035) acc_u 12.5000 (12.4167) lr 1.5750e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2013
confident_label rate tensor(0.2188, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 686
clean true:357
clean false:329
clean_rate:0.5204081632653061
noisy true:766
noisy false:1684
after delete: len(clean_dataset) 686
after delete: len(noisy_dataset) 2450
epoch [64/200] batch [5/21] time 0.468 (0.521) data 0.337 (0.390) loss_x loss_x 1.7676 (1.3000) acc_x 62.5000 (66.8750) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [10/21] time 0.436 (0.499) data 0.306 (0.369) loss_x loss_x 1.8896 (1.3785) acc_x 50.0000 (64.3750) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [15/21] time 0.382 (0.490) data 0.251 (0.360) loss_x loss_x 1.8135 (1.3816) acc_x 65.6250 (63.7500) lr 1.5621e-03 eta 0:00:02
epoch [64/200] batch [20/21] time 0.445 (0.489) data 0.315 (0.358) loss_x loss_x 0.9922 (1.3710) acc_x 71.8750 (63.9062) lr 1.5621e-03 eta 0:00:00
epoch [64/200] batch [5/76] time 0.470 (0.479) data 0.340 (0.348) loss_u loss_u 0.9287 (0.8988) acc_u 6.2500 (13.1250) lr 1.5621e-03 eta 0:00:34
epoch [64/200] batch [10/76] time 0.332 (0.465) data 0.200 (0.334) loss_u loss_u 0.9600 (0.9042) acc_u 3.1250 (12.8125) lr 1.5621e-03 eta 0:00:30
epoch [64/200] batch [15/76] time 0.519 (0.465) data 0.388 (0.334) loss_u loss_u 0.9600 (0.9095) acc_u 9.3750 (12.5000) lr 1.5621e-03 eta 0:00:28
epoch [64/200] batch [20/76] time 0.461 (0.466) data 0.330 (0.334) loss_u loss_u 0.8721 (0.9050) acc_u 15.6250 (12.6562) lr 1.5621e-03 eta 0:00:26
epoch [64/200] batch [25/76] time 0.661 (0.468) data 0.529 (0.337) loss_u loss_u 0.9067 (0.9049) acc_u 12.5000 (12.7500) lr 1.5621e-03 eta 0:00:23
epoch [64/200] batch [30/76] time 0.430 (0.465) data 0.299 (0.333) loss_u loss_u 0.9834 (0.9085) acc_u 3.1250 (12.2917) lr 1.5621e-03 eta 0:00:21
epoch [64/200] batch [35/76] time 0.563 (0.462) data 0.432 (0.331) loss_u loss_u 0.8813 (0.9084) acc_u 15.6250 (12.2321) lr 1.5621e-03 eta 0:00:18
epoch [64/200] batch [40/76] time 0.573 (0.471) data 0.439 (0.340) loss_u loss_u 0.9521 (0.9031) acc_u 3.1250 (13.0469) lr 1.5621e-03 eta 0:00:16
epoch [64/200] batch [45/76] time 0.440 (0.474) data 0.307 (0.342) loss_u loss_u 0.9302 (0.9048) acc_u 12.5000 (13.0556) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [50/76] time 0.439 (0.474) data 0.307 (0.343) loss_u loss_u 0.9639 (0.9046) acc_u 3.1250 (13.0000) lr 1.5621e-03 eta 0:00:12
epoch [64/200] batch [55/76] time 0.465 (0.477) data 0.334 (0.345) loss_u loss_u 0.9307 (0.9049) acc_u 6.2500 (12.9545) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [60/76] time 0.389 (0.476) data 0.258 (0.344) loss_u loss_u 0.9507 (0.9071) acc_u 6.2500 (12.7604) lr 1.5621e-03 eta 0:00:07
epoch [64/200] batch [65/76] time 0.356 (0.474) data 0.224 (0.343) loss_u loss_u 0.9380 (0.9092) acc_u 9.3750 (12.4038) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [70/76] time 0.497 (0.475) data 0.366 (0.343) loss_u loss_u 0.8696 (0.9096) acc_u 15.6250 (12.3214) lr 1.5621e-03 eta 0:00:02
epoch [64/200] batch [75/76] time 0.470 (0.474) data 0.339 (0.342) loss_u loss_u 0.8652 (0.9111) acc_u 21.8750 (12.0833) lr 1.5621e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2029
confident_label rate tensor(0.2140, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 671
clean true:348
clean false:323
clean_rate:0.518628912071535
noisy true:759
noisy false:1706
after delete: len(clean_dataset) 671
after delete: len(noisy_dataset) 2465
epoch [65/200] batch [5/20] time 0.488 (0.496) data 0.358 (0.365) loss_x loss_x 1.3730 (1.4108) acc_x 59.3750 (64.3750) lr 1.5490e-03 eta 0:00:07
epoch [65/200] batch [10/20] time 0.407 (0.468) data 0.278 (0.338) loss_x loss_x 1.0527 (1.4280) acc_x 68.7500 (63.4375) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [15/20] time 0.431 (0.474) data 0.300 (0.343) loss_x loss_x 1.1074 (1.4021) acc_x 65.6250 (64.1667) lr 1.5490e-03 eta 0:00:02
epoch [65/200] batch [20/20] time 0.576 (0.463) data 0.445 (0.333) loss_x loss_x 1.1543 (1.3831) acc_x 62.5000 (64.2188) lr 1.5490e-03 eta 0:00:00
epoch [65/200] batch [5/77] time 0.497 (0.463) data 0.362 (0.332) loss_u loss_u 0.9404 (0.9279) acc_u 12.5000 (8.1250) lr 1.5490e-03 eta 0:00:33
epoch [65/200] batch [10/77] time 0.390 (0.464) data 0.257 (0.333) loss_u loss_u 0.8667 (0.9209) acc_u 18.7500 (10.3125) lr 1.5490e-03 eta 0:00:31
epoch [65/200] batch [15/77] time 0.414 (0.466) data 0.282 (0.334) loss_u loss_u 0.9233 (0.9220) acc_u 6.2500 (10.0000) lr 1.5490e-03 eta 0:00:28
epoch [65/200] batch [20/77] time 0.393 (0.461) data 0.263 (0.330) loss_u loss_u 0.9121 (0.9201) acc_u 12.5000 (10.7812) lr 1.5490e-03 eta 0:00:26
epoch [65/200] batch [25/77] time 0.444 (0.456) data 0.313 (0.325) loss_u loss_u 0.8770 (0.9156) acc_u 12.5000 (11.0000) lr 1.5490e-03 eta 0:00:23
epoch [65/200] batch [30/77] time 0.330 (0.457) data 0.199 (0.326) loss_u loss_u 0.8955 (0.9158) acc_u 9.3750 (10.7292) lr 1.5490e-03 eta 0:00:21
epoch [65/200] batch [35/77] time 0.446 (0.450) data 0.315 (0.319) loss_u loss_u 0.8945 (0.9161) acc_u 9.3750 (10.5357) lr 1.5490e-03 eta 0:00:18
epoch [65/200] batch [40/77] time 0.587 (0.453) data 0.453 (0.322) loss_u loss_u 0.9199 (0.9166) acc_u 12.5000 (10.5469) lr 1.5490e-03 eta 0:00:16
epoch [65/200] batch [45/77] time 0.392 (0.460) data 0.261 (0.329) loss_u loss_u 0.8862 (0.9139) acc_u 12.5000 (10.6250) lr 1.5490e-03 eta 0:00:14
epoch [65/200] batch [50/77] time 0.456 (0.458) data 0.323 (0.327) loss_u loss_u 0.9590 (0.9143) acc_u 6.2500 (10.5625) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [55/77] time 0.560 (0.461) data 0.428 (0.330) loss_u loss_u 0.8994 (0.9124) acc_u 15.6250 (11.0227) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [60/77] time 0.581 (0.463) data 0.450 (0.331) loss_u loss_u 0.8647 (0.9092) acc_u 15.6250 (11.6146) lr 1.5490e-03 eta 0:00:07
epoch [65/200] batch [65/77] time 0.576 (0.462) data 0.444 (0.330) loss_u loss_u 0.8921 (0.9065) acc_u 12.5000 (11.8750) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [70/77] time 0.403 (0.459) data 0.271 (0.328) loss_u loss_u 0.9404 (0.9079) acc_u 6.2500 (11.5625) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [75/77] time 0.428 (0.461) data 0.298 (0.330) loss_u loss_u 0.9233 (0.9080) acc_u 12.5000 (11.5417) lr 1.5490e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2026
confident_label rate tensor(0.2168, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 680
clean true:353
clean false:327
clean_rate:0.5191176470588236
noisy true:757
noisy false:1699
after delete: len(clean_dataset) 680
after delete: len(noisy_dataset) 2456
epoch [66/200] batch [5/21] time 0.471 (0.503) data 0.341 (0.372) loss_x loss_x 0.9805 (1.1147) acc_x 68.7500 (71.2500) lr 1.5358e-03 eta 0:00:08
epoch [66/200] batch [10/21] time 0.496 (0.510) data 0.366 (0.380) loss_x loss_x 1.1777 (1.2683) acc_x 75.0000 (68.1250) lr 1.5358e-03 eta 0:00:05
epoch [66/200] batch [15/21] time 0.497 (0.506) data 0.367 (0.375) loss_x loss_x 1.1348 (1.3028) acc_x 75.0000 (66.0417) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [20/21] time 0.475 (0.490) data 0.344 (0.359) loss_x loss_x 1.9707 (1.3809) acc_x 56.2500 (65.0000) lr 1.5358e-03 eta 0:00:00
epoch [66/200] batch [5/76] time 0.466 (0.478) data 0.332 (0.347) loss_u loss_u 0.9692 (0.9226) acc_u 3.1250 (8.7500) lr 1.5358e-03 eta 0:00:33
epoch [66/200] batch [10/76] time 0.412 (0.473) data 0.281 (0.342) loss_u loss_u 0.8696 (0.9169) acc_u 28.1250 (11.8750) lr 1.5358e-03 eta 0:00:31
epoch [66/200] batch [15/76] time 0.378 (0.468) data 0.246 (0.337) loss_u loss_u 0.9731 (0.9211) acc_u 0.0000 (10.2083) lr 1.5358e-03 eta 0:00:28
epoch [66/200] batch [20/76] time 0.593 (0.465) data 0.461 (0.334) loss_u loss_u 0.8467 (0.9174) acc_u 18.7500 (10.3125) lr 1.5358e-03 eta 0:00:26
epoch [66/200] batch [25/76] time 0.441 (0.463) data 0.310 (0.332) loss_u loss_u 0.8921 (0.9184) acc_u 15.6250 (10.1250) lr 1.5358e-03 eta 0:00:23
epoch [66/200] batch [30/76] time 0.490 (0.462) data 0.358 (0.331) loss_u loss_u 0.9512 (0.9166) acc_u 6.2500 (10.4167) lr 1.5358e-03 eta 0:00:21
epoch [66/200] batch [35/76] time 0.474 (0.462) data 0.341 (0.331) loss_u loss_u 0.8882 (0.9171) acc_u 15.6250 (10.3571) lr 1.5358e-03 eta 0:00:18
epoch [66/200] batch [40/76] time 0.490 (0.461) data 0.357 (0.330) loss_u loss_u 0.9521 (0.9162) acc_u 9.3750 (10.5469) lr 1.5358e-03 eta 0:00:16
epoch [66/200] batch [45/76] time 0.489 (0.464) data 0.358 (0.332) loss_u loss_u 0.8936 (0.9154) acc_u 12.5000 (10.7639) lr 1.5358e-03 eta 0:00:14
epoch [66/200] batch [50/76] time 0.453 (0.466) data 0.322 (0.335) loss_u loss_u 0.9390 (0.9140) acc_u 12.5000 (11.2500) lr 1.5358e-03 eta 0:00:12
epoch [66/200] batch [55/76] time 0.390 (0.466) data 0.258 (0.334) loss_u loss_u 0.9502 (0.9133) acc_u 6.2500 (11.2500) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [60/76] time 0.505 (0.464) data 0.374 (0.333) loss_u loss_u 0.8809 (0.9116) acc_u 6.2500 (11.3542) lr 1.5358e-03 eta 0:00:07
epoch [66/200] batch [65/76] time 0.443 (0.464) data 0.311 (0.332) loss_u loss_u 0.9541 (0.9127) acc_u 3.1250 (11.2019) lr 1.5358e-03 eta 0:00:05
epoch [66/200] batch [70/76] time 0.451 (0.462) data 0.321 (0.330) loss_u loss_u 0.8857 (0.9121) acc_u 12.5000 (11.2946) lr 1.5358e-03 eta 0:00:02
epoch [66/200] batch [75/76] time 0.489 (0.462) data 0.357 (0.331) loss_u loss_u 0.8848 (0.9116) acc_u 12.5000 (11.3750) lr 1.5358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2060
confident_label rate tensor(0.2226, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 698
clean true:342
clean false:356
clean_rate:0.4899713467048711
noisy true:734
noisy false:1704
after delete: len(clean_dataset) 698
after delete: len(noisy_dataset) 2438
epoch [67/200] batch [5/21] time 0.448 (0.450) data 0.318 (0.319) loss_x loss_x 1.4932 (1.3322) acc_x 59.3750 (66.2500) lr 1.5225e-03 eta 0:00:07
epoch [67/200] batch [10/21] time 0.361 (0.432) data 0.230 (0.301) loss_x loss_x 1.3232 (1.3599) acc_x 59.3750 (65.9375) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [15/21] time 0.643 (0.458) data 0.511 (0.327) loss_x loss_x 1.2178 (1.3876) acc_x 59.3750 (63.7500) lr 1.5225e-03 eta 0:00:02
epoch [67/200] batch [20/21] time 0.541 (0.478) data 0.410 (0.347) loss_x loss_x 0.8057 (1.3730) acc_x 78.1250 (64.2188) lr 1.5225e-03 eta 0:00:00
epoch [67/200] batch [5/76] time 0.361 (0.468) data 0.229 (0.337) loss_u loss_u 0.8320 (0.9019) acc_u 21.8750 (14.3750) lr 1.5225e-03 eta 0:00:33
epoch [67/200] batch [10/76] time 0.380 (0.466) data 0.248 (0.335) loss_u loss_u 0.9116 (0.9017) acc_u 12.5000 (12.5000) lr 1.5225e-03 eta 0:00:30
epoch [67/200] batch [15/76] time 0.334 (0.464) data 0.203 (0.333) loss_u loss_u 0.8608 (0.9037) acc_u 15.6250 (12.7083) lr 1.5225e-03 eta 0:00:28
epoch [67/200] batch [20/76] time 0.394 (0.462) data 0.261 (0.330) loss_u loss_u 0.9302 (0.9052) acc_u 9.3750 (12.6562) lr 1.5225e-03 eta 0:00:25
epoch [67/200] batch [25/76] time 0.409 (0.457) data 0.278 (0.326) loss_u loss_u 0.8740 (0.8979) acc_u 18.7500 (13.7500) lr 1.5225e-03 eta 0:00:23
epoch [67/200] batch [30/76] time 0.495 (0.459) data 0.365 (0.327) loss_u loss_u 0.9189 (0.9034) acc_u 12.5000 (12.9167) lr 1.5225e-03 eta 0:00:21
epoch [67/200] batch [35/76] time 0.584 (0.453) data 0.454 (0.321) loss_u loss_u 0.9150 (0.9063) acc_u 9.3750 (12.1429) lr 1.5225e-03 eta 0:00:18
epoch [67/200] batch [40/76] time 0.437 (0.455) data 0.306 (0.323) loss_u loss_u 0.9644 (0.9068) acc_u 3.1250 (11.8750) lr 1.5225e-03 eta 0:00:16
epoch [67/200] batch [45/76] time 0.379 (0.452) data 0.246 (0.321) loss_u loss_u 0.9106 (0.9064) acc_u 9.3750 (11.6667) lr 1.5225e-03 eta 0:00:14
epoch [67/200] batch [50/76] time 0.536 (0.455) data 0.405 (0.323) loss_u loss_u 0.8916 (0.9055) acc_u 18.7500 (11.8750) lr 1.5225e-03 eta 0:00:11
epoch [67/200] batch [55/76] time 0.491 (0.457) data 0.360 (0.326) loss_u loss_u 0.9590 (0.9079) acc_u 3.1250 (11.6477) lr 1.5225e-03 eta 0:00:09
epoch [67/200] batch [60/76] time 0.468 (0.460) data 0.335 (0.329) loss_u loss_u 0.9683 (0.9077) acc_u 0.0000 (11.4583) lr 1.5225e-03 eta 0:00:07
epoch [67/200] batch [65/76] time 0.441 (0.458) data 0.310 (0.326) loss_u loss_u 0.8711 (0.9055) acc_u 12.5000 (11.7788) lr 1.5225e-03 eta 0:00:05
epoch [67/200] batch [70/76] time 0.554 (0.458) data 0.423 (0.327) loss_u loss_u 0.8965 (0.9071) acc_u 12.5000 (11.6071) lr 1.5225e-03 eta 0:00:02
epoch [67/200] batch [75/76] time 0.553 (0.460) data 0.423 (0.328) loss_u loss_u 0.9429 (0.9060) acc_u 12.5000 (11.9583) lr 1.5225e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2018
confident_label rate tensor(0.2232, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 700
clean true:350
clean false:350
clean_rate:0.5
noisy true:768
noisy false:1668
after delete: len(clean_dataset) 700
after delete: len(noisy_dataset) 2436
epoch [68/200] batch [5/21] time 0.434 (0.446) data 0.303 (0.315) loss_x loss_x 1.2529 (1.3252) acc_x 68.7500 (62.5000) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [10/21] time 0.360 (0.441) data 0.229 (0.310) loss_x loss_x 1.4424 (1.3856) acc_x 59.3750 (60.9375) lr 1.5090e-03 eta 0:00:04
epoch [68/200] batch [15/21] time 0.454 (0.438) data 0.324 (0.307) loss_x loss_x 1.6143 (1.4025) acc_x 46.8750 (60.2083) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [20/21] time 0.458 (0.436) data 0.327 (0.305) loss_x loss_x 1.1973 (1.3818) acc_x 75.0000 (61.4062) lr 1.5090e-03 eta 0:00:00
epoch [68/200] batch [5/76] time 0.357 (0.448) data 0.226 (0.318) loss_u loss_u 0.9014 (0.9155) acc_u 9.3750 (9.3750) lr 1.5090e-03 eta 0:00:31
epoch [68/200] batch [10/76] time 0.345 (0.447) data 0.214 (0.316) loss_u loss_u 0.8955 (0.9117) acc_u 15.6250 (10.6250) lr 1.5090e-03 eta 0:00:29
epoch [68/200] batch [15/76] time 0.407 (0.446) data 0.276 (0.316) loss_u loss_u 0.9585 (0.9202) acc_u 6.2500 (10.2083) lr 1.5090e-03 eta 0:00:27
epoch [68/200] batch [20/76] time 0.446 (0.454) data 0.315 (0.324) loss_u loss_u 0.8960 (0.9141) acc_u 12.5000 (11.5625) lr 1.5090e-03 eta 0:00:25
epoch [68/200] batch [25/76] time 0.474 (0.453) data 0.341 (0.322) loss_u loss_u 0.8999 (0.9137) acc_u 18.7500 (11.5000) lr 1.5090e-03 eta 0:00:23
epoch [68/200] batch [30/76] time 0.565 (0.456) data 0.434 (0.325) loss_u loss_u 0.8843 (0.9132) acc_u 15.6250 (11.5625) lr 1.5090e-03 eta 0:00:20
epoch [68/200] batch [35/76] time 0.494 (0.459) data 0.363 (0.328) loss_u loss_u 0.9028 (0.9114) acc_u 12.5000 (11.5179) lr 1.5090e-03 eta 0:00:18
epoch [68/200] batch [40/76] time 0.483 (0.459) data 0.352 (0.328) loss_u loss_u 0.9307 (0.9142) acc_u 9.3750 (11.3281) lr 1.5090e-03 eta 0:00:16
epoch [68/200] batch [45/76] time 0.496 (0.457) data 0.365 (0.326) loss_u loss_u 0.8677 (0.9081) acc_u 21.8750 (12.0833) lr 1.5090e-03 eta 0:00:14
epoch [68/200] batch [50/76] time 0.446 (0.457) data 0.315 (0.326) loss_u loss_u 0.8784 (0.9102) acc_u 12.5000 (11.8125) lr 1.5090e-03 eta 0:00:11
epoch [68/200] batch [55/76] time 0.428 (0.456) data 0.297 (0.325) loss_u loss_u 0.8867 (0.9086) acc_u 12.5000 (11.9886) lr 1.5090e-03 eta 0:00:09
epoch [68/200] batch [60/76] time 0.428 (0.460) data 0.296 (0.329) loss_u loss_u 0.9521 (0.9097) acc_u 6.2500 (11.9271) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [65/76] time 0.356 (0.459) data 0.224 (0.328) loss_u loss_u 0.8945 (0.9081) acc_u 12.5000 (12.0673) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [70/76] time 0.395 (0.461) data 0.263 (0.330) loss_u loss_u 0.9185 (0.9078) acc_u 12.5000 (12.2321) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [75/76] time 0.418 (0.459) data 0.286 (0.328) loss_u loss_u 0.9238 (0.9069) acc_u 9.3750 (12.3333) lr 1.5090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2043
confident_label rate tensor(0.2184, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 685
clean true:352
clean false:333
clean_rate:0.5138686131386861
noisy true:741
noisy false:1710
after delete: len(clean_dataset) 685
after delete: len(noisy_dataset) 2451
epoch [69/200] batch [5/21] time 0.500 (0.480) data 0.370 (0.350) loss_x loss_x 1.5381 (1.2968) acc_x 59.3750 (65.6250) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [10/21] time 0.424 (0.460) data 0.294 (0.330) loss_x loss_x 1.8223 (1.3998) acc_x 56.2500 (63.7500) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [15/21] time 0.412 (0.462) data 0.282 (0.332) loss_x loss_x 1.4736 (1.4455) acc_x 50.0000 (61.8750) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [20/21] time 0.434 (0.454) data 0.304 (0.323) loss_x loss_x 1.1406 (1.3804) acc_x 68.7500 (63.1250) lr 1.4955e-03 eta 0:00:00
epoch [69/200] batch [5/76] time 0.339 (0.456) data 0.208 (0.326) loss_u loss_u 0.9644 (0.9186) acc_u 3.1250 (9.3750) lr 1.4955e-03 eta 0:00:32
epoch [69/200] batch [10/76] time 0.333 (0.449) data 0.201 (0.318) loss_u loss_u 0.9321 (0.9197) acc_u 6.2500 (9.6875) lr 1.4955e-03 eta 0:00:29
epoch [69/200] batch [15/76] time 0.595 (0.450) data 0.464 (0.319) loss_u loss_u 0.9209 (0.9123) acc_u 6.2500 (11.0417) lr 1.4955e-03 eta 0:00:27
epoch [69/200] batch [20/76] time 0.596 (0.450) data 0.464 (0.320) loss_u loss_u 0.8428 (0.9077) acc_u 21.8750 (11.4062) lr 1.4955e-03 eta 0:00:25
epoch [69/200] batch [25/76] time 0.447 (0.460) data 0.313 (0.329) loss_u loss_u 0.9644 (0.9074) acc_u 3.1250 (11.1250) lr 1.4955e-03 eta 0:00:23
epoch [69/200] batch [30/76] time 0.424 (0.465) data 0.292 (0.334) loss_u loss_u 0.8955 (0.9081) acc_u 18.7500 (11.3542) lr 1.4955e-03 eta 0:00:21
epoch [69/200] batch [35/76] time 0.397 (0.458) data 0.266 (0.327) loss_u loss_u 0.9556 (0.9093) acc_u 6.2500 (11.2500) lr 1.4955e-03 eta 0:00:18
epoch [69/200] batch [40/76] time 0.431 (0.456) data 0.299 (0.324) loss_u loss_u 0.8691 (0.9087) acc_u 15.6250 (11.2500) lr 1.4955e-03 eta 0:00:16
epoch [69/200] batch [45/76] time 0.512 (0.457) data 0.381 (0.325) loss_u loss_u 0.8999 (0.9086) acc_u 15.6250 (11.1111) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [50/76] time 0.500 (0.460) data 0.369 (0.328) loss_u loss_u 0.8765 (0.9071) acc_u 18.7500 (11.1250) lr 1.4955e-03 eta 0:00:11
epoch [69/200] batch [55/76] time 0.408 (0.460) data 0.276 (0.329) loss_u loss_u 0.9023 (0.9089) acc_u 9.3750 (10.7386) lr 1.4955e-03 eta 0:00:09
epoch [69/200] batch [60/76] time 0.414 (0.459) data 0.282 (0.328) loss_u loss_u 0.8135 (0.9074) acc_u 21.8750 (10.9375) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [65/76] time 0.346 (0.455) data 0.216 (0.324) loss_u loss_u 0.9111 (0.9076) acc_u 15.6250 (11.0577) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [70/76] time 0.518 (0.455) data 0.386 (0.324) loss_u loss_u 0.8267 (0.9063) acc_u 18.7500 (11.1607) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [75/76] time 0.527 (0.456) data 0.396 (0.325) loss_u loss_u 0.8994 (0.9056) acc_u 9.3750 (11.3333) lr 1.4955e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2048
confident_label rate tensor(0.2245, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 704
clean true:343
clean false:361
clean_rate:0.4872159090909091
noisy true:745
noisy false:1687
after delete: len(clean_dataset) 704
after delete: len(noisy_dataset) 2432
epoch [70/200] batch [5/22] time 0.396 (0.447) data 0.265 (0.316) loss_x loss_x 1.1631 (1.5633) acc_x 65.6250 (58.7500) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [10/22] time 0.509 (0.465) data 0.378 (0.334) loss_x loss_x 0.9844 (1.3748) acc_x 65.6250 (62.5000) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [15/22] time 0.408 (0.468) data 0.277 (0.337) loss_x loss_x 0.9941 (1.3589) acc_x 75.0000 (63.5417) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [20/22] time 0.434 (0.465) data 0.303 (0.334) loss_x loss_x 1.6523 (1.3888) acc_x 43.7500 (61.7188) lr 1.4818e-03 eta 0:00:00
epoch [70/200] batch [5/76] time 0.472 (0.458) data 0.341 (0.327) loss_u loss_u 0.8428 (0.8997) acc_u 21.8750 (12.5000) lr 1.4818e-03 eta 0:00:32
epoch [70/200] batch [10/76] time 0.408 (0.456) data 0.276 (0.325) loss_u loss_u 0.8877 (0.9111) acc_u 9.3750 (11.8750) lr 1.4818e-03 eta 0:00:30
epoch [70/200] batch [15/76] time 0.378 (0.456) data 0.247 (0.325) loss_u loss_u 0.9043 (0.9151) acc_u 9.3750 (10.6250) lr 1.4818e-03 eta 0:00:27
epoch [70/200] batch [20/76] time 0.373 (0.453) data 0.242 (0.322) loss_u loss_u 0.9336 (0.9134) acc_u 12.5000 (10.7812) lr 1.4818e-03 eta 0:00:25
epoch [70/200] batch [25/76] time 0.469 (0.454) data 0.337 (0.323) loss_u loss_u 0.8960 (0.9105) acc_u 18.7500 (11.7500) lr 1.4818e-03 eta 0:00:23
epoch [70/200] batch [30/76] time 0.445 (0.451) data 0.313 (0.320) loss_u loss_u 0.9160 (0.9086) acc_u 12.5000 (11.9792) lr 1.4818e-03 eta 0:00:20
epoch [70/200] batch [35/76] time 0.536 (0.454) data 0.405 (0.323) loss_u loss_u 0.9233 (0.9077) acc_u 12.5000 (11.8750) lr 1.4818e-03 eta 0:00:18
epoch [70/200] batch [40/76] time 0.340 (0.460) data 0.209 (0.329) loss_u loss_u 0.9214 (0.9091) acc_u 6.2500 (11.7188) lr 1.4818e-03 eta 0:00:16
epoch [70/200] batch [45/76] time 0.539 (0.461) data 0.409 (0.330) loss_u loss_u 0.8691 (0.9068) acc_u 15.6250 (12.0139) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [50/76] time 0.339 (0.461) data 0.208 (0.330) loss_u loss_u 0.9194 (0.9093) acc_u 12.5000 (11.7500) lr 1.4818e-03 eta 0:00:11
epoch [70/200] batch [55/76] time 0.572 (0.462) data 0.441 (0.331) loss_u loss_u 0.9414 (0.9101) acc_u 3.1250 (11.4773) lr 1.4818e-03 eta 0:00:09
epoch [70/200] batch [60/76] time 0.474 (0.461) data 0.343 (0.330) loss_u loss_u 0.8940 (0.9103) acc_u 9.3750 (11.4583) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [65/76] time 0.430 (0.460) data 0.299 (0.329) loss_u loss_u 0.9321 (0.9120) acc_u 12.5000 (11.3462) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [70/76] time 0.484 (0.461) data 0.352 (0.329) loss_u loss_u 0.8706 (0.9128) acc_u 15.6250 (11.2054) lr 1.4818e-03 eta 0:00:02
epoch [70/200] batch [75/76] time 0.410 (0.461) data 0.279 (0.330) loss_u loss_u 0.8892 (0.9128) acc_u 12.5000 (11.2083) lr 1.4818e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2009
confident_label rate tensor(0.2248, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 705
clean true:364
clean false:341
clean_rate:0.5163120567375886
noisy true:763
noisy false:1668
after delete: len(clean_dataset) 705
after delete: len(noisy_dataset) 2431
epoch [71/200] batch [5/22] time 0.398 (0.440) data 0.267 (0.310) loss_x loss_x 1.1943 (1.2254) acc_x 68.7500 (67.5000) lr 1.4679e-03 eta 0:00:07
epoch [71/200] batch [10/22] time 0.410 (0.447) data 0.280 (0.317) loss_x loss_x 0.8965 (1.2832) acc_x 81.2500 (66.2500) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [15/22] time 0.414 (0.447) data 0.284 (0.316) loss_x loss_x 1.4502 (1.3313) acc_x 56.2500 (65.2083) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [20/22] time 0.483 (0.449) data 0.352 (0.319) loss_x loss_x 1.6475 (1.3235) acc_x 59.3750 (66.0938) lr 1.4679e-03 eta 0:00:00
epoch [71/200] batch [5/75] time 0.400 (0.441) data 0.268 (0.311) loss_u loss_u 0.8882 (0.8976) acc_u 15.6250 (15.0000) lr 1.4679e-03 eta 0:00:30
epoch [71/200] batch [10/75] time 0.419 (0.437) data 0.288 (0.306) loss_u loss_u 0.9199 (0.9181) acc_u 9.3750 (11.5625) lr 1.4679e-03 eta 0:00:28
epoch [71/200] batch [15/75] time 0.520 (0.438) data 0.389 (0.308) loss_u loss_u 0.9102 (0.9174) acc_u 15.6250 (11.6667) lr 1.4679e-03 eta 0:00:26
epoch [71/200] batch [20/75] time 0.515 (0.448) data 0.383 (0.317) loss_u loss_u 0.9238 (0.9206) acc_u 15.6250 (11.4062) lr 1.4679e-03 eta 0:00:24
epoch [71/200] batch [25/75] time 0.544 (0.454) data 0.411 (0.323) loss_u loss_u 0.8979 (0.9185) acc_u 12.5000 (11.3750) lr 1.4679e-03 eta 0:00:22
epoch [71/200] batch [30/75] time 0.584 (0.455) data 0.452 (0.323) loss_u loss_u 0.9375 (0.9138) acc_u 9.3750 (11.8750) lr 1.4679e-03 eta 0:00:20
epoch [71/200] batch [35/75] time 0.532 (0.458) data 0.400 (0.327) loss_u loss_u 0.9326 (0.9130) acc_u 9.3750 (11.6964) lr 1.4679e-03 eta 0:00:18
epoch [71/200] batch [40/75] time 0.408 (0.457) data 0.277 (0.326) loss_u loss_u 0.9458 (0.9123) acc_u 9.3750 (11.7188) lr 1.4679e-03 eta 0:00:15
epoch [71/200] batch [45/75] time 0.427 (0.455) data 0.296 (0.324) loss_u loss_u 0.8418 (0.9123) acc_u 25.0000 (11.8056) lr 1.4679e-03 eta 0:00:13
epoch [71/200] batch [50/75] time 0.398 (0.455) data 0.267 (0.324) loss_u loss_u 0.8877 (0.9139) acc_u 18.7500 (11.6250) lr 1.4679e-03 eta 0:00:11
epoch [71/200] batch [55/75] time 0.430 (0.456) data 0.298 (0.324) loss_u loss_u 0.8799 (0.9109) acc_u 18.7500 (11.9318) lr 1.4679e-03 eta 0:00:09
epoch [71/200] batch [60/75] time 0.399 (0.454) data 0.269 (0.323) loss_u loss_u 0.9072 (0.9099) acc_u 12.5000 (12.0833) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [65/75] time 0.506 (0.454) data 0.374 (0.323) loss_u loss_u 0.8779 (0.9082) acc_u 12.5000 (12.2115) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [70/75] time 0.408 (0.454) data 0.276 (0.323) loss_u loss_u 0.9424 (0.9075) acc_u 6.2500 (12.4107) lr 1.4679e-03 eta 0:00:02
epoch [71/200] batch [75/75] time 0.530 (0.453) data 0.398 (0.322) loss_u loss_u 0.9434 (0.9081) acc_u 6.2500 (12.3333) lr 1.4679e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2048
confident_label rate tensor(0.2207, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 692
clean true:356
clean false:336
clean_rate:0.5144508670520231
noisy true:732
noisy false:1712
after delete: len(clean_dataset) 692
after delete: len(noisy_dataset) 2444
epoch [72/200] batch [5/21] time 0.554 (0.521) data 0.423 (0.390) loss_x loss_x 1.4990 (1.3928) acc_x 59.3750 (66.2500) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [10/21] time 0.629 (0.539) data 0.498 (0.408) loss_x loss_x 1.0049 (1.3887) acc_x 71.8750 (64.3750) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [15/21] time 0.454 (0.515) data 0.323 (0.384) loss_x loss_x 0.9136 (1.3949) acc_x 81.2500 (65.4167) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [20/21] time 0.378 (0.511) data 0.248 (0.380) loss_x loss_x 1.4854 (1.3558) acc_x 56.2500 (65.6250) lr 1.4540e-03 eta 0:00:00
epoch [72/200] batch [5/76] time 0.587 (0.502) data 0.457 (0.371) loss_u loss_u 0.8369 (0.8933) acc_u 18.7500 (13.7500) lr 1.4540e-03 eta 0:00:35
epoch [72/200] batch [10/76] time 0.504 (0.498) data 0.372 (0.367) loss_u loss_u 0.8638 (0.9054) acc_u 25.0000 (13.1250) lr 1.4540e-03 eta 0:00:32
epoch [72/200] batch [15/76] time 0.344 (0.486) data 0.213 (0.354) loss_u loss_u 0.9751 (0.9047) acc_u 0.0000 (13.7500) lr 1.4540e-03 eta 0:00:29
epoch [72/200] batch [20/76] time 0.411 (0.473) data 0.278 (0.342) loss_u loss_u 0.8813 (0.9104) acc_u 21.8750 (12.6562) lr 1.4540e-03 eta 0:00:26
epoch [72/200] batch [25/76] time 0.480 (0.472) data 0.350 (0.340) loss_u loss_u 0.9302 (0.9089) acc_u 6.2500 (12.3750) lr 1.4540e-03 eta 0:00:24
epoch [72/200] batch [30/76] time 0.529 (0.470) data 0.399 (0.339) loss_u loss_u 0.8892 (0.9095) acc_u 12.5000 (11.9792) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [35/76] time 0.405 (0.465) data 0.273 (0.334) loss_u loss_u 0.9023 (0.9084) acc_u 12.5000 (12.3214) lr 1.4540e-03 eta 0:00:19
epoch [72/200] batch [40/76] time 0.610 (0.468) data 0.478 (0.337) loss_u loss_u 0.8950 (0.9090) acc_u 12.5000 (12.1875) lr 1.4540e-03 eta 0:00:16
epoch [72/200] batch [45/76] time 0.480 (0.467) data 0.349 (0.335) loss_u loss_u 0.9331 (0.9095) acc_u 6.2500 (12.2222) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [50/76] time 0.455 (0.464) data 0.324 (0.332) loss_u loss_u 0.8491 (0.9062) acc_u 18.7500 (12.8125) lr 1.4540e-03 eta 0:00:12
epoch [72/200] batch [55/76] time 0.624 (0.464) data 0.492 (0.333) loss_u loss_u 0.8467 (0.9056) acc_u 18.7500 (12.7841) lr 1.4540e-03 eta 0:00:09
epoch [72/200] batch [60/76] time 0.425 (0.466) data 0.293 (0.334) loss_u loss_u 0.9351 (0.9068) acc_u 6.2500 (12.5521) lr 1.4540e-03 eta 0:00:07
epoch [72/200] batch [65/76] time 0.419 (0.467) data 0.287 (0.336) loss_u loss_u 0.9409 (0.9071) acc_u 6.2500 (12.5962) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [70/76] time 0.464 (0.465) data 0.333 (0.334) loss_u loss_u 0.9097 (0.9061) acc_u 15.6250 (12.6786) lr 1.4540e-03 eta 0:00:02
epoch [72/200] batch [75/76] time 0.434 (0.463) data 0.303 (0.332) loss_u loss_u 0.9062 (0.9070) acc_u 3.1250 (12.3750) lr 1.4540e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2061
confident_label rate tensor(0.2114, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 663
clean true:333
clean false:330
clean_rate:0.502262443438914
noisy true:742
noisy false:1731
after delete: len(clean_dataset) 663
after delete: len(noisy_dataset) 2473
epoch [73/200] batch [5/20] time 0.386 (0.488) data 0.255 (0.357) loss_x loss_x 1.3799 (1.2244) acc_x 62.5000 (67.5000) lr 1.4399e-03 eta 0:00:07
epoch [73/200] batch [10/20] time 0.527 (0.479) data 0.397 (0.349) loss_x loss_x 0.9966 (1.2910) acc_x 71.8750 (66.8750) lr 1.4399e-03 eta 0:00:04
epoch [73/200] batch [15/20] time 0.472 (0.485) data 0.341 (0.354) loss_x loss_x 1.2363 (1.2506) acc_x 68.7500 (67.2917) lr 1.4399e-03 eta 0:00:02
epoch [73/200] batch [20/20] time 0.443 (0.476) data 0.312 (0.346) loss_x loss_x 1.0381 (1.2745) acc_x 78.1250 (66.8750) lr 1.4399e-03 eta 0:00:00
epoch [73/200] batch [5/77] time 0.445 (0.463) data 0.314 (0.333) loss_u loss_u 0.9302 (0.9256) acc_u 6.2500 (10.6250) lr 1.4399e-03 eta 0:00:33
epoch [73/200] batch [10/77] time 0.472 (0.456) data 0.341 (0.325) loss_u loss_u 0.9360 (0.9210) acc_u 9.3750 (10.6250) lr 1.4399e-03 eta 0:00:30
epoch [73/200] batch [15/77] time 0.547 (0.455) data 0.416 (0.325) loss_u loss_u 0.9224 (0.9176) acc_u 9.3750 (10.6250) lr 1.4399e-03 eta 0:00:28
epoch [73/200] batch [20/77] time 0.424 (0.453) data 0.293 (0.322) loss_u loss_u 0.9297 (0.9163) acc_u 9.3750 (10.9375) lr 1.4399e-03 eta 0:00:25
epoch [73/200] batch [25/77] time 0.330 (0.450) data 0.199 (0.320) loss_u loss_u 0.8623 (0.9101) acc_u 12.5000 (11.6250) lr 1.4399e-03 eta 0:00:23
epoch [73/200] batch [30/77] time 0.419 (0.448) data 0.289 (0.317) loss_u loss_u 0.8037 (0.9103) acc_u 28.1250 (11.5625) lr 1.4399e-03 eta 0:00:21
epoch [73/200] batch [35/77] time 0.422 (0.450) data 0.290 (0.319) loss_u loss_u 0.9331 (0.9114) acc_u 6.2500 (11.3393) lr 1.4399e-03 eta 0:00:18
epoch [73/200] batch [40/77] time 0.691 (0.454) data 0.559 (0.323) loss_u loss_u 0.8584 (0.9070) acc_u 21.8750 (12.0312) lr 1.4399e-03 eta 0:00:16
epoch [73/200] batch [45/77] time 0.345 (0.454) data 0.214 (0.323) loss_u loss_u 0.9141 (0.9069) acc_u 12.5000 (12.0139) lr 1.4399e-03 eta 0:00:14
epoch [73/200] batch [50/77] time 0.428 (0.449) data 0.297 (0.319) loss_u loss_u 0.8755 (0.9057) acc_u 18.7500 (12.3750) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [55/77] time 0.512 (0.453) data 0.380 (0.322) loss_u loss_u 0.9341 (0.9056) acc_u 6.2500 (12.3864) lr 1.4399e-03 eta 0:00:09
epoch [73/200] batch [60/77] time 0.369 (0.449) data 0.237 (0.318) loss_u loss_u 0.8862 (0.9058) acc_u 12.5000 (12.2917) lr 1.4399e-03 eta 0:00:07
epoch [73/200] batch [65/77] time 0.346 (0.448) data 0.215 (0.317) loss_u loss_u 0.8975 (0.9062) acc_u 15.6250 (12.2596) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [70/77] time 0.398 (0.450) data 0.267 (0.319) loss_u loss_u 0.9502 (0.9058) acc_u 3.1250 (12.0982) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [75/77] time 0.470 (0.452) data 0.339 (0.321) loss_u loss_u 0.8682 (0.9058) acc_u 18.7500 (12.2083) lr 1.4399e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2058
confident_label rate tensor(0.2293, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 719
clean true:359
clean false:360
clean_rate:0.4993045897079277
noisy true:719
noisy false:1698
after delete: len(clean_dataset) 719
after delete: len(noisy_dataset) 2417
epoch [74/200] batch [5/22] time 0.421 (0.429) data 0.290 (0.298) loss_x loss_x 1.5791 (1.1637) acc_x 56.2500 (65.6250) lr 1.4258e-03 eta 0:00:07
epoch [74/200] batch [10/22] time 0.409 (0.431) data 0.278 (0.300) loss_x loss_x 1.5879 (1.2145) acc_x 59.3750 (66.8750) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [15/22] time 0.445 (0.430) data 0.314 (0.299) loss_x loss_x 1.2549 (1.2885) acc_x 68.7500 (65.0000) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [20/22] time 0.417 (0.451) data 0.286 (0.320) loss_x loss_x 1.5420 (1.3330) acc_x 50.0000 (63.5938) lr 1.4258e-03 eta 0:00:00
epoch [74/200] batch [5/75] time 0.480 (0.454) data 0.349 (0.323) loss_u loss_u 0.7827 (0.8862) acc_u 28.1250 (14.3750) lr 1.4258e-03 eta 0:00:31
epoch [74/200] batch [10/75] time 0.495 (0.452) data 0.364 (0.322) loss_u loss_u 0.9175 (0.8971) acc_u 12.5000 (11.8750) lr 1.4258e-03 eta 0:00:29
epoch [74/200] batch [15/75] time 0.467 (0.450) data 0.336 (0.319) loss_u loss_u 0.9331 (0.9059) acc_u 3.1250 (10.4167) lr 1.4258e-03 eta 0:00:26
epoch [74/200] batch [20/75] time 0.433 (0.454) data 0.300 (0.323) loss_u loss_u 0.8911 (0.9036) acc_u 18.7500 (10.9375) lr 1.4258e-03 eta 0:00:24
epoch [74/200] batch [25/75] time 0.397 (0.458) data 0.266 (0.327) loss_u loss_u 0.8618 (0.9047) acc_u 15.6250 (11.0000) lr 1.4258e-03 eta 0:00:22
epoch [74/200] batch [30/75] time 0.481 (0.460) data 0.349 (0.328) loss_u loss_u 0.9058 (0.9063) acc_u 12.5000 (11.5625) lr 1.4258e-03 eta 0:00:20
epoch [74/200] batch [35/75] time 0.491 (0.459) data 0.359 (0.328) loss_u loss_u 0.9067 (0.9088) acc_u 12.5000 (11.2500) lr 1.4258e-03 eta 0:00:18
epoch [74/200] batch [40/75] time 0.423 (0.463) data 0.291 (0.332) loss_u loss_u 0.9204 (0.9120) acc_u 6.2500 (10.8594) lr 1.4258e-03 eta 0:00:16
epoch [74/200] batch [45/75] time 0.527 (0.463) data 0.394 (0.332) loss_u loss_u 0.9561 (0.9129) acc_u 3.1250 (10.9722) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [50/75] time 0.414 (0.463) data 0.282 (0.332) loss_u loss_u 0.8892 (0.9114) acc_u 9.3750 (11.1250) lr 1.4258e-03 eta 0:00:11
epoch [74/200] batch [55/75] time 0.443 (0.463) data 0.311 (0.332) loss_u loss_u 0.8516 (0.9127) acc_u 18.7500 (10.9659) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [60/75] time 0.524 (0.463) data 0.393 (0.332) loss_u loss_u 0.8384 (0.9128) acc_u 31.2500 (11.0938) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [65/75] time 0.452 (0.460) data 0.321 (0.329) loss_u loss_u 0.9150 (0.9138) acc_u 6.2500 (10.9135) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [70/75] time 0.381 (0.459) data 0.249 (0.328) loss_u loss_u 0.8779 (0.9121) acc_u 15.6250 (11.1607) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [75/75] time 0.364 (0.458) data 0.231 (0.327) loss_u loss_u 0.9155 (0.9128) acc_u 15.6250 (10.9583) lr 1.4258e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2085
confident_label rate tensor(0.2216, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 695
clean true:346
clean false:349
clean_rate:0.497841726618705
noisy true:705
noisy false:1736
after delete: len(clean_dataset) 695
after delete: len(noisy_dataset) 2441
epoch [75/200] batch [5/21] time 0.419 (0.461) data 0.288 (0.330) loss_x loss_x 0.9189 (1.2500) acc_x 71.8750 (70.0000) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [10/21] time 0.489 (0.468) data 0.358 (0.337) loss_x loss_x 1.0400 (1.3140) acc_x 65.6250 (65.3125) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [15/21] time 0.403 (0.456) data 0.273 (0.325) loss_x loss_x 1.1270 (1.3833) acc_x 75.0000 (64.1667) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [20/21] time 0.529 (0.453) data 0.398 (0.322) loss_x loss_x 1.5186 (1.3602) acc_x 68.7500 (64.8438) lr 1.4115e-03 eta 0:00:00
epoch [75/200] batch [5/76] time 0.423 (0.446) data 0.292 (0.315) loss_u loss_u 0.9092 (0.9163) acc_u 12.5000 (10.6250) lr 1.4115e-03 eta 0:00:31
epoch [75/200] batch [10/76] time 0.502 (0.456) data 0.371 (0.325) loss_u loss_u 0.9370 (0.9188) acc_u 9.3750 (11.2500) lr 1.4115e-03 eta 0:00:30
epoch [75/200] batch [15/76] time 0.471 (0.449) data 0.340 (0.318) loss_u loss_u 0.8740 (0.9004) acc_u 18.7500 (12.7083) lr 1.4115e-03 eta 0:00:27
epoch [75/200] batch [20/76] time 0.319 (0.443) data 0.186 (0.312) loss_u loss_u 0.8569 (0.8960) acc_u 25.0000 (14.2188) lr 1.4115e-03 eta 0:00:24
epoch [75/200] batch [25/76] time 0.403 (0.444) data 0.272 (0.313) loss_u loss_u 0.8872 (0.8955) acc_u 18.7500 (14.2500) lr 1.4115e-03 eta 0:00:22
epoch [75/200] batch [30/76] time 0.453 (0.442) data 0.322 (0.311) loss_u loss_u 0.8892 (0.8945) acc_u 12.5000 (14.2708) lr 1.4115e-03 eta 0:00:20
epoch [75/200] batch [35/76] time 0.510 (0.450) data 0.378 (0.319) loss_u loss_u 0.9653 (0.9004) acc_u 0.0000 (13.1250) lr 1.4115e-03 eta 0:00:18
epoch [75/200] batch [40/76] time 0.386 (0.450) data 0.254 (0.318) loss_u loss_u 0.9360 (0.8996) acc_u 9.3750 (13.2031) lr 1.4115e-03 eta 0:00:16
epoch [75/200] batch [45/76] time 0.704 (0.454) data 0.573 (0.323) loss_u loss_u 0.9473 (0.9028) acc_u 3.1250 (12.7778) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [50/76] time 0.399 (0.450) data 0.268 (0.319) loss_u loss_u 0.8955 (0.9025) acc_u 9.3750 (12.6875) lr 1.4115e-03 eta 0:00:11
epoch [75/200] batch [55/76] time 0.387 (0.452) data 0.255 (0.321) loss_u loss_u 0.8584 (0.9016) acc_u 18.7500 (12.7841) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [60/76] time 0.468 (0.453) data 0.338 (0.322) loss_u loss_u 0.9800 (0.9038) acc_u 3.1250 (12.5521) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [65/76] time 0.510 (0.456) data 0.379 (0.324) loss_u loss_u 0.8364 (0.9049) acc_u 21.8750 (12.2115) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [70/76] time 0.484 (0.456) data 0.353 (0.325) loss_u loss_u 0.9175 (0.9037) acc_u 9.3750 (12.2768) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [75/76] time 0.605 (0.460) data 0.474 (0.329) loss_u loss_u 0.9258 (0.9033) acc_u 12.5000 (12.3750) lr 1.4115e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2039
confident_label rate tensor(0.2357, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 739
clean true:368
clean false:371
clean_rate:0.4979702300405954
noisy true:729
noisy false:1668
after delete: len(clean_dataset) 739
after delete: len(noisy_dataset) 2397
epoch [76/200] batch [5/23] time 0.349 (0.405) data 0.218 (0.274) loss_x loss_x 1.0391 (1.3104) acc_x 71.8750 (66.8750) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [10/23] time 0.447 (0.463) data 0.315 (0.332) loss_x loss_x 1.3027 (1.3630) acc_x 59.3750 (65.9375) lr 1.3971e-03 eta 0:00:06
epoch [76/200] batch [15/23] time 0.639 (0.483) data 0.509 (0.352) loss_x loss_x 1.5576 (1.3289) acc_x 62.5000 (66.4583) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [20/23] time 0.577 (0.467) data 0.447 (0.337) loss_x loss_x 1.3506 (1.3073) acc_x 65.6250 (67.3438) lr 1.3971e-03 eta 0:00:01
epoch [76/200] batch [5/74] time 0.360 (0.448) data 0.228 (0.317) loss_u loss_u 0.9204 (0.8851) acc_u 15.6250 (14.3750) lr 1.3971e-03 eta 0:00:30
epoch [76/200] batch [10/74] time 0.486 (0.446) data 0.355 (0.315) loss_u loss_u 0.9199 (0.8838) acc_u 12.5000 (15.0000) lr 1.3971e-03 eta 0:00:28
epoch [76/200] batch [15/74] time 0.390 (0.444) data 0.258 (0.313) loss_u loss_u 0.9292 (0.8891) acc_u 3.1250 (14.1667) lr 1.3971e-03 eta 0:00:26
epoch [76/200] batch [20/74] time 0.474 (0.445) data 0.342 (0.314) loss_u loss_u 0.8506 (0.8886) acc_u 15.6250 (14.6875) lr 1.3971e-03 eta 0:00:24
epoch [76/200] batch [25/74] time 0.341 (0.441) data 0.211 (0.310) loss_u loss_u 0.8647 (0.8954) acc_u 18.7500 (13.6250) lr 1.3971e-03 eta 0:00:21
epoch [76/200] batch [30/74] time 0.488 (0.445) data 0.357 (0.313) loss_u loss_u 0.9380 (0.8984) acc_u 9.3750 (13.1250) lr 1.3971e-03 eta 0:00:19
epoch [76/200] batch [35/74] time 0.452 (0.449) data 0.322 (0.318) loss_u loss_u 0.9585 (0.9019) acc_u 3.1250 (12.9464) lr 1.3971e-03 eta 0:00:17
epoch [76/200] batch [40/74] time 0.434 (0.448) data 0.304 (0.317) loss_u loss_u 0.9248 (0.9032) acc_u 9.3750 (12.7344) lr 1.3971e-03 eta 0:00:15
epoch [76/200] batch [45/74] time 0.462 (0.452) data 0.330 (0.321) loss_u loss_u 0.8516 (0.9030) acc_u 18.7500 (12.5694) lr 1.3971e-03 eta 0:00:13
epoch [76/200] batch [50/74] time 0.392 (0.450) data 0.260 (0.319) loss_u loss_u 0.8662 (0.9013) acc_u 15.6250 (12.6250) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [55/74] time 0.444 (0.449) data 0.312 (0.318) loss_u loss_u 0.8472 (0.9008) acc_u 15.6250 (12.5000) lr 1.3971e-03 eta 0:00:08
epoch [76/200] batch [60/74] time 0.585 (0.456) data 0.453 (0.324) loss_u loss_u 0.9116 (0.8999) acc_u 15.6250 (12.7083) lr 1.3971e-03 eta 0:00:06
epoch [76/200] batch [65/74] time 0.465 (0.457) data 0.334 (0.325) loss_u loss_u 0.8394 (0.8982) acc_u 18.7500 (12.8846) lr 1.3971e-03 eta 0:00:04
epoch [76/200] batch [70/74] time 0.343 (0.456) data 0.212 (0.325) loss_u loss_u 0.9321 (0.8987) acc_u 6.2500 (12.8571) lr 1.3971e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2070
confident_label rate tensor(0.2302, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 722
clean true:368
clean false:354
clean_rate:0.5096952908587258
noisy true:698
noisy false:1716
after delete: len(clean_dataset) 722
after delete: len(noisy_dataset) 2414
epoch [77/200] batch [5/22] time 0.487 (0.492) data 0.356 (0.361) loss_x loss_x 2.2051 (1.4913) acc_x 62.5000 (64.3750) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [10/22] time 0.484 (0.479) data 0.354 (0.349) loss_x loss_x 1.6523 (1.3945) acc_x 62.5000 (64.6875) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [15/22] time 0.516 (0.469) data 0.385 (0.339) loss_x loss_x 1.2539 (1.3608) acc_x 75.0000 (65.2083) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [20/22] time 0.426 (0.461) data 0.295 (0.331) loss_x loss_x 1.1348 (1.3773) acc_x 71.8750 (66.0938) lr 1.3827e-03 eta 0:00:00
epoch [77/200] batch [5/75] time 0.459 (0.459) data 0.327 (0.328) loss_u loss_u 0.8169 (0.8788) acc_u 31.2500 (16.8750) lr 1.3827e-03 eta 0:00:32
epoch [77/200] batch [10/75] time 0.402 (0.465) data 0.270 (0.334) loss_u loss_u 0.9346 (0.8889) acc_u 9.3750 (14.3750) lr 1.3827e-03 eta 0:00:30
epoch [77/200] batch [15/75] time 0.482 (0.468) data 0.349 (0.337) loss_u loss_u 0.9546 (0.8953) acc_u 3.1250 (13.7500) lr 1.3827e-03 eta 0:00:28
epoch [77/200] batch [20/75] time 0.451 (0.469) data 0.318 (0.337) loss_u loss_u 0.8926 (0.8968) acc_u 12.5000 (13.4375) lr 1.3827e-03 eta 0:00:25
epoch [77/200] batch [25/75] time 0.383 (0.459) data 0.253 (0.327) loss_u loss_u 0.9292 (0.9032) acc_u 9.3750 (12.7500) lr 1.3827e-03 eta 0:00:22
epoch [77/200] batch [30/75] time 0.512 (0.454) data 0.380 (0.323) loss_u loss_u 0.9619 (0.9049) acc_u 0.0000 (12.1875) lr 1.3827e-03 eta 0:00:20
epoch [77/200] batch [35/75] time 0.452 (0.453) data 0.321 (0.321) loss_u loss_u 0.9282 (0.9039) acc_u 12.5000 (12.5893) lr 1.3827e-03 eta 0:00:18
epoch [77/200] batch [40/75] time 0.359 (0.454) data 0.227 (0.322) loss_u loss_u 0.9624 (0.9058) acc_u 3.1250 (12.2656) lr 1.3827e-03 eta 0:00:15
epoch [77/200] batch [45/75] time 0.397 (0.452) data 0.267 (0.321) loss_u loss_u 0.9419 (0.9055) acc_u 12.5000 (12.5000) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [50/75] time 0.444 (0.454) data 0.313 (0.323) loss_u loss_u 0.8979 (0.9055) acc_u 12.5000 (12.5000) lr 1.3827e-03 eta 0:00:11
epoch [77/200] batch [55/75] time 0.471 (0.460) data 0.340 (0.328) loss_u loss_u 0.9385 (0.9073) acc_u 6.2500 (12.1023) lr 1.3827e-03 eta 0:00:09
epoch [77/200] batch [60/75] time 0.401 (0.458) data 0.268 (0.326) loss_u loss_u 0.9268 (0.9072) acc_u 9.3750 (11.9271) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [65/75] time 0.572 (0.457) data 0.441 (0.325) loss_u loss_u 0.8794 (0.9067) acc_u 15.6250 (11.9712) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [70/75] time 0.395 (0.457) data 0.264 (0.326) loss_u loss_u 0.8818 (0.9058) acc_u 15.6250 (12.0982) lr 1.3827e-03 eta 0:00:02
epoch [77/200] batch [75/75] time 0.425 (0.458) data 0.294 (0.326) loss_u loss_u 0.8037 (0.9032) acc_u 28.1250 (12.5833) lr 1.3827e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2098
confident_label rate tensor(0.2293, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 719
clean true:344
clean false:375
clean_rate:0.47844228094575797
noisy true:694
noisy false:1723
after delete: len(clean_dataset) 719
after delete: len(noisy_dataset) 2417
epoch [78/200] batch [5/22] time 0.376 (0.451) data 0.246 (0.320) loss_x loss_x 1.2607 (1.1430) acc_x 68.7500 (71.8750) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [10/22] time 0.452 (0.450) data 0.322 (0.319) loss_x loss_x 1.7432 (1.3111) acc_x 59.3750 (66.5625) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [15/22] time 0.532 (0.461) data 0.401 (0.330) loss_x loss_x 1.2324 (1.2560) acc_x 62.5000 (67.5000) lr 1.3681e-03 eta 0:00:03
epoch [78/200] batch [20/22] time 0.480 (0.468) data 0.348 (0.337) loss_x loss_x 1.7773 (1.2882) acc_x 62.5000 (66.7188) lr 1.3681e-03 eta 0:00:00
epoch [78/200] batch [5/75] time 0.387 (0.456) data 0.256 (0.325) loss_u loss_u 0.9473 (0.9158) acc_u 6.2500 (10.0000) lr 1.3681e-03 eta 0:00:31
epoch [78/200] batch [10/75] time 0.610 (0.460) data 0.478 (0.330) loss_u loss_u 0.9507 (0.9162) acc_u 6.2500 (10.6250) lr 1.3681e-03 eta 0:00:29
epoch [78/200] batch [15/75] time 0.463 (0.460) data 0.332 (0.329) loss_u loss_u 0.9189 (0.9074) acc_u 12.5000 (12.0833) lr 1.3681e-03 eta 0:00:27
epoch [78/200] batch [20/75] time 0.514 (0.460) data 0.382 (0.329) loss_u loss_u 0.9224 (0.9086) acc_u 15.6250 (12.6562) lr 1.3681e-03 eta 0:00:25
epoch [78/200] batch [25/75] time 0.391 (0.459) data 0.261 (0.328) loss_u loss_u 0.8989 (0.9066) acc_u 12.5000 (12.3750) lr 1.3681e-03 eta 0:00:22
epoch [78/200] batch [30/75] time 0.457 (0.459) data 0.326 (0.328) loss_u loss_u 0.8887 (0.9039) acc_u 9.3750 (12.8125) lr 1.3681e-03 eta 0:00:20
epoch [78/200] batch [35/75] time 0.357 (0.455) data 0.225 (0.324) loss_u loss_u 0.8975 (0.9031) acc_u 12.5000 (12.8571) lr 1.3681e-03 eta 0:00:18
epoch [78/200] batch [40/75] time 0.421 (0.454) data 0.290 (0.322) loss_u loss_u 0.9697 (0.9033) acc_u 3.1250 (12.8125) lr 1.3681e-03 eta 0:00:15
epoch [78/200] batch [45/75] time 0.649 (0.461) data 0.517 (0.330) loss_u loss_u 0.8755 (0.9005) acc_u 12.5000 (12.8472) lr 1.3681e-03 eta 0:00:13
epoch [78/200] batch [50/75] time 0.529 (0.462) data 0.397 (0.331) loss_u loss_u 0.9092 (0.8993) acc_u 12.5000 (13.0000) lr 1.3681e-03 eta 0:00:11
epoch [78/200] batch [55/75] time 0.563 (0.463) data 0.433 (0.332) loss_u loss_u 0.8813 (0.8998) acc_u 15.6250 (12.8409) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [60/75] time 0.391 (0.459) data 0.260 (0.327) loss_u loss_u 0.8887 (0.9023) acc_u 12.5000 (12.3958) lr 1.3681e-03 eta 0:00:06
epoch [78/200] batch [65/75] time 0.457 (0.456) data 0.326 (0.325) loss_u loss_u 0.9126 (0.9016) acc_u 12.5000 (12.5000) lr 1.3681e-03 eta 0:00:04
epoch [78/200] batch [70/75] time 0.498 (0.458) data 0.367 (0.326) loss_u loss_u 0.8208 (0.9013) acc_u 21.8750 (12.5000) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [75/75] time 0.460 (0.458) data 0.329 (0.327) loss_u loss_u 0.9175 (0.9011) acc_u 18.7500 (12.5000) lr 1.3681e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2094
confident_label rate tensor(0.2254, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 707
clean true:351
clean false:356
clean_rate:0.49646393210749645
noisy true:691
noisy false:1738
after delete: len(clean_dataset) 707
after delete: len(noisy_dataset) 2429
epoch [79/200] batch [5/22] time 0.414 (0.485) data 0.283 (0.354) loss_x loss_x 1.2119 (1.2079) acc_x 68.7500 (68.7500) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [10/22] time 0.482 (0.463) data 0.351 (0.332) loss_x loss_x 1.5146 (1.3583) acc_x 71.8750 (66.5625) lr 1.3535e-03 eta 0:00:05
epoch [79/200] batch [15/22] time 0.577 (0.468) data 0.447 (0.337) loss_x loss_x 1.2119 (1.3187) acc_x 68.7500 (66.4583) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [20/22] time 0.456 (0.455) data 0.325 (0.324) loss_x loss_x 0.9014 (1.2811) acc_x 62.5000 (67.1875) lr 1.3535e-03 eta 0:00:00
epoch [79/200] batch [5/75] time 0.519 (0.447) data 0.387 (0.316) loss_u loss_u 0.9082 (0.9223) acc_u 9.3750 (10.6250) lr 1.3535e-03 eta 0:00:31
epoch [79/200] batch [10/75] time 0.415 (0.442) data 0.283 (0.311) loss_u loss_u 0.9248 (0.9045) acc_u 9.3750 (12.8125) lr 1.3535e-03 eta 0:00:28
epoch [79/200] batch [15/75] time 0.433 (0.446) data 0.300 (0.315) loss_u loss_u 0.8682 (0.9024) acc_u 18.7500 (13.1250) lr 1.3535e-03 eta 0:00:26
epoch [79/200] batch [20/75] time 0.407 (0.448) data 0.277 (0.317) loss_u loss_u 0.8755 (0.9031) acc_u 12.5000 (12.5000) lr 1.3535e-03 eta 0:00:24
epoch [79/200] batch [25/75] time 0.420 (0.446) data 0.288 (0.314) loss_u loss_u 0.8716 (0.8997) acc_u 12.5000 (12.8750) lr 1.3535e-03 eta 0:00:22
epoch [79/200] batch [30/75] time 0.468 (0.451) data 0.337 (0.320) loss_u loss_u 0.9824 (0.9049) acc_u 0.0000 (12.0833) lr 1.3535e-03 eta 0:00:20
epoch [79/200] batch [35/75] time 0.436 (0.455) data 0.305 (0.324) loss_u loss_u 0.9185 (0.9063) acc_u 9.3750 (11.6071) lr 1.3535e-03 eta 0:00:18
epoch [79/200] batch [40/75] time 0.386 (0.456) data 0.256 (0.325) loss_u loss_u 0.8579 (0.9027) acc_u 15.6250 (12.2656) lr 1.3535e-03 eta 0:00:15
epoch [79/200] batch [45/75] time 0.370 (0.459) data 0.239 (0.328) loss_u loss_u 0.8286 (0.9019) acc_u 25.0000 (12.7083) lr 1.3535e-03 eta 0:00:13
epoch [79/200] batch [50/75] time 0.453 (0.465) data 0.321 (0.334) loss_u loss_u 0.9165 (0.9016) acc_u 9.3750 (12.5625) lr 1.3535e-03 eta 0:00:11
epoch [79/200] batch [55/75] time 0.422 (0.465) data 0.290 (0.334) loss_u loss_u 0.8999 (0.9025) acc_u 12.5000 (12.2727) lr 1.3535e-03 eta 0:00:09
epoch [79/200] batch [60/75] time 0.452 (0.466) data 0.320 (0.335) loss_u loss_u 0.8599 (0.9027) acc_u 15.6250 (12.3438) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [65/75] time 0.410 (0.465) data 0.279 (0.334) loss_u loss_u 0.9092 (0.9022) acc_u 9.3750 (12.2115) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [70/75] time 0.573 (0.465) data 0.441 (0.334) loss_u loss_u 0.9019 (0.9037) acc_u 12.5000 (12.0089) lr 1.3535e-03 eta 0:00:02
epoch [79/200] batch [75/75] time 0.554 (0.467) data 0.423 (0.335) loss_u loss_u 0.9209 (0.9052) acc_u 9.3750 (11.8333) lr 1.3535e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2016
confident_label rate tensor(0.2299, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 721
clean true:373
clean false:348
clean_rate:0.5173370319001387
noisy true:747
noisy false:1668
after delete: len(clean_dataset) 721
after delete: len(noisy_dataset) 2415
epoch [80/200] batch [5/22] time 0.471 (0.436) data 0.341 (0.305) loss_x loss_x 1.0723 (1.1041) acc_x 65.6250 (72.5000) lr 1.3387e-03 eta 0:00:07
epoch [80/200] batch [10/22] time 0.587 (0.471) data 0.456 (0.340) loss_x loss_x 1.2949 (1.3632) acc_x 68.7500 (66.2500) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [15/22] time 0.415 (0.464) data 0.284 (0.333) loss_x loss_x 1.1465 (1.3768) acc_x 75.0000 (65.6250) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [20/22] time 0.397 (0.464) data 0.266 (0.333) loss_x loss_x 1.2627 (1.2881) acc_x 62.5000 (66.5625) lr 1.3387e-03 eta 0:00:00
epoch [80/200] batch [5/75] time 0.370 (0.453) data 0.238 (0.322) loss_u loss_u 0.9072 (0.8984) acc_u 12.5000 (15.6250) lr 1.3387e-03 eta 0:00:31
epoch [80/200] batch [10/75] time 0.460 (0.460) data 0.328 (0.329) loss_u loss_u 0.9639 (0.9196) acc_u 3.1250 (11.5625) lr 1.3387e-03 eta 0:00:29
epoch [80/200] batch [15/75] time 0.422 (0.461) data 0.291 (0.330) loss_u loss_u 0.9043 (0.9182) acc_u 15.6250 (11.0417) lr 1.3387e-03 eta 0:00:27
epoch [80/200] batch [20/75] time 0.487 (0.455) data 0.355 (0.324) loss_u loss_u 0.8550 (0.9166) acc_u 21.8750 (11.0938) lr 1.3387e-03 eta 0:00:25
epoch [80/200] batch [25/75] time 0.376 (0.453) data 0.245 (0.321) loss_u loss_u 0.9536 (0.9132) acc_u 3.1250 (11.1250) lr 1.3387e-03 eta 0:00:22
epoch [80/200] batch [30/75] time 0.440 (0.451) data 0.307 (0.320) loss_u loss_u 0.8843 (0.9147) acc_u 9.3750 (10.8333) lr 1.3387e-03 eta 0:00:20
epoch [80/200] batch [35/75] time 0.468 (0.450) data 0.337 (0.319) loss_u loss_u 0.9219 (0.9118) acc_u 12.5000 (11.3393) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [40/75] time 0.485 (0.450) data 0.354 (0.318) loss_u loss_u 0.9146 (0.9115) acc_u 9.3750 (11.5625) lr 1.3387e-03 eta 0:00:15
epoch [80/200] batch [45/75] time 0.552 (0.450) data 0.420 (0.319) loss_u loss_u 0.8350 (0.9090) acc_u 34.3750 (12.0139) lr 1.3387e-03 eta 0:00:13
epoch [80/200] batch [50/75] time 0.346 (0.451) data 0.213 (0.320) loss_u loss_u 0.9507 (0.9086) acc_u 3.1250 (11.8125) lr 1.3387e-03 eta 0:00:11
epoch [80/200] batch [55/75] time 0.438 (0.452) data 0.307 (0.321) loss_u loss_u 0.8726 (0.9076) acc_u 18.7500 (11.8182) lr 1.3387e-03 eta 0:00:09
epoch [80/200] batch [60/75] time 0.418 (0.450) data 0.287 (0.319) loss_u loss_u 0.9526 (0.9063) acc_u 3.1250 (12.0312) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [65/75] time 0.385 (0.449) data 0.253 (0.318) loss_u loss_u 0.8350 (0.9045) acc_u 18.7500 (12.3077) lr 1.3387e-03 eta 0:00:04
epoch [80/200] batch [70/75] time 0.401 (0.448) data 0.269 (0.316) loss_u loss_u 0.8232 (0.9031) acc_u 28.1250 (12.5446) lr 1.3387e-03 eta 0:00:02
epoch [80/200] batch [75/75] time 0.434 (0.447) data 0.302 (0.315) loss_u loss_u 0.9058 (0.9031) acc_u 9.3750 (12.5000) lr 1.3387e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2019
confident_label rate tensor(0.2379, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 746
clean true:352
clean false:394
clean_rate:0.4718498659517426
noisy true:765
noisy false:1625
after delete: len(clean_dataset) 746
after delete: len(noisy_dataset) 2390
epoch [81/200] batch [5/23] time 0.526 (0.468) data 0.396 (0.338) loss_x loss_x 1.1680 (1.3064) acc_x 78.1250 (64.3750) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [10/23] time 0.460 (0.435) data 0.330 (0.305) loss_x loss_x 1.7568 (1.3804) acc_x 56.2500 (63.1250) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [15/23] time 0.591 (0.438) data 0.461 (0.308) loss_x loss_x 1.3662 (1.4271) acc_x 68.7500 (62.5000) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [20/23] time 0.429 (0.441) data 0.299 (0.311) loss_x loss_x 1.4482 (1.4585) acc_x 56.2500 (60.9375) lr 1.3239e-03 eta 0:00:01
epoch [81/200] batch [5/74] time 0.437 (0.449) data 0.305 (0.319) loss_u loss_u 0.9604 (0.9088) acc_u 6.2500 (14.3750) lr 1.3239e-03 eta 0:00:31
epoch [81/200] batch [10/74] time 0.508 (0.451) data 0.376 (0.321) loss_u loss_u 0.8457 (0.8991) acc_u 21.8750 (13.7500) lr 1.3239e-03 eta 0:00:28
epoch [81/200] batch [15/74] time 0.502 (0.463) data 0.370 (0.332) loss_u loss_u 0.9062 (0.8932) acc_u 12.5000 (13.9583) lr 1.3239e-03 eta 0:00:27
epoch [81/200] batch [20/74] time 0.537 (0.462) data 0.405 (0.331) loss_u loss_u 0.9395 (0.8982) acc_u 9.3750 (13.2812) lr 1.3239e-03 eta 0:00:24
epoch [81/200] batch [25/74] time 0.367 (0.458) data 0.237 (0.328) loss_u loss_u 0.9331 (0.9028) acc_u 3.1250 (12.3750) lr 1.3239e-03 eta 0:00:22
epoch [81/200] batch [30/74] time 0.406 (0.456) data 0.274 (0.325) loss_u loss_u 0.9077 (0.9029) acc_u 9.3750 (12.6042) lr 1.3239e-03 eta 0:00:20
epoch [81/200] batch [35/74] time 0.372 (0.457) data 0.239 (0.326) loss_u loss_u 0.8843 (0.8997) acc_u 18.7500 (13.1250) lr 1.3239e-03 eta 0:00:17
epoch [81/200] batch [40/74] time 0.711 (0.461) data 0.579 (0.329) loss_u loss_u 0.9170 (0.9014) acc_u 9.3750 (12.8125) lr 1.3239e-03 eta 0:00:15
epoch [81/200] batch [45/74] time 0.410 (0.457) data 0.279 (0.326) loss_u loss_u 0.9531 (0.9044) acc_u 6.2500 (12.6389) lr 1.3239e-03 eta 0:00:13
epoch [81/200] batch [50/74] time 0.444 (0.455) data 0.312 (0.324) loss_u loss_u 0.8374 (0.9032) acc_u 21.8750 (12.9375) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [55/74] time 0.444 (0.459) data 0.312 (0.328) loss_u loss_u 0.9644 (0.9033) acc_u 3.1250 (12.7841) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [60/74] time 0.446 (0.460) data 0.314 (0.329) loss_u loss_u 0.9204 (0.9033) acc_u 9.3750 (12.5521) lr 1.3239e-03 eta 0:00:06
epoch [81/200] batch [65/74] time 0.556 (0.462) data 0.424 (0.331) loss_u loss_u 0.9370 (0.9044) acc_u 9.3750 (12.3558) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [70/74] time 0.429 (0.461) data 0.297 (0.329) loss_u loss_u 0.9580 (0.9054) acc_u 6.2500 (12.2768) lr 1.3239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2073
confident_label rate tensor(0.2318, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 727
clean true:360
clean false:367
clean_rate:0.4951856946354883
noisy true:703
noisy false:1706
after delete: len(clean_dataset) 727
after delete: len(noisy_dataset) 2409
epoch [82/200] batch [5/22] time 0.534 (0.492) data 0.402 (0.362) loss_x loss_x 1.2383 (1.5027) acc_x 62.5000 (57.5000) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [10/22] time 0.359 (0.467) data 0.228 (0.336) loss_x loss_x 1.2041 (1.4346) acc_x 68.7500 (60.0000) lr 1.3090e-03 eta 0:00:05
epoch [82/200] batch [15/22] time 0.457 (0.457) data 0.327 (0.327) loss_x loss_x 1.1758 (1.3332) acc_x 62.5000 (62.2917) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [20/22] time 0.539 (0.461) data 0.408 (0.330) loss_x loss_x 1.2422 (1.3243) acc_x 59.3750 (63.7500) lr 1.3090e-03 eta 0:00:00
epoch [82/200] batch [5/75] time 0.713 (0.472) data 0.581 (0.341) loss_u loss_u 0.9507 (0.9327) acc_u 9.3750 (10.0000) lr 1.3090e-03 eta 0:00:33
epoch [82/200] batch [10/75] time 0.377 (0.466) data 0.246 (0.335) loss_u loss_u 0.8062 (0.9142) acc_u 25.0000 (10.9375) lr 1.3090e-03 eta 0:00:30
epoch [82/200] batch [15/75] time 0.444 (0.472) data 0.312 (0.341) loss_u loss_u 0.8970 (0.9110) acc_u 12.5000 (11.6667) lr 1.3090e-03 eta 0:00:28
epoch [82/200] batch [20/75] time 0.433 (0.472) data 0.302 (0.340) loss_u loss_u 0.9688 (0.9121) acc_u 3.1250 (11.2500) lr 1.3090e-03 eta 0:00:25
epoch [82/200] batch [25/75] time 0.467 (0.469) data 0.337 (0.338) loss_u loss_u 0.9526 (0.9127) acc_u 3.1250 (10.7500) lr 1.3090e-03 eta 0:00:23
epoch [82/200] batch [30/75] time 0.436 (0.471) data 0.304 (0.340) loss_u loss_u 0.9731 (0.9116) acc_u 3.1250 (10.9375) lr 1.3090e-03 eta 0:00:21
epoch [82/200] batch [35/75] time 0.517 (0.471) data 0.386 (0.340) loss_u loss_u 0.8584 (0.9090) acc_u 15.6250 (11.4286) lr 1.3090e-03 eta 0:00:18
epoch [82/200] batch [40/75] time 0.492 (0.467) data 0.361 (0.335) loss_u loss_u 0.9243 (0.9079) acc_u 9.3750 (11.5625) lr 1.3090e-03 eta 0:00:16
epoch [82/200] batch [45/75] time 0.606 (0.469) data 0.475 (0.338) loss_u loss_u 0.9185 (0.9070) acc_u 9.3750 (11.8056) lr 1.3090e-03 eta 0:00:14
epoch [82/200] batch [50/75] time 0.423 (0.467) data 0.292 (0.336) loss_u loss_u 0.9551 (0.9059) acc_u 6.2500 (11.8750) lr 1.3090e-03 eta 0:00:11
epoch [82/200] batch [55/75] time 0.375 (0.465) data 0.245 (0.334) loss_u loss_u 0.8403 (0.9051) acc_u 18.7500 (11.8750) lr 1.3090e-03 eta 0:00:09
epoch [82/200] batch [60/75] time 0.326 (0.461) data 0.194 (0.330) loss_u loss_u 0.9282 (0.9051) acc_u 12.5000 (11.9792) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [65/75] time 0.425 (0.460) data 0.294 (0.329) loss_u loss_u 0.9355 (0.9071) acc_u 9.3750 (11.6346) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [70/75] time 0.434 (0.460) data 0.303 (0.329) loss_u loss_u 0.9443 (0.9059) acc_u 9.3750 (11.7857) lr 1.3090e-03 eta 0:00:02
epoch [82/200] batch [75/75] time 0.521 (0.458) data 0.389 (0.327) loss_u loss_u 0.9253 (0.9043) acc_u 9.3750 (12.0000) lr 1.3090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2058
confident_label rate tensor(0.2296, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 720
clean true:350
clean false:370
clean_rate:0.4861111111111111
noisy true:728
noisy false:1688
after delete: len(clean_dataset) 720
after delete: len(noisy_dataset) 2416
epoch [83/200] batch [5/22] time 0.457 (0.451) data 0.325 (0.320) loss_x loss_x 1.7598 (1.4441) acc_x 59.3750 (63.1250) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [10/22] time 0.388 (0.429) data 0.257 (0.298) loss_x loss_x 1.5107 (1.4283) acc_x 62.5000 (65.3125) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [15/22] time 0.619 (0.455) data 0.489 (0.324) loss_x loss_x 1.1484 (1.4856) acc_x 75.0000 (64.3750) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [20/22] time 0.436 (0.454) data 0.305 (0.323) loss_x loss_x 1.2822 (1.4421) acc_x 71.8750 (65.3125) lr 1.2940e-03 eta 0:00:00
epoch [83/200] batch [5/75] time 0.412 (0.454) data 0.280 (0.323) loss_u loss_u 0.8550 (0.8814) acc_u 18.7500 (16.2500) lr 1.2940e-03 eta 0:00:31
epoch [83/200] batch [10/75] time 0.393 (0.455) data 0.261 (0.324) loss_u loss_u 0.8447 (0.8789) acc_u 18.7500 (16.2500) lr 1.2940e-03 eta 0:00:29
epoch [83/200] batch [15/75] time 0.417 (0.449) data 0.285 (0.318) loss_u loss_u 0.9048 (0.8896) acc_u 12.5000 (14.1667) lr 1.2940e-03 eta 0:00:26
epoch [83/200] batch [20/75] time 0.403 (0.451) data 0.272 (0.319) loss_u loss_u 0.8320 (0.8919) acc_u 28.1250 (14.2188) lr 1.2940e-03 eta 0:00:24
epoch [83/200] batch [25/75] time 0.476 (0.455) data 0.344 (0.324) loss_u loss_u 0.8643 (0.8911) acc_u 12.5000 (14.5000) lr 1.2940e-03 eta 0:00:22
epoch [83/200] batch [30/75] time 0.408 (0.458) data 0.276 (0.327) loss_u loss_u 0.9170 (0.8902) acc_u 9.3750 (14.4792) lr 1.2940e-03 eta 0:00:20
epoch [83/200] batch [35/75] time 0.439 (0.464) data 0.308 (0.332) loss_u loss_u 0.8936 (0.8930) acc_u 12.5000 (13.9286) lr 1.2940e-03 eta 0:00:18
epoch [83/200] batch [40/75] time 0.607 (0.469) data 0.475 (0.337) loss_u loss_u 0.9043 (0.8925) acc_u 9.3750 (13.9844) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [45/75] time 0.401 (0.465) data 0.270 (0.333) loss_u loss_u 0.8984 (0.8926) acc_u 12.5000 (13.8889) lr 1.2940e-03 eta 0:00:13
epoch [83/200] batch [50/75] time 0.558 (0.469) data 0.427 (0.337) loss_u loss_u 0.9321 (0.8954) acc_u 6.2500 (13.6250) lr 1.2940e-03 eta 0:00:11
epoch [83/200] batch [55/75] time 0.420 (0.469) data 0.286 (0.337) loss_u loss_u 0.9277 (0.8977) acc_u 9.3750 (13.1250) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [60/75] time 0.440 (0.474) data 0.308 (0.342) loss_u loss_u 0.9731 (0.8973) acc_u 3.1250 (13.3333) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [65/75] time 0.407 (0.474) data 0.276 (0.342) loss_u loss_u 0.9243 (0.8981) acc_u 12.5000 (13.2212) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [70/75] time 0.414 (0.471) data 0.283 (0.339) loss_u loss_u 0.8809 (0.8991) acc_u 15.6250 (12.9911) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [75/75] time 0.532 (0.473) data 0.400 (0.341) loss_u loss_u 0.9580 (0.9008) acc_u 3.1250 (12.5833) lr 1.2940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2092
confident_label rate tensor(0.2309, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 724
clean true:341
clean false:383
clean_rate:0.47099447513812154
noisy true:703
noisy false:1709
after delete: len(clean_dataset) 724
after delete: len(noisy_dataset) 2412
epoch [84/200] batch [5/22] time 0.552 (0.536) data 0.422 (0.406) loss_x loss_x 1.5029 (1.5947) acc_x 65.6250 (65.6250) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [10/22] time 0.343 (0.495) data 0.212 (0.364) loss_x loss_x 1.2861 (1.5183) acc_x 65.6250 (64.3750) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [15/22] time 0.445 (0.490) data 0.314 (0.359) loss_x loss_x 1.0537 (1.4999) acc_x 78.1250 (65.0000) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [20/22] time 0.606 (0.488) data 0.475 (0.358) loss_x loss_x 1.0420 (1.4472) acc_x 75.0000 (65.6250) lr 1.2790e-03 eta 0:00:00
epoch [84/200] batch [5/75] time 0.372 (0.474) data 0.240 (0.343) loss_u loss_u 0.8643 (0.8600) acc_u 12.5000 (17.5000) lr 1.2790e-03 eta 0:00:33
epoch [84/200] batch [10/75] time 0.456 (0.475) data 0.323 (0.343) loss_u loss_u 0.9346 (0.8807) acc_u 12.5000 (15.3125) lr 1.2790e-03 eta 0:00:30
epoch [84/200] batch [15/75] time 0.545 (0.473) data 0.413 (0.342) loss_u loss_u 0.9185 (0.8875) acc_u 9.3750 (14.1667) lr 1.2790e-03 eta 0:00:28
epoch [84/200] batch [20/75] time 0.367 (0.462) data 0.235 (0.331) loss_u loss_u 0.8960 (0.8916) acc_u 15.6250 (13.5938) lr 1.2790e-03 eta 0:00:25
epoch [84/200] batch [25/75] time 0.388 (0.460) data 0.256 (0.329) loss_u loss_u 0.8726 (0.8881) acc_u 12.5000 (14.2500) lr 1.2790e-03 eta 0:00:22
epoch [84/200] batch [30/75] time 0.470 (0.461) data 0.340 (0.330) loss_u loss_u 0.8784 (0.8912) acc_u 12.5000 (13.6458) lr 1.2790e-03 eta 0:00:20
epoch [84/200] batch [35/75] time 0.368 (0.458) data 0.238 (0.327) loss_u loss_u 0.9009 (0.8912) acc_u 12.5000 (13.6607) lr 1.2790e-03 eta 0:00:18
epoch [84/200] batch [40/75] time 0.415 (0.455) data 0.283 (0.323) loss_u loss_u 0.9536 (0.8947) acc_u 3.1250 (13.3594) lr 1.2790e-03 eta 0:00:15
epoch [84/200] batch [45/75] time 0.450 (0.452) data 0.319 (0.321) loss_u loss_u 0.9072 (0.8976) acc_u 9.3750 (13.0556) lr 1.2790e-03 eta 0:00:13
epoch [84/200] batch [50/75] time 0.434 (0.454) data 0.302 (0.323) loss_u loss_u 0.8853 (0.8963) acc_u 18.7500 (13.4375) lr 1.2790e-03 eta 0:00:11
epoch [84/200] batch [55/75] time 0.418 (0.451) data 0.286 (0.320) loss_u loss_u 0.9209 (0.8969) acc_u 9.3750 (13.2386) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [60/75] time 0.668 (0.454) data 0.536 (0.323) loss_u loss_u 0.9097 (0.8990) acc_u 6.2500 (12.9167) lr 1.2790e-03 eta 0:00:06
epoch [84/200] batch [65/75] time 0.604 (0.457) data 0.473 (0.326) loss_u loss_u 0.8892 (0.8982) acc_u 15.6250 (12.9327) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [70/75] time 0.478 (0.459) data 0.346 (0.328) loss_u loss_u 0.9116 (0.8985) acc_u 9.3750 (12.9018) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [75/75] time 0.473 (0.457) data 0.342 (0.326) loss_u loss_u 0.9233 (0.8984) acc_u 9.3750 (12.7917) lr 1.2790e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2083
confident_label rate tensor(0.2213, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 694
clean true:351
clean false:343
clean_rate:0.5057636887608069
noisy true:702
noisy false:1740
after delete: len(clean_dataset) 694
after delete: len(noisy_dataset) 2442
epoch [85/200] batch [5/21] time 0.588 (0.428) data 0.458 (0.298) loss_x loss_x 1.4004 (1.2770) acc_x 75.0000 (70.6250) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [10/21] time 0.514 (0.434) data 0.383 (0.304) loss_x loss_x 0.5288 (1.2706) acc_x 84.3750 (69.6875) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [15/21] time 0.483 (0.441) data 0.352 (0.311) loss_x loss_x 1.3457 (1.2915) acc_x 71.8750 (67.7083) lr 1.2639e-03 eta 0:00:02
epoch [85/200] batch [20/21] time 0.386 (0.444) data 0.255 (0.313) loss_x loss_x 1.9502 (1.3819) acc_x 56.2500 (66.2500) lr 1.2639e-03 eta 0:00:00
epoch [85/200] batch [5/76] time 0.375 (0.438) data 0.244 (0.307) loss_u loss_u 0.8931 (0.8930) acc_u 15.6250 (13.7500) lr 1.2639e-03 eta 0:00:31
epoch [85/200] batch [10/76] time 0.357 (0.441) data 0.226 (0.310) loss_u loss_u 0.9004 (0.9052) acc_u 9.3750 (11.5625) lr 1.2639e-03 eta 0:00:29
epoch [85/200] batch [15/76] time 0.416 (0.441) data 0.284 (0.309) loss_u loss_u 0.9189 (0.9095) acc_u 6.2500 (10.8333) lr 1.2639e-03 eta 0:00:26
epoch [85/200] batch [20/76] time 0.493 (0.444) data 0.361 (0.313) loss_u loss_u 0.8906 (0.9089) acc_u 12.5000 (11.0938) lr 1.2639e-03 eta 0:00:24
epoch [85/200] batch [25/76] time 0.449 (0.441) data 0.317 (0.310) loss_u loss_u 0.7993 (0.9048) acc_u 31.2500 (11.7500) lr 1.2639e-03 eta 0:00:22
epoch [85/200] batch [30/76] time 0.535 (0.442) data 0.403 (0.311) loss_u loss_u 0.8887 (0.9006) acc_u 9.3750 (11.8750) lr 1.2639e-03 eta 0:00:20
epoch [85/200] batch [35/76] time 0.451 (0.447) data 0.319 (0.316) loss_u loss_u 0.8647 (0.9007) acc_u 21.8750 (12.2321) lr 1.2639e-03 eta 0:00:18
epoch [85/200] batch [40/76] time 0.517 (0.448) data 0.385 (0.316) loss_u loss_u 0.8657 (0.8992) acc_u 15.6250 (12.8906) lr 1.2639e-03 eta 0:00:16
epoch [85/200] batch [45/76] time 0.383 (0.447) data 0.251 (0.316) loss_u loss_u 0.9028 (0.8982) acc_u 9.3750 (13.0556) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [50/76] time 0.413 (0.446) data 0.281 (0.315) loss_u loss_u 0.9487 (0.9000) acc_u 3.1250 (12.7500) lr 1.2639e-03 eta 0:00:11
epoch [85/200] batch [55/76] time 0.520 (0.453) data 0.388 (0.322) loss_u loss_u 0.8906 (0.8989) acc_u 12.5000 (12.7841) lr 1.2639e-03 eta 0:00:09
epoch [85/200] batch [60/76] time 0.467 (0.452) data 0.335 (0.321) loss_u loss_u 0.8765 (0.8996) acc_u 21.8750 (12.8125) lr 1.2639e-03 eta 0:00:07
epoch [85/200] batch [65/76] time 0.442 (0.449) data 0.310 (0.318) loss_u loss_u 0.9175 (0.8996) acc_u 15.6250 (13.0288) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [70/76] time 0.444 (0.448) data 0.312 (0.317) loss_u loss_u 0.9238 (0.8989) acc_u 15.6250 (13.2589) lr 1.2639e-03 eta 0:00:02
epoch [85/200] batch [75/76] time 0.362 (0.448) data 0.230 (0.317) loss_u loss_u 0.9224 (0.8984) acc_u 9.3750 (13.1250) lr 1.2639e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2054
confident_label rate tensor(0.2270, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 712
clean true:351
clean false:361
clean_rate:0.49297752808988765
noisy true:731
noisy false:1693
after delete: len(clean_dataset) 712
after delete: len(noisy_dataset) 2424
epoch [86/200] batch [5/22] time 0.600 (0.491) data 0.469 (0.360) loss_x loss_x 1.3789 (1.2133) acc_x 71.8750 (68.7500) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [10/22] time 0.550 (0.476) data 0.419 (0.345) loss_x loss_x 1.2842 (1.2700) acc_x 68.7500 (69.3750) lr 1.2487e-03 eta 0:00:05
epoch [86/200] batch [15/22] time 0.413 (0.470) data 0.283 (0.339) loss_x loss_x 2.0898 (1.3276) acc_x 43.7500 (67.2917) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [20/22] time 0.666 (0.501) data 0.533 (0.370) loss_x loss_x 1.1924 (1.3094) acc_x 65.6250 (67.6562) lr 1.2487e-03 eta 0:00:01
epoch [86/200] batch [5/75] time 0.427 (0.486) data 0.294 (0.355) loss_u loss_u 0.8994 (0.9070) acc_u 12.5000 (10.6250) lr 1.2487e-03 eta 0:00:34
epoch [86/200] batch [10/75] time 0.596 (0.486) data 0.464 (0.355) loss_u loss_u 0.9067 (0.9106) acc_u 12.5000 (10.6250) lr 1.2487e-03 eta 0:00:31
epoch [86/200] batch [15/75] time 0.452 (0.479) data 0.321 (0.347) loss_u loss_u 0.9014 (0.9101) acc_u 9.3750 (11.8750) lr 1.2487e-03 eta 0:00:28
epoch [86/200] batch [20/75] time 0.636 (0.482) data 0.502 (0.350) loss_u loss_u 0.8936 (0.9014) acc_u 12.5000 (13.2812) lr 1.2487e-03 eta 0:00:26
epoch [86/200] batch [25/75] time 0.450 (0.477) data 0.318 (0.345) loss_u loss_u 0.9175 (0.9037) acc_u 6.2500 (12.5000) lr 1.2487e-03 eta 0:00:23
epoch [86/200] batch [30/75] time 0.404 (0.475) data 0.271 (0.343) loss_u loss_u 0.9082 (0.9035) acc_u 9.3750 (12.3958) lr 1.2487e-03 eta 0:00:21
epoch [86/200] batch [35/75] time 0.452 (0.473) data 0.319 (0.342) loss_u loss_u 0.9077 (0.9026) acc_u 9.3750 (12.2321) lr 1.2487e-03 eta 0:00:18
epoch [86/200] batch [40/75] time 0.374 (0.467) data 0.242 (0.335) loss_u loss_u 0.9370 (0.9057) acc_u 6.2500 (11.7188) lr 1.2487e-03 eta 0:00:16
epoch [86/200] batch [45/75] time 0.415 (0.466) data 0.283 (0.334) loss_u loss_u 0.9048 (0.9033) acc_u 12.5000 (12.0833) lr 1.2487e-03 eta 0:00:13
epoch [86/200] batch [50/75] time 0.456 (0.465) data 0.325 (0.333) loss_u loss_u 0.8335 (0.9044) acc_u 21.8750 (12.0625) lr 1.2487e-03 eta 0:00:11
epoch [86/200] batch [55/75] time 0.496 (0.465) data 0.364 (0.333) loss_u loss_u 0.8530 (0.9042) acc_u 12.5000 (11.8182) lr 1.2487e-03 eta 0:00:09
epoch [86/200] batch [60/75] time 0.534 (0.468) data 0.402 (0.336) loss_u loss_u 0.8662 (0.9041) acc_u 12.5000 (11.9271) lr 1.2487e-03 eta 0:00:07
epoch [86/200] batch [65/75] time 0.397 (0.467) data 0.265 (0.335) loss_u loss_u 0.9453 (0.9032) acc_u 6.2500 (12.0673) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [70/75] time 0.517 (0.469) data 0.383 (0.337) loss_u loss_u 0.8462 (0.9020) acc_u 21.8750 (12.4107) lr 1.2487e-03 eta 0:00:02
epoch [86/200] batch [75/75] time 0.632 (0.471) data 0.501 (0.339) loss_u loss_u 0.8516 (0.9012) acc_u 21.8750 (12.5833) lr 1.2487e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2039
confident_label rate tensor(0.2459, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 771
clean true:377
clean false:394
clean_rate:0.4889753566796368
noisy true:720
noisy false:1645
after delete: len(clean_dataset) 771
after delete: len(noisy_dataset) 2365
epoch [87/200] batch [5/24] time 0.423 (0.442) data 0.292 (0.311) loss_x loss_x 1.1641 (1.2247) acc_x 75.0000 (70.6250) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [10/24] time 0.408 (0.421) data 0.277 (0.290) loss_x loss_x 1.3857 (1.2702) acc_x 71.8750 (70.3125) lr 1.2334e-03 eta 0:00:05
epoch [87/200] batch [15/24] time 0.645 (0.457) data 0.514 (0.326) loss_x loss_x 1.1318 (1.2809) acc_x 78.1250 (68.3333) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [20/24] time 0.398 (0.476) data 0.267 (0.345) loss_x loss_x 1.1787 (1.2807) acc_x 68.7500 (67.9688) lr 1.2334e-03 eta 0:00:01
epoch [87/200] batch [5/73] time 0.422 (0.465) data 0.292 (0.334) loss_u loss_u 0.9287 (0.8991) acc_u 9.3750 (13.1250) lr 1.2334e-03 eta 0:00:31
epoch [87/200] batch [10/73] time 0.438 (0.472) data 0.307 (0.341) loss_u loss_u 0.8940 (0.9044) acc_u 12.5000 (12.5000) lr 1.2334e-03 eta 0:00:29
epoch [87/200] batch [15/73] time 0.565 (0.476) data 0.434 (0.345) loss_u loss_u 0.9233 (0.9075) acc_u 6.2500 (11.8750) lr 1.2334e-03 eta 0:00:27
epoch [87/200] batch [20/73] time 0.421 (0.475) data 0.290 (0.344) loss_u loss_u 0.8467 (0.9049) acc_u 28.1250 (12.5000) lr 1.2334e-03 eta 0:00:25
epoch [87/200] batch [25/73] time 0.449 (0.472) data 0.318 (0.341) loss_u loss_u 0.8530 (0.8998) acc_u 21.8750 (13.3750) lr 1.2334e-03 eta 0:00:22
epoch [87/200] batch [30/73] time 0.454 (0.471) data 0.322 (0.339) loss_u loss_u 0.9111 (0.9007) acc_u 15.6250 (13.4375) lr 1.2334e-03 eta 0:00:20
epoch [87/200] batch [35/73] time 0.453 (0.465) data 0.323 (0.334) loss_u loss_u 0.9297 (0.9041) acc_u 12.5000 (12.8571) lr 1.2334e-03 eta 0:00:17
epoch [87/200] batch [40/73] time 0.481 (0.463) data 0.350 (0.332) loss_u loss_u 0.9214 (0.9056) acc_u 9.3750 (12.5000) lr 1.2334e-03 eta 0:00:15
epoch [87/200] batch [45/73] time 0.471 (0.471) data 0.339 (0.340) loss_u loss_u 0.8306 (0.9052) acc_u 18.7500 (12.2222) lr 1.2334e-03 eta 0:00:13
epoch [87/200] batch [50/73] time 0.413 (0.469) data 0.283 (0.338) loss_u loss_u 0.9385 (0.9066) acc_u 9.3750 (12.1875) lr 1.2334e-03 eta 0:00:10
epoch [87/200] batch [55/73] time 0.449 (0.467) data 0.318 (0.335) loss_u loss_u 0.8735 (0.9014) acc_u 25.0000 (13.2386) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [60/73] time 0.392 (0.464) data 0.262 (0.333) loss_u loss_u 0.9092 (0.9026) acc_u 12.5000 (13.0208) lr 1.2334e-03 eta 0:00:06
epoch [87/200] batch [65/73] time 0.636 (0.467) data 0.505 (0.335) loss_u loss_u 0.8633 (0.9022) acc_u 18.7500 (12.9327) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [70/73] time 0.397 (0.467) data 0.266 (0.335) loss_u loss_u 0.8809 (0.9017) acc_u 12.5000 (12.9018) lr 1.2334e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2024
confident_label rate tensor(0.2353, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 738
clean true:368
clean false:370
clean_rate:0.4986449864498645
noisy true:744
noisy false:1654
after delete: len(clean_dataset) 738
after delete: len(noisy_dataset) 2398
epoch [88/200] batch [5/23] time 0.354 (0.471) data 0.223 (0.340) loss_x loss_x 1.5732 (1.3725) acc_x 71.8750 (73.1250) lr 1.2181e-03 eta 0:00:08
epoch [88/200] batch [10/23] time 0.398 (0.454) data 0.266 (0.323) loss_x loss_x 1.7949 (1.5044) acc_x 46.8750 (66.5625) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [15/23] time 0.522 (0.456) data 0.388 (0.325) loss_x loss_x 1.2588 (1.4421) acc_x 62.5000 (65.8333) lr 1.2181e-03 eta 0:00:03
epoch [88/200] batch [20/23] time 0.635 (0.479) data 0.505 (0.348) loss_x loss_x 1.1436 (1.4010) acc_x 71.8750 (66.0938) lr 1.2181e-03 eta 0:00:01
epoch [88/200] batch [5/74] time 0.315 (0.451) data 0.184 (0.320) loss_u loss_u 0.8970 (0.8986) acc_u 9.3750 (12.5000) lr 1.2181e-03 eta 0:00:31
epoch [88/200] batch [10/74] time 0.457 (0.447) data 0.324 (0.316) loss_u loss_u 0.9341 (0.8982) acc_u 6.2500 (11.8750) lr 1.2181e-03 eta 0:00:28
epoch [88/200] batch [15/74] time 0.427 (0.449) data 0.297 (0.318) loss_u loss_u 0.8882 (0.9062) acc_u 15.6250 (11.4583) lr 1.2181e-03 eta 0:00:26
epoch [88/200] batch [20/74] time 0.392 (0.446) data 0.261 (0.315) loss_u loss_u 0.9570 (0.9072) acc_u 3.1250 (11.5625) lr 1.2181e-03 eta 0:00:24
epoch [88/200] batch [25/74] time 0.437 (0.453) data 0.305 (0.322) loss_u loss_u 0.8784 (0.9053) acc_u 15.6250 (12.1250) lr 1.2181e-03 eta 0:00:22
epoch [88/200] batch [30/74] time 0.390 (0.453) data 0.258 (0.322) loss_u loss_u 0.8745 (0.9009) acc_u 18.7500 (12.7083) lr 1.2181e-03 eta 0:00:19
epoch [88/200] batch [35/74] time 0.422 (0.463) data 0.291 (0.332) loss_u loss_u 0.8892 (0.9027) acc_u 12.5000 (12.5000) lr 1.2181e-03 eta 0:00:18
epoch [88/200] batch [40/74] time 0.470 (0.460) data 0.338 (0.329) loss_u loss_u 0.8862 (0.9016) acc_u 21.8750 (13.0469) lr 1.2181e-03 eta 0:00:15
epoch [88/200] batch [45/74] time 0.483 (0.457) data 0.352 (0.326) loss_u loss_u 0.9556 (0.9040) acc_u 6.2500 (12.7083) lr 1.2181e-03 eta 0:00:13
epoch [88/200] batch [50/74] time 0.350 (0.454) data 0.219 (0.323) loss_u loss_u 0.9351 (0.9060) acc_u 9.3750 (12.2500) lr 1.2181e-03 eta 0:00:10
epoch [88/200] batch [55/74] time 0.379 (0.453) data 0.248 (0.322) loss_u loss_u 0.8657 (0.9046) acc_u 15.6250 (12.3295) lr 1.2181e-03 eta 0:00:08
epoch [88/200] batch [60/74] time 0.326 (0.453) data 0.196 (0.321) loss_u loss_u 0.9277 (0.9063) acc_u 9.3750 (12.0833) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [65/74] time 0.362 (0.450) data 0.231 (0.319) loss_u loss_u 0.8643 (0.9048) acc_u 18.7500 (12.3077) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [70/74] time 0.495 (0.450) data 0.365 (0.318) loss_u loss_u 0.9170 (0.9035) acc_u 9.3750 (12.5446) lr 1.2181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2048
confident_label rate tensor(0.2395, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 751
clean true:364
clean false:387
clean_rate:0.48468708388814913
noisy true:724
noisy false:1661
after delete: len(clean_dataset) 751
after delete: len(noisy_dataset) 2385
epoch [89/200] batch [5/23] time 0.388 (0.449) data 0.258 (0.318) loss_x loss_x 1.0693 (1.1906) acc_x 68.7500 (69.3750) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [10/23] time 0.420 (0.438) data 0.289 (0.308) loss_x loss_x 1.5244 (1.3028) acc_x 59.3750 (67.5000) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [15/23] time 0.373 (0.441) data 0.242 (0.311) loss_x loss_x 2.0898 (1.4167) acc_x 40.6250 (63.9583) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [20/23] time 0.350 (0.444) data 0.219 (0.314) loss_x loss_x 1.0811 (1.3591) acc_x 75.0000 (64.3750) lr 1.2028e-03 eta 0:00:01
epoch [89/200] batch [5/74] time 0.368 (0.431) data 0.237 (0.301) loss_u loss_u 0.8770 (0.9054) acc_u 18.7500 (13.1250) lr 1.2028e-03 eta 0:00:29
epoch [89/200] batch [10/74] time 0.395 (0.431) data 0.264 (0.300) loss_u loss_u 0.9277 (0.9276) acc_u 6.2500 (8.4375) lr 1.2028e-03 eta 0:00:27
epoch [89/200] batch [15/74] time 0.358 (0.429) data 0.228 (0.298) loss_u loss_u 0.9189 (0.9175) acc_u 18.7500 (10.6250) lr 1.2028e-03 eta 0:00:25
epoch [89/200] batch [20/74] time 0.403 (0.430) data 0.271 (0.299) loss_u loss_u 0.9604 (0.9202) acc_u 6.2500 (10.3125) lr 1.2028e-03 eta 0:00:23
epoch [89/200] batch [25/74] time 0.493 (0.431) data 0.361 (0.300) loss_u loss_u 0.8540 (0.9090) acc_u 15.6250 (11.7500) lr 1.2028e-03 eta 0:00:21
epoch [89/200] batch [30/74] time 0.381 (0.428) data 0.250 (0.297) loss_u loss_u 0.8584 (0.9089) acc_u 15.6250 (11.5625) lr 1.2028e-03 eta 0:00:18
epoch [89/200] batch [35/74] time 0.573 (0.432) data 0.441 (0.301) loss_u loss_u 0.8677 (0.9092) acc_u 15.6250 (11.2500) lr 1.2028e-03 eta 0:00:16
epoch [89/200] batch [40/74] time 0.430 (0.430) data 0.299 (0.299) loss_u loss_u 0.8672 (0.9035) acc_u 15.6250 (12.1094) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [45/74] time 0.371 (0.437) data 0.241 (0.306) loss_u loss_u 0.9746 (0.9055) acc_u 3.1250 (12.0139) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [50/74] time 0.382 (0.442) data 0.251 (0.311) loss_u loss_u 0.8887 (0.9061) acc_u 12.5000 (11.8125) lr 1.2028e-03 eta 0:00:10
epoch [89/200] batch [55/74] time 0.439 (0.450) data 0.307 (0.319) loss_u loss_u 0.9170 (0.9070) acc_u 6.2500 (11.7045) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [60/74] time 0.370 (0.451) data 0.240 (0.320) loss_u loss_u 0.9370 (0.9079) acc_u 9.3750 (11.5104) lr 1.2028e-03 eta 0:00:06
epoch [89/200] batch [65/74] time 0.439 (0.454) data 0.308 (0.322) loss_u loss_u 0.8379 (0.9064) acc_u 18.7500 (11.6827) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [70/74] time 0.575 (0.459) data 0.444 (0.328) loss_u loss_u 0.9238 (0.9033) acc_u 9.3750 (12.0089) lr 1.2028e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2032
confident_label rate tensor(0.2309, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 724
clean true:351
clean false:373
clean_rate:0.48480662983425415
noisy true:753
noisy false:1659
after delete: len(clean_dataset) 724
after delete: len(noisy_dataset) 2412
epoch [90/200] batch [5/22] time 0.368 (0.483) data 0.238 (0.352) loss_x loss_x 1.2695 (1.1288) acc_x 71.8750 (73.1250) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [10/22] time 0.483 (0.454) data 0.352 (0.324) loss_x loss_x 1.8457 (1.1836) acc_x 59.3750 (69.3750) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [15/22] time 0.462 (0.463) data 0.330 (0.333) loss_x loss_x 1.2432 (1.1888) acc_x 62.5000 (68.9583) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [20/22] time 0.394 (0.459) data 0.263 (0.328) loss_x loss_x 1.3955 (1.2601) acc_x 50.0000 (67.1875) lr 1.1874e-03 eta 0:00:00
epoch [90/200] batch [5/75] time 0.434 (0.457) data 0.302 (0.326) loss_u loss_u 0.8096 (0.8852) acc_u 21.8750 (13.7500) lr 1.1874e-03 eta 0:00:32
epoch [90/200] batch [10/75] time 0.467 (0.463) data 0.336 (0.332) loss_u loss_u 0.9756 (0.9125) acc_u 0.0000 (10.6250) lr 1.1874e-03 eta 0:00:30
epoch [90/200] batch [15/75] time 0.464 (0.460) data 0.333 (0.329) loss_u loss_u 0.8921 (0.9031) acc_u 15.6250 (12.2917) lr 1.1874e-03 eta 0:00:27
epoch [90/200] batch [20/75] time 0.459 (0.464) data 0.328 (0.333) loss_u loss_u 0.9058 (0.9004) acc_u 9.3750 (12.3438) lr 1.1874e-03 eta 0:00:25
epoch [90/200] batch [25/75] time 0.521 (0.463) data 0.390 (0.332) loss_u loss_u 0.9365 (0.9017) acc_u 3.1250 (11.8750) lr 1.1874e-03 eta 0:00:23
epoch [90/200] batch [30/75] time 0.396 (0.460) data 0.265 (0.330) loss_u loss_u 0.8374 (0.8997) acc_u 18.7500 (12.0833) lr 1.1874e-03 eta 0:00:20
epoch [90/200] batch [35/75] time 0.457 (0.462) data 0.324 (0.331) loss_u loss_u 0.9365 (0.9000) acc_u 6.2500 (12.1429) lr 1.1874e-03 eta 0:00:18
epoch [90/200] batch [40/75] time 0.410 (0.463) data 0.277 (0.332) loss_u loss_u 0.9160 (0.9026) acc_u 9.3750 (11.6406) lr 1.1874e-03 eta 0:00:16
epoch [90/200] batch [45/75] time 0.495 (0.462) data 0.363 (0.331) loss_u loss_u 0.9146 (0.9032) acc_u 12.5000 (11.7361) lr 1.1874e-03 eta 0:00:13
epoch [90/200] batch [50/75] time 0.538 (0.469) data 0.408 (0.338) loss_u loss_u 0.9028 (0.9031) acc_u 15.6250 (12.0000) lr 1.1874e-03 eta 0:00:11
epoch [90/200] batch [55/75] time 0.428 (0.465) data 0.297 (0.334) loss_u loss_u 0.8906 (0.9028) acc_u 18.7500 (12.2159) lr 1.1874e-03 eta 0:00:09
epoch [90/200] batch [60/75] time 0.401 (0.465) data 0.270 (0.334) loss_u loss_u 0.8193 (0.9027) acc_u 25.0000 (12.1354) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [65/75] time 0.458 (0.464) data 0.327 (0.333) loss_u loss_u 0.9141 (0.9037) acc_u 9.3750 (12.0192) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [70/75] time 0.635 (0.467) data 0.503 (0.336) loss_u loss_u 0.8936 (0.9036) acc_u 18.7500 (12.2321) lr 1.1874e-03 eta 0:00:02
epoch [90/200] batch [75/75] time 0.566 (0.469) data 0.434 (0.338) loss_u loss_u 0.9380 (0.9026) acc_u 6.2500 (12.2083) lr 1.1874e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2088
confident_label rate tensor(0.2293, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 719
clean true:342
clean false:377
clean_rate:0.4756606397774687
noisy true:706
noisy false:1711
after delete: len(clean_dataset) 719
after delete: len(noisy_dataset) 2417
epoch [91/200] batch [5/22] time 0.398 (0.480) data 0.266 (0.349) loss_x loss_x 1.3945 (1.2711) acc_x 56.2500 (66.2500) lr 1.1719e-03 eta 0:00:08
epoch [91/200] batch [10/22] time 0.418 (0.468) data 0.287 (0.338) loss_x loss_x 1.0537 (1.3103) acc_x 65.6250 (67.8125) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [15/22] time 0.448 (0.446) data 0.317 (0.315) loss_x loss_x 1.2832 (1.3246) acc_x 68.7500 (65.8333) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [20/22] time 0.375 (0.445) data 0.245 (0.315) loss_x loss_x 0.9678 (1.2535) acc_x 75.0000 (68.5938) lr 1.1719e-03 eta 0:00:00
epoch [91/200] batch [5/75] time 0.405 (0.447) data 0.273 (0.317) loss_u loss_u 0.9272 (0.8882) acc_u 6.2500 (14.3750) lr 1.1719e-03 eta 0:00:31
epoch [91/200] batch [10/75] time 0.377 (0.441) data 0.246 (0.310) loss_u loss_u 0.9663 (0.9005) acc_u 0.0000 (12.1875) lr 1.1719e-03 eta 0:00:28
epoch [91/200] batch [15/75] time 0.537 (0.444) data 0.406 (0.313) loss_u loss_u 0.9248 (0.9046) acc_u 9.3750 (11.2500) lr 1.1719e-03 eta 0:00:26
epoch [91/200] batch [20/75] time 0.384 (0.444) data 0.253 (0.314) loss_u loss_u 0.9170 (0.9055) acc_u 9.3750 (11.0938) lr 1.1719e-03 eta 0:00:24
epoch [91/200] batch [25/75] time 0.433 (0.448) data 0.301 (0.317) loss_u loss_u 0.9087 (0.9059) acc_u 9.3750 (11.3750) lr 1.1719e-03 eta 0:00:22
epoch [91/200] batch [30/75] time 0.473 (0.450) data 0.342 (0.320) loss_u loss_u 0.9155 (0.9046) acc_u 6.2500 (11.3542) lr 1.1719e-03 eta 0:00:20
epoch [91/200] batch [35/75] time 0.425 (0.457) data 0.293 (0.326) loss_u loss_u 0.8120 (0.8996) acc_u 15.6250 (11.7857) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [40/75] time 0.486 (0.459) data 0.354 (0.328) loss_u loss_u 0.8677 (0.9006) acc_u 21.8750 (11.7969) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [45/75] time 0.432 (0.457) data 0.300 (0.326) loss_u loss_u 0.9316 (0.9005) acc_u 9.3750 (11.8056) lr 1.1719e-03 eta 0:00:13
epoch [91/200] batch [50/75] time 0.413 (0.455) data 0.281 (0.324) loss_u loss_u 0.8823 (0.9016) acc_u 15.6250 (11.5625) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [55/75] time 0.416 (0.456) data 0.285 (0.325) loss_u loss_u 0.9111 (0.9023) acc_u 9.3750 (11.5341) lr 1.1719e-03 eta 0:00:09
epoch [91/200] batch [60/75] time 0.606 (0.456) data 0.473 (0.325) loss_u loss_u 0.8242 (0.9002) acc_u 21.8750 (11.9271) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [65/75] time 0.364 (0.456) data 0.232 (0.325) loss_u loss_u 0.9316 (0.9006) acc_u 6.2500 (12.0192) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [70/75] time 0.494 (0.457) data 0.363 (0.325) loss_u loss_u 0.8857 (0.9014) acc_u 12.5000 (11.9643) lr 1.1719e-03 eta 0:00:02
epoch [91/200] batch [75/75] time 0.343 (0.456) data 0.212 (0.325) loss_u loss_u 0.9253 (0.9032) acc_u 6.2500 (11.7500) lr 1.1719e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2062
confident_label rate tensor(0.2325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 729
clean true:357
clean false:372
clean_rate:0.4897119341563786
noisy true:717
noisy false:1690
after delete: len(clean_dataset) 729
after delete: len(noisy_dataset) 2407
epoch [92/200] batch [5/22] time 0.819 (0.539) data 0.688 (0.408) loss_x loss_x 1.3584 (1.2808) acc_x 65.6250 (68.1250) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [10/22] time 0.497 (0.488) data 0.367 (0.358) loss_x loss_x 0.6118 (1.2490) acc_x 87.5000 (68.7500) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [15/22] time 0.415 (0.470) data 0.285 (0.339) loss_x loss_x 1.5107 (1.3567) acc_x 62.5000 (67.5000) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [20/22] time 0.566 (0.473) data 0.435 (0.343) loss_x loss_x 0.9375 (1.3008) acc_x 75.0000 (68.5938) lr 1.1564e-03 eta 0:00:00
epoch [92/200] batch [5/75] time 0.470 (0.465) data 0.339 (0.334) loss_u loss_u 0.9458 (0.8941) acc_u 9.3750 (15.6250) lr 1.1564e-03 eta 0:00:32
epoch [92/200] batch [10/75] time 0.620 (0.475) data 0.488 (0.344) loss_u loss_u 0.9126 (0.8919) acc_u 9.3750 (15.9375) lr 1.1564e-03 eta 0:00:30
epoch [92/200] batch [15/75] time 0.417 (0.468) data 0.286 (0.337) loss_u loss_u 0.9243 (0.8912) acc_u 12.5000 (15.4167) lr 1.1564e-03 eta 0:00:28
epoch [92/200] batch [20/75] time 0.399 (0.462) data 0.268 (0.331) loss_u loss_u 0.8584 (0.8878) acc_u 15.6250 (15.6250) lr 1.1564e-03 eta 0:00:25
epoch [92/200] batch [25/75] time 0.349 (0.454) data 0.219 (0.323) loss_u loss_u 0.8765 (0.8935) acc_u 18.7500 (14.7500) lr 1.1564e-03 eta 0:00:22
epoch [92/200] batch [30/75] time 0.499 (0.459) data 0.368 (0.328) loss_u loss_u 0.8857 (0.8923) acc_u 15.6250 (14.4792) lr 1.1564e-03 eta 0:00:20
epoch [92/200] batch [35/75] time 0.381 (0.461) data 0.251 (0.330) loss_u loss_u 0.8496 (0.8921) acc_u 18.7500 (14.1964) lr 1.1564e-03 eta 0:00:18
epoch [92/200] batch [40/75] time 0.428 (0.458) data 0.297 (0.327) loss_u loss_u 0.9043 (0.8923) acc_u 12.5000 (14.2188) lr 1.1564e-03 eta 0:00:16
epoch [92/200] batch [45/75] time 0.641 (0.461) data 0.509 (0.330) loss_u loss_u 0.9448 (0.8957) acc_u 6.2500 (13.6111) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [50/75] time 0.360 (0.460) data 0.228 (0.329) loss_u loss_u 0.8545 (0.8967) acc_u 18.7500 (13.4375) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [55/75] time 0.455 (0.468) data 0.323 (0.337) loss_u loss_u 0.8564 (0.8971) acc_u 18.7500 (13.2955) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [60/75] time 0.456 (0.467) data 0.326 (0.336) loss_u loss_u 0.9570 (0.8987) acc_u 0.0000 (12.9688) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [65/75] time 0.489 (0.469) data 0.358 (0.338) loss_u loss_u 0.9033 (0.8990) acc_u 9.3750 (12.8365) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [70/75] time 0.439 (0.466) data 0.308 (0.335) loss_u loss_u 0.8833 (0.8985) acc_u 18.7500 (13.0357) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [75/75] time 0.411 (0.467) data 0.280 (0.335) loss_u loss_u 0.9185 (0.8991) acc_u 12.5000 (12.8750) lr 1.1564e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2074
confident_label rate tensor(0.2264, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 710
clean true:357
clean false:353
clean_rate:0.5028169014084507
noisy true:705
noisy false:1721
after delete: len(clean_dataset) 710
after delete: len(noisy_dataset) 2426
epoch [93/200] batch [5/22] time 0.378 (0.496) data 0.247 (0.365) loss_x loss_x 0.9873 (1.2139) acc_x 71.8750 (68.7500) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [10/22] time 0.464 (0.493) data 0.334 (0.362) loss_x loss_x 1.8203 (1.3128) acc_x 56.2500 (64.6875) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [15/22] time 0.482 (0.492) data 0.352 (0.361) loss_x loss_x 1.0693 (1.3209) acc_x 75.0000 (64.7917) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [20/22] time 0.382 (0.488) data 0.251 (0.357) loss_x loss_x 1.5811 (1.3392) acc_x 62.5000 (65.3125) lr 1.1409e-03 eta 0:00:00
epoch [93/200] batch [5/75] time 0.479 (0.481) data 0.348 (0.350) loss_u loss_u 0.8984 (0.9128) acc_u 9.3750 (8.1250) lr 1.1409e-03 eta 0:00:33
epoch [93/200] batch [10/75] time 0.598 (0.480) data 0.468 (0.349) loss_u loss_u 0.9204 (0.9082) acc_u 9.3750 (10.3125) lr 1.1409e-03 eta 0:00:31
epoch [93/200] batch [15/75] time 0.500 (0.483) data 0.369 (0.352) loss_u loss_u 0.8770 (0.9012) acc_u 18.7500 (12.0833) lr 1.1409e-03 eta 0:00:29
epoch [93/200] batch [20/75] time 0.381 (0.476) data 0.249 (0.345) loss_u loss_u 0.8647 (0.9032) acc_u 18.7500 (11.8750) lr 1.1409e-03 eta 0:00:26
epoch [93/200] batch [25/75] time 0.467 (0.475) data 0.336 (0.344) loss_u loss_u 0.9175 (0.9018) acc_u 9.3750 (11.7500) lr 1.1409e-03 eta 0:00:23
epoch [93/200] batch [30/75] time 0.380 (0.472) data 0.248 (0.341) loss_u loss_u 0.9014 (0.9041) acc_u 15.6250 (11.3542) lr 1.1409e-03 eta 0:00:21
epoch [93/200] batch [35/75] time 0.416 (0.471) data 0.283 (0.340) loss_u loss_u 0.9199 (0.9029) acc_u 9.3750 (11.6071) lr 1.1409e-03 eta 0:00:18
epoch [93/200] batch [40/75] time 0.461 (0.467) data 0.329 (0.336) loss_u loss_u 0.9043 (0.8987) acc_u 12.5000 (12.1094) lr 1.1409e-03 eta 0:00:16
epoch [93/200] batch [45/75] time 0.349 (0.465) data 0.215 (0.334) loss_u loss_u 0.8643 (0.8994) acc_u 18.7500 (12.3611) lr 1.1409e-03 eta 0:00:13
epoch [93/200] batch [50/75] time 0.428 (0.466) data 0.297 (0.335) loss_u loss_u 0.8608 (0.8964) acc_u 18.7500 (12.8125) lr 1.1409e-03 eta 0:00:11
epoch [93/200] batch [55/75] time 0.387 (0.463) data 0.255 (0.332) loss_u loss_u 0.8301 (0.8976) acc_u 21.8750 (12.6136) lr 1.1409e-03 eta 0:00:09
epoch [93/200] batch [60/75] time 0.453 (0.466) data 0.320 (0.335) loss_u loss_u 0.8950 (0.8958) acc_u 12.5000 (12.9688) lr 1.1409e-03 eta 0:00:06
epoch [93/200] batch [65/75] time 0.418 (0.467) data 0.286 (0.335) loss_u loss_u 0.8877 (0.8956) acc_u 12.5000 (13.1250) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [70/75] time 0.364 (0.467) data 0.233 (0.336) loss_u loss_u 0.8501 (0.8960) acc_u 15.6250 (13.0804) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [75/75] time 0.514 (0.469) data 0.382 (0.338) loss_u loss_u 0.9082 (0.8952) acc_u 9.3750 (13.2917) lr 1.1409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2040
confident_label rate tensor(0.2312, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 725
clean true:370
clean false:355
clean_rate:0.5103448275862069
noisy true:726
noisy false:1685
after delete: len(clean_dataset) 725
after delete: len(noisy_dataset) 2411
epoch [94/200] batch [5/22] time 0.411 (0.427) data 0.281 (0.296) loss_x loss_x 2.0039 (1.4793) acc_x 62.5000 (64.3750) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [10/22] time 0.424 (0.454) data 0.293 (0.324) loss_x loss_x 1.4932 (1.4348) acc_x 59.3750 (64.3750) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [15/22] time 0.589 (0.489) data 0.457 (0.358) loss_x loss_x 1.1670 (1.3950) acc_x 65.6250 (65.6250) lr 1.1253e-03 eta 0:00:03
epoch [94/200] batch [20/22] time 0.488 (0.487) data 0.357 (0.356) loss_x loss_x 1.5693 (1.4373) acc_x 65.6250 (64.3750) lr 1.1253e-03 eta 0:00:00
epoch [94/200] batch [5/75] time 0.667 (0.483) data 0.536 (0.353) loss_u loss_u 0.9004 (0.9144) acc_u 12.5000 (11.2500) lr 1.1253e-03 eta 0:00:33
epoch [94/200] batch [10/75] time 0.412 (0.481) data 0.279 (0.350) loss_u loss_u 0.9072 (0.9088) acc_u 12.5000 (11.2500) lr 1.1253e-03 eta 0:00:31
epoch [94/200] batch [15/75] time 0.400 (0.472) data 0.270 (0.341) loss_u loss_u 0.8589 (0.8983) acc_u 21.8750 (13.5417) lr 1.1253e-03 eta 0:00:28
epoch [94/200] batch [20/75] time 0.434 (0.472) data 0.302 (0.340) loss_u loss_u 0.8921 (0.8989) acc_u 9.3750 (13.2812) lr 1.1253e-03 eta 0:00:25
epoch [94/200] batch [25/75] time 0.537 (0.467) data 0.406 (0.336) loss_u loss_u 0.8867 (0.8967) acc_u 15.6250 (13.7500) lr 1.1253e-03 eta 0:00:23
epoch [94/200] batch [30/75] time 0.409 (0.473) data 0.277 (0.342) loss_u loss_u 0.9062 (0.8949) acc_u 12.5000 (13.8542) lr 1.1253e-03 eta 0:00:21
epoch [94/200] batch [35/75] time 0.478 (0.472) data 0.347 (0.341) loss_u loss_u 0.9585 (0.8967) acc_u 3.1250 (13.6607) lr 1.1253e-03 eta 0:00:18
epoch [94/200] batch [40/75] time 0.384 (0.472) data 0.253 (0.340) loss_u loss_u 0.9692 (0.8978) acc_u 3.1250 (13.6719) lr 1.1253e-03 eta 0:00:16
epoch [94/200] batch [45/75] time 0.470 (0.470) data 0.338 (0.338) loss_u loss_u 0.8994 (0.8987) acc_u 12.5000 (13.3333) lr 1.1253e-03 eta 0:00:14
epoch [94/200] batch [50/75] time 0.493 (0.468) data 0.361 (0.336) loss_u loss_u 0.8066 (0.8968) acc_u 28.1250 (13.7500) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [55/75] time 0.445 (0.469) data 0.313 (0.337) loss_u loss_u 0.8340 (0.8935) acc_u 25.0000 (14.2045) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [60/75] time 0.551 (0.468) data 0.420 (0.336) loss_u loss_u 0.9219 (0.8960) acc_u 6.2500 (13.8021) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [65/75] time 0.499 (0.466) data 0.368 (0.335) loss_u loss_u 0.9106 (0.8964) acc_u 9.3750 (13.7019) lr 1.1253e-03 eta 0:00:04
epoch [94/200] batch [70/75] time 0.405 (0.466) data 0.273 (0.335) loss_u loss_u 0.8911 (0.8955) acc_u 12.5000 (13.7500) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [75/75] time 0.383 (0.462) data 0.251 (0.331) loss_u loss_u 0.9009 (0.8946) acc_u 18.7500 (14.0000) lr 1.1253e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2063
confident_label rate tensor(0.2423, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 760
clean true:362
clean false:398
clean_rate:0.4763157894736842
noisy true:711
noisy false:1665
after delete: len(clean_dataset) 760
after delete: len(noisy_dataset) 2376
epoch [95/200] batch [5/23] time 0.404 (0.487) data 0.273 (0.356) loss_x loss_x 1.3760 (1.3762) acc_x 59.3750 (62.5000) lr 1.1097e-03 eta 0:00:08
epoch [95/200] batch [10/23] time 0.396 (0.488) data 0.266 (0.357) loss_x loss_x 0.8638 (1.3632) acc_x 71.8750 (63.4375) lr 1.1097e-03 eta 0:00:06
epoch [95/200] batch [15/23] time 0.361 (0.476) data 0.229 (0.345) loss_x loss_x 1.0439 (1.3412) acc_x 75.0000 (64.1667) lr 1.1097e-03 eta 0:00:03
epoch [95/200] batch [20/23] time 0.462 (0.465) data 0.332 (0.334) loss_x loss_x 1.6162 (1.3677) acc_x 65.6250 (64.5312) lr 1.1097e-03 eta 0:00:01
epoch [95/200] batch [5/74] time 0.443 (0.467) data 0.311 (0.336) loss_u loss_u 0.9443 (0.9180) acc_u 3.1250 (10.0000) lr 1.1097e-03 eta 0:00:32
epoch [95/200] batch [10/74] time 0.557 (0.476) data 0.424 (0.345) loss_u loss_u 0.9043 (0.9137) acc_u 12.5000 (10.6250) lr 1.1097e-03 eta 0:00:30
epoch [95/200] batch [15/74] time 0.401 (0.478) data 0.269 (0.347) loss_u loss_u 0.8657 (0.9090) acc_u 15.6250 (11.2500) lr 1.1097e-03 eta 0:00:28
epoch [95/200] batch [20/74] time 0.400 (0.471) data 0.269 (0.340) loss_u loss_u 0.8965 (0.9057) acc_u 12.5000 (12.0312) lr 1.1097e-03 eta 0:00:25
epoch [95/200] batch [25/74] time 0.408 (0.469) data 0.278 (0.337) loss_u loss_u 0.9121 (0.9062) acc_u 12.5000 (12.5000) lr 1.1097e-03 eta 0:00:22
epoch [95/200] batch [30/74] time 0.448 (0.466) data 0.318 (0.334) loss_u loss_u 0.8906 (0.9043) acc_u 12.5000 (12.7083) lr 1.1097e-03 eta 0:00:20
epoch [95/200] batch [35/74] time 0.379 (0.463) data 0.248 (0.331) loss_u loss_u 0.9526 (0.9057) acc_u 6.2500 (12.5893) lr 1.1097e-03 eta 0:00:18
epoch [95/200] batch [40/74] time 0.484 (0.466) data 0.352 (0.334) loss_u loss_u 0.9526 (0.9046) acc_u 6.2500 (12.6562) lr 1.1097e-03 eta 0:00:15
epoch [95/200] batch [45/74] time 0.431 (0.466) data 0.298 (0.335) loss_u loss_u 0.9194 (0.9035) acc_u 9.3750 (12.8472) lr 1.1097e-03 eta 0:00:13
epoch [95/200] batch [50/74] time 0.616 (0.467) data 0.485 (0.336) loss_u loss_u 0.8843 (0.9025) acc_u 18.7500 (13.0625) lr 1.1097e-03 eta 0:00:11
epoch [95/200] batch [55/74] time 0.401 (0.467) data 0.271 (0.336) loss_u loss_u 0.9473 (0.9045) acc_u 3.1250 (12.7841) lr 1.1097e-03 eta 0:00:08
epoch [95/200] batch [60/74] time 0.431 (0.465) data 0.299 (0.333) loss_u loss_u 0.8667 (0.9063) acc_u 18.7500 (12.5000) lr 1.1097e-03 eta 0:00:06
epoch [95/200] batch [65/74] time 0.452 (0.463) data 0.321 (0.331) loss_u loss_u 0.8804 (0.9058) acc_u 18.7500 (12.4519) lr 1.1097e-03 eta 0:00:04
epoch [95/200] batch [70/74] time 0.371 (0.461) data 0.240 (0.329) loss_u loss_u 0.8979 (0.9047) acc_u 12.5000 (12.7232) lr 1.1097e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2072
confident_label rate tensor(0.2363, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 741
clean true:356
clean false:385
clean_rate:0.48043184885290147
noisy true:708
noisy false:1687
after delete: len(clean_dataset) 741
after delete: len(noisy_dataset) 2395
epoch [96/200] batch [5/23] time 0.395 (0.464) data 0.265 (0.333) loss_x loss_x 1.8809 (1.2900) acc_x 56.2500 (68.1250) lr 1.0941e-03 eta 0:00:08
epoch [96/200] batch [10/23] time 0.551 (0.471) data 0.420 (0.341) loss_x loss_x 1.7031 (1.2978) acc_x 65.6250 (68.4375) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [15/23] time 0.465 (0.471) data 0.333 (0.340) loss_x loss_x 1.2988 (1.3100) acc_x 75.0000 (67.2917) lr 1.0941e-03 eta 0:00:03
epoch [96/200] batch [20/23] time 0.571 (0.476) data 0.440 (0.345) loss_x loss_x 1.1328 (1.3554) acc_x 68.7500 (66.0938) lr 1.0941e-03 eta 0:00:01
epoch [96/200] batch [5/74] time 0.429 (0.479) data 0.297 (0.348) loss_u loss_u 0.8833 (0.8904) acc_u 12.5000 (12.5000) lr 1.0941e-03 eta 0:00:33
epoch [96/200] batch [10/74] time 0.416 (0.475) data 0.283 (0.344) loss_u loss_u 0.9434 (0.9057) acc_u 6.2500 (10.6250) lr 1.0941e-03 eta 0:00:30
epoch [96/200] batch [15/74] time 0.416 (0.472) data 0.286 (0.341) loss_u loss_u 0.8623 (0.8981) acc_u 15.6250 (12.2917) lr 1.0941e-03 eta 0:00:27
epoch [96/200] batch [20/74] time 0.476 (0.474) data 0.344 (0.343) loss_u loss_u 0.9521 (0.9007) acc_u 3.1250 (11.4062) lr 1.0941e-03 eta 0:00:25
epoch [96/200] batch [25/74] time 0.454 (0.467) data 0.323 (0.336) loss_u loss_u 0.9253 (0.8969) acc_u 15.6250 (12.5000) lr 1.0941e-03 eta 0:00:22
epoch [96/200] batch [30/74] time 0.403 (0.462) data 0.272 (0.331) loss_u loss_u 0.9048 (0.9000) acc_u 15.6250 (12.1875) lr 1.0941e-03 eta 0:00:20
epoch [96/200] batch [35/74] time 0.512 (0.458) data 0.380 (0.327) loss_u loss_u 0.9053 (0.9011) acc_u 12.5000 (12.2321) lr 1.0941e-03 eta 0:00:17
epoch [96/200] batch [40/74] time 0.413 (0.458) data 0.282 (0.327) loss_u loss_u 0.8828 (0.9004) acc_u 18.7500 (12.5781) lr 1.0941e-03 eta 0:00:15
epoch [96/200] batch [45/74] time 0.459 (0.458) data 0.327 (0.326) loss_u loss_u 0.9326 (0.9020) acc_u 9.3750 (12.3611) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [50/74] time 0.354 (0.456) data 0.221 (0.325) loss_u loss_u 0.9111 (0.9028) acc_u 15.6250 (12.1875) lr 1.0941e-03 eta 0:00:10
epoch [96/200] batch [55/74] time 0.427 (0.458) data 0.296 (0.326) loss_u loss_u 0.9258 (0.9019) acc_u 12.5000 (12.5568) lr 1.0941e-03 eta 0:00:08
epoch [96/200] batch [60/74] time 0.445 (0.462) data 0.314 (0.330) loss_u loss_u 0.8921 (0.9016) acc_u 9.3750 (12.4479) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [65/74] time 0.481 (0.464) data 0.349 (0.332) loss_u loss_u 0.8521 (0.9013) acc_u 15.6250 (12.3077) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [70/74] time 0.449 (0.462) data 0.317 (0.330) loss_u loss_u 0.8799 (0.9006) acc_u 12.5000 (12.2321) lr 1.0941e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2058
confident_label rate tensor(0.2334, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 732
clean true:345
clean false:387
clean_rate:0.4713114754098361
noisy true:733
noisy false:1671
after delete: len(clean_dataset) 732
after delete: len(noisy_dataset) 2404
epoch [97/200] batch [5/22] time 0.422 (0.445) data 0.292 (0.314) loss_x loss_x 1.9561 (1.6391) acc_x 46.8750 (60.0000) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [10/22] time 0.364 (0.472) data 0.234 (0.341) loss_x loss_x 1.6611 (1.5141) acc_x 56.2500 (63.1250) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [15/22] time 0.404 (0.471) data 0.273 (0.341) loss_x loss_x 1.5654 (1.4651) acc_x 71.8750 (65.4167) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [20/22] time 0.482 (0.460) data 0.351 (0.329) loss_x loss_x 1.5195 (1.4221) acc_x 62.5000 (65.7812) lr 1.0785e-03 eta 0:00:00
epoch [97/200] batch [5/75] time 0.502 (0.451) data 0.370 (0.320) loss_u loss_u 0.8477 (0.8766) acc_u 18.7500 (16.8750) lr 1.0785e-03 eta 0:00:31
epoch [97/200] batch [10/75] time 0.371 (0.449) data 0.240 (0.318) loss_u loss_u 0.9272 (0.8880) acc_u 6.2500 (15.0000) lr 1.0785e-03 eta 0:00:29
epoch [97/200] batch [15/75] time 0.437 (0.446) data 0.307 (0.315) loss_u loss_u 0.9214 (0.8958) acc_u 12.5000 (13.5417) lr 1.0785e-03 eta 0:00:26
epoch [97/200] batch [20/75] time 0.420 (0.447) data 0.288 (0.316) loss_u loss_u 0.9414 (0.8991) acc_u 9.3750 (13.1250) lr 1.0785e-03 eta 0:00:24
epoch [97/200] batch [25/75] time 0.544 (0.455) data 0.412 (0.324) loss_u loss_u 0.9292 (0.9005) acc_u 12.5000 (13.2500) lr 1.0785e-03 eta 0:00:22
epoch [97/200] batch [30/75] time 0.424 (0.453) data 0.293 (0.322) loss_u loss_u 0.8984 (0.9018) acc_u 12.5000 (13.0208) lr 1.0785e-03 eta 0:00:20
epoch [97/200] batch [35/75] time 0.391 (0.450) data 0.260 (0.319) loss_u loss_u 0.8867 (0.9029) acc_u 15.6250 (13.0357) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [40/75] time 0.470 (0.450) data 0.338 (0.319) loss_u loss_u 0.8823 (0.9030) acc_u 15.6250 (13.0469) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [45/75] time 0.401 (0.451) data 0.271 (0.320) loss_u loss_u 0.9106 (0.9045) acc_u 12.5000 (12.9861) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [50/75] time 0.376 (0.454) data 0.244 (0.323) loss_u loss_u 0.8833 (0.9006) acc_u 18.7500 (13.5625) lr 1.0785e-03 eta 0:00:11
epoch [97/200] batch [55/75] time 0.603 (0.458) data 0.471 (0.326) loss_u loss_u 0.9048 (0.8982) acc_u 12.5000 (13.7500) lr 1.0785e-03 eta 0:00:09
epoch [97/200] batch [60/75] time 0.397 (0.459) data 0.265 (0.328) loss_u loss_u 0.8667 (0.8980) acc_u 15.6250 (13.6458) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [65/75] time 0.373 (0.464) data 0.242 (0.333) loss_u loss_u 0.8896 (0.8982) acc_u 15.6250 (13.5577) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [70/75] time 0.452 (0.467) data 0.320 (0.336) loss_u loss_u 0.9087 (0.8967) acc_u 12.5000 (13.6607) lr 1.0785e-03 eta 0:00:02
epoch [97/200] batch [75/75] time 0.515 (0.468) data 0.383 (0.337) loss_u loss_u 0.8882 (0.8954) acc_u 12.5000 (13.7917) lr 1.0785e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2063
confident_label rate tensor(0.2436, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 764
clean true:358
clean false:406
clean_rate:0.468586387434555
noisy true:715
noisy false:1657
after delete: len(clean_dataset) 764
after delete: len(noisy_dataset) 2372
epoch [98/200] batch [5/23] time 0.382 (0.441) data 0.251 (0.311) loss_x loss_x 1.3428 (1.3315) acc_x 68.7500 (71.2500) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [10/23] time 0.463 (0.477) data 0.332 (0.346) loss_x loss_x 1.3291 (1.3676) acc_x 62.5000 (68.7500) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [15/23] time 0.438 (0.475) data 0.307 (0.344) loss_x loss_x 1.4814 (1.3181) acc_x 62.5000 (68.9583) lr 1.0628e-03 eta 0:00:03
epoch [98/200] batch [20/23] time 0.334 (0.466) data 0.203 (0.336) loss_x loss_x 1.0312 (1.2741) acc_x 84.3750 (69.8438) lr 1.0628e-03 eta 0:00:01
epoch [98/200] batch [5/74] time 0.501 (0.464) data 0.370 (0.334) loss_u loss_u 0.8774 (0.9102) acc_u 9.3750 (8.7500) lr 1.0628e-03 eta 0:00:32
epoch [98/200] batch [10/74] time 0.512 (0.462) data 0.381 (0.332) loss_u loss_u 0.8994 (0.9116) acc_u 12.5000 (9.3750) lr 1.0628e-03 eta 0:00:29
epoch [98/200] batch [15/74] time 0.391 (0.462) data 0.261 (0.331) loss_u loss_u 0.8843 (0.9084) acc_u 12.5000 (10.0000) lr 1.0628e-03 eta 0:00:27
epoch [98/200] batch [20/74] time 0.653 (0.464) data 0.521 (0.333) loss_u loss_u 0.8765 (0.9044) acc_u 15.6250 (10.9375) lr 1.0628e-03 eta 0:00:25
epoch [98/200] batch [25/74] time 0.448 (0.462) data 0.317 (0.331) loss_u loss_u 0.8589 (0.9026) acc_u 21.8750 (11.5000) lr 1.0628e-03 eta 0:00:22
epoch [98/200] batch [30/74] time 0.534 (0.460) data 0.404 (0.330) loss_u loss_u 0.8916 (0.9000) acc_u 12.5000 (11.8750) lr 1.0628e-03 eta 0:00:20
epoch [98/200] batch [35/74] time 0.407 (0.466) data 0.276 (0.335) loss_u loss_u 0.8672 (0.9018) acc_u 18.7500 (11.9643) lr 1.0628e-03 eta 0:00:18
epoch [98/200] batch [40/74] time 0.393 (0.463) data 0.262 (0.332) loss_u loss_u 0.9185 (0.9001) acc_u 15.6250 (12.3438) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [45/74] time 0.381 (0.467) data 0.250 (0.337) loss_u loss_u 0.8516 (0.8964) acc_u 21.8750 (12.9861) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [50/74] time 0.468 (0.465) data 0.337 (0.334) loss_u loss_u 0.9160 (0.8958) acc_u 9.3750 (13.1875) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [55/74] time 0.547 (0.465) data 0.416 (0.334) loss_u loss_u 0.8452 (0.8953) acc_u 21.8750 (13.4091) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [60/74] time 0.402 (0.466) data 0.269 (0.335) loss_u loss_u 0.9194 (0.8942) acc_u 9.3750 (13.5938) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [65/74] time 0.304 (0.461) data 0.173 (0.330) loss_u loss_u 0.8940 (0.8931) acc_u 15.6250 (13.8462) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [70/74] time 0.469 (0.459) data 0.339 (0.328) loss_u loss_u 0.9038 (0.8939) acc_u 12.5000 (13.7946) lr 1.0628e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2086
confident_label rate tensor(0.2382, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 747
clean true:355
clean false:392
clean_rate:0.47523427041499333
noisy true:695
noisy false:1694
after delete: len(clean_dataset) 747
after delete: len(noisy_dataset) 2389
epoch [99/200] batch [5/23] time 0.403 (0.424) data 0.270 (0.291) loss_x loss_x 1.3643 (1.1236) acc_x 68.7500 (73.7500) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [10/23] time 0.472 (0.448) data 0.338 (0.316) loss_x loss_x 1.0996 (1.1963) acc_x 65.6250 (69.6875) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [15/23] time 0.440 (0.448) data 0.309 (0.317) loss_x loss_x 1.3281 (1.2593) acc_x 71.8750 (67.5000) lr 1.0471e-03 eta 0:00:03
epoch [99/200] batch [20/23] time 0.390 (0.463) data 0.259 (0.332) loss_x loss_x 1.1943 (1.2344) acc_x 75.0000 (68.2812) lr 1.0471e-03 eta 0:00:01
epoch [99/200] batch [5/74] time 0.518 (0.469) data 0.387 (0.338) loss_u loss_u 0.9331 (0.9078) acc_u 9.3750 (11.8750) lr 1.0471e-03 eta 0:00:32
epoch [99/200] batch [10/74] time 0.397 (0.474) data 0.266 (0.343) loss_u loss_u 0.8887 (0.9138) acc_u 15.6250 (10.9375) lr 1.0471e-03 eta 0:00:30
epoch [99/200] batch [15/74] time 0.583 (0.469) data 0.452 (0.337) loss_u loss_u 0.8135 (0.9050) acc_u 25.0000 (12.0833) lr 1.0471e-03 eta 0:00:27
epoch [99/200] batch [20/74] time 0.425 (0.466) data 0.294 (0.334) loss_u loss_u 0.9067 (0.9098) acc_u 9.3750 (11.7188) lr 1.0471e-03 eta 0:00:25
epoch [99/200] batch [25/74] time 0.443 (0.467) data 0.312 (0.336) loss_u loss_u 0.8647 (0.9041) acc_u 15.6250 (12.6250) lr 1.0471e-03 eta 0:00:22
epoch [99/200] batch [30/74] time 0.494 (0.470) data 0.362 (0.339) loss_u loss_u 0.8315 (0.9051) acc_u 15.6250 (12.0833) lr 1.0471e-03 eta 0:00:20
epoch [99/200] batch [35/74] time 0.499 (0.470) data 0.368 (0.339) loss_u loss_u 0.9028 (0.9030) acc_u 15.6250 (12.6786) lr 1.0471e-03 eta 0:00:18
epoch [99/200] batch [40/74] time 0.419 (0.466) data 0.287 (0.334) loss_u loss_u 0.9531 (0.9062) acc_u 3.1250 (12.0312) lr 1.0471e-03 eta 0:00:15
epoch [99/200] batch [45/74] time 0.635 (0.468) data 0.504 (0.337) loss_u loss_u 0.8721 (0.9044) acc_u 18.7500 (12.2917) lr 1.0471e-03 eta 0:00:13
epoch [99/200] batch [50/74] time 0.355 (0.466) data 0.224 (0.335) loss_u loss_u 0.9116 (0.9022) acc_u 12.5000 (12.7500) lr 1.0471e-03 eta 0:00:11
epoch [99/200] batch [55/74] time 0.373 (0.469) data 0.243 (0.338) loss_u loss_u 0.9062 (0.9021) acc_u 9.3750 (12.6705) lr 1.0471e-03 eta 0:00:08
epoch [99/200] batch [60/74] time 0.493 (0.471) data 0.362 (0.339) loss_u loss_u 0.9243 (0.9003) acc_u 12.5000 (13.1771) lr 1.0471e-03 eta 0:00:06
epoch [99/200] batch [65/74] time 0.535 (0.471) data 0.403 (0.340) loss_u loss_u 0.8618 (0.9003) acc_u 21.8750 (13.2212) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [70/74] time 0.389 (0.468) data 0.258 (0.337) loss_u loss_u 0.8281 (0.8988) acc_u 18.7500 (13.2143) lr 1.0471e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2087
confident_label rate tensor(0.2439, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 765
clean true:356
clean false:409
clean_rate:0.465359477124183
noisy true:693
noisy false:1678
all clean rate:  [0.6968325791855203, 0.7065637065637066, 0.7760814249363868, 0.7294117647058823, 0.7226277372262774, 0.7391304347826086, 0.7558139534883721, 0.7458823529411764, 0.7123595505617978, 0.7333333333333333, 0.7161016949152542, 0.6880165289256198, 0.6869369369369369, 0.6995614035087719, 0.6652806652806653, 0.7035490605427975, 0.6376518218623481, 0.6586345381526104, 0.650375939849624, 0.6372745490981964, 0.6587771203155819, 0.6425925925925926, 0.6713426853707415, 0.6342342342342342, 0.6058823529411764, 0.6309751434034416, 0.5876865671641791, 0.5831842576028623, 0.6309090909090909, 0.5732600732600732, 0.5870736086175943, 0.5892193308550185, 0.5707154742096506, 0.5595238095238095, 0.5815972222222222, 0.575, 0.5546623794212219, 0.5536626916524702, 0.5647058823529412, 0.5575657894736842, 0.5359477124183006, 0.5517241379310345, 0.5597920277296361, 0.5475040257648953, 0.5300813008130081, 0.5357710651828299, 0.5367047308319739, 0.5448717948717948, 0.5355987055016181, 0.548489666136725, 0.5312977099236641, 0.5322834645669291, 0.5339652448657188, 0.5015673981191222, 0.52, 0.5402476780185759, 0.5113464447806354, 0.5146831530139103, 0.5213414634146342, 0.5132743362831859, 0.4933920704845815, 0.47581903276131043, 0.5212765957446809, 0.5204081632653061, 0.518628912071535, 0.5191176470588236, 0.4899713467048711, 0.5, 0.5138686131386861, 0.4872159090909091, 0.5163120567375886, 0.5144508670520231, 0.502262443438914, 0.4993045897079277, 0.497841726618705, 0.4979702300405954, 0.5096952908587258, 0.47844228094575797, 0.49646393210749645, 0.5173370319001387, 0.4718498659517426, 0.4951856946354883, 0.4861111111111111, 0.47099447513812154, 0.5057636887608069, 0.49297752808988765, 0.4889753566796368, 0.4986449864498645, 0.48468708388814913, 0.48480662983425415, 0.4756606397774687, 0.4897119341563786, 0.5028169014084507, 0.5103448275862069, 0.4763157894736842, 0.48043184885290147, 0.4713114754098361, 0.468586387434555, 0.47523427041499333, 0.465359477124183]
after delete: len(clean_dataset) 765
after delete: len(noisy_dataset) 2371
epoch [100/200] batch [5/23] time 0.531 (0.458) data 0.400 (0.327) loss_x loss_x 1.6240 (1.2972) acc_x 59.3750 (66.8750) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [10/23] time 0.442 (0.482) data 0.311 (0.351) loss_x loss_x 1.2568 (1.2485) acc_x 56.2500 (67.1875) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [15/23] time 0.656 (0.478) data 0.524 (0.348) loss_x loss_x 1.0469 (1.2041) acc_x 75.0000 (67.2917) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [20/23] time 0.429 (0.474) data 0.298 (0.344) loss_x loss_x 1.5596 (1.2277) acc_x 59.3750 (66.4062) lr 1.0314e-03 eta 0:00:01
epoch [100/200] batch [5/74] time 0.367 (0.466) data 0.235 (0.335) loss_u loss_u 0.9321 (0.9106) acc_u 6.2500 (10.0000) lr 1.0314e-03 eta 0:00:32
epoch [100/200] batch [10/74] time 0.704 (0.470) data 0.572 (0.339) loss_u loss_u 0.9443 (0.9167) acc_u 6.2500 (10.0000) lr 1.0314e-03 eta 0:00:30
epoch [100/200] batch [15/74] time 0.321 (0.459) data 0.190 (0.328) loss_u loss_u 0.8906 (0.9021) acc_u 12.5000 (12.2917) lr 1.0314e-03 eta 0:00:27
epoch [100/200] batch [20/74] time 0.522 (0.466) data 0.390 (0.335) loss_u loss_u 0.8496 (0.8985) acc_u 18.7500 (12.6562) lr 1.0314e-03 eta 0:00:25
epoch [100/200] batch [25/74] time 0.509 (0.465) data 0.379 (0.334) loss_u loss_u 0.9229 (0.9028) acc_u 6.2500 (12.2500) lr 1.0314e-03 eta 0:00:22
epoch [100/200] batch [30/74] time 0.480 (0.464) data 0.348 (0.333) loss_u loss_u 0.9316 (0.9036) acc_u 6.2500 (11.9792) lr 1.0314e-03 eta 0:00:20
epoch [100/200] batch [35/74] time 0.563 (0.464) data 0.432 (0.333) loss_u loss_u 0.8862 (0.9027) acc_u 12.5000 (12.2321) lr 1.0314e-03 eta 0:00:18
epoch [100/200] batch [40/74] time 0.553 (0.469) data 0.422 (0.338) loss_u loss_u 0.8359 (0.8995) acc_u 15.6250 (12.4219) lr 1.0314e-03 eta 0:00:15
epoch [100/200] batch [45/74] time 0.494 (0.470) data 0.363 (0.339) loss_u loss_u 0.9380 (0.9004) acc_u 6.2500 (12.3611) lr 1.0314e-03 eta 0:00:13
epoch [100/200] batch [50/74] time 0.318 (0.471) data 0.186 (0.340) loss_u loss_u 0.9482 (0.9018) acc_u 3.1250 (12.1250) lr 1.0314e-03 eta 0:00:11
epoch [100/200] batch [55/74] time 0.354 (0.470) data 0.223 (0.338) loss_u loss_u 0.9580 (0.9042) acc_u 3.1250 (11.6477) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [60/74] time 0.447 (0.472) data 0.317 (0.340) loss_u loss_u 0.9014 (0.9035) acc_u 15.6250 (11.7708) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [65/74] time 0.485 (0.471) data 0.354 (0.340) loss_u loss_u 0.9014 (0.9046) acc_u 9.3750 (11.7308) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [70/74] time 0.542 (0.470) data 0.411 (0.339) loss_u loss_u 0.9058 (0.9046) acc_u 18.7500 (11.8750) lr 1.0314e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2093
confident_label rate tensor(0.2411, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 756
clean true:349
clean false:407
clean_rate:0.46164021164021163
noisy true:694
noisy false:1686
after delete: len(clean_dataset) 756
after delete: len(noisy_dataset) 2380
epoch [101/200] batch [5/23] time 0.449 (0.476) data 0.318 (0.345) loss_x loss_x 1.3105 (1.0862) acc_x 71.8750 (72.5000) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [10/23] time 0.476 (0.452) data 0.345 (0.321) loss_x loss_x 1.0273 (1.1032) acc_x 71.8750 (70.9375) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [15/23] time 0.482 (0.444) data 0.351 (0.314) loss_x loss_x 1.1396 (1.1950) acc_x 75.0000 (69.3750) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [20/23] time 0.532 (0.448) data 0.402 (0.317) loss_x loss_x 1.1084 (1.1899) acc_x 78.1250 (69.8438) lr 1.0157e-03 eta 0:00:01
epoch [101/200] batch [5/74] time 0.563 (0.453) data 0.432 (0.323) loss_u loss_u 0.8975 (0.9032) acc_u 12.5000 (11.2500) lr 1.0157e-03 eta 0:00:31
epoch [101/200] batch [10/74] time 0.494 (0.462) data 0.362 (0.332) loss_u loss_u 0.8218 (0.8958) acc_u 18.7500 (13.4375) lr 1.0157e-03 eta 0:00:29
epoch [101/200] batch [15/74] time 0.468 (0.462) data 0.338 (0.331) loss_u loss_u 0.8281 (0.8900) acc_u 18.7500 (13.9583) lr 1.0157e-03 eta 0:00:27
epoch [101/200] batch [20/74] time 0.455 (0.456) data 0.325 (0.325) loss_u loss_u 0.9795 (0.8979) acc_u 0.0000 (12.9688) lr 1.0157e-03 eta 0:00:24
epoch [101/200] batch [25/74] time 0.430 (0.459) data 0.299 (0.328) loss_u loss_u 0.9214 (0.9001) acc_u 12.5000 (12.3750) lr 1.0157e-03 eta 0:00:22
epoch [101/200] batch [30/74] time 0.529 (0.460) data 0.398 (0.329) loss_u loss_u 0.8789 (0.8981) acc_u 21.8750 (13.2292) lr 1.0157e-03 eta 0:00:20
epoch [101/200] batch [35/74] time 0.360 (0.459) data 0.229 (0.329) loss_u loss_u 0.9497 (0.9007) acc_u 6.2500 (12.7679) lr 1.0157e-03 eta 0:00:17
epoch [101/200] batch [40/74] time 0.349 (0.458) data 0.217 (0.327) loss_u loss_u 0.9326 (0.9011) acc_u 6.2500 (12.6562) lr 1.0157e-03 eta 0:00:15
epoch [101/200] batch [45/74] time 0.473 (0.457) data 0.340 (0.326) loss_u loss_u 0.8667 (0.8997) acc_u 15.6250 (12.9861) lr 1.0157e-03 eta 0:00:13
epoch [101/200] batch [50/74] time 0.714 (0.462) data 0.582 (0.331) loss_u loss_u 0.9175 (0.8993) acc_u 9.3750 (13.0000) lr 1.0157e-03 eta 0:00:11
epoch [101/200] batch [55/74] time 0.414 (0.463) data 0.283 (0.332) loss_u loss_u 0.8145 (0.8971) acc_u 25.0000 (13.2955) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [60/74] time 0.573 (0.466) data 0.441 (0.335) loss_u loss_u 0.9141 (0.9002) acc_u 12.5000 (12.8125) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [65/74] time 0.442 (0.468) data 0.310 (0.337) loss_u loss_u 0.9238 (0.9011) acc_u 6.2500 (12.5962) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [70/74] time 0.479 (0.469) data 0.347 (0.337) loss_u loss_u 0.8950 (0.9014) acc_u 9.3750 (12.5446) lr 1.0157e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2106
confident_label rate tensor(0.2408, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 755
clean true:347
clean false:408
clean_rate:0.4596026490066225
noisy true:683
noisy false:1698
after delete: len(clean_dataset) 755
after delete: len(noisy_dataset) 2381
epoch [102/200] batch [5/23] time 0.386 (0.435) data 0.256 (0.304) loss_x loss_x 1.4746 (1.2680) acc_x 62.5000 (66.8750) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [10/23] time 0.393 (0.419) data 0.261 (0.287) loss_x loss_x 1.2520 (1.2946) acc_x 71.8750 (68.1250) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [15/23] time 0.487 (0.441) data 0.356 (0.310) loss_x loss_x 1.6270 (1.2850) acc_x 62.5000 (68.3333) lr 1.0000e-03 eta 0:00:03
epoch [102/200] batch [20/23] time 0.556 (0.442) data 0.424 (0.311) loss_x loss_x 1.5205 (1.3101) acc_x 62.5000 (68.5938) lr 1.0000e-03 eta 0:00:01
epoch [102/200] batch [5/74] time 0.497 (0.442) data 0.365 (0.311) loss_u loss_u 0.9258 (0.8828) acc_u 6.2500 (13.7500) lr 1.0000e-03 eta 0:00:30
epoch [102/200] batch [10/74] time 0.392 (0.440) data 0.260 (0.309) loss_u loss_u 0.9248 (0.8909) acc_u 12.5000 (14.0625) lr 1.0000e-03 eta 0:00:28
epoch [102/200] batch [15/74] time 0.482 (0.445) data 0.350 (0.314) loss_u loss_u 0.8384 (0.8983) acc_u 25.0000 (13.1250) lr 1.0000e-03 eta 0:00:26
epoch [102/200] batch [20/74] time 0.389 (0.442) data 0.257 (0.311) loss_u loss_u 0.8960 (0.9016) acc_u 9.3750 (12.5000) lr 1.0000e-03 eta 0:00:23
epoch [102/200] batch [25/74] time 0.546 (0.446) data 0.414 (0.314) loss_u loss_u 0.9038 (0.9002) acc_u 12.5000 (12.6250) lr 1.0000e-03 eta 0:00:21
epoch [102/200] batch [30/74] time 0.513 (0.446) data 0.381 (0.315) loss_u loss_u 0.8359 (0.8929) acc_u 18.7500 (13.3333) lr 1.0000e-03 eta 0:00:19
epoch [102/200] batch [35/74] time 0.419 (0.447) data 0.288 (0.315) loss_u loss_u 0.8809 (0.8950) acc_u 15.6250 (13.3929) lr 1.0000e-03 eta 0:00:17
epoch [102/200] batch [40/74] time 0.454 (0.446) data 0.321 (0.315) loss_u loss_u 0.8350 (0.8951) acc_u 25.0000 (13.5156) lr 1.0000e-03 eta 0:00:15
epoch [102/200] batch [45/74] time 0.727 (0.455) data 0.595 (0.323) loss_u loss_u 0.9165 (0.8969) acc_u 9.3750 (13.3333) lr 1.0000e-03 eta 0:00:13
epoch [102/200] batch [50/74] time 0.374 (0.455) data 0.243 (0.323) loss_u loss_u 0.8872 (0.8968) acc_u 9.3750 (13.0625) lr 1.0000e-03 eta 0:00:10
epoch [102/200] batch [55/74] time 0.646 (0.458) data 0.514 (0.326) loss_u loss_u 0.9160 (0.8979) acc_u 9.3750 (13.0114) lr 1.0000e-03 eta 0:00:08
epoch [102/200] batch [60/74] time 0.364 (0.456) data 0.233 (0.325) loss_u loss_u 0.8237 (0.8962) acc_u 25.0000 (13.1250) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [65/74] time 0.406 (0.455) data 0.274 (0.324) loss_u loss_u 0.8823 (0.8964) acc_u 18.7500 (13.1731) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [70/74] time 0.607 (0.459) data 0.476 (0.328) loss_u loss_u 0.8530 (0.8947) acc_u 15.6250 (13.3482) lr 1.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2089
confident_label rate tensor(0.2490, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 781
clean true:354
clean false:427
clean_rate:0.4532650448143406
noisy true:693
noisy false:1662
after delete: len(clean_dataset) 781
after delete: len(noisy_dataset) 2355
epoch [103/200] batch [5/24] time 0.428 (0.516) data 0.298 (0.385) loss_x loss_x 0.9067 (1.1412) acc_x 78.1250 (70.6250) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [10/24] time 0.498 (0.471) data 0.366 (0.340) loss_x loss_x 0.7896 (1.2035) acc_x 75.0000 (69.3750) lr 9.8429e-04 eta 0:00:06
epoch [103/200] batch [15/24] time 0.686 (0.476) data 0.556 (0.346) loss_x loss_x 1.0977 (1.2145) acc_x 59.3750 (67.5000) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [20/24] time 0.513 (0.486) data 0.382 (0.355) loss_x loss_x 1.1641 (1.2211) acc_x 68.7500 (67.1875) lr 9.8429e-04 eta 0:00:01
epoch [103/200] batch [5/73] time 0.417 (0.460) data 0.286 (0.329) loss_u loss_u 0.9150 (0.9029) acc_u 9.3750 (13.7500) lr 9.8429e-04 eta 0:00:31
epoch [103/200] batch [10/73] time 0.403 (0.460) data 0.272 (0.329) loss_u loss_u 0.9360 (0.9083) acc_u 6.2500 (12.5000) lr 9.8429e-04 eta 0:00:29
epoch [103/200] batch [15/73] time 0.691 (0.463) data 0.560 (0.332) loss_u loss_u 0.8481 (0.9016) acc_u 21.8750 (13.1250) lr 9.8429e-04 eta 0:00:26
epoch [103/200] batch [20/73] time 0.448 (0.459) data 0.315 (0.328) loss_u loss_u 0.8833 (0.8879) acc_u 15.6250 (15.1562) lr 9.8429e-04 eta 0:00:24
epoch [103/200] batch [25/73] time 0.377 (0.454) data 0.246 (0.323) loss_u loss_u 0.8403 (0.8907) acc_u 21.8750 (14.5000) lr 9.8429e-04 eta 0:00:21
epoch [103/200] batch [30/73] time 0.376 (0.452) data 0.245 (0.321) loss_u loss_u 0.9604 (0.8954) acc_u 3.1250 (13.7500) lr 9.8429e-04 eta 0:00:19
epoch [103/200] batch [35/73] time 0.387 (0.454) data 0.256 (0.323) loss_u loss_u 0.9082 (0.8952) acc_u 6.2500 (13.4821) lr 9.8429e-04 eta 0:00:17
epoch [103/200] batch [40/73] time 0.457 (0.454) data 0.325 (0.323) loss_u loss_u 0.8936 (0.8961) acc_u 18.7500 (13.4375) lr 9.8429e-04 eta 0:00:14
epoch [103/200] batch [45/73] time 0.740 (0.458) data 0.609 (0.327) loss_u loss_u 0.9258 (0.8979) acc_u 12.5000 (13.2639) lr 9.8429e-04 eta 0:00:12
epoch [103/200] batch [50/73] time 0.386 (0.457) data 0.256 (0.326) loss_u loss_u 0.8706 (0.8981) acc_u 18.7500 (13.1875) lr 9.8429e-04 eta 0:00:10
epoch [103/200] batch [55/73] time 0.511 (0.455) data 0.379 (0.324) loss_u loss_u 0.8789 (0.8970) acc_u 15.6250 (13.3523) lr 9.8429e-04 eta 0:00:08
epoch [103/200] batch [60/73] time 0.458 (0.455) data 0.326 (0.324) loss_u loss_u 0.8936 (0.8994) acc_u 12.5000 (13.0208) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [65/73] time 0.418 (0.455) data 0.286 (0.324) loss_u loss_u 0.8896 (0.8992) acc_u 15.6250 (13.1731) lr 9.8429e-04 eta 0:00:03
epoch [103/200] batch [70/73] time 0.400 (0.457) data 0.270 (0.326) loss_u loss_u 0.9438 (0.8994) acc_u 9.3750 (13.3036) lr 9.8429e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2071
confident_label rate tensor(0.2484, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 779
clean true:362
clean false:417
clean_rate:0.46469833119383824
noisy true:703
noisy false:1654
after delete: len(clean_dataset) 779
after delete: len(noisy_dataset) 2357
epoch [104/200] batch [5/24] time 0.485 (0.470) data 0.354 (0.339) loss_x loss_x 1.0332 (1.3325) acc_x 75.0000 (66.2500) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [10/24] time 0.625 (0.492) data 0.493 (0.361) loss_x loss_x 1.3164 (1.3328) acc_x 71.8750 (67.5000) lr 9.6859e-04 eta 0:00:06
epoch [104/200] batch [15/24] time 0.425 (0.500) data 0.294 (0.369) loss_x loss_x 1.7783 (1.4369) acc_x 53.1250 (66.4583) lr 9.6859e-04 eta 0:00:04
epoch [104/200] batch [20/24] time 0.443 (0.494) data 0.313 (0.363) loss_x loss_x 1.0928 (1.4231) acc_x 78.1250 (66.8750) lr 9.6859e-04 eta 0:00:01
epoch [104/200] batch [5/73] time 0.323 (0.473) data 0.191 (0.342) loss_u loss_u 0.8911 (0.9056) acc_u 12.5000 (14.3750) lr 9.6859e-04 eta 0:00:32
epoch [104/200] batch [10/73] time 0.458 (0.473) data 0.327 (0.342) loss_u loss_u 0.8823 (0.9096) acc_u 25.0000 (14.3750) lr 9.6859e-04 eta 0:00:29
epoch [104/200] batch [15/73] time 0.415 (0.475) data 0.282 (0.344) loss_u loss_u 0.9316 (0.9129) acc_u 12.5000 (13.5417) lr 9.6859e-04 eta 0:00:27
epoch [104/200] batch [20/73] time 0.429 (0.475) data 0.298 (0.343) loss_u loss_u 0.9482 (0.9097) acc_u 3.1250 (13.1250) lr 9.6859e-04 eta 0:00:25
epoch [104/200] batch [25/73] time 0.497 (0.473) data 0.366 (0.342) loss_u loss_u 0.9204 (0.9115) acc_u 12.5000 (12.3750) lr 9.6859e-04 eta 0:00:22
epoch [104/200] batch [30/73] time 0.425 (0.470) data 0.295 (0.339) loss_u loss_u 0.9482 (0.9152) acc_u 9.3750 (11.4583) lr 9.6859e-04 eta 0:00:20
epoch [104/200] batch [35/73] time 0.465 (0.472) data 0.334 (0.341) loss_u loss_u 0.8979 (0.9103) acc_u 18.7500 (12.1429) lr 9.6859e-04 eta 0:00:17
epoch [104/200] batch [40/73] time 0.455 (0.471) data 0.322 (0.340) loss_u loss_u 0.8936 (0.9105) acc_u 15.6250 (12.2656) lr 9.6859e-04 eta 0:00:15
epoch [104/200] batch [45/73] time 0.590 (0.471) data 0.458 (0.340) loss_u loss_u 0.8525 (0.9064) acc_u 21.8750 (12.7778) lr 9.6859e-04 eta 0:00:13
epoch [104/200] batch [50/73] time 0.526 (0.471) data 0.394 (0.340) loss_u loss_u 0.8804 (0.9059) acc_u 12.5000 (12.7500) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [55/73] time 0.444 (0.470) data 0.312 (0.339) loss_u loss_u 0.8140 (0.9046) acc_u 25.0000 (12.9545) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [60/73] time 0.354 (0.468) data 0.222 (0.337) loss_u loss_u 0.7607 (0.9031) acc_u 34.3750 (13.1250) lr 9.6859e-04 eta 0:00:06
epoch [104/200] batch [65/73] time 0.673 (0.470) data 0.542 (0.339) loss_u loss_u 0.8745 (0.9019) acc_u 15.6250 (13.1250) lr 9.6859e-04 eta 0:00:03
epoch [104/200] batch [70/73] time 0.376 (0.467) data 0.244 (0.335) loss_u loss_u 0.8867 (0.9007) acc_u 15.6250 (13.3036) lr 9.6859e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2056
confident_label rate tensor(0.2500, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 784
clean true:364
clean false:420
clean_rate:0.4642857142857143
noisy true:716
noisy false:1636
after delete: len(clean_dataset) 784
after delete: len(noisy_dataset) 2352
epoch [105/200] batch [5/24] time 0.454 (0.474) data 0.322 (0.343) loss_x loss_x 1.2070 (1.2104) acc_x 68.7500 (70.6250) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [10/24] time 0.418 (0.457) data 0.287 (0.325) loss_x loss_x 1.2207 (1.2837) acc_x 65.6250 (67.1875) lr 9.5289e-04 eta 0:00:06
epoch [105/200] batch [15/24] time 0.552 (0.472) data 0.422 (0.341) loss_x loss_x 0.8286 (1.2473) acc_x 84.3750 (67.5000) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [20/24] time 0.671 (0.480) data 0.539 (0.349) loss_x loss_x 0.8379 (1.2134) acc_x 75.0000 (67.1875) lr 9.5289e-04 eta 0:00:01
epoch [105/200] batch [5/73] time 0.435 (0.478) data 0.303 (0.347) loss_u loss_u 0.9180 (0.8914) acc_u 15.6250 (15.6250) lr 9.5289e-04 eta 0:00:32
epoch [105/200] batch [10/73] time 0.375 (0.471) data 0.244 (0.339) loss_u loss_u 0.9263 (0.8903) acc_u 9.3750 (16.2500) lr 9.5289e-04 eta 0:00:29
epoch [105/200] batch [15/73] time 0.334 (0.462) data 0.203 (0.331) loss_u loss_u 0.8789 (0.8869) acc_u 15.6250 (15.8333) lr 9.5289e-04 eta 0:00:26
epoch [105/200] batch [20/73] time 0.559 (0.461) data 0.428 (0.330) loss_u loss_u 0.8921 (0.8893) acc_u 12.5000 (14.8438) lr 9.5289e-04 eta 0:00:24
epoch [105/200] batch [25/73] time 0.354 (0.461) data 0.222 (0.329) loss_u loss_u 0.8052 (0.8871) acc_u 31.2500 (15.2500) lr 9.5289e-04 eta 0:00:22
epoch [105/200] batch [30/73] time 0.476 (0.464) data 0.346 (0.333) loss_u loss_u 0.9590 (0.8887) acc_u 3.1250 (14.7917) lr 9.5289e-04 eta 0:00:19
epoch [105/200] batch [35/73] time 0.565 (0.465) data 0.433 (0.334) loss_u loss_u 0.9087 (0.8889) acc_u 12.5000 (14.5536) lr 9.5289e-04 eta 0:00:17
epoch [105/200] batch [40/73] time 0.474 (0.467) data 0.342 (0.336) loss_u loss_u 0.9194 (0.8921) acc_u 9.3750 (13.9844) lr 9.5289e-04 eta 0:00:15
epoch [105/200] batch [45/73] time 0.514 (0.466) data 0.383 (0.335) loss_u loss_u 0.8882 (0.8945) acc_u 15.6250 (13.4722) lr 9.5289e-04 eta 0:00:13
epoch [105/200] batch [50/73] time 0.417 (0.465) data 0.285 (0.333) loss_u loss_u 0.9375 (0.8964) acc_u 9.3750 (13.1875) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [55/73] time 0.552 (0.467) data 0.420 (0.336) loss_u loss_u 0.8931 (0.8964) acc_u 12.5000 (13.0114) lr 9.5289e-04 eta 0:00:08
epoch [105/200] batch [60/73] time 0.452 (0.471) data 0.320 (0.339) loss_u loss_u 0.9229 (0.8992) acc_u 9.3750 (12.7083) lr 9.5289e-04 eta 0:00:06
epoch [105/200] batch [65/73] time 0.451 (0.472) data 0.319 (0.341) loss_u loss_u 0.9595 (0.9003) acc_u 6.2500 (12.4519) lr 9.5289e-04 eta 0:00:03
epoch [105/200] batch [70/73] time 0.421 (0.471) data 0.289 (0.340) loss_u loss_u 0.9780 (0.8983) acc_u 3.1250 (12.7679) lr 9.5289e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2062
confident_label rate tensor(0.2516, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 789
clean true:380
clean false:409
clean_rate:0.4816223067173637
noisy true:694
noisy false:1653
after delete: len(clean_dataset) 789
after delete: len(noisy_dataset) 2347
epoch [106/200] batch [5/24] time 0.363 (0.453) data 0.232 (0.322) loss_x loss_x 1.0859 (1.1918) acc_x 71.8750 (68.1250) lr 9.3721e-04 eta 0:00:08
epoch [106/200] batch [10/24] time 0.448 (0.461) data 0.318 (0.331) loss_x loss_x 1.0127 (1.1358) acc_x 78.1250 (69.6875) lr 9.3721e-04 eta 0:00:06
epoch [106/200] batch [15/24] time 0.423 (0.456) data 0.293 (0.325) loss_x loss_x 0.9336 (1.1890) acc_x 78.1250 (67.7083) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [20/24] time 0.685 (0.463) data 0.554 (0.332) loss_x loss_x 1.4268 (1.2047) acc_x 59.3750 (67.6562) lr 9.3721e-04 eta 0:00:01
epoch [106/200] batch [5/73] time 0.348 (0.447) data 0.218 (0.317) loss_u loss_u 0.8281 (0.8799) acc_u 18.7500 (15.0000) lr 9.3721e-04 eta 0:00:30
epoch [106/200] batch [10/73] time 0.481 (0.450) data 0.350 (0.319) loss_u loss_u 0.8169 (0.8827) acc_u 18.7500 (14.0625) lr 9.3721e-04 eta 0:00:28
epoch [106/200] batch [15/73] time 0.382 (0.446) data 0.251 (0.316) loss_u loss_u 0.9253 (0.8840) acc_u 12.5000 (14.5833) lr 9.3721e-04 eta 0:00:25
epoch [106/200] batch [20/73] time 0.615 (0.449) data 0.485 (0.319) loss_u loss_u 0.8857 (0.8934) acc_u 9.3750 (13.4375) lr 9.3721e-04 eta 0:00:23
epoch [106/200] batch [25/73] time 0.514 (0.453) data 0.382 (0.322) loss_u loss_u 0.9351 (0.8963) acc_u 9.3750 (13.1250) lr 9.3721e-04 eta 0:00:21
epoch [106/200] batch [30/73] time 0.452 (0.457) data 0.320 (0.326) loss_u loss_u 0.9165 (0.9003) acc_u 18.7500 (13.1250) lr 9.3721e-04 eta 0:00:19
epoch [106/200] batch [35/73] time 0.399 (0.455) data 0.268 (0.324) loss_u loss_u 0.9033 (0.9000) acc_u 12.5000 (13.1250) lr 9.3721e-04 eta 0:00:17
epoch [106/200] batch [40/73] time 0.409 (0.458) data 0.279 (0.327) loss_u loss_u 0.9312 (0.9027) acc_u 15.6250 (12.8125) lr 9.3721e-04 eta 0:00:15
epoch [106/200] batch [45/73] time 0.396 (0.458) data 0.263 (0.327) loss_u loss_u 0.9551 (0.9046) acc_u 3.1250 (12.6389) lr 9.3721e-04 eta 0:00:12
epoch [106/200] batch [50/73] time 0.417 (0.456) data 0.283 (0.325) loss_u loss_u 0.9214 (0.9069) acc_u 9.3750 (12.4375) lr 9.3721e-04 eta 0:00:10
epoch [106/200] batch [55/73] time 0.653 (0.460) data 0.522 (0.329) loss_u loss_u 0.9243 (0.9072) acc_u 9.3750 (12.2159) lr 9.3721e-04 eta 0:00:08
epoch [106/200] batch [60/73] time 0.487 (0.462) data 0.355 (0.331) loss_u loss_u 0.9639 (0.9073) acc_u 9.3750 (12.1354) lr 9.3721e-04 eta 0:00:06
epoch [106/200] batch [65/73] time 0.368 (0.458) data 0.235 (0.327) loss_u loss_u 0.8950 (0.9072) acc_u 15.6250 (12.2115) lr 9.3721e-04 eta 0:00:03
epoch [106/200] batch [70/73] time 0.365 (0.459) data 0.233 (0.328) loss_u loss_u 0.8765 (0.9050) acc_u 12.5000 (12.4554) lr 9.3721e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2077
confident_label rate tensor(0.2455, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 770
clean true:370
clean false:400
clean_rate:0.4805194805194805
noisy true:689
noisy false:1677
after delete: len(clean_dataset) 770
after delete: len(noisy_dataset) 2366
epoch [107/200] batch [5/24] time 0.432 (0.433) data 0.301 (0.303) loss_x loss_x 1.1943 (1.2240) acc_x 62.5000 (67.5000) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [10/24] time 0.428 (0.462) data 0.297 (0.331) loss_x loss_x 1.9795 (1.3482) acc_x 62.5000 (67.1875) lr 9.2154e-04 eta 0:00:06
epoch [107/200] batch [15/24] time 0.555 (0.478) data 0.423 (0.347) loss_x loss_x 1.4854 (1.4010) acc_x 53.1250 (63.5417) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [20/24] time 0.498 (0.483) data 0.367 (0.352) loss_x loss_x 1.2910 (1.3404) acc_x 59.3750 (64.6875) lr 9.2154e-04 eta 0:00:01
epoch [107/200] batch [5/73] time 0.363 (0.468) data 0.230 (0.337) loss_u loss_u 0.8574 (0.8895) acc_u 21.8750 (16.2500) lr 9.2154e-04 eta 0:00:31
epoch [107/200] batch [10/73] time 0.322 (0.460) data 0.190 (0.328) loss_u loss_u 0.9111 (0.8908) acc_u 12.5000 (14.0625) lr 9.2154e-04 eta 0:00:28
epoch [107/200] batch [15/73] time 0.539 (0.461) data 0.407 (0.330) loss_u loss_u 0.9131 (0.8870) acc_u 12.5000 (13.5417) lr 9.2154e-04 eta 0:00:26
epoch [107/200] batch [20/73] time 0.657 (0.474) data 0.526 (0.342) loss_u loss_u 0.8730 (0.8878) acc_u 18.7500 (13.4375) lr 9.2154e-04 eta 0:00:25
epoch [107/200] batch [25/73] time 0.390 (0.471) data 0.259 (0.339) loss_u loss_u 0.9287 (0.8934) acc_u 9.3750 (13.0000) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [30/73] time 0.600 (0.467) data 0.469 (0.336) loss_u loss_u 0.9111 (0.8932) acc_u 9.3750 (12.9167) lr 9.2154e-04 eta 0:00:20
epoch [107/200] batch [35/73] time 0.417 (0.467) data 0.286 (0.335) loss_u loss_u 0.9082 (0.8901) acc_u 12.5000 (13.5714) lr 9.2154e-04 eta 0:00:17
epoch [107/200] batch [40/73] time 0.415 (0.464) data 0.283 (0.333) loss_u loss_u 0.9385 (0.8925) acc_u 9.3750 (13.4375) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [45/73] time 0.386 (0.459) data 0.254 (0.328) loss_u loss_u 0.9048 (0.8935) acc_u 12.5000 (13.1250) lr 9.2154e-04 eta 0:00:12
epoch [107/200] batch [50/73] time 0.421 (0.460) data 0.290 (0.328) loss_u loss_u 0.8623 (0.8933) acc_u 18.7500 (13.0625) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [55/73] time 0.473 (0.458) data 0.342 (0.326) loss_u loss_u 0.8716 (0.8945) acc_u 15.6250 (12.8977) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [60/73] time 0.356 (0.457) data 0.224 (0.326) loss_u loss_u 0.8296 (0.8933) acc_u 25.0000 (13.1250) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [65/73] time 0.457 (0.458) data 0.325 (0.326) loss_u loss_u 0.9229 (0.8961) acc_u 9.3750 (12.7404) lr 9.2154e-04 eta 0:00:03
epoch [107/200] batch [70/73] time 0.412 (0.457) data 0.280 (0.326) loss_u loss_u 0.8452 (0.8979) acc_u 21.8750 (12.5000) lr 9.2154e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2057
confident_label rate tensor(0.2529, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 793
clean true:370
clean false:423
clean_rate:0.4665825977301387
noisy true:709
noisy false:1634
after delete: len(clean_dataset) 793
after delete: len(noisy_dataset) 2343
epoch [108/200] batch [5/24] time 0.383 (0.446) data 0.253 (0.315) loss_x loss_x 1.5557 (1.3807) acc_x 65.6250 (63.1250) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [10/24] time 0.467 (0.454) data 0.336 (0.324) loss_x loss_x 1.1289 (1.2273) acc_x 75.0000 (68.1250) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [15/24] time 0.417 (0.454) data 0.286 (0.323) loss_x loss_x 1.1592 (1.2511) acc_x 59.3750 (66.2500) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [20/24] time 0.600 (0.461) data 0.469 (0.331) loss_x loss_x 1.7510 (1.3079) acc_x 46.8750 (65.4688) lr 9.0589e-04 eta 0:00:01
epoch [108/200] batch [5/73] time 0.470 (0.457) data 0.339 (0.326) loss_u loss_u 0.9365 (0.9061) acc_u 12.5000 (11.2500) lr 9.0589e-04 eta 0:00:31
epoch [108/200] batch [10/73] time 0.789 (0.468) data 0.657 (0.337) loss_u loss_u 0.9248 (0.9013) acc_u 15.6250 (12.5000) lr 9.0589e-04 eta 0:00:29
epoch [108/200] batch [15/73] time 0.377 (0.466) data 0.247 (0.335) loss_u loss_u 0.8789 (0.9057) acc_u 18.7500 (11.4583) lr 9.0589e-04 eta 0:00:27
epoch [108/200] batch [20/73] time 0.467 (0.469) data 0.336 (0.338) loss_u loss_u 0.9121 (0.9081) acc_u 9.3750 (11.5625) lr 9.0589e-04 eta 0:00:24
epoch [108/200] batch [25/73] time 0.579 (0.464) data 0.448 (0.333) loss_u loss_u 0.9321 (0.9061) acc_u 6.2500 (11.7500) lr 9.0589e-04 eta 0:00:22
epoch [108/200] batch [30/73] time 0.541 (0.465) data 0.411 (0.334) loss_u loss_u 0.8213 (0.9034) acc_u 18.7500 (12.0833) lr 9.0589e-04 eta 0:00:19
epoch [108/200] batch [35/73] time 0.576 (0.470) data 0.443 (0.339) loss_u loss_u 0.9341 (0.8989) acc_u 6.2500 (12.6786) lr 9.0589e-04 eta 0:00:17
epoch [108/200] batch [40/73] time 0.464 (0.473) data 0.332 (0.342) loss_u loss_u 0.9385 (0.8966) acc_u 6.2500 (12.9688) lr 9.0589e-04 eta 0:00:15
epoch [108/200] batch [45/73] time 0.470 (0.474) data 0.338 (0.343) loss_u loss_u 0.9004 (0.8995) acc_u 15.6250 (12.5694) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [50/73] time 0.480 (0.473) data 0.349 (0.342) loss_u loss_u 0.8071 (0.8966) acc_u 25.0000 (13.0625) lr 9.0589e-04 eta 0:00:10
epoch [108/200] batch [55/73] time 0.389 (0.472) data 0.257 (0.340) loss_u loss_u 0.8779 (0.8988) acc_u 15.6250 (12.8409) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [60/73] time 0.401 (0.469) data 0.270 (0.338) loss_u loss_u 0.8657 (0.8961) acc_u 18.7500 (13.0729) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [65/73] time 0.449 (0.467) data 0.317 (0.336) loss_u loss_u 0.8872 (0.8958) acc_u 9.3750 (12.9808) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [70/73] time 0.380 (0.465) data 0.249 (0.333) loss_u loss_u 0.9634 (0.8971) acc_u 6.2500 (12.9018) lr 9.0589e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2069
confident_label rate tensor(0.2519, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 790
clean true:356
clean false:434
clean_rate:0.4506329113924051
noisy true:711
noisy false:1635
after delete: len(clean_dataset) 790
after delete: len(noisy_dataset) 2346
epoch [109/200] batch [5/24] time 0.441 (0.445) data 0.310 (0.315) loss_x loss_x 1.1191 (1.2061) acc_x 65.6250 (70.0000) lr 8.9027e-04 eta 0:00:08
epoch [109/200] batch [10/24] time 0.414 (0.446) data 0.283 (0.315) loss_x loss_x 1.2100 (1.2618) acc_x 71.8750 (67.8125) lr 8.9027e-04 eta 0:00:06
epoch [109/200] batch [15/24] time 0.406 (0.451) data 0.276 (0.320) loss_x loss_x 1.2666 (1.2714) acc_x 68.7500 (67.7083) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [20/24] time 0.366 (0.450) data 0.236 (0.319) loss_x loss_x 1.3506 (1.3056) acc_x 59.3750 (66.4062) lr 8.9027e-04 eta 0:00:01
epoch [109/200] batch [5/73] time 0.373 (0.452) data 0.242 (0.321) loss_u loss_u 0.8965 (0.9193) acc_u 12.5000 (9.3750) lr 8.9027e-04 eta 0:00:30
epoch [109/200] batch [10/73] time 0.364 (0.447) data 0.233 (0.316) loss_u loss_u 0.9609 (0.9178) acc_u 0.0000 (8.7500) lr 8.9027e-04 eta 0:00:28
epoch [109/200] batch [15/73] time 0.356 (0.443) data 0.225 (0.312) loss_u loss_u 0.9268 (0.9027) acc_u 9.3750 (11.6667) lr 8.9027e-04 eta 0:00:25
epoch [109/200] batch [20/73] time 0.562 (0.447) data 0.431 (0.316) loss_u loss_u 0.8794 (0.9011) acc_u 18.7500 (12.3438) lr 8.9027e-04 eta 0:00:23
epoch [109/200] batch [25/73] time 0.349 (0.448) data 0.218 (0.317) loss_u loss_u 0.8477 (0.9014) acc_u 18.7500 (12.1250) lr 8.9027e-04 eta 0:00:21
epoch [109/200] batch [30/73] time 0.488 (0.451) data 0.356 (0.320) loss_u loss_u 0.8472 (0.9004) acc_u 21.8750 (12.3958) lr 8.9027e-04 eta 0:00:19
epoch [109/200] batch [35/73] time 0.460 (0.455) data 0.330 (0.324) loss_u loss_u 0.8428 (0.8978) acc_u 21.8750 (12.7679) lr 8.9027e-04 eta 0:00:17
epoch [109/200] batch [40/73] time 0.426 (0.457) data 0.295 (0.326) loss_u loss_u 0.8989 (0.8962) acc_u 12.5000 (13.0469) lr 8.9027e-04 eta 0:00:15
epoch [109/200] batch [45/73] time 0.631 (0.461) data 0.501 (0.330) loss_u loss_u 0.8735 (0.8982) acc_u 18.7500 (12.7778) lr 8.9027e-04 eta 0:00:12
epoch [109/200] batch [50/73] time 0.386 (0.457) data 0.254 (0.326) loss_u loss_u 0.9160 (0.9001) acc_u 15.6250 (12.7500) lr 8.9027e-04 eta 0:00:10
epoch [109/200] batch [55/73] time 0.368 (0.456) data 0.237 (0.325) loss_u loss_u 0.9087 (0.9008) acc_u 12.5000 (12.6705) lr 8.9027e-04 eta 0:00:08
epoch [109/200] batch [60/73] time 0.418 (0.457) data 0.288 (0.326) loss_u loss_u 0.8735 (0.8993) acc_u 15.6250 (12.7083) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [65/73] time 0.297 (0.456) data 0.165 (0.325) loss_u loss_u 0.9414 (0.8994) acc_u 9.3750 (12.5481) lr 8.9027e-04 eta 0:00:03
epoch [109/200] batch [70/73] time 0.578 (0.458) data 0.445 (0.327) loss_u loss_u 0.8394 (0.8986) acc_u 21.8750 (12.7232) lr 8.9027e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2079
confident_label rate tensor(0.2417, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 758
clean true:345
clean false:413
clean_rate:0.4551451187335092
noisy true:712
noisy false:1666
after delete: len(clean_dataset) 758
after delete: len(noisy_dataset) 2378
epoch [110/200] batch [5/23] time 0.366 (0.447) data 0.236 (0.316) loss_x loss_x 1.5273 (1.4157) acc_x 59.3750 (69.3750) lr 8.7467e-04 eta 0:00:08
epoch [110/200] batch [10/23] time 0.434 (0.462) data 0.304 (0.331) loss_x loss_x 0.9424 (1.4023) acc_x 68.7500 (65.6250) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [15/23] time 0.426 (0.459) data 0.295 (0.328) loss_x loss_x 0.9253 (1.3726) acc_x 87.5000 (67.2917) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [20/23] time 0.439 (0.467) data 0.309 (0.336) loss_x loss_x 1.1240 (1.3362) acc_x 68.7500 (67.0312) lr 8.7467e-04 eta 0:00:01
epoch [110/200] batch [5/74] time 0.475 (0.458) data 0.344 (0.327) loss_u loss_u 0.8838 (0.8800) acc_u 15.6250 (17.5000) lr 8.7467e-04 eta 0:00:31
epoch [110/200] batch [10/74] time 0.454 (0.456) data 0.323 (0.325) loss_u loss_u 0.9653 (0.9007) acc_u 3.1250 (13.7500) lr 8.7467e-04 eta 0:00:29
epoch [110/200] batch [15/74] time 0.366 (0.452) data 0.236 (0.321) loss_u loss_u 0.9683 (0.8928) acc_u 3.1250 (14.5833) lr 8.7467e-04 eta 0:00:26
epoch [110/200] batch [20/74] time 0.405 (0.452) data 0.275 (0.321) loss_u loss_u 0.8950 (0.8908) acc_u 15.6250 (15.1562) lr 8.7467e-04 eta 0:00:24
epoch [110/200] batch [25/74] time 0.449 (0.448) data 0.318 (0.317) loss_u loss_u 0.8887 (0.8937) acc_u 15.6250 (14.5000) lr 8.7467e-04 eta 0:00:21
epoch [110/200] batch [30/74] time 0.392 (0.444) data 0.261 (0.314) loss_u loss_u 0.8306 (0.8930) acc_u 18.7500 (14.3750) lr 8.7467e-04 eta 0:00:19
epoch [110/200] batch [35/74] time 0.477 (0.442) data 0.346 (0.311) loss_u loss_u 0.9180 (0.8937) acc_u 12.5000 (14.3750) lr 8.7467e-04 eta 0:00:17
epoch [110/200] batch [40/74] time 0.505 (0.447) data 0.374 (0.316) loss_u loss_u 0.9072 (0.8926) acc_u 9.3750 (14.2969) lr 8.7467e-04 eta 0:00:15
epoch [110/200] batch [45/74] time 0.392 (0.455) data 0.262 (0.324) loss_u loss_u 0.9175 (0.8924) acc_u 9.3750 (14.3750) lr 8.7467e-04 eta 0:00:13
epoch [110/200] batch [50/74] time 0.506 (0.455) data 0.375 (0.323) loss_u loss_u 0.9053 (0.8922) acc_u 9.3750 (14.2500) lr 8.7467e-04 eta 0:00:10
epoch [110/200] batch [55/74] time 0.650 (0.458) data 0.519 (0.327) loss_u loss_u 0.8784 (0.8928) acc_u 12.5000 (14.2045) lr 8.7467e-04 eta 0:00:08
epoch [110/200] batch [60/74] time 0.332 (0.455) data 0.202 (0.324) loss_u loss_u 0.8955 (0.8906) acc_u 15.6250 (14.4271) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [65/74] time 0.507 (0.453) data 0.377 (0.322) loss_u loss_u 0.9062 (0.8930) acc_u 15.6250 (14.1827) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [70/74] time 0.359 (0.449) data 0.228 (0.318) loss_u loss_u 0.8872 (0.8942) acc_u 15.6250 (13.9732) lr 8.7467e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2089
confident_label rate tensor(0.2455, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 770
clean true:363
clean false:407
clean_rate:0.4714285714285714
noisy true:684
noisy false:1682
after delete: len(clean_dataset) 770
after delete: len(noisy_dataset) 2366
epoch [111/200] batch [5/24] time 0.334 (0.436) data 0.203 (0.306) loss_x loss_x 1.2949 (1.3656) acc_x 65.6250 (65.6250) lr 8.5910e-04 eta 0:00:08
epoch [111/200] batch [10/24] time 0.445 (0.449) data 0.315 (0.319) loss_x loss_x 1.5508 (1.2505) acc_x 68.7500 (68.1250) lr 8.5910e-04 eta 0:00:06
epoch [111/200] batch [15/24] time 0.539 (0.463) data 0.408 (0.333) loss_x loss_x 1.5156 (1.1948) acc_x 56.2500 (68.1250) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [20/24] time 0.362 (0.458) data 0.232 (0.328) loss_x loss_x 1.3096 (1.1999) acc_x 68.7500 (68.1250) lr 8.5910e-04 eta 0:00:01
epoch [111/200] batch [5/73] time 0.429 (0.456) data 0.297 (0.325) loss_u loss_u 0.8882 (0.8877) acc_u 15.6250 (14.3750) lr 8.5910e-04 eta 0:00:30
epoch [111/200] batch [10/73] time 0.414 (0.459) data 0.283 (0.329) loss_u loss_u 0.9297 (0.9049) acc_u 9.3750 (12.5000) lr 8.5910e-04 eta 0:00:28
epoch [111/200] batch [15/73] time 0.475 (0.460) data 0.344 (0.330) loss_u loss_u 0.8877 (0.9009) acc_u 15.6250 (12.9167) lr 8.5910e-04 eta 0:00:26
epoch [111/200] batch [20/73] time 0.369 (0.460) data 0.237 (0.330) loss_u loss_u 0.9209 (0.9069) acc_u 9.3750 (12.0312) lr 8.5910e-04 eta 0:00:24
epoch [111/200] batch [25/73] time 0.401 (0.460) data 0.271 (0.330) loss_u loss_u 0.9346 (0.9049) acc_u 6.2500 (12.0000) lr 8.5910e-04 eta 0:00:22
epoch [111/200] batch [30/73] time 0.465 (0.461) data 0.334 (0.331) loss_u loss_u 0.9756 (0.9062) acc_u 3.1250 (11.8750) lr 8.5910e-04 eta 0:00:19
epoch [111/200] batch [35/73] time 0.341 (0.456) data 0.210 (0.326) loss_u loss_u 0.8950 (0.9040) acc_u 18.7500 (12.3214) lr 8.5910e-04 eta 0:00:17
epoch [111/200] batch [40/73] time 0.412 (0.453) data 0.281 (0.322) loss_u loss_u 0.9727 (0.9088) acc_u 3.1250 (11.6406) lr 8.5910e-04 eta 0:00:14
epoch [111/200] batch [45/73] time 0.458 (0.454) data 0.326 (0.323) loss_u loss_u 0.8979 (0.9093) acc_u 9.3750 (11.3889) lr 8.5910e-04 eta 0:00:12
epoch [111/200] batch [50/73] time 0.427 (0.457) data 0.296 (0.326) loss_u loss_u 0.9600 (0.9040) acc_u 3.1250 (12.1875) lr 8.5910e-04 eta 0:00:10
epoch [111/200] batch [55/73] time 0.460 (0.456) data 0.329 (0.325) loss_u loss_u 0.9434 (0.9052) acc_u 3.1250 (12.0455) lr 8.5910e-04 eta 0:00:08
epoch [111/200] batch [60/73] time 0.497 (0.455) data 0.367 (0.324) loss_u loss_u 0.8081 (0.9035) acc_u 18.7500 (12.1875) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [65/73] time 0.467 (0.458) data 0.336 (0.327) loss_u loss_u 0.8911 (0.9014) acc_u 15.6250 (12.4519) lr 8.5910e-04 eta 0:00:03
epoch [111/200] batch [70/73] time 0.666 (0.459) data 0.535 (0.328) loss_u loss_u 0.7490 (0.8982) acc_u 37.5000 (13.0804) lr 8.5910e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2114
confident_label rate tensor(0.2532, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 794
clean true:368
clean false:426
clean_rate:0.4634760705289673
noisy true:654
noisy false:1688
after delete: len(clean_dataset) 794
after delete: len(noisy_dataset) 2342
epoch [112/200] batch [5/24] time 0.522 (0.480) data 0.392 (0.349) loss_x loss_x 1.4033 (1.3559) acc_x 53.1250 (59.3750) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [10/24] time 0.496 (0.473) data 0.364 (0.342) loss_x loss_x 1.3730 (1.3677) acc_x 65.6250 (63.1250) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [15/24] time 0.424 (0.468) data 0.294 (0.337) loss_x loss_x 1.2568 (1.3726) acc_x 68.7500 (64.3750) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [20/24] time 0.599 (0.466) data 0.468 (0.335) loss_x loss_x 1.1299 (1.3256) acc_x 59.3750 (65.3125) lr 8.4357e-04 eta 0:00:01
epoch [112/200] batch [5/73] time 0.447 (0.451) data 0.315 (0.320) loss_u loss_u 0.8521 (0.8851) acc_u 21.8750 (15.0000) lr 8.4357e-04 eta 0:00:30
epoch [112/200] batch [10/73] time 0.520 (0.457) data 0.389 (0.326) loss_u loss_u 0.8452 (0.8861) acc_u 15.6250 (13.4375) lr 8.4357e-04 eta 0:00:28
epoch [112/200] batch [15/73] time 0.424 (0.453) data 0.293 (0.322) loss_u loss_u 0.9331 (0.8937) acc_u 9.3750 (12.7083) lr 8.4357e-04 eta 0:00:26
epoch [112/200] batch [20/73] time 0.429 (0.454) data 0.297 (0.323) loss_u loss_u 0.9126 (0.8909) acc_u 9.3750 (13.1250) lr 8.4357e-04 eta 0:00:24
epoch [112/200] batch [25/73] time 0.595 (0.464) data 0.464 (0.333) loss_u loss_u 0.9185 (0.8903) acc_u 12.5000 (13.2500) lr 8.4357e-04 eta 0:00:22
epoch [112/200] batch [30/73] time 0.410 (0.467) data 0.277 (0.336) loss_u loss_u 0.9458 (0.8950) acc_u 3.1250 (12.3958) lr 8.4357e-04 eta 0:00:20
epoch [112/200] batch [35/73] time 0.513 (0.466) data 0.382 (0.335) loss_u loss_u 0.9106 (0.8970) acc_u 9.3750 (12.3214) lr 8.4357e-04 eta 0:00:17
epoch [112/200] batch [40/73] time 0.427 (0.468) data 0.296 (0.336) loss_u loss_u 0.8750 (0.8957) acc_u 12.5000 (12.5000) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [45/73] time 0.550 (0.469) data 0.419 (0.338) loss_u loss_u 0.9214 (0.8927) acc_u 9.3750 (12.8472) lr 8.4357e-04 eta 0:00:13
epoch [112/200] batch [50/73] time 0.509 (0.472) data 0.378 (0.340) loss_u loss_u 0.9058 (0.8923) acc_u 12.5000 (13.1250) lr 8.4357e-04 eta 0:00:10
epoch [112/200] batch [55/73] time 0.408 (0.470) data 0.277 (0.338) loss_u loss_u 0.8408 (0.8938) acc_u 15.6250 (12.9545) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [60/73] time 0.881 (0.473) data 0.750 (0.342) loss_u loss_u 0.9180 (0.8937) acc_u 15.6250 (12.9167) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [65/73] time 0.387 (0.471) data 0.255 (0.340) loss_u loss_u 0.8730 (0.8915) acc_u 21.8750 (13.2692) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [70/73] time 0.571 (0.472) data 0.439 (0.340) loss_u loss_u 0.9150 (0.8922) acc_u 12.5000 (13.1696) lr 8.4357e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2067
confident_label rate tensor(0.2529, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 793
clean true:376
clean false:417
clean_rate:0.47414880201765447
noisy true:693
noisy false:1650
after delete: len(clean_dataset) 793
after delete: len(noisy_dataset) 2343
epoch [113/200] batch [5/24] time 0.501 (0.452) data 0.371 (0.322) loss_x loss_x 1.0967 (1.1438) acc_x 71.8750 (68.7500) lr 8.2807e-04 eta 0:00:08
epoch [113/200] batch [10/24] time 0.530 (0.455) data 0.399 (0.325) loss_x loss_x 1.7764 (1.1657) acc_x 59.3750 (69.3750) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [15/24] time 0.465 (0.445) data 0.335 (0.314) loss_x loss_x 1.0176 (1.1348) acc_x 68.7500 (70.2083) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [20/24] time 0.440 (0.448) data 0.310 (0.318) loss_x loss_x 0.7876 (1.1655) acc_x 84.3750 (69.3750) lr 8.2807e-04 eta 0:00:01
epoch [113/200] batch [5/73] time 0.403 (0.445) data 0.272 (0.314) loss_u loss_u 0.8540 (0.8869) acc_u 25.0000 (18.1250) lr 8.2807e-04 eta 0:00:30
epoch [113/200] batch [10/73] time 0.420 (0.447) data 0.290 (0.317) loss_u loss_u 0.8989 (0.8886) acc_u 12.5000 (16.5625) lr 8.2807e-04 eta 0:00:28
epoch [113/200] batch [15/73] time 0.401 (0.451) data 0.271 (0.321) loss_u loss_u 0.9038 (0.8898) acc_u 18.7500 (15.4167) lr 8.2807e-04 eta 0:00:26
epoch [113/200] batch [20/73] time 0.510 (0.449) data 0.380 (0.318) loss_u loss_u 0.8872 (0.8938) acc_u 15.6250 (14.3750) lr 8.2807e-04 eta 0:00:23
epoch [113/200] batch [25/73] time 0.478 (0.447) data 0.346 (0.316) loss_u loss_u 0.9248 (0.8946) acc_u 9.3750 (14.0000) lr 8.2807e-04 eta 0:00:21
epoch [113/200] batch [30/73] time 0.498 (0.451) data 0.366 (0.321) loss_u loss_u 0.9424 (0.8982) acc_u 6.2500 (13.1250) lr 8.2807e-04 eta 0:00:19
epoch [113/200] batch [35/73] time 0.430 (0.451) data 0.298 (0.320) loss_u loss_u 0.8711 (0.8991) acc_u 15.6250 (13.3929) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [40/73] time 0.446 (0.447) data 0.315 (0.316) loss_u loss_u 0.8931 (0.9031) acc_u 12.5000 (12.6562) lr 8.2807e-04 eta 0:00:14
epoch [113/200] batch [45/73] time 0.459 (0.448) data 0.328 (0.317) loss_u loss_u 0.9463 (0.9013) acc_u 6.2500 (12.8472) lr 8.2807e-04 eta 0:00:12
epoch [113/200] batch [50/73] time 0.498 (0.450) data 0.367 (0.319) loss_u loss_u 0.9097 (0.9044) acc_u 12.5000 (12.5000) lr 8.2807e-04 eta 0:00:10
epoch [113/200] batch [55/73] time 0.450 (0.452) data 0.318 (0.321) loss_u loss_u 0.8779 (0.9065) acc_u 15.6250 (12.2159) lr 8.2807e-04 eta 0:00:08
epoch [113/200] batch [60/73] time 0.371 (0.449) data 0.239 (0.319) loss_u loss_u 0.8984 (0.9053) acc_u 12.5000 (12.4479) lr 8.2807e-04 eta 0:00:05
epoch [113/200] batch [65/73] time 0.444 (0.449) data 0.313 (0.318) loss_u loss_u 0.9453 (0.9054) acc_u 6.2500 (12.3558) lr 8.2807e-04 eta 0:00:03
epoch [113/200] batch [70/73] time 0.598 (0.450) data 0.467 (0.319) loss_u loss_u 0.8784 (0.9027) acc_u 18.7500 (12.7232) lr 8.2807e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2104
confident_label rate tensor(0.2522, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 791
clean true:351
clean false:440
clean_rate:0.44374209860935526
noisy true:681
noisy false:1664
after delete: len(clean_dataset) 791
after delete: len(noisy_dataset) 2345
epoch [114/200] batch [5/24] time 0.478 (0.475) data 0.347 (0.343) loss_x loss_x 1.5957 (1.1780) acc_x 62.5000 (70.0000) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [10/24] time 0.430 (0.468) data 0.299 (0.337) loss_x loss_x 1.2363 (1.1629) acc_x 71.8750 (70.0000) lr 8.1262e-04 eta 0:00:06
epoch [114/200] batch [15/24] time 0.444 (0.469) data 0.313 (0.339) loss_x loss_x 1.7910 (1.2063) acc_x 62.5000 (68.7500) lr 8.1262e-04 eta 0:00:04
epoch [114/200] batch [20/24] time 0.499 (0.471) data 0.368 (0.340) loss_x loss_x 1.0771 (1.1750) acc_x 65.6250 (69.2188) lr 8.1262e-04 eta 0:00:01
epoch [114/200] batch [5/73] time 0.607 (0.480) data 0.475 (0.349) loss_u loss_u 0.9365 (0.9210) acc_u 9.3750 (10.0000) lr 8.1262e-04 eta 0:00:32
epoch [114/200] batch [10/73] time 0.376 (0.475) data 0.244 (0.344) loss_u loss_u 0.9097 (0.9242) acc_u 12.5000 (10.9375) lr 8.1262e-04 eta 0:00:29
epoch [114/200] batch [15/73] time 0.525 (0.477) data 0.393 (0.346) loss_u loss_u 0.9331 (0.9200) acc_u 9.3750 (11.2500) lr 8.1262e-04 eta 0:00:27
epoch [114/200] batch [20/73] time 0.480 (0.484) data 0.348 (0.353) loss_u loss_u 0.9126 (0.9139) acc_u 12.5000 (11.7188) lr 8.1262e-04 eta 0:00:25
epoch [114/200] batch [25/73] time 0.427 (0.482) data 0.295 (0.351) loss_u loss_u 0.8960 (0.9067) acc_u 12.5000 (12.7500) lr 8.1262e-04 eta 0:00:23
epoch [114/200] batch [30/73] time 0.548 (0.484) data 0.417 (0.353) loss_u loss_u 0.9331 (0.9073) acc_u 9.3750 (12.8125) lr 8.1262e-04 eta 0:00:20
epoch [114/200] batch [35/73] time 0.373 (0.482) data 0.241 (0.350) loss_u loss_u 0.9062 (0.9051) acc_u 9.3750 (12.7679) lr 8.1262e-04 eta 0:00:18
epoch [114/200] batch [40/73] time 0.395 (0.476) data 0.264 (0.345) loss_u loss_u 0.9458 (0.9071) acc_u 6.2500 (12.3438) lr 8.1262e-04 eta 0:00:15
epoch [114/200] batch [45/73] time 0.431 (0.471) data 0.299 (0.340) loss_u loss_u 0.9214 (0.9053) acc_u 12.5000 (12.3611) lr 8.1262e-04 eta 0:00:13
epoch [114/200] batch [50/73] time 0.457 (0.471) data 0.327 (0.340) loss_u loss_u 0.8872 (0.9049) acc_u 18.7500 (12.5000) lr 8.1262e-04 eta 0:00:10
epoch [114/200] batch [55/73] time 0.378 (0.467) data 0.247 (0.335) loss_u loss_u 0.9189 (0.9047) acc_u 9.3750 (12.3864) lr 8.1262e-04 eta 0:00:08
epoch [114/200] batch [60/73] time 0.466 (0.468) data 0.334 (0.337) loss_u loss_u 0.8359 (0.9048) acc_u 25.0000 (12.2917) lr 8.1262e-04 eta 0:00:06
epoch [114/200] batch [65/73] time 0.560 (0.475) data 0.429 (0.344) loss_u loss_u 0.9351 (0.9028) acc_u 6.2500 (12.5000) lr 8.1262e-04 eta 0:00:03
epoch [114/200] batch [70/73] time 0.507 (0.475) data 0.376 (0.343) loss_u loss_u 0.8877 (0.9015) acc_u 12.5000 (12.6339) lr 8.1262e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2098
confident_label rate tensor(0.2474, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 776
clean true:343
clean false:433
clean_rate:0.4420103092783505
noisy true:695
noisy false:1665
after delete: len(clean_dataset) 776
after delete: len(noisy_dataset) 2360
epoch [115/200] batch [5/24] time 0.779 (0.478) data 0.648 (0.347) loss_x loss_x 1.2012 (1.3236) acc_x 71.8750 (68.7500) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [10/24] time 0.634 (0.491) data 0.504 (0.360) loss_x loss_x 0.9766 (1.1932) acc_x 78.1250 (73.1250) lr 7.9721e-04 eta 0:00:06
epoch [115/200] batch [15/24] time 0.473 (0.486) data 0.342 (0.356) loss_x loss_x 1.0283 (1.2534) acc_x 81.2500 (71.6667) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [20/24] time 0.413 (0.478) data 0.282 (0.348) loss_x loss_x 1.5137 (1.2527) acc_x 62.5000 (71.4062) lr 7.9721e-04 eta 0:00:01
epoch [115/200] batch [5/73] time 0.418 (0.458) data 0.288 (0.328) loss_u loss_u 0.8926 (0.8604) acc_u 15.6250 (18.7500) lr 7.9721e-04 eta 0:00:31
epoch [115/200] batch [10/73] time 0.352 (0.452) data 0.221 (0.322) loss_u loss_u 0.9229 (0.8716) acc_u 12.5000 (16.8750) lr 7.9721e-04 eta 0:00:28
epoch [115/200] batch [15/73] time 0.545 (0.451) data 0.413 (0.320) loss_u loss_u 0.9058 (0.8848) acc_u 9.3750 (14.5833) lr 7.9721e-04 eta 0:00:26
epoch [115/200] batch [20/73] time 0.390 (0.447) data 0.258 (0.316) loss_u loss_u 0.9102 (0.8836) acc_u 9.3750 (14.8438) lr 7.9721e-04 eta 0:00:23
epoch [115/200] batch [25/73] time 0.497 (0.452) data 0.365 (0.321) loss_u loss_u 0.9517 (0.8899) acc_u 3.1250 (13.8750) lr 7.9721e-04 eta 0:00:21
epoch [115/200] batch [30/73] time 0.471 (0.454) data 0.339 (0.323) loss_u loss_u 0.9404 (0.8938) acc_u 9.3750 (13.3333) lr 7.9721e-04 eta 0:00:19
epoch [115/200] batch [35/73] time 0.386 (0.454) data 0.254 (0.323) loss_u loss_u 0.8975 (0.8944) acc_u 15.6250 (13.3036) lr 7.9721e-04 eta 0:00:17
epoch [115/200] batch [40/73] time 0.489 (0.452) data 0.358 (0.321) loss_u loss_u 0.8477 (0.8932) acc_u 15.6250 (13.6719) lr 7.9721e-04 eta 0:00:14
epoch [115/200] batch [45/73] time 0.394 (0.451) data 0.263 (0.320) loss_u loss_u 0.9146 (0.8931) acc_u 9.3750 (13.6806) lr 7.9721e-04 eta 0:00:12
epoch [115/200] batch [50/73] time 0.822 (0.456) data 0.690 (0.325) loss_u loss_u 0.8862 (0.8933) acc_u 9.3750 (13.6250) lr 7.9721e-04 eta 0:00:10
epoch [115/200] batch [55/73] time 0.447 (0.455) data 0.317 (0.323) loss_u loss_u 0.9321 (0.8935) acc_u 9.3750 (13.5227) lr 7.9721e-04 eta 0:00:08
epoch [115/200] batch [60/73] time 0.395 (0.453) data 0.265 (0.322) loss_u loss_u 0.9487 (0.8966) acc_u 6.2500 (13.0729) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [65/73] time 0.469 (0.454) data 0.338 (0.323) loss_u loss_u 0.8809 (0.8938) acc_u 18.7500 (13.5096) lr 7.9721e-04 eta 0:00:03
epoch [115/200] batch [70/73] time 0.434 (0.455) data 0.302 (0.324) loss_u loss_u 0.8911 (0.8941) acc_u 21.8750 (13.5714) lr 7.9721e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2106
confident_label rate tensor(0.2538, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 796
clean true:363
clean false:433
clean_rate:0.45603015075376885
noisy true:667
noisy false:1673
after delete: len(clean_dataset) 796
after delete: len(noisy_dataset) 2340
epoch [116/200] batch [5/24] time 0.506 (0.449) data 0.376 (0.319) loss_x loss_x 1.2461 (1.4159) acc_x 59.3750 (67.5000) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [10/24] time 0.399 (0.460) data 0.268 (0.330) loss_x loss_x 1.4473 (1.2610) acc_x 62.5000 (70.6250) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [15/24] time 0.535 (0.476) data 0.405 (0.345) loss_x loss_x 1.3145 (1.1641) acc_x 62.5000 (70.8333) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [20/24] time 0.448 (0.471) data 0.317 (0.340) loss_x loss_x 1.0430 (1.1537) acc_x 75.0000 (70.7812) lr 7.8186e-04 eta 0:00:01
epoch [116/200] batch [5/73] time 0.476 (0.473) data 0.344 (0.343) loss_u loss_u 0.8721 (0.8785) acc_u 18.7500 (16.8750) lr 7.8186e-04 eta 0:00:32
epoch [116/200] batch [10/73] time 0.420 (0.471) data 0.288 (0.340) loss_u loss_u 0.9033 (0.8969) acc_u 12.5000 (13.7500) lr 7.8186e-04 eta 0:00:29
epoch [116/200] batch [15/73] time 0.481 (0.461) data 0.350 (0.331) loss_u loss_u 0.8853 (0.8880) acc_u 15.6250 (15.0000) lr 7.8186e-04 eta 0:00:26
epoch [116/200] batch [20/73] time 0.409 (0.460) data 0.277 (0.329) loss_u loss_u 0.9453 (0.8996) acc_u 9.3750 (13.2812) lr 7.8186e-04 eta 0:00:24
epoch [116/200] batch [25/73] time 0.441 (0.459) data 0.310 (0.328) loss_u loss_u 0.8984 (0.8977) acc_u 9.3750 (13.0000) lr 7.8186e-04 eta 0:00:22
epoch [116/200] batch [30/73] time 0.344 (0.454) data 0.214 (0.323) loss_u loss_u 0.9106 (0.8982) acc_u 9.3750 (12.5000) lr 7.8186e-04 eta 0:00:19
epoch [116/200] batch [35/73] time 0.643 (0.457) data 0.512 (0.326) loss_u loss_u 0.8965 (0.8983) acc_u 9.3750 (12.8571) lr 7.8186e-04 eta 0:00:17
epoch [116/200] batch [40/73] time 0.451 (0.459) data 0.320 (0.328) loss_u loss_u 0.9336 (0.9001) acc_u 9.3750 (12.5781) lr 7.8186e-04 eta 0:00:15
epoch [116/200] batch [45/73] time 0.474 (0.457) data 0.342 (0.326) loss_u loss_u 0.9429 (0.9028) acc_u 6.2500 (12.0139) lr 7.8186e-04 eta 0:00:12
epoch [116/200] batch [50/73] time 0.639 (0.459) data 0.508 (0.328) loss_u loss_u 0.9009 (0.9009) acc_u 12.5000 (12.3125) lr 7.8186e-04 eta 0:00:10
epoch [116/200] batch [55/73] time 0.452 (0.459) data 0.320 (0.328) loss_u loss_u 0.8975 (0.9011) acc_u 15.6250 (12.2727) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [60/73] time 0.498 (0.462) data 0.367 (0.331) loss_u loss_u 0.9556 (0.9034) acc_u 6.2500 (12.0312) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [65/73] time 0.405 (0.463) data 0.274 (0.332) loss_u loss_u 0.8618 (0.9031) acc_u 21.8750 (12.2596) lr 7.8186e-04 eta 0:00:03
epoch [116/200] batch [70/73] time 0.628 (0.463) data 0.495 (0.332) loss_u loss_u 0.9009 (0.9038) acc_u 12.5000 (12.1429) lr 7.8186e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2102
confident_label rate tensor(0.2487, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 780
clean true:341
clean false:439
clean_rate:0.4371794871794872
noisy true:693
noisy false:1663
after delete: len(clean_dataset) 780
after delete: len(noisy_dataset) 2356
epoch [117/200] batch [5/24] time 0.402 (0.449) data 0.271 (0.318) loss_x loss_x 1.7822 (1.4129) acc_x 59.3750 (63.1250) lr 7.6655e-04 eta 0:00:08
epoch [117/200] batch [10/24] time 0.469 (0.439) data 0.338 (0.307) loss_x loss_x 1.3086 (1.3202) acc_x 62.5000 (63.1250) lr 7.6655e-04 eta 0:00:06
epoch [117/200] batch [15/24] time 0.665 (0.451) data 0.534 (0.319) loss_x loss_x 1.2803 (1.2915) acc_x 68.7500 (64.7917) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [20/24] time 0.405 (0.454) data 0.274 (0.323) loss_x loss_x 1.4072 (1.2874) acc_x 59.3750 (65.9375) lr 7.6655e-04 eta 0:00:01
epoch [117/200] batch [5/73] time 0.451 (0.458) data 0.321 (0.326) loss_u loss_u 0.8481 (0.8949) acc_u 25.0000 (15.0000) lr 7.6655e-04 eta 0:00:31
epoch [117/200] batch [10/73] time 0.448 (0.461) data 0.316 (0.330) loss_u loss_u 0.9102 (0.8860) acc_u 9.3750 (15.3125) lr 7.6655e-04 eta 0:00:29
epoch [117/200] batch [15/73] time 0.527 (0.462) data 0.392 (0.330) loss_u loss_u 0.9116 (0.8922) acc_u 12.5000 (14.1667) lr 7.6655e-04 eta 0:00:26
epoch [117/200] batch [20/73] time 0.384 (0.461) data 0.252 (0.329) loss_u loss_u 0.9326 (0.8930) acc_u 9.3750 (13.7500) lr 7.6655e-04 eta 0:00:24
epoch [117/200] batch [25/73] time 0.445 (0.462) data 0.312 (0.331) loss_u loss_u 0.9028 (0.8915) acc_u 9.3750 (14.2500) lr 7.6655e-04 eta 0:00:22
epoch [117/200] batch [30/73] time 0.375 (0.459) data 0.244 (0.328) loss_u loss_u 0.8750 (0.8928) acc_u 18.7500 (14.1667) lr 7.6655e-04 eta 0:00:19
epoch [117/200] batch [35/73] time 0.439 (0.461) data 0.308 (0.329) loss_u loss_u 0.9004 (0.8944) acc_u 12.5000 (13.9286) lr 7.6655e-04 eta 0:00:17
epoch [117/200] batch [40/73] time 0.441 (0.462) data 0.310 (0.330) loss_u loss_u 0.9243 (0.8936) acc_u 15.6250 (14.2969) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [45/73] time 0.458 (0.462) data 0.327 (0.331) loss_u loss_u 0.9199 (0.8974) acc_u 12.5000 (13.7500) lr 7.6655e-04 eta 0:00:12
epoch [117/200] batch [50/73] time 0.441 (0.462) data 0.310 (0.330) loss_u loss_u 0.8750 (0.8981) acc_u 9.3750 (13.3125) lr 7.6655e-04 eta 0:00:10
epoch [117/200] batch [55/73] time 0.377 (0.458) data 0.246 (0.326) loss_u loss_u 0.9077 (0.8973) acc_u 12.5000 (13.2386) lr 7.6655e-04 eta 0:00:08
epoch [117/200] batch [60/73] time 0.501 (0.456) data 0.370 (0.325) loss_u loss_u 0.8843 (0.8983) acc_u 9.3750 (13.2292) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [65/73] time 0.437 (0.455) data 0.306 (0.324) loss_u loss_u 0.7573 (0.8939) acc_u 34.3750 (13.7981) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [70/73] time 0.401 (0.458) data 0.269 (0.327) loss_u loss_u 0.8354 (0.8924) acc_u 18.7500 (13.8839) lr 7.6655e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2080
confident_label rate tensor(0.2596, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 814
clean true:367
clean false:447
clean_rate:0.45085995085995084
noisy true:689
noisy false:1633
after delete: len(clean_dataset) 814
after delete: len(noisy_dataset) 2322
epoch [118/200] batch [5/25] time 0.412 (0.464) data 0.282 (0.334) loss_x loss_x 1.5898 (1.2132) acc_x 65.6250 (70.6250) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [10/25] time 0.421 (0.454) data 0.291 (0.324) loss_x loss_x 1.3398 (1.2219) acc_x 68.7500 (70.0000) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [15/25] time 0.424 (0.447) data 0.293 (0.316) loss_x loss_x 1.2715 (1.2264) acc_x 65.6250 (68.3333) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [20/25] time 0.570 (0.457) data 0.439 (0.326) loss_x loss_x 1.0713 (1.2815) acc_x 71.8750 (67.5000) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [25/25] time 0.592 (0.460) data 0.461 (0.329) loss_x loss_x 1.6025 (1.2913) acc_x 59.3750 (66.1250) lr 7.5131e-04 eta 0:00:00
epoch [118/200] batch [5/72] time 0.376 (0.460) data 0.244 (0.329) loss_u loss_u 0.8696 (0.9006) acc_u 21.8750 (14.3750) lr 7.5131e-04 eta 0:00:30
epoch [118/200] batch [10/72] time 0.384 (0.455) data 0.252 (0.324) loss_u loss_u 0.8931 (0.9023) acc_u 12.5000 (12.8125) lr 7.5131e-04 eta 0:00:28
epoch [118/200] batch [15/72] time 0.394 (0.461) data 0.262 (0.330) loss_u loss_u 0.8921 (0.9103) acc_u 15.6250 (12.0833) lr 7.5131e-04 eta 0:00:26
epoch [118/200] batch [20/72] time 0.456 (0.462) data 0.324 (0.331) loss_u loss_u 0.9458 (0.9025) acc_u 9.3750 (13.2812) lr 7.5131e-04 eta 0:00:24
epoch [118/200] batch [25/72] time 0.550 (0.465) data 0.419 (0.334) loss_u loss_u 0.9517 (0.8973) acc_u 6.2500 (14.1250) lr 7.5131e-04 eta 0:00:21
epoch [118/200] batch [30/72] time 0.470 (0.465) data 0.339 (0.334) loss_u loss_u 0.9097 (0.9002) acc_u 9.3750 (13.7500) lr 7.5131e-04 eta 0:00:19
epoch [118/200] batch [35/72] time 0.457 (0.465) data 0.326 (0.333) loss_u loss_u 0.9570 (0.9009) acc_u 6.2500 (13.7500) lr 7.5131e-04 eta 0:00:17
epoch [118/200] batch [40/72] time 0.493 (0.470) data 0.362 (0.338) loss_u loss_u 0.9360 (0.9012) acc_u 6.2500 (13.3594) lr 7.5131e-04 eta 0:00:15
epoch [118/200] batch [45/72] time 0.509 (0.469) data 0.377 (0.338) loss_u loss_u 0.8940 (0.9018) acc_u 6.2500 (13.0556) lr 7.5131e-04 eta 0:00:12
epoch [118/200] batch [50/72] time 0.412 (0.472) data 0.282 (0.341) loss_u loss_u 0.9795 (0.9031) acc_u 0.0000 (12.8750) lr 7.5131e-04 eta 0:00:10
epoch [118/200] batch [55/72] time 0.373 (0.469) data 0.241 (0.337) loss_u loss_u 0.9097 (0.9025) acc_u 12.5000 (12.7841) lr 7.5131e-04 eta 0:00:07
epoch [118/200] batch [60/72] time 0.378 (0.468) data 0.248 (0.336) loss_u loss_u 0.9072 (0.9033) acc_u 12.5000 (12.5521) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [65/72] time 0.541 (0.469) data 0.409 (0.338) loss_u loss_u 0.8833 (0.9038) acc_u 21.8750 (12.5481) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [70/72] time 0.401 (0.471) data 0.270 (0.340) loss_u loss_u 0.9419 (0.9028) acc_u 12.5000 (12.6339) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2091
confident_label rate tensor(0.2452, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 769
clean true:354
clean false:415
clean_rate:0.46033810143042914
noisy true:691
noisy false:1676
after delete: len(clean_dataset) 769
after delete: len(noisy_dataset) 2367
epoch [119/200] batch [5/24] time 0.364 (0.431) data 0.234 (0.300) loss_x loss_x 1.3682 (1.0921) acc_x 56.2500 (68.7500) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [10/24] time 0.417 (0.435) data 0.287 (0.304) loss_x loss_x 1.2451 (1.1801) acc_x 65.6250 (69.0625) lr 7.3613e-04 eta 0:00:06
epoch [119/200] batch [15/24] time 0.449 (0.441) data 0.318 (0.310) loss_x loss_x 1.0137 (1.1555) acc_x 78.1250 (70.0000) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [20/24] time 0.562 (0.461) data 0.432 (0.331) loss_x loss_x 1.0186 (1.1377) acc_x 65.6250 (70.0000) lr 7.3613e-04 eta 0:00:01
epoch [119/200] batch [5/73] time 0.406 (0.459) data 0.275 (0.328) loss_u loss_u 0.9482 (0.9159) acc_u 3.1250 (10.0000) lr 7.3613e-04 eta 0:00:31
epoch [119/200] batch [10/73] time 0.359 (0.449) data 0.227 (0.319) loss_u loss_u 0.9121 (0.9065) acc_u 15.6250 (12.1875) lr 7.3613e-04 eta 0:00:28
epoch [119/200] batch [15/73] time 0.482 (0.454) data 0.351 (0.323) loss_u loss_u 0.8760 (0.9067) acc_u 15.6250 (12.0833) lr 7.3613e-04 eta 0:00:26
epoch [119/200] batch [20/73] time 0.379 (0.453) data 0.247 (0.322) loss_u loss_u 0.8154 (0.8990) acc_u 28.1250 (12.9688) lr 7.3613e-04 eta 0:00:23
epoch [119/200] batch [25/73] time 0.390 (0.454) data 0.259 (0.323) loss_u loss_u 0.8140 (0.8974) acc_u 25.0000 (13.1250) lr 7.3613e-04 eta 0:00:21
epoch [119/200] batch [30/73] time 0.405 (0.452) data 0.274 (0.321) loss_u loss_u 0.8945 (0.8960) acc_u 9.3750 (13.0208) lr 7.3613e-04 eta 0:00:19
epoch [119/200] batch [35/73] time 0.570 (0.453) data 0.438 (0.322) loss_u loss_u 0.8755 (0.8953) acc_u 12.5000 (12.8571) lr 7.3613e-04 eta 0:00:17
epoch [119/200] batch [40/73] time 0.522 (0.457) data 0.390 (0.326) loss_u loss_u 0.8540 (0.8968) acc_u 15.6250 (12.5000) lr 7.3613e-04 eta 0:00:15
epoch [119/200] batch [45/73] time 0.360 (0.455) data 0.230 (0.324) loss_u loss_u 0.8242 (0.8952) acc_u 25.0000 (12.5694) lr 7.3613e-04 eta 0:00:12
epoch [119/200] batch [50/73] time 0.404 (0.456) data 0.273 (0.324) loss_u loss_u 0.8467 (0.8909) acc_u 18.7500 (13.1875) lr 7.3613e-04 eta 0:00:10
epoch [119/200] batch [55/73] time 0.419 (0.458) data 0.288 (0.327) loss_u loss_u 0.9189 (0.8931) acc_u 9.3750 (12.8977) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [60/73] time 0.400 (0.457) data 0.268 (0.326) loss_u loss_u 0.9121 (0.8947) acc_u 12.5000 (12.6562) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [65/73] time 0.398 (0.456) data 0.267 (0.325) loss_u loss_u 0.8574 (0.8942) acc_u 18.7500 (12.6923) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [70/73] time 0.612 (0.456) data 0.481 (0.325) loss_u loss_u 0.9263 (0.8948) acc_u 12.5000 (12.6339) lr 7.3613e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2075
confident_label rate tensor(0.2526, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 792
clean true:377
clean false:415
clean_rate:0.476010101010101
noisy true:684
noisy false:1660
after delete: len(clean_dataset) 792
after delete: len(noisy_dataset) 2344
epoch [120/200] batch [5/24] time 0.426 (0.475) data 0.296 (0.344) loss_x loss_x 1.0684 (1.1319) acc_x 65.6250 (71.8750) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [10/24] time 0.513 (0.472) data 0.382 (0.342) loss_x loss_x 1.1279 (1.2013) acc_x 65.6250 (70.0000) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [15/24] time 0.604 (0.491) data 0.474 (0.360) loss_x loss_x 0.9854 (1.1645) acc_x 78.1250 (70.6250) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [20/24] time 0.442 (0.476) data 0.311 (0.345) loss_x loss_x 1.2432 (1.1687) acc_x 65.6250 (69.8438) lr 7.2101e-04 eta 0:00:01
epoch [120/200] batch [5/73] time 0.484 (0.471) data 0.352 (0.340) loss_u loss_u 0.9316 (0.9182) acc_u 9.3750 (10.0000) lr 7.2101e-04 eta 0:00:32
epoch [120/200] batch [10/73] time 0.409 (0.469) data 0.277 (0.338) loss_u loss_u 0.9087 (0.9160) acc_u 12.5000 (10.0000) lr 7.2101e-04 eta 0:00:29
epoch [120/200] batch [15/73] time 0.549 (0.474) data 0.418 (0.343) loss_u loss_u 0.8677 (0.9087) acc_u 15.6250 (11.4583) lr 7.2101e-04 eta 0:00:27
epoch [120/200] batch [20/73] time 0.477 (0.470) data 0.345 (0.339) loss_u loss_u 0.8779 (0.9031) acc_u 15.6250 (12.1875) lr 7.2101e-04 eta 0:00:24
epoch [120/200] batch [25/73] time 0.403 (0.466) data 0.273 (0.335) loss_u loss_u 0.8545 (0.8984) acc_u 21.8750 (13.2500) lr 7.2101e-04 eta 0:00:22
epoch [120/200] batch [30/73] time 0.375 (0.461) data 0.245 (0.330) loss_u loss_u 0.8687 (0.8965) acc_u 18.7500 (13.3333) lr 7.2101e-04 eta 0:00:19
epoch [120/200] batch [35/73] time 0.417 (0.468) data 0.287 (0.337) loss_u loss_u 0.9351 (0.8963) acc_u 6.2500 (13.4821) lr 7.2101e-04 eta 0:00:17
epoch [120/200] batch [40/73] time 0.432 (0.466) data 0.301 (0.335) loss_u loss_u 0.9238 (0.8954) acc_u 12.5000 (13.7500) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [45/73] time 0.419 (0.470) data 0.288 (0.339) loss_u loss_u 0.9058 (0.8974) acc_u 12.5000 (13.5417) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [50/73] time 0.391 (0.467) data 0.259 (0.336) loss_u loss_u 0.8940 (0.8963) acc_u 12.5000 (13.7500) lr 7.2101e-04 eta 0:00:10
epoch [120/200] batch [55/73] time 0.364 (0.467) data 0.233 (0.336) loss_u loss_u 0.9761 (0.8972) acc_u 3.1250 (13.5795) lr 7.2101e-04 eta 0:00:08
epoch [120/200] batch [60/73] time 0.474 (0.466) data 0.344 (0.335) loss_u loss_u 0.9131 (0.9003) acc_u 9.3750 (12.9167) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [65/73] time 0.528 (0.472) data 0.398 (0.341) loss_u loss_u 0.9048 (0.9007) acc_u 12.5000 (12.6923) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [70/73] time 0.428 (0.471) data 0.297 (0.340) loss_u loss_u 0.9419 (0.9009) acc_u 9.3750 (12.7679) lr 7.2101e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2093
confident_label rate tensor(0.2615, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 820
clean true:379
clean false:441
clean_rate:0.46219512195121953
noisy true:664
noisy false:1652
after delete: len(clean_dataset) 820
after delete: len(noisy_dataset) 2316
epoch [121/200] batch [5/25] time 0.431 (0.417) data 0.301 (0.286) loss_x loss_x 1.0400 (1.3082) acc_x 81.2500 (70.6250) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [10/25] time 0.382 (0.435) data 0.252 (0.304) loss_x loss_x 0.8672 (1.2721) acc_x 81.2500 (69.6875) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [15/25] time 0.502 (0.451) data 0.372 (0.320) loss_x loss_x 1.3867 (1.2664) acc_x 71.8750 (69.5833) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [20/25] time 0.561 (0.448) data 0.430 (0.318) loss_x loss_x 0.7983 (1.2306) acc_x 81.2500 (69.8438) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [25/25] time 0.402 (0.455) data 0.271 (0.324) loss_x loss_x 0.9233 (1.2228) acc_x 71.8750 (69.2500) lr 7.0596e-04 eta 0:00:00
epoch [121/200] batch [5/72] time 0.451 (0.443) data 0.321 (0.312) loss_u loss_u 0.9355 (0.9235) acc_u 9.3750 (9.3750) lr 7.0596e-04 eta 0:00:29
epoch [121/200] batch [10/72] time 0.430 (0.448) data 0.298 (0.318) loss_u loss_u 0.7871 (0.8891) acc_u 25.0000 (14.0625) lr 7.0596e-04 eta 0:00:27
epoch [121/200] batch [15/72] time 0.509 (0.449) data 0.375 (0.318) loss_u loss_u 0.9507 (0.8887) acc_u 6.2500 (14.1667) lr 7.0596e-04 eta 0:00:25
epoch [121/200] batch [20/72] time 0.442 (0.448) data 0.310 (0.317) loss_u loss_u 0.8345 (0.8871) acc_u 15.6250 (14.3750) lr 7.0596e-04 eta 0:00:23
epoch [121/200] batch [25/72] time 0.443 (0.452) data 0.312 (0.321) loss_u loss_u 0.9292 (0.8838) acc_u 6.2500 (14.5000) lr 7.0596e-04 eta 0:00:21
epoch [121/200] batch [30/72] time 0.442 (0.454) data 0.309 (0.323) loss_u loss_u 0.8696 (0.8829) acc_u 21.8750 (14.5833) lr 7.0596e-04 eta 0:00:19
epoch [121/200] batch [35/72] time 0.593 (0.458) data 0.462 (0.327) loss_u loss_u 0.8462 (0.8848) acc_u 21.8750 (14.5536) lr 7.0596e-04 eta 0:00:16
epoch [121/200] batch [40/72] time 0.572 (0.463) data 0.441 (0.332) loss_u loss_u 0.8721 (0.8871) acc_u 18.7500 (14.2969) lr 7.0596e-04 eta 0:00:14
epoch [121/200] batch [45/72] time 0.440 (0.464) data 0.308 (0.333) loss_u loss_u 0.9282 (0.8931) acc_u 9.3750 (13.6806) lr 7.0596e-04 eta 0:00:12
epoch [121/200] batch [50/72] time 0.515 (0.466) data 0.383 (0.335) loss_u loss_u 0.9263 (0.8950) acc_u 9.3750 (13.4375) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [55/72] time 0.501 (0.465) data 0.370 (0.334) loss_u loss_u 0.9048 (0.8963) acc_u 12.5000 (13.4091) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [60/72] time 0.495 (0.464) data 0.364 (0.333) loss_u loss_u 0.9009 (0.8961) acc_u 15.6250 (13.5938) lr 7.0596e-04 eta 0:00:05
epoch [121/200] batch [65/72] time 0.359 (0.467) data 0.228 (0.336) loss_u loss_u 0.8921 (0.8989) acc_u 15.6250 (13.2212) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [70/72] time 0.600 (0.468) data 0.468 (0.337) loss_u loss_u 0.8765 (0.8970) acc_u 18.7500 (13.3929) lr 7.0596e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2110
confident_label rate tensor(0.2605, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 817
clean true:365
clean false:452
clean_rate:0.4467564259485924
noisy true:661
noisy false:1658
after delete: len(clean_dataset) 817
after delete: len(noisy_dataset) 2319
epoch [122/200] batch [5/25] time 0.604 (0.477) data 0.473 (0.346) loss_x loss_x 0.6787 (1.2613) acc_x 78.1250 (68.7500) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [10/25] time 0.410 (0.444) data 0.280 (0.313) loss_x loss_x 1.0684 (1.2617) acc_x 71.8750 (70.6250) lr 6.9098e-04 eta 0:00:06
epoch [122/200] batch [15/25] time 0.460 (0.449) data 0.330 (0.318) loss_x loss_x 1.1914 (1.2203) acc_x 75.0000 (70.8333) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [20/25] time 0.487 (0.464) data 0.357 (0.333) loss_x loss_x 1.4189 (1.2640) acc_x 65.6250 (70.0000) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [25/25] time 0.363 (0.463) data 0.233 (0.332) loss_x loss_x 1.6367 (1.2681) acc_x 56.2500 (69.5000) lr 6.9098e-04 eta 0:00:00
epoch [122/200] batch [5/72] time 0.379 (0.455) data 0.248 (0.324) loss_u loss_u 0.8628 (0.8907) acc_u 21.8750 (14.3750) lr 6.9098e-04 eta 0:00:30
epoch [122/200] batch [10/72] time 0.372 (0.448) data 0.239 (0.317) loss_u loss_u 0.9473 (0.8933) acc_u 6.2500 (13.4375) lr 6.9098e-04 eta 0:00:27
epoch [122/200] batch [15/72] time 0.367 (0.451) data 0.237 (0.320) loss_u loss_u 0.9229 (0.8951) acc_u 6.2500 (12.5000) lr 6.9098e-04 eta 0:00:25
epoch [122/200] batch [20/72] time 0.440 (0.448) data 0.309 (0.317) loss_u loss_u 0.8765 (0.8969) acc_u 9.3750 (12.1875) lr 6.9098e-04 eta 0:00:23
epoch [122/200] batch [25/72] time 0.400 (0.447) data 0.268 (0.316) loss_u loss_u 0.9004 (0.8971) acc_u 18.7500 (12.2500) lr 6.9098e-04 eta 0:00:20
epoch [122/200] batch [30/72] time 0.539 (0.453) data 0.407 (0.322) loss_u loss_u 0.9048 (0.8976) acc_u 18.7500 (12.5000) lr 6.9098e-04 eta 0:00:19
epoch [122/200] batch [35/72] time 0.532 (0.460) data 0.401 (0.329) loss_u loss_u 0.8892 (0.8970) acc_u 15.6250 (12.5000) lr 6.9098e-04 eta 0:00:17
epoch [122/200] batch [40/72] time 0.388 (0.457) data 0.258 (0.326) loss_u loss_u 0.8569 (0.8974) acc_u 18.7500 (12.4219) lr 6.9098e-04 eta 0:00:14
epoch [122/200] batch [45/72] time 0.496 (0.456) data 0.366 (0.325) loss_u loss_u 0.9351 (0.8996) acc_u 9.3750 (12.2222) lr 6.9098e-04 eta 0:00:12
epoch [122/200] batch [50/72] time 0.547 (0.455) data 0.416 (0.324) loss_u loss_u 0.8896 (0.9005) acc_u 15.6250 (12.0000) lr 6.9098e-04 eta 0:00:10
epoch [122/200] batch [55/72] time 0.369 (0.457) data 0.238 (0.326) loss_u loss_u 0.9209 (0.9007) acc_u 9.3750 (11.9318) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [60/72] time 0.488 (0.457) data 0.356 (0.326) loss_u loss_u 0.8516 (0.9019) acc_u 15.6250 (11.7188) lr 6.9098e-04 eta 0:00:05
epoch [122/200] batch [65/72] time 0.410 (0.460) data 0.279 (0.329) loss_u loss_u 0.9023 (0.9017) acc_u 9.3750 (11.7788) lr 6.9098e-04 eta 0:00:03
epoch [122/200] batch [70/72] time 0.415 (0.462) data 0.285 (0.331) loss_u loss_u 0.8789 (0.8994) acc_u 12.5000 (12.0089) lr 6.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2081
confident_label rate tensor(0.2615, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 820
clean true:372
clean false:448
clean_rate:0.45365853658536587
noisy true:683
noisy false:1633
after delete: len(clean_dataset) 820
after delete: len(noisy_dataset) 2316
epoch [123/200] batch [5/25] time 0.453 (0.506) data 0.322 (0.374) loss_x loss_x 1.1514 (1.0322) acc_x 71.8750 (77.5000) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [10/25] time 0.580 (0.483) data 0.449 (0.352) loss_x loss_x 1.7100 (1.2389) acc_x 65.6250 (72.1875) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [15/25] time 0.464 (0.483) data 0.333 (0.351) loss_x loss_x 0.3762 (1.2210) acc_x 93.7500 (71.6667) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [20/25] time 0.409 (0.468) data 0.278 (0.336) loss_x loss_x 1.1406 (1.1903) acc_x 75.0000 (71.4062) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [25/25] time 0.470 (0.466) data 0.340 (0.335) loss_x loss_x 1.1006 (1.1721) acc_x 71.8750 (71.2500) lr 6.7608e-04 eta 0:00:00
epoch [123/200] batch [5/72] time 0.512 (0.465) data 0.381 (0.334) loss_u loss_u 0.8931 (0.8893) acc_u 15.6250 (16.8750) lr 6.7608e-04 eta 0:00:31
epoch [123/200] batch [10/72] time 0.449 (0.471) data 0.318 (0.340) loss_u loss_u 0.8750 (0.9001) acc_u 18.7500 (15.0000) lr 6.7608e-04 eta 0:00:29
epoch [123/200] batch [15/72] time 0.664 (0.474) data 0.531 (0.343) loss_u loss_u 0.9282 (0.9070) acc_u 9.3750 (13.5417) lr 6.7608e-04 eta 0:00:27
epoch [123/200] batch [20/72] time 0.492 (0.473) data 0.361 (0.342) loss_u loss_u 0.8877 (0.9012) acc_u 12.5000 (13.5938) lr 6.7608e-04 eta 0:00:24
epoch [123/200] batch [25/72] time 0.452 (0.473) data 0.321 (0.342) loss_u loss_u 0.9536 (0.9079) acc_u 6.2500 (12.6250) lr 6.7608e-04 eta 0:00:22
epoch [123/200] batch [30/72] time 0.418 (0.472) data 0.287 (0.341) loss_u loss_u 0.8999 (0.9070) acc_u 12.5000 (12.3958) lr 6.7608e-04 eta 0:00:19
epoch [123/200] batch [35/72] time 0.445 (0.475) data 0.314 (0.344) loss_u loss_u 0.9023 (0.9056) acc_u 12.5000 (12.1429) lr 6.7608e-04 eta 0:00:17
epoch [123/200] batch [40/72] time 0.753 (0.477) data 0.621 (0.346) loss_u loss_u 0.8574 (0.9013) acc_u 15.6250 (12.5781) lr 6.7608e-04 eta 0:00:15
epoch [123/200] batch [45/72] time 0.364 (0.471) data 0.233 (0.340) loss_u loss_u 0.9346 (0.9034) acc_u 9.3750 (12.4306) lr 6.7608e-04 eta 0:00:12
epoch [123/200] batch [50/72] time 0.504 (0.468) data 0.373 (0.337) loss_u loss_u 0.9004 (0.9015) acc_u 9.3750 (12.6250) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [55/72] time 0.603 (0.469) data 0.473 (0.338) loss_u loss_u 0.9023 (0.8987) acc_u 15.6250 (13.0682) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [60/72] time 0.416 (0.467) data 0.285 (0.336) loss_u loss_u 0.8677 (0.8982) acc_u 18.7500 (13.2812) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [65/72] time 0.361 (0.467) data 0.228 (0.336) loss_u loss_u 0.9053 (0.8966) acc_u 15.6250 (13.4615) lr 6.7608e-04 eta 0:00:03
epoch [123/200] batch [70/72] time 0.512 (0.466) data 0.381 (0.335) loss_u loss_u 0.9238 (0.8993) acc_u 6.2500 (12.8571) lr 6.7608e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2061
confident_label rate tensor(0.2557, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 802
clean true:367
clean false:435
clean_rate:0.45760598503740646
noisy true:708
noisy false:1626
after delete: len(clean_dataset) 802
after delete: len(noisy_dataset) 2334
epoch [124/200] batch [5/25] time 0.613 (0.493) data 0.480 (0.360) loss_x loss_x 1.3008 (1.2838) acc_x 65.6250 (66.8750) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [10/25] time 0.585 (0.490) data 0.454 (0.358) loss_x loss_x 1.1855 (1.3003) acc_x 68.7500 (65.9375) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [15/25] time 0.527 (0.485) data 0.397 (0.353) loss_x loss_x 1.1494 (1.2863) acc_x 62.5000 (66.2500) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [20/25] time 0.399 (0.480) data 0.267 (0.348) loss_x loss_x 1.0400 (1.2680) acc_x 75.0000 (67.1875) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [25/25] time 0.468 (0.478) data 0.337 (0.347) loss_x loss_x 0.8003 (1.2425) acc_x 75.0000 (68.0000) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/72] time 0.409 (0.484) data 0.278 (0.352) loss_u loss_u 0.8481 (0.8873) acc_u 25.0000 (16.8750) lr 6.6126e-04 eta 0:00:32
epoch [124/200] batch [10/72] time 0.402 (0.477) data 0.271 (0.346) loss_u loss_u 0.9204 (0.8876) acc_u 12.5000 (15.3125) lr 6.6126e-04 eta 0:00:29
epoch [124/200] batch [15/72] time 0.466 (0.473) data 0.335 (0.342) loss_u loss_u 0.8940 (0.8910) acc_u 12.5000 (14.7917) lr 6.6126e-04 eta 0:00:26
epoch [124/200] batch [20/72] time 0.475 (0.474) data 0.345 (0.343) loss_u loss_u 0.9175 (0.9017) acc_u 12.5000 (12.8125) lr 6.6126e-04 eta 0:00:24
epoch [124/200] batch [25/72] time 0.660 (0.472) data 0.530 (0.341) loss_u loss_u 0.8833 (0.8965) acc_u 9.3750 (13.3750) lr 6.6126e-04 eta 0:00:22
epoch [124/200] batch [30/72] time 0.535 (0.473) data 0.403 (0.342) loss_u loss_u 0.8384 (0.8967) acc_u 25.0000 (13.5417) lr 6.6126e-04 eta 0:00:19
epoch [124/200] batch [35/72] time 0.402 (0.474) data 0.270 (0.342) loss_u loss_u 0.9097 (0.8977) acc_u 12.5000 (13.1250) lr 6.6126e-04 eta 0:00:17
epoch [124/200] batch [40/72] time 0.370 (0.470) data 0.239 (0.339) loss_u loss_u 0.9087 (0.8990) acc_u 9.3750 (12.7344) lr 6.6126e-04 eta 0:00:15
epoch [124/200] batch [45/72] time 0.592 (0.477) data 0.461 (0.346) loss_u loss_u 0.9771 (0.9024) acc_u 3.1250 (12.5694) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [50/72] time 0.419 (0.476) data 0.287 (0.345) loss_u loss_u 0.8535 (0.8973) acc_u 15.6250 (13.0625) lr 6.6126e-04 eta 0:00:10
epoch [124/200] batch [55/72] time 0.457 (0.475) data 0.326 (0.344) loss_u loss_u 0.9067 (0.8979) acc_u 12.5000 (13.0682) lr 6.6126e-04 eta 0:00:08
epoch [124/200] batch [60/72] time 0.472 (0.479) data 0.341 (0.348) loss_u loss_u 0.8984 (0.8960) acc_u 12.5000 (13.4896) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [65/72] time 0.551 (0.480) data 0.420 (0.349) loss_u loss_u 0.9478 (0.8966) acc_u 9.3750 (13.5096) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [70/72] time 0.552 (0.480) data 0.421 (0.349) loss_u loss_u 0.8950 (0.8955) acc_u 12.5000 (13.6161) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2100
confident_label rate tensor(0.2580, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 809
clean true:377
clean false:432
clean_rate:0.46600741656365885
noisy true:659
noisy false:1668
after delete: len(clean_dataset) 809
after delete: len(noisy_dataset) 2327
epoch [125/200] batch [5/25] time 0.543 (0.486) data 0.413 (0.355) loss_x loss_x 1.3613 (1.3283) acc_x 71.8750 (70.6250) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [10/25] time 0.473 (0.496) data 0.342 (0.364) loss_x loss_x 1.7090 (1.3273) acc_x 62.5000 (68.7500) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [15/25] time 0.370 (0.482) data 0.240 (0.351) loss_x loss_x 1.7334 (1.3600) acc_x 59.3750 (66.2500) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [20/25] time 0.432 (0.485) data 0.301 (0.354) loss_x loss_x 1.4238 (1.3591) acc_x 56.2500 (64.8438) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [25/25] time 0.545 (0.492) data 0.414 (0.361) loss_x loss_x 1.5059 (1.3338) acc_x 68.7500 (65.6250) lr 6.4653e-04 eta 0:00:00
epoch [125/200] batch [5/72] time 0.544 (0.488) data 0.413 (0.357) loss_u loss_u 0.9614 (0.9061) acc_u 6.2500 (11.2500) lr 6.4653e-04 eta 0:00:32
epoch [125/200] batch [10/72] time 0.322 (0.482) data 0.191 (0.351) loss_u loss_u 0.9297 (0.8980) acc_u 12.5000 (12.8125) lr 6.4653e-04 eta 0:00:29
epoch [125/200] batch [15/72] time 0.420 (0.475) data 0.288 (0.344) loss_u loss_u 0.9346 (0.8937) acc_u 9.3750 (13.7500) lr 6.4653e-04 eta 0:00:27
epoch [125/200] batch [20/72] time 0.582 (0.470) data 0.451 (0.339) loss_u loss_u 0.8906 (0.8925) acc_u 18.7500 (14.0625) lr 6.4653e-04 eta 0:00:24
epoch [125/200] batch [25/72] time 0.435 (0.467) data 0.304 (0.336) loss_u loss_u 0.9316 (0.8940) acc_u 6.2500 (13.6250) lr 6.4653e-04 eta 0:00:21
epoch [125/200] batch [30/72] time 0.416 (0.473) data 0.285 (0.341) loss_u loss_u 0.8535 (0.8947) acc_u 18.7500 (13.4375) lr 6.4653e-04 eta 0:00:19
epoch [125/200] batch [35/72] time 0.402 (0.469) data 0.270 (0.337) loss_u loss_u 0.9526 (0.8980) acc_u 3.1250 (12.9464) lr 6.4653e-04 eta 0:00:17
epoch [125/200] batch [40/72] time 0.427 (0.467) data 0.297 (0.336) loss_u loss_u 0.8638 (0.8960) acc_u 21.8750 (13.1250) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [45/72] time 0.415 (0.464) data 0.284 (0.333) loss_u loss_u 0.9092 (0.8969) acc_u 12.5000 (12.9167) lr 6.4653e-04 eta 0:00:12
epoch [125/200] batch [50/72] time 0.337 (0.458) data 0.206 (0.327) loss_u loss_u 0.8364 (0.8945) acc_u 21.8750 (13.2500) lr 6.4653e-04 eta 0:00:10
epoch [125/200] batch [55/72] time 0.368 (0.458) data 0.236 (0.327) loss_u loss_u 0.8677 (0.8926) acc_u 18.7500 (13.4659) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [60/72] time 0.409 (0.455) data 0.278 (0.324) loss_u loss_u 0.8447 (0.8915) acc_u 28.1250 (13.7500) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [65/72] time 0.460 (0.454) data 0.329 (0.323) loss_u loss_u 0.9375 (0.8928) acc_u 6.2500 (13.4615) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [70/72] time 0.418 (0.455) data 0.287 (0.324) loss_u loss_u 0.9395 (0.8938) acc_u 6.2500 (13.2589) lr 6.4653e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2063
confident_label rate tensor(0.2583, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 810
clean true:377
clean false:433
clean_rate:0.4654320987654321
noisy true:696
noisy false:1630
after delete: len(clean_dataset) 810
after delete: len(noisy_dataset) 2326
epoch [126/200] batch [5/25] time 0.354 (0.432) data 0.222 (0.300) loss_x loss_x 0.9395 (1.0918) acc_x 65.6250 (68.7500) lr 6.3188e-04 eta 0:00:08
epoch [126/200] batch [10/25] time 0.476 (0.438) data 0.345 (0.307) loss_x loss_x 1.3232 (1.1438) acc_x 62.5000 (70.3125) lr 6.3188e-04 eta 0:00:06
epoch [126/200] batch [15/25] time 0.492 (0.458) data 0.362 (0.327) loss_x loss_x 0.7788 (1.1291) acc_x 81.2500 (71.8750) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [20/25] time 0.451 (0.448) data 0.319 (0.316) loss_x loss_x 1.3350 (1.1290) acc_x 62.5000 (71.7188) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [25/25] time 0.456 (0.457) data 0.326 (0.326) loss_x loss_x 0.9487 (1.0803) acc_x 71.8750 (72.6250) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/72] time 0.435 (0.451) data 0.303 (0.320) loss_u loss_u 0.9258 (0.9102) acc_u 9.3750 (10.6250) lr 6.3188e-04 eta 0:00:30
epoch [126/200] batch [10/72] time 0.482 (0.452) data 0.351 (0.321) loss_u loss_u 0.9448 (0.8999) acc_u 3.1250 (12.1875) lr 6.3188e-04 eta 0:00:28
epoch [126/200] batch [15/72] time 0.411 (0.448) data 0.279 (0.317) loss_u loss_u 0.9326 (0.8931) acc_u 12.5000 (13.3333) lr 6.3188e-04 eta 0:00:25
epoch [126/200] batch [20/72] time 0.462 (0.444) data 0.331 (0.313) loss_u loss_u 0.9009 (0.8893) acc_u 12.5000 (14.0625) lr 6.3188e-04 eta 0:00:23
epoch [126/200] batch [25/72] time 0.578 (0.450) data 0.447 (0.319) loss_u loss_u 0.9351 (0.8903) acc_u 15.6250 (14.0000) lr 6.3188e-04 eta 0:00:21
epoch [126/200] batch [30/72] time 0.485 (0.450) data 0.353 (0.318) loss_u loss_u 0.8472 (0.8877) acc_u 18.7500 (14.2708) lr 6.3188e-04 eta 0:00:18
epoch [126/200] batch [35/72] time 0.529 (0.453) data 0.399 (0.322) loss_u loss_u 0.8940 (0.8885) acc_u 12.5000 (14.1964) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [40/72] time 0.376 (0.452) data 0.245 (0.321) loss_u loss_u 0.8633 (0.8879) acc_u 15.6250 (14.2969) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [45/72] time 0.413 (0.450) data 0.282 (0.319) loss_u loss_u 0.8921 (0.8884) acc_u 15.6250 (14.1667) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [50/72] time 0.413 (0.453) data 0.282 (0.322) loss_u loss_u 0.9233 (0.8876) acc_u 6.2500 (14.3125) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [55/72] time 0.403 (0.455) data 0.271 (0.324) loss_u loss_u 0.9233 (0.8896) acc_u 6.2500 (13.9205) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [60/72] time 0.404 (0.453) data 0.272 (0.322) loss_u loss_u 0.9341 (0.8916) acc_u 15.6250 (13.9062) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [65/72] time 0.443 (0.451) data 0.311 (0.320) loss_u loss_u 0.9233 (0.8920) acc_u 9.3750 (13.8942) lr 6.3188e-04 eta 0:00:03
epoch [126/200] batch [70/72] time 0.621 (0.453) data 0.489 (0.322) loss_u loss_u 0.8667 (0.8932) acc_u 21.8750 (13.7946) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2107
confident_label rate tensor(0.2580, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 809
clean true:365
clean false:444
clean_rate:0.4511742892459827
noisy true:664
noisy false:1663
after delete: len(clean_dataset) 809
after delete: len(noisy_dataset) 2327
epoch [127/200] batch [5/25] time 0.531 (0.525) data 0.399 (0.392) loss_x loss_x 1.7998 (1.3707) acc_x 59.3750 (66.2500) lr 6.1732e-04 eta 0:00:10
epoch [127/200] batch [10/25] time 0.426 (0.481) data 0.296 (0.350) loss_x loss_x 1.4580 (1.2739) acc_x 68.7500 (68.4375) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [15/25] time 0.414 (0.462) data 0.283 (0.331) loss_x loss_x 1.6523 (1.2449) acc_x 62.5000 (68.3333) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [20/25] time 0.441 (0.455) data 0.311 (0.324) loss_x loss_x 1.2529 (1.2055) acc_x 71.8750 (69.2188) lr 6.1732e-04 eta 0:00:02
epoch [127/200] batch [25/25] time 0.488 (0.458) data 0.357 (0.327) loss_x loss_x 1.0596 (1.2165) acc_x 78.1250 (69.5000) lr 6.1732e-04 eta 0:00:00
epoch [127/200] batch [5/72] time 0.378 (0.458) data 0.247 (0.327) loss_u loss_u 0.8813 (0.9042) acc_u 21.8750 (12.5000) lr 6.1732e-04 eta 0:00:30
epoch [127/200] batch [10/72] time 0.363 (0.456) data 0.231 (0.325) loss_u loss_u 0.8672 (0.8782) acc_u 15.6250 (15.9375) lr 6.1732e-04 eta 0:00:28
epoch [127/200] batch [15/72] time 0.442 (0.456) data 0.310 (0.325) loss_u loss_u 0.8506 (0.8745) acc_u 18.7500 (15.8333) lr 6.1732e-04 eta 0:00:25
epoch [127/200] batch [20/72] time 0.434 (0.449) data 0.303 (0.318) loss_u loss_u 0.9326 (0.8844) acc_u 6.2500 (14.5312) lr 6.1732e-04 eta 0:00:23
epoch [127/200] batch [25/72] time 0.422 (0.446) data 0.292 (0.315) loss_u loss_u 0.9077 (0.8878) acc_u 15.6250 (14.0000) lr 6.1732e-04 eta 0:00:20
epoch [127/200] batch [30/72] time 0.749 (0.451) data 0.617 (0.320) loss_u loss_u 0.9375 (0.8933) acc_u 6.2500 (13.3333) lr 6.1732e-04 eta 0:00:18
epoch [127/200] batch [35/72] time 0.547 (0.452) data 0.416 (0.321) loss_u loss_u 0.9346 (0.8953) acc_u 12.5000 (13.3036) lr 6.1732e-04 eta 0:00:16
epoch [127/200] batch [40/72] time 0.526 (0.457) data 0.394 (0.326) loss_u loss_u 0.8252 (0.8903) acc_u 21.8750 (13.8281) lr 6.1732e-04 eta 0:00:14
epoch [127/200] batch [45/72] time 0.502 (0.456) data 0.371 (0.325) loss_u loss_u 0.9116 (0.8921) acc_u 9.3750 (13.4722) lr 6.1732e-04 eta 0:00:12
epoch [127/200] batch [50/72] time 0.446 (0.456) data 0.314 (0.325) loss_u loss_u 0.9194 (0.8938) acc_u 9.3750 (13.3125) lr 6.1732e-04 eta 0:00:10
epoch [127/200] batch [55/72] time 0.441 (0.459) data 0.310 (0.328) loss_u loss_u 0.8550 (0.8914) acc_u 15.6250 (13.5795) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [60/72] time 0.531 (0.461) data 0.400 (0.330) loss_u loss_u 0.8613 (0.8929) acc_u 21.8750 (13.5938) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [65/72] time 0.376 (0.462) data 0.244 (0.331) loss_u loss_u 0.9209 (0.8936) acc_u 6.2500 (13.3654) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [70/72] time 0.399 (0.462) data 0.269 (0.331) loss_u loss_u 0.9180 (0.8913) acc_u 9.3750 (13.6161) lr 6.1732e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2044
confident_label rate tensor(0.2624, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 823
clean true:380
clean false:443
clean_rate:0.4617253948967193
noisy true:712
noisy false:1601
after delete: len(clean_dataset) 823
after delete: len(noisy_dataset) 2313
epoch [128/200] batch [5/25] time 0.400 (0.454) data 0.268 (0.322) loss_x loss_x 1.5166 (1.3107) acc_x 56.2500 (65.6250) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [10/25] time 0.603 (0.473) data 0.472 (0.342) loss_x loss_x 1.3828 (1.2252) acc_x 71.8750 (70.6250) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [15/25] time 0.471 (0.484) data 0.340 (0.353) loss_x loss_x 1.5986 (1.2845) acc_x 62.5000 (69.7917) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [20/25] time 0.493 (0.484) data 0.362 (0.353) loss_x loss_x 1.7012 (1.2872) acc_x 56.2500 (68.4375) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [25/25] time 0.492 (0.480) data 0.362 (0.349) loss_x loss_x 1.0752 (1.2730) acc_x 68.7500 (68.5000) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/72] time 0.435 (0.478) data 0.305 (0.347) loss_u loss_u 0.8613 (0.8683) acc_u 15.6250 (15.0000) lr 6.0285e-04 eta 0:00:31
epoch [128/200] batch [10/72] time 0.403 (0.474) data 0.273 (0.343) loss_u loss_u 0.9307 (0.8867) acc_u 6.2500 (13.7500) lr 6.0285e-04 eta 0:00:29
epoch [128/200] batch [15/72] time 0.456 (0.469) data 0.325 (0.338) loss_u loss_u 0.9614 (0.9006) acc_u 3.1250 (12.0833) lr 6.0285e-04 eta 0:00:26
epoch [128/200] batch [20/72] time 0.491 (0.475) data 0.360 (0.344) loss_u loss_u 0.9424 (0.8938) acc_u 3.1250 (12.8125) lr 6.0285e-04 eta 0:00:24
epoch [128/200] batch [25/72] time 0.397 (0.475) data 0.265 (0.343) loss_u loss_u 0.8511 (0.8961) acc_u 12.5000 (12.5000) lr 6.0285e-04 eta 0:00:22
epoch [128/200] batch [30/72] time 0.491 (0.471) data 0.359 (0.340) loss_u loss_u 0.8267 (0.8968) acc_u 18.7500 (12.3958) lr 6.0285e-04 eta 0:00:19
epoch [128/200] batch [35/72] time 0.424 (0.474) data 0.294 (0.343) loss_u loss_u 0.8389 (0.8966) acc_u 18.7500 (12.4107) lr 6.0285e-04 eta 0:00:17
epoch [128/200] batch [40/72] time 0.474 (0.473) data 0.344 (0.342) loss_u loss_u 0.8765 (0.9009) acc_u 18.7500 (11.9531) lr 6.0285e-04 eta 0:00:15
epoch [128/200] batch [45/72] time 0.419 (0.471) data 0.287 (0.340) loss_u loss_u 0.9302 (0.9016) acc_u 6.2500 (11.6667) lr 6.0285e-04 eta 0:00:12
epoch [128/200] batch [50/72] time 0.511 (0.468) data 0.380 (0.337) loss_u loss_u 0.9409 (0.9003) acc_u 9.3750 (12.0625) lr 6.0285e-04 eta 0:00:10
epoch [128/200] batch [55/72] time 0.490 (0.467) data 0.359 (0.336) loss_u loss_u 0.8501 (0.8973) acc_u 18.7500 (12.5000) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [60/72] time 0.431 (0.466) data 0.301 (0.334) loss_u loss_u 0.8823 (0.8970) acc_u 15.6250 (12.5521) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [65/72] time 0.342 (0.464) data 0.211 (0.333) loss_u loss_u 0.9199 (0.9005) acc_u 12.5000 (12.1635) lr 6.0285e-04 eta 0:00:03
epoch [128/200] batch [70/72] time 0.408 (0.462) data 0.276 (0.331) loss_u loss_u 0.8970 (0.9005) acc_u 9.3750 (12.1875) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2089
confident_label rate tensor(0.2659, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 834
clean true:376
clean false:458
clean_rate:0.45083932853717024
noisy true:671
noisy false:1631
after delete: len(clean_dataset) 834
after delete: len(noisy_dataset) 2302
epoch [129/200] batch [5/26] time 0.434 (0.437) data 0.303 (0.306) loss_x loss_x 1.4619 (1.1817) acc_x 65.6250 (73.1250) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [10/26] time 0.586 (0.445) data 0.455 (0.313) loss_x loss_x 1.1211 (1.1940) acc_x 68.7500 (70.0000) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [15/26] time 0.372 (0.451) data 0.242 (0.320) loss_x loss_x 1.1211 (1.2147) acc_x 65.6250 (67.9167) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [20/26] time 0.558 (0.460) data 0.429 (0.329) loss_x loss_x 1.1006 (1.2230) acc_x 62.5000 (67.5000) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [25/26] time 0.442 (0.461) data 0.311 (0.330) loss_x loss_x 1.6523 (1.2671) acc_x 65.6250 (66.6250) lr 5.8849e-04 eta 0:00:00
epoch [129/200] batch [5/71] time 0.374 (0.458) data 0.243 (0.326) loss_u loss_u 0.9463 (0.9298) acc_u 6.2500 (11.2500) lr 5.8849e-04 eta 0:00:30
epoch [129/200] batch [10/71] time 0.635 (0.465) data 0.503 (0.333) loss_u loss_u 0.9038 (0.9126) acc_u 15.6250 (12.8125) lr 5.8849e-04 eta 0:00:28
epoch [129/200] batch [15/71] time 0.383 (0.460) data 0.251 (0.329) loss_u loss_u 0.8369 (0.9016) acc_u 18.7500 (13.3333) lr 5.8849e-04 eta 0:00:25
epoch [129/200] batch [20/71] time 0.354 (0.457) data 0.223 (0.325) loss_u loss_u 0.9404 (0.9045) acc_u 6.2500 (12.3438) lr 5.8849e-04 eta 0:00:23
epoch [129/200] batch [25/71] time 0.599 (0.454) data 0.467 (0.323) loss_u loss_u 0.9512 (0.9007) acc_u 9.3750 (13.2500) lr 5.8849e-04 eta 0:00:20
epoch [129/200] batch [30/71] time 0.563 (0.457) data 0.432 (0.325) loss_u loss_u 0.9131 (0.9022) acc_u 9.3750 (12.9167) lr 5.8849e-04 eta 0:00:18
epoch [129/200] batch [35/71] time 0.477 (0.458) data 0.346 (0.327) loss_u loss_u 0.8569 (0.8986) acc_u 15.6250 (13.2143) lr 5.8849e-04 eta 0:00:16
epoch [129/200] batch [40/71] time 0.627 (0.461) data 0.495 (0.329) loss_u loss_u 0.8374 (0.8966) acc_u 28.1250 (13.6719) lr 5.8849e-04 eta 0:00:14
epoch [129/200] batch [45/71] time 0.473 (0.460) data 0.341 (0.328) loss_u loss_u 0.8950 (0.8966) acc_u 12.5000 (13.7500) lr 5.8849e-04 eta 0:00:11
epoch [129/200] batch [50/71] time 0.542 (0.465) data 0.409 (0.333) loss_u loss_u 0.9248 (0.8961) acc_u 12.5000 (13.8125) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [55/71] time 0.440 (0.471) data 0.309 (0.340) loss_u loss_u 0.8896 (0.8968) acc_u 15.6250 (13.8068) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [60/71] time 0.396 (0.467) data 0.265 (0.336) loss_u loss_u 0.8838 (0.8949) acc_u 12.5000 (14.2188) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [65/71] time 0.570 (0.470) data 0.439 (0.339) loss_u loss_u 0.9399 (0.8975) acc_u 9.3750 (13.7981) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [70/71] time 0.448 (0.470) data 0.317 (0.339) loss_u loss_u 0.9146 (0.8982) acc_u 9.3750 (13.7500) lr 5.8849e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2091
confident_label rate tensor(0.2449, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 768
clean true:339
clean false:429
clean_rate:0.44140625
noisy true:706
noisy false:1662
after delete: len(clean_dataset) 768
after delete: len(noisy_dataset) 2368
epoch [130/200] batch [5/24] time 0.403 (0.452) data 0.273 (0.320) loss_x loss_x 1.0605 (1.1729) acc_x 87.5000 (74.3750) lr 5.7422e-04 eta 0:00:08
epoch [130/200] batch [10/24] time 0.460 (0.468) data 0.330 (0.337) loss_x loss_x 1.1494 (1.1906) acc_x 65.6250 (72.8125) lr 5.7422e-04 eta 0:00:06
epoch [130/200] batch [15/24] time 0.498 (0.472) data 0.368 (0.341) loss_x loss_x 1.7383 (1.1813) acc_x 62.5000 (73.9583) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [20/24] time 0.489 (0.481) data 0.358 (0.350) loss_x loss_x 1.3301 (1.1473) acc_x 75.0000 (74.8438) lr 5.7422e-04 eta 0:00:01
epoch [130/200] batch [5/74] time 0.361 (0.480) data 0.230 (0.348) loss_u loss_u 0.9282 (0.9113) acc_u 6.2500 (9.3750) lr 5.7422e-04 eta 0:00:33
epoch [130/200] batch [10/74] time 0.543 (0.475) data 0.411 (0.344) loss_u loss_u 0.9155 (0.9066) acc_u 15.6250 (11.8750) lr 5.7422e-04 eta 0:00:30
epoch [130/200] batch [15/74] time 0.401 (0.478) data 0.269 (0.347) loss_u loss_u 0.8257 (0.9025) acc_u 21.8750 (12.9167) lr 5.7422e-04 eta 0:00:28
epoch [130/200] batch [20/74] time 0.488 (0.475) data 0.357 (0.344) loss_u loss_u 0.9189 (0.9027) acc_u 9.3750 (12.9688) lr 5.7422e-04 eta 0:00:25
epoch [130/200] batch [25/74] time 0.466 (0.476) data 0.334 (0.345) loss_u loss_u 0.8789 (0.9004) acc_u 15.6250 (13.3750) lr 5.7422e-04 eta 0:00:23
epoch [130/200] batch [30/74] time 0.505 (0.477) data 0.373 (0.346) loss_u loss_u 0.8042 (0.8995) acc_u 28.1250 (12.9167) lr 5.7422e-04 eta 0:00:21
epoch [130/200] batch [35/74] time 0.492 (0.478) data 0.361 (0.346) loss_u loss_u 0.8960 (0.8965) acc_u 9.3750 (13.3036) lr 5.7422e-04 eta 0:00:18
epoch [130/200] batch [40/74] time 0.469 (0.477) data 0.338 (0.345) loss_u loss_u 0.8008 (0.8906) acc_u 28.1250 (13.9062) lr 5.7422e-04 eta 0:00:16
epoch [130/200] batch [45/74] time 0.450 (0.481) data 0.319 (0.349) loss_u loss_u 0.9009 (0.8914) acc_u 15.6250 (14.0278) lr 5.7422e-04 eta 0:00:13
epoch [130/200] batch [50/74] time 0.407 (0.479) data 0.276 (0.348) loss_u loss_u 0.8564 (0.8902) acc_u 18.7500 (14.2500) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [55/74] time 0.547 (0.478) data 0.416 (0.347) loss_u loss_u 0.8979 (0.8888) acc_u 12.5000 (14.2045) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [60/74] time 0.457 (0.477) data 0.325 (0.345) loss_u loss_u 0.8486 (0.8890) acc_u 21.8750 (14.2188) lr 5.7422e-04 eta 0:00:06
epoch [130/200] batch [65/74] time 0.501 (0.477) data 0.370 (0.345) loss_u loss_u 0.9019 (0.8901) acc_u 12.5000 (14.1346) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [70/74] time 0.617 (0.479) data 0.486 (0.348) loss_u loss_u 0.9150 (0.8901) acc_u 12.5000 (14.1518) lr 5.7422e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2099
confident_label rate tensor(0.2634, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 826
clean true:371
clean false:455
clean_rate:0.4491525423728814
noisy true:666
noisy false:1644
after delete: len(clean_dataset) 826
after delete: len(noisy_dataset) 2310
epoch [131/200] batch [5/25] time 0.323 (0.444) data 0.192 (0.313) loss_x loss_x 1.1299 (1.3113) acc_x 62.5000 (66.2500) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [10/25] time 0.413 (0.460) data 0.282 (0.329) loss_x loss_x 0.9595 (1.2041) acc_x 65.6250 (66.8750) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [15/25] time 0.452 (0.457) data 0.321 (0.325) loss_x loss_x 1.3398 (1.2073) acc_x 65.6250 (67.2917) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [20/25] time 0.460 (0.453) data 0.329 (0.322) loss_x loss_x 1.0605 (1.2113) acc_x 71.8750 (66.7188) lr 5.6006e-04 eta 0:00:02
epoch [131/200] batch [25/25] time 0.412 (0.458) data 0.281 (0.328) loss_x loss_x 1.2441 (1.2347) acc_x 59.3750 (66.5000) lr 5.6006e-04 eta 0:00:00
epoch [131/200] batch [5/72] time 0.511 (0.460) data 0.380 (0.329) loss_u loss_u 0.8857 (0.8797) acc_u 15.6250 (16.8750) lr 5.6006e-04 eta 0:00:30
epoch [131/200] batch [10/72] time 0.387 (0.459) data 0.256 (0.328) loss_u loss_u 0.9248 (0.8894) acc_u 9.3750 (14.3750) lr 5.6006e-04 eta 0:00:28
epoch [131/200] batch [15/72] time 0.452 (0.452) data 0.321 (0.321) loss_u loss_u 0.9282 (0.8938) acc_u 6.2500 (13.7500) lr 5.6006e-04 eta 0:00:25
epoch [131/200] batch [20/72] time 0.712 (0.455) data 0.581 (0.324) loss_u loss_u 0.9307 (0.8936) acc_u 12.5000 (14.2188) lr 5.6006e-04 eta 0:00:23
epoch [131/200] batch [25/72] time 0.495 (0.459) data 0.363 (0.328) loss_u loss_u 0.9272 (0.8942) acc_u 12.5000 (14.3750) lr 5.6006e-04 eta 0:00:21
epoch [131/200] batch [30/72] time 0.376 (0.455) data 0.244 (0.324) loss_u loss_u 0.8130 (0.8907) acc_u 21.8750 (14.4792) lr 5.6006e-04 eta 0:00:19
epoch [131/200] batch [35/72] time 0.387 (0.458) data 0.256 (0.327) loss_u loss_u 0.9263 (0.8915) acc_u 6.2500 (14.2857) lr 5.6006e-04 eta 0:00:16
epoch [131/200] batch [40/72] time 0.343 (0.458) data 0.211 (0.327) loss_u loss_u 0.9355 (0.8923) acc_u 6.2500 (13.9844) lr 5.6006e-04 eta 0:00:14
epoch [131/200] batch [45/72] time 0.500 (0.460) data 0.369 (0.329) loss_u loss_u 0.9438 (0.8935) acc_u 3.1250 (13.6111) lr 5.6006e-04 eta 0:00:12
epoch [131/200] batch [50/72] time 0.416 (0.464) data 0.285 (0.333) loss_u loss_u 0.7983 (0.8919) acc_u 21.8750 (13.7500) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [55/72] time 0.498 (0.470) data 0.366 (0.339) loss_u loss_u 0.8735 (0.8917) acc_u 18.7500 (13.8068) lr 5.6006e-04 eta 0:00:07
epoch [131/200] batch [60/72] time 0.466 (0.474) data 0.335 (0.342) loss_u loss_u 0.9072 (0.8913) acc_u 9.3750 (14.1146) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [65/72] time 0.421 (0.471) data 0.289 (0.340) loss_u loss_u 0.9072 (0.8913) acc_u 9.3750 (13.9904) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [70/72] time 0.440 (0.472) data 0.307 (0.341) loss_u loss_u 0.9297 (0.8918) acc_u 9.3750 (13.9286) lr 5.6006e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2080
confident_label rate tensor(0.2577, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 808
clean true:366
clean false:442
clean_rate:0.452970297029703
noisy true:690
noisy false:1638
after delete: len(clean_dataset) 808
after delete: len(noisy_dataset) 2328
epoch [132/200] batch [5/25] time 0.678 (0.519) data 0.547 (0.388) loss_x loss_x 1.2217 (1.3244) acc_x 50.0000 (66.8750) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [10/25] time 0.404 (0.482) data 0.273 (0.351) loss_x loss_x 1.3262 (1.2950) acc_x 68.7500 (66.5625) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [15/25] time 0.534 (0.480) data 0.404 (0.350) loss_x loss_x 1.2402 (1.2746) acc_x 62.5000 (67.0833) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [20/25] time 0.498 (0.494) data 0.368 (0.363) loss_x loss_x 1.4062 (1.3118) acc_x 68.7500 (67.3438) lr 5.4601e-04 eta 0:00:02
epoch [132/200] batch [25/25] time 0.426 (0.480) data 0.295 (0.349) loss_x loss_x 0.9741 (1.3381) acc_x 71.8750 (66.8750) lr 5.4601e-04 eta 0:00:00
epoch [132/200] batch [5/72] time 0.417 (0.471) data 0.285 (0.340) loss_u loss_u 0.8809 (0.9009) acc_u 18.7500 (13.1250) lr 5.4601e-04 eta 0:00:31
epoch [132/200] batch [10/72] time 0.552 (0.473) data 0.421 (0.342) loss_u loss_u 0.8726 (0.9025) acc_u 18.7500 (11.8750) lr 5.4601e-04 eta 0:00:29
epoch [132/200] batch [15/72] time 0.446 (0.480) data 0.314 (0.349) loss_u loss_u 0.9019 (0.8966) acc_u 15.6250 (12.7083) lr 5.4601e-04 eta 0:00:27
epoch [132/200] batch [20/72] time 0.554 (0.475) data 0.423 (0.344) loss_u loss_u 0.8965 (0.8973) acc_u 15.6250 (12.6562) lr 5.4601e-04 eta 0:00:24
epoch [132/200] batch [25/72] time 0.396 (0.477) data 0.265 (0.346) loss_u loss_u 0.7837 (0.8867) acc_u 31.2500 (14.3750) lr 5.4601e-04 eta 0:00:22
epoch [132/200] batch [30/72] time 0.333 (0.468) data 0.202 (0.337) loss_u loss_u 0.9004 (0.8906) acc_u 9.3750 (13.5417) lr 5.4601e-04 eta 0:00:19
epoch [132/200] batch [35/72] time 0.492 (0.472) data 0.360 (0.341) loss_u loss_u 0.8667 (0.8915) acc_u 21.8750 (13.5714) lr 5.4601e-04 eta 0:00:17
epoch [132/200] batch [40/72] time 0.529 (0.470) data 0.399 (0.339) loss_u loss_u 0.8579 (0.8926) acc_u 15.6250 (13.1250) lr 5.4601e-04 eta 0:00:15
epoch [132/200] batch [45/72] time 0.419 (0.468) data 0.287 (0.337) loss_u loss_u 0.8340 (0.8918) acc_u 28.1250 (13.5417) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [50/72] time 0.487 (0.471) data 0.356 (0.340) loss_u loss_u 0.8960 (0.8940) acc_u 15.6250 (13.1875) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [55/72] time 0.361 (0.467) data 0.230 (0.336) loss_u loss_u 0.8882 (0.8923) acc_u 15.6250 (13.4091) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [60/72] time 0.390 (0.465) data 0.260 (0.333) loss_u loss_u 0.9395 (0.8946) acc_u 6.2500 (13.1250) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [65/72] time 0.457 (0.465) data 0.326 (0.334) loss_u loss_u 0.9102 (0.8951) acc_u 9.3750 (13.1250) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [70/72] time 0.488 (0.468) data 0.355 (0.337) loss_u loss_u 0.9482 (0.8942) acc_u 6.2500 (13.2143) lr 5.4601e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2104
confident_label rate tensor(0.2596, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 814
clean true:359
clean false:455
clean_rate:0.44103194103194104
noisy true:673
noisy false:1649
after delete: len(clean_dataset) 814
after delete: len(noisy_dataset) 2322
epoch [133/200] batch [5/25] time 0.426 (0.445) data 0.294 (0.314) loss_x loss_x 1.1953 (1.2596) acc_x 75.0000 (65.6250) lr 5.3207e-04 eta 0:00:08
epoch [133/200] batch [10/25] time 0.418 (0.473) data 0.287 (0.342) loss_x loss_x 2.0371 (1.3191) acc_x 53.1250 (65.6250) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [15/25] time 0.390 (0.462) data 0.260 (0.331) loss_x loss_x 1.2881 (1.2543) acc_x 65.6250 (68.1250) lr 5.3207e-04 eta 0:00:04
epoch [133/200] batch [20/25] time 0.408 (0.463) data 0.277 (0.332) loss_x loss_x 1.1064 (1.2546) acc_x 71.8750 (67.8125) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [25/25] time 0.394 (0.454) data 0.263 (0.323) loss_x loss_x 0.9131 (1.2416) acc_x 84.3750 (68.6250) lr 5.3207e-04 eta 0:00:00
epoch [133/200] batch [5/72] time 0.479 (0.451) data 0.347 (0.320) loss_u loss_u 0.9009 (0.9013) acc_u 18.7500 (14.3750) lr 5.3207e-04 eta 0:00:30
epoch [133/200] batch [10/72] time 0.512 (0.449) data 0.381 (0.318) loss_u loss_u 0.9287 (0.9182) acc_u 15.6250 (12.5000) lr 5.3207e-04 eta 0:00:27
epoch [133/200] batch [15/72] time 0.506 (0.449) data 0.374 (0.318) loss_u loss_u 0.8735 (0.9049) acc_u 18.7500 (13.9583) lr 5.3207e-04 eta 0:00:25
epoch [133/200] batch [20/72] time 0.422 (0.450) data 0.291 (0.319) loss_u loss_u 0.9082 (0.8984) acc_u 9.3750 (14.0625) lr 5.3207e-04 eta 0:00:23
epoch [133/200] batch [25/72] time 0.765 (0.463) data 0.634 (0.332) loss_u loss_u 0.9092 (0.9001) acc_u 9.3750 (13.2500) lr 5.3207e-04 eta 0:00:21
epoch [133/200] batch [30/72] time 0.473 (0.461) data 0.342 (0.330) loss_u loss_u 0.9370 (0.9020) acc_u 9.3750 (13.0208) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [35/72] time 0.384 (0.457) data 0.253 (0.326) loss_u loss_u 0.8281 (0.8980) acc_u 21.8750 (13.4821) lr 5.3207e-04 eta 0:00:16
epoch [133/200] batch [40/72] time 0.434 (0.459) data 0.304 (0.328) loss_u loss_u 0.8784 (0.8987) acc_u 9.3750 (13.2812) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [45/72] time 0.379 (0.455) data 0.248 (0.324) loss_u loss_u 0.8574 (0.8983) acc_u 15.6250 (13.2639) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [50/72] time 0.567 (0.456) data 0.435 (0.325) loss_u loss_u 0.8931 (0.8973) acc_u 15.6250 (13.2500) lr 5.3207e-04 eta 0:00:10
epoch [133/200] batch [55/72] time 0.449 (0.455) data 0.318 (0.324) loss_u loss_u 0.8921 (0.8978) acc_u 18.7500 (13.3523) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [60/72] time 0.495 (0.455) data 0.364 (0.324) loss_u loss_u 0.8867 (0.8978) acc_u 15.6250 (13.3854) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [65/72] time 0.544 (0.458) data 0.412 (0.327) loss_u loss_u 0.8564 (0.8978) acc_u 18.7500 (13.4615) lr 5.3207e-04 eta 0:00:03
epoch [133/200] batch [70/72] time 0.530 (0.458) data 0.398 (0.327) loss_u loss_u 0.8726 (0.8972) acc_u 18.7500 (13.5268) lr 5.3207e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2096
confident_label rate tensor(0.2666, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 836
clean true:376
clean false:460
clean_rate:0.44976076555023925
noisy true:664
noisy false:1636
after delete: len(clean_dataset) 836
after delete: len(noisy_dataset) 2300
epoch [134/200] batch [5/26] time 0.550 (0.507) data 0.420 (0.375) loss_x loss_x 1.3711 (1.2672) acc_x 65.6250 (70.0000) lr 5.1825e-04 eta 0:00:10
epoch [134/200] batch [10/26] time 0.461 (0.457) data 0.329 (0.326) loss_x loss_x 1.3369 (1.2990) acc_x 75.0000 (68.7500) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [15/26] time 0.503 (0.463) data 0.373 (0.332) loss_x loss_x 1.6035 (1.2650) acc_x 65.6250 (69.1667) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [20/26] time 0.490 (0.473) data 0.359 (0.342) loss_x loss_x 1.2686 (1.2865) acc_x 75.0000 (68.2812) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [25/26] time 0.445 (0.467) data 0.315 (0.336) loss_x loss_x 1.1914 (1.2931) acc_x 68.7500 (67.7500) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/71] time 0.588 (0.480) data 0.457 (0.349) loss_u loss_u 0.8745 (0.8957) acc_u 15.6250 (15.0000) lr 5.1825e-04 eta 0:00:31
epoch [134/200] batch [10/71] time 0.457 (0.481) data 0.326 (0.350) loss_u loss_u 0.9419 (0.9029) acc_u 9.3750 (13.4375) lr 5.1825e-04 eta 0:00:29
epoch [134/200] batch [15/71] time 0.596 (0.474) data 0.466 (0.342) loss_u loss_u 0.9214 (0.8993) acc_u 15.6250 (13.9583) lr 5.1825e-04 eta 0:00:26
epoch [134/200] batch [20/71] time 0.406 (0.470) data 0.275 (0.339) loss_u loss_u 0.9126 (0.8936) acc_u 6.2500 (14.5312) lr 5.1825e-04 eta 0:00:23
epoch [134/200] batch [25/71] time 0.507 (0.474) data 0.377 (0.343) loss_u loss_u 0.8525 (0.8939) acc_u 18.7500 (14.2500) lr 5.1825e-04 eta 0:00:21
epoch [134/200] batch [30/71] time 0.438 (0.470) data 0.306 (0.339) loss_u loss_u 0.8730 (0.8905) acc_u 21.8750 (14.7917) lr 5.1825e-04 eta 0:00:19
epoch [134/200] batch [35/71] time 0.415 (0.468) data 0.283 (0.337) loss_u loss_u 0.9409 (0.8937) acc_u 9.3750 (14.2857) lr 5.1825e-04 eta 0:00:16
epoch [134/200] batch [40/71] time 0.438 (0.470) data 0.307 (0.339) loss_u loss_u 0.8799 (0.8951) acc_u 12.5000 (14.0625) lr 5.1825e-04 eta 0:00:14
epoch [134/200] batch [45/71] time 0.609 (0.469) data 0.477 (0.338) loss_u loss_u 0.9253 (0.8945) acc_u 9.3750 (14.0972) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [50/71] time 0.453 (0.468) data 0.321 (0.337) loss_u loss_u 0.8970 (0.8911) acc_u 15.6250 (14.4375) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [55/71] time 0.363 (0.465) data 0.233 (0.334) loss_u loss_u 0.9111 (0.8933) acc_u 9.3750 (13.9773) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [60/71] time 0.582 (0.469) data 0.451 (0.338) loss_u loss_u 0.9121 (0.8912) acc_u 6.2500 (14.2188) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [65/71] time 0.450 (0.468) data 0.319 (0.337) loss_u loss_u 0.9600 (0.8950) acc_u 3.1250 (13.5577) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [70/71] time 0.490 (0.470) data 0.359 (0.338) loss_u loss_u 0.8955 (0.8950) acc_u 18.7500 (13.6161) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2104
confident_label rate tensor(0.2688, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 843
clean true:387
clean false:456
clean_rate:0.45907473309608543
noisy true:645
noisy false:1648
after delete: len(clean_dataset) 843
after delete: len(noisy_dataset) 2293
epoch [135/200] batch [5/26] time 0.491 (0.482) data 0.359 (0.351) loss_x loss_x 1.2070 (1.0754) acc_x 65.6250 (70.6250) lr 5.0454e-04 eta 0:00:10
epoch [135/200] batch [10/26] time 0.560 (0.475) data 0.429 (0.344) loss_x loss_x 1.3320 (1.1958) acc_x 68.7500 (70.0000) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [15/26] time 0.491 (0.477) data 0.360 (0.346) loss_x loss_x 1.0234 (1.1500) acc_x 59.3750 (68.7500) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [20/26] time 0.409 (0.477) data 0.278 (0.346) loss_x loss_x 0.8745 (1.1318) acc_x 75.0000 (69.0625) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [25/26] time 0.509 (0.473) data 0.379 (0.342) loss_x loss_x 1.2344 (1.1592) acc_x 71.8750 (68.2500) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/71] time 0.383 (0.469) data 0.250 (0.338) loss_u loss_u 0.9233 (0.8877) acc_u 9.3750 (14.3750) lr 5.0454e-04 eta 0:00:30
epoch [135/200] batch [10/71] time 0.515 (0.470) data 0.383 (0.339) loss_u loss_u 0.9268 (0.8966) acc_u 12.5000 (12.5000) lr 5.0454e-04 eta 0:00:28
epoch [135/200] batch [15/71] time 0.382 (0.467) data 0.250 (0.336) loss_u loss_u 0.9512 (0.9033) acc_u 6.2500 (12.0833) lr 5.0454e-04 eta 0:00:26
epoch [135/200] batch [20/71] time 0.749 (0.474) data 0.617 (0.343) loss_u loss_u 0.9395 (0.8990) acc_u 6.2500 (12.8125) lr 5.0454e-04 eta 0:00:24
epoch [135/200] batch [25/71] time 0.419 (0.471) data 0.287 (0.340) loss_u loss_u 0.8726 (0.8977) acc_u 21.8750 (13.1250) lr 5.0454e-04 eta 0:00:21
epoch [135/200] batch [30/71] time 0.388 (0.471) data 0.256 (0.340) loss_u loss_u 0.9209 (0.9017) acc_u 12.5000 (12.6042) lr 5.0454e-04 eta 0:00:19
epoch [135/200] batch [35/71] time 0.358 (0.470) data 0.226 (0.339) loss_u loss_u 0.8604 (0.9003) acc_u 15.6250 (12.6786) lr 5.0454e-04 eta 0:00:16
epoch [135/200] batch [40/71] time 0.397 (0.468) data 0.266 (0.337) loss_u loss_u 0.9033 (0.8988) acc_u 12.5000 (12.8906) lr 5.0454e-04 eta 0:00:14
epoch [135/200] batch [45/71] time 0.376 (0.464) data 0.246 (0.333) loss_u loss_u 0.9497 (0.9013) acc_u 3.1250 (12.5694) lr 5.0454e-04 eta 0:00:12
epoch [135/200] batch [50/71] time 0.445 (0.463) data 0.314 (0.332) loss_u loss_u 0.9336 (0.9048) acc_u 6.2500 (12.0000) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [55/71] time 0.383 (0.461) data 0.251 (0.330) loss_u loss_u 0.8257 (0.9022) acc_u 18.7500 (12.2159) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [60/71] time 0.439 (0.462) data 0.308 (0.331) loss_u loss_u 0.8330 (0.9024) acc_u 21.8750 (12.2917) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [65/71] time 0.467 (0.463) data 0.336 (0.331) loss_u loss_u 0.9502 (0.9017) acc_u 3.1250 (12.3077) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [70/71] time 0.420 (0.461) data 0.290 (0.330) loss_u loss_u 0.8999 (0.8993) acc_u 12.5000 (12.7679) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2075
confident_label rate tensor(0.2640, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 828
clean true:362
clean false:466
clean_rate:0.43719806763285024
noisy true:699
noisy false:1609
after delete: len(clean_dataset) 828
after delete: len(noisy_dataset) 2308
epoch [136/200] batch [5/25] time 0.528 (0.429) data 0.397 (0.298) loss_x loss_x 1.0527 (1.4410) acc_x 68.7500 (62.5000) lr 4.9096e-04 eta 0:00:08
epoch [136/200] batch [10/25] time 0.368 (0.447) data 0.238 (0.316) loss_x loss_x 0.9258 (1.2529) acc_x 75.0000 (67.8125) lr 4.9096e-04 eta 0:00:06
epoch [136/200] batch [15/25] time 0.531 (0.441) data 0.401 (0.310) loss_x loss_x 0.8516 (1.1860) acc_x 71.8750 (70.6250) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [20/25] time 0.498 (0.451) data 0.367 (0.320) loss_x loss_x 1.0742 (1.1825) acc_x 59.3750 (69.2188) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [25/25] time 0.410 (0.450) data 0.280 (0.320) loss_x loss_x 1.2227 (1.1981) acc_x 71.8750 (69.0000) lr 4.9096e-04 eta 0:00:00
epoch [136/200] batch [5/72] time 0.633 (0.452) data 0.502 (0.322) loss_u loss_u 0.8228 (0.8928) acc_u 25.0000 (13.7500) lr 4.9096e-04 eta 0:00:30
epoch [136/200] batch [10/72] time 0.524 (0.456) data 0.392 (0.326) loss_u loss_u 0.9277 (0.9030) acc_u 6.2500 (12.5000) lr 4.9096e-04 eta 0:00:28
epoch [136/200] batch [15/72] time 0.411 (0.454) data 0.280 (0.323) loss_u loss_u 0.9028 (0.9039) acc_u 12.5000 (12.2917) lr 4.9096e-04 eta 0:00:25
epoch [136/200] batch [20/72] time 0.432 (0.450) data 0.302 (0.320) loss_u loss_u 0.8770 (0.8964) acc_u 12.5000 (13.4375) lr 4.9096e-04 eta 0:00:23
epoch [136/200] batch [25/72] time 0.447 (0.452) data 0.317 (0.322) loss_u loss_u 0.9404 (0.9026) acc_u 9.3750 (12.7500) lr 4.9096e-04 eta 0:00:21
epoch [136/200] batch [30/72] time 0.392 (0.455) data 0.260 (0.325) loss_u loss_u 0.8945 (0.9026) acc_u 18.7500 (13.0208) lr 4.9096e-04 eta 0:00:19
epoch [136/200] batch [35/72] time 0.432 (0.456) data 0.301 (0.325) loss_u loss_u 0.8413 (0.9020) acc_u 15.6250 (12.8571) lr 4.9096e-04 eta 0:00:16
epoch [136/200] batch [40/72] time 0.424 (0.456) data 0.292 (0.325) loss_u loss_u 0.9229 (0.8997) acc_u 12.5000 (13.0469) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [45/72] time 0.497 (0.455) data 0.365 (0.324) loss_u loss_u 0.9663 (0.9027) acc_u 0.0000 (12.7083) lr 4.9096e-04 eta 0:00:12
epoch [136/200] batch [50/72] time 0.411 (0.454) data 0.281 (0.323) loss_u loss_u 0.8550 (0.9023) acc_u 21.8750 (12.6250) lr 4.9096e-04 eta 0:00:09
epoch [136/200] batch [55/72] time 0.364 (0.451) data 0.232 (0.320) loss_u loss_u 0.9395 (0.9026) acc_u 9.3750 (12.6136) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [60/72] time 0.389 (0.458) data 0.258 (0.327) loss_u loss_u 0.8823 (0.9038) acc_u 15.6250 (12.3958) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [65/72] time 0.399 (0.457) data 0.267 (0.325) loss_u loss_u 0.8535 (0.9039) acc_u 15.6250 (12.3558) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [70/72] time 0.355 (0.456) data 0.224 (0.325) loss_u loss_u 0.7954 (0.9014) acc_u 25.0000 (12.7679) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2076
confident_label rate tensor(0.2605, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 817
clean true:374
clean false:443
clean_rate:0.45777233782129745
noisy true:686
noisy false:1633
after delete: len(clean_dataset) 817
after delete: len(noisy_dataset) 2319
epoch [137/200] batch [5/25] time 0.374 (0.497) data 0.243 (0.366) loss_x loss_x 1.1934 (1.2133) acc_x 75.0000 (70.0000) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [10/25] time 0.459 (0.480) data 0.327 (0.348) loss_x loss_x 1.2119 (1.3117) acc_x 68.7500 (67.5000) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [15/25] time 0.332 (0.459) data 0.201 (0.328) loss_x loss_x 1.3301 (1.2946) acc_x 75.0000 (69.1667) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [20/25] time 0.455 (0.463) data 0.324 (0.332) loss_x loss_x 1.4541 (1.2897) acc_x 59.3750 (68.2812) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [25/25] time 0.452 (0.463) data 0.321 (0.331) loss_x loss_x 1.1553 (1.2740) acc_x 65.6250 (68.1250) lr 4.7750e-04 eta 0:00:00
epoch [137/200] batch [5/72] time 0.579 (0.463) data 0.447 (0.331) loss_u loss_u 0.8086 (0.8621) acc_u 25.0000 (16.8750) lr 4.7750e-04 eta 0:00:30
epoch [137/200] batch [10/72] time 0.413 (0.465) data 0.283 (0.334) loss_u loss_u 0.9429 (0.8689) acc_u 3.1250 (16.8750) lr 4.7750e-04 eta 0:00:28
epoch [137/200] batch [15/72] time 0.551 (0.461) data 0.419 (0.330) loss_u loss_u 0.9297 (0.8806) acc_u 6.2500 (14.3750) lr 4.7750e-04 eta 0:00:26
epoch [137/200] batch [20/72] time 0.468 (0.457) data 0.336 (0.326) loss_u loss_u 0.8618 (0.8790) acc_u 21.8750 (15.3125) lr 4.7750e-04 eta 0:00:23
epoch [137/200] batch [25/72] time 0.379 (0.455) data 0.248 (0.324) loss_u loss_u 0.8530 (0.8804) acc_u 15.6250 (15.1250) lr 4.7750e-04 eta 0:00:21
epoch [137/200] batch [30/72] time 0.449 (0.453) data 0.318 (0.321) loss_u loss_u 0.8750 (0.8851) acc_u 15.6250 (14.2708) lr 4.7750e-04 eta 0:00:19
epoch [137/200] batch [35/72] time 0.420 (0.454) data 0.289 (0.322) loss_u loss_u 0.9121 (0.8882) acc_u 12.5000 (13.8393) lr 4.7750e-04 eta 0:00:16
epoch [137/200] batch [40/72] time 0.356 (0.452) data 0.225 (0.321) loss_u loss_u 0.9062 (0.8890) acc_u 12.5000 (13.5938) lr 4.7750e-04 eta 0:00:14
epoch [137/200] batch [45/72] time 0.475 (0.455) data 0.345 (0.323) loss_u loss_u 0.8901 (0.8857) acc_u 18.7500 (14.1667) lr 4.7750e-04 eta 0:00:12
epoch [137/200] batch [50/72] time 0.398 (0.456) data 0.268 (0.324) loss_u loss_u 0.9136 (0.8890) acc_u 9.3750 (13.9375) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [55/72] time 0.387 (0.457) data 0.255 (0.326) loss_u loss_u 0.9756 (0.8900) acc_u 3.1250 (13.8636) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [60/72] time 0.369 (0.455) data 0.237 (0.324) loss_u loss_u 0.9751 (0.8897) acc_u 3.1250 (14.0104) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [65/72] time 0.760 (0.458) data 0.629 (0.327) loss_u loss_u 0.9175 (0.8910) acc_u 9.3750 (13.8942) lr 4.7750e-04 eta 0:00:03
epoch [137/200] batch [70/72] time 0.476 (0.457) data 0.346 (0.326) loss_u loss_u 0.9033 (0.8895) acc_u 12.5000 (14.1964) lr 4.7750e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2083
confident_label rate tensor(0.2647, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 830
clean true:380
clean false:450
clean_rate:0.4578313253012048
noisy true:673
noisy false:1633
after delete: len(clean_dataset) 830
after delete: len(noisy_dataset) 2306
epoch [138/200] batch [5/25] time 0.411 (0.472) data 0.280 (0.341) loss_x loss_x 0.9976 (1.1204) acc_x 62.5000 (70.0000) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [10/25] time 0.450 (0.489) data 0.318 (0.358) loss_x loss_x 1.0596 (1.1593) acc_x 71.8750 (70.9375) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [15/25] time 0.490 (0.491) data 0.360 (0.360) loss_x loss_x 1.1914 (1.1513) acc_x 68.7500 (72.0833) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [20/25] time 0.481 (0.493) data 0.351 (0.362) loss_x loss_x 1.3340 (1.1355) acc_x 59.3750 (72.6562) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [25/25] time 0.404 (0.488) data 0.274 (0.357) loss_x loss_x 1.2139 (1.1269) acc_x 68.7500 (72.7500) lr 4.6417e-04 eta 0:00:00
epoch [138/200] batch [5/72] time 0.331 (0.476) data 0.199 (0.345) loss_u loss_u 0.9209 (0.8922) acc_u 15.6250 (18.1250) lr 4.6417e-04 eta 0:00:31
epoch [138/200] batch [10/72] time 0.372 (0.466) data 0.242 (0.335) loss_u loss_u 0.8247 (0.8943) acc_u 25.0000 (16.2500) lr 4.6417e-04 eta 0:00:28
epoch [138/200] batch [15/72] time 0.460 (0.464) data 0.329 (0.333) loss_u loss_u 0.9165 (0.8947) acc_u 9.3750 (14.5833) lr 4.6417e-04 eta 0:00:26
epoch [138/200] batch [20/72] time 0.434 (0.459) data 0.304 (0.328) loss_u loss_u 0.9238 (0.8949) acc_u 9.3750 (14.2188) lr 4.6417e-04 eta 0:00:23
epoch [138/200] batch [25/72] time 0.505 (0.459) data 0.374 (0.328) loss_u loss_u 0.9399 (0.8985) acc_u 9.3750 (13.7500) lr 4.6417e-04 eta 0:00:21
epoch [138/200] batch [30/72] time 0.550 (0.460) data 0.419 (0.329) loss_u loss_u 0.8516 (0.9004) acc_u 15.6250 (13.3333) lr 4.6417e-04 eta 0:00:19
epoch [138/200] batch [35/72] time 0.433 (0.459) data 0.302 (0.328) loss_u loss_u 0.9033 (0.8982) acc_u 9.3750 (13.5714) lr 4.6417e-04 eta 0:00:16
epoch [138/200] batch [40/72] time 0.467 (0.466) data 0.337 (0.335) loss_u loss_u 0.9292 (0.8986) acc_u 9.3750 (13.2812) lr 4.6417e-04 eta 0:00:14
epoch [138/200] batch [45/72] time 0.436 (0.467) data 0.306 (0.336) loss_u loss_u 0.9214 (0.8980) acc_u 9.3750 (13.1944) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [50/72] time 0.338 (0.463) data 0.207 (0.331) loss_u loss_u 0.8477 (0.8979) acc_u 18.7500 (12.9375) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [55/72] time 0.436 (0.465) data 0.305 (0.333) loss_u loss_u 0.7979 (0.8947) acc_u 28.1250 (13.4659) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [60/72] time 0.362 (0.467) data 0.232 (0.336) loss_u loss_u 0.9238 (0.8949) acc_u 9.3750 (13.5417) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [65/72] time 0.382 (0.462) data 0.251 (0.331) loss_u loss_u 0.8149 (0.8945) acc_u 21.8750 (13.5577) lr 4.6417e-04 eta 0:00:03
epoch [138/200] batch [70/72] time 0.507 (0.462) data 0.376 (0.331) loss_u loss_u 0.9800 (0.8952) acc_u 3.1250 (13.3482) lr 4.6417e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2080
confident_label rate tensor(0.2624, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 823
clean true:382
clean false:441
clean_rate:0.4641555285540705
noisy true:674
noisy false:1639
after delete: len(clean_dataset) 823
after delete: len(noisy_dataset) 2313
epoch [139/200] batch [5/25] time 0.438 (0.386) data 0.308 (0.256) loss_x loss_x 1.1299 (1.1590) acc_x 65.6250 (66.2500) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [10/25] time 0.569 (0.431) data 0.438 (0.301) loss_x loss_x 1.4551 (1.1368) acc_x 50.0000 (68.7500) lr 4.5098e-04 eta 0:00:06
epoch [139/200] batch [15/25] time 0.567 (0.443) data 0.436 (0.312) loss_x loss_x 1.2646 (1.2014) acc_x 75.0000 (67.7083) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [20/25] time 0.415 (0.448) data 0.285 (0.317) loss_x loss_x 1.4775 (1.2111) acc_x 59.3750 (68.1250) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [25/25] time 0.483 (0.453) data 0.353 (0.322) loss_x loss_x 1.1611 (1.2172) acc_x 78.1250 (68.2500) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/72] time 0.463 (0.458) data 0.333 (0.327) loss_u loss_u 0.9136 (0.8974) acc_u 9.3750 (13.7500) lr 4.5098e-04 eta 0:00:30
epoch [139/200] batch [10/72] time 0.535 (0.462) data 0.404 (0.332) loss_u loss_u 0.9102 (0.8998) acc_u 9.3750 (12.8125) lr 4.5098e-04 eta 0:00:28
epoch [139/200] batch [15/72] time 0.558 (0.468) data 0.427 (0.337) loss_u loss_u 0.8955 (0.9009) acc_u 12.5000 (12.5000) lr 4.5098e-04 eta 0:00:26
epoch [139/200] batch [20/72] time 0.400 (0.460) data 0.269 (0.329) loss_u loss_u 0.8672 (0.8970) acc_u 21.8750 (13.4375) lr 4.5098e-04 eta 0:00:23
epoch [139/200] batch [25/72] time 0.476 (0.459) data 0.345 (0.328) loss_u loss_u 0.9170 (0.8897) acc_u 12.5000 (14.8750) lr 4.5098e-04 eta 0:00:21
epoch [139/200] batch [30/72] time 0.415 (0.457) data 0.283 (0.326) loss_u loss_u 0.9531 (0.8897) acc_u 9.3750 (14.6875) lr 4.5098e-04 eta 0:00:19
epoch [139/200] batch [35/72] time 0.424 (0.458) data 0.294 (0.327) loss_u loss_u 0.9170 (0.8928) acc_u 9.3750 (14.1071) lr 4.5098e-04 eta 0:00:16
epoch [139/200] batch [40/72] time 0.466 (0.459) data 0.335 (0.328) loss_u loss_u 0.9316 (0.8927) acc_u 6.2500 (13.9844) lr 4.5098e-04 eta 0:00:14
epoch [139/200] batch [45/72] time 0.470 (0.458) data 0.340 (0.327) loss_u loss_u 0.9072 (0.8935) acc_u 9.3750 (14.0278) lr 4.5098e-04 eta 0:00:12
epoch [139/200] batch [50/72] time 0.467 (0.457) data 0.336 (0.326) loss_u loss_u 0.8887 (0.8955) acc_u 12.5000 (13.5625) lr 4.5098e-04 eta 0:00:10
epoch [139/200] batch [55/72] time 0.462 (0.457) data 0.331 (0.326) loss_u loss_u 0.9341 (0.8942) acc_u 6.2500 (13.6932) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [60/72] time 0.353 (0.457) data 0.221 (0.326) loss_u loss_u 0.9136 (0.8951) acc_u 9.3750 (13.4896) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [65/72] time 0.425 (0.454) data 0.294 (0.323) loss_u loss_u 0.9297 (0.8950) acc_u 9.3750 (13.5577) lr 4.5098e-04 eta 0:00:03
epoch [139/200] batch [70/72] time 0.413 (0.454) data 0.282 (0.323) loss_u loss_u 0.9194 (0.8927) acc_u 9.3750 (13.9286) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2050
confident_label rate tensor(0.2701, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 847
clean true:392
clean false:455
clean_rate:0.4628099173553719
noisy true:694
noisy false:1595
after delete: len(clean_dataset) 847
after delete: len(noisy_dataset) 2289
epoch [140/200] batch [5/26] time 0.566 (0.500) data 0.435 (0.368) loss_x loss_x 0.7378 (1.1767) acc_x 87.5000 (71.8750) lr 4.3792e-04 eta 0:00:10
epoch [140/200] batch [10/26] time 0.515 (0.502) data 0.384 (0.371) loss_x loss_x 1.8408 (1.2448) acc_x 62.5000 (70.9375) lr 4.3792e-04 eta 0:00:08
epoch [140/200] batch [15/26] time 0.488 (0.494) data 0.357 (0.363) loss_x loss_x 1.1553 (1.3066) acc_x 71.8750 (70.0000) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [20/26] time 0.415 (0.483) data 0.284 (0.352) loss_x loss_x 1.0938 (1.2492) acc_x 78.1250 (70.9375) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [25/26] time 0.422 (0.478) data 0.292 (0.347) loss_x loss_x 1.0166 (1.2309) acc_x 75.0000 (70.6250) lr 4.3792e-04 eta 0:00:00
epoch [140/200] batch [5/71] time 0.510 (0.473) data 0.378 (0.342) loss_u loss_u 0.9028 (0.8949) acc_u 12.5000 (13.1250) lr 4.3792e-04 eta 0:00:31
epoch [140/200] batch [10/71] time 0.499 (0.468) data 0.367 (0.336) loss_u loss_u 0.9106 (0.8967) acc_u 12.5000 (13.4375) lr 4.3792e-04 eta 0:00:28
epoch [140/200] batch [15/71] time 0.600 (0.473) data 0.468 (0.342) loss_u loss_u 0.8721 (0.8930) acc_u 12.5000 (13.5417) lr 4.3792e-04 eta 0:00:26
epoch [140/200] batch [20/71] time 0.455 (0.469) data 0.324 (0.337) loss_u loss_u 0.9053 (0.8898) acc_u 18.7500 (14.0625) lr 4.3792e-04 eta 0:00:23
epoch [140/200] batch [25/71] time 0.478 (0.467) data 0.347 (0.336) loss_u loss_u 0.9297 (0.8930) acc_u 9.3750 (13.6250) lr 4.3792e-04 eta 0:00:21
epoch [140/200] batch [30/71] time 0.422 (0.467) data 0.290 (0.336) loss_u loss_u 0.8784 (0.8943) acc_u 15.6250 (13.4375) lr 4.3792e-04 eta 0:00:19
epoch [140/200] batch [35/71] time 0.448 (0.467) data 0.316 (0.335) loss_u loss_u 0.9141 (0.8927) acc_u 9.3750 (13.6607) lr 4.3792e-04 eta 0:00:16
epoch [140/200] batch [40/71] time 0.404 (0.465) data 0.271 (0.334) loss_u loss_u 0.9170 (0.8947) acc_u 6.2500 (13.2812) lr 4.3792e-04 eta 0:00:14
epoch [140/200] batch [45/71] time 0.608 (0.467) data 0.476 (0.336) loss_u loss_u 0.8574 (0.8910) acc_u 18.7500 (13.9583) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [50/71] time 0.464 (0.467) data 0.332 (0.335) loss_u loss_u 0.9434 (0.8968) acc_u 6.2500 (13.0625) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [55/71] time 0.469 (0.472) data 0.338 (0.340) loss_u loss_u 0.9653 (0.8975) acc_u 6.2500 (13.0114) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [60/71] time 0.505 (0.472) data 0.374 (0.341) loss_u loss_u 0.9766 (0.8978) acc_u 3.1250 (13.0208) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [65/71] time 0.531 (0.471) data 0.400 (0.339) loss_u loss_u 0.8652 (0.8975) acc_u 15.6250 (13.1731) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [70/71] time 0.403 (0.469) data 0.272 (0.338) loss_u loss_u 0.9077 (0.8964) acc_u 15.6250 (13.3929) lr 4.3792e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2095
confident_label rate tensor(0.2612, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 819
clean true:367
clean false:452
clean_rate:0.4481074481074481
noisy true:674
noisy false:1643
after delete: len(clean_dataset) 819
after delete: len(noisy_dataset) 2317
epoch [141/200] batch [5/25] time 0.441 (0.469) data 0.310 (0.338) loss_x loss_x 0.9199 (1.0197) acc_x 75.0000 (76.2500) lr 4.2499e-04 eta 0:00:09
epoch [141/200] batch [10/25] time 0.463 (0.465) data 0.331 (0.334) loss_x loss_x 0.9580 (1.1623) acc_x 78.1250 (72.1875) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [15/25] time 0.452 (0.457) data 0.321 (0.326) loss_x loss_x 1.2422 (1.1559) acc_x 68.7500 (71.4583) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [20/25] time 0.360 (0.448) data 0.229 (0.316) loss_x loss_x 1.1904 (1.1469) acc_x 81.2500 (71.8750) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [25/25] time 0.392 (0.449) data 0.261 (0.318) loss_x loss_x 0.8066 (1.1527) acc_x 81.2500 (72.0000) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/72] time 0.473 (0.457) data 0.335 (0.326) loss_u loss_u 0.9102 (0.8710) acc_u 12.5000 (15.6250) lr 4.2499e-04 eta 0:00:30
epoch [141/200] batch [10/72] time 0.406 (0.456) data 0.274 (0.324) loss_u loss_u 0.9619 (0.8837) acc_u 6.2500 (14.3750) lr 4.2499e-04 eta 0:00:28
epoch [141/200] batch [15/72] time 0.418 (0.458) data 0.288 (0.327) loss_u loss_u 0.8125 (0.8907) acc_u 21.8750 (13.7500) lr 4.2499e-04 eta 0:00:26
epoch [141/200] batch [20/72] time 0.420 (0.467) data 0.289 (0.336) loss_u loss_u 0.9307 (0.8991) acc_u 6.2500 (12.6562) lr 4.2499e-04 eta 0:00:24
epoch [141/200] batch [25/72] time 0.432 (0.468) data 0.300 (0.336) loss_u loss_u 0.9082 (0.8989) acc_u 15.6250 (12.7500) lr 4.2499e-04 eta 0:00:21
epoch [141/200] batch [30/72] time 0.371 (0.466) data 0.240 (0.334) loss_u loss_u 0.9219 (0.8982) acc_u 9.3750 (12.9167) lr 4.2499e-04 eta 0:00:19
epoch [141/200] batch [35/72] time 0.611 (0.464) data 0.479 (0.333) loss_u loss_u 0.9155 (0.8985) acc_u 15.6250 (13.2143) lr 4.2499e-04 eta 0:00:17
epoch [141/200] batch [40/72] time 0.449 (0.461) data 0.318 (0.330) loss_u loss_u 0.9185 (0.9008) acc_u 9.3750 (12.7344) lr 4.2499e-04 eta 0:00:14
epoch [141/200] batch [45/72] time 0.455 (0.464) data 0.323 (0.332) loss_u loss_u 0.9521 (0.9029) acc_u 3.1250 (12.5694) lr 4.2499e-04 eta 0:00:12
epoch [141/200] batch [50/72] time 0.496 (0.462) data 0.363 (0.330) loss_u loss_u 0.8052 (0.8996) acc_u 21.8750 (12.7500) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [55/72] time 0.372 (0.461) data 0.242 (0.329) loss_u loss_u 0.8350 (0.8977) acc_u 21.8750 (12.8409) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [60/72] time 0.397 (0.463) data 0.265 (0.331) loss_u loss_u 0.8271 (0.8962) acc_u 15.6250 (12.8646) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [65/72] time 0.467 (0.459) data 0.336 (0.327) loss_u loss_u 0.9565 (0.8965) acc_u 6.2500 (12.7885) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [70/72] time 0.398 (0.458) data 0.266 (0.327) loss_u loss_u 0.8037 (0.8937) acc_u 25.0000 (13.1250) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2112
confident_label rate tensor(0.2624, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 823
clean true:360
clean false:463
clean_rate:0.4374240583232078
noisy true:664
noisy false:1649
after delete: len(clean_dataset) 823
after delete: len(noisy_dataset) 2313
epoch [142/200] batch [5/25] time 0.557 (0.458) data 0.425 (0.327) loss_x loss_x 0.9951 (1.1479) acc_x 68.7500 (69.3750) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [10/25] time 0.443 (0.448) data 0.312 (0.317) loss_x loss_x 1.0361 (1.1454) acc_x 71.8750 (71.2500) lr 4.1221e-04 eta 0:00:06
epoch [142/200] batch [15/25] time 0.465 (0.466) data 0.334 (0.335) loss_x loss_x 1.1875 (1.1821) acc_x 68.7500 (71.4583) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [20/25] time 0.459 (0.459) data 0.328 (0.328) loss_x loss_x 1.5439 (1.1830) acc_x 59.3750 (71.7188) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [25/25] time 0.506 (0.468) data 0.375 (0.337) loss_x loss_x 1.2031 (1.1626) acc_x 68.7500 (71.0000) lr 4.1221e-04 eta 0:00:00
epoch [142/200] batch [5/72] time 0.447 (0.469) data 0.316 (0.338) loss_u loss_u 0.8594 (0.8837) acc_u 15.6250 (13.7500) lr 4.1221e-04 eta 0:00:31
epoch [142/200] batch [10/72] time 0.380 (0.468) data 0.247 (0.337) loss_u loss_u 0.9229 (0.9027) acc_u 6.2500 (11.8750) lr 4.1221e-04 eta 0:00:28
epoch [142/200] batch [15/72] time 0.745 (0.482) data 0.614 (0.351) loss_u loss_u 0.9341 (0.9022) acc_u 6.2500 (11.2500) lr 4.1221e-04 eta 0:00:27
epoch [142/200] batch [20/72] time 0.659 (0.484) data 0.527 (0.353) loss_u loss_u 0.8730 (0.9001) acc_u 18.7500 (12.3438) lr 4.1221e-04 eta 0:00:25
epoch [142/200] batch [25/72] time 0.427 (0.486) data 0.295 (0.354) loss_u loss_u 0.9595 (0.8930) acc_u 3.1250 (13.1250) lr 4.1221e-04 eta 0:00:22
epoch [142/200] batch [30/72] time 0.354 (0.480) data 0.223 (0.349) loss_u loss_u 0.8774 (0.8896) acc_u 12.5000 (13.2292) lr 4.1221e-04 eta 0:00:20
epoch [142/200] batch [35/72] time 0.450 (0.474) data 0.319 (0.343) loss_u loss_u 0.9771 (0.8943) acc_u 0.0000 (12.5893) lr 4.1221e-04 eta 0:00:17
epoch [142/200] batch [40/72] time 0.387 (0.472) data 0.255 (0.341) loss_u loss_u 0.8936 (0.8928) acc_u 12.5000 (12.7344) lr 4.1221e-04 eta 0:00:15
epoch [142/200] batch [45/72] time 0.370 (0.468) data 0.239 (0.336) loss_u loss_u 0.9258 (0.8945) acc_u 9.3750 (12.7083) lr 4.1221e-04 eta 0:00:12
epoch [142/200] batch [50/72] time 0.472 (0.467) data 0.340 (0.335) loss_u loss_u 0.9102 (0.8938) acc_u 12.5000 (13.0625) lr 4.1221e-04 eta 0:00:10
epoch [142/200] batch [55/72] time 0.546 (0.465) data 0.415 (0.333) loss_u loss_u 0.9160 (0.8939) acc_u 9.3750 (13.0114) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [60/72] time 0.531 (0.466) data 0.401 (0.334) loss_u loss_u 0.8750 (0.8941) acc_u 18.7500 (13.0208) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [65/72] time 0.473 (0.466) data 0.343 (0.334) loss_u loss_u 0.8999 (0.8945) acc_u 12.5000 (12.9327) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [70/72] time 0.379 (0.464) data 0.248 (0.332) loss_u loss_u 0.9146 (0.8943) acc_u 12.5000 (13.0804) lr 4.1221e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2110
confident_label rate tensor(0.2656, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 833
clean true:369
clean false:464
clean_rate:0.4429771908763505
noisy true:657
noisy false:1646
after delete: len(clean_dataset) 833
after delete: len(noisy_dataset) 2303
epoch [143/200] batch [5/26] time 0.372 (0.461) data 0.240 (0.330) loss_x loss_x 1.4688 (1.1881) acc_x 62.5000 (68.1250) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [10/26] time 0.433 (0.476) data 0.303 (0.345) loss_x loss_x 1.2002 (1.2433) acc_x 68.7500 (69.6875) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [15/26] time 0.451 (0.468) data 0.320 (0.337) loss_x loss_x 1.7275 (1.3023) acc_x 59.3750 (68.3333) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [20/26] time 0.550 (0.476) data 0.419 (0.345) loss_x loss_x 0.9478 (1.2887) acc_x 81.2500 (68.1250) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [25/26] time 0.417 (0.467) data 0.286 (0.336) loss_x loss_x 0.9819 (1.2713) acc_x 75.0000 (68.3750) lr 3.9958e-04 eta 0:00:00
epoch [143/200] batch [5/71] time 0.395 (0.466) data 0.263 (0.335) loss_u loss_u 0.8833 (0.9125) acc_u 12.5000 (9.3750) lr 3.9958e-04 eta 0:00:30
epoch [143/200] batch [10/71] time 0.449 (0.465) data 0.317 (0.334) loss_u loss_u 0.9058 (0.9085) acc_u 15.6250 (10.0000) lr 3.9958e-04 eta 0:00:28
epoch [143/200] batch [15/71] time 0.338 (0.458) data 0.206 (0.327) loss_u loss_u 0.9673 (0.9067) acc_u 3.1250 (11.4583) lr 3.9958e-04 eta 0:00:25
epoch [143/200] batch [20/71] time 0.413 (0.463) data 0.281 (0.332) loss_u loss_u 0.8652 (0.9044) acc_u 15.6250 (12.0312) lr 3.9958e-04 eta 0:00:23
epoch [143/200] batch [25/71] time 0.363 (0.462) data 0.231 (0.331) loss_u loss_u 0.8926 (0.9013) acc_u 18.7500 (12.7500) lr 3.9958e-04 eta 0:00:21
epoch [143/200] batch [30/71] time 0.395 (0.462) data 0.264 (0.330) loss_u loss_u 0.8584 (0.9023) acc_u 21.8750 (12.2917) lr 3.9958e-04 eta 0:00:18
epoch [143/200] batch [35/71] time 0.409 (0.462) data 0.278 (0.330) loss_u loss_u 0.9116 (0.9037) acc_u 9.3750 (12.1429) lr 3.9958e-04 eta 0:00:16
epoch [143/200] batch [40/71] time 0.557 (0.460) data 0.426 (0.329) loss_u loss_u 0.9287 (0.9021) acc_u 9.3750 (12.3438) lr 3.9958e-04 eta 0:00:14
epoch [143/200] batch [45/71] time 0.398 (0.456) data 0.267 (0.324) loss_u loss_u 0.8906 (0.9031) acc_u 12.5000 (12.2917) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [50/71] time 0.515 (0.457) data 0.384 (0.326) loss_u loss_u 0.8579 (0.8983) acc_u 15.6250 (12.7500) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [55/71] time 0.386 (0.454) data 0.255 (0.323) loss_u loss_u 0.8530 (0.8985) acc_u 15.6250 (12.8409) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [60/71] time 0.433 (0.456) data 0.301 (0.325) loss_u loss_u 0.8574 (0.8969) acc_u 15.6250 (12.9688) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [65/71] time 0.393 (0.456) data 0.261 (0.325) loss_u loss_u 0.8584 (0.8954) acc_u 15.6250 (13.0769) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [70/71] time 0.393 (0.461) data 0.262 (0.329) loss_u loss_u 0.9033 (0.8967) acc_u 12.5000 (12.9018) lr 3.9958e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2092
confident_label rate tensor(0.2659, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 834
clean true:381
clean false:453
clean_rate:0.4568345323741007
noisy true:663
noisy false:1639
after delete: len(clean_dataset) 834
after delete: len(noisy_dataset) 2302
epoch [144/200] batch [5/26] time 0.504 (0.496) data 0.374 (0.366) loss_x loss_x 0.7856 (1.3173) acc_x 78.1250 (66.2500) lr 3.8709e-04 eta 0:00:10
epoch [144/200] batch [10/26] time 0.468 (0.480) data 0.337 (0.349) loss_x loss_x 0.9824 (1.1895) acc_x 75.0000 (70.6250) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [15/26] time 0.614 (0.493) data 0.482 (0.362) loss_x loss_x 0.8384 (1.1959) acc_x 78.1250 (69.5833) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [20/26] time 0.402 (0.476) data 0.271 (0.345) loss_x loss_x 1.9629 (1.2537) acc_x 56.2500 (68.7500) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [25/26] time 0.618 (0.491) data 0.487 (0.360) loss_x loss_x 1.0938 (1.2304) acc_x 75.0000 (69.7500) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/71] time 0.458 (0.478) data 0.326 (0.347) loss_u loss_u 0.9448 (0.8979) acc_u 9.3750 (17.5000) lr 3.8709e-04 eta 0:00:31
epoch [144/200] batch [10/71] time 0.555 (0.486) data 0.423 (0.354) loss_u loss_u 0.9375 (0.8902) acc_u 9.3750 (16.5625) lr 3.8709e-04 eta 0:00:29
epoch [144/200] batch [15/71] time 0.387 (0.476) data 0.255 (0.344) loss_u loss_u 0.9072 (0.8915) acc_u 9.3750 (15.6250) lr 3.8709e-04 eta 0:00:26
epoch [144/200] batch [20/71] time 0.380 (0.474) data 0.248 (0.343) loss_u loss_u 0.8906 (0.8930) acc_u 9.3750 (14.8438) lr 3.8709e-04 eta 0:00:24
epoch [144/200] batch [25/71] time 0.446 (0.470) data 0.316 (0.338) loss_u loss_u 0.8960 (0.8975) acc_u 12.5000 (14.2500) lr 3.8709e-04 eta 0:00:21
epoch [144/200] batch [30/71] time 0.534 (0.471) data 0.403 (0.340) loss_u loss_u 0.8833 (0.8992) acc_u 12.5000 (13.6458) lr 3.8709e-04 eta 0:00:19
epoch [144/200] batch [35/71] time 0.411 (0.469) data 0.280 (0.338) loss_u loss_u 0.9346 (0.8980) acc_u 6.2500 (13.5714) lr 3.8709e-04 eta 0:00:16
epoch [144/200] batch [40/71] time 0.414 (0.469) data 0.282 (0.337) loss_u loss_u 0.9111 (0.8961) acc_u 9.3750 (13.6719) lr 3.8709e-04 eta 0:00:14
epoch [144/200] batch [45/71] time 0.361 (0.466) data 0.230 (0.335) loss_u loss_u 0.8999 (0.8959) acc_u 15.6250 (13.8194) lr 3.8709e-04 eta 0:00:12
epoch [144/200] batch [50/71] time 0.499 (0.465) data 0.368 (0.334) loss_u loss_u 0.9028 (0.8975) acc_u 9.3750 (13.5000) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [55/71] time 0.492 (0.466) data 0.361 (0.334) loss_u loss_u 0.9102 (0.8952) acc_u 12.5000 (13.7500) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [60/71] time 0.578 (0.466) data 0.446 (0.334) loss_u loss_u 0.8149 (0.8938) acc_u 21.8750 (13.7500) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [65/71] time 0.465 (0.464) data 0.334 (0.333) loss_u loss_u 0.9648 (0.8963) acc_u 6.2500 (13.6058) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [70/71] time 0.464 (0.466) data 0.333 (0.335) loss_u loss_u 0.8569 (0.8947) acc_u 18.7500 (13.8393) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2105
confident_label rate tensor(0.2650, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 831
clean true:384
clean false:447
clean_rate:0.4620938628158845
noisy true:647
noisy false:1658
after delete: len(clean_dataset) 831
after delete: len(noisy_dataset) 2305
epoch [145/200] batch [5/25] time 0.448 (0.443) data 0.318 (0.312) loss_x loss_x 0.7354 (1.1191) acc_x 75.0000 (68.1250) lr 3.7476e-04 eta 0:00:08
epoch [145/200] batch [10/25] time 0.442 (0.438) data 0.312 (0.307) loss_x loss_x 1.7910 (1.1425) acc_x 56.2500 (67.8125) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [15/25] time 0.479 (0.445) data 0.348 (0.314) loss_x loss_x 0.8965 (1.1207) acc_x 75.0000 (69.3750) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [20/25] time 0.440 (0.439) data 0.308 (0.308) loss_x loss_x 1.6846 (1.1266) acc_x 56.2500 (69.5312) lr 3.7476e-04 eta 0:00:02
epoch [145/200] batch [25/25] time 0.615 (0.457) data 0.483 (0.327) loss_x loss_x 0.6196 (1.1618) acc_x 87.5000 (69.3750) lr 3.7476e-04 eta 0:00:00
epoch [145/200] batch [5/72] time 0.522 (0.455) data 0.391 (0.324) loss_u loss_u 0.9268 (0.9028) acc_u 9.3750 (13.7500) lr 3.7476e-04 eta 0:00:30
epoch [145/200] batch [10/72] time 0.377 (0.451) data 0.246 (0.320) loss_u loss_u 0.9517 (0.9014) acc_u 6.2500 (13.1250) lr 3.7476e-04 eta 0:00:27
epoch [145/200] batch [15/72] time 0.582 (0.453) data 0.450 (0.322) loss_u loss_u 0.8877 (0.8986) acc_u 9.3750 (12.9167) lr 3.7476e-04 eta 0:00:25
epoch [145/200] batch [20/72] time 0.462 (0.456) data 0.331 (0.325) loss_u loss_u 0.8975 (0.8990) acc_u 15.6250 (12.9688) lr 3.7476e-04 eta 0:00:23
epoch [145/200] batch [25/72] time 0.645 (0.460) data 0.512 (0.329) loss_u loss_u 0.9009 (0.8979) acc_u 9.3750 (13.3750) lr 3.7476e-04 eta 0:00:21
epoch [145/200] batch [30/72] time 0.390 (0.468) data 0.258 (0.337) loss_u loss_u 0.9141 (0.8981) acc_u 12.5000 (13.4375) lr 3.7476e-04 eta 0:00:19
epoch [145/200] batch [35/72] time 0.435 (0.468) data 0.302 (0.336) loss_u loss_u 0.8193 (0.8953) acc_u 25.0000 (14.0179) lr 3.7476e-04 eta 0:00:17
epoch [145/200] batch [40/72] time 0.446 (0.462) data 0.314 (0.331) loss_u loss_u 0.9277 (0.8970) acc_u 9.3750 (13.5938) lr 3.7476e-04 eta 0:00:14
epoch [145/200] batch [45/72] time 0.484 (0.462) data 0.352 (0.331) loss_u loss_u 0.9180 (0.8973) acc_u 15.6250 (13.5417) lr 3.7476e-04 eta 0:00:12
epoch [145/200] batch [50/72] time 0.486 (0.465) data 0.352 (0.333) loss_u loss_u 0.8799 (0.8978) acc_u 15.6250 (13.3750) lr 3.7476e-04 eta 0:00:10
epoch [145/200] batch [55/72] time 0.480 (0.466) data 0.347 (0.334) loss_u loss_u 0.8960 (0.8978) acc_u 6.2500 (13.2386) lr 3.7476e-04 eta 0:00:07
epoch [145/200] batch [60/72] time 0.699 (0.467) data 0.567 (0.336) loss_u loss_u 0.8789 (0.8976) acc_u 15.6250 (13.3333) lr 3.7476e-04 eta 0:00:05
epoch [145/200] batch [65/72] time 0.406 (0.465) data 0.274 (0.334) loss_u loss_u 0.9668 (0.8969) acc_u 6.2500 (13.3654) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [70/72] time 0.478 (0.465) data 0.347 (0.333) loss_u loss_u 0.8862 (0.8958) acc_u 12.5000 (13.3929) lr 3.7476e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2094
confident_label rate tensor(0.2812, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 882
clean true:395
clean false:487
clean_rate:0.4478458049886621
noisy true:647
noisy false:1607
after delete: len(clean_dataset) 882
after delete: len(noisy_dataset) 2254
epoch [146/200] batch [5/27] time 0.439 (0.496) data 0.309 (0.366) loss_x loss_x 0.9043 (0.8729) acc_x 71.8750 (77.5000) lr 3.6258e-04 eta 0:00:10
epoch [146/200] batch [10/27] time 0.379 (0.460) data 0.248 (0.329) loss_x loss_x 0.7720 (1.0142) acc_x 84.3750 (74.6875) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [15/27] time 0.472 (0.453) data 0.342 (0.323) loss_x loss_x 0.5688 (1.0133) acc_x 81.2500 (73.9583) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [20/27] time 0.530 (0.457) data 0.399 (0.327) loss_x loss_x 1.5049 (1.0724) acc_x 59.3750 (72.5000) lr 3.6258e-04 eta 0:00:03
epoch [146/200] batch [25/27] time 0.555 (0.465) data 0.424 (0.334) loss_x loss_x 1.0010 (1.0765) acc_x 68.7500 (71.7500) lr 3.6258e-04 eta 0:00:00
epoch [146/200] batch [5/70] time 0.385 (0.467) data 0.254 (0.336) loss_u loss_u 0.9077 (0.8936) acc_u 12.5000 (14.3750) lr 3.6258e-04 eta 0:00:30
epoch [146/200] batch [10/70] time 0.471 (0.471) data 0.336 (0.340) loss_u loss_u 0.9116 (0.9000) acc_u 12.5000 (13.1250) lr 3.6258e-04 eta 0:00:28
epoch [146/200] batch [15/70] time 0.485 (0.481) data 0.354 (0.350) loss_u loss_u 0.8701 (0.9029) acc_u 15.6250 (12.0833) lr 3.6258e-04 eta 0:00:26
epoch [146/200] batch [20/70] time 0.353 (0.478) data 0.223 (0.347) loss_u loss_u 0.9551 (0.9056) acc_u 3.1250 (11.2500) lr 3.6258e-04 eta 0:00:23
epoch [146/200] batch [25/70] time 0.364 (0.470) data 0.232 (0.339) loss_u loss_u 0.8545 (0.9059) acc_u 18.7500 (11.1250) lr 3.6258e-04 eta 0:00:21
epoch [146/200] batch [30/70] time 0.489 (0.469) data 0.358 (0.338) loss_u loss_u 0.9399 (0.9072) acc_u 9.3750 (11.0417) lr 3.6258e-04 eta 0:00:18
epoch [146/200] batch [35/70] time 0.435 (0.466) data 0.303 (0.335) loss_u loss_u 0.8911 (0.9061) acc_u 15.6250 (11.3393) lr 3.6258e-04 eta 0:00:16
epoch [146/200] batch [40/70] time 0.460 (0.463) data 0.328 (0.332) loss_u loss_u 0.8901 (0.9060) acc_u 12.5000 (11.4062) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [45/70] time 0.518 (0.463) data 0.387 (0.332) loss_u loss_u 0.9160 (0.9049) acc_u 9.3750 (11.5278) lr 3.6258e-04 eta 0:00:11
epoch [146/200] batch [50/70] time 0.496 (0.465) data 0.364 (0.334) loss_u loss_u 0.9062 (0.9067) acc_u 9.3750 (11.4375) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [55/70] time 0.396 (0.466) data 0.265 (0.334) loss_u loss_u 0.8638 (0.9074) acc_u 15.6250 (11.3636) lr 3.6258e-04 eta 0:00:06
epoch [146/200] batch [60/70] time 0.400 (0.467) data 0.268 (0.335) loss_u loss_u 0.8657 (0.9046) acc_u 18.7500 (11.8229) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [65/70] time 0.539 (0.470) data 0.408 (0.339) loss_u loss_u 0.9077 (0.9032) acc_u 9.3750 (12.0673) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [70/70] time 0.465 (0.467) data 0.333 (0.336) loss_u loss_u 0.8896 (0.9030) acc_u 9.3750 (11.9643) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2039
confident_label rate tensor(0.2695, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 845
clean true:393
clean false:452
clean_rate:0.4650887573964497
noisy true:704
noisy false:1587
after delete: len(clean_dataset) 845
after delete: len(noisy_dataset) 2291
epoch [147/200] batch [5/26] time 0.513 (0.457) data 0.383 (0.326) loss_x loss_x 1.1680 (1.2090) acc_x 75.0000 (68.7500) lr 3.5055e-04 eta 0:00:09
epoch [147/200] batch [10/26] time 0.442 (0.444) data 0.311 (0.313) loss_x loss_x 1.0850 (1.1946) acc_x 71.8750 (68.7500) lr 3.5055e-04 eta 0:00:07
epoch [147/200] batch [15/26] time 0.630 (0.454) data 0.499 (0.323) loss_x loss_x 1.0859 (1.2195) acc_x 81.2500 (68.7500) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [20/26] time 0.393 (0.447) data 0.263 (0.316) loss_x loss_x 1.0078 (1.2176) acc_x 75.0000 (69.3750) lr 3.5055e-04 eta 0:00:02
epoch [147/200] batch [25/26] time 0.442 (0.453) data 0.312 (0.322) loss_x loss_x 0.8564 (1.1688) acc_x 81.2500 (70.6250) lr 3.5055e-04 eta 0:00:00
epoch [147/200] batch [5/71] time 0.402 (0.454) data 0.271 (0.323) loss_u loss_u 0.9077 (0.8960) acc_u 9.3750 (14.3750) lr 3.5055e-04 eta 0:00:29
epoch [147/200] batch [10/71] time 0.416 (0.453) data 0.285 (0.322) loss_u loss_u 0.9438 (0.9105) acc_u 3.1250 (11.5625) lr 3.5055e-04 eta 0:00:27
epoch [147/200] batch [15/71] time 0.393 (0.456) data 0.261 (0.325) loss_u loss_u 0.8706 (0.9023) acc_u 18.7500 (13.1250) lr 3.5055e-04 eta 0:00:25
epoch [147/200] batch [20/71] time 0.515 (0.460) data 0.384 (0.329) loss_u loss_u 0.9131 (0.8963) acc_u 6.2500 (13.2812) lr 3.5055e-04 eta 0:00:23
epoch [147/200] batch [25/71] time 0.379 (0.457) data 0.247 (0.326) loss_u loss_u 0.8906 (0.8988) acc_u 15.6250 (12.8750) lr 3.5055e-04 eta 0:00:21
epoch [147/200] batch [30/71] time 0.464 (0.461) data 0.331 (0.330) loss_u loss_u 0.8193 (0.8939) acc_u 25.0000 (13.7500) lr 3.5055e-04 eta 0:00:18
epoch [147/200] batch [35/71] time 0.420 (0.462) data 0.288 (0.331) loss_u loss_u 0.8770 (0.8933) acc_u 18.7500 (14.1071) lr 3.5055e-04 eta 0:00:16
epoch [147/200] batch [40/71] time 0.501 (0.463) data 0.370 (0.331) loss_u loss_u 0.8467 (0.8903) acc_u 21.8750 (14.4531) lr 3.5055e-04 eta 0:00:14
epoch [147/200] batch [45/71] time 0.399 (0.462) data 0.267 (0.330) loss_u loss_u 0.8657 (0.8906) acc_u 18.7500 (14.5833) lr 3.5055e-04 eta 0:00:12
epoch [147/200] batch [50/71] time 0.374 (0.458) data 0.243 (0.327) loss_u loss_u 0.9419 (0.8875) acc_u 9.3750 (15.1875) lr 3.5055e-04 eta 0:00:09
epoch [147/200] batch [55/71] time 0.351 (0.462) data 0.219 (0.330) loss_u loss_u 0.8447 (0.8860) acc_u 15.6250 (15.1705) lr 3.5055e-04 eta 0:00:07
epoch [147/200] batch [60/71] time 0.410 (0.462) data 0.279 (0.330) loss_u loss_u 0.8794 (0.8868) acc_u 15.6250 (15.0521) lr 3.5055e-04 eta 0:00:05
epoch [147/200] batch [65/71] time 0.515 (0.462) data 0.384 (0.331) loss_u loss_u 0.8989 (0.8879) acc_u 15.6250 (14.8077) lr 3.5055e-04 eta 0:00:02
epoch [147/200] batch [70/71] time 0.436 (0.460) data 0.304 (0.329) loss_u loss_u 0.9375 (0.8901) acc_u 9.3750 (14.4643) lr 3.5055e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2037
confident_label rate tensor(0.2679, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 840
clean true:399
clean false:441
clean_rate:0.475
noisy true:700
noisy false:1596
after delete: len(clean_dataset) 840
after delete: len(noisy_dataset) 2296
epoch [148/200] batch [5/26] time 0.492 (0.436) data 0.359 (0.304) loss_x loss_x 1.0225 (1.5715) acc_x 78.1250 (65.0000) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [10/26] time 0.412 (0.459) data 0.279 (0.327) loss_x loss_x 2.0176 (1.4167) acc_x 68.7500 (69.3750) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [15/26] time 0.462 (0.451) data 0.331 (0.320) loss_x loss_x 0.6318 (1.2578) acc_x 78.1250 (71.2500) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [20/26] time 0.404 (0.448) data 0.274 (0.317) loss_x loss_x 1.2480 (1.2736) acc_x 65.6250 (70.9375) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [25/26] time 0.558 (0.459) data 0.427 (0.328) loss_x loss_x 1.3994 (1.2986) acc_x 50.0000 (69.1250) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/71] time 0.460 (0.463) data 0.330 (0.332) loss_u loss_u 0.9204 (0.8592) acc_u 6.2500 (16.8750) lr 3.3869e-04 eta 0:00:30
epoch [148/200] batch [10/71] time 0.441 (0.459) data 0.311 (0.328) loss_u loss_u 0.9097 (0.8743) acc_u 12.5000 (15.3125) lr 3.3869e-04 eta 0:00:27
epoch [148/200] batch [15/71] time 0.457 (0.451) data 0.325 (0.320) loss_u loss_u 0.7549 (0.8724) acc_u 34.3750 (16.2500) lr 3.3869e-04 eta 0:00:25
epoch [148/200] batch [20/71] time 0.414 (0.446) data 0.283 (0.315) loss_u loss_u 0.8477 (0.8772) acc_u 18.7500 (15.0000) lr 3.3869e-04 eta 0:00:22
epoch [148/200] batch [25/71] time 0.469 (0.446) data 0.337 (0.315) loss_u loss_u 0.8799 (0.8765) acc_u 15.6250 (15.3750) lr 3.3869e-04 eta 0:00:20
epoch [148/200] batch [30/71] time 0.588 (0.451) data 0.457 (0.320) loss_u loss_u 0.8623 (0.8781) acc_u 18.7500 (15.0000) lr 3.3869e-04 eta 0:00:18
epoch [148/200] batch [35/71] time 0.389 (0.455) data 0.256 (0.324) loss_u loss_u 0.9189 (0.8794) acc_u 9.3750 (15.0893) lr 3.3869e-04 eta 0:00:16
epoch [148/200] batch [40/71] time 0.421 (0.456) data 0.290 (0.325) loss_u loss_u 0.8779 (0.8850) acc_u 12.5000 (14.0625) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [45/71] time 0.539 (0.457) data 0.408 (0.325) loss_u loss_u 0.8179 (0.8845) acc_u 21.8750 (14.3056) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [50/71] time 0.499 (0.459) data 0.368 (0.327) loss_u loss_u 0.8931 (0.8840) acc_u 15.6250 (14.5000) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [55/71] time 0.617 (0.458) data 0.483 (0.327) loss_u loss_u 0.9473 (0.8869) acc_u 6.2500 (14.0909) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [60/71] time 0.452 (0.458) data 0.321 (0.327) loss_u loss_u 0.8472 (0.8876) acc_u 18.7500 (13.9583) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [65/71] time 0.453 (0.458) data 0.321 (0.327) loss_u loss_u 0.8901 (0.8883) acc_u 12.5000 (13.9904) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [70/71] time 0.492 (0.462) data 0.360 (0.331) loss_u loss_u 0.8711 (0.8901) acc_u 15.6250 (13.6607) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2113
confident_label rate tensor(0.2596, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 814
clean true:365
clean false:449
clean_rate:0.4484029484029484
noisy true:658
noisy false:1664
after delete: len(clean_dataset) 814
after delete: len(noisy_dataset) 2322
epoch [149/200] batch [5/25] time 0.462 (0.451) data 0.331 (0.320) loss_x loss_x 0.8506 (1.3219) acc_x 78.1250 (66.8750) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [10/25] time 0.396 (0.429) data 0.265 (0.298) loss_x loss_x 1.3154 (1.2877) acc_x 50.0000 (66.2500) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [15/25] time 0.493 (0.456) data 0.362 (0.325) loss_x loss_x 1.3076 (1.3630) acc_x 68.7500 (65.4167) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [20/25] time 0.583 (0.462) data 0.453 (0.331) loss_x loss_x 1.2402 (1.3614) acc_x 71.8750 (66.7188) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [25/25] time 0.435 (0.458) data 0.305 (0.327) loss_x loss_x 0.7915 (1.3335) acc_x 78.1250 (66.8750) lr 3.2699e-04 eta 0:00:00
epoch [149/200] batch [5/72] time 0.505 (0.461) data 0.374 (0.330) loss_u loss_u 0.9668 (0.8983) acc_u 6.2500 (11.8750) lr 3.2699e-04 eta 0:00:30
epoch [149/200] batch [10/72] time 0.427 (0.463) data 0.295 (0.332) loss_u loss_u 0.8770 (0.8833) acc_u 12.5000 (13.7500) lr 3.2699e-04 eta 0:00:28
epoch [149/200] batch [15/72] time 0.651 (0.459) data 0.520 (0.328) loss_u loss_u 0.8530 (0.8874) acc_u 18.7500 (13.7500) lr 3.2699e-04 eta 0:00:26
epoch [149/200] batch [20/72] time 0.368 (0.452) data 0.236 (0.321) loss_u loss_u 0.9175 (0.8902) acc_u 15.6250 (14.0625) lr 3.2699e-04 eta 0:00:23
epoch [149/200] batch [25/72] time 0.422 (0.457) data 0.291 (0.326) loss_u loss_u 0.8574 (0.8844) acc_u 12.5000 (14.7500) lr 3.2699e-04 eta 0:00:21
epoch [149/200] batch [30/72] time 0.524 (0.461) data 0.393 (0.330) loss_u loss_u 0.9336 (0.8898) acc_u 12.5000 (14.4792) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [35/72] time 0.453 (0.462) data 0.322 (0.330) loss_u loss_u 0.8042 (0.8881) acc_u 25.0000 (14.6429) lr 3.2699e-04 eta 0:00:17
epoch [149/200] batch [40/72] time 0.360 (0.463) data 0.228 (0.332) loss_u loss_u 0.9399 (0.8910) acc_u 6.2500 (14.2188) lr 3.2699e-04 eta 0:00:14
epoch [149/200] batch [45/72] time 0.650 (0.463) data 0.518 (0.332) loss_u loss_u 0.9268 (0.8927) acc_u 9.3750 (13.8889) lr 3.2699e-04 eta 0:00:12
epoch [149/200] batch [50/72] time 0.532 (0.466) data 0.401 (0.334) loss_u loss_u 0.9312 (0.8941) acc_u 9.3750 (13.5625) lr 3.2699e-04 eta 0:00:10
epoch [149/200] batch [55/72] time 0.410 (0.463) data 0.279 (0.332) loss_u loss_u 0.8726 (0.8923) acc_u 18.7500 (13.9205) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [60/72] time 0.421 (0.463) data 0.289 (0.331) loss_u loss_u 0.9092 (0.8933) acc_u 9.3750 (13.8021) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [65/72] time 0.483 (0.462) data 0.351 (0.331) loss_u loss_u 0.9014 (0.8909) acc_u 15.6250 (14.2308) lr 3.2699e-04 eta 0:00:03
epoch [149/200] batch [70/72] time 0.430 (0.463) data 0.298 (0.332) loss_u loss_u 0.9067 (0.8893) acc_u 12.5000 (14.5089) lr 3.2699e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2083
confident_label rate tensor(0.2771, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 869
clean true:383
clean false:486
clean_rate:0.4407364787111623
noisy true:670
noisy false:1597
after delete: len(clean_dataset) 869
after delete: len(noisy_dataset) 2267
epoch [150/200] batch [5/27] time 0.461 (0.469) data 0.330 (0.338) loss_x loss_x 1.1895 (1.1714) acc_x 71.8750 (67.5000) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [10/27] time 0.352 (0.465) data 0.221 (0.334) loss_x loss_x 1.2197 (1.2628) acc_x 71.8750 (68.4375) lr 3.1545e-04 eta 0:00:07
epoch [150/200] batch [15/27] time 0.453 (0.461) data 0.322 (0.330) loss_x loss_x 1.6777 (1.2802) acc_x 56.2500 (67.2917) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [20/27] time 0.424 (0.470) data 0.294 (0.340) loss_x loss_x 1.1562 (1.2525) acc_x 75.0000 (68.4375) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [25/27] time 0.506 (0.467) data 0.375 (0.336) loss_x loss_x 1.1797 (1.2492) acc_x 68.7500 (67.8750) lr 3.1545e-04 eta 0:00:00
epoch [150/200] batch [5/70] time 0.467 (0.456) data 0.336 (0.326) loss_u loss_u 0.9775 (0.9210) acc_u 0.0000 (10.6250) lr 3.1545e-04 eta 0:00:29
epoch [150/200] batch [10/70] time 0.502 (0.455) data 0.370 (0.324) loss_u loss_u 0.9707 (0.9184) acc_u 3.1250 (10.6250) lr 3.1545e-04 eta 0:00:27
epoch [150/200] batch [15/70] time 0.396 (0.456) data 0.264 (0.325) loss_u loss_u 0.9180 (0.9127) acc_u 12.5000 (11.2500) lr 3.1545e-04 eta 0:00:25
epoch [150/200] batch [20/70] time 0.397 (0.455) data 0.265 (0.324) loss_u loss_u 0.9297 (0.9078) acc_u 9.3750 (11.8750) lr 3.1545e-04 eta 0:00:22
epoch [150/200] batch [25/70] time 0.446 (0.467) data 0.315 (0.336) loss_u loss_u 0.8823 (0.9069) acc_u 18.7500 (12.2500) lr 3.1545e-04 eta 0:00:21
epoch [150/200] batch [30/70] time 0.426 (0.466) data 0.294 (0.334) loss_u loss_u 0.8711 (0.9074) acc_u 25.0000 (12.3958) lr 3.1545e-04 eta 0:00:18
epoch [150/200] batch [35/70] time 0.350 (0.468) data 0.218 (0.337) loss_u loss_u 0.9844 (0.9067) acc_u 0.0000 (12.2321) lr 3.1545e-04 eta 0:00:16
epoch [150/200] batch [40/70] time 0.432 (0.467) data 0.301 (0.336) loss_u loss_u 0.8887 (0.9064) acc_u 15.6250 (12.1094) lr 3.1545e-04 eta 0:00:14
epoch [150/200] batch [45/70] time 0.378 (0.467) data 0.247 (0.336) loss_u loss_u 0.9097 (0.9071) acc_u 9.3750 (11.9444) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [50/70] time 0.434 (0.470) data 0.303 (0.339) loss_u loss_u 0.8804 (0.9038) acc_u 15.6250 (12.3750) lr 3.1545e-04 eta 0:00:09
epoch [150/200] batch [55/70] time 0.366 (0.467) data 0.235 (0.335) loss_u loss_u 0.9155 (0.9038) acc_u 12.5000 (12.4432) lr 3.1545e-04 eta 0:00:07
epoch [150/200] batch [60/70] time 0.372 (0.463) data 0.241 (0.332) loss_u loss_u 0.9243 (0.9041) acc_u 6.2500 (12.2917) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [65/70] time 0.498 (0.463) data 0.367 (0.332) loss_u loss_u 0.9121 (0.9047) acc_u 9.3750 (12.2115) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [70/70] time 0.558 (0.467) data 0.425 (0.336) loss_u loss_u 0.8784 (0.9057) acc_u 18.7500 (12.1875) lr 3.1545e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2086
confident_label rate tensor(0.2640, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 828
clean true:375
clean false:453
clean_rate:0.4528985507246377
noisy true:675
noisy false:1633
after delete: len(clean_dataset) 828
after delete: len(noisy_dataset) 2308
epoch [151/200] batch [5/25] time 0.398 (0.435) data 0.266 (0.304) loss_x loss_x 1.3984 (1.0234) acc_x 68.7500 (73.7500) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [10/25] time 0.557 (0.469) data 0.427 (0.338) loss_x loss_x 0.8955 (1.0668) acc_x 75.0000 (70.6250) lr 3.0409e-04 eta 0:00:07
epoch [151/200] batch [15/25] time 0.590 (0.473) data 0.459 (0.342) loss_x loss_x 2.0078 (1.1836) acc_x 68.7500 (69.7917) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [20/25] time 0.439 (0.475) data 0.308 (0.344) loss_x loss_x 1.7217 (1.2514) acc_x 53.1250 (68.1250) lr 3.0409e-04 eta 0:00:02
epoch [151/200] batch [25/25] time 0.672 (0.474) data 0.541 (0.343) loss_x loss_x 1.2461 (1.2189) acc_x 75.0000 (68.7500) lr 3.0409e-04 eta 0:00:00
epoch [151/200] batch [5/72] time 0.406 (0.476) data 0.274 (0.345) loss_u loss_u 0.7949 (0.8745) acc_u 28.1250 (16.8750) lr 3.0409e-04 eta 0:00:31
epoch [151/200] batch [10/72] time 0.399 (0.474) data 0.267 (0.343) loss_u loss_u 0.7856 (0.8600) acc_u 25.0000 (19.3750) lr 3.0409e-04 eta 0:00:29
epoch [151/200] batch [15/72] time 0.413 (0.485) data 0.281 (0.354) loss_u loss_u 0.8428 (0.8680) acc_u 21.8750 (17.9167) lr 3.0409e-04 eta 0:00:27
epoch [151/200] batch [20/72] time 0.530 (0.486) data 0.398 (0.355) loss_u loss_u 0.8423 (0.8762) acc_u 21.8750 (16.7188) lr 3.0409e-04 eta 0:00:25
epoch [151/200] batch [25/72] time 0.522 (0.486) data 0.391 (0.355) loss_u loss_u 0.8267 (0.8721) acc_u 18.7500 (16.8750) lr 3.0409e-04 eta 0:00:22
epoch [151/200] batch [30/72] time 0.428 (0.481) data 0.296 (0.350) loss_u loss_u 0.8311 (0.8728) acc_u 25.0000 (16.5625) lr 3.0409e-04 eta 0:00:20
epoch [151/200] batch [35/72] time 0.434 (0.481) data 0.303 (0.349) loss_u loss_u 0.7793 (0.8747) acc_u 28.1250 (16.0714) lr 3.0409e-04 eta 0:00:17
epoch [151/200] batch [40/72] time 0.574 (0.482) data 0.443 (0.351) loss_u loss_u 0.8843 (0.8776) acc_u 12.5000 (15.7031) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [45/72] time 0.469 (0.483) data 0.338 (0.351) loss_u loss_u 0.9106 (0.8744) acc_u 9.3750 (15.9028) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [50/72] time 0.393 (0.478) data 0.262 (0.347) loss_u loss_u 0.8594 (0.8763) acc_u 25.0000 (15.6250) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [55/72] time 0.435 (0.478) data 0.303 (0.347) loss_u loss_u 0.7764 (0.8761) acc_u 28.1250 (15.7386) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [60/72] time 0.435 (0.476) data 0.302 (0.344) loss_u loss_u 0.8354 (0.8752) acc_u 15.6250 (15.7812) lr 3.0409e-04 eta 0:00:05
epoch [151/200] batch [65/72] time 0.380 (0.474) data 0.249 (0.343) loss_u loss_u 0.8306 (0.8755) acc_u 21.8750 (15.7692) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [70/72] time 0.452 (0.472) data 0.320 (0.341) loss_u loss_u 0.8438 (0.8779) acc_u 18.7500 (15.5804) lr 3.0409e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2080
confident_label rate tensor(0.2720, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 853
clean true:385
clean false:468
clean_rate:0.451348182883939
noisy true:671
noisy false:1612
after delete: len(clean_dataset) 853
after delete: len(noisy_dataset) 2283
epoch [152/200] batch [5/26] time 0.444 (0.450) data 0.314 (0.320) loss_x loss_x 0.9219 (1.2113) acc_x 78.1250 (67.5000) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [10/26] time 0.494 (0.458) data 0.364 (0.327) loss_x loss_x 1.4570 (1.2144) acc_x 68.7500 (68.7500) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [15/26] time 0.564 (0.461) data 0.434 (0.331) loss_x loss_x 0.6133 (1.1144) acc_x 84.3750 (70.8333) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [20/26] time 0.492 (0.458) data 0.361 (0.328) loss_x loss_x 1.4043 (1.0882) acc_x 65.6250 (71.0938) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [25/26] time 0.424 (0.454) data 0.293 (0.324) loss_x loss_x 0.9082 (1.0827) acc_x 78.1250 (71.1250) lr 2.9289e-04 eta 0:00:00
epoch [152/200] batch [5/71] time 0.362 (0.443) data 0.231 (0.313) loss_u loss_u 0.8921 (0.9069) acc_u 12.5000 (10.0000) lr 2.9289e-04 eta 0:00:29
epoch [152/200] batch [10/71] time 0.625 (0.448) data 0.494 (0.317) loss_u loss_u 0.9307 (0.9043) acc_u 6.2500 (11.2500) lr 2.9289e-04 eta 0:00:27
epoch [152/200] batch [15/71] time 0.520 (0.448) data 0.389 (0.317) loss_u loss_u 0.9199 (0.9034) acc_u 12.5000 (11.2500) lr 2.9289e-04 eta 0:00:25
epoch [152/200] batch [20/71] time 0.481 (0.462) data 0.350 (0.332) loss_u loss_u 0.9346 (0.9042) acc_u 6.2500 (10.9375) lr 2.9289e-04 eta 0:00:23
epoch [152/200] batch [25/71] time 0.524 (0.460) data 0.392 (0.329) loss_u loss_u 0.9326 (0.9065) acc_u 6.2500 (10.2500) lr 2.9289e-04 eta 0:00:21
epoch [152/200] batch [30/71] time 0.528 (0.464) data 0.396 (0.334) loss_u loss_u 0.8003 (0.8981) acc_u 28.1250 (11.8750) lr 2.9289e-04 eta 0:00:19
epoch [152/200] batch [35/71] time 0.433 (0.473) data 0.303 (0.342) loss_u loss_u 0.8955 (0.8956) acc_u 15.6250 (12.4107) lr 2.9289e-04 eta 0:00:17
epoch [152/200] batch [40/71] time 0.513 (0.469) data 0.382 (0.339) loss_u loss_u 0.9014 (0.8965) acc_u 12.5000 (12.1875) lr 2.9289e-04 eta 0:00:14
epoch [152/200] batch [45/71] time 0.373 (0.469) data 0.242 (0.339) loss_u loss_u 0.8428 (0.8949) acc_u 25.0000 (12.7778) lr 2.9289e-04 eta 0:00:12
epoch [152/200] batch [50/71] time 0.409 (0.466) data 0.278 (0.336) loss_u loss_u 0.9106 (0.8953) acc_u 9.3750 (12.9375) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [55/71] time 0.407 (0.465) data 0.275 (0.334) loss_u loss_u 0.9243 (0.8962) acc_u 12.5000 (12.8977) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [60/71] time 0.382 (0.467) data 0.250 (0.336) loss_u loss_u 0.8647 (0.8966) acc_u 15.6250 (12.9167) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [65/71] time 0.474 (0.469) data 0.344 (0.338) loss_u loss_u 0.9175 (0.8963) acc_u 12.5000 (12.9808) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [70/71] time 0.432 (0.467) data 0.300 (0.337) loss_u loss_u 0.8799 (0.8951) acc_u 15.6250 (13.2589) lr 2.9289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2086
confident_label rate tensor(0.2720, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 853
clean true:375
clean false:478
clean_rate:0.4396248534583822
noisy true:675
noisy false:1608
after delete: len(clean_dataset) 853
after delete: len(noisy_dataset) 2283
epoch [153/200] batch [5/26] time 0.407 (0.451) data 0.277 (0.320) loss_x loss_x 1.1445 (1.0845) acc_x 75.0000 (75.6250) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [10/26] time 0.489 (0.447) data 0.358 (0.316) loss_x loss_x 1.1875 (1.0434) acc_x 78.1250 (76.5625) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [15/26] time 0.514 (0.444) data 0.383 (0.313) loss_x loss_x 1.1426 (1.0575) acc_x 59.3750 (73.7500) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [20/26] time 0.397 (0.458) data 0.266 (0.327) loss_x loss_x 1.1787 (1.1171) acc_x 84.3750 (72.8125) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [25/26] time 0.348 (0.456) data 0.215 (0.325) loss_x loss_x 1.4443 (1.1477) acc_x 65.6250 (72.7500) lr 2.8187e-04 eta 0:00:00
epoch [153/200] batch [5/71] time 0.409 (0.449) data 0.278 (0.318) loss_u loss_u 0.9307 (0.8703) acc_u 6.2500 (15.6250) lr 2.8187e-04 eta 0:00:29
epoch [153/200] batch [10/71] time 0.598 (0.456) data 0.466 (0.325) loss_u loss_u 0.8555 (0.8739) acc_u 15.6250 (15.0000) lr 2.8187e-04 eta 0:00:27
epoch [153/200] batch [15/71] time 0.478 (0.454) data 0.346 (0.323) loss_u loss_u 0.8286 (0.8795) acc_u 15.6250 (14.1667) lr 2.8187e-04 eta 0:00:25
epoch [153/200] batch [20/71] time 0.353 (0.455) data 0.221 (0.324) loss_u loss_u 0.8608 (0.8815) acc_u 15.6250 (14.0625) lr 2.8187e-04 eta 0:00:23
epoch [153/200] batch [25/71] time 0.528 (0.457) data 0.398 (0.326) loss_u loss_u 0.8506 (0.8823) acc_u 31.2500 (14.7500) lr 2.8187e-04 eta 0:00:21
epoch [153/200] batch [30/71] time 0.650 (0.464) data 0.519 (0.333) loss_u loss_u 0.7710 (0.8836) acc_u 28.1250 (14.5833) lr 2.8187e-04 eta 0:00:19
epoch [153/200] batch [35/71] time 0.453 (0.464) data 0.321 (0.333) loss_u loss_u 0.9614 (0.8887) acc_u 3.1250 (13.6607) lr 2.8187e-04 eta 0:00:16
epoch [153/200] batch [40/71] time 0.515 (0.466) data 0.384 (0.335) loss_u loss_u 0.8550 (0.8880) acc_u 18.7500 (13.7500) lr 2.8187e-04 eta 0:00:14
epoch [153/200] batch [45/71] time 0.486 (0.467) data 0.355 (0.336) loss_u loss_u 0.8804 (0.8879) acc_u 15.6250 (14.0278) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [50/71] time 0.371 (0.468) data 0.240 (0.337) loss_u loss_u 0.8184 (0.8887) acc_u 21.8750 (13.8125) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [55/71] time 0.506 (0.468) data 0.374 (0.337) loss_u loss_u 0.9336 (0.8880) acc_u 6.2500 (14.0341) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [60/71] time 0.415 (0.466) data 0.283 (0.335) loss_u loss_u 0.9316 (0.8899) acc_u 9.3750 (13.6458) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [65/71] time 0.429 (0.464) data 0.298 (0.333) loss_u loss_u 0.8589 (0.8890) acc_u 21.8750 (13.8942) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [70/71] time 0.435 (0.461) data 0.303 (0.330) loss_u loss_u 0.8530 (0.8891) acc_u 21.8750 (14.0179) lr 2.8187e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2081
confident_label rate tensor(0.2752, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 863
clean true:392
clean false:471
clean_rate:0.4542294322132097
noisy true:663
noisy false:1610
after delete: len(clean_dataset) 863
after delete: len(noisy_dataset) 2273
epoch [154/200] batch [5/26] time 0.428 (0.471) data 0.297 (0.340) loss_x loss_x 1.3682 (1.3547) acc_x 56.2500 (66.2500) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [10/26] time 0.491 (0.488) data 0.361 (0.357) loss_x loss_x 1.0781 (1.2792) acc_x 78.1250 (68.1250) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [15/26] time 0.462 (0.481) data 0.332 (0.350) loss_x loss_x 1.2676 (1.2621) acc_x 65.6250 (67.9167) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [20/26] time 0.589 (0.499) data 0.458 (0.368) loss_x loss_x 1.0039 (1.2044) acc_x 71.8750 (70.0000) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [25/26] time 0.491 (0.493) data 0.362 (0.363) loss_x loss_x 1.4316 (1.2432) acc_x 59.3750 (69.1250) lr 2.7103e-04 eta 0:00:00
epoch [154/200] batch [5/71] time 0.487 (0.494) data 0.356 (0.363) loss_u loss_u 0.8823 (0.8808) acc_u 15.6250 (16.8750) lr 2.7103e-04 eta 0:00:32
epoch [154/200] batch [10/71] time 0.496 (0.484) data 0.364 (0.353) loss_u loss_u 0.9150 (0.8824) acc_u 12.5000 (16.2500) lr 2.7103e-04 eta 0:00:29
epoch [154/200] batch [15/71] time 0.545 (0.486) data 0.414 (0.355) loss_u loss_u 0.8799 (0.8852) acc_u 18.7500 (15.4167) lr 2.7103e-04 eta 0:00:27
epoch [154/200] batch [20/71] time 0.567 (0.489) data 0.435 (0.358) loss_u loss_u 0.8965 (0.8900) acc_u 15.6250 (14.6875) lr 2.7103e-04 eta 0:00:24
epoch [154/200] batch [25/71] time 0.358 (0.480) data 0.227 (0.349) loss_u loss_u 0.9229 (0.8922) acc_u 6.2500 (14.2500) lr 2.7103e-04 eta 0:00:22
epoch [154/200] batch [30/71] time 0.350 (0.473) data 0.218 (0.342) loss_u loss_u 0.8652 (0.8900) acc_u 15.6250 (14.3750) lr 2.7103e-04 eta 0:00:19
epoch [154/200] batch [35/71] time 0.781 (0.475) data 0.649 (0.344) loss_u loss_u 0.8281 (0.8922) acc_u 18.7500 (14.0179) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [40/71] time 0.465 (0.472) data 0.333 (0.341) loss_u loss_u 0.9185 (0.8954) acc_u 9.3750 (13.5156) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [45/71] time 0.613 (0.475) data 0.482 (0.344) loss_u loss_u 0.9404 (0.8990) acc_u 6.2500 (13.1944) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [50/71] time 0.403 (0.474) data 0.271 (0.343) loss_u loss_u 0.9604 (0.8998) acc_u 3.1250 (13.0000) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [55/71] time 0.374 (0.472) data 0.243 (0.341) loss_u loss_u 0.8081 (0.8987) acc_u 28.1250 (13.2386) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [60/71] time 0.438 (0.470) data 0.307 (0.338) loss_u loss_u 0.9058 (0.8970) acc_u 12.5000 (13.5417) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [65/71] time 0.543 (0.475) data 0.411 (0.344) loss_u loss_u 0.9492 (0.8970) acc_u 6.2500 (13.5577) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [70/71] time 0.338 (0.470) data 0.207 (0.339) loss_u loss_u 0.9697 (0.8972) acc_u 3.1250 (13.6607) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2110
confident_label rate tensor(0.2637, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 827
clean true:365
clean false:462
clean_rate:0.44135429262394194
noisy true:661
noisy false:1648
after delete: len(clean_dataset) 827
after delete: len(noisy_dataset) 2309
epoch [155/200] batch [5/25] time 0.599 (0.494) data 0.468 (0.362) loss_x loss_x 1.1914 (1.0800) acc_x 75.0000 (74.3750) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [10/25] time 0.595 (0.481) data 0.464 (0.350) loss_x loss_x 1.1143 (1.0496) acc_x 65.6250 (74.6875) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [15/25] time 0.548 (0.471) data 0.418 (0.340) loss_x loss_x 0.7700 (1.0763) acc_x 78.1250 (73.5417) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [20/25] time 0.578 (0.457) data 0.448 (0.326) loss_x loss_x 0.8145 (1.1282) acc_x 81.2500 (73.4375) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [25/25] time 0.433 (0.453) data 0.302 (0.323) loss_x loss_x 1.4160 (1.1310) acc_x 62.5000 (72.3750) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/72] time 0.414 (0.452) data 0.283 (0.321) loss_u loss_u 0.8677 (0.8928) acc_u 15.6250 (14.3750) lr 2.6037e-04 eta 0:00:30
epoch [155/200] batch [10/72] time 0.486 (0.460) data 0.355 (0.330) loss_u loss_u 0.8945 (0.8730) acc_u 12.5000 (17.8125) lr 2.6037e-04 eta 0:00:28
epoch [155/200] batch [15/72] time 0.440 (0.463) data 0.309 (0.332) loss_u loss_u 0.8481 (0.8750) acc_u 21.8750 (17.0833) lr 2.6037e-04 eta 0:00:26
epoch [155/200] batch [20/72] time 0.621 (0.465) data 0.490 (0.334) loss_u loss_u 0.8760 (0.8749) acc_u 18.7500 (17.0312) lr 2.6037e-04 eta 0:00:24
epoch [155/200] batch [25/72] time 0.375 (0.461) data 0.244 (0.330) loss_u loss_u 0.8818 (0.8785) acc_u 12.5000 (16.5000) lr 2.6037e-04 eta 0:00:21
epoch [155/200] batch [30/72] time 0.445 (0.462) data 0.314 (0.331) loss_u loss_u 0.9009 (0.8803) acc_u 15.6250 (16.3542) lr 2.6037e-04 eta 0:00:19
epoch [155/200] batch [35/72] time 0.401 (0.459) data 0.269 (0.328) loss_u loss_u 0.9194 (0.8813) acc_u 9.3750 (16.1607) lr 2.6037e-04 eta 0:00:17
epoch [155/200] batch [40/72] time 0.384 (0.457) data 0.252 (0.326) loss_u loss_u 0.9351 (0.8882) acc_u 6.2500 (15.0000) lr 2.6037e-04 eta 0:00:14
epoch [155/200] batch [45/72] time 0.421 (0.457) data 0.289 (0.326) loss_u loss_u 0.9155 (0.8857) acc_u 6.2500 (15.1389) lr 2.6037e-04 eta 0:00:12
epoch [155/200] batch [50/72] time 0.412 (0.462) data 0.281 (0.331) loss_u loss_u 0.8486 (0.8878) acc_u 18.7500 (14.5625) lr 2.6037e-04 eta 0:00:10
epoch [155/200] batch [55/72] time 0.493 (0.460) data 0.362 (0.329) loss_u loss_u 0.8770 (0.8874) acc_u 15.6250 (14.4886) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [60/72] time 0.520 (0.463) data 0.388 (0.332) loss_u loss_u 0.9131 (0.8874) acc_u 9.3750 (14.5833) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [65/72] time 0.577 (0.462) data 0.446 (0.331) loss_u loss_u 0.9258 (0.8893) acc_u 9.3750 (14.3750) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [70/72] time 0.466 (0.464) data 0.334 (0.333) loss_u loss_u 0.9067 (0.8883) acc_u 6.2500 (14.3304) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2091
confident_label rate tensor(0.2653, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 832
clean true:366
clean false:466
clean_rate:0.43990384615384615
noisy true:679
noisy false:1625
after delete: len(clean_dataset) 832
after delete: len(noisy_dataset) 2304
epoch [156/200] batch [5/26] time 0.441 (0.493) data 0.308 (0.362) loss_x loss_x 1.3379 (1.1877) acc_x 59.3750 (70.0000) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [10/26] time 0.476 (0.492) data 0.345 (0.361) loss_x loss_x 0.5845 (1.2143) acc_x 84.3750 (68.7500) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [15/26] time 0.361 (0.465) data 0.231 (0.334) loss_x loss_x 1.2637 (1.1665) acc_x 65.6250 (69.3750) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [20/26] time 0.510 (0.472) data 0.379 (0.341) loss_x loss_x 1.1660 (1.1508) acc_x 75.0000 (69.6875) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [25/26] time 0.654 (0.470) data 0.524 (0.339) loss_x loss_x 0.9634 (1.1272) acc_x 71.8750 (70.2500) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/72] time 0.357 (0.457) data 0.227 (0.327) loss_u loss_u 0.8984 (0.9184) acc_u 9.3750 (10.0000) lr 2.4989e-04 eta 0:00:30
epoch [156/200] batch [10/72] time 0.638 (0.459) data 0.507 (0.328) loss_u loss_u 0.8936 (0.8946) acc_u 15.6250 (13.7500) lr 2.4989e-04 eta 0:00:28
epoch [156/200] batch [15/72] time 0.421 (0.454) data 0.291 (0.323) loss_u loss_u 0.8818 (0.8930) acc_u 15.6250 (13.9583) lr 2.4989e-04 eta 0:00:25
epoch [156/200] batch [20/72] time 0.466 (0.457) data 0.336 (0.326) loss_u loss_u 0.8359 (0.8833) acc_u 18.7500 (15.1562) lr 2.4989e-04 eta 0:00:23
epoch [156/200] batch [25/72] time 0.448 (0.455) data 0.318 (0.324) loss_u loss_u 0.9453 (0.8867) acc_u 3.1250 (14.6250) lr 2.4989e-04 eta 0:00:21
epoch [156/200] batch [30/72] time 0.662 (0.458) data 0.531 (0.327) loss_u loss_u 0.9316 (0.8947) acc_u 3.1250 (13.4375) lr 2.4989e-04 eta 0:00:19
epoch [156/200] batch [35/72] time 0.384 (0.455) data 0.252 (0.324) loss_u loss_u 0.8442 (0.8948) acc_u 18.7500 (13.4821) lr 2.4989e-04 eta 0:00:16
epoch [156/200] batch [40/72] time 0.382 (0.452) data 0.251 (0.321) loss_u loss_u 0.9009 (0.8950) acc_u 12.5000 (13.5156) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [45/72] time 0.526 (0.453) data 0.393 (0.322) loss_u loss_u 0.9336 (0.8984) acc_u 9.3750 (12.9861) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [50/72] time 0.456 (0.452) data 0.325 (0.321) loss_u loss_u 0.8643 (0.8978) acc_u 21.8750 (13.1875) lr 2.4989e-04 eta 0:00:09
epoch [156/200] batch [55/72] time 0.368 (0.454) data 0.237 (0.323) loss_u loss_u 0.8936 (0.8956) acc_u 15.6250 (13.4659) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [60/72] time 0.491 (0.455) data 0.360 (0.324) loss_u loss_u 0.8706 (0.8959) acc_u 18.7500 (13.4896) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [65/72] time 0.381 (0.454) data 0.249 (0.323) loss_u loss_u 0.8838 (0.8939) acc_u 12.5000 (13.6058) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [70/72] time 0.591 (0.460) data 0.460 (0.329) loss_u loss_u 0.8711 (0.8935) acc_u 15.6250 (13.5268) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2121
confident_label rate tensor(0.2704, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 848
clean true:377
clean false:471
clean_rate:0.44457547169811323
noisy true:638
noisy false:1650
after delete: len(clean_dataset) 848
after delete: len(noisy_dataset) 2288
epoch [157/200] batch [5/26] time 0.481 (0.459) data 0.351 (0.327) loss_x loss_x 1.8584 (1.4564) acc_x 59.3750 (67.5000) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [10/26] time 0.386 (0.455) data 0.254 (0.324) loss_x loss_x 1.4238 (1.2876) acc_x 65.6250 (67.8125) lr 2.3959e-04 eta 0:00:07
epoch [157/200] batch [15/26] time 0.576 (0.485) data 0.444 (0.354) loss_x loss_x 1.2461 (1.2263) acc_x 62.5000 (68.7500) lr 2.3959e-04 eta 0:00:05
epoch [157/200] batch [20/26] time 0.430 (0.477) data 0.300 (0.346) loss_x loss_x 1.1064 (1.1943) acc_x 68.7500 (69.8438) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [25/26] time 0.505 (0.470) data 0.374 (0.339) loss_x loss_x 1.5693 (1.2171) acc_x 50.0000 (69.0000) lr 2.3959e-04 eta 0:00:00
epoch [157/200] batch [5/71] time 0.390 (0.460) data 0.259 (0.329) loss_u loss_u 0.9224 (0.8945) acc_u 12.5000 (11.8750) lr 2.3959e-04 eta 0:00:30
epoch [157/200] batch [10/71] time 0.471 (0.461) data 0.338 (0.329) loss_u loss_u 0.8276 (0.8925) acc_u 21.8750 (14.0625) lr 2.3959e-04 eta 0:00:28
epoch [157/200] batch [15/71] time 0.623 (0.462) data 0.491 (0.331) loss_u loss_u 0.9072 (0.9018) acc_u 9.3750 (12.5000) lr 2.3959e-04 eta 0:00:25
epoch [157/200] batch [20/71] time 0.471 (0.458) data 0.339 (0.327) loss_u loss_u 0.8965 (0.8895) acc_u 15.6250 (14.0625) lr 2.3959e-04 eta 0:00:23
epoch [157/200] batch [25/71] time 0.406 (0.459) data 0.274 (0.327) loss_u loss_u 0.9126 (0.8940) acc_u 9.3750 (13.3750) lr 2.3959e-04 eta 0:00:21
epoch [157/200] batch [30/71] time 0.382 (0.461) data 0.251 (0.330) loss_u loss_u 0.8579 (0.8882) acc_u 18.7500 (13.8542) lr 2.3959e-04 eta 0:00:18
epoch [157/200] batch [35/71] time 0.482 (0.460) data 0.351 (0.329) loss_u loss_u 0.9023 (0.8894) acc_u 9.3750 (13.7500) lr 2.3959e-04 eta 0:00:16
epoch [157/200] batch [40/71] time 0.372 (0.455) data 0.240 (0.324) loss_u loss_u 0.8867 (0.8898) acc_u 18.7500 (13.7500) lr 2.3959e-04 eta 0:00:14
epoch [157/200] batch [45/71] time 0.546 (0.455) data 0.415 (0.324) loss_u loss_u 0.9810 (0.8905) acc_u 0.0000 (13.6806) lr 2.3959e-04 eta 0:00:11
epoch [157/200] batch [50/71] time 0.457 (0.456) data 0.326 (0.325) loss_u loss_u 0.8491 (0.8912) acc_u 18.7500 (13.6250) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [55/71] time 0.446 (0.458) data 0.316 (0.326) loss_u loss_u 0.9263 (0.8931) acc_u 12.5000 (13.5227) lr 2.3959e-04 eta 0:00:07
epoch [157/200] batch [60/71] time 0.356 (0.460) data 0.225 (0.328) loss_u loss_u 0.9058 (0.8924) acc_u 12.5000 (13.7500) lr 2.3959e-04 eta 0:00:05
epoch [157/200] batch [65/71] time 0.389 (0.458) data 0.258 (0.326) loss_u loss_u 0.8765 (0.8946) acc_u 18.7500 (13.5096) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [70/71] time 0.448 (0.457) data 0.318 (0.326) loss_u loss_u 0.8921 (0.8960) acc_u 15.6250 (13.3929) lr 2.3959e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2070
confident_label rate tensor(0.2787, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 874
clean true:402
clean false:472
clean_rate:0.459954233409611
noisy true:664
noisy false:1598
after delete: len(clean_dataset) 874
after delete: len(noisy_dataset) 2262
epoch [158/200] batch [5/27] time 0.484 (0.483) data 0.352 (0.351) loss_x loss_x 1.4336 (1.1450) acc_x 59.3750 (68.1250) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [10/27] time 0.460 (0.463) data 0.329 (0.332) loss_x loss_x 1.2236 (1.1650) acc_x 59.3750 (68.7500) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [15/27] time 0.468 (0.458) data 0.337 (0.327) loss_x loss_x 1.6221 (1.1996) acc_x 59.3750 (68.9583) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [20/27] time 0.370 (0.464) data 0.239 (0.333) loss_x loss_x 0.8926 (1.1222) acc_x 78.1250 (70.0000) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [25/27] time 0.592 (0.473) data 0.461 (0.342) loss_x loss_x 1.2334 (1.1666) acc_x 68.7500 (69.0000) lr 2.2949e-04 eta 0:00:00
epoch [158/200] batch [5/70] time 0.484 (0.465) data 0.352 (0.334) loss_u loss_u 0.9248 (0.9271) acc_u 9.3750 (10.0000) lr 2.2949e-04 eta 0:00:30
epoch [158/200] batch [10/70] time 0.453 (0.455) data 0.322 (0.324) loss_u loss_u 0.8823 (0.8938) acc_u 12.5000 (13.1250) lr 2.2949e-04 eta 0:00:27
epoch [158/200] batch [15/70] time 0.431 (0.453) data 0.299 (0.322) loss_u loss_u 0.9282 (0.8967) acc_u 12.5000 (12.2917) lr 2.2949e-04 eta 0:00:24
epoch [158/200] batch [20/70] time 0.476 (0.456) data 0.345 (0.325) loss_u loss_u 0.8560 (0.9017) acc_u 18.7500 (11.5625) lr 2.2949e-04 eta 0:00:22
epoch [158/200] batch [25/70] time 0.536 (0.462) data 0.404 (0.331) loss_u loss_u 0.9668 (0.9029) acc_u 3.1250 (11.6250) lr 2.2949e-04 eta 0:00:20
epoch [158/200] batch [30/70] time 0.706 (0.469) data 0.575 (0.338) loss_u loss_u 0.9102 (0.9029) acc_u 12.5000 (11.4583) lr 2.2949e-04 eta 0:00:18
epoch [158/200] batch [35/70] time 0.452 (0.469) data 0.321 (0.338) loss_u loss_u 0.9009 (0.9047) acc_u 9.3750 (11.4286) lr 2.2949e-04 eta 0:00:16
epoch [158/200] batch [40/70] time 0.418 (0.474) data 0.287 (0.343) loss_u loss_u 0.8779 (0.9046) acc_u 15.6250 (11.3281) lr 2.2949e-04 eta 0:00:14
epoch [158/200] batch [45/70] time 0.499 (0.473) data 0.369 (0.342) loss_u loss_u 0.8945 (0.9004) acc_u 12.5000 (11.9444) lr 2.2949e-04 eta 0:00:11
epoch [158/200] batch [50/70] time 0.508 (0.478) data 0.376 (0.347) loss_u loss_u 0.9619 (0.8998) acc_u 3.1250 (11.8750) lr 2.2949e-04 eta 0:00:09
epoch [158/200] batch [55/70] time 0.435 (0.477) data 0.304 (0.346) loss_u loss_u 0.9121 (0.8998) acc_u 9.3750 (12.0455) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [60/70] time 0.346 (0.472) data 0.215 (0.341) loss_u loss_u 0.9619 (0.8981) acc_u 3.1250 (12.5000) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [65/70] time 0.362 (0.470) data 0.231 (0.339) loss_u loss_u 0.8408 (0.8987) acc_u 15.6250 (12.4519) lr 2.2949e-04 eta 0:00:02
epoch [158/200] batch [70/70] time 0.484 (0.467) data 0.353 (0.336) loss_u loss_u 0.9517 (0.9001) acc_u 3.1250 (12.2321) lr 2.2949e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2084
confident_label rate tensor(0.2691, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 844
clean true:387
clean false:457
clean_rate:0.4585308056872038
noisy true:665
noisy false:1627
after delete: len(clean_dataset) 844
after delete: len(noisy_dataset) 2292
epoch [159/200] batch [5/26] time 0.669 (0.503) data 0.537 (0.371) loss_x loss_x 0.9927 (1.1224) acc_x 78.1250 (75.6250) lr 2.1957e-04 eta 0:00:10
epoch [159/200] batch [10/26] time 0.451 (0.481) data 0.320 (0.349) loss_x loss_x 1.1797 (1.2308) acc_x 65.6250 (70.9375) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [15/26] time 0.480 (0.479) data 0.349 (0.347) loss_x loss_x 1.2188 (1.2700) acc_x 68.7500 (69.5833) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [20/26] time 0.380 (0.477) data 0.249 (0.346) loss_x loss_x 1.3896 (1.2015) acc_x 68.7500 (71.0938) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [25/26] time 0.592 (0.475) data 0.460 (0.344) loss_x loss_x 1.1904 (1.2516) acc_x 71.8750 (70.2500) lr 2.1957e-04 eta 0:00:00
epoch [159/200] batch [5/71] time 0.488 (0.472) data 0.357 (0.341) loss_u loss_u 0.8711 (0.8885) acc_u 12.5000 (13.1250) lr 2.1957e-04 eta 0:00:31
epoch [159/200] batch [10/71] time 0.438 (0.470) data 0.307 (0.339) loss_u loss_u 0.7896 (0.8969) acc_u 28.1250 (12.5000) lr 2.1957e-04 eta 0:00:28
epoch [159/200] batch [15/71] time 0.456 (0.469) data 0.324 (0.337) loss_u loss_u 0.8999 (0.8809) acc_u 15.6250 (14.7917) lr 2.1957e-04 eta 0:00:26
epoch [159/200] batch [20/71] time 0.484 (0.463) data 0.352 (0.331) loss_u loss_u 0.8696 (0.8816) acc_u 18.7500 (15.1562) lr 2.1957e-04 eta 0:00:23
epoch [159/200] batch [25/71] time 0.529 (0.461) data 0.396 (0.330) loss_u loss_u 0.9160 (0.8882) acc_u 15.6250 (14.7500) lr 2.1957e-04 eta 0:00:21
epoch [159/200] batch [30/71] time 0.466 (0.461) data 0.335 (0.329) loss_u loss_u 0.9746 (0.8958) acc_u 3.1250 (13.5417) lr 2.1957e-04 eta 0:00:18
epoch [159/200] batch [35/71] time 0.486 (0.461) data 0.355 (0.330) loss_u loss_u 0.8848 (0.8969) acc_u 9.3750 (13.2143) lr 2.1957e-04 eta 0:00:16
epoch [159/200] batch [40/71] time 0.449 (0.460) data 0.317 (0.329) loss_u loss_u 0.9531 (0.8980) acc_u 6.2500 (13.0469) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [45/71] time 0.454 (0.458) data 0.322 (0.326) loss_u loss_u 0.9512 (0.8996) acc_u 6.2500 (12.7778) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [50/71] time 0.536 (0.458) data 0.404 (0.327) loss_u loss_u 0.9673 (0.8999) acc_u 6.2500 (12.5000) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [55/71] time 0.331 (0.453) data 0.198 (0.322) loss_u loss_u 0.9067 (0.9018) acc_u 12.5000 (12.3295) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [60/71] time 0.421 (0.456) data 0.291 (0.324) loss_u loss_u 0.8364 (0.9002) acc_u 25.0000 (12.7083) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [65/71] time 0.390 (0.455) data 0.258 (0.323) loss_u loss_u 0.8921 (0.9004) acc_u 18.7500 (12.6442) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [70/71] time 0.559 (0.457) data 0.427 (0.326) loss_u loss_u 0.8569 (0.9000) acc_u 18.7500 (12.7679) lr 2.1957e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2065
confident_label rate tensor(0.2669, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 837
clean true:378
clean false:459
clean_rate:0.45161290322580644
noisy true:693
noisy false:1606
after delete: len(clean_dataset) 837
after delete: len(noisy_dataset) 2299
epoch [160/200] batch [5/26] time 0.554 (0.428) data 0.421 (0.297) loss_x loss_x 1.0166 (1.0938) acc_x 75.0000 (71.2500) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [10/26] time 0.392 (0.443) data 0.261 (0.312) loss_x loss_x 0.9053 (1.0975) acc_x 68.7500 (71.2500) lr 2.0984e-04 eta 0:00:07
epoch [160/200] batch [15/26] time 0.427 (0.441) data 0.297 (0.310) loss_x loss_x 0.8384 (1.1229) acc_x 71.8750 (71.6667) lr 2.0984e-04 eta 0:00:04
epoch [160/200] batch [20/26] time 0.386 (0.442) data 0.256 (0.311) loss_x loss_x 0.8936 (1.1292) acc_x 71.8750 (71.4062) lr 2.0984e-04 eta 0:00:02
epoch [160/200] batch [25/26] time 0.588 (0.447) data 0.458 (0.316) loss_x loss_x 0.5894 (1.1450) acc_x 90.6250 (71.3750) lr 2.0984e-04 eta 0:00:00
epoch [160/200] batch [5/71] time 0.442 (0.457) data 0.311 (0.327) loss_u loss_u 0.8008 (0.8710) acc_u 28.1250 (13.1250) lr 2.0984e-04 eta 0:00:30
epoch [160/200] batch [10/71] time 0.388 (0.456) data 0.257 (0.325) loss_u loss_u 0.9219 (0.8857) acc_u 9.3750 (12.5000) lr 2.0984e-04 eta 0:00:27
epoch [160/200] batch [15/71] time 0.498 (0.465) data 0.367 (0.334) loss_u loss_u 0.8340 (0.8964) acc_u 21.8750 (11.8750) lr 2.0984e-04 eta 0:00:26
epoch [160/200] batch [20/71] time 0.374 (0.462) data 0.244 (0.331) loss_u loss_u 0.9404 (0.9023) acc_u 6.2500 (11.7188) lr 2.0984e-04 eta 0:00:23
epoch [160/200] batch [25/71] time 0.526 (0.459) data 0.394 (0.328) loss_u loss_u 0.9248 (0.9001) acc_u 6.2500 (11.8750) lr 2.0984e-04 eta 0:00:21
epoch [160/200] batch [30/71] time 0.361 (0.461) data 0.229 (0.330) loss_u loss_u 0.8628 (0.8987) acc_u 15.6250 (11.8750) lr 2.0984e-04 eta 0:00:18
epoch [160/200] batch [35/71] time 0.395 (0.462) data 0.264 (0.331) loss_u loss_u 0.8496 (0.8962) acc_u 18.7500 (11.9643) lr 2.0984e-04 eta 0:00:16
epoch [160/200] batch [40/71] time 0.555 (0.465) data 0.425 (0.334) loss_u loss_u 0.9014 (0.9009) acc_u 12.5000 (11.4844) lr 2.0984e-04 eta 0:00:14
epoch [160/200] batch [45/71] time 0.386 (0.464) data 0.255 (0.333) loss_u loss_u 0.9067 (0.9006) acc_u 12.5000 (11.8056) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [50/71] time 0.379 (0.461) data 0.247 (0.330) loss_u loss_u 0.9053 (0.8995) acc_u 15.6250 (12.0625) lr 2.0984e-04 eta 0:00:09
epoch [160/200] batch [55/71] time 0.371 (0.458) data 0.241 (0.327) loss_u loss_u 0.8335 (0.8991) acc_u 25.0000 (12.3295) lr 2.0984e-04 eta 0:00:07
epoch [160/200] batch [60/71] time 0.455 (0.456) data 0.324 (0.325) loss_u loss_u 0.8413 (0.8979) acc_u 21.8750 (12.7083) lr 2.0984e-04 eta 0:00:05
epoch [160/200] batch [65/71] time 0.519 (0.455) data 0.387 (0.324) loss_u loss_u 0.8892 (0.8967) acc_u 12.5000 (12.7885) lr 2.0984e-04 eta 0:00:02
epoch [160/200] batch [70/71] time 0.596 (0.457) data 0.464 (0.326) loss_u loss_u 0.8467 (0.8963) acc_u 15.6250 (12.6786) lr 2.0984e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2077
confident_label rate tensor(0.2602, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 816
clean true:380
clean false:436
clean_rate:0.46568627450980393
noisy true:679
noisy false:1641
after delete: len(clean_dataset) 816
after delete: len(noisy_dataset) 2320
epoch [161/200] batch [5/25] time 0.440 (0.491) data 0.309 (0.361) loss_x loss_x 0.7026 (1.0964) acc_x 78.1250 (73.7500) lr 2.0032e-04 eta 0:00:09
epoch [161/200] batch [10/25] time 0.455 (0.501) data 0.323 (0.370) loss_x loss_x 0.7510 (0.9922) acc_x 81.2500 (75.6250) lr 2.0032e-04 eta 0:00:07
epoch [161/200] batch [15/25] time 0.412 (0.478) data 0.281 (0.347) loss_x loss_x 1.2207 (1.0260) acc_x 62.5000 (73.7500) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [20/25] time 0.437 (0.464) data 0.306 (0.334) loss_x loss_x 1.5557 (1.1045) acc_x 53.1250 (70.9375) lr 2.0032e-04 eta 0:00:02
epoch [161/200] batch [25/25] time 0.481 (0.472) data 0.350 (0.341) loss_x loss_x 0.9619 (1.1144) acc_x 81.2500 (71.2500) lr 2.0032e-04 eta 0:00:00
epoch [161/200] batch [5/72] time 0.384 (0.462) data 0.252 (0.331) loss_u loss_u 0.8257 (0.8651) acc_u 18.7500 (17.5000) lr 2.0032e-04 eta 0:00:30
epoch [161/200] batch [10/72] time 0.416 (0.453) data 0.285 (0.322) loss_u loss_u 0.9312 (0.8740) acc_u 12.5000 (17.5000) lr 2.0032e-04 eta 0:00:28
epoch [161/200] batch [15/72] time 0.509 (0.454) data 0.378 (0.323) loss_u loss_u 0.9199 (0.8744) acc_u 9.3750 (17.5000) lr 2.0032e-04 eta 0:00:25
epoch [161/200] batch [20/72] time 0.457 (0.460) data 0.326 (0.329) loss_u loss_u 0.8833 (0.8871) acc_u 12.5000 (15.1562) lr 2.0032e-04 eta 0:00:23
epoch [161/200] batch [25/72] time 0.501 (0.467) data 0.369 (0.336) loss_u loss_u 0.9341 (0.8819) acc_u 6.2500 (15.6250) lr 2.0032e-04 eta 0:00:21
epoch [161/200] batch [30/72] time 0.389 (0.463) data 0.257 (0.332) loss_u loss_u 0.8608 (0.8763) acc_u 15.6250 (16.4583) lr 2.0032e-04 eta 0:00:19
epoch [161/200] batch [35/72] time 0.444 (0.460) data 0.311 (0.328) loss_u loss_u 0.9673 (0.8740) acc_u 3.1250 (16.8750) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [40/72] time 0.429 (0.461) data 0.297 (0.330) loss_u loss_u 0.8374 (0.8781) acc_u 21.8750 (16.0938) lr 2.0032e-04 eta 0:00:14
epoch [161/200] batch [45/72] time 0.432 (0.460) data 0.301 (0.329) loss_u loss_u 0.8984 (0.8831) acc_u 12.5000 (15.4167) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [50/72] time 0.313 (0.456) data 0.183 (0.325) loss_u loss_u 0.9092 (0.8856) acc_u 9.3750 (15.0000) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [55/72] time 0.393 (0.459) data 0.262 (0.328) loss_u loss_u 0.8984 (0.8883) acc_u 12.5000 (14.4886) lr 2.0032e-04 eta 0:00:07
epoch [161/200] batch [60/72] time 0.469 (0.462) data 0.338 (0.331) loss_u loss_u 0.8838 (0.8885) acc_u 18.7500 (14.3750) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [65/72] time 0.499 (0.461) data 0.369 (0.330) loss_u loss_u 0.8779 (0.8879) acc_u 18.7500 (14.6635) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [70/72] time 0.392 (0.458) data 0.260 (0.327) loss_u loss_u 0.9233 (0.8885) acc_u 12.5000 (14.5982) lr 2.0032e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2090
confident_label rate tensor(0.2758, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 865
clean true:398
clean false:467
clean_rate:0.4601156069364162
noisy true:648
noisy false:1623
after delete: len(clean_dataset) 865
after delete: len(noisy_dataset) 2271
epoch [162/200] batch [5/27] time 0.460 (0.457) data 0.328 (0.326) loss_x loss_x 1.5527 (1.2320) acc_x 62.5000 (66.2500) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [10/27] time 0.395 (0.431) data 0.264 (0.300) loss_x loss_x 0.6406 (1.1844) acc_x 87.5000 (68.4375) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [15/27] time 0.449 (0.433) data 0.319 (0.302) loss_x loss_x 1.3994 (1.1967) acc_x 68.7500 (67.9167) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [20/27] time 0.472 (0.434) data 0.342 (0.303) loss_x loss_x 1.5459 (1.2064) acc_x 62.5000 (67.1875) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [25/27] time 0.485 (0.442) data 0.354 (0.311) loss_x loss_x 1.1904 (1.2241) acc_x 75.0000 (67.2500) lr 1.9098e-04 eta 0:00:00
epoch [162/200] batch [5/70] time 0.486 (0.445) data 0.355 (0.314) loss_u loss_u 0.9150 (0.8923) acc_u 9.3750 (12.5000) lr 1.9098e-04 eta 0:00:28
epoch [162/200] batch [10/70] time 0.368 (0.458) data 0.236 (0.327) loss_u loss_u 0.9487 (0.9028) acc_u 9.3750 (12.5000) lr 1.9098e-04 eta 0:00:27
epoch [162/200] batch [15/70] time 0.465 (0.458) data 0.334 (0.327) loss_u loss_u 0.9097 (0.9049) acc_u 12.5000 (12.2917) lr 1.9098e-04 eta 0:00:25
epoch [162/200] batch [20/70] time 0.538 (0.460) data 0.406 (0.329) loss_u loss_u 0.9268 (0.8921) acc_u 9.3750 (13.5938) lr 1.9098e-04 eta 0:00:22
epoch [162/200] batch [25/70] time 0.509 (0.457) data 0.378 (0.326) loss_u loss_u 0.8799 (0.8894) acc_u 18.7500 (14.1250) lr 1.9098e-04 eta 0:00:20
epoch [162/200] batch [30/70] time 0.479 (0.455) data 0.347 (0.324) loss_u loss_u 0.8872 (0.8918) acc_u 15.6250 (13.7500) lr 1.9098e-04 eta 0:00:18
epoch [162/200] batch [35/70] time 0.317 (0.451) data 0.186 (0.320) loss_u loss_u 0.8989 (0.8932) acc_u 18.7500 (13.7500) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [40/70] time 0.524 (0.449) data 0.392 (0.318) loss_u loss_u 0.8662 (0.8929) acc_u 21.8750 (14.0625) lr 1.9098e-04 eta 0:00:13
epoch [162/200] batch [45/70] time 0.423 (0.453) data 0.291 (0.322) loss_u loss_u 0.8828 (0.8921) acc_u 12.5000 (14.1667) lr 1.9098e-04 eta 0:00:11
epoch [162/200] batch [50/70] time 0.445 (0.452) data 0.314 (0.321) loss_u loss_u 0.9209 (0.8920) acc_u 6.2500 (13.9375) lr 1.9098e-04 eta 0:00:09
epoch [162/200] batch [55/70] time 0.354 (0.452) data 0.223 (0.321) loss_u loss_u 0.8540 (0.8933) acc_u 15.6250 (13.7500) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [60/70] time 0.453 (0.451) data 0.321 (0.320) loss_u loss_u 0.9102 (0.8916) acc_u 9.3750 (13.7500) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [65/70] time 0.387 (0.449) data 0.256 (0.318) loss_u loss_u 0.8779 (0.8924) acc_u 18.7500 (13.7981) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [70/70] time 0.502 (0.448) data 0.370 (0.317) loss_u loss_u 0.9380 (0.8927) acc_u 3.1250 (13.7500) lr 1.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2068
confident_label rate tensor(0.2777, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 871
clean true:396
clean false:475
clean_rate:0.45464982778415614
noisy true:672
noisy false:1593
after delete: len(clean_dataset) 871
after delete: len(noisy_dataset) 2265
epoch [163/200] batch [5/27] time 0.450 (0.561) data 0.318 (0.430) loss_x loss_x 0.8745 (1.2759) acc_x 81.2500 (69.3750) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [10/27] time 0.449 (0.497) data 0.318 (0.366) loss_x loss_x 1.1660 (1.2560) acc_x 62.5000 (66.8750) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [15/27] time 0.373 (0.481) data 0.243 (0.350) loss_x loss_x 1.0078 (1.1576) acc_x 71.8750 (69.7917) lr 1.8185e-04 eta 0:00:05
epoch [163/200] batch [20/27] time 0.483 (0.476) data 0.352 (0.346) loss_x loss_x 0.7271 (1.1518) acc_x 84.3750 (70.4688) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [25/27] time 0.504 (0.472) data 0.373 (0.341) loss_x loss_x 1.0664 (1.1460) acc_x 78.1250 (70.3750) lr 1.8185e-04 eta 0:00:00
epoch [163/200] batch [5/70] time 0.463 (0.470) data 0.333 (0.339) loss_u loss_u 0.8765 (0.8919) acc_u 9.3750 (10.6250) lr 1.8185e-04 eta 0:00:30
epoch [163/200] batch [10/70] time 0.481 (0.478) data 0.349 (0.347) loss_u loss_u 0.9170 (0.8814) acc_u 9.3750 (12.8125) lr 1.8185e-04 eta 0:00:28
epoch [163/200] batch [15/70] time 0.422 (0.469) data 0.289 (0.338) loss_u loss_u 0.8931 (0.8859) acc_u 15.6250 (13.1250) lr 1.8185e-04 eta 0:00:25
epoch [163/200] batch [20/70] time 0.397 (0.469) data 0.265 (0.338) loss_u loss_u 0.9146 (0.8865) acc_u 12.5000 (13.2812) lr 1.8185e-04 eta 0:00:23
epoch [163/200] batch [25/70] time 0.539 (0.479) data 0.408 (0.347) loss_u loss_u 0.8877 (0.8900) acc_u 15.6250 (13.1250) lr 1.8185e-04 eta 0:00:21
epoch [163/200] batch [30/70] time 0.407 (0.472) data 0.275 (0.340) loss_u loss_u 0.8320 (0.8863) acc_u 25.0000 (13.5417) lr 1.8185e-04 eta 0:00:18
epoch [163/200] batch [35/70] time 0.523 (0.469) data 0.391 (0.337) loss_u loss_u 0.9048 (0.8845) acc_u 9.3750 (13.7500) lr 1.8185e-04 eta 0:00:16
epoch [163/200] batch [40/70] time 0.486 (0.468) data 0.354 (0.337) loss_u loss_u 0.9077 (0.8868) acc_u 6.2500 (13.3594) lr 1.8185e-04 eta 0:00:14
epoch [163/200] batch [45/70] time 0.488 (0.469) data 0.356 (0.338) loss_u loss_u 0.8853 (0.8895) acc_u 18.7500 (13.2639) lr 1.8185e-04 eta 0:00:11
epoch [163/200] batch [50/70] time 0.370 (0.465) data 0.238 (0.334) loss_u loss_u 0.9268 (0.8907) acc_u 6.2500 (13.1250) lr 1.8185e-04 eta 0:00:09
epoch [163/200] batch [55/70] time 0.388 (0.465) data 0.257 (0.333) loss_u loss_u 0.7935 (0.8883) acc_u 25.0000 (13.5227) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [60/70] time 0.400 (0.464) data 0.269 (0.333) loss_u loss_u 0.9438 (0.8904) acc_u 6.2500 (13.2812) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [65/70] time 0.398 (0.460) data 0.266 (0.329) loss_u loss_u 0.8169 (0.8881) acc_u 21.8750 (13.6058) lr 1.8185e-04 eta 0:00:02
epoch [163/200] batch [70/70] time 0.407 (0.459) data 0.276 (0.327) loss_u loss_u 0.9287 (0.8918) acc_u 12.5000 (13.1696) lr 1.8185e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2084
confident_label rate tensor(0.2701, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 847
clean true:376
clean false:471
clean_rate:0.44391971664698937
noisy true:676
noisy false:1613
after delete: len(clean_dataset) 847
after delete: len(noisy_dataset) 2289
epoch [164/200] batch [5/26] time 0.354 (0.486) data 0.223 (0.355) loss_x loss_x 1.6553 (1.0531) acc_x 62.5000 (74.3750) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [10/26] time 0.462 (0.482) data 0.330 (0.351) loss_x loss_x 1.3223 (1.1250) acc_x 65.6250 (72.5000) lr 1.7292e-04 eta 0:00:07
epoch [164/200] batch [15/26] time 0.655 (0.498) data 0.524 (0.367) loss_x loss_x 1.4004 (1.1954) acc_x 68.7500 (70.4167) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [20/26] time 0.431 (0.485) data 0.301 (0.354) loss_x loss_x 1.1143 (1.1144) acc_x 71.8750 (73.1250) lr 1.7292e-04 eta 0:00:02
epoch [164/200] batch [25/26] time 0.499 (0.490) data 0.368 (0.359) loss_x loss_x 1.1240 (1.1110) acc_x 81.2500 (73.2500) lr 1.7292e-04 eta 0:00:00
epoch [164/200] batch [5/71] time 0.518 (0.491) data 0.386 (0.359) loss_u loss_u 0.8374 (0.8912) acc_u 21.8750 (13.7500) lr 1.7292e-04 eta 0:00:32
epoch [164/200] batch [10/71] time 0.368 (0.482) data 0.236 (0.351) loss_u loss_u 0.9258 (0.8980) acc_u 9.3750 (12.5000) lr 1.7292e-04 eta 0:00:29
epoch [164/200] batch [15/71] time 0.512 (0.481) data 0.380 (0.350) loss_u loss_u 0.8389 (0.8969) acc_u 21.8750 (12.7083) lr 1.7292e-04 eta 0:00:26
epoch [164/200] batch [20/71] time 0.666 (0.480) data 0.534 (0.348) loss_u loss_u 0.9160 (0.8957) acc_u 6.2500 (13.1250) lr 1.7292e-04 eta 0:00:24
epoch [164/200] batch [25/71] time 0.503 (0.476) data 0.371 (0.345) loss_u loss_u 0.8882 (0.8911) acc_u 18.7500 (13.6250) lr 1.7292e-04 eta 0:00:21
epoch [164/200] batch [30/71] time 0.439 (0.478) data 0.308 (0.346) loss_u loss_u 0.9287 (0.8932) acc_u 6.2500 (13.3333) lr 1.7292e-04 eta 0:00:19
epoch [164/200] batch [35/71] time 0.407 (0.476) data 0.277 (0.344) loss_u loss_u 0.8789 (0.8906) acc_u 15.6250 (13.7500) lr 1.7292e-04 eta 0:00:17
epoch [164/200] batch [40/71] time 0.505 (0.470) data 0.373 (0.339) loss_u loss_u 0.9185 (0.8896) acc_u 9.3750 (14.0625) lr 1.7292e-04 eta 0:00:14
epoch [164/200] batch [45/71] time 0.351 (0.467) data 0.219 (0.336) loss_u loss_u 0.8906 (0.8873) acc_u 12.5000 (14.1667) lr 1.7292e-04 eta 0:00:12
epoch [164/200] batch [50/71] time 0.519 (0.468) data 0.388 (0.336) loss_u loss_u 0.9355 (0.8874) acc_u 9.3750 (14.1875) lr 1.7292e-04 eta 0:00:09
epoch [164/200] batch [55/71] time 0.336 (0.466) data 0.204 (0.335) loss_u loss_u 0.8516 (0.8862) acc_u 18.7500 (14.4318) lr 1.7292e-04 eta 0:00:07
epoch [164/200] batch [60/71] time 0.415 (0.466) data 0.283 (0.335) loss_u loss_u 0.8257 (0.8843) acc_u 21.8750 (14.6354) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [65/71] time 0.386 (0.465) data 0.255 (0.334) loss_u loss_u 0.9189 (0.8834) acc_u 9.3750 (14.7596) lr 1.7292e-04 eta 0:00:02
epoch [164/200] batch [70/71] time 0.467 (0.468) data 0.334 (0.337) loss_u loss_u 0.9219 (0.8843) acc_u 9.3750 (14.5536) lr 1.7292e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2075
confident_label rate tensor(0.2685, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 842
clean true:386
clean false:456
clean_rate:0.4584323040380047
noisy true:675
noisy false:1619
after delete: len(clean_dataset) 842
after delete: len(noisy_dataset) 2294
epoch [165/200] batch [5/26] time 0.465 (0.490) data 0.332 (0.358) loss_x loss_x 1.5010 (1.0270) acc_x 65.6250 (75.0000) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [10/26] time 0.527 (0.499) data 0.395 (0.367) loss_x loss_x 1.2646 (1.1150) acc_x 68.7500 (72.8125) lr 1.6419e-04 eta 0:00:07
epoch [165/200] batch [15/26] time 0.402 (0.480) data 0.270 (0.349) loss_x loss_x 0.8096 (1.1143) acc_x 84.3750 (71.8750) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [20/26] time 0.368 (0.471) data 0.236 (0.339) loss_x loss_x 1.8555 (1.1611) acc_x 62.5000 (70.4688) lr 1.6419e-04 eta 0:00:02
epoch [165/200] batch [25/26] time 0.655 (0.476) data 0.524 (0.345) loss_x loss_x 1.2197 (1.1929) acc_x 68.7500 (69.7500) lr 1.6419e-04 eta 0:00:00
epoch [165/200] batch [5/71] time 0.422 (0.481) data 0.290 (0.349) loss_u loss_u 0.8877 (0.9068) acc_u 9.3750 (11.8750) lr 1.6419e-04 eta 0:00:31
epoch [165/200] batch [10/71] time 0.438 (0.474) data 0.307 (0.343) loss_u loss_u 0.9360 (0.9005) acc_u 9.3750 (11.8750) lr 1.6419e-04 eta 0:00:28
epoch [165/200] batch [15/71] time 0.513 (0.471) data 0.381 (0.339) loss_u loss_u 0.9185 (0.8999) acc_u 6.2500 (11.6667) lr 1.6419e-04 eta 0:00:26
epoch [165/200] batch [20/71] time 0.457 (0.471) data 0.326 (0.340) loss_u loss_u 0.7808 (0.8941) acc_u 28.1250 (12.6562) lr 1.6419e-04 eta 0:00:24
epoch [165/200] batch [25/71] time 0.397 (0.467) data 0.266 (0.336) loss_u loss_u 0.9111 (0.8943) acc_u 15.6250 (13.2500) lr 1.6419e-04 eta 0:00:21
epoch [165/200] batch [30/71] time 0.378 (0.465) data 0.247 (0.333) loss_u loss_u 0.9106 (0.8961) acc_u 9.3750 (13.0208) lr 1.6419e-04 eta 0:00:19
epoch [165/200] batch [35/71] time 0.435 (0.466) data 0.303 (0.335) loss_u loss_u 0.8589 (0.8935) acc_u 15.6250 (13.0357) lr 1.6419e-04 eta 0:00:16
epoch [165/200] batch [40/71] time 0.465 (0.464) data 0.333 (0.333) loss_u loss_u 0.8643 (0.8903) acc_u 18.7500 (13.5938) lr 1.6419e-04 eta 0:00:14
epoch [165/200] batch [45/71] time 0.442 (0.464) data 0.309 (0.333) loss_u loss_u 0.8882 (0.8929) acc_u 18.7500 (13.1944) lr 1.6419e-04 eta 0:00:12
epoch [165/200] batch [50/71] time 0.753 (0.473) data 0.622 (0.341) loss_u loss_u 0.9160 (0.8887) acc_u 12.5000 (13.5625) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [55/71] time 0.424 (0.472) data 0.291 (0.340) loss_u loss_u 0.8154 (0.8866) acc_u 18.7500 (14.0341) lr 1.6419e-04 eta 0:00:07
epoch [165/200] batch [60/71] time 0.511 (0.472) data 0.380 (0.340) loss_u loss_u 0.9404 (0.8861) acc_u 6.2500 (14.0104) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [65/71] time 0.447 (0.473) data 0.315 (0.341) loss_u loss_u 0.8677 (0.8854) acc_u 15.6250 (14.0865) lr 1.6419e-04 eta 0:00:02
epoch [165/200] batch [70/71] time 0.506 (0.475) data 0.374 (0.344) loss_u loss_u 0.9590 (0.8866) acc_u 6.2500 (14.0179) lr 1.6419e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2070
confident_label rate tensor(0.2771, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 869
clean true:404
clean false:465
clean_rate:0.46490218642117376
noisy true:662
noisy false:1605
after delete: len(clean_dataset) 869
after delete: len(noisy_dataset) 2267
epoch [166/200] batch [5/27] time 0.446 (0.564) data 0.316 (0.434) loss_x loss_x 1.1348 (1.1971) acc_x 62.5000 (67.5000) lr 1.5567e-04 eta 0:00:12
epoch [166/200] batch [10/27] time 0.499 (0.518) data 0.368 (0.387) loss_x loss_x 0.8062 (1.0340) acc_x 75.0000 (71.8750) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [15/27] time 0.494 (0.490) data 0.363 (0.360) loss_x loss_x 1.2168 (1.1092) acc_x 71.8750 (71.4583) lr 1.5567e-04 eta 0:00:05
epoch [166/200] batch [20/27] time 0.585 (0.485) data 0.454 (0.355) loss_x loss_x 1.1123 (1.1123) acc_x 68.7500 (72.0312) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [25/27] time 0.431 (0.474) data 0.300 (0.343) loss_x loss_x 1.6367 (1.1013) acc_x 56.2500 (72.3750) lr 1.5567e-04 eta 0:00:00
epoch [166/200] batch [5/70] time 0.451 (0.462) data 0.320 (0.331) loss_u loss_u 0.9194 (0.9214) acc_u 12.5000 (10.6250) lr 1.5567e-04 eta 0:00:30
epoch [166/200] batch [10/70] time 0.441 (0.456) data 0.309 (0.325) loss_u loss_u 0.8540 (0.9052) acc_u 18.7500 (12.5000) lr 1.5567e-04 eta 0:00:27
epoch [166/200] batch [15/70] time 0.399 (0.457) data 0.267 (0.326) loss_u loss_u 0.9194 (0.9009) acc_u 9.3750 (12.7083) lr 1.5567e-04 eta 0:00:25
epoch [166/200] batch [20/70] time 0.639 (0.462) data 0.507 (0.331) loss_u loss_u 0.8735 (0.8935) acc_u 15.6250 (13.9062) lr 1.5567e-04 eta 0:00:23
epoch [166/200] batch [25/70] time 0.394 (0.458) data 0.262 (0.327) loss_u loss_u 0.9678 (0.8955) acc_u 3.1250 (13.7500) lr 1.5567e-04 eta 0:00:20
epoch [166/200] batch [30/70] time 0.551 (0.458) data 0.420 (0.327) loss_u loss_u 0.8286 (0.8896) acc_u 25.0000 (14.3750) lr 1.5567e-04 eta 0:00:18
epoch [166/200] batch [35/70] time 0.421 (0.460) data 0.289 (0.329) loss_u loss_u 0.8682 (0.8927) acc_u 21.8750 (14.1964) lr 1.5567e-04 eta 0:00:16
epoch [166/200] batch [40/70] time 0.422 (0.462) data 0.290 (0.331) loss_u loss_u 0.8730 (0.8919) acc_u 12.5000 (14.2188) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [45/70] time 0.411 (0.462) data 0.278 (0.331) loss_u loss_u 0.8936 (0.8904) acc_u 18.7500 (14.5139) lr 1.5567e-04 eta 0:00:11
epoch [166/200] batch [50/70] time 0.469 (0.462) data 0.337 (0.331) loss_u loss_u 0.8892 (0.8893) acc_u 9.3750 (14.3125) lr 1.5567e-04 eta 0:00:09
epoch [166/200] batch [55/70] time 0.417 (0.460) data 0.285 (0.329) loss_u loss_u 0.8223 (0.8906) acc_u 25.0000 (14.0909) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [60/70] time 0.406 (0.464) data 0.274 (0.332) loss_u loss_u 0.8384 (0.8883) acc_u 21.8750 (14.4271) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [65/70] time 0.434 (0.467) data 0.302 (0.336) loss_u loss_u 0.8584 (0.8871) acc_u 15.6250 (14.5673) lr 1.5567e-04 eta 0:00:02
epoch [166/200] batch [70/70] time 0.350 (0.466) data 0.218 (0.334) loss_u loss_u 0.9634 (0.8886) acc_u 3.1250 (14.3304) lr 1.5567e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2086
confident_label rate tensor(0.2698, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 846
clean true:383
clean false:463
clean_rate:0.45271867612293143
noisy true:667
noisy false:1623
after delete: len(clean_dataset) 846
after delete: len(noisy_dataset) 2290
epoch [167/200] batch [5/26] time 0.405 (0.514) data 0.273 (0.383) loss_x loss_x 1.8174 (1.2480) acc_x 71.8750 (70.6250) lr 1.4736e-04 eta 0:00:10
epoch [167/200] batch [10/26] time 0.484 (0.478) data 0.354 (0.347) loss_x loss_x 1.7285 (1.2591) acc_x 62.5000 (67.8125) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [15/26] time 0.591 (0.477) data 0.461 (0.345) loss_x loss_x 1.0928 (1.2337) acc_x 68.7500 (68.3333) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [20/26] time 0.407 (0.482) data 0.275 (0.351) loss_x loss_x 1.2910 (1.1956) acc_x 75.0000 (70.1562) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [25/26] time 0.537 (0.484) data 0.406 (0.353) loss_x loss_x 1.1514 (1.1671) acc_x 65.6250 (70.3750) lr 1.4736e-04 eta 0:00:00
epoch [167/200] batch [5/71] time 0.397 (0.487) data 0.265 (0.356) loss_u loss_u 0.8745 (0.8828) acc_u 15.6250 (14.3750) lr 1.4736e-04 eta 0:00:32
epoch [167/200] batch [10/71] time 0.363 (0.478) data 0.232 (0.347) loss_u loss_u 0.8501 (0.8709) acc_u 18.7500 (16.2500) lr 1.4736e-04 eta 0:00:29
epoch [167/200] batch [15/71] time 0.510 (0.481) data 0.379 (0.350) loss_u loss_u 0.9214 (0.8768) acc_u 9.3750 (15.8333) lr 1.4736e-04 eta 0:00:26
epoch [167/200] batch [20/71] time 0.457 (0.474) data 0.326 (0.342) loss_u loss_u 0.8823 (0.8830) acc_u 12.5000 (14.8438) lr 1.4736e-04 eta 0:00:24
epoch [167/200] batch [25/71] time 0.502 (0.470) data 0.370 (0.339) loss_u loss_u 0.8774 (0.8860) acc_u 18.7500 (15.0000) lr 1.4736e-04 eta 0:00:21
epoch [167/200] batch [30/71] time 0.479 (0.473) data 0.347 (0.342) loss_u loss_u 0.8623 (0.8865) acc_u 18.7500 (15.1042) lr 1.4736e-04 eta 0:00:19
epoch [167/200] batch [35/71] time 0.427 (0.468) data 0.296 (0.336) loss_u loss_u 0.8472 (0.8829) acc_u 15.6250 (15.2679) lr 1.4736e-04 eta 0:00:16
epoch [167/200] batch [40/71] time 0.467 (0.468) data 0.336 (0.336) loss_u loss_u 0.9194 (0.8829) acc_u 9.3750 (15.3906) lr 1.4736e-04 eta 0:00:14
epoch [167/200] batch [45/71] time 0.458 (0.467) data 0.326 (0.336) loss_u loss_u 0.8101 (0.8828) acc_u 25.0000 (15.3472) lr 1.4736e-04 eta 0:00:12
epoch [167/200] batch [50/71] time 0.404 (0.471) data 0.274 (0.340) loss_u loss_u 0.8628 (0.8812) acc_u 15.6250 (15.3750) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [55/71] time 0.468 (0.468) data 0.337 (0.337) loss_u loss_u 0.9062 (0.8846) acc_u 12.5000 (14.8295) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [60/71] time 0.510 (0.468) data 0.377 (0.337) loss_u loss_u 0.8618 (0.8839) acc_u 18.7500 (14.8438) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [65/71] time 0.489 (0.467) data 0.357 (0.336) loss_u loss_u 0.9473 (0.8845) acc_u 3.1250 (14.6154) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [70/71] time 0.507 (0.467) data 0.375 (0.336) loss_u loss_u 0.9512 (0.8850) acc_u 6.2500 (14.5089) lr 1.4736e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2120
confident_label rate tensor(0.2688, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 843
clean true:371
clean false:472
clean_rate:0.4400948991696323
noisy true:645
noisy false:1648
after delete: len(clean_dataset) 843
after delete: len(noisy_dataset) 2293
epoch [168/200] batch [5/26] time 0.361 (0.461) data 0.230 (0.330) loss_x loss_x 0.8608 (1.1558) acc_x 78.1250 (68.7500) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [10/26] time 0.370 (0.435) data 0.239 (0.304) loss_x loss_x 1.3125 (1.1308) acc_x 71.8750 (72.5000) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [15/26] time 0.451 (0.457) data 0.320 (0.327) loss_x loss_x 1.0664 (1.1727) acc_x 65.6250 (71.0417) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [20/26] time 0.415 (0.455) data 0.285 (0.324) loss_x loss_x 1.5186 (1.1726) acc_x 65.6250 (70.4688) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [25/26] time 0.464 (0.462) data 0.333 (0.332) loss_x loss_x 0.8081 (1.1384) acc_x 78.1250 (71.5000) lr 1.3926e-04 eta 0:00:00
epoch [168/200] batch [5/71] time 0.442 (0.471) data 0.311 (0.340) loss_u loss_u 0.9214 (0.8704) acc_u 9.3750 (15.6250) lr 1.3926e-04 eta 0:00:31
epoch [168/200] batch [10/71] time 0.473 (0.467) data 0.342 (0.336) loss_u loss_u 0.8608 (0.8797) acc_u 15.6250 (15.0000) lr 1.3926e-04 eta 0:00:28
epoch [168/200] batch [15/71] time 0.383 (0.461) data 0.251 (0.330) loss_u loss_u 0.8989 (0.8826) acc_u 12.5000 (14.1667) lr 1.3926e-04 eta 0:00:25
epoch [168/200] batch [20/71] time 0.343 (0.458) data 0.212 (0.327) loss_u loss_u 0.8481 (0.8796) acc_u 15.6250 (14.8438) lr 1.3926e-04 eta 0:00:23
epoch [168/200] batch [25/71] time 0.516 (0.463) data 0.385 (0.332) loss_u loss_u 0.8354 (0.8784) acc_u 21.8750 (14.8750) lr 1.3926e-04 eta 0:00:21
epoch [168/200] batch [30/71] time 0.595 (0.464) data 0.464 (0.333) loss_u loss_u 0.9243 (0.8807) acc_u 9.3750 (14.7917) lr 1.3926e-04 eta 0:00:19
epoch [168/200] batch [35/71] time 0.516 (0.464) data 0.386 (0.333) loss_u loss_u 0.8989 (0.8823) acc_u 12.5000 (14.8214) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [40/71] time 0.529 (0.465) data 0.398 (0.334) loss_u loss_u 0.8101 (0.8834) acc_u 25.0000 (14.7656) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [45/71] time 0.426 (0.464) data 0.295 (0.332) loss_u loss_u 0.9219 (0.8821) acc_u 12.5000 (14.7917) lr 1.3926e-04 eta 0:00:12
epoch [168/200] batch [50/71] time 0.705 (0.465) data 0.573 (0.334) loss_u loss_u 0.8794 (0.8844) acc_u 12.5000 (14.5000) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [55/71] time 0.424 (0.464) data 0.293 (0.333) loss_u loss_u 0.9243 (0.8865) acc_u 6.2500 (14.0341) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [60/71] time 0.363 (0.462) data 0.233 (0.331) loss_u loss_u 0.8579 (0.8888) acc_u 18.7500 (13.7500) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [65/71] time 0.509 (0.461) data 0.379 (0.330) loss_u loss_u 0.8848 (0.8878) acc_u 9.3750 (13.8462) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [70/71] time 0.422 (0.461) data 0.291 (0.330) loss_u loss_u 0.9136 (0.8897) acc_u 18.7500 (13.8393) lr 1.3926e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2076
confident_label rate tensor(0.2742, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 860
clean true:385
clean false:475
clean_rate:0.4476744186046512
noisy true:675
noisy false:1601
after delete: len(clean_dataset) 860
after delete: len(noisy_dataset) 2276
epoch [169/200] batch [5/26] time 0.513 (0.489) data 0.382 (0.358) loss_x loss_x 1.2979 (1.1355) acc_x 62.5000 (72.5000) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [10/26] time 0.454 (0.493) data 0.323 (0.362) loss_x loss_x 0.7930 (1.0559) acc_x 75.0000 (73.4375) lr 1.3137e-04 eta 0:00:07
epoch [169/200] batch [15/26] time 0.373 (0.462) data 0.243 (0.332) loss_x loss_x 0.6719 (0.9894) acc_x 81.2500 (74.3750) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [20/26] time 0.587 (0.464) data 0.457 (0.334) loss_x loss_x 1.3154 (1.0645) acc_x 75.0000 (73.5938) lr 1.3137e-04 eta 0:00:02
epoch [169/200] batch [25/26] time 0.391 (0.470) data 0.259 (0.339) loss_x loss_x 1.3760 (1.0922) acc_x 65.6250 (72.5000) lr 1.3137e-04 eta 0:00:00
epoch [169/200] batch [5/71] time 0.560 (0.478) data 0.428 (0.348) loss_u loss_u 0.8242 (0.8633) acc_u 21.8750 (16.2500) lr 1.3137e-04 eta 0:00:31
epoch [169/200] batch [10/71] time 0.514 (0.477) data 0.383 (0.346) loss_u loss_u 0.8101 (0.8734) acc_u 21.8750 (15.0000) lr 1.3137e-04 eta 0:00:29
epoch [169/200] batch [15/71] time 0.382 (0.474) data 0.251 (0.343) loss_u loss_u 0.8398 (0.8821) acc_u 18.7500 (13.5417) lr 1.3137e-04 eta 0:00:26
epoch [169/200] batch [20/71] time 0.431 (0.478) data 0.301 (0.347) loss_u loss_u 0.8882 (0.8883) acc_u 18.7500 (13.4375) lr 1.3137e-04 eta 0:00:24
epoch [169/200] batch [25/71] time 0.475 (0.477) data 0.344 (0.346) loss_u loss_u 0.8604 (0.8904) acc_u 18.7500 (13.0000) lr 1.3137e-04 eta 0:00:21
epoch [169/200] batch [30/71] time 0.500 (0.474) data 0.368 (0.343) loss_u loss_u 0.9150 (0.8886) acc_u 12.5000 (13.4375) lr 1.3137e-04 eta 0:00:19
epoch [169/200] batch [35/71] time 0.617 (0.472) data 0.486 (0.341) loss_u loss_u 0.9302 (0.8897) acc_u 6.2500 (13.3036) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [40/71] time 0.556 (0.477) data 0.424 (0.346) loss_u loss_u 0.9370 (0.8938) acc_u 6.2500 (12.8125) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [45/71] time 0.478 (0.475) data 0.348 (0.344) loss_u loss_u 0.8364 (0.8903) acc_u 18.7500 (13.2639) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [50/71] time 0.384 (0.474) data 0.252 (0.343) loss_u loss_u 0.8608 (0.8870) acc_u 15.6250 (13.6875) lr 1.3137e-04 eta 0:00:09
epoch [169/200] batch [55/71] time 0.473 (0.473) data 0.341 (0.342) loss_u loss_u 0.9326 (0.8872) acc_u 9.3750 (13.5795) lr 1.3137e-04 eta 0:00:07
epoch [169/200] batch [60/71] time 0.485 (0.474) data 0.354 (0.343) loss_u loss_u 0.8560 (0.8855) acc_u 15.6250 (13.9583) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [65/71] time 0.497 (0.477) data 0.367 (0.346) loss_u loss_u 0.8516 (0.8850) acc_u 18.7500 (14.1827) lr 1.3137e-04 eta 0:00:02
epoch [169/200] batch [70/71] time 0.542 (0.477) data 0.411 (0.346) loss_u loss_u 0.9038 (0.8860) acc_u 9.3750 (14.0179) lr 1.3137e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2087
confident_label rate tensor(0.2656, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 833
clean true:381
clean false:452
clean_rate:0.45738295318127253
noisy true:668
noisy false:1635
after delete: len(clean_dataset) 833
after delete: len(noisy_dataset) 2303
epoch [170/200] batch [5/26] time 0.473 (0.438) data 0.342 (0.308) loss_x loss_x 1.1660 (1.2166) acc_x 71.8750 (70.0000) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [10/26] time 0.447 (0.446) data 0.316 (0.315) loss_x loss_x 1.1670 (1.1440) acc_x 75.0000 (73.7500) lr 1.2369e-04 eta 0:00:07
epoch [170/200] batch [15/26] time 0.459 (0.441) data 0.328 (0.310) loss_x loss_x 1.2959 (1.1425) acc_x 62.5000 (72.0833) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [20/26] time 0.384 (0.435) data 0.252 (0.303) loss_x loss_x 0.8857 (1.1458) acc_x 75.0000 (72.3438) lr 1.2369e-04 eta 0:00:02
epoch [170/200] batch [25/26] time 0.510 (0.442) data 0.379 (0.311) loss_x loss_x 1.3633 (1.1920) acc_x 62.5000 (71.0000) lr 1.2369e-04 eta 0:00:00
epoch [170/200] batch [5/71] time 0.638 (0.444) data 0.505 (0.313) loss_u loss_u 0.8989 (0.9069) acc_u 9.3750 (10.6250) lr 1.2369e-04 eta 0:00:29
epoch [170/200] batch [10/71] time 0.479 (0.449) data 0.347 (0.318) loss_u loss_u 0.9380 (0.9218) acc_u 6.2500 (8.1250) lr 1.2369e-04 eta 0:00:27
epoch [170/200] batch [15/71] time 0.466 (0.453) data 0.333 (0.322) loss_u loss_u 0.9053 (0.9093) acc_u 9.3750 (10.4167) lr 1.2369e-04 eta 0:00:25
epoch [170/200] batch [20/71] time 0.488 (0.451) data 0.357 (0.320) loss_u loss_u 0.9229 (0.9083) acc_u 9.3750 (11.4062) lr 1.2369e-04 eta 0:00:23
epoch [170/200] batch [25/71] time 0.405 (0.450) data 0.274 (0.318) loss_u loss_u 0.8936 (0.9039) acc_u 12.5000 (11.8750) lr 1.2369e-04 eta 0:00:20
epoch [170/200] batch [30/71] time 0.531 (0.457) data 0.401 (0.326) loss_u loss_u 0.8052 (0.8986) acc_u 25.0000 (12.3958) lr 1.2369e-04 eta 0:00:18
epoch [170/200] batch [35/71] time 0.632 (0.458) data 0.499 (0.327) loss_u loss_u 0.8936 (0.8965) acc_u 12.5000 (12.5893) lr 1.2369e-04 eta 0:00:16
epoch [170/200] batch [40/71] time 0.599 (0.462) data 0.468 (0.330) loss_u loss_u 0.8960 (0.8931) acc_u 18.7500 (13.2031) lr 1.2369e-04 eta 0:00:14
epoch [170/200] batch [45/71] time 0.438 (0.461) data 0.307 (0.329) loss_u loss_u 0.9580 (0.8941) acc_u 3.1250 (13.2639) lr 1.2369e-04 eta 0:00:11
epoch [170/200] batch [50/71] time 0.378 (0.458) data 0.248 (0.327) loss_u loss_u 0.8779 (0.8951) acc_u 15.6250 (13.0625) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [55/71] time 0.573 (0.463) data 0.443 (0.331) loss_u loss_u 0.8501 (0.8955) acc_u 18.7500 (13.1818) lr 1.2369e-04 eta 0:00:07
epoch [170/200] batch [60/71] time 0.425 (0.462) data 0.294 (0.330) loss_u loss_u 0.8540 (0.8937) acc_u 18.7500 (13.4896) lr 1.2369e-04 eta 0:00:05
epoch [170/200] batch [65/71] time 0.466 (0.459) data 0.335 (0.328) loss_u loss_u 0.9102 (0.8923) acc_u 12.5000 (13.7981) lr 1.2369e-04 eta 0:00:02
epoch [170/200] batch [70/71] time 0.426 (0.461) data 0.293 (0.330) loss_u loss_u 0.9268 (0.8917) acc_u 15.6250 (13.9732) lr 1.2369e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2076
confident_label rate tensor(0.2787, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 874
clean true:379
clean false:495
clean_rate:0.43363844393592677
noisy true:681
noisy false:1581
after delete: len(clean_dataset) 874
after delete: len(noisy_dataset) 2262
epoch [171/200] batch [5/27] time 0.401 (0.472) data 0.269 (0.341) loss_x loss_x 0.9126 (1.0696) acc_x 68.7500 (68.7500) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [10/27] time 0.344 (0.464) data 0.213 (0.332) loss_x loss_x 1.5195 (1.1760) acc_x 65.6250 (68.7500) lr 1.1623e-04 eta 0:00:07
epoch [171/200] batch [15/27] time 0.557 (0.479) data 0.426 (0.348) loss_x loss_x 1.1133 (1.2109) acc_x 68.7500 (69.1667) lr 1.1623e-04 eta 0:00:05
epoch [171/200] batch [20/27] time 0.481 (0.479) data 0.350 (0.348) loss_x loss_x 1.1318 (1.1730) acc_x 75.0000 (69.6875) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [25/27] time 0.458 (0.475) data 0.327 (0.344) loss_x loss_x 1.2324 (1.1603) acc_x 71.8750 (70.3750) lr 1.1623e-04 eta 0:00:00
epoch [171/200] batch [5/70] time 0.456 (0.469) data 0.324 (0.338) loss_u loss_u 0.8613 (0.8834) acc_u 15.6250 (13.7500) lr 1.1623e-04 eta 0:00:30
epoch [171/200] batch [10/70] time 0.468 (0.462) data 0.336 (0.331) loss_u loss_u 0.9160 (0.8922) acc_u 9.3750 (12.5000) lr 1.1623e-04 eta 0:00:27
epoch [171/200] batch [15/70] time 0.514 (0.457) data 0.382 (0.326) loss_u loss_u 0.8936 (0.8912) acc_u 12.5000 (13.7500) lr 1.1623e-04 eta 0:00:25
epoch [171/200] batch [20/70] time 0.384 (0.455) data 0.253 (0.324) loss_u loss_u 0.8457 (0.8926) acc_u 21.8750 (13.2812) lr 1.1623e-04 eta 0:00:22
epoch [171/200] batch [25/70] time 0.556 (0.456) data 0.425 (0.325) loss_u loss_u 0.9419 (0.8928) acc_u 3.1250 (13.3750) lr 1.1623e-04 eta 0:00:20
epoch [171/200] batch [30/70] time 0.387 (0.457) data 0.257 (0.326) loss_u loss_u 0.8892 (0.8932) acc_u 15.6250 (13.2292) lr 1.1623e-04 eta 0:00:18
epoch [171/200] batch [35/70] time 0.401 (0.459) data 0.270 (0.328) loss_u loss_u 0.8984 (0.8939) acc_u 15.6250 (13.3036) lr 1.1623e-04 eta 0:00:16
epoch [171/200] batch [40/70] time 0.463 (0.459) data 0.332 (0.328) loss_u loss_u 0.9028 (0.8923) acc_u 18.7500 (13.5156) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [45/70] time 0.520 (0.463) data 0.389 (0.332) loss_u loss_u 0.9385 (0.8931) acc_u 9.3750 (13.4722) lr 1.1623e-04 eta 0:00:11
epoch [171/200] batch [50/70] time 0.506 (0.464) data 0.376 (0.333) loss_u loss_u 0.8691 (0.8941) acc_u 15.6250 (13.3750) lr 1.1623e-04 eta 0:00:09
epoch [171/200] batch [55/70] time 0.434 (0.466) data 0.302 (0.334) loss_u loss_u 0.9463 (0.8959) acc_u 9.3750 (13.1250) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [60/70] time 0.492 (0.468) data 0.360 (0.337) loss_u loss_u 0.8892 (0.8935) acc_u 12.5000 (13.3333) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [65/70] time 0.383 (0.467) data 0.252 (0.336) loss_u loss_u 0.9238 (0.8959) acc_u 6.2500 (12.9808) lr 1.1623e-04 eta 0:00:02
epoch [171/200] batch [70/70] time 0.394 (0.466) data 0.262 (0.335) loss_u loss_u 0.8911 (0.8956) acc_u 15.6250 (13.1250) lr 1.1623e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2051
confident_label rate tensor(0.2663, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 835
clean true:384
clean false:451
clean_rate:0.4598802395209581
noisy true:701
noisy false:1600
after delete: len(clean_dataset) 835
after delete: len(noisy_dataset) 2301
epoch [172/200] batch [5/26] time 0.651 (0.550) data 0.520 (0.420) loss_x loss_x 1.2695 (1.2498) acc_x 62.5000 (66.2500) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [10/26] time 0.485 (0.488) data 0.353 (0.357) loss_x loss_x 0.9331 (1.1861) acc_x 78.1250 (68.7500) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [15/26] time 0.387 (0.473) data 0.257 (0.342) loss_x loss_x 1.4521 (1.1815) acc_x 62.5000 (69.5833) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [20/26] time 0.488 (0.464) data 0.358 (0.333) loss_x loss_x 1.0273 (1.1620) acc_x 75.0000 (69.5312) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [25/26] time 0.422 (0.463) data 0.291 (0.333) loss_x loss_x 0.9917 (1.1546) acc_x 68.7500 (69.6250) lr 1.0899e-04 eta 0:00:00
epoch [172/200] batch [5/71] time 0.499 (0.462) data 0.369 (0.331) loss_u loss_u 0.8716 (0.8714) acc_u 15.6250 (14.3750) lr 1.0899e-04 eta 0:00:30
epoch [172/200] batch [10/71] time 0.407 (0.466) data 0.275 (0.335) loss_u loss_u 0.8975 (0.8804) acc_u 12.5000 (15.3125) lr 1.0899e-04 eta 0:00:28
epoch [172/200] batch [15/71] time 0.429 (0.467) data 0.296 (0.336) loss_u loss_u 0.8525 (0.8791) acc_u 15.6250 (15.6250) lr 1.0899e-04 eta 0:00:26
epoch [172/200] batch [20/71] time 0.824 (0.470) data 0.693 (0.339) loss_u loss_u 0.9199 (0.8810) acc_u 6.2500 (14.8438) lr 1.0899e-04 eta 0:00:23
epoch [172/200] batch [25/71] time 0.376 (0.469) data 0.244 (0.338) loss_u loss_u 0.8760 (0.8760) acc_u 15.6250 (16.0000) lr 1.0899e-04 eta 0:00:21
epoch [172/200] batch [30/71] time 0.499 (0.469) data 0.367 (0.337) loss_u loss_u 0.8521 (0.8794) acc_u 25.0000 (15.6250) lr 1.0899e-04 eta 0:00:19
epoch [172/200] batch [35/71] time 0.526 (0.469) data 0.394 (0.338) loss_u loss_u 0.8608 (0.8747) acc_u 12.5000 (16.0714) lr 1.0899e-04 eta 0:00:16
epoch [172/200] batch [40/71] time 0.433 (0.469) data 0.302 (0.338) loss_u loss_u 0.8745 (0.8781) acc_u 15.6250 (15.3125) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [45/71] time 0.411 (0.468) data 0.281 (0.336) loss_u loss_u 0.8945 (0.8767) acc_u 21.8750 (15.4861) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [50/71] time 0.435 (0.464) data 0.304 (0.332) loss_u loss_u 0.8867 (0.8779) acc_u 15.6250 (15.1250) lr 1.0899e-04 eta 0:00:09
epoch [172/200] batch [55/71] time 0.465 (0.461) data 0.334 (0.330) loss_u loss_u 0.8833 (0.8821) acc_u 15.6250 (14.5455) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [60/71] time 0.428 (0.461) data 0.297 (0.330) loss_u loss_u 0.8428 (0.8827) acc_u 18.7500 (14.4792) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [65/71] time 0.466 (0.463) data 0.335 (0.332) loss_u loss_u 0.8960 (0.8825) acc_u 15.6250 (14.6635) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [70/71] time 0.482 (0.462) data 0.352 (0.330) loss_u loss_u 0.9043 (0.8849) acc_u 9.3750 (14.3750) lr 1.0899e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2080
confident_label rate tensor(0.2691, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 844
clean true:377
clean false:467
clean_rate:0.4466824644549763
noisy true:679
noisy false:1613
after delete: len(clean_dataset) 844
after delete: len(noisy_dataset) 2292
epoch [173/200] batch [5/26] time 0.404 (0.498) data 0.273 (0.366) loss_x loss_x 1.5820 (1.3018) acc_x 53.1250 (68.1250) lr 1.0197e-04 eta 0:00:10
epoch [173/200] batch [10/26] time 0.551 (0.497) data 0.421 (0.365) loss_x loss_x 1.0801 (1.3129) acc_x 71.8750 (68.1250) lr 1.0197e-04 eta 0:00:07
epoch [173/200] batch [15/26] time 0.371 (0.468) data 0.240 (0.337) loss_x loss_x 1.6104 (1.3187) acc_x 65.6250 (66.8750) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [20/26] time 0.471 (0.463) data 0.340 (0.333) loss_x loss_x 1.1592 (1.2332) acc_x 75.0000 (69.3750) lr 1.0197e-04 eta 0:00:02
epoch [173/200] batch [25/26] time 0.348 (0.452) data 0.218 (0.321) loss_x loss_x 1.2822 (1.2319) acc_x 65.6250 (68.5000) lr 1.0197e-04 eta 0:00:00
epoch [173/200] batch [5/71] time 0.358 (0.446) data 0.227 (0.315) loss_u loss_u 0.8569 (0.8730) acc_u 15.6250 (18.1250) lr 1.0197e-04 eta 0:00:29
epoch [173/200] batch [10/71] time 0.377 (0.445) data 0.245 (0.314) loss_u loss_u 0.8784 (0.8868) acc_u 18.7500 (16.5625) lr 1.0197e-04 eta 0:00:27
epoch [173/200] batch [15/71] time 0.491 (0.442) data 0.359 (0.311) loss_u loss_u 0.9302 (0.8967) acc_u 9.3750 (14.7917) lr 1.0197e-04 eta 0:00:24
epoch [173/200] batch [20/71] time 0.433 (0.447) data 0.302 (0.315) loss_u loss_u 0.9141 (0.8993) acc_u 15.6250 (14.3750) lr 1.0197e-04 eta 0:00:22
epoch [173/200] batch [25/71] time 0.452 (0.443) data 0.321 (0.312) loss_u loss_u 0.8892 (0.8934) acc_u 12.5000 (14.5000) lr 1.0197e-04 eta 0:00:20
epoch [173/200] batch [30/71] time 0.411 (0.445) data 0.279 (0.314) loss_u loss_u 0.8564 (0.8910) acc_u 18.7500 (14.4792) lr 1.0197e-04 eta 0:00:18
epoch [173/200] batch [35/71] time 0.366 (0.448) data 0.233 (0.317) loss_u loss_u 0.9136 (0.8869) acc_u 6.2500 (14.9107) lr 1.0197e-04 eta 0:00:16
epoch [173/200] batch [40/71] time 0.366 (0.446) data 0.234 (0.315) loss_u loss_u 0.8716 (0.8874) acc_u 15.6250 (14.8438) lr 1.0197e-04 eta 0:00:13
epoch [173/200] batch [45/71] time 0.371 (0.443) data 0.240 (0.311) loss_u loss_u 0.8008 (0.8870) acc_u 28.1250 (15.0694) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [50/71] time 0.396 (0.439) data 0.266 (0.308) loss_u loss_u 0.8765 (0.8881) acc_u 12.5000 (14.8125) lr 1.0197e-04 eta 0:00:09
epoch [173/200] batch [55/71] time 0.445 (0.439) data 0.313 (0.308) loss_u loss_u 0.8320 (0.8840) acc_u 25.0000 (15.3977) lr 1.0197e-04 eta 0:00:07
epoch [173/200] batch [60/71] time 0.401 (0.442) data 0.270 (0.310) loss_u loss_u 0.9263 (0.8859) acc_u 9.3750 (14.9479) lr 1.0197e-04 eta 0:00:04
epoch [173/200] batch [65/71] time 0.441 (0.449) data 0.309 (0.318) loss_u loss_u 0.9795 (0.8892) acc_u 3.1250 (14.4231) lr 1.0197e-04 eta 0:00:02
epoch [173/200] batch [70/71] time 0.364 (0.449) data 0.233 (0.318) loss_u loss_u 0.9521 (0.8900) acc_u 6.2500 (14.2857) lr 1.0197e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2082
confident_label rate tensor(0.2663, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 835
clean true:390
clean false:445
clean_rate:0.46706586826347307
noisy true:664
noisy false:1637
after delete: len(clean_dataset) 835
after delete: len(noisy_dataset) 2301
epoch [174/200] batch [5/26] time 0.433 (0.432) data 0.303 (0.302) loss_x loss_x 0.7119 (1.0818) acc_x 84.3750 (71.2500) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [10/26] time 0.381 (0.455) data 0.250 (0.324) loss_x loss_x 1.6240 (1.0719) acc_x 56.2500 (72.5000) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [15/26] time 0.330 (0.444) data 0.200 (0.313) loss_x loss_x 0.9531 (1.0282) acc_x 75.0000 (73.7500) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [20/26] time 0.514 (0.453) data 0.383 (0.322) loss_x loss_x 1.3633 (1.0386) acc_x 65.6250 (73.2812) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [25/26] time 0.432 (0.455) data 0.302 (0.325) loss_x loss_x 1.1455 (1.0934) acc_x 65.6250 (72.0000) lr 9.5173e-05 eta 0:00:00
epoch [174/200] batch [5/71] time 0.401 (0.453) data 0.270 (0.322) loss_u loss_u 0.8843 (0.9121) acc_u 12.5000 (9.3750) lr 9.5173e-05 eta 0:00:29
epoch [174/200] batch [10/71] time 0.461 (0.450) data 0.329 (0.319) loss_u loss_u 0.8867 (0.9034) acc_u 18.7500 (12.8125) lr 9.5173e-05 eta 0:00:27
epoch [174/200] batch [15/71] time 0.452 (0.457) data 0.320 (0.326) loss_u loss_u 0.8848 (0.9014) acc_u 15.6250 (13.1250) lr 9.5173e-05 eta 0:00:25
epoch [174/200] batch [20/71] time 0.405 (0.456) data 0.273 (0.325) loss_u loss_u 0.9141 (0.9039) acc_u 12.5000 (12.9688) lr 9.5173e-05 eta 0:00:23
epoch [174/200] batch [25/71] time 0.482 (0.466) data 0.351 (0.335) loss_u loss_u 0.9160 (0.8932) acc_u 9.3750 (14.1250) lr 9.5173e-05 eta 0:00:21
epoch [174/200] batch [30/71] time 0.445 (0.461) data 0.314 (0.330) loss_u loss_u 0.8994 (0.8903) acc_u 9.3750 (14.0625) lr 9.5173e-05 eta 0:00:18
epoch [174/200] batch [35/71] time 0.489 (0.460) data 0.357 (0.329) loss_u loss_u 0.9009 (0.8944) acc_u 12.5000 (13.3929) lr 9.5173e-05 eta 0:00:16
epoch [174/200] batch [40/71] time 0.633 (0.461) data 0.501 (0.330) loss_u loss_u 0.9170 (0.8962) acc_u 12.5000 (13.2031) lr 9.5173e-05 eta 0:00:14
epoch [174/200] batch [45/71] time 0.562 (0.463) data 0.430 (0.332) loss_u loss_u 0.8501 (0.8942) acc_u 21.8750 (13.8194) lr 9.5173e-05 eta 0:00:12
epoch [174/200] batch [50/71] time 0.423 (0.465) data 0.291 (0.334) loss_u loss_u 0.9434 (0.8958) acc_u 6.2500 (13.5625) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [55/71] time 0.477 (0.464) data 0.345 (0.333) loss_u loss_u 0.8340 (0.8948) acc_u 18.7500 (13.6364) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [60/71] time 0.360 (0.464) data 0.228 (0.333) loss_u loss_u 0.9370 (0.8936) acc_u 9.3750 (13.8021) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [65/71] time 0.544 (0.468) data 0.412 (0.336) loss_u loss_u 0.8301 (0.8927) acc_u 25.0000 (14.0385) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [70/71] time 0.433 (0.467) data 0.302 (0.336) loss_u loss_u 0.9253 (0.8930) acc_u 9.3750 (14.1518) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2068
confident_label rate tensor(0.2761, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 866
clean true:400
clean false:466
clean_rate:0.4618937644341801
noisy true:668
noisy false:1602
after delete: len(clean_dataset) 866
after delete: len(noisy_dataset) 2270
epoch [175/200] batch [5/27] time 0.549 (0.473) data 0.418 (0.342) loss_x loss_x 1.0049 (1.1117) acc_x 71.8750 (71.2500) lr 8.8597e-05 eta 0:00:10
epoch [175/200] batch [10/27] time 0.372 (0.447) data 0.241 (0.316) loss_x loss_x 0.8843 (0.9753) acc_x 71.8750 (72.1875) lr 8.8597e-05 eta 0:00:07
epoch [175/200] batch [15/27] time 0.359 (0.448) data 0.229 (0.318) loss_x loss_x 1.3389 (1.0432) acc_x 71.8750 (72.5000) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [20/27] time 0.461 (0.448) data 0.329 (0.317) loss_x loss_x 1.3418 (1.0523) acc_x 68.7500 (71.7188) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [25/27] time 0.395 (0.455) data 0.265 (0.324) loss_x loss_x 0.9849 (1.0747) acc_x 71.8750 (72.0000) lr 8.8597e-05 eta 0:00:00
epoch [175/200] batch [5/70] time 0.547 (0.469) data 0.417 (0.338) loss_u loss_u 0.8267 (0.8494) acc_u 18.7500 (18.1250) lr 8.8597e-05 eta 0:00:30
epoch [175/200] batch [10/70] time 0.380 (0.470) data 0.249 (0.339) loss_u loss_u 0.9927 (0.8954) acc_u 0.0000 (12.5000) lr 8.8597e-05 eta 0:00:28
epoch [175/200] batch [15/70] time 0.547 (0.467) data 0.416 (0.336) loss_u loss_u 0.9668 (0.9055) acc_u 3.1250 (10.8333) lr 8.8597e-05 eta 0:00:25
epoch [175/200] batch [20/70] time 0.514 (0.465) data 0.382 (0.334) loss_u loss_u 0.8022 (0.8926) acc_u 28.1250 (12.3438) lr 8.8597e-05 eta 0:00:23
epoch [175/200] batch [25/70] time 0.444 (0.464) data 0.312 (0.333) loss_u loss_u 0.8823 (0.8913) acc_u 12.5000 (13.0000) lr 8.8597e-05 eta 0:00:20
epoch [175/200] batch [30/70] time 0.361 (0.466) data 0.230 (0.335) loss_u loss_u 0.8999 (0.8868) acc_u 9.3750 (13.6458) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [35/70] time 0.456 (0.469) data 0.325 (0.338) loss_u loss_u 0.9087 (0.8848) acc_u 15.6250 (14.3750) lr 8.8597e-05 eta 0:00:16
epoch [175/200] batch [40/70] time 0.521 (0.467) data 0.389 (0.336) loss_u loss_u 0.9170 (0.8837) acc_u 9.3750 (14.5312) lr 8.8597e-05 eta 0:00:14
epoch [175/200] batch [45/70] time 0.616 (0.468) data 0.484 (0.337) loss_u loss_u 0.8911 (0.8826) acc_u 9.3750 (14.5139) lr 8.8597e-05 eta 0:00:11
epoch [175/200] batch [50/70] time 0.492 (0.468) data 0.360 (0.337) loss_u loss_u 0.9395 (0.8842) acc_u 6.2500 (14.2500) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [55/70] time 0.475 (0.465) data 0.344 (0.334) loss_u loss_u 0.9238 (0.8865) acc_u 6.2500 (13.9773) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [60/70] time 0.357 (0.464) data 0.225 (0.333) loss_u loss_u 0.8481 (0.8869) acc_u 18.7500 (14.1146) lr 8.8597e-05 eta 0:00:04
epoch [175/200] batch [65/70] time 0.504 (0.463) data 0.374 (0.332) loss_u loss_u 0.8809 (0.8883) acc_u 15.6250 (14.0865) lr 8.8597e-05 eta 0:00:02
epoch [175/200] batch [70/70] time 0.511 (0.463) data 0.380 (0.332) loss_u loss_u 0.8979 (0.8885) acc_u 12.5000 (13.8839) lr 8.8597e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2086
confident_label rate tensor(0.2717, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 852
clean true:369
clean false:483
clean_rate:0.43309859154929575
noisy true:681
noisy false:1603
after delete: len(clean_dataset) 852
after delete: len(noisy_dataset) 2284
epoch [176/200] batch [5/26] time 0.330 (0.450) data 0.199 (0.319) loss_x loss_x 1.1250 (1.1936) acc_x 68.7500 (70.0000) lr 8.2245e-05 eta 0:00:09
epoch [176/200] batch [10/26] time 0.416 (0.459) data 0.285 (0.328) loss_x loss_x 1.1807 (1.0833) acc_x 75.0000 (70.0000) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [15/26] time 0.469 (0.453) data 0.338 (0.322) loss_x loss_x 0.8140 (1.1352) acc_x 84.3750 (69.5833) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [20/26] time 0.464 (0.463) data 0.334 (0.332) loss_x loss_x 1.4678 (1.1356) acc_x 68.7500 (70.4688) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [25/26] time 0.498 (0.458) data 0.367 (0.327) loss_x loss_x 0.7935 (1.1056) acc_x 75.0000 (71.2500) lr 8.2245e-05 eta 0:00:00
epoch [176/200] batch [5/71] time 0.457 (0.459) data 0.325 (0.329) loss_u loss_u 0.8486 (0.9066) acc_u 18.7500 (11.2500) lr 8.2245e-05 eta 0:00:30
epoch [176/200] batch [10/71] time 0.557 (0.467) data 0.426 (0.336) loss_u loss_u 0.9014 (0.8893) acc_u 15.6250 (14.6875) lr 8.2245e-05 eta 0:00:28
epoch [176/200] batch [15/71] time 0.452 (0.468) data 0.321 (0.337) loss_u loss_u 0.8184 (0.8821) acc_u 25.0000 (15.6250) lr 8.2245e-05 eta 0:00:26
epoch [176/200] batch [20/71] time 0.414 (0.467) data 0.284 (0.336) loss_u loss_u 0.9214 (0.8905) acc_u 9.3750 (14.3750) lr 8.2245e-05 eta 0:00:23
epoch [176/200] batch [25/71] time 0.429 (0.463) data 0.297 (0.332) loss_u loss_u 0.8784 (0.8946) acc_u 21.8750 (14.0000) lr 8.2245e-05 eta 0:00:21
epoch [176/200] batch [30/71] time 0.480 (0.467) data 0.349 (0.336) loss_u loss_u 0.8198 (0.8933) acc_u 21.8750 (13.9583) lr 8.2245e-05 eta 0:00:19
epoch [176/200] batch [35/71] time 0.319 (0.464) data 0.188 (0.333) loss_u loss_u 0.9771 (0.8902) acc_u 3.1250 (14.4643) lr 8.2245e-05 eta 0:00:16
epoch [176/200] batch [40/71] time 0.359 (0.461) data 0.227 (0.330) loss_u loss_u 0.8169 (0.8901) acc_u 21.8750 (14.4531) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [45/71] time 0.573 (0.464) data 0.441 (0.333) loss_u loss_u 0.8926 (0.8897) acc_u 12.5000 (14.3750) lr 8.2245e-05 eta 0:00:12
epoch [176/200] batch [50/71] time 0.460 (0.464) data 0.328 (0.333) loss_u loss_u 0.9414 (0.8896) acc_u 9.3750 (14.3125) lr 8.2245e-05 eta 0:00:09
epoch [176/200] batch [55/71] time 0.373 (0.463) data 0.241 (0.331) loss_u loss_u 0.9004 (0.8904) acc_u 12.5000 (14.2045) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [60/71] time 0.391 (0.461) data 0.259 (0.330) loss_u loss_u 0.8472 (0.8902) acc_u 21.8750 (14.4271) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [65/71] time 0.597 (0.461) data 0.465 (0.329) loss_u loss_u 0.9268 (0.8923) acc_u 9.3750 (14.0385) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [70/71] time 0.488 (0.458) data 0.356 (0.327) loss_u loss_u 0.9097 (0.8925) acc_u 6.2500 (13.8393) lr 8.2245e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2104
confident_label rate tensor(0.2659, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 834
clean true:376
clean false:458
clean_rate:0.45083932853717024
noisy true:656
noisy false:1646
after delete: len(clean_dataset) 834
after delete: len(noisy_dataset) 2302
epoch [177/200] batch [5/26] time 0.574 (0.505) data 0.443 (0.374) loss_x loss_x 0.8857 (1.3572) acc_x 78.1250 (69.3750) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [10/26] time 0.417 (0.471) data 0.286 (0.340) loss_x loss_x 0.6118 (1.1969) acc_x 84.3750 (71.8750) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [15/26] time 0.545 (0.461) data 0.413 (0.330) loss_x loss_x 0.5493 (1.1398) acc_x 90.6250 (71.4583) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [20/26] time 0.424 (0.467) data 0.293 (0.335) loss_x loss_x 0.7549 (1.0750) acc_x 71.8750 (72.9688) lr 7.6120e-05 eta 0:00:02
epoch [177/200] batch [25/26] time 0.445 (0.463) data 0.315 (0.332) loss_x loss_x 1.0537 (1.0447) acc_x 75.0000 (74.1250) lr 7.6120e-05 eta 0:00:00
epoch [177/200] batch [5/71] time 0.490 (0.468) data 0.358 (0.336) loss_u loss_u 0.8960 (0.8858) acc_u 12.5000 (15.0000) lr 7.6120e-05 eta 0:00:30
epoch [177/200] batch [10/71] time 0.483 (0.469) data 0.351 (0.338) loss_u loss_u 0.9136 (0.8987) acc_u 12.5000 (13.7500) lr 7.6120e-05 eta 0:00:28
epoch [177/200] batch [15/71] time 0.476 (0.469) data 0.344 (0.338) loss_u loss_u 0.9619 (0.8952) acc_u 3.1250 (14.1667) lr 7.6120e-05 eta 0:00:26
epoch [177/200] batch [20/71] time 0.515 (0.467) data 0.385 (0.335) loss_u loss_u 0.8350 (0.8860) acc_u 18.7500 (15.0000) lr 7.6120e-05 eta 0:00:23
epoch [177/200] batch [25/71] time 0.502 (0.469) data 0.371 (0.338) loss_u loss_u 0.9126 (0.8883) acc_u 6.2500 (14.0000) lr 7.6120e-05 eta 0:00:21
epoch [177/200] batch [30/71] time 0.377 (0.470) data 0.247 (0.339) loss_u loss_u 0.9023 (0.8865) acc_u 12.5000 (14.4792) lr 7.6120e-05 eta 0:00:19
epoch [177/200] batch [35/71] time 0.449 (0.476) data 0.319 (0.345) loss_u loss_u 0.9336 (0.8910) acc_u 18.7500 (14.1071) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [40/71] time 0.501 (0.477) data 0.369 (0.346) loss_u loss_u 0.8643 (0.8892) acc_u 12.5000 (14.2188) lr 7.6120e-05 eta 0:00:14
epoch [177/200] batch [45/71] time 0.495 (0.476) data 0.364 (0.345) loss_u loss_u 0.8560 (0.8878) acc_u 15.6250 (14.3056) lr 7.6120e-05 eta 0:00:12
epoch [177/200] batch [50/71] time 0.519 (0.475) data 0.388 (0.344) loss_u loss_u 0.9526 (0.8852) acc_u 3.1250 (14.7500) lr 7.6120e-05 eta 0:00:09
epoch [177/200] batch [55/71] time 0.457 (0.474) data 0.325 (0.343) loss_u loss_u 0.8994 (0.8841) acc_u 12.5000 (14.8864) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [60/71] time 0.466 (0.475) data 0.336 (0.343) loss_u loss_u 0.8335 (0.8849) acc_u 18.7500 (14.7917) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [65/71] time 0.409 (0.473) data 0.278 (0.342) loss_u loss_u 0.8862 (0.8844) acc_u 12.5000 (14.8558) lr 7.6120e-05 eta 0:00:02
epoch [177/200] batch [70/71] time 0.591 (0.473) data 0.459 (0.342) loss_u loss_u 0.8838 (0.8822) acc_u 15.6250 (15.1339) lr 7.6120e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2052
confident_label rate tensor(0.2774, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 870
clean true:396
clean false:474
clean_rate:0.45517241379310347
noisy true:688
noisy false:1578
after delete: len(clean_dataset) 870
after delete: len(noisy_dataset) 2266
epoch [178/200] batch [5/27] time 0.589 (0.466) data 0.458 (0.335) loss_x loss_x 1.3779 (0.9836) acc_x 75.0000 (77.5000) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [10/27] time 0.491 (0.451) data 0.361 (0.320) loss_x loss_x 1.2793 (1.0680) acc_x 78.1250 (75.6250) lr 7.0224e-05 eta 0:00:07
epoch [178/200] batch [15/27] time 0.564 (0.454) data 0.434 (0.324) loss_x loss_x 1.1777 (1.0439) acc_x 65.6250 (75.0000) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [20/27] time 0.416 (0.454) data 0.286 (0.324) loss_x loss_x 1.2373 (1.0645) acc_x 65.6250 (73.9062) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [25/27] time 0.404 (0.455) data 0.273 (0.325) loss_x loss_x 1.0586 (1.0815) acc_x 68.7500 (73.6250) lr 7.0224e-05 eta 0:00:00
epoch [178/200] batch [5/70] time 0.374 (0.455) data 0.242 (0.324) loss_u loss_u 0.8413 (0.8970) acc_u 18.7500 (13.1250) lr 7.0224e-05 eta 0:00:29
epoch [178/200] batch [10/70] time 0.388 (0.454) data 0.256 (0.323) loss_u loss_u 0.9229 (0.9074) acc_u 15.6250 (11.8750) lr 7.0224e-05 eta 0:00:27
epoch [178/200] batch [15/70] time 0.375 (0.458) data 0.243 (0.327) loss_u loss_u 0.9023 (0.9120) acc_u 12.5000 (11.6667) lr 7.0224e-05 eta 0:00:25
epoch [178/200] batch [20/70] time 0.454 (0.455) data 0.322 (0.324) loss_u loss_u 0.8892 (0.8994) acc_u 12.5000 (13.4375) lr 7.0224e-05 eta 0:00:22
epoch [178/200] batch [25/70] time 0.410 (0.454) data 0.279 (0.323) loss_u loss_u 0.9009 (0.9050) acc_u 12.5000 (12.6250) lr 7.0224e-05 eta 0:00:20
epoch [178/200] batch [30/70] time 0.635 (0.456) data 0.504 (0.325) loss_u loss_u 0.9180 (0.8998) acc_u 9.3750 (13.5417) lr 7.0224e-05 eta 0:00:18
epoch [178/200] batch [35/70] time 0.465 (0.458) data 0.334 (0.326) loss_u loss_u 0.9595 (0.9022) acc_u 3.1250 (12.9464) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [40/70] time 0.360 (0.455) data 0.229 (0.323) loss_u loss_u 0.9019 (0.9048) acc_u 6.2500 (12.4219) lr 7.0224e-05 eta 0:00:13
epoch [178/200] batch [45/70] time 0.593 (0.458) data 0.461 (0.327) loss_u loss_u 0.9043 (0.9008) acc_u 12.5000 (12.5694) lr 7.0224e-05 eta 0:00:11
epoch [178/200] batch [50/70] time 0.548 (0.457) data 0.416 (0.326) loss_u loss_u 0.8672 (0.8994) acc_u 15.6250 (12.5625) lr 7.0224e-05 eta 0:00:09
epoch [178/200] batch [55/70] time 0.508 (0.457) data 0.377 (0.326) loss_u loss_u 0.8501 (0.8949) acc_u 15.6250 (13.2386) lr 7.0224e-05 eta 0:00:06
epoch [178/200] batch [60/70] time 0.427 (0.456) data 0.295 (0.325) loss_u loss_u 0.8149 (0.8908) acc_u 21.8750 (13.4896) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [65/70] time 0.400 (0.456) data 0.269 (0.325) loss_u loss_u 0.9019 (0.8929) acc_u 12.5000 (13.2212) lr 7.0224e-05 eta 0:00:02
epoch [178/200] batch [70/70] time 0.398 (0.456) data 0.266 (0.325) loss_u loss_u 0.9478 (0.8920) acc_u 9.3750 (13.3036) lr 7.0224e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2068
confident_label rate tensor(0.2771, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 869
clean true:386
clean false:483
clean_rate:0.4441887226697353
noisy true:682
noisy false:1585
after delete: len(clean_dataset) 869
after delete: len(noisy_dataset) 2267
epoch [179/200] batch [5/27] time 0.415 (0.479) data 0.285 (0.348) loss_x loss_x 1.5400 (1.1217) acc_x 56.2500 (68.1250) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [10/27] time 0.425 (0.471) data 0.294 (0.340) loss_x loss_x 0.8354 (1.0036) acc_x 75.0000 (71.8750) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [15/27] time 0.484 (0.458) data 0.353 (0.327) loss_x loss_x 1.0576 (1.0261) acc_x 65.6250 (71.0417) lr 6.4556e-05 eta 0:00:05
epoch [179/200] batch [20/27] time 0.378 (0.446) data 0.247 (0.315) loss_x loss_x 1.5801 (1.0787) acc_x 62.5000 (70.1562) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [25/27] time 0.594 (0.464) data 0.463 (0.333) loss_x loss_x 1.7949 (1.0989) acc_x 59.3750 (69.8750) lr 6.4556e-05 eta 0:00:00
epoch [179/200] batch [5/70] time 0.423 (0.456) data 0.291 (0.325) loss_u loss_u 0.9258 (0.8837) acc_u 9.3750 (15.0000) lr 6.4556e-05 eta 0:00:29
epoch [179/200] batch [10/70] time 0.431 (0.457) data 0.299 (0.326) loss_u loss_u 0.9004 (0.8840) acc_u 12.5000 (14.6875) lr 6.4556e-05 eta 0:00:27
epoch [179/200] batch [15/70] time 0.429 (0.454) data 0.297 (0.322) loss_u loss_u 0.8281 (0.8914) acc_u 21.8750 (13.1250) lr 6.4556e-05 eta 0:00:24
epoch [179/200] batch [20/70] time 0.389 (0.450) data 0.257 (0.319) loss_u loss_u 0.9038 (0.8874) acc_u 12.5000 (14.0625) lr 6.4556e-05 eta 0:00:22
epoch [179/200] batch [25/70] time 0.633 (0.453) data 0.501 (0.322) loss_u loss_u 0.8618 (0.8862) acc_u 18.7500 (14.1250) lr 6.4556e-05 eta 0:00:20
epoch [179/200] batch [30/70] time 0.467 (0.449) data 0.336 (0.318) loss_u loss_u 0.8618 (0.8855) acc_u 18.7500 (14.0625) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [35/70] time 0.447 (0.455) data 0.316 (0.324) loss_u loss_u 0.9531 (0.8882) acc_u 6.2500 (13.6607) lr 6.4556e-05 eta 0:00:15
epoch [179/200] batch [40/70] time 0.583 (0.455) data 0.451 (0.324) loss_u loss_u 0.9116 (0.8873) acc_u 12.5000 (13.7500) lr 6.4556e-05 eta 0:00:13
epoch [179/200] batch [45/70] time 0.352 (0.454) data 0.221 (0.323) loss_u loss_u 0.8848 (0.8875) acc_u 12.5000 (13.6806) lr 6.4556e-05 eta 0:00:11
epoch [179/200] batch [50/70] time 0.707 (0.460) data 0.575 (0.329) loss_u loss_u 0.8550 (0.8872) acc_u 15.6250 (13.5000) lr 6.4556e-05 eta 0:00:09
epoch [179/200] batch [55/70] time 0.555 (0.464) data 0.421 (0.333) loss_u loss_u 0.9482 (0.8902) acc_u 3.1250 (13.1818) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [60/70] time 0.526 (0.466) data 0.394 (0.335) loss_u loss_u 0.9316 (0.8913) acc_u 6.2500 (13.1771) lr 6.4556e-05 eta 0:00:04
epoch [179/200] batch [65/70] time 0.460 (0.468) data 0.329 (0.336) loss_u loss_u 0.8770 (0.8912) acc_u 18.7500 (13.2212) lr 6.4556e-05 eta 0:00:02
epoch [179/200] batch [70/70] time 0.435 (0.465) data 0.305 (0.334) loss_u loss_u 0.9272 (0.8913) acc_u 12.5000 (13.2589) lr 6.4556e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2060
confident_label rate tensor(0.2701, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 847
clean true:375
clean false:472
clean_rate:0.4427390791027155
noisy true:701
noisy false:1588
after delete: len(clean_dataset) 847
after delete: len(noisy_dataset) 2289
epoch [180/200] batch [5/26] time 0.375 (0.439) data 0.244 (0.308) loss_x loss_x 0.8696 (1.1694) acc_x 71.8750 (68.7500) lr 5.9119e-05 eta 0:00:09
epoch [180/200] batch [10/26] time 0.431 (0.439) data 0.301 (0.309) loss_x loss_x 0.9683 (1.1258) acc_x 78.1250 (71.5625) lr 5.9119e-05 eta 0:00:07
epoch [180/200] batch [15/26] time 0.414 (0.463) data 0.283 (0.332) loss_x loss_x 1.4775 (1.1307) acc_x 65.6250 (71.8750) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [20/26] time 0.456 (0.470) data 0.325 (0.340) loss_x loss_x 1.2363 (1.1371) acc_x 62.5000 (71.5625) lr 5.9119e-05 eta 0:00:02
epoch [180/200] batch [25/26] time 0.453 (0.468) data 0.322 (0.337) loss_x loss_x 0.8096 (1.1366) acc_x 81.2500 (71.3750) lr 5.9119e-05 eta 0:00:00
epoch [180/200] batch [5/71] time 0.409 (0.468) data 0.278 (0.337) loss_u loss_u 0.8145 (0.8760) acc_u 25.0000 (15.0000) lr 5.9119e-05 eta 0:00:30
epoch [180/200] batch [10/71] time 0.376 (0.459) data 0.244 (0.328) loss_u loss_u 0.9028 (0.8687) acc_u 15.6250 (17.1875) lr 5.9119e-05 eta 0:00:27
epoch [180/200] batch [15/71] time 0.557 (0.457) data 0.426 (0.326) loss_u loss_u 0.8784 (0.8739) acc_u 15.6250 (16.4583) lr 5.9119e-05 eta 0:00:25
epoch [180/200] batch [20/71] time 0.402 (0.459) data 0.271 (0.328) loss_u loss_u 0.8853 (0.8808) acc_u 9.3750 (15.1562) lr 5.9119e-05 eta 0:00:23
epoch [180/200] batch [25/71] time 0.467 (0.465) data 0.336 (0.334) loss_u loss_u 0.9336 (0.8850) acc_u 9.3750 (14.2500) lr 5.9119e-05 eta 0:00:21
epoch [180/200] batch [30/71] time 0.391 (0.461) data 0.261 (0.330) loss_u loss_u 0.7998 (0.8861) acc_u 28.1250 (13.9583) lr 5.9119e-05 eta 0:00:18
epoch [180/200] batch [35/71] time 0.459 (0.461) data 0.328 (0.330) loss_u loss_u 0.9224 (0.8850) acc_u 9.3750 (13.8393) lr 5.9119e-05 eta 0:00:16
epoch [180/200] batch [40/71] time 0.496 (0.462) data 0.365 (0.331) loss_u loss_u 0.9570 (0.8877) acc_u 3.1250 (13.5938) lr 5.9119e-05 eta 0:00:14
epoch [180/200] batch [45/71] time 0.469 (0.463) data 0.337 (0.332) loss_u loss_u 0.9370 (0.8882) acc_u 12.5000 (13.6111) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [50/71] time 0.456 (0.463) data 0.324 (0.332) loss_u loss_u 0.8618 (0.8878) acc_u 18.7500 (13.8750) lr 5.9119e-05 eta 0:00:09
epoch [180/200] batch [55/71] time 0.366 (0.459) data 0.235 (0.328) loss_u loss_u 0.8887 (0.8894) acc_u 21.8750 (13.8068) lr 5.9119e-05 eta 0:00:07
epoch [180/200] batch [60/71] time 0.446 (0.461) data 0.312 (0.330) loss_u loss_u 0.9087 (0.8915) acc_u 12.5000 (13.5417) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [65/71] time 0.582 (0.460) data 0.450 (0.329) loss_u loss_u 0.9253 (0.8925) acc_u 6.2500 (13.4615) lr 5.9119e-05 eta 0:00:02
epoch [180/200] batch [70/71] time 0.417 (0.461) data 0.287 (0.329) loss_u loss_u 0.9233 (0.8901) acc_u 6.2500 (13.8393) lr 5.9119e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2067
confident_label rate tensor(0.2739, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 859
clean true:379
clean false:480
clean_rate:0.4412107101280559
noisy true:690
noisy false:1587
after delete: len(clean_dataset) 859
after delete: len(noisy_dataset) 2277
epoch [181/200] batch [5/26] time 0.475 (0.454) data 0.344 (0.323) loss_x loss_x 1.4512 (1.1254) acc_x 65.6250 (74.3750) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [10/26] time 0.587 (0.486) data 0.455 (0.355) loss_x loss_x 1.1445 (1.1688) acc_x 75.0000 (72.8125) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [15/26] time 0.659 (0.504) data 0.528 (0.373) loss_x loss_x 1.2168 (1.1703) acc_x 65.6250 (71.8750) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [20/26] time 0.491 (0.486) data 0.360 (0.355) loss_x loss_x 0.7900 (1.1099) acc_x 75.0000 (73.1250) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [25/26] time 0.594 (0.478) data 0.464 (0.347) loss_x loss_x 1.3457 (1.1324) acc_x 68.7500 (71.8750) lr 5.3915e-05 eta 0:00:00
epoch [181/200] batch [5/71] time 0.459 (0.479) data 0.328 (0.349) loss_u loss_u 0.8955 (0.8885) acc_u 12.5000 (16.2500) lr 5.3915e-05 eta 0:00:31
epoch [181/200] batch [10/71] time 0.370 (0.483) data 0.240 (0.352) loss_u loss_u 0.8960 (0.8808) acc_u 9.3750 (15.0000) lr 5.3915e-05 eta 0:00:29
epoch [181/200] batch [15/71] time 0.534 (0.481) data 0.403 (0.350) loss_u loss_u 0.9272 (0.8989) acc_u 12.5000 (12.9167) lr 5.3915e-05 eta 0:00:26
epoch [181/200] batch [20/71] time 0.450 (0.479) data 0.319 (0.348) loss_u loss_u 0.8657 (0.8987) acc_u 15.6250 (12.9688) lr 5.3915e-05 eta 0:00:24
epoch [181/200] batch [25/71] time 0.518 (0.476) data 0.386 (0.345) loss_u loss_u 0.8857 (0.9017) acc_u 12.5000 (12.5000) lr 5.3915e-05 eta 0:00:21
epoch [181/200] batch [30/71] time 0.427 (0.477) data 0.295 (0.346) loss_u loss_u 0.8872 (0.9012) acc_u 18.7500 (12.7083) lr 5.3915e-05 eta 0:00:19
epoch [181/200] batch [35/71] time 0.500 (0.475) data 0.369 (0.344) loss_u loss_u 0.8730 (0.9011) acc_u 15.6250 (12.6786) lr 5.3915e-05 eta 0:00:17
epoch [181/200] batch [40/71] time 0.378 (0.470) data 0.245 (0.339) loss_u loss_u 0.9004 (0.8986) acc_u 6.2500 (12.8125) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [45/71] time 0.566 (0.472) data 0.435 (0.341) loss_u loss_u 0.9053 (0.9009) acc_u 12.5000 (12.5000) lr 5.3915e-05 eta 0:00:12
epoch [181/200] batch [50/71] time 0.480 (0.467) data 0.348 (0.336) loss_u loss_u 0.9600 (0.9027) acc_u 3.1250 (12.2500) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [55/71] time 0.369 (0.462) data 0.237 (0.331) loss_u loss_u 0.9097 (0.8992) acc_u 9.3750 (12.6705) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [60/71] time 0.475 (0.459) data 0.343 (0.328) loss_u loss_u 0.9316 (0.8981) acc_u 9.3750 (12.8646) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [65/71] time 0.377 (0.458) data 0.246 (0.327) loss_u loss_u 0.8257 (0.8972) acc_u 25.0000 (12.9808) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [70/71] time 0.419 (0.458) data 0.289 (0.326) loss_u loss_u 0.8706 (0.8959) acc_u 18.7500 (13.1696) lr 5.3915e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2052
confident_label rate tensor(0.2841, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 891
clean true:408
clean false:483
clean_rate:0.45791245791245794
noisy true:676
noisy false:1569
after delete: len(clean_dataset) 891
after delete: len(noisy_dataset) 2245
epoch [182/200] batch [5/27] time 0.482 (0.472) data 0.352 (0.341) loss_x loss_x 0.8955 (1.1616) acc_x 84.3750 (73.1250) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [10/27] time 0.441 (0.461) data 0.310 (0.331) loss_x loss_x 1.1035 (1.1982) acc_x 75.0000 (70.9375) lr 4.8943e-05 eta 0:00:07
epoch [182/200] batch [15/27] time 0.372 (0.458) data 0.243 (0.328) loss_x loss_x 0.9893 (1.2186) acc_x 71.8750 (70.2083) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [20/27] time 0.438 (0.447) data 0.308 (0.317) loss_x loss_x 1.0693 (1.2039) acc_x 81.2500 (70.7812) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [25/27] time 0.377 (0.445) data 0.246 (0.315) loss_x loss_x 1.3184 (1.1895) acc_x 65.6250 (71.0000) lr 4.8943e-05 eta 0:00:00
epoch [182/200] batch [5/70] time 0.358 (0.441) data 0.228 (0.310) loss_u loss_u 0.9028 (0.8977) acc_u 12.5000 (12.5000) lr 4.8943e-05 eta 0:00:28
epoch [182/200] batch [10/70] time 0.414 (0.446) data 0.282 (0.316) loss_u loss_u 0.9043 (0.9023) acc_u 12.5000 (12.5000) lr 4.8943e-05 eta 0:00:26
epoch [182/200] batch [15/70] time 0.458 (0.453) data 0.327 (0.322) loss_u loss_u 0.9077 (0.8982) acc_u 9.3750 (12.7083) lr 4.8943e-05 eta 0:00:24
epoch [182/200] batch [20/70] time 0.412 (0.460) data 0.279 (0.329) loss_u loss_u 0.8638 (0.8987) acc_u 12.5000 (11.8750) lr 4.8943e-05 eta 0:00:23
epoch [182/200] batch [25/70] time 0.379 (0.458) data 0.247 (0.327) loss_u loss_u 0.9102 (0.8994) acc_u 12.5000 (11.8750) lr 4.8943e-05 eta 0:00:20
epoch [182/200] batch [30/70] time 0.428 (0.459) data 0.295 (0.328) loss_u loss_u 0.8394 (0.8970) acc_u 21.8750 (12.3958) lr 4.8943e-05 eta 0:00:18
epoch [182/200] batch [35/70] time 0.439 (0.458) data 0.307 (0.327) loss_u loss_u 0.9927 (0.9012) acc_u 0.0000 (12.1429) lr 4.8943e-05 eta 0:00:16
epoch [182/200] batch [40/70] time 0.640 (0.460) data 0.507 (0.329) loss_u loss_u 0.8970 (0.9006) acc_u 12.5000 (12.3438) lr 4.8943e-05 eta 0:00:13
epoch [182/200] batch [45/70] time 0.402 (0.458) data 0.270 (0.326) loss_u loss_u 0.9390 (0.8979) acc_u 6.2500 (12.6389) lr 4.8943e-05 eta 0:00:11
epoch [182/200] batch [50/70] time 0.472 (0.455) data 0.341 (0.324) loss_u loss_u 0.8486 (0.8930) acc_u 15.6250 (13.0000) lr 4.8943e-05 eta 0:00:09
epoch [182/200] batch [55/70] time 0.406 (0.455) data 0.274 (0.324) loss_u loss_u 0.9121 (0.8935) acc_u 15.6250 (12.9545) lr 4.8943e-05 eta 0:00:06
epoch [182/200] batch [60/70] time 0.586 (0.460) data 0.453 (0.329) loss_u loss_u 0.9204 (0.8948) acc_u 12.5000 (12.8646) lr 4.8943e-05 eta 0:00:04
epoch [182/200] batch [65/70] time 0.418 (0.461) data 0.288 (0.330) loss_u loss_u 0.8896 (0.8952) acc_u 9.3750 (12.8365) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [70/70] time 0.518 (0.461) data 0.386 (0.329) loss_u loss_u 0.9341 (0.8970) acc_u 6.2500 (12.5000) lr 4.8943e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2080
confident_label rate tensor(0.2832, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 888
clean true:394
clean false:494
clean_rate:0.4436936936936937
noisy true:662
noisy false:1586
after delete: len(clean_dataset) 888
after delete: len(noisy_dataset) 2248
epoch [183/200] batch [5/27] time 0.430 (0.490) data 0.299 (0.359) loss_x loss_x 1.2080 (1.1233) acc_x 71.8750 (74.3750) lr 4.4207e-05 eta 0:00:10
epoch [183/200] batch [10/27] time 0.617 (0.493) data 0.485 (0.362) loss_x loss_x 0.7886 (1.0319) acc_x 71.8750 (73.7500) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [15/27] time 0.389 (0.492) data 0.258 (0.361) loss_x loss_x 1.1855 (1.0989) acc_x 75.0000 (73.7500) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [20/27] time 0.395 (0.480) data 0.265 (0.349) loss_x loss_x 0.5181 (1.0524) acc_x 87.5000 (74.8438) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [25/27] time 0.485 (0.485) data 0.354 (0.355) loss_x loss_x 1.1641 (1.0830) acc_x 78.1250 (74.1250) lr 4.4207e-05 eta 0:00:00
epoch [183/200] batch [5/70] time 0.669 (0.491) data 0.536 (0.360) loss_u loss_u 0.9082 (0.8612) acc_u 12.5000 (18.1250) lr 4.4207e-05 eta 0:00:31
epoch [183/200] batch [10/70] time 0.469 (0.485) data 0.337 (0.354) loss_u loss_u 0.8848 (0.8887) acc_u 12.5000 (14.3750) lr 4.4207e-05 eta 0:00:29
epoch [183/200] batch [15/70] time 0.509 (0.490) data 0.377 (0.359) loss_u loss_u 0.9360 (0.8900) acc_u 9.3750 (13.7500) lr 4.4207e-05 eta 0:00:26
epoch [183/200] batch [20/70] time 0.476 (0.486) data 0.345 (0.355) loss_u loss_u 0.9570 (0.8902) acc_u 6.2500 (14.0625) lr 4.4207e-05 eta 0:00:24
epoch [183/200] batch [25/70] time 0.386 (0.477) data 0.255 (0.346) loss_u loss_u 0.9009 (0.8953) acc_u 15.6250 (13.3750) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [30/70] time 0.441 (0.472) data 0.308 (0.341) loss_u loss_u 0.8584 (0.8930) acc_u 18.7500 (13.8542) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [35/70] time 0.500 (0.471) data 0.368 (0.339) loss_u loss_u 0.8936 (0.8941) acc_u 12.5000 (13.5714) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [40/70] time 0.487 (0.468) data 0.354 (0.337) loss_u loss_u 0.9644 (0.8963) acc_u 3.1250 (13.2812) lr 4.4207e-05 eta 0:00:14
epoch [183/200] batch [45/70] time 0.540 (0.471) data 0.409 (0.339) loss_u loss_u 0.8193 (0.8964) acc_u 25.0000 (13.2639) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [50/70] time 0.492 (0.472) data 0.360 (0.341) loss_u loss_u 0.8848 (0.8954) acc_u 12.5000 (13.2500) lr 4.4207e-05 eta 0:00:09
epoch [183/200] batch [55/70] time 0.426 (0.471) data 0.293 (0.339) loss_u loss_u 0.9385 (0.8955) acc_u 6.2500 (13.0114) lr 4.4207e-05 eta 0:00:07
epoch [183/200] batch [60/70] time 0.576 (0.471) data 0.444 (0.339) loss_u loss_u 0.8696 (0.8968) acc_u 9.3750 (12.7083) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [65/70] time 0.406 (0.469) data 0.275 (0.338) loss_u loss_u 0.8916 (0.8979) acc_u 15.6250 (12.6442) lr 4.4207e-05 eta 0:00:02
epoch [183/200] batch [70/70] time 0.458 (0.471) data 0.327 (0.340) loss_u loss_u 0.8521 (0.8988) acc_u 18.7500 (12.5446) lr 4.4207e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2076
confident_label rate tensor(0.2749, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 862
clean true:392
clean false:470
clean_rate:0.4547563805104408
noisy true:668
noisy false:1606
after delete: len(clean_dataset) 862
after delete: len(noisy_dataset) 2274
epoch [184/200] batch [5/26] time 0.534 (0.446) data 0.403 (0.314) loss_x loss_x 1.3213 (1.1855) acc_x 56.2500 (66.2500) lr 3.9706e-05 eta 0:00:09
epoch [184/200] batch [10/26] time 0.609 (0.504) data 0.479 (0.373) loss_x loss_x 1.0420 (1.1557) acc_x 65.6250 (68.7500) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [15/26] time 0.700 (0.508) data 0.569 (0.377) loss_x loss_x 0.8604 (1.1328) acc_x 81.2500 (70.0000) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [20/26] time 0.399 (0.489) data 0.268 (0.358) loss_x loss_x 1.1777 (1.1536) acc_x 56.2500 (69.0625) lr 3.9706e-05 eta 0:00:02
epoch [184/200] batch [25/26] time 0.469 (0.481) data 0.339 (0.350) loss_x loss_x 1.2783 (1.1726) acc_x 59.3750 (68.7500) lr 3.9706e-05 eta 0:00:00
epoch [184/200] batch [5/71] time 0.402 (0.477) data 0.270 (0.346) loss_u loss_u 0.8936 (0.8856) acc_u 12.5000 (15.0000) lr 3.9706e-05 eta 0:00:31
epoch [184/200] batch [10/71] time 0.455 (0.467) data 0.323 (0.336) loss_u loss_u 0.9028 (0.8845) acc_u 12.5000 (15.0000) lr 3.9706e-05 eta 0:00:28
epoch [184/200] batch [15/71] time 0.743 (0.473) data 0.612 (0.342) loss_u loss_u 0.8223 (0.8903) acc_u 18.7500 (13.5417) lr 3.9706e-05 eta 0:00:26
epoch [184/200] batch [20/71] time 0.424 (0.476) data 0.292 (0.345) loss_u loss_u 0.9243 (0.8898) acc_u 15.6250 (13.9062) lr 3.9706e-05 eta 0:00:24
epoch [184/200] batch [25/71] time 0.570 (0.482) data 0.439 (0.351) loss_u loss_u 0.8540 (0.8853) acc_u 15.6250 (14.5000) lr 3.9706e-05 eta 0:00:22
epoch [184/200] batch [30/71] time 0.489 (0.479) data 0.357 (0.348) loss_u loss_u 0.9478 (0.8908) acc_u 3.1250 (13.4375) lr 3.9706e-05 eta 0:00:19
epoch [184/200] batch [35/71] time 0.583 (0.477) data 0.451 (0.346) loss_u loss_u 0.7808 (0.8878) acc_u 28.1250 (13.7500) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [40/71] time 0.563 (0.474) data 0.432 (0.343) loss_u loss_u 0.9551 (0.8894) acc_u 9.3750 (13.6719) lr 3.9706e-05 eta 0:00:14
epoch [184/200] batch [45/71] time 0.406 (0.473) data 0.274 (0.341) loss_u loss_u 0.8271 (0.8855) acc_u 21.8750 (14.3750) lr 3.9706e-05 eta 0:00:12
epoch [184/200] batch [50/71] time 0.416 (0.476) data 0.286 (0.344) loss_u loss_u 0.9385 (0.8846) acc_u 9.3750 (14.4375) lr 3.9706e-05 eta 0:00:09
epoch [184/200] batch [55/71] time 0.459 (0.473) data 0.326 (0.341) loss_u loss_u 0.9355 (0.8872) acc_u 12.5000 (14.1477) lr 3.9706e-05 eta 0:00:07
epoch [184/200] batch [60/71] time 0.456 (0.473) data 0.325 (0.342) loss_u loss_u 0.9053 (0.8855) acc_u 12.5000 (14.2708) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [65/71] time 0.388 (0.474) data 0.256 (0.342) loss_u loss_u 0.9180 (0.8860) acc_u 9.3750 (14.0865) lr 3.9706e-05 eta 0:00:02
epoch [184/200] batch [70/71] time 0.379 (0.471) data 0.248 (0.340) loss_u loss_u 0.9414 (0.8873) acc_u 3.1250 (13.8839) lr 3.9706e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2063
confident_label rate tensor(0.2851, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 894
clean true:395
clean false:499
clean_rate:0.441834451901566
noisy true:678
noisy false:1564
after delete: len(clean_dataset) 894
after delete: len(noisy_dataset) 2242
epoch [185/200] batch [5/27] time 0.388 (0.441) data 0.258 (0.310) loss_x loss_x 1.3477 (1.1780) acc_x 68.7500 (72.5000) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [10/27] time 0.584 (0.449) data 0.453 (0.318) loss_x loss_x 0.8296 (1.1676) acc_x 81.2500 (70.0000) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [15/27] time 0.501 (0.443) data 0.370 (0.312) loss_x loss_x 1.4834 (1.1993) acc_x 56.2500 (69.1667) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [20/27] time 0.452 (0.457) data 0.321 (0.326) loss_x loss_x 0.8555 (1.1652) acc_x 81.2500 (69.8438) lr 3.5443e-05 eta 0:00:03
epoch [185/200] batch [25/27] time 0.574 (0.464) data 0.443 (0.333) loss_x loss_x 1.3643 (1.2210) acc_x 56.2500 (68.3750) lr 3.5443e-05 eta 0:00:00
epoch [185/200] batch [5/70] time 0.476 (0.465) data 0.345 (0.334) loss_u loss_u 0.8442 (0.8855) acc_u 18.7500 (15.6250) lr 3.5443e-05 eta 0:00:30
epoch [185/200] batch [10/70] time 0.572 (0.472) data 0.441 (0.340) loss_u loss_u 0.9331 (0.8943) acc_u 6.2500 (14.0625) lr 3.5443e-05 eta 0:00:28
epoch [185/200] batch [15/70] time 0.468 (0.480) data 0.336 (0.349) loss_u loss_u 0.9160 (0.8945) acc_u 9.3750 (14.1667) lr 3.5443e-05 eta 0:00:26
epoch [185/200] batch [20/70] time 0.482 (0.478) data 0.350 (0.347) loss_u loss_u 0.9126 (0.8998) acc_u 12.5000 (13.1250) lr 3.5443e-05 eta 0:00:23
epoch [185/200] batch [25/70] time 0.494 (0.479) data 0.363 (0.348) loss_u loss_u 0.9678 (0.8992) acc_u 3.1250 (13.1250) lr 3.5443e-05 eta 0:00:21
epoch [185/200] batch [30/70] time 0.357 (0.478) data 0.225 (0.347) loss_u loss_u 0.8711 (0.8960) acc_u 15.6250 (13.6458) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [35/70] time 0.446 (0.477) data 0.314 (0.346) loss_u loss_u 0.9194 (0.8937) acc_u 9.3750 (14.0179) lr 3.5443e-05 eta 0:00:16
epoch [185/200] batch [40/70] time 0.408 (0.476) data 0.276 (0.344) loss_u loss_u 0.9106 (0.8977) acc_u 15.6250 (13.5156) lr 3.5443e-05 eta 0:00:14
epoch [185/200] batch [45/70] time 0.560 (0.477) data 0.428 (0.345) loss_u loss_u 0.9805 (0.8958) acc_u 3.1250 (13.7500) lr 3.5443e-05 eta 0:00:11
epoch [185/200] batch [50/70] time 0.395 (0.474) data 0.262 (0.343) loss_u loss_u 0.8955 (0.8937) acc_u 12.5000 (14.1875) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [55/70] time 0.404 (0.472) data 0.273 (0.340) loss_u loss_u 0.9019 (0.8947) acc_u 12.5000 (13.9205) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [60/70] time 0.480 (0.472) data 0.349 (0.341) loss_u loss_u 0.8936 (0.8955) acc_u 12.5000 (13.8021) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [65/70] time 0.448 (0.468) data 0.316 (0.337) loss_u loss_u 0.9019 (0.8940) acc_u 12.5000 (14.0385) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [70/70] time 0.444 (0.465) data 0.313 (0.334) loss_u loss_u 0.8843 (0.8946) acc_u 12.5000 (13.9286) lr 3.5443e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2045
confident_label rate tensor(0.2768, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 868
clean true:392
clean false:476
clean_rate:0.45161290322580644
noisy true:699
noisy false:1569
after delete: len(clean_dataset) 868
after delete: len(noisy_dataset) 2268
epoch [186/200] batch [5/27] time 0.488 (0.546) data 0.358 (0.415) loss_x loss_x 0.8008 (1.0754) acc_x 75.0000 (74.3750) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [10/27] time 0.502 (0.539) data 0.371 (0.408) loss_x loss_x 0.8184 (1.1399) acc_x 75.0000 (71.2500) lr 3.1417e-05 eta 0:00:09
epoch [186/200] batch [15/27] time 0.472 (0.534) data 0.342 (0.403) loss_x loss_x 1.3506 (1.1414) acc_x 71.8750 (71.4583) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [20/27] time 0.554 (0.516) data 0.424 (0.385) loss_x loss_x 1.0576 (1.1205) acc_x 71.8750 (72.0312) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [25/27] time 0.431 (0.498) data 0.300 (0.367) loss_x loss_x 0.7959 (1.1039) acc_x 75.0000 (71.5000) lr 3.1417e-05 eta 0:00:00
epoch [186/200] batch [5/70] time 0.475 (0.500) data 0.344 (0.369) loss_u loss_u 0.8677 (0.8984) acc_u 12.5000 (11.2500) lr 3.1417e-05 eta 0:00:32
epoch [186/200] batch [10/70] time 0.611 (0.497) data 0.481 (0.366) loss_u loss_u 0.8091 (0.8848) acc_u 25.0000 (13.4375) lr 3.1417e-05 eta 0:00:29
epoch [186/200] batch [15/70] time 0.451 (0.489) data 0.318 (0.358) loss_u loss_u 0.9116 (0.8890) acc_u 9.3750 (13.1250) lr 3.1417e-05 eta 0:00:26
epoch [186/200] batch [20/70] time 0.544 (0.487) data 0.413 (0.356) loss_u loss_u 0.8271 (0.8794) acc_u 25.0000 (14.8438) lr 3.1417e-05 eta 0:00:24
epoch [186/200] batch [25/70] time 0.426 (0.480) data 0.295 (0.349) loss_u loss_u 0.8926 (0.8833) acc_u 12.5000 (14.1250) lr 3.1417e-05 eta 0:00:21
epoch [186/200] batch [30/70] time 0.397 (0.481) data 0.265 (0.350) loss_u loss_u 0.9292 (0.8862) acc_u 9.3750 (13.7500) lr 3.1417e-05 eta 0:00:19
epoch [186/200] batch [35/70] time 0.505 (0.483) data 0.373 (0.352) loss_u loss_u 0.9092 (0.8868) acc_u 6.2500 (13.6607) lr 3.1417e-05 eta 0:00:16
epoch [186/200] batch [40/70] time 0.532 (0.486) data 0.401 (0.355) loss_u loss_u 0.9434 (0.8917) acc_u 9.3750 (13.5156) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [45/70] time 0.432 (0.485) data 0.301 (0.354) loss_u loss_u 0.9155 (0.8879) acc_u 9.3750 (13.9583) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [50/70] time 0.551 (0.486) data 0.420 (0.354) loss_u loss_u 0.9077 (0.8861) acc_u 12.5000 (14.1875) lr 3.1417e-05 eta 0:00:09
epoch [186/200] batch [55/70] time 0.399 (0.483) data 0.268 (0.351) loss_u loss_u 0.9517 (0.8892) acc_u 3.1250 (13.8636) lr 3.1417e-05 eta 0:00:07
epoch [186/200] batch [60/70] time 0.508 (0.484) data 0.377 (0.353) loss_u loss_u 0.8960 (0.8898) acc_u 9.3750 (13.7500) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [65/70] time 0.378 (0.482) data 0.247 (0.351) loss_u loss_u 0.8765 (0.8904) acc_u 15.6250 (13.7019) lr 3.1417e-05 eta 0:00:02
epoch [186/200] batch [70/70] time 0.435 (0.481) data 0.303 (0.349) loss_u loss_u 0.8608 (0.8892) acc_u 15.6250 (13.7946) lr 3.1417e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2095
confident_label rate tensor(0.2755, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 864
clean true:394
clean false:470
clean_rate:0.45601851851851855
noisy true:647
noisy false:1625
after delete: len(clean_dataset) 864
after delete: len(noisy_dataset) 2272
epoch [187/200] batch [5/27] time 0.381 (0.457) data 0.250 (0.327) loss_x loss_x 0.9683 (0.9559) acc_x 81.2500 (76.8750) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [10/27] time 0.330 (0.466) data 0.200 (0.336) loss_x loss_x 1.1514 (1.0379) acc_x 68.7500 (73.7500) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [15/27] time 0.475 (0.491) data 0.345 (0.361) loss_x loss_x 1.2559 (1.0773) acc_x 71.8750 (72.9167) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [20/27] time 0.404 (0.481) data 0.273 (0.350) loss_x loss_x 1.5664 (1.1063) acc_x 65.6250 (72.1875) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [25/27] time 0.449 (0.465) data 0.319 (0.334) loss_x loss_x 1.4902 (1.1621) acc_x 65.6250 (71.3750) lr 2.7630e-05 eta 0:00:00
epoch [187/200] batch [5/71] time 0.415 (0.455) data 0.282 (0.324) loss_u loss_u 0.8633 (0.8713) acc_u 15.6250 (15.0000) lr 2.7630e-05 eta 0:00:30
epoch [187/200] batch [10/71] time 0.510 (0.461) data 0.379 (0.330) loss_u loss_u 0.8979 (0.8801) acc_u 12.5000 (15.0000) lr 2.7630e-05 eta 0:00:28
epoch [187/200] batch [15/71] time 0.447 (0.475) data 0.314 (0.344) loss_u loss_u 0.8711 (0.8873) acc_u 15.6250 (14.3750) lr 2.7630e-05 eta 0:00:26
epoch [187/200] batch [20/71] time 0.508 (0.469) data 0.376 (0.338) loss_u loss_u 0.9087 (0.8958) acc_u 9.3750 (13.1250) lr 2.7630e-05 eta 0:00:23
epoch [187/200] batch [25/71] time 0.383 (0.473) data 0.251 (0.341) loss_u loss_u 0.8550 (0.8977) acc_u 21.8750 (13.0000) lr 2.7630e-05 eta 0:00:21
epoch [187/200] batch [30/71] time 0.414 (0.470) data 0.283 (0.338) loss_u loss_u 0.9248 (0.8976) acc_u 6.2500 (13.0208) lr 2.7630e-05 eta 0:00:19
epoch [187/200] batch [35/71] time 0.392 (0.467) data 0.261 (0.336) loss_u loss_u 0.8691 (0.8961) acc_u 15.6250 (13.3929) lr 2.7630e-05 eta 0:00:16
epoch [187/200] batch [40/71] time 0.466 (0.471) data 0.336 (0.339) loss_u loss_u 0.8433 (0.8956) acc_u 18.7500 (13.2812) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [45/71] time 0.411 (0.469) data 0.280 (0.338) loss_u loss_u 0.9233 (0.8961) acc_u 9.3750 (13.0556) lr 2.7630e-05 eta 0:00:12
epoch [187/200] batch [50/71] time 0.511 (0.466) data 0.380 (0.335) loss_u loss_u 0.9209 (0.8977) acc_u 6.2500 (12.8750) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [55/71] time 0.472 (0.465) data 0.341 (0.334) loss_u loss_u 0.8818 (0.8957) acc_u 15.6250 (13.1818) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [60/71] time 0.439 (0.466) data 0.308 (0.335) loss_u loss_u 0.9160 (0.8957) acc_u 6.2500 (13.0729) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [65/71] time 0.445 (0.467) data 0.313 (0.335) loss_u loss_u 0.8716 (0.8945) acc_u 15.6250 (13.1731) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [70/71] time 0.507 (0.465) data 0.375 (0.334) loss_u loss_u 0.9507 (0.8958) acc_u 3.1250 (12.9464) lr 2.7630e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2069
confident_label rate tensor(0.2710, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 850
clean true:383
clean false:467
clean_rate:0.4505882352941176
noisy true:684
noisy false:1602
after delete: len(clean_dataset) 850
after delete: len(noisy_dataset) 2286
epoch [188/200] batch [5/26] time 0.395 (0.437) data 0.264 (0.307) loss_x loss_x 1.5488 (1.0925) acc_x 68.7500 (74.3750) lr 2.4083e-05 eta 0:00:09
epoch [188/200] batch [10/26] time 0.613 (0.455) data 0.482 (0.324) loss_x loss_x 1.0898 (1.2250) acc_x 78.1250 (70.6250) lr 2.4083e-05 eta 0:00:07
epoch [188/200] batch [15/26] time 0.449 (0.450) data 0.319 (0.319) loss_x loss_x 1.0791 (1.2504) acc_x 71.8750 (70.6250) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [20/26] time 0.573 (0.454) data 0.442 (0.323) loss_x loss_x 0.9189 (1.2365) acc_x 81.2500 (71.0938) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [25/26] time 0.502 (0.458) data 0.370 (0.328) loss_x loss_x 0.9907 (1.2383) acc_x 68.7500 (70.3750) lr 2.4083e-05 eta 0:00:00
epoch [188/200] batch [5/71] time 0.445 (0.454) data 0.312 (0.323) loss_u loss_u 0.8945 (0.9019) acc_u 9.3750 (11.8750) lr 2.4083e-05 eta 0:00:29
epoch [188/200] batch [10/71] time 0.392 (0.453) data 0.260 (0.322) loss_u loss_u 0.8237 (0.8981) acc_u 25.0000 (14.0625) lr 2.4083e-05 eta 0:00:27
epoch [188/200] batch [15/71] time 0.459 (0.455) data 0.326 (0.324) loss_u loss_u 0.8750 (0.8957) acc_u 18.7500 (14.7917) lr 2.4083e-05 eta 0:00:25
epoch [188/200] batch [20/71] time 0.514 (0.463) data 0.382 (0.332) loss_u loss_u 0.9688 (0.8957) acc_u 3.1250 (14.2188) lr 2.4083e-05 eta 0:00:23
epoch [188/200] batch [25/71] time 0.558 (0.466) data 0.427 (0.335) loss_u loss_u 0.8125 (0.8908) acc_u 28.1250 (14.7500) lr 2.4083e-05 eta 0:00:21
epoch [188/200] batch [30/71] time 0.365 (0.463) data 0.234 (0.332) loss_u loss_u 0.8833 (0.8902) acc_u 15.6250 (14.6875) lr 2.4083e-05 eta 0:00:18
epoch [188/200] batch [35/71] time 0.541 (0.461) data 0.409 (0.330) loss_u loss_u 0.9014 (0.8903) acc_u 12.5000 (14.2857) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [40/71] time 0.506 (0.460) data 0.375 (0.329) loss_u loss_u 0.9600 (0.8885) acc_u 6.2500 (14.2969) lr 2.4083e-05 eta 0:00:14
epoch [188/200] batch [45/71] time 0.371 (0.459) data 0.240 (0.328) loss_u loss_u 0.9082 (0.8913) acc_u 15.6250 (13.8889) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [50/71] time 0.508 (0.459) data 0.378 (0.328) loss_u loss_u 0.8838 (0.8919) acc_u 15.6250 (13.7500) lr 2.4083e-05 eta 0:00:09
epoch [188/200] batch [55/71] time 0.460 (0.465) data 0.327 (0.334) loss_u loss_u 0.9307 (0.8915) acc_u 6.2500 (13.6932) lr 2.4083e-05 eta 0:00:07
epoch [188/200] batch [60/71] time 0.421 (0.461) data 0.288 (0.330) loss_u loss_u 0.8716 (0.8916) acc_u 21.8750 (13.8542) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [65/71] time 0.503 (0.458) data 0.373 (0.327) loss_u loss_u 0.9277 (0.8917) acc_u 6.2500 (13.9423) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [70/71] time 0.378 (0.458) data 0.247 (0.327) loss_u loss_u 0.8770 (0.8898) acc_u 15.6250 (14.1518) lr 2.4083e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2082
confident_label rate tensor(0.2765, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 867
clean true:388
clean false:479
clean_rate:0.447520184544406
noisy true:666
noisy false:1603
after delete: len(clean_dataset) 867
after delete: len(noisy_dataset) 2269
epoch [189/200] batch [5/27] time 0.531 (0.478) data 0.399 (0.346) loss_x loss_x 1.2910 (0.9883) acc_x 78.1250 (76.2500) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [10/27] time 0.433 (0.495) data 0.303 (0.364) loss_x loss_x 1.2178 (1.1226) acc_x 75.0000 (75.0000) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [15/27] time 0.415 (0.483) data 0.284 (0.352) loss_x loss_x 1.4287 (1.1593) acc_x 68.7500 (72.9167) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [20/27] time 0.456 (0.479) data 0.324 (0.348) loss_x loss_x 0.9634 (1.1721) acc_x 84.3750 (72.6562) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [25/27] time 0.416 (0.476) data 0.284 (0.345) loss_x loss_x 1.1699 (1.1434) acc_x 71.8750 (73.0000) lr 2.0777e-05 eta 0:00:00
epoch [189/200] batch [5/70] time 0.343 (0.473) data 0.211 (0.342) loss_u loss_u 0.9355 (0.8807) acc_u 6.2500 (16.8750) lr 2.0777e-05 eta 0:00:30
epoch [189/200] batch [10/70] time 0.483 (0.474) data 0.351 (0.343) loss_u loss_u 0.8833 (0.8923) acc_u 12.5000 (13.7500) lr 2.0777e-05 eta 0:00:28
epoch [189/200] batch [15/70] time 0.389 (0.470) data 0.256 (0.339) loss_u loss_u 0.9526 (0.9043) acc_u 3.1250 (12.5000) lr 2.0777e-05 eta 0:00:25
epoch [189/200] batch [20/70] time 0.381 (0.465) data 0.249 (0.333) loss_u loss_u 0.9282 (0.9077) acc_u 9.3750 (12.0312) lr 2.0777e-05 eta 0:00:23
epoch [189/200] batch [25/70] time 0.411 (0.463) data 0.278 (0.332) loss_u loss_u 0.9229 (0.9044) acc_u 9.3750 (12.0000) lr 2.0777e-05 eta 0:00:20
epoch [189/200] batch [30/70] time 0.412 (0.459) data 0.281 (0.327) loss_u loss_u 0.8862 (0.9025) acc_u 18.7500 (12.1875) lr 2.0777e-05 eta 0:00:18
epoch [189/200] batch [35/70] time 0.414 (0.459) data 0.283 (0.327) loss_u loss_u 0.8896 (0.8966) acc_u 12.5000 (12.9464) lr 2.0777e-05 eta 0:00:16
epoch [189/200] batch [40/70] time 0.607 (0.464) data 0.475 (0.332) loss_u loss_u 0.8408 (0.8946) acc_u 18.7500 (13.2031) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [45/70] time 0.524 (0.461) data 0.392 (0.330) loss_u loss_u 0.9009 (0.8921) acc_u 15.6250 (13.6111) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [50/70] time 0.389 (0.463) data 0.256 (0.331) loss_u loss_u 0.9004 (0.8908) acc_u 12.5000 (13.8125) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [55/70] time 0.357 (0.464) data 0.225 (0.332) loss_u loss_u 0.9048 (0.8904) acc_u 12.5000 (13.7500) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [60/70] time 0.511 (0.465) data 0.379 (0.334) loss_u loss_u 0.9307 (0.8905) acc_u 9.3750 (13.9062) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [65/70] time 0.567 (0.467) data 0.436 (0.335) loss_u loss_u 0.7632 (0.8883) acc_u 28.1250 (14.0865) lr 2.0777e-05 eta 0:00:02
epoch [189/200] batch [70/70] time 0.446 (0.468) data 0.315 (0.336) loss_u loss_u 0.9087 (0.8874) acc_u 9.3750 (14.1964) lr 2.0777e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2082
confident_label rate tensor(0.2787, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 874
clean true:395
clean false:479
clean_rate:0.45194508009153317
noisy true:659
noisy false:1603
after delete: len(clean_dataset) 874
after delete: len(noisy_dataset) 2262
epoch [190/200] batch [5/27] time 0.361 (0.433) data 0.230 (0.303) loss_x loss_x 1.2461 (1.0044) acc_x 65.6250 (75.6250) lr 1.7713e-05 eta 0:00:09
epoch [190/200] batch [10/27] time 0.540 (0.445) data 0.409 (0.314) loss_x loss_x 0.8228 (0.9753) acc_x 78.1250 (77.1875) lr 1.7713e-05 eta 0:00:07
epoch [190/200] batch [15/27] time 0.484 (0.449) data 0.353 (0.318) loss_x loss_x 1.1445 (1.0130) acc_x 71.8750 (75.0000) lr 1.7713e-05 eta 0:00:05
epoch [190/200] batch [20/27] time 0.347 (0.450) data 0.217 (0.319) loss_x loss_x 1.2305 (1.0449) acc_x 68.7500 (74.2188) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [25/27] time 0.453 (0.447) data 0.321 (0.316) loss_x loss_x 1.2695 (1.0794) acc_x 59.3750 (73.0000) lr 1.7713e-05 eta 0:00:00
epoch [190/200] batch [5/70] time 0.397 (0.461) data 0.266 (0.330) loss_u loss_u 0.8843 (0.9176) acc_u 12.5000 (10.0000) lr 1.7713e-05 eta 0:00:29
epoch [190/200] batch [10/70] time 0.439 (0.464) data 0.308 (0.333) loss_u loss_u 0.8960 (0.9030) acc_u 9.3750 (11.2500) lr 1.7713e-05 eta 0:00:27
epoch [190/200] batch [15/70] time 0.418 (0.467) data 0.286 (0.336) loss_u loss_u 0.8867 (0.8989) acc_u 15.6250 (11.6667) lr 1.7713e-05 eta 0:00:25
epoch [190/200] batch [20/70] time 0.482 (0.463) data 0.351 (0.332) loss_u loss_u 0.8755 (0.8957) acc_u 15.6250 (12.8125) lr 1.7713e-05 eta 0:00:23
epoch [190/200] batch [25/70] time 0.596 (0.473) data 0.465 (0.341) loss_u loss_u 0.8433 (0.8896) acc_u 21.8750 (13.7500) lr 1.7713e-05 eta 0:00:21
epoch [190/200] batch [30/70] time 0.566 (0.474) data 0.435 (0.343) loss_u loss_u 0.9243 (0.8880) acc_u 9.3750 (14.0625) lr 1.7713e-05 eta 0:00:18
epoch [190/200] batch [35/70] time 0.412 (0.473) data 0.280 (0.342) loss_u loss_u 0.8965 (0.8869) acc_u 9.3750 (14.1071) lr 1.7713e-05 eta 0:00:16
epoch [190/200] batch [40/70] time 0.547 (0.469) data 0.415 (0.338) loss_u loss_u 0.8296 (0.8880) acc_u 21.8750 (13.9062) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [45/70] time 0.352 (0.466) data 0.221 (0.334) loss_u loss_u 0.9287 (0.8898) acc_u 9.3750 (13.8194) lr 1.7713e-05 eta 0:00:11
epoch [190/200] batch [50/70] time 0.578 (0.468) data 0.446 (0.337) loss_u loss_u 0.8794 (0.8903) acc_u 18.7500 (13.8750) lr 1.7713e-05 eta 0:00:09
epoch [190/200] batch [55/70] time 0.412 (0.468) data 0.280 (0.337) loss_u loss_u 0.9395 (0.8906) acc_u 6.2500 (13.9205) lr 1.7713e-05 eta 0:00:07
epoch [190/200] batch [60/70] time 0.397 (0.467) data 0.266 (0.335) loss_u loss_u 0.8145 (0.8923) acc_u 25.0000 (13.7500) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [65/70] time 0.518 (0.466) data 0.386 (0.335) loss_u loss_u 0.8682 (0.8907) acc_u 15.6250 (13.8462) lr 1.7713e-05 eta 0:00:02
epoch [190/200] batch [70/70] time 0.378 (0.464) data 0.247 (0.333) loss_u loss_u 0.9502 (0.8924) acc_u 6.2500 (13.7054) lr 1.7713e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2080
confident_label rate tensor(0.2758, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 865
clean true:383
clean false:482
clean_rate:0.44277456647398844
noisy true:673
noisy false:1598
after delete: len(clean_dataset) 865
after delete: len(noisy_dataset) 2271
epoch [191/200] batch [5/27] time 0.498 (0.498) data 0.367 (0.367) loss_x loss_x 0.9712 (1.1758) acc_x 75.0000 (70.6250) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [10/27] time 0.588 (0.485) data 0.457 (0.354) loss_x loss_x 0.6782 (1.1019) acc_x 75.0000 (70.9375) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [15/27] time 0.389 (0.476) data 0.259 (0.345) loss_x loss_x 0.9463 (1.1085) acc_x 75.0000 (71.4583) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [20/27] time 0.427 (0.473) data 0.296 (0.343) loss_x loss_x 1.3379 (1.1487) acc_x 59.3750 (69.8438) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [25/27] time 0.445 (0.477) data 0.313 (0.346) loss_x loss_x 1.0508 (1.1490) acc_x 75.0000 (70.3750) lr 1.4891e-05 eta 0:00:00
epoch [191/200] batch [5/70] time 0.382 (0.471) data 0.250 (0.340) loss_u loss_u 0.9580 (0.9182) acc_u 6.2500 (11.8750) lr 1.4891e-05 eta 0:00:30
epoch [191/200] batch [10/70] time 0.357 (0.467) data 0.225 (0.336) loss_u loss_u 0.9297 (0.9052) acc_u 9.3750 (13.4375) lr 1.4891e-05 eta 0:00:28
epoch [191/200] batch [15/70] time 0.420 (0.472) data 0.290 (0.341) loss_u loss_u 0.8765 (0.9065) acc_u 12.5000 (12.9167) lr 1.4891e-05 eta 0:00:25
epoch [191/200] batch [20/70] time 0.432 (0.472) data 0.300 (0.341) loss_u loss_u 0.7812 (0.8880) acc_u 31.2500 (15.1562) lr 1.4891e-05 eta 0:00:23
epoch [191/200] batch [25/70] time 0.472 (0.482) data 0.341 (0.351) loss_u loss_u 0.9360 (0.8933) acc_u 6.2500 (14.0000) lr 1.4891e-05 eta 0:00:21
epoch [191/200] batch [30/70] time 0.422 (0.476) data 0.291 (0.345) loss_u loss_u 0.8398 (0.8896) acc_u 28.1250 (14.3750) lr 1.4891e-05 eta 0:00:19
epoch [191/200] batch [35/70] time 0.454 (0.475) data 0.322 (0.344) loss_u loss_u 0.8940 (0.8898) acc_u 15.6250 (14.3750) lr 1.4891e-05 eta 0:00:16
epoch [191/200] batch [40/70] time 0.432 (0.470) data 0.300 (0.339) loss_u loss_u 0.8911 (0.8942) acc_u 18.7500 (13.7500) lr 1.4891e-05 eta 0:00:14
epoch [191/200] batch [45/70] time 0.592 (0.470) data 0.461 (0.339) loss_u loss_u 0.8794 (0.8936) acc_u 18.7500 (13.8194) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [50/70] time 0.515 (0.470) data 0.384 (0.339) loss_u loss_u 0.8911 (0.8905) acc_u 15.6250 (14.2500) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [55/70] time 0.380 (0.469) data 0.249 (0.337) loss_u loss_u 0.8394 (0.8889) acc_u 21.8750 (14.6023) lr 1.4891e-05 eta 0:00:07
epoch [191/200] batch [60/70] time 0.371 (0.466) data 0.238 (0.335) loss_u loss_u 0.8242 (0.8892) acc_u 21.8750 (14.2708) lr 1.4891e-05 eta 0:00:04
epoch [191/200] batch [65/70] time 0.364 (0.464) data 0.233 (0.333) loss_u loss_u 0.8149 (0.8879) acc_u 18.7500 (14.3269) lr 1.4891e-05 eta 0:00:02
epoch [191/200] batch [70/70] time 0.590 (0.467) data 0.459 (0.336) loss_u loss_u 0.8613 (0.8869) acc_u 15.6250 (14.5536) lr 1.4891e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2089
confident_label rate tensor(0.2672, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 838
clean true:369
clean false:469
clean_rate:0.4403341288782816
noisy true:678
noisy false:1620
after delete: len(clean_dataset) 838
after delete: len(noisy_dataset) 2298
epoch [192/200] batch [5/26] time 0.554 (0.455) data 0.424 (0.324) loss_x loss_x 1.0859 (1.1378) acc_x 71.8750 (71.2500) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [10/26] time 0.399 (0.461) data 0.268 (0.330) loss_x loss_x 0.6934 (1.2087) acc_x 81.2500 (69.3750) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [15/26] time 0.544 (0.478) data 0.414 (0.347) loss_x loss_x 0.9507 (1.1060) acc_x 78.1250 (72.5000) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [20/26] time 0.492 (0.469) data 0.362 (0.338) loss_x loss_x 0.8145 (1.1451) acc_x 81.2500 (71.2500) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [25/26] time 0.380 (0.465) data 0.250 (0.334) loss_x loss_x 0.8311 (1.1421) acc_x 84.3750 (71.6250) lr 1.2312e-05 eta 0:00:00
epoch [192/200] batch [5/71] time 0.477 (0.465) data 0.345 (0.334) loss_u loss_u 0.7959 (0.8894) acc_u 28.1250 (13.7500) lr 1.2312e-05 eta 0:00:30
epoch [192/200] batch [10/71] time 0.512 (0.465) data 0.382 (0.334) loss_u loss_u 0.9380 (0.8966) acc_u 6.2500 (13.1250) lr 1.2312e-05 eta 0:00:28
epoch [192/200] batch [15/71] time 0.748 (0.474) data 0.617 (0.343) loss_u loss_u 0.8789 (0.8990) acc_u 12.5000 (12.5000) lr 1.2312e-05 eta 0:00:26
epoch [192/200] batch [20/71] time 0.437 (0.481) data 0.306 (0.350) loss_u loss_u 0.9282 (0.8938) acc_u 6.2500 (12.9688) lr 1.2312e-05 eta 0:00:24
epoch [192/200] batch [25/71] time 0.407 (0.475) data 0.276 (0.344) loss_u loss_u 0.8911 (0.8876) acc_u 12.5000 (14.2500) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [30/71] time 0.430 (0.475) data 0.299 (0.344) loss_u loss_u 0.9409 (0.8848) acc_u 12.5000 (14.7917) lr 1.2312e-05 eta 0:00:19
epoch [192/200] batch [35/71] time 0.519 (0.475) data 0.388 (0.344) loss_u loss_u 0.9390 (0.8824) acc_u 6.2500 (14.8214) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [40/71] time 0.395 (0.471) data 0.264 (0.340) loss_u loss_u 0.8828 (0.8839) acc_u 15.6250 (14.6875) lr 1.2312e-05 eta 0:00:14
epoch [192/200] batch [45/71] time 0.434 (0.472) data 0.302 (0.341) loss_u loss_u 0.9463 (0.8831) acc_u 9.3750 (15.0000) lr 1.2312e-05 eta 0:00:12
epoch [192/200] batch [50/71] time 0.387 (0.470) data 0.257 (0.339) loss_u loss_u 0.8389 (0.8849) acc_u 18.7500 (14.8125) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [55/71] time 0.525 (0.468) data 0.395 (0.337) loss_u loss_u 0.8506 (0.8851) acc_u 18.7500 (14.7159) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [60/71] time 0.651 (0.468) data 0.519 (0.337) loss_u loss_u 0.8701 (0.8858) acc_u 15.6250 (14.4792) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [65/71] time 0.480 (0.470) data 0.349 (0.339) loss_u loss_u 0.8442 (0.8852) acc_u 18.7500 (14.6154) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [70/71] time 0.481 (0.469) data 0.350 (0.338) loss_u loss_u 0.8369 (0.8862) acc_u 18.7500 (14.3750) lr 1.2312e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2090
confident_label rate tensor(0.2771, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 869
clean true:390
clean false:479
clean_rate:0.4487917146144994
noisy true:656
noisy false:1611
after delete: len(clean_dataset) 869
after delete: len(noisy_dataset) 2267
epoch [193/200] batch [5/27] time 0.515 (0.495) data 0.384 (0.365) loss_x loss_x 0.9092 (1.2295) acc_x 81.2500 (65.0000) lr 9.9763e-06 eta 0:00:10
epoch [193/200] batch [10/27] time 0.369 (0.460) data 0.239 (0.330) loss_x loss_x 1.2012 (1.0775) acc_x 75.0000 (70.3125) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [15/27] time 0.385 (0.445) data 0.254 (0.315) loss_x loss_x 0.9424 (1.0049) acc_x 81.2500 (73.1250) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [20/27] time 0.477 (0.442) data 0.346 (0.311) loss_x loss_x 1.2383 (1.0729) acc_x 75.0000 (72.1875) lr 9.9763e-06 eta 0:00:03
epoch [193/200] batch [25/27] time 0.344 (0.457) data 0.213 (0.326) loss_x loss_x 1.0449 (1.0490) acc_x 62.5000 (72.2500) lr 9.9763e-06 eta 0:00:00
epoch [193/200] batch [5/70] time 0.554 (0.460) data 0.422 (0.329) loss_u loss_u 0.9292 (0.9189) acc_u 9.3750 (8.1250) lr 9.9763e-06 eta 0:00:29
epoch [193/200] batch [10/70] time 0.451 (0.466) data 0.319 (0.335) loss_u loss_u 0.8262 (0.8958) acc_u 25.0000 (12.5000) lr 9.9763e-06 eta 0:00:27
epoch [193/200] batch [15/70] time 0.506 (0.458) data 0.374 (0.328) loss_u loss_u 0.9419 (0.9001) acc_u 6.2500 (12.7083) lr 9.9763e-06 eta 0:00:25
epoch [193/200] batch [20/70] time 0.459 (0.462) data 0.328 (0.331) loss_u loss_u 0.8687 (0.8982) acc_u 15.6250 (12.5000) lr 9.9763e-06 eta 0:00:23
epoch [193/200] batch [25/70] time 0.419 (0.465) data 0.287 (0.334) loss_u loss_u 0.9097 (0.8977) acc_u 9.3750 (12.7500) lr 9.9763e-06 eta 0:00:20
epoch [193/200] batch [30/70] time 0.365 (0.460) data 0.235 (0.329) loss_u loss_u 0.8760 (0.8991) acc_u 12.5000 (12.3958) lr 9.9763e-06 eta 0:00:18
epoch [193/200] batch [35/70] time 0.571 (0.463) data 0.439 (0.332) loss_u loss_u 0.8657 (0.9002) acc_u 18.7500 (12.1429) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [40/70] time 0.486 (0.467) data 0.356 (0.336) loss_u loss_u 0.8105 (0.8949) acc_u 28.1250 (12.8906) lr 9.9763e-06 eta 0:00:14
epoch [193/200] batch [45/70] time 0.450 (0.464) data 0.319 (0.333) loss_u loss_u 0.8936 (0.8951) acc_u 9.3750 (12.7083) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [50/70] time 0.384 (0.462) data 0.253 (0.331) loss_u loss_u 0.8677 (0.8954) acc_u 12.5000 (12.8125) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [55/70] time 0.351 (0.460) data 0.220 (0.329) loss_u loss_u 0.8296 (0.8960) acc_u 25.0000 (12.7273) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [60/70] time 0.665 (0.463) data 0.531 (0.332) loss_u loss_u 0.8442 (0.8948) acc_u 21.8750 (12.8125) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [65/70] time 0.361 (0.465) data 0.230 (0.333) loss_u loss_u 0.8652 (0.8930) acc_u 18.7500 (13.1250) lr 9.9763e-06 eta 0:00:02
epoch [193/200] batch [70/70] time 0.358 (0.464) data 0.226 (0.332) loss_u loss_u 0.8477 (0.8896) acc_u 15.6250 (13.6161) lr 9.9763e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2098
confident_label rate tensor(0.2714, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 851
clean true:393
clean false:458
clean_rate:0.4618096357226792
noisy true:645
noisy false:1640
after delete: len(clean_dataset) 851
after delete: len(noisy_dataset) 2285
epoch [194/200] batch [5/26] time 0.465 (0.491) data 0.333 (0.361) loss_x loss_x 1.0771 (1.1191) acc_x 71.8750 (75.0000) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [10/26] time 0.405 (0.471) data 0.275 (0.341) loss_x loss_x 1.0488 (1.2154) acc_x 78.1250 (71.2500) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [15/26] time 0.402 (0.455) data 0.272 (0.324) loss_x loss_x 1.4697 (1.1520) acc_x 62.5000 (72.5000) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [20/26] time 0.485 (0.449) data 0.355 (0.318) loss_x loss_x 1.2939 (1.1727) acc_x 71.8750 (72.1875) lr 7.8853e-06 eta 0:00:02
epoch [194/200] batch [25/26] time 0.558 (0.455) data 0.426 (0.325) loss_x loss_x 0.6597 (1.1723) acc_x 87.5000 (71.7500) lr 7.8853e-06 eta 0:00:00
epoch [194/200] batch [5/71] time 0.491 (0.449) data 0.359 (0.318) loss_u loss_u 0.8677 (0.8859) acc_u 15.6250 (13.7500) lr 7.8853e-06 eta 0:00:29
epoch [194/200] batch [10/71] time 0.708 (0.470) data 0.578 (0.339) loss_u loss_u 0.9111 (0.8960) acc_u 12.5000 (12.5000) lr 7.8853e-06 eta 0:00:28
epoch [194/200] batch [15/71] time 0.480 (0.468) data 0.349 (0.337) loss_u loss_u 0.9268 (0.8907) acc_u 9.3750 (13.1250) lr 7.8853e-06 eta 0:00:26
epoch [194/200] batch [20/71] time 0.361 (0.467) data 0.229 (0.336) loss_u loss_u 0.8418 (0.8911) acc_u 18.7500 (12.9688) lr 7.8853e-06 eta 0:00:23
epoch [194/200] batch [25/71] time 0.645 (0.467) data 0.514 (0.336) loss_u loss_u 0.8423 (0.8906) acc_u 18.7500 (12.7500) lr 7.8853e-06 eta 0:00:21
epoch [194/200] batch [30/71] time 0.414 (0.462) data 0.283 (0.331) loss_u loss_u 0.9023 (0.8920) acc_u 9.3750 (12.6042) lr 7.8853e-06 eta 0:00:18
epoch [194/200] batch [35/71] time 0.409 (0.460) data 0.279 (0.329) loss_u loss_u 0.9077 (0.8917) acc_u 9.3750 (12.7679) lr 7.8853e-06 eta 0:00:16
epoch [194/200] batch [40/71] time 0.341 (0.461) data 0.210 (0.330) loss_u loss_u 0.8784 (0.8876) acc_u 15.6250 (13.3594) lr 7.8853e-06 eta 0:00:14
epoch [194/200] batch [45/71] time 0.410 (0.459) data 0.280 (0.328) loss_u loss_u 0.8423 (0.8873) acc_u 25.0000 (13.6806) lr 7.8853e-06 eta 0:00:11
epoch [194/200] batch [50/71] time 0.453 (0.463) data 0.321 (0.332) loss_u loss_u 0.8965 (0.8885) acc_u 15.6250 (13.6250) lr 7.8853e-06 eta 0:00:09
epoch [194/200] batch [55/71] time 0.496 (0.464) data 0.364 (0.333) loss_u loss_u 0.9565 (0.8904) acc_u 3.1250 (13.4091) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [60/71] time 0.408 (0.461) data 0.277 (0.330) loss_u loss_u 0.9351 (0.8895) acc_u 6.2500 (13.5417) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [65/71] time 0.512 (0.460) data 0.381 (0.329) loss_u loss_u 0.9116 (0.8913) acc_u 12.5000 (13.3173) lr 7.8853e-06 eta 0:00:02
epoch [194/200] batch [70/71] time 0.600 (0.461) data 0.469 (0.330) loss_u loss_u 0.8726 (0.8895) acc_u 21.8750 (13.5268) lr 7.8853e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2079
confident_label rate tensor(0.2765, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 867
clean true:398
clean false:469
clean_rate:0.4590542099192618
noisy true:659
noisy false:1610
after delete: len(clean_dataset) 867
after delete: len(noisy_dataset) 2269
epoch [195/200] batch [5/27] time 0.418 (0.422) data 0.286 (0.291) loss_x loss_x 1.1816 (0.9965) acc_x 65.6250 (74.3750) lr 6.0390e-06 eta 0:00:09
epoch [195/200] batch [10/27] time 0.449 (0.437) data 0.318 (0.306) loss_x loss_x 0.9390 (0.9886) acc_x 81.2500 (74.0625) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [15/27] time 0.382 (0.436) data 0.251 (0.305) loss_x loss_x 1.5137 (0.9509) acc_x 59.3750 (76.2500) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [20/27] time 0.491 (0.445) data 0.359 (0.314) loss_x loss_x 0.8418 (0.9883) acc_x 75.0000 (74.3750) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [25/27] time 0.710 (0.449) data 0.579 (0.318) loss_x loss_x 1.0889 (1.0175) acc_x 62.5000 (73.3750) lr 6.0390e-06 eta 0:00:00
epoch [195/200] batch [5/70] time 0.415 (0.448) data 0.283 (0.317) loss_u loss_u 0.8726 (0.9038) acc_u 18.7500 (11.8750) lr 6.0390e-06 eta 0:00:29
epoch [195/200] batch [10/70] time 0.444 (0.443) data 0.313 (0.312) loss_u loss_u 0.9180 (0.9023) acc_u 12.5000 (12.5000) lr 6.0390e-06 eta 0:00:26
epoch [195/200] batch [15/70] time 0.660 (0.446) data 0.528 (0.315) loss_u loss_u 0.9307 (0.8993) acc_u 6.2500 (12.2917) lr 6.0390e-06 eta 0:00:24
epoch [195/200] batch [20/70] time 0.450 (0.450) data 0.319 (0.319) loss_u loss_u 0.9653 (0.9035) acc_u 6.2500 (12.0312) lr 6.0390e-06 eta 0:00:22
epoch [195/200] batch [25/70] time 0.406 (0.456) data 0.275 (0.325) loss_u loss_u 0.8496 (0.8994) acc_u 15.6250 (12.5000) lr 6.0390e-06 eta 0:00:20
epoch [195/200] batch [30/70] time 0.494 (0.455) data 0.363 (0.324) loss_u loss_u 0.9033 (0.8934) acc_u 9.3750 (13.2292) lr 6.0390e-06 eta 0:00:18
epoch [195/200] batch [35/70] time 0.392 (0.452) data 0.262 (0.321) loss_u loss_u 0.8906 (0.8949) acc_u 12.5000 (13.1250) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [40/70] time 0.390 (0.456) data 0.257 (0.325) loss_u loss_u 0.8364 (0.8935) acc_u 25.0000 (13.2031) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [45/70] time 0.419 (0.457) data 0.286 (0.326) loss_u loss_u 0.8804 (0.8933) acc_u 12.5000 (13.1944) lr 6.0390e-06 eta 0:00:11
epoch [195/200] batch [50/70] time 0.469 (0.458) data 0.337 (0.327) loss_u loss_u 0.8311 (0.8903) acc_u 21.8750 (13.7500) lr 6.0390e-06 eta 0:00:09
epoch [195/200] batch [55/70] time 0.437 (0.459) data 0.306 (0.327) loss_u loss_u 0.9565 (0.8929) acc_u 6.2500 (13.4659) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [60/70] time 0.397 (0.456) data 0.265 (0.325) loss_u loss_u 0.9551 (0.8946) acc_u 6.2500 (13.2292) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [65/70] time 0.378 (0.459) data 0.247 (0.328) loss_u loss_u 0.8535 (0.8928) acc_u 18.7500 (13.4615) lr 6.0390e-06 eta 0:00:02
epoch [195/200] batch [70/70] time 0.463 (0.459) data 0.332 (0.327) loss_u loss_u 0.8447 (0.8924) acc_u 15.6250 (13.4375) lr 6.0390e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2105
confident_label rate tensor(0.2720, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 853
clean true:387
clean false:466
clean_rate:0.4536928487690504
noisy true:644
noisy false:1639
after delete: len(clean_dataset) 853
after delete: len(noisy_dataset) 2283
epoch [196/200] batch [5/26] time 0.490 (0.460) data 0.359 (0.329) loss_x loss_x 0.9189 (1.1148) acc_x 71.8750 (68.1250) lr 4.4380e-06 eta 0:00:09
epoch [196/200] batch [10/26] time 0.528 (0.492) data 0.398 (0.361) loss_x loss_x 1.4805 (1.2564) acc_x 75.0000 (69.6875) lr 4.4380e-06 eta 0:00:07
epoch [196/200] batch [15/26] time 0.425 (0.494) data 0.295 (0.363) loss_x loss_x 1.4072 (1.2146) acc_x 84.3750 (70.8333) lr 4.4380e-06 eta 0:00:05
epoch [196/200] batch [20/26] time 0.385 (0.486) data 0.253 (0.355) loss_x loss_x 1.4385 (1.1709) acc_x 75.0000 (72.0312) lr 4.4380e-06 eta 0:00:02
epoch [196/200] batch [25/26] time 0.549 (0.482) data 0.418 (0.351) loss_x loss_x 1.4355 (1.1800) acc_x 71.8750 (71.6250) lr 4.4380e-06 eta 0:00:00
epoch [196/200] batch [5/71] time 0.455 (0.477) data 0.324 (0.346) loss_u loss_u 0.9258 (0.9285) acc_u 6.2500 (8.1250) lr 4.4380e-06 eta 0:00:31
epoch [196/200] batch [10/71] time 0.394 (0.474) data 0.263 (0.343) loss_u loss_u 0.8203 (0.8975) acc_u 25.0000 (12.1875) lr 4.4380e-06 eta 0:00:28
epoch [196/200] batch [15/71] time 0.501 (0.474) data 0.369 (0.343) loss_u loss_u 0.8262 (0.8888) acc_u 21.8750 (13.5417) lr 4.4380e-06 eta 0:00:26
epoch [196/200] batch [20/71] time 0.357 (0.468) data 0.226 (0.337) loss_u loss_u 0.9194 (0.8883) acc_u 9.3750 (14.2188) lr 4.4380e-06 eta 0:00:23
epoch [196/200] batch [25/71] time 0.486 (0.474) data 0.356 (0.343) loss_u loss_u 0.9390 (0.8929) acc_u 9.3750 (13.5000) lr 4.4380e-06 eta 0:00:21
epoch [196/200] batch [30/71] time 0.452 (0.479) data 0.322 (0.347) loss_u loss_u 0.8809 (0.8916) acc_u 12.5000 (13.5417) lr 4.4380e-06 eta 0:00:19
epoch [196/200] batch [35/71] time 0.354 (0.472) data 0.223 (0.341) loss_u loss_u 0.9131 (0.8921) acc_u 9.3750 (13.1250) lr 4.4380e-06 eta 0:00:16
epoch [196/200] batch [40/71] time 0.421 (0.471) data 0.289 (0.340) loss_u loss_u 0.9077 (0.8916) acc_u 9.3750 (13.2031) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [45/71] time 0.502 (0.471) data 0.370 (0.340) loss_u loss_u 0.8901 (0.8939) acc_u 12.5000 (12.9861) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [50/71] time 0.526 (0.470) data 0.395 (0.339) loss_u loss_u 0.9111 (0.8920) acc_u 12.5000 (13.3750) lr 4.4380e-06 eta 0:00:09
epoch [196/200] batch [55/71] time 0.416 (0.467) data 0.286 (0.336) loss_u loss_u 0.8433 (0.8890) acc_u 21.8750 (13.7500) lr 4.4380e-06 eta 0:00:07
epoch [196/200] batch [60/71] time 0.343 (0.465) data 0.211 (0.333) loss_u loss_u 0.9390 (0.8911) acc_u 6.2500 (13.4375) lr 4.4380e-06 eta 0:00:05
epoch [196/200] batch [65/71] time 0.526 (0.465) data 0.395 (0.334) loss_u loss_u 0.9453 (0.8889) acc_u 6.2500 (13.7981) lr 4.4380e-06 eta 0:00:02
epoch [196/200] batch [70/71] time 0.430 (0.467) data 0.299 (0.335) loss_u loss_u 0.9321 (0.8887) acc_u 9.3750 (13.8393) lr 4.4380e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2070
confident_label rate tensor(0.2781, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 872
clean true:410
clean false:462
clean_rate:0.4701834862385321
noisy true:656
noisy false:1608
after delete: len(clean_dataset) 872
after delete: len(noisy_dataset) 2264
epoch [197/200] batch [5/27] time 0.463 (0.422) data 0.332 (0.291) loss_x loss_x 0.9150 (1.0192) acc_x 75.0000 (73.7500) lr 3.0827e-06 eta 0:00:09
epoch [197/200] batch [10/27] time 0.652 (0.456) data 0.521 (0.325) loss_x loss_x 1.3975 (0.9837) acc_x 65.6250 (73.7500) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [15/27] time 0.464 (0.468) data 0.333 (0.337) loss_x loss_x 0.8867 (1.0054) acc_x 84.3750 (72.5000) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [20/27] time 0.398 (0.459) data 0.267 (0.328) loss_x loss_x 0.8491 (0.9924) acc_x 81.2500 (73.7500) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [25/27] time 0.468 (0.465) data 0.336 (0.334) loss_x loss_x 0.9307 (1.0572) acc_x 71.8750 (71.8750) lr 3.0827e-06 eta 0:00:00
epoch [197/200] batch [5/70] time 0.546 (0.465) data 0.415 (0.334) loss_u loss_u 0.9634 (0.8777) acc_u 3.1250 (15.6250) lr 3.0827e-06 eta 0:00:30
epoch [197/200] batch [10/70] time 0.361 (0.463) data 0.230 (0.332) loss_u loss_u 0.9316 (0.8854) acc_u 6.2500 (13.1250) lr 3.0827e-06 eta 0:00:27
epoch [197/200] batch [15/70] time 0.446 (0.457) data 0.315 (0.326) loss_u loss_u 0.8364 (0.8849) acc_u 25.0000 (13.7500) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [20/70] time 0.526 (0.460) data 0.395 (0.329) loss_u loss_u 0.9253 (0.8944) acc_u 12.5000 (12.9688) lr 3.0827e-06 eta 0:00:23
epoch [197/200] batch [25/70] time 0.416 (0.464) data 0.284 (0.333) loss_u loss_u 0.8979 (0.8901) acc_u 12.5000 (13.3750) lr 3.0827e-06 eta 0:00:20
epoch [197/200] batch [30/70] time 0.455 (0.461) data 0.323 (0.330) loss_u loss_u 0.9023 (0.8892) acc_u 12.5000 (13.4375) lr 3.0827e-06 eta 0:00:18
epoch [197/200] batch [35/70] time 0.521 (0.467) data 0.388 (0.335) loss_u loss_u 0.9346 (0.8887) acc_u 6.2500 (13.5714) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [40/70] time 0.609 (0.473) data 0.478 (0.342) loss_u loss_u 0.9272 (0.8894) acc_u 12.5000 (13.5938) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [45/70] time 0.482 (0.478) data 0.350 (0.346) loss_u loss_u 0.9580 (0.8905) acc_u 6.2500 (13.6806) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [50/70] time 0.412 (0.476) data 0.280 (0.345) loss_u loss_u 0.8940 (0.8911) acc_u 12.5000 (13.7500) lr 3.0827e-06 eta 0:00:09
epoch [197/200] batch [55/70] time 0.380 (0.475) data 0.250 (0.343) loss_u loss_u 0.9263 (0.8903) acc_u 12.5000 (13.8636) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [60/70] time 0.441 (0.473) data 0.311 (0.342) loss_u loss_u 0.8555 (0.8918) acc_u 21.8750 (13.7500) lr 3.0827e-06 eta 0:00:04
epoch [197/200] batch [65/70] time 0.358 (0.470) data 0.227 (0.339) loss_u loss_u 0.8716 (0.8884) acc_u 18.7500 (14.3750) lr 3.0827e-06 eta 0:00:02
epoch [197/200] batch [70/70] time 0.586 (0.470) data 0.454 (0.338) loss_u loss_u 0.8389 (0.8865) acc_u 18.7500 (14.5089) lr 3.0827e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2083
confident_label rate tensor(0.2809, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 881
clean true:386
clean false:495
clean_rate:0.43813847900113506
noisy true:667
noisy false:1588
after delete: len(clean_dataset) 881
after delete: len(noisy_dataset) 2255
epoch [198/200] batch [5/27] time 0.455 (0.469) data 0.324 (0.338) loss_x loss_x 0.7969 (0.8920) acc_x 78.1250 (76.2500) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [10/27] time 0.413 (0.457) data 0.283 (0.327) loss_x loss_x 0.8633 (0.9495) acc_x 84.3750 (75.3125) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [15/27] time 0.432 (0.450) data 0.302 (0.319) loss_x loss_x 1.2598 (1.0463) acc_x 68.7500 (73.5417) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [20/27] time 0.528 (0.468) data 0.398 (0.338) loss_x loss_x 0.9980 (1.0703) acc_x 68.7500 (73.2812) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [25/27] time 0.443 (0.462) data 0.312 (0.332) loss_x loss_x 1.0254 (1.0775) acc_x 75.0000 (73.0000) lr 1.9733e-06 eta 0:00:00
epoch [198/200] batch [5/70] time 0.395 (0.451) data 0.264 (0.321) loss_u loss_u 0.8770 (0.8985) acc_u 18.7500 (13.1250) lr 1.9733e-06 eta 0:00:29
epoch [198/200] batch [10/70] time 0.389 (0.451) data 0.257 (0.320) loss_u loss_u 0.8848 (0.8905) acc_u 15.6250 (13.1250) lr 1.9733e-06 eta 0:00:27
epoch [198/200] batch [15/70] time 0.495 (0.446) data 0.364 (0.315) loss_u loss_u 0.8765 (0.8846) acc_u 15.6250 (13.7500) lr 1.9733e-06 eta 0:00:24
epoch [198/200] batch [20/70] time 0.463 (0.444) data 0.333 (0.313) loss_u loss_u 0.8921 (0.8834) acc_u 9.3750 (13.9062) lr 1.9733e-06 eta 0:00:22
epoch [198/200] batch [25/70] time 0.497 (0.445) data 0.365 (0.314) loss_u loss_u 0.9185 (0.8858) acc_u 15.6250 (14.1250) lr 1.9733e-06 eta 0:00:20
epoch [198/200] batch [30/70] time 0.348 (0.452) data 0.218 (0.321) loss_u loss_u 0.9541 (0.8893) acc_u 3.1250 (13.5417) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [35/70] time 0.416 (0.452) data 0.284 (0.321) loss_u loss_u 0.9077 (0.8886) acc_u 9.3750 (13.7500) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [40/70] time 0.470 (0.450) data 0.339 (0.319) loss_u loss_u 0.9165 (0.8891) acc_u 9.3750 (13.5938) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [45/70] time 0.404 (0.453) data 0.273 (0.322) loss_u loss_u 0.9336 (0.8926) acc_u 9.3750 (13.1944) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [50/70] time 0.437 (0.455) data 0.305 (0.324) loss_u loss_u 0.8179 (0.8917) acc_u 25.0000 (13.2500) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [55/70] time 0.501 (0.454) data 0.370 (0.323) loss_u loss_u 0.9165 (0.8933) acc_u 9.3750 (13.1818) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [60/70] time 0.411 (0.456) data 0.279 (0.325) loss_u loss_u 0.8354 (0.8940) acc_u 18.7500 (13.1771) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [65/70] time 0.500 (0.457) data 0.368 (0.326) loss_u loss_u 0.8857 (0.8958) acc_u 18.7500 (12.9808) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [70/70] time 0.446 (0.460) data 0.316 (0.329) loss_u loss_u 0.8716 (0.8974) acc_u 12.5000 (12.7232) lr 1.9733e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2060
confident_label rate tensor(0.2663, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 835
clean true:382
clean false:453
clean_rate:0.45748502994011975
noisy true:694
noisy false:1607
after delete: len(clean_dataset) 835
after delete: len(noisy_dataset) 2301
epoch [199/200] batch [5/26] time 0.705 (0.514) data 0.574 (0.383) loss_x loss_x 1.4854 (1.1570) acc_x 71.8750 (75.6250) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [10/26] time 0.479 (0.509) data 0.348 (0.378) loss_x loss_x 1.1064 (1.0975) acc_x 62.5000 (73.1250) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [15/26] time 0.336 (0.486) data 0.206 (0.355) loss_x loss_x 0.9082 (1.1031) acc_x 71.8750 (72.2917) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [20/26] time 0.401 (0.475) data 0.269 (0.344) loss_x loss_x 0.9375 (1.1302) acc_x 75.0000 (71.7188) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [25/26] time 0.411 (0.461) data 0.280 (0.330) loss_x loss_x 1.0234 (1.0905) acc_x 81.2500 (73.2500) lr 1.1101e-06 eta 0:00:00
epoch [199/200] batch [5/71] time 0.465 (0.452) data 0.334 (0.321) loss_u loss_u 0.9136 (0.8903) acc_u 15.6250 (16.2500) lr 1.1101e-06 eta 0:00:29
epoch [199/200] batch [10/71] time 0.475 (0.454) data 0.344 (0.323) loss_u loss_u 0.9248 (0.9078) acc_u 6.2500 (12.8125) lr 1.1101e-06 eta 0:00:27
epoch [199/200] batch [15/71] time 0.478 (0.456) data 0.347 (0.325) loss_u loss_u 0.8936 (0.9065) acc_u 12.5000 (12.5000) lr 1.1101e-06 eta 0:00:25
epoch [199/200] batch [20/71] time 0.411 (0.462) data 0.280 (0.330) loss_u loss_u 0.8955 (0.9047) acc_u 9.3750 (12.3438) lr 1.1101e-06 eta 0:00:23
epoch [199/200] batch [25/71] time 0.444 (0.460) data 0.313 (0.329) loss_u loss_u 0.8911 (0.8966) acc_u 15.6250 (13.3750) lr 1.1101e-06 eta 0:00:21
epoch [199/200] batch [30/71] time 0.515 (0.461) data 0.384 (0.330) loss_u loss_u 0.9380 (0.8986) acc_u 6.2500 (12.9167) lr 1.1101e-06 eta 0:00:18
epoch [199/200] batch [35/71] time 0.455 (0.468) data 0.324 (0.337) loss_u loss_u 0.8730 (0.8957) acc_u 12.5000 (13.3036) lr 1.1101e-06 eta 0:00:16
epoch [199/200] batch [40/71] time 0.484 (0.465) data 0.353 (0.334) loss_u loss_u 0.8950 (0.8912) acc_u 9.3750 (14.0625) lr 1.1101e-06 eta 0:00:14
epoch [199/200] batch [45/71] time 0.631 (0.465) data 0.500 (0.334) loss_u loss_u 0.9390 (0.8910) acc_u 9.3750 (14.0972) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [50/71] time 0.593 (0.465) data 0.462 (0.333) loss_u loss_u 0.9106 (0.8921) acc_u 18.7500 (14.0625) lr 1.1101e-06 eta 0:00:09
epoch [199/200] batch [55/71] time 0.409 (0.464) data 0.278 (0.333) loss_u loss_u 0.9609 (0.8923) acc_u 3.1250 (13.9773) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [60/71] time 0.768 (0.468) data 0.637 (0.337) loss_u loss_u 0.9199 (0.8900) acc_u 6.2500 (14.1146) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [65/71] time 0.432 (0.469) data 0.301 (0.338) loss_u loss_u 0.8735 (0.8899) acc_u 18.7500 (14.0385) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [70/71] time 0.473 (0.470) data 0.343 (0.339) loss_u loss_u 0.8633 (0.8899) acc_u 18.7500 (14.1964) lr 1.1101e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2103
confident_label rate tensor(0.2774, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 870
clean true:383
clean false:487
clean_rate:0.44022988505747124
noisy true:650
noisy false:1616
after delete: len(clean_dataset) 870
after delete: len(noisy_dataset) 2266
epoch [200/200] batch [5/27] time 0.378 (0.445) data 0.248 (0.315) loss_x loss_x 1.2861 (1.2055) acc_x 68.7500 (68.7500) lr 4.9344e-07 eta 0:00:09
epoch [200/200] batch [10/27] time 0.419 (0.444) data 0.288 (0.314) loss_x loss_x 1.2783 (1.1848) acc_x 53.1250 (68.1250) lr 4.9344e-07 eta 0:00:07
epoch [200/200] batch [15/27] time 0.701 (0.452) data 0.570 (0.322) loss_x loss_x 1.5088 (1.1919) acc_x 50.0000 (67.2917) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [20/27] time 0.356 (0.464) data 0.225 (0.334) loss_x loss_x 0.5454 (1.1342) acc_x 90.6250 (70.6250) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [25/27] time 0.430 (0.466) data 0.300 (0.335) loss_x loss_x 1.5732 (1.1351) acc_x 68.7500 (70.7500) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [5/70] time 0.383 (0.458) data 0.251 (0.327) loss_u loss_u 0.9155 (0.8688) acc_u 9.3750 (15.0000) lr 4.9344e-07 eta 0:00:29
epoch [200/200] batch [10/70] time 0.597 (0.459) data 0.465 (0.328) loss_u loss_u 0.8745 (0.8807) acc_u 12.5000 (14.0625) lr 4.9344e-07 eta 0:00:27
epoch [200/200] batch [15/70] time 0.414 (0.456) data 0.282 (0.325) loss_u loss_u 0.9521 (0.8823) acc_u 6.2500 (14.5833) lr 4.9344e-07 eta 0:00:25
epoch [200/200] batch [20/70] time 0.399 (0.456) data 0.268 (0.324) loss_u loss_u 0.9180 (0.8786) acc_u 9.3750 (15.0000) lr 4.9344e-07 eta 0:00:22
epoch [200/200] batch [25/70] time 0.641 (0.455) data 0.509 (0.324) loss_u loss_u 0.8784 (0.8829) acc_u 12.5000 (14.2500) lr 4.9344e-07 eta 0:00:20
epoch [200/200] batch [30/70] time 0.663 (0.456) data 0.531 (0.325) loss_u loss_u 0.9316 (0.8866) acc_u 6.2500 (13.8542) lr 4.9344e-07 eta 0:00:18
epoch [200/200] batch [35/70] time 0.484 (0.451) data 0.352 (0.320) loss_u loss_u 0.8838 (0.8829) acc_u 18.7500 (14.5536) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [40/70] time 0.432 (0.450) data 0.300 (0.318) loss_u loss_u 0.9043 (0.8857) acc_u 15.6250 (14.2188) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [45/70] time 0.408 (0.445) data 0.276 (0.314) loss_u loss_u 0.9580 (0.8867) acc_u 3.1250 (14.2361) lr 4.9344e-07 eta 0:00:11
epoch [200/200] batch [50/70] time 0.376 (0.442) data 0.244 (0.310) loss_u loss_u 0.9282 (0.8888) acc_u 9.3750 (13.9375) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [55/70] time 0.452 (0.443) data 0.320 (0.311) loss_u loss_u 0.9355 (0.8901) acc_u 3.1250 (13.8636) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [60/70] time 0.415 (0.444) data 0.283 (0.313) loss_u loss_u 0.8916 (0.8917) acc_u 9.3750 (13.5417) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [65/70] time 0.552 (0.444) data 0.420 (0.312) loss_u loss_u 0.9082 (0.8931) acc_u 9.3750 (13.5577) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [70/70] time 0.443 (0.446) data 0.311 (0.314) loss_u loss_u 0.8418 (0.8928) acc_u 21.8750 (13.5714) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.75/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 3,184
* accuracy: 39.6%
* error: 60.4%
* macro_f1: 36.9%
Elapsed: 5:04:42
