***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.375', 'DATASET.NOISE_TYPE', 'sym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.375/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.375
  NOISE_TYPE: sym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.375/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.375/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.1177, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 369
clean true:363
clean false:6
clean_rate:0.983739837398374
noisy true:202
noisy false:2565
after delete: len(clean_dataset) 369
after delete: len(noisy_dataset) 2767
epoch [1/200] batch [5/11] time 0.435 (0.631) data 0.304 (0.447) loss_x loss_x 3.0117 (3.0516) acc_x 31.2500 (35.6250) lr 1.0000e-05 eta 0:00:03
epoch [1/200] batch [10/11] time 0.575 (0.617) data 0.357 (0.450) loss_x loss_x 2.3750 (2.7567) acc_x 53.1250 (45.0000) lr 1.0000e-05 eta 0:00:00
epoch [1/200] batch [5/86] time 0.481 (0.578) data 0.346 (0.422) loss_u loss_u 0.9712 (0.9705) acc_u 3.1250 (6.8750) lr 1.0000e-05 eta 0:00:46
epoch [1/200] batch [10/86] time 0.437 (0.596) data 0.304 (0.444) loss_u loss_u 0.9458 (0.9606) acc_u 15.6250 (9.3750) lr 1.0000e-05 eta 0:00:45
epoch [1/200] batch [15/86] time 0.820 (0.599) data 0.688 (0.449) loss_u loss_u 0.9663 (0.9555) acc_u 3.1250 (9.7917) lr 1.0000e-05 eta 0:00:42
epoch [1/200] batch [20/86] time 0.505 (0.579) data 0.373 (0.430) loss_u loss_u 0.9126 (0.9529) acc_u 15.6250 (9.6875) lr 1.0000e-05 eta 0:00:38
epoch [1/200] batch [25/86] time 0.364 (0.564) data 0.233 (0.412) loss_u loss_u 0.9448 (0.9514) acc_u 6.2500 (9.7500) lr 1.0000e-05 eta 0:00:34
epoch [1/200] batch [30/86] time 0.303 (0.547) data 0.165 (0.395) loss_u loss_u 0.9590 (0.9496) acc_u 15.6250 (10.4167) lr 1.0000e-05 eta 0:00:30
epoch [1/200] batch [35/86] time 0.491 (0.538) data 0.359 (0.386) loss_u loss_u 0.9121 (0.9484) acc_u 15.6250 (10.5357) lr 1.0000e-05 eta 0:00:27
epoch [1/200] batch [40/86] time 0.421 (0.529) data 0.289 (0.376) loss_u loss_u 0.9619 (0.9474) acc_u 6.2500 (10.7031) lr 1.0000e-05 eta 0:00:24
epoch [1/200] batch [45/86] time 0.411 (0.516) data 0.277 (0.364) loss_u loss_u 0.9229 (0.9477) acc_u 21.8750 (10.7639) lr 1.0000e-05 eta 0:00:21
epoch [1/200] batch [50/86] time 0.480 (0.508) data 0.346 (0.358) loss_u loss_u 0.9111 (0.9457) acc_u 12.5000 (11.0000) lr 1.0000e-05 eta 0:00:18
epoch [1/200] batch [55/86] time 0.501 (0.502) data 0.369 (0.353) loss_u loss_u 0.9365 (0.9463) acc_u 12.5000 (10.6818) lr 1.0000e-05 eta 0:00:15
epoch [1/200] batch [60/86] time 0.401 (0.494) data 0.265 (0.346) loss_u loss_u 0.9175 (0.9457) acc_u 18.7500 (10.8854) lr 1.0000e-05 eta 0:00:12
epoch [1/200] batch [65/86] time 0.351 (0.487) data 0.219 (0.338) loss_u loss_u 0.9536 (0.9463) acc_u 6.2500 (10.5769) lr 1.0000e-05 eta 0:00:10
epoch [1/200] batch [70/86] time 0.614 (0.486) data 0.306 (0.336) loss_u loss_u 0.9385 (0.9445) acc_u 15.6250 (10.8036) lr 1.0000e-05 eta 0:00:07
epoch [1/200] batch [75/86] time 0.347 (0.479) data 0.215 (0.330) loss_u loss_u 0.9375 (0.9443) acc_u 6.2500 (10.7500) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [80/86] time 0.439 (0.476) data 0.307 (0.327) loss_u loss_u 0.9448 (0.9436) acc_u 12.5000 (10.9375) lr 1.0000e-05 eta 0:00:02
epoch [1/200] batch [85/86] time 0.398 (0.471) data 0.266 (0.323) loss_u loss_u 0.9443 (0.9441) acc_u 9.3750 (10.8088) lr 1.0000e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2159
confident_label rate tensor(0.2018, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 633
clean true:629
clean false:4
clean_rate:0.9936808846761453
noisy true:348
noisy false:2155
after delete: len(clean_dataset) 633
after delete: len(noisy_dataset) 2503
epoch [2/200] batch [5/19] time 0.395 (0.504) data 0.263 (0.373) loss_x loss_x 1.5811 (1.8223) acc_x 53.1250 (55.0000) lr 2.0000e-03 eta 0:00:07
epoch [2/200] batch [10/19] time 0.471 (0.514) data 0.340 (0.369) loss_x loss_x 1.5156 (1.8142) acc_x 59.3750 (55.3125) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [15/19] time 0.533 (0.518) data 0.400 (0.369) loss_x loss_x 2.3457 (1.8488) acc_x 37.5000 (53.1250) lr 2.0000e-03 eta 0:00:02
epoch [2/200] batch [5/78] time 0.491 (0.513) data 0.357 (0.367) loss_u loss_u 0.9102 (0.8795) acc_u 12.5000 (17.5000) lr 2.0000e-03 eta 0:00:37
epoch [2/200] batch [10/78] time 0.452 (0.503) data 0.317 (0.359) loss_u loss_u 0.8896 (0.8853) acc_u 12.5000 (16.8750) lr 2.0000e-03 eta 0:00:34
epoch [2/200] batch [15/78] time 0.582 (0.507) data 0.449 (0.365) loss_u loss_u 0.8652 (0.8873) acc_u 28.1250 (16.8750) lr 2.0000e-03 eta 0:00:31
epoch [2/200] batch [20/78] time 0.483 (0.512) data 0.349 (0.371) loss_u loss_u 0.8472 (0.8781) acc_u 25.0000 (18.4375) lr 2.0000e-03 eta 0:00:29
epoch [2/200] batch [25/78] time 0.483 (0.509) data 0.350 (0.369) loss_u loss_u 0.8677 (0.8834) acc_u 18.7500 (17.2500) lr 2.0000e-03 eta 0:00:26
epoch [2/200] batch [30/78] time 0.711 (0.519) data 0.448 (0.373) loss_u loss_u 0.8701 (0.8819) acc_u 21.8750 (17.2917) lr 2.0000e-03 eta 0:00:24
epoch [2/200] batch [35/78] time 0.411 (0.513) data 0.269 (0.368) loss_u loss_u 0.9009 (0.8817) acc_u 15.6250 (16.8750) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [40/78] time 0.353 (0.510) data 0.220 (0.366) loss_u loss_u 0.8672 (0.8822) acc_u 15.6250 (16.8750) lr 2.0000e-03 eta 0:00:19
epoch [2/200] batch [45/78] time 0.334 (0.509) data 0.202 (0.364) loss_u loss_u 0.8828 (0.8816) acc_u 18.7500 (17.2222) lr 2.0000e-03 eta 0:00:16
epoch [2/200] batch [50/78] time 0.396 (0.502) data 0.264 (0.357) loss_u loss_u 0.9185 (0.8803) acc_u 6.2500 (17.3125) lr 2.0000e-03 eta 0:00:14
epoch [2/200] batch [55/78] time 0.366 (0.498) data 0.234 (0.352) loss_u loss_u 0.8804 (0.8791) acc_u 18.7500 (17.4432) lr 2.0000e-03 eta 0:00:11
epoch [2/200] batch [60/78] time 0.401 (0.493) data 0.268 (0.348) loss_u loss_u 0.8184 (0.8767) acc_u 21.8750 (17.6562) lr 2.0000e-03 eta 0:00:08
epoch [2/200] batch [65/78] time 0.448 (0.491) data 0.316 (0.346) loss_u loss_u 0.9067 (0.8759) acc_u 9.3750 (17.5481) lr 2.0000e-03 eta 0:00:06
epoch [2/200] batch [70/78] time 0.515 (0.487) data 0.383 (0.343) loss_u loss_u 0.9629 (0.8777) acc_u 0.0000 (17.0982) lr 2.0000e-03 eta 0:00:03
epoch [2/200] batch [75/78] time 0.475 (0.485) data 0.342 (0.342) loss_u loss_u 0.8765 (0.8760) acc_u 15.6250 (17.3333) lr 2.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1880
confident_label rate tensor(0.2589, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 812
clean true:805
clean false:7
clean_rate:0.9913793103448276
noisy true:451
noisy false:1873
after delete: len(clean_dataset) 812
after delete: len(noisy_dataset) 2324
epoch [3/200] batch [5/25] time 0.514 (0.502) data 0.383 (0.359) loss_x loss_x 1.6660 (1.6170) acc_x 56.2500 (57.5000) lr 1.9999e-03 eta 0:00:10
epoch [3/200] batch [10/25] time 0.883 (0.558) data 0.592 (0.405) loss_x loss_x 2.0449 (1.6495) acc_x 43.7500 (55.0000) lr 1.9999e-03 eta 0:00:08
epoch [3/200] batch [15/25] time 0.402 (0.542) data 0.272 (0.396) loss_x loss_x 1.3799 (1.5940) acc_x 59.3750 (57.0833) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [20/25] time 0.391 (0.537) data 0.261 (0.386) loss_x loss_x 2.0859 (1.6240) acc_x 46.8750 (56.7188) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [25/25] time 0.598 (0.526) data 0.330 (0.374) loss_x loss_x 2.0586 (1.6221) acc_x 46.8750 (57.0000) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/72] time 0.545 (0.527) data 0.414 (0.378) loss_u loss_u 0.9526 (0.8873) acc_u 3.1250 (14.3750) lr 1.9999e-03 eta 0:00:35
epoch [3/200] batch [10/72] time 0.592 (0.518) data 0.297 (0.367) loss_u loss_u 0.9448 (0.8908) acc_u 3.1250 (13.4375) lr 1.9999e-03 eta 0:00:32
epoch [3/200] batch [15/72] time 0.433 (0.512) data 0.301 (0.363) loss_u loss_u 0.8403 (0.8881) acc_u 21.8750 (13.7500) lr 1.9999e-03 eta 0:00:29
epoch [3/200] batch [20/72] time 0.362 (0.502) data 0.230 (0.352) loss_u loss_u 0.9443 (0.8917) acc_u 3.1250 (13.9062) lr 1.9999e-03 eta 0:00:26
epoch [3/200] batch [25/72] time 0.553 (0.496) data 0.422 (0.348) loss_u loss_u 0.8823 (0.8951) acc_u 9.3750 (13.6250) lr 1.9999e-03 eta 0:00:23
epoch [3/200] batch [30/72] time 0.486 (0.488) data 0.356 (0.340) loss_u loss_u 0.9126 (0.8939) acc_u 12.5000 (13.9583) lr 1.9999e-03 eta 0:00:20
epoch [3/200] batch [35/72] time 0.409 (0.480) data 0.279 (0.334) loss_u loss_u 0.9312 (0.8958) acc_u 12.5000 (14.1964) lr 1.9999e-03 eta 0:00:17
epoch [3/200] batch [40/72] time 0.417 (0.480) data 0.286 (0.334) loss_u loss_u 0.8696 (0.8957) acc_u 18.7500 (14.1406) lr 1.9999e-03 eta 0:00:15
epoch [3/200] batch [45/72] time 0.474 (0.482) data 0.342 (0.335) loss_u loss_u 0.8887 (0.8913) acc_u 9.3750 (14.6528) lr 1.9999e-03 eta 0:00:13
epoch [3/200] batch [50/72] time 0.373 (0.478) data 0.243 (0.331) loss_u loss_u 0.8809 (0.8891) acc_u 15.6250 (15.3125) lr 1.9999e-03 eta 0:00:10
epoch [3/200] batch [55/72] time 0.489 (0.479) data 0.357 (0.331) loss_u loss_u 0.9277 (0.8890) acc_u 9.3750 (15.3409) lr 1.9999e-03 eta 0:00:08
epoch [3/200] batch [60/72] time 0.348 (0.475) data 0.216 (0.328) loss_u loss_u 0.8735 (0.8901) acc_u 21.8750 (15.3646) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [65/72] time 0.333 (0.472) data 0.202 (0.326) loss_u loss_u 0.8447 (0.8906) acc_u 15.6250 (15.0000) lr 1.9999e-03 eta 0:00:03
epoch [3/200] batch [70/72] time 0.365 (0.471) data 0.234 (0.325) loss_u loss_u 0.8936 (0.8897) acc_u 9.3750 (15.0000) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1832
confident_label rate tensor(0.2714, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 851
clean true:848
clean false:3
clean_rate:0.9964747356051704
noisy true:456
noisy false:1829
after delete: len(clean_dataset) 851
after delete: len(noisy_dataset) 2285
epoch [4/200] batch [5/26] time 0.500 (0.475) data 0.369 (0.344) loss_x loss_x 1.5449 (1.4887) acc_x 68.7500 (60.6250) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [10/26] time 0.534 (0.520) data 0.403 (0.389) loss_x loss_x 1.5869 (1.6267) acc_x 68.7500 (59.6875) lr 1.9995e-03 eta 0:00:08
epoch [4/200] batch [15/26] time 0.524 (0.489) data 0.393 (0.359) loss_x loss_x 1.2510 (1.5869) acc_x 56.2500 (58.7500) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [20/26] time 0.373 (0.488) data 0.244 (0.350) loss_x loss_x 1.3613 (1.5953) acc_x 59.3750 (58.1250) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [25/26] time 0.465 (0.485) data 0.335 (0.344) loss_x loss_x 1.3486 (1.6308) acc_x 59.3750 (57.6250) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/71] time 0.530 (0.479) data 0.399 (0.338) loss_u loss_u 0.8789 (0.9046) acc_u 15.6250 (13.1250) lr 1.9995e-03 eta 0:00:31
epoch [4/200] batch [10/71] time 0.470 (0.474) data 0.339 (0.334) loss_u loss_u 0.9185 (0.8798) acc_u 6.2500 (16.2500) lr 1.9995e-03 eta 0:00:28
epoch [4/200] batch [15/71] time 0.367 (0.474) data 0.236 (0.336) loss_u loss_u 0.9438 (0.8856) acc_u 12.5000 (15.8333) lr 1.9995e-03 eta 0:00:26
epoch [4/200] batch [20/71] time 0.431 (0.469) data 0.300 (0.331) loss_u loss_u 0.8682 (0.8843) acc_u 15.6250 (15.3125) lr 1.9995e-03 eta 0:00:23
epoch [4/200] batch [25/71] time 0.653 (0.467) data 0.354 (0.327) loss_u loss_u 0.8486 (0.8800) acc_u 21.8750 (16.5000) lr 1.9995e-03 eta 0:00:21
epoch [4/200] batch [30/71] time 0.467 (0.462) data 0.336 (0.323) loss_u loss_u 0.8906 (0.8832) acc_u 12.5000 (15.7292) lr 1.9995e-03 eta 0:00:18
epoch [4/200] batch [35/71] time 0.456 (0.462) data 0.326 (0.323) loss_u loss_u 0.9014 (0.8846) acc_u 9.3750 (15.3571) lr 1.9995e-03 eta 0:00:16
epoch [4/200] batch [40/71] time 0.349 (0.459) data 0.217 (0.321) loss_u loss_u 0.9370 (0.8878) acc_u 6.2500 (15.0781) lr 1.9995e-03 eta 0:00:14
epoch [4/200] batch [45/71] time 0.639 (0.464) data 0.508 (0.325) loss_u loss_u 0.8999 (0.8890) acc_u 15.6250 (14.8611) lr 1.9995e-03 eta 0:00:12
epoch [4/200] batch [50/71] time 0.440 (0.460) data 0.309 (0.320) loss_u loss_u 0.8267 (0.8883) acc_u 25.0000 (15.1250) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [55/71] time 0.403 (0.459) data 0.273 (0.320) loss_u loss_u 0.8916 (0.8888) acc_u 12.5000 (15.1705) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [60/71] time 0.382 (0.454) data 0.252 (0.315) loss_u loss_u 0.8267 (0.8886) acc_u 25.0000 (15.2083) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [65/71] time 0.379 (0.452) data 0.249 (0.313) loss_u loss_u 0.9629 (0.8895) acc_u 3.1250 (15.0962) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [70/71] time 0.498 (0.452) data 0.367 (0.313) loss_u loss_u 0.8604 (0.8903) acc_u 18.7500 (15.0893) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1820
confident_label rate tensor(0.2688, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 843
clean true:841
clean false:2
clean_rate:0.9976275207591934
noisy true:475
noisy false:1818
after delete: len(clean_dataset) 843
after delete: len(noisy_dataset) 2293
epoch [5/200] batch [5/26] time 0.446 (0.444) data 0.257 (0.302) loss_x loss_x 1.5615 (1.6150) acc_x 65.6250 (55.6250) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [10/26] time 0.586 (0.480) data 0.455 (0.343) loss_x loss_x 1.4346 (1.7582) acc_x 75.0000 (54.6875) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [15/26] time 0.587 (0.496) data 0.456 (0.349) loss_x loss_x 1.2324 (1.6583) acc_x 65.6250 (57.2917) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [20/26] time 0.381 (0.480) data 0.251 (0.337) loss_x loss_x 1.4902 (1.6409) acc_x 50.0000 (56.5625) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [25/26] time 0.523 (0.487) data 0.393 (0.347) loss_x loss_x 1.2812 (1.6188) acc_x 75.0000 (57.0000) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/71] time 0.412 (0.478) data 0.281 (0.340) loss_u loss_u 0.8984 (0.8896) acc_u 12.5000 (13.7500) lr 1.9989e-03 eta 0:00:31
epoch [5/200] batch [10/71] time 0.433 (0.481) data 0.302 (0.338) loss_u loss_u 0.8501 (0.8803) acc_u 18.7500 (15.9375) lr 1.9989e-03 eta 0:00:29
epoch [5/200] batch [15/71] time 0.428 (0.478) data 0.296 (0.337) loss_u loss_u 0.8667 (0.8928) acc_u 25.0000 (14.1667) lr 1.9989e-03 eta 0:00:26
epoch [5/200] batch [20/71] time 0.355 (0.481) data 0.223 (0.337) loss_u loss_u 0.8794 (0.8933) acc_u 12.5000 (14.0625) lr 1.9989e-03 eta 0:00:24
epoch [5/200] batch [25/71] time 0.499 (0.476) data 0.368 (0.333) loss_u loss_u 0.8896 (0.8949) acc_u 18.7500 (14.0000) lr 1.9989e-03 eta 0:00:21
epoch [5/200] batch [30/71] time 0.413 (0.467) data 0.281 (0.325) loss_u loss_u 0.9126 (0.8973) acc_u 12.5000 (14.1667) lr 1.9989e-03 eta 0:00:19
epoch [5/200] batch [35/71] time 0.423 (0.467) data 0.288 (0.323) loss_u loss_u 0.8091 (0.8931) acc_u 25.0000 (14.4643) lr 1.9989e-03 eta 0:00:16
epoch [5/200] batch [40/71] time 0.570 (0.467) data 0.439 (0.324) loss_u loss_u 0.8936 (0.8948) acc_u 15.6250 (14.3750) lr 1.9989e-03 eta 0:00:14
epoch [5/200] batch [45/71] time 0.356 (0.473) data 0.223 (0.329) loss_u loss_u 0.8950 (0.8942) acc_u 12.5000 (14.3750) lr 1.9989e-03 eta 0:00:12
epoch [5/200] batch [50/71] time 0.646 (0.472) data 0.389 (0.327) loss_u loss_u 0.9268 (0.8953) acc_u 6.2500 (14.0625) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [55/71] time 0.655 (0.473) data 0.346 (0.326) loss_u loss_u 0.8691 (0.8955) acc_u 15.6250 (13.8068) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [60/71] time 0.393 (0.470) data 0.262 (0.324) loss_u loss_u 0.8584 (0.8939) acc_u 21.8750 (14.1146) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [65/71] time 0.499 (0.469) data 0.367 (0.323) loss_u loss_u 0.8511 (0.8928) acc_u 25.0000 (14.4231) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [70/71] time 0.357 (0.471) data 0.226 (0.325) loss_u loss_u 0.9634 (0.8943) acc_u 6.2500 (14.2857) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1810
confident_label rate tensor(0.2698, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 846
clean true:845
clean false:1
clean_rate:0.9988179669030733
noisy true:481
noisy false:1809
after delete: len(clean_dataset) 846
after delete: len(noisy_dataset) 2290
epoch [6/200] batch [5/26] time 0.496 (0.501) data 0.365 (0.367) loss_x loss_x 1.9170 (1.6859) acc_x 65.6250 (57.5000) lr 1.9980e-03 eta 0:00:10
epoch [6/200] batch [10/26] time 0.535 (0.473) data 0.405 (0.341) loss_x loss_x 1.8867 (1.7116) acc_x 46.8750 (56.2500) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [15/26] time 0.460 (0.493) data 0.329 (0.349) loss_x loss_x 1.6035 (1.6292) acc_x 59.3750 (58.7500) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [20/26] time 0.738 (0.503) data 0.530 (0.359) loss_x loss_x 1.1592 (1.6095) acc_x 75.0000 (59.3750) lr 1.9980e-03 eta 0:00:03
epoch [6/200] batch [25/26] time 0.542 (0.493) data 0.412 (0.351) loss_x loss_x 1.7305 (1.6350) acc_x 53.1250 (58.3750) lr 1.9980e-03 eta 0:00:00
epoch [6/200] batch [5/71] time 0.409 (0.488) data 0.278 (0.344) loss_u loss_u 0.9297 (0.8934) acc_u 12.5000 (13.1250) lr 1.9980e-03 eta 0:00:32
epoch [6/200] batch [10/71] time 0.408 (0.483) data 0.277 (0.341) loss_u loss_u 0.8472 (0.8854) acc_u 18.7500 (15.0000) lr 1.9980e-03 eta 0:00:29
epoch [6/200] batch [15/71] time 0.310 (0.477) data 0.180 (0.331) loss_u loss_u 0.8745 (0.8911) acc_u 18.7500 (13.5417) lr 1.9980e-03 eta 0:00:26
epoch [6/200] batch [20/71] time 0.588 (0.476) data 0.319 (0.330) loss_u loss_u 0.8564 (0.8900) acc_u 25.0000 (14.2188) lr 1.9980e-03 eta 0:00:24
epoch [6/200] batch [25/71] time 0.365 (0.469) data 0.235 (0.324) loss_u loss_u 0.8887 (0.8955) acc_u 15.6250 (13.2500) lr 1.9980e-03 eta 0:00:21
epoch [6/200] batch [30/71] time 0.548 (0.468) data 0.235 (0.320) loss_u loss_u 0.9102 (0.8963) acc_u 15.6250 (13.4375) lr 1.9980e-03 eta 0:00:19
epoch [6/200] batch [35/71] time 0.369 (0.461) data 0.238 (0.315) loss_u loss_u 0.9375 (0.8976) acc_u 3.1250 (13.0357) lr 1.9980e-03 eta 0:00:16
epoch [6/200] batch [40/71] time 0.376 (0.461) data 0.245 (0.316) loss_u loss_u 0.8818 (0.8969) acc_u 18.7500 (13.2031) lr 1.9980e-03 eta 0:00:14
epoch [6/200] batch [45/71] time 0.467 (0.460) data 0.337 (0.314) loss_u loss_u 0.8579 (0.8956) acc_u 28.1250 (13.4028) lr 1.9980e-03 eta 0:00:11
epoch [6/200] batch [50/71] time 0.462 (0.464) data 0.332 (0.319) loss_u loss_u 0.9106 (0.8942) acc_u 9.3750 (13.6250) lr 1.9980e-03 eta 0:00:09
epoch [6/200] batch [55/71] time 0.386 (0.465) data 0.254 (0.319) loss_u loss_u 0.9116 (0.8914) acc_u 12.5000 (14.3750) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [60/71] time 0.423 (0.467) data 0.291 (0.322) loss_u loss_u 0.9316 (0.8917) acc_u 9.3750 (14.3229) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [65/71] time 0.377 (0.466) data 0.245 (0.321) loss_u loss_u 0.8682 (0.8905) acc_u 21.8750 (14.5192) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [70/71] time 0.470 (0.463) data 0.339 (0.318) loss_u loss_u 0.9165 (0.8909) acc_u 9.3750 (14.5089) lr 1.9980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1798
confident_label rate tensor(0.2707, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 849
clean true:847
clean false:2
clean_rate:0.9976442873969376
noisy true:491
noisy false:1796
after delete: len(clean_dataset) 849
after delete: len(noisy_dataset) 2287
epoch [7/200] batch [5/26] time 0.444 (0.548) data 0.314 (0.382) loss_x loss_x 1.4541 (1.4654) acc_x 59.3750 (60.0000) lr 1.9969e-03 eta 0:00:11
epoch [7/200] batch [10/26] time 0.408 (0.496) data 0.277 (0.331) loss_x loss_x 1.1113 (1.5179) acc_x 78.1250 (61.5625) lr 1.9969e-03 eta 0:00:07
epoch [7/200] batch [15/26] time 0.482 (0.486) data 0.351 (0.333) loss_x loss_x 1.6973 (1.5641) acc_x 56.2500 (60.0000) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [20/26] time 0.471 (0.482) data 0.340 (0.334) loss_x loss_x 1.9199 (1.6166) acc_x 46.8750 (58.5938) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [25/26] time 0.500 (0.478) data 0.369 (0.333) loss_x loss_x 1.5410 (1.5786) acc_x 56.2500 (59.5000) lr 1.9969e-03 eta 0:00:00
epoch [7/200] batch [5/71] time 0.454 (0.471) data 0.323 (0.325) loss_u loss_u 0.8604 (0.9057) acc_u 18.7500 (11.2500) lr 1.9969e-03 eta 0:00:31
epoch [7/200] batch [10/71] time 0.408 (0.465) data 0.277 (0.322) loss_u loss_u 0.9180 (0.9021) acc_u 9.3750 (11.5625) lr 1.9969e-03 eta 0:00:28
epoch [7/200] batch [15/71] time 0.430 (0.460) data 0.298 (0.318) loss_u loss_u 0.9482 (0.9004) acc_u 6.2500 (11.8750) lr 1.9969e-03 eta 0:00:25
epoch [7/200] batch [20/71] time 0.388 (0.460) data 0.256 (0.315) loss_u loss_u 0.8628 (0.8979) acc_u 21.8750 (12.5000) lr 1.9969e-03 eta 0:00:23
epoch [7/200] batch [25/71] time 0.353 (0.458) data 0.222 (0.314) loss_u loss_u 0.8486 (0.8976) acc_u 18.7500 (13.1250) lr 1.9969e-03 eta 0:00:21
epoch [7/200] batch [30/71] time 0.419 (0.459) data 0.288 (0.315) loss_u loss_u 0.9575 (0.8932) acc_u 3.1250 (13.9583) lr 1.9969e-03 eta 0:00:18
epoch [7/200] batch [35/71] time 0.639 (0.459) data 0.508 (0.316) loss_u loss_u 0.8018 (0.8890) acc_u 31.2500 (14.7321) lr 1.9969e-03 eta 0:00:16
epoch [7/200] batch [40/71] time 0.322 (0.451) data 0.190 (0.309) loss_u loss_u 0.9443 (0.8907) acc_u 6.2500 (14.3750) lr 1.9969e-03 eta 0:00:13
epoch [7/200] batch [45/71] time 0.433 (0.451) data 0.301 (0.309) loss_u loss_u 0.8125 (0.8889) acc_u 18.7500 (14.5139) lr 1.9969e-03 eta 0:00:11
epoch [7/200] batch [50/71] time 0.448 (0.451) data 0.317 (0.310) loss_u loss_u 0.9185 (0.8899) acc_u 12.5000 (14.3125) lr 1.9969e-03 eta 0:00:09
epoch [7/200] batch [55/71] time 0.439 (0.449) data 0.301 (0.308) loss_u loss_u 0.8423 (0.8884) acc_u 28.1250 (14.7159) lr 1.9969e-03 eta 0:00:07
epoch [7/200] batch [60/71] time 0.377 (0.447) data 0.245 (0.307) loss_u loss_u 0.8516 (0.8870) acc_u 25.0000 (15.1562) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [65/71] time 0.402 (0.448) data 0.272 (0.308) loss_u loss_u 0.9502 (0.8889) acc_u 6.2500 (14.7115) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [70/71] time 0.429 (0.452) data 0.299 (0.311) loss_u loss_u 0.9028 (0.8888) acc_u 12.5000 (14.6875) lr 1.9969e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1791
confident_label rate tensor(0.2800, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 878
clean true:876
clean false:2
clean_rate:0.9977220956719818
noisy true:469
noisy false:1789
after delete: len(clean_dataset) 878
after delete: len(noisy_dataset) 2258
epoch [8/200] batch [5/27] time 0.404 (0.421) data 0.274 (0.291) loss_x loss_x 1.0635 (1.2406) acc_x 71.8750 (66.8750) lr 1.9956e-03 eta 0:00:09
epoch [8/200] batch [10/27] time 0.379 (0.428) data 0.248 (0.297) loss_x loss_x 1.6465 (1.3908) acc_x 56.2500 (64.0625) lr 1.9956e-03 eta 0:00:07
epoch [8/200] batch [15/27] time 0.369 (0.420) data 0.238 (0.289) loss_x loss_x 1.1055 (1.3854) acc_x 71.8750 (64.7917) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [20/27] time 0.431 (0.421) data 0.300 (0.291) loss_x loss_x 1.7666 (1.4528) acc_x 59.3750 (64.0625) lr 1.9956e-03 eta 0:00:02
epoch [8/200] batch [25/27] time 0.428 (0.447) data 0.298 (0.316) loss_x loss_x 1.7529 (1.4657) acc_x 65.6250 (64.1250) lr 1.9956e-03 eta 0:00:00
epoch [8/200] batch [5/70] time 0.573 (0.451) data 0.442 (0.320) loss_u loss_u 0.8994 (0.9120) acc_u 9.3750 (10.0000) lr 1.9956e-03 eta 0:00:29
epoch [8/200] batch [10/70] time 0.479 (0.455) data 0.346 (0.324) loss_u loss_u 0.8164 (0.8974) acc_u 21.8750 (12.1875) lr 1.9956e-03 eta 0:00:27
epoch [8/200] batch [15/70] time 0.449 (0.453) data 0.317 (0.322) loss_u loss_u 0.9375 (0.8994) acc_u 12.5000 (12.5000) lr 1.9956e-03 eta 0:00:24
epoch [8/200] batch [20/70] time 0.427 (0.448) data 0.295 (0.317) loss_u loss_u 0.8813 (0.8931) acc_u 18.7500 (13.2812) lr 1.9956e-03 eta 0:00:22
epoch [8/200] batch [25/70] time 0.388 (0.449) data 0.257 (0.318) loss_u loss_u 0.8877 (0.8846) acc_u 15.6250 (14.7500) lr 1.9956e-03 eta 0:00:20
epoch [8/200] batch [30/70] time 0.324 (0.441) data 0.193 (0.310) loss_u loss_u 0.9136 (0.8895) acc_u 3.1250 (13.6458) lr 1.9956e-03 eta 0:00:17
epoch [8/200] batch [35/70] time 0.433 (0.438) data 0.302 (0.307) loss_u loss_u 0.8955 (0.8885) acc_u 12.5000 (14.0179) lr 1.9956e-03 eta 0:00:15
epoch [8/200] batch [40/70] time 0.426 (0.436) data 0.295 (0.305) loss_u loss_u 0.8169 (0.8861) acc_u 31.2500 (14.5312) lr 1.9956e-03 eta 0:00:13
epoch [8/200] batch [45/70] time 0.497 (0.434) data 0.366 (0.303) loss_u loss_u 0.9292 (0.8890) acc_u 15.6250 (14.3750) lr 1.9956e-03 eta 0:00:10
epoch [8/200] batch [50/70] time 0.522 (0.432) data 0.390 (0.301) loss_u loss_u 0.9219 (0.8908) acc_u 15.6250 (14.1875) lr 1.9956e-03 eta 0:00:08
epoch [8/200] batch [55/70] time 0.594 (0.433) data 0.463 (0.302) loss_u loss_u 0.8813 (0.8901) acc_u 15.6250 (14.3182) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [60/70] time 0.440 (0.437) data 0.309 (0.306) loss_u loss_u 0.8477 (0.8926) acc_u 15.6250 (13.8021) lr 1.9956e-03 eta 0:00:04
epoch [8/200] batch [65/70] time 0.391 (0.439) data 0.261 (0.308) loss_u loss_u 0.8862 (0.8945) acc_u 12.5000 (13.3654) lr 1.9956e-03 eta 0:00:02
epoch [8/200] batch [70/70] time 0.368 (0.439) data 0.238 (0.307) loss_u loss_u 0.8540 (0.8942) acc_u 21.8750 (13.3929) lr 1.9956e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1758
confident_label rate tensor(0.2806, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 880
clean true:880
clean false:0
clean_rate:1.0
noisy true:498
noisy false:1758
after delete: len(clean_dataset) 880
after delete: len(noisy_dataset) 2256
epoch [9/200] batch [5/27] time 0.371 (0.492) data 0.240 (0.361) loss_x loss_x 1.5732 (1.5340) acc_x 59.3750 (64.3750) lr 1.9940e-03 eta 0:00:10
epoch [9/200] batch [10/27] time 0.305 (0.450) data 0.176 (0.319) loss_x loss_x 1.3223 (1.5457) acc_x 68.7500 (61.8750) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [15/27] time 0.593 (0.467) data 0.462 (0.336) loss_x loss_x 1.9326 (1.5161) acc_x 53.1250 (62.0833) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [20/27] time 0.418 (0.481) data 0.287 (0.350) loss_x loss_x 1.1426 (1.4732) acc_x 68.7500 (62.3438) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [25/27] time 0.642 (0.489) data 0.511 (0.358) loss_x loss_x 1.8301 (1.4759) acc_x 56.2500 (62.7500) lr 1.9940e-03 eta 0:00:00
epoch [9/200] batch [5/70] time 0.481 (0.474) data 0.350 (0.343) loss_u loss_u 0.9487 (0.8947) acc_u 12.5000 (13.1250) lr 1.9940e-03 eta 0:00:30
epoch [9/200] batch [10/70] time 0.400 (0.472) data 0.269 (0.341) loss_u loss_u 0.8560 (0.8893) acc_u 21.8750 (14.3750) lr 1.9940e-03 eta 0:00:28
epoch [9/200] batch [15/70] time 0.483 (0.474) data 0.352 (0.343) loss_u loss_u 0.8389 (0.8746) acc_u 25.0000 (17.5000) lr 1.9940e-03 eta 0:00:26
epoch [9/200] batch [20/70] time 0.402 (0.467) data 0.269 (0.336) loss_u loss_u 0.8911 (0.8774) acc_u 12.5000 (17.1875) lr 1.9940e-03 eta 0:00:23
epoch [9/200] batch [25/70] time 0.560 (0.470) data 0.428 (0.339) loss_u loss_u 0.7988 (0.8746) acc_u 28.1250 (17.5000) lr 1.9940e-03 eta 0:00:21
epoch [9/200] batch [30/70] time 0.441 (0.472) data 0.310 (0.340) loss_u loss_u 0.8931 (0.8742) acc_u 12.5000 (17.3958) lr 1.9940e-03 eta 0:00:18
epoch [9/200] batch [35/70] time 0.378 (0.462) data 0.247 (0.331) loss_u loss_u 0.8784 (0.8754) acc_u 15.6250 (17.1429) lr 1.9940e-03 eta 0:00:16
epoch [9/200] batch [40/70] time 0.447 (0.461) data 0.316 (0.330) loss_u loss_u 0.9126 (0.8803) acc_u 3.1250 (16.0156) lr 1.9940e-03 eta 0:00:13
epoch [9/200] batch [45/70] time 0.463 (0.460) data 0.331 (0.329) loss_u loss_u 0.9116 (0.8837) acc_u 9.3750 (15.3472) lr 1.9940e-03 eta 0:00:11
epoch [9/200] batch [50/70] time 0.378 (0.459) data 0.246 (0.327) loss_u loss_u 0.9277 (0.8859) acc_u 9.3750 (15.1250) lr 1.9940e-03 eta 0:00:09
epoch [9/200] batch [55/70] time 0.411 (0.455) data 0.280 (0.324) loss_u loss_u 0.8677 (0.8868) acc_u 18.7500 (15.1136) lr 1.9940e-03 eta 0:00:06
epoch [9/200] batch [60/70] time 0.516 (0.453) data 0.385 (0.322) loss_u loss_u 0.8311 (0.8855) acc_u 18.7500 (15.1042) lr 1.9940e-03 eta 0:00:04
epoch [9/200] batch [65/70] time 0.426 (0.452) data 0.295 (0.321) loss_u loss_u 0.8955 (0.8855) acc_u 12.5000 (14.9519) lr 1.9940e-03 eta 0:00:02
epoch [9/200] batch [70/70] time 0.392 (0.454) data 0.260 (0.322) loss_u loss_u 0.9321 (0.8862) acc_u 9.3750 (14.9554) lr 1.9940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1754
confident_label rate tensor(0.2886, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 905
clean true:901
clean false:4
clean_rate:0.9955801104972376
noisy true:481
noisy false:1750
after delete: len(clean_dataset) 905
after delete: len(noisy_dataset) 2231
epoch [10/200] batch [5/28] time 0.474 (0.455) data 0.345 (0.325) loss_x loss_x 1.3203 (1.5408) acc_x 62.5000 (60.6250) lr 1.9921e-03 eta 0:00:10
epoch [10/200] batch [10/28] time 0.425 (0.476) data 0.296 (0.346) loss_x loss_x 2.2266 (1.6041) acc_x 56.2500 (58.7500) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [15/28] time 0.540 (0.466) data 0.411 (0.336) loss_x loss_x 2.0117 (1.5172) acc_x 40.6250 (58.5417) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [20/28] time 0.414 (0.455) data 0.282 (0.325) loss_x loss_x 1.7295 (1.5113) acc_x 53.1250 (59.8438) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [25/28] time 0.610 (0.455) data 0.479 (0.324) loss_x loss_x 1.3477 (1.4807) acc_x 56.2500 (60.8750) lr 1.9921e-03 eta 0:00:01
epoch [10/200] batch [5/69] time 0.426 (0.445) data 0.294 (0.314) loss_u loss_u 0.8472 (0.8974) acc_u 28.1250 (15.0000) lr 1.9921e-03 eta 0:00:28
epoch [10/200] batch [10/69] time 0.379 (0.438) data 0.248 (0.307) loss_u loss_u 0.8848 (0.9144) acc_u 9.3750 (10.6250) lr 1.9921e-03 eta 0:00:25
epoch [10/200] batch [15/69] time 0.495 (0.439) data 0.364 (0.308) loss_u loss_u 0.8740 (0.9084) acc_u 15.6250 (11.8750) lr 1.9921e-03 eta 0:00:23
epoch [10/200] batch [20/69] time 0.478 (0.436) data 0.346 (0.305) loss_u loss_u 0.8906 (0.8991) acc_u 18.7500 (13.1250) lr 1.9921e-03 eta 0:00:21
epoch [10/200] batch [25/69] time 0.492 (0.442) data 0.360 (0.312) loss_u loss_u 0.9365 (0.8958) acc_u 9.3750 (13.7500) lr 1.9921e-03 eta 0:00:19
epoch [10/200] batch [30/69] time 0.450 (0.439) data 0.319 (0.308) loss_u loss_u 0.8511 (0.8950) acc_u 25.0000 (13.6458) lr 1.9921e-03 eta 0:00:17
epoch [10/200] batch [35/69] time 0.465 (0.438) data 0.334 (0.307) loss_u loss_u 0.9414 (0.8966) acc_u 0.0000 (13.3036) lr 1.9921e-03 eta 0:00:14
epoch [10/200] batch [40/69] time 0.440 (0.436) data 0.309 (0.305) loss_u loss_u 0.8882 (0.8949) acc_u 15.6250 (13.4375) lr 1.9921e-03 eta 0:00:12
epoch [10/200] batch [45/69] time 0.420 (0.434) data 0.289 (0.303) loss_u loss_u 0.9043 (0.8944) acc_u 12.5000 (13.4722) lr 1.9921e-03 eta 0:00:10
epoch [10/200] batch [50/69] time 0.463 (0.434) data 0.328 (0.303) loss_u loss_u 0.9253 (0.8965) acc_u 6.2500 (13.1250) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [55/69] time 0.459 (0.435) data 0.328 (0.304) loss_u loss_u 0.8657 (0.8936) acc_u 12.5000 (13.5795) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [60/69] time 0.412 (0.435) data 0.281 (0.304) loss_u loss_u 0.8862 (0.8933) acc_u 9.3750 (13.5938) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [65/69] time 0.586 (0.440) data 0.455 (0.308) loss_u loss_u 0.8354 (0.8929) acc_u 18.7500 (13.6538) lr 1.9921e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1803
confident_label rate tensor(0.2812, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 882
clean true:877
clean false:5
clean_rate:0.9943310657596371
noisy true:456
noisy false:1798
after delete: len(clean_dataset) 882
after delete: len(noisy_dataset) 2254
epoch [11/200] batch [5/27] time 0.356 (0.434) data 0.226 (0.304) loss_x loss_x 1.0322 (1.2365) acc_x 68.7500 (64.3750) lr 1.9900e-03 eta 0:00:09
epoch [11/200] batch [10/27] time 0.393 (0.434) data 0.263 (0.304) loss_x loss_x 1.7568 (1.4511) acc_x 59.3750 (63.1250) lr 1.9900e-03 eta 0:00:07
epoch [11/200] batch [15/27] time 0.426 (0.448) data 0.297 (0.318) loss_x loss_x 1.6865 (1.5133) acc_x 53.1250 (61.8750) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [20/27] time 0.410 (0.448) data 0.279 (0.317) loss_x loss_x 1.6914 (1.5118) acc_x 59.3750 (62.0312) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [25/27] time 0.552 (0.454) data 0.421 (0.323) loss_x loss_x 1.5654 (1.5470) acc_x 59.3750 (61.7500) lr 1.9900e-03 eta 0:00:00
epoch [11/200] batch [5/70] time 0.382 (0.454) data 0.250 (0.323) loss_u loss_u 0.8867 (0.8864) acc_u 18.7500 (16.8750) lr 1.9900e-03 eta 0:00:29
epoch [11/200] batch [10/70] time 0.373 (0.450) data 0.242 (0.319) loss_u loss_u 0.9624 (0.8990) acc_u 3.1250 (12.8125) lr 1.9900e-03 eta 0:00:26
epoch [11/200] batch [15/70] time 0.547 (0.453) data 0.416 (0.322) loss_u loss_u 0.9111 (0.8930) acc_u 6.2500 (12.9167) lr 1.9900e-03 eta 0:00:24
epoch [11/200] batch [20/70] time 0.469 (0.459) data 0.337 (0.328) loss_u loss_u 0.8418 (0.8908) acc_u 28.1250 (13.7500) lr 1.9900e-03 eta 0:00:22
epoch [11/200] batch [25/70] time 0.626 (0.461) data 0.494 (0.330) loss_u loss_u 0.8306 (0.8812) acc_u 21.8750 (14.8750) lr 1.9900e-03 eta 0:00:20
epoch [11/200] batch [30/70] time 0.492 (0.462) data 0.359 (0.331) loss_u loss_u 0.9360 (0.8881) acc_u 9.3750 (14.0625) lr 1.9900e-03 eta 0:00:18
epoch [11/200] batch [35/70] time 0.547 (0.464) data 0.416 (0.333) loss_u loss_u 0.8960 (0.8875) acc_u 6.2500 (13.9286) lr 1.9900e-03 eta 0:00:16
epoch [11/200] batch [40/70] time 0.452 (0.464) data 0.320 (0.333) loss_u loss_u 0.9014 (0.8861) acc_u 18.7500 (14.3750) lr 1.9900e-03 eta 0:00:13
epoch [11/200] batch [45/70] time 0.408 (0.466) data 0.276 (0.334) loss_u loss_u 0.7974 (0.8837) acc_u 28.1250 (14.7222) lr 1.9900e-03 eta 0:00:11
epoch [11/200] batch [50/70] time 0.359 (0.461) data 0.227 (0.330) loss_u loss_u 0.8794 (0.8838) acc_u 12.5000 (14.8125) lr 1.9900e-03 eta 0:00:09
epoch [11/200] batch [55/70] time 0.635 (0.461) data 0.503 (0.329) loss_u loss_u 0.8730 (0.8824) acc_u 21.8750 (14.8864) lr 1.9900e-03 eta 0:00:06
epoch [11/200] batch [60/70] time 0.360 (0.458) data 0.228 (0.326) loss_u loss_u 0.8638 (0.8827) acc_u 21.8750 (14.9479) lr 1.9900e-03 eta 0:00:04
epoch [11/200] batch [65/70] time 0.413 (0.456) data 0.281 (0.325) loss_u loss_u 0.9180 (0.8833) acc_u 18.7500 (15.1442) lr 1.9900e-03 eta 0:00:02
epoch [11/200] batch [70/70] time 0.461 (0.456) data 0.330 (0.324) loss_u loss_u 0.9248 (0.8838) acc_u 9.3750 (15.0893) lr 1.9900e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1783
confident_label rate tensor(0.2835, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 889
clean true:885
clean false:4
clean_rate:0.9955005624296963
noisy true:468
noisy false:1779
after delete: len(clean_dataset) 889
after delete: len(noisy_dataset) 2247
epoch [12/200] batch [5/27] time 0.339 (0.434) data 0.208 (0.303) loss_x loss_x 0.9062 (1.4447) acc_x 78.1250 (62.5000) lr 1.9877e-03 eta 0:00:09
epoch [12/200] batch [10/27] time 0.356 (0.415) data 0.225 (0.284) loss_x loss_x 1.6846 (1.5926) acc_x 56.2500 (59.3750) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [15/27] time 0.439 (0.420) data 0.309 (0.290) loss_x loss_x 1.2217 (1.5233) acc_x 71.8750 (60.4167) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [20/27] time 0.473 (0.434) data 0.344 (0.303) loss_x loss_x 1.8965 (1.4751) acc_x 59.3750 (61.2500) lr 1.9877e-03 eta 0:00:03
epoch [12/200] batch [25/27] time 0.458 (0.440) data 0.328 (0.309) loss_x loss_x 1.3008 (1.5119) acc_x 65.6250 (61.2500) lr 1.9877e-03 eta 0:00:00
epoch [12/200] batch [5/70] time 0.601 (0.453) data 0.471 (0.322) loss_u loss_u 0.8628 (0.8704) acc_u 18.7500 (15.6250) lr 1.9877e-03 eta 0:00:29
epoch [12/200] batch [10/70] time 0.461 (0.459) data 0.329 (0.328) loss_u loss_u 0.9165 (0.8932) acc_u 9.3750 (12.8125) lr 1.9877e-03 eta 0:00:27
epoch [12/200] batch [15/70] time 0.505 (0.459) data 0.373 (0.328) loss_u loss_u 0.8843 (0.8847) acc_u 12.5000 (14.5833) lr 1.9877e-03 eta 0:00:25
epoch [12/200] batch [20/70] time 0.331 (0.460) data 0.199 (0.329) loss_u loss_u 0.8330 (0.8811) acc_u 28.1250 (15.4688) lr 1.9877e-03 eta 0:00:22
epoch [12/200] batch [25/70] time 0.359 (0.454) data 0.228 (0.323) loss_u loss_u 0.9199 (0.8825) acc_u 15.6250 (15.5000) lr 1.9877e-03 eta 0:00:20
epoch [12/200] batch [30/70] time 0.379 (0.452) data 0.248 (0.321) loss_u loss_u 0.9517 (0.8856) acc_u 3.1250 (14.8958) lr 1.9877e-03 eta 0:00:18
epoch [12/200] batch [35/70] time 0.337 (0.448) data 0.206 (0.317) loss_u loss_u 0.8818 (0.8856) acc_u 15.6250 (14.7321) lr 1.9877e-03 eta 0:00:15
epoch [12/200] batch [40/70] time 0.374 (0.448) data 0.243 (0.317) loss_u loss_u 0.8730 (0.8887) acc_u 12.5000 (13.9844) lr 1.9877e-03 eta 0:00:13
epoch [12/200] batch [45/70] time 0.591 (0.445) data 0.459 (0.314) loss_u loss_u 0.9194 (0.8911) acc_u 12.5000 (13.4722) lr 1.9877e-03 eta 0:00:11
epoch [12/200] batch [50/70] time 0.500 (0.445) data 0.368 (0.314) loss_u loss_u 0.8979 (0.8921) acc_u 12.5000 (13.4375) lr 1.9877e-03 eta 0:00:08
epoch [12/200] batch [55/70] time 0.380 (0.443) data 0.249 (0.312) loss_u loss_u 0.9087 (0.8921) acc_u 9.3750 (13.4659) lr 1.9877e-03 eta 0:00:06
epoch [12/200] batch [60/70] time 0.503 (0.443) data 0.372 (0.312) loss_u loss_u 0.8252 (0.8925) acc_u 28.1250 (13.5938) lr 1.9877e-03 eta 0:00:04
epoch [12/200] batch [65/70] time 0.490 (0.444) data 0.359 (0.313) loss_u loss_u 0.8799 (0.8924) acc_u 15.6250 (13.7981) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [70/70] time 0.393 (0.444) data 0.262 (0.313) loss_u loss_u 0.8574 (0.8914) acc_u 25.0000 (14.2411) lr 1.9877e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1764
confident_label rate tensor(0.2876, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 902
clean true:898
clean false:4
clean_rate:0.9955654101995566
noisy true:474
noisy false:1760
after delete: len(clean_dataset) 902
after delete: len(noisy_dataset) 2234
epoch [13/200] batch [5/28] time 0.387 (0.441) data 0.256 (0.310) loss_x loss_x 1.1230 (1.5451) acc_x 68.7500 (61.2500) lr 1.9851e-03 eta 0:00:10
epoch [13/200] batch [10/28] time 0.672 (0.468) data 0.542 (0.338) loss_x loss_x 1.2305 (1.3778) acc_x 68.7500 (66.2500) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [15/28] time 0.444 (0.471) data 0.314 (0.341) loss_x loss_x 1.4980 (1.3935) acc_x 56.2500 (65.0000) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [20/28] time 0.343 (0.450) data 0.213 (0.320) loss_x loss_x 1.5908 (1.4099) acc_x 65.6250 (63.5938) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [25/28] time 0.428 (0.453) data 0.297 (0.322) loss_x loss_x 1.5215 (1.4697) acc_x 56.2500 (62.1250) lr 1.9851e-03 eta 0:00:01
epoch [13/200] batch [5/69] time 0.437 (0.449) data 0.306 (0.319) loss_u loss_u 0.8252 (0.8810) acc_u 18.7500 (14.3750) lr 1.9851e-03 eta 0:00:28
epoch [13/200] batch [10/69] time 0.426 (0.447) data 0.294 (0.316) loss_u loss_u 0.8599 (0.8858) acc_u 25.0000 (15.3125) lr 1.9851e-03 eta 0:00:26
epoch [13/200] batch [15/69] time 0.458 (0.447) data 0.327 (0.316) loss_u loss_u 0.9204 (0.8849) acc_u 9.3750 (15.6250) lr 1.9851e-03 eta 0:00:24
epoch [13/200] batch [20/69] time 0.564 (0.450) data 0.432 (0.319) loss_u loss_u 0.8687 (0.8890) acc_u 18.7500 (14.8438) lr 1.9851e-03 eta 0:00:22
epoch [13/200] batch [25/69] time 0.510 (0.445) data 0.379 (0.314) loss_u loss_u 0.8506 (0.8843) acc_u 18.7500 (15.1250) lr 1.9851e-03 eta 0:00:19
epoch [13/200] batch [30/69] time 0.609 (0.445) data 0.479 (0.314) loss_u loss_u 0.9126 (0.8855) acc_u 15.6250 (15.2083) lr 1.9851e-03 eta 0:00:17
epoch [13/200] batch [35/69] time 0.460 (0.445) data 0.330 (0.314) loss_u loss_u 0.9443 (0.8864) acc_u 6.2500 (15.3571) lr 1.9851e-03 eta 0:00:15
epoch [13/200] batch [40/69] time 0.614 (0.452) data 0.484 (0.321) loss_u loss_u 0.8892 (0.8859) acc_u 15.6250 (15.6250) lr 1.9851e-03 eta 0:00:13
epoch [13/200] batch [45/69] time 0.365 (0.446) data 0.234 (0.315) loss_u loss_u 0.9238 (0.8872) acc_u 9.3750 (15.1389) lr 1.9851e-03 eta 0:00:10
epoch [13/200] batch [50/69] time 0.304 (0.444) data 0.174 (0.314) loss_u loss_u 0.8647 (0.8848) acc_u 15.6250 (15.2500) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [55/69] time 0.394 (0.443) data 0.263 (0.312) loss_u loss_u 0.8823 (0.8845) acc_u 18.7500 (15.3977) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [60/69] time 0.361 (0.440) data 0.231 (0.309) loss_u loss_u 0.8613 (0.8836) acc_u 18.7500 (15.4688) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [65/69] time 0.538 (0.444) data 0.407 (0.313) loss_u loss_u 0.8394 (0.8845) acc_u 21.8750 (15.4808) lr 1.9851e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1737
confident_label rate tensor(0.2930, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 919
clean true:916
clean false:3
clean_rate:0.9967355821545157
noisy true:483
noisy false:1734
after delete: len(clean_dataset) 919
after delete: len(noisy_dataset) 2217
epoch [14/200] batch [5/28] time 0.625 (0.492) data 0.494 (0.361) loss_x loss_x 1.5215 (1.6049) acc_x 75.0000 (64.3750) lr 1.9823e-03 eta 0:00:11
epoch [14/200] batch [10/28] time 0.639 (0.502) data 0.509 (0.371) loss_x loss_x 1.1621 (1.4890) acc_x 71.8750 (62.5000) lr 1.9823e-03 eta 0:00:09
epoch [14/200] batch [15/28] time 0.622 (0.489) data 0.491 (0.358) loss_x loss_x 1.4863 (1.4800) acc_x 50.0000 (61.2500) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [20/28] time 0.479 (0.482) data 0.348 (0.351) loss_x loss_x 1.0029 (1.4689) acc_x 59.3750 (61.5625) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [25/28] time 0.410 (0.470) data 0.280 (0.339) loss_x loss_x 1.4238 (1.4991) acc_x 53.1250 (61.1250) lr 1.9823e-03 eta 0:00:01
epoch [14/200] batch [5/69] time 0.321 (0.465) data 0.190 (0.334) loss_u loss_u 0.8931 (0.8865) acc_u 12.5000 (15.6250) lr 1.9823e-03 eta 0:00:29
epoch [14/200] batch [10/69] time 0.368 (0.459) data 0.237 (0.328) loss_u loss_u 0.9033 (0.8935) acc_u 15.6250 (15.0000) lr 1.9823e-03 eta 0:00:27
epoch [14/200] batch [15/69] time 0.504 (0.458) data 0.374 (0.327) loss_u loss_u 0.9365 (0.8984) acc_u 9.3750 (13.9583) lr 1.9823e-03 eta 0:00:24
epoch [14/200] batch [20/69] time 0.402 (0.457) data 0.272 (0.326) loss_u loss_u 0.8765 (0.8845) acc_u 12.5000 (15.6250) lr 1.9823e-03 eta 0:00:22
epoch [14/200] batch [25/69] time 0.441 (0.454) data 0.309 (0.323) loss_u loss_u 0.9126 (0.8865) acc_u 12.5000 (15.0000) lr 1.9823e-03 eta 0:00:19
epoch [14/200] batch [30/69] time 0.390 (0.448) data 0.259 (0.317) loss_u loss_u 0.8599 (0.8868) acc_u 18.7500 (14.5833) lr 1.9823e-03 eta 0:00:17
epoch [14/200] batch [35/69] time 0.395 (0.440) data 0.264 (0.309) loss_u loss_u 0.8130 (0.8807) acc_u 21.8750 (15.3571) lr 1.9823e-03 eta 0:00:14
epoch [14/200] batch [40/69] time 0.641 (0.443) data 0.510 (0.312) loss_u loss_u 0.9268 (0.8810) acc_u 6.2500 (15.3906) lr 1.9823e-03 eta 0:00:12
epoch [14/200] batch [45/69] time 0.550 (0.442) data 0.420 (0.311) loss_u loss_u 0.8096 (0.8810) acc_u 34.3750 (15.6250) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [50/69] time 0.470 (0.442) data 0.340 (0.311) loss_u loss_u 0.8970 (0.8798) acc_u 15.6250 (16.0625) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [55/69] time 0.385 (0.440) data 0.255 (0.309) loss_u loss_u 0.8638 (0.8817) acc_u 18.7500 (15.7386) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [60/69] time 0.412 (0.440) data 0.278 (0.309) loss_u loss_u 0.8867 (0.8830) acc_u 12.5000 (15.5208) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [65/69] time 0.506 (0.442) data 0.374 (0.311) loss_u loss_u 0.8564 (0.8836) acc_u 15.6250 (15.2404) lr 1.9823e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1690
confident_label rate tensor(0.3042, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 954
clean true:950
clean false:4
clean_rate:0.9958071278825996
noisy true:496
noisy false:1686
after delete: len(clean_dataset) 954
after delete: len(noisy_dataset) 2182
epoch [15/200] batch [5/29] time 0.550 (0.549) data 0.421 (0.418) loss_x loss_x 1.6621 (1.3781) acc_x 65.6250 (64.3750) lr 1.9792e-03 eta 0:00:13
epoch [15/200] batch [10/29] time 0.405 (0.481) data 0.274 (0.350) loss_x loss_x 1.7500 (1.4396) acc_x 50.0000 (62.8125) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [15/29] time 0.564 (0.482) data 0.434 (0.351) loss_x loss_x 1.5859 (1.3986) acc_x 56.2500 (64.1667) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [20/29] time 0.640 (0.476) data 0.510 (0.345) loss_x loss_x 1.3438 (1.4139) acc_x 56.2500 (63.4375) lr 1.9792e-03 eta 0:00:04
epoch [15/200] batch [25/29] time 0.424 (0.476) data 0.294 (0.345) loss_x loss_x 1.5186 (1.4678) acc_x 53.1250 (61.2500) lr 1.9792e-03 eta 0:00:01
epoch [15/200] batch [5/68] time 0.446 (0.470) data 0.314 (0.340) loss_u loss_u 0.9165 (0.8959) acc_u 9.3750 (13.7500) lr 1.9792e-03 eta 0:00:29
epoch [15/200] batch [10/68] time 0.485 (0.465) data 0.353 (0.334) loss_u loss_u 0.9043 (0.9044) acc_u 18.7500 (13.7500) lr 1.9792e-03 eta 0:00:26
epoch [15/200] batch [15/68] time 0.568 (0.469) data 0.437 (0.338) loss_u loss_u 0.9009 (0.9007) acc_u 15.6250 (14.5833) lr 1.9792e-03 eta 0:00:24
epoch [15/200] batch [20/68] time 0.314 (0.462) data 0.183 (0.331) loss_u loss_u 0.8911 (0.8906) acc_u 15.6250 (15.4688) lr 1.9792e-03 eta 0:00:22
epoch [15/200] batch [25/68] time 0.383 (0.453) data 0.252 (0.322) loss_u loss_u 0.8179 (0.8843) acc_u 25.0000 (16.6250) lr 1.9792e-03 eta 0:00:19
epoch [15/200] batch [30/68] time 0.387 (0.452) data 0.257 (0.321) loss_u loss_u 0.9443 (0.8838) acc_u 6.2500 (16.7708) lr 1.9792e-03 eta 0:00:17
epoch [15/200] batch [35/68] time 0.457 (0.448) data 0.326 (0.317) loss_u loss_u 0.9741 (0.8858) acc_u 3.1250 (16.4286) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [40/68] time 0.446 (0.445) data 0.315 (0.314) loss_u loss_u 0.8970 (0.8847) acc_u 15.6250 (16.4844) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [45/68] time 0.428 (0.443) data 0.297 (0.312) loss_u loss_u 0.9326 (0.8883) acc_u 9.3750 (15.9028) lr 1.9792e-03 eta 0:00:10
epoch [15/200] batch [50/68] time 0.374 (0.441) data 0.242 (0.310) loss_u loss_u 0.8530 (0.8889) acc_u 15.6250 (15.6250) lr 1.9792e-03 eta 0:00:07
epoch [15/200] batch [55/68] time 0.317 (0.437) data 0.187 (0.306) loss_u loss_u 0.8315 (0.8865) acc_u 21.8750 (15.9091) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [60/68] time 0.423 (0.439) data 0.292 (0.308) loss_u loss_u 0.9580 (0.8885) acc_u 9.3750 (15.6250) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [65/68] time 0.369 (0.438) data 0.238 (0.307) loss_u loss_u 0.9614 (0.8902) acc_u 3.1250 (15.2885) lr 1.9792e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1731
confident_label rate tensor(0.2921, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 916
clean true:911
clean false:5
clean_rate:0.9945414847161572
noisy true:494
noisy false:1726
after delete: len(clean_dataset) 916
after delete: len(noisy_dataset) 2220
epoch [16/200] batch [5/28] time 0.456 (0.431) data 0.325 (0.301) loss_x loss_x 1.6230 (1.3982) acc_x 59.3750 (61.8750) lr 1.9759e-03 eta 0:00:09
epoch [16/200] batch [10/28] time 0.459 (0.426) data 0.328 (0.296) loss_x loss_x 1.1484 (1.3811) acc_x 68.7500 (63.4375) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [15/28] time 0.494 (0.464) data 0.364 (0.334) loss_x loss_x 1.0947 (1.2967) acc_x 81.2500 (67.2917) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [20/28] time 0.388 (0.449) data 0.258 (0.318) loss_x loss_x 1.2090 (1.2824) acc_x 62.5000 (66.2500) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [25/28] time 0.433 (0.449) data 0.302 (0.318) loss_x loss_x 2.0410 (1.3240) acc_x 46.8750 (64.5000) lr 1.9759e-03 eta 0:00:01
epoch [16/200] batch [5/69] time 0.410 (0.446) data 0.280 (0.316) loss_u loss_u 0.9453 (0.8953) acc_u 12.5000 (16.2500) lr 1.9759e-03 eta 0:00:28
epoch [16/200] batch [10/69] time 0.546 (0.441) data 0.414 (0.311) loss_u loss_u 0.9634 (0.9007) acc_u 6.2500 (15.6250) lr 1.9759e-03 eta 0:00:26
epoch [16/200] batch [15/69] time 0.321 (0.437) data 0.190 (0.306) loss_u loss_u 0.8999 (0.9042) acc_u 9.3750 (14.1667) lr 1.9759e-03 eta 0:00:23
epoch [16/200] batch [20/69] time 0.372 (0.440) data 0.241 (0.309) loss_u loss_u 0.9014 (0.8989) acc_u 9.3750 (14.2188) lr 1.9759e-03 eta 0:00:21
epoch [16/200] batch [25/69] time 0.469 (0.437) data 0.338 (0.307) loss_u loss_u 0.8926 (0.8930) acc_u 12.5000 (14.7500) lr 1.9759e-03 eta 0:00:19
epoch [16/200] batch [30/69] time 0.422 (0.439) data 0.291 (0.308) loss_u loss_u 0.8730 (0.8904) acc_u 21.8750 (15.4167) lr 1.9759e-03 eta 0:00:17
epoch [16/200] batch [35/69] time 0.366 (0.440) data 0.235 (0.309) loss_u loss_u 0.8149 (0.8905) acc_u 25.0000 (15.3571) lr 1.9759e-03 eta 0:00:14
epoch [16/200] batch [40/69] time 0.337 (0.434) data 0.206 (0.303) loss_u loss_u 0.8516 (0.8901) acc_u 18.7500 (15.0781) lr 1.9759e-03 eta 0:00:12
epoch [16/200] batch [45/69] time 0.394 (0.432) data 0.261 (0.302) loss_u loss_u 0.9077 (0.8899) acc_u 12.5000 (15.0000) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [50/69] time 0.411 (0.437) data 0.280 (0.306) loss_u loss_u 0.9341 (0.8919) acc_u 12.5000 (14.5000) lr 1.9759e-03 eta 0:00:08
epoch [16/200] batch [55/69] time 0.331 (0.435) data 0.199 (0.304) loss_u loss_u 0.9155 (0.8929) acc_u 6.2500 (14.2614) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [60/69] time 0.466 (0.436) data 0.335 (0.304) loss_u loss_u 0.9590 (0.8918) acc_u 3.1250 (14.2708) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [65/69] time 0.409 (0.438) data 0.278 (0.307) loss_u loss_u 0.8818 (0.8913) acc_u 21.8750 (14.6154) lr 1.9759e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1701
confident_label rate tensor(0.2997, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 940
clean true:938
clean false:2
clean_rate:0.997872340425532
noisy true:497
noisy false:1699
after delete: len(clean_dataset) 940
after delete: len(noisy_dataset) 2196
epoch [17/200] batch [5/29] time 0.530 (0.508) data 0.399 (0.377) loss_x loss_x 1.3438 (1.3154) acc_x 68.7500 (65.0000) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [10/29] time 0.402 (0.488) data 0.272 (0.357) loss_x loss_x 1.3936 (1.3324) acc_x 56.2500 (64.6875) lr 1.9724e-03 eta 0:00:09
epoch [17/200] batch [15/29] time 0.396 (0.474) data 0.266 (0.344) loss_x loss_x 1.5605 (1.3754) acc_x 62.5000 (64.1667) lr 1.9724e-03 eta 0:00:06
epoch [17/200] batch [20/29] time 0.480 (0.471) data 0.349 (0.340) loss_x loss_x 1.2080 (1.4159) acc_x 68.7500 (63.4375) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [25/29] time 0.419 (0.471) data 0.289 (0.340) loss_x loss_x 2.0820 (1.4197) acc_x 53.1250 (63.2500) lr 1.9724e-03 eta 0:00:01
epoch [17/200] batch [5/68] time 0.484 (0.473) data 0.352 (0.343) loss_u loss_u 0.8369 (0.8820) acc_u 28.1250 (14.3750) lr 1.9724e-03 eta 0:00:29
epoch [17/200] batch [10/68] time 0.529 (0.471) data 0.399 (0.340) loss_u loss_u 0.9561 (0.8937) acc_u 6.2500 (13.1250) lr 1.9724e-03 eta 0:00:27
epoch [17/200] batch [15/68] time 0.338 (0.461) data 0.208 (0.330) loss_u loss_u 0.9341 (0.8938) acc_u 9.3750 (12.7083) lr 1.9724e-03 eta 0:00:24
epoch [17/200] batch [20/68] time 0.415 (0.465) data 0.284 (0.334) loss_u loss_u 0.8452 (0.8876) acc_u 18.7500 (13.5938) lr 1.9724e-03 eta 0:00:22
epoch [17/200] batch [25/68] time 0.404 (0.462) data 0.274 (0.331) loss_u loss_u 0.8701 (0.8868) acc_u 21.8750 (13.8750) lr 1.9724e-03 eta 0:00:19
epoch [17/200] batch [30/68] time 0.444 (0.458) data 0.314 (0.327) loss_u loss_u 0.9233 (0.8896) acc_u 9.3750 (13.5417) lr 1.9724e-03 eta 0:00:17
epoch [17/200] batch [35/68] time 0.352 (0.455) data 0.220 (0.324) loss_u loss_u 0.9248 (0.8946) acc_u 6.2500 (12.9464) lr 1.9724e-03 eta 0:00:15
epoch [17/200] batch [40/68] time 0.342 (0.450) data 0.212 (0.319) loss_u loss_u 0.9004 (0.8951) acc_u 12.5000 (12.7344) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [45/68] time 0.465 (0.451) data 0.334 (0.320) loss_u loss_u 0.8594 (0.8909) acc_u 18.7500 (13.4028) lr 1.9724e-03 eta 0:00:10
epoch [17/200] batch [50/68] time 0.439 (0.451) data 0.308 (0.320) loss_u loss_u 0.8779 (0.8896) acc_u 21.8750 (13.7500) lr 1.9724e-03 eta 0:00:08
epoch [17/200] batch [55/68] time 0.378 (0.446) data 0.246 (0.315) loss_u loss_u 0.8369 (0.8881) acc_u 18.7500 (14.0909) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [60/68] time 0.353 (0.444) data 0.222 (0.314) loss_u loss_u 0.8374 (0.8872) acc_u 31.2500 (14.3750) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [65/68] time 0.489 (0.444) data 0.358 (0.313) loss_u loss_u 0.9019 (0.8905) acc_u 15.6250 (14.0385) lr 1.9724e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1720
confident_label rate tensor(0.2997, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 940
clean true:936
clean false:4
clean_rate:0.9957446808510638
noisy true:480
noisy false:1716
after delete: len(clean_dataset) 940
after delete: len(noisy_dataset) 2196
epoch [18/200] batch [5/29] time 0.390 (0.470) data 0.259 (0.339) loss_x loss_x 1.2676 (1.3021) acc_x 75.0000 (67.5000) lr 1.9686e-03 eta 0:00:11
epoch [18/200] batch [10/29] time 0.376 (0.455) data 0.246 (0.324) loss_x loss_x 1.0771 (1.3888) acc_x 71.8750 (66.5625) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [15/29] time 0.391 (0.454) data 0.260 (0.324) loss_x loss_x 1.2803 (1.3667) acc_x 68.7500 (66.2500) lr 1.9686e-03 eta 0:00:06
epoch [18/200] batch [20/29] time 0.438 (0.453) data 0.308 (0.323) loss_x loss_x 1.5840 (1.3533) acc_x 68.7500 (66.7188) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [25/29] time 0.546 (0.462) data 0.416 (0.332) loss_x loss_x 1.5225 (1.3793) acc_x 56.2500 (66.2500) lr 1.9686e-03 eta 0:00:01
epoch [18/200] batch [5/68] time 0.420 (0.467) data 0.288 (0.337) loss_u loss_u 0.8638 (0.8645) acc_u 21.8750 (20.0000) lr 1.9686e-03 eta 0:00:29
epoch [18/200] batch [10/68] time 0.449 (0.461) data 0.319 (0.330) loss_u loss_u 0.8926 (0.8831) acc_u 9.3750 (15.6250) lr 1.9686e-03 eta 0:00:26
epoch [18/200] batch [15/68] time 0.468 (0.462) data 0.336 (0.332) loss_u loss_u 0.8315 (0.8752) acc_u 18.7500 (15.6250) lr 1.9686e-03 eta 0:00:24
epoch [18/200] batch [20/68] time 0.439 (0.455) data 0.308 (0.324) loss_u loss_u 0.9536 (0.8878) acc_u 12.5000 (14.6875) lr 1.9686e-03 eta 0:00:21
epoch [18/200] batch [25/68] time 0.404 (0.452) data 0.272 (0.321) loss_u loss_u 0.9316 (0.8948) acc_u 6.2500 (13.7500) lr 1.9686e-03 eta 0:00:19
epoch [18/200] batch [30/68] time 0.478 (0.449) data 0.346 (0.318) loss_u loss_u 0.9424 (0.8912) acc_u 9.3750 (14.7917) lr 1.9686e-03 eta 0:00:17
epoch [18/200] batch [35/68] time 0.381 (0.446) data 0.250 (0.315) loss_u loss_u 0.9346 (0.8898) acc_u 6.2500 (14.9107) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [40/68] time 0.378 (0.444) data 0.247 (0.313) loss_u loss_u 0.7651 (0.8850) acc_u 34.3750 (15.8594) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [45/68] time 0.376 (0.448) data 0.244 (0.317) loss_u loss_u 0.8716 (0.8867) acc_u 12.5000 (15.6250) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [50/68] time 0.386 (0.446) data 0.256 (0.315) loss_u loss_u 0.8481 (0.8856) acc_u 28.1250 (15.8750) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [55/68] time 0.395 (0.444) data 0.263 (0.313) loss_u loss_u 0.8711 (0.8848) acc_u 15.6250 (15.6818) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [60/68] time 0.490 (0.444) data 0.358 (0.313) loss_u loss_u 0.8999 (0.8830) acc_u 18.7500 (15.7812) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [65/68] time 0.418 (0.444) data 0.286 (0.313) loss_u loss_u 0.8862 (0.8845) acc_u 12.5000 (15.5769) lr 1.9686e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1692
confident_label rate tensor(0.3090, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 969
clean true:967
clean false:2
clean_rate:0.9979360165118679
noisy true:477
noisy false:1690
after delete: len(clean_dataset) 969
after delete: len(noisy_dataset) 2167
epoch [19/200] batch [5/30] time 0.454 (0.450) data 0.323 (0.320) loss_x loss_x 1.5264 (1.3961) acc_x 65.6250 (63.1250) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [10/30] time 0.339 (0.454) data 0.208 (0.323) loss_x loss_x 1.5088 (1.3673) acc_x 59.3750 (63.7500) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [15/30] time 0.361 (0.453) data 0.230 (0.322) loss_x loss_x 1.9951 (1.4561) acc_x 59.3750 (62.5000) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [20/30] time 0.393 (0.456) data 0.263 (0.326) loss_x loss_x 1.4736 (1.3999) acc_x 62.5000 (64.3750) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [25/30] time 0.477 (0.463) data 0.344 (0.333) loss_x loss_x 1.9658 (1.4391) acc_x 50.0000 (63.3750) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [30/30] time 0.541 (0.474) data 0.410 (0.343) loss_x loss_x 1.6729 (1.4129) acc_x 53.1250 (63.6458) lr 1.9646e-03 eta 0:00:00
epoch [19/200] batch [5/67] time 0.502 (0.470) data 0.371 (0.339) loss_u loss_u 0.9536 (0.8780) acc_u 6.2500 (15.6250) lr 1.9646e-03 eta 0:00:29
epoch [19/200] batch [10/67] time 0.410 (0.464) data 0.279 (0.333) loss_u loss_u 0.8774 (0.8835) acc_u 18.7500 (15.9375) lr 1.9646e-03 eta 0:00:26
epoch [19/200] batch [15/67] time 0.347 (0.458) data 0.216 (0.327) loss_u loss_u 0.9438 (0.8893) acc_u 6.2500 (14.1667) lr 1.9646e-03 eta 0:00:23
epoch [19/200] batch [20/67] time 0.537 (0.462) data 0.405 (0.331) loss_u loss_u 0.8672 (0.8867) acc_u 21.8750 (15.0000) lr 1.9646e-03 eta 0:00:21
epoch [19/200] batch [25/67] time 0.425 (0.460) data 0.294 (0.329) loss_u loss_u 0.8818 (0.8919) acc_u 12.5000 (14.1250) lr 1.9646e-03 eta 0:00:19
epoch [19/200] batch [30/67] time 0.424 (0.456) data 0.293 (0.325) loss_u loss_u 0.8291 (0.8894) acc_u 21.8750 (15.0000) lr 1.9646e-03 eta 0:00:16
epoch [19/200] batch [35/67] time 0.363 (0.453) data 0.232 (0.322) loss_u loss_u 0.9614 (0.8911) acc_u 0.0000 (14.3750) lr 1.9646e-03 eta 0:00:14
epoch [19/200] batch [40/67] time 0.351 (0.449) data 0.219 (0.318) loss_u loss_u 0.8438 (0.8894) acc_u 28.1250 (14.6875) lr 1.9646e-03 eta 0:00:12
epoch [19/200] batch [45/67] time 0.352 (0.446) data 0.221 (0.315) loss_u loss_u 0.8750 (0.8898) acc_u 21.8750 (15.0000) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [50/67] time 0.410 (0.446) data 0.279 (0.315) loss_u loss_u 0.8330 (0.8904) acc_u 25.0000 (14.8125) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [55/67] time 0.441 (0.445) data 0.309 (0.314) loss_u loss_u 0.9116 (0.8902) acc_u 12.5000 (14.9432) lr 1.9646e-03 eta 0:00:05
epoch [19/200] batch [60/67] time 0.630 (0.449) data 0.499 (0.318) loss_u loss_u 0.8979 (0.8898) acc_u 12.5000 (14.8958) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [65/67] time 0.309 (0.448) data 0.178 (0.316) loss_u loss_u 0.9146 (0.8899) acc_u 9.3750 (14.8077) lr 1.9646e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1722
confident_label rate tensor(0.2966, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 930
clean true:927
clean false:3
clean_rate:0.9967741935483871
noisy true:487
noisy false:1719
after delete: len(clean_dataset) 930
after delete: len(noisy_dataset) 2206
epoch [20/200] batch [5/29] time 0.354 (0.432) data 0.225 (0.302) loss_x loss_x 1.2676 (1.3381) acc_x 75.0000 (67.5000) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [10/29] time 0.431 (0.445) data 0.301 (0.315) loss_x loss_x 1.1025 (1.3999) acc_x 71.8750 (64.0625) lr 1.9603e-03 eta 0:00:08
epoch [20/200] batch [15/29] time 0.468 (0.450) data 0.338 (0.319) loss_x loss_x 1.9180 (1.4212) acc_x 53.1250 (63.7500) lr 1.9603e-03 eta 0:00:06
epoch [20/200] batch [20/29] time 0.433 (0.446) data 0.302 (0.316) loss_x loss_x 1.9766 (1.4629) acc_x 50.0000 (62.1875) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [25/29] time 0.452 (0.450) data 0.319 (0.320) loss_x loss_x 1.6504 (1.4677) acc_x 53.1250 (61.7500) lr 1.9603e-03 eta 0:00:01
epoch [20/200] batch [5/68] time 0.395 (0.453) data 0.264 (0.322) loss_u loss_u 0.8823 (0.8557) acc_u 21.8750 (21.8750) lr 1.9603e-03 eta 0:00:28
epoch [20/200] batch [10/68] time 0.367 (0.446) data 0.237 (0.315) loss_u loss_u 0.8569 (0.8791) acc_u 25.0000 (17.5000) lr 1.9603e-03 eta 0:00:25
epoch [20/200] batch [15/68] time 0.401 (0.447) data 0.270 (0.316) loss_u loss_u 0.9038 (0.8827) acc_u 12.5000 (16.6667) lr 1.9603e-03 eta 0:00:23
epoch [20/200] batch [20/68] time 0.429 (0.450) data 0.298 (0.319) loss_u loss_u 0.9092 (0.8792) acc_u 12.5000 (17.0312) lr 1.9603e-03 eta 0:00:21
epoch [20/200] batch [25/68] time 0.387 (0.450) data 0.256 (0.320) loss_u loss_u 0.8896 (0.8739) acc_u 18.7500 (17.8750) lr 1.9603e-03 eta 0:00:19
epoch [20/200] batch [30/68] time 0.337 (0.445) data 0.206 (0.314) loss_u loss_u 0.9614 (0.8766) acc_u 6.2500 (17.6042) lr 1.9603e-03 eta 0:00:16
epoch [20/200] batch [35/68] time 0.638 (0.451) data 0.507 (0.320) loss_u loss_u 0.9360 (0.8789) acc_u 9.3750 (17.5000) lr 1.9603e-03 eta 0:00:14
epoch [20/200] batch [40/68] time 0.331 (0.447) data 0.200 (0.316) loss_u loss_u 0.8218 (0.8799) acc_u 25.0000 (17.0312) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [45/68] time 0.407 (0.445) data 0.276 (0.314) loss_u loss_u 0.8369 (0.8789) acc_u 18.7500 (16.7361) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [50/68] time 0.454 (0.442) data 0.323 (0.311) loss_u loss_u 0.9185 (0.8808) acc_u 12.5000 (16.3125) lr 1.9603e-03 eta 0:00:07
epoch [20/200] batch [55/68] time 0.335 (0.441) data 0.204 (0.310) loss_u loss_u 0.9155 (0.8819) acc_u 6.2500 (15.8523) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [60/68] time 0.468 (0.444) data 0.337 (0.313) loss_u loss_u 0.8545 (0.8804) acc_u 15.6250 (15.9375) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [65/68] time 0.441 (0.441) data 0.310 (0.311) loss_u loss_u 0.8887 (0.8802) acc_u 15.6250 (15.8173) lr 1.9603e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1670
confident_label rate tensor(0.3061, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 960
clean true:959
clean false:1
clean_rate:0.9989583333333333
noisy true:507
noisy false:1669
after delete: len(clean_dataset) 960
after delete: len(noisy_dataset) 2176
epoch [21/200] batch [5/30] time 0.537 (0.489) data 0.407 (0.359) loss_x loss_x 1.2646 (1.0863) acc_x 59.3750 (66.2500) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [10/30] time 0.406 (0.455) data 0.276 (0.325) loss_x loss_x 0.9102 (1.2187) acc_x 84.3750 (65.0000) lr 1.9558e-03 eta 0:00:09
epoch [21/200] batch [15/30] time 0.453 (0.463) data 0.322 (0.333) loss_x loss_x 1.8896 (1.3671) acc_x 59.3750 (64.1667) lr 1.9558e-03 eta 0:00:06
epoch [21/200] batch [20/30] time 0.546 (0.476) data 0.414 (0.345) loss_x loss_x 1.5518 (1.3866) acc_x 68.7500 (64.6875) lr 1.9558e-03 eta 0:00:04
epoch [21/200] batch [25/30] time 0.504 (0.469) data 0.374 (0.338) loss_x loss_x 1.3408 (1.4186) acc_x 65.6250 (63.5000) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [30/30] time 0.372 (0.472) data 0.241 (0.341) loss_x loss_x 0.9902 (1.4100) acc_x 78.1250 (63.5417) lr 1.9558e-03 eta 0:00:00
epoch [21/200] batch [5/68] time 0.433 (0.465) data 0.301 (0.334) loss_u loss_u 0.8521 (0.8507) acc_u 18.7500 (20.0000) lr 1.9558e-03 eta 0:00:29
epoch [21/200] batch [10/68] time 0.404 (0.458) data 0.273 (0.327) loss_u loss_u 0.8633 (0.8569) acc_u 25.0000 (19.3750) lr 1.9558e-03 eta 0:00:26
epoch [21/200] batch [15/68] time 0.386 (0.452) data 0.255 (0.321) loss_u loss_u 0.9087 (0.8623) acc_u 12.5000 (18.1250) lr 1.9558e-03 eta 0:00:23
epoch [21/200] batch [20/68] time 0.487 (0.456) data 0.355 (0.325) loss_u loss_u 0.9019 (0.8681) acc_u 15.6250 (18.1250) lr 1.9558e-03 eta 0:00:21
epoch [21/200] batch [25/68] time 0.461 (0.451) data 0.330 (0.320) loss_u loss_u 0.8535 (0.8744) acc_u 15.6250 (17.0000) lr 1.9558e-03 eta 0:00:19
epoch [21/200] batch [30/68] time 0.349 (0.447) data 0.217 (0.316) loss_u loss_u 0.9248 (0.8815) acc_u 9.3750 (15.7292) lr 1.9558e-03 eta 0:00:16
epoch [21/200] batch [35/68] time 0.502 (0.449) data 0.371 (0.318) loss_u loss_u 0.9014 (0.8831) acc_u 12.5000 (15.4464) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [40/68] time 0.374 (0.444) data 0.242 (0.313) loss_u loss_u 0.8511 (0.8804) acc_u 15.6250 (15.7812) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [45/68] time 0.393 (0.445) data 0.261 (0.314) loss_u loss_u 0.9082 (0.8821) acc_u 9.3750 (15.4167) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [50/68] time 0.576 (0.446) data 0.445 (0.315) loss_u loss_u 0.9497 (0.8841) acc_u 9.3750 (15.1250) lr 1.9558e-03 eta 0:00:08
epoch [21/200] batch [55/68] time 0.585 (0.448) data 0.451 (0.316) loss_u loss_u 0.9258 (0.8851) acc_u 12.5000 (15.0000) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [60/68] time 0.426 (0.451) data 0.294 (0.320) loss_u loss_u 0.9019 (0.8859) acc_u 12.5000 (15.1562) lr 1.9558e-03 eta 0:00:03
epoch [21/200] batch [65/68] time 0.483 (0.451) data 0.352 (0.319) loss_u loss_u 0.8745 (0.8848) acc_u 25.0000 (15.2885) lr 1.9558e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1646
confident_label rate tensor(0.3176, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 996
clean true:989
clean false:7
clean_rate:0.9929718875502008
noisy true:501
noisy false:1639
after delete: len(clean_dataset) 996
after delete: len(noisy_dataset) 2140
epoch [22/200] batch [5/31] time 0.505 (0.506) data 0.374 (0.375) loss_x loss_x 1.5859 (1.4099) acc_x 59.3750 (61.8750) lr 1.9511e-03 eta 0:00:13
epoch [22/200] batch [10/31] time 0.428 (0.510) data 0.297 (0.380) loss_x loss_x 1.2520 (1.4817) acc_x 71.8750 (62.5000) lr 1.9511e-03 eta 0:00:10
epoch [22/200] batch [15/31] time 0.448 (0.505) data 0.318 (0.374) loss_x loss_x 1.7490 (1.4843) acc_x 62.5000 (63.3333) lr 1.9511e-03 eta 0:00:08
epoch [22/200] batch [20/31] time 0.346 (0.501) data 0.215 (0.370) loss_x loss_x 2.0488 (1.5348) acc_x 50.0000 (62.0312) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [25/31] time 0.466 (0.490) data 0.336 (0.360) loss_x loss_x 1.7256 (1.5293) acc_x 59.3750 (61.1250) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [30/31] time 0.389 (0.481) data 0.258 (0.350) loss_x loss_x 1.6719 (1.5254) acc_x 62.5000 (61.2500) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/66] time 0.399 (0.475) data 0.267 (0.344) loss_u loss_u 0.9121 (0.9140) acc_u 12.5000 (14.3750) lr 1.9511e-03 eta 0:00:28
epoch [22/200] batch [10/66] time 0.394 (0.463) data 0.263 (0.332) loss_u loss_u 0.8364 (0.8919) acc_u 25.0000 (16.5625) lr 1.9511e-03 eta 0:00:25
epoch [22/200] batch [15/66] time 0.350 (0.461) data 0.219 (0.330) loss_u loss_u 0.9292 (0.8994) acc_u 12.5000 (14.3750) lr 1.9511e-03 eta 0:00:23
epoch [22/200] batch [20/66] time 0.388 (0.453) data 0.258 (0.322) loss_u loss_u 0.9155 (0.9026) acc_u 9.3750 (13.4375) lr 1.9511e-03 eta 0:00:20
epoch [22/200] batch [25/66] time 0.402 (0.449) data 0.270 (0.318) loss_u loss_u 0.9116 (0.9038) acc_u 12.5000 (13.0000) lr 1.9511e-03 eta 0:00:18
epoch [22/200] batch [30/66] time 0.419 (0.448) data 0.287 (0.317) loss_u loss_u 0.8818 (0.9031) acc_u 15.6250 (13.1250) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [35/66] time 0.376 (0.447) data 0.245 (0.316) loss_u loss_u 0.9155 (0.9014) acc_u 15.6250 (13.5714) lr 1.9511e-03 eta 0:00:13
epoch [22/200] batch [40/66] time 0.369 (0.444) data 0.237 (0.312) loss_u loss_u 0.9644 (0.9014) acc_u 6.2500 (13.4375) lr 1.9511e-03 eta 0:00:11
epoch [22/200] batch [45/66] time 0.378 (0.442) data 0.247 (0.311) loss_u loss_u 0.8828 (0.8988) acc_u 15.6250 (13.6806) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [50/66] time 0.523 (0.445) data 0.391 (0.314) loss_u loss_u 0.8096 (0.8969) acc_u 25.0000 (13.8125) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [55/66] time 0.408 (0.444) data 0.276 (0.313) loss_u loss_u 0.8623 (0.8953) acc_u 12.5000 (14.0341) lr 1.9511e-03 eta 0:00:04
epoch [22/200] batch [60/66] time 0.423 (0.443) data 0.292 (0.312) loss_u loss_u 0.8765 (0.8946) acc_u 15.6250 (14.2188) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [65/66] time 0.443 (0.445) data 0.311 (0.313) loss_u loss_u 0.8984 (0.8917) acc_u 12.5000 (14.6635) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1655
confident_label rate tensor(0.3106, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 974
clean true:970
clean false:4
clean_rate:0.9958932238193019
noisy true:511
noisy false:1651
after delete: len(clean_dataset) 974
after delete: len(noisy_dataset) 2162
epoch [23/200] batch [5/30] time 0.417 (0.466) data 0.286 (0.335) loss_x loss_x 0.9521 (1.3373) acc_x 68.7500 (66.2500) lr 1.9461e-03 eta 0:00:11
epoch [23/200] batch [10/30] time 0.418 (0.462) data 0.287 (0.331) loss_x loss_x 1.7471 (1.4223) acc_x 56.2500 (64.6875) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [15/30] time 0.508 (0.466) data 0.378 (0.335) loss_x loss_x 1.6660 (1.4503) acc_x 62.5000 (64.5833) lr 1.9461e-03 eta 0:00:06
epoch [23/200] batch [20/30] time 0.438 (0.461) data 0.308 (0.330) loss_x loss_x 1.5029 (1.4284) acc_x 62.5000 (65.3125) lr 1.9461e-03 eta 0:00:04
epoch [23/200] batch [25/30] time 0.366 (0.449) data 0.235 (0.318) loss_x loss_x 1.5420 (1.4144) acc_x 56.2500 (64.3750) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [30/30] time 0.527 (0.461) data 0.397 (0.330) loss_x loss_x 1.9248 (1.4085) acc_x 59.3750 (64.7917) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/67] time 0.424 (0.455) data 0.293 (0.324) loss_u loss_u 0.8638 (0.9061) acc_u 18.7500 (11.2500) lr 1.9461e-03 eta 0:00:28
epoch [23/200] batch [10/67] time 0.479 (0.456) data 0.349 (0.325) loss_u loss_u 0.8804 (0.8889) acc_u 15.6250 (14.3750) lr 1.9461e-03 eta 0:00:25
epoch [23/200] batch [15/67] time 0.459 (0.448) data 0.329 (0.318) loss_u loss_u 0.9302 (0.8912) acc_u 9.3750 (14.3750) lr 1.9461e-03 eta 0:00:23
epoch [23/200] batch [20/67] time 0.444 (0.445) data 0.314 (0.314) loss_u loss_u 0.8662 (0.8890) acc_u 12.5000 (14.0625) lr 1.9461e-03 eta 0:00:20
epoch [23/200] batch [25/67] time 0.336 (0.442) data 0.205 (0.312) loss_u loss_u 0.9575 (0.8914) acc_u 6.2500 (14.6250) lr 1.9461e-03 eta 0:00:18
epoch [23/200] batch [30/67] time 0.502 (0.442) data 0.372 (0.311) loss_u loss_u 0.8687 (0.8895) acc_u 21.8750 (14.8958) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [35/67] time 0.476 (0.443) data 0.346 (0.312) loss_u loss_u 0.8833 (0.8866) acc_u 9.3750 (14.8214) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [40/67] time 0.387 (0.441) data 0.255 (0.310) loss_u loss_u 0.8936 (0.8880) acc_u 15.6250 (14.6094) lr 1.9461e-03 eta 0:00:11
epoch [23/200] batch [45/67] time 0.377 (0.439) data 0.246 (0.308) loss_u loss_u 0.9233 (0.8905) acc_u 12.5000 (14.5139) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [50/67] time 0.369 (0.440) data 0.238 (0.309) loss_u loss_u 0.8428 (0.8848) acc_u 18.7500 (15.0625) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [55/67] time 0.464 (0.438) data 0.330 (0.308) loss_u loss_u 0.8882 (0.8845) acc_u 12.5000 (15.4545) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [60/67] time 0.547 (0.439) data 0.416 (0.308) loss_u loss_u 0.9541 (0.8874) acc_u 6.2500 (15.1562) lr 1.9461e-03 eta 0:00:03
epoch [23/200] batch [65/67] time 0.395 (0.440) data 0.264 (0.309) loss_u loss_u 0.8794 (0.8883) acc_u 15.6250 (15.1442) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1675
confident_label rate tensor(0.3017, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 946
clean true:943
clean false:3
clean_rate:0.9968287526427061
noisy true:518
noisy false:1672
after delete: len(clean_dataset) 946
after delete: len(noisy_dataset) 2190
epoch [24/200] batch [5/29] time 0.402 (0.432) data 0.272 (0.301) loss_x loss_x 1.4941 (1.3515) acc_x 65.6250 (68.7500) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [10/29] time 0.451 (0.427) data 0.321 (0.296) loss_x loss_x 1.6953 (1.3823) acc_x 53.1250 (66.5625) lr 1.9409e-03 eta 0:00:08
epoch [24/200] batch [15/29] time 0.429 (0.427) data 0.299 (0.296) loss_x loss_x 1.0586 (1.3563) acc_x 53.1250 (65.2083) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [20/29] time 0.346 (0.436) data 0.216 (0.305) loss_x loss_x 1.9180 (1.3676) acc_x 59.3750 (64.5312) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [25/29] time 0.669 (0.447) data 0.538 (0.317) loss_x loss_x 1.2256 (1.4282) acc_x 75.0000 (64.1250) lr 1.9409e-03 eta 0:00:01
epoch [24/200] batch [5/68] time 0.327 (0.457) data 0.196 (0.326) loss_u loss_u 0.9707 (0.9138) acc_u 3.1250 (11.8750) lr 1.9409e-03 eta 0:00:28
epoch [24/200] batch [10/68] time 0.526 (0.450) data 0.394 (0.319) loss_u loss_u 0.8647 (0.8979) acc_u 15.6250 (13.4375) lr 1.9409e-03 eta 0:00:26
epoch [24/200] batch [15/68] time 0.396 (0.448) data 0.265 (0.317) loss_u loss_u 0.8579 (0.8997) acc_u 18.7500 (13.3333) lr 1.9409e-03 eta 0:00:23
epoch [24/200] batch [20/68] time 0.385 (0.449) data 0.254 (0.318) loss_u loss_u 0.8213 (0.8880) acc_u 25.0000 (14.8438) lr 1.9409e-03 eta 0:00:21
epoch [24/200] batch [25/68] time 0.395 (0.448) data 0.265 (0.317) loss_u loss_u 0.8525 (0.8884) acc_u 21.8750 (15.1250) lr 1.9409e-03 eta 0:00:19
epoch [24/200] batch [30/68] time 0.586 (0.449) data 0.453 (0.317) loss_u loss_u 0.9307 (0.8853) acc_u 6.2500 (15.4167) lr 1.9409e-03 eta 0:00:17
epoch [24/200] batch [35/68] time 0.403 (0.445) data 0.272 (0.314) loss_u loss_u 0.9346 (0.8887) acc_u 9.3750 (14.6429) lr 1.9409e-03 eta 0:00:14
epoch [24/200] batch [40/68] time 0.437 (0.443) data 0.306 (0.312) loss_u loss_u 0.8667 (0.8854) acc_u 18.7500 (15.0781) lr 1.9409e-03 eta 0:00:12
epoch [24/200] batch [45/68] time 0.371 (0.442) data 0.240 (0.311) loss_u loss_u 0.9141 (0.8839) acc_u 9.3750 (15.2083) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [50/68] time 0.442 (0.441) data 0.312 (0.310) loss_u loss_u 0.9375 (0.8859) acc_u 6.2500 (14.8750) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [55/68] time 0.415 (0.439) data 0.284 (0.308) loss_u loss_u 0.9116 (0.8878) acc_u 6.2500 (14.6591) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [60/68] time 0.396 (0.438) data 0.265 (0.306) loss_u loss_u 0.8052 (0.8875) acc_u 31.2500 (14.7917) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [65/68] time 0.603 (0.440) data 0.472 (0.309) loss_u loss_u 0.8228 (0.8872) acc_u 21.8750 (14.7596) lr 1.9409e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1684
confident_label rate tensor(0.3026, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 949
clean true:946
clean false:3
clean_rate:0.9968387776606955
noisy true:506
noisy false:1681
after delete: len(clean_dataset) 949
after delete: len(noisy_dataset) 2187
epoch [25/200] batch [5/29] time 0.396 (0.442) data 0.266 (0.311) loss_x loss_x 1.7939 (1.5039) acc_x 62.5000 (63.1250) lr 1.9354e-03 eta 0:00:10
epoch [25/200] batch [10/29] time 0.453 (0.438) data 0.323 (0.308) loss_x loss_x 1.0869 (1.4888) acc_x 65.6250 (63.4375) lr 1.9354e-03 eta 0:00:08
epoch [25/200] batch [15/29] time 0.419 (0.451) data 0.289 (0.320) loss_x loss_x 1.3320 (1.4635) acc_x 59.3750 (62.7083) lr 1.9354e-03 eta 0:00:06
epoch [25/200] batch [20/29] time 0.396 (0.446) data 0.266 (0.316) loss_x loss_x 1.8516 (1.4646) acc_x 50.0000 (62.6562) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [25/29] time 0.505 (0.456) data 0.375 (0.326) loss_x loss_x 1.3154 (1.4768) acc_x 75.0000 (63.2500) lr 1.9354e-03 eta 0:00:01
epoch [25/200] batch [5/68] time 0.431 (0.452) data 0.300 (0.322) loss_u loss_u 0.8765 (0.8721) acc_u 21.8750 (16.2500) lr 1.9354e-03 eta 0:00:28
epoch [25/200] batch [10/68] time 0.469 (0.458) data 0.334 (0.328) loss_u loss_u 0.8662 (0.8781) acc_u 18.7500 (16.2500) lr 1.9354e-03 eta 0:00:26
epoch [25/200] batch [15/68] time 0.398 (0.458) data 0.267 (0.328) loss_u loss_u 0.8892 (0.8853) acc_u 12.5000 (15.0000) lr 1.9354e-03 eta 0:00:24
epoch [25/200] batch [20/68] time 0.334 (0.457) data 0.203 (0.326) loss_u loss_u 0.8867 (0.8829) acc_u 15.6250 (15.4688) lr 1.9354e-03 eta 0:00:21
epoch [25/200] batch [25/68] time 0.400 (0.451) data 0.270 (0.320) loss_u loss_u 0.8574 (0.8826) acc_u 18.7500 (16.2500) lr 1.9354e-03 eta 0:00:19
epoch [25/200] batch [30/68] time 0.533 (0.451) data 0.402 (0.320) loss_u loss_u 0.8677 (0.8853) acc_u 18.7500 (15.8333) lr 1.9354e-03 eta 0:00:17
epoch [25/200] batch [35/68] time 0.545 (0.455) data 0.413 (0.324) loss_u loss_u 0.8447 (0.8861) acc_u 18.7500 (15.5357) lr 1.9354e-03 eta 0:00:15
epoch [25/200] batch [40/68] time 0.348 (0.452) data 0.217 (0.322) loss_u loss_u 0.8940 (0.8881) acc_u 15.6250 (15.1562) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [45/68] time 0.336 (0.448) data 0.205 (0.318) loss_u loss_u 0.9585 (0.8895) acc_u 6.2500 (14.9306) lr 1.9354e-03 eta 0:00:10
epoch [25/200] batch [50/68] time 0.440 (0.449) data 0.309 (0.319) loss_u loss_u 0.8511 (0.8879) acc_u 18.7500 (14.9375) lr 1.9354e-03 eta 0:00:08
epoch [25/200] batch [55/68] time 0.357 (0.446) data 0.225 (0.316) loss_u loss_u 0.8584 (0.8846) acc_u 15.6250 (15.4545) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [60/68] time 0.408 (0.445) data 0.276 (0.315) loss_u loss_u 0.8726 (0.8855) acc_u 15.6250 (15.4688) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [65/68] time 0.422 (0.444) data 0.290 (0.313) loss_u loss_u 0.8623 (0.8844) acc_u 21.8750 (15.8173) lr 1.9354e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1615
confident_label rate tensor(0.3189, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1000
clean true:994
clean false:6
clean_rate:0.994
noisy true:527
noisy false:1609
after delete: len(clean_dataset) 1000
after delete: len(noisy_dataset) 2136
epoch [26/200] batch [5/31] time 0.451 (0.423) data 0.320 (0.292) loss_x loss_x 1.1113 (1.1586) acc_x 68.7500 (68.1250) lr 1.9298e-03 eta 0:00:10
epoch [26/200] batch [10/31] time 0.362 (0.431) data 0.231 (0.300) loss_x loss_x 0.8101 (1.1550) acc_x 68.7500 (66.8750) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [15/31] time 0.517 (0.446) data 0.386 (0.316) loss_x loss_x 1.8477 (1.3200) acc_x 50.0000 (63.9583) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [20/31] time 0.499 (0.450) data 0.368 (0.319) loss_x loss_x 1.0693 (1.2765) acc_x 84.3750 (65.7812) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [25/31] time 0.508 (0.449) data 0.378 (0.318) loss_x loss_x 1.2861 (1.2968) acc_x 65.6250 (66.0000) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [30/31] time 0.386 (0.442) data 0.255 (0.312) loss_x loss_x 1.0820 (1.2938) acc_x 78.1250 (65.9375) lr 1.9298e-03 eta 0:00:00
epoch [26/200] batch [5/66] time 0.348 (0.441) data 0.217 (0.310) loss_u loss_u 0.8818 (0.8753) acc_u 18.7500 (15.6250) lr 1.9298e-03 eta 0:00:26
epoch [26/200] batch [10/66] time 0.405 (0.445) data 0.274 (0.314) loss_u loss_u 0.8130 (0.8801) acc_u 25.0000 (15.0000) lr 1.9298e-03 eta 0:00:24
epoch [26/200] batch [15/66] time 0.378 (0.442) data 0.246 (0.311) loss_u loss_u 0.9067 (0.8792) acc_u 12.5000 (15.0000) lr 1.9298e-03 eta 0:00:22
epoch [26/200] batch [20/66] time 0.470 (0.448) data 0.338 (0.317) loss_u loss_u 0.9204 (0.8872) acc_u 12.5000 (14.3750) lr 1.9298e-03 eta 0:00:20
epoch [26/200] batch [25/66] time 0.448 (0.444) data 0.315 (0.313) loss_u loss_u 0.9565 (0.8906) acc_u 3.1250 (14.0000) lr 1.9298e-03 eta 0:00:18
epoch [26/200] batch [30/66] time 0.341 (0.443) data 0.209 (0.312) loss_u loss_u 0.9043 (0.8912) acc_u 18.7500 (14.8958) lr 1.9298e-03 eta 0:00:15
epoch [26/200] batch [35/66] time 0.630 (0.447) data 0.499 (0.316) loss_u loss_u 0.8735 (0.8907) acc_u 15.6250 (15.0000) lr 1.9298e-03 eta 0:00:13
epoch [26/200] batch [40/66] time 0.369 (0.448) data 0.237 (0.317) loss_u loss_u 0.8638 (0.8857) acc_u 12.5000 (15.5469) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [45/66] time 0.546 (0.450) data 0.415 (0.319) loss_u loss_u 0.9009 (0.8837) acc_u 12.5000 (15.6250) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [50/66] time 0.392 (0.446) data 0.260 (0.315) loss_u loss_u 0.8687 (0.8839) acc_u 12.5000 (15.2500) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [55/66] time 0.427 (0.443) data 0.296 (0.312) loss_u loss_u 0.9160 (0.8850) acc_u 12.5000 (15.1705) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [60/66] time 0.414 (0.443) data 0.282 (0.312) loss_u loss_u 0.8950 (0.8849) acc_u 12.5000 (15.2604) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [65/66] time 0.461 (0.442) data 0.329 (0.311) loss_u loss_u 0.9033 (0.8854) acc_u 12.5000 (15.1442) lr 1.9298e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1655
confident_label rate tensor(0.3103, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 973
clean true:969
clean false:4
clean_rate:0.9958890030832477
noisy true:512
noisy false:1651
after delete: len(clean_dataset) 973
after delete: len(noisy_dataset) 2163
epoch [27/200] batch [5/30] time 0.415 (0.388) data 0.285 (0.257) loss_x loss_x 1.1396 (1.4594) acc_x 65.6250 (64.3750) lr 1.9239e-03 eta 0:00:09
epoch [27/200] batch [10/30] time 0.325 (0.392) data 0.195 (0.262) loss_x loss_x 1.7422 (1.3520) acc_x 59.3750 (66.5625) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [15/30] time 0.580 (0.425) data 0.449 (0.294) loss_x loss_x 1.0664 (1.3688) acc_x 71.8750 (65.8333) lr 1.9239e-03 eta 0:00:06
epoch [27/200] batch [20/30] time 0.484 (0.435) data 0.353 (0.304) loss_x loss_x 1.3125 (1.4035) acc_x 68.7500 (65.6250) lr 1.9239e-03 eta 0:00:04
epoch [27/200] batch [25/30] time 0.407 (0.440) data 0.276 (0.310) loss_x loss_x 1.5557 (1.4041) acc_x 50.0000 (64.3750) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [30/30] time 0.498 (0.442) data 0.367 (0.311) loss_x loss_x 1.2432 (1.3994) acc_x 68.7500 (64.5833) lr 1.9239e-03 eta 0:00:00
epoch [27/200] batch [5/67] time 0.397 (0.453) data 0.264 (0.322) loss_u loss_u 0.8486 (0.8750) acc_u 15.6250 (14.3750) lr 1.9239e-03 eta 0:00:28
epoch [27/200] batch [10/67] time 0.481 (0.453) data 0.350 (0.322) loss_u loss_u 0.8667 (0.8846) acc_u 15.6250 (14.6875) lr 1.9239e-03 eta 0:00:25
epoch [27/200] batch [15/67] time 0.535 (0.456) data 0.403 (0.325) loss_u loss_u 0.8833 (0.8888) acc_u 12.5000 (13.9583) lr 1.9239e-03 eta 0:00:23
epoch [27/200] batch [20/67] time 0.328 (0.453) data 0.196 (0.321) loss_u loss_u 0.9053 (0.8773) acc_u 12.5000 (15.6250) lr 1.9239e-03 eta 0:00:21
epoch [27/200] batch [25/67] time 0.376 (0.448) data 0.245 (0.317) loss_u loss_u 0.8779 (0.8768) acc_u 15.6250 (16.3750) lr 1.9239e-03 eta 0:00:18
epoch [27/200] batch [30/67] time 0.440 (0.447) data 0.308 (0.316) loss_u loss_u 0.9526 (0.8796) acc_u 6.2500 (15.5208) lr 1.9239e-03 eta 0:00:16
epoch [27/200] batch [35/67] time 0.545 (0.446) data 0.413 (0.315) loss_u loss_u 0.8643 (0.8781) acc_u 28.1250 (16.0714) lr 1.9239e-03 eta 0:00:14
epoch [27/200] batch [40/67] time 0.387 (0.442) data 0.256 (0.311) loss_u loss_u 0.9229 (0.8813) acc_u 15.6250 (15.7031) lr 1.9239e-03 eta 0:00:11
epoch [27/200] batch [45/67] time 0.535 (0.442) data 0.403 (0.310) loss_u loss_u 0.8120 (0.8793) acc_u 18.7500 (15.6250) lr 1.9239e-03 eta 0:00:09
epoch [27/200] batch [50/67] time 0.466 (0.442) data 0.335 (0.311) loss_u loss_u 0.8813 (0.8812) acc_u 15.6250 (15.3750) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [55/67] time 0.433 (0.441) data 0.302 (0.310) loss_u loss_u 0.8848 (0.8807) acc_u 12.5000 (15.3409) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [60/67] time 0.443 (0.445) data 0.311 (0.314) loss_u loss_u 0.9385 (0.8816) acc_u 6.2500 (15.1562) lr 1.9239e-03 eta 0:00:03
epoch [27/200] batch [65/67] time 0.397 (0.445) data 0.265 (0.313) loss_u loss_u 0.8408 (0.8818) acc_u 21.8750 (15.2885) lr 1.9239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1657
confident_label rate tensor(0.3106, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 974
clean true:971
clean false:3
clean_rate:0.9969199178644764
noisy true:508
noisy false:1654
after delete: len(clean_dataset) 974
after delete: len(noisy_dataset) 2162
epoch [28/200] batch [5/30] time 0.452 (0.446) data 0.321 (0.315) loss_x loss_x 1.5215 (1.4113) acc_x 56.2500 (61.8750) lr 1.9178e-03 eta 0:00:11
epoch [28/200] batch [10/30] time 0.482 (0.434) data 0.351 (0.304) loss_x loss_x 1.3164 (1.4105) acc_x 53.1250 (61.5625) lr 1.9178e-03 eta 0:00:08
epoch [28/200] batch [15/30] time 0.397 (0.445) data 0.266 (0.314) loss_x loss_x 1.6494 (1.4187) acc_x 53.1250 (60.8333) lr 1.9178e-03 eta 0:00:06
epoch [28/200] batch [20/30] time 0.548 (0.449) data 0.417 (0.318) loss_x loss_x 1.5811 (1.4054) acc_x 62.5000 (62.3438) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [25/30] time 0.446 (0.446) data 0.315 (0.315) loss_x loss_x 1.6650 (1.4105) acc_x 53.1250 (62.6250) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [30/30] time 0.413 (0.445) data 0.282 (0.314) loss_x loss_x 1.5596 (1.4364) acc_x 59.3750 (61.5625) lr 1.9178e-03 eta 0:00:00
epoch [28/200] batch [5/67] time 0.395 (0.450) data 0.263 (0.320) loss_u loss_u 0.9219 (0.8583) acc_u 12.5000 (20.0000) lr 1.9178e-03 eta 0:00:27
epoch [28/200] batch [10/67] time 0.441 (0.448) data 0.309 (0.317) loss_u loss_u 0.9126 (0.8663) acc_u 9.3750 (18.7500) lr 1.9178e-03 eta 0:00:25
epoch [28/200] batch [15/67] time 0.431 (0.447) data 0.300 (0.316) loss_u loss_u 0.9209 (0.8756) acc_u 9.3750 (17.2917) lr 1.9178e-03 eta 0:00:23
epoch [28/200] batch [20/67] time 0.412 (0.444) data 0.280 (0.313) loss_u loss_u 0.8940 (0.8771) acc_u 12.5000 (17.5000) lr 1.9178e-03 eta 0:00:20
epoch [28/200] batch [25/67] time 0.370 (0.444) data 0.240 (0.313) loss_u loss_u 0.9316 (0.8754) acc_u 12.5000 (17.7500) lr 1.9178e-03 eta 0:00:18
epoch [28/200] batch [30/67] time 0.394 (0.445) data 0.263 (0.314) loss_u loss_u 0.9282 (0.8788) acc_u 6.2500 (16.7708) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [35/67] time 0.594 (0.444) data 0.464 (0.313) loss_u loss_u 0.9141 (0.8821) acc_u 9.3750 (16.2500) lr 1.9178e-03 eta 0:00:14
epoch [28/200] batch [40/67] time 0.387 (0.442) data 0.256 (0.311) loss_u loss_u 0.9004 (0.8849) acc_u 12.5000 (15.7812) lr 1.9178e-03 eta 0:00:11
epoch [28/200] batch [45/67] time 0.389 (0.440) data 0.256 (0.309) loss_u loss_u 0.8926 (0.8839) acc_u 9.3750 (16.1111) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [50/67] time 0.468 (0.443) data 0.337 (0.312) loss_u loss_u 0.9448 (0.8823) acc_u 6.2500 (16.0625) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [55/67] time 0.421 (0.441) data 0.290 (0.309) loss_u loss_u 0.8638 (0.8832) acc_u 21.8750 (15.8523) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [60/67] time 0.403 (0.437) data 0.271 (0.306) loss_u loss_u 0.8486 (0.8826) acc_u 21.8750 (16.0417) lr 1.9178e-03 eta 0:00:03
epoch [28/200] batch [65/67] time 0.813 (0.440) data 0.681 (0.309) loss_u loss_u 0.8721 (0.8834) acc_u 15.6250 (15.9615) lr 1.9178e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1635
confident_label rate tensor(0.3211, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1007
clean true:1002
clean false:5
clean_rate:0.9950347567030785
noisy true:499
noisy false:1630
after delete: len(clean_dataset) 1007
after delete: len(noisy_dataset) 2129
epoch [29/200] batch [5/31] time 0.427 (0.491) data 0.296 (0.361) loss_x loss_x 1.1035 (1.2957) acc_x 78.1250 (63.7500) lr 1.9114e-03 eta 0:00:12
epoch [29/200] batch [10/31] time 0.423 (0.489) data 0.292 (0.358) loss_x loss_x 1.8623 (1.4539) acc_x 53.1250 (61.5625) lr 1.9114e-03 eta 0:00:10
epoch [29/200] batch [15/31] time 0.540 (0.484) data 0.409 (0.354) loss_x loss_x 1.2529 (1.3984) acc_x 75.0000 (64.1667) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [20/31] time 0.470 (0.479) data 0.339 (0.348) loss_x loss_x 1.2461 (1.3850) acc_x 75.0000 (63.5938) lr 1.9114e-03 eta 0:00:05
epoch [29/200] batch [25/31] time 0.476 (0.468) data 0.346 (0.338) loss_x loss_x 1.9756 (1.3878) acc_x 50.0000 (63.7500) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [30/31] time 0.475 (0.461) data 0.345 (0.330) loss_x loss_x 1.5996 (1.3952) acc_x 65.6250 (63.9583) lr 1.9114e-03 eta 0:00:00
epoch [29/200] batch [5/66] time 0.337 (0.454) data 0.204 (0.323) loss_u loss_u 0.9019 (0.8613) acc_u 12.5000 (18.7500) lr 1.9114e-03 eta 0:00:27
epoch [29/200] batch [10/66] time 0.375 (0.461) data 0.245 (0.330) loss_u loss_u 0.9146 (0.8753) acc_u 12.5000 (16.5625) lr 1.9114e-03 eta 0:00:25
epoch [29/200] batch [15/66] time 0.384 (0.456) data 0.252 (0.325) loss_u loss_u 0.8447 (0.8776) acc_u 21.8750 (16.2500) lr 1.9114e-03 eta 0:00:23
epoch [29/200] batch [20/66] time 0.437 (0.460) data 0.306 (0.329) loss_u loss_u 0.8779 (0.8828) acc_u 18.7500 (15.3125) lr 1.9114e-03 eta 0:00:21
epoch [29/200] batch [25/66] time 0.345 (0.455) data 0.213 (0.324) loss_u loss_u 0.8491 (0.8787) acc_u 21.8750 (15.7500) lr 1.9114e-03 eta 0:00:18
epoch [29/200] batch [30/66] time 0.411 (0.453) data 0.280 (0.322) loss_u loss_u 0.9194 (0.8810) acc_u 9.3750 (15.4167) lr 1.9114e-03 eta 0:00:16
epoch [29/200] batch [35/66] time 0.354 (0.450) data 0.222 (0.319) loss_u loss_u 0.8677 (0.8825) acc_u 21.8750 (15.1786) lr 1.9114e-03 eta 0:00:13
epoch [29/200] batch [40/66] time 0.312 (0.446) data 0.181 (0.314) loss_u loss_u 0.8076 (0.8819) acc_u 25.0000 (15.3906) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [45/66] time 0.433 (0.443) data 0.301 (0.312) loss_u loss_u 0.8135 (0.8814) acc_u 28.1250 (15.4167) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [50/66] time 0.367 (0.439) data 0.236 (0.308) loss_u loss_u 0.8335 (0.8821) acc_u 25.0000 (15.4375) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [55/66] time 0.446 (0.439) data 0.316 (0.308) loss_u loss_u 0.9009 (0.8828) acc_u 9.3750 (15.1705) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [60/66] time 0.470 (0.442) data 0.338 (0.311) loss_u loss_u 0.9072 (0.8854) acc_u 15.6250 (15.0000) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [65/66] time 0.432 (0.441) data 0.302 (0.309) loss_u loss_u 0.9395 (0.8858) acc_u 6.2500 (14.8077) lr 1.9114e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1661
confident_label rate tensor(0.3119, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 978
clean true:970
clean false:8
clean_rate:0.9918200408997955
noisy true:505
noisy false:1653
after delete: len(clean_dataset) 978
after delete: len(noisy_dataset) 2158
epoch [30/200] batch [5/30] time 0.487 (0.514) data 0.356 (0.383) loss_x loss_x 0.9985 (1.2713) acc_x 75.0000 (63.1250) lr 1.9048e-03 eta 0:00:12
epoch [30/200] batch [10/30] time 0.423 (0.499) data 0.292 (0.368) loss_x loss_x 1.7344 (1.3303) acc_x 56.2500 (63.1250) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [15/30] time 0.490 (0.470) data 0.359 (0.339) loss_x loss_x 1.2217 (1.2688) acc_x 65.6250 (65.2083) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [20/30] time 0.510 (0.467) data 0.379 (0.336) loss_x loss_x 1.1436 (1.3071) acc_x 75.0000 (65.6250) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [25/30] time 0.491 (0.466) data 0.360 (0.335) loss_x loss_x 1.2490 (1.3202) acc_x 68.7500 (65.3750) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [30/30] time 0.476 (0.460) data 0.345 (0.329) loss_x loss_x 1.1631 (1.3275) acc_x 65.6250 (64.7917) lr 1.9048e-03 eta 0:00:00
epoch [30/200] batch [5/67] time 0.363 (0.452) data 0.232 (0.321) loss_u loss_u 0.7798 (0.8510) acc_u 28.1250 (18.1250) lr 1.9048e-03 eta 0:00:28
epoch [30/200] batch [10/67] time 0.408 (0.442) data 0.276 (0.311) loss_u loss_u 0.9194 (0.8708) acc_u 6.2500 (15.3125) lr 1.9048e-03 eta 0:00:25
epoch [30/200] batch [15/67] time 0.384 (0.440) data 0.253 (0.309) loss_u loss_u 0.8872 (0.8720) acc_u 15.6250 (16.2500) lr 1.9048e-03 eta 0:00:22
epoch [30/200] batch [20/67] time 0.436 (0.439) data 0.305 (0.308) loss_u loss_u 0.9092 (0.8749) acc_u 12.5000 (16.0938) lr 1.9048e-03 eta 0:00:20
epoch [30/200] batch [25/67] time 0.372 (0.440) data 0.241 (0.309) loss_u loss_u 0.9272 (0.8780) acc_u 6.2500 (15.2500) lr 1.9048e-03 eta 0:00:18
epoch [30/200] batch [30/67] time 0.543 (0.444) data 0.411 (0.313) loss_u loss_u 0.9189 (0.8833) acc_u 9.3750 (14.3750) lr 1.9048e-03 eta 0:00:16
epoch [30/200] batch [35/67] time 0.392 (0.443) data 0.261 (0.311) loss_u loss_u 0.9341 (0.8866) acc_u 12.5000 (14.4643) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [40/67] time 0.377 (0.442) data 0.245 (0.311) loss_u loss_u 0.8516 (0.8853) acc_u 15.6250 (14.3750) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [45/67] time 0.399 (0.438) data 0.268 (0.307) loss_u loss_u 0.8608 (0.8824) acc_u 18.7500 (14.7222) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [50/67] time 0.462 (0.442) data 0.329 (0.311) loss_u loss_u 0.8037 (0.8807) acc_u 28.1250 (15.1875) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [55/67] time 0.357 (0.448) data 0.225 (0.317) loss_u loss_u 0.8857 (0.8818) acc_u 15.6250 (15.1705) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [60/67] time 0.378 (0.447) data 0.247 (0.315) loss_u loss_u 0.8345 (0.8815) acc_u 18.7500 (15.0000) lr 1.9048e-03 eta 0:00:03
epoch [30/200] batch [65/67] time 0.398 (0.445) data 0.267 (0.313) loss_u loss_u 0.9199 (0.8822) acc_u 12.5000 (15.0481) lr 1.9048e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1658
confident_label rate tensor(0.3189, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1000
clean true:993
clean false:7
clean_rate:0.993
noisy true:485
noisy false:1651
after delete: len(clean_dataset) 1000
after delete: len(noisy_dataset) 2136
epoch [31/200] batch [5/31] time 0.427 (0.425) data 0.297 (0.294) loss_x loss_x 1.2900 (1.3311) acc_x 68.7500 (67.5000) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [10/31] time 0.462 (0.430) data 0.332 (0.300) loss_x loss_x 1.0547 (1.2627) acc_x 75.0000 (69.0625) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [15/31] time 0.430 (0.450) data 0.299 (0.320) loss_x loss_x 1.3496 (1.3250) acc_x 53.1250 (66.8750) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [20/31] time 0.526 (0.450) data 0.396 (0.320) loss_x loss_x 1.6436 (1.3490) acc_x 56.2500 (65.1562) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [25/31] time 0.417 (0.450) data 0.286 (0.319) loss_x loss_x 1.6953 (1.3781) acc_x 59.3750 (64.8750) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [30/31] time 0.374 (0.456) data 0.243 (0.325) loss_x loss_x 1.3955 (1.3848) acc_x 71.8750 (65.0000) lr 1.8980e-03 eta 0:00:00
epoch [31/200] batch [5/66] time 0.507 (0.449) data 0.376 (0.318) loss_u loss_u 0.8667 (0.8944) acc_u 15.6250 (11.8750) lr 1.8980e-03 eta 0:00:27
epoch [31/200] batch [10/66] time 0.405 (0.446) data 0.274 (0.315) loss_u loss_u 0.9131 (0.8886) acc_u 12.5000 (15.0000) lr 1.8980e-03 eta 0:00:24
epoch [31/200] batch [15/66] time 0.507 (0.449) data 0.374 (0.318) loss_u loss_u 0.8433 (0.8917) acc_u 18.7500 (15.2083) lr 1.8980e-03 eta 0:00:22
epoch [31/200] batch [20/66] time 0.459 (0.453) data 0.328 (0.321) loss_u loss_u 0.8267 (0.8748) acc_u 21.8750 (17.1875) lr 1.8980e-03 eta 0:00:20
epoch [31/200] batch [25/66] time 0.437 (0.457) data 0.306 (0.325) loss_u loss_u 0.9561 (0.8831) acc_u 3.1250 (15.8750) lr 1.8980e-03 eta 0:00:18
epoch [31/200] batch [30/66] time 0.366 (0.455) data 0.234 (0.323) loss_u loss_u 0.9307 (0.8851) acc_u 6.2500 (15.4167) lr 1.8980e-03 eta 0:00:16
epoch [31/200] batch [35/66] time 0.407 (0.450) data 0.276 (0.319) loss_u loss_u 0.8970 (0.8848) acc_u 12.5000 (15.2679) lr 1.8980e-03 eta 0:00:13
epoch [31/200] batch [40/66] time 0.480 (0.446) data 0.350 (0.315) loss_u loss_u 0.8730 (0.8814) acc_u 15.6250 (15.7031) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [45/66] time 0.514 (0.447) data 0.384 (0.316) loss_u loss_u 0.8682 (0.8824) acc_u 15.6250 (15.5556) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [50/66] time 0.587 (0.446) data 0.457 (0.315) loss_u loss_u 0.8071 (0.8793) acc_u 25.0000 (16.0625) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [55/66] time 0.346 (0.443) data 0.214 (0.312) loss_u loss_u 0.8979 (0.8798) acc_u 9.3750 (15.9091) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [60/66] time 0.398 (0.443) data 0.267 (0.312) loss_u loss_u 0.8271 (0.8791) acc_u 21.8750 (15.9375) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [65/66] time 0.421 (0.443) data 0.290 (0.312) loss_u loss_u 0.8892 (0.8804) acc_u 9.3750 (15.6250) lr 1.8980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1671
confident_label rate tensor(0.3061, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 960
clean true:956
clean false:4
clean_rate:0.9958333333333333
noisy true:509
noisy false:1667
after delete: len(clean_dataset) 960
after delete: len(noisy_dataset) 2176
epoch [32/200] batch [5/30] time 0.491 (0.493) data 0.361 (0.362) loss_x loss_x 1.2930 (1.4279) acc_x 56.2500 (61.8750) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [10/30] time 0.419 (0.475) data 0.288 (0.344) loss_x loss_x 1.6416 (1.5241) acc_x 71.8750 (63.7500) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [15/30] time 0.338 (0.461) data 0.206 (0.330) loss_x loss_x 1.1572 (1.4174) acc_x 68.7500 (63.9583) lr 1.8910e-03 eta 0:00:06
epoch [32/200] batch [20/30] time 0.456 (0.453) data 0.325 (0.322) loss_x loss_x 1.5010 (1.4082) acc_x 65.6250 (64.2188) lr 1.8910e-03 eta 0:00:04
epoch [32/200] batch [25/30] time 0.384 (0.453) data 0.253 (0.323) loss_x loss_x 1.4043 (1.3773) acc_x 56.2500 (64.2500) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [30/30] time 0.549 (0.460) data 0.419 (0.329) loss_x loss_x 1.5693 (1.3692) acc_x 68.7500 (64.3750) lr 1.8910e-03 eta 0:00:00
epoch [32/200] batch [5/68] time 0.426 (0.451) data 0.294 (0.320) loss_u loss_u 0.8516 (0.8505) acc_u 15.6250 (20.6250) lr 1.8910e-03 eta 0:00:28
epoch [32/200] batch [10/68] time 0.409 (0.448) data 0.279 (0.317) loss_u loss_u 0.8423 (0.8570) acc_u 25.0000 (19.0625) lr 1.8910e-03 eta 0:00:25
epoch [32/200] batch [15/68] time 0.408 (0.448) data 0.277 (0.317) loss_u loss_u 0.9473 (0.8778) acc_u 3.1250 (15.8333) lr 1.8910e-03 eta 0:00:23
epoch [32/200] batch [20/68] time 0.447 (0.446) data 0.315 (0.315) loss_u loss_u 0.8511 (0.8772) acc_u 15.6250 (15.4688) lr 1.8910e-03 eta 0:00:21
epoch [32/200] batch [25/68] time 0.365 (0.442) data 0.233 (0.311) loss_u loss_u 0.8848 (0.8848) acc_u 12.5000 (14.5000) lr 1.8910e-03 eta 0:00:19
epoch [32/200] batch [30/68] time 0.350 (0.447) data 0.219 (0.316) loss_u loss_u 0.8999 (0.8775) acc_u 9.3750 (15.4167) lr 1.8910e-03 eta 0:00:16
epoch [32/200] batch [35/68] time 0.301 (0.447) data 0.171 (0.316) loss_u loss_u 0.9316 (0.8789) acc_u 9.3750 (15.5357) lr 1.8910e-03 eta 0:00:14
epoch [32/200] batch [40/68] time 0.392 (0.451) data 0.259 (0.320) loss_u loss_u 0.8516 (0.8802) acc_u 21.8750 (15.3906) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [45/68] time 0.465 (0.450) data 0.334 (0.319) loss_u loss_u 0.9395 (0.8819) acc_u 6.2500 (15.2083) lr 1.8910e-03 eta 0:00:10
epoch [32/200] batch [50/68] time 0.394 (0.447) data 0.262 (0.316) loss_u loss_u 0.8740 (0.8824) acc_u 12.5000 (15.1875) lr 1.8910e-03 eta 0:00:08
epoch [32/200] batch [55/68] time 0.434 (0.443) data 0.303 (0.312) loss_u loss_u 0.8677 (0.8816) acc_u 21.8750 (15.4545) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [60/68] time 0.367 (0.440) data 0.235 (0.309) loss_u loss_u 0.8984 (0.8828) acc_u 12.5000 (15.3125) lr 1.8910e-03 eta 0:00:03
epoch [32/200] batch [65/68] time 0.550 (0.441) data 0.419 (0.310) loss_u loss_u 0.9097 (0.8853) acc_u 9.3750 (14.9038) lr 1.8910e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1684
confident_label rate tensor(0.3128, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 981
clean true:970
clean false:11
clean_rate:0.9887869520897044
noisy true:482
noisy false:1673
after delete: len(clean_dataset) 981
after delete: len(noisy_dataset) 2155
epoch [33/200] batch [5/30] time 0.426 (0.435) data 0.296 (0.304) loss_x loss_x 1.3633 (1.3246) acc_x 68.7500 (68.7500) lr 1.8838e-03 eta 0:00:10
epoch [33/200] batch [10/30] time 0.508 (0.459) data 0.378 (0.328) loss_x loss_x 1.2627 (1.4236) acc_x 68.7500 (66.8750) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [15/30] time 0.369 (0.439) data 0.239 (0.309) loss_x loss_x 1.3350 (1.3349) acc_x 65.6250 (67.0833) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [20/30] time 0.389 (0.434) data 0.259 (0.304) loss_x loss_x 1.2949 (1.3330) acc_x 65.6250 (66.8750) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [25/30] time 0.497 (0.440) data 0.366 (0.310) loss_x loss_x 1.1865 (1.3154) acc_x 71.8750 (67.5000) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [30/30] time 0.527 (0.447) data 0.396 (0.316) loss_x loss_x 2.0410 (1.3790) acc_x 62.5000 (66.4583) lr 1.8838e-03 eta 0:00:00
epoch [33/200] batch [5/67] time 0.452 (0.451) data 0.322 (0.320) loss_u loss_u 0.8208 (0.8554) acc_u 28.1250 (18.1250) lr 1.8838e-03 eta 0:00:27
epoch [33/200] batch [10/67] time 0.381 (0.446) data 0.250 (0.315) loss_u loss_u 0.8403 (0.8624) acc_u 28.1250 (18.4375) lr 1.8838e-03 eta 0:00:25
epoch [33/200] batch [15/67] time 0.548 (0.452) data 0.418 (0.321) loss_u loss_u 0.8555 (0.8653) acc_u 15.6250 (17.2917) lr 1.8838e-03 eta 0:00:23
epoch [33/200] batch [20/67] time 0.472 (0.450) data 0.340 (0.319) loss_u loss_u 0.8853 (0.8651) acc_u 15.6250 (17.6562) lr 1.8838e-03 eta 0:00:21
epoch [33/200] batch [25/67] time 0.388 (0.447) data 0.256 (0.316) loss_u loss_u 0.8706 (0.8666) acc_u 15.6250 (17.1250) lr 1.8838e-03 eta 0:00:18
epoch [33/200] batch [30/67] time 0.414 (0.445) data 0.282 (0.314) loss_u loss_u 0.8979 (0.8702) acc_u 6.2500 (16.7708) lr 1.8838e-03 eta 0:00:16
epoch [33/200] batch [35/67] time 0.373 (0.443) data 0.242 (0.312) loss_u loss_u 0.8735 (0.8658) acc_u 15.6250 (17.3214) lr 1.8838e-03 eta 0:00:14
epoch [33/200] batch [40/67] time 0.361 (0.439) data 0.229 (0.308) loss_u loss_u 0.9141 (0.8660) acc_u 9.3750 (17.1094) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [45/67] time 0.500 (0.442) data 0.369 (0.311) loss_u loss_u 0.9346 (0.8695) acc_u 9.3750 (16.6667) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [50/67] time 0.434 (0.442) data 0.302 (0.311) loss_u loss_u 0.8540 (0.8712) acc_u 21.8750 (16.5000) lr 1.8838e-03 eta 0:00:07
epoch [33/200] batch [55/67] time 0.352 (0.441) data 0.221 (0.310) loss_u loss_u 0.8442 (0.8713) acc_u 25.0000 (16.5909) lr 1.8838e-03 eta 0:00:05
epoch [33/200] batch [60/67] time 0.425 (0.438) data 0.295 (0.307) loss_u loss_u 0.9326 (0.8745) acc_u 6.2500 (16.1979) lr 1.8838e-03 eta 0:00:03
epoch [33/200] batch [65/67] time 0.467 (0.438) data 0.336 (0.307) loss_u loss_u 0.9375 (0.8766) acc_u 12.5000 (15.9615) lr 1.8838e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1630
confident_label rate tensor(0.3154, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 989
clean true:987
clean false:2
clean_rate:0.9979777553083923
noisy true:519
noisy false:1628
after delete: len(clean_dataset) 989
after delete: len(noisy_dataset) 2147
epoch [34/200] batch [5/30] time 0.424 (0.466) data 0.294 (0.335) loss_x loss_x 1.4551 (1.2486) acc_x 68.7500 (66.8750) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [10/30] time 0.373 (0.432) data 0.243 (0.302) loss_x loss_x 1.3301 (1.3468) acc_x 62.5000 (66.2500) lr 1.8763e-03 eta 0:00:08
epoch [34/200] batch [15/30] time 0.459 (0.434) data 0.329 (0.304) loss_x loss_x 0.9609 (1.4139) acc_x 68.7500 (65.6250) lr 1.8763e-03 eta 0:00:06
epoch [34/200] batch [20/30] time 0.619 (0.438) data 0.489 (0.307) loss_x loss_x 1.4023 (1.4264) acc_x 65.6250 (64.8438) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [25/30] time 0.429 (0.442) data 0.299 (0.312) loss_x loss_x 2.0273 (1.4447) acc_x 53.1250 (64.7500) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [30/30] time 0.381 (0.443) data 0.250 (0.312) loss_x loss_x 1.4395 (1.4195) acc_x 59.3750 (64.7917) lr 1.8763e-03 eta 0:00:00
epoch [34/200] batch [5/67] time 0.373 (0.445) data 0.242 (0.314) loss_u loss_u 0.9419 (0.8976) acc_u 9.3750 (14.3750) lr 1.8763e-03 eta 0:00:27
epoch [34/200] batch [10/67] time 0.490 (0.449) data 0.358 (0.318) loss_u loss_u 0.9131 (0.8762) acc_u 12.5000 (16.8750) lr 1.8763e-03 eta 0:00:25
epoch [34/200] batch [15/67] time 0.455 (0.446) data 0.324 (0.315) loss_u loss_u 0.9404 (0.8765) acc_u 6.2500 (16.0417) lr 1.8763e-03 eta 0:00:23
epoch [34/200] batch [20/67] time 0.443 (0.448) data 0.313 (0.317) loss_u loss_u 0.8398 (0.8815) acc_u 21.8750 (15.3125) lr 1.8763e-03 eta 0:00:21
epoch [34/200] batch [25/67] time 0.402 (0.443) data 0.272 (0.312) loss_u loss_u 0.9463 (0.8834) acc_u 6.2500 (15.1250) lr 1.8763e-03 eta 0:00:18
epoch [34/200] batch [30/67] time 0.429 (0.440) data 0.298 (0.309) loss_u loss_u 0.8306 (0.8844) acc_u 21.8750 (15.0000) lr 1.8763e-03 eta 0:00:16
epoch [34/200] batch [35/67] time 0.395 (0.440) data 0.264 (0.309) loss_u loss_u 0.8765 (0.8833) acc_u 15.6250 (15.1786) lr 1.8763e-03 eta 0:00:14
epoch [34/200] batch [40/67] time 0.459 (0.439) data 0.328 (0.309) loss_u loss_u 0.8765 (0.8836) acc_u 21.8750 (15.0781) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [45/67] time 0.372 (0.437) data 0.241 (0.306) loss_u loss_u 0.8809 (0.8821) acc_u 18.7500 (15.4861) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [50/67] time 0.368 (0.434) data 0.237 (0.303) loss_u loss_u 0.8657 (0.8812) acc_u 12.5000 (15.5625) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [55/67] time 0.529 (0.434) data 0.397 (0.304) loss_u loss_u 0.8496 (0.8803) acc_u 28.1250 (15.9659) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [60/67] time 0.455 (0.435) data 0.324 (0.304) loss_u loss_u 0.8774 (0.8812) acc_u 21.8750 (15.9896) lr 1.8763e-03 eta 0:00:03
epoch [34/200] batch [65/67] time 0.433 (0.437) data 0.301 (0.306) loss_u loss_u 0.9023 (0.8794) acc_u 15.6250 (16.2019) lr 1.8763e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1615
confident_label rate tensor(0.3176, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 996
clean true:992
clean false:4
clean_rate:0.9959839357429718
noisy true:529
noisy false:1611
after delete: len(clean_dataset) 996
after delete: len(noisy_dataset) 2140
epoch [35/200] batch [5/31] time 0.457 (0.425) data 0.326 (0.294) loss_x loss_x 1.5186 (1.3338) acc_x 68.7500 (70.0000) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [10/31] time 0.526 (0.453) data 0.395 (0.322) loss_x loss_x 1.8076 (1.3450) acc_x 56.2500 (66.2500) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [15/31] time 0.487 (0.457) data 0.356 (0.327) loss_x loss_x 1.8008 (1.4525) acc_x 59.3750 (63.3333) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [20/31] time 0.432 (0.451) data 0.301 (0.320) loss_x loss_x 1.0557 (1.3529) acc_x 68.7500 (65.0000) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [25/31] time 0.570 (0.462) data 0.439 (0.331) loss_x loss_x 1.1611 (1.3133) acc_x 56.2500 (65.5000) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [30/31] time 0.468 (0.460) data 0.338 (0.329) loss_x loss_x 1.0488 (1.3209) acc_x 68.7500 (66.1458) lr 1.8686e-03 eta 0:00:00
epoch [35/200] batch [5/66] time 0.716 (0.460) data 0.585 (0.329) loss_u loss_u 0.9004 (0.8998) acc_u 15.6250 (13.7500) lr 1.8686e-03 eta 0:00:28
epoch [35/200] batch [10/66] time 0.655 (0.475) data 0.525 (0.344) loss_u loss_u 0.9341 (0.9086) acc_u 9.3750 (12.5000) lr 1.8686e-03 eta 0:00:26
epoch [35/200] batch [15/66] time 0.447 (0.468) data 0.315 (0.337) loss_u loss_u 0.8188 (0.8883) acc_u 28.1250 (15.6250) lr 1.8686e-03 eta 0:00:23
epoch [35/200] batch [20/66] time 0.357 (0.463) data 0.226 (0.332) loss_u loss_u 0.8433 (0.8845) acc_u 18.7500 (15.4688) lr 1.8686e-03 eta 0:00:21
epoch [35/200] batch [25/66] time 0.377 (0.458) data 0.245 (0.327) loss_u loss_u 0.9360 (0.8802) acc_u 3.1250 (15.6250) lr 1.8686e-03 eta 0:00:18
epoch [35/200] batch [30/66] time 0.427 (0.454) data 0.296 (0.323) loss_u loss_u 0.8809 (0.8797) acc_u 18.7500 (15.8333) lr 1.8686e-03 eta 0:00:16
epoch [35/200] batch [35/66] time 0.449 (0.450) data 0.317 (0.319) loss_u loss_u 0.8740 (0.8801) acc_u 15.6250 (15.7143) lr 1.8686e-03 eta 0:00:13
epoch [35/200] batch [40/66] time 0.388 (0.447) data 0.256 (0.316) loss_u loss_u 0.8984 (0.8823) acc_u 12.5000 (15.4688) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [45/66] time 0.409 (0.445) data 0.278 (0.314) loss_u loss_u 0.9209 (0.8807) acc_u 9.3750 (15.9028) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [50/66] time 0.360 (0.444) data 0.229 (0.312) loss_u loss_u 0.8389 (0.8803) acc_u 18.7500 (15.8750) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [55/66] time 0.447 (0.444) data 0.315 (0.313) loss_u loss_u 0.9194 (0.8832) acc_u 18.7500 (15.7955) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [60/66] time 0.371 (0.445) data 0.239 (0.314) loss_u loss_u 0.9536 (0.8833) acc_u 6.2500 (15.8333) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [65/66] time 0.340 (0.442) data 0.208 (0.310) loss_u loss_u 0.8447 (0.8815) acc_u 18.7500 (16.0096) lr 1.8686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1596
confident_label rate tensor(0.3224, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1011
clean true:1003
clean false:8
clean_rate:0.9920870425321464
noisy true:537
noisy false:1588
after delete: len(clean_dataset) 1011
after delete: len(noisy_dataset) 2125
epoch [36/200] batch [5/31] time 0.546 (0.533) data 0.412 (0.401) loss_x loss_x 0.9351 (1.3540) acc_x 75.0000 (65.6250) lr 1.8607e-03 eta 0:00:13
epoch [36/200] batch [10/31] time 0.390 (0.500) data 0.258 (0.368) loss_x loss_x 1.1758 (1.3228) acc_x 68.7500 (64.6875) lr 1.8607e-03 eta 0:00:10
epoch [36/200] batch [15/31] time 0.485 (0.481) data 0.354 (0.349) loss_x loss_x 1.2793 (1.3004) acc_x 68.7500 (67.0833) lr 1.8607e-03 eta 0:00:07
epoch [36/200] batch [20/31] time 0.387 (0.477) data 0.256 (0.345) loss_x loss_x 2.1270 (1.3766) acc_x 53.1250 (66.7188) lr 1.8607e-03 eta 0:00:05
epoch [36/200] batch [25/31] time 0.422 (0.467) data 0.291 (0.335) loss_x loss_x 1.2949 (1.3969) acc_x 62.5000 (66.1250) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [30/31] time 0.358 (0.460) data 0.227 (0.329) loss_x loss_x 1.2979 (1.3818) acc_x 75.0000 (66.3542) lr 1.8607e-03 eta 0:00:00
epoch [36/200] batch [5/66] time 0.381 (0.457) data 0.249 (0.326) loss_u loss_u 0.8843 (0.8827) acc_u 18.7500 (16.2500) lr 1.8607e-03 eta 0:00:27
epoch [36/200] batch [10/66] time 0.516 (0.452) data 0.384 (0.321) loss_u loss_u 0.8413 (0.8810) acc_u 18.7500 (16.2500) lr 1.8607e-03 eta 0:00:25
epoch [36/200] batch [15/66] time 0.402 (0.449) data 0.270 (0.317) loss_u loss_u 0.9419 (0.8855) acc_u 6.2500 (15.6250) lr 1.8607e-03 eta 0:00:22
epoch [36/200] batch [20/66] time 0.478 (0.453) data 0.347 (0.321) loss_u loss_u 0.9238 (0.8801) acc_u 9.3750 (15.6250) lr 1.8607e-03 eta 0:00:20
epoch [36/200] batch [25/66] time 0.326 (0.450) data 0.194 (0.319) loss_u loss_u 0.9116 (0.8805) acc_u 9.3750 (15.7500) lr 1.8607e-03 eta 0:00:18
epoch [36/200] batch [30/66] time 0.435 (0.449) data 0.303 (0.318) loss_u loss_u 0.8799 (0.8783) acc_u 15.6250 (15.8333) lr 1.8607e-03 eta 0:00:16
epoch [36/200] batch [35/66] time 0.403 (0.451) data 0.271 (0.320) loss_u loss_u 0.8989 (0.8776) acc_u 15.6250 (16.0714) lr 1.8607e-03 eta 0:00:13
epoch [36/200] batch [40/66] time 0.533 (0.451) data 0.402 (0.319) loss_u loss_u 0.8979 (0.8787) acc_u 15.6250 (15.7812) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [45/66] time 0.387 (0.448) data 0.255 (0.317) loss_u loss_u 0.8989 (0.8792) acc_u 9.3750 (15.6250) lr 1.8607e-03 eta 0:00:09
epoch [36/200] batch [50/66] time 0.310 (0.445) data 0.178 (0.314) loss_u loss_u 0.8955 (0.8817) acc_u 9.3750 (15.1875) lr 1.8607e-03 eta 0:00:07
epoch [36/200] batch [55/66] time 0.520 (0.447) data 0.390 (0.315) loss_u loss_u 0.9199 (0.8794) acc_u 6.2500 (15.4545) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [60/66] time 0.460 (0.449) data 0.329 (0.318) loss_u loss_u 0.9038 (0.8792) acc_u 12.5000 (15.5208) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [65/66] time 0.392 (0.447) data 0.260 (0.316) loss_u loss_u 0.8838 (0.8775) acc_u 18.7500 (15.8654) lr 1.8607e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1600
confident_label rate tensor(0.3249, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1019
clean true:1015
clean false:4
clean_rate:0.9960745829244357
noisy true:521
noisy false:1596
after delete: len(clean_dataset) 1019
after delete: len(noisy_dataset) 2117
epoch [37/200] batch [5/31] time 0.459 (0.481) data 0.328 (0.351) loss_x loss_x 1.1260 (1.3135) acc_x 71.8750 (64.3750) lr 1.8526e-03 eta 0:00:12
epoch [37/200] batch [10/31] time 0.404 (0.469) data 0.273 (0.339) loss_x loss_x 1.4111 (1.1935) acc_x 59.3750 (67.5000) lr 1.8526e-03 eta 0:00:09
epoch [37/200] batch [15/31] time 0.486 (0.444) data 0.355 (0.313) loss_x loss_x 1.4346 (1.2470) acc_x 65.6250 (68.5417) lr 1.8526e-03 eta 0:00:07
epoch [37/200] batch [20/31] time 0.384 (0.440) data 0.254 (0.310) loss_x loss_x 1.2344 (1.2541) acc_x 59.3750 (68.2812) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [25/31] time 0.494 (0.450) data 0.364 (0.319) loss_x loss_x 1.4922 (1.2685) acc_x 68.7500 (68.6250) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [30/31] time 0.498 (0.450) data 0.367 (0.319) loss_x loss_x 1.6123 (1.3123) acc_x 56.2500 (68.0208) lr 1.8526e-03 eta 0:00:00
epoch [37/200] batch [5/66] time 0.533 (0.446) data 0.402 (0.315) loss_u loss_u 0.8789 (0.8607) acc_u 18.7500 (18.1250) lr 1.8526e-03 eta 0:00:27
epoch [37/200] batch [10/66] time 0.404 (0.440) data 0.272 (0.309) loss_u loss_u 0.8828 (0.8779) acc_u 15.6250 (15.6250) lr 1.8526e-03 eta 0:00:24
epoch [37/200] batch [15/66] time 0.685 (0.445) data 0.554 (0.314) loss_u loss_u 0.9321 (0.8855) acc_u 15.6250 (14.5833) lr 1.8526e-03 eta 0:00:22
epoch [37/200] batch [20/66] time 0.440 (0.445) data 0.309 (0.314) loss_u loss_u 0.8682 (0.8839) acc_u 15.6250 (15.0000) lr 1.8526e-03 eta 0:00:20
epoch [37/200] batch [25/66] time 0.374 (0.445) data 0.243 (0.314) loss_u loss_u 0.9165 (0.8857) acc_u 9.3750 (14.6250) lr 1.8526e-03 eta 0:00:18
epoch [37/200] batch [30/66] time 0.328 (0.443) data 0.196 (0.312) loss_u loss_u 0.9141 (0.8850) acc_u 12.5000 (15.1042) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [35/66] time 0.499 (0.440) data 0.368 (0.309) loss_u loss_u 0.8818 (0.8895) acc_u 18.7500 (14.7321) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [40/66] time 0.349 (0.440) data 0.218 (0.309) loss_u loss_u 0.9087 (0.8879) acc_u 6.2500 (14.7656) lr 1.8526e-03 eta 0:00:11
epoch [37/200] batch [45/66] time 0.339 (0.440) data 0.207 (0.309) loss_u loss_u 0.8115 (0.8871) acc_u 18.7500 (14.7917) lr 1.8526e-03 eta 0:00:09
epoch [37/200] batch [50/66] time 0.403 (0.438) data 0.271 (0.307) loss_u loss_u 0.8525 (0.8859) acc_u 21.8750 (15.1875) lr 1.8526e-03 eta 0:00:07
epoch [37/200] batch [55/66] time 0.543 (0.438) data 0.411 (0.307) loss_u loss_u 0.8071 (0.8834) acc_u 28.1250 (15.7386) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [60/66] time 0.441 (0.437) data 0.309 (0.306) loss_u loss_u 0.9131 (0.8838) acc_u 9.3750 (15.5729) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [65/66] time 0.433 (0.437) data 0.301 (0.306) loss_u loss_u 0.8716 (0.8829) acc_u 15.6250 (15.6250) lr 1.8526e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1587
confident_label rate tensor(0.3323, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1042
clean true:1036
clean false:6
clean_rate:0.9942418426103646
noisy true:513
noisy false:1581
after delete: len(clean_dataset) 1042
after delete: len(noisy_dataset) 2094
epoch [38/200] batch [5/32] time 0.492 (0.465) data 0.361 (0.334) loss_x loss_x 1.0391 (1.3342) acc_x 68.7500 (65.0000) lr 1.8443e-03 eta 0:00:12
epoch [38/200] batch [10/32] time 0.497 (0.476) data 0.367 (0.345) loss_x loss_x 0.9780 (1.3087) acc_x 75.0000 (64.3750) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [15/32] time 0.380 (0.461) data 0.249 (0.330) loss_x loss_x 1.0371 (1.3496) acc_x 59.3750 (63.3333) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [20/32] time 0.407 (0.459) data 0.276 (0.328) loss_x loss_x 1.2520 (1.3573) acc_x 68.7500 (63.4375) lr 1.8443e-03 eta 0:00:05
epoch [38/200] batch [25/32] time 0.359 (0.460) data 0.225 (0.329) loss_x loss_x 1.3525 (1.3658) acc_x 65.6250 (64.0000) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [30/32] time 0.433 (0.464) data 0.302 (0.333) loss_x loss_x 1.4736 (1.3872) acc_x 65.6250 (63.4375) lr 1.8443e-03 eta 0:00:00
epoch [38/200] batch [5/65] time 0.419 (0.457) data 0.287 (0.326) loss_u loss_u 0.9395 (0.8863) acc_u 3.1250 (15.0000) lr 1.8443e-03 eta 0:00:27
epoch [38/200] batch [10/65] time 0.440 (0.454) data 0.307 (0.323) loss_u loss_u 0.8857 (0.8873) acc_u 12.5000 (13.4375) lr 1.8443e-03 eta 0:00:24
epoch [38/200] batch [15/65] time 0.422 (0.450) data 0.290 (0.318) loss_u loss_u 0.8691 (0.8864) acc_u 12.5000 (13.3333) lr 1.8443e-03 eta 0:00:22
epoch [38/200] batch [20/65] time 0.692 (0.452) data 0.560 (0.321) loss_u loss_u 0.9521 (0.8871) acc_u 6.2500 (13.9062) lr 1.8443e-03 eta 0:00:20
epoch [38/200] batch [25/65] time 0.396 (0.446) data 0.264 (0.315) loss_u loss_u 0.9214 (0.8888) acc_u 9.3750 (13.0000) lr 1.8443e-03 eta 0:00:17
epoch [38/200] batch [30/65] time 0.421 (0.442) data 0.289 (0.311) loss_u loss_u 0.9043 (0.8895) acc_u 12.5000 (13.3333) lr 1.8443e-03 eta 0:00:15
epoch [38/200] batch [35/65] time 0.335 (0.441) data 0.204 (0.310) loss_u loss_u 0.8618 (0.8887) acc_u 15.6250 (13.4821) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [40/65] time 0.354 (0.439) data 0.222 (0.307) loss_u loss_u 0.9297 (0.8918) acc_u 6.2500 (12.9688) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [45/65] time 0.369 (0.439) data 0.237 (0.308) loss_u loss_u 0.8208 (0.8926) acc_u 25.0000 (13.1944) lr 1.8443e-03 eta 0:00:08
epoch [38/200] batch [50/65] time 0.409 (0.442) data 0.278 (0.310) loss_u loss_u 0.8281 (0.8930) acc_u 15.6250 (12.7500) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [55/65] time 0.432 (0.443) data 0.300 (0.311) loss_u loss_u 0.8760 (0.8915) acc_u 15.6250 (13.0114) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [60/65] time 0.488 (0.443) data 0.356 (0.312) loss_u loss_u 0.8848 (0.8904) acc_u 15.6250 (13.2292) lr 1.8443e-03 eta 0:00:02
epoch [38/200] batch [65/65] time 0.471 (0.443) data 0.339 (0.312) loss_u loss_u 0.9219 (0.8900) acc_u 6.2500 (13.1731) lr 1.8443e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1609
confident_label rate tensor(0.3208, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1006
clean true:1005
clean false:1
clean_rate:0.9990059642147118
noisy true:522
noisy false:1608
after delete: len(clean_dataset) 1006
after delete: len(noisy_dataset) 2130
epoch [39/200] batch [5/31] time 0.504 (0.466) data 0.373 (0.335) loss_x loss_x 1.6484 (1.3809) acc_x 56.2500 (66.2500) lr 1.8358e-03 eta 0:00:12
epoch [39/200] batch [10/31] time 0.659 (0.491) data 0.528 (0.360) loss_x loss_x 1.0684 (1.2010) acc_x 68.7500 (70.3125) lr 1.8358e-03 eta 0:00:10
epoch [39/200] batch [15/31] time 0.545 (0.480) data 0.414 (0.350) loss_x loss_x 1.3613 (1.2199) acc_x 56.2500 (68.7500) lr 1.8358e-03 eta 0:00:07
epoch [39/200] batch [20/31] time 0.427 (0.467) data 0.296 (0.336) loss_x loss_x 1.5068 (1.2463) acc_x 59.3750 (68.4375) lr 1.8358e-03 eta 0:00:05
epoch [39/200] batch [25/31] time 0.475 (0.462) data 0.345 (0.331) loss_x loss_x 1.3457 (1.3013) acc_x 65.6250 (67.1250) lr 1.8358e-03 eta 0:00:02
epoch [39/200] batch [30/31] time 0.420 (0.459) data 0.289 (0.328) loss_x loss_x 1.9004 (1.3267) acc_x 50.0000 (66.8750) lr 1.8358e-03 eta 0:00:00
epoch [39/200] batch [5/66] time 0.452 (0.453) data 0.320 (0.322) loss_u loss_u 0.8896 (0.9049) acc_u 12.5000 (10.0000) lr 1.8358e-03 eta 0:00:27
epoch [39/200] batch [10/66] time 0.382 (0.444) data 0.250 (0.313) loss_u loss_u 0.8472 (0.9029) acc_u 18.7500 (11.2500) lr 1.8358e-03 eta 0:00:24
epoch [39/200] batch [15/66] time 0.575 (0.449) data 0.444 (0.317) loss_u loss_u 0.8550 (0.8998) acc_u 21.8750 (12.0833) lr 1.8358e-03 eta 0:00:22
epoch [39/200] batch [20/66] time 0.708 (0.453) data 0.576 (0.322) loss_u loss_u 0.8247 (0.8976) acc_u 18.7500 (11.8750) lr 1.8358e-03 eta 0:00:20
epoch [39/200] batch [25/66] time 0.306 (0.449) data 0.175 (0.317) loss_u loss_u 0.8936 (0.8991) acc_u 15.6250 (11.7500) lr 1.8358e-03 eta 0:00:18
epoch [39/200] batch [30/66] time 0.356 (0.448) data 0.224 (0.317) loss_u loss_u 0.8237 (0.8936) acc_u 15.6250 (12.5000) lr 1.8358e-03 eta 0:00:16
epoch [39/200] batch [35/66] time 0.419 (0.446) data 0.287 (0.315) loss_u loss_u 0.9341 (0.8921) acc_u 3.1250 (12.5000) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [40/66] time 0.430 (0.446) data 0.298 (0.314) loss_u loss_u 0.9131 (0.8891) acc_u 6.2500 (12.7344) lr 1.8358e-03 eta 0:00:11
epoch [39/200] batch [45/66] time 0.533 (0.449) data 0.402 (0.318) loss_u loss_u 0.9106 (0.8891) acc_u 12.5000 (12.9861) lr 1.8358e-03 eta 0:00:09
epoch [39/200] batch [50/66] time 0.368 (0.448) data 0.236 (0.316) loss_u loss_u 0.9463 (0.8907) acc_u 6.2500 (13.0000) lr 1.8358e-03 eta 0:00:07
epoch [39/200] batch [55/66] time 0.425 (0.444) data 0.294 (0.312) loss_u loss_u 0.8408 (0.8881) acc_u 21.8750 (13.5795) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [60/66] time 0.412 (0.445) data 0.280 (0.313) loss_u loss_u 0.8701 (0.8856) acc_u 12.5000 (13.8021) lr 1.8358e-03 eta 0:00:02
epoch [39/200] batch [65/66] time 0.434 (0.443) data 0.303 (0.312) loss_u loss_u 0.7729 (0.8815) acc_u 28.1250 (14.3750) lr 1.8358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1619
confident_label rate tensor(0.3288, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1031
clean true:1022
clean false:9
clean_rate:0.991270611057226
noisy true:495
noisy false:1610
after delete: len(clean_dataset) 1031
after delete: len(noisy_dataset) 2105
epoch [40/200] batch [5/32] time 0.424 (0.450) data 0.293 (0.320) loss_x loss_x 1.3887 (1.3028) acc_x 65.6250 (68.7500) lr 1.8271e-03 eta 0:00:12
epoch [40/200] batch [10/32] time 0.518 (0.453) data 0.388 (0.323) loss_x loss_x 1.6367 (1.3736) acc_x 56.2500 (63.7500) lr 1.8271e-03 eta 0:00:09
epoch [40/200] batch [15/32] time 0.559 (0.466) data 0.429 (0.336) loss_x loss_x 1.7139 (1.3683) acc_x 59.3750 (65.0000) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [20/32] time 0.464 (0.463) data 0.334 (0.333) loss_x loss_x 1.0459 (1.3399) acc_x 71.8750 (65.6250) lr 1.8271e-03 eta 0:00:05
epoch [40/200] batch [25/32] time 0.540 (0.470) data 0.409 (0.340) loss_x loss_x 1.1406 (1.3236) acc_x 68.7500 (65.7500) lr 1.8271e-03 eta 0:00:03
epoch [40/200] batch [30/32] time 0.499 (0.468) data 0.370 (0.338) loss_x loss_x 1.2051 (1.3217) acc_x 68.7500 (65.9375) lr 1.8271e-03 eta 0:00:00
epoch [40/200] batch [5/65] time 0.426 (0.461) data 0.295 (0.331) loss_u loss_u 0.8477 (0.8952) acc_u 21.8750 (14.3750) lr 1.8271e-03 eta 0:00:27
epoch [40/200] batch [10/65] time 0.377 (0.457) data 0.247 (0.327) loss_u loss_u 0.9248 (0.9102) acc_u 12.5000 (12.1875) lr 1.8271e-03 eta 0:00:25
epoch [40/200] batch [15/65] time 0.352 (0.457) data 0.221 (0.326) loss_u loss_u 0.8384 (0.8968) acc_u 25.0000 (13.7500) lr 1.8271e-03 eta 0:00:22
epoch [40/200] batch [20/65] time 0.395 (0.451) data 0.264 (0.320) loss_u loss_u 0.8633 (0.8924) acc_u 18.7500 (14.2188) lr 1.8271e-03 eta 0:00:20
epoch [40/200] batch [25/65] time 0.378 (0.446) data 0.248 (0.315) loss_u loss_u 0.8613 (0.8868) acc_u 15.6250 (14.7500) lr 1.8271e-03 eta 0:00:17
epoch [40/200] batch [30/65] time 0.362 (0.443) data 0.231 (0.312) loss_u loss_u 0.8428 (0.8839) acc_u 18.7500 (15.2083) lr 1.8271e-03 eta 0:00:15
epoch [40/200] batch [35/65] time 0.392 (0.440) data 0.262 (0.310) loss_u loss_u 0.8955 (0.8866) acc_u 12.5000 (14.8214) lr 1.8271e-03 eta 0:00:13
epoch [40/200] batch [40/65] time 0.500 (0.438) data 0.370 (0.308) loss_u loss_u 0.9175 (0.8862) acc_u 12.5000 (15.1562) lr 1.8271e-03 eta 0:00:10
epoch [40/200] batch [45/65] time 0.416 (0.436) data 0.286 (0.305) loss_u loss_u 0.9292 (0.8892) acc_u 6.2500 (14.6528) lr 1.8271e-03 eta 0:00:08
epoch [40/200] batch [50/65] time 0.407 (0.440) data 0.277 (0.309) loss_u loss_u 0.8486 (0.8893) acc_u 18.7500 (14.3750) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [55/65] time 0.479 (0.440) data 0.349 (0.310) loss_u loss_u 0.8594 (0.8888) acc_u 18.7500 (14.3750) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [60/65] time 0.353 (0.441) data 0.223 (0.311) loss_u loss_u 0.8506 (0.8883) acc_u 21.8750 (14.4271) lr 1.8271e-03 eta 0:00:02
epoch [40/200] batch [65/65] time 0.380 (0.442) data 0.250 (0.311) loss_u loss_u 0.8613 (0.8850) acc_u 15.6250 (14.8077) lr 1.8271e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1550
confident_label rate tensor(0.3342, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1048
clean true:1043
clean false:5
clean_rate:0.9952290076335878
noisy true:543
noisy false:1545
after delete: len(clean_dataset) 1048
after delete: len(noisy_dataset) 2088
epoch [41/200] batch [5/32] time 0.469 (0.471) data 0.338 (0.339) loss_x loss_x 1.3428 (1.3579) acc_x 65.6250 (68.7500) lr 1.8181e-03 eta 0:00:12
epoch [41/200] batch [10/32] time 0.399 (0.462) data 0.268 (0.330) loss_x loss_x 1.1123 (1.4039) acc_x 75.0000 (66.8750) lr 1.8181e-03 eta 0:00:10
epoch [41/200] batch [15/32] time 0.471 (0.478) data 0.340 (0.346) loss_x loss_x 0.9419 (1.3361) acc_x 81.2500 (68.1250) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [20/32] time 0.436 (0.474) data 0.305 (0.343) loss_x loss_x 1.3086 (1.3161) acc_x 71.8750 (67.3438) lr 1.8181e-03 eta 0:00:05
epoch [41/200] batch [25/32] time 0.404 (0.464) data 0.272 (0.332) loss_x loss_x 1.7051 (1.3498) acc_x 56.2500 (67.0000) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [30/32] time 0.373 (0.467) data 0.243 (0.335) loss_x loss_x 0.9385 (1.3398) acc_x 75.0000 (66.4583) lr 1.8181e-03 eta 0:00:00
epoch [41/200] batch [5/65] time 0.567 (0.462) data 0.435 (0.331) loss_u loss_u 0.9009 (0.9153) acc_u 12.5000 (11.8750) lr 1.8181e-03 eta 0:00:27
epoch [41/200] batch [10/65] time 0.441 (0.453) data 0.309 (0.322) loss_u loss_u 0.8477 (0.8911) acc_u 25.0000 (16.8750) lr 1.8181e-03 eta 0:00:24
epoch [41/200] batch [15/65] time 0.437 (0.451) data 0.306 (0.320) loss_u loss_u 0.8916 (0.8886) acc_u 15.6250 (15.6250) lr 1.8181e-03 eta 0:00:22
epoch [41/200] batch [20/65] time 0.494 (0.451) data 0.363 (0.320) loss_u loss_u 0.8354 (0.8904) acc_u 18.7500 (15.0000) lr 1.8181e-03 eta 0:00:20
epoch [41/200] batch [25/65] time 0.384 (0.446) data 0.253 (0.315) loss_u loss_u 0.8887 (0.8898) acc_u 15.6250 (15.0000) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [30/65] time 0.721 (0.450) data 0.591 (0.319) loss_u loss_u 0.8721 (0.8881) acc_u 18.7500 (15.2083) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [35/65] time 0.414 (0.449) data 0.283 (0.318) loss_u loss_u 0.8740 (0.8884) acc_u 12.5000 (15.0893) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [40/65] time 0.343 (0.444) data 0.212 (0.313) loss_u loss_u 0.8936 (0.8898) acc_u 12.5000 (14.8438) lr 1.8181e-03 eta 0:00:11
epoch [41/200] batch [45/65] time 0.424 (0.442) data 0.294 (0.311) loss_u loss_u 0.8501 (0.8898) acc_u 25.0000 (14.6528) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [50/65] time 0.549 (0.444) data 0.418 (0.313) loss_u loss_u 0.9316 (0.8917) acc_u 12.5000 (14.1875) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [55/65] time 0.432 (0.446) data 0.300 (0.315) loss_u loss_u 0.9082 (0.8917) acc_u 15.6250 (14.1477) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [60/65] time 0.389 (0.445) data 0.258 (0.313) loss_u loss_u 0.8643 (0.8922) acc_u 21.8750 (14.2708) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [65/65] time 0.411 (0.444) data 0.280 (0.313) loss_u loss_u 0.8511 (0.8871) acc_u 21.8750 (14.9519) lr 1.8181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1589
confident_label rate tensor(0.3288, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1031
clean true:1027
clean false:4
clean_rate:0.9961202715809894
noisy true:520
noisy false:1585
after delete: len(clean_dataset) 1031
after delete: len(noisy_dataset) 2105
epoch [42/200] batch [5/32] time 0.414 (0.465) data 0.283 (0.334) loss_x loss_x 1.2383 (1.2820) acc_x 62.5000 (71.2500) lr 1.8090e-03 eta 0:00:12
epoch [42/200] batch [10/32] time 0.418 (0.445) data 0.287 (0.314) loss_x loss_x 1.3916 (1.3602) acc_x 62.5000 (68.1250) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [15/32] time 0.438 (0.436) data 0.307 (0.306) loss_x loss_x 1.2979 (1.3921) acc_x 53.1250 (66.2500) lr 1.8090e-03 eta 0:00:07
epoch [42/200] batch [20/32] time 0.340 (0.433) data 0.209 (0.302) loss_x loss_x 1.6016 (1.3880) acc_x 62.5000 (65.9375) lr 1.8090e-03 eta 0:00:05
epoch [42/200] batch [25/32] time 0.368 (0.442) data 0.238 (0.311) loss_x loss_x 1.5234 (1.3803) acc_x 62.5000 (66.2500) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [30/32] time 0.519 (0.449) data 0.389 (0.319) loss_x loss_x 1.3223 (1.3872) acc_x 71.8750 (66.0417) lr 1.8090e-03 eta 0:00:00
epoch [42/200] batch [5/65] time 0.568 (0.447) data 0.436 (0.316) loss_u loss_u 0.8462 (0.8733) acc_u 21.8750 (17.5000) lr 1.8090e-03 eta 0:00:26
epoch [42/200] batch [10/65] time 0.433 (0.441) data 0.301 (0.311) loss_u loss_u 0.9023 (0.8667) acc_u 15.6250 (18.1250) lr 1.8090e-03 eta 0:00:24
epoch [42/200] batch [15/65] time 0.462 (0.447) data 0.331 (0.316) loss_u loss_u 0.9248 (0.8802) acc_u 9.3750 (15.6250) lr 1.8090e-03 eta 0:00:22
epoch [42/200] batch [20/65] time 0.531 (0.447) data 0.400 (0.316) loss_u loss_u 0.9023 (0.8783) acc_u 12.5000 (15.7812) lr 1.8090e-03 eta 0:00:20
epoch [42/200] batch [25/65] time 0.625 (0.451) data 0.493 (0.320) loss_u loss_u 0.8423 (0.8748) acc_u 25.0000 (16.0000) lr 1.8090e-03 eta 0:00:18
epoch [42/200] batch [30/65] time 0.393 (0.450) data 0.261 (0.319) loss_u loss_u 0.9116 (0.8781) acc_u 12.5000 (15.6250) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [35/65] time 0.398 (0.457) data 0.267 (0.326) loss_u loss_u 0.8433 (0.8752) acc_u 18.7500 (15.9821) lr 1.8090e-03 eta 0:00:13
epoch [42/200] batch [40/65] time 0.564 (0.458) data 0.432 (0.326) loss_u loss_u 0.9316 (0.8807) acc_u 9.3750 (15.1562) lr 1.8090e-03 eta 0:00:11
epoch [42/200] batch [45/65] time 0.540 (0.459) data 0.409 (0.327) loss_u loss_u 0.8242 (0.8831) acc_u 25.0000 (15.0694) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [50/65] time 0.501 (0.460) data 0.369 (0.328) loss_u loss_u 0.9229 (0.8842) acc_u 9.3750 (15.0625) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [55/65] time 0.488 (0.461) data 0.357 (0.329) loss_u loss_u 0.8345 (0.8836) acc_u 21.8750 (15.0000) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [60/65] time 0.406 (0.457) data 0.275 (0.326) loss_u loss_u 0.8804 (0.8866) acc_u 18.7500 (14.5833) lr 1.8090e-03 eta 0:00:02
epoch [42/200] batch [65/65] time 0.447 (0.456) data 0.315 (0.324) loss_u loss_u 0.8491 (0.8852) acc_u 21.8750 (14.8558) lr 1.8090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1528
confident_label rate tensor(0.3460, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1085
clean true:1083
clean false:2
clean_rate:0.9981566820276497
noisy true:525
noisy false:1526
after delete: len(clean_dataset) 1085
after delete: len(noisy_dataset) 2051
epoch [43/200] batch [5/33] time 0.449 (0.495) data 0.318 (0.364) loss_x loss_x 1.2578 (1.2917) acc_x 75.0000 (71.2500) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [10/33] time 0.652 (0.504) data 0.523 (0.373) loss_x loss_x 1.0186 (1.3083) acc_x 68.7500 (68.7500) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [15/33] time 0.412 (0.472) data 0.282 (0.341) loss_x loss_x 1.4316 (1.2802) acc_x 75.0000 (70.0000) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [20/33] time 0.505 (0.472) data 0.374 (0.342) loss_x loss_x 1.2168 (1.2912) acc_x 71.8750 (68.5938) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [25/33] time 0.491 (0.466) data 0.361 (0.336) loss_x loss_x 1.0332 (1.2763) acc_x 75.0000 (69.2500) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [30/33] time 0.464 (0.464) data 0.335 (0.333) loss_x loss_x 1.4385 (1.3064) acc_x 59.3750 (68.4375) lr 1.7997e-03 eta 0:00:01
epoch [43/200] batch [5/64] time 0.386 (0.464) data 0.256 (0.333) loss_u loss_u 0.8540 (0.8972) acc_u 21.8750 (13.1250) lr 1.7997e-03 eta 0:00:27
epoch [43/200] batch [10/64] time 0.362 (0.461) data 0.231 (0.331) loss_u loss_u 0.9048 (0.8881) acc_u 12.5000 (14.6875) lr 1.7997e-03 eta 0:00:24
epoch [43/200] batch [15/64] time 0.360 (0.455) data 0.230 (0.324) loss_u loss_u 0.9023 (0.8798) acc_u 12.5000 (15.2083) lr 1.7997e-03 eta 0:00:22
epoch [43/200] batch [20/64] time 0.373 (0.447) data 0.240 (0.317) loss_u loss_u 0.8467 (0.8727) acc_u 25.0000 (16.0938) lr 1.7997e-03 eta 0:00:19
epoch [43/200] batch [25/64] time 0.561 (0.448) data 0.429 (0.318) loss_u loss_u 0.8818 (0.8753) acc_u 15.6250 (16.0000) lr 1.7997e-03 eta 0:00:17
epoch [43/200] batch [30/64] time 0.442 (0.451) data 0.310 (0.320) loss_u loss_u 0.9194 (0.8811) acc_u 6.2500 (15.1042) lr 1.7997e-03 eta 0:00:15
epoch [43/200] batch [35/64] time 0.483 (0.447) data 0.353 (0.316) loss_u loss_u 0.9419 (0.8846) acc_u 12.5000 (14.8214) lr 1.7997e-03 eta 0:00:12
epoch [43/200] batch [40/64] time 0.337 (0.444) data 0.207 (0.313) loss_u loss_u 0.8833 (0.8861) acc_u 12.5000 (14.4531) lr 1.7997e-03 eta 0:00:10
epoch [43/200] batch [45/64] time 0.406 (0.442) data 0.274 (0.311) loss_u loss_u 0.8691 (0.8864) acc_u 18.7500 (14.4444) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [50/64] time 0.436 (0.443) data 0.305 (0.312) loss_u loss_u 0.8916 (0.8868) acc_u 12.5000 (14.2500) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [55/64] time 0.407 (0.441) data 0.276 (0.310) loss_u loss_u 0.9756 (0.8880) acc_u 0.0000 (14.2045) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [60/64] time 0.542 (0.442) data 0.410 (0.311) loss_u loss_u 0.8916 (0.8891) acc_u 12.5000 (14.0104) lr 1.7997e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1636
confident_label rate tensor(0.3182, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 998
clean true:994
clean false:4
clean_rate:0.9959919839679359
noisy true:506
noisy false:1632
after delete: len(clean_dataset) 998
after delete: len(noisy_dataset) 2138
epoch [44/200] batch [5/31] time 0.463 (0.501) data 0.333 (0.370) loss_x loss_x 1.1338 (1.3986) acc_x 75.0000 (68.1250) lr 1.7902e-03 eta 0:00:13
epoch [44/200] batch [10/31] time 0.608 (0.483) data 0.477 (0.352) loss_x loss_x 0.7974 (1.2875) acc_x 75.0000 (69.0625) lr 1.7902e-03 eta 0:00:10
epoch [44/200] batch [15/31] time 0.453 (0.473) data 0.323 (0.342) loss_x loss_x 1.5684 (1.2881) acc_x 59.3750 (69.1667) lr 1.7902e-03 eta 0:00:07
epoch [44/200] batch [20/31] time 0.542 (0.483) data 0.411 (0.352) loss_x loss_x 0.8711 (1.3131) acc_x 84.3750 (69.3750) lr 1.7902e-03 eta 0:00:05
epoch [44/200] batch [25/31] time 0.443 (0.470) data 0.312 (0.339) loss_x loss_x 1.3232 (1.3080) acc_x 65.6250 (68.6250) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [30/31] time 0.422 (0.472) data 0.291 (0.341) loss_x loss_x 1.7666 (1.3175) acc_x 53.1250 (67.8125) lr 1.7902e-03 eta 0:00:00
epoch [44/200] batch [5/66] time 0.406 (0.474) data 0.272 (0.343) loss_u loss_u 0.9468 (0.8853) acc_u 6.2500 (16.2500) lr 1.7902e-03 eta 0:00:28
epoch [44/200] batch [10/66] time 0.533 (0.473) data 0.401 (0.341) loss_u loss_u 0.7974 (0.8909) acc_u 21.8750 (14.0625) lr 1.7902e-03 eta 0:00:26
epoch [44/200] batch [15/66] time 0.672 (0.474) data 0.541 (0.342) loss_u loss_u 0.9219 (0.8832) acc_u 9.3750 (15.6250) lr 1.7902e-03 eta 0:00:24
epoch [44/200] batch [20/66] time 0.337 (0.470) data 0.205 (0.339) loss_u loss_u 0.9141 (0.8894) acc_u 6.2500 (14.5312) lr 1.7902e-03 eta 0:00:21
epoch [44/200] batch [25/66] time 0.611 (0.477) data 0.478 (0.345) loss_u loss_u 0.7900 (0.8794) acc_u 21.8750 (15.7500) lr 1.7902e-03 eta 0:00:19
epoch [44/200] batch [30/66] time 0.373 (0.473) data 0.241 (0.341) loss_u loss_u 0.9258 (0.8790) acc_u 9.3750 (15.8333) lr 1.7902e-03 eta 0:00:17
epoch [44/200] batch [35/66] time 0.780 (0.482) data 0.648 (0.350) loss_u loss_u 0.8882 (0.8798) acc_u 12.5000 (15.5357) lr 1.7902e-03 eta 0:00:14
epoch [44/200] batch [40/66] time 0.336 (0.478) data 0.205 (0.346) loss_u loss_u 0.8970 (0.8793) acc_u 12.5000 (15.7031) lr 1.7902e-03 eta 0:00:12
epoch [44/200] batch [45/66] time 0.449 (0.475) data 0.317 (0.343) loss_u loss_u 0.8271 (0.8769) acc_u 28.1250 (16.0417) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [50/66] time 0.479 (0.471) data 0.348 (0.339) loss_u loss_u 0.8672 (0.8776) acc_u 15.6250 (15.8750) lr 1.7902e-03 eta 0:00:07
epoch [44/200] batch [55/66] time 0.374 (0.467) data 0.242 (0.335) loss_u loss_u 0.8687 (0.8758) acc_u 15.6250 (15.9091) lr 1.7902e-03 eta 0:00:05
epoch [44/200] batch [60/66] time 0.391 (0.463) data 0.259 (0.331) loss_u loss_u 0.8511 (0.8742) acc_u 21.8750 (16.1458) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [65/66] time 0.459 (0.464) data 0.327 (0.332) loss_u loss_u 0.9150 (0.8749) acc_u 6.2500 (15.8654) lr 1.7902e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1538
confident_label rate tensor(0.3390, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1063
clean true:1054
clean false:9
clean_rate:0.9915333960489181
noisy true:544
noisy false:1529
after delete: len(clean_dataset) 1063
after delete: len(noisy_dataset) 2073
epoch [45/200] batch [5/33] time 0.559 (0.463) data 0.428 (0.332) loss_x loss_x 1.3633 (1.3129) acc_x 65.6250 (65.0000) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [10/33] time 0.358 (0.437) data 0.227 (0.306) loss_x loss_x 1.3242 (1.3372) acc_x 62.5000 (64.6875) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [15/33] time 0.498 (0.453) data 0.367 (0.322) loss_x loss_x 1.7598 (1.3562) acc_x 65.6250 (64.5833) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [20/33] time 0.560 (0.470) data 0.429 (0.339) loss_x loss_x 1.2051 (1.3972) acc_x 68.7500 (64.3750) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [25/33] time 0.375 (0.457) data 0.244 (0.326) loss_x loss_x 1.6650 (1.3915) acc_x 56.2500 (64.8750) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [30/33] time 0.460 (0.453) data 0.330 (0.322) loss_x loss_x 1.0879 (1.4006) acc_x 75.0000 (64.1667) lr 1.7804e-03 eta 0:00:01
epoch [45/200] batch [5/64] time 0.533 (0.453) data 0.402 (0.322) loss_u loss_u 0.9312 (0.8743) acc_u 9.3750 (18.1250) lr 1.7804e-03 eta 0:00:26
epoch [45/200] batch [10/64] time 0.401 (0.447) data 0.270 (0.316) loss_u loss_u 0.8745 (0.8735) acc_u 15.6250 (17.1875) lr 1.7804e-03 eta 0:00:24
epoch [45/200] batch [15/64] time 0.455 (0.443) data 0.323 (0.312) loss_u loss_u 0.9009 (0.8752) acc_u 15.6250 (17.9167) lr 1.7804e-03 eta 0:00:21
epoch [45/200] batch [20/64] time 0.447 (0.442) data 0.315 (0.311) loss_u loss_u 0.9150 (0.8780) acc_u 12.5000 (17.0312) lr 1.7804e-03 eta 0:00:19
epoch [45/200] batch [25/64] time 0.697 (0.441) data 0.565 (0.310) loss_u loss_u 0.8096 (0.8713) acc_u 25.0000 (17.5000) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [30/64] time 0.412 (0.440) data 0.280 (0.309) loss_u loss_u 0.8491 (0.8762) acc_u 25.0000 (16.7708) lr 1.7804e-03 eta 0:00:14
epoch [45/200] batch [35/64] time 0.489 (0.440) data 0.353 (0.309) loss_u loss_u 0.8872 (0.8761) acc_u 15.6250 (16.8750) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [40/64] time 0.442 (0.444) data 0.311 (0.312) loss_u loss_u 0.8535 (0.8794) acc_u 18.7500 (16.4062) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [45/64] time 0.451 (0.446) data 0.318 (0.315) loss_u loss_u 0.8496 (0.8779) acc_u 25.0000 (16.3889) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [50/64] time 0.363 (0.451) data 0.233 (0.320) loss_u loss_u 0.8228 (0.8750) acc_u 25.0000 (16.6875) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [55/64] time 0.405 (0.449) data 0.275 (0.318) loss_u loss_u 0.8721 (0.8774) acc_u 21.8750 (16.3636) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [60/64] time 0.482 (0.447) data 0.352 (0.316) loss_u loss_u 0.8569 (0.8776) acc_u 15.6250 (16.2500) lr 1.7804e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1550
confident_label rate tensor(0.3444, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1080
clean true:1074
clean false:6
clean_rate:0.9944444444444445
noisy true:512
noisy false:1544
after delete: len(clean_dataset) 1080
after delete: len(noisy_dataset) 2056
epoch [46/200] batch [5/33] time 0.464 (0.441) data 0.333 (0.311) loss_x loss_x 1.3770 (1.3875) acc_x 59.3750 (61.2500) lr 1.7705e-03 eta 0:00:12
epoch [46/200] batch [10/33] time 0.424 (0.438) data 0.294 (0.307) loss_x loss_x 0.7539 (1.3404) acc_x 81.2500 (65.0000) lr 1.7705e-03 eta 0:00:10
epoch [46/200] batch [15/33] time 0.490 (0.441) data 0.360 (0.310) loss_x loss_x 0.8154 (1.2966) acc_x 78.1250 (66.2500) lr 1.7705e-03 eta 0:00:07
epoch [46/200] batch [20/33] time 0.439 (0.439) data 0.306 (0.308) loss_x loss_x 1.1514 (1.2802) acc_x 68.7500 (67.8125) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [25/33] time 0.579 (0.458) data 0.448 (0.327) loss_x loss_x 1.2539 (1.2829) acc_x 68.7500 (67.6250) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [30/33] time 0.478 (0.462) data 0.348 (0.332) loss_x loss_x 1.3584 (1.2748) acc_x 68.7500 (67.7083) lr 1.7705e-03 eta 0:00:01
epoch [46/200] batch [5/64] time 0.425 (0.464) data 0.294 (0.333) loss_u loss_u 0.9019 (0.8966) acc_u 15.6250 (13.7500) lr 1.7705e-03 eta 0:00:27
epoch [46/200] batch [10/64] time 0.424 (0.455) data 0.294 (0.325) loss_u loss_u 0.7915 (0.8877) acc_u 25.0000 (13.7500) lr 1.7705e-03 eta 0:00:24
epoch [46/200] batch [15/64] time 0.294 (0.456) data 0.163 (0.325) loss_u loss_u 0.9546 (0.8887) acc_u 6.2500 (13.3333) lr 1.7705e-03 eta 0:00:22
epoch [46/200] batch [20/64] time 0.445 (0.454) data 0.313 (0.323) loss_u loss_u 0.8330 (0.8918) acc_u 25.0000 (13.1250) lr 1.7705e-03 eta 0:00:19
epoch [46/200] batch [25/64] time 0.378 (0.453) data 0.247 (0.323) loss_u loss_u 0.9077 (0.8914) acc_u 9.3750 (13.0000) lr 1.7705e-03 eta 0:00:17
epoch [46/200] batch [30/64] time 0.472 (0.451) data 0.340 (0.320) loss_u loss_u 0.8345 (0.8918) acc_u 25.0000 (13.0208) lr 1.7705e-03 eta 0:00:15
epoch [46/200] batch [35/64] time 0.459 (0.448) data 0.327 (0.317) loss_u loss_u 0.8428 (0.8880) acc_u 15.6250 (13.6607) lr 1.7705e-03 eta 0:00:12
epoch [46/200] batch [40/64] time 0.500 (0.446) data 0.369 (0.315) loss_u loss_u 0.8882 (0.8881) acc_u 15.6250 (13.7500) lr 1.7705e-03 eta 0:00:10
epoch [46/200] batch [45/64] time 0.357 (0.444) data 0.225 (0.313) loss_u loss_u 0.8564 (0.8891) acc_u 18.7500 (13.7500) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [50/64] time 0.457 (0.446) data 0.325 (0.315) loss_u loss_u 0.8438 (0.8849) acc_u 18.7500 (14.1250) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [55/64] time 0.559 (0.447) data 0.428 (0.316) loss_u loss_u 0.8506 (0.8864) acc_u 15.6250 (13.9773) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [60/64] time 0.355 (0.443) data 0.225 (0.312) loss_u loss_u 0.8604 (0.8850) acc_u 18.7500 (14.1667) lr 1.7705e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1545
confident_label rate tensor(0.3361, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1054
clean true:1047
clean false:7
clean_rate:0.9933586337760911
noisy true:544
noisy false:1538
after delete: len(clean_dataset) 1054
after delete: len(noisy_dataset) 2082
epoch [47/200] batch [5/32] time 0.458 (0.452) data 0.327 (0.321) loss_x loss_x 1.0596 (1.3855) acc_x 71.8750 (64.3750) lr 1.7604e-03 eta 0:00:12
epoch [47/200] batch [10/32] time 0.379 (0.463) data 0.249 (0.332) loss_x loss_x 1.4365 (1.3420) acc_x 71.8750 (65.3125) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [15/32] time 0.496 (0.471) data 0.365 (0.340) loss_x loss_x 1.6338 (1.3643) acc_x 59.3750 (65.6250) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [20/32] time 0.479 (0.472) data 0.348 (0.341) loss_x loss_x 1.0723 (1.3590) acc_x 71.8750 (65.4688) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [25/32] time 0.439 (0.466) data 0.308 (0.335) loss_x loss_x 1.1514 (1.3505) acc_x 71.8750 (65.8750) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [30/32] time 0.590 (0.470) data 0.459 (0.339) loss_x loss_x 1.5312 (1.3465) acc_x 62.5000 (66.0417) lr 1.7604e-03 eta 0:00:00
epoch [47/200] batch [5/65] time 0.726 (0.465) data 0.595 (0.334) loss_u loss_u 0.8662 (0.8717) acc_u 15.6250 (18.1250) lr 1.7604e-03 eta 0:00:27
epoch [47/200] batch [10/65] time 0.492 (0.464) data 0.360 (0.333) loss_u loss_u 0.9341 (0.8868) acc_u 15.6250 (16.5625) lr 1.7604e-03 eta 0:00:25
epoch [47/200] batch [15/65] time 0.329 (0.458) data 0.198 (0.327) loss_u loss_u 0.8765 (0.8866) acc_u 21.8750 (16.4583) lr 1.7604e-03 eta 0:00:22
epoch [47/200] batch [20/65] time 0.569 (0.455) data 0.438 (0.324) loss_u loss_u 0.8770 (0.8812) acc_u 15.6250 (17.0312) lr 1.7604e-03 eta 0:00:20
epoch [47/200] batch [25/65] time 0.425 (0.453) data 0.294 (0.322) loss_u loss_u 0.8154 (0.8786) acc_u 25.0000 (17.1250) lr 1.7604e-03 eta 0:00:18
epoch [47/200] batch [30/65] time 0.378 (0.451) data 0.244 (0.320) loss_u loss_u 0.8628 (0.8778) acc_u 15.6250 (16.9792) lr 1.7604e-03 eta 0:00:15
epoch [47/200] batch [35/65] time 0.507 (0.456) data 0.375 (0.324) loss_u loss_u 0.8735 (0.8774) acc_u 18.7500 (16.9643) lr 1.7604e-03 eta 0:00:13
epoch [47/200] batch [40/65] time 0.466 (0.452) data 0.335 (0.321) loss_u loss_u 0.9600 (0.8788) acc_u 3.1250 (16.7969) lr 1.7604e-03 eta 0:00:11
epoch [47/200] batch [45/65] time 0.434 (0.452) data 0.303 (0.321) loss_u loss_u 0.9277 (0.8793) acc_u 9.3750 (16.5972) lr 1.7604e-03 eta 0:00:09
epoch [47/200] batch [50/65] time 0.517 (0.453) data 0.386 (0.322) loss_u loss_u 0.8735 (0.8776) acc_u 12.5000 (16.6875) lr 1.7604e-03 eta 0:00:06
epoch [47/200] batch [55/65] time 0.513 (0.452) data 0.381 (0.321) loss_u loss_u 0.8408 (0.8766) acc_u 15.6250 (16.7614) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [60/65] time 0.434 (0.449) data 0.303 (0.318) loss_u loss_u 0.8901 (0.8773) acc_u 15.6250 (16.6146) lr 1.7604e-03 eta 0:00:02
epoch [47/200] batch [65/65] time 0.355 (0.447) data 0.223 (0.316) loss_u loss_u 0.9106 (0.8784) acc_u 9.3750 (16.4423) lr 1.7604e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1596
confident_label rate tensor(0.3272, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1026
clean true:1022
clean false:4
clean_rate:0.9961013645224172
noisy true:518
noisy false:1592
after delete: len(clean_dataset) 1026
after delete: len(noisy_dataset) 2110
epoch [48/200] batch [5/32] time 0.676 (0.463) data 0.545 (0.332) loss_x loss_x 1.0430 (1.2679) acc_x 68.7500 (66.8750) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [10/32] time 0.593 (0.482) data 0.463 (0.352) loss_x loss_x 1.2754 (1.2435) acc_x 68.7500 (67.1875) lr 1.7501e-03 eta 0:00:10
epoch [48/200] batch [15/32] time 0.538 (0.487) data 0.405 (0.357) loss_x loss_x 1.3369 (1.2761) acc_x 59.3750 (66.8750) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [20/32] time 0.469 (0.472) data 0.338 (0.341) loss_x loss_x 0.7295 (1.2713) acc_x 81.2500 (66.8750) lr 1.7501e-03 eta 0:00:05
epoch [48/200] batch [25/32] time 0.442 (0.470) data 0.311 (0.339) loss_x loss_x 1.1758 (1.2688) acc_x 75.0000 (66.5000) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [30/32] time 0.545 (0.465) data 0.415 (0.334) loss_x loss_x 1.2354 (1.2912) acc_x 62.5000 (66.2500) lr 1.7501e-03 eta 0:00:00
epoch [48/200] batch [5/65] time 0.383 (0.458) data 0.249 (0.327) loss_u loss_u 0.8198 (0.8566) acc_u 21.8750 (17.5000) lr 1.7501e-03 eta 0:00:27
epoch [48/200] batch [10/65] time 0.453 (0.453) data 0.322 (0.322) loss_u loss_u 0.7422 (0.8628) acc_u 31.2500 (16.8750) lr 1.7501e-03 eta 0:00:24
epoch [48/200] batch [15/65] time 0.503 (0.452) data 0.372 (0.321) loss_u loss_u 0.8145 (0.8606) acc_u 21.8750 (17.5000) lr 1.7501e-03 eta 0:00:22
epoch [48/200] batch [20/65] time 0.330 (0.447) data 0.199 (0.316) loss_u loss_u 0.8496 (0.8659) acc_u 18.7500 (17.0312) lr 1.7501e-03 eta 0:00:20
epoch [48/200] batch [25/65] time 0.372 (0.445) data 0.240 (0.314) loss_u loss_u 0.8638 (0.8652) acc_u 9.3750 (16.2500) lr 1.7501e-03 eta 0:00:17
epoch [48/200] batch [30/65] time 0.358 (0.440) data 0.227 (0.309) loss_u loss_u 0.8569 (0.8691) acc_u 12.5000 (16.0417) lr 1.7501e-03 eta 0:00:15
epoch [48/200] batch [35/65] time 0.444 (0.442) data 0.313 (0.311) loss_u loss_u 0.9443 (0.8723) acc_u 6.2500 (15.6250) lr 1.7501e-03 eta 0:00:13
epoch [48/200] batch [40/65] time 0.371 (0.439) data 0.240 (0.308) loss_u loss_u 0.9478 (0.8751) acc_u 6.2500 (15.3125) lr 1.7501e-03 eta 0:00:10
epoch [48/200] batch [45/65] time 0.443 (0.437) data 0.312 (0.306) loss_u loss_u 0.8867 (0.8748) acc_u 15.6250 (15.2083) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [50/65] time 0.446 (0.437) data 0.315 (0.306) loss_u loss_u 0.8159 (0.8749) acc_u 21.8750 (15.1250) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [55/65] time 0.525 (0.440) data 0.393 (0.309) loss_u loss_u 0.8589 (0.8787) acc_u 18.7500 (14.8295) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [60/65] time 0.431 (0.442) data 0.299 (0.310) loss_u loss_u 0.9146 (0.8766) acc_u 12.5000 (15.1562) lr 1.7501e-03 eta 0:00:02
epoch [48/200] batch [65/65] time 0.426 (0.442) data 0.294 (0.311) loss_u loss_u 0.8794 (0.8771) acc_u 12.5000 (15.1923) lr 1.7501e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1578
confident_label rate tensor(0.3268, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1025
clean true:1021
clean false:4
clean_rate:0.9960975609756098
noisy true:537
noisy false:1574
after delete: len(clean_dataset) 1025
after delete: len(noisy_dataset) 2111
epoch [49/200] batch [5/32] time 0.499 (0.463) data 0.369 (0.332) loss_x loss_x 1.5010 (1.2281) acc_x 68.7500 (73.7500) lr 1.7396e-03 eta 0:00:12
epoch [49/200] batch [10/32] time 0.654 (0.500) data 0.524 (0.369) loss_x loss_x 1.1953 (1.3131) acc_x 68.7500 (70.3125) lr 1.7396e-03 eta 0:00:10
epoch [49/200] batch [15/32] time 0.546 (0.500) data 0.416 (0.369) loss_x loss_x 1.1133 (1.2596) acc_x 68.7500 (69.7917) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [20/32] time 0.412 (0.485) data 0.281 (0.354) loss_x loss_x 1.1025 (1.2960) acc_x 68.7500 (67.9688) lr 1.7396e-03 eta 0:00:05
epoch [49/200] batch [25/32] time 0.494 (0.479) data 0.364 (0.348) loss_x loss_x 1.3145 (1.2918) acc_x 71.8750 (67.7500) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [30/32] time 0.378 (0.478) data 0.247 (0.347) loss_x loss_x 1.1025 (1.3054) acc_x 65.6250 (66.7708) lr 1.7396e-03 eta 0:00:00
epoch [49/200] batch [5/65] time 0.485 (0.465) data 0.354 (0.334) loss_u loss_u 0.9468 (0.8927) acc_u 6.2500 (14.3750) lr 1.7396e-03 eta 0:00:27
epoch [49/200] batch [10/65] time 0.450 (0.466) data 0.319 (0.335) loss_u loss_u 0.8828 (0.8651) acc_u 9.3750 (18.7500) lr 1.7396e-03 eta 0:00:25
epoch [49/200] batch [15/65] time 0.462 (0.463) data 0.331 (0.332) loss_u loss_u 0.8657 (0.8675) acc_u 18.7500 (18.1250) lr 1.7396e-03 eta 0:00:23
epoch [49/200] batch [20/65] time 0.497 (0.459) data 0.366 (0.328) loss_u loss_u 0.8682 (0.8745) acc_u 18.7500 (17.1875) lr 1.7396e-03 eta 0:00:20
epoch [49/200] batch [25/65] time 0.475 (0.459) data 0.342 (0.328) loss_u loss_u 0.9263 (0.8798) acc_u 6.2500 (16.3750) lr 1.7396e-03 eta 0:00:18
epoch [49/200] batch [30/65] time 0.351 (0.460) data 0.219 (0.329) loss_u loss_u 0.9062 (0.8824) acc_u 9.3750 (15.8333) lr 1.7396e-03 eta 0:00:16
epoch [49/200] batch [35/65] time 0.469 (0.458) data 0.337 (0.327) loss_u loss_u 0.9170 (0.8859) acc_u 15.6250 (15.1786) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [40/65] time 0.346 (0.453) data 0.215 (0.321) loss_u loss_u 0.9092 (0.8863) acc_u 9.3750 (15.0000) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [45/65] time 0.463 (0.452) data 0.331 (0.321) loss_u loss_u 0.8716 (0.8873) acc_u 18.7500 (14.7917) lr 1.7396e-03 eta 0:00:09
epoch [49/200] batch [50/65] time 0.338 (0.450) data 0.206 (0.319) loss_u loss_u 0.9175 (0.8878) acc_u 9.3750 (14.8125) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [55/65] time 0.430 (0.449) data 0.298 (0.317) loss_u loss_u 0.9072 (0.8872) acc_u 6.2500 (14.6591) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [60/65] time 0.440 (0.445) data 0.308 (0.314) loss_u loss_u 0.8701 (0.8888) acc_u 18.7500 (14.4792) lr 1.7396e-03 eta 0:00:02
epoch [49/200] batch [65/65] time 0.382 (0.445) data 0.250 (0.313) loss_u loss_u 0.8813 (0.8877) acc_u 15.6250 (14.3750) lr 1.7396e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1599
confident_label rate tensor(0.3284, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1030
clean true:1026
clean false:4
clean_rate:0.996116504854369
noisy true:511
noisy false:1595
after delete: len(clean_dataset) 1030
after delete: len(noisy_dataset) 2106
epoch [50/200] batch [5/32] time 0.400 (0.453) data 0.270 (0.322) loss_x loss_x 1.3604 (1.1260) acc_x 75.0000 (76.2500) lr 1.7290e-03 eta 0:00:12
epoch [50/200] batch [10/32] time 0.550 (0.474) data 0.420 (0.343) loss_x loss_x 1.4385 (1.2917) acc_x 68.7500 (70.0000) lr 1.7290e-03 eta 0:00:10
epoch [50/200] batch [15/32] time 0.543 (0.465) data 0.412 (0.334) loss_x loss_x 1.6035 (1.3331) acc_x 65.6250 (68.9583) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [20/32] time 0.460 (0.466) data 0.329 (0.335) loss_x loss_x 1.5195 (1.4328) acc_x 50.0000 (66.8750) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [25/32] time 0.534 (0.456) data 0.403 (0.325) loss_x loss_x 1.0205 (1.3729) acc_x 68.7500 (67.6250) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [30/32] time 0.561 (0.480) data 0.432 (0.349) loss_x loss_x 1.2236 (1.3578) acc_x 68.7500 (67.9167) lr 1.7290e-03 eta 0:00:00
epoch [50/200] batch [5/65] time 0.358 (0.471) data 0.227 (0.340) loss_u loss_u 0.9072 (0.9309) acc_u 9.3750 (8.1250) lr 1.7290e-03 eta 0:00:28
epoch [50/200] batch [10/65] time 0.473 (0.463) data 0.342 (0.333) loss_u loss_u 0.8574 (0.9080) acc_u 15.6250 (10.9375) lr 1.7290e-03 eta 0:00:25
epoch [50/200] batch [15/65] time 0.433 (0.462) data 0.301 (0.331) loss_u loss_u 0.8208 (0.8921) acc_u 18.7500 (12.7083) lr 1.7290e-03 eta 0:00:23
epoch [50/200] batch [20/65] time 0.475 (0.462) data 0.343 (0.331) loss_u loss_u 0.8682 (0.8914) acc_u 12.5000 (12.0312) lr 1.7290e-03 eta 0:00:20
epoch [50/200] batch [25/65] time 0.410 (0.457) data 0.278 (0.326) loss_u loss_u 0.8228 (0.8876) acc_u 21.8750 (12.8750) lr 1.7290e-03 eta 0:00:18
epoch [50/200] batch [30/65] time 0.492 (0.456) data 0.361 (0.325) loss_u loss_u 0.9072 (0.8833) acc_u 12.5000 (13.6458) lr 1.7290e-03 eta 0:00:15
epoch [50/200] batch [35/65] time 0.498 (0.456) data 0.368 (0.325) loss_u loss_u 0.8848 (0.8867) acc_u 12.5000 (13.7500) lr 1.7290e-03 eta 0:00:13
epoch [50/200] batch [40/65] time 0.374 (0.451) data 0.242 (0.320) loss_u loss_u 0.8555 (0.8849) acc_u 25.0000 (14.5312) lr 1.7290e-03 eta 0:00:11
epoch [50/200] batch [45/65] time 0.569 (0.452) data 0.438 (0.321) loss_u loss_u 0.8936 (0.8818) acc_u 9.3750 (14.7917) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [50/65] time 0.358 (0.451) data 0.226 (0.320) loss_u loss_u 0.9370 (0.8839) acc_u 6.2500 (14.6875) lr 1.7290e-03 eta 0:00:06
epoch [50/200] batch [55/65] time 0.480 (0.449) data 0.349 (0.318) loss_u loss_u 0.9243 (0.8845) acc_u 6.2500 (14.4886) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [60/65] time 0.338 (0.446) data 0.206 (0.315) loss_u loss_u 0.8853 (0.8818) acc_u 18.7500 (14.7917) lr 1.7290e-03 eta 0:00:02
epoch [50/200] batch [65/65] time 0.429 (0.445) data 0.293 (0.314) loss_u loss_u 0.9053 (0.8817) acc_u 12.5000 (14.8558) lr 1.7290e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1507
confident_label rate tensor(0.3552, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1114
clean true:1111
clean false:3
clean_rate:0.9973070017953322
noisy true:518
noisy false:1504
after delete: len(clean_dataset) 1114
after delete: len(noisy_dataset) 2022
epoch [51/200] batch [5/34] time 0.426 (0.457) data 0.296 (0.326) loss_x loss_x 1.0791 (1.1313) acc_x 71.8750 (71.2500) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [10/34] time 0.355 (0.453) data 0.224 (0.322) loss_x loss_x 1.4805 (1.1833) acc_x 68.7500 (70.0000) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [15/34] time 0.384 (0.456) data 0.254 (0.325) loss_x loss_x 1.2695 (1.2681) acc_x 68.7500 (67.5000) lr 1.7181e-03 eta 0:00:08
epoch [51/200] batch [20/34] time 0.482 (0.464) data 0.351 (0.332) loss_x loss_x 1.4512 (1.2978) acc_x 56.2500 (67.1875) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [25/34] time 0.446 (0.454) data 0.315 (0.323) loss_x loss_x 1.2607 (1.2528) acc_x 65.6250 (68.3750) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [30/34] time 0.400 (0.452) data 0.269 (0.321) loss_x loss_x 1.1797 (1.2877) acc_x 78.1250 (67.9167) lr 1.7181e-03 eta 0:00:01
epoch [51/200] batch [5/63] time 0.444 (0.449) data 0.313 (0.318) loss_u loss_u 0.8955 (0.9017) acc_u 18.7500 (13.1250) lr 1.7181e-03 eta 0:00:26
epoch [51/200] batch [10/63] time 0.376 (0.445) data 0.244 (0.313) loss_u loss_u 0.9536 (0.9009) acc_u 6.2500 (12.5000) lr 1.7181e-03 eta 0:00:23
epoch [51/200] batch [15/63] time 0.390 (0.447) data 0.258 (0.316) loss_u loss_u 0.8374 (0.8987) acc_u 25.0000 (14.1667) lr 1.7181e-03 eta 0:00:21
epoch [51/200] batch [20/63] time 0.555 (0.451) data 0.422 (0.320) loss_u loss_u 0.9287 (0.8940) acc_u 9.3750 (14.6875) lr 1.7181e-03 eta 0:00:19
epoch [51/200] batch [25/63] time 0.800 (0.456) data 0.668 (0.324) loss_u loss_u 0.8232 (0.8897) acc_u 18.7500 (14.8750) lr 1.7181e-03 eta 0:00:17
epoch [51/200] batch [30/63] time 0.413 (0.454) data 0.282 (0.322) loss_u loss_u 0.9282 (0.8859) acc_u 15.6250 (15.4167) lr 1.7181e-03 eta 0:00:14
epoch [51/200] batch [35/63] time 0.416 (0.454) data 0.284 (0.323) loss_u loss_u 0.9102 (0.8838) acc_u 15.6250 (15.4464) lr 1.7181e-03 eta 0:00:12
epoch [51/200] batch [40/63] time 0.380 (0.452) data 0.248 (0.321) loss_u loss_u 0.8467 (0.8852) acc_u 21.8750 (15.3125) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [45/63] time 0.407 (0.449) data 0.276 (0.317) loss_u loss_u 0.8345 (0.8867) acc_u 21.8750 (14.9306) lr 1.7181e-03 eta 0:00:08
epoch [51/200] batch [50/63] time 0.416 (0.445) data 0.285 (0.314) loss_u loss_u 0.7964 (0.8854) acc_u 21.8750 (14.8750) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [55/63] time 0.385 (0.444) data 0.254 (0.313) loss_u loss_u 0.9336 (0.8828) acc_u 6.2500 (15.0568) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [60/63] time 0.404 (0.443) data 0.272 (0.312) loss_u loss_u 0.8916 (0.8842) acc_u 12.5000 (14.7917) lr 1.7181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1530
confident_label rate tensor(0.3460, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1085
clean true:1080
clean false:5
clean_rate:0.9953917050691244
noisy true:526
noisy false:1525
after delete: len(clean_dataset) 1085
after delete: len(noisy_dataset) 2051
epoch [52/200] batch [5/33] time 0.411 (0.478) data 0.281 (0.348) loss_x loss_x 1.4082 (1.3695) acc_x 68.7500 (65.0000) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [10/33] time 0.444 (0.453) data 0.314 (0.323) loss_x loss_x 1.4268 (1.3873) acc_x 65.6250 (64.3750) lr 1.7071e-03 eta 0:00:10
epoch [52/200] batch [15/33] time 0.713 (0.472) data 0.583 (0.342) loss_x loss_x 1.5410 (1.3446) acc_x 65.6250 (66.6667) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [20/33] time 0.415 (0.479) data 0.285 (0.348) loss_x loss_x 1.0801 (1.3396) acc_x 71.8750 (66.4062) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [25/33] time 0.392 (0.476) data 0.261 (0.345) loss_x loss_x 1.5020 (1.3067) acc_x 65.6250 (66.2500) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [30/33] time 0.426 (0.467) data 0.296 (0.337) loss_x loss_x 0.8359 (1.2888) acc_x 75.0000 (66.0417) lr 1.7071e-03 eta 0:00:01
epoch [52/200] batch [5/64] time 0.423 (0.458) data 0.292 (0.328) loss_u loss_u 0.8804 (0.8628) acc_u 15.6250 (18.1250) lr 1.7071e-03 eta 0:00:27
epoch [52/200] batch [10/64] time 0.354 (0.459) data 0.224 (0.328) loss_u loss_u 0.9312 (0.8734) acc_u 3.1250 (16.8750) lr 1.7071e-03 eta 0:00:24
epoch [52/200] batch [15/64] time 0.371 (0.451) data 0.240 (0.320) loss_u loss_u 0.8906 (0.8796) acc_u 15.6250 (15.6250) lr 1.7071e-03 eta 0:00:22
epoch [52/200] batch [20/64] time 0.452 (0.448) data 0.320 (0.318) loss_u loss_u 0.8921 (0.8826) acc_u 15.6250 (15.7812) lr 1.7071e-03 eta 0:00:19
epoch [52/200] batch [25/64] time 0.428 (0.447) data 0.297 (0.317) loss_u loss_u 0.8735 (0.8832) acc_u 21.8750 (15.5000) lr 1.7071e-03 eta 0:00:17
epoch [52/200] batch [30/64] time 0.341 (0.445) data 0.209 (0.314) loss_u loss_u 0.8364 (0.8815) acc_u 18.7500 (15.7292) lr 1.7071e-03 eta 0:00:15
epoch [52/200] batch [35/64] time 0.381 (0.446) data 0.250 (0.315) loss_u loss_u 0.8521 (0.8812) acc_u 18.7500 (15.6250) lr 1.7071e-03 eta 0:00:12
epoch [52/200] batch [40/64] time 0.607 (0.445) data 0.476 (0.314) loss_u loss_u 0.8857 (0.8773) acc_u 12.5000 (15.9375) lr 1.7071e-03 eta 0:00:10
epoch [52/200] batch [45/64] time 0.375 (0.444) data 0.244 (0.313) loss_u loss_u 0.8247 (0.8778) acc_u 15.6250 (15.9722) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [50/64] time 0.391 (0.443) data 0.260 (0.312) loss_u loss_u 0.9111 (0.8747) acc_u 12.5000 (16.4375) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [55/64] time 0.563 (0.444) data 0.432 (0.313) loss_u loss_u 0.9131 (0.8760) acc_u 9.3750 (16.1364) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [60/64] time 0.390 (0.444) data 0.258 (0.313) loss_u loss_u 0.9097 (0.8774) acc_u 12.5000 (15.8854) lr 1.7071e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1538
confident_label rate tensor(0.3453, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1083
clean true:1077
clean false:6
clean_rate:0.9944598337950139
noisy true:521
noisy false:1532
after delete: len(clean_dataset) 1083
after delete: len(noisy_dataset) 2053
epoch [53/200] batch [5/33] time 0.476 (0.468) data 0.345 (0.337) loss_x loss_x 0.9785 (1.3795) acc_x 81.2500 (65.6250) lr 1.6959e-03 eta 0:00:13
epoch [53/200] batch [10/33] time 0.375 (0.444) data 0.244 (0.313) loss_x loss_x 0.7065 (1.2665) acc_x 81.2500 (69.6875) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [15/33] time 0.595 (0.450) data 0.464 (0.319) loss_x loss_x 1.9297 (1.3245) acc_x 50.0000 (68.7500) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [20/33] time 0.451 (0.450) data 0.320 (0.319) loss_x loss_x 1.0684 (1.2833) acc_x 68.7500 (69.6875) lr 1.6959e-03 eta 0:00:05
epoch [53/200] batch [25/33] time 0.444 (0.441) data 0.313 (0.311) loss_x loss_x 1.1582 (1.2546) acc_x 68.7500 (69.2500) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [30/33] time 0.385 (0.439) data 0.254 (0.309) loss_x loss_x 1.2939 (1.2527) acc_x 68.7500 (68.6458) lr 1.6959e-03 eta 0:00:01
epoch [53/200] batch [5/64] time 0.413 (0.442) data 0.282 (0.311) loss_u loss_u 0.9238 (0.8843) acc_u 9.3750 (13.1250) lr 1.6959e-03 eta 0:00:26
epoch [53/200] batch [10/64] time 0.451 (0.439) data 0.321 (0.308) loss_u loss_u 0.8872 (0.8847) acc_u 12.5000 (14.6875) lr 1.6959e-03 eta 0:00:23
epoch [53/200] batch [15/64] time 0.352 (0.431) data 0.220 (0.300) loss_u loss_u 0.8965 (0.8820) acc_u 12.5000 (15.4167) lr 1.6959e-03 eta 0:00:21
epoch [53/200] batch [20/64] time 0.394 (0.430) data 0.262 (0.299) loss_u loss_u 0.8936 (0.8892) acc_u 12.5000 (14.2188) lr 1.6959e-03 eta 0:00:18
epoch [53/200] batch [25/64] time 0.393 (0.428) data 0.261 (0.297) loss_u loss_u 0.8755 (0.8921) acc_u 18.7500 (14.0000) lr 1.6959e-03 eta 0:00:16
epoch [53/200] batch [30/64] time 0.527 (0.432) data 0.396 (0.301) loss_u loss_u 0.9268 (0.8944) acc_u 6.2500 (13.7500) lr 1.6959e-03 eta 0:00:14
epoch [53/200] batch [35/64] time 0.450 (0.433) data 0.318 (0.302) loss_u loss_u 0.8501 (0.8867) acc_u 15.6250 (14.6429) lr 1.6959e-03 eta 0:00:12
epoch [53/200] batch [40/64] time 0.440 (0.432) data 0.309 (0.301) loss_u loss_u 0.8799 (0.8882) acc_u 12.5000 (14.6875) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [45/64] time 0.395 (0.432) data 0.264 (0.301) loss_u loss_u 0.8555 (0.8876) acc_u 15.6250 (14.5139) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [50/64] time 0.740 (0.437) data 0.606 (0.306) loss_u loss_u 0.9233 (0.8892) acc_u 9.3750 (14.1875) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [55/64] time 0.456 (0.441) data 0.325 (0.309) loss_u loss_u 0.8501 (0.8895) acc_u 18.7500 (14.1477) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [60/64] time 0.523 (0.442) data 0.386 (0.311) loss_u loss_u 0.7974 (0.8870) acc_u 25.0000 (14.4792) lr 1.6959e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1587
confident_label rate tensor(0.3323, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1042
clean true:1037
clean false:5
clean_rate:0.9952015355086372
noisy true:512
noisy false:1582
after delete: len(clean_dataset) 1042
after delete: len(noisy_dataset) 2094
epoch [54/200] batch [5/32] time 0.405 (0.416) data 0.274 (0.285) loss_x loss_x 0.9824 (1.2027) acc_x 65.6250 (64.3750) lr 1.6845e-03 eta 0:00:11
epoch [54/200] batch [10/32] time 0.387 (0.450) data 0.257 (0.319) loss_x loss_x 1.4463 (1.2543) acc_x 56.2500 (66.2500) lr 1.6845e-03 eta 0:00:09
epoch [54/200] batch [15/32] time 0.444 (0.454) data 0.313 (0.323) loss_x loss_x 1.0352 (1.2336) acc_x 75.0000 (68.1250) lr 1.6845e-03 eta 0:00:07
epoch [54/200] batch [20/32] time 0.521 (0.449) data 0.390 (0.318) loss_x loss_x 1.2842 (1.2605) acc_x 65.6250 (67.8125) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [25/32] time 0.393 (0.441) data 0.263 (0.310) loss_x loss_x 1.5762 (1.2789) acc_x 50.0000 (66.6250) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [30/32] time 0.514 (0.457) data 0.383 (0.326) loss_x loss_x 1.0947 (1.2352) acc_x 71.8750 (67.6042) lr 1.6845e-03 eta 0:00:00
epoch [54/200] batch [5/65] time 0.438 (0.459) data 0.307 (0.328) loss_u loss_u 0.8584 (0.8917) acc_u 18.7500 (14.3750) lr 1.6845e-03 eta 0:00:27
epoch [54/200] batch [10/65] time 0.435 (0.464) data 0.304 (0.333) loss_u loss_u 0.9019 (0.8782) acc_u 12.5000 (15.6250) lr 1.6845e-03 eta 0:00:25
epoch [54/200] batch [15/65] time 0.435 (0.461) data 0.305 (0.329) loss_u loss_u 0.8237 (0.8756) acc_u 25.0000 (16.2500) lr 1.6845e-03 eta 0:00:23
epoch [54/200] batch [20/65] time 0.425 (0.457) data 0.294 (0.326) loss_u loss_u 0.8740 (0.8854) acc_u 12.5000 (14.3750) lr 1.6845e-03 eta 0:00:20
epoch [54/200] batch [25/65] time 0.425 (0.459) data 0.295 (0.327) loss_u loss_u 0.9209 (0.8840) acc_u 9.3750 (14.2500) lr 1.6845e-03 eta 0:00:18
epoch [54/200] batch [30/65] time 0.348 (0.460) data 0.216 (0.328) loss_u loss_u 0.8896 (0.8867) acc_u 9.3750 (13.6458) lr 1.6845e-03 eta 0:00:16
epoch [54/200] batch [35/65] time 0.373 (0.457) data 0.241 (0.326) loss_u loss_u 0.9136 (0.8888) acc_u 6.2500 (13.1250) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [40/65] time 0.413 (0.453) data 0.282 (0.322) loss_u loss_u 0.9258 (0.8860) acc_u 12.5000 (13.9062) lr 1.6845e-03 eta 0:00:11
epoch [54/200] batch [45/65] time 0.347 (0.450) data 0.215 (0.319) loss_u loss_u 0.9102 (0.8847) acc_u 9.3750 (14.1667) lr 1.6845e-03 eta 0:00:09
epoch [54/200] batch [50/65] time 0.465 (0.448) data 0.333 (0.317) loss_u loss_u 0.9531 (0.8829) acc_u 3.1250 (14.3750) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [55/65] time 0.384 (0.442) data 0.253 (0.311) loss_u loss_u 0.8975 (0.8842) acc_u 18.7500 (14.3750) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [60/65] time 0.528 (0.443) data 0.397 (0.311) loss_u loss_u 0.9292 (0.8844) acc_u 9.3750 (14.3750) lr 1.6845e-03 eta 0:00:02
epoch [54/200] batch [65/65] time 0.430 (0.442) data 0.295 (0.310) loss_u loss_u 0.8945 (0.8846) acc_u 21.8750 (14.6635) lr 1.6845e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1582
confident_label rate tensor(0.3326, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1043
clean true:1039
clean false:4
clean_rate:0.9961649089165868
noisy true:515
noisy false:1578
after delete: len(clean_dataset) 1043
after delete: len(noisy_dataset) 2093
epoch [55/200] batch [5/32] time 0.522 (0.477) data 0.392 (0.347) loss_x loss_x 1.1943 (1.2605) acc_x 65.6250 (68.1250) lr 1.6730e-03 eta 0:00:12
epoch [55/200] batch [10/32] time 0.513 (0.462) data 0.383 (0.331) loss_x loss_x 0.9536 (1.2278) acc_x 71.8750 (69.3750) lr 1.6730e-03 eta 0:00:10
epoch [55/200] batch [15/32] time 0.445 (0.462) data 0.316 (0.332) loss_x loss_x 1.2803 (1.2578) acc_x 71.8750 (69.1667) lr 1.6730e-03 eta 0:00:07
epoch [55/200] batch [20/32] time 0.484 (0.462) data 0.353 (0.331) loss_x loss_x 1.5205 (1.2531) acc_x 62.5000 (69.3750) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [25/32] time 0.467 (0.468) data 0.336 (0.337) loss_x loss_x 1.0166 (1.2629) acc_x 81.2500 (69.6250) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [30/32] time 0.441 (0.466) data 0.310 (0.335) loss_x loss_x 1.6143 (1.2914) acc_x 62.5000 (68.3333) lr 1.6730e-03 eta 0:00:00
epoch [55/200] batch [5/65] time 0.391 (0.469) data 0.260 (0.338) loss_u loss_u 0.9214 (0.9104) acc_u 12.5000 (12.5000) lr 1.6730e-03 eta 0:00:28
epoch [55/200] batch [10/65] time 0.360 (0.472) data 0.229 (0.341) loss_u loss_u 0.8315 (0.8783) acc_u 21.8750 (15.9375) lr 1.6730e-03 eta 0:00:25
epoch [55/200] batch [15/65] time 0.429 (0.467) data 0.298 (0.336) loss_u loss_u 0.8818 (0.8772) acc_u 15.6250 (15.2083) lr 1.6730e-03 eta 0:00:23
epoch [55/200] batch [20/65] time 0.532 (0.465) data 0.400 (0.334) loss_u loss_u 0.8320 (0.8803) acc_u 25.0000 (15.4688) lr 1.6730e-03 eta 0:00:20
epoch [55/200] batch [25/65] time 0.339 (0.458) data 0.209 (0.327) loss_u loss_u 0.9434 (0.8838) acc_u 6.2500 (15.1250) lr 1.6730e-03 eta 0:00:18
epoch [55/200] batch [30/65] time 0.496 (0.455) data 0.365 (0.324) loss_u loss_u 0.8726 (0.8809) acc_u 18.7500 (15.4167) lr 1.6730e-03 eta 0:00:15
epoch [55/200] batch [35/65] time 0.417 (0.455) data 0.285 (0.324) loss_u loss_u 0.9033 (0.8797) acc_u 12.5000 (15.6250) lr 1.6730e-03 eta 0:00:13
epoch [55/200] batch [40/65] time 0.443 (0.457) data 0.312 (0.326) loss_u loss_u 0.8335 (0.8787) acc_u 18.7500 (15.6250) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [45/65] time 0.334 (0.456) data 0.203 (0.324) loss_u loss_u 0.9019 (0.8762) acc_u 15.6250 (15.8333) lr 1.6730e-03 eta 0:00:09
epoch [55/200] batch [50/65] time 0.335 (0.451) data 0.205 (0.320) loss_u loss_u 0.8672 (0.8770) acc_u 15.6250 (15.7500) lr 1.6730e-03 eta 0:00:06
epoch [55/200] batch [55/65] time 0.337 (0.449) data 0.206 (0.318) loss_u loss_u 0.8730 (0.8757) acc_u 18.7500 (15.8523) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [60/65] time 0.316 (0.446) data 0.185 (0.315) loss_u loss_u 0.9292 (0.8770) acc_u 6.2500 (15.5208) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [65/65] time 0.390 (0.444) data 0.259 (0.313) loss_u loss_u 0.8350 (0.8753) acc_u 25.0000 (15.8654) lr 1.6730e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1572
confident_label rate tensor(0.3355, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1052
clean true:1048
clean false:4
clean_rate:0.9961977186311787
noisy true:516
noisy false:1568
after delete: len(clean_dataset) 1052
after delete: len(noisy_dataset) 2084
epoch [56/200] batch [5/32] time 0.482 (0.417) data 0.351 (0.287) loss_x loss_x 1.9111 (1.4297) acc_x 53.1250 (63.1250) lr 1.6613e-03 eta 0:00:11
epoch [56/200] batch [10/32] time 0.425 (0.434) data 0.294 (0.304) loss_x loss_x 1.0410 (1.3527) acc_x 75.0000 (65.0000) lr 1.6613e-03 eta 0:00:09
epoch [56/200] batch [15/32] time 0.396 (0.442) data 0.265 (0.312) loss_x loss_x 1.0078 (1.3106) acc_x 78.1250 (66.6667) lr 1.6613e-03 eta 0:00:07
epoch [56/200] batch [20/32] time 0.493 (0.455) data 0.362 (0.324) loss_x loss_x 1.0459 (1.2498) acc_x 71.8750 (67.9688) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [25/32] time 0.343 (0.449) data 0.212 (0.318) loss_x loss_x 1.0586 (1.2859) acc_x 78.1250 (67.7500) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [30/32] time 0.513 (0.469) data 0.382 (0.338) loss_x loss_x 1.0713 (1.2593) acc_x 84.3750 (68.9583) lr 1.6613e-03 eta 0:00:00
epoch [56/200] batch [5/65] time 0.494 (0.478) data 0.362 (0.346) loss_u loss_u 0.8652 (0.8612) acc_u 18.7500 (18.7500) lr 1.6613e-03 eta 0:00:28
epoch [56/200] batch [10/65] time 0.349 (0.473) data 0.218 (0.341) loss_u loss_u 0.8672 (0.8678) acc_u 15.6250 (17.1875) lr 1.6613e-03 eta 0:00:25
epoch [56/200] batch [15/65] time 0.376 (0.473) data 0.244 (0.341) loss_u loss_u 0.8433 (0.8658) acc_u 28.1250 (17.9167) lr 1.6613e-03 eta 0:00:23
epoch [56/200] batch [20/65] time 0.369 (0.468) data 0.238 (0.336) loss_u loss_u 0.8647 (0.8756) acc_u 18.7500 (17.3438) lr 1.6613e-03 eta 0:00:21
epoch [56/200] batch [25/65] time 0.512 (0.470) data 0.381 (0.338) loss_u loss_u 0.9028 (0.8762) acc_u 15.6250 (17.5000) lr 1.6613e-03 eta 0:00:18
epoch [56/200] batch [30/65] time 0.394 (0.465) data 0.262 (0.333) loss_u loss_u 0.8491 (0.8757) acc_u 18.7500 (16.9792) lr 1.6613e-03 eta 0:00:16
epoch [56/200] batch [35/65] time 0.395 (0.461) data 0.263 (0.329) loss_u loss_u 0.9072 (0.8724) acc_u 9.3750 (17.3214) lr 1.6613e-03 eta 0:00:13
epoch [56/200] batch [40/65] time 0.442 (0.458) data 0.311 (0.327) loss_u loss_u 0.9077 (0.8770) acc_u 18.7500 (16.9531) lr 1.6613e-03 eta 0:00:11
epoch [56/200] batch [45/65] time 0.396 (0.456) data 0.266 (0.325) loss_u loss_u 0.8384 (0.8741) acc_u 21.8750 (17.2917) lr 1.6613e-03 eta 0:00:09
epoch [56/200] batch [50/65] time 0.363 (0.456) data 0.233 (0.325) loss_u loss_u 0.8818 (0.8760) acc_u 15.6250 (16.8125) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [55/65] time 0.410 (0.456) data 0.279 (0.324) loss_u loss_u 0.8911 (0.8765) acc_u 15.6250 (16.8750) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [60/65] time 0.359 (0.451) data 0.229 (0.319) loss_u loss_u 0.8770 (0.8755) acc_u 15.6250 (16.8750) lr 1.6613e-03 eta 0:00:02
epoch [56/200] batch [65/65] time 0.412 (0.447) data 0.281 (0.316) loss_u loss_u 0.8994 (0.8771) acc_u 15.6250 (16.7308) lr 1.6613e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1537
confident_label rate tensor(0.3457, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1084
clean true:1080
clean false:4
clean_rate:0.996309963099631
noisy true:519
noisy false:1533
after delete: len(clean_dataset) 1084
after delete: len(noisy_dataset) 2052
epoch [57/200] batch [5/33] time 0.413 (0.464) data 0.282 (0.333) loss_x loss_x 0.9351 (1.2172) acc_x 78.1250 (71.2500) lr 1.6494e-03 eta 0:00:12
epoch [57/200] batch [10/33] time 0.435 (0.464) data 0.304 (0.333) loss_x loss_x 1.4551 (1.3080) acc_x 68.7500 (70.9375) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [15/33] time 0.380 (0.458) data 0.249 (0.327) loss_x loss_x 1.6543 (1.3141) acc_x 62.5000 (71.8750) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [20/33] time 0.406 (0.448) data 0.275 (0.317) loss_x loss_x 1.2598 (1.2718) acc_x 59.3750 (71.4062) lr 1.6494e-03 eta 0:00:05
epoch [57/200] batch [25/33] time 0.519 (0.453) data 0.388 (0.322) loss_x loss_x 1.3105 (1.2579) acc_x 68.7500 (71.7500) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [30/33] time 0.498 (0.458) data 0.367 (0.327) loss_x loss_x 0.8794 (1.2633) acc_x 75.0000 (70.9375) lr 1.6494e-03 eta 0:00:01
epoch [57/200] batch [5/64] time 0.402 (0.457) data 0.271 (0.326) loss_u loss_u 0.8887 (0.8853) acc_u 15.6250 (13.7500) lr 1.6494e-03 eta 0:00:26
epoch [57/200] batch [10/64] time 0.415 (0.451) data 0.283 (0.320) loss_u loss_u 0.8384 (0.8870) acc_u 25.0000 (14.3750) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [15/64] time 0.378 (0.444) data 0.247 (0.313) loss_u loss_u 0.8730 (0.8847) acc_u 18.7500 (14.3750) lr 1.6494e-03 eta 0:00:21
epoch [57/200] batch [20/64] time 0.329 (0.439) data 0.197 (0.307) loss_u loss_u 0.9004 (0.8881) acc_u 12.5000 (14.2188) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [25/64] time 0.557 (0.441) data 0.425 (0.310) loss_u loss_u 0.9014 (0.8920) acc_u 15.6250 (14.0000) lr 1.6494e-03 eta 0:00:17
epoch [57/200] batch [30/64] time 0.338 (0.439) data 0.206 (0.307) loss_u loss_u 0.8301 (0.8884) acc_u 18.7500 (14.4792) lr 1.6494e-03 eta 0:00:14
epoch [57/200] batch [35/64] time 0.480 (0.437) data 0.348 (0.306) loss_u loss_u 0.8950 (0.8876) acc_u 15.6250 (14.6429) lr 1.6494e-03 eta 0:00:12
epoch [57/200] batch [40/64] time 0.436 (0.442) data 0.302 (0.311) loss_u loss_u 0.8564 (0.8849) acc_u 18.7500 (14.6875) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [45/64] time 0.487 (0.443) data 0.355 (0.311) loss_u loss_u 0.8198 (0.8839) acc_u 21.8750 (14.5139) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [50/64] time 0.373 (0.441) data 0.242 (0.310) loss_u loss_u 0.9209 (0.8829) acc_u 15.6250 (14.8125) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [55/64] time 0.672 (0.443) data 0.540 (0.312) loss_u loss_u 0.9111 (0.8823) acc_u 9.3750 (15.0568) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [60/64] time 0.316 (0.442) data 0.185 (0.311) loss_u loss_u 0.8882 (0.8831) acc_u 9.3750 (14.7396) lr 1.6494e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1548
confident_label rate tensor(0.3412, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1070
clean true:1065
clean false:5
clean_rate:0.9953271028037384
noisy true:523
noisy false:1543
after delete: len(clean_dataset) 1070
after delete: len(noisy_dataset) 2066
epoch [58/200] batch [5/33] time 0.438 (0.462) data 0.308 (0.332) loss_x loss_x 1.1191 (1.2038) acc_x 78.1250 (68.1250) lr 1.6374e-03 eta 0:00:12
epoch [58/200] batch [10/33] time 0.445 (0.468) data 0.315 (0.338) loss_x loss_x 1.1680 (1.1612) acc_x 78.1250 (70.3125) lr 1.6374e-03 eta 0:00:10
epoch [58/200] batch [15/33] time 0.408 (0.449) data 0.277 (0.319) loss_x loss_x 1.4062 (1.2476) acc_x 71.8750 (69.7917) lr 1.6374e-03 eta 0:00:08
epoch [58/200] batch [20/33] time 0.416 (0.458) data 0.285 (0.328) loss_x loss_x 1.6816 (1.2391) acc_x 65.6250 (69.3750) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [25/33] time 0.439 (0.456) data 0.308 (0.326) loss_x loss_x 1.0879 (1.2611) acc_x 75.0000 (69.2500) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [30/33] time 0.672 (0.473) data 0.541 (0.342) loss_x loss_x 1.3730 (1.2376) acc_x 71.8750 (69.7917) lr 1.6374e-03 eta 0:00:01
epoch [58/200] batch [5/64] time 0.472 (0.463) data 0.341 (0.332) loss_u loss_u 0.8115 (0.8826) acc_u 21.8750 (15.6250) lr 1.6374e-03 eta 0:00:27
epoch [58/200] batch [10/64] time 0.390 (0.451) data 0.258 (0.320) loss_u loss_u 0.8545 (0.8684) acc_u 21.8750 (17.8125) lr 1.6374e-03 eta 0:00:24
epoch [58/200] batch [15/64] time 0.535 (0.448) data 0.404 (0.317) loss_u loss_u 0.7871 (0.8683) acc_u 25.0000 (17.9167) lr 1.6374e-03 eta 0:00:21
epoch [58/200] batch [20/64] time 0.388 (0.449) data 0.257 (0.318) loss_u loss_u 0.8950 (0.8761) acc_u 12.5000 (16.4062) lr 1.6374e-03 eta 0:00:19
epoch [58/200] batch [25/64] time 0.414 (0.448) data 0.282 (0.317) loss_u loss_u 0.8281 (0.8680) acc_u 21.8750 (17.6250) lr 1.6374e-03 eta 0:00:17
epoch [58/200] batch [30/64] time 0.424 (0.447) data 0.293 (0.316) loss_u loss_u 0.8882 (0.8727) acc_u 15.6250 (16.8750) lr 1.6374e-03 eta 0:00:15
epoch [58/200] batch [35/64] time 0.402 (0.446) data 0.270 (0.315) loss_u loss_u 0.8813 (0.8732) acc_u 15.6250 (16.8750) lr 1.6374e-03 eta 0:00:12
epoch [58/200] batch [40/64] time 0.395 (0.443) data 0.263 (0.312) loss_u loss_u 0.9497 (0.8777) acc_u 6.2500 (16.0938) lr 1.6374e-03 eta 0:00:10
epoch [58/200] batch [45/64] time 0.372 (0.439) data 0.240 (0.308) loss_u loss_u 0.8789 (0.8788) acc_u 18.7500 (16.2500) lr 1.6374e-03 eta 0:00:08
epoch [58/200] batch [50/64] time 0.336 (0.438) data 0.204 (0.307) loss_u loss_u 0.8579 (0.8798) acc_u 18.7500 (16.0625) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [55/64] time 0.435 (0.439) data 0.304 (0.308) loss_u loss_u 0.8477 (0.8804) acc_u 28.1250 (16.1932) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [60/64] time 0.354 (0.438) data 0.223 (0.307) loss_u loss_u 0.9043 (0.8786) acc_u 12.5000 (16.3542) lr 1.6374e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1523
confident_label rate tensor(0.3476, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1090
clean true:1083
clean false:7
clean_rate:0.9935779816513761
noisy true:530
noisy false:1516
after delete: len(clean_dataset) 1090
after delete: len(noisy_dataset) 2046
epoch [59/200] batch [5/34] time 0.507 (0.486) data 0.376 (0.355) loss_x loss_x 1.2197 (1.1336) acc_x 68.7500 (75.0000) lr 1.6252e-03 eta 0:00:14
epoch [59/200] batch [10/34] time 0.453 (0.464) data 0.323 (0.333) loss_x loss_x 1.4502 (1.2576) acc_x 59.3750 (68.4375) lr 1.6252e-03 eta 0:00:11
epoch [59/200] batch [15/34] time 0.502 (0.451) data 0.371 (0.321) loss_x loss_x 1.8057 (1.3155) acc_x 50.0000 (66.2500) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [20/34] time 0.400 (0.451) data 0.270 (0.321) loss_x loss_x 1.6631 (1.2664) acc_x 59.3750 (66.8750) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [25/34] time 0.417 (0.444) data 0.286 (0.313) loss_x loss_x 1.3184 (1.2629) acc_x 75.0000 (67.2500) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [30/34] time 0.436 (0.443) data 0.306 (0.313) loss_x loss_x 1.2881 (1.2626) acc_x 62.5000 (67.1875) lr 1.6252e-03 eta 0:00:01
epoch [59/200] batch [5/63] time 0.336 (0.444) data 0.204 (0.313) loss_u loss_u 0.8701 (0.8734) acc_u 15.6250 (15.0000) lr 1.6252e-03 eta 0:00:25
epoch [59/200] batch [10/63] time 0.540 (0.447) data 0.409 (0.316) loss_u loss_u 0.8809 (0.8840) acc_u 15.6250 (13.7500) lr 1.6252e-03 eta 0:00:23
epoch [59/200] batch [15/63] time 0.378 (0.442) data 0.246 (0.311) loss_u loss_u 0.8921 (0.8841) acc_u 15.6250 (15.0000) lr 1.6252e-03 eta 0:00:21
epoch [59/200] batch [20/63] time 0.431 (0.442) data 0.299 (0.311) loss_u loss_u 0.9209 (0.8880) acc_u 9.3750 (14.0625) lr 1.6252e-03 eta 0:00:18
epoch [59/200] batch [25/63] time 0.478 (0.439) data 0.347 (0.307) loss_u loss_u 0.9541 (0.8929) acc_u 9.3750 (14.0000) lr 1.6252e-03 eta 0:00:16
epoch [59/200] batch [30/63] time 0.456 (0.442) data 0.324 (0.310) loss_u loss_u 0.9253 (0.8950) acc_u 9.3750 (13.4375) lr 1.6252e-03 eta 0:00:14
epoch [59/200] batch [35/63] time 0.677 (0.444) data 0.547 (0.313) loss_u loss_u 0.9292 (0.8910) acc_u 6.2500 (14.1071) lr 1.6252e-03 eta 0:00:12
epoch [59/200] batch [40/63] time 0.439 (0.445) data 0.307 (0.314) loss_u loss_u 0.8413 (0.8880) acc_u 18.7500 (14.6094) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [45/63] time 0.364 (0.444) data 0.233 (0.312) loss_u loss_u 0.9233 (0.8891) acc_u 6.2500 (14.3056) lr 1.6252e-03 eta 0:00:07
epoch [59/200] batch [50/63] time 0.499 (0.444) data 0.368 (0.313) loss_u loss_u 0.8247 (0.8883) acc_u 25.0000 (14.3750) lr 1.6252e-03 eta 0:00:05
epoch [59/200] batch [55/63] time 0.330 (0.441) data 0.199 (0.310) loss_u loss_u 0.9111 (0.8866) acc_u 15.6250 (14.8295) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [60/63] time 0.440 (0.442) data 0.309 (0.311) loss_u loss_u 0.7822 (0.8854) acc_u 25.0000 (15.0000) lr 1.6252e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1536
confident_label rate tensor(0.3431, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1076
clean true:1071
clean false:5
clean_rate:0.9953531598513011
noisy true:529
noisy false:1531
after delete: len(clean_dataset) 1076
after delete: len(noisy_dataset) 2060
epoch [60/200] batch [5/33] time 0.341 (0.465) data 0.210 (0.334) loss_x loss_x 0.9966 (1.1805) acc_x 75.0000 (71.2500) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [10/33] time 0.557 (0.477) data 0.425 (0.346) loss_x loss_x 1.1338 (1.2209) acc_x 75.0000 (69.0625) lr 1.6129e-03 eta 0:00:10
epoch [60/200] batch [15/33] time 0.457 (0.477) data 0.327 (0.346) loss_x loss_x 1.5732 (1.3070) acc_x 62.5000 (67.5000) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [20/33] time 0.500 (0.477) data 0.368 (0.346) loss_x loss_x 1.1943 (1.2952) acc_x 62.5000 (67.8125) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [25/33] time 0.583 (0.479) data 0.452 (0.348) loss_x loss_x 1.2998 (1.2877) acc_x 59.3750 (66.7500) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [30/33] time 0.441 (0.469) data 0.311 (0.338) loss_x loss_x 2.0195 (1.3227) acc_x 50.0000 (66.0417) lr 1.6129e-03 eta 0:00:01
epoch [60/200] batch [5/64] time 0.419 (0.461) data 0.288 (0.330) loss_u loss_u 0.8545 (0.8632) acc_u 15.6250 (18.1250) lr 1.6129e-03 eta 0:00:27
epoch [60/200] batch [10/64] time 0.405 (0.455) data 0.273 (0.323) loss_u loss_u 0.9194 (0.8791) acc_u 12.5000 (15.3125) lr 1.6129e-03 eta 0:00:24
epoch [60/200] batch [15/64] time 0.512 (0.452) data 0.381 (0.321) loss_u loss_u 0.9014 (0.8906) acc_u 12.5000 (14.1667) lr 1.6129e-03 eta 0:00:22
epoch [60/200] batch [20/64] time 0.364 (0.447) data 0.233 (0.316) loss_u loss_u 0.8335 (0.8813) acc_u 21.8750 (15.7812) lr 1.6129e-03 eta 0:00:19
epoch [60/200] batch [25/64] time 0.412 (0.447) data 0.280 (0.316) loss_u loss_u 0.8960 (0.8860) acc_u 15.6250 (15.2500) lr 1.6129e-03 eta 0:00:17
epoch [60/200] batch [30/64] time 0.376 (0.443) data 0.245 (0.311) loss_u loss_u 0.8936 (0.8847) acc_u 12.5000 (15.1042) lr 1.6129e-03 eta 0:00:15
epoch [60/200] batch [35/64] time 0.400 (0.442) data 0.268 (0.310) loss_u loss_u 0.8403 (0.8823) acc_u 18.7500 (15.3571) lr 1.6129e-03 eta 0:00:12
epoch [60/200] batch [40/64] time 0.352 (0.441) data 0.221 (0.310) loss_u loss_u 0.9180 (0.8803) acc_u 9.3750 (15.7031) lr 1.6129e-03 eta 0:00:10
epoch [60/200] batch [45/64] time 0.695 (0.441) data 0.561 (0.310) loss_u loss_u 0.9478 (0.8814) acc_u 9.3750 (15.4861) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [50/64] time 0.367 (0.441) data 0.236 (0.310) loss_u loss_u 0.9253 (0.8825) acc_u 6.2500 (15.3125) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [55/64] time 0.482 (0.446) data 0.350 (0.315) loss_u loss_u 0.9014 (0.8856) acc_u 18.7500 (14.8295) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [60/64] time 0.471 (0.449) data 0.340 (0.317) loss_u loss_u 0.8857 (0.8822) acc_u 9.3750 (15.1562) lr 1.6129e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1493
confident_label rate tensor(0.3565, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1118
clean true:1112
clean false:6
clean_rate:0.9946332737030411
noisy true:531
noisy false:1487
after delete: len(clean_dataset) 1118
after delete: len(noisy_dataset) 2018
epoch [61/200] batch [5/34] time 0.377 (0.418) data 0.247 (0.287) loss_x loss_x 1.8838 (1.5717) acc_x 56.2500 (63.1250) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [10/34] time 0.442 (0.418) data 0.311 (0.288) loss_x loss_x 1.4980 (1.5384) acc_x 65.6250 (64.0625) lr 1.6004e-03 eta 0:00:10
epoch [61/200] batch [15/34] time 0.565 (0.429) data 0.434 (0.299) loss_x loss_x 0.9805 (1.4691) acc_x 75.0000 (64.1667) lr 1.6004e-03 eta 0:00:08
epoch [61/200] batch [20/34] time 0.498 (0.435) data 0.367 (0.305) loss_x loss_x 0.6714 (1.3821) acc_x 87.5000 (65.3125) lr 1.6004e-03 eta 0:00:06
epoch [61/200] batch [25/34] time 0.376 (0.440) data 0.245 (0.309) loss_x loss_x 1.4893 (1.3810) acc_x 68.7500 (65.8750) lr 1.6004e-03 eta 0:00:03
epoch [61/200] batch [30/34] time 0.413 (0.449) data 0.282 (0.318) loss_x loss_x 1.4072 (1.3841) acc_x 65.6250 (66.1458) lr 1.6004e-03 eta 0:00:01
epoch [61/200] batch [5/63] time 0.703 (0.449) data 0.571 (0.318) loss_u loss_u 0.8330 (0.8638) acc_u 28.1250 (18.1250) lr 1.6004e-03 eta 0:00:26
epoch [61/200] batch [10/63] time 0.580 (0.451) data 0.446 (0.320) loss_u loss_u 0.8481 (0.8681) acc_u 21.8750 (17.8125) lr 1.6004e-03 eta 0:00:23
epoch [61/200] batch [15/63] time 0.387 (0.452) data 0.252 (0.320) loss_u loss_u 0.8745 (0.8746) acc_u 12.5000 (16.8750) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [20/63] time 0.475 (0.454) data 0.344 (0.322) loss_u loss_u 0.9561 (0.8798) acc_u 6.2500 (16.0938) lr 1.6004e-03 eta 0:00:19
epoch [61/200] batch [25/63] time 0.368 (0.451) data 0.237 (0.319) loss_u loss_u 0.8960 (0.8817) acc_u 9.3750 (15.5000) lr 1.6004e-03 eta 0:00:17
epoch [61/200] batch [30/63] time 0.537 (0.452) data 0.406 (0.320) loss_u loss_u 0.8711 (0.8854) acc_u 25.0000 (15.2083) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [35/63] time 0.355 (0.450) data 0.224 (0.318) loss_u loss_u 0.9277 (0.8818) acc_u 9.3750 (15.3571) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [40/63] time 0.604 (0.450) data 0.472 (0.319) loss_u loss_u 0.9512 (0.8843) acc_u 3.1250 (14.8438) lr 1.6004e-03 eta 0:00:10
epoch [61/200] batch [45/63] time 0.343 (0.447) data 0.212 (0.315) loss_u loss_u 0.8379 (0.8812) acc_u 18.7500 (15.2778) lr 1.6004e-03 eta 0:00:08
epoch [61/200] batch [50/63] time 0.712 (0.449) data 0.580 (0.318) loss_u loss_u 0.9097 (0.8808) acc_u 12.5000 (15.3125) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [55/63] time 0.358 (0.447) data 0.227 (0.316) loss_u loss_u 0.8745 (0.8801) acc_u 12.5000 (15.3409) lr 1.6004e-03 eta 0:00:03
epoch [61/200] batch [60/63] time 0.425 (0.446) data 0.293 (0.314) loss_u loss_u 0.9551 (0.8857) acc_u 6.2500 (14.6875) lr 1.6004e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1526
confident_label rate tensor(0.3364, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1055
clean true:1051
clean false:4
clean_rate:0.9962085308056872
noisy true:559
noisy false:1522
after delete: len(clean_dataset) 1055
after delete: len(noisy_dataset) 2081
epoch [62/200] batch [5/32] time 0.474 (0.447) data 0.344 (0.317) loss_x loss_x 1.4727 (1.2288) acc_x 62.5000 (68.1250) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [10/32] time 0.464 (0.462) data 0.333 (0.332) loss_x loss_x 0.9731 (1.1768) acc_x 75.0000 (70.0000) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [15/32] time 0.449 (0.473) data 0.315 (0.342) loss_x loss_x 1.1689 (1.1892) acc_x 78.1250 (70.4167) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [20/32] time 0.421 (0.467) data 0.290 (0.336) loss_x loss_x 0.9624 (1.1961) acc_x 68.7500 (70.6250) lr 1.5878e-03 eta 0:00:05
epoch [62/200] batch [25/32] time 0.541 (0.470) data 0.410 (0.339) loss_x loss_x 1.7637 (1.2221) acc_x 43.7500 (69.6250) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [30/32] time 0.455 (0.466) data 0.324 (0.336) loss_x loss_x 0.8599 (1.1663) acc_x 68.7500 (70.8333) lr 1.5878e-03 eta 0:00:00
epoch [62/200] batch [5/65] time 0.401 (0.461) data 0.269 (0.330) loss_u loss_u 0.8740 (0.9025) acc_u 9.3750 (8.1250) lr 1.5878e-03 eta 0:00:27
epoch [62/200] batch [10/65] time 0.503 (0.465) data 0.373 (0.334) loss_u loss_u 0.8779 (0.8740) acc_u 18.7500 (14.0625) lr 1.5878e-03 eta 0:00:25
epoch [62/200] batch [15/65] time 0.433 (0.463) data 0.301 (0.332) loss_u loss_u 0.9136 (0.8725) acc_u 9.3750 (15.2083) lr 1.5878e-03 eta 0:00:23
epoch [62/200] batch [20/65] time 0.422 (0.460) data 0.290 (0.330) loss_u loss_u 0.8765 (0.8703) acc_u 12.5000 (15.7812) lr 1.5878e-03 eta 0:00:20
epoch [62/200] batch [25/65] time 0.381 (0.460) data 0.251 (0.329) loss_u loss_u 0.8999 (0.8754) acc_u 15.6250 (15.2500) lr 1.5878e-03 eta 0:00:18
epoch [62/200] batch [30/65] time 0.366 (0.454) data 0.235 (0.323) loss_u loss_u 0.8926 (0.8752) acc_u 15.6250 (15.2083) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [35/65] time 0.352 (0.448) data 0.221 (0.317) loss_u loss_u 0.8315 (0.8765) acc_u 21.8750 (15.1786) lr 1.5878e-03 eta 0:00:13
epoch [62/200] batch [40/65] time 0.473 (0.447) data 0.341 (0.316) loss_u loss_u 0.8389 (0.8756) acc_u 18.7500 (15.3906) lr 1.5878e-03 eta 0:00:11
epoch [62/200] batch [45/65] time 0.416 (0.446) data 0.285 (0.315) loss_u loss_u 0.8198 (0.8744) acc_u 25.0000 (15.4861) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [50/65] time 0.454 (0.444) data 0.322 (0.313) loss_u loss_u 0.8701 (0.8753) acc_u 18.7500 (15.4375) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [55/65] time 0.444 (0.444) data 0.312 (0.313) loss_u loss_u 0.8584 (0.8748) acc_u 18.7500 (15.5682) lr 1.5878e-03 eta 0:00:04
epoch [62/200] batch [60/65] time 0.534 (0.446) data 0.402 (0.315) loss_u loss_u 0.8916 (0.8754) acc_u 12.5000 (15.5208) lr 1.5878e-03 eta 0:00:02
epoch [62/200] batch [65/65] time 0.540 (0.447) data 0.408 (0.316) loss_u loss_u 0.8613 (0.8732) acc_u 18.7500 (15.7692) lr 1.5878e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1533
confident_label rate tensor(0.3450, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1082
clean true:1076
clean false:6
clean_rate:0.9944547134935305
noisy true:527
noisy false:1527
after delete: len(clean_dataset) 1082
after delete: len(noisy_dataset) 2054
epoch [63/200] batch [5/33] time 0.463 (0.506) data 0.332 (0.375) loss_x loss_x 1.1836 (1.1021) acc_x 65.6250 (72.5000) lr 1.5750e-03 eta 0:00:14
epoch [63/200] batch [10/33] time 0.564 (0.527) data 0.434 (0.395) loss_x loss_x 1.0137 (1.2109) acc_x 68.7500 (71.5625) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [15/33] time 0.385 (0.508) data 0.253 (0.376) loss_x loss_x 0.9458 (1.1608) acc_x 71.8750 (71.2500) lr 1.5750e-03 eta 0:00:09
epoch [63/200] batch [20/33] time 0.385 (0.493) data 0.254 (0.361) loss_x loss_x 1.1318 (1.1771) acc_x 68.7500 (69.2188) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [25/33] time 0.450 (0.476) data 0.319 (0.345) loss_x loss_x 0.9790 (1.1830) acc_x 78.1250 (69.6250) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [30/33] time 0.359 (0.468) data 0.228 (0.337) loss_x loss_x 0.9624 (1.1735) acc_x 78.1250 (70.4167) lr 1.5750e-03 eta 0:00:01
epoch [63/200] batch [5/64] time 0.374 (0.470) data 0.242 (0.339) loss_u loss_u 0.8203 (0.8805) acc_u 34.3750 (18.1250) lr 1.5750e-03 eta 0:00:27
epoch [63/200] batch [10/64] time 0.369 (0.460) data 0.238 (0.328) loss_u loss_u 0.8584 (0.8774) acc_u 18.7500 (16.2500) lr 1.5750e-03 eta 0:00:24
epoch [63/200] batch [15/64] time 0.418 (0.453) data 0.287 (0.322) loss_u loss_u 0.9097 (0.8924) acc_u 9.3750 (14.1667) lr 1.5750e-03 eta 0:00:22
epoch [63/200] batch [20/64] time 0.413 (0.453) data 0.282 (0.322) loss_u loss_u 0.9194 (0.8896) acc_u 9.3750 (14.2188) lr 1.5750e-03 eta 0:00:19
epoch [63/200] batch [25/64] time 0.368 (0.449) data 0.237 (0.318) loss_u loss_u 0.8867 (0.8856) acc_u 15.6250 (15.1250) lr 1.5750e-03 eta 0:00:17
epoch [63/200] batch [30/64] time 0.554 (0.449) data 0.423 (0.318) loss_u loss_u 0.8994 (0.8866) acc_u 9.3750 (15.1042) lr 1.5750e-03 eta 0:00:15
epoch [63/200] batch [35/64] time 0.382 (0.449) data 0.251 (0.318) loss_u loss_u 0.8477 (0.8856) acc_u 18.7500 (15.4464) lr 1.5750e-03 eta 0:00:13
epoch [63/200] batch [40/64] time 0.398 (0.449) data 0.266 (0.318) loss_u loss_u 0.8750 (0.8845) acc_u 18.7500 (15.3125) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [45/64] time 0.519 (0.449) data 0.388 (0.318) loss_u loss_u 0.8418 (0.8846) acc_u 21.8750 (15.1389) lr 1.5750e-03 eta 0:00:08
epoch [63/200] batch [50/64] time 0.507 (0.450) data 0.375 (0.318) loss_u loss_u 0.9102 (0.8864) acc_u 12.5000 (15.0625) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [55/64] time 0.422 (0.449) data 0.290 (0.318) loss_u loss_u 0.7778 (0.8823) acc_u 28.1250 (15.4545) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [60/64] time 0.375 (0.447) data 0.243 (0.315) loss_u loss_u 0.8579 (0.8817) acc_u 21.8750 (15.6250) lr 1.5750e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1513
confident_label rate tensor(0.3530, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1107
clean true:1099
clean false:8
clean_rate:0.992773261065944
noisy true:524
noisy false:1505
after delete: len(clean_dataset) 1107
after delete: len(noisy_dataset) 2029
epoch [64/200] batch [5/34] time 0.453 (0.466) data 0.322 (0.335) loss_x loss_x 0.6646 (1.0546) acc_x 87.5000 (73.7500) lr 1.5621e-03 eta 0:00:13
epoch [64/200] batch [10/34] time 0.516 (0.479) data 0.385 (0.348) loss_x loss_x 1.5088 (1.2014) acc_x 62.5000 (69.6875) lr 1.5621e-03 eta 0:00:11
epoch [64/200] batch [15/34] time 0.486 (0.487) data 0.356 (0.356) loss_x loss_x 1.3086 (1.2143) acc_x 71.8750 (70.2083) lr 1.5621e-03 eta 0:00:09
epoch [64/200] batch [20/34] time 0.357 (0.470) data 0.227 (0.340) loss_x loss_x 1.3623 (1.2909) acc_x 62.5000 (67.8125) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [25/34] time 0.354 (0.452) data 0.224 (0.322) loss_x loss_x 0.7139 (1.2327) acc_x 84.3750 (69.3750) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [30/34] time 0.615 (0.455) data 0.485 (0.325) loss_x loss_x 1.0381 (1.2259) acc_x 75.0000 (69.7917) lr 1.5621e-03 eta 0:00:01
epoch [64/200] batch [5/63] time 0.466 (0.445) data 0.335 (0.315) loss_u loss_u 0.9082 (0.9204) acc_u 18.7500 (11.8750) lr 1.5621e-03 eta 0:00:25
epoch [64/200] batch [10/63] time 0.529 (0.446) data 0.398 (0.315) loss_u loss_u 0.8159 (0.8929) acc_u 21.8750 (14.6875) lr 1.5621e-03 eta 0:00:23
epoch [64/200] batch [15/63] time 0.344 (0.442) data 0.211 (0.312) loss_u loss_u 0.9141 (0.8847) acc_u 12.5000 (16.4583) lr 1.5621e-03 eta 0:00:21
epoch [64/200] batch [20/63] time 0.483 (0.444) data 0.351 (0.313) loss_u loss_u 0.8721 (0.8851) acc_u 9.3750 (15.7812) lr 1.5621e-03 eta 0:00:19
epoch [64/200] batch [25/63] time 0.355 (0.446) data 0.223 (0.316) loss_u loss_u 0.8296 (0.8837) acc_u 18.7500 (15.6250) lr 1.5621e-03 eta 0:00:16
epoch [64/200] batch [30/63] time 0.416 (0.445) data 0.284 (0.314) loss_u loss_u 0.8594 (0.8813) acc_u 15.6250 (15.7292) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [35/63] time 0.338 (0.443) data 0.206 (0.313) loss_u loss_u 0.9136 (0.8832) acc_u 9.3750 (15.2679) lr 1.5621e-03 eta 0:00:12
epoch [64/200] batch [40/63] time 0.369 (0.441) data 0.238 (0.310) loss_u loss_u 0.9160 (0.8865) acc_u 9.3750 (14.8438) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [45/63] time 0.631 (0.442) data 0.500 (0.311) loss_u loss_u 0.9214 (0.8836) acc_u 12.5000 (15.1389) lr 1.5621e-03 eta 0:00:07
epoch [64/200] batch [50/63] time 0.373 (0.441) data 0.242 (0.310) loss_u loss_u 0.8643 (0.8835) acc_u 18.7500 (15.0000) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [55/63] time 0.319 (0.441) data 0.188 (0.310) loss_u loss_u 0.8784 (0.8833) acc_u 21.8750 (15.2841) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [60/63] time 0.418 (0.441) data 0.286 (0.310) loss_u loss_u 0.9126 (0.8815) acc_u 9.3750 (15.5208) lr 1.5621e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1511
confident_label rate tensor(0.3460, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1085
clean true:1082
clean false:3
clean_rate:0.9972350230414746
noisy true:543
noisy false:1508
after delete: len(clean_dataset) 1085
after delete: len(noisy_dataset) 2051
epoch [65/200] batch [5/33] time 0.537 (0.458) data 0.406 (0.326) loss_x loss_x 1.6270 (1.2671) acc_x 62.5000 (67.5000) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [10/33] time 0.517 (0.466) data 0.386 (0.334) loss_x loss_x 1.1934 (1.3148) acc_x 78.1250 (66.5625) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [15/33] time 0.524 (0.457) data 0.393 (0.326) loss_x loss_x 1.7363 (1.3001) acc_x 53.1250 (66.2500) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [20/33] time 0.429 (0.451) data 0.299 (0.320) loss_x loss_x 1.2422 (1.3313) acc_x 71.8750 (66.5625) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [25/33] time 0.544 (0.462) data 0.414 (0.331) loss_x loss_x 0.8721 (1.3246) acc_x 81.2500 (66.8750) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [30/33] time 0.403 (0.460) data 0.273 (0.329) loss_x loss_x 1.7646 (1.3557) acc_x 65.6250 (66.4583) lr 1.5490e-03 eta 0:00:01
epoch [65/200] batch [5/64] time 0.365 (0.453) data 0.234 (0.322) loss_u loss_u 0.9009 (0.8812) acc_u 9.3750 (15.6250) lr 1.5490e-03 eta 0:00:26
epoch [65/200] batch [10/64] time 0.352 (0.452) data 0.222 (0.321) loss_u loss_u 0.8672 (0.8903) acc_u 18.7500 (14.3750) lr 1.5490e-03 eta 0:00:24
epoch [65/200] batch [15/64] time 0.351 (0.449) data 0.219 (0.318) loss_u loss_u 0.8154 (0.8910) acc_u 15.6250 (12.2917) lr 1.5490e-03 eta 0:00:21
epoch [65/200] batch [20/64] time 0.493 (0.453) data 0.361 (0.322) loss_u loss_u 0.8975 (0.8839) acc_u 9.3750 (13.5938) lr 1.5490e-03 eta 0:00:19
epoch [65/200] batch [25/64] time 0.353 (0.449) data 0.221 (0.318) loss_u loss_u 0.9019 (0.8828) acc_u 15.6250 (14.6250) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [30/64] time 0.377 (0.450) data 0.246 (0.319) loss_u loss_u 0.8911 (0.8850) acc_u 12.5000 (14.3750) lr 1.5490e-03 eta 0:00:15
epoch [65/200] batch [35/64] time 0.447 (0.450) data 0.315 (0.319) loss_u loss_u 0.7876 (0.8809) acc_u 28.1250 (14.9107) lr 1.5490e-03 eta 0:00:13
epoch [65/200] batch [40/64] time 0.505 (0.448) data 0.374 (0.317) loss_u loss_u 0.8809 (0.8831) acc_u 15.6250 (14.8438) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [45/64] time 0.390 (0.447) data 0.259 (0.316) loss_u loss_u 0.8730 (0.8839) acc_u 9.3750 (14.5139) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [50/64] time 0.371 (0.445) data 0.237 (0.314) loss_u loss_u 0.8921 (0.8851) acc_u 12.5000 (14.4375) lr 1.5490e-03 eta 0:00:06
epoch [65/200] batch [55/64] time 0.505 (0.446) data 0.374 (0.315) loss_u loss_u 0.9082 (0.8862) acc_u 9.3750 (14.4318) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [60/64] time 0.421 (0.444) data 0.289 (0.312) loss_u loss_u 0.9121 (0.8842) acc_u 9.3750 (14.5833) lr 1.5490e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1552
confident_label rate tensor(0.3396, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1065
clean true:1057
clean false:8
clean_rate:0.9924882629107982
noisy true:527
noisy false:1544
after delete: len(clean_dataset) 1065
after delete: len(noisy_dataset) 2071
epoch [66/200] batch [5/33] time 0.494 (0.468) data 0.365 (0.337) loss_x loss_x 1.2559 (1.3027) acc_x 65.6250 (72.5000) lr 1.5358e-03 eta 0:00:13
epoch [66/200] batch [10/33] time 0.516 (0.486) data 0.386 (0.356) loss_x loss_x 1.4326 (1.2642) acc_x 62.5000 (70.3125) lr 1.5358e-03 eta 0:00:11
epoch [66/200] batch [15/33] time 0.704 (0.489) data 0.573 (0.358) loss_x loss_x 1.7852 (1.2455) acc_x 53.1250 (69.1667) lr 1.5358e-03 eta 0:00:08
epoch [66/200] batch [20/33] time 0.380 (0.470) data 0.249 (0.339) loss_x loss_x 1.2900 (1.2763) acc_x 65.6250 (68.7500) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [25/33] time 0.523 (0.465) data 0.391 (0.335) loss_x loss_x 1.5791 (1.2938) acc_x 56.2500 (67.8750) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [30/33] time 0.374 (0.455) data 0.243 (0.324) loss_x loss_x 1.0781 (1.3052) acc_x 65.6250 (67.6042) lr 1.5358e-03 eta 0:00:01
epoch [66/200] batch [5/64] time 0.406 (0.452) data 0.274 (0.322) loss_u loss_u 0.8809 (0.8854) acc_u 12.5000 (14.3750) lr 1.5358e-03 eta 0:00:26
epoch [66/200] batch [10/64] time 0.488 (0.464) data 0.356 (0.333) loss_u loss_u 0.8789 (0.8760) acc_u 18.7500 (15.6250) lr 1.5358e-03 eta 0:00:25
epoch [66/200] batch [15/64] time 0.393 (0.463) data 0.261 (0.332) loss_u loss_u 0.8760 (0.8741) acc_u 12.5000 (15.4167) lr 1.5358e-03 eta 0:00:22
epoch [66/200] batch [20/64] time 0.366 (0.458) data 0.235 (0.327) loss_u loss_u 0.8159 (0.8746) acc_u 18.7500 (15.4688) lr 1.5358e-03 eta 0:00:20
epoch [66/200] batch [25/64] time 0.474 (0.456) data 0.342 (0.325) loss_u loss_u 0.9263 (0.8754) acc_u 6.2500 (15.5000) lr 1.5358e-03 eta 0:00:17
epoch [66/200] batch [30/64] time 0.351 (0.457) data 0.219 (0.326) loss_u loss_u 0.8091 (0.8688) acc_u 28.1250 (16.2500) lr 1.5358e-03 eta 0:00:15
epoch [66/200] batch [35/64] time 0.419 (0.456) data 0.287 (0.325) loss_u loss_u 0.9326 (0.8689) acc_u 9.3750 (16.3393) lr 1.5358e-03 eta 0:00:13
epoch [66/200] batch [40/64] time 0.425 (0.452) data 0.293 (0.321) loss_u loss_u 0.9131 (0.8713) acc_u 12.5000 (16.2500) lr 1.5358e-03 eta 0:00:10
epoch [66/200] batch [45/64] time 0.458 (0.450) data 0.326 (0.319) loss_u loss_u 0.8530 (0.8683) acc_u 18.7500 (16.8056) lr 1.5358e-03 eta 0:00:08
epoch [66/200] batch [50/64] time 0.390 (0.450) data 0.258 (0.319) loss_u loss_u 0.8931 (0.8692) acc_u 18.7500 (16.8750) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [55/64] time 0.387 (0.446) data 0.256 (0.314) loss_u loss_u 0.8828 (0.8698) acc_u 12.5000 (16.7614) lr 1.5358e-03 eta 0:00:04
epoch [66/200] batch [60/64] time 0.448 (0.444) data 0.316 (0.313) loss_u loss_u 0.8267 (0.8722) acc_u 21.8750 (16.5104) lr 1.5358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1538
confident_label rate tensor(0.3393, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1064
clean true:1061
clean false:3
clean_rate:0.9971804511278195
noisy true:537
noisy false:1535
after delete: len(clean_dataset) 1064
after delete: len(noisy_dataset) 2072
epoch [67/200] batch [5/33] time 0.479 (0.472) data 0.347 (0.342) loss_x loss_x 1.2842 (1.1977) acc_x 78.1250 (74.3750) lr 1.5225e-03 eta 0:00:13
epoch [67/200] batch [10/33] time 0.496 (0.475) data 0.365 (0.344) loss_x loss_x 1.3262 (1.1419) acc_x 56.2500 (74.6875) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [15/33] time 0.339 (0.464) data 0.209 (0.333) loss_x loss_x 1.6523 (1.2042) acc_x 75.0000 (72.5000) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [20/33] time 0.508 (0.462) data 0.377 (0.331) loss_x loss_x 1.0098 (1.1783) acc_x 78.1250 (72.0312) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [25/33] time 0.434 (0.456) data 0.303 (0.326) loss_x loss_x 0.8354 (1.2188) acc_x 78.1250 (71.1250) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [30/33] time 0.468 (0.465) data 0.337 (0.334) loss_x loss_x 1.4658 (1.2669) acc_x 68.7500 (69.4792) lr 1.5225e-03 eta 0:00:01
epoch [67/200] batch [5/64] time 0.428 (0.461) data 0.296 (0.330) loss_u loss_u 0.8955 (0.8753) acc_u 15.6250 (16.2500) lr 1.5225e-03 eta 0:00:27
epoch [67/200] batch [10/64] time 0.644 (0.459) data 0.512 (0.328) loss_u loss_u 0.9243 (0.8639) acc_u 9.3750 (17.1875) lr 1.5225e-03 eta 0:00:24
epoch [67/200] batch [15/64] time 0.468 (0.453) data 0.336 (0.322) loss_u loss_u 0.9019 (0.8721) acc_u 12.5000 (15.6250) lr 1.5225e-03 eta 0:00:22
epoch [67/200] batch [20/64] time 0.442 (0.454) data 0.310 (0.323) loss_u loss_u 0.8901 (0.8758) acc_u 15.6250 (15.1562) lr 1.5225e-03 eta 0:00:19
epoch [67/200] batch [25/64] time 0.350 (0.452) data 0.220 (0.321) loss_u loss_u 0.9141 (0.8753) acc_u 12.5000 (15.0000) lr 1.5225e-03 eta 0:00:17
epoch [67/200] batch [30/64] time 0.445 (0.449) data 0.314 (0.318) loss_u loss_u 0.7881 (0.8755) acc_u 25.0000 (14.8958) lr 1.5225e-03 eta 0:00:15
epoch [67/200] batch [35/64] time 0.463 (0.450) data 0.331 (0.319) loss_u loss_u 0.8369 (0.8717) acc_u 25.0000 (15.8929) lr 1.5225e-03 eta 0:00:13
epoch [67/200] batch [40/64] time 0.464 (0.448) data 0.332 (0.317) loss_u loss_u 0.8643 (0.8681) acc_u 12.5000 (16.1719) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [45/64] time 0.406 (0.448) data 0.274 (0.317) loss_u loss_u 0.9282 (0.8699) acc_u 6.2500 (15.9028) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [50/64] time 0.467 (0.448) data 0.336 (0.316) loss_u loss_u 0.8828 (0.8719) acc_u 18.7500 (15.7500) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [55/64] time 0.346 (0.445) data 0.214 (0.314) loss_u loss_u 0.8589 (0.8735) acc_u 18.7500 (15.6818) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [60/64] time 0.509 (0.445) data 0.378 (0.314) loss_u loss_u 0.9053 (0.8699) acc_u 12.5000 (16.1979) lr 1.5225e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1478
confident_label rate tensor(0.3504, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1099
clean true:1096
clean false:3
clean_rate:0.997270245677889
noisy true:562
noisy false:1475
after delete: len(clean_dataset) 1099
after delete: len(noisy_dataset) 2037
epoch [68/200] batch [5/34] time 0.617 (0.529) data 0.487 (0.399) loss_x loss_x 1.5996 (1.1286) acc_x 59.3750 (70.6250) lr 1.5090e-03 eta 0:00:15
epoch [68/200] batch [10/34] time 0.509 (0.500) data 0.378 (0.370) loss_x loss_x 0.9985 (1.0831) acc_x 78.1250 (72.8125) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [15/34] time 0.483 (0.500) data 0.353 (0.369) loss_x loss_x 1.0889 (1.1740) acc_x 71.8750 (68.9583) lr 1.5090e-03 eta 0:00:09
epoch [68/200] batch [20/34] time 0.340 (0.474) data 0.208 (0.344) loss_x loss_x 1.1953 (1.2002) acc_x 71.8750 (68.7500) lr 1.5090e-03 eta 0:00:06
epoch [68/200] batch [25/34] time 0.414 (0.466) data 0.283 (0.336) loss_x loss_x 1.5225 (1.2034) acc_x 56.2500 (68.5000) lr 1.5090e-03 eta 0:00:04
epoch [68/200] batch [30/34] time 0.482 (0.460) data 0.352 (0.329) loss_x loss_x 0.6987 (1.1847) acc_x 78.1250 (68.9583) lr 1.5090e-03 eta 0:00:01
epoch [68/200] batch [5/63] time 0.392 (0.452) data 0.261 (0.321) loss_u loss_u 0.8745 (0.8805) acc_u 15.6250 (14.3750) lr 1.5090e-03 eta 0:00:26
epoch [68/200] batch [10/63] time 0.359 (0.447) data 0.228 (0.316) loss_u loss_u 0.9165 (0.8912) acc_u 9.3750 (12.8125) lr 1.5090e-03 eta 0:00:23
epoch [68/200] batch [15/63] time 0.446 (0.449) data 0.314 (0.319) loss_u loss_u 0.8467 (0.8809) acc_u 18.7500 (14.7917) lr 1.5090e-03 eta 0:00:21
epoch [68/200] batch [20/63] time 0.378 (0.448) data 0.247 (0.317) loss_u loss_u 0.8481 (0.8859) acc_u 21.8750 (14.0625) lr 1.5090e-03 eta 0:00:19
epoch [68/200] batch [25/63] time 0.453 (0.447) data 0.323 (0.316) loss_u loss_u 0.9155 (0.8904) acc_u 15.6250 (13.7500) lr 1.5090e-03 eta 0:00:16
epoch [68/200] batch [30/63] time 0.509 (0.450) data 0.379 (0.319) loss_u loss_u 0.8965 (0.8873) acc_u 12.5000 (14.2708) lr 1.5090e-03 eta 0:00:14
epoch [68/200] batch [35/63] time 0.357 (0.447) data 0.226 (0.316) loss_u loss_u 0.8789 (0.8848) acc_u 18.7500 (14.8214) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [40/63] time 0.411 (0.446) data 0.281 (0.315) loss_u loss_u 0.8892 (0.8864) acc_u 15.6250 (14.4531) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [45/63] time 0.409 (0.447) data 0.279 (0.317) loss_u loss_u 0.8564 (0.8850) acc_u 15.6250 (14.5139) lr 1.5090e-03 eta 0:00:08
epoch [68/200] batch [50/63] time 0.351 (0.443) data 0.221 (0.312) loss_u loss_u 0.7993 (0.8840) acc_u 21.8750 (14.6250) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [55/63] time 0.370 (0.443) data 0.240 (0.312) loss_u loss_u 0.8369 (0.8853) acc_u 25.0000 (14.6591) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [60/63] time 0.385 (0.439) data 0.255 (0.309) loss_u loss_u 0.8481 (0.8857) acc_u 18.7500 (14.7396) lr 1.5090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1519
confident_label rate tensor(0.3495, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1096
clean true:1091
clean false:5
clean_rate:0.9954379562043796
noisy true:526
noisy false:1514
after delete: len(clean_dataset) 1096
after delete: len(noisy_dataset) 2040
epoch [69/200] batch [5/34] time 0.498 (0.463) data 0.366 (0.332) loss_x loss_x 0.9570 (1.1215) acc_x 71.8750 (70.0000) lr 1.4955e-03 eta 0:00:13
epoch [69/200] batch [10/34] time 0.389 (0.445) data 0.258 (0.314) loss_x loss_x 1.7051 (1.2787) acc_x 68.7500 (67.8125) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [15/34] time 0.371 (0.467) data 0.240 (0.336) loss_x loss_x 1.0234 (1.3087) acc_x 81.2500 (66.8750) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [20/34] time 0.479 (0.473) data 0.349 (0.342) loss_x loss_x 1.3545 (1.3055) acc_x 62.5000 (67.1875) lr 1.4955e-03 eta 0:00:06
epoch [69/200] batch [25/34] time 0.519 (0.467) data 0.388 (0.336) loss_x loss_x 1.1113 (1.2926) acc_x 71.8750 (67.3750) lr 1.4955e-03 eta 0:00:04
epoch [69/200] batch [30/34] time 0.401 (0.468) data 0.270 (0.337) loss_x loss_x 0.9253 (1.2439) acc_x 71.8750 (68.6458) lr 1.4955e-03 eta 0:00:01
epoch [69/200] batch [5/63] time 0.572 (0.478) data 0.441 (0.347) loss_u loss_u 0.9072 (0.8954) acc_u 12.5000 (13.1250) lr 1.4955e-03 eta 0:00:27
epoch [69/200] batch [10/63] time 0.490 (0.468) data 0.358 (0.337) loss_u loss_u 0.8794 (0.8932) acc_u 15.6250 (13.1250) lr 1.4955e-03 eta 0:00:24
epoch [69/200] batch [15/63] time 0.375 (0.464) data 0.245 (0.333) loss_u loss_u 0.9004 (0.8928) acc_u 15.6250 (13.5417) lr 1.4955e-03 eta 0:00:22
epoch [69/200] batch [20/63] time 0.390 (0.457) data 0.260 (0.326) loss_u loss_u 0.8359 (0.8861) acc_u 18.7500 (14.3750) lr 1.4955e-03 eta 0:00:19
epoch [69/200] batch [25/63] time 0.437 (0.452) data 0.306 (0.322) loss_u loss_u 0.8799 (0.8912) acc_u 15.6250 (13.7500) lr 1.4955e-03 eta 0:00:17
epoch [69/200] batch [30/63] time 0.534 (0.450) data 0.403 (0.320) loss_u loss_u 0.9204 (0.8909) acc_u 9.3750 (13.8542) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [35/63] time 0.452 (0.456) data 0.320 (0.325) loss_u loss_u 0.8652 (0.8913) acc_u 18.7500 (13.8393) lr 1.4955e-03 eta 0:00:12
epoch [69/200] batch [40/63] time 0.336 (0.456) data 0.206 (0.325) loss_u loss_u 0.7979 (0.8862) acc_u 25.0000 (14.4531) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [45/63] time 0.343 (0.454) data 0.212 (0.323) loss_u loss_u 0.9502 (0.8857) acc_u 3.1250 (14.3750) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [50/63] time 0.387 (0.453) data 0.256 (0.322) loss_u loss_u 0.9277 (0.8874) acc_u 9.3750 (14.1875) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [55/63] time 0.480 (0.452) data 0.348 (0.321) loss_u loss_u 0.8755 (0.8852) acc_u 9.3750 (14.5455) lr 1.4955e-03 eta 0:00:03
epoch [69/200] batch [60/63] time 0.378 (0.448) data 0.247 (0.318) loss_u loss_u 0.8672 (0.8850) acc_u 9.3750 (14.3750) lr 1.4955e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1493
confident_label rate tensor(0.3581, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1123
clean true:1117
clean false:6
clean_rate:0.9946571682991986
noisy true:526
noisy false:1487
after delete: len(clean_dataset) 1123
after delete: len(noisy_dataset) 2013
epoch [70/200] batch [5/35] time 0.525 (0.481) data 0.394 (0.350) loss_x loss_x 1.1230 (1.2250) acc_x 68.7500 (70.0000) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [10/35] time 0.467 (0.485) data 0.337 (0.354) loss_x loss_x 1.2168 (1.2379) acc_x 65.6250 (70.6250) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [15/35] time 0.594 (0.500) data 0.463 (0.369) loss_x loss_x 1.0234 (1.1912) acc_x 81.2500 (70.6250) lr 1.4818e-03 eta 0:00:09
epoch [70/200] batch [20/35] time 0.598 (0.513) data 0.464 (0.382) loss_x loss_x 1.1572 (1.2106) acc_x 65.6250 (70.0000) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [25/35] time 0.392 (0.505) data 0.261 (0.374) loss_x loss_x 1.0713 (1.2101) acc_x 78.1250 (69.7500) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [30/35] time 0.508 (0.493) data 0.375 (0.362) loss_x loss_x 1.2646 (1.2001) acc_x 68.7500 (69.7917) lr 1.4818e-03 eta 0:00:02
epoch [70/200] batch [35/35] time 0.416 (0.494) data 0.285 (0.363) loss_x loss_x 1.2607 (1.1884) acc_x 65.6250 (70.2679) lr 1.4818e-03 eta 0:00:00
epoch [70/200] batch [5/62] time 0.352 (0.481) data 0.220 (0.350) loss_u loss_u 0.8579 (0.8991) acc_u 18.7500 (11.8750) lr 1.4818e-03 eta 0:00:27
epoch [70/200] batch [10/62] time 0.374 (0.472) data 0.243 (0.341) loss_u loss_u 0.8608 (0.8913) acc_u 15.6250 (11.8750) lr 1.4818e-03 eta 0:00:24
epoch [70/200] batch [15/62] time 0.484 (0.470) data 0.352 (0.339) loss_u loss_u 0.9473 (0.8955) acc_u 9.3750 (12.5000) lr 1.4818e-03 eta 0:00:22
epoch [70/200] batch [20/62] time 0.485 (0.466) data 0.353 (0.335) loss_u loss_u 0.9673 (0.9020) acc_u 3.1250 (12.0312) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [25/62] time 0.368 (0.457) data 0.236 (0.326) loss_u loss_u 0.8462 (0.8932) acc_u 18.7500 (13.2500) lr 1.4818e-03 eta 0:00:16
epoch [70/200] batch [30/62] time 0.459 (0.460) data 0.328 (0.329) loss_u loss_u 0.9272 (0.8922) acc_u 9.3750 (13.5417) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [35/62] time 0.479 (0.462) data 0.347 (0.331) loss_u loss_u 0.8672 (0.8904) acc_u 12.5000 (13.8393) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [40/62] time 0.386 (0.459) data 0.254 (0.327) loss_u loss_u 0.8979 (0.8893) acc_u 12.5000 (14.1406) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [45/62] time 0.435 (0.457) data 0.303 (0.326) loss_u loss_u 0.8716 (0.8865) acc_u 12.5000 (14.5833) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [50/62] time 0.441 (0.453) data 0.309 (0.322) loss_u loss_u 0.8892 (0.8862) acc_u 12.5000 (14.8125) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [55/62] time 0.514 (0.449) data 0.382 (0.317) loss_u loss_u 0.9678 (0.8885) acc_u 6.2500 (14.4318) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [60/62] time 0.606 (0.449) data 0.474 (0.318) loss_u loss_u 0.9307 (0.8871) acc_u 12.5000 (14.7396) lr 1.4818e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1511
confident_label rate tensor(0.3514, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1102
clean true:1098
clean false:4
clean_rate:0.9963702359346642
noisy true:527
noisy false:1507
after delete: len(clean_dataset) 1102
after delete: len(noisy_dataset) 2034
epoch [71/200] batch [5/34] time 0.417 (0.469) data 0.287 (0.338) loss_x loss_x 0.9121 (1.0359) acc_x 75.0000 (71.8750) lr 1.4679e-03 eta 0:00:13
epoch [71/200] batch [10/34] time 0.462 (0.479) data 0.332 (0.348) loss_x loss_x 0.9561 (1.1000) acc_x 68.7500 (70.9375) lr 1.4679e-03 eta 0:00:11
epoch [71/200] batch [15/34] time 0.728 (0.491) data 0.597 (0.361) loss_x loss_x 0.7656 (1.1076) acc_x 71.8750 (72.0833) lr 1.4679e-03 eta 0:00:09
epoch [71/200] batch [20/34] time 0.419 (0.488) data 0.289 (0.357) loss_x loss_x 1.3184 (1.1110) acc_x 75.0000 (72.8125) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [25/34] time 0.386 (0.476) data 0.256 (0.345) loss_x loss_x 1.1562 (1.1068) acc_x 75.0000 (72.3750) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [30/34] time 0.478 (0.468) data 0.347 (0.338) loss_x loss_x 1.2773 (1.1468) acc_x 68.7500 (71.7708) lr 1.4679e-03 eta 0:00:01
epoch [71/200] batch [5/63] time 0.396 (0.461) data 0.265 (0.330) loss_u loss_u 0.9243 (0.8784) acc_u 9.3750 (14.3750) lr 1.4679e-03 eta 0:00:26
epoch [71/200] batch [10/63] time 0.414 (0.458) data 0.282 (0.328) loss_u loss_u 0.8730 (0.8822) acc_u 18.7500 (15.6250) lr 1.4679e-03 eta 0:00:24
epoch [71/200] batch [15/63] time 0.436 (0.455) data 0.305 (0.324) loss_u loss_u 0.8789 (0.8725) acc_u 12.5000 (17.0833) lr 1.4679e-03 eta 0:00:21
epoch [71/200] batch [20/63] time 0.363 (0.452) data 0.229 (0.321) loss_u loss_u 0.8359 (0.8794) acc_u 18.7500 (15.6250) lr 1.4679e-03 eta 0:00:19
epoch [71/200] batch [25/63] time 0.399 (0.450) data 0.268 (0.319) loss_u loss_u 0.7871 (0.8746) acc_u 25.0000 (16.2500) lr 1.4679e-03 eta 0:00:17
epoch [71/200] batch [30/63] time 0.493 (0.449) data 0.361 (0.318) loss_u loss_u 0.8638 (0.8782) acc_u 12.5000 (15.5208) lr 1.4679e-03 eta 0:00:14
epoch [71/200] batch [35/63] time 0.400 (0.447) data 0.269 (0.316) loss_u loss_u 0.8804 (0.8759) acc_u 18.7500 (16.0714) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [40/63] time 0.436 (0.443) data 0.304 (0.312) loss_u loss_u 0.9424 (0.8807) acc_u 6.2500 (15.3906) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [45/63] time 0.587 (0.445) data 0.455 (0.314) loss_u loss_u 0.7915 (0.8780) acc_u 34.3750 (15.7639) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [50/63] time 0.514 (0.443) data 0.382 (0.312) loss_u loss_u 0.8564 (0.8732) acc_u 12.5000 (16.2500) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [55/63] time 0.477 (0.442) data 0.345 (0.311) loss_u loss_u 0.8569 (0.8750) acc_u 15.6250 (15.7955) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [60/63] time 0.651 (0.445) data 0.520 (0.314) loss_u loss_u 0.8706 (0.8721) acc_u 18.7500 (16.3021) lr 1.4679e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1534
confident_label rate tensor(0.3438, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1078
clean true:1076
clean false:2
clean_rate:0.9981447124304267
noisy true:526
noisy false:1532
after delete: len(clean_dataset) 1078
after delete: len(noisy_dataset) 2058
epoch [72/200] batch [5/33] time 0.485 (0.467) data 0.355 (0.337) loss_x loss_x 1.0830 (0.9820) acc_x 71.8750 (74.3750) lr 1.4540e-03 eta 0:00:13
epoch [72/200] batch [10/33] time 0.445 (0.473) data 0.315 (0.342) loss_x loss_x 0.8838 (1.1687) acc_x 78.1250 (69.6875) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [15/33] time 0.459 (0.482) data 0.327 (0.352) loss_x loss_x 0.8896 (1.0898) acc_x 75.0000 (71.2500) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [20/33] time 0.688 (0.484) data 0.557 (0.353) loss_x loss_x 0.9487 (1.0838) acc_x 78.1250 (72.1875) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [25/33] time 0.412 (0.472) data 0.281 (0.342) loss_x loss_x 1.2061 (1.0783) acc_x 71.8750 (72.3750) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [30/33] time 0.361 (0.470) data 0.231 (0.340) loss_x loss_x 1.3877 (1.1016) acc_x 62.5000 (71.7708) lr 1.4540e-03 eta 0:00:01
epoch [72/200] batch [5/64] time 0.389 (0.465) data 0.257 (0.335) loss_u loss_u 0.8213 (0.8742) acc_u 21.8750 (14.3750) lr 1.4540e-03 eta 0:00:27
epoch [72/200] batch [10/64] time 0.369 (0.458) data 0.238 (0.328) loss_u loss_u 0.9038 (0.8742) acc_u 12.5000 (14.3750) lr 1.4540e-03 eta 0:00:24
epoch [72/200] batch [15/64] time 0.459 (0.459) data 0.328 (0.329) loss_u loss_u 0.8599 (0.8710) acc_u 25.0000 (16.2500) lr 1.4540e-03 eta 0:00:22
epoch [72/200] batch [20/64] time 0.424 (0.461) data 0.293 (0.331) loss_u loss_u 0.8843 (0.8763) acc_u 18.7500 (16.2500) lr 1.4540e-03 eta 0:00:20
epoch [72/200] batch [25/64] time 0.482 (0.455) data 0.351 (0.324) loss_u loss_u 0.8525 (0.8776) acc_u 21.8750 (16.3750) lr 1.4540e-03 eta 0:00:17
epoch [72/200] batch [30/64] time 0.462 (0.458) data 0.331 (0.327) loss_u loss_u 0.8521 (0.8722) acc_u 18.7500 (16.9792) lr 1.4540e-03 eta 0:00:15
epoch [72/200] batch [35/64] time 0.339 (0.454) data 0.208 (0.324) loss_u loss_u 0.8149 (0.8731) acc_u 21.8750 (16.8750) lr 1.4540e-03 eta 0:00:13
epoch [72/200] batch [40/64] time 0.365 (0.451) data 0.234 (0.321) loss_u loss_u 0.8032 (0.8676) acc_u 25.0000 (17.7344) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [45/64] time 0.361 (0.450) data 0.230 (0.319) loss_u loss_u 0.9521 (0.8693) acc_u 9.3750 (17.6389) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [50/64] time 0.406 (0.448) data 0.275 (0.317) loss_u loss_u 0.8911 (0.8729) acc_u 15.6250 (17.3125) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [55/64] time 0.389 (0.448) data 0.257 (0.317) loss_u loss_u 0.8218 (0.8735) acc_u 18.7500 (16.9886) lr 1.4540e-03 eta 0:00:04
epoch [72/200] batch [60/64] time 0.552 (0.452) data 0.419 (0.321) loss_u loss_u 0.8936 (0.8720) acc_u 12.5000 (17.1354) lr 1.4540e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1476
confident_label rate tensor(0.3520, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1104
clean true:1099
clean false:5
clean_rate:0.9954710144927537
noisy true:561
noisy false:1471
after delete: len(clean_dataset) 1104
after delete: len(noisy_dataset) 2032
epoch [73/200] batch [5/34] time 0.531 (0.515) data 0.400 (0.385) loss_x loss_x 1.4629 (1.1169) acc_x 68.7500 (73.7500) lr 1.4399e-03 eta 0:00:14
epoch [73/200] batch [10/34] time 0.454 (0.505) data 0.324 (0.374) loss_x loss_x 1.0762 (1.0517) acc_x 68.7500 (74.3750) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [15/34] time 0.457 (0.508) data 0.327 (0.377) loss_x loss_x 0.8716 (1.0391) acc_x 78.1250 (74.5833) lr 1.4399e-03 eta 0:00:09
epoch [73/200] batch [20/34] time 0.482 (0.496) data 0.351 (0.366) loss_x loss_x 1.3115 (1.0551) acc_x 71.8750 (73.4375) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [25/34] time 0.429 (0.485) data 0.298 (0.355) loss_x loss_x 1.2402 (1.1053) acc_x 65.6250 (71.8750) lr 1.4399e-03 eta 0:00:04
epoch [73/200] batch [30/34] time 0.464 (0.480) data 0.333 (0.349) loss_x loss_x 1.3779 (1.1532) acc_x 71.8750 (70.0000) lr 1.4399e-03 eta 0:00:01
epoch [73/200] batch [5/63] time 0.424 (0.472) data 0.292 (0.341) loss_u loss_u 0.8418 (0.8689) acc_u 25.0000 (18.1250) lr 1.4399e-03 eta 0:00:27
epoch [73/200] batch [10/63] time 0.411 (0.473) data 0.279 (0.342) loss_u loss_u 0.9263 (0.8884) acc_u 9.3750 (14.0625) lr 1.4399e-03 eta 0:00:25
epoch [73/200] batch [15/63] time 0.399 (0.476) data 0.267 (0.345) loss_u loss_u 0.9292 (0.8888) acc_u 9.3750 (13.9583) lr 1.4399e-03 eta 0:00:22
epoch [73/200] batch [20/63] time 0.538 (0.478) data 0.406 (0.347) loss_u loss_u 0.8843 (0.8931) acc_u 18.7500 (13.4375) lr 1.4399e-03 eta 0:00:20
epoch [73/200] batch [25/63] time 0.403 (0.470) data 0.271 (0.338) loss_u loss_u 0.7734 (0.8863) acc_u 34.3750 (14.8750) lr 1.4399e-03 eta 0:00:17
epoch [73/200] batch [30/63] time 0.382 (0.467) data 0.251 (0.336) loss_u loss_u 0.9341 (0.8916) acc_u 6.2500 (14.0625) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [35/63] time 0.425 (0.468) data 0.293 (0.337) loss_u loss_u 0.8584 (0.8879) acc_u 21.8750 (14.5536) lr 1.4399e-03 eta 0:00:13
epoch [73/200] batch [40/63] time 0.476 (0.466) data 0.344 (0.334) loss_u loss_u 0.9639 (0.8857) acc_u 6.2500 (14.7656) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [45/63] time 0.437 (0.464) data 0.306 (0.332) loss_u loss_u 0.8340 (0.8844) acc_u 18.7500 (14.5833) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [50/63] time 0.447 (0.464) data 0.316 (0.332) loss_u loss_u 0.9229 (0.8866) acc_u 12.5000 (14.4375) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [55/63] time 0.527 (0.463) data 0.396 (0.331) loss_u loss_u 0.8770 (0.8849) acc_u 15.6250 (14.5455) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [60/63] time 0.363 (0.458) data 0.232 (0.326) loss_u loss_u 0.9219 (0.8851) acc_u 9.3750 (14.5833) lr 1.4399e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1446
confident_label rate tensor(0.3677, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1153
clean true:1146
clean false:7
clean_rate:0.9939288811795317
noisy true:544
noisy false:1439
after delete: len(clean_dataset) 1153
after delete: len(noisy_dataset) 1983
epoch [74/200] batch [5/36] time 0.450 (0.475) data 0.319 (0.345) loss_x loss_x 1.3271 (1.3498) acc_x 62.5000 (61.8750) lr 1.4258e-03 eta 0:00:14
epoch [74/200] batch [10/36] time 0.389 (0.480) data 0.259 (0.349) loss_x loss_x 1.3281 (1.2846) acc_x 68.7500 (66.8750) lr 1.4258e-03 eta 0:00:12
epoch [74/200] batch [15/36] time 0.429 (0.487) data 0.298 (0.356) loss_x loss_x 0.9717 (1.2507) acc_x 71.8750 (67.5000) lr 1.4258e-03 eta 0:00:10
epoch [74/200] batch [20/36] time 0.485 (0.473) data 0.354 (0.343) loss_x loss_x 1.0840 (1.2869) acc_x 75.0000 (67.0312) lr 1.4258e-03 eta 0:00:07
epoch [74/200] batch [25/36] time 0.431 (0.465) data 0.301 (0.334) loss_x loss_x 1.0869 (1.2751) acc_x 78.1250 (67.8750) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [30/36] time 0.463 (0.456) data 0.332 (0.325) loss_x loss_x 0.9648 (1.2611) acc_x 71.8750 (67.9167) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [35/36] time 0.471 (0.456) data 0.340 (0.325) loss_x loss_x 1.6191 (1.2661) acc_x 50.0000 (67.5000) lr 1.4258e-03 eta 0:00:00
epoch [74/200] batch [5/61] time 0.449 (0.458) data 0.317 (0.327) loss_u loss_u 0.9268 (0.9042) acc_u 6.2500 (12.5000) lr 1.4258e-03 eta 0:00:25
epoch [74/200] batch [10/61] time 0.474 (0.456) data 0.343 (0.325) loss_u loss_u 0.8647 (0.9025) acc_u 21.8750 (13.7500) lr 1.4258e-03 eta 0:00:23
epoch [74/200] batch [15/61] time 0.331 (0.450) data 0.199 (0.319) loss_u loss_u 0.9175 (0.9002) acc_u 9.3750 (12.9167) lr 1.4258e-03 eta 0:00:20
epoch [74/200] batch [20/61] time 0.368 (0.442) data 0.236 (0.311) loss_u loss_u 0.8125 (0.8994) acc_u 18.7500 (12.5000) lr 1.4258e-03 eta 0:00:18
epoch [74/200] batch [25/61] time 0.415 (0.436) data 0.283 (0.305) loss_u loss_u 0.9229 (0.8983) acc_u 12.5000 (12.8750) lr 1.4258e-03 eta 0:00:15
epoch [74/200] batch [30/61] time 0.538 (0.437) data 0.406 (0.306) loss_u loss_u 0.9189 (0.8955) acc_u 12.5000 (13.3333) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [35/61] time 0.466 (0.443) data 0.334 (0.311) loss_u loss_u 0.8604 (0.8976) acc_u 18.7500 (12.9464) lr 1.4258e-03 eta 0:00:11
epoch [74/200] batch [40/61] time 0.392 (0.443) data 0.261 (0.311) loss_u loss_u 0.9316 (0.9008) acc_u 9.3750 (12.5781) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [45/61] time 0.427 (0.440) data 0.296 (0.309) loss_u loss_u 0.8296 (0.8973) acc_u 21.8750 (13.0556) lr 1.4258e-03 eta 0:00:07
epoch [74/200] batch [50/61] time 0.578 (0.447) data 0.447 (0.315) loss_u loss_u 0.8892 (0.8936) acc_u 9.3750 (13.5000) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [55/61] time 0.367 (0.448) data 0.235 (0.317) loss_u loss_u 0.9248 (0.8914) acc_u 9.3750 (13.6364) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [60/61] time 0.391 (0.444) data 0.261 (0.313) loss_u loss_u 0.9321 (0.8920) acc_u 9.3750 (13.6979) lr 1.4258e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1500
confident_label rate tensor(0.3473, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1089
clean true:1084
clean false:5
clean_rate:0.9954086317722681
noisy true:552
noisy false:1495
after delete: len(clean_dataset) 1089
after delete: len(noisy_dataset) 2047
epoch [75/200] batch [5/34] time 0.490 (0.445) data 0.359 (0.314) loss_x loss_x 0.7358 (1.2001) acc_x 78.1250 (71.2500) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [10/34] time 0.423 (0.460) data 0.292 (0.329) loss_x loss_x 0.9609 (1.1955) acc_x 78.1250 (70.6250) lr 1.4115e-03 eta 0:00:11
epoch [75/200] batch [15/34] time 0.448 (0.459) data 0.318 (0.328) loss_x loss_x 1.4619 (1.2093) acc_x 65.6250 (68.9583) lr 1.4115e-03 eta 0:00:08
epoch [75/200] batch [20/34] time 0.432 (0.452) data 0.301 (0.322) loss_x loss_x 1.3281 (1.2061) acc_x 62.5000 (68.5938) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [25/34] time 0.617 (0.460) data 0.485 (0.329) loss_x loss_x 1.0098 (1.2075) acc_x 68.7500 (69.0000) lr 1.4115e-03 eta 0:00:04
epoch [75/200] batch [30/34] time 0.494 (0.470) data 0.364 (0.339) loss_x loss_x 1.3223 (1.2174) acc_x 71.8750 (69.2708) lr 1.4115e-03 eta 0:00:01
epoch [75/200] batch [5/63] time 0.360 (0.459) data 0.228 (0.328) loss_u loss_u 0.9355 (0.9016) acc_u 6.2500 (12.5000) lr 1.4115e-03 eta 0:00:26
epoch [75/200] batch [10/63] time 0.375 (0.450) data 0.244 (0.319) loss_u loss_u 0.9370 (0.8870) acc_u 6.2500 (14.3750) lr 1.4115e-03 eta 0:00:23
epoch [75/200] batch [15/63] time 0.424 (0.449) data 0.292 (0.318) loss_u loss_u 0.8999 (0.8806) acc_u 12.5000 (15.6250) lr 1.4115e-03 eta 0:00:21
epoch [75/200] batch [20/63] time 0.538 (0.451) data 0.406 (0.320) loss_u loss_u 0.9038 (0.8800) acc_u 9.3750 (15.3125) lr 1.4115e-03 eta 0:00:19
epoch [75/200] batch [25/63] time 0.401 (0.446) data 0.270 (0.315) loss_u loss_u 0.8931 (0.8736) acc_u 18.7500 (16.0000) lr 1.4115e-03 eta 0:00:16
epoch [75/200] batch [30/63] time 0.395 (0.444) data 0.263 (0.313) loss_u loss_u 0.8677 (0.8759) acc_u 15.6250 (15.7292) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [35/63] time 0.474 (0.442) data 0.342 (0.311) loss_u loss_u 0.8672 (0.8768) acc_u 15.6250 (15.6250) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [40/63] time 0.472 (0.443) data 0.340 (0.312) loss_u loss_u 0.8994 (0.8806) acc_u 6.2500 (15.0781) lr 1.4115e-03 eta 0:00:10
epoch [75/200] batch [45/63] time 0.394 (0.443) data 0.262 (0.312) loss_u loss_u 0.8779 (0.8771) acc_u 15.6250 (15.4167) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [50/63] time 0.376 (0.444) data 0.245 (0.313) loss_u loss_u 0.8555 (0.8782) acc_u 21.8750 (15.3750) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [55/63] time 0.411 (0.443) data 0.280 (0.311) loss_u loss_u 0.9028 (0.8795) acc_u 12.5000 (15.3409) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [60/63] time 0.421 (0.440) data 0.290 (0.308) loss_u loss_u 0.8604 (0.8764) acc_u 21.8750 (16.2500) lr 1.4115e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1485
confident_label rate tensor(0.3578, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1122
clean true:1115
clean false:7
clean_rate:0.9937611408199644
noisy true:536
noisy false:1478
after delete: len(clean_dataset) 1122
after delete: len(noisy_dataset) 2014
epoch [76/200] batch [5/35] time 0.427 (0.460) data 0.297 (0.329) loss_x loss_x 1.4111 (1.2343) acc_x 65.6250 (70.6250) lr 1.3971e-03 eta 0:00:13
epoch [76/200] batch [10/35] time 0.457 (0.454) data 0.327 (0.323) loss_x loss_x 0.7607 (1.2069) acc_x 84.3750 (71.8750) lr 1.3971e-03 eta 0:00:11
epoch [76/200] batch [15/35] time 0.432 (0.466) data 0.301 (0.335) loss_x loss_x 1.2725 (1.1945) acc_x 75.0000 (72.2917) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [20/35] time 0.496 (0.473) data 0.365 (0.342) loss_x loss_x 0.9810 (1.1844) acc_x 75.0000 (72.0312) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [25/35] time 0.381 (0.464) data 0.250 (0.333) loss_x loss_x 1.2256 (1.1577) acc_x 68.7500 (72.6250) lr 1.3971e-03 eta 0:00:04
epoch [76/200] batch [30/35] time 0.538 (0.463) data 0.407 (0.332) loss_x loss_x 1.3721 (1.1543) acc_x 71.8750 (73.1250) lr 1.3971e-03 eta 0:00:02
epoch [76/200] batch [35/35] time 0.524 (0.462) data 0.393 (0.331) loss_x loss_x 1.4326 (1.1780) acc_x 59.3750 (71.5179) lr 1.3971e-03 eta 0:00:00
epoch [76/200] batch [5/62] time 0.434 (0.458) data 0.302 (0.327) loss_u loss_u 0.9287 (0.8821) acc_u 15.6250 (16.8750) lr 1.3971e-03 eta 0:00:26
epoch [76/200] batch [10/62] time 0.451 (0.453) data 0.319 (0.322) loss_u loss_u 0.8687 (0.8738) acc_u 21.8750 (17.8125) lr 1.3971e-03 eta 0:00:23
epoch [76/200] batch [15/62] time 0.370 (0.445) data 0.239 (0.314) loss_u loss_u 0.9351 (0.8821) acc_u 9.3750 (15.6250) lr 1.3971e-03 eta 0:00:20
epoch [76/200] batch [20/62] time 0.572 (0.444) data 0.440 (0.313) loss_u loss_u 0.8818 (0.8776) acc_u 18.7500 (15.7812) lr 1.3971e-03 eta 0:00:18
epoch [76/200] batch [25/62] time 0.338 (0.442) data 0.206 (0.311) loss_u loss_u 0.9053 (0.8728) acc_u 12.5000 (16.5000) lr 1.3971e-03 eta 0:00:16
epoch [76/200] batch [30/62] time 0.413 (0.441) data 0.282 (0.310) loss_u loss_u 0.8745 (0.8756) acc_u 25.0000 (16.3542) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [35/62] time 0.731 (0.446) data 0.601 (0.315) loss_u loss_u 0.7495 (0.8694) acc_u 28.1250 (16.8750) lr 1.3971e-03 eta 0:00:12
epoch [76/200] batch [40/62] time 0.365 (0.447) data 0.234 (0.316) loss_u loss_u 0.9146 (0.8722) acc_u 6.2500 (16.4844) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [45/62] time 0.413 (0.446) data 0.282 (0.315) loss_u loss_u 0.9043 (0.8743) acc_u 9.3750 (16.1806) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [50/62] time 0.454 (0.448) data 0.322 (0.317) loss_u loss_u 0.9189 (0.8764) acc_u 6.2500 (15.6875) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [55/62] time 0.431 (0.446) data 0.301 (0.315) loss_u loss_u 0.9521 (0.8765) acc_u 9.3750 (15.6818) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [60/62] time 0.408 (0.445) data 0.278 (0.314) loss_u loss_u 0.9155 (0.8771) acc_u 9.3750 (15.6250) lr 1.3971e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1467
confident_label rate tensor(0.3584, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1124
clean true:1118
clean false:6
clean_rate:0.994661921708185
noisy true:551
noisy false:1461
after delete: len(clean_dataset) 1124
after delete: len(noisy_dataset) 2012
epoch [77/200] batch [5/35] time 0.458 (0.449) data 0.326 (0.319) loss_x loss_x 1.3848 (1.1992) acc_x 65.6250 (68.1250) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [10/35] time 0.387 (0.444) data 0.257 (0.313) loss_x loss_x 0.8774 (1.1525) acc_x 78.1250 (69.0625) lr 1.3827e-03 eta 0:00:11
epoch [77/200] batch [15/35] time 0.399 (0.455) data 0.268 (0.325) loss_x loss_x 1.0703 (1.1124) acc_x 65.6250 (69.7917) lr 1.3827e-03 eta 0:00:09
epoch [77/200] batch [20/35] time 0.554 (0.462) data 0.424 (0.331) loss_x loss_x 1.7402 (1.1740) acc_x 68.7500 (70.0000) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [25/35] time 0.379 (0.454) data 0.249 (0.323) loss_x loss_x 1.5957 (1.2226) acc_x 56.2500 (68.5000) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [30/35] time 0.551 (0.454) data 0.421 (0.323) loss_x loss_x 1.1650 (1.2207) acc_x 62.5000 (68.5417) lr 1.3827e-03 eta 0:00:02
epoch [77/200] batch [35/35] time 0.486 (0.453) data 0.356 (0.323) loss_x loss_x 1.1113 (1.2185) acc_x 71.8750 (68.5714) lr 1.3827e-03 eta 0:00:00
epoch [77/200] batch [5/62] time 0.423 (0.453) data 0.293 (0.322) loss_u loss_u 0.8911 (0.8699) acc_u 12.5000 (17.5000) lr 1.3827e-03 eta 0:00:25
epoch [77/200] batch [10/62] time 0.475 (0.458) data 0.345 (0.327) loss_u loss_u 0.8843 (0.8876) acc_u 15.6250 (15.0000) lr 1.3827e-03 eta 0:00:23
epoch [77/200] batch [15/62] time 0.595 (0.457) data 0.464 (0.327) loss_u loss_u 0.8350 (0.8806) acc_u 25.0000 (15.6250) lr 1.3827e-03 eta 0:00:21
epoch [77/200] batch [20/62] time 0.571 (0.457) data 0.440 (0.326) loss_u loss_u 0.8789 (0.8842) acc_u 12.5000 (14.8438) lr 1.3827e-03 eta 0:00:19
epoch [77/200] batch [25/62] time 0.549 (0.455) data 0.419 (0.324) loss_u loss_u 0.8735 (0.8797) acc_u 15.6250 (15.5000) lr 1.3827e-03 eta 0:00:16
epoch [77/200] batch [30/62] time 0.445 (0.454) data 0.313 (0.323) loss_u loss_u 0.8726 (0.8762) acc_u 15.6250 (15.7292) lr 1.3827e-03 eta 0:00:14
epoch [77/200] batch [35/62] time 0.383 (0.450) data 0.252 (0.319) loss_u loss_u 0.8945 (0.8779) acc_u 12.5000 (15.3571) lr 1.3827e-03 eta 0:00:12
epoch [77/200] batch [40/62] time 0.489 (0.447) data 0.357 (0.316) loss_u loss_u 0.7744 (0.8724) acc_u 34.3750 (16.3281) lr 1.3827e-03 eta 0:00:09
epoch [77/200] batch [45/62] time 0.500 (0.447) data 0.369 (0.316) loss_u loss_u 0.8838 (0.8726) acc_u 18.7500 (16.3889) lr 1.3827e-03 eta 0:00:07
epoch [77/200] batch [50/62] time 0.437 (0.444) data 0.306 (0.313) loss_u loss_u 0.9414 (0.8774) acc_u 6.2500 (15.7500) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [55/62] time 0.319 (0.443) data 0.188 (0.312) loss_u loss_u 0.9590 (0.8773) acc_u 6.2500 (16.1364) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [60/62] time 0.388 (0.444) data 0.258 (0.313) loss_u loss_u 0.9053 (0.8783) acc_u 12.5000 (16.0417) lr 1.3827e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1494
confident_label rate tensor(0.3501, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1098
clean true:1095
clean false:3
clean_rate:0.9972677595628415
noisy true:547
noisy false:1491
after delete: len(clean_dataset) 1098
after delete: len(noisy_dataset) 2038
epoch [78/200] batch [5/34] time 0.569 (0.479) data 0.438 (0.348) loss_x loss_x 1.3838 (1.3182) acc_x 62.5000 (68.1250) lr 1.3681e-03 eta 0:00:13
epoch [78/200] batch [10/34] time 0.413 (0.501) data 0.283 (0.370) loss_x loss_x 1.6797 (1.2927) acc_x 62.5000 (67.5000) lr 1.3681e-03 eta 0:00:12
epoch [78/200] batch [15/34] time 0.438 (0.486) data 0.308 (0.355) loss_x loss_x 1.2344 (1.2653) acc_x 68.7500 (68.1250) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [20/34] time 0.468 (0.461) data 0.337 (0.330) loss_x loss_x 0.8408 (1.2641) acc_x 78.1250 (67.8125) lr 1.3681e-03 eta 0:00:06
epoch [78/200] batch [25/34] time 0.523 (0.461) data 0.392 (0.330) loss_x loss_x 1.2148 (1.2719) acc_x 59.3750 (67.3750) lr 1.3681e-03 eta 0:00:04
epoch [78/200] batch [30/34] time 0.415 (0.465) data 0.284 (0.334) loss_x loss_x 1.1426 (1.2628) acc_x 84.3750 (67.3958) lr 1.3681e-03 eta 0:00:01
epoch [78/200] batch [5/63] time 0.467 (0.480) data 0.334 (0.349) loss_u loss_u 0.9277 (0.8617) acc_u 6.2500 (15.0000) lr 1.3681e-03 eta 0:00:27
epoch [78/200] batch [10/63] time 0.651 (0.477) data 0.520 (0.346) loss_u loss_u 0.9473 (0.8805) acc_u 6.2500 (14.0625) lr 1.3681e-03 eta 0:00:25
epoch [78/200] batch [15/63] time 0.365 (0.472) data 0.234 (0.340) loss_u loss_u 0.8589 (0.8757) acc_u 18.7500 (15.0000) lr 1.3681e-03 eta 0:00:22
epoch [78/200] batch [20/63] time 0.404 (0.463) data 0.272 (0.332) loss_u loss_u 0.8848 (0.8724) acc_u 18.7500 (15.7812) lr 1.3681e-03 eta 0:00:19
epoch [78/200] batch [25/63] time 0.505 (0.462) data 0.373 (0.331) loss_u loss_u 0.9297 (0.8791) acc_u 9.3750 (15.2500) lr 1.3681e-03 eta 0:00:17
epoch [78/200] batch [30/63] time 0.365 (0.459) data 0.233 (0.327) loss_u loss_u 0.8306 (0.8788) acc_u 21.8750 (15.2083) lr 1.3681e-03 eta 0:00:15
epoch [78/200] batch [35/63] time 0.406 (0.455) data 0.275 (0.323) loss_u loss_u 0.8735 (0.8796) acc_u 18.7500 (15.4464) lr 1.3681e-03 eta 0:00:12
epoch [78/200] batch [40/63] time 0.643 (0.455) data 0.512 (0.323) loss_u loss_u 0.9355 (0.8817) acc_u 3.1250 (15.1562) lr 1.3681e-03 eta 0:00:10
epoch [78/200] batch [45/63] time 0.339 (0.454) data 0.209 (0.323) loss_u loss_u 0.8198 (0.8796) acc_u 25.0000 (15.4861) lr 1.3681e-03 eta 0:00:08
epoch [78/200] batch [50/63] time 0.410 (0.455) data 0.277 (0.324) loss_u loss_u 0.8857 (0.8786) acc_u 12.5000 (15.5000) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [55/63] time 0.406 (0.453) data 0.275 (0.322) loss_u loss_u 0.8418 (0.8782) acc_u 15.6250 (15.4545) lr 1.3681e-03 eta 0:00:03
epoch [78/200] batch [60/63] time 0.490 (0.451) data 0.359 (0.319) loss_u loss_u 0.8975 (0.8800) acc_u 21.8750 (15.4167) lr 1.3681e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.3473, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1089
clean true:1085
clean false:4
clean_rate:0.9963269054178145
noisy true:534
noisy false:1513
after delete: len(clean_dataset) 1089
after delete: len(noisy_dataset) 2047
epoch [79/200] batch [5/34] time 0.419 (0.425) data 0.289 (0.294) loss_x loss_x 1.4473 (1.1949) acc_x 62.5000 (69.3750) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [10/34] time 0.398 (0.419) data 0.267 (0.289) loss_x loss_x 0.9751 (1.1634) acc_x 81.2500 (70.9375) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [15/34] time 0.504 (0.431) data 0.373 (0.301) loss_x loss_x 1.2930 (1.2170) acc_x 68.7500 (69.3750) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [20/34] time 0.509 (0.443) data 0.378 (0.313) loss_x loss_x 0.9497 (1.2165) acc_x 78.1250 (69.8438) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [25/34] time 0.501 (0.446) data 0.371 (0.316) loss_x loss_x 1.3184 (1.2461) acc_x 59.3750 (68.3750) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [30/34] time 0.574 (0.453) data 0.444 (0.323) loss_x loss_x 1.2949 (1.2278) acc_x 62.5000 (68.6458) lr 1.3535e-03 eta 0:00:01
epoch [79/200] batch [5/63] time 0.371 (0.443) data 0.240 (0.312) loss_u loss_u 0.9097 (0.8478) acc_u 9.3750 (17.5000) lr 1.3535e-03 eta 0:00:25
epoch [79/200] batch [10/63] time 0.587 (0.450) data 0.456 (0.319) loss_u loss_u 0.9463 (0.8682) acc_u 9.3750 (16.2500) lr 1.3535e-03 eta 0:00:23
epoch [79/200] batch [15/63] time 0.385 (0.452) data 0.254 (0.321) loss_u loss_u 0.9160 (0.8744) acc_u 12.5000 (15.6250) lr 1.3535e-03 eta 0:00:21
epoch [79/200] batch [20/63] time 0.368 (0.452) data 0.237 (0.322) loss_u loss_u 0.8159 (0.8675) acc_u 25.0000 (16.4062) lr 1.3535e-03 eta 0:00:19
epoch [79/200] batch [25/63] time 0.487 (0.449) data 0.357 (0.318) loss_u loss_u 0.8696 (0.8679) acc_u 18.7500 (16.3750) lr 1.3535e-03 eta 0:00:17
epoch [79/200] batch [30/63] time 0.320 (0.452) data 0.189 (0.321) loss_u loss_u 0.8867 (0.8665) acc_u 12.5000 (16.4583) lr 1.3535e-03 eta 0:00:14
epoch [79/200] batch [35/63] time 0.342 (0.448) data 0.212 (0.317) loss_u loss_u 0.8433 (0.8694) acc_u 25.0000 (16.2500) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [40/63] time 0.400 (0.447) data 0.269 (0.317) loss_u loss_u 0.9385 (0.8713) acc_u 9.3750 (16.1719) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [45/63] time 0.432 (0.447) data 0.301 (0.317) loss_u loss_u 0.8369 (0.8710) acc_u 18.7500 (16.2500) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [50/63] time 0.404 (0.446) data 0.273 (0.315) loss_u loss_u 0.8447 (0.8690) acc_u 15.6250 (16.5000) lr 1.3535e-03 eta 0:00:05
epoch [79/200] batch [55/63] time 0.369 (0.444) data 0.238 (0.313) loss_u loss_u 0.7529 (0.8695) acc_u 21.8750 (16.2500) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [60/63] time 0.370 (0.444) data 0.239 (0.313) loss_u loss_u 0.9849 (0.8695) acc_u 3.1250 (16.3542) lr 1.3535e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1494
confident_label rate tensor(0.3511, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1101
clean true:1099
clean false:2
clean_rate:0.9981834695731153
noisy true:543
noisy false:1492
after delete: len(clean_dataset) 1101
after delete: len(noisy_dataset) 2035
epoch [80/200] batch [5/34] time 0.558 (0.509) data 0.427 (0.378) loss_x loss_x 0.9224 (1.1236) acc_x 75.0000 (71.2500) lr 1.3387e-03 eta 0:00:14
epoch [80/200] batch [10/34] time 0.614 (0.522) data 0.482 (0.390) loss_x loss_x 1.3809 (1.1233) acc_x 65.6250 (72.5000) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [15/34] time 0.424 (0.505) data 0.294 (0.374) loss_x loss_x 1.6123 (1.1661) acc_x 56.2500 (70.6250) lr 1.3387e-03 eta 0:00:09
epoch [80/200] batch [20/34] time 0.521 (0.489) data 0.390 (0.358) loss_x loss_x 0.8833 (1.1589) acc_x 71.8750 (71.0938) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [25/34] time 0.414 (0.475) data 0.283 (0.344) loss_x loss_x 1.7832 (1.2010) acc_x 50.0000 (69.7500) lr 1.3387e-03 eta 0:00:04
epoch [80/200] batch [30/34] time 0.370 (0.469) data 0.239 (0.338) loss_x loss_x 1.1152 (1.2000) acc_x 62.5000 (69.2708) lr 1.3387e-03 eta 0:00:01
epoch [80/200] batch [5/63] time 0.501 (0.453) data 0.370 (0.322) loss_u loss_u 0.9155 (0.8976) acc_u 15.6250 (14.3750) lr 1.3387e-03 eta 0:00:26
epoch [80/200] batch [10/63] time 0.300 (0.444) data 0.168 (0.312) loss_u loss_u 0.9043 (0.8868) acc_u 12.5000 (14.6875) lr 1.3387e-03 eta 0:00:23
epoch [80/200] batch [15/63] time 0.468 (0.452) data 0.337 (0.321) loss_u loss_u 0.9531 (0.8851) acc_u 6.2500 (15.0000) lr 1.3387e-03 eta 0:00:21
epoch [80/200] batch [20/63] time 0.371 (0.456) data 0.240 (0.325) loss_u loss_u 0.9521 (0.8948) acc_u 9.3750 (14.2188) lr 1.3387e-03 eta 0:00:19
epoch [80/200] batch [25/63] time 0.445 (0.455) data 0.315 (0.324) loss_u loss_u 0.8789 (0.8904) acc_u 15.6250 (14.8750) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [30/63] time 0.446 (0.452) data 0.315 (0.321) loss_u loss_u 0.9385 (0.8903) acc_u 12.5000 (14.7917) lr 1.3387e-03 eta 0:00:14
epoch [80/200] batch [35/63] time 0.422 (0.450) data 0.291 (0.319) loss_u loss_u 0.8594 (0.8872) acc_u 15.6250 (15.1786) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [40/63] time 0.452 (0.447) data 0.321 (0.316) loss_u loss_u 0.8174 (0.8849) acc_u 25.0000 (15.3125) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [45/63] time 0.355 (0.443) data 0.224 (0.312) loss_u loss_u 0.8169 (0.8805) acc_u 28.1250 (15.9028) lr 1.3387e-03 eta 0:00:07
epoch [80/200] batch [50/63] time 0.362 (0.442) data 0.231 (0.311) loss_u loss_u 0.8613 (0.8774) acc_u 15.6250 (16.1875) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [55/63] time 0.686 (0.444) data 0.552 (0.312) loss_u loss_u 0.7612 (0.8717) acc_u 25.0000 (16.7045) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [60/63] time 0.377 (0.444) data 0.245 (0.313) loss_u loss_u 0.8462 (0.8717) acc_u 25.0000 (16.7188) lr 1.3387e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1481
confident_label rate tensor(0.3530, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1107
clean true:1105
clean false:2
clean_rate:0.998193315266486
noisy true:550
noisy false:1479
after delete: len(clean_dataset) 1107
after delete: len(noisy_dataset) 2029
epoch [81/200] batch [5/34] time 0.477 (0.502) data 0.346 (0.371) loss_x loss_x 1.0107 (1.1176) acc_x 84.3750 (74.3750) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [10/34] time 0.419 (0.460) data 0.288 (0.329) loss_x loss_x 1.0527 (1.1532) acc_x 78.1250 (72.5000) lr 1.3239e-03 eta 0:00:11
epoch [81/200] batch [15/34] time 0.442 (0.471) data 0.311 (0.340) loss_x loss_x 1.4580 (1.1428) acc_x 71.8750 (73.3333) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [20/34] time 0.415 (0.456) data 0.285 (0.325) loss_x loss_x 1.5527 (1.2159) acc_x 62.5000 (71.2500) lr 1.3239e-03 eta 0:00:06
epoch [81/200] batch [25/34] time 0.494 (0.450) data 0.363 (0.319) loss_x loss_x 1.3232 (1.1993) acc_x 68.7500 (72.0000) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [30/34] time 0.476 (0.448) data 0.345 (0.317) loss_x loss_x 0.5063 (1.2407) acc_x 87.5000 (70.7292) lr 1.3239e-03 eta 0:00:01
epoch [81/200] batch [5/63] time 0.375 (0.451) data 0.243 (0.321) loss_u loss_u 0.8931 (0.8671) acc_u 12.5000 (14.3750) lr 1.3239e-03 eta 0:00:26
epoch [81/200] batch [10/63] time 0.348 (0.448) data 0.216 (0.317) loss_u loss_u 0.8735 (0.8677) acc_u 12.5000 (14.6875) lr 1.3239e-03 eta 0:00:23
epoch [81/200] batch [15/63] time 0.340 (0.446) data 0.209 (0.315) loss_u loss_u 0.8911 (0.8609) acc_u 15.6250 (16.8750) lr 1.3239e-03 eta 0:00:21
epoch [81/200] batch [20/63] time 0.465 (0.451) data 0.332 (0.320) loss_u loss_u 0.9033 (0.8703) acc_u 9.3750 (15.6250) lr 1.3239e-03 eta 0:00:19
epoch [81/200] batch [25/63] time 0.424 (0.448) data 0.292 (0.317) loss_u loss_u 0.8613 (0.8678) acc_u 21.8750 (16.5000) lr 1.3239e-03 eta 0:00:17
epoch [81/200] batch [30/63] time 0.371 (0.447) data 0.240 (0.315) loss_u loss_u 0.8599 (0.8687) acc_u 18.7500 (16.4583) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [35/63] time 0.381 (0.443) data 0.249 (0.312) loss_u loss_u 0.9014 (0.8742) acc_u 12.5000 (15.8036) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [40/63] time 0.416 (0.443) data 0.285 (0.312) loss_u loss_u 0.9023 (0.8747) acc_u 12.5000 (15.9375) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [45/63] time 0.362 (0.441) data 0.230 (0.310) loss_u loss_u 0.8989 (0.8766) acc_u 18.7500 (16.0417) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [50/63] time 0.412 (0.439) data 0.280 (0.308) loss_u loss_u 0.8057 (0.8757) acc_u 28.1250 (16.0625) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [55/63] time 0.436 (0.440) data 0.305 (0.308) loss_u loss_u 0.9058 (0.8787) acc_u 12.5000 (15.5682) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [60/63] time 0.560 (0.438) data 0.429 (0.307) loss_u loss_u 0.8848 (0.8797) acc_u 18.7500 (15.5729) lr 1.3239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1557
confident_label rate tensor(0.3428, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1075
clean true:1069
clean false:6
clean_rate:0.9944186046511628
noisy true:510
noisy false:1551
after delete: len(clean_dataset) 1075
after delete: len(noisy_dataset) 2061
epoch [82/200] batch [5/33] time 0.485 (0.503) data 0.354 (0.372) loss_x loss_x 0.9907 (1.0884) acc_x 78.1250 (71.8750) lr 1.3090e-03 eta 0:00:14
epoch [82/200] batch [10/33] time 0.428 (0.453) data 0.297 (0.322) loss_x loss_x 1.0762 (1.1737) acc_x 65.6250 (69.6875) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [15/33] time 0.411 (0.437) data 0.281 (0.306) loss_x loss_x 1.2275 (1.2047) acc_x 65.6250 (68.5417) lr 1.3090e-03 eta 0:00:07
epoch [82/200] batch [20/33] time 0.552 (0.460) data 0.422 (0.329) loss_x loss_x 0.7969 (1.1774) acc_x 71.8750 (69.0625) lr 1.3090e-03 eta 0:00:05
epoch [82/200] batch [25/33] time 0.386 (0.463) data 0.256 (0.333) loss_x loss_x 0.6904 (1.1187) acc_x 90.6250 (71.6250) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [30/33] time 0.399 (0.460) data 0.267 (0.329) loss_x loss_x 1.4346 (1.1748) acc_x 68.7500 (70.6250) lr 1.3090e-03 eta 0:00:01
epoch [82/200] batch [5/64] time 0.365 (0.456) data 0.233 (0.325) loss_u loss_u 0.9009 (0.8678) acc_u 12.5000 (16.2500) lr 1.3090e-03 eta 0:00:26
epoch [82/200] batch [10/64] time 0.535 (0.455) data 0.403 (0.324) loss_u loss_u 0.9272 (0.8634) acc_u 9.3750 (16.8750) lr 1.3090e-03 eta 0:00:24
epoch [82/200] batch [15/64] time 0.414 (0.453) data 0.282 (0.322) loss_u loss_u 0.8818 (0.8754) acc_u 15.6250 (15.8333) lr 1.3090e-03 eta 0:00:22
epoch [82/200] batch [20/64] time 0.501 (0.451) data 0.369 (0.320) loss_u loss_u 0.8755 (0.8810) acc_u 15.6250 (15.0000) lr 1.3090e-03 eta 0:00:19
epoch [82/200] batch [25/64] time 0.437 (0.454) data 0.305 (0.323) loss_u loss_u 0.8750 (0.8774) acc_u 12.5000 (15.2500) lr 1.3090e-03 eta 0:00:17
epoch [82/200] batch [30/64] time 0.431 (0.450) data 0.300 (0.319) loss_u loss_u 0.8892 (0.8771) acc_u 12.5000 (15.1042) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [35/64] time 0.391 (0.443) data 0.259 (0.312) loss_u loss_u 0.8872 (0.8767) acc_u 12.5000 (15.2679) lr 1.3090e-03 eta 0:00:12
epoch [82/200] batch [40/64] time 0.343 (0.444) data 0.211 (0.313) loss_u loss_u 0.7710 (0.8747) acc_u 31.2500 (16.0156) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [45/64] time 0.464 (0.442) data 0.332 (0.311) loss_u loss_u 0.8916 (0.8759) acc_u 12.5000 (15.9722) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [50/64] time 0.376 (0.440) data 0.244 (0.309) loss_u loss_u 0.8369 (0.8724) acc_u 18.7500 (16.3750) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [55/64] time 0.403 (0.437) data 0.272 (0.306) loss_u loss_u 0.8394 (0.8712) acc_u 21.8750 (16.5909) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [60/64] time 0.284 (0.436) data 0.153 (0.304) loss_u loss_u 0.8735 (0.8707) acc_u 12.5000 (16.6146) lr 1.3090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1462
confident_label rate tensor(0.3667, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1150
clean true:1148
clean false:2
clean_rate:0.9982608695652174
noisy true:526
noisy false:1460
after delete: len(clean_dataset) 1150
after delete: len(noisy_dataset) 1986
epoch [83/200] batch [5/35] time 0.362 (0.425) data 0.231 (0.294) loss_x loss_x 1.2744 (1.0211) acc_x 75.0000 (76.8750) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [10/35] time 0.346 (0.417) data 0.215 (0.286) loss_x loss_x 1.1787 (1.0408) acc_x 71.8750 (75.3125) lr 1.2940e-03 eta 0:00:10
epoch [83/200] batch [15/35] time 0.447 (0.420) data 0.316 (0.290) loss_x loss_x 0.8843 (1.0956) acc_x 81.2500 (73.1250) lr 1.2940e-03 eta 0:00:08
epoch [83/200] batch [20/35] time 0.413 (0.431) data 0.283 (0.300) loss_x loss_x 1.2812 (1.1084) acc_x 75.0000 (73.1250) lr 1.2940e-03 eta 0:00:06
epoch [83/200] batch [25/35] time 0.479 (0.435) data 0.348 (0.304) loss_x loss_x 0.6016 (1.1056) acc_x 90.6250 (72.8750) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [30/35] time 0.480 (0.441) data 0.349 (0.311) loss_x loss_x 0.8892 (1.1013) acc_x 78.1250 (72.6042) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [35/35] time 0.555 (0.449) data 0.424 (0.319) loss_x loss_x 1.6465 (1.1424) acc_x 59.3750 (71.7857) lr 1.2940e-03 eta 0:00:00
epoch [83/200] batch [5/62] time 0.536 (0.449) data 0.404 (0.318) loss_u loss_u 0.9526 (0.9000) acc_u 9.3750 (13.7500) lr 1.2940e-03 eta 0:00:25
epoch [83/200] batch [10/62] time 0.594 (0.448) data 0.462 (0.317) loss_u loss_u 0.9316 (0.8864) acc_u 9.3750 (15.3125) lr 1.2940e-03 eta 0:00:23
epoch [83/200] batch [15/62] time 0.384 (0.444) data 0.253 (0.313) loss_u loss_u 0.8789 (0.8779) acc_u 18.7500 (16.4583) lr 1.2940e-03 eta 0:00:20
epoch [83/200] batch [20/62] time 0.401 (0.445) data 0.267 (0.314) loss_u loss_u 0.8911 (0.8831) acc_u 15.6250 (15.7812) lr 1.2940e-03 eta 0:00:18
epoch [83/200] batch [25/62] time 0.449 (0.443) data 0.317 (0.312) loss_u loss_u 0.8984 (0.8834) acc_u 12.5000 (15.5000) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [30/62] time 0.596 (0.446) data 0.461 (0.315) loss_u loss_u 0.8228 (0.8845) acc_u 21.8750 (15.3125) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [35/62] time 0.386 (0.446) data 0.255 (0.314) loss_u loss_u 0.8140 (0.8790) acc_u 25.0000 (15.8036) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [40/62] time 0.368 (0.444) data 0.237 (0.312) loss_u loss_u 0.9175 (0.8783) acc_u 12.5000 (15.8594) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [45/62] time 0.367 (0.446) data 0.235 (0.315) loss_u loss_u 0.8511 (0.8755) acc_u 18.7500 (16.3889) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [50/62] time 0.419 (0.447) data 0.287 (0.316) loss_u loss_u 0.8818 (0.8766) acc_u 15.6250 (16.1875) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [55/62] time 0.362 (0.447) data 0.230 (0.315) loss_u loss_u 0.9062 (0.8778) acc_u 15.6250 (16.0227) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [60/62] time 0.563 (0.446) data 0.432 (0.314) loss_u loss_u 0.8711 (0.8802) acc_u 15.6250 (15.7292) lr 1.2940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1472
confident_label rate tensor(0.3632, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1139
clean true:1137
clean false:2
clean_rate:0.9982440737489026
noisy true:527
noisy false:1470
after delete: len(clean_dataset) 1139
after delete: len(noisy_dataset) 1997
epoch [84/200] batch [5/35] time 0.410 (0.451) data 0.280 (0.321) loss_x loss_x 1.3975 (1.3713) acc_x 62.5000 (64.3750) lr 1.2790e-03 eta 0:00:13
epoch [84/200] batch [10/35] time 0.443 (0.451) data 0.313 (0.321) loss_x loss_x 1.6201 (1.3961) acc_x 59.3750 (63.1250) lr 1.2790e-03 eta 0:00:11
epoch [84/200] batch [15/35] time 0.437 (0.452) data 0.306 (0.321) loss_x loss_x 2.1348 (1.3492) acc_x 50.0000 (63.5417) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [20/35] time 0.429 (0.450) data 0.298 (0.320) loss_x loss_x 1.0352 (1.3503) acc_x 81.2500 (65.7812) lr 1.2790e-03 eta 0:00:06
epoch [84/200] batch [25/35] time 0.453 (0.456) data 0.323 (0.325) loss_x loss_x 1.0625 (1.2775) acc_x 78.1250 (68.2500) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [30/35] time 0.437 (0.462) data 0.305 (0.330) loss_x loss_x 1.1318 (1.3032) acc_x 75.0000 (67.7083) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [35/35] time 0.427 (0.464) data 0.294 (0.333) loss_x loss_x 0.8384 (1.2789) acc_x 78.1250 (67.9464) lr 1.2790e-03 eta 0:00:00
epoch [84/200] batch [5/62] time 0.545 (0.460) data 0.414 (0.329) loss_u loss_u 0.8950 (0.8812) acc_u 18.7500 (15.0000) lr 1.2790e-03 eta 0:00:26
epoch [84/200] batch [10/62] time 0.520 (0.459) data 0.388 (0.327) loss_u loss_u 0.8853 (0.8838) acc_u 18.7500 (15.0000) lr 1.2790e-03 eta 0:00:23
epoch [84/200] batch [15/62] time 0.478 (0.455) data 0.347 (0.323) loss_u loss_u 0.8813 (0.8755) acc_u 9.3750 (16.0417) lr 1.2790e-03 eta 0:00:21
epoch [84/200] batch [20/62] time 0.355 (0.451) data 0.223 (0.319) loss_u loss_u 0.8931 (0.8799) acc_u 18.7500 (16.2500) lr 1.2790e-03 eta 0:00:18
epoch [84/200] batch [25/62] time 0.478 (0.449) data 0.347 (0.318) loss_u loss_u 0.9360 (0.8861) acc_u 3.1250 (15.1250) lr 1.2790e-03 eta 0:00:16
epoch [84/200] batch [30/62] time 0.402 (0.450) data 0.271 (0.318) loss_u loss_u 0.8535 (0.8852) acc_u 18.7500 (15.4167) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [35/62] time 0.368 (0.450) data 0.237 (0.319) loss_u loss_u 0.8755 (0.8823) acc_u 12.5000 (15.5357) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [40/62] time 0.540 (0.449) data 0.408 (0.317) loss_u loss_u 0.8438 (0.8851) acc_u 15.6250 (15.0781) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [45/62] time 0.381 (0.447) data 0.250 (0.316) loss_u loss_u 0.8403 (0.8776) acc_u 25.0000 (15.9028) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [50/62] time 0.438 (0.447) data 0.306 (0.316) loss_u loss_u 0.7695 (0.8799) acc_u 28.1250 (15.4375) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [55/62] time 0.383 (0.444) data 0.252 (0.313) loss_u loss_u 0.8979 (0.8777) acc_u 12.5000 (15.4545) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [60/62] time 0.398 (0.444) data 0.267 (0.312) loss_u loss_u 0.7896 (0.8759) acc_u 28.1250 (15.7812) lr 1.2790e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1539
confident_label rate tensor(0.3466, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1087
clean true:1082
clean false:5
clean_rate:0.9954001839926403
noisy true:515
noisy false:1534
after delete: len(clean_dataset) 1087
after delete: len(noisy_dataset) 2049
epoch [85/200] batch [5/33] time 0.372 (0.470) data 0.242 (0.339) loss_x loss_x 0.9492 (1.0163) acc_x 78.1250 (72.5000) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [10/33] time 0.373 (0.459) data 0.243 (0.328) loss_x loss_x 0.8291 (1.0697) acc_x 81.2500 (71.8750) lr 1.2639e-03 eta 0:00:10
epoch [85/200] batch [15/33] time 0.444 (0.447) data 0.314 (0.317) loss_x loss_x 1.1729 (1.0778) acc_x 78.1250 (72.0833) lr 1.2639e-03 eta 0:00:08
epoch [85/200] batch [20/33] time 0.448 (0.449) data 0.317 (0.319) loss_x loss_x 1.1963 (1.1450) acc_x 78.1250 (70.7812) lr 1.2639e-03 eta 0:00:05
epoch [85/200] batch [25/33] time 0.414 (0.450) data 0.283 (0.319) loss_x loss_x 1.4326 (1.1478) acc_x 62.5000 (70.8750) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [30/33] time 0.481 (0.460) data 0.350 (0.330) loss_x loss_x 1.1719 (1.1466) acc_x 71.8750 (71.1458) lr 1.2639e-03 eta 0:00:01
epoch [85/200] batch [5/64] time 0.384 (0.457) data 0.252 (0.326) loss_u loss_u 0.8491 (0.8865) acc_u 25.0000 (16.8750) lr 1.2639e-03 eta 0:00:26
epoch [85/200] batch [10/64] time 0.436 (0.451) data 0.305 (0.320) loss_u loss_u 0.8174 (0.8640) acc_u 21.8750 (17.1875) lr 1.2639e-03 eta 0:00:24
epoch [85/200] batch [15/64] time 0.722 (0.454) data 0.590 (0.323) loss_u loss_u 0.9507 (0.8724) acc_u 6.2500 (17.2917) lr 1.2639e-03 eta 0:00:22
epoch [85/200] batch [20/64] time 0.420 (0.456) data 0.289 (0.325) loss_u loss_u 0.8721 (0.8698) acc_u 15.6250 (17.3438) lr 1.2639e-03 eta 0:00:20
epoch [85/200] batch [25/64] time 0.468 (0.453) data 0.336 (0.322) loss_u loss_u 0.9126 (0.8727) acc_u 9.3750 (17.1250) lr 1.2639e-03 eta 0:00:17
epoch [85/200] batch [30/64] time 0.510 (0.454) data 0.379 (0.323) loss_u loss_u 0.9287 (0.8784) acc_u 3.1250 (16.1458) lr 1.2639e-03 eta 0:00:15
epoch [85/200] batch [35/64] time 0.528 (0.450) data 0.396 (0.319) loss_u loss_u 0.9419 (0.8769) acc_u 6.2500 (16.3393) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [40/64] time 0.452 (0.447) data 0.319 (0.316) loss_u loss_u 0.8745 (0.8758) acc_u 15.6250 (16.4062) lr 1.2639e-03 eta 0:00:10
epoch [85/200] batch [45/64] time 0.435 (0.446) data 0.303 (0.315) loss_u loss_u 0.7930 (0.8720) acc_u 25.0000 (16.7361) lr 1.2639e-03 eta 0:00:08
epoch [85/200] batch [50/64] time 0.411 (0.447) data 0.279 (0.316) loss_u loss_u 0.9058 (0.8727) acc_u 12.5000 (16.6875) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [55/64] time 0.395 (0.447) data 0.264 (0.315) loss_u loss_u 0.8545 (0.8711) acc_u 15.6250 (16.9318) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [60/64] time 0.566 (0.446) data 0.434 (0.315) loss_u loss_u 0.8677 (0.8724) acc_u 15.6250 (16.8229) lr 1.2639e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1469
confident_label rate tensor(0.3559, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1116
clean true:1109
clean false:7
clean_rate:0.9937275985663082
noisy true:558
noisy false:1462
after delete: len(clean_dataset) 1116
after delete: len(noisy_dataset) 2020
epoch [86/200] batch [5/34] time 0.544 (0.516) data 0.413 (0.385) loss_x loss_x 0.6514 (1.0221) acc_x 84.3750 (75.6250) lr 1.2487e-03 eta 0:00:14
epoch [86/200] batch [10/34] time 0.444 (0.496) data 0.313 (0.365) loss_x loss_x 0.8042 (1.0891) acc_x 87.5000 (73.7500) lr 1.2487e-03 eta 0:00:11
epoch [86/200] batch [15/34] time 0.423 (0.491) data 0.292 (0.360) loss_x loss_x 0.7432 (1.1173) acc_x 78.1250 (73.9583) lr 1.2487e-03 eta 0:00:09
epoch [86/200] batch [20/34] time 0.521 (0.493) data 0.390 (0.362) loss_x loss_x 0.9170 (1.1422) acc_x 81.2500 (72.1875) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [25/34] time 0.675 (0.483) data 0.544 (0.352) loss_x loss_x 1.0098 (1.1786) acc_x 81.2500 (71.1250) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [30/34] time 0.408 (0.475) data 0.276 (0.345) loss_x loss_x 1.2285 (1.1659) acc_x 68.7500 (71.3542) lr 1.2487e-03 eta 0:00:01
epoch [86/200] batch [5/63] time 0.436 (0.472) data 0.305 (0.341) loss_u loss_u 0.9448 (0.8816) acc_u 6.2500 (13.1250) lr 1.2487e-03 eta 0:00:27
epoch [86/200] batch [10/63] time 0.491 (0.465) data 0.360 (0.334) loss_u loss_u 0.9116 (0.8835) acc_u 12.5000 (13.1250) lr 1.2487e-03 eta 0:00:24
epoch [86/200] batch [15/63] time 0.337 (0.461) data 0.205 (0.330) loss_u loss_u 0.9434 (0.8789) acc_u 9.3750 (14.1667) lr 1.2487e-03 eta 0:00:22
epoch [86/200] batch [20/63] time 0.359 (0.461) data 0.228 (0.330) loss_u loss_u 0.8618 (0.8824) acc_u 15.6250 (14.2188) lr 1.2487e-03 eta 0:00:19
epoch [86/200] batch [25/63] time 0.421 (0.461) data 0.291 (0.330) loss_u loss_u 0.9238 (0.8838) acc_u 9.3750 (14.1250) lr 1.2487e-03 eta 0:00:17
epoch [86/200] batch [30/63] time 0.342 (0.458) data 0.211 (0.327) loss_u loss_u 0.8071 (0.8774) acc_u 25.0000 (15.1042) lr 1.2487e-03 eta 0:00:15
epoch [86/200] batch [35/63] time 0.337 (0.456) data 0.205 (0.325) loss_u loss_u 0.9087 (0.8791) acc_u 6.2500 (14.9107) lr 1.2487e-03 eta 0:00:12
epoch [86/200] batch [40/63] time 0.415 (0.455) data 0.283 (0.323) loss_u loss_u 0.8477 (0.8818) acc_u 15.6250 (14.5312) lr 1.2487e-03 eta 0:00:10
epoch [86/200] batch [45/63] time 0.618 (0.455) data 0.487 (0.323) loss_u loss_u 0.8223 (0.8783) acc_u 21.8750 (15.2083) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [50/63] time 0.364 (0.454) data 0.233 (0.322) loss_u loss_u 0.8550 (0.8739) acc_u 18.7500 (15.6875) lr 1.2487e-03 eta 0:00:05
epoch [86/200] batch [55/63] time 0.396 (0.452) data 0.264 (0.320) loss_u loss_u 0.7969 (0.8753) acc_u 31.2500 (15.5682) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [60/63] time 0.404 (0.447) data 0.272 (0.316) loss_u loss_u 0.9165 (0.8776) acc_u 9.3750 (15.3125) lr 1.2487e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1455
confident_label rate tensor(0.3667, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1150
clean true:1141
clean false:9
clean_rate:0.9921739130434782
noisy true:540
noisy false:1446
after delete: len(clean_dataset) 1150
after delete: len(noisy_dataset) 1986
epoch [87/200] batch [5/35] time 0.430 (0.507) data 0.299 (0.376) loss_x loss_x 1.3076 (1.0913) acc_x 71.8750 (74.3750) lr 1.2334e-03 eta 0:00:15
epoch [87/200] batch [10/35] time 0.381 (0.493) data 0.251 (0.362) loss_x loss_x 1.0479 (1.0402) acc_x 56.2500 (71.8750) lr 1.2334e-03 eta 0:00:12
epoch [87/200] batch [15/35] time 0.488 (0.470) data 0.358 (0.339) loss_x loss_x 1.5322 (1.1031) acc_x 65.6250 (71.8750) lr 1.2334e-03 eta 0:00:09
epoch [87/200] batch [20/35] time 0.401 (0.471) data 0.271 (0.341) loss_x loss_x 0.8760 (1.1154) acc_x 68.7500 (71.4062) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [25/35] time 0.498 (0.466) data 0.367 (0.336) loss_x loss_x 1.0889 (1.1445) acc_x 68.7500 (71.0000) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [30/35] time 0.406 (0.459) data 0.275 (0.329) loss_x loss_x 0.7642 (1.1369) acc_x 81.2500 (71.5625) lr 1.2334e-03 eta 0:00:02
epoch [87/200] batch [35/35] time 0.460 (0.463) data 0.329 (0.333) loss_x loss_x 1.0869 (1.1299) acc_x 75.0000 (71.8750) lr 1.2334e-03 eta 0:00:00
epoch [87/200] batch [5/62] time 0.319 (0.457) data 0.188 (0.327) loss_u loss_u 0.9146 (0.8554) acc_u 6.2500 (16.8750) lr 1.2334e-03 eta 0:00:26
epoch [87/200] batch [10/62] time 0.422 (0.452) data 0.291 (0.321) loss_u loss_u 0.8442 (0.8716) acc_u 21.8750 (16.2500) lr 1.2334e-03 eta 0:00:23
epoch [87/200] batch [15/62] time 0.471 (0.447) data 0.339 (0.317) loss_u loss_u 0.9282 (0.8785) acc_u 6.2500 (14.3750) lr 1.2334e-03 eta 0:00:21
epoch [87/200] batch [20/62] time 0.382 (0.445) data 0.251 (0.314) loss_u loss_u 0.7686 (0.8817) acc_u 31.2500 (14.2188) lr 1.2334e-03 eta 0:00:18
epoch [87/200] batch [25/62] time 0.507 (0.443) data 0.375 (0.313) loss_u loss_u 0.8687 (0.8831) acc_u 12.5000 (14.1250) lr 1.2334e-03 eta 0:00:16
epoch [87/200] batch [30/62] time 0.447 (0.445) data 0.316 (0.315) loss_u loss_u 0.8154 (0.8878) acc_u 28.1250 (14.1667) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [35/62] time 0.475 (0.444) data 0.343 (0.313) loss_u loss_u 0.8486 (0.8837) acc_u 18.7500 (14.6429) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [40/62] time 0.425 (0.446) data 0.295 (0.316) loss_u loss_u 0.8608 (0.8808) acc_u 21.8750 (15.0781) lr 1.2334e-03 eta 0:00:09
epoch [87/200] batch [45/62] time 0.414 (0.449) data 0.280 (0.318) loss_u loss_u 0.7905 (0.8786) acc_u 31.2500 (15.2083) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [50/62] time 0.307 (0.445) data 0.176 (0.314) loss_u loss_u 0.8721 (0.8757) acc_u 15.6250 (15.6250) lr 1.2334e-03 eta 0:00:05
epoch [87/200] batch [55/62] time 0.315 (0.442) data 0.184 (0.311) loss_u loss_u 0.9116 (0.8782) acc_u 12.5000 (15.5114) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [60/62] time 0.378 (0.443) data 0.246 (0.312) loss_u loss_u 0.8857 (0.8788) acc_u 15.6250 (15.4167) lr 1.2334e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1458
confident_label rate tensor(0.3664, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1149
clean true:1144
clean false:5
clean_rate:0.9956483899042646
noisy true:534
noisy false:1453
after delete: len(clean_dataset) 1149
after delete: len(noisy_dataset) 1987
epoch [88/200] batch [5/35] time 0.355 (0.463) data 0.225 (0.332) loss_x loss_x 1.2617 (1.1351) acc_x 68.7500 (70.6250) lr 1.2181e-03 eta 0:00:13
epoch [88/200] batch [10/35] time 0.356 (0.447) data 0.225 (0.317) loss_x loss_x 0.8862 (1.1515) acc_x 78.1250 (71.8750) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [15/35] time 0.408 (0.438) data 0.278 (0.307) loss_x loss_x 1.9814 (1.1841) acc_x 59.3750 (71.6667) lr 1.2181e-03 eta 0:00:08
epoch [88/200] batch [20/35] time 0.432 (0.445) data 0.302 (0.314) loss_x loss_x 1.0840 (1.1729) acc_x 78.1250 (71.8750) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [25/35] time 0.523 (0.455) data 0.393 (0.325) loss_x loss_x 0.8574 (1.1386) acc_x 81.2500 (72.8750) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [30/35] time 0.542 (0.461) data 0.412 (0.331) loss_x loss_x 1.4033 (1.1667) acc_x 71.8750 (72.1875) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [35/35] time 0.409 (0.468) data 0.278 (0.338) loss_x loss_x 1.3701 (1.1815) acc_x 56.2500 (71.2500) lr 1.2181e-03 eta 0:00:00
epoch [88/200] batch [5/62] time 0.369 (0.470) data 0.237 (0.340) loss_u loss_u 0.8765 (0.8897) acc_u 18.7500 (12.5000) lr 1.2181e-03 eta 0:00:26
epoch [88/200] batch [10/62] time 0.454 (0.471) data 0.323 (0.340) loss_u loss_u 0.9268 (0.8825) acc_u 9.3750 (13.4375) lr 1.2181e-03 eta 0:00:24
epoch [88/200] batch [15/62] time 0.465 (0.471) data 0.334 (0.341) loss_u loss_u 0.9155 (0.8796) acc_u 18.7500 (14.5833) lr 1.2181e-03 eta 0:00:22
epoch [88/200] batch [20/62] time 0.452 (0.465) data 0.321 (0.334) loss_u loss_u 0.8525 (0.8740) acc_u 15.6250 (15.6250) lr 1.2181e-03 eta 0:00:19
epoch [88/200] batch [25/62] time 0.515 (0.460) data 0.385 (0.329) loss_u loss_u 0.9595 (0.8778) acc_u 3.1250 (15.2500) lr 1.2181e-03 eta 0:00:17
epoch [88/200] batch [30/62] time 0.358 (0.457) data 0.228 (0.327) loss_u loss_u 0.8706 (0.8725) acc_u 12.5000 (15.8333) lr 1.2181e-03 eta 0:00:14
epoch [88/200] batch [35/62] time 0.389 (0.454) data 0.257 (0.324) loss_u loss_u 0.8555 (0.8701) acc_u 25.0000 (16.5179) lr 1.2181e-03 eta 0:00:12
epoch [88/200] batch [40/62] time 0.404 (0.449) data 0.274 (0.319) loss_u loss_u 0.9106 (0.8751) acc_u 15.6250 (15.9375) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [45/62] time 0.402 (0.446) data 0.271 (0.315) loss_u loss_u 0.8677 (0.8740) acc_u 15.6250 (16.2500) lr 1.2181e-03 eta 0:00:07
epoch [88/200] batch [50/62] time 0.467 (0.448) data 0.336 (0.317) loss_u loss_u 0.9365 (0.8769) acc_u 9.3750 (15.8125) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [55/62] time 0.509 (0.447) data 0.378 (0.316) loss_u loss_u 0.8926 (0.8763) acc_u 15.6250 (15.9091) lr 1.2181e-03 eta 0:00:03
epoch [88/200] batch [60/62] time 0.413 (0.450) data 0.282 (0.320) loss_u loss_u 0.9111 (0.8790) acc_u 12.5000 (15.5729) lr 1.2181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1461
confident_label rate tensor(0.3504, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1099
clean true:1093
clean false:6
clean_rate:0.9945404913557779
noisy true:582
noisy false:1455
after delete: len(clean_dataset) 1099
after delete: len(noisy_dataset) 2037
epoch [89/200] batch [5/34] time 0.433 (0.413) data 0.302 (0.282) loss_x loss_x 1.2793 (1.2429) acc_x 59.3750 (68.1250) lr 1.2028e-03 eta 0:00:11
epoch [89/200] batch [10/34] time 0.474 (0.448) data 0.343 (0.317) loss_x loss_x 1.1299 (1.1689) acc_x 78.1250 (70.9375) lr 1.2028e-03 eta 0:00:10
epoch [89/200] batch [15/34] time 0.470 (0.464) data 0.339 (0.333) loss_x loss_x 1.0967 (1.1508) acc_x 75.0000 (70.6250) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [20/34] time 0.451 (0.463) data 0.320 (0.332) loss_x loss_x 1.1416 (1.1883) acc_x 68.7500 (69.2188) lr 1.2028e-03 eta 0:00:06
epoch [89/200] batch [25/34] time 0.405 (0.457) data 0.274 (0.326) loss_x loss_x 1.2676 (1.2019) acc_x 65.6250 (69.7500) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [30/34] time 0.353 (0.457) data 0.222 (0.326) loss_x loss_x 1.5840 (1.2164) acc_x 65.6250 (69.6875) lr 1.2028e-03 eta 0:00:01
epoch [89/200] batch [5/63] time 0.465 (0.455) data 0.335 (0.324) loss_u loss_u 0.8730 (0.8436) acc_u 18.7500 (21.8750) lr 1.2028e-03 eta 0:00:26
epoch [89/200] batch [10/63] time 0.421 (0.452) data 0.290 (0.321) loss_u loss_u 0.8296 (0.8588) acc_u 21.8750 (18.4375) lr 1.2028e-03 eta 0:00:23
epoch [89/200] batch [15/63] time 0.585 (0.455) data 0.454 (0.324) loss_u loss_u 0.7910 (0.8674) acc_u 21.8750 (17.0833) lr 1.2028e-03 eta 0:00:21
epoch [89/200] batch [20/63] time 0.390 (0.453) data 0.258 (0.322) loss_u loss_u 0.8853 (0.8697) acc_u 12.5000 (16.8750) lr 1.2028e-03 eta 0:00:19
epoch [89/200] batch [25/63] time 0.358 (0.448) data 0.227 (0.317) loss_u loss_u 0.7407 (0.8605) acc_u 34.3750 (18.1250) lr 1.2028e-03 eta 0:00:17
epoch [89/200] batch [30/63] time 0.419 (0.449) data 0.288 (0.318) loss_u loss_u 0.8945 (0.8618) acc_u 9.3750 (18.0208) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [35/63] time 0.340 (0.447) data 0.209 (0.316) loss_u loss_u 0.7109 (0.8596) acc_u 31.2500 (18.3036) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [40/63] time 0.456 (0.445) data 0.325 (0.314) loss_u loss_u 0.8145 (0.8650) acc_u 25.0000 (17.5781) lr 1.2028e-03 eta 0:00:10
epoch [89/200] batch [45/63] time 0.391 (0.449) data 0.260 (0.318) loss_u loss_u 0.8857 (0.8658) acc_u 18.7500 (17.7083) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [50/63] time 0.439 (0.448) data 0.308 (0.317) loss_u loss_u 0.9375 (0.8675) acc_u 6.2500 (17.5625) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [55/63] time 0.381 (0.447) data 0.249 (0.315) loss_u loss_u 0.9312 (0.8672) acc_u 12.5000 (17.5568) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [60/63] time 0.412 (0.445) data 0.281 (0.314) loss_u loss_u 0.8755 (0.8681) acc_u 18.7500 (17.4479) lr 1.2028e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1452
confident_label rate tensor(0.3661, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1148
clean true:1140
clean false:8
clean_rate:0.9930313588850174
noisy true:544
noisy false:1444
after delete: len(clean_dataset) 1148
after delete: len(noisy_dataset) 1988
epoch [90/200] batch [5/35] time 0.475 (0.439) data 0.344 (0.308) loss_x loss_x 0.7031 (0.7998) acc_x 78.1250 (78.7500) lr 1.1874e-03 eta 0:00:13
epoch [90/200] batch [10/35] time 0.533 (0.438) data 0.403 (0.307) loss_x loss_x 0.9678 (0.9711) acc_x 78.1250 (74.0625) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [15/35] time 0.375 (0.442) data 0.245 (0.312) loss_x loss_x 1.3252 (1.0745) acc_x 65.6250 (72.0833) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [20/35] time 0.597 (0.454) data 0.465 (0.323) loss_x loss_x 0.8965 (1.0715) acc_x 78.1250 (72.1875) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [25/35] time 0.410 (0.464) data 0.280 (0.333) loss_x loss_x 0.8271 (1.1050) acc_x 68.7500 (70.7500) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [30/35] time 0.643 (0.471) data 0.513 (0.341) loss_x loss_x 1.1992 (1.0714) acc_x 75.0000 (71.9792) lr 1.1874e-03 eta 0:00:02
epoch [90/200] batch [35/35] time 0.483 (0.477) data 0.352 (0.347) loss_x loss_x 1.4717 (1.0949) acc_x 62.5000 (71.7857) lr 1.1874e-03 eta 0:00:00
epoch [90/200] batch [5/62] time 0.483 (0.477) data 0.352 (0.347) loss_u loss_u 0.8691 (0.8769) acc_u 18.7500 (17.5000) lr 1.1874e-03 eta 0:00:27
epoch [90/200] batch [10/62] time 0.378 (0.469) data 0.248 (0.338) loss_u loss_u 0.8574 (0.8753) acc_u 21.8750 (17.8125) lr 1.1874e-03 eta 0:00:24
epoch [90/200] batch [15/62] time 0.375 (0.468) data 0.243 (0.337) loss_u loss_u 0.8784 (0.8864) acc_u 18.7500 (15.4167) lr 1.1874e-03 eta 0:00:22
epoch [90/200] batch [20/62] time 0.356 (0.461) data 0.225 (0.331) loss_u loss_u 0.8535 (0.8824) acc_u 12.5000 (15.4688) lr 1.1874e-03 eta 0:00:19
epoch [90/200] batch [25/62] time 0.423 (0.458) data 0.292 (0.328) loss_u loss_u 0.8188 (0.8787) acc_u 21.8750 (15.6250) lr 1.1874e-03 eta 0:00:16
epoch [90/200] batch [30/62] time 0.438 (0.459) data 0.307 (0.328) loss_u loss_u 0.9326 (0.8828) acc_u 9.3750 (15.0000) lr 1.1874e-03 eta 0:00:14
epoch [90/200] batch [35/62] time 0.483 (0.463) data 0.351 (0.332) loss_u loss_u 0.8721 (0.8812) acc_u 15.6250 (15.1786) lr 1.1874e-03 eta 0:00:12
epoch [90/200] batch [40/62] time 0.411 (0.460) data 0.281 (0.329) loss_u loss_u 0.9092 (0.8823) acc_u 9.3750 (14.8438) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [45/62] time 0.456 (0.457) data 0.324 (0.326) loss_u loss_u 0.8906 (0.8838) acc_u 15.6250 (14.8611) lr 1.1874e-03 eta 0:00:07
epoch [90/200] batch [50/62] time 0.608 (0.458) data 0.477 (0.327) loss_u loss_u 0.9092 (0.8800) acc_u 15.6250 (15.3750) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [55/62] time 0.404 (0.456) data 0.272 (0.325) loss_u loss_u 0.8369 (0.8784) acc_u 21.8750 (15.5114) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [60/62] time 0.397 (0.453) data 0.265 (0.322) loss_u loss_u 0.8945 (0.8800) acc_u 15.6250 (15.3646) lr 1.1874e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1460
confident_label rate tensor(0.3520, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1104
clean true:1098
clean false:6
clean_rate:0.9945652173913043
noisy true:578
noisy false:1454
after delete: len(clean_dataset) 1104
after delete: len(noisy_dataset) 2032
epoch [91/200] batch [5/34] time 0.364 (0.405) data 0.234 (0.274) loss_x loss_x 0.8716 (1.1780) acc_x 81.2500 (73.7500) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [10/34] time 0.354 (0.413) data 0.223 (0.283) loss_x loss_x 1.4648 (1.2606) acc_x 71.8750 (71.5625) lr 1.1719e-03 eta 0:00:09
epoch [91/200] batch [15/34] time 0.387 (0.415) data 0.257 (0.285) loss_x loss_x 1.2002 (1.1792) acc_x 71.8750 (72.9167) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [20/34] time 0.485 (0.431) data 0.355 (0.300) loss_x loss_x 1.2393 (1.1572) acc_x 75.0000 (74.5312) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [25/34] time 0.480 (0.444) data 0.349 (0.313) loss_x loss_x 0.5996 (1.1562) acc_x 81.2500 (73.3750) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [30/34] time 0.423 (0.441) data 0.293 (0.311) loss_x loss_x 1.3809 (1.1709) acc_x 65.6250 (71.9792) lr 1.1719e-03 eta 0:00:01
epoch [91/200] batch [5/63] time 0.409 (0.435) data 0.279 (0.305) loss_u loss_u 0.9028 (0.8723) acc_u 9.3750 (16.8750) lr 1.1719e-03 eta 0:00:25
epoch [91/200] batch [10/63] time 0.420 (0.437) data 0.290 (0.307) loss_u loss_u 0.8794 (0.8590) acc_u 21.8750 (18.1250) lr 1.1719e-03 eta 0:00:23
epoch [91/200] batch [15/63] time 0.452 (0.437) data 0.320 (0.306) loss_u loss_u 0.8105 (0.8542) acc_u 28.1250 (19.3750) lr 1.1719e-03 eta 0:00:20
epoch [91/200] batch [20/63] time 0.591 (0.439) data 0.461 (0.308) loss_u loss_u 0.8774 (0.8553) acc_u 15.6250 (18.5938) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [25/63] time 0.337 (0.438) data 0.205 (0.307) loss_u loss_u 0.8809 (0.8600) acc_u 15.6250 (17.8750) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [30/63] time 0.385 (0.436) data 0.255 (0.306) loss_u loss_u 0.8867 (0.8611) acc_u 12.5000 (17.6042) lr 1.1719e-03 eta 0:00:14
epoch [91/200] batch [35/63] time 0.503 (0.442) data 0.373 (0.311) loss_u loss_u 0.9424 (0.8603) acc_u 6.2500 (17.9464) lr 1.1719e-03 eta 0:00:12
epoch [91/200] batch [40/63] time 0.423 (0.441) data 0.291 (0.311) loss_u loss_u 0.8818 (0.8607) acc_u 15.6250 (17.9688) lr 1.1719e-03 eta 0:00:10
epoch [91/200] batch [45/63] time 0.431 (0.441) data 0.299 (0.310) loss_u loss_u 0.8594 (0.8640) acc_u 18.7500 (17.3611) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [50/63] time 0.457 (0.439) data 0.326 (0.308) loss_u loss_u 0.9204 (0.8638) acc_u 12.5000 (17.5000) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [55/63] time 0.412 (0.437) data 0.281 (0.306) loss_u loss_u 0.8120 (0.8609) acc_u 15.6250 (17.7841) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [60/63] time 0.473 (0.441) data 0.341 (0.310) loss_u loss_u 0.9092 (0.8631) acc_u 15.6250 (17.6042) lr 1.1719e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1467
confident_label rate tensor(0.3626, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1137
clean true:1133
clean false:4
clean_rate:0.9964819700967458
noisy true:536
noisy false:1463
after delete: len(clean_dataset) 1137
after delete: len(noisy_dataset) 1999
epoch [92/200] batch [5/35] time 0.551 (0.475) data 0.420 (0.344) loss_x loss_x 1.2930 (1.0895) acc_x 65.6250 (72.5000) lr 1.1564e-03 eta 0:00:14
epoch [92/200] batch [10/35] time 0.528 (0.479) data 0.397 (0.347) loss_x loss_x 1.1748 (1.1516) acc_x 75.0000 (71.5625) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [15/35] time 0.506 (0.459) data 0.376 (0.328) loss_x loss_x 0.6538 (1.0777) acc_x 90.6250 (75.2083) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [20/35] time 0.419 (0.457) data 0.288 (0.326) loss_x loss_x 1.4619 (1.0753) acc_x 62.5000 (74.6875) lr 1.1564e-03 eta 0:00:06
epoch [92/200] batch [25/35] time 0.391 (0.450) data 0.260 (0.319) loss_x loss_x 1.2881 (1.1084) acc_x 68.7500 (73.3750) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [30/35] time 0.452 (0.456) data 0.318 (0.325) loss_x loss_x 1.2793 (1.1170) acc_x 68.7500 (73.2292) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [35/35] time 0.381 (0.456) data 0.250 (0.325) loss_x loss_x 1.4131 (1.1464) acc_x 65.6250 (72.7679) lr 1.1564e-03 eta 0:00:00
epoch [92/200] batch [5/62] time 0.377 (0.453) data 0.246 (0.322) loss_u loss_u 0.8726 (0.8823) acc_u 15.6250 (11.8750) lr 1.1564e-03 eta 0:00:25
epoch [92/200] batch [10/62] time 0.382 (0.449) data 0.252 (0.318) loss_u loss_u 0.8687 (0.8821) acc_u 21.8750 (13.7500) lr 1.1564e-03 eta 0:00:23
epoch [92/200] batch [15/62] time 0.384 (0.453) data 0.252 (0.322) loss_u loss_u 0.8970 (0.8753) acc_u 12.5000 (15.6250) lr 1.1564e-03 eta 0:00:21
epoch [92/200] batch [20/62] time 0.440 (0.455) data 0.308 (0.324) loss_u loss_u 0.8618 (0.8719) acc_u 18.7500 (16.4062) lr 1.1564e-03 eta 0:00:19
epoch [92/200] batch [25/62] time 0.329 (0.451) data 0.199 (0.320) loss_u loss_u 0.8560 (0.8705) acc_u 21.8750 (16.6250) lr 1.1564e-03 eta 0:00:16
epoch [92/200] batch [30/62] time 0.437 (0.449) data 0.306 (0.318) loss_u loss_u 0.8408 (0.8727) acc_u 25.0000 (16.3542) lr 1.1564e-03 eta 0:00:14
epoch [92/200] batch [35/62] time 0.425 (0.446) data 0.294 (0.315) loss_u loss_u 0.9019 (0.8746) acc_u 15.6250 (16.0714) lr 1.1564e-03 eta 0:00:12
epoch [92/200] batch [40/62] time 0.443 (0.448) data 0.311 (0.317) loss_u loss_u 0.9526 (0.8732) acc_u 9.3750 (16.0938) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [45/62] time 0.384 (0.445) data 0.254 (0.314) loss_u loss_u 0.8804 (0.8716) acc_u 18.7500 (15.9722) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [50/62] time 0.370 (0.440) data 0.239 (0.309) loss_u loss_u 0.8857 (0.8738) acc_u 12.5000 (15.6250) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [55/62] time 0.546 (0.439) data 0.414 (0.308) loss_u loss_u 0.9038 (0.8774) acc_u 9.3750 (15.0568) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [60/62] time 0.476 (0.442) data 0.344 (0.311) loss_u loss_u 0.9263 (0.8796) acc_u 9.3750 (14.7917) lr 1.1564e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1502
confident_label rate tensor(0.3540, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1110
clean true:1106
clean false:4
clean_rate:0.9963963963963964
noisy true:528
noisy false:1498
after delete: len(clean_dataset) 1110
after delete: len(noisy_dataset) 2026
epoch [93/200] batch [5/34] time 0.529 (0.443) data 0.398 (0.312) loss_x loss_x 1.5908 (1.2023) acc_x 62.5000 (69.3750) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [10/34] time 0.456 (0.435) data 0.326 (0.304) loss_x loss_x 1.7412 (1.2858) acc_x 62.5000 (67.8125) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [15/34] time 0.393 (0.437) data 0.262 (0.306) loss_x loss_x 0.9741 (1.2424) acc_x 75.0000 (68.7500) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [20/34] time 0.491 (0.443) data 0.360 (0.313) loss_x loss_x 1.3623 (1.1923) acc_x 65.6250 (69.8438) lr 1.1409e-03 eta 0:00:06
epoch [93/200] batch [25/34] time 0.415 (0.447) data 0.284 (0.316) loss_x loss_x 1.3145 (1.1922) acc_x 71.8750 (69.7500) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [30/34] time 0.430 (0.453) data 0.299 (0.322) loss_x loss_x 1.2881 (1.2110) acc_x 71.8750 (70.1042) lr 1.1409e-03 eta 0:00:01
epoch [93/200] batch [5/63] time 0.438 (0.454) data 0.306 (0.323) loss_u loss_u 0.8184 (0.8422) acc_u 21.8750 (22.5000) lr 1.1409e-03 eta 0:00:26
epoch [93/200] batch [10/63] time 0.380 (0.449) data 0.249 (0.318) loss_u loss_u 0.7944 (0.8337) acc_u 21.8750 (22.1875) lr 1.1409e-03 eta 0:00:23
epoch [93/200] batch [15/63] time 0.451 (0.447) data 0.319 (0.316) loss_u loss_u 0.8218 (0.8427) acc_u 21.8750 (21.0417) lr 1.1409e-03 eta 0:00:21
epoch [93/200] batch [20/63] time 0.393 (0.444) data 0.262 (0.312) loss_u loss_u 0.8403 (0.8499) acc_u 21.8750 (19.5312) lr 1.1409e-03 eta 0:00:19
epoch [93/200] batch [25/63] time 0.345 (0.444) data 0.213 (0.313) loss_u loss_u 0.9536 (0.8611) acc_u 6.2500 (17.8750) lr 1.1409e-03 eta 0:00:16
epoch [93/200] batch [30/63] time 0.470 (0.450) data 0.338 (0.319) loss_u loss_u 0.8032 (0.8646) acc_u 25.0000 (17.0833) lr 1.1409e-03 eta 0:00:14
epoch [93/200] batch [35/63] time 0.383 (0.447) data 0.252 (0.315) loss_u loss_u 0.8784 (0.8628) acc_u 12.5000 (17.4107) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [40/63] time 0.587 (0.449) data 0.454 (0.317) loss_u loss_u 0.9014 (0.8696) acc_u 18.7500 (17.1875) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [45/63] time 0.503 (0.448) data 0.371 (0.316) loss_u loss_u 0.9346 (0.8691) acc_u 6.2500 (17.0833) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [50/63] time 0.479 (0.447) data 0.347 (0.316) loss_u loss_u 0.8721 (0.8672) acc_u 15.6250 (17.3750) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [55/63] time 0.616 (0.451) data 0.483 (0.320) loss_u loss_u 0.8149 (0.8667) acc_u 28.1250 (17.6705) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [60/63] time 0.417 (0.451) data 0.285 (0.319) loss_u loss_u 0.8696 (0.8656) acc_u 18.7500 (17.8646) lr 1.1409e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.3654, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1146
clean true:1141
clean false:5
clean_rate:0.9956369982547993
noisy true:554
noisy false:1436
after delete: len(clean_dataset) 1146
after delete: len(noisy_dataset) 1990
epoch [94/200] batch [5/35] time 0.596 (0.466) data 0.466 (0.335) loss_x loss_x 1.5264 (1.1624) acc_x 62.5000 (73.7500) lr 1.1253e-03 eta 0:00:13
epoch [94/200] batch [10/35] time 0.396 (0.451) data 0.265 (0.321) loss_x loss_x 1.6562 (1.1959) acc_x 65.6250 (70.6250) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [15/35] time 0.437 (0.442) data 0.306 (0.311) loss_x loss_x 1.3779 (1.1896) acc_x 65.6250 (71.0417) lr 1.1253e-03 eta 0:00:08
epoch [94/200] batch [20/35] time 0.467 (0.440) data 0.336 (0.309) loss_x loss_x 1.0918 (1.1788) acc_x 65.6250 (70.3125) lr 1.1253e-03 eta 0:00:06
epoch [94/200] batch [25/35] time 0.534 (0.444) data 0.403 (0.314) loss_x loss_x 0.9868 (1.1447) acc_x 71.8750 (71.0000) lr 1.1253e-03 eta 0:00:04
epoch [94/200] batch [30/35] time 0.435 (0.452) data 0.305 (0.321) loss_x loss_x 0.8286 (1.1064) acc_x 81.2500 (71.7708) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [35/35] time 0.370 (0.453) data 0.239 (0.322) loss_x loss_x 1.1943 (1.1366) acc_x 78.1250 (71.2500) lr 1.1253e-03 eta 0:00:00
epoch [94/200] batch [5/62] time 0.432 (0.452) data 0.300 (0.321) loss_u loss_u 0.9233 (0.8967) acc_u 12.5000 (13.1250) lr 1.1253e-03 eta 0:00:25
epoch [94/200] batch [10/62] time 0.529 (0.450) data 0.397 (0.319) loss_u loss_u 0.8730 (0.8797) acc_u 15.6250 (15.0000) lr 1.1253e-03 eta 0:00:23
epoch [94/200] batch [15/62] time 0.466 (0.449) data 0.334 (0.318) loss_u loss_u 0.9644 (0.8838) acc_u 3.1250 (14.3750) lr 1.1253e-03 eta 0:00:21
epoch [94/200] batch [20/62] time 0.375 (0.450) data 0.245 (0.319) loss_u loss_u 0.8999 (0.8817) acc_u 12.5000 (14.6875) lr 1.1253e-03 eta 0:00:18
epoch [94/200] batch [25/62] time 0.498 (0.449) data 0.367 (0.318) loss_u loss_u 0.9346 (0.8744) acc_u 9.3750 (15.3750) lr 1.1253e-03 eta 0:00:16
epoch [94/200] batch [30/62] time 0.383 (0.446) data 0.252 (0.315) loss_u loss_u 0.8496 (0.8734) acc_u 18.7500 (15.6250) lr 1.1253e-03 eta 0:00:14
epoch [94/200] batch [35/62] time 0.417 (0.443) data 0.285 (0.312) loss_u loss_u 0.9028 (0.8742) acc_u 12.5000 (15.3571) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [40/62] time 0.355 (0.442) data 0.223 (0.311) loss_u loss_u 0.9077 (0.8741) acc_u 12.5000 (15.3906) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [45/62] time 0.387 (0.443) data 0.256 (0.312) loss_u loss_u 0.8999 (0.8771) acc_u 21.8750 (15.1389) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [50/62] time 0.376 (0.444) data 0.244 (0.313) loss_u loss_u 0.8872 (0.8781) acc_u 15.6250 (15.1875) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [55/62] time 0.443 (0.442) data 0.310 (0.311) loss_u loss_u 0.8843 (0.8766) acc_u 9.3750 (15.3409) lr 1.1253e-03 eta 0:00:03
epoch [94/200] batch [60/62] time 0.385 (0.439) data 0.253 (0.308) loss_u loss_u 0.9043 (0.8743) acc_u 12.5000 (15.6250) lr 1.1253e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1439
confident_label rate tensor(0.3667, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1150
clean true:1144
clean false:6
clean_rate:0.9947826086956522
noisy true:553
noisy false:1433
after delete: len(clean_dataset) 1150
after delete: len(noisy_dataset) 1986
epoch [95/200] batch [5/35] time 0.492 (0.493) data 0.360 (0.361) loss_x loss_x 1.3525 (1.1566) acc_x 68.7500 (70.0000) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [10/35] time 0.432 (0.460) data 0.302 (0.329) loss_x loss_x 0.7100 (1.1830) acc_x 81.2500 (70.6250) lr 1.1097e-03 eta 0:00:11
epoch [95/200] batch [15/35] time 0.515 (0.453) data 0.383 (0.322) loss_x loss_x 1.4072 (1.2849) acc_x 71.8750 (69.7917) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [20/35] time 0.367 (0.467) data 0.236 (0.336) loss_x loss_x 1.1514 (1.2680) acc_x 71.8750 (70.4688) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [25/35] time 0.412 (0.460) data 0.282 (0.329) loss_x loss_x 1.1934 (1.2404) acc_x 68.7500 (71.0000) lr 1.1097e-03 eta 0:00:04
epoch [95/200] batch [30/35] time 0.493 (0.463) data 0.362 (0.332) loss_x loss_x 0.7095 (1.2589) acc_x 90.6250 (70.2083) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [35/35] time 0.407 (0.462) data 0.276 (0.331) loss_x loss_x 0.6030 (1.2201) acc_x 81.2500 (70.7143) lr 1.1097e-03 eta 0:00:00
epoch [95/200] batch [5/62] time 0.418 (0.459) data 0.287 (0.328) loss_u loss_u 0.8457 (0.8467) acc_u 21.8750 (21.2500) lr 1.1097e-03 eta 0:00:26
epoch [95/200] batch [10/62] time 0.477 (0.456) data 0.345 (0.325) loss_u loss_u 0.9019 (0.8570) acc_u 9.3750 (19.3750) lr 1.1097e-03 eta 0:00:23
epoch [95/200] batch [15/62] time 0.414 (0.452) data 0.283 (0.321) loss_u loss_u 0.9814 (0.8736) acc_u 3.1250 (16.6667) lr 1.1097e-03 eta 0:00:21
epoch [95/200] batch [20/62] time 0.335 (0.454) data 0.205 (0.323) loss_u loss_u 0.8813 (0.8750) acc_u 15.6250 (16.2500) lr 1.1097e-03 eta 0:00:19
epoch [95/200] batch [25/62] time 0.316 (0.451) data 0.184 (0.320) loss_u loss_u 0.8325 (0.8810) acc_u 21.8750 (15.3750) lr 1.1097e-03 eta 0:00:16
epoch [95/200] batch [30/62] time 0.433 (0.449) data 0.302 (0.318) loss_u loss_u 0.9326 (0.8835) acc_u 9.3750 (15.2083) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [35/62] time 0.480 (0.450) data 0.348 (0.319) loss_u loss_u 0.8799 (0.8795) acc_u 15.6250 (15.4464) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [40/62] time 0.608 (0.449) data 0.476 (0.318) loss_u loss_u 0.9072 (0.8806) acc_u 9.3750 (14.9219) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [45/62] time 0.440 (0.449) data 0.304 (0.317) loss_u loss_u 0.9155 (0.8806) acc_u 6.2500 (14.6528) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [50/62] time 0.468 (0.454) data 0.337 (0.322) loss_u loss_u 0.8613 (0.8784) acc_u 18.7500 (14.8125) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [55/62] time 0.395 (0.452) data 0.263 (0.320) loss_u loss_u 0.7114 (0.8743) acc_u 43.7500 (15.5114) lr 1.1097e-03 eta 0:00:03
epoch [95/200] batch [60/62] time 0.361 (0.449) data 0.230 (0.317) loss_u loss_u 0.7798 (0.8732) acc_u 31.2500 (15.8333) lr 1.1097e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1428
confident_label rate tensor(0.3689, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1157
clean true:1151
clean false:6
clean_rate:0.9948141745894555
noisy true:557
noisy false:1422
after delete: len(clean_dataset) 1157
after delete: len(noisy_dataset) 1979
epoch [96/200] batch [5/36] time 0.386 (0.440) data 0.254 (0.308) loss_x loss_x 1.2490 (1.2181) acc_x 62.5000 (68.1250) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [10/36] time 0.421 (0.430) data 0.291 (0.298) loss_x loss_x 0.7930 (1.2448) acc_x 68.7500 (67.1875) lr 1.0941e-03 eta 0:00:11
epoch [96/200] batch [15/36] time 0.541 (0.455) data 0.410 (0.324) loss_x loss_x 1.1035 (1.2281) acc_x 78.1250 (68.3333) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [20/36] time 0.446 (0.467) data 0.316 (0.336) loss_x loss_x 0.9292 (1.1886) acc_x 81.2500 (69.6875) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [25/36] time 0.425 (0.466) data 0.295 (0.336) loss_x loss_x 1.2725 (1.2083) acc_x 65.6250 (68.8750) lr 1.0941e-03 eta 0:00:05
epoch [96/200] batch [30/36] time 0.364 (0.471) data 0.234 (0.340) loss_x loss_x 1.5400 (1.2417) acc_x 50.0000 (68.0208) lr 1.0941e-03 eta 0:00:02
epoch [96/200] batch [35/36] time 0.445 (0.467) data 0.314 (0.336) loss_x loss_x 1.2529 (1.2318) acc_x 68.7500 (68.3036) lr 1.0941e-03 eta 0:00:00
epoch [96/200] batch [5/61] time 0.388 (0.457) data 0.257 (0.326) loss_u loss_u 0.8398 (0.8902) acc_u 25.0000 (14.3750) lr 1.0941e-03 eta 0:00:25
epoch [96/200] batch [10/61] time 0.434 (0.462) data 0.297 (0.331) loss_u loss_u 0.8745 (0.8967) acc_u 18.7500 (14.0625) lr 1.0941e-03 eta 0:00:23
epoch [96/200] batch [15/61] time 0.374 (0.459) data 0.242 (0.328) loss_u loss_u 0.8926 (0.8866) acc_u 15.6250 (15.6250) lr 1.0941e-03 eta 0:00:21
epoch [96/200] batch [20/61] time 0.523 (0.454) data 0.391 (0.323) loss_u loss_u 0.8813 (0.8849) acc_u 15.6250 (15.6250) lr 1.0941e-03 eta 0:00:18
epoch [96/200] batch [25/61] time 0.639 (0.457) data 0.507 (0.325) loss_u loss_u 0.8818 (0.8835) acc_u 12.5000 (15.7500) lr 1.0941e-03 eta 0:00:16
epoch [96/200] batch [30/61] time 0.348 (0.455) data 0.216 (0.323) loss_u loss_u 0.7988 (0.8822) acc_u 25.0000 (15.7292) lr 1.0941e-03 eta 0:00:14
epoch [96/200] batch [35/61] time 0.440 (0.457) data 0.308 (0.325) loss_u loss_u 0.9023 (0.8789) acc_u 12.5000 (15.9821) lr 1.0941e-03 eta 0:00:11
epoch [96/200] batch [40/61] time 0.426 (0.454) data 0.295 (0.323) loss_u loss_u 0.9014 (0.8763) acc_u 15.6250 (16.4844) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [45/61] time 0.496 (0.453) data 0.365 (0.322) loss_u loss_u 0.9438 (0.8775) acc_u 9.3750 (16.6667) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [50/61] time 0.397 (0.453) data 0.265 (0.321) loss_u loss_u 0.8965 (0.8812) acc_u 12.5000 (16.1250) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [55/61] time 0.489 (0.453) data 0.357 (0.322) loss_u loss_u 0.8657 (0.8792) acc_u 15.6250 (16.2500) lr 1.0941e-03 eta 0:00:02
epoch [96/200] batch [60/61] time 0.453 (0.452) data 0.323 (0.320) loss_u loss_u 0.8779 (0.8778) acc_u 18.7500 (16.4583) lr 1.0941e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1475
confident_label rate tensor(0.3613, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1133
clean true:1130
clean false:3
clean_rate:0.9973521624007061
noisy true:531
noisy false:1472
after delete: len(clean_dataset) 1133
after delete: len(noisy_dataset) 2003
epoch [97/200] batch [5/35] time 0.548 (0.505) data 0.418 (0.374) loss_x loss_x 1.2920 (1.2127) acc_x 68.7500 (66.2500) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [10/35] time 0.562 (0.509) data 0.432 (0.379) loss_x loss_x 1.5869 (1.1181) acc_x 59.3750 (70.0000) lr 1.0785e-03 eta 0:00:12
epoch [97/200] batch [15/35] time 0.501 (0.504) data 0.370 (0.373) loss_x loss_x 1.3125 (1.1172) acc_x 68.7500 (70.8333) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [20/35] time 0.393 (0.483) data 0.262 (0.352) loss_x loss_x 1.2090 (1.1430) acc_x 71.8750 (70.0000) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [25/35] time 0.439 (0.475) data 0.308 (0.345) loss_x loss_x 1.2520 (1.1504) acc_x 78.1250 (69.6250) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [30/35] time 0.441 (0.471) data 0.310 (0.340) loss_x loss_x 1.2363 (1.1726) acc_x 68.7500 (69.5833) lr 1.0785e-03 eta 0:00:02
epoch [97/200] batch [35/35] time 0.344 (0.468) data 0.214 (0.337) loss_x loss_x 1.1426 (1.1794) acc_x 62.5000 (69.2857) lr 1.0785e-03 eta 0:00:00
epoch [97/200] batch [5/62] time 0.531 (0.468) data 0.400 (0.337) loss_u loss_u 0.7954 (0.8756) acc_u 28.1250 (15.6250) lr 1.0785e-03 eta 0:00:26
epoch [97/200] batch [10/62] time 0.466 (0.468) data 0.335 (0.337) loss_u loss_u 0.9175 (0.8686) acc_u 9.3750 (16.5625) lr 1.0785e-03 eta 0:00:24
epoch [97/200] batch [15/62] time 0.383 (0.468) data 0.251 (0.337) loss_u loss_u 0.8994 (0.8682) acc_u 12.5000 (16.2500) lr 1.0785e-03 eta 0:00:21
epoch [97/200] batch [20/62] time 0.367 (0.460) data 0.236 (0.329) loss_u loss_u 0.7773 (0.8632) acc_u 34.3750 (17.3438) lr 1.0785e-03 eta 0:00:19
epoch [97/200] batch [25/62] time 0.612 (0.461) data 0.482 (0.331) loss_u loss_u 0.9150 (0.8728) acc_u 9.3750 (15.7500) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [30/62] time 0.465 (0.459) data 0.335 (0.329) loss_u loss_u 0.9087 (0.8740) acc_u 15.6250 (15.9375) lr 1.0785e-03 eta 0:00:14
epoch [97/200] batch [35/62] time 0.442 (0.455) data 0.311 (0.324) loss_u loss_u 0.8555 (0.8746) acc_u 18.7500 (15.8036) lr 1.0785e-03 eta 0:00:12
epoch [97/200] batch [40/62] time 0.514 (0.452) data 0.382 (0.321) loss_u loss_u 0.9175 (0.8732) acc_u 9.3750 (15.9375) lr 1.0785e-03 eta 0:00:09
epoch [97/200] batch [45/62] time 0.506 (0.457) data 0.373 (0.326) loss_u loss_u 0.8774 (0.8725) acc_u 15.6250 (15.9722) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [50/62] time 0.344 (0.452) data 0.213 (0.321) loss_u loss_u 0.8833 (0.8743) acc_u 21.8750 (15.8750) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [55/62] time 0.421 (0.450) data 0.289 (0.319) loss_u loss_u 0.9160 (0.8733) acc_u 9.3750 (16.0227) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [60/62] time 0.445 (0.449) data 0.313 (0.318) loss_u loss_u 0.9238 (0.8757) acc_u 9.3750 (15.7292) lr 1.0785e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1474
confident_label rate tensor(0.3603, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1130
clean true:1126
clean false:4
clean_rate:0.9964601769911504
noisy true:536
noisy false:1470
after delete: len(clean_dataset) 1130
after delete: len(noisy_dataset) 2006
epoch [98/200] batch [5/35] time 0.431 (0.429) data 0.300 (0.298) loss_x loss_x 1.6523 (1.1916) acc_x 65.6250 (70.6250) lr 1.0628e-03 eta 0:00:12
epoch [98/200] batch [10/35] time 0.515 (0.461) data 0.384 (0.330) loss_x loss_x 0.7446 (1.1033) acc_x 87.5000 (73.4375) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [15/35] time 0.408 (0.454) data 0.277 (0.323) loss_x loss_x 1.0342 (1.1713) acc_x 68.7500 (71.2500) lr 1.0628e-03 eta 0:00:09
epoch [98/200] batch [20/35] time 0.588 (0.461) data 0.457 (0.330) loss_x loss_x 0.8955 (1.1760) acc_x 78.1250 (70.6250) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [25/35] time 0.527 (0.471) data 0.396 (0.340) loss_x loss_x 0.8813 (1.1747) acc_x 59.3750 (69.8750) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [30/35] time 0.502 (0.466) data 0.371 (0.336) loss_x loss_x 1.2012 (1.2104) acc_x 75.0000 (69.2708) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [35/35] time 0.370 (0.457) data 0.240 (0.326) loss_x loss_x 1.0195 (1.2005) acc_x 62.5000 (69.1071) lr 1.0628e-03 eta 0:00:00
epoch [98/200] batch [5/62] time 0.501 (0.456) data 0.369 (0.326) loss_u loss_u 0.7847 (0.8568) acc_u 34.3750 (21.8750) lr 1.0628e-03 eta 0:00:26
epoch [98/200] batch [10/62] time 0.486 (0.458) data 0.354 (0.327) loss_u loss_u 0.8052 (0.8436) acc_u 25.0000 (22.5000) lr 1.0628e-03 eta 0:00:23
epoch [98/200] batch [15/62] time 0.544 (0.455) data 0.412 (0.323) loss_u loss_u 0.9131 (0.8485) acc_u 12.5000 (20.6250) lr 1.0628e-03 eta 0:00:21
epoch [98/200] batch [20/62] time 0.522 (0.453) data 0.390 (0.322) loss_u loss_u 0.8486 (0.8531) acc_u 21.8750 (20.1562) lr 1.0628e-03 eta 0:00:19
epoch [98/200] batch [25/62] time 0.555 (0.458) data 0.424 (0.327) loss_u loss_u 0.8374 (0.8536) acc_u 21.8750 (19.8750) lr 1.0628e-03 eta 0:00:16
epoch [98/200] batch [30/62] time 0.394 (0.456) data 0.262 (0.325) loss_u loss_u 0.8403 (0.8546) acc_u 18.7500 (19.2708) lr 1.0628e-03 eta 0:00:14
epoch [98/200] batch [35/62] time 0.454 (0.459) data 0.323 (0.328) loss_u loss_u 0.8511 (0.8554) acc_u 18.7500 (19.1964) lr 1.0628e-03 eta 0:00:12
epoch [98/200] batch [40/62] time 0.401 (0.458) data 0.269 (0.326) loss_u loss_u 0.8271 (0.8559) acc_u 25.0000 (19.1406) lr 1.0628e-03 eta 0:00:10
epoch [98/200] batch [45/62] time 0.390 (0.455) data 0.259 (0.323) loss_u loss_u 0.8662 (0.8586) acc_u 21.8750 (18.8889) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [50/62] time 0.716 (0.458) data 0.585 (0.326) loss_u loss_u 0.8535 (0.8587) acc_u 15.6250 (18.7500) lr 1.0628e-03 eta 0:00:05
epoch [98/200] batch [55/62] time 0.425 (0.456) data 0.294 (0.324) loss_u loss_u 0.8271 (0.8586) acc_u 25.0000 (18.5227) lr 1.0628e-03 eta 0:00:03
epoch [98/200] batch [60/62] time 0.375 (0.451) data 0.244 (0.319) loss_u loss_u 0.8599 (0.8615) acc_u 18.7500 (18.0729) lr 1.0628e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1427
confident_label rate tensor(0.3753, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1177
clean true:1171
clean false:6
clean_rate:0.9949022939677146
noisy true:538
noisy false:1421
after delete: len(clean_dataset) 1177
after delete: len(noisy_dataset) 1959
epoch [99/200] batch [5/36] time 0.354 (0.503) data 0.223 (0.373) loss_x loss_x 1.2715 (0.9744) acc_x 71.8750 (75.0000) lr 1.0471e-03 eta 0:00:15
epoch [99/200] batch [10/36] time 0.483 (0.516) data 0.352 (0.384) loss_x loss_x 0.9014 (1.0115) acc_x 78.1250 (75.3125) lr 1.0471e-03 eta 0:00:13
epoch [99/200] batch [15/36] time 0.378 (0.505) data 0.247 (0.374) loss_x loss_x 1.0176 (1.0982) acc_x 68.7500 (74.3750) lr 1.0471e-03 eta 0:00:10
epoch [99/200] batch [20/36] time 0.366 (0.483) data 0.236 (0.352) loss_x loss_x 1.3193 (1.0827) acc_x 62.5000 (72.9688) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [25/36] time 0.499 (0.490) data 0.368 (0.359) loss_x loss_x 0.9009 (1.0753) acc_x 75.0000 (72.7500) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [30/36] time 0.594 (0.490) data 0.463 (0.359) loss_x loss_x 1.2549 (1.1260) acc_x 65.6250 (71.4583) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [35/36] time 0.364 (0.478) data 0.233 (0.347) loss_x loss_x 1.2695 (1.1179) acc_x 71.8750 (71.6964) lr 1.0471e-03 eta 0:00:00
epoch [99/200] batch [5/61] time 0.445 (0.472) data 0.314 (0.340) loss_u loss_u 0.8657 (0.8657) acc_u 18.7500 (20.0000) lr 1.0471e-03 eta 0:00:26
epoch [99/200] batch [10/61] time 0.486 (0.468) data 0.355 (0.337) loss_u loss_u 0.8838 (0.8740) acc_u 18.7500 (17.1875) lr 1.0471e-03 eta 0:00:23
epoch [99/200] batch [15/61] time 0.505 (0.466) data 0.373 (0.335) loss_u loss_u 0.9346 (0.8773) acc_u 12.5000 (16.6667) lr 1.0471e-03 eta 0:00:21
epoch [99/200] batch [20/61] time 0.505 (0.464) data 0.374 (0.333) loss_u loss_u 0.8525 (0.8808) acc_u 21.8750 (16.0938) lr 1.0471e-03 eta 0:00:19
epoch [99/200] batch [25/61] time 0.570 (0.464) data 0.438 (0.333) loss_u loss_u 0.9292 (0.8815) acc_u 9.3750 (15.7500) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [30/61] time 0.429 (0.463) data 0.297 (0.332) loss_u loss_u 0.9385 (0.8855) acc_u 6.2500 (15.0000) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [35/61] time 0.520 (0.461) data 0.389 (0.329) loss_u loss_u 0.9546 (0.8826) acc_u 3.1250 (15.4464) lr 1.0471e-03 eta 0:00:11
epoch [99/200] batch [40/61] time 0.431 (0.461) data 0.300 (0.329) loss_u loss_u 0.8599 (0.8817) acc_u 21.8750 (15.6250) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [45/61] time 0.423 (0.460) data 0.292 (0.329) loss_u loss_u 0.8677 (0.8833) acc_u 18.7500 (15.2778) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [50/61] time 0.403 (0.457) data 0.270 (0.326) loss_u loss_u 0.8877 (0.8836) acc_u 12.5000 (15.0625) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [55/61] time 0.405 (0.457) data 0.273 (0.326) loss_u loss_u 0.8843 (0.8831) acc_u 15.6250 (15.1136) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [60/61] time 0.401 (0.456) data 0.269 (0.325) loss_u loss_u 0.9106 (0.8816) acc_u 12.5000 (15.3646) lr 1.0471e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1475
confident_label rate tensor(0.3635, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1140
clean true:1133
clean false:7
clean_rate:0.993859649122807
noisy true:528
noisy false:1468
all clean rate:  [0.983739837398374, 0.9936808846761453, 0.9913793103448276, 0.9964747356051704, 0.9976275207591934, 0.9988179669030733, 0.9976442873969376, 0.9977220956719818, 1.0, 0.9955801104972376, 0.9943310657596371, 0.9955005624296963, 0.9955654101995566, 0.9967355821545157, 0.9958071278825996, 0.9945414847161572, 0.997872340425532, 0.9957446808510638, 0.9979360165118679, 0.9967741935483871, 0.9989583333333333, 0.9929718875502008, 0.9958932238193019, 0.9968287526427061, 0.9968387776606955, 0.994, 0.9958890030832477, 0.9969199178644764, 0.9950347567030785, 0.9918200408997955, 0.993, 0.9958333333333333, 0.9887869520897044, 0.9979777553083923, 0.9959839357429718, 0.9920870425321464, 0.9960745829244357, 0.9942418426103646, 0.9990059642147118, 0.991270611057226, 0.9952290076335878, 0.9961202715809894, 0.9981566820276497, 0.9959919839679359, 0.9915333960489181, 0.9944444444444445, 0.9933586337760911, 0.9961013645224172, 0.9960975609756098, 0.996116504854369, 0.9973070017953322, 0.9953917050691244, 0.9944598337950139, 0.9952015355086372, 0.9961649089165868, 0.9961977186311787, 0.996309963099631, 0.9953271028037384, 0.9935779816513761, 0.9953531598513011, 0.9946332737030411, 0.9962085308056872, 0.9944547134935305, 0.992773261065944, 0.9972350230414746, 0.9924882629107982, 0.9971804511278195, 0.997270245677889, 0.9954379562043796, 0.9946571682991986, 0.9963702359346642, 0.9981447124304267, 0.9954710144927537, 0.9939288811795317, 0.9954086317722681, 0.9937611408199644, 0.994661921708185, 0.9972677595628415, 0.9963269054178145, 0.9981834695731153, 0.998193315266486, 0.9944186046511628, 0.9982608695652174, 0.9982440737489026, 0.9954001839926403, 0.9937275985663082, 0.9921739130434782, 0.9956483899042646, 0.9945404913557779, 0.9930313588850174, 0.9945652173913043, 0.9964819700967458, 0.9963963963963964, 0.9956369982547993, 0.9947826086956522, 0.9948141745894555, 0.9973521624007061, 0.9964601769911504, 0.9949022939677146, 0.993859649122807]
after delete: len(clean_dataset) 1140
after delete: len(noisy_dataset) 1996
epoch [100/200] batch [5/35] time 0.363 (0.434) data 0.233 (0.304) loss_x loss_x 1.4834 (1.1750) acc_x 65.6250 (69.3750) lr 1.0314e-03 eta 0:00:13
epoch [100/200] batch [10/35] time 0.486 (0.452) data 0.356 (0.322) loss_x loss_x 1.1367 (1.1844) acc_x 68.7500 (69.3750) lr 1.0314e-03 eta 0:00:11
epoch [100/200] batch [15/35] time 0.492 (0.452) data 0.361 (0.322) loss_x loss_x 1.3350 (1.1175) acc_x 62.5000 (71.0417) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [20/35] time 0.539 (0.447) data 0.410 (0.317) loss_x loss_x 0.9912 (1.1167) acc_x 65.6250 (71.4062) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [25/35] time 0.494 (0.455) data 0.364 (0.325) loss_x loss_x 1.2051 (1.1051) acc_x 65.6250 (70.5000) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [30/35] time 0.393 (0.455) data 0.264 (0.325) loss_x loss_x 0.9321 (1.1069) acc_x 75.0000 (71.4583) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [35/35] time 0.592 (0.461) data 0.461 (0.331) loss_x loss_x 1.1035 (1.1127) acc_x 71.8750 (71.1607) lr 1.0314e-03 eta 0:00:00
epoch [100/200] batch [5/62] time 0.369 (0.452) data 0.238 (0.321) loss_u loss_u 0.8662 (0.8644) acc_u 15.6250 (19.3750) lr 1.0314e-03 eta 0:00:25
epoch [100/200] batch [10/62] time 0.400 (0.451) data 0.268 (0.320) loss_u loss_u 0.8848 (0.8618) acc_u 15.6250 (18.4375) lr 1.0314e-03 eta 0:00:23
epoch [100/200] batch [15/62] time 0.451 (0.458) data 0.320 (0.327) loss_u loss_u 0.8345 (0.8674) acc_u 18.7500 (17.5000) lr 1.0314e-03 eta 0:00:21
epoch [100/200] batch [20/62] time 0.408 (0.460) data 0.276 (0.329) loss_u loss_u 0.8794 (0.8641) acc_u 15.6250 (18.1250) lr 1.0314e-03 eta 0:00:19
epoch [100/200] batch [25/62] time 0.350 (0.456) data 0.219 (0.325) loss_u loss_u 0.8545 (0.8667) acc_u 18.7500 (17.3750) lr 1.0314e-03 eta 0:00:16
epoch [100/200] batch [30/62] time 0.394 (0.454) data 0.262 (0.323) loss_u loss_u 0.8584 (0.8633) acc_u 18.7500 (18.2292) lr 1.0314e-03 eta 0:00:14
epoch [100/200] batch [35/62] time 0.435 (0.450) data 0.303 (0.320) loss_u loss_u 0.9082 (0.8658) acc_u 15.6250 (17.7679) lr 1.0314e-03 eta 0:00:12
epoch [100/200] batch [40/62] time 0.434 (0.449) data 0.303 (0.318) loss_u loss_u 0.9087 (0.8665) acc_u 9.3750 (17.4219) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [45/62] time 0.409 (0.451) data 0.278 (0.320) loss_u loss_u 0.8901 (0.8691) acc_u 12.5000 (17.0139) lr 1.0314e-03 eta 0:00:07
epoch [100/200] batch [50/62] time 0.587 (0.452) data 0.455 (0.322) loss_u loss_u 0.8608 (0.8663) acc_u 18.7500 (17.3125) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [55/62] time 0.394 (0.451) data 0.263 (0.320) loss_u loss_u 0.9082 (0.8683) acc_u 12.5000 (17.1591) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [60/62] time 0.423 (0.449) data 0.291 (0.319) loss_u loss_u 0.8271 (0.8639) acc_u 21.8750 (17.6562) lr 1.0314e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1407
confident_label rate tensor(0.3642, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1142
clean true:1139
clean false:3
clean_rate:0.9973730297723292
noisy true:590
noisy false:1404
after delete: len(clean_dataset) 1142
after delete: len(noisy_dataset) 1994
epoch [101/200] batch [5/35] time 0.429 (0.453) data 0.298 (0.322) loss_x loss_x 1.0195 (1.2388) acc_x 71.8750 (70.0000) lr 1.0157e-03 eta 0:00:13
epoch [101/200] batch [10/35] time 0.438 (0.489) data 0.303 (0.358) loss_x loss_x 1.0283 (1.2183) acc_x 78.1250 (71.8750) lr 1.0157e-03 eta 0:00:12
epoch [101/200] batch [15/35] time 0.429 (0.472) data 0.299 (0.341) loss_x loss_x 0.8994 (1.1824) acc_x 78.1250 (71.0417) lr 1.0157e-03 eta 0:00:09
epoch [101/200] batch [20/35] time 0.428 (0.475) data 0.297 (0.344) loss_x loss_x 1.7490 (1.1751) acc_x 65.6250 (71.5625) lr 1.0157e-03 eta 0:00:07
epoch [101/200] batch [25/35] time 0.412 (0.463) data 0.281 (0.332) loss_x loss_x 1.4180 (1.1249) acc_x 75.0000 (73.1250) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [30/35] time 0.327 (0.452) data 0.196 (0.321) loss_x loss_x 1.1943 (1.1565) acc_x 68.7500 (72.6042) lr 1.0157e-03 eta 0:00:02
epoch [101/200] batch [35/35] time 0.477 (0.454) data 0.346 (0.323) loss_x loss_x 0.9360 (1.1473) acc_x 78.1250 (71.9643) lr 1.0157e-03 eta 0:00:00
epoch [101/200] batch [5/62] time 0.468 (0.453) data 0.337 (0.322) loss_u loss_u 0.8550 (0.8551) acc_u 18.7500 (20.6250) lr 1.0157e-03 eta 0:00:25
epoch [101/200] batch [10/62] time 0.442 (0.451) data 0.311 (0.320) loss_u loss_u 0.7993 (0.8407) acc_u 28.1250 (21.5625) lr 1.0157e-03 eta 0:00:23
epoch [101/200] batch [15/62] time 0.367 (0.451) data 0.235 (0.320) loss_u loss_u 0.8555 (0.8487) acc_u 15.6250 (19.1667) lr 1.0157e-03 eta 0:00:21
epoch [101/200] batch [20/62] time 0.349 (0.449) data 0.218 (0.317) loss_u loss_u 0.9502 (0.8549) acc_u 6.2500 (17.9688) lr 1.0157e-03 eta 0:00:18
epoch [101/200] batch [25/62] time 0.456 (0.449) data 0.325 (0.318) loss_u loss_u 0.8267 (0.8554) acc_u 21.8750 (18.2500) lr 1.0157e-03 eta 0:00:16
epoch [101/200] batch [30/62] time 0.520 (0.453) data 0.388 (0.322) loss_u loss_u 0.8857 (0.8616) acc_u 9.3750 (17.6042) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [35/62] time 0.422 (0.453) data 0.291 (0.322) loss_u loss_u 0.8560 (0.8645) acc_u 18.7500 (17.2321) lr 1.0157e-03 eta 0:00:12
epoch [101/200] batch [40/62] time 0.623 (0.455) data 0.492 (0.323) loss_u loss_u 0.8164 (0.8673) acc_u 21.8750 (16.7969) lr 1.0157e-03 eta 0:00:10
epoch [101/200] batch [45/62] time 0.430 (0.451) data 0.298 (0.320) loss_u loss_u 0.8242 (0.8688) acc_u 21.8750 (16.5972) lr 1.0157e-03 eta 0:00:07
epoch [101/200] batch [50/62] time 0.489 (0.450) data 0.357 (0.319) loss_u loss_u 0.9072 (0.8715) acc_u 12.5000 (16.2500) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [55/62] time 0.412 (0.451) data 0.280 (0.320) loss_u loss_u 0.9189 (0.8749) acc_u 15.6250 (15.8523) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [60/62] time 0.404 (0.448) data 0.272 (0.317) loss_u loss_u 0.8394 (0.8736) acc_u 18.7500 (15.8854) lr 1.0157e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1443
confident_label rate tensor(0.3622, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1136
clean true:1133
clean false:3
clean_rate:0.9973591549295775
noisy true:560
noisy false:1440
after delete: len(clean_dataset) 1136
after delete: len(noisy_dataset) 2000
epoch [102/200] batch [5/35] time 0.557 (0.444) data 0.426 (0.313) loss_x loss_x 1.0898 (1.3133) acc_x 65.6250 (63.7500) lr 1.0000e-03 eta 0:00:13
epoch [102/200] batch [10/35] time 0.347 (0.417) data 0.215 (0.286) loss_x loss_x 1.1699 (1.2755) acc_x 68.7500 (66.5625) lr 1.0000e-03 eta 0:00:10
epoch [102/200] batch [15/35] time 0.385 (0.434) data 0.254 (0.303) loss_x loss_x 0.7563 (1.1469) acc_x 84.3750 (69.7917) lr 1.0000e-03 eta 0:00:08
epoch [102/200] batch [20/35] time 0.517 (0.438) data 0.386 (0.307) loss_x loss_x 0.8154 (1.1448) acc_x 75.0000 (71.2500) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [25/35] time 0.431 (0.440) data 0.301 (0.309) loss_x loss_x 1.1094 (1.0977) acc_x 62.5000 (72.3750) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [30/35] time 0.420 (0.437) data 0.289 (0.306) loss_x loss_x 1.4434 (1.1371) acc_x 65.6250 (71.7708) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [35/35] time 0.547 (0.446) data 0.416 (0.315) loss_x loss_x 0.7559 (1.1479) acc_x 78.1250 (71.5179) lr 1.0000e-03 eta 0:00:00
epoch [102/200] batch [5/62] time 0.413 (0.439) data 0.282 (0.308) loss_u loss_u 0.8418 (0.8837) acc_u 21.8750 (15.0000) lr 1.0000e-03 eta 0:00:25
epoch [102/200] batch [10/62] time 0.358 (0.443) data 0.226 (0.312) loss_u loss_u 0.8511 (0.8832) acc_u 15.6250 (15.0000) lr 1.0000e-03 eta 0:00:23
epoch [102/200] batch [15/62] time 0.550 (0.443) data 0.417 (0.312) loss_u loss_u 0.8760 (0.8763) acc_u 15.6250 (15.2083) lr 1.0000e-03 eta 0:00:20
epoch [102/200] batch [20/62] time 0.521 (0.457) data 0.390 (0.326) loss_u loss_u 0.9517 (0.8826) acc_u 6.2500 (14.5312) lr 1.0000e-03 eta 0:00:19
epoch [102/200] batch [25/62] time 0.402 (0.454) data 0.270 (0.322) loss_u loss_u 0.8555 (0.8796) acc_u 18.7500 (15.0000) lr 1.0000e-03 eta 0:00:16
epoch [102/200] batch [30/62] time 0.344 (0.451) data 0.213 (0.320) loss_u loss_u 0.8726 (0.8715) acc_u 15.6250 (16.0417) lr 1.0000e-03 eta 0:00:14
epoch [102/200] batch [35/62] time 0.421 (0.451) data 0.289 (0.319) loss_u loss_u 0.9106 (0.8687) acc_u 9.3750 (16.6071) lr 1.0000e-03 eta 0:00:12
epoch [102/200] batch [40/62] time 0.343 (0.448) data 0.211 (0.317) loss_u loss_u 0.9258 (0.8665) acc_u 9.3750 (16.7969) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [45/62] time 0.485 (0.447) data 0.354 (0.316) loss_u loss_u 0.8696 (0.8687) acc_u 15.6250 (16.4583) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [50/62] time 0.454 (0.447) data 0.324 (0.315) loss_u loss_u 0.8604 (0.8681) acc_u 25.0000 (16.6875) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [55/62] time 0.520 (0.448) data 0.390 (0.317) loss_u loss_u 0.8696 (0.8690) acc_u 12.5000 (16.5341) lr 1.0000e-03 eta 0:00:03
epoch [102/200] batch [60/62] time 0.375 (0.448) data 0.243 (0.316) loss_u loss_u 0.8989 (0.8713) acc_u 12.5000 (16.1979) lr 1.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1437
confident_label rate tensor(0.3721, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1167
clean true:1162
clean false:5
clean_rate:0.9957155098543273
noisy true:537
noisy false:1432
after delete: len(clean_dataset) 1167
after delete: len(noisy_dataset) 1969
epoch [103/200] batch [5/36] time 0.404 (0.506) data 0.274 (0.376) loss_x loss_x 1.3018 (1.1960) acc_x 71.8750 (69.3750) lr 9.8429e-04 eta 0:00:15
epoch [103/200] batch [10/36] time 0.381 (0.454) data 0.251 (0.324) loss_x loss_x 1.3574 (1.1174) acc_x 65.6250 (72.8125) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [15/36] time 0.514 (0.478) data 0.383 (0.347) loss_x loss_x 1.1865 (1.2073) acc_x 65.6250 (69.7917) lr 9.8429e-04 eta 0:00:10
epoch [103/200] batch [20/36] time 0.386 (0.474) data 0.255 (0.343) loss_x loss_x 1.4463 (1.2408) acc_x 62.5000 (68.5938) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [25/36] time 0.437 (0.467) data 0.307 (0.336) loss_x loss_x 1.5801 (1.2782) acc_x 50.0000 (67.0000) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [30/36] time 0.440 (0.465) data 0.308 (0.334) loss_x loss_x 1.2539 (1.2406) acc_x 65.6250 (68.2292) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [35/36] time 0.391 (0.461) data 0.260 (0.330) loss_x loss_x 1.7998 (1.2397) acc_x 59.3750 (68.6607) lr 9.8429e-04 eta 0:00:00
epoch [103/200] batch [5/61] time 0.421 (0.460) data 0.289 (0.329) loss_u loss_u 0.9165 (0.8681) acc_u 6.2500 (16.8750) lr 9.8429e-04 eta 0:00:25
epoch [103/200] batch [10/61] time 0.384 (0.459) data 0.252 (0.328) loss_u loss_u 0.8989 (0.8742) acc_u 15.6250 (16.5625) lr 9.8429e-04 eta 0:00:23
epoch [103/200] batch [15/61] time 0.552 (0.462) data 0.420 (0.330) loss_u loss_u 0.8472 (0.8794) acc_u 18.7500 (15.8333) lr 9.8429e-04 eta 0:00:21
epoch [103/200] batch [20/61] time 0.484 (0.461) data 0.353 (0.330) loss_u loss_u 0.8525 (0.8724) acc_u 21.8750 (17.1875) lr 9.8429e-04 eta 0:00:18
epoch [103/200] batch [25/61] time 0.554 (0.458) data 0.423 (0.326) loss_u loss_u 0.9097 (0.8800) acc_u 9.3750 (15.7500) lr 9.8429e-04 eta 0:00:16
epoch [103/200] batch [30/61] time 0.394 (0.454) data 0.262 (0.323) loss_u loss_u 0.8115 (0.8735) acc_u 25.0000 (16.6667) lr 9.8429e-04 eta 0:00:14
epoch [103/200] batch [35/61] time 0.535 (0.453) data 0.403 (0.322) loss_u loss_u 0.9585 (0.8744) acc_u 3.1250 (16.4286) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [40/61] time 0.637 (0.457) data 0.506 (0.326) loss_u loss_u 0.8389 (0.8710) acc_u 21.8750 (17.1094) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [45/61] time 0.516 (0.456) data 0.384 (0.324) loss_u loss_u 0.8613 (0.8700) acc_u 15.6250 (17.0833) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [50/61] time 0.460 (0.453) data 0.328 (0.322) loss_u loss_u 0.8037 (0.8661) acc_u 28.1250 (17.5625) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [55/61] time 0.442 (0.451) data 0.309 (0.320) loss_u loss_u 0.9067 (0.8669) acc_u 15.6250 (17.5000) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [60/61] time 0.334 (0.448) data 0.203 (0.317) loss_u loss_u 0.8560 (0.8689) acc_u 25.0000 (17.4479) lr 9.8429e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1452
confident_label rate tensor(0.3581, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1123
clean true:1118
clean false:5
clean_rate:0.9955476402493322
noisy true:566
noisy false:1447
after delete: len(clean_dataset) 1123
after delete: len(noisy_dataset) 2013
epoch [104/200] batch [5/35] time 0.375 (0.439) data 0.244 (0.308) loss_x loss_x 1.1777 (1.1506) acc_x 78.1250 (72.5000) lr 9.6859e-04 eta 0:00:13
epoch [104/200] batch [10/35] time 0.684 (0.470) data 0.553 (0.339) loss_x loss_x 0.8174 (1.0925) acc_x 81.2500 (72.5000) lr 9.6859e-04 eta 0:00:11
epoch [104/200] batch [15/35] time 0.703 (0.501) data 0.572 (0.370) loss_x loss_x 0.6992 (1.0278) acc_x 78.1250 (74.1667) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [20/35] time 0.557 (0.488) data 0.426 (0.358) loss_x loss_x 0.9053 (1.0838) acc_x 81.2500 (73.4375) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [25/35] time 0.376 (0.482) data 0.245 (0.351) loss_x loss_x 1.3672 (1.0874) acc_x 65.6250 (72.8750) lr 9.6859e-04 eta 0:00:04
epoch [104/200] batch [30/35] time 0.377 (0.473) data 0.247 (0.342) loss_x loss_x 1.2666 (1.0816) acc_x 71.8750 (73.0208) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [35/35] time 0.539 (0.474) data 0.409 (0.343) loss_x loss_x 0.8574 (1.0542) acc_x 78.1250 (74.1071) lr 9.6859e-04 eta 0:00:00
epoch [104/200] batch [5/62] time 0.365 (0.470) data 0.235 (0.339) loss_u loss_u 0.8560 (0.8381) acc_u 18.7500 (20.6250) lr 9.6859e-04 eta 0:00:26
epoch [104/200] batch [10/62] time 0.423 (0.467) data 0.292 (0.337) loss_u loss_u 0.8540 (0.8549) acc_u 18.7500 (18.7500) lr 9.6859e-04 eta 0:00:24
epoch [104/200] batch [15/62] time 0.448 (0.470) data 0.315 (0.339) loss_u loss_u 0.8320 (0.8443) acc_u 21.8750 (20.0000) lr 9.6859e-04 eta 0:00:22
epoch [104/200] batch [20/62] time 0.539 (0.480) data 0.407 (0.348) loss_u loss_u 0.8506 (0.8512) acc_u 18.7500 (19.5312) lr 9.6859e-04 eta 0:00:20
epoch [104/200] batch [25/62] time 0.622 (0.477) data 0.490 (0.346) loss_u loss_u 0.8662 (0.8547) acc_u 18.7500 (19.2500) lr 9.6859e-04 eta 0:00:17
epoch [104/200] batch [30/62] time 0.328 (0.474) data 0.196 (0.342) loss_u loss_u 0.8477 (0.8553) acc_u 21.8750 (18.9583) lr 9.6859e-04 eta 0:00:15
epoch [104/200] batch [35/62] time 0.339 (0.466) data 0.207 (0.334) loss_u loss_u 0.8540 (0.8595) acc_u 15.6250 (18.0357) lr 9.6859e-04 eta 0:00:12
epoch [104/200] batch [40/62] time 0.378 (0.459) data 0.246 (0.328) loss_u loss_u 0.7700 (0.8585) acc_u 25.0000 (17.9688) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [45/62] time 0.362 (0.458) data 0.230 (0.327) loss_u loss_u 0.8687 (0.8600) acc_u 18.7500 (17.9167) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [50/62] time 0.403 (0.455) data 0.271 (0.324) loss_u loss_u 0.8320 (0.8603) acc_u 15.6250 (17.8750) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [55/62] time 0.392 (0.453) data 0.259 (0.321) loss_u loss_u 0.9321 (0.8611) acc_u 12.5000 (18.0682) lr 9.6859e-04 eta 0:00:03
epoch [104/200] batch [60/62] time 0.461 (0.453) data 0.329 (0.322) loss_u loss_u 0.8638 (0.8627) acc_u 18.7500 (18.2292) lr 9.6859e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.3753, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1177
clean true:1172
clean false:5
clean_rate:0.9957519116397621
noisy true:523
noisy false:1436
after delete: len(clean_dataset) 1177
after delete: len(noisy_dataset) 1959
epoch [105/200] batch [5/36] time 0.468 (0.515) data 0.337 (0.384) loss_x loss_x 1.4814 (1.2320) acc_x 65.6250 (68.7500) lr 9.5289e-04 eta 0:00:15
epoch [105/200] batch [10/36] time 0.449 (0.514) data 0.318 (0.384) loss_x loss_x 0.8936 (1.1758) acc_x 75.0000 (71.5625) lr 9.5289e-04 eta 0:00:13
epoch [105/200] batch [15/36] time 0.498 (0.503) data 0.368 (0.372) loss_x loss_x 1.0020 (1.0956) acc_x 68.7500 (72.9167) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [20/36] time 0.464 (0.497) data 0.334 (0.367) loss_x loss_x 0.8428 (1.1453) acc_x 75.0000 (71.8750) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [25/36] time 0.411 (0.485) data 0.281 (0.355) loss_x loss_x 0.8291 (1.1471) acc_x 84.3750 (72.1250) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [30/36] time 0.474 (0.479) data 0.343 (0.349) loss_x loss_x 1.3242 (1.1735) acc_x 62.5000 (71.3542) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [35/36] time 0.480 (0.471) data 0.349 (0.340) loss_x loss_x 1.1670 (1.1711) acc_x 65.6250 (71.5179) lr 9.5289e-04 eta 0:00:00
epoch [105/200] batch [5/61] time 0.346 (0.459) data 0.214 (0.329) loss_u loss_u 0.8687 (0.9080) acc_u 12.5000 (10.6250) lr 9.5289e-04 eta 0:00:25
epoch [105/200] batch [10/61] time 0.407 (0.459) data 0.275 (0.328) loss_u loss_u 0.8818 (0.9014) acc_u 21.8750 (12.1875) lr 9.5289e-04 eta 0:00:23
epoch [105/200] batch [15/61] time 0.378 (0.454) data 0.248 (0.324) loss_u loss_u 0.8184 (0.8919) acc_u 21.8750 (12.9167) lr 9.5289e-04 eta 0:00:20
epoch [105/200] batch [20/61] time 0.335 (0.455) data 0.205 (0.325) loss_u loss_u 0.8584 (0.8808) acc_u 18.7500 (14.6875) lr 9.5289e-04 eta 0:00:18
epoch [105/200] batch [25/61] time 0.463 (0.453) data 0.331 (0.322) loss_u loss_u 0.9175 (0.8803) acc_u 12.5000 (14.7500) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [30/61] time 0.476 (0.454) data 0.346 (0.323) loss_u loss_u 0.9014 (0.8861) acc_u 15.6250 (14.1667) lr 9.5289e-04 eta 0:00:14
epoch [105/200] batch [35/61] time 0.363 (0.451) data 0.232 (0.321) loss_u loss_u 0.8999 (0.8832) acc_u 18.7500 (14.6429) lr 9.5289e-04 eta 0:00:11
epoch [105/200] batch [40/61] time 0.459 (0.455) data 0.328 (0.324) loss_u loss_u 0.9146 (0.8854) acc_u 6.2500 (14.4531) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [45/61] time 0.489 (0.455) data 0.357 (0.324) loss_u loss_u 0.9165 (0.8843) acc_u 9.3750 (14.5139) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [50/61] time 0.394 (0.453) data 0.264 (0.322) loss_u loss_u 0.8628 (0.8804) acc_u 18.7500 (14.9375) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [55/61] time 0.541 (0.453) data 0.410 (0.322) loss_u loss_u 0.8623 (0.8829) acc_u 28.1250 (15.0000) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [60/61] time 0.352 (0.452) data 0.220 (0.321) loss_u loss_u 0.9795 (0.8854) acc_u 0.0000 (14.5833) lr 9.5289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1439
confident_label rate tensor(0.3696, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1159
clean true:1153
clean false:6
clean_rate:0.994823123382226
noisy true:544
noisy false:1433
after delete: len(clean_dataset) 1159
after delete: len(noisy_dataset) 1977
epoch [106/200] batch [5/36] time 0.343 (0.444) data 0.212 (0.313) loss_x loss_x 1.3281 (1.1648) acc_x 78.1250 (75.6250) lr 9.3721e-04 eta 0:00:13
epoch [106/200] batch [10/36] time 0.630 (0.514) data 0.499 (0.383) loss_x loss_x 1.2227 (1.1346) acc_x 65.6250 (72.5000) lr 9.3721e-04 eta 0:00:13
epoch [106/200] batch [15/36] time 0.594 (0.517) data 0.463 (0.386) loss_x loss_x 1.5400 (1.1563) acc_x 68.7500 (72.5000) lr 9.3721e-04 eta 0:00:10
epoch [106/200] batch [20/36] time 0.504 (0.509) data 0.373 (0.378) loss_x loss_x 1.3027 (1.1078) acc_x 65.6250 (72.9688) lr 9.3721e-04 eta 0:00:08
epoch [106/200] batch [25/36] time 0.459 (0.501) data 0.328 (0.370) loss_x loss_x 1.2725 (1.0856) acc_x 71.8750 (73.5000) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [30/36] time 0.418 (0.485) data 0.287 (0.355) loss_x loss_x 1.2959 (1.1186) acc_x 68.7500 (72.9167) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [35/36] time 0.358 (0.480) data 0.227 (0.349) loss_x loss_x 1.3574 (1.1158) acc_x 71.8750 (73.3929) lr 9.3721e-04 eta 0:00:00
epoch [106/200] batch [5/61] time 0.433 (0.479) data 0.302 (0.348) loss_u loss_u 0.8721 (0.8705) acc_u 15.6250 (17.5000) lr 9.3721e-04 eta 0:00:26
epoch [106/200] batch [10/61] time 0.398 (0.472) data 0.267 (0.341) loss_u loss_u 0.7168 (0.8613) acc_u 40.6250 (17.8125) lr 9.3721e-04 eta 0:00:24
epoch [106/200] batch [15/61] time 0.383 (0.472) data 0.251 (0.341) loss_u loss_u 0.8457 (0.8661) acc_u 21.8750 (17.2917) lr 9.3721e-04 eta 0:00:21
epoch [106/200] batch [20/61] time 0.406 (0.466) data 0.274 (0.335) loss_u loss_u 0.8232 (0.8680) acc_u 25.0000 (17.1875) lr 9.3721e-04 eta 0:00:19
epoch [106/200] batch [25/61] time 0.451 (0.463) data 0.321 (0.332) loss_u loss_u 0.9038 (0.8694) acc_u 9.3750 (16.8750) lr 9.3721e-04 eta 0:00:16
epoch [106/200] batch [30/61] time 0.392 (0.461) data 0.261 (0.330) loss_u loss_u 0.9248 (0.8763) acc_u 12.5000 (16.0417) lr 9.3721e-04 eta 0:00:14
epoch [106/200] batch [35/61] time 0.552 (0.459) data 0.422 (0.328) loss_u loss_u 0.7852 (0.8722) acc_u 25.0000 (16.4286) lr 9.3721e-04 eta 0:00:11
epoch [106/200] batch [40/61] time 0.518 (0.456) data 0.387 (0.325) loss_u loss_u 0.9585 (0.8754) acc_u 3.1250 (16.0156) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [45/61] time 0.393 (0.456) data 0.261 (0.325) loss_u loss_u 0.8389 (0.8748) acc_u 21.8750 (16.1806) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [50/61] time 0.372 (0.454) data 0.240 (0.323) loss_u loss_u 0.8452 (0.8715) acc_u 15.6250 (16.6250) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [55/61] time 0.417 (0.455) data 0.285 (0.323) loss_u loss_u 0.8691 (0.8714) acc_u 18.7500 (16.6477) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [60/61] time 0.357 (0.454) data 0.225 (0.323) loss_u loss_u 0.8340 (0.8706) acc_u 18.7500 (16.6146) lr 9.3721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1417
confident_label rate tensor(0.3766, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1181
clean true:1175
clean false:6
clean_rate:0.9949195596951735
noisy true:544
noisy false:1411
after delete: len(clean_dataset) 1181
after delete: len(noisy_dataset) 1955
epoch [107/200] batch [5/36] time 0.511 (0.498) data 0.378 (0.367) loss_x loss_x 0.6431 (1.0907) acc_x 87.5000 (75.0000) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [10/36] time 0.461 (0.505) data 0.330 (0.373) loss_x loss_x 1.1172 (1.1013) acc_x 65.6250 (73.4375) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [15/36] time 0.455 (0.495) data 0.325 (0.363) loss_x loss_x 1.1406 (1.0876) acc_x 71.8750 (73.1250) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [20/36] time 0.427 (0.474) data 0.296 (0.343) loss_x loss_x 1.0176 (1.1419) acc_x 81.2500 (72.3438) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [25/36] time 0.361 (0.465) data 0.230 (0.334) loss_x loss_x 1.0400 (1.1170) acc_x 59.3750 (72.0000) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [30/36] time 0.398 (0.462) data 0.267 (0.331) loss_x loss_x 1.5293 (1.1402) acc_x 56.2500 (71.9792) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [35/36] time 0.469 (0.461) data 0.338 (0.330) loss_x loss_x 1.5977 (1.1571) acc_x 65.6250 (71.7857) lr 9.2154e-04 eta 0:00:00
epoch [107/200] batch [5/61] time 0.493 (0.453) data 0.362 (0.322) loss_u loss_u 0.8301 (0.8924) acc_u 25.0000 (14.3750) lr 9.2154e-04 eta 0:00:25
epoch [107/200] batch [10/61] time 0.440 (0.451) data 0.310 (0.320) loss_u loss_u 0.8892 (0.8879) acc_u 15.6250 (14.6875) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [15/61] time 0.516 (0.453) data 0.385 (0.322) loss_u loss_u 0.8911 (0.8717) acc_u 9.3750 (15.8333) lr 9.2154e-04 eta 0:00:20
epoch [107/200] batch [20/61] time 0.345 (0.450) data 0.214 (0.319) loss_u loss_u 0.8774 (0.8705) acc_u 12.5000 (16.5625) lr 9.2154e-04 eta 0:00:18
epoch [107/200] batch [25/61] time 0.369 (0.451) data 0.237 (0.320) loss_u loss_u 0.8188 (0.8735) acc_u 25.0000 (16.3750) lr 9.2154e-04 eta 0:00:16
epoch [107/200] batch [30/61] time 0.411 (0.448) data 0.281 (0.317) loss_u loss_u 0.8394 (0.8704) acc_u 18.7500 (16.1458) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [35/61] time 0.399 (0.449) data 0.267 (0.317) loss_u loss_u 0.8442 (0.8771) acc_u 15.6250 (15.0893) lr 9.2154e-04 eta 0:00:11
epoch [107/200] batch [40/61] time 0.434 (0.445) data 0.302 (0.313) loss_u loss_u 0.9629 (0.8796) acc_u 6.2500 (15.0781) lr 9.2154e-04 eta 0:00:09
epoch [107/200] batch [45/61] time 0.371 (0.445) data 0.239 (0.313) loss_u loss_u 0.7876 (0.8742) acc_u 28.1250 (15.8333) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [50/61] time 0.411 (0.445) data 0.279 (0.314) loss_u loss_u 0.9409 (0.8758) acc_u 9.3750 (15.8750) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [55/61] time 0.457 (0.444) data 0.325 (0.313) loss_u loss_u 0.9136 (0.8779) acc_u 9.3750 (15.7386) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [60/61] time 0.348 (0.443) data 0.216 (0.312) loss_u loss_u 0.8916 (0.8754) acc_u 15.6250 (16.0417) lr 9.2154e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1406
confident_label rate tensor(0.3737, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1172
clean true:1165
clean false:7
clean_rate:0.9940273037542662
noisy true:565
noisy false:1399
after delete: len(clean_dataset) 1172
after delete: len(noisy_dataset) 1964
epoch [108/200] batch [5/36] time 0.352 (0.491) data 0.222 (0.360) loss_x loss_x 0.8999 (1.2133) acc_x 78.1250 (70.0000) lr 9.0589e-04 eta 0:00:15
epoch [108/200] batch [10/36] time 0.513 (0.483) data 0.380 (0.352) loss_x loss_x 1.2324 (1.2429) acc_x 59.3750 (70.6250) lr 9.0589e-04 eta 0:00:12
epoch [108/200] batch [15/36] time 0.621 (0.478) data 0.490 (0.347) loss_x loss_x 1.3574 (1.2329) acc_x 68.7500 (70.8333) lr 9.0589e-04 eta 0:00:10
epoch [108/200] batch [20/36] time 0.537 (0.472) data 0.406 (0.341) loss_x loss_x 1.2705 (1.2025) acc_x 71.8750 (71.2500) lr 9.0589e-04 eta 0:00:07
epoch [108/200] batch [25/36] time 0.460 (0.465) data 0.329 (0.334) loss_x loss_x 1.1455 (1.2446) acc_x 68.7500 (70.0000) lr 9.0589e-04 eta 0:00:05
epoch [108/200] batch [30/36] time 0.462 (0.465) data 0.331 (0.334) loss_x loss_x 0.6733 (1.2490) acc_x 75.0000 (69.3750) lr 9.0589e-04 eta 0:00:02
epoch [108/200] batch [35/36] time 0.689 (0.476) data 0.558 (0.345) loss_x loss_x 1.2109 (1.2422) acc_x 71.8750 (69.2857) lr 9.0589e-04 eta 0:00:00
epoch [108/200] batch [5/61] time 0.365 (0.467) data 0.234 (0.336) loss_u loss_u 0.8477 (0.8651) acc_u 21.8750 (16.8750) lr 9.0589e-04 eta 0:00:26
epoch [108/200] batch [10/61] time 0.414 (0.462) data 0.284 (0.331) loss_u loss_u 0.8477 (0.8757) acc_u 21.8750 (16.2500) lr 9.0589e-04 eta 0:00:23
epoch [108/200] batch [15/61] time 0.407 (0.459) data 0.277 (0.328) loss_u loss_u 0.9429 (0.8941) acc_u 6.2500 (14.5833) lr 9.0589e-04 eta 0:00:21
epoch [108/200] batch [20/61] time 0.398 (0.456) data 0.268 (0.325) loss_u loss_u 0.8037 (0.8859) acc_u 28.1250 (15.7812) lr 9.0589e-04 eta 0:00:18
epoch [108/200] batch [25/61] time 0.355 (0.455) data 0.225 (0.324) loss_u loss_u 0.7725 (0.8789) acc_u 25.0000 (16.2500) lr 9.0589e-04 eta 0:00:16
epoch [108/200] batch [30/61] time 0.580 (0.454) data 0.449 (0.323) loss_u loss_u 0.8975 (0.8801) acc_u 9.3750 (16.0417) lr 9.0589e-04 eta 0:00:14
epoch [108/200] batch [35/61] time 0.357 (0.452) data 0.225 (0.321) loss_u loss_u 0.9014 (0.8806) acc_u 15.6250 (15.8929) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [40/61] time 0.547 (0.450) data 0.415 (0.319) loss_u loss_u 0.9072 (0.8803) acc_u 12.5000 (15.7031) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [45/61] time 0.483 (0.452) data 0.351 (0.321) loss_u loss_u 0.8516 (0.8784) acc_u 18.7500 (16.0417) lr 9.0589e-04 eta 0:00:07
epoch [108/200] batch [50/61] time 0.443 (0.450) data 0.311 (0.319) loss_u loss_u 0.8506 (0.8774) acc_u 15.6250 (16.0000) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [55/61] time 0.425 (0.452) data 0.292 (0.321) loss_u loss_u 0.8594 (0.8785) acc_u 21.8750 (15.9659) lr 9.0589e-04 eta 0:00:02
epoch [108/200] batch [60/61] time 0.432 (0.455) data 0.300 (0.324) loss_u loss_u 0.7764 (0.8754) acc_u 34.3750 (16.3021) lr 9.0589e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.3753, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1177
clean true:1171
clean false:6
clean_rate:0.9949022939677146
noisy true:531
noisy false:1428
after delete: len(clean_dataset) 1177
after delete: len(noisy_dataset) 1959
epoch [109/200] batch [5/36] time 0.393 (0.462) data 0.262 (0.331) loss_x loss_x 1.6562 (1.3095) acc_x 62.5000 (70.0000) lr 8.9027e-04 eta 0:00:14
epoch [109/200] batch [10/36] time 0.418 (0.452) data 0.287 (0.321) loss_x loss_x 0.8965 (1.1625) acc_x 78.1250 (72.8125) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [15/36] time 0.399 (0.443) data 0.268 (0.312) loss_x loss_x 1.1592 (1.2672) acc_x 68.7500 (69.1667) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [20/36] time 0.446 (0.449) data 0.315 (0.318) loss_x loss_x 1.0479 (1.2807) acc_x 68.7500 (69.5312) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [25/36] time 0.488 (0.450) data 0.357 (0.319) loss_x loss_x 1.1172 (1.2973) acc_x 75.0000 (69.3750) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [30/36] time 0.431 (0.451) data 0.300 (0.320) loss_x loss_x 1.4443 (1.2769) acc_x 65.6250 (69.3750) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [35/36] time 0.405 (0.455) data 0.274 (0.324) loss_x loss_x 0.6938 (1.2225) acc_x 78.1250 (70.1786) lr 8.9027e-04 eta 0:00:00
epoch [109/200] batch [5/61] time 0.430 (0.449) data 0.298 (0.317) loss_u loss_u 0.8345 (0.8674) acc_u 15.6250 (14.3750) lr 8.9027e-04 eta 0:00:25
epoch [109/200] batch [10/61] time 0.329 (0.447) data 0.197 (0.316) loss_u loss_u 0.8535 (0.8761) acc_u 21.8750 (14.6875) lr 8.9027e-04 eta 0:00:22
epoch [109/200] batch [15/61] time 0.529 (0.448) data 0.397 (0.316) loss_u loss_u 0.8979 (0.8800) acc_u 12.5000 (14.1667) lr 8.9027e-04 eta 0:00:20
epoch [109/200] batch [20/61] time 0.810 (0.457) data 0.678 (0.325) loss_u loss_u 0.7900 (0.8711) acc_u 25.0000 (14.8438) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [25/61] time 0.523 (0.458) data 0.391 (0.327) loss_u loss_u 0.8584 (0.8618) acc_u 18.7500 (16.2500) lr 8.9027e-04 eta 0:00:16
epoch [109/200] batch [30/61] time 0.610 (0.457) data 0.478 (0.326) loss_u loss_u 0.8047 (0.8654) acc_u 25.0000 (15.9375) lr 8.9027e-04 eta 0:00:14
epoch [109/200] batch [35/61] time 0.366 (0.456) data 0.234 (0.325) loss_u loss_u 0.9390 (0.8708) acc_u 9.3750 (15.7143) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [40/61] time 0.360 (0.454) data 0.229 (0.323) loss_u loss_u 0.9009 (0.8740) acc_u 15.6250 (15.6250) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [45/61] time 0.391 (0.450) data 0.261 (0.319) loss_u loss_u 0.8794 (0.8765) acc_u 18.7500 (15.3472) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [50/61] time 0.408 (0.447) data 0.276 (0.315) loss_u loss_u 0.8315 (0.8727) acc_u 25.0000 (16.1250) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [55/61] time 0.363 (0.444) data 0.231 (0.313) loss_u loss_u 0.9238 (0.8740) acc_u 9.3750 (16.0795) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [60/61] time 0.448 (0.444) data 0.316 (0.313) loss_u loss_u 0.9326 (0.8767) acc_u 3.1250 (15.6250) lr 8.9027e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1437
confident_label rate tensor(0.3705, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1162
clean true:1155
clean false:7
clean_rate:0.9939759036144579
noisy true:544
noisy false:1430
after delete: len(clean_dataset) 1162
after delete: len(noisy_dataset) 1974
epoch [110/200] batch [5/36] time 0.459 (0.456) data 0.328 (0.325) loss_x loss_x 1.4707 (1.1672) acc_x 65.6250 (73.1250) lr 8.7467e-04 eta 0:00:14
epoch [110/200] batch [10/36] time 0.486 (0.480) data 0.354 (0.349) loss_x loss_x 0.8721 (1.1350) acc_x 75.0000 (73.1250) lr 8.7467e-04 eta 0:00:12
epoch [110/200] batch [15/36] time 0.432 (0.494) data 0.302 (0.362) loss_x loss_x 1.0820 (1.1340) acc_x 75.0000 (72.9167) lr 8.7467e-04 eta 0:00:10
epoch [110/200] batch [20/36] time 0.498 (0.473) data 0.367 (0.341) loss_x loss_x 0.9873 (1.1408) acc_x 78.1250 (72.3438) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [25/36] time 0.575 (0.469) data 0.444 (0.338) loss_x loss_x 1.1182 (1.1400) acc_x 65.6250 (71.6250) lr 8.7467e-04 eta 0:00:05
epoch [110/200] batch [30/36] time 0.420 (0.464) data 0.289 (0.333) loss_x loss_x 1.1045 (1.1318) acc_x 75.0000 (71.7708) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [35/36] time 0.377 (0.459) data 0.247 (0.328) loss_x loss_x 0.7812 (1.1095) acc_x 81.2500 (72.2321) lr 8.7467e-04 eta 0:00:00
epoch [110/200] batch [5/61] time 0.403 (0.455) data 0.272 (0.324) loss_u loss_u 0.8135 (0.8521) acc_u 18.7500 (17.5000) lr 8.7467e-04 eta 0:00:25
epoch [110/200] batch [10/61] time 0.465 (0.455) data 0.334 (0.324) loss_u loss_u 0.8511 (0.8619) acc_u 15.6250 (15.9375) lr 8.7467e-04 eta 0:00:23
epoch [110/200] batch [15/61] time 0.577 (0.453) data 0.446 (0.322) loss_u loss_u 0.8662 (0.8790) acc_u 25.0000 (14.3750) lr 8.7467e-04 eta 0:00:20
epoch [110/200] batch [20/61] time 0.509 (0.449) data 0.377 (0.317) loss_u loss_u 0.8945 (0.8840) acc_u 15.6250 (14.3750) lr 8.7467e-04 eta 0:00:18
epoch [110/200] batch [25/61] time 0.445 (0.447) data 0.315 (0.316) loss_u loss_u 0.9058 (0.8860) acc_u 12.5000 (14.3750) lr 8.7467e-04 eta 0:00:16
epoch [110/200] batch [30/61] time 0.442 (0.444) data 0.312 (0.313) loss_u loss_u 0.8545 (0.8819) acc_u 18.7500 (15.1042) lr 8.7467e-04 eta 0:00:13
epoch [110/200] batch [35/61] time 0.462 (0.444) data 0.332 (0.313) loss_u loss_u 0.8789 (0.8766) acc_u 15.6250 (15.9821) lr 8.7467e-04 eta 0:00:11
epoch [110/200] batch [40/61] time 0.444 (0.450) data 0.313 (0.319) loss_u loss_u 0.8721 (0.8773) acc_u 12.5000 (15.8594) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [45/61] time 0.543 (0.449) data 0.413 (0.317) loss_u loss_u 0.8853 (0.8757) acc_u 15.6250 (15.9028) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [50/61] time 0.380 (0.448) data 0.248 (0.317) loss_u loss_u 0.8823 (0.8776) acc_u 15.6250 (15.8125) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [55/61] time 0.608 (0.448) data 0.476 (0.317) loss_u loss_u 0.8984 (0.8771) acc_u 15.6250 (15.7955) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [60/61] time 0.411 (0.447) data 0.279 (0.316) loss_u loss_u 0.8672 (0.8764) acc_u 18.7500 (15.8333) lr 8.7467e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.3705, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1162
clean true:1154
clean false:8
clean_rate:0.9931153184165232
noisy true:562
noisy false:1412
after delete: len(clean_dataset) 1162
after delete: len(noisy_dataset) 1974
epoch [111/200] batch [5/36] time 0.570 (0.465) data 0.439 (0.334) loss_x loss_x 1.1650 (1.0243) acc_x 71.8750 (76.8750) lr 8.5910e-04 eta 0:00:14
epoch [111/200] batch [10/36] time 0.408 (0.443) data 0.277 (0.311) loss_x loss_x 1.1709 (1.1203) acc_x 68.7500 (73.4375) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [15/36] time 0.395 (0.440) data 0.263 (0.309) loss_x loss_x 0.8662 (1.0886) acc_x 68.7500 (73.9583) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [20/36] time 0.344 (0.456) data 0.212 (0.324) loss_x loss_x 0.6733 (1.1103) acc_x 84.3750 (73.5938) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [25/36] time 0.428 (0.464) data 0.297 (0.333) loss_x loss_x 1.1680 (1.0961) acc_x 68.7500 (74.0000) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [30/36] time 0.467 (0.459) data 0.336 (0.328) loss_x loss_x 1.0410 (1.0939) acc_x 71.8750 (73.8542) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [35/36] time 0.408 (0.452) data 0.277 (0.321) loss_x loss_x 1.2441 (1.0972) acc_x 68.7500 (73.4821) lr 8.5910e-04 eta 0:00:00
epoch [111/200] batch [5/61] time 0.467 (0.452) data 0.335 (0.321) loss_u loss_u 0.8525 (0.8754) acc_u 12.5000 (15.6250) lr 8.5910e-04 eta 0:00:25
epoch [111/200] batch [10/61] time 0.577 (0.449) data 0.446 (0.318) loss_u loss_u 0.8647 (0.8783) acc_u 15.6250 (14.6875) lr 8.5910e-04 eta 0:00:22
epoch [111/200] batch [15/61] time 0.449 (0.448) data 0.317 (0.317) loss_u loss_u 0.8789 (0.8737) acc_u 18.7500 (16.2500) lr 8.5910e-04 eta 0:00:20
epoch [111/200] batch [20/61] time 0.436 (0.453) data 0.305 (0.322) loss_u loss_u 0.8379 (0.8653) acc_u 28.1250 (17.8125) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [25/61] time 0.598 (0.453) data 0.466 (0.322) loss_u loss_u 0.9297 (0.8688) acc_u 12.5000 (17.5000) lr 8.5910e-04 eta 0:00:16
epoch [111/200] batch [30/61] time 0.360 (0.451) data 0.229 (0.320) loss_u loss_u 0.8755 (0.8687) acc_u 18.7500 (17.2917) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [35/61] time 0.494 (0.451) data 0.363 (0.320) loss_u loss_u 0.9414 (0.8718) acc_u 6.2500 (16.7857) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [40/61] time 0.371 (0.446) data 0.241 (0.315) loss_u loss_u 0.9268 (0.8722) acc_u 9.3750 (16.4062) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [45/61] time 0.408 (0.442) data 0.276 (0.310) loss_u loss_u 0.7769 (0.8659) acc_u 31.2500 (17.0833) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [50/61] time 0.445 (0.442) data 0.314 (0.310) loss_u loss_u 0.8618 (0.8681) acc_u 18.7500 (17.0000) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [55/61] time 0.485 (0.443) data 0.354 (0.312) loss_u loss_u 0.8857 (0.8714) acc_u 15.6250 (16.5909) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [60/61] time 0.657 (0.442) data 0.525 (0.311) loss_u loss_u 0.9233 (0.8699) acc_u 9.3750 (16.6146) lr 8.5910e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1468
confident_label rate tensor(0.3597, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1128
clean true:1119
clean false:9
clean_rate:0.9920212765957447
noisy true:549
noisy false:1459
after delete: len(clean_dataset) 1128
after delete: len(noisy_dataset) 2008
epoch [112/200] batch [5/35] time 0.522 (0.427) data 0.391 (0.296) loss_x loss_x 1.2842 (1.2945) acc_x 71.8750 (71.8750) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [10/35] time 0.354 (0.431) data 0.222 (0.300) loss_x loss_x 1.4580 (1.2714) acc_x 65.6250 (69.0625) lr 8.4357e-04 eta 0:00:10
epoch [112/200] batch [15/35] time 0.430 (0.442) data 0.299 (0.311) loss_x loss_x 0.6069 (1.1221) acc_x 87.5000 (72.5000) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [20/35] time 0.354 (0.438) data 0.223 (0.307) loss_x loss_x 1.6953 (1.1747) acc_x 65.6250 (72.9688) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [25/35] time 0.355 (0.438) data 0.224 (0.307) loss_x loss_x 1.0361 (1.1686) acc_x 78.1250 (72.5000) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [30/35] time 0.560 (0.437) data 0.429 (0.306) loss_x loss_x 1.1914 (1.1516) acc_x 75.0000 (72.0833) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [35/35] time 0.376 (0.439) data 0.245 (0.308) loss_x loss_x 0.8369 (1.1463) acc_x 78.1250 (72.5893) lr 8.4357e-04 eta 0:00:00
epoch [112/200] batch [5/62] time 0.619 (0.446) data 0.488 (0.314) loss_u loss_u 0.8091 (0.8342) acc_u 25.0000 (22.5000) lr 8.4357e-04 eta 0:00:25
epoch [112/200] batch [10/62] time 0.314 (0.441) data 0.182 (0.310) loss_u loss_u 0.8994 (0.8516) acc_u 6.2500 (19.6875) lr 8.4357e-04 eta 0:00:22
epoch [112/200] batch [15/62] time 0.391 (0.444) data 0.260 (0.313) loss_u loss_u 0.8833 (0.8537) acc_u 15.6250 (19.3750) lr 8.4357e-04 eta 0:00:20
epoch [112/200] batch [20/62] time 0.399 (0.447) data 0.267 (0.315) loss_u loss_u 0.8462 (0.8652) acc_u 15.6250 (17.9688) lr 8.4357e-04 eta 0:00:18
epoch [112/200] batch [25/62] time 0.348 (0.443) data 0.216 (0.311) loss_u loss_u 0.8550 (0.8682) acc_u 18.7500 (17.7500) lr 8.4357e-04 eta 0:00:16
epoch [112/200] batch [30/62] time 0.351 (0.445) data 0.219 (0.313) loss_u loss_u 0.8359 (0.8645) acc_u 25.0000 (18.0208) lr 8.4357e-04 eta 0:00:14
epoch [112/200] batch [35/62] time 0.382 (0.441) data 0.250 (0.310) loss_u loss_u 0.9033 (0.8629) acc_u 9.3750 (17.6786) lr 8.4357e-04 eta 0:00:11
epoch [112/200] batch [40/62] time 0.441 (0.444) data 0.309 (0.312) loss_u loss_u 0.8491 (0.8645) acc_u 15.6250 (17.2656) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [45/62] time 0.339 (0.442) data 0.207 (0.311) loss_u loss_u 0.9072 (0.8646) acc_u 9.3750 (17.4306) lr 8.4357e-04 eta 0:00:07
epoch [112/200] batch [50/62] time 0.431 (0.441) data 0.299 (0.309) loss_u loss_u 0.7837 (0.8658) acc_u 25.0000 (17.2500) lr 8.4357e-04 eta 0:00:05
epoch [112/200] batch [55/62] time 0.783 (0.448) data 0.651 (0.317) loss_u loss_u 0.8325 (0.8656) acc_u 18.7500 (17.1591) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [60/62] time 0.399 (0.445) data 0.268 (0.313) loss_u loss_u 0.8442 (0.8634) acc_u 21.8750 (17.4479) lr 8.4357e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.3769, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1182
clean true:1175
clean false:7
clean_rate:0.994077834179357
noisy true:548
noisy false:1406
after delete: len(clean_dataset) 1182
after delete: len(noisy_dataset) 1954
epoch [113/200] batch [5/36] time 0.407 (0.423) data 0.274 (0.292) loss_x loss_x 1.0166 (1.2053) acc_x 65.6250 (73.1250) lr 8.2807e-04 eta 0:00:13
epoch [113/200] batch [10/36] time 0.357 (0.443) data 0.226 (0.311) loss_x loss_x 0.9209 (1.0949) acc_x 78.1250 (75.6250) lr 8.2807e-04 eta 0:00:11
epoch [113/200] batch [15/36] time 0.454 (0.447) data 0.324 (0.316) loss_x loss_x 0.8994 (1.0968) acc_x 78.1250 (75.0000) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [20/36] time 0.407 (0.440) data 0.276 (0.309) loss_x loss_x 1.8535 (1.1318) acc_x 43.7500 (73.5938) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [25/36] time 0.440 (0.441) data 0.310 (0.310) loss_x loss_x 1.3291 (1.1517) acc_x 71.8750 (73.2500) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [30/36] time 0.416 (0.441) data 0.286 (0.310) loss_x loss_x 1.1650 (1.1678) acc_x 78.1250 (73.1250) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [35/36] time 0.538 (0.450) data 0.408 (0.319) loss_x loss_x 1.1211 (1.1552) acc_x 71.8750 (73.2143) lr 8.2807e-04 eta 0:00:00
epoch [113/200] batch [5/61] time 0.375 (0.448) data 0.243 (0.317) loss_u loss_u 0.9072 (0.8611) acc_u 9.3750 (16.8750) lr 8.2807e-04 eta 0:00:25
epoch [113/200] batch [10/61] time 0.573 (0.454) data 0.440 (0.323) loss_u loss_u 0.8218 (0.8645) acc_u 21.8750 (16.8750) lr 8.2807e-04 eta 0:00:23
epoch [113/200] batch [15/61] time 0.520 (0.454) data 0.389 (0.323) loss_u loss_u 0.8408 (0.8767) acc_u 15.6250 (15.2083) lr 8.2807e-04 eta 0:00:20
epoch [113/200] batch [20/61] time 0.354 (0.451) data 0.223 (0.320) loss_u loss_u 0.8833 (0.8745) acc_u 12.5000 (15.4688) lr 8.2807e-04 eta 0:00:18
epoch [113/200] batch [25/61] time 0.412 (0.447) data 0.280 (0.315) loss_u loss_u 0.8716 (0.8762) acc_u 21.8750 (15.5000) lr 8.2807e-04 eta 0:00:16
epoch [113/200] batch [30/61] time 0.410 (0.446) data 0.279 (0.315) loss_u loss_u 0.8535 (0.8689) acc_u 18.7500 (16.2500) lr 8.2807e-04 eta 0:00:13
epoch [113/200] batch [35/61] time 0.285 (0.445) data 0.153 (0.314) loss_u loss_u 0.8799 (0.8697) acc_u 15.6250 (16.3393) lr 8.2807e-04 eta 0:00:11
epoch [113/200] batch [40/61] time 0.331 (0.442) data 0.200 (0.310) loss_u loss_u 0.8643 (0.8736) acc_u 12.5000 (15.5469) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [45/61] time 0.380 (0.441) data 0.248 (0.309) loss_u loss_u 0.8950 (0.8722) acc_u 12.5000 (15.9028) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [50/61] time 0.487 (0.440) data 0.356 (0.309) loss_u loss_u 0.9014 (0.8730) acc_u 9.3750 (15.6875) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [55/61] time 0.406 (0.438) data 0.275 (0.307) loss_u loss_u 0.8877 (0.8754) acc_u 15.6250 (15.3977) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [60/61] time 0.437 (0.443) data 0.305 (0.312) loss_u loss_u 0.9248 (0.8725) acc_u 6.2500 (15.7292) lr 8.2807e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1414
confident_label rate tensor(0.3766, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1181
clean true:1175
clean false:6
clean_rate:0.9949195596951735
noisy true:547
noisy false:1408
after delete: len(clean_dataset) 1181
after delete: len(noisy_dataset) 1955
epoch [114/200] batch [5/36] time 0.461 (0.454) data 0.330 (0.323) loss_x loss_x 1.2891 (1.1597) acc_x 59.3750 (68.7500) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [10/36] time 0.425 (0.463) data 0.294 (0.332) loss_x loss_x 1.2168 (1.0743) acc_x 62.5000 (72.1875) lr 8.1262e-04 eta 0:00:12
epoch [114/200] batch [15/36] time 0.408 (0.473) data 0.278 (0.342) loss_x loss_x 1.6914 (1.1215) acc_x 71.8750 (70.6250) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [20/36] time 0.554 (0.476) data 0.422 (0.345) loss_x loss_x 0.9409 (1.0902) acc_x 75.0000 (72.1875) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [25/36] time 0.338 (0.473) data 0.207 (0.342) loss_x loss_x 0.8623 (1.1068) acc_x 81.2500 (71.7500) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [30/36] time 0.532 (0.476) data 0.401 (0.345) loss_x loss_x 1.2598 (1.1162) acc_x 68.7500 (71.4583) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [35/36] time 0.377 (0.471) data 0.246 (0.340) loss_x loss_x 0.7944 (1.1013) acc_x 75.0000 (71.3393) lr 8.1262e-04 eta 0:00:00
epoch [114/200] batch [5/61] time 0.468 (0.467) data 0.336 (0.336) loss_u loss_u 0.8169 (0.8806) acc_u 21.8750 (16.2500) lr 8.1262e-04 eta 0:00:26
epoch [114/200] batch [10/61] time 0.422 (0.463) data 0.291 (0.332) loss_u loss_u 0.7939 (0.8640) acc_u 28.1250 (17.8125) lr 8.1262e-04 eta 0:00:23
epoch [114/200] batch [15/61] time 0.325 (0.461) data 0.194 (0.330) loss_u loss_u 0.8970 (0.8656) acc_u 9.3750 (17.9167) lr 8.1262e-04 eta 0:00:21
epoch [114/200] batch [20/61] time 0.386 (0.460) data 0.254 (0.329) loss_u loss_u 0.8804 (0.8657) acc_u 18.7500 (17.5000) lr 8.1262e-04 eta 0:00:18
epoch [114/200] batch [25/61] time 0.440 (0.462) data 0.309 (0.331) loss_u loss_u 0.8242 (0.8647) acc_u 25.0000 (17.5000) lr 8.1262e-04 eta 0:00:16
epoch [114/200] batch [30/61] time 0.400 (0.457) data 0.269 (0.326) loss_u loss_u 0.8750 (0.8686) acc_u 12.5000 (16.8750) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [35/61] time 0.374 (0.452) data 0.243 (0.321) loss_u loss_u 0.8691 (0.8691) acc_u 18.7500 (16.8750) lr 8.1262e-04 eta 0:00:11
epoch [114/200] batch [40/61] time 0.393 (0.454) data 0.262 (0.323) loss_u loss_u 0.8506 (0.8706) acc_u 15.6250 (16.4844) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [45/61] time 0.515 (0.452) data 0.382 (0.321) loss_u loss_u 0.8672 (0.8726) acc_u 15.6250 (16.1806) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [50/61] time 0.497 (0.454) data 0.363 (0.322) loss_u loss_u 0.8857 (0.8699) acc_u 12.5000 (16.7500) lr 8.1262e-04 eta 0:00:04
epoch [114/200] batch [55/61] time 0.415 (0.454) data 0.283 (0.323) loss_u loss_u 0.8872 (0.8725) acc_u 18.7500 (16.4205) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [60/61] time 0.418 (0.453) data 0.288 (0.321) loss_u loss_u 0.8159 (0.8719) acc_u 21.8750 (16.5104) lr 8.1262e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1403
confident_label rate tensor(0.3814, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1196
clean true:1191
clean false:5
clean_rate:0.995819397993311
noisy true:542
noisy false:1398
after delete: len(clean_dataset) 1196
after delete: len(noisy_dataset) 1940
epoch [115/200] batch [5/37] time 0.459 (0.509) data 0.328 (0.377) loss_x loss_x 1.1396 (1.0070) acc_x 75.0000 (76.2500) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [10/37] time 0.411 (0.461) data 0.280 (0.330) loss_x loss_x 1.4336 (1.1449) acc_x 68.7500 (73.4375) lr 7.9721e-04 eta 0:00:12
epoch [115/200] batch [15/37] time 0.449 (0.462) data 0.318 (0.331) loss_x loss_x 1.3906 (1.1400) acc_x 59.3750 (71.6667) lr 7.9721e-04 eta 0:00:10
epoch [115/200] batch [20/37] time 0.438 (0.458) data 0.305 (0.327) loss_x loss_x 1.0605 (1.1421) acc_x 75.0000 (71.7188) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [25/37] time 0.591 (0.469) data 0.460 (0.338) loss_x loss_x 1.2158 (1.1192) acc_x 71.8750 (72.1250) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [30/37] time 0.444 (0.467) data 0.313 (0.336) loss_x loss_x 1.1631 (1.1042) acc_x 71.8750 (72.3958) lr 7.9721e-04 eta 0:00:03
epoch [115/200] batch [35/37] time 0.376 (0.465) data 0.245 (0.333) loss_x loss_x 1.0723 (1.1030) acc_x 65.6250 (72.3214) lr 7.9721e-04 eta 0:00:00
epoch [115/200] batch [5/60] time 0.437 (0.458) data 0.306 (0.327) loss_u loss_u 0.8418 (0.8698) acc_u 18.7500 (15.6250) lr 7.9721e-04 eta 0:00:25
epoch [115/200] batch [10/60] time 0.531 (0.458) data 0.399 (0.327) loss_u loss_u 0.9722 (0.8817) acc_u 3.1250 (13.4375) lr 7.9721e-04 eta 0:00:22
epoch [115/200] batch [15/60] time 0.431 (0.455) data 0.300 (0.323) loss_u loss_u 0.8667 (0.8849) acc_u 21.8750 (13.7500) lr 7.9721e-04 eta 0:00:20
epoch [115/200] batch [20/60] time 0.434 (0.451) data 0.303 (0.319) loss_u loss_u 0.8784 (0.8866) acc_u 15.6250 (14.2188) lr 7.9721e-04 eta 0:00:18
epoch [115/200] batch [25/60] time 0.423 (0.450) data 0.291 (0.318) loss_u loss_u 0.8706 (0.8870) acc_u 18.7500 (14.7500) lr 7.9721e-04 eta 0:00:15
epoch [115/200] batch [30/60] time 0.419 (0.447) data 0.288 (0.316) loss_u loss_u 0.9185 (0.8873) acc_u 6.2500 (14.4792) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [35/60] time 0.471 (0.447) data 0.339 (0.316) loss_u loss_u 0.8950 (0.8825) acc_u 15.6250 (15.0893) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [40/60] time 0.455 (0.450) data 0.323 (0.318) loss_u loss_u 0.8325 (0.8825) acc_u 18.7500 (15.2344) lr 7.9721e-04 eta 0:00:08
epoch [115/200] batch [45/60] time 0.386 (0.449) data 0.255 (0.317) loss_u loss_u 0.8057 (0.8816) acc_u 25.0000 (15.4861) lr 7.9721e-04 eta 0:00:06
epoch [115/200] batch [50/60] time 0.554 (0.450) data 0.423 (0.318) loss_u loss_u 0.8613 (0.8814) acc_u 12.5000 (15.3750) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [55/60] time 0.380 (0.450) data 0.246 (0.318) loss_u loss_u 0.8125 (0.8757) acc_u 25.0000 (16.0227) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [60/60] time 0.397 (0.448) data 0.261 (0.316) loss_u loss_u 0.8560 (0.8772) acc_u 15.6250 (15.7812) lr 7.9721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1422
confident_label rate tensor(0.3702, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1161
clean true:1153
clean false:8
clean_rate:0.9931093884582257
noisy true:561
noisy false:1414
after delete: len(clean_dataset) 1161
after delete: len(noisy_dataset) 1975
epoch [116/200] batch [5/36] time 0.427 (0.451) data 0.296 (0.320) loss_x loss_x 1.2217 (1.0934) acc_x 68.7500 (71.2500) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [10/36] time 0.441 (0.467) data 0.310 (0.336) loss_x loss_x 1.0879 (1.1348) acc_x 68.7500 (69.0625) lr 7.8186e-04 eta 0:00:12
epoch [116/200] batch [15/36] time 0.474 (0.468) data 0.343 (0.337) loss_x loss_x 1.3574 (1.1684) acc_x 71.8750 (70.2083) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [20/36] time 0.375 (0.459) data 0.244 (0.328) loss_x loss_x 1.0254 (1.1240) acc_x 75.0000 (71.7188) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [25/36] time 0.532 (0.467) data 0.402 (0.336) loss_x loss_x 0.8965 (1.1145) acc_x 81.2500 (71.8750) lr 7.8186e-04 eta 0:00:05
epoch [116/200] batch [30/36] time 0.456 (0.466) data 0.325 (0.335) loss_x loss_x 1.4062 (1.1419) acc_x 65.6250 (71.2500) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [35/36] time 0.469 (0.457) data 0.337 (0.326) loss_x loss_x 1.0039 (1.1574) acc_x 81.2500 (71.2500) lr 7.8186e-04 eta 0:00:00
epoch [116/200] batch [5/61] time 0.771 (0.461) data 0.638 (0.330) loss_u loss_u 0.8267 (0.8489) acc_u 21.8750 (19.3750) lr 7.8186e-04 eta 0:00:25
epoch [116/200] batch [10/61] time 0.371 (0.462) data 0.240 (0.331) loss_u loss_u 0.9136 (0.8644) acc_u 12.5000 (16.8750) lr 7.8186e-04 eta 0:00:23
epoch [116/200] batch [15/61] time 0.615 (0.462) data 0.485 (0.331) loss_u loss_u 0.8779 (0.8691) acc_u 15.6250 (16.8750) lr 7.8186e-04 eta 0:00:21
epoch [116/200] batch [20/61] time 0.553 (0.460) data 0.421 (0.329) loss_u loss_u 0.8911 (0.8691) acc_u 9.3750 (17.0312) lr 7.8186e-04 eta 0:00:18
epoch [116/200] batch [25/61] time 0.390 (0.457) data 0.259 (0.326) loss_u loss_u 0.8965 (0.8608) acc_u 9.3750 (18.1250) lr 7.8186e-04 eta 0:00:16
epoch [116/200] batch [30/61] time 0.476 (0.453) data 0.346 (0.322) loss_u loss_u 0.8848 (0.8650) acc_u 12.5000 (17.3958) lr 7.8186e-04 eta 0:00:14
epoch [116/200] batch [35/61] time 0.351 (0.452) data 0.221 (0.321) loss_u loss_u 0.8643 (0.8692) acc_u 18.7500 (16.6071) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [40/61] time 0.442 (0.453) data 0.311 (0.322) loss_u loss_u 0.8462 (0.8701) acc_u 18.7500 (16.4062) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [45/61] time 0.578 (0.452) data 0.447 (0.321) loss_u loss_u 0.8091 (0.8652) acc_u 18.7500 (16.7361) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [50/61] time 0.482 (0.453) data 0.351 (0.322) loss_u loss_u 0.8853 (0.8672) acc_u 9.3750 (16.2500) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [55/61] time 0.426 (0.452) data 0.296 (0.321) loss_u loss_u 0.7896 (0.8690) acc_u 31.2500 (16.0227) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [60/61] time 0.351 (0.448) data 0.220 (0.317) loss_u loss_u 0.8809 (0.8718) acc_u 12.5000 (15.6771) lr 7.8186e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1437
confident_label rate tensor(0.3702, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1161
clean true:1155
clean false:6
clean_rate:0.9948320413436692
noisy true:544
noisy false:1431
after delete: len(clean_dataset) 1161
after delete: len(noisy_dataset) 1975
epoch [117/200] batch [5/36] time 0.566 (0.517) data 0.435 (0.386) loss_x loss_x 1.1729 (1.4000) acc_x 75.0000 (68.1250) lr 7.6655e-04 eta 0:00:16
epoch [117/200] batch [10/36] time 0.544 (0.491) data 0.413 (0.360) loss_x loss_x 1.3789 (1.3589) acc_x 59.3750 (67.8125) lr 7.6655e-04 eta 0:00:12
epoch [117/200] batch [15/36] time 0.377 (0.471) data 0.246 (0.340) loss_x loss_x 1.1826 (1.2468) acc_x 65.6250 (69.3750) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [20/36] time 0.630 (0.485) data 0.498 (0.354) loss_x loss_x 0.9468 (1.1708) acc_x 75.0000 (70.6250) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [25/36] time 0.492 (0.495) data 0.358 (0.364) loss_x loss_x 1.0195 (1.1545) acc_x 68.7500 (71.0000) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [30/36] time 0.518 (0.502) data 0.385 (0.371) loss_x loss_x 1.1270 (1.1267) acc_x 71.8750 (71.3542) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [35/36] time 0.537 (0.500) data 0.406 (0.368) loss_x loss_x 1.0576 (1.1339) acc_x 75.0000 (71.2500) lr 7.6655e-04 eta 0:00:00
epoch [117/200] batch [5/61] time 0.445 (0.487) data 0.313 (0.356) loss_u loss_u 0.8765 (0.8755) acc_u 18.7500 (15.6250) lr 7.6655e-04 eta 0:00:27
epoch [117/200] batch [10/61] time 0.572 (0.484) data 0.440 (0.352) loss_u loss_u 0.9087 (0.8598) acc_u 6.2500 (17.1875) lr 7.6655e-04 eta 0:00:24
epoch [117/200] batch [15/61] time 0.618 (0.481) data 0.485 (0.349) loss_u loss_u 0.8169 (0.8521) acc_u 21.8750 (18.3333) lr 7.6655e-04 eta 0:00:22
epoch [117/200] batch [20/61] time 0.476 (0.477) data 0.345 (0.345) loss_u loss_u 0.8735 (0.8588) acc_u 21.8750 (17.9688) lr 7.6655e-04 eta 0:00:19
epoch [117/200] batch [25/61] time 0.330 (0.471) data 0.198 (0.339) loss_u loss_u 0.8364 (0.8570) acc_u 18.7500 (18.2500) lr 7.6655e-04 eta 0:00:16
epoch [117/200] batch [30/61] time 0.404 (0.469) data 0.272 (0.338) loss_u loss_u 0.8325 (0.8590) acc_u 25.0000 (17.9167) lr 7.6655e-04 eta 0:00:14
epoch [117/200] batch [35/61] time 0.433 (0.466) data 0.302 (0.334) loss_u loss_u 0.8252 (0.8565) acc_u 21.8750 (18.3036) lr 7.6655e-04 eta 0:00:12
epoch [117/200] batch [40/61] time 0.481 (0.466) data 0.350 (0.334) loss_u loss_u 0.9424 (0.8623) acc_u 6.2500 (17.8906) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [45/61] time 0.387 (0.462) data 0.255 (0.330) loss_u loss_u 0.8345 (0.8651) acc_u 25.0000 (17.5694) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [50/61] time 0.402 (0.461) data 0.271 (0.329) loss_u loss_u 0.9131 (0.8683) acc_u 9.3750 (17.0625) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [55/61] time 0.439 (0.462) data 0.307 (0.330) loss_u loss_u 0.8428 (0.8697) acc_u 18.7500 (16.8750) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [60/61] time 0.557 (0.463) data 0.424 (0.332) loss_u loss_u 0.7915 (0.8662) acc_u 31.2500 (17.3438) lr 7.6655e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1372
confident_label rate tensor(0.3817, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1197
clean true:1193
clean false:4
clean_rate:0.9966583124477861
noisy true:571
noisy false:1368
after delete: len(clean_dataset) 1197
after delete: len(noisy_dataset) 1939
epoch [118/200] batch [5/37] time 0.455 (0.435) data 0.324 (0.304) loss_x loss_x 1.3965 (1.1609) acc_x 71.8750 (75.6250) lr 7.5131e-04 eta 0:00:13
epoch [118/200] batch [10/37] time 0.553 (0.448) data 0.422 (0.317) loss_x loss_x 1.0518 (1.0943) acc_x 75.0000 (74.6875) lr 7.5131e-04 eta 0:00:12
epoch [118/200] batch [15/37] time 0.575 (0.466) data 0.444 (0.335) loss_x loss_x 1.0391 (1.1524) acc_x 71.8750 (73.7500) lr 7.5131e-04 eta 0:00:10
epoch [118/200] batch [20/37] time 0.379 (0.473) data 0.247 (0.342) loss_x loss_x 1.2168 (1.1215) acc_x 78.1250 (73.9062) lr 7.5131e-04 eta 0:00:08
epoch [118/200] batch [25/37] time 0.404 (0.467) data 0.273 (0.336) loss_x loss_x 0.7090 (1.1208) acc_x 84.3750 (72.8750) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [30/37] time 0.458 (0.455) data 0.327 (0.324) loss_x loss_x 0.6772 (1.0919) acc_x 87.5000 (73.2292) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [35/37] time 0.402 (0.459) data 0.271 (0.328) loss_x loss_x 1.8164 (1.1540) acc_x 59.3750 (71.6071) lr 7.5131e-04 eta 0:00:00
epoch [118/200] batch [5/60] time 0.454 (0.453) data 0.322 (0.321) loss_u loss_u 0.8364 (0.8718) acc_u 25.0000 (15.6250) lr 7.5131e-04 eta 0:00:24
epoch [118/200] batch [10/60] time 0.632 (0.456) data 0.497 (0.325) loss_u loss_u 0.8853 (0.8685) acc_u 18.7500 (16.8750) lr 7.5131e-04 eta 0:00:22
epoch [118/200] batch [15/60] time 0.427 (0.455) data 0.295 (0.324) loss_u loss_u 0.8750 (0.8724) acc_u 12.5000 (15.6250) lr 7.5131e-04 eta 0:00:20
epoch [118/200] batch [20/60] time 0.445 (0.455) data 0.314 (0.324) loss_u loss_u 0.8638 (0.8731) acc_u 18.7500 (15.9375) lr 7.5131e-04 eta 0:00:18
epoch [118/200] batch [25/60] time 0.406 (0.448) data 0.274 (0.317) loss_u loss_u 0.8931 (0.8706) acc_u 9.3750 (16.0000) lr 7.5131e-04 eta 0:00:15
epoch [118/200] batch [30/60] time 0.517 (0.450) data 0.382 (0.319) loss_u loss_u 0.9385 (0.8739) acc_u 9.3750 (15.6250) lr 7.5131e-04 eta 0:00:13
epoch [118/200] batch [35/60] time 0.368 (0.447) data 0.237 (0.315) loss_u loss_u 0.9121 (0.8754) acc_u 15.6250 (15.5357) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [40/60] time 0.409 (0.446) data 0.277 (0.314) loss_u loss_u 0.9131 (0.8738) acc_u 15.6250 (15.8594) lr 7.5131e-04 eta 0:00:08
epoch [118/200] batch [45/60] time 0.560 (0.444) data 0.428 (0.313) loss_u loss_u 0.8950 (0.8754) acc_u 15.6250 (15.6250) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [50/60] time 0.387 (0.443) data 0.255 (0.312) loss_u loss_u 0.9102 (0.8756) acc_u 9.3750 (15.5625) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [55/60] time 0.454 (0.445) data 0.323 (0.313) loss_u loss_u 0.8945 (0.8755) acc_u 6.2500 (15.5682) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [60/60] time 0.532 (0.443) data 0.401 (0.311) loss_u loss_u 0.8120 (0.8760) acc_u 25.0000 (15.5729) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1400
confident_label rate tensor(0.3804, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1193
clean true:1188
clean false:5
clean_rate:0.9958088851634534
noisy true:548
noisy false:1395
after delete: len(clean_dataset) 1193
after delete: len(noisy_dataset) 1943
epoch [119/200] batch [5/37] time 0.389 (0.438) data 0.259 (0.307) loss_x loss_x 0.8003 (1.0159) acc_x 84.3750 (76.8750) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [10/37] time 0.412 (0.470) data 0.281 (0.339) loss_x loss_x 1.0967 (1.0068) acc_x 78.1250 (76.5625) lr 7.3613e-04 eta 0:00:12
epoch [119/200] batch [15/37] time 0.513 (0.464) data 0.383 (0.333) loss_x loss_x 0.8809 (1.0093) acc_x 75.0000 (76.0417) lr 7.3613e-04 eta 0:00:10
epoch [119/200] batch [20/37] time 0.484 (0.468) data 0.350 (0.337) loss_x loss_x 0.7256 (1.0131) acc_x 81.2500 (75.6250) lr 7.3613e-04 eta 0:00:07
epoch [119/200] batch [25/37] time 0.520 (0.481) data 0.389 (0.350) loss_x loss_x 0.9966 (1.0304) acc_x 71.8750 (74.8750) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [30/37] time 0.533 (0.474) data 0.402 (0.344) loss_x loss_x 1.2842 (1.0572) acc_x 68.7500 (73.9583) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [35/37] time 0.490 (0.467) data 0.359 (0.336) loss_x loss_x 1.2227 (1.0668) acc_x 71.8750 (73.8393) lr 7.3613e-04 eta 0:00:00
epoch [119/200] batch [5/60] time 0.423 (0.468) data 0.291 (0.338) loss_u loss_u 0.8877 (0.9088) acc_u 15.6250 (10.6250) lr 7.3613e-04 eta 0:00:25
epoch [119/200] batch [10/60] time 0.389 (0.458) data 0.257 (0.327) loss_u loss_u 0.8755 (0.8909) acc_u 12.5000 (14.0625) lr 7.3613e-04 eta 0:00:22
epoch [119/200] batch [15/60] time 0.354 (0.454) data 0.223 (0.323) loss_u loss_u 0.9307 (0.8823) acc_u 9.3750 (14.7917) lr 7.3613e-04 eta 0:00:20
epoch [119/200] batch [20/60] time 0.347 (0.447) data 0.214 (0.315) loss_u loss_u 0.8760 (0.8895) acc_u 25.0000 (14.3750) lr 7.3613e-04 eta 0:00:17
epoch [119/200] batch [25/60] time 0.473 (0.450) data 0.342 (0.319) loss_u loss_u 0.8765 (0.8838) acc_u 15.6250 (15.1250) lr 7.3613e-04 eta 0:00:15
epoch [119/200] batch [30/60] time 0.403 (0.447) data 0.271 (0.316) loss_u loss_u 0.8804 (0.8786) acc_u 15.6250 (15.6250) lr 7.3613e-04 eta 0:00:13
epoch [119/200] batch [35/60] time 0.471 (0.450) data 0.339 (0.319) loss_u loss_u 0.8706 (0.8774) acc_u 18.7500 (15.8929) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [40/60] time 0.465 (0.457) data 0.335 (0.326) loss_u loss_u 0.8428 (0.8770) acc_u 18.7500 (15.7031) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [45/60] time 0.379 (0.452) data 0.248 (0.321) loss_u loss_u 0.8779 (0.8748) acc_u 18.7500 (16.3194) lr 7.3613e-04 eta 0:00:06
epoch [119/200] batch [50/60] time 0.369 (0.451) data 0.237 (0.320) loss_u loss_u 0.9165 (0.8735) acc_u 6.2500 (16.3750) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [55/60] time 0.451 (0.450) data 0.321 (0.319) loss_u loss_u 0.8667 (0.8732) acc_u 18.7500 (16.5341) lr 7.3613e-04 eta 0:00:02
epoch [119/200] batch [60/60] time 0.400 (0.450) data 0.267 (0.319) loss_u loss_u 0.8857 (0.8741) acc_u 12.5000 (16.3542) lr 7.3613e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1387
confident_label rate tensor(0.3833, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1202
clean true:1193
clean false:9
clean_rate:0.9925124792013311
noisy true:556
noisy false:1378
after delete: len(clean_dataset) 1202
after delete: len(noisy_dataset) 1934
epoch [120/200] batch [5/37] time 0.438 (0.442) data 0.307 (0.311) loss_x loss_x 1.1934 (1.2850) acc_x 65.6250 (65.6250) lr 7.2101e-04 eta 0:00:14
epoch [120/200] batch [10/37] time 0.382 (0.435) data 0.251 (0.305) loss_x loss_x 1.9961 (1.2672) acc_x 62.5000 (67.8125) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [15/37] time 0.520 (0.445) data 0.389 (0.315) loss_x loss_x 1.0742 (1.2355) acc_x 75.0000 (68.5417) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [20/37] time 0.415 (0.454) data 0.285 (0.324) loss_x loss_x 0.7451 (1.2097) acc_x 81.2500 (69.2188) lr 7.2101e-04 eta 0:00:07
epoch [120/200] batch [25/37] time 0.493 (0.453) data 0.363 (0.323) loss_x loss_x 1.1855 (1.1787) acc_x 71.8750 (69.5000) lr 7.2101e-04 eta 0:00:05
epoch [120/200] batch [30/37] time 0.524 (0.451) data 0.394 (0.321) loss_x loss_x 1.1689 (1.2035) acc_x 75.0000 (69.4792) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [35/37] time 0.450 (0.452) data 0.319 (0.321) loss_x loss_x 0.8945 (1.1798) acc_x 87.5000 (70.6250) lr 7.2101e-04 eta 0:00:00
epoch [120/200] batch [5/60] time 0.452 (0.450) data 0.320 (0.319) loss_u loss_u 0.8745 (0.8570) acc_u 15.6250 (21.2500) lr 7.2101e-04 eta 0:00:24
epoch [120/200] batch [10/60] time 0.438 (0.448) data 0.307 (0.317) loss_u loss_u 0.8726 (0.8628) acc_u 15.6250 (18.7500) lr 7.2101e-04 eta 0:00:22
epoch [120/200] batch [15/60] time 0.577 (0.449) data 0.445 (0.318) loss_u loss_u 0.9282 (0.8702) acc_u 9.3750 (16.8750) lr 7.2101e-04 eta 0:00:20
epoch [120/200] batch [20/60] time 0.528 (0.455) data 0.395 (0.324) loss_u loss_u 0.8882 (0.8701) acc_u 15.6250 (16.7188) lr 7.2101e-04 eta 0:00:18
epoch [120/200] batch [25/60] time 0.354 (0.453) data 0.224 (0.322) loss_u loss_u 0.8687 (0.8700) acc_u 18.7500 (17.0000) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [30/60] time 0.453 (0.454) data 0.321 (0.323) loss_u loss_u 0.9287 (0.8708) acc_u 6.2500 (16.5625) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [35/60] time 0.613 (0.454) data 0.481 (0.323) loss_u loss_u 0.8711 (0.8736) acc_u 12.5000 (15.9821) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [40/60] time 0.435 (0.450) data 0.304 (0.318) loss_u loss_u 0.8789 (0.8697) acc_u 15.6250 (16.5625) lr 7.2101e-04 eta 0:00:08
epoch [120/200] batch [45/60] time 0.360 (0.447) data 0.228 (0.316) loss_u loss_u 0.8350 (0.8677) acc_u 18.7500 (16.8750) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [50/60] time 0.449 (0.444) data 0.318 (0.313) loss_u loss_u 0.8975 (0.8703) acc_u 9.3750 (16.3125) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [55/60] time 0.396 (0.443) data 0.265 (0.312) loss_u loss_u 0.8804 (0.8732) acc_u 15.6250 (16.0795) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [60/60] time 0.513 (0.443) data 0.382 (0.312) loss_u loss_u 0.8760 (0.8746) acc_u 15.6250 (15.8333) lr 7.2101e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1481
confident_label rate tensor(0.3591, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1126
clean true:1117
clean false:9
clean_rate:0.9920071047957372
noisy true:538
noisy false:1472
after delete: len(clean_dataset) 1126
after delete: len(noisy_dataset) 2010
epoch [121/200] batch [5/35] time 0.415 (0.448) data 0.284 (0.317) loss_x loss_x 1.2988 (1.0688) acc_x 65.6250 (73.1250) lr 7.0596e-04 eta 0:00:13
epoch [121/200] batch [10/35] time 0.375 (0.434) data 0.245 (0.304) loss_x loss_x 1.2295 (1.0833) acc_x 65.6250 (74.3750) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [15/35] time 0.353 (0.427) data 0.223 (0.296) loss_x loss_x 1.4268 (1.0628) acc_x 75.0000 (76.0417) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [20/35] time 0.404 (0.432) data 0.274 (0.301) loss_x loss_x 0.8784 (1.0493) acc_x 78.1250 (75.4688) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [25/35] time 0.640 (0.436) data 0.509 (0.306) loss_x loss_x 1.5635 (1.1056) acc_x 65.6250 (74.7500) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [30/35] time 0.500 (0.447) data 0.369 (0.316) loss_x loss_x 1.3477 (1.1143) acc_x 62.5000 (74.1667) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [35/35] time 0.512 (0.455) data 0.381 (0.324) loss_x loss_x 1.0645 (1.1376) acc_x 75.0000 (73.8393) lr 7.0596e-04 eta 0:00:00
epoch [121/200] batch [5/62] time 0.436 (0.454) data 0.305 (0.323) loss_u loss_u 0.9355 (0.8965) acc_u 3.1250 (11.8750) lr 7.0596e-04 eta 0:00:25
epoch [121/200] batch [10/62] time 0.516 (0.458) data 0.385 (0.326) loss_u loss_u 0.8838 (0.8877) acc_u 18.7500 (13.7500) lr 7.0596e-04 eta 0:00:23
epoch [121/200] batch [15/62] time 0.353 (0.454) data 0.222 (0.323) loss_u loss_u 0.9561 (0.8850) acc_u 6.2500 (15.8333) lr 7.0596e-04 eta 0:00:21
epoch [121/200] batch [20/62] time 0.517 (0.458) data 0.385 (0.327) loss_u loss_u 0.8691 (0.8834) acc_u 18.7500 (15.7812) lr 7.0596e-04 eta 0:00:19
epoch [121/200] batch [25/62] time 0.353 (0.454) data 0.221 (0.323) loss_u loss_u 0.8564 (0.8797) acc_u 21.8750 (16.5000) lr 7.0596e-04 eta 0:00:16
epoch [121/200] batch [30/62] time 0.555 (0.449) data 0.423 (0.318) loss_u loss_u 0.7280 (0.8709) acc_u 31.2500 (17.0833) lr 7.0596e-04 eta 0:00:14
epoch [121/200] batch [35/62] time 0.373 (0.447) data 0.242 (0.315) loss_u loss_u 0.9028 (0.8739) acc_u 12.5000 (16.6964) lr 7.0596e-04 eta 0:00:12
epoch [121/200] batch [40/62] time 0.393 (0.445) data 0.262 (0.313) loss_u loss_u 0.8594 (0.8743) acc_u 15.6250 (16.4062) lr 7.0596e-04 eta 0:00:09
epoch [121/200] batch [45/62] time 0.470 (0.447) data 0.339 (0.316) loss_u loss_u 0.8545 (0.8743) acc_u 15.6250 (16.3194) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [50/62] time 0.448 (0.445) data 0.316 (0.313) loss_u loss_u 0.8955 (0.8732) acc_u 9.3750 (16.5000) lr 7.0596e-04 eta 0:00:05
epoch [121/200] batch [55/62] time 0.329 (0.446) data 0.197 (0.315) loss_u loss_u 0.9067 (0.8730) acc_u 12.5000 (16.7045) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [60/62] time 0.380 (0.446) data 0.248 (0.315) loss_u loss_u 0.7637 (0.8686) acc_u 25.0000 (16.9792) lr 7.0596e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1424
confident_label rate tensor(0.3760, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1179
clean true:1175
clean false:4
clean_rate:0.996607294317218
noisy true:537
noisy false:1420
after delete: len(clean_dataset) 1179
after delete: len(noisy_dataset) 1957
epoch [122/200] batch [5/36] time 0.636 (0.479) data 0.506 (0.348) loss_x loss_x 1.2061 (1.1723) acc_x 68.7500 (70.0000) lr 6.9098e-04 eta 0:00:14
epoch [122/200] batch [10/36] time 0.472 (0.458) data 0.340 (0.326) loss_x loss_x 0.8950 (1.0880) acc_x 68.7500 (69.6875) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [15/36] time 0.434 (0.456) data 0.304 (0.325) loss_x loss_x 1.0898 (1.1019) acc_x 71.8750 (71.0417) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [20/36] time 0.507 (0.460) data 0.376 (0.329) loss_x loss_x 0.7178 (1.0890) acc_x 75.0000 (70.6250) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [25/36] time 0.590 (0.453) data 0.459 (0.322) loss_x loss_x 1.2900 (1.1290) acc_x 75.0000 (70.5000) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [30/36] time 0.466 (0.445) data 0.335 (0.314) loss_x loss_x 0.8901 (1.1442) acc_x 81.2500 (70.6250) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [35/36] time 0.432 (0.445) data 0.302 (0.314) loss_x loss_x 0.7358 (1.1208) acc_x 81.2500 (71.6071) lr 6.9098e-04 eta 0:00:00
epoch [122/200] batch [5/61] time 0.660 (0.447) data 0.528 (0.316) loss_u loss_u 0.9116 (0.8721) acc_u 12.5000 (16.8750) lr 6.9098e-04 eta 0:00:25
epoch [122/200] batch [10/61] time 0.414 (0.444) data 0.282 (0.313) loss_u loss_u 0.8574 (0.8761) acc_u 21.8750 (16.8750) lr 6.9098e-04 eta 0:00:22
epoch [122/200] batch [15/61] time 0.477 (0.438) data 0.346 (0.307) loss_u loss_u 0.9019 (0.8826) acc_u 12.5000 (15.8333) lr 6.9098e-04 eta 0:00:20
epoch [122/200] batch [20/61] time 0.506 (0.443) data 0.375 (0.312) loss_u loss_u 0.9053 (0.8772) acc_u 15.6250 (16.4062) lr 6.9098e-04 eta 0:00:18
epoch [122/200] batch [25/61] time 0.440 (0.439) data 0.308 (0.308) loss_u loss_u 0.7749 (0.8726) acc_u 28.1250 (16.7500) lr 6.9098e-04 eta 0:00:15
epoch [122/200] batch [30/61] time 0.434 (0.437) data 0.304 (0.306) loss_u loss_u 0.8149 (0.8722) acc_u 25.0000 (16.6667) lr 6.9098e-04 eta 0:00:13
epoch [122/200] batch [35/61] time 0.525 (0.442) data 0.395 (0.311) loss_u loss_u 0.8647 (0.8725) acc_u 15.6250 (16.6071) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [40/61] time 0.556 (0.443) data 0.424 (0.312) loss_u loss_u 0.8018 (0.8708) acc_u 21.8750 (16.5625) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [45/61] time 0.331 (0.443) data 0.200 (0.312) loss_u loss_u 0.9429 (0.8749) acc_u 6.2500 (15.8333) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [50/61] time 0.418 (0.445) data 0.287 (0.314) loss_u loss_u 0.8672 (0.8762) acc_u 18.7500 (15.8125) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [55/61] time 0.429 (0.446) data 0.298 (0.315) loss_u loss_u 0.8809 (0.8746) acc_u 15.6250 (15.9091) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [60/61] time 0.457 (0.447) data 0.325 (0.316) loss_u loss_u 0.8760 (0.8765) acc_u 18.7500 (15.6250) lr 6.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1394
confident_label rate tensor(0.3753, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1177
clean true:1171
clean false:6
clean_rate:0.9949022939677146
noisy true:571
noisy false:1388
after delete: len(clean_dataset) 1177
after delete: len(noisy_dataset) 1959
epoch [123/200] batch [5/36] time 0.488 (0.451) data 0.355 (0.319) loss_x loss_x 1.0742 (1.2187) acc_x 78.1250 (71.2500) lr 6.7608e-04 eta 0:00:13
epoch [123/200] batch [10/36] time 0.428 (0.435) data 0.297 (0.304) loss_x loss_x 1.4473 (1.1696) acc_x 59.3750 (69.3750) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [15/36] time 0.506 (0.433) data 0.376 (0.302) loss_x loss_x 1.4229 (1.1830) acc_x 71.8750 (69.7917) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [20/36] time 0.524 (0.452) data 0.394 (0.321) loss_x loss_x 1.6162 (1.1691) acc_x 65.6250 (70.1562) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [25/36] time 0.428 (0.455) data 0.297 (0.324) loss_x loss_x 0.6147 (1.1394) acc_x 81.2500 (70.2500) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [30/36] time 0.488 (0.450) data 0.357 (0.319) loss_x loss_x 1.2031 (1.1346) acc_x 78.1250 (70.6250) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [35/36] time 0.461 (0.459) data 0.330 (0.328) loss_x loss_x 1.2207 (1.1113) acc_x 75.0000 (71.8750) lr 6.7608e-04 eta 0:00:00
epoch [123/200] batch [5/61] time 0.386 (0.458) data 0.254 (0.327) loss_u loss_u 0.8657 (0.8864) acc_u 18.7500 (15.0000) lr 6.7608e-04 eta 0:00:25
epoch [123/200] batch [10/61] time 0.565 (0.458) data 0.433 (0.327) loss_u loss_u 0.9512 (0.8907) acc_u 6.2500 (13.4375) lr 6.7608e-04 eta 0:00:23
epoch [123/200] batch [15/61] time 0.446 (0.458) data 0.315 (0.327) loss_u loss_u 0.8643 (0.8797) acc_u 15.6250 (14.5833) lr 6.7608e-04 eta 0:00:21
epoch [123/200] batch [20/61] time 0.451 (0.453) data 0.319 (0.322) loss_u loss_u 0.8892 (0.8800) acc_u 15.6250 (15.0000) lr 6.7608e-04 eta 0:00:18
epoch [123/200] batch [25/61] time 0.405 (0.452) data 0.273 (0.321) loss_u loss_u 0.8823 (0.8791) acc_u 12.5000 (14.8750) lr 6.7608e-04 eta 0:00:16
epoch [123/200] batch [30/61] time 0.444 (0.452) data 0.313 (0.321) loss_u loss_u 0.9180 (0.8745) acc_u 9.3750 (15.9375) lr 6.7608e-04 eta 0:00:14
epoch [123/200] batch [35/61] time 0.507 (0.454) data 0.376 (0.323) loss_u loss_u 0.8364 (0.8703) acc_u 18.7500 (16.2500) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [40/61] time 0.402 (0.454) data 0.271 (0.323) loss_u loss_u 0.9009 (0.8697) acc_u 12.5000 (16.6406) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [45/61] time 0.514 (0.452) data 0.382 (0.321) loss_u loss_u 0.8467 (0.8719) acc_u 18.7500 (16.4583) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [50/61] time 0.473 (0.451) data 0.341 (0.320) loss_u loss_u 0.8789 (0.8721) acc_u 18.7500 (16.1875) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [55/61] time 0.439 (0.447) data 0.308 (0.316) loss_u loss_u 0.8916 (0.8719) acc_u 12.5000 (16.1364) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [60/61] time 0.406 (0.444) data 0.276 (0.313) loss_u loss_u 0.8506 (0.8705) acc_u 21.8750 (16.3542) lr 6.7608e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1417
confident_label rate tensor(0.3705, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1162
clean true:1157
clean false:5
clean_rate:0.995697074010327
noisy true:562
noisy false:1412
after delete: len(clean_dataset) 1162
after delete: len(noisy_dataset) 1974
epoch [124/200] batch [5/36] time 0.371 (0.433) data 0.241 (0.302) loss_x loss_x 1.1143 (1.5396) acc_x 65.6250 (63.1250) lr 6.6126e-04 eta 0:00:13
epoch [124/200] batch [10/36] time 0.515 (0.458) data 0.384 (0.328) loss_x loss_x 0.9814 (1.3174) acc_x 75.0000 (66.2500) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [15/36] time 0.351 (0.445) data 0.220 (0.314) loss_x loss_x 1.0957 (1.2181) acc_x 71.8750 (68.9583) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [20/36] time 0.485 (0.453) data 0.354 (0.322) loss_x loss_x 1.9131 (1.2310) acc_x 56.2500 (68.9062) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [25/36] time 0.463 (0.446) data 0.332 (0.315) loss_x loss_x 1.2227 (1.1863) acc_x 65.6250 (70.5000) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [30/36] time 0.447 (0.458) data 0.316 (0.327) loss_x loss_x 0.9194 (1.1823) acc_x 71.8750 (70.7292) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [35/36] time 0.470 (0.462) data 0.339 (0.331) loss_x loss_x 0.9712 (1.1974) acc_x 68.7500 (70.3571) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/61] time 0.334 (0.455) data 0.202 (0.324) loss_u loss_u 0.8691 (0.8631) acc_u 21.8750 (18.1250) lr 6.6126e-04 eta 0:00:25
epoch [124/200] batch [10/61] time 0.375 (0.454) data 0.243 (0.322) loss_u loss_u 0.8848 (0.8643) acc_u 15.6250 (17.8125) lr 6.6126e-04 eta 0:00:23
epoch [124/200] batch [15/61] time 0.508 (0.454) data 0.376 (0.323) loss_u loss_u 0.8755 (0.8644) acc_u 18.7500 (18.1250) lr 6.6126e-04 eta 0:00:20
epoch [124/200] batch [20/61] time 0.363 (0.454) data 0.232 (0.323) loss_u loss_u 0.8599 (0.8715) acc_u 15.6250 (16.4062) lr 6.6126e-04 eta 0:00:18
epoch [124/200] batch [25/61] time 0.491 (0.451) data 0.359 (0.319) loss_u loss_u 0.7886 (0.8687) acc_u 25.0000 (16.5000) lr 6.6126e-04 eta 0:00:16
epoch [124/200] batch [30/61] time 0.357 (0.447) data 0.225 (0.315) loss_u loss_u 0.9048 (0.8719) acc_u 12.5000 (16.3542) lr 6.6126e-04 eta 0:00:13
epoch [124/200] batch [35/61] time 0.570 (0.451) data 0.437 (0.319) loss_u loss_u 0.9116 (0.8712) acc_u 12.5000 (16.5179) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [40/61] time 0.524 (0.454) data 0.392 (0.323) loss_u loss_u 0.8501 (0.8696) acc_u 21.8750 (17.1094) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [45/61] time 0.476 (0.454) data 0.343 (0.323) loss_u loss_u 0.6968 (0.8648) acc_u 43.7500 (17.8472) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [50/61] time 0.384 (0.456) data 0.252 (0.324) loss_u loss_u 0.9019 (0.8632) acc_u 9.3750 (18.0625) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [55/61] time 0.424 (0.455) data 0.292 (0.324) loss_u loss_u 0.8408 (0.8634) acc_u 15.6250 (17.8409) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [60/61] time 0.511 (0.454) data 0.380 (0.322) loss_u loss_u 0.8145 (0.8640) acc_u 28.1250 (17.5521) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1461
confident_label rate tensor(0.3642, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1142
clean true:1139
clean false:3
clean_rate:0.9973730297723292
noisy true:536
noisy false:1458
after delete: len(clean_dataset) 1142
after delete: len(noisy_dataset) 1994
epoch [125/200] batch [5/35] time 0.473 (0.418) data 0.342 (0.287) loss_x loss_x 1.7002 (1.3139) acc_x 59.3750 (67.5000) lr 6.4653e-04 eta 0:00:12
epoch [125/200] batch [10/35] time 0.631 (0.469) data 0.500 (0.338) loss_x loss_x 1.2061 (1.2032) acc_x 65.6250 (69.0625) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [15/35] time 0.364 (0.467) data 0.234 (0.336) loss_x loss_x 1.5127 (1.2079) acc_x 59.3750 (70.2083) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [20/35] time 0.624 (0.481) data 0.493 (0.350) loss_x loss_x 0.6470 (1.1374) acc_x 81.2500 (72.1875) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [25/35] time 0.409 (0.472) data 0.278 (0.341) loss_x loss_x 1.3174 (1.1697) acc_x 68.7500 (71.8750) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [30/35] time 0.469 (0.466) data 0.338 (0.335) loss_x loss_x 1.6064 (1.1656) acc_x 50.0000 (70.9375) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [35/35] time 0.505 (0.465) data 0.374 (0.335) loss_x loss_x 1.1719 (1.1593) acc_x 68.7500 (70.8036) lr 6.4653e-04 eta 0:00:00
epoch [125/200] batch [5/62] time 0.569 (0.476) data 0.438 (0.345) loss_u loss_u 0.8027 (0.8663) acc_u 25.0000 (15.6250) lr 6.4653e-04 eta 0:00:27
epoch [125/200] batch [10/62] time 0.384 (0.476) data 0.252 (0.345) loss_u loss_u 0.7783 (0.8484) acc_u 31.2500 (18.1250) lr 6.4653e-04 eta 0:00:24
epoch [125/200] batch [15/62] time 0.546 (0.469) data 0.414 (0.338) loss_u loss_u 0.8726 (0.8566) acc_u 18.7500 (17.2917) lr 6.4653e-04 eta 0:00:22
epoch [125/200] batch [20/62] time 0.426 (0.462) data 0.294 (0.331) loss_u loss_u 0.8823 (0.8641) acc_u 15.6250 (16.2500) lr 6.4653e-04 eta 0:00:19
epoch [125/200] batch [25/62] time 0.480 (0.462) data 0.349 (0.331) loss_u loss_u 0.8887 (0.8577) acc_u 15.6250 (17.5000) lr 6.4653e-04 eta 0:00:17
epoch [125/200] batch [30/62] time 0.370 (0.457) data 0.238 (0.326) loss_u loss_u 0.8848 (0.8565) acc_u 15.6250 (17.6042) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [35/62] time 0.506 (0.456) data 0.373 (0.325) loss_u loss_u 0.8838 (0.8565) acc_u 15.6250 (17.7679) lr 6.4653e-04 eta 0:00:12
epoch [125/200] batch [40/62] time 0.388 (0.453) data 0.256 (0.322) loss_u loss_u 0.7485 (0.8546) acc_u 28.1250 (18.0469) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [45/62] time 0.504 (0.451) data 0.372 (0.319) loss_u loss_u 0.9390 (0.8589) acc_u 6.2500 (17.3611) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [50/62] time 0.421 (0.452) data 0.289 (0.320) loss_u loss_u 0.8545 (0.8632) acc_u 18.7500 (16.9375) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [55/62] time 0.433 (0.449) data 0.301 (0.318) loss_u loss_u 0.8818 (0.8631) acc_u 15.6250 (17.1023) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [60/62] time 0.467 (0.447) data 0.336 (0.316) loss_u loss_u 0.9175 (0.8654) acc_u 15.6250 (17.0312) lr 6.4653e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1410
confident_label rate tensor(0.3750, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1176
clean true:1171
clean false:5
clean_rate:0.9957482993197279
noisy true:555
noisy false:1405
after delete: len(clean_dataset) 1176
after delete: len(noisy_dataset) 1960
epoch [126/200] batch [5/36] time 0.395 (0.419) data 0.264 (0.288) loss_x loss_x 0.8979 (1.1546) acc_x 71.8750 (67.5000) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [10/36] time 0.562 (0.470) data 0.431 (0.339) loss_x loss_x 1.2061 (1.0840) acc_x 68.7500 (70.9375) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [15/36] time 0.367 (0.470) data 0.236 (0.339) loss_x loss_x 1.6279 (1.1477) acc_x 56.2500 (71.0417) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [20/36] time 0.411 (0.470) data 0.281 (0.340) loss_x loss_x 0.6919 (1.0832) acc_x 81.2500 (72.8125) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [25/36] time 0.477 (0.463) data 0.347 (0.332) loss_x loss_x 1.1543 (1.0919) acc_x 65.6250 (72.0000) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [30/36] time 0.463 (0.456) data 0.332 (0.325) loss_x loss_x 0.9966 (1.1007) acc_x 81.2500 (71.9792) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [35/36] time 0.414 (0.452) data 0.283 (0.321) loss_x loss_x 0.9136 (1.0985) acc_x 78.1250 (72.5000) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/61] time 0.419 (0.446) data 0.287 (0.316) loss_u loss_u 0.8994 (0.8810) acc_u 21.8750 (16.2500) lr 6.3188e-04 eta 0:00:25
epoch [126/200] batch [10/61] time 0.452 (0.440) data 0.321 (0.309) loss_u loss_u 0.8818 (0.8719) acc_u 18.7500 (17.5000) lr 6.3188e-04 eta 0:00:22
epoch [126/200] batch [15/61] time 0.374 (0.441) data 0.242 (0.310) loss_u loss_u 0.8452 (0.8776) acc_u 18.7500 (15.8333) lr 6.3188e-04 eta 0:00:20
epoch [126/200] batch [20/61] time 0.454 (0.447) data 0.322 (0.316) loss_u loss_u 0.8457 (0.8725) acc_u 21.8750 (16.4062) lr 6.3188e-04 eta 0:00:18
epoch [126/200] batch [25/61] time 0.372 (0.442) data 0.241 (0.311) loss_u loss_u 0.8550 (0.8665) acc_u 15.6250 (17.0000) lr 6.3188e-04 eta 0:00:15
epoch [126/200] batch [30/61] time 0.315 (0.442) data 0.183 (0.311) loss_u loss_u 0.8740 (0.8682) acc_u 15.6250 (16.8750) lr 6.3188e-04 eta 0:00:13
epoch [126/200] batch [35/61] time 0.413 (0.440) data 0.281 (0.309) loss_u loss_u 0.8286 (0.8695) acc_u 21.8750 (16.8750) lr 6.3188e-04 eta 0:00:11
epoch [126/200] batch [40/61] time 0.468 (0.440) data 0.336 (0.309) loss_u loss_u 0.8652 (0.8677) acc_u 15.6250 (16.9531) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [45/61] time 0.383 (0.440) data 0.251 (0.309) loss_u loss_u 0.9497 (0.8666) acc_u 9.3750 (17.1528) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [50/61] time 0.395 (0.439) data 0.263 (0.307) loss_u loss_u 0.8462 (0.8678) acc_u 21.8750 (17.0625) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [55/61] time 0.446 (0.439) data 0.314 (0.308) loss_u loss_u 0.8276 (0.8648) acc_u 25.0000 (17.5568) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [60/61] time 0.561 (0.438) data 0.430 (0.307) loss_u loss_u 0.8965 (0.8662) acc_u 9.3750 (17.3438) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1422
confident_label rate tensor(0.3747, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1175
clean true:1171
clean false:4
clean_rate:0.9965957446808511
noisy true:543
noisy false:1418
after delete: len(clean_dataset) 1175
after delete: len(noisy_dataset) 1961
epoch [127/200] batch [5/36] time 0.462 (0.442) data 0.331 (0.311) loss_x loss_x 0.8008 (1.1216) acc_x 84.3750 (73.7500) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [10/36] time 0.489 (0.487) data 0.358 (0.357) loss_x loss_x 0.9897 (1.0336) acc_x 71.8750 (74.6875) lr 6.1732e-04 eta 0:00:12
epoch [127/200] batch [15/36] time 0.652 (0.496) data 0.519 (0.365) loss_x loss_x 0.8271 (1.0054) acc_x 84.3750 (74.5833) lr 6.1732e-04 eta 0:00:10
epoch [127/200] batch [20/36] time 0.547 (0.493) data 0.416 (0.362) loss_x loss_x 0.5396 (0.9864) acc_x 87.5000 (76.5625) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [25/36] time 0.533 (0.488) data 0.402 (0.357) loss_x loss_x 0.8228 (0.9456) acc_x 84.3750 (77.6250) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [30/36] time 0.346 (0.479) data 0.215 (0.348) loss_x loss_x 1.5645 (0.9771) acc_x 56.2500 (76.4583) lr 6.1732e-04 eta 0:00:02
epoch [127/200] batch [35/36] time 0.345 (0.468) data 0.214 (0.337) loss_x loss_x 1.7441 (0.9888) acc_x 62.5000 (76.2500) lr 6.1732e-04 eta 0:00:00
epoch [127/200] batch [5/61] time 0.481 (0.464) data 0.349 (0.333) loss_u loss_u 0.9341 (0.8350) acc_u 9.3750 (21.8750) lr 6.1732e-04 eta 0:00:25
epoch [127/200] batch [10/61] time 0.461 (0.462) data 0.330 (0.330) loss_u loss_u 0.9180 (0.8549) acc_u 12.5000 (18.7500) lr 6.1732e-04 eta 0:00:23
epoch [127/200] batch [15/61] time 0.327 (0.461) data 0.195 (0.330) loss_u loss_u 0.8989 (0.8759) acc_u 12.5000 (15.6250) lr 6.1732e-04 eta 0:00:21
epoch [127/200] batch [20/61] time 0.387 (0.459) data 0.255 (0.327) loss_u loss_u 0.9219 (0.8753) acc_u 6.2500 (15.4688) lr 6.1732e-04 eta 0:00:18
epoch [127/200] batch [25/61] time 0.386 (0.455) data 0.255 (0.323) loss_u loss_u 0.8940 (0.8755) acc_u 15.6250 (16.2500) lr 6.1732e-04 eta 0:00:16
epoch [127/200] batch [30/61] time 0.322 (0.458) data 0.190 (0.327) loss_u loss_u 0.8696 (0.8722) acc_u 12.5000 (16.5625) lr 6.1732e-04 eta 0:00:14
epoch [127/200] batch [35/61] time 0.402 (0.456) data 0.270 (0.325) loss_u loss_u 0.8857 (0.8766) acc_u 12.5000 (16.0714) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [40/61] time 0.542 (0.457) data 0.411 (0.326) loss_u loss_u 0.8896 (0.8782) acc_u 12.5000 (15.8594) lr 6.1732e-04 eta 0:00:09
epoch [127/200] batch [45/61] time 0.444 (0.458) data 0.312 (0.327) loss_u loss_u 0.7412 (0.8744) acc_u 31.2500 (16.1111) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [50/61] time 0.396 (0.456) data 0.264 (0.325) loss_u loss_u 0.8726 (0.8765) acc_u 12.5000 (15.7500) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [55/61] time 0.399 (0.455) data 0.267 (0.324) loss_u loss_u 0.8970 (0.8792) acc_u 12.5000 (15.2841) lr 6.1732e-04 eta 0:00:02
epoch [127/200] batch [60/61] time 0.487 (0.453) data 0.354 (0.322) loss_u loss_u 0.8169 (0.8788) acc_u 21.8750 (15.3646) lr 6.1732e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1408
confident_label rate tensor(0.3772, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1183
clean true:1174
clean false:9
clean_rate:0.9923922231614539
noisy true:554
noisy false:1399
after delete: len(clean_dataset) 1183
after delete: len(noisy_dataset) 1953
epoch [128/200] batch [5/36] time 0.364 (0.405) data 0.233 (0.274) loss_x loss_x 1.4277 (1.1646) acc_x 71.8750 (71.8750) lr 6.0285e-04 eta 0:00:12
epoch [128/200] batch [10/36] time 0.520 (0.438) data 0.389 (0.307) loss_x loss_x 1.4785 (1.1709) acc_x 68.7500 (72.8125) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [15/36] time 0.429 (0.439) data 0.298 (0.308) loss_x loss_x 1.2939 (1.1661) acc_x 71.8750 (72.2917) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [20/36] time 0.595 (0.451) data 0.464 (0.320) loss_x loss_x 0.6079 (1.1467) acc_x 90.6250 (72.3438) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [25/36] time 0.708 (0.461) data 0.577 (0.330) loss_x loss_x 0.9243 (1.1270) acc_x 75.0000 (72.1250) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [30/36] time 0.397 (0.461) data 0.266 (0.330) loss_x loss_x 1.5820 (1.1205) acc_x 59.3750 (72.6042) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [35/36] time 0.488 (0.465) data 0.357 (0.334) loss_x loss_x 1.2998 (1.0939) acc_x 71.8750 (73.3929) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/61] time 0.491 (0.460) data 0.360 (0.329) loss_u loss_u 0.8789 (0.8595) acc_u 12.5000 (18.1250) lr 6.0285e-04 eta 0:00:25
epoch [128/200] batch [10/61] time 0.338 (0.454) data 0.206 (0.323) loss_u loss_u 0.8750 (0.8765) acc_u 15.6250 (15.3125) lr 6.0285e-04 eta 0:00:23
epoch [128/200] batch [15/61] time 0.408 (0.453) data 0.277 (0.321) loss_u loss_u 0.8525 (0.8686) acc_u 18.7500 (15.8333) lr 6.0285e-04 eta 0:00:20
epoch [128/200] batch [20/61] time 0.528 (0.456) data 0.394 (0.324) loss_u loss_u 0.8862 (0.8751) acc_u 15.6250 (15.4688) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [25/61] time 0.388 (0.460) data 0.256 (0.329) loss_u loss_u 0.9111 (0.8739) acc_u 12.5000 (16.1250) lr 6.0285e-04 eta 0:00:16
epoch [128/200] batch [30/61] time 0.376 (0.453) data 0.244 (0.322) loss_u loss_u 0.8555 (0.8757) acc_u 21.8750 (15.8333) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [35/61] time 0.443 (0.454) data 0.312 (0.323) loss_u loss_u 0.8838 (0.8760) acc_u 15.6250 (15.9821) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [40/61] time 0.506 (0.453) data 0.375 (0.322) loss_u loss_u 0.8848 (0.8763) acc_u 12.5000 (15.7812) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [45/61] time 0.338 (0.449) data 0.207 (0.318) loss_u loss_u 0.8179 (0.8717) acc_u 18.7500 (16.1806) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [50/61] time 0.378 (0.446) data 0.247 (0.315) loss_u loss_u 0.8379 (0.8726) acc_u 25.0000 (15.9375) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [55/61] time 0.395 (0.445) data 0.264 (0.314) loss_u loss_u 0.8716 (0.8729) acc_u 12.5000 (15.7386) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [60/61] time 0.334 (0.444) data 0.203 (0.312) loss_u loss_u 0.8994 (0.8722) acc_u 12.5000 (15.7812) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1442
confident_label rate tensor(0.3673, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1152
clean true:1147
clean false:5
clean_rate:0.9956597222222222
noisy true:547
noisy false:1437
after delete: len(clean_dataset) 1152
after delete: len(noisy_dataset) 1984
epoch [129/200] batch [5/36] time 0.428 (0.458) data 0.297 (0.327) loss_x loss_x 0.8604 (1.0088) acc_x 75.0000 (75.0000) lr 5.8849e-04 eta 0:00:14
epoch [129/200] batch [10/36] time 0.388 (0.471) data 0.257 (0.340) loss_x loss_x 0.7246 (1.0686) acc_x 87.5000 (74.6875) lr 5.8849e-04 eta 0:00:12
epoch [129/200] batch [15/36] time 0.508 (0.462) data 0.377 (0.331) loss_x loss_x 1.2676 (1.0837) acc_x 68.7500 (73.9583) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [20/36] time 0.401 (0.463) data 0.269 (0.331) loss_x loss_x 1.3594 (1.1348) acc_x 65.6250 (73.2812) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [25/36] time 0.353 (0.452) data 0.223 (0.321) loss_x loss_x 1.3809 (1.1293) acc_x 65.6250 (72.6250) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [30/36] time 0.614 (0.457) data 0.483 (0.326) loss_x loss_x 1.1064 (1.1101) acc_x 71.8750 (72.8125) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [35/36] time 0.446 (0.457) data 0.315 (0.325) loss_x loss_x 1.1631 (1.0876) acc_x 65.6250 (72.6786) lr 5.8849e-04 eta 0:00:00
epoch [129/200] batch [5/62] time 0.428 (0.449) data 0.296 (0.318) loss_u loss_u 0.8325 (0.8664) acc_u 18.7500 (16.8750) lr 5.8849e-04 eta 0:00:25
epoch [129/200] batch [10/62] time 0.378 (0.452) data 0.246 (0.321) loss_u loss_u 0.8706 (0.8634) acc_u 12.5000 (15.9375) lr 5.8849e-04 eta 0:00:23
epoch [129/200] batch [15/62] time 0.422 (0.449) data 0.291 (0.318) loss_u loss_u 0.9097 (0.8654) acc_u 18.7500 (16.0417) lr 5.8849e-04 eta 0:00:21
epoch [129/200] batch [20/62] time 0.438 (0.447) data 0.306 (0.316) loss_u loss_u 0.8716 (0.8752) acc_u 15.6250 (15.3125) lr 5.8849e-04 eta 0:00:18
epoch [129/200] batch [25/62] time 0.407 (0.443) data 0.275 (0.312) loss_u loss_u 0.8691 (0.8738) acc_u 21.8750 (15.6250) lr 5.8849e-04 eta 0:00:16
epoch [129/200] batch [30/62] time 0.404 (0.441) data 0.272 (0.310) loss_u loss_u 0.9082 (0.8737) acc_u 15.6250 (15.7292) lr 5.8849e-04 eta 0:00:14
epoch [129/200] batch [35/62] time 0.434 (0.439) data 0.302 (0.308) loss_u loss_u 0.8940 (0.8770) acc_u 15.6250 (15.6250) lr 5.8849e-04 eta 0:00:11
epoch [129/200] batch [40/62] time 0.371 (0.438) data 0.239 (0.307) loss_u loss_u 0.8320 (0.8764) acc_u 21.8750 (15.7812) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [45/62] time 0.456 (0.440) data 0.325 (0.309) loss_u loss_u 0.9033 (0.8744) acc_u 9.3750 (15.9722) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [50/62] time 0.378 (0.443) data 0.247 (0.311) loss_u loss_u 0.8276 (0.8743) acc_u 21.8750 (15.9375) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [55/62] time 0.376 (0.441) data 0.245 (0.310) loss_u loss_u 0.8896 (0.8713) acc_u 9.3750 (16.1932) lr 5.8849e-04 eta 0:00:03
epoch [129/200] batch [60/62] time 0.430 (0.441) data 0.298 (0.309) loss_u loss_u 0.7861 (0.8694) acc_u 28.1250 (16.4062) lr 5.8849e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1375
confident_label rate tensor(0.3833, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1202
clean true:1197
clean false:5
clean_rate:0.9958402662229617
noisy true:564
noisy false:1370
after delete: len(clean_dataset) 1202
after delete: len(noisy_dataset) 1934
epoch [130/200] batch [5/37] time 0.603 (0.476) data 0.472 (0.345) loss_x loss_x 0.6807 (0.9422) acc_x 81.2500 (74.3750) lr 5.7422e-04 eta 0:00:15
epoch [130/200] batch [10/37] time 0.556 (0.473) data 0.425 (0.342) loss_x loss_x 1.0439 (0.9740) acc_x 62.5000 (73.4375) lr 5.7422e-04 eta 0:00:12
epoch [130/200] batch [15/37] time 0.368 (0.456) data 0.237 (0.325) loss_x loss_x 1.0869 (1.0510) acc_x 75.0000 (72.5000) lr 5.7422e-04 eta 0:00:10
epoch [130/200] batch [20/37] time 0.523 (0.460) data 0.393 (0.330) loss_x loss_x 1.5791 (1.1328) acc_x 65.6250 (71.2500) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [25/37] time 0.484 (0.464) data 0.353 (0.333) loss_x loss_x 1.2236 (1.1460) acc_x 78.1250 (71.5000) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [30/37] time 0.355 (0.457) data 0.224 (0.326) loss_x loss_x 0.6621 (1.1061) acc_x 87.5000 (72.8125) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [35/37] time 0.352 (0.448) data 0.221 (0.318) loss_x loss_x 1.1367 (1.1052) acc_x 75.0000 (72.9464) lr 5.7422e-04 eta 0:00:00
epoch [130/200] batch [5/60] time 0.402 (0.455) data 0.271 (0.324) loss_u loss_u 0.9004 (0.8754) acc_u 15.6250 (15.0000) lr 5.7422e-04 eta 0:00:25
epoch [130/200] batch [10/60] time 0.460 (0.460) data 0.327 (0.329) loss_u loss_u 0.9058 (0.8821) acc_u 9.3750 (14.6875) lr 5.7422e-04 eta 0:00:22
epoch [130/200] batch [15/60] time 0.381 (0.472) data 0.249 (0.341) loss_u loss_u 0.9360 (0.8860) acc_u 12.5000 (15.2083) lr 5.7422e-04 eta 0:00:21
epoch [130/200] batch [20/60] time 0.446 (0.476) data 0.315 (0.345) loss_u loss_u 0.8315 (0.8848) acc_u 25.0000 (15.3125) lr 5.7422e-04 eta 0:00:19
epoch [130/200] batch [25/60] time 0.387 (0.470) data 0.255 (0.339) loss_u loss_u 0.9072 (0.8861) acc_u 15.6250 (15.3750) lr 5.7422e-04 eta 0:00:16
epoch [130/200] batch [30/60] time 0.447 (0.472) data 0.315 (0.341) loss_u loss_u 0.8477 (0.8828) acc_u 18.7500 (15.4167) lr 5.7422e-04 eta 0:00:14
epoch [130/200] batch [35/60] time 0.462 (0.467) data 0.330 (0.336) loss_u loss_u 0.8438 (0.8811) acc_u 21.8750 (15.8929) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [40/60] time 0.495 (0.466) data 0.363 (0.335) loss_u loss_u 0.8081 (0.8778) acc_u 28.1250 (16.4844) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [45/60] time 0.381 (0.464) data 0.249 (0.333) loss_u loss_u 0.8159 (0.8788) acc_u 25.0000 (16.4583) lr 5.7422e-04 eta 0:00:06
epoch [130/200] batch [50/60] time 0.497 (0.462) data 0.366 (0.331) loss_u loss_u 0.7920 (0.8804) acc_u 28.1250 (16.1250) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [55/60] time 0.420 (0.459) data 0.289 (0.328) loss_u loss_u 0.8975 (0.8789) acc_u 12.5000 (16.2500) lr 5.7422e-04 eta 0:00:02
epoch [130/200] batch [60/60] time 0.378 (0.455) data 0.246 (0.323) loss_u loss_u 0.8564 (0.8807) acc_u 15.6250 (15.8854) lr 5.7422e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1390
confident_label rate tensor(0.3852, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1208
clean true:1204
clean false:4
clean_rate:0.9966887417218543
noisy true:542
noisy false:1386
after delete: len(clean_dataset) 1208
after delete: len(noisy_dataset) 1928
epoch [131/200] batch [5/37] time 0.510 (0.452) data 0.379 (0.321) loss_x loss_x 1.0215 (0.9442) acc_x 78.1250 (78.7500) lr 5.6006e-04 eta 0:00:14
epoch [131/200] batch [10/37] time 0.382 (0.467) data 0.252 (0.336) loss_x loss_x 0.4524 (1.0414) acc_x 93.7500 (76.8750) lr 5.6006e-04 eta 0:00:12
epoch [131/200] batch [15/37] time 0.531 (0.464) data 0.399 (0.333) loss_x loss_x 1.1621 (1.0805) acc_x 65.6250 (74.5833) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [20/37] time 0.481 (0.457) data 0.351 (0.326) loss_x loss_x 0.9839 (1.0765) acc_x 68.7500 (73.9062) lr 5.6006e-04 eta 0:00:07
epoch [131/200] batch [25/37] time 0.438 (0.452) data 0.307 (0.321) loss_x loss_x 0.9854 (1.0655) acc_x 78.1250 (74.2500) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [30/37] time 0.405 (0.455) data 0.274 (0.324) loss_x loss_x 1.0342 (1.0956) acc_x 65.6250 (73.5417) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [35/37] time 0.441 (0.457) data 0.310 (0.326) loss_x loss_x 1.5566 (1.1124) acc_x 56.2500 (72.8571) lr 5.6006e-04 eta 0:00:00
epoch [131/200] batch [5/60] time 0.428 (0.450) data 0.297 (0.319) loss_u loss_u 0.8740 (0.9025) acc_u 15.6250 (13.7500) lr 5.6006e-04 eta 0:00:24
epoch [131/200] batch [10/60] time 0.390 (0.447) data 0.258 (0.316) loss_u loss_u 0.9648 (0.9002) acc_u 0.0000 (14.0625) lr 5.6006e-04 eta 0:00:22
epoch [131/200] batch [15/60] time 0.345 (0.450) data 0.213 (0.319) loss_u loss_u 0.9185 (0.8911) acc_u 15.6250 (14.7917) lr 5.6006e-04 eta 0:00:20
epoch [131/200] batch [20/60] time 0.404 (0.447) data 0.272 (0.316) loss_u loss_u 0.8594 (0.8851) acc_u 18.7500 (15.0000) lr 5.6006e-04 eta 0:00:17
epoch [131/200] batch [25/60] time 0.407 (0.445) data 0.276 (0.314) loss_u loss_u 0.8350 (0.8806) acc_u 18.7500 (15.2500) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [30/60] time 0.543 (0.450) data 0.412 (0.319) loss_u loss_u 0.8130 (0.8747) acc_u 28.1250 (16.1458) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [35/60] time 0.377 (0.446) data 0.246 (0.314) loss_u loss_u 0.8457 (0.8758) acc_u 25.0000 (15.9821) lr 5.6006e-04 eta 0:00:11
epoch [131/200] batch [40/60] time 0.569 (0.448) data 0.437 (0.317) loss_u loss_u 0.8252 (0.8764) acc_u 25.0000 (16.0938) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [45/60] time 0.448 (0.448) data 0.316 (0.317) loss_u loss_u 0.8926 (0.8758) acc_u 12.5000 (16.1111) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [50/60] time 0.419 (0.446) data 0.287 (0.314) loss_u loss_u 0.8711 (0.8769) acc_u 18.7500 (16.1250) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [55/60] time 0.444 (0.443) data 0.313 (0.312) loss_u loss_u 0.8604 (0.8745) acc_u 21.8750 (16.4205) lr 5.6006e-04 eta 0:00:02
epoch [131/200] batch [60/60] time 0.665 (0.444) data 0.527 (0.313) loss_u loss_u 0.8984 (0.8754) acc_u 12.5000 (16.3021) lr 5.6006e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1378
confident_label rate tensor(0.3804, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1193
clean true:1188
clean false:5
clean_rate:0.9958088851634534
noisy true:570
noisy false:1373
after delete: len(clean_dataset) 1193
after delete: len(noisy_dataset) 1943
epoch [132/200] batch [5/37] time 0.440 (0.431) data 0.310 (0.301) loss_x loss_x 0.8843 (1.0757) acc_x 75.0000 (72.5000) lr 5.4601e-04 eta 0:00:13
epoch [132/200] batch [10/37] time 0.565 (0.459) data 0.435 (0.329) loss_x loss_x 1.2910 (1.1147) acc_x 62.5000 (71.8750) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [15/37] time 0.560 (0.475) data 0.429 (0.344) loss_x loss_x 1.6885 (1.1833) acc_x 65.6250 (70.0000) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [20/37] time 0.382 (0.474) data 0.251 (0.344) loss_x loss_x 0.9429 (1.2527) acc_x 81.2500 (69.0625) lr 5.4601e-04 eta 0:00:08
epoch [132/200] batch [25/37] time 0.637 (0.479) data 0.506 (0.348) loss_x loss_x 0.6577 (1.1958) acc_x 84.3750 (70.3750) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [30/37] time 0.371 (0.463) data 0.240 (0.332) loss_x loss_x 0.8208 (1.1786) acc_x 78.1250 (70.4167) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [35/37] time 0.537 (0.471) data 0.406 (0.341) loss_x loss_x 0.8232 (1.1776) acc_x 75.0000 (70.2679) lr 5.4601e-04 eta 0:00:00
epoch [132/200] batch [5/60] time 0.366 (0.461) data 0.235 (0.330) loss_u loss_u 0.9058 (0.8641) acc_u 9.3750 (16.8750) lr 5.4601e-04 eta 0:00:25
epoch [132/200] batch [10/60] time 0.415 (0.455) data 0.283 (0.325) loss_u loss_u 0.8291 (0.8599) acc_u 18.7500 (17.1875) lr 5.4601e-04 eta 0:00:22
epoch [132/200] batch [15/60] time 0.487 (0.458) data 0.356 (0.327) loss_u loss_u 0.9814 (0.8671) acc_u 0.0000 (16.6667) lr 5.4601e-04 eta 0:00:20
epoch [132/200] batch [20/60] time 0.451 (0.460) data 0.319 (0.329) loss_u loss_u 0.9272 (0.8738) acc_u 6.2500 (15.7812) lr 5.4601e-04 eta 0:00:18
epoch [132/200] batch [25/60] time 0.580 (0.468) data 0.449 (0.337) loss_u loss_u 0.8906 (0.8769) acc_u 18.7500 (15.5000) lr 5.4601e-04 eta 0:00:16
epoch [132/200] batch [30/60] time 0.368 (0.465) data 0.238 (0.334) loss_u loss_u 0.8657 (0.8740) acc_u 15.6250 (15.8333) lr 5.4601e-04 eta 0:00:13
epoch [132/200] batch [35/60] time 0.508 (0.463) data 0.377 (0.332) loss_u loss_u 0.7876 (0.8687) acc_u 28.1250 (16.6964) lr 5.4601e-04 eta 0:00:11
epoch [132/200] batch [40/60] time 0.441 (0.461) data 0.310 (0.330) loss_u loss_u 0.8789 (0.8705) acc_u 15.6250 (16.4062) lr 5.4601e-04 eta 0:00:09
epoch [132/200] batch [45/60] time 0.369 (0.457) data 0.238 (0.326) loss_u loss_u 0.8286 (0.8721) acc_u 18.7500 (16.0417) lr 5.4601e-04 eta 0:00:06
epoch [132/200] batch [50/60] time 0.394 (0.453) data 0.263 (0.322) loss_u loss_u 0.9297 (0.8710) acc_u 9.3750 (16.3125) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [55/60] time 0.483 (0.449) data 0.352 (0.318) loss_u loss_u 0.9292 (0.8721) acc_u 9.3750 (16.1932) lr 5.4601e-04 eta 0:00:02
epoch [132/200] batch [60/60] time 0.312 (0.447) data 0.182 (0.316) loss_u loss_u 0.8521 (0.8720) acc_u 18.7500 (16.3542) lr 5.4601e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1399
confident_label rate tensor(0.3811, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1195
clean true:1186
clean false:9
clean_rate:0.9924686192468619
noisy true:551
noisy false:1390
after delete: len(clean_dataset) 1195
after delete: len(noisy_dataset) 1941
epoch [133/200] batch [5/37] time 0.527 (0.465) data 0.396 (0.334) loss_x loss_x 1.2715 (1.0447) acc_x 81.2500 (78.1250) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [10/37] time 0.405 (0.462) data 0.274 (0.331) loss_x loss_x 0.7725 (1.0501) acc_x 81.2500 (77.1875) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [15/37] time 0.439 (0.451) data 0.307 (0.320) loss_x loss_x 1.4316 (1.1905) acc_x 75.0000 (74.1667) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [20/37] time 0.416 (0.461) data 0.285 (0.329) loss_x loss_x 1.2119 (1.1397) acc_x 71.8750 (74.5312) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [25/37] time 0.390 (0.456) data 0.260 (0.325) loss_x loss_x 0.9717 (1.1782) acc_x 87.5000 (74.2500) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [30/37] time 0.427 (0.451) data 0.296 (0.320) loss_x loss_x 0.4541 (1.1256) acc_x 84.3750 (74.8958) lr 5.3207e-04 eta 0:00:03
epoch [133/200] batch [35/37] time 0.439 (0.444) data 0.308 (0.313) loss_x loss_x 1.6270 (1.1598) acc_x 56.2500 (73.8393) lr 5.3207e-04 eta 0:00:00
epoch [133/200] batch [5/60] time 0.454 (0.453) data 0.322 (0.322) loss_u loss_u 0.8379 (0.8714) acc_u 18.7500 (18.7500) lr 5.3207e-04 eta 0:00:24
epoch [133/200] batch [10/60] time 0.444 (0.451) data 0.313 (0.320) loss_u loss_u 0.8276 (0.8792) acc_u 25.0000 (16.8750) lr 5.3207e-04 eta 0:00:22
epoch [133/200] batch [15/60] time 0.552 (0.455) data 0.421 (0.323) loss_u loss_u 0.8584 (0.8688) acc_u 25.0000 (18.1250) lr 5.3207e-04 eta 0:00:20
epoch [133/200] batch [20/60] time 0.372 (0.451) data 0.240 (0.320) loss_u loss_u 0.8696 (0.8723) acc_u 15.6250 (17.3438) lr 5.3207e-04 eta 0:00:18
epoch [133/200] batch [25/60] time 0.396 (0.450) data 0.264 (0.319) loss_u loss_u 0.8535 (0.8750) acc_u 15.6250 (16.5000) lr 5.3207e-04 eta 0:00:15
epoch [133/200] batch [30/60] time 0.467 (0.449) data 0.336 (0.318) loss_u loss_u 0.8359 (0.8753) acc_u 15.6250 (16.1458) lr 5.3207e-04 eta 0:00:13
epoch [133/200] batch [35/60] time 0.350 (0.443) data 0.218 (0.312) loss_u loss_u 0.8711 (0.8744) acc_u 15.6250 (15.9821) lr 5.3207e-04 eta 0:00:11
epoch [133/200] batch [40/60] time 0.440 (0.443) data 0.308 (0.312) loss_u loss_u 0.8682 (0.8754) acc_u 15.6250 (15.7812) lr 5.3207e-04 eta 0:00:08
epoch [133/200] batch [45/60] time 0.502 (0.444) data 0.370 (0.313) loss_u loss_u 0.8042 (0.8746) acc_u 25.0000 (15.8333) lr 5.3207e-04 eta 0:00:06
epoch [133/200] batch [50/60] time 0.438 (0.442) data 0.307 (0.310) loss_u loss_u 0.8691 (0.8755) acc_u 12.5000 (15.6250) lr 5.3207e-04 eta 0:00:04
epoch [133/200] batch [55/60] time 0.460 (0.441) data 0.328 (0.310) loss_u loss_u 0.8550 (0.8742) acc_u 21.8750 (15.8523) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [60/60] time 0.548 (0.443) data 0.416 (0.311) loss_u loss_u 0.8784 (0.8746) acc_u 12.5000 (15.7812) lr 5.3207e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1476
confident_label rate tensor(0.3645, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1143
clean true:1135
clean false:8
clean_rate:0.9930008748906387
noisy true:525
noisy false:1468
after delete: len(clean_dataset) 1143
after delete: len(noisy_dataset) 1993
epoch [134/200] batch [5/35] time 0.520 (0.474) data 0.389 (0.343) loss_x loss_x 1.5547 (1.2048) acc_x 59.3750 (70.6250) lr 5.1825e-04 eta 0:00:14
epoch [134/200] batch [10/35] time 0.422 (0.459) data 0.291 (0.328) loss_x loss_x 0.9648 (1.1783) acc_x 62.5000 (67.8125) lr 5.1825e-04 eta 0:00:11
epoch [134/200] batch [15/35] time 0.381 (0.453) data 0.250 (0.322) loss_x loss_x 0.8301 (1.1720) acc_x 84.3750 (69.5833) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [20/35] time 0.559 (0.477) data 0.429 (0.346) loss_x loss_x 1.8135 (1.2127) acc_x 56.2500 (69.3750) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [25/35] time 0.418 (0.480) data 0.288 (0.349) loss_x loss_x 1.1162 (1.2087) acc_x 62.5000 (69.7500) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [30/35] time 0.422 (0.473) data 0.292 (0.342) loss_x loss_x 1.0586 (1.2164) acc_x 65.6250 (69.1667) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [35/35] time 0.410 (0.469) data 0.280 (0.338) loss_x loss_x 1.3037 (1.1977) acc_x 65.6250 (69.2857) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/62] time 0.427 (0.462) data 0.296 (0.331) loss_u loss_u 0.8032 (0.8531) acc_u 21.8750 (18.7500) lr 5.1825e-04 eta 0:00:26
epoch [134/200] batch [10/62] time 0.550 (0.462) data 0.419 (0.331) loss_u loss_u 0.8823 (0.8537) acc_u 15.6250 (19.0625) lr 5.1825e-04 eta 0:00:24
epoch [134/200] batch [15/62] time 0.382 (0.456) data 0.250 (0.326) loss_u loss_u 0.9478 (0.8673) acc_u 3.1250 (17.0833) lr 5.1825e-04 eta 0:00:21
epoch [134/200] batch [20/62] time 0.346 (0.456) data 0.216 (0.325) loss_u loss_u 0.8647 (0.8623) acc_u 18.7500 (17.8125) lr 5.1825e-04 eta 0:00:19
epoch [134/200] batch [25/62] time 0.426 (0.453) data 0.295 (0.322) loss_u loss_u 0.8755 (0.8600) acc_u 12.5000 (17.7500) lr 5.1825e-04 eta 0:00:16
epoch [134/200] batch [30/62] time 0.508 (0.458) data 0.376 (0.327) loss_u loss_u 0.8638 (0.8596) acc_u 15.6250 (17.6042) lr 5.1825e-04 eta 0:00:14
epoch [134/200] batch [35/62] time 0.450 (0.457) data 0.319 (0.326) loss_u loss_u 0.8442 (0.8588) acc_u 18.7500 (17.7679) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [40/62] time 0.347 (0.451) data 0.215 (0.320) loss_u loss_u 0.8389 (0.8583) acc_u 21.8750 (17.7344) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [45/62] time 0.556 (0.454) data 0.425 (0.323) loss_u loss_u 0.9048 (0.8594) acc_u 12.5000 (17.7778) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [50/62] time 0.400 (0.452) data 0.269 (0.321) loss_u loss_u 0.8853 (0.8600) acc_u 15.6250 (17.9375) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [55/62] time 0.425 (0.450) data 0.295 (0.319) loss_u loss_u 0.8989 (0.8620) acc_u 15.6250 (17.8977) lr 5.1825e-04 eta 0:00:03
epoch [134/200] batch [60/62] time 0.360 (0.449) data 0.229 (0.318) loss_u loss_u 0.8506 (0.8617) acc_u 18.7500 (17.9167) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1391
confident_label rate tensor(0.3798, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1191
clean true:1184
clean false:7
clean_rate:0.9941225860621327
noisy true:561
noisy false:1384
after delete: len(clean_dataset) 1191
after delete: len(noisy_dataset) 1945
epoch [135/200] batch [5/37] time 0.435 (0.534) data 0.304 (0.403) loss_x loss_x 0.9258 (1.1084) acc_x 75.0000 (71.8750) lr 5.0454e-04 eta 0:00:17
epoch [135/200] batch [10/37] time 0.460 (0.503) data 0.330 (0.371) loss_x loss_x 0.8496 (0.9998) acc_x 71.8750 (75.3125) lr 5.0454e-04 eta 0:00:13
epoch [135/200] batch [15/37] time 0.422 (0.484) data 0.291 (0.353) loss_x loss_x 0.8608 (1.0287) acc_x 78.1250 (73.7500) lr 5.0454e-04 eta 0:00:10
epoch [135/200] batch [20/37] time 0.362 (0.466) data 0.231 (0.335) loss_x loss_x 1.0400 (1.0566) acc_x 68.7500 (73.4375) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [25/37] time 0.388 (0.463) data 0.257 (0.332) loss_x loss_x 1.1738 (1.0720) acc_x 78.1250 (73.2500) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [30/37] time 0.424 (0.456) data 0.293 (0.325) loss_x loss_x 1.3408 (1.0793) acc_x 75.0000 (73.2292) lr 5.0454e-04 eta 0:00:03
epoch [135/200] batch [35/37] time 0.420 (0.460) data 0.289 (0.329) loss_x loss_x 1.6289 (1.0931) acc_x 59.3750 (72.9464) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/60] time 0.365 (0.457) data 0.233 (0.326) loss_u loss_u 0.8223 (0.8650) acc_u 25.0000 (17.5000) lr 5.0454e-04 eta 0:00:25
epoch [135/200] batch [10/60] time 0.550 (0.458) data 0.418 (0.327) loss_u loss_u 0.8750 (0.8747) acc_u 15.6250 (15.3125) lr 5.0454e-04 eta 0:00:22
epoch [135/200] batch [15/60] time 0.447 (0.458) data 0.316 (0.327) loss_u loss_u 0.9355 (0.8808) acc_u 3.1250 (14.3750) lr 5.0454e-04 eta 0:00:20
epoch [135/200] batch [20/60] time 0.479 (0.456) data 0.346 (0.325) loss_u loss_u 0.9175 (0.8867) acc_u 6.2500 (13.2812) lr 5.0454e-04 eta 0:00:18
epoch [135/200] batch [25/60] time 0.431 (0.456) data 0.299 (0.325) loss_u loss_u 0.8091 (0.8811) acc_u 18.7500 (14.0000) lr 5.0454e-04 eta 0:00:15
epoch [135/200] batch [30/60] time 0.393 (0.457) data 0.258 (0.325) loss_u loss_u 0.9336 (0.8811) acc_u 6.2500 (14.2708) lr 5.0454e-04 eta 0:00:13
epoch [135/200] batch [35/60] time 0.496 (0.458) data 0.364 (0.326) loss_u loss_u 0.8618 (0.8785) acc_u 18.7500 (14.6429) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [40/60] time 0.334 (0.453) data 0.202 (0.322) loss_u loss_u 0.8486 (0.8776) acc_u 18.7500 (15.0781) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [45/60] time 0.364 (0.451) data 0.232 (0.320) loss_u loss_u 0.9077 (0.8746) acc_u 15.6250 (15.8333) lr 5.0454e-04 eta 0:00:06
epoch [135/200] batch [50/60] time 0.314 (0.450) data 0.182 (0.319) loss_u loss_u 0.8926 (0.8743) acc_u 12.5000 (15.8125) lr 5.0454e-04 eta 0:00:04
epoch [135/200] batch [55/60] time 0.355 (0.448) data 0.223 (0.316) loss_u loss_u 0.8926 (0.8761) acc_u 18.7500 (15.6250) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [60/60] time 0.463 (0.447) data 0.332 (0.315) loss_u loss_u 0.7959 (0.8757) acc_u 28.1250 (15.7812) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1365
confident_label rate tensor(0.3878, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1216
clean true:1212
clean false:4
clean_rate:0.9967105263157895
noisy true:559
noisy false:1361
after delete: len(clean_dataset) 1216
after delete: len(noisy_dataset) 1920
epoch [136/200] batch [5/38] time 0.421 (0.460) data 0.290 (0.329) loss_x loss_x 0.7812 (0.9776) acc_x 84.3750 (77.5000) lr 4.9096e-04 eta 0:00:15
epoch [136/200] batch [10/38] time 0.485 (0.459) data 0.354 (0.328) loss_x loss_x 0.8960 (0.9707) acc_x 75.0000 (78.4375) lr 4.9096e-04 eta 0:00:12
epoch [136/200] batch [15/38] time 0.426 (0.461) data 0.295 (0.330) loss_x loss_x 1.3584 (1.0097) acc_x 65.6250 (76.4583) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [20/38] time 0.523 (0.478) data 0.392 (0.347) loss_x loss_x 1.1582 (1.0510) acc_x 68.7500 (74.5312) lr 4.9096e-04 eta 0:00:08
epoch [136/200] batch [25/38] time 0.478 (0.473) data 0.344 (0.342) loss_x loss_x 1.3125 (1.0908) acc_x 62.5000 (73.0000) lr 4.9096e-04 eta 0:00:06
epoch [136/200] batch [30/38] time 0.432 (0.478) data 0.301 (0.347) loss_x loss_x 1.6758 (1.1259) acc_x 62.5000 (72.1875) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [35/38] time 0.454 (0.477) data 0.323 (0.346) loss_x loss_x 1.0801 (1.1387) acc_x 68.7500 (71.5179) lr 4.9096e-04 eta 0:00:01
epoch [136/200] batch [5/60] time 0.484 (0.475) data 0.353 (0.344) loss_u loss_u 0.9316 (0.9033) acc_u 9.3750 (11.2500) lr 4.9096e-04 eta 0:00:26
epoch [136/200] batch [10/60] time 0.418 (0.474) data 0.287 (0.343) loss_u loss_u 0.8989 (0.8949) acc_u 12.5000 (13.4375) lr 4.9096e-04 eta 0:00:23
epoch [136/200] batch [15/60] time 0.502 (0.468) data 0.370 (0.337) loss_u loss_u 0.8774 (0.8901) acc_u 21.8750 (15.0000) lr 4.9096e-04 eta 0:00:21
epoch [136/200] batch [20/60] time 0.548 (0.467) data 0.416 (0.336) loss_u loss_u 0.8457 (0.8834) acc_u 25.0000 (15.9375) lr 4.9096e-04 eta 0:00:18
epoch [136/200] batch [25/60] time 0.342 (0.461) data 0.211 (0.329) loss_u loss_u 0.8789 (0.8759) acc_u 15.6250 (16.7500) lr 4.9096e-04 eta 0:00:16
epoch [136/200] batch [30/60] time 0.427 (0.456) data 0.296 (0.324) loss_u loss_u 0.8960 (0.8730) acc_u 12.5000 (16.7708) lr 4.9096e-04 eta 0:00:13
epoch [136/200] batch [35/60] time 0.397 (0.455) data 0.265 (0.324) loss_u loss_u 0.8755 (0.8740) acc_u 12.5000 (16.2500) lr 4.9096e-04 eta 0:00:11
epoch [136/200] batch [40/60] time 0.410 (0.452) data 0.279 (0.321) loss_u loss_u 0.8379 (0.8723) acc_u 21.8750 (16.4062) lr 4.9096e-04 eta 0:00:09
epoch [136/200] batch [45/60] time 0.377 (0.450) data 0.246 (0.318) loss_u loss_u 0.8877 (0.8739) acc_u 12.5000 (16.1111) lr 4.9096e-04 eta 0:00:06
epoch [136/200] batch [50/60] time 0.421 (0.448) data 0.289 (0.317) loss_u loss_u 0.9141 (0.8744) acc_u 9.3750 (16.0000) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [55/60] time 0.412 (0.445) data 0.280 (0.314) loss_u loss_u 0.8574 (0.8737) acc_u 25.0000 (16.0227) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [60/60] time 0.450 (0.449) data 0.318 (0.317) loss_u loss_u 0.8750 (0.8739) acc_u 18.7500 (16.0417) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1378
confident_label rate tensor(0.3916, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1228
clean true:1223
clean false:5
clean_rate:0.995928338762215
noisy true:535
noisy false:1373
after delete: len(clean_dataset) 1228
after delete: len(noisy_dataset) 1908
epoch [137/200] batch [5/38] time 0.436 (0.477) data 0.305 (0.346) loss_x loss_x 1.5127 (1.2565) acc_x 68.7500 (68.7500) lr 4.7750e-04 eta 0:00:15
epoch [137/200] batch [10/38] time 0.444 (0.466) data 0.315 (0.335) loss_x loss_x 1.0742 (1.1728) acc_x 75.0000 (71.8750) lr 4.7750e-04 eta 0:00:13
epoch [137/200] batch [15/38] time 0.345 (0.458) data 0.215 (0.328) loss_x loss_x 1.1201 (1.1564) acc_x 65.6250 (71.6667) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [20/38] time 0.449 (0.463) data 0.318 (0.333) loss_x loss_x 1.0303 (1.1563) acc_x 75.0000 (70.6250) lr 4.7750e-04 eta 0:00:08
epoch [137/200] batch [25/38] time 0.613 (0.463) data 0.482 (0.333) loss_x loss_x 1.4395 (1.1520) acc_x 68.7500 (70.7500) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [30/38] time 0.500 (0.461) data 0.369 (0.331) loss_x loss_x 1.1445 (1.1764) acc_x 59.3750 (69.7917) lr 4.7750e-04 eta 0:00:03
epoch [137/200] batch [35/38] time 0.493 (0.461) data 0.363 (0.331) loss_x loss_x 0.6733 (1.1463) acc_x 81.2500 (70.8929) lr 4.7750e-04 eta 0:00:01
epoch [137/200] batch [5/59] time 0.349 (0.448) data 0.217 (0.317) loss_u loss_u 0.8242 (0.8650) acc_u 25.0000 (17.5000) lr 4.7750e-04 eta 0:00:24
epoch [137/200] batch [10/59] time 0.539 (0.449) data 0.407 (0.319) loss_u loss_u 0.8784 (0.8681) acc_u 18.7500 (17.5000) lr 4.7750e-04 eta 0:00:22
epoch [137/200] batch [15/59] time 0.460 (0.452) data 0.328 (0.321) loss_u loss_u 0.8882 (0.8679) acc_u 18.7500 (17.5000) lr 4.7750e-04 eta 0:00:19
epoch [137/200] batch [20/59] time 0.389 (0.455) data 0.257 (0.324) loss_u loss_u 0.9155 (0.8672) acc_u 12.5000 (17.0312) lr 4.7750e-04 eta 0:00:17
epoch [137/200] batch [25/59] time 0.395 (0.454) data 0.263 (0.323) loss_u loss_u 0.8618 (0.8701) acc_u 15.6250 (16.8750) lr 4.7750e-04 eta 0:00:15
epoch [137/200] batch [30/59] time 0.442 (0.451) data 0.311 (0.320) loss_u loss_u 0.8540 (0.8671) acc_u 21.8750 (17.5000) lr 4.7750e-04 eta 0:00:13
epoch [137/200] batch [35/59] time 0.406 (0.450) data 0.276 (0.319) loss_u loss_u 0.7622 (0.8713) acc_u 28.1250 (16.6964) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [40/59] time 0.516 (0.453) data 0.384 (0.322) loss_u loss_u 0.8472 (0.8737) acc_u 15.6250 (16.3281) lr 4.7750e-04 eta 0:00:08
epoch [137/200] batch [45/59] time 0.373 (0.449) data 0.243 (0.318) loss_u loss_u 0.9331 (0.8760) acc_u 9.3750 (16.1111) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [50/59] time 0.449 (0.446) data 0.318 (0.315) loss_u loss_u 0.8887 (0.8772) acc_u 15.6250 (16.0000) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [55/59] time 0.496 (0.445) data 0.365 (0.313) loss_u loss_u 0.8887 (0.8764) acc_u 15.6250 (16.0227) lr 4.7750e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1396
confident_label rate tensor(0.3836, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1203
clean true:1196
clean false:7
clean_rate:0.9941812136325852
noisy true:544
noisy false:1389
after delete: len(clean_dataset) 1203
after delete: len(noisy_dataset) 1933
epoch [138/200] batch [5/37] time 0.424 (0.436) data 0.293 (0.306) loss_x loss_x 1.3057 (1.1739) acc_x 75.0000 (71.2500) lr 4.6417e-04 eta 0:00:13
epoch [138/200] batch [10/37] time 0.517 (0.454) data 0.387 (0.323) loss_x loss_x 1.0010 (1.1568) acc_x 71.8750 (71.8750) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [15/37] time 0.644 (0.484) data 0.513 (0.353) loss_x loss_x 1.2607 (1.0954) acc_x 75.0000 (73.9583) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [20/37] time 0.450 (0.476) data 0.319 (0.345) loss_x loss_x 0.8901 (1.1237) acc_x 71.8750 (71.8750) lr 4.6417e-04 eta 0:00:08
epoch [138/200] batch [25/37] time 0.538 (0.473) data 0.407 (0.342) loss_x loss_x 1.5547 (1.1154) acc_x 59.3750 (72.3750) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [30/37] time 0.512 (0.472) data 0.380 (0.341) loss_x loss_x 1.2822 (1.1216) acc_x 75.0000 (72.7083) lr 4.6417e-04 eta 0:00:03
epoch [138/200] batch [35/37] time 0.434 (0.471) data 0.303 (0.340) loss_x loss_x 0.8291 (1.1292) acc_x 75.0000 (72.5893) lr 4.6417e-04 eta 0:00:00
epoch [138/200] batch [5/60] time 0.380 (0.461) data 0.248 (0.330) loss_u loss_u 0.8862 (0.8554) acc_u 15.6250 (19.3750) lr 4.6417e-04 eta 0:00:25
epoch [138/200] batch [10/60] time 0.453 (0.458) data 0.321 (0.327) loss_u loss_u 0.8486 (0.8583) acc_u 21.8750 (18.1250) lr 4.6417e-04 eta 0:00:22
epoch [138/200] batch [15/60] time 0.502 (0.463) data 0.370 (0.331) loss_u loss_u 0.9053 (0.8667) acc_u 12.5000 (17.7083) lr 4.6417e-04 eta 0:00:20
epoch [138/200] batch [20/60] time 0.407 (0.457) data 0.276 (0.325) loss_u loss_u 0.8945 (0.8605) acc_u 12.5000 (17.6562) lr 4.6417e-04 eta 0:00:18
epoch [138/200] batch [25/60] time 0.401 (0.452) data 0.269 (0.321) loss_u loss_u 0.8545 (0.8671) acc_u 15.6250 (17.1250) lr 4.6417e-04 eta 0:00:15
epoch [138/200] batch [30/60] time 0.400 (0.453) data 0.268 (0.322) loss_u loss_u 0.8057 (0.8568) acc_u 25.0000 (18.6458) lr 4.6417e-04 eta 0:00:13
epoch [138/200] batch [35/60] time 0.529 (0.454) data 0.397 (0.323) loss_u loss_u 0.9312 (0.8646) acc_u 6.2500 (17.4107) lr 4.6417e-04 eta 0:00:11
epoch [138/200] batch [40/60] time 0.430 (0.450) data 0.299 (0.319) loss_u loss_u 0.8652 (0.8650) acc_u 12.5000 (17.1094) lr 4.6417e-04 eta 0:00:08
epoch [138/200] batch [45/60] time 0.409 (0.449) data 0.277 (0.318) loss_u loss_u 0.9087 (0.8692) acc_u 12.5000 (16.5972) lr 4.6417e-04 eta 0:00:06
epoch [138/200] batch [50/60] time 0.379 (0.448) data 0.248 (0.316) loss_u loss_u 0.8657 (0.8718) acc_u 18.7500 (16.3750) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [55/60] time 0.358 (0.446) data 0.227 (0.315) loss_u loss_u 0.9297 (0.8721) acc_u 9.3750 (16.3068) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [60/60] time 0.394 (0.445) data 0.263 (0.314) loss_u loss_u 0.9111 (0.8700) acc_u 9.3750 (16.4583) lr 4.6417e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1404
confident_label rate tensor(0.3734, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1171
clean true:1167
clean false:4
clean_rate:0.9965841161400513
noisy true:565
noisy false:1400
after delete: len(clean_dataset) 1171
after delete: len(noisy_dataset) 1965
epoch [139/200] batch [5/36] time 0.379 (0.422) data 0.248 (0.291) loss_x loss_x 0.9551 (1.0077) acc_x 75.0000 (75.0000) lr 4.5098e-04 eta 0:00:13
epoch [139/200] batch [10/36] time 0.534 (0.443) data 0.403 (0.312) loss_x loss_x 1.5322 (1.0247) acc_x 81.2500 (75.6250) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [15/36] time 0.460 (0.464) data 0.329 (0.333) loss_x loss_x 0.8511 (1.0210) acc_x 81.2500 (76.0417) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [20/36] time 0.410 (0.458) data 0.279 (0.327) loss_x loss_x 1.2217 (1.0249) acc_x 65.6250 (75.7812) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [25/36] time 0.488 (0.451) data 0.357 (0.321) loss_x loss_x 1.2529 (1.0429) acc_x 62.5000 (74.3750) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [30/36] time 0.392 (0.450) data 0.262 (0.319) loss_x loss_x 1.2285 (1.0606) acc_x 65.6250 (73.6458) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [35/36] time 0.508 (0.448) data 0.375 (0.317) loss_x loss_x 1.4111 (1.1199) acc_x 68.7500 (72.5000) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/61] time 0.423 (0.445) data 0.291 (0.314) loss_u loss_u 0.8354 (0.8802) acc_u 21.8750 (15.6250) lr 4.5098e-04 eta 0:00:24
epoch [139/200] batch [10/61] time 0.420 (0.441) data 0.288 (0.310) loss_u loss_u 0.8848 (0.8780) acc_u 15.6250 (15.6250) lr 4.5098e-04 eta 0:00:22
epoch [139/200] batch [15/61] time 0.409 (0.442) data 0.279 (0.311) loss_u loss_u 0.8921 (0.8779) acc_u 12.5000 (15.6250) lr 4.5098e-04 eta 0:00:20
epoch [139/200] batch [20/61] time 0.398 (0.439) data 0.267 (0.308) loss_u loss_u 0.7993 (0.8803) acc_u 28.1250 (15.6250) lr 4.5098e-04 eta 0:00:17
epoch [139/200] batch [25/61] time 0.424 (0.439) data 0.294 (0.308) loss_u loss_u 0.8613 (0.8763) acc_u 15.6250 (16.1250) lr 4.5098e-04 eta 0:00:15
epoch [139/200] batch [30/61] time 0.423 (0.437) data 0.291 (0.306) loss_u loss_u 0.9634 (0.8758) acc_u 3.1250 (15.9375) lr 4.5098e-04 eta 0:00:13
epoch [139/200] batch [35/61] time 0.401 (0.436) data 0.270 (0.304) loss_u loss_u 0.8833 (0.8724) acc_u 12.5000 (16.4286) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [40/61] time 0.646 (0.440) data 0.515 (0.309) loss_u loss_u 0.8584 (0.8735) acc_u 21.8750 (16.4062) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [45/61] time 0.414 (0.438) data 0.284 (0.307) loss_u loss_u 0.8970 (0.8714) acc_u 9.3750 (16.5278) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [50/61] time 0.381 (0.442) data 0.251 (0.311) loss_u loss_u 0.9092 (0.8729) acc_u 12.5000 (16.3125) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [55/61] time 0.415 (0.441) data 0.284 (0.310) loss_u loss_u 0.8706 (0.8707) acc_u 18.7500 (16.7614) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [60/61] time 0.321 (0.443) data 0.190 (0.312) loss_u loss_u 0.8862 (0.8698) acc_u 12.5000 (16.6667) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1383
confident_label rate tensor(0.3823, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1199
clean true:1189
clean false:10
clean_rate:0.9916597164303587
noisy true:564
noisy false:1373
after delete: len(clean_dataset) 1199
after delete: len(noisy_dataset) 1937
epoch [140/200] batch [5/37] time 0.452 (0.445) data 0.321 (0.314) loss_x loss_x 0.9507 (0.8458) acc_x 78.1250 (81.2500) lr 4.3792e-04 eta 0:00:14
epoch [140/200] batch [10/37] time 0.536 (0.447) data 0.405 (0.316) loss_x loss_x 1.5225 (1.0693) acc_x 68.7500 (75.6250) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [15/37] time 0.488 (0.452) data 0.357 (0.321) loss_x loss_x 1.1973 (1.0917) acc_x 65.6250 (73.5417) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [20/37] time 0.415 (0.444) data 0.284 (0.314) loss_x loss_x 1.1562 (1.1084) acc_x 75.0000 (72.9688) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [25/37] time 0.506 (0.442) data 0.375 (0.311) loss_x loss_x 0.7793 (1.0884) acc_x 75.0000 (73.7500) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [30/37] time 0.579 (0.447) data 0.447 (0.316) loss_x loss_x 0.9922 (1.0872) acc_x 78.1250 (74.1667) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [35/37] time 0.421 (0.444) data 0.290 (0.313) loss_x loss_x 1.2842 (1.0952) acc_x 68.7500 (74.0179) lr 4.3792e-04 eta 0:00:00
epoch [140/200] batch [5/60] time 0.531 (0.444) data 0.400 (0.313) loss_u loss_u 0.8247 (0.8542) acc_u 18.7500 (18.7500) lr 4.3792e-04 eta 0:00:24
epoch [140/200] batch [10/60] time 0.351 (0.441) data 0.220 (0.310) loss_u loss_u 0.8799 (0.8578) acc_u 18.7500 (18.7500) lr 4.3792e-04 eta 0:00:22
epoch [140/200] batch [15/60] time 0.467 (0.438) data 0.335 (0.306) loss_u loss_u 0.8755 (0.8570) acc_u 18.7500 (18.9583) lr 4.3792e-04 eta 0:00:19
epoch [140/200] batch [20/60] time 0.374 (0.444) data 0.242 (0.313) loss_u loss_u 0.8779 (0.8664) acc_u 18.7500 (17.6562) lr 4.3792e-04 eta 0:00:17
epoch [140/200] batch [25/60] time 0.456 (0.443) data 0.324 (0.312) loss_u loss_u 0.8398 (0.8647) acc_u 21.8750 (18.0000) lr 4.3792e-04 eta 0:00:15
epoch [140/200] batch [30/60] time 0.545 (0.444) data 0.413 (0.313) loss_u loss_u 0.7993 (0.8622) acc_u 21.8750 (18.3333) lr 4.3792e-04 eta 0:00:13
epoch [140/200] batch [35/60] time 0.403 (0.444) data 0.271 (0.312) loss_u loss_u 0.9287 (0.8670) acc_u 6.2500 (17.7679) lr 4.3792e-04 eta 0:00:11
epoch [140/200] batch [40/60] time 0.556 (0.447) data 0.424 (0.315) loss_u loss_u 0.8896 (0.8682) acc_u 12.5000 (17.3438) lr 4.3792e-04 eta 0:00:08
epoch [140/200] batch [45/60] time 0.481 (0.446) data 0.349 (0.314) loss_u loss_u 0.9595 (0.8700) acc_u 6.2500 (17.0139) lr 4.3792e-04 eta 0:00:06
epoch [140/200] batch [50/60] time 0.347 (0.442) data 0.215 (0.310) loss_u loss_u 0.9189 (0.8738) acc_u 9.3750 (16.3125) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [55/60] time 0.462 (0.441) data 0.330 (0.310) loss_u loss_u 0.8687 (0.8747) acc_u 15.6250 (16.1364) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [60/60] time 0.407 (0.442) data 0.273 (0.310) loss_u loss_u 0.8672 (0.8753) acc_u 18.7500 (16.1458) lr 4.3792e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1369
confident_label rate tensor(0.3865, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1212
clean true:1201
clean false:11
clean_rate:0.990924092409241
noisy true:566
noisy false:1358
after delete: len(clean_dataset) 1212
after delete: len(noisy_dataset) 1924
epoch [141/200] batch [5/37] time 0.419 (0.417) data 0.288 (0.286) loss_x loss_x 1.1660 (1.0915) acc_x 71.8750 (75.6250) lr 4.2499e-04 eta 0:00:13
epoch [141/200] batch [10/37] time 0.471 (0.447) data 0.340 (0.316) loss_x loss_x 0.9717 (1.0777) acc_x 75.0000 (76.2500) lr 4.2499e-04 eta 0:00:12
epoch [141/200] batch [15/37] time 0.539 (0.449) data 0.408 (0.318) loss_x loss_x 1.6123 (1.0551) acc_x 59.3750 (75.8333) lr 4.2499e-04 eta 0:00:09
epoch [141/200] batch [20/37] time 0.559 (0.460) data 0.428 (0.329) loss_x loss_x 1.6865 (1.1054) acc_x 68.7500 (74.3750) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [25/37] time 0.507 (0.453) data 0.376 (0.322) loss_x loss_x 1.4912 (1.1342) acc_x 62.5000 (73.2500) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [30/37] time 0.397 (0.448) data 0.266 (0.317) loss_x loss_x 0.8071 (1.1436) acc_x 81.2500 (72.7083) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [35/37] time 0.334 (0.442) data 0.203 (0.311) loss_x loss_x 0.6807 (1.1343) acc_x 81.2500 (73.1250) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/60] time 0.480 (0.446) data 0.349 (0.315) loss_u loss_u 0.9160 (0.9063) acc_u 12.5000 (13.1250) lr 4.2499e-04 eta 0:00:24
epoch [141/200] batch [10/60] time 0.447 (0.441) data 0.315 (0.310) loss_u loss_u 0.8291 (0.8987) acc_u 18.7500 (13.4375) lr 4.2499e-04 eta 0:00:22
epoch [141/200] batch [15/60] time 0.449 (0.441) data 0.317 (0.310) loss_u loss_u 0.8892 (0.8895) acc_u 9.3750 (14.7917) lr 4.2499e-04 eta 0:00:19
epoch [141/200] batch [20/60] time 0.437 (0.439) data 0.305 (0.308) loss_u loss_u 0.9282 (0.8925) acc_u 12.5000 (14.5312) lr 4.2499e-04 eta 0:00:17
epoch [141/200] batch [25/60] time 0.493 (0.441) data 0.362 (0.310) loss_u loss_u 0.9741 (0.8916) acc_u 3.1250 (14.2500) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [30/60] time 0.458 (0.441) data 0.325 (0.310) loss_u loss_u 0.8120 (0.8833) acc_u 25.0000 (15.3125) lr 4.2499e-04 eta 0:00:13
epoch [141/200] batch [35/60] time 0.479 (0.442) data 0.347 (0.311) loss_u loss_u 0.8726 (0.8827) acc_u 6.2500 (15.3571) lr 4.2499e-04 eta 0:00:11
epoch [141/200] batch [40/60] time 0.467 (0.441) data 0.335 (0.310) loss_u loss_u 0.8223 (0.8802) acc_u 25.0000 (15.5469) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [45/60] time 0.428 (0.444) data 0.297 (0.313) loss_u loss_u 0.8701 (0.8807) acc_u 15.6250 (15.6250) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [50/60] time 0.603 (0.445) data 0.471 (0.313) loss_u loss_u 0.7974 (0.8778) acc_u 28.1250 (16.2500) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [55/60] time 0.453 (0.444) data 0.321 (0.313) loss_u loss_u 0.8042 (0.8772) acc_u 25.0000 (16.2500) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [60/60] time 0.457 (0.443) data 0.325 (0.312) loss_u loss_u 0.8447 (0.8741) acc_u 18.7500 (16.5104) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1399
confident_label rate tensor(0.3785, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1187
clean true:1181
clean false:6
clean_rate:0.9949452401010952
noisy true:556
noisy false:1393
after delete: len(clean_dataset) 1187
after delete: len(noisy_dataset) 1949
epoch [142/200] batch [5/37] time 0.505 (0.489) data 0.374 (0.358) loss_x loss_x 1.1064 (1.1370) acc_x 62.5000 (69.3750) lr 4.1221e-04 eta 0:00:15
epoch [142/200] batch [10/37] time 0.496 (0.484) data 0.365 (0.352) loss_x loss_x 0.9224 (0.9894) acc_x 68.7500 (73.4375) lr 4.1221e-04 eta 0:00:13
epoch [142/200] batch [15/37] time 0.421 (0.501) data 0.291 (0.370) loss_x loss_x 1.1191 (1.0025) acc_x 75.0000 (74.7917) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [20/37] time 0.352 (0.485) data 0.221 (0.354) loss_x loss_x 1.0684 (1.0224) acc_x 71.8750 (74.2188) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [25/37] time 0.406 (0.480) data 0.275 (0.349) loss_x loss_x 0.9868 (1.0041) acc_x 78.1250 (74.7500) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [30/37] time 0.502 (0.479) data 0.370 (0.348) loss_x loss_x 1.0410 (1.0334) acc_x 71.8750 (73.9583) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [35/37] time 0.521 (0.477) data 0.389 (0.346) loss_x loss_x 1.2891 (1.0542) acc_x 68.7500 (73.3036) lr 4.1221e-04 eta 0:00:00
epoch [142/200] batch [5/60] time 0.428 (0.470) data 0.297 (0.338) loss_u loss_u 0.9287 (0.8939) acc_u 9.3750 (14.3750) lr 4.1221e-04 eta 0:00:25
epoch [142/200] batch [10/60] time 0.436 (0.472) data 0.304 (0.341) loss_u loss_u 0.8770 (0.8862) acc_u 12.5000 (14.3750) lr 4.1221e-04 eta 0:00:23
epoch [142/200] batch [15/60] time 0.417 (0.474) data 0.285 (0.343) loss_u loss_u 0.9414 (0.8914) acc_u 3.1250 (13.5417) lr 4.1221e-04 eta 0:00:21
epoch [142/200] batch [20/60] time 0.428 (0.469) data 0.297 (0.338) loss_u loss_u 0.8325 (0.8806) acc_u 25.0000 (15.9375) lr 4.1221e-04 eta 0:00:18
epoch [142/200] batch [25/60] time 0.353 (0.462) data 0.221 (0.331) loss_u loss_u 0.8916 (0.8816) acc_u 15.6250 (16.0000) lr 4.1221e-04 eta 0:00:16
epoch [142/200] batch [30/60] time 0.553 (0.460) data 0.421 (0.328) loss_u loss_u 0.8354 (0.8745) acc_u 18.7500 (16.8750) lr 4.1221e-04 eta 0:00:13
epoch [142/200] batch [35/60] time 0.556 (0.463) data 0.424 (0.331) loss_u loss_u 0.8740 (0.8761) acc_u 12.5000 (16.2500) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [40/60] time 0.412 (0.462) data 0.281 (0.330) loss_u loss_u 0.9097 (0.8736) acc_u 9.3750 (16.4844) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [45/60] time 0.581 (0.461) data 0.449 (0.329) loss_u loss_u 0.8599 (0.8699) acc_u 18.7500 (16.6667) lr 4.1221e-04 eta 0:00:06
epoch [142/200] batch [50/60] time 0.446 (0.460) data 0.315 (0.329) loss_u loss_u 0.9365 (0.8697) acc_u 6.2500 (16.6250) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [55/60] time 0.568 (0.459) data 0.437 (0.328) loss_u loss_u 0.7812 (0.8698) acc_u 25.0000 (16.5909) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [60/60] time 0.392 (0.457) data 0.260 (0.326) loss_u loss_u 0.8462 (0.8704) acc_u 18.7500 (16.4062) lr 4.1221e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1397
confident_label rate tensor(0.3795, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1190
clean true:1185
clean false:5
clean_rate:0.9957983193277311
noisy true:554
noisy false:1392
after delete: len(clean_dataset) 1190
after delete: len(noisy_dataset) 1946
epoch [143/200] batch [5/37] time 0.495 (0.488) data 0.363 (0.357) loss_x loss_x 1.3867 (1.1644) acc_x 65.6250 (70.0000) lr 3.9958e-04 eta 0:00:15
epoch [143/200] batch [10/37] time 0.458 (0.461) data 0.327 (0.330) loss_x loss_x 1.4980 (1.1956) acc_x 65.6250 (68.7500) lr 3.9958e-04 eta 0:00:12
epoch [143/200] batch [15/37] time 0.436 (0.476) data 0.305 (0.346) loss_x loss_x 1.0615 (1.1833) acc_x 75.0000 (70.6250) lr 3.9958e-04 eta 0:00:10
epoch [143/200] batch [20/37] time 0.336 (0.468) data 0.205 (0.337) loss_x loss_x 0.8721 (1.1593) acc_x 87.5000 (71.8750) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [25/37] time 0.375 (0.461) data 0.244 (0.330) loss_x loss_x 1.2432 (1.1638) acc_x 71.8750 (71.6250) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [30/37] time 0.425 (0.468) data 0.294 (0.337) loss_x loss_x 1.4453 (1.1685) acc_x 62.5000 (71.7708) lr 3.9958e-04 eta 0:00:03
epoch [143/200] batch [35/37] time 0.414 (0.465) data 0.283 (0.334) loss_x loss_x 0.9507 (1.1693) acc_x 75.0000 (71.6071) lr 3.9958e-04 eta 0:00:00
epoch [143/200] batch [5/60] time 0.455 (0.463) data 0.323 (0.332) loss_u loss_u 0.8022 (0.8734) acc_u 21.8750 (16.8750) lr 3.9958e-04 eta 0:00:25
epoch [143/200] batch [10/60] time 0.457 (0.457) data 0.325 (0.325) loss_u loss_u 0.7793 (0.8604) acc_u 25.0000 (18.4375) lr 3.9958e-04 eta 0:00:22
epoch [143/200] batch [15/60] time 0.391 (0.454) data 0.260 (0.323) loss_u loss_u 0.8672 (0.8702) acc_u 21.8750 (17.2917) lr 3.9958e-04 eta 0:00:20
epoch [143/200] batch [20/60] time 0.556 (0.459) data 0.424 (0.327) loss_u loss_u 0.8735 (0.8698) acc_u 12.5000 (17.0312) lr 3.9958e-04 eta 0:00:18
epoch [143/200] batch [25/60] time 0.560 (0.458) data 0.428 (0.327) loss_u loss_u 0.8984 (0.8696) acc_u 6.2500 (16.8750) lr 3.9958e-04 eta 0:00:16
epoch [143/200] batch [30/60] time 0.426 (0.460) data 0.294 (0.329) loss_u loss_u 0.8599 (0.8690) acc_u 21.8750 (17.0833) lr 3.9958e-04 eta 0:00:13
epoch [143/200] batch [35/60] time 0.496 (0.458) data 0.366 (0.326) loss_u loss_u 0.8066 (0.8675) acc_u 25.0000 (17.5000) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [40/60] time 0.443 (0.456) data 0.312 (0.324) loss_u loss_u 0.9536 (0.8682) acc_u 6.2500 (17.3438) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [45/60] time 0.431 (0.457) data 0.300 (0.326) loss_u loss_u 0.8828 (0.8692) acc_u 12.5000 (17.1528) lr 3.9958e-04 eta 0:00:06
epoch [143/200] batch [50/60] time 0.340 (0.454) data 0.208 (0.323) loss_u loss_u 0.7217 (0.8673) acc_u 34.3750 (17.5625) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [55/60] time 0.404 (0.450) data 0.272 (0.319) loss_u loss_u 0.8037 (0.8661) acc_u 28.1250 (17.7273) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [60/60] time 0.431 (0.449) data 0.299 (0.317) loss_u loss_u 0.9038 (0.8651) acc_u 12.5000 (17.9688) lr 3.9958e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1423
confident_label rate tensor(0.3795, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1190
clean true:1184
clean false:6
clean_rate:0.9949579831932773
noisy true:529
noisy false:1417
after delete: len(clean_dataset) 1190
after delete: len(noisy_dataset) 1946
epoch [144/200] batch [5/37] time 0.341 (0.467) data 0.210 (0.336) loss_x loss_x 0.9985 (1.0017) acc_x 71.8750 (73.1250) lr 3.8709e-04 eta 0:00:14
epoch [144/200] batch [10/37] time 0.477 (0.483) data 0.346 (0.353) loss_x loss_x 1.3525 (1.0363) acc_x 65.6250 (72.1875) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [15/37] time 0.383 (0.461) data 0.252 (0.330) loss_x loss_x 0.9692 (0.9725) acc_x 78.1250 (75.6250) lr 3.8709e-04 eta 0:00:10
epoch [144/200] batch [20/37] time 0.374 (0.451) data 0.243 (0.320) loss_x loss_x 0.7485 (1.0067) acc_x 78.1250 (74.5312) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [25/37] time 0.519 (0.445) data 0.388 (0.314) loss_x loss_x 1.1992 (1.0271) acc_x 62.5000 (73.1250) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [30/37] time 0.530 (0.463) data 0.400 (0.333) loss_x loss_x 1.0664 (1.0463) acc_x 75.0000 (73.4375) lr 3.8709e-04 eta 0:00:03
epoch [144/200] batch [35/37] time 0.423 (0.465) data 0.291 (0.334) loss_x loss_x 1.4521 (1.0725) acc_x 56.2500 (72.5000) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/60] time 0.391 (0.463) data 0.260 (0.332) loss_u loss_u 0.7969 (0.8498) acc_u 28.1250 (20.0000) lr 3.8709e-04 eta 0:00:25
epoch [144/200] batch [10/60] time 0.445 (0.457) data 0.313 (0.326) loss_u loss_u 0.8203 (0.8438) acc_u 25.0000 (20.6250) lr 3.8709e-04 eta 0:00:22
epoch [144/200] batch [15/60] time 0.446 (0.460) data 0.315 (0.329) loss_u loss_u 0.9072 (0.8576) acc_u 12.5000 (18.5417) lr 3.8709e-04 eta 0:00:20
epoch [144/200] batch [20/60] time 0.473 (0.457) data 0.341 (0.326) loss_u loss_u 0.8120 (0.8599) acc_u 21.8750 (17.9688) lr 3.8709e-04 eta 0:00:18
epoch [144/200] batch [25/60] time 0.502 (0.458) data 0.370 (0.327) loss_u loss_u 0.8647 (0.8638) acc_u 15.6250 (17.6250) lr 3.8709e-04 eta 0:00:16
epoch [144/200] batch [30/60] time 0.334 (0.454) data 0.203 (0.323) loss_u loss_u 0.8809 (0.8598) acc_u 12.5000 (17.7083) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [35/60] time 0.338 (0.448) data 0.207 (0.317) loss_u loss_u 0.8916 (0.8622) acc_u 15.6250 (17.5000) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [40/60] time 0.323 (0.450) data 0.191 (0.319) loss_u loss_u 0.9336 (0.8622) acc_u 6.2500 (17.4219) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [45/60] time 0.569 (0.457) data 0.437 (0.326) loss_u loss_u 0.8477 (0.8595) acc_u 18.7500 (17.7083) lr 3.8709e-04 eta 0:00:06
epoch [144/200] batch [50/60] time 0.378 (0.454) data 0.247 (0.323) loss_u loss_u 0.9141 (0.8617) acc_u 12.5000 (17.3125) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [55/60] time 0.349 (0.452) data 0.219 (0.321) loss_u loss_u 0.8877 (0.8634) acc_u 12.5000 (16.9886) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [60/60] time 0.469 (0.448) data 0.338 (0.317) loss_u loss_u 0.8892 (0.8644) acc_u 15.6250 (16.9271) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1370
confident_label rate tensor(0.3798, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1191
clean true:1183
clean false:8
clean_rate:0.9932829554995802
noisy true:583
noisy false:1362
after delete: len(clean_dataset) 1191
after delete: len(noisy_dataset) 1945
epoch [145/200] batch [5/37] time 0.530 (0.476) data 0.399 (0.345) loss_x loss_x 1.0566 (0.9592) acc_x 75.0000 (79.3750) lr 3.7476e-04 eta 0:00:15
epoch [145/200] batch [10/37] time 0.390 (0.450) data 0.260 (0.319) loss_x loss_x 0.5864 (1.0059) acc_x 87.5000 (77.1875) lr 3.7476e-04 eta 0:00:12
epoch [145/200] batch [15/37] time 0.400 (0.445) data 0.269 (0.315) loss_x loss_x 0.8472 (1.0318) acc_x 81.2500 (75.4167) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [20/37] time 0.472 (0.448) data 0.342 (0.318) loss_x loss_x 0.8540 (1.0263) acc_x 78.1250 (75.1562) lr 3.7476e-04 eta 0:00:07
epoch [145/200] batch [25/37] time 0.620 (0.455) data 0.489 (0.325) loss_x loss_x 1.2412 (1.0281) acc_x 75.0000 (75.0000) lr 3.7476e-04 eta 0:00:05
epoch [145/200] batch [30/37] time 0.468 (0.458) data 0.338 (0.328) loss_x loss_x 0.6836 (1.0625) acc_x 84.3750 (74.0625) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [35/37] time 0.409 (0.464) data 0.278 (0.333) loss_x loss_x 0.7466 (1.0394) acc_x 81.2500 (74.3750) lr 3.7476e-04 eta 0:00:00
epoch [145/200] batch [5/60] time 0.433 (0.464) data 0.301 (0.333) loss_u loss_u 0.8867 (0.9080) acc_u 15.6250 (14.3750) lr 3.7476e-04 eta 0:00:25
epoch [145/200] batch [10/60] time 0.487 (0.468) data 0.355 (0.337) loss_u loss_u 0.9175 (0.8870) acc_u 9.3750 (14.6875) lr 3.7476e-04 eta 0:00:23
epoch [145/200] batch [15/60] time 0.437 (0.463) data 0.305 (0.332) loss_u loss_u 0.8760 (0.8834) acc_u 15.6250 (15.4167) lr 3.7476e-04 eta 0:00:20
epoch [145/200] batch [20/60] time 0.371 (0.464) data 0.240 (0.333) loss_u loss_u 0.8784 (0.8870) acc_u 15.6250 (15.0000) lr 3.7476e-04 eta 0:00:18
epoch [145/200] batch [25/60] time 0.511 (0.458) data 0.380 (0.328) loss_u loss_u 0.8340 (0.8819) acc_u 21.8750 (15.7500) lr 3.7476e-04 eta 0:00:16
epoch [145/200] batch [30/60] time 0.383 (0.460) data 0.251 (0.329) loss_u loss_u 0.8672 (0.8790) acc_u 15.6250 (15.7292) lr 3.7476e-04 eta 0:00:13
epoch [145/200] batch [35/60] time 0.522 (0.460) data 0.390 (0.329) loss_u loss_u 0.9580 (0.8796) acc_u 3.1250 (15.6250) lr 3.7476e-04 eta 0:00:11
epoch [145/200] batch [40/60] time 0.472 (0.460) data 0.338 (0.329) loss_u loss_u 0.8232 (0.8753) acc_u 28.1250 (16.4062) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [45/60] time 0.398 (0.459) data 0.267 (0.327) loss_u loss_u 0.9019 (0.8736) acc_u 9.3750 (16.3889) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [50/60] time 0.348 (0.459) data 0.216 (0.327) loss_u loss_u 0.8799 (0.8721) acc_u 15.6250 (16.5000) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [55/60] time 0.324 (0.460) data 0.192 (0.329) loss_u loss_u 0.9507 (0.8735) acc_u 6.2500 (16.4773) lr 3.7476e-04 eta 0:00:02
epoch [145/200] batch [60/60] time 0.403 (0.458) data 0.272 (0.327) loss_u loss_u 0.9150 (0.8752) acc_u 9.3750 (16.0938) lr 3.7476e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1425
confident_label rate tensor(0.3830, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1201
clean true:1196
clean false:5
clean_rate:0.9958368026644463
noisy true:515
noisy false:1420
after delete: len(clean_dataset) 1201
after delete: len(noisy_dataset) 1935
epoch [146/200] batch [5/37] time 0.409 (0.456) data 0.278 (0.325) loss_x loss_x 1.1436 (0.9598) acc_x 71.8750 (77.5000) lr 3.6258e-04 eta 0:00:14
epoch [146/200] batch [10/37] time 0.527 (0.461) data 0.397 (0.331) loss_x loss_x 0.9585 (1.0070) acc_x 65.6250 (74.0625) lr 3.6258e-04 eta 0:00:12
epoch [146/200] batch [15/37] time 0.567 (0.455) data 0.437 (0.324) loss_x loss_x 1.1309 (1.0844) acc_x 75.0000 (74.3750) lr 3.6258e-04 eta 0:00:10
epoch [146/200] batch [20/37] time 0.572 (0.450) data 0.441 (0.319) loss_x loss_x 1.1660 (1.1500) acc_x 78.1250 (72.9688) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [25/37] time 0.472 (0.449) data 0.342 (0.318) loss_x loss_x 0.9658 (1.1823) acc_x 75.0000 (72.0000) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [30/37] time 0.525 (0.453) data 0.393 (0.323) loss_x loss_x 1.2666 (1.1918) acc_x 65.6250 (71.2500) lr 3.6258e-04 eta 0:00:03
epoch [146/200] batch [35/37] time 0.462 (0.449) data 0.331 (0.318) loss_x loss_x 1.5117 (1.1670) acc_x 65.6250 (71.5179) lr 3.6258e-04 eta 0:00:00
epoch [146/200] batch [5/60] time 0.384 (0.454) data 0.251 (0.323) loss_u loss_u 0.7983 (0.8239) acc_u 25.0000 (21.2500) lr 3.6258e-04 eta 0:00:24
epoch [146/200] batch [10/60] time 0.431 (0.452) data 0.300 (0.321) loss_u loss_u 0.9492 (0.8358) acc_u 6.2500 (20.0000) lr 3.6258e-04 eta 0:00:22
epoch [146/200] batch [15/60] time 0.437 (0.448) data 0.305 (0.317) loss_u loss_u 0.8223 (0.8424) acc_u 28.1250 (20.0000) lr 3.6258e-04 eta 0:00:20
epoch [146/200] batch [20/60] time 0.376 (0.447) data 0.245 (0.316) loss_u loss_u 0.8481 (0.8463) acc_u 18.7500 (19.6875) lr 3.6258e-04 eta 0:00:17
epoch [146/200] batch [25/60] time 0.401 (0.448) data 0.271 (0.317) loss_u loss_u 0.8057 (0.8477) acc_u 18.7500 (19.3750) lr 3.6258e-04 eta 0:00:15
epoch [146/200] batch [30/60] time 0.506 (0.446) data 0.375 (0.315) loss_u loss_u 0.8525 (0.8485) acc_u 21.8750 (19.2708) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [35/60] time 0.388 (0.444) data 0.256 (0.313) loss_u loss_u 0.7983 (0.8486) acc_u 25.0000 (19.1071) lr 3.6258e-04 eta 0:00:11
epoch [146/200] batch [40/60] time 0.426 (0.445) data 0.294 (0.314) loss_u loss_u 0.8252 (0.8497) acc_u 15.6250 (18.9062) lr 3.6258e-04 eta 0:00:08
epoch [146/200] batch [45/60] time 0.492 (0.443) data 0.360 (0.312) loss_u loss_u 0.9141 (0.8519) acc_u 12.5000 (18.6111) lr 3.6258e-04 eta 0:00:06
epoch [146/200] batch [50/60] time 0.389 (0.440) data 0.257 (0.308) loss_u loss_u 0.8765 (0.8551) acc_u 15.6250 (18.1250) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [55/60] time 0.418 (0.442) data 0.286 (0.311) loss_u loss_u 0.8750 (0.8571) acc_u 18.7500 (17.7841) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [60/60] time 0.528 (0.444) data 0.396 (0.313) loss_u loss_u 0.9263 (0.8625) acc_u 9.3750 (17.1875) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1405
confident_label rate tensor(0.3830, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1201
clean true:1196
clean false:5
clean_rate:0.9958368026644463
noisy true:535
noisy false:1400
after delete: len(clean_dataset) 1201
after delete: len(noisy_dataset) 1935
epoch [147/200] batch [5/37] time 0.382 (0.465) data 0.251 (0.335) loss_x loss_x 1.0322 (1.0265) acc_x 68.7500 (73.1250) lr 3.5055e-04 eta 0:00:14
epoch [147/200] batch [10/37] time 0.587 (0.540) data 0.453 (0.408) loss_x loss_x 0.9233 (1.0110) acc_x 71.8750 (72.1875) lr 3.5055e-04 eta 0:00:14
epoch [147/200] batch [15/37] time 0.444 (0.530) data 0.313 (0.398) loss_x loss_x 0.9282 (1.0382) acc_x 84.3750 (74.1667) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [20/37] time 0.322 (0.490) data 0.191 (0.359) loss_x loss_x 0.7231 (1.0333) acc_x 84.3750 (75.1562) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [25/37] time 0.491 (0.482) data 0.360 (0.351) loss_x loss_x 1.2324 (1.0200) acc_x 65.6250 (75.6250) lr 3.5055e-04 eta 0:00:05
epoch [147/200] batch [30/37] time 0.515 (0.477) data 0.385 (0.346) loss_x loss_x 0.8843 (1.0275) acc_x 75.0000 (75.3125) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [35/37] time 0.581 (0.480) data 0.450 (0.348) loss_x loss_x 0.8350 (1.0222) acc_x 71.8750 (74.8214) lr 3.5055e-04 eta 0:00:00
epoch [147/200] batch [5/60] time 0.380 (0.465) data 0.249 (0.334) loss_u loss_u 0.8545 (0.8820) acc_u 18.7500 (14.3750) lr 3.5055e-04 eta 0:00:25
epoch [147/200] batch [10/60] time 0.499 (0.460) data 0.366 (0.328) loss_u loss_u 0.7778 (0.8738) acc_u 25.0000 (15.6250) lr 3.5055e-04 eta 0:00:22
epoch [147/200] batch [15/60] time 0.344 (0.459) data 0.212 (0.327) loss_u loss_u 0.9561 (0.8765) acc_u 3.1250 (15.0000) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [20/60] time 0.465 (0.466) data 0.333 (0.334) loss_u loss_u 0.8799 (0.8747) acc_u 18.7500 (15.7812) lr 3.5055e-04 eta 0:00:18
epoch [147/200] batch [25/60] time 0.414 (0.462) data 0.283 (0.330) loss_u loss_u 0.8667 (0.8729) acc_u 18.7500 (16.3750) lr 3.5055e-04 eta 0:00:16
epoch [147/200] batch [30/60] time 0.361 (0.459) data 0.229 (0.327) loss_u loss_u 0.8076 (0.8714) acc_u 21.8750 (16.5625) lr 3.5055e-04 eta 0:00:13
epoch [147/200] batch [35/60] time 0.443 (0.463) data 0.312 (0.331) loss_u loss_u 0.8965 (0.8680) acc_u 12.5000 (16.6964) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [40/60] time 0.413 (0.458) data 0.281 (0.326) loss_u loss_u 0.8633 (0.8682) acc_u 12.5000 (16.5625) lr 3.5055e-04 eta 0:00:09
epoch [147/200] batch [45/60] time 0.362 (0.455) data 0.231 (0.324) loss_u loss_u 0.7466 (0.8653) acc_u 31.2500 (16.9444) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [50/60] time 0.508 (0.453) data 0.377 (0.322) loss_u loss_u 0.9351 (0.8671) acc_u 15.6250 (17.0625) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [55/60] time 0.464 (0.454) data 0.332 (0.322) loss_u loss_u 0.7812 (0.8677) acc_u 28.1250 (16.9318) lr 3.5055e-04 eta 0:00:02
epoch [147/200] batch [60/60] time 0.389 (0.454) data 0.257 (0.323) loss_u loss_u 0.9019 (0.8707) acc_u 9.3750 (16.4583) lr 3.5055e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1375
confident_label rate tensor(0.3849, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1207
clean true:1201
clean false:6
clean_rate:0.9950289975144988
noisy true:560
noisy false:1369
after delete: len(clean_dataset) 1207
after delete: len(noisy_dataset) 1929
epoch [148/200] batch [5/37] time 0.436 (0.425) data 0.306 (0.294) loss_x loss_x 0.7793 (1.0530) acc_x 87.5000 (75.6250) lr 3.3869e-04 eta 0:00:13
epoch [148/200] batch [10/37] time 0.424 (0.442) data 0.293 (0.311) loss_x loss_x 1.1387 (0.9757) acc_x 75.0000 (76.5625) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [15/37] time 0.475 (0.431) data 0.344 (0.300) loss_x loss_x 1.1953 (0.9753) acc_x 65.6250 (76.2500) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [20/37] time 0.418 (0.433) data 0.287 (0.302) loss_x loss_x 0.8052 (1.0436) acc_x 78.1250 (74.5312) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [25/37] time 0.372 (0.440) data 0.241 (0.309) loss_x loss_x 0.9277 (1.0403) acc_x 68.7500 (74.2500) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [30/37] time 0.467 (0.440) data 0.337 (0.309) loss_x loss_x 1.2246 (1.0830) acc_x 78.1250 (74.1667) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [35/37] time 0.466 (0.439) data 0.335 (0.308) loss_x loss_x 1.3838 (1.0904) acc_x 71.8750 (74.2857) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/60] time 0.364 (0.429) data 0.232 (0.298) loss_u loss_u 0.8525 (0.8889) acc_u 18.7500 (17.5000) lr 3.3869e-04 eta 0:00:23
epoch [148/200] batch [10/60] time 0.385 (0.431) data 0.254 (0.300) loss_u loss_u 0.8218 (0.8849) acc_u 18.7500 (16.5625) lr 3.3869e-04 eta 0:00:21
epoch [148/200] batch [15/60] time 0.395 (0.430) data 0.263 (0.299) loss_u loss_u 0.8413 (0.8796) acc_u 15.6250 (17.0833) lr 3.3869e-04 eta 0:00:19
epoch [148/200] batch [20/60] time 0.493 (0.436) data 0.361 (0.305) loss_u loss_u 0.8862 (0.8778) acc_u 9.3750 (16.4062) lr 3.3869e-04 eta 0:00:17
epoch [148/200] batch [25/60] time 0.365 (0.436) data 0.233 (0.305) loss_u loss_u 0.9297 (0.8787) acc_u 9.3750 (16.2500) lr 3.3869e-04 eta 0:00:15
epoch [148/200] batch [30/60] time 0.603 (0.446) data 0.470 (0.315) loss_u loss_u 0.8140 (0.8766) acc_u 25.0000 (16.5625) lr 3.3869e-04 eta 0:00:13
epoch [148/200] batch [35/60] time 0.447 (0.450) data 0.311 (0.318) loss_u loss_u 0.9180 (0.8798) acc_u 12.5000 (15.9821) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [40/60] time 0.362 (0.448) data 0.230 (0.316) loss_u loss_u 0.9258 (0.8861) acc_u 9.3750 (14.9219) lr 3.3869e-04 eta 0:00:08
epoch [148/200] batch [45/60] time 0.337 (0.447) data 0.206 (0.316) loss_u loss_u 0.8647 (0.8842) acc_u 25.0000 (15.2083) lr 3.3869e-04 eta 0:00:06
epoch [148/200] batch [50/60] time 0.511 (0.447) data 0.379 (0.315) loss_u loss_u 0.7393 (0.8773) acc_u 34.3750 (16.0625) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [55/60] time 0.458 (0.447) data 0.326 (0.316) loss_u loss_u 0.8735 (0.8778) acc_u 15.6250 (15.7386) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [60/60] time 0.413 (0.448) data 0.281 (0.316) loss_u loss_u 0.8481 (0.8741) acc_u 18.7500 (16.3542) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1377
confident_label rate tensor(0.3868, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1213
clean true:1209
clean false:4
clean_rate:0.9967023907666942
noisy true:550
noisy false:1373
after delete: len(clean_dataset) 1213
after delete: len(noisy_dataset) 1923
epoch [149/200] batch [5/37] time 0.461 (0.480) data 0.330 (0.349) loss_x loss_x 0.8706 (1.1933) acc_x 75.0000 (71.2500) lr 3.2699e-04 eta 0:00:15
epoch [149/200] batch [10/37] time 0.577 (0.485) data 0.446 (0.355) loss_x loss_x 1.0986 (1.1162) acc_x 68.7500 (71.5625) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [15/37] time 0.434 (0.471) data 0.302 (0.340) loss_x loss_x 0.8467 (1.0839) acc_x 84.3750 (72.2917) lr 3.2699e-04 eta 0:00:10
epoch [149/200] batch [20/37] time 0.476 (0.473) data 0.345 (0.342) loss_x loss_x 1.3291 (1.0889) acc_x 68.7500 (72.0312) lr 3.2699e-04 eta 0:00:08
epoch [149/200] batch [25/37] time 0.457 (0.476) data 0.324 (0.345) loss_x loss_x 1.4297 (1.1167) acc_x 62.5000 (71.5000) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [30/37] time 0.421 (0.472) data 0.290 (0.341) loss_x loss_x 0.8394 (1.1028) acc_x 71.8750 (71.5625) lr 3.2699e-04 eta 0:00:03
epoch [149/200] batch [35/37] time 0.370 (0.464) data 0.240 (0.333) loss_x loss_x 1.5381 (1.1320) acc_x 65.6250 (71.2500) lr 3.2699e-04 eta 0:00:00
epoch [149/200] batch [5/60] time 0.411 (0.459) data 0.279 (0.328) loss_u loss_u 0.8579 (0.8728) acc_u 15.6250 (13.7500) lr 3.2699e-04 eta 0:00:25
epoch [149/200] batch [10/60] time 0.410 (0.458) data 0.278 (0.326) loss_u loss_u 0.8779 (0.8775) acc_u 15.6250 (14.3750) lr 3.2699e-04 eta 0:00:22
epoch [149/200] batch [15/60] time 0.465 (0.453) data 0.333 (0.322) loss_u loss_u 0.8716 (0.8718) acc_u 15.6250 (14.7917) lr 3.2699e-04 eta 0:00:20
epoch [149/200] batch [20/60] time 0.335 (0.456) data 0.203 (0.325) loss_u loss_u 0.9214 (0.8723) acc_u 15.6250 (15.3125) lr 3.2699e-04 eta 0:00:18
epoch [149/200] batch [25/60] time 0.419 (0.453) data 0.287 (0.322) loss_u loss_u 0.8784 (0.8722) acc_u 15.6250 (16.0000) lr 3.2699e-04 eta 0:00:15
epoch [149/200] batch [30/60] time 0.496 (0.459) data 0.364 (0.327) loss_u loss_u 0.8853 (0.8752) acc_u 9.3750 (15.5208) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [35/60] time 0.394 (0.457) data 0.262 (0.326) loss_u loss_u 0.9028 (0.8809) acc_u 12.5000 (15.0000) lr 3.2699e-04 eta 0:00:11
epoch [149/200] batch [40/60] time 0.458 (0.455) data 0.325 (0.324) loss_u loss_u 0.9121 (0.8798) acc_u 12.5000 (15.0781) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [45/60] time 0.453 (0.452) data 0.320 (0.321) loss_u loss_u 0.9121 (0.8784) acc_u 9.3750 (15.1389) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [50/60] time 0.442 (0.452) data 0.310 (0.321) loss_u loss_u 0.9282 (0.8784) acc_u 12.5000 (15.1250) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [55/60] time 0.419 (0.450) data 0.287 (0.319) loss_u loss_u 0.7993 (0.8775) acc_u 28.1250 (15.5114) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [60/60] time 0.476 (0.450) data 0.345 (0.318) loss_u loss_u 0.8481 (0.8786) acc_u 18.7500 (15.4167) lr 3.2699e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1343
confident_label rate tensor(0.3925, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1231
clean true:1228
clean false:3
clean_rate:0.9975629569455727
noisy true:565
noisy false:1340
after delete: len(clean_dataset) 1231
after delete: len(noisy_dataset) 1905
epoch [150/200] batch [5/38] time 0.449 (0.415) data 0.319 (0.285) loss_x loss_x 0.7568 (0.9459) acc_x 81.2500 (77.5000) lr 3.1545e-04 eta 0:00:13
epoch [150/200] batch [10/38] time 0.538 (0.422) data 0.408 (0.292) loss_x loss_x 0.6279 (0.8461) acc_x 81.2500 (79.6875) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [15/38] time 0.331 (0.431) data 0.201 (0.301) loss_x loss_x 0.9961 (0.8629) acc_x 78.1250 (79.5833) lr 3.1545e-04 eta 0:00:09
epoch [150/200] batch [20/38] time 0.428 (0.431) data 0.298 (0.301) loss_x loss_x 1.2725 (0.9450) acc_x 65.6250 (77.3438) lr 3.1545e-04 eta 0:00:07
epoch [150/200] batch [25/38] time 0.363 (0.436) data 0.232 (0.306) loss_x loss_x 1.7324 (1.0138) acc_x 65.6250 (75.7500) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [30/38] time 0.373 (0.434) data 0.242 (0.304) loss_x loss_x 1.3164 (1.0253) acc_x 68.7500 (75.6250) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [35/38] time 0.400 (0.439) data 0.270 (0.308) loss_x loss_x 1.5898 (1.0405) acc_x 65.6250 (75.3571) lr 3.1545e-04 eta 0:00:01
epoch [150/200] batch [5/59] time 0.425 (0.441) data 0.294 (0.311) loss_u loss_u 0.8662 (0.8879) acc_u 12.5000 (12.5000) lr 3.1545e-04 eta 0:00:23
epoch [150/200] batch [10/59] time 0.354 (0.439) data 0.223 (0.308) loss_u loss_u 0.8892 (0.8711) acc_u 12.5000 (15.0000) lr 3.1545e-04 eta 0:00:21
epoch [150/200] batch [15/59] time 0.363 (0.437) data 0.233 (0.306) loss_u loss_u 0.8086 (0.8672) acc_u 21.8750 (16.4583) lr 3.1545e-04 eta 0:00:19
epoch [150/200] batch [20/59] time 0.430 (0.443) data 0.296 (0.312) loss_u loss_u 0.7363 (0.8667) acc_u 37.5000 (17.1875) lr 3.1545e-04 eta 0:00:17
epoch [150/200] batch [25/59] time 0.551 (0.448) data 0.420 (0.317) loss_u loss_u 0.8984 (0.8650) acc_u 9.3750 (17.5000) lr 3.1545e-04 eta 0:00:15
epoch [150/200] batch [30/59] time 0.332 (0.444) data 0.200 (0.314) loss_u loss_u 0.8618 (0.8691) acc_u 15.6250 (16.7708) lr 3.1545e-04 eta 0:00:12
epoch [150/200] batch [35/59] time 0.414 (0.443) data 0.284 (0.313) loss_u loss_u 0.9170 (0.8736) acc_u 9.3750 (16.2500) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [40/59] time 0.371 (0.442) data 0.240 (0.311) loss_u loss_u 0.8755 (0.8728) acc_u 15.6250 (16.4062) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [45/59] time 0.424 (0.445) data 0.294 (0.315) loss_u loss_u 0.8018 (0.8725) acc_u 25.0000 (16.5278) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [50/59] time 0.425 (0.444) data 0.294 (0.314) loss_u loss_u 0.8818 (0.8741) acc_u 18.7500 (16.6250) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [55/59] time 0.572 (0.444) data 0.440 (0.313) loss_u loss_u 0.8877 (0.8752) acc_u 15.6250 (16.4773) lr 3.1545e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1359
confident_label rate tensor(0.3919, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1229
clean true:1225
clean false:4
clean_rate:0.9967453213995118
noisy true:552
noisy false:1355
after delete: len(clean_dataset) 1229
after delete: len(noisy_dataset) 1907
epoch [151/200] batch [5/38] time 0.431 (0.438) data 0.300 (0.307) loss_x loss_x 1.4023 (1.2657) acc_x 71.8750 (68.7500) lr 3.0409e-04 eta 0:00:14
epoch [151/200] batch [10/38] time 0.453 (0.435) data 0.322 (0.304) loss_x loss_x 0.7422 (1.0309) acc_x 75.0000 (75.3125) lr 3.0409e-04 eta 0:00:12
epoch [151/200] batch [15/38] time 0.507 (0.440) data 0.375 (0.309) loss_x loss_x 1.1523 (1.0889) acc_x 65.6250 (73.9583) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [20/38] time 0.369 (0.428) data 0.238 (0.297) loss_x loss_x 1.6416 (1.1194) acc_x 59.3750 (73.4375) lr 3.0409e-04 eta 0:00:07
epoch [151/200] batch [25/38] time 0.574 (0.437) data 0.442 (0.306) loss_x loss_x 1.1660 (1.1431) acc_x 78.1250 (72.7500) lr 3.0409e-04 eta 0:00:05
epoch [151/200] batch [30/38] time 0.402 (0.438) data 0.271 (0.307) loss_x loss_x 0.9126 (1.1008) acc_x 75.0000 (73.6458) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [35/38] time 0.839 (0.453) data 0.708 (0.322) loss_x loss_x 1.4756 (1.0861) acc_x 62.5000 (73.7500) lr 3.0409e-04 eta 0:00:01
epoch [151/200] batch [5/59] time 0.392 (0.450) data 0.260 (0.319) loss_u loss_u 0.8267 (0.8444) acc_u 21.8750 (18.7500) lr 3.0409e-04 eta 0:00:24
epoch [151/200] batch [10/59] time 0.447 (0.449) data 0.316 (0.318) loss_u loss_u 0.8452 (0.8532) acc_u 21.8750 (18.4375) lr 3.0409e-04 eta 0:00:22
epoch [151/200] batch [15/59] time 0.375 (0.457) data 0.243 (0.326) loss_u loss_u 0.8496 (0.8631) acc_u 21.8750 (17.7083) lr 3.0409e-04 eta 0:00:20
epoch [151/200] batch [20/59] time 0.407 (0.454) data 0.275 (0.323) loss_u loss_u 0.9043 (0.8696) acc_u 9.3750 (16.4062) lr 3.0409e-04 eta 0:00:17
epoch [151/200] batch [25/59] time 0.351 (0.450) data 0.220 (0.318) loss_u loss_u 0.8516 (0.8654) acc_u 15.6250 (16.8750) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [30/59] time 0.481 (0.449) data 0.349 (0.318) loss_u loss_u 0.9297 (0.8742) acc_u 6.2500 (15.9375) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [35/59] time 0.406 (0.451) data 0.274 (0.320) loss_u loss_u 0.8574 (0.8691) acc_u 21.8750 (16.8750) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [40/59] time 0.406 (0.451) data 0.275 (0.320) loss_u loss_u 0.8452 (0.8636) acc_u 18.7500 (17.4219) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [45/59] time 0.312 (0.449) data 0.180 (0.318) loss_u loss_u 0.8179 (0.8668) acc_u 28.1250 (17.0139) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [50/59] time 0.485 (0.448) data 0.354 (0.317) loss_u loss_u 0.8828 (0.8686) acc_u 18.7500 (16.8125) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [55/59] time 0.439 (0.447) data 0.307 (0.316) loss_u loss_u 0.9385 (0.8716) acc_u 6.2500 (16.4773) lr 3.0409e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1411
confident_label rate tensor(0.3731, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1170
clean true:1161
clean false:9
clean_rate:0.9923076923076923
noisy true:564
noisy false:1402
after delete: len(clean_dataset) 1170
after delete: len(noisy_dataset) 1966
epoch [152/200] batch [5/36] time 0.364 (0.430) data 0.234 (0.300) loss_x loss_x 1.6104 (0.9768) acc_x 59.3750 (73.7500) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [10/36] time 0.497 (0.423) data 0.366 (0.293) loss_x loss_x 1.1777 (1.0195) acc_x 65.6250 (72.8125) lr 2.9289e-04 eta 0:00:11
epoch [152/200] batch [15/36] time 0.529 (0.439) data 0.398 (0.308) loss_x loss_x 0.8491 (1.0388) acc_x 75.0000 (73.7500) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [20/36] time 0.508 (0.443) data 0.378 (0.312) loss_x loss_x 1.1396 (1.0172) acc_x 75.0000 (74.5312) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [25/36] time 0.416 (0.442) data 0.286 (0.312) loss_x loss_x 1.0459 (1.0228) acc_x 62.5000 (73.7500) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [30/36] time 0.347 (0.446) data 0.217 (0.316) loss_x loss_x 1.0977 (1.0385) acc_x 78.1250 (74.2708) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [35/36] time 0.600 (0.451) data 0.470 (0.320) loss_x loss_x 0.7192 (1.0255) acc_x 84.3750 (74.7321) lr 2.9289e-04 eta 0:00:00
epoch [152/200] batch [5/61] time 0.363 (0.447) data 0.231 (0.316) loss_u loss_u 0.8379 (0.8836) acc_u 21.8750 (13.7500) lr 2.9289e-04 eta 0:00:25
epoch [152/200] batch [10/61] time 0.453 (0.441) data 0.322 (0.310) loss_u loss_u 0.8267 (0.8759) acc_u 21.8750 (14.3750) lr 2.9289e-04 eta 0:00:22
epoch [152/200] batch [15/61] time 0.388 (0.438) data 0.257 (0.308) loss_u loss_u 0.8979 (0.8663) acc_u 15.6250 (15.8333) lr 2.9289e-04 eta 0:00:20
epoch [152/200] batch [20/61] time 0.472 (0.439) data 0.341 (0.308) loss_u loss_u 0.8462 (0.8577) acc_u 18.7500 (17.3438) lr 2.9289e-04 eta 0:00:18
epoch [152/200] batch [25/61] time 0.445 (0.443) data 0.314 (0.312) loss_u loss_u 0.9160 (0.8625) acc_u 9.3750 (16.7500) lr 2.9289e-04 eta 0:00:15
epoch [152/200] batch [30/61] time 0.402 (0.444) data 0.271 (0.313) loss_u loss_u 0.9360 (0.8556) acc_u 12.5000 (18.2292) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [35/61] time 0.522 (0.442) data 0.391 (0.311) loss_u loss_u 0.8447 (0.8521) acc_u 21.8750 (18.6607) lr 2.9289e-04 eta 0:00:11
epoch [152/200] batch [40/61] time 0.343 (0.440) data 0.211 (0.309) loss_u loss_u 0.8428 (0.8555) acc_u 21.8750 (18.5156) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [45/61] time 0.399 (0.439) data 0.268 (0.307) loss_u loss_u 0.9121 (0.8621) acc_u 12.5000 (17.7083) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [50/61] time 0.406 (0.440) data 0.274 (0.309) loss_u loss_u 0.7915 (0.8629) acc_u 31.2500 (17.6250) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [55/61] time 0.393 (0.439) data 0.261 (0.308) loss_u loss_u 0.9434 (0.8627) acc_u 6.2500 (17.5568) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [60/61] time 0.372 (0.441) data 0.240 (0.309) loss_u loss_u 0.9038 (0.8653) acc_u 9.3750 (17.1875) lr 2.9289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1405
confident_label rate tensor(0.3753, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1177
clean true:1173
clean false:4
clean_rate:0.9966015293118097
noisy true:558
noisy false:1401
after delete: len(clean_dataset) 1177
after delete: len(noisy_dataset) 1959
epoch [153/200] batch [5/36] time 0.477 (0.438) data 0.347 (0.308) loss_x loss_x 1.1250 (1.0783) acc_x 71.8750 (72.5000) lr 2.8187e-04 eta 0:00:13
epoch [153/200] batch [10/36] time 0.538 (0.435) data 0.407 (0.305) loss_x loss_x 0.8716 (1.0432) acc_x 75.0000 (72.1875) lr 2.8187e-04 eta 0:00:11
epoch [153/200] batch [15/36] time 0.500 (0.450) data 0.368 (0.319) loss_x loss_x 0.9326 (1.0290) acc_x 71.8750 (73.1250) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [20/36] time 0.453 (0.489) data 0.321 (0.358) loss_x loss_x 1.0918 (1.0315) acc_x 71.8750 (72.6562) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [25/36] time 0.427 (0.491) data 0.297 (0.360) loss_x loss_x 0.9756 (1.0126) acc_x 75.0000 (73.6250) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [30/36] time 0.486 (0.481) data 0.355 (0.351) loss_x loss_x 1.4482 (1.0816) acc_x 71.8750 (72.5000) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [35/36] time 0.454 (0.466) data 0.323 (0.336) loss_x loss_x 1.4746 (1.0852) acc_x 65.6250 (72.4107) lr 2.8187e-04 eta 0:00:00
epoch [153/200] batch [5/61] time 0.382 (0.462) data 0.250 (0.331) loss_u loss_u 0.9131 (0.8595) acc_u 6.2500 (18.1250) lr 2.8187e-04 eta 0:00:25
epoch [153/200] batch [10/61] time 0.578 (0.461) data 0.446 (0.330) loss_u loss_u 0.8027 (0.8700) acc_u 21.8750 (16.5625) lr 2.8187e-04 eta 0:00:23
epoch [153/200] batch [15/61] time 0.444 (0.454) data 0.312 (0.323) loss_u loss_u 0.8711 (0.8569) acc_u 15.6250 (18.5417) lr 2.8187e-04 eta 0:00:20
epoch [153/200] batch [20/61] time 0.460 (0.451) data 0.328 (0.320) loss_u loss_u 0.8740 (0.8572) acc_u 12.5000 (18.4375) lr 2.8187e-04 eta 0:00:18
epoch [153/200] batch [25/61] time 0.510 (0.449) data 0.379 (0.318) loss_u loss_u 0.7983 (0.8554) acc_u 18.7500 (18.5000) lr 2.8187e-04 eta 0:00:16
epoch [153/200] batch [30/61] time 0.508 (0.449) data 0.377 (0.317) loss_u loss_u 0.8965 (0.8574) acc_u 15.6250 (18.2292) lr 2.8187e-04 eta 0:00:13
epoch [153/200] batch [35/61] time 0.400 (0.449) data 0.268 (0.318) loss_u loss_u 0.7280 (0.8554) acc_u 37.5000 (18.3036) lr 2.8187e-04 eta 0:00:11
epoch [153/200] batch [40/61] time 0.410 (0.447) data 0.279 (0.316) loss_u loss_u 0.8931 (0.8603) acc_u 12.5000 (17.5000) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [45/61] time 0.373 (0.445) data 0.242 (0.314) loss_u loss_u 0.8701 (0.8639) acc_u 15.6250 (17.0139) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [50/61] time 0.351 (0.445) data 0.219 (0.314) loss_u loss_u 0.8364 (0.8629) acc_u 25.0000 (17.1250) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [55/61] time 0.388 (0.445) data 0.256 (0.314) loss_u loss_u 0.8398 (0.8619) acc_u 21.8750 (17.3295) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [60/61] time 0.343 (0.444) data 0.211 (0.313) loss_u loss_u 0.8198 (0.8629) acc_u 25.0000 (17.2396) lr 2.8187e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1369
confident_label rate tensor(0.3839, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1204
clean true:1199
clean false:5
clean_rate:0.9958471760797342
noisy true:568
noisy false:1364
after delete: len(clean_dataset) 1204
after delete: len(noisy_dataset) 1932
epoch [154/200] batch [5/37] time 0.405 (0.432) data 0.274 (0.301) loss_x loss_x 1.5459 (1.2094) acc_x 62.5000 (71.8750) lr 2.7103e-04 eta 0:00:13
epoch [154/200] batch [10/37] time 0.451 (0.454) data 0.321 (0.323) loss_x loss_x 1.8125 (1.2884) acc_x 59.3750 (68.4375) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [15/37] time 0.562 (0.466) data 0.429 (0.335) loss_x loss_x 1.2061 (1.2022) acc_x 65.6250 (70.4167) lr 2.7103e-04 eta 0:00:10
epoch [154/200] batch [20/37] time 0.347 (0.455) data 0.216 (0.324) loss_x loss_x 1.0605 (1.1893) acc_x 71.8750 (70.9375) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [25/37] time 0.517 (0.455) data 0.386 (0.323) loss_x loss_x 0.6538 (1.1611) acc_x 81.2500 (71.1250) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [30/37] time 0.461 (0.454) data 0.330 (0.323) loss_x loss_x 1.1211 (1.1222) acc_x 78.1250 (72.9167) lr 2.7103e-04 eta 0:00:03
epoch [154/200] batch [35/37] time 0.332 (0.450) data 0.201 (0.319) loss_x loss_x 1.9736 (1.1516) acc_x 59.3750 (72.3214) lr 2.7103e-04 eta 0:00:00
epoch [154/200] batch [5/60] time 0.451 (0.448) data 0.319 (0.317) loss_u loss_u 0.8877 (0.9090) acc_u 18.7500 (11.8750) lr 2.7103e-04 eta 0:00:24
epoch [154/200] batch [10/60] time 0.589 (0.455) data 0.457 (0.324) loss_u loss_u 0.9990 (0.8976) acc_u 0.0000 (13.1250) lr 2.7103e-04 eta 0:00:22
epoch [154/200] batch [15/60] time 0.422 (0.452) data 0.290 (0.321) loss_u loss_u 0.8936 (0.9006) acc_u 9.3750 (12.7083) lr 2.7103e-04 eta 0:00:20
epoch [154/200] batch [20/60] time 0.394 (0.450) data 0.262 (0.318) loss_u loss_u 0.9038 (0.8956) acc_u 12.5000 (13.1250) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [25/60] time 0.551 (0.447) data 0.419 (0.316) loss_u loss_u 0.8740 (0.8894) acc_u 18.7500 (13.6250) lr 2.7103e-04 eta 0:00:15
epoch [154/200] batch [30/60] time 0.466 (0.443) data 0.335 (0.312) loss_u loss_u 0.8638 (0.8802) acc_u 12.5000 (15.0000) lr 2.7103e-04 eta 0:00:13
epoch [154/200] batch [35/60] time 0.418 (0.440) data 0.286 (0.309) loss_u loss_u 0.8262 (0.8770) acc_u 25.0000 (15.7143) lr 2.7103e-04 eta 0:00:10
epoch [154/200] batch [40/60] time 0.348 (0.437) data 0.216 (0.305) loss_u loss_u 0.9072 (0.8776) acc_u 12.5000 (15.7031) lr 2.7103e-04 eta 0:00:08
epoch [154/200] batch [45/60] time 0.404 (0.436) data 0.272 (0.305) loss_u loss_u 0.8398 (0.8785) acc_u 18.7500 (15.4861) lr 2.7103e-04 eta 0:00:06
epoch [154/200] batch [50/60] time 0.531 (0.441) data 0.399 (0.310) loss_u loss_u 0.7891 (0.8767) acc_u 31.2500 (15.6250) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [55/60] time 0.323 (0.440) data 0.192 (0.309) loss_u loss_u 0.8066 (0.8731) acc_u 25.0000 (16.1364) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [60/60] time 0.387 (0.440) data 0.256 (0.309) loss_u loss_u 0.7725 (0.8694) acc_u 31.2500 (16.8229) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1400
confident_label rate tensor(0.3827, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1200
clean true:1195
clean false:5
clean_rate:0.9958333333333333
noisy true:541
noisy false:1395
after delete: len(clean_dataset) 1200
after delete: len(noisy_dataset) 1936
epoch [155/200] batch [5/37] time 0.432 (0.424) data 0.301 (0.293) loss_x loss_x 0.9541 (1.2557) acc_x 81.2500 (67.5000) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [10/37] time 0.421 (0.446) data 0.290 (0.314) loss_x loss_x 1.1816 (1.2277) acc_x 71.8750 (68.4375) lr 2.6037e-04 eta 0:00:12
epoch [155/200] batch [15/37] time 0.370 (0.444) data 0.239 (0.313) loss_x loss_x 0.9556 (1.2238) acc_x 71.8750 (68.7500) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [20/37] time 0.512 (0.449) data 0.382 (0.318) loss_x loss_x 0.9678 (1.1932) acc_x 71.8750 (69.8438) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [25/37] time 0.453 (0.452) data 0.321 (0.321) loss_x loss_x 1.3213 (1.1699) acc_x 65.6250 (70.7500) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [30/37] time 0.531 (0.466) data 0.401 (0.335) loss_x loss_x 0.8071 (1.1609) acc_x 75.0000 (70.7292) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [35/37] time 0.631 (0.473) data 0.500 (0.342) loss_x loss_x 0.9199 (1.1309) acc_x 78.1250 (71.4286) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/60] time 0.460 (0.465) data 0.329 (0.334) loss_u loss_u 0.9302 (0.8850) acc_u 6.2500 (16.2500) lr 2.6037e-04 eta 0:00:25
epoch [155/200] batch [10/60] time 0.384 (0.457) data 0.253 (0.326) loss_u loss_u 0.8921 (0.8612) acc_u 12.5000 (18.7500) lr 2.6037e-04 eta 0:00:22
epoch [155/200] batch [15/60] time 0.396 (0.460) data 0.265 (0.329) loss_u loss_u 0.8926 (0.8632) acc_u 12.5000 (17.7083) lr 2.6037e-04 eta 0:00:20
epoch [155/200] batch [20/60] time 0.439 (0.461) data 0.306 (0.330) loss_u loss_u 0.8267 (0.8676) acc_u 25.0000 (16.8750) lr 2.6037e-04 eta 0:00:18
epoch [155/200] batch [25/60] time 0.560 (0.461) data 0.428 (0.330) loss_u loss_u 0.8901 (0.8711) acc_u 9.3750 (15.7500) lr 2.6037e-04 eta 0:00:16
epoch [155/200] batch [30/60] time 0.413 (0.461) data 0.282 (0.330) loss_u loss_u 0.8628 (0.8764) acc_u 15.6250 (15.0000) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [35/60] time 0.492 (0.461) data 0.361 (0.330) loss_u loss_u 0.7998 (0.8744) acc_u 21.8750 (15.1786) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [40/60] time 0.463 (0.458) data 0.331 (0.326) loss_u loss_u 0.8120 (0.8739) acc_u 28.1250 (15.2344) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [45/60] time 0.516 (0.456) data 0.384 (0.325) loss_u loss_u 0.8643 (0.8732) acc_u 21.8750 (15.4861) lr 2.6037e-04 eta 0:00:06
epoch [155/200] batch [50/60] time 0.422 (0.460) data 0.290 (0.328) loss_u loss_u 0.8325 (0.8715) acc_u 25.0000 (15.8750) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [55/60] time 0.384 (0.458) data 0.252 (0.326) loss_u loss_u 0.7974 (0.8694) acc_u 25.0000 (16.1932) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [60/60] time 0.364 (0.455) data 0.233 (0.323) loss_u loss_u 0.8389 (0.8684) acc_u 21.8750 (16.3542) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1396
confident_label rate tensor(0.3836, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1203
clean true:1199
clean false:4
clean_rate:0.9966749792186201
noisy true:541
noisy false:1392
after delete: len(clean_dataset) 1203
after delete: len(noisy_dataset) 1933
epoch [156/200] batch [5/37] time 0.505 (0.458) data 0.374 (0.327) loss_x loss_x 1.0850 (1.1518) acc_x 65.6250 (71.2500) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [10/37] time 0.452 (0.459) data 0.321 (0.328) loss_x loss_x 1.0352 (1.1576) acc_x 78.1250 (73.1250) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [15/37] time 0.662 (0.468) data 0.532 (0.337) loss_x loss_x 0.7485 (1.0701) acc_x 81.2500 (74.7917) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [20/37] time 0.463 (0.479) data 0.332 (0.348) loss_x loss_x 1.1045 (1.0271) acc_x 71.8750 (75.3125) lr 2.4989e-04 eta 0:00:08
epoch [156/200] batch [25/37] time 0.425 (0.488) data 0.295 (0.356) loss_x loss_x 1.0127 (1.0431) acc_x 71.8750 (74.8750) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [30/37] time 0.450 (0.477) data 0.320 (0.346) loss_x loss_x 1.0010 (1.0576) acc_x 71.8750 (73.8542) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [35/37] time 0.348 (0.472) data 0.217 (0.341) loss_x loss_x 0.8828 (1.0429) acc_x 78.1250 (74.4643) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/60] time 0.399 (0.468) data 0.267 (0.337) loss_u loss_u 0.8823 (0.8886) acc_u 15.6250 (13.7500) lr 2.4989e-04 eta 0:00:25
epoch [156/200] batch [10/60] time 0.413 (0.467) data 0.281 (0.336) loss_u loss_u 0.8667 (0.8805) acc_u 18.7500 (15.0000) lr 2.4989e-04 eta 0:00:23
epoch [156/200] batch [15/60] time 0.473 (0.474) data 0.342 (0.343) loss_u loss_u 0.9019 (0.8781) acc_u 6.2500 (14.3750) lr 2.4989e-04 eta 0:00:21
epoch [156/200] batch [20/60] time 0.443 (0.467) data 0.312 (0.336) loss_u loss_u 0.9072 (0.8841) acc_u 12.5000 (14.0625) lr 2.4989e-04 eta 0:00:18
epoch [156/200] batch [25/60] time 0.396 (0.464) data 0.266 (0.333) loss_u loss_u 0.8594 (0.8818) acc_u 15.6250 (14.5000) lr 2.4989e-04 eta 0:00:16
epoch [156/200] batch [30/60] time 0.364 (0.457) data 0.232 (0.326) loss_u loss_u 0.8760 (0.8784) acc_u 15.6250 (14.6875) lr 2.4989e-04 eta 0:00:13
epoch [156/200] batch [35/60] time 0.422 (0.455) data 0.292 (0.324) loss_u loss_u 0.7925 (0.8695) acc_u 25.0000 (15.9821) lr 2.4989e-04 eta 0:00:11
epoch [156/200] batch [40/60] time 0.413 (0.453) data 0.282 (0.322) loss_u loss_u 0.8311 (0.8691) acc_u 18.7500 (15.7812) lr 2.4989e-04 eta 0:00:09
epoch [156/200] batch [45/60] time 0.430 (0.453) data 0.299 (0.322) loss_u loss_u 0.8828 (0.8692) acc_u 12.5000 (15.9028) lr 2.4989e-04 eta 0:00:06
epoch [156/200] batch [50/60] time 0.591 (0.455) data 0.459 (0.324) loss_u loss_u 0.8628 (0.8700) acc_u 18.7500 (15.8750) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [55/60] time 0.436 (0.454) data 0.305 (0.322) loss_u loss_u 0.8467 (0.8717) acc_u 21.8750 (15.7386) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [60/60] time 0.382 (0.452) data 0.251 (0.321) loss_u loss_u 0.8438 (0.8714) acc_u 21.8750 (15.8333) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1389
confident_label rate tensor(0.3817, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1197
clean true:1189
clean false:8
clean_rate:0.9933166248955723
noisy true:558
noisy false:1381
after delete: len(clean_dataset) 1197
after delete: len(noisy_dataset) 1939
epoch [157/200] batch [5/37] time 0.561 (0.478) data 0.430 (0.347) loss_x loss_x 1.0078 (1.3992) acc_x 84.3750 (72.5000) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [10/37] time 0.436 (0.447) data 0.306 (0.316) loss_x loss_x 1.1172 (1.1476) acc_x 75.0000 (75.6250) lr 2.3959e-04 eta 0:00:12
epoch [157/200] batch [15/37] time 0.543 (0.450) data 0.413 (0.319) loss_x loss_x 1.0518 (1.0838) acc_x 78.1250 (75.8333) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [20/37] time 0.365 (0.445) data 0.235 (0.315) loss_x loss_x 0.8887 (1.0988) acc_x 68.7500 (74.2188) lr 2.3959e-04 eta 0:00:07
epoch [157/200] batch [25/37] time 0.591 (0.443) data 0.461 (0.313) loss_x loss_x 1.3457 (1.1365) acc_x 75.0000 (73.3750) lr 2.3959e-04 eta 0:00:05
epoch [157/200] batch [30/37] time 0.489 (0.460) data 0.358 (0.329) loss_x loss_x 0.9604 (1.1060) acc_x 68.7500 (73.6458) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [35/37] time 0.451 (0.454) data 0.321 (0.324) loss_x loss_x 1.1904 (1.0826) acc_x 71.8750 (74.2857) lr 2.3959e-04 eta 0:00:00
epoch [157/200] batch [5/60] time 0.423 (0.449) data 0.291 (0.318) loss_u loss_u 0.8096 (0.8383) acc_u 21.8750 (21.8750) lr 2.3959e-04 eta 0:00:24
epoch [157/200] batch [10/60] time 0.395 (0.446) data 0.263 (0.316) loss_u loss_u 0.8252 (0.8489) acc_u 21.8750 (19.6875) lr 2.3959e-04 eta 0:00:22
epoch [157/200] batch [15/60] time 0.450 (0.444) data 0.320 (0.313) loss_u loss_u 0.8555 (0.8529) acc_u 18.7500 (18.7500) lr 2.3959e-04 eta 0:00:19
epoch [157/200] batch [20/60] time 0.425 (0.446) data 0.293 (0.315) loss_u loss_u 0.8110 (0.8542) acc_u 18.7500 (18.4375) lr 2.3959e-04 eta 0:00:17
epoch [157/200] batch [25/60] time 0.349 (0.444) data 0.218 (0.313) loss_u loss_u 0.8940 (0.8573) acc_u 18.7500 (18.0000) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [30/60] time 0.421 (0.443) data 0.289 (0.312) loss_u loss_u 0.9126 (0.8616) acc_u 9.3750 (17.5000) lr 2.3959e-04 eta 0:00:13
epoch [157/200] batch [35/60] time 0.453 (0.444) data 0.323 (0.313) loss_u loss_u 0.8843 (0.8645) acc_u 12.5000 (17.1429) lr 2.3959e-04 eta 0:00:11
epoch [157/200] batch [40/60] time 0.369 (0.447) data 0.239 (0.316) loss_u loss_u 0.9087 (0.8633) acc_u 12.5000 (17.2656) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [45/60] time 0.444 (0.447) data 0.313 (0.316) loss_u loss_u 0.8818 (0.8648) acc_u 12.5000 (17.1528) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [50/60] time 0.611 (0.447) data 0.481 (0.316) loss_u loss_u 0.7979 (0.8637) acc_u 25.0000 (17.5625) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [55/60] time 0.535 (0.446) data 0.404 (0.315) loss_u loss_u 0.8242 (0.8640) acc_u 25.0000 (17.8409) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [60/60] time 0.388 (0.446) data 0.256 (0.315) loss_u loss_u 0.9014 (0.8651) acc_u 9.3750 (17.6042) lr 2.3959e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1323
confident_label rate tensor(0.3938, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1235
clean true:1230
clean false:5
clean_rate:0.9959514170040485
noisy true:583
noisy false:1318
after delete: len(clean_dataset) 1235
after delete: len(noisy_dataset) 1901
epoch [158/200] batch [5/38] time 0.423 (0.452) data 0.292 (0.321) loss_x loss_x 1.1914 (1.0814) acc_x 78.1250 (76.2500) lr 2.2949e-04 eta 0:00:14
epoch [158/200] batch [10/38] time 0.386 (0.437) data 0.255 (0.306) loss_x loss_x 1.2695 (1.1839) acc_x 65.6250 (70.3125) lr 2.2949e-04 eta 0:00:12
epoch [158/200] batch [15/38] time 0.424 (0.430) data 0.291 (0.299) loss_x loss_x 1.0420 (1.1034) acc_x 65.6250 (72.2917) lr 2.2949e-04 eta 0:00:09
epoch [158/200] batch [20/38] time 0.595 (0.450) data 0.464 (0.318) loss_x loss_x 0.6323 (1.1040) acc_x 84.3750 (72.1875) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [25/38] time 0.504 (0.454) data 0.373 (0.323) loss_x loss_x 0.8560 (1.0705) acc_x 78.1250 (72.3750) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [30/38] time 0.584 (0.467) data 0.452 (0.336) loss_x loss_x 1.4609 (1.0924) acc_x 65.6250 (72.3958) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [35/38] time 0.484 (0.465) data 0.352 (0.334) loss_x loss_x 1.0137 (1.0824) acc_x 75.0000 (72.4107) lr 2.2949e-04 eta 0:00:01
epoch [158/200] batch [5/59] time 0.535 (0.467) data 0.403 (0.336) loss_u loss_u 0.8760 (0.8630) acc_u 15.6250 (17.5000) lr 2.2949e-04 eta 0:00:25
epoch [158/200] batch [10/59] time 0.438 (0.466) data 0.307 (0.335) loss_u loss_u 0.8594 (0.8757) acc_u 18.7500 (16.2500) lr 2.2949e-04 eta 0:00:22
epoch [158/200] batch [15/59] time 0.496 (0.462) data 0.364 (0.331) loss_u loss_u 0.9170 (0.8833) acc_u 12.5000 (15.8333) lr 2.2949e-04 eta 0:00:20
epoch [158/200] batch [20/59] time 0.475 (0.459) data 0.344 (0.328) loss_u loss_u 0.8389 (0.8797) acc_u 18.7500 (15.9375) lr 2.2949e-04 eta 0:00:17
epoch [158/200] batch [25/59] time 0.452 (0.456) data 0.320 (0.325) loss_u loss_u 0.8418 (0.8809) acc_u 25.0000 (15.7500) lr 2.2949e-04 eta 0:00:15
epoch [158/200] batch [30/59] time 0.464 (0.458) data 0.333 (0.326) loss_u loss_u 0.9189 (0.8831) acc_u 12.5000 (15.5208) lr 2.2949e-04 eta 0:00:13
epoch [158/200] batch [35/59] time 0.410 (0.457) data 0.278 (0.325) loss_u loss_u 0.9097 (0.8846) acc_u 15.6250 (15.3571) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [40/59] time 0.367 (0.453) data 0.235 (0.321) loss_u loss_u 0.9209 (0.8827) acc_u 9.3750 (15.7031) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [45/59] time 0.590 (0.453) data 0.459 (0.321) loss_u loss_u 0.8525 (0.8786) acc_u 21.8750 (16.0417) lr 2.2949e-04 eta 0:00:06
epoch [158/200] batch [50/59] time 0.309 (0.449) data 0.178 (0.317) loss_u loss_u 0.9395 (0.8750) acc_u 6.2500 (16.4375) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [55/59] time 0.492 (0.448) data 0.360 (0.317) loss_u loss_u 0.8667 (0.8762) acc_u 15.6250 (16.2500) lr 2.2949e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1385
confident_label rate tensor(0.3801, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1192
clean true:1188
clean false:4
clean_rate:0.9966442953020134
noisy true:563
noisy false:1381
after delete: len(clean_dataset) 1192
after delete: len(noisy_dataset) 1944
epoch [159/200] batch [5/37] time 0.520 (0.444) data 0.389 (0.313) loss_x loss_x 0.7739 (1.0312) acc_x 81.2500 (75.0000) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [10/37] time 0.415 (0.444) data 0.284 (0.313) loss_x loss_x 0.8135 (0.9448) acc_x 90.6250 (77.8125) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [15/37] time 0.462 (0.445) data 0.332 (0.314) loss_x loss_x 0.9888 (0.9913) acc_x 71.8750 (75.4167) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [20/37] time 0.356 (0.456) data 0.225 (0.325) loss_x loss_x 1.7578 (1.0814) acc_x 62.5000 (72.3438) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [25/37] time 0.390 (0.456) data 0.259 (0.326) loss_x loss_x 1.0938 (1.0614) acc_x 78.1250 (73.0000) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [30/37] time 0.483 (0.458) data 0.352 (0.327) loss_x loss_x 1.2871 (1.0728) acc_x 68.7500 (72.3958) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [35/37] time 0.447 (0.453) data 0.317 (0.323) loss_x loss_x 0.7769 (1.0481) acc_x 78.1250 (72.7679) lr 2.1957e-04 eta 0:00:00
epoch [159/200] batch [5/60] time 0.484 (0.453) data 0.352 (0.322) loss_u loss_u 0.9165 (0.9017) acc_u 6.2500 (11.8750) lr 2.1957e-04 eta 0:00:24
epoch [159/200] batch [10/60] time 0.377 (0.451) data 0.245 (0.320) loss_u loss_u 0.8398 (0.8905) acc_u 25.0000 (13.7500) lr 2.1957e-04 eta 0:00:22
epoch [159/200] batch [15/60] time 0.468 (0.448) data 0.336 (0.317) loss_u loss_u 0.8232 (0.8777) acc_u 25.0000 (15.6250) lr 2.1957e-04 eta 0:00:20
epoch [159/200] batch [20/60] time 0.382 (0.445) data 0.250 (0.314) loss_u loss_u 0.8032 (0.8681) acc_u 28.1250 (17.5000) lr 2.1957e-04 eta 0:00:17
epoch [159/200] batch [25/60] time 0.455 (0.444) data 0.323 (0.313) loss_u loss_u 0.8511 (0.8673) acc_u 15.6250 (17.0000) lr 2.1957e-04 eta 0:00:15
epoch [159/200] batch [30/60] time 0.366 (0.442) data 0.235 (0.311) loss_u loss_u 0.8735 (0.8685) acc_u 15.6250 (16.7708) lr 2.1957e-04 eta 0:00:13
epoch [159/200] batch [35/60] time 0.383 (0.442) data 0.252 (0.311) loss_u loss_u 0.8809 (0.8646) acc_u 12.5000 (17.2321) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [40/60] time 0.411 (0.444) data 0.279 (0.313) loss_u loss_u 0.9199 (0.8675) acc_u 9.3750 (16.8750) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [45/60] time 0.484 (0.443) data 0.353 (0.312) loss_u loss_u 0.8892 (0.8711) acc_u 18.7500 (16.5278) lr 2.1957e-04 eta 0:00:06
epoch [159/200] batch [50/60] time 0.336 (0.443) data 0.205 (0.311) loss_u loss_u 0.8149 (0.8722) acc_u 21.8750 (16.3125) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [55/60] time 0.470 (0.443) data 0.338 (0.312) loss_u loss_u 0.8599 (0.8725) acc_u 15.6250 (16.0795) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [60/60] time 0.463 (0.441) data 0.333 (0.310) loss_u loss_u 0.7217 (0.8715) acc_u 31.2500 (15.9896) lr 2.1957e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1356
confident_label rate tensor(0.3884, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1218
clean true:1213
clean false:5
clean_rate:0.9958949096880131
noisy true:567
noisy false:1351
after delete: len(clean_dataset) 1218
after delete: len(noisy_dataset) 1918
epoch [160/200] batch [5/38] time 0.519 (0.495) data 0.389 (0.365) loss_x loss_x 1.6299 (1.1791) acc_x 65.6250 (69.3750) lr 2.0984e-04 eta 0:00:16
epoch [160/200] batch [10/38] time 0.535 (0.471) data 0.404 (0.340) loss_x loss_x 1.3428 (1.1644) acc_x 71.8750 (72.8125) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [15/38] time 0.459 (0.474) data 0.328 (0.343) loss_x loss_x 0.6924 (1.0748) acc_x 84.3750 (74.1667) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [20/38] time 0.465 (0.478) data 0.334 (0.348) loss_x loss_x 0.9683 (1.0631) acc_x 65.6250 (73.5938) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [25/38] time 0.403 (0.468) data 0.272 (0.337) loss_x loss_x 0.7388 (1.0739) acc_x 75.0000 (72.6250) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [30/38] time 0.407 (0.456) data 0.276 (0.326) loss_x loss_x 1.5117 (1.1134) acc_x 68.7500 (71.8750) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [35/38] time 0.434 (0.458) data 0.303 (0.328) loss_x loss_x 1.2412 (1.0779) acc_x 68.7500 (73.2143) lr 2.0984e-04 eta 0:00:01
epoch [160/200] batch [5/59] time 0.379 (0.454) data 0.247 (0.323) loss_u loss_u 0.9111 (0.8915) acc_u 12.5000 (13.7500) lr 2.0984e-04 eta 0:00:24
epoch [160/200] batch [10/59] time 0.432 (0.450) data 0.300 (0.320) loss_u loss_u 0.8579 (0.8705) acc_u 18.7500 (16.5625) lr 2.0984e-04 eta 0:00:22
epoch [160/200] batch [15/59] time 0.534 (0.451) data 0.402 (0.320) loss_u loss_u 0.9023 (0.8711) acc_u 9.3750 (15.8333) lr 2.0984e-04 eta 0:00:19
epoch [160/200] batch [20/59] time 0.347 (0.447) data 0.216 (0.316) loss_u loss_u 0.8970 (0.8663) acc_u 12.5000 (15.9375) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [25/59] time 0.403 (0.448) data 0.272 (0.317) loss_u loss_u 0.7920 (0.8659) acc_u 21.8750 (15.8750) lr 2.0984e-04 eta 0:00:15
epoch [160/200] batch [30/59] time 0.517 (0.451) data 0.385 (0.320) loss_u loss_u 0.9312 (0.8688) acc_u 9.3750 (15.9375) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [35/59] time 0.395 (0.448) data 0.264 (0.317) loss_u loss_u 0.9258 (0.8714) acc_u 9.3750 (15.8929) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [40/59] time 0.526 (0.449) data 0.394 (0.317) loss_u loss_u 0.9092 (0.8737) acc_u 15.6250 (15.4688) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [45/59] time 0.478 (0.445) data 0.346 (0.314) loss_u loss_u 0.8853 (0.8717) acc_u 15.6250 (15.9028) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [50/59] time 0.479 (0.450) data 0.346 (0.318) loss_u loss_u 0.9155 (0.8725) acc_u 12.5000 (15.6875) lr 2.0984e-04 eta 0:00:04
epoch [160/200] batch [55/59] time 0.425 (0.449) data 0.293 (0.317) loss_u loss_u 0.7988 (0.8711) acc_u 31.2500 (15.9091) lr 2.0984e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1368
confident_label rate tensor(0.3913, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1227
clean true:1219
clean false:8
clean_rate:0.993480032599837
noisy true:549
noisy false:1360
after delete: len(clean_dataset) 1227
after delete: len(noisy_dataset) 1909
epoch [161/200] batch [5/38] time 0.425 (0.495) data 0.296 (0.365) loss_x loss_x 1.4805 (1.1827) acc_x 65.6250 (71.2500) lr 2.0032e-04 eta 0:00:16
epoch [161/200] batch [10/38] time 0.428 (0.482) data 0.298 (0.351) loss_x loss_x 0.9600 (1.0964) acc_x 71.8750 (73.1250) lr 2.0032e-04 eta 0:00:13
epoch [161/200] batch [15/38] time 0.476 (0.477) data 0.346 (0.346) loss_x loss_x 1.0635 (1.0367) acc_x 71.8750 (75.0000) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [20/38] time 0.497 (0.473) data 0.367 (0.343) loss_x loss_x 0.7402 (1.0276) acc_x 84.3750 (74.8438) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [25/38] time 0.546 (0.468) data 0.415 (0.338) loss_x loss_x 1.2158 (1.0410) acc_x 68.7500 (75.2500) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [30/38] time 0.372 (0.457) data 0.242 (0.327) loss_x loss_x 0.9985 (1.0429) acc_x 84.3750 (75.4167) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [35/38] time 0.408 (0.457) data 0.277 (0.327) loss_x loss_x 0.9873 (1.0423) acc_x 75.0000 (75.5357) lr 2.0032e-04 eta 0:00:01
epoch [161/200] batch [5/59] time 0.365 (0.450) data 0.235 (0.320) loss_u loss_u 0.8770 (0.8569) acc_u 18.7500 (18.7500) lr 2.0032e-04 eta 0:00:24
epoch [161/200] batch [10/59] time 0.566 (0.457) data 0.435 (0.326) loss_u loss_u 0.8315 (0.8615) acc_u 18.7500 (17.8125) lr 2.0032e-04 eta 0:00:22
epoch [161/200] batch [15/59] time 0.528 (0.457) data 0.396 (0.327) loss_u loss_u 0.9077 (0.8728) acc_u 12.5000 (16.2500) lr 2.0032e-04 eta 0:00:20
epoch [161/200] batch [20/59] time 0.369 (0.454) data 0.237 (0.323) loss_u loss_u 0.7520 (0.8715) acc_u 34.3750 (16.4062) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [25/59] time 0.453 (0.456) data 0.321 (0.325) loss_u loss_u 0.7896 (0.8658) acc_u 25.0000 (17.1250) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [30/59] time 0.393 (0.457) data 0.262 (0.327) loss_u loss_u 0.8950 (0.8653) acc_u 12.5000 (17.0833) lr 2.0032e-04 eta 0:00:13
epoch [161/200] batch [35/59] time 0.419 (0.453) data 0.288 (0.323) loss_u loss_u 0.8662 (0.8664) acc_u 18.7500 (16.8750) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [40/59] time 0.386 (0.451) data 0.253 (0.320) loss_u loss_u 0.9077 (0.8697) acc_u 15.6250 (16.6406) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [45/59] time 0.364 (0.450) data 0.233 (0.319) loss_u loss_u 0.8267 (0.8682) acc_u 18.7500 (16.5278) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [50/59] time 0.375 (0.451) data 0.243 (0.320) loss_u loss_u 0.8740 (0.8684) acc_u 15.6250 (16.5625) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [55/59] time 0.373 (0.448) data 0.242 (0.317) loss_u loss_u 0.7959 (0.8701) acc_u 28.1250 (16.5341) lr 2.0032e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.3740, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1173
clean true:1169
clean false:4
clean_rate:0.9965899403239556
noisy true:554
noisy false:1409
after delete: len(clean_dataset) 1173
after delete: len(noisy_dataset) 1963
epoch [162/200] batch [5/36] time 0.406 (0.496) data 0.276 (0.365) loss_x loss_x 1.2578 (1.1542) acc_x 68.7500 (72.5000) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [10/36] time 0.558 (0.481) data 0.428 (0.350) loss_x loss_x 1.2305 (1.1282) acc_x 65.6250 (73.1250) lr 1.9098e-04 eta 0:00:12
epoch [162/200] batch [15/36] time 0.431 (0.481) data 0.301 (0.351) loss_x loss_x 1.1230 (1.0984) acc_x 75.0000 (74.5833) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [20/36] time 0.407 (0.486) data 0.276 (0.355) loss_x loss_x 1.2344 (1.1014) acc_x 68.7500 (74.5312) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [25/36] time 0.548 (0.483) data 0.418 (0.352) loss_x loss_x 1.6084 (1.0963) acc_x 62.5000 (74.2500) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [30/36] time 0.429 (0.473) data 0.298 (0.342) loss_x loss_x 1.0889 (1.1524) acc_x 65.6250 (72.8125) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [35/36] time 0.379 (0.466) data 0.248 (0.335) loss_x loss_x 0.7979 (1.1274) acc_x 81.2500 (73.1250) lr 1.9098e-04 eta 0:00:00
epoch [162/200] batch [5/61] time 0.484 (0.462) data 0.353 (0.331) loss_u loss_u 0.9326 (0.8707) acc_u 15.6250 (16.8750) lr 1.9098e-04 eta 0:00:25
epoch [162/200] batch [10/61] time 0.513 (0.459) data 0.383 (0.328) loss_u loss_u 0.8579 (0.8587) acc_u 15.6250 (17.5000) lr 1.9098e-04 eta 0:00:23
epoch [162/200] batch [15/61] time 0.514 (0.460) data 0.382 (0.329) loss_u loss_u 0.9429 (0.8672) acc_u 6.2500 (16.4583) lr 1.9098e-04 eta 0:00:21
epoch [162/200] batch [20/61] time 0.418 (0.458) data 0.286 (0.327) loss_u loss_u 0.8145 (0.8665) acc_u 18.7500 (15.7812) lr 1.9098e-04 eta 0:00:18
epoch [162/200] batch [25/61] time 0.470 (0.459) data 0.338 (0.327) loss_u loss_u 0.9224 (0.8723) acc_u 9.3750 (15.1250) lr 1.9098e-04 eta 0:00:16
epoch [162/200] batch [30/61] time 0.391 (0.454) data 0.259 (0.323) loss_u loss_u 0.8052 (0.8624) acc_u 25.0000 (16.4583) lr 1.9098e-04 eta 0:00:14
epoch [162/200] batch [35/61] time 0.490 (0.456) data 0.358 (0.325) loss_u loss_u 0.8745 (0.8640) acc_u 9.3750 (16.0714) lr 1.9098e-04 eta 0:00:11
epoch [162/200] batch [40/61] time 0.389 (0.458) data 0.257 (0.326) loss_u loss_u 0.8828 (0.8640) acc_u 18.7500 (16.1719) lr 1.9098e-04 eta 0:00:09
epoch [162/200] batch [45/61] time 0.331 (0.454) data 0.199 (0.323) loss_u loss_u 0.8789 (0.8639) acc_u 18.7500 (16.4583) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [50/61] time 0.402 (0.451) data 0.270 (0.319) loss_u loss_u 0.8716 (0.8640) acc_u 18.7500 (16.3750) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [55/61] time 0.375 (0.449) data 0.244 (0.317) loss_u loss_u 0.8921 (0.8637) acc_u 15.6250 (16.5341) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [60/61] time 0.432 (0.448) data 0.300 (0.316) loss_u loss_u 0.8960 (0.8622) acc_u 15.6250 (16.8750) lr 1.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1338
confident_label rate tensor(0.3951, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1239
clean true:1231
clean false:8
clean_rate:0.993543179983858
noisy true:567
noisy false:1330
after delete: len(clean_dataset) 1239
after delete: len(noisy_dataset) 1897
epoch [163/200] batch [5/38] time 0.414 (0.456) data 0.283 (0.325) loss_x loss_x 1.3838 (1.2138) acc_x 65.6250 (71.8750) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [10/38] time 0.485 (0.465) data 0.354 (0.334) loss_x loss_x 1.2412 (1.0953) acc_x 75.0000 (75.9375) lr 1.8185e-04 eta 0:00:13
epoch [163/200] batch [15/38] time 0.412 (0.453) data 0.281 (0.322) loss_x loss_x 0.8398 (1.0868) acc_x 75.0000 (74.5833) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [20/38] time 0.406 (0.448) data 0.274 (0.317) loss_x loss_x 1.1074 (1.0979) acc_x 68.7500 (73.7500) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [25/38] time 0.407 (0.453) data 0.276 (0.322) loss_x loss_x 0.6978 (1.0867) acc_x 84.3750 (74.0000) lr 1.8185e-04 eta 0:00:05
epoch [163/200] batch [30/38] time 0.454 (0.460) data 0.323 (0.329) loss_x loss_x 1.2090 (1.0692) acc_x 65.6250 (74.3750) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [35/38] time 0.434 (0.454) data 0.303 (0.323) loss_x loss_x 0.9106 (1.0666) acc_x 81.2500 (74.4643) lr 1.8185e-04 eta 0:00:01
epoch [163/200] batch [5/59] time 0.461 (0.455) data 0.330 (0.324) loss_u loss_u 0.8979 (0.8572) acc_u 9.3750 (19.3750) lr 1.8185e-04 eta 0:00:24
epoch [163/200] batch [10/59] time 0.440 (0.451) data 0.308 (0.320) loss_u loss_u 0.9067 (0.8662) acc_u 9.3750 (16.5625) lr 1.8185e-04 eta 0:00:22
epoch [163/200] batch [15/59] time 0.392 (0.449) data 0.261 (0.318) loss_u loss_u 0.9092 (0.8610) acc_u 9.3750 (17.5000) lr 1.8185e-04 eta 0:00:19
epoch [163/200] batch [20/59] time 0.385 (0.443) data 0.253 (0.312) loss_u loss_u 0.8779 (0.8608) acc_u 18.7500 (17.8125) lr 1.8185e-04 eta 0:00:17
epoch [163/200] batch [25/59] time 0.443 (0.447) data 0.311 (0.315) loss_u loss_u 0.9375 (0.8650) acc_u 6.2500 (16.8750) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [30/59] time 0.547 (0.445) data 0.415 (0.314) loss_u loss_u 0.8970 (0.8670) acc_u 9.3750 (16.7708) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [35/59] time 0.485 (0.447) data 0.353 (0.315) loss_u loss_u 0.9321 (0.8698) acc_u 6.2500 (15.9821) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [40/59] time 0.429 (0.448) data 0.298 (0.316) loss_u loss_u 0.8848 (0.8738) acc_u 18.7500 (15.7812) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [45/59] time 0.404 (0.448) data 0.272 (0.316) loss_u loss_u 0.8867 (0.8722) acc_u 12.5000 (15.9722) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [50/59] time 0.439 (0.445) data 0.308 (0.313) loss_u loss_u 0.9141 (0.8749) acc_u 12.5000 (15.4375) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [55/59] time 0.583 (0.446) data 0.451 (0.314) loss_u loss_u 0.8735 (0.8722) acc_u 12.5000 (15.6818) lr 1.8185e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1370
confident_label rate tensor(0.3865, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1212
clean true:1206
clean false:6
clean_rate:0.995049504950495
noisy true:560
noisy false:1364
after delete: len(clean_dataset) 1212
after delete: len(noisy_dataset) 1924
epoch [164/200] batch [5/37] time 0.519 (0.453) data 0.388 (0.322) loss_x loss_x 1.3096 (1.1792) acc_x 65.6250 (72.5000) lr 1.7292e-04 eta 0:00:14
epoch [164/200] batch [10/37] time 0.364 (0.440) data 0.233 (0.309) loss_x loss_x 1.1992 (1.1342) acc_x 68.7500 (73.7500) lr 1.7292e-04 eta 0:00:11
epoch [164/200] batch [15/37] time 0.474 (0.449) data 0.343 (0.318) loss_x loss_x 1.0947 (1.0775) acc_x 71.8750 (75.0000) lr 1.7292e-04 eta 0:00:09
epoch [164/200] batch [20/37] time 0.423 (0.436) data 0.293 (0.305) loss_x loss_x 0.9780 (1.0955) acc_x 75.0000 (73.7500) lr 1.7292e-04 eta 0:00:07
epoch [164/200] batch [25/37] time 0.376 (0.435) data 0.245 (0.304) loss_x loss_x 1.2422 (1.0795) acc_x 68.7500 (73.8750) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [30/37] time 0.504 (0.434) data 0.373 (0.303) loss_x loss_x 0.9976 (1.0690) acc_x 75.0000 (74.0625) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [35/37] time 0.487 (0.435) data 0.356 (0.304) loss_x loss_x 0.8960 (1.1088) acc_x 81.2500 (73.5714) lr 1.7292e-04 eta 0:00:00
epoch [164/200] batch [5/60] time 0.363 (0.439) data 0.232 (0.308) loss_u loss_u 0.9531 (0.8685) acc_u 6.2500 (18.7500) lr 1.7292e-04 eta 0:00:24
epoch [164/200] batch [10/60] time 0.469 (0.443) data 0.337 (0.312) loss_u loss_u 0.8701 (0.8744) acc_u 18.7500 (17.5000) lr 1.7292e-04 eta 0:00:22
epoch [164/200] batch [15/60] time 0.440 (0.445) data 0.308 (0.314) loss_u loss_u 0.9360 (0.8857) acc_u 12.5000 (15.8333) lr 1.7292e-04 eta 0:00:20
epoch [164/200] batch [20/60] time 0.403 (0.446) data 0.272 (0.315) loss_u loss_u 0.8525 (0.8856) acc_u 15.6250 (14.8438) lr 1.7292e-04 eta 0:00:17
epoch [164/200] batch [25/60] time 0.364 (0.448) data 0.232 (0.316) loss_u loss_u 0.8921 (0.8847) acc_u 12.5000 (14.7500) lr 1.7292e-04 eta 0:00:15
epoch [164/200] batch [30/60] time 0.424 (0.451) data 0.293 (0.320) loss_u loss_u 0.8394 (0.8846) acc_u 21.8750 (15.2083) lr 1.7292e-04 eta 0:00:13
epoch [164/200] batch [35/60] time 0.444 (0.450) data 0.312 (0.319) loss_u loss_u 0.8486 (0.8807) acc_u 28.1250 (15.6250) lr 1.7292e-04 eta 0:00:11
epoch [164/200] batch [40/60] time 0.365 (0.446) data 0.234 (0.315) loss_u loss_u 0.8931 (0.8756) acc_u 15.6250 (16.1719) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [45/60] time 0.448 (0.444) data 0.316 (0.312) loss_u loss_u 0.8730 (0.8741) acc_u 15.6250 (16.1806) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [50/60] time 0.427 (0.441) data 0.295 (0.310) loss_u loss_u 0.9482 (0.8779) acc_u 12.5000 (15.8125) lr 1.7292e-04 eta 0:00:04
epoch [164/200] batch [55/60] time 0.403 (0.441) data 0.271 (0.310) loss_u loss_u 0.8081 (0.8760) acc_u 21.8750 (16.0795) lr 1.7292e-04 eta 0:00:02
epoch [164/200] batch [60/60] time 0.461 (0.441) data 0.327 (0.310) loss_u loss_u 0.8340 (0.8736) acc_u 18.7500 (16.1979) lr 1.7292e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1387
confident_label rate tensor(0.3817, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1197
clean true:1193
clean false:4
clean_rate:0.9966583124477861
noisy true:556
noisy false:1383
after delete: len(clean_dataset) 1197
after delete: len(noisy_dataset) 1939
epoch [165/200] batch [5/37] time 0.442 (0.498) data 0.311 (0.367) loss_x loss_x 1.3115 (0.8808) acc_x 62.5000 (74.3750) lr 1.6419e-04 eta 0:00:15
epoch [165/200] batch [10/37] time 0.650 (0.493) data 0.518 (0.362) loss_x loss_x 0.8564 (0.8514) acc_x 81.2500 (77.5000) lr 1.6419e-04 eta 0:00:13
epoch [165/200] batch [15/37] time 0.381 (0.484) data 0.251 (0.353) loss_x loss_x 1.3320 (1.0265) acc_x 68.7500 (72.5000) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [20/37] time 0.428 (0.478) data 0.297 (0.346) loss_x loss_x 0.9038 (0.9858) acc_x 71.8750 (73.7500) lr 1.6419e-04 eta 0:00:08
epoch [165/200] batch [25/37] time 0.489 (0.475) data 0.358 (0.343) loss_x loss_x 1.1689 (0.9929) acc_x 62.5000 (74.2500) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [30/37] time 0.357 (0.465) data 0.226 (0.334) loss_x loss_x 1.0322 (0.9874) acc_x 78.1250 (75.2083) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [35/37] time 0.625 (0.462) data 0.494 (0.331) loss_x loss_x 0.7339 (1.0048) acc_x 78.1250 (74.4643) lr 1.6419e-04 eta 0:00:00
epoch [165/200] batch [5/60] time 0.388 (0.462) data 0.256 (0.330) loss_u loss_u 0.8223 (0.8579) acc_u 25.0000 (17.5000) lr 1.6419e-04 eta 0:00:25
epoch [165/200] batch [10/60] time 0.483 (0.462) data 0.352 (0.330) loss_u loss_u 0.7837 (0.8560) acc_u 21.8750 (17.1875) lr 1.6419e-04 eta 0:00:23
epoch [165/200] batch [15/60] time 0.496 (0.463) data 0.366 (0.332) loss_u loss_u 0.9150 (0.8641) acc_u 9.3750 (16.2500) lr 1.6419e-04 eta 0:00:20
epoch [165/200] batch [20/60] time 0.398 (0.461) data 0.266 (0.330) loss_u loss_u 0.7773 (0.8588) acc_u 31.2500 (17.3438) lr 1.6419e-04 eta 0:00:18
epoch [165/200] batch [25/60] time 0.387 (0.460) data 0.255 (0.328) loss_u loss_u 0.9185 (0.8575) acc_u 6.2500 (17.6250) lr 1.6419e-04 eta 0:00:16
epoch [165/200] batch [30/60] time 0.357 (0.466) data 0.225 (0.334) loss_u loss_u 0.7759 (0.8549) acc_u 28.1250 (18.4375) lr 1.6419e-04 eta 0:00:13
epoch [165/200] batch [35/60] time 0.426 (0.465) data 0.295 (0.333) loss_u loss_u 0.8516 (0.8641) acc_u 25.0000 (17.4107) lr 1.6419e-04 eta 0:00:11
epoch [165/200] batch [40/60] time 0.333 (0.459) data 0.201 (0.327) loss_u loss_u 0.8955 (0.8656) acc_u 12.5000 (17.0312) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [45/60] time 0.399 (0.458) data 0.268 (0.327) loss_u loss_u 0.8394 (0.8614) acc_u 21.8750 (17.5694) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [50/60] time 0.453 (0.457) data 0.320 (0.326) loss_u loss_u 0.8770 (0.8626) acc_u 12.5000 (17.3125) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [55/60] time 0.527 (0.456) data 0.395 (0.325) loss_u loss_u 0.7944 (0.8636) acc_u 28.1250 (17.2727) lr 1.6419e-04 eta 0:00:02
epoch [165/200] batch [60/60] time 0.378 (0.453) data 0.246 (0.321) loss_u loss_u 0.9092 (0.8635) acc_u 6.2500 (17.2396) lr 1.6419e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1364
confident_label rate tensor(0.3922, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1230
clean true:1226
clean false:4
clean_rate:0.9967479674796748
noisy true:546
noisy false:1360
after delete: len(clean_dataset) 1230
after delete: len(noisy_dataset) 1906
epoch [166/200] batch [5/38] time 0.375 (0.438) data 0.244 (0.307) loss_x loss_x 0.7915 (1.0312) acc_x 81.2500 (72.5000) lr 1.5567e-04 eta 0:00:14
epoch [166/200] batch [10/38] time 0.442 (0.431) data 0.311 (0.300) loss_x loss_x 1.7012 (1.1291) acc_x 56.2500 (71.5625) lr 1.5567e-04 eta 0:00:12
epoch [166/200] batch [15/38] time 0.436 (0.449) data 0.305 (0.319) loss_x loss_x 1.9287 (1.2014) acc_x 53.1250 (70.2083) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [20/38] time 0.375 (0.456) data 0.244 (0.325) loss_x loss_x 1.1582 (1.1269) acc_x 68.7500 (72.1875) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [25/38] time 0.370 (0.450) data 0.239 (0.319) loss_x loss_x 0.7749 (1.1041) acc_x 68.7500 (72.2500) lr 1.5567e-04 eta 0:00:05
epoch [166/200] batch [30/38] time 0.445 (0.456) data 0.314 (0.325) loss_x loss_x 0.9644 (1.1033) acc_x 71.8750 (71.4583) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [35/38] time 0.539 (0.463) data 0.408 (0.332) loss_x loss_x 0.6494 (1.0941) acc_x 87.5000 (72.1429) lr 1.5567e-04 eta 0:00:01
epoch [166/200] batch [5/59] time 0.484 (0.461) data 0.353 (0.330) loss_u loss_u 0.7573 (0.8588) acc_u 34.3750 (18.1250) lr 1.5567e-04 eta 0:00:24
epoch [166/200] batch [10/59] time 0.442 (0.457) data 0.311 (0.326) loss_u loss_u 0.8013 (0.8592) acc_u 28.1250 (18.1250) lr 1.5567e-04 eta 0:00:22
epoch [166/200] batch [15/59] time 0.468 (0.460) data 0.337 (0.329) loss_u loss_u 0.8745 (0.8682) acc_u 12.5000 (16.2500) lr 1.5567e-04 eta 0:00:20
epoch [166/200] batch [20/59] time 0.382 (0.456) data 0.248 (0.325) loss_u loss_u 0.9072 (0.8732) acc_u 15.6250 (15.7812) lr 1.5567e-04 eta 0:00:17
epoch [166/200] batch [25/59] time 0.431 (0.458) data 0.300 (0.327) loss_u loss_u 0.8882 (0.8807) acc_u 15.6250 (14.7500) lr 1.5567e-04 eta 0:00:15
epoch [166/200] batch [30/59] time 0.618 (0.461) data 0.484 (0.329) loss_u loss_u 0.9209 (0.8818) acc_u 12.5000 (14.7917) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [35/59] time 0.405 (0.459) data 0.272 (0.328) loss_u loss_u 0.8838 (0.8798) acc_u 15.6250 (15.1786) lr 1.5567e-04 eta 0:00:11
epoch [166/200] batch [40/59] time 0.446 (0.457) data 0.314 (0.326) loss_u loss_u 0.9185 (0.8785) acc_u 9.3750 (15.3125) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [45/59] time 0.425 (0.461) data 0.293 (0.330) loss_u loss_u 0.8096 (0.8760) acc_u 25.0000 (15.7639) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [50/59] time 0.364 (0.457) data 0.232 (0.326) loss_u loss_u 0.8735 (0.8748) acc_u 21.8750 (16.1250) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [55/59] time 0.410 (0.454) data 0.279 (0.322) loss_u loss_u 0.9375 (0.8752) acc_u 9.3750 (15.9659) lr 1.5567e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1409
confident_label rate tensor(0.3782, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1186
clean true:1180
clean false:6
clean_rate:0.9949409780775716
noisy true:547
noisy false:1403
after delete: len(clean_dataset) 1186
after delete: len(noisy_dataset) 1950
epoch [167/200] batch [5/37] time 0.583 (0.449) data 0.450 (0.317) loss_x loss_x 0.7256 (1.1189) acc_x 87.5000 (74.3750) lr 1.4736e-04 eta 0:00:14
epoch [167/200] batch [10/37] time 0.423 (0.448) data 0.292 (0.316) loss_x loss_x 0.7773 (1.0501) acc_x 78.1250 (74.6875) lr 1.4736e-04 eta 0:00:12
epoch [167/200] batch [15/37] time 0.435 (0.440) data 0.304 (0.308) loss_x loss_x 0.4741 (1.0285) acc_x 90.6250 (75.4167) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [20/37] time 0.639 (0.457) data 0.508 (0.326) loss_x loss_x 0.6592 (0.9909) acc_x 84.3750 (76.2500) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [25/37] time 0.409 (0.447) data 0.278 (0.315) loss_x loss_x 0.8145 (0.9942) acc_x 84.3750 (76.1250) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [30/37] time 0.558 (0.466) data 0.427 (0.334) loss_x loss_x 0.5205 (1.0054) acc_x 93.7500 (76.0417) lr 1.4736e-04 eta 0:00:03
epoch [167/200] batch [35/37] time 0.349 (0.465) data 0.218 (0.333) loss_x loss_x 1.1377 (1.0123) acc_x 71.8750 (75.4464) lr 1.4736e-04 eta 0:00:00
epoch [167/200] batch [5/60] time 0.388 (0.458) data 0.256 (0.327) loss_u loss_u 0.8726 (0.8868) acc_u 15.6250 (15.6250) lr 1.4736e-04 eta 0:00:25
epoch [167/200] batch [10/60] time 0.736 (0.461) data 0.604 (0.329) loss_u loss_u 0.8726 (0.8860) acc_u 15.6250 (14.3750) lr 1.4736e-04 eta 0:00:23
epoch [167/200] batch [15/60] time 0.435 (0.459) data 0.303 (0.327) loss_u loss_u 0.8555 (0.8688) acc_u 18.7500 (16.8750) lr 1.4736e-04 eta 0:00:20
epoch [167/200] batch [20/60] time 0.381 (0.454) data 0.250 (0.323) loss_u loss_u 0.8735 (0.8649) acc_u 18.7500 (17.1875) lr 1.4736e-04 eta 0:00:18
epoch [167/200] batch [25/60] time 0.424 (0.452) data 0.292 (0.320) loss_u loss_u 0.8047 (0.8567) acc_u 21.8750 (17.7500) lr 1.4736e-04 eta 0:00:15
epoch [167/200] batch [30/60] time 0.420 (0.450) data 0.288 (0.318) loss_u loss_u 0.8423 (0.8584) acc_u 25.0000 (17.7083) lr 1.4736e-04 eta 0:00:13
epoch [167/200] batch [35/60] time 0.437 (0.450) data 0.305 (0.318) loss_u loss_u 0.9292 (0.8558) acc_u 6.2500 (18.2143) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [40/60] time 0.596 (0.452) data 0.465 (0.320) loss_u loss_u 0.8364 (0.8584) acc_u 18.7500 (17.8906) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [45/60] time 0.619 (0.453) data 0.487 (0.321) loss_u loss_u 0.8828 (0.8632) acc_u 18.7500 (17.5694) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [50/60] time 0.394 (0.454) data 0.263 (0.323) loss_u loss_u 0.8672 (0.8641) acc_u 18.7500 (17.3125) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [55/60] time 0.546 (0.453) data 0.416 (0.321) loss_u loss_u 0.8101 (0.8632) acc_u 21.8750 (17.5568) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [60/60] time 0.508 (0.453) data 0.376 (0.321) loss_u loss_u 0.9395 (0.8653) acc_u 6.2500 (17.2917) lr 1.4736e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1362
confident_label rate tensor(0.3913, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1227
clean true:1222
clean false:5
clean_rate:0.9959250203748982
noisy true:552
noisy false:1357
after delete: len(clean_dataset) 1227
after delete: len(noisy_dataset) 1909
epoch [168/200] batch [5/38] time 0.438 (0.434) data 0.307 (0.304) loss_x loss_x 0.9409 (0.9740) acc_x 78.1250 (77.5000) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [10/38] time 0.461 (0.485) data 0.331 (0.354) loss_x loss_x 1.0234 (0.9760) acc_x 68.7500 (75.3125) lr 1.3926e-04 eta 0:00:13
epoch [168/200] batch [15/38] time 0.439 (0.460) data 0.308 (0.330) loss_x loss_x 0.9961 (0.9520) acc_x 84.3750 (75.6250) lr 1.3926e-04 eta 0:00:10
epoch [168/200] batch [20/38] time 0.366 (0.450) data 0.235 (0.319) loss_x loss_x 0.7495 (0.9797) acc_x 75.0000 (74.8438) lr 1.3926e-04 eta 0:00:08
epoch [168/200] batch [25/38] time 0.473 (0.453) data 0.342 (0.323) loss_x loss_x 1.9883 (1.0142) acc_x 53.1250 (73.7500) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [30/38] time 0.494 (0.458) data 0.363 (0.328) loss_x loss_x 1.2383 (1.0343) acc_x 68.7500 (73.5417) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [35/38] time 0.634 (0.458) data 0.503 (0.327) loss_x loss_x 1.0723 (1.0689) acc_x 68.7500 (73.2143) lr 1.3926e-04 eta 0:00:01
epoch [168/200] batch [5/59] time 0.457 (0.456) data 0.325 (0.325) loss_u loss_u 0.8257 (0.8779) acc_u 21.8750 (15.6250) lr 1.3926e-04 eta 0:00:24
epoch [168/200] batch [10/59] time 0.344 (0.450) data 0.212 (0.319) loss_u loss_u 0.9390 (0.8890) acc_u 6.2500 (14.3750) lr 1.3926e-04 eta 0:00:22
epoch [168/200] batch [15/59] time 0.471 (0.451) data 0.339 (0.320) loss_u loss_u 0.8779 (0.8838) acc_u 15.6250 (15.6250) lr 1.3926e-04 eta 0:00:19
epoch [168/200] batch [20/59] time 0.349 (0.448) data 0.218 (0.317) loss_u loss_u 0.8125 (0.8817) acc_u 28.1250 (16.0938) lr 1.3926e-04 eta 0:00:17
epoch [168/200] batch [25/59] time 0.437 (0.446) data 0.306 (0.314) loss_u loss_u 0.9360 (0.8803) acc_u 6.2500 (16.1250) lr 1.3926e-04 eta 0:00:15
epoch [168/200] batch [30/59] time 0.473 (0.443) data 0.341 (0.312) loss_u loss_u 0.8037 (0.8770) acc_u 18.7500 (16.0417) lr 1.3926e-04 eta 0:00:12
epoch [168/200] batch [35/59] time 0.496 (0.444) data 0.365 (0.313) loss_u loss_u 0.8848 (0.8760) acc_u 12.5000 (16.3393) lr 1.3926e-04 eta 0:00:10
epoch [168/200] batch [40/59] time 0.555 (0.444) data 0.422 (0.313) loss_u loss_u 0.9053 (0.8755) acc_u 9.3750 (15.8594) lr 1.3926e-04 eta 0:00:08
epoch [168/200] batch [45/59] time 0.502 (0.445) data 0.371 (0.313) loss_u loss_u 0.8999 (0.8756) acc_u 15.6250 (16.0417) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [50/59] time 0.380 (0.444) data 0.248 (0.312) loss_u loss_u 0.8994 (0.8754) acc_u 9.3750 (16.0000) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [55/59] time 0.370 (0.441) data 0.239 (0.309) loss_u loss_u 0.8818 (0.8769) acc_u 15.6250 (15.7386) lr 1.3926e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1342
confident_label rate tensor(0.3919, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1229
clean true:1223
clean false:6
clean_rate:0.9951179820992677
noisy true:571
noisy false:1336
after delete: len(clean_dataset) 1229
after delete: len(noisy_dataset) 1907
epoch [169/200] batch [5/38] time 0.359 (0.450) data 0.225 (0.318) loss_x loss_x 1.0508 (0.9784) acc_x 75.0000 (76.8750) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [10/38] time 0.422 (0.469) data 0.291 (0.338) loss_x loss_x 1.2998 (1.0822) acc_x 62.5000 (71.8750) lr 1.3137e-04 eta 0:00:13
epoch [169/200] batch [15/38] time 0.338 (0.453) data 0.208 (0.322) loss_x loss_x 0.8320 (0.9840) acc_x 68.7500 (73.3333) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [20/38] time 0.451 (0.458) data 0.321 (0.327) loss_x loss_x 1.3066 (0.9982) acc_x 75.0000 (74.0625) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [25/38] time 0.418 (0.445) data 0.287 (0.313) loss_x loss_x 0.7373 (0.9853) acc_x 84.3750 (75.0000) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [30/38] time 0.392 (0.447) data 0.261 (0.316) loss_x loss_x 1.4854 (1.0094) acc_x 53.1250 (74.0625) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [35/38] time 0.539 (0.450) data 0.408 (0.319) loss_x loss_x 1.2285 (1.0198) acc_x 71.8750 (73.5714) lr 1.3137e-04 eta 0:00:01
epoch [169/200] batch [5/59] time 0.624 (0.451) data 0.493 (0.320) loss_u loss_u 0.7666 (0.8693) acc_u 31.2500 (21.2500) lr 1.3137e-04 eta 0:00:24
epoch [169/200] batch [10/59] time 0.406 (0.448) data 0.275 (0.318) loss_u loss_u 0.8789 (0.8743) acc_u 18.7500 (17.8125) lr 1.3137e-04 eta 0:00:21
epoch [169/200] batch [15/59] time 0.507 (0.454) data 0.375 (0.323) loss_u loss_u 0.8994 (0.8774) acc_u 12.5000 (16.8750) lr 1.3137e-04 eta 0:00:19
epoch [169/200] batch [20/59] time 0.425 (0.453) data 0.294 (0.322) loss_u loss_u 0.8818 (0.8781) acc_u 12.5000 (16.2500) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [25/59] time 0.621 (0.454) data 0.490 (0.323) loss_u loss_u 0.9053 (0.8809) acc_u 12.5000 (16.0000) lr 1.3137e-04 eta 0:00:15
epoch [169/200] batch [30/59] time 0.326 (0.447) data 0.193 (0.316) loss_u loss_u 0.7949 (0.8702) acc_u 31.2500 (17.3958) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [35/59] time 0.384 (0.448) data 0.252 (0.317) loss_u loss_u 0.9482 (0.8761) acc_u 3.1250 (16.4286) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [40/59] time 0.445 (0.448) data 0.315 (0.317) loss_u loss_u 0.9341 (0.8810) acc_u 6.2500 (15.7812) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [45/59] time 0.465 (0.447) data 0.331 (0.316) loss_u loss_u 0.8022 (0.8772) acc_u 31.2500 (16.4583) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [50/59] time 0.463 (0.449) data 0.331 (0.318) loss_u loss_u 0.8867 (0.8750) acc_u 15.6250 (16.8125) lr 1.3137e-04 eta 0:00:04
epoch [169/200] batch [55/59] time 0.426 (0.448) data 0.294 (0.317) loss_u loss_u 0.8125 (0.8752) acc_u 21.8750 (16.5909) lr 1.3137e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1360
confident_label rate tensor(0.3913, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1227
clean true:1222
clean false:5
clean_rate:0.9959250203748982
noisy true:554
noisy false:1355
after delete: len(clean_dataset) 1227
after delete: len(noisy_dataset) 1909
epoch [170/200] batch [5/38] time 0.386 (0.437) data 0.255 (0.305) loss_x loss_x 0.6123 (0.8496) acc_x 84.3750 (80.0000) lr 1.2369e-04 eta 0:00:14
epoch [170/200] batch [10/38] time 0.390 (0.429) data 0.259 (0.298) loss_x loss_x 1.1094 (1.0179) acc_x 78.1250 (75.9375) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [15/38] time 0.501 (0.443) data 0.370 (0.312) loss_x loss_x 0.7334 (0.9878) acc_x 84.3750 (75.8333) lr 1.2369e-04 eta 0:00:10
epoch [170/200] batch [20/38] time 0.425 (0.451) data 0.294 (0.320) loss_x loss_x 1.5547 (1.0253) acc_x 62.5000 (74.2188) lr 1.2369e-04 eta 0:00:08
epoch [170/200] batch [25/38] time 0.545 (0.454) data 0.414 (0.323) loss_x loss_x 1.0371 (1.0514) acc_x 65.6250 (73.5000) lr 1.2369e-04 eta 0:00:05
epoch [170/200] batch [30/38] time 0.348 (0.450) data 0.217 (0.319) loss_x loss_x 1.5000 (1.0806) acc_x 71.8750 (72.7083) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [35/38] time 0.477 (0.453) data 0.345 (0.322) loss_x loss_x 0.9756 (1.0718) acc_x 75.0000 (73.3929) lr 1.2369e-04 eta 0:00:01
epoch [170/200] batch [5/59] time 0.368 (0.453) data 0.237 (0.322) loss_u loss_u 0.9541 (0.8975) acc_u 6.2500 (13.1250) lr 1.2369e-04 eta 0:00:24
epoch [170/200] batch [10/59] time 0.437 (0.453) data 0.305 (0.322) loss_u loss_u 0.7085 (0.8706) acc_u 37.5000 (15.9375) lr 1.2369e-04 eta 0:00:22
epoch [170/200] batch [15/59] time 0.388 (0.450) data 0.257 (0.318) loss_u loss_u 0.9409 (0.8843) acc_u 9.3750 (14.3750) lr 1.2369e-04 eta 0:00:19
epoch [170/200] batch [20/59] time 0.364 (0.447) data 0.233 (0.316) loss_u loss_u 0.8149 (0.8765) acc_u 28.1250 (15.9375) lr 1.2369e-04 eta 0:00:17
epoch [170/200] batch [25/59] time 0.531 (0.448) data 0.399 (0.317) loss_u loss_u 0.8687 (0.8746) acc_u 15.6250 (16.2500) lr 1.2369e-04 eta 0:00:15
epoch [170/200] batch [30/59] time 0.445 (0.447) data 0.313 (0.316) loss_u loss_u 0.8779 (0.8755) acc_u 12.5000 (15.8333) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [35/59] time 0.388 (0.443) data 0.257 (0.312) loss_u loss_u 0.8555 (0.8722) acc_u 18.7500 (16.1607) lr 1.2369e-04 eta 0:00:10
epoch [170/200] batch [40/59] time 0.540 (0.443) data 0.408 (0.312) loss_u loss_u 0.9048 (0.8727) acc_u 12.5000 (16.0156) lr 1.2369e-04 eta 0:00:08
epoch [170/200] batch [45/59] time 0.373 (0.441) data 0.242 (0.310) loss_u loss_u 0.8271 (0.8718) acc_u 18.7500 (16.2500) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [50/59] time 0.365 (0.441) data 0.233 (0.309) loss_u loss_u 0.8862 (0.8710) acc_u 15.6250 (16.4375) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [55/59] time 0.375 (0.439) data 0.243 (0.308) loss_u loss_u 0.8911 (0.8720) acc_u 12.5000 (16.1932) lr 1.2369e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1362
confident_label rate tensor(0.3900, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1223
clean true:1219
clean false:4
clean_rate:0.9967293540474244
noisy true:555
noisy false:1358
after delete: len(clean_dataset) 1223
after delete: len(noisy_dataset) 1913
epoch [171/200] batch [5/38] time 0.741 (0.518) data 0.607 (0.386) loss_x loss_x 1.2568 (0.9418) acc_x 71.8750 (75.6250) lr 1.1623e-04 eta 0:00:17
epoch [171/200] batch [10/38] time 0.502 (0.479) data 0.371 (0.347) loss_x loss_x 0.9507 (1.0895) acc_x 75.0000 (71.2500) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [15/38] time 0.388 (0.466) data 0.257 (0.335) loss_x loss_x 0.9458 (1.0508) acc_x 81.2500 (73.9583) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [20/38] time 0.525 (0.465) data 0.394 (0.334) loss_x loss_x 1.2451 (1.0254) acc_x 68.7500 (75.4688) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [25/38] time 0.351 (0.466) data 0.220 (0.334) loss_x loss_x 0.9531 (1.0302) acc_x 68.7500 (75.0000) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [30/38] time 0.392 (0.458) data 0.261 (0.327) loss_x loss_x 0.8120 (1.0369) acc_x 84.3750 (75.5208) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [35/38] time 0.538 (0.463) data 0.407 (0.332) loss_x loss_x 1.0391 (1.0188) acc_x 78.1250 (76.1607) lr 1.1623e-04 eta 0:00:01
epoch [171/200] batch [5/59] time 0.472 (0.460) data 0.340 (0.329) loss_u loss_u 0.7817 (0.8669) acc_u 34.3750 (18.7500) lr 1.1623e-04 eta 0:00:24
epoch [171/200] batch [10/59] time 0.401 (0.457) data 0.269 (0.326) loss_u loss_u 0.9619 (0.8773) acc_u 3.1250 (17.8125) lr 1.1623e-04 eta 0:00:22
epoch [171/200] batch [15/59] time 0.486 (0.459) data 0.353 (0.327) loss_u loss_u 0.8403 (0.8758) acc_u 25.0000 (17.2917) lr 1.1623e-04 eta 0:00:20
epoch [171/200] batch [20/59] time 0.385 (0.458) data 0.253 (0.326) loss_u loss_u 0.8979 (0.8768) acc_u 15.6250 (17.1875) lr 1.1623e-04 eta 0:00:17
epoch [171/200] batch [25/59] time 0.388 (0.457) data 0.256 (0.325) loss_u loss_u 0.8857 (0.8739) acc_u 12.5000 (16.8750) lr 1.1623e-04 eta 0:00:15
epoch [171/200] batch [30/59] time 0.456 (0.455) data 0.324 (0.324) loss_u loss_u 0.9141 (0.8722) acc_u 9.3750 (16.8750) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [35/59] time 0.443 (0.455) data 0.311 (0.323) loss_u loss_u 0.8521 (0.8688) acc_u 18.7500 (17.2321) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [40/59] time 0.465 (0.453) data 0.332 (0.321) loss_u loss_u 0.8447 (0.8687) acc_u 21.8750 (17.1094) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [45/59] time 0.469 (0.460) data 0.337 (0.329) loss_u loss_u 0.9380 (0.8712) acc_u 3.1250 (16.7361) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [50/59] time 0.541 (0.458) data 0.408 (0.326) loss_u loss_u 0.9648 (0.8745) acc_u 6.2500 (16.3750) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [55/59] time 0.443 (0.455) data 0.311 (0.323) loss_u loss_u 0.7842 (0.8700) acc_u 28.1250 (16.9318) lr 1.1623e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1349
confident_label rate tensor(0.3996, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1253
clean true:1245
clean false:8
clean_rate:0.9936153232242618
noisy true:542
noisy false:1341
after delete: len(clean_dataset) 1253
after delete: len(noisy_dataset) 1883
epoch [172/200] batch [5/39] time 0.435 (0.500) data 0.304 (0.369) loss_x loss_x 1.3369 (1.0555) acc_x 71.8750 (76.2500) lr 1.0899e-04 eta 0:00:16
epoch [172/200] batch [10/39] time 0.444 (0.487) data 0.313 (0.356) loss_x loss_x 0.8926 (1.0354) acc_x 68.7500 (74.3750) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [15/39] time 0.565 (0.476) data 0.434 (0.345) loss_x loss_x 1.0771 (1.0542) acc_x 81.2500 (75.2083) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [20/39] time 0.408 (0.464) data 0.277 (0.333) loss_x loss_x 1.3691 (1.0583) acc_x 62.5000 (74.6875) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [25/39] time 0.491 (0.459) data 0.360 (0.328) loss_x loss_x 1.0488 (1.0513) acc_x 81.2500 (74.7500) lr 1.0899e-04 eta 0:00:06
epoch [172/200] batch [30/39] time 0.364 (0.452) data 0.233 (0.321) loss_x loss_x 0.9902 (1.0480) acc_x 81.2500 (75.0000) lr 1.0899e-04 eta 0:00:04
epoch [172/200] batch [35/39] time 0.444 (0.455) data 0.313 (0.324) loss_x loss_x 1.3818 (1.0570) acc_x 56.2500 (74.4643) lr 1.0899e-04 eta 0:00:01
epoch [172/200] batch [5/58] time 0.394 (0.461) data 0.263 (0.330) loss_u loss_u 0.8901 (0.8671) acc_u 15.6250 (16.8750) lr 1.0899e-04 eta 0:00:24
epoch [172/200] batch [10/58] time 0.386 (0.456) data 0.254 (0.325) loss_u loss_u 0.8926 (0.8654) acc_u 12.5000 (17.8125) lr 1.0899e-04 eta 0:00:21
epoch [172/200] batch [15/58] time 0.442 (0.455) data 0.312 (0.324) loss_u loss_u 0.8867 (0.8712) acc_u 18.7500 (17.0833) lr 1.0899e-04 eta 0:00:19
epoch [172/200] batch [20/58] time 0.662 (0.453) data 0.532 (0.322) loss_u loss_u 0.8145 (0.8626) acc_u 25.0000 (17.8125) lr 1.0899e-04 eta 0:00:17
epoch [172/200] batch [25/58] time 0.453 (0.453) data 0.322 (0.322) loss_u loss_u 0.8057 (0.8644) acc_u 21.8750 (17.6250) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [30/58] time 0.379 (0.448) data 0.248 (0.317) loss_u loss_u 0.8789 (0.8699) acc_u 12.5000 (16.7708) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [35/58] time 0.371 (0.446) data 0.241 (0.315) loss_u loss_u 0.8789 (0.8711) acc_u 18.7500 (16.6071) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [40/58] time 0.296 (0.440) data 0.166 (0.309) loss_u loss_u 0.9219 (0.8754) acc_u 9.3750 (15.9375) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [45/58] time 0.381 (0.439) data 0.250 (0.308) loss_u loss_u 0.8975 (0.8756) acc_u 12.5000 (15.8333) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [50/58] time 0.558 (0.442) data 0.426 (0.311) loss_u loss_u 0.9458 (0.8781) acc_u 6.2500 (15.3750) lr 1.0899e-04 eta 0:00:03
epoch [172/200] batch [55/58] time 0.344 (0.441) data 0.212 (0.310) loss_u loss_u 0.8311 (0.8762) acc_u 18.7500 (15.5682) lr 1.0899e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1324
confident_label rate tensor(0.3948, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1238
clean true:1232
clean false:6
clean_rate:0.9951534733441034
noisy true:580
noisy false:1318
after delete: len(clean_dataset) 1238
after delete: len(noisy_dataset) 1898
epoch [173/200] batch [5/38] time 0.506 (0.454) data 0.375 (0.323) loss_x loss_x 1.1572 (1.1763) acc_x 75.0000 (73.1250) lr 1.0197e-04 eta 0:00:14
epoch [173/200] batch [10/38] time 0.405 (0.462) data 0.275 (0.331) loss_x loss_x 0.6094 (1.1531) acc_x 87.5000 (74.0625) lr 1.0197e-04 eta 0:00:12
epoch [173/200] batch [15/38] time 0.415 (0.463) data 0.284 (0.332) loss_x loss_x 0.9766 (1.1125) acc_x 71.8750 (74.1667) lr 1.0197e-04 eta 0:00:10
epoch [173/200] batch [20/38] time 0.542 (0.464) data 0.412 (0.333) loss_x loss_x 1.3535 (1.0709) acc_x 59.3750 (75.0000) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [25/38] time 0.581 (0.457) data 0.450 (0.326) loss_x loss_x 1.3633 (1.0645) acc_x 62.5000 (74.6250) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [30/38] time 0.459 (0.463) data 0.329 (0.332) loss_x loss_x 0.5410 (1.0724) acc_x 78.1250 (74.3750) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [35/38] time 0.568 (0.457) data 0.437 (0.326) loss_x loss_x 1.1455 (1.0745) acc_x 71.8750 (74.1964) lr 1.0197e-04 eta 0:00:01
epoch [173/200] batch [5/59] time 0.460 (0.454) data 0.328 (0.323) loss_u loss_u 0.9619 (0.9073) acc_u 3.1250 (10.0000) lr 1.0197e-04 eta 0:00:24
epoch [173/200] batch [10/59] time 0.362 (0.449) data 0.230 (0.318) loss_u loss_u 0.8896 (0.9002) acc_u 9.3750 (11.8750) lr 1.0197e-04 eta 0:00:22
epoch [173/200] batch [15/59] time 0.395 (0.445) data 0.263 (0.314) loss_u loss_u 0.8149 (0.8883) acc_u 21.8750 (13.5417) lr 1.0197e-04 eta 0:00:19
epoch [173/200] batch [20/59] time 0.445 (0.446) data 0.313 (0.315) loss_u loss_u 0.7661 (0.8711) acc_u 31.2500 (15.4688) lr 1.0197e-04 eta 0:00:17
epoch [173/200] batch [25/59] time 0.455 (0.447) data 0.322 (0.315) loss_u loss_u 0.9404 (0.8721) acc_u 9.3750 (15.6250) lr 1.0197e-04 eta 0:00:15
epoch [173/200] batch [30/59] time 0.411 (0.447) data 0.279 (0.315) loss_u loss_u 0.8228 (0.8665) acc_u 21.8750 (16.6667) lr 1.0197e-04 eta 0:00:12
epoch [173/200] batch [35/59] time 0.383 (0.444) data 0.251 (0.312) loss_u loss_u 0.8623 (0.8662) acc_u 18.7500 (16.8750) lr 1.0197e-04 eta 0:00:10
epoch [173/200] batch [40/59] time 0.443 (0.445) data 0.311 (0.313) loss_u loss_u 0.8799 (0.8663) acc_u 15.6250 (16.6406) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [45/59] time 0.463 (0.445) data 0.331 (0.313) loss_u loss_u 0.9287 (0.8687) acc_u 9.3750 (16.3889) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [50/59] time 0.612 (0.445) data 0.480 (0.314) loss_u loss_u 0.8740 (0.8702) acc_u 15.6250 (16.3750) lr 1.0197e-04 eta 0:00:04
epoch [173/200] batch [55/59] time 0.363 (0.445) data 0.231 (0.313) loss_u loss_u 0.8374 (0.8685) acc_u 28.1250 (16.6477) lr 1.0197e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1383
confident_label rate tensor(0.3830, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1201
clean true:1196
clean false:5
clean_rate:0.9958368026644463
noisy true:557
noisy false:1378
after delete: len(clean_dataset) 1201
after delete: len(noisy_dataset) 1935
epoch [174/200] batch [5/37] time 0.498 (0.475) data 0.367 (0.344) loss_x loss_x 1.0195 (0.9725) acc_x 78.1250 (76.2500) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [10/37] time 0.418 (0.463) data 0.287 (0.332) loss_x loss_x 0.9995 (1.0626) acc_x 81.2500 (75.6250) lr 9.5173e-05 eta 0:00:12
epoch [174/200] batch [15/37] time 0.512 (0.472) data 0.381 (0.341) loss_x loss_x 0.6470 (1.0501) acc_x 81.2500 (74.1667) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [20/37] time 0.357 (0.464) data 0.226 (0.333) loss_x loss_x 0.8623 (1.0688) acc_x 75.0000 (74.2188) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [25/37] time 0.371 (0.466) data 0.240 (0.335) loss_x loss_x 0.7856 (1.0503) acc_x 81.2500 (74.0000) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [30/37] time 0.486 (0.460) data 0.355 (0.329) loss_x loss_x 0.3926 (1.0642) acc_x 93.7500 (73.8542) lr 9.5173e-05 eta 0:00:03
epoch [174/200] batch [35/37] time 0.388 (0.462) data 0.258 (0.331) loss_x loss_x 0.7168 (1.0345) acc_x 84.3750 (74.8214) lr 9.5173e-05 eta 0:00:00
epoch [174/200] batch [5/60] time 0.412 (0.461) data 0.281 (0.329) loss_u loss_u 0.9004 (0.8282) acc_u 12.5000 (23.1250) lr 9.5173e-05 eta 0:00:25
epoch [174/200] batch [10/60] time 0.533 (0.465) data 0.401 (0.334) loss_u loss_u 0.8804 (0.8476) acc_u 15.6250 (19.3750) lr 9.5173e-05 eta 0:00:23
epoch [174/200] batch [15/60] time 0.346 (0.459) data 0.214 (0.327) loss_u loss_u 0.9429 (0.8596) acc_u 6.2500 (17.9167) lr 9.5173e-05 eta 0:00:20
epoch [174/200] batch [20/60] time 0.401 (0.456) data 0.270 (0.325) loss_u loss_u 0.9185 (0.8653) acc_u 12.5000 (17.0312) lr 9.5173e-05 eta 0:00:18
epoch [174/200] batch [25/60] time 0.410 (0.453) data 0.278 (0.322) loss_u loss_u 0.7466 (0.8686) acc_u 37.5000 (17.1250) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [30/60] time 0.482 (0.453) data 0.350 (0.322) loss_u loss_u 0.8335 (0.8648) acc_u 21.8750 (17.3958) lr 9.5173e-05 eta 0:00:13
epoch [174/200] batch [35/60] time 0.584 (0.454) data 0.452 (0.323) loss_u loss_u 0.9429 (0.8662) acc_u 6.2500 (17.1429) lr 9.5173e-05 eta 0:00:11
epoch [174/200] batch [40/60] time 0.372 (0.451) data 0.240 (0.319) loss_u loss_u 0.8755 (0.8646) acc_u 15.6250 (16.9531) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [45/60] time 0.349 (0.449) data 0.217 (0.318) loss_u loss_u 0.8105 (0.8646) acc_u 21.8750 (16.9444) lr 9.5173e-05 eta 0:00:06
epoch [174/200] batch [50/60] time 0.374 (0.451) data 0.242 (0.319) loss_u loss_u 0.8794 (0.8641) acc_u 12.5000 (17.2500) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [55/60] time 0.573 (0.452) data 0.441 (0.320) loss_u loss_u 0.9043 (0.8629) acc_u 12.5000 (17.4432) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [60/60] time 0.613 (0.452) data 0.481 (0.320) loss_u loss_u 0.8423 (0.8628) acc_u 15.6250 (17.2917) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1379
confident_label rate tensor(0.3839, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1204
clean true:1197
clean false:7
clean_rate:0.9941860465116279
noisy true:560
noisy false:1372
after delete: len(clean_dataset) 1204
after delete: len(noisy_dataset) 1932
epoch [175/200] batch [5/37] time 0.461 (0.474) data 0.330 (0.343) loss_x loss_x 1.2568 (1.0725) acc_x 71.8750 (76.8750) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [10/37] time 0.405 (0.427) data 0.274 (0.296) loss_x loss_x 0.8896 (1.0415) acc_x 75.0000 (74.6875) lr 8.8597e-05 eta 0:00:11
epoch [175/200] batch [15/37] time 0.395 (0.425) data 0.264 (0.294) loss_x loss_x 1.1855 (1.0410) acc_x 68.7500 (74.5833) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [20/37] time 0.438 (0.444) data 0.307 (0.313) loss_x loss_x 0.7446 (1.0679) acc_x 78.1250 (73.4375) lr 8.8597e-05 eta 0:00:07
epoch [175/200] batch [25/37] time 0.385 (0.449) data 0.253 (0.318) loss_x loss_x 1.0625 (1.0403) acc_x 75.0000 (74.1250) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [30/37] time 0.434 (0.450) data 0.303 (0.318) loss_x loss_x 1.7139 (1.0563) acc_x 56.2500 (73.2292) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [35/37] time 0.391 (0.454) data 0.259 (0.323) loss_x loss_x 0.7056 (1.0675) acc_x 81.2500 (73.3929) lr 8.8597e-05 eta 0:00:00
epoch [175/200] batch [5/60] time 0.462 (0.467) data 0.330 (0.336) loss_u loss_u 0.8447 (0.8780) acc_u 15.6250 (13.7500) lr 8.8597e-05 eta 0:00:25
epoch [175/200] batch [10/60] time 0.400 (0.461) data 0.268 (0.329) loss_u loss_u 0.8145 (0.8723) acc_u 28.1250 (15.9375) lr 8.8597e-05 eta 0:00:23
epoch [175/200] batch [15/60] time 0.372 (0.461) data 0.240 (0.330) loss_u loss_u 0.8486 (0.8728) acc_u 21.8750 (16.2500) lr 8.8597e-05 eta 0:00:20
epoch [175/200] batch [20/60] time 0.449 (0.460) data 0.317 (0.328) loss_u loss_u 0.8564 (0.8704) acc_u 15.6250 (16.2500) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [25/60] time 0.430 (0.456) data 0.298 (0.325) loss_u loss_u 0.8809 (0.8694) acc_u 18.7500 (16.3750) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [30/60] time 0.374 (0.459) data 0.242 (0.327) loss_u loss_u 0.7305 (0.8660) acc_u 34.3750 (16.5625) lr 8.8597e-05 eta 0:00:13
epoch [175/200] batch [35/60] time 0.443 (0.457) data 0.311 (0.325) loss_u loss_u 0.8804 (0.8642) acc_u 12.5000 (16.8750) lr 8.8597e-05 eta 0:00:11
epoch [175/200] batch [40/60] time 0.431 (0.455) data 0.299 (0.323) loss_u loss_u 0.7690 (0.8607) acc_u 28.1250 (17.4219) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [45/60] time 0.374 (0.454) data 0.242 (0.322) loss_u loss_u 0.8589 (0.8595) acc_u 21.8750 (17.7778) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [50/60] time 0.371 (0.454) data 0.239 (0.322) loss_u loss_u 0.9248 (0.8621) acc_u 6.2500 (17.6250) lr 8.8597e-05 eta 0:00:04
epoch [175/200] batch [55/60] time 0.415 (0.453) data 0.283 (0.322) loss_u loss_u 0.8857 (0.8621) acc_u 15.6250 (17.6705) lr 8.8597e-05 eta 0:00:02
epoch [175/200] batch [60/60] time 0.343 (0.450) data 0.211 (0.318) loss_u loss_u 0.8115 (0.8608) acc_u 21.8750 (17.8125) lr 8.8597e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1331
confident_label rate tensor(0.3906, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1225
clean true:1221
clean false:4
clean_rate:0.996734693877551
noisy true:584
noisy false:1327
after delete: len(clean_dataset) 1225
after delete: len(noisy_dataset) 1911
epoch [176/200] batch [5/38] time 0.480 (0.494) data 0.348 (0.362) loss_x loss_x 0.9121 (1.0118) acc_x 81.2500 (73.7500) lr 8.2245e-05 eta 0:00:16
epoch [176/200] batch [10/38] time 0.579 (0.501) data 0.447 (0.369) loss_x loss_x 0.8203 (0.9752) acc_x 84.3750 (76.2500) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [15/38] time 0.451 (0.495) data 0.319 (0.363) loss_x loss_x 1.6807 (0.9794) acc_x 59.3750 (75.6250) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [20/38] time 0.499 (0.500) data 0.368 (0.368) loss_x loss_x 1.0586 (0.9817) acc_x 68.7500 (74.5312) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [25/38] time 0.488 (0.512) data 0.356 (0.380) loss_x loss_x 1.2471 (0.9811) acc_x 68.7500 (74.2500) lr 8.2245e-05 eta 0:00:06
epoch [176/200] batch [30/38] time 0.454 (0.502) data 0.323 (0.371) loss_x loss_x 0.8521 (0.9842) acc_x 71.8750 (73.9583) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [35/38] time 0.569 (0.506) data 0.439 (0.375) loss_x loss_x 0.8169 (1.0065) acc_x 75.0000 (73.3929) lr 8.2245e-05 eta 0:00:01
epoch [176/200] batch [5/59] time 0.563 (0.505) data 0.431 (0.373) loss_u loss_u 0.8228 (0.8633) acc_u 25.0000 (16.2500) lr 8.2245e-05 eta 0:00:27
epoch [176/200] batch [10/59] time 0.417 (0.500) data 0.285 (0.368) loss_u loss_u 0.8682 (0.8589) acc_u 18.7500 (17.1875) lr 8.2245e-05 eta 0:00:24
epoch [176/200] batch [15/59] time 0.544 (0.495) data 0.280 (0.360) loss_u loss_u 0.8677 (0.8789) acc_u 18.7500 (14.7917) lr 8.2245e-05 eta 0:00:21
epoch [176/200] batch [20/59] time 0.528 (0.489) data 0.396 (0.355) loss_u loss_u 0.7681 (0.8744) acc_u 25.0000 (15.4688) lr 8.2245e-05 eta 0:00:19
epoch [176/200] batch [25/59] time 0.402 (0.487) data 0.270 (0.353) loss_u loss_u 0.8613 (0.8736) acc_u 15.6250 (15.5000) lr 8.2245e-05 eta 0:00:16
epoch [176/200] batch [30/59] time 0.434 (0.484) data 0.302 (0.349) loss_u loss_u 0.8882 (0.8751) acc_u 12.5000 (15.2083) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [35/59] time 0.559 (0.481) data 0.249 (0.344) loss_u loss_u 0.8071 (0.8666) acc_u 21.8750 (16.4286) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [40/59] time 0.414 (0.478) data 0.283 (0.340) loss_u loss_u 0.8276 (0.8661) acc_u 28.1250 (16.4844) lr 8.2245e-05 eta 0:00:09
epoch [176/200] batch [45/59] time 0.342 (0.472) data 0.211 (0.335) loss_u loss_u 0.8579 (0.8628) acc_u 15.6250 (16.9444) lr 8.2245e-05 eta 0:00:06
epoch [176/200] batch [50/59] time 0.487 (0.467) data 0.356 (0.331) loss_u loss_u 0.8984 (0.8643) acc_u 12.5000 (16.5000) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [55/59] time 0.435 (0.468) data 0.303 (0.332) loss_u loss_u 0.9102 (0.8633) acc_u 6.2500 (16.5341) lr 8.2245e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1395
confident_label rate tensor(0.3890, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1220
clean true:1212
clean false:8
clean_rate:0.9934426229508196
noisy true:529
noisy false:1387
after delete: len(clean_dataset) 1220
after delete: len(noisy_dataset) 1916
epoch [177/200] batch [5/38] time 0.427 (0.469) data 0.296 (0.338) loss_x loss_x 1.7451 (1.1699) acc_x 71.8750 (75.6250) lr 7.6120e-05 eta 0:00:15
epoch [177/200] batch [10/38] time 0.408 (0.489) data 0.277 (0.358) loss_x loss_x 1.3633 (1.0996) acc_x 71.8750 (75.3125) lr 7.6120e-05 eta 0:00:13
epoch [177/200] batch [15/38] time 0.510 (0.478) data 0.379 (0.344) loss_x loss_x 0.7827 (1.0425) acc_x 78.1250 (76.2500) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [20/38] time 0.531 (0.476) data 0.400 (0.343) loss_x loss_x 1.5713 (1.1183) acc_x 62.5000 (73.5938) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [25/38] time 0.524 (0.478) data 0.393 (0.338) loss_x loss_x 1.8057 (1.1241) acc_x 59.3750 (73.7500) lr 7.6120e-05 eta 0:00:06
epoch [177/200] batch [30/38] time 0.532 (0.468) data 0.402 (0.330) loss_x loss_x 1.0449 (1.1007) acc_x 78.1250 (73.9583) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [35/38] time 0.514 (0.463) data 0.383 (0.326) loss_x loss_x 1.5059 (1.1195) acc_x 65.6250 (73.6607) lr 7.6120e-05 eta 0:00:01
epoch [177/200] batch [5/59] time 0.404 (0.460) data 0.273 (0.320) loss_u loss_u 0.8486 (0.8793) acc_u 18.7500 (16.2500) lr 7.6120e-05 eta 0:00:24
epoch [177/200] batch [10/59] time 0.513 (0.455) data 0.383 (0.316) loss_u loss_u 0.7983 (0.8713) acc_u 28.1250 (16.5625) lr 7.6120e-05 eta 0:00:22
epoch [177/200] batch [15/59] time 0.426 (0.452) data 0.296 (0.313) loss_u loss_u 0.9370 (0.8772) acc_u 9.3750 (16.0417) lr 7.6120e-05 eta 0:00:19
epoch [177/200] batch [20/59] time 0.349 (0.448) data 0.217 (0.307) loss_u loss_u 0.9062 (0.8799) acc_u 12.5000 (15.6250) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [25/59] time 0.447 (0.444) data 0.315 (0.304) loss_u loss_u 0.8735 (0.8767) acc_u 18.7500 (16.0000) lr 7.6120e-05 eta 0:00:15
epoch [177/200] batch [30/59] time 0.521 (0.451) data 0.389 (0.309) loss_u loss_u 0.8760 (0.8781) acc_u 15.6250 (15.7292) lr 7.6120e-05 eta 0:00:13
epoch [177/200] batch [35/59] time 0.468 (0.452) data 0.336 (0.311) loss_u loss_u 0.8345 (0.8736) acc_u 15.6250 (16.0714) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [40/59] time 0.498 (0.456) data 0.366 (0.315) loss_u loss_u 0.8823 (0.8748) acc_u 15.6250 (15.9375) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [45/59] time 0.472 (0.458) data 0.340 (0.316) loss_u loss_u 0.9336 (0.8760) acc_u 12.5000 (15.9028) lr 7.6120e-05 eta 0:00:06
epoch [177/200] batch [50/59] time 0.370 (0.458) data 0.239 (0.317) loss_u loss_u 0.9150 (0.8730) acc_u 12.5000 (16.3125) lr 7.6120e-05 eta 0:00:04
epoch [177/200] batch [55/59] time 0.405 (0.459) data 0.272 (0.317) loss_u loss_u 0.8228 (0.8699) acc_u 21.8750 (16.6477) lr 7.6120e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1351
confident_label rate tensor(0.3925, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1231
clean true:1226
clean false:5
clean_rate:0.9959382615759546
noisy true:559
noisy false:1346
after delete: len(clean_dataset) 1231
after delete: len(noisy_dataset) 1905
epoch [178/200] batch [5/38] time 0.324 (0.510) data 0.193 (0.371) loss_x loss_x 0.7197 (1.0128) acc_x 81.2500 (75.0000) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [10/38] time 0.356 (0.463) data 0.225 (0.328) loss_x loss_x 1.1367 (1.0061) acc_x 71.8750 (76.5625) lr 7.0224e-05 eta 0:00:12
epoch [178/200] batch [15/38] time 0.632 (0.457) data 0.501 (0.323) loss_x loss_x 0.9619 (1.0240) acc_x 78.1250 (75.0000) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [20/38] time 0.501 (0.460) data 0.370 (0.322) loss_x loss_x 1.3037 (1.0680) acc_x 68.7500 (72.8125) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [25/38] time 0.434 (0.459) data 0.303 (0.323) loss_x loss_x 0.9800 (1.0332) acc_x 71.8750 (74.0000) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [30/38] time 0.420 (0.452) data 0.289 (0.317) loss_x loss_x 0.8809 (1.0339) acc_x 81.2500 (73.3333) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [35/38] time 0.414 (0.453) data 0.282 (0.319) loss_x loss_x 1.2383 (1.0529) acc_x 71.8750 (73.0357) lr 7.0224e-05 eta 0:00:01
epoch [178/200] batch [5/59] time 0.373 (0.459) data 0.241 (0.321) loss_u loss_u 0.9253 (0.8755) acc_u 9.3750 (16.2500) lr 7.0224e-05 eta 0:00:24
epoch [178/200] batch [10/59] time 0.364 (0.458) data 0.232 (0.321) loss_u loss_u 0.9146 (0.8700) acc_u 15.6250 (17.1875) lr 7.0224e-05 eta 0:00:22
epoch [178/200] batch [15/59] time 0.488 (0.457) data 0.356 (0.320) loss_u loss_u 0.8853 (0.8733) acc_u 15.6250 (16.2500) lr 7.0224e-05 eta 0:00:20
epoch [178/200] batch [20/59] time 0.351 (0.453) data 0.219 (0.316) loss_u loss_u 0.8994 (0.8656) acc_u 18.7500 (17.8125) lr 7.0224e-05 eta 0:00:17
epoch [178/200] batch [25/59] time 0.380 (0.452) data 0.248 (0.315) loss_u loss_u 0.8521 (0.8651) acc_u 15.6250 (17.6250) lr 7.0224e-05 eta 0:00:15
epoch [178/200] batch [30/59] time 0.457 (0.449) data 0.326 (0.313) loss_u loss_u 0.8623 (0.8666) acc_u 18.7500 (17.3958) lr 7.0224e-05 eta 0:00:13
epoch [178/200] batch [35/59] time 0.446 (0.452) data 0.315 (0.314) loss_u loss_u 0.9380 (0.8656) acc_u 9.3750 (17.3214) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [40/59] time 0.389 (0.450) data 0.258 (0.312) loss_u loss_u 0.8799 (0.8703) acc_u 12.5000 (16.4062) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [45/59] time 0.569 (0.450) data 0.437 (0.313) loss_u loss_u 0.7378 (0.8646) acc_u 34.3750 (17.1528) lr 7.0224e-05 eta 0:00:06
epoch [178/200] batch [50/59] time 0.501 (0.451) data 0.370 (0.314) loss_u loss_u 0.9204 (0.8682) acc_u 6.2500 (16.6250) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [55/59] time 0.394 (0.449) data 0.262 (0.312) loss_u loss_u 0.8501 (0.8649) acc_u 18.7500 (17.2159) lr 7.0224e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1347
confident_label rate tensor(0.3906, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1225
clean true:1220
clean false:5
clean_rate:0.9959183673469387
noisy true:569
noisy false:1342
after delete: len(clean_dataset) 1225
after delete: len(noisy_dataset) 1911
epoch [179/200] batch [5/38] time 0.451 (0.455) data 0.320 (0.325) loss_x loss_x 1.3418 (1.1011) acc_x 56.2500 (71.8750) lr 6.4556e-05 eta 0:00:15
epoch [179/200] batch [10/38] time 0.399 (0.444) data 0.269 (0.313) loss_x loss_x 1.7480 (1.0693) acc_x 62.5000 (73.7500) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [15/38] time 0.423 (0.433) data 0.291 (0.302) loss_x loss_x 1.0000 (1.1116) acc_x 84.3750 (72.0833) lr 6.4556e-05 eta 0:00:09
epoch [179/200] batch [20/38] time 0.401 (0.436) data 0.270 (0.306) loss_x loss_x 0.8877 (1.1188) acc_x 81.2500 (72.3438) lr 6.4556e-05 eta 0:00:07
epoch [179/200] batch [25/38] time 0.468 (0.437) data 0.260 (0.304) loss_x loss_x 0.8413 (1.1308) acc_x 84.3750 (71.8750) lr 6.4556e-05 eta 0:00:05
epoch [179/200] batch [30/38] time 0.460 (0.439) data 0.330 (0.305) loss_x loss_x 0.8706 (1.1316) acc_x 78.1250 (71.5625) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [35/38] time 0.507 (0.442) data 0.377 (0.309) loss_x loss_x 1.4912 (1.1317) acc_x 75.0000 (72.0536) lr 6.4556e-05 eta 0:00:01
epoch [179/200] batch [5/59] time 0.424 (0.446) data 0.293 (0.309) loss_u loss_u 0.8413 (0.8619) acc_u 21.8750 (16.8750) lr 6.4556e-05 eta 0:00:24
epoch [179/200] batch [10/59] time 0.428 (0.449) data 0.295 (0.313) loss_u loss_u 0.8872 (0.8767) acc_u 12.5000 (16.2500) lr 6.4556e-05 eta 0:00:21
epoch [179/200] batch [15/59] time 0.425 (0.454) data 0.294 (0.318) loss_u loss_u 0.9609 (0.8779) acc_u 3.1250 (16.0417) lr 6.4556e-05 eta 0:00:19
epoch [179/200] batch [20/59] time 0.370 (0.453) data 0.238 (0.316) loss_u loss_u 0.8799 (0.8814) acc_u 18.7500 (15.9375) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [25/59] time 0.400 (0.456) data 0.269 (0.319) loss_u loss_u 0.8638 (0.8680) acc_u 18.7500 (17.3750) lr 6.4556e-05 eta 0:00:15
epoch [179/200] batch [30/59] time 0.387 (0.452) data 0.255 (0.316) loss_u loss_u 0.7480 (0.8627) acc_u 31.2500 (17.8125) lr 6.4556e-05 eta 0:00:13
epoch [179/200] batch [35/59] time 0.362 (0.450) data 0.230 (0.314) loss_u loss_u 0.8975 (0.8622) acc_u 12.5000 (17.5000) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [40/59] time 0.453 (0.448) data 0.319 (0.313) loss_u loss_u 0.8911 (0.8637) acc_u 15.6250 (17.1094) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [45/59] time 0.504 (0.451) data 0.372 (0.315) loss_u loss_u 0.8306 (0.8638) acc_u 21.8750 (17.0833) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [50/59] time 0.437 (0.450) data 0.305 (0.314) loss_u loss_u 0.7534 (0.8610) acc_u 34.3750 (17.6250) lr 6.4556e-05 eta 0:00:04
epoch [179/200] batch [55/59] time 0.496 (0.451) data 0.364 (0.315) loss_u loss_u 0.8462 (0.8621) acc_u 25.0000 (17.6705) lr 6.4556e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1336
confident_label rate tensor(0.3964, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1243
clean true:1241
clean false:2
clean_rate:0.998390989541432
noisy true:559
noisy false:1334
after delete: len(clean_dataset) 1243
after delete: len(noisy_dataset) 1893
epoch [180/200] batch [5/38] time 0.506 (0.503) data 0.375 (0.363) loss_x loss_x 1.1035 (0.7664) acc_x 75.0000 (78.1250) lr 5.9119e-05 eta 0:00:16
epoch [180/200] batch [10/38] time 0.468 (0.458) data 0.338 (0.323) loss_x loss_x 0.9409 (0.8644) acc_x 81.2500 (78.1250) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [15/38] time 0.447 (0.470) data 0.316 (0.336) loss_x loss_x 0.7573 (0.9368) acc_x 71.8750 (75.4167) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [20/38] time 0.460 (0.462) data 0.329 (0.329) loss_x loss_x 0.7305 (0.9584) acc_x 71.8750 (74.5312) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [25/38] time 0.461 (0.464) data 0.330 (0.332) loss_x loss_x 1.4873 (0.9703) acc_x 65.6250 (75.2500) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [30/38] time 0.462 (0.460) data 0.331 (0.328) loss_x loss_x 0.8120 (0.9744) acc_x 84.3750 (75.4167) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [35/38] time 0.445 (0.458) data 0.314 (0.326) loss_x loss_x 1.0908 (0.9976) acc_x 75.0000 (74.6429) lr 5.9119e-05 eta 0:00:01
epoch [180/200] batch [5/59] time 0.386 (0.456) data 0.255 (0.324) loss_u loss_u 0.9019 (0.8988) acc_u 12.5000 (12.5000) lr 5.9119e-05 eta 0:00:24
epoch [180/200] batch [10/59] time 0.630 (0.460) data 0.498 (0.324) loss_u loss_u 0.8691 (0.8714) acc_u 21.8750 (17.8125) lr 5.9119e-05 eta 0:00:22
epoch [180/200] batch [15/59] time 0.431 (0.460) data 0.300 (0.324) loss_u loss_u 0.8359 (0.8701) acc_u 15.6250 (16.6667) lr 5.9119e-05 eta 0:00:20
epoch [180/200] batch [20/59] time 0.405 (0.458) data 0.274 (0.323) loss_u loss_u 0.8667 (0.8601) acc_u 18.7500 (17.3438) lr 5.9119e-05 eta 0:00:17
epoch [180/200] batch [25/59] time 0.463 (0.457) data 0.331 (0.322) loss_u loss_u 0.9272 (0.8696) acc_u 12.5000 (16.2500) lr 5.9119e-05 eta 0:00:15
epoch [180/200] batch [30/59] time 0.369 (0.459) data 0.238 (0.324) loss_u loss_u 0.8848 (0.8721) acc_u 12.5000 (15.3125) lr 5.9119e-05 eta 0:00:13
epoch [180/200] batch [35/59] time 0.440 (0.457) data 0.308 (0.322) loss_u loss_u 0.9146 (0.8787) acc_u 12.5000 (14.7321) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [40/59] time 0.366 (0.453) data 0.235 (0.318) loss_u loss_u 0.8872 (0.8743) acc_u 15.6250 (15.5469) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [45/59] time 0.331 (0.455) data 0.199 (0.318) loss_u loss_u 0.9360 (0.8764) acc_u 6.2500 (15.4167) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [50/59] time 0.432 (0.452) data 0.301 (0.316) loss_u loss_u 0.9438 (0.8750) acc_u 6.2500 (15.8750) lr 5.9119e-05 eta 0:00:04
epoch [180/200] batch [55/59] time 0.370 (0.453) data 0.238 (0.316) loss_u loss_u 0.8882 (0.8748) acc_u 15.6250 (15.9091) lr 5.9119e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1372
confident_label rate tensor(0.3897, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1222
clean true:1217
clean false:5
clean_rate:0.9959083469721768
noisy true:547
noisy false:1367
after delete: len(clean_dataset) 1222
after delete: len(noisy_dataset) 1914
epoch [181/200] batch [5/38] time 0.479 (0.472) data 0.348 (0.318) loss_x loss_x 1.2080 (1.1545) acc_x 68.7500 (74.3750) lr 5.3915e-05 eta 0:00:15
epoch [181/200] batch [10/38] time 0.534 (0.467) data 0.404 (0.324) loss_x loss_x 0.7930 (1.0259) acc_x 78.1250 (76.2500) lr 5.3915e-05 eta 0:00:13
epoch [181/200] batch [15/38] time 0.716 (0.471) data 0.584 (0.332) loss_x loss_x 0.7578 (0.9895) acc_x 84.3750 (75.6250) lr 5.3915e-05 eta 0:00:10
epoch [181/200] batch [20/38] time 0.483 (0.463) data 0.352 (0.326) loss_x loss_x 1.1924 (1.0326) acc_x 65.6250 (74.5312) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [25/38] time 0.521 (0.467) data 0.390 (0.331) loss_x loss_x 1.1875 (1.0361) acc_x 71.8750 (73.8750) lr 5.3915e-05 eta 0:00:06
epoch [181/200] batch [30/38] time 0.638 (0.467) data 0.335 (0.326) loss_x loss_x 1.0215 (1.0278) acc_x 71.8750 (74.2708) lr 5.3915e-05 eta 0:00:03
epoch [181/200] batch [35/38] time 0.335 (0.459) data 0.204 (0.320) loss_x loss_x 1.1426 (1.0426) acc_x 84.3750 (74.5536) lr 5.3915e-05 eta 0:00:01
epoch [181/200] batch [5/59] time 0.464 (0.456) data 0.332 (0.318) loss_u loss_u 0.8770 (0.8756) acc_u 15.6250 (16.8750) lr 5.3915e-05 eta 0:00:24
epoch [181/200] batch [10/59] time 0.764 (0.459) data 0.632 (0.322) loss_u loss_u 0.9141 (0.8813) acc_u 9.3750 (15.9375) lr 5.3915e-05 eta 0:00:22
epoch [181/200] batch [15/59] time 0.358 (0.457) data 0.227 (0.321) loss_u loss_u 0.8423 (0.8766) acc_u 25.0000 (16.8750) lr 5.3915e-05 eta 0:00:20
epoch [181/200] batch [20/59] time 0.459 (0.459) data 0.327 (0.323) loss_u loss_u 0.8745 (0.8706) acc_u 12.5000 (17.0312) lr 5.3915e-05 eta 0:00:17
epoch [181/200] batch [25/59] time 0.454 (0.458) data 0.322 (0.321) loss_u loss_u 0.8706 (0.8699) acc_u 25.0000 (17.0000) lr 5.3915e-05 eta 0:00:15
epoch [181/200] batch [30/59] time 0.486 (0.458) data 0.354 (0.321) loss_u loss_u 0.7993 (0.8596) acc_u 28.1250 (18.2292) lr 5.3915e-05 eta 0:00:13
epoch [181/200] batch [35/59] time 0.490 (0.456) data 0.358 (0.319) loss_u loss_u 0.8599 (0.8658) acc_u 15.6250 (17.1429) lr 5.3915e-05 eta 0:00:10
epoch [181/200] batch [40/59] time 0.445 (0.454) data 0.280 (0.317) loss_u loss_u 0.8872 (0.8667) acc_u 15.6250 (16.9531) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [45/59] time 0.380 (0.454) data 0.248 (0.318) loss_u loss_u 0.9854 (0.8683) acc_u 0.0000 (16.7361) lr 5.3915e-05 eta 0:00:06
epoch [181/200] batch [50/59] time 0.436 (0.451) data 0.302 (0.314) loss_u loss_u 0.8193 (0.8647) acc_u 25.0000 (17.1875) lr 5.3915e-05 eta 0:00:04
epoch [181/200] batch [55/59] time 0.500 (0.455) data 0.369 (0.317) loss_u loss_u 0.8618 (0.8648) acc_u 15.6250 (17.2727) lr 5.3915e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1344
confident_label rate tensor(0.3884, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1218
clean true:1210
clean false:8
clean_rate:0.993431855500821
noisy true:582
noisy false:1336
after delete: len(clean_dataset) 1218
after delete: len(noisy_dataset) 1918
epoch [182/200] batch [5/38] time 0.409 (0.500) data 0.277 (0.369) loss_x loss_x 0.7070 (0.9464) acc_x 84.3750 (72.5000) lr 4.8943e-05 eta 0:00:16
epoch [182/200] batch [10/38] time 0.520 (0.468) data 0.389 (0.337) loss_x loss_x 1.8301 (1.1331) acc_x 65.6250 (72.1875) lr 4.8943e-05 eta 0:00:13
epoch [182/200] batch [15/38] time 0.383 (0.469) data 0.252 (0.337) loss_x loss_x 0.8301 (1.0858) acc_x 78.1250 (73.7500) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [20/38] time 0.477 (0.465) data 0.345 (0.334) loss_x loss_x 1.1465 (1.0702) acc_x 75.0000 (74.0625) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [25/38] time 0.550 (0.459) data 0.419 (0.327) loss_x loss_x 1.3350 (1.1101) acc_x 71.8750 (73.6250) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [30/38] time 0.381 (0.449) data 0.250 (0.316) loss_x loss_x 1.3809 (1.1261) acc_x 62.5000 (73.1250) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [35/38] time 0.481 (0.455) data 0.350 (0.323) loss_x loss_x 1.5176 (1.1081) acc_x 68.7500 (73.3929) lr 4.8943e-05 eta 0:00:01
epoch [182/200] batch [5/59] time 0.462 (0.451) data 0.330 (0.319) loss_u loss_u 0.8076 (0.8692) acc_u 25.0000 (16.2500) lr 4.8943e-05 eta 0:00:24
epoch [182/200] batch [10/59] time 0.390 (0.448) data 0.259 (0.316) loss_u loss_u 0.9038 (0.8747) acc_u 15.6250 (16.5625) lr 4.8943e-05 eta 0:00:21
epoch [182/200] batch [15/59] time 0.387 (0.447) data 0.254 (0.315) loss_u loss_u 0.9800 (0.8756) acc_u 3.1250 (15.6250) lr 4.8943e-05 eta 0:00:19
epoch [182/200] batch [20/59] time 0.501 (0.446) data 0.368 (0.314) loss_u loss_u 0.8438 (0.8761) acc_u 21.8750 (15.6250) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [25/59] time 0.478 (0.455) data 0.347 (0.320) loss_u loss_u 0.8477 (0.8781) acc_u 18.7500 (15.5000) lr 4.8943e-05 eta 0:00:15
epoch [182/200] batch [30/59] time 0.495 (0.455) data 0.364 (0.321) loss_u loss_u 0.8560 (0.8770) acc_u 12.5000 (15.2083) lr 4.8943e-05 eta 0:00:13
epoch [182/200] batch [35/59] time 0.416 (0.454) data 0.284 (0.319) loss_u loss_u 0.7798 (0.8699) acc_u 28.1250 (16.2500) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [40/59] time 0.523 (0.452) data 0.392 (0.318) loss_u loss_u 0.8149 (0.8721) acc_u 28.1250 (16.1719) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [45/59] time 0.404 (0.449) data 0.272 (0.315) loss_u loss_u 0.9355 (0.8727) acc_u 12.5000 (16.1111) lr 4.8943e-05 eta 0:00:06
epoch [182/200] batch [50/59] time 0.489 (0.451) data 0.357 (0.317) loss_u loss_u 0.8379 (0.8742) acc_u 15.6250 (15.9375) lr 4.8943e-05 eta 0:00:04
epoch [182/200] batch [55/59] time 0.359 (0.450) data 0.228 (0.317) loss_u loss_u 0.8892 (0.8748) acc_u 12.5000 (15.7955) lr 4.8943e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1375
confident_label rate tensor(0.3858, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1210
clean true:1203
clean false:7
clean_rate:0.9942148760330578
noisy true:558
noisy false:1368
after delete: len(clean_dataset) 1210
after delete: len(noisy_dataset) 1926
epoch [183/200] batch [5/37] time 0.484 (0.459) data 0.353 (0.328) loss_x loss_x 0.9292 (1.0479) acc_x 87.5000 (72.5000) lr 4.4207e-05 eta 0:00:14
epoch [183/200] batch [10/37] time 0.386 (0.436) data 0.255 (0.305) loss_x loss_x 1.1777 (1.1630) acc_x 75.0000 (69.6875) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [15/37] time 0.416 (0.437) data 0.286 (0.303) loss_x loss_x 1.0264 (1.0930) acc_x 75.0000 (71.6667) lr 4.4207e-05 eta 0:00:09
epoch [183/200] batch [20/37] time 0.398 (0.430) data 0.267 (0.297) loss_x loss_x 1.0703 (1.0720) acc_x 81.2500 (72.3438) lr 4.4207e-05 eta 0:00:07
epoch [183/200] batch [25/37] time 0.494 (0.436) data 0.364 (0.302) loss_x loss_x 0.9131 (1.0391) acc_x 84.3750 (73.0000) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [30/37] time 0.417 (0.437) data 0.286 (0.303) loss_x loss_x 0.5718 (1.0056) acc_x 87.5000 (74.0625) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [35/37] time 0.434 (0.440) data 0.303 (0.306) loss_x loss_x 1.3965 (1.0142) acc_x 62.5000 (73.5714) lr 4.4207e-05 eta 0:00:00
epoch [183/200] batch [5/60] time 0.706 (0.467) data 0.574 (0.331) loss_u loss_u 0.8813 (0.8787) acc_u 15.6250 (14.3750) lr 4.4207e-05 eta 0:00:25
epoch [183/200] batch [10/60] time 0.412 (0.466) data 0.281 (0.331) loss_u loss_u 0.8032 (0.8738) acc_u 18.7500 (15.3125) lr 4.4207e-05 eta 0:00:23
epoch [183/200] batch [15/60] time 0.455 (0.462) data 0.323 (0.327) loss_u loss_u 0.9053 (0.8717) acc_u 12.5000 (16.8750) lr 4.4207e-05 eta 0:00:20
epoch [183/200] batch [20/60] time 0.319 (0.462) data 0.187 (0.324) loss_u loss_u 0.7891 (0.8616) acc_u 31.2500 (18.4375) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [25/60] time 0.453 (0.459) data 0.321 (0.321) loss_u loss_u 0.8623 (0.8628) acc_u 21.8750 (17.8750) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [30/60] time 0.424 (0.456) data 0.292 (0.319) loss_u loss_u 0.8359 (0.8617) acc_u 15.6250 (17.9167) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [35/60] time 0.638 (0.456) data 0.325 (0.316) loss_u loss_u 0.8442 (0.8613) acc_u 18.7500 (18.2143) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [40/60] time 0.372 (0.452) data 0.240 (0.313) loss_u loss_u 0.7510 (0.8557) acc_u 28.1250 (18.6719) lr 4.4207e-05 eta 0:00:09
epoch [183/200] batch [45/60] time 0.516 (0.452) data 0.276 (0.313) loss_u loss_u 0.7710 (0.8577) acc_u 25.0000 (18.3333) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [50/60] time 0.420 (0.449) data 0.289 (0.310) loss_u loss_u 0.8545 (0.8610) acc_u 21.8750 (17.9375) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [55/60] time 0.576 (0.449) data 0.445 (0.311) loss_u loss_u 0.8706 (0.8614) acc_u 21.8750 (18.0114) lr 4.4207e-05 eta 0:00:02
epoch [183/200] batch [60/60] time 0.469 (0.450) data 0.338 (0.311) loss_u loss_u 0.8921 (0.8649) acc_u 12.5000 (17.5000) lr 4.4207e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1340
confident_label rate tensor(0.3967, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1244
clean true:1240
clean false:4
clean_rate:0.9967845659163987
noisy true:556
noisy false:1336
after delete: len(clean_dataset) 1244
after delete: len(noisy_dataset) 1892
epoch [184/200] batch [5/38] time 0.608 (0.506) data 0.477 (0.341) loss_x loss_x 1.1572 (0.8419) acc_x 75.0000 (81.8750) lr 3.9706e-05 eta 0:00:16
epoch [184/200] batch [10/38] time 0.379 (0.442) data 0.249 (0.294) loss_x loss_x 0.8687 (0.8650) acc_x 71.8750 (80.3125) lr 3.9706e-05 eta 0:00:12
epoch [184/200] batch [15/38] time 0.617 (0.469) data 0.486 (0.327) loss_x loss_x 1.0693 (0.9985) acc_x 68.7500 (76.4583) lr 3.9706e-05 eta 0:00:10
epoch [184/200] batch [20/38] time 0.502 (0.465) data 0.372 (0.326) loss_x loss_x 0.5386 (1.0236) acc_x 84.3750 (75.9375) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [25/38] time 0.458 (0.460) data 0.328 (0.322) loss_x loss_x 0.8579 (1.0181) acc_x 75.0000 (75.7500) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [30/38] time 0.456 (0.462) data 0.325 (0.319) loss_x loss_x 0.9512 (1.0254) acc_x 68.7500 (75.0000) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [35/38] time 0.427 (0.457) data 0.297 (0.317) loss_x loss_x 1.1797 (1.0422) acc_x 71.8750 (74.4643) lr 3.9706e-05 eta 0:00:01
epoch [184/200] batch [5/59] time 0.412 (0.462) data 0.282 (0.319) loss_u loss_u 0.8735 (0.8772) acc_u 15.6250 (15.6250) lr 3.9706e-05 eta 0:00:24
epoch [184/200] batch [10/59] time 0.379 (0.458) data 0.248 (0.316) loss_u loss_u 0.9463 (0.8893) acc_u 6.2500 (15.0000) lr 3.9706e-05 eta 0:00:22
epoch [184/200] batch [15/59] time 0.433 (0.452) data 0.303 (0.309) loss_u loss_u 0.9141 (0.8950) acc_u 9.3750 (12.9167) lr 3.9706e-05 eta 0:00:19
epoch [184/200] batch [20/59] time 0.398 (0.454) data 0.266 (0.312) loss_u loss_u 0.8467 (0.8854) acc_u 15.6250 (13.4375) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [25/59] time 0.553 (0.456) data 0.422 (0.316) loss_u loss_u 0.8877 (0.8860) acc_u 15.6250 (13.6250) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [30/59] time 0.378 (0.455) data 0.246 (0.315) loss_u loss_u 0.8301 (0.8866) acc_u 25.0000 (13.7500) lr 3.9706e-05 eta 0:00:13
epoch [184/200] batch [35/59] time 0.459 (0.451) data 0.327 (0.312) loss_u loss_u 0.8164 (0.8836) acc_u 21.8750 (14.1071) lr 3.9706e-05 eta 0:00:10
epoch [184/200] batch [40/59] time 0.351 (0.450) data 0.219 (0.311) loss_u loss_u 0.8823 (0.8837) acc_u 12.5000 (14.1406) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [45/59] time 0.457 (0.447) data 0.326 (0.308) loss_u loss_u 0.9658 (0.8837) acc_u 3.1250 (14.0972) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [50/59] time 0.513 (0.448) data 0.382 (0.308) loss_u loss_u 0.8647 (0.8804) acc_u 18.7500 (14.7500) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [55/59] time 0.458 (0.447) data 0.326 (0.307) loss_u loss_u 0.8511 (0.8758) acc_u 31.2500 (15.6250) lr 3.9706e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1341
confident_label rate tensor(0.3919, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1229
clean true:1222
clean false:7
clean_rate:0.9943043124491456
noisy true:573
noisy false:1334
after delete: len(clean_dataset) 1229
after delete: len(noisy_dataset) 1907
epoch [185/200] batch [5/38] time 0.386 (0.393) data 0.255 (0.262) loss_x loss_x 1.4834 (1.2072) acc_x 62.5000 (72.5000) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [10/38] time 0.391 (0.420) data 0.260 (0.289) loss_x loss_x 1.3066 (1.1080) acc_x 62.5000 (72.1875) lr 3.5443e-05 eta 0:00:11
epoch [185/200] batch [15/38] time 0.610 (0.438) data 0.461 (0.305) loss_x loss_x 1.5566 (1.1351) acc_x 59.3750 (71.8750) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [20/38] time 0.520 (0.444) data 0.389 (0.312) loss_x loss_x 1.2725 (1.1920) acc_x 68.7500 (70.4688) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [25/38] time 0.406 (0.447) data 0.275 (0.316) loss_x loss_x 1.2549 (1.1425) acc_x 75.0000 (71.3750) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [30/38] time 0.506 (0.451) data 0.374 (0.319) loss_x loss_x 1.6436 (1.1726) acc_x 62.5000 (70.9375) lr 3.5443e-05 eta 0:00:03
epoch [185/200] batch [35/38] time 0.415 (0.446) data 0.284 (0.313) loss_x loss_x 0.9858 (1.1429) acc_x 71.8750 (70.9821) lr 3.5443e-05 eta 0:00:01
epoch [185/200] batch [5/59] time 0.429 (0.446) data 0.297 (0.313) loss_u loss_u 0.8359 (0.8610) acc_u 18.7500 (17.5000) lr 3.5443e-05 eta 0:00:24
epoch [185/200] batch [10/59] time 0.340 (0.443) data 0.209 (0.310) loss_u loss_u 0.8296 (0.8610) acc_u 25.0000 (17.1875) lr 3.5443e-05 eta 0:00:21
epoch [185/200] batch [15/59] time 0.335 (0.445) data 0.202 (0.309) loss_u loss_u 0.8965 (0.8615) acc_u 15.6250 (17.2917) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [20/59] time 0.374 (0.449) data 0.242 (0.313) loss_u loss_u 0.8242 (0.8592) acc_u 21.8750 (17.8125) lr 3.5443e-05 eta 0:00:17
epoch [185/200] batch [25/59] time 0.356 (0.448) data 0.224 (0.312) loss_u loss_u 0.8809 (0.8627) acc_u 15.6250 (17.5000) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [30/59] time 0.563 (0.449) data 0.317 (0.312) loss_u loss_u 0.8921 (0.8721) acc_u 12.5000 (16.2500) lr 3.5443e-05 eta 0:00:13
epoch [185/200] batch [35/59] time 0.449 (0.448) data 0.318 (0.311) loss_u loss_u 0.8281 (0.8727) acc_u 15.6250 (15.8929) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [40/59] time 0.382 (0.447) data 0.250 (0.311) loss_u loss_u 0.8892 (0.8722) acc_u 12.5000 (15.7812) lr 3.5443e-05 eta 0:00:08
epoch [185/200] batch [45/59] time 0.393 (0.445) data 0.261 (0.309) loss_u loss_u 0.8701 (0.8719) acc_u 21.8750 (15.9028) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [50/59] time 0.453 (0.450) data 0.322 (0.315) loss_u loss_u 0.8936 (0.8722) acc_u 12.5000 (15.5625) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [55/59] time 0.464 (0.450) data 0.333 (0.315) loss_u loss_u 0.8833 (0.8755) acc_u 15.6250 (15.2273) lr 3.5443e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1390
confident_label rate tensor(0.3791, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1189
clean true:1183
clean false:6
clean_rate:0.9949537426408747
noisy true:563
noisy false:1384
after delete: len(clean_dataset) 1189
after delete: len(noisy_dataset) 1947
epoch [186/200] batch [5/37] time 0.436 (0.454) data 0.305 (0.324) loss_x loss_x 1.1562 (0.8610) acc_x 75.0000 (78.7500) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [10/37] time 0.359 (0.476) data 0.229 (0.328) loss_x loss_x 0.6470 (0.8774) acc_x 81.2500 (78.4375) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [15/37] time 0.393 (0.470) data 0.263 (0.328) loss_x loss_x 0.9512 (0.9337) acc_x 78.1250 (77.0833) lr 3.1417e-05 eta 0:00:10
epoch [186/200] batch [20/37] time 0.553 (0.471) data 0.422 (0.332) loss_x loss_x 1.5713 (0.9472) acc_x 65.6250 (75.9375) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [25/37] time 0.359 (0.460) data 0.228 (0.322) loss_x loss_x 1.5400 (0.9780) acc_x 59.3750 (74.5000) lr 3.1417e-05 eta 0:00:05
epoch [186/200] batch [30/37] time 0.473 (0.453) data 0.342 (0.317) loss_x loss_x 1.3213 (0.9721) acc_x 62.5000 (74.2708) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [35/37] time 0.353 (0.449) data 0.222 (0.314) loss_x loss_x 0.8188 (1.0055) acc_x 78.1250 (73.6607) lr 3.1417e-05 eta 0:00:00
epoch [186/200] batch [5/60] time 0.540 (0.447) data 0.408 (0.312) loss_u loss_u 0.8330 (0.8211) acc_u 25.0000 (23.7500) lr 3.1417e-05 eta 0:00:24
epoch [186/200] batch [10/60] time 0.411 (0.446) data 0.279 (0.310) loss_u loss_u 0.9004 (0.8599) acc_u 12.5000 (19.3750) lr 3.1417e-05 eta 0:00:22
epoch [186/200] batch [15/60] time 0.320 (0.445) data 0.189 (0.310) loss_u loss_u 0.9131 (0.8681) acc_u 12.5000 (17.7083) lr 3.1417e-05 eta 0:00:20
epoch [186/200] batch [20/60] time 0.370 (0.442) data 0.239 (0.308) loss_u loss_u 0.8960 (0.8595) acc_u 15.6250 (19.0625) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [25/60] time 0.391 (0.440) data 0.259 (0.306) loss_u loss_u 0.8979 (0.8666) acc_u 12.5000 (17.7500) lr 3.1417e-05 eta 0:00:15
epoch [186/200] batch [30/60] time 0.431 (0.451) data 0.299 (0.317) loss_u loss_u 0.8896 (0.8688) acc_u 15.6250 (17.5000) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [35/60] time 0.373 (0.452) data 0.241 (0.315) loss_u loss_u 0.8960 (0.8667) acc_u 9.3750 (17.3214) lr 3.1417e-05 eta 0:00:11
epoch [186/200] batch [40/60] time 0.402 (0.451) data 0.271 (0.315) loss_u loss_u 0.8950 (0.8642) acc_u 9.3750 (17.5000) lr 3.1417e-05 eta 0:00:09
epoch [186/200] batch [45/60] time 0.540 (0.451) data 0.410 (0.315) loss_u loss_u 0.7886 (0.8614) acc_u 28.1250 (17.5694) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [50/60] time 0.676 (0.454) data 0.367 (0.316) loss_u loss_u 0.8657 (0.8633) acc_u 18.7500 (17.3750) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [55/60] time 0.363 (0.453) data 0.231 (0.316) loss_u loss_u 0.8423 (0.8644) acc_u 25.0000 (17.2727) lr 3.1417e-05 eta 0:00:02
epoch [186/200] batch [60/60] time 0.407 (0.453) data 0.276 (0.317) loss_u loss_u 0.8965 (0.8658) acc_u 6.2500 (16.9792) lr 3.1417e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1371
confident_label rate tensor(0.3890, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1220
clean true:1213
clean false:7
clean_rate:0.9942622950819672
noisy true:552
noisy false:1364
after delete: len(clean_dataset) 1220
after delete: len(noisy_dataset) 1916
epoch [187/200] batch [5/38] time 0.431 (0.424) data 0.300 (0.293) loss_x loss_x 0.4736 (1.0792) acc_x 87.5000 (71.8750) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [10/38] time 0.476 (0.465) data 0.345 (0.330) loss_x loss_x 1.3535 (1.0764) acc_x 71.8750 (74.3750) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [15/38] time 0.534 (0.451) data 0.403 (0.317) loss_x loss_x 1.5010 (1.1081) acc_x 65.6250 (73.3333) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [20/38] time 0.593 (0.462) data 0.462 (0.322) loss_x loss_x 0.6641 (1.1288) acc_x 81.2500 (72.1875) lr 2.7630e-05 eta 0:00:08
epoch [187/200] batch [25/38] time 0.510 (0.465) data 0.376 (0.327) loss_x loss_x 1.2969 (1.1111) acc_x 68.7500 (72.7500) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [30/38] time 0.322 (0.459) data 0.191 (0.322) loss_x loss_x 1.4766 (1.1277) acc_x 59.3750 (72.2917) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [35/38] time 0.402 (0.459) data 0.272 (0.323) loss_x loss_x 0.8369 (1.1048) acc_x 84.3750 (72.9464) lr 2.7630e-05 eta 0:00:01
epoch [187/200] batch [5/59] time 0.534 (0.463) data 0.403 (0.328) loss_u loss_u 0.8032 (0.8706) acc_u 21.8750 (16.2500) lr 2.7630e-05 eta 0:00:24
epoch [187/200] batch [10/59] time 0.455 (0.460) data 0.323 (0.325) loss_u loss_u 0.8359 (0.8700) acc_u 15.6250 (15.9375) lr 2.7630e-05 eta 0:00:22
epoch [187/200] batch [15/59] time 0.400 (0.455) data 0.268 (0.320) loss_u loss_u 0.8882 (0.8702) acc_u 12.5000 (15.2083) lr 2.7630e-05 eta 0:00:20
epoch [187/200] batch [20/59] time 0.485 (0.453) data 0.353 (0.319) loss_u loss_u 0.8418 (0.8718) acc_u 15.6250 (15.1562) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [25/59] time 0.436 (0.452) data 0.303 (0.317) loss_u loss_u 0.8857 (0.8683) acc_u 15.6250 (15.6250) lr 2.7630e-05 eta 0:00:15
epoch [187/200] batch [30/59] time 0.340 (0.451) data 0.207 (0.315) loss_u loss_u 0.8975 (0.8724) acc_u 12.5000 (15.4167) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [35/59] time 0.461 (0.451) data 0.329 (0.315) loss_u loss_u 0.9014 (0.8690) acc_u 9.3750 (15.7143) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [40/59] time 0.370 (0.456) data 0.238 (0.320) loss_u loss_u 0.8940 (0.8692) acc_u 15.6250 (15.7031) lr 2.7630e-05 eta 0:00:08
epoch [187/200] batch [45/59] time 0.502 (0.454) data 0.371 (0.318) loss_u loss_u 0.7178 (0.8639) acc_u 37.5000 (16.5972) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [50/59] time 0.367 (0.452) data 0.235 (0.316) loss_u loss_u 0.7495 (0.8631) acc_u 31.2500 (16.8750) lr 2.7630e-05 eta 0:00:04
epoch [187/200] batch [55/59] time 0.433 (0.450) data 0.302 (0.313) loss_u loss_u 0.9062 (0.8690) acc_u 12.5000 (16.1364) lr 2.7630e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1389
confident_label rate tensor(0.3811, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1195
clean true:1187
clean false:8
clean_rate:0.9933054393305439
noisy true:560
noisy false:1381
after delete: len(clean_dataset) 1195
after delete: len(noisy_dataset) 1941
epoch [188/200] batch [5/37] time 0.629 (0.489) data 0.498 (0.357) loss_x loss_x 0.6597 (1.2001) acc_x 93.7500 (75.0000) lr 2.4083e-05 eta 0:00:15
epoch [188/200] batch [10/37] time 0.469 (0.492) data 0.338 (0.344) loss_x loss_x 1.1855 (1.1186) acc_x 62.5000 (73.7500) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [15/37] time 0.491 (0.497) data 0.361 (0.354) loss_x loss_x 0.8628 (1.1235) acc_x 75.0000 (72.7083) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [20/37] time 0.459 (0.481) data 0.328 (0.341) loss_x loss_x 1.1895 (1.0851) acc_x 62.5000 (72.5000) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [25/37] time 0.447 (0.479) data 0.316 (0.336) loss_x loss_x 0.8638 (1.0484) acc_x 78.1250 (73.3750) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [30/37] time 0.435 (0.477) data 0.304 (0.335) loss_x loss_x 0.9629 (1.0308) acc_x 84.3750 (74.2708) lr 2.4083e-05 eta 0:00:03
epoch [188/200] batch [35/37] time 0.493 (0.481) data 0.362 (0.342) loss_x loss_x 1.0527 (1.0260) acc_x 71.8750 (74.1964) lr 2.4083e-05 eta 0:00:00
epoch [188/200] batch [5/60] time 0.423 (0.476) data 0.292 (0.337) loss_u loss_u 0.8564 (0.8700) acc_u 18.7500 (16.8750) lr 2.4083e-05 eta 0:00:26
epoch [188/200] batch [10/60] time 0.468 (0.476) data 0.336 (0.338) loss_u loss_u 0.8965 (0.8731) acc_u 9.3750 (15.6250) lr 2.4083e-05 eta 0:00:23
epoch [188/200] batch [15/60] time 0.367 (0.468) data 0.236 (0.331) loss_u loss_u 0.9072 (0.8708) acc_u 18.7500 (16.2500) lr 2.4083e-05 eta 0:00:21
epoch [188/200] batch [20/60] time 0.386 (0.466) data 0.256 (0.327) loss_u loss_u 0.9150 (0.8720) acc_u 9.3750 (15.7812) lr 2.4083e-05 eta 0:00:18
epoch [188/200] batch [25/60] time 0.435 (0.460) data 0.304 (0.321) loss_u loss_u 0.8423 (0.8605) acc_u 21.8750 (17.8750) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [30/60] time 0.582 (0.461) data 0.450 (0.322) loss_u loss_u 0.8750 (0.8624) acc_u 15.6250 (17.7083) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [35/60] time 0.387 (0.460) data 0.255 (0.322) loss_u loss_u 0.8999 (0.8646) acc_u 12.5000 (17.3214) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [40/60] time 0.709 (0.462) data 0.578 (0.324) loss_u loss_u 0.9023 (0.8630) acc_u 9.3750 (17.9688) lr 2.4083e-05 eta 0:00:09
epoch [188/200] batch [45/60] time 0.394 (0.462) data 0.262 (0.323) loss_u loss_u 0.8809 (0.8626) acc_u 12.5000 (17.9861) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [50/60] time 0.371 (0.458) data 0.239 (0.319) loss_u loss_u 0.8823 (0.8640) acc_u 12.5000 (17.6250) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [55/60] time 0.410 (0.459) data 0.278 (0.320) loss_u loss_u 0.8169 (0.8643) acc_u 25.0000 (17.5000) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [60/60] time 0.357 (0.458) data 0.226 (0.319) loss_u loss_u 0.8486 (0.8641) acc_u 15.6250 (17.2396) lr 2.4083e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1347
confident_label rate tensor(0.3833, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1202
clean true:1199
clean false:3
clean_rate:0.997504159733777
noisy true:590
noisy false:1344
after delete: len(clean_dataset) 1202
after delete: len(noisy_dataset) 1934
epoch [189/200] batch [5/37] time 0.404 (0.412) data 0.273 (0.281) loss_x loss_x 1.2676 (1.3062) acc_x 62.5000 (67.5000) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [10/37] time 0.416 (0.421) data 0.285 (0.290) loss_x loss_x 1.1436 (1.2071) acc_x 75.0000 (69.6875) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [15/37] time 0.604 (0.438) data 0.472 (0.307) loss_x loss_x 1.2471 (1.1842) acc_x 62.5000 (70.4167) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [20/37] time 0.452 (0.434) data 0.241 (0.299) loss_x loss_x 0.8774 (1.1261) acc_x 78.1250 (72.8125) lr 2.0777e-05 eta 0:00:07
epoch [189/200] batch [25/37] time 0.645 (0.444) data 0.514 (0.310) loss_x loss_x 0.7544 (1.1060) acc_x 78.1250 (72.3750) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [30/37] time 0.656 (0.446) data 0.347 (0.306) loss_x loss_x 1.4482 (1.1073) acc_x 65.6250 (72.2917) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [35/37] time 0.506 (0.457) data 0.374 (0.318) loss_x loss_x 0.8677 (1.1147) acc_x 75.0000 (72.3214) lr 2.0777e-05 eta 0:00:00
epoch [189/200] batch [5/60] time 0.345 (0.454) data 0.213 (0.315) loss_u loss_u 0.8145 (0.8737) acc_u 18.7500 (13.7500) lr 2.0777e-05 eta 0:00:24
epoch [189/200] batch [10/60] time 0.607 (0.456) data 0.475 (0.318) loss_u loss_u 0.9619 (0.8772) acc_u 6.2500 (15.6250) lr 2.0777e-05 eta 0:00:22
epoch [189/200] batch [15/60] time 0.368 (0.456) data 0.236 (0.318) loss_u loss_u 0.9019 (0.8766) acc_u 9.3750 (15.6250) lr 2.0777e-05 eta 0:00:20
epoch [189/200] batch [20/60] time 0.586 (0.457) data 0.454 (0.319) loss_u loss_u 0.8672 (0.8770) acc_u 18.7500 (15.7812) lr 2.0777e-05 eta 0:00:18
epoch [189/200] batch [25/60] time 0.419 (0.460) data 0.288 (0.322) loss_u loss_u 0.7666 (0.8718) acc_u 31.2500 (16.5000) lr 2.0777e-05 eta 0:00:16
epoch [189/200] batch [30/60] time 0.459 (0.461) data 0.327 (0.321) loss_u loss_u 0.8130 (0.8677) acc_u 18.7500 (16.9792) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [35/60] time 0.404 (0.460) data 0.272 (0.320) loss_u loss_u 0.8496 (0.8625) acc_u 18.7500 (17.4107) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [40/60] time 0.561 (0.459) data 0.252 (0.317) loss_u loss_u 0.9053 (0.8641) acc_u 12.5000 (17.1094) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [45/60] time 0.368 (0.455) data 0.234 (0.314) loss_u loss_u 0.8857 (0.8625) acc_u 18.7500 (17.4306) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [50/60] time 0.492 (0.454) data 0.360 (0.314) loss_u loss_u 0.8667 (0.8607) acc_u 15.6250 (17.5625) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [55/60] time 0.439 (0.458) data 0.307 (0.317) loss_u loss_u 0.8579 (0.8606) acc_u 18.7500 (17.6705) lr 2.0777e-05 eta 0:00:02
epoch [189/200] batch [60/60] time 0.385 (0.456) data 0.253 (0.315) loss_u loss_u 0.8774 (0.8623) acc_u 12.5000 (17.3438) lr 2.0777e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1332
confident_label rate tensor(0.3913, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1227
clean true:1220
clean false:7
clean_rate:0.9942950285248574
noisy true:584
noisy false:1325
after delete: len(clean_dataset) 1227
after delete: len(noisy_dataset) 1909
epoch [190/200] batch [5/38] time 0.457 (0.428) data 0.326 (0.297) loss_x loss_x 1.0635 (1.0768) acc_x 78.1250 (74.3750) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [10/38] time 0.594 (0.451) data 0.463 (0.320) loss_x loss_x 0.8203 (1.0323) acc_x 78.1250 (75.3125) lr 1.7713e-05 eta 0:00:12
epoch [190/200] batch [15/38] time 0.440 (0.449) data 0.309 (0.318) loss_x loss_x 0.7876 (1.0281) acc_x 75.0000 (75.2083) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [20/38] time 0.347 (0.449) data 0.216 (0.318) loss_x loss_x 0.7632 (1.0038) acc_x 81.2500 (74.6875) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [25/38] time 0.386 (0.445) data 0.255 (0.315) loss_x loss_x 1.2119 (1.0041) acc_x 68.7500 (75.0000) lr 1.7713e-05 eta 0:00:05
epoch [190/200] batch [30/38] time 0.417 (0.445) data 0.286 (0.308) loss_x loss_x 0.7373 (1.0029) acc_x 81.2500 (74.6875) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [35/38] time 0.486 (0.447) data 0.355 (0.311) loss_x loss_x 1.1836 (1.0047) acc_x 78.1250 (74.9107) lr 1.7713e-05 eta 0:00:01
epoch [190/200] batch [5/59] time 0.430 (0.455) data 0.298 (0.320) loss_u loss_u 0.9062 (0.8479) acc_u 12.5000 (18.7500) lr 1.7713e-05 eta 0:00:24
epoch [190/200] batch [10/59] time 0.414 (0.463) data 0.282 (0.324) loss_u loss_u 0.7969 (0.8596) acc_u 28.1250 (17.5000) lr 1.7713e-05 eta 0:00:22
epoch [190/200] batch [15/59] time 0.494 (0.463) data 0.363 (0.325) loss_u loss_u 0.8906 (0.8676) acc_u 18.7500 (16.6667) lr 1.7713e-05 eta 0:00:20
epoch [190/200] batch [20/59] time 0.493 (0.464) data 0.362 (0.325) loss_u loss_u 0.9141 (0.8609) acc_u 9.3750 (17.0312) lr 1.7713e-05 eta 0:00:18
epoch [190/200] batch [25/59] time 0.422 (0.459) data 0.291 (0.321) loss_u loss_u 0.8472 (0.8640) acc_u 18.7500 (16.6250) lr 1.7713e-05 eta 0:00:15
epoch [190/200] batch [30/59] time 0.455 (0.458) data 0.323 (0.321) loss_u loss_u 0.8804 (0.8638) acc_u 15.6250 (16.8750) lr 1.7713e-05 eta 0:00:13
epoch [190/200] batch [35/59] time 0.464 (0.453) data 0.333 (0.317) loss_u loss_u 0.8779 (0.8686) acc_u 15.6250 (16.3393) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [40/59] time 0.427 (0.453) data 0.296 (0.315) loss_u loss_u 0.8867 (0.8686) acc_u 12.5000 (16.3281) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [45/59] time 0.446 (0.453) data 0.316 (0.315) loss_u loss_u 0.9351 (0.8741) acc_u 6.2500 (15.6250) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [50/59] time 0.386 (0.451) data 0.254 (0.313) loss_u loss_u 0.7871 (0.8706) acc_u 34.3750 (16.1875) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [55/59] time 0.584 (0.455) data 0.450 (0.318) loss_u loss_u 0.8574 (0.8692) acc_u 15.6250 (16.3068) lr 1.7713e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1380
confident_label rate tensor(0.3874, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1215
clean true:1209
clean false:6
clean_rate:0.9950617283950617
noisy true:547
noisy false:1374
after delete: len(clean_dataset) 1215
after delete: len(noisy_dataset) 1921
epoch [191/200] batch [5/37] time 0.411 (0.466) data 0.280 (0.335) loss_x loss_x 1.0000 (0.9074) acc_x 71.8750 (81.2500) lr 1.4891e-05 eta 0:00:14
epoch [191/200] batch [10/37] time 0.385 (0.444) data 0.254 (0.313) loss_x loss_x 1.1914 (0.9193) acc_x 75.0000 (80.0000) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [15/37] time 0.402 (0.465) data 0.271 (0.334) loss_x loss_x 0.9556 (1.0260) acc_x 71.8750 (75.6250) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [20/37] time 0.622 (0.467) data 0.492 (0.336) loss_x loss_x 0.7612 (1.0073) acc_x 78.1250 (76.4062) lr 1.4891e-05 eta 0:00:07
epoch [191/200] batch [25/37] time 0.468 (0.459) data 0.337 (0.328) loss_x loss_x 0.8794 (1.0243) acc_x 78.1250 (75.6250) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [30/37] time 0.440 (0.465) data 0.309 (0.334) loss_x loss_x 0.8896 (1.0509) acc_x 84.3750 (75.2083) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [35/37] time 0.429 (0.466) data 0.298 (0.335) loss_x loss_x 1.2139 (1.0450) acc_x 71.8750 (75.4464) lr 1.4891e-05 eta 0:00:00
epoch [191/200] batch [5/60] time 0.618 (0.461) data 0.486 (0.330) loss_u loss_u 0.9155 (0.9138) acc_u 9.3750 (11.2500) lr 1.4891e-05 eta 0:00:25
epoch [191/200] batch [10/60] time 0.340 (0.459) data 0.210 (0.328) loss_u loss_u 0.8569 (0.8852) acc_u 18.7500 (15.0000) lr 1.4891e-05 eta 0:00:22
epoch [191/200] batch [15/60] time 0.456 (0.451) data 0.325 (0.320) loss_u loss_u 0.7881 (0.8767) acc_u 21.8750 (15.2083) lr 1.4891e-05 eta 0:00:20
epoch [191/200] batch [20/60] time 0.391 (0.445) data 0.258 (0.314) loss_u loss_u 0.9316 (0.8777) acc_u 12.5000 (15.4688) lr 1.4891e-05 eta 0:00:17
epoch [191/200] batch [25/60] time 0.394 (0.444) data 0.263 (0.312) loss_u loss_u 0.8193 (0.8680) acc_u 18.7500 (17.0000) lr 1.4891e-05 eta 0:00:15
epoch [191/200] batch [30/60] time 0.417 (0.442) data 0.285 (0.311) loss_u loss_u 0.9189 (0.8684) acc_u 9.3750 (16.9792) lr 1.4891e-05 eta 0:00:13
epoch [191/200] batch [35/60] time 0.497 (0.443) data 0.365 (0.311) loss_u loss_u 0.8696 (0.8692) acc_u 18.7500 (17.1429) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [40/60] time 0.428 (0.442) data 0.296 (0.311) loss_u loss_u 0.8945 (0.8669) acc_u 12.5000 (17.1875) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [45/60] time 0.449 (0.444) data 0.317 (0.313) loss_u loss_u 0.8892 (0.8667) acc_u 21.8750 (17.2917) lr 1.4891e-05 eta 0:00:06
epoch [191/200] batch [50/60] time 0.347 (0.445) data 0.217 (0.314) loss_u loss_u 0.8022 (0.8650) acc_u 28.1250 (17.5625) lr 1.4891e-05 eta 0:00:04
epoch [191/200] batch [55/60] time 0.420 (0.445) data 0.289 (0.314) loss_u loss_u 0.8696 (0.8649) acc_u 18.7500 (17.5000) lr 1.4891e-05 eta 0:00:02
epoch [191/200] batch [60/60] time 0.413 (0.443) data 0.283 (0.311) loss_u loss_u 0.9639 (0.8660) acc_u 6.2500 (17.1875) lr 1.4891e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1353
confident_label rate tensor(0.3929, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1232
clean true:1226
clean false:6
clean_rate:0.9951298701298701
noisy true:557
noisy false:1347
after delete: len(clean_dataset) 1232
after delete: len(noisy_dataset) 1904
epoch [192/200] batch [5/38] time 0.468 (0.433) data 0.337 (0.303) loss_x loss_x 1.1201 (1.1931) acc_x 75.0000 (73.1250) lr 1.2312e-05 eta 0:00:14
epoch [192/200] batch [10/38] time 0.387 (0.459) data 0.256 (0.329) loss_x loss_x 1.5889 (1.0756) acc_x 53.1250 (75.3125) lr 1.2312e-05 eta 0:00:12
epoch [192/200] batch [15/38] time 0.398 (0.445) data 0.268 (0.315) loss_x loss_x 0.9277 (1.0705) acc_x 75.0000 (74.1667) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [20/38] time 0.420 (0.447) data 0.289 (0.316) loss_x loss_x 1.3633 (1.0604) acc_x 71.8750 (74.5312) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [25/38] time 0.444 (0.442) data 0.314 (0.311) loss_x loss_x 0.7480 (1.0926) acc_x 84.3750 (73.7500) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [30/38] time 0.371 (0.444) data 0.240 (0.314) loss_x loss_x 0.9336 (1.0838) acc_x 78.1250 (73.6458) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [35/38] time 0.464 (0.445) data 0.333 (0.315) loss_x loss_x 1.0947 (1.0736) acc_x 75.0000 (73.5714) lr 1.2312e-05 eta 0:00:01
epoch [192/200] batch [5/59] time 0.381 (0.439) data 0.249 (0.309) loss_u loss_u 0.8257 (0.8547) acc_u 21.8750 (17.5000) lr 1.2312e-05 eta 0:00:23
epoch [192/200] batch [10/59] time 0.396 (0.439) data 0.265 (0.308) loss_u loss_u 0.8569 (0.8689) acc_u 18.7500 (16.8750) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [15/59] time 0.472 (0.446) data 0.339 (0.315) loss_u loss_u 0.7661 (0.8724) acc_u 28.1250 (17.0833) lr 1.2312e-05 eta 0:00:19
epoch [192/200] batch [20/59] time 0.396 (0.447) data 0.264 (0.316) loss_u loss_u 0.8872 (0.8763) acc_u 9.3750 (16.2500) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [25/59] time 0.404 (0.448) data 0.274 (0.317) loss_u loss_u 0.8096 (0.8730) acc_u 25.0000 (16.5000) lr 1.2312e-05 eta 0:00:15
epoch [192/200] batch [30/59] time 0.468 (0.449) data 0.337 (0.318) loss_u loss_u 0.8853 (0.8719) acc_u 12.5000 (16.6667) lr 1.2312e-05 eta 0:00:13
epoch [192/200] batch [35/59] time 0.383 (0.447) data 0.251 (0.316) loss_u loss_u 0.8047 (0.8723) acc_u 28.1250 (16.6071) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [40/59] time 0.486 (0.450) data 0.355 (0.319) loss_u loss_u 0.9058 (0.8740) acc_u 15.6250 (16.4062) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [45/59] time 0.383 (0.447) data 0.251 (0.316) loss_u loss_u 0.8506 (0.8722) acc_u 18.7500 (16.5972) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [50/59] time 0.450 (0.445) data 0.318 (0.314) loss_u loss_u 0.7383 (0.8681) acc_u 28.1250 (16.9375) lr 1.2312e-05 eta 0:00:04
epoch [192/200] batch [55/59] time 0.477 (0.446) data 0.346 (0.315) loss_u loss_u 0.8521 (0.8682) acc_u 21.8750 (16.9886) lr 1.2312e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1368
confident_label rate tensor(0.3871, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1214
clean true:1209
clean false:5
clean_rate:0.9958813838550247
noisy true:559
noisy false:1363
after delete: len(clean_dataset) 1214
after delete: len(noisy_dataset) 1922
epoch [193/200] batch [5/37] time 0.448 (0.502) data 0.318 (0.372) loss_x loss_x 1.3105 (1.1268) acc_x 75.0000 (73.7500) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [10/37] time 0.428 (0.505) data 0.297 (0.375) loss_x loss_x 1.0615 (1.1033) acc_x 81.2500 (75.6250) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [15/37] time 0.412 (0.474) data 0.281 (0.344) loss_x loss_x 0.7964 (0.9977) acc_x 81.2500 (76.6667) lr 9.9763e-06 eta 0:00:10
epoch [193/200] batch [20/37] time 0.376 (0.463) data 0.245 (0.332) loss_x loss_x 0.5200 (0.9872) acc_x 81.2500 (76.0938) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [25/37] time 0.423 (0.475) data 0.292 (0.344) loss_x loss_x 0.7783 (0.9677) acc_x 75.0000 (75.7500) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [30/37] time 0.522 (0.479) data 0.392 (0.348) loss_x loss_x 0.9121 (0.9715) acc_x 75.0000 (75.4167) lr 9.9763e-06 eta 0:00:03
epoch [193/200] batch [35/37] time 0.541 (0.473) data 0.409 (0.342) loss_x loss_x 0.7446 (0.9557) acc_x 84.3750 (75.5357) lr 9.9763e-06 eta 0:00:00
epoch [193/200] batch [5/60] time 0.337 (0.465) data 0.205 (0.334) loss_u loss_u 0.8701 (0.8771) acc_u 15.6250 (16.8750) lr 9.9763e-06 eta 0:00:25
epoch [193/200] batch [10/60] time 0.385 (0.472) data 0.255 (0.341) loss_u loss_u 0.7344 (0.8512) acc_u 31.2500 (19.0625) lr 9.9763e-06 eta 0:00:23
epoch [193/200] batch [15/60] time 0.405 (0.465) data 0.275 (0.334) loss_u loss_u 0.9341 (0.8729) acc_u 9.3750 (16.2500) lr 9.9763e-06 eta 0:00:20
epoch [193/200] batch [20/60] time 0.447 (0.462) data 0.316 (0.331) loss_u loss_u 0.8501 (0.8713) acc_u 18.7500 (16.8750) lr 9.9763e-06 eta 0:00:18
epoch [193/200] batch [25/60] time 0.359 (0.458) data 0.228 (0.327) loss_u loss_u 0.8091 (0.8705) acc_u 25.0000 (17.1250) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [30/60] time 0.381 (0.456) data 0.250 (0.325) loss_u loss_u 0.8608 (0.8690) acc_u 18.7500 (17.2917) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [35/60] time 0.479 (0.455) data 0.347 (0.325) loss_u loss_u 0.8574 (0.8658) acc_u 18.7500 (17.5893) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [40/60] time 0.385 (0.454) data 0.253 (0.323) loss_u loss_u 0.8843 (0.8667) acc_u 12.5000 (17.5000) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [45/60] time 0.318 (0.451) data 0.187 (0.320) loss_u loss_u 0.9019 (0.8666) acc_u 15.6250 (17.4306) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [50/60] time 0.404 (0.451) data 0.273 (0.320) loss_u loss_u 0.9302 (0.8691) acc_u 9.3750 (16.8750) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [55/60] time 0.402 (0.448) data 0.272 (0.317) loss_u loss_u 0.9517 (0.8704) acc_u 6.2500 (16.5909) lr 9.9763e-06 eta 0:00:02
epoch [193/200] batch [60/60] time 0.445 (0.447) data 0.314 (0.316) loss_u loss_u 0.8447 (0.8666) acc_u 18.7500 (16.9792) lr 9.9763e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1361
confident_label rate tensor(0.3865, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1212
clean true:1207
clean false:5
clean_rate:0.9958745874587459
noisy true:568
noisy false:1356
after delete: len(clean_dataset) 1212
after delete: len(noisy_dataset) 1924
epoch [194/200] batch [5/37] time 0.567 (0.462) data 0.437 (0.332) loss_x loss_x 1.1201 (1.1179) acc_x 78.1250 (75.0000) lr 7.8853e-06 eta 0:00:14
epoch [194/200] batch [10/37] time 0.409 (0.450) data 0.278 (0.320) loss_x loss_x 1.4326 (1.0456) acc_x 62.5000 (73.1250) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [15/37] time 0.488 (0.458) data 0.357 (0.328) loss_x loss_x 0.7280 (1.0094) acc_x 81.2500 (73.9583) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [20/37] time 0.471 (0.461) data 0.341 (0.330) loss_x loss_x 0.9297 (1.0353) acc_x 81.2500 (73.1250) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [25/37] time 0.499 (0.464) data 0.368 (0.334) loss_x loss_x 1.3760 (1.0750) acc_x 71.8750 (72.7500) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [30/37] time 0.333 (0.452) data 0.202 (0.322) loss_x loss_x 1.2080 (1.0632) acc_x 71.8750 (72.8125) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [35/37] time 0.518 (0.454) data 0.388 (0.323) loss_x loss_x 0.9341 (1.0246) acc_x 71.8750 (74.3750) lr 7.8853e-06 eta 0:00:00
epoch [194/200] batch [5/60] time 0.464 (0.453) data 0.333 (0.322) loss_u loss_u 0.8179 (0.8531) acc_u 25.0000 (20.6250) lr 7.8853e-06 eta 0:00:24
epoch [194/200] batch [10/60] time 0.415 (0.452) data 0.284 (0.321) loss_u loss_u 0.9219 (0.8698) acc_u 6.2500 (17.5000) lr 7.8853e-06 eta 0:00:22
epoch [194/200] batch [15/60] time 0.487 (0.451) data 0.356 (0.321) loss_u loss_u 0.9189 (0.8707) acc_u 12.5000 (17.2917) lr 7.8853e-06 eta 0:00:20
epoch [194/200] batch [20/60] time 0.472 (0.449) data 0.340 (0.318) loss_u loss_u 0.8564 (0.8763) acc_u 18.7500 (16.0938) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [25/60] time 0.402 (0.443) data 0.270 (0.312) loss_u loss_u 0.8960 (0.8734) acc_u 9.3750 (16.0000) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [30/60] time 0.461 (0.442) data 0.330 (0.311) loss_u loss_u 0.8770 (0.8724) acc_u 18.7500 (16.0417) lr 7.8853e-06 eta 0:00:13
epoch [194/200] batch [35/60] time 0.386 (0.439) data 0.255 (0.308) loss_u loss_u 0.8525 (0.8719) acc_u 21.8750 (16.5179) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [40/60] time 0.509 (0.442) data 0.377 (0.311) loss_u loss_u 0.7485 (0.8683) acc_u 31.2500 (16.8750) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [45/60] time 0.381 (0.442) data 0.249 (0.311) loss_u loss_u 0.8594 (0.8711) acc_u 18.7500 (16.5278) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [50/60] time 0.482 (0.440) data 0.351 (0.309) loss_u loss_u 0.9146 (0.8704) acc_u 12.5000 (16.5625) lr 7.8853e-06 eta 0:00:04
epoch [194/200] batch [55/60] time 0.437 (0.437) data 0.305 (0.306) loss_u loss_u 0.8887 (0.8722) acc_u 21.8750 (16.5341) lr 7.8853e-06 eta 0:00:02
epoch [194/200] batch [60/60] time 0.422 (0.439) data 0.290 (0.308) loss_u loss_u 0.8550 (0.8715) acc_u 18.7500 (16.5625) lr 7.8853e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1346
confident_label rate tensor(0.3916, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1228
clean true:1222
clean false:6
clean_rate:0.995114006514658
noisy true:568
noisy false:1340
after delete: len(clean_dataset) 1228
after delete: len(noisy_dataset) 1908
epoch [195/200] batch [5/38] time 0.604 (0.518) data 0.471 (0.387) loss_x loss_x 1.4033 (1.0919) acc_x 71.8750 (74.3750) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [10/38] time 0.474 (0.486) data 0.340 (0.354) loss_x loss_x 1.2910 (1.1169) acc_x 59.3750 (70.9375) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [15/38] time 0.429 (0.497) data 0.296 (0.365) loss_x loss_x 0.9863 (1.0819) acc_x 68.7500 (71.8750) lr 6.0390e-06 eta 0:00:11
epoch [195/200] batch [20/38] time 0.554 (0.504) data 0.423 (0.372) loss_x loss_x 0.8389 (1.1085) acc_x 75.0000 (71.2500) lr 6.0390e-06 eta 0:00:09
epoch [195/200] batch [25/38] time 0.350 (0.491) data 0.220 (0.359) loss_x loss_x 1.0566 (1.0706) acc_x 71.8750 (72.7500) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [30/38] time 0.530 (0.490) data 0.400 (0.359) loss_x loss_x 1.3066 (1.0669) acc_x 68.7500 (73.0208) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [35/38] time 0.540 (0.487) data 0.410 (0.356) loss_x loss_x 0.7573 (1.0738) acc_x 71.8750 (72.8571) lr 6.0390e-06 eta 0:00:01
epoch [195/200] batch [5/59] time 0.737 (0.488) data 0.605 (0.357) loss_u loss_u 0.8291 (0.8707) acc_u 18.7500 (15.0000) lr 6.0390e-06 eta 0:00:26
epoch [195/200] batch [10/59] time 0.480 (0.482) data 0.349 (0.351) loss_u loss_u 0.8501 (0.8636) acc_u 18.7500 (17.5000) lr 6.0390e-06 eta 0:00:23
epoch [195/200] batch [15/59] time 0.426 (0.476) data 0.294 (0.345) loss_u loss_u 0.8574 (0.8624) acc_u 15.6250 (17.9167) lr 6.0390e-06 eta 0:00:20
epoch [195/200] batch [20/59] time 0.425 (0.473) data 0.294 (0.341) loss_u loss_u 0.7598 (0.8568) acc_u 31.2500 (18.9062) lr 6.0390e-06 eta 0:00:18
epoch [195/200] batch [25/59] time 0.359 (0.469) data 0.229 (0.338) loss_u loss_u 0.9102 (0.8631) acc_u 12.5000 (17.8750) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [30/59] time 0.480 (0.467) data 0.349 (0.336) loss_u loss_u 0.8970 (0.8625) acc_u 12.5000 (18.2292) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [35/59] time 0.330 (0.464) data 0.198 (0.333) loss_u loss_u 0.8970 (0.8636) acc_u 12.5000 (18.2143) lr 6.0390e-06 eta 0:00:11
epoch [195/200] batch [40/59] time 0.425 (0.463) data 0.293 (0.331) loss_u loss_u 0.8877 (0.8689) acc_u 18.7500 (17.5000) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [45/59] time 0.465 (0.461) data 0.334 (0.330) loss_u loss_u 0.9043 (0.8687) acc_u 12.5000 (17.4306) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [50/59] time 0.617 (0.463) data 0.485 (0.332) loss_u loss_u 0.8618 (0.8696) acc_u 15.6250 (17.3125) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [55/59] time 0.438 (0.462) data 0.308 (0.331) loss_u loss_u 0.9585 (0.8708) acc_u 6.2500 (17.1023) lr 6.0390e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1351
confident_label rate tensor(0.4018, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1260
clean true:1249
clean false:11
clean_rate:0.9912698412698413
noisy true:536
noisy false:1340
after delete: len(clean_dataset) 1260
after delete: len(noisy_dataset) 1876
epoch [196/200] batch [5/39] time 0.413 (0.431) data 0.282 (0.299) loss_x loss_x 1.3799 (1.1936) acc_x 71.8750 (73.7500) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [10/39] time 0.463 (0.463) data 0.333 (0.332) loss_x loss_x 0.9863 (1.1966) acc_x 84.3750 (72.1875) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [15/39] time 0.430 (0.475) data 0.300 (0.344) loss_x loss_x 1.2773 (1.1697) acc_x 71.8750 (71.6667) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [20/39] time 0.440 (0.462) data 0.309 (0.332) loss_x loss_x 0.9541 (1.1260) acc_x 81.2500 (73.2812) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [25/39] time 0.325 (0.474) data 0.193 (0.344) loss_x loss_x 1.2236 (1.1286) acc_x 68.7500 (72.7500) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [30/39] time 0.370 (0.468) data 0.239 (0.337) loss_x loss_x 1.4521 (1.1217) acc_x 65.6250 (73.0208) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [35/39] time 0.494 (0.462) data 0.364 (0.331) loss_x loss_x 0.6284 (1.1082) acc_x 87.5000 (73.3036) lr 4.4380e-06 eta 0:00:01
epoch [196/200] batch [5/58] time 0.404 (0.456) data 0.272 (0.325) loss_u loss_u 0.7803 (0.8497) acc_u 28.1250 (19.3750) lr 4.4380e-06 eta 0:00:24
epoch [196/200] batch [10/58] time 0.410 (0.457) data 0.278 (0.326) loss_u loss_u 0.8169 (0.8539) acc_u 21.8750 (18.1250) lr 4.4380e-06 eta 0:00:21
epoch [196/200] batch [15/58] time 0.508 (0.453) data 0.378 (0.322) loss_u loss_u 0.8491 (0.8526) acc_u 18.7500 (18.7500) lr 4.4380e-06 eta 0:00:19
epoch [196/200] batch [20/58] time 0.534 (0.451) data 0.403 (0.320) loss_u loss_u 0.8999 (0.8666) acc_u 9.3750 (16.8750) lr 4.4380e-06 eta 0:00:17
epoch [196/200] batch [25/58] time 0.429 (0.452) data 0.299 (0.321) loss_u loss_u 0.8843 (0.8753) acc_u 12.5000 (15.5000) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [30/58] time 0.418 (0.448) data 0.287 (0.317) loss_u loss_u 0.8823 (0.8769) acc_u 12.5000 (15.2083) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [35/58] time 0.427 (0.447) data 0.296 (0.316) loss_u loss_u 0.8481 (0.8769) acc_u 15.6250 (15.4464) lr 4.4380e-06 eta 0:00:10
epoch [196/200] batch [40/58] time 0.548 (0.450) data 0.417 (0.319) loss_u loss_u 0.8770 (0.8769) acc_u 12.5000 (15.6250) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [45/58] time 0.493 (0.455) data 0.362 (0.324) loss_u loss_u 0.8535 (0.8735) acc_u 18.7500 (15.8333) lr 4.4380e-06 eta 0:00:05
epoch [196/200] batch [50/58] time 0.458 (0.454) data 0.327 (0.323) loss_u loss_u 0.8730 (0.8742) acc_u 18.7500 (15.6875) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [55/58] time 0.390 (0.451) data 0.258 (0.320) loss_u loss_u 0.8921 (0.8755) acc_u 15.6250 (15.6818) lr 4.4380e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1360
confident_label rate tensor(0.3900, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1223
clean true:1217
clean false:6
clean_rate:0.9950940310711366
noisy true:559
noisy false:1354
after delete: len(clean_dataset) 1223
after delete: len(noisy_dataset) 1913
epoch [197/200] batch [5/38] time 0.393 (0.451) data 0.261 (0.319) loss_x loss_x 0.6914 (0.9999) acc_x 87.5000 (77.5000) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [10/38] time 0.395 (0.447) data 0.264 (0.316) loss_x loss_x 1.2773 (0.9845) acc_x 75.0000 (79.3750) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [15/38] time 0.407 (0.437) data 0.277 (0.306) loss_x loss_x 0.7905 (1.0388) acc_x 78.1250 (76.0417) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [20/38] time 0.409 (0.439) data 0.278 (0.308) loss_x loss_x 1.2705 (1.0471) acc_x 78.1250 (76.4062) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [25/38] time 0.390 (0.453) data 0.259 (0.323) loss_x loss_x 0.9868 (1.0211) acc_x 75.0000 (76.0000) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [30/38] time 0.481 (0.454) data 0.350 (0.323) loss_x loss_x 0.8164 (0.9943) acc_x 84.3750 (76.6667) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [35/38] time 0.554 (0.457) data 0.423 (0.326) loss_x loss_x 1.2871 (0.9991) acc_x 65.6250 (76.2500) lr 3.0827e-06 eta 0:00:01
epoch [197/200] batch [5/59] time 0.463 (0.465) data 0.331 (0.334) loss_u loss_u 0.8965 (0.8690) acc_u 15.6250 (15.6250) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [10/59] time 0.460 (0.458) data 0.329 (0.327) loss_u loss_u 0.8521 (0.8661) acc_u 18.7500 (16.5625) lr 3.0827e-06 eta 0:00:22
epoch [197/200] batch [15/59] time 0.398 (0.456) data 0.267 (0.325) loss_u loss_u 0.8716 (0.8580) acc_u 12.5000 (18.1250) lr 3.0827e-06 eta 0:00:20
epoch [197/200] batch [20/59] time 0.513 (0.458) data 0.381 (0.326) loss_u loss_u 0.8794 (0.8659) acc_u 15.6250 (17.0312) lr 3.0827e-06 eta 0:00:17
epoch [197/200] batch [25/59] time 0.367 (0.457) data 0.235 (0.326) loss_u loss_u 0.8691 (0.8687) acc_u 15.6250 (16.3750) lr 3.0827e-06 eta 0:00:15
epoch [197/200] batch [30/59] time 0.401 (0.454) data 0.269 (0.323) loss_u loss_u 0.8984 (0.8627) acc_u 12.5000 (16.9792) lr 3.0827e-06 eta 0:00:13
epoch [197/200] batch [35/59] time 0.380 (0.450) data 0.249 (0.319) loss_u loss_u 0.8984 (0.8652) acc_u 15.6250 (16.8750) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [40/59] time 0.567 (0.449) data 0.435 (0.317) loss_u loss_u 0.8232 (0.8655) acc_u 21.8750 (16.7969) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [45/59] time 0.463 (0.451) data 0.331 (0.320) loss_u loss_u 0.8848 (0.8680) acc_u 12.5000 (16.3889) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [50/59] time 0.368 (0.449) data 0.235 (0.318) loss_u loss_u 0.9395 (0.8683) acc_u 12.5000 (16.6250) lr 3.0827e-06 eta 0:00:04
epoch [197/200] batch [55/59] time 0.391 (0.448) data 0.259 (0.316) loss_u loss_u 0.8745 (0.8695) acc_u 12.5000 (16.2500) lr 3.0827e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1350
confident_label rate tensor(0.3960, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1242
clean true:1236
clean false:6
clean_rate:0.9951690821256038
noisy true:550
noisy false:1344
after delete: len(clean_dataset) 1242
after delete: len(noisy_dataset) 1894
epoch [198/200] batch [5/38] time 0.437 (0.437) data 0.306 (0.306) loss_x loss_x 0.8364 (1.0294) acc_x 65.6250 (72.5000) lr 1.9733e-06 eta 0:00:14
epoch [198/200] batch [10/38] time 0.460 (0.460) data 0.329 (0.329) loss_x loss_x 1.3262 (1.0038) acc_x 75.0000 (74.0625) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [15/38] time 0.452 (0.459) data 0.321 (0.328) loss_x loss_x 0.5488 (1.0447) acc_x 90.6250 (73.9583) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [20/38] time 0.517 (0.455) data 0.386 (0.324) loss_x loss_x 1.2646 (1.0556) acc_x 78.1250 (74.8438) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [25/38] time 0.570 (0.464) data 0.439 (0.333) loss_x loss_x 1.2266 (1.0383) acc_x 68.7500 (74.8750) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [30/38] time 0.494 (0.462) data 0.361 (0.332) loss_x loss_x 0.7583 (1.0242) acc_x 78.1250 (74.5833) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [35/38] time 0.431 (0.455) data 0.300 (0.324) loss_x loss_x 0.7354 (1.0161) acc_x 75.0000 (74.6429) lr 1.9733e-06 eta 0:00:01
epoch [198/200] batch [5/59] time 0.665 (0.464) data 0.533 (0.333) loss_u loss_u 0.8198 (0.8674) acc_u 21.8750 (16.2500) lr 1.9733e-06 eta 0:00:25
epoch [198/200] batch [10/59] time 0.445 (0.464) data 0.312 (0.333) loss_u loss_u 0.8496 (0.8746) acc_u 18.7500 (16.2500) lr 1.9733e-06 eta 0:00:22
epoch [198/200] batch [15/59] time 0.580 (0.467) data 0.449 (0.335) loss_u loss_u 0.8433 (0.8732) acc_u 18.7500 (16.2500) lr 1.9733e-06 eta 0:00:20
epoch [198/200] batch [20/59] time 0.483 (0.469) data 0.352 (0.338) loss_u loss_u 0.8560 (0.8765) acc_u 18.7500 (15.6250) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [25/59] time 0.390 (0.469) data 0.259 (0.337) loss_u loss_u 0.9463 (0.8785) acc_u 9.3750 (15.3750) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [30/59] time 0.390 (0.465) data 0.258 (0.334) loss_u loss_u 0.8403 (0.8780) acc_u 25.0000 (15.8333) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [35/59] time 0.511 (0.464) data 0.378 (0.332) loss_u loss_u 0.8691 (0.8806) acc_u 15.6250 (15.4464) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [40/59] time 0.391 (0.458) data 0.258 (0.327) loss_u loss_u 0.8589 (0.8802) acc_u 15.6250 (15.3906) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [45/59] time 0.481 (0.459) data 0.349 (0.328) loss_u loss_u 0.8965 (0.8792) acc_u 12.5000 (15.2083) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [50/59] time 0.492 (0.457) data 0.361 (0.325) loss_u loss_u 0.7964 (0.8776) acc_u 28.1250 (15.3750) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [55/59] time 0.470 (0.457) data 0.339 (0.325) loss_u loss_u 0.8223 (0.8765) acc_u 18.7500 (15.3409) lr 1.9733e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1341
confident_label rate tensor(0.3938, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1235
clean true:1229
clean false:6
clean_rate:0.9951417004048583
noisy true:566
noisy false:1335
after delete: len(clean_dataset) 1235
after delete: len(noisy_dataset) 1901
epoch [199/200] batch [5/38] time 0.410 (0.430) data 0.280 (0.299) loss_x loss_x 1.4902 (1.3397) acc_x 75.0000 (68.7500) lr 1.1101e-06 eta 0:00:14
epoch [199/200] batch [10/38] time 0.495 (0.441) data 0.364 (0.310) loss_x loss_x 0.9302 (1.1949) acc_x 78.1250 (72.1875) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [15/38] time 0.359 (0.444) data 0.228 (0.313) loss_x loss_x 1.0605 (1.1330) acc_x 71.8750 (73.5417) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [20/38] time 0.368 (0.444) data 0.237 (0.313) loss_x loss_x 0.9478 (1.1169) acc_x 71.8750 (73.4375) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [25/38] time 0.509 (0.450) data 0.378 (0.319) loss_x loss_x 1.0137 (1.0975) acc_x 65.6250 (73.5000) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [30/38] time 0.481 (0.462) data 0.350 (0.331) loss_x loss_x 0.5503 (1.0466) acc_x 81.2500 (74.5833) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [35/38] time 0.471 (0.464) data 0.339 (0.333) loss_x loss_x 1.1045 (1.0357) acc_x 75.0000 (75.1786) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [5/59] time 0.433 (0.458) data 0.302 (0.327) loss_u loss_u 0.8008 (0.8396) acc_u 21.8750 (21.2500) lr 1.1101e-06 eta 0:00:24
epoch [199/200] batch [10/59] time 0.582 (0.461) data 0.451 (0.330) loss_u loss_u 0.8594 (0.8632) acc_u 18.7500 (17.5000) lr 1.1101e-06 eta 0:00:22
epoch [199/200] batch [15/59] time 0.421 (0.456) data 0.290 (0.325) loss_u loss_u 0.9380 (0.8773) acc_u 6.2500 (15.8333) lr 1.1101e-06 eta 0:00:20
epoch [199/200] batch [20/59] time 0.565 (0.456) data 0.433 (0.325) loss_u loss_u 0.8687 (0.8794) acc_u 18.7500 (15.3125) lr 1.1101e-06 eta 0:00:17
epoch [199/200] batch [25/59] time 0.440 (0.453) data 0.309 (0.322) loss_u loss_u 0.8950 (0.8705) acc_u 21.8750 (16.7500) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [30/59] time 0.413 (0.451) data 0.280 (0.320) loss_u loss_u 0.9150 (0.8769) acc_u 9.3750 (15.9375) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [35/59] time 0.448 (0.450) data 0.316 (0.318) loss_u loss_u 0.8799 (0.8804) acc_u 15.6250 (15.3571) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [40/59] time 0.436 (0.448) data 0.304 (0.317) loss_u loss_u 0.8442 (0.8810) acc_u 25.0000 (15.3125) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [45/59] time 0.450 (0.448) data 0.318 (0.316) loss_u loss_u 0.8589 (0.8789) acc_u 15.6250 (15.6250) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [50/59] time 0.380 (0.446) data 0.248 (0.315) loss_u loss_u 0.9131 (0.8755) acc_u 9.3750 (16.1875) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [55/59] time 0.654 (0.447) data 0.523 (0.316) loss_u loss_u 0.9106 (0.8755) acc_u 9.3750 (16.1364) lr 1.1101e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1352
confident_label rate tensor(0.3913, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1227
clean true:1222
clean false:5
clean_rate:0.9959250203748982
noisy true:562
noisy false:1347
after delete: len(clean_dataset) 1227
after delete: len(noisy_dataset) 1909
epoch [200/200] batch [5/38] time 0.462 (0.460) data 0.331 (0.329) loss_x loss_x 1.1484 (1.0886) acc_x 71.8750 (72.5000) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [10/38] time 0.486 (0.458) data 0.355 (0.327) loss_x loss_x 0.7896 (1.0700) acc_x 84.3750 (73.4375) lr 4.9344e-07 eta 0:00:12
epoch [200/200] batch [15/38] time 0.613 (0.453) data 0.482 (0.322) loss_x loss_x 1.4668 (1.1096) acc_x 53.1250 (73.3333) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [20/38] time 0.541 (0.459) data 0.410 (0.328) loss_x loss_x 0.8501 (1.0650) acc_x 84.3750 (75.0000) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [25/38] time 0.502 (0.463) data 0.372 (0.333) loss_x loss_x 1.1436 (1.0658) acc_x 68.7500 (74.3750) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [30/38] time 0.325 (0.463) data 0.194 (0.333) loss_x loss_x 1.1113 (1.0454) acc_x 71.8750 (74.6875) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [35/38] time 0.481 (0.459) data 0.350 (0.328) loss_x loss_x 1.0576 (1.0520) acc_x 78.1250 (74.7321) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [5/59] time 0.478 (0.462) data 0.346 (0.331) loss_u loss_u 0.9102 (0.8666) acc_u 12.5000 (18.1250) lr 4.9344e-07 eta 0:00:24
epoch [200/200] batch [10/59] time 0.450 (0.455) data 0.318 (0.324) loss_u loss_u 0.9575 (0.8853) acc_u 6.2500 (15.6250) lr 4.9344e-07 eta 0:00:22
epoch [200/200] batch [15/59] time 0.440 (0.457) data 0.308 (0.326) loss_u loss_u 0.8936 (0.8845) acc_u 15.6250 (15.8333) lr 4.9344e-07 eta 0:00:20
epoch [200/200] batch [20/59] time 0.390 (0.457) data 0.258 (0.326) loss_u loss_u 0.9023 (0.8878) acc_u 12.5000 (15.7812) lr 4.9344e-07 eta 0:00:17
epoch [200/200] batch [25/59] time 0.346 (0.452) data 0.214 (0.321) loss_u loss_u 0.9248 (0.8859) acc_u 12.5000 (15.7500) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [30/59] time 0.373 (0.449) data 0.243 (0.318) loss_u loss_u 0.8521 (0.8834) acc_u 15.6250 (15.7292) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [35/59] time 0.405 (0.450) data 0.273 (0.319) loss_u loss_u 0.8853 (0.8819) acc_u 15.6250 (15.6250) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [40/59] time 0.433 (0.450) data 0.301 (0.319) loss_u loss_u 0.8848 (0.8788) acc_u 9.3750 (15.5469) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [45/59] time 0.644 (0.454) data 0.512 (0.323) loss_u loss_u 0.9395 (0.8775) acc_u 9.3750 (15.8333) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [50/59] time 0.417 (0.452) data 0.285 (0.321) loss_u loss_u 0.9346 (0.8765) acc_u 6.2500 (16.0000) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [55/59] time 0.560 (0.451) data 0.428 (0.320) loss_u loss_u 0.8530 (0.8763) acc_u 21.8750 (15.9091) lr 4.9344e-07 eta 0:00:01
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.375/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 5,426
* accuracy: 67.5%
* error: 32.5%
* macro_f1: 66.1%
Elapsed: 5:01:55
