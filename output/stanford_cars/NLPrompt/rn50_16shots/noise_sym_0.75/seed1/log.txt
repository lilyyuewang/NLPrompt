***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.75', 'DATASET.NOISE_TYPE', 'sym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.75/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.75
  NOISE_TYPE: sym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.75/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.75/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.0533, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 167
clean true:154
clean false:13
clean_rate:0.9221556886227545
noisy true:411
noisy false:2558
after delete: len(clean_dataset) 167
after delete: len(noisy_dataset) 2969
epoch [1/200] batch [5/5] time 0.491 (0.484) data 0.361 (0.336) loss_x loss_x 2.8418 (3.0434) acc_x 50.0000 (38.7500) lr 1.0000e-05 eta 0:00:00
epoch [1/200] batch [5/92] time 0.473 (0.465) data 0.343 (0.323) loss_u loss_u 0.9766 (0.9855) acc_u 3.1250 (4.3750) lr 1.0000e-05 eta 0:00:40
epoch [1/200] batch [10/92] time 0.417 (0.470) data 0.287 (0.332) loss_u loss_u 0.9902 (0.9873) acc_u 0.0000 (2.5000) lr 1.0000e-05 eta 0:00:38
epoch [1/200] batch [15/92] time 0.437 (0.477) data 0.307 (0.341) loss_u loss_u 0.9917 (0.9884) acc_u 3.1250 (2.0833) lr 1.0000e-05 eta 0:00:36
epoch [1/200] batch [20/92] time 0.520 (0.470) data 0.390 (0.335) loss_u loss_u 0.9829 (0.9888) acc_u 6.2500 (2.3438) lr 1.0000e-05 eta 0:00:33
epoch [1/200] batch [25/92] time 0.516 (0.471) data 0.386 (0.337) loss_u loss_u 0.9756 (0.9884) acc_u 6.2500 (2.5000) lr 1.0000e-05 eta 0:00:31
epoch [1/200] batch [30/92] time 0.461 (0.472) data 0.330 (0.338) loss_u loss_u 0.9897 (0.9883) acc_u 0.0000 (2.0833) lr 1.0000e-05 eta 0:00:29
epoch [1/200] batch [35/92] time 0.396 (0.462) data 0.266 (0.329) loss_u loss_u 0.9790 (0.9878) acc_u 3.1250 (2.2321) lr 1.0000e-05 eta 0:00:26
epoch [1/200] batch [40/92] time 0.493 (0.463) data 0.362 (0.330) loss_u loss_u 0.9678 (0.9874) acc_u 9.3750 (2.3438) lr 1.0000e-05 eta 0:00:24
epoch [1/200] batch [45/92] time 0.468 (0.465) data 0.338 (0.332) loss_u loss_u 0.9712 (0.9860) acc_u 9.3750 (2.8472) lr 1.0000e-05 eta 0:00:21
epoch [1/200] batch [50/92] time 0.430 (0.468) data 0.299 (0.336) loss_u loss_u 0.9751 (0.9858) acc_u 6.2500 (2.9375) lr 1.0000e-05 eta 0:00:19
epoch [1/200] batch [55/92] time 0.489 (0.465) data 0.359 (0.332) loss_u loss_u 0.9878 (0.9858) acc_u 0.0000 (2.7841) lr 1.0000e-05 eta 0:00:17
epoch [1/200] batch [60/92] time 0.438 (0.460) data 0.308 (0.328) loss_u loss_u 0.9917 (0.9860) acc_u 3.1250 (2.6562) lr 1.0000e-05 eta 0:00:14
epoch [1/200] batch [65/92] time 0.539 (0.462) data 0.409 (0.330) loss_u loss_u 0.9619 (0.9859) acc_u 6.2500 (2.6442) lr 1.0000e-05 eta 0:00:12
epoch [1/200] batch [70/92] time 0.376 (0.462) data 0.245 (0.330) loss_u loss_u 0.9849 (0.9861) acc_u 3.1250 (2.5000) lr 1.0000e-05 eta 0:00:10
epoch [1/200] batch [75/92] time 0.394 (0.459) data 0.264 (0.327) loss_u loss_u 0.9907 (0.9859) acc_u 3.1250 (2.5833) lr 1.0000e-05 eta 0:00:07
epoch [1/200] batch [80/92] time 0.397 (0.459) data 0.267 (0.327) loss_u loss_u 0.9961 (0.9862) acc_u 0.0000 (2.5391) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [85/92] time 0.429 (0.463) data 0.298 (0.331) loss_u loss_u 0.9932 (0.9863) acc_u 0.0000 (2.4632) lr 1.0000e-05 eta 0:00:03
epoch [1/200] batch [90/92] time 0.388 (0.460) data 0.258 (0.329) loss_u loss_u 0.9902 (0.9863) acc_u 3.1250 (2.4653) lr 1.0000e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2370
confident_label rate tensor(0.0663, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 208
clean true:193
clean false:15
clean_rate:0.9278846153846154
noisy true:573
noisy false:2355
after delete: len(clean_dataset) 208
after delete: len(noisy_dataset) 2928
epoch [2/200] batch [5/6] time 0.490 (0.505) data 0.360 (0.360) loss_x loss_x 1.9219 (2.2928) acc_x 53.1250 (47.5000) lr 2.0000e-03 eta 0:00:00
epoch [2/200] batch [5/91] time 0.379 (0.454) data 0.246 (0.316) loss_u loss_u 0.9629 (0.9641) acc_u 3.1250 (5.0000) lr 2.0000e-03 eta 0:00:39
epoch [2/200] batch [10/91] time 0.434 (0.441) data 0.303 (0.305) loss_u loss_u 0.9697 (0.9681) acc_u 6.2500 (4.0625) lr 2.0000e-03 eta 0:00:35
epoch [2/200] batch [15/91] time 0.523 (0.448) data 0.393 (0.313) loss_u loss_u 0.9453 (0.9635) acc_u 6.2500 (4.7917) lr 2.0000e-03 eta 0:00:34
epoch [2/200] batch [20/91] time 0.364 (0.446) data 0.233 (0.312) loss_u loss_u 0.8984 (0.9595) acc_u 12.5000 (5.7812) lr 2.0000e-03 eta 0:00:31
epoch [2/200] batch [25/91] time 0.399 (0.443) data 0.268 (0.310) loss_u loss_u 0.9653 (0.9598) acc_u 3.1250 (5.7500) lr 2.0000e-03 eta 0:00:29
epoch [2/200] batch [30/91] time 0.412 (0.439) data 0.281 (0.306) loss_u loss_u 0.9180 (0.9597) acc_u 15.6250 (5.8333) lr 2.0000e-03 eta 0:00:26
epoch [2/200] batch [35/91] time 0.330 (0.437) data 0.200 (0.305) loss_u loss_u 0.9370 (0.9607) acc_u 6.2500 (5.5357) lr 2.0000e-03 eta 0:00:24
epoch [2/200] batch [40/91] time 0.379 (0.430) data 0.248 (0.297) loss_u loss_u 0.9668 (0.9599) acc_u 3.1250 (5.5469) lr 2.0000e-03 eta 0:00:21
epoch [2/200] batch [45/91] time 0.405 (0.434) data 0.274 (0.302) loss_u loss_u 0.9526 (0.9595) acc_u 3.1250 (5.4861) lr 2.0000e-03 eta 0:00:19
epoch [2/200] batch [50/91] time 0.431 (0.435) data 0.300 (0.303) loss_u loss_u 0.9741 (0.9608) acc_u 3.1250 (5.2500) lr 2.0000e-03 eta 0:00:17
epoch [2/200] batch [55/91] time 0.446 (0.434) data 0.315 (0.302) loss_u loss_u 0.9644 (0.9597) acc_u 9.3750 (5.5682) lr 2.0000e-03 eta 0:00:15
epoch [2/200] batch [60/91] time 0.340 (0.437) data 0.210 (0.305) loss_u loss_u 0.9199 (0.9587) acc_u 12.5000 (5.8333) lr 2.0000e-03 eta 0:00:13
epoch [2/200] batch [65/91] time 0.352 (0.435) data 0.221 (0.303) loss_u loss_u 0.9570 (0.9580) acc_u 6.2500 (5.9135) lr 2.0000e-03 eta 0:00:11
epoch [2/200] batch [70/91] time 0.378 (0.435) data 0.247 (0.303) loss_u loss_u 0.9321 (0.9581) acc_u 9.3750 (5.8929) lr 2.0000e-03 eta 0:00:09
epoch [2/200] batch [75/91] time 0.507 (0.438) data 0.375 (0.306) loss_u loss_u 0.9634 (0.9569) acc_u 9.3750 (6.1250) lr 2.0000e-03 eta 0:00:07
epoch [2/200] batch [80/91] time 0.398 (0.434) data 0.268 (0.302) loss_u loss_u 0.9282 (0.9559) acc_u 15.6250 (6.2891) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [85/91] time 0.354 (0.435) data 0.223 (0.303) loss_u loss_u 0.9966 (0.9566) acc_u 0.0000 (6.2500) lr 2.0000e-03 eta 0:00:02
epoch [2/200] batch [90/91] time 0.554 (0.438) data 0.423 (0.306) loss_u loss_u 0.9067 (0.9555) acc_u 12.5000 (6.3542) lr 2.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1941
confident_label rate tensor(0.1049, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 329
clean true:314
clean false:15
clean_rate:0.9544072948328267
noisy true:881
noisy false:1926
after delete: len(clean_dataset) 329
after delete: len(noisy_dataset) 2807
epoch [3/200] batch [5/10] time 0.434 (0.461) data 0.303 (0.331) loss_x loss_x 1.7314 (1.7566) acc_x 59.3750 (54.3750) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [10/10] time 0.444 (0.456) data 0.313 (0.326) loss_x loss_x 1.0859 (1.7631) acc_x 71.8750 (55.0000) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/87] time 0.393 (0.440) data 0.262 (0.309) loss_u loss_u 0.9858 (0.9589) acc_u 0.0000 (7.5000) lr 1.9999e-03 eta 0:00:36
epoch [3/200] batch [10/87] time 0.340 (0.433) data 0.209 (0.302) loss_u loss_u 0.9736 (0.9650) acc_u 6.2500 (5.9375) lr 1.9999e-03 eta 0:00:33
epoch [3/200] batch [15/87] time 0.368 (0.442) data 0.238 (0.311) loss_u loss_u 0.9736 (0.9652) acc_u 3.1250 (6.0417) lr 1.9999e-03 eta 0:00:31
epoch [3/200] batch [20/87] time 0.474 (0.447) data 0.344 (0.317) loss_u loss_u 0.9609 (0.9621) acc_u 3.1250 (6.2500) lr 1.9999e-03 eta 0:00:29
epoch [3/200] batch [25/87] time 0.346 (0.442) data 0.215 (0.312) loss_u loss_u 0.9873 (0.9649) acc_u 0.0000 (5.7500) lr 1.9999e-03 eta 0:00:27
epoch [3/200] batch [30/87] time 0.449 (0.444) data 0.318 (0.313) loss_u loss_u 0.9648 (0.9642) acc_u 3.1250 (5.7292) lr 1.9999e-03 eta 0:00:25
epoch [3/200] batch [35/87] time 0.529 (0.448) data 0.398 (0.317) loss_u loss_u 0.9243 (0.9630) acc_u 9.3750 (5.8036) lr 1.9999e-03 eta 0:00:23
epoch [3/200] batch [40/87] time 0.351 (0.446) data 0.221 (0.315) loss_u loss_u 0.9448 (0.9635) acc_u 9.3750 (5.7031) lr 1.9999e-03 eta 0:00:20
epoch [3/200] batch [45/87] time 0.358 (0.441) data 0.227 (0.310) loss_u loss_u 0.9458 (0.9617) acc_u 6.2500 (5.8333) lr 1.9999e-03 eta 0:00:18
epoch [3/200] batch [50/87] time 0.398 (0.437) data 0.267 (0.307) loss_u loss_u 0.9600 (0.9624) acc_u 3.1250 (5.6250) lr 1.9999e-03 eta 0:00:16
epoch [3/200] batch [55/87] time 0.472 (0.438) data 0.341 (0.308) loss_u loss_u 0.9976 (0.9635) acc_u 0.0000 (5.2841) lr 1.9999e-03 eta 0:00:14
epoch [3/200] batch [60/87] time 0.481 (0.439) data 0.350 (0.308) loss_u loss_u 0.9863 (0.9646) acc_u 3.1250 (5.1562) lr 1.9999e-03 eta 0:00:11
epoch [3/200] batch [65/87] time 0.368 (0.439) data 0.237 (0.309) loss_u loss_u 0.9556 (0.9642) acc_u 9.3750 (5.1923) lr 1.9999e-03 eta 0:00:09
epoch [3/200] batch [70/87] time 0.384 (0.438) data 0.253 (0.308) loss_u loss_u 0.9966 (0.9651) acc_u 0.0000 (5.0000) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [75/87] time 0.419 (0.438) data 0.288 (0.308) loss_u loss_u 0.9570 (0.9652) acc_u 6.2500 (5.0000) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [80/87] time 0.376 (0.437) data 0.245 (0.306) loss_u loss_u 0.9858 (0.9648) acc_u 0.0000 (5.0391) lr 1.9999e-03 eta 0:00:03
epoch [3/200] batch [85/87] time 0.368 (0.435) data 0.237 (0.305) loss_u loss_u 0.9917 (0.9642) acc_u 0.0000 (5.0735) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1909
confident_label rate tensor(0.1062, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 333
clean true:325
clean false:8
clean_rate:0.975975975975976
noisy true:902
noisy false:1901
after delete: len(clean_dataset) 333
after delete: len(noisy_dataset) 2803
epoch [4/200] batch [5/10] time 0.483 (0.435) data 0.353 (0.305) loss_x loss_x 1.3135 (1.4783) acc_x 68.7500 (60.6250) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [10/10] time 0.432 (0.444) data 0.303 (0.314) loss_x loss_x 1.0645 (1.5677) acc_x 78.1250 (58.1250) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/87] time 0.365 (0.438) data 0.234 (0.308) loss_u loss_u 0.9463 (0.9539) acc_u 6.2500 (6.2500) lr 1.9995e-03 eta 0:00:35
epoch [4/200] batch [10/87] time 0.426 (0.440) data 0.295 (0.309) loss_u loss_u 0.9717 (0.9624) acc_u 6.2500 (5.0000) lr 1.9995e-03 eta 0:00:33
epoch [4/200] batch [15/87] time 0.408 (0.439) data 0.277 (0.308) loss_u loss_u 0.9414 (0.9665) acc_u 6.2500 (4.1667) lr 1.9995e-03 eta 0:00:31
epoch [4/200] batch [20/87] time 0.351 (0.440) data 0.220 (0.309) loss_u loss_u 0.9951 (0.9688) acc_u 0.0000 (4.0625) lr 1.9995e-03 eta 0:00:29
epoch [4/200] batch [25/87] time 0.502 (0.445) data 0.370 (0.314) loss_u loss_u 0.9658 (0.9674) acc_u 6.2500 (4.3750) lr 1.9995e-03 eta 0:00:27
epoch [4/200] batch [30/87] time 0.471 (0.464) data 0.340 (0.333) loss_u loss_u 0.9336 (0.9684) acc_u 9.3750 (4.0625) lr 1.9995e-03 eta 0:00:26
epoch [4/200] batch [35/87] time 0.375 (0.458) data 0.244 (0.327) loss_u loss_u 0.9702 (0.9680) acc_u 3.1250 (4.1071) lr 1.9995e-03 eta 0:00:23
epoch [4/200] batch [40/87] time 0.531 (0.460) data 0.401 (0.329) loss_u loss_u 0.9507 (0.9660) acc_u 9.3750 (4.5312) lr 1.9995e-03 eta 0:00:21
epoch [4/200] batch [45/87] time 0.437 (0.460) data 0.306 (0.330) loss_u loss_u 0.9272 (0.9658) acc_u 9.3750 (4.5833) lr 1.9995e-03 eta 0:00:19
epoch [4/200] batch [50/87] time 0.390 (0.455) data 0.259 (0.324) loss_u loss_u 0.9580 (0.9649) acc_u 3.1250 (4.6875) lr 1.9995e-03 eta 0:00:16
epoch [4/200] batch [55/87] time 0.392 (0.453) data 0.262 (0.322) loss_u loss_u 0.9688 (0.9643) acc_u 6.2500 (4.7159) lr 1.9995e-03 eta 0:00:14
epoch [4/200] batch [60/87] time 0.418 (0.451) data 0.287 (0.320) loss_u loss_u 0.9082 (0.9642) acc_u 12.5000 (4.7396) lr 1.9995e-03 eta 0:00:12
epoch [4/200] batch [65/87] time 0.461 (0.448) data 0.330 (0.318) loss_u loss_u 0.9741 (0.9637) acc_u 3.1250 (4.7115) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [70/87] time 0.409 (0.445) data 0.278 (0.314) loss_u loss_u 0.9736 (0.9634) acc_u 3.1250 (4.7321) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [75/87] time 0.433 (0.445) data 0.303 (0.315) loss_u loss_u 0.9653 (0.9631) acc_u 3.1250 (4.7917) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [80/87] time 0.351 (0.445) data 0.221 (0.314) loss_u loss_u 0.9521 (0.9624) acc_u 9.3750 (4.9219) lr 1.9995e-03 eta 0:00:03
epoch [4/200] batch [85/87] time 0.366 (0.441) data 0.236 (0.310) loss_u loss_u 0.9873 (0.9634) acc_u 0.0000 (4.8162) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1885
confident_label rate tensor(0.1046, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 328
clean true:320
clean false:8
clean_rate:0.975609756097561
noisy true:931
noisy false:1877
after delete: len(clean_dataset) 328
after delete: len(noisy_dataset) 2808
epoch [5/200] batch [5/10] time 0.391 (0.462) data 0.262 (0.332) loss_x loss_x 1.9951 (1.5998) acc_x 43.7500 (57.5000) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [10/10] time 0.603 (0.460) data 0.473 (0.330) loss_x loss_x 1.3955 (1.8136) acc_x 68.7500 (54.0625) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/87] time 0.530 (0.467) data 0.401 (0.337) loss_u loss_u 0.9722 (0.9629) acc_u 3.1250 (3.7500) lr 1.9989e-03 eta 0:00:38
epoch [5/200] batch [10/87] time 0.458 (0.464) data 0.327 (0.334) loss_u loss_u 0.9902 (0.9695) acc_u 0.0000 (3.7500) lr 1.9989e-03 eta 0:00:35
epoch [5/200] batch [15/87] time 0.673 (0.458) data 0.542 (0.328) loss_u loss_u 0.9868 (0.9707) acc_u 3.1250 (3.5417) lr 1.9989e-03 eta 0:00:32
epoch [5/200] batch [20/87] time 0.454 (0.459) data 0.322 (0.329) loss_u loss_u 0.9497 (0.9689) acc_u 6.2500 (3.9062) lr 1.9989e-03 eta 0:00:30
epoch [5/200] batch [25/87] time 0.398 (0.448) data 0.266 (0.318) loss_u loss_u 0.9268 (0.9657) acc_u 6.2500 (4.0000) lr 1.9989e-03 eta 0:00:27
epoch [5/200] batch [30/87] time 0.601 (0.452) data 0.470 (0.321) loss_u loss_u 0.9907 (0.9648) acc_u 0.0000 (4.1667) lr 1.9989e-03 eta 0:00:25
epoch [5/200] batch [35/87] time 0.396 (0.455) data 0.265 (0.324) loss_u loss_u 0.9463 (0.9632) acc_u 6.2500 (4.2857) lr 1.9989e-03 eta 0:00:23
epoch [5/200] batch [40/87] time 0.462 (0.454) data 0.331 (0.323) loss_u loss_u 0.9883 (0.9640) acc_u 0.0000 (4.1406) lr 1.9989e-03 eta 0:00:21
epoch [5/200] batch [45/87] time 0.333 (0.453) data 0.202 (0.322) loss_u loss_u 0.9717 (0.9625) acc_u 3.1250 (4.6528) lr 1.9989e-03 eta 0:00:19
epoch [5/200] batch [50/87] time 0.452 (0.452) data 0.321 (0.322) loss_u loss_u 0.9980 (0.9633) acc_u 0.0000 (4.5000) lr 1.9989e-03 eta 0:00:16
epoch [5/200] batch [55/87] time 0.411 (0.453) data 0.280 (0.322) loss_u loss_u 0.9653 (0.9642) acc_u 3.1250 (4.4318) lr 1.9989e-03 eta 0:00:14
epoch [5/200] batch [60/87] time 0.363 (0.448) data 0.232 (0.317) loss_u loss_u 0.9556 (0.9644) acc_u 9.3750 (4.3750) lr 1.9989e-03 eta 0:00:12
epoch [5/200] batch [65/87] time 0.421 (0.446) data 0.290 (0.315) loss_u loss_u 0.9785 (0.9639) acc_u 6.2500 (4.5673) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [70/87] time 0.443 (0.445) data 0.313 (0.314) loss_u loss_u 0.9722 (0.9641) acc_u 3.1250 (4.5982) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [75/87] time 0.713 (0.448) data 0.581 (0.317) loss_u loss_u 0.9858 (0.9643) acc_u 3.1250 (4.5833) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [80/87] time 0.416 (0.447) data 0.285 (0.316) loss_u loss_u 0.9341 (0.9642) acc_u 6.2500 (4.5703) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [85/87] time 0.364 (0.444) data 0.233 (0.313) loss_u loss_u 0.9653 (0.9641) acc_u 6.2500 (4.6691) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1888
confident_label rate tensor(0.0985, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 309
clean true:301
clean false:8
clean_rate:0.9741100323624595
noisy true:947
noisy false:1880
after delete: len(clean_dataset) 309
after delete: len(noisy_dataset) 2827
epoch [6/200] batch [5/9] time 0.415 (0.497) data 0.284 (0.366) loss_x loss_x 1.4766 (1.4990) acc_x 65.6250 (58.1250) lr 1.9980e-03 eta 0:00:01
epoch [6/200] batch [5/88] time 0.418 (0.469) data 0.287 (0.338) loss_u loss_u 0.9526 (0.9547) acc_u 3.1250 (5.6250) lr 1.9980e-03 eta 0:00:38
epoch [6/200] batch [10/88] time 0.528 (0.457) data 0.397 (0.326) loss_u loss_u 0.9258 (0.9490) acc_u 15.6250 (6.2500) lr 1.9980e-03 eta 0:00:35
epoch [6/200] batch [15/88] time 0.542 (0.461) data 0.410 (0.330) loss_u loss_u 0.9390 (0.9404) acc_u 6.2500 (7.5000) lr 1.9980e-03 eta 0:00:33
epoch [6/200] batch [20/88] time 0.357 (0.449) data 0.225 (0.318) loss_u loss_u 0.9893 (0.9477) acc_u 0.0000 (6.2500) lr 1.9980e-03 eta 0:00:30
epoch [6/200] batch [25/88] time 0.397 (0.446) data 0.266 (0.315) loss_u loss_u 0.9917 (0.9503) acc_u 3.1250 (6.0000) lr 1.9980e-03 eta 0:00:28
epoch [6/200] batch [30/88] time 0.372 (0.442) data 0.241 (0.311) loss_u loss_u 0.9316 (0.9508) acc_u 6.2500 (5.7292) lr 1.9980e-03 eta 0:00:25
epoch [6/200] batch [35/88] time 0.465 (0.447) data 0.334 (0.316) loss_u loss_u 0.9536 (0.9545) acc_u 9.3750 (5.4464) lr 1.9980e-03 eta 0:00:23
epoch [6/200] batch [40/88] time 0.354 (0.448) data 0.223 (0.317) loss_u loss_u 0.9897 (0.9568) acc_u 0.0000 (5.3906) lr 1.9980e-03 eta 0:00:21
epoch [6/200] batch [45/88] time 0.376 (0.444) data 0.245 (0.313) loss_u loss_u 0.9404 (0.9567) acc_u 6.2500 (5.4167) lr 1.9980e-03 eta 0:00:19
epoch [6/200] batch [50/88] time 0.368 (0.443) data 0.238 (0.312) loss_u loss_u 0.9209 (0.9569) acc_u 12.5000 (5.5000) lr 1.9980e-03 eta 0:00:16
epoch [6/200] batch [55/88] time 0.414 (0.452) data 0.283 (0.320) loss_u loss_u 0.9888 (0.9574) acc_u 0.0000 (5.3409) lr 1.9980e-03 eta 0:00:14
epoch [6/200] batch [60/88] time 0.356 (0.447) data 0.227 (0.316) loss_u loss_u 0.9790 (0.9572) acc_u 6.2500 (5.3646) lr 1.9980e-03 eta 0:00:12
epoch [6/200] batch [65/88] time 0.399 (0.444) data 0.268 (0.313) loss_u loss_u 0.9756 (0.9576) acc_u 0.0000 (5.3846) lr 1.9980e-03 eta 0:00:10
epoch [6/200] batch [70/88] time 0.384 (0.443) data 0.253 (0.312) loss_u loss_u 0.9614 (0.9581) acc_u 6.2500 (5.3125) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [75/88] time 0.437 (0.442) data 0.306 (0.311) loss_u loss_u 0.9785 (0.9590) acc_u 3.1250 (5.2083) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [80/88] time 0.433 (0.442) data 0.302 (0.311) loss_u loss_u 0.9663 (0.9582) acc_u 3.1250 (5.4297) lr 1.9980e-03 eta 0:00:03
epoch [6/200] batch [85/88] time 0.616 (0.447) data 0.484 (0.316) loss_u loss_u 0.9985 (0.9598) acc_u 0.0000 (5.1838) lr 1.9980e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1851
confident_label rate tensor(0.1033, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 324
clean true:316
clean false:8
clean_rate:0.9753086419753086
noisy true:969
noisy false:1843
after delete: len(clean_dataset) 324
after delete: len(noisy_dataset) 2812
epoch [7/200] batch [5/10] time 0.425 (0.431) data 0.295 (0.301) loss_x loss_x 1.1934 (1.5656) acc_x 65.6250 (55.0000) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [10/10] time 0.489 (0.456) data 0.359 (0.326) loss_x loss_x 1.5332 (1.6607) acc_x 56.2500 (56.2500) lr 1.9969e-03 eta 0:00:00
epoch [7/200] batch [5/87] time 0.450 (0.444) data 0.319 (0.314) loss_u loss_u 0.9668 (0.9640) acc_u 3.1250 (5.6250) lr 1.9969e-03 eta 0:00:36
epoch [7/200] batch [10/87] time 0.469 (0.440) data 0.338 (0.309) loss_u loss_u 0.9551 (0.9464) acc_u 6.2500 (8.1250) lr 1.9969e-03 eta 0:00:33
epoch [7/200] batch [15/87] time 0.689 (0.438) data 0.558 (0.308) loss_u loss_u 0.9976 (0.9558) acc_u 0.0000 (6.8750) lr 1.9969e-03 eta 0:00:31
epoch [7/200] batch [20/87] time 0.456 (0.433) data 0.324 (0.302) loss_u loss_u 0.9814 (0.9624) acc_u 0.0000 (5.6250) lr 1.9969e-03 eta 0:00:29
epoch [7/200] batch [25/87] time 0.431 (0.436) data 0.301 (0.305) loss_u loss_u 0.9517 (0.9622) acc_u 12.5000 (5.3750) lr 1.9969e-03 eta 0:00:27
epoch [7/200] batch [30/87] time 0.385 (0.433) data 0.255 (0.302) loss_u loss_u 0.9727 (0.9620) acc_u 3.1250 (5.4167) lr 1.9969e-03 eta 0:00:24
epoch [7/200] batch [35/87] time 0.489 (0.431) data 0.358 (0.300) loss_u loss_u 0.9790 (0.9631) acc_u 0.0000 (5.0893) lr 1.9969e-03 eta 0:00:22
epoch [7/200] batch [40/87] time 0.409 (0.428) data 0.278 (0.297) loss_u loss_u 0.9849 (0.9640) acc_u 0.0000 (4.6875) lr 1.9969e-03 eta 0:00:20
epoch [7/200] batch [45/87] time 0.332 (0.425) data 0.201 (0.294) loss_u loss_u 0.9819 (0.9634) acc_u 0.0000 (4.7917) lr 1.9969e-03 eta 0:00:17
epoch [7/200] batch [50/87] time 0.374 (0.425) data 0.243 (0.294) loss_u loss_u 0.9619 (0.9637) acc_u 0.0000 (4.5000) lr 1.9969e-03 eta 0:00:15
epoch [7/200] batch [55/87] time 0.672 (0.429) data 0.541 (0.299) loss_u loss_u 0.9658 (0.9647) acc_u 3.1250 (4.3182) lr 1.9969e-03 eta 0:00:13
epoch [7/200] batch [60/87] time 0.452 (0.432) data 0.322 (0.301) loss_u loss_u 0.9453 (0.9651) acc_u 6.2500 (4.2708) lr 1.9969e-03 eta 0:00:11
epoch [7/200] batch [65/87] time 0.420 (0.430) data 0.289 (0.300) loss_u loss_u 0.9766 (0.9662) acc_u 3.1250 (4.1346) lr 1.9969e-03 eta 0:00:09
epoch [7/200] batch [70/87] time 0.360 (0.429) data 0.229 (0.299) loss_u loss_u 0.9785 (0.9651) acc_u 3.1250 (4.3750) lr 1.9969e-03 eta 0:00:07
epoch [7/200] batch [75/87] time 0.435 (0.435) data 0.305 (0.304) loss_u loss_u 0.9775 (0.9656) acc_u 3.1250 (4.3333) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [80/87] time 0.382 (0.435) data 0.252 (0.304) loss_u loss_u 0.9849 (0.9649) acc_u 0.0000 (4.3359) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [85/87] time 0.522 (0.436) data 0.391 (0.306) loss_u loss_u 0.9868 (0.9650) acc_u 3.1250 (4.3750) lr 1.9969e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1881
confident_label rate tensor(0.1068, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 335
clean true:328
clean false:7
clean_rate:0.9791044776119403
noisy true:927
noisy false:1874
after delete: len(clean_dataset) 335
after delete: len(noisy_dataset) 2801
epoch [8/200] batch [5/10] time 0.489 (0.509) data 0.359 (0.378) loss_x loss_x 1.5674 (1.5746) acc_x 65.6250 (60.0000) lr 1.9956e-03 eta 0:00:02
epoch [8/200] batch [10/10] time 0.360 (0.492) data 0.230 (0.361) loss_x loss_x 1.7539 (1.7176) acc_x 46.8750 (55.3125) lr 1.9956e-03 eta 0:00:00
epoch [8/200] batch [5/87] time 0.527 (0.479) data 0.395 (0.348) loss_u loss_u 0.9497 (0.9694) acc_u 6.2500 (4.3750) lr 1.9956e-03 eta 0:00:39
epoch [8/200] batch [10/87] time 0.413 (0.457) data 0.283 (0.326) loss_u loss_u 0.9795 (0.9645) acc_u 3.1250 (5.3125) lr 1.9956e-03 eta 0:00:35
epoch [8/200] batch [15/87] time 0.403 (0.455) data 0.273 (0.324) loss_u loss_u 0.9346 (0.9660) acc_u 9.3750 (5.0000) lr 1.9956e-03 eta 0:00:32
epoch [8/200] batch [20/87] time 0.418 (0.449) data 0.285 (0.319) loss_u loss_u 0.9893 (0.9671) acc_u 3.1250 (4.8438) lr 1.9956e-03 eta 0:00:30
epoch [8/200] batch [25/87] time 0.501 (0.452) data 0.371 (0.321) loss_u loss_u 0.9414 (0.9671) acc_u 6.2500 (4.6250) lr 1.9956e-03 eta 0:00:28
epoch [8/200] batch [30/87] time 0.505 (0.451) data 0.374 (0.320) loss_u loss_u 0.9688 (0.9659) acc_u 3.1250 (4.6875) lr 1.9956e-03 eta 0:00:25
epoch [8/200] batch [35/87] time 0.399 (0.453) data 0.268 (0.322) loss_u loss_u 0.9155 (0.9651) acc_u 9.3750 (4.8214) lr 1.9956e-03 eta 0:00:23
epoch [8/200] batch [40/87] time 0.377 (0.452) data 0.246 (0.321) loss_u loss_u 0.9326 (0.9651) acc_u 6.2500 (4.5312) lr 1.9956e-03 eta 0:00:21
epoch [8/200] batch [45/87] time 0.317 (0.445) data 0.186 (0.314) loss_u loss_u 0.9927 (0.9658) acc_u 0.0000 (4.4444) lr 1.9956e-03 eta 0:00:18
epoch [8/200] batch [50/87] time 0.441 (0.443) data 0.310 (0.312) loss_u loss_u 0.9688 (0.9653) acc_u 3.1250 (4.3750) lr 1.9956e-03 eta 0:00:16
epoch [8/200] batch [55/87] time 0.498 (0.446) data 0.368 (0.315) loss_u loss_u 0.9961 (0.9649) acc_u 0.0000 (4.4886) lr 1.9956e-03 eta 0:00:14
epoch [8/200] batch [60/87] time 0.448 (0.445) data 0.317 (0.314) loss_u loss_u 0.9849 (0.9651) acc_u 0.0000 (4.4271) lr 1.9956e-03 eta 0:00:12
epoch [8/200] batch [65/87] time 0.384 (0.442) data 0.254 (0.312) loss_u loss_u 0.9780 (0.9657) acc_u 3.1250 (4.4231) lr 1.9956e-03 eta 0:00:09
epoch [8/200] batch [70/87] time 0.425 (0.443) data 0.295 (0.312) loss_u loss_u 0.9526 (0.9658) acc_u 6.2500 (4.3750) lr 1.9956e-03 eta 0:00:07
epoch [8/200] batch [75/87] time 0.464 (0.442) data 0.333 (0.312) loss_u loss_u 0.9688 (0.9653) acc_u 6.2500 (4.4583) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [80/87] time 0.418 (0.441) data 0.287 (0.310) loss_u loss_u 0.9844 (0.9650) acc_u 0.0000 (4.3750) lr 1.9956e-03 eta 0:00:03
epoch [8/200] batch [85/87] time 0.485 (0.442) data 0.354 (0.311) loss_u loss_u 0.9604 (0.9647) acc_u 3.1250 (4.4485) lr 1.9956e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1846
confident_label rate tensor(0.1110, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 348
clean true:337
clean false:11
clean_rate:0.9683908045977011
noisy true:953
noisy false:1835
after delete: len(clean_dataset) 348
after delete: len(noisy_dataset) 2788
epoch [9/200] batch [5/10] time 0.425 (0.490) data 0.294 (0.360) loss_x loss_x 1.5547 (1.4555) acc_x 53.1250 (58.1250) lr 1.9940e-03 eta 0:00:02
epoch [9/200] batch [10/10] time 0.515 (0.451) data 0.385 (0.320) loss_x loss_x 1.7783 (1.6425) acc_x 59.3750 (57.5000) lr 1.9940e-03 eta 0:00:00
epoch [9/200] batch [5/87] time 0.361 (0.440) data 0.230 (0.309) loss_u loss_u 0.9253 (0.9570) acc_u 9.3750 (6.2500) lr 1.9940e-03 eta 0:00:36
epoch [9/200] batch [10/87] time 0.630 (0.464) data 0.498 (0.333) loss_u loss_u 0.9883 (0.9568) acc_u 0.0000 (5.9375) lr 1.9940e-03 eta 0:00:35
epoch [9/200] batch [15/87] time 0.466 (0.461) data 0.333 (0.330) loss_u loss_u 0.9434 (0.9608) acc_u 9.3750 (5.4167) lr 1.9940e-03 eta 0:00:33
epoch [9/200] batch [20/87] time 0.390 (0.457) data 0.259 (0.326) loss_u loss_u 0.9521 (0.9634) acc_u 3.1250 (5.1562) lr 1.9940e-03 eta 0:00:30
epoch [9/200] batch [25/87] time 0.506 (0.457) data 0.374 (0.326) loss_u loss_u 0.9976 (0.9664) acc_u 0.0000 (4.7500) lr 1.9940e-03 eta 0:00:28
epoch [9/200] batch [30/87] time 0.430 (0.453) data 0.300 (0.323) loss_u loss_u 0.9761 (0.9661) acc_u 3.1250 (4.6875) lr 1.9940e-03 eta 0:00:25
epoch [9/200] batch [35/87] time 0.549 (0.458) data 0.419 (0.328) loss_u loss_u 0.9888 (0.9669) acc_u 0.0000 (4.3750) lr 1.9940e-03 eta 0:00:23
epoch [9/200] batch [40/87] time 0.421 (0.459) data 0.289 (0.329) loss_u loss_u 0.9351 (0.9664) acc_u 6.2500 (4.2969) lr 1.9940e-03 eta 0:00:21
epoch [9/200] batch [45/87] time 0.414 (0.456) data 0.282 (0.325) loss_u loss_u 0.9810 (0.9670) acc_u 0.0000 (4.2361) lr 1.9940e-03 eta 0:00:19
epoch [9/200] batch [50/87] time 0.454 (0.453) data 0.323 (0.322) loss_u loss_u 0.9409 (0.9639) acc_u 9.3750 (4.7500) lr 1.9940e-03 eta 0:00:16
epoch [9/200] batch [55/87] time 0.348 (0.449) data 0.217 (0.319) loss_u loss_u 0.9790 (0.9642) acc_u 6.2500 (4.8295) lr 1.9940e-03 eta 0:00:14
epoch [9/200] batch [60/87] time 0.451 (0.450) data 0.321 (0.319) loss_u loss_u 0.9785 (0.9634) acc_u 3.1250 (4.9479) lr 1.9940e-03 eta 0:00:12
epoch [9/200] batch [65/87] time 0.393 (0.450) data 0.260 (0.319) loss_u loss_u 0.9634 (0.9627) acc_u 3.1250 (5.0962) lr 1.9940e-03 eta 0:00:09
epoch [9/200] batch [70/87] time 0.456 (0.448) data 0.326 (0.317) loss_u loss_u 0.9644 (0.9631) acc_u 3.1250 (5.0446) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [75/87] time 0.385 (0.446) data 0.255 (0.315) loss_u loss_u 0.9448 (0.9638) acc_u 6.2500 (4.9583) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [80/87] time 0.384 (0.445) data 0.254 (0.314) loss_u loss_u 0.9897 (0.9639) acc_u 0.0000 (4.8438) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [85/87] time 0.569 (0.445) data 0.437 (0.315) loss_u loss_u 0.9727 (0.9641) acc_u 3.1250 (4.8162) lr 1.9940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1847
confident_label rate tensor(0.1148, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 360
clean true:354
clean false:6
clean_rate:0.9833333333333333
noisy true:935
noisy false:1841
after delete: len(clean_dataset) 360
after delete: len(noisy_dataset) 2776
epoch [10/200] batch [5/11] time 0.469 (0.471) data 0.339 (0.341) loss_x loss_x 1.5117 (1.6783) acc_x 65.6250 (54.3750) lr 1.9921e-03 eta 0:00:02
epoch [10/200] batch [10/11] time 0.686 (0.476) data 0.556 (0.346) loss_x loss_x 2.1543 (1.7097) acc_x 43.7500 (56.5625) lr 1.9921e-03 eta 0:00:00
epoch [10/200] batch [5/86] time 0.373 (0.475) data 0.243 (0.345) loss_u loss_u 0.9707 (0.9750) acc_u 3.1250 (3.7500) lr 1.9921e-03 eta 0:00:38
epoch [10/200] batch [10/86] time 0.447 (0.466) data 0.317 (0.336) loss_u loss_u 0.9722 (0.9686) acc_u 6.2500 (4.6875) lr 1.9921e-03 eta 0:00:35
epoch [10/200] batch [15/86] time 0.462 (0.480) data 0.331 (0.350) loss_u loss_u 0.9785 (0.9725) acc_u 6.2500 (3.9583) lr 1.9921e-03 eta 0:00:34
epoch [10/200] batch [20/86] time 0.362 (0.481) data 0.232 (0.351) loss_u loss_u 0.9932 (0.9678) acc_u 0.0000 (4.8438) lr 1.9921e-03 eta 0:00:31
epoch [10/200] batch [25/86] time 0.443 (0.474) data 0.312 (0.344) loss_u loss_u 0.9102 (0.9679) acc_u 12.5000 (4.3750) lr 1.9921e-03 eta 0:00:28
epoch [10/200] batch [30/86] time 0.468 (0.474) data 0.337 (0.343) loss_u loss_u 0.9829 (0.9689) acc_u 0.0000 (3.8542) lr 1.9921e-03 eta 0:00:26
epoch [10/200] batch [35/86] time 0.519 (0.474) data 0.389 (0.343) loss_u loss_u 0.9468 (0.9669) acc_u 6.2500 (4.0179) lr 1.9921e-03 eta 0:00:24
epoch [10/200] batch [40/86] time 0.481 (0.471) data 0.349 (0.340) loss_u loss_u 0.9458 (0.9659) acc_u 6.2500 (4.2188) lr 1.9921e-03 eta 0:00:21
epoch [10/200] batch [45/86] time 0.413 (0.465) data 0.283 (0.334) loss_u loss_u 0.9893 (0.9670) acc_u 0.0000 (4.0972) lr 1.9921e-03 eta 0:00:19
epoch [10/200] batch [50/86] time 0.439 (0.468) data 0.308 (0.337) loss_u loss_u 0.9907 (0.9676) acc_u 0.0000 (4.0000) lr 1.9921e-03 eta 0:00:16
epoch [10/200] batch [55/86] time 0.458 (0.468) data 0.328 (0.338) loss_u loss_u 0.9727 (0.9687) acc_u 3.1250 (3.8068) lr 1.9921e-03 eta 0:00:14
epoch [10/200] batch [60/86] time 0.360 (0.465) data 0.228 (0.334) loss_u loss_u 0.9570 (0.9687) acc_u 9.3750 (3.8542) lr 1.9921e-03 eta 0:00:12
epoch [10/200] batch [65/86] time 0.448 (0.463) data 0.317 (0.333) loss_u loss_u 0.9883 (0.9684) acc_u 0.0000 (3.8942) lr 1.9921e-03 eta 0:00:09
epoch [10/200] batch [70/86] time 0.470 (0.463) data 0.340 (0.332) loss_u loss_u 0.9512 (0.9674) acc_u 6.2500 (4.1518) lr 1.9921e-03 eta 0:00:07
epoch [10/200] batch [75/86] time 0.472 (0.462) data 0.342 (0.331) loss_u loss_u 0.9956 (0.9676) acc_u 0.0000 (4.0417) lr 1.9921e-03 eta 0:00:05
epoch [10/200] batch [80/86] time 0.379 (0.460) data 0.249 (0.330) loss_u loss_u 0.9702 (0.9665) acc_u 3.1250 (4.1797) lr 1.9921e-03 eta 0:00:02
epoch [10/200] batch [85/86] time 0.356 (0.459) data 0.226 (0.329) loss_u loss_u 0.9868 (0.9658) acc_u 0.0000 (4.3015) lr 1.9921e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1845
confident_label rate tensor(0.1116, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 350
clean true:344
clean false:6
clean_rate:0.9828571428571429
noisy true:947
noisy false:1839
after delete: len(clean_dataset) 350
after delete: len(noisy_dataset) 2786
epoch [11/200] batch [5/10] time 0.444 (0.453) data 0.314 (0.323) loss_x loss_x 1.9600 (1.7047) acc_x 59.3750 (61.8750) lr 1.9900e-03 eta 0:00:02
epoch [11/200] batch [10/10] time 0.460 (0.461) data 0.331 (0.331) loss_x loss_x 1.6221 (1.6943) acc_x 56.2500 (59.6875) lr 1.9900e-03 eta 0:00:00
epoch [11/200] batch [5/87] time 0.329 (0.432) data 0.199 (0.302) loss_u loss_u 0.9893 (0.9712) acc_u 3.1250 (4.3750) lr 1.9900e-03 eta 0:00:35
epoch [11/200] batch [10/87] time 0.379 (0.437) data 0.249 (0.307) loss_u loss_u 0.9600 (0.9674) acc_u 3.1250 (3.7500) lr 1.9900e-03 eta 0:00:33
epoch [11/200] batch [15/87] time 0.732 (0.444) data 0.600 (0.314) loss_u loss_u 0.9272 (0.9649) acc_u 6.2500 (4.1667) lr 1.9900e-03 eta 0:00:31
epoch [11/200] batch [20/87] time 0.406 (0.440) data 0.276 (0.310) loss_u loss_u 0.9775 (0.9600) acc_u 6.2500 (5.0000) lr 1.9900e-03 eta 0:00:29
epoch [11/200] batch [25/87] time 0.403 (0.443) data 0.273 (0.313) loss_u loss_u 0.9370 (0.9576) acc_u 6.2500 (5.2500) lr 1.9900e-03 eta 0:00:27
epoch [11/200] batch [30/87] time 0.358 (0.439) data 0.227 (0.308) loss_u loss_u 0.9556 (0.9599) acc_u 6.2500 (5.0000) lr 1.9900e-03 eta 0:00:25
epoch [11/200] batch [35/87] time 0.440 (0.439) data 0.309 (0.309) loss_u loss_u 0.9717 (0.9605) acc_u 3.1250 (5.0893) lr 1.9900e-03 eta 0:00:22
epoch [11/200] batch [40/87] time 0.439 (0.441) data 0.308 (0.310) loss_u loss_u 0.9844 (0.9602) acc_u 0.0000 (5.2344) lr 1.9900e-03 eta 0:00:20
epoch [11/200] batch [45/87] time 0.417 (0.436) data 0.286 (0.305) loss_u loss_u 0.9673 (0.9600) acc_u 0.0000 (5.1389) lr 1.9900e-03 eta 0:00:18
epoch [11/200] batch [50/87] time 0.520 (0.441) data 0.389 (0.311) loss_u loss_u 0.9873 (0.9613) acc_u 3.1250 (5.0625) lr 1.9900e-03 eta 0:00:16
epoch [11/200] batch [55/87] time 0.412 (0.440) data 0.281 (0.309) loss_u loss_u 0.9707 (0.9628) acc_u 3.1250 (4.8295) lr 1.9900e-03 eta 0:00:14
epoch [11/200] batch [60/87] time 0.386 (0.441) data 0.256 (0.310) loss_u loss_u 0.9766 (0.9641) acc_u 3.1250 (4.6354) lr 1.9900e-03 eta 0:00:11
epoch [11/200] batch [65/87] time 0.334 (0.442) data 0.202 (0.312) loss_u loss_u 0.9829 (0.9643) acc_u 3.1250 (4.6154) lr 1.9900e-03 eta 0:00:09
epoch [11/200] batch [70/87] time 0.390 (0.438) data 0.259 (0.308) loss_u loss_u 0.9897 (0.9658) acc_u 0.0000 (4.4643) lr 1.9900e-03 eta 0:00:07
epoch [11/200] batch [75/87] time 0.358 (0.440) data 0.228 (0.310) loss_u loss_u 0.8872 (0.9656) acc_u 12.5000 (4.4583) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [80/87] time 0.423 (0.441) data 0.293 (0.311) loss_u loss_u 0.9961 (0.9663) acc_u 0.0000 (4.3359) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [85/87] time 0.446 (0.441) data 0.316 (0.311) loss_u loss_u 0.9331 (0.9659) acc_u 6.2500 (4.3750) lr 1.9900e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1848
confident_label rate tensor(0.1100, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 345
clean true:339
clean false:6
clean_rate:0.9826086956521739
noisy true:949
noisy false:1842
after delete: len(clean_dataset) 345
after delete: len(noisy_dataset) 2791
epoch [12/200] batch [5/10] time 0.441 (0.463) data 0.311 (0.332) loss_x loss_x 1.6406 (1.6119) acc_x 59.3750 (60.6250) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [10/10] time 0.441 (0.499) data 0.311 (0.369) loss_x loss_x 1.4775 (1.7282) acc_x 62.5000 (59.6875) lr 1.9877e-03 eta 0:00:00
epoch [12/200] batch [5/87] time 0.523 (0.496) data 0.393 (0.365) loss_u loss_u 0.9287 (0.9513) acc_u 6.2500 (5.0000) lr 1.9877e-03 eta 0:00:40
epoch [12/200] batch [10/87] time 0.375 (0.479) data 0.244 (0.349) loss_u loss_u 0.9824 (0.9634) acc_u 3.1250 (4.0625) lr 1.9877e-03 eta 0:00:36
epoch [12/200] batch [15/87] time 0.499 (0.465) data 0.369 (0.335) loss_u loss_u 0.9702 (0.9673) acc_u 3.1250 (3.5417) lr 1.9877e-03 eta 0:00:33
epoch [12/200] batch [20/87] time 0.473 (0.468) data 0.342 (0.338) loss_u loss_u 0.9971 (0.9687) acc_u 0.0000 (3.5938) lr 1.9877e-03 eta 0:00:31
epoch [12/200] batch [25/87] time 0.448 (0.464) data 0.317 (0.333) loss_u loss_u 0.9463 (0.9696) acc_u 6.2500 (3.6250) lr 1.9877e-03 eta 0:00:28
epoch [12/200] batch [30/87] time 0.388 (0.454) data 0.256 (0.324) loss_u loss_u 0.9126 (0.9626) acc_u 12.5000 (4.7917) lr 1.9877e-03 eta 0:00:25
epoch [12/200] batch [35/87] time 0.391 (0.451) data 0.260 (0.320) loss_u loss_u 0.9678 (0.9620) acc_u 6.2500 (5.0000) lr 1.9877e-03 eta 0:00:23
epoch [12/200] batch [40/87] time 0.404 (0.445) data 0.273 (0.314) loss_u loss_u 0.9512 (0.9617) acc_u 6.2500 (5.0781) lr 1.9877e-03 eta 0:00:20
epoch [12/200] batch [45/87] time 0.458 (0.448) data 0.328 (0.317) loss_u loss_u 0.9824 (0.9622) acc_u 0.0000 (5.0000) lr 1.9877e-03 eta 0:00:18
epoch [12/200] batch [50/87] time 0.408 (0.448) data 0.276 (0.318) loss_u loss_u 0.9526 (0.9614) acc_u 6.2500 (5.0000) lr 1.9877e-03 eta 0:00:16
epoch [12/200] batch [55/87] time 0.421 (0.449) data 0.290 (0.318) loss_u loss_u 0.9648 (0.9619) acc_u 6.2500 (4.8864) lr 1.9877e-03 eta 0:00:14
epoch [12/200] batch [60/87] time 0.663 (0.457) data 0.533 (0.327) loss_u loss_u 0.9463 (0.9628) acc_u 6.2500 (4.7396) lr 1.9877e-03 eta 0:00:12
epoch [12/200] batch [65/87] time 0.445 (0.454) data 0.315 (0.324) loss_u loss_u 0.9951 (0.9624) acc_u 0.0000 (4.6635) lr 1.9877e-03 eta 0:00:09
epoch [12/200] batch [70/87] time 0.492 (0.455) data 0.362 (0.325) loss_u loss_u 0.9849 (0.9640) acc_u 3.1250 (4.4643) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [75/87] time 0.360 (0.454) data 0.230 (0.324) loss_u loss_u 0.9536 (0.9640) acc_u 6.2500 (4.4583) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [80/87] time 0.389 (0.453) data 0.258 (0.322) loss_u loss_u 0.9517 (0.9638) acc_u 6.2500 (4.5312) lr 1.9877e-03 eta 0:00:03
epoch [12/200] batch [85/87] time 0.335 (0.450) data 0.204 (0.320) loss_u loss_u 0.9717 (0.9640) acc_u 3.1250 (4.5588) lr 1.9877e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1850
confident_label rate tensor(0.1097, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 344
clean true:337
clean false:7
clean_rate:0.9796511627906976
noisy true:949
noisy false:1843
after delete: len(clean_dataset) 344
after delete: len(noisy_dataset) 2792
epoch [13/200] batch [5/10] time 0.462 (0.466) data 0.332 (0.336) loss_x loss_x 1.5166 (1.3199) acc_x 71.8750 (65.6250) lr 1.9851e-03 eta 0:00:02
epoch [13/200] batch [10/10] time 0.488 (0.494) data 0.357 (0.364) loss_x loss_x 1.9561 (1.5297) acc_x 53.1250 (60.9375) lr 1.9851e-03 eta 0:00:00
epoch [13/200] batch [5/87] time 0.368 (0.454) data 0.238 (0.324) loss_u loss_u 0.9692 (0.9550) acc_u 6.2500 (5.0000) lr 1.9851e-03 eta 0:00:37
epoch [13/200] batch [10/87] time 0.525 (0.466) data 0.394 (0.336) loss_u loss_u 0.8926 (0.9495) acc_u 18.7500 (6.5625) lr 1.9851e-03 eta 0:00:35
epoch [13/200] batch [15/87] time 0.408 (0.453) data 0.278 (0.322) loss_u loss_u 0.9526 (0.9526) acc_u 6.2500 (6.4583) lr 1.9851e-03 eta 0:00:32
epoch [13/200] batch [20/87] time 0.526 (0.456) data 0.396 (0.325) loss_u loss_u 0.9902 (0.9548) acc_u 0.0000 (5.9375) lr 1.9851e-03 eta 0:00:30
epoch [13/200] batch [25/87] time 0.412 (0.455) data 0.281 (0.325) loss_u loss_u 0.9780 (0.9589) acc_u 6.2500 (5.3750) lr 1.9851e-03 eta 0:00:28
epoch [13/200] batch [30/87] time 0.473 (0.452) data 0.341 (0.322) loss_u loss_u 0.9761 (0.9587) acc_u 3.1250 (5.3125) lr 1.9851e-03 eta 0:00:25
epoch [13/200] batch [35/87] time 0.522 (0.454) data 0.391 (0.323) loss_u loss_u 0.9424 (0.9583) acc_u 9.3750 (5.2679) lr 1.9851e-03 eta 0:00:23
epoch [13/200] batch [40/87] time 0.384 (0.451) data 0.253 (0.321) loss_u loss_u 0.9629 (0.9566) acc_u 6.2500 (5.4688) lr 1.9851e-03 eta 0:00:21
epoch [13/200] batch [45/87] time 0.419 (0.453) data 0.289 (0.322) loss_u loss_u 0.9775 (0.9593) acc_u 3.1250 (5.1389) lr 1.9851e-03 eta 0:00:19
epoch [13/200] batch [50/87] time 0.394 (0.450) data 0.263 (0.319) loss_u loss_u 0.9663 (0.9597) acc_u 6.2500 (5.1875) lr 1.9851e-03 eta 0:00:16
epoch [13/200] batch [55/87] time 0.437 (0.450) data 0.307 (0.319) loss_u loss_u 0.9507 (0.9615) acc_u 6.2500 (4.8864) lr 1.9851e-03 eta 0:00:14
epoch [13/200] batch [60/87] time 0.354 (0.447) data 0.224 (0.316) loss_u loss_u 0.9580 (0.9629) acc_u 9.3750 (4.8438) lr 1.9851e-03 eta 0:00:12
epoch [13/200] batch [65/87] time 0.463 (0.447) data 0.332 (0.316) loss_u loss_u 0.9736 (0.9616) acc_u 3.1250 (5.0000) lr 1.9851e-03 eta 0:00:09
epoch [13/200] batch [70/87] time 0.458 (0.447) data 0.327 (0.316) loss_u loss_u 0.9800 (0.9626) acc_u 0.0000 (4.7768) lr 1.9851e-03 eta 0:00:07
epoch [13/200] batch [75/87] time 0.497 (0.444) data 0.367 (0.314) loss_u loss_u 0.9966 (0.9625) acc_u 0.0000 (4.8333) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [80/87] time 0.473 (0.443) data 0.342 (0.312) loss_u loss_u 0.9668 (0.9636) acc_u 6.2500 (4.6875) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [85/87] time 0.538 (0.444) data 0.408 (0.313) loss_u loss_u 0.9927 (0.9640) acc_u 3.1250 (4.6324) lr 1.9851e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1814
confident_label rate tensor(0.1129, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 354
clean true:349
clean false:5
clean_rate:0.9858757062146892
noisy true:973
noisy false:1809
after delete: len(clean_dataset) 354
after delete: len(noisy_dataset) 2782
epoch [14/200] batch [5/11] time 0.615 (0.468) data 0.485 (0.337) loss_x loss_x 1.6572 (1.7209) acc_x 62.5000 (55.6250) lr 1.9823e-03 eta 0:00:02
epoch [14/200] batch [10/11] time 0.581 (0.457) data 0.450 (0.326) loss_x loss_x 1.4531 (1.6262) acc_x 75.0000 (58.1250) lr 1.9823e-03 eta 0:00:00
epoch [14/200] batch [5/86] time 0.537 (0.471) data 0.407 (0.340) loss_u loss_u 0.9741 (0.9712) acc_u 3.1250 (5.6250) lr 1.9823e-03 eta 0:00:38
epoch [14/200] batch [10/86] time 0.398 (0.455) data 0.268 (0.324) loss_u loss_u 0.9834 (0.9606) acc_u 3.1250 (6.2500) lr 1.9823e-03 eta 0:00:34
epoch [14/200] batch [15/86] time 0.367 (0.455) data 0.236 (0.324) loss_u loss_u 0.9072 (0.9616) acc_u 12.5000 (5.8333) lr 1.9823e-03 eta 0:00:32
epoch [14/200] batch [20/86] time 0.443 (0.458) data 0.312 (0.327) loss_u loss_u 0.9316 (0.9588) acc_u 9.3750 (6.2500) lr 1.9823e-03 eta 0:00:30
epoch [14/200] batch [25/86] time 0.412 (0.451) data 0.281 (0.320) loss_u loss_u 0.9707 (0.9603) acc_u 0.0000 (5.6250) lr 1.9823e-03 eta 0:00:27
epoch [14/200] batch [30/86] time 0.501 (0.453) data 0.370 (0.322) loss_u loss_u 0.9639 (0.9640) acc_u 3.1250 (5.0000) lr 1.9823e-03 eta 0:00:25
epoch [14/200] batch [35/86] time 0.427 (0.456) data 0.296 (0.326) loss_u loss_u 0.9873 (0.9663) acc_u 0.0000 (4.7321) lr 1.9823e-03 eta 0:00:23
epoch [14/200] batch [40/86] time 0.485 (0.459) data 0.354 (0.328) loss_u loss_u 0.9673 (0.9677) acc_u 0.0000 (4.4531) lr 1.9823e-03 eta 0:00:21
epoch [14/200] batch [45/86] time 0.429 (0.459) data 0.299 (0.328) loss_u loss_u 0.9639 (0.9673) acc_u 3.1250 (4.3750) lr 1.9823e-03 eta 0:00:18
epoch [14/200] batch [50/86] time 0.397 (0.464) data 0.267 (0.333) loss_u loss_u 0.9541 (0.9661) acc_u 6.2500 (4.5625) lr 1.9823e-03 eta 0:00:16
epoch [14/200] batch [55/86] time 0.373 (0.459) data 0.242 (0.328) loss_u loss_u 0.9946 (0.9657) acc_u 0.0000 (4.6023) lr 1.9823e-03 eta 0:00:14
epoch [14/200] batch [60/86] time 0.435 (0.467) data 0.305 (0.336) loss_u loss_u 0.9766 (0.9649) acc_u 3.1250 (4.7396) lr 1.9823e-03 eta 0:00:12
epoch [14/200] batch [65/86] time 0.362 (0.465) data 0.230 (0.335) loss_u loss_u 0.9512 (0.9653) acc_u 3.1250 (4.5192) lr 1.9823e-03 eta 0:00:09
epoch [14/200] batch [70/86] time 0.552 (0.463) data 0.421 (0.332) loss_u loss_u 0.9644 (0.9648) acc_u 3.1250 (4.5536) lr 1.9823e-03 eta 0:00:07
epoch [14/200] batch [75/86] time 0.339 (0.459) data 0.205 (0.328) loss_u loss_u 0.9766 (0.9653) acc_u 6.2500 (4.5417) lr 1.9823e-03 eta 0:00:05
epoch [14/200] batch [80/86] time 0.475 (0.456) data 0.345 (0.325) loss_u loss_u 0.9902 (0.9655) acc_u 0.0000 (4.5703) lr 1.9823e-03 eta 0:00:02
epoch [14/200] batch [85/86] time 0.617 (0.455) data 0.488 (0.324) loss_u loss_u 0.9785 (0.9657) acc_u 3.1250 (4.5588) lr 1.9823e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1876
confident_label rate tensor(0.1081, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 339
clean true:329
clean false:10
clean_rate:0.9705014749262537
noisy true:931
noisy false:1866
after delete: len(clean_dataset) 339
after delete: len(noisy_dataset) 2797
epoch [15/200] batch [5/10] time 0.395 (0.432) data 0.265 (0.301) loss_x loss_x 1.4014 (1.6381) acc_x 71.8750 (57.5000) lr 1.9792e-03 eta 0:00:02
epoch [15/200] batch [10/10] time 0.403 (0.469) data 0.272 (0.339) loss_x loss_x 1.5586 (1.5600) acc_x 46.8750 (55.6250) lr 1.9792e-03 eta 0:00:00
epoch [15/200] batch [5/87] time 0.392 (0.447) data 0.261 (0.316) loss_u loss_u 0.9722 (0.9522) acc_u 6.2500 (6.2500) lr 1.9792e-03 eta 0:00:36
epoch [15/200] batch [10/87] time 0.366 (0.436) data 0.234 (0.305) loss_u loss_u 0.9756 (0.9526) acc_u 6.2500 (5.9375) lr 1.9792e-03 eta 0:00:33
epoch [15/200] batch [15/87] time 0.492 (0.433) data 0.361 (0.302) loss_u loss_u 0.9468 (0.9534) acc_u 6.2500 (5.8333) lr 1.9792e-03 eta 0:00:31
epoch [15/200] batch [20/87] time 0.558 (0.443) data 0.426 (0.313) loss_u loss_u 0.9795 (0.9538) acc_u 0.0000 (5.9375) lr 1.9792e-03 eta 0:00:29
epoch [15/200] batch [25/87] time 0.427 (0.450) data 0.296 (0.319) loss_u loss_u 0.9951 (0.9608) acc_u 0.0000 (5.0000) lr 1.9792e-03 eta 0:00:27
epoch [15/200] batch [30/87] time 0.437 (0.451) data 0.306 (0.320) loss_u loss_u 0.9634 (0.9598) acc_u 6.2500 (5.2083) lr 1.9792e-03 eta 0:00:25
epoch [15/200] batch [35/87] time 0.383 (0.452) data 0.253 (0.321) loss_u loss_u 0.9404 (0.9606) acc_u 6.2500 (5.0000) lr 1.9792e-03 eta 0:00:23
epoch [15/200] batch [40/87] time 0.358 (0.452) data 0.227 (0.321) loss_u loss_u 0.9609 (0.9612) acc_u 9.3750 (5.0781) lr 1.9792e-03 eta 0:00:21
epoch [15/200] batch [45/87] time 0.444 (0.457) data 0.313 (0.326) loss_u loss_u 0.9751 (0.9626) acc_u 0.0000 (4.7917) lr 1.9792e-03 eta 0:00:19
epoch [15/200] batch [50/87] time 0.440 (0.464) data 0.309 (0.333) loss_u loss_u 0.9741 (0.9635) acc_u 3.1250 (4.6250) lr 1.9792e-03 eta 0:00:17
epoch [15/200] batch [55/87] time 0.435 (0.460) data 0.305 (0.329) loss_u loss_u 0.9590 (0.9634) acc_u 3.1250 (4.6023) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [60/87] time 0.400 (0.458) data 0.269 (0.327) loss_u loss_u 0.9336 (0.9638) acc_u 9.3750 (4.4792) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [65/87] time 0.347 (0.454) data 0.217 (0.323) loss_u loss_u 0.9321 (0.9638) acc_u 9.3750 (4.5192) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [70/87] time 0.430 (0.452) data 0.299 (0.321) loss_u loss_u 0.9609 (0.9645) acc_u 3.1250 (4.4643) lr 1.9792e-03 eta 0:00:07
epoch [15/200] batch [75/87] time 0.429 (0.448) data 0.298 (0.317) loss_u loss_u 0.9165 (0.9639) acc_u 12.5000 (4.6250) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [80/87] time 0.347 (0.445) data 0.217 (0.314) loss_u loss_u 0.9385 (0.9641) acc_u 9.3750 (4.6484) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [85/87] time 0.561 (0.445) data 0.431 (0.314) loss_u loss_u 0.9419 (0.9642) acc_u 9.3750 (4.6324) lr 1.9792e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1833
confident_label rate tensor(0.1052, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 330
clean true:324
clean false:6
clean_rate:0.9818181818181818
noisy true:979
noisy false:1827
after delete: len(clean_dataset) 330
after delete: len(noisy_dataset) 2806
epoch [16/200] batch [5/10] time 0.532 (0.464) data 0.401 (0.334) loss_x loss_x 1.1846 (1.2363) acc_x 78.1250 (70.6250) lr 1.9759e-03 eta 0:00:02
epoch [16/200] batch [10/10] time 0.390 (0.471) data 0.260 (0.341) loss_x loss_x 1.9766 (1.4878) acc_x 46.8750 (66.5625) lr 1.9759e-03 eta 0:00:00
epoch [16/200] batch [5/87] time 0.489 (0.460) data 0.359 (0.330) loss_u loss_u 0.9512 (0.9720) acc_u 6.2500 (4.3750) lr 1.9759e-03 eta 0:00:37
epoch [16/200] batch [10/87] time 0.405 (0.450) data 0.274 (0.319) loss_u loss_u 0.9453 (0.9652) acc_u 9.3750 (5.6250) lr 1.9759e-03 eta 0:00:34
epoch [16/200] batch [15/87] time 0.448 (0.456) data 0.317 (0.326) loss_u loss_u 0.9648 (0.9652) acc_u 3.1250 (5.2083) lr 1.9759e-03 eta 0:00:32
epoch [16/200] batch [20/87] time 0.447 (0.455) data 0.316 (0.324) loss_u loss_u 0.8945 (0.9634) acc_u 15.6250 (5.0000) lr 1.9759e-03 eta 0:00:30
epoch [16/200] batch [25/87] time 0.449 (0.450) data 0.319 (0.320) loss_u loss_u 0.9238 (0.9634) acc_u 6.2500 (4.7500) lr 1.9759e-03 eta 0:00:27
epoch [16/200] batch [30/87] time 0.478 (0.449) data 0.348 (0.318) loss_u loss_u 0.9741 (0.9646) acc_u 3.1250 (4.5833) lr 1.9759e-03 eta 0:00:25
epoch [16/200] batch [35/87] time 0.345 (0.447) data 0.214 (0.317) loss_u loss_u 0.9824 (0.9671) acc_u 0.0000 (4.1964) lr 1.9759e-03 eta 0:00:23
epoch [16/200] batch [40/87] time 0.437 (0.446) data 0.307 (0.315) loss_u loss_u 0.9761 (0.9664) acc_u 6.2500 (4.3750) lr 1.9759e-03 eta 0:00:20
epoch [16/200] batch [45/87] time 0.522 (0.449) data 0.392 (0.318) loss_u loss_u 0.9810 (0.9651) acc_u 3.1250 (4.5833) lr 1.9759e-03 eta 0:00:18
epoch [16/200] batch [50/87] time 0.382 (0.447) data 0.251 (0.317) loss_u loss_u 0.9414 (0.9637) acc_u 9.3750 (4.7500) lr 1.9759e-03 eta 0:00:16
epoch [16/200] batch [55/87] time 0.396 (0.443) data 0.266 (0.312) loss_u loss_u 0.9785 (0.9646) acc_u 3.1250 (4.7159) lr 1.9759e-03 eta 0:00:14
epoch [16/200] batch [60/87] time 0.561 (0.443) data 0.429 (0.313) loss_u loss_u 0.9614 (0.9648) acc_u 6.2500 (4.6354) lr 1.9759e-03 eta 0:00:11
epoch [16/200] batch [65/87] time 0.419 (0.449) data 0.289 (0.318) loss_u loss_u 0.9907 (0.9645) acc_u 0.0000 (4.7115) lr 1.9759e-03 eta 0:00:09
epoch [16/200] batch [70/87] time 0.444 (0.452) data 0.312 (0.322) loss_u loss_u 0.9634 (0.9644) acc_u 9.3750 (4.7768) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [75/87] time 0.564 (0.453) data 0.434 (0.323) loss_u loss_u 0.9790 (0.9632) acc_u 3.1250 (4.9583) lr 1.9759e-03 eta 0:00:05
epoch [16/200] batch [80/87] time 0.390 (0.448) data 0.258 (0.317) loss_u loss_u 0.9775 (0.9623) acc_u 3.1250 (5.0781) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [85/87] time 0.527 (0.449) data 0.395 (0.318) loss_u loss_u 0.9575 (0.9623) acc_u 6.2500 (5.0368) lr 1.9759e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1834
confident_label rate tensor(0.1122, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 352
clean true:343
clean false:9
clean_rate:0.9744318181818182
noisy true:959
noisy false:1825
after delete: len(clean_dataset) 352
after delete: len(noisy_dataset) 2784
epoch [17/200] batch [5/11] time 0.412 (0.433) data 0.282 (0.303) loss_x loss_x 1.3018 (1.4803) acc_x 59.3750 (61.8750) lr 1.9724e-03 eta 0:00:02
epoch [17/200] batch [10/11] time 0.478 (0.460) data 0.348 (0.330) loss_x loss_x 1.8301 (1.6342) acc_x 56.2500 (60.3125) lr 1.9724e-03 eta 0:00:00
epoch [17/200] batch [5/87] time 0.569 (0.466) data 0.437 (0.336) loss_u loss_u 0.9546 (0.9433) acc_u 6.2500 (8.7500) lr 1.9724e-03 eta 0:00:38
epoch [17/200] batch [10/87] time 0.587 (0.470) data 0.457 (0.340) loss_u loss_u 0.9263 (0.9568) acc_u 15.6250 (6.8750) lr 1.9724e-03 eta 0:00:36
epoch [17/200] batch [15/87] time 0.566 (0.474) data 0.436 (0.343) loss_u loss_u 0.9448 (0.9566) acc_u 6.2500 (6.8750) lr 1.9724e-03 eta 0:00:34
epoch [17/200] batch [20/87] time 0.734 (0.469) data 0.603 (0.339) loss_u loss_u 0.9497 (0.9609) acc_u 3.1250 (5.7812) lr 1.9724e-03 eta 0:00:31
epoch [17/200] batch [25/87] time 0.318 (0.456) data 0.186 (0.326) loss_u loss_u 0.9976 (0.9632) acc_u 0.0000 (5.5000) lr 1.9724e-03 eta 0:00:28
epoch [17/200] batch [30/87] time 0.430 (0.457) data 0.299 (0.326) loss_u loss_u 0.9902 (0.9644) acc_u 0.0000 (5.2083) lr 1.9724e-03 eta 0:00:26
epoch [17/200] batch [35/87] time 0.408 (0.453) data 0.277 (0.322) loss_u loss_u 0.9805 (0.9654) acc_u 3.1250 (5.0893) lr 1.9724e-03 eta 0:00:23
epoch [17/200] batch [40/87] time 0.474 (0.455) data 0.342 (0.324) loss_u loss_u 0.9497 (0.9643) acc_u 6.2500 (5.1562) lr 1.9724e-03 eta 0:00:21
epoch [17/200] batch [45/87] time 0.477 (0.458) data 0.345 (0.327) loss_u loss_u 0.9570 (0.9623) acc_u 6.2500 (5.2778) lr 1.9724e-03 eta 0:00:19
epoch [17/200] batch [50/87] time 0.451 (0.455) data 0.319 (0.324) loss_u loss_u 0.9741 (0.9621) acc_u 3.1250 (5.1250) lr 1.9724e-03 eta 0:00:16
epoch [17/200] batch [55/87] time 0.469 (0.457) data 0.338 (0.326) loss_u loss_u 0.9766 (0.9633) acc_u 3.1250 (4.9432) lr 1.9724e-03 eta 0:00:14
epoch [17/200] batch [60/87] time 0.504 (0.455) data 0.373 (0.324) loss_u loss_u 0.9644 (0.9643) acc_u 6.2500 (4.7917) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [65/87] time 0.378 (0.454) data 0.247 (0.323) loss_u loss_u 0.9678 (0.9644) acc_u 3.1250 (4.7596) lr 1.9724e-03 eta 0:00:09
epoch [17/200] batch [70/87] time 0.416 (0.452) data 0.285 (0.321) loss_u loss_u 0.9448 (0.9639) acc_u 6.2500 (4.8214) lr 1.9724e-03 eta 0:00:07
epoch [17/200] batch [75/87] time 0.397 (0.451) data 0.267 (0.320) loss_u loss_u 0.9854 (0.9641) acc_u 3.1250 (4.7917) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [80/87] time 0.429 (0.451) data 0.299 (0.320) loss_u loss_u 0.9609 (0.9642) acc_u 3.1250 (4.8047) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [85/87] time 0.517 (0.449) data 0.386 (0.319) loss_u loss_u 0.9771 (0.9638) acc_u 0.0000 (4.8162) lr 1.9724e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1908
confident_label rate tensor(0.1027, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 322
clean true:317
clean false:5
clean_rate:0.984472049689441
noisy true:911
noisy false:1903
after delete: len(clean_dataset) 322
after delete: len(noisy_dataset) 2814
epoch [18/200] batch [5/10] time 0.394 (0.450) data 0.263 (0.320) loss_x loss_x 1.8828 (1.4781) acc_x 53.1250 (61.8750) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [10/10] time 0.586 (0.479) data 0.456 (0.349) loss_x loss_x 1.5850 (1.4834) acc_x 59.3750 (63.7500) lr 1.9686e-03 eta 0:00:00
epoch [18/200] batch [5/87] time 0.358 (0.449) data 0.227 (0.318) loss_u loss_u 0.9775 (0.9723) acc_u 3.1250 (3.7500) lr 1.9686e-03 eta 0:00:36
epoch [18/200] batch [10/87] time 0.428 (0.450) data 0.297 (0.319) loss_u loss_u 0.9194 (0.9606) acc_u 12.5000 (5.9375) lr 1.9686e-03 eta 0:00:34
epoch [18/200] batch [15/87] time 0.584 (0.452) data 0.453 (0.321) loss_u loss_u 0.9688 (0.9623) acc_u 6.2500 (5.0000) lr 1.9686e-03 eta 0:00:32
epoch [18/200] batch [20/87] time 0.380 (0.451) data 0.250 (0.321) loss_u loss_u 0.9453 (0.9568) acc_u 9.3750 (5.9375) lr 1.9686e-03 eta 0:00:30
epoch [18/200] batch [25/87] time 0.336 (0.451) data 0.205 (0.320) loss_u loss_u 0.9551 (0.9565) acc_u 6.2500 (5.7500) lr 1.9686e-03 eta 0:00:27
epoch [18/200] batch [30/87] time 0.457 (0.450) data 0.326 (0.319) loss_u loss_u 0.9761 (0.9549) acc_u 3.1250 (6.0417) lr 1.9686e-03 eta 0:00:25
epoch [18/200] batch [35/87] time 0.559 (0.451) data 0.427 (0.320) loss_u loss_u 0.9712 (0.9575) acc_u 3.1250 (5.8929) lr 1.9686e-03 eta 0:00:23
epoch [18/200] batch [40/87] time 0.361 (0.458) data 0.230 (0.327) loss_u loss_u 0.9536 (0.9571) acc_u 6.2500 (6.0938) lr 1.9686e-03 eta 0:00:21
epoch [18/200] batch [45/87] time 0.491 (0.460) data 0.360 (0.329) loss_u loss_u 0.9292 (0.9537) acc_u 9.3750 (6.3889) lr 1.9686e-03 eta 0:00:19
epoch [18/200] batch [50/87] time 0.348 (0.456) data 0.217 (0.326) loss_u loss_u 0.9434 (0.9544) acc_u 6.2500 (6.2500) lr 1.9686e-03 eta 0:00:16
epoch [18/200] batch [55/87] time 0.312 (0.456) data 0.182 (0.325) loss_u loss_u 0.9551 (0.9548) acc_u 6.2500 (6.1364) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [60/87] time 0.406 (0.452) data 0.276 (0.321) loss_u loss_u 0.9629 (0.9556) acc_u 3.1250 (6.1458) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [65/87] time 0.451 (0.450) data 0.320 (0.320) loss_u loss_u 0.9771 (0.9561) acc_u 0.0000 (6.0096) lr 1.9686e-03 eta 0:00:09
epoch [18/200] batch [70/87] time 0.419 (0.448) data 0.288 (0.317) loss_u loss_u 0.9717 (0.9568) acc_u 9.3750 (6.0714) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [75/87] time 0.376 (0.448) data 0.245 (0.317) loss_u loss_u 0.9893 (0.9573) acc_u 0.0000 (5.8750) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [80/87] time 0.424 (0.446) data 0.293 (0.315) loss_u loss_u 0.9443 (0.9572) acc_u 6.2500 (5.8203) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [85/87] time 0.549 (0.446) data 0.418 (0.315) loss_u loss_u 0.9604 (0.9569) acc_u 9.3750 (5.8088) lr 1.9686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1831
confident_label rate tensor(0.1142, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 358
clean true:345
clean false:13
clean_rate:0.9636871508379888
noisy true:960
noisy false:1818
after delete: len(clean_dataset) 358
after delete: len(noisy_dataset) 2778
epoch [19/200] batch [5/11] time 0.449 (0.487) data 0.319 (0.357) loss_x loss_x 1.2090 (1.4713) acc_x 68.7500 (63.7500) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [10/11] time 0.538 (0.484) data 0.408 (0.354) loss_x loss_x 1.5439 (1.4929) acc_x 59.3750 (64.0625) lr 1.9646e-03 eta 0:00:00
epoch [19/200] batch [5/86] time 0.402 (0.447) data 0.271 (0.316) loss_u loss_u 0.9922 (0.9593) acc_u 0.0000 (4.3750) lr 1.9646e-03 eta 0:00:36
epoch [19/200] batch [10/86] time 0.434 (0.443) data 0.303 (0.312) loss_u loss_u 0.9590 (0.9587) acc_u 3.1250 (5.3125) lr 1.9646e-03 eta 0:00:33
epoch [19/200] batch [15/86] time 0.400 (0.438) data 0.269 (0.307) loss_u loss_u 0.9902 (0.9663) acc_u 0.0000 (4.1667) lr 1.9646e-03 eta 0:00:31
epoch [19/200] batch [20/86] time 0.374 (0.432) data 0.242 (0.301) loss_u loss_u 0.9756 (0.9628) acc_u 3.1250 (5.0000) lr 1.9646e-03 eta 0:00:28
epoch [19/200] batch [25/86] time 0.390 (0.436) data 0.258 (0.305) loss_u loss_u 0.9404 (0.9615) acc_u 12.5000 (5.0000) lr 1.9646e-03 eta 0:00:26
epoch [19/200] batch [30/86] time 0.375 (0.435) data 0.245 (0.304) loss_u loss_u 0.9517 (0.9626) acc_u 3.1250 (4.6875) lr 1.9646e-03 eta 0:00:24
epoch [19/200] batch [35/86] time 0.458 (0.438) data 0.328 (0.307) loss_u loss_u 0.9712 (0.9641) acc_u 0.0000 (4.3750) lr 1.9646e-03 eta 0:00:22
epoch [19/200] batch [40/86] time 0.592 (0.441) data 0.460 (0.310) loss_u loss_u 0.9810 (0.9642) acc_u 0.0000 (4.2188) lr 1.9646e-03 eta 0:00:20
epoch [19/200] batch [45/86] time 0.389 (0.435) data 0.257 (0.304) loss_u loss_u 0.9478 (0.9645) acc_u 6.2500 (4.2361) lr 1.9646e-03 eta 0:00:17
epoch [19/200] batch [50/86] time 0.344 (0.439) data 0.212 (0.308) loss_u loss_u 0.9224 (0.9633) acc_u 6.2500 (4.3750) lr 1.9646e-03 eta 0:00:15
epoch [19/200] batch [55/86] time 0.361 (0.438) data 0.230 (0.307) loss_u loss_u 0.9775 (0.9640) acc_u 3.1250 (4.3182) lr 1.9646e-03 eta 0:00:13
epoch [19/200] batch [60/86] time 0.421 (0.441) data 0.290 (0.310) loss_u loss_u 0.9863 (0.9646) acc_u 0.0000 (4.3229) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [65/86] time 0.361 (0.440) data 0.230 (0.309) loss_u loss_u 0.9727 (0.9643) acc_u 6.2500 (4.3750) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [70/86] time 0.406 (0.441) data 0.276 (0.310) loss_u loss_u 0.9678 (0.9640) acc_u 9.3750 (4.5089) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [75/86] time 0.475 (0.446) data 0.344 (0.315) loss_u loss_u 0.9761 (0.9645) acc_u 3.1250 (4.4167) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [80/86] time 0.467 (0.446) data 0.336 (0.315) loss_u loss_u 0.9248 (0.9644) acc_u 12.5000 (4.4531) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [85/86] time 0.369 (0.445) data 0.237 (0.314) loss_u loss_u 0.9541 (0.9631) acc_u 3.1250 (4.5588) lr 1.9646e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1871
confident_label rate tensor(0.1081, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 339
clean true:332
clean false:7
clean_rate:0.9793510324483776
noisy true:933
noisy false:1864
after delete: len(clean_dataset) 339
after delete: len(noisy_dataset) 2797
epoch [20/200] batch [5/10] time 0.479 (0.451) data 0.349 (0.321) loss_x loss_x 1.7744 (1.5576) acc_x 50.0000 (60.0000) lr 1.9603e-03 eta 0:00:02
epoch [20/200] batch [10/10] time 0.449 (0.459) data 0.319 (0.329) loss_x loss_x 1.4385 (1.4563) acc_x 68.7500 (61.2500) lr 1.9603e-03 eta 0:00:00
epoch [20/200] batch [5/87] time 0.463 (0.449) data 0.333 (0.319) loss_u loss_u 0.9429 (0.9661) acc_u 9.3750 (5.6250) lr 1.9603e-03 eta 0:00:36
epoch [20/200] batch [10/87] time 0.523 (0.466) data 0.391 (0.335) loss_u loss_u 0.9639 (0.9708) acc_u 6.2500 (4.6875) lr 1.9603e-03 eta 0:00:35
epoch [20/200] batch [15/87] time 0.504 (0.464) data 0.373 (0.333) loss_u loss_u 0.9868 (0.9686) acc_u 0.0000 (4.7917) lr 1.9603e-03 eta 0:00:33
epoch [20/200] batch [20/87] time 0.491 (0.473) data 0.361 (0.342) loss_u loss_u 0.9976 (0.9720) acc_u 0.0000 (4.0625) lr 1.9603e-03 eta 0:00:31
epoch [20/200] batch [25/87] time 0.366 (0.463) data 0.236 (0.332) loss_u loss_u 0.9648 (0.9692) acc_u 3.1250 (4.0000) lr 1.9603e-03 eta 0:00:28
epoch [20/200] batch [30/87] time 0.378 (0.454) data 0.247 (0.323) loss_u loss_u 0.9097 (0.9682) acc_u 9.3750 (3.9583) lr 1.9603e-03 eta 0:00:25
epoch [20/200] batch [35/87] time 0.415 (0.450) data 0.283 (0.319) loss_u loss_u 0.9736 (0.9674) acc_u 3.1250 (4.3750) lr 1.9603e-03 eta 0:00:23
epoch [20/200] batch [40/87] time 0.425 (0.450) data 0.294 (0.319) loss_u loss_u 0.9897 (0.9693) acc_u 0.0000 (3.9844) lr 1.9603e-03 eta 0:00:21
epoch [20/200] batch [45/87] time 0.499 (0.454) data 0.368 (0.324) loss_u loss_u 0.9785 (0.9687) acc_u 3.1250 (3.9583) lr 1.9603e-03 eta 0:00:19
epoch [20/200] batch [50/87] time 0.422 (0.453) data 0.291 (0.322) loss_u loss_u 0.9399 (0.9670) acc_u 9.3750 (4.2500) lr 1.9603e-03 eta 0:00:16
epoch [20/200] batch [55/87] time 0.414 (0.452) data 0.283 (0.321) loss_u loss_u 0.9048 (0.9655) acc_u 12.5000 (4.4318) lr 1.9603e-03 eta 0:00:14
epoch [20/200] batch [60/87] time 0.344 (0.451) data 0.214 (0.320) loss_u loss_u 0.9893 (0.9641) acc_u 3.1250 (4.6875) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [65/87] time 0.417 (0.450) data 0.287 (0.319) loss_u loss_u 0.9194 (0.9634) acc_u 12.5000 (4.7596) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [70/87] time 0.348 (0.447) data 0.217 (0.317) loss_u loss_u 0.9263 (0.9628) acc_u 9.3750 (4.9107) lr 1.9603e-03 eta 0:00:07
epoch [20/200] batch [75/87] time 0.704 (0.450) data 0.573 (0.319) loss_u loss_u 0.9492 (0.9633) acc_u 3.1250 (4.8333) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [80/87] time 0.491 (0.448) data 0.360 (0.317) loss_u loss_u 0.9937 (0.9631) acc_u 0.0000 (4.8438) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [85/87] time 0.383 (0.448) data 0.252 (0.317) loss_u loss_u 0.9404 (0.9633) acc_u 6.2500 (4.8162) lr 1.9603e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1874
confident_label rate tensor(0.1091, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 342
clean true:331
clean false:11
clean_rate:0.9678362573099415
noisy true:931
noisy false:1863
after delete: len(clean_dataset) 342
after delete: len(noisy_dataset) 2794
epoch [21/200] batch [5/10] time 0.429 (0.495) data 0.299 (0.365) loss_x loss_x 1.2178 (1.4117) acc_x 78.1250 (68.7500) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [10/10] time 0.406 (0.477) data 0.277 (0.347) loss_x loss_x 1.5859 (1.6396) acc_x 53.1250 (60.3125) lr 1.9558e-03 eta 0:00:00
epoch [21/200] batch [5/87] time 0.504 (0.490) data 0.373 (0.359) loss_u loss_u 0.9902 (0.9667) acc_u 3.1250 (6.2500) lr 1.9558e-03 eta 0:00:40
epoch [21/200] batch [10/87] time 0.329 (0.464) data 0.199 (0.334) loss_u loss_u 0.9951 (0.9669) acc_u 0.0000 (5.0000) lr 1.9558e-03 eta 0:00:35
epoch [21/200] batch [15/87] time 0.396 (0.454) data 0.265 (0.323) loss_u loss_u 0.9575 (0.9692) acc_u 6.2500 (4.3750) lr 1.9558e-03 eta 0:00:32
epoch [21/200] batch [20/87] time 0.521 (0.459) data 0.391 (0.329) loss_u loss_u 0.9497 (0.9661) acc_u 6.2500 (4.6875) lr 1.9558e-03 eta 0:00:30
epoch [21/200] batch [25/87] time 0.455 (0.472) data 0.324 (0.341) loss_u loss_u 0.9775 (0.9688) acc_u 3.1250 (4.2500) lr 1.9558e-03 eta 0:00:29
epoch [21/200] batch [30/87] time 0.519 (0.465) data 0.388 (0.335) loss_u loss_u 0.9683 (0.9694) acc_u 3.1250 (4.1667) lr 1.9558e-03 eta 0:00:26
epoch [21/200] batch [35/87] time 0.365 (0.467) data 0.234 (0.336) loss_u loss_u 0.9351 (0.9649) acc_u 12.5000 (4.7321) lr 1.9558e-03 eta 0:00:24
epoch [21/200] batch [40/87] time 0.380 (0.465) data 0.250 (0.334) loss_u loss_u 0.9717 (0.9638) acc_u 6.2500 (4.9219) lr 1.9558e-03 eta 0:00:21
epoch [21/200] batch [45/87] time 0.407 (0.464) data 0.276 (0.334) loss_u loss_u 0.9922 (0.9641) acc_u 0.0000 (4.9306) lr 1.9558e-03 eta 0:00:19
epoch [21/200] batch [50/87] time 0.433 (0.463) data 0.302 (0.332) loss_u loss_u 0.9238 (0.9642) acc_u 9.3750 (4.8125) lr 1.9558e-03 eta 0:00:17
epoch [21/200] batch [55/87] time 0.505 (0.462) data 0.373 (0.332) loss_u loss_u 0.9194 (0.9641) acc_u 12.5000 (4.8295) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [60/87] time 0.377 (0.462) data 0.247 (0.331) loss_u loss_u 0.9165 (0.9632) acc_u 12.5000 (5.0000) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [65/87] time 0.440 (0.460) data 0.309 (0.329) loss_u loss_u 0.9502 (0.9634) acc_u 6.2500 (5.0000) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [70/87] time 0.612 (0.462) data 0.481 (0.331) loss_u loss_u 0.9507 (0.9634) acc_u 6.2500 (4.9107) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [75/87] time 0.444 (0.461) data 0.314 (0.331) loss_u loss_u 0.9644 (0.9643) acc_u 3.1250 (4.7917) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [80/87] time 0.413 (0.462) data 0.283 (0.331) loss_u loss_u 0.9316 (0.9636) acc_u 9.3750 (4.8438) lr 1.9558e-03 eta 0:00:03
epoch [21/200] batch [85/87] time 0.405 (0.458) data 0.275 (0.327) loss_u loss_u 0.9814 (0.9638) acc_u 3.1250 (4.8162) lr 1.9558e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1871
confident_label rate tensor(0.1135, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 356
clean true:347
clean false:9
clean_rate:0.9747191011235955
noisy true:918
noisy false:1862
after delete: len(clean_dataset) 356
after delete: len(noisy_dataset) 2780
epoch [22/200] batch [5/11] time 0.552 (0.502) data 0.421 (0.372) loss_x loss_x 1.3408 (1.5883) acc_x 65.6250 (63.7500) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [10/11] time 0.501 (0.492) data 0.370 (0.362) loss_x loss_x 1.2441 (1.5417) acc_x 68.7500 (62.5000) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/86] time 0.450 (0.476) data 0.320 (0.345) loss_u loss_u 0.9727 (0.9649) acc_u 6.2500 (5.0000) lr 1.9511e-03 eta 0:00:38
epoch [22/200] batch [10/86] time 0.516 (0.491) data 0.386 (0.360) loss_u loss_u 0.9712 (0.9642) acc_u 3.1250 (5.6250) lr 1.9511e-03 eta 0:00:37
epoch [22/200] batch [15/86] time 0.603 (0.484) data 0.473 (0.353) loss_u loss_u 0.9873 (0.9699) acc_u 0.0000 (4.3750) lr 1.9511e-03 eta 0:00:34
epoch [22/200] batch [20/86] time 0.520 (0.487) data 0.390 (0.357) loss_u loss_u 0.9678 (0.9692) acc_u 3.1250 (4.0625) lr 1.9511e-03 eta 0:00:32
epoch [22/200] batch [25/86] time 0.461 (0.483) data 0.329 (0.353) loss_u loss_u 0.9644 (0.9675) acc_u 6.2500 (4.6250) lr 1.9511e-03 eta 0:00:29
epoch [22/200] batch [30/86] time 0.436 (0.480) data 0.305 (0.349) loss_u loss_u 0.9771 (0.9671) acc_u 6.2500 (4.8958) lr 1.9511e-03 eta 0:00:26
epoch [22/200] batch [35/86] time 0.533 (0.482) data 0.402 (0.351) loss_u loss_u 0.9844 (0.9666) acc_u 3.1250 (5.0000) lr 1.9511e-03 eta 0:00:24
epoch [22/200] batch [40/86] time 0.437 (0.479) data 0.307 (0.349) loss_u loss_u 0.9863 (0.9651) acc_u 0.0000 (4.9219) lr 1.9511e-03 eta 0:00:22
epoch [22/200] batch [45/86] time 0.440 (0.476) data 0.310 (0.346) loss_u loss_u 0.9541 (0.9654) acc_u 6.2500 (4.7222) lr 1.9511e-03 eta 0:00:19
epoch [22/200] batch [50/86] time 0.461 (0.476) data 0.330 (0.345) loss_u loss_u 0.9824 (0.9666) acc_u 3.1250 (4.6875) lr 1.9511e-03 eta 0:00:17
epoch [22/200] batch [55/86] time 0.547 (0.476) data 0.416 (0.345) loss_u loss_u 0.9536 (0.9667) acc_u 6.2500 (4.5455) lr 1.9511e-03 eta 0:00:14
epoch [22/200] batch [60/86] time 0.480 (0.475) data 0.348 (0.344) loss_u loss_u 0.9819 (0.9663) acc_u 6.2500 (4.6354) lr 1.9511e-03 eta 0:00:12
epoch [22/200] batch [65/86] time 0.455 (0.472) data 0.323 (0.341) loss_u loss_u 0.9448 (0.9651) acc_u 6.2500 (4.8077) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [70/86] time 0.416 (0.470) data 0.286 (0.339) loss_u loss_u 0.9966 (0.9652) acc_u 0.0000 (4.7768) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [75/86] time 0.431 (0.470) data 0.301 (0.339) loss_u loss_u 0.9790 (0.9657) acc_u 0.0000 (4.7083) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [80/86] time 0.640 (0.472) data 0.509 (0.341) loss_u loss_u 0.9683 (0.9647) acc_u 3.1250 (4.9219) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [85/86] time 0.461 (0.471) data 0.330 (0.341) loss_u loss_u 0.9917 (0.9647) acc_u 0.0000 (4.8529) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1785
confident_label rate tensor(0.1189, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 373
clean true:363
clean false:10
clean_rate:0.9731903485254692
noisy true:988
noisy false:1775
after delete: len(clean_dataset) 373
after delete: len(noisy_dataset) 2763
epoch [23/200] batch [5/11] time 0.524 (0.476) data 0.394 (0.346) loss_x loss_x 1.7061 (1.4672) acc_x 50.0000 (61.2500) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [10/11] time 0.531 (0.491) data 0.401 (0.361) loss_x loss_x 1.4072 (1.5387) acc_x 56.2500 (57.8125) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/86] time 0.400 (0.490) data 0.269 (0.359) loss_u loss_u 0.9941 (0.9722) acc_u 0.0000 (4.3750) lr 1.9461e-03 eta 0:00:39
epoch [23/200] batch [10/86] time 0.374 (0.484) data 0.243 (0.353) loss_u loss_u 0.9727 (0.9666) acc_u 3.1250 (5.6250) lr 1.9461e-03 eta 0:00:36
epoch [23/200] batch [15/86] time 0.522 (0.470) data 0.389 (0.339) loss_u loss_u 0.9907 (0.9660) acc_u 3.1250 (5.8333) lr 1.9461e-03 eta 0:00:33
epoch [23/200] batch [20/86] time 0.384 (0.469) data 0.254 (0.339) loss_u loss_u 0.9854 (0.9660) acc_u 3.1250 (5.6250) lr 1.9461e-03 eta 0:00:30
epoch [23/200] batch [25/86] time 0.704 (0.468) data 0.572 (0.337) loss_u loss_u 0.9561 (0.9686) acc_u 6.2500 (5.1250) lr 1.9461e-03 eta 0:00:28
epoch [23/200] batch [30/86] time 0.610 (0.473) data 0.480 (0.342) loss_u loss_u 0.9746 (0.9667) acc_u 3.1250 (5.1042) lr 1.9461e-03 eta 0:00:26
epoch [23/200] batch [35/86] time 0.499 (0.471) data 0.368 (0.340) loss_u loss_u 0.9761 (0.9676) acc_u 3.1250 (4.9107) lr 1.9461e-03 eta 0:00:24
epoch [23/200] batch [40/86] time 0.428 (0.465) data 0.298 (0.334) loss_u loss_u 0.9521 (0.9659) acc_u 3.1250 (5.1562) lr 1.9461e-03 eta 0:00:21
epoch [23/200] batch [45/86] time 0.592 (0.467) data 0.461 (0.336) loss_u loss_u 0.9961 (0.9649) acc_u 0.0000 (5.2778) lr 1.9461e-03 eta 0:00:19
epoch [23/200] batch [50/86] time 0.362 (0.462) data 0.231 (0.332) loss_u loss_u 0.9380 (0.9654) acc_u 6.2500 (5.0625) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [55/86] time 0.449 (0.462) data 0.319 (0.331) loss_u loss_u 0.9844 (0.9650) acc_u 0.0000 (5.0568) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [60/86] time 0.396 (0.458) data 0.265 (0.328) loss_u loss_u 0.9854 (0.9652) acc_u 0.0000 (5.0000) lr 1.9461e-03 eta 0:00:11
epoch [23/200] batch [65/86] time 0.429 (0.461) data 0.298 (0.330) loss_u loss_u 0.9824 (0.9650) acc_u 0.0000 (4.9519) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [70/86] time 0.453 (0.459) data 0.323 (0.328) loss_u loss_u 0.9658 (0.9644) acc_u 3.1250 (5.0000) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [75/86] time 0.370 (0.456) data 0.239 (0.325) loss_u loss_u 0.9863 (0.9634) acc_u 0.0000 (5.1250) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [80/86] time 0.397 (0.454) data 0.266 (0.323) loss_u loss_u 0.9434 (0.9631) acc_u 6.2500 (5.1172) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [85/86] time 0.421 (0.452) data 0.289 (0.321) loss_u loss_u 0.9526 (0.9638) acc_u 6.2500 (4.9632) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1802
confident_label rate tensor(0.1180, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 370
clean true:360
clean false:10
clean_rate:0.972972972972973
noisy true:974
noisy false:1792
after delete: len(clean_dataset) 370
after delete: len(noisy_dataset) 2766
epoch [24/200] batch [5/11] time 0.560 (0.501) data 0.430 (0.371) loss_x loss_x 1.2363 (1.4402) acc_x 75.0000 (65.0000) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [10/11] time 0.464 (0.512) data 0.334 (0.382) loss_x loss_x 1.3428 (1.4609) acc_x 71.8750 (64.0625) lr 1.9409e-03 eta 0:00:00
epoch [24/200] batch [5/86] time 0.403 (0.466) data 0.273 (0.335) loss_u loss_u 0.9639 (0.9684) acc_u 6.2500 (3.7500) lr 1.9409e-03 eta 0:00:37
epoch [24/200] batch [10/86] time 0.390 (0.472) data 0.260 (0.342) loss_u loss_u 0.9790 (0.9612) acc_u 3.1250 (4.6875) lr 1.9409e-03 eta 0:00:35
epoch [24/200] batch [15/86] time 0.525 (0.465) data 0.393 (0.335) loss_u loss_u 0.9849 (0.9672) acc_u 0.0000 (3.9583) lr 1.9409e-03 eta 0:00:33
epoch [24/200] batch [20/86] time 0.359 (0.465) data 0.227 (0.334) loss_u loss_u 0.9424 (0.9657) acc_u 6.2500 (4.0625) lr 1.9409e-03 eta 0:00:30
epoch [24/200] batch [25/86] time 0.460 (0.474) data 0.329 (0.343) loss_u loss_u 0.9941 (0.9670) acc_u 0.0000 (4.0000) lr 1.9409e-03 eta 0:00:28
epoch [24/200] batch [30/86] time 0.453 (0.460) data 0.323 (0.330) loss_u loss_u 0.9805 (0.9651) acc_u 0.0000 (4.3750) lr 1.9409e-03 eta 0:00:25
epoch [24/200] batch [35/86] time 0.401 (0.455) data 0.269 (0.325) loss_u loss_u 0.9165 (0.9652) acc_u 9.3750 (4.4643) lr 1.9409e-03 eta 0:00:23
epoch [24/200] batch [40/86] time 0.501 (0.457) data 0.370 (0.326) loss_u loss_u 0.9380 (0.9628) acc_u 9.3750 (4.7656) lr 1.9409e-03 eta 0:00:21
epoch [24/200] batch [45/86] time 0.431 (0.458) data 0.300 (0.327) loss_u loss_u 0.9404 (0.9635) acc_u 6.2500 (4.5833) lr 1.9409e-03 eta 0:00:18
epoch [24/200] batch [50/86] time 0.400 (0.452) data 0.268 (0.321) loss_u loss_u 0.9727 (0.9648) acc_u 3.1250 (4.3750) lr 1.9409e-03 eta 0:00:16
epoch [24/200] batch [55/86] time 0.625 (0.454) data 0.494 (0.323) loss_u loss_u 0.9604 (0.9657) acc_u 3.1250 (4.4318) lr 1.9409e-03 eta 0:00:14
epoch [24/200] batch [60/86] time 0.438 (0.456) data 0.306 (0.325) loss_u loss_u 0.9937 (0.9669) acc_u 0.0000 (4.3750) lr 1.9409e-03 eta 0:00:11
epoch [24/200] batch [65/86] time 0.504 (0.459) data 0.372 (0.328) loss_u loss_u 0.9370 (0.9661) acc_u 12.5000 (4.5673) lr 1.9409e-03 eta 0:00:09
epoch [24/200] batch [70/86] time 0.461 (0.456) data 0.330 (0.325) loss_u loss_u 0.9326 (0.9662) acc_u 12.5000 (4.5536) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [75/86] time 0.442 (0.454) data 0.310 (0.323) loss_u loss_u 0.9761 (0.9659) acc_u 3.1250 (4.6250) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [80/86] time 0.342 (0.452) data 0.211 (0.321) loss_u loss_u 0.9980 (0.9670) acc_u 0.0000 (4.4922) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [85/86] time 0.387 (0.451) data 0.256 (0.320) loss_u loss_u 0.9648 (0.9673) acc_u 6.2500 (4.5221) lr 1.9409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1894
confident_label rate tensor(0.1084, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 340
clean true:330
clean false:10
clean_rate:0.9705882352941176
noisy true:912
noisy false:1884
after delete: len(clean_dataset) 340
after delete: len(noisy_dataset) 2796
epoch [25/200] batch [5/10] time 0.480 (0.429) data 0.350 (0.299) loss_x loss_x 1.4023 (1.5498) acc_x 62.5000 (62.5000) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [10/10] time 0.472 (0.465) data 0.341 (0.335) loss_x loss_x 0.9731 (1.4118) acc_x 75.0000 (65.9375) lr 1.9354e-03 eta 0:00:00
epoch [25/200] batch [5/87] time 0.397 (0.458) data 0.265 (0.328) loss_u loss_u 0.9873 (0.9824) acc_u 0.0000 (3.1250) lr 1.9354e-03 eta 0:00:37
epoch [25/200] batch [10/87] time 0.405 (0.452) data 0.274 (0.321) loss_u loss_u 0.9092 (0.9710) acc_u 15.6250 (4.3750) lr 1.9354e-03 eta 0:00:34
epoch [25/200] batch [15/87] time 0.381 (0.445) data 0.251 (0.314) loss_u loss_u 0.9541 (0.9699) acc_u 3.1250 (4.3750) lr 1.9354e-03 eta 0:00:32
epoch [25/200] batch [20/87] time 0.331 (0.443) data 0.200 (0.312) loss_u loss_u 0.9751 (0.9684) acc_u 0.0000 (4.2188) lr 1.9354e-03 eta 0:00:29
epoch [25/200] batch [25/87] time 0.366 (0.436) data 0.235 (0.305) loss_u loss_u 0.9604 (0.9639) acc_u 6.2500 (4.6250) lr 1.9354e-03 eta 0:00:27
epoch [25/200] batch [30/87] time 0.443 (0.443) data 0.312 (0.312) loss_u loss_u 0.9531 (0.9634) acc_u 6.2500 (4.8958) lr 1.9354e-03 eta 0:00:25
epoch [25/200] batch [35/87] time 0.401 (0.445) data 0.271 (0.314) loss_u loss_u 0.9541 (0.9632) acc_u 6.2500 (4.8214) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [40/87] time 0.565 (0.444) data 0.435 (0.314) loss_u loss_u 0.9785 (0.9614) acc_u 3.1250 (4.8438) lr 1.9354e-03 eta 0:00:20
epoch [25/200] batch [45/87] time 0.503 (0.452) data 0.373 (0.321) loss_u loss_u 0.9277 (0.9602) acc_u 6.2500 (4.7917) lr 1.9354e-03 eta 0:00:18
epoch [25/200] batch [50/87] time 0.313 (0.450) data 0.182 (0.320) loss_u loss_u 0.9575 (0.9595) acc_u 3.1250 (4.8125) lr 1.9354e-03 eta 0:00:16
epoch [25/200] batch [55/87] time 0.413 (0.449) data 0.283 (0.318) loss_u loss_u 0.9580 (0.9597) acc_u 9.3750 (5.0568) lr 1.9354e-03 eta 0:00:14
epoch [25/200] batch [60/87] time 0.479 (0.452) data 0.348 (0.321) loss_u loss_u 0.9717 (0.9600) acc_u 3.1250 (4.9479) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [65/87] time 0.461 (0.453) data 0.330 (0.322) loss_u loss_u 0.9639 (0.9598) acc_u 3.1250 (5.0481) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [70/87] time 0.715 (0.458) data 0.583 (0.327) loss_u loss_u 0.9336 (0.9596) acc_u 6.2500 (5.0446) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [75/87] time 0.349 (0.454) data 0.218 (0.323) loss_u loss_u 0.9609 (0.9601) acc_u 6.2500 (5.0417) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [80/87] time 0.455 (0.455) data 0.324 (0.324) loss_u loss_u 0.9751 (0.9597) acc_u 6.2500 (5.2344) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [85/87] time 0.452 (0.454) data 0.320 (0.324) loss_u loss_u 0.9463 (0.9596) acc_u 6.2500 (5.2941) lr 1.9354e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1831
confident_label rate tensor(0.1221, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 383
clean true:373
clean false:10
clean_rate:0.9738903394255874
noisy true:932
noisy false:1821
after delete: len(clean_dataset) 383
after delete: len(noisy_dataset) 2753
epoch [26/200] batch [5/11] time 0.387 (0.524) data 0.257 (0.393) loss_x loss_x 1.6348 (1.7836) acc_x 56.2500 (58.1250) lr 1.9298e-03 eta 0:00:03
epoch [26/200] batch [10/11] time 0.577 (0.530) data 0.447 (0.400) loss_x loss_x 1.2832 (1.6103) acc_x 68.7500 (60.6250) lr 1.9298e-03 eta 0:00:00
epoch [26/200] batch [5/86] time 0.387 (0.511) data 0.257 (0.380) loss_u loss_u 0.9639 (0.9642) acc_u 3.1250 (5.0000) lr 1.9298e-03 eta 0:00:41
epoch [26/200] batch [10/86] time 0.447 (0.492) data 0.316 (0.361) loss_u loss_u 0.9365 (0.9470) acc_u 6.2500 (7.5000) lr 1.9298e-03 eta 0:00:37
epoch [26/200] batch [15/86] time 0.418 (0.484) data 0.288 (0.353) loss_u loss_u 0.9717 (0.9530) acc_u 6.2500 (7.0833) lr 1.9298e-03 eta 0:00:34
epoch [26/200] batch [20/86] time 0.370 (0.472) data 0.240 (0.341) loss_u loss_u 0.9810 (0.9562) acc_u 3.1250 (6.5625) lr 1.9298e-03 eta 0:00:31
epoch [26/200] batch [25/86] time 0.515 (0.474) data 0.385 (0.343) loss_u loss_u 0.9663 (0.9581) acc_u 3.1250 (6.0000) lr 1.9298e-03 eta 0:00:28
epoch [26/200] batch [30/86] time 0.472 (0.470) data 0.341 (0.339) loss_u loss_u 0.9766 (0.9602) acc_u 6.2500 (5.8333) lr 1.9298e-03 eta 0:00:26
epoch [26/200] batch [35/86] time 0.448 (0.464) data 0.317 (0.333) loss_u loss_u 0.9116 (0.9575) acc_u 15.6250 (6.2500) lr 1.9298e-03 eta 0:00:23
epoch [26/200] batch [40/86] time 0.432 (0.462) data 0.301 (0.331) loss_u loss_u 0.9937 (0.9599) acc_u 0.0000 (5.9375) lr 1.9298e-03 eta 0:00:21
epoch [26/200] batch [45/86] time 0.432 (0.459) data 0.301 (0.329) loss_u loss_u 0.9370 (0.9605) acc_u 6.2500 (5.9028) lr 1.9298e-03 eta 0:00:18
epoch [26/200] batch [50/86] time 0.452 (0.457) data 0.321 (0.327) loss_u loss_u 0.9946 (0.9618) acc_u 0.0000 (5.6250) lr 1.9298e-03 eta 0:00:16
epoch [26/200] batch [55/86] time 0.461 (0.459) data 0.331 (0.328) loss_u loss_u 0.9648 (0.9628) acc_u 3.1250 (5.3977) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [60/86] time 0.344 (0.460) data 0.212 (0.329) loss_u loss_u 0.9185 (0.9629) acc_u 12.5000 (5.3646) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [65/86] time 0.434 (0.461) data 0.304 (0.330) loss_u loss_u 0.9756 (0.9639) acc_u 3.1250 (5.1923) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [70/86] time 0.428 (0.457) data 0.297 (0.327) loss_u loss_u 0.9800 (0.9644) acc_u 0.0000 (5.0446) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [75/86] time 0.415 (0.457) data 0.284 (0.326) loss_u loss_u 0.9702 (0.9645) acc_u 6.2500 (5.0000) lr 1.9298e-03 eta 0:00:05
epoch [26/200] batch [80/86] time 0.394 (0.456) data 0.262 (0.325) loss_u loss_u 0.9951 (0.9634) acc_u 0.0000 (5.1953) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [85/86] time 0.394 (0.454) data 0.263 (0.323) loss_u loss_u 0.9668 (0.9632) acc_u 3.1250 (5.2574) lr 1.9298e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1818
confident_label rate tensor(0.1240, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 389
clean true:378
clean false:11
clean_rate:0.9717223650385605
noisy true:940
noisy false:1807
after delete: len(clean_dataset) 389
after delete: len(noisy_dataset) 2747
epoch [27/200] batch [5/12] time 0.517 (0.458) data 0.387 (0.328) loss_x loss_x 1.3232 (1.3320) acc_x 75.0000 (66.8750) lr 1.9239e-03 eta 0:00:03
epoch [27/200] batch [10/12] time 0.538 (0.498) data 0.407 (0.368) loss_x loss_x 1.7109 (1.4553) acc_x 59.3750 (63.4375) lr 1.9239e-03 eta 0:00:00
epoch [27/200] batch [5/85] time 0.372 (0.479) data 0.241 (0.349) loss_u loss_u 0.9556 (0.9681) acc_u 6.2500 (4.3750) lr 1.9239e-03 eta 0:00:38
epoch [27/200] batch [10/85] time 0.406 (0.470) data 0.275 (0.339) loss_u loss_u 0.9453 (0.9641) acc_u 12.5000 (5.3125) lr 1.9239e-03 eta 0:00:35
epoch [27/200] batch [15/85] time 0.450 (0.468) data 0.320 (0.338) loss_u loss_u 0.9741 (0.9694) acc_u 3.1250 (4.1667) lr 1.9239e-03 eta 0:00:32
epoch [27/200] batch [20/85] time 0.405 (0.457) data 0.274 (0.326) loss_u loss_u 0.9253 (0.9684) acc_u 12.5000 (4.2188) lr 1.9239e-03 eta 0:00:29
epoch [27/200] batch [25/85] time 0.337 (0.453) data 0.206 (0.322) loss_u loss_u 0.9575 (0.9681) acc_u 6.2500 (4.1250) lr 1.9239e-03 eta 0:00:27
epoch [27/200] batch [30/85] time 0.493 (0.450) data 0.362 (0.319) loss_u loss_u 0.9780 (0.9698) acc_u 6.2500 (3.9583) lr 1.9239e-03 eta 0:00:24
epoch [27/200] batch [35/85] time 0.464 (0.449) data 0.333 (0.318) loss_u loss_u 0.9863 (0.9719) acc_u 3.1250 (3.7500) lr 1.9239e-03 eta 0:00:22
epoch [27/200] batch [40/85] time 0.590 (0.448) data 0.459 (0.317) loss_u loss_u 0.9897 (0.9718) acc_u 3.1250 (3.6719) lr 1.9239e-03 eta 0:00:20
epoch [27/200] batch [45/85] time 0.366 (0.446) data 0.236 (0.315) loss_u loss_u 0.9893 (0.9691) acc_u 0.0000 (3.8889) lr 1.9239e-03 eta 0:00:17
epoch [27/200] batch [50/85] time 0.389 (0.447) data 0.259 (0.316) loss_u loss_u 0.9658 (0.9683) acc_u 3.1250 (4.0000) lr 1.9239e-03 eta 0:00:15
epoch [27/200] batch [55/85] time 0.379 (0.443) data 0.249 (0.312) loss_u loss_u 0.9438 (0.9676) acc_u 6.2500 (4.0341) lr 1.9239e-03 eta 0:00:13
epoch [27/200] batch [60/85] time 0.425 (0.442) data 0.294 (0.311) loss_u loss_u 0.9717 (0.9673) acc_u 6.2500 (4.1667) lr 1.9239e-03 eta 0:00:11
epoch [27/200] batch [65/85] time 0.420 (0.440) data 0.290 (0.310) loss_u loss_u 0.9819 (0.9675) acc_u 6.2500 (4.1346) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [70/85] time 0.508 (0.443) data 0.375 (0.312) loss_u loss_u 0.9868 (0.9674) acc_u 0.0000 (4.1518) lr 1.9239e-03 eta 0:00:06
epoch [27/200] batch [75/85] time 0.343 (0.444) data 0.212 (0.313) loss_u loss_u 0.9702 (0.9680) acc_u 0.0000 (4.0000) lr 1.9239e-03 eta 0:00:04
epoch [27/200] batch [80/85] time 0.450 (0.446) data 0.320 (0.315) loss_u loss_u 0.9404 (0.9673) acc_u 6.2500 (4.0234) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [85/85] time 0.409 (0.445) data 0.278 (0.314) loss_u loss_u 0.9771 (0.9671) acc_u 0.0000 (4.0074) lr 1.9239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1901
confident_label rate tensor(0.1116, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 350
clean true:340
clean false:10
clean_rate:0.9714285714285714
noisy true:895
noisy false:1891
after delete: len(clean_dataset) 350
after delete: len(noisy_dataset) 2786
epoch [28/200] batch [5/10] time 0.440 (0.531) data 0.310 (0.401) loss_x loss_x 1.1592 (1.2846) acc_x 62.5000 (63.1250) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [10/10] time 0.441 (0.495) data 0.310 (0.364) loss_x loss_x 1.8008 (1.4145) acc_x 53.1250 (61.8750) lr 1.9178e-03 eta 0:00:00
epoch [28/200] batch [5/87] time 0.436 (0.466) data 0.305 (0.335) loss_u loss_u 0.9634 (0.9759) acc_u 6.2500 (3.1250) lr 1.9178e-03 eta 0:00:38
epoch [28/200] batch [10/87] time 0.375 (0.448) data 0.243 (0.317) loss_u loss_u 0.9941 (0.9710) acc_u 0.0000 (3.4375) lr 1.9178e-03 eta 0:00:34
epoch [28/200] batch [15/87] time 0.351 (0.443) data 0.220 (0.313) loss_u loss_u 0.9785 (0.9710) acc_u 3.1250 (3.9583) lr 1.9178e-03 eta 0:00:31
epoch [28/200] batch [20/87] time 0.405 (0.441) data 0.275 (0.310) loss_u loss_u 0.9648 (0.9728) acc_u 3.1250 (3.5938) lr 1.9178e-03 eta 0:00:29
epoch [28/200] batch [25/87] time 0.416 (0.451) data 0.285 (0.320) loss_u loss_u 0.9463 (0.9684) acc_u 9.3750 (4.2500) lr 1.9178e-03 eta 0:00:27
epoch [28/200] batch [30/87] time 0.397 (0.451) data 0.266 (0.320) loss_u loss_u 0.9512 (0.9685) acc_u 3.1250 (4.1667) lr 1.9178e-03 eta 0:00:25
epoch [28/200] batch [35/87] time 0.556 (0.453) data 0.426 (0.322) loss_u loss_u 0.9751 (0.9672) acc_u 6.2500 (4.5536) lr 1.9178e-03 eta 0:00:23
epoch [28/200] batch [40/87] time 0.474 (0.454) data 0.344 (0.323) loss_u loss_u 0.8965 (0.9658) acc_u 9.3750 (4.6094) lr 1.9178e-03 eta 0:00:21
epoch [28/200] batch [45/87] time 0.511 (0.457) data 0.380 (0.326) loss_u loss_u 0.9292 (0.9641) acc_u 6.2500 (4.7222) lr 1.9178e-03 eta 0:00:19
epoch [28/200] batch [50/87] time 0.483 (0.459) data 0.353 (0.328) loss_u loss_u 0.9351 (0.9623) acc_u 6.2500 (5.0625) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [55/87] time 0.449 (0.456) data 0.318 (0.326) loss_u loss_u 0.9756 (0.9634) acc_u 3.1250 (4.9432) lr 1.9178e-03 eta 0:00:14
epoch [28/200] batch [60/87] time 0.451 (0.455) data 0.319 (0.324) loss_u loss_u 0.9282 (0.9632) acc_u 6.2500 (4.8438) lr 1.9178e-03 eta 0:00:12
epoch [28/200] batch [65/87] time 0.472 (0.452) data 0.342 (0.321) loss_u loss_u 0.9961 (0.9641) acc_u 0.0000 (4.7596) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [70/87] time 0.447 (0.454) data 0.315 (0.323) loss_u loss_u 0.9736 (0.9633) acc_u 3.1250 (4.8214) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [75/87] time 0.484 (0.459) data 0.354 (0.328) loss_u loss_u 0.9658 (0.9629) acc_u 6.2500 (4.9583) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [80/87] time 0.527 (0.458) data 0.396 (0.328) loss_u loss_u 0.9648 (0.9627) acc_u 6.2500 (5.0781) lr 1.9178e-03 eta 0:00:03
epoch [28/200] batch [85/87] time 0.429 (0.458) data 0.297 (0.327) loss_u loss_u 0.9492 (0.9625) acc_u 9.3750 (5.2206) lr 1.9178e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1793
confident_label rate tensor(0.1183, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 371
clean true:364
clean false:7
clean_rate:0.9811320754716981
noisy true:979
noisy false:1786
after delete: len(clean_dataset) 371
after delete: len(noisy_dataset) 2765
epoch [29/200] batch [5/11] time 0.507 (0.511) data 0.377 (0.381) loss_x loss_x 1.6299 (1.6420) acc_x 62.5000 (62.5000) lr 1.9114e-03 eta 0:00:03
epoch [29/200] batch [10/11] time 0.531 (0.497) data 0.401 (0.367) loss_x loss_x 1.4590 (1.6031) acc_x 56.2500 (60.0000) lr 1.9114e-03 eta 0:00:00
epoch [29/200] batch [5/86] time 0.387 (0.494) data 0.256 (0.363) loss_u loss_u 0.9873 (0.9734) acc_u 0.0000 (2.5000) lr 1.9114e-03 eta 0:00:39
epoch [29/200] batch [10/86] time 0.508 (0.497) data 0.377 (0.367) loss_u loss_u 0.9634 (0.9632) acc_u 3.1250 (4.6875) lr 1.9114e-03 eta 0:00:37
epoch [29/200] batch [15/86] time 0.428 (0.489) data 0.298 (0.358) loss_u loss_u 0.9688 (0.9597) acc_u 6.2500 (5.0000) lr 1.9114e-03 eta 0:00:34
epoch [29/200] batch [20/86] time 0.418 (0.489) data 0.288 (0.358) loss_u loss_u 0.9839 (0.9593) acc_u 0.0000 (5.0000) lr 1.9114e-03 eta 0:00:32
epoch [29/200] batch [25/86] time 0.382 (0.489) data 0.252 (0.359) loss_u loss_u 0.9912 (0.9619) acc_u 0.0000 (5.0000) lr 1.9114e-03 eta 0:00:29
epoch [29/200] batch [30/86] time 0.529 (0.488) data 0.398 (0.358) loss_u loss_u 0.9609 (0.9632) acc_u 6.2500 (4.6875) lr 1.9114e-03 eta 0:00:27
epoch [29/200] batch [35/86] time 0.385 (0.477) data 0.255 (0.347) loss_u loss_u 0.9487 (0.9622) acc_u 9.3750 (5.0000) lr 1.9114e-03 eta 0:00:24
epoch [29/200] batch [40/86] time 0.384 (0.471) data 0.254 (0.341) loss_u loss_u 0.9648 (0.9645) acc_u 9.3750 (4.7656) lr 1.9114e-03 eta 0:00:21
epoch [29/200] batch [45/86] time 0.588 (0.470) data 0.458 (0.339) loss_u loss_u 0.9775 (0.9637) acc_u 3.1250 (5.0000) lr 1.9114e-03 eta 0:00:19
epoch [29/200] batch [50/86] time 0.471 (0.465) data 0.340 (0.335) loss_u loss_u 0.9922 (0.9651) acc_u 0.0000 (4.7500) lr 1.9114e-03 eta 0:00:16
epoch [29/200] batch [55/86] time 0.391 (0.463) data 0.261 (0.332) loss_u loss_u 0.9800 (0.9663) acc_u 3.1250 (4.6591) lr 1.9114e-03 eta 0:00:14
epoch [29/200] batch [60/86] time 0.532 (0.461) data 0.401 (0.331) loss_u loss_u 0.9414 (0.9645) acc_u 3.1250 (4.6875) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [65/86] time 0.374 (0.457) data 0.244 (0.326) loss_u loss_u 0.9673 (0.9653) acc_u 6.2500 (4.5192) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [70/86] time 0.446 (0.455) data 0.315 (0.324) loss_u loss_u 0.9971 (0.9661) acc_u 0.0000 (4.3750) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [75/86] time 0.399 (0.454) data 0.268 (0.324) loss_u loss_u 0.9634 (0.9669) acc_u 3.1250 (4.2500) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [80/86] time 0.401 (0.450) data 0.270 (0.320) loss_u loss_u 0.9863 (0.9671) acc_u 3.1250 (4.3359) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [85/86] time 0.402 (0.450) data 0.272 (0.320) loss_u loss_u 0.9507 (0.9664) acc_u 9.3750 (4.4485) lr 1.9114e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1857
confident_label rate tensor(0.1151, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 361
clean true:347
clean false:14
clean_rate:0.961218836565097
noisy true:932
noisy false:1843
after delete: len(clean_dataset) 361
after delete: len(noisy_dataset) 2775
epoch [30/200] batch [5/11] time 0.408 (0.423) data 0.278 (0.292) loss_x loss_x 1.9580 (1.6830) acc_x 53.1250 (60.6250) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [10/11] time 0.453 (0.471) data 0.323 (0.341) loss_x loss_x 1.6914 (1.6656) acc_x 68.7500 (61.2500) lr 1.9048e-03 eta 0:00:00
epoch [30/200] batch [5/86] time 0.391 (0.455) data 0.260 (0.324) loss_u loss_u 0.9863 (0.9545) acc_u 0.0000 (8.1250) lr 1.9048e-03 eta 0:00:36
epoch [30/200] batch [10/86] time 0.427 (0.454) data 0.295 (0.324) loss_u loss_u 0.9575 (0.9560) acc_u 3.1250 (6.8750) lr 1.9048e-03 eta 0:00:34
epoch [30/200] batch [15/86] time 0.371 (0.455) data 0.239 (0.325) loss_u loss_u 0.9922 (0.9587) acc_u 3.1250 (6.0417) lr 1.9048e-03 eta 0:00:32
epoch [30/200] batch [20/86] time 0.330 (0.453) data 0.199 (0.323) loss_u loss_u 0.9824 (0.9640) acc_u 3.1250 (5.3125) lr 1.9048e-03 eta 0:00:29
epoch [30/200] batch [25/86] time 0.508 (0.454) data 0.377 (0.323) loss_u loss_u 0.9653 (0.9648) acc_u 3.1250 (5.1250) lr 1.9048e-03 eta 0:00:27
epoch [30/200] batch [30/86] time 0.406 (0.451) data 0.275 (0.320) loss_u loss_u 0.9585 (0.9625) acc_u 6.2500 (5.2083) lr 1.9048e-03 eta 0:00:25
epoch [30/200] batch [35/86] time 0.417 (0.448) data 0.287 (0.318) loss_u loss_u 0.9824 (0.9624) acc_u 0.0000 (5.2679) lr 1.9048e-03 eta 0:00:22
epoch [30/200] batch [40/86] time 0.409 (0.445) data 0.279 (0.315) loss_u loss_u 0.9629 (0.9630) acc_u 3.1250 (5.1562) lr 1.9048e-03 eta 0:00:20
epoch [30/200] batch [45/86] time 0.373 (0.444) data 0.243 (0.313) loss_u loss_u 0.9717 (0.9630) acc_u 3.1250 (5.2083) lr 1.9048e-03 eta 0:00:18
epoch [30/200] batch [50/86] time 0.558 (0.447) data 0.428 (0.316) loss_u loss_u 0.9634 (0.9635) acc_u 3.1250 (5.0625) lr 1.9048e-03 eta 0:00:16
epoch [30/200] batch [55/86] time 0.481 (0.444) data 0.350 (0.314) loss_u loss_u 0.9204 (0.9638) acc_u 12.5000 (5.0000) lr 1.9048e-03 eta 0:00:13
epoch [30/200] batch [60/86] time 0.386 (0.443) data 0.255 (0.312) loss_u loss_u 0.9668 (0.9649) acc_u 3.1250 (4.8438) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [65/86] time 0.524 (0.445) data 0.394 (0.314) loss_u loss_u 0.9473 (0.9653) acc_u 3.1250 (4.6635) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [70/86] time 0.387 (0.443) data 0.255 (0.313) loss_u loss_u 0.9473 (0.9645) acc_u 3.1250 (4.7321) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [75/86] time 0.418 (0.445) data 0.286 (0.314) loss_u loss_u 0.9805 (0.9639) acc_u 3.1250 (4.8333) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [80/86] time 0.453 (0.444) data 0.323 (0.313) loss_u loss_u 0.9849 (0.9640) acc_u 3.1250 (4.8047) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [85/86] time 0.453 (0.448) data 0.322 (0.317) loss_u loss_u 0.9458 (0.9632) acc_u 9.3750 (4.9265) lr 1.9048e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1840
confident_label rate tensor(0.1167, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 366
clean true:356
clean false:10
clean_rate:0.9726775956284153
noisy true:940
noisy false:1830
after delete: len(clean_dataset) 366
after delete: len(noisy_dataset) 2770
epoch [31/200] batch [5/11] time 0.397 (0.431) data 0.267 (0.300) loss_x loss_x 1.0146 (1.4684) acc_x 78.1250 (60.0000) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [10/11] time 0.503 (0.436) data 0.373 (0.306) loss_x loss_x 1.7051 (1.4805) acc_x 46.8750 (60.9375) lr 1.8980e-03 eta 0:00:00
epoch [31/200] batch [5/86] time 0.501 (0.440) data 0.369 (0.310) loss_u loss_u 0.9653 (0.9682) acc_u 6.2500 (5.0000) lr 1.8980e-03 eta 0:00:35
epoch [31/200] batch [10/86] time 0.442 (0.452) data 0.311 (0.321) loss_u loss_u 0.9512 (0.9615) acc_u 9.3750 (5.3125) lr 1.8980e-03 eta 0:00:34
epoch [31/200] batch [15/86] time 0.413 (0.456) data 0.282 (0.325) loss_u loss_u 0.9790 (0.9630) acc_u 0.0000 (5.0000) lr 1.8980e-03 eta 0:00:32
epoch [31/200] batch [20/86] time 0.394 (0.452) data 0.263 (0.321) loss_u loss_u 0.9204 (0.9599) acc_u 12.5000 (5.3125) lr 1.8980e-03 eta 0:00:29
epoch [31/200] batch [25/86] time 0.423 (0.455) data 0.292 (0.324) loss_u loss_u 0.9292 (0.9599) acc_u 6.2500 (5.1250) lr 1.8980e-03 eta 0:00:27
epoch [31/200] batch [30/86] time 0.366 (0.453) data 0.236 (0.322) loss_u loss_u 0.9087 (0.9596) acc_u 18.7500 (5.2083) lr 1.8980e-03 eta 0:00:25
epoch [31/200] batch [35/86] time 0.432 (0.447) data 0.302 (0.316) loss_u loss_u 0.9580 (0.9598) acc_u 3.1250 (5.1786) lr 1.8980e-03 eta 0:00:22
epoch [31/200] batch [40/86] time 0.567 (0.447) data 0.436 (0.316) loss_u loss_u 0.9468 (0.9598) acc_u 6.2500 (5.1562) lr 1.8980e-03 eta 0:00:20
epoch [31/200] batch [45/86] time 0.570 (0.447) data 0.439 (0.316) loss_u loss_u 0.9951 (0.9603) acc_u 0.0000 (5.0694) lr 1.8980e-03 eta 0:00:18
epoch [31/200] batch [50/86] time 0.355 (0.443) data 0.223 (0.313) loss_u loss_u 0.9580 (0.9612) acc_u 6.2500 (5.0625) lr 1.8980e-03 eta 0:00:15
epoch [31/200] batch [55/86] time 0.440 (0.442) data 0.309 (0.311) loss_u loss_u 0.9448 (0.9619) acc_u 9.3750 (4.9432) lr 1.8980e-03 eta 0:00:13
epoch [31/200] batch [60/86] time 0.426 (0.444) data 0.294 (0.314) loss_u loss_u 0.9565 (0.9609) acc_u 6.2500 (5.0521) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [65/86] time 0.375 (0.444) data 0.243 (0.313) loss_u loss_u 0.9966 (0.9606) acc_u 0.0000 (5.0481) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [70/86] time 0.559 (0.445) data 0.428 (0.314) loss_u loss_u 0.9419 (0.9608) acc_u 6.2500 (4.9554) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [75/86] time 0.509 (0.447) data 0.379 (0.316) loss_u loss_u 0.9697 (0.9612) acc_u 3.1250 (5.0000) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [80/86] time 0.511 (0.449) data 0.380 (0.318) loss_u loss_u 0.9902 (0.9615) acc_u 0.0000 (4.9219) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [85/86] time 0.387 (0.449) data 0.256 (0.318) loss_u loss_u 0.9575 (0.9615) acc_u 6.2500 (5.0368) lr 1.8980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1791
confident_label rate tensor(0.1298, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 407
clean true:398
clean false:9
clean_rate:0.9778869778869779
noisy true:947
noisy false:1782
after delete: len(clean_dataset) 407
after delete: len(noisy_dataset) 2729
epoch [32/200] batch [5/12] time 0.595 (0.553) data 0.465 (0.422) loss_x loss_x 1.7588 (1.6221) acc_x 62.5000 (61.2500) lr 1.8910e-03 eta 0:00:03
epoch [32/200] batch [10/12] time 0.547 (0.493) data 0.416 (0.363) loss_x loss_x 1.0811 (1.5198) acc_x 81.2500 (62.5000) lr 1.8910e-03 eta 0:00:00
epoch [32/200] batch [5/85] time 0.333 (0.450) data 0.201 (0.319) loss_u loss_u 0.9697 (0.9567) acc_u 3.1250 (5.0000) lr 1.8910e-03 eta 0:00:35
epoch [32/200] batch [10/85] time 0.457 (0.475) data 0.327 (0.345) loss_u loss_u 0.9302 (0.9528) acc_u 9.3750 (5.3125) lr 1.8910e-03 eta 0:00:35
epoch [32/200] batch [15/85] time 0.439 (0.462) data 0.307 (0.332) loss_u loss_u 0.9351 (0.9568) acc_u 9.3750 (5.2083) lr 1.8910e-03 eta 0:00:32
epoch [32/200] batch [20/85] time 0.446 (0.457) data 0.316 (0.326) loss_u loss_u 0.9985 (0.9568) acc_u 0.0000 (5.7812) lr 1.8910e-03 eta 0:00:29
epoch [32/200] batch [25/85] time 0.426 (0.456) data 0.295 (0.325) loss_u loss_u 0.9971 (0.9608) acc_u 0.0000 (5.2500) lr 1.8910e-03 eta 0:00:27
epoch [32/200] batch [30/85] time 0.387 (0.453) data 0.256 (0.323) loss_u loss_u 0.9619 (0.9631) acc_u 6.2500 (4.8958) lr 1.8910e-03 eta 0:00:24
epoch [32/200] batch [35/85] time 0.406 (0.450) data 0.276 (0.320) loss_u loss_u 0.9556 (0.9651) acc_u 9.3750 (4.8214) lr 1.8910e-03 eta 0:00:22
epoch [32/200] batch [40/85] time 0.398 (0.449) data 0.268 (0.319) loss_u loss_u 0.9961 (0.9670) acc_u 0.0000 (4.5312) lr 1.8910e-03 eta 0:00:20
epoch [32/200] batch [45/85] time 0.511 (0.445) data 0.381 (0.314) loss_u loss_u 0.9629 (0.9652) acc_u 6.2500 (4.7917) lr 1.8910e-03 eta 0:00:17
epoch [32/200] batch [50/85] time 0.373 (0.446) data 0.242 (0.315) loss_u loss_u 0.9658 (0.9661) acc_u 3.1250 (4.6250) lr 1.8910e-03 eta 0:00:15
epoch [32/200] batch [55/85] time 0.365 (0.443) data 0.235 (0.313) loss_u loss_u 0.9736 (0.9675) acc_u 3.1250 (4.3750) lr 1.8910e-03 eta 0:00:13
epoch [32/200] batch [60/85] time 0.501 (0.442) data 0.370 (0.312) loss_u loss_u 0.9912 (0.9681) acc_u 0.0000 (4.2188) lr 1.8910e-03 eta 0:00:11
epoch [32/200] batch [65/85] time 0.522 (0.444) data 0.391 (0.314) loss_u loss_u 0.9829 (0.9688) acc_u 0.0000 (4.1346) lr 1.8910e-03 eta 0:00:08
epoch [32/200] batch [70/85] time 0.436 (0.450) data 0.306 (0.319) loss_u loss_u 0.9683 (0.9681) acc_u 6.2500 (4.3304) lr 1.8910e-03 eta 0:00:06
epoch [32/200] batch [75/85] time 0.299 (0.448) data 0.168 (0.318) loss_u loss_u 0.9624 (0.9676) acc_u 6.2500 (4.4583) lr 1.8910e-03 eta 0:00:04
epoch [32/200] batch [80/85] time 0.574 (0.449) data 0.444 (0.318) loss_u loss_u 0.9854 (0.9680) acc_u 0.0000 (4.4141) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [85/85] time 0.474 (0.449) data 0.344 (0.318) loss_u loss_u 0.9922 (0.9678) acc_u 0.0000 (4.3750) lr 1.8910e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1852
confident_label rate tensor(0.1132, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 355
clean true:341
clean false:14
clean_rate:0.9605633802816902
noisy true:943
noisy false:1838
after delete: len(clean_dataset) 355
after delete: len(noisy_dataset) 2781
epoch [33/200] batch [5/11] time 0.456 (0.480) data 0.326 (0.350) loss_x loss_x 2.1738 (1.6525) acc_x 46.8750 (57.5000) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [10/11] time 0.524 (0.472) data 0.393 (0.342) loss_x loss_x 1.7197 (1.5680) acc_x 65.6250 (62.8125) lr 1.8838e-03 eta 0:00:00
epoch [33/200] batch [5/86] time 0.360 (0.452) data 0.229 (0.322) loss_u loss_u 0.9971 (0.9824) acc_u 0.0000 (0.6250) lr 1.8838e-03 eta 0:00:36
epoch [33/200] batch [10/86] time 0.597 (0.451) data 0.466 (0.320) loss_u loss_u 0.9658 (0.9791) acc_u 9.3750 (2.5000) lr 1.8838e-03 eta 0:00:34
epoch [33/200] batch [15/86] time 0.479 (0.461) data 0.347 (0.330) loss_u loss_u 0.9341 (0.9670) acc_u 9.3750 (4.1667) lr 1.8838e-03 eta 0:00:32
epoch [33/200] batch [20/86] time 0.432 (0.472) data 0.300 (0.341) loss_u loss_u 0.8892 (0.9604) acc_u 15.6250 (5.1562) lr 1.8838e-03 eta 0:00:31
epoch [33/200] batch [25/86] time 0.517 (0.468) data 0.387 (0.337) loss_u loss_u 0.9761 (0.9613) acc_u 3.1250 (5.0000) lr 1.8838e-03 eta 0:00:28
epoch [33/200] batch [30/86] time 0.576 (0.465) data 0.446 (0.334) loss_u loss_u 0.9570 (0.9622) acc_u 6.2500 (4.7917) lr 1.8838e-03 eta 0:00:26
epoch [33/200] batch [35/86] time 0.388 (0.459) data 0.257 (0.328) loss_u loss_u 0.9648 (0.9642) acc_u 3.1250 (4.4643) lr 1.8838e-03 eta 0:00:23
epoch [33/200] batch [40/86] time 0.513 (0.461) data 0.382 (0.330) loss_u loss_u 0.9658 (0.9608) acc_u 6.2500 (5.0781) lr 1.8838e-03 eta 0:00:21
epoch [33/200] batch [45/86] time 0.464 (0.460) data 0.332 (0.329) loss_u loss_u 0.9438 (0.9610) acc_u 3.1250 (5.0694) lr 1.8838e-03 eta 0:00:18
epoch [33/200] batch [50/86] time 0.452 (0.455) data 0.320 (0.324) loss_u loss_u 0.9941 (0.9616) acc_u 0.0000 (4.8750) lr 1.8838e-03 eta 0:00:16
epoch [33/200] batch [55/86] time 0.330 (0.452) data 0.199 (0.321) loss_u loss_u 0.9028 (0.9605) acc_u 15.6250 (5.0568) lr 1.8838e-03 eta 0:00:14
epoch [33/200] batch [60/86] time 0.453 (0.450) data 0.322 (0.318) loss_u loss_u 0.9370 (0.9591) acc_u 9.3750 (5.3125) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [65/86] time 0.356 (0.446) data 0.225 (0.315) loss_u loss_u 0.9863 (0.9581) acc_u 3.1250 (5.3846) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [70/86] time 0.670 (0.449) data 0.538 (0.318) loss_u loss_u 0.9521 (0.9576) acc_u 6.2500 (5.3571) lr 1.8838e-03 eta 0:00:07
epoch [33/200] batch [75/86] time 0.331 (0.446) data 0.200 (0.314) loss_u loss_u 0.9941 (0.9585) acc_u 0.0000 (5.2917) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [80/86] time 0.485 (0.446) data 0.355 (0.315) loss_u loss_u 0.9512 (0.9585) acc_u 9.3750 (5.3516) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [85/86] time 0.325 (0.446) data 0.193 (0.315) loss_u loss_u 0.8984 (0.9579) acc_u 9.3750 (5.4044) lr 1.8838e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1797
confident_label rate tensor(0.1228, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 385
clean true:377
clean false:8
clean_rate:0.9792207792207792
noisy true:962
noisy false:1789
after delete: len(clean_dataset) 385
after delete: len(noisy_dataset) 2751
epoch [34/200] batch [5/12] time 0.406 (0.449) data 0.275 (0.319) loss_x loss_x 1.1064 (1.6352) acc_x 71.8750 (63.1250) lr 1.8763e-03 eta 0:00:03
epoch [34/200] batch [10/12] time 0.417 (0.477) data 0.287 (0.346) loss_x loss_x 0.8521 (1.4417) acc_x 81.2500 (66.8750) lr 1.8763e-03 eta 0:00:00
epoch [34/200] batch [5/85] time 0.577 (0.459) data 0.444 (0.329) loss_u loss_u 0.9536 (0.9607) acc_u 3.1250 (4.3750) lr 1.8763e-03 eta 0:00:36
epoch [34/200] batch [10/85] time 0.434 (0.460) data 0.301 (0.329) loss_u loss_u 0.9854 (0.9648) acc_u 0.0000 (3.7500) lr 1.8763e-03 eta 0:00:34
epoch [34/200] batch [15/85] time 0.377 (0.450) data 0.247 (0.320) loss_u loss_u 0.9688 (0.9675) acc_u 6.2500 (3.7500) lr 1.8763e-03 eta 0:00:31
epoch [34/200] batch [20/85] time 0.398 (0.452) data 0.266 (0.321) loss_u loss_u 0.9951 (0.9704) acc_u 0.0000 (3.4375) lr 1.8763e-03 eta 0:00:29
epoch [34/200] batch [25/85] time 0.581 (0.456) data 0.450 (0.326) loss_u loss_u 0.9863 (0.9695) acc_u 0.0000 (4.0000) lr 1.8763e-03 eta 0:00:27
epoch [34/200] batch [30/85] time 0.469 (0.458) data 0.338 (0.327) loss_u loss_u 0.9248 (0.9667) acc_u 12.5000 (4.5833) lr 1.8763e-03 eta 0:00:25
epoch [34/200] batch [35/85] time 0.435 (0.460) data 0.304 (0.330) loss_u loss_u 0.9927 (0.9670) acc_u 3.1250 (4.4643) lr 1.8763e-03 eta 0:00:23
epoch [34/200] batch [40/85] time 0.371 (0.453) data 0.239 (0.322) loss_u loss_u 0.9604 (0.9670) acc_u 3.1250 (4.4531) lr 1.8763e-03 eta 0:00:20
epoch [34/200] batch [45/85] time 0.389 (0.450) data 0.258 (0.320) loss_u loss_u 0.9697 (0.9671) acc_u 3.1250 (4.5139) lr 1.8763e-03 eta 0:00:18
epoch [34/200] batch [50/85] time 0.337 (0.450) data 0.207 (0.320) loss_u loss_u 0.9536 (0.9667) acc_u 3.1250 (4.5000) lr 1.8763e-03 eta 0:00:15
epoch [34/200] batch [55/85] time 0.331 (0.447) data 0.200 (0.316) loss_u loss_u 0.9731 (0.9677) acc_u 0.0000 (4.2614) lr 1.8763e-03 eta 0:00:13
epoch [34/200] batch [60/85] time 0.507 (0.446) data 0.375 (0.315) loss_u loss_u 0.9585 (0.9658) acc_u 3.1250 (4.4271) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [65/85] time 0.451 (0.446) data 0.320 (0.315) loss_u loss_u 0.9985 (0.9672) acc_u 0.0000 (4.1827) lr 1.8763e-03 eta 0:00:08
epoch [34/200] batch [70/85] time 0.738 (0.448) data 0.606 (0.317) loss_u loss_u 0.9517 (0.9677) acc_u 6.2500 (4.1071) lr 1.8763e-03 eta 0:00:06
epoch [34/200] batch [75/85] time 0.441 (0.453) data 0.310 (0.322) loss_u loss_u 0.9541 (0.9664) acc_u 9.3750 (4.4167) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [80/85] time 0.429 (0.451) data 0.297 (0.320) loss_u loss_u 0.9600 (0.9666) acc_u 9.3750 (4.4141) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [85/85] time 0.355 (0.451) data 0.224 (0.320) loss_u loss_u 0.8828 (0.9654) acc_u 12.5000 (4.5221) lr 1.8763e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1808
confident_label rate tensor(0.1256, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 394
clean true:383
clean false:11
clean_rate:0.9720812182741116
noisy true:945
noisy false:1797
after delete: len(clean_dataset) 394
after delete: len(noisy_dataset) 2742
epoch [35/200] batch [5/12] time 0.444 (0.500) data 0.314 (0.370) loss_x loss_x 1.1816 (1.1945) acc_x 62.5000 (64.3750) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [10/12] time 0.454 (0.474) data 0.324 (0.344) loss_x loss_x 1.9268 (1.4120) acc_x 43.7500 (61.8750) lr 1.8686e-03 eta 0:00:00
epoch [35/200] batch [5/85] time 0.406 (0.476) data 0.274 (0.346) loss_u loss_u 0.9727 (0.9647) acc_u 3.1250 (3.7500) lr 1.8686e-03 eta 0:00:38
epoch [35/200] batch [10/85] time 0.544 (0.480) data 0.413 (0.349) loss_u loss_u 0.9639 (0.9682) acc_u 3.1250 (3.7500) lr 1.8686e-03 eta 0:00:36
epoch [35/200] batch [15/85] time 0.362 (0.477) data 0.231 (0.347) loss_u loss_u 0.9521 (0.9618) acc_u 6.2500 (4.7917) lr 1.8686e-03 eta 0:00:33
epoch [35/200] batch [20/85] time 0.434 (0.467) data 0.303 (0.337) loss_u loss_u 0.9609 (0.9620) acc_u 3.1250 (4.6875) lr 1.8686e-03 eta 0:00:30
epoch [35/200] batch [25/85] time 0.601 (0.470) data 0.471 (0.340) loss_u loss_u 0.9697 (0.9640) acc_u 3.1250 (4.6250) lr 1.8686e-03 eta 0:00:28
epoch [35/200] batch [30/85] time 0.355 (0.472) data 0.225 (0.341) loss_u loss_u 0.9600 (0.9636) acc_u 3.1250 (4.6875) lr 1.8686e-03 eta 0:00:25
epoch [35/200] batch [35/85] time 0.472 (0.470) data 0.341 (0.339) loss_u loss_u 0.9990 (0.9659) acc_u 0.0000 (4.3750) lr 1.8686e-03 eta 0:00:23
epoch [35/200] batch [40/85] time 0.411 (0.464) data 0.281 (0.333) loss_u loss_u 0.9712 (0.9664) acc_u 3.1250 (4.2969) lr 1.8686e-03 eta 0:00:20
epoch [35/200] batch [45/85] time 0.471 (0.460) data 0.340 (0.329) loss_u loss_u 0.9922 (0.9658) acc_u 0.0000 (4.3750) lr 1.8686e-03 eta 0:00:18
epoch [35/200] batch [50/85] time 0.530 (0.460) data 0.399 (0.329) loss_u loss_u 0.9663 (0.9655) acc_u 6.2500 (4.4375) lr 1.8686e-03 eta 0:00:16
epoch [35/200] batch [55/85] time 0.438 (0.459) data 0.307 (0.328) loss_u loss_u 0.9814 (0.9657) acc_u 3.1250 (4.5455) lr 1.8686e-03 eta 0:00:13
epoch [35/200] batch [60/85] time 0.412 (0.457) data 0.282 (0.327) loss_u loss_u 0.9106 (0.9638) acc_u 9.3750 (4.6875) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [65/85] time 0.605 (0.458) data 0.474 (0.327) loss_u loss_u 0.9648 (0.9648) acc_u 3.1250 (4.6154) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [70/85] time 0.446 (0.457) data 0.315 (0.327) loss_u loss_u 0.9712 (0.9648) acc_u 6.2500 (4.6875) lr 1.8686e-03 eta 0:00:06
epoch [35/200] batch [75/85] time 0.405 (0.457) data 0.274 (0.327) loss_u loss_u 0.9702 (0.9655) acc_u 3.1250 (4.5417) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [80/85] time 0.366 (0.455) data 0.235 (0.324) loss_u loss_u 0.9751 (0.9651) acc_u 3.1250 (4.6094) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [85/85] time 0.372 (0.453) data 0.243 (0.322) loss_u loss_u 0.9873 (0.9646) acc_u 0.0000 (4.6691) lr 1.8686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1801
confident_label rate tensor(0.1320, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 414
clean true:403
clean false:11
clean_rate:0.9734299516908212
noisy true:932
noisy false:1790
after delete: len(clean_dataset) 414
after delete: len(noisy_dataset) 2722
epoch [36/200] batch [5/12] time 0.560 (0.463) data 0.430 (0.332) loss_x loss_x 1.1602 (1.4926) acc_x 71.8750 (63.7500) lr 1.8607e-03 eta 0:00:03
epoch [36/200] batch [10/12] time 0.470 (0.472) data 0.339 (0.341) loss_x loss_x 1.2881 (1.3985) acc_x 71.8750 (66.5625) lr 1.8607e-03 eta 0:00:00
epoch [36/200] batch [5/85] time 0.530 (0.462) data 0.399 (0.331) loss_u loss_u 0.9849 (0.9713) acc_u 3.1250 (3.7500) lr 1.8607e-03 eta 0:00:36
epoch [36/200] batch [10/85] time 0.499 (0.462) data 0.368 (0.331) loss_u loss_u 0.9629 (0.9687) acc_u 6.2500 (3.7500) lr 1.8607e-03 eta 0:00:34
epoch [36/200] batch [15/85] time 0.418 (0.468) data 0.286 (0.337) loss_u loss_u 0.9595 (0.9667) acc_u 6.2500 (4.5833) lr 1.8607e-03 eta 0:00:32
epoch [36/200] batch [20/85] time 0.362 (0.464) data 0.230 (0.333) loss_u loss_u 0.9023 (0.9620) acc_u 9.3750 (5.1562) lr 1.8607e-03 eta 0:00:30
epoch [36/200] batch [25/85] time 0.397 (0.465) data 0.265 (0.334) loss_u loss_u 0.9272 (0.9606) acc_u 6.2500 (4.7500) lr 1.8607e-03 eta 0:00:27
epoch [36/200] batch [30/85] time 0.442 (0.458) data 0.310 (0.327) loss_u loss_u 0.9883 (0.9637) acc_u 0.0000 (4.3750) lr 1.8607e-03 eta 0:00:25
epoch [36/200] batch [35/85] time 0.499 (0.455) data 0.369 (0.324) loss_u loss_u 0.9683 (0.9638) acc_u 3.1250 (4.2857) lr 1.8607e-03 eta 0:00:22
epoch [36/200] batch [40/85] time 0.448 (0.456) data 0.316 (0.325) loss_u loss_u 0.9663 (0.9652) acc_u 3.1250 (4.0625) lr 1.8607e-03 eta 0:00:20
epoch [36/200] batch [45/85] time 0.493 (0.458) data 0.362 (0.327) loss_u loss_u 0.9648 (0.9647) acc_u 0.0000 (4.2361) lr 1.8607e-03 eta 0:00:18
epoch [36/200] batch [50/85] time 0.371 (0.457) data 0.240 (0.325) loss_u loss_u 0.9067 (0.9641) acc_u 12.5000 (4.2500) lr 1.8607e-03 eta 0:00:15
epoch [36/200] batch [55/85] time 0.434 (0.456) data 0.304 (0.325) loss_u loss_u 0.9805 (0.9643) acc_u 3.1250 (4.2614) lr 1.8607e-03 eta 0:00:13
epoch [36/200] batch [60/85] time 0.456 (0.455) data 0.325 (0.324) loss_u loss_u 0.9995 (0.9660) acc_u 0.0000 (4.0104) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [65/85] time 0.429 (0.454) data 0.298 (0.323) loss_u loss_u 0.9521 (0.9650) acc_u 6.2500 (4.1346) lr 1.8607e-03 eta 0:00:09
epoch [36/200] batch [70/85] time 0.522 (0.457) data 0.390 (0.326) loss_u loss_u 0.9414 (0.9637) acc_u 9.3750 (4.4196) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [75/85] time 0.445 (0.456) data 0.314 (0.325) loss_u loss_u 0.9824 (0.9634) acc_u 3.1250 (4.5833) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [80/85] time 0.429 (0.453) data 0.299 (0.322) loss_u loss_u 0.9756 (0.9630) acc_u 0.0000 (4.5703) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [85/85] time 0.400 (0.452) data 0.270 (0.321) loss_u loss_u 0.9653 (0.9634) acc_u 6.2500 (4.5221) lr 1.8607e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1811
confident_label rate tensor(0.1282, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 402
clean true:387
clean false:15
clean_rate:0.9626865671641791
noisy true:938
noisy false:1796
after delete: len(clean_dataset) 402
after delete: len(noisy_dataset) 2734
epoch [37/200] batch [5/12] time 0.543 (0.518) data 0.413 (0.388) loss_x loss_x 1.0635 (1.3215) acc_x 78.1250 (69.3750) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [10/12] time 0.458 (0.474) data 0.328 (0.344) loss_x loss_x 1.3740 (1.3960) acc_x 59.3750 (65.6250) lr 1.8526e-03 eta 0:00:00
epoch [37/200] batch [5/85] time 0.485 (0.469) data 0.355 (0.339) loss_u loss_u 0.9780 (0.9777) acc_u 3.1250 (1.8750) lr 1.8526e-03 eta 0:00:37
epoch [37/200] batch [10/85] time 0.443 (0.462) data 0.313 (0.331) loss_u loss_u 0.9741 (0.9748) acc_u 3.1250 (3.7500) lr 1.8526e-03 eta 0:00:34
epoch [37/200] batch [15/85] time 0.752 (0.477) data 0.622 (0.347) loss_u loss_u 0.9468 (0.9723) acc_u 9.3750 (3.9583) lr 1.8526e-03 eta 0:00:33
epoch [37/200] batch [20/85] time 0.470 (0.474) data 0.340 (0.343) loss_u loss_u 0.9507 (0.9658) acc_u 12.5000 (5.1562) lr 1.8526e-03 eta 0:00:30
epoch [37/200] batch [25/85] time 0.487 (0.476) data 0.356 (0.346) loss_u loss_u 0.9849 (0.9654) acc_u 0.0000 (4.8750) lr 1.8526e-03 eta 0:00:28
epoch [37/200] batch [30/85] time 0.410 (0.480) data 0.278 (0.349) loss_u loss_u 0.9487 (0.9648) acc_u 6.2500 (5.0000) lr 1.8526e-03 eta 0:00:26
epoch [37/200] batch [35/85] time 0.440 (0.471) data 0.309 (0.340) loss_u loss_u 0.9502 (0.9645) acc_u 6.2500 (4.9107) lr 1.8526e-03 eta 0:00:23
epoch [37/200] batch [40/85] time 0.455 (0.471) data 0.323 (0.341) loss_u loss_u 0.9951 (0.9633) acc_u 3.1250 (5.0000) lr 1.8526e-03 eta 0:00:21
epoch [37/200] batch [45/85] time 0.338 (0.464) data 0.206 (0.333) loss_u loss_u 0.9673 (0.9635) acc_u 3.1250 (4.9306) lr 1.8526e-03 eta 0:00:18
epoch [37/200] batch [50/85] time 0.585 (0.468) data 0.453 (0.337) loss_u loss_u 0.9736 (0.9644) acc_u 3.1250 (4.7500) lr 1.8526e-03 eta 0:00:16
epoch [37/200] batch [55/85] time 0.407 (0.465) data 0.276 (0.334) loss_u loss_u 0.9355 (0.9650) acc_u 9.3750 (4.6591) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [60/85] time 0.404 (0.463) data 0.273 (0.332) loss_u loss_u 0.9644 (0.9655) acc_u 9.3750 (4.6354) lr 1.8526e-03 eta 0:00:11
epoch [37/200] batch [65/85] time 0.403 (0.460) data 0.272 (0.329) loss_u loss_u 0.9458 (0.9653) acc_u 9.3750 (4.8077) lr 1.8526e-03 eta 0:00:09
epoch [37/200] batch [70/85] time 0.386 (0.455) data 0.254 (0.324) loss_u loss_u 0.9272 (0.9651) acc_u 9.3750 (4.7768) lr 1.8526e-03 eta 0:00:06
epoch [37/200] batch [75/85] time 0.319 (0.452) data 0.187 (0.321) loss_u loss_u 0.9380 (0.9642) acc_u 9.3750 (4.9167) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [80/85] time 0.580 (0.452) data 0.450 (0.321) loss_u loss_u 0.9917 (0.9641) acc_u 0.0000 (4.8828) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [85/85] time 0.447 (0.452) data 0.317 (0.321) loss_u loss_u 0.9790 (0.9647) acc_u 0.0000 (4.7426) lr 1.8526e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1822
confident_label rate tensor(0.1276, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 400
clean true:387
clean false:13
clean_rate:0.9675
noisy true:927
noisy false:1809
after delete: len(clean_dataset) 400
after delete: len(noisy_dataset) 2736
epoch [38/200] batch [5/12] time 0.451 (0.463) data 0.321 (0.333) loss_x loss_x 1.6104 (1.6762) acc_x 62.5000 (61.2500) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [10/12] time 0.437 (0.478) data 0.307 (0.348) loss_x loss_x 1.8076 (1.6477) acc_x 53.1250 (60.3125) lr 1.8443e-03 eta 0:00:00
epoch [38/200] batch [5/85] time 0.466 (0.463) data 0.335 (0.333) loss_u loss_u 0.9658 (0.9519) acc_u 0.0000 (6.8750) lr 1.8443e-03 eta 0:00:37
epoch [38/200] batch [10/85] time 0.416 (0.475) data 0.286 (0.344) loss_u loss_u 0.9561 (0.9638) acc_u 3.1250 (4.6875) lr 1.8443e-03 eta 0:00:35
epoch [38/200] batch [15/85] time 0.456 (0.475) data 0.325 (0.344) loss_u loss_u 0.9277 (0.9563) acc_u 12.5000 (5.8333) lr 1.8443e-03 eta 0:00:33
epoch [38/200] batch [20/85] time 0.415 (0.466) data 0.285 (0.336) loss_u loss_u 0.9844 (0.9605) acc_u 0.0000 (4.8438) lr 1.8443e-03 eta 0:00:30
epoch [38/200] batch [25/85] time 0.422 (0.461) data 0.292 (0.330) loss_u loss_u 0.9448 (0.9612) acc_u 6.2500 (4.7500) lr 1.8443e-03 eta 0:00:27
epoch [38/200] batch [30/85] time 0.446 (0.465) data 0.316 (0.334) loss_u loss_u 0.9736 (0.9648) acc_u 3.1250 (4.3750) lr 1.8443e-03 eta 0:00:25
epoch [38/200] batch [35/85] time 0.451 (0.465) data 0.319 (0.335) loss_u loss_u 0.9751 (0.9655) acc_u 3.1250 (4.4643) lr 1.8443e-03 eta 0:00:23
epoch [38/200] batch [40/85] time 0.466 (0.463) data 0.334 (0.332) loss_u loss_u 0.9536 (0.9644) acc_u 6.2500 (4.7656) lr 1.8443e-03 eta 0:00:20
epoch [38/200] batch [45/85] time 0.473 (0.460) data 0.342 (0.329) loss_u loss_u 0.9771 (0.9657) acc_u 0.0000 (4.5833) lr 1.8443e-03 eta 0:00:18
epoch [38/200] batch [50/85] time 0.411 (0.460) data 0.280 (0.330) loss_u loss_u 0.9600 (0.9662) acc_u 3.1250 (4.5625) lr 1.8443e-03 eta 0:00:16
epoch [38/200] batch [55/85] time 0.343 (0.457) data 0.212 (0.326) loss_u loss_u 0.9810 (0.9662) acc_u 3.1250 (4.5455) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [60/85] time 0.315 (0.455) data 0.184 (0.324) loss_u loss_u 0.9941 (0.9665) acc_u 0.0000 (4.4792) lr 1.8443e-03 eta 0:00:11
epoch [38/200] batch [65/85] time 0.489 (0.455) data 0.358 (0.324) loss_u loss_u 0.9570 (0.9663) acc_u 6.2500 (4.5673) lr 1.8443e-03 eta 0:00:09
epoch [38/200] batch [70/85] time 0.386 (0.453) data 0.255 (0.322) loss_u loss_u 0.9312 (0.9665) acc_u 6.2500 (4.5089) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [75/85] time 0.497 (0.451) data 0.366 (0.320) loss_u loss_u 0.9731 (0.9668) acc_u 6.2500 (4.5000) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [80/85] time 0.441 (0.449) data 0.310 (0.318) loss_u loss_u 0.9854 (0.9679) acc_u 3.1250 (4.3359) lr 1.8443e-03 eta 0:00:02
epoch [38/200] batch [85/85] time 0.401 (0.449) data 0.269 (0.318) loss_u loss_u 0.9561 (0.9677) acc_u 3.1250 (4.3382) lr 1.8443e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1836
confident_label rate tensor(0.1234, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 387
clean true:381
clean false:6
clean_rate:0.9844961240310077
noisy true:919
noisy false:1830
after delete: len(clean_dataset) 387
after delete: len(noisy_dataset) 2749
epoch [39/200] batch [5/12] time 0.476 (0.460) data 0.345 (0.329) loss_x loss_x 1.8330 (1.4663) acc_x 59.3750 (61.2500) lr 1.8358e-03 eta 0:00:03
epoch [39/200] batch [10/12] time 0.408 (0.462) data 0.278 (0.331) loss_x loss_x 1.4844 (1.4297) acc_x 56.2500 (65.0000) lr 1.8358e-03 eta 0:00:00
epoch [39/200] batch [5/85] time 0.419 (0.464) data 0.289 (0.333) loss_u loss_u 0.9658 (0.9640) acc_u 6.2500 (4.3750) lr 1.8358e-03 eta 0:00:37
epoch [39/200] batch [10/85] time 0.499 (0.453) data 0.368 (0.323) loss_u loss_u 0.9565 (0.9640) acc_u 6.2500 (4.3750) lr 1.8358e-03 eta 0:00:33
epoch [39/200] batch [15/85] time 0.510 (0.456) data 0.379 (0.325) loss_u loss_u 0.9893 (0.9695) acc_u 0.0000 (3.9583) lr 1.8358e-03 eta 0:00:31
epoch [39/200] batch [20/85] time 0.432 (0.459) data 0.301 (0.328) loss_u loss_u 0.9707 (0.9674) acc_u 3.1250 (4.0625) lr 1.8358e-03 eta 0:00:29
epoch [39/200] batch [25/85] time 0.484 (0.457) data 0.353 (0.327) loss_u loss_u 0.8931 (0.9636) acc_u 18.7500 (4.8750) lr 1.8358e-03 eta 0:00:27
epoch [39/200] batch [30/85] time 0.531 (0.458) data 0.401 (0.327) loss_u loss_u 0.9736 (0.9653) acc_u 6.2500 (4.7917) lr 1.8358e-03 eta 0:00:25
epoch [39/200] batch [35/85] time 0.491 (0.456) data 0.359 (0.326) loss_u loss_u 0.9307 (0.9621) acc_u 9.3750 (5.3571) lr 1.8358e-03 eta 0:00:22
epoch [39/200] batch [40/85] time 0.381 (0.455) data 0.250 (0.324) loss_u loss_u 0.9429 (0.9628) acc_u 9.3750 (5.2344) lr 1.8358e-03 eta 0:00:20
epoch [39/200] batch [45/85] time 0.346 (0.452) data 0.215 (0.321) loss_u loss_u 0.9517 (0.9620) acc_u 9.3750 (5.4167) lr 1.8358e-03 eta 0:00:18
epoch [39/200] batch [50/85] time 0.369 (0.449) data 0.238 (0.318) loss_u loss_u 0.9868 (0.9630) acc_u 0.0000 (5.1250) lr 1.8358e-03 eta 0:00:15
epoch [39/200] batch [55/85] time 0.435 (0.450) data 0.304 (0.319) loss_u loss_u 0.9956 (0.9633) acc_u 0.0000 (5.0000) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [60/85] time 0.413 (0.448) data 0.282 (0.317) loss_u loss_u 0.9717 (0.9637) acc_u 3.1250 (4.9479) lr 1.8358e-03 eta 0:00:11
epoch [39/200] batch [65/85] time 0.498 (0.447) data 0.368 (0.316) loss_u loss_u 0.9580 (0.9638) acc_u 6.2500 (4.9038) lr 1.8358e-03 eta 0:00:08
epoch [39/200] batch [70/85] time 0.418 (0.446) data 0.288 (0.315) loss_u loss_u 0.9565 (0.9637) acc_u 3.1250 (4.8214) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [75/85] time 0.320 (0.444) data 0.189 (0.313) loss_u loss_u 0.9756 (0.9642) acc_u 0.0000 (4.7083) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [80/85] time 0.875 (0.449) data 0.744 (0.319) loss_u loss_u 0.9385 (0.9647) acc_u 6.2500 (4.5703) lr 1.8358e-03 eta 0:00:02
epoch [39/200] batch [85/85] time 0.441 (0.450) data 0.310 (0.319) loss_u loss_u 0.9722 (0.9654) acc_u 6.2500 (4.4853) lr 1.8358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1822
confident_label rate tensor(0.1311, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 411
clean true:400
clean false:11
clean_rate:0.9732360097323601
noisy true:914
noisy false:1811
after delete: len(clean_dataset) 411
after delete: len(noisy_dataset) 2725
epoch [40/200] batch [5/12] time 0.560 (0.437) data 0.429 (0.307) loss_x loss_x 1.6377 (1.3633) acc_x 56.2500 (66.8750) lr 1.8271e-03 eta 0:00:03
epoch [40/200] batch [10/12] time 0.428 (0.474) data 0.297 (0.344) loss_x loss_x 1.1553 (1.4010) acc_x 75.0000 (66.5625) lr 1.8271e-03 eta 0:00:00
epoch [40/200] batch [5/85] time 0.487 (0.453) data 0.356 (0.323) loss_u loss_u 0.9800 (0.9520) acc_u 3.1250 (6.2500) lr 1.8271e-03 eta 0:00:36
epoch [40/200] batch [10/85] time 0.501 (0.463) data 0.369 (0.332) loss_u loss_u 0.9624 (0.9637) acc_u 0.0000 (5.0000) lr 1.8271e-03 eta 0:00:34
epoch [40/200] batch [15/85] time 0.416 (0.471) data 0.285 (0.340) loss_u loss_u 0.9497 (0.9640) acc_u 6.2500 (4.5833) lr 1.8271e-03 eta 0:00:32
epoch [40/200] batch [20/85] time 0.560 (0.470) data 0.429 (0.339) loss_u loss_u 0.9497 (0.9628) acc_u 3.1250 (4.3750) lr 1.8271e-03 eta 0:00:30
epoch [40/200] batch [25/85] time 0.592 (0.468) data 0.461 (0.337) loss_u loss_u 0.9775 (0.9659) acc_u 6.2500 (4.1250) lr 1.8271e-03 eta 0:00:28
epoch [40/200] batch [30/85] time 0.378 (0.465) data 0.247 (0.333) loss_u loss_u 0.9790 (0.9649) acc_u 3.1250 (4.2708) lr 1.8271e-03 eta 0:00:25
epoch [40/200] batch [35/85] time 0.435 (0.457) data 0.303 (0.326) loss_u loss_u 0.9658 (0.9650) acc_u 3.1250 (4.0179) lr 1.8271e-03 eta 0:00:22
epoch [40/200] batch [40/85] time 0.456 (0.459) data 0.324 (0.328) loss_u loss_u 0.9883 (0.9671) acc_u 0.0000 (3.8281) lr 1.8271e-03 eta 0:00:20
epoch [40/200] batch [45/85] time 0.399 (0.456) data 0.267 (0.325) loss_u loss_u 0.9595 (0.9667) acc_u 3.1250 (3.8889) lr 1.8271e-03 eta 0:00:18
epoch [40/200] batch [50/85] time 0.530 (0.457) data 0.399 (0.326) loss_u loss_u 0.9468 (0.9661) acc_u 6.2500 (4.1250) lr 1.8271e-03 eta 0:00:15
epoch [40/200] batch [55/85] time 0.568 (0.460) data 0.436 (0.329) loss_u loss_u 0.9775 (0.9661) acc_u 0.0000 (3.9773) lr 1.8271e-03 eta 0:00:13
epoch [40/200] batch [60/85] time 0.493 (0.461) data 0.361 (0.330) loss_u loss_u 0.9766 (0.9653) acc_u 6.2500 (4.1667) lr 1.8271e-03 eta 0:00:11
epoch [40/200] batch [65/85] time 0.356 (0.457) data 0.226 (0.326) loss_u loss_u 0.9521 (0.9647) acc_u 6.2500 (4.3750) lr 1.8271e-03 eta 0:00:09
epoch [40/200] batch [70/85] time 0.528 (0.455) data 0.396 (0.324) loss_u loss_u 0.9839 (0.9653) acc_u 3.1250 (4.4196) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [75/85] time 0.364 (0.456) data 0.234 (0.325) loss_u loss_u 0.9639 (0.9651) acc_u 6.2500 (4.4583) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [80/85] time 0.548 (0.455) data 0.417 (0.324) loss_u loss_u 0.9673 (0.9651) acc_u 3.1250 (4.4531) lr 1.8271e-03 eta 0:00:02
epoch [40/200] batch [85/85] time 0.383 (0.451) data 0.253 (0.320) loss_u loss_u 0.9819 (0.9664) acc_u 3.1250 (4.3015) lr 1.8271e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1829
confident_label rate tensor(0.1240, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 389
clean true:379
clean false:10
clean_rate:0.974293059125964
noisy true:928
noisy false:1819
after delete: len(clean_dataset) 389
after delete: len(noisy_dataset) 2747
epoch [41/200] batch [5/12] time 0.452 (0.456) data 0.321 (0.326) loss_x loss_x 1.3408 (1.4262) acc_x 68.7500 (66.2500) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [10/12] time 0.387 (0.481) data 0.256 (0.351) loss_x loss_x 1.0439 (1.4006) acc_x 75.0000 (66.2500) lr 1.8181e-03 eta 0:00:00
epoch [41/200] batch [5/85] time 0.445 (0.475) data 0.315 (0.345) loss_u loss_u 0.9634 (0.9509) acc_u 3.1250 (5.6250) lr 1.8181e-03 eta 0:00:38
epoch [41/200] batch [10/85] time 0.472 (0.471) data 0.342 (0.341) loss_u loss_u 0.9575 (0.9594) acc_u 3.1250 (5.0000) lr 1.8181e-03 eta 0:00:35
epoch [41/200] batch [15/85] time 0.478 (0.467) data 0.348 (0.337) loss_u loss_u 0.9800 (0.9622) acc_u 3.1250 (5.0000) lr 1.8181e-03 eta 0:00:32
epoch [41/200] batch [20/85] time 0.440 (0.458) data 0.310 (0.328) loss_u loss_u 0.9409 (0.9616) acc_u 6.2500 (5.0000) lr 1.8181e-03 eta 0:00:29
epoch [41/200] batch [25/85] time 0.342 (0.455) data 0.211 (0.324) loss_u loss_u 0.9771 (0.9628) acc_u 3.1250 (5.0000) lr 1.8181e-03 eta 0:00:27
epoch [41/200] batch [30/85] time 0.473 (0.456) data 0.343 (0.326) loss_u loss_u 0.9365 (0.9641) acc_u 12.5000 (4.8958) lr 1.8181e-03 eta 0:00:25
epoch [41/200] batch [35/85] time 0.395 (0.449) data 0.265 (0.318) loss_u loss_u 0.9785 (0.9625) acc_u 3.1250 (5.0000) lr 1.8181e-03 eta 0:00:22
epoch [41/200] batch [40/85] time 0.458 (0.451) data 0.327 (0.321) loss_u loss_u 0.9448 (0.9599) acc_u 6.2500 (5.4688) lr 1.8181e-03 eta 0:00:20
epoch [41/200] batch [45/85] time 0.416 (0.451) data 0.284 (0.321) loss_u loss_u 0.9395 (0.9602) acc_u 6.2500 (5.3472) lr 1.8181e-03 eta 0:00:18
epoch [41/200] batch [50/85] time 0.584 (0.454) data 0.452 (0.324) loss_u loss_u 0.9404 (0.9591) acc_u 6.2500 (5.4375) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [55/85] time 0.464 (0.454) data 0.333 (0.323) loss_u loss_u 0.9639 (0.9602) acc_u 6.2500 (5.2841) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [60/85] time 0.595 (0.456) data 0.463 (0.326) loss_u loss_u 0.9434 (0.9598) acc_u 9.3750 (5.3646) lr 1.8181e-03 eta 0:00:11
epoch [41/200] batch [65/85] time 0.345 (0.457) data 0.215 (0.326) loss_u loss_u 0.9067 (0.9596) acc_u 12.5000 (5.3846) lr 1.8181e-03 eta 0:00:09
epoch [41/200] batch [70/85] time 0.384 (0.455) data 0.254 (0.324) loss_u loss_u 0.9438 (0.9605) acc_u 9.3750 (5.3125) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [75/85] time 0.405 (0.457) data 0.275 (0.326) loss_u loss_u 0.9604 (0.9609) acc_u 6.2500 (5.2500) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [80/85] time 0.429 (0.455) data 0.299 (0.324) loss_u loss_u 0.9727 (0.9621) acc_u 3.1250 (5.0391) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [85/85] time 0.558 (0.455) data 0.427 (0.324) loss_u loss_u 0.9092 (0.9614) acc_u 15.6250 (5.2574) lr 1.8181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1786
confident_label rate tensor(0.1237, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 388
clean true:375
clean false:13
clean_rate:0.9664948453608248
noisy true:975
noisy false:1773
after delete: len(clean_dataset) 388
after delete: len(noisy_dataset) 2748
epoch [42/200] batch [5/12] time 0.452 (0.476) data 0.322 (0.346) loss_x loss_x 1.4053 (1.3555) acc_x 65.6250 (66.2500) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [10/12] time 0.465 (0.467) data 0.335 (0.337) loss_x loss_x 1.5176 (1.3697) acc_x 50.0000 (67.1875) lr 1.8090e-03 eta 0:00:00
epoch [42/200] batch [5/85] time 0.402 (0.466) data 0.271 (0.336) loss_u loss_u 0.9614 (0.9630) acc_u 6.2500 (6.2500) lr 1.8090e-03 eta 0:00:37
epoch [42/200] batch [10/85] time 0.526 (0.460) data 0.395 (0.329) loss_u loss_u 0.9707 (0.9686) acc_u 6.2500 (5.3125) lr 1.8090e-03 eta 0:00:34
epoch [42/200] batch [15/85] time 0.585 (0.466) data 0.454 (0.335) loss_u loss_u 0.9785 (0.9662) acc_u 3.1250 (5.8333) lr 1.8090e-03 eta 0:00:32
epoch [42/200] batch [20/85] time 0.405 (0.460) data 0.274 (0.329) loss_u loss_u 0.9526 (0.9642) acc_u 12.5000 (6.2500) lr 1.8090e-03 eta 0:00:29
epoch [42/200] batch [25/85] time 0.537 (0.466) data 0.405 (0.335) loss_u loss_u 0.9604 (0.9653) acc_u 3.1250 (5.6250) lr 1.8090e-03 eta 0:00:27
epoch [42/200] batch [30/85] time 0.400 (0.461) data 0.268 (0.330) loss_u loss_u 0.9375 (0.9650) acc_u 6.2500 (5.4167) lr 1.8090e-03 eta 0:00:25
epoch [42/200] batch [35/85] time 0.400 (0.461) data 0.269 (0.330) loss_u loss_u 0.9443 (0.9636) acc_u 6.2500 (5.4464) lr 1.8090e-03 eta 0:00:23
epoch [42/200] batch [40/85] time 0.441 (0.459) data 0.310 (0.328) loss_u loss_u 0.9849 (0.9637) acc_u 3.1250 (5.5469) lr 1.8090e-03 eta 0:00:20
epoch [42/200] batch [45/85] time 0.429 (0.457) data 0.299 (0.326) loss_u loss_u 0.9844 (0.9650) acc_u 0.0000 (5.2778) lr 1.8090e-03 eta 0:00:18
epoch [42/200] batch [50/85] time 0.420 (0.460) data 0.288 (0.329) loss_u loss_u 0.9575 (0.9627) acc_u 3.1250 (5.5000) lr 1.8090e-03 eta 0:00:16
epoch [42/200] batch [55/85] time 0.394 (0.457) data 0.263 (0.326) loss_u loss_u 0.9800 (0.9632) acc_u 6.2500 (5.3977) lr 1.8090e-03 eta 0:00:13
epoch [42/200] batch [60/85] time 0.496 (0.457) data 0.364 (0.326) loss_u loss_u 0.9150 (0.9615) acc_u 12.5000 (5.7292) lr 1.8090e-03 eta 0:00:11
epoch [42/200] batch [65/85] time 0.451 (0.457) data 0.321 (0.326) loss_u loss_u 0.9785 (0.9615) acc_u 3.1250 (5.6731) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [70/85] time 0.438 (0.456) data 0.307 (0.324) loss_u loss_u 0.9580 (0.9618) acc_u 6.2500 (5.6250) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [75/85] time 0.367 (0.451) data 0.236 (0.320) loss_u loss_u 0.9937 (0.9623) acc_u 0.0000 (5.5417) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [80/85] time 0.467 (0.451) data 0.335 (0.320) loss_u loss_u 0.9771 (0.9630) acc_u 3.1250 (5.3906) lr 1.8090e-03 eta 0:00:02
epoch [42/200] batch [85/85] time 0.547 (0.453) data 0.416 (0.322) loss_u loss_u 0.9697 (0.9635) acc_u 3.1250 (5.2941) lr 1.8090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1803
confident_label rate tensor(0.1279, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 401
clean true:391
clean false:10
clean_rate:0.9750623441396509
noisy true:942
noisy false:1793
after delete: len(clean_dataset) 401
after delete: len(noisy_dataset) 2735
epoch [43/200] batch [5/12] time 0.594 (0.586) data 0.463 (0.456) loss_x loss_x 0.6870 (1.1802) acc_x 84.3750 (69.3750) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [10/12] time 0.423 (0.531) data 0.294 (0.401) loss_x loss_x 1.8730 (1.4003) acc_x 56.2500 (64.0625) lr 1.7997e-03 eta 0:00:01
epoch [43/200] batch [5/85] time 0.484 (0.504) data 0.352 (0.373) loss_u loss_u 0.9468 (0.9581) acc_u 6.2500 (5.6250) lr 1.7997e-03 eta 0:00:40
epoch [43/200] batch [10/85] time 0.450 (0.491) data 0.319 (0.360) loss_u loss_u 0.9976 (0.9610) acc_u 0.0000 (5.6250) lr 1.7997e-03 eta 0:00:36
epoch [43/200] batch [15/85] time 0.363 (0.479) data 0.232 (0.348) loss_u loss_u 0.9766 (0.9581) acc_u 3.1250 (6.0417) lr 1.7997e-03 eta 0:00:33
epoch [43/200] batch [20/85] time 0.404 (0.469) data 0.274 (0.338) loss_u loss_u 0.9419 (0.9609) acc_u 6.2500 (5.6250) lr 1.7997e-03 eta 0:00:30
epoch [43/200] batch [25/85] time 0.533 (0.470) data 0.402 (0.340) loss_u loss_u 0.9741 (0.9609) acc_u 3.1250 (5.3750) lr 1.7997e-03 eta 0:00:28
epoch [43/200] batch [30/85] time 0.382 (0.467) data 0.250 (0.336) loss_u loss_u 0.9951 (0.9620) acc_u 0.0000 (5.2083) lr 1.7997e-03 eta 0:00:25
epoch [43/200] batch [35/85] time 0.379 (0.459) data 0.248 (0.328) loss_u loss_u 0.9912 (0.9631) acc_u 0.0000 (4.9107) lr 1.7997e-03 eta 0:00:22
epoch [43/200] batch [40/85] time 0.414 (0.460) data 0.283 (0.329) loss_u loss_u 0.9409 (0.9607) acc_u 9.3750 (5.1562) lr 1.7997e-03 eta 0:00:20
epoch [43/200] batch [45/85] time 0.391 (0.455) data 0.260 (0.324) loss_u loss_u 0.9902 (0.9627) acc_u 3.1250 (4.8611) lr 1.7997e-03 eta 0:00:18
epoch [43/200] batch [50/85] time 0.555 (0.458) data 0.424 (0.327) loss_u loss_u 0.9849 (0.9624) acc_u 3.1250 (4.9375) lr 1.7997e-03 eta 0:00:16
epoch [43/200] batch [55/85] time 0.402 (0.456) data 0.271 (0.325) loss_u loss_u 0.9624 (0.9631) acc_u 6.2500 (4.8864) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [60/85] time 0.350 (0.453) data 0.218 (0.322) loss_u loss_u 0.9551 (0.9639) acc_u 3.1250 (4.7396) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [65/85] time 0.494 (0.457) data 0.362 (0.326) loss_u loss_u 0.9692 (0.9640) acc_u 3.1250 (4.6154) lr 1.7997e-03 eta 0:00:09
epoch [43/200] batch [70/85] time 0.378 (0.459) data 0.247 (0.328) loss_u loss_u 0.9795 (0.9644) acc_u 0.0000 (4.5089) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [75/85] time 0.587 (0.461) data 0.456 (0.330) loss_u loss_u 0.9863 (0.9642) acc_u 0.0000 (4.5000) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [80/85] time 0.407 (0.459) data 0.276 (0.328) loss_u loss_u 0.9751 (0.9640) acc_u 6.2500 (4.6094) lr 1.7997e-03 eta 0:00:02
epoch [43/200] batch [85/85] time 0.357 (0.456) data 0.226 (0.325) loss_u loss_u 0.9375 (0.9636) acc_u 6.2500 (4.6324) lr 1.7997e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1814
confident_label rate tensor(0.1256, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 394
clean true:380
clean false:14
clean_rate:0.9644670050761421
noisy true:942
noisy false:1800
after delete: len(clean_dataset) 394
after delete: len(noisy_dataset) 2742
epoch [44/200] batch [5/12] time 0.436 (0.456) data 0.305 (0.325) loss_x loss_x 1.3057 (1.5449) acc_x 62.5000 (59.3750) lr 1.7902e-03 eta 0:00:03
epoch [44/200] batch [10/12] time 0.414 (0.441) data 0.283 (0.310) loss_x loss_x 1.2480 (1.5234) acc_x 78.1250 (61.2500) lr 1.7902e-03 eta 0:00:00
epoch [44/200] batch [5/85] time 0.591 (0.450) data 0.459 (0.319) loss_u loss_u 0.9673 (0.9605) acc_u 6.2500 (6.8750) lr 1.7902e-03 eta 0:00:36
epoch [44/200] batch [10/85] time 0.598 (0.463) data 0.466 (0.332) loss_u loss_u 0.9971 (0.9668) acc_u 0.0000 (5.6250) lr 1.7902e-03 eta 0:00:34
epoch [44/200] batch [15/85] time 0.410 (0.459) data 0.278 (0.328) loss_u loss_u 0.9780 (0.9689) acc_u 6.2500 (5.0000) lr 1.7902e-03 eta 0:00:32
epoch [44/200] batch [20/85] time 0.514 (0.471) data 0.382 (0.340) loss_u loss_u 0.9688 (0.9660) acc_u 6.2500 (5.3125) lr 1.7902e-03 eta 0:00:30
epoch [44/200] batch [25/85] time 0.414 (0.470) data 0.282 (0.339) loss_u loss_u 0.9961 (0.9657) acc_u 0.0000 (5.0000) lr 1.7902e-03 eta 0:00:28
epoch [44/200] batch [30/85] time 0.441 (0.468) data 0.310 (0.336) loss_u loss_u 0.9546 (0.9672) acc_u 9.3750 (4.7917) lr 1.7902e-03 eta 0:00:25
epoch [44/200] batch [35/85] time 0.405 (0.471) data 0.273 (0.339) loss_u loss_u 0.9727 (0.9655) acc_u 3.1250 (4.8214) lr 1.7902e-03 eta 0:00:23
epoch [44/200] batch [40/85] time 0.389 (0.467) data 0.257 (0.336) loss_u loss_u 0.9492 (0.9615) acc_u 6.2500 (5.2344) lr 1.7902e-03 eta 0:00:21
epoch [44/200] batch [45/85] time 0.470 (0.464) data 0.339 (0.333) loss_u loss_u 0.9927 (0.9615) acc_u 0.0000 (5.1389) lr 1.7902e-03 eta 0:00:18
epoch [44/200] batch [50/85] time 0.560 (0.466) data 0.428 (0.334) loss_u loss_u 0.9829 (0.9624) acc_u 0.0000 (4.9375) lr 1.7902e-03 eta 0:00:16
epoch [44/200] batch [55/85] time 0.380 (0.465) data 0.249 (0.334) loss_u loss_u 0.9863 (0.9624) acc_u 3.1250 (5.0000) lr 1.7902e-03 eta 0:00:13
epoch [44/200] batch [60/85] time 0.504 (0.462) data 0.373 (0.331) loss_u loss_u 0.9766 (0.9634) acc_u 0.0000 (4.8438) lr 1.7902e-03 eta 0:00:11
epoch [44/200] batch [65/85] time 0.416 (0.458) data 0.284 (0.327) loss_u loss_u 0.9468 (0.9631) acc_u 6.2500 (4.9038) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [70/85] time 0.393 (0.459) data 0.262 (0.328) loss_u loss_u 0.9614 (0.9618) acc_u 6.2500 (4.9554) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [75/85] time 0.493 (0.459) data 0.362 (0.328) loss_u loss_u 0.9434 (0.9618) acc_u 6.2500 (4.9167) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [80/85] time 0.392 (0.455) data 0.261 (0.324) loss_u loss_u 0.9824 (0.9623) acc_u 0.0000 (4.9219) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [85/85] time 0.505 (0.454) data 0.374 (0.322) loss_u loss_u 0.9741 (0.9623) acc_u 3.1250 (4.8897) lr 1.7902e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1836
confident_label rate tensor(0.1266, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 397
clean true:386
clean false:11
clean_rate:0.9722921914357683
noisy true:914
noisy false:1825
after delete: len(clean_dataset) 397
after delete: len(noisy_dataset) 2739
epoch [45/200] batch [5/12] time 0.551 (0.500) data 0.420 (0.369) loss_x loss_x 1.3740 (1.3695) acc_x 71.8750 (66.2500) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [10/12] time 0.566 (0.489) data 0.435 (0.358) loss_x loss_x 1.1582 (1.3278) acc_x 78.1250 (67.1875) lr 1.7804e-03 eta 0:00:00
epoch [45/200] batch [5/85] time 0.364 (0.459) data 0.235 (0.329) loss_u loss_u 0.9434 (0.9593) acc_u 6.2500 (6.2500) lr 1.7804e-03 eta 0:00:36
epoch [45/200] batch [10/85] time 0.396 (0.468) data 0.265 (0.338) loss_u loss_u 0.9346 (0.9518) acc_u 9.3750 (6.8750) lr 1.7804e-03 eta 0:00:35
epoch [45/200] batch [15/85] time 0.405 (0.458) data 0.273 (0.328) loss_u loss_u 0.9243 (0.9498) acc_u 9.3750 (6.8750) lr 1.7804e-03 eta 0:00:32
epoch [45/200] batch [20/85] time 0.491 (0.459) data 0.359 (0.328) loss_u loss_u 0.9658 (0.9528) acc_u 6.2500 (6.4062) lr 1.7804e-03 eta 0:00:29
epoch [45/200] batch [25/85] time 0.414 (0.458) data 0.283 (0.327) loss_u loss_u 0.9966 (0.9529) acc_u 0.0000 (6.3750) lr 1.7804e-03 eta 0:00:27
epoch [45/200] batch [30/85] time 0.702 (0.463) data 0.570 (0.332) loss_u loss_u 0.8901 (0.9537) acc_u 18.7500 (6.4583) lr 1.7804e-03 eta 0:00:25
epoch [45/200] batch [35/85] time 0.430 (0.460) data 0.298 (0.329) loss_u loss_u 0.9756 (0.9545) acc_u 3.1250 (6.2500) lr 1.7804e-03 eta 0:00:22
epoch [45/200] batch [40/85] time 0.391 (0.454) data 0.258 (0.322) loss_u loss_u 0.9810 (0.9565) acc_u 3.1250 (5.9375) lr 1.7804e-03 eta 0:00:20
epoch [45/200] batch [45/85] time 0.470 (0.452) data 0.337 (0.320) loss_u loss_u 0.9604 (0.9579) acc_u 6.2500 (5.7639) lr 1.7804e-03 eta 0:00:18
epoch [45/200] batch [50/85] time 0.454 (0.451) data 0.322 (0.319) loss_u loss_u 0.9497 (0.9577) acc_u 9.3750 (5.8125) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [55/85] time 0.424 (0.453) data 0.292 (0.322) loss_u loss_u 0.9053 (0.9585) acc_u 15.6250 (5.7386) lr 1.7804e-03 eta 0:00:13
epoch [45/200] batch [60/85] time 0.371 (0.453) data 0.240 (0.322) loss_u loss_u 0.9722 (0.9588) acc_u 3.1250 (5.6250) lr 1.7804e-03 eta 0:00:11
epoch [45/200] batch [65/85] time 0.475 (0.452) data 0.344 (0.320) loss_u loss_u 0.9761 (0.9583) acc_u 3.1250 (5.6731) lr 1.7804e-03 eta 0:00:09
epoch [45/200] batch [70/85] time 0.659 (0.452) data 0.529 (0.321) loss_u loss_u 0.9604 (0.9585) acc_u 6.2500 (5.6250) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [75/85] time 0.423 (0.454) data 0.293 (0.322) loss_u loss_u 0.9688 (0.9600) acc_u 6.2500 (5.4167) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [80/85] time 0.493 (0.454) data 0.363 (0.322) loss_u loss_u 0.9497 (0.9607) acc_u 6.2500 (5.2344) lr 1.7804e-03 eta 0:00:02
epoch [45/200] batch [85/85] time 0.396 (0.453) data 0.266 (0.322) loss_u loss_u 0.9653 (0.9604) acc_u 3.1250 (5.2574) lr 1.7804e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1767
confident_label rate tensor(0.1295, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 406
clean true:392
clean false:14
clean_rate:0.9655172413793104
noisy true:977
noisy false:1753
after delete: len(clean_dataset) 406
after delete: len(noisy_dataset) 2730
epoch [46/200] batch [5/12] time 0.392 (0.448) data 0.261 (0.317) loss_x loss_x 1.2910 (1.2992) acc_x 68.7500 (70.6250) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [10/12] time 0.456 (0.462) data 0.325 (0.331) loss_x loss_x 1.3818 (1.3284) acc_x 56.2500 (67.5000) lr 1.7705e-03 eta 0:00:00
epoch [46/200] batch [5/85] time 0.659 (0.496) data 0.528 (0.365) loss_u loss_u 0.9092 (0.9653) acc_u 12.5000 (4.3750) lr 1.7705e-03 eta 0:00:39
epoch [46/200] batch [10/85] time 0.451 (0.486) data 0.319 (0.355) loss_u loss_u 0.9395 (0.9639) acc_u 6.2500 (4.6875) lr 1.7705e-03 eta 0:00:36
epoch [46/200] batch [15/85] time 0.360 (0.468) data 0.228 (0.337) loss_u loss_u 0.9561 (0.9638) acc_u 6.2500 (4.5833) lr 1.7705e-03 eta 0:00:32
epoch [46/200] batch [20/85] time 0.505 (0.471) data 0.374 (0.340) loss_u loss_u 0.9419 (0.9636) acc_u 9.3750 (4.5312) lr 1.7705e-03 eta 0:00:30
epoch [46/200] batch [25/85] time 0.358 (0.471) data 0.227 (0.340) loss_u loss_u 0.9902 (0.9659) acc_u 3.1250 (4.6250) lr 1.7705e-03 eta 0:00:28
epoch [46/200] batch [30/85] time 0.445 (0.467) data 0.313 (0.335) loss_u loss_u 0.9688 (0.9661) acc_u 6.2500 (4.7917) lr 1.7705e-03 eta 0:00:25
epoch [46/200] batch [35/85] time 0.628 (0.463) data 0.496 (0.332) loss_u loss_u 0.9922 (0.9676) acc_u 0.0000 (4.2857) lr 1.7705e-03 eta 0:00:23
epoch [46/200] batch [40/85] time 0.481 (0.464) data 0.351 (0.333) loss_u loss_u 0.9644 (0.9656) acc_u 6.2500 (4.5312) lr 1.7705e-03 eta 0:00:20
epoch [46/200] batch [45/85] time 0.416 (0.461) data 0.285 (0.330) loss_u loss_u 0.9517 (0.9660) acc_u 6.2500 (4.5139) lr 1.7705e-03 eta 0:00:18
epoch [46/200] batch [50/85] time 0.480 (0.457) data 0.348 (0.326) loss_u loss_u 0.9678 (0.9652) acc_u 6.2500 (4.6875) lr 1.7705e-03 eta 0:00:16
epoch [46/200] batch [55/85] time 0.465 (0.459) data 0.334 (0.328) loss_u loss_u 0.9673 (0.9654) acc_u 6.2500 (4.7159) lr 1.7705e-03 eta 0:00:13
epoch [46/200] batch [60/85] time 0.485 (0.460) data 0.353 (0.329) loss_u loss_u 0.9771 (0.9655) acc_u 3.1250 (4.6875) lr 1.7705e-03 eta 0:00:11
epoch [46/200] batch [65/85] time 0.390 (0.458) data 0.259 (0.327) loss_u loss_u 0.9966 (0.9668) acc_u 0.0000 (4.4712) lr 1.7705e-03 eta 0:00:09
epoch [46/200] batch [70/85] time 0.482 (0.456) data 0.351 (0.325) loss_u loss_u 0.9897 (0.9659) acc_u 0.0000 (4.5536) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [75/85] time 0.482 (0.455) data 0.351 (0.324) loss_u loss_u 0.9541 (0.9655) acc_u 9.3750 (4.6667) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [80/85] time 0.503 (0.458) data 0.371 (0.327) loss_u loss_u 0.9531 (0.9657) acc_u 3.1250 (4.5703) lr 1.7705e-03 eta 0:00:02
epoch [46/200] batch [85/85] time 0.393 (0.458) data 0.262 (0.327) loss_u loss_u 0.9927 (0.9664) acc_u 0.0000 (4.5588) lr 1.7705e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1818
confident_label rate tensor(0.1304, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 409
clean true:399
clean false:10
clean_rate:0.9755501222493888
noisy true:919
noisy false:1808
after delete: len(clean_dataset) 409
after delete: len(noisy_dataset) 2727
epoch [47/200] batch [5/12] time 0.486 (0.494) data 0.356 (0.363) loss_x loss_x 1.2793 (1.3166) acc_x 59.3750 (66.2500) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [10/12] time 0.556 (0.484) data 0.427 (0.354) loss_x loss_x 1.1104 (1.2926) acc_x 68.7500 (68.1250) lr 1.7604e-03 eta 0:00:00
epoch [47/200] batch [5/85] time 0.408 (0.462) data 0.277 (0.332) loss_u loss_u 0.9365 (0.9420) acc_u 6.2500 (6.8750) lr 1.7604e-03 eta 0:00:36
epoch [47/200] batch [10/85] time 0.513 (0.459) data 0.383 (0.328) loss_u loss_u 0.9033 (0.9510) acc_u 12.5000 (6.2500) lr 1.7604e-03 eta 0:00:34
epoch [47/200] batch [15/85] time 0.377 (0.449) data 0.245 (0.318) loss_u loss_u 0.9580 (0.9526) acc_u 6.2500 (6.4583) lr 1.7604e-03 eta 0:00:31
epoch [47/200] batch [20/85] time 0.561 (0.446) data 0.430 (0.315) loss_u loss_u 0.9834 (0.9610) acc_u 0.0000 (5.3125) lr 1.7604e-03 eta 0:00:28
epoch [47/200] batch [25/85] time 0.448 (0.447) data 0.317 (0.316) loss_u loss_u 0.8906 (0.9604) acc_u 9.3750 (5.2500) lr 1.7604e-03 eta 0:00:26
epoch [47/200] batch [30/85] time 0.449 (0.445) data 0.319 (0.314) loss_u loss_u 0.9937 (0.9615) acc_u 0.0000 (5.2083) lr 1.7604e-03 eta 0:00:24
epoch [47/200] batch [35/85] time 0.442 (0.445) data 0.310 (0.314) loss_u loss_u 0.9932 (0.9630) acc_u 0.0000 (5.0893) lr 1.7604e-03 eta 0:00:22
epoch [47/200] batch [40/85] time 0.467 (0.446) data 0.335 (0.316) loss_u loss_u 0.9771 (0.9630) acc_u 3.1250 (5.0000) lr 1.7604e-03 eta 0:00:20
epoch [47/200] batch [45/85] time 0.462 (0.451) data 0.332 (0.320) loss_u loss_u 0.9849 (0.9641) acc_u 3.1250 (4.9306) lr 1.7604e-03 eta 0:00:18
epoch [47/200] batch [50/85] time 0.644 (0.457) data 0.512 (0.327) loss_u loss_u 0.9600 (0.9644) acc_u 6.2500 (4.8750) lr 1.7604e-03 eta 0:00:16
epoch [47/200] batch [55/85] time 0.394 (0.458) data 0.263 (0.327) loss_u loss_u 0.9810 (0.9655) acc_u 3.1250 (4.6023) lr 1.7604e-03 eta 0:00:13
epoch [47/200] batch [60/85] time 0.521 (0.461) data 0.390 (0.330) loss_u loss_u 0.9668 (0.9663) acc_u 6.2500 (4.5312) lr 1.7604e-03 eta 0:00:11
epoch [47/200] batch [65/85] time 0.529 (0.462) data 0.398 (0.331) loss_u loss_u 0.9624 (0.9664) acc_u 6.2500 (4.5673) lr 1.7604e-03 eta 0:00:09
epoch [47/200] batch [70/85] time 0.378 (0.461) data 0.248 (0.330) loss_u loss_u 0.9922 (0.9655) acc_u 0.0000 (4.6875) lr 1.7604e-03 eta 0:00:06
epoch [47/200] batch [75/85] time 0.378 (0.458) data 0.247 (0.327) loss_u loss_u 0.9565 (0.9653) acc_u 9.3750 (4.7083) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [80/85] time 0.463 (0.460) data 0.332 (0.329) loss_u loss_u 0.9556 (0.9652) acc_u 9.3750 (4.8047) lr 1.7604e-03 eta 0:00:02
epoch [47/200] batch [85/85] time 0.444 (0.459) data 0.313 (0.328) loss_u loss_u 0.9761 (0.9662) acc_u 0.0000 (4.6324) lr 1.7604e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1788
confident_label rate tensor(0.1342, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 421
clean true:406
clean false:15
clean_rate:0.9643705463182898
noisy true:942
noisy false:1773
after delete: len(clean_dataset) 421
after delete: len(noisy_dataset) 2715
epoch [48/200] batch [5/13] time 0.620 (0.550) data 0.489 (0.419) loss_x loss_x 1.1348 (1.3131) acc_x 71.8750 (65.6250) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [10/13] time 0.407 (0.493) data 0.277 (0.363) loss_x loss_x 1.5254 (1.3659) acc_x 56.2500 (65.0000) lr 1.7501e-03 eta 0:00:01
epoch [48/200] batch [5/84] time 0.551 (0.472) data 0.420 (0.341) loss_u loss_u 0.9873 (0.9651) acc_u 0.0000 (3.1250) lr 1.7501e-03 eta 0:00:37
epoch [48/200] batch [10/84] time 0.451 (0.470) data 0.320 (0.340) loss_u loss_u 0.9941 (0.9711) acc_u 3.1250 (2.8125) lr 1.7501e-03 eta 0:00:34
epoch [48/200] batch [15/84] time 0.346 (0.456) data 0.216 (0.325) loss_u loss_u 0.9844 (0.9720) acc_u 3.1250 (3.1250) lr 1.7501e-03 eta 0:00:31
epoch [48/200] batch [20/84] time 0.384 (0.462) data 0.252 (0.331) loss_u loss_u 0.9639 (0.9688) acc_u 3.1250 (3.9062) lr 1.7501e-03 eta 0:00:29
epoch [48/200] batch [25/84] time 0.371 (0.462) data 0.241 (0.331) loss_u loss_u 0.9565 (0.9665) acc_u 9.3750 (4.3750) lr 1.7501e-03 eta 0:00:27
epoch [48/200] batch [30/84] time 0.455 (0.458) data 0.324 (0.327) loss_u loss_u 0.9722 (0.9659) acc_u 3.1250 (4.2708) lr 1.7501e-03 eta 0:00:24
epoch [48/200] batch [35/84] time 0.423 (0.458) data 0.292 (0.327) loss_u loss_u 0.9844 (0.9670) acc_u 0.0000 (4.1071) lr 1.7501e-03 eta 0:00:22
epoch [48/200] batch [40/84] time 0.437 (0.456) data 0.307 (0.325) loss_u loss_u 0.9580 (0.9653) acc_u 3.1250 (4.3750) lr 1.7501e-03 eta 0:00:20
epoch [48/200] batch [45/84] time 0.460 (0.458) data 0.330 (0.327) loss_u loss_u 0.9585 (0.9649) acc_u 6.2500 (4.4444) lr 1.7501e-03 eta 0:00:17
epoch [48/200] batch [50/84] time 0.409 (0.459) data 0.278 (0.328) loss_u loss_u 0.9917 (0.9631) acc_u 0.0000 (4.6875) lr 1.7501e-03 eta 0:00:15
epoch [48/200] batch [55/84] time 0.467 (0.458) data 0.336 (0.327) loss_u loss_u 0.9478 (0.9637) acc_u 6.2500 (4.6023) lr 1.7501e-03 eta 0:00:13
epoch [48/200] batch [60/84] time 0.409 (0.458) data 0.278 (0.328) loss_u loss_u 0.9897 (0.9652) acc_u 0.0000 (4.3229) lr 1.7501e-03 eta 0:00:11
epoch [48/200] batch [65/84] time 0.404 (0.458) data 0.274 (0.327) loss_u loss_u 0.9614 (0.9648) acc_u 3.1250 (4.3750) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [70/84] time 0.476 (0.455) data 0.345 (0.325) loss_u loss_u 0.9922 (0.9637) acc_u 3.1250 (4.4643) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [75/84] time 0.497 (0.454) data 0.365 (0.323) loss_u loss_u 0.9688 (0.9635) acc_u 6.2500 (4.4583) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [80/84] time 0.371 (0.452) data 0.240 (0.322) loss_u loss_u 0.9897 (0.9641) acc_u 0.0000 (4.3750) lr 1.7501e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1812
confident_label rate tensor(0.1279, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 401
clean true:386
clean false:15
clean_rate:0.9625935162094763
noisy true:938
noisy false:1797
after delete: len(clean_dataset) 401
after delete: len(noisy_dataset) 2735
epoch [49/200] batch [5/12] time 0.404 (0.470) data 0.274 (0.340) loss_x loss_x 1.9375 (1.4883) acc_x 62.5000 (66.2500) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [10/12] time 0.441 (0.458) data 0.311 (0.327) loss_x loss_x 0.9629 (1.4270) acc_x 84.3750 (67.8125) lr 1.7396e-03 eta 0:00:00
epoch [49/200] batch [5/85] time 0.389 (0.454) data 0.258 (0.324) loss_u loss_u 0.9858 (0.9529) acc_u 3.1250 (7.5000) lr 1.7396e-03 eta 0:00:36
epoch [49/200] batch [10/85] time 0.415 (0.443) data 0.284 (0.312) loss_u loss_u 0.9585 (0.9560) acc_u 3.1250 (5.9375) lr 1.7396e-03 eta 0:00:33
epoch [49/200] batch [15/85] time 0.436 (0.434) data 0.305 (0.303) loss_u loss_u 0.9448 (0.9548) acc_u 9.3750 (6.2500) lr 1.7396e-03 eta 0:00:30
epoch [49/200] batch [20/85] time 0.386 (0.429) data 0.255 (0.298) loss_u loss_u 0.9746 (0.9562) acc_u 6.2500 (5.9375) lr 1.7396e-03 eta 0:00:27
epoch [49/200] batch [25/85] time 0.631 (0.433) data 0.500 (0.302) loss_u loss_u 0.9966 (0.9566) acc_u 0.0000 (6.0000) lr 1.7396e-03 eta 0:00:25
epoch [49/200] batch [30/85] time 0.334 (0.433) data 0.203 (0.302) loss_u loss_u 0.9653 (0.9552) acc_u 3.1250 (6.0417) lr 1.7396e-03 eta 0:00:23
epoch [49/200] batch [35/85] time 0.485 (0.435) data 0.354 (0.304) loss_u loss_u 0.9980 (0.9559) acc_u 0.0000 (5.7143) lr 1.7396e-03 eta 0:00:21
epoch [49/200] batch [40/85] time 0.356 (0.441) data 0.225 (0.310) loss_u loss_u 0.9849 (0.9571) acc_u 3.1250 (5.5469) lr 1.7396e-03 eta 0:00:19
epoch [49/200] batch [45/85] time 0.506 (0.441) data 0.375 (0.310) loss_u loss_u 0.9863 (0.9581) acc_u 0.0000 (5.3472) lr 1.7396e-03 eta 0:00:17
epoch [49/200] batch [50/85] time 0.580 (0.445) data 0.450 (0.314) loss_u loss_u 0.9175 (0.9584) acc_u 9.3750 (5.3750) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [55/85] time 0.429 (0.444) data 0.298 (0.313) loss_u loss_u 0.9873 (0.9600) acc_u 0.0000 (5.1705) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [60/85] time 0.425 (0.445) data 0.294 (0.314) loss_u loss_u 0.9883 (0.9612) acc_u 0.0000 (5.0000) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [65/85] time 0.487 (0.444) data 0.355 (0.313) loss_u loss_u 0.9341 (0.9602) acc_u 9.3750 (5.0962) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [70/85] time 0.393 (0.445) data 0.263 (0.314) loss_u loss_u 0.9980 (0.9608) acc_u 0.0000 (5.1786) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [75/85] time 0.536 (0.446) data 0.405 (0.315) loss_u loss_u 0.9854 (0.9614) acc_u 3.1250 (5.1250) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [80/85] time 0.463 (0.449) data 0.333 (0.318) loss_u loss_u 0.9517 (0.9615) acc_u 9.3750 (5.1172) lr 1.7396e-03 eta 0:00:02
epoch [49/200] batch [85/85] time 0.374 (0.448) data 0.243 (0.318) loss_u loss_u 0.9780 (0.9615) acc_u 3.1250 (5.1471) lr 1.7396e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1795
confident_label rate tensor(0.1276, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 400
clean true:385
clean false:15
clean_rate:0.9625
noisy true:956
noisy false:1780
after delete: len(clean_dataset) 400
after delete: len(noisy_dataset) 2736
epoch [50/200] batch [5/12] time 0.388 (0.456) data 0.258 (0.325) loss_x loss_x 1.4336 (1.3604) acc_x 65.6250 (67.5000) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [10/12] time 0.478 (0.464) data 0.347 (0.333) loss_x loss_x 1.3232 (1.3536) acc_x 56.2500 (69.3750) lr 1.7290e-03 eta 0:00:00
epoch [50/200] batch [5/85] time 0.487 (0.466) data 0.356 (0.336) loss_u loss_u 0.9512 (0.9647) acc_u 12.5000 (6.2500) lr 1.7290e-03 eta 0:00:37
epoch [50/200] batch [10/85] time 0.404 (0.458) data 0.272 (0.327) loss_u loss_u 0.9829 (0.9631) acc_u 3.1250 (5.6250) lr 1.7290e-03 eta 0:00:34
epoch [50/200] batch [15/85] time 0.553 (0.464) data 0.422 (0.333) loss_u loss_u 0.9775 (0.9611) acc_u 3.1250 (5.6250) lr 1.7290e-03 eta 0:00:32
epoch [50/200] batch [20/85] time 0.436 (0.458) data 0.306 (0.327) loss_u loss_u 0.9570 (0.9600) acc_u 6.2500 (5.7812) lr 1.7290e-03 eta 0:00:29
epoch [50/200] batch [25/85] time 0.367 (0.456) data 0.237 (0.325) loss_u loss_u 0.9521 (0.9608) acc_u 3.1250 (5.3750) lr 1.7290e-03 eta 0:00:27
epoch [50/200] batch [30/85] time 0.624 (0.461) data 0.494 (0.330) loss_u loss_u 0.9365 (0.9617) acc_u 9.3750 (5.1042) lr 1.7290e-03 eta 0:00:25
epoch [50/200] batch [35/85] time 0.347 (0.455) data 0.215 (0.325) loss_u loss_u 0.9941 (0.9634) acc_u 0.0000 (4.9107) lr 1.7290e-03 eta 0:00:22
epoch [50/200] batch [40/85] time 0.418 (0.452) data 0.288 (0.322) loss_u loss_u 0.9243 (0.9603) acc_u 9.3750 (5.3125) lr 1.7290e-03 eta 0:00:20
epoch [50/200] batch [45/85] time 0.477 (0.454) data 0.347 (0.324) loss_u loss_u 0.9814 (0.9607) acc_u 3.1250 (5.3472) lr 1.7290e-03 eta 0:00:18
epoch [50/200] batch [50/85] time 0.570 (0.458) data 0.439 (0.327) loss_u loss_u 0.9727 (0.9610) acc_u 3.1250 (5.1875) lr 1.7290e-03 eta 0:00:16
epoch [50/200] batch [55/85] time 0.442 (0.455) data 0.311 (0.324) loss_u loss_u 0.9697 (0.9619) acc_u 3.1250 (5.0568) lr 1.7290e-03 eta 0:00:13
epoch [50/200] batch [60/85] time 0.583 (0.456) data 0.453 (0.325) loss_u loss_u 0.9438 (0.9610) acc_u 6.2500 (5.1562) lr 1.7290e-03 eta 0:00:11
epoch [50/200] batch [65/85] time 0.486 (0.461) data 0.355 (0.330) loss_u loss_u 0.9546 (0.9611) acc_u 9.3750 (5.1923) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [70/85] time 0.438 (0.462) data 0.307 (0.331) loss_u loss_u 0.9912 (0.9616) acc_u 0.0000 (5.0000) lr 1.7290e-03 eta 0:00:06
epoch [50/200] batch [75/85] time 0.401 (0.461) data 0.270 (0.330) loss_u loss_u 0.9102 (0.9602) acc_u 15.6250 (5.1667) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [80/85] time 0.394 (0.458) data 0.263 (0.327) loss_u loss_u 0.9819 (0.9609) acc_u 3.1250 (5.0781) lr 1.7290e-03 eta 0:00:02
epoch [50/200] batch [85/85] time 0.407 (0.456) data 0.276 (0.325) loss_u loss_u 0.9473 (0.9604) acc_u 6.2500 (5.1471) lr 1.7290e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1766
confident_label rate tensor(0.1260, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 395
clean true:385
clean false:10
clean_rate:0.9746835443037974
noisy true:985
noisy false:1756
after delete: len(clean_dataset) 395
after delete: len(noisy_dataset) 2741
epoch [51/200] batch [5/12] time 0.684 (0.479) data 0.554 (0.349) loss_x loss_x 1.4932 (1.2443) acc_x 65.6250 (68.1250) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [10/12] time 0.410 (0.480) data 0.281 (0.350) loss_x loss_x 1.6680 (1.2896) acc_x 65.6250 (69.6875) lr 1.7181e-03 eta 0:00:00
epoch [51/200] batch [5/85] time 0.428 (0.449) data 0.298 (0.319) loss_u loss_u 0.9951 (0.9656) acc_u 0.0000 (5.6250) lr 1.7181e-03 eta 0:00:35
epoch [51/200] batch [10/85] time 0.376 (0.455) data 0.244 (0.325) loss_u loss_u 0.9702 (0.9631) acc_u 3.1250 (5.6250) lr 1.7181e-03 eta 0:00:34
epoch [51/200] batch [15/85] time 0.416 (0.444) data 0.286 (0.314) loss_u loss_u 0.9341 (0.9620) acc_u 9.3750 (5.8333) lr 1.7181e-03 eta 0:00:31
epoch [51/200] batch [20/85] time 0.560 (0.450) data 0.428 (0.320) loss_u loss_u 0.9468 (0.9639) acc_u 6.2500 (5.1562) lr 1.7181e-03 eta 0:00:29
epoch [51/200] batch [25/85] time 0.345 (0.454) data 0.214 (0.323) loss_u loss_u 0.9863 (0.9629) acc_u 0.0000 (5.1250) lr 1.7181e-03 eta 0:00:27
epoch [51/200] batch [30/85] time 0.449 (0.454) data 0.319 (0.323) loss_u loss_u 0.9629 (0.9605) acc_u 6.2500 (5.7292) lr 1.7181e-03 eta 0:00:24
epoch [51/200] batch [35/85] time 0.370 (0.447) data 0.239 (0.317) loss_u loss_u 0.9541 (0.9619) acc_u 6.2500 (5.5357) lr 1.7181e-03 eta 0:00:22
epoch [51/200] batch [40/85] time 0.483 (0.449) data 0.353 (0.318) loss_u loss_u 0.9263 (0.9607) acc_u 9.3750 (5.6250) lr 1.7181e-03 eta 0:00:20
epoch [51/200] batch [45/85] time 0.476 (0.451) data 0.345 (0.321) loss_u loss_u 0.9741 (0.9600) acc_u 0.0000 (5.6944) lr 1.7181e-03 eta 0:00:18
epoch [51/200] batch [50/85] time 0.387 (0.450) data 0.257 (0.319) loss_u loss_u 0.9429 (0.9607) acc_u 6.2500 (5.5625) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [55/85] time 0.464 (0.451) data 0.332 (0.321) loss_u loss_u 0.9497 (0.9623) acc_u 6.2500 (5.2841) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [60/85] time 0.429 (0.453) data 0.297 (0.322) loss_u loss_u 0.9917 (0.9625) acc_u 0.0000 (5.2604) lr 1.7181e-03 eta 0:00:11
epoch [51/200] batch [65/85] time 0.388 (0.451) data 0.257 (0.320) loss_u loss_u 0.9473 (0.9623) acc_u 9.3750 (5.2885) lr 1.7181e-03 eta 0:00:09
epoch [51/200] batch [70/85] time 0.501 (0.452) data 0.370 (0.321) loss_u loss_u 0.9482 (0.9622) acc_u 6.2500 (5.3125) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [75/85] time 0.474 (0.454) data 0.343 (0.323) loss_u loss_u 0.9292 (0.9601) acc_u 6.2500 (5.5417) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [80/85] time 0.426 (0.454) data 0.295 (0.323) loss_u loss_u 0.9707 (0.9590) acc_u 6.2500 (5.5859) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [85/85] time 0.412 (0.454) data 0.280 (0.324) loss_u loss_u 0.9380 (0.9593) acc_u 6.2500 (5.4779) lr 1.7181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1786
confident_label rate tensor(0.1320, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 414
clean true:406
clean false:8
clean_rate:0.9806763285024155
noisy true:944
noisy false:1778
after delete: len(clean_dataset) 414
after delete: len(noisy_dataset) 2722
epoch [52/200] batch [5/12] time 0.482 (0.478) data 0.351 (0.347) loss_x loss_x 1.1562 (1.3789) acc_x 68.7500 (65.6250) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [10/12] time 0.567 (0.469) data 0.435 (0.338) loss_x loss_x 1.1631 (1.3533) acc_x 71.8750 (68.1250) lr 1.7071e-03 eta 0:00:00
epoch [52/200] batch [5/85] time 0.457 (0.465) data 0.325 (0.335) loss_u loss_u 0.9746 (0.9707) acc_u 3.1250 (4.3750) lr 1.7071e-03 eta 0:00:37
epoch [52/200] batch [10/85] time 0.410 (0.456) data 0.279 (0.326) loss_u loss_u 0.9248 (0.9653) acc_u 12.5000 (5.0000) lr 1.7071e-03 eta 0:00:34
epoch [52/200] batch [15/85] time 0.389 (0.459) data 0.257 (0.328) loss_u loss_u 0.9976 (0.9639) acc_u 0.0000 (4.7917) lr 1.7071e-03 eta 0:00:32
epoch [52/200] batch [20/85] time 0.434 (0.451) data 0.303 (0.320) loss_u loss_u 0.9634 (0.9642) acc_u 6.2500 (4.6875) lr 1.7071e-03 eta 0:00:29
epoch [52/200] batch [25/85] time 0.637 (0.453) data 0.505 (0.322) loss_u loss_u 0.9521 (0.9614) acc_u 3.1250 (4.8750) lr 1.7071e-03 eta 0:00:27
epoch [52/200] batch [30/85] time 0.389 (0.449) data 0.258 (0.318) loss_u loss_u 0.9634 (0.9623) acc_u 6.2500 (4.6875) lr 1.7071e-03 eta 0:00:24
epoch [52/200] batch [35/85] time 0.445 (0.444) data 0.313 (0.313) loss_u loss_u 0.9673 (0.9606) acc_u 3.1250 (5.0000) lr 1.7071e-03 eta 0:00:22
epoch [52/200] batch [40/85] time 0.472 (0.449) data 0.341 (0.318) loss_u loss_u 0.9053 (0.9586) acc_u 18.7500 (5.5469) lr 1.7071e-03 eta 0:00:20
epoch [52/200] batch [45/85] time 0.423 (0.448) data 0.292 (0.317) loss_u loss_u 0.9902 (0.9604) acc_u 0.0000 (5.3472) lr 1.7071e-03 eta 0:00:17
epoch [52/200] batch [50/85] time 0.448 (0.449) data 0.316 (0.318) loss_u loss_u 0.9316 (0.9617) acc_u 6.2500 (5.0625) lr 1.7071e-03 eta 0:00:15
epoch [52/200] batch [55/85] time 0.582 (0.458) data 0.450 (0.327) loss_u loss_u 0.9692 (0.9619) acc_u 6.2500 (5.0568) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [60/85] time 0.463 (0.456) data 0.332 (0.325) loss_u loss_u 0.9741 (0.9623) acc_u 3.1250 (5.0000) lr 1.7071e-03 eta 0:00:11
epoch [52/200] batch [65/85] time 0.509 (0.454) data 0.377 (0.323) loss_u loss_u 0.8921 (0.9611) acc_u 15.6250 (5.1442) lr 1.7071e-03 eta 0:00:09
epoch [52/200] batch [70/85] time 0.498 (0.454) data 0.366 (0.322) loss_u loss_u 0.9863 (0.9619) acc_u 3.1250 (5.0893) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [75/85] time 0.403 (0.452) data 0.272 (0.321) loss_u loss_u 0.9844 (0.9618) acc_u 3.1250 (5.1250) lr 1.7071e-03 eta 0:00:04
epoch [52/200] batch [80/85] time 0.515 (0.454) data 0.385 (0.322) loss_u loss_u 0.9653 (0.9612) acc_u 6.2500 (5.1953) lr 1.7071e-03 eta 0:00:02
epoch [52/200] batch [85/85] time 0.394 (0.453) data 0.263 (0.322) loss_u loss_u 0.9624 (0.9619) acc_u 3.1250 (5.1103) lr 1.7071e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1800
confident_label rate tensor(0.1384, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 434
clean true:420
clean false:14
clean_rate:0.967741935483871
noisy true:916
noisy false:1786
after delete: len(clean_dataset) 434
after delete: len(noisy_dataset) 2702
epoch [53/200] batch [5/13] time 0.381 (0.443) data 0.252 (0.313) loss_x loss_x 1.4434 (1.2518) acc_x 62.5000 (68.7500) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [10/13] time 0.439 (0.455) data 0.308 (0.326) loss_x loss_x 0.9414 (1.2233) acc_x 81.2500 (70.3125) lr 1.6959e-03 eta 0:00:01
epoch [53/200] batch [5/84] time 0.405 (0.453) data 0.273 (0.323) loss_u loss_u 0.9751 (0.9628) acc_u 3.1250 (5.6250) lr 1.6959e-03 eta 0:00:35
epoch [53/200] batch [10/84] time 0.394 (0.471) data 0.263 (0.341) loss_u loss_u 0.9590 (0.9679) acc_u 6.2500 (4.3750) lr 1.6959e-03 eta 0:00:34
epoch [53/200] batch [15/84] time 0.368 (0.464) data 0.236 (0.334) loss_u loss_u 0.9604 (0.9661) acc_u 3.1250 (4.7917) lr 1.6959e-03 eta 0:00:32
epoch [53/200] batch [20/84] time 0.371 (0.454) data 0.240 (0.323) loss_u loss_u 0.9844 (0.9678) acc_u 0.0000 (4.2188) lr 1.6959e-03 eta 0:00:29
epoch [53/200] batch [25/84] time 0.497 (0.450) data 0.365 (0.319) loss_u loss_u 0.9854 (0.9676) acc_u 3.1250 (4.5000) lr 1.6959e-03 eta 0:00:26
epoch [53/200] batch [30/84] time 0.391 (0.446) data 0.260 (0.315) loss_u loss_u 0.9824 (0.9671) acc_u 3.1250 (4.6875) lr 1.6959e-03 eta 0:00:24
epoch [53/200] batch [35/84] time 0.504 (0.447) data 0.373 (0.316) loss_u loss_u 0.9478 (0.9679) acc_u 3.1250 (4.3750) lr 1.6959e-03 eta 0:00:21
epoch [53/200] batch [40/84] time 0.501 (0.453) data 0.369 (0.322) loss_u loss_u 0.9585 (0.9685) acc_u 6.2500 (4.2969) lr 1.6959e-03 eta 0:00:19
epoch [53/200] batch [45/84] time 0.391 (0.452) data 0.259 (0.321) loss_u loss_u 0.9829 (0.9696) acc_u 0.0000 (4.0972) lr 1.6959e-03 eta 0:00:17
epoch [53/200] batch [50/84] time 0.428 (0.450) data 0.296 (0.319) loss_u loss_u 0.9604 (0.9708) acc_u 9.3750 (3.9375) lr 1.6959e-03 eta 0:00:15
epoch [53/200] batch [55/84] time 0.384 (0.450) data 0.253 (0.319) loss_u loss_u 0.9346 (0.9705) acc_u 6.2500 (3.8068) lr 1.6959e-03 eta 0:00:13
epoch [53/200] batch [60/84] time 0.487 (0.449) data 0.356 (0.318) loss_u loss_u 0.9277 (0.9684) acc_u 15.6250 (4.2708) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [65/84] time 0.574 (0.450) data 0.442 (0.319) loss_u loss_u 0.9951 (0.9689) acc_u 0.0000 (4.0865) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [70/84] time 0.417 (0.445) data 0.285 (0.314) loss_u loss_u 0.9839 (0.9692) acc_u 3.1250 (3.9732) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [75/84] time 0.425 (0.446) data 0.293 (0.315) loss_u loss_u 0.9736 (0.9691) acc_u 3.1250 (4.0000) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [80/84] time 0.420 (0.447) data 0.289 (0.316) loss_u loss_u 0.9468 (0.9689) acc_u 6.2500 (4.0625) lr 1.6959e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1825
confident_label rate tensor(0.1266, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 397
clean true:387
clean false:10
clean_rate:0.9748110831234257
noisy true:924
noisy false:1815
after delete: len(clean_dataset) 397
after delete: len(noisy_dataset) 2739
epoch [54/200] batch [5/12] time 0.390 (0.477) data 0.260 (0.346) loss_x loss_x 1.0918 (1.4408) acc_x 75.0000 (67.5000) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [10/12] time 0.412 (0.470) data 0.281 (0.340) loss_x loss_x 1.1104 (1.3109) acc_x 71.8750 (68.1250) lr 1.6845e-03 eta 0:00:00
epoch [54/200] batch [5/85] time 0.379 (0.452) data 0.248 (0.322) loss_u loss_u 0.9409 (0.9693) acc_u 9.3750 (3.1250) lr 1.6845e-03 eta 0:00:36
epoch [54/200] batch [10/85] time 0.448 (0.443) data 0.318 (0.313) loss_u loss_u 0.9648 (0.9666) acc_u 3.1250 (4.0625) lr 1.6845e-03 eta 0:00:33
epoch [54/200] batch [15/85] time 0.478 (0.443) data 0.346 (0.312) loss_u loss_u 0.9424 (0.9646) acc_u 9.3750 (4.5833) lr 1.6845e-03 eta 0:00:30
epoch [54/200] batch [20/85] time 0.354 (0.447) data 0.223 (0.317) loss_u loss_u 0.9453 (0.9640) acc_u 6.2500 (5.0000) lr 1.6845e-03 eta 0:00:29
epoch [54/200] batch [25/85] time 0.412 (0.440) data 0.280 (0.309) loss_u loss_u 0.9858 (0.9660) acc_u 0.0000 (4.6250) lr 1.6845e-03 eta 0:00:26
epoch [54/200] batch [30/85] time 0.486 (0.439) data 0.355 (0.308) loss_u loss_u 0.9526 (0.9623) acc_u 6.2500 (5.1042) lr 1.6845e-03 eta 0:00:24
epoch [54/200] batch [35/85] time 0.397 (0.437) data 0.266 (0.306) loss_u loss_u 0.9268 (0.9638) acc_u 9.3750 (4.8214) lr 1.6845e-03 eta 0:00:21
epoch [54/200] batch [40/85] time 0.363 (0.434) data 0.232 (0.303) loss_u loss_u 0.9600 (0.9642) acc_u 6.2500 (4.8438) lr 1.6845e-03 eta 0:00:19
epoch [54/200] batch [45/85] time 0.417 (0.431) data 0.285 (0.300) loss_u loss_u 0.9795 (0.9641) acc_u 6.2500 (4.8611) lr 1.6845e-03 eta 0:00:17
epoch [54/200] batch [50/85] time 0.586 (0.433) data 0.455 (0.302) loss_u loss_u 0.9722 (0.9647) acc_u 3.1250 (4.8125) lr 1.6845e-03 eta 0:00:15
epoch [54/200] batch [55/85] time 0.406 (0.437) data 0.274 (0.306) loss_u loss_u 0.9629 (0.9645) acc_u 6.2500 (4.7159) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [60/85] time 0.432 (0.440) data 0.301 (0.308) loss_u loss_u 0.9751 (0.9646) acc_u 3.1250 (4.6875) lr 1.6845e-03 eta 0:00:10
epoch [54/200] batch [65/85] time 0.522 (0.442) data 0.391 (0.311) loss_u loss_u 0.9741 (0.9654) acc_u 6.2500 (4.5673) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [70/85] time 0.398 (0.446) data 0.267 (0.315) loss_u loss_u 0.9785 (0.9648) acc_u 3.1250 (4.6429) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [75/85] time 0.568 (0.445) data 0.439 (0.314) loss_u loss_u 0.9512 (0.9641) acc_u 12.5000 (4.9167) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [80/85] time 0.441 (0.448) data 0.309 (0.316) loss_u loss_u 0.9951 (0.9650) acc_u 0.0000 (4.8438) lr 1.6845e-03 eta 0:00:02
epoch [54/200] batch [85/85] time 0.427 (0.449) data 0.295 (0.318) loss_u loss_u 0.9644 (0.9652) acc_u 3.1250 (4.8529) lr 1.6845e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1801
confident_label rate tensor(0.1330, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 417
clean true:403
clean false:14
clean_rate:0.9664268585131894
noisy true:932
noisy false:1787
after delete: len(clean_dataset) 417
after delete: len(noisy_dataset) 2719
epoch [55/200] batch [5/13] time 0.421 (0.448) data 0.290 (0.318) loss_x loss_x 1.0303 (1.1521) acc_x 68.7500 (71.8750) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [10/13] time 0.397 (0.435) data 0.267 (0.304) loss_x loss_x 1.2832 (1.1590) acc_x 62.5000 (70.0000) lr 1.6730e-03 eta 0:00:01
epoch [55/200] batch [5/84] time 0.626 (0.458) data 0.495 (0.327) loss_u loss_u 0.9922 (0.9676) acc_u 0.0000 (4.3750) lr 1.6730e-03 eta 0:00:36
epoch [55/200] batch [10/84] time 0.313 (0.438) data 0.181 (0.307) loss_u loss_u 0.9292 (0.9606) acc_u 9.3750 (5.0000) lr 1.6730e-03 eta 0:00:32
epoch [55/200] batch [15/84] time 0.386 (0.435) data 0.254 (0.304) loss_u loss_u 0.9390 (0.9639) acc_u 9.3750 (4.7917) lr 1.6730e-03 eta 0:00:30
epoch [55/200] batch [20/84] time 0.474 (0.443) data 0.343 (0.312) loss_u loss_u 0.9551 (0.9643) acc_u 3.1250 (4.5312) lr 1.6730e-03 eta 0:00:28
epoch [55/200] batch [25/84] time 0.358 (0.437) data 0.227 (0.306) loss_u loss_u 0.9658 (0.9643) acc_u 6.2500 (4.6250) lr 1.6730e-03 eta 0:00:25
epoch [55/200] batch [30/84] time 0.374 (0.432) data 0.242 (0.301) loss_u loss_u 0.9746 (0.9617) acc_u 3.1250 (4.8958) lr 1.6730e-03 eta 0:00:23
epoch [55/200] batch [35/84] time 0.451 (0.432) data 0.319 (0.301) loss_u loss_u 0.9678 (0.9619) acc_u 6.2500 (5.0000) lr 1.6730e-03 eta 0:00:21
epoch [55/200] batch [40/84] time 0.511 (0.432) data 0.380 (0.301) loss_u loss_u 0.9883 (0.9616) acc_u 3.1250 (5.3125) lr 1.6730e-03 eta 0:00:19
epoch [55/200] batch [45/84] time 0.474 (0.437) data 0.342 (0.306) loss_u loss_u 0.9932 (0.9644) acc_u 0.0000 (4.9306) lr 1.6730e-03 eta 0:00:17
epoch [55/200] batch [50/84] time 0.591 (0.439) data 0.459 (0.308) loss_u loss_u 0.9741 (0.9642) acc_u 6.2500 (4.9375) lr 1.6730e-03 eta 0:00:14
epoch [55/200] batch [55/84] time 0.355 (0.439) data 0.224 (0.308) loss_u loss_u 0.9194 (0.9636) acc_u 21.8750 (5.2273) lr 1.6730e-03 eta 0:00:12
epoch [55/200] batch [60/84] time 0.341 (0.440) data 0.210 (0.309) loss_u loss_u 0.9766 (0.9649) acc_u 3.1250 (5.0000) lr 1.6730e-03 eta 0:00:10
epoch [55/200] batch [65/84] time 0.568 (0.440) data 0.436 (0.309) loss_u loss_u 0.9253 (0.9649) acc_u 12.5000 (5.0481) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [70/84] time 0.415 (0.443) data 0.284 (0.311) loss_u loss_u 0.9995 (0.9642) acc_u 0.0000 (5.0893) lr 1.6730e-03 eta 0:00:06
epoch [55/200] batch [75/84] time 0.381 (0.442) data 0.250 (0.311) loss_u loss_u 0.9863 (0.9647) acc_u 3.1250 (5.0000) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [80/84] time 0.520 (0.445) data 0.389 (0.314) loss_u loss_u 0.9663 (0.9645) acc_u 6.2500 (5.0000) lr 1.6730e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1797
confident_label rate tensor(0.1387, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 435
clean true:419
clean false:16
clean_rate:0.9632183908045977
noisy true:920
noisy false:1781
after delete: len(clean_dataset) 435
after delete: len(noisy_dataset) 2701
epoch [56/200] batch [5/13] time 0.770 (0.528) data 0.640 (0.398) loss_x loss_x 2.3027 (1.6596) acc_x 46.8750 (63.1250) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [10/13] time 0.454 (0.489) data 0.323 (0.358) loss_x loss_x 1.5293 (1.5843) acc_x 65.6250 (62.8125) lr 1.6613e-03 eta 0:00:01
epoch [56/200] batch [5/84] time 0.497 (0.449) data 0.366 (0.318) loss_u loss_u 0.9307 (0.9659) acc_u 9.3750 (3.7500) lr 1.6613e-03 eta 0:00:35
epoch [56/200] batch [10/84] time 0.410 (0.443) data 0.279 (0.312) loss_u loss_u 0.9077 (0.9575) acc_u 15.6250 (5.6250) lr 1.6613e-03 eta 0:00:32
epoch [56/200] batch [15/84] time 0.429 (0.449) data 0.298 (0.318) loss_u loss_u 0.9531 (0.9588) acc_u 6.2500 (5.8333) lr 1.6613e-03 eta 0:00:30
epoch [56/200] batch [20/84] time 0.435 (0.449) data 0.304 (0.317) loss_u loss_u 0.9707 (0.9597) acc_u 6.2500 (5.6250) lr 1.6613e-03 eta 0:00:28
epoch [56/200] batch [25/84] time 0.477 (0.446) data 0.346 (0.315) loss_u loss_u 0.9541 (0.9594) acc_u 6.2500 (5.7500) lr 1.6613e-03 eta 0:00:26
epoch [56/200] batch [30/84] time 0.436 (0.440) data 0.304 (0.309) loss_u loss_u 0.8906 (0.9599) acc_u 21.8750 (5.9375) lr 1.6613e-03 eta 0:00:23
epoch [56/200] batch [35/84] time 0.601 (0.442) data 0.469 (0.311) loss_u loss_u 0.9604 (0.9601) acc_u 6.2500 (5.9821) lr 1.6613e-03 eta 0:00:21
epoch [56/200] batch [40/84] time 0.359 (0.440) data 0.227 (0.309) loss_u loss_u 0.9429 (0.9579) acc_u 6.2500 (6.0156) lr 1.6613e-03 eta 0:00:19
epoch [56/200] batch [45/84] time 0.493 (0.439) data 0.361 (0.308) loss_u loss_u 0.9858 (0.9574) acc_u 0.0000 (5.9722) lr 1.6613e-03 eta 0:00:17
epoch [56/200] batch [50/84] time 0.408 (0.439) data 0.277 (0.308) loss_u loss_u 0.9902 (0.9604) acc_u 0.0000 (5.5625) lr 1.6613e-03 eta 0:00:14
epoch [56/200] batch [55/84] time 0.470 (0.439) data 0.338 (0.308) loss_u loss_u 0.9580 (0.9611) acc_u 6.2500 (5.3977) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [60/84] time 0.507 (0.442) data 0.376 (0.310) loss_u loss_u 0.9648 (0.9594) acc_u 3.1250 (5.5729) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [65/84] time 0.475 (0.443) data 0.344 (0.312) loss_u loss_u 0.9448 (0.9603) acc_u 6.2500 (5.3846) lr 1.6613e-03 eta 0:00:08
epoch [56/200] batch [70/84] time 0.720 (0.446) data 0.588 (0.314) loss_u loss_u 0.9912 (0.9614) acc_u 0.0000 (5.2232) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [75/84] time 0.360 (0.447) data 0.228 (0.316) loss_u loss_u 0.9707 (0.9619) acc_u 3.1250 (5.1667) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [80/84] time 0.387 (0.449) data 0.257 (0.317) loss_u loss_u 0.9604 (0.9627) acc_u 6.2500 (5.0781) lr 1.6613e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1825
confident_label rate tensor(0.1231, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 386
clean true:375
clean false:11
clean_rate:0.9715025906735751
noisy true:936
noisy false:1814
after delete: len(clean_dataset) 386
after delete: len(noisy_dataset) 2750
epoch [57/200] batch [5/12] time 0.519 (0.468) data 0.390 (0.337) loss_x loss_x 0.9688 (1.1972) acc_x 75.0000 (67.5000) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [10/12] time 0.454 (0.451) data 0.323 (0.321) loss_x loss_x 1.1250 (1.2499) acc_x 68.7500 (69.0625) lr 1.6494e-03 eta 0:00:00
epoch [57/200] batch [5/85] time 0.394 (0.446) data 0.263 (0.315) loss_u loss_u 0.9917 (0.9611) acc_u 0.0000 (5.0000) lr 1.6494e-03 eta 0:00:35
epoch [57/200] batch [10/85] time 0.358 (0.444) data 0.227 (0.313) loss_u loss_u 0.9966 (0.9572) acc_u 0.0000 (5.3125) lr 1.6494e-03 eta 0:00:33
epoch [57/200] batch [15/85] time 0.450 (0.443) data 0.319 (0.312) loss_u loss_u 0.9473 (0.9610) acc_u 9.3750 (5.2083) lr 1.6494e-03 eta 0:00:30
epoch [57/200] batch [20/85] time 0.517 (0.446) data 0.386 (0.315) loss_u loss_u 0.9409 (0.9604) acc_u 15.6250 (5.7812) lr 1.6494e-03 eta 0:00:28
epoch [57/200] batch [25/85] time 0.402 (0.443) data 0.270 (0.312) loss_u loss_u 0.9741 (0.9634) acc_u 3.1250 (5.2500) lr 1.6494e-03 eta 0:00:26
epoch [57/200] batch [30/85] time 0.376 (0.446) data 0.244 (0.315) loss_u loss_u 0.9443 (0.9610) acc_u 9.3750 (5.8333) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [35/85] time 0.496 (0.445) data 0.365 (0.313) loss_u loss_u 0.9956 (0.9636) acc_u 0.0000 (5.6250) lr 1.6494e-03 eta 0:00:22
epoch [57/200] batch [40/85] time 0.327 (0.441) data 0.196 (0.310) loss_u loss_u 0.9561 (0.9631) acc_u 6.2500 (5.7031) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [45/85] time 0.375 (0.446) data 0.244 (0.315) loss_u loss_u 0.9521 (0.9630) acc_u 9.3750 (5.4861) lr 1.6494e-03 eta 0:00:17
epoch [57/200] batch [50/85] time 0.430 (0.446) data 0.300 (0.315) loss_u loss_u 0.9727 (0.9600) acc_u 6.2500 (6.0000) lr 1.6494e-03 eta 0:00:15
epoch [57/200] batch [55/85] time 0.412 (0.443) data 0.281 (0.312) loss_u loss_u 0.9868 (0.9585) acc_u 3.1250 (6.1364) lr 1.6494e-03 eta 0:00:13
epoch [57/200] batch [60/85] time 0.498 (0.445) data 0.367 (0.314) loss_u loss_u 0.9824 (0.9574) acc_u 3.1250 (6.4062) lr 1.6494e-03 eta 0:00:11
epoch [57/200] batch [65/85] time 0.445 (0.444) data 0.315 (0.313) loss_u loss_u 0.9580 (0.9586) acc_u 6.2500 (6.1538) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [70/85] time 0.563 (0.446) data 0.432 (0.315) loss_u loss_u 0.9951 (0.9587) acc_u 0.0000 (6.0268) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [75/85] time 0.423 (0.447) data 0.292 (0.316) loss_u loss_u 0.9419 (0.9588) acc_u 6.2500 (5.9583) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [80/85] time 0.419 (0.446) data 0.288 (0.315) loss_u loss_u 0.9272 (0.9595) acc_u 9.3750 (5.8203) lr 1.6494e-03 eta 0:00:02
epoch [57/200] batch [85/85] time 0.591 (0.449) data 0.460 (0.318) loss_u loss_u 0.9819 (0.9598) acc_u 0.0000 (5.6985) lr 1.6494e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1829
confident_label rate tensor(0.1317, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 413
clean true:399
clean false:14
clean_rate:0.9661016949152542
noisy true:908
noisy false:1815
after delete: len(clean_dataset) 413
after delete: len(noisy_dataset) 2723
epoch [58/200] batch [5/12] time 0.428 (0.467) data 0.298 (0.337) loss_x loss_x 1.6152 (1.3412) acc_x 56.2500 (67.5000) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [10/12] time 0.522 (0.480) data 0.392 (0.350) loss_x loss_x 0.6626 (1.2666) acc_x 81.2500 (69.6875) lr 1.6374e-03 eta 0:00:00
epoch [58/200] batch [5/85] time 0.482 (0.480) data 0.351 (0.350) loss_u loss_u 0.9775 (0.9734) acc_u 3.1250 (5.0000) lr 1.6374e-03 eta 0:00:38
epoch [58/200] batch [10/85] time 0.436 (0.473) data 0.305 (0.342) loss_u loss_u 0.9795 (0.9726) acc_u 3.1250 (4.3750) lr 1.6374e-03 eta 0:00:35
epoch [58/200] batch [15/85] time 0.422 (0.468) data 0.291 (0.337) loss_u loss_u 0.9473 (0.9680) acc_u 9.3750 (4.7917) lr 1.6374e-03 eta 0:00:32
epoch [58/200] batch [20/85] time 0.355 (0.457) data 0.224 (0.327) loss_u loss_u 0.9868 (0.9646) acc_u 3.1250 (5.1562) lr 1.6374e-03 eta 0:00:29
epoch [58/200] batch [25/85] time 0.496 (0.461) data 0.366 (0.331) loss_u loss_u 0.9126 (0.9601) acc_u 12.5000 (5.6250) lr 1.6374e-03 eta 0:00:27
epoch [58/200] batch [30/85] time 0.433 (0.459) data 0.302 (0.328) loss_u loss_u 0.9712 (0.9617) acc_u 6.2500 (5.6250) lr 1.6374e-03 eta 0:00:25
epoch [58/200] batch [35/85] time 0.628 (0.461) data 0.498 (0.330) loss_u loss_u 0.9600 (0.9602) acc_u 6.2500 (5.8929) lr 1.6374e-03 eta 0:00:23
epoch [58/200] batch [40/85] time 0.536 (0.463) data 0.404 (0.332) loss_u loss_u 0.9805 (0.9639) acc_u 3.1250 (5.3125) lr 1.6374e-03 eta 0:00:20
epoch [58/200] batch [45/85] time 0.368 (0.462) data 0.237 (0.331) loss_u loss_u 0.9951 (0.9631) acc_u 3.1250 (5.3472) lr 1.6374e-03 eta 0:00:18
epoch [58/200] batch [50/85] time 0.422 (0.461) data 0.291 (0.330) loss_u loss_u 0.9648 (0.9637) acc_u 3.1250 (5.2500) lr 1.6374e-03 eta 0:00:16
epoch [58/200] batch [55/85] time 0.417 (0.458) data 0.285 (0.327) loss_u loss_u 0.9219 (0.9625) acc_u 9.3750 (5.3977) lr 1.6374e-03 eta 0:00:13
epoch [58/200] batch [60/85] time 0.420 (0.456) data 0.288 (0.325) loss_u loss_u 0.9678 (0.9627) acc_u 3.1250 (5.2604) lr 1.6374e-03 eta 0:00:11
epoch [58/200] batch [65/85] time 0.470 (0.455) data 0.339 (0.324) loss_u loss_u 0.9854 (0.9635) acc_u 3.1250 (5.2404) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [70/85] time 0.351 (0.453) data 0.220 (0.322) loss_u loss_u 0.9688 (0.9631) acc_u 3.1250 (5.3125) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [75/85] time 0.353 (0.449) data 0.223 (0.318) loss_u loss_u 0.9678 (0.9633) acc_u 0.0000 (5.2917) lr 1.6374e-03 eta 0:00:04
epoch [58/200] batch [80/85] time 0.458 (0.453) data 0.327 (0.322) loss_u loss_u 0.9541 (0.9626) acc_u 3.1250 (5.3516) lr 1.6374e-03 eta 0:00:02
epoch [58/200] batch [85/85] time 0.451 (0.450) data 0.320 (0.319) loss_u loss_u 0.9316 (0.9626) acc_u 9.3750 (5.3309) lr 1.6374e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1782
confident_label rate tensor(0.1384, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 434
clean true:416
clean false:18
clean_rate:0.9585253456221198
noisy true:938
noisy false:1764
after delete: len(clean_dataset) 434
after delete: len(noisy_dataset) 2702
epoch [59/200] batch [5/13] time 0.409 (0.466) data 0.279 (0.336) loss_x loss_x 1.8701 (1.4398) acc_x 62.5000 (73.1250) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [10/13] time 0.480 (0.470) data 0.349 (0.340) loss_x loss_x 0.9604 (1.3665) acc_x 71.8750 (70.6250) lr 1.6252e-03 eta 0:00:01
epoch [59/200] batch [5/84] time 0.486 (0.471) data 0.354 (0.340) loss_u loss_u 0.9727 (0.9577) acc_u 0.0000 (5.0000) lr 1.6252e-03 eta 0:00:37
epoch [59/200] batch [10/84] time 0.394 (0.451) data 0.262 (0.320) loss_u loss_u 0.9326 (0.9619) acc_u 12.5000 (5.6250) lr 1.6252e-03 eta 0:00:33
epoch [59/200] batch [15/84] time 0.399 (0.449) data 0.267 (0.318) loss_u loss_u 0.9707 (0.9643) acc_u 3.1250 (5.2083) lr 1.6252e-03 eta 0:00:31
epoch [59/200] batch [20/84] time 0.490 (0.456) data 0.359 (0.325) loss_u loss_u 0.9878 (0.9653) acc_u 0.0000 (4.6875) lr 1.6252e-03 eta 0:00:29
epoch [59/200] batch [25/84] time 0.458 (0.455) data 0.327 (0.324) loss_u loss_u 0.9419 (0.9651) acc_u 9.3750 (4.6250) lr 1.6252e-03 eta 0:00:26
epoch [59/200] batch [30/84] time 0.388 (0.448) data 0.257 (0.317) loss_u loss_u 0.9429 (0.9662) acc_u 6.2500 (4.3750) lr 1.6252e-03 eta 0:00:24
epoch [59/200] batch [35/84] time 0.415 (0.447) data 0.283 (0.316) loss_u loss_u 0.9082 (0.9648) acc_u 9.3750 (4.4643) lr 1.6252e-03 eta 0:00:21
epoch [59/200] batch [40/84] time 0.345 (0.447) data 0.213 (0.315) loss_u loss_u 0.9316 (0.9648) acc_u 9.3750 (4.4531) lr 1.6252e-03 eta 0:00:19
epoch [59/200] batch [45/84] time 0.412 (0.450) data 0.280 (0.319) loss_u loss_u 0.9575 (0.9658) acc_u 6.2500 (4.3056) lr 1.6252e-03 eta 0:00:17
epoch [59/200] batch [50/84] time 0.442 (0.451) data 0.310 (0.320) loss_u loss_u 0.9980 (0.9663) acc_u 0.0000 (4.1875) lr 1.6252e-03 eta 0:00:15
epoch [59/200] batch [55/84] time 0.518 (0.453) data 0.386 (0.322) loss_u loss_u 0.9512 (0.9670) acc_u 9.3750 (4.0909) lr 1.6252e-03 eta 0:00:13
epoch [59/200] batch [60/84] time 0.491 (0.451) data 0.359 (0.320) loss_u loss_u 0.9688 (0.9676) acc_u 3.1250 (4.0104) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [65/84] time 0.371 (0.449) data 0.240 (0.318) loss_u loss_u 0.9692 (0.9658) acc_u 3.1250 (4.2308) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [70/84] time 0.575 (0.450) data 0.443 (0.318) loss_u loss_u 0.9419 (0.9659) acc_u 9.3750 (4.2411) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [75/84] time 0.449 (0.447) data 0.318 (0.316) loss_u loss_u 0.9268 (0.9655) acc_u 6.2500 (4.3333) lr 1.6252e-03 eta 0:00:04
epoch [59/200] batch [80/84] time 0.401 (0.449) data 0.270 (0.318) loss_u loss_u 0.9565 (0.9657) acc_u 3.1250 (4.2969) lr 1.6252e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1782
confident_label rate tensor(0.1349, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 423
clean true:412
clean false:11
clean_rate:0.9739952718676123
noisy true:942
noisy false:1771
after delete: len(clean_dataset) 423
after delete: len(noisy_dataset) 2713
epoch [60/200] batch [5/13] time 0.398 (0.416) data 0.268 (0.285) loss_x loss_x 1.0508 (1.3898) acc_x 71.8750 (67.5000) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [10/13] time 0.416 (0.424) data 0.286 (0.293) loss_x loss_x 0.8599 (1.2603) acc_x 78.1250 (69.3750) lr 1.6129e-03 eta 0:00:01
epoch [60/200] batch [5/84] time 0.474 (0.443) data 0.343 (0.313) loss_u loss_u 0.9062 (0.9373) acc_u 9.3750 (7.5000) lr 1.6129e-03 eta 0:00:35
epoch [60/200] batch [10/84] time 0.498 (0.451) data 0.367 (0.321) loss_u loss_u 0.9619 (0.9545) acc_u 3.1250 (5.3125) lr 1.6129e-03 eta 0:00:33
epoch [60/200] batch [15/84] time 0.390 (0.441) data 0.258 (0.310) loss_u loss_u 0.9209 (0.9544) acc_u 12.5000 (5.6250) lr 1.6129e-03 eta 0:00:30
epoch [60/200] batch [20/84] time 0.451 (0.442) data 0.320 (0.312) loss_u loss_u 0.9561 (0.9596) acc_u 6.2500 (5.1562) lr 1.6129e-03 eta 0:00:28
epoch [60/200] batch [25/84] time 0.472 (0.448) data 0.340 (0.317) loss_u loss_u 0.9844 (0.9603) acc_u 0.0000 (4.8750) lr 1.6129e-03 eta 0:00:26
epoch [60/200] batch [30/84] time 0.507 (0.457) data 0.376 (0.326) loss_u loss_u 0.9775 (0.9608) acc_u 3.1250 (4.8958) lr 1.6129e-03 eta 0:00:24
epoch [60/200] batch [35/84] time 0.481 (0.454) data 0.350 (0.323) loss_u loss_u 0.9409 (0.9622) acc_u 6.2500 (4.5536) lr 1.6129e-03 eta 0:00:22
epoch [60/200] batch [40/84] time 0.482 (0.455) data 0.351 (0.324) loss_u loss_u 0.9595 (0.9619) acc_u 3.1250 (4.6094) lr 1.6129e-03 eta 0:00:20
epoch [60/200] batch [45/84] time 0.646 (0.455) data 0.514 (0.324) loss_u loss_u 0.9795 (0.9633) acc_u 3.1250 (4.4444) lr 1.6129e-03 eta 0:00:17
epoch [60/200] batch [50/84] time 0.494 (0.455) data 0.362 (0.324) loss_u loss_u 0.9663 (0.9616) acc_u 3.1250 (4.6250) lr 1.6129e-03 eta 0:00:15
epoch [60/200] batch [55/84] time 0.446 (0.453) data 0.315 (0.321) loss_u loss_u 0.9365 (0.9628) acc_u 9.3750 (4.4886) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [60/84] time 0.365 (0.450) data 0.234 (0.319) loss_u loss_u 0.9800 (0.9633) acc_u 0.0000 (4.5312) lr 1.6129e-03 eta 0:00:10
epoch [60/200] batch [65/84] time 0.357 (0.447) data 0.225 (0.316) loss_u loss_u 0.9722 (0.9626) acc_u 3.1250 (4.6154) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [70/84] time 0.470 (0.446) data 0.339 (0.315) loss_u loss_u 0.9985 (0.9641) acc_u 0.0000 (4.3750) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [75/84] time 0.374 (0.445) data 0.244 (0.314) loss_u loss_u 0.9536 (0.9637) acc_u 6.2500 (4.4583) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [80/84] time 0.399 (0.447) data 0.267 (0.316) loss_u loss_u 0.9634 (0.9628) acc_u 3.1250 (4.6484) lr 1.6129e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1792
confident_label rate tensor(0.1346, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 422
clean true:407
clean false:15
clean_rate:0.9644549763033176
noisy true:937
noisy false:1777
after delete: len(clean_dataset) 422
after delete: len(noisy_dataset) 2714
epoch [61/200] batch [5/13] time 0.562 (0.450) data 0.432 (0.319) loss_x loss_x 1.5234 (1.4826) acc_x 65.6250 (66.8750) lr 1.6004e-03 eta 0:00:03
epoch [61/200] batch [10/13] time 0.417 (0.444) data 0.286 (0.314) loss_x loss_x 1.3428 (1.4921) acc_x 71.8750 (66.2500) lr 1.6004e-03 eta 0:00:01
epoch [61/200] batch [5/84] time 0.380 (0.453) data 0.249 (0.322) loss_u loss_u 0.9395 (0.9569) acc_u 9.3750 (5.6250) lr 1.6004e-03 eta 0:00:35
epoch [61/200] batch [10/84] time 0.442 (0.442) data 0.311 (0.312) loss_u loss_u 0.9531 (0.9622) acc_u 6.2500 (5.0000) lr 1.6004e-03 eta 0:00:32
epoch [61/200] batch [15/84] time 0.491 (0.447) data 0.360 (0.316) loss_u loss_u 0.9775 (0.9639) acc_u 3.1250 (4.7917) lr 1.6004e-03 eta 0:00:30
epoch [61/200] batch [20/84] time 0.434 (0.444) data 0.303 (0.313) loss_u loss_u 0.9434 (0.9669) acc_u 6.2500 (4.2188) lr 1.6004e-03 eta 0:00:28
epoch [61/200] batch [25/84] time 0.385 (0.439) data 0.254 (0.308) loss_u loss_u 0.9697 (0.9667) acc_u 3.1250 (4.1250) lr 1.6004e-03 eta 0:00:25
epoch [61/200] batch [30/84] time 0.374 (0.446) data 0.243 (0.315) loss_u loss_u 0.9336 (0.9621) acc_u 9.3750 (4.7917) lr 1.6004e-03 eta 0:00:24
epoch [61/200] batch [35/84] time 0.524 (0.455) data 0.393 (0.324) loss_u loss_u 0.9858 (0.9609) acc_u 0.0000 (5.0893) lr 1.6004e-03 eta 0:00:22
epoch [61/200] batch [40/84] time 0.440 (0.454) data 0.309 (0.323) loss_u loss_u 0.9951 (0.9615) acc_u 0.0000 (5.0000) lr 1.6004e-03 eta 0:00:19
epoch [61/200] batch [45/84] time 0.487 (0.453) data 0.356 (0.322) loss_u loss_u 0.9521 (0.9599) acc_u 6.2500 (5.2778) lr 1.6004e-03 eta 0:00:17
epoch [61/200] batch [50/84] time 0.346 (0.451) data 0.214 (0.320) loss_u loss_u 0.9258 (0.9585) acc_u 9.3750 (5.4375) lr 1.6004e-03 eta 0:00:15
epoch [61/200] batch [55/84] time 0.398 (0.449) data 0.267 (0.317) loss_u loss_u 0.9263 (0.9598) acc_u 6.2500 (5.1136) lr 1.6004e-03 eta 0:00:13
epoch [61/200] batch [60/84] time 0.524 (0.447) data 0.393 (0.315) loss_u loss_u 0.9131 (0.9593) acc_u 9.3750 (5.0000) lr 1.6004e-03 eta 0:00:10
epoch [61/200] batch [65/84] time 0.419 (0.446) data 0.288 (0.315) loss_u loss_u 0.9795 (0.9610) acc_u 3.1250 (4.8077) lr 1.6004e-03 eta 0:00:08
epoch [61/200] batch [70/84] time 0.673 (0.448) data 0.541 (0.317) loss_u loss_u 0.9761 (0.9594) acc_u 6.2500 (5.0446) lr 1.6004e-03 eta 0:00:06
epoch [61/200] batch [75/84] time 0.413 (0.450) data 0.282 (0.318) loss_u loss_u 0.9536 (0.9603) acc_u 6.2500 (4.9583) lr 1.6004e-03 eta 0:00:04
epoch [61/200] batch [80/84] time 0.572 (0.449) data 0.441 (0.318) loss_u loss_u 0.9468 (0.9603) acc_u 6.2500 (4.9609) lr 1.6004e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1770
confident_label rate tensor(0.1413, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 443
clean true:429
clean false:14
clean_rate:0.9683972911963883
noisy true:937
noisy false:1756
after delete: len(clean_dataset) 443
after delete: len(noisy_dataset) 2693
epoch [62/200] batch [5/13] time 0.441 (0.441) data 0.311 (0.311) loss_x loss_x 1.5566 (1.4242) acc_x 62.5000 (61.2500) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [10/13] time 0.553 (0.449) data 0.423 (0.319) loss_x loss_x 1.0283 (1.3130) acc_x 75.0000 (64.6875) lr 1.5878e-03 eta 0:00:01
epoch [62/200] batch [5/84] time 0.476 (0.463) data 0.345 (0.332) loss_u loss_u 0.9663 (0.9574) acc_u 3.1250 (5.0000) lr 1.5878e-03 eta 0:00:36
epoch [62/200] batch [10/84] time 0.482 (0.458) data 0.351 (0.328) loss_u loss_u 0.9692 (0.9634) acc_u 3.1250 (5.0000) lr 1.5878e-03 eta 0:00:33
epoch [62/200] batch [15/84] time 0.437 (0.457) data 0.305 (0.326) loss_u loss_u 0.9824 (0.9624) acc_u 3.1250 (5.2083) lr 1.5878e-03 eta 0:00:31
epoch [62/200] batch [20/84] time 0.465 (0.459) data 0.335 (0.329) loss_u loss_u 0.9414 (0.9637) acc_u 9.3750 (5.1562) lr 1.5878e-03 eta 0:00:29
epoch [62/200] batch [25/84] time 0.392 (0.458) data 0.262 (0.327) loss_u loss_u 0.9570 (0.9671) acc_u 6.2500 (4.5000) lr 1.5878e-03 eta 0:00:27
epoch [62/200] batch [30/84] time 0.441 (0.451) data 0.311 (0.321) loss_u loss_u 0.9648 (0.9667) acc_u 6.2500 (4.4792) lr 1.5878e-03 eta 0:00:24
epoch [62/200] batch [35/84] time 0.422 (0.455) data 0.291 (0.325) loss_u loss_u 0.9580 (0.9655) acc_u 6.2500 (4.8214) lr 1.5878e-03 eta 0:00:22
epoch [62/200] batch [40/84] time 0.386 (0.452) data 0.254 (0.321) loss_u loss_u 0.9717 (0.9649) acc_u 3.1250 (4.7656) lr 1.5878e-03 eta 0:00:19
epoch [62/200] batch [45/84] time 0.520 (0.450) data 0.390 (0.320) loss_u loss_u 0.9863 (0.9653) acc_u 3.1250 (4.7222) lr 1.5878e-03 eta 0:00:17
epoch [62/200] batch [50/84] time 0.485 (0.450) data 0.353 (0.319) loss_u loss_u 0.9673 (0.9652) acc_u 6.2500 (4.6875) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [55/84] time 0.502 (0.452) data 0.370 (0.321) loss_u loss_u 0.9512 (0.9654) acc_u 6.2500 (4.5455) lr 1.5878e-03 eta 0:00:13
epoch [62/200] batch [60/84] time 0.361 (0.449) data 0.231 (0.319) loss_u loss_u 0.9917 (0.9647) acc_u 0.0000 (4.6354) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [65/84] time 0.426 (0.450) data 0.294 (0.319) loss_u loss_u 0.9648 (0.9651) acc_u 3.1250 (4.6154) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [70/84] time 0.409 (0.449) data 0.277 (0.318) loss_u loss_u 0.9727 (0.9661) acc_u 3.1250 (4.4196) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [75/84] time 0.460 (0.451) data 0.329 (0.320) loss_u loss_u 0.9805 (0.9664) acc_u 0.0000 (4.3333) lr 1.5878e-03 eta 0:00:04
epoch [62/200] batch [80/84] time 0.505 (0.450) data 0.374 (0.319) loss_u loss_u 0.9922 (0.9665) acc_u 0.0000 (4.2188) lr 1.5878e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1821
confident_label rate tensor(0.1323, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 415
clean true:397
clean false:18
clean_rate:0.9566265060240964
noisy true:918
noisy false:1803
after delete: len(clean_dataset) 415
after delete: len(noisy_dataset) 2721
epoch [63/200] batch [5/12] time 0.523 (0.509) data 0.393 (0.379) loss_x loss_x 1.4648 (1.3951) acc_x 62.5000 (68.7500) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [10/12] time 0.504 (0.456) data 0.373 (0.326) loss_x loss_x 1.3320 (1.3986) acc_x 78.1250 (67.1875) lr 1.5750e-03 eta 0:00:00
epoch [63/200] batch [5/85] time 0.346 (0.449) data 0.215 (0.318) loss_u loss_u 0.9849 (0.9633) acc_u 3.1250 (4.3750) lr 1.5750e-03 eta 0:00:35
epoch [63/200] batch [10/85] time 0.522 (0.447) data 0.391 (0.316) loss_u loss_u 0.9980 (0.9651) acc_u 0.0000 (4.3750) lr 1.5750e-03 eta 0:00:33
epoch [63/200] batch [15/85] time 0.395 (0.461) data 0.265 (0.330) loss_u loss_u 0.9780 (0.9672) acc_u 3.1250 (4.1667) lr 1.5750e-03 eta 0:00:32
epoch [63/200] batch [20/85] time 0.404 (0.469) data 0.273 (0.338) loss_u loss_u 0.9819 (0.9666) acc_u 3.1250 (4.6875) lr 1.5750e-03 eta 0:00:30
epoch [63/200] batch [25/85] time 0.469 (0.466) data 0.339 (0.335) loss_u loss_u 0.9873 (0.9677) acc_u 0.0000 (4.3750) lr 1.5750e-03 eta 0:00:27
epoch [63/200] batch [30/85] time 0.411 (0.461) data 0.281 (0.330) loss_u loss_u 0.9927 (0.9677) acc_u 0.0000 (4.1667) lr 1.5750e-03 eta 0:00:25
epoch [63/200] batch [35/85] time 0.428 (0.461) data 0.296 (0.330) loss_u loss_u 0.9814 (0.9681) acc_u 3.1250 (4.1964) lr 1.5750e-03 eta 0:00:23
epoch [63/200] batch [40/85] time 0.666 (0.458) data 0.536 (0.327) loss_u loss_u 0.9824 (0.9682) acc_u 3.1250 (4.3750) lr 1.5750e-03 eta 0:00:20
epoch [63/200] batch [45/85] time 0.390 (0.455) data 0.260 (0.324) loss_u loss_u 0.9717 (0.9688) acc_u 3.1250 (4.3056) lr 1.5750e-03 eta 0:00:18
epoch [63/200] batch [50/85] time 0.681 (0.459) data 0.549 (0.328) loss_u loss_u 0.9507 (0.9673) acc_u 6.2500 (4.3750) lr 1.5750e-03 eta 0:00:16
epoch [63/200] batch [55/85] time 0.355 (0.456) data 0.224 (0.325) loss_u loss_u 0.9258 (0.9660) acc_u 9.3750 (4.3750) lr 1.5750e-03 eta 0:00:13
epoch [63/200] batch [60/85] time 0.538 (0.456) data 0.407 (0.325) loss_u loss_u 0.9819 (0.9655) acc_u 3.1250 (4.4792) lr 1.5750e-03 eta 0:00:11
epoch [63/200] batch [65/85] time 0.340 (0.450) data 0.209 (0.320) loss_u loss_u 0.9580 (0.9650) acc_u 3.1250 (4.5192) lr 1.5750e-03 eta 0:00:09
epoch [63/200] batch [70/85] time 0.407 (0.449) data 0.276 (0.318) loss_u loss_u 0.9814 (0.9648) acc_u 0.0000 (4.5089) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [75/85] time 0.361 (0.446) data 0.230 (0.315) loss_u loss_u 0.9380 (0.9643) acc_u 9.3750 (4.5417) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [80/85] time 0.559 (0.449) data 0.428 (0.318) loss_u loss_u 0.9937 (0.9644) acc_u 3.1250 (4.5703) lr 1.5750e-03 eta 0:00:02
epoch [63/200] batch [85/85] time 0.419 (0.448) data 0.288 (0.317) loss_u loss_u 0.9863 (0.9647) acc_u 3.1250 (4.5588) lr 1.5750e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1769
confident_label rate tensor(0.1301, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 408
clean true:394
clean false:14
clean_rate:0.9656862745098039
noisy true:973
noisy false:1755
after delete: len(clean_dataset) 408
after delete: len(noisy_dataset) 2728
epoch [64/200] batch [5/12] time 0.552 (0.506) data 0.422 (0.376) loss_x loss_x 1.4678 (1.2779) acc_x 62.5000 (66.2500) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [10/12] time 0.408 (0.472) data 0.277 (0.341) loss_x loss_x 1.6182 (1.3162) acc_x 65.6250 (67.1875) lr 1.5621e-03 eta 0:00:00
epoch [64/200] batch [5/85] time 0.495 (0.464) data 0.364 (0.333) loss_u loss_u 0.9619 (0.9786) acc_u 3.1250 (2.5000) lr 1.5621e-03 eta 0:00:37
epoch [64/200] batch [10/85] time 0.409 (0.460) data 0.278 (0.329) loss_u loss_u 0.9429 (0.9597) acc_u 9.3750 (6.2500) lr 1.5621e-03 eta 0:00:34
epoch [64/200] batch [15/85] time 0.418 (0.451) data 0.287 (0.320) loss_u loss_u 0.9751 (0.9605) acc_u 3.1250 (5.8333) lr 1.5621e-03 eta 0:00:31
epoch [64/200] batch [20/85] time 0.361 (0.450) data 0.230 (0.320) loss_u loss_u 0.9648 (0.9642) acc_u 3.1250 (5.3125) lr 1.5621e-03 eta 0:00:29
epoch [64/200] batch [25/85] time 0.415 (0.447) data 0.285 (0.316) loss_u loss_u 0.9663 (0.9595) acc_u 3.1250 (5.7500) lr 1.5621e-03 eta 0:00:26
epoch [64/200] batch [30/85] time 0.600 (0.451) data 0.468 (0.320) loss_u loss_u 0.9365 (0.9604) acc_u 9.3750 (5.6250) lr 1.5621e-03 eta 0:00:24
epoch [64/200] batch [35/85] time 0.431 (0.456) data 0.299 (0.325) loss_u loss_u 0.9556 (0.9601) acc_u 6.2500 (5.8036) lr 1.5621e-03 eta 0:00:22
epoch [64/200] batch [40/85] time 0.391 (0.460) data 0.260 (0.329) loss_u loss_u 0.9824 (0.9609) acc_u 3.1250 (5.6250) lr 1.5621e-03 eta 0:00:20
epoch [64/200] batch [45/85] time 0.453 (0.459) data 0.322 (0.329) loss_u loss_u 0.9634 (0.9618) acc_u 6.2500 (5.4861) lr 1.5621e-03 eta 0:00:18
epoch [64/200] batch [50/85] time 0.377 (0.456) data 0.245 (0.325) loss_u loss_u 0.9023 (0.9607) acc_u 12.5000 (5.6250) lr 1.5621e-03 eta 0:00:15
epoch [64/200] batch [55/85] time 0.444 (0.451) data 0.313 (0.320) loss_u loss_u 0.9702 (0.9618) acc_u 3.1250 (5.4545) lr 1.5621e-03 eta 0:00:13
epoch [64/200] batch [60/85] time 0.454 (0.452) data 0.323 (0.322) loss_u loss_u 0.9790 (0.9609) acc_u 0.0000 (5.5729) lr 1.5621e-03 eta 0:00:11
epoch [64/200] batch [65/85] time 0.449 (0.453) data 0.317 (0.322) loss_u loss_u 0.9829 (0.9600) acc_u 6.2500 (5.7692) lr 1.5621e-03 eta 0:00:09
epoch [64/200] batch [70/85] time 0.411 (0.452) data 0.280 (0.321) loss_u loss_u 0.9526 (0.9603) acc_u 6.2500 (5.7589) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [75/85] time 0.352 (0.453) data 0.221 (0.322) loss_u loss_u 0.9771 (0.9617) acc_u 3.1250 (5.5000) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [80/85] time 0.516 (0.451) data 0.384 (0.320) loss_u loss_u 0.9380 (0.9609) acc_u 9.3750 (5.5469) lr 1.5621e-03 eta 0:00:02
epoch [64/200] batch [85/85] time 0.441 (0.450) data 0.309 (0.319) loss_u loss_u 0.9922 (0.9604) acc_u 3.1250 (5.5882) lr 1.5621e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1832
confident_label rate tensor(0.1378, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 432
clean true:416
clean false:16
clean_rate:0.9629629629629629
noisy true:888
noisy false:1816
after delete: len(clean_dataset) 432
after delete: len(noisy_dataset) 2704
epoch [65/200] batch [5/13] time 0.464 (0.486) data 0.333 (0.356) loss_x loss_x 1.4043 (1.5912) acc_x 65.6250 (60.0000) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [10/13] time 0.416 (0.475) data 0.285 (0.344) loss_x loss_x 1.4805 (1.4689) acc_x 53.1250 (60.6250) lr 1.5490e-03 eta 0:00:01
epoch [65/200] batch [5/84] time 0.510 (0.470) data 0.379 (0.339) loss_u loss_u 0.9814 (0.9616) acc_u 3.1250 (5.0000) lr 1.5490e-03 eta 0:00:37
epoch [65/200] batch [10/84] time 0.407 (0.462) data 0.275 (0.331) loss_u loss_u 0.9795 (0.9593) acc_u 3.1250 (5.6250) lr 1.5490e-03 eta 0:00:34
epoch [65/200] batch [15/84] time 0.482 (0.462) data 0.351 (0.330) loss_u loss_u 0.9404 (0.9595) acc_u 9.3750 (5.8333) lr 1.5490e-03 eta 0:00:31
epoch [65/200] batch [20/84] time 0.442 (0.457) data 0.311 (0.326) loss_u loss_u 0.9502 (0.9632) acc_u 3.1250 (5.0000) lr 1.5490e-03 eta 0:00:29
epoch [65/200] batch [25/84] time 0.441 (0.458) data 0.310 (0.327) loss_u loss_u 0.9590 (0.9654) acc_u 3.1250 (4.3750) lr 1.5490e-03 eta 0:00:27
epoch [65/200] batch [30/84] time 0.483 (0.458) data 0.352 (0.327) loss_u loss_u 0.9927 (0.9656) acc_u 0.0000 (4.5833) lr 1.5490e-03 eta 0:00:24
epoch [65/200] batch [35/84] time 0.615 (0.461) data 0.484 (0.330) loss_u loss_u 0.9956 (0.9672) acc_u 0.0000 (4.2857) lr 1.5490e-03 eta 0:00:22
epoch [65/200] batch [40/84] time 0.420 (0.462) data 0.290 (0.331) loss_u loss_u 0.9688 (0.9665) acc_u 6.2500 (4.4531) lr 1.5490e-03 eta 0:00:20
epoch [65/200] batch [45/84] time 0.394 (0.458) data 0.262 (0.326) loss_u loss_u 0.9805 (0.9657) acc_u 3.1250 (4.7222) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [50/84] time 0.564 (0.457) data 0.434 (0.326) loss_u loss_u 0.9434 (0.9652) acc_u 6.2500 (4.8750) lr 1.5490e-03 eta 0:00:15
epoch [65/200] batch [55/84] time 0.381 (0.453) data 0.249 (0.322) loss_u loss_u 0.9775 (0.9637) acc_u 3.1250 (5.0000) lr 1.5490e-03 eta 0:00:13
epoch [65/200] batch [60/84] time 0.406 (0.450) data 0.274 (0.319) loss_u loss_u 0.9795 (0.9650) acc_u 3.1250 (4.6875) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [65/84] time 0.611 (0.452) data 0.480 (0.321) loss_u loss_u 0.9702 (0.9653) acc_u 6.2500 (4.5673) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [70/84] time 0.526 (0.455) data 0.395 (0.324) loss_u loss_u 0.9644 (0.9649) acc_u 3.1250 (4.5982) lr 1.5490e-03 eta 0:00:06
epoch [65/200] batch [75/84] time 0.485 (0.454) data 0.354 (0.323) loss_u loss_u 0.9165 (0.9626) acc_u 12.5000 (4.9583) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [80/84] time 0.336 (0.452) data 0.205 (0.321) loss_u loss_u 0.9746 (0.9637) acc_u 3.1250 (4.8047) lr 1.5490e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1795
confident_label rate tensor(0.1301, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 408
clean true:394
clean false:14
clean_rate:0.9656862745098039
noisy true:947
noisy false:1781
after delete: len(clean_dataset) 408
after delete: len(noisy_dataset) 2728
epoch [66/200] batch [5/12] time 0.584 (0.446) data 0.454 (0.316) loss_x loss_x 1.4707 (1.3607) acc_x 65.6250 (63.1250) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [10/12] time 0.405 (0.438) data 0.275 (0.307) loss_x loss_x 1.2734 (1.3098) acc_x 65.6250 (68.1250) lr 1.5358e-03 eta 0:00:00
epoch [66/200] batch [5/85] time 0.386 (0.429) data 0.255 (0.299) loss_u loss_u 0.9951 (0.9678) acc_u 0.0000 (3.7500) lr 1.5358e-03 eta 0:00:34
epoch [66/200] batch [10/85] time 0.392 (0.436) data 0.259 (0.305) loss_u loss_u 0.9546 (0.9570) acc_u 6.2500 (5.3125) lr 1.5358e-03 eta 0:00:32
epoch [66/200] batch [15/85] time 0.364 (0.438) data 0.233 (0.308) loss_u loss_u 0.9814 (0.9629) acc_u 0.0000 (4.3750) lr 1.5358e-03 eta 0:00:30
epoch [66/200] batch [20/85] time 0.364 (0.434) data 0.233 (0.303) loss_u loss_u 0.9824 (0.9625) acc_u 0.0000 (4.3750) lr 1.5358e-03 eta 0:00:28
epoch [66/200] batch [25/85] time 0.667 (0.444) data 0.537 (0.313) loss_u loss_u 0.9712 (0.9616) acc_u 3.1250 (4.6250) lr 1.5358e-03 eta 0:00:26
epoch [66/200] batch [30/85] time 0.354 (0.448) data 0.224 (0.318) loss_u loss_u 0.9834 (0.9632) acc_u 0.0000 (4.3750) lr 1.5358e-03 eta 0:00:24
epoch [66/200] batch [35/85] time 0.525 (0.451) data 0.394 (0.320) loss_u loss_u 0.9917 (0.9646) acc_u 0.0000 (4.1071) lr 1.5358e-03 eta 0:00:22
epoch [66/200] batch [40/85] time 0.372 (0.449) data 0.241 (0.318) loss_u loss_u 0.9399 (0.9616) acc_u 9.3750 (4.5312) lr 1.5358e-03 eta 0:00:20
epoch [66/200] batch [45/85] time 0.449 (0.451) data 0.318 (0.320) loss_u loss_u 0.9614 (0.9611) acc_u 9.3750 (4.7917) lr 1.5358e-03 eta 0:00:18
epoch [66/200] batch [50/85] time 0.448 (0.454) data 0.316 (0.323) loss_u loss_u 0.9546 (0.9612) acc_u 6.2500 (4.8125) lr 1.5358e-03 eta 0:00:15
epoch [66/200] batch [55/85] time 0.382 (0.451) data 0.250 (0.320) loss_u loss_u 0.9692 (0.9603) acc_u 3.1250 (4.8864) lr 1.5358e-03 eta 0:00:13
epoch [66/200] batch [60/85] time 0.446 (0.449) data 0.314 (0.319) loss_u loss_u 0.9497 (0.9574) acc_u 6.2500 (5.3125) lr 1.5358e-03 eta 0:00:11
epoch [66/200] batch [65/85] time 0.551 (0.450) data 0.420 (0.319) loss_u loss_u 0.9468 (0.9578) acc_u 12.5000 (5.3846) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [70/85] time 0.403 (0.450) data 0.271 (0.319) loss_u loss_u 0.9629 (0.9581) acc_u 6.2500 (5.3125) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [75/85] time 0.448 (0.447) data 0.316 (0.316) loss_u loss_u 0.9722 (0.9583) acc_u 9.3750 (5.4167) lr 1.5358e-03 eta 0:00:04
epoch [66/200] batch [80/85] time 0.371 (0.445) data 0.240 (0.314) loss_u loss_u 0.9600 (0.9589) acc_u 6.2500 (5.3125) lr 1.5358e-03 eta 0:00:02
epoch [66/200] batch [85/85] time 0.598 (0.444) data 0.468 (0.313) loss_u loss_u 0.9248 (0.9584) acc_u 9.3750 (5.3676) lr 1.5358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1795
confident_label rate tensor(0.1323, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 415
clean true:401
clean false:14
clean_rate:0.9662650602409638
noisy true:940
noisy false:1781
after delete: len(clean_dataset) 415
after delete: len(noisy_dataset) 2721
epoch [67/200] batch [5/12] time 0.604 (0.468) data 0.474 (0.337) loss_x loss_x 0.9966 (1.2899) acc_x 71.8750 (69.3750) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [10/12] time 0.570 (0.449) data 0.440 (0.319) loss_x loss_x 1.4570 (1.2974) acc_x 71.8750 (69.6875) lr 1.5225e-03 eta 0:00:00
epoch [67/200] batch [5/85] time 0.377 (0.443) data 0.246 (0.313) loss_u loss_u 0.9873 (0.9531) acc_u 0.0000 (5.0000) lr 1.5225e-03 eta 0:00:35
epoch [67/200] batch [10/85] time 0.426 (0.440) data 0.295 (0.309) loss_u loss_u 0.9819 (0.9564) acc_u 6.2500 (5.6250) lr 1.5225e-03 eta 0:00:32
epoch [67/200] batch [15/85] time 0.506 (0.445) data 0.376 (0.314) loss_u loss_u 0.8706 (0.9588) acc_u 21.8750 (5.6250) lr 1.5225e-03 eta 0:00:31
epoch [67/200] batch [20/85] time 0.566 (0.458) data 0.435 (0.327) loss_u loss_u 0.9590 (0.9588) acc_u 6.2500 (5.6250) lr 1.5225e-03 eta 0:00:29
epoch [67/200] batch [25/85] time 0.558 (0.466) data 0.427 (0.335) loss_u loss_u 0.9741 (0.9607) acc_u 3.1250 (5.3750) lr 1.5225e-03 eta 0:00:27
epoch [67/200] batch [30/85] time 0.360 (0.461) data 0.229 (0.331) loss_u loss_u 0.9604 (0.9602) acc_u 6.2500 (5.3125) lr 1.5225e-03 eta 0:00:25
epoch [67/200] batch [35/85] time 0.366 (0.458) data 0.234 (0.327) loss_u loss_u 0.9487 (0.9619) acc_u 9.3750 (5.0000) lr 1.5225e-03 eta 0:00:22
epoch [67/200] batch [40/85] time 0.426 (0.456) data 0.295 (0.325) loss_u loss_u 0.9795 (0.9586) acc_u 3.1250 (5.3906) lr 1.5225e-03 eta 0:00:20
epoch [67/200] batch [45/85] time 0.422 (0.454) data 0.291 (0.323) loss_u loss_u 0.9307 (0.9564) acc_u 6.2500 (5.5556) lr 1.5225e-03 eta 0:00:18
epoch [67/200] batch [50/85] time 0.766 (0.461) data 0.633 (0.330) loss_u loss_u 0.9814 (0.9571) acc_u 3.1250 (5.5625) lr 1.5225e-03 eta 0:00:16
epoch [67/200] batch [55/85] time 0.519 (0.464) data 0.388 (0.333) loss_u loss_u 0.9395 (0.9583) acc_u 9.3750 (5.4545) lr 1.5225e-03 eta 0:00:13
epoch [67/200] batch [60/85] time 0.372 (0.459) data 0.242 (0.328) loss_u loss_u 0.9727 (0.9592) acc_u 3.1250 (5.3646) lr 1.5225e-03 eta 0:00:11
epoch [67/200] batch [65/85] time 0.410 (0.458) data 0.279 (0.327) loss_u loss_u 0.9507 (0.9592) acc_u 6.2500 (5.4808) lr 1.5225e-03 eta 0:00:09
epoch [67/200] batch [70/85] time 0.482 (0.459) data 0.352 (0.328) loss_u loss_u 0.9297 (0.9594) acc_u 9.3750 (5.4911) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [75/85] time 0.451 (0.456) data 0.320 (0.325) loss_u loss_u 0.9375 (0.9595) acc_u 9.3750 (5.5000) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [80/85] time 0.608 (0.459) data 0.477 (0.328) loss_u loss_u 0.9453 (0.9596) acc_u 9.3750 (5.4688) lr 1.5225e-03 eta 0:00:02
epoch [67/200] batch [85/85] time 0.368 (0.457) data 0.237 (0.326) loss_u loss_u 0.9146 (0.9588) acc_u 15.6250 (5.6618) lr 1.5225e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1802
confident_label rate tensor(0.1342, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 421
clean true:407
clean false:14
clean_rate:0.9667458432304038
noisy true:927
noisy false:1788
after delete: len(clean_dataset) 421
after delete: len(noisy_dataset) 2715
epoch [68/200] batch [5/13] time 0.503 (0.454) data 0.372 (0.324) loss_x loss_x 1.2305 (1.3373) acc_x 59.3750 (63.7500) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [10/13] time 0.383 (0.448) data 0.253 (0.318) loss_x loss_x 1.5762 (1.3741) acc_x 65.6250 (63.1250) lr 1.5090e-03 eta 0:00:01
epoch [68/200] batch [5/84] time 0.423 (0.463) data 0.293 (0.333) loss_u loss_u 0.9697 (0.9561) acc_u 3.1250 (5.0000) lr 1.5090e-03 eta 0:00:36
epoch [68/200] batch [10/84] time 0.409 (0.454) data 0.279 (0.323) loss_u loss_u 0.9824 (0.9499) acc_u 0.0000 (5.3125) lr 1.5090e-03 eta 0:00:33
epoch [68/200] batch [15/84] time 0.521 (0.459) data 0.390 (0.329) loss_u loss_u 0.9585 (0.9554) acc_u 6.2500 (5.2083) lr 1.5090e-03 eta 0:00:31
epoch [68/200] batch [20/84] time 0.563 (0.460) data 0.432 (0.329) loss_u loss_u 0.9751 (0.9593) acc_u 6.2500 (4.8438) lr 1.5090e-03 eta 0:00:29
epoch [68/200] batch [25/84] time 0.448 (0.462) data 0.318 (0.332) loss_u loss_u 0.9800 (0.9613) acc_u 3.1250 (5.0000) lr 1.5090e-03 eta 0:00:27
epoch [68/200] batch [30/84] time 0.461 (0.454) data 0.330 (0.324) loss_u loss_u 0.9482 (0.9632) acc_u 9.3750 (4.7917) lr 1.5090e-03 eta 0:00:24
epoch [68/200] batch [35/84] time 0.398 (0.457) data 0.267 (0.326) loss_u loss_u 0.9585 (0.9642) acc_u 3.1250 (4.6429) lr 1.5090e-03 eta 0:00:22
epoch [68/200] batch [40/84] time 0.426 (0.457) data 0.296 (0.326) loss_u loss_u 0.9595 (0.9635) acc_u 6.2500 (4.7656) lr 1.5090e-03 eta 0:00:20
epoch [68/200] batch [45/84] time 0.478 (0.455) data 0.346 (0.324) loss_u loss_u 0.9888 (0.9651) acc_u 3.1250 (4.5139) lr 1.5090e-03 eta 0:00:17
epoch [68/200] batch [50/84] time 0.416 (0.455) data 0.286 (0.325) loss_u loss_u 0.9492 (0.9653) acc_u 9.3750 (4.5000) lr 1.5090e-03 eta 0:00:15
epoch [68/200] batch [55/84] time 0.356 (0.458) data 0.224 (0.327) loss_u loss_u 0.9136 (0.9623) acc_u 12.5000 (4.9432) lr 1.5090e-03 eta 0:00:13
epoch [68/200] batch [60/84] time 0.380 (0.456) data 0.249 (0.325) loss_u loss_u 0.9941 (0.9636) acc_u 0.0000 (4.7396) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [65/84] time 0.518 (0.454) data 0.387 (0.323) loss_u loss_u 0.9790 (0.9643) acc_u 0.0000 (4.7115) lr 1.5090e-03 eta 0:00:08
epoch [68/200] batch [70/84] time 0.351 (0.453) data 0.219 (0.322) loss_u loss_u 0.9834 (0.9635) acc_u 0.0000 (4.8214) lr 1.5090e-03 eta 0:00:06
epoch [68/200] batch [75/84] time 0.413 (0.452) data 0.281 (0.321) loss_u loss_u 0.9839 (0.9635) acc_u 3.1250 (4.8333) lr 1.5090e-03 eta 0:00:04
epoch [68/200] batch [80/84] time 0.452 (0.451) data 0.321 (0.320) loss_u loss_u 0.9131 (0.9626) acc_u 12.5000 (4.9219) lr 1.5090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1772
confident_label rate tensor(0.1384, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 434
clean true:424
clean false:10
clean_rate:0.9769585253456221
noisy true:940
noisy false:1762
after delete: len(clean_dataset) 434
after delete: len(noisy_dataset) 2702
epoch [69/200] batch [5/13] time 0.352 (0.503) data 0.222 (0.373) loss_x loss_x 1.3799 (1.4344) acc_x 68.7500 (65.0000) lr 1.4955e-03 eta 0:00:04
epoch [69/200] batch [10/13] time 0.440 (0.479) data 0.311 (0.348) loss_x loss_x 1.4893 (1.4899) acc_x 59.3750 (64.0625) lr 1.4955e-03 eta 0:00:01
epoch [69/200] batch [5/84] time 0.484 (0.460) data 0.353 (0.330) loss_u loss_u 0.9746 (0.9758) acc_u 3.1250 (2.5000) lr 1.4955e-03 eta 0:00:36
epoch [69/200] batch [10/84] time 0.401 (0.460) data 0.270 (0.329) loss_u loss_u 0.9492 (0.9680) acc_u 6.2500 (3.7500) lr 1.4955e-03 eta 0:00:34
epoch [69/200] batch [15/84] time 0.483 (0.459) data 0.352 (0.328) loss_u loss_u 0.9917 (0.9687) acc_u 0.0000 (3.5417) lr 1.4955e-03 eta 0:00:31
epoch [69/200] batch [20/84] time 0.383 (0.451) data 0.251 (0.320) loss_u loss_u 0.9604 (0.9678) acc_u 9.3750 (4.2188) lr 1.4955e-03 eta 0:00:28
epoch [69/200] batch [25/84] time 0.446 (0.454) data 0.315 (0.323) loss_u loss_u 0.9160 (0.9660) acc_u 12.5000 (4.6250) lr 1.4955e-03 eta 0:00:26
epoch [69/200] batch [30/84] time 0.551 (0.458) data 0.420 (0.328) loss_u loss_u 0.9644 (0.9669) acc_u 3.1250 (4.6875) lr 1.4955e-03 eta 0:00:24
epoch [69/200] batch [35/84] time 0.384 (0.460) data 0.253 (0.330) loss_u loss_u 0.9946 (0.9644) acc_u 0.0000 (5.0000) lr 1.4955e-03 eta 0:00:22
epoch [69/200] batch [40/84] time 0.363 (0.461) data 0.232 (0.330) loss_u loss_u 0.9668 (0.9645) acc_u 6.2500 (4.8438) lr 1.4955e-03 eta 0:00:20
epoch [69/200] batch [45/84] time 0.381 (0.456) data 0.250 (0.325) loss_u loss_u 0.9292 (0.9632) acc_u 6.2500 (4.8611) lr 1.4955e-03 eta 0:00:17
epoch [69/200] batch [50/84] time 0.394 (0.454) data 0.264 (0.324) loss_u loss_u 0.9917 (0.9642) acc_u 0.0000 (4.8125) lr 1.4955e-03 eta 0:00:15
epoch [69/200] batch [55/84] time 0.460 (0.452) data 0.330 (0.321) loss_u loss_u 0.9805 (0.9659) acc_u 0.0000 (4.4886) lr 1.4955e-03 eta 0:00:13
epoch [69/200] batch [60/84] time 0.448 (0.451) data 0.318 (0.321) loss_u loss_u 0.9551 (0.9659) acc_u 6.2500 (4.4792) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [65/84] time 0.485 (0.451) data 0.353 (0.320) loss_u loss_u 0.9648 (0.9663) acc_u 3.1250 (4.4231) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [70/84] time 0.420 (0.448) data 0.288 (0.317) loss_u loss_u 0.9888 (0.9654) acc_u 0.0000 (4.5089) lr 1.4955e-03 eta 0:00:06
epoch [69/200] batch [75/84] time 0.461 (0.451) data 0.330 (0.320) loss_u loss_u 0.9600 (0.9649) acc_u 3.1250 (4.5417) lr 1.4955e-03 eta 0:00:04
epoch [69/200] batch [80/84] time 0.551 (0.450) data 0.419 (0.319) loss_u loss_u 0.9551 (0.9633) acc_u 3.1250 (4.7266) lr 1.4955e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1773
confident_label rate tensor(0.1441, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 452
clean true:440
clean false:12
clean_rate:0.9734513274336283
noisy true:923
noisy false:1761
after delete: len(clean_dataset) 452
after delete: len(noisy_dataset) 2684
epoch [70/200] batch [5/14] time 0.576 (0.519) data 0.445 (0.389) loss_x loss_x 1.6963 (1.4199) acc_x 56.2500 (64.3750) lr 1.4818e-03 eta 0:00:04
epoch [70/200] batch [10/14] time 0.371 (0.457) data 0.240 (0.326) loss_x loss_x 1.1318 (1.4826) acc_x 65.6250 (61.5625) lr 1.4818e-03 eta 0:00:01
epoch [70/200] batch [5/83] time 0.342 (0.448) data 0.212 (0.318) loss_u loss_u 0.8843 (0.9408) acc_u 15.6250 (8.7500) lr 1.4818e-03 eta 0:00:34
epoch [70/200] batch [10/83] time 0.561 (0.453) data 0.431 (0.322) loss_u loss_u 0.9902 (0.9514) acc_u 0.0000 (6.2500) lr 1.4818e-03 eta 0:00:33
epoch [70/200] batch [15/83] time 0.427 (0.449) data 0.296 (0.318) loss_u loss_u 0.9678 (0.9568) acc_u 3.1250 (5.8333) lr 1.4818e-03 eta 0:00:30
epoch [70/200] batch [20/83] time 0.369 (0.453) data 0.239 (0.323) loss_u loss_u 0.9443 (0.9555) acc_u 9.3750 (6.2500) lr 1.4818e-03 eta 0:00:28
epoch [70/200] batch [25/83] time 0.616 (0.461) data 0.484 (0.330) loss_u loss_u 0.9922 (0.9599) acc_u 0.0000 (5.3750) lr 1.4818e-03 eta 0:00:26
epoch [70/200] batch [30/83] time 0.575 (0.461) data 0.444 (0.331) loss_u loss_u 0.9761 (0.9625) acc_u 0.0000 (4.8958) lr 1.4818e-03 eta 0:00:24
epoch [70/200] batch [35/83] time 0.580 (0.463) data 0.449 (0.333) loss_u loss_u 0.9248 (0.9598) acc_u 12.5000 (5.4464) lr 1.4818e-03 eta 0:00:22
epoch [70/200] batch [40/83] time 0.391 (0.457) data 0.261 (0.326) loss_u loss_u 0.9824 (0.9611) acc_u 3.1250 (5.3906) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [45/83] time 0.384 (0.454) data 0.254 (0.324) loss_u loss_u 0.9658 (0.9609) acc_u 6.2500 (5.3472) lr 1.4818e-03 eta 0:00:17
epoch [70/200] batch [50/83] time 0.656 (0.460) data 0.525 (0.329) loss_u loss_u 0.9951 (0.9627) acc_u 3.1250 (5.1250) lr 1.4818e-03 eta 0:00:15
epoch [70/200] batch [55/83] time 0.365 (0.458) data 0.234 (0.328) loss_u loss_u 0.9551 (0.9624) acc_u 6.2500 (5.1705) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [60/83] time 0.389 (0.458) data 0.258 (0.327) loss_u loss_u 0.9224 (0.9625) acc_u 12.5000 (5.2083) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [65/83] time 0.391 (0.455) data 0.261 (0.325) loss_u loss_u 0.9229 (0.9631) acc_u 12.5000 (5.1923) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [70/83] time 0.357 (0.453) data 0.225 (0.322) loss_u loss_u 0.9824 (0.9642) acc_u 3.1250 (4.9554) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [75/83] time 0.429 (0.450) data 0.299 (0.320) loss_u loss_u 0.9561 (0.9642) acc_u 6.2500 (4.9167) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [80/83] time 0.387 (0.451) data 0.256 (0.321) loss_u loss_u 0.9985 (0.9648) acc_u 0.0000 (4.8438) lr 1.4818e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1830
confident_label rate tensor(0.1346, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 422
clean true:403
clean false:19
clean_rate:0.9549763033175356
noisy true:903
noisy false:1811
after delete: len(clean_dataset) 422
after delete: len(noisy_dataset) 2714
epoch [71/200] batch [5/13] time 0.680 (0.494) data 0.549 (0.364) loss_x loss_x 0.9541 (1.6594) acc_x 81.2500 (66.8750) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [10/13] time 0.518 (0.496) data 0.387 (0.365) loss_x loss_x 1.6934 (1.5546) acc_x 68.7500 (65.0000) lr 1.4679e-03 eta 0:00:01
epoch [71/200] batch [5/84] time 0.563 (0.487) data 0.432 (0.356) loss_u loss_u 0.9497 (0.9529) acc_u 3.1250 (5.0000) lr 1.4679e-03 eta 0:00:38
epoch [71/200] batch [10/84] time 0.460 (0.474) data 0.329 (0.344) loss_u loss_u 0.9873 (0.9635) acc_u 0.0000 (3.7500) lr 1.4679e-03 eta 0:00:35
epoch [71/200] batch [15/84] time 0.610 (0.474) data 0.478 (0.343) loss_u loss_u 0.9004 (0.9605) acc_u 18.7500 (5.2083) lr 1.4679e-03 eta 0:00:32
epoch [71/200] batch [20/84] time 0.447 (0.469) data 0.317 (0.338) loss_u loss_u 0.9941 (0.9639) acc_u 3.1250 (5.0000) lr 1.4679e-03 eta 0:00:30
epoch [71/200] batch [25/84] time 0.489 (0.468) data 0.357 (0.338) loss_u loss_u 0.9883 (0.9645) acc_u 0.0000 (4.7500) lr 1.4679e-03 eta 0:00:27
epoch [71/200] batch [30/84] time 0.427 (0.472) data 0.296 (0.341) loss_u loss_u 0.8916 (0.9606) acc_u 12.5000 (4.8958) lr 1.4679e-03 eta 0:00:25
epoch [71/200] batch [35/84] time 0.612 (0.473) data 0.482 (0.342) loss_u loss_u 0.9307 (0.9622) acc_u 9.3750 (4.7321) lr 1.4679e-03 eta 0:00:23
epoch [71/200] batch [40/84] time 0.373 (0.471) data 0.242 (0.340) loss_u loss_u 0.9712 (0.9618) acc_u 6.2500 (4.8438) lr 1.4679e-03 eta 0:00:20
epoch [71/200] batch [45/84] time 0.416 (0.470) data 0.286 (0.339) loss_u loss_u 0.9609 (0.9619) acc_u 6.2500 (4.8611) lr 1.4679e-03 eta 0:00:18
epoch [71/200] batch [50/84] time 0.426 (0.466) data 0.296 (0.335) loss_u loss_u 0.9380 (0.9611) acc_u 9.3750 (5.0000) lr 1.4679e-03 eta 0:00:15
epoch [71/200] batch [55/84] time 0.453 (0.468) data 0.322 (0.337) loss_u loss_u 0.9302 (0.9604) acc_u 9.3750 (5.0568) lr 1.4679e-03 eta 0:00:13
epoch [71/200] batch [60/84] time 0.413 (0.464) data 0.282 (0.333) loss_u loss_u 0.9814 (0.9596) acc_u 3.1250 (5.1042) lr 1.4679e-03 eta 0:00:11
epoch [71/200] batch [65/84] time 0.335 (0.461) data 0.203 (0.330) loss_u loss_u 0.9497 (0.9588) acc_u 6.2500 (5.2404) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [70/84] time 0.380 (0.460) data 0.249 (0.329) loss_u loss_u 0.9619 (0.9578) acc_u 3.1250 (5.3571) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [75/84] time 0.420 (0.459) data 0.289 (0.328) loss_u loss_u 0.9600 (0.9577) acc_u 6.2500 (5.3750) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [80/84] time 0.393 (0.462) data 0.262 (0.331) loss_u loss_u 0.9424 (0.9575) acc_u 6.2500 (5.3906) lr 1.4679e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1728
confident_label rate tensor(0.1473, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 462
clean true:445
clean false:17
clean_rate:0.9632034632034632
noisy true:963
noisy false:1711
after delete: len(clean_dataset) 462
after delete: len(noisy_dataset) 2674
epoch [72/200] batch [5/14] time 0.439 (0.435) data 0.310 (0.305) loss_x loss_x 0.9277 (1.2461) acc_x 81.2500 (66.8750) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [10/14] time 0.411 (0.430) data 0.281 (0.300) loss_x loss_x 0.8643 (1.2145) acc_x 81.2500 (69.3750) lr 1.4540e-03 eta 0:00:01
epoch [72/200] batch [5/83] time 0.361 (0.445) data 0.230 (0.315) loss_u loss_u 0.9644 (0.9604) acc_u 3.1250 (5.0000) lr 1.4540e-03 eta 0:00:34
epoch [72/200] batch [10/83] time 0.348 (0.434) data 0.216 (0.304) loss_u loss_u 0.9995 (0.9645) acc_u 0.0000 (3.7500) lr 1.4540e-03 eta 0:00:31
epoch [72/200] batch [15/83] time 0.418 (0.438) data 0.287 (0.307) loss_u loss_u 0.9531 (0.9624) acc_u 6.2500 (4.1667) lr 1.4540e-03 eta 0:00:29
epoch [72/200] batch [20/83] time 0.413 (0.438) data 0.282 (0.307) loss_u loss_u 0.9712 (0.9621) acc_u 6.2500 (4.8438) lr 1.4540e-03 eta 0:00:27
epoch [72/200] batch [25/83] time 0.446 (0.438) data 0.315 (0.307) loss_u loss_u 0.9517 (0.9613) acc_u 6.2500 (4.8750) lr 1.4540e-03 eta 0:00:25
epoch [72/200] batch [30/83] time 0.496 (0.440) data 0.365 (0.309) loss_u loss_u 0.9531 (0.9597) acc_u 6.2500 (5.3125) lr 1.4540e-03 eta 0:00:23
epoch [72/200] batch [35/83] time 0.531 (0.440) data 0.401 (0.309) loss_u loss_u 0.9575 (0.9608) acc_u 6.2500 (5.2679) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [40/83] time 0.467 (0.441) data 0.335 (0.310) loss_u loss_u 0.9502 (0.9615) acc_u 9.3750 (5.2344) lr 1.4540e-03 eta 0:00:18
epoch [72/200] batch [45/83] time 0.523 (0.443) data 0.391 (0.312) loss_u loss_u 0.9517 (0.9636) acc_u 6.2500 (4.9306) lr 1.4540e-03 eta 0:00:16
epoch [72/200] batch [50/83] time 0.687 (0.445) data 0.555 (0.314) loss_u loss_u 0.9927 (0.9650) acc_u 0.0000 (4.6250) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [55/83] time 0.460 (0.443) data 0.328 (0.312) loss_u loss_u 0.9546 (0.9659) acc_u 6.2500 (4.4318) lr 1.4540e-03 eta 0:00:12
epoch [72/200] batch [60/83] time 0.406 (0.444) data 0.274 (0.313) loss_u loss_u 0.9712 (0.9676) acc_u 6.2500 (4.3229) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [65/83] time 0.522 (0.444) data 0.391 (0.313) loss_u loss_u 0.9673 (0.9679) acc_u 3.1250 (4.1827) lr 1.4540e-03 eta 0:00:07
epoch [72/200] batch [70/83] time 0.517 (0.446) data 0.386 (0.315) loss_u loss_u 0.9639 (0.9674) acc_u 3.1250 (4.2857) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [75/83] time 0.354 (0.444) data 0.223 (0.313) loss_u loss_u 0.9541 (0.9666) acc_u 3.1250 (4.3333) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [80/83] time 0.455 (0.447) data 0.324 (0.316) loss_u loss_u 0.9883 (0.9667) acc_u 0.0000 (4.3750) lr 1.4540e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1751
confident_label rate tensor(0.1416, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 444
clean true:426
clean false:18
clean_rate:0.9594594594594594
noisy true:959
noisy false:1733
after delete: len(clean_dataset) 444
after delete: len(noisy_dataset) 2692
epoch [73/200] batch [5/13] time 0.378 (0.491) data 0.249 (0.361) loss_x loss_x 0.6621 (1.1248) acc_x 81.2500 (70.6250) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [10/13] time 0.456 (0.462) data 0.327 (0.331) loss_x loss_x 2.2676 (1.2475) acc_x 62.5000 (69.0625) lr 1.4399e-03 eta 0:00:01
epoch [73/200] batch [5/84] time 0.451 (0.477) data 0.320 (0.347) loss_u loss_u 0.9839 (0.9757) acc_u 0.0000 (3.1250) lr 1.4399e-03 eta 0:00:37
epoch [73/200] batch [10/84] time 0.430 (0.469) data 0.300 (0.338) loss_u loss_u 0.9756 (0.9612) acc_u 3.1250 (5.3125) lr 1.4399e-03 eta 0:00:34
epoch [73/200] batch [15/84] time 0.422 (0.475) data 0.290 (0.344) loss_u loss_u 0.9316 (0.9588) acc_u 6.2500 (5.2083) lr 1.4399e-03 eta 0:00:32
epoch [73/200] batch [20/84] time 0.441 (0.463) data 0.310 (0.333) loss_u loss_u 0.9204 (0.9595) acc_u 9.3750 (5.3125) lr 1.4399e-03 eta 0:00:29
epoch [73/200] batch [25/84] time 0.341 (0.464) data 0.209 (0.333) loss_u loss_u 0.9448 (0.9601) acc_u 6.2500 (5.2500) lr 1.4399e-03 eta 0:00:27
epoch [73/200] batch [30/84] time 0.446 (0.468) data 0.315 (0.337) loss_u loss_u 0.9624 (0.9618) acc_u 6.2500 (5.2083) lr 1.4399e-03 eta 0:00:25
epoch [73/200] batch [35/84] time 0.504 (0.466) data 0.373 (0.335) loss_u loss_u 0.9614 (0.9619) acc_u 3.1250 (5.1786) lr 1.4399e-03 eta 0:00:22
epoch [73/200] batch [40/84] time 0.472 (0.463) data 0.341 (0.332) loss_u loss_u 0.9800 (0.9630) acc_u 6.2500 (5.0781) lr 1.4399e-03 eta 0:00:20
epoch [73/200] batch [45/84] time 0.519 (0.467) data 0.388 (0.336) loss_u loss_u 0.9580 (0.9632) acc_u 3.1250 (5.0000) lr 1.4399e-03 eta 0:00:18
epoch [73/200] batch [50/84] time 0.557 (0.466) data 0.426 (0.335) loss_u loss_u 0.9688 (0.9621) acc_u 3.1250 (5.0000) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [55/84] time 0.389 (0.461) data 0.258 (0.330) loss_u loss_u 0.9424 (0.9620) acc_u 6.2500 (5.0568) lr 1.4399e-03 eta 0:00:13
epoch [73/200] batch [60/84] time 0.415 (0.459) data 0.285 (0.328) loss_u loss_u 0.9780 (0.9636) acc_u 6.2500 (4.9479) lr 1.4399e-03 eta 0:00:11
epoch [73/200] batch [65/84] time 0.493 (0.463) data 0.361 (0.332) loss_u loss_u 0.9751 (0.9629) acc_u 6.2500 (5.0962) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [70/84] time 0.372 (0.459) data 0.240 (0.328) loss_u loss_u 0.9878 (0.9632) acc_u 3.1250 (5.0446) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [75/84] time 0.357 (0.456) data 0.226 (0.325) loss_u loss_u 0.9883 (0.9636) acc_u 3.1250 (5.0000) lr 1.4399e-03 eta 0:00:04
epoch [73/200] batch [80/84] time 0.410 (0.453) data 0.279 (0.322) loss_u loss_u 0.9692 (0.9631) acc_u 6.2500 (5.0781) lr 1.4399e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1762
confident_label rate tensor(0.1378, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 432
clean true:420
clean false:12
clean_rate:0.9722222222222222
noisy true:954
noisy false:1750
after delete: len(clean_dataset) 432
after delete: len(noisy_dataset) 2704
epoch [74/200] batch [5/13] time 0.392 (0.476) data 0.262 (0.345) loss_x loss_x 1.3564 (1.1875) acc_x 62.5000 (69.3750) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [10/13] time 0.409 (0.487) data 0.279 (0.356) loss_x loss_x 1.7158 (1.3090) acc_x 59.3750 (66.8750) lr 1.4258e-03 eta 0:00:01
epoch [74/200] batch [5/84] time 0.446 (0.471) data 0.315 (0.341) loss_u loss_u 0.9380 (0.9585) acc_u 6.2500 (4.3750) lr 1.4258e-03 eta 0:00:37
epoch [74/200] batch [10/84] time 0.441 (0.470) data 0.310 (0.339) loss_u loss_u 0.9448 (0.9621) acc_u 6.2500 (4.0625) lr 1.4258e-03 eta 0:00:34
epoch [74/200] batch [15/84] time 0.585 (0.472) data 0.455 (0.341) loss_u loss_u 0.9888 (0.9659) acc_u 3.1250 (3.9583) lr 1.4258e-03 eta 0:00:32
epoch [74/200] batch [20/84] time 0.503 (0.467) data 0.372 (0.336) loss_u loss_u 0.9741 (0.9683) acc_u 3.1250 (3.9062) lr 1.4258e-03 eta 0:00:29
epoch [74/200] batch [25/84] time 0.501 (0.466) data 0.370 (0.335) loss_u loss_u 0.9404 (0.9655) acc_u 9.3750 (4.3750) lr 1.4258e-03 eta 0:00:27
epoch [74/200] batch [30/84] time 0.471 (0.460) data 0.340 (0.330) loss_u loss_u 0.9180 (0.9622) acc_u 12.5000 (4.7917) lr 1.4258e-03 eta 0:00:24
epoch [74/200] batch [35/84] time 0.655 (0.462) data 0.524 (0.332) loss_u loss_u 0.9780 (0.9635) acc_u 3.1250 (4.7321) lr 1.4258e-03 eta 0:00:22
epoch [74/200] batch [40/84] time 0.419 (0.461) data 0.288 (0.330) loss_u loss_u 0.9644 (0.9635) acc_u 6.2500 (4.8438) lr 1.4258e-03 eta 0:00:20
epoch [74/200] batch [45/84] time 0.367 (0.458) data 0.236 (0.327) loss_u loss_u 0.9683 (0.9637) acc_u 3.1250 (4.7917) lr 1.4258e-03 eta 0:00:17
epoch [74/200] batch [50/84] time 0.489 (0.458) data 0.358 (0.327) loss_u loss_u 0.9194 (0.9623) acc_u 9.3750 (5.1250) lr 1.4258e-03 eta 0:00:15
epoch [74/200] batch [55/84] time 0.494 (0.460) data 0.363 (0.329) loss_u loss_u 0.9365 (0.9631) acc_u 12.5000 (5.0000) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [60/84] time 0.363 (0.461) data 0.232 (0.331) loss_u loss_u 0.9512 (0.9631) acc_u 9.3750 (5.0000) lr 1.4258e-03 eta 0:00:11
epoch [74/200] batch [65/84] time 0.514 (0.466) data 0.383 (0.336) loss_u loss_u 0.9492 (0.9634) acc_u 6.2500 (4.9038) lr 1.4258e-03 eta 0:00:08
epoch [74/200] batch [70/84] time 0.463 (0.465) data 0.332 (0.335) loss_u loss_u 0.9971 (0.9638) acc_u 0.0000 (4.7768) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [75/84] time 0.361 (0.462) data 0.231 (0.331) loss_u loss_u 0.9541 (0.9638) acc_u 6.2500 (4.8750) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [80/84] time 0.508 (0.461) data 0.376 (0.330) loss_u loss_u 0.9805 (0.9643) acc_u 3.1250 (4.8047) lr 1.4258e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1750
confident_label rate tensor(0.1425, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 447
clean true:429
clean false:18
clean_rate:0.959731543624161
noisy true:957
noisy false:1732
after delete: len(clean_dataset) 447
after delete: len(noisy_dataset) 2689
epoch [75/200] batch [5/13] time 0.424 (0.475) data 0.294 (0.345) loss_x loss_x 2.1016 (1.6545) acc_x 59.3750 (62.5000) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [10/13] time 0.434 (0.471) data 0.303 (0.340) loss_x loss_x 1.3555 (1.4626) acc_x 65.6250 (65.9375) lr 1.4115e-03 eta 0:00:01
epoch [75/200] batch [5/84] time 0.362 (0.437) data 0.232 (0.306) loss_u loss_u 0.9775 (0.9657) acc_u 0.0000 (3.1250) lr 1.4115e-03 eta 0:00:34
epoch [75/200] batch [10/84] time 0.430 (0.444) data 0.298 (0.314) loss_u loss_u 0.9800 (0.9733) acc_u 3.1250 (3.1250) lr 1.4115e-03 eta 0:00:32
epoch [75/200] batch [15/84] time 0.363 (0.446) data 0.233 (0.315) loss_u loss_u 0.9873 (0.9735) acc_u 3.1250 (3.1250) lr 1.4115e-03 eta 0:00:30
epoch [75/200] batch [20/84] time 0.414 (0.449) data 0.283 (0.318) loss_u loss_u 0.9893 (0.9751) acc_u 3.1250 (3.1250) lr 1.4115e-03 eta 0:00:28
epoch [75/200] batch [25/84] time 0.442 (0.447) data 0.311 (0.316) loss_u loss_u 0.9448 (0.9719) acc_u 6.2500 (3.5000) lr 1.4115e-03 eta 0:00:26
epoch [75/200] batch [30/84] time 0.499 (0.454) data 0.369 (0.323) loss_u loss_u 0.9634 (0.9698) acc_u 3.1250 (3.7500) lr 1.4115e-03 eta 0:00:24
epoch [75/200] batch [35/84] time 0.462 (0.450) data 0.332 (0.319) loss_u loss_u 0.9443 (0.9690) acc_u 6.2500 (3.8393) lr 1.4115e-03 eta 0:00:22
epoch [75/200] batch [40/84] time 0.531 (0.453) data 0.400 (0.323) loss_u loss_u 0.9902 (0.9694) acc_u 0.0000 (3.8281) lr 1.4115e-03 eta 0:00:19
epoch [75/200] batch [45/84] time 0.466 (0.450) data 0.336 (0.320) loss_u loss_u 0.9546 (0.9686) acc_u 6.2500 (3.9583) lr 1.4115e-03 eta 0:00:17
epoch [75/200] batch [50/84] time 0.406 (0.448) data 0.275 (0.317) loss_u loss_u 0.9673 (0.9687) acc_u 6.2500 (4.1250) lr 1.4115e-03 eta 0:00:15
epoch [75/200] batch [55/84] time 0.481 (0.445) data 0.351 (0.314) loss_u loss_u 0.9678 (0.9693) acc_u 6.2500 (4.0909) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [60/84] time 0.448 (0.446) data 0.317 (0.315) loss_u loss_u 0.9536 (0.9675) acc_u 3.1250 (4.3750) lr 1.4115e-03 eta 0:00:10
epoch [75/200] batch [65/84] time 0.445 (0.445) data 0.314 (0.315) loss_u loss_u 0.9121 (0.9651) acc_u 12.5000 (4.7115) lr 1.4115e-03 eta 0:00:08
epoch [75/200] batch [70/84] time 0.514 (0.449) data 0.383 (0.318) loss_u loss_u 0.9854 (0.9661) acc_u 3.1250 (4.5536) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [75/84] time 0.459 (0.450) data 0.328 (0.320) loss_u loss_u 0.9927 (0.9650) acc_u 0.0000 (4.5833) lr 1.4115e-03 eta 0:00:04
epoch [75/200] batch [80/84] time 0.531 (0.452) data 0.401 (0.321) loss_u loss_u 0.9985 (0.9645) acc_u 0.0000 (4.7656) lr 1.4115e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1763
confident_label rate tensor(0.1416, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 444
clean true:429
clean false:15
clean_rate:0.9662162162162162
noisy true:944
noisy false:1748
after delete: len(clean_dataset) 444
after delete: len(noisy_dataset) 2692
epoch [76/200] batch [5/13] time 0.465 (0.462) data 0.334 (0.331) loss_x loss_x 1.1309 (1.1938) acc_x 62.5000 (68.7500) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [10/13] time 0.381 (0.483) data 0.250 (0.352) loss_x loss_x 1.0127 (1.2008) acc_x 68.7500 (70.0000) lr 1.3971e-03 eta 0:00:01
epoch [76/200] batch [5/84] time 0.473 (0.475) data 0.341 (0.344) loss_u loss_u 0.9331 (0.9616) acc_u 9.3750 (5.0000) lr 1.3971e-03 eta 0:00:37
epoch [76/200] batch [10/84] time 0.459 (0.474) data 0.328 (0.343) loss_u loss_u 0.9536 (0.9674) acc_u 6.2500 (4.3750) lr 1.3971e-03 eta 0:00:35
epoch [76/200] batch [15/84] time 0.351 (0.477) data 0.219 (0.346) loss_u loss_u 0.9766 (0.9649) acc_u 3.1250 (4.5833) lr 1.3971e-03 eta 0:00:32
epoch [76/200] batch [20/84] time 0.373 (0.466) data 0.241 (0.335) loss_u loss_u 0.9492 (0.9637) acc_u 6.2500 (4.8438) lr 1.3971e-03 eta 0:00:29
epoch [76/200] batch [25/84] time 0.359 (0.463) data 0.227 (0.332) loss_u loss_u 0.9731 (0.9646) acc_u 3.1250 (4.8750) lr 1.3971e-03 eta 0:00:27
epoch [76/200] batch [30/84] time 0.465 (0.466) data 0.334 (0.335) loss_u loss_u 0.9712 (0.9644) acc_u 6.2500 (4.7917) lr 1.3971e-03 eta 0:00:25
epoch [76/200] batch [35/84] time 0.492 (0.462) data 0.361 (0.331) loss_u loss_u 0.9863 (0.9635) acc_u 3.1250 (5.0000) lr 1.3971e-03 eta 0:00:22
epoch [76/200] batch [40/84] time 0.472 (0.462) data 0.342 (0.331) loss_u loss_u 0.9688 (0.9652) acc_u 3.1250 (4.8438) lr 1.3971e-03 eta 0:00:20
epoch [76/200] batch [45/84] time 0.385 (0.457) data 0.255 (0.326) loss_u loss_u 0.9951 (0.9655) acc_u 0.0000 (4.7917) lr 1.3971e-03 eta 0:00:17
epoch [76/200] batch [50/84] time 0.389 (0.455) data 0.259 (0.324) loss_u loss_u 0.9814 (0.9670) acc_u 6.2500 (4.6875) lr 1.3971e-03 eta 0:00:15
epoch [76/200] batch [55/84] time 0.460 (0.453) data 0.328 (0.322) loss_u loss_u 0.9888 (0.9656) acc_u 3.1250 (4.8864) lr 1.3971e-03 eta 0:00:13
epoch [76/200] batch [60/84] time 0.418 (0.450) data 0.288 (0.319) loss_u loss_u 0.9482 (0.9639) acc_u 3.1250 (5.0000) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [65/84] time 0.362 (0.449) data 0.230 (0.318) loss_u loss_u 0.9956 (0.9649) acc_u 0.0000 (4.9038) lr 1.3971e-03 eta 0:00:08
epoch [76/200] batch [70/84] time 0.413 (0.446) data 0.283 (0.315) loss_u loss_u 0.9951 (0.9648) acc_u 0.0000 (4.9107) lr 1.3971e-03 eta 0:00:06
epoch [76/200] batch [75/84] time 0.515 (0.451) data 0.384 (0.320) loss_u loss_u 0.9365 (0.9645) acc_u 9.3750 (4.9583) lr 1.3971e-03 eta 0:00:04
epoch [76/200] batch [80/84] time 0.373 (0.451) data 0.241 (0.320) loss_u loss_u 0.9414 (0.9640) acc_u 6.2500 (5.0000) lr 1.3971e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1755
confident_label rate tensor(0.1400, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 439
clean true:427
clean false:12
clean_rate:0.9726651480637813
noisy true:954
noisy false:1743
after delete: len(clean_dataset) 439
after delete: len(noisy_dataset) 2697
epoch [77/200] batch [5/13] time 0.371 (0.431) data 0.241 (0.301) loss_x loss_x 1.8242 (1.4967) acc_x 68.7500 (66.8750) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [10/13] time 0.407 (0.440) data 0.276 (0.310) loss_x loss_x 1.0615 (1.3711) acc_x 75.0000 (66.8750) lr 1.3827e-03 eta 0:00:01
epoch [77/200] batch [5/84] time 0.551 (0.465) data 0.419 (0.335) loss_u loss_u 0.9287 (0.9525) acc_u 9.3750 (5.6250) lr 1.3827e-03 eta 0:00:36
epoch [77/200] batch [10/84] time 0.330 (0.469) data 0.198 (0.338) loss_u loss_u 0.9531 (0.9601) acc_u 6.2500 (5.0000) lr 1.3827e-03 eta 0:00:34
epoch [77/200] batch [15/84] time 0.426 (0.465) data 0.294 (0.334) loss_u loss_u 0.9897 (0.9638) acc_u 0.0000 (4.5833) lr 1.3827e-03 eta 0:00:32
epoch [77/200] batch [20/84] time 0.545 (0.464) data 0.413 (0.333) loss_u loss_u 0.9390 (0.9658) acc_u 9.3750 (4.2188) lr 1.3827e-03 eta 0:00:29
epoch [77/200] batch [25/84] time 0.441 (0.458) data 0.309 (0.326) loss_u loss_u 0.9287 (0.9633) acc_u 9.3750 (4.6250) lr 1.3827e-03 eta 0:00:27
epoch [77/200] batch [30/84] time 0.309 (0.458) data 0.178 (0.327) loss_u loss_u 0.9639 (0.9627) acc_u 12.5000 (5.2083) lr 1.3827e-03 eta 0:00:24
epoch [77/200] batch [35/84] time 0.535 (0.460) data 0.405 (0.328) loss_u loss_u 0.9834 (0.9637) acc_u 3.1250 (5.0000) lr 1.3827e-03 eta 0:00:22
epoch [77/200] batch [40/84] time 0.398 (0.469) data 0.267 (0.338) loss_u loss_u 0.9741 (0.9645) acc_u 6.2500 (5.0000) lr 1.3827e-03 eta 0:00:20
epoch [77/200] batch [45/84] time 0.485 (0.471) data 0.354 (0.340) loss_u loss_u 0.9531 (0.9633) acc_u 6.2500 (5.0694) lr 1.3827e-03 eta 0:00:18
epoch [77/200] batch [50/84] time 0.412 (0.469) data 0.281 (0.338) loss_u loss_u 0.9629 (0.9617) acc_u 6.2500 (5.2500) lr 1.3827e-03 eta 0:00:15
epoch [77/200] batch [55/84] time 0.390 (0.463) data 0.257 (0.332) loss_u loss_u 0.9658 (0.9623) acc_u 3.1250 (5.1705) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [60/84] time 0.380 (0.461) data 0.249 (0.329) loss_u loss_u 0.9595 (0.9635) acc_u 6.2500 (4.9479) lr 1.3827e-03 eta 0:00:11
epoch [77/200] batch [65/84] time 0.412 (0.457) data 0.280 (0.326) loss_u loss_u 0.9775 (0.9632) acc_u 6.2500 (5.0000) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [70/84] time 0.420 (0.455) data 0.289 (0.324) loss_u loss_u 0.9736 (0.9635) acc_u 3.1250 (4.9107) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [75/84] time 0.445 (0.454) data 0.314 (0.323) loss_u loss_u 0.9438 (0.9640) acc_u 6.2500 (4.8333) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [80/84] time 0.399 (0.454) data 0.266 (0.323) loss_u loss_u 0.9277 (0.9629) acc_u 6.2500 (4.9609) lr 1.3827e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1760
confident_label rate tensor(0.1368, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 429
clean true:419
clean false:10
clean_rate:0.9766899766899767
noisy true:957
noisy false:1750
after delete: len(clean_dataset) 429
after delete: len(noisy_dataset) 2707
epoch [78/200] batch [5/13] time 0.527 (0.482) data 0.397 (0.352) loss_x loss_x 1.3613 (1.2732) acc_x 71.8750 (73.1250) lr 1.3681e-03 eta 0:00:03
epoch [78/200] batch [10/13] time 0.447 (0.469) data 0.318 (0.339) loss_x loss_x 0.8691 (1.3923) acc_x 78.1250 (67.5000) lr 1.3681e-03 eta 0:00:01
epoch [78/200] batch [5/84] time 0.563 (0.463) data 0.432 (0.333) loss_u loss_u 0.9775 (0.9650) acc_u 3.1250 (5.0000) lr 1.3681e-03 eta 0:00:36
epoch [78/200] batch [10/84] time 0.391 (0.460) data 0.259 (0.329) loss_u loss_u 0.9688 (0.9676) acc_u 3.1250 (4.3750) lr 1.3681e-03 eta 0:00:34
epoch [78/200] batch [15/84] time 0.550 (0.453) data 0.419 (0.322) loss_u loss_u 0.9170 (0.9609) acc_u 15.6250 (5.4167) lr 1.3681e-03 eta 0:00:31
epoch [78/200] batch [20/84] time 0.417 (0.449) data 0.287 (0.318) loss_u loss_u 0.9541 (0.9585) acc_u 6.2500 (5.4688) lr 1.3681e-03 eta 0:00:28
epoch [78/200] batch [25/84] time 0.479 (0.455) data 0.348 (0.324) loss_u loss_u 0.9810 (0.9600) acc_u 3.1250 (5.1250) lr 1.3681e-03 eta 0:00:26
epoch [78/200] batch [30/84] time 0.427 (0.459) data 0.297 (0.328) loss_u loss_u 0.9390 (0.9631) acc_u 9.3750 (4.7917) lr 1.3681e-03 eta 0:00:24
epoch [78/200] batch [35/84] time 0.319 (0.451) data 0.188 (0.321) loss_u loss_u 0.9585 (0.9643) acc_u 9.3750 (4.7321) lr 1.3681e-03 eta 0:00:22
epoch [78/200] batch [40/84] time 0.462 (0.451) data 0.331 (0.321) loss_u loss_u 0.9282 (0.9617) acc_u 9.3750 (4.9219) lr 1.3681e-03 eta 0:00:19
epoch [78/200] batch [45/84] time 0.467 (0.451) data 0.337 (0.321) loss_u loss_u 0.9629 (0.9623) acc_u 3.1250 (4.7222) lr 1.3681e-03 eta 0:00:17
epoch [78/200] batch [50/84] time 0.371 (0.449) data 0.240 (0.319) loss_u loss_u 0.9551 (0.9621) acc_u 6.2500 (4.7500) lr 1.3681e-03 eta 0:00:15
epoch [78/200] batch [55/84] time 0.456 (0.451) data 0.325 (0.320) loss_u loss_u 0.9785 (0.9639) acc_u 6.2500 (4.6023) lr 1.3681e-03 eta 0:00:13
epoch [78/200] batch [60/84] time 0.549 (0.455) data 0.418 (0.324) loss_u loss_u 0.9277 (0.9639) acc_u 6.2500 (4.5833) lr 1.3681e-03 eta 0:00:10
epoch [78/200] batch [65/84] time 0.501 (0.455) data 0.371 (0.325) loss_u loss_u 0.9287 (0.9632) acc_u 9.3750 (4.7596) lr 1.3681e-03 eta 0:00:08
epoch [78/200] batch [70/84] time 0.510 (0.459) data 0.378 (0.328) loss_u loss_u 0.9805 (0.9623) acc_u 0.0000 (4.8661) lr 1.3681e-03 eta 0:00:06
epoch [78/200] batch [75/84] time 0.504 (0.460) data 0.373 (0.329) loss_u loss_u 0.9707 (0.9621) acc_u 3.1250 (4.7917) lr 1.3681e-03 eta 0:00:04
epoch [78/200] batch [80/84] time 0.585 (0.461) data 0.454 (0.330) loss_u loss_u 0.9922 (0.9624) acc_u 0.0000 (4.7266) lr 1.3681e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1759
confident_label rate tensor(0.1460, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 458
clean true:444
clean false:14
clean_rate:0.9694323144104804
noisy true:933
noisy false:1745
after delete: len(clean_dataset) 458
after delete: len(noisy_dataset) 2678
epoch [79/200] batch [5/14] time 0.371 (0.451) data 0.241 (0.320) loss_x loss_x 1.5000 (1.2917) acc_x 68.7500 (68.7500) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [10/14] time 0.573 (0.458) data 0.443 (0.327) loss_x loss_x 1.1543 (1.2626) acc_x 68.7500 (67.8125) lr 1.3535e-03 eta 0:00:01
epoch [79/200] batch [5/83] time 0.554 (0.451) data 0.423 (0.321) loss_u loss_u 0.9941 (0.9777) acc_u 0.0000 (2.5000) lr 1.3535e-03 eta 0:00:35
epoch [79/200] batch [10/83] time 0.351 (0.442) data 0.221 (0.311) loss_u loss_u 0.9766 (0.9730) acc_u 6.2500 (3.1250) lr 1.3535e-03 eta 0:00:32
epoch [79/200] batch [15/83] time 0.505 (0.439) data 0.373 (0.308) loss_u loss_u 0.9829 (0.9759) acc_u 3.1250 (3.1250) lr 1.3535e-03 eta 0:00:29
epoch [79/200] batch [20/83] time 0.395 (0.431) data 0.264 (0.300) loss_u loss_u 0.9453 (0.9718) acc_u 9.3750 (3.9062) lr 1.3535e-03 eta 0:00:27
epoch [79/200] batch [25/83] time 0.420 (0.431) data 0.289 (0.300) loss_u loss_u 0.9619 (0.9704) acc_u 6.2500 (3.8750) lr 1.3535e-03 eta 0:00:25
epoch [79/200] batch [30/83] time 0.435 (0.431) data 0.304 (0.300) loss_u loss_u 0.9922 (0.9727) acc_u 0.0000 (3.5417) lr 1.3535e-03 eta 0:00:22
epoch [79/200] batch [35/83] time 0.444 (0.431) data 0.313 (0.300) loss_u loss_u 0.9883 (0.9749) acc_u 0.0000 (3.1250) lr 1.3535e-03 eta 0:00:20
epoch [79/200] batch [40/83] time 0.394 (0.434) data 0.263 (0.303) loss_u loss_u 0.9702 (0.9723) acc_u 3.1250 (3.5938) lr 1.3535e-03 eta 0:00:18
epoch [79/200] batch [45/83] time 0.404 (0.435) data 0.273 (0.304) loss_u loss_u 0.9600 (0.9710) acc_u 6.2500 (3.9583) lr 1.3535e-03 eta 0:00:16
epoch [79/200] batch [50/83] time 0.461 (0.438) data 0.330 (0.307) loss_u loss_u 0.9521 (0.9698) acc_u 6.2500 (4.0625) lr 1.3535e-03 eta 0:00:14
epoch [79/200] batch [55/83] time 0.430 (0.439) data 0.299 (0.308) loss_u loss_u 0.9463 (0.9684) acc_u 9.3750 (4.2614) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [60/83] time 0.472 (0.439) data 0.342 (0.308) loss_u loss_u 0.9595 (0.9674) acc_u 3.1250 (4.2188) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [65/83] time 0.397 (0.438) data 0.266 (0.307) loss_u loss_u 0.9678 (0.9670) acc_u 6.2500 (4.3269) lr 1.3535e-03 eta 0:00:07
epoch [79/200] batch [70/83] time 0.441 (0.438) data 0.309 (0.307) loss_u loss_u 0.9712 (0.9675) acc_u 3.1250 (4.2857) lr 1.3535e-03 eta 0:00:05
epoch [79/200] batch [75/83] time 0.380 (0.437) data 0.249 (0.306) loss_u loss_u 0.9561 (0.9671) acc_u 3.1250 (4.3750) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [80/83] time 0.477 (0.444) data 0.346 (0.313) loss_u loss_u 0.9419 (0.9666) acc_u 12.5000 (4.4531) lr 1.3535e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1755
confident_label rate tensor(0.1473, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 462
clean true:451
clean false:11
clean_rate:0.9761904761904762
noisy true:930
noisy false:1744
after delete: len(clean_dataset) 462
after delete: len(noisy_dataset) 2674
epoch [80/200] batch [5/14] time 0.346 (0.410) data 0.217 (0.280) loss_x loss_x 1.1729 (1.5475) acc_x 71.8750 (63.1250) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [10/14] time 0.487 (0.454) data 0.357 (0.324) loss_x loss_x 1.3223 (1.4563) acc_x 53.1250 (62.8125) lr 1.3387e-03 eta 0:00:01
epoch [80/200] batch [5/83] time 0.362 (0.437) data 0.230 (0.306) loss_u loss_u 0.9688 (0.9708) acc_u 3.1250 (4.3750) lr 1.3387e-03 eta 0:00:34
epoch [80/200] batch [10/83] time 0.464 (0.438) data 0.333 (0.308) loss_u loss_u 0.9482 (0.9612) acc_u 9.3750 (5.6250) lr 1.3387e-03 eta 0:00:32
epoch [80/200] batch [15/83] time 0.378 (0.440) data 0.247 (0.309) loss_u loss_u 0.9570 (0.9663) acc_u 9.3750 (5.0000) lr 1.3387e-03 eta 0:00:29
epoch [80/200] batch [20/83] time 0.569 (0.448) data 0.438 (0.317) loss_u loss_u 0.9829 (0.9673) acc_u 3.1250 (4.5312) lr 1.3387e-03 eta 0:00:28
epoch [80/200] batch [25/83] time 0.468 (0.443) data 0.337 (0.312) loss_u loss_u 0.9570 (0.9657) acc_u 6.2500 (4.7500) lr 1.3387e-03 eta 0:00:25
epoch [80/200] batch [30/83] time 0.363 (0.448) data 0.232 (0.317) loss_u loss_u 0.9937 (0.9656) acc_u 0.0000 (4.6875) lr 1.3387e-03 eta 0:00:23
epoch [80/200] batch [35/83] time 0.479 (0.451) data 0.346 (0.320) loss_u loss_u 0.9502 (0.9646) acc_u 6.2500 (4.8214) lr 1.3387e-03 eta 0:00:21
epoch [80/200] batch [40/83] time 0.459 (0.451) data 0.327 (0.320) loss_u loss_u 0.9609 (0.9655) acc_u 3.1250 (4.7656) lr 1.3387e-03 eta 0:00:19
epoch [80/200] batch [45/83] time 0.751 (0.456) data 0.620 (0.325) loss_u loss_u 0.9814 (0.9638) acc_u 0.0000 (5.0000) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [50/83] time 0.346 (0.458) data 0.215 (0.327) loss_u loss_u 0.9214 (0.9630) acc_u 12.5000 (5.1250) lr 1.3387e-03 eta 0:00:15
epoch [80/200] batch [55/83] time 0.394 (0.454) data 0.262 (0.323) loss_u loss_u 0.9453 (0.9620) acc_u 6.2500 (5.1705) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [60/83] time 0.399 (0.451) data 0.267 (0.320) loss_u loss_u 0.9868 (0.9631) acc_u 3.1250 (4.9479) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [65/83] time 0.420 (0.451) data 0.289 (0.319) loss_u loss_u 0.9648 (0.9631) acc_u 3.1250 (4.9519) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [70/83] time 0.460 (0.448) data 0.329 (0.317) loss_u loss_u 0.9912 (0.9641) acc_u 0.0000 (4.7321) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [75/83] time 0.352 (0.446) data 0.220 (0.315) loss_u loss_u 0.9688 (0.9644) acc_u 3.1250 (4.7500) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [80/83] time 0.508 (0.447) data 0.376 (0.316) loss_u loss_u 0.9878 (0.9659) acc_u 0.0000 (4.5312) lr 1.3387e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1767
confident_label rate tensor(0.1451, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 455
clean true:439
clean false:16
clean_rate:0.9648351648351648
noisy true:930
noisy false:1751
after delete: len(clean_dataset) 455
after delete: len(noisy_dataset) 2681
epoch [81/200] batch [5/14] time 0.528 (0.534) data 0.398 (0.404) loss_x loss_x 1.1846 (1.1683) acc_x 75.0000 (73.7500) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [10/14] time 0.364 (0.489) data 0.233 (0.359) loss_x loss_x 1.4668 (1.1876) acc_x 65.6250 (71.8750) lr 1.3239e-03 eta 0:00:01
epoch [81/200] batch [5/83] time 0.375 (0.460) data 0.244 (0.329) loss_u loss_u 0.9688 (0.9674) acc_u 9.3750 (5.6250) lr 1.3239e-03 eta 0:00:35
epoch [81/200] batch [10/83] time 0.334 (0.454) data 0.203 (0.323) loss_u loss_u 0.9805 (0.9772) acc_u 6.2500 (3.7500) lr 1.3239e-03 eta 0:00:33
epoch [81/200] batch [15/83] time 0.447 (0.444) data 0.315 (0.313) loss_u loss_u 0.9673 (0.9661) acc_u 6.2500 (5.0000) lr 1.3239e-03 eta 0:00:30
epoch [81/200] batch [20/83] time 0.338 (0.439) data 0.207 (0.308) loss_u loss_u 0.9761 (0.9653) acc_u 6.2500 (4.8438) lr 1.3239e-03 eta 0:00:27
epoch [81/200] batch [25/83] time 0.332 (0.436) data 0.201 (0.305) loss_u loss_u 0.9839 (0.9680) acc_u 3.1250 (4.3750) lr 1.3239e-03 eta 0:00:25
epoch [81/200] batch [30/83] time 0.437 (0.438) data 0.306 (0.307) loss_u loss_u 0.9883 (0.9693) acc_u 3.1250 (4.4792) lr 1.3239e-03 eta 0:00:23
epoch [81/200] batch [35/83] time 0.638 (0.450) data 0.506 (0.319) loss_u loss_u 0.9771 (0.9700) acc_u 3.1250 (4.4643) lr 1.3239e-03 eta 0:00:21
epoch [81/200] batch [40/83] time 0.495 (0.453) data 0.364 (0.322) loss_u loss_u 0.9619 (0.9694) acc_u 3.1250 (4.3750) lr 1.3239e-03 eta 0:00:19
epoch [81/200] batch [45/83] time 0.483 (0.454) data 0.351 (0.323) loss_u loss_u 0.9834 (0.9680) acc_u 3.1250 (4.7222) lr 1.3239e-03 eta 0:00:17
epoch [81/200] batch [50/83] time 0.470 (0.454) data 0.339 (0.323) loss_u loss_u 0.9097 (0.9643) acc_u 12.5000 (5.1875) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [55/83] time 0.472 (0.452) data 0.341 (0.320) loss_u loss_u 0.9331 (0.9634) acc_u 9.3750 (5.2841) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [60/83] time 0.464 (0.448) data 0.332 (0.317) loss_u loss_u 0.9673 (0.9632) acc_u 6.2500 (5.3125) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [65/83] time 0.390 (0.445) data 0.259 (0.314) loss_u loss_u 0.9253 (0.9621) acc_u 9.3750 (5.4327) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [70/83] time 0.515 (0.447) data 0.383 (0.315) loss_u loss_u 0.9634 (0.9625) acc_u 6.2500 (5.4018) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [75/83] time 0.565 (0.448) data 0.434 (0.317) loss_u loss_u 0.9785 (0.9636) acc_u 3.1250 (5.2083) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [80/83] time 0.404 (0.449) data 0.272 (0.317) loss_u loss_u 0.9585 (0.9629) acc_u 3.1250 (5.2344) lr 1.3239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1729
confident_label rate tensor(0.1409, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 442
clean true:429
clean false:13
clean_rate:0.9705882352941176
noisy true:978
noisy false:1716
after delete: len(clean_dataset) 442
after delete: len(noisy_dataset) 2694
epoch [82/200] batch [5/13] time 0.389 (0.480) data 0.259 (0.349) loss_x loss_x 0.9443 (1.0688) acc_x 81.2500 (72.5000) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [10/13] time 0.506 (0.445) data 0.376 (0.314) loss_x loss_x 1.2783 (1.2126) acc_x 68.7500 (68.1250) lr 1.3090e-03 eta 0:00:01
epoch [82/200] batch [5/84] time 0.350 (0.455) data 0.219 (0.324) loss_u loss_u 0.9629 (0.9377) acc_u 6.2500 (8.1250) lr 1.3090e-03 eta 0:00:35
epoch [82/200] batch [10/84] time 0.390 (0.457) data 0.259 (0.326) loss_u loss_u 0.9512 (0.9529) acc_u 6.2500 (5.9375) lr 1.3090e-03 eta 0:00:33
epoch [82/200] batch [15/84] time 0.442 (0.450) data 0.311 (0.319) loss_u loss_u 0.9883 (0.9619) acc_u 0.0000 (5.0000) lr 1.3090e-03 eta 0:00:31
epoch [82/200] batch [20/84] time 0.440 (0.448) data 0.309 (0.317) loss_u loss_u 0.9785 (0.9580) acc_u 0.0000 (5.3125) lr 1.3090e-03 eta 0:00:28
epoch [82/200] batch [25/84] time 0.472 (0.449) data 0.341 (0.318) loss_u loss_u 0.9736 (0.9611) acc_u 0.0000 (5.0000) lr 1.3090e-03 eta 0:00:26
epoch [82/200] batch [30/84] time 0.487 (0.449) data 0.357 (0.318) loss_u loss_u 0.9878 (0.9644) acc_u 0.0000 (4.3750) lr 1.3090e-03 eta 0:00:24
epoch [82/200] batch [35/84] time 0.377 (0.444) data 0.246 (0.313) loss_u loss_u 0.9653 (0.9630) acc_u 3.1250 (4.6429) lr 1.3090e-03 eta 0:00:21
epoch [82/200] batch [40/84] time 0.454 (0.442) data 0.321 (0.311) loss_u loss_u 0.9761 (0.9631) acc_u 6.2500 (4.6875) lr 1.3090e-03 eta 0:00:19
epoch [82/200] batch [45/84] time 0.372 (0.437) data 0.241 (0.306) loss_u loss_u 0.9697 (0.9632) acc_u 6.2500 (4.7917) lr 1.3090e-03 eta 0:00:17
epoch [82/200] batch [50/84] time 0.501 (0.437) data 0.370 (0.306) loss_u loss_u 0.9932 (0.9633) acc_u 0.0000 (4.6250) lr 1.3090e-03 eta 0:00:14
epoch [82/200] batch [55/84] time 0.355 (0.438) data 0.224 (0.307) loss_u loss_u 0.9893 (0.9637) acc_u 0.0000 (4.4886) lr 1.3090e-03 eta 0:00:12
epoch [82/200] batch [60/84] time 0.647 (0.442) data 0.516 (0.311) loss_u loss_u 0.9712 (0.9635) acc_u 9.3750 (4.6875) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [65/84] time 0.524 (0.444) data 0.393 (0.313) loss_u loss_u 0.9844 (0.9628) acc_u 3.1250 (4.8077) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [70/84] time 0.446 (0.447) data 0.316 (0.316) loss_u loss_u 0.9624 (0.9630) acc_u 9.3750 (4.9107) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [75/84] time 0.409 (0.446) data 0.277 (0.315) loss_u loss_u 0.9629 (0.9631) acc_u 6.2500 (4.9167) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [80/84] time 0.338 (0.446) data 0.207 (0.314) loss_u loss_u 0.9414 (0.9630) acc_u 9.3750 (4.9609) lr 1.3090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1789
confident_label rate tensor(0.1432, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 449
clean true:436
clean false:13
clean_rate:0.9710467706013363
noisy true:911
noisy false:1776
after delete: len(clean_dataset) 449
after delete: len(noisy_dataset) 2687
epoch [83/200] batch [5/14] time 0.489 (0.464) data 0.359 (0.333) loss_x loss_x 1.2754 (1.3822) acc_x 75.0000 (70.6250) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [10/14] time 0.411 (0.479) data 0.281 (0.348) loss_x loss_x 0.8687 (1.2326) acc_x 78.1250 (72.8125) lr 1.2940e-03 eta 0:00:01
epoch [83/200] batch [5/83] time 0.411 (0.467) data 0.280 (0.336) loss_u loss_u 0.9624 (0.9683) acc_u 3.1250 (2.5000) lr 1.2940e-03 eta 0:00:36
epoch [83/200] batch [10/83] time 0.463 (0.455) data 0.331 (0.324) loss_u loss_u 0.9888 (0.9763) acc_u 3.1250 (2.1875) lr 1.2940e-03 eta 0:00:33
epoch [83/200] batch [15/83] time 0.369 (0.450) data 0.237 (0.319) loss_u loss_u 0.9238 (0.9743) acc_u 12.5000 (2.9167) lr 1.2940e-03 eta 0:00:30
epoch [83/200] batch [20/83] time 0.454 (0.446) data 0.322 (0.315) loss_u loss_u 0.9565 (0.9694) acc_u 6.2500 (3.7500) lr 1.2940e-03 eta 0:00:28
epoch [83/200] batch [25/83] time 0.369 (0.446) data 0.238 (0.315) loss_u loss_u 0.9946 (0.9691) acc_u 0.0000 (3.7500) lr 1.2940e-03 eta 0:00:25
epoch [83/200] batch [30/83] time 0.463 (0.445) data 0.332 (0.314) loss_u loss_u 0.9429 (0.9649) acc_u 6.2500 (4.3750) lr 1.2940e-03 eta 0:00:23
epoch [83/200] batch [35/83] time 0.453 (0.448) data 0.323 (0.317) loss_u loss_u 0.9424 (0.9649) acc_u 6.2500 (4.3750) lr 1.2940e-03 eta 0:00:21
epoch [83/200] batch [40/83] time 0.388 (0.445) data 0.256 (0.314) loss_u loss_u 0.9570 (0.9645) acc_u 6.2500 (4.4531) lr 1.2940e-03 eta 0:00:19
epoch [83/200] batch [45/83] time 0.340 (0.447) data 0.209 (0.315) loss_u loss_u 0.9609 (0.9638) acc_u 6.2500 (4.6528) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [50/83] time 0.572 (0.453) data 0.440 (0.322) loss_u loss_u 0.9365 (0.9635) acc_u 9.3750 (4.7500) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [55/83] time 0.378 (0.457) data 0.247 (0.326) loss_u loss_u 0.9790 (0.9620) acc_u 3.1250 (4.9432) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [60/83] time 0.453 (0.455) data 0.322 (0.324) loss_u loss_u 0.9517 (0.9633) acc_u 3.1250 (4.7396) lr 1.2940e-03 eta 0:00:10
epoch [83/200] batch [65/83] time 0.480 (0.457) data 0.350 (0.326) loss_u loss_u 0.9644 (0.9635) acc_u 9.3750 (4.7596) lr 1.2940e-03 eta 0:00:08
epoch [83/200] batch [70/83] time 0.550 (0.453) data 0.420 (0.322) loss_u loss_u 0.9790 (0.9635) acc_u 3.1250 (4.6875) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [75/83] time 0.401 (0.454) data 0.270 (0.323) loss_u loss_u 0.9551 (0.9632) acc_u 9.3750 (4.7917) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [80/83] time 0.532 (0.454) data 0.401 (0.323) loss_u loss_u 0.9873 (0.9637) acc_u 3.1250 (4.7266) lr 1.2940e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1775
confident_label rate tensor(0.1467, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 460
clean true:444
clean false:16
clean_rate:0.9652173913043478
noisy true:917
noisy false:1759
after delete: len(clean_dataset) 460
after delete: len(noisy_dataset) 2676
epoch [84/200] batch [5/14] time 0.338 (0.470) data 0.208 (0.340) loss_x loss_x 1.2197 (1.2640) acc_x 71.8750 (70.0000) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [10/14] time 0.466 (0.465) data 0.336 (0.335) loss_x loss_x 1.2646 (1.3370) acc_x 65.6250 (67.1875) lr 1.2790e-03 eta 0:00:01
epoch [84/200] batch [5/83] time 0.439 (0.457) data 0.307 (0.327) loss_u loss_u 0.9727 (0.9737) acc_u 6.2500 (4.3750) lr 1.2790e-03 eta 0:00:35
epoch [84/200] batch [10/83] time 0.381 (0.449) data 0.249 (0.318) loss_u loss_u 0.9678 (0.9692) acc_u 3.1250 (4.0625) lr 1.2790e-03 eta 0:00:32
epoch [84/200] batch [15/83] time 0.406 (0.445) data 0.275 (0.314) loss_u loss_u 0.9941 (0.9674) acc_u 0.0000 (3.7500) lr 1.2790e-03 eta 0:00:30
epoch [84/200] batch [20/83] time 0.434 (0.460) data 0.303 (0.329) loss_u loss_u 0.9697 (0.9681) acc_u 3.1250 (3.7500) lr 1.2790e-03 eta 0:00:28
epoch [84/200] batch [25/83] time 0.397 (0.457) data 0.266 (0.326) loss_u loss_u 0.9268 (0.9633) acc_u 9.3750 (4.5000) lr 1.2790e-03 eta 0:00:26
epoch [84/200] batch [30/83] time 0.601 (0.464) data 0.471 (0.333) loss_u loss_u 0.9341 (0.9620) acc_u 9.3750 (4.4792) lr 1.2790e-03 eta 0:00:24
epoch [84/200] batch [35/83] time 0.452 (0.460) data 0.321 (0.329) loss_u loss_u 0.9878 (0.9621) acc_u 0.0000 (4.4643) lr 1.2790e-03 eta 0:00:22
epoch [84/200] batch [40/83] time 0.371 (0.453) data 0.241 (0.323) loss_u loss_u 0.9277 (0.9610) acc_u 9.3750 (4.7656) lr 1.2790e-03 eta 0:00:19
epoch [84/200] batch [45/83] time 0.414 (0.454) data 0.284 (0.323) loss_u loss_u 0.9834 (0.9619) acc_u 3.1250 (4.6528) lr 1.2790e-03 eta 0:00:17
epoch [84/200] batch [50/83] time 0.457 (0.453) data 0.327 (0.322) loss_u loss_u 0.9653 (0.9619) acc_u 3.1250 (4.7500) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [55/83] time 0.528 (0.455) data 0.398 (0.324) loss_u loss_u 0.9663 (0.9627) acc_u 6.2500 (4.6591) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [60/83] time 0.432 (0.459) data 0.301 (0.328) loss_u loss_u 0.9829 (0.9628) acc_u 0.0000 (4.6875) lr 1.2790e-03 eta 0:00:10
epoch [84/200] batch [65/83] time 0.406 (0.456) data 0.276 (0.325) loss_u loss_u 0.9814 (0.9629) acc_u 0.0000 (4.6154) lr 1.2790e-03 eta 0:00:08
epoch [84/200] batch [70/83] time 0.525 (0.457) data 0.394 (0.326) loss_u loss_u 0.9604 (0.9635) acc_u 6.2500 (4.5089) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [75/83] time 0.481 (0.457) data 0.350 (0.326) loss_u loss_u 0.9658 (0.9623) acc_u 6.2500 (4.7917) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [80/83] time 0.482 (0.457) data 0.351 (0.326) loss_u loss_u 0.9194 (0.9616) acc_u 12.5000 (5.0000) lr 1.2790e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1811
confident_label rate tensor(0.1441, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 452
clean true:439
clean false:13
clean_rate:0.9712389380530974
noisy true:886
noisy false:1798
after delete: len(clean_dataset) 452
after delete: len(noisy_dataset) 2684
epoch [85/200] batch [5/14] time 0.452 (0.414) data 0.322 (0.283) loss_x loss_x 1.4668 (1.1928) acc_x 65.6250 (67.5000) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [10/14] time 0.437 (0.427) data 0.306 (0.296) loss_x loss_x 1.0283 (1.1453) acc_x 68.7500 (69.0625) lr 1.2639e-03 eta 0:00:01
epoch [85/200] batch [5/83] time 0.585 (0.469) data 0.453 (0.338) loss_u loss_u 0.9644 (0.9691) acc_u 3.1250 (3.7500) lr 1.2639e-03 eta 0:00:36
epoch [85/200] batch [10/83] time 0.441 (0.474) data 0.309 (0.343) loss_u loss_u 0.9302 (0.9700) acc_u 9.3750 (3.7500) lr 1.2639e-03 eta 0:00:34
epoch [85/200] batch [15/83] time 0.382 (0.468) data 0.249 (0.337) loss_u loss_u 0.9258 (0.9700) acc_u 12.5000 (3.9583) lr 1.2639e-03 eta 0:00:31
epoch [85/200] batch [20/83] time 0.330 (0.470) data 0.199 (0.338) loss_u loss_u 0.9497 (0.9661) acc_u 9.3750 (4.5312) lr 1.2639e-03 eta 0:00:29
epoch [85/200] batch [25/83] time 0.403 (0.466) data 0.271 (0.335) loss_u loss_u 0.9082 (0.9653) acc_u 12.5000 (4.6250) lr 1.2639e-03 eta 0:00:27
epoch [85/200] batch [30/83] time 0.469 (0.468) data 0.337 (0.337) loss_u loss_u 0.9980 (0.9672) acc_u 0.0000 (4.3750) lr 1.2639e-03 eta 0:00:24
epoch [85/200] batch [35/83] time 0.439 (0.465) data 0.307 (0.333) loss_u loss_u 0.9956 (0.9682) acc_u 0.0000 (4.2857) lr 1.2639e-03 eta 0:00:22
epoch [85/200] batch [40/83] time 0.342 (0.462) data 0.211 (0.331) loss_u loss_u 0.9077 (0.9661) acc_u 12.5000 (4.6094) lr 1.2639e-03 eta 0:00:19
epoch [85/200] batch [45/83] time 0.454 (0.458) data 0.324 (0.326) loss_u loss_u 0.9609 (0.9655) acc_u 6.2500 (4.6528) lr 1.2639e-03 eta 0:00:17
epoch [85/200] batch [50/83] time 0.365 (0.454) data 0.233 (0.323) loss_u loss_u 0.9209 (0.9635) acc_u 12.5000 (4.8125) lr 1.2639e-03 eta 0:00:14
epoch [85/200] batch [55/83] time 0.372 (0.451) data 0.242 (0.320) loss_u loss_u 0.9790 (0.9641) acc_u 6.2500 (4.8864) lr 1.2639e-03 eta 0:00:12
epoch [85/200] batch [60/83] time 0.555 (0.452) data 0.424 (0.321) loss_u loss_u 0.9717 (0.9634) acc_u 3.1250 (5.0000) lr 1.2639e-03 eta 0:00:10
epoch [85/200] batch [65/83] time 0.449 (0.453) data 0.319 (0.322) loss_u loss_u 0.9541 (0.9629) acc_u 9.3750 (5.0481) lr 1.2639e-03 eta 0:00:08
epoch [85/200] batch [70/83] time 0.448 (0.453) data 0.318 (0.321) loss_u loss_u 0.9351 (0.9617) acc_u 9.3750 (5.1339) lr 1.2639e-03 eta 0:00:05
epoch [85/200] batch [75/83] time 0.362 (0.452) data 0.230 (0.321) loss_u loss_u 0.9658 (0.9623) acc_u 3.1250 (5.0417) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [80/83] time 0.426 (0.451) data 0.294 (0.320) loss_u loss_u 0.9385 (0.9619) acc_u 6.2500 (5.0781) lr 1.2639e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.1486, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 466
clean true:454
clean false:12
clean_rate:0.9742489270386266
noisy true:933
noisy false:1737
after delete: len(clean_dataset) 466
after delete: len(noisy_dataset) 2670
epoch [86/200] batch [5/14] time 0.378 (0.452) data 0.247 (0.321) loss_x loss_x 1.3691 (1.3628) acc_x 71.8750 (68.7500) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [10/14] time 0.358 (0.440) data 0.227 (0.309) loss_x loss_x 1.3076 (1.3256) acc_x 68.7500 (69.6875) lr 1.2487e-03 eta 0:00:01
epoch [86/200] batch [5/83] time 0.413 (0.454) data 0.282 (0.323) loss_u loss_u 0.9531 (0.9539) acc_u 6.2500 (5.6250) lr 1.2487e-03 eta 0:00:35
epoch [86/200] batch [10/83] time 0.521 (0.453) data 0.389 (0.322) loss_u loss_u 0.9878 (0.9545) acc_u 0.0000 (5.9375) lr 1.2487e-03 eta 0:00:33
epoch [86/200] batch [15/83] time 0.486 (0.448) data 0.355 (0.317) loss_u loss_u 0.9351 (0.9563) acc_u 12.5000 (5.8333) lr 1.2487e-03 eta 0:00:30
epoch [86/200] batch [20/83] time 0.437 (0.443) data 0.306 (0.311) loss_u loss_u 0.9629 (0.9571) acc_u 6.2500 (5.9375) lr 1.2487e-03 eta 0:00:27
epoch [86/200] batch [25/83] time 0.406 (0.443) data 0.276 (0.312) loss_u loss_u 0.9688 (0.9594) acc_u 3.1250 (5.5000) lr 1.2487e-03 eta 0:00:25
epoch [86/200] batch [30/83] time 0.444 (0.442) data 0.313 (0.311) loss_u loss_u 0.9683 (0.9601) acc_u 3.1250 (5.2083) lr 1.2487e-03 eta 0:00:23
epoch [86/200] batch [35/83] time 0.508 (0.441) data 0.378 (0.310) loss_u loss_u 0.9829 (0.9616) acc_u 3.1250 (5.0000) lr 1.2487e-03 eta 0:00:21
epoch [86/200] batch [40/83] time 0.610 (0.445) data 0.478 (0.314) loss_u loss_u 0.9976 (0.9639) acc_u 0.0000 (4.6875) lr 1.2487e-03 eta 0:00:19
epoch [86/200] batch [45/83] time 0.416 (0.446) data 0.285 (0.315) loss_u loss_u 0.9644 (0.9643) acc_u 9.3750 (4.7917) lr 1.2487e-03 eta 0:00:16
epoch [86/200] batch [50/83] time 0.507 (0.448) data 0.375 (0.316) loss_u loss_u 0.9746 (0.9636) acc_u 3.1250 (4.8750) lr 1.2487e-03 eta 0:00:14
epoch [86/200] batch [55/83] time 0.410 (0.447) data 0.279 (0.315) loss_u loss_u 0.9395 (0.9630) acc_u 9.3750 (5.0000) lr 1.2487e-03 eta 0:00:12
epoch [86/200] batch [60/83] time 0.455 (0.447) data 0.324 (0.316) loss_u loss_u 0.9897 (0.9634) acc_u 0.0000 (4.9479) lr 1.2487e-03 eta 0:00:10
epoch [86/200] batch [65/83] time 0.380 (0.447) data 0.249 (0.316) loss_u loss_u 0.9443 (0.9631) acc_u 9.3750 (5.0000) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [70/83] time 0.566 (0.446) data 0.435 (0.315) loss_u loss_u 0.9614 (0.9635) acc_u 6.2500 (4.9107) lr 1.2487e-03 eta 0:00:05
epoch [86/200] batch [75/83] time 0.475 (0.445) data 0.344 (0.314) loss_u loss_u 0.9917 (0.9645) acc_u 3.1250 (4.7083) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [80/83] time 0.405 (0.451) data 0.274 (0.320) loss_u loss_u 0.9946 (0.9654) acc_u 0.0000 (4.5312) lr 1.2487e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1751
confident_label rate tensor(0.1435, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 450
clean true:435
clean false:15
clean_rate:0.9666666666666667
noisy true:950
noisy false:1736
after delete: len(clean_dataset) 450
after delete: len(noisy_dataset) 2686
epoch [87/200] batch [5/14] time 0.330 (0.438) data 0.200 (0.307) loss_x loss_x 2.0430 (1.2688) acc_x 53.1250 (70.6250) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [10/14] time 0.448 (0.464) data 0.318 (0.333) loss_x loss_x 1.2910 (1.2926) acc_x 71.8750 (70.6250) lr 1.2334e-03 eta 0:00:01
epoch [87/200] batch [5/83] time 0.453 (0.465) data 0.322 (0.335) loss_u loss_u 0.9282 (0.9567) acc_u 9.3750 (5.0000) lr 1.2334e-03 eta 0:00:36
epoch [87/200] batch [10/83] time 0.501 (0.459) data 0.370 (0.328) loss_u loss_u 0.9683 (0.9601) acc_u 3.1250 (4.6875) lr 1.2334e-03 eta 0:00:33
epoch [87/200] batch [15/83] time 0.451 (0.454) data 0.320 (0.324) loss_u loss_u 0.9849 (0.9651) acc_u 0.0000 (3.9583) lr 1.2334e-03 eta 0:00:30
epoch [87/200] batch [20/83] time 0.484 (0.453) data 0.352 (0.322) loss_u loss_u 0.9673 (0.9659) acc_u 3.1250 (3.9062) lr 1.2334e-03 eta 0:00:28
epoch [87/200] batch [25/83] time 0.570 (0.450) data 0.439 (0.320) loss_u loss_u 0.9658 (0.9651) acc_u 3.1250 (4.2500) lr 1.2334e-03 eta 0:00:26
epoch [87/200] batch [30/83] time 0.454 (0.449) data 0.323 (0.318) loss_u loss_u 0.9707 (0.9657) acc_u 3.1250 (4.1667) lr 1.2334e-03 eta 0:00:23
epoch [87/200] batch [35/83] time 0.455 (0.452) data 0.324 (0.321) loss_u loss_u 0.9146 (0.9647) acc_u 12.5000 (4.2857) lr 1.2334e-03 eta 0:00:21
epoch [87/200] batch [40/83] time 0.562 (0.452) data 0.429 (0.321) loss_u loss_u 0.9907 (0.9663) acc_u 3.1250 (4.1406) lr 1.2334e-03 eta 0:00:19
epoch [87/200] batch [45/83] time 0.464 (0.453) data 0.331 (0.322) loss_u loss_u 0.9990 (0.9672) acc_u 0.0000 (4.0972) lr 1.2334e-03 eta 0:00:17
epoch [87/200] batch [50/83] time 0.437 (0.451) data 0.306 (0.320) loss_u loss_u 0.9927 (0.9667) acc_u 0.0000 (4.2500) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [55/83] time 0.546 (0.456) data 0.414 (0.325) loss_u loss_u 0.9580 (0.9663) acc_u 3.1250 (4.4318) lr 1.2334e-03 eta 0:00:12
epoch [87/200] batch [60/83] time 0.551 (0.459) data 0.419 (0.328) loss_u loss_u 0.9302 (0.9656) acc_u 9.3750 (4.4792) lr 1.2334e-03 eta 0:00:10
epoch [87/200] batch [65/83] time 0.422 (0.459) data 0.291 (0.328) loss_u loss_u 0.9399 (0.9651) acc_u 6.2500 (4.4712) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [70/83] time 0.383 (0.461) data 0.250 (0.329) loss_u loss_u 0.9810 (0.9663) acc_u 3.1250 (4.2857) lr 1.2334e-03 eta 0:00:05
epoch [87/200] batch [75/83] time 0.427 (0.458) data 0.294 (0.327) loss_u loss_u 0.9941 (0.9673) acc_u 0.0000 (4.1667) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [80/83] time 0.491 (0.461) data 0.358 (0.330) loss_u loss_u 0.9697 (0.9672) acc_u 6.2500 (4.1797) lr 1.2334e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1786
confident_label rate tensor(0.1419, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 445
clean true:431
clean false:14
clean_rate:0.9685393258426966
noisy true:919
noisy false:1772
after delete: len(clean_dataset) 445
after delete: len(noisy_dataset) 2691
epoch [88/200] batch [5/13] time 0.444 (0.485) data 0.313 (0.355) loss_x loss_x 1.7285 (1.3223) acc_x 59.3750 (65.6250) lr 1.2181e-03 eta 0:00:03
epoch [88/200] batch [10/13] time 0.479 (0.468) data 0.350 (0.337) loss_x loss_x 1.1660 (1.2845) acc_x 75.0000 (68.7500) lr 1.2181e-03 eta 0:00:01
epoch [88/200] batch [5/84] time 0.536 (0.486) data 0.406 (0.356) loss_u loss_u 0.9927 (0.9526) acc_u 0.0000 (5.0000) lr 1.2181e-03 eta 0:00:38
epoch [88/200] batch [10/84] time 0.373 (0.467) data 0.241 (0.336) loss_u loss_u 0.9868 (0.9655) acc_u 3.1250 (4.3750) lr 1.2181e-03 eta 0:00:34
epoch [88/200] batch [15/84] time 0.369 (0.464) data 0.238 (0.333) loss_u loss_u 0.9697 (0.9640) acc_u 6.2500 (5.0000) lr 1.2181e-03 eta 0:00:32
epoch [88/200] batch [20/84] time 0.539 (0.454) data 0.409 (0.324) loss_u loss_u 0.9473 (0.9665) acc_u 9.3750 (4.6875) lr 1.2181e-03 eta 0:00:29
epoch [88/200] batch [25/84] time 0.529 (0.459) data 0.399 (0.328) loss_u loss_u 0.9741 (0.9668) acc_u 3.1250 (4.3750) lr 1.2181e-03 eta 0:00:27
epoch [88/200] batch [30/84] time 0.378 (0.458) data 0.247 (0.327) loss_u loss_u 0.9448 (0.9645) acc_u 6.2500 (4.6875) lr 1.2181e-03 eta 0:00:24
epoch [88/200] batch [35/84] time 0.427 (0.458) data 0.297 (0.327) loss_u loss_u 0.9917 (0.9653) acc_u 0.0000 (4.4643) lr 1.2181e-03 eta 0:00:22
epoch [88/200] batch [40/84] time 0.525 (0.457) data 0.395 (0.326) loss_u loss_u 0.9409 (0.9620) acc_u 9.3750 (5.0781) lr 1.2181e-03 eta 0:00:20
epoch [88/200] batch [45/84] time 0.346 (0.455) data 0.215 (0.324) loss_u loss_u 0.9902 (0.9614) acc_u 0.0000 (5.1389) lr 1.2181e-03 eta 0:00:17
epoch [88/200] batch [50/84] time 0.378 (0.452) data 0.248 (0.321) loss_u loss_u 0.9692 (0.9606) acc_u 3.1250 (5.1250) lr 1.2181e-03 eta 0:00:15
epoch [88/200] batch [55/84] time 0.392 (0.448) data 0.261 (0.318) loss_u loss_u 0.9678 (0.9621) acc_u 6.2500 (4.9432) lr 1.2181e-03 eta 0:00:13
epoch [88/200] batch [60/84] time 0.371 (0.453) data 0.241 (0.322) loss_u loss_u 0.9717 (0.9626) acc_u 3.1250 (4.8958) lr 1.2181e-03 eta 0:00:10
epoch [88/200] batch [65/84] time 0.474 (0.453) data 0.343 (0.322) loss_u loss_u 0.9746 (0.9618) acc_u 0.0000 (4.8558) lr 1.2181e-03 eta 0:00:08
epoch [88/200] batch [70/84] time 0.397 (0.450) data 0.266 (0.319) loss_u loss_u 0.9058 (0.9609) acc_u 15.6250 (4.9554) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [75/84] time 0.486 (0.448) data 0.355 (0.317) loss_u loss_u 0.9478 (0.9613) acc_u 3.1250 (4.9167) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [80/84] time 0.498 (0.451) data 0.367 (0.320) loss_u loss_u 0.9424 (0.9613) acc_u 6.2500 (4.8438) lr 1.2181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1738
confident_label rate tensor(0.1492, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 468
clean true:452
clean false:16
clean_rate:0.9658119658119658
noisy true:946
noisy false:1722
after delete: len(clean_dataset) 468
after delete: len(noisy_dataset) 2668
epoch [89/200] batch [5/14] time 0.720 (0.508) data 0.589 (0.378) loss_x loss_x 1.0684 (1.2448) acc_x 75.0000 (70.0000) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [10/14] time 0.378 (0.470) data 0.248 (0.340) loss_x loss_x 1.3145 (1.3859) acc_x 65.6250 (66.5625) lr 1.2028e-03 eta 0:00:01
epoch [89/200] batch [5/83] time 0.370 (0.472) data 0.239 (0.342) loss_u loss_u 0.9800 (0.9690) acc_u 6.2500 (5.0000) lr 1.2028e-03 eta 0:00:36
epoch [89/200] batch [10/83] time 0.388 (0.459) data 0.258 (0.329) loss_u loss_u 0.9497 (0.9658) acc_u 6.2500 (5.3125) lr 1.2028e-03 eta 0:00:33
epoch [89/200] batch [15/83] time 0.402 (0.460) data 0.270 (0.329) loss_u loss_u 0.9580 (0.9664) acc_u 6.2500 (5.2083) lr 1.2028e-03 eta 0:00:31
epoch [89/200] batch [20/83] time 0.426 (0.453) data 0.296 (0.322) loss_u loss_u 0.9141 (0.9672) acc_u 9.3750 (4.6875) lr 1.2028e-03 eta 0:00:28
epoch [89/200] batch [25/83] time 0.523 (0.457) data 0.393 (0.327) loss_u loss_u 0.9468 (0.9693) acc_u 9.3750 (4.2500) lr 1.2028e-03 eta 0:00:26
epoch [89/200] batch [30/83] time 0.398 (0.453) data 0.268 (0.322) loss_u loss_u 0.9814 (0.9672) acc_u 3.1250 (4.3750) lr 1.2028e-03 eta 0:00:23
epoch [89/200] batch [35/83] time 0.502 (0.450) data 0.372 (0.320) loss_u loss_u 0.9897 (0.9657) acc_u 0.0000 (4.5536) lr 1.2028e-03 eta 0:00:21
epoch [89/200] batch [40/83] time 0.399 (0.456) data 0.268 (0.325) loss_u loss_u 0.9648 (0.9656) acc_u 3.1250 (4.5312) lr 1.2028e-03 eta 0:00:19
epoch [89/200] batch [45/83] time 0.350 (0.452) data 0.220 (0.321) loss_u loss_u 0.9312 (0.9645) acc_u 12.5000 (4.6528) lr 1.2028e-03 eta 0:00:17
epoch [89/200] batch [50/83] time 0.445 (0.451) data 0.314 (0.320) loss_u loss_u 0.9048 (0.9642) acc_u 15.6250 (4.6875) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [55/83] time 0.414 (0.452) data 0.283 (0.321) loss_u loss_u 0.9707 (0.9655) acc_u 3.1250 (4.7159) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [60/83] time 0.397 (0.448) data 0.266 (0.318) loss_u loss_u 0.9951 (0.9662) acc_u 0.0000 (4.6354) lr 1.2028e-03 eta 0:00:10
epoch [89/200] batch [65/83] time 0.431 (0.447) data 0.298 (0.316) loss_u loss_u 0.9653 (0.9656) acc_u 3.1250 (4.7115) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [70/83] time 0.429 (0.450) data 0.298 (0.319) loss_u loss_u 0.9673 (0.9657) acc_u 6.2500 (4.6875) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [75/83] time 0.404 (0.451) data 0.271 (0.320) loss_u loss_u 0.9810 (0.9657) acc_u 3.1250 (4.6250) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [80/83] time 0.571 (0.454) data 0.439 (0.323) loss_u loss_u 0.9902 (0.9660) acc_u 0.0000 (4.6094) lr 1.2028e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1797
confident_label rate tensor(0.1378, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 432
clean true:420
clean false:12
clean_rate:0.9722222222222222
noisy true:919
noisy false:1785
after delete: len(clean_dataset) 432
after delete: len(noisy_dataset) 2704
epoch [90/200] batch [5/13] time 0.418 (0.447) data 0.289 (0.317) loss_x loss_x 1.0088 (1.2641) acc_x 78.1250 (71.2500) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [10/13] time 0.442 (0.438) data 0.311 (0.308) loss_x loss_x 0.7266 (1.1115) acc_x 87.5000 (75.0000) lr 1.1874e-03 eta 0:00:01
epoch [90/200] batch [5/84] time 0.483 (0.443) data 0.351 (0.312) loss_u loss_u 0.9849 (0.9518) acc_u 3.1250 (7.5000) lr 1.1874e-03 eta 0:00:34
epoch [90/200] batch [10/84] time 0.433 (0.434) data 0.301 (0.303) loss_u loss_u 0.9077 (0.9508) acc_u 15.6250 (6.5625) lr 1.1874e-03 eta 0:00:32
epoch [90/200] batch [15/84] time 0.443 (0.431) data 0.312 (0.300) loss_u loss_u 0.9956 (0.9602) acc_u 0.0000 (5.2083) lr 1.1874e-03 eta 0:00:29
epoch [90/200] batch [20/84] time 0.368 (0.425) data 0.238 (0.295) loss_u loss_u 0.9658 (0.9660) acc_u 6.2500 (4.6875) lr 1.1874e-03 eta 0:00:27
epoch [90/200] batch [25/84] time 0.372 (0.425) data 0.240 (0.294) loss_u loss_u 0.9619 (0.9610) acc_u 3.1250 (5.5000) lr 1.1874e-03 eta 0:00:25
epoch [90/200] batch [30/84] time 0.395 (0.431) data 0.263 (0.300) loss_u loss_u 0.9370 (0.9616) acc_u 12.5000 (5.4167) lr 1.1874e-03 eta 0:00:23
epoch [90/200] batch [35/84] time 0.532 (0.432) data 0.400 (0.301) loss_u loss_u 0.9448 (0.9613) acc_u 6.2500 (5.4464) lr 1.1874e-03 eta 0:00:21
epoch [90/200] batch [40/84] time 0.442 (0.436) data 0.310 (0.305) loss_u loss_u 0.9902 (0.9603) acc_u 0.0000 (5.5469) lr 1.1874e-03 eta 0:00:19
epoch [90/200] batch [45/84] time 0.577 (0.440) data 0.446 (0.309) loss_u loss_u 0.9917 (0.9612) acc_u 0.0000 (5.3472) lr 1.1874e-03 eta 0:00:17
epoch [90/200] batch [50/84] time 0.460 (0.441) data 0.329 (0.310) loss_u loss_u 0.9341 (0.9585) acc_u 9.3750 (5.7500) lr 1.1874e-03 eta 0:00:14
epoch [90/200] batch [55/84] time 0.478 (0.443) data 0.347 (0.312) loss_u loss_u 0.9502 (0.9606) acc_u 6.2500 (5.4545) lr 1.1874e-03 eta 0:00:12
epoch [90/200] batch [60/84] time 0.357 (0.443) data 0.226 (0.312) loss_u loss_u 0.9307 (0.9597) acc_u 12.5000 (5.6250) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [65/84] time 0.473 (0.445) data 0.341 (0.314) loss_u loss_u 0.9717 (0.9589) acc_u 3.1250 (5.6250) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [70/84] time 0.427 (0.445) data 0.296 (0.314) loss_u loss_u 0.9990 (0.9603) acc_u 0.0000 (5.4018) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [75/84] time 0.454 (0.447) data 0.322 (0.316) loss_u loss_u 0.9380 (0.9596) acc_u 6.2500 (5.5417) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [80/84] time 0.490 (0.451) data 0.359 (0.320) loss_u loss_u 0.9990 (0.9586) acc_u 0.0000 (5.6641) lr 1.1874e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1798
confident_label rate tensor(0.1406, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 441
clean true:429
clean false:12
clean_rate:0.9727891156462585
noisy true:909
noisy false:1786
after delete: len(clean_dataset) 441
after delete: len(noisy_dataset) 2695
epoch [91/200] batch [5/13] time 0.520 (0.506) data 0.390 (0.376) loss_x loss_x 1.0957 (0.9975) acc_x 68.7500 (75.6250) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [10/13] time 0.526 (0.499) data 0.395 (0.369) loss_x loss_x 0.9038 (1.0593) acc_x 84.3750 (76.2500) lr 1.1719e-03 eta 0:00:01
epoch [91/200] batch [5/84] time 0.483 (0.527) data 0.349 (0.396) loss_u loss_u 0.8999 (0.9647) acc_u 12.5000 (3.1250) lr 1.1719e-03 eta 0:00:41
epoch [91/200] batch [10/84] time 0.483 (0.517) data 0.352 (0.386) loss_u loss_u 0.9805 (0.9678) acc_u 3.1250 (3.4375) lr 1.1719e-03 eta 0:00:38
epoch [91/200] batch [15/84] time 0.482 (0.510) data 0.352 (0.379) loss_u loss_u 0.9658 (0.9647) acc_u 3.1250 (3.5417) lr 1.1719e-03 eta 0:00:35
epoch [91/200] batch [20/84] time 0.428 (0.511) data 0.296 (0.380) loss_u loss_u 0.9976 (0.9661) acc_u 0.0000 (3.5938) lr 1.1719e-03 eta 0:00:32
epoch [91/200] batch [25/84] time 0.384 (0.500) data 0.253 (0.369) loss_u loss_u 0.9355 (0.9633) acc_u 6.2500 (4.2500) lr 1.1719e-03 eta 0:00:29
epoch [91/200] batch [30/84] time 0.707 (0.498) data 0.575 (0.367) loss_u loss_u 0.8892 (0.9571) acc_u 12.5000 (5.1042) lr 1.1719e-03 eta 0:00:26
epoch [91/200] batch [35/84] time 0.407 (0.486) data 0.275 (0.355) loss_u loss_u 0.9668 (0.9593) acc_u 3.1250 (4.9107) lr 1.1719e-03 eta 0:00:23
epoch [91/200] batch [40/84] time 0.446 (0.483) data 0.316 (0.351) loss_u loss_u 0.9404 (0.9594) acc_u 6.2500 (4.8438) lr 1.1719e-03 eta 0:00:21
epoch [91/200] batch [45/84] time 0.445 (0.482) data 0.314 (0.351) loss_u loss_u 0.9775 (0.9596) acc_u 3.1250 (4.9306) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [50/84] time 0.370 (0.477) data 0.239 (0.346) loss_u loss_u 0.9512 (0.9582) acc_u 6.2500 (5.0625) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [55/84] time 0.388 (0.477) data 0.257 (0.346) loss_u loss_u 0.9946 (0.9587) acc_u 0.0000 (5.1136) lr 1.1719e-03 eta 0:00:13
epoch [91/200] batch [60/84] time 0.444 (0.475) data 0.313 (0.344) loss_u loss_u 0.9912 (0.9590) acc_u 0.0000 (5.1042) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [65/84] time 0.394 (0.471) data 0.261 (0.340) loss_u loss_u 0.9736 (0.9581) acc_u 3.1250 (5.2404) lr 1.1719e-03 eta 0:00:08
epoch [91/200] batch [70/84] time 0.511 (0.470) data 0.380 (0.338) loss_u loss_u 0.9502 (0.9586) acc_u 9.3750 (5.2679) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [75/84] time 0.366 (0.468) data 0.235 (0.337) loss_u loss_u 0.9727 (0.9600) acc_u 3.1250 (5.0833) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [80/84] time 0.364 (0.469) data 0.233 (0.337) loss_u loss_u 0.9541 (0.9597) acc_u 9.3750 (5.1953) lr 1.1719e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1765
confident_label rate tensor(0.1527, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 479
clean true:461
clean false:18
clean_rate:0.9624217118997912
noisy true:910
noisy false:1747
after delete: len(clean_dataset) 479
after delete: len(noisy_dataset) 2657
epoch [92/200] batch [5/14] time 0.568 (0.499) data 0.438 (0.369) loss_x loss_x 1.2656 (1.1393) acc_x 65.6250 (73.7500) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [10/14] time 0.437 (0.468) data 0.306 (0.337) loss_x loss_x 1.0918 (1.1066) acc_x 84.3750 (75.9375) lr 1.1564e-03 eta 0:00:01
epoch [92/200] batch [5/83] time 0.508 (0.449) data 0.377 (0.318) loss_u loss_u 0.9868 (0.9677) acc_u 3.1250 (6.2500) lr 1.1564e-03 eta 0:00:35
epoch [92/200] batch [10/83] time 0.413 (0.453) data 0.282 (0.322) loss_u loss_u 0.9756 (0.9659) acc_u 3.1250 (5.9375) lr 1.1564e-03 eta 0:00:33
epoch [92/200] batch [15/83] time 0.435 (0.449) data 0.304 (0.318) loss_u loss_u 0.9937 (0.9722) acc_u 0.0000 (4.3750) lr 1.1564e-03 eta 0:00:30
epoch [92/200] batch [20/83] time 0.464 (0.447) data 0.333 (0.316) loss_u loss_u 0.9873 (0.9749) acc_u 0.0000 (3.9062) lr 1.1564e-03 eta 0:00:28
epoch [92/200] batch [25/83] time 0.384 (0.443) data 0.254 (0.312) loss_u loss_u 0.9536 (0.9699) acc_u 6.2500 (4.3750) lr 1.1564e-03 eta 0:00:25
epoch [92/200] batch [30/83] time 0.452 (0.447) data 0.321 (0.316) loss_u loss_u 0.9771 (0.9695) acc_u 3.1250 (4.3750) lr 1.1564e-03 eta 0:00:23
epoch [92/200] batch [35/83] time 0.475 (0.448) data 0.344 (0.317) loss_u loss_u 0.9849 (0.9688) acc_u 0.0000 (4.2857) lr 1.1564e-03 eta 0:00:21
epoch [92/200] batch [40/83] time 0.457 (0.449) data 0.324 (0.318) loss_u loss_u 0.9609 (0.9688) acc_u 6.2500 (4.2188) lr 1.1564e-03 eta 0:00:19
epoch [92/200] batch [45/83] time 0.465 (0.448) data 0.335 (0.317) loss_u loss_u 0.9624 (0.9664) acc_u 6.2500 (4.4444) lr 1.1564e-03 eta 0:00:17
epoch [92/200] batch [50/83] time 0.404 (0.456) data 0.272 (0.325) loss_u loss_u 0.9990 (0.9654) acc_u 0.0000 (4.5000) lr 1.1564e-03 eta 0:00:15
epoch [92/200] batch [55/83] time 0.577 (0.459) data 0.445 (0.328) loss_u loss_u 0.9971 (0.9655) acc_u 0.0000 (4.4886) lr 1.1564e-03 eta 0:00:12
epoch [92/200] batch [60/83] time 0.461 (0.458) data 0.330 (0.327) loss_u loss_u 0.9790 (0.9653) acc_u 3.1250 (4.4792) lr 1.1564e-03 eta 0:00:10
epoch [92/200] batch [65/83] time 0.564 (0.460) data 0.432 (0.329) loss_u loss_u 0.9648 (0.9662) acc_u 3.1250 (4.3750) lr 1.1564e-03 eta 0:00:08
epoch [92/200] batch [70/83] time 0.418 (0.460) data 0.287 (0.329) loss_u loss_u 0.9336 (0.9651) acc_u 9.3750 (4.5536) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [75/83] time 0.519 (0.460) data 0.387 (0.329) loss_u loss_u 0.9385 (0.9645) acc_u 9.3750 (4.5833) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [80/83] time 0.462 (0.459) data 0.331 (0.328) loss_u loss_u 0.9731 (0.9646) acc_u 3.1250 (4.5703) lr 1.1564e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1797
confident_label rate tensor(0.1435, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 450
clean true:434
clean false:16
clean_rate:0.9644444444444444
noisy true:905
noisy false:1781
after delete: len(clean_dataset) 450
after delete: len(noisy_dataset) 2686
epoch [93/200] batch [5/14] time 0.524 (0.505) data 0.394 (0.374) loss_x loss_x 1.1348 (1.2840) acc_x 68.7500 (68.7500) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [10/14] time 0.339 (0.467) data 0.207 (0.336) loss_x loss_x 1.3887 (1.2217) acc_x 65.6250 (69.3750) lr 1.1409e-03 eta 0:00:01
epoch [93/200] batch [5/83] time 0.608 (0.462) data 0.476 (0.330) loss_u loss_u 0.8423 (0.9230) acc_u 25.0000 (11.2500) lr 1.1409e-03 eta 0:00:36
epoch [93/200] batch [10/83] time 0.405 (0.461) data 0.273 (0.330) loss_u loss_u 0.9956 (0.9422) acc_u 0.0000 (7.8125) lr 1.1409e-03 eta 0:00:33
epoch [93/200] batch [15/83] time 0.533 (0.460) data 0.402 (0.328) loss_u loss_u 0.9966 (0.9537) acc_u 0.0000 (6.2500) lr 1.1409e-03 eta 0:00:31
epoch [93/200] batch [20/83] time 0.469 (0.461) data 0.339 (0.329) loss_u loss_u 0.9604 (0.9592) acc_u 3.1250 (5.4688) lr 1.1409e-03 eta 0:00:29
epoch [93/200] batch [25/83] time 0.405 (0.460) data 0.274 (0.329) loss_u loss_u 0.9575 (0.9574) acc_u 6.2500 (6.0000) lr 1.1409e-03 eta 0:00:26
epoch [93/200] batch [30/83] time 0.526 (0.458) data 0.396 (0.327) loss_u loss_u 0.9204 (0.9599) acc_u 6.2500 (5.4167) lr 1.1409e-03 eta 0:00:24
epoch [93/200] batch [35/83] time 0.377 (0.460) data 0.246 (0.329) loss_u loss_u 0.9561 (0.9607) acc_u 3.1250 (5.1786) lr 1.1409e-03 eta 0:00:22
epoch [93/200] batch [40/83] time 0.472 (0.459) data 0.342 (0.328) loss_u loss_u 0.9966 (0.9615) acc_u 0.0000 (5.1562) lr 1.1409e-03 eta 0:00:19
epoch [93/200] batch [45/83] time 0.416 (0.454) data 0.284 (0.323) loss_u loss_u 0.9717 (0.9621) acc_u 3.1250 (5.0694) lr 1.1409e-03 eta 0:00:17
epoch [93/200] batch [50/83] time 0.342 (0.455) data 0.211 (0.323) loss_u loss_u 0.9424 (0.9596) acc_u 6.2500 (5.3125) lr 1.1409e-03 eta 0:00:15
epoch [93/200] batch [55/83] time 0.432 (0.455) data 0.302 (0.324) loss_u loss_u 0.9609 (0.9588) acc_u 3.1250 (5.2841) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [60/83] time 0.457 (0.460) data 0.326 (0.329) loss_u loss_u 0.9341 (0.9590) acc_u 6.2500 (5.2083) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [65/83] time 0.633 (0.463) data 0.502 (0.332) loss_u loss_u 0.8926 (0.9573) acc_u 15.6250 (5.3846) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [70/83] time 0.355 (0.465) data 0.224 (0.334) loss_u loss_u 0.9639 (0.9588) acc_u 9.3750 (5.2232) lr 1.1409e-03 eta 0:00:06
epoch [93/200] batch [75/83] time 0.562 (0.464) data 0.431 (0.333) loss_u loss_u 0.9551 (0.9595) acc_u 9.3750 (5.1667) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [80/83] time 0.448 (0.464) data 0.316 (0.333) loss_u loss_u 0.9551 (0.9602) acc_u 6.2500 (5.1172) lr 1.1409e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1722
confident_label rate tensor(0.1467, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 460
clean true:447
clean false:13
clean_rate:0.9717391304347827
noisy true:967
noisy false:1709
after delete: len(clean_dataset) 460
after delete: len(noisy_dataset) 2676
epoch [94/200] batch [5/14] time 0.467 (0.502) data 0.337 (0.371) loss_x loss_x 0.9844 (1.3240) acc_x 75.0000 (67.5000) lr 1.1253e-03 eta 0:00:04
epoch [94/200] batch [10/14] time 0.386 (0.486) data 0.256 (0.355) loss_x loss_x 1.4980 (1.2966) acc_x 65.6250 (68.4375) lr 1.1253e-03 eta 0:00:01
epoch [94/200] batch [5/83] time 0.444 (0.461) data 0.314 (0.331) loss_u loss_u 0.9570 (0.9629) acc_u 6.2500 (5.6250) lr 1.1253e-03 eta 0:00:35
epoch [94/200] batch [10/83] time 0.437 (0.466) data 0.306 (0.335) loss_u loss_u 0.9756 (0.9560) acc_u 6.2500 (6.5625) lr 1.1253e-03 eta 0:00:34
epoch [94/200] batch [15/83] time 0.488 (0.473) data 0.356 (0.343) loss_u loss_u 0.9580 (0.9606) acc_u 3.1250 (5.8333) lr 1.1253e-03 eta 0:00:32
epoch [94/200] batch [20/83] time 0.696 (0.480) data 0.564 (0.349) loss_u loss_u 0.9609 (0.9630) acc_u 6.2500 (5.3125) lr 1.1253e-03 eta 0:00:30
epoch [94/200] batch [25/83] time 0.390 (0.466) data 0.259 (0.336) loss_u loss_u 0.9736 (0.9660) acc_u 3.1250 (5.0000) lr 1.1253e-03 eta 0:00:27
epoch [94/200] batch [30/83] time 0.462 (0.462) data 0.331 (0.331) loss_u loss_u 0.9595 (0.9653) acc_u 6.2500 (5.1042) lr 1.1253e-03 eta 0:00:24
epoch [94/200] batch [35/83] time 0.543 (0.469) data 0.412 (0.338) loss_u loss_u 0.9976 (0.9643) acc_u 0.0000 (5.3571) lr 1.1253e-03 eta 0:00:22
epoch [94/200] batch [40/83] time 0.412 (0.469) data 0.282 (0.338) loss_u loss_u 0.9561 (0.9643) acc_u 6.2500 (5.4688) lr 1.1253e-03 eta 0:00:20
epoch [94/200] batch [45/83] time 0.434 (0.463) data 0.303 (0.332) loss_u loss_u 0.9575 (0.9644) acc_u 3.1250 (5.3472) lr 1.1253e-03 eta 0:00:17
epoch [94/200] batch [50/83] time 0.474 (0.460) data 0.343 (0.329) loss_u loss_u 0.9463 (0.9636) acc_u 6.2500 (5.5000) lr 1.1253e-03 eta 0:00:15
epoch [94/200] batch [55/83] time 0.538 (0.459) data 0.407 (0.328) loss_u loss_u 0.9473 (0.9638) acc_u 6.2500 (5.3977) lr 1.1253e-03 eta 0:00:12
epoch [94/200] batch [60/83] time 0.469 (0.457) data 0.338 (0.326) loss_u loss_u 0.9614 (0.9636) acc_u 3.1250 (5.3646) lr 1.1253e-03 eta 0:00:10
epoch [94/200] batch [65/83] time 0.367 (0.452) data 0.236 (0.321) loss_u loss_u 0.9092 (0.9626) acc_u 15.6250 (5.5769) lr 1.1253e-03 eta 0:00:08
epoch [94/200] batch [70/83] time 0.396 (0.451) data 0.265 (0.321) loss_u loss_u 0.9272 (0.9613) acc_u 12.5000 (5.8036) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [75/83] time 0.814 (0.456) data 0.681 (0.325) loss_u loss_u 0.9746 (0.9599) acc_u 3.1250 (5.8750) lr 1.1253e-03 eta 0:00:03
epoch [94/200] batch [80/83] time 0.471 (0.455) data 0.340 (0.325) loss_u loss_u 0.9683 (0.9610) acc_u 3.1250 (5.6250) lr 1.1253e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1703
confident_label rate tensor(0.1540, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 483
clean true:471
clean false:12
clean_rate:0.9751552795031055
noisy true:962
noisy false:1691
after delete: len(clean_dataset) 483
after delete: len(noisy_dataset) 2653
epoch [95/200] batch [5/15] time 0.611 (0.501) data 0.480 (0.370) loss_x loss_x 0.9805 (1.1237) acc_x 75.0000 (73.1250) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [10/15] time 0.547 (0.494) data 0.417 (0.363) loss_x loss_x 0.8696 (1.1017) acc_x 78.1250 (74.3750) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [15/15] time 0.423 (0.468) data 0.293 (0.337) loss_x loss_x 1.1230 (1.1738) acc_x 59.3750 (71.4583) lr 1.1097e-03 eta 0:00:00
epoch [95/200] batch [5/82] time 0.579 (0.468) data 0.448 (0.337) loss_u loss_u 0.9683 (0.9706) acc_u 6.2500 (5.0000) lr 1.1097e-03 eta 0:00:36
epoch [95/200] batch [10/82] time 0.772 (0.473) data 0.641 (0.342) loss_u loss_u 0.9487 (0.9725) acc_u 6.2500 (4.0625) lr 1.1097e-03 eta 0:00:34
epoch [95/200] batch [15/82] time 0.370 (0.471) data 0.239 (0.341) loss_u loss_u 0.9668 (0.9684) acc_u 3.1250 (4.3750) lr 1.1097e-03 eta 0:00:31
epoch [95/200] batch [20/82] time 0.428 (0.468) data 0.297 (0.337) loss_u loss_u 0.9736 (0.9689) acc_u 3.1250 (4.2188) lr 1.1097e-03 eta 0:00:28
epoch [95/200] batch [25/82] time 0.498 (0.464) data 0.367 (0.333) loss_u loss_u 0.9795 (0.9710) acc_u 3.1250 (4.0000) lr 1.1097e-03 eta 0:00:26
epoch [95/200] batch [30/82] time 0.476 (0.463) data 0.346 (0.332) loss_u loss_u 0.9219 (0.9679) acc_u 12.5000 (4.4792) lr 1.1097e-03 eta 0:00:24
epoch [95/200] batch [35/82] time 0.379 (0.460) data 0.249 (0.329) loss_u loss_u 0.9761 (0.9699) acc_u 3.1250 (4.1071) lr 1.1097e-03 eta 0:00:21
epoch [95/200] batch [40/82] time 0.640 (0.461) data 0.509 (0.331) loss_u loss_u 0.9321 (0.9680) acc_u 9.3750 (4.2969) lr 1.1097e-03 eta 0:00:19
epoch [95/200] batch [45/82] time 0.618 (0.460) data 0.487 (0.329) loss_u loss_u 0.9985 (0.9690) acc_u 0.0000 (4.1667) lr 1.1097e-03 eta 0:00:17
epoch [95/200] batch [50/82] time 0.409 (0.456) data 0.279 (0.325) loss_u loss_u 0.9648 (0.9690) acc_u 3.1250 (4.1875) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [55/82] time 0.399 (0.456) data 0.268 (0.326) loss_u loss_u 0.9385 (0.9673) acc_u 6.2500 (4.3182) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [60/82] time 0.545 (0.456) data 0.413 (0.325) loss_u loss_u 0.9810 (0.9671) acc_u 3.1250 (4.3750) lr 1.1097e-03 eta 0:00:10
epoch [95/200] batch [65/82] time 0.457 (0.455) data 0.326 (0.324) loss_u loss_u 0.9883 (0.9675) acc_u 3.1250 (4.2788) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [70/82] time 0.497 (0.453) data 0.366 (0.322) loss_u loss_u 0.9849 (0.9668) acc_u 3.1250 (4.4196) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [75/82] time 0.450 (0.453) data 0.319 (0.322) loss_u loss_u 0.9990 (0.9670) acc_u 0.0000 (4.3750) lr 1.1097e-03 eta 0:00:03
epoch [95/200] batch [80/82] time 0.579 (0.453) data 0.448 (0.322) loss_u loss_u 0.9702 (0.9667) acc_u 3.1250 (4.4141) lr 1.1097e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1754
confident_label rate tensor(0.1390, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 436
clean true:423
clean false:13
clean_rate:0.9701834862385321
noisy true:959
noisy false:1741
after delete: len(clean_dataset) 436
after delete: len(noisy_dataset) 2700
epoch [96/200] batch [5/13] time 0.437 (0.453) data 0.307 (0.323) loss_x loss_x 1.3857 (1.4391) acc_x 62.5000 (62.5000) lr 1.0941e-03 eta 0:00:03
epoch [96/200] batch [10/13] time 0.510 (0.462) data 0.379 (0.331) loss_x loss_x 1.0781 (1.3556) acc_x 68.7500 (66.8750) lr 1.0941e-03 eta 0:00:01
epoch [96/200] batch [5/84] time 0.485 (0.461) data 0.355 (0.331) loss_u loss_u 0.9565 (0.9608) acc_u 6.2500 (5.6250) lr 1.0941e-03 eta 0:00:36
epoch [96/200] batch [10/84] time 0.426 (0.453) data 0.294 (0.322) loss_u loss_u 0.9482 (0.9540) acc_u 9.3750 (6.5625) lr 1.0941e-03 eta 0:00:33
epoch [96/200] batch [15/84] time 0.585 (0.454) data 0.454 (0.323) loss_u loss_u 0.9521 (0.9500) acc_u 3.1250 (6.4583) lr 1.0941e-03 eta 0:00:31
epoch [96/200] batch [20/84] time 0.467 (0.448) data 0.337 (0.317) loss_u loss_u 0.9795 (0.9527) acc_u 6.2500 (6.4062) lr 1.0941e-03 eta 0:00:28
epoch [96/200] batch [25/84] time 0.395 (0.443) data 0.265 (0.312) loss_u loss_u 0.9102 (0.9542) acc_u 15.6250 (6.1250) lr 1.0941e-03 eta 0:00:26
epoch [96/200] batch [30/84] time 0.476 (0.440) data 0.345 (0.310) loss_u loss_u 0.9780 (0.9553) acc_u 3.1250 (5.9375) lr 1.0941e-03 eta 0:00:23
epoch [96/200] batch [35/84] time 0.355 (0.438) data 0.225 (0.307) loss_u loss_u 0.9927 (0.9579) acc_u 0.0000 (5.6250) lr 1.0941e-03 eta 0:00:21
epoch [96/200] batch [40/84] time 0.420 (0.443) data 0.289 (0.312) loss_u loss_u 0.9570 (0.9572) acc_u 6.2500 (5.7812) lr 1.0941e-03 eta 0:00:19
epoch [96/200] batch [45/84] time 0.341 (0.443) data 0.209 (0.313) loss_u loss_u 0.9624 (0.9585) acc_u 3.1250 (5.5556) lr 1.0941e-03 eta 0:00:17
epoch [96/200] batch [50/84] time 0.429 (0.448) data 0.299 (0.318) loss_u loss_u 0.9478 (0.9587) acc_u 6.2500 (5.5000) lr 1.0941e-03 eta 0:00:15
epoch [96/200] batch [55/84] time 0.450 (0.448) data 0.320 (0.317) loss_u loss_u 0.9722 (0.9587) acc_u 3.1250 (5.2841) lr 1.0941e-03 eta 0:00:12
epoch [96/200] batch [60/84] time 0.421 (0.448) data 0.290 (0.317) loss_u loss_u 0.9497 (0.9597) acc_u 6.2500 (5.2083) lr 1.0941e-03 eta 0:00:10
epoch [96/200] batch [65/84] time 0.371 (0.446) data 0.240 (0.315) loss_u loss_u 0.9609 (0.9603) acc_u 6.2500 (5.1442) lr 1.0941e-03 eta 0:00:08
epoch [96/200] batch [70/84] time 0.386 (0.443) data 0.254 (0.312) loss_u loss_u 0.9365 (0.9599) acc_u 9.3750 (5.1786) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [75/84] time 0.562 (0.447) data 0.431 (0.317) loss_u loss_u 0.9253 (0.9599) acc_u 9.3750 (5.1250) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [80/84] time 0.419 (0.447) data 0.287 (0.316) loss_u loss_u 0.9658 (0.9606) acc_u 6.2500 (5.0000) lr 1.0941e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1773
confident_label rate tensor(0.1441, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 452
clean true:438
clean false:14
clean_rate:0.9690265486725663
noisy true:925
noisy false:1759
after delete: len(clean_dataset) 452
after delete: len(noisy_dataset) 2684
epoch [97/200] batch [5/14] time 0.430 (0.460) data 0.299 (0.330) loss_x loss_x 0.9160 (1.2403) acc_x 81.2500 (72.5000) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [10/14] time 0.508 (0.446) data 0.377 (0.316) loss_x loss_x 0.9697 (1.1763) acc_x 62.5000 (70.9375) lr 1.0785e-03 eta 0:00:01
epoch [97/200] batch [5/83] time 0.530 (0.459) data 0.399 (0.329) loss_u loss_u 0.9839 (0.9860) acc_u 3.1250 (1.8750) lr 1.0785e-03 eta 0:00:35
epoch [97/200] batch [10/83] time 0.563 (0.465) data 0.432 (0.335) loss_u loss_u 0.9536 (0.9706) acc_u 9.3750 (5.3125) lr 1.0785e-03 eta 0:00:33
epoch [97/200] batch [15/83] time 0.377 (0.457) data 0.247 (0.327) loss_u loss_u 0.9648 (0.9646) acc_u 3.1250 (5.6250) lr 1.0785e-03 eta 0:00:31
epoch [97/200] batch [20/83] time 0.414 (0.461) data 0.283 (0.330) loss_u loss_u 0.9355 (0.9605) acc_u 6.2500 (5.9375) lr 1.0785e-03 eta 0:00:29
epoch [97/200] batch [25/83] time 0.400 (0.462) data 0.269 (0.331) loss_u loss_u 0.9570 (0.9628) acc_u 6.2500 (5.5000) lr 1.0785e-03 eta 0:00:26
epoch [97/200] batch [30/83] time 0.459 (0.459) data 0.327 (0.328) loss_u loss_u 0.9854 (0.9647) acc_u 6.2500 (5.3125) lr 1.0785e-03 eta 0:00:24
epoch [97/200] batch [35/83] time 0.370 (0.454) data 0.239 (0.323) loss_u loss_u 0.9072 (0.9610) acc_u 9.3750 (5.6250) lr 1.0785e-03 eta 0:00:21
epoch [97/200] batch [40/83] time 0.420 (0.451) data 0.289 (0.320) loss_u loss_u 0.9922 (0.9634) acc_u 0.0000 (5.2344) lr 1.0785e-03 eta 0:00:19
epoch [97/200] batch [45/83] time 0.518 (0.454) data 0.386 (0.323) loss_u loss_u 0.9873 (0.9631) acc_u 0.0000 (5.1389) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [50/83] time 0.456 (0.455) data 0.325 (0.324) loss_u loss_u 0.9668 (0.9635) acc_u 6.2500 (5.1250) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [55/83] time 0.397 (0.455) data 0.265 (0.324) loss_u loss_u 0.9585 (0.9629) acc_u 3.1250 (5.0568) lr 1.0785e-03 eta 0:00:12
epoch [97/200] batch [60/83] time 0.388 (0.452) data 0.256 (0.321) loss_u loss_u 0.9165 (0.9624) acc_u 9.3750 (5.0521) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [65/83] time 0.507 (0.450) data 0.375 (0.319) loss_u loss_u 0.9702 (0.9632) acc_u 3.1250 (4.9519) lr 1.0785e-03 eta 0:00:08
epoch [97/200] batch [70/83] time 0.392 (0.455) data 0.261 (0.324) loss_u loss_u 0.9937 (0.9631) acc_u 0.0000 (4.9107) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [75/83] time 0.391 (0.453) data 0.260 (0.322) loss_u loss_u 0.9370 (0.9623) acc_u 6.2500 (4.9583) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [80/83] time 0.502 (0.453) data 0.370 (0.322) loss_u loss_u 0.9717 (0.9619) acc_u 3.1250 (4.9609) lr 1.0785e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1746
confident_label rate tensor(0.1467, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 460
clean true:448
clean false:12
clean_rate:0.9739130434782609
noisy true:942
noisy false:1734
after delete: len(clean_dataset) 460
after delete: len(noisy_dataset) 2676
epoch [98/200] batch [5/14] time 0.536 (0.515) data 0.406 (0.385) loss_x loss_x 1.1689 (1.1109) acc_x 75.0000 (72.5000) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [10/14] time 0.538 (0.481) data 0.408 (0.351) loss_x loss_x 1.0850 (1.2072) acc_x 75.0000 (70.9375) lr 1.0628e-03 eta 0:00:01
epoch [98/200] batch [5/83] time 0.588 (0.475) data 0.456 (0.344) loss_u loss_u 0.8911 (0.9697) acc_u 18.7500 (5.0000) lr 1.0628e-03 eta 0:00:37
epoch [98/200] batch [10/83] time 0.421 (0.474) data 0.291 (0.344) loss_u loss_u 0.9883 (0.9689) acc_u 3.1250 (4.3750) lr 1.0628e-03 eta 0:00:34
epoch [98/200] batch [15/83] time 0.397 (0.469) data 0.266 (0.339) loss_u loss_u 0.9854 (0.9669) acc_u 3.1250 (4.3750) lr 1.0628e-03 eta 0:00:31
epoch [98/200] batch [20/83] time 0.459 (0.465) data 0.327 (0.335) loss_u loss_u 0.9927 (0.9685) acc_u 0.0000 (4.0625) lr 1.0628e-03 eta 0:00:29
epoch [98/200] batch [25/83] time 0.437 (0.460) data 0.307 (0.330) loss_u loss_u 0.9683 (0.9683) acc_u 3.1250 (3.8750) lr 1.0628e-03 eta 0:00:26
epoch [98/200] batch [30/83] time 0.505 (0.457) data 0.375 (0.327) loss_u loss_u 0.9570 (0.9650) acc_u 6.2500 (4.3750) lr 1.0628e-03 eta 0:00:24
epoch [98/200] batch [35/83] time 0.394 (0.456) data 0.263 (0.326) loss_u loss_u 0.9595 (0.9650) acc_u 6.2500 (4.3750) lr 1.0628e-03 eta 0:00:21
epoch [98/200] batch [40/83] time 0.527 (0.452) data 0.397 (0.322) loss_u loss_u 0.9893 (0.9651) acc_u 0.0000 (4.3750) lr 1.0628e-03 eta 0:00:19
epoch [98/200] batch [45/83] time 0.485 (0.455) data 0.355 (0.324) loss_u loss_u 0.9810 (0.9647) acc_u 3.1250 (4.5833) lr 1.0628e-03 eta 0:00:17
epoch [98/200] batch [50/83] time 0.423 (0.456) data 0.291 (0.326) loss_u loss_u 0.9609 (0.9646) acc_u 6.2500 (4.6250) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [55/83] time 0.513 (0.456) data 0.382 (0.325) loss_u loss_u 0.9731 (0.9645) acc_u 3.1250 (4.7159) lr 1.0628e-03 eta 0:00:12
epoch [98/200] batch [60/83] time 0.469 (0.457) data 0.339 (0.326) loss_u loss_u 0.9043 (0.9635) acc_u 9.3750 (4.7396) lr 1.0628e-03 eta 0:00:10
epoch [98/200] batch [65/83] time 0.350 (0.453) data 0.219 (0.322) loss_u loss_u 0.9961 (0.9641) acc_u 0.0000 (4.7115) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [70/83] time 0.412 (0.452) data 0.282 (0.322) loss_u loss_u 0.9907 (0.9634) acc_u 0.0000 (4.8661) lr 1.0628e-03 eta 0:00:05
epoch [98/200] batch [75/83] time 0.586 (0.452) data 0.456 (0.322) loss_u loss_u 0.9316 (0.9613) acc_u 6.2500 (5.1250) lr 1.0628e-03 eta 0:00:03
epoch [98/200] batch [80/83] time 0.458 (0.450) data 0.327 (0.320) loss_u loss_u 0.9272 (0.9614) acc_u 6.2500 (5.0781) lr 1.0628e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1762
confident_label rate tensor(0.1425, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 447
clean true:436
clean false:11
clean_rate:0.9753914988814317
noisy true:938
noisy false:1751
after delete: len(clean_dataset) 447
after delete: len(noisy_dataset) 2689
epoch [99/200] batch [5/13] time 0.413 (0.439) data 0.282 (0.309) loss_x loss_x 1.3516 (1.1031) acc_x 78.1250 (78.1250) lr 1.0471e-03 eta 0:00:03
epoch [99/200] batch [10/13] time 0.540 (0.489) data 0.409 (0.359) loss_x loss_x 1.1543 (1.1471) acc_x 71.8750 (77.1875) lr 1.0471e-03 eta 0:00:01
epoch [99/200] batch [5/84] time 0.470 (0.470) data 0.340 (0.339) loss_u loss_u 0.9692 (0.9647) acc_u 3.1250 (4.3750) lr 1.0471e-03 eta 0:00:37
epoch [99/200] batch [10/84] time 0.432 (0.465) data 0.301 (0.335) loss_u loss_u 0.9888 (0.9706) acc_u 3.1250 (3.4375) lr 1.0471e-03 eta 0:00:34
epoch [99/200] batch [15/84] time 0.479 (0.463) data 0.348 (0.332) loss_u loss_u 0.9824 (0.9738) acc_u 3.1250 (2.9167) lr 1.0471e-03 eta 0:00:31
epoch [99/200] batch [20/84] time 0.400 (0.457) data 0.270 (0.326) loss_u loss_u 0.9600 (0.9692) acc_u 6.2500 (3.5938) lr 1.0471e-03 eta 0:00:29
epoch [99/200] batch [25/84] time 0.377 (0.452) data 0.246 (0.321) loss_u loss_u 0.9844 (0.9697) acc_u 6.2500 (3.8750) lr 1.0471e-03 eta 0:00:26
epoch [99/200] batch [30/84] time 0.441 (0.451) data 0.309 (0.320) loss_u loss_u 0.9106 (0.9694) acc_u 12.5000 (4.0625) lr 1.0471e-03 eta 0:00:24
epoch [99/200] batch [35/84] time 0.432 (0.453) data 0.301 (0.322) loss_u loss_u 0.9717 (0.9677) acc_u 3.1250 (4.4643) lr 1.0471e-03 eta 0:00:22
epoch [99/200] batch [40/84] time 0.516 (0.453) data 0.385 (0.322) loss_u loss_u 0.9395 (0.9646) acc_u 6.2500 (4.8438) lr 1.0471e-03 eta 0:00:19
epoch [99/200] batch [45/84] time 0.545 (0.455) data 0.414 (0.324) loss_u loss_u 0.9844 (0.9631) acc_u 3.1250 (5.0694) lr 1.0471e-03 eta 0:00:17
epoch [99/200] batch [50/84] time 0.552 (0.460) data 0.421 (0.329) loss_u loss_u 0.9746 (0.9620) acc_u 3.1250 (5.1875) lr 1.0471e-03 eta 0:00:15
epoch [99/200] batch [55/84] time 0.484 (0.458) data 0.352 (0.327) loss_u loss_u 0.9287 (0.9597) acc_u 15.6250 (5.5114) lr 1.0471e-03 eta 0:00:13
epoch [99/200] batch [60/84] time 0.425 (0.456) data 0.295 (0.325) loss_u loss_u 0.9893 (0.9606) acc_u 3.1250 (5.4688) lr 1.0471e-03 eta 0:00:10
epoch [99/200] batch [65/84] time 0.361 (0.456) data 0.231 (0.325) loss_u loss_u 0.9644 (0.9615) acc_u 3.1250 (5.2885) lr 1.0471e-03 eta 0:00:08
epoch [99/200] batch [70/84] time 0.444 (0.456) data 0.312 (0.325) loss_u loss_u 0.9463 (0.9606) acc_u 6.2500 (5.3571) lr 1.0471e-03 eta 0:00:06
epoch [99/200] batch [75/84] time 0.398 (0.454) data 0.266 (0.323) loss_u loss_u 0.9551 (0.9619) acc_u 6.2500 (5.1667) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [80/84] time 0.462 (0.452) data 0.332 (0.321) loss_u loss_u 0.9487 (0.9607) acc_u 12.5000 (5.3906) lr 1.0471e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1762
confident_label rate tensor(0.1457, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 457
clean true:445
clean false:12
clean_rate:0.973741794310722
noisy true:929
noisy false:1750
all clean rate:  [0.9221556886227545, 0.9278846153846154, 0.9544072948328267, 0.975975975975976, 0.975609756097561, 0.9741100323624595, 0.9753086419753086, 0.9791044776119403, 0.9683908045977011, 0.9833333333333333, 0.9828571428571429, 0.9826086956521739, 0.9796511627906976, 0.9858757062146892, 0.9705014749262537, 0.9818181818181818, 0.9744318181818182, 0.984472049689441, 0.9636871508379888, 0.9793510324483776, 0.9678362573099415, 0.9747191011235955, 0.9731903485254692, 0.972972972972973, 0.9705882352941176, 0.9738903394255874, 0.9717223650385605, 0.9714285714285714, 0.9811320754716981, 0.961218836565097, 0.9726775956284153, 0.9778869778869779, 0.9605633802816902, 0.9792207792207792, 0.9720812182741116, 0.9734299516908212, 0.9626865671641791, 0.9675, 0.9844961240310077, 0.9732360097323601, 0.974293059125964, 0.9664948453608248, 0.9750623441396509, 0.9644670050761421, 0.9722921914357683, 0.9655172413793104, 0.9755501222493888, 0.9643705463182898, 0.9625935162094763, 0.9625, 0.9746835443037974, 0.9806763285024155, 0.967741935483871, 0.9748110831234257, 0.9664268585131894, 0.9632183908045977, 0.9715025906735751, 0.9661016949152542, 0.9585253456221198, 0.9739952718676123, 0.9644549763033176, 0.9683972911963883, 0.9566265060240964, 0.9656862745098039, 0.9629629629629629, 0.9656862745098039, 0.9662650602409638, 0.9667458432304038, 0.9769585253456221, 0.9734513274336283, 0.9549763033175356, 0.9632034632034632, 0.9594594594594594, 0.9722222222222222, 0.959731543624161, 0.9662162162162162, 0.9726651480637813, 0.9766899766899767, 0.9694323144104804, 0.9761904761904762, 0.9648351648351648, 0.9705882352941176, 0.9710467706013363, 0.9652173913043478, 0.9712389380530974, 0.9742489270386266, 0.9666666666666667, 0.9685393258426966, 0.9658119658119658, 0.9722222222222222, 0.9727891156462585, 0.9624217118997912, 0.9644444444444444, 0.9717391304347827, 0.9751552795031055, 0.9701834862385321, 0.9690265486725663, 0.9739130434782609, 0.9753914988814317, 0.973741794310722]
after delete: len(clean_dataset) 457
after delete: len(noisy_dataset) 2679
epoch [100/200] batch [5/14] time 0.509 (0.498) data 0.379 (0.368) loss_x loss_x 0.8169 (0.8850) acc_x 81.2500 (80.6250) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [10/14] time 0.547 (0.486) data 0.417 (0.356) loss_x loss_x 1.1465 (1.0496) acc_x 78.1250 (76.5625) lr 1.0314e-03 eta 0:00:01
epoch [100/200] batch [5/83] time 0.414 (0.485) data 0.283 (0.355) loss_u loss_u 0.9668 (0.9580) acc_u 3.1250 (5.0000) lr 1.0314e-03 eta 0:00:37
epoch [100/200] batch [10/83] time 0.428 (0.492) data 0.296 (0.361) loss_u loss_u 0.9727 (0.9587) acc_u 3.1250 (4.6875) lr 1.0314e-03 eta 0:00:35
epoch [100/200] batch [15/83] time 0.404 (0.487) data 0.274 (0.356) loss_u loss_u 0.9834 (0.9659) acc_u 0.0000 (4.1667) lr 1.0314e-03 eta 0:00:33
epoch [100/200] batch [20/83] time 0.374 (0.478) data 0.242 (0.347) loss_u loss_u 0.9946 (0.9644) acc_u 0.0000 (4.3750) lr 1.0314e-03 eta 0:00:30
epoch [100/200] batch [25/83] time 0.384 (0.478) data 0.254 (0.347) loss_u loss_u 0.9668 (0.9660) acc_u 9.3750 (4.5000) lr 1.0314e-03 eta 0:00:27
epoch [100/200] batch [30/83] time 0.372 (0.468) data 0.242 (0.337) loss_u loss_u 0.9429 (0.9636) acc_u 9.3750 (4.6875) lr 1.0314e-03 eta 0:00:24
epoch [100/200] batch [35/83] time 0.444 (0.466) data 0.313 (0.336) loss_u loss_u 0.9487 (0.9625) acc_u 6.2500 (4.6429) lr 1.0314e-03 eta 0:00:22
epoch [100/200] batch [40/83] time 0.477 (0.463) data 0.347 (0.332) loss_u loss_u 0.9150 (0.9607) acc_u 12.5000 (4.9219) lr 1.0314e-03 eta 0:00:19
epoch [100/200] batch [45/83] time 0.429 (0.467) data 0.298 (0.336) loss_u loss_u 0.9780 (0.9632) acc_u 3.1250 (4.5139) lr 1.0314e-03 eta 0:00:17
epoch [100/200] batch [50/83] time 0.468 (0.465) data 0.338 (0.335) loss_u loss_u 0.9521 (0.9638) acc_u 3.1250 (4.3125) lr 1.0314e-03 eta 0:00:15
epoch [100/200] batch [55/83] time 0.485 (0.463) data 0.354 (0.332) loss_u loss_u 0.9658 (0.9643) acc_u 6.2500 (4.2614) lr 1.0314e-03 eta 0:00:12
epoch [100/200] batch [60/83] time 0.400 (0.460) data 0.270 (0.329) loss_u loss_u 0.9297 (0.9632) acc_u 9.3750 (4.4792) lr 1.0314e-03 eta 0:00:10
epoch [100/200] batch [65/83] time 0.434 (0.458) data 0.304 (0.327) loss_u loss_u 0.9580 (0.9629) acc_u 6.2500 (4.5192) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [70/83] time 0.426 (0.457) data 0.294 (0.326) loss_u loss_u 0.9976 (0.9626) acc_u 0.0000 (4.6429) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [75/83] time 0.446 (0.457) data 0.315 (0.326) loss_u loss_u 0.9741 (0.9621) acc_u 0.0000 (4.6667) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [80/83] time 0.494 (0.456) data 0.364 (0.325) loss_u loss_u 0.9858 (0.9629) acc_u 0.0000 (4.6094) lr 1.0314e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1745
confident_label rate tensor(0.1476, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 463
clean true:450
clean false:13
clean_rate:0.9719222462203023
noisy true:941
noisy false:1732
after delete: len(clean_dataset) 463
after delete: len(noisy_dataset) 2673
epoch [101/200] batch [5/14] time 0.485 (0.447) data 0.353 (0.315) loss_x loss_x 0.8511 (1.0529) acc_x 90.6250 (78.1250) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [10/14] time 0.530 (0.486) data 0.398 (0.354) loss_x loss_x 1.5166 (1.1029) acc_x 62.5000 (75.9375) lr 1.0157e-03 eta 0:00:01
epoch [101/200] batch [5/83] time 0.469 (0.471) data 0.336 (0.339) loss_u loss_u 0.9331 (0.9575) acc_u 9.3750 (5.0000) lr 1.0157e-03 eta 0:00:36
epoch [101/200] batch [10/83] time 0.524 (0.468) data 0.392 (0.336) loss_u loss_u 0.9277 (0.9599) acc_u 9.3750 (4.6875) lr 1.0157e-03 eta 0:00:34
epoch [101/200] batch [15/83] time 0.435 (0.465) data 0.303 (0.333) loss_u loss_u 0.9702 (0.9652) acc_u 3.1250 (3.7500) lr 1.0157e-03 eta 0:00:31
epoch [101/200] batch [20/83] time 0.441 (0.465) data 0.308 (0.333) loss_u loss_u 0.9692 (0.9645) acc_u 6.2500 (3.9062) lr 1.0157e-03 eta 0:00:29
epoch [101/200] batch [25/83] time 0.384 (0.464) data 0.252 (0.332) loss_u loss_u 0.9219 (0.9642) acc_u 9.3750 (4.0000) lr 1.0157e-03 eta 0:00:26
epoch [101/200] batch [30/83] time 0.463 (0.467) data 0.331 (0.334) loss_u loss_u 0.9355 (0.9650) acc_u 6.2500 (3.8542) lr 1.0157e-03 eta 0:00:24
epoch [101/200] batch [35/83] time 0.484 (0.463) data 0.352 (0.331) loss_u loss_u 0.9829 (0.9669) acc_u 0.0000 (3.6607) lr 1.0157e-03 eta 0:00:22
epoch [101/200] batch [40/83] time 0.442 (0.473) data 0.310 (0.341) loss_u loss_u 0.9839 (0.9655) acc_u 0.0000 (3.7500) lr 1.0157e-03 eta 0:00:20
epoch [101/200] batch [45/83] time 0.506 (0.471) data 0.374 (0.339) loss_u loss_u 0.9595 (0.9672) acc_u 6.2500 (3.5417) lr 1.0157e-03 eta 0:00:17
epoch [101/200] batch [50/83] time 0.412 (0.471) data 0.282 (0.339) loss_u loss_u 0.9800 (0.9679) acc_u 6.2500 (3.5625) lr 1.0157e-03 eta 0:00:15
epoch [101/200] batch [55/83] time 0.311 (0.466) data 0.182 (0.334) loss_u loss_u 0.9497 (0.9673) acc_u 6.2500 (3.7500) lr 1.0157e-03 eta 0:00:13
epoch [101/200] batch [60/83] time 0.398 (0.463) data 0.267 (0.331) loss_u loss_u 0.9307 (0.9659) acc_u 9.3750 (3.9062) lr 1.0157e-03 eta 0:00:10
epoch [101/200] batch [65/83] time 0.443 (0.461) data 0.311 (0.329) loss_u loss_u 0.9863 (0.9667) acc_u 3.1250 (3.8942) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [70/83] time 0.449 (0.463) data 0.318 (0.331) loss_u loss_u 0.9702 (0.9668) acc_u 3.1250 (3.9286) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [75/83] time 0.391 (0.458) data 0.259 (0.326) loss_u loss_u 0.9600 (0.9673) acc_u 3.1250 (3.8333) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [80/83] time 0.625 (0.458) data 0.494 (0.326) loss_u loss_u 0.9790 (0.9672) acc_u 3.1250 (3.9844) lr 1.0157e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1813
confident_label rate tensor(0.1483, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 465
clean true:447
clean false:18
clean_rate:0.9612903225806452
noisy true:876
noisy false:1795
after delete: len(clean_dataset) 465
after delete: len(noisy_dataset) 2671
epoch [102/200] batch [5/14] time 0.454 (0.495) data 0.323 (0.365) loss_x loss_x 1.1885 (1.0354) acc_x 75.0000 (71.8750) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [10/14] time 0.455 (0.479) data 0.325 (0.348) loss_x loss_x 1.2588 (1.0925) acc_x 78.1250 (71.8750) lr 1.0000e-03 eta 0:00:01
epoch [102/200] batch [5/83] time 0.492 (0.486) data 0.361 (0.355) loss_u loss_u 0.9990 (0.9694) acc_u 0.0000 (2.5000) lr 1.0000e-03 eta 0:00:37
epoch [102/200] batch [10/83] time 0.357 (0.468) data 0.225 (0.337) loss_u loss_u 0.9536 (0.9719) acc_u 6.2500 (2.8125) lr 1.0000e-03 eta 0:00:34
epoch [102/200] batch [15/83] time 0.486 (0.461) data 0.356 (0.330) loss_u loss_u 0.9473 (0.9688) acc_u 6.2500 (3.3333) lr 1.0000e-03 eta 0:00:31
epoch [102/200] batch [20/83] time 0.354 (0.464) data 0.222 (0.334) loss_u loss_u 0.9526 (0.9670) acc_u 6.2500 (3.7500) lr 1.0000e-03 eta 0:00:29
epoch [102/200] batch [25/83] time 0.414 (0.466) data 0.283 (0.335) loss_u loss_u 0.9673 (0.9655) acc_u 3.1250 (4.1250) lr 1.0000e-03 eta 0:00:27
epoch [102/200] batch [30/83] time 0.462 (0.464) data 0.331 (0.333) loss_u loss_u 0.9619 (0.9642) acc_u 6.2500 (4.5833) lr 1.0000e-03 eta 0:00:24
epoch [102/200] batch [35/83] time 0.418 (0.468) data 0.286 (0.337) loss_u loss_u 0.9814 (0.9634) acc_u 3.1250 (4.7321) lr 1.0000e-03 eta 0:00:22
epoch [102/200] batch [40/83] time 0.472 (0.469) data 0.340 (0.338) loss_u loss_u 0.9639 (0.9630) acc_u 6.2500 (4.7656) lr 1.0000e-03 eta 0:00:20
epoch [102/200] batch [45/83] time 0.437 (0.467) data 0.305 (0.336) loss_u loss_u 0.9351 (0.9623) acc_u 12.5000 (5.0000) lr 1.0000e-03 eta 0:00:17
epoch [102/200] batch [50/83] time 0.361 (0.460) data 0.229 (0.329) loss_u loss_u 0.9497 (0.9600) acc_u 6.2500 (5.3125) lr 1.0000e-03 eta 0:00:15
epoch [102/200] batch [55/83] time 0.506 (0.459) data 0.375 (0.328) loss_u loss_u 0.9619 (0.9607) acc_u 6.2500 (5.2273) lr 1.0000e-03 eta 0:00:12
epoch [102/200] batch [60/83] time 0.532 (0.461) data 0.401 (0.330) loss_u loss_u 0.9756 (0.9617) acc_u 0.0000 (5.1042) lr 1.0000e-03 eta 0:00:10
epoch [102/200] batch [65/83] time 0.470 (0.460) data 0.340 (0.329) loss_u loss_u 0.9863 (0.9627) acc_u 3.1250 (4.9519) lr 1.0000e-03 eta 0:00:08
epoch [102/200] batch [70/83] time 0.466 (0.462) data 0.334 (0.331) loss_u loss_u 0.9619 (0.9635) acc_u 6.2500 (4.8214) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [75/83] time 0.433 (0.461) data 0.303 (0.330) loss_u loss_u 0.9746 (0.9631) acc_u 3.1250 (4.8750) lr 1.0000e-03 eta 0:00:03
epoch [102/200] batch [80/83] time 0.607 (0.461) data 0.476 (0.330) loss_u loss_u 0.9272 (0.9633) acc_u 9.3750 (4.8047) lr 1.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1756
confident_label rate tensor(0.1518, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 476
clean true:462
clean false:14
clean_rate:0.9705882352941176
noisy true:918
noisy false:1742
after delete: len(clean_dataset) 476
after delete: len(noisy_dataset) 2660
epoch [103/200] batch [5/14] time 0.425 (0.448) data 0.295 (0.318) loss_x loss_x 1.2656 (1.2857) acc_x 62.5000 (69.3750) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [10/14] time 0.444 (0.459) data 0.314 (0.329) loss_x loss_x 1.3906 (1.3380) acc_x 59.3750 (66.8750) lr 9.8429e-04 eta 0:00:01
epoch [103/200] batch [5/83] time 0.403 (0.446) data 0.272 (0.315) loss_u loss_u 0.9531 (0.9637) acc_u 3.1250 (4.3750) lr 9.8429e-04 eta 0:00:34
epoch [103/200] batch [10/83] time 0.409 (0.439) data 0.277 (0.309) loss_u loss_u 0.9463 (0.9561) acc_u 6.2500 (5.9375) lr 9.8429e-04 eta 0:00:32
epoch [103/200] batch [15/83] time 0.395 (0.444) data 0.265 (0.313) loss_u loss_u 0.9839 (0.9588) acc_u 3.1250 (5.2083) lr 9.8429e-04 eta 0:00:30
epoch [103/200] batch [20/83] time 0.571 (0.447) data 0.441 (0.316) loss_u loss_u 0.9434 (0.9585) acc_u 6.2500 (5.0000) lr 9.8429e-04 eta 0:00:28
epoch [103/200] batch [25/83] time 0.559 (0.452) data 0.429 (0.321) loss_u loss_u 0.9385 (0.9585) acc_u 9.3750 (5.1250) lr 9.8429e-04 eta 0:00:26
epoch [103/200] batch [30/83] time 0.420 (0.450) data 0.290 (0.319) loss_u loss_u 0.9810 (0.9605) acc_u 0.0000 (4.4792) lr 9.8429e-04 eta 0:00:23
epoch [103/200] batch [35/83] time 0.400 (0.446) data 0.270 (0.315) loss_u loss_u 0.9937 (0.9624) acc_u 0.0000 (4.2857) lr 9.8429e-04 eta 0:00:21
epoch [103/200] batch [40/83] time 0.398 (0.446) data 0.267 (0.316) loss_u loss_u 0.9766 (0.9630) acc_u 3.1250 (4.3750) lr 9.8429e-04 eta 0:00:19
epoch [103/200] batch [45/83] time 0.518 (0.449) data 0.387 (0.318) loss_u loss_u 0.9404 (0.9641) acc_u 6.2500 (4.2361) lr 9.8429e-04 eta 0:00:17
epoch [103/200] batch [50/83] time 0.395 (0.454) data 0.264 (0.323) loss_u loss_u 0.9736 (0.9637) acc_u 6.2500 (4.3750) lr 9.8429e-04 eta 0:00:14
epoch [103/200] batch [55/83] time 0.425 (0.452) data 0.294 (0.321) loss_u loss_u 0.9761 (0.9618) acc_u 3.1250 (4.6023) lr 9.8429e-04 eta 0:00:12
epoch [103/200] batch [60/83] time 0.381 (0.451) data 0.251 (0.320) loss_u loss_u 0.9424 (0.9611) acc_u 9.3750 (4.8438) lr 9.8429e-04 eta 0:00:10
epoch [103/200] batch [65/83] time 0.482 (0.451) data 0.351 (0.321) loss_u loss_u 0.9912 (0.9625) acc_u 0.0000 (4.6154) lr 9.8429e-04 eta 0:00:08
epoch [103/200] batch [70/83] time 0.551 (0.450) data 0.420 (0.319) loss_u loss_u 0.9893 (0.9635) acc_u 3.1250 (4.6875) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [75/83] time 0.346 (0.451) data 0.216 (0.321) loss_u loss_u 0.9097 (0.9625) acc_u 18.7500 (4.8750) lr 9.8429e-04 eta 0:00:03
epoch [103/200] batch [80/83] time 0.622 (0.452) data 0.491 (0.321) loss_u loss_u 0.9707 (0.9631) acc_u 0.0000 (4.7656) lr 9.8429e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1761
confident_label rate tensor(0.1425, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 447
clean true:427
clean false:20
clean_rate:0.9552572706935123
noisy true:948
noisy false:1741
after delete: len(clean_dataset) 447
after delete: len(noisy_dataset) 2689
epoch [104/200] batch [5/13] time 0.539 (0.488) data 0.408 (0.358) loss_x loss_x 0.7979 (1.1652) acc_x 81.2500 (70.6250) lr 9.6859e-04 eta 0:00:03
epoch [104/200] batch [10/13] time 0.449 (0.489) data 0.319 (0.358) loss_x loss_x 0.9971 (1.1081) acc_x 81.2500 (74.0625) lr 9.6859e-04 eta 0:00:01
epoch [104/200] batch [5/84] time 0.640 (0.499) data 0.508 (0.368) loss_u loss_u 0.9092 (0.9526) acc_u 9.3750 (6.2500) lr 9.6859e-04 eta 0:00:39
epoch [104/200] batch [10/84] time 0.540 (0.494) data 0.408 (0.363) loss_u loss_u 0.9761 (0.9549) acc_u 0.0000 (5.6250) lr 9.6859e-04 eta 0:00:36
epoch [104/200] batch [15/84] time 0.455 (0.495) data 0.324 (0.364) loss_u loss_u 0.9707 (0.9601) acc_u 3.1250 (5.2083) lr 9.6859e-04 eta 0:00:34
epoch [104/200] batch [20/84] time 0.364 (0.489) data 0.233 (0.358) loss_u loss_u 0.9355 (0.9576) acc_u 9.3750 (5.4688) lr 9.6859e-04 eta 0:00:31
epoch [104/200] batch [25/84] time 0.626 (0.487) data 0.495 (0.356) loss_u loss_u 0.9546 (0.9592) acc_u 9.3750 (5.2500) lr 9.6859e-04 eta 0:00:28
epoch [104/200] batch [30/84] time 0.393 (0.480) data 0.263 (0.349) loss_u loss_u 0.9165 (0.9561) acc_u 9.3750 (5.5208) lr 9.6859e-04 eta 0:00:25
epoch [104/200] batch [35/84] time 0.454 (0.478) data 0.323 (0.348) loss_u loss_u 0.9683 (0.9584) acc_u 3.1250 (5.2679) lr 9.6859e-04 eta 0:00:23
epoch [104/200] batch [40/84] time 0.407 (0.471) data 0.276 (0.340) loss_u loss_u 0.9609 (0.9604) acc_u 3.1250 (5.0000) lr 9.6859e-04 eta 0:00:20
epoch [104/200] batch [45/84] time 0.395 (0.467) data 0.265 (0.336) loss_u loss_u 0.9370 (0.9590) acc_u 9.3750 (5.2083) lr 9.6859e-04 eta 0:00:18
epoch [104/200] batch [50/84] time 0.468 (0.464) data 0.337 (0.333) loss_u loss_u 0.8765 (0.9574) acc_u 15.6250 (5.5000) lr 9.6859e-04 eta 0:00:15
epoch [104/200] batch [55/84] time 0.330 (0.462) data 0.199 (0.331) loss_u loss_u 0.9561 (0.9570) acc_u 6.2500 (5.6250) lr 9.6859e-04 eta 0:00:13
epoch [104/200] batch [60/84] time 0.381 (0.462) data 0.250 (0.331) loss_u loss_u 0.9707 (0.9565) acc_u 6.2500 (5.7292) lr 9.6859e-04 eta 0:00:11
epoch [104/200] batch [65/84] time 0.422 (0.462) data 0.292 (0.331) loss_u loss_u 0.9575 (0.9576) acc_u 6.2500 (5.6250) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [70/84] time 0.558 (0.461) data 0.426 (0.330) loss_u loss_u 0.9785 (0.9577) acc_u 3.1250 (5.5804) lr 9.6859e-04 eta 0:00:06
epoch [104/200] batch [75/84] time 0.750 (0.464) data 0.619 (0.333) loss_u loss_u 0.9404 (0.9581) acc_u 6.2500 (5.4583) lr 9.6859e-04 eta 0:00:04
epoch [104/200] batch [80/84] time 0.456 (0.463) data 0.325 (0.332) loss_u loss_u 0.9668 (0.9583) acc_u 3.1250 (5.4297) lr 9.6859e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1716
confident_label rate tensor(0.1527, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 479
clean true:464
clean false:15
clean_rate:0.9686847599164927
noisy true:956
noisy false:1701
after delete: len(clean_dataset) 479
after delete: len(noisy_dataset) 2657
epoch [105/200] batch [5/14] time 0.488 (0.457) data 0.358 (0.327) loss_x loss_x 1.0811 (1.0529) acc_x 78.1250 (75.0000) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [10/14] time 0.426 (0.453) data 0.296 (0.323) loss_x loss_x 1.3350 (1.1413) acc_x 65.6250 (72.5000) lr 9.5289e-04 eta 0:00:01
epoch [105/200] batch [5/83] time 0.554 (0.464) data 0.424 (0.333) loss_u loss_u 0.8970 (0.9323) acc_u 15.6250 (8.7500) lr 9.5289e-04 eta 0:00:36
epoch [105/200] batch [10/83] time 0.557 (0.466) data 0.426 (0.336) loss_u loss_u 0.9780 (0.9498) acc_u 3.1250 (6.8750) lr 9.5289e-04 eta 0:00:34
epoch [105/200] batch [15/83] time 0.465 (0.469) data 0.333 (0.339) loss_u loss_u 0.9463 (0.9480) acc_u 6.2500 (6.4583) lr 9.5289e-04 eta 0:00:31
epoch [105/200] batch [20/83] time 0.371 (0.468) data 0.240 (0.337) loss_u loss_u 0.9995 (0.9497) acc_u 0.0000 (7.0312) lr 9.5289e-04 eta 0:00:29
epoch [105/200] batch [25/83] time 0.425 (0.462) data 0.294 (0.331) loss_u loss_u 0.9355 (0.9516) acc_u 6.2500 (6.7500) lr 9.5289e-04 eta 0:00:26
epoch [105/200] batch [30/83] time 0.600 (0.465) data 0.468 (0.334) loss_u loss_u 0.9756 (0.9544) acc_u 3.1250 (6.2500) lr 9.5289e-04 eta 0:00:24
epoch [105/200] batch [35/83] time 0.409 (0.460) data 0.278 (0.330) loss_u loss_u 0.9639 (0.9572) acc_u 3.1250 (5.8036) lr 9.5289e-04 eta 0:00:22
epoch [105/200] batch [40/83] time 0.440 (0.463) data 0.310 (0.332) loss_u loss_u 0.9512 (0.9591) acc_u 9.3750 (5.4688) lr 9.5289e-04 eta 0:00:19
epoch [105/200] batch [45/83] time 0.504 (0.459) data 0.374 (0.328) loss_u loss_u 0.9746 (0.9594) acc_u 0.0000 (5.2778) lr 9.5289e-04 eta 0:00:17
epoch [105/200] batch [50/83] time 0.379 (0.455) data 0.249 (0.324) loss_u loss_u 0.9277 (0.9609) acc_u 9.3750 (5.1875) lr 9.5289e-04 eta 0:00:15
epoch [105/200] batch [55/83] time 0.428 (0.454) data 0.296 (0.323) loss_u loss_u 0.9624 (0.9615) acc_u 6.2500 (5.0568) lr 9.5289e-04 eta 0:00:12
epoch [105/200] batch [60/83] time 0.515 (0.454) data 0.384 (0.323) loss_u loss_u 0.9658 (0.9625) acc_u 3.1250 (4.8958) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [65/83] time 0.427 (0.452) data 0.297 (0.322) loss_u loss_u 0.9111 (0.9618) acc_u 15.6250 (5.0962) lr 9.5289e-04 eta 0:00:08
epoch [105/200] batch [70/83] time 0.544 (0.452) data 0.414 (0.321) loss_u loss_u 0.9492 (0.9617) acc_u 9.3750 (5.0893) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [75/83] time 0.396 (0.450) data 0.265 (0.319) loss_u loss_u 0.9570 (0.9620) acc_u 6.2500 (5.0833) lr 9.5289e-04 eta 0:00:03
epoch [105/200] batch [80/83] time 0.382 (0.447) data 0.251 (0.317) loss_u loss_u 1.0000 (0.9626) acc_u 0.0000 (5.0391) lr 9.5289e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1747
confident_label rate tensor(0.1470, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 461
clean true:449
clean false:12
clean_rate:0.9739696312364425
noisy true:940
noisy false:1735
after delete: len(clean_dataset) 461
after delete: len(noisy_dataset) 2675
epoch [106/200] batch [5/14] time 0.524 (0.458) data 0.393 (0.327) loss_x loss_x 1.2354 (1.0006) acc_x 68.7500 (73.7500) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [10/14] time 0.427 (0.452) data 0.297 (0.321) loss_x loss_x 1.3691 (1.1281) acc_x 59.3750 (72.1875) lr 9.3721e-04 eta 0:00:01
epoch [106/200] batch [5/83] time 0.518 (0.447) data 0.386 (0.316) loss_u loss_u 0.9966 (0.9636) acc_u 0.0000 (4.3750) lr 9.3721e-04 eta 0:00:34
epoch [106/200] batch [10/83] time 0.473 (0.449) data 0.342 (0.318) loss_u loss_u 0.9683 (0.9653) acc_u 3.1250 (4.0625) lr 9.3721e-04 eta 0:00:32
epoch [106/200] batch [15/83] time 0.469 (0.450) data 0.337 (0.319) loss_u loss_u 0.9521 (0.9655) acc_u 6.2500 (3.9583) lr 9.3721e-04 eta 0:00:30
epoch [106/200] batch [20/83] time 0.391 (0.444) data 0.259 (0.314) loss_u loss_u 0.9722 (0.9652) acc_u 3.1250 (3.9062) lr 9.3721e-04 eta 0:00:27
epoch [106/200] batch [25/83] time 0.453 (0.445) data 0.323 (0.315) loss_u loss_u 0.9404 (0.9609) acc_u 9.3750 (4.6250) lr 9.3721e-04 eta 0:00:25
epoch [106/200] batch [30/83] time 0.448 (0.448) data 0.316 (0.318) loss_u loss_u 0.9580 (0.9590) acc_u 6.2500 (5.2083) lr 9.3721e-04 eta 0:00:23
epoch [106/200] batch [35/83] time 0.407 (0.452) data 0.275 (0.321) loss_u loss_u 0.9629 (0.9583) acc_u 6.2500 (5.3571) lr 9.3721e-04 eta 0:00:21
epoch [106/200] batch [40/83] time 0.454 (0.452) data 0.323 (0.321) loss_u loss_u 0.9497 (0.9580) acc_u 6.2500 (5.3906) lr 9.3721e-04 eta 0:00:19
epoch [106/200] batch [45/83] time 0.440 (0.456) data 0.310 (0.325) loss_u loss_u 0.9600 (0.9559) acc_u 6.2500 (5.5556) lr 9.3721e-04 eta 0:00:17
epoch [106/200] batch [50/83] time 0.530 (0.455) data 0.399 (0.324) loss_u loss_u 0.9429 (0.9569) acc_u 9.3750 (5.5000) lr 9.3721e-04 eta 0:00:15
epoch [106/200] batch [55/83] time 0.412 (0.457) data 0.281 (0.326) loss_u loss_u 0.9585 (0.9576) acc_u 6.2500 (5.3977) lr 9.3721e-04 eta 0:00:12
epoch [106/200] batch [60/83] time 0.449 (0.454) data 0.318 (0.323) loss_u loss_u 0.9771 (0.9590) acc_u 0.0000 (5.3125) lr 9.3721e-04 eta 0:00:10
epoch [106/200] batch [65/83] time 0.413 (0.452) data 0.283 (0.321) loss_u loss_u 0.9834 (0.9581) acc_u 0.0000 (5.3846) lr 9.3721e-04 eta 0:00:08
epoch [106/200] batch [70/83] time 0.654 (0.454) data 0.523 (0.323) loss_u loss_u 0.9395 (0.9573) acc_u 6.2500 (5.4911) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [75/83] time 0.407 (0.453) data 0.277 (0.322) loss_u loss_u 0.9888 (0.9578) acc_u 3.1250 (5.5000) lr 9.3721e-04 eta 0:00:03
epoch [106/200] batch [80/83] time 0.530 (0.453) data 0.399 (0.322) loss_u loss_u 0.9209 (0.9561) acc_u 9.3750 (5.7031) lr 9.3721e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1759
confident_label rate tensor(0.1451, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 455
clean true:441
clean false:14
clean_rate:0.9692307692307692
noisy true:936
noisy false:1745
after delete: len(clean_dataset) 455
after delete: len(noisy_dataset) 2681
epoch [107/200] batch [5/14] time 0.424 (0.471) data 0.293 (0.341) loss_x loss_x 0.7915 (1.1079) acc_x 84.3750 (75.0000) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [10/14] time 0.350 (0.446) data 0.220 (0.316) loss_x loss_x 1.7832 (1.1762) acc_x 53.1250 (73.1250) lr 9.2154e-04 eta 0:00:01
epoch [107/200] batch [5/83] time 0.468 (0.463) data 0.336 (0.332) loss_u loss_u 0.9668 (0.9616) acc_u 6.2500 (4.3750) lr 9.2154e-04 eta 0:00:36
epoch [107/200] batch [10/83] time 0.436 (0.453) data 0.305 (0.322) loss_u loss_u 0.9561 (0.9599) acc_u 6.2500 (4.6875) lr 9.2154e-04 eta 0:00:33
epoch [107/200] batch [15/83] time 0.513 (0.462) data 0.383 (0.331) loss_u loss_u 0.9746 (0.9609) acc_u 3.1250 (4.5833) lr 9.2154e-04 eta 0:00:31
epoch [107/200] batch [20/83] time 0.421 (0.459) data 0.290 (0.328) loss_u loss_u 0.9272 (0.9602) acc_u 9.3750 (4.8438) lr 9.2154e-04 eta 0:00:28
epoch [107/200] batch [25/83] time 0.496 (0.462) data 0.366 (0.332) loss_u loss_u 0.9668 (0.9570) acc_u 3.1250 (5.5000) lr 9.2154e-04 eta 0:00:26
epoch [107/200] batch [30/83] time 0.658 (0.467) data 0.528 (0.337) loss_u loss_u 0.9487 (0.9571) acc_u 6.2500 (5.4167) lr 9.2154e-04 eta 0:00:24
epoch [107/200] batch [35/83] time 0.417 (0.461) data 0.286 (0.331) loss_u loss_u 0.9639 (0.9569) acc_u 3.1250 (5.2679) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [40/83] time 0.387 (0.460) data 0.257 (0.329) loss_u loss_u 0.9692 (0.9575) acc_u 6.2500 (5.3906) lr 9.2154e-04 eta 0:00:19
epoch [107/200] batch [45/83] time 0.397 (0.456) data 0.267 (0.325) loss_u loss_u 0.9585 (0.9577) acc_u 6.2500 (5.3472) lr 9.2154e-04 eta 0:00:17
epoch [107/200] batch [50/83] time 0.426 (0.455) data 0.296 (0.324) loss_u loss_u 0.9268 (0.9581) acc_u 9.3750 (5.2500) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [55/83] time 0.394 (0.451) data 0.263 (0.321) loss_u loss_u 0.9702 (0.9569) acc_u 3.1250 (5.3977) lr 9.2154e-04 eta 0:00:12
epoch [107/200] batch [60/83] time 0.348 (0.449) data 0.217 (0.319) loss_u loss_u 0.9585 (0.9582) acc_u 6.2500 (5.1562) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [65/83] time 0.348 (0.449) data 0.217 (0.319) loss_u loss_u 0.9512 (0.9572) acc_u 9.3750 (5.3365) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [70/83] time 0.340 (0.447) data 0.208 (0.316) loss_u loss_u 0.9727 (0.9578) acc_u 9.3750 (5.4464) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [75/83] time 0.459 (0.444) data 0.327 (0.314) loss_u loss_u 0.9717 (0.9572) acc_u 3.1250 (5.5000) lr 9.2154e-04 eta 0:00:03
epoch [107/200] batch [80/83] time 0.390 (0.445) data 0.258 (0.314) loss_u loss_u 0.9609 (0.9569) acc_u 9.3750 (5.6250) lr 9.2154e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1726
confident_label rate tensor(0.1441, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 452
clean true:435
clean false:17
clean_rate:0.9623893805309734
noisy true:975
noisy false:1709
after delete: len(clean_dataset) 452
after delete: len(noisy_dataset) 2684
epoch [108/200] batch [5/14] time 0.496 (0.468) data 0.367 (0.338) loss_x loss_x 1.1182 (1.1888) acc_x 81.2500 (76.8750) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [10/14] time 0.557 (0.482) data 0.426 (0.352) loss_x loss_x 1.2031 (1.1236) acc_x 75.0000 (75.6250) lr 9.0589e-04 eta 0:00:01
epoch [108/200] batch [5/83] time 0.388 (0.467) data 0.256 (0.336) loss_u loss_u 0.9585 (0.9620) acc_u 6.2500 (5.0000) lr 9.0589e-04 eta 0:00:36
epoch [108/200] batch [10/83] time 0.567 (0.481) data 0.437 (0.351) loss_u loss_u 0.9883 (0.9669) acc_u 0.0000 (3.7500) lr 9.0589e-04 eta 0:00:35
epoch [108/200] batch [15/83] time 0.379 (0.463) data 0.249 (0.333) loss_u loss_u 0.9785 (0.9619) acc_u 3.1250 (4.5833) lr 9.0589e-04 eta 0:00:31
epoch [108/200] batch [20/83] time 0.364 (0.458) data 0.234 (0.328) loss_u loss_u 0.9697 (0.9609) acc_u 0.0000 (4.6875) lr 9.0589e-04 eta 0:00:28
epoch [108/200] batch [25/83] time 0.504 (0.464) data 0.372 (0.333) loss_u loss_u 0.9727 (0.9592) acc_u 3.1250 (5.1250) lr 9.0589e-04 eta 0:00:26
epoch [108/200] batch [30/83] time 0.425 (0.460) data 0.294 (0.329) loss_u loss_u 0.9438 (0.9586) acc_u 6.2500 (5.2083) lr 9.0589e-04 eta 0:00:24
epoch [108/200] batch [35/83] time 0.352 (0.457) data 0.221 (0.327) loss_u loss_u 0.9883 (0.9594) acc_u 0.0000 (5.0893) lr 9.0589e-04 eta 0:00:21
epoch [108/200] batch [40/83] time 0.477 (0.456) data 0.346 (0.325) loss_u loss_u 0.9438 (0.9590) acc_u 9.3750 (5.2344) lr 9.0589e-04 eta 0:00:19
epoch [108/200] batch [45/83] time 0.440 (0.452) data 0.309 (0.321) loss_u loss_u 0.9883 (0.9583) acc_u 0.0000 (5.2083) lr 9.0589e-04 eta 0:00:17
epoch [108/200] batch [50/83] time 0.438 (0.449) data 0.308 (0.319) loss_u loss_u 0.9390 (0.9567) acc_u 9.3750 (5.6250) lr 9.0589e-04 eta 0:00:14
epoch [108/200] batch [55/83] time 0.648 (0.451) data 0.517 (0.320) loss_u loss_u 0.9219 (0.9546) acc_u 9.3750 (5.9091) lr 9.0589e-04 eta 0:00:12
epoch [108/200] batch [60/83] time 0.394 (0.448) data 0.262 (0.317) loss_u loss_u 0.9492 (0.9551) acc_u 9.3750 (5.9375) lr 9.0589e-04 eta 0:00:10
epoch [108/200] batch [65/83] time 0.429 (0.448) data 0.299 (0.318) loss_u loss_u 0.9048 (0.9548) acc_u 12.5000 (5.9615) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [70/83] time 0.392 (0.449) data 0.261 (0.319) loss_u loss_u 0.9893 (0.9548) acc_u 0.0000 (5.9821) lr 9.0589e-04 eta 0:00:05
epoch [108/200] batch [75/83] time 0.343 (0.448) data 0.213 (0.317) loss_u loss_u 0.9731 (0.9559) acc_u 3.1250 (5.8750) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [80/83] time 0.462 (0.446) data 0.330 (0.316) loss_u loss_u 0.9336 (0.9569) acc_u 6.2500 (5.6641) lr 9.0589e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1783
confident_label rate tensor(0.1454, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 456
clean true:441
clean false:15
clean_rate:0.9671052631578947
noisy true:912
noisy false:1768
after delete: len(clean_dataset) 456
after delete: len(noisy_dataset) 2680
epoch [109/200] batch [5/14] time 0.440 (0.479) data 0.310 (0.349) loss_x loss_x 1.1553 (1.0999) acc_x 68.7500 (68.7500) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [10/14] time 0.412 (0.449) data 0.282 (0.319) loss_x loss_x 1.0938 (1.1592) acc_x 78.1250 (72.1875) lr 8.9027e-04 eta 0:00:01
epoch [109/200] batch [5/83] time 0.426 (0.453) data 0.295 (0.323) loss_u loss_u 0.9648 (0.9552) acc_u 6.2500 (6.2500) lr 8.9027e-04 eta 0:00:35
epoch [109/200] batch [10/83] time 0.391 (0.452) data 0.261 (0.321) loss_u loss_u 0.9692 (0.9589) acc_u 3.1250 (5.0000) lr 8.9027e-04 eta 0:00:32
epoch [109/200] batch [15/83] time 0.462 (0.450) data 0.332 (0.320) loss_u loss_u 0.9731 (0.9668) acc_u 6.2500 (4.1667) lr 8.9027e-04 eta 0:00:30
epoch [109/200] batch [20/83] time 0.406 (0.449) data 0.276 (0.319) loss_u loss_u 0.9214 (0.9628) acc_u 9.3750 (4.8438) lr 8.9027e-04 eta 0:00:28
epoch [109/200] batch [25/83] time 0.368 (0.454) data 0.236 (0.323) loss_u loss_u 0.9321 (0.9596) acc_u 9.3750 (5.6250) lr 8.9027e-04 eta 0:00:26
epoch [109/200] batch [30/83] time 0.428 (0.457) data 0.296 (0.326) loss_u loss_u 0.9966 (0.9582) acc_u 0.0000 (5.8333) lr 8.9027e-04 eta 0:00:24
epoch [109/200] batch [35/83] time 0.424 (0.451) data 0.294 (0.320) loss_u loss_u 0.9487 (0.9601) acc_u 6.2500 (5.6250) lr 8.9027e-04 eta 0:00:21
epoch [109/200] batch [40/83] time 0.454 (0.455) data 0.322 (0.324) loss_u loss_u 0.9634 (0.9588) acc_u 3.1250 (5.7031) lr 8.9027e-04 eta 0:00:19
epoch [109/200] batch [45/83] time 0.470 (0.451) data 0.339 (0.320) loss_u loss_u 0.9692 (0.9587) acc_u 3.1250 (5.6250) lr 8.9027e-04 eta 0:00:17
epoch [109/200] batch [50/83] time 0.525 (0.452) data 0.395 (0.322) loss_u loss_u 0.9536 (0.9597) acc_u 6.2500 (5.4375) lr 8.9027e-04 eta 0:00:14
epoch [109/200] batch [55/83] time 0.468 (0.452) data 0.338 (0.321) loss_u loss_u 0.9536 (0.9596) acc_u 6.2500 (5.5114) lr 8.9027e-04 eta 0:00:12
epoch [109/200] batch [60/83] time 0.341 (0.450) data 0.210 (0.319) loss_u loss_u 0.9800 (0.9595) acc_u 3.1250 (5.4167) lr 8.9027e-04 eta 0:00:10
epoch [109/200] batch [65/83] time 0.416 (0.451) data 0.285 (0.320) loss_u loss_u 0.9688 (0.9591) acc_u 3.1250 (5.4327) lr 8.9027e-04 eta 0:00:08
epoch [109/200] batch [70/83] time 0.416 (0.451) data 0.285 (0.320) loss_u loss_u 0.9609 (0.9581) acc_u 3.1250 (5.5357) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [75/83] time 0.439 (0.449) data 0.308 (0.318) loss_u loss_u 0.9819 (0.9580) acc_u 3.1250 (5.5833) lr 8.9027e-04 eta 0:00:03
epoch [109/200] batch [80/83] time 0.716 (0.455) data 0.584 (0.324) loss_u loss_u 0.9336 (0.9589) acc_u 6.2500 (5.3906) lr 8.9027e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1697
confident_label rate tensor(0.1562, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 490
clean true:473
clean false:17
clean_rate:0.9653061224489796
noisy true:966
noisy false:1680
after delete: len(clean_dataset) 490
after delete: len(noisy_dataset) 2646
epoch [110/200] batch [5/15] time 0.391 (0.394) data 0.261 (0.263) loss_x loss_x 1.1689 (1.3189) acc_x 62.5000 (63.7500) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [10/15] time 0.539 (0.463) data 0.408 (0.333) loss_x loss_x 1.2539 (1.3073) acc_x 65.6250 (65.3125) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [15/15] time 0.458 (0.460) data 0.328 (0.329) loss_x loss_x 0.6948 (1.2405) acc_x 87.5000 (68.9583) lr 8.7467e-04 eta 0:00:00
epoch [110/200] batch [5/82] time 0.393 (0.454) data 0.262 (0.324) loss_u loss_u 0.9766 (0.9622) acc_u 6.2500 (6.8750) lr 8.7467e-04 eta 0:00:34
epoch [110/200] batch [10/82] time 0.525 (0.465) data 0.393 (0.334) loss_u loss_u 0.9062 (0.9589) acc_u 12.5000 (6.5625) lr 8.7467e-04 eta 0:00:33
epoch [110/200] batch [15/82] time 0.404 (0.460) data 0.274 (0.329) loss_u loss_u 0.9448 (0.9628) acc_u 6.2500 (5.6250) lr 8.7467e-04 eta 0:00:30
epoch [110/200] batch [20/82] time 0.455 (0.459) data 0.324 (0.328) loss_u loss_u 0.9966 (0.9599) acc_u 0.0000 (5.6250) lr 8.7467e-04 eta 0:00:28
epoch [110/200] batch [25/82] time 0.427 (0.453) data 0.296 (0.322) loss_u loss_u 0.9995 (0.9611) acc_u 0.0000 (5.7500) lr 8.7467e-04 eta 0:00:25
epoch [110/200] batch [30/82] time 0.457 (0.454) data 0.325 (0.323) loss_u loss_u 0.9844 (0.9641) acc_u 3.1250 (5.2083) lr 8.7467e-04 eta 0:00:23
epoch [110/200] batch [35/82] time 0.463 (0.454) data 0.332 (0.324) loss_u loss_u 0.9824 (0.9639) acc_u 6.2500 (5.1786) lr 8.7467e-04 eta 0:00:21
epoch [110/200] batch [40/82] time 0.428 (0.453) data 0.297 (0.322) loss_u loss_u 0.9673 (0.9649) acc_u 3.1250 (5.0000) lr 8.7467e-04 eta 0:00:19
epoch [110/200] batch [45/82] time 0.444 (0.457) data 0.313 (0.326) loss_u loss_u 0.9604 (0.9645) acc_u 3.1250 (5.0000) lr 8.7467e-04 eta 0:00:16
epoch [110/200] batch [50/82] time 0.493 (0.462) data 0.363 (0.331) loss_u loss_u 0.9561 (0.9652) acc_u 6.2500 (4.8750) lr 8.7467e-04 eta 0:00:14
epoch [110/200] batch [55/82] time 0.454 (0.461) data 0.323 (0.330) loss_u loss_u 0.9883 (0.9667) acc_u 0.0000 (4.6023) lr 8.7467e-04 eta 0:00:12
epoch [110/200] batch [60/82] time 0.644 (0.466) data 0.514 (0.335) loss_u loss_u 0.9771 (0.9667) acc_u 3.1250 (4.4792) lr 8.7467e-04 eta 0:00:10
epoch [110/200] batch [65/82] time 0.599 (0.467) data 0.468 (0.336) loss_u loss_u 0.9897 (0.9672) acc_u 0.0000 (4.3750) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [70/82] time 0.564 (0.471) data 0.433 (0.340) loss_u loss_u 0.9937 (0.9673) acc_u 3.1250 (4.3304) lr 8.7467e-04 eta 0:00:05
epoch [110/200] batch [75/82] time 0.341 (0.468) data 0.211 (0.337) loss_u loss_u 0.9810 (0.9679) acc_u 0.0000 (4.2500) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [80/82] time 0.444 (0.466) data 0.314 (0.335) loss_u loss_u 0.9810 (0.9683) acc_u 0.0000 (4.1406) lr 8.7467e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1766
confident_label rate tensor(0.1553, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 487
clean true:466
clean false:21
clean_rate:0.9568788501026694
noisy true:904
noisy false:1745
after delete: len(clean_dataset) 487
after delete: len(noisy_dataset) 2649
epoch [111/200] batch [5/15] time 0.472 (0.481) data 0.341 (0.351) loss_x loss_x 1.4219 (1.1349) acc_x 65.6250 (73.1250) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [10/15] time 0.553 (0.502) data 0.422 (0.371) loss_x loss_x 1.2207 (1.1339) acc_x 62.5000 (73.7500) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [15/15] time 0.333 (0.480) data 0.203 (0.349) loss_x loss_x 1.3828 (1.2338) acc_x 68.7500 (71.6667) lr 8.5910e-04 eta 0:00:00
epoch [111/200] batch [5/82] time 0.433 (0.473) data 0.302 (0.342) loss_u loss_u 0.9404 (0.9662) acc_u 6.2500 (3.7500) lr 8.5910e-04 eta 0:00:36
epoch [111/200] batch [10/82] time 0.528 (0.470) data 0.397 (0.339) loss_u loss_u 0.9834 (0.9526) acc_u 0.0000 (6.2500) lr 8.5910e-04 eta 0:00:33
epoch [111/200] batch [15/82] time 0.418 (0.463) data 0.287 (0.332) loss_u loss_u 0.9136 (0.9507) acc_u 9.3750 (6.6667) lr 8.5910e-04 eta 0:00:31
epoch [111/200] batch [20/82] time 0.491 (0.464) data 0.360 (0.333) loss_u loss_u 0.9854 (0.9552) acc_u 0.0000 (5.4688) lr 8.5910e-04 eta 0:00:28
epoch [111/200] batch [25/82] time 0.504 (0.466) data 0.373 (0.335) loss_u loss_u 0.9302 (0.9567) acc_u 9.3750 (5.2500) lr 8.5910e-04 eta 0:00:26
epoch [111/200] batch [30/82] time 0.506 (0.467) data 0.374 (0.336) loss_u loss_u 0.9917 (0.9582) acc_u 0.0000 (5.0000) lr 8.5910e-04 eta 0:00:24
epoch [111/200] batch [35/82] time 0.548 (0.465) data 0.416 (0.334) loss_u loss_u 0.9849 (0.9613) acc_u 3.1250 (4.6429) lr 8.5910e-04 eta 0:00:21
epoch [111/200] batch [40/82] time 0.495 (0.465) data 0.363 (0.334) loss_u loss_u 0.9961 (0.9640) acc_u 0.0000 (4.2969) lr 8.5910e-04 eta 0:00:19
epoch [111/200] batch [45/82] time 0.420 (0.458) data 0.289 (0.327) loss_u loss_u 0.9551 (0.9622) acc_u 6.2500 (4.5833) lr 8.5910e-04 eta 0:00:16
epoch [111/200] batch [50/82] time 0.465 (0.455) data 0.334 (0.324) loss_u loss_u 0.9331 (0.9630) acc_u 6.2500 (4.5000) lr 8.5910e-04 eta 0:00:14
epoch [111/200] batch [55/82] time 0.376 (0.455) data 0.245 (0.324) loss_u loss_u 0.9795 (0.9628) acc_u 0.0000 (4.4886) lr 8.5910e-04 eta 0:00:12
epoch [111/200] batch [60/82] time 0.460 (0.451) data 0.329 (0.320) loss_u loss_u 0.9697 (0.9640) acc_u 3.1250 (4.3750) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [65/82] time 0.393 (0.452) data 0.263 (0.321) loss_u loss_u 0.9663 (0.9635) acc_u 6.2500 (4.4712) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [70/82] time 0.463 (0.453) data 0.332 (0.322) loss_u loss_u 0.9717 (0.9634) acc_u 3.1250 (4.4196) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [75/82] time 0.680 (0.456) data 0.550 (0.325) loss_u loss_u 0.9277 (0.9621) acc_u 9.3750 (4.7083) lr 8.5910e-04 eta 0:00:03
epoch [111/200] batch [80/82] time 0.401 (0.457) data 0.270 (0.326) loss_u loss_u 0.9780 (0.9617) acc_u 3.1250 (4.6875) lr 8.5910e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1745
confident_label rate tensor(0.1486, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 466
clean true:453
clean false:13
clean_rate:0.9721030042918455
noisy true:938
noisy false:1732
after delete: len(clean_dataset) 466
after delete: len(noisy_dataset) 2670
epoch [112/200] batch [5/14] time 0.585 (0.517) data 0.454 (0.386) loss_x loss_x 0.9229 (1.2496) acc_x 71.8750 (68.1250) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [10/14] time 0.578 (0.477) data 0.447 (0.347) loss_x loss_x 1.7227 (1.2795) acc_x 53.1250 (67.1875) lr 8.4357e-04 eta 0:00:01
epoch [112/200] batch [5/83] time 0.458 (0.458) data 0.327 (0.327) loss_u loss_u 0.9585 (0.9375) acc_u 6.2500 (8.7500) lr 8.4357e-04 eta 0:00:35
epoch [112/200] batch [10/83] time 0.349 (0.458) data 0.219 (0.327) loss_u loss_u 0.9600 (0.9484) acc_u 3.1250 (6.2500) lr 8.4357e-04 eta 0:00:33
epoch [112/200] batch [15/83] time 0.583 (0.465) data 0.452 (0.334) loss_u loss_u 0.9819 (0.9459) acc_u 3.1250 (6.8750) lr 8.4357e-04 eta 0:00:31
epoch [112/200] batch [20/83] time 0.535 (0.465) data 0.402 (0.334) loss_u loss_u 0.9995 (0.9530) acc_u 0.0000 (6.0938) lr 8.4357e-04 eta 0:00:29
epoch [112/200] batch [25/83] time 0.471 (0.462) data 0.340 (0.331) loss_u loss_u 0.9551 (0.9570) acc_u 9.3750 (5.5000) lr 8.4357e-04 eta 0:00:26
epoch [112/200] batch [30/83] time 0.523 (0.459) data 0.391 (0.328) loss_u loss_u 0.9707 (0.9593) acc_u 6.2500 (5.2083) lr 8.4357e-04 eta 0:00:24
epoch [112/200] batch [35/83] time 0.504 (0.458) data 0.373 (0.327) loss_u loss_u 0.9170 (0.9596) acc_u 15.6250 (5.2679) lr 8.4357e-04 eta 0:00:21
epoch [112/200] batch [40/83] time 0.501 (0.463) data 0.371 (0.332) loss_u loss_u 0.9487 (0.9599) acc_u 6.2500 (5.1562) lr 8.4357e-04 eta 0:00:19
epoch [112/200] batch [45/83] time 0.387 (0.465) data 0.256 (0.334) loss_u loss_u 0.9736 (0.9595) acc_u 3.1250 (5.2083) lr 8.4357e-04 eta 0:00:17
epoch [112/200] batch [50/83] time 0.426 (0.463) data 0.295 (0.332) loss_u loss_u 0.9243 (0.9592) acc_u 9.3750 (5.2500) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [55/83] time 0.349 (0.459) data 0.218 (0.328) loss_u loss_u 0.9937 (0.9608) acc_u 0.0000 (5.0568) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [60/83] time 0.397 (0.455) data 0.265 (0.324) loss_u loss_u 0.9766 (0.9607) acc_u 3.1250 (5.3125) lr 8.4357e-04 eta 0:00:10
epoch [112/200] batch [65/83] time 0.381 (0.452) data 0.251 (0.321) loss_u loss_u 0.9585 (0.9600) acc_u 3.1250 (5.3365) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [70/83] time 0.480 (0.455) data 0.348 (0.324) loss_u loss_u 0.9888 (0.9605) acc_u 3.1250 (5.2679) lr 8.4357e-04 eta 0:00:05
epoch [112/200] batch [75/83] time 0.400 (0.455) data 0.270 (0.324) loss_u loss_u 0.9868 (0.9620) acc_u 3.1250 (5.0833) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [80/83] time 0.407 (0.453) data 0.275 (0.322) loss_u loss_u 0.9536 (0.9619) acc_u 6.2500 (5.1172) lr 8.4357e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1758
confident_label rate tensor(0.1543, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 484
clean true:466
clean false:18
clean_rate:0.9628099173553719
noisy true:912
noisy false:1740
after delete: len(clean_dataset) 484
after delete: len(noisy_dataset) 2652
epoch [113/200] batch [5/15] time 0.484 (0.457) data 0.354 (0.327) loss_x loss_x 1.3809 (1.0584) acc_x 68.7500 (73.7500) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [10/15] time 0.414 (0.441) data 0.284 (0.311) loss_x loss_x 1.1309 (1.1424) acc_x 78.1250 (73.1250) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [15/15] time 0.409 (0.451) data 0.279 (0.321) loss_x loss_x 1.0586 (1.1462) acc_x 78.1250 (72.0833) lr 8.2807e-04 eta 0:00:00
epoch [113/200] batch [5/82] time 0.604 (0.462) data 0.473 (0.332) loss_u loss_u 0.9800 (0.9521) acc_u 3.1250 (6.2500) lr 8.2807e-04 eta 0:00:35
epoch [113/200] batch [10/82] time 0.397 (0.453) data 0.266 (0.323) loss_u loss_u 0.9849 (0.9560) acc_u 3.1250 (5.6250) lr 8.2807e-04 eta 0:00:32
epoch [113/200] batch [15/82] time 0.453 (0.452) data 0.322 (0.322) loss_u loss_u 0.9683 (0.9554) acc_u 0.0000 (5.2083) lr 8.2807e-04 eta 0:00:30
epoch [113/200] batch [20/82] time 0.581 (0.456) data 0.451 (0.325) loss_u loss_u 0.9854 (0.9553) acc_u 3.1250 (5.3125) lr 8.2807e-04 eta 0:00:28
epoch [113/200] batch [25/82] time 0.713 (0.462) data 0.582 (0.332) loss_u loss_u 0.9985 (0.9588) acc_u 0.0000 (5.0000) lr 8.2807e-04 eta 0:00:26
epoch [113/200] batch [30/82] time 0.486 (0.461) data 0.354 (0.330) loss_u loss_u 0.9995 (0.9627) acc_u 0.0000 (4.6875) lr 8.2807e-04 eta 0:00:23
epoch [113/200] batch [35/82] time 0.535 (0.461) data 0.404 (0.330) loss_u loss_u 0.9541 (0.9631) acc_u 3.1250 (4.6429) lr 8.2807e-04 eta 0:00:21
epoch [113/200] batch [40/82] time 0.362 (0.463) data 0.231 (0.332) loss_u loss_u 0.9648 (0.9636) acc_u 3.1250 (4.5312) lr 8.2807e-04 eta 0:00:19
epoch [113/200] batch [45/82] time 0.460 (0.464) data 0.330 (0.333) loss_u loss_u 0.9204 (0.9628) acc_u 12.5000 (4.6528) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [50/82] time 0.334 (0.457) data 0.204 (0.327) loss_u loss_u 0.9990 (0.9630) acc_u 0.0000 (4.5000) lr 8.2807e-04 eta 0:00:14
epoch [113/200] batch [55/82] time 0.453 (0.458) data 0.322 (0.328) loss_u loss_u 0.9487 (0.9632) acc_u 6.2500 (4.5455) lr 8.2807e-04 eta 0:00:12
epoch [113/200] batch [60/82] time 0.395 (0.457) data 0.265 (0.326) loss_u loss_u 0.9980 (0.9633) acc_u 0.0000 (4.4792) lr 8.2807e-04 eta 0:00:10
epoch [113/200] batch [65/82] time 0.427 (0.456) data 0.297 (0.325) loss_u loss_u 0.9429 (0.9612) acc_u 9.3750 (4.8558) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [70/82] time 0.356 (0.454) data 0.226 (0.323) loss_u loss_u 0.9912 (0.9625) acc_u 0.0000 (4.5982) lr 8.2807e-04 eta 0:00:05
epoch [113/200] batch [75/82] time 0.423 (0.451) data 0.293 (0.321) loss_u loss_u 0.9429 (0.9629) acc_u 6.2500 (4.5417) lr 8.2807e-04 eta 0:00:03
epoch [113/200] batch [80/82] time 0.464 (0.450) data 0.334 (0.319) loss_u loss_u 0.9609 (0.9627) acc_u 6.2500 (4.6094) lr 8.2807e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1727
confident_label rate tensor(0.1521, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 477
clean true:461
clean false:16
clean_rate:0.9664570230607966
noisy true:948
noisy false:1711
after delete: len(clean_dataset) 477
after delete: len(noisy_dataset) 2659
epoch [114/200] batch [5/14] time 0.380 (0.411) data 0.249 (0.281) loss_x loss_x 1.0967 (1.2519) acc_x 75.0000 (67.5000) lr 8.1262e-04 eta 0:00:03
epoch [114/200] batch [10/14] time 0.588 (0.434) data 0.458 (0.303) loss_x loss_x 1.1924 (1.1876) acc_x 78.1250 (72.8125) lr 8.1262e-04 eta 0:00:01
epoch [114/200] batch [5/83] time 0.481 (0.461) data 0.349 (0.330) loss_u loss_u 0.9185 (0.9696) acc_u 9.3750 (4.3750) lr 8.1262e-04 eta 0:00:35
epoch [114/200] batch [10/83] time 0.349 (0.450) data 0.217 (0.319) loss_u loss_u 0.9976 (0.9644) acc_u 0.0000 (5.0000) lr 8.1262e-04 eta 0:00:32
epoch [114/200] batch [15/83] time 0.352 (0.448) data 0.221 (0.317) loss_u loss_u 0.9385 (0.9638) acc_u 12.5000 (5.4167) lr 8.1262e-04 eta 0:00:30
epoch [114/200] batch [20/83] time 0.655 (0.465) data 0.523 (0.334) loss_u loss_u 0.9517 (0.9568) acc_u 6.2500 (6.4062) lr 8.1262e-04 eta 0:00:29
epoch [114/200] batch [25/83] time 0.667 (0.470) data 0.535 (0.339) loss_u loss_u 0.9429 (0.9576) acc_u 9.3750 (6.0000) lr 8.1262e-04 eta 0:00:27
epoch [114/200] batch [30/83] time 0.521 (0.466) data 0.390 (0.335) loss_u loss_u 0.9585 (0.9578) acc_u 6.2500 (5.9375) lr 8.1262e-04 eta 0:00:24
epoch [114/200] batch [35/83] time 0.396 (0.463) data 0.265 (0.331) loss_u loss_u 0.9702 (0.9603) acc_u 3.1250 (5.5357) lr 8.1262e-04 eta 0:00:22
epoch [114/200] batch [40/83] time 0.405 (0.457) data 0.274 (0.326) loss_u loss_u 0.9521 (0.9604) acc_u 9.3750 (5.4688) lr 8.1262e-04 eta 0:00:19
epoch [114/200] batch [45/83] time 0.535 (0.456) data 0.403 (0.325) loss_u loss_u 0.8911 (0.9580) acc_u 15.6250 (5.7639) lr 8.1262e-04 eta 0:00:17
epoch [114/200] batch [50/83] time 0.374 (0.455) data 0.244 (0.324) loss_u loss_u 0.9458 (0.9586) acc_u 6.2500 (5.5625) lr 8.1262e-04 eta 0:00:15
epoch [114/200] batch [55/83] time 0.409 (0.457) data 0.279 (0.326) loss_u loss_u 0.9399 (0.9596) acc_u 12.5000 (5.5114) lr 8.1262e-04 eta 0:00:12
epoch [114/200] batch [60/83] time 0.492 (0.459) data 0.361 (0.328) loss_u loss_u 0.9810 (0.9615) acc_u 3.1250 (5.2083) lr 8.1262e-04 eta 0:00:10
epoch [114/200] batch [65/83] time 0.432 (0.457) data 0.301 (0.326) loss_u loss_u 0.9805 (0.9629) acc_u 3.1250 (4.9519) lr 8.1262e-04 eta 0:00:08
epoch [114/200] batch [70/83] time 0.460 (0.458) data 0.329 (0.327) loss_u loss_u 0.9565 (0.9624) acc_u 9.3750 (5.0446) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [75/83] time 0.330 (0.454) data 0.198 (0.323) loss_u loss_u 0.9648 (0.9629) acc_u 3.1250 (4.9167) lr 8.1262e-04 eta 0:00:03
epoch [114/200] batch [80/83] time 0.413 (0.452) data 0.282 (0.321) loss_u loss_u 0.9800 (0.9636) acc_u 3.1250 (4.8438) lr 8.1262e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1734
confident_label rate tensor(0.1562, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 490
clean true:470
clean false:20
clean_rate:0.9591836734693877
noisy true:932
noisy false:1714
after delete: len(clean_dataset) 490
after delete: len(noisy_dataset) 2646
epoch [115/200] batch [5/15] time 0.449 (0.520) data 0.318 (0.389) loss_x loss_x 1.4980 (1.2762) acc_x 53.1250 (65.0000) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [10/15] time 0.580 (0.503) data 0.450 (0.373) loss_x loss_x 1.4697 (1.2229) acc_x 75.0000 (69.3750) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [15/15] time 0.355 (0.489) data 0.224 (0.358) loss_x loss_x 1.3545 (1.2670) acc_x 68.7500 (70.0000) lr 7.9721e-04 eta 0:00:00
epoch [115/200] batch [5/82] time 0.385 (0.475) data 0.254 (0.344) loss_u loss_u 0.9795 (0.9758) acc_u 3.1250 (4.3750) lr 7.9721e-04 eta 0:00:36
epoch [115/200] batch [10/82] time 0.480 (0.474) data 0.349 (0.343) loss_u loss_u 0.9160 (0.9655) acc_u 9.3750 (5.0000) lr 7.9721e-04 eta 0:00:34
epoch [115/200] batch [15/82] time 0.493 (0.466) data 0.362 (0.335) loss_u loss_u 0.9575 (0.9658) acc_u 9.3750 (4.7917) lr 7.9721e-04 eta 0:00:31
epoch [115/200] batch [20/82] time 0.394 (0.467) data 0.262 (0.336) loss_u loss_u 0.9536 (0.9644) acc_u 9.3750 (4.8438) lr 7.9721e-04 eta 0:00:28
epoch [115/200] batch [25/82] time 0.423 (0.465) data 0.291 (0.334) loss_u loss_u 0.9443 (0.9646) acc_u 9.3750 (5.0000) lr 7.9721e-04 eta 0:00:26
epoch [115/200] batch [30/82] time 0.716 (0.467) data 0.586 (0.336) loss_u loss_u 0.9634 (0.9629) acc_u 3.1250 (5.2083) lr 7.9721e-04 eta 0:00:24
epoch [115/200] batch [35/82] time 0.568 (0.464) data 0.438 (0.334) loss_u loss_u 0.9883 (0.9630) acc_u 0.0000 (5.1786) lr 7.9721e-04 eta 0:00:21
epoch [115/200] batch [40/82] time 0.561 (0.464) data 0.430 (0.333) loss_u loss_u 0.9858 (0.9637) acc_u 3.1250 (5.0781) lr 7.9721e-04 eta 0:00:19
epoch [115/200] batch [45/82] time 0.368 (0.459) data 0.237 (0.329) loss_u loss_u 0.9585 (0.9621) acc_u 3.1250 (5.2083) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [50/82] time 0.430 (0.462) data 0.299 (0.331) loss_u loss_u 0.9790 (0.9608) acc_u 3.1250 (5.2500) lr 7.9721e-04 eta 0:00:14
epoch [115/200] batch [55/82] time 0.452 (0.460) data 0.321 (0.329) loss_u loss_u 0.9546 (0.9604) acc_u 6.2500 (5.3409) lr 7.9721e-04 eta 0:00:12
epoch [115/200] batch [60/82] time 0.347 (0.459) data 0.216 (0.328) loss_u loss_u 0.9937 (0.9621) acc_u 0.0000 (5.0521) lr 7.9721e-04 eta 0:00:10
epoch [115/200] batch [65/82] time 0.477 (0.462) data 0.345 (0.331) loss_u loss_u 0.9434 (0.9624) acc_u 6.2500 (4.9038) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [70/82] time 0.372 (0.459) data 0.241 (0.328) loss_u loss_u 0.9531 (0.9627) acc_u 6.2500 (4.8661) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [75/82] time 0.325 (0.456) data 0.193 (0.325) loss_u loss_u 0.9678 (0.9632) acc_u 3.1250 (4.7500) lr 7.9721e-04 eta 0:00:03
epoch [115/200] batch [80/82] time 0.332 (0.453) data 0.201 (0.322) loss_u loss_u 0.9756 (0.9618) acc_u 0.0000 (4.8828) lr 7.9721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1710
confident_label rate tensor(0.1502, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 471
clean true:457
clean false:14
clean_rate:0.970276008492569
noisy true:969
noisy false:1696
after delete: len(clean_dataset) 471
after delete: len(noisy_dataset) 2665
epoch [116/200] batch [5/14] time 0.410 (0.480) data 0.281 (0.350) loss_x loss_x 1.7441 (1.3029) acc_x 53.1250 (63.7500) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [10/14] time 0.371 (0.453) data 0.242 (0.323) loss_x loss_x 1.2598 (1.2145) acc_x 78.1250 (70.0000) lr 7.8186e-04 eta 0:00:01
epoch [116/200] batch [5/83] time 0.540 (0.488) data 0.409 (0.358) loss_u loss_u 0.9375 (0.9473) acc_u 6.2500 (7.5000) lr 7.8186e-04 eta 0:00:38
epoch [116/200] batch [10/83] time 0.499 (0.486) data 0.368 (0.356) loss_u loss_u 0.9175 (0.9491) acc_u 6.2500 (6.8750) lr 7.8186e-04 eta 0:00:35
epoch [116/200] batch [15/83] time 0.344 (0.478) data 0.212 (0.347) loss_u loss_u 0.9438 (0.9521) acc_u 6.2500 (6.0417) lr 7.8186e-04 eta 0:00:32
epoch [116/200] batch [20/83] time 0.512 (0.474) data 0.381 (0.343) loss_u loss_u 0.9438 (0.9532) acc_u 6.2500 (6.4062) lr 7.8186e-04 eta 0:00:29
epoch [116/200] batch [25/83] time 0.429 (0.468) data 0.297 (0.337) loss_u loss_u 0.9653 (0.9480) acc_u 3.1250 (7.1250) lr 7.8186e-04 eta 0:00:27
epoch [116/200] batch [30/83] time 0.460 (0.462) data 0.329 (0.331) loss_u loss_u 0.9854 (0.9480) acc_u 3.1250 (6.8750) lr 7.8186e-04 eta 0:00:24
epoch [116/200] batch [35/83] time 0.487 (0.460) data 0.355 (0.329) loss_u loss_u 0.9492 (0.9489) acc_u 6.2500 (6.6964) lr 7.8186e-04 eta 0:00:22
epoch [116/200] batch [40/83] time 0.347 (0.458) data 0.215 (0.327) loss_u loss_u 0.9575 (0.9507) acc_u 6.2500 (6.3281) lr 7.8186e-04 eta 0:00:19
epoch [116/200] batch [45/83] time 0.358 (0.454) data 0.227 (0.323) loss_u loss_u 0.9731 (0.9516) acc_u 3.1250 (6.3194) lr 7.8186e-04 eta 0:00:17
epoch [116/200] batch [50/83] time 0.718 (0.456) data 0.586 (0.325) loss_u loss_u 0.9990 (0.9549) acc_u 0.0000 (5.8750) lr 7.8186e-04 eta 0:00:15
epoch [116/200] batch [55/83] time 0.411 (0.458) data 0.279 (0.327) loss_u loss_u 0.9624 (0.9565) acc_u 6.2500 (5.6250) lr 7.8186e-04 eta 0:00:12
epoch [116/200] batch [60/83] time 0.374 (0.455) data 0.244 (0.324) loss_u loss_u 0.9482 (0.9569) acc_u 6.2500 (5.4688) lr 7.8186e-04 eta 0:00:10
epoch [116/200] batch [65/83] time 0.414 (0.453) data 0.284 (0.322) loss_u loss_u 0.8965 (0.9571) acc_u 12.5000 (5.4327) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [70/83] time 0.469 (0.452) data 0.339 (0.321) loss_u loss_u 0.9795 (0.9587) acc_u 6.2500 (5.2679) lr 7.8186e-04 eta 0:00:05
epoch [116/200] batch [75/83] time 0.415 (0.451) data 0.284 (0.320) loss_u loss_u 0.9204 (0.9594) acc_u 9.3750 (5.1667) lr 7.8186e-04 eta 0:00:03
epoch [116/200] batch [80/83] time 0.453 (0.452) data 0.323 (0.321) loss_u loss_u 0.9971 (0.9598) acc_u 0.0000 (5.0781) lr 7.8186e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1716
confident_label rate tensor(0.1547, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 485
clean true:468
clean false:17
clean_rate:0.9649484536082474
noisy true:952
noisy false:1699
after delete: len(clean_dataset) 485
after delete: len(noisy_dataset) 2651
epoch [117/200] batch [5/15] time 0.386 (0.451) data 0.256 (0.321) loss_x loss_x 1.2139 (1.2541) acc_x 71.8750 (71.2500) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [10/15] time 0.419 (0.447) data 0.288 (0.316) loss_x loss_x 1.2910 (1.2429) acc_x 75.0000 (71.8750) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [15/15] time 0.434 (0.452) data 0.304 (0.322) loss_x loss_x 1.3281 (1.2601) acc_x 68.7500 (71.0417) lr 7.6655e-04 eta 0:00:00
epoch [117/200] batch [5/82] time 0.522 (0.444) data 0.391 (0.313) loss_u loss_u 0.9126 (0.9342) acc_u 9.3750 (7.5000) lr 7.6655e-04 eta 0:00:34
epoch [117/200] batch [10/82] time 0.499 (0.437) data 0.369 (0.306) loss_u loss_u 0.9033 (0.9457) acc_u 15.6250 (6.5625) lr 7.6655e-04 eta 0:00:31
epoch [117/200] batch [15/82] time 0.319 (0.428) data 0.189 (0.297) loss_u loss_u 0.9399 (0.9501) acc_u 6.2500 (6.0417) lr 7.6655e-04 eta 0:00:28
epoch [117/200] batch [20/82] time 0.343 (0.437) data 0.212 (0.306) loss_u loss_u 0.9531 (0.9512) acc_u 6.2500 (5.7812) lr 7.6655e-04 eta 0:00:27
epoch [117/200] batch [25/82] time 0.479 (0.435) data 0.348 (0.304) loss_u loss_u 0.9614 (0.9566) acc_u 6.2500 (5.2500) lr 7.6655e-04 eta 0:00:24
epoch [117/200] batch [30/82] time 0.412 (0.441) data 0.281 (0.310) loss_u loss_u 0.9829 (0.9581) acc_u 0.0000 (5.1042) lr 7.6655e-04 eta 0:00:22
epoch [117/200] batch [35/82] time 0.395 (0.438) data 0.264 (0.307) loss_u loss_u 0.9365 (0.9570) acc_u 9.3750 (5.2679) lr 7.6655e-04 eta 0:00:20
epoch [117/200] batch [40/82] time 0.592 (0.435) data 0.461 (0.304) loss_u loss_u 0.9688 (0.9581) acc_u 3.1250 (5.2344) lr 7.6655e-04 eta 0:00:18
epoch [117/200] batch [45/82] time 0.476 (0.437) data 0.345 (0.306) loss_u loss_u 0.9775 (0.9610) acc_u 3.1250 (4.8611) lr 7.6655e-04 eta 0:00:16
epoch [117/200] batch [50/82] time 0.463 (0.437) data 0.332 (0.306) loss_u loss_u 0.9224 (0.9593) acc_u 9.3750 (5.1250) lr 7.6655e-04 eta 0:00:13
epoch [117/200] batch [55/82] time 0.600 (0.443) data 0.468 (0.312) loss_u loss_u 0.9888 (0.9610) acc_u 3.1250 (4.8864) lr 7.6655e-04 eta 0:00:11
epoch [117/200] batch [60/82] time 0.541 (0.445) data 0.410 (0.314) loss_u loss_u 0.9980 (0.9624) acc_u 0.0000 (4.7917) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [65/82] time 0.448 (0.448) data 0.318 (0.317) loss_u loss_u 0.9873 (0.9619) acc_u 3.1250 (4.9519) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [70/82] time 0.447 (0.447) data 0.317 (0.316) loss_u loss_u 0.9565 (0.9614) acc_u 6.2500 (5.0446) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [75/82] time 0.523 (0.444) data 0.393 (0.314) loss_u loss_u 0.9736 (0.9617) acc_u 3.1250 (5.0000) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [80/82] time 0.526 (0.447) data 0.395 (0.316) loss_u loss_u 0.9883 (0.9617) acc_u 3.1250 (5.0000) lr 7.6655e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1711
confident_label rate tensor(0.1623, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 509
clean true:496
clean false:13
clean_rate:0.9744597249508841
noisy true:929
noisy false:1698
after delete: len(clean_dataset) 509
after delete: len(noisy_dataset) 2627
epoch [118/200] batch [5/15] time 0.535 (0.493) data 0.404 (0.363) loss_x loss_x 1.0742 (1.2809) acc_x 65.6250 (66.8750) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [10/15] time 0.332 (0.464) data 0.202 (0.334) loss_x loss_x 1.5254 (1.3619) acc_x 78.1250 (67.5000) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [15/15] time 0.478 (0.476) data 0.348 (0.346) loss_x loss_x 0.8008 (1.2498) acc_x 78.1250 (68.9583) lr 7.5131e-04 eta 0:00:00
epoch [118/200] batch [5/82] time 0.488 (0.466) data 0.357 (0.336) loss_u loss_u 0.9878 (0.9729) acc_u 3.1250 (4.3750) lr 7.5131e-04 eta 0:00:35
epoch [118/200] batch [10/82] time 0.501 (0.464) data 0.371 (0.334) loss_u loss_u 0.9863 (0.9719) acc_u 3.1250 (4.3750) lr 7.5131e-04 eta 0:00:33
epoch [118/200] batch [15/82] time 0.353 (0.461) data 0.222 (0.331) loss_u loss_u 0.9814 (0.9648) acc_u 3.1250 (5.4167) lr 7.5131e-04 eta 0:00:30
epoch [118/200] batch [20/82] time 0.403 (0.459) data 0.273 (0.329) loss_u loss_u 0.9160 (0.9648) acc_u 9.3750 (5.0000) lr 7.5131e-04 eta 0:00:28
epoch [118/200] batch [25/82] time 0.530 (0.454) data 0.400 (0.323) loss_u loss_u 0.9946 (0.9664) acc_u 0.0000 (4.7500) lr 7.5131e-04 eta 0:00:25
epoch [118/200] batch [30/82] time 0.454 (0.452) data 0.322 (0.321) loss_u loss_u 0.9697 (0.9660) acc_u 3.1250 (4.6875) lr 7.5131e-04 eta 0:00:23
epoch [118/200] batch [35/82] time 0.534 (0.456) data 0.403 (0.326) loss_u loss_u 0.9473 (0.9662) acc_u 6.2500 (4.4643) lr 7.5131e-04 eta 0:00:21
epoch [118/200] batch [40/82] time 0.743 (0.462) data 0.610 (0.331) loss_u loss_u 0.9800 (0.9646) acc_u 3.1250 (4.6094) lr 7.5131e-04 eta 0:00:19
epoch [118/200] batch [45/82] time 0.423 (0.460) data 0.292 (0.329) loss_u loss_u 0.9751 (0.9651) acc_u 9.3750 (4.7222) lr 7.5131e-04 eta 0:00:17
epoch [118/200] batch [50/82] time 0.418 (0.466) data 0.287 (0.335) loss_u loss_u 0.9971 (0.9642) acc_u 0.0000 (4.8125) lr 7.5131e-04 eta 0:00:14
epoch [118/200] batch [55/82] time 0.455 (0.468) data 0.324 (0.337) loss_u loss_u 0.9810 (0.9649) acc_u 0.0000 (4.6591) lr 7.5131e-04 eta 0:00:12
epoch [118/200] batch [60/82] time 0.423 (0.466) data 0.292 (0.335) loss_u loss_u 0.9482 (0.9652) acc_u 9.3750 (4.6354) lr 7.5131e-04 eta 0:00:10
epoch [118/200] batch [65/82] time 0.390 (0.461) data 0.260 (0.331) loss_u loss_u 0.9111 (0.9641) acc_u 9.3750 (4.7115) lr 7.5131e-04 eta 0:00:07
epoch [118/200] batch [70/82] time 0.432 (0.460) data 0.301 (0.329) loss_u loss_u 0.9946 (0.9649) acc_u 0.0000 (4.6429) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [75/82] time 0.463 (0.460) data 0.331 (0.329) loss_u loss_u 0.9692 (0.9654) acc_u 3.1250 (4.5833) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [80/82] time 0.339 (0.456) data 0.208 (0.325) loss_u loss_u 0.9668 (0.9655) acc_u 3.1250 (4.5703) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1739
confident_label rate tensor(0.1502, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 471
clean true:457
clean false:14
clean_rate:0.970276008492569
noisy true:940
noisy false:1725
after delete: len(clean_dataset) 471
after delete: len(noisy_dataset) 2665
epoch [119/200] batch [5/14] time 0.360 (0.420) data 0.229 (0.290) loss_x loss_x 1.2822 (1.0366) acc_x 68.7500 (73.7500) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [10/14] time 0.477 (0.449) data 0.346 (0.318) loss_x loss_x 0.6113 (1.0363) acc_x 93.7500 (73.7500) lr 7.3613e-04 eta 0:00:01
epoch [119/200] batch [5/83] time 0.395 (0.455) data 0.263 (0.324) loss_u loss_u 0.9839 (0.9821) acc_u 3.1250 (1.2500) lr 7.3613e-04 eta 0:00:35
epoch [119/200] batch [10/83] time 0.495 (0.446) data 0.363 (0.315) loss_u loss_u 0.9810 (0.9660) acc_u 3.1250 (4.0625) lr 7.3613e-04 eta 0:00:32
epoch [119/200] batch [15/83] time 0.482 (0.447) data 0.352 (0.316) loss_u loss_u 0.9824 (0.9642) acc_u 3.1250 (4.1667) lr 7.3613e-04 eta 0:00:30
epoch [119/200] batch [20/83] time 0.334 (0.444) data 0.204 (0.313) loss_u loss_u 0.9004 (0.9608) acc_u 15.6250 (5.0000) lr 7.3613e-04 eta 0:00:27
epoch [119/200] batch [25/83] time 0.599 (0.442) data 0.468 (0.311) loss_u loss_u 0.9756 (0.9599) acc_u 3.1250 (5.0000) lr 7.3613e-04 eta 0:00:25
epoch [119/200] batch [30/83] time 0.583 (0.445) data 0.452 (0.314) loss_u loss_u 0.9756 (0.9576) acc_u 3.1250 (5.2083) lr 7.3613e-04 eta 0:00:23
epoch [119/200] batch [35/83] time 0.614 (0.451) data 0.484 (0.320) loss_u loss_u 0.9897 (0.9600) acc_u 3.1250 (5.0000) lr 7.3613e-04 eta 0:00:21
epoch [119/200] batch [40/83] time 0.461 (0.449) data 0.331 (0.319) loss_u loss_u 0.9380 (0.9597) acc_u 9.3750 (5.0781) lr 7.3613e-04 eta 0:00:19
epoch [119/200] batch [45/83] time 0.508 (0.447) data 0.377 (0.316) loss_u loss_u 0.9414 (0.9591) acc_u 9.3750 (5.3472) lr 7.3613e-04 eta 0:00:16
epoch [119/200] batch [50/83] time 0.531 (0.446) data 0.401 (0.315) loss_u loss_u 0.9639 (0.9597) acc_u 3.1250 (5.1875) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [55/83] time 0.370 (0.445) data 0.239 (0.315) loss_u loss_u 0.9248 (0.9581) acc_u 12.5000 (5.5114) lr 7.3613e-04 eta 0:00:12
epoch [119/200] batch [60/83] time 0.346 (0.446) data 0.215 (0.316) loss_u loss_u 0.9722 (0.9598) acc_u 3.1250 (5.3125) lr 7.3613e-04 eta 0:00:10
epoch [119/200] batch [65/83] time 0.390 (0.448) data 0.260 (0.317) loss_u loss_u 0.9819 (0.9593) acc_u 6.2500 (5.4327) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [70/83] time 0.459 (0.446) data 0.328 (0.315) loss_u loss_u 0.9014 (0.9594) acc_u 12.5000 (5.4464) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [75/83] time 0.460 (0.447) data 0.328 (0.316) loss_u loss_u 0.8901 (0.9589) acc_u 9.3750 (5.4167) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [80/83] time 0.422 (0.446) data 0.291 (0.315) loss_u loss_u 0.9077 (0.9594) acc_u 12.5000 (5.3125) lr 7.3613e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1753
confident_label rate tensor(0.1524, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 478
clean true:457
clean false:21
clean_rate:0.9560669456066946
noisy true:926
noisy false:1732
after delete: len(clean_dataset) 478
after delete: len(noisy_dataset) 2658
epoch [120/200] batch [5/14] time 0.489 (0.454) data 0.358 (0.323) loss_x loss_x 0.8467 (1.0720) acc_x 84.3750 (77.5000) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [10/14] time 0.463 (0.479) data 0.332 (0.349) loss_x loss_x 1.5137 (1.1981) acc_x 71.8750 (73.7500) lr 7.2101e-04 eta 0:00:01
epoch [120/200] batch [5/83] time 0.427 (0.480) data 0.295 (0.349) loss_u loss_u 0.9648 (0.9574) acc_u 3.1250 (3.7500) lr 7.2101e-04 eta 0:00:37
epoch [120/200] batch [10/83] time 0.502 (0.499) data 0.371 (0.368) loss_u loss_u 0.9609 (0.9466) acc_u 6.2500 (6.5625) lr 7.2101e-04 eta 0:00:36
epoch [120/200] batch [15/83] time 0.444 (0.493) data 0.314 (0.363) loss_u loss_u 0.9614 (0.9508) acc_u 3.1250 (5.6250) lr 7.2101e-04 eta 0:00:33
epoch [120/200] batch [20/83] time 0.498 (0.498) data 0.366 (0.367) loss_u loss_u 0.9600 (0.9574) acc_u 3.1250 (4.8438) lr 7.2101e-04 eta 0:00:31
epoch [120/200] batch [25/83] time 0.420 (0.497) data 0.288 (0.366) loss_u loss_u 0.9702 (0.9584) acc_u 6.2500 (4.7500) lr 7.2101e-04 eta 0:00:28
epoch [120/200] batch [30/83] time 0.403 (0.489) data 0.271 (0.359) loss_u loss_u 0.9355 (0.9550) acc_u 9.3750 (5.5208) lr 7.2101e-04 eta 0:00:25
epoch [120/200] batch [35/83] time 0.485 (0.489) data 0.354 (0.358) loss_u loss_u 0.9468 (0.9570) acc_u 9.3750 (5.3571) lr 7.2101e-04 eta 0:00:23
epoch [120/200] batch [40/83] time 0.525 (0.486) data 0.394 (0.355) loss_u loss_u 0.9331 (0.9572) acc_u 6.2500 (5.3906) lr 7.2101e-04 eta 0:00:20
epoch [120/200] batch [45/83] time 0.470 (0.482) data 0.339 (0.351) loss_u loss_u 0.9619 (0.9575) acc_u 3.1250 (5.2083) lr 7.2101e-04 eta 0:00:18
epoch [120/200] batch [50/83] time 0.410 (0.479) data 0.279 (0.348) loss_u loss_u 0.9873 (0.9590) acc_u 0.0000 (4.9375) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [55/83] time 0.387 (0.476) data 0.257 (0.345) loss_u loss_u 0.9746 (0.9591) acc_u 3.1250 (5.0000) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [60/83] time 0.415 (0.471) data 0.283 (0.340) loss_u loss_u 0.9526 (0.9579) acc_u 3.1250 (5.1042) lr 7.2101e-04 eta 0:00:10
epoch [120/200] batch [65/83] time 0.370 (0.467) data 0.239 (0.336) loss_u loss_u 0.9824 (0.9589) acc_u 3.1250 (4.9519) lr 7.2101e-04 eta 0:00:08
epoch [120/200] batch [70/83] time 0.503 (0.467) data 0.372 (0.336) loss_u loss_u 0.8901 (0.9596) acc_u 15.6250 (4.9554) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [75/83] time 0.409 (0.462) data 0.278 (0.331) loss_u loss_u 0.9746 (0.9604) acc_u 0.0000 (4.7917) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [80/83] time 0.410 (0.458) data 0.279 (0.327) loss_u loss_u 0.9985 (0.9607) acc_u 0.0000 (4.7656) lr 7.2101e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1717
confident_label rate tensor(0.1537, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 482
clean true:463
clean false:19
clean_rate:0.9605809128630706
noisy true:956
noisy false:1698
after delete: len(clean_dataset) 482
after delete: len(noisy_dataset) 2654
epoch [121/200] batch [5/15] time 0.505 (0.460) data 0.375 (0.329) loss_x loss_x 1.2236 (1.3441) acc_x 78.1250 (71.2500) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [10/15] time 0.464 (0.451) data 0.335 (0.320) loss_x loss_x 1.3730 (1.2597) acc_x 71.8750 (70.6250) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [15/15] time 0.578 (0.451) data 0.448 (0.321) loss_x loss_x 1.1113 (1.1308) acc_x 78.1250 (73.7500) lr 7.0596e-04 eta 0:00:00
epoch [121/200] batch [5/82] time 0.390 (0.460) data 0.260 (0.330) loss_u loss_u 0.9761 (0.9599) acc_u 3.1250 (5.0000) lr 7.0596e-04 eta 0:00:35
epoch [121/200] batch [10/82] time 0.465 (0.457) data 0.335 (0.327) loss_u loss_u 0.9648 (0.9592) acc_u 6.2500 (5.0000) lr 7.0596e-04 eta 0:00:32
epoch [121/200] batch [15/82] time 0.446 (0.453) data 0.316 (0.323) loss_u loss_u 0.9995 (0.9709) acc_u 0.0000 (3.5417) lr 7.0596e-04 eta 0:00:30
epoch [121/200] batch [20/82] time 0.655 (0.452) data 0.524 (0.322) loss_u loss_u 0.9780 (0.9743) acc_u 3.1250 (3.1250) lr 7.0596e-04 eta 0:00:28
epoch [121/200] batch [25/82] time 0.396 (0.446) data 0.264 (0.315) loss_u loss_u 0.9209 (0.9718) acc_u 9.3750 (3.5000) lr 7.0596e-04 eta 0:00:25
epoch [121/200] batch [30/82] time 0.349 (0.447) data 0.218 (0.317) loss_u loss_u 0.9492 (0.9702) acc_u 6.2500 (3.6458) lr 7.0596e-04 eta 0:00:23
epoch [121/200] batch [35/82] time 0.447 (0.450) data 0.315 (0.320) loss_u loss_u 0.9409 (0.9687) acc_u 6.2500 (3.7500) lr 7.0596e-04 eta 0:00:21
epoch [121/200] batch [40/82] time 0.444 (0.452) data 0.314 (0.322) loss_u loss_u 0.9458 (0.9678) acc_u 9.3750 (3.9844) lr 7.0596e-04 eta 0:00:18
epoch [121/200] batch [45/82] time 0.514 (0.452) data 0.384 (0.322) loss_u loss_u 0.9229 (0.9610) acc_u 9.3750 (4.9306) lr 7.0596e-04 eta 0:00:16
epoch [121/200] batch [50/82] time 0.395 (0.452) data 0.264 (0.322) loss_u loss_u 0.9536 (0.9602) acc_u 6.2500 (5.0000) lr 7.0596e-04 eta 0:00:14
epoch [121/200] batch [55/82] time 0.379 (0.454) data 0.248 (0.324) loss_u loss_u 0.9043 (0.9594) acc_u 12.5000 (5.1136) lr 7.0596e-04 eta 0:00:12
epoch [121/200] batch [60/82] time 0.507 (0.454) data 0.377 (0.323) loss_u loss_u 0.8940 (0.9576) acc_u 12.5000 (5.4167) lr 7.0596e-04 eta 0:00:09
epoch [121/200] batch [65/82] time 0.526 (0.454) data 0.395 (0.323) loss_u loss_u 0.9937 (0.9576) acc_u 0.0000 (5.4327) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [70/82] time 0.451 (0.451) data 0.319 (0.320) loss_u loss_u 0.9263 (0.9578) acc_u 6.2500 (5.3125) lr 7.0596e-04 eta 0:00:05
epoch [121/200] batch [75/82] time 0.379 (0.450) data 0.248 (0.319) loss_u loss_u 0.8955 (0.9569) acc_u 12.5000 (5.3750) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [80/82] time 0.393 (0.449) data 0.263 (0.318) loss_u loss_u 0.9536 (0.9575) acc_u 6.2500 (5.2344) lr 7.0596e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1724
confident_label rate tensor(0.1537, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 482
clean true:466
clean false:16
clean_rate:0.966804979253112
noisy true:946
noisy false:1708
after delete: len(clean_dataset) 482
after delete: len(noisy_dataset) 2654
epoch [122/200] batch [5/15] time 0.545 (0.472) data 0.414 (0.342) loss_x loss_x 0.8228 (0.8822) acc_x 75.0000 (77.5000) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [10/15] time 0.325 (0.456) data 0.195 (0.326) loss_x loss_x 0.8066 (0.8055) acc_x 81.2500 (80.6250) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [15/15] time 0.423 (0.449) data 0.292 (0.319) loss_x loss_x 1.0986 (0.8993) acc_x 78.1250 (77.5000) lr 6.9098e-04 eta 0:00:00
epoch [122/200] batch [5/82] time 0.371 (0.447) data 0.240 (0.317) loss_u loss_u 0.9766 (0.9706) acc_u 3.1250 (4.3750) lr 6.9098e-04 eta 0:00:34
epoch [122/200] batch [10/82] time 0.485 (0.442) data 0.354 (0.312) loss_u loss_u 0.9814 (0.9697) acc_u 3.1250 (4.0625) lr 6.9098e-04 eta 0:00:31
epoch [122/200] batch [15/82] time 0.408 (0.448) data 0.278 (0.317) loss_u loss_u 0.9683 (0.9642) acc_u 3.1250 (4.3750) lr 6.9098e-04 eta 0:00:29
epoch [122/200] batch [20/82] time 0.579 (0.459) data 0.448 (0.329) loss_u loss_u 0.9482 (0.9681) acc_u 9.3750 (3.9062) lr 6.9098e-04 eta 0:00:28
epoch [122/200] batch [25/82] time 0.424 (0.459) data 0.293 (0.329) loss_u loss_u 0.9800 (0.9696) acc_u 3.1250 (3.7500) lr 6.9098e-04 eta 0:00:26
epoch [122/200] batch [30/82] time 0.624 (0.473) data 0.493 (0.342) loss_u loss_u 0.9731 (0.9665) acc_u 3.1250 (4.2708) lr 6.9098e-04 eta 0:00:24
epoch [122/200] batch [35/82] time 0.675 (0.472) data 0.544 (0.341) loss_u loss_u 0.9238 (0.9642) acc_u 12.5000 (4.6429) lr 6.9098e-04 eta 0:00:22
epoch [122/200] batch [40/82] time 0.487 (0.472) data 0.357 (0.341) loss_u loss_u 0.9937 (0.9642) acc_u 0.0000 (4.6094) lr 6.9098e-04 eta 0:00:19
epoch [122/200] batch [45/82] time 0.321 (0.468) data 0.190 (0.337) loss_u loss_u 0.9707 (0.9643) acc_u 3.1250 (4.5139) lr 6.9098e-04 eta 0:00:17
epoch [122/200] batch [50/82] time 0.556 (0.472) data 0.424 (0.341) loss_u loss_u 0.9111 (0.9631) acc_u 12.5000 (4.7500) lr 6.9098e-04 eta 0:00:15
epoch [122/200] batch [55/82] time 0.423 (0.473) data 0.293 (0.342) loss_u loss_u 0.9590 (0.9617) acc_u 9.3750 (5.0000) lr 6.9098e-04 eta 0:00:12
epoch [122/200] batch [60/82] time 0.490 (0.470) data 0.359 (0.339) loss_u loss_u 0.9443 (0.9614) acc_u 6.2500 (5.0000) lr 6.9098e-04 eta 0:00:10
epoch [122/200] batch [65/82] time 0.408 (0.471) data 0.277 (0.340) loss_u loss_u 0.9956 (0.9618) acc_u 0.0000 (4.9038) lr 6.9098e-04 eta 0:00:08
epoch [122/200] batch [70/82] time 0.546 (0.469) data 0.415 (0.339) loss_u loss_u 0.9556 (0.9608) acc_u 3.1250 (4.9107) lr 6.9098e-04 eta 0:00:05
epoch [122/200] batch [75/82] time 0.479 (0.465) data 0.349 (0.334) loss_u loss_u 0.9624 (0.9612) acc_u 3.1250 (4.8750) lr 6.9098e-04 eta 0:00:03
epoch [122/200] batch [80/82] time 0.574 (0.462) data 0.443 (0.332) loss_u loss_u 0.9600 (0.9619) acc_u 3.1250 (4.7266) lr 6.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1723
confident_label rate tensor(0.1575, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 494
clean true:480
clean false:14
clean_rate:0.97165991902834
noisy true:933
noisy false:1709
after delete: len(clean_dataset) 494
after delete: len(noisy_dataset) 2642
epoch [123/200] batch [5/15] time 0.435 (0.471) data 0.305 (0.340) loss_x loss_x 1.4434 (1.1267) acc_x 68.7500 (71.2500) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [10/15] time 0.399 (0.466) data 0.269 (0.336) loss_x loss_x 1.4521 (1.1729) acc_x 62.5000 (70.6250) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [15/15] time 0.526 (0.458) data 0.394 (0.328) loss_x loss_x 0.9185 (1.1349) acc_x 78.1250 (70.4167) lr 6.7608e-04 eta 0:00:00
epoch [123/200] batch [5/82] time 0.477 (0.454) data 0.346 (0.324) loss_u loss_u 0.9551 (0.9694) acc_u 6.2500 (4.3750) lr 6.7608e-04 eta 0:00:34
epoch [123/200] batch [10/82] time 0.416 (0.458) data 0.285 (0.327) loss_u loss_u 0.9756 (0.9664) acc_u 3.1250 (4.3750) lr 6.7608e-04 eta 0:00:32
epoch [123/200] batch [15/82] time 0.416 (0.461) data 0.285 (0.330) loss_u loss_u 0.9131 (0.9566) acc_u 12.5000 (5.4167) lr 6.7608e-04 eta 0:00:30
epoch [123/200] batch [20/82] time 0.453 (0.471) data 0.322 (0.340) loss_u loss_u 0.9932 (0.9570) acc_u 0.0000 (5.1562) lr 6.7608e-04 eta 0:00:29
epoch [123/200] batch [25/82] time 0.434 (0.475) data 0.303 (0.344) loss_u loss_u 0.9810 (0.9594) acc_u 3.1250 (4.7500) lr 6.7608e-04 eta 0:00:27
epoch [123/200] batch [30/82] time 0.430 (0.468) data 0.299 (0.337) loss_u loss_u 0.9521 (0.9604) acc_u 6.2500 (4.8958) lr 6.7608e-04 eta 0:00:24
epoch [123/200] batch [35/82] time 0.470 (0.462) data 0.338 (0.331) loss_u loss_u 0.9814 (0.9590) acc_u 3.1250 (5.0000) lr 6.7608e-04 eta 0:00:21
epoch [123/200] batch [40/82] time 0.530 (0.466) data 0.398 (0.334) loss_u loss_u 0.9741 (0.9591) acc_u 6.2500 (5.1562) lr 6.7608e-04 eta 0:00:19
epoch [123/200] batch [45/82] time 0.500 (0.464) data 0.369 (0.333) loss_u loss_u 0.9590 (0.9603) acc_u 3.1250 (5.0000) lr 6.7608e-04 eta 0:00:17
epoch [123/200] batch [50/82] time 0.362 (0.460) data 0.231 (0.329) loss_u loss_u 0.9990 (0.9614) acc_u 0.0000 (4.8125) lr 6.7608e-04 eta 0:00:14
epoch [123/200] batch [55/82] time 0.474 (0.459) data 0.343 (0.328) loss_u loss_u 0.9790 (0.9629) acc_u 3.1250 (4.6023) lr 6.7608e-04 eta 0:00:12
epoch [123/200] batch [60/82] time 0.436 (0.458) data 0.305 (0.327) loss_u loss_u 0.9541 (0.9627) acc_u 6.2500 (4.6354) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [65/82] time 0.437 (0.456) data 0.306 (0.325) loss_u loss_u 0.9941 (0.9629) acc_u 0.0000 (4.5673) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [70/82] time 0.462 (0.456) data 0.332 (0.324) loss_u loss_u 0.9443 (0.9635) acc_u 9.3750 (4.5982) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [75/82] time 0.426 (0.453) data 0.294 (0.322) loss_u loss_u 0.9707 (0.9645) acc_u 3.1250 (4.4583) lr 6.7608e-04 eta 0:00:03
epoch [123/200] batch [80/82] time 0.456 (0.452) data 0.326 (0.321) loss_u loss_u 0.9873 (0.9647) acc_u 0.0000 (4.3750) lr 6.7608e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1710
confident_label rate tensor(0.1540, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 483
clean true:470
clean false:13
clean_rate:0.9730848861283644
noisy true:956
noisy false:1697
after delete: len(clean_dataset) 483
after delete: len(noisy_dataset) 2653
epoch [124/200] batch [5/15] time 0.427 (0.471) data 0.297 (0.340) loss_x loss_x 1.4121 (1.3838) acc_x 62.5000 (66.2500) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [10/15] time 0.496 (0.474) data 0.366 (0.344) loss_x loss_x 1.3799 (1.3066) acc_x 71.8750 (69.0625) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [15/15] time 0.474 (0.468) data 0.343 (0.338) loss_x loss_x 0.9756 (1.2305) acc_x 78.1250 (71.4583) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/82] time 0.551 (0.464) data 0.419 (0.333) loss_u loss_u 0.9443 (0.9704) acc_u 9.3750 (4.3750) lr 6.6126e-04 eta 0:00:35
epoch [124/200] batch [10/82] time 0.395 (0.450) data 0.264 (0.320) loss_u loss_u 0.9014 (0.9599) acc_u 15.6250 (5.9375) lr 6.6126e-04 eta 0:00:32
epoch [124/200] batch [15/82] time 0.392 (0.444) data 0.261 (0.313) loss_u loss_u 0.9521 (0.9565) acc_u 6.2500 (6.0417) lr 6.6126e-04 eta 0:00:29
epoch [124/200] batch [20/82] time 0.401 (0.447) data 0.269 (0.317) loss_u loss_u 0.9932 (0.9585) acc_u 0.0000 (5.3125) lr 6.6126e-04 eta 0:00:27
epoch [124/200] batch [25/82] time 0.351 (0.450) data 0.221 (0.319) loss_u loss_u 0.9009 (0.9561) acc_u 15.6250 (5.8750) lr 6.6126e-04 eta 0:00:25
epoch [124/200] batch [30/82] time 0.402 (0.448) data 0.270 (0.317) loss_u loss_u 0.9146 (0.9544) acc_u 12.5000 (6.2500) lr 6.6126e-04 eta 0:00:23
epoch [124/200] batch [35/82] time 0.375 (0.456) data 0.243 (0.325) loss_u loss_u 0.9683 (0.9568) acc_u 6.2500 (5.9821) lr 6.6126e-04 eta 0:00:21
epoch [124/200] batch [40/82] time 0.360 (0.450) data 0.228 (0.319) loss_u loss_u 0.9238 (0.9559) acc_u 9.3750 (6.3281) lr 6.6126e-04 eta 0:00:18
epoch [124/200] batch [45/82] time 0.613 (0.449) data 0.481 (0.318) loss_u loss_u 0.9209 (0.9562) acc_u 12.5000 (6.3194) lr 6.6126e-04 eta 0:00:16
epoch [124/200] batch [50/82] time 0.465 (0.450) data 0.335 (0.319) loss_u loss_u 0.9741 (0.9578) acc_u 3.1250 (5.9375) lr 6.6126e-04 eta 0:00:14
epoch [124/200] batch [55/82] time 0.488 (0.456) data 0.357 (0.325) loss_u loss_u 0.9517 (0.9584) acc_u 6.2500 (5.7955) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [60/82] time 0.537 (0.458) data 0.406 (0.327) loss_u loss_u 0.9692 (0.9604) acc_u 3.1250 (5.4688) lr 6.6126e-04 eta 0:00:10
epoch [124/200] batch [65/82] time 0.504 (0.458) data 0.373 (0.327) loss_u loss_u 0.9746 (0.9614) acc_u 3.1250 (5.3365) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [70/82] time 0.420 (0.456) data 0.288 (0.325) loss_u loss_u 0.9731 (0.9606) acc_u 0.0000 (5.3125) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [75/82] time 0.444 (0.457) data 0.312 (0.326) loss_u loss_u 0.9551 (0.9614) acc_u 6.2500 (5.1667) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [80/82] time 0.386 (0.455) data 0.255 (0.323) loss_u loss_u 0.9292 (0.9610) acc_u 9.3750 (5.1953) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1769
confident_label rate tensor(0.1502, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 471
clean true:458
clean false:13
clean_rate:0.9723991507430998
noisy true:909
noisy false:1756
after delete: len(clean_dataset) 471
after delete: len(noisy_dataset) 2665
epoch [125/200] batch [5/14] time 0.498 (0.432) data 0.367 (0.302) loss_x loss_x 1.0195 (1.0913) acc_x 75.0000 (74.3750) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [10/14] time 0.447 (0.440) data 0.317 (0.310) loss_x loss_x 0.9624 (1.1536) acc_x 71.8750 (73.1250) lr 6.4653e-04 eta 0:00:01
epoch [125/200] batch [5/83] time 0.369 (0.430) data 0.238 (0.300) loss_u loss_u 0.9595 (0.9640) acc_u 6.2500 (5.6250) lr 6.4653e-04 eta 0:00:33
epoch [125/200] batch [10/83] time 0.403 (0.431) data 0.273 (0.301) loss_u loss_u 0.9775 (0.9612) acc_u 3.1250 (5.6250) lr 6.4653e-04 eta 0:00:31
epoch [125/200] batch [15/83] time 0.469 (0.441) data 0.339 (0.311) loss_u loss_u 0.9873 (0.9662) acc_u 3.1250 (5.2083) lr 6.4653e-04 eta 0:00:29
epoch [125/200] batch [20/83] time 0.489 (0.443) data 0.357 (0.313) loss_u loss_u 0.8984 (0.9591) acc_u 12.5000 (5.6250) lr 6.4653e-04 eta 0:00:27
epoch [125/200] batch [25/83] time 0.352 (0.450) data 0.221 (0.319) loss_u loss_u 0.9238 (0.9578) acc_u 9.3750 (5.8750) lr 6.4653e-04 eta 0:00:26
epoch [125/200] batch [30/83] time 0.381 (0.450) data 0.249 (0.319) loss_u loss_u 0.9419 (0.9609) acc_u 9.3750 (5.5208) lr 6.4653e-04 eta 0:00:23
epoch [125/200] batch [35/83] time 0.504 (0.447) data 0.373 (0.316) loss_u loss_u 0.9595 (0.9595) acc_u 6.2500 (5.7143) lr 6.4653e-04 eta 0:00:21
epoch [125/200] batch [40/83] time 0.444 (0.450) data 0.314 (0.319) loss_u loss_u 0.9028 (0.9576) acc_u 12.5000 (6.0156) lr 6.4653e-04 eta 0:00:19
epoch [125/200] batch [45/83] time 0.389 (0.449) data 0.258 (0.318) loss_u loss_u 0.9487 (0.9569) acc_u 6.2500 (6.0417) lr 6.4653e-04 eta 0:00:17
epoch [125/200] batch [50/83] time 0.492 (0.453) data 0.361 (0.323) loss_u loss_u 0.9116 (0.9564) acc_u 9.3750 (6.0000) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [55/83] time 0.422 (0.455) data 0.290 (0.324) loss_u loss_u 0.9282 (0.9567) acc_u 9.3750 (5.9659) lr 6.4653e-04 eta 0:00:12
epoch [125/200] batch [60/83] time 0.576 (0.453) data 0.444 (0.322) loss_u loss_u 0.9355 (0.9580) acc_u 9.3750 (5.7812) lr 6.4653e-04 eta 0:00:10
epoch [125/200] batch [65/83] time 0.432 (0.450) data 0.301 (0.319) loss_u loss_u 0.9702 (0.9574) acc_u 3.1250 (5.7212) lr 6.4653e-04 eta 0:00:08
epoch [125/200] batch [70/83] time 0.392 (0.448) data 0.262 (0.317) loss_u loss_u 0.9897 (0.9567) acc_u 0.0000 (5.7589) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [75/83] time 0.777 (0.450) data 0.645 (0.319) loss_u loss_u 0.9531 (0.9574) acc_u 3.1250 (5.6250) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [80/83] time 0.381 (0.448) data 0.250 (0.317) loss_u loss_u 0.9980 (0.9594) acc_u 0.0000 (5.3516) lr 6.4653e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1746
confident_label rate tensor(0.1480, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 464
clean true:446
clean false:18
clean_rate:0.9612068965517241
noisy true:944
noisy false:1728
after delete: len(clean_dataset) 464
after delete: len(noisy_dataset) 2672
epoch [126/200] batch [5/14] time 0.458 (0.466) data 0.326 (0.335) loss_x loss_x 1.1465 (0.9949) acc_x 65.6250 (72.5000) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [10/14] time 0.475 (0.444) data 0.344 (0.313) loss_x loss_x 1.3945 (1.1140) acc_x 81.2500 (73.7500) lr 6.3188e-04 eta 0:00:01
epoch [126/200] batch [5/83] time 0.495 (0.456) data 0.363 (0.325) loss_u loss_u 0.9585 (0.9653) acc_u 6.2500 (5.0000) lr 6.3188e-04 eta 0:00:35
epoch [126/200] batch [10/83] time 0.455 (0.450) data 0.323 (0.319) loss_u loss_u 0.9302 (0.9433) acc_u 6.2500 (7.1875) lr 6.3188e-04 eta 0:00:32
epoch [126/200] batch [15/83] time 0.381 (0.454) data 0.249 (0.322) loss_u loss_u 0.9370 (0.9455) acc_u 9.3750 (6.8750) lr 6.3188e-04 eta 0:00:30
epoch [126/200] batch [20/83] time 0.313 (0.453) data 0.181 (0.322) loss_u loss_u 0.9805 (0.9469) acc_u 3.1250 (6.7188) lr 6.3188e-04 eta 0:00:28
epoch [126/200] batch [25/83] time 0.373 (0.454) data 0.242 (0.323) loss_u loss_u 0.9497 (0.9510) acc_u 6.2500 (6.2500) lr 6.3188e-04 eta 0:00:26
epoch [126/200] batch [30/83] time 0.362 (0.456) data 0.231 (0.324) loss_u loss_u 0.9189 (0.9503) acc_u 12.5000 (6.4583) lr 6.3188e-04 eta 0:00:24
epoch [126/200] batch [35/83] time 0.405 (0.458) data 0.274 (0.327) loss_u loss_u 0.9629 (0.9510) acc_u 6.2500 (6.5179) lr 6.3188e-04 eta 0:00:21
epoch [126/200] batch [40/83] time 0.399 (0.455) data 0.267 (0.324) loss_u loss_u 0.9556 (0.9524) acc_u 3.1250 (6.3281) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [45/83] time 0.570 (0.458) data 0.440 (0.327) loss_u loss_u 0.9702 (0.9527) acc_u 6.2500 (6.2500) lr 6.3188e-04 eta 0:00:17
epoch [126/200] batch [50/83] time 0.424 (0.458) data 0.293 (0.327) loss_u loss_u 0.9292 (0.9546) acc_u 9.3750 (6.0000) lr 6.3188e-04 eta 0:00:15
epoch [126/200] batch [55/83] time 0.390 (0.455) data 0.259 (0.323) loss_u loss_u 0.9614 (0.9554) acc_u 3.1250 (5.9091) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [60/83] time 0.414 (0.453) data 0.283 (0.322) loss_u loss_u 0.9648 (0.9558) acc_u 6.2500 (5.8333) lr 6.3188e-04 eta 0:00:10
epoch [126/200] batch [65/83] time 0.406 (0.456) data 0.275 (0.325) loss_u loss_u 0.9316 (0.9562) acc_u 6.2500 (5.7212) lr 6.3188e-04 eta 0:00:08
epoch [126/200] batch [70/83] time 0.371 (0.456) data 0.240 (0.325) loss_u loss_u 0.9243 (0.9574) acc_u 12.5000 (5.5804) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [75/83] time 0.466 (0.455) data 0.336 (0.324) loss_u loss_u 0.9453 (0.9576) acc_u 6.2500 (5.6667) lr 6.3188e-04 eta 0:00:03
epoch [126/200] batch [80/83] time 0.358 (0.455) data 0.228 (0.323) loss_u loss_u 0.9873 (0.9582) acc_u 0.0000 (5.5469) lr 6.3188e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1729
confident_label rate tensor(0.1518, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 476
clean true:456
clean false:20
clean_rate:0.957983193277311
noisy true:951
noisy false:1709
after delete: len(clean_dataset) 476
after delete: len(noisy_dataset) 2660
epoch [127/200] batch [5/14] time 0.450 (0.425) data 0.320 (0.294) loss_x loss_x 1.3770 (1.1413) acc_x 71.8750 (77.5000) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [10/14] time 0.434 (0.432) data 0.304 (0.301) loss_x loss_x 1.0156 (1.0556) acc_x 78.1250 (78.1250) lr 6.1732e-04 eta 0:00:01
epoch [127/200] batch [5/83] time 0.528 (0.451) data 0.398 (0.320) loss_u loss_u 0.9268 (0.9418) acc_u 9.3750 (6.8750) lr 6.1732e-04 eta 0:00:35
epoch [127/200] batch [10/83] time 0.470 (0.460) data 0.340 (0.329) loss_u loss_u 0.9722 (0.9520) acc_u 6.2500 (6.5625) lr 6.1732e-04 eta 0:00:33
epoch [127/200] batch [15/83] time 0.449 (0.457) data 0.318 (0.326) loss_u loss_u 0.9736 (0.9598) acc_u 3.1250 (5.4167) lr 6.1732e-04 eta 0:00:31
epoch [127/200] batch [20/83] time 0.509 (0.459) data 0.379 (0.328) loss_u loss_u 0.9795 (0.9595) acc_u 3.1250 (5.3125) lr 6.1732e-04 eta 0:00:28
epoch [127/200] batch [25/83] time 0.667 (0.467) data 0.537 (0.336) loss_u loss_u 0.9692 (0.9588) acc_u 3.1250 (5.3750) lr 6.1732e-04 eta 0:00:27
epoch [127/200] batch [30/83] time 0.464 (0.463) data 0.333 (0.332) loss_u loss_u 0.9688 (0.9593) acc_u 3.1250 (5.3125) lr 6.1732e-04 eta 0:00:24
epoch [127/200] batch [35/83] time 0.346 (0.462) data 0.215 (0.331) loss_u loss_u 0.9658 (0.9569) acc_u 6.2500 (5.7143) lr 6.1732e-04 eta 0:00:22
epoch [127/200] batch [40/83] time 0.433 (0.461) data 0.301 (0.330) loss_u loss_u 0.9673 (0.9573) acc_u 3.1250 (5.5469) lr 6.1732e-04 eta 0:00:19
epoch [127/200] batch [45/83] time 0.388 (0.459) data 0.256 (0.328) loss_u loss_u 0.9629 (0.9582) acc_u 6.2500 (5.4861) lr 6.1732e-04 eta 0:00:17
epoch [127/200] batch [50/83] time 0.431 (0.459) data 0.299 (0.328) loss_u loss_u 0.9663 (0.9594) acc_u 3.1250 (5.3125) lr 6.1732e-04 eta 0:00:15
epoch [127/200] batch [55/83] time 0.395 (0.458) data 0.264 (0.327) loss_u loss_u 0.9561 (0.9597) acc_u 9.3750 (5.2841) lr 6.1732e-04 eta 0:00:12
epoch [127/200] batch [60/83] time 0.509 (0.456) data 0.378 (0.325) loss_u loss_u 0.9980 (0.9601) acc_u 0.0000 (5.2604) lr 6.1732e-04 eta 0:00:10
epoch [127/200] batch [65/83] time 0.502 (0.455) data 0.371 (0.324) loss_u loss_u 0.9434 (0.9588) acc_u 3.1250 (5.3846) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [70/83] time 0.459 (0.455) data 0.327 (0.324) loss_u loss_u 0.9756 (0.9586) acc_u 3.1250 (5.3571) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [75/83] time 0.432 (0.455) data 0.299 (0.324) loss_u loss_u 0.9688 (0.9583) acc_u 6.2500 (5.3750) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [80/83] time 0.425 (0.456) data 0.293 (0.325) loss_u loss_u 0.9614 (0.9564) acc_u 9.3750 (5.6250) lr 6.1732e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1748
confident_label rate tensor(0.1566, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 491
clean true:474
clean false:17
clean_rate:0.9653767820773931
noisy true:914
noisy false:1731
after delete: len(clean_dataset) 491
after delete: len(noisy_dataset) 2645
epoch [128/200] batch [5/15] time 0.446 (0.489) data 0.315 (0.358) loss_x loss_x 0.9668 (1.0076) acc_x 81.2500 (77.5000) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [10/15] time 0.387 (0.470) data 0.256 (0.339) loss_x loss_x 0.7114 (0.9363) acc_x 84.3750 (77.1875) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [15/15] time 0.458 (0.472) data 0.327 (0.341) loss_x loss_x 1.0381 (1.0132) acc_x 71.8750 (74.5833) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/82] time 0.374 (0.462) data 0.243 (0.331) loss_u loss_u 0.9331 (0.9643) acc_u 9.3750 (4.3750) lr 6.0285e-04 eta 0:00:35
epoch [128/200] batch [10/82] time 0.462 (0.459) data 0.330 (0.328) loss_u loss_u 0.9546 (0.9688) acc_u 6.2500 (3.4375) lr 6.0285e-04 eta 0:00:33
epoch [128/200] batch [15/82] time 0.469 (0.456) data 0.338 (0.324) loss_u loss_u 0.9663 (0.9592) acc_u 3.1250 (4.5833) lr 6.0285e-04 eta 0:00:30
epoch [128/200] batch [20/82] time 0.579 (0.459) data 0.448 (0.328) loss_u loss_u 0.9961 (0.9607) acc_u 0.0000 (4.5312) lr 6.0285e-04 eta 0:00:28
epoch [128/200] batch [25/82] time 0.632 (0.468) data 0.500 (0.337) loss_u loss_u 0.9648 (0.9623) acc_u 9.3750 (4.5000) lr 6.0285e-04 eta 0:00:26
epoch [128/200] batch [30/82] time 0.465 (0.464) data 0.334 (0.333) loss_u loss_u 0.9541 (0.9607) acc_u 6.2500 (4.8958) lr 6.0285e-04 eta 0:00:24
epoch [128/200] batch [35/82] time 0.459 (0.461) data 0.328 (0.330) loss_u loss_u 0.9980 (0.9609) acc_u 0.0000 (5.0893) lr 6.0285e-04 eta 0:00:21
epoch [128/200] batch [40/82] time 0.441 (0.459) data 0.309 (0.328) loss_u loss_u 0.9541 (0.9625) acc_u 3.1250 (5.0000) lr 6.0285e-04 eta 0:00:19
epoch [128/200] batch [45/82] time 0.417 (0.457) data 0.287 (0.325) loss_u loss_u 0.9932 (0.9638) acc_u 0.0000 (4.7917) lr 6.0285e-04 eta 0:00:16
epoch [128/200] batch [50/82] time 0.487 (0.462) data 0.355 (0.331) loss_u loss_u 0.9780 (0.9638) acc_u 3.1250 (4.8125) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [55/82] time 0.524 (0.465) data 0.392 (0.334) loss_u loss_u 0.9844 (0.9639) acc_u 0.0000 (4.7727) lr 6.0285e-04 eta 0:00:12
epoch [128/200] batch [60/82] time 0.409 (0.461) data 0.277 (0.330) loss_u loss_u 0.9453 (0.9642) acc_u 6.2500 (4.6354) lr 6.0285e-04 eta 0:00:10
epoch [128/200] batch [65/82] time 0.399 (0.459) data 0.267 (0.328) loss_u loss_u 0.9424 (0.9624) acc_u 6.2500 (4.8558) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [70/82] time 0.411 (0.463) data 0.279 (0.331) loss_u loss_u 0.9517 (0.9633) acc_u 9.3750 (4.7768) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [75/82] time 0.471 (0.461) data 0.339 (0.330) loss_u loss_u 0.9956 (0.9634) acc_u 0.0000 (4.8750) lr 6.0285e-04 eta 0:00:03
epoch [128/200] batch [80/82] time 0.423 (0.458) data 0.292 (0.327) loss_u loss_u 0.9995 (0.9644) acc_u 0.0000 (4.6875) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1712
confident_label rate tensor(0.1566, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 491
clean true:469
clean false:22
clean_rate:0.955193482688391
noisy true:955
noisy false:1690
after delete: len(clean_dataset) 491
after delete: len(noisy_dataset) 2645
epoch [129/200] batch [5/15] time 0.370 (0.427) data 0.240 (0.296) loss_x loss_x 1.0000 (1.1879) acc_x 75.0000 (72.5000) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [10/15] time 0.492 (0.451) data 0.361 (0.321) loss_x loss_x 1.3398 (1.2068) acc_x 62.5000 (71.2500) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [15/15] time 0.341 (0.457) data 0.211 (0.327) loss_x loss_x 0.9521 (1.1674) acc_x 71.8750 (70.8333) lr 5.8849e-04 eta 0:00:00
epoch [129/200] batch [5/82] time 0.488 (0.458) data 0.357 (0.328) loss_u loss_u 0.9268 (0.9403) acc_u 9.3750 (8.7500) lr 5.8849e-04 eta 0:00:35
epoch [129/200] batch [10/82] time 0.591 (0.469) data 0.460 (0.338) loss_u loss_u 0.9863 (0.9487) acc_u 3.1250 (7.1875) lr 5.8849e-04 eta 0:00:33
epoch [129/200] batch [15/82] time 0.401 (0.461) data 0.271 (0.331) loss_u loss_u 0.9819 (0.9540) acc_u 3.1250 (6.4583) lr 5.8849e-04 eta 0:00:30
epoch [129/200] batch [20/82] time 0.357 (0.456) data 0.226 (0.325) loss_u loss_u 0.9780 (0.9544) acc_u 3.1250 (6.4062) lr 5.8849e-04 eta 0:00:28
epoch [129/200] batch [25/82] time 0.495 (0.452) data 0.364 (0.322) loss_u loss_u 0.9707 (0.9537) acc_u 3.1250 (6.3750) lr 5.8849e-04 eta 0:00:25
epoch [129/200] batch [30/82] time 0.515 (0.452) data 0.383 (0.322) loss_u loss_u 0.9829 (0.9533) acc_u 3.1250 (6.4583) lr 5.8849e-04 eta 0:00:23
epoch [129/200] batch [35/82] time 0.462 (0.458) data 0.331 (0.327) loss_u loss_u 0.9736 (0.9545) acc_u 3.1250 (6.1607) lr 5.8849e-04 eta 0:00:21
epoch [129/200] batch [40/82] time 0.411 (0.453) data 0.279 (0.322) loss_u loss_u 0.9619 (0.9552) acc_u 3.1250 (6.0156) lr 5.8849e-04 eta 0:00:19
epoch [129/200] batch [45/82] time 0.410 (0.454) data 0.278 (0.323) loss_u loss_u 0.9653 (0.9564) acc_u 3.1250 (5.8333) lr 5.8849e-04 eta 0:00:16
epoch [129/200] batch [50/82] time 0.428 (0.450) data 0.297 (0.319) loss_u loss_u 0.9351 (0.9554) acc_u 9.3750 (6.0000) lr 5.8849e-04 eta 0:00:14
epoch [129/200] batch [55/82] time 0.510 (0.451) data 0.379 (0.320) loss_u loss_u 0.9292 (0.9548) acc_u 9.3750 (6.0795) lr 5.8849e-04 eta 0:00:12
epoch [129/200] batch [60/82] time 0.396 (0.449) data 0.265 (0.318) loss_u loss_u 0.9653 (0.9563) acc_u 3.1250 (5.8333) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [65/82] time 0.377 (0.451) data 0.245 (0.320) loss_u loss_u 0.9556 (0.9567) acc_u 6.2500 (5.7692) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [70/82] time 0.587 (0.452) data 0.455 (0.321) loss_u loss_u 0.9673 (0.9585) acc_u 3.1250 (5.5357) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [75/82] time 0.438 (0.451) data 0.307 (0.320) loss_u loss_u 0.9648 (0.9590) acc_u 6.2500 (5.5833) lr 5.8849e-04 eta 0:00:03
epoch [129/200] batch [80/82] time 0.391 (0.449) data 0.260 (0.318) loss_u loss_u 0.9487 (0.9597) acc_u 6.2500 (5.4297) lr 5.8849e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1750
confident_label rate tensor(0.1559, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 489
clean true:470
clean false:19
clean_rate:0.9611451942740287
noisy true:916
noisy false:1731
after delete: len(clean_dataset) 489
after delete: len(noisy_dataset) 2647
epoch [130/200] batch [5/15] time 0.472 (0.440) data 0.342 (0.309) loss_x loss_x 1.0215 (1.2160) acc_x 75.0000 (71.8750) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [10/15] time 0.526 (0.464) data 0.396 (0.334) loss_x loss_x 0.8462 (1.1507) acc_x 84.3750 (74.3750) lr 5.7422e-04 eta 0:00:02
epoch [130/200] batch [15/15] time 0.479 (0.470) data 0.350 (0.340) loss_x loss_x 1.2754 (1.1624) acc_x 71.8750 (73.3333) lr 5.7422e-04 eta 0:00:00
epoch [130/200] batch [5/82] time 0.452 (0.463) data 0.321 (0.332) loss_u loss_u 0.9556 (0.9383) acc_u 9.3750 (8.1250) lr 5.7422e-04 eta 0:00:35
epoch [130/200] batch [10/82] time 0.362 (0.457) data 0.231 (0.327) loss_u loss_u 0.9502 (0.9511) acc_u 9.3750 (6.5625) lr 5.7422e-04 eta 0:00:32
epoch [130/200] batch [15/82] time 0.567 (0.458) data 0.437 (0.328) loss_u loss_u 0.9976 (0.9543) acc_u 0.0000 (6.0417) lr 5.7422e-04 eta 0:00:30
epoch [130/200] batch [20/82] time 0.411 (0.454) data 0.280 (0.324) loss_u loss_u 0.9692 (0.9569) acc_u 3.1250 (5.6250) lr 5.7422e-04 eta 0:00:28
epoch [130/200] batch [25/82] time 0.463 (0.453) data 0.332 (0.322) loss_u loss_u 0.8999 (0.9572) acc_u 12.5000 (5.5000) lr 5.7422e-04 eta 0:00:25
epoch [130/200] batch [30/82] time 0.448 (0.452) data 0.317 (0.322) loss_u loss_u 0.9658 (0.9565) acc_u 3.1250 (5.6250) lr 5.7422e-04 eta 0:00:23
epoch [130/200] batch [35/82] time 0.443 (0.450) data 0.312 (0.319) loss_u loss_u 0.9438 (0.9577) acc_u 6.2500 (5.6250) lr 5.7422e-04 eta 0:00:21
epoch [130/200] batch [40/82] time 0.512 (0.452) data 0.380 (0.321) loss_u loss_u 0.9658 (0.9586) acc_u 3.1250 (5.4688) lr 5.7422e-04 eta 0:00:19
epoch [130/200] batch [45/82] time 0.442 (0.450) data 0.310 (0.319) loss_u loss_u 0.9658 (0.9574) acc_u 6.2500 (5.6250) lr 5.7422e-04 eta 0:00:16
epoch [130/200] batch [50/82] time 0.536 (0.449) data 0.404 (0.318) loss_u loss_u 0.9092 (0.9578) acc_u 12.5000 (5.5625) lr 5.7422e-04 eta 0:00:14
epoch [130/200] batch [55/82] time 0.559 (0.451) data 0.428 (0.320) loss_u loss_u 0.9951 (0.9603) acc_u 0.0000 (5.1705) lr 5.7422e-04 eta 0:00:12
epoch [130/200] batch [60/82] time 0.600 (0.454) data 0.469 (0.323) loss_u loss_u 0.9771 (0.9616) acc_u 3.1250 (5.0000) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [65/82] time 0.385 (0.452) data 0.254 (0.321) loss_u loss_u 0.9678 (0.9619) acc_u 6.2500 (5.0000) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [70/82] time 0.387 (0.451) data 0.256 (0.320) loss_u loss_u 0.9648 (0.9617) acc_u 6.2500 (5.0893) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [75/82] time 0.608 (0.452) data 0.477 (0.321) loss_u loss_u 0.9600 (0.9618) acc_u 6.2500 (5.0833) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [80/82] time 0.551 (0.452) data 0.420 (0.321) loss_u loss_u 0.9531 (0.9625) acc_u 3.1250 (4.9219) lr 5.7422e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1674
confident_label rate tensor(0.1629, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 511
clean true:494
clean false:17
clean_rate:0.9667318982387475
noisy true:968
noisy false:1657
after delete: len(clean_dataset) 511
after delete: len(noisy_dataset) 2625
epoch [131/200] batch [5/15] time 0.524 (0.487) data 0.394 (0.357) loss_x loss_x 1.1055 (1.0237) acc_x 68.7500 (75.6250) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [10/15] time 0.408 (0.455) data 0.277 (0.324) loss_x loss_x 0.5601 (0.9832) acc_x 78.1250 (75.0000) lr 5.6006e-04 eta 0:00:02
epoch [131/200] batch [15/15] time 0.515 (0.445) data 0.385 (0.315) loss_x loss_x 1.0967 (1.0699) acc_x 62.5000 (72.9167) lr 5.6006e-04 eta 0:00:00
epoch [131/200] batch [5/82] time 0.406 (0.452) data 0.276 (0.321) loss_u loss_u 0.9453 (0.9718) acc_u 6.2500 (3.1250) lr 5.6006e-04 eta 0:00:34
epoch [131/200] batch [10/82] time 0.358 (0.447) data 0.227 (0.317) loss_u loss_u 0.9575 (0.9587) acc_u 9.3750 (5.9375) lr 5.6006e-04 eta 0:00:32
epoch [131/200] batch [15/82] time 0.453 (0.447) data 0.323 (0.317) loss_u loss_u 0.9688 (0.9611) acc_u 6.2500 (5.6250) lr 5.6006e-04 eta 0:00:29
epoch [131/200] batch [20/82] time 0.506 (0.448) data 0.376 (0.317) loss_u loss_u 0.9600 (0.9648) acc_u 6.2500 (4.8438) lr 5.6006e-04 eta 0:00:27
epoch [131/200] batch [25/82] time 0.520 (0.448) data 0.389 (0.318) loss_u loss_u 0.9448 (0.9611) acc_u 9.3750 (5.5000) lr 5.6006e-04 eta 0:00:25
epoch [131/200] batch [30/82] time 0.407 (0.444) data 0.277 (0.313) loss_u loss_u 0.9937 (0.9631) acc_u 0.0000 (5.0000) lr 5.6006e-04 eta 0:00:23
epoch [131/200] batch [35/82] time 0.366 (0.442) data 0.235 (0.311) loss_u loss_u 0.9658 (0.9642) acc_u 3.1250 (5.0000) lr 5.6006e-04 eta 0:00:20
epoch [131/200] batch [40/82] time 0.581 (0.447) data 0.449 (0.317) loss_u loss_u 0.9805 (0.9660) acc_u 3.1250 (4.6094) lr 5.6006e-04 eta 0:00:18
epoch [131/200] batch [45/82] time 0.358 (0.445) data 0.226 (0.314) loss_u loss_u 0.9722 (0.9682) acc_u 3.1250 (4.3056) lr 5.6006e-04 eta 0:00:16
epoch [131/200] batch [50/82] time 0.723 (0.448) data 0.592 (0.318) loss_u loss_u 0.9653 (0.9677) acc_u 3.1250 (4.3125) lr 5.6006e-04 eta 0:00:14
epoch [131/200] batch [55/82] time 0.400 (0.445) data 0.269 (0.314) loss_u loss_u 0.9995 (0.9674) acc_u 0.0000 (4.3182) lr 5.6006e-04 eta 0:00:12
epoch [131/200] batch [60/82] time 0.422 (0.444) data 0.292 (0.313) loss_u loss_u 0.9902 (0.9666) acc_u 0.0000 (4.3229) lr 5.6006e-04 eta 0:00:09
epoch [131/200] batch [65/82] time 0.483 (0.442) data 0.351 (0.312) loss_u loss_u 0.9795 (0.9670) acc_u 3.1250 (4.3269) lr 5.6006e-04 eta 0:00:07
epoch [131/200] batch [70/82] time 0.468 (0.443) data 0.336 (0.312) loss_u loss_u 0.9834 (0.9676) acc_u 3.1250 (4.2411) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [75/82] time 0.428 (0.442) data 0.297 (0.312) loss_u loss_u 0.9717 (0.9683) acc_u 3.1250 (4.0417) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [80/82] time 0.467 (0.441) data 0.335 (0.311) loss_u loss_u 0.9346 (0.9670) acc_u 12.5000 (4.2188) lr 5.6006e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1726
confident_label rate tensor(0.1518, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 476
clean true:459
clean false:17
clean_rate:0.9642857142857143
noisy true:951
noisy false:1709
after delete: len(clean_dataset) 476
after delete: len(noisy_dataset) 2660
epoch [132/200] batch [5/14] time 0.494 (0.439) data 0.364 (0.309) loss_x loss_x 1.1768 (1.2811) acc_x 59.3750 (66.8750) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [10/14] time 0.355 (0.448) data 0.225 (0.318) loss_x loss_x 1.2158 (1.1719) acc_x 65.6250 (69.0625) lr 5.4601e-04 eta 0:00:01
epoch [132/200] batch [5/83] time 0.534 (0.448) data 0.404 (0.317) loss_u loss_u 0.9629 (0.9369) acc_u 6.2500 (8.1250) lr 5.4601e-04 eta 0:00:34
epoch [132/200] batch [10/83] time 0.533 (0.462) data 0.402 (0.331) loss_u loss_u 0.9922 (0.9485) acc_u 0.0000 (6.5625) lr 5.4601e-04 eta 0:00:33
epoch [132/200] batch [15/83] time 0.635 (0.469) data 0.504 (0.339) loss_u loss_u 0.9800 (0.9577) acc_u 3.1250 (5.6250) lr 5.4601e-04 eta 0:00:31
epoch [132/200] batch [20/83] time 0.421 (0.461) data 0.289 (0.331) loss_u loss_u 0.9692 (0.9622) acc_u 3.1250 (4.8438) lr 5.4601e-04 eta 0:00:29
epoch [132/200] batch [25/83] time 0.446 (0.461) data 0.315 (0.331) loss_u loss_u 0.9097 (0.9594) acc_u 12.5000 (5.1250) lr 5.4601e-04 eta 0:00:26
epoch [132/200] batch [30/83] time 0.453 (0.457) data 0.322 (0.327) loss_u loss_u 0.9673 (0.9606) acc_u 6.2500 (5.0000) lr 5.4601e-04 eta 0:00:24
epoch [132/200] batch [35/83] time 0.381 (0.453) data 0.251 (0.322) loss_u loss_u 0.9487 (0.9620) acc_u 6.2500 (4.8214) lr 5.4601e-04 eta 0:00:21
epoch [132/200] batch [40/83] time 0.445 (0.452) data 0.314 (0.322) loss_u loss_u 0.9741 (0.9641) acc_u 3.1250 (4.4531) lr 5.4601e-04 eta 0:00:19
epoch [132/200] batch [45/83] time 0.372 (0.453) data 0.240 (0.322) loss_u loss_u 0.9233 (0.9615) acc_u 12.5000 (4.8611) lr 5.4601e-04 eta 0:00:17
epoch [132/200] batch [50/83] time 0.478 (0.453) data 0.347 (0.323) loss_u loss_u 0.9199 (0.9596) acc_u 12.5000 (5.1875) lr 5.4601e-04 eta 0:00:14
epoch [132/200] batch [55/83] time 0.366 (0.450) data 0.235 (0.320) loss_u loss_u 0.9287 (0.9591) acc_u 9.3750 (5.1705) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [60/83] time 0.660 (0.451) data 0.528 (0.320) loss_u loss_u 0.9810 (0.9598) acc_u 3.1250 (5.1042) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [65/83] time 0.382 (0.447) data 0.251 (0.316) loss_u loss_u 0.9624 (0.9593) acc_u 3.1250 (5.0962) lr 5.4601e-04 eta 0:00:08
epoch [132/200] batch [70/83] time 0.419 (0.449) data 0.288 (0.318) loss_u loss_u 0.9595 (0.9590) acc_u 6.2500 (5.1786) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [75/83] time 0.466 (0.451) data 0.335 (0.320) loss_u loss_u 0.9976 (0.9591) acc_u 0.0000 (5.2083) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [80/83] time 0.527 (0.449) data 0.396 (0.318) loss_u loss_u 0.9888 (0.9597) acc_u 0.0000 (5.0781) lr 5.4601e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1713
confident_label rate tensor(0.1540, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 483
clean true:468
clean false:15
clean_rate:0.968944099378882
noisy true:955
noisy false:1698
after delete: len(clean_dataset) 483
after delete: len(noisy_dataset) 2653
epoch [133/200] batch [5/15] time 0.543 (0.452) data 0.412 (0.321) loss_x loss_x 1.3457 (1.3258) acc_x 81.2500 (73.7500) lr 5.3207e-04 eta 0:00:04
epoch [133/200] batch [10/15] time 0.400 (0.467) data 0.269 (0.337) loss_x loss_x 1.7070 (1.2333) acc_x 68.7500 (72.8125) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [15/15] time 0.420 (0.460) data 0.289 (0.330) loss_x loss_x 1.2021 (1.1699) acc_x 65.6250 (73.1250) lr 5.3207e-04 eta 0:00:00
epoch [133/200] batch [5/82] time 0.383 (0.450) data 0.252 (0.319) loss_u loss_u 0.9697 (0.9563) acc_u 3.1250 (5.6250) lr 5.3207e-04 eta 0:00:34
epoch [133/200] batch [10/82] time 0.504 (0.460) data 0.373 (0.329) loss_u loss_u 0.9526 (0.9567) acc_u 6.2500 (5.6250) lr 5.3207e-04 eta 0:00:33
epoch [133/200] batch [15/82] time 0.593 (0.464) data 0.461 (0.332) loss_u loss_u 0.9692 (0.9597) acc_u 3.1250 (5.4167) lr 5.3207e-04 eta 0:00:31
epoch [133/200] batch [20/82] time 0.509 (0.466) data 0.378 (0.335) loss_u loss_u 0.9614 (0.9572) acc_u 3.1250 (5.4688) lr 5.3207e-04 eta 0:00:28
epoch [133/200] batch [25/82] time 0.337 (0.458) data 0.205 (0.327) loss_u loss_u 0.9243 (0.9557) acc_u 12.5000 (5.6250) lr 5.3207e-04 eta 0:00:26
epoch [133/200] batch [30/82] time 0.361 (0.461) data 0.229 (0.330) loss_u loss_u 0.9688 (0.9576) acc_u 3.1250 (5.3125) lr 5.3207e-04 eta 0:00:23
epoch [133/200] batch [35/82] time 0.379 (0.452) data 0.247 (0.321) loss_u loss_u 0.9282 (0.9570) acc_u 9.3750 (5.3571) lr 5.3207e-04 eta 0:00:21
epoch [133/200] batch [40/82] time 0.577 (0.455) data 0.446 (0.324) loss_u loss_u 0.9019 (0.9566) acc_u 9.3750 (5.3125) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [45/82] time 0.428 (0.453) data 0.297 (0.321) loss_u loss_u 0.9785 (0.9575) acc_u 3.1250 (5.3472) lr 5.3207e-04 eta 0:00:16
epoch [133/200] batch [50/82] time 0.472 (0.454) data 0.341 (0.323) loss_u loss_u 0.9351 (0.9581) acc_u 6.2500 (5.1875) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [55/82] time 0.373 (0.453) data 0.241 (0.322) loss_u loss_u 0.9507 (0.9583) acc_u 6.2500 (5.1705) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [60/82] time 0.461 (0.452) data 0.330 (0.321) loss_u loss_u 0.9980 (0.9607) acc_u 0.0000 (4.8438) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [65/82] time 0.435 (0.452) data 0.303 (0.321) loss_u loss_u 0.9683 (0.9614) acc_u 3.1250 (4.7596) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [70/82] time 0.373 (0.452) data 0.241 (0.321) loss_u loss_u 0.9946 (0.9622) acc_u 0.0000 (4.6875) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [75/82] time 0.427 (0.452) data 0.296 (0.321) loss_u loss_u 0.9883 (0.9626) acc_u 0.0000 (4.6667) lr 5.3207e-04 eta 0:00:03
epoch [133/200] batch [80/82] time 0.399 (0.449) data 0.269 (0.318) loss_u loss_u 0.9951 (0.9622) acc_u 0.0000 (4.8047) lr 5.3207e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1745
confident_label rate tensor(0.1569, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 492
clean true:475
clean false:17
clean_rate:0.9654471544715447
noisy true:916
noisy false:1728
after delete: len(clean_dataset) 492
after delete: len(noisy_dataset) 2644
epoch [134/200] batch [5/15] time 0.429 (0.481) data 0.298 (0.350) loss_x loss_x 1.1807 (1.1398) acc_x 65.6250 (71.2500) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [10/15] time 0.435 (0.474) data 0.304 (0.343) loss_x loss_x 1.0312 (1.2105) acc_x 81.2500 (71.5625) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [15/15] time 0.426 (0.472) data 0.295 (0.341) loss_x loss_x 1.2812 (1.2028) acc_x 68.7500 (71.4583) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/82] time 0.535 (0.468) data 0.403 (0.337) loss_u loss_u 0.9653 (0.9466) acc_u 3.1250 (6.2500) lr 5.1825e-04 eta 0:00:36
epoch [134/200] batch [10/82] time 0.465 (0.460) data 0.334 (0.328) loss_u loss_u 0.9941 (0.9603) acc_u 0.0000 (5.0000) lr 5.1825e-04 eta 0:00:33
epoch [134/200] batch [15/82] time 0.485 (0.451) data 0.354 (0.320) loss_u loss_u 0.9380 (0.9583) acc_u 6.2500 (5.0000) lr 5.1825e-04 eta 0:00:30
epoch [134/200] batch [20/82] time 0.383 (0.444) data 0.252 (0.313) loss_u loss_u 0.9668 (0.9585) acc_u 3.1250 (5.0000) lr 5.1825e-04 eta 0:00:27
epoch [134/200] batch [25/82] time 0.531 (0.450) data 0.400 (0.319) loss_u loss_u 0.9355 (0.9589) acc_u 9.3750 (5.1250) lr 5.1825e-04 eta 0:00:25
epoch [134/200] batch [30/82] time 0.570 (0.456) data 0.440 (0.325) loss_u loss_u 0.9600 (0.9611) acc_u 6.2500 (5.0000) lr 5.1825e-04 eta 0:00:23
epoch [134/200] batch [35/82] time 0.387 (0.454) data 0.256 (0.323) loss_u loss_u 0.9404 (0.9615) acc_u 6.2500 (4.9107) lr 5.1825e-04 eta 0:00:21
epoch [134/200] batch [40/82] time 0.467 (0.453) data 0.336 (0.322) loss_u loss_u 0.9639 (0.9623) acc_u 3.1250 (4.7656) lr 5.1825e-04 eta 0:00:19
epoch [134/200] batch [45/82] time 0.515 (0.455) data 0.383 (0.324) loss_u loss_u 0.9458 (0.9619) acc_u 9.3750 (4.7917) lr 5.1825e-04 eta 0:00:16
epoch [134/200] batch [50/82] time 0.507 (0.453) data 0.375 (0.321) loss_u loss_u 0.9751 (0.9610) acc_u 3.1250 (4.8750) lr 5.1825e-04 eta 0:00:14
epoch [134/200] batch [55/82] time 0.364 (0.450) data 0.232 (0.319) loss_u loss_u 0.9961 (0.9600) acc_u 0.0000 (5.0000) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [60/82] time 0.496 (0.456) data 0.365 (0.325) loss_u loss_u 0.9727 (0.9584) acc_u 3.1250 (5.2604) lr 5.1825e-04 eta 0:00:10
epoch [134/200] batch [65/82] time 0.489 (0.456) data 0.358 (0.325) loss_u loss_u 0.9990 (0.9588) acc_u 0.0000 (5.2404) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [70/82] time 0.472 (0.457) data 0.341 (0.326) loss_u loss_u 0.9604 (0.9588) acc_u 6.2500 (5.2232) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [75/82] time 0.475 (0.457) data 0.344 (0.326) loss_u loss_u 0.9448 (0.9592) acc_u 6.2500 (5.1667) lr 5.1825e-04 eta 0:00:03
epoch [134/200] batch [80/82] time 0.381 (0.457) data 0.250 (0.326) loss_u loss_u 0.9482 (0.9597) acc_u 6.2500 (5.1562) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1760
confident_label rate tensor(0.1569, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 492
clean true:476
clean false:16
clean_rate:0.967479674796748
noisy true:900
noisy false:1744
after delete: len(clean_dataset) 492
after delete: len(noisy_dataset) 2644
epoch [135/200] batch [5/15] time 0.471 (0.494) data 0.340 (0.363) loss_x loss_x 1.0654 (1.2060) acc_x 81.2500 (71.2500) lr 5.0454e-04 eta 0:00:04
epoch [135/200] batch [10/15] time 0.462 (0.486) data 0.332 (0.355) loss_x loss_x 1.5283 (1.1796) acc_x 59.3750 (70.6250) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [15/15] time 0.490 (0.475) data 0.359 (0.344) loss_x loss_x 0.7886 (1.0717) acc_x 84.3750 (73.7500) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/82] time 0.455 (0.472) data 0.324 (0.341) loss_u loss_u 0.9673 (0.9755) acc_u 3.1250 (2.5000) lr 5.0454e-04 eta 0:00:36
epoch [135/200] batch [10/82] time 0.364 (0.483) data 0.233 (0.352) loss_u loss_u 0.9404 (0.9717) acc_u 6.2500 (2.8125) lr 5.0454e-04 eta 0:00:34
epoch [135/200] batch [15/82] time 0.401 (0.477) data 0.269 (0.346) loss_u loss_u 0.9473 (0.9692) acc_u 6.2500 (3.5417) lr 5.0454e-04 eta 0:00:31
epoch [135/200] batch [20/82] time 0.395 (0.474) data 0.264 (0.343) loss_u loss_u 0.9297 (0.9682) acc_u 9.3750 (3.7500) lr 5.0454e-04 eta 0:00:29
epoch [135/200] batch [25/82] time 0.337 (0.477) data 0.205 (0.346) loss_u loss_u 0.9873 (0.9718) acc_u 3.1250 (3.3750) lr 5.0454e-04 eta 0:00:27
epoch [135/200] batch [30/82] time 0.374 (0.471) data 0.242 (0.340) loss_u loss_u 0.9609 (0.9732) acc_u 6.2500 (3.3333) lr 5.0454e-04 eta 0:00:24
epoch [135/200] batch [35/82] time 0.342 (0.461) data 0.211 (0.330) loss_u loss_u 0.9712 (0.9711) acc_u 3.1250 (3.8393) lr 5.0454e-04 eta 0:00:21
epoch [135/200] batch [40/82] time 0.407 (0.461) data 0.276 (0.330) loss_u loss_u 0.9653 (0.9708) acc_u 3.1250 (3.9844) lr 5.0454e-04 eta 0:00:19
epoch [135/200] batch [45/82] time 0.386 (0.463) data 0.254 (0.332) loss_u loss_u 0.9883 (0.9701) acc_u 3.1250 (4.0972) lr 5.0454e-04 eta 0:00:17
epoch [135/200] batch [50/82] time 0.579 (0.472) data 0.447 (0.340) loss_u loss_u 0.9751 (0.9691) acc_u 3.1250 (4.1250) lr 5.0454e-04 eta 0:00:15
epoch [135/200] batch [55/82] time 0.523 (0.472) data 0.391 (0.341) loss_u loss_u 0.9458 (0.9673) acc_u 9.3750 (4.3182) lr 5.0454e-04 eta 0:00:12
epoch [135/200] batch [60/82] time 0.506 (0.471) data 0.375 (0.339) loss_u loss_u 0.9312 (0.9660) acc_u 9.3750 (4.4792) lr 5.0454e-04 eta 0:00:10
epoch [135/200] batch [65/82] time 0.398 (0.467) data 0.267 (0.336) loss_u loss_u 0.9697 (0.9661) acc_u 6.2500 (4.4231) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [70/82] time 0.524 (0.468) data 0.392 (0.337) loss_u loss_u 0.9824 (0.9663) acc_u 3.1250 (4.4196) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [75/82] time 0.476 (0.468) data 0.346 (0.336) loss_u loss_u 0.9570 (0.9671) acc_u 3.1250 (4.2500) lr 5.0454e-04 eta 0:00:03
epoch [135/200] batch [80/82] time 0.376 (0.464) data 0.245 (0.333) loss_u loss_u 0.9497 (0.9664) acc_u 6.2500 (4.3359) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1714
confident_label rate tensor(0.1604, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 503
clean true:485
clean false:18
clean_rate:0.9642147117296223
noisy true:937
noisy false:1696
after delete: len(clean_dataset) 503
after delete: len(noisy_dataset) 2633
epoch [136/200] batch [5/15] time 0.511 (0.465) data 0.380 (0.335) loss_x loss_x 1.3135 (0.9400) acc_x 78.1250 (81.2500) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [10/15] time 0.626 (0.496) data 0.494 (0.365) loss_x loss_x 0.6621 (0.9901) acc_x 78.1250 (79.6875) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [15/15] time 0.437 (0.494) data 0.307 (0.363) loss_x loss_x 0.8022 (0.9388) acc_x 81.2500 (79.7917) lr 4.9096e-04 eta 0:00:00
epoch [136/200] batch [5/82] time 0.608 (0.496) data 0.477 (0.365) loss_u loss_u 0.9907 (0.9676) acc_u 3.1250 (4.3750) lr 4.9096e-04 eta 0:00:38
epoch [136/200] batch [10/82] time 0.686 (0.508) data 0.554 (0.377) loss_u loss_u 0.9722 (0.9720) acc_u 6.2500 (4.0625) lr 4.9096e-04 eta 0:00:36
epoch [136/200] batch [15/82] time 0.445 (0.508) data 0.314 (0.377) loss_u loss_u 0.8955 (0.9661) acc_u 12.5000 (4.5833) lr 4.9096e-04 eta 0:00:34
epoch [136/200] batch [20/82] time 0.396 (0.503) data 0.265 (0.372) loss_u loss_u 0.9941 (0.9679) acc_u 0.0000 (4.3750) lr 4.9096e-04 eta 0:00:31
epoch [136/200] batch [25/82] time 0.607 (0.502) data 0.475 (0.370) loss_u loss_u 0.9834 (0.9621) acc_u 3.1250 (5.1250) lr 4.9096e-04 eta 0:00:28
epoch [136/200] batch [30/82] time 0.462 (0.500) data 0.331 (0.369) loss_u loss_u 0.9897 (0.9625) acc_u 3.1250 (4.8958) lr 4.9096e-04 eta 0:00:26
epoch [136/200] batch [35/82] time 0.435 (0.497) data 0.304 (0.366) loss_u loss_u 0.9604 (0.9647) acc_u 6.2500 (4.6429) lr 4.9096e-04 eta 0:00:23
epoch [136/200] batch [40/82] time 0.407 (0.491) data 0.276 (0.360) loss_u loss_u 0.9570 (0.9626) acc_u 9.3750 (4.8438) lr 4.9096e-04 eta 0:00:20
epoch [136/200] batch [45/82] time 0.459 (0.488) data 0.328 (0.357) loss_u loss_u 0.9971 (0.9607) acc_u 0.0000 (5.0000) lr 4.9096e-04 eta 0:00:18
epoch [136/200] batch [50/82] time 0.366 (0.481) data 0.235 (0.350) loss_u loss_u 0.9385 (0.9602) acc_u 9.3750 (5.0625) lr 4.9096e-04 eta 0:00:15
epoch [136/200] batch [55/82] time 0.522 (0.479) data 0.391 (0.347) loss_u loss_u 0.9956 (0.9624) acc_u 0.0000 (4.8295) lr 4.9096e-04 eta 0:00:12
epoch [136/200] batch [60/82] time 0.409 (0.475) data 0.278 (0.343) loss_u loss_u 0.9785 (0.9628) acc_u 3.1250 (4.7917) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [65/82] time 0.663 (0.476) data 0.532 (0.344) loss_u loss_u 0.9902 (0.9628) acc_u 0.0000 (4.7596) lr 4.9096e-04 eta 0:00:08
epoch [136/200] batch [70/82] time 0.452 (0.472) data 0.321 (0.341) loss_u loss_u 0.9990 (0.9644) acc_u 0.0000 (4.5089) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [75/82] time 0.441 (0.470) data 0.310 (0.339) loss_u loss_u 0.9863 (0.9645) acc_u 0.0000 (4.4167) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [80/82] time 0.335 (0.466) data 0.204 (0.334) loss_u loss_u 0.9897 (0.9636) acc_u 0.0000 (4.5703) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1725
confident_label rate tensor(0.1617, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 507
clean true:488
clean false:19
clean_rate:0.9625246548323472
noisy true:923
noisy false:1706
after delete: len(clean_dataset) 507
after delete: len(noisy_dataset) 2629
epoch [137/200] batch [5/15] time 0.410 (0.453) data 0.279 (0.322) loss_x loss_x 1.8428 (1.1269) acc_x 65.6250 (75.6250) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [10/15] time 0.414 (0.436) data 0.283 (0.305) loss_x loss_x 1.0918 (1.2031) acc_x 68.7500 (71.5625) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [15/15] time 0.499 (0.451) data 0.369 (0.320) loss_x loss_x 0.9438 (1.1521) acc_x 84.3750 (73.1250) lr 4.7750e-04 eta 0:00:00
epoch [137/200] batch [5/82] time 0.543 (0.454) data 0.411 (0.323) loss_u loss_u 0.9575 (0.9623) acc_u 6.2500 (5.6250) lr 4.7750e-04 eta 0:00:34
epoch [137/200] batch [10/82] time 0.415 (0.447) data 0.284 (0.316) loss_u loss_u 0.9761 (0.9613) acc_u 3.1250 (5.3125) lr 4.7750e-04 eta 0:00:32
epoch [137/200] batch [15/82] time 0.528 (0.448) data 0.398 (0.317) loss_u loss_u 0.9541 (0.9632) acc_u 6.2500 (4.7917) lr 4.7750e-04 eta 0:00:30
epoch [137/200] batch [20/82] time 0.378 (0.456) data 0.248 (0.326) loss_u loss_u 0.9224 (0.9607) acc_u 9.3750 (5.1562) lr 4.7750e-04 eta 0:00:28
epoch [137/200] batch [25/82] time 0.477 (0.458) data 0.345 (0.327) loss_u loss_u 0.9634 (0.9577) acc_u 3.1250 (5.6250) lr 4.7750e-04 eta 0:00:26
epoch [137/200] batch [30/82] time 0.557 (0.461) data 0.425 (0.330) loss_u loss_u 0.9688 (0.9568) acc_u 6.2500 (5.7292) lr 4.7750e-04 eta 0:00:23
epoch [137/200] batch [35/82] time 0.453 (0.462) data 0.322 (0.332) loss_u loss_u 0.9746 (0.9593) acc_u 3.1250 (5.5357) lr 4.7750e-04 eta 0:00:21
epoch [137/200] batch [40/82] time 0.508 (0.465) data 0.377 (0.334) loss_u loss_u 0.9956 (0.9621) acc_u 0.0000 (5.0000) lr 4.7750e-04 eta 0:00:19
epoch [137/200] batch [45/82] time 0.421 (0.467) data 0.289 (0.336) loss_u loss_u 0.9883 (0.9618) acc_u 0.0000 (5.0694) lr 4.7750e-04 eta 0:00:17
epoch [137/200] batch [50/82] time 0.473 (0.465) data 0.341 (0.334) loss_u loss_u 0.9795 (0.9608) acc_u 3.1250 (5.1250) lr 4.7750e-04 eta 0:00:14
epoch [137/200] batch [55/82] time 0.452 (0.465) data 0.321 (0.334) loss_u loss_u 0.9951 (0.9615) acc_u 0.0000 (5.1705) lr 4.7750e-04 eta 0:00:12
epoch [137/200] batch [60/82] time 0.515 (0.463) data 0.384 (0.332) loss_u loss_u 0.9717 (0.9617) acc_u 3.1250 (5.2083) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [65/82] time 0.403 (0.461) data 0.272 (0.330) loss_u loss_u 0.9648 (0.9621) acc_u 3.1250 (5.2404) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [70/82] time 0.621 (0.465) data 0.489 (0.334) loss_u loss_u 0.9697 (0.9628) acc_u 3.1250 (5.1339) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [75/82] time 0.408 (0.462) data 0.277 (0.331) loss_u loss_u 0.9946 (0.9629) acc_u 0.0000 (5.0833) lr 4.7750e-04 eta 0:00:03
epoch [137/200] batch [80/82] time 0.582 (0.465) data 0.452 (0.334) loss_u loss_u 0.9683 (0.9629) acc_u 3.1250 (4.9609) lr 4.7750e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1729
confident_label rate tensor(0.1582, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 496
clean true:485
clean false:11
clean_rate:0.9778225806451613
noisy true:922
noisy false:1718
after delete: len(clean_dataset) 496
after delete: len(noisy_dataset) 2640
epoch [138/200] batch [5/15] time 0.389 (0.418) data 0.258 (0.287) loss_x loss_x 1.4834 (1.2180) acc_x 62.5000 (72.5000) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [10/15] time 0.390 (0.420) data 0.260 (0.289) loss_x loss_x 0.8984 (1.1516) acc_x 78.1250 (74.0625) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [15/15] time 0.569 (0.454) data 0.438 (0.324) loss_x loss_x 1.3154 (1.0939) acc_x 71.8750 (75.0000) lr 4.6417e-04 eta 0:00:00
epoch [138/200] batch [5/82] time 0.402 (0.448) data 0.271 (0.317) loss_u loss_u 0.9863 (0.9799) acc_u 3.1250 (3.7500) lr 4.6417e-04 eta 0:00:34
epoch [138/200] batch [10/82] time 0.532 (0.448) data 0.401 (0.317) loss_u loss_u 0.9634 (0.9729) acc_u 6.2500 (4.3750) lr 4.6417e-04 eta 0:00:32
epoch [138/200] batch [15/82] time 0.476 (0.464) data 0.345 (0.333) loss_u loss_u 0.9731 (0.9678) acc_u 3.1250 (4.5833) lr 4.6417e-04 eta 0:00:31
epoch [138/200] batch [20/82] time 0.423 (0.455) data 0.293 (0.324) loss_u loss_u 0.9272 (0.9650) acc_u 9.3750 (4.8438) lr 4.6417e-04 eta 0:00:28
epoch [138/200] batch [25/82] time 0.572 (0.456) data 0.440 (0.325) loss_u loss_u 0.9741 (0.9675) acc_u 3.1250 (4.5000) lr 4.6417e-04 eta 0:00:25
epoch [138/200] batch [30/82] time 0.433 (0.457) data 0.302 (0.326) loss_u loss_u 0.9585 (0.9660) acc_u 6.2500 (4.6875) lr 4.6417e-04 eta 0:00:23
epoch [138/200] batch [35/82] time 0.408 (0.457) data 0.277 (0.326) loss_u loss_u 0.9409 (0.9641) acc_u 6.2500 (4.7321) lr 4.6417e-04 eta 0:00:21
epoch [138/200] batch [40/82] time 0.430 (0.459) data 0.300 (0.328) loss_u loss_u 0.9990 (0.9646) acc_u 0.0000 (4.6875) lr 4.6417e-04 eta 0:00:19
epoch [138/200] batch [45/82] time 0.439 (0.452) data 0.308 (0.321) loss_u loss_u 0.9541 (0.9627) acc_u 6.2500 (5.0000) lr 4.6417e-04 eta 0:00:16
epoch [138/200] batch [50/82] time 0.430 (0.451) data 0.300 (0.320) loss_u loss_u 0.9165 (0.9633) acc_u 9.3750 (4.8750) lr 4.6417e-04 eta 0:00:14
epoch [138/200] batch [55/82] time 0.458 (0.459) data 0.327 (0.328) loss_u loss_u 0.9644 (0.9636) acc_u 3.1250 (4.7727) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [60/82] time 0.384 (0.459) data 0.252 (0.328) loss_u loss_u 0.9741 (0.9643) acc_u 3.1250 (4.6875) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [65/82] time 0.376 (0.459) data 0.245 (0.328) loss_u loss_u 0.9497 (0.9650) acc_u 6.2500 (4.6154) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [70/82] time 0.462 (0.458) data 0.331 (0.327) loss_u loss_u 0.9810 (0.9653) acc_u 3.1250 (4.5536) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [75/82] time 0.691 (0.460) data 0.559 (0.329) loss_u loss_u 0.9473 (0.9649) acc_u 6.2500 (4.5833) lr 4.6417e-04 eta 0:00:03
epoch [138/200] batch [80/82] time 0.404 (0.462) data 0.273 (0.331) loss_u loss_u 0.9663 (0.9655) acc_u 9.3750 (4.4922) lr 4.6417e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1747
confident_label rate tensor(0.1623, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 509
clean true:494
clean false:15
clean_rate:0.9705304518664047
noisy true:895
noisy false:1732
after delete: len(clean_dataset) 509
after delete: len(noisy_dataset) 2627
epoch [139/200] batch [5/15] time 0.462 (0.421) data 0.332 (0.291) loss_x loss_x 1.0938 (1.0634) acc_x 75.0000 (73.7500) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [10/15] time 0.435 (0.419) data 0.304 (0.288) loss_x loss_x 1.0898 (1.0349) acc_x 71.8750 (73.1250) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [15/15] time 0.491 (0.440) data 0.360 (0.310) loss_x loss_x 1.2539 (1.1655) acc_x 71.8750 (71.2500) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/82] time 0.534 (0.463) data 0.402 (0.332) loss_u loss_u 0.9883 (0.9535) acc_u 3.1250 (5.6250) lr 4.5098e-04 eta 0:00:35
epoch [139/200] batch [10/82] time 0.362 (0.451) data 0.231 (0.320) loss_u loss_u 0.9678 (0.9638) acc_u 3.1250 (4.6875) lr 4.5098e-04 eta 0:00:32
epoch [139/200] batch [15/82] time 0.536 (0.462) data 0.406 (0.331) loss_u loss_u 0.9463 (0.9611) acc_u 6.2500 (5.0000) lr 4.5098e-04 eta 0:00:30
epoch [139/200] batch [20/82] time 0.409 (0.464) data 0.279 (0.333) loss_u loss_u 0.9380 (0.9626) acc_u 6.2500 (4.8438) lr 4.5098e-04 eta 0:00:28
epoch [139/200] batch [25/82] time 0.544 (0.464) data 0.412 (0.334) loss_u loss_u 0.9453 (0.9609) acc_u 6.2500 (5.1250) lr 4.5098e-04 eta 0:00:26
epoch [139/200] batch [30/82] time 0.393 (0.470) data 0.261 (0.339) loss_u loss_u 0.9580 (0.9615) acc_u 3.1250 (4.7917) lr 4.5098e-04 eta 0:00:24
epoch [139/200] batch [35/82] time 0.400 (0.470) data 0.268 (0.339) loss_u loss_u 0.9482 (0.9618) acc_u 6.2500 (4.8214) lr 4.5098e-04 eta 0:00:22
epoch [139/200] batch [40/82] time 0.510 (0.471) data 0.380 (0.340) loss_u loss_u 0.9937 (0.9635) acc_u 0.0000 (4.6094) lr 4.5098e-04 eta 0:00:19
epoch [139/200] batch [45/82] time 0.420 (0.470) data 0.287 (0.339) loss_u loss_u 0.9790 (0.9647) acc_u 3.1250 (4.4444) lr 4.5098e-04 eta 0:00:17
epoch [139/200] batch [50/82] time 0.601 (0.471) data 0.470 (0.340) loss_u loss_u 0.9365 (0.9640) acc_u 9.3750 (4.5625) lr 4.5098e-04 eta 0:00:15
epoch [139/200] batch [55/82] time 0.399 (0.469) data 0.267 (0.338) loss_u loss_u 0.9917 (0.9632) acc_u 0.0000 (4.6591) lr 4.5098e-04 eta 0:00:12
epoch [139/200] batch [60/82] time 0.401 (0.466) data 0.270 (0.335) loss_u loss_u 0.9888 (0.9639) acc_u 0.0000 (4.6354) lr 4.5098e-04 eta 0:00:10
epoch [139/200] batch [65/82] time 0.477 (0.466) data 0.347 (0.335) loss_u loss_u 0.9229 (0.9626) acc_u 9.3750 (4.7115) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [70/82] time 0.429 (0.467) data 0.298 (0.336) loss_u loss_u 0.9702 (0.9629) acc_u 3.1250 (4.6429) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [75/82] time 0.380 (0.464) data 0.250 (0.333) loss_u loss_u 0.9819 (0.9634) acc_u 3.1250 (4.5417) lr 4.5098e-04 eta 0:00:03
epoch [139/200] batch [80/82] time 0.470 (0.461) data 0.338 (0.330) loss_u loss_u 0.9766 (0.9638) acc_u 3.1250 (4.5703) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1723
confident_label rate tensor(0.1534, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 481
clean true:469
clean false:12
clean_rate:0.975051975051975
noisy true:944
noisy false:1711
after delete: len(clean_dataset) 481
after delete: len(noisy_dataset) 2655
epoch [140/200] batch [5/15] time 0.544 (0.505) data 0.413 (0.374) loss_x loss_x 1.4121 (1.1783) acc_x 81.2500 (76.8750) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [10/15] time 0.478 (0.479) data 0.348 (0.348) loss_x loss_x 1.4209 (1.1706) acc_x 59.3750 (73.7500) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [15/15] time 0.468 (0.474) data 0.337 (0.343) loss_x loss_x 1.6436 (1.0877) acc_x 59.3750 (75.2083) lr 4.3792e-04 eta 0:00:00
epoch [140/200] batch [5/82] time 0.535 (0.467) data 0.403 (0.337) loss_u loss_u 0.9678 (0.9644) acc_u 3.1250 (3.7500) lr 4.3792e-04 eta 0:00:35
epoch [140/200] batch [10/82] time 0.473 (0.468) data 0.342 (0.337) loss_u loss_u 0.9985 (0.9596) acc_u 0.0000 (5.0000) lr 4.3792e-04 eta 0:00:33
epoch [140/200] batch [15/82] time 0.432 (0.464) data 0.301 (0.333) loss_u loss_u 0.9692 (0.9610) acc_u 6.2500 (5.0000) lr 4.3792e-04 eta 0:00:31
epoch [140/200] batch [20/82] time 0.521 (0.473) data 0.389 (0.342) loss_u loss_u 0.9526 (0.9632) acc_u 6.2500 (4.6875) lr 4.3792e-04 eta 0:00:29
epoch [140/200] batch [25/82] time 0.524 (0.480) data 0.393 (0.349) loss_u loss_u 0.9609 (0.9642) acc_u 3.1250 (4.5000) lr 4.3792e-04 eta 0:00:27
epoch [140/200] batch [30/82] time 0.596 (0.477) data 0.464 (0.346) loss_u loss_u 0.9438 (0.9650) acc_u 12.5000 (4.5833) lr 4.3792e-04 eta 0:00:24
epoch [140/200] batch [35/82] time 0.603 (0.474) data 0.472 (0.343) loss_u loss_u 0.9453 (0.9657) acc_u 9.3750 (4.5536) lr 4.3792e-04 eta 0:00:22
epoch [140/200] batch [40/82] time 0.411 (0.472) data 0.280 (0.341) loss_u loss_u 0.9614 (0.9648) acc_u 6.2500 (4.6094) lr 4.3792e-04 eta 0:00:19
epoch [140/200] batch [45/82] time 0.411 (0.470) data 0.279 (0.339) loss_u loss_u 0.9678 (0.9648) acc_u 3.1250 (4.5833) lr 4.3792e-04 eta 0:00:17
epoch [140/200] batch [50/82] time 0.360 (0.469) data 0.229 (0.338) loss_u loss_u 0.9653 (0.9630) acc_u 6.2500 (4.8750) lr 4.3792e-04 eta 0:00:15
epoch [140/200] batch [55/82] time 0.542 (0.469) data 0.411 (0.338) loss_u loss_u 0.9272 (0.9621) acc_u 9.3750 (5.0000) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [60/82] time 0.396 (0.464) data 0.264 (0.333) loss_u loss_u 0.9722 (0.9623) acc_u 3.1250 (4.9479) lr 4.3792e-04 eta 0:00:10
epoch [140/200] batch [65/82] time 0.478 (0.461) data 0.347 (0.330) loss_u loss_u 0.9351 (0.9612) acc_u 6.2500 (5.0000) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [70/82] time 0.499 (0.459) data 0.367 (0.327) loss_u loss_u 0.9873 (0.9610) acc_u 0.0000 (4.9554) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [75/82] time 0.470 (0.461) data 0.339 (0.329) loss_u loss_u 0.9766 (0.9608) acc_u 3.1250 (4.9583) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [80/82] time 0.418 (0.458) data 0.288 (0.327) loss_u loss_u 0.9658 (0.9608) acc_u 3.1250 (5.0000) lr 4.3792e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1741
confident_label rate tensor(0.1553, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 487
clean true:473
clean false:14
clean_rate:0.971252566735113
noisy true:922
noisy false:1727
after delete: len(clean_dataset) 487
after delete: len(noisy_dataset) 2649
epoch [141/200] batch [5/15] time 0.462 (0.412) data 0.332 (0.281) loss_x loss_x 1.4160 (1.0876) acc_x 62.5000 (70.6250) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [10/15] time 0.391 (0.451) data 0.260 (0.320) loss_x loss_x 1.0234 (1.0578) acc_x 75.0000 (73.7500) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [15/15] time 0.512 (0.449) data 0.381 (0.318) loss_x loss_x 0.9028 (1.0568) acc_x 78.1250 (73.5417) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/82] time 0.494 (0.449) data 0.363 (0.318) loss_u loss_u 0.9478 (0.9677) acc_u 6.2500 (5.0000) lr 4.2499e-04 eta 0:00:34
epoch [141/200] batch [10/82] time 0.354 (0.442) data 0.223 (0.311) loss_u loss_u 0.9727 (0.9635) acc_u 0.0000 (5.3125) lr 4.2499e-04 eta 0:00:31
epoch [141/200] batch [15/82] time 0.422 (0.449) data 0.290 (0.317) loss_u loss_u 0.9937 (0.9642) acc_u 0.0000 (5.0000) lr 4.2499e-04 eta 0:00:30
epoch [141/200] batch [20/82] time 0.331 (0.460) data 0.200 (0.329) loss_u loss_u 0.9585 (0.9590) acc_u 3.1250 (5.4688) lr 4.2499e-04 eta 0:00:28
epoch [141/200] batch [25/82] time 0.579 (0.457) data 0.447 (0.326) loss_u loss_u 0.9160 (0.9536) acc_u 9.3750 (6.0000) lr 4.2499e-04 eta 0:00:26
epoch [141/200] batch [30/82] time 0.466 (0.455) data 0.335 (0.324) loss_u loss_u 0.9453 (0.9538) acc_u 6.2500 (5.7292) lr 4.2499e-04 eta 0:00:23
epoch [141/200] batch [35/82] time 0.403 (0.450) data 0.272 (0.319) loss_u loss_u 0.9600 (0.9517) acc_u 3.1250 (5.9821) lr 4.2499e-04 eta 0:00:21
epoch [141/200] batch [40/82] time 0.430 (0.450) data 0.299 (0.319) loss_u loss_u 0.9502 (0.9532) acc_u 6.2500 (5.7812) lr 4.2499e-04 eta 0:00:18
epoch [141/200] batch [45/82] time 0.419 (0.450) data 0.287 (0.318) loss_u loss_u 0.9824 (0.9553) acc_u 3.1250 (5.4167) lr 4.2499e-04 eta 0:00:16
epoch [141/200] batch [50/82] time 0.512 (0.448) data 0.381 (0.317) loss_u loss_u 0.9990 (0.9567) acc_u 0.0000 (5.1250) lr 4.2499e-04 eta 0:00:14
epoch [141/200] batch [55/82] time 0.425 (0.449) data 0.294 (0.318) loss_u loss_u 0.9756 (0.9573) acc_u 3.1250 (5.0000) lr 4.2499e-04 eta 0:00:12
epoch [141/200] batch [60/82] time 0.367 (0.445) data 0.236 (0.314) loss_u loss_u 0.9912 (0.9579) acc_u 3.1250 (4.9479) lr 4.2499e-04 eta 0:00:09
epoch [141/200] batch [65/82] time 0.445 (0.443) data 0.314 (0.312) loss_u loss_u 0.9717 (0.9588) acc_u 3.1250 (4.9519) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [70/82] time 0.438 (0.448) data 0.307 (0.316) loss_u loss_u 0.9429 (0.9596) acc_u 9.3750 (4.9554) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [75/82] time 0.432 (0.447) data 0.300 (0.315) loss_u loss_u 0.9688 (0.9613) acc_u 3.1250 (4.7083) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [80/82] time 0.623 (0.448) data 0.492 (0.317) loss_u loss_u 0.9077 (0.9606) acc_u 12.5000 (4.7656) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1714
confident_label rate tensor(0.1588, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 498
clean true:478
clean false:20
clean_rate:0.9598393574297188
noisy true:944
noisy false:1694
after delete: len(clean_dataset) 498
after delete: len(noisy_dataset) 2638
epoch [142/200] batch [5/15] time 0.401 (0.461) data 0.270 (0.330) loss_x loss_x 0.8140 (1.1573) acc_x 75.0000 (71.2500) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [10/15] time 0.406 (0.456) data 0.275 (0.326) loss_x loss_x 0.9121 (1.1097) acc_x 78.1250 (72.8125) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [15/15] time 0.416 (0.446) data 0.285 (0.315) loss_x loss_x 1.7148 (1.1497) acc_x 62.5000 (72.5000) lr 4.1221e-04 eta 0:00:00
epoch [142/200] batch [5/82] time 0.556 (0.452) data 0.425 (0.321) loss_u loss_u 0.9819 (0.9572) acc_u 0.0000 (5.6250) lr 4.1221e-04 eta 0:00:34
epoch [142/200] batch [10/82] time 0.543 (0.461) data 0.412 (0.330) loss_u loss_u 0.9858 (0.9636) acc_u 0.0000 (4.3750) lr 4.1221e-04 eta 0:00:33
epoch [142/200] batch [15/82] time 0.467 (0.460) data 0.336 (0.329) loss_u loss_u 0.9883 (0.9690) acc_u 0.0000 (3.9583) lr 4.1221e-04 eta 0:00:30
epoch [142/200] batch [20/82] time 0.449 (0.455) data 0.317 (0.324) loss_u loss_u 0.9263 (0.9656) acc_u 6.2500 (4.3750) lr 4.1221e-04 eta 0:00:28
epoch [142/200] batch [25/82] time 0.449 (0.454) data 0.317 (0.323) loss_u loss_u 0.9883 (0.9646) acc_u 3.1250 (4.5000) lr 4.1221e-04 eta 0:00:25
epoch [142/200] batch [30/82] time 0.486 (0.452) data 0.354 (0.321) loss_u loss_u 0.9629 (0.9631) acc_u 3.1250 (4.6875) lr 4.1221e-04 eta 0:00:23
epoch [142/200] batch [35/82] time 0.536 (0.454) data 0.404 (0.322) loss_u loss_u 0.9771 (0.9646) acc_u 3.1250 (4.5536) lr 4.1221e-04 eta 0:00:21
epoch [142/200] batch [40/82] time 0.450 (0.452) data 0.318 (0.320) loss_u loss_u 0.9976 (0.9635) acc_u 0.0000 (4.6875) lr 4.1221e-04 eta 0:00:18
epoch [142/200] batch [45/82] time 0.451 (0.451) data 0.319 (0.320) loss_u loss_u 0.9644 (0.9654) acc_u 6.2500 (4.5139) lr 4.1221e-04 eta 0:00:16
epoch [142/200] batch [50/82] time 0.344 (0.449) data 0.212 (0.317) loss_u loss_u 0.9829 (0.9664) acc_u 3.1250 (4.4375) lr 4.1221e-04 eta 0:00:14
epoch [142/200] batch [55/82] time 0.576 (0.447) data 0.444 (0.316) loss_u loss_u 0.9863 (0.9658) acc_u 3.1250 (4.4318) lr 4.1221e-04 eta 0:00:12
epoch [142/200] batch [60/82] time 0.574 (0.447) data 0.442 (0.316) loss_u loss_u 0.9448 (0.9659) acc_u 6.2500 (4.4271) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [65/82] time 0.355 (0.446) data 0.223 (0.314) loss_u loss_u 0.9404 (0.9656) acc_u 6.2500 (4.4231) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [70/82] time 0.471 (0.448) data 0.339 (0.317) loss_u loss_u 0.9038 (0.9652) acc_u 12.5000 (4.4196) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [75/82] time 0.380 (0.448) data 0.248 (0.317) loss_u loss_u 0.9644 (0.9639) acc_u 6.2500 (4.5833) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [80/82] time 0.499 (0.452) data 0.368 (0.320) loss_u loss_u 0.9473 (0.9620) acc_u 6.2500 (4.8438) lr 4.1221e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1702
confident_label rate tensor(0.1547, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 485
clean true:475
clean false:10
clean_rate:0.979381443298969
noisy true:959
noisy false:1692
after delete: len(clean_dataset) 485
after delete: len(noisy_dataset) 2651
epoch [143/200] batch [5/15] time 0.438 (0.467) data 0.307 (0.336) loss_x loss_x 0.8638 (0.9664) acc_x 75.0000 (76.8750) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [10/15] time 0.387 (0.437) data 0.256 (0.306) loss_x loss_x 1.1143 (1.0066) acc_x 71.8750 (76.5625) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [15/15] time 0.427 (0.448) data 0.296 (0.317) loss_x loss_x 1.0459 (1.0223) acc_x 78.1250 (76.0417) lr 3.9958e-04 eta 0:00:00
epoch [143/200] batch [5/82] time 0.464 (0.441) data 0.332 (0.310) loss_u loss_u 0.9648 (0.9738) acc_u 6.2500 (3.7500) lr 3.9958e-04 eta 0:00:33
epoch [143/200] batch [10/82] time 0.486 (0.450) data 0.356 (0.319) loss_u loss_u 0.9868 (0.9688) acc_u 3.1250 (4.3750) lr 3.9958e-04 eta 0:00:32
epoch [143/200] batch [15/82] time 0.522 (0.454) data 0.392 (0.323) loss_u loss_u 0.9727 (0.9570) acc_u 0.0000 (5.6250) lr 3.9958e-04 eta 0:00:30
epoch [143/200] batch [20/82] time 0.388 (0.470) data 0.257 (0.339) loss_u loss_u 0.9355 (0.9601) acc_u 9.3750 (5.3125) lr 3.9958e-04 eta 0:00:29
epoch [143/200] batch [25/82] time 0.391 (0.460) data 0.261 (0.329) loss_u loss_u 0.9551 (0.9604) acc_u 6.2500 (5.1250) lr 3.9958e-04 eta 0:00:26
epoch [143/200] batch [30/82] time 0.485 (0.458) data 0.354 (0.327) loss_u loss_u 0.9644 (0.9608) acc_u 6.2500 (5.1042) lr 3.9958e-04 eta 0:00:23
epoch [143/200] batch [35/82] time 0.479 (0.456) data 0.348 (0.325) loss_u loss_u 0.9536 (0.9612) acc_u 6.2500 (5.0893) lr 3.9958e-04 eta 0:00:21
epoch [143/200] batch [40/82] time 0.394 (0.455) data 0.263 (0.324) loss_u loss_u 0.9902 (0.9614) acc_u 0.0000 (4.9219) lr 3.9958e-04 eta 0:00:19
epoch [143/200] batch [45/82] time 0.322 (0.452) data 0.190 (0.320) loss_u loss_u 0.9429 (0.9616) acc_u 3.1250 (4.9306) lr 3.9958e-04 eta 0:00:16
epoch [143/200] batch [50/82] time 0.372 (0.449) data 0.241 (0.318) loss_u loss_u 0.9902 (0.9627) acc_u 3.1250 (4.7500) lr 3.9958e-04 eta 0:00:14
epoch [143/200] batch [55/82] time 0.329 (0.448) data 0.198 (0.316) loss_u loss_u 0.9624 (0.9632) acc_u 3.1250 (4.6591) lr 3.9958e-04 eta 0:00:12
epoch [143/200] batch [60/82] time 0.344 (0.447) data 0.213 (0.316) loss_u loss_u 0.9766 (0.9638) acc_u 6.2500 (4.5312) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [65/82] time 0.431 (0.447) data 0.300 (0.316) loss_u loss_u 0.9731 (0.9628) acc_u 0.0000 (4.5673) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [70/82] time 0.548 (0.448) data 0.417 (0.317) loss_u loss_u 0.9912 (0.9626) acc_u 0.0000 (4.5982) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [75/82] time 0.448 (0.451) data 0.317 (0.320) loss_u loss_u 0.9746 (0.9633) acc_u 3.1250 (4.5417) lr 3.9958e-04 eta 0:00:03
epoch [143/200] batch [80/82] time 0.454 (0.452) data 0.323 (0.321) loss_u loss_u 0.9683 (0.9641) acc_u 3.1250 (4.4141) lr 3.9958e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1718
confident_label rate tensor(0.1543, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 484
clean true:469
clean false:15
clean_rate:0.96900826446281
noisy true:949
noisy false:1703
after delete: len(clean_dataset) 484
after delete: len(noisy_dataset) 2652
epoch [144/200] batch [5/15] time 0.562 (0.472) data 0.432 (0.342) loss_x loss_x 1.3418 (1.2064) acc_x 75.0000 (75.6250) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [10/15] time 0.378 (0.445) data 0.248 (0.315) loss_x loss_x 1.3809 (1.1851) acc_x 62.5000 (72.5000) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [15/15] time 0.514 (0.443) data 0.384 (0.312) loss_x loss_x 0.8569 (1.1644) acc_x 81.2500 (72.0833) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/82] time 0.575 (0.441) data 0.444 (0.311) loss_u loss_u 0.9878 (0.9706) acc_u 3.1250 (5.0000) lr 3.8709e-04 eta 0:00:33
epoch [144/200] batch [10/82] time 0.458 (0.443) data 0.328 (0.312) loss_u loss_u 0.9644 (0.9679) acc_u 3.1250 (4.6875) lr 3.8709e-04 eta 0:00:31
epoch [144/200] batch [15/82] time 0.425 (0.436) data 0.295 (0.306) loss_u loss_u 0.9746 (0.9659) acc_u 3.1250 (4.5833) lr 3.8709e-04 eta 0:00:29
epoch [144/200] batch [20/82] time 0.719 (0.457) data 0.588 (0.326) loss_u loss_u 0.9131 (0.9650) acc_u 9.3750 (4.3750) lr 3.8709e-04 eta 0:00:28
epoch [144/200] batch [25/82] time 0.448 (0.459) data 0.317 (0.328) loss_u loss_u 0.9277 (0.9652) acc_u 9.3750 (4.3750) lr 3.8709e-04 eta 0:00:26
epoch [144/200] batch [30/82] time 0.505 (0.459) data 0.374 (0.328) loss_u loss_u 0.9268 (0.9652) acc_u 9.3750 (4.3750) lr 3.8709e-04 eta 0:00:23
epoch [144/200] batch [35/82] time 0.533 (0.466) data 0.402 (0.335) loss_u loss_u 0.9600 (0.9636) acc_u 6.2500 (4.6429) lr 3.8709e-04 eta 0:00:21
epoch [144/200] batch [40/82] time 0.401 (0.464) data 0.270 (0.334) loss_u loss_u 0.9341 (0.9603) acc_u 9.3750 (5.1562) lr 3.8709e-04 eta 0:00:19
epoch [144/200] batch [45/82] time 0.539 (0.464) data 0.407 (0.333) loss_u loss_u 0.9580 (0.9614) acc_u 3.1250 (4.8611) lr 3.8709e-04 eta 0:00:17
epoch [144/200] batch [50/82] time 0.339 (0.460) data 0.208 (0.329) loss_u loss_u 0.9961 (0.9607) acc_u 0.0000 (4.8750) lr 3.8709e-04 eta 0:00:14
epoch [144/200] batch [55/82] time 0.539 (0.460) data 0.409 (0.329) loss_u loss_u 0.9971 (0.9613) acc_u 0.0000 (4.8295) lr 3.8709e-04 eta 0:00:12
epoch [144/200] batch [60/82] time 0.486 (0.458) data 0.355 (0.327) loss_u loss_u 0.9561 (0.9613) acc_u 6.2500 (4.8438) lr 3.8709e-04 eta 0:00:10
epoch [144/200] batch [65/82] time 0.390 (0.458) data 0.260 (0.328) loss_u loss_u 0.9604 (0.9616) acc_u 6.2500 (4.8558) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [70/82] time 0.426 (0.458) data 0.296 (0.327) loss_u loss_u 0.9365 (0.9613) acc_u 6.2500 (4.8214) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [75/82] time 0.355 (0.456) data 0.224 (0.325) loss_u loss_u 0.9243 (0.9616) acc_u 9.3750 (4.7500) lr 3.8709e-04 eta 0:00:03
epoch [144/200] batch [80/82] time 0.495 (0.458) data 0.364 (0.328) loss_u loss_u 0.9561 (0.9609) acc_u 3.1250 (4.8047) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1715
confident_label rate tensor(0.1566, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 491
clean true:468
clean false:23
clean_rate:0.9531568228105907
noisy true:953
noisy false:1692
after delete: len(clean_dataset) 491
after delete: len(noisy_dataset) 2645
epoch [145/200] batch [5/15] time 0.422 (0.480) data 0.292 (0.350) loss_x loss_x 1.2061 (0.9651) acc_x 68.7500 (78.1250) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [10/15] time 0.397 (0.466) data 0.267 (0.336) loss_x loss_x 1.2480 (0.9470) acc_x 75.0000 (75.9375) lr 3.7476e-04 eta 0:00:02
epoch [145/200] batch [15/15] time 0.361 (0.442) data 0.230 (0.312) loss_x loss_x 1.6377 (1.1108) acc_x 65.6250 (73.7500) lr 3.7476e-04 eta 0:00:00
epoch [145/200] batch [5/82] time 0.648 (0.470) data 0.516 (0.340) loss_u loss_u 0.9473 (0.9563) acc_u 9.3750 (5.6250) lr 3.7476e-04 eta 0:00:36
epoch [145/200] batch [10/82] time 0.482 (0.475) data 0.352 (0.344) loss_u loss_u 0.9751 (0.9552) acc_u 3.1250 (5.9375) lr 3.7476e-04 eta 0:00:34
epoch [145/200] batch [15/82] time 0.324 (0.460) data 0.193 (0.329) loss_u loss_u 0.9995 (0.9585) acc_u 0.0000 (5.6250) lr 3.7476e-04 eta 0:00:30
epoch [145/200] batch [20/82] time 0.652 (0.474) data 0.521 (0.343) loss_u loss_u 0.9282 (0.9590) acc_u 6.2500 (5.1562) lr 3.7476e-04 eta 0:00:29
epoch [145/200] batch [25/82] time 0.409 (0.480) data 0.278 (0.349) loss_u loss_u 0.9941 (0.9604) acc_u 0.0000 (4.8750) lr 3.7476e-04 eta 0:00:27
epoch [145/200] batch [30/82] time 0.334 (0.476) data 0.203 (0.345) loss_u loss_u 0.9775 (0.9633) acc_u 3.1250 (4.4792) lr 3.7476e-04 eta 0:00:24
epoch [145/200] batch [35/82] time 0.387 (0.472) data 0.256 (0.341) loss_u loss_u 0.9604 (0.9632) acc_u 3.1250 (4.2857) lr 3.7476e-04 eta 0:00:22
epoch [145/200] batch [40/82] time 0.403 (0.466) data 0.271 (0.335) loss_u loss_u 0.9541 (0.9605) acc_u 6.2500 (4.6875) lr 3.7476e-04 eta 0:00:19
epoch [145/200] batch [45/82] time 0.503 (0.462) data 0.372 (0.331) loss_u loss_u 0.9360 (0.9627) acc_u 6.2500 (4.3750) lr 3.7476e-04 eta 0:00:17
epoch [145/200] batch [50/82] time 0.354 (0.456) data 0.223 (0.325) loss_u loss_u 0.9805 (0.9629) acc_u 3.1250 (4.5000) lr 3.7476e-04 eta 0:00:14
epoch [145/200] batch [55/82] time 0.425 (0.453) data 0.294 (0.323) loss_u loss_u 0.9644 (0.9620) acc_u 6.2500 (4.6591) lr 3.7476e-04 eta 0:00:12
epoch [145/200] batch [60/82] time 0.351 (0.452) data 0.220 (0.321) loss_u loss_u 0.9902 (0.9621) acc_u 0.0000 (4.6875) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [65/82] time 0.411 (0.450) data 0.280 (0.319) loss_u loss_u 0.9531 (0.9612) acc_u 6.2500 (4.8558) lr 3.7476e-04 eta 0:00:07
epoch [145/200] batch [70/82] time 0.390 (0.450) data 0.259 (0.319) loss_u loss_u 0.9409 (0.9614) acc_u 6.2500 (4.8661) lr 3.7476e-04 eta 0:00:05
epoch [145/200] batch [75/82] time 0.496 (0.449) data 0.365 (0.318) loss_u loss_u 0.9473 (0.9616) acc_u 6.2500 (4.7500) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [80/82] time 0.430 (0.450) data 0.299 (0.319) loss_u loss_u 0.9175 (0.9614) acc_u 12.5000 (4.8438) lr 3.7476e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1747
confident_label rate tensor(0.1547, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 485
clean true:463
clean false:22
clean_rate:0.954639175257732
noisy true:926
noisy false:1725
after delete: len(clean_dataset) 485
after delete: len(noisy_dataset) 2651
epoch [146/200] batch [5/15] time 0.498 (0.476) data 0.367 (0.345) loss_x loss_x 1.7588 (1.0896) acc_x 56.2500 (71.2500) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [10/15] time 0.492 (0.463) data 0.362 (0.333) loss_x loss_x 0.7402 (0.9397) acc_x 81.2500 (76.2500) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [15/15] time 0.575 (0.455) data 0.445 (0.325) loss_x loss_x 1.0234 (0.9563) acc_x 71.8750 (76.2500) lr 3.6258e-04 eta 0:00:00
epoch [146/200] batch [5/82] time 0.357 (0.449) data 0.226 (0.318) loss_u loss_u 0.9438 (0.9292) acc_u 6.2500 (10.0000) lr 3.6258e-04 eta 0:00:34
epoch [146/200] batch [10/82] time 0.412 (0.443) data 0.281 (0.312) loss_u loss_u 0.9902 (0.9452) acc_u 0.0000 (6.8750) lr 3.6258e-04 eta 0:00:31
epoch [146/200] batch [15/82] time 0.414 (0.450) data 0.282 (0.319) loss_u loss_u 0.9995 (0.9509) acc_u 0.0000 (6.6667) lr 3.6258e-04 eta 0:00:30
epoch [146/200] batch [20/82] time 0.402 (0.456) data 0.270 (0.325) loss_u loss_u 0.9102 (0.9498) acc_u 9.3750 (6.5625) lr 3.6258e-04 eta 0:00:28
epoch [146/200] batch [25/82] time 0.628 (0.463) data 0.497 (0.332) loss_u loss_u 0.9697 (0.9556) acc_u 3.1250 (5.6250) lr 3.6258e-04 eta 0:00:26
epoch [146/200] batch [30/82] time 0.510 (0.467) data 0.378 (0.336) loss_u loss_u 0.9736 (0.9593) acc_u 3.1250 (5.2083) lr 3.6258e-04 eta 0:00:24
epoch [146/200] batch [35/82] time 0.498 (0.468) data 0.366 (0.337) loss_u loss_u 0.9702 (0.9605) acc_u 3.1250 (5.0000) lr 3.6258e-04 eta 0:00:21
epoch [146/200] batch [40/82] time 0.435 (0.464) data 0.304 (0.333) loss_u loss_u 0.9956 (0.9610) acc_u 0.0000 (4.9219) lr 3.6258e-04 eta 0:00:19
epoch [146/200] batch [45/82] time 0.393 (0.461) data 0.262 (0.330) loss_u loss_u 0.8984 (0.9608) acc_u 15.6250 (5.0000) lr 3.6258e-04 eta 0:00:17
epoch [146/200] batch [50/82] time 0.369 (0.458) data 0.237 (0.327) loss_u loss_u 0.9814 (0.9603) acc_u 3.1250 (4.9375) lr 3.6258e-04 eta 0:00:14
epoch [146/200] batch [55/82] time 0.464 (0.455) data 0.333 (0.324) loss_u loss_u 0.9985 (0.9603) acc_u 0.0000 (5.0568) lr 3.6258e-04 eta 0:00:12
epoch [146/200] batch [60/82] time 0.463 (0.452) data 0.331 (0.321) loss_u loss_u 0.9971 (0.9617) acc_u 0.0000 (4.7917) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [65/82] time 0.359 (0.453) data 0.228 (0.321) loss_u loss_u 0.8999 (0.9575) acc_u 12.5000 (5.2404) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [70/82] time 0.465 (0.454) data 0.334 (0.323) loss_u loss_u 0.9966 (0.9574) acc_u 0.0000 (5.3125) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [75/82] time 0.647 (0.456) data 0.516 (0.325) loss_u loss_u 0.9619 (0.9577) acc_u 3.1250 (5.2083) lr 3.6258e-04 eta 0:00:03
epoch [146/200] batch [80/82] time 0.416 (0.457) data 0.286 (0.326) loss_u loss_u 0.9585 (0.9574) acc_u 6.2500 (5.2734) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1741
confident_label rate tensor(0.1623, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 509
clean true:489
clean false:20
clean_rate:0.9607072691552063
noisy true:906
noisy false:1721
after delete: len(clean_dataset) 509
after delete: len(noisy_dataset) 2627
epoch [147/200] batch [5/15] time 0.504 (0.510) data 0.374 (0.380) loss_x loss_x 1.4326 (1.1817) acc_x 75.0000 (71.2500) lr 3.5055e-04 eta 0:00:05
epoch [147/200] batch [10/15] time 0.509 (0.494) data 0.380 (0.364) loss_x loss_x 0.7236 (1.1828) acc_x 81.2500 (72.5000) lr 3.5055e-04 eta 0:00:02
epoch [147/200] batch [15/15] time 0.465 (0.490) data 0.335 (0.359) loss_x loss_x 1.0635 (1.1362) acc_x 84.3750 (73.7500) lr 3.5055e-04 eta 0:00:00
epoch [147/200] batch [5/82] time 0.427 (0.485) data 0.295 (0.355) loss_u loss_u 0.9297 (0.9584) acc_u 6.2500 (4.3750) lr 3.5055e-04 eta 0:00:37
epoch [147/200] batch [10/82] time 0.445 (0.483) data 0.314 (0.353) loss_u loss_u 0.9482 (0.9639) acc_u 3.1250 (4.0625) lr 3.5055e-04 eta 0:00:34
epoch [147/200] batch [15/82] time 0.451 (0.479) data 0.321 (0.349) loss_u loss_u 0.9399 (0.9643) acc_u 3.1250 (3.5417) lr 3.5055e-04 eta 0:00:32
epoch [147/200] batch [20/82] time 0.443 (0.472) data 0.311 (0.341) loss_u loss_u 0.9766 (0.9622) acc_u 3.1250 (4.2188) lr 3.5055e-04 eta 0:00:29
epoch [147/200] batch [25/82] time 0.383 (0.466) data 0.251 (0.335) loss_u loss_u 0.9873 (0.9621) acc_u 3.1250 (4.3750) lr 3.5055e-04 eta 0:00:26
epoch [147/200] batch [30/82] time 0.467 (0.476) data 0.336 (0.345) loss_u loss_u 0.9976 (0.9660) acc_u 0.0000 (3.9583) lr 3.5055e-04 eta 0:00:24
epoch [147/200] batch [35/82] time 0.589 (0.483) data 0.457 (0.352) loss_u loss_u 0.9941 (0.9673) acc_u 0.0000 (3.8393) lr 3.5055e-04 eta 0:00:22
epoch [147/200] batch [40/82] time 0.458 (0.478) data 0.327 (0.347) loss_u loss_u 0.9521 (0.9682) acc_u 6.2500 (3.8281) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [45/82] time 0.393 (0.479) data 0.262 (0.348) loss_u loss_u 0.9917 (0.9680) acc_u 0.0000 (4.0278) lr 3.5055e-04 eta 0:00:17
epoch [147/200] batch [50/82] time 0.429 (0.482) data 0.297 (0.350) loss_u loss_u 0.9644 (0.9664) acc_u 6.2500 (4.3125) lr 3.5055e-04 eta 0:00:15
epoch [147/200] batch [55/82] time 0.393 (0.478) data 0.261 (0.347) loss_u loss_u 0.9619 (0.9669) acc_u 3.1250 (4.1477) lr 3.5055e-04 eta 0:00:12
epoch [147/200] batch [60/82] time 0.479 (0.476) data 0.347 (0.344) loss_u loss_u 0.8740 (0.9641) acc_u 15.6250 (4.4792) lr 3.5055e-04 eta 0:00:10
epoch [147/200] batch [65/82] time 0.512 (0.472) data 0.381 (0.341) loss_u loss_u 0.9746 (0.9644) acc_u 6.2500 (4.6154) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [70/82] time 0.449 (0.468) data 0.317 (0.337) loss_u loss_u 0.9639 (0.9634) acc_u 3.1250 (4.6875) lr 3.5055e-04 eta 0:00:05
epoch [147/200] batch [75/82] time 0.463 (0.468) data 0.332 (0.337) loss_u loss_u 0.9316 (0.9614) acc_u 6.2500 (4.8750) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [80/82] time 0.572 (0.470) data 0.440 (0.339) loss_u loss_u 0.9150 (0.9608) acc_u 12.5000 (4.9219) lr 3.5055e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1711
confident_label rate tensor(0.1617, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 507
clean true:486
clean false:21
clean_rate:0.9585798816568047
noisy true:939
noisy false:1690
after delete: len(clean_dataset) 507
after delete: len(noisy_dataset) 2629
epoch [148/200] batch [5/15] time 0.432 (0.497) data 0.301 (0.366) loss_x loss_x 1.1904 (0.8431) acc_x 68.7500 (78.1250) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [10/15] time 0.553 (0.495) data 0.423 (0.365) loss_x loss_x 0.6050 (0.9387) acc_x 81.2500 (76.5625) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [15/15] time 0.495 (0.488) data 0.365 (0.357) loss_x loss_x 1.1729 (0.9804) acc_x 71.8750 (76.2500) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/82] time 0.361 (0.468) data 0.231 (0.338) loss_u loss_u 0.9985 (0.9754) acc_u 0.0000 (2.5000) lr 3.3869e-04 eta 0:00:36
epoch [148/200] batch [10/82] time 0.397 (0.470) data 0.267 (0.339) loss_u loss_u 0.8965 (0.9689) acc_u 15.6250 (3.4375) lr 3.3869e-04 eta 0:00:33
epoch [148/200] batch [15/82] time 0.342 (0.461) data 0.210 (0.331) loss_u loss_u 0.9653 (0.9695) acc_u 3.1250 (3.1250) lr 3.3869e-04 eta 0:00:30
epoch [148/200] batch [20/82] time 0.527 (0.470) data 0.396 (0.339) loss_u loss_u 0.9434 (0.9667) acc_u 9.3750 (3.5938) lr 3.3869e-04 eta 0:00:29
epoch [148/200] batch [25/82] time 0.431 (0.467) data 0.300 (0.336) loss_u loss_u 0.9985 (0.9700) acc_u 0.0000 (3.3750) lr 3.3869e-04 eta 0:00:26
epoch [148/200] batch [30/82] time 0.461 (0.466) data 0.330 (0.335) loss_u loss_u 0.9043 (0.9692) acc_u 12.5000 (3.5417) lr 3.3869e-04 eta 0:00:24
epoch [148/200] batch [35/82] time 0.363 (0.462) data 0.232 (0.331) loss_u loss_u 0.9404 (0.9684) acc_u 6.2500 (3.7500) lr 3.3869e-04 eta 0:00:21
epoch [148/200] batch [40/82] time 0.379 (0.459) data 0.248 (0.328) loss_u loss_u 0.9473 (0.9653) acc_u 6.2500 (4.2188) lr 3.3869e-04 eta 0:00:19
epoch [148/200] batch [45/82] time 0.543 (0.461) data 0.411 (0.330) loss_u loss_u 0.9365 (0.9654) acc_u 6.2500 (4.1667) lr 3.3869e-04 eta 0:00:17
epoch [148/200] batch [50/82] time 0.577 (0.466) data 0.446 (0.336) loss_u loss_u 0.9302 (0.9627) acc_u 9.3750 (4.5625) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [55/82] time 0.451 (0.468) data 0.321 (0.338) loss_u loss_u 0.9673 (0.9623) acc_u 3.1250 (4.6023) lr 3.3869e-04 eta 0:00:12
epoch [148/200] batch [60/82] time 0.448 (0.466) data 0.318 (0.336) loss_u loss_u 0.9961 (0.9634) acc_u 0.0000 (4.5833) lr 3.3869e-04 eta 0:00:10
epoch [148/200] batch [65/82] time 0.457 (0.465) data 0.325 (0.334) loss_u loss_u 0.9995 (0.9649) acc_u 0.0000 (4.3750) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [70/82] time 0.469 (0.464) data 0.337 (0.333) loss_u loss_u 0.9917 (0.9646) acc_u 0.0000 (4.3750) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [75/82] time 0.366 (0.464) data 0.234 (0.333) loss_u loss_u 0.9766 (0.9647) acc_u 3.1250 (4.3333) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [80/82] time 0.328 (0.462) data 0.197 (0.331) loss_u loss_u 0.9868 (0.9650) acc_u 3.1250 (4.3359) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1680
confident_label rate tensor(0.1617, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 507
clean true:486
clean false:21
clean_rate:0.9585798816568047
noisy true:970
noisy false:1659
after delete: len(clean_dataset) 507
after delete: len(noisy_dataset) 2629
epoch [149/200] batch [5/15] time 0.470 (0.463) data 0.338 (0.331) loss_x loss_x 0.8936 (1.1325) acc_x 78.1250 (73.7500) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [10/15] time 0.463 (0.493) data 0.332 (0.362) loss_x loss_x 0.8765 (1.1253) acc_x 75.0000 (71.8750) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [15/15] time 0.560 (0.483) data 0.430 (0.352) loss_x loss_x 1.4492 (1.1435) acc_x 71.8750 (72.2917) lr 3.2699e-04 eta 0:00:00
epoch [149/200] batch [5/82] time 0.392 (0.470) data 0.262 (0.339) loss_u loss_u 0.9570 (0.9688) acc_u 9.3750 (4.3750) lr 3.2699e-04 eta 0:00:36
epoch [149/200] batch [10/82] time 0.379 (0.456) data 0.249 (0.326) loss_u loss_u 0.9492 (0.9659) acc_u 6.2500 (4.6875) lr 3.2699e-04 eta 0:00:32
epoch [149/200] batch [15/82] time 0.517 (0.457) data 0.385 (0.326) loss_u loss_u 0.9634 (0.9611) acc_u 3.1250 (5.0000) lr 3.2699e-04 eta 0:00:30
epoch [149/200] batch [20/82] time 0.491 (0.456) data 0.359 (0.325) loss_u loss_u 0.9888 (0.9611) acc_u 0.0000 (4.6875) lr 3.2699e-04 eta 0:00:28
epoch [149/200] batch [25/82] time 0.450 (0.461) data 0.318 (0.331) loss_u loss_u 0.9839 (0.9650) acc_u 0.0000 (4.1250) lr 3.2699e-04 eta 0:00:26
epoch [149/200] batch [30/82] time 0.449 (0.458) data 0.319 (0.327) loss_u loss_u 0.8916 (0.9628) acc_u 18.7500 (4.6875) lr 3.2699e-04 eta 0:00:23
epoch [149/200] batch [35/82] time 0.494 (0.464) data 0.362 (0.333) loss_u loss_u 0.9863 (0.9650) acc_u 3.1250 (4.4643) lr 3.2699e-04 eta 0:00:21
epoch [149/200] batch [40/82] time 0.494 (0.464) data 0.362 (0.333) loss_u loss_u 0.9927 (0.9657) acc_u 0.0000 (4.2188) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [45/82] time 0.338 (0.468) data 0.207 (0.337) loss_u loss_u 0.9790 (0.9673) acc_u 3.1250 (4.0972) lr 3.2699e-04 eta 0:00:17
epoch [149/200] batch [50/82] time 0.356 (0.466) data 0.224 (0.335) loss_u loss_u 0.9819 (0.9670) acc_u 3.1250 (4.1250) lr 3.2699e-04 eta 0:00:14
epoch [149/200] batch [55/82] time 0.445 (0.464) data 0.314 (0.333) loss_u loss_u 0.9648 (0.9664) acc_u 3.1250 (4.3750) lr 3.2699e-04 eta 0:00:12
epoch [149/200] batch [60/82] time 0.420 (0.463) data 0.289 (0.332) loss_u loss_u 0.9761 (0.9652) acc_u 3.1250 (4.5312) lr 3.2699e-04 eta 0:00:10
epoch [149/200] batch [65/82] time 0.482 (0.464) data 0.352 (0.333) loss_u loss_u 0.9136 (0.9640) acc_u 9.3750 (4.7115) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [70/82] time 0.376 (0.464) data 0.245 (0.333) loss_u loss_u 0.9961 (0.9644) acc_u 0.0000 (4.6875) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [75/82] time 0.497 (0.462) data 0.365 (0.331) loss_u loss_u 0.9404 (0.9635) acc_u 9.3750 (4.8750) lr 3.2699e-04 eta 0:00:03
epoch [149/200] batch [80/82] time 0.783 (0.465) data 0.653 (0.334) loss_u loss_u 0.8467 (0.9628) acc_u 21.8750 (4.9609) lr 3.2699e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1731
confident_label rate tensor(0.1639, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 514
clean true:494
clean false:20
clean_rate:0.9610894941634242
noisy true:911
noisy false:1711
after delete: len(clean_dataset) 514
after delete: len(noisy_dataset) 2622
epoch [150/200] batch [5/16] time 0.461 (0.524) data 0.331 (0.393) loss_x loss_x 1.2041 (1.1844) acc_x 81.2500 (75.0000) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [10/16] time 0.418 (0.496) data 0.289 (0.366) loss_x loss_x 0.7070 (1.0527) acc_x 84.3750 (76.2500) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [15/16] time 0.379 (0.479) data 0.249 (0.349) loss_x loss_x 1.4219 (1.0984) acc_x 56.2500 (74.3750) lr 3.1545e-04 eta 0:00:00
epoch [150/200] batch [5/81] time 0.438 (0.467) data 0.309 (0.337) loss_u loss_u 0.9810 (0.9530) acc_u 3.1250 (7.5000) lr 3.1545e-04 eta 0:00:35
epoch [150/200] batch [10/81] time 0.651 (0.464) data 0.520 (0.334) loss_u loss_u 0.9331 (0.9576) acc_u 9.3750 (6.2500) lr 3.1545e-04 eta 0:00:32
epoch [150/200] batch [15/81] time 0.401 (0.458) data 0.271 (0.328) loss_u loss_u 0.9756 (0.9609) acc_u 0.0000 (5.2083) lr 3.1545e-04 eta 0:00:30
epoch [150/200] batch [20/81] time 0.418 (0.459) data 0.289 (0.328) loss_u loss_u 0.9712 (0.9670) acc_u 3.1250 (4.2188) lr 3.1545e-04 eta 0:00:27
epoch [150/200] batch [25/81] time 0.384 (0.452) data 0.253 (0.322) loss_u loss_u 0.9668 (0.9658) acc_u 3.1250 (4.5000) lr 3.1545e-04 eta 0:00:25
epoch [150/200] batch [30/81] time 0.588 (0.460) data 0.457 (0.329) loss_u loss_u 0.9307 (0.9652) acc_u 9.3750 (4.3750) lr 3.1545e-04 eta 0:00:23
epoch [150/200] batch [35/81] time 0.456 (0.458) data 0.326 (0.327) loss_u loss_u 0.9775 (0.9653) acc_u 3.1250 (4.4643) lr 3.1545e-04 eta 0:00:21
epoch [150/200] batch [40/81] time 0.374 (0.456) data 0.244 (0.326) loss_u loss_u 0.9932 (0.9644) acc_u 0.0000 (4.5312) lr 3.1545e-04 eta 0:00:18
epoch [150/200] batch [45/81] time 0.410 (0.453) data 0.279 (0.323) loss_u loss_u 0.9946 (0.9657) acc_u 0.0000 (4.4444) lr 3.1545e-04 eta 0:00:16
epoch [150/200] batch [50/81] time 0.405 (0.452) data 0.274 (0.322) loss_u loss_u 0.9683 (0.9649) acc_u 3.1250 (4.5625) lr 3.1545e-04 eta 0:00:14
epoch [150/200] batch [55/81] time 0.487 (0.450) data 0.357 (0.319) loss_u loss_u 0.9258 (0.9621) acc_u 9.3750 (4.9432) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [60/81] time 0.441 (0.449) data 0.310 (0.319) loss_u loss_u 0.9937 (0.9621) acc_u 0.0000 (4.8438) lr 3.1545e-04 eta 0:00:09
epoch [150/200] batch [65/81] time 0.403 (0.449) data 0.274 (0.318) loss_u loss_u 0.9512 (0.9627) acc_u 6.2500 (4.8077) lr 3.1545e-04 eta 0:00:07
epoch [150/200] batch [70/81] time 0.434 (0.450) data 0.303 (0.320) loss_u loss_u 0.9897 (0.9637) acc_u 0.0000 (4.6429) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [75/81] time 0.398 (0.449) data 0.267 (0.318) loss_u loss_u 0.9712 (0.9635) acc_u 3.1250 (4.6667) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [80/81] time 0.406 (0.448) data 0.275 (0.318) loss_u loss_u 0.9282 (0.9630) acc_u 9.3750 (4.7266) lr 3.1545e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1707
confident_label rate tensor(0.1636, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 513
clean true:492
clean false:21
clean_rate:0.9590643274853801
noisy true:937
noisy false:1686
after delete: len(clean_dataset) 513
after delete: len(noisy_dataset) 2623
epoch [151/200] batch [5/16] time 0.423 (0.439) data 0.292 (0.308) loss_x loss_x 0.9292 (1.1993) acc_x 81.2500 (73.1250) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [10/16] time 0.437 (0.441) data 0.306 (0.310) loss_x loss_x 1.4678 (1.0660) acc_x 65.6250 (74.3750) lr 3.0409e-04 eta 0:00:02
epoch [151/200] batch [15/16] time 0.406 (0.464) data 0.276 (0.333) loss_x loss_x 0.9287 (1.0118) acc_x 78.1250 (75.8333) lr 3.0409e-04 eta 0:00:00
epoch [151/200] batch [5/81] time 0.536 (0.472) data 0.406 (0.342) loss_u loss_u 0.9243 (0.9519) acc_u 9.3750 (7.5000) lr 3.0409e-04 eta 0:00:35
epoch [151/200] batch [10/81] time 0.641 (0.473) data 0.511 (0.343) loss_u loss_u 0.9258 (0.9515) acc_u 6.2500 (7.5000) lr 3.0409e-04 eta 0:00:33
epoch [151/200] batch [15/81] time 0.710 (0.482) data 0.578 (0.351) loss_u loss_u 0.9214 (0.9578) acc_u 15.6250 (6.4583) lr 3.0409e-04 eta 0:00:31
epoch [151/200] batch [20/81] time 0.419 (0.474) data 0.289 (0.344) loss_u loss_u 0.9648 (0.9624) acc_u 9.3750 (6.0938) lr 3.0409e-04 eta 0:00:28
epoch [151/200] batch [25/81] time 0.470 (0.472) data 0.340 (0.341) loss_u loss_u 0.9751 (0.9618) acc_u 3.1250 (6.2500) lr 3.0409e-04 eta 0:00:26
epoch [151/200] batch [30/81] time 0.427 (0.477) data 0.296 (0.346) loss_u loss_u 0.9971 (0.9661) acc_u 0.0000 (5.3125) lr 3.0409e-04 eta 0:00:24
epoch [151/200] batch [35/81] time 0.479 (0.476) data 0.347 (0.346) loss_u loss_u 0.9521 (0.9664) acc_u 3.1250 (5.0893) lr 3.0409e-04 eta 0:00:21
epoch [151/200] batch [40/81] time 0.431 (0.470) data 0.301 (0.339) loss_u loss_u 0.9355 (0.9624) acc_u 9.3750 (5.6250) lr 3.0409e-04 eta 0:00:19
epoch [151/200] batch [45/81] time 0.473 (0.473) data 0.342 (0.343) loss_u loss_u 0.9482 (0.9627) acc_u 6.2500 (5.4861) lr 3.0409e-04 eta 0:00:17
epoch [151/200] batch [50/81] time 0.478 (0.474) data 0.347 (0.344) loss_u loss_u 0.9985 (0.9638) acc_u 0.0000 (5.3125) lr 3.0409e-04 eta 0:00:14
epoch [151/200] batch [55/81] time 0.429 (0.475) data 0.298 (0.344) loss_u loss_u 0.9590 (0.9643) acc_u 6.2500 (5.2841) lr 3.0409e-04 eta 0:00:12
epoch [151/200] batch [60/81] time 0.441 (0.472) data 0.310 (0.342) loss_u loss_u 0.9912 (0.9633) acc_u 0.0000 (5.3125) lr 3.0409e-04 eta 0:00:09
epoch [151/200] batch [65/81] time 0.430 (0.470) data 0.299 (0.340) loss_u loss_u 0.9697 (0.9631) acc_u 6.2500 (5.2885) lr 3.0409e-04 eta 0:00:07
epoch [151/200] batch [70/81] time 0.448 (0.473) data 0.318 (0.342) loss_u loss_u 0.9976 (0.9639) acc_u 0.0000 (5.1339) lr 3.0409e-04 eta 0:00:05
epoch [151/200] batch [75/81] time 0.416 (0.471) data 0.286 (0.340) loss_u loss_u 0.9565 (0.9633) acc_u 6.2500 (5.2083) lr 3.0409e-04 eta 0:00:02
epoch [151/200] batch [80/81] time 0.430 (0.467) data 0.298 (0.337) loss_u loss_u 0.9673 (0.9641) acc_u 3.1250 (5.0000) lr 3.0409e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1689
confident_label rate tensor(0.1655, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 519
clean true:505
clean false:14
clean_rate:0.9730250481695568
noisy true:942
noisy false:1675
after delete: len(clean_dataset) 519
after delete: len(noisy_dataset) 2617
epoch [152/200] batch [5/16] time 0.471 (0.510) data 0.340 (0.379) loss_x loss_x 1.5654 (0.9171) acc_x 62.5000 (76.2500) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [10/16] time 0.358 (0.486) data 0.227 (0.355) loss_x loss_x 0.7754 (0.8719) acc_x 81.2500 (77.1875) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [15/16] time 0.530 (0.478) data 0.400 (0.347) loss_x loss_x 1.6865 (1.0251) acc_x 65.6250 (73.5417) lr 2.9289e-04 eta 0:00:00
epoch [152/200] batch [5/81] time 0.540 (0.474) data 0.409 (0.344) loss_u loss_u 0.9824 (0.9667) acc_u 0.0000 (3.7500) lr 2.9289e-04 eta 0:00:36
epoch [152/200] batch [10/81] time 0.405 (0.461) data 0.275 (0.330) loss_u loss_u 0.9434 (0.9678) acc_u 9.3750 (4.0625) lr 2.9289e-04 eta 0:00:32
epoch [152/200] batch [15/81] time 0.473 (0.458) data 0.342 (0.327) loss_u loss_u 0.9673 (0.9660) acc_u 3.1250 (4.1667) lr 2.9289e-04 eta 0:00:30
epoch [152/200] batch [20/81] time 0.645 (0.465) data 0.513 (0.334) loss_u loss_u 0.9580 (0.9649) acc_u 6.2500 (4.6875) lr 2.9289e-04 eta 0:00:28
epoch [152/200] batch [25/81] time 0.697 (0.490) data 0.566 (0.359) loss_u loss_u 0.9658 (0.9669) acc_u 3.1250 (4.2500) lr 2.9289e-04 eta 0:00:27
epoch [152/200] batch [30/81] time 0.490 (0.483) data 0.359 (0.352) loss_u loss_u 0.9604 (0.9661) acc_u 6.2500 (4.5833) lr 2.9289e-04 eta 0:00:24
epoch [152/200] batch [35/81] time 0.589 (0.480) data 0.459 (0.349) loss_u loss_u 0.9990 (0.9682) acc_u 0.0000 (4.2857) lr 2.9289e-04 eta 0:00:22
epoch [152/200] batch [40/81] time 0.340 (0.473) data 0.209 (0.342) loss_u loss_u 0.9282 (0.9672) acc_u 12.5000 (4.4531) lr 2.9289e-04 eta 0:00:19
epoch [152/200] batch [45/81] time 0.446 (0.470) data 0.315 (0.339) loss_u loss_u 0.9272 (0.9645) acc_u 9.3750 (4.6528) lr 2.9289e-04 eta 0:00:16
epoch [152/200] batch [50/81] time 0.451 (0.467) data 0.320 (0.337) loss_u loss_u 0.9980 (0.9652) acc_u 0.0000 (4.5000) lr 2.9289e-04 eta 0:00:14
epoch [152/200] batch [55/81] time 0.440 (0.464) data 0.309 (0.334) loss_u loss_u 0.9775 (0.9646) acc_u 3.1250 (4.6023) lr 2.9289e-04 eta 0:00:12
epoch [152/200] batch [60/81] time 0.449 (0.462) data 0.319 (0.332) loss_u loss_u 0.9697 (0.9626) acc_u 6.2500 (5.0000) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [65/81] time 0.406 (0.461) data 0.274 (0.331) loss_u loss_u 0.9434 (0.9621) acc_u 9.3750 (5.0481) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [70/81] time 0.405 (0.458) data 0.275 (0.328) loss_u loss_u 0.9414 (0.9618) acc_u 6.2500 (5.0446) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [75/81] time 0.506 (0.458) data 0.375 (0.327) loss_u loss_u 0.9727 (0.9616) acc_u 3.1250 (5.0000) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [80/81] time 0.354 (0.456) data 0.223 (0.325) loss_u loss_u 0.9990 (0.9624) acc_u 0.0000 (4.8438) lr 2.9289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1707
confident_label rate tensor(0.1588, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 498
clean true:479
clean false:19
clean_rate:0.9618473895582329
noisy true:950
noisy false:1688
after delete: len(clean_dataset) 498
after delete: len(noisy_dataset) 2638
epoch [153/200] batch [5/15] time 0.674 (0.477) data 0.543 (0.347) loss_x loss_x 1.0068 (1.1427) acc_x 81.2500 (73.1250) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [10/15] time 0.422 (0.457) data 0.292 (0.326) loss_x loss_x 1.4668 (1.1036) acc_x 65.6250 (74.3750) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [15/15] time 0.510 (0.477) data 0.379 (0.346) loss_x loss_x 1.0273 (1.0541) acc_x 81.2500 (75.4167) lr 2.8187e-04 eta 0:00:00
epoch [153/200] batch [5/82] time 0.406 (0.469) data 0.276 (0.338) loss_u loss_u 0.9751 (0.9693) acc_u 3.1250 (3.7500) lr 2.8187e-04 eta 0:00:36
epoch [153/200] batch [10/82] time 0.521 (0.460) data 0.389 (0.329) loss_u loss_u 0.9688 (0.9574) acc_u 3.1250 (4.6875) lr 2.8187e-04 eta 0:00:33
epoch [153/200] batch [15/82] time 0.538 (0.464) data 0.407 (0.334) loss_u loss_u 0.9971 (0.9648) acc_u 0.0000 (3.9583) lr 2.8187e-04 eta 0:00:31
epoch [153/200] batch [20/82] time 0.491 (0.465) data 0.361 (0.334) loss_u loss_u 0.9424 (0.9623) acc_u 9.3750 (4.3750) lr 2.8187e-04 eta 0:00:28
epoch [153/200] batch [25/82] time 0.392 (0.455) data 0.262 (0.324) loss_u loss_u 0.9648 (0.9616) acc_u 6.2500 (4.6250) lr 2.8187e-04 eta 0:00:25
epoch [153/200] batch [30/82] time 0.421 (0.448) data 0.290 (0.317) loss_u loss_u 0.9922 (0.9625) acc_u 0.0000 (4.4792) lr 2.8187e-04 eta 0:00:23
epoch [153/200] batch [35/82] time 0.335 (0.447) data 0.204 (0.316) loss_u loss_u 0.9785 (0.9628) acc_u 6.2500 (4.5536) lr 2.8187e-04 eta 0:00:21
epoch [153/200] batch [40/82] time 0.393 (0.450) data 0.262 (0.319) loss_u loss_u 0.9331 (0.9630) acc_u 6.2500 (4.6094) lr 2.8187e-04 eta 0:00:18
epoch [153/200] batch [45/82] time 0.448 (0.451) data 0.318 (0.320) loss_u loss_u 0.9995 (0.9659) acc_u 0.0000 (4.1667) lr 2.8187e-04 eta 0:00:16
epoch [153/200] batch [50/82] time 0.351 (0.449) data 0.220 (0.318) loss_u loss_u 0.9155 (0.9646) acc_u 15.6250 (4.4375) lr 2.8187e-04 eta 0:00:14
epoch [153/200] batch [55/82] time 0.462 (0.454) data 0.330 (0.324) loss_u loss_u 0.9629 (0.9642) acc_u 3.1250 (4.4886) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [60/82] time 0.477 (0.451) data 0.346 (0.320) loss_u loss_u 0.9937 (0.9647) acc_u 0.0000 (4.4271) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [65/82] time 0.561 (0.451) data 0.431 (0.320) loss_u loss_u 0.9639 (0.9643) acc_u 3.1250 (4.5673) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [70/82] time 0.502 (0.453) data 0.371 (0.322) loss_u loss_u 0.9355 (0.9624) acc_u 9.3750 (4.8214) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [75/82] time 0.395 (0.449) data 0.265 (0.318) loss_u loss_u 0.9961 (0.9600) acc_u 0.0000 (5.2500) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [80/82] time 0.540 (0.452) data 0.409 (0.321) loss_u loss_u 0.9570 (0.9596) acc_u 3.1250 (5.1953) lr 2.8187e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1673
confident_label rate tensor(0.1639, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 514
clean true:498
clean false:16
clean_rate:0.9688715953307393
noisy true:965
noisy false:1657
after delete: len(clean_dataset) 514
after delete: len(noisy_dataset) 2622
epoch [154/200] batch [5/16] time 0.452 (0.458) data 0.322 (0.328) loss_x loss_x 1.0010 (1.0550) acc_x 71.8750 (74.3750) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [10/16] time 0.659 (0.491) data 0.529 (0.361) loss_x loss_x 1.4619 (1.1155) acc_x 65.6250 (72.5000) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [15/16] time 0.441 (0.480) data 0.310 (0.349) loss_x loss_x 1.2432 (1.0955) acc_x 68.7500 (72.7083) lr 2.7103e-04 eta 0:00:00
epoch [154/200] batch [5/81] time 0.595 (0.476) data 0.464 (0.346) loss_u loss_u 0.9673 (0.9440) acc_u 9.3750 (8.1250) lr 2.7103e-04 eta 0:00:36
epoch [154/200] batch [10/81] time 0.620 (0.483) data 0.490 (0.352) loss_u loss_u 0.9229 (0.9546) acc_u 12.5000 (6.5625) lr 2.7103e-04 eta 0:00:34
epoch [154/200] batch [15/81] time 0.456 (0.478) data 0.325 (0.347) loss_u loss_u 0.9194 (0.9549) acc_u 9.3750 (6.0417) lr 2.7103e-04 eta 0:00:31
epoch [154/200] batch [20/81] time 0.403 (0.469) data 0.273 (0.339) loss_u loss_u 0.9717 (0.9521) acc_u 3.1250 (6.4062) lr 2.7103e-04 eta 0:00:28
epoch [154/200] batch [25/81] time 0.413 (0.470) data 0.282 (0.339) loss_u loss_u 1.0000 (0.9583) acc_u 0.0000 (5.5000) lr 2.7103e-04 eta 0:00:26
epoch [154/200] batch [30/81] time 0.594 (0.465) data 0.463 (0.334) loss_u loss_u 0.9634 (0.9578) acc_u 3.1250 (5.4167) lr 2.7103e-04 eta 0:00:23
epoch [154/200] batch [35/81] time 0.486 (0.464) data 0.355 (0.334) loss_u loss_u 0.9478 (0.9561) acc_u 6.2500 (5.5357) lr 2.7103e-04 eta 0:00:21
epoch [154/200] batch [40/81] time 0.361 (0.469) data 0.231 (0.338) loss_u loss_u 0.9585 (0.9589) acc_u 6.2500 (5.2344) lr 2.7103e-04 eta 0:00:19
epoch [154/200] batch [45/81] time 0.542 (0.469) data 0.411 (0.338) loss_u loss_u 0.9907 (0.9600) acc_u 3.1250 (5.2083) lr 2.7103e-04 eta 0:00:16
epoch [154/200] batch [50/81] time 0.534 (0.469) data 0.402 (0.338) loss_u loss_u 0.9966 (0.9609) acc_u 0.0000 (5.1250) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [55/81] time 0.413 (0.465) data 0.282 (0.335) loss_u loss_u 0.9546 (0.9608) acc_u 6.2500 (5.1136) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [60/81] time 0.349 (0.464) data 0.218 (0.333) loss_u loss_u 0.9629 (0.9606) acc_u 6.2500 (5.1562) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [65/81] time 0.572 (0.467) data 0.442 (0.336) loss_u loss_u 0.9570 (0.9610) acc_u 6.2500 (5.0481) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [70/81] time 0.540 (0.468) data 0.409 (0.337) loss_u loss_u 0.9458 (0.9618) acc_u 6.2500 (4.8661) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [75/81] time 0.394 (0.468) data 0.264 (0.337) loss_u loss_u 0.9751 (0.9629) acc_u 6.2500 (4.7917) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [80/81] time 0.385 (0.465) data 0.253 (0.335) loss_u loss_u 0.9580 (0.9628) acc_u 6.2500 (4.7266) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1745
confident_label rate tensor(0.1543, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 484
clean true:461
clean false:23
clean_rate:0.9524793388429752
noisy true:930
noisy false:1722
after delete: len(clean_dataset) 484
after delete: len(noisy_dataset) 2652
epoch [155/200] batch [5/15] time 0.399 (0.451) data 0.270 (0.321) loss_x loss_x 1.2188 (1.0274) acc_x 68.7500 (78.7500) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [10/15] time 0.494 (0.461) data 0.363 (0.331) loss_x loss_x 1.1357 (1.0827) acc_x 75.0000 (78.1250) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [15/15] time 0.426 (0.459) data 0.296 (0.329) loss_x loss_x 1.1133 (1.0740) acc_x 65.6250 (76.4583) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/82] time 0.377 (0.456) data 0.245 (0.326) loss_u loss_u 0.9478 (0.9658) acc_u 6.2500 (5.0000) lr 2.6037e-04 eta 0:00:35
epoch [155/200] batch [10/82] time 0.363 (0.454) data 0.233 (0.323) loss_u loss_u 0.9917 (0.9576) acc_u 0.0000 (5.6250) lr 2.6037e-04 eta 0:00:32
epoch [155/200] batch [15/82] time 0.497 (0.448) data 0.366 (0.317) loss_u loss_u 0.9624 (0.9561) acc_u 3.1250 (5.6250) lr 2.6037e-04 eta 0:00:30
epoch [155/200] batch [20/82] time 0.455 (0.451) data 0.325 (0.321) loss_u loss_u 0.9790 (0.9582) acc_u 3.1250 (5.4688) lr 2.6037e-04 eta 0:00:27
epoch [155/200] batch [25/82] time 0.461 (0.454) data 0.329 (0.323) loss_u loss_u 0.9570 (0.9607) acc_u 6.2500 (5.0000) lr 2.6037e-04 eta 0:00:25
epoch [155/200] batch [30/82] time 0.468 (0.452) data 0.338 (0.321) loss_u loss_u 0.9316 (0.9558) acc_u 9.3750 (5.6250) lr 2.6037e-04 eta 0:00:23
epoch [155/200] batch [35/82] time 0.434 (0.451) data 0.303 (0.320) loss_u loss_u 0.9844 (0.9556) acc_u 3.1250 (5.8036) lr 2.6037e-04 eta 0:00:21
epoch [155/200] batch [40/82] time 0.520 (0.456) data 0.389 (0.325) loss_u loss_u 0.9390 (0.9545) acc_u 9.3750 (6.0156) lr 2.6037e-04 eta 0:00:19
epoch [155/200] batch [45/82] time 0.534 (0.459) data 0.402 (0.328) loss_u loss_u 1.0000 (0.9543) acc_u 0.0000 (6.0417) lr 2.6037e-04 eta 0:00:16
epoch [155/200] batch [50/82] time 0.639 (0.463) data 0.508 (0.332) loss_u loss_u 0.9614 (0.9539) acc_u 3.1250 (5.9375) lr 2.6037e-04 eta 0:00:14
epoch [155/200] batch [55/82] time 0.391 (0.466) data 0.261 (0.335) loss_u loss_u 0.9399 (0.9547) acc_u 6.2500 (5.8523) lr 2.6037e-04 eta 0:00:12
epoch [155/200] batch [60/82] time 0.512 (0.462) data 0.380 (0.331) loss_u loss_u 0.9648 (0.9559) acc_u 6.2500 (5.6250) lr 2.6037e-04 eta 0:00:10
epoch [155/200] batch [65/82] time 0.456 (0.463) data 0.324 (0.332) loss_u loss_u 0.9956 (0.9586) acc_u 0.0000 (5.2885) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [70/82] time 0.443 (0.462) data 0.313 (0.331) loss_u loss_u 0.9897 (0.9598) acc_u 3.1250 (5.2232) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [75/82] time 0.558 (0.464) data 0.427 (0.333) loss_u loss_u 0.9932 (0.9606) acc_u 0.0000 (5.1250) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [80/82] time 0.446 (0.470) data 0.314 (0.339) loss_u loss_u 0.9839 (0.9598) acc_u 3.1250 (5.2344) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1728
confident_label rate tensor(0.1575, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 494
clean true:473
clean false:21
clean_rate:0.9574898785425101
noisy true:935
noisy false:1707
after delete: len(clean_dataset) 494
after delete: len(noisy_dataset) 2642
epoch [156/200] batch [5/15] time 0.435 (0.479) data 0.304 (0.349) loss_x loss_x 1.1631 (1.2108) acc_x 68.7500 (71.2500) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [10/15] time 0.485 (0.482) data 0.354 (0.352) loss_x loss_x 1.3350 (1.2540) acc_x 68.7500 (71.5625) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [15/15] time 0.589 (0.476) data 0.458 (0.346) loss_x loss_x 1.3789 (1.2234) acc_x 59.3750 (71.8750) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/82] time 0.434 (0.479) data 0.303 (0.348) loss_u loss_u 0.9775 (0.9563) acc_u 3.1250 (5.0000) lr 2.4989e-04 eta 0:00:36
epoch [156/200] batch [10/82] time 0.536 (0.491) data 0.405 (0.361) loss_u loss_u 0.9663 (0.9463) acc_u 3.1250 (6.2500) lr 2.4989e-04 eta 0:00:35
epoch [156/200] batch [15/82] time 0.465 (0.486) data 0.335 (0.355) loss_u loss_u 0.9688 (0.9593) acc_u 3.1250 (4.7917) lr 2.4989e-04 eta 0:00:32
epoch [156/200] batch [20/82] time 0.528 (0.486) data 0.396 (0.356) loss_u loss_u 0.9126 (0.9585) acc_u 12.5000 (5.0000) lr 2.4989e-04 eta 0:00:30
epoch [156/200] batch [25/82] time 0.330 (0.481) data 0.199 (0.350) loss_u loss_u 0.9233 (0.9556) acc_u 9.3750 (5.5000) lr 2.4989e-04 eta 0:00:27
epoch [156/200] batch [30/82] time 0.347 (0.476) data 0.216 (0.346) loss_u loss_u 0.9512 (0.9549) acc_u 6.2500 (5.8333) lr 2.4989e-04 eta 0:00:24
epoch [156/200] batch [35/82] time 0.447 (0.472) data 0.317 (0.342) loss_u loss_u 0.9497 (0.9565) acc_u 6.2500 (5.8036) lr 2.4989e-04 eta 0:00:22
epoch [156/200] batch [40/82] time 0.424 (0.468) data 0.294 (0.337) loss_u loss_u 0.9741 (0.9595) acc_u 3.1250 (5.3906) lr 2.4989e-04 eta 0:00:19
epoch [156/200] batch [45/82] time 0.448 (0.467) data 0.317 (0.336) loss_u loss_u 0.9482 (0.9616) acc_u 9.3750 (5.1389) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [50/82] time 0.564 (0.467) data 0.434 (0.336) loss_u loss_u 0.9570 (0.9599) acc_u 6.2500 (5.3125) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [55/82] time 0.408 (0.465) data 0.276 (0.334) loss_u loss_u 0.9888 (0.9601) acc_u 0.0000 (5.3977) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [60/82] time 0.479 (0.465) data 0.349 (0.334) loss_u loss_u 0.8906 (0.9588) acc_u 15.6250 (5.6250) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [65/82] time 0.388 (0.460) data 0.257 (0.329) loss_u loss_u 0.9873 (0.9589) acc_u 0.0000 (5.5769) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [70/82] time 0.546 (0.460) data 0.415 (0.329) loss_u loss_u 0.9521 (0.9585) acc_u 6.2500 (5.6250) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [75/82] time 0.519 (0.460) data 0.388 (0.330) loss_u loss_u 0.9712 (0.9586) acc_u 6.2500 (5.5833) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [80/82] time 0.431 (0.462) data 0.301 (0.331) loss_u loss_u 0.9805 (0.9597) acc_u 3.1250 (5.4297) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1734
confident_label rate tensor(0.1585, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 497
clean true:480
clean false:17
clean_rate:0.96579476861167
noisy true:922
noisy false:1717
after delete: len(clean_dataset) 497
after delete: len(noisy_dataset) 2639
epoch [157/200] batch [5/15] time 0.505 (0.484) data 0.375 (0.354) loss_x loss_x 0.7329 (1.1321) acc_x 87.5000 (74.3750) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [10/15] time 0.473 (0.461) data 0.342 (0.331) loss_x loss_x 1.3906 (1.0830) acc_x 65.6250 (75.0000) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [15/15] time 0.555 (0.456) data 0.424 (0.326) loss_x loss_x 1.2539 (1.0804) acc_x 78.1250 (75.2083) lr 2.3959e-04 eta 0:00:00
epoch [157/200] batch [5/82] time 0.451 (0.454) data 0.319 (0.323) loss_u loss_u 0.9663 (0.9608) acc_u 6.2500 (5.0000) lr 2.3959e-04 eta 0:00:34
epoch [157/200] batch [10/82] time 0.448 (0.448) data 0.317 (0.317) loss_u loss_u 0.9976 (0.9577) acc_u 0.0000 (5.0000) lr 2.3959e-04 eta 0:00:32
epoch [157/200] batch [15/82] time 0.333 (0.441) data 0.202 (0.310) loss_u loss_u 0.9917 (0.9659) acc_u 0.0000 (4.1667) lr 2.3959e-04 eta 0:00:29
epoch [157/200] batch [20/82] time 0.358 (0.455) data 0.227 (0.324) loss_u loss_u 0.9839 (0.9651) acc_u 0.0000 (4.3750) lr 2.3959e-04 eta 0:00:28
epoch [157/200] batch [25/82] time 0.435 (0.455) data 0.304 (0.325) loss_u loss_u 0.9790 (0.9668) acc_u 3.1250 (4.5000) lr 2.3959e-04 eta 0:00:25
epoch [157/200] batch [30/82] time 0.443 (0.456) data 0.312 (0.325) loss_u loss_u 0.9468 (0.9649) acc_u 9.3750 (4.7917) lr 2.3959e-04 eta 0:00:23
epoch [157/200] batch [35/82] time 0.378 (0.454) data 0.248 (0.324) loss_u loss_u 0.9697 (0.9662) acc_u 3.1250 (4.5536) lr 2.3959e-04 eta 0:00:21
epoch [157/200] batch [40/82] time 0.467 (0.456) data 0.336 (0.325) loss_u loss_u 0.9893 (0.9651) acc_u 3.1250 (4.6094) lr 2.3959e-04 eta 0:00:19
epoch [157/200] batch [45/82] time 0.468 (0.453) data 0.336 (0.323) loss_u loss_u 0.9653 (0.9635) acc_u 6.2500 (5.0694) lr 2.3959e-04 eta 0:00:16
epoch [157/200] batch [50/82] time 0.436 (0.452) data 0.304 (0.321) loss_u loss_u 0.9824 (0.9631) acc_u 3.1250 (5.2500) lr 2.3959e-04 eta 0:00:14
epoch [157/200] batch [55/82] time 0.363 (0.453) data 0.232 (0.322) loss_u loss_u 0.9556 (0.9615) acc_u 6.2500 (5.3977) lr 2.3959e-04 eta 0:00:12
epoch [157/200] batch [60/82] time 0.398 (0.451) data 0.266 (0.320) loss_u loss_u 0.9009 (0.9596) acc_u 12.5000 (5.5208) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [65/82] time 0.488 (0.454) data 0.356 (0.323) loss_u loss_u 0.9985 (0.9609) acc_u 0.0000 (5.3365) lr 2.3959e-04 eta 0:00:07
epoch [157/200] batch [70/82] time 0.387 (0.454) data 0.256 (0.323) loss_u loss_u 0.9629 (0.9610) acc_u 6.2500 (5.3571) lr 2.3959e-04 eta 0:00:05
epoch [157/200] batch [75/82] time 0.479 (0.454) data 0.349 (0.323) loss_u loss_u 0.9604 (0.9614) acc_u 3.1250 (5.2500) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [80/82] time 0.374 (0.456) data 0.244 (0.325) loss_u loss_u 0.9795 (0.9613) acc_u 0.0000 (5.2734) lr 2.3959e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1663
confident_label rate tensor(0.1639, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 514
clean true:494
clean false:20
clean_rate:0.9610894941634242
noisy true:979
noisy false:1643
after delete: len(clean_dataset) 514
after delete: len(noisy_dataset) 2622
epoch [158/200] batch [5/16] time 0.462 (0.450) data 0.332 (0.319) loss_x loss_x 1.1729 (1.2370) acc_x 71.8750 (67.5000) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [10/16] time 0.494 (0.451) data 0.364 (0.320) loss_x loss_x 0.6128 (1.1882) acc_x 84.3750 (71.5625) lr 2.2949e-04 eta 0:00:02
epoch [158/200] batch [15/16] time 0.392 (0.449) data 0.261 (0.318) loss_x loss_x 0.6826 (1.1024) acc_x 84.3750 (74.5833) lr 2.2949e-04 eta 0:00:00
epoch [158/200] batch [5/81] time 0.489 (0.455) data 0.357 (0.324) loss_u loss_u 0.9932 (0.9646) acc_u 0.0000 (4.3750) lr 2.2949e-04 eta 0:00:34
epoch [158/200] batch [10/81] time 0.464 (0.451) data 0.332 (0.320) loss_u loss_u 0.9658 (0.9700) acc_u 3.1250 (3.7500) lr 2.2949e-04 eta 0:00:31
epoch [158/200] batch [15/81] time 0.491 (0.453) data 0.360 (0.322) loss_u loss_u 0.9873 (0.9615) acc_u 3.1250 (4.7917) lr 2.2949e-04 eta 0:00:29
epoch [158/200] batch [20/81] time 0.402 (0.446) data 0.271 (0.315) loss_u loss_u 0.9683 (0.9641) acc_u 6.2500 (4.3750) lr 2.2949e-04 eta 0:00:27
epoch [158/200] batch [25/81] time 0.490 (0.449) data 0.360 (0.318) loss_u loss_u 0.9287 (0.9644) acc_u 12.5000 (4.5000) lr 2.2949e-04 eta 0:00:25
epoch [158/200] batch [30/81] time 0.600 (0.451) data 0.470 (0.320) loss_u loss_u 0.9668 (0.9623) acc_u 9.3750 (5.0000) lr 2.2949e-04 eta 0:00:23
epoch [158/200] batch [35/81] time 0.678 (0.455) data 0.547 (0.324) loss_u loss_u 0.9824 (0.9636) acc_u 3.1250 (4.7321) lr 2.2949e-04 eta 0:00:20
epoch [158/200] batch [40/81] time 0.402 (0.455) data 0.271 (0.324) loss_u loss_u 0.9434 (0.9618) acc_u 6.2500 (4.9219) lr 2.2949e-04 eta 0:00:18
epoch [158/200] batch [45/81] time 0.394 (0.457) data 0.264 (0.326) loss_u loss_u 0.9824 (0.9628) acc_u 3.1250 (4.7222) lr 2.2949e-04 eta 0:00:16
epoch [158/200] batch [50/81] time 0.501 (0.456) data 0.369 (0.325) loss_u loss_u 0.9473 (0.9629) acc_u 6.2500 (4.6250) lr 2.2949e-04 eta 0:00:14
epoch [158/200] batch [55/81] time 0.417 (0.462) data 0.285 (0.330) loss_u loss_u 0.9497 (0.9626) acc_u 6.2500 (4.7159) lr 2.2949e-04 eta 0:00:11
epoch [158/200] batch [60/81] time 0.374 (0.459) data 0.242 (0.328) loss_u loss_u 0.9526 (0.9632) acc_u 6.2500 (4.6875) lr 2.2949e-04 eta 0:00:09
epoch [158/200] batch [65/81] time 0.412 (0.457) data 0.281 (0.326) loss_u loss_u 0.9199 (0.9633) acc_u 9.3750 (4.6635) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [70/81] time 0.504 (0.457) data 0.373 (0.326) loss_u loss_u 0.9941 (0.9641) acc_u 0.0000 (4.5089) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [75/81] time 0.489 (0.455) data 0.358 (0.324) loss_u loss_u 0.9668 (0.9632) acc_u 3.1250 (4.5417) lr 2.2949e-04 eta 0:00:02
epoch [158/200] batch [80/81] time 0.477 (0.456) data 0.346 (0.325) loss_u loss_u 0.9038 (0.9621) acc_u 12.5000 (4.6875) lr 2.2949e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1714
confident_label rate tensor(0.1572, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 493
clean true:474
clean false:19
clean_rate:0.9614604462474645
noisy true:948
noisy false:1695
after delete: len(clean_dataset) 493
after delete: len(noisy_dataset) 2643
epoch [159/200] batch [5/15] time 0.542 (0.452) data 0.412 (0.322) loss_x loss_x 1.0635 (1.0249) acc_x 78.1250 (77.5000) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [10/15] time 0.421 (0.439) data 0.292 (0.309) loss_x loss_x 1.1230 (0.9836) acc_x 75.0000 (77.8125) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [15/15] time 0.492 (0.453) data 0.362 (0.323) loss_x loss_x 0.8291 (1.0263) acc_x 84.3750 (77.5000) lr 2.1957e-04 eta 0:00:00
epoch [159/200] batch [5/82] time 0.560 (0.462) data 0.429 (0.331) loss_u loss_u 0.9585 (0.9484) acc_u 9.3750 (6.2500) lr 2.1957e-04 eta 0:00:35
epoch [159/200] batch [10/82] time 0.415 (0.457) data 0.283 (0.326) loss_u loss_u 0.9707 (0.9528) acc_u 3.1250 (5.6250) lr 2.1957e-04 eta 0:00:32
epoch [159/200] batch [15/82] time 0.388 (0.462) data 0.258 (0.331) loss_u loss_u 0.9849 (0.9535) acc_u 3.1250 (5.8333) lr 2.1957e-04 eta 0:00:30
epoch [159/200] batch [20/82] time 0.480 (0.460) data 0.349 (0.329) loss_u loss_u 0.9409 (0.9545) acc_u 9.3750 (5.7812) lr 2.1957e-04 eta 0:00:28
epoch [159/200] batch [25/82] time 0.529 (0.470) data 0.399 (0.339) loss_u loss_u 0.9497 (0.9544) acc_u 9.3750 (6.0000) lr 2.1957e-04 eta 0:00:26
epoch [159/200] batch [30/82] time 0.383 (0.467) data 0.253 (0.336) loss_u loss_u 0.9292 (0.9538) acc_u 6.2500 (5.8333) lr 2.1957e-04 eta 0:00:24
epoch [159/200] batch [35/82] time 0.413 (0.462) data 0.281 (0.331) loss_u loss_u 0.9785 (0.9566) acc_u 3.1250 (5.5357) lr 2.1957e-04 eta 0:00:21
epoch [159/200] batch [40/82] time 0.435 (0.461) data 0.303 (0.330) loss_u loss_u 0.9956 (0.9561) acc_u 0.0000 (5.6250) lr 2.1957e-04 eta 0:00:19
epoch [159/200] batch [45/82] time 0.497 (0.462) data 0.366 (0.331) loss_u loss_u 0.9565 (0.9560) acc_u 6.2500 (5.6944) lr 2.1957e-04 eta 0:00:17
epoch [159/200] batch [50/82] time 0.381 (0.462) data 0.249 (0.331) loss_u loss_u 0.9038 (0.9557) acc_u 12.5000 (5.6250) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [55/82] time 0.504 (0.467) data 0.373 (0.335) loss_u loss_u 0.9224 (0.9554) acc_u 9.3750 (5.7386) lr 2.1957e-04 eta 0:00:12
epoch [159/200] batch [60/82] time 0.515 (0.471) data 0.383 (0.340) loss_u loss_u 0.9482 (0.9571) acc_u 6.2500 (5.4688) lr 2.1957e-04 eta 0:00:10
epoch [159/200] batch [65/82] time 0.407 (0.473) data 0.276 (0.342) loss_u loss_u 0.9487 (0.9585) acc_u 6.2500 (5.2885) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [70/82] time 0.455 (0.470) data 0.324 (0.339) loss_u loss_u 0.9199 (0.9582) acc_u 9.3750 (5.3125) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [75/82] time 0.383 (0.467) data 0.251 (0.336) loss_u loss_u 0.9180 (0.9584) acc_u 9.3750 (5.2500) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [80/82] time 0.366 (0.464) data 0.235 (0.333) loss_u loss_u 0.9878 (0.9597) acc_u 0.0000 (5.0781) lr 2.1957e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1690
confident_label rate tensor(0.1677, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 526
clean true:509
clean false:17
clean_rate:0.967680608365019
noisy true:937
noisy false:1673
after delete: len(clean_dataset) 526
after delete: len(noisy_dataset) 2610
epoch [160/200] batch [5/16] time 0.398 (0.461) data 0.268 (0.330) loss_x loss_x 0.9907 (1.0553) acc_x 71.8750 (71.8750) lr 2.0984e-04 eta 0:00:05
epoch [160/200] batch [10/16] time 0.505 (0.452) data 0.376 (0.321) loss_x loss_x 0.8833 (1.0305) acc_x 84.3750 (73.7500) lr 2.0984e-04 eta 0:00:02
epoch [160/200] batch [15/16] time 0.476 (0.461) data 0.346 (0.331) loss_x loss_x 1.2852 (1.0027) acc_x 68.7500 (74.3750) lr 2.0984e-04 eta 0:00:00
epoch [160/200] batch [5/81] time 0.363 (0.459) data 0.233 (0.329) loss_u loss_u 0.9536 (0.9579) acc_u 6.2500 (6.8750) lr 2.0984e-04 eta 0:00:34
epoch [160/200] batch [10/81] time 0.350 (0.455) data 0.218 (0.325) loss_u loss_u 0.9604 (0.9663) acc_u 6.2500 (5.3125) lr 2.0984e-04 eta 0:00:32
epoch [160/200] batch [15/81] time 0.409 (0.450) data 0.277 (0.320) loss_u loss_u 0.9888 (0.9744) acc_u 0.0000 (3.5417) lr 2.0984e-04 eta 0:00:29
epoch [160/200] batch [20/81] time 0.544 (0.455) data 0.413 (0.324) loss_u loss_u 0.9956 (0.9708) acc_u 0.0000 (4.0625) lr 2.0984e-04 eta 0:00:27
epoch [160/200] batch [25/81] time 0.579 (0.460) data 0.449 (0.329) loss_u loss_u 0.9712 (0.9707) acc_u 3.1250 (4.0000) lr 2.0984e-04 eta 0:00:25
epoch [160/200] batch [30/81] time 0.449 (0.458) data 0.317 (0.327) loss_u loss_u 0.9897 (0.9735) acc_u 0.0000 (3.5417) lr 2.0984e-04 eta 0:00:23
epoch [160/200] batch [35/81] time 0.379 (0.453) data 0.248 (0.322) loss_u loss_u 0.9292 (0.9727) acc_u 9.3750 (3.5714) lr 2.0984e-04 eta 0:00:20
epoch [160/200] batch [40/81] time 0.489 (0.456) data 0.358 (0.326) loss_u loss_u 0.9849 (0.9734) acc_u 0.0000 (3.5156) lr 2.0984e-04 eta 0:00:18
epoch [160/200] batch [45/81] time 0.486 (0.461) data 0.354 (0.331) loss_u loss_u 0.9658 (0.9732) acc_u 6.2500 (3.4722) lr 2.0984e-04 eta 0:00:16
epoch [160/200] batch [50/81] time 0.427 (0.466) data 0.297 (0.335) loss_u loss_u 0.9575 (0.9711) acc_u 6.2500 (3.6875) lr 2.0984e-04 eta 0:00:14
epoch [160/200] batch [55/81] time 0.402 (0.471) data 0.271 (0.340) loss_u loss_u 0.9434 (0.9699) acc_u 9.3750 (3.9773) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [60/81] time 0.416 (0.472) data 0.285 (0.341) loss_u loss_u 0.9268 (0.9680) acc_u 6.2500 (4.1667) lr 2.0984e-04 eta 0:00:09
epoch [160/200] batch [65/81] time 0.369 (0.470) data 0.238 (0.339) loss_u loss_u 0.9067 (0.9658) acc_u 12.5000 (4.4231) lr 2.0984e-04 eta 0:00:07
epoch [160/200] batch [70/81] time 0.525 (0.474) data 0.393 (0.343) loss_u loss_u 0.9858 (0.9656) acc_u 3.1250 (4.5089) lr 2.0984e-04 eta 0:00:05
epoch [160/200] batch [75/81] time 0.391 (0.474) data 0.260 (0.343) loss_u loss_u 0.9976 (0.9649) acc_u 0.0000 (4.6250) lr 2.0984e-04 eta 0:00:02
epoch [160/200] batch [80/81] time 0.517 (0.473) data 0.385 (0.342) loss_u loss_u 0.9985 (0.9655) acc_u 0.0000 (4.5312) lr 2.0984e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1728
confident_label rate tensor(0.1610, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 505
clean true:489
clean false:16
clean_rate:0.9683168316831683
noisy true:919
noisy false:1712
after delete: len(clean_dataset) 505
after delete: len(noisy_dataset) 2631
epoch [161/200] batch [5/15] time 0.398 (0.465) data 0.268 (0.334) loss_x loss_x 1.1348 (0.9310) acc_x 78.1250 (81.2500) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [10/15] time 0.419 (0.448) data 0.289 (0.317) loss_x loss_x 1.3662 (0.9874) acc_x 65.6250 (78.4375) lr 2.0032e-04 eta 0:00:02
epoch [161/200] batch [15/15] time 0.434 (0.457) data 0.303 (0.326) loss_x loss_x 0.7681 (0.9529) acc_x 81.2500 (79.3750) lr 2.0032e-04 eta 0:00:00
epoch [161/200] batch [5/82] time 0.454 (0.455) data 0.324 (0.325) loss_u loss_u 0.9512 (0.9572) acc_u 9.3750 (6.2500) lr 2.0032e-04 eta 0:00:35
epoch [161/200] batch [10/82] time 0.473 (0.461) data 0.341 (0.330) loss_u loss_u 0.9985 (0.9650) acc_u 0.0000 (5.0000) lr 2.0032e-04 eta 0:00:33
epoch [161/200] batch [15/82] time 0.444 (0.456) data 0.313 (0.325) loss_u loss_u 0.9214 (0.9597) acc_u 9.3750 (5.4167) lr 2.0032e-04 eta 0:00:30
epoch [161/200] batch [20/82] time 0.483 (0.453) data 0.351 (0.323) loss_u loss_u 0.9897 (0.9545) acc_u 3.1250 (5.7812) lr 2.0032e-04 eta 0:00:28
epoch [161/200] batch [25/82] time 0.535 (0.462) data 0.404 (0.331) loss_u loss_u 0.9150 (0.9538) acc_u 9.3750 (6.0000) lr 2.0032e-04 eta 0:00:26
epoch [161/200] batch [30/82] time 0.458 (0.462) data 0.327 (0.331) loss_u loss_u 0.9800 (0.9555) acc_u 3.1250 (5.7292) lr 2.0032e-04 eta 0:00:23
epoch [161/200] batch [35/82] time 0.595 (0.461) data 0.464 (0.330) loss_u loss_u 0.9370 (0.9584) acc_u 9.3750 (5.3571) lr 2.0032e-04 eta 0:00:21
epoch [161/200] batch [40/82] time 0.564 (0.461) data 0.433 (0.330) loss_u loss_u 0.9575 (0.9594) acc_u 6.2500 (5.2344) lr 2.0032e-04 eta 0:00:19
epoch [161/200] batch [45/82] time 0.365 (0.456) data 0.234 (0.326) loss_u loss_u 0.9590 (0.9606) acc_u 6.2500 (5.0694) lr 2.0032e-04 eta 0:00:16
epoch [161/200] batch [50/82] time 0.498 (0.456) data 0.368 (0.326) loss_u loss_u 0.9199 (0.9604) acc_u 9.3750 (5.0000) lr 2.0032e-04 eta 0:00:14
epoch [161/200] batch [55/82] time 0.503 (0.455) data 0.372 (0.324) loss_u loss_u 0.9780 (0.9617) acc_u 3.1250 (4.8864) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [60/82] time 0.397 (0.458) data 0.267 (0.327) loss_u loss_u 0.9824 (0.9620) acc_u 3.1250 (4.7917) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [65/82] time 0.419 (0.458) data 0.288 (0.327) loss_u loss_u 0.9976 (0.9629) acc_u 0.0000 (4.7115) lr 2.0032e-04 eta 0:00:07
epoch [161/200] batch [70/82] time 0.592 (0.463) data 0.460 (0.332) loss_u loss_u 0.9126 (0.9625) acc_u 12.5000 (4.7768) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [75/82] time 0.483 (0.463) data 0.352 (0.332) loss_u loss_u 0.9570 (0.9626) acc_u 9.3750 (4.7917) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [80/82] time 0.552 (0.466) data 0.421 (0.335) loss_u loss_u 0.9858 (0.9636) acc_u 3.1250 (4.6094) lr 2.0032e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1727
confident_label rate tensor(0.1598, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 501
clean true:484
clean false:17
clean_rate:0.9660678642714571
noisy true:925
noisy false:1710
after delete: len(clean_dataset) 501
after delete: len(noisy_dataset) 2635
epoch [162/200] batch [5/15] time 0.426 (0.434) data 0.296 (0.303) loss_x loss_x 1.7041 (1.2343) acc_x 62.5000 (71.8750) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [10/15] time 0.479 (0.466) data 0.349 (0.335) loss_x loss_x 0.6914 (1.1353) acc_x 81.2500 (71.5625) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [15/15] time 0.426 (0.472) data 0.296 (0.341) loss_x loss_x 0.6353 (1.0750) acc_x 84.3750 (72.9167) lr 1.9098e-04 eta 0:00:00
epoch [162/200] batch [5/82] time 0.469 (0.480) data 0.337 (0.350) loss_u loss_u 0.9995 (0.9535) acc_u 0.0000 (5.0000) lr 1.9098e-04 eta 0:00:36
epoch [162/200] batch [10/82] time 0.516 (0.472) data 0.385 (0.341) loss_u loss_u 0.9814 (0.9532) acc_u 3.1250 (5.6250) lr 1.9098e-04 eta 0:00:33
epoch [162/200] batch [15/82] time 0.399 (0.473) data 0.267 (0.342) loss_u loss_u 0.9478 (0.9575) acc_u 6.2500 (5.2083) lr 1.9098e-04 eta 0:00:31
epoch [162/200] batch [20/82] time 0.324 (0.464) data 0.193 (0.334) loss_u loss_u 0.9551 (0.9546) acc_u 3.1250 (5.6250) lr 1.9098e-04 eta 0:00:28
epoch [162/200] batch [25/82] time 0.442 (0.464) data 0.311 (0.333) loss_u loss_u 0.9355 (0.9550) acc_u 6.2500 (5.5000) lr 1.9098e-04 eta 0:00:26
epoch [162/200] batch [30/82] time 0.400 (0.467) data 0.270 (0.337) loss_u loss_u 0.9814 (0.9573) acc_u 3.1250 (5.3125) lr 1.9098e-04 eta 0:00:24
epoch [162/200] batch [35/82] time 0.347 (0.469) data 0.216 (0.339) loss_u loss_u 0.9907 (0.9553) acc_u 3.1250 (5.6250) lr 1.9098e-04 eta 0:00:22
epoch [162/200] batch [40/82] time 0.518 (0.471) data 0.388 (0.340) loss_u loss_u 0.9937 (0.9576) acc_u 0.0000 (5.2344) lr 1.9098e-04 eta 0:00:19
epoch [162/200] batch [45/82] time 0.423 (0.471) data 0.292 (0.340) loss_u loss_u 0.9590 (0.9563) acc_u 6.2500 (5.4861) lr 1.9098e-04 eta 0:00:17
epoch [162/200] batch [50/82] time 0.572 (0.476) data 0.441 (0.345) loss_u loss_u 0.9961 (0.9569) acc_u 0.0000 (5.5000) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [55/82] time 0.367 (0.476) data 0.236 (0.346) loss_u loss_u 0.9951 (0.9571) acc_u 0.0000 (5.4545) lr 1.9098e-04 eta 0:00:12
epoch [162/200] batch [60/82] time 0.475 (0.476) data 0.344 (0.345) loss_u loss_u 0.9580 (0.9581) acc_u 6.2500 (5.2604) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [65/82] time 0.410 (0.477) data 0.279 (0.346) loss_u loss_u 0.9619 (0.9579) acc_u 3.1250 (5.1923) lr 1.9098e-04 eta 0:00:08
epoch [162/200] batch [70/82] time 0.393 (0.477) data 0.262 (0.346) loss_u loss_u 0.9370 (0.9582) acc_u 6.2500 (5.0446) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [75/82] time 0.386 (0.474) data 0.256 (0.344) loss_u loss_u 0.9243 (0.9583) acc_u 9.3750 (5.0833) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [80/82] time 0.377 (0.471) data 0.245 (0.340) loss_u loss_u 0.9473 (0.9580) acc_u 6.2500 (5.1953) lr 1.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1680
confident_label rate tensor(0.1639, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 514
clean true:493
clean false:21
clean_rate:0.9591439688715954
noisy true:963
noisy false:1659
after delete: len(clean_dataset) 514
after delete: len(noisy_dataset) 2622
epoch [163/200] batch [5/16] time 0.586 (0.445) data 0.455 (0.315) loss_x loss_x 1.3457 (1.2458) acc_x 65.6250 (69.3750) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [10/16] time 0.441 (0.447) data 0.311 (0.317) loss_x loss_x 1.6035 (1.0929) acc_x 59.3750 (73.7500) lr 1.8185e-04 eta 0:00:02
epoch [163/200] batch [15/16] time 0.531 (0.469) data 0.402 (0.339) loss_x loss_x 1.1211 (1.0533) acc_x 62.5000 (74.7917) lr 1.8185e-04 eta 0:00:00
epoch [163/200] batch [5/81] time 0.412 (0.466) data 0.281 (0.336) loss_u loss_u 0.9839 (0.9874) acc_u 0.0000 (1.2500) lr 1.8185e-04 eta 0:00:35
epoch [163/200] batch [10/81] time 0.372 (0.456) data 0.240 (0.326) loss_u loss_u 0.9980 (0.9834) acc_u 0.0000 (1.8750) lr 1.8185e-04 eta 0:00:32
epoch [163/200] batch [15/81] time 0.470 (0.456) data 0.339 (0.326) loss_u loss_u 0.9873 (0.9801) acc_u 0.0000 (2.2917) lr 1.8185e-04 eta 0:00:30
epoch [163/200] batch [20/81] time 0.432 (0.454) data 0.300 (0.323) loss_u loss_u 0.9434 (0.9713) acc_u 9.3750 (3.7500) lr 1.8185e-04 eta 0:00:27
epoch [163/200] batch [25/81] time 0.475 (0.460) data 0.345 (0.329) loss_u loss_u 0.9961 (0.9722) acc_u 0.0000 (3.5000) lr 1.8185e-04 eta 0:00:25
epoch [163/200] batch [30/81] time 0.473 (0.461) data 0.343 (0.330) loss_u loss_u 0.9966 (0.9687) acc_u 0.0000 (4.0625) lr 1.8185e-04 eta 0:00:23
epoch [163/200] batch [35/81] time 0.394 (0.458) data 0.263 (0.328) loss_u loss_u 0.8745 (0.9647) acc_u 18.7500 (4.5536) lr 1.8185e-04 eta 0:00:21
epoch [163/200] batch [40/81] time 0.550 (0.461) data 0.418 (0.330) loss_u loss_u 0.9731 (0.9646) acc_u 3.1250 (4.6094) lr 1.8185e-04 eta 0:00:18
epoch [163/200] batch [45/81] time 0.597 (0.465) data 0.465 (0.334) loss_u loss_u 0.9673 (0.9632) acc_u 3.1250 (4.7917) lr 1.8185e-04 eta 0:00:16
epoch [163/200] batch [50/81] time 0.544 (0.463) data 0.413 (0.332) loss_u loss_u 0.9570 (0.9615) acc_u 6.2500 (5.0000) lr 1.8185e-04 eta 0:00:14
epoch [163/200] batch [55/81] time 0.406 (0.465) data 0.275 (0.334) loss_u loss_u 0.9385 (0.9610) acc_u 6.2500 (5.1136) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [60/81] time 0.476 (0.465) data 0.345 (0.334) loss_u loss_u 0.9912 (0.9621) acc_u 0.0000 (4.9479) lr 1.8185e-04 eta 0:00:09
epoch [163/200] batch [65/81] time 0.681 (0.468) data 0.549 (0.337) loss_u loss_u 0.9360 (0.9633) acc_u 3.1250 (4.7115) lr 1.8185e-04 eta 0:00:07
epoch [163/200] batch [70/81] time 0.376 (0.465) data 0.246 (0.334) loss_u loss_u 0.9624 (0.9635) acc_u 6.2500 (4.7321) lr 1.8185e-04 eta 0:00:05
epoch [163/200] batch [75/81] time 0.392 (0.463) data 0.262 (0.332) loss_u loss_u 0.9385 (0.9626) acc_u 6.2500 (4.8333) lr 1.8185e-04 eta 0:00:02
epoch [163/200] batch [80/81] time 0.536 (0.463) data 0.405 (0.332) loss_u loss_u 0.9668 (0.9616) acc_u 6.2500 (4.9219) lr 1.8185e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1695
confident_label rate tensor(0.1594, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 500
clean true:485
clean false:15
clean_rate:0.97
noisy true:956
noisy false:1680
after delete: len(clean_dataset) 500
after delete: len(noisy_dataset) 2636
epoch [164/200] batch [5/15] time 0.372 (0.477) data 0.241 (0.346) loss_x loss_x 1.1934 (1.2256) acc_x 75.0000 (71.2500) lr 1.7292e-04 eta 0:00:04
epoch [164/200] batch [10/15] time 0.404 (0.457) data 0.273 (0.326) loss_x loss_x 0.6543 (1.1297) acc_x 81.2500 (73.1250) lr 1.7292e-04 eta 0:00:02
epoch [164/200] batch [15/15] time 0.552 (0.464) data 0.421 (0.333) loss_x loss_x 1.1924 (1.1581) acc_x 71.8750 (73.3333) lr 1.7292e-04 eta 0:00:00
epoch [164/200] batch [5/82] time 0.391 (0.476) data 0.260 (0.345) loss_u loss_u 0.9692 (0.9762) acc_u 3.1250 (2.5000) lr 1.7292e-04 eta 0:00:36
epoch [164/200] batch [10/82] time 0.474 (0.474) data 0.344 (0.343) loss_u loss_u 0.9399 (0.9691) acc_u 6.2500 (2.8125) lr 1.7292e-04 eta 0:00:34
epoch [164/200] batch [15/82] time 0.456 (0.477) data 0.324 (0.346) loss_u loss_u 0.9575 (0.9635) acc_u 6.2500 (3.5417) lr 1.7292e-04 eta 0:00:31
epoch [164/200] batch [20/82] time 0.441 (0.473) data 0.310 (0.342) loss_u loss_u 0.9307 (0.9629) acc_u 12.5000 (4.0625) lr 1.7292e-04 eta 0:00:29
epoch [164/200] batch [25/82] time 0.636 (0.474) data 0.504 (0.343) loss_u loss_u 0.9346 (0.9598) acc_u 6.2500 (4.5000) lr 1.7292e-04 eta 0:00:27
epoch [164/200] batch [30/82] time 0.386 (0.477) data 0.253 (0.346) loss_u loss_u 0.9653 (0.9585) acc_u 6.2500 (5.0000) lr 1.7292e-04 eta 0:00:24
epoch [164/200] batch [35/82] time 0.510 (0.479) data 0.380 (0.347) loss_u loss_u 0.9155 (0.9583) acc_u 15.6250 (5.1786) lr 1.7292e-04 eta 0:00:22
epoch [164/200] batch [40/82] time 0.525 (0.478) data 0.395 (0.347) loss_u loss_u 0.9360 (0.9597) acc_u 12.5000 (5.1562) lr 1.7292e-04 eta 0:00:20
epoch [164/200] batch [45/82] time 0.437 (0.477) data 0.305 (0.346) loss_u loss_u 0.9590 (0.9594) acc_u 3.1250 (5.0694) lr 1.7292e-04 eta 0:00:17
epoch [164/200] batch [50/82] time 0.445 (0.474) data 0.314 (0.343) loss_u loss_u 0.9546 (0.9613) acc_u 3.1250 (4.8750) lr 1.7292e-04 eta 0:00:15
epoch [164/200] batch [55/82] time 0.570 (0.475) data 0.440 (0.344) loss_u loss_u 0.9624 (0.9591) acc_u 9.3750 (5.1136) lr 1.7292e-04 eta 0:00:12
epoch [164/200] batch [60/82] time 0.435 (0.476) data 0.305 (0.345) loss_u loss_u 0.9609 (0.9600) acc_u 6.2500 (5.0521) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [65/82] time 0.459 (0.474) data 0.327 (0.343) loss_u loss_u 0.9473 (0.9595) acc_u 6.2500 (5.1442) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [70/82] time 0.430 (0.472) data 0.300 (0.341) loss_u loss_u 0.9844 (0.9599) acc_u 0.0000 (5.0893) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [75/82] time 0.412 (0.472) data 0.282 (0.341) loss_u loss_u 0.9507 (0.9602) acc_u 9.3750 (5.0833) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [80/82] time 0.410 (0.468) data 0.279 (0.337) loss_u loss_u 0.9653 (0.9602) acc_u 3.1250 (5.0781) lr 1.7292e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1675
confident_label rate tensor(0.1614, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 506
clean true:488
clean false:18
clean_rate:0.9644268774703557
noisy true:973
noisy false:1657
after delete: len(clean_dataset) 506
after delete: len(noisy_dataset) 2630
epoch [165/200] batch [5/15] time 0.485 (0.462) data 0.354 (0.332) loss_x loss_x 1.1299 (1.0257) acc_x 62.5000 (71.2500) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [10/15] time 0.439 (0.454) data 0.309 (0.324) loss_x loss_x 0.9028 (0.9678) acc_x 84.3750 (74.6875) lr 1.6419e-04 eta 0:00:02
epoch [165/200] batch [15/15] time 0.522 (0.463) data 0.392 (0.332) loss_x loss_x 1.4922 (1.0881) acc_x 78.1250 (73.9583) lr 1.6419e-04 eta 0:00:00
epoch [165/200] batch [5/82] time 0.469 (0.455) data 0.337 (0.325) loss_u loss_u 0.9941 (0.9629) acc_u 0.0000 (3.7500) lr 1.6419e-04 eta 0:00:35
epoch [165/200] batch [10/82] time 0.503 (0.459) data 0.371 (0.328) loss_u loss_u 0.9712 (0.9607) acc_u 3.1250 (4.3750) lr 1.6419e-04 eta 0:00:33
epoch [165/200] batch [15/82] time 0.555 (0.456) data 0.423 (0.326) loss_u loss_u 0.9814 (0.9582) acc_u 3.1250 (5.0000) lr 1.6419e-04 eta 0:00:30
epoch [165/200] batch [20/82] time 0.611 (0.467) data 0.479 (0.336) loss_u loss_u 0.9995 (0.9622) acc_u 0.0000 (4.3750) lr 1.6419e-04 eta 0:00:28
epoch [165/200] batch [25/82] time 0.495 (0.464) data 0.365 (0.333) loss_u loss_u 0.9414 (0.9612) acc_u 9.3750 (4.6250) lr 1.6419e-04 eta 0:00:26
epoch [165/200] batch [30/82] time 0.498 (0.461) data 0.368 (0.330) loss_u loss_u 0.9819 (0.9624) acc_u 3.1250 (4.7917) lr 1.6419e-04 eta 0:00:23
epoch [165/200] batch [35/82] time 0.408 (0.457) data 0.277 (0.326) loss_u loss_u 0.9478 (0.9591) acc_u 6.2500 (5.0893) lr 1.6419e-04 eta 0:00:21
epoch [165/200] batch [40/82] time 0.477 (0.453) data 0.346 (0.322) loss_u loss_u 0.9683 (0.9591) acc_u 3.1250 (5.0000) lr 1.6419e-04 eta 0:00:19
epoch [165/200] batch [45/82] time 0.460 (0.456) data 0.329 (0.325) loss_u loss_u 0.9023 (0.9597) acc_u 9.3750 (4.8611) lr 1.6419e-04 eta 0:00:16
epoch [165/200] batch [50/82] time 0.499 (0.455) data 0.369 (0.324) loss_u loss_u 0.9722 (0.9599) acc_u 3.1250 (4.9375) lr 1.6419e-04 eta 0:00:14
epoch [165/200] batch [55/82] time 0.411 (0.455) data 0.279 (0.324) loss_u loss_u 0.9771 (0.9598) acc_u 3.1250 (5.0000) lr 1.6419e-04 eta 0:00:12
epoch [165/200] batch [60/82] time 0.426 (0.456) data 0.295 (0.326) loss_u loss_u 0.9683 (0.9595) acc_u 3.1250 (5.0521) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [65/82] time 0.355 (0.451) data 0.224 (0.320) loss_u loss_u 0.9990 (0.9605) acc_u 0.0000 (4.8558) lr 1.6419e-04 eta 0:00:07
epoch [165/200] batch [70/82] time 0.412 (0.452) data 0.281 (0.321) loss_u loss_u 0.9492 (0.9608) acc_u 6.2500 (4.8661) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [75/82] time 0.451 (0.453) data 0.320 (0.322) loss_u loss_u 0.9634 (0.9585) acc_u 3.1250 (5.2083) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [80/82] time 0.543 (0.456) data 0.411 (0.325) loss_u loss_u 0.9731 (0.9592) acc_u 3.1250 (5.0781) lr 1.6419e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1696
confident_label rate tensor(0.1680, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 527
clean true:506
clean false:21
clean_rate:0.9601518026565465
noisy true:934
noisy false:1675
after delete: len(clean_dataset) 527
after delete: len(noisy_dataset) 2609
epoch [166/200] batch [5/16] time 0.363 (0.416) data 0.232 (0.286) loss_x loss_x 0.7998 (0.7754) acc_x 84.3750 (80.6250) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [10/16] time 0.514 (0.461) data 0.384 (0.331) loss_x loss_x 1.0586 (1.0114) acc_x 75.0000 (77.8125) lr 1.5567e-04 eta 0:00:02
epoch [166/200] batch [15/16] time 0.505 (0.475) data 0.374 (0.344) loss_x loss_x 1.1562 (0.9889) acc_x 71.8750 (78.1250) lr 1.5567e-04 eta 0:00:00
epoch [166/200] batch [5/81] time 0.509 (0.482) data 0.378 (0.351) loss_u loss_u 0.9517 (0.9609) acc_u 6.2500 (5.0000) lr 1.5567e-04 eta 0:00:36
epoch [166/200] batch [10/81] time 0.511 (0.482) data 0.380 (0.352) loss_u loss_u 0.8691 (0.9557) acc_u 18.7500 (5.9375) lr 1.5567e-04 eta 0:00:34
epoch [166/200] batch [15/81] time 0.410 (0.480) data 0.278 (0.350) loss_u loss_u 0.9673 (0.9564) acc_u 6.2500 (5.8333) lr 1.5567e-04 eta 0:00:31
epoch [166/200] batch [20/81] time 0.432 (0.478) data 0.301 (0.348) loss_u loss_u 0.9985 (0.9645) acc_u 0.0000 (4.6875) lr 1.5567e-04 eta 0:00:29
epoch [166/200] batch [25/81] time 0.480 (0.471) data 0.349 (0.340) loss_u loss_u 0.9878 (0.9640) acc_u 3.1250 (4.8750) lr 1.5567e-04 eta 0:00:26
epoch [166/200] batch [30/81] time 0.389 (0.467) data 0.259 (0.337) loss_u loss_u 0.9434 (0.9667) acc_u 6.2500 (4.3750) lr 1.5567e-04 eta 0:00:23
epoch [166/200] batch [35/81] time 0.513 (0.466) data 0.382 (0.335) loss_u loss_u 0.9521 (0.9628) acc_u 6.2500 (4.7321) lr 1.5567e-04 eta 0:00:21
epoch [166/200] batch [40/81] time 0.517 (0.468) data 0.387 (0.337) loss_u loss_u 0.9214 (0.9643) acc_u 6.2500 (4.3750) lr 1.5567e-04 eta 0:00:19
epoch [166/200] batch [45/81] time 0.362 (0.463) data 0.231 (0.333) loss_u loss_u 0.9717 (0.9648) acc_u 3.1250 (4.2361) lr 1.5567e-04 eta 0:00:16
epoch [166/200] batch [50/81] time 0.474 (0.465) data 0.343 (0.334) loss_u loss_u 0.9580 (0.9650) acc_u 3.1250 (4.1250) lr 1.5567e-04 eta 0:00:14
epoch [166/200] batch [55/81] time 0.445 (0.467) data 0.314 (0.336) loss_u loss_u 0.9224 (0.9640) acc_u 9.3750 (4.3750) lr 1.5567e-04 eta 0:00:12
epoch [166/200] batch [60/81] time 0.788 (0.469) data 0.657 (0.338) loss_u loss_u 0.9922 (0.9640) acc_u 0.0000 (4.3750) lr 1.5567e-04 eta 0:00:09
epoch [166/200] batch [65/81] time 0.475 (0.468) data 0.344 (0.338) loss_u loss_u 0.9590 (0.9649) acc_u 6.2500 (4.2308) lr 1.5567e-04 eta 0:00:07
epoch [166/200] batch [70/81] time 0.385 (0.468) data 0.255 (0.337) loss_u loss_u 0.9614 (0.9649) acc_u 6.2500 (4.2411) lr 1.5567e-04 eta 0:00:05
epoch [166/200] batch [75/81] time 0.571 (0.467) data 0.440 (0.336) loss_u loss_u 0.9893 (0.9660) acc_u 3.1250 (4.1667) lr 1.5567e-04 eta 0:00:02
epoch [166/200] batch [80/81] time 0.428 (0.463) data 0.296 (0.333) loss_u loss_u 0.9961 (0.9663) acc_u 0.0000 (4.1406) lr 1.5567e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1652
confident_label rate tensor(0.1674, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 525
clean true:502
clean false:23
clean_rate:0.9561904761904761
noisy true:982
noisy false:1629
after delete: len(clean_dataset) 525
after delete: len(noisy_dataset) 2611
epoch [167/200] batch [5/16] time 0.446 (0.488) data 0.316 (0.358) loss_x loss_x 1.2344 (1.1320) acc_x 71.8750 (71.2500) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [10/16] time 0.380 (0.488) data 0.250 (0.358) loss_x loss_x 1.1836 (1.1797) acc_x 68.7500 (70.0000) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [15/16] time 0.369 (0.464) data 0.239 (0.334) loss_x loss_x 0.7451 (1.1048) acc_x 87.5000 (73.5417) lr 1.4736e-04 eta 0:00:00
epoch [167/200] batch [5/81] time 0.427 (0.469) data 0.297 (0.339) loss_u loss_u 0.9482 (0.9295) acc_u 9.3750 (8.7500) lr 1.4736e-04 eta 0:00:35
epoch [167/200] batch [10/81] time 0.429 (0.475) data 0.298 (0.344) loss_u loss_u 0.9224 (0.9448) acc_u 9.3750 (7.1875) lr 1.4736e-04 eta 0:00:33
epoch [167/200] batch [15/81] time 0.389 (0.482) data 0.257 (0.351) loss_u loss_u 0.9673 (0.9555) acc_u 3.1250 (5.8333) lr 1.4736e-04 eta 0:00:31
epoch [167/200] batch [20/81] time 0.398 (0.476) data 0.265 (0.345) loss_u loss_u 0.9292 (0.9566) acc_u 9.3750 (5.6250) lr 1.4736e-04 eta 0:00:29
epoch [167/200] batch [25/81] time 0.514 (0.476) data 0.382 (0.344) loss_u loss_u 0.9648 (0.9603) acc_u 3.1250 (5.1250) lr 1.4736e-04 eta 0:00:26
epoch [167/200] batch [30/81] time 0.403 (0.473) data 0.272 (0.342) loss_u loss_u 0.9839 (0.9623) acc_u 3.1250 (5.0000) lr 1.4736e-04 eta 0:00:24
epoch [167/200] batch [35/81] time 0.435 (0.473) data 0.303 (0.341) loss_u loss_u 0.9736 (0.9647) acc_u 3.1250 (4.5536) lr 1.4736e-04 eta 0:00:21
epoch [167/200] batch [40/81] time 0.599 (0.475) data 0.467 (0.344) loss_u loss_u 0.9585 (0.9630) acc_u 6.2500 (4.6875) lr 1.4736e-04 eta 0:00:19
epoch [167/200] batch [45/81] time 0.573 (0.479) data 0.441 (0.347) loss_u loss_u 0.9375 (0.9603) acc_u 6.2500 (5.1389) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [50/81] time 0.391 (0.474) data 0.258 (0.343) loss_u loss_u 0.9600 (0.9616) acc_u 6.2500 (4.9375) lr 1.4736e-04 eta 0:00:14
epoch [167/200] batch [55/81] time 0.380 (0.473) data 0.248 (0.341) loss_u loss_u 0.9619 (0.9619) acc_u 6.2500 (4.8864) lr 1.4736e-04 eta 0:00:12
epoch [167/200] batch [60/81] time 0.431 (0.469) data 0.300 (0.338) loss_u loss_u 0.9897 (0.9624) acc_u 0.0000 (4.7917) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [65/81] time 0.532 (0.469) data 0.399 (0.337) loss_u loss_u 0.9487 (0.9601) acc_u 6.2500 (5.1442) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [70/81] time 0.565 (0.469) data 0.434 (0.338) loss_u loss_u 0.9829 (0.9591) acc_u 3.1250 (5.3125) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [75/81] time 0.422 (0.468) data 0.291 (0.337) loss_u loss_u 0.9805 (0.9593) acc_u 3.1250 (5.2917) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [80/81] time 0.743 (0.470) data 0.611 (0.338) loss_u loss_u 0.9688 (0.9603) acc_u 3.1250 (5.1953) lr 1.4736e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1726
confident_label rate tensor(0.1607, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 504
clean true:483
clean false:21
clean_rate:0.9583333333333334
noisy true:927
noisy false:1705
after delete: len(clean_dataset) 504
after delete: len(noisy_dataset) 2632
epoch [168/200] batch [5/15] time 0.456 (0.434) data 0.326 (0.303) loss_x loss_x 1.4727 (1.1073) acc_x 71.8750 (76.2500) lr 1.3926e-04 eta 0:00:04
epoch [168/200] batch [10/15] time 0.471 (0.457) data 0.341 (0.327) loss_x loss_x 0.9990 (1.0629) acc_x 84.3750 (77.1875) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [15/15] time 0.539 (0.464) data 0.409 (0.333) loss_x loss_x 1.3008 (1.0226) acc_x 78.1250 (78.3333) lr 1.3926e-04 eta 0:00:00
epoch [168/200] batch [5/82] time 0.322 (0.455) data 0.191 (0.325) loss_u loss_u 0.9136 (0.9403) acc_u 9.3750 (8.1250) lr 1.3926e-04 eta 0:00:35
epoch [168/200] batch [10/82] time 0.413 (0.453) data 0.283 (0.323) loss_u loss_u 0.9185 (0.9470) acc_u 9.3750 (6.8750) lr 1.3926e-04 eta 0:00:32
epoch [168/200] batch [15/82] time 0.383 (0.449) data 0.252 (0.318) loss_u loss_u 0.9302 (0.9471) acc_u 9.3750 (6.8750) lr 1.3926e-04 eta 0:00:30
epoch [168/200] batch [20/82] time 0.397 (0.450) data 0.265 (0.319) loss_u loss_u 0.9453 (0.9515) acc_u 6.2500 (6.4062) lr 1.3926e-04 eta 0:00:27
epoch [168/200] batch [25/82] time 0.357 (0.451) data 0.225 (0.320) loss_u loss_u 0.9126 (0.9536) acc_u 9.3750 (6.0000) lr 1.3926e-04 eta 0:00:25
epoch [168/200] batch [30/82] time 0.693 (0.454) data 0.560 (0.323) loss_u loss_u 0.9980 (0.9573) acc_u 0.0000 (5.4167) lr 1.3926e-04 eta 0:00:23
epoch [168/200] batch [35/82] time 0.507 (0.453) data 0.376 (0.322) loss_u loss_u 0.9380 (0.9592) acc_u 6.2500 (5.0893) lr 1.3926e-04 eta 0:00:21
epoch [168/200] batch [40/82] time 0.472 (0.456) data 0.342 (0.325) loss_u loss_u 0.9395 (0.9572) acc_u 6.2500 (5.1562) lr 1.3926e-04 eta 0:00:19
epoch [168/200] batch [45/82] time 0.411 (0.456) data 0.280 (0.326) loss_u loss_u 0.9385 (0.9557) acc_u 6.2500 (5.2083) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [50/82] time 0.457 (0.459) data 0.327 (0.328) loss_u loss_u 0.9580 (0.9572) acc_u 6.2500 (5.1875) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [55/82] time 0.492 (0.457) data 0.360 (0.326) loss_u loss_u 0.9980 (0.9574) acc_u 0.0000 (5.1705) lr 1.3926e-04 eta 0:00:12
epoch [168/200] batch [60/82] time 0.469 (0.457) data 0.338 (0.326) loss_u loss_u 0.9429 (0.9578) acc_u 6.2500 (5.1562) lr 1.3926e-04 eta 0:00:10
epoch [168/200] batch [65/82] time 0.508 (0.458) data 0.376 (0.327) loss_u loss_u 0.9536 (0.9574) acc_u 6.2500 (5.1923) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [70/82] time 0.538 (0.457) data 0.405 (0.326) loss_u loss_u 0.9072 (0.9581) acc_u 9.3750 (5.1339) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [75/82] time 0.376 (0.456) data 0.244 (0.325) loss_u loss_u 0.9580 (0.9586) acc_u 6.2500 (5.1250) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [80/82] time 0.456 (0.457) data 0.324 (0.325) loss_u loss_u 0.9395 (0.9590) acc_u 9.3750 (5.1172) lr 1.3926e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1693
confident_label rate tensor(0.1629, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 511
clean true:492
clean false:19
clean_rate:0.9628180039138943
noisy true:951
noisy false:1674
after delete: len(clean_dataset) 511
after delete: len(noisy_dataset) 2625
epoch [169/200] batch [5/15] time 0.482 (0.444) data 0.351 (0.313) loss_x loss_x 0.9160 (0.8289) acc_x 78.1250 (83.1250) lr 1.3137e-04 eta 0:00:04
epoch [169/200] batch [10/15] time 0.446 (0.458) data 0.316 (0.327) loss_x loss_x 0.9116 (0.9195) acc_x 68.7500 (78.4375) lr 1.3137e-04 eta 0:00:02
epoch [169/200] batch [15/15] time 0.479 (0.453) data 0.349 (0.322) loss_x loss_x 1.0479 (0.9748) acc_x 78.1250 (76.6667) lr 1.3137e-04 eta 0:00:00
epoch [169/200] batch [5/82] time 0.489 (0.457) data 0.357 (0.327) loss_u loss_u 0.9565 (0.9755) acc_u 6.2500 (3.7500) lr 1.3137e-04 eta 0:00:35
epoch [169/200] batch [10/82] time 0.494 (0.457) data 0.364 (0.326) loss_u loss_u 0.9551 (0.9711) acc_u 6.2500 (4.0625) lr 1.3137e-04 eta 0:00:32
epoch [169/200] batch [15/82] time 0.419 (0.453) data 0.288 (0.322) loss_u loss_u 0.9990 (0.9722) acc_u 0.0000 (3.9583) lr 1.3137e-04 eta 0:00:30
epoch [169/200] batch [20/82] time 0.354 (0.453) data 0.223 (0.322) loss_u loss_u 0.9604 (0.9672) acc_u 6.2500 (4.5312) lr 1.3137e-04 eta 0:00:28
epoch [169/200] batch [25/82] time 0.399 (0.453) data 0.267 (0.322) loss_u loss_u 0.8447 (0.9569) acc_u 18.7500 (5.8750) lr 1.3137e-04 eta 0:00:25
epoch [169/200] batch [30/82] time 0.416 (0.448) data 0.285 (0.317) loss_u loss_u 0.9531 (0.9582) acc_u 9.3750 (5.7292) lr 1.3137e-04 eta 0:00:23
epoch [169/200] batch [35/82] time 0.437 (0.449) data 0.306 (0.318) loss_u loss_u 0.9849 (0.9575) acc_u 3.1250 (5.7143) lr 1.3137e-04 eta 0:00:21
epoch [169/200] batch [40/82] time 0.706 (0.458) data 0.575 (0.327) loss_u loss_u 0.9326 (0.9567) acc_u 12.5000 (5.8594) lr 1.3137e-04 eta 0:00:19
epoch [169/200] batch [45/82] time 0.381 (0.459) data 0.250 (0.328) loss_u loss_u 0.9800 (0.9560) acc_u 6.2500 (5.9028) lr 1.3137e-04 eta 0:00:16
epoch [169/200] batch [50/82] time 0.417 (0.458) data 0.285 (0.327) loss_u loss_u 0.9219 (0.9578) acc_u 12.5000 (5.6250) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [55/82] time 0.367 (0.454) data 0.235 (0.322) loss_u loss_u 0.9917 (0.9587) acc_u 0.0000 (5.3409) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [60/82] time 0.457 (0.452) data 0.325 (0.321) loss_u loss_u 0.9536 (0.9576) acc_u 6.2500 (5.4688) lr 1.3137e-04 eta 0:00:09
epoch [169/200] batch [65/82] time 0.443 (0.454) data 0.312 (0.323) loss_u loss_u 0.9536 (0.9586) acc_u 6.2500 (5.3365) lr 1.3137e-04 eta 0:00:07
epoch [169/200] batch [70/82] time 0.386 (0.452) data 0.256 (0.321) loss_u loss_u 0.9595 (0.9593) acc_u 9.3750 (5.3125) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [75/82] time 0.434 (0.451) data 0.302 (0.320) loss_u loss_u 0.9600 (0.9595) acc_u 3.1250 (5.2500) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [80/82] time 0.382 (0.450) data 0.250 (0.319) loss_u loss_u 0.9585 (0.9593) acc_u 6.2500 (5.2344) lr 1.3137e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1691
confident_label rate tensor(0.1601, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 502
clean true:486
clean false:16
clean_rate:0.9681274900398407
noisy true:959
noisy false:1675
after delete: len(clean_dataset) 502
after delete: len(noisy_dataset) 2634
epoch [170/200] batch [5/15] time 0.467 (0.462) data 0.336 (0.331) loss_x loss_x 1.4863 (1.0467) acc_x 71.8750 (76.2500) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [10/15] time 0.377 (0.463) data 0.247 (0.332) loss_x loss_x 0.9185 (1.0076) acc_x 75.0000 (76.8750) lr 1.2369e-04 eta 0:00:02
epoch [170/200] batch [15/15] time 0.597 (0.471) data 0.467 (0.341) loss_x loss_x 0.8511 (1.0771) acc_x 71.8750 (75.0000) lr 1.2369e-04 eta 0:00:00
epoch [170/200] batch [5/82] time 0.571 (0.470) data 0.440 (0.339) loss_u loss_u 0.9668 (0.9626) acc_u 6.2500 (5.6250) lr 1.2369e-04 eta 0:00:36
epoch [170/200] batch [10/82] time 0.501 (0.466) data 0.370 (0.335) loss_u loss_u 0.9229 (0.9570) acc_u 9.3750 (5.9375) lr 1.2369e-04 eta 0:00:33
epoch [170/200] batch [15/82] time 0.487 (0.466) data 0.357 (0.335) loss_u loss_u 0.9858 (0.9579) acc_u 3.1250 (6.4583) lr 1.2369e-04 eta 0:00:31
epoch [170/200] batch [20/82] time 0.534 (0.469) data 0.404 (0.338) loss_u loss_u 0.9683 (0.9555) acc_u 3.1250 (6.5625) lr 1.2369e-04 eta 0:00:29
epoch [170/200] batch [25/82] time 0.411 (0.467) data 0.279 (0.336) loss_u loss_u 0.9634 (0.9557) acc_u 3.1250 (6.2500) lr 1.2369e-04 eta 0:00:26
epoch [170/200] batch [30/82] time 0.417 (0.462) data 0.286 (0.331) loss_u loss_u 0.9731 (0.9585) acc_u 3.1250 (6.0417) lr 1.2369e-04 eta 0:00:23
epoch [170/200] batch [35/82] time 0.514 (0.457) data 0.383 (0.327) loss_u loss_u 0.9629 (0.9599) acc_u 3.1250 (5.8929) lr 1.2369e-04 eta 0:00:21
epoch [170/200] batch [40/82] time 0.413 (0.454) data 0.282 (0.324) loss_u loss_u 0.9570 (0.9603) acc_u 3.1250 (5.7812) lr 1.2369e-04 eta 0:00:19
epoch [170/200] batch [45/82] time 0.455 (0.455) data 0.324 (0.324) loss_u loss_u 0.9312 (0.9579) acc_u 6.2500 (5.9722) lr 1.2369e-04 eta 0:00:16
epoch [170/200] batch [50/82] time 0.498 (0.454) data 0.366 (0.324) loss_u loss_u 0.9751 (0.9595) acc_u 3.1250 (5.8125) lr 1.2369e-04 eta 0:00:14
epoch [170/200] batch [55/82] time 0.608 (0.459) data 0.476 (0.328) loss_u loss_u 0.9917 (0.9587) acc_u 0.0000 (5.9091) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [60/82] time 0.466 (0.462) data 0.335 (0.331) loss_u loss_u 0.9561 (0.9592) acc_u 9.3750 (5.7812) lr 1.2369e-04 eta 0:00:10
epoch [170/200] batch [65/82] time 0.491 (0.461) data 0.361 (0.331) loss_u loss_u 0.9707 (0.9580) acc_u 3.1250 (5.8173) lr 1.2369e-04 eta 0:00:07
epoch [170/200] batch [70/82] time 0.499 (0.463) data 0.369 (0.332) loss_u loss_u 0.9448 (0.9578) acc_u 6.2500 (5.8482) lr 1.2369e-04 eta 0:00:05
epoch [170/200] batch [75/82] time 0.352 (0.463) data 0.221 (0.332) loss_u loss_u 0.8945 (0.9580) acc_u 15.6250 (5.7917) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [80/82] time 0.420 (0.462) data 0.289 (0.332) loss_u loss_u 0.9551 (0.9583) acc_u 6.2500 (5.7812) lr 1.2369e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1706
confident_label rate tensor(0.1617, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 507
clean true:493
clean false:14
clean_rate:0.9723865877712031
noisy true:937
noisy false:1692
after delete: len(clean_dataset) 507
after delete: len(noisy_dataset) 2629
epoch [171/200] batch [5/15] time 0.476 (0.477) data 0.346 (0.347) loss_x loss_x 0.8936 (1.1177) acc_x 75.0000 (69.3750) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [10/15] time 0.503 (0.459) data 0.373 (0.329) loss_x loss_x 1.0879 (1.0234) acc_x 65.6250 (71.5625) lr 1.1623e-04 eta 0:00:02
epoch [171/200] batch [15/15] time 0.403 (0.444) data 0.273 (0.314) loss_x loss_x 1.1045 (1.0337) acc_x 71.8750 (71.2500) lr 1.1623e-04 eta 0:00:00
epoch [171/200] batch [5/82] time 0.485 (0.442) data 0.354 (0.312) loss_u loss_u 0.9688 (0.9500) acc_u 3.1250 (6.2500) lr 1.1623e-04 eta 0:00:34
epoch [171/200] batch [10/82] time 0.498 (0.468) data 0.367 (0.337) loss_u loss_u 0.9141 (0.9593) acc_u 12.5000 (5.0000) lr 1.1623e-04 eta 0:00:33
epoch [171/200] batch [15/82] time 0.412 (0.468) data 0.281 (0.337) loss_u loss_u 0.9463 (0.9620) acc_u 6.2500 (4.3750) lr 1.1623e-04 eta 0:00:31
epoch [171/200] batch [20/82] time 0.556 (0.466) data 0.425 (0.335) loss_u loss_u 0.9922 (0.9634) acc_u 0.0000 (4.2188) lr 1.1623e-04 eta 0:00:28
epoch [171/200] batch [25/82] time 0.388 (0.468) data 0.257 (0.337) loss_u loss_u 0.9619 (0.9638) acc_u 6.2500 (4.3750) lr 1.1623e-04 eta 0:00:26
epoch [171/200] batch [30/82] time 0.370 (0.468) data 0.238 (0.337) loss_u loss_u 0.9980 (0.9641) acc_u 0.0000 (4.3750) lr 1.1623e-04 eta 0:00:24
epoch [171/200] batch [35/82] time 0.554 (0.468) data 0.423 (0.337) loss_u loss_u 0.9951 (0.9667) acc_u 0.0000 (4.0179) lr 1.1623e-04 eta 0:00:21
epoch [171/200] batch [40/82] time 0.426 (0.470) data 0.296 (0.339) loss_u loss_u 0.9385 (0.9653) acc_u 6.2500 (4.3750) lr 1.1623e-04 eta 0:00:19
epoch [171/200] batch [45/82] time 0.638 (0.473) data 0.507 (0.342) loss_u loss_u 0.9258 (0.9652) acc_u 6.2500 (4.3750) lr 1.1623e-04 eta 0:00:17
epoch [171/200] batch [50/82] time 0.438 (0.475) data 0.308 (0.344) loss_u loss_u 0.9795 (0.9652) acc_u 3.1250 (4.3750) lr 1.1623e-04 eta 0:00:15
epoch [171/200] batch [55/82] time 0.476 (0.476) data 0.346 (0.345) loss_u loss_u 0.9692 (0.9656) acc_u 3.1250 (4.3182) lr 1.1623e-04 eta 0:00:12
epoch [171/200] batch [60/82] time 0.368 (0.472) data 0.238 (0.342) loss_u loss_u 0.9756 (0.9658) acc_u 3.1250 (4.2708) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [65/82] time 0.380 (0.467) data 0.250 (0.337) loss_u loss_u 0.9995 (0.9660) acc_u 0.0000 (4.2308) lr 1.1623e-04 eta 0:00:07
epoch [171/200] batch [70/82] time 0.349 (0.463) data 0.218 (0.332) loss_u loss_u 0.9712 (0.9647) acc_u 6.2500 (4.3750) lr 1.1623e-04 eta 0:00:05
epoch [171/200] batch [75/82] time 0.363 (0.459) data 0.232 (0.328) loss_u loss_u 0.9951 (0.9649) acc_u 0.0000 (4.3333) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [80/82] time 0.391 (0.457) data 0.259 (0.327) loss_u loss_u 0.9775 (0.9642) acc_u 3.1250 (4.3359) lr 1.1623e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1708
confident_label rate tensor(0.1607, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 504
clean true:482
clean false:22
clean_rate:0.9563492063492064
noisy true:946
noisy false:1686
after delete: len(clean_dataset) 504
after delete: len(noisy_dataset) 2632
epoch [172/200] batch [5/15] time 0.440 (0.470) data 0.310 (0.339) loss_x loss_x 0.6992 (1.0896) acc_x 84.3750 (77.5000) lr 1.0899e-04 eta 0:00:04
epoch [172/200] batch [10/15] time 0.451 (0.458) data 0.320 (0.328) loss_x loss_x 0.7783 (1.0385) acc_x 78.1250 (78.4375) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [15/15] time 0.487 (0.468) data 0.357 (0.338) loss_x loss_x 1.6094 (1.0665) acc_x 59.3750 (77.7083) lr 1.0899e-04 eta 0:00:00
epoch [172/200] batch [5/82] time 0.495 (0.479) data 0.363 (0.349) loss_u loss_u 0.9766 (0.9819) acc_u 3.1250 (3.1250) lr 1.0899e-04 eta 0:00:36
epoch [172/200] batch [10/82] time 0.515 (0.497) data 0.384 (0.366) loss_u loss_u 0.9736 (0.9709) acc_u 3.1250 (4.3750) lr 1.0899e-04 eta 0:00:35
epoch [172/200] batch [15/82] time 0.406 (0.488) data 0.274 (0.357) loss_u loss_u 0.9165 (0.9645) acc_u 12.5000 (5.0000) lr 1.0899e-04 eta 0:00:32
epoch [172/200] batch [20/82] time 0.518 (0.491) data 0.388 (0.360) loss_u loss_u 0.9121 (0.9621) acc_u 15.6250 (5.3125) lr 1.0899e-04 eta 0:00:30
epoch [172/200] batch [25/82] time 0.526 (0.486) data 0.395 (0.355) loss_u loss_u 0.9243 (0.9634) acc_u 12.5000 (5.1250) lr 1.0899e-04 eta 0:00:27
epoch [172/200] batch [30/82] time 0.557 (0.487) data 0.426 (0.356) loss_u loss_u 0.9731 (0.9643) acc_u 3.1250 (4.8958) lr 1.0899e-04 eta 0:00:25
epoch [172/200] batch [35/82] time 0.678 (0.489) data 0.548 (0.358) loss_u loss_u 0.9346 (0.9627) acc_u 6.2500 (4.9107) lr 1.0899e-04 eta 0:00:22
epoch [172/200] batch [40/82] time 0.374 (0.485) data 0.243 (0.354) loss_u loss_u 0.9722 (0.9622) acc_u 3.1250 (4.9219) lr 1.0899e-04 eta 0:00:20
epoch [172/200] batch [45/82] time 0.428 (0.481) data 0.298 (0.350) loss_u loss_u 0.9907 (0.9618) acc_u 0.0000 (4.9306) lr 1.0899e-04 eta 0:00:17
epoch [172/200] batch [50/82] time 0.383 (0.482) data 0.252 (0.351) loss_u loss_u 0.9604 (0.9613) acc_u 6.2500 (5.1250) lr 1.0899e-04 eta 0:00:15
epoch [172/200] batch [55/82] time 0.411 (0.479) data 0.281 (0.348) loss_u loss_u 0.9668 (0.9612) acc_u 3.1250 (5.1705) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [60/82] time 0.361 (0.473) data 0.230 (0.342) loss_u loss_u 0.9336 (0.9604) acc_u 9.3750 (5.3125) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [65/82] time 0.443 (0.471) data 0.312 (0.340) loss_u loss_u 0.9658 (0.9612) acc_u 6.2500 (5.2404) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [70/82] time 0.407 (0.468) data 0.276 (0.337) loss_u loss_u 0.9282 (0.9593) acc_u 9.3750 (5.4464) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [75/82] time 0.471 (0.467) data 0.340 (0.336) loss_u loss_u 0.9614 (0.9602) acc_u 3.1250 (5.2917) lr 1.0899e-04 eta 0:00:03
epoch [172/200] batch [80/82] time 0.431 (0.465) data 0.300 (0.334) loss_u loss_u 0.9526 (0.9599) acc_u 6.2500 (5.3125) lr 1.0899e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1707
confident_label rate tensor(0.1674, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 525
clean true:506
clean false:19
clean_rate:0.9638095238095238
noisy true:923
noisy false:1688
after delete: len(clean_dataset) 525
after delete: len(noisy_dataset) 2611
epoch [173/200] batch [5/16] time 0.568 (0.512) data 0.438 (0.381) loss_x loss_x 1.3926 (1.0419) acc_x 75.0000 (74.3750) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [10/16] time 0.601 (0.524) data 0.470 (0.393) loss_x loss_x 1.3857 (1.0771) acc_x 56.2500 (73.7500) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [15/16] time 0.495 (0.506) data 0.364 (0.376) loss_x loss_x 0.6724 (1.0195) acc_x 93.7500 (76.4583) lr 1.0197e-04 eta 0:00:00
epoch [173/200] batch [5/81] time 0.380 (0.494) data 0.248 (0.363) loss_u loss_u 0.9951 (0.9687) acc_u 0.0000 (3.7500) lr 1.0197e-04 eta 0:00:37
epoch [173/200] batch [10/81] time 0.654 (0.500) data 0.523 (0.369) loss_u loss_u 0.9985 (0.9712) acc_u 0.0000 (3.4375) lr 1.0197e-04 eta 0:00:35
epoch [173/200] batch [15/81] time 0.350 (0.480) data 0.220 (0.349) loss_u loss_u 0.9722 (0.9662) acc_u 3.1250 (4.3750) lr 1.0197e-04 eta 0:00:31
epoch [173/200] batch [20/81] time 0.535 (0.479) data 0.404 (0.348) loss_u loss_u 0.9639 (0.9622) acc_u 3.1250 (4.6875) lr 1.0197e-04 eta 0:00:29
epoch [173/200] batch [25/81] time 0.558 (0.478) data 0.427 (0.348) loss_u loss_u 0.9692 (0.9627) acc_u 3.1250 (4.6250) lr 1.0197e-04 eta 0:00:26
epoch [173/200] batch [30/81] time 0.481 (0.474) data 0.350 (0.344) loss_u loss_u 0.9409 (0.9620) acc_u 6.2500 (4.6875) lr 1.0197e-04 eta 0:00:24
epoch [173/200] batch [35/81] time 0.481 (0.476) data 0.351 (0.346) loss_u loss_u 0.9575 (0.9594) acc_u 9.3750 (5.1786) lr 1.0197e-04 eta 0:00:21
epoch [173/200] batch [40/81] time 0.373 (0.469) data 0.242 (0.338) loss_u loss_u 0.9990 (0.9590) acc_u 0.0000 (5.3906) lr 1.0197e-04 eta 0:00:19
epoch [173/200] batch [45/81] time 0.488 (0.467) data 0.357 (0.337) loss_u loss_u 0.9771 (0.9582) acc_u 0.0000 (5.4167) lr 1.0197e-04 eta 0:00:16
epoch [173/200] batch [50/81] time 0.394 (0.465) data 0.263 (0.334) loss_u loss_u 0.9194 (0.9567) acc_u 9.3750 (5.5625) lr 1.0197e-04 eta 0:00:14
epoch [173/200] batch [55/81] time 0.459 (0.462) data 0.327 (0.332) loss_u loss_u 0.9585 (0.9586) acc_u 3.1250 (5.2841) lr 1.0197e-04 eta 0:00:12
epoch [173/200] batch [60/81] time 0.455 (0.465) data 0.324 (0.335) loss_u loss_u 0.9937 (0.9603) acc_u 0.0000 (5.0521) lr 1.0197e-04 eta 0:00:09
epoch [173/200] batch [65/81] time 0.448 (0.469) data 0.316 (0.338) loss_u loss_u 0.9839 (0.9613) acc_u 0.0000 (4.7596) lr 1.0197e-04 eta 0:00:07
epoch [173/200] batch [70/81] time 0.458 (0.468) data 0.326 (0.337) loss_u loss_u 0.9810 (0.9593) acc_u 6.2500 (5.0893) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [75/81] time 0.504 (0.467) data 0.373 (0.336) loss_u loss_u 0.9727 (0.9606) acc_u 3.1250 (4.9167) lr 1.0197e-04 eta 0:00:02
epoch [173/200] batch [80/81] time 0.532 (0.467) data 0.400 (0.336) loss_u loss_u 0.9546 (0.9620) acc_u 6.2500 (4.7656) lr 1.0197e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1699
confident_label rate tensor(0.1671, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 524
clean true:503
clean false:21
clean_rate:0.9599236641221374
noisy true:934
noisy false:1678
after delete: len(clean_dataset) 524
after delete: len(noisy_dataset) 2612
epoch [174/200] batch [5/16] time 0.481 (0.440) data 0.350 (0.310) loss_x loss_x 0.9355 (0.9253) acc_x 81.2500 (79.3750) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [10/16] time 0.333 (0.469) data 0.203 (0.339) loss_x loss_x 0.8633 (0.9719) acc_x 78.1250 (76.2500) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [15/16] time 0.458 (0.472) data 0.328 (0.341) loss_x loss_x 0.5889 (0.9942) acc_x 84.3750 (75.4167) lr 9.5173e-05 eta 0:00:00
epoch [174/200] batch [5/81] time 0.369 (0.467) data 0.238 (0.337) loss_u loss_u 0.9678 (0.9719) acc_u 6.2500 (5.6250) lr 9.5173e-05 eta 0:00:35
epoch [174/200] batch [10/81] time 0.361 (0.456) data 0.231 (0.325) loss_u loss_u 0.9785 (0.9622) acc_u 3.1250 (5.9375) lr 9.5173e-05 eta 0:00:32
epoch [174/200] batch [15/81] time 0.622 (0.463) data 0.491 (0.333) loss_u loss_u 0.9448 (0.9580) acc_u 6.2500 (6.0417) lr 9.5173e-05 eta 0:00:30
epoch [174/200] batch [20/81] time 0.449 (0.459) data 0.317 (0.329) loss_u loss_u 0.9404 (0.9636) acc_u 6.2500 (5.1562) lr 9.5173e-05 eta 0:00:28
epoch [174/200] batch [25/81] time 0.441 (0.456) data 0.310 (0.326) loss_u loss_u 0.9990 (0.9647) acc_u 0.0000 (5.0000) lr 9.5173e-05 eta 0:00:25
epoch [174/200] batch [30/81] time 0.412 (0.452) data 0.281 (0.322) loss_u loss_u 0.9092 (0.9614) acc_u 12.5000 (5.4167) lr 9.5173e-05 eta 0:00:23
epoch [174/200] batch [35/81] time 0.489 (0.454) data 0.359 (0.323) loss_u loss_u 0.9414 (0.9621) acc_u 6.2500 (5.2679) lr 9.5173e-05 eta 0:00:20
epoch [174/200] batch [40/81] time 0.374 (0.453) data 0.243 (0.322) loss_u loss_u 0.9912 (0.9612) acc_u 0.0000 (5.4688) lr 9.5173e-05 eta 0:00:18
epoch [174/200] batch [45/81] time 0.727 (0.462) data 0.597 (0.331) loss_u loss_u 0.9878 (0.9624) acc_u 3.1250 (5.1389) lr 9.5173e-05 eta 0:00:16
epoch [174/200] batch [50/81] time 0.572 (0.463) data 0.441 (0.333) loss_u loss_u 0.9385 (0.9635) acc_u 6.2500 (4.9375) lr 9.5173e-05 eta 0:00:14
epoch [174/200] batch [55/81] time 0.407 (0.461) data 0.276 (0.330) loss_u loss_u 0.9502 (0.9636) acc_u 6.2500 (4.8864) lr 9.5173e-05 eta 0:00:11
epoch [174/200] batch [60/81] time 0.515 (0.460) data 0.384 (0.329) loss_u loss_u 0.9639 (0.9651) acc_u 3.1250 (4.5833) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [65/81] time 0.501 (0.458) data 0.371 (0.327) loss_u loss_u 0.9658 (0.9640) acc_u 3.1250 (4.7596) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [70/81] time 0.444 (0.458) data 0.313 (0.328) loss_u loss_u 0.9609 (0.9633) acc_u 6.2500 (4.8214) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [75/81] time 0.452 (0.459) data 0.321 (0.328) loss_u loss_u 0.9761 (0.9644) acc_u 3.1250 (4.7500) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [80/81] time 0.420 (0.456) data 0.289 (0.325) loss_u loss_u 0.9697 (0.9635) acc_u 3.1250 (4.8438) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1717
confident_label rate tensor(0.1617, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 507
clean true:488
clean false:19
clean_rate:0.9625246548323472
noisy true:931
noisy false:1698
after delete: len(clean_dataset) 507
after delete: len(noisy_dataset) 2629
epoch [175/200] batch [5/15] time 0.401 (0.508) data 0.271 (0.378) loss_x loss_x 1.3779 (1.2021) acc_x 75.0000 (73.1250) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [10/15] time 0.612 (0.506) data 0.481 (0.376) loss_x loss_x 1.2422 (1.1245) acc_x 75.0000 (75.3125) lr 8.8597e-05 eta 0:00:02
epoch [175/200] batch [15/15] time 0.565 (0.501) data 0.434 (0.370) loss_x loss_x 1.9307 (1.1437) acc_x 68.7500 (75.4167) lr 8.8597e-05 eta 0:00:00
epoch [175/200] batch [5/82] time 0.444 (0.493) data 0.314 (0.362) loss_u loss_u 0.9761 (0.9562) acc_u 3.1250 (5.6250) lr 8.8597e-05 eta 0:00:37
epoch [175/200] batch [10/82] time 0.400 (0.483) data 0.269 (0.352) loss_u loss_u 0.9004 (0.9488) acc_u 12.5000 (6.5625) lr 8.8597e-05 eta 0:00:34
epoch [175/200] batch [15/82] time 0.390 (0.471) data 0.259 (0.340) loss_u loss_u 0.9854 (0.9557) acc_u 3.1250 (5.4167) lr 8.8597e-05 eta 0:00:31
epoch [175/200] batch [20/82] time 0.468 (0.468) data 0.338 (0.337) loss_u loss_u 0.9893 (0.9527) acc_u 0.0000 (5.9375) lr 8.8597e-05 eta 0:00:29
epoch [175/200] batch [25/82] time 0.482 (0.466) data 0.350 (0.335) loss_u loss_u 0.8701 (0.9502) acc_u 15.6250 (6.2500) lr 8.8597e-05 eta 0:00:26
epoch [175/200] batch [30/82] time 0.506 (0.466) data 0.376 (0.335) loss_u loss_u 0.9570 (0.9542) acc_u 6.2500 (5.8333) lr 8.8597e-05 eta 0:00:24
epoch [175/200] batch [35/82] time 0.387 (0.460) data 0.256 (0.329) loss_u loss_u 0.9683 (0.9549) acc_u 3.1250 (5.8929) lr 8.8597e-05 eta 0:00:21
epoch [175/200] batch [40/82] time 0.416 (0.458) data 0.284 (0.327) loss_u loss_u 0.9214 (0.9532) acc_u 12.5000 (6.1719) lr 8.8597e-05 eta 0:00:19
epoch [175/200] batch [45/82] time 0.455 (0.458) data 0.325 (0.327) loss_u loss_u 0.9761 (0.9529) acc_u 3.1250 (6.1111) lr 8.8597e-05 eta 0:00:16
epoch [175/200] batch [50/82] time 0.386 (0.455) data 0.255 (0.324) loss_u loss_u 0.9839 (0.9531) acc_u 3.1250 (6.1875) lr 8.8597e-05 eta 0:00:14
epoch [175/200] batch [55/82] time 0.588 (0.461) data 0.456 (0.330) loss_u loss_u 1.0000 (0.9558) acc_u 0.0000 (5.7955) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [60/82] time 0.486 (0.459) data 0.354 (0.328) loss_u loss_u 0.8823 (0.9546) acc_u 12.5000 (5.8333) lr 8.8597e-05 eta 0:00:10
epoch [175/200] batch [65/82] time 0.435 (0.460) data 0.302 (0.329) loss_u loss_u 0.9370 (0.9551) acc_u 6.2500 (5.7212) lr 8.8597e-05 eta 0:00:07
epoch [175/200] batch [70/82] time 0.507 (0.460) data 0.377 (0.329) loss_u loss_u 0.9946 (0.9565) acc_u 0.0000 (5.5804) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [75/82] time 0.505 (0.462) data 0.374 (0.331) loss_u loss_u 0.9751 (0.9571) acc_u 3.1250 (5.5000) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [80/82] time 0.382 (0.461) data 0.252 (0.330) loss_u loss_u 0.9478 (0.9571) acc_u 6.2500 (5.4297) lr 8.8597e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1699
confident_label rate tensor(0.1639, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 514
clean true:489
clean false:25
clean_rate:0.9513618677042801
noisy true:948
noisy false:1674
after delete: len(clean_dataset) 514
after delete: len(noisy_dataset) 2622
epoch [176/200] batch [5/16] time 0.427 (0.425) data 0.297 (0.295) loss_x loss_x 1.1631 (1.0285) acc_x 75.0000 (75.6250) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [10/16] time 0.474 (0.424) data 0.344 (0.294) loss_x loss_x 1.2393 (1.1164) acc_x 75.0000 (74.0625) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [15/16] time 0.532 (0.450) data 0.402 (0.320) loss_x loss_x 0.8501 (1.0617) acc_x 84.3750 (75.4167) lr 8.2245e-05 eta 0:00:00
epoch [176/200] batch [5/81] time 0.486 (0.446) data 0.356 (0.316) loss_u loss_u 0.9126 (0.9643) acc_u 9.3750 (4.3750) lr 8.2245e-05 eta 0:00:33
epoch [176/200] batch [10/81] time 0.535 (0.459) data 0.404 (0.329) loss_u loss_u 0.9824 (0.9645) acc_u 3.1250 (4.3750) lr 8.2245e-05 eta 0:00:32
epoch [176/200] batch [15/81] time 0.454 (0.460) data 0.323 (0.330) loss_u loss_u 0.9375 (0.9645) acc_u 9.3750 (4.5833) lr 8.2245e-05 eta 0:00:30
epoch [176/200] batch [20/81] time 0.404 (0.460) data 0.273 (0.329) loss_u loss_u 0.9688 (0.9654) acc_u 6.2500 (4.5312) lr 8.2245e-05 eta 0:00:28
epoch [176/200] batch [25/81] time 0.404 (0.458) data 0.273 (0.328) loss_u loss_u 0.9443 (0.9606) acc_u 6.2500 (5.1250) lr 8.2245e-05 eta 0:00:25
epoch [176/200] batch [30/81] time 0.409 (0.456) data 0.277 (0.325) loss_u loss_u 0.9180 (0.9597) acc_u 12.5000 (5.3125) lr 8.2245e-05 eta 0:00:23
epoch [176/200] batch [35/81] time 0.438 (0.463) data 0.308 (0.332) loss_u loss_u 0.9731 (0.9587) acc_u 3.1250 (5.4464) lr 8.2245e-05 eta 0:00:21
epoch [176/200] batch [40/81] time 0.695 (0.469) data 0.564 (0.338) loss_u loss_u 0.8960 (0.9588) acc_u 12.5000 (5.2344) lr 8.2245e-05 eta 0:00:19
epoch [176/200] batch [45/81] time 0.557 (0.474) data 0.426 (0.343) loss_u loss_u 0.9849 (0.9574) acc_u 0.0000 (5.2778) lr 8.2245e-05 eta 0:00:17
epoch [176/200] batch [50/81] time 0.735 (0.478) data 0.604 (0.347) loss_u loss_u 0.9810 (0.9590) acc_u 3.1250 (5.0000) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [55/81] time 0.350 (0.477) data 0.220 (0.346) loss_u loss_u 0.9663 (0.9577) acc_u 3.1250 (5.1136) lr 8.2245e-05 eta 0:00:12
epoch [176/200] batch [60/81] time 0.492 (0.477) data 0.361 (0.346) loss_u loss_u 0.9531 (0.9572) acc_u 6.2500 (5.2083) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [65/81] time 0.477 (0.475) data 0.346 (0.344) loss_u loss_u 0.9194 (0.9565) acc_u 18.7500 (5.4327) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [70/81] time 0.482 (0.473) data 0.352 (0.342) loss_u loss_u 0.9941 (0.9563) acc_u 0.0000 (5.4464) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [75/81] time 0.437 (0.472) data 0.307 (0.341) loss_u loss_u 0.9634 (0.9574) acc_u 6.2500 (5.3750) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [80/81] time 0.522 (0.473) data 0.392 (0.342) loss_u loss_u 0.9248 (0.9577) acc_u 9.3750 (5.3125) lr 8.2245e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1677
confident_label rate tensor(0.1623, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 509
clean true:495
clean false:14
clean_rate:0.9724950884086444
noisy true:964
noisy false:1663
after delete: len(clean_dataset) 509
after delete: len(noisy_dataset) 2627
epoch [177/200] batch [5/15] time 0.453 (0.530) data 0.323 (0.399) loss_x loss_x 1.1445 (0.9701) acc_x 84.3750 (80.0000) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [10/15] time 0.443 (0.488) data 0.313 (0.357) loss_x loss_x 0.8525 (1.0379) acc_x 81.2500 (78.1250) lr 7.6120e-05 eta 0:00:02
epoch [177/200] batch [15/15] time 0.481 (0.473) data 0.351 (0.343) loss_x loss_x 0.8638 (1.0153) acc_x 78.1250 (76.8750) lr 7.6120e-05 eta 0:00:00
epoch [177/200] batch [5/82] time 0.450 (0.481) data 0.319 (0.350) loss_u loss_u 0.9673 (0.9326) acc_u 9.3750 (9.3750) lr 7.6120e-05 eta 0:00:37
epoch [177/200] batch [10/82] time 0.492 (0.481) data 0.362 (0.350) loss_u loss_u 0.9961 (0.9548) acc_u 0.0000 (5.9375) lr 7.6120e-05 eta 0:00:34
epoch [177/200] batch [15/82] time 0.437 (0.487) data 0.306 (0.357) loss_u loss_u 0.9785 (0.9528) acc_u 3.1250 (5.8333) lr 7.6120e-05 eta 0:00:32
epoch [177/200] batch [20/82] time 0.467 (0.486) data 0.336 (0.356) loss_u loss_u 0.9985 (0.9522) acc_u 0.0000 (5.6250) lr 7.6120e-05 eta 0:00:30
epoch [177/200] batch [25/82] time 0.508 (0.481) data 0.376 (0.351) loss_u loss_u 0.9390 (0.9563) acc_u 9.3750 (5.3750) lr 7.6120e-05 eta 0:00:27
epoch [177/200] batch [30/82] time 0.377 (0.477) data 0.245 (0.347) loss_u loss_u 0.9800 (0.9566) acc_u 3.1250 (5.3125) lr 7.6120e-05 eta 0:00:24
epoch [177/200] batch [35/82] time 0.512 (0.479) data 0.381 (0.348) loss_u loss_u 0.9062 (0.9569) acc_u 12.5000 (5.2679) lr 7.6120e-05 eta 0:00:22
epoch [177/200] batch [40/82] time 0.448 (0.478) data 0.317 (0.347) loss_u loss_u 0.9624 (0.9597) acc_u 3.1250 (4.9219) lr 7.6120e-05 eta 0:00:20
epoch [177/200] batch [45/82] time 0.512 (0.479) data 0.382 (0.348) loss_u loss_u 0.9863 (0.9611) acc_u 3.1250 (4.8611) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [50/82] time 0.619 (0.481) data 0.488 (0.351) loss_u loss_u 0.9385 (0.9599) acc_u 6.2500 (4.9375) lr 7.6120e-05 eta 0:00:15
epoch [177/200] batch [55/82] time 0.450 (0.485) data 0.320 (0.354) loss_u loss_u 0.9863 (0.9597) acc_u 3.1250 (5.0000) lr 7.6120e-05 eta 0:00:13
epoch [177/200] batch [60/82] time 0.445 (0.480) data 0.312 (0.349) loss_u loss_u 0.9985 (0.9609) acc_u 0.0000 (4.8438) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [65/82] time 0.460 (0.476) data 0.329 (0.345) loss_u loss_u 0.9414 (0.9613) acc_u 6.2500 (4.7596) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [70/82] time 0.390 (0.475) data 0.259 (0.344) loss_u loss_u 0.9448 (0.9606) acc_u 6.2500 (4.8661) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [75/82] time 0.476 (0.473) data 0.344 (0.342) loss_u loss_u 0.9922 (0.9598) acc_u 0.0000 (4.9167) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [80/82] time 0.571 (0.473) data 0.440 (0.342) loss_u loss_u 0.9121 (0.9594) acc_u 18.7500 (5.0000) lr 7.6120e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1697
confident_label rate tensor(0.1706, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 535
clean true:517
clean false:18
clean_rate:0.9663551401869159
noisy true:922
noisy false:1679
after delete: len(clean_dataset) 535
after delete: len(noisy_dataset) 2601
epoch [178/200] batch [5/16] time 0.487 (0.447) data 0.356 (0.316) loss_x loss_x 0.5908 (1.1070) acc_x 87.5000 (74.3750) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [10/16] time 0.427 (0.443) data 0.296 (0.313) loss_x loss_x 1.0537 (1.0672) acc_x 62.5000 (75.3125) lr 7.0224e-05 eta 0:00:02
epoch [178/200] batch [15/16] time 0.585 (0.476) data 0.455 (0.346) loss_x loss_x 1.1045 (1.0061) acc_x 78.1250 (76.6667) lr 7.0224e-05 eta 0:00:00
epoch [178/200] batch [5/81] time 0.708 (0.465) data 0.577 (0.335) loss_u loss_u 0.9941 (0.9710) acc_u 0.0000 (3.7500) lr 7.0224e-05 eta 0:00:35
epoch [178/200] batch [10/81] time 0.421 (0.457) data 0.290 (0.326) loss_u loss_u 0.9438 (0.9724) acc_u 6.2500 (3.4375) lr 7.0224e-05 eta 0:00:32
epoch [178/200] batch [15/81] time 0.416 (0.457) data 0.286 (0.327) loss_u loss_u 0.9541 (0.9670) acc_u 6.2500 (4.5833) lr 7.0224e-05 eta 0:00:30
epoch [178/200] batch [20/81] time 0.419 (0.451) data 0.287 (0.320) loss_u loss_u 0.9565 (0.9691) acc_u 3.1250 (4.0625) lr 7.0224e-05 eta 0:00:27
epoch [178/200] batch [25/81] time 0.476 (0.451) data 0.345 (0.320) loss_u loss_u 0.9907 (0.9634) acc_u 0.0000 (4.7500) lr 7.0224e-05 eta 0:00:25
epoch [178/200] batch [30/81] time 0.485 (0.451) data 0.354 (0.320) loss_u loss_u 0.9990 (0.9630) acc_u 0.0000 (4.7917) lr 7.0224e-05 eta 0:00:22
epoch [178/200] batch [35/81] time 0.455 (0.449) data 0.324 (0.318) loss_u loss_u 0.9277 (0.9611) acc_u 9.3750 (5.0893) lr 7.0224e-05 eta 0:00:20
epoch [178/200] batch [40/81] time 0.342 (0.450) data 0.211 (0.319) loss_u loss_u 0.9971 (0.9635) acc_u 0.0000 (4.9219) lr 7.0224e-05 eta 0:00:18
epoch [178/200] batch [45/81] time 0.482 (0.458) data 0.352 (0.327) loss_u loss_u 0.9502 (0.9629) acc_u 6.2500 (5.0000) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [50/81] time 0.449 (0.455) data 0.319 (0.324) loss_u loss_u 0.8960 (0.9611) acc_u 12.5000 (5.1875) lr 7.0224e-05 eta 0:00:14
epoch [178/200] batch [55/81] time 0.486 (0.457) data 0.355 (0.326) loss_u loss_u 0.9604 (0.9623) acc_u 6.2500 (5.0000) lr 7.0224e-05 eta 0:00:11
epoch [178/200] batch [60/81] time 0.446 (0.454) data 0.315 (0.324) loss_u loss_u 0.9565 (0.9620) acc_u 6.2500 (5.0000) lr 7.0224e-05 eta 0:00:09
epoch [178/200] batch [65/81] time 0.446 (0.454) data 0.315 (0.323) loss_u loss_u 0.9380 (0.9630) acc_u 6.2500 (4.8077) lr 7.0224e-05 eta 0:00:07
epoch [178/200] batch [70/81] time 0.422 (0.453) data 0.291 (0.322) loss_u loss_u 0.9976 (0.9635) acc_u 0.0000 (4.6875) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [75/81] time 0.451 (0.454) data 0.320 (0.323) loss_u loss_u 0.9912 (0.9638) acc_u 0.0000 (4.5833) lr 7.0224e-05 eta 0:00:02
epoch [178/200] batch [80/81] time 0.496 (0.456) data 0.365 (0.325) loss_u loss_u 0.9819 (0.9634) acc_u 0.0000 (4.6094) lr 7.0224e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1691
confident_label rate tensor(0.1658, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 520
clean true:508
clean false:12
clean_rate:0.9769230769230769
noisy true:937
noisy false:1679
after delete: len(clean_dataset) 520
after delete: len(noisy_dataset) 2616
epoch [179/200] batch [5/16] time 0.453 (0.458) data 0.322 (0.327) loss_x loss_x 0.8853 (1.2116) acc_x 81.2500 (70.6250) lr 6.4556e-05 eta 0:00:05
epoch [179/200] batch [10/16] time 0.436 (0.463) data 0.306 (0.333) loss_x loss_x 1.1006 (1.0752) acc_x 75.0000 (72.8125) lr 6.4556e-05 eta 0:00:02
epoch [179/200] batch [15/16] time 0.563 (0.456) data 0.433 (0.326) loss_x loss_x 1.0459 (0.9962) acc_x 75.0000 (74.3750) lr 6.4556e-05 eta 0:00:00
epoch [179/200] batch [5/81] time 0.385 (0.450) data 0.255 (0.319) loss_u loss_u 0.9800 (0.9747) acc_u 6.2500 (3.7500) lr 6.4556e-05 eta 0:00:34
epoch [179/200] batch [10/81] time 0.649 (0.458) data 0.518 (0.327) loss_u loss_u 0.9819 (0.9662) acc_u 3.1250 (4.3750) lr 6.4556e-05 eta 0:00:32
epoch [179/200] batch [15/81] time 0.363 (0.460) data 0.233 (0.330) loss_u loss_u 0.9556 (0.9629) acc_u 3.1250 (4.3750) lr 6.4556e-05 eta 0:00:30
epoch [179/200] batch [20/81] time 0.490 (0.457) data 0.358 (0.326) loss_u loss_u 0.9551 (0.9608) acc_u 3.1250 (4.6875) lr 6.4556e-05 eta 0:00:27
epoch [179/200] batch [25/81] time 0.453 (0.455) data 0.322 (0.324) loss_u loss_u 0.9683 (0.9616) acc_u 6.2500 (4.6250) lr 6.4556e-05 eta 0:00:25
epoch [179/200] batch [30/81] time 0.467 (0.458) data 0.337 (0.327) loss_u loss_u 0.9941 (0.9641) acc_u 0.0000 (4.2708) lr 6.4556e-05 eta 0:00:23
epoch [179/200] batch [35/81] time 0.385 (0.458) data 0.254 (0.327) loss_u loss_u 0.9785 (0.9636) acc_u 3.1250 (4.5536) lr 6.4556e-05 eta 0:00:21
epoch [179/200] batch [40/81] time 0.453 (0.456) data 0.323 (0.325) loss_u loss_u 0.9785 (0.9630) acc_u 3.1250 (4.6875) lr 6.4556e-05 eta 0:00:18
epoch [179/200] batch [45/81] time 0.352 (0.456) data 0.221 (0.325) loss_u loss_u 0.9951 (0.9630) acc_u 0.0000 (4.5833) lr 6.4556e-05 eta 0:00:16
epoch [179/200] batch [50/81] time 0.493 (0.460) data 0.363 (0.329) loss_u loss_u 0.9873 (0.9640) acc_u 3.1250 (4.5625) lr 6.4556e-05 eta 0:00:14
epoch [179/200] batch [55/81] time 0.590 (0.457) data 0.460 (0.326) loss_u loss_u 0.9858 (0.9646) acc_u 3.1250 (4.4886) lr 6.4556e-05 eta 0:00:11
epoch [179/200] batch [60/81] time 0.456 (0.457) data 0.325 (0.327) loss_u loss_u 0.9624 (0.9648) acc_u 9.3750 (4.4792) lr 6.4556e-05 eta 0:00:09
epoch [179/200] batch [65/81] time 0.637 (0.457) data 0.506 (0.327) loss_u loss_u 0.9399 (0.9653) acc_u 6.2500 (4.4231) lr 6.4556e-05 eta 0:00:07
epoch [179/200] batch [70/81] time 0.359 (0.455) data 0.229 (0.325) loss_u loss_u 0.9180 (0.9656) acc_u 9.3750 (4.3304) lr 6.4556e-05 eta 0:00:05
epoch [179/200] batch [75/81] time 0.421 (0.453) data 0.290 (0.322) loss_u loss_u 0.9429 (0.9655) acc_u 9.3750 (4.3333) lr 6.4556e-05 eta 0:00:02
epoch [179/200] batch [80/81] time 0.407 (0.453) data 0.276 (0.322) loss_u loss_u 0.9478 (0.9652) acc_u 6.2500 (4.3750) lr 6.4556e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1740
confident_label rate tensor(0.1607, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 504
clean true:486
clean false:18
clean_rate:0.9642857142857143
noisy true:910
noisy false:1722
after delete: len(clean_dataset) 504
after delete: len(noisy_dataset) 2632
epoch [180/200] batch [5/15] time 0.629 (0.545) data 0.499 (0.414) loss_x loss_x 1.1328 (1.0532) acc_x 71.8750 (73.7500) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [10/15] time 0.422 (0.491) data 0.292 (0.361) loss_x loss_x 1.5322 (1.0901) acc_x 62.5000 (72.8125) lr 5.9119e-05 eta 0:00:02
epoch [180/200] batch [15/15] time 0.470 (0.487) data 0.339 (0.356) loss_x loss_x 1.0742 (1.0269) acc_x 75.0000 (75.8333) lr 5.9119e-05 eta 0:00:00
epoch [180/200] batch [5/82] time 0.552 (0.482) data 0.422 (0.351) loss_u loss_u 0.9043 (0.9464) acc_u 15.6250 (8.1250) lr 5.9119e-05 eta 0:00:37
epoch [180/200] batch [10/82] time 0.457 (0.469) data 0.327 (0.338) loss_u loss_u 0.9175 (0.9514) acc_u 12.5000 (6.8750) lr 5.9119e-05 eta 0:00:33
epoch [180/200] batch [15/82] time 0.683 (0.467) data 0.553 (0.336) loss_u loss_u 0.9336 (0.9538) acc_u 9.3750 (6.4583) lr 5.9119e-05 eta 0:00:31
epoch [180/200] batch [20/82] time 0.611 (0.474) data 0.481 (0.343) loss_u loss_u 0.9922 (0.9525) acc_u 0.0000 (6.2500) lr 5.9119e-05 eta 0:00:29
epoch [180/200] batch [25/82] time 0.736 (0.478) data 0.605 (0.347) loss_u loss_u 0.9707 (0.9505) acc_u 3.1250 (6.2500) lr 5.9119e-05 eta 0:00:27
epoch [180/200] batch [30/82] time 0.378 (0.470) data 0.247 (0.339) loss_u loss_u 0.9595 (0.9525) acc_u 6.2500 (6.0417) lr 5.9119e-05 eta 0:00:24
epoch [180/200] batch [35/82] time 0.457 (0.474) data 0.325 (0.343) loss_u loss_u 0.9961 (0.9542) acc_u 0.0000 (5.8036) lr 5.9119e-05 eta 0:00:22
epoch [180/200] batch [40/82] time 0.567 (0.477) data 0.435 (0.346) loss_u loss_u 0.9766 (0.9551) acc_u 6.2500 (5.9375) lr 5.9119e-05 eta 0:00:20
epoch [180/200] batch [45/82] time 0.436 (0.472) data 0.306 (0.341) loss_u loss_u 0.9956 (0.9572) acc_u 0.0000 (5.6250) lr 5.9119e-05 eta 0:00:17
epoch [180/200] batch [50/82] time 0.437 (0.476) data 0.307 (0.345) loss_u loss_u 0.9526 (0.9560) acc_u 6.2500 (5.8125) lr 5.9119e-05 eta 0:00:15
epoch [180/200] batch [55/82] time 0.512 (0.476) data 0.382 (0.345) loss_u loss_u 0.9336 (0.9547) acc_u 9.3750 (6.0795) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [60/82] time 0.494 (0.474) data 0.362 (0.343) loss_u loss_u 0.9912 (0.9572) acc_u 0.0000 (5.6771) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [65/82] time 0.387 (0.473) data 0.257 (0.342) loss_u loss_u 0.9683 (0.9572) acc_u 3.1250 (5.6731) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [70/82] time 0.420 (0.471) data 0.290 (0.340) loss_u loss_u 0.9434 (0.9565) acc_u 6.2500 (5.8036) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [75/82] time 0.503 (0.469) data 0.371 (0.338) loss_u loss_u 0.9287 (0.9566) acc_u 6.2500 (5.7083) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [80/82] time 0.522 (0.470) data 0.391 (0.339) loss_u loss_u 0.9307 (0.9576) acc_u 9.3750 (5.6250) lr 5.9119e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1716
confident_label rate tensor(0.1594, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 500
clean true:478
clean false:22
clean_rate:0.956
noisy true:942
noisy false:1694
after delete: len(clean_dataset) 500
after delete: len(noisy_dataset) 2636
epoch [181/200] batch [5/15] time 0.398 (0.414) data 0.268 (0.284) loss_x loss_x 0.8721 (0.9079) acc_x 75.0000 (78.1250) lr 5.3915e-05 eta 0:00:04
epoch [181/200] batch [10/15] time 0.527 (0.446) data 0.397 (0.316) loss_x loss_x 0.8037 (0.8569) acc_x 84.3750 (78.7500) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [15/15] time 0.501 (0.453) data 0.371 (0.323) loss_x loss_x 0.8506 (0.9435) acc_x 84.3750 (78.1250) lr 5.3915e-05 eta 0:00:00
epoch [181/200] batch [5/82] time 0.520 (0.442) data 0.389 (0.312) loss_u loss_u 0.9985 (0.9597) acc_u 0.0000 (4.3750) lr 5.3915e-05 eta 0:00:34
epoch [181/200] batch [10/82] time 0.451 (0.441) data 0.321 (0.310) loss_u loss_u 0.9424 (0.9649) acc_u 6.2500 (4.0625) lr 5.3915e-05 eta 0:00:31
epoch [181/200] batch [15/82] time 0.377 (0.435) data 0.246 (0.305) loss_u loss_u 0.9365 (0.9643) acc_u 6.2500 (4.3750) lr 5.3915e-05 eta 0:00:29
epoch [181/200] batch [20/82] time 0.351 (0.430) data 0.220 (0.299) loss_u loss_u 0.9897 (0.9666) acc_u 0.0000 (3.9062) lr 5.3915e-05 eta 0:00:26
epoch [181/200] batch [25/82] time 0.703 (0.442) data 0.572 (0.312) loss_u loss_u 0.8960 (0.9611) acc_u 9.3750 (4.5000) lr 5.3915e-05 eta 0:00:25
epoch [181/200] batch [30/82] time 0.387 (0.441) data 0.255 (0.310) loss_u loss_u 0.9888 (0.9596) acc_u 0.0000 (4.6875) lr 5.3915e-05 eta 0:00:22
epoch [181/200] batch [35/82] time 0.493 (0.448) data 0.362 (0.317) loss_u loss_u 0.9624 (0.9597) acc_u 3.1250 (4.6429) lr 5.3915e-05 eta 0:00:21
epoch [181/200] batch [40/82] time 0.383 (0.448) data 0.252 (0.317) loss_u loss_u 0.9595 (0.9610) acc_u 3.1250 (4.6094) lr 5.3915e-05 eta 0:00:18
epoch [181/200] batch [45/82] time 0.448 (0.449) data 0.317 (0.318) loss_u loss_u 0.9629 (0.9603) acc_u 6.2500 (4.9306) lr 5.3915e-05 eta 0:00:16
epoch [181/200] batch [50/82] time 0.451 (0.450) data 0.320 (0.319) loss_u loss_u 0.9395 (0.9615) acc_u 9.3750 (4.8750) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [55/82] time 0.466 (0.450) data 0.333 (0.319) loss_u loss_u 0.9600 (0.9586) acc_u 3.1250 (5.1705) lr 5.3915e-05 eta 0:00:12
epoch [181/200] batch [60/82] time 0.550 (0.450) data 0.420 (0.320) loss_u loss_u 0.9243 (0.9581) acc_u 9.3750 (5.1562) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [65/82] time 0.708 (0.455) data 0.577 (0.324) loss_u loss_u 0.9688 (0.9593) acc_u 3.1250 (5.0000) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [70/82] time 0.479 (0.458) data 0.349 (0.327) loss_u loss_u 0.9277 (0.9601) acc_u 9.3750 (4.9107) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [75/82] time 0.513 (0.461) data 0.381 (0.330) loss_u loss_u 0.8867 (0.9590) acc_u 12.5000 (5.0000) lr 5.3915e-05 eta 0:00:03
epoch [181/200] batch [80/82] time 0.367 (0.462) data 0.236 (0.332) loss_u loss_u 0.9609 (0.9589) acc_u 6.2500 (5.0391) lr 5.3915e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1706
confident_label rate tensor(0.1566, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 491
clean true:469
clean false:22
clean_rate:0.955193482688391
noisy true:961
noisy false:1684
after delete: len(clean_dataset) 491
after delete: len(noisy_dataset) 2645
epoch [182/200] batch [5/15] time 0.382 (0.502) data 0.252 (0.371) loss_x loss_x 1.1816 (1.1597) acc_x 68.7500 (72.5000) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [10/15] time 0.372 (0.452) data 0.241 (0.321) loss_x loss_x 0.6035 (1.0610) acc_x 90.6250 (75.0000) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [15/15] time 0.457 (0.460) data 0.327 (0.329) loss_x loss_x 1.0664 (1.0513) acc_x 84.3750 (76.6667) lr 4.8943e-05 eta 0:00:00
epoch [182/200] batch [5/82] time 0.465 (0.450) data 0.334 (0.319) loss_u loss_u 0.9590 (0.9624) acc_u 3.1250 (4.3750) lr 4.8943e-05 eta 0:00:34
epoch [182/200] batch [10/82] time 0.382 (0.461) data 0.251 (0.330) loss_u loss_u 0.9893 (0.9588) acc_u 0.0000 (4.3750) lr 4.8943e-05 eta 0:00:33
epoch [182/200] batch [15/82] time 0.532 (0.475) data 0.401 (0.344) loss_u loss_u 0.9507 (0.9522) acc_u 3.1250 (5.4167) lr 4.8943e-05 eta 0:00:31
epoch [182/200] batch [20/82] time 0.400 (0.465) data 0.268 (0.333) loss_u loss_u 0.9614 (0.9545) acc_u 9.3750 (5.6250) lr 4.8943e-05 eta 0:00:28
epoch [182/200] batch [25/82] time 0.460 (0.459) data 0.328 (0.327) loss_u loss_u 0.8750 (0.9530) acc_u 18.7500 (5.8750) lr 4.8943e-05 eta 0:00:26
epoch [182/200] batch [30/82] time 0.571 (0.455) data 0.440 (0.324) loss_u loss_u 0.9482 (0.9549) acc_u 6.2500 (5.4167) lr 4.8943e-05 eta 0:00:23
epoch [182/200] batch [35/82] time 0.588 (0.454) data 0.457 (0.322) loss_u loss_u 0.9829 (0.9566) acc_u 3.1250 (5.2679) lr 4.8943e-05 eta 0:00:21
epoch [182/200] batch [40/82] time 0.402 (0.450) data 0.271 (0.319) loss_u loss_u 0.9624 (0.9593) acc_u 6.2500 (4.9219) lr 4.8943e-05 eta 0:00:18
epoch [182/200] batch [45/82] time 0.481 (0.451) data 0.350 (0.319) loss_u loss_u 0.9556 (0.9603) acc_u 6.2500 (4.7917) lr 4.8943e-05 eta 0:00:16
epoch [182/200] batch [50/82] time 0.403 (0.451) data 0.272 (0.319) loss_u loss_u 0.9639 (0.9593) acc_u 6.2500 (4.9375) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [55/82] time 0.331 (0.448) data 0.201 (0.317) loss_u loss_u 0.9663 (0.9595) acc_u 3.1250 (5.0000) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [60/82] time 0.513 (0.447) data 0.382 (0.316) loss_u loss_u 0.9614 (0.9599) acc_u 6.2500 (5.0000) lr 4.8943e-05 eta 0:00:09
epoch [182/200] batch [65/82] time 0.521 (0.450) data 0.389 (0.319) loss_u loss_u 0.9644 (0.9602) acc_u 9.3750 (5.0962) lr 4.8943e-05 eta 0:00:07
epoch [182/200] batch [70/82] time 0.453 (0.451) data 0.322 (0.320) loss_u loss_u 0.9761 (0.9588) acc_u 3.1250 (5.3125) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [75/82] time 0.547 (0.453) data 0.416 (0.322) loss_u loss_u 0.9375 (0.9587) acc_u 6.2500 (5.2917) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [80/82] time 0.398 (0.453) data 0.266 (0.322) loss_u loss_u 0.9434 (0.9565) acc_u 6.2500 (5.5859) lr 4.8943e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1685
confident_label rate tensor(0.1642, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 515
clean true:491
clean false:24
clean_rate:0.9533980582524272
noisy true:960
noisy false:1661
after delete: len(clean_dataset) 515
after delete: len(noisy_dataset) 2621
epoch [183/200] batch [5/16] time 0.403 (0.431) data 0.273 (0.301) loss_x loss_x 1.0918 (1.0320) acc_x 65.6250 (75.6250) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [10/16] time 0.413 (0.437) data 0.283 (0.306) loss_x loss_x 0.8345 (0.9677) acc_x 78.1250 (76.2500) lr 4.4207e-05 eta 0:00:02
epoch [183/200] batch [15/16] time 0.705 (0.450) data 0.575 (0.319) loss_x loss_x 1.0488 (0.9759) acc_x 78.1250 (76.0417) lr 4.4207e-05 eta 0:00:00
epoch [183/200] batch [5/81] time 0.432 (0.471) data 0.300 (0.340) loss_u loss_u 0.9473 (0.9410) acc_u 6.2500 (7.5000) lr 4.4207e-05 eta 0:00:35
epoch [183/200] batch [10/81] time 0.878 (0.492) data 0.747 (0.362) loss_u loss_u 0.9561 (0.9520) acc_u 6.2500 (6.2500) lr 4.4207e-05 eta 0:00:34
epoch [183/200] batch [15/81] time 0.458 (0.484) data 0.327 (0.353) loss_u loss_u 0.9395 (0.9569) acc_u 6.2500 (5.4167) lr 4.4207e-05 eta 0:00:31
epoch [183/200] batch [20/81] time 0.485 (0.478) data 0.353 (0.347) loss_u loss_u 0.9985 (0.9576) acc_u 0.0000 (5.1562) lr 4.4207e-05 eta 0:00:29
epoch [183/200] batch [25/81] time 0.399 (0.473) data 0.267 (0.343) loss_u loss_u 0.9888 (0.9609) acc_u 0.0000 (5.0000) lr 4.4207e-05 eta 0:00:26
epoch [183/200] batch [30/81] time 0.362 (0.468) data 0.230 (0.337) loss_u loss_u 0.9941 (0.9644) acc_u 3.1250 (4.6875) lr 4.4207e-05 eta 0:00:23
epoch [183/200] batch [35/81] time 0.425 (0.469) data 0.295 (0.338) loss_u loss_u 0.9492 (0.9641) acc_u 6.2500 (4.6429) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [40/81] time 0.371 (0.464) data 0.240 (0.333) loss_u loss_u 0.9756 (0.9633) acc_u 3.1250 (4.6875) lr 4.4207e-05 eta 0:00:19
epoch [183/200] batch [45/81] time 0.386 (0.463) data 0.255 (0.332) loss_u loss_u 0.9561 (0.9611) acc_u 3.1250 (5.0000) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [50/81] time 0.489 (0.461) data 0.358 (0.331) loss_u loss_u 0.9839 (0.9606) acc_u 0.0000 (5.0625) lr 4.4207e-05 eta 0:00:14
epoch [183/200] batch [55/81] time 0.462 (0.460) data 0.331 (0.329) loss_u loss_u 0.9629 (0.9612) acc_u 6.2500 (4.9432) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [60/81] time 0.414 (0.458) data 0.284 (0.327) loss_u loss_u 0.9653 (0.9616) acc_u 3.1250 (4.8438) lr 4.4207e-05 eta 0:00:09
epoch [183/200] batch [65/81] time 0.487 (0.464) data 0.356 (0.333) loss_u loss_u 0.9648 (0.9631) acc_u 3.1250 (4.6154) lr 4.4207e-05 eta 0:00:07
epoch [183/200] batch [70/81] time 0.413 (0.462) data 0.282 (0.331) loss_u loss_u 0.9106 (0.9627) acc_u 9.3750 (4.6875) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [75/81] time 0.409 (0.459) data 0.279 (0.329) loss_u loss_u 0.9814 (0.9632) acc_u 3.1250 (4.6250) lr 4.4207e-05 eta 0:00:02
epoch [183/200] batch [80/81] time 0.575 (0.460) data 0.444 (0.329) loss_u loss_u 0.9927 (0.9636) acc_u 0.0000 (4.5703) lr 4.4207e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1741
confident_label rate tensor(0.1591, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 499
clean true:483
clean false:16
clean_rate:0.9679358717434869
noisy true:912
noisy false:1725
after delete: len(clean_dataset) 499
after delete: len(noisy_dataset) 2637
epoch [184/200] batch [5/15] time 0.540 (0.480) data 0.410 (0.350) loss_x loss_x 0.9512 (0.9132) acc_x 75.0000 (78.1250) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [10/15] time 0.527 (0.485) data 0.397 (0.354) loss_x loss_x 0.9604 (1.0396) acc_x 78.1250 (75.6250) lr 3.9706e-05 eta 0:00:02
epoch [184/200] batch [15/15] time 0.434 (0.477) data 0.303 (0.346) loss_x loss_x 1.3838 (1.0498) acc_x 71.8750 (75.6250) lr 3.9706e-05 eta 0:00:00
epoch [184/200] batch [5/82] time 0.575 (0.467) data 0.444 (0.336) loss_u loss_u 0.9658 (0.9609) acc_u 3.1250 (5.0000) lr 3.9706e-05 eta 0:00:35
epoch [184/200] batch [10/82] time 0.397 (0.462) data 0.266 (0.332) loss_u loss_u 0.9917 (0.9461) acc_u 3.1250 (6.8750) lr 3.9706e-05 eta 0:00:33
epoch [184/200] batch [15/82] time 0.411 (0.461) data 0.281 (0.330) loss_u loss_u 0.9868 (0.9520) acc_u 3.1250 (6.4583) lr 3.9706e-05 eta 0:00:30
epoch [184/200] batch [20/82] time 0.453 (0.462) data 0.321 (0.331) loss_u loss_u 0.9321 (0.9527) acc_u 12.5000 (6.5625) lr 3.9706e-05 eta 0:00:28
epoch [184/200] batch [25/82] time 0.480 (0.460) data 0.348 (0.329) loss_u loss_u 0.9106 (0.9493) acc_u 9.3750 (6.7500) lr 3.9706e-05 eta 0:00:26
epoch [184/200] batch [30/82] time 0.407 (0.458) data 0.275 (0.327) loss_u loss_u 0.9976 (0.9513) acc_u 0.0000 (6.4583) lr 3.9706e-05 eta 0:00:23
epoch [184/200] batch [35/82] time 0.514 (0.454) data 0.383 (0.323) loss_u loss_u 0.9893 (0.9555) acc_u 0.0000 (5.8929) lr 3.9706e-05 eta 0:00:21
epoch [184/200] batch [40/82] time 0.455 (0.455) data 0.323 (0.325) loss_u loss_u 0.8975 (0.9563) acc_u 9.3750 (5.7031) lr 3.9706e-05 eta 0:00:19
epoch [184/200] batch [45/82] time 0.453 (0.454) data 0.321 (0.323) loss_u loss_u 0.9678 (0.9558) acc_u 3.1250 (5.6944) lr 3.9706e-05 eta 0:00:16
epoch [184/200] batch [50/82] time 0.364 (0.455) data 0.232 (0.324) loss_u loss_u 0.9238 (0.9564) acc_u 9.3750 (5.5000) lr 3.9706e-05 eta 0:00:14
epoch [184/200] batch [55/82] time 0.518 (0.457) data 0.387 (0.326) loss_u loss_u 0.9810 (0.9580) acc_u 3.1250 (5.3977) lr 3.9706e-05 eta 0:00:12
epoch [184/200] batch [60/82] time 0.506 (0.454) data 0.376 (0.323) loss_u loss_u 0.9419 (0.9577) acc_u 12.5000 (5.5729) lr 3.9706e-05 eta 0:00:09
epoch [184/200] batch [65/82] time 0.485 (0.458) data 0.353 (0.327) loss_u loss_u 0.9712 (0.9573) acc_u 6.2500 (5.6250) lr 3.9706e-05 eta 0:00:07
epoch [184/200] batch [70/82] time 0.421 (0.463) data 0.291 (0.332) loss_u loss_u 0.9321 (0.9576) acc_u 9.3750 (5.5804) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [75/82] time 0.399 (0.462) data 0.269 (0.331) loss_u loss_u 0.9868 (0.9587) acc_u 0.0000 (5.4167) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [80/82] time 0.492 (0.466) data 0.362 (0.335) loss_u loss_u 0.9951 (0.9587) acc_u 0.0000 (5.3516) lr 3.9706e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1722
confident_label rate tensor(0.1620, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 508
clean true:491
clean false:17
clean_rate:0.9665354330708661
noisy true:923
noisy false:1705
after delete: len(clean_dataset) 508
after delete: len(noisy_dataset) 2628
epoch [185/200] batch [5/15] time 0.467 (0.494) data 0.337 (0.364) loss_x loss_x 1.2939 (1.0469) acc_x 68.7500 (75.0000) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [10/15] time 0.612 (0.479) data 0.482 (0.348) loss_x loss_x 0.9043 (1.0395) acc_x 78.1250 (75.9375) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [15/15] time 0.375 (0.458) data 0.245 (0.328) loss_x loss_x 1.0029 (1.0560) acc_x 71.8750 (75.0000) lr 3.5443e-05 eta 0:00:00
epoch [185/200] batch [5/82] time 0.352 (0.461) data 0.222 (0.331) loss_u loss_u 0.9946 (0.9778) acc_u 0.0000 (2.5000) lr 3.5443e-05 eta 0:00:35
epoch [185/200] batch [10/82] time 0.581 (0.461) data 0.450 (0.331) loss_u loss_u 0.9526 (0.9683) acc_u 6.2500 (3.4375) lr 3.5443e-05 eta 0:00:33
epoch [185/200] batch [15/82] time 0.431 (0.457) data 0.299 (0.327) loss_u loss_u 0.9468 (0.9661) acc_u 6.2500 (3.9583) lr 3.5443e-05 eta 0:00:30
epoch [185/200] batch [20/82] time 0.598 (0.463) data 0.466 (0.332) loss_u loss_u 0.9980 (0.9615) acc_u 0.0000 (4.8438) lr 3.5443e-05 eta 0:00:28
epoch [185/200] batch [25/82] time 0.393 (0.467) data 0.262 (0.337) loss_u loss_u 0.9771 (0.9596) acc_u 0.0000 (5.0000) lr 3.5443e-05 eta 0:00:26
epoch [185/200] batch [30/82] time 0.357 (0.465) data 0.225 (0.335) loss_u loss_u 0.9810 (0.9597) acc_u 3.1250 (4.8958) lr 3.5443e-05 eta 0:00:24
epoch [185/200] batch [35/82] time 0.415 (0.462) data 0.283 (0.331) loss_u loss_u 0.9312 (0.9600) acc_u 6.2500 (4.8214) lr 3.5443e-05 eta 0:00:21
epoch [185/200] batch [40/82] time 0.443 (0.464) data 0.312 (0.333) loss_u loss_u 0.9678 (0.9598) acc_u 3.1250 (4.8438) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [45/82] time 0.566 (0.464) data 0.436 (0.334) loss_u loss_u 0.9316 (0.9572) acc_u 9.3750 (5.2083) lr 3.5443e-05 eta 0:00:17
epoch [185/200] batch [50/82] time 0.429 (0.466) data 0.298 (0.335) loss_u loss_u 0.9863 (0.9592) acc_u 0.0000 (4.8750) lr 3.5443e-05 eta 0:00:14
epoch [185/200] batch [55/82] time 0.587 (0.464) data 0.456 (0.333) loss_u loss_u 0.9253 (0.9584) acc_u 9.3750 (5.0000) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [60/82] time 0.389 (0.462) data 0.259 (0.331) loss_u loss_u 0.9917 (0.9605) acc_u 0.0000 (4.7917) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [65/82] time 0.411 (0.466) data 0.279 (0.335) loss_u loss_u 0.9844 (0.9622) acc_u 0.0000 (4.5673) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [70/82] time 0.382 (0.464) data 0.252 (0.333) loss_u loss_u 0.9614 (0.9620) acc_u 6.2500 (4.6875) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [75/82] time 0.426 (0.466) data 0.295 (0.335) loss_u loss_u 0.9673 (0.9628) acc_u 3.1250 (4.5417) lr 3.5443e-05 eta 0:00:03
epoch [185/200] batch [80/82] time 0.394 (0.462) data 0.264 (0.331) loss_u loss_u 0.9507 (0.9632) acc_u 6.2500 (4.5312) lr 3.5443e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1654
confident_label rate tensor(0.1693, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 531
clean true:513
clean false:18
clean_rate:0.9661016949152542
noisy true:969
noisy false:1636
after delete: len(clean_dataset) 531
after delete: len(noisy_dataset) 2605
epoch [186/200] batch [5/16] time 0.475 (0.507) data 0.344 (0.377) loss_x loss_x 1.4697 (1.1900) acc_x 71.8750 (76.2500) lr 3.1417e-05 eta 0:00:05
epoch [186/200] batch [10/16] time 0.390 (0.489) data 0.260 (0.359) loss_x loss_x 0.9033 (1.1316) acc_x 81.2500 (76.2500) lr 3.1417e-05 eta 0:00:02
epoch [186/200] batch [15/16] time 0.543 (0.496) data 0.413 (0.366) loss_x loss_x 0.7783 (1.0744) acc_x 84.3750 (76.2500) lr 3.1417e-05 eta 0:00:00
epoch [186/200] batch [5/81] time 0.480 (0.491) data 0.349 (0.361) loss_u loss_u 0.9521 (0.9490) acc_u 3.1250 (5.0000) lr 3.1417e-05 eta 0:00:37
epoch [186/200] batch [10/81] time 0.554 (0.478) data 0.423 (0.348) loss_u loss_u 0.9976 (0.9611) acc_u 0.0000 (4.0625) lr 3.1417e-05 eta 0:00:33
epoch [186/200] batch [15/81] time 0.491 (0.474) data 0.361 (0.344) loss_u loss_u 0.9517 (0.9593) acc_u 3.1250 (4.7917) lr 3.1417e-05 eta 0:00:31
epoch [186/200] batch [20/81] time 0.416 (0.467) data 0.285 (0.337) loss_u loss_u 0.9346 (0.9601) acc_u 9.3750 (4.6875) lr 3.1417e-05 eta 0:00:28
epoch [186/200] batch [25/81] time 0.435 (0.463) data 0.305 (0.333) loss_u loss_u 0.9746 (0.9576) acc_u 3.1250 (5.1250) lr 3.1417e-05 eta 0:00:25
epoch [186/200] batch [30/81] time 0.362 (0.459) data 0.231 (0.328) loss_u loss_u 0.9297 (0.9574) acc_u 6.2500 (5.0000) lr 3.1417e-05 eta 0:00:23
epoch [186/200] batch [35/81] time 0.513 (0.458) data 0.381 (0.328) loss_u loss_u 0.9937 (0.9590) acc_u 0.0000 (4.7321) lr 3.1417e-05 eta 0:00:21
epoch [186/200] batch [40/81] time 0.525 (0.461) data 0.394 (0.330) loss_u loss_u 0.9771 (0.9598) acc_u 3.1250 (4.6875) lr 3.1417e-05 eta 0:00:18
epoch [186/200] batch [45/81] time 0.392 (0.463) data 0.262 (0.332) loss_u loss_u 0.9663 (0.9611) acc_u 3.1250 (4.5139) lr 3.1417e-05 eta 0:00:16
epoch [186/200] batch [50/81] time 0.399 (0.468) data 0.267 (0.338) loss_u loss_u 0.9414 (0.9604) acc_u 6.2500 (4.6250) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [55/81] time 0.561 (0.473) data 0.430 (0.342) loss_u loss_u 0.8760 (0.9592) acc_u 15.6250 (4.9432) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [60/81] time 0.653 (0.477) data 0.521 (0.346) loss_u loss_u 0.9946 (0.9607) acc_u 0.0000 (4.7396) lr 3.1417e-05 eta 0:00:10
epoch [186/200] batch [65/81] time 0.438 (0.477) data 0.307 (0.347) loss_u loss_u 0.9644 (0.9608) acc_u 3.1250 (4.7115) lr 3.1417e-05 eta 0:00:07
epoch [186/200] batch [70/81] time 0.387 (0.478) data 0.257 (0.347) loss_u loss_u 0.9512 (0.9613) acc_u 6.2500 (4.6429) lr 3.1417e-05 eta 0:00:05
epoch [186/200] batch [75/81] time 0.427 (0.477) data 0.297 (0.346) loss_u loss_u 0.9517 (0.9617) acc_u 6.2500 (4.5833) lr 3.1417e-05 eta 0:00:02
epoch [186/200] batch [80/81] time 0.596 (0.481) data 0.464 (0.350) loss_u loss_u 0.9590 (0.9626) acc_u 3.1250 (4.4531) lr 3.1417e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1671
confident_label rate tensor(0.1712, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 537
clean true:508
clean false:29
clean_rate:0.9459962756052142
noisy true:957
noisy false:1642
after delete: len(clean_dataset) 537
after delete: len(noisy_dataset) 2599
epoch [187/200] batch [5/16] time 0.463 (0.468) data 0.332 (0.337) loss_x loss_x 1.2900 (1.1271) acc_x 78.1250 (74.3750) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [10/16] time 0.370 (0.483) data 0.239 (0.352) loss_x loss_x 0.8560 (1.1180) acc_x 87.5000 (74.3750) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [15/16] time 0.508 (0.491) data 0.378 (0.361) loss_x loss_x 1.1289 (1.1765) acc_x 62.5000 (71.0417) lr 2.7630e-05 eta 0:00:00
epoch [187/200] batch [5/81] time 0.457 (0.488) data 0.325 (0.357) loss_u loss_u 0.9932 (0.9821) acc_u 0.0000 (1.2500) lr 2.7630e-05 eta 0:00:37
epoch [187/200] batch [10/81] time 0.457 (0.476) data 0.325 (0.345) loss_u loss_u 0.9565 (0.9657) acc_u 3.1250 (3.1250) lr 2.7630e-05 eta 0:00:33
epoch [187/200] batch [15/81] time 0.550 (0.474) data 0.419 (0.343) loss_u loss_u 0.9653 (0.9646) acc_u 3.1250 (3.7500) lr 2.7630e-05 eta 0:00:31
epoch [187/200] batch [20/81] time 0.376 (0.465) data 0.244 (0.334) loss_u loss_u 0.9668 (0.9618) acc_u 6.2500 (4.5312) lr 2.7630e-05 eta 0:00:28
epoch [187/200] batch [25/81] time 0.353 (0.462) data 0.221 (0.331) loss_u loss_u 0.9722 (0.9614) acc_u 3.1250 (4.7500) lr 2.7630e-05 eta 0:00:25
epoch [187/200] batch [30/81] time 0.378 (0.461) data 0.246 (0.330) loss_u loss_u 0.9927 (0.9595) acc_u 3.1250 (5.1042) lr 2.7630e-05 eta 0:00:23
epoch [187/200] batch [35/81] time 0.440 (0.462) data 0.308 (0.331) loss_u loss_u 0.9800 (0.9618) acc_u 6.2500 (4.8214) lr 2.7630e-05 eta 0:00:21
epoch [187/200] batch [40/81] time 0.391 (0.456) data 0.260 (0.325) loss_u loss_u 0.9951 (0.9646) acc_u 0.0000 (4.3750) lr 2.7630e-05 eta 0:00:18
epoch [187/200] batch [45/81] time 0.548 (0.458) data 0.418 (0.327) loss_u loss_u 0.9697 (0.9641) acc_u 3.1250 (4.6528) lr 2.7630e-05 eta 0:00:16
epoch [187/200] batch [50/81] time 0.378 (0.458) data 0.246 (0.327) loss_u loss_u 1.0000 (0.9650) acc_u 0.0000 (4.5000) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [55/81] time 0.615 (0.468) data 0.484 (0.337) loss_u loss_u 0.9316 (0.9651) acc_u 9.3750 (4.5455) lr 2.7630e-05 eta 0:00:12
epoch [187/200] batch [60/81] time 0.400 (0.468) data 0.270 (0.337) loss_u loss_u 0.9585 (0.9634) acc_u 3.1250 (4.7917) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [65/81] time 0.461 (0.471) data 0.330 (0.339) loss_u loss_u 0.8691 (0.9623) acc_u 18.7500 (4.9519) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [70/81] time 0.368 (0.467) data 0.236 (0.336) loss_u loss_u 0.9937 (0.9634) acc_u 0.0000 (4.7768) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [75/81] time 0.458 (0.468) data 0.328 (0.337) loss_u loss_u 0.9702 (0.9627) acc_u 6.2500 (4.8333) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [80/81] time 0.380 (0.467) data 0.249 (0.336) loss_u loss_u 0.8315 (0.9620) acc_u 21.8750 (4.9219) lr 2.7630e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1689
confident_label rate tensor(0.1677, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 526
clean true:505
clean false:21
clean_rate:0.9600760456273765
noisy true:942
noisy false:1668
after delete: len(clean_dataset) 526
after delete: len(noisy_dataset) 2610
epoch [188/200] batch [5/16] time 0.438 (0.451) data 0.308 (0.320) loss_x loss_x 1.2070 (1.0725) acc_x 68.7500 (71.2500) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [10/16] time 0.453 (0.453) data 0.323 (0.323) loss_x loss_x 0.7905 (1.0082) acc_x 78.1250 (75.0000) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [15/16] time 0.520 (0.456) data 0.390 (0.326) loss_x loss_x 0.8481 (1.0363) acc_x 84.3750 (75.2083) lr 2.4083e-05 eta 0:00:00
epoch [188/200] batch [5/81] time 0.514 (0.449) data 0.382 (0.318) loss_u loss_u 0.9766 (0.9449) acc_u 3.1250 (7.5000) lr 2.4083e-05 eta 0:00:34
epoch [188/200] batch [10/81] time 0.661 (0.452) data 0.529 (0.321) loss_u loss_u 0.9995 (0.9525) acc_u 0.0000 (6.2500) lr 2.4083e-05 eta 0:00:32
epoch [188/200] batch [15/81] time 0.416 (0.452) data 0.285 (0.322) loss_u loss_u 0.9575 (0.9589) acc_u 6.2500 (5.4167) lr 2.4083e-05 eta 0:00:29
epoch [188/200] batch [20/81] time 0.517 (0.457) data 0.386 (0.326) loss_u loss_u 0.9424 (0.9569) acc_u 6.2500 (5.6250) lr 2.4083e-05 eta 0:00:27
epoch [188/200] batch [25/81] time 0.454 (0.456) data 0.324 (0.325) loss_u loss_u 0.9946 (0.9591) acc_u 0.0000 (5.3750) lr 2.4083e-05 eta 0:00:25
epoch [188/200] batch [30/81] time 0.516 (0.454) data 0.386 (0.323) loss_u loss_u 0.9731 (0.9582) acc_u 3.1250 (5.3125) lr 2.4083e-05 eta 0:00:23
epoch [188/200] batch [35/81] time 0.417 (0.455) data 0.287 (0.324) loss_u loss_u 0.9551 (0.9582) acc_u 3.1250 (5.2679) lr 2.4083e-05 eta 0:00:20
epoch [188/200] batch [40/81] time 0.382 (0.451) data 0.251 (0.320) loss_u loss_u 0.9966 (0.9610) acc_u 0.0000 (5.0000) lr 2.4083e-05 eta 0:00:18
epoch [188/200] batch [45/81] time 0.432 (0.451) data 0.300 (0.321) loss_u loss_u 0.9482 (0.9602) acc_u 6.2500 (5.0694) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [50/81] time 0.524 (0.459) data 0.393 (0.328) loss_u loss_u 0.9692 (0.9586) acc_u 3.1250 (5.2500) lr 2.4083e-05 eta 0:00:14
epoch [188/200] batch [55/81] time 0.445 (0.459) data 0.314 (0.329) loss_u loss_u 0.9746 (0.9580) acc_u 3.1250 (5.2841) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [60/81] time 0.341 (0.460) data 0.211 (0.329) loss_u loss_u 0.8853 (0.9568) acc_u 12.5000 (5.3646) lr 2.4083e-05 eta 0:00:09
epoch [188/200] batch [65/81] time 0.521 (0.463) data 0.390 (0.332) loss_u loss_u 0.9985 (0.9581) acc_u 0.0000 (5.2404) lr 2.4083e-05 eta 0:00:07
epoch [188/200] batch [70/81] time 0.408 (0.464) data 0.276 (0.333) loss_u loss_u 0.9546 (0.9588) acc_u 3.1250 (5.0893) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [75/81] time 0.409 (0.462) data 0.277 (0.331) loss_u loss_u 0.9917 (0.9601) acc_u 0.0000 (4.9167) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [80/81] time 0.434 (0.462) data 0.302 (0.331) loss_u loss_u 0.9961 (0.9607) acc_u 0.0000 (4.8047) lr 2.4083e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1728
confident_label rate tensor(0.1636, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 513
clean true:498
clean false:15
clean_rate:0.9707602339181286
noisy true:910
noisy false:1713
after delete: len(clean_dataset) 513
after delete: len(noisy_dataset) 2623
epoch [189/200] batch [5/16] time 0.488 (0.439) data 0.358 (0.309) loss_x loss_x 0.6060 (0.8539) acc_x 84.3750 (78.7500) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [10/16] time 0.416 (0.462) data 0.287 (0.332) loss_x loss_x 1.2012 (0.8740) acc_x 65.6250 (78.4375) lr 2.0777e-05 eta 0:00:02
epoch [189/200] batch [15/16] time 0.559 (0.469) data 0.429 (0.339) loss_x loss_x 0.8623 (0.9118) acc_x 78.1250 (78.1250) lr 2.0777e-05 eta 0:00:00
epoch [189/200] batch [5/81] time 0.552 (0.465) data 0.422 (0.334) loss_u loss_u 0.9790 (0.9750) acc_u 3.1250 (3.1250) lr 2.0777e-05 eta 0:00:35
epoch [189/200] batch [10/81] time 0.387 (0.459) data 0.257 (0.329) loss_u loss_u 0.9165 (0.9587) acc_u 12.5000 (6.2500) lr 2.0777e-05 eta 0:00:32
epoch [189/200] batch [15/81] time 0.445 (0.454) data 0.314 (0.324) loss_u loss_u 0.9712 (0.9614) acc_u 3.1250 (5.6250) lr 2.0777e-05 eta 0:00:29
epoch [189/200] batch [20/81] time 0.412 (0.461) data 0.280 (0.331) loss_u loss_u 0.9814 (0.9634) acc_u 3.1250 (5.3125) lr 2.0777e-05 eta 0:00:28
epoch [189/200] batch [25/81] time 0.482 (0.457) data 0.352 (0.327) loss_u loss_u 0.9409 (0.9639) acc_u 6.2500 (5.0000) lr 2.0777e-05 eta 0:00:25
epoch [189/200] batch [30/81] time 0.374 (0.458) data 0.244 (0.328) loss_u loss_u 0.9985 (0.9661) acc_u 0.0000 (4.8958) lr 2.0777e-05 eta 0:00:23
epoch [189/200] batch [35/81] time 0.612 (0.465) data 0.481 (0.334) loss_u loss_u 0.9883 (0.9667) acc_u 3.1250 (4.7321) lr 2.0777e-05 eta 0:00:21
epoch [189/200] batch [40/81] time 0.385 (0.466) data 0.254 (0.336) loss_u loss_u 0.9888 (0.9654) acc_u 0.0000 (4.6875) lr 2.0777e-05 eta 0:00:19
epoch [189/200] batch [45/81] time 0.499 (0.473) data 0.368 (0.342) loss_u loss_u 0.9683 (0.9654) acc_u 9.3750 (4.7222) lr 2.0777e-05 eta 0:00:17
epoch [189/200] batch [50/81] time 0.349 (0.471) data 0.218 (0.340) loss_u loss_u 0.9888 (0.9652) acc_u 0.0000 (4.7500) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [55/81] time 0.400 (0.466) data 0.269 (0.335) loss_u loss_u 0.9819 (0.9653) acc_u 3.1250 (4.7159) lr 2.0777e-05 eta 0:00:12
epoch [189/200] batch [60/81] time 0.596 (0.468) data 0.464 (0.338) loss_u loss_u 0.9355 (0.9645) acc_u 6.2500 (4.8958) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [65/81] time 0.419 (0.470) data 0.288 (0.339) loss_u loss_u 0.9956 (0.9644) acc_u 0.0000 (4.8077) lr 2.0777e-05 eta 0:00:07
epoch [189/200] batch [70/81] time 0.480 (0.469) data 0.349 (0.338) loss_u loss_u 0.9824 (0.9643) acc_u 3.1250 (4.8214) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [75/81] time 0.398 (0.467) data 0.268 (0.336) loss_u loss_u 0.9932 (0.9650) acc_u 0.0000 (4.7500) lr 2.0777e-05 eta 0:00:02
epoch [189/200] batch [80/81] time 0.356 (0.464) data 0.225 (0.334) loss_u loss_u 0.9570 (0.9646) acc_u 6.2500 (4.7266) lr 2.0777e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1680
confident_label rate tensor(0.1677, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 526
clean true:503
clean false:23
clean_rate:0.9562737642585551
noisy true:953
noisy false:1657
after delete: len(clean_dataset) 526
after delete: len(noisy_dataset) 2610
epoch [190/200] batch [5/16] time 0.365 (0.493) data 0.234 (0.362) loss_x loss_x 1.1953 (0.9536) acc_x 75.0000 (74.3750) lr 1.7713e-05 eta 0:00:05
epoch [190/200] batch [10/16] time 0.531 (0.509) data 0.400 (0.378) loss_x loss_x 1.1445 (1.0050) acc_x 78.1250 (75.9375) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [15/16] time 0.418 (0.489) data 0.287 (0.358) loss_x loss_x 0.7134 (1.0070) acc_x 90.6250 (75.6250) lr 1.7713e-05 eta 0:00:00
epoch [190/200] batch [5/81] time 0.381 (0.472) data 0.250 (0.341) loss_u loss_u 0.9009 (0.9563) acc_u 12.5000 (5.6250) lr 1.7713e-05 eta 0:00:35
epoch [190/200] batch [10/81] time 0.434 (0.470) data 0.303 (0.339) loss_u loss_u 0.9497 (0.9604) acc_u 6.2500 (5.0000) lr 1.7713e-05 eta 0:00:33
epoch [190/200] batch [15/81] time 0.498 (0.469) data 0.367 (0.338) loss_u loss_u 0.9385 (0.9607) acc_u 6.2500 (4.5833) lr 1.7713e-05 eta 0:00:30
epoch [190/200] batch [20/81] time 0.517 (0.474) data 0.386 (0.343) loss_u loss_u 0.9473 (0.9600) acc_u 6.2500 (4.6875) lr 1.7713e-05 eta 0:00:28
epoch [190/200] batch [25/81] time 0.384 (0.476) data 0.254 (0.345) loss_u loss_u 0.9561 (0.9633) acc_u 3.1250 (4.1250) lr 1.7713e-05 eta 0:00:26
epoch [190/200] batch [30/81] time 0.525 (0.480) data 0.395 (0.350) loss_u loss_u 0.9971 (0.9645) acc_u 0.0000 (4.0625) lr 1.7713e-05 eta 0:00:24
epoch [190/200] batch [35/81] time 0.467 (0.477) data 0.337 (0.346) loss_u loss_u 0.9624 (0.9648) acc_u 3.1250 (3.9286) lr 1.7713e-05 eta 0:00:21
epoch [190/200] batch [40/81] time 0.392 (0.471) data 0.261 (0.341) loss_u loss_u 0.9512 (0.9655) acc_u 6.2500 (3.7500) lr 1.7713e-05 eta 0:00:19
epoch [190/200] batch [45/81] time 0.359 (0.472) data 0.227 (0.341) loss_u loss_u 0.9834 (0.9642) acc_u 3.1250 (3.9583) lr 1.7713e-05 eta 0:00:16
epoch [190/200] batch [50/81] time 0.442 (0.469) data 0.310 (0.338) loss_u loss_u 0.9268 (0.9640) acc_u 12.5000 (4.0625) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [55/81] time 0.655 (0.472) data 0.525 (0.341) loss_u loss_u 0.9570 (0.9644) acc_u 6.2500 (4.0909) lr 1.7713e-05 eta 0:00:12
epoch [190/200] batch [60/81] time 0.433 (0.469) data 0.302 (0.338) loss_u loss_u 0.9419 (0.9631) acc_u 6.2500 (4.2708) lr 1.7713e-05 eta 0:00:09
epoch [190/200] batch [65/81] time 0.509 (0.470) data 0.378 (0.339) loss_u loss_u 0.9390 (0.9620) acc_u 6.2500 (4.3750) lr 1.7713e-05 eta 0:00:07
epoch [190/200] batch [70/81] time 0.444 (0.471) data 0.313 (0.340) loss_u loss_u 0.9663 (0.9613) acc_u 3.1250 (4.4643) lr 1.7713e-05 eta 0:00:05
epoch [190/200] batch [75/81] time 0.585 (0.471) data 0.454 (0.340) loss_u loss_u 0.9521 (0.9614) acc_u 6.2500 (4.5000) lr 1.7713e-05 eta 0:00:02
epoch [190/200] batch [80/81] time 0.556 (0.472) data 0.425 (0.341) loss_u loss_u 0.9478 (0.9612) acc_u 6.2500 (4.5312) lr 1.7713e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1711
confident_label rate tensor(0.1614, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 506
clean true:486
clean false:20
clean_rate:0.9604743083003953
noisy true:939
noisy false:1691
after delete: len(clean_dataset) 506
after delete: len(noisy_dataset) 2630
epoch [191/200] batch [5/15] time 0.554 (0.496) data 0.423 (0.366) loss_x loss_x 1.0439 (0.9315) acc_x 75.0000 (76.8750) lr 1.4891e-05 eta 0:00:04
epoch [191/200] batch [10/15] time 0.381 (0.490) data 0.251 (0.360) loss_x loss_x 1.1768 (0.9286) acc_x 75.0000 (77.1875) lr 1.4891e-05 eta 0:00:02
epoch [191/200] batch [15/15] time 0.587 (0.495) data 0.456 (0.365) loss_x loss_x 1.1123 (1.0163) acc_x 78.1250 (75.6250) lr 1.4891e-05 eta 0:00:00
epoch [191/200] batch [5/82] time 0.362 (0.499) data 0.231 (0.368) loss_u loss_u 0.8979 (0.9395) acc_u 12.5000 (7.5000) lr 1.4891e-05 eta 0:00:38
epoch [191/200] batch [10/82] time 0.380 (0.490) data 0.248 (0.359) loss_u loss_u 0.9990 (0.9518) acc_u 0.0000 (6.2500) lr 1.4891e-05 eta 0:00:35
epoch [191/200] batch [15/82] time 0.440 (0.485) data 0.309 (0.354) loss_u loss_u 0.9385 (0.9498) acc_u 6.2500 (6.2500) lr 1.4891e-05 eta 0:00:32
epoch [191/200] batch [20/82] time 0.416 (0.490) data 0.286 (0.360) loss_u loss_u 0.9180 (0.9524) acc_u 9.3750 (5.9375) lr 1.4891e-05 eta 0:00:30
epoch [191/200] batch [25/82] time 0.506 (0.483) data 0.376 (0.352) loss_u loss_u 0.9175 (0.9548) acc_u 9.3750 (5.7500) lr 1.4891e-05 eta 0:00:27
epoch [191/200] batch [30/82] time 0.395 (0.477) data 0.263 (0.346) loss_u loss_u 0.9634 (0.9541) acc_u 6.2500 (6.0417) lr 1.4891e-05 eta 0:00:24
epoch [191/200] batch [35/82] time 0.598 (0.480) data 0.467 (0.349) loss_u loss_u 0.9819 (0.9545) acc_u 0.0000 (5.8929) lr 1.4891e-05 eta 0:00:22
epoch [191/200] batch [40/82] time 0.580 (0.479) data 0.449 (0.348) loss_u loss_u 0.9688 (0.9557) acc_u 6.2500 (5.8594) lr 1.4891e-05 eta 0:00:20
epoch [191/200] batch [45/82] time 0.494 (0.476) data 0.363 (0.346) loss_u loss_u 0.9341 (0.9533) acc_u 9.3750 (6.0417) lr 1.4891e-05 eta 0:00:17
epoch [191/200] batch [50/82] time 0.461 (0.471) data 0.330 (0.340) loss_u loss_u 0.9678 (0.9528) acc_u 6.2500 (6.1875) lr 1.4891e-05 eta 0:00:15
epoch [191/200] batch [55/82] time 0.612 (0.473) data 0.482 (0.342) loss_u loss_u 0.9707 (0.9533) acc_u 3.1250 (6.1932) lr 1.4891e-05 eta 0:00:12
epoch [191/200] batch [60/82] time 0.524 (0.470) data 0.393 (0.339) loss_u loss_u 0.9678 (0.9555) acc_u 6.2500 (5.9896) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [65/82] time 0.389 (0.469) data 0.258 (0.338) loss_u loss_u 0.9653 (0.9557) acc_u 3.1250 (5.8654) lr 1.4891e-05 eta 0:00:07
epoch [191/200] batch [70/82] time 0.341 (0.463) data 0.211 (0.332) loss_u loss_u 0.9570 (0.9566) acc_u 6.2500 (5.6696) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [75/82] time 0.356 (0.460) data 0.225 (0.329) loss_u loss_u 0.9785 (0.9579) acc_u 3.1250 (5.5417) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [80/82] time 0.361 (0.459) data 0.231 (0.328) loss_u loss_u 0.9082 (0.9581) acc_u 12.5000 (5.5469) lr 1.4891e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1719
confident_label rate tensor(0.1633, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 512
clean true:491
clean false:21
clean_rate:0.958984375
noisy true:926
noisy false:1698
after delete: len(clean_dataset) 512
after delete: len(noisy_dataset) 2624
epoch [192/200] batch [5/16] time 0.375 (0.449) data 0.244 (0.318) loss_x loss_x 1.0498 (0.9226) acc_x 75.0000 (75.6250) lr 1.2312e-05 eta 0:00:04
epoch [192/200] batch [10/16] time 0.562 (0.465) data 0.432 (0.334) loss_x loss_x 1.1641 (0.9822) acc_x 68.7500 (75.6250) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [15/16] time 0.504 (0.465) data 0.375 (0.335) loss_x loss_x 1.5361 (0.9952) acc_x 71.8750 (77.0833) lr 1.2312e-05 eta 0:00:00
epoch [192/200] batch [5/82] time 0.377 (0.459) data 0.247 (0.328) loss_u loss_u 0.9746 (0.9629) acc_u 3.1250 (5.6250) lr 1.2312e-05 eta 0:00:35
epoch [192/200] batch [10/82] time 0.457 (0.469) data 0.326 (0.338) loss_u loss_u 0.9600 (0.9640) acc_u 3.1250 (5.0000) lr 1.2312e-05 eta 0:00:33
epoch [192/200] batch [15/82] time 0.540 (0.474) data 0.409 (0.344) loss_u loss_u 0.9507 (0.9647) acc_u 6.2500 (4.5833) lr 1.2312e-05 eta 0:00:31
epoch [192/200] batch [20/82] time 0.364 (0.474) data 0.233 (0.344) loss_u loss_u 0.9775 (0.9608) acc_u 3.1250 (5.3125) lr 1.2312e-05 eta 0:00:29
epoch [192/200] batch [25/82] time 0.377 (0.472) data 0.246 (0.341) loss_u loss_u 0.9834 (0.9548) acc_u 3.1250 (5.8750) lr 1.2312e-05 eta 0:00:26
epoch [192/200] batch [30/82] time 0.397 (0.465) data 0.267 (0.334) loss_u loss_u 0.9980 (0.9543) acc_u 0.0000 (6.1458) lr 1.2312e-05 eta 0:00:24
epoch [192/200] batch [35/82] time 0.368 (0.462) data 0.238 (0.331) loss_u loss_u 0.9883 (0.9565) acc_u 0.0000 (5.8036) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [40/82] time 0.549 (0.460) data 0.419 (0.329) loss_u loss_u 0.9893 (0.9565) acc_u 0.0000 (5.7812) lr 1.2312e-05 eta 0:00:19
epoch [192/200] batch [45/82] time 0.385 (0.463) data 0.255 (0.332) loss_u loss_u 0.9795 (0.9574) acc_u 3.1250 (5.6944) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [50/82] time 0.384 (0.459) data 0.252 (0.328) loss_u loss_u 0.9307 (0.9587) acc_u 6.2500 (5.5000) lr 1.2312e-05 eta 0:00:14
epoch [192/200] batch [55/82] time 0.395 (0.457) data 0.265 (0.326) loss_u loss_u 0.9712 (0.9603) acc_u 6.2500 (5.3409) lr 1.2312e-05 eta 0:00:12
epoch [192/200] batch [60/82] time 0.487 (0.458) data 0.356 (0.327) loss_u loss_u 0.9800 (0.9607) acc_u 3.1250 (5.2083) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [65/82] time 0.357 (0.456) data 0.227 (0.325) loss_u loss_u 0.9551 (0.9599) acc_u 6.2500 (5.3365) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [70/82] time 0.533 (0.458) data 0.402 (0.327) loss_u loss_u 0.9297 (0.9601) acc_u 6.2500 (5.2232) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [75/82] time 0.812 (0.462) data 0.682 (0.332) loss_u loss_u 0.9160 (0.9588) acc_u 6.2500 (5.3750) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [80/82] time 0.452 (0.461) data 0.321 (0.331) loss_u loss_u 0.9556 (0.9593) acc_u 6.2500 (5.3516) lr 1.2312e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1676
confident_label rate tensor(0.1687, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 529
clean true:510
clean false:19
clean_rate:0.9640831758034026
noisy true:950
noisy false:1657
after delete: len(clean_dataset) 529
after delete: len(noisy_dataset) 2607
epoch [193/200] batch [5/16] time 0.494 (0.455) data 0.365 (0.325) loss_x loss_x 0.8247 (0.7750) acc_x 71.8750 (79.3750) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [10/16] time 0.555 (0.455) data 0.424 (0.325) loss_x loss_x 0.7412 (0.8687) acc_x 78.1250 (80.3125) lr 9.9763e-06 eta 0:00:02
epoch [193/200] batch [15/16] time 0.496 (0.473) data 0.367 (0.343) loss_x loss_x 1.2734 (0.8905) acc_x 71.8750 (80.2083) lr 9.9763e-06 eta 0:00:00
epoch [193/200] batch [5/81] time 0.444 (0.473) data 0.314 (0.343) loss_u loss_u 0.9702 (0.9660) acc_u 3.1250 (3.7500) lr 9.9763e-06 eta 0:00:35
epoch [193/200] batch [10/81] time 0.366 (0.464) data 0.236 (0.333) loss_u loss_u 0.9551 (0.9589) acc_u 3.1250 (4.3750) lr 9.9763e-06 eta 0:00:32
epoch [193/200] batch [15/81] time 0.384 (0.450) data 0.254 (0.319) loss_u loss_u 0.9561 (0.9585) acc_u 3.1250 (4.3750) lr 9.9763e-06 eta 0:00:29
epoch [193/200] batch [20/81] time 0.424 (0.445) data 0.293 (0.315) loss_u loss_u 0.9932 (0.9518) acc_u 0.0000 (5.1562) lr 9.9763e-06 eta 0:00:27
epoch [193/200] batch [25/81] time 0.622 (0.457) data 0.492 (0.326) loss_u loss_u 0.9751 (0.9547) acc_u 3.1250 (5.0000) lr 9.9763e-06 eta 0:00:25
epoch [193/200] batch [30/81] time 0.467 (0.462) data 0.335 (0.331) loss_u loss_u 0.9624 (0.9585) acc_u 6.2500 (4.8958) lr 9.9763e-06 eta 0:00:23
epoch [193/200] batch [35/81] time 0.550 (0.466) data 0.418 (0.335) loss_u loss_u 0.9502 (0.9583) acc_u 6.2500 (5.0893) lr 9.9763e-06 eta 0:00:21
epoch [193/200] batch [40/81] time 0.439 (0.469) data 0.309 (0.339) loss_u loss_u 0.9243 (0.9598) acc_u 9.3750 (5.0000) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [45/81] time 0.526 (0.468) data 0.395 (0.337) loss_u loss_u 0.9844 (0.9622) acc_u 0.0000 (4.6528) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [50/81] time 0.558 (0.469) data 0.427 (0.339) loss_u loss_u 0.9375 (0.9614) acc_u 6.2500 (4.6875) lr 9.9763e-06 eta 0:00:14
epoch [193/200] batch [55/81] time 0.394 (0.466) data 0.262 (0.335) loss_u loss_u 0.9536 (0.9619) acc_u 9.3750 (4.7727) lr 9.9763e-06 eta 0:00:12
epoch [193/200] batch [60/81] time 0.441 (0.466) data 0.311 (0.335) loss_u loss_u 0.9399 (0.9609) acc_u 6.2500 (4.7917) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [65/81] time 0.422 (0.465) data 0.291 (0.334) loss_u loss_u 0.9614 (0.9612) acc_u 3.1250 (4.7596) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [70/81] time 0.632 (0.467) data 0.500 (0.336) loss_u loss_u 0.9297 (0.9594) acc_u 9.3750 (4.9554) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [75/81] time 0.331 (0.466) data 0.200 (0.335) loss_u loss_u 0.9502 (0.9598) acc_u 6.2500 (4.8750) lr 9.9763e-06 eta 0:00:02
epoch [193/200] batch [80/81] time 0.547 (0.463) data 0.416 (0.333) loss_u loss_u 0.9741 (0.9612) acc_u 3.1250 (4.6875) lr 9.9763e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1698
confident_label rate tensor(0.1588, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 498
clean true:482
clean false:16
clean_rate:0.9678714859437751
noisy true:956
noisy false:1682
after delete: len(clean_dataset) 498
after delete: len(noisy_dataset) 2638
epoch [194/200] batch [5/15] time 0.525 (0.424) data 0.394 (0.293) loss_x loss_x 0.6431 (0.9334) acc_x 87.5000 (78.1250) lr 7.8853e-06 eta 0:00:04
epoch [194/200] batch [10/15] time 0.391 (0.455) data 0.261 (0.325) loss_x loss_x 1.3682 (1.2273) acc_x 71.8750 (75.0000) lr 7.8853e-06 eta 0:00:02
epoch [194/200] batch [15/15] time 0.547 (0.500) data 0.417 (0.370) loss_x loss_x 0.3845 (1.0872) acc_x 96.8750 (78.9583) lr 7.8853e-06 eta 0:00:00
epoch [194/200] batch [5/82] time 0.411 (0.474) data 0.280 (0.344) loss_u loss_u 0.9243 (0.9674) acc_u 9.3750 (3.7500) lr 7.8853e-06 eta 0:00:36
epoch [194/200] batch [10/82] time 0.390 (0.464) data 0.260 (0.334) loss_u loss_u 0.9702 (0.9658) acc_u 3.1250 (4.6875) lr 7.8853e-06 eta 0:00:33
epoch [194/200] batch [15/82] time 0.604 (0.467) data 0.472 (0.337) loss_u loss_u 0.8892 (0.9598) acc_u 12.5000 (4.7917) lr 7.8853e-06 eta 0:00:31
epoch [194/200] batch [20/82] time 0.636 (0.476) data 0.504 (0.346) loss_u loss_u 0.9551 (0.9621) acc_u 6.2500 (4.5312) lr 7.8853e-06 eta 0:00:29
epoch [194/200] batch [25/82] time 0.467 (0.471) data 0.336 (0.341) loss_u loss_u 0.9653 (0.9632) acc_u 3.1250 (4.6250) lr 7.8853e-06 eta 0:00:26
epoch [194/200] batch [30/82] time 0.468 (0.465) data 0.338 (0.335) loss_u loss_u 0.9590 (0.9634) acc_u 6.2500 (4.8958) lr 7.8853e-06 eta 0:00:24
epoch [194/200] batch [35/82] time 0.452 (0.465) data 0.322 (0.334) loss_u loss_u 0.9624 (0.9605) acc_u 3.1250 (5.1786) lr 7.8853e-06 eta 0:00:21
epoch [194/200] batch [40/82] time 0.555 (0.467) data 0.424 (0.337) loss_u loss_u 0.9619 (0.9612) acc_u 3.1250 (5.0000) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [45/82] time 0.447 (0.468) data 0.316 (0.337) loss_u loss_u 0.9932 (0.9590) acc_u 0.0000 (5.2083) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [50/82] time 0.541 (0.469) data 0.411 (0.338) loss_u loss_u 0.9668 (0.9593) acc_u 3.1250 (5.1250) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [55/82] time 0.502 (0.468) data 0.371 (0.337) loss_u loss_u 0.9292 (0.9580) acc_u 9.3750 (5.2273) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [60/82] time 0.431 (0.465) data 0.299 (0.335) loss_u loss_u 0.9478 (0.9597) acc_u 6.2500 (5.0000) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [65/82] time 0.380 (0.462) data 0.249 (0.331) loss_u loss_u 0.9409 (0.9594) acc_u 6.2500 (5.0962) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [70/82] time 0.346 (0.459) data 0.216 (0.329) loss_u loss_u 0.9546 (0.9586) acc_u 6.2500 (5.2679) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [75/82] time 0.483 (0.458) data 0.353 (0.327) loss_u loss_u 0.9414 (0.9578) acc_u 6.2500 (5.3750) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [80/82] time 0.372 (0.458) data 0.242 (0.327) loss_u loss_u 0.9873 (0.9588) acc_u 0.0000 (5.2344) lr 7.8853e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1708
confident_label rate tensor(0.1636, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 513
clean true:491
clean false:22
clean_rate:0.9571150097465887
noisy true:937
noisy false:1686
after delete: len(clean_dataset) 513
after delete: len(noisy_dataset) 2623
epoch [195/200] batch [5/16] time 0.432 (0.426) data 0.302 (0.296) loss_x loss_x 0.8130 (1.0039) acc_x 87.5000 (78.1250) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [10/16] time 0.382 (0.429) data 0.251 (0.299) loss_x loss_x 1.1045 (1.0400) acc_x 68.7500 (74.3750) lr 6.0390e-06 eta 0:00:02
epoch [195/200] batch [15/16] time 0.470 (0.434) data 0.340 (0.304) loss_x loss_x 1.0029 (1.0091) acc_x 75.0000 (75.2083) lr 6.0390e-06 eta 0:00:00
epoch [195/200] batch [5/81] time 0.392 (0.437) data 0.261 (0.307) loss_u loss_u 0.9639 (0.9667) acc_u 3.1250 (3.7500) lr 6.0390e-06 eta 0:00:33
epoch [195/200] batch [10/81] time 0.433 (0.442) data 0.302 (0.312) loss_u loss_u 0.9751 (0.9640) acc_u 3.1250 (4.6875) lr 6.0390e-06 eta 0:00:31
epoch [195/200] batch [15/81] time 0.398 (0.441) data 0.267 (0.310) loss_u loss_u 0.9131 (0.9584) acc_u 9.3750 (5.2083) lr 6.0390e-06 eta 0:00:29
epoch [195/200] batch [20/81] time 0.554 (0.442) data 0.422 (0.311) loss_u loss_u 0.9326 (0.9588) acc_u 6.2500 (5.0000) lr 6.0390e-06 eta 0:00:26
epoch [195/200] batch [25/81] time 0.390 (0.439) data 0.259 (0.308) loss_u loss_u 0.9312 (0.9588) acc_u 9.3750 (5.0000) lr 6.0390e-06 eta 0:00:24
epoch [195/200] batch [30/81] time 0.384 (0.433) data 0.253 (0.303) loss_u loss_u 0.9448 (0.9600) acc_u 9.3750 (5.1042) lr 6.0390e-06 eta 0:00:22
epoch [195/200] batch [35/81] time 0.426 (0.431) data 0.295 (0.300) loss_u loss_u 0.9604 (0.9607) acc_u 3.1250 (5.0000) lr 6.0390e-06 eta 0:00:19
epoch [195/200] batch [40/81] time 0.440 (0.432) data 0.309 (0.301) loss_u loss_u 0.9780 (0.9608) acc_u 3.1250 (5.0000) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [45/81] time 0.452 (0.433) data 0.320 (0.302) loss_u loss_u 0.9917 (0.9602) acc_u 3.1250 (5.0000) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [50/81] time 0.515 (0.438) data 0.384 (0.307) loss_u loss_u 0.9551 (0.9615) acc_u 6.2500 (4.8125) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [55/81] time 0.418 (0.439) data 0.286 (0.308) loss_u loss_u 0.9595 (0.9608) acc_u 3.1250 (4.8295) lr 6.0390e-06 eta 0:00:11
epoch [195/200] batch [60/81] time 0.491 (0.442) data 0.360 (0.311) loss_u loss_u 0.9790 (0.9579) acc_u 3.1250 (5.1562) lr 6.0390e-06 eta 0:00:09
epoch [195/200] batch [65/81] time 0.501 (0.443) data 0.370 (0.312) loss_u loss_u 0.9897 (0.9594) acc_u 0.0000 (4.9038) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [70/81] time 0.532 (0.449) data 0.400 (0.318) loss_u loss_u 0.9380 (0.9597) acc_u 9.3750 (4.8661) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [75/81] time 0.449 (0.450) data 0.318 (0.319) loss_u loss_u 0.9819 (0.9603) acc_u 0.0000 (4.7500) lr 6.0390e-06 eta 0:00:02
epoch [195/200] batch [80/81] time 0.450 (0.453) data 0.320 (0.322) loss_u loss_u 0.9600 (0.9600) acc_u 3.1250 (4.8438) lr 6.0390e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1678
confident_label rate tensor(0.1703, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 534
clean true:514
clean false:20
clean_rate:0.9625468164794008
noisy true:944
noisy false:1658
after delete: len(clean_dataset) 534
after delete: len(noisy_dataset) 2602
epoch [196/200] batch [5/16] time 0.431 (0.439) data 0.301 (0.309) loss_x loss_x 1.2227 (1.1443) acc_x 68.7500 (70.0000) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [10/16] time 0.462 (0.464) data 0.332 (0.334) loss_x loss_x 1.4785 (1.0870) acc_x 68.7500 (73.7500) lr 4.4380e-06 eta 0:00:02
epoch [196/200] batch [15/16] time 0.475 (0.457) data 0.345 (0.327) loss_x loss_x 0.8848 (1.0209) acc_x 78.1250 (75.2083) lr 4.4380e-06 eta 0:00:00
epoch [196/200] batch [5/81] time 0.577 (0.457) data 0.446 (0.327) loss_u loss_u 0.9951 (0.9639) acc_u 0.0000 (5.0000) lr 4.4380e-06 eta 0:00:34
epoch [196/200] batch [10/81] time 0.390 (0.446) data 0.259 (0.316) loss_u loss_u 0.9707 (0.9731) acc_u 6.2500 (4.0625) lr 4.4380e-06 eta 0:00:31
epoch [196/200] batch [15/81] time 0.392 (0.443) data 0.261 (0.313) loss_u loss_u 0.9438 (0.9721) acc_u 12.5000 (4.1667) lr 4.4380e-06 eta 0:00:29
epoch [196/200] batch [20/81] time 0.406 (0.438) data 0.276 (0.308) loss_u loss_u 0.9106 (0.9693) acc_u 12.5000 (4.2188) lr 4.4380e-06 eta 0:00:26
epoch [196/200] batch [25/81] time 0.482 (0.433) data 0.351 (0.303) loss_u loss_u 0.9990 (0.9629) acc_u 0.0000 (4.8750) lr 4.4380e-06 eta 0:00:24
epoch [196/200] batch [30/81] time 0.490 (0.440) data 0.359 (0.310) loss_u loss_u 0.9624 (0.9608) acc_u 3.1250 (5.2083) lr 4.4380e-06 eta 0:00:22
epoch [196/200] batch [35/81] time 0.441 (0.437) data 0.310 (0.307) loss_u loss_u 0.9199 (0.9595) acc_u 12.5000 (5.4464) lr 4.4380e-06 eta 0:00:20
epoch [196/200] batch [40/81] time 0.521 (0.436) data 0.389 (0.305) loss_u loss_u 0.9561 (0.9605) acc_u 3.1250 (5.3125) lr 4.4380e-06 eta 0:00:17
epoch [196/200] batch [45/81] time 0.402 (0.433) data 0.271 (0.303) loss_u loss_u 0.9668 (0.9622) acc_u 6.2500 (5.0000) lr 4.4380e-06 eta 0:00:15
epoch [196/200] batch [50/81] time 0.386 (0.432) data 0.255 (0.302) loss_u loss_u 0.9902 (0.9641) acc_u 3.1250 (4.7500) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [55/81] time 0.413 (0.431) data 0.282 (0.301) loss_u loss_u 0.9517 (0.9634) acc_u 9.3750 (4.8864) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [60/81] time 0.466 (0.437) data 0.335 (0.307) loss_u loss_u 0.9370 (0.9623) acc_u 9.3750 (5.0521) lr 4.4380e-06 eta 0:00:09
epoch [196/200] batch [65/81] time 0.415 (0.433) data 0.283 (0.303) loss_u loss_u 0.9580 (0.9628) acc_u 3.1250 (5.0000) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [70/81] time 0.611 (0.433) data 0.480 (0.303) loss_u loss_u 0.9814 (0.9635) acc_u 3.1250 (4.9554) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [75/81] time 0.479 (0.435) data 0.348 (0.304) loss_u loss_u 0.9507 (0.9634) acc_u 6.2500 (4.9583) lr 4.4380e-06 eta 0:00:02
epoch [196/200] batch [80/81] time 0.386 (0.436) data 0.255 (0.305) loss_u loss_u 0.9609 (0.9641) acc_u 3.1250 (4.8438) lr 4.4380e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1707
confident_label rate tensor(0.1601, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 502
clean true:486
clean false:16
clean_rate:0.9681274900398407
noisy true:943
noisy false:1691
after delete: len(clean_dataset) 502
after delete: len(noisy_dataset) 2634
epoch [197/200] batch [5/15] time 0.391 (0.456) data 0.262 (0.326) loss_x loss_x 0.9731 (0.9521) acc_x 75.0000 (75.0000) lr 3.0827e-06 eta 0:00:04
epoch [197/200] batch [10/15] time 0.474 (0.433) data 0.344 (0.304) loss_x loss_x 0.7095 (0.8661) acc_x 84.3750 (78.4375) lr 3.0827e-06 eta 0:00:02
epoch [197/200] batch [15/15] time 0.473 (0.433) data 0.343 (0.303) loss_x loss_x 0.7012 (0.8514) acc_x 84.3750 (79.1667) lr 3.0827e-06 eta 0:00:00
epoch [197/200] batch [5/82] time 0.451 (0.430) data 0.320 (0.301) loss_u loss_u 0.9790 (0.9460) acc_u 3.1250 (7.5000) lr 3.0827e-06 eta 0:00:33
epoch [197/200] batch [10/82] time 0.437 (0.432) data 0.306 (0.302) loss_u loss_u 0.9219 (0.9563) acc_u 12.5000 (6.2500) lr 3.0827e-06 eta 0:00:31
epoch [197/200] batch [15/82] time 0.369 (0.426) data 0.238 (0.296) loss_u loss_u 0.9946 (0.9561) acc_u 0.0000 (6.0417) lr 3.0827e-06 eta 0:00:28
epoch [197/200] batch [20/82] time 0.432 (0.428) data 0.301 (0.297) loss_u loss_u 0.9756 (0.9554) acc_u 3.1250 (5.9375) lr 3.0827e-06 eta 0:00:26
epoch [197/200] batch [25/82] time 0.365 (0.434) data 0.234 (0.304) loss_u loss_u 0.9697 (0.9584) acc_u 3.1250 (5.3750) lr 3.0827e-06 eta 0:00:24
epoch [197/200] batch [30/82] time 0.388 (0.433) data 0.257 (0.302) loss_u loss_u 0.9365 (0.9606) acc_u 6.2500 (5.1042) lr 3.0827e-06 eta 0:00:22
epoch [197/200] batch [35/82] time 0.457 (0.432) data 0.325 (0.302) loss_u loss_u 0.9854 (0.9621) acc_u 3.1250 (4.9107) lr 3.0827e-06 eta 0:00:20
epoch [197/200] batch [40/82] time 0.446 (0.434) data 0.316 (0.303) loss_u loss_u 0.9668 (0.9614) acc_u 3.1250 (4.9219) lr 3.0827e-06 eta 0:00:18
epoch [197/200] batch [45/82] time 0.484 (0.439) data 0.353 (0.309) loss_u loss_u 0.9780 (0.9610) acc_u 3.1250 (5.1389) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [50/82] time 0.495 (0.439) data 0.364 (0.308) loss_u loss_u 0.9004 (0.9589) acc_u 15.6250 (5.5000) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [55/82] time 0.383 (0.437) data 0.251 (0.307) loss_u loss_u 0.9536 (0.9582) acc_u 6.2500 (5.5682) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [60/82] time 0.523 (0.437) data 0.391 (0.306) loss_u loss_u 0.9326 (0.9570) acc_u 9.3750 (5.6771) lr 3.0827e-06 eta 0:00:09
epoch [197/200] batch [65/82] time 0.414 (0.435) data 0.283 (0.305) loss_u loss_u 0.9575 (0.9578) acc_u 3.1250 (5.5288) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [70/82] time 0.452 (0.436) data 0.321 (0.306) loss_u loss_u 0.9863 (0.9572) acc_u 3.1250 (5.6696) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [75/82] time 0.557 (0.436) data 0.426 (0.305) loss_u loss_u 0.9565 (0.9579) acc_u 6.2500 (5.6250) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [80/82] time 0.377 (0.437) data 0.246 (0.306) loss_u loss_u 0.9873 (0.9596) acc_u 0.0000 (5.3125) lr 3.0827e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1694
confident_label rate tensor(0.1623, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 509
clean true:486
clean false:23
clean_rate:0.9548133595284872
noisy true:956
noisy false:1671
after delete: len(clean_dataset) 509
after delete: len(noisy_dataset) 2627
epoch [198/200] batch [5/15] time 0.427 (0.472) data 0.296 (0.341) loss_x loss_x 1.2529 (1.1338) acc_x 71.8750 (71.8750) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [10/15] time 0.433 (0.453) data 0.302 (0.323) loss_x loss_x 0.9175 (0.9757) acc_x 78.1250 (75.0000) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [15/15] time 0.448 (0.444) data 0.318 (0.314) loss_x loss_x 1.2158 (1.0053) acc_x 81.2500 (75.2083) lr 1.9733e-06 eta 0:00:00
epoch [198/200] batch [5/82] time 0.424 (0.448) data 0.293 (0.317) loss_u loss_u 0.9404 (0.9505) acc_u 15.6250 (8.1250) lr 1.9733e-06 eta 0:00:34
epoch [198/200] batch [10/82] time 0.446 (0.440) data 0.315 (0.310) loss_u loss_u 0.9712 (0.9670) acc_u 3.1250 (4.6875) lr 1.9733e-06 eta 0:00:31
epoch [198/200] batch [15/82] time 0.357 (0.440) data 0.226 (0.309) loss_u loss_u 0.9907 (0.9615) acc_u 0.0000 (5.4167) lr 1.9733e-06 eta 0:00:29
epoch [198/200] batch [20/82] time 0.474 (0.445) data 0.343 (0.315) loss_u loss_u 0.9712 (0.9621) acc_u 3.1250 (5.1562) lr 1.9733e-06 eta 0:00:27
epoch [198/200] batch [25/82] time 0.431 (0.441) data 0.301 (0.310) loss_u loss_u 0.9316 (0.9557) acc_u 9.3750 (6.0000) lr 1.9733e-06 eta 0:00:25
epoch [198/200] batch [30/82] time 0.509 (0.440) data 0.378 (0.309) loss_u loss_u 0.9370 (0.9583) acc_u 9.3750 (5.5208) lr 1.9733e-06 eta 0:00:22
epoch [198/200] batch [35/82] time 0.444 (0.445) data 0.313 (0.314) loss_u loss_u 0.9644 (0.9580) acc_u 6.2500 (5.6250) lr 1.9733e-06 eta 0:00:20
epoch [198/200] batch [40/82] time 0.372 (0.442) data 0.241 (0.312) loss_u loss_u 0.9810 (0.9568) acc_u 3.1250 (5.7812) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [45/82] time 0.357 (0.438) data 0.226 (0.307) loss_u loss_u 0.9941 (0.9576) acc_u 0.0000 (5.7639) lr 1.9733e-06 eta 0:00:16
epoch [198/200] batch [50/82] time 0.565 (0.440) data 0.434 (0.309) loss_u loss_u 0.9355 (0.9583) acc_u 6.2500 (5.5625) lr 1.9733e-06 eta 0:00:14
epoch [198/200] batch [55/82] time 0.404 (0.439) data 0.274 (0.308) loss_u loss_u 0.9624 (0.9581) acc_u 3.1250 (5.6250) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [60/82] time 0.379 (0.439) data 0.248 (0.308) loss_u loss_u 0.9692 (0.9587) acc_u 3.1250 (5.4688) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [65/82] time 0.477 (0.439) data 0.346 (0.308) loss_u loss_u 0.9688 (0.9585) acc_u 6.2500 (5.4808) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [70/82] time 0.403 (0.443) data 0.273 (0.312) loss_u loss_u 0.9512 (0.9577) acc_u 9.3750 (5.5804) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [75/82] time 0.357 (0.439) data 0.226 (0.309) loss_u loss_u 0.9380 (0.9574) acc_u 12.5000 (5.6250) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [80/82] time 0.370 (0.436) data 0.239 (0.306) loss_u loss_u 0.9551 (0.9569) acc_u 3.1250 (5.5859) lr 1.9733e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1676
confident_label rate tensor(0.1658, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 520
clean true:504
clean false:16
clean_rate:0.9692307692307692
noisy true:956
noisy false:1660
after delete: len(clean_dataset) 520
after delete: len(noisy_dataset) 2616
epoch [199/200] batch [5/16] time 0.418 (0.416) data 0.289 (0.286) loss_x loss_x 1.5010 (1.1575) acc_x 65.6250 (74.3750) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [10/16] time 0.453 (0.430) data 0.322 (0.300) loss_x loss_x 0.8799 (1.0433) acc_x 84.3750 (77.8125) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [15/16] time 0.645 (0.450) data 0.513 (0.320) loss_x loss_x 1.1582 (1.0527) acc_x 81.2500 (76.8750) lr 1.1101e-06 eta 0:00:00
epoch [199/200] batch [5/81] time 0.531 (0.464) data 0.400 (0.333) loss_u loss_u 0.9644 (0.9641) acc_u 3.1250 (3.7500) lr 1.1101e-06 eta 0:00:35
epoch [199/200] batch [10/81] time 0.471 (0.465) data 0.340 (0.335) loss_u loss_u 0.9946 (0.9674) acc_u 0.0000 (3.7500) lr 1.1101e-06 eta 0:00:33
epoch [199/200] batch [15/81] time 0.381 (0.459) data 0.250 (0.329) loss_u loss_u 0.9541 (0.9663) acc_u 6.2500 (3.7500) lr 1.1101e-06 eta 0:00:30
epoch [199/200] batch [20/81] time 0.452 (0.459) data 0.321 (0.328) loss_u loss_u 0.9668 (0.9614) acc_u 9.3750 (4.8438) lr 1.1101e-06 eta 0:00:27
epoch [199/200] batch [25/81] time 0.491 (0.462) data 0.360 (0.331) loss_u loss_u 0.9438 (0.9601) acc_u 9.3750 (5.2500) lr 1.1101e-06 eta 0:00:25
epoch [199/200] batch [30/81] time 0.447 (0.453) data 0.316 (0.322) loss_u loss_u 0.9048 (0.9614) acc_u 12.5000 (4.8958) lr 1.1101e-06 eta 0:00:23
epoch [199/200] batch [35/81] time 0.427 (0.450) data 0.296 (0.319) loss_u loss_u 0.9648 (0.9598) acc_u 3.1250 (5.1786) lr 1.1101e-06 eta 0:00:20
epoch [199/200] batch [40/81] time 0.467 (0.450) data 0.336 (0.319) loss_u loss_u 0.9912 (0.9590) acc_u 0.0000 (5.2344) lr 1.1101e-06 eta 0:00:18
epoch [199/200] batch [45/81] time 0.471 (0.449) data 0.340 (0.318) loss_u loss_u 0.9644 (0.9590) acc_u 3.1250 (5.1389) lr 1.1101e-06 eta 0:00:16
epoch [199/200] batch [50/81] time 0.375 (0.448) data 0.244 (0.317) loss_u loss_u 0.9766 (0.9611) acc_u 3.1250 (4.8125) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [55/81] time 0.404 (0.447) data 0.273 (0.316) loss_u loss_u 0.9819 (0.9612) acc_u 3.1250 (4.7727) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [60/81] time 0.380 (0.443) data 0.249 (0.312) loss_u loss_u 0.9751 (0.9608) acc_u 3.1250 (4.7917) lr 1.1101e-06 eta 0:00:09
epoch [199/200] batch [65/81] time 0.391 (0.440) data 0.260 (0.309) loss_u loss_u 0.9136 (0.9613) acc_u 9.3750 (4.7115) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [70/81] time 0.498 (0.441) data 0.367 (0.310) loss_u loss_u 0.9912 (0.9615) acc_u 0.0000 (4.6875) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [75/81] time 0.440 (0.440) data 0.310 (0.309) loss_u loss_u 0.9951 (0.9617) acc_u 0.0000 (4.6667) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [80/81] time 0.402 (0.440) data 0.272 (0.309) loss_u loss_u 0.9761 (0.9618) acc_u 3.1250 (4.6484) lr 1.1101e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1687
confident_label rate tensor(0.1674, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 525
clean true:508
clean false:17
clean_rate:0.9676190476190476
noisy true:941
noisy false:1670
after delete: len(clean_dataset) 525
after delete: len(noisy_dataset) 2611
epoch [200/200] batch [5/16] time 0.600 (0.488) data 0.470 (0.357) loss_x loss_x 1.3057 (1.1060) acc_x 78.1250 (76.8750) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [10/16] time 0.370 (0.447) data 0.239 (0.316) loss_x loss_x 1.1924 (1.1205) acc_x 71.8750 (74.3750) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [15/16] time 0.425 (0.454) data 0.294 (0.324) loss_x loss_x 1.4629 (1.1197) acc_x 71.8750 (74.3750) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [5/81] time 0.516 (0.444) data 0.385 (0.313) loss_u loss_u 0.9868 (0.9861) acc_u 6.2500 (2.5000) lr 4.9344e-07 eta 0:00:33
epoch [200/200] batch [10/81] time 0.462 (0.437) data 0.331 (0.306) loss_u loss_u 0.9722 (0.9690) acc_u 6.2500 (4.0625) lr 4.9344e-07 eta 0:00:31
epoch [200/200] batch [15/81] time 0.396 (0.428) data 0.265 (0.297) loss_u loss_u 0.9692 (0.9612) acc_u 6.2500 (5.2083) lr 4.9344e-07 eta 0:00:28
epoch [200/200] batch [20/81] time 0.402 (0.429) data 0.271 (0.298) loss_u loss_u 0.9976 (0.9630) acc_u 0.0000 (4.8438) lr 4.9344e-07 eta 0:00:26
epoch [200/200] batch [25/81] time 0.361 (0.428) data 0.230 (0.297) loss_u loss_u 0.9727 (0.9639) acc_u 3.1250 (4.6250) lr 4.9344e-07 eta 0:00:23
epoch [200/200] batch [30/81] time 0.386 (0.428) data 0.255 (0.297) loss_u loss_u 0.9331 (0.9626) acc_u 6.2500 (4.7917) lr 4.9344e-07 eta 0:00:21
epoch [200/200] batch [35/81] time 0.473 (0.427) data 0.342 (0.296) loss_u loss_u 0.9697 (0.9627) acc_u 3.1250 (4.6429) lr 4.9344e-07 eta 0:00:19
epoch [200/200] batch [40/81] time 0.689 (0.435) data 0.558 (0.304) loss_u loss_u 0.9453 (0.9639) acc_u 6.2500 (4.6094) lr 4.9344e-07 eta 0:00:17
epoch [200/200] batch [45/81] time 0.466 (0.435) data 0.336 (0.304) loss_u loss_u 0.9897 (0.9647) acc_u 3.1250 (4.6528) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [50/81] time 0.339 (0.431) data 0.208 (0.301) loss_u loss_u 0.9897 (0.9648) acc_u 0.0000 (4.5625) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [55/81] time 0.408 (0.429) data 0.277 (0.299) loss_u loss_u 0.9692 (0.9638) acc_u 3.1250 (4.7727) lr 4.9344e-07 eta 0:00:11
epoch [200/200] batch [60/81] time 0.373 (0.432) data 0.242 (0.301) loss_u loss_u 0.9653 (0.9642) acc_u 3.1250 (4.7396) lr 4.9344e-07 eta 0:00:09
epoch [200/200] batch [65/81] time 0.376 (0.429) data 0.245 (0.299) loss_u loss_u 0.9946 (0.9659) acc_u 0.0000 (4.4712) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [70/81] time 0.439 (0.429) data 0.308 (0.299) loss_u loss_u 0.9629 (0.9663) acc_u 6.2500 (4.3304) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [75/81] time 0.397 (0.431) data 0.266 (0.300) loss_u loss_u 0.9941 (0.9647) acc_u 0.0000 (4.4583) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [80/81] time 0.744 (0.433) data 0.613 (0.302) loss_u loss_u 0.9326 (0.9647) acc_u 6.2500 (4.4141) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.75/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 4,511
* accuracy: 56.1%
* error: 43.9%
* macro_f1: 53.8%
Elapsed: 5:14:35
