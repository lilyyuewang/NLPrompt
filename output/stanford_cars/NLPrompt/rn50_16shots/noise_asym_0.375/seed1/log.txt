***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.375', 'DATASET.NOISE_TYPE', 'asym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.375/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.375
  NOISE_TYPE: asym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.375/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.375/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.1279, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 401
clean true:363
clean false:38
clean_rate:0.9052369077306733
noisy true:202
noisy false:2533
after delete: len(clean_dataset) 401
after delete: len(noisy_dataset) 2735
epoch [1/200] batch [5/12] time 0.672 (0.631) data 0.397 (0.336) loss_x loss_x 2.9492 (2.9633) acc_x 43.7500 (42.5000) lr 1.0000e-05 eta 0:00:04
epoch [1/200] batch [10/12] time 0.635 (0.632) data 0.365 (0.350) loss_x loss_x 2.8320 (2.7953) acc_x 31.2500 (42.8125) lr 1.0000e-05 eta 0:00:01
epoch [1/200] batch [5/85] time 0.517 (0.602) data 0.238 (0.322) loss_u loss_u 0.9492 (0.9567) acc_u 6.2500 (6.8750) lr 1.0000e-05 eta 0:00:48
epoch [1/200] batch [10/85] time 0.703 (0.597) data 0.418 (0.318) loss_u loss_u 0.9258 (0.9470) acc_u 15.6250 (10.9375) lr 1.0000e-05 eta 0:00:44
epoch [1/200] batch [15/85] time 0.598 (0.598) data 0.319 (0.321) loss_u loss_u 0.9233 (0.9466) acc_u 18.7500 (10.6250) lr 1.0000e-05 eta 0:00:41
epoch [1/200] batch [20/85] time 0.615 (0.599) data 0.343 (0.322) loss_u loss_u 0.9321 (0.9423) acc_u 12.5000 (11.2500) lr 1.0000e-05 eta 0:00:38
epoch [1/200] batch [25/85] time 0.532 (0.599) data 0.252 (0.322) loss_u loss_u 0.9351 (0.9367) acc_u 15.6250 (12.1250) lr 1.0000e-05 eta 0:00:35
epoch [1/200] batch [30/85] time 0.417 (0.593) data 0.225 (0.318) loss_u loss_u 0.9297 (0.9347) acc_u 12.5000 (12.3958) lr 1.0000e-05 eta 0:00:32
epoch [1/200] batch [35/85] time 0.515 (0.589) data 0.234 (0.314) loss_u loss_u 0.9146 (0.9325) acc_u 9.3750 (12.5000) lr 1.0000e-05 eta 0:00:29
epoch [1/200] batch [40/85] time 0.704 (0.591) data 0.438 (0.316) loss_u loss_u 0.9292 (0.9322) acc_u 9.3750 (12.4219) lr 1.0000e-05 eta 0:00:26
epoch [1/200] batch [45/85] time 0.600 (0.593) data 0.320 (0.319) loss_u loss_u 0.9253 (0.9308) acc_u 12.5000 (12.1528) lr 1.0000e-05 eta 0:00:23
epoch [1/200] batch [50/85] time 0.547 (0.589) data 0.280 (0.315) loss_u loss_u 0.9312 (0.9281) acc_u 6.2500 (12.6250) lr 1.0000e-05 eta 0:00:20
epoch [1/200] batch [55/85] time 0.468 (0.592) data 0.193 (0.319) loss_u loss_u 0.9536 (0.9288) acc_u 9.3750 (12.2727) lr 1.0000e-05 eta 0:00:17
epoch [1/200] batch [60/85] time 0.509 (0.589) data 0.240 (0.315) loss_u loss_u 0.9561 (0.9287) acc_u 9.3750 (12.2396) lr 1.0000e-05 eta 0:00:14
epoch [1/200] batch [65/85] time 0.491 (0.584) data 0.216 (0.312) loss_u loss_u 0.9541 (0.9308) acc_u 3.1250 (11.7788) lr 1.0000e-05 eta 0:00:11
epoch [1/200] batch [70/85] time 0.502 (0.583) data 0.228 (0.311) loss_u loss_u 0.9360 (0.9295) acc_u 9.3750 (12.1429) lr 1.0000e-05 eta 0:00:08
epoch [1/200] batch [75/85] time 0.491 (0.576) data 0.214 (0.305) loss_u loss_u 0.9258 (0.9289) acc_u 15.6250 (12.1667) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [80/85] time 0.581 (0.575) data 0.310 (0.304) loss_u loss_u 0.9429 (0.9287) acc_u 6.2500 (12.0703) lr 1.0000e-05 eta 0:00:02
epoch [1/200] batch [85/85] time 0.607 (0.577) data 0.330 (0.305) loss_u loss_u 0.9424 (0.9280) acc_u 9.3750 (12.3529) lr 1.0000e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2169
confident_label rate tensor(0.1996, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 626
clean true:596
clean false:30
clean_rate:0.952076677316294
noisy true:371
noisy false:2139
after delete: len(clean_dataset) 626
after delete: len(noisy_dataset) 2510
epoch [2/200] batch [5/19] time 0.604 (0.631) data 0.329 (0.357) loss_x loss_x 1.2178 (1.6215) acc_x 62.5000 (55.6250) lr 2.0000e-03 eta 0:00:08
epoch [2/200] batch [10/19] time 0.623 (0.606) data 0.345 (0.346) loss_x loss_x 2.1719 (1.6497) acc_x 43.7500 (55.3125) lr 2.0000e-03 eta 0:00:05
epoch [2/200] batch [15/19] time 0.585 (0.609) data 0.314 (0.347) loss_x loss_x 1.7520 (1.7399) acc_x 50.0000 (54.5833) lr 2.0000e-03 eta 0:00:02
epoch [2/200] batch [5/78] time 0.536 (0.612) data 0.307 (0.348) loss_u loss_u 0.8916 (0.8618) acc_u 15.6250 (18.7500) lr 2.0000e-03 eta 0:00:44
epoch [2/200] batch [10/78] time 0.543 (0.606) data 0.269 (0.340) loss_u loss_u 0.9072 (0.8784) acc_u 12.5000 (15.3125) lr 2.0000e-03 eta 0:00:41
epoch [2/200] batch [15/78] time 0.563 (0.602) data 0.283 (0.335) loss_u loss_u 0.8789 (0.8755) acc_u 15.6250 (15.4167) lr 2.0000e-03 eta 0:00:37
epoch [2/200] batch [20/78] time 0.596 (0.601) data 0.329 (0.336) loss_u loss_u 0.8574 (0.8741) acc_u 18.7500 (15.9375) lr 2.0000e-03 eta 0:00:34
epoch [2/200] batch [25/78] time 0.786 (0.600) data 0.504 (0.333) loss_u loss_u 0.7666 (0.8638) acc_u 34.3750 (17.8750) lr 2.0000e-03 eta 0:00:31
epoch [2/200] batch [30/78] time 0.512 (0.595) data 0.242 (0.327) loss_u loss_u 0.8579 (0.8622) acc_u 15.6250 (18.1250) lr 2.0000e-03 eta 0:00:28
epoch [2/200] batch [35/78] time 0.666 (0.598) data 0.390 (0.330) loss_u loss_u 0.8804 (0.8637) acc_u 15.6250 (17.5000) lr 2.0000e-03 eta 0:00:25
epoch [2/200] batch [40/78] time 0.481 (0.596) data 0.197 (0.328) loss_u loss_u 0.8359 (0.8649) acc_u 18.7500 (17.3438) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [45/78] time 0.530 (0.595) data 0.262 (0.327) loss_u loss_u 0.8379 (0.8635) acc_u 15.6250 (17.3611) lr 2.0000e-03 eta 0:00:19
epoch [2/200] batch [50/78] time 0.486 (0.597) data 0.209 (0.329) loss_u loss_u 0.9019 (0.8648) acc_u 15.6250 (17.2500) lr 2.0000e-03 eta 0:00:16
epoch [2/200] batch [55/78] time 0.502 (0.596) data 0.231 (0.327) loss_u loss_u 0.8438 (0.8650) acc_u 18.7500 (17.2159) lr 2.0000e-03 eta 0:00:13
epoch [2/200] batch [60/78] time 0.505 (0.592) data 0.230 (0.323) loss_u loss_u 0.8203 (0.8652) acc_u 25.0000 (17.1354) lr 2.0000e-03 eta 0:00:10
epoch [2/200] batch [65/78] time 0.637 (0.589) data 0.371 (0.320) loss_u loss_u 0.8784 (0.8653) acc_u 12.5000 (17.1154) lr 2.0000e-03 eta 0:00:07
epoch [2/200] batch [70/78] time 0.572 (0.585) data 0.290 (0.317) loss_u loss_u 0.8711 (0.8658) acc_u 15.6250 (16.8750) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [75/78] time 0.529 (0.582) data 0.248 (0.313) loss_u loss_u 0.8550 (0.8647) acc_u 18.7500 (17.0000) lr 2.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1908
confident_label rate tensor(0.2669, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 837
clean true:795
clean false:42
clean_rate:0.9498207885304659
noisy true:433
noisy false:1866
after delete: len(clean_dataset) 837
after delete: len(noisy_dataset) 2299
epoch [3/200] batch [5/26] time 0.676 (0.610) data 0.400 (0.336) loss_x loss_x 2.1777 (1.6645) acc_x 40.6250 (54.3750) lr 1.9999e-03 eta 0:00:12
epoch [3/200] batch [10/26] time 0.546 (0.589) data 0.280 (0.317) loss_x loss_x 1.5234 (1.6699) acc_x 59.3750 (55.3125) lr 1.9999e-03 eta 0:00:09
epoch [3/200] batch [15/26] time 0.663 (0.602) data 0.394 (0.330) loss_x loss_x 2.2754 (1.6946) acc_x 46.8750 (55.0000) lr 1.9999e-03 eta 0:00:06
epoch [3/200] batch [20/26] time 0.498 (0.599) data 0.235 (0.335) loss_x loss_x 1.3135 (1.6360) acc_x 59.3750 (55.9375) lr 1.9999e-03 eta 0:00:03
epoch [3/200] batch [25/26] time 0.545 (0.591) data 0.269 (0.325) loss_x loss_x 1.7275 (1.6581) acc_x 56.2500 (55.6250) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/71] time 0.649 (0.587) data 0.375 (0.320) loss_u loss_u 0.8057 (0.8653) acc_u 28.1250 (16.8750) lr 1.9999e-03 eta 0:00:38
epoch [3/200] batch [10/71] time 0.549 (0.583) data 0.281 (0.315) loss_u loss_u 0.9077 (0.8764) acc_u 12.5000 (14.3750) lr 1.9999e-03 eta 0:00:35
epoch [3/200] batch [15/71] time 0.537 (0.579) data 0.262 (0.314) loss_u loss_u 0.8926 (0.8767) acc_u 12.5000 (15.4167) lr 1.9999e-03 eta 0:00:32
epoch [3/200] batch [20/71] time 0.493 (0.576) data 0.222 (0.310) loss_u loss_u 0.8857 (0.8704) acc_u 18.7500 (17.3438) lr 1.9999e-03 eta 0:00:29
epoch [3/200] batch [25/71] time 0.540 (0.570) data 0.262 (0.306) loss_u loss_u 0.8882 (0.8730) acc_u 18.7500 (17.6250) lr 1.9999e-03 eta 0:00:26
epoch [3/200] batch [30/71] time 0.475 (0.569) data 0.200 (0.304) loss_u loss_u 0.8833 (0.8718) acc_u 9.3750 (17.1875) lr 1.9999e-03 eta 0:00:23
epoch [3/200] batch [35/71] time 0.493 (0.567) data 0.214 (0.303) loss_u loss_u 0.9102 (0.8738) acc_u 6.2500 (16.6964) lr 1.9999e-03 eta 0:00:20
epoch [3/200] batch [40/71] time 0.634 (0.570) data 0.366 (0.305) loss_u loss_u 0.8828 (0.8748) acc_u 15.6250 (16.3281) lr 1.9999e-03 eta 0:00:17
epoch [3/200] batch [45/71] time 0.557 (0.570) data 0.277 (0.304) loss_u loss_u 0.9194 (0.8770) acc_u 15.6250 (16.0417) lr 1.9999e-03 eta 0:00:14
epoch [3/200] batch [50/71] time 0.553 (0.570) data 0.287 (0.306) loss_u loss_u 0.8882 (0.8764) acc_u 12.5000 (15.8125) lr 1.9999e-03 eta 0:00:11
epoch [3/200] batch [55/71] time 0.553 (0.570) data 0.272 (0.305) loss_u loss_u 0.8438 (0.8770) acc_u 15.6250 (15.6818) lr 1.9999e-03 eta 0:00:09
epoch [3/200] batch [60/71] time 0.627 (0.575) data 0.344 (0.309) loss_u loss_u 0.8413 (0.8777) acc_u 18.7500 (15.4167) lr 1.9999e-03 eta 0:00:06
epoch [3/200] batch [65/71] time 0.604 (0.576) data 0.329 (0.310) loss_u loss_u 0.8652 (0.8767) acc_u 25.0000 (15.6250) lr 1.9999e-03 eta 0:00:03
epoch [3/200] batch [70/71] time 0.543 (0.575) data 0.261 (0.308) loss_u loss_u 0.8706 (0.8751) acc_u 12.5000 (15.8036) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1861
confident_label rate tensor(0.2771, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 869
clean true:826
clean false:43
clean_rate:0.950517836593786
noisy true:449
noisy false:1818
after delete: len(clean_dataset) 869
after delete: len(noisy_dataset) 2267
epoch [4/200] batch [5/27] time 0.517 (0.608) data 0.241 (0.337) loss_x loss_x 1.9082 (1.6670) acc_x 46.8750 (48.7500) lr 1.9995e-03 eta 0:00:13
epoch [4/200] batch [10/27] time 0.634 (0.622) data 0.358 (0.357) loss_x loss_x 1.3496 (1.6291) acc_x 68.7500 (54.3750) lr 1.9995e-03 eta 0:00:10
epoch [4/200] batch [15/27] time 0.512 (0.623) data 0.239 (0.356) loss_x loss_x 1.7734 (1.6626) acc_x 59.3750 (54.7917) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [20/27] time 0.563 (0.606) data 0.292 (0.341) loss_x loss_x 0.9868 (1.6891) acc_x 62.5000 (54.8438) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [25/27] time 0.567 (0.602) data 0.289 (0.335) loss_x loss_x 2.2051 (1.7059) acc_x 56.2500 (54.7500) lr 1.9995e-03 eta 0:00:01
epoch [4/200] batch [5/70] time 0.532 (0.596) data 0.255 (0.331) loss_u loss_u 0.8491 (0.8854) acc_u 21.8750 (15.0000) lr 1.9995e-03 eta 0:00:38
epoch [4/200] batch [10/70] time 0.590 (0.592) data 0.310 (0.325) loss_u loss_u 0.8516 (0.8779) acc_u 15.6250 (16.2500) lr 1.9995e-03 eta 0:00:35
epoch [4/200] batch [15/70] time 0.536 (0.595) data 0.265 (0.327) loss_u loss_u 0.9434 (0.8812) acc_u 0.0000 (14.7917) lr 1.9995e-03 eta 0:00:32
epoch [4/200] batch [20/70] time 0.637 (0.592) data 0.371 (0.327) loss_u loss_u 0.8457 (0.8806) acc_u 18.7500 (14.5312) lr 1.9995e-03 eta 0:00:29
epoch [4/200] batch [25/70] time 0.743 (0.591) data 0.461 (0.324) loss_u loss_u 0.8599 (0.8769) acc_u 9.3750 (15.5000) lr 1.9995e-03 eta 0:00:26
epoch [4/200] batch [30/70] time 0.512 (0.595) data 0.237 (0.328) loss_u loss_u 0.9424 (0.8815) acc_u 6.2500 (14.6875) lr 1.9995e-03 eta 0:00:23
epoch [4/200] batch [35/70] time 0.469 (0.585) data 0.199 (0.320) loss_u loss_u 0.8643 (0.8779) acc_u 21.8750 (15.3571) lr 1.9995e-03 eta 0:00:20
epoch [4/200] batch [40/70] time 0.513 (0.583) data 0.242 (0.317) loss_u loss_u 0.9277 (0.8820) acc_u 9.3750 (14.6875) lr 1.9995e-03 eta 0:00:17
epoch [4/200] batch [45/70] time 0.587 (0.582) data 0.314 (0.316) loss_u loss_u 0.8569 (0.8799) acc_u 28.1250 (14.9306) lr 1.9995e-03 eta 0:00:14
epoch [4/200] batch [50/70] time 0.714 (0.584) data 0.445 (0.318) loss_u loss_u 0.9160 (0.8792) acc_u 12.5000 (15.1250) lr 1.9995e-03 eta 0:00:11
epoch [4/200] batch [55/70] time 0.468 (0.580) data 0.195 (0.314) loss_u loss_u 0.8916 (0.8810) acc_u 12.5000 (14.7727) lr 1.9995e-03 eta 0:00:08
epoch [4/200] batch [60/70] time 0.516 (0.580) data 0.244 (0.314) loss_u loss_u 0.8711 (0.8797) acc_u 25.0000 (15.1042) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [65/70] time 0.596 (0.582) data 0.318 (0.316) loss_u loss_u 0.8682 (0.8775) acc_u 21.8750 (15.5769) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [70/70] time 0.560 (0.580) data 0.290 (0.315) loss_u loss_u 0.8906 (0.8760) acc_u 9.3750 (15.7589) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1890
confident_label rate tensor(0.2695, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 845
clean true:801
clean false:44
clean_rate:0.9479289940828403
noisy true:445
noisy false:1846
after delete: len(clean_dataset) 845
after delete: len(noisy_dataset) 2291
epoch [5/200] batch [5/26] time 0.604 (0.595) data 0.330 (0.320) loss_x loss_x 2.0410 (1.6461) acc_x 56.2500 (60.0000) lr 1.9989e-03 eta 0:00:12
epoch [5/200] batch [10/26] time 0.707 (0.592) data 0.429 (0.327) loss_x loss_x 0.9600 (1.5090) acc_x 71.8750 (61.5625) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [15/26] time 0.497 (0.584) data 0.223 (0.317) loss_x loss_x 1.5420 (1.5047) acc_x 53.1250 (60.2083) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [20/26] time 0.633 (0.595) data 0.363 (0.328) loss_x loss_x 1.6016 (1.5273) acc_x 56.2500 (58.4375) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [25/26] time 0.579 (0.588) data 0.298 (0.324) loss_x loss_x 1.0918 (1.5136) acc_x 62.5000 (59.3750) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/71] time 0.638 (0.588) data 0.365 (0.322) loss_u loss_u 0.9111 (0.8529) acc_u 12.5000 (18.1250) lr 1.9989e-03 eta 0:00:38
epoch [5/200] batch [10/71] time 0.742 (0.587) data 0.469 (0.319) loss_u loss_u 0.8232 (0.8704) acc_u 25.0000 (15.6250) lr 1.9989e-03 eta 0:00:35
epoch [5/200] batch [15/71] time 0.515 (0.584) data 0.239 (0.316) loss_u loss_u 0.8604 (0.8688) acc_u 18.7500 (16.2500) lr 1.9989e-03 eta 0:00:32
epoch [5/200] batch [20/71] time 0.708 (0.585) data 0.440 (0.316) loss_u loss_u 0.8960 (0.8691) acc_u 9.3750 (15.9375) lr 1.9989e-03 eta 0:00:29
epoch [5/200] batch [25/71] time 0.921 (0.586) data 0.639 (0.319) loss_u loss_u 0.8989 (0.8685) acc_u 25.0000 (17.0000) lr 1.9989e-03 eta 0:00:26
epoch [5/200] batch [30/71] time 0.626 (0.588) data 0.359 (0.321) loss_u loss_u 0.8931 (0.8743) acc_u 9.3750 (15.9375) lr 1.9989e-03 eta 0:00:24
epoch [5/200] batch [35/71] time 0.532 (0.585) data 0.260 (0.317) loss_u loss_u 0.8628 (0.8700) acc_u 18.7500 (16.3393) lr 1.9989e-03 eta 0:00:21
epoch [5/200] batch [40/71] time 0.464 (0.586) data 0.271 (0.319) loss_u loss_u 0.9238 (0.8737) acc_u 6.2500 (16.1719) lr 1.9989e-03 eta 0:00:18
epoch [5/200] batch [45/71] time 0.612 (0.585) data 0.331 (0.317) loss_u loss_u 0.8730 (0.8740) acc_u 15.6250 (16.0417) lr 1.9989e-03 eta 0:00:15
epoch [5/200] batch [50/71] time 0.567 (0.587) data 0.292 (0.318) loss_u loss_u 0.8931 (0.8757) acc_u 15.6250 (15.7500) lr 1.9989e-03 eta 0:00:12
epoch [5/200] batch [55/71] time 0.665 (0.586) data 0.393 (0.319) loss_u loss_u 0.8716 (0.8764) acc_u 18.7500 (15.6250) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [60/71] time 0.659 (0.590) data 0.374 (0.323) loss_u loss_u 0.9106 (0.8778) acc_u 6.2500 (15.3646) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [65/71] time 0.541 (0.591) data 0.266 (0.324) loss_u loss_u 0.8154 (0.8760) acc_u 31.2500 (15.8173) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [70/71] time 0.563 (0.590) data 0.286 (0.322) loss_u loss_u 0.9209 (0.8765) acc_u 9.3750 (15.8482) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1837
confident_label rate tensor(0.2832, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 888
clean true:840
clean false:48
clean_rate:0.9459459459459459
noisy true:459
noisy false:1789
after delete: len(clean_dataset) 888
after delete: len(noisy_dataset) 2248
epoch [6/200] batch [5/27] time 0.507 (0.612) data 0.229 (0.335) loss_x loss_x 1.7803 (1.3514) acc_x 62.5000 (65.0000) lr 1.9980e-03 eta 0:00:13
epoch [6/200] batch [10/27] time 0.489 (0.578) data 0.358 (0.315) loss_x loss_x 1.3223 (1.3471) acc_x 59.3750 (63.7500) lr 1.9980e-03 eta 0:00:09
epoch [6/200] batch [15/27] time 0.563 (0.596) data 0.292 (0.330) loss_x loss_x 2.2109 (1.4527) acc_x 37.5000 (61.6667) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [20/27] time 0.603 (0.609) data 0.325 (0.340) loss_x loss_x 1.5840 (1.5243) acc_x 56.2500 (60.3125) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [25/27] time 0.593 (0.615) data 0.318 (0.346) loss_x loss_x 0.9941 (1.5423) acc_x 71.8750 (60.0000) lr 1.9980e-03 eta 0:00:01
epoch [6/200] batch [5/70] time 0.673 (0.607) data 0.389 (0.340) loss_u loss_u 0.8853 (0.8970) acc_u 18.7500 (13.1250) lr 1.9980e-03 eta 0:00:39
epoch [6/200] batch [10/70] time 0.573 (0.599) data 0.302 (0.334) loss_u loss_u 0.8608 (0.8806) acc_u 15.6250 (14.6875) lr 1.9980e-03 eta 0:00:35
epoch [6/200] batch [15/70] time 0.506 (0.597) data 0.227 (0.331) loss_u loss_u 0.9141 (0.8789) acc_u 12.5000 (14.7917) lr 1.9980e-03 eta 0:00:32
epoch [6/200] batch [20/70] time 0.534 (0.599) data 0.255 (0.331) loss_u loss_u 0.8823 (0.8785) acc_u 15.6250 (14.5312) lr 1.9980e-03 eta 0:00:29
epoch [6/200] batch [25/70] time 0.596 (0.598) data 0.316 (0.329) loss_u loss_u 0.8916 (0.8792) acc_u 12.5000 (14.7500) lr 1.9980e-03 eta 0:00:26
epoch [6/200] batch [30/70] time 0.583 (0.595) data 0.308 (0.326) loss_u loss_u 0.8691 (0.8810) acc_u 12.5000 (14.4792) lr 1.9980e-03 eta 0:00:23
epoch [6/200] batch [35/70] time 0.499 (0.596) data 0.222 (0.326) loss_u loss_u 0.9253 (0.8834) acc_u 3.1250 (13.8393) lr 1.9980e-03 eta 0:00:20
epoch [6/200] batch [40/70] time 0.584 (0.595) data 0.432 (0.326) loss_u loss_u 0.8770 (0.8848) acc_u 12.5000 (13.5938) lr 1.9980e-03 eta 0:00:17
epoch [6/200] batch [45/70] time 0.654 (0.592) data 0.386 (0.323) loss_u loss_u 0.8345 (0.8823) acc_u 25.0000 (14.0278) lr 1.9980e-03 eta 0:00:14
epoch [6/200] batch [50/70] time 0.704 (0.591) data 0.434 (0.322) loss_u loss_u 0.8511 (0.8805) acc_u 21.8750 (14.1875) lr 1.9980e-03 eta 0:00:11
epoch [6/200] batch [55/70] time 0.492 (0.592) data 0.217 (0.322) loss_u loss_u 0.8755 (0.8801) acc_u 21.8750 (14.4886) lr 1.9980e-03 eta 0:00:08
epoch [6/200] batch [60/70] time 0.541 (0.592) data 0.265 (0.322) loss_u loss_u 0.9189 (0.8808) acc_u 9.3750 (14.2708) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [65/70] time 0.729 (0.592) data 0.463 (0.322) loss_u loss_u 0.8853 (0.8822) acc_u 9.3750 (14.0865) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [70/70] time 0.514 (0.588) data 0.243 (0.318) loss_u loss_u 0.8906 (0.8835) acc_u 12.5000 (14.0625) lr 1.9980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1791
confident_label rate tensor(0.2911, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 913
clean true:871
clean false:42
clean_rate:0.9539978094194962
noisy true:474
noisy false:1749
after delete: len(clean_dataset) 913
after delete: len(noisy_dataset) 2223
epoch [7/200] batch [5/28] time 0.637 (0.685) data 0.374 (0.439) loss_x loss_x 1.2676 (1.6318) acc_x 71.8750 (60.6250) lr 1.9969e-03 eta 0:00:15
epoch [7/200] batch [10/28] time 0.555 (0.622) data 0.284 (0.363) loss_x loss_x 2.2441 (1.7053) acc_x 40.6250 (55.3125) lr 1.9969e-03 eta 0:00:11
epoch [7/200] batch [15/28] time 0.670 (0.633) data 0.405 (0.370) loss_x loss_x 1.4785 (1.7091) acc_x 62.5000 (56.8750) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [20/28] time 0.681 (0.647) data 0.418 (0.382) loss_x loss_x 1.8311 (1.6902) acc_x 50.0000 (57.9688) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [25/28] time 0.632 (0.634) data 0.355 (0.373) loss_x loss_x 2.0117 (1.6632) acc_x 43.7500 (57.6250) lr 1.9969e-03 eta 0:00:01
epoch [7/200] batch [5/69] time 0.556 (0.614) data 0.284 (0.350) loss_u loss_u 0.8125 (0.8658) acc_u 25.0000 (18.7500) lr 1.9969e-03 eta 0:00:39
epoch [7/200] batch [10/69] time 0.651 (0.614) data 0.373 (0.348) loss_u loss_u 0.8926 (0.8849) acc_u 6.2500 (14.3750) lr 1.9969e-03 eta 0:00:36
epoch [7/200] batch [15/69] time 0.549 (0.608) data 0.286 (0.341) loss_u loss_u 0.8931 (0.8907) acc_u 21.8750 (13.5417) lr 1.9969e-03 eta 0:00:32
epoch [7/200] batch [20/69] time 0.579 (0.602) data 0.311 (0.335) loss_u loss_u 0.9233 (0.8915) acc_u 9.3750 (13.5938) lr 1.9969e-03 eta 0:00:29
epoch [7/200] batch [25/69] time 0.504 (0.601) data 0.221 (0.333) loss_u loss_u 0.8735 (0.8864) acc_u 15.6250 (14.3750) lr 1.9969e-03 eta 0:00:26
epoch [7/200] batch [30/69] time 0.475 (0.599) data 0.203 (0.333) loss_u loss_u 0.8848 (0.8806) acc_u 15.6250 (15.1042) lr 1.9969e-03 eta 0:00:23
epoch [7/200] batch [35/69] time 0.616 (0.597) data 0.342 (0.330) loss_u loss_u 0.9028 (0.8804) acc_u 6.2500 (14.7321) lr 1.9969e-03 eta 0:00:20
epoch [7/200] batch [40/69] time 0.524 (0.593) data 0.251 (0.326) loss_u loss_u 0.8882 (0.8797) acc_u 12.5000 (15.0000) lr 1.9969e-03 eta 0:00:17
epoch [7/200] batch [45/69] time 0.711 (0.591) data 0.434 (0.324) loss_u loss_u 0.8579 (0.8794) acc_u 15.6250 (15.2083) lr 1.9969e-03 eta 0:00:14
epoch [7/200] batch [50/69] time 0.544 (0.587) data 0.270 (0.321) loss_u loss_u 0.8813 (0.8784) acc_u 12.5000 (15.3125) lr 1.9969e-03 eta 0:00:11
epoch [7/200] batch [55/69] time 0.570 (0.588) data 0.292 (0.321) loss_u loss_u 0.7773 (0.8771) acc_u 31.2500 (15.3977) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [60/69] time 0.561 (0.586) data 0.286 (0.319) loss_u loss_u 0.8599 (0.8774) acc_u 15.6250 (15.2083) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [65/69] time 0.663 (0.588) data 0.385 (0.321) loss_u loss_u 0.8867 (0.8779) acc_u 12.5000 (15.1923) lr 1.9969e-03 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1760
confident_label rate tensor(0.2908, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 912
clean true:863
clean false:49
clean_rate:0.9462719298245614
noisy true:513
noisy false:1711
after delete: len(clean_dataset) 912
after delete: len(noisy_dataset) 2224
epoch [8/200] batch [5/28] time 0.594 (0.604) data 0.323 (0.327) loss_x loss_x 1.1768 (1.2621) acc_x 68.7500 (61.8750) lr 1.9956e-03 eta 0:00:13
epoch [8/200] batch [10/28] time 0.644 (0.616) data 0.366 (0.340) loss_x loss_x 1.8447 (1.4562) acc_x 50.0000 (60.3125) lr 1.9956e-03 eta 0:00:11
epoch [8/200] batch [15/28] time 0.574 (0.609) data 0.334 (0.337) loss_x loss_x 1.3975 (1.4568) acc_x 62.5000 (60.6250) lr 1.9956e-03 eta 0:00:07
epoch [8/200] batch [20/28] time 0.548 (0.603) data 0.278 (0.331) loss_x loss_x 1.3545 (1.4690) acc_x 59.3750 (60.4688) lr 1.9956e-03 eta 0:00:04
epoch [8/200] batch [25/28] time 0.688 (0.613) data 0.408 (0.341) loss_x loss_x 1.6504 (1.4700) acc_x 62.5000 (60.7500) lr 1.9956e-03 eta 0:00:01
epoch [8/200] batch [5/69] time 0.489 (0.611) data 0.210 (0.338) loss_u loss_u 0.8638 (0.8761) acc_u 12.5000 (16.2500) lr 1.9956e-03 eta 0:00:39
epoch [8/200] batch [10/69] time 0.507 (0.604) data 0.232 (0.331) loss_u loss_u 0.8936 (0.8812) acc_u 15.6250 (16.2500) lr 1.9956e-03 eta 0:00:35
epoch [8/200] batch [15/69] time 0.549 (0.605) data 0.280 (0.331) loss_u loss_u 0.8794 (0.8749) acc_u 18.7500 (17.0833) lr 1.9956e-03 eta 0:00:32
epoch [8/200] batch [20/69] time 0.528 (0.604) data 0.251 (0.331) loss_u loss_u 0.8892 (0.8747) acc_u 9.3750 (16.8750) lr 1.9956e-03 eta 0:00:29
epoch [8/200] batch [25/69] time 0.618 (0.603) data 0.338 (0.329) loss_u loss_u 0.8843 (0.8757) acc_u 15.6250 (16.6250) lr 1.9956e-03 eta 0:00:26
epoch [8/200] batch [30/69] time 0.563 (0.597) data 0.280 (0.323) loss_u loss_u 0.8164 (0.8731) acc_u 25.0000 (17.2917) lr 1.9956e-03 eta 0:00:23
epoch [8/200] batch [35/69] time 0.621 (0.593) data 0.351 (0.320) loss_u loss_u 0.8789 (0.8753) acc_u 12.5000 (16.7857) lr 1.9956e-03 eta 0:00:20
epoch [8/200] batch [40/69] time 0.554 (0.594) data 0.282 (0.321) loss_u loss_u 0.8818 (0.8775) acc_u 18.7500 (16.5625) lr 1.9956e-03 eta 0:00:17
epoch [8/200] batch [45/69] time 0.573 (0.591) data 0.294 (0.318) loss_u loss_u 0.8809 (0.8743) acc_u 15.6250 (16.8750) lr 1.9956e-03 eta 0:00:14
epoch [8/200] batch [50/69] time 0.570 (0.591) data 0.299 (0.318) loss_u loss_u 0.8604 (0.8730) acc_u 18.7500 (16.8125) lr 1.9956e-03 eta 0:00:11
epoch [8/200] batch [55/69] time 0.664 (0.593) data 0.381 (0.322) loss_u loss_u 0.9062 (0.8741) acc_u 9.3750 (16.8182) lr 1.9956e-03 eta 0:00:08
epoch [8/200] batch [60/69] time 0.662 (0.593) data 0.392 (0.321) loss_u loss_u 0.9355 (0.8716) acc_u 12.5000 (17.0833) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [65/69] time 0.605 (0.595) data 0.330 (0.323) loss_u loss_u 0.8291 (0.8724) acc_u 21.8750 (16.8269) lr 1.9956e-03 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1777
confident_label rate tensor(0.2867, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 899
clean true:858
clean false:41
clean_rate:0.9543937708565072
noisy true:501
noisy false:1736
after delete: len(clean_dataset) 899
after delete: len(noisy_dataset) 2237
epoch [9/200] batch [5/28] time 0.761 (0.571) data 0.486 (0.322) loss_x loss_x 0.9976 (1.3423) acc_x 71.8750 (62.5000) lr 1.9940e-03 eta 0:00:13
epoch [9/200] batch [10/28] time 0.610 (0.600) data 0.342 (0.339) loss_x loss_x 1.7197 (1.3773) acc_x 50.0000 (62.8125) lr 1.9940e-03 eta 0:00:10
epoch [9/200] batch [15/28] time 0.780 (0.610) data 0.510 (0.354) loss_x loss_x 1.6240 (1.4051) acc_x 56.2500 (61.4583) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [20/28] time 0.742 (0.616) data 0.469 (0.356) loss_x loss_x 1.2383 (1.3915) acc_x 65.6250 (62.5000) lr 1.9940e-03 eta 0:00:04
epoch [9/200] batch [25/28] time 0.601 (0.610) data 0.336 (0.350) loss_x loss_x 1.8818 (1.4231) acc_x 59.3750 (62.7500) lr 1.9940e-03 eta 0:00:01
epoch [9/200] batch [5/69] time 0.503 (0.601) data 0.227 (0.337) loss_u loss_u 0.8726 (0.8908) acc_u 12.5000 (14.3750) lr 1.9940e-03 eta 0:00:38
epoch [9/200] batch [10/69] time 0.590 (0.599) data 0.317 (0.333) loss_u loss_u 0.8501 (0.8938) acc_u 18.7500 (13.4375) lr 1.9940e-03 eta 0:00:35
epoch [9/200] batch [15/69] time 0.580 (0.600) data 0.303 (0.333) loss_u loss_u 0.9023 (0.8898) acc_u 9.3750 (13.1250) lr 1.9940e-03 eta 0:00:32
epoch [9/200] batch [20/69] time 0.612 (0.605) data 0.336 (0.338) loss_u loss_u 0.8911 (0.8903) acc_u 15.6250 (13.4375) lr 1.9940e-03 eta 0:00:29
epoch [9/200] batch [25/69] time 0.638 (0.609) data 0.359 (0.341) loss_u loss_u 0.8979 (0.8879) acc_u 12.5000 (14.0000) lr 1.9940e-03 eta 0:00:26
epoch [9/200] batch [30/69] time 0.491 (0.609) data 0.223 (0.340) loss_u loss_u 0.8682 (0.8810) acc_u 18.7500 (15.3125) lr 1.9940e-03 eta 0:00:23
epoch [9/200] batch [35/69] time 0.527 (0.612) data 0.228 (0.343) loss_u loss_u 0.8838 (0.8813) acc_u 21.8750 (15.4464) lr 1.9940e-03 eta 0:00:20
epoch [9/200] batch [40/69] time 0.576 (0.611) data 0.306 (0.342) loss_u loss_u 0.8794 (0.8818) acc_u 15.6250 (15.3125) lr 1.9940e-03 eta 0:00:17
epoch [9/200] batch [45/69] time 0.568 (0.610) data 0.300 (0.341) loss_u loss_u 0.8813 (0.8803) acc_u 15.6250 (15.2778) lr 1.9940e-03 eta 0:00:14
epoch [9/200] batch [50/69] time 0.614 (0.609) data 0.346 (0.339) loss_u loss_u 0.8203 (0.8815) acc_u 25.0000 (14.9375) lr 1.9940e-03 eta 0:00:11
epoch [9/200] batch [55/69] time 0.482 (0.603) data 0.211 (0.333) loss_u loss_u 0.9038 (0.8809) acc_u 12.5000 (15.0000) lr 1.9940e-03 eta 0:00:08
epoch [9/200] batch [60/69] time 0.604 (0.599) data 0.325 (0.330) loss_u loss_u 0.8589 (0.8810) acc_u 18.7500 (15.0000) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [65/69] time 0.606 (0.597) data 0.322 (0.328) loss_u loss_u 0.7939 (0.8774) acc_u 31.2500 (15.4808) lr 1.9940e-03 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1804
confident_label rate tensor(0.2902, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 910
clean true:856
clean false:54
clean_rate:0.9406593406593406
noisy true:476
noisy false:1750
after delete: len(clean_dataset) 910
after delete: len(noisy_dataset) 2226
epoch [10/200] batch [5/28] time 0.444 (0.580) data 0.175 (0.305) loss_x loss_x 1.4824 (1.4928) acc_x 68.7500 (59.3750) lr 1.9921e-03 eta 0:00:13
epoch [10/200] batch [10/28] time 0.601 (0.587) data 0.328 (0.312) loss_x loss_x 1.8643 (1.5413) acc_x 53.1250 (58.1250) lr 1.9921e-03 eta 0:00:10
epoch [10/200] batch [15/28] time 0.547 (0.588) data 0.270 (0.312) loss_x loss_x 2.1680 (1.5608) acc_x 40.6250 (58.3333) lr 1.9921e-03 eta 0:00:07
epoch [10/200] batch [20/28] time 0.540 (0.587) data 0.267 (0.319) loss_x loss_x 2.4590 (1.5663) acc_x 46.8750 (58.4375) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [25/28] time 0.633 (0.598) data 0.358 (0.330) loss_x loss_x 1.6504 (1.5537) acc_x 46.8750 (58.8750) lr 1.9921e-03 eta 0:00:01
epoch [10/200] batch [5/69] time 0.659 (0.593) data 0.385 (0.326) loss_u loss_u 0.8232 (0.8789) acc_u 25.0000 (15.6250) lr 1.9921e-03 eta 0:00:37
epoch [10/200] batch [10/69] time 0.674 (0.598) data 0.398 (0.329) loss_u loss_u 0.8955 (0.8754) acc_u 9.3750 (16.2500) lr 1.9921e-03 eta 0:00:35
epoch [10/200] batch [15/69] time 0.417 (0.589) data 0.234 (0.322) loss_u loss_u 0.9272 (0.8801) acc_u 9.3750 (15.4167) lr 1.9921e-03 eta 0:00:31
epoch [10/200] batch [20/69] time 0.759 (0.591) data 0.483 (0.323) loss_u loss_u 0.9175 (0.8841) acc_u 9.3750 (15.0000) lr 1.9921e-03 eta 0:00:28
epoch [10/200] batch [25/69] time 0.674 (0.594) data 0.399 (0.325) loss_u loss_u 0.9204 (0.8849) acc_u 12.5000 (14.6250) lr 1.9921e-03 eta 0:00:26
epoch [10/200] batch [30/69] time 0.506 (0.590) data 0.220 (0.322) loss_u loss_u 0.8979 (0.8803) acc_u 18.7500 (15.2083) lr 1.9921e-03 eta 0:00:23
epoch [10/200] batch [35/69] time 0.615 (0.592) data 0.332 (0.324) loss_u loss_u 0.8701 (0.8764) acc_u 15.6250 (15.6250) lr 1.9921e-03 eta 0:00:20
epoch [10/200] batch [40/69] time 0.568 (0.595) data 0.295 (0.326) loss_u loss_u 0.8418 (0.8738) acc_u 28.1250 (16.0938) lr 1.9921e-03 eta 0:00:17
epoch [10/200] batch [45/69] time 0.521 (0.594) data 0.253 (0.325) loss_u loss_u 0.9082 (0.8735) acc_u 18.7500 (16.1806) lr 1.9921e-03 eta 0:00:14
epoch [10/200] batch [50/69] time 0.576 (0.593) data 0.306 (0.323) loss_u loss_u 0.8672 (0.8707) acc_u 18.7500 (16.2500) lr 1.9921e-03 eta 0:00:11
epoch [10/200] batch [55/69] time 0.985 (0.598) data 0.718 (0.328) loss_u loss_u 0.9116 (0.8728) acc_u 9.3750 (15.7955) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [60/69] time 0.576 (0.598) data 0.308 (0.328) loss_u loss_u 0.8770 (0.8729) acc_u 12.5000 (15.7812) lr 1.9921e-03 eta 0:00:05
epoch [10/200] batch [65/69] time 0.555 (0.597) data 0.281 (0.328) loss_u loss_u 0.8291 (0.8718) acc_u 28.1250 (16.0096) lr 1.9921e-03 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1754
confident_label rate tensor(0.2982, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 935
clean true:894
clean false:41
clean_rate:0.9561497326203209
noisy true:488
noisy false:1713
after delete: len(clean_dataset) 935
after delete: len(noisy_dataset) 2201
epoch [11/200] batch [5/29] time 0.840 (0.661) data 0.569 (0.387) loss_x loss_x 2.2109 (1.6748) acc_x 43.7500 (56.8750) lr 1.9900e-03 eta 0:00:15
epoch [11/200] batch [10/29] time 0.767 (0.645) data 0.489 (0.370) loss_x loss_x 1.1074 (1.5961) acc_x 68.7500 (58.4375) lr 1.9900e-03 eta 0:00:12
epoch [11/200] batch [15/29] time 0.661 (0.645) data 0.381 (0.369) loss_x loss_x 1.2314 (1.5116) acc_x 62.5000 (60.0000) lr 1.9900e-03 eta 0:00:09
epoch [11/200] batch [20/29] time 0.607 (0.643) data 0.342 (0.369) loss_x loss_x 1.7021 (1.5608) acc_x 62.5000 (58.5938) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [25/29] time 0.604 (0.643) data 0.332 (0.371) loss_x loss_x 1.7637 (1.5630) acc_x 40.6250 (58.3750) lr 1.9900e-03 eta 0:00:02
epoch [11/200] batch [5/68] time 0.458 (0.633) data 0.187 (0.361) loss_u loss_u 0.9009 (0.8993) acc_u 15.6250 (11.2500) lr 1.9900e-03 eta 0:00:39
epoch [11/200] batch [10/68] time 0.585 (0.627) data 0.314 (0.354) loss_u loss_u 0.8613 (0.8815) acc_u 15.6250 (14.3750) lr 1.9900e-03 eta 0:00:36
epoch [11/200] batch [15/68] time 0.476 (0.619) data 0.342 (0.348) loss_u loss_u 0.8184 (0.8721) acc_u 15.6250 (15.2083) lr 1.9900e-03 eta 0:00:32
epoch [11/200] batch [20/68] time 0.670 (0.624) data 0.398 (0.353) loss_u loss_u 0.8916 (0.8721) acc_u 9.3750 (15.4688) lr 1.9900e-03 eta 0:00:29
epoch [11/200] batch [25/68] time 0.563 (0.617) data 0.285 (0.346) loss_u loss_u 0.8696 (0.8738) acc_u 18.7500 (16.0000) lr 1.9900e-03 eta 0:00:26
epoch [11/200] batch [30/68] time 0.450 (0.611) data 0.175 (0.342) loss_u loss_u 0.8599 (0.8715) acc_u 18.7500 (16.5625) lr 1.9900e-03 eta 0:00:23
epoch [11/200] batch [35/68] time 0.565 (0.608) data 0.285 (0.338) loss_u loss_u 0.9263 (0.8718) acc_u 3.1250 (16.3393) lr 1.9900e-03 eta 0:00:20
epoch [11/200] batch [40/68] time 0.545 (0.605) data 0.269 (0.336) loss_u loss_u 0.7983 (0.8697) acc_u 21.8750 (16.5625) lr 1.9900e-03 eta 0:00:16
epoch [11/200] batch [45/68] time 0.591 (0.602) data 0.323 (0.333) loss_u loss_u 0.8711 (0.8725) acc_u 15.6250 (16.3889) lr 1.9900e-03 eta 0:00:13
epoch [11/200] batch [50/68] time 0.617 (0.601) data 0.348 (0.332) loss_u loss_u 0.8687 (0.8739) acc_u 15.6250 (16.3750) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [55/68] time 0.655 (0.603) data 0.377 (0.333) loss_u loss_u 0.9224 (0.8730) acc_u 6.2500 (16.3068) lr 1.9900e-03 eta 0:00:07
epoch [11/200] batch [60/68] time 0.590 (0.602) data 0.316 (0.334) loss_u loss_u 0.8491 (0.8754) acc_u 15.6250 (15.8854) lr 1.9900e-03 eta 0:00:04
epoch [11/200] batch [65/68] time 0.499 (0.602) data 0.224 (0.333) loss_u loss_u 0.8301 (0.8739) acc_u 15.6250 (15.8173) lr 1.9900e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1739
confident_label rate tensor(0.3036, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 952
clean true:904
clean false:48
clean_rate:0.9495798319327731
noisy true:493
noisy false:1691
after delete: len(clean_dataset) 952
after delete: len(noisy_dataset) 2184
epoch [12/200] batch [5/29] time 0.734 (0.641) data 0.459 (0.364) loss_x loss_x 1.6836 (1.2898) acc_x 53.1250 (66.2500) lr 1.9877e-03 eta 0:00:15
epoch [12/200] batch [10/29] time 0.553 (0.607) data 0.282 (0.332) loss_x loss_x 1.5273 (1.5765) acc_x 56.2500 (56.5625) lr 1.9877e-03 eta 0:00:11
epoch [12/200] batch [15/29] time 0.674 (0.625) data 0.397 (0.351) loss_x loss_x 1.7285 (1.5508) acc_x 53.1250 (59.1667) lr 1.9877e-03 eta 0:00:08
epoch [12/200] batch [20/29] time 0.616 (0.622) data 0.418 (0.352) loss_x loss_x 1.5742 (1.5363) acc_x 56.2500 (59.2188) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [25/29] time 0.651 (0.623) data 0.379 (0.353) loss_x loss_x 1.3926 (1.5296) acc_x 65.6250 (59.8750) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [5/68] time 0.554 (0.613) data 0.282 (0.342) loss_u loss_u 0.8345 (0.8583) acc_u 25.0000 (20.6250) lr 1.9877e-03 eta 0:00:38
epoch [12/200] batch [10/68] time 0.644 (0.599) data 0.512 (0.342) loss_u loss_u 0.8799 (0.8750) acc_u 15.6250 (16.8750) lr 1.9877e-03 eta 0:00:34
epoch [12/200] batch [15/68] time 0.477 (0.588) data 0.346 (0.345) loss_u loss_u 0.7876 (0.8644) acc_u 28.1250 (18.5417) lr 1.9877e-03 eta 0:00:31
epoch [12/200] batch [20/68] time 0.528 (0.572) data 0.398 (0.340) loss_u loss_u 0.8574 (0.8655) acc_u 21.8750 (18.2812) lr 1.9877e-03 eta 0:00:27
epoch [12/200] batch [25/68] time 0.399 (0.553) data 0.267 (0.331) loss_u loss_u 0.8770 (0.8647) acc_u 12.5000 (18.1250) lr 1.9877e-03 eta 0:00:23
epoch [12/200] batch [30/68] time 0.350 (0.540) data 0.218 (0.326) loss_u loss_u 0.9546 (0.8677) acc_u 6.2500 (18.2292) lr 1.9877e-03 eta 0:00:20
epoch [12/200] batch [35/68] time 0.463 (0.533) data 0.331 (0.325) loss_u loss_u 0.8940 (0.8689) acc_u 15.6250 (18.1250) lr 1.9877e-03 eta 0:00:17
epoch [12/200] batch [40/68] time 0.513 (0.527) data 0.381 (0.324) loss_u loss_u 0.9141 (0.8682) acc_u 9.3750 (17.7344) lr 1.9877e-03 eta 0:00:14
epoch [12/200] batch [45/68] time 0.431 (0.521) data 0.298 (0.324) loss_u loss_u 0.8896 (0.8701) acc_u 9.3750 (17.3611) lr 1.9877e-03 eta 0:00:11
epoch [12/200] batch [50/68] time 0.375 (0.519) data 0.243 (0.325) loss_u loss_u 0.8472 (0.8704) acc_u 21.8750 (16.9375) lr 1.9877e-03 eta 0:00:09
epoch [12/200] batch [55/68] time 0.416 (0.513) data 0.285 (0.323) loss_u loss_u 0.8423 (0.8718) acc_u 15.6250 (16.5909) lr 1.9877e-03 eta 0:00:06
epoch [12/200] batch [60/68] time 0.355 (0.508) data 0.224 (0.322) loss_u loss_u 0.8628 (0.8726) acc_u 18.7500 (16.3021) lr 1.9877e-03 eta 0:00:04
epoch [12/200] batch [65/68] time 0.452 (0.506) data 0.321 (0.323) loss_u loss_u 0.9028 (0.8737) acc_u 9.3750 (16.0577) lr 1.9877e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1772
confident_label rate tensor(0.2966, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 930
clean true:881
clean false:49
clean_rate:0.9473118279569892
noisy true:483
noisy false:1723
after delete: len(clean_dataset) 930
after delete: len(noisy_dataset) 2206
epoch [13/200] batch [5/29] time 0.342 (0.460) data 0.211 (0.329) loss_x loss_x 1.5176 (1.3562) acc_x 46.8750 (64.3750) lr 1.9851e-03 eta 0:00:11
epoch [13/200] batch [10/29] time 0.333 (0.438) data 0.202 (0.307) loss_x loss_x 1.4707 (1.3845) acc_x 59.3750 (63.7500) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [15/29] time 0.456 (0.442) data 0.324 (0.311) loss_x loss_x 1.0918 (1.3486) acc_x 68.7500 (64.5833) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [20/29] time 0.489 (0.455) data 0.359 (0.324) loss_x loss_x 1.4102 (1.4004) acc_x 56.2500 (62.9688) lr 1.9851e-03 eta 0:00:04
epoch [13/200] batch [25/29] time 0.552 (0.459) data 0.421 (0.328) loss_x loss_x 1.4404 (1.4396) acc_x 68.7500 (62.1250) lr 1.9851e-03 eta 0:00:01
epoch [13/200] batch [5/68] time 0.531 (0.466) data 0.399 (0.335) loss_u loss_u 0.8730 (0.8473) acc_u 18.7500 (18.1250) lr 1.9851e-03 eta 0:00:29
epoch [13/200] batch [10/68] time 0.401 (0.463) data 0.271 (0.332) loss_u loss_u 0.9209 (0.8692) acc_u 9.3750 (15.0000) lr 1.9851e-03 eta 0:00:26
epoch [13/200] batch [15/68] time 0.378 (0.453) data 0.246 (0.322) loss_u loss_u 0.8794 (0.8640) acc_u 18.7500 (16.2500) lr 1.9851e-03 eta 0:00:24
epoch [13/200] batch [20/68] time 0.492 (0.455) data 0.361 (0.324) loss_u loss_u 0.8896 (0.8719) acc_u 21.8750 (16.4062) lr 1.9851e-03 eta 0:00:21
epoch [13/200] batch [25/68] time 0.415 (0.450) data 0.284 (0.319) loss_u loss_u 0.8711 (0.8722) acc_u 18.7500 (16.6250) lr 1.9851e-03 eta 0:00:19
epoch [13/200] batch [30/68] time 0.388 (0.449) data 0.257 (0.318) loss_u loss_u 0.9419 (0.8770) acc_u 6.2500 (15.7292) lr 1.9851e-03 eta 0:00:17
epoch [13/200] batch [35/68] time 0.369 (0.444) data 0.238 (0.313) loss_u loss_u 0.8525 (0.8778) acc_u 15.6250 (15.3571) lr 1.9851e-03 eta 0:00:14
epoch [13/200] batch [40/68] time 0.545 (0.442) data 0.413 (0.311) loss_u loss_u 0.7886 (0.8728) acc_u 34.3750 (16.2500) lr 1.9851e-03 eta 0:00:12
epoch [13/200] batch [45/68] time 0.428 (0.441) data 0.296 (0.310) loss_u loss_u 0.8770 (0.8730) acc_u 21.8750 (16.1806) lr 1.9851e-03 eta 0:00:10
epoch [13/200] batch [50/68] time 0.347 (0.440) data 0.217 (0.309) loss_u loss_u 0.8613 (0.8727) acc_u 15.6250 (16.0000) lr 1.9851e-03 eta 0:00:07
epoch [13/200] batch [55/68] time 0.403 (0.439) data 0.272 (0.308) loss_u loss_u 0.8491 (0.8720) acc_u 21.8750 (16.3068) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [60/68] time 0.380 (0.440) data 0.248 (0.309) loss_u loss_u 0.9116 (0.8737) acc_u 3.1250 (15.8854) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [65/68] time 0.384 (0.441) data 0.254 (0.310) loss_u loss_u 0.9263 (0.8733) acc_u 6.2500 (16.2019) lr 1.9851e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1743
confident_label rate tensor(0.3026, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 949
clean true:896
clean false:53
clean_rate:0.9441517386722866
noisy true:497
noisy false:1690
after delete: len(clean_dataset) 949
after delete: len(noisy_dataset) 2187
epoch [14/200] batch [5/29] time 0.420 (0.427) data 0.289 (0.296) loss_x loss_x 1.9248 (1.7379) acc_x 59.3750 (59.3750) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [10/29] time 0.578 (0.457) data 0.448 (0.327) loss_x loss_x 1.3975 (1.6739) acc_x 59.3750 (58.7500) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [15/29] time 0.378 (0.446) data 0.247 (0.316) loss_x loss_x 1.4766 (1.6104) acc_x 59.3750 (59.5833) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [20/29] time 0.436 (0.450) data 0.305 (0.320) loss_x loss_x 1.0693 (1.5737) acc_x 71.8750 (60.4688) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [25/29] time 0.488 (0.461) data 0.358 (0.330) loss_x loss_x 1.4209 (1.5752) acc_x 56.2500 (60.0000) lr 1.9823e-03 eta 0:00:01
epoch [14/200] batch [5/68] time 0.465 (0.462) data 0.334 (0.331) loss_u loss_u 0.8896 (0.8803) acc_u 9.3750 (11.2500) lr 1.9823e-03 eta 0:00:29
epoch [14/200] batch [10/68] time 0.471 (0.460) data 0.340 (0.330) loss_u loss_u 0.9150 (0.8750) acc_u 9.3750 (13.1250) lr 1.9823e-03 eta 0:00:26
epoch [14/200] batch [15/68] time 0.397 (0.467) data 0.266 (0.336) loss_u loss_u 0.7729 (0.8668) acc_u 34.3750 (16.0417) lr 1.9823e-03 eta 0:00:24
epoch [14/200] batch [20/68] time 0.576 (0.464) data 0.444 (0.333) loss_u loss_u 0.8696 (0.8717) acc_u 21.8750 (15.9375) lr 1.9823e-03 eta 0:00:22
epoch [14/200] batch [25/68] time 0.530 (0.458) data 0.399 (0.327) loss_u loss_u 0.8481 (0.8649) acc_u 18.7500 (16.8750) lr 1.9823e-03 eta 0:00:19
epoch [14/200] batch [30/68] time 0.454 (0.456) data 0.323 (0.325) loss_u loss_u 0.8755 (0.8697) acc_u 15.6250 (16.1458) lr 1.9823e-03 eta 0:00:17
epoch [14/200] batch [35/68] time 0.607 (0.459) data 0.472 (0.328) loss_u loss_u 0.8848 (0.8734) acc_u 6.2500 (15.6250) lr 1.9823e-03 eta 0:00:15
epoch [14/200] batch [40/68] time 0.414 (0.461) data 0.283 (0.330) loss_u loss_u 0.8330 (0.8720) acc_u 15.6250 (15.4688) lr 1.9823e-03 eta 0:00:12
epoch [14/200] batch [45/68] time 0.404 (0.462) data 0.272 (0.330) loss_u loss_u 0.8926 (0.8741) acc_u 9.3750 (15.1389) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [50/68] time 0.406 (0.458) data 0.274 (0.327) loss_u loss_u 0.8867 (0.8758) acc_u 18.7500 (15.1250) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [55/68] time 0.528 (0.456) data 0.397 (0.325) loss_u loss_u 0.8896 (0.8752) acc_u 12.5000 (15.2273) lr 1.9823e-03 eta 0:00:05
epoch [14/200] batch [60/68] time 0.416 (0.456) data 0.285 (0.324) loss_u loss_u 0.8628 (0.8753) acc_u 18.7500 (15.3125) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [65/68] time 0.313 (0.454) data 0.182 (0.322) loss_u loss_u 0.8662 (0.8729) acc_u 21.8750 (15.7692) lr 1.9823e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1736
confident_label rate tensor(0.3125, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 980
clean true:924
clean false:56
clean_rate:0.9428571428571428
noisy true:476
noisy false:1680
after delete: len(clean_dataset) 980
after delete: len(noisy_dataset) 2156
epoch [15/200] batch [5/30] time 0.497 (0.446) data 0.366 (0.315) loss_x loss_x 1.7930 (1.7000) acc_x 56.2500 (53.7500) lr 1.9792e-03 eta 0:00:11
epoch [15/200] batch [10/30] time 0.383 (0.467) data 0.252 (0.336) loss_x loss_x 1.1260 (1.5397) acc_x 68.7500 (59.6875) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [15/30] time 0.475 (0.456) data 0.344 (0.325) loss_x loss_x 1.2979 (1.5303) acc_x 65.6250 (59.5833) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [20/30] time 0.462 (0.454) data 0.331 (0.323) loss_x loss_x 1.1152 (1.4866) acc_x 78.1250 (60.9375) lr 1.9792e-03 eta 0:00:04
epoch [15/200] batch [25/30] time 0.497 (0.467) data 0.366 (0.336) loss_x loss_x 1.6416 (1.4783) acc_x 56.2500 (60.7500) lr 1.9792e-03 eta 0:00:02
epoch [15/200] batch [30/30] time 0.506 (0.471) data 0.375 (0.340) loss_x loss_x 1.2490 (1.4570) acc_x 68.7500 (61.5625) lr 1.9792e-03 eta 0:00:00
epoch [15/200] batch [5/67] time 0.458 (0.468) data 0.326 (0.337) loss_u loss_u 0.8672 (0.8725) acc_u 21.8750 (18.7500) lr 1.9792e-03 eta 0:00:29
epoch [15/200] batch [10/67] time 0.457 (0.475) data 0.325 (0.344) loss_u loss_u 0.8911 (0.8659) acc_u 15.6250 (18.1250) lr 1.9792e-03 eta 0:00:27
epoch [15/200] batch [15/67] time 0.482 (0.469) data 0.350 (0.338) loss_u loss_u 0.8462 (0.8639) acc_u 21.8750 (18.1250) lr 1.9792e-03 eta 0:00:24
epoch [15/200] batch [20/67] time 0.464 (0.465) data 0.332 (0.334) loss_u loss_u 0.9346 (0.8693) acc_u 3.1250 (17.1875) lr 1.9792e-03 eta 0:00:21
epoch [15/200] batch [25/67] time 0.568 (0.466) data 0.437 (0.334) loss_u loss_u 0.9072 (0.8696) acc_u 12.5000 (17.0000) lr 1.9792e-03 eta 0:00:19
epoch [15/200] batch [30/67] time 0.364 (0.462) data 0.233 (0.331) loss_u loss_u 0.8896 (0.8651) acc_u 15.6250 (17.5000) lr 1.9792e-03 eta 0:00:17
epoch [15/200] batch [35/67] time 0.585 (0.461) data 0.454 (0.330) loss_u loss_u 0.8560 (0.8665) acc_u 18.7500 (16.9643) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [40/67] time 0.516 (0.463) data 0.383 (0.331) loss_u loss_u 0.8828 (0.8692) acc_u 12.5000 (16.5625) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [45/67] time 0.457 (0.459) data 0.325 (0.327) loss_u loss_u 0.8633 (0.8731) acc_u 18.7500 (16.0417) lr 1.9792e-03 eta 0:00:10
epoch [15/200] batch [50/67] time 0.531 (0.460) data 0.400 (0.328) loss_u loss_u 0.8188 (0.8731) acc_u 21.8750 (15.7500) lr 1.9792e-03 eta 0:00:07
epoch [15/200] batch [55/67] time 0.363 (0.458) data 0.232 (0.327) loss_u loss_u 0.9082 (0.8743) acc_u 15.6250 (15.6818) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [60/67] time 0.344 (0.456) data 0.213 (0.324) loss_u loss_u 0.8335 (0.8741) acc_u 21.8750 (15.6250) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [65/67] time 0.618 (0.457) data 0.487 (0.326) loss_u loss_u 0.8584 (0.8734) acc_u 15.6250 (15.7212) lr 1.9792e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1706
confident_label rate tensor(0.3151, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 988
clean true:929
clean false:59
clean_rate:0.9402834008097166
noisy true:501
noisy false:1647
after delete: len(clean_dataset) 988
after delete: len(noisy_dataset) 2148
epoch [16/200] batch [5/30] time 0.368 (0.401) data 0.238 (0.270) loss_x loss_x 0.9639 (1.3893) acc_x 59.3750 (58.7500) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [10/30] time 0.480 (0.431) data 0.350 (0.301) loss_x loss_x 1.1309 (1.3987) acc_x 68.7500 (62.8125) lr 1.9759e-03 eta 0:00:08
epoch [16/200] batch [15/30] time 0.558 (0.454) data 0.428 (0.324) loss_x loss_x 1.2354 (1.4466) acc_x 59.3750 (61.2500) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [20/30] time 0.615 (0.456) data 0.485 (0.326) loss_x loss_x 1.5049 (1.4265) acc_x 62.5000 (61.5625) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [25/30] time 0.408 (0.458) data 0.278 (0.327) loss_x loss_x 1.6494 (1.4968) acc_x 62.5000 (60.3750) lr 1.9759e-03 eta 0:00:02
epoch [16/200] batch [30/30] time 0.431 (0.458) data 0.300 (0.327) loss_x loss_x 1.9219 (1.5186) acc_x 50.0000 (60.5208) lr 1.9759e-03 eta 0:00:00
epoch [16/200] batch [5/67] time 0.717 (0.462) data 0.586 (0.331) loss_u loss_u 0.8882 (0.8969) acc_u 15.6250 (11.2500) lr 1.9759e-03 eta 0:00:28
epoch [16/200] batch [10/67] time 0.377 (0.454) data 0.246 (0.324) loss_u loss_u 0.8267 (0.8881) acc_u 25.0000 (13.4375) lr 1.9759e-03 eta 0:00:25
epoch [16/200] batch [15/67] time 0.472 (0.453) data 0.340 (0.323) loss_u loss_u 0.9214 (0.8895) acc_u 15.6250 (13.7500) lr 1.9759e-03 eta 0:00:23
epoch [16/200] batch [20/67] time 0.452 (0.454) data 0.320 (0.323) loss_u loss_u 0.8823 (0.8859) acc_u 12.5000 (14.2188) lr 1.9759e-03 eta 0:00:21
epoch [16/200] batch [25/67] time 0.342 (0.452) data 0.210 (0.321) loss_u loss_u 0.9258 (0.8864) acc_u 15.6250 (14.0000) lr 1.9759e-03 eta 0:00:18
epoch [16/200] batch [30/67] time 0.351 (0.448) data 0.219 (0.317) loss_u loss_u 0.8804 (0.8804) acc_u 9.3750 (14.4792) lr 1.9759e-03 eta 0:00:16
epoch [16/200] batch [35/67] time 0.479 (0.449) data 0.347 (0.317) loss_u loss_u 0.8823 (0.8821) acc_u 18.7500 (14.5536) lr 1.9759e-03 eta 0:00:14
epoch [16/200] batch [40/67] time 0.416 (0.447) data 0.285 (0.315) loss_u loss_u 0.6934 (0.8788) acc_u 37.5000 (15.0000) lr 1.9759e-03 eta 0:00:12
epoch [16/200] batch [45/67] time 0.462 (0.450) data 0.331 (0.319) loss_u loss_u 0.8477 (0.8739) acc_u 18.7500 (15.5556) lr 1.9759e-03 eta 0:00:09
epoch [16/200] batch [50/67] time 0.408 (0.449) data 0.276 (0.318) loss_u loss_u 0.8975 (0.8755) acc_u 9.3750 (15.4375) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [55/67] time 0.384 (0.447) data 0.253 (0.316) loss_u loss_u 0.8237 (0.8722) acc_u 21.8750 (15.8523) lr 1.9759e-03 eta 0:00:05
epoch [16/200] batch [60/67] time 0.461 (0.446) data 0.330 (0.315) loss_u loss_u 0.8062 (0.8692) acc_u 21.8750 (16.3542) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [65/67] time 0.505 (0.445) data 0.374 (0.314) loss_u loss_u 0.9028 (0.8695) acc_u 6.2500 (16.1538) lr 1.9759e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1741
confident_label rate tensor(0.3042, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 954
clean true:901
clean false:53
clean_rate:0.9444444444444444
noisy true:494
noisy false:1688
after delete: len(clean_dataset) 954
after delete: len(noisy_dataset) 2182
epoch [17/200] batch [5/29] time 0.601 (0.496) data 0.469 (0.365) loss_x loss_x 1.4111 (1.3156) acc_x 62.5000 (63.7500) lr 1.9724e-03 eta 0:00:11
epoch [17/200] batch [10/29] time 0.528 (0.502) data 0.396 (0.371) loss_x loss_x 1.0898 (1.3771) acc_x 78.1250 (63.7500) lr 1.9724e-03 eta 0:00:09
epoch [17/200] batch [15/29] time 0.509 (0.497) data 0.378 (0.365) loss_x loss_x 1.4150 (1.4427) acc_x 59.3750 (62.5000) lr 1.9724e-03 eta 0:00:06
epoch [17/200] batch [20/29] time 0.559 (0.495) data 0.426 (0.363) loss_x loss_x 1.6182 (1.4590) acc_x 59.3750 (61.7188) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [25/29] time 0.391 (0.493) data 0.261 (0.362) loss_x loss_x 1.5244 (1.4996) acc_x 65.6250 (61.5000) lr 1.9724e-03 eta 0:00:01
epoch [17/200] batch [5/68] time 0.362 (0.489) data 0.230 (0.357) loss_u loss_u 0.9307 (0.8788) acc_u 6.2500 (14.3750) lr 1.9724e-03 eta 0:00:30
epoch [17/200] batch [10/68] time 0.318 (0.475) data 0.186 (0.344) loss_u loss_u 0.8125 (0.8721) acc_u 18.7500 (14.6875) lr 1.9724e-03 eta 0:00:27
epoch [17/200] batch [15/68] time 0.445 (0.466) data 0.313 (0.334) loss_u loss_u 0.9102 (0.8721) acc_u 9.3750 (14.7917) lr 1.9724e-03 eta 0:00:24
epoch [17/200] batch [20/68] time 0.371 (0.462) data 0.239 (0.331) loss_u loss_u 0.9072 (0.8776) acc_u 12.5000 (14.3750) lr 1.9724e-03 eta 0:00:22
epoch [17/200] batch [25/68] time 0.370 (0.459) data 0.240 (0.327) loss_u loss_u 0.8184 (0.8788) acc_u 18.7500 (14.5000) lr 1.9724e-03 eta 0:00:19
epoch [17/200] batch [30/68] time 0.382 (0.457) data 0.251 (0.326) loss_u loss_u 0.8848 (0.8762) acc_u 12.5000 (15.1042) lr 1.9724e-03 eta 0:00:17
epoch [17/200] batch [35/68] time 0.457 (0.454) data 0.325 (0.323) loss_u loss_u 0.8589 (0.8758) acc_u 21.8750 (15.8929) lr 1.9724e-03 eta 0:00:14
epoch [17/200] batch [40/68] time 0.454 (0.454) data 0.323 (0.322) loss_u loss_u 0.8003 (0.8736) acc_u 21.8750 (16.0156) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [45/68] time 0.378 (0.453) data 0.247 (0.321) loss_u loss_u 0.9170 (0.8726) acc_u 9.3750 (15.9028) lr 1.9724e-03 eta 0:00:10
epoch [17/200] batch [50/68] time 0.402 (0.452) data 0.270 (0.321) loss_u loss_u 0.9087 (0.8710) acc_u 12.5000 (16.1250) lr 1.9724e-03 eta 0:00:08
epoch [17/200] batch [55/68] time 0.634 (0.453) data 0.502 (0.321) loss_u loss_u 0.8716 (0.8699) acc_u 9.3750 (15.7955) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [60/68] time 0.413 (0.452) data 0.281 (0.320) loss_u loss_u 0.8613 (0.8673) acc_u 21.8750 (15.9375) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [65/68] time 0.361 (0.452) data 0.229 (0.320) loss_u loss_u 0.9351 (0.8704) acc_u 9.3750 (15.5288) lr 1.9724e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1733
confident_label rate tensor(0.3080, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 966
clean true:909
clean false:57
clean_rate:0.9409937888198758
noisy true:494
noisy false:1676
after delete: len(clean_dataset) 966
after delete: len(noisy_dataset) 2170
epoch [18/200] batch [5/30] time 0.544 (0.479) data 0.412 (0.348) loss_x loss_x 1.7188 (1.6396) acc_x 43.7500 (59.3750) lr 1.9686e-03 eta 0:00:11
epoch [18/200] batch [10/30] time 0.470 (0.447) data 0.337 (0.316) loss_x loss_x 1.2979 (1.5172) acc_x 62.5000 (61.8750) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [15/30] time 0.684 (0.466) data 0.554 (0.335) loss_x loss_x 1.5996 (1.5257) acc_x 59.3750 (61.2500) lr 1.9686e-03 eta 0:00:06
epoch [18/200] batch [20/30] time 0.459 (0.455) data 0.328 (0.324) loss_x loss_x 1.6533 (1.4966) acc_x 62.5000 (61.8750) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [25/30] time 0.472 (0.452) data 0.342 (0.322) loss_x loss_x 1.5264 (1.5394) acc_x 59.3750 (60.8750) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [30/30] time 0.505 (0.468) data 0.374 (0.337) loss_x loss_x 1.6943 (1.5574) acc_x 56.2500 (60.5208) lr 1.9686e-03 eta 0:00:00
epoch [18/200] batch [5/67] time 0.465 (0.469) data 0.334 (0.338) loss_u loss_u 0.8901 (0.8742) acc_u 18.7500 (17.5000) lr 1.9686e-03 eta 0:00:29
epoch [18/200] batch [10/67] time 0.404 (0.465) data 0.272 (0.334) loss_u loss_u 0.8594 (0.8622) acc_u 12.5000 (17.5000) lr 1.9686e-03 eta 0:00:26
epoch [18/200] batch [15/67] time 0.501 (0.462) data 0.369 (0.331) loss_u loss_u 0.8716 (0.8719) acc_u 18.7500 (16.4583) lr 1.9686e-03 eta 0:00:24
epoch [18/200] batch [20/67] time 0.453 (0.464) data 0.320 (0.333) loss_u loss_u 0.8521 (0.8688) acc_u 15.6250 (16.4062) lr 1.9686e-03 eta 0:00:21
epoch [18/200] batch [25/67] time 0.491 (0.466) data 0.357 (0.334) loss_u loss_u 0.8413 (0.8669) acc_u 21.8750 (17.2500) lr 1.9686e-03 eta 0:00:19
epoch [18/200] batch [30/67] time 0.401 (0.460) data 0.269 (0.328) loss_u loss_u 0.8481 (0.8626) acc_u 18.7500 (17.7083) lr 1.9686e-03 eta 0:00:17
epoch [18/200] batch [35/67] time 0.375 (0.455) data 0.244 (0.324) loss_u loss_u 0.9258 (0.8645) acc_u 9.3750 (17.6786) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [40/67] time 0.417 (0.459) data 0.286 (0.328) loss_u loss_u 0.8691 (0.8636) acc_u 18.7500 (17.8906) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [45/67] time 0.447 (0.458) data 0.316 (0.327) loss_u loss_u 0.8511 (0.8666) acc_u 12.5000 (17.0139) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [50/67] time 0.489 (0.458) data 0.356 (0.327) loss_u loss_u 0.7949 (0.8641) acc_u 21.8750 (17.2500) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [55/67] time 0.426 (0.458) data 0.294 (0.327) loss_u loss_u 0.8809 (0.8651) acc_u 15.6250 (17.2727) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [60/67] time 0.454 (0.454) data 0.323 (0.323) loss_u loss_u 0.8501 (0.8636) acc_u 15.6250 (17.6042) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [65/67] time 0.403 (0.453) data 0.271 (0.322) loss_u loss_u 0.8667 (0.8622) acc_u 15.6250 (17.8365) lr 1.9686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1707
confident_label rate tensor(0.3125, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 980
clean true:929
clean false:51
clean_rate:0.9479591836734694
noisy true:500
noisy false:1656
after delete: len(clean_dataset) 980
after delete: len(noisy_dataset) 2156
epoch [19/200] batch [5/30] time 0.467 (0.447) data 0.336 (0.316) loss_x loss_x 1.9277 (1.5119) acc_x 62.5000 (66.2500) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [10/30] time 0.416 (0.466) data 0.286 (0.335) loss_x loss_x 1.5586 (1.6606) acc_x 68.7500 (63.7500) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [15/30] time 0.466 (0.463) data 0.334 (0.332) loss_x loss_x 1.2031 (1.5050) acc_x 68.7500 (65.0000) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [20/30] time 0.439 (0.474) data 0.308 (0.343) loss_x loss_x 1.6494 (1.5475) acc_x 59.3750 (63.7500) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [25/30] time 0.424 (0.477) data 0.293 (0.346) loss_x loss_x 1.7891 (1.5382) acc_x 50.0000 (62.0000) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [30/30] time 0.341 (0.471) data 0.210 (0.340) loss_x loss_x 1.4580 (1.5249) acc_x 59.3750 (62.1875) lr 1.9646e-03 eta 0:00:00
epoch [19/200] batch [5/67] time 0.484 (0.466) data 0.352 (0.335) loss_u loss_u 0.8203 (0.8609) acc_u 25.0000 (19.3750) lr 1.9646e-03 eta 0:00:28
epoch [19/200] batch [10/67] time 0.377 (0.463) data 0.245 (0.332) loss_u loss_u 0.9102 (0.8821) acc_u 9.3750 (15.3125) lr 1.9646e-03 eta 0:00:26
epoch [19/200] batch [15/67] time 0.404 (0.461) data 0.272 (0.330) loss_u loss_u 0.8511 (0.8686) acc_u 25.0000 (17.0833) lr 1.9646e-03 eta 0:00:23
epoch [19/200] batch [20/67] time 0.741 (0.463) data 0.610 (0.331) loss_u loss_u 0.8276 (0.8654) acc_u 15.6250 (17.9688) lr 1.9646e-03 eta 0:00:21
epoch [19/200] batch [25/67] time 0.367 (0.462) data 0.235 (0.330) loss_u loss_u 0.8291 (0.8623) acc_u 25.0000 (18.5000) lr 1.9646e-03 eta 0:00:19
epoch [19/200] batch [30/67] time 0.471 (0.463) data 0.339 (0.331) loss_u loss_u 0.8813 (0.8667) acc_u 15.6250 (17.7083) lr 1.9646e-03 eta 0:00:17
epoch [19/200] batch [35/67] time 0.450 (0.460) data 0.318 (0.329) loss_u loss_u 0.9160 (0.8668) acc_u 3.1250 (17.3214) lr 1.9646e-03 eta 0:00:14
epoch [19/200] batch [40/67] time 0.446 (0.460) data 0.314 (0.328) loss_u loss_u 0.8579 (0.8614) acc_u 12.5000 (17.8906) lr 1.9646e-03 eta 0:00:12
epoch [19/200] batch [45/67] time 0.489 (0.458) data 0.357 (0.327) loss_u loss_u 0.8726 (0.8627) acc_u 18.7500 (18.1250) lr 1.9646e-03 eta 0:00:10
epoch [19/200] batch [50/67] time 0.422 (0.454) data 0.291 (0.322) loss_u loss_u 0.8745 (0.8629) acc_u 12.5000 (18.0000) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [55/67] time 0.355 (0.453) data 0.222 (0.321) loss_u loss_u 0.8701 (0.8632) acc_u 18.7500 (17.7273) lr 1.9646e-03 eta 0:00:05
epoch [19/200] batch [60/67] time 0.449 (0.454) data 0.317 (0.323) loss_u loss_u 0.9106 (0.8657) acc_u 9.3750 (17.5000) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [65/67] time 0.589 (0.454) data 0.457 (0.323) loss_u loss_u 0.8657 (0.8657) acc_u 15.6250 (17.3558) lr 1.9646e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1726
confident_label rate tensor(0.3119, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 978
clean true:912
clean false:66
clean_rate:0.9325153374233128
noisy true:498
noisy false:1660
after delete: len(clean_dataset) 978
after delete: len(noisy_dataset) 2158
epoch [20/200] batch [5/30] time 0.364 (0.488) data 0.233 (0.358) loss_x loss_x 1.8770 (1.4914) acc_x 56.2500 (60.6250) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [10/30] time 0.354 (0.463) data 0.224 (0.332) loss_x loss_x 1.5596 (1.4327) acc_x 56.2500 (61.2500) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [15/30] time 0.534 (0.463) data 0.404 (0.332) loss_x loss_x 1.9199 (1.4430) acc_x 50.0000 (63.1250) lr 1.9603e-03 eta 0:00:06
epoch [20/200] batch [20/30] time 0.418 (0.473) data 0.287 (0.343) loss_x loss_x 1.7324 (1.5030) acc_x 56.2500 (61.0938) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [25/30] time 0.473 (0.466) data 0.341 (0.336) loss_x loss_x 1.9258 (1.5328) acc_x 62.5000 (62.0000) lr 1.9603e-03 eta 0:00:02
epoch [20/200] batch [30/30] time 0.506 (0.474) data 0.374 (0.343) loss_x loss_x 1.2891 (1.4876) acc_x 62.5000 (63.0208) lr 1.9603e-03 eta 0:00:00
epoch [20/200] batch [5/67] time 0.364 (0.463) data 0.233 (0.332) loss_u loss_u 0.7422 (0.8408) acc_u 28.1250 (18.1250) lr 1.9603e-03 eta 0:00:28
epoch [20/200] batch [10/67] time 0.513 (0.463) data 0.381 (0.332) loss_u loss_u 0.8384 (0.8664) acc_u 15.6250 (15.0000) lr 1.9603e-03 eta 0:00:26
epoch [20/200] batch [15/67] time 0.377 (0.464) data 0.245 (0.333) loss_u loss_u 0.8452 (0.8508) acc_u 18.7500 (17.2917) lr 1.9603e-03 eta 0:00:24
epoch [20/200] batch [20/67] time 0.494 (0.464) data 0.364 (0.333) loss_u loss_u 0.8696 (0.8548) acc_u 12.5000 (16.5625) lr 1.9603e-03 eta 0:00:21
epoch [20/200] batch [25/67] time 0.475 (0.459) data 0.345 (0.328) loss_u loss_u 0.9482 (0.8603) acc_u 3.1250 (16.1250) lr 1.9603e-03 eta 0:00:19
epoch [20/200] batch [30/67] time 0.419 (0.454) data 0.289 (0.323) loss_u loss_u 0.9165 (0.8689) acc_u 12.5000 (15.6250) lr 1.9603e-03 eta 0:00:16
epoch [20/200] batch [35/67] time 0.381 (0.450) data 0.250 (0.319) loss_u loss_u 0.8647 (0.8656) acc_u 18.7500 (16.4286) lr 1.9603e-03 eta 0:00:14
epoch [20/200] batch [40/67] time 0.438 (0.452) data 0.306 (0.321) loss_u loss_u 0.8896 (0.8634) acc_u 15.6250 (16.4844) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [45/67] time 0.485 (0.451) data 0.354 (0.320) loss_u loss_u 0.8857 (0.8633) acc_u 15.6250 (16.5278) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [50/67] time 0.382 (0.454) data 0.251 (0.323) loss_u loss_u 0.8745 (0.8664) acc_u 18.7500 (16.2500) lr 1.9603e-03 eta 0:00:07
epoch [20/200] batch [55/67] time 0.453 (0.455) data 0.323 (0.324) loss_u loss_u 0.8716 (0.8696) acc_u 15.6250 (15.9659) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [60/67] time 0.397 (0.455) data 0.263 (0.324) loss_u loss_u 0.8320 (0.8685) acc_u 15.6250 (16.1979) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [65/67] time 0.592 (0.455) data 0.461 (0.324) loss_u loss_u 0.8223 (0.8668) acc_u 18.7500 (16.4423) lr 1.9603e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.3068, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 962
clean true:895
clean false:67
clean_rate:0.9303534303534303
noisy true:492
noisy false:1682
after delete: len(clean_dataset) 962
after delete: len(noisy_dataset) 2174
epoch [21/200] batch [5/30] time 0.479 (0.496) data 0.347 (0.365) loss_x loss_x 1.5039 (1.4521) acc_x 56.2500 (62.5000) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [10/30] time 0.556 (0.508) data 0.424 (0.377) loss_x loss_x 1.6279 (1.4885) acc_x 59.3750 (60.9375) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [15/30] time 0.404 (0.510) data 0.273 (0.378) loss_x loss_x 1.5908 (1.5419) acc_x 62.5000 (60.6250) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [20/30] time 0.744 (0.508) data 0.613 (0.377) loss_x loss_x 2.1055 (1.5474) acc_x 40.6250 (60.3125) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [25/30] time 0.422 (0.492) data 0.291 (0.361) loss_x loss_x 1.2803 (1.5230) acc_x 75.0000 (61.1250) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [30/30] time 0.386 (0.474) data 0.255 (0.343) loss_x loss_x 0.9438 (1.5039) acc_x 81.2500 (61.7708) lr 1.9558e-03 eta 0:00:00
epoch [21/200] batch [5/67] time 0.515 (0.480) data 0.383 (0.349) loss_u loss_u 0.8311 (0.8496) acc_u 18.7500 (18.7500) lr 1.9558e-03 eta 0:00:29
epoch [21/200] batch [10/67] time 0.435 (0.474) data 0.304 (0.343) loss_u loss_u 0.9102 (0.8700) acc_u 6.2500 (15.3125) lr 1.9558e-03 eta 0:00:27
epoch [21/200] batch [15/67] time 0.449 (0.472) data 0.319 (0.341) loss_u loss_u 0.8667 (0.8757) acc_u 21.8750 (15.8333) lr 1.9558e-03 eta 0:00:24
epoch [21/200] batch [20/67] time 0.373 (0.470) data 0.242 (0.338) loss_u loss_u 0.8208 (0.8698) acc_u 25.0000 (16.4062) lr 1.9558e-03 eta 0:00:22
epoch [21/200] batch [25/67] time 0.466 (0.466) data 0.334 (0.335) loss_u loss_u 0.8740 (0.8680) acc_u 18.7500 (16.5000) lr 1.9558e-03 eta 0:00:19
epoch [21/200] batch [30/67] time 0.361 (0.460) data 0.230 (0.329) loss_u loss_u 0.8579 (0.8712) acc_u 18.7500 (16.3542) lr 1.9558e-03 eta 0:00:17
epoch [21/200] batch [35/67] time 0.384 (0.460) data 0.252 (0.329) loss_u loss_u 0.8525 (0.8723) acc_u 15.6250 (15.9821) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [40/67] time 0.504 (0.459) data 0.373 (0.328) loss_u loss_u 0.9087 (0.8712) acc_u 15.6250 (16.4844) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [45/67] time 0.455 (0.457) data 0.323 (0.326) loss_u loss_u 0.9370 (0.8719) acc_u 9.3750 (16.6667) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [50/67] time 0.364 (0.455) data 0.232 (0.324) loss_u loss_u 0.8105 (0.8700) acc_u 25.0000 (17.1250) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [55/67] time 0.397 (0.455) data 0.263 (0.324) loss_u loss_u 0.8936 (0.8699) acc_u 12.5000 (16.8750) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [60/67] time 0.454 (0.461) data 0.323 (0.330) loss_u loss_u 0.9263 (0.8714) acc_u 9.3750 (16.5104) lr 1.9558e-03 eta 0:00:03
epoch [21/200] batch [65/67] time 0.401 (0.458) data 0.269 (0.327) loss_u loss_u 0.9048 (0.8688) acc_u 9.3750 (16.8269) lr 1.9558e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1698
confident_label rate tensor(0.3240, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1016
clean true:956
clean false:60
clean_rate:0.9409448818897638
noisy true:482
noisy false:1638
after delete: len(clean_dataset) 1016
after delete: len(noisy_dataset) 2120
epoch [22/200] batch [5/31] time 0.593 (0.514) data 0.463 (0.383) loss_x loss_x 0.8096 (1.5066) acc_x 75.0000 (59.3750) lr 1.9511e-03 eta 0:00:13
epoch [22/200] batch [10/31] time 0.477 (0.467) data 0.347 (0.336) loss_x loss_x 1.0762 (1.4277) acc_x 71.8750 (61.8750) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [15/31] time 0.448 (0.453) data 0.317 (0.323) loss_x loss_x 1.4189 (1.4234) acc_x 56.2500 (59.7917) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [20/31] time 0.478 (0.452) data 0.348 (0.321) loss_x loss_x 1.1895 (1.4146) acc_x 59.3750 (60.9375) lr 1.9511e-03 eta 0:00:04
epoch [22/200] batch [25/31] time 0.415 (0.441) data 0.284 (0.310) loss_x loss_x 1.2324 (1.4711) acc_x 71.8750 (60.5000) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [30/31] time 0.434 (0.449) data 0.303 (0.318) loss_x loss_x 1.4492 (1.4524) acc_x 59.3750 (61.5625) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/66] time 0.415 (0.452) data 0.284 (0.321) loss_u loss_u 0.8457 (0.8943) acc_u 18.7500 (13.1250) lr 1.9511e-03 eta 0:00:27
epoch [22/200] batch [10/66] time 0.326 (0.450) data 0.194 (0.320) loss_u loss_u 0.8301 (0.8676) acc_u 12.5000 (15.6250) lr 1.9511e-03 eta 0:00:25
epoch [22/200] batch [15/66] time 0.418 (0.451) data 0.285 (0.320) loss_u loss_u 0.8506 (0.8666) acc_u 18.7500 (16.4583) lr 1.9511e-03 eta 0:00:22
epoch [22/200] batch [20/66] time 0.480 (0.454) data 0.348 (0.323) loss_u loss_u 0.9097 (0.8638) acc_u 12.5000 (16.8750) lr 1.9511e-03 eta 0:00:20
epoch [22/200] batch [25/66] time 0.429 (0.449) data 0.298 (0.318) loss_u loss_u 0.8345 (0.8649) acc_u 21.8750 (16.2500) lr 1.9511e-03 eta 0:00:18
epoch [22/200] batch [30/66] time 0.688 (0.451) data 0.556 (0.320) loss_u loss_u 0.8608 (0.8662) acc_u 12.5000 (15.8333) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [35/66] time 0.365 (0.454) data 0.234 (0.323) loss_u loss_u 0.8979 (0.8664) acc_u 6.2500 (15.8929) lr 1.9511e-03 eta 0:00:14
epoch [22/200] batch [40/66] time 0.428 (0.455) data 0.297 (0.324) loss_u loss_u 0.8530 (0.8687) acc_u 15.6250 (15.8594) lr 1.9511e-03 eta 0:00:11
epoch [22/200] batch [45/66] time 0.360 (0.452) data 0.230 (0.321) loss_u loss_u 0.8428 (0.8700) acc_u 18.7500 (16.1111) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [50/66] time 0.342 (0.450) data 0.210 (0.319) loss_u loss_u 0.8589 (0.8724) acc_u 18.7500 (15.9375) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [55/66] time 0.574 (0.450) data 0.443 (0.319) loss_u loss_u 0.8892 (0.8714) acc_u 15.6250 (15.8523) lr 1.9511e-03 eta 0:00:04
epoch [22/200] batch [60/66] time 0.477 (0.447) data 0.347 (0.316) loss_u loss_u 0.9409 (0.8736) acc_u 9.3750 (15.7292) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [65/66] time 0.418 (0.446) data 0.285 (0.315) loss_u loss_u 0.8760 (0.8710) acc_u 15.6250 (15.8654) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1696
confident_label rate tensor(0.3217, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1009
clean true:950
clean false:59
clean_rate:0.9415262636273538
noisy true:490
noisy false:1637
after delete: len(clean_dataset) 1009
after delete: len(noisy_dataset) 2127
epoch [23/200] batch [5/31] time 0.581 (0.436) data 0.448 (0.305) loss_x loss_x 1.0908 (1.2705) acc_x 71.8750 (66.8750) lr 1.9461e-03 eta 0:00:11
epoch [23/200] batch [10/31] time 0.425 (0.465) data 0.293 (0.333) loss_x loss_x 1.7217 (1.3483) acc_x 56.2500 (66.8750) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [15/31] time 0.414 (0.457) data 0.283 (0.326) loss_x loss_x 1.2568 (1.4479) acc_x 65.6250 (63.3333) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [20/31] time 0.447 (0.478) data 0.315 (0.346) loss_x loss_x 1.2959 (1.4292) acc_x 71.8750 (63.9062) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [25/31] time 0.397 (0.469) data 0.266 (0.338) loss_x loss_x 1.9189 (1.4532) acc_x 50.0000 (62.6250) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [30/31] time 0.412 (0.472) data 0.281 (0.341) loss_x loss_x 1.3428 (1.4226) acc_x 62.5000 (63.5417) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/66] time 0.382 (0.466) data 0.250 (0.334) loss_u loss_u 0.9141 (0.8864) acc_u 9.3750 (14.3750) lr 1.9461e-03 eta 0:00:28
epoch [23/200] batch [10/66] time 0.567 (0.465) data 0.435 (0.333) loss_u loss_u 0.8740 (0.8898) acc_u 9.3750 (13.1250) lr 1.9461e-03 eta 0:00:26
epoch [23/200] batch [15/66] time 0.502 (0.463) data 0.371 (0.332) loss_u loss_u 0.8975 (0.8832) acc_u 12.5000 (15.0000) lr 1.9461e-03 eta 0:00:23
epoch [23/200] batch [20/66] time 0.448 (0.464) data 0.317 (0.333) loss_u loss_u 0.9243 (0.8790) acc_u 12.5000 (15.4688) lr 1.9461e-03 eta 0:00:21
epoch [23/200] batch [25/66] time 0.420 (0.470) data 0.288 (0.339) loss_u loss_u 0.8760 (0.8804) acc_u 9.3750 (15.1250) lr 1.9461e-03 eta 0:00:19
epoch [23/200] batch [30/66] time 0.474 (0.469) data 0.342 (0.337) loss_u loss_u 0.8799 (0.8748) acc_u 18.7500 (15.8333) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [35/66] time 0.595 (0.468) data 0.464 (0.337) loss_u loss_u 0.8774 (0.8725) acc_u 25.0000 (16.2500) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [40/66] time 0.455 (0.470) data 0.323 (0.338) loss_u loss_u 0.8853 (0.8734) acc_u 15.6250 (16.2500) lr 1.9461e-03 eta 0:00:12
epoch [23/200] batch [45/66] time 0.462 (0.466) data 0.329 (0.335) loss_u loss_u 0.8965 (0.8705) acc_u 12.5000 (16.4583) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [50/66] time 0.385 (0.463) data 0.252 (0.332) loss_u loss_u 0.7974 (0.8683) acc_u 28.1250 (16.9375) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [55/66] time 0.337 (0.461) data 0.205 (0.329) loss_u loss_u 0.8706 (0.8677) acc_u 18.7500 (16.9886) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [60/66] time 0.522 (0.461) data 0.390 (0.330) loss_u loss_u 0.8721 (0.8682) acc_u 18.7500 (16.9792) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [65/66] time 0.516 (0.461) data 0.385 (0.329) loss_u loss_u 0.9009 (0.8690) acc_u 12.5000 (16.8750) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1714
confident_label rate tensor(0.3217, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1009
clean true:944
clean false:65
clean_rate:0.9355797819623389
noisy true:478
noisy false:1649
after delete: len(clean_dataset) 1009
after delete: len(noisy_dataset) 2127
epoch [24/200] batch [5/31] time 0.432 (0.413) data 0.302 (0.283) loss_x loss_x 1.4834 (1.4369) acc_x 65.6250 (63.1250) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [10/31] time 0.377 (0.425) data 0.246 (0.294) loss_x loss_x 1.2627 (1.3135) acc_x 65.6250 (65.6250) lr 1.9409e-03 eta 0:00:08
epoch [24/200] batch [15/31] time 0.470 (0.444) data 0.340 (0.314) loss_x loss_x 1.3975 (1.3235) acc_x 65.6250 (65.6250) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [20/31] time 0.433 (0.454) data 0.303 (0.324) loss_x loss_x 1.3369 (1.4390) acc_x 59.3750 (62.9688) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [25/31] time 0.391 (0.449) data 0.261 (0.319) loss_x loss_x 1.5811 (1.4166) acc_x 59.3750 (63.6250) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [30/31] time 0.443 (0.454) data 0.313 (0.323) loss_x loss_x 0.9634 (1.3844) acc_x 68.7500 (64.0625) lr 1.9409e-03 eta 0:00:00
epoch [24/200] batch [5/66] time 0.459 (0.451) data 0.326 (0.320) loss_u loss_u 0.8281 (0.8595) acc_u 18.7500 (16.8750) lr 1.9409e-03 eta 0:00:27
epoch [24/200] batch [10/66] time 0.508 (0.455) data 0.377 (0.324) loss_u loss_u 0.8354 (0.8577) acc_u 21.8750 (17.1875) lr 1.9409e-03 eta 0:00:25
epoch [24/200] batch [15/66] time 0.503 (0.455) data 0.372 (0.324) loss_u loss_u 0.9180 (0.8685) acc_u 9.3750 (16.2500) lr 1.9409e-03 eta 0:00:23
epoch [24/200] batch [20/66] time 0.439 (0.451) data 0.308 (0.320) loss_u loss_u 0.8379 (0.8615) acc_u 21.8750 (17.5000) lr 1.9409e-03 eta 0:00:20
epoch [24/200] batch [25/66] time 0.534 (0.450) data 0.402 (0.319) loss_u loss_u 0.8364 (0.8636) acc_u 21.8750 (17.1250) lr 1.9409e-03 eta 0:00:18
epoch [24/200] batch [30/66] time 0.475 (0.452) data 0.344 (0.321) loss_u loss_u 0.8516 (0.8581) acc_u 15.6250 (17.7083) lr 1.9409e-03 eta 0:00:16
epoch [24/200] batch [35/66] time 0.417 (0.450) data 0.286 (0.319) loss_u loss_u 0.9033 (0.8571) acc_u 9.3750 (17.7679) lr 1.9409e-03 eta 0:00:13
epoch [24/200] batch [40/66] time 0.432 (0.449) data 0.300 (0.318) loss_u loss_u 0.8110 (0.8601) acc_u 25.0000 (17.5781) lr 1.9409e-03 eta 0:00:11
epoch [24/200] batch [45/66] time 0.439 (0.447) data 0.308 (0.316) loss_u loss_u 0.8784 (0.8636) acc_u 18.7500 (17.2222) lr 1.9409e-03 eta 0:00:09
epoch [24/200] batch [50/66] time 0.634 (0.448) data 0.502 (0.317) loss_u loss_u 0.8311 (0.8626) acc_u 21.8750 (17.3125) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [55/66] time 0.428 (0.446) data 0.298 (0.315) loss_u loss_u 0.8765 (0.8625) acc_u 15.6250 (17.2159) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [60/66] time 0.398 (0.447) data 0.266 (0.316) loss_u loss_u 0.8184 (0.8620) acc_u 15.6250 (17.1875) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [65/66] time 0.379 (0.449) data 0.249 (0.318) loss_u loss_u 0.8687 (0.8639) acc_u 25.0000 (17.3077) lr 1.9409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1714
confident_label rate tensor(0.3192, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1001
clean true:928
clean false:73
clean_rate:0.9270729270729271
noisy true:494
noisy false:1641
after delete: len(clean_dataset) 1001
after delete: len(noisy_dataset) 2135
epoch [25/200] batch [5/31] time 0.564 (0.476) data 0.434 (0.346) loss_x loss_x 1.1611 (1.5182) acc_x 68.7500 (57.5000) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [10/31] time 0.475 (0.500) data 0.344 (0.369) loss_x loss_x 1.3867 (1.4752) acc_x 62.5000 (60.3125) lr 1.9354e-03 eta 0:00:10
epoch [25/200] batch [15/31] time 0.377 (0.472) data 0.246 (0.341) loss_x loss_x 2.0742 (1.5509) acc_x 43.7500 (58.7500) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [20/31] time 0.430 (0.473) data 0.300 (0.342) loss_x loss_x 1.8984 (1.6116) acc_x 53.1250 (57.6562) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [25/31] time 0.564 (0.483) data 0.433 (0.353) loss_x loss_x 1.2197 (1.5690) acc_x 71.8750 (59.1250) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [30/31] time 0.346 (0.474) data 0.215 (0.343) loss_x loss_x 1.3174 (1.5458) acc_x 59.3750 (60.0000) lr 1.9354e-03 eta 0:00:00
epoch [25/200] batch [5/66] time 0.426 (0.467) data 0.293 (0.336) loss_u loss_u 0.8472 (0.8560) acc_u 21.8750 (20.6250) lr 1.9354e-03 eta 0:00:28
epoch [25/200] batch [10/66] time 0.379 (0.461) data 0.248 (0.330) loss_u loss_u 0.9292 (0.8781) acc_u 6.2500 (16.5625) lr 1.9354e-03 eta 0:00:25
epoch [25/200] batch [15/66] time 0.439 (0.455) data 0.307 (0.324) loss_u loss_u 0.8599 (0.8771) acc_u 15.6250 (16.2500) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [20/66] time 0.379 (0.453) data 0.248 (0.322) loss_u loss_u 0.8789 (0.8769) acc_u 15.6250 (16.0938) lr 1.9354e-03 eta 0:00:20
epoch [25/200] batch [25/66] time 0.378 (0.449) data 0.247 (0.318) loss_u loss_u 0.8774 (0.8724) acc_u 18.7500 (16.7500) lr 1.9354e-03 eta 0:00:18
epoch [25/200] batch [30/66] time 0.308 (0.451) data 0.178 (0.320) loss_u loss_u 0.9258 (0.8721) acc_u 3.1250 (16.8750) lr 1.9354e-03 eta 0:00:16
epoch [25/200] batch [35/66] time 0.434 (0.447) data 0.302 (0.316) loss_u loss_u 0.8501 (0.8741) acc_u 28.1250 (16.8750) lr 1.9354e-03 eta 0:00:13
epoch [25/200] batch [40/66] time 0.554 (0.447) data 0.423 (0.316) loss_u loss_u 0.8174 (0.8700) acc_u 28.1250 (17.5000) lr 1.9354e-03 eta 0:00:11
epoch [25/200] batch [45/66] time 0.448 (0.447) data 0.316 (0.316) loss_u loss_u 0.9258 (0.8715) acc_u 15.6250 (17.6389) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [50/66] time 0.408 (0.448) data 0.276 (0.316) loss_u loss_u 0.8799 (0.8711) acc_u 21.8750 (17.5625) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [55/66] time 0.507 (0.446) data 0.376 (0.315) loss_u loss_u 0.7949 (0.8695) acc_u 31.2500 (17.8977) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [60/66] time 0.396 (0.443) data 0.266 (0.312) loss_u loss_u 0.8340 (0.8682) acc_u 18.7500 (17.8125) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [65/66] time 0.535 (0.448) data 0.403 (0.317) loss_u loss_u 0.8721 (0.8647) acc_u 12.5000 (18.1731) lr 1.9354e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1679
confident_label rate tensor(0.3211, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1007
clean true:944
clean false:63
clean_rate:0.9374379344587885
noisy true:513
noisy false:1616
after delete: len(clean_dataset) 1007
after delete: len(noisy_dataset) 2129
epoch [26/200] batch [5/31] time 0.447 (0.502) data 0.317 (0.371) loss_x loss_x 0.9653 (1.4013) acc_x 68.7500 (67.5000) lr 1.9298e-03 eta 0:00:13
epoch [26/200] batch [10/31] time 0.427 (0.463) data 0.296 (0.333) loss_x loss_x 1.3809 (1.3420) acc_x 68.7500 (68.4375) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [15/31] time 0.531 (0.475) data 0.400 (0.344) loss_x loss_x 1.1748 (1.3412) acc_x 75.0000 (68.7500) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [20/31] time 0.407 (0.468) data 0.276 (0.337) loss_x loss_x 1.0771 (1.3189) acc_x 65.6250 (67.5000) lr 1.9298e-03 eta 0:00:05
epoch [26/200] batch [25/31] time 0.487 (0.467) data 0.353 (0.336) loss_x loss_x 1.4102 (1.3529) acc_x 71.8750 (65.7500) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [30/31] time 0.483 (0.479) data 0.353 (0.348) loss_x loss_x 1.1094 (1.3668) acc_x 68.7500 (64.8958) lr 1.9298e-03 eta 0:00:00
epoch [26/200] batch [5/66] time 0.492 (0.469) data 0.362 (0.338) loss_u loss_u 0.8535 (0.8523) acc_u 18.7500 (16.8750) lr 1.9298e-03 eta 0:00:28
epoch [26/200] batch [10/66] time 0.539 (0.464) data 0.409 (0.334) loss_u loss_u 0.8525 (0.8514) acc_u 15.6250 (16.5625) lr 1.9298e-03 eta 0:00:26
epoch [26/200] batch [15/66] time 0.365 (0.461) data 0.233 (0.330) loss_u loss_u 0.8057 (0.8599) acc_u 21.8750 (16.4583) lr 1.9298e-03 eta 0:00:23
epoch [26/200] batch [20/66] time 0.455 (0.462) data 0.324 (0.331) loss_u loss_u 0.9136 (0.8658) acc_u 9.3750 (16.0938) lr 1.9298e-03 eta 0:00:21
epoch [26/200] batch [25/66] time 0.535 (0.459) data 0.405 (0.329) loss_u loss_u 0.8525 (0.8687) acc_u 18.7500 (16.1250) lr 1.9298e-03 eta 0:00:18
epoch [26/200] batch [30/66] time 0.412 (0.456) data 0.281 (0.325) loss_u loss_u 0.8760 (0.8679) acc_u 15.6250 (16.3542) lr 1.9298e-03 eta 0:00:16
epoch [26/200] batch [35/66] time 0.548 (0.454) data 0.416 (0.323) loss_u loss_u 0.8115 (0.8650) acc_u 31.2500 (17.4107) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [40/66] time 0.359 (0.448) data 0.227 (0.317) loss_u loss_u 0.8965 (0.8631) acc_u 21.8750 (17.7344) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [45/66] time 0.740 (0.449) data 0.608 (0.318) loss_u loss_u 0.9111 (0.8661) acc_u 12.5000 (17.2917) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [50/66] time 0.550 (0.451) data 0.419 (0.320) loss_u loss_u 0.8535 (0.8656) acc_u 12.5000 (17.0000) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [55/66] time 0.398 (0.452) data 0.268 (0.321) loss_u loss_u 0.8560 (0.8660) acc_u 18.7500 (16.8750) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [60/66] time 0.430 (0.449) data 0.300 (0.318) loss_u loss_u 0.8164 (0.8648) acc_u 21.8750 (16.9792) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [65/66] time 0.339 (0.446) data 0.207 (0.315) loss_u loss_u 0.9009 (0.8663) acc_u 12.5000 (16.8750) lr 1.9298e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1704
confident_label rate tensor(0.3259, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1022
clean true:947
clean false:75
clean_rate:0.9266144814090019
noisy true:485
noisy false:1629
after delete: len(clean_dataset) 1022
after delete: len(noisy_dataset) 2114
epoch [27/200] batch [5/31] time 0.514 (0.499) data 0.383 (0.368) loss_x loss_x 0.9868 (1.4698) acc_x 78.1250 (62.5000) lr 1.9239e-03 eta 0:00:12
epoch [27/200] batch [10/31] time 0.583 (0.501) data 0.452 (0.370) loss_x loss_x 1.6104 (1.4813) acc_x 59.3750 (62.5000) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [15/31] time 0.548 (0.498) data 0.417 (0.367) loss_x loss_x 1.5332 (1.4604) acc_x 53.1250 (61.6667) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [20/31] time 0.486 (0.483) data 0.355 (0.352) loss_x loss_x 1.7832 (1.4890) acc_x 56.2500 (60.1562) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [25/31] time 0.447 (0.466) data 0.316 (0.335) loss_x loss_x 1.9385 (1.5069) acc_x 56.2500 (60.3750) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [30/31] time 0.588 (0.469) data 0.457 (0.338) loss_x loss_x 1.0732 (1.4786) acc_x 71.8750 (61.3542) lr 1.9239e-03 eta 0:00:00
epoch [27/200] batch [5/66] time 0.488 (0.474) data 0.357 (0.343) loss_u loss_u 0.8745 (0.8781) acc_u 15.6250 (14.3750) lr 1.9239e-03 eta 0:00:28
epoch [27/200] batch [10/66] time 0.376 (0.467) data 0.245 (0.336) loss_u loss_u 0.8818 (0.8748) acc_u 15.6250 (16.2500) lr 1.9239e-03 eta 0:00:26
epoch [27/200] batch [15/66] time 0.425 (0.462) data 0.294 (0.331) loss_u loss_u 0.8159 (0.8644) acc_u 21.8750 (17.2917) lr 1.9239e-03 eta 0:00:23
epoch [27/200] batch [20/66] time 0.444 (0.460) data 0.313 (0.329) loss_u loss_u 0.8218 (0.8565) acc_u 18.7500 (18.1250) lr 1.9239e-03 eta 0:00:21
epoch [27/200] batch [25/66] time 0.462 (0.460) data 0.330 (0.329) loss_u loss_u 0.8843 (0.8603) acc_u 21.8750 (17.8750) lr 1.9239e-03 eta 0:00:18
epoch [27/200] batch [30/66] time 0.446 (0.462) data 0.314 (0.331) loss_u loss_u 0.8560 (0.8634) acc_u 18.7500 (17.6042) lr 1.9239e-03 eta 0:00:16
epoch [27/200] batch [35/66] time 0.447 (0.464) data 0.315 (0.333) loss_u loss_u 0.8540 (0.8638) acc_u 18.7500 (17.3214) lr 1.9239e-03 eta 0:00:14
epoch [27/200] batch [40/66] time 0.346 (0.464) data 0.214 (0.333) loss_u loss_u 0.7930 (0.8641) acc_u 25.0000 (17.1875) lr 1.9239e-03 eta 0:00:12
epoch [27/200] batch [45/66] time 0.291 (0.458) data 0.159 (0.327) loss_u loss_u 0.9351 (0.8671) acc_u 3.1250 (16.8056) lr 1.9239e-03 eta 0:00:09
epoch [27/200] batch [50/66] time 0.508 (0.457) data 0.376 (0.326) loss_u loss_u 0.9033 (0.8680) acc_u 15.6250 (16.8750) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [55/66] time 0.510 (0.457) data 0.379 (0.325) loss_u loss_u 0.9297 (0.8708) acc_u 6.2500 (16.4205) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [60/66] time 0.388 (0.456) data 0.256 (0.324) loss_u loss_u 0.8672 (0.8706) acc_u 21.8750 (16.5625) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [65/66] time 0.460 (0.456) data 0.329 (0.324) loss_u loss_u 0.8994 (0.8699) acc_u 12.5000 (16.4904) lr 1.9239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1695
confident_label rate tensor(0.3237, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1015
clean true:951
clean false:64
clean_rate:0.9369458128078818
noisy true:490
noisy false:1631
after delete: len(clean_dataset) 1015
after delete: len(noisy_dataset) 2121
epoch [28/200] batch [5/31] time 0.426 (0.435) data 0.295 (0.305) loss_x loss_x 1.8535 (1.5445) acc_x 56.2500 (58.7500) lr 1.9178e-03 eta 0:00:11
epoch [28/200] batch [10/31] time 0.501 (0.445) data 0.370 (0.315) loss_x loss_x 1.1641 (1.4772) acc_x 78.1250 (62.8125) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [15/31] time 0.446 (0.441) data 0.314 (0.310) loss_x loss_x 1.3447 (1.4680) acc_x 62.5000 (62.0833) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [20/31] time 0.506 (0.467) data 0.376 (0.336) loss_x loss_x 1.5488 (1.4335) acc_x 62.5000 (63.4375) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [25/31] time 0.397 (0.479) data 0.267 (0.348) loss_x loss_x 1.6602 (1.4272) acc_x 56.2500 (63.8750) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [30/31] time 0.480 (0.486) data 0.350 (0.355) loss_x loss_x 1.1670 (1.4055) acc_x 59.3750 (64.1667) lr 1.9178e-03 eta 0:00:00
epoch [28/200] batch [5/66] time 0.352 (0.484) data 0.221 (0.353) loss_u loss_u 0.8301 (0.8432) acc_u 21.8750 (17.5000) lr 1.9178e-03 eta 0:00:29
epoch [28/200] batch [10/66] time 0.414 (0.477) data 0.284 (0.346) loss_u loss_u 0.8418 (0.8419) acc_u 25.0000 (19.3750) lr 1.9178e-03 eta 0:00:26
epoch [28/200] batch [15/66] time 0.447 (0.474) data 0.317 (0.343) loss_u loss_u 0.8726 (0.8436) acc_u 15.6250 (19.7917) lr 1.9178e-03 eta 0:00:24
epoch [28/200] batch [20/66] time 0.431 (0.472) data 0.300 (0.341) loss_u loss_u 0.9336 (0.8553) acc_u 3.1250 (18.1250) lr 1.9178e-03 eta 0:00:21
epoch [28/200] batch [25/66] time 0.566 (0.471) data 0.435 (0.340) loss_u loss_u 0.9004 (0.8549) acc_u 9.3750 (18.5000) lr 1.9178e-03 eta 0:00:19
epoch [28/200] batch [30/66] time 0.391 (0.466) data 0.259 (0.335) loss_u loss_u 0.8882 (0.8579) acc_u 9.3750 (17.8125) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [35/66] time 0.574 (0.464) data 0.443 (0.333) loss_u loss_u 0.8389 (0.8552) acc_u 21.8750 (18.1250) lr 1.9178e-03 eta 0:00:14
epoch [28/200] batch [40/66] time 0.406 (0.464) data 0.274 (0.333) loss_u loss_u 0.8608 (0.8544) acc_u 18.7500 (18.5156) lr 1.9178e-03 eta 0:00:12
epoch [28/200] batch [45/66] time 0.462 (0.461) data 0.330 (0.330) loss_u loss_u 0.8218 (0.8538) acc_u 18.7500 (18.5417) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [50/66] time 0.417 (0.460) data 0.286 (0.328) loss_u loss_u 0.9287 (0.8543) acc_u 15.6250 (18.8125) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [55/66] time 0.408 (0.460) data 0.275 (0.329) loss_u loss_u 0.7847 (0.8527) acc_u 25.0000 (18.9205) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [60/66] time 0.556 (0.458) data 0.421 (0.326) loss_u loss_u 0.7896 (0.8527) acc_u 21.8750 (18.9062) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [65/66] time 0.375 (0.460) data 0.245 (0.328) loss_u loss_u 0.8999 (0.8523) acc_u 21.8750 (18.9904) lr 1.9178e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1643
confident_label rate tensor(0.3326, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1043
clean true:970
clean false:73
clean_rate:0.9300095877277086
noisy true:523
noisy false:1570
after delete: len(clean_dataset) 1043
after delete: len(noisy_dataset) 2093
epoch [29/200] batch [5/32] time 0.496 (0.429) data 0.365 (0.297) loss_x loss_x 0.7612 (1.4138) acc_x 87.5000 (65.0000) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [10/32] time 0.409 (0.418) data 0.279 (0.287) loss_x loss_x 1.2812 (1.4108) acc_x 65.6250 (65.3125) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [15/32] time 0.430 (0.444) data 0.298 (0.313) loss_x loss_x 1.2324 (1.4254) acc_x 59.3750 (64.1667) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [20/32] time 0.435 (0.458) data 0.304 (0.327) loss_x loss_x 1.8125 (1.4204) acc_x 56.2500 (63.9062) lr 1.9114e-03 eta 0:00:05
epoch [29/200] batch [25/32] time 0.439 (0.467) data 0.309 (0.336) loss_x loss_x 1.0078 (1.4353) acc_x 71.8750 (63.1250) lr 1.9114e-03 eta 0:00:03
epoch [29/200] batch [30/32] time 0.465 (0.469) data 0.334 (0.338) loss_x loss_x 1.2725 (1.4398) acc_x 68.7500 (62.6042) lr 1.9114e-03 eta 0:00:00
epoch [29/200] batch [5/65] time 0.381 (0.468) data 0.250 (0.337) loss_u loss_u 0.7930 (0.8463) acc_u 21.8750 (18.1250) lr 1.9114e-03 eta 0:00:28
epoch [29/200] batch [10/65] time 0.383 (0.460) data 0.251 (0.329) loss_u loss_u 0.8184 (0.8462) acc_u 18.7500 (19.0625) lr 1.9114e-03 eta 0:00:25
epoch [29/200] batch [15/65] time 0.376 (0.456) data 0.243 (0.325) loss_u loss_u 0.9067 (0.8633) acc_u 6.2500 (17.0833) lr 1.9114e-03 eta 0:00:22
epoch [29/200] batch [20/65] time 0.417 (0.454) data 0.286 (0.322) loss_u loss_u 0.9463 (0.8683) acc_u 6.2500 (16.4062) lr 1.9114e-03 eta 0:00:20
epoch [29/200] batch [25/65] time 0.379 (0.453) data 0.249 (0.322) loss_u loss_u 0.8784 (0.8669) acc_u 12.5000 (17.0000) lr 1.9114e-03 eta 0:00:18
epoch [29/200] batch [30/65] time 0.516 (0.452) data 0.385 (0.321) loss_u loss_u 0.8687 (0.8671) acc_u 15.6250 (16.9792) lr 1.9114e-03 eta 0:00:15
epoch [29/200] batch [35/65] time 0.424 (0.449) data 0.294 (0.318) loss_u loss_u 0.8818 (0.8674) acc_u 21.8750 (17.3214) lr 1.9114e-03 eta 0:00:13
epoch [29/200] batch [40/65] time 0.597 (0.453) data 0.465 (0.322) loss_u loss_u 0.8765 (0.8717) acc_u 18.7500 (16.4844) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [45/65] time 0.637 (0.455) data 0.505 (0.324) loss_u loss_u 0.9004 (0.8754) acc_u 6.2500 (15.9028) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [50/65] time 0.427 (0.455) data 0.296 (0.323) loss_u loss_u 0.9438 (0.8765) acc_u 3.1250 (15.6875) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [55/65] time 0.426 (0.451) data 0.294 (0.319) loss_u loss_u 0.9087 (0.8749) acc_u 15.6250 (16.0227) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [60/65] time 0.446 (0.450) data 0.314 (0.318) loss_u loss_u 0.7524 (0.8692) acc_u 34.3750 (16.7188) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [65/65] time 0.408 (0.448) data 0.276 (0.316) loss_u loss_u 0.8682 (0.8699) acc_u 18.7500 (16.6827) lr 1.9114e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1653
confident_label rate tensor(0.3268, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1025
clean true:970
clean false:55
clean_rate:0.9463414634146341
noisy true:513
noisy false:1598
after delete: len(clean_dataset) 1025
after delete: len(noisy_dataset) 2111
epoch [30/200] batch [5/32] time 0.623 (0.540) data 0.490 (0.408) loss_x loss_x 1.7656 (1.5205) acc_x 53.1250 (61.8750) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [10/32] time 0.504 (0.519) data 0.373 (0.387) loss_x loss_x 1.5381 (1.4474) acc_x 59.3750 (61.5625) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [15/32] time 0.373 (0.487) data 0.243 (0.355) loss_x loss_x 1.4824 (1.4479) acc_x 56.2500 (61.8750) lr 1.9048e-03 eta 0:00:08
epoch [30/200] batch [20/32] time 0.491 (0.472) data 0.360 (0.341) loss_x loss_x 2.1328 (1.4739) acc_x 53.1250 (60.9375) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [25/32] time 0.419 (0.469) data 0.288 (0.338) loss_x loss_x 0.9409 (1.4472) acc_x 75.0000 (62.3750) lr 1.9048e-03 eta 0:00:03
epoch [30/200] batch [30/32] time 0.467 (0.471) data 0.336 (0.340) loss_x loss_x 1.5293 (1.4596) acc_x 62.5000 (62.5000) lr 1.9048e-03 eta 0:00:00
epoch [30/200] batch [5/65] time 0.621 (0.479) data 0.490 (0.348) loss_u loss_u 0.9160 (0.8423) acc_u 12.5000 (21.8750) lr 1.9048e-03 eta 0:00:28
epoch [30/200] batch [10/65] time 0.434 (0.474) data 0.302 (0.343) loss_u loss_u 0.8579 (0.8553) acc_u 18.7500 (18.1250) lr 1.9048e-03 eta 0:00:26
epoch [30/200] batch [15/65] time 0.395 (0.468) data 0.264 (0.337) loss_u loss_u 0.7808 (0.8564) acc_u 34.3750 (18.1250) lr 1.9048e-03 eta 0:00:23
epoch [30/200] batch [20/65] time 0.483 (0.465) data 0.351 (0.334) loss_u loss_u 0.8359 (0.8577) acc_u 34.3750 (18.7500) lr 1.9048e-03 eta 0:00:20
epoch [30/200] batch [25/65] time 0.486 (0.467) data 0.355 (0.336) loss_u loss_u 0.8633 (0.8571) acc_u 18.7500 (18.7500) lr 1.9048e-03 eta 0:00:18
epoch [30/200] batch [30/65] time 0.470 (0.465) data 0.339 (0.334) loss_u loss_u 0.8833 (0.8605) acc_u 18.7500 (17.9167) lr 1.9048e-03 eta 0:00:16
epoch [30/200] batch [35/65] time 0.456 (0.462) data 0.325 (0.331) loss_u loss_u 0.8438 (0.8615) acc_u 18.7500 (17.8571) lr 1.9048e-03 eta 0:00:13
epoch [30/200] batch [40/65] time 0.391 (0.460) data 0.258 (0.329) loss_u loss_u 0.8589 (0.8594) acc_u 18.7500 (18.0469) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [45/65] time 0.378 (0.460) data 0.247 (0.329) loss_u loss_u 0.8486 (0.8587) acc_u 25.0000 (18.1944) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [50/65] time 0.734 (0.466) data 0.602 (0.335) loss_u loss_u 0.8994 (0.8591) acc_u 15.6250 (18.1250) lr 1.9048e-03 eta 0:00:06
epoch [30/200] batch [55/65] time 0.408 (0.462) data 0.277 (0.330) loss_u loss_u 0.8657 (0.8617) acc_u 15.6250 (17.7273) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [60/65] time 0.365 (0.461) data 0.233 (0.329) loss_u loss_u 0.7705 (0.8603) acc_u 25.0000 (17.7604) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [65/65] time 0.455 (0.460) data 0.323 (0.328) loss_u loss_u 0.9263 (0.8625) acc_u 9.3750 (17.3558) lr 1.9048e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1653
confident_label rate tensor(0.3320, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1041
clean true:983
clean false:58
clean_rate:0.9442843419788665
noisy true:500
noisy false:1595
after delete: len(clean_dataset) 1041
after delete: len(noisy_dataset) 2095
epoch [31/200] batch [5/32] time 0.448 (0.449) data 0.317 (0.318) loss_x loss_x 1.0928 (1.4811) acc_x 75.0000 (67.5000) lr 1.8980e-03 eta 0:00:12
epoch [31/200] batch [10/32] time 0.570 (0.483) data 0.439 (0.352) loss_x loss_x 1.3145 (1.3609) acc_x 68.7500 (66.2500) lr 1.8980e-03 eta 0:00:10
epoch [31/200] batch [15/32] time 0.410 (0.463) data 0.279 (0.332) loss_x loss_x 1.2383 (1.3565) acc_x 71.8750 (67.0833) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [20/32] time 0.415 (0.461) data 0.284 (0.330) loss_x loss_x 1.5146 (1.3573) acc_x 68.7500 (66.8750) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [25/32] time 0.451 (0.458) data 0.320 (0.326) loss_x loss_x 1.3311 (1.3831) acc_x 59.3750 (67.0000) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [30/32] time 0.442 (0.455) data 0.311 (0.324) loss_x loss_x 1.3291 (1.3756) acc_x 75.0000 (66.9792) lr 1.8980e-03 eta 0:00:00
epoch [31/200] batch [5/65] time 0.635 (0.457) data 0.503 (0.326) loss_u loss_u 0.8706 (0.8535) acc_u 21.8750 (20.6250) lr 1.8980e-03 eta 0:00:27
epoch [31/200] batch [10/65] time 0.389 (0.460) data 0.257 (0.329) loss_u loss_u 0.8984 (0.8640) acc_u 18.7500 (19.0625) lr 1.8980e-03 eta 0:00:25
epoch [31/200] batch [15/65] time 0.480 (0.462) data 0.349 (0.331) loss_u loss_u 0.8696 (0.8565) acc_u 18.7500 (18.9583) lr 1.8980e-03 eta 0:00:23
epoch [31/200] batch [20/65] time 0.463 (0.462) data 0.330 (0.331) loss_u loss_u 0.8740 (0.8588) acc_u 15.6250 (18.5938) lr 1.8980e-03 eta 0:00:20
epoch [31/200] batch [25/65] time 0.388 (0.458) data 0.256 (0.327) loss_u loss_u 0.8188 (0.8600) acc_u 28.1250 (18.8750) lr 1.8980e-03 eta 0:00:18
epoch [31/200] batch [30/65] time 0.538 (0.459) data 0.406 (0.328) loss_u loss_u 0.8501 (0.8597) acc_u 21.8750 (18.8542) lr 1.8980e-03 eta 0:00:16
epoch [31/200] batch [35/65] time 0.385 (0.457) data 0.254 (0.325) loss_u loss_u 0.7905 (0.8585) acc_u 15.6250 (18.5714) lr 1.8980e-03 eta 0:00:13
epoch [31/200] batch [40/65] time 0.448 (0.460) data 0.316 (0.328) loss_u loss_u 0.9160 (0.8613) acc_u 12.5000 (18.3594) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [45/65] time 0.499 (0.457) data 0.367 (0.325) loss_u loss_u 0.9229 (0.8620) acc_u 6.2500 (17.9861) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [50/65] time 0.309 (0.457) data 0.178 (0.325) loss_u loss_u 0.9048 (0.8613) acc_u 15.6250 (18.5625) lr 1.8980e-03 eta 0:00:06
epoch [31/200] batch [55/65] time 0.422 (0.453) data 0.290 (0.321) loss_u loss_u 0.8804 (0.8618) acc_u 15.6250 (18.5795) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [60/65] time 0.456 (0.453) data 0.323 (0.321) loss_u loss_u 0.9204 (0.8612) acc_u 9.3750 (18.6979) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [65/65] time 0.427 (0.452) data 0.295 (0.320) loss_u loss_u 0.9556 (0.8623) acc_u 6.2500 (18.4615) lr 1.8980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1709
confident_label rate tensor(0.3240, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1016
clean true:958
clean false:58
clean_rate:0.9429133858267716
noisy true:469
noisy false:1651
after delete: len(clean_dataset) 1016
after delete: len(noisy_dataset) 2120
epoch [32/200] batch [5/31] time 0.712 (0.538) data 0.579 (0.407) loss_x loss_x 1.7080 (1.5928) acc_x 50.0000 (56.2500) lr 1.8910e-03 eta 0:00:13
epoch [32/200] batch [10/31] time 0.546 (0.510) data 0.416 (0.379) loss_x loss_x 0.9189 (1.4824) acc_x 75.0000 (58.4375) lr 1.8910e-03 eta 0:00:10
epoch [32/200] batch [15/31] time 0.396 (0.480) data 0.265 (0.349) loss_x loss_x 1.9111 (1.5098) acc_x 46.8750 (58.3333) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [20/31] time 0.420 (0.473) data 0.288 (0.342) loss_x loss_x 1.1660 (1.4619) acc_x 68.7500 (60.0000) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [25/31] time 0.475 (0.471) data 0.344 (0.340) loss_x loss_x 1.2314 (1.4360) acc_x 68.7500 (61.6250) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [30/31] time 0.415 (0.469) data 0.284 (0.338) loss_x loss_x 1.8428 (1.4538) acc_x 56.2500 (60.9375) lr 1.8910e-03 eta 0:00:00
epoch [32/200] batch [5/66] time 0.376 (0.469) data 0.243 (0.338) loss_u loss_u 0.8633 (0.8534) acc_u 21.8750 (17.5000) lr 1.8910e-03 eta 0:00:28
epoch [32/200] batch [10/66] time 0.473 (0.468) data 0.341 (0.336) loss_u loss_u 0.8354 (0.8624) acc_u 25.0000 (18.1250) lr 1.8910e-03 eta 0:00:26
epoch [32/200] batch [15/66] time 0.402 (0.468) data 0.272 (0.337) loss_u loss_u 0.9365 (0.8671) acc_u 9.3750 (17.2917) lr 1.8910e-03 eta 0:00:23
epoch [32/200] batch [20/66] time 0.661 (0.469) data 0.529 (0.338) loss_u loss_u 0.8823 (0.8602) acc_u 12.5000 (17.6562) lr 1.8910e-03 eta 0:00:21
epoch [32/200] batch [25/66] time 0.396 (0.471) data 0.265 (0.339) loss_u loss_u 0.8804 (0.8606) acc_u 9.3750 (17.3750) lr 1.8910e-03 eta 0:00:19
epoch [32/200] batch [30/66] time 0.520 (0.480) data 0.389 (0.348) loss_u loss_u 0.8628 (0.8634) acc_u 12.5000 (16.8750) lr 1.8910e-03 eta 0:00:17
epoch [32/200] batch [35/66] time 0.480 (0.480) data 0.348 (0.349) loss_u loss_u 0.9375 (0.8671) acc_u 3.1250 (16.2500) lr 1.8910e-03 eta 0:00:14
epoch [32/200] batch [40/66] time 0.434 (0.476) data 0.302 (0.345) loss_u loss_u 0.8223 (0.8613) acc_u 21.8750 (17.1094) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [45/66] time 0.390 (0.472) data 0.259 (0.341) loss_u loss_u 0.8638 (0.8636) acc_u 18.7500 (16.9444) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [50/66] time 0.418 (0.473) data 0.287 (0.342) loss_u loss_u 0.8999 (0.8631) acc_u 12.5000 (17.0625) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [55/66] time 0.419 (0.470) data 0.288 (0.339) loss_u loss_u 0.8066 (0.8600) acc_u 31.2500 (17.8409) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [60/66] time 0.384 (0.469) data 0.252 (0.338) loss_u loss_u 0.8804 (0.8600) acc_u 12.5000 (17.9688) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [65/66] time 0.399 (0.466) data 0.267 (0.334) loss_u loss_u 0.8101 (0.8604) acc_u 18.7500 (17.8846) lr 1.8910e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1663
confident_label rate tensor(0.3192, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1001
clean true:947
clean false:54
clean_rate:0.9460539460539461
noisy true:526
noisy false:1609
after delete: len(clean_dataset) 1001
after delete: len(noisy_dataset) 2135
epoch [33/200] batch [5/31] time 0.458 (0.435) data 0.326 (0.304) loss_x loss_x 1.8457 (1.3192) acc_x 59.3750 (66.2500) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [10/31] time 0.572 (0.464) data 0.441 (0.332) loss_x loss_x 1.2314 (1.4169) acc_x 65.6250 (65.6250) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [15/31] time 0.419 (0.456) data 0.289 (0.325) loss_x loss_x 1.6416 (1.3871) acc_x 56.2500 (65.4167) lr 1.8838e-03 eta 0:00:07
epoch [33/200] batch [20/31] time 0.379 (0.449) data 0.247 (0.318) loss_x loss_x 1.2266 (1.3358) acc_x 65.6250 (65.9375) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [25/31] time 0.508 (0.453) data 0.377 (0.322) loss_x loss_x 1.1943 (1.3295) acc_x 75.0000 (65.8750) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [30/31] time 0.337 (0.452) data 0.206 (0.321) loss_x loss_x 1.2803 (1.3534) acc_x 59.3750 (65.5208) lr 1.8838e-03 eta 0:00:00
epoch [33/200] batch [5/66] time 0.470 (0.448) data 0.338 (0.317) loss_u loss_u 0.8389 (0.8425) acc_u 18.7500 (23.1250) lr 1.8838e-03 eta 0:00:27
epoch [33/200] batch [10/66] time 0.387 (0.448) data 0.255 (0.317) loss_u loss_u 0.8882 (0.8494) acc_u 12.5000 (20.3125) lr 1.8838e-03 eta 0:00:25
epoch [33/200] batch [15/66] time 0.422 (0.450) data 0.291 (0.318) loss_u loss_u 0.8364 (0.8481) acc_u 15.6250 (19.7917) lr 1.8838e-03 eta 0:00:22
epoch [33/200] batch [20/66] time 0.404 (0.450) data 0.272 (0.319) loss_u loss_u 0.8428 (0.8438) acc_u 18.7500 (20.7812) lr 1.8838e-03 eta 0:00:20
epoch [33/200] batch [25/66] time 0.553 (0.447) data 0.422 (0.315) loss_u loss_u 0.9136 (0.8522) acc_u 9.3750 (19.2500) lr 1.8838e-03 eta 0:00:18
epoch [33/200] batch [30/66] time 0.366 (0.445) data 0.232 (0.313) loss_u loss_u 0.8584 (0.8505) acc_u 21.8750 (19.3750) lr 1.8838e-03 eta 0:00:16
epoch [33/200] batch [35/66] time 0.524 (0.448) data 0.392 (0.316) loss_u loss_u 0.8218 (0.8508) acc_u 21.8750 (19.2857) lr 1.8838e-03 eta 0:00:13
epoch [33/200] batch [40/66] time 0.583 (0.449) data 0.451 (0.318) loss_u loss_u 0.9106 (0.8487) acc_u 18.7500 (19.7656) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [45/66] time 0.430 (0.451) data 0.298 (0.320) loss_u loss_u 0.9370 (0.8519) acc_u 9.3750 (19.3056) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [50/66] time 0.654 (0.458) data 0.522 (0.326) loss_u loss_u 0.8608 (0.8542) acc_u 21.8750 (19.2500) lr 1.8838e-03 eta 0:00:07
epoch [33/200] batch [55/66] time 0.553 (0.456) data 0.420 (0.324) loss_u loss_u 0.8081 (0.8549) acc_u 21.8750 (19.2614) lr 1.8838e-03 eta 0:00:05
epoch [33/200] batch [60/66] time 0.383 (0.452) data 0.251 (0.321) loss_u loss_u 0.8467 (0.8557) acc_u 21.8750 (19.1146) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [65/66] time 0.461 (0.449) data 0.330 (0.317) loss_u loss_u 0.8813 (0.8594) acc_u 6.2500 (18.2212) lr 1.8838e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1698
confident_label rate tensor(0.3233, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1014
clean true:938
clean false:76
clean_rate:0.9250493096646942
noisy true:500
noisy false:1622
after delete: len(clean_dataset) 1014
after delete: len(noisy_dataset) 2122
epoch [34/200] batch [5/31] time 0.588 (0.591) data 0.456 (0.460) loss_x loss_x 1.2314 (1.5391) acc_x 65.6250 (61.8750) lr 1.8763e-03 eta 0:00:15
epoch [34/200] batch [10/31] time 0.352 (0.532) data 0.221 (0.400) loss_x loss_x 1.2695 (1.3948) acc_x 71.8750 (66.8750) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [15/31] time 0.409 (0.511) data 0.277 (0.379) loss_x loss_x 0.9668 (1.3592) acc_x 68.7500 (65.8333) lr 1.8763e-03 eta 0:00:08
epoch [34/200] batch [20/31] time 0.377 (0.495) data 0.246 (0.363) loss_x loss_x 1.7471 (1.4040) acc_x 53.1250 (65.0000) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [25/31] time 0.539 (0.482) data 0.408 (0.350) loss_x loss_x 1.8330 (1.4218) acc_x 50.0000 (64.2500) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [30/31] time 0.434 (0.479) data 0.302 (0.347) loss_x loss_x 1.3613 (1.4211) acc_x 71.8750 (64.1667) lr 1.8763e-03 eta 0:00:00
epoch [34/200] batch [5/66] time 0.513 (0.481) data 0.381 (0.349) loss_u loss_u 0.8569 (0.8492) acc_u 15.6250 (19.3750) lr 1.8763e-03 eta 0:00:29
epoch [34/200] batch [10/66] time 0.392 (0.481) data 0.260 (0.349) loss_u loss_u 0.8901 (0.8394) acc_u 15.6250 (20.0000) lr 1.8763e-03 eta 0:00:26
epoch [34/200] batch [15/66] time 0.471 (0.477) data 0.339 (0.345) loss_u loss_u 0.8716 (0.8548) acc_u 12.5000 (17.2917) lr 1.8763e-03 eta 0:00:24
epoch [34/200] batch [20/66] time 0.615 (0.482) data 0.483 (0.351) loss_u loss_u 0.8584 (0.8531) acc_u 15.6250 (17.5000) lr 1.8763e-03 eta 0:00:22
epoch [34/200] batch [25/66] time 0.394 (0.481) data 0.263 (0.349) loss_u loss_u 0.8335 (0.8522) acc_u 18.7500 (18.0000) lr 1.8763e-03 eta 0:00:19
epoch [34/200] batch [30/66] time 0.391 (0.477) data 0.259 (0.345) loss_u loss_u 0.8506 (0.8441) acc_u 25.0000 (19.3750) lr 1.8763e-03 eta 0:00:17
epoch [34/200] batch [35/66] time 0.418 (0.478) data 0.287 (0.346) loss_u loss_u 0.8984 (0.8451) acc_u 15.6250 (19.3750) lr 1.8763e-03 eta 0:00:14
epoch [34/200] batch [40/66] time 0.468 (0.472) data 0.336 (0.341) loss_u loss_u 0.8608 (0.8480) acc_u 15.6250 (18.9844) lr 1.8763e-03 eta 0:00:12
epoch [34/200] batch [45/66] time 0.315 (0.472) data 0.183 (0.340) loss_u loss_u 0.8896 (0.8495) acc_u 9.3750 (18.4722) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [50/66] time 0.446 (0.471) data 0.314 (0.339) loss_u loss_u 0.8535 (0.8520) acc_u 18.7500 (18.2500) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [55/66] time 0.508 (0.474) data 0.376 (0.342) loss_u loss_u 0.8618 (0.8523) acc_u 15.6250 (18.4091) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [60/66] time 0.453 (0.475) data 0.320 (0.343) loss_u loss_u 0.8047 (0.8531) acc_u 28.1250 (18.3854) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [65/66] time 0.527 (0.473) data 0.395 (0.341) loss_u loss_u 0.8555 (0.8546) acc_u 18.7500 (18.0769) lr 1.8763e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1626
confident_label rate tensor(0.3422, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1073
clean true:1016
clean false:57
clean_rate:0.9468779123951537
noisy true:494
noisy false:1569
after delete: len(clean_dataset) 1073
after delete: len(noisy_dataset) 2063
epoch [35/200] batch [5/33] time 0.490 (0.462) data 0.359 (0.331) loss_x loss_x 1.3408 (1.3235) acc_x 56.2500 (65.0000) lr 1.8686e-03 eta 0:00:12
epoch [35/200] batch [10/33] time 0.480 (0.467) data 0.349 (0.335) loss_x loss_x 1.4238 (1.2948) acc_x 62.5000 (67.5000) lr 1.8686e-03 eta 0:00:10
epoch [35/200] batch [15/33] time 0.556 (0.461) data 0.425 (0.330) loss_x loss_x 1.3730 (1.3028) acc_x 59.3750 (67.2917) lr 1.8686e-03 eta 0:00:08
epoch [35/200] batch [20/33] time 0.395 (0.475) data 0.264 (0.343) loss_x loss_x 1.2051 (1.3265) acc_x 84.3750 (67.9688) lr 1.8686e-03 eta 0:00:06
epoch [35/200] batch [25/33] time 0.458 (0.475) data 0.327 (0.343) loss_x loss_x 0.9238 (1.3125) acc_x 75.0000 (67.8750) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [30/33] time 0.525 (0.469) data 0.395 (0.338) loss_x loss_x 1.1729 (1.3497) acc_x 71.8750 (66.6667) lr 1.8686e-03 eta 0:00:01
epoch [35/200] batch [5/64] time 0.458 (0.460) data 0.326 (0.329) loss_u loss_u 0.8906 (0.8714) acc_u 12.5000 (13.1250) lr 1.8686e-03 eta 0:00:27
epoch [35/200] batch [10/64] time 0.586 (0.457) data 0.454 (0.326) loss_u loss_u 0.8984 (0.8706) acc_u 15.6250 (15.6250) lr 1.8686e-03 eta 0:00:24
epoch [35/200] batch [15/64] time 0.347 (0.452) data 0.215 (0.320) loss_u loss_u 0.8198 (0.8584) acc_u 21.8750 (17.0833) lr 1.8686e-03 eta 0:00:22
epoch [35/200] batch [20/64] time 0.373 (0.447) data 0.241 (0.316) loss_u loss_u 0.8892 (0.8627) acc_u 6.2500 (15.7812) lr 1.8686e-03 eta 0:00:19
epoch [35/200] batch [25/64] time 0.402 (0.443) data 0.272 (0.311) loss_u loss_u 0.8540 (0.8607) acc_u 18.7500 (16.6250) lr 1.8686e-03 eta 0:00:17
epoch [35/200] batch [30/64] time 0.373 (0.444) data 0.242 (0.312) loss_u loss_u 0.8936 (0.8613) acc_u 9.3750 (16.7708) lr 1.8686e-03 eta 0:00:15
epoch [35/200] batch [35/64] time 0.451 (0.445) data 0.319 (0.313) loss_u loss_u 0.8252 (0.8602) acc_u 25.0000 (17.0536) lr 1.8686e-03 eta 0:00:12
epoch [35/200] batch [40/64] time 0.425 (0.446) data 0.293 (0.315) loss_u loss_u 0.8691 (0.8600) acc_u 15.6250 (17.4219) lr 1.8686e-03 eta 0:00:10
epoch [35/200] batch [45/64] time 0.450 (0.451) data 0.319 (0.319) loss_u loss_u 0.8770 (0.8615) acc_u 15.6250 (17.5000) lr 1.8686e-03 eta 0:00:08
epoch [35/200] batch [50/64] time 0.401 (0.448) data 0.269 (0.317) loss_u loss_u 0.8770 (0.8635) acc_u 18.7500 (17.3750) lr 1.8686e-03 eta 0:00:06
epoch [35/200] batch [55/64] time 0.412 (0.444) data 0.281 (0.313) loss_u loss_u 0.8848 (0.8635) acc_u 18.7500 (17.6705) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [60/64] time 0.376 (0.450) data 0.244 (0.318) loss_u loss_u 0.8291 (0.8645) acc_u 15.6250 (17.3438) lr 1.8686e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1677
confident_label rate tensor(0.3320, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1041
clean true:978
clean false:63
clean_rate:0.9394812680115274
noisy true:481
noisy false:1614
after delete: len(clean_dataset) 1041
after delete: len(noisy_dataset) 2095
epoch [36/200] batch [5/32] time 0.573 (0.470) data 0.443 (0.340) loss_x loss_x 1.3115 (1.3467) acc_x 59.3750 (64.3750) lr 1.8607e-03 eta 0:00:12
epoch [36/200] batch [10/32] time 0.482 (0.506) data 0.351 (0.375) loss_x loss_x 0.7603 (1.2339) acc_x 78.1250 (65.0000) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [15/32] time 0.478 (0.498) data 0.347 (0.366) loss_x loss_x 1.3584 (1.2695) acc_x 65.6250 (65.4167) lr 1.8607e-03 eta 0:00:08
epoch [36/200] batch [20/32] time 0.594 (0.498) data 0.463 (0.367) loss_x loss_x 1.6377 (1.3315) acc_x 65.6250 (65.0000) lr 1.8607e-03 eta 0:00:05
epoch [36/200] batch [25/32] time 0.487 (0.490) data 0.356 (0.359) loss_x loss_x 1.1709 (1.3514) acc_x 65.6250 (64.3750) lr 1.8607e-03 eta 0:00:03
epoch [36/200] batch [30/32] time 0.627 (0.497) data 0.496 (0.365) loss_x loss_x 0.9351 (1.3406) acc_x 78.1250 (65.0000) lr 1.8607e-03 eta 0:00:00
epoch [36/200] batch [5/65] time 0.435 (0.487) data 0.303 (0.356) loss_u loss_u 0.8091 (0.8651) acc_u 21.8750 (15.6250) lr 1.8607e-03 eta 0:00:29
epoch [36/200] batch [10/65] time 0.451 (0.482) data 0.320 (0.351) loss_u loss_u 0.8682 (0.8555) acc_u 15.6250 (16.8750) lr 1.8607e-03 eta 0:00:26
epoch [36/200] batch [15/65] time 0.428 (0.481) data 0.296 (0.350) loss_u loss_u 0.7910 (0.8564) acc_u 25.0000 (17.5000) lr 1.8607e-03 eta 0:00:24
epoch [36/200] batch [20/65] time 0.416 (0.473) data 0.285 (0.342) loss_u loss_u 0.8657 (0.8520) acc_u 18.7500 (18.7500) lr 1.8607e-03 eta 0:00:21
epoch [36/200] batch [25/65] time 0.381 (0.470) data 0.250 (0.339) loss_u loss_u 0.7949 (0.8543) acc_u 25.0000 (18.6250) lr 1.8607e-03 eta 0:00:18
epoch [36/200] batch [30/65] time 0.710 (0.473) data 0.578 (0.342) loss_u loss_u 0.7925 (0.8473) acc_u 31.2500 (19.5833) lr 1.8607e-03 eta 0:00:16
epoch [36/200] batch [35/65] time 0.381 (0.469) data 0.249 (0.337) loss_u loss_u 0.9409 (0.8521) acc_u 6.2500 (18.6607) lr 1.8607e-03 eta 0:00:14
epoch [36/200] batch [40/65] time 0.316 (0.465) data 0.185 (0.334) loss_u loss_u 0.8813 (0.8523) acc_u 15.6250 (18.5938) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [45/65] time 0.443 (0.465) data 0.312 (0.333) loss_u loss_u 0.9224 (0.8510) acc_u 6.2500 (18.6806) lr 1.8607e-03 eta 0:00:09
epoch [36/200] batch [50/65] time 0.496 (0.462) data 0.364 (0.330) loss_u loss_u 0.8872 (0.8559) acc_u 12.5000 (18.0000) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [55/65] time 0.370 (0.463) data 0.239 (0.331) loss_u loss_u 0.9370 (0.8576) acc_u 0.0000 (17.6705) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [60/65] time 0.440 (0.459) data 0.308 (0.328) loss_u loss_u 0.8691 (0.8584) acc_u 18.7500 (17.7604) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [65/65] time 0.512 (0.457) data 0.381 (0.326) loss_u loss_u 0.8911 (0.8598) acc_u 12.5000 (17.5481) lr 1.8607e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1672
confident_label rate tensor(0.3409, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1069
clean true:993
clean false:76
clean_rate:0.9289055191768008
noisy true:471
noisy false:1596
after delete: len(clean_dataset) 1069
after delete: len(noisy_dataset) 2067
epoch [37/200] batch [5/33] time 0.430 (0.469) data 0.299 (0.338) loss_x loss_x 1.5908 (1.4818) acc_x 56.2500 (58.7500) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [10/33] time 0.391 (0.438) data 0.260 (0.307) loss_x loss_x 1.0850 (1.3394) acc_x 62.5000 (62.8125) lr 1.8526e-03 eta 0:00:10
epoch [37/200] batch [15/33] time 0.483 (0.442) data 0.352 (0.311) loss_x loss_x 1.4990 (1.3564) acc_x 62.5000 (63.3333) lr 1.8526e-03 eta 0:00:07
epoch [37/200] batch [20/33] time 0.401 (0.448) data 0.269 (0.317) loss_x loss_x 1.8135 (1.3709) acc_x 62.5000 (63.9062) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [25/33] time 0.420 (0.444) data 0.289 (0.313) loss_x loss_x 1.4502 (1.4197) acc_x 62.5000 (62.5000) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [30/33] time 0.422 (0.441) data 0.291 (0.310) loss_x loss_x 1.3691 (1.4181) acc_x 71.8750 (62.2917) lr 1.8526e-03 eta 0:00:01
epoch [37/200] batch [5/64] time 0.441 (0.438) data 0.311 (0.306) loss_u loss_u 0.9189 (0.8770) acc_u 9.3750 (15.6250) lr 1.8526e-03 eta 0:00:25
epoch [37/200] batch [10/64] time 0.370 (0.436) data 0.239 (0.305) loss_u loss_u 0.9004 (0.8847) acc_u 12.5000 (14.3750) lr 1.8526e-03 eta 0:00:23
epoch [37/200] batch [15/64] time 0.630 (0.442) data 0.495 (0.311) loss_u loss_u 0.7808 (0.8635) acc_u 21.8750 (17.0833) lr 1.8526e-03 eta 0:00:21
epoch [37/200] batch [20/64] time 0.404 (0.445) data 0.273 (0.314) loss_u loss_u 0.8188 (0.8688) acc_u 21.8750 (16.4062) lr 1.8526e-03 eta 0:00:19
epoch [37/200] batch [25/64] time 0.360 (0.443) data 0.228 (0.312) loss_u loss_u 0.8784 (0.8701) acc_u 12.5000 (15.8750) lr 1.8526e-03 eta 0:00:17
epoch [37/200] batch [30/64] time 0.462 (0.446) data 0.330 (0.315) loss_u loss_u 0.8301 (0.8676) acc_u 18.7500 (16.2500) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [35/64] time 0.416 (0.444) data 0.285 (0.313) loss_u loss_u 0.8560 (0.8704) acc_u 15.6250 (15.9821) lr 1.8526e-03 eta 0:00:12
epoch [37/200] batch [40/64] time 0.426 (0.447) data 0.295 (0.315) loss_u loss_u 0.8931 (0.8644) acc_u 12.5000 (17.1094) lr 1.8526e-03 eta 0:00:10
epoch [37/200] batch [45/64] time 0.462 (0.448) data 0.331 (0.316) loss_u loss_u 0.8545 (0.8651) acc_u 18.7500 (16.8056) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [50/64] time 0.459 (0.453) data 0.327 (0.321) loss_u loss_u 0.8066 (0.8609) acc_u 25.0000 (17.6250) lr 1.8526e-03 eta 0:00:06
epoch [37/200] batch [55/64] time 0.711 (0.454) data 0.579 (0.322) loss_u loss_u 0.7949 (0.8600) acc_u 18.7500 (17.5568) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [60/64] time 0.467 (0.453) data 0.335 (0.321) loss_u loss_u 0.8320 (0.8586) acc_u 25.0000 (17.9167) lr 1.8526e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1621
confident_label rate tensor(0.3406, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1068
clean true:1003
clean false:65
clean_rate:0.9391385767790262
noisy true:512
noisy false:1556
after delete: len(clean_dataset) 1068
after delete: len(noisy_dataset) 2068
epoch [38/200] batch [5/33] time 0.483 (0.467) data 0.351 (0.336) loss_x loss_x 1.4658 (1.4381) acc_x 56.2500 (61.8750) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [10/33] time 0.423 (0.455) data 0.292 (0.324) loss_x loss_x 1.7578 (1.4312) acc_x 65.6250 (64.3750) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [15/33] time 0.505 (0.489) data 0.373 (0.358) loss_x loss_x 1.4512 (1.4525) acc_x 50.0000 (63.7500) lr 1.8443e-03 eta 0:00:08
epoch [38/200] batch [20/33] time 0.494 (0.493) data 0.363 (0.361) loss_x loss_x 1.2266 (1.4185) acc_x 65.6250 (64.5312) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [25/33] time 0.523 (0.490) data 0.391 (0.359) loss_x loss_x 1.0537 (1.3818) acc_x 78.1250 (64.8750) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [30/33] time 0.492 (0.486) data 0.361 (0.355) loss_x loss_x 1.6484 (1.3732) acc_x 56.2500 (64.8958) lr 1.8443e-03 eta 0:00:01
epoch [38/200] batch [5/64] time 0.309 (0.487) data 0.178 (0.355) loss_u loss_u 0.9111 (0.8751) acc_u 9.3750 (17.5000) lr 1.8443e-03 eta 0:00:28
epoch [38/200] batch [10/64] time 0.360 (0.477) data 0.229 (0.346) loss_u loss_u 0.8369 (0.8716) acc_u 21.8750 (16.2500) lr 1.8443e-03 eta 0:00:25
epoch [38/200] batch [15/64] time 0.323 (0.472) data 0.191 (0.340) loss_u loss_u 0.8584 (0.8623) acc_u 15.6250 (16.4583) lr 1.8443e-03 eta 0:00:23
epoch [38/200] batch [20/64] time 0.471 (0.474) data 0.339 (0.342) loss_u loss_u 0.9077 (0.8604) acc_u 18.7500 (17.1875) lr 1.8443e-03 eta 0:00:20
epoch [38/200] batch [25/64] time 0.524 (0.475) data 0.390 (0.343) loss_u loss_u 0.8218 (0.8587) acc_u 15.6250 (17.3750) lr 1.8443e-03 eta 0:00:18
epoch [38/200] batch [30/64] time 0.475 (0.477) data 0.342 (0.345) loss_u loss_u 0.8730 (0.8583) acc_u 12.5000 (17.8125) lr 1.8443e-03 eta 0:00:16
epoch [38/200] batch [35/64] time 0.441 (0.479) data 0.309 (0.348) loss_u loss_u 0.8965 (0.8610) acc_u 15.6250 (17.5000) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [40/64] time 0.363 (0.479) data 0.230 (0.348) loss_u loss_u 0.8281 (0.8589) acc_u 15.6250 (17.8906) lr 1.8443e-03 eta 0:00:11
epoch [38/200] batch [45/64] time 0.545 (0.480) data 0.410 (0.348) loss_u loss_u 0.8779 (0.8608) acc_u 18.7500 (17.7083) lr 1.8443e-03 eta 0:00:09
epoch [38/200] batch [50/64] time 0.409 (0.478) data 0.276 (0.346) loss_u loss_u 0.8247 (0.8584) acc_u 31.2500 (18.1250) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [55/64] time 0.434 (0.476) data 0.302 (0.344) loss_u loss_u 0.8369 (0.8588) acc_u 21.8750 (17.9545) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [60/64] time 0.497 (0.481) data 0.364 (0.349) loss_u loss_u 0.9072 (0.8597) acc_u 9.3750 (17.8646) lr 1.8443e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1635
confident_label rate tensor(0.3390, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1063
clean true:998
clean false:65
clean_rate:0.9388523047977423
noisy true:503
noisy false:1570
after delete: len(clean_dataset) 1063
after delete: len(noisy_dataset) 2073
epoch [39/200] batch [5/33] time 0.514 (0.451) data 0.384 (0.321) loss_x loss_x 1.3232 (1.5512) acc_x 59.3750 (61.2500) lr 1.8358e-03 eta 0:00:12
epoch [39/200] batch [10/33] time 0.402 (0.458) data 0.271 (0.328) loss_x loss_x 1.0361 (1.3587) acc_x 78.1250 (65.0000) lr 1.8358e-03 eta 0:00:10
epoch [39/200] batch [15/33] time 0.513 (0.458) data 0.382 (0.327) loss_x loss_x 1.4521 (1.3743) acc_x 53.1250 (65.0000) lr 1.8358e-03 eta 0:00:08
epoch [39/200] batch [20/33] time 0.434 (0.456) data 0.303 (0.326) loss_x loss_x 2.0977 (1.3901) acc_x 53.1250 (65.0000) lr 1.8358e-03 eta 0:00:05
epoch [39/200] batch [25/33] time 0.397 (0.448) data 0.266 (0.318) loss_x loss_x 1.3809 (1.4148) acc_x 65.6250 (63.7500) lr 1.8358e-03 eta 0:00:03
epoch [39/200] batch [30/33] time 0.429 (0.451) data 0.299 (0.320) loss_x loss_x 1.2578 (1.4366) acc_x 65.6250 (63.2292) lr 1.8358e-03 eta 0:00:01
epoch [39/200] batch [5/64] time 0.492 (0.451) data 0.360 (0.321) loss_u loss_u 0.9268 (0.8887) acc_u 9.3750 (14.3750) lr 1.8358e-03 eta 0:00:26
epoch [39/200] batch [10/64] time 0.417 (0.457) data 0.285 (0.326) loss_u loss_u 0.8730 (0.8681) acc_u 15.6250 (17.5000) lr 1.8358e-03 eta 0:00:24
epoch [39/200] batch [15/64] time 0.387 (0.458) data 0.256 (0.327) loss_u loss_u 0.9243 (0.8630) acc_u 6.2500 (17.7083) lr 1.8358e-03 eta 0:00:22
epoch [39/200] batch [20/64] time 0.445 (0.455) data 0.314 (0.324) loss_u loss_u 0.8677 (0.8582) acc_u 15.6250 (18.5938) lr 1.8358e-03 eta 0:00:20
epoch [39/200] batch [25/64] time 0.437 (0.458) data 0.305 (0.327) loss_u loss_u 0.8931 (0.8608) acc_u 9.3750 (17.7500) lr 1.8358e-03 eta 0:00:17
epoch [39/200] batch [30/64] time 0.384 (0.452) data 0.253 (0.321) loss_u loss_u 0.8765 (0.8579) acc_u 15.6250 (18.3333) lr 1.8358e-03 eta 0:00:15
epoch [39/200] batch [35/64] time 0.424 (0.451) data 0.293 (0.320) loss_u loss_u 0.8428 (0.8582) acc_u 25.0000 (18.2143) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [40/64] time 0.409 (0.449) data 0.277 (0.318) loss_u loss_u 0.8101 (0.8588) acc_u 18.7500 (18.2812) lr 1.8358e-03 eta 0:00:10
epoch [39/200] batch [45/64] time 0.377 (0.449) data 0.247 (0.318) loss_u loss_u 0.8594 (0.8604) acc_u 21.8750 (17.9861) lr 1.8358e-03 eta 0:00:08
epoch [39/200] batch [50/64] time 0.371 (0.445) data 0.239 (0.314) loss_u loss_u 0.9136 (0.8619) acc_u 9.3750 (17.6250) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [55/64] time 0.491 (0.447) data 0.360 (0.315) loss_u loss_u 0.8418 (0.8597) acc_u 21.8750 (17.7841) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [60/64] time 0.457 (0.449) data 0.326 (0.318) loss_u loss_u 0.8779 (0.8617) acc_u 18.7500 (17.4479) lr 1.8358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1593
confident_label rate tensor(0.3482, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1092
clean true:1033
clean false:59
clean_rate:0.9459706959706959
noisy true:510
noisy false:1534
after delete: len(clean_dataset) 1092
after delete: len(noisy_dataset) 2044
epoch [40/200] batch [5/34] time 0.471 (0.459) data 0.341 (0.329) loss_x loss_x 1.4229 (1.4385) acc_x 59.3750 (63.7500) lr 1.8271e-03 eta 0:00:13
epoch [40/200] batch [10/34] time 0.497 (0.449) data 0.366 (0.318) loss_x loss_x 1.5088 (1.4367) acc_x 62.5000 (60.9375) lr 1.8271e-03 eta 0:00:10
epoch [40/200] batch [15/34] time 0.531 (0.479) data 0.398 (0.348) loss_x loss_x 0.8682 (1.3507) acc_x 87.5000 (64.1667) lr 1.8271e-03 eta 0:00:09
epoch [40/200] batch [20/34] time 0.363 (0.479) data 0.232 (0.347) loss_x loss_x 1.3760 (1.3728) acc_x 75.0000 (65.0000) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [25/34] time 0.681 (0.481) data 0.548 (0.350) loss_x loss_x 1.1982 (1.3559) acc_x 68.7500 (64.1250) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [30/34] time 0.491 (0.483) data 0.359 (0.351) loss_x loss_x 0.9092 (1.3364) acc_x 68.7500 (64.2708) lr 1.8271e-03 eta 0:00:01
epoch [40/200] batch [5/63] time 0.510 (0.477) data 0.378 (0.345) loss_u loss_u 0.8579 (0.8828) acc_u 15.6250 (15.0000) lr 1.8271e-03 eta 0:00:27
epoch [40/200] batch [10/63] time 0.518 (0.484) data 0.386 (0.352) loss_u loss_u 0.9229 (0.8701) acc_u 6.2500 (15.9375) lr 1.8271e-03 eta 0:00:25
epoch [40/200] batch [15/63] time 0.500 (0.487) data 0.368 (0.355) loss_u loss_u 0.8779 (0.8694) acc_u 15.6250 (16.2500) lr 1.8271e-03 eta 0:00:23
epoch [40/200] batch [20/63] time 0.392 (0.484) data 0.259 (0.352) loss_u loss_u 0.8481 (0.8673) acc_u 21.8750 (17.0312) lr 1.8271e-03 eta 0:00:20
epoch [40/200] batch [25/63] time 0.471 (0.482) data 0.339 (0.350) loss_u loss_u 0.7876 (0.8606) acc_u 28.1250 (17.8750) lr 1.8271e-03 eta 0:00:18
epoch [40/200] batch [30/63] time 0.421 (0.489) data 0.290 (0.357) loss_u loss_u 0.9258 (0.8673) acc_u 6.2500 (17.1875) lr 1.8271e-03 eta 0:00:16
epoch [40/200] batch [35/63] time 0.397 (0.482) data 0.266 (0.350) loss_u loss_u 0.8530 (0.8665) acc_u 18.7500 (17.1429) lr 1.8271e-03 eta 0:00:13
epoch [40/200] batch [40/63] time 0.358 (0.481) data 0.226 (0.349) loss_u loss_u 0.9424 (0.8686) acc_u 6.2500 (16.8750) lr 1.8271e-03 eta 0:00:11
epoch [40/200] batch [45/63] time 0.407 (0.476) data 0.275 (0.344) loss_u loss_u 0.8267 (0.8664) acc_u 25.0000 (16.9444) lr 1.8271e-03 eta 0:00:08
epoch [40/200] batch [50/63] time 0.465 (0.474) data 0.333 (0.342) loss_u loss_u 0.8701 (0.8667) acc_u 28.1250 (17.1875) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [55/63] time 0.486 (0.474) data 0.355 (0.342) loss_u loss_u 0.8428 (0.8675) acc_u 18.7500 (17.0455) lr 1.8271e-03 eta 0:00:03
epoch [40/200] batch [60/63] time 0.358 (0.472) data 0.227 (0.340) loss_u loss_u 0.9253 (0.8662) acc_u 9.3750 (17.1354) lr 1.8271e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1619
confident_label rate tensor(0.3415, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1071
clean true:1008
clean false:63
clean_rate:0.9411764705882353
noisy true:509
noisy false:1556
after delete: len(clean_dataset) 1071
after delete: len(noisy_dataset) 2065
epoch [41/200] batch [5/33] time 0.386 (0.435) data 0.255 (0.304) loss_x loss_x 1.2549 (1.2270) acc_x 78.1250 (75.0000) lr 1.8181e-03 eta 0:00:12
epoch [41/200] batch [10/33] time 0.494 (0.443) data 0.363 (0.312) loss_x loss_x 1.5430 (1.3525) acc_x 62.5000 (70.0000) lr 1.8181e-03 eta 0:00:10
epoch [41/200] batch [15/33] time 0.412 (0.439) data 0.282 (0.309) loss_x loss_x 1.2266 (1.3128) acc_x 68.7500 (68.9583) lr 1.8181e-03 eta 0:00:07
epoch [41/200] batch [20/33] time 0.484 (0.449) data 0.353 (0.319) loss_x loss_x 1.0654 (1.2830) acc_x 68.7500 (69.2188) lr 1.8181e-03 eta 0:00:05
epoch [41/200] batch [25/33] time 0.427 (0.450) data 0.297 (0.319) loss_x loss_x 1.3555 (1.2869) acc_x 68.7500 (68.0000) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [30/33] time 0.473 (0.455) data 0.343 (0.325) loss_x loss_x 1.5420 (1.3027) acc_x 59.3750 (66.9792) lr 1.8181e-03 eta 0:00:01
epoch [41/200] batch [5/64] time 0.393 (0.461) data 0.262 (0.330) loss_u loss_u 0.8730 (0.8631) acc_u 21.8750 (16.2500) lr 1.8181e-03 eta 0:00:27
epoch [41/200] batch [10/64] time 0.346 (0.459) data 0.215 (0.328) loss_u loss_u 0.8613 (0.8575) acc_u 21.8750 (17.1875) lr 1.8181e-03 eta 0:00:24
epoch [41/200] batch [15/64] time 0.368 (0.457) data 0.237 (0.326) loss_u loss_u 0.9155 (0.8631) acc_u 12.5000 (16.8750) lr 1.8181e-03 eta 0:00:22
epoch [41/200] batch [20/64] time 0.452 (0.457) data 0.317 (0.326) loss_u loss_u 0.8687 (0.8655) acc_u 25.0000 (17.1875) lr 1.8181e-03 eta 0:00:20
epoch [41/200] batch [25/64] time 0.447 (0.459) data 0.316 (0.328) loss_u loss_u 0.7480 (0.8626) acc_u 37.5000 (18.1250) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [30/64] time 0.433 (0.459) data 0.302 (0.327) loss_u loss_u 0.8149 (0.8585) acc_u 28.1250 (18.6458) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [35/64] time 0.520 (0.461) data 0.388 (0.330) loss_u loss_u 0.8091 (0.8593) acc_u 18.7500 (18.3036) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [40/64] time 0.303 (0.457) data 0.172 (0.326) loss_u loss_u 0.8398 (0.8578) acc_u 18.7500 (18.6719) lr 1.8181e-03 eta 0:00:10
epoch [41/200] batch [45/64] time 0.363 (0.458) data 0.231 (0.327) loss_u loss_u 0.8008 (0.8556) acc_u 18.7500 (18.6806) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [50/64] time 0.368 (0.457) data 0.237 (0.325) loss_u loss_u 0.9165 (0.8548) acc_u 12.5000 (18.6875) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [55/64] time 0.407 (0.453) data 0.275 (0.321) loss_u loss_u 0.9375 (0.8558) acc_u 9.3750 (18.4091) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [60/64] time 0.611 (0.453) data 0.479 (0.321) loss_u loss_u 0.8872 (0.8571) acc_u 12.5000 (18.2292) lr 1.8181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1596
confident_label rate tensor(0.3469, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1088
clean true:1017
clean false:71
clean_rate:0.9347426470588235
noisy true:523
noisy false:1525
after delete: len(clean_dataset) 1088
after delete: len(noisy_dataset) 2048
epoch [42/200] batch [5/34] time 0.406 (0.471) data 0.275 (0.340) loss_x loss_x 1.3770 (1.4056) acc_x 62.5000 (66.8750) lr 1.8090e-03 eta 0:00:13
epoch [42/200] batch [10/34] time 0.434 (0.487) data 0.303 (0.356) loss_x loss_x 1.2852 (1.3434) acc_x 71.8750 (69.6875) lr 1.8090e-03 eta 0:00:11
epoch [42/200] batch [15/34] time 0.508 (0.486) data 0.377 (0.355) loss_x loss_x 1.3184 (1.3337) acc_x 65.6250 (69.3750) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [20/34] time 0.469 (0.493) data 0.338 (0.363) loss_x loss_x 1.3135 (1.3719) acc_x 65.6250 (67.5000) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [25/34] time 0.554 (0.489) data 0.423 (0.359) loss_x loss_x 1.0391 (1.3422) acc_x 68.7500 (67.3750) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [30/34] time 0.477 (0.480) data 0.346 (0.350) loss_x loss_x 1.0586 (1.3313) acc_x 65.6250 (66.5625) lr 1.8090e-03 eta 0:00:01
epoch [42/200] batch [5/64] time 0.631 (0.475) data 0.500 (0.344) loss_u loss_u 0.8726 (0.8708) acc_u 21.8750 (17.5000) lr 1.8090e-03 eta 0:00:28
epoch [42/200] batch [10/64] time 0.408 (0.469) data 0.275 (0.338) loss_u loss_u 0.8940 (0.8566) acc_u 9.3750 (18.1250) lr 1.8090e-03 eta 0:00:25
epoch [42/200] batch [15/64] time 0.480 (0.473) data 0.348 (0.342) loss_u loss_u 0.8403 (0.8632) acc_u 21.8750 (17.5000) lr 1.8090e-03 eta 0:00:23
epoch [42/200] batch [20/64] time 0.396 (0.467) data 0.265 (0.336) loss_u loss_u 0.8799 (0.8588) acc_u 12.5000 (17.5000) lr 1.8090e-03 eta 0:00:20
epoch [42/200] batch [25/64] time 0.476 (0.466) data 0.344 (0.334) loss_u loss_u 0.8643 (0.8613) acc_u 18.7500 (17.6250) lr 1.8090e-03 eta 0:00:18
epoch [42/200] batch [30/64] time 0.494 (0.464) data 0.363 (0.333) loss_u loss_u 0.8691 (0.8584) acc_u 15.6250 (17.7083) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [35/64] time 0.352 (0.461) data 0.219 (0.330) loss_u loss_u 0.8545 (0.8599) acc_u 12.5000 (17.4107) lr 1.8090e-03 eta 0:00:13
epoch [42/200] batch [40/64] time 0.410 (0.460) data 0.278 (0.329) loss_u loss_u 0.8735 (0.8643) acc_u 15.6250 (16.8750) lr 1.8090e-03 eta 0:00:11
epoch [42/200] batch [45/64] time 0.374 (0.459) data 0.242 (0.328) loss_u loss_u 0.8677 (0.8621) acc_u 15.6250 (16.8750) lr 1.8090e-03 eta 0:00:08
epoch [42/200] batch [50/64] time 0.568 (0.460) data 0.436 (0.329) loss_u loss_u 0.9258 (0.8638) acc_u 6.2500 (16.6875) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [55/64] time 0.540 (0.467) data 0.404 (0.336) loss_u loss_u 0.8252 (0.8633) acc_u 15.6250 (16.8182) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [60/64] time 0.401 (0.464) data 0.269 (0.333) loss_u loss_u 0.9121 (0.8641) acc_u 15.6250 (16.8229) lr 1.8090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1606
confident_label rate tensor(0.3460, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1085
clean true:1015
clean false:70
clean_rate:0.9354838709677419
noisy true:515
noisy false:1536
after delete: len(clean_dataset) 1085
after delete: len(noisy_dataset) 2051
epoch [43/200] batch [5/33] time 0.419 (0.498) data 0.288 (0.367) loss_x loss_x 1.9785 (1.3512) acc_x 62.5000 (66.8750) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [10/33] time 0.398 (0.480) data 0.267 (0.349) loss_x loss_x 1.3584 (1.2522) acc_x 68.7500 (68.7500) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [15/33] time 0.481 (0.472) data 0.350 (0.341) loss_x loss_x 1.0244 (1.2107) acc_x 75.0000 (69.5833) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [20/33] time 0.382 (0.462) data 0.250 (0.331) loss_x loss_x 1.7490 (1.2690) acc_x 50.0000 (67.8125) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [25/33] time 0.405 (0.463) data 0.274 (0.332) loss_x loss_x 1.4834 (1.2678) acc_x 53.1250 (67.3750) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [30/33] time 0.414 (0.460) data 0.283 (0.329) loss_x loss_x 1.3965 (1.2568) acc_x 65.6250 (67.5000) lr 1.7997e-03 eta 0:00:01
epoch [43/200] batch [5/64] time 0.433 (0.473) data 0.301 (0.342) loss_u loss_u 0.8071 (0.8401) acc_u 21.8750 (19.3750) lr 1.7997e-03 eta 0:00:27
epoch [43/200] batch [10/64] time 0.323 (0.460) data 0.192 (0.329) loss_u loss_u 0.8652 (0.8523) acc_u 12.5000 (18.7500) lr 1.7997e-03 eta 0:00:24
epoch [43/200] batch [15/64] time 0.404 (0.458) data 0.272 (0.326) loss_u loss_u 0.8887 (0.8553) acc_u 15.6250 (17.7083) lr 1.7997e-03 eta 0:00:22
epoch [43/200] batch [20/64] time 0.447 (0.455) data 0.315 (0.324) loss_u loss_u 0.9209 (0.8638) acc_u 9.3750 (16.8750) lr 1.7997e-03 eta 0:00:20
epoch [43/200] batch [25/64] time 0.515 (0.465) data 0.381 (0.333) loss_u loss_u 0.9009 (0.8671) acc_u 15.6250 (16.5000) lr 1.7997e-03 eta 0:00:18
epoch [43/200] batch [30/64] time 0.479 (0.468) data 0.344 (0.337) loss_u loss_u 0.7485 (0.8667) acc_u 34.3750 (16.4583) lr 1.7997e-03 eta 0:00:15
epoch [43/200] batch [35/64] time 0.515 (0.469) data 0.383 (0.337) loss_u loss_u 0.8442 (0.8630) acc_u 18.7500 (16.8750) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [40/64] time 0.425 (0.467) data 0.292 (0.336) loss_u loss_u 0.8647 (0.8629) acc_u 18.7500 (17.1875) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [45/64] time 0.427 (0.464) data 0.294 (0.332) loss_u loss_u 0.8398 (0.8614) acc_u 28.1250 (17.6389) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [50/64] time 0.462 (0.465) data 0.330 (0.333) loss_u loss_u 0.8496 (0.8617) acc_u 18.7500 (17.6250) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [55/64] time 0.378 (0.466) data 0.245 (0.334) loss_u loss_u 0.8442 (0.8628) acc_u 15.6250 (17.3295) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [60/64] time 0.324 (0.465) data 0.192 (0.333) loss_u loss_u 0.8242 (0.8614) acc_u 21.8750 (17.5521) lr 1.7997e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1627
confident_label rate tensor(0.3425, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1074
clean true:1008
clean false:66
clean_rate:0.9385474860335196
noisy true:501
noisy false:1561
after delete: len(clean_dataset) 1074
after delete: len(noisy_dataset) 2062
epoch [44/200] batch [5/33] time 0.418 (0.443) data 0.287 (0.312) loss_x loss_x 1.3486 (1.2398) acc_x 62.5000 (66.8750) lr 1.7902e-03 eta 0:00:12
epoch [44/200] batch [10/33] time 0.576 (0.441) data 0.445 (0.310) loss_x loss_x 1.3994 (1.2922) acc_x 68.7500 (68.4375) lr 1.7902e-03 eta 0:00:10
epoch [44/200] batch [15/33] time 0.562 (0.468) data 0.431 (0.337) loss_x loss_x 1.6553 (1.3866) acc_x 59.3750 (66.6667) lr 1.7902e-03 eta 0:00:08
epoch [44/200] batch [20/33] time 0.544 (0.458) data 0.413 (0.327) loss_x loss_x 1.0303 (1.3823) acc_x 71.8750 (65.6250) lr 1.7902e-03 eta 0:00:05
epoch [44/200] batch [25/33] time 0.445 (0.461) data 0.314 (0.330) loss_x loss_x 1.5859 (1.3990) acc_x 56.2500 (65.0000) lr 1.7902e-03 eta 0:00:03
epoch [44/200] batch [30/33] time 0.457 (0.476) data 0.326 (0.345) loss_x loss_x 1.5391 (1.3856) acc_x 59.3750 (65.0000) lr 1.7902e-03 eta 0:00:01
epoch [44/200] batch [5/64] time 0.509 (0.472) data 0.376 (0.341) loss_u loss_u 0.8389 (0.8688) acc_u 21.8750 (17.5000) lr 1.7902e-03 eta 0:00:27
epoch [44/200] batch [10/64] time 0.537 (0.469) data 0.405 (0.338) loss_u loss_u 0.8423 (0.8672) acc_u 21.8750 (17.5000) lr 1.7902e-03 eta 0:00:25
epoch [44/200] batch [15/64] time 0.711 (0.480) data 0.578 (0.349) loss_u loss_u 0.8369 (0.8701) acc_u 21.8750 (17.2917) lr 1.7902e-03 eta 0:00:23
epoch [44/200] batch [20/64] time 0.435 (0.484) data 0.300 (0.353) loss_u loss_u 0.8989 (0.8702) acc_u 18.7500 (17.5000) lr 1.7902e-03 eta 0:00:21
epoch [44/200] batch [25/64] time 0.456 (0.480) data 0.324 (0.348) loss_u loss_u 0.8589 (0.8708) acc_u 15.6250 (17.1250) lr 1.7902e-03 eta 0:00:18
epoch [44/200] batch [30/64] time 0.406 (0.473) data 0.274 (0.342) loss_u loss_u 0.8496 (0.8613) acc_u 15.6250 (17.7083) lr 1.7902e-03 eta 0:00:16
epoch [44/200] batch [35/64] time 0.461 (0.477) data 0.330 (0.345) loss_u loss_u 0.9316 (0.8627) acc_u 9.3750 (17.5893) lr 1.7902e-03 eta 0:00:13
epoch [44/200] batch [40/64] time 0.633 (0.480) data 0.501 (0.349) loss_u loss_u 0.8696 (0.8646) acc_u 18.7500 (17.2656) lr 1.7902e-03 eta 0:00:11
epoch [44/200] batch [45/64] time 0.400 (0.476) data 0.268 (0.345) loss_u loss_u 0.8457 (0.8649) acc_u 21.8750 (17.1528) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [50/64] time 0.527 (0.475) data 0.396 (0.343) loss_u loss_u 0.8262 (0.8636) acc_u 18.7500 (16.9375) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [55/64] time 0.340 (0.474) data 0.210 (0.343) loss_u loss_u 0.9160 (0.8623) acc_u 9.3750 (17.1023) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [60/64] time 0.378 (0.471) data 0.247 (0.339) loss_u loss_u 0.8770 (0.8627) acc_u 15.6250 (17.0833) lr 1.7902e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1612
confident_label rate tensor(0.3485, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1093
clean true:1019
clean false:74
clean_rate:0.9322964318389753
noisy true:505
noisy false:1538
after delete: len(clean_dataset) 1093
after delete: len(noisy_dataset) 2043
epoch [45/200] batch [5/34] time 0.503 (0.512) data 0.372 (0.381) loss_x loss_x 1.0576 (1.0577) acc_x 81.2500 (75.0000) lr 1.7804e-03 eta 0:00:14
epoch [45/200] batch [10/34] time 0.350 (0.498) data 0.218 (0.367) loss_x loss_x 1.2354 (1.2045) acc_x 59.3750 (70.6250) lr 1.7804e-03 eta 0:00:11
epoch [45/200] batch [15/34] time 0.469 (0.476) data 0.338 (0.345) loss_x loss_x 0.9800 (1.2263) acc_x 65.6250 (69.1667) lr 1.7804e-03 eta 0:00:09
epoch [45/200] batch [20/34] time 0.441 (0.461) data 0.310 (0.330) loss_x loss_x 1.6133 (1.2646) acc_x 65.6250 (68.7500) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [25/34] time 0.453 (0.455) data 0.321 (0.324) loss_x loss_x 1.4443 (1.2926) acc_x 53.1250 (67.2500) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [30/34] time 0.502 (0.458) data 0.372 (0.327) loss_x loss_x 1.4102 (1.2846) acc_x 71.8750 (67.1875) lr 1.7804e-03 eta 0:00:01
epoch [45/200] batch [5/63] time 0.500 (0.464) data 0.369 (0.333) loss_u loss_u 0.8867 (0.8633) acc_u 12.5000 (17.5000) lr 1.7804e-03 eta 0:00:26
epoch [45/200] batch [10/63] time 0.373 (0.458) data 0.237 (0.326) loss_u loss_u 0.8862 (0.8475) acc_u 12.5000 (19.0625) lr 1.7804e-03 eta 0:00:24
epoch [45/200] batch [15/63] time 0.476 (0.453) data 0.344 (0.322) loss_u loss_u 0.9023 (0.8532) acc_u 9.3750 (18.1250) lr 1.7804e-03 eta 0:00:21
epoch [45/200] batch [20/63] time 0.457 (0.457) data 0.325 (0.325) loss_u loss_u 0.8994 (0.8541) acc_u 12.5000 (18.2812) lr 1.7804e-03 eta 0:00:19
epoch [45/200] batch [25/63] time 0.402 (0.460) data 0.270 (0.329) loss_u loss_u 0.8760 (0.8572) acc_u 15.6250 (17.8750) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [30/63] time 0.561 (0.464) data 0.429 (0.332) loss_u loss_u 0.8066 (0.8599) acc_u 21.8750 (17.0833) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [35/63] time 0.535 (0.463) data 0.403 (0.331) loss_u loss_u 0.9248 (0.8602) acc_u 6.2500 (17.4107) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [40/63] time 0.576 (0.464) data 0.443 (0.332) loss_u loss_u 0.8735 (0.8651) acc_u 25.0000 (16.8750) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [45/63] time 0.450 (0.465) data 0.318 (0.333) loss_u loss_u 0.8647 (0.8623) acc_u 21.8750 (17.2222) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [50/63] time 0.460 (0.465) data 0.328 (0.333) loss_u loss_u 0.8813 (0.8623) acc_u 12.5000 (16.8125) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [55/63] time 0.341 (0.465) data 0.209 (0.333) loss_u loss_u 0.9336 (0.8632) acc_u 9.3750 (16.8182) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [60/63] time 0.506 (0.466) data 0.374 (0.334) loss_u loss_u 0.9126 (0.8651) acc_u 3.1250 (16.3021) lr 1.7804e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1617
confident_label rate tensor(0.3498, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1097
clean true:1039
clean false:58
clean_rate:0.9471285323609845
noisy true:480
noisy false:1559
after delete: len(clean_dataset) 1097
after delete: len(noisy_dataset) 2039
epoch [46/200] batch [5/34] time 0.451 (0.488) data 0.320 (0.357) loss_x loss_x 0.8950 (1.2091) acc_x 78.1250 (70.0000) lr 1.7705e-03 eta 0:00:14
epoch [46/200] batch [10/34] time 0.439 (0.479) data 0.309 (0.348) loss_x loss_x 2.1309 (1.3191) acc_x 50.0000 (66.5625) lr 1.7705e-03 eta 0:00:11
epoch [46/200] batch [15/34] time 0.367 (0.463) data 0.236 (0.332) loss_x loss_x 1.5771 (1.3282) acc_x 53.1250 (65.6250) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [20/34] time 0.435 (0.454) data 0.304 (0.324) loss_x loss_x 1.3867 (1.3595) acc_x 68.7500 (65.3125) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [25/34] time 0.400 (0.455) data 0.269 (0.324) loss_x loss_x 1.1191 (1.3363) acc_x 71.8750 (65.6250) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [30/34] time 0.465 (0.461) data 0.334 (0.330) loss_x loss_x 1.3105 (1.3232) acc_x 68.7500 (65.8333) lr 1.7705e-03 eta 0:00:01
epoch [46/200] batch [5/63] time 0.422 (0.468) data 0.289 (0.336) loss_u loss_u 0.9009 (0.8509) acc_u 3.1250 (17.5000) lr 1.7705e-03 eta 0:00:27
epoch [46/200] batch [10/63] time 0.586 (0.465) data 0.453 (0.334) loss_u loss_u 0.9043 (0.8739) acc_u 15.6250 (15.3125) lr 1.7705e-03 eta 0:00:24
epoch [46/200] batch [15/63] time 0.356 (0.462) data 0.223 (0.330) loss_u loss_u 0.8779 (0.8673) acc_u 12.5000 (15.6250) lr 1.7705e-03 eta 0:00:22
epoch [46/200] batch [20/63] time 0.553 (0.460) data 0.420 (0.329) loss_u loss_u 0.8433 (0.8617) acc_u 18.7500 (16.2500) lr 1.7705e-03 eta 0:00:19
epoch [46/200] batch [25/63] time 0.518 (0.460) data 0.386 (0.329) loss_u loss_u 0.9219 (0.8647) acc_u 12.5000 (15.7500) lr 1.7705e-03 eta 0:00:17
epoch [46/200] batch [30/63] time 0.388 (0.458) data 0.256 (0.326) loss_u loss_u 0.8564 (0.8644) acc_u 21.8750 (15.9375) lr 1.7705e-03 eta 0:00:15
epoch [46/200] batch [35/63] time 0.372 (0.458) data 0.242 (0.326) loss_u loss_u 0.8198 (0.8629) acc_u 18.7500 (16.3393) lr 1.7705e-03 eta 0:00:12
epoch [46/200] batch [40/63] time 0.467 (0.460) data 0.336 (0.329) loss_u loss_u 0.7739 (0.8644) acc_u 31.2500 (16.1719) lr 1.7705e-03 eta 0:00:10
epoch [46/200] batch [45/63] time 0.485 (0.460) data 0.353 (0.328) loss_u loss_u 0.8750 (0.8639) acc_u 18.7500 (16.3194) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [50/63] time 0.543 (0.460) data 0.411 (0.328) loss_u loss_u 0.8838 (0.8656) acc_u 12.5000 (16.3125) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [55/63] time 0.452 (0.456) data 0.320 (0.325) loss_u loss_u 0.8818 (0.8681) acc_u 15.6250 (15.9659) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [60/63] time 0.377 (0.455) data 0.245 (0.324) loss_u loss_u 0.8276 (0.8656) acc_u 25.0000 (16.2500) lr 1.7705e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1598
confident_label rate tensor(0.3498, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1097
clean true:1031
clean false:66
clean_rate:0.9398359161349134
noisy true:507
noisy false:1532
after delete: len(clean_dataset) 1097
after delete: len(noisy_dataset) 2039
epoch [47/200] batch [5/34] time 0.526 (0.484) data 0.395 (0.353) loss_x loss_x 1.1182 (1.2946) acc_x 71.8750 (66.2500) lr 1.7604e-03 eta 0:00:14
epoch [47/200] batch [10/34] time 0.452 (0.462) data 0.321 (0.331) loss_x loss_x 1.5117 (1.3686) acc_x 71.8750 (63.7500) lr 1.7604e-03 eta 0:00:11
epoch [47/200] batch [15/34] time 0.413 (0.458) data 0.283 (0.328) loss_x loss_x 1.0537 (1.2561) acc_x 62.5000 (65.8333) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [20/34] time 0.620 (0.478) data 0.490 (0.347) loss_x loss_x 1.3447 (1.2571) acc_x 68.7500 (66.2500) lr 1.7604e-03 eta 0:00:06
epoch [47/200] batch [25/34] time 0.374 (0.475) data 0.244 (0.344) loss_x loss_x 1.0684 (1.2466) acc_x 71.8750 (67.2500) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [30/34] time 0.417 (0.471) data 0.285 (0.340) loss_x loss_x 1.0859 (1.2096) acc_x 71.8750 (68.2292) lr 1.7604e-03 eta 0:00:01
epoch [47/200] batch [5/63] time 0.404 (0.465) data 0.273 (0.335) loss_u loss_u 0.9565 (0.9096) acc_u 6.2500 (12.5000) lr 1.7604e-03 eta 0:00:26
epoch [47/200] batch [10/63] time 0.472 (0.461) data 0.341 (0.330) loss_u loss_u 0.9224 (0.9050) acc_u 9.3750 (12.8125) lr 1.7604e-03 eta 0:00:24
epoch [47/200] batch [15/63] time 0.563 (0.467) data 0.432 (0.336) loss_u loss_u 0.9106 (0.8965) acc_u 6.2500 (12.5000) lr 1.7604e-03 eta 0:00:22
epoch [47/200] batch [20/63] time 0.474 (0.462) data 0.342 (0.331) loss_u loss_u 0.8779 (0.8915) acc_u 15.6250 (13.1250) lr 1.7604e-03 eta 0:00:19
epoch [47/200] batch [25/63] time 0.407 (0.457) data 0.275 (0.326) loss_u loss_u 0.8872 (0.8808) acc_u 12.5000 (14.7500) lr 1.7604e-03 eta 0:00:17
epoch [47/200] batch [30/63] time 0.444 (0.460) data 0.312 (0.328) loss_u loss_u 0.8774 (0.8743) acc_u 12.5000 (15.2083) lr 1.7604e-03 eta 0:00:15
epoch [47/200] batch [35/63] time 0.386 (0.461) data 0.256 (0.330) loss_u loss_u 0.8389 (0.8733) acc_u 15.6250 (15.1786) lr 1.7604e-03 eta 0:00:12
epoch [47/200] batch [40/63] time 0.544 (0.463) data 0.413 (0.331) loss_u loss_u 0.8516 (0.8711) acc_u 15.6250 (15.5469) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [45/63] time 0.458 (0.461) data 0.327 (0.329) loss_u loss_u 0.8735 (0.8709) acc_u 12.5000 (15.4861) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [50/63] time 0.428 (0.458) data 0.296 (0.326) loss_u loss_u 0.8789 (0.8704) acc_u 15.6250 (15.6875) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [55/63] time 0.420 (0.457) data 0.290 (0.326) loss_u loss_u 0.8550 (0.8688) acc_u 15.6250 (15.9091) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [60/63] time 0.435 (0.456) data 0.304 (0.325) loss_u loss_u 0.7808 (0.8668) acc_u 25.0000 (15.9896) lr 1.7604e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1631
confident_label rate tensor(0.3422, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1073
clean true:1017
clean false:56
clean_rate:0.9478098788443616
noisy true:488
noisy false:1575
after delete: len(clean_dataset) 1073
after delete: len(noisy_dataset) 2063
epoch [48/200] batch [5/33] time 0.522 (0.437) data 0.391 (0.306) loss_x loss_x 1.0986 (1.0846) acc_x 71.8750 (71.8750) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [10/33] time 0.418 (0.450) data 0.287 (0.319) loss_x loss_x 0.8906 (1.1584) acc_x 75.0000 (69.6875) lr 1.7501e-03 eta 0:00:10
epoch [48/200] batch [15/33] time 0.465 (0.473) data 0.335 (0.343) loss_x loss_x 1.1201 (1.2040) acc_x 62.5000 (69.1667) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [20/33] time 0.562 (0.492) data 0.430 (0.361) loss_x loss_x 1.0605 (1.1472) acc_x 62.5000 (70.0000) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [25/33] time 0.446 (0.492) data 0.314 (0.361) loss_x loss_x 1.1611 (1.1618) acc_x 75.0000 (70.1250) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [30/33] time 0.347 (0.482) data 0.215 (0.351) loss_x loss_x 1.6885 (1.2049) acc_x 62.5000 (70.0000) lr 1.7501e-03 eta 0:00:01
epoch [48/200] batch [5/64] time 0.426 (0.472) data 0.295 (0.341) loss_u loss_u 0.9019 (0.8268) acc_u 6.2500 (20.6250) lr 1.7501e-03 eta 0:00:27
epoch [48/200] batch [10/64] time 0.489 (0.472) data 0.356 (0.340) loss_u loss_u 0.8647 (0.8576) acc_u 18.7500 (17.8125) lr 1.7501e-03 eta 0:00:25
epoch [48/200] batch [15/64] time 0.391 (0.471) data 0.260 (0.339) loss_u loss_u 0.8140 (0.8632) acc_u 18.7500 (16.8750) lr 1.7501e-03 eta 0:00:23
epoch [48/200] batch [20/64] time 0.424 (0.475) data 0.291 (0.343) loss_u loss_u 0.8877 (0.8655) acc_u 12.5000 (16.5625) lr 1.7501e-03 eta 0:00:20
epoch [48/200] batch [25/64] time 0.496 (0.478) data 0.361 (0.346) loss_u loss_u 0.8955 (0.8637) acc_u 12.5000 (16.7500) lr 1.7501e-03 eta 0:00:18
epoch [48/200] batch [30/64] time 0.622 (0.477) data 0.487 (0.345) loss_u loss_u 0.7441 (0.8608) acc_u 31.2500 (17.5000) lr 1.7501e-03 eta 0:00:16
epoch [48/200] batch [35/64] time 0.386 (0.477) data 0.251 (0.345) loss_u loss_u 0.7915 (0.8593) acc_u 34.3750 (17.8571) lr 1.7501e-03 eta 0:00:13
epoch [48/200] batch [40/64] time 0.443 (0.476) data 0.310 (0.343) loss_u loss_u 0.7334 (0.8563) acc_u 31.2500 (18.3594) lr 1.7501e-03 eta 0:00:11
epoch [48/200] batch [45/64] time 0.548 (0.476) data 0.416 (0.343) loss_u loss_u 0.8862 (0.8589) acc_u 15.6250 (17.9861) lr 1.7501e-03 eta 0:00:09
epoch [48/200] batch [50/64] time 0.629 (0.482) data 0.497 (0.349) loss_u loss_u 0.8408 (0.8579) acc_u 28.1250 (18.6250) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [55/64] time 0.567 (0.481) data 0.435 (0.348) loss_u loss_u 0.8496 (0.8550) acc_u 21.8750 (18.9773) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [60/64] time 0.522 (0.483) data 0.390 (0.351) loss_u loss_u 0.8560 (0.8538) acc_u 15.6250 (19.2708) lr 1.7501e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1566
confident_label rate tensor(0.3571, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1120
clean true:1058
clean false:62
clean_rate:0.9446428571428571
noisy true:512
noisy false:1504
after delete: len(clean_dataset) 1120
after delete: len(noisy_dataset) 2016
epoch [49/200] batch [5/35] time 0.434 (0.536) data 0.303 (0.405) loss_x loss_x 0.9395 (1.2248) acc_x 71.8750 (69.3750) lr 1.7396e-03 eta 0:00:16
epoch [49/200] batch [10/35] time 0.475 (0.487) data 0.343 (0.355) loss_x loss_x 1.0947 (1.1921) acc_x 78.1250 (70.3125) lr 1.7396e-03 eta 0:00:12
epoch [49/200] batch [15/35] time 0.559 (0.471) data 0.427 (0.340) loss_x loss_x 1.1973 (1.2251) acc_x 75.0000 (68.7500) lr 1.7396e-03 eta 0:00:09
epoch [49/200] batch [20/35] time 0.492 (0.476) data 0.361 (0.345) loss_x loss_x 1.6768 (1.2513) acc_x 59.3750 (67.5000) lr 1.7396e-03 eta 0:00:07
epoch [49/200] batch [25/35] time 0.583 (0.477) data 0.452 (0.346) loss_x loss_x 1.2930 (1.2637) acc_x 59.3750 (67.0000) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [30/35] time 0.482 (0.478) data 0.352 (0.346) loss_x loss_x 1.6406 (1.3334) acc_x 59.3750 (66.3542) lr 1.7396e-03 eta 0:00:02
epoch [49/200] batch [35/35] time 0.467 (0.492) data 0.337 (0.361) loss_x loss_x 1.1406 (1.3446) acc_x 65.6250 (65.8036) lr 1.7396e-03 eta 0:00:00
epoch [49/200] batch [5/63] time 0.689 (0.493) data 0.558 (0.362) loss_u loss_u 0.8306 (0.8403) acc_u 31.2500 (20.6250) lr 1.7396e-03 eta 0:00:28
epoch [49/200] batch [10/63] time 0.563 (0.492) data 0.430 (0.360) loss_u loss_u 0.9653 (0.8631) acc_u 3.1250 (16.5625) lr 1.7396e-03 eta 0:00:26
epoch [49/200] batch [15/63] time 0.385 (0.491) data 0.252 (0.360) loss_u loss_u 0.7769 (0.8564) acc_u 21.8750 (17.0833) lr 1.7396e-03 eta 0:00:23
epoch [49/200] batch [20/63] time 0.486 (0.484) data 0.355 (0.353) loss_u loss_u 0.8721 (0.8588) acc_u 21.8750 (17.1875) lr 1.7396e-03 eta 0:00:20
epoch [49/200] batch [25/63] time 0.509 (0.482) data 0.377 (0.350) loss_u loss_u 0.8789 (0.8604) acc_u 18.7500 (16.8750) lr 1.7396e-03 eta 0:00:18
epoch [49/200] batch [30/63] time 0.444 (0.477) data 0.312 (0.346) loss_u loss_u 0.9009 (0.8588) acc_u 12.5000 (17.0833) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [35/63] time 0.571 (0.477) data 0.439 (0.346) loss_u loss_u 0.9170 (0.8624) acc_u 15.6250 (17.2321) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [40/63] time 0.341 (0.474) data 0.208 (0.342) loss_u loss_u 0.9028 (0.8665) acc_u 12.5000 (16.7188) lr 1.7396e-03 eta 0:00:10
epoch [49/200] batch [45/63] time 0.586 (0.473) data 0.454 (0.341) loss_u loss_u 0.7988 (0.8652) acc_u 25.0000 (16.8056) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [50/63] time 0.320 (0.470) data 0.188 (0.338) loss_u loss_u 0.9189 (0.8650) acc_u 15.6250 (16.9375) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [55/63] time 0.439 (0.466) data 0.309 (0.334) loss_u loss_u 0.8594 (0.8663) acc_u 18.7500 (16.7045) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [60/63] time 0.461 (0.464) data 0.329 (0.333) loss_u loss_u 0.8506 (0.8662) acc_u 25.0000 (16.8750) lr 1.7396e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1629
confident_label rate tensor(0.3412, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1070
clean true:1010
clean false:60
clean_rate:0.9439252336448598
noisy true:497
noisy false:1569
after delete: len(clean_dataset) 1070
after delete: len(noisy_dataset) 2066
epoch [50/200] batch [5/33] time 0.459 (0.499) data 0.328 (0.368) loss_x loss_x 1.4600 (1.4311) acc_x 59.3750 (63.7500) lr 1.7290e-03 eta 0:00:13
epoch [50/200] batch [10/33] time 0.528 (0.506) data 0.397 (0.374) loss_x loss_x 1.8662 (1.3816) acc_x 56.2500 (61.8750) lr 1.7290e-03 eta 0:00:11
epoch [50/200] batch [15/33] time 0.423 (0.487) data 0.292 (0.355) loss_x loss_x 1.3330 (1.3154) acc_x 75.0000 (64.3750) lr 1.7290e-03 eta 0:00:08
epoch [50/200] batch [20/33] time 0.443 (0.493) data 0.313 (0.361) loss_x loss_x 1.3711 (1.2801) acc_x 65.6250 (64.6875) lr 1.7290e-03 eta 0:00:06
epoch [50/200] batch [25/33] time 0.396 (0.482) data 0.265 (0.350) loss_x loss_x 1.9434 (1.3085) acc_x 53.1250 (64.2500) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [30/33] time 0.380 (0.479) data 0.249 (0.347) loss_x loss_x 1.4893 (1.3004) acc_x 50.0000 (64.4792) lr 1.7290e-03 eta 0:00:01
epoch [50/200] batch [5/64] time 0.542 (0.476) data 0.410 (0.344) loss_u loss_u 0.8911 (0.8268) acc_u 15.6250 (22.5000) lr 1.7290e-03 eta 0:00:28
epoch [50/200] batch [10/64] time 0.487 (0.480) data 0.356 (0.348) loss_u loss_u 0.8560 (0.8246) acc_u 15.6250 (21.8750) lr 1.7290e-03 eta 0:00:25
epoch [50/200] batch [15/64] time 0.537 (0.489) data 0.405 (0.357) loss_u loss_u 0.9556 (0.8410) acc_u 0.0000 (20.0000) lr 1.7290e-03 eta 0:00:23
epoch [50/200] batch [20/64] time 0.464 (0.483) data 0.332 (0.351) loss_u loss_u 0.8950 (0.8536) acc_u 12.5000 (18.4375) lr 1.7290e-03 eta 0:00:21
epoch [50/200] batch [25/64] time 0.417 (0.475) data 0.286 (0.344) loss_u loss_u 0.9053 (0.8576) acc_u 12.5000 (18.7500) lr 1.7290e-03 eta 0:00:18
epoch [50/200] batch [30/64] time 0.433 (0.478) data 0.301 (0.346) loss_u loss_u 0.8726 (0.8583) acc_u 18.7500 (18.5417) lr 1.7290e-03 eta 0:00:16
epoch [50/200] batch [35/64] time 0.533 (0.476) data 0.398 (0.345) loss_u loss_u 0.8833 (0.8624) acc_u 15.6250 (18.1250) lr 1.7290e-03 eta 0:00:13
epoch [50/200] batch [40/64] time 0.417 (0.475) data 0.283 (0.343) loss_u loss_u 0.8433 (0.8602) acc_u 15.6250 (18.2812) lr 1.7290e-03 eta 0:00:11
epoch [50/200] batch [45/64] time 0.862 (0.478) data 0.730 (0.346) loss_u loss_u 0.9204 (0.8615) acc_u 9.3750 (18.1250) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [50/64] time 0.528 (0.475) data 0.393 (0.343) loss_u loss_u 0.8525 (0.8592) acc_u 21.8750 (18.1250) lr 1.7290e-03 eta 0:00:06
epoch [50/200] batch [55/64] time 0.511 (0.476) data 0.377 (0.344) loss_u loss_u 0.8403 (0.8585) acc_u 21.8750 (18.2955) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [60/64] time 0.432 (0.475) data 0.299 (0.343) loss_u loss_u 0.7505 (0.8590) acc_u 25.0000 (18.1771) lr 1.7290e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1551
confident_label rate tensor(0.3616, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1134
clean true:1067
clean false:67
clean_rate:0.9409171075837742
noisy true:518
noisy false:1484
after delete: len(clean_dataset) 1134
after delete: len(noisy_dataset) 2002
epoch [51/200] batch [5/35] time 0.581 (0.526) data 0.450 (0.394) loss_x loss_x 0.9048 (1.3029) acc_x 68.7500 (70.0000) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [10/35] time 0.440 (0.475) data 0.309 (0.344) loss_x loss_x 1.5293 (1.3945) acc_x 78.1250 (68.1250) lr 1.7181e-03 eta 0:00:11
epoch [51/200] batch [15/35] time 0.329 (0.460) data 0.198 (0.329) loss_x loss_x 1.1133 (1.3113) acc_x 68.7500 (68.1250) lr 1.7181e-03 eta 0:00:09
epoch [51/200] batch [20/35] time 0.539 (0.464) data 0.409 (0.333) loss_x loss_x 1.1523 (1.3047) acc_x 81.2500 (69.2188) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [25/35] time 0.471 (0.469) data 0.340 (0.337) loss_x loss_x 1.5244 (1.3489) acc_x 71.8750 (67.6250) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [30/35] time 0.401 (0.468) data 0.271 (0.337) loss_x loss_x 1.7676 (1.3552) acc_x 59.3750 (67.1875) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [35/35] time 0.446 (0.464) data 0.315 (0.333) loss_x loss_x 1.2002 (1.3369) acc_x 75.0000 (67.7679) lr 1.7181e-03 eta 0:00:00
epoch [51/200] batch [5/62] time 0.423 (0.465) data 0.291 (0.334) loss_u loss_u 0.9077 (0.8629) acc_u 12.5000 (18.1250) lr 1.7181e-03 eta 0:00:26
epoch [51/200] batch [10/62] time 0.473 (0.463) data 0.341 (0.331) loss_u loss_u 0.8989 (0.8652) acc_u 6.2500 (15.6250) lr 1.7181e-03 eta 0:00:24
epoch [51/200] batch [15/62] time 0.500 (0.457) data 0.368 (0.325) loss_u loss_u 0.8965 (0.8654) acc_u 9.3750 (15.8333) lr 1.7181e-03 eta 0:00:21
epoch [51/200] batch [20/62] time 0.386 (0.460) data 0.255 (0.329) loss_u loss_u 0.8872 (0.8679) acc_u 12.5000 (15.9375) lr 1.7181e-03 eta 0:00:19
epoch [51/200] batch [25/62] time 0.448 (0.457) data 0.315 (0.326) loss_u loss_u 0.8291 (0.8670) acc_u 21.8750 (16.3750) lr 1.7181e-03 eta 0:00:16
epoch [51/200] batch [30/62] time 0.375 (0.454) data 0.243 (0.322) loss_u loss_u 0.8218 (0.8699) acc_u 25.0000 (15.8333) lr 1.7181e-03 eta 0:00:14
epoch [51/200] batch [35/62] time 0.419 (0.451) data 0.288 (0.320) loss_u loss_u 0.8281 (0.8693) acc_u 15.6250 (15.8036) lr 1.7181e-03 eta 0:00:12
epoch [51/200] batch [40/62] time 0.460 (0.454) data 0.329 (0.323) loss_u loss_u 0.8965 (0.8691) acc_u 12.5000 (15.6250) lr 1.7181e-03 eta 0:00:09
epoch [51/200] batch [45/62] time 0.689 (0.457) data 0.558 (0.326) loss_u loss_u 0.9194 (0.8666) acc_u 12.5000 (16.1806) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [50/62] time 0.486 (0.458) data 0.354 (0.326) loss_u loss_u 0.8091 (0.8639) acc_u 21.8750 (16.6875) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [55/62] time 0.409 (0.458) data 0.278 (0.326) loss_u loss_u 0.9272 (0.8645) acc_u 3.1250 (16.5909) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [60/62] time 0.465 (0.456) data 0.333 (0.325) loss_u loss_u 0.8779 (0.8646) acc_u 12.5000 (16.5625) lr 1.7181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1625
confident_label rate tensor(0.3463, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1086
clean true:1017
clean false:69
clean_rate:0.93646408839779
noisy true:494
noisy false:1556
after delete: len(clean_dataset) 1086
after delete: len(noisy_dataset) 2050
epoch [52/200] batch [5/33] time 0.562 (0.446) data 0.431 (0.314) loss_x loss_x 0.8940 (1.2173) acc_x 71.8750 (68.7500) lr 1.7071e-03 eta 0:00:12
epoch [52/200] batch [10/33] time 0.446 (0.462) data 0.315 (0.331) loss_x loss_x 0.9238 (1.1721) acc_x 78.1250 (69.3750) lr 1.7071e-03 eta 0:00:10
epoch [52/200] batch [15/33] time 0.487 (0.459) data 0.356 (0.328) loss_x loss_x 1.3164 (1.1859) acc_x 65.6250 (68.9583) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [20/33] time 0.411 (0.449) data 0.279 (0.318) loss_x loss_x 1.6270 (1.2114) acc_x 62.5000 (68.2812) lr 1.7071e-03 eta 0:00:05
epoch [52/200] batch [25/33] time 0.383 (0.450) data 0.253 (0.319) loss_x loss_x 1.5918 (1.2532) acc_x 53.1250 (67.6250) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [30/33] time 0.548 (0.455) data 0.417 (0.324) loss_x loss_x 1.2930 (1.3080) acc_x 71.8750 (66.3542) lr 1.7071e-03 eta 0:00:01
epoch [52/200] batch [5/64] time 0.513 (0.465) data 0.382 (0.334) loss_u loss_u 0.8872 (0.8482) acc_u 15.6250 (19.3750) lr 1.7071e-03 eta 0:00:27
epoch [52/200] batch [10/64] time 0.521 (0.470) data 0.390 (0.339) loss_u loss_u 0.8911 (0.8605) acc_u 9.3750 (17.5000) lr 1.7071e-03 eta 0:00:25
epoch [52/200] batch [15/64] time 0.616 (0.465) data 0.484 (0.334) loss_u loss_u 0.8657 (0.8591) acc_u 15.6250 (18.3333) lr 1.7071e-03 eta 0:00:22
epoch [52/200] batch [20/64] time 0.396 (0.463) data 0.265 (0.331) loss_u loss_u 0.8442 (0.8625) acc_u 15.6250 (17.6562) lr 1.7071e-03 eta 0:00:20
epoch [52/200] batch [25/64] time 0.500 (0.471) data 0.369 (0.340) loss_u loss_u 0.8447 (0.8621) acc_u 21.8750 (18.1250) lr 1.7071e-03 eta 0:00:18
epoch [52/200] batch [30/64] time 0.367 (0.471) data 0.235 (0.340) loss_u loss_u 0.8682 (0.8616) acc_u 21.8750 (18.3333) lr 1.7071e-03 eta 0:00:16
epoch [52/200] batch [35/64] time 0.497 (0.469) data 0.362 (0.338) loss_u loss_u 0.8130 (0.8589) acc_u 25.0000 (18.5714) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [40/64] time 0.434 (0.470) data 0.302 (0.339) loss_u loss_u 0.8242 (0.8583) acc_u 25.0000 (18.8281) lr 1.7071e-03 eta 0:00:11
epoch [52/200] batch [45/64] time 0.403 (0.467) data 0.271 (0.335) loss_u loss_u 0.8257 (0.8572) acc_u 25.0000 (19.0278) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [50/64] time 0.347 (0.464) data 0.217 (0.333) loss_u loss_u 0.8018 (0.8556) acc_u 28.1250 (19.1875) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [55/64] time 0.441 (0.462) data 0.309 (0.330) loss_u loss_u 0.8867 (0.8541) acc_u 15.6250 (19.2614) lr 1.7071e-03 eta 0:00:04
epoch [52/200] batch [60/64] time 0.467 (0.461) data 0.335 (0.329) loss_u loss_u 0.7764 (0.8525) acc_u 28.1250 (19.5312) lr 1.7071e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1566
confident_label rate tensor(0.3536, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1109
clean true:1047
clean false:62
clean_rate:0.9440937781785392
noisy true:523
noisy false:1504
after delete: len(clean_dataset) 1109
after delete: len(noisy_dataset) 2027
epoch [53/200] batch [5/34] time 0.512 (0.464) data 0.381 (0.333) loss_x loss_x 1.4541 (1.4164) acc_x 75.0000 (67.5000) lr 1.6959e-03 eta 0:00:13
epoch [53/200] batch [10/34] time 0.506 (0.468) data 0.374 (0.337) loss_x loss_x 1.4424 (1.4559) acc_x 53.1250 (64.6875) lr 1.6959e-03 eta 0:00:11
epoch [53/200] batch [15/34] time 0.485 (0.460) data 0.354 (0.329) loss_x loss_x 0.9395 (1.4111) acc_x 75.0000 (65.2083) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [20/34] time 0.525 (0.475) data 0.394 (0.344) loss_x loss_x 1.2100 (1.3406) acc_x 68.7500 (66.0938) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [25/34] time 0.499 (0.479) data 0.367 (0.348) loss_x loss_x 0.8735 (1.3324) acc_x 75.0000 (66.1250) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [30/34] time 0.388 (0.476) data 0.257 (0.345) loss_x loss_x 1.0039 (1.2976) acc_x 71.8750 (66.7708) lr 1.6959e-03 eta 0:00:01
epoch [53/200] batch [5/63] time 0.455 (0.474) data 0.323 (0.343) loss_u loss_u 0.8716 (0.8143) acc_u 18.7500 (23.7500) lr 1.6959e-03 eta 0:00:27
epoch [53/200] batch [10/63] time 0.389 (0.466) data 0.257 (0.335) loss_u loss_u 0.8721 (0.8348) acc_u 15.6250 (20.3125) lr 1.6959e-03 eta 0:00:24
epoch [53/200] batch [15/63] time 0.388 (0.468) data 0.256 (0.337) loss_u loss_u 0.8770 (0.8503) acc_u 9.3750 (17.2917) lr 1.6959e-03 eta 0:00:22
epoch [53/200] batch [20/63] time 0.399 (0.469) data 0.267 (0.338) loss_u loss_u 0.8569 (0.8563) acc_u 12.5000 (17.3438) lr 1.6959e-03 eta 0:00:20
epoch [53/200] batch [25/63] time 0.326 (0.469) data 0.194 (0.337) loss_u loss_u 0.8589 (0.8620) acc_u 18.7500 (17.0000) lr 1.6959e-03 eta 0:00:17
epoch [53/200] batch [30/63] time 0.588 (0.468) data 0.456 (0.336) loss_u loss_u 0.8623 (0.8641) acc_u 21.8750 (16.8750) lr 1.6959e-03 eta 0:00:15
epoch [53/200] batch [35/63] time 0.473 (0.465) data 0.341 (0.333) loss_u loss_u 0.8735 (0.8667) acc_u 15.6250 (16.2500) lr 1.6959e-03 eta 0:00:13
epoch [53/200] batch [40/63] time 0.398 (0.459) data 0.266 (0.328) loss_u loss_u 0.9014 (0.8672) acc_u 15.6250 (16.2500) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [45/63] time 0.495 (0.463) data 0.365 (0.331) loss_u loss_u 0.8940 (0.8670) acc_u 9.3750 (15.8333) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [50/63] time 0.456 (0.463) data 0.324 (0.332) loss_u loss_u 0.8975 (0.8661) acc_u 9.3750 (16.1250) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [55/63] time 0.478 (0.465) data 0.347 (0.333) loss_u loss_u 0.8872 (0.8697) acc_u 18.7500 (15.6818) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [60/63] time 0.464 (0.466) data 0.331 (0.335) loss_u loss_u 0.8022 (0.8667) acc_u 28.1250 (16.4062) lr 1.6959e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1604
confident_label rate tensor(0.3444, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1080
clean true:1015
clean false:65
clean_rate:0.9398148148148148
noisy true:517
noisy false:1539
after delete: len(clean_dataset) 1080
after delete: len(noisy_dataset) 2056
epoch [54/200] batch [5/33] time 0.476 (0.490) data 0.345 (0.359) loss_x loss_x 0.7920 (1.2635) acc_x 87.5000 (69.3750) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [10/33] time 0.592 (0.497) data 0.459 (0.366) loss_x loss_x 1.0586 (1.1944) acc_x 71.8750 (68.7500) lr 1.6845e-03 eta 0:00:11
epoch [54/200] batch [15/33] time 0.574 (0.493) data 0.443 (0.361) loss_x loss_x 1.8779 (1.2350) acc_x 40.6250 (66.0417) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [20/33] time 0.454 (0.501) data 0.323 (0.370) loss_x loss_x 1.6260 (1.2609) acc_x 62.5000 (67.0312) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [25/33] time 0.457 (0.503) data 0.324 (0.372) loss_x loss_x 1.2373 (1.2575) acc_x 71.8750 (67.0000) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [30/33] time 0.441 (0.500) data 0.310 (0.368) loss_x loss_x 1.2109 (1.2227) acc_x 65.6250 (67.8125) lr 1.6845e-03 eta 0:00:01
epoch [54/200] batch [5/64] time 0.297 (0.486) data 0.166 (0.355) loss_u loss_u 0.9072 (0.8688) acc_u 9.3750 (12.5000) lr 1.6845e-03 eta 0:00:28
epoch [54/200] batch [10/64] time 0.471 (0.480) data 0.340 (0.349) loss_u loss_u 0.8667 (0.8728) acc_u 12.5000 (12.8125) lr 1.6845e-03 eta 0:00:25
epoch [54/200] batch [15/64] time 0.384 (0.481) data 0.253 (0.350) loss_u loss_u 0.8398 (0.8667) acc_u 21.8750 (14.3750) lr 1.6845e-03 eta 0:00:23
epoch [54/200] batch [20/64] time 0.454 (0.484) data 0.323 (0.352) loss_u loss_u 0.8491 (0.8651) acc_u 15.6250 (15.3125) lr 1.6845e-03 eta 0:00:21
epoch [54/200] batch [25/64] time 0.476 (0.482) data 0.345 (0.351) loss_u loss_u 0.8447 (0.8607) acc_u 18.7500 (16.1250) lr 1.6845e-03 eta 0:00:18
epoch [54/200] batch [30/64] time 0.552 (0.477) data 0.421 (0.345) loss_u loss_u 0.8213 (0.8577) acc_u 21.8750 (16.5625) lr 1.6845e-03 eta 0:00:16
epoch [54/200] batch [35/64] time 0.474 (0.476) data 0.343 (0.345) loss_u loss_u 0.8647 (0.8549) acc_u 12.5000 (16.6964) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [40/64] time 0.525 (0.473) data 0.393 (0.342) loss_u loss_u 0.9185 (0.8606) acc_u 12.5000 (16.3281) lr 1.6845e-03 eta 0:00:11
epoch [54/200] batch [45/64] time 0.527 (0.474) data 0.396 (0.342) loss_u loss_u 0.8999 (0.8588) acc_u 9.3750 (16.4583) lr 1.6845e-03 eta 0:00:09
epoch [54/200] batch [50/64] time 0.430 (0.473) data 0.299 (0.342) loss_u loss_u 0.7661 (0.8577) acc_u 34.3750 (16.6250) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [55/64] time 0.334 (0.469) data 0.203 (0.338) loss_u loss_u 0.8325 (0.8566) acc_u 21.8750 (16.7614) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [60/64] time 0.393 (0.467) data 0.262 (0.336) loss_u loss_u 0.8657 (0.8585) acc_u 18.7500 (16.6146) lr 1.6845e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1591
confident_label rate tensor(0.3514, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1102
clean true:1030
clean false:72
clean_rate:0.9346642468239564
noisy true:515
noisy false:1519
after delete: len(clean_dataset) 1102
after delete: len(noisy_dataset) 2034
epoch [55/200] batch [5/34] time 0.457 (0.434) data 0.327 (0.303) loss_x loss_x 0.8345 (1.3089) acc_x 81.2500 (68.7500) lr 1.6730e-03 eta 0:00:12
epoch [55/200] batch [10/34] time 0.487 (0.448) data 0.355 (0.317) loss_x loss_x 0.6089 (1.2429) acc_x 78.1250 (68.1250) lr 1.6730e-03 eta 0:00:10
epoch [55/200] batch [15/34] time 0.435 (0.468) data 0.303 (0.337) loss_x loss_x 1.1514 (1.2999) acc_x 65.6250 (66.2500) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [20/34] time 0.404 (0.472) data 0.273 (0.341) loss_x loss_x 0.8267 (1.2898) acc_x 78.1250 (66.8750) lr 1.6730e-03 eta 0:00:06
epoch [55/200] batch [25/34] time 0.482 (0.467) data 0.351 (0.336) loss_x loss_x 1.1875 (1.2999) acc_x 71.8750 (66.8750) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [30/34] time 0.344 (0.465) data 0.214 (0.334) loss_x loss_x 1.4873 (1.2845) acc_x 62.5000 (67.8125) lr 1.6730e-03 eta 0:00:01
epoch [55/200] batch [5/63] time 0.485 (0.456) data 0.353 (0.325) loss_u loss_u 0.8457 (0.8448) acc_u 21.8750 (20.6250) lr 1.6730e-03 eta 0:00:26
epoch [55/200] batch [10/63] time 0.401 (0.456) data 0.270 (0.324) loss_u loss_u 0.9321 (0.8765) acc_u 3.1250 (15.9375) lr 1.6730e-03 eta 0:00:24
epoch [55/200] batch [15/63] time 0.502 (0.453) data 0.370 (0.322) loss_u loss_u 0.9287 (0.8716) acc_u 6.2500 (16.0417) lr 1.6730e-03 eta 0:00:21
epoch [55/200] batch [20/63] time 0.443 (0.454) data 0.312 (0.322) loss_u loss_u 0.8989 (0.8712) acc_u 15.6250 (15.9375) lr 1.6730e-03 eta 0:00:19
epoch [55/200] batch [25/63] time 0.368 (0.452) data 0.236 (0.321) loss_u loss_u 0.7783 (0.8681) acc_u 25.0000 (15.8750) lr 1.6730e-03 eta 0:00:17
epoch [55/200] batch [30/63] time 0.516 (0.457) data 0.385 (0.325) loss_u loss_u 0.8604 (0.8672) acc_u 12.5000 (15.8333) lr 1.6730e-03 eta 0:00:15
epoch [55/200] batch [35/63] time 0.431 (0.456) data 0.301 (0.324) loss_u loss_u 0.8159 (0.8623) acc_u 21.8750 (16.7857) lr 1.6730e-03 eta 0:00:12
epoch [55/200] batch [40/63] time 0.340 (0.455) data 0.209 (0.323) loss_u loss_u 0.8540 (0.8583) acc_u 18.7500 (17.2656) lr 1.6730e-03 eta 0:00:10
epoch [55/200] batch [45/63] time 0.384 (0.449) data 0.253 (0.318) loss_u loss_u 0.8735 (0.8594) acc_u 15.6250 (17.0833) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [50/63] time 0.507 (0.451) data 0.375 (0.319) loss_u loss_u 0.8911 (0.8575) acc_u 12.5000 (17.3125) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [55/63] time 0.394 (0.447) data 0.263 (0.316) loss_u loss_u 0.7979 (0.8560) acc_u 25.0000 (17.4432) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [60/63] time 0.477 (0.450) data 0.345 (0.318) loss_u loss_u 0.9219 (0.8573) acc_u 6.2500 (17.3958) lr 1.6730e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1556
confident_label rate tensor(0.3594, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1127
clean true:1067
clean false:60
clean_rate:0.9467613132209406
noisy true:513
noisy false:1496
after delete: len(clean_dataset) 1127
after delete: len(noisy_dataset) 2009
epoch [56/200] batch [5/35] time 0.590 (0.555) data 0.459 (0.425) loss_x loss_x 0.6216 (1.2830) acc_x 87.5000 (66.8750) lr 1.6613e-03 eta 0:00:16
epoch [56/200] batch [10/35] time 0.465 (0.547) data 0.334 (0.416) loss_x loss_x 1.3350 (1.1289) acc_x 68.7500 (71.5625) lr 1.6613e-03 eta 0:00:13
epoch [56/200] batch [15/35] time 0.448 (0.526) data 0.317 (0.395) loss_x loss_x 1.3857 (1.0921) acc_x 65.6250 (71.8750) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [20/35] time 0.589 (0.512) data 0.458 (0.381) loss_x loss_x 1.8887 (1.1888) acc_x 62.5000 (69.2188) lr 1.6613e-03 eta 0:00:07
epoch [56/200] batch [25/35] time 0.463 (0.499) data 0.333 (0.368) loss_x loss_x 1.2910 (1.2588) acc_x 68.7500 (67.3750) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [30/35] time 0.339 (0.496) data 0.208 (0.365) loss_x loss_x 1.1865 (1.2495) acc_x 71.8750 (68.0208) lr 1.6613e-03 eta 0:00:02
epoch [56/200] batch [35/35] time 0.409 (0.492) data 0.278 (0.361) loss_x loss_x 1.0410 (1.2474) acc_x 75.0000 (67.5893) lr 1.6613e-03 eta 0:00:00
epoch [56/200] batch [5/62] time 0.436 (0.491) data 0.306 (0.360) loss_u loss_u 0.8003 (0.8538) acc_u 31.2500 (20.0000) lr 1.6613e-03 eta 0:00:27
epoch [56/200] batch [10/62] time 0.487 (0.485) data 0.356 (0.354) loss_u loss_u 0.8242 (0.8451) acc_u 21.8750 (20.0000) lr 1.6613e-03 eta 0:00:25
epoch [56/200] batch [15/62] time 0.367 (0.480) data 0.235 (0.349) loss_u loss_u 0.9614 (0.8571) acc_u 3.1250 (18.1250) lr 1.6613e-03 eta 0:00:22
epoch [56/200] batch [20/62] time 0.418 (0.480) data 0.287 (0.349) loss_u loss_u 0.7944 (0.8547) acc_u 28.1250 (18.1250) lr 1.6613e-03 eta 0:00:20
epoch [56/200] batch [25/62] time 0.382 (0.477) data 0.251 (0.346) loss_u loss_u 0.8271 (0.8560) acc_u 21.8750 (17.8750) lr 1.6613e-03 eta 0:00:17
epoch [56/200] batch [30/62] time 0.588 (0.476) data 0.456 (0.345) loss_u loss_u 0.8145 (0.8567) acc_u 18.7500 (17.8125) lr 1.6613e-03 eta 0:00:15
epoch [56/200] batch [35/62] time 0.436 (0.472) data 0.304 (0.341) loss_u loss_u 0.8901 (0.8613) acc_u 21.8750 (17.7679) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [40/62] time 0.457 (0.470) data 0.327 (0.339) loss_u loss_u 0.8643 (0.8616) acc_u 15.6250 (17.6562) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [45/62] time 0.389 (0.465) data 0.258 (0.334) loss_u loss_u 0.7158 (0.8572) acc_u 37.5000 (18.4028) lr 1.6613e-03 eta 0:00:07
epoch [56/200] batch [50/62] time 0.507 (0.468) data 0.374 (0.336) loss_u loss_u 0.8472 (0.8570) acc_u 18.7500 (18.3125) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [55/62] time 0.423 (0.466) data 0.293 (0.335) loss_u loss_u 0.8774 (0.8579) acc_u 18.7500 (18.1818) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [60/62] time 0.423 (0.465) data 0.291 (0.334) loss_u loss_u 0.7739 (0.8570) acc_u 34.3750 (18.5417) lr 1.6613e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1561
confident_label rate tensor(0.3626, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1137
clean true:1064
clean false:73
clean_rate:0.9357959542656112
noisy true:511
noisy false:1488
after delete: len(clean_dataset) 1137
after delete: len(noisy_dataset) 1999
epoch [57/200] batch [5/35] time 0.411 (0.456) data 0.280 (0.326) loss_x loss_x 1.2422 (1.3809) acc_x 68.7500 (68.7500) lr 1.6494e-03 eta 0:00:13
epoch [57/200] batch [10/35] time 0.508 (0.523) data 0.377 (0.392) loss_x loss_x 1.1299 (1.3261) acc_x 68.7500 (67.1875) lr 1.6494e-03 eta 0:00:13
epoch [57/200] batch [15/35] time 0.442 (0.511) data 0.312 (0.380) loss_x loss_x 1.5430 (1.3285) acc_x 68.7500 (66.2500) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [20/35] time 0.542 (0.506) data 0.412 (0.375) loss_x loss_x 0.9512 (1.3244) acc_x 75.0000 (66.2500) lr 1.6494e-03 eta 0:00:07
epoch [57/200] batch [25/35] time 0.498 (0.487) data 0.367 (0.356) loss_x loss_x 1.0312 (1.2652) acc_x 81.2500 (68.1250) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [30/35] time 0.378 (0.478) data 0.248 (0.347) loss_x loss_x 1.2930 (1.2540) acc_x 68.7500 (68.1250) lr 1.6494e-03 eta 0:00:02
epoch [57/200] batch [35/35] time 0.457 (0.479) data 0.326 (0.348) loss_x loss_x 1.2051 (1.2875) acc_x 65.6250 (67.2321) lr 1.6494e-03 eta 0:00:00
epoch [57/200] batch [5/62] time 0.413 (0.474) data 0.282 (0.343) loss_u loss_u 0.8589 (0.8610) acc_u 18.7500 (18.7500) lr 1.6494e-03 eta 0:00:27
epoch [57/200] batch [10/62] time 0.467 (0.472) data 0.335 (0.342) loss_u loss_u 0.8892 (0.8659) acc_u 9.3750 (16.5625) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [15/62] time 0.446 (0.467) data 0.315 (0.336) loss_u loss_u 0.8677 (0.8650) acc_u 15.6250 (17.0833) lr 1.6494e-03 eta 0:00:21
epoch [57/200] batch [20/62] time 0.457 (0.468) data 0.326 (0.337) loss_u loss_u 0.8618 (0.8614) acc_u 15.6250 (17.6562) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [25/62] time 0.481 (0.463) data 0.350 (0.332) loss_u loss_u 0.9004 (0.8614) acc_u 12.5000 (17.5000) lr 1.6494e-03 eta 0:00:17
epoch [57/200] batch [30/62] time 0.430 (0.459) data 0.298 (0.328) loss_u loss_u 0.8848 (0.8599) acc_u 9.3750 (17.9167) lr 1.6494e-03 eta 0:00:14
epoch [57/200] batch [35/62] time 0.459 (0.459) data 0.327 (0.328) loss_u loss_u 0.8330 (0.8608) acc_u 15.6250 (17.7679) lr 1.6494e-03 eta 0:00:12
epoch [57/200] batch [40/62] time 0.538 (0.462) data 0.406 (0.331) loss_u loss_u 0.8657 (0.8616) acc_u 15.6250 (18.0469) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [45/62] time 0.371 (0.461) data 0.239 (0.330) loss_u loss_u 0.7944 (0.8604) acc_u 31.2500 (18.2639) lr 1.6494e-03 eta 0:00:07
epoch [57/200] batch [50/62] time 0.454 (0.458) data 0.323 (0.327) loss_u loss_u 0.7910 (0.8591) acc_u 34.3750 (18.3125) lr 1.6494e-03 eta 0:00:05
epoch [57/200] batch [55/62] time 0.423 (0.457) data 0.292 (0.325) loss_u loss_u 0.8921 (0.8580) acc_u 15.6250 (18.5227) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [60/62] time 0.364 (0.454) data 0.233 (0.323) loss_u loss_u 0.8760 (0.8560) acc_u 15.6250 (18.4896) lr 1.6494e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1597
confident_label rate tensor(0.3463, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1086
clean true:1026
clean false:60
clean_rate:0.9447513812154696
noisy true:513
noisy false:1537
after delete: len(clean_dataset) 1086
after delete: len(noisy_dataset) 2050
epoch [58/200] batch [5/33] time 0.623 (0.491) data 0.492 (0.359) loss_x loss_x 1.2988 (1.2512) acc_x 75.0000 (68.1250) lr 1.6374e-03 eta 0:00:13
epoch [58/200] batch [10/33] time 0.688 (0.512) data 0.553 (0.380) loss_x loss_x 1.0068 (1.3219) acc_x 78.1250 (65.9375) lr 1.6374e-03 eta 0:00:11
epoch [58/200] batch [15/33] time 0.409 (0.513) data 0.278 (0.381) loss_x loss_x 1.1328 (1.2479) acc_x 65.6250 (67.7083) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [20/33] time 0.546 (0.521) data 0.412 (0.388) loss_x loss_x 1.1562 (1.2983) acc_x 71.8750 (67.1875) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [25/33] time 0.523 (0.520) data 0.390 (0.388) loss_x loss_x 1.4541 (1.2407) acc_x 59.3750 (68.1250) lr 1.6374e-03 eta 0:00:04
epoch [58/200] batch [30/33] time 0.479 (0.512) data 0.345 (0.380) loss_x loss_x 1.5459 (1.2736) acc_x 62.5000 (67.1875) lr 1.6374e-03 eta 0:00:01
epoch [58/200] batch [5/64] time 0.553 (0.516) data 0.420 (0.383) loss_u loss_u 0.8350 (0.8248) acc_u 21.8750 (23.1250) lr 1.6374e-03 eta 0:00:30
epoch [58/200] batch [10/64] time 0.407 (0.506) data 0.275 (0.374) loss_u loss_u 0.8701 (0.8491) acc_u 18.7500 (19.6875) lr 1.6374e-03 eta 0:00:27
epoch [58/200] batch [15/64] time 0.760 (0.515) data 0.628 (0.382) loss_u loss_u 0.8774 (0.8534) acc_u 12.5000 (17.9167) lr 1.6374e-03 eta 0:00:25
epoch [58/200] batch [20/64] time 0.363 (0.507) data 0.232 (0.374) loss_u loss_u 0.8945 (0.8489) acc_u 9.3750 (18.2812) lr 1.6374e-03 eta 0:00:22
epoch [58/200] batch [25/64] time 0.562 (0.503) data 0.431 (0.371) loss_u loss_u 0.8857 (0.8497) acc_u 15.6250 (18.0000) lr 1.6374e-03 eta 0:00:19
epoch [58/200] batch [30/64] time 0.375 (0.497) data 0.243 (0.365) loss_u loss_u 0.9146 (0.8534) acc_u 12.5000 (17.8125) lr 1.6374e-03 eta 0:00:16
epoch [58/200] batch [35/64] time 0.416 (0.493) data 0.284 (0.360) loss_u loss_u 0.7925 (0.8539) acc_u 28.1250 (17.5893) lr 1.6374e-03 eta 0:00:14
epoch [58/200] batch [40/64] time 0.519 (0.494) data 0.388 (0.362) loss_u loss_u 0.8638 (0.8541) acc_u 18.7500 (17.5781) lr 1.6374e-03 eta 0:00:11
epoch [58/200] batch [45/64] time 0.433 (0.492) data 0.302 (0.360) loss_u loss_u 0.8662 (0.8531) acc_u 12.5000 (17.6389) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [50/64] time 0.456 (0.491) data 0.325 (0.359) loss_u loss_u 0.8345 (0.8533) acc_u 25.0000 (17.8750) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [55/64] time 0.393 (0.487) data 0.261 (0.355) loss_u loss_u 0.8784 (0.8548) acc_u 18.7500 (17.7841) lr 1.6374e-03 eta 0:00:04
epoch [58/200] batch [60/64] time 0.392 (0.486) data 0.261 (0.354) loss_u loss_u 0.8770 (0.8569) acc_u 18.7500 (17.6562) lr 1.6374e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1587
confident_label rate tensor(0.3571, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1120
clean true:1047
clean false:73
clean_rate:0.9348214285714286
noisy true:502
noisy false:1514
after delete: len(clean_dataset) 1120
after delete: len(noisy_dataset) 2016
epoch [59/200] batch [5/35] time 0.410 (0.481) data 0.279 (0.350) loss_x loss_x 1.0654 (1.2775) acc_x 65.6250 (65.6250) lr 1.6252e-03 eta 0:00:14
epoch [59/200] batch [10/35] time 0.386 (0.437) data 0.256 (0.306) loss_x loss_x 1.1602 (1.2944) acc_x 78.1250 (67.1875) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [15/35] time 0.429 (0.458) data 0.299 (0.327) loss_x loss_x 1.2324 (1.3590) acc_x 68.7500 (66.4583) lr 1.6252e-03 eta 0:00:09
epoch [59/200] batch [20/35] time 0.415 (0.451) data 0.285 (0.320) loss_x loss_x 1.2822 (1.3130) acc_x 62.5000 (67.6562) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [25/35] time 0.551 (0.452) data 0.421 (0.321) loss_x loss_x 1.1689 (1.3176) acc_x 78.1250 (67.6250) lr 1.6252e-03 eta 0:00:04
epoch [59/200] batch [30/35] time 0.548 (0.456) data 0.416 (0.326) loss_x loss_x 0.8584 (1.2870) acc_x 78.1250 (67.5000) lr 1.6252e-03 eta 0:00:02
epoch [59/200] batch [35/35] time 0.381 (0.453) data 0.250 (0.322) loss_x loss_x 1.0391 (1.2816) acc_x 71.8750 (67.1429) lr 1.6252e-03 eta 0:00:00
epoch [59/200] batch [5/63] time 0.436 (0.451) data 0.304 (0.320) loss_u loss_u 0.9102 (0.8636) acc_u 9.3750 (18.1250) lr 1.6252e-03 eta 0:00:26
epoch [59/200] batch [10/63] time 0.456 (0.448) data 0.323 (0.317) loss_u loss_u 0.8477 (0.8628) acc_u 25.0000 (19.0625) lr 1.6252e-03 eta 0:00:23
epoch [59/200] batch [15/63] time 0.378 (0.451) data 0.245 (0.319) loss_u loss_u 0.7925 (0.8499) acc_u 28.1250 (20.0000) lr 1.6252e-03 eta 0:00:21
epoch [59/200] batch [20/63] time 0.554 (0.459) data 0.423 (0.328) loss_u loss_u 0.8906 (0.8602) acc_u 6.2500 (17.8125) lr 1.6252e-03 eta 0:00:19
epoch [59/200] batch [25/63] time 0.407 (0.460) data 0.276 (0.328) loss_u loss_u 0.7744 (0.8541) acc_u 25.0000 (18.5000) lr 1.6252e-03 eta 0:00:17
epoch [59/200] batch [30/63] time 0.518 (0.461) data 0.387 (0.330) loss_u loss_u 0.8877 (0.8552) acc_u 15.6250 (18.4375) lr 1.6252e-03 eta 0:00:15
epoch [59/200] batch [35/63] time 0.349 (0.456) data 0.217 (0.325) loss_u loss_u 0.8613 (0.8530) acc_u 15.6250 (18.3036) lr 1.6252e-03 eta 0:00:12
epoch [59/200] batch [40/63] time 0.335 (0.462) data 0.203 (0.331) loss_u loss_u 0.9595 (0.8578) acc_u 3.1250 (17.5000) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [45/63] time 0.693 (0.464) data 0.562 (0.333) loss_u loss_u 0.8359 (0.8569) acc_u 15.6250 (17.5000) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [50/63] time 0.387 (0.462) data 0.256 (0.331) loss_u loss_u 0.8428 (0.8572) acc_u 21.8750 (17.4375) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [55/63] time 0.350 (0.458) data 0.218 (0.327) loss_u loss_u 0.9248 (0.8588) acc_u 9.3750 (17.3864) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [60/63] time 0.545 (0.458) data 0.413 (0.326) loss_u loss_u 0.8369 (0.8599) acc_u 18.7500 (17.1354) lr 1.6252e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1575
confident_label rate tensor(0.3603, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1130
clean true:1055
clean false:75
clean_rate:0.9336283185840708
noisy true:506
noisy false:1500
after delete: len(clean_dataset) 1130
after delete: len(noisy_dataset) 2006
epoch [60/200] batch [5/35] time 0.582 (0.491) data 0.449 (0.360) loss_x loss_x 1.7441 (1.5436) acc_x 56.2500 (61.2500) lr 1.6129e-03 eta 0:00:14
epoch [60/200] batch [10/35] time 0.468 (0.488) data 0.336 (0.357) loss_x loss_x 1.7188 (1.5433) acc_x 59.3750 (62.5000) lr 1.6129e-03 eta 0:00:12
epoch [60/200] batch [15/35] time 0.449 (0.475) data 0.318 (0.343) loss_x loss_x 0.8848 (1.4363) acc_x 59.3750 (62.7083) lr 1.6129e-03 eta 0:00:09
epoch [60/200] batch [20/35] time 0.449 (0.479) data 0.318 (0.347) loss_x loss_x 1.0547 (1.3664) acc_x 78.1250 (64.6875) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [25/35] time 0.496 (0.480) data 0.364 (0.348) loss_x loss_x 1.2842 (1.3220) acc_x 71.8750 (65.3750) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [30/35] time 0.432 (0.476) data 0.302 (0.345) loss_x loss_x 0.9927 (1.2911) acc_x 68.7500 (66.2500) lr 1.6129e-03 eta 0:00:02
epoch [60/200] batch [35/35] time 0.556 (0.480) data 0.424 (0.348) loss_x loss_x 0.8667 (1.2768) acc_x 84.3750 (66.9643) lr 1.6129e-03 eta 0:00:00
epoch [60/200] batch [5/62] time 0.387 (0.471) data 0.257 (0.339) loss_u loss_u 0.8853 (0.8641) acc_u 12.5000 (16.2500) lr 1.6129e-03 eta 0:00:26
epoch [60/200] batch [10/62] time 0.508 (0.469) data 0.377 (0.338) loss_u loss_u 0.8618 (0.8435) acc_u 25.0000 (20.9375) lr 1.6129e-03 eta 0:00:24
epoch [60/200] batch [15/62] time 0.469 (0.467) data 0.337 (0.336) loss_u loss_u 0.9023 (0.8472) acc_u 12.5000 (20.8333) lr 1.6129e-03 eta 0:00:21
epoch [60/200] batch [20/62] time 0.612 (0.474) data 0.480 (0.343) loss_u loss_u 0.8726 (0.8528) acc_u 15.6250 (19.5312) lr 1.6129e-03 eta 0:00:19
epoch [60/200] batch [25/62] time 0.363 (0.472) data 0.231 (0.340) loss_u loss_u 0.8813 (0.8558) acc_u 12.5000 (19.1250) lr 1.6129e-03 eta 0:00:17
epoch [60/200] batch [30/62] time 0.419 (0.468) data 0.287 (0.337) loss_u loss_u 0.9116 (0.8577) acc_u 12.5000 (18.6458) lr 1.6129e-03 eta 0:00:14
epoch [60/200] batch [35/62] time 0.460 (0.466) data 0.328 (0.334) loss_u loss_u 0.8486 (0.8587) acc_u 15.6250 (18.3929) lr 1.6129e-03 eta 0:00:12
epoch [60/200] batch [40/62] time 0.512 (0.469) data 0.381 (0.338) loss_u loss_u 0.8848 (0.8583) acc_u 12.5000 (18.5938) lr 1.6129e-03 eta 0:00:10
epoch [60/200] batch [45/62] time 0.381 (0.466) data 0.250 (0.335) loss_u loss_u 0.8271 (0.8553) acc_u 34.3750 (19.0278) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [50/62] time 0.444 (0.466) data 0.314 (0.335) loss_u loss_u 0.8384 (0.8557) acc_u 21.8750 (18.8750) lr 1.6129e-03 eta 0:00:05
epoch [60/200] batch [55/62] time 0.413 (0.465) data 0.281 (0.333) loss_u loss_u 0.9292 (0.8579) acc_u 9.3750 (18.5795) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [60/62] time 0.376 (0.461) data 0.244 (0.330) loss_u loss_u 0.8677 (0.8554) acc_u 18.7500 (18.6458) lr 1.6129e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1537
confident_label rate tensor(0.3689, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1157
clean true:1082
clean false:75
clean_rate:0.9351771823681936
noisy true:517
noisy false:1462
after delete: len(clean_dataset) 1157
after delete: len(noisy_dataset) 1979
epoch [61/200] batch [5/36] time 0.514 (0.473) data 0.384 (0.342) loss_x loss_x 1.3252 (1.3996) acc_x 71.8750 (66.8750) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [10/36] time 0.460 (0.462) data 0.330 (0.331) loss_x loss_x 1.3154 (1.3149) acc_x 71.8750 (69.3750) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [15/36] time 0.419 (0.471) data 0.288 (0.340) loss_x loss_x 1.3984 (1.3270) acc_x 65.6250 (68.3333) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [20/36] time 0.449 (0.470) data 0.319 (0.339) loss_x loss_x 1.5771 (1.3460) acc_x 62.5000 (67.8125) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [25/36] time 0.418 (0.469) data 0.287 (0.338) loss_x loss_x 2.3535 (1.3488) acc_x 53.1250 (68.5000) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [30/36] time 0.519 (0.469) data 0.387 (0.338) loss_x loss_x 1.0732 (1.3227) acc_x 68.7500 (68.6458) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [35/36] time 0.604 (0.474) data 0.473 (0.343) loss_x loss_x 1.0908 (1.3083) acc_x 68.7500 (68.8393) lr 1.6004e-03 eta 0:00:00
epoch [61/200] batch [5/61] time 0.366 (0.470) data 0.235 (0.339) loss_u loss_u 0.8608 (0.8595) acc_u 21.8750 (17.5000) lr 1.6004e-03 eta 0:00:26
epoch [61/200] batch [10/61] time 0.385 (0.467) data 0.254 (0.336) loss_u loss_u 0.9204 (0.8786) acc_u 9.3750 (15.0000) lr 1.6004e-03 eta 0:00:23
epoch [61/200] batch [15/61] time 0.438 (0.462) data 0.305 (0.331) loss_u loss_u 0.8701 (0.8704) acc_u 12.5000 (15.2083) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [20/61] time 0.486 (0.460) data 0.354 (0.329) loss_u loss_u 0.8989 (0.8626) acc_u 12.5000 (16.8750) lr 1.6004e-03 eta 0:00:18
epoch [61/200] batch [25/61] time 0.418 (0.458) data 0.287 (0.327) loss_u loss_u 0.8457 (0.8627) acc_u 18.7500 (16.1250) lr 1.6004e-03 eta 0:00:16
epoch [61/200] batch [30/61] time 0.501 (0.454) data 0.369 (0.323) loss_u loss_u 0.8384 (0.8592) acc_u 21.8750 (16.8750) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [35/61] time 0.467 (0.453) data 0.335 (0.322) loss_u loss_u 0.8667 (0.8637) acc_u 25.0000 (16.8750) lr 1.6004e-03 eta 0:00:11
epoch [61/200] batch [40/61] time 0.420 (0.453) data 0.289 (0.321) loss_u loss_u 0.8779 (0.8581) acc_u 12.5000 (17.5781) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [45/61] time 0.512 (0.454) data 0.380 (0.323) loss_u loss_u 0.8213 (0.8617) acc_u 25.0000 (17.2917) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [50/61] time 0.462 (0.452) data 0.331 (0.321) loss_u loss_u 0.7935 (0.8623) acc_u 25.0000 (17.0000) lr 1.6004e-03 eta 0:00:04
epoch [61/200] batch [55/61] time 0.410 (0.453) data 0.278 (0.322) loss_u loss_u 0.8188 (0.8608) acc_u 21.8750 (17.1023) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [60/61] time 0.383 (0.453) data 0.251 (0.322) loss_u loss_u 0.8408 (0.8609) acc_u 18.7500 (17.1354) lr 1.6004e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1584
confident_label rate tensor(0.3552, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1114
clean true:1044
clean false:70
clean_rate:0.9371633752244165
noisy true:508
noisy false:1514
after delete: len(clean_dataset) 1114
after delete: len(noisy_dataset) 2022
epoch [62/200] batch [5/34] time 0.454 (0.394) data 0.324 (0.264) loss_x loss_x 1.0518 (0.9811) acc_x 78.1250 (74.3750) lr 1.5878e-03 eta 0:00:11
epoch [62/200] batch [10/34] time 0.597 (0.447) data 0.466 (0.316) loss_x loss_x 1.6152 (1.1009) acc_x 65.6250 (70.0000) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [15/34] time 0.445 (0.443) data 0.315 (0.312) loss_x loss_x 0.9932 (1.1267) acc_x 75.0000 (70.0000) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [20/34] time 0.347 (0.440) data 0.216 (0.309) loss_x loss_x 1.3135 (1.1412) acc_x 71.8750 (69.8438) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [25/34] time 0.401 (0.434) data 0.271 (0.304) loss_x loss_x 1.1162 (1.1204) acc_x 65.6250 (70.0000) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [30/34] time 0.526 (0.437) data 0.396 (0.307) loss_x loss_x 1.2041 (1.1786) acc_x 62.5000 (68.5417) lr 1.5878e-03 eta 0:00:01
epoch [62/200] batch [5/63] time 0.510 (0.447) data 0.378 (0.316) loss_u loss_u 0.7930 (0.8169) acc_u 25.0000 (24.3750) lr 1.5878e-03 eta 0:00:25
epoch [62/200] batch [10/63] time 0.633 (0.455) data 0.501 (0.324) loss_u loss_u 0.8643 (0.8294) acc_u 18.7500 (22.1875) lr 1.5878e-03 eta 0:00:24
epoch [62/200] batch [15/63] time 0.402 (0.449) data 0.270 (0.318) loss_u loss_u 0.8130 (0.8320) acc_u 21.8750 (21.2500) lr 1.5878e-03 eta 0:00:21
epoch [62/200] batch [20/63] time 0.430 (0.453) data 0.300 (0.322) loss_u loss_u 0.8516 (0.8375) acc_u 15.6250 (19.8438) lr 1.5878e-03 eta 0:00:19
epoch [62/200] batch [25/63] time 0.390 (0.450) data 0.259 (0.319) loss_u loss_u 0.8530 (0.8382) acc_u 21.8750 (20.5000) lr 1.5878e-03 eta 0:00:17
epoch [62/200] batch [30/63] time 0.367 (0.450) data 0.237 (0.318) loss_u loss_u 0.8379 (0.8327) acc_u 18.7500 (21.7708) lr 1.5878e-03 eta 0:00:14
epoch [62/200] batch [35/63] time 0.461 (0.447) data 0.329 (0.316) loss_u loss_u 0.8447 (0.8332) acc_u 31.2500 (21.8750) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [40/63] time 0.425 (0.446) data 0.293 (0.315) loss_u loss_u 0.8418 (0.8356) acc_u 21.8750 (21.5625) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [45/63] time 0.498 (0.449) data 0.367 (0.318) loss_u loss_u 0.8643 (0.8364) acc_u 21.8750 (21.3194) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [50/63] time 0.542 (0.449) data 0.411 (0.317) loss_u loss_u 0.8931 (0.8379) acc_u 15.6250 (21.3750) lr 1.5878e-03 eta 0:00:05
epoch [62/200] batch [55/63] time 0.409 (0.449) data 0.278 (0.318) loss_u loss_u 0.9229 (0.8429) acc_u 6.2500 (20.5114) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [60/63] time 0.435 (0.449) data 0.304 (0.318) loss_u loss_u 0.9072 (0.8457) acc_u 9.3750 (20.2083) lr 1.5878e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1559
confident_label rate tensor(0.3613, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1133
clean true:1067
clean false:66
clean_rate:0.941747572815534
noisy true:510
noisy false:1493
after delete: len(clean_dataset) 1133
after delete: len(noisy_dataset) 2003
epoch [63/200] batch [5/35] time 0.401 (0.406) data 0.270 (0.275) loss_x loss_x 1.4189 (1.3121) acc_x 53.1250 (64.3750) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [10/35] time 0.437 (0.437) data 0.307 (0.306) loss_x loss_x 0.9932 (1.1833) acc_x 78.1250 (68.7500) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [15/35] time 0.392 (0.465) data 0.262 (0.334) loss_x loss_x 1.7256 (1.2464) acc_x 62.5000 (68.9583) lr 1.5750e-03 eta 0:00:09
epoch [63/200] batch [20/35] time 0.372 (0.463) data 0.241 (0.333) loss_x loss_x 0.8647 (1.2900) acc_x 78.1250 (68.1250) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [25/35] time 0.556 (0.466) data 0.426 (0.336) loss_x loss_x 1.3340 (1.2921) acc_x 65.6250 (68.7500) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [30/35] time 0.402 (0.464) data 0.272 (0.333) loss_x loss_x 2.3594 (1.3727) acc_x 53.1250 (67.0833) lr 1.5750e-03 eta 0:00:02
epoch [63/200] batch [35/35] time 0.553 (0.470) data 0.421 (0.339) loss_x loss_x 1.9199 (1.3653) acc_x 46.8750 (66.6071) lr 1.5750e-03 eta 0:00:00
epoch [63/200] batch [5/62] time 0.406 (0.465) data 0.275 (0.335) loss_u loss_u 0.8330 (0.8574) acc_u 25.0000 (21.2500) lr 1.5750e-03 eta 0:00:26
epoch [63/200] batch [10/62] time 0.498 (0.463) data 0.367 (0.332) loss_u loss_u 0.8301 (0.8599) acc_u 21.8750 (19.6875) lr 1.5750e-03 eta 0:00:24
epoch [63/200] batch [15/62] time 0.475 (0.466) data 0.344 (0.335) loss_u loss_u 0.9219 (0.8596) acc_u 15.6250 (18.9583) lr 1.5750e-03 eta 0:00:21
epoch [63/200] batch [20/62] time 0.562 (0.471) data 0.429 (0.340) loss_u loss_u 0.9165 (0.8534) acc_u 12.5000 (19.6875) lr 1.5750e-03 eta 0:00:19
epoch [63/200] batch [25/62] time 0.415 (0.467) data 0.283 (0.336) loss_u loss_u 0.8804 (0.8546) acc_u 9.3750 (19.2500) lr 1.5750e-03 eta 0:00:17
epoch [63/200] batch [30/62] time 0.415 (0.464) data 0.283 (0.333) loss_u loss_u 0.9126 (0.8600) acc_u 9.3750 (18.2292) lr 1.5750e-03 eta 0:00:14
epoch [63/200] batch [35/62] time 0.388 (0.463) data 0.257 (0.332) loss_u loss_u 0.9150 (0.8613) acc_u 9.3750 (18.0357) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [40/62] time 0.404 (0.466) data 0.274 (0.335) loss_u loss_u 0.9019 (0.8566) acc_u 9.3750 (18.5156) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [45/62] time 0.355 (0.462) data 0.223 (0.331) loss_u loss_u 0.8867 (0.8553) acc_u 15.6250 (18.7500) lr 1.5750e-03 eta 0:00:07
epoch [63/200] batch [50/62] time 0.476 (0.459) data 0.345 (0.328) loss_u loss_u 0.8735 (0.8563) acc_u 18.7500 (18.8125) lr 1.5750e-03 eta 0:00:05
epoch [63/200] batch [55/62] time 0.498 (0.457) data 0.366 (0.326) loss_u loss_u 0.8101 (0.8550) acc_u 28.1250 (18.9773) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [60/62] time 0.471 (0.454) data 0.337 (0.323) loss_u loss_u 0.8271 (0.8563) acc_u 21.8750 (18.7500) lr 1.5750e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1540
confident_label rate tensor(0.3587, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1125
clean true:1056
clean false:69
clean_rate:0.9386666666666666
noisy true:540
noisy false:1471
after delete: len(clean_dataset) 1125
after delete: len(noisy_dataset) 2011
epoch [64/200] batch [5/35] time 0.463 (0.479) data 0.333 (0.348) loss_x loss_x 1.3506 (1.4412) acc_x 68.7500 (67.5000) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [10/35] time 0.483 (0.472) data 0.350 (0.341) loss_x loss_x 1.4854 (1.3796) acc_x 68.7500 (65.9375) lr 1.5621e-03 eta 0:00:11
epoch [64/200] batch [15/35] time 0.325 (0.455) data 0.194 (0.323) loss_x loss_x 1.5479 (1.3266) acc_x 68.7500 (67.2917) lr 1.5621e-03 eta 0:00:09
epoch [64/200] batch [20/35] time 0.411 (0.446) data 0.280 (0.314) loss_x loss_x 1.3652 (1.2293) acc_x 75.0000 (70.0000) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [25/35] time 0.410 (0.443) data 0.279 (0.312) loss_x loss_x 1.0156 (1.1934) acc_x 68.7500 (69.7500) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [30/35] time 0.409 (0.439) data 0.279 (0.308) loss_x loss_x 1.0967 (1.2212) acc_x 75.0000 (69.4792) lr 1.5621e-03 eta 0:00:02
epoch [64/200] batch [35/35] time 0.485 (0.455) data 0.354 (0.324) loss_x loss_x 1.5156 (1.2361) acc_x 53.1250 (68.5714) lr 1.5621e-03 eta 0:00:00
epoch [64/200] batch [5/62] time 0.453 (0.448) data 0.321 (0.317) loss_u loss_u 0.8032 (0.8187) acc_u 21.8750 (23.7500) lr 1.5621e-03 eta 0:00:25
epoch [64/200] batch [10/62] time 0.499 (0.447) data 0.367 (0.316) loss_u loss_u 0.8594 (0.8450) acc_u 18.7500 (19.6875) lr 1.5621e-03 eta 0:00:23
epoch [64/200] batch [15/62] time 0.539 (0.448) data 0.408 (0.317) loss_u loss_u 0.8779 (0.8512) acc_u 21.8750 (19.5833) lr 1.5621e-03 eta 0:00:21
epoch [64/200] batch [20/62] time 0.380 (0.443) data 0.248 (0.312) loss_u loss_u 0.8218 (0.8561) acc_u 21.8750 (18.4375) lr 1.5621e-03 eta 0:00:18
epoch [64/200] batch [25/62] time 0.465 (0.443) data 0.335 (0.311) loss_u loss_u 0.8213 (0.8535) acc_u 25.0000 (18.6250) lr 1.5621e-03 eta 0:00:16
epoch [64/200] batch [30/62] time 0.461 (0.444) data 0.329 (0.313) loss_u loss_u 0.8955 (0.8582) acc_u 12.5000 (18.0208) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [35/62] time 0.531 (0.445) data 0.400 (0.314) loss_u loss_u 0.8530 (0.8570) acc_u 15.6250 (17.9464) lr 1.5621e-03 eta 0:00:12
epoch [64/200] batch [40/62] time 0.553 (0.448) data 0.422 (0.316) loss_u loss_u 0.8560 (0.8589) acc_u 15.6250 (17.6562) lr 1.5621e-03 eta 0:00:09
epoch [64/200] batch [45/62] time 0.376 (0.445) data 0.245 (0.314) loss_u loss_u 0.7778 (0.8559) acc_u 37.5000 (18.5417) lr 1.5621e-03 eta 0:00:07
epoch [64/200] batch [50/62] time 0.496 (0.446) data 0.364 (0.314) loss_u loss_u 0.9141 (0.8564) acc_u 12.5000 (18.5000) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [55/62] time 0.356 (0.446) data 0.224 (0.315) loss_u loss_u 0.8491 (0.8568) acc_u 21.8750 (18.5227) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [60/62] time 0.829 (0.451) data 0.698 (0.319) loss_u loss_u 0.8447 (0.8538) acc_u 25.0000 (18.9062) lr 1.5621e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1592
confident_label rate tensor(0.3485, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1093
clean true:1028
clean false:65
clean_rate:0.9405306495882891
noisy true:516
noisy false:1527
after delete: len(clean_dataset) 1093
after delete: len(noisy_dataset) 2043
epoch [65/200] batch [5/34] time 0.440 (0.426) data 0.309 (0.295) loss_x loss_x 1.3057 (1.2565) acc_x 71.8750 (70.0000) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [10/34] time 0.489 (0.481) data 0.359 (0.350) loss_x loss_x 1.3486 (1.2687) acc_x 65.6250 (68.7500) lr 1.5490e-03 eta 0:00:11
epoch [65/200] batch [15/34] time 0.486 (0.480) data 0.355 (0.349) loss_x loss_x 2.0547 (1.2864) acc_x 50.0000 (68.7500) lr 1.5490e-03 eta 0:00:09
epoch [65/200] batch [20/34] time 0.446 (0.472) data 0.316 (0.341) loss_x loss_x 1.2812 (1.2788) acc_x 65.6250 (67.9688) lr 1.5490e-03 eta 0:00:06
epoch [65/200] batch [25/34] time 0.405 (0.469) data 0.273 (0.339) loss_x loss_x 1.4199 (1.2875) acc_x 71.8750 (67.6250) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [30/34] time 0.462 (0.467) data 0.331 (0.336) loss_x loss_x 0.8511 (1.2498) acc_x 78.1250 (68.5417) lr 1.5490e-03 eta 0:00:01
epoch [65/200] batch [5/63] time 0.432 (0.488) data 0.302 (0.358) loss_u loss_u 0.8481 (0.8465) acc_u 18.7500 (18.1250) lr 1.5490e-03 eta 0:00:28
epoch [65/200] batch [10/63] time 0.424 (0.481) data 0.293 (0.351) loss_u loss_u 0.8916 (0.8567) acc_u 15.6250 (19.0625) lr 1.5490e-03 eta 0:00:25
epoch [65/200] batch [15/63] time 0.576 (0.481) data 0.445 (0.350) loss_u loss_u 0.9092 (0.8549) acc_u 6.2500 (18.5417) lr 1.5490e-03 eta 0:00:23
epoch [65/200] batch [20/63] time 0.457 (0.476) data 0.326 (0.346) loss_u loss_u 0.8408 (0.8511) acc_u 18.7500 (18.5938) lr 1.5490e-03 eta 0:00:20
epoch [65/200] batch [25/63] time 0.411 (0.474) data 0.280 (0.343) loss_u loss_u 0.8281 (0.8556) acc_u 25.0000 (18.6250) lr 1.5490e-03 eta 0:00:18
epoch [65/200] batch [30/63] time 0.381 (0.471) data 0.249 (0.340) loss_u loss_u 0.8359 (0.8525) acc_u 15.6250 (18.7500) lr 1.5490e-03 eta 0:00:15
epoch [65/200] batch [35/63] time 0.385 (0.467) data 0.253 (0.336) loss_u loss_u 0.7969 (0.8512) acc_u 21.8750 (18.7500) lr 1.5490e-03 eta 0:00:13
epoch [65/200] batch [40/63] time 0.582 (0.465) data 0.451 (0.334) loss_u loss_u 0.9292 (0.8530) acc_u 12.5000 (18.6719) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [45/63] time 0.420 (0.463) data 0.288 (0.332) loss_u loss_u 0.9028 (0.8551) acc_u 15.6250 (18.7500) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [50/63] time 0.756 (0.463) data 0.620 (0.331) loss_u loss_u 0.8760 (0.8549) acc_u 9.3750 (18.5000) lr 1.5490e-03 eta 0:00:06
epoch [65/200] batch [55/63] time 0.503 (0.462) data 0.371 (0.331) loss_u loss_u 0.7646 (0.8517) acc_u 34.3750 (19.0909) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [60/63] time 0.431 (0.463) data 0.299 (0.332) loss_u loss_u 0.8394 (0.8518) acc_u 25.0000 (19.0625) lr 1.5490e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1560
confident_label rate tensor(0.3689, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1157
clean true:1082
clean false:75
clean_rate:0.9351771823681936
noisy true:494
noisy false:1485
after delete: len(clean_dataset) 1157
after delete: len(noisy_dataset) 1979
epoch [66/200] batch [5/36] time 0.457 (0.414) data 0.327 (0.283) loss_x loss_x 1.2500 (1.0153) acc_x 68.7500 (73.7500) lr 1.5358e-03 eta 0:00:12
epoch [66/200] batch [10/36] time 0.571 (0.475) data 0.440 (0.345) loss_x loss_x 0.9893 (1.1098) acc_x 78.1250 (72.8125) lr 1.5358e-03 eta 0:00:12
epoch [66/200] batch [15/36] time 0.364 (0.467) data 0.234 (0.336) loss_x loss_x 1.5371 (1.1838) acc_x 62.5000 (70.8333) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [20/36] time 0.513 (0.455) data 0.382 (0.324) loss_x loss_x 0.9839 (1.2313) acc_x 75.0000 (69.0625) lr 1.5358e-03 eta 0:00:07
epoch [66/200] batch [25/36] time 0.391 (0.455) data 0.260 (0.324) loss_x loss_x 0.9219 (1.2385) acc_x 78.1250 (68.3750) lr 1.5358e-03 eta 0:00:05
epoch [66/200] batch [30/36] time 0.444 (0.451) data 0.312 (0.320) loss_x loss_x 0.9639 (1.2461) acc_x 71.8750 (68.1250) lr 1.5358e-03 eta 0:00:02
epoch [66/200] batch [35/36] time 0.397 (0.459) data 0.267 (0.328) loss_x loss_x 1.7285 (1.2486) acc_x 56.2500 (68.2143) lr 1.5358e-03 eta 0:00:00
epoch [66/200] batch [5/61] time 0.393 (0.460) data 0.262 (0.329) loss_u loss_u 0.8989 (0.8852) acc_u 9.3750 (10.6250) lr 1.5358e-03 eta 0:00:25
epoch [66/200] batch [10/61] time 0.473 (0.459) data 0.341 (0.327) loss_u loss_u 0.7915 (0.8574) acc_u 25.0000 (16.2500) lr 1.5358e-03 eta 0:00:23
epoch [66/200] batch [15/61] time 0.414 (0.457) data 0.284 (0.325) loss_u loss_u 0.8965 (0.8589) acc_u 15.6250 (17.0833) lr 1.5358e-03 eta 0:00:21
epoch [66/200] batch [20/61] time 0.391 (0.455) data 0.261 (0.323) loss_u loss_u 0.8242 (0.8592) acc_u 21.8750 (16.8750) lr 1.5358e-03 eta 0:00:18
epoch [66/200] batch [25/61] time 0.501 (0.457) data 0.369 (0.326) loss_u loss_u 0.8999 (0.8573) acc_u 9.3750 (17.0000) lr 1.5358e-03 eta 0:00:16
epoch [66/200] batch [30/61] time 0.654 (0.457) data 0.523 (0.325) loss_u loss_u 0.8350 (0.8566) acc_u 12.5000 (16.9792) lr 1.5358e-03 eta 0:00:14
epoch [66/200] batch [35/61] time 0.424 (0.458) data 0.293 (0.327) loss_u loss_u 0.8633 (0.8564) acc_u 15.6250 (17.4107) lr 1.5358e-03 eta 0:00:11
epoch [66/200] batch [40/61] time 0.409 (0.454) data 0.278 (0.322) loss_u loss_u 0.8184 (0.8530) acc_u 21.8750 (17.9688) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [45/61] time 0.566 (0.454) data 0.436 (0.322) loss_u loss_u 0.8828 (0.8521) acc_u 12.5000 (17.9861) lr 1.5358e-03 eta 0:00:07
epoch [66/200] batch [50/61] time 0.513 (0.457) data 0.381 (0.325) loss_u loss_u 0.8564 (0.8498) acc_u 18.7500 (18.2500) lr 1.5358e-03 eta 0:00:05
epoch [66/200] batch [55/61] time 0.439 (0.456) data 0.305 (0.325) loss_u loss_u 0.8203 (0.8507) acc_u 25.0000 (18.1250) lr 1.5358e-03 eta 0:00:02
epoch [66/200] batch [60/61] time 0.356 (0.454) data 0.223 (0.323) loss_u loss_u 0.8608 (0.8519) acc_u 12.5000 (18.0729) lr 1.5358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1542
confident_label rate tensor(0.3673, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1152
clean true:1070
clean false:82
clean_rate:0.9288194444444444
noisy true:524
noisy false:1460
after delete: len(clean_dataset) 1152
after delete: len(noisy_dataset) 1984
epoch [67/200] batch [5/36] time 0.368 (0.417) data 0.238 (0.287) loss_x loss_x 1.6279 (1.3245) acc_x 65.6250 (66.2500) lr 1.5225e-03 eta 0:00:12
epoch [67/200] batch [10/36] time 0.490 (0.420) data 0.359 (0.290) loss_x loss_x 1.4766 (1.3978) acc_x 65.6250 (65.9375) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [15/36] time 0.359 (0.428) data 0.229 (0.297) loss_x loss_x 1.8877 (1.3884) acc_x 59.3750 (66.4583) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [20/36] time 0.549 (0.443) data 0.418 (0.312) loss_x loss_x 0.9570 (1.3160) acc_x 71.8750 (67.6562) lr 1.5225e-03 eta 0:00:07
epoch [67/200] batch [25/36] time 0.449 (0.460) data 0.318 (0.329) loss_x loss_x 1.3838 (1.3554) acc_x 68.7500 (67.0000) lr 1.5225e-03 eta 0:00:05
epoch [67/200] batch [30/36] time 0.545 (0.464) data 0.414 (0.333) loss_x loss_x 0.8154 (1.3022) acc_x 90.6250 (67.7083) lr 1.5225e-03 eta 0:00:02
epoch [67/200] batch [35/36] time 0.423 (0.461) data 0.292 (0.330) loss_x loss_x 1.6738 (1.2971) acc_x 65.6250 (67.9464) lr 1.5225e-03 eta 0:00:00
epoch [67/200] batch [5/62] time 0.358 (0.451) data 0.227 (0.320) loss_u loss_u 0.8574 (0.8653) acc_u 18.7500 (16.8750) lr 1.5225e-03 eta 0:00:25
epoch [67/200] batch [10/62] time 0.445 (0.446) data 0.314 (0.315) loss_u loss_u 0.8076 (0.8521) acc_u 34.3750 (19.6875) lr 1.5225e-03 eta 0:00:23
epoch [67/200] batch [15/62] time 0.494 (0.453) data 0.362 (0.322) loss_u loss_u 0.9062 (0.8615) acc_u 9.3750 (18.3333) lr 1.5225e-03 eta 0:00:21
epoch [67/200] batch [20/62] time 0.396 (0.452) data 0.264 (0.321) loss_u loss_u 0.8252 (0.8582) acc_u 18.7500 (18.2812) lr 1.5225e-03 eta 0:00:19
epoch [67/200] batch [25/62] time 0.551 (0.458) data 0.419 (0.327) loss_u loss_u 0.8789 (0.8579) acc_u 25.0000 (18.6250) lr 1.5225e-03 eta 0:00:16
epoch [67/200] batch [30/62] time 0.475 (0.457) data 0.343 (0.325) loss_u loss_u 0.8682 (0.8612) acc_u 15.6250 (18.1250) lr 1.5225e-03 eta 0:00:14
epoch [67/200] batch [35/62] time 0.484 (0.455) data 0.353 (0.323) loss_u loss_u 0.8975 (0.8654) acc_u 15.6250 (17.5000) lr 1.5225e-03 eta 0:00:12
epoch [67/200] batch [40/62] time 0.647 (0.455) data 0.515 (0.324) loss_u loss_u 0.8486 (0.8651) acc_u 18.7500 (17.5000) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [45/62] time 0.331 (0.454) data 0.199 (0.322) loss_u loss_u 0.8447 (0.8604) acc_u 15.6250 (17.9861) lr 1.5225e-03 eta 0:00:07
epoch [67/200] batch [50/62] time 0.434 (0.451) data 0.303 (0.320) loss_u loss_u 0.7812 (0.8594) acc_u 25.0000 (18.2500) lr 1.5225e-03 eta 0:00:05
epoch [67/200] batch [55/62] time 0.366 (0.449) data 0.233 (0.317) loss_u loss_u 0.8320 (0.8584) acc_u 15.6250 (18.2955) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [60/62] time 0.630 (0.450) data 0.497 (0.319) loss_u loss_u 0.8320 (0.8583) acc_u 15.6250 (18.0729) lr 1.5225e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1558
confident_label rate tensor(0.3610, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1132
clean true:1060
clean false:72
clean_rate:0.9363957597173145
noisy true:518
noisy false:1486
after delete: len(clean_dataset) 1132
after delete: len(noisy_dataset) 2004
epoch [68/200] batch [5/35] time 0.407 (0.432) data 0.277 (0.301) loss_x loss_x 1.6387 (1.4180) acc_x 65.6250 (63.1250) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [10/35] time 0.357 (0.451) data 0.227 (0.321) loss_x loss_x 1.4248 (1.3153) acc_x 62.5000 (64.6875) lr 1.5090e-03 eta 0:00:11
epoch [68/200] batch [15/35] time 0.429 (0.457) data 0.299 (0.327) loss_x loss_x 1.4346 (1.2961) acc_x 65.6250 (65.8333) lr 1.5090e-03 eta 0:00:09
epoch [68/200] batch [20/35] time 0.491 (0.454) data 0.361 (0.324) loss_x loss_x 1.5361 (1.3009) acc_x 65.6250 (67.1875) lr 1.5090e-03 eta 0:00:06
epoch [68/200] batch [25/35] time 0.491 (0.459) data 0.361 (0.329) loss_x loss_x 1.4258 (1.3195) acc_x 65.6250 (66.5000) lr 1.5090e-03 eta 0:00:04
epoch [68/200] batch [30/35] time 0.398 (0.462) data 0.267 (0.332) loss_x loss_x 1.1025 (1.3260) acc_x 71.8750 (66.1458) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [35/35] time 0.453 (0.463) data 0.322 (0.332) loss_x loss_x 1.3779 (1.2915) acc_x 62.5000 (66.4286) lr 1.5090e-03 eta 0:00:00
epoch [68/200] batch [5/62] time 0.545 (0.466) data 0.414 (0.335) loss_u loss_u 0.8281 (0.8845) acc_u 21.8750 (11.8750) lr 1.5090e-03 eta 0:00:26
epoch [68/200] batch [10/62] time 0.404 (0.469) data 0.273 (0.338) loss_u loss_u 0.8516 (0.8770) acc_u 15.6250 (14.6875) lr 1.5090e-03 eta 0:00:24
epoch [68/200] batch [15/62] time 0.447 (0.471) data 0.316 (0.340) loss_u loss_u 0.8926 (0.8641) acc_u 6.2500 (15.4167) lr 1.5090e-03 eta 0:00:22
epoch [68/200] batch [20/62] time 0.433 (0.468) data 0.302 (0.337) loss_u loss_u 0.8550 (0.8565) acc_u 18.7500 (17.1875) lr 1.5090e-03 eta 0:00:19
epoch [68/200] batch [25/62] time 0.372 (0.464) data 0.240 (0.333) loss_u loss_u 0.8682 (0.8582) acc_u 15.6250 (16.8750) lr 1.5090e-03 eta 0:00:17
epoch [68/200] batch [30/62] time 0.410 (0.459) data 0.275 (0.328) loss_u loss_u 0.8916 (0.8565) acc_u 15.6250 (17.0833) lr 1.5090e-03 eta 0:00:14
epoch [68/200] batch [35/62] time 0.688 (0.461) data 0.555 (0.330) loss_u loss_u 0.8179 (0.8548) acc_u 25.0000 (17.3214) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [40/62] time 0.624 (0.465) data 0.493 (0.333) loss_u loss_u 0.8809 (0.8538) acc_u 21.8750 (18.1250) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [45/62] time 0.350 (0.459) data 0.218 (0.327) loss_u loss_u 0.9224 (0.8554) acc_u 12.5000 (18.2639) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [50/62] time 0.449 (0.458) data 0.319 (0.326) loss_u loss_u 0.7930 (0.8561) acc_u 28.1250 (18.0000) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [55/62] time 0.392 (0.455) data 0.261 (0.324) loss_u loss_u 0.8247 (0.8555) acc_u 15.6250 (17.9545) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [60/62] time 0.573 (0.455) data 0.441 (0.324) loss_u loss_u 0.8613 (0.8542) acc_u 18.7500 (18.1771) lr 1.5090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1503
confident_label rate tensor(0.3705, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1162
clean true:1099
clean false:63
clean_rate:0.9457831325301205
noisy true:534
noisy false:1440
after delete: len(clean_dataset) 1162
after delete: len(noisy_dataset) 1974
epoch [69/200] batch [5/36] time 0.518 (0.503) data 0.386 (0.371) loss_x loss_x 0.9087 (1.2526) acc_x 71.8750 (69.3750) lr 1.4955e-03 eta 0:00:15
epoch [69/200] batch [10/36] time 0.537 (0.496) data 0.406 (0.364) loss_x loss_x 1.3281 (1.3152) acc_x 65.6250 (67.5000) lr 1.4955e-03 eta 0:00:12
epoch [69/200] batch [15/36] time 0.477 (0.480) data 0.347 (0.349) loss_x loss_x 1.7256 (1.3870) acc_x 59.3750 (66.6667) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [20/36] time 0.540 (0.487) data 0.410 (0.356) loss_x loss_x 1.0293 (1.3246) acc_x 71.8750 (67.5000) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [25/36] time 0.423 (0.487) data 0.293 (0.356) loss_x loss_x 1.2129 (1.3063) acc_x 62.5000 (67.3750) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [30/36] time 0.543 (0.488) data 0.412 (0.356) loss_x loss_x 1.1660 (1.2906) acc_x 68.7500 (67.1875) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [35/36] time 0.519 (0.483) data 0.388 (0.352) loss_x loss_x 1.1475 (1.2710) acc_x 71.8750 (67.3214) lr 1.4955e-03 eta 0:00:00
epoch [69/200] batch [5/61] time 0.354 (0.472) data 0.222 (0.341) loss_u loss_u 0.8154 (0.8425) acc_u 25.0000 (20.0000) lr 1.4955e-03 eta 0:00:26
epoch [69/200] batch [10/61] time 0.417 (0.470) data 0.286 (0.339) loss_u loss_u 0.8428 (0.8446) acc_u 15.6250 (19.0625) lr 1.4955e-03 eta 0:00:23
epoch [69/200] batch [15/61] time 0.415 (0.465) data 0.284 (0.334) loss_u loss_u 0.8115 (0.8514) acc_u 21.8750 (18.1250) lr 1.4955e-03 eta 0:00:21
epoch [69/200] batch [20/61] time 0.348 (0.463) data 0.217 (0.332) loss_u loss_u 0.8408 (0.8520) acc_u 15.6250 (17.8125) lr 1.4955e-03 eta 0:00:19
epoch [69/200] batch [25/61] time 0.498 (0.463) data 0.367 (0.332) loss_u loss_u 0.8198 (0.8460) acc_u 21.8750 (18.3750) lr 1.4955e-03 eta 0:00:16
epoch [69/200] batch [30/61] time 0.500 (0.460) data 0.369 (0.329) loss_u loss_u 0.7632 (0.8461) acc_u 28.1250 (18.4375) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [35/61] time 0.526 (0.460) data 0.395 (0.329) loss_u loss_u 0.8081 (0.8428) acc_u 25.0000 (19.0179) lr 1.4955e-03 eta 0:00:11
epoch [69/200] batch [40/61] time 0.595 (0.463) data 0.464 (0.332) loss_u loss_u 0.7886 (0.8442) acc_u 25.0000 (18.6719) lr 1.4955e-03 eta 0:00:09
epoch [69/200] batch [45/61] time 0.441 (0.462) data 0.310 (0.330) loss_u loss_u 0.8457 (0.8451) acc_u 15.6250 (18.3333) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [50/61] time 0.416 (0.461) data 0.284 (0.330) loss_u loss_u 0.8481 (0.8446) acc_u 25.0000 (18.4375) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [55/61] time 0.570 (0.463) data 0.436 (0.332) loss_u loss_u 0.9058 (0.8498) acc_u 6.2500 (17.6136) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [60/61] time 0.591 (0.462) data 0.459 (0.331) loss_u loss_u 0.8809 (0.8510) acc_u 15.6250 (17.5521) lr 1.4955e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1566
confident_label rate tensor(0.3648, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1144
clean true:1069
clean false:75
clean_rate:0.9344405594405595
noisy true:501
noisy false:1491
after delete: len(clean_dataset) 1144
after delete: len(noisy_dataset) 1992
epoch [70/200] batch [5/35] time 0.389 (0.415) data 0.258 (0.284) loss_x loss_x 1.1299 (1.1399) acc_x 81.2500 (76.2500) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [10/35] time 0.564 (0.422) data 0.433 (0.291) loss_x loss_x 2.1328 (1.3526) acc_x 53.1250 (70.9375) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [15/35] time 0.428 (0.420) data 0.297 (0.289) loss_x loss_x 0.9907 (1.3173) acc_x 71.8750 (69.1667) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [20/35] time 0.444 (0.449) data 0.313 (0.318) loss_x loss_x 0.9116 (1.2776) acc_x 71.8750 (69.0625) lr 1.4818e-03 eta 0:00:06
epoch [70/200] batch [25/35] time 0.432 (0.449) data 0.301 (0.318) loss_x loss_x 1.4814 (1.2659) acc_x 65.6250 (69.1250) lr 1.4818e-03 eta 0:00:04
epoch [70/200] batch [30/35] time 0.589 (0.460) data 0.457 (0.329) loss_x loss_x 1.1299 (1.2607) acc_x 68.7500 (69.0625) lr 1.4818e-03 eta 0:00:02
epoch [70/200] batch [35/35] time 0.465 (0.465) data 0.334 (0.334) loss_x loss_x 0.8735 (1.2550) acc_x 71.8750 (69.2857) lr 1.4818e-03 eta 0:00:00
epoch [70/200] batch [5/62] time 0.516 (0.464) data 0.384 (0.333) loss_u loss_u 0.8306 (0.8714) acc_u 18.7500 (15.0000) lr 1.4818e-03 eta 0:00:26
epoch [70/200] batch [10/62] time 0.528 (0.460) data 0.398 (0.329) loss_u loss_u 0.8145 (0.8627) acc_u 15.6250 (16.2500) lr 1.4818e-03 eta 0:00:23
epoch [70/200] batch [15/62] time 0.448 (0.463) data 0.318 (0.332) loss_u loss_u 0.8560 (0.8547) acc_u 18.7500 (17.2917) lr 1.4818e-03 eta 0:00:21
epoch [70/200] batch [20/62] time 0.424 (0.462) data 0.292 (0.330) loss_u loss_u 0.8511 (0.8538) acc_u 15.6250 (16.8750) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [25/62] time 0.386 (0.456) data 0.255 (0.325) loss_u loss_u 0.8223 (0.8533) acc_u 15.6250 (17.0000) lr 1.4818e-03 eta 0:00:16
epoch [70/200] batch [30/62] time 0.440 (0.455) data 0.310 (0.324) loss_u loss_u 0.8379 (0.8570) acc_u 18.7500 (16.8750) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [35/62] time 0.395 (0.450) data 0.264 (0.319) loss_u loss_u 0.8853 (0.8534) acc_u 9.3750 (17.0536) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [40/62] time 0.440 (0.448) data 0.308 (0.317) loss_u loss_u 0.8877 (0.8530) acc_u 15.6250 (17.1875) lr 1.4818e-03 eta 0:00:09
epoch [70/200] batch [45/62] time 0.677 (0.449) data 0.545 (0.318) loss_u loss_u 0.9082 (0.8556) acc_u 12.5000 (16.8056) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [50/62] time 0.437 (0.453) data 0.305 (0.322) loss_u loss_u 0.8447 (0.8554) acc_u 12.5000 (16.7500) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [55/62] time 0.366 (0.453) data 0.234 (0.322) loss_u loss_u 0.9048 (0.8570) acc_u 9.3750 (16.6477) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [60/62] time 0.395 (0.450) data 0.263 (0.319) loss_u loss_u 0.8882 (0.8548) acc_u 15.6250 (17.0312) lr 1.4818e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1556
confident_label rate tensor(0.3658, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1147
clean true:1061
clean false:86
clean_rate:0.9250217959895379
noisy true:519
noisy false:1470
after delete: len(clean_dataset) 1147
after delete: len(noisy_dataset) 1989
epoch [71/200] batch [5/35] time 0.434 (0.444) data 0.302 (0.313) loss_x loss_x 0.8281 (1.2730) acc_x 84.3750 (69.3750) lr 1.4679e-03 eta 0:00:13
epoch [71/200] batch [10/35] time 0.450 (0.476) data 0.315 (0.344) loss_x loss_x 1.0742 (1.1955) acc_x 81.2500 (68.7500) lr 1.4679e-03 eta 0:00:11
epoch [71/200] batch [15/35] time 0.522 (0.479) data 0.392 (0.348) loss_x loss_x 0.9487 (1.2053) acc_x 68.7500 (69.5833) lr 1.4679e-03 eta 0:00:09
epoch [71/200] batch [20/35] time 0.343 (0.460) data 0.212 (0.328) loss_x loss_x 1.1621 (1.2090) acc_x 68.7500 (69.3750) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [25/35] time 0.421 (0.459) data 0.290 (0.328) loss_x loss_x 1.6748 (1.2710) acc_x 59.3750 (68.1250) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [30/35] time 0.433 (0.464) data 0.303 (0.334) loss_x loss_x 1.3525 (1.2588) acc_x 75.0000 (68.5417) lr 1.4679e-03 eta 0:00:02
epoch [71/200] batch [35/35] time 0.432 (0.460) data 0.302 (0.329) loss_x loss_x 1.0479 (1.2341) acc_x 71.8750 (69.1071) lr 1.4679e-03 eta 0:00:00
epoch [71/200] batch [5/62] time 0.617 (0.464) data 0.487 (0.333) loss_u loss_u 0.8550 (0.8440) acc_u 15.6250 (18.7500) lr 1.4679e-03 eta 0:00:26
epoch [71/200] batch [10/62] time 0.587 (0.465) data 0.455 (0.334) loss_u loss_u 0.8696 (0.8477) acc_u 18.7500 (19.3750) lr 1.4679e-03 eta 0:00:24
epoch [71/200] batch [15/62] time 0.328 (0.461) data 0.196 (0.331) loss_u loss_u 0.9102 (0.8624) acc_u 9.3750 (17.0833) lr 1.4679e-03 eta 0:00:21
epoch [71/200] batch [20/62] time 0.431 (0.456) data 0.299 (0.325) loss_u loss_u 0.7983 (0.8591) acc_u 28.1250 (17.6562) lr 1.4679e-03 eta 0:00:19
epoch [71/200] batch [25/62] time 0.535 (0.458) data 0.404 (0.327) loss_u loss_u 0.8506 (0.8550) acc_u 18.7500 (18.0000) lr 1.4679e-03 eta 0:00:16
epoch [71/200] batch [30/62] time 0.439 (0.457) data 0.308 (0.326) loss_u loss_u 0.8413 (0.8588) acc_u 21.8750 (17.8125) lr 1.4679e-03 eta 0:00:14
epoch [71/200] batch [35/62] time 0.490 (0.459) data 0.358 (0.328) loss_u loss_u 0.8838 (0.8610) acc_u 12.5000 (17.5893) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [40/62] time 0.457 (0.458) data 0.320 (0.327) loss_u loss_u 0.8677 (0.8576) acc_u 15.6250 (18.2812) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [45/62] time 0.561 (0.459) data 0.430 (0.328) loss_u loss_u 0.8652 (0.8590) acc_u 15.6250 (17.9861) lr 1.4679e-03 eta 0:00:07
epoch [71/200] batch [50/62] time 0.533 (0.459) data 0.400 (0.328) loss_u loss_u 0.8501 (0.8586) acc_u 12.5000 (17.8750) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [55/62] time 0.455 (0.457) data 0.323 (0.326) loss_u loss_u 0.8799 (0.8571) acc_u 9.3750 (18.0114) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [60/62] time 0.417 (0.454) data 0.285 (0.322) loss_u loss_u 0.8843 (0.8560) acc_u 15.6250 (17.9688) lr 1.4679e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1514
confident_label rate tensor(0.3626, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1137
clean true:1069
clean false:68
clean_rate:0.940193491644679
noisy true:553
noisy false:1446
after delete: len(clean_dataset) 1137
after delete: len(noisy_dataset) 1999
epoch [72/200] batch [5/35] time 0.419 (0.446) data 0.287 (0.315) loss_x loss_x 1.3682 (1.1830) acc_x 65.6250 (68.1250) lr 1.4540e-03 eta 0:00:13
epoch [72/200] batch [10/35] time 0.479 (0.451) data 0.347 (0.320) loss_x loss_x 1.1035 (1.0782) acc_x 75.0000 (70.9375) lr 1.4540e-03 eta 0:00:11
epoch [72/200] batch [15/35] time 0.418 (0.453) data 0.287 (0.322) loss_x loss_x 1.9600 (1.1922) acc_x 59.3750 (68.3333) lr 1.4540e-03 eta 0:00:09
epoch [72/200] batch [20/35] time 0.502 (0.453) data 0.371 (0.322) loss_x loss_x 1.6309 (1.2605) acc_x 65.6250 (67.1875) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [25/35] time 0.492 (0.460) data 0.361 (0.330) loss_x loss_x 1.2920 (1.2343) acc_x 65.6250 (67.2500) lr 1.4540e-03 eta 0:00:04
epoch [72/200] batch [30/35] time 0.479 (0.454) data 0.349 (0.323) loss_x loss_x 1.0801 (1.2221) acc_x 71.8750 (67.8125) lr 1.4540e-03 eta 0:00:02
epoch [72/200] batch [35/35] time 0.469 (0.452) data 0.339 (0.321) loss_x loss_x 1.0049 (1.1866) acc_x 75.0000 (68.9286) lr 1.4540e-03 eta 0:00:00
epoch [72/200] batch [5/62] time 0.696 (0.460) data 0.564 (0.329) loss_u loss_u 0.8462 (0.8522) acc_u 15.6250 (16.2500) lr 1.4540e-03 eta 0:00:26
epoch [72/200] batch [10/62] time 0.392 (0.450) data 0.260 (0.319) loss_u loss_u 0.8496 (0.8587) acc_u 21.8750 (17.1875) lr 1.4540e-03 eta 0:00:23
epoch [72/200] batch [15/62] time 0.380 (0.448) data 0.249 (0.317) loss_u loss_u 0.8618 (0.8678) acc_u 18.7500 (16.6667) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [20/62] time 0.471 (0.446) data 0.340 (0.315) loss_u loss_u 0.8403 (0.8628) acc_u 15.6250 (16.7188) lr 1.4540e-03 eta 0:00:18
epoch [72/200] batch [25/62] time 0.404 (0.444) data 0.272 (0.313) loss_u loss_u 0.8188 (0.8557) acc_u 21.8750 (17.7500) lr 1.4540e-03 eta 0:00:16
epoch [72/200] batch [30/62] time 0.404 (0.442) data 0.272 (0.311) loss_u loss_u 0.8901 (0.8517) acc_u 12.5000 (18.0208) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [35/62] time 0.497 (0.444) data 0.365 (0.313) loss_u loss_u 0.8228 (0.8540) acc_u 25.0000 (17.6786) lr 1.4540e-03 eta 0:00:11
epoch [72/200] batch [40/62] time 0.322 (0.446) data 0.191 (0.315) loss_u loss_u 0.8740 (0.8481) acc_u 15.6250 (18.2812) lr 1.4540e-03 eta 0:00:09
epoch [72/200] batch [45/62] time 0.410 (0.447) data 0.279 (0.316) loss_u loss_u 0.9087 (0.8503) acc_u 12.5000 (18.4028) lr 1.4540e-03 eta 0:00:07
epoch [72/200] batch [50/62] time 0.443 (0.446) data 0.311 (0.314) loss_u loss_u 0.8252 (0.8472) acc_u 31.2500 (18.8125) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [55/62] time 0.421 (0.446) data 0.289 (0.315) loss_u loss_u 0.8247 (0.8452) acc_u 21.8750 (19.2045) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [60/62] time 0.438 (0.446) data 0.306 (0.315) loss_u loss_u 0.8491 (0.8488) acc_u 21.8750 (18.9583) lr 1.4540e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1513
confident_label rate tensor(0.3782, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1186
clean true:1118
clean false:68
clean_rate:0.9426644182124789
noisy true:505
noisy false:1445
after delete: len(clean_dataset) 1186
after delete: len(noisy_dataset) 1950
epoch [73/200] batch [5/37] time 0.520 (0.453) data 0.388 (0.322) loss_x loss_x 1.1709 (1.0926) acc_x 71.8750 (73.7500) lr 1.4399e-03 eta 0:00:14
epoch [73/200] batch [10/37] time 0.395 (0.479) data 0.264 (0.349) loss_x loss_x 0.9321 (1.1705) acc_x 71.8750 (71.8750) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [15/37] time 0.385 (0.464) data 0.255 (0.333) loss_x loss_x 1.4033 (1.1941) acc_x 62.5000 (71.0417) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [20/37] time 0.460 (0.470) data 0.329 (0.339) loss_x loss_x 1.4639 (1.2486) acc_x 53.1250 (68.9062) lr 1.4399e-03 eta 0:00:07
epoch [73/200] batch [25/37] time 0.449 (0.460) data 0.319 (0.329) loss_x loss_x 0.8921 (1.2407) acc_x 68.7500 (68.5000) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [30/37] time 0.437 (0.459) data 0.306 (0.328) loss_x loss_x 1.8047 (1.2771) acc_x 53.1250 (67.5000) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [35/37] time 0.331 (0.455) data 0.201 (0.324) loss_x loss_x 0.8613 (1.2591) acc_x 75.0000 (67.8571) lr 1.4399e-03 eta 0:00:00
epoch [73/200] batch [5/60] time 0.328 (0.453) data 0.197 (0.322) loss_u loss_u 0.9263 (0.8904) acc_u 6.2500 (11.8750) lr 1.4399e-03 eta 0:00:24
epoch [73/200] batch [10/60] time 0.415 (0.448) data 0.283 (0.317) loss_u loss_u 0.9287 (0.8846) acc_u 12.5000 (13.1250) lr 1.4399e-03 eta 0:00:22
epoch [73/200] batch [15/60] time 0.594 (0.453) data 0.463 (0.322) loss_u loss_u 0.8066 (0.8738) acc_u 21.8750 (15.0000) lr 1.4399e-03 eta 0:00:20
epoch [73/200] batch [20/60] time 0.543 (0.453) data 0.413 (0.322) loss_u loss_u 0.8843 (0.8753) acc_u 12.5000 (15.3125) lr 1.4399e-03 eta 0:00:18
epoch [73/200] batch [25/60] time 0.375 (0.453) data 0.243 (0.322) loss_u loss_u 0.8428 (0.8702) acc_u 15.6250 (16.0000) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [30/60] time 0.399 (0.451) data 0.268 (0.320) loss_u loss_u 0.7881 (0.8687) acc_u 25.0000 (15.6250) lr 1.4399e-03 eta 0:00:13
epoch [73/200] batch [35/60] time 0.486 (0.448) data 0.356 (0.317) loss_u loss_u 0.8403 (0.8664) acc_u 18.7500 (15.8036) lr 1.4399e-03 eta 0:00:11
epoch [73/200] batch [40/60] time 0.489 (0.453) data 0.358 (0.322) loss_u loss_u 0.8755 (0.8655) acc_u 15.6250 (15.8594) lr 1.4399e-03 eta 0:00:09
epoch [73/200] batch [45/60] time 0.482 (0.457) data 0.351 (0.327) loss_u loss_u 0.9097 (0.8683) acc_u 15.6250 (15.9722) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [50/60] time 0.548 (0.457) data 0.418 (0.326) loss_u loss_u 0.8525 (0.8654) acc_u 25.0000 (16.3125) lr 1.4399e-03 eta 0:00:04
epoch [73/200] batch [55/60] time 0.398 (0.455) data 0.266 (0.325) loss_u loss_u 0.8599 (0.8642) acc_u 12.5000 (16.4205) lr 1.4399e-03 eta 0:00:02
epoch [73/200] batch [60/60] time 0.358 (0.452) data 0.226 (0.321) loss_u loss_u 0.7856 (0.8652) acc_u 18.7500 (16.2500) lr 1.4399e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1528
confident_label rate tensor(0.3642, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1142
clean true:1068
clean false:74
clean_rate:0.9352014010507881
noisy true:540
noisy false:1454
after delete: len(clean_dataset) 1142
after delete: len(noisy_dataset) 1994
epoch [74/200] batch [5/35] time 0.540 (0.480) data 0.410 (0.349) loss_x loss_x 1.0703 (1.2725) acc_x 78.1250 (70.6250) lr 1.4258e-03 eta 0:00:14
epoch [74/200] batch [10/35] time 0.416 (0.462) data 0.286 (0.331) loss_x loss_x 1.4258 (1.3309) acc_x 68.7500 (68.4375) lr 1.4258e-03 eta 0:00:11
epoch [74/200] batch [15/35] time 0.426 (0.446) data 0.296 (0.315) loss_x loss_x 1.2803 (1.2956) acc_x 68.7500 (68.5417) lr 1.4258e-03 eta 0:00:08
epoch [74/200] batch [20/35] time 0.595 (0.448) data 0.464 (0.318) loss_x loss_x 1.1719 (1.2702) acc_x 71.8750 (68.9062) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [25/35] time 0.511 (0.455) data 0.381 (0.324) loss_x loss_x 1.3438 (1.2747) acc_x 71.8750 (69.1250) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [30/35] time 0.483 (0.458) data 0.353 (0.327) loss_x loss_x 1.5596 (1.2873) acc_x 59.3750 (68.6458) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [35/35] time 0.463 (0.459) data 0.333 (0.329) loss_x loss_x 1.1621 (1.2906) acc_x 68.7500 (68.6607) lr 1.4258e-03 eta 0:00:00
epoch [74/200] batch [5/62] time 0.453 (0.458) data 0.322 (0.327) loss_u loss_u 0.8882 (0.8706) acc_u 15.6250 (18.1250) lr 1.4258e-03 eta 0:00:26
epoch [74/200] batch [10/62] time 0.411 (0.452) data 0.280 (0.321) loss_u loss_u 0.8726 (0.8531) acc_u 18.7500 (19.6875) lr 1.4258e-03 eta 0:00:23
epoch [74/200] batch [15/62] time 0.526 (0.453) data 0.394 (0.322) loss_u loss_u 0.8652 (0.8637) acc_u 18.7500 (18.3333) lr 1.4258e-03 eta 0:00:21
epoch [74/200] batch [20/62] time 0.582 (0.454) data 0.451 (0.323) loss_u loss_u 0.8359 (0.8636) acc_u 21.8750 (18.1250) lr 1.4258e-03 eta 0:00:19
epoch [74/200] batch [25/62] time 0.516 (0.452) data 0.384 (0.322) loss_u loss_u 0.8535 (0.8595) acc_u 18.7500 (18.3750) lr 1.4258e-03 eta 0:00:16
epoch [74/200] batch [30/62] time 0.529 (0.453) data 0.397 (0.322) loss_u loss_u 0.8638 (0.8568) acc_u 18.7500 (18.9583) lr 1.4258e-03 eta 0:00:14
epoch [74/200] batch [35/62] time 0.396 (0.450) data 0.264 (0.319) loss_u loss_u 0.9189 (0.8610) acc_u 12.5000 (18.5714) lr 1.4258e-03 eta 0:00:12
epoch [74/200] batch [40/62] time 0.531 (0.454) data 0.399 (0.323) loss_u loss_u 0.7925 (0.8555) acc_u 25.0000 (18.9844) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [45/62] time 0.385 (0.454) data 0.253 (0.323) loss_u loss_u 0.8037 (0.8511) acc_u 21.8750 (19.2361) lr 1.4258e-03 eta 0:00:07
epoch [74/200] batch [50/62] time 0.323 (0.452) data 0.191 (0.321) loss_u loss_u 0.8579 (0.8521) acc_u 15.6250 (19.1250) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [55/62] time 0.427 (0.450) data 0.296 (0.319) loss_u loss_u 0.9058 (0.8544) acc_u 9.3750 (18.7500) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [60/62] time 0.593 (0.449) data 0.460 (0.318) loss_u loss_u 0.7759 (0.8517) acc_u 25.0000 (18.8542) lr 1.4258e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1510
confident_label rate tensor(0.3731, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1170
clean true:1100
clean false:70
clean_rate:0.9401709401709402
noisy true:526
noisy false:1440
after delete: len(clean_dataset) 1170
after delete: len(noisy_dataset) 1966
epoch [75/200] batch [5/36] time 0.669 (0.471) data 0.538 (0.340) loss_x loss_x 0.9526 (1.2677) acc_x 68.7500 (66.8750) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [10/36] time 0.498 (0.472) data 0.367 (0.341) loss_x loss_x 1.3760 (1.3005) acc_x 56.2500 (64.6875) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [15/36] time 0.627 (0.482) data 0.496 (0.351) loss_x loss_x 2.1426 (1.3227) acc_x 56.2500 (65.0000) lr 1.4115e-03 eta 0:00:10
epoch [75/200] batch [20/36] time 0.470 (0.471) data 0.339 (0.340) loss_x loss_x 1.3096 (1.2388) acc_x 65.6250 (67.5000) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [25/36] time 0.363 (0.471) data 0.232 (0.340) loss_x loss_x 1.6641 (1.2636) acc_x 59.3750 (66.6250) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [30/36] time 0.539 (0.469) data 0.408 (0.338) loss_x loss_x 1.2822 (1.2595) acc_x 68.7500 (67.5000) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [35/36] time 0.484 (0.468) data 0.353 (0.337) loss_x loss_x 1.6777 (1.3113) acc_x 59.3750 (66.6071) lr 1.4115e-03 eta 0:00:00
epoch [75/200] batch [5/61] time 0.381 (0.463) data 0.249 (0.332) loss_u loss_u 0.8301 (0.8691) acc_u 28.1250 (17.5000) lr 1.4115e-03 eta 0:00:25
epoch [75/200] batch [10/61] time 0.578 (0.459) data 0.447 (0.328) loss_u loss_u 0.6860 (0.8393) acc_u 37.5000 (21.5625) lr 1.4115e-03 eta 0:00:23
epoch [75/200] batch [15/61] time 0.411 (0.452) data 0.279 (0.321) loss_u loss_u 0.8022 (0.8369) acc_u 21.8750 (20.8333) lr 1.4115e-03 eta 0:00:20
epoch [75/200] batch [20/61] time 0.594 (0.454) data 0.462 (0.323) loss_u loss_u 0.8892 (0.8418) acc_u 12.5000 (20.6250) lr 1.4115e-03 eta 0:00:18
epoch [75/200] batch [25/61] time 0.439 (0.457) data 0.307 (0.325) loss_u loss_u 0.8984 (0.8536) acc_u 18.7500 (19.0000) lr 1.4115e-03 eta 0:00:16
epoch [75/200] batch [30/61] time 0.413 (0.456) data 0.281 (0.325) loss_u loss_u 0.8320 (0.8555) acc_u 21.8750 (19.0625) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [35/61] time 0.492 (0.455) data 0.361 (0.324) loss_u loss_u 0.9526 (0.8532) acc_u 6.2500 (19.0179) lr 1.4115e-03 eta 0:00:11
epoch [75/200] batch [40/61] time 0.351 (0.455) data 0.220 (0.323) loss_u loss_u 0.9072 (0.8571) acc_u 12.5000 (18.4375) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [45/61] time 0.383 (0.455) data 0.253 (0.324) loss_u loss_u 0.8721 (0.8566) acc_u 15.6250 (18.1944) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [50/61] time 0.336 (0.454) data 0.204 (0.323) loss_u loss_u 0.8164 (0.8541) acc_u 21.8750 (18.4375) lr 1.4115e-03 eta 0:00:04
epoch [75/200] batch [55/61] time 0.666 (0.456) data 0.535 (0.325) loss_u loss_u 0.8662 (0.8532) acc_u 15.6250 (18.6932) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [60/61] time 0.466 (0.453) data 0.334 (0.322) loss_u loss_u 0.9341 (0.8548) acc_u 6.2500 (18.6458) lr 1.4115e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1547
confident_label rate tensor(0.3728, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1169
clean true:1081
clean false:88
clean_rate:0.9247219846022241
noisy true:508
noisy false:1459
after delete: len(clean_dataset) 1169
after delete: len(noisy_dataset) 1967
epoch [76/200] batch [5/36] time 0.466 (0.437) data 0.336 (0.307) loss_x loss_x 1.1143 (1.2782) acc_x 68.7500 (68.1250) lr 1.3971e-03 eta 0:00:13
epoch [76/200] batch [10/36] time 0.460 (0.445) data 0.330 (0.314) loss_x loss_x 1.1885 (1.3477) acc_x 75.0000 (67.1875) lr 1.3971e-03 eta 0:00:11
epoch [76/200] batch [15/36] time 0.602 (0.460) data 0.471 (0.330) loss_x loss_x 1.5303 (1.3581) acc_x 59.3750 (66.2500) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [20/36] time 0.371 (0.484) data 0.241 (0.354) loss_x loss_x 1.3184 (1.3186) acc_x 59.3750 (66.7188) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [25/36] time 0.424 (0.484) data 0.294 (0.354) loss_x loss_x 0.8394 (1.2391) acc_x 81.2500 (69.1250) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [30/36] time 0.408 (0.474) data 0.278 (0.344) loss_x loss_x 1.0176 (1.2381) acc_x 75.0000 (68.9583) lr 1.3971e-03 eta 0:00:02
epoch [76/200] batch [35/36] time 0.400 (0.466) data 0.270 (0.336) loss_x loss_x 1.1250 (1.2213) acc_x 65.6250 (69.3750) lr 1.3971e-03 eta 0:00:00
epoch [76/200] batch [5/61] time 0.443 (0.458) data 0.312 (0.328) loss_u loss_u 0.8184 (0.8438) acc_u 28.1250 (20.0000) lr 1.3971e-03 eta 0:00:25
epoch [76/200] batch [10/61] time 0.444 (0.451) data 0.313 (0.321) loss_u loss_u 0.8804 (0.8454) acc_u 12.5000 (20.0000) lr 1.3971e-03 eta 0:00:23
epoch [76/200] batch [15/61] time 0.380 (0.447) data 0.249 (0.316) loss_u loss_u 0.8208 (0.8477) acc_u 21.8750 (19.1667) lr 1.3971e-03 eta 0:00:20
epoch [76/200] batch [20/61] time 0.337 (0.445) data 0.205 (0.315) loss_u loss_u 0.8008 (0.8485) acc_u 31.2500 (19.0625) lr 1.3971e-03 eta 0:00:18
epoch [76/200] batch [25/61] time 0.429 (0.449) data 0.298 (0.318) loss_u loss_u 0.8330 (0.8462) acc_u 21.8750 (19.5000) lr 1.3971e-03 eta 0:00:16
epoch [76/200] batch [30/61] time 0.555 (0.451) data 0.423 (0.320) loss_u loss_u 0.8994 (0.8480) acc_u 9.3750 (19.3750) lr 1.3971e-03 eta 0:00:13
epoch [76/200] batch [35/61] time 0.467 (0.453) data 0.335 (0.322) loss_u loss_u 0.7915 (0.8430) acc_u 31.2500 (20.1786) lr 1.3971e-03 eta 0:00:11
epoch [76/200] batch [40/61] time 0.476 (0.450) data 0.345 (0.319) loss_u loss_u 0.8442 (0.8448) acc_u 15.6250 (19.6875) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [45/61] time 0.481 (0.454) data 0.351 (0.323) loss_u loss_u 0.8359 (0.8472) acc_u 25.0000 (19.4444) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [50/61] time 0.371 (0.451) data 0.239 (0.320) loss_u loss_u 0.8960 (0.8512) acc_u 12.5000 (19.0000) lr 1.3971e-03 eta 0:00:04
epoch [76/200] batch [55/61] time 0.343 (0.451) data 0.212 (0.320) loss_u loss_u 0.8379 (0.8520) acc_u 21.8750 (18.8068) lr 1.3971e-03 eta 0:00:02
epoch [76/200] batch [60/61] time 0.465 (0.454) data 0.334 (0.324) loss_u loss_u 0.8540 (0.8532) acc_u 15.6250 (18.5938) lr 1.3971e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1502
confident_label rate tensor(0.3807, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1194
clean true:1116
clean false:78
clean_rate:0.9346733668341709
noisy true:518
noisy false:1424
after delete: len(clean_dataset) 1194
after delete: len(noisy_dataset) 1942
epoch [77/200] batch [5/37] time 0.487 (0.464) data 0.357 (0.333) loss_x loss_x 1.2451 (0.9315) acc_x 71.8750 (80.0000) lr 1.3827e-03 eta 0:00:14
epoch [77/200] batch [10/37] time 0.463 (0.458) data 0.332 (0.327) loss_x loss_x 1.3564 (1.0737) acc_x 59.3750 (74.0625) lr 1.3827e-03 eta 0:00:12
epoch [77/200] batch [15/37] time 0.431 (0.485) data 0.300 (0.354) loss_x loss_x 1.3242 (1.1363) acc_x 62.5000 (70.8333) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [20/37] time 0.576 (0.478) data 0.446 (0.348) loss_x loss_x 1.2578 (1.1388) acc_x 59.3750 (70.3125) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [25/37] time 0.466 (0.471) data 0.335 (0.340) loss_x loss_x 1.9932 (1.1834) acc_x 50.0000 (69.0000) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [30/37] time 0.452 (0.465) data 0.321 (0.334) loss_x loss_x 1.1709 (1.1926) acc_x 71.8750 (69.0625) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [35/37] time 0.436 (0.464) data 0.305 (0.333) loss_x loss_x 0.7729 (1.1905) acc_x 78.1250 (68.6607) lr 1.3827e-03 eta 0:00:00
epoch [77/200] batch [5/60] time 0.411 (0.460) data 0.279 (0.329) loss_u loss_u 0.8730 (0.8086) acc_u 15.6250 (23.1250) lr 1.3827e-03 eta 0:00:25
epoch [77/200] batch [10/60] time 0.449 (0.459) data 0.318 (0.328) loss_u loss_u 0.8521 (0.8316) acc_u 21.8750 (19.6875) lr 1.3827e-03 eta 0:00:22
epoch [77/200] batch [15/60] time 0.496 (0.461) data 0.366 (0.330) loss_u loss_u 0.9111 (0.8455) acc_u 12.5000 (17.5000) lr 1.3827e-03 eta 0:00:20
epoch [77/200] batch [20/60] time 0.387 (0.457) data 0.256 (0.326) loss_u loss_u 0.8364 (0.8382) acc_u 18.7500 (18.4375) lr 1.3827e-03 eta 0:00:18
epoch [77/200] batch [25/60] time 0.475 (0.457) data 0.345 (0.326) loss_u loss_u 0.8447 (0.8457) acc_u 18.7500 (18.0000) lr 1.3827e-03 eta 0:00:16
epoch [77/200] batch [30/60] time 0.647 (0.460) data 0.515 (0.329) loss_u loss_u 0.8711 (0.8450) acc_u 21.8750 (18.7500) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [35/60] time 0.377 (0.454) data 0.245 (0.323) loss_u loss_u 0.9453 (0.8493) acc_u 6.2500 (18.1250) lr 1.3827e-03 eta 0:00:11
epoch [77/200] batch [40/60] time 0.458 (0.451) data 0.327 (0.320) loss_u loss_u 0.8198 (0.8505) acc_u 21.8750 (17.8906) lr 1.3827e-03 eta 0:00:09
epoch [77/200] batch [45/60] time 0.546 (0.452) data 0.416 (0.321) loss_u loss_u 0.8564 (0.8520) acc_u 15.6250 (17.7778) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [50/60] time 0.513 (0.451) data 0.382 (0.320) loss_u loss_u 0.8716 (0.8543) acc_u 15.6250 (17.7500) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [55/60] time 0.379 (0.453) data 0.249 (0.322) loss_u loss_u 0.7852 (0.8549) acc_u 25.0000 (17.4432) lr 1.3827e-03 eta 0:00:02
epoch [77/200] batch [60/60] time 0.420 (0.451) data 0.288 (0.320) loss_u loss_u 0.8281 (0.8564) acc_u 31.2500 (17.5000) lr 1.3827e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.3766, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1181
clean true:1114
clean false:67
clean_rate:0.9432684165961049
noisy true:505
noisy false:1450
after delete: len(clean_dataset) 1181
after delete: len(noisy_dataset) 1955
epoch [78/200] batch [5/36] time 0.459 (0.472) data 0.328 (0.341) loss_x loss_x 1.1406 (1.3654) acc_x 71.8750 (65.6250) lr 1.3681e-03 eta 0:00:14
epoch [78/200] batch [10/36] time 0.494 (0.463) data 0.362 (0.332) loss_x loss_x 1.3604 (1.4096) acc_x 62.5000 (64.3750) lr 1.3681e-03 eta 0:00:12
epoch [78/200] batch [15/36] time 0.463 (0.467) data 0.332 (0.337) loss_x loss_x 1.1152 (1.4134) acc_x 68.7500 (65.8333) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [20/36] time 0.536 (0.481) data 0.406 (0.351) loss_x loss_x 0.8433 (1.3480) acc_x 84.3750 (66.7188) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [25/36] time 0.436 (0.485) data 0.306 (0.354) loss_x loss_x 1.8906 (1.3668) acc_x 62.5000 (66.0000) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [30/36] time 0.462 (0.474) data 0.332 (0.344) loss_x loss_x 1.4023 (1.3702) acc_x 62.5000 (65.4167) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [35/36] time 0.400 (0.466) data 0.269 (0.335) loss_x loss_x 0.8794 (1.3710) acc_x 78.1250 (65.7143) lr 1.3681e-03 eta 0:00:00
epoch [78/200] batch [5/61] time 0.486 (0.459) data 0.355 (0.328) loss_u loss_u 0.7910 (0.8560) acc_u 25.0000 (15.6250) lr 1.3681e-03 eta 0:00:25
epoch [78/200] batch [10/61] time 0.382 (0.452) data 0.250 (0.321) loss_u loss_u 0.8350 (0.8508) acc_u 25.0000 (19.3750) lr 1.3681e-03 eta 0:00:23
epoch [78/200] batch [15/61] time 0.456 (0.455) data 0.325 (0.324) loss_u loss_u 0.8457 (0.8520) acc_u 21.8750 (18.3333) lr 1.3681e-03 eta 0:00:20
epoch [78/200] batch [20/61] time 0.494 (0.452) data 0.363 (0.321) loss_u loss_u 0.8906 (0.8540) acc_u 12.5000 (18.1250) lr 1.3681e-03 eta 0:00:18
epoch [78/200] batch [25/61] time 0.384 (0.459) data 0.253 (0.329) loss_u loss_u 0.8730 (0.8548) acc_u 18.7500 (18.3750) lr 1.3681e-03 eta 0:00:16
epoch [78/200] batch [30/61] time 0.404 (0.454) data 0.272 (0.324) loss_u loss_u 0.7925 (0.8549) acc_u 34.3750 (18.4375) lr 1.3681e-03 eta 0:00:14
epoch [78/200] batch [35/61] time 0.435 (0.449) data 0.304 (0.319) loss_u loss_u 0.8574 (0.8553) acc_u 15.6250 (18.4821) lr 1.3681e-03 eta 0:00:11
epoch [78/200] batch [40/61] time 0.458 (0.452) data 0.327 (0.321) loss_u loss_u 0.8896 (0.8498) acc_u 9.3750 (18.9844) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [45/61] time 0.621 (0.453) data 0.489 (0.322) loss_u loss_u 0.8975 (0.8501) acc_u 9.3750 (19.0972) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [50/61] time 0.533 (0.456) data 0.402 (0.325) loss_u loss_u 0.8979 (0.8522) acc_u 12.5000 (19.0000) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [55/61] time 0.457 (0.457) data 0.325 (0.326) loss_u loss_u 0.9033 (0.8524) acc_u 21.8750 (19.3182) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [60/61] time 0.382 (0.459) data 0.251 (0.327) loss_u loss_u 0.8296 (0.8519) acc_u 25.0000 (19.3750) lr 1.3681e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1506
confident_label rate tensor(0.3846, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1206
clean true:1130
clean false:76
clean_rate:0.9369817578772802
noisy true:500
noisy false:1430
after delete: len(clean_dataset) 1206
after delete: len(noisy_dataset) 1930
epoch [79/200] batch [5/37] time 0.428 (0.470) data 0.297 (0.339) loss_x loss_x 1.0830 (1.2118) acc_x 68.7500 (73.1250) lr 1.3535e-03 eta 0:00:15
epoch [79/200] batch [10/37] time 0.590 (0.469) data 0.459 (0.338) loss_x loss_x 0.9854 (1.2567) acc_x 78.1250 (71.8750) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [15/37] time 0.379 (0.458) data 0.249 (0.327) loss_x loss_x 0.7783 (1.2137) acc_x 87.5000 (71.8750) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [20/37] time 0.377 (0.452) data 0.247 (0.321) loss_x loss_x 0.8633 (1.1561) acc_x 65.6250 (71.2500) lr 1.3535e-03 eta 0:00:07
epoch [79/200] batch [25/37] time 0.371 (0.453) data 0.240 (0.322) loss_x loss_x 1.0361 (1.1730) acc_x 75.0000 (69.7500) lr 1.3535e-03 eta 0:00:05
epoch [79/200] batch [30/37] time 0.460 (0.454) data 0.330 (0.324) loss_x loss_x 0.9126 (1.1851) acc_x 75.0000 (70.0000) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [35/37] time 0.431 (0.456) data 0.301 (0.325) loss_x loss_x 1.1689 (1.2112) acc_x 78.1250 (69.7321) lr 1.3535e-03 eta 0:00:00
epoch [79/200] batch [5/60] time 0.396 (0.458) data 0.265 (0.328) loss_u loss_u 0.7788 (0.8080) acc_u 21.8750 (24.3750) lr 1.3535e-03 eta 0:00:25
epoch [79/200] batch [10/60] time 0.542 (0.460) data 0.411 (0.329) loss_u loss_u 0.8491 (0.8334) acc_u 21.8750 (21.5625) lr 1.3535e-03 eta 0:00:22
epoch [79/200] batch [15/60] time 0.792 (0.465) data 0.661 (0.335) loss_u loss_u 0.8794 (0.8569) acc_u 15.6250 (17.9167) lr 1.3535e-03 eta 0:00:20
epoch [79/200] batch [20/60] time 0.476 (0.466) data 0.345 (0.335) loss_u loss_u 0.8608 (0.8625) acc_u 15.6250 (17.5000) lr 1.3535e-03 eta 0:00:18
epoch [79/200] batch [25/60] time 0.547 (0.464) data 0.416 (0.333) loss_u loss_u 0.7964 (0.8596) acc_u 31.2500 (18.1250) lr 1.3535e-03 eta 0:00:16
epoch [79/200] batch [30/60] time 0.465 (0.462) data 0.334 (0.331) loss_u loss_u 0.8271 (0.8565) acc_u 25.0000 (18.2292) lr 1.3535e-03 eta 0:00:13
epoch [79/200] batch [35/60] time 0.378 (0.460) data 0.248 (0.329) loss_u loss_u 0.8770 (0.8574) acc_u 12.5000 (18.2143) lr 1.3535e-03 eta 0:00:11
epoch [79/200] batch [40/60] time 0.423 (0.462) data 0.292 (0.331) loss_u loss_u 0.8525 (0.8572) acc_u 18.7500 (18.2812) lr 1.3535e-03 eta 0:00:09
epoch [79/200] batch [45/60] time 0.493 (0.459) data 0.361 (0.328) loss_u loss_u 0.8086 (0.8546) acc_u 25.0000 (18.6806) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [50/60] time 0.411 (0.457) data 0.280 (0.326) loss_u loss_u 0.8535 (0.8525) acc_u 18.7500 (19.1875) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [55/60] time 0.389 (0.455) data 0.259 (0.324) loss_u loss_u 0.8506 (0.8539) acc_u 21.8750 (19.2045) lr 1.3535e-03 eta 0:00:02
epoch [79/200] batch [60/60] time 0.500 (0.452) data 0.370 (0.321) loss_u loss_u 0.8203 (0.8529) acc_u 25.0000 (19.3750) lr 1.3535e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1490
confident_label rate tensor(0.3766, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1181
clean true:1113
clean false:68
clean_rate:0.9424216765453006
noisy true:533
noisy false:1422
after delete: len(clean_dataset) 1181
after delete: len(noisy_dataset) 1955
epoch [80/200] batch [5/36] time 0.541 (0.470) data 0.410 (0.339) loss_x loss_x 1.8623 (1.3104) acc_x 62.5000 (66.2500) lr 1.3387e-03 eta 0:00:14
epoch [80/200] batch [10/36] time 0.412 (0.464) data 0.282 (0.334) loss_x loss_x 1.5527 (1.2333) acc_x 65.6250 (69.3750) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [15/36] time 0.661 (0.477) data 0.530 (0.347) loss_x loss_x 1.3799 (1.2702) acc_x 65.6250 (67.9167) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [20/36] time 0.416 (0.475) data 0.286 (0.345) loss_x loss_x 1.7412 (1.3044) acc_x 56.2500 (67.8125) lr 1.3387e-03 eta 0:00:07
epoch [80/200] batch [25/36] time 0.489 (0.469) data 0.358 (0.338) loss_x loss_x 0.9722 (1.2670) acc_x 65.6250 (68.6250) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [30/36] time 0.512 (0.465) data 0.382 (0.334) loss_x loss_x 1.3711 (1.3000) acc_x 62.5000 (68.1250) lr 1.3387e-03 eta 0:00:02
epoch [80/200] batch [35/36] time 0.401 (0.470) data 0.271 (0.339) loss_x loss_x 1.1338 (1.2995) acc_x 71.8750 (68.3929) lr 1.3387e-03 eta 0:00:00
epoch [80/200] batch [5/61] time 0.426 (0.470) data 0.294 (0.340) loss_u loss_u 0.8716 (0.8369) acc_u 12.5000 (23.7500) lr 1.3387e-03 eta 0:00:26
epoch [80/200] batch [10/61] time 0.455 (0.466) data 0.323 (0.335) loss_u loss_u 0.7998 (0.8330) acc_u 28.1250 (22.1875) lr 1.3387e-03 eta 0:00:23
epoch [80/200] batch [15/61] time 0.406 (0.469) data 0.275 (0.338) loss_u loss_u 0.8379 (0.8384) acc_u 25.0000 (21.0417) lr 1.3387e-03 eta 0:00:21
epoch [80/200] batch [20/61] time 0.442 (0.464) data 0.311 (0.333) loss_u loss_u 0.8164 (0.8437) acc_u 25.0000 (20.6250) lr 1.3387e-03 eta 0:00:19
epoch [80/200] batch [25/61] time 0.433 (0.462) data 0.302 (0.332) loss_u loss_u 0.8667 (0.8489) acc_u 12.5000 (19.2500) lr 1.3387e-03 eta 0:00:16
epoch [80/200] batch [30/61] time 0.346 (0.458) data 0.214 (0.327) loss_u loss_u 0.8364 (0.8495) acc_u 15.6250 (18.8542) lr 1.3387e-03 eta 0:00:14
epoch [80/200] batch [35/61] time 0.427 (0.453) data 0.296 (0.322) loss_u loss_u 0.8354 (0.8474) acc_u 18.7500 (19.0179) lr 1.3387e-03 eta 0:00:11
epoch [80/200] batch [40/61] time 0.411 (0.451) data 0.279 (0.320) loss_u loss_u 0.9165 (0.8482) acc_u 12.5000 (18.5938) lr 1.3387e-03 eta 0:00:09
epoch [80/200] batch [45/61] time 0.391 (0.450) data 0.259 (0.319) loss_u loss_u 0.8071 (0.8491) acc_u 18.7500 (18.3333) lr 1.3387e-03 eta 0:00:07
epoch [80/200] batch [50/61] time 0.509 (0.453) data 0.378 (0.322) loss_u loss_u 0.8647 (0.8502) acc_u 15.6250 (18.3125) lr 1.3387e-03 eta 0:00:04
epoch [80/200] batch [55/61] time 0.482 (0.453) data 0.351 (0.322) loss_u loss_u 0.8408 (0.8517) acc_u 25.0000 (18.2386) lr 1.3387e-03 eta 0:00:02
epoch [80/200] batch [60/61] time 0.594 (0.453) data 0.463 (0.322) loss_u loss_u 0.8960 (0.8527) acc_u 12.5000 (18.1771) lr 1.3387e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1544
confident_label rate tensor(0.3766, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1181
clean true:1101
clean false:80
clean_rate:0.9322607959356477
noisy true:491
noisy false:1464
after delete: len(clean_dataset) 1181
after delete: len(noisy_dataset) 1955
epoch [81/200] batch [5/36] time 0.469 (0.465) data 0.339 (0.335) loss_x loss_x 1.6748 (1.1688) acc_x 59.3750 (70.0000) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [10/36] time 0.373 (0.458) data 0.243 (0.327) loss_x loss_x 1.3086 (1.1374) acc_x 68.7500 (69.3750) lr 1.3239e-03 eta 0:00:11
epoch [81/200] batch [15/36] time 0.442 (0.457) data 0.312 (0.327) loss_x loss_x 1.1621 (1.1029) acc_x 71.8750 (70.6250) lr 1.3239e-03 eta 0:00:09
epoch [81/200] batch [20/36] time 0.471 (0.464) data 0.340 (0.334) loss_x loss_x 1.2881 (1.1748) acc_x 59.3750 (68.5938) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [25/36] time 0.431 (0.486) data 0.301 (0.355) loss_x loss_x 1.6738 (1.2061) acc_x 68.7500 (68.8750) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [30/36] time 0.451 (0.476) data 0.320 (0.346) loss_x loss_x 1.6260 (1.2298) acc_x 59.3750 (68.4375) lr 1.3239e-03 eta 0:00:02
epoch [81/200] batch [35/36] time 0.512 (0.467) data 0.382 (0.337) loss_x loss_x 1.2480 (1.2560) acc_x 68.7500 (67.5000) lr 1.3239e-03 eta 0:00:00
epoch [81/200] batch [5/61] time 0.504 (0.468) data 0.374 (0.337) loss_u loss_u 0.8643 (0.8473) acc_u 21.8750 (18.7500) lr 1.3239e-03 eta 0:00:26
epoch [81/200] batch [10/61] time 0.412 (0.463) data 0.281 (0.333) loss_u loss_u 0.8662 (0.8581) acc_u 18.7500 (18.1250) lr 1.3239e-03 eta 0:00:23
epoch [81/200] batch [15/61] time 0.754 (0.466) data 0.622 (0.335) loss_u loss_u 0.8511 (0.8530) acc_u 21.8750 (18.3333) lr 1.3239e-03 eta 0:00:21
epoch [81/200] batch [20/61] time 0.426 (0.469) data 0.295 (0.338) loss_u loss_u 0.8809 (0.8557) acc_u 15.6250 (17.8125) lr 1.3239e-03 eta 0:00:19
epoch [81/200] batch [25/61] time 0.469 (0.471) data 0.337 (0.340) loss_u loss_u 0.8389 (0.8531) acc_u 18.7500 (18.1250) lr 1.3239e-03 eta 0:00:16
epoch [81/200] batch [30/61] time 0.382 (0.467) data 0.250 (0.336) loss_u loss_u 0.8257 (0.8505) acc_u 31.2500 (18.1250) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [35/61] time 0.396 (0.461) data 0.264 (0.330) loss_u loss_u 0.9287 (0.8525) acc_u 9.3750 (17.9464) lr 1.3239e-03 eta 0:00:11
epoch [81/200] batch [40/61] time 0.378 (0.458) data 0.246 (0.327) loss_u loss_u 0.8823 (0.8514) acc_u 15.6250 (18.2031) lr 1.3239e-03 eta 0:00:09
epoch [81/200] batch [45/61] time 0.758 (0.461) data 0.627 (0.330) loss_u loss_u 0.8784 (0.8513) acc_u 18.7500 (18.2639) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [50/61] time 0.434 (0.462) data 0.303 (0.331) loss_u loss_u 0.8760 (0.8517) acc_u 15.6250 (18.3125) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [55/61] time 0.402 (0.460) data 0.271 (0.329) loss_u loss_u 0.8647 (0.8527) acc_u 28.1250 (18.5227) lr 1.3239e-03 eta 0:00:02
epoch [81/200] batch [60/61] time 0.361 (0.457) data 0.230 (0.326) loss_u loss_u 0.9331 (0.8554) acc_u 12.5000 (18.0729) lr 1.3239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1555
confident_label rate tensor(0.3638, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1141
clean true:1064
clean false:77
clean_rate:0.9325153374233128
noisy true:517
noisy false:1478
after delete: len(clean_dataset) 1141
after delete: len(noisy_dataset) 1995
epoch [82/200] batch [5/35] time 0.509 (0.486) data 0.379 (0.355) loss_x loss_x 1.1758 (1.2152) acc_x 62.5000 (68.7500) lr 1.3090e-03 eta 0:00:14
epoch [82/200] batch [10/35] time 0.395 (0.486) data 0.264 (0.356) loss_x loss_x 0.7710 (1.2720) acc_x 81.2500 (70.0000) lr 1.3090e-03 eta 0:00:12
epoch [82/200] batch [15/35] time 0.428 (0.481) data 0.298 (0.350) loss_x loss_x 0.5005 (1.1973) acc_x 81.2500 (70.0000) lr 1.3090e-03 eta 0:00:09
epoch [82/200] batch [20/35] time 0.392 (0.465) data 0.260 (0.334) loss_x loss_x 1.0781 (1.1637) acc_x 68.7500 (71.4062) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [25/35] time 0.415 (0.468) data 0.284 (0.338) loss_x loss_x 0.9717 (1.1529) acc_x 78.1250 (71.2500) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [30/35] time 0.458 (0.464) data 0.328 (0.333) loss_x loss_x 1.0215 (1.1655) acc_x 75.0000 (70.7292) lr 1.3090e-03 eta 0:00:02
epoch [82/200] batch [35/35] time 0.429 (0.460) data 0.299 (0.329) loss_x loss_x 1.5723 (1.1852) acc_x 62.5000 (70.2679) lr 1.3090e-03 eta 0:00:00
epoch [82/200] batch [5/62] time 0.441 (0.454) data 0.309 (0.323) loss_u loss_u 0.8984 (0.8689) acc_u 9.3750 (15.6250) lr 1.3090e-03 eta 0:00:25
epoch [82/200] batch [10/62] time 0.383 (0.455) data 0.252 (0.325) loss_u loss_u 0.8418 (0.8572) acc_u 15.6250 (16.8750) lr 1.3090e-03 eta 0:00:23
epoch [82/200] batch [15/62] time 0.427 (0.457) data 0.296 (0.326) loss_u loss_u 0.8213 (0.8528) acc_u 28.1250 (17.7083) lr 1.3090e-03 eta 0:00:21
epoch [82/200] batch [20/62] time 0.455 (0.459) data 0.324 (0.329) loss_u loss_u 0.8677 (0.8525) acc_u 15.6250 (17.8125) lr 1.3090e-03 eta 0:00:19
epoch [82/200] batch [25/62] time 0.383 (0.458) data 0.253 (0.327) loss_u loss_u 0.7734 (0.8459) acc_u 31.2500 (18.7500) lr 1.3090e-03 eta 0:00:16
epoch [82/200] batch [30/62] time 0.500 (0.461) data 0.369 (0.330) loss_u loss_u 0.8271 (0.8439) acc_u 18.7500 (18.8542) lr 1.3090e-03 eta 0:00:14
epoch [82/200] batch [35/62] time 0.356 (0.461) data 0.224 (0.330) loss_u loss_u 0.8613 (0.8402) acc_u 21.8750 (19.6429) lr 1.3090e-03 eta 0:00:12
epoch [82/200] batch [40/62] time 0.503 (0.457) data 0.371 (0.326) loss_u loss_u 0.8286 (0.8408) acc_u 18.7500 (19.6094) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [45/62] time 0.394 (0.452) data 0.263 (0.321) loss_u loss_u 0.8394 (0.8459) acc_u 21.8750 (18.9583) lr 1.3090e-03 eta 0:00:07
epoch [82/200] batch [50/62] time 0.500 (0.455) data 0.369 (0.324) loss_u loss_u 0.8506 (0.8437) acc_u 18.7500 (19.2500) lr 1.3090e-03 eta 0:00:05
epoch [82/200] batch [55/62] time 0.510 (0.457) data 0.378 (0.326) loss_u loss_u 0.8149 (0.8442) acc_u 21.8750 (19.1477) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [60/62] time 0.399 (0.455) data 0.267 (0.324) loss_u loss_u 0.8291 (0.8429) acc_u 18.7500 (19.2188) lr 1.3090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1536
confident_label rate tensor(0.3769, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1182
clean true:1103
clean false:79
clean_rate:0.9331641285956007
noisy true:497
noisy false:1457
after delete: len(clean_dataset) 1182
after delete: len(noisy_dataset) 1954
epoch [83/200] batch [5/36] time 0.454 (0.415) data 0.323 (0.284) loss_x loss_x 0.9707 (1.3572) acc_x 71.8750 (68.1250) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [10/36] time 0.561 (0.444) data 0.430 (0.313) loss_x loss_x 1.3818 (1.2596) acc_x 65.6250 (70.0000) lr 1.2940e-03 eta 0:00:11
epoch [83/200] batch [15/36] time 0.568 (0.458) data 0.437 (0.327) loss_x loss_x 1.0791 (1.2928) acc_x 71.8750 (69.7917) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [20/36] time 0.516 (0.459) data 0.386 (0.328) loss_x loss_x 1.1055 (1.2248) acc_x 62.5000 (70.1562) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [25/36] time 0.407 (0.452) data 0.275 (0.321) loss_x loss_x 1.2832 (1.2115) acc_x 68.7500 (69.6250) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [30/36] time 0.485 (0.446) data 0.353 (0.315) loss_x loss_x 1.0938 (1.2091) acc_x 75.0000 (69.4792) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [35/36] time 0.553 (0.456) data 0.422 (0.325) loss_x loss_x 1.1445 (1.2069) acc_x 71.8750 (69.3750) lr 1.2940e-03 eta 0:00:00
epoch [83/200] batch [5/61] time 0.408 (0.454) data 0.276 (0.323) loss_u loss_u 0.7876 (0.8308) acc_u 21.8750 (18.1250) lr 1.2940e-03 eta 0:00:25
epoch [83/200] batch [10/61] time 0.545 (0.467) data 0.413 (0.335) loss_u loss_u 0.8325 (0.8212) acc_u 18.7500 (19.6875) lr 1.2940e-03 eta 0:00:23
epoch [83/200] batch [15/61] time 0.501 (0.469) data 0.369 (0.338) loss_u loss_u 0.8364 (0.8349) acc_u 28.1250 (19.7917) lr 1.2940e-03 eta 0:00:21
epoch [83/200] batch [20/61] time 0.349 (0.466) data 0.218 (0.334) loss_u loss_u 0.8887 (0.8416) acc_u 12.5000 (19.3750) lr 1.2940e-03 eta 0:00:19
epoch [83/200] batch [25/61] time 0.413 (0.468) data 0.282 (0.337) loss_u loss_u 0.8325 (0.8482) acc_u 18.7500 (18.7500) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [30/61] time 0.365 (0.464) data 0.235 (0.333) loss_u loss_u 0.8643 (0.8542) acc_u 18.7500 (17.7083) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [35/61] time 0.315 (0.463) data 0.184 (0.332) loss_u loss_u 0.8638 (0.8565) acc_u 18.7500 (17.5000) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [40/61] time 0.464 (0.460) data 0.332 (0.328) loss_u loss_u 0.9214 (0.8601) acc_u 12.5000 (17.2656) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [45/61] time 0.368 (0.459) data 0.237 (0.327) loss_u loss_u 0.8125 (0.8584) acc_u 28.1250 (17.9861) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [50/61] time 0.484 (0.455) data 0.353 (0.324) loss_u loss_u 0.8745 (0.8589) acc_u 15.6250 (17.8750) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [55/61] time 0.368 (0.453) data 0.238 (0.322) loss_u loss_u 0.7881 (0.8553) acc_u 25.0000 (18.4659) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [60/61] time 0.448 (0.451) data 0.316 (0.320) loss_u loss_u 0.8760 (0.8573) acc_u 18.7500 (18.1250) lr 1.2940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1500
confident_label rate tensor(0.3830, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1201
clean true:1131
clean false:70
clean_rate:0.9417152373022482
noisy true:505
noisy false:1430
after delete: len(clean_dataset) 1201
after delete: len(noisy_dataset) 1935
epoch [84/200] batch [5/37] time 0.478 (0.466) data 0.348 (0.335) loss_x loss_x 1.5537 (1.2646) acc_x 62.5000 (70.6250) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [10/37] time 0.355 (0.466) data 0.224 (0.336) loss_x loss_x 0.9316 (1.1954) acc_x 68.7500 (69.0625) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [15/37] time 0.393 (0.448) data 0.263 (0.318) loss_x loss_x 1.8750 (1.2797) acc_x 59.3750 (67.9167) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [20/37] time 0.568 (0.449) data 0.438 (0.319) loss_x loss_x 1.3066 (1.2637) acc_x 68.7500 (67.6562) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [25/37] time 0.476 (0.444) data 0.345 (0.313) loss_x loss_x 1.4541 (1.2691) acc_x 59.3750 (67.6250) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [30/37] time 0.486 (0.446) data 0.356 (0.316) loss_x loss_x 1.0957 (1.2566) acc_x 65.6250 (67.3958) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [35/37] time 0.447 (0.443) data 0.317 (0.313) loss_x loss_x 1.3047 (1.2503) acc_x 56.2500 (67.6786) lr 1.2790e-03 eta 0:00:00
epoch [84/200] batch [5/60] time 0.414 (0.450) data 0.283 (0.319) loss_u loss_u 0.8945 (0.8672) acc_u 12.5000 (16.8750) lr 1.2790e-03 eta 0:00:24
epoch [84/200] batch [10/60] time 0.478 (0.443) data 0.347 (0.313) loss_u loss_u 0.8994 (0.8642) acc_u 21.8750 (18.4375) lr 1.2790e-03 eta 0:00:22
epoch [84/200] batch [15/60] time 0.429 (0.442) data 0.298 (0.312) loss_u loss_u 0.8506 (0.8515) acc_u 18.7500 (20.0000) lr 1.2790e-03 eta 0:00:19
epoch [84/200] batch [20/60] time 0.539 (0.448) data 0.408 (0.318) loss_u loss_u 0.8647 (0.8580) acc_u 15.6250 (18.5938) lr 1.2790e-03 eta 0:00:17
epoch [84/200] batch [25/60] time 0.461 (0.446) data 0.330 (0.316) loss_u loss_u 0.9365 (0.8611) acc_u 9.3750 (17.6250) lr 1.2790e-03 eta 0:00:15
epoch [84/200] batch [30/60] time 0.366 (0.443) data 0.235 (0.312) loss_u loss_u 0.8584 (0.8641) acc_u 15.6250 (16.8750) lr 1.2790e-03 eta 0:00:13
epoch [84/200] batch [35/60] time 0.518 (0.446) data 0.387 (0.315) loss_u loss_u 0.8188 (0.8636) acc_u 25.0000 (16.7857) lr 1.2790e-03 eta 0:00:11
epoch [84/200] batch [40/60] time 0.408 (0.447) data 0.277 (0.316) loss_u loss_u 0.8691 (0.8585) acc_u 15.6250 (17.1875) lr 1.2790e-03 eta 0:00:08
epoch [84/200] batch [45/60] time 0.553 (0.449) data 0.420 (0.318) loss_u loss_u 0.8276 (0.8606) acc_u 21.8750 (16.9444) lr 1.2790e-03 eta 0:00:06
epoch [84/200] batch [50/60] time 0.513 (0.450) data 0.382 (0.319) loss_u loss_u 0.8198 (0.8590) acc_u 28.1250 (17.2500) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [55/60] time 0.457 (0.448) data 0.326 (0.317) loss_u loss_u 0.8872 (0.8590) acc_u 18.7500 (17.4432) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [60/60] time 0.365 (0.449) data 0.235 (0.318) loss_u loss_u 0.8906 (0.8592) acc_u 18.7500 (17.5521) lr 1.2790e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1503
confident_label rate tensor(0.3820, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1198
clean true:1113
clean false:85
clean_rate:0.9290484140233722
noisy true:520
noisy false:1418
after delete: len(clean_dataset) 1198
after delete: len(noisy_dataset) 1938
epoch [85/200] batch [5/37] time 0.440 (0.443) data 0.309 (0.312) loss_x loss_x 0.7505 (1.1821) acc_x 78.1250 (67.5000) lr 1.2639e-03 eta 0:00:14
epoch [85/200] batch [10/37] time 0.665 (0.469) data 0.534 (0.338) loss_x loss_x 1.2988 (1.2867) acc_x 68.7500 (66.5625) lr 1.2639e-03 eta 0:00:12
epoch [85/200] batch [15/37] time 0.433 (0.452) data 0.302 (0.321) loss_x loss_x 0.9209 (1.2970) acc_x 84.3750 (67.9167) lr 1.2639e-03 eta 0:00:09
epoch [85/200] batch [20/37] time 0.556 (0.465) data 0.424 (0.334) loss_x loss_x 1.2959 (1.3375) acc_x 65.6250 (66.5625) lr 1.2639e-03 eta 0:00:07
epoch [85/200] batch [25/37] time 0.501 (0.479) data 0.371 (0.348) loss_x loss_x 0.9473 (1.2888) acc_x 71.8750 (68.0000) lr 1.2639e-03 eta 0:00:05
epoch [85/200] batch [30/37] time 0.469 (0.472) data 0.338 (0.341) loss_x loss_x 1.1621 (1.2928) acc_x 68.7500 (67.5000) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [35/37] time 0.544 (0.475) data 0.413 (0.344) loss_x loss_x 1.1904 (1.2608) acc_x 71.8750 (68.4821) lr 1.2639e-03 eta 0:00:00
epoch [85/200] batch [5/60] time 0.480 (0.466) data 0.349 (0.336) loss_u loss_u 0.8555 (0.8707) acc_u 25.0000 (16.8750) lr 1.2639e-03 eta 0:00:25
epoch [85/200] batch [10/60] time 0.433 (0.463) data 0.302 (0.332) loss_u loss_u 0.8472 (0.8642) acc_u 21.8750 (18.1250) lr 1.2639e-03 eta 0:00:23
epoch [85/200] batch [15/60] time 0.615 (0.458) data 0.484 (0.327) loss_u loss_u 0.8760 (0.8586) acc_u 15.6250 (17.7083) lr 1.2639e-03 eta 0:00:20
epoch [85/200] batch [20/60] time 0.441 (0.466) data 0.309 (0.335) loss_u loss_u 0.8530 (0.8516) acc_u 15.6250 (18.5938) lr 1.2639e-03 eta 0:00:18
epoch [85/200] batch [25/60] time 0.399 (0.461) data 0.268 (0.331) loss_u loss_u 0.8604 (0.8480) acc_u 18.7500 (19.2500) lr 1.2639e-03 eta 0:00:16
epoch [85/200] batch [30/60] time 0.481 (0.459) data 0.350 (0.328) loss_u loss_u 0.8198 (0.8469) acc_u 21.8750 (19.2708) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [35/60] time 0.364 (0.459) data 0.232 (0.328) loss_u loss_u 0.9062 (0.8504) acc_u 15.6250 (18.8393) lr 1.2639e-03 eta 0:00:11
epoch [85/200] batch [40/60] time 0.442 (0.458) data 0.311 (0.327) loss_u loss_u 0.8638 (0.8506) acc_u 18.7500 (18.9062) lr 1.2639e-03 eta 0:00:09
epoch [85/200] batch [45/60] time 0.392 (0.453) data 0.260 (0.322) loss_u loss_u 0.8677 (0.8523) acc_u 15.6250 (18.5417) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [50/60] time 0.446 (0.450) data 0.314 (0.319) loss_u loss_u 0.8110 (0.8536) acc_u 25.0000 (18.5625) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [55/60] time 0.457 (0.448) data 0.325 (0.317) loss_u loss_u 0.9014 (0.8558) acc_u 9.3750 (18.1250) lr 1.2639e-03 eta 0:00:02
epoch [85/200] batch [60/60] time 0.614 (0.451) data 0.480 (0.320) loss_u loss_u 0.9292 (0.8570) acc_u 6.2500 (17.7604) lr 1.2639e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.3705, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1162
clean true:1090
clean false:72
clean_rate:0.9380378657487092
noisy true:529
noisy false:1445
after delete: len(clean_dataset) 1162
after delete: len(noisy_dataset) 1974
epoch [86/200] batch [5/36] time 0.560 (0.426) data 0.429 (0.295) loss_x loss_x 1.0830 (1.3391) acc_x 62.5000 (62.5000) lr 1.2487e-03 eta 0:00:13
epoch [86/200] batch [10/36] time 0.482 (0.432) data 0.351 (0.301) loss_x loss_x 1.7988 (1.2974) acc_x 50.0000 (65.0000) lr 1.2487e-03 eta 0:00:11
epoch [86/200] batch [15/36] time 0.448 (0.438) data 0.317 (0.307) loss_x loss_x 1.5820 (1.2364) acc_x 59.3750 (67.0833) lr 1.2487e-03 eta 0:00:09
epoch [86/200] batch [20/36] time 0.455 (0.451) data 0.324 (0.320) loss_x loss_x 1.1543 (1.2033) acc_x 75.0000 (68.4375) lr 1.2487e-03 eta 0:00:07
epoch [86/200] batch [25/36] time 0.445 (0.449) data 0.316 (0.319) loss_x loss_x 1.7090 (1.2085) acc_x 56.2500 (67.7500) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [30/36] time 0.397 (0.447) data 0.267 (0.316) loss_x loss_x 1.5312 (1.2242) acc_x 62.5000 (67.1875) lr 1.2487e-03 eta 0:00:02
epoch [86/200] batch [35/36] time 0.460 (0.450) data 0.330 (0.319) loss_x loss_x 0.9189 (1.2264) acc_x 78.1250 (67.5893) lr 1.2487e-03 eta 0:00:00
epoch [86/200] batch [5/61] time 0.410 (0.449) data 0.278 (0.319) loss_u loss_u 0.7822 (0.8354) acc_u 25.0000 (22.5000) lr 1.2487e-03 eta 0:00:25
epoch [86/200] batch [10/61] time 0.589 (0.449) data 0.457 (0.318) loss_u loss_u 0.9053 (0.8362) acc_u 12.5000 (21.8750) lr 1.2487e-03 eta 0:00:22
epoch [86/200] batch [15/61] time 0.470 (0.446) data 0.338 (0.315) loss_u loss_u 0.7388 (0.8374) acc_u 31.2500 (20.8333) lr 1.2487e-03 eta 0:00:20
epoch [86/200] batch [20/61] time 0.584 (0.455) data 0.451 (0.324) loss_u loss_u 0.8501 (0.8381) acc_u 18.7500 (20.7812) lr 1.2487e-03 eta 0:00:18
epoch [86/200] batch [25/61] time 0.516 (0.459) data 0.385 (0.328) loss_u loss_u 0.8818 (0.8379) acc_u 9.3750 (20.3750) lr 1.2487e-03 eta 0:00:16
epoch [86/200] batch [30/61] time 0.390 (0.456) data 0.258 (0.325) loss_u loss_u 0.8672 (0.8356) acc_u 12.5000 (20.5208) lr 1.2487e-03 eta 0:00:14
epoch [86/200] batch [35/61] time 0.424 (0.454) data 0.292 (0.323) loss_u loss_u 0.8320 (0.8372) acc_u 18.7500 (20.2679) lr 1.2487e-03 eta 0:00:11
epoch [86/200] batch [40/61] time 0.424 (0.453) data 0.293 (0.322) loss_u loss_u 0.8086 (0.8375) acc_u 28.1250 (20.3125) lr 1.2487e-03 eta 0:00:09
epoch [86/200] batch [45/61] time 0.453 (0.452) data 0.322 (0.321) loss_u loss_u 0.9165 (0.8419) acc_u 15.6250 (19.6528) lr 1.2487e-03 eta 0:00:07
epoch [86/200] batch [50/61] time 0.482 (0.451) data 0.350 (0.319) loss_u loss_u 0.9126 (0.8427) acc_u 6.2500 (19.1875) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [55/61] time 0.491 (0.448) data 0.359 (0.317) loss_u loss_u 0.8145 (0.8465) acc_u 25.0000 (19.0341) lr 1.2487e-03 eta 0:00:02
epoch [86/200] batch [60/61] time 0.339 (0.446) data 0.208 (0.315) loss_u loss_u 0.9224 (0.8467) acc_u 12.5000 (19.1667) lr 1.2487e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1525
confident_label rate tensor(0.3763, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1180
clean true:1099
clean false:81
clean_rate:0.9313559322033899
noisy true:512
noisy false:1444
after delete: len(clean_dataset) 1180
after delete: len(noisy_dataset) 1956
epoch [87/200] batch [5/36] time 0.421 (0.454) data 0.290 (0.322) loss_x loss_x 1.2295 (1.2598) acc_x 68.7500 (63.7500) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [10/36] time 0.576 (0.456) data 0.445 (0.325) loss_x loss_x 1.1758 (1.2945) acc_x 65.6250 (65.6250) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [15/36] time 0.439 (0.452) data 0.307 (0.320) loss_x loss_x 1.3096 (1.2459) acc_x 71.8750 (67.7083) lr 1.2334e-03 eta 0:00:09
epoch [87/200] batch [20/36] time 0.477 (0.473) data 0.346 (0.342) loss_x loss_x 0.8218 (1.2146) acc_x 62.5000 (68.2812) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [25/36] time 0.399 (0.470) data 0.268 (0.339) loss_x loss_x 0.5562 (1.1960) acc_x 84.3750 (69.0000) lr 1.2334e-03 eta 0:00:05
epoch [87/200] batch [30/36] time 0.608 (0.470) data 0.478 (0.339) loss_x loss_x 1.5771 (1.1931) acc_x 65.6250 (69.5833) lr 1.2334e-03 eta 0:00:02
epoch [87/200] batch [35/36] time 0.673 (0.476) data 0.543 (0.345) loss_x loss_x 1.1143 (1.1948) acc_x 71.8750 (69.9107) lr 1.2334e-03 eta 0:00:00
epoch [87/200] batch [5/61] time 0.449 (0.467) data 0.317 (0.336) loss_u loss_u 0.9497 (0.9045) acc_u 6.2500 (12.5000) lr 1.2334e-03 eta 0:00:26
epoch [87/200] batch [10/61] time 0.414 (0.465) data 0.283 (0.334) loss_u loss_u 0.8662 (0.8786) acc_u 15.6250 (15.3125) lr 1.2334e-03 eta 0:00:23
epoch [87/200] batch [15/61] time 0.387 (0.469) data 0.256 (0.338) loss_u loss_u 0.8501 (0.8660) acc_u 15.6250 (17.2917) lr 1.2334e-03 eta 0:00:21
epoch [87/200] batch [20/61] time 0.486 (0.464) data 0.355 (0.333) loss_u loss_u 0.8271 (0.8563) acc_u 31.2500 (19.3750) lr 1.2334e-03 eta 0:00:19
epoch [87/200] batch [25/61] time 0.437 (0.461) data 0.307 (0.330) loss_u loss_u 0.9116 (0.8566) acc_u 9.3750 (18.7500) lr 1.2334e-03 eta 0:00:16
epoch [87/200] batch [30/61] time 0.408 (0.457) data 0.276 (0.326) loss_u loss_u 0.8291 (0.8481) acc_u 25.0000 (19.5833) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [35/61] time 0.385 (0.457) data 0.253 (0.326) loss_u loss_u 0.8872 (0.8556) acc_u 15.6250 (18.5714) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [40/61] time 0.393 (0.452) data 0.262 (0.321) loss_u loss_u 0.9355 (0.8592) acc_u 6.2500 (17.8906) lr 1.2334e-03 eta 0:00:09
epoch [87/200] batch [45/61] time 0.395 (0.451) data 0.264 (0.320) loss_u loss_u 0.8618 (0.8562) acc_u 18.7500 (18.5417) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [50/61] time 0.367 (0.450) data 0.236 (0.319) loss_u loss_u 0.8770 (0.8522) acc_u 15.6250 (18.9375) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [55/61] time 0.358 (0.449) data 0.227 (0.318) loss_u loss_u 0.8164 (0.8518) acc_u 31.2500 (19.2614) lr 1.2334e-03 eta 0:00:02
epoch [87/200] batch [60/61] time 0.550 (0.452) data 0.419 (0.321) loss_u loss_u 0.8086 (0.8493) acc_u 21.8750 (19.4271) lr 1.2334e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1529
confident_label rate tensor(0.3779, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1185
clean true:1103
clean false:82
clean_rate:0.9308016877637131
noisy true:504
noisy false:1447
after delete: len(clean_dataset) 1185
after delete: len(noisy_dataset) 1951
epoch [88/200] batch [5/37] time 0.441 (0.453) data 0.310 (0.322) loss_x loss_x 0.9160 (1.1473) acc_x 75.0000 (70.6250) lr 1.2181e-03 eta 0:00:14
epoch [88/200] batch [10/37] time 0.615 (0.476) data 0.484 (0.345) loss_x loss_x 1.7158 (1.1999) acc_x 68.7500 (70.3125) lr 1.2181e-03 eta 0:00:12
epoch [88/200] batch [15/37] time 0.448 (0.468) data 0.318 (0.337) loss_x loss_x 1.0986 (1.2246) acc_x 62.5000 (70.2083) lr 1.2181e-03 eta 0:00:10
epoch [88/200] batch [20/37] time 0.432 (0.466) data 0.302 (0.335) loss_x loss_x 1.7051 (1.2460) acc_x 59.3750 (70.3125) lr 1.2181e-03 eta 0:00:07
epoch [88/200] batch [25/37] time 0.377 (0.461) data 0.246 (0.330) loss_x loss_x 1.1475 (1.2301) acc_x 68.7500 (69.6250) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [30/37] time 0.512 (0.461) data 0.381 (0.331) loss_x loss_x 1.4668 (1.2387) acc_x 59.3750 (69.7917) lr 1.2181e-03 eta 0:00:03
epoch [88/200] batch [35/37] time 0.518 (0.460) data 0.389 (0.330) loss_x loss_x 1.1553 (1.2323) acc_x 71.8750 (70.2679) lr 1.2181e-03 eta 0:00:00
epoch [88/200] batch [5/60] time 0.406 (0.459) data 0.276 (0.329) loss_u loss_u 0.8369 (0.8716) acc_u 18.7500 (15.6250) lr 1.2181e-03 eta 0:00:25
epoch [88/200] batch [10/60] time 0.365 (0.454) data 0.235 (0.323) loss_u loss_u 0.8315 (0.8563) acc_u 25.0000 (17.5000) lr 1.2181e-03 eta 0:00:22
epoch [88/200] batch [15/60] time 0.624 (0.457) data 0.491 (0.326) loss_u loss_u 0.8911 (0.8605) acc_u 9.3750 (16.8750) lr 1.2181e-03 eta 0:00:20
epoch [88/200] batch [20/60] time 0.468 (0.454) data 0.338 (0.324) loss_u loss_u 0.7954 (0.8587) acc_u 25.0000 (17.5000) lr 1.2181e-03 eta 0:00:18
epoch [88/200] batch [25/60] time 0.631 (0.456) data 0.499 (0.326) loss_u loss_u 0.8091 (0.8591) acc_u 18.7500 (17.1250) lr 1.2181e-03 eta 0:00:15
epoch [88/200] batch [30/60] time 0.432 (0.458) data 0.298 (0.327) loss_u loss_u 0.8350 (0.8522) acc_u 15.6250 (17.8125) lr 1.2181e-03 eta 0:00:13
epoch [88/200] batch [35/60] time 0.494 (0.462) data 0.363 (0.331) loss_u loss_u 0.8833 (0.8513) acc_u 9.3750 (17.5000) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [40/60] time 0.645 (0.464) data 0.514 (0.333) loss_u loss_u 0.8291 (0.8535) acc_u 12.5000 (16.8750) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [45/60] time 0.419 (0.463) data 0.287 (0.332) loss_u loss_u 0.7690 (0.8490) acc_u 37.5000 (17.7778) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [50/60] time 0.403 (0.460) data 0.270 (0.328) loss_u loss_u 0.8706 (0.8494) acc_u 15.6250 (17.9375) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [55/60] time 0.402 (0.458) data 0.270 (0.327) loss_u loss_u 0.7808 (0.8507) acc_u 28.1250 (18.0114) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [60/60] time 0.442 (0.462) data 0.310 (0.331) loss_u loss_u 0.8174 (0.8492) acc_u 21.8750 (18.1771) lr 1.2181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1511
confident_label rate tensor(0.3740, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1173
clean true:1101
clean false:72
clean_rate:0.9386189258312021
noisy true:524
noisy false:1439
after delete: len(clean_dataset) 1173
after delete: len(noisy_dataset) 1963
epoch [89/200] batch [5/36] time 0.409 (0.406) data 0.279 (0.276) loss_x loss_x 2.0137 (1.5209) acc_x 50.0000 (64.3750) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [10/36] time 0.520 (0.424) data 0.389 (0.294) loss_x loss_x 0.7949 (1.3720) acc_x 81.2500 (67.1875) lr 1.2028e-03 eta 0:00:11
epoch [89/200] batch [15/36] time 0.712 (0.455) data 0.581 (0.325) loss_x loss_x 0.9941 (1.3409) acc_x 78.1250 (67.2917) lr 1.2028e-03 eta 0:00:09
epoch [89/200] batch [20/36] time 0.475 (0.463) data 0.344 (0.333) loss_x loss_x 0.8672 (1.3063) acc_x 75.0000 (68.1250) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [25/36] time 0.437 (0.469) data 0.306 (0.338) loss_x loss_x 1.5166 (1.3414) acc_x 65.6250 (67.5000) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [30/36] time 0.527 (0.477) data 0.397 (0.346) loss_x loss_x 1.3252 (1.3099) acc_x 71.8750 (68.5417) lr 1.2028e-03 eta 0:00:02
epoch [89/200] batch [35/36] time 0.329 (0.468) data 0.198 (0.338) loss_x loss_x 1.2920 (1.3140) acc_x 65.6250 (68.0357) lr 1.2028e-03 eta 0:00:00
epoch [89/200] batch [5/61] time 0.444 (0.464) data 0.313 (0.334) loss_u loss_u 0.8101 (0.8398) acc_u 28.1250 (20.0000) lr 1.2028e-03 eta 0:00:26
epoch [89/200] batch [10/61] time 0.358 (0.462) data 0.228 (0.331) loss_u loss_u 0.7466 (0.8356) acc_u 37.5000 (20.9375) lr 1.2028e-03 eta 0:00:23
epoch [89/200] batch [15/61] time 0.478 (0.459) data 0.347 (0.329) loss_u loss_u 0.8057 (0.8432) acc_u 25.0000 (19.1667) lr 1.2028e-03 eta 0:00:21
epoch [89/200] batch [20/61] time 0.453 (0.454) data 0.323 (0.323) loss_u loss_u 0.8975 (0.8517) acc_u 12.5000 (18.4375) lr 1.2028e-03 eta 0:00:18
epoch [89/200] batch [25/61] time 0.417 (0.454) data 0.285 (0.324) loss_u loss_u 0.8320 (0.8488) acc_u 18.7500 (18.7500) lr 1.2028e-03 eta 0:00:16
epoch [89/200] batch [30/61] time 0.450 (0.454) data 0.318 (0.323) loss_u loss_u 0.8809 (0.8512) acc_u 12.5000 (18.3333) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [35/61] time 0.390 (0.450) data 0.259 (0.319) loss_u loss_u 0.8032 (0.8526) acc_u 31.2500 (18.4821) lr 1.2028e-03 eta 0:00:11
epoch [89/200] batch [40/61] time 0.491 (0.450) data 0.360 (0.319) loss_u loss_u 0.8862 (0.8561) acc_u 12.5000 (18.0469) lr 1.2028e-03 eta 0:00:09
epoch [89/200] batch [45/61] time 0.601 (0.450) data 0.471 (0.319) loss_u loss_u 0.8188 (0.8556) acc_u 25.0000 (18.2639) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [50/61] time 0.430 (0.445) data 0.299 (0.314) loss_u loss_u 0.9126 (0.8571) acc_u 9.3750 (17.8125) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [55/61] time 0.433 (0.447) data 0.302 (0.316) loss_u loss_u 0.8247 (0.8556) acc_u 15.6250 (17.8977) lr 1.2028e-03 eta 0:00:02
epoch [89/200] batch [60/61] time 0.491 (0.450) data 0.359 (0.319) loss_u loss_u 0.7324 (0.8515) acc_u 34.3750 (18.4896) lr 1.2028e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1505
confident_label rate tensor(0.3874, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1215
clean true:1131
clean false:84
clean_rate:0.9308641975308642
noisy true:500
noisy false:1421
after delete: len(clean_dataset) 1215
after delete: len(noisy_dataset) 1921
epoch [90/200] batch [5/37] time 0.543 (0.490) data 0.412 (0.359) loss_x loss_x 1.0752 (1.0251) acc_x 75.0000 (74.3750) lr 1.1874e-03 eta 0:00:15
epoch [90/200] batch [10/37] time 0.462 (0.508) data 0.331 (0.377) loss_x loss_x 1.1436 (1.0880) acc_x 75.0000 (75.6250) lr 1.1874e-03 eta 0:00:13
epoch [90/200] batch [15/37] time 0.506 (0.487) data 0.375 (0.357) loss_x loss_x 0.7534 (1.0924) acc_x 78.1250 (74.1667) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [20/37] time 0.549 (0.470) data 0.418 (0.340) loss_x loss_x 1.2451 (1.1183) acc_x 62.5000 (73.5938) lr 1.1874e-03 eta 0:00:07
epoch [90/200] batch [25/37] time 0.486 (0.476) data 0.356 (0.346) loss_x loss_x 0.7427 (1.1154) acc_x 87.5000 (73.2500) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [30/37] time 0.370 (0.472) data 0.240 (0.341) loss_x loss_x 1.2393 (1.1663) acc_x 71.8750 (72.0833) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [35/37] time 0.472 (0.472) data 0.342 (0.341) loss_x loss_x 0.9395 (1.1418) acc_x 81.2500 (72.6786) lr 1.1874e-03 eta 0:00:00
epoch [90/200] batch [5/60] time 0.349 (0.463) data 0.219 (0.333) loss_u loss_u 0.9229 (0.8692) acc_u 12.5000 (15.6250) lr 1.1874e-03 eta 0:00:25
epoch [90/200] batch [10/60] time 0.542 (0.457) data 0.411 (0.326) loss_u loss_u 0.8579 (0.8495) acc_u 18.7500 (19.6875) lr 1.1874e-03 eta 0:00:22
epoch [90/200] batch [15/60] time 0.490 (0.459) data 0.359 (0.329) loss_u loss_u 0.8140 (0.8472) acc_u 25.0000 (19.7917) lr 1.1874e-03 eta 0:00:20
epoch [90/200] batch [20/60] time 0.420 (0.460) data 0.288 (0.329) loss_u loss_u 0.8862 (0.8500) acc_u 15.6250 (19.2188) lr 1.1874e-03 eta 0:00:18
epoch [90/200] batch [25/60] time 0.560 (0.461) data 0.429 (0.330) loss_u loss_u 0.8965 (0.8518) acc_u 9.3750 (18.6250) lr 1.1874e-03 eta 0:00:16
epoch [90/200] batch [30/60] time 0.920 (0.469) data 0.787 (0.338) loss_u loss_u 0.8125 (0.8510) acc_u 18.7500 (18.4375) lr 1.1874e-03 eta 0:00:14
epoch [90/200] batch [35/60] time 0.406 (0.468) data 0.275 (0.338) loss_u loss_u 0.8413 (0.8526) acc_u 18.7500 (18.1250) lr 1.1874e-03 eta 0:00:11
epoch [90/200] batch [40/60] time 0.360 (0.470) data 0.228 (0.339) loss_u loss_u 0.7998 (0.8540) acc_u 25.0000 (18.1250) lr 1.1874e-03 eta 0:00:09
epoch [90/200] batch [45/60] time 0.507 (0.466) data 0.376 (0.335) loss_u loss_u 0.8291 (0.8527) acc_u 21.8750 (18.2639) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [50/60] time 0.398 (0.464) data 0.268 (0.333) loss_u loss_u 0.8774 (0.8549) acc_u 15.6250 (18.1250) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [55/60] time 0.389 (0.460) data 0.258 (0.329) loss_u loss_u 0.8452 (0.8564) acc_u 12.5000 (18.0114) lr 1.1874e-03 eta 0:00:02
epoch [90/200] batch [60/60] time 0.375 (0.458) data 0.245 (0.327) loss_u loss_u 0.8032 (0.8573) acc_u 31.2500 (17.9688) lr 1.1874e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1539
confident_label rate tensor(0.3731, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1170
clean true:1097
clean false:73
clean_rate:0.9376068376068376
noisy true:500
noisy false:1466
after delete: len(clean_dataset) 1170
after delete: len(noisy_dataset) 1966
epoch [91/200] batch [5/36] time 0.512 (0.485) data 0.381 (0.354) loss_x loss_x 1.0078 (1.0854) acc_x 78.1250 (72.5000) lr 1.1719e-03 eta 0:00:15
epoch [91/200] batch [10/36] time 0.596 (0.487) data 0.466 (0.356) loss_x loss_x 1.2344 (1.2218) acc_x 78.1250 (71.2500) lr 1.1719e-03 eta 0:00:12
epoch [91/200] batch [15/36] time 0.483 (0.480) data 0.352 (0.349) loss_x loss_x 1.1895 (1.2015) acc_x 68.7500 (70.6250) lr 1.1719e-03 eta 0:00:10
epoch [91/200] batch [20/36] time 0.386 (0.471) data 0.255 (0.340) loss_x loss_x 0.9917 (1.1855) acc_x 75.0000 (70.7812) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [25/36] time 0.496 (0.470) data 0.365 (0.339) loss_x loss_x 1.3730 (1.2160) acc_x 65.6250 (70.0000) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [30/36] time 0.499 (0.470) data 0.368 (0.339) loss_x loss_x 0.7827 (1.2106) acc_x 78.1250 (69.5833) lr 1.1719e-03 eta 0:00:02
epoch [91/200] batch [35/36] time 0.402 (0.460) data 0.272 (0.329) loss_x loss_x 1.6289 (1.2137) acc_x 62.5000 (69.5536) lr 1.1719e-03 eta 0:00:00
epoch [91/200] batch [5/61] time 0.394 (0.456) data 0.261 (0.325) loss_u loss_u 0.8657 (0.8457) acc_u 15.6250 (18.7500) lr 1.1719e-03 eta 0:00:25
epoch [91/200] batch [10/61] time 0.414 (0.453) data 0.283 (0.322) loss_u loss_u 0.8691 (0.8331) acc_u 15.6250 (19.6875) lr 1.1719e-03 eta 0:00:23
epoch [91/200] batch [15/61] time 0.459 (0.451) data 0.327 (0.320) loss_u loss_u 0.8242 (0.8425) acc_u 25.0000 (19.3750) lr 1.1719e-03 eta 0:00:20
epoch [91/200] batch [20/61] time 0.418 (0.454) data 0.286 (0.323) loss_u loss_u 0.7598 (0.8420) acc_u 34.3750 (19.3750) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [25/61] time 0.605 (0.453) data 0.473 (0.322) loss_u loss_u 0.8135 (0.8403) acc_u 28.1250 (19.7500) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [30/61] time 0.680 (0.457) data 0.548 (0.326) loss_u loss_u 0.8711 (0.8416) acc_u 12.5000 (19.0625) lr 1.1719e-03 eta 0:00:14
epoch [91/200] batch [35/61] time 0.395 (0.454) data 0.263 (0.322) loss_u loss_u 0.9097 (0.8437) acc_u 12.5000 (18.9286) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [40/61] time 0.475 (0.456) data 0.343 (0.324) loss_u loss_u 0.7344 (0.8442) acc_u 31.2500 (19.2188) lr 1.1719e-03 eta 0:00:09
epoch [91/200] batch [45/61] time 0.405 (0.456) data 0.272 (0.325) loss_u loss_u 0.8647 (0.8453) acc_u 15.6250 (19.0972) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [50/61] time 0.612 (0.456) data 0.480 (0.324) loss_u loss_u 0.8423 (0.8454) acc_u 21.8750 (19.2500) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [55/61] time 0.385 (0.457) data 0.253 (0.325) loss_u loss_u 0.8955 (0.8449) acc_u 9.3750 (19.1477) lr 1.1719e-03 eta 0:00:02
epoch [91/200] batch [60/61] time 0.395 (0.457) data 0.264 (0.326) loss_u loss_u 0.8862 (0.8437) acc_u 12.5000 (19.1667) lr 1.1719e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1491
confident_label rate tensor(0.3842, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1205
clean true:1123
clean false:82
clean_rate:0.9319502074688797
noisy true:522
noisy false:1409
after delete: len(clean_dataset) 1205
after delete: len(noisy_dataset) 1931
epoch [92/200] batch [5/37] time 0.533 (0.483) data 0.399 (0.352) loss_x loss_x 1.1396 (1.2123) acc_x 78.1250 (69.3750) lr 1.1564e-03 eta 0:00:15
epoch [92/200] batch [10/37] time 0.455 (0.490) data 0.324 (0.359) loss_x loss_x 0.7524 (1.1598) acc_x 81.2500 (70.3125) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [15/37] time 0.383 (0.473) data 0.252 (0.342) loss_x loss_x 1.4551 (1.2005) acc_x 59.3750 (69.1667) lr 1.1564e-03 eta 0:00:10
epoch [92/200] batch [20/37] time 0.427 (0.472) data 0.296 (0.341) loss_x loss_x 1.1201 (1.1979) acc_x 75.0000 (70.0000) lr 1.1564e-03 eta 0:00:08
epoch [92/200] batch [25/37] time 0.402 (0.466) data 0.271 (0.335) loss_x loss_x 0.7695 (1.2125) acc_x 81.2500 (70.3750) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [30/37] time 0.459 (0.457) data 0.328 (0.326) loss_x loss_x 1.1719 (1.2459) acc_x 75.0000 (69.6875) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [35/37] time 0.431 (0.456) data 0.300 (0.325) loss_x loss_x 1.4297 (1.2275) acc_x 68.7500 (70.0893) lr 1.1564e-03 eta 0:00:00
epoch [92/200] batch [5/60] time 0.456 (0.456) data 0.324 (0.325) loss_u loss_u 0.9458 (0.8633) acc_u 6.2500 (16.8750) lr 1.1564e-03 eta 0:00:25
epoch [92/200] batch [10/60] time 0.389 (0.449) data 0.257 (0.318) loss_u loss_u 0.8940 (0.8673) acc_u 12.5000 (16.8750) lr 1.1564e-03 eta 0:00:22
epoch [92/200] batch [15/60] time 0.351 (0.447) data 0.219 (0.315) loss_u loss_u 0.8228 (0.8633) acc_u 18.7500 (17.5000) lr 1.1564e-03 eta 0:00:20
epoch [92/200] batch [20/60] time 0.454 (0.447) data 0.323 (0.316) loss_u loss_u 0.8003 (0.8546) acc_u 25.0000 (18.7500) lr 1.1564e-03 eta 0:00:17
epoch [92/200] batch [25/60] time 0.528 (0.451) data 0.396 (0.320) loss_u loss_u 0.8408 (0.8558) acc_u 15.6250 (17.8750) lr 1.1564e-03 eta 0:00:15
epoch [92/200] batch [30/60] time 0.491 (0.459) data 0.360 (0.327) loss_u loss_u 0.8755 (0.8506) acc_u 12.5000 (18.4375) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [35/60] time 0.378 (0.457) data 0.247 (0.326) loss_u loss_u 0.8521 (0.8478) acc_u 18.7500 (18.9286) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [40/60] time 0.386 (0.453) data 0.254 (0.322) loss_u loss_u 0.8828 (0.8467) acc_u 9.3750 (19.2969) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [45/60] time 0.520 (0.451) data 0.389 (0.320) loss_u loss_u 0.9419 (0.8499) acc_u 3.1250 (18.8194) lr 1.1564e-03 eta 0:00:06
epoch [92/200] batch [50/60] time 0.418 (0.448) data 0.286 (0.317) loss_u loss_u 0.8750 (0.8515) acc_u 21.8750 (18.8125) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [55/60] time 0.371 (0.451) data 0.239 (0.319) loss_u loss_u 0.7935 (0.8510) acc_u 25.0000 (18.6932) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [60/60] time 0.433 (0.448) data 0.301 (0.316) loss_u loss_u 0.8345 (0.8508) acc_u 18.7500 (18.4896) lr 1.1564e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1488
confident_label rate tensor(0.3862, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1211
clean true:1131
clean false:80
clean_rate:0.9339388934764657
noisy true:517
noisy false:1408
after delete: len(clean_dataset) 1211
after delete: len(noisy_dataset) 1925
epoch [93/200] batch [5/37] time 0.416 (0.455) data 0.285 (0.325) loss_x loss_x 1.5098 (1.2825) acc_x 62.5000 (71.2500) lr 1.1409e-03 eta 0:00:14
epoch [93/200] batch [10/37] time 0.358 (0.427) data 0.227 (0.297) loss_x loss_x 0.8218 (1.1592) acc_x 78.1250 (72.1875) lr 1.1409e-03 eta 0:00:11
epoch [93/200] batch [15/37] time 0.538 (0.457) data 0.408 (0.327) loss_x loss_x 1.8086 (1.2735) acc_x 68.7500 (70.0000) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [20/37] time 0.385 (0.471) data 0.255 (0.340) loss_x loss_x 1.1748 (1.2194) acc_x 62.5000 (69.6875) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [25/37] time 0.389 (0.466) data 0.259 (0.335) loss_x loss_x 1.8594 (1.2380) acc_x 56.2500 (68.5000) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [30/37] time 0.417 (0.461) data 0.286 (0.330) loss_x loss_x 1.0713 (1.2179) acc_x 71.8750 (68.7500) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [35/37] time 0.538 (0.462) data 0.407 (0.331) loss_x loss_x 1.3779 (1.2152) acc_x 62.5000 (68.3929) lr 1.1409e-03 eta 0:00:00
epoch [93/200] batch [5/60] time 0.509 (0.465) data 0.378 (0.335) loss_u loss_u 0.9175 (0.8536) acc_u 9.3750 (15.6250) lr 1.1409e-03 eta 0:00:25
epoch [93/200] batch [10/60] time 0.461 (0.465) data 0.329 (0.335) loss_u loss_u 0.8726 (0.8601) acc_u 12.5000 (14.0625) lr 1.1409e-03 eta 0:00:23
epoch [93/200] batch [15/60] time 0.342 (0.459) data 0.211 (0.328) loss_u loss_u 0.8291 (0.8556) acc_u 25.0000 (15.6250) lr 1.1409e-03 eta 0:00:20
epoch [93/200] batch [20/60] time 0.469 (0.454) data 0.337 (0.323) loss_u loss_u 0.9097 (0.8509) acc_u 9.3750 (16.7188) lr 1.1409e-03 eta 0:00:18
epoch [93/200] batch [25/60] time 0.404 (0.453) data 0.273 (0.322) loss_u loss_u 0.8560 (0.8485) acc_u 18.7500 (17.3750) lr 1.1409e-03 eta 0:00:15
epoch [93/200] batch [30/60] time 0.411 (0.447) data 0.279 (0.316) loss_u loss_u 0.8145 (0.8480) acc_u 28.1250 (18.3333) lr 1.1409e-03 eta 0:00:13
epoch [93/200] batch [35/60] time 0.429 (0.448) data 0.298 (0.317) loss_u loss_u 0.8853 (0.8542) acc_u 25.0000 (17.8571) lr 1.1409e-03 eta 0:00:11
epoch [93/200] batch [40/60] time 0.449 (0.451) data 0.317 (0.320) loss_u loss_u 0.8906 (0.8561) acc_u 15.6250 (17.7344) lr 1.1409e-03 eta 0:00:09
epoch [93/200] batch [45/60] time 0.396 (0.449) data 0.265 (0.317) loss_u loss_u 0.8818 (0.8565) acc_u 12.5000 (17.6389) lr 1.1409e-03 eta 0:00:06
epoch [93/200] batch [50/60] time 0.562 (0.449) data 0.431 (0.318) loss_u loss_u 0.9365 (0.8558) acc_u 9.3750 (17.5625) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [55/60] time 0.438 (0.452) data 0.306 (0.321) loss_u loss_u 0.8491 (0.8540) acc_u 25.0000 (17.9545) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [60/60] time 0.441 (0.452) data 0.309 (0.321) loss_u loss_u 0.8452 (0.8542) acc_u 15.6250 (17.7083) lr 1.1409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1502
confident_label rate tensor(0.3817, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1197
clean true:1116
clean false:81
clean_rate:0.9323308270676691
noisy true:518
noisy false:1421
after delete: len(clean_dataset) 1197
after delete: len(noisy_dataset) 1939
epoch [94/200] batch [5/37] time 0.446 (0.450) data 0.315 (0.319) loss_x loss_x 0.9507 (1.2105) acc_x 65.6250 (66.8750) lr 1.1253e-03 eta 0:00:14
epoch [94/200] batch [10/37] time 0.372 (0.441) data 0.241 (0.310) loss_x loss_x 1.2285 (1.1691) acc_x 68.7500 (68.4375) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [15/37] time 0.414 (0.432) data 0.283 (0.301) loss_x loss_x 1.4355 (1.1561) acc_x 53.1250 (67.9167) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [20/37] time 0.560 (0.444) data 0.429 (0.313) loss_x loss_x 1.2529 (1.1050) acc_x 75.0000 (69.8438) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [25/37] time 0.400 (0.440) data 0.268 (0.309) loss_x loss_x 0.7598 (1.1210) acc_x 71.8750 (69.7500) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [30/37] time 0.427 (0.444) data 0.296 (0.313) loss_x loss_x 0.9282 (1.1447) acc_x 78.1250 (69.8958) lr 1.1253e-03 eta 0:00:03
epoch [94/200] batch [35/37] time 0.403 (0.446) data 0.272 (0.315) loss_x loss_x 1.4805 (1.1638) acc_x 59.3750 (69.7321) lr 1.1253e-03 eta 0:00:00
epoch [94/200] batch [5/60] time 0.502 (0.445) data 0.371 (0.314) loss_u loss_u 0.8618 (0.8614) acc_u 15.6250 (18.7500) lr 1.1253e-03 eta 0:00:24
epoch [94/200] batch [10/60] time 0.429 (0.442) data 0.298 (0.311) loss_u loss_u 0.8799 (0.8799) acc_u 9.3750 (15.0000) lr 1.1253e-03 eta 0:00:22
epoch [94/200] batch [15/60] time 0.416 (0.450) data 0.280 (0.318) loss_u loss_u 0.8467 (0.8726) acc_u 15.6250 (15.6250) lr 1.1253e-03 eta 0:00:20
epoch [94/200] batch [20/60] time 0.508 (0.454) data 0.378 (0.323) loss_u loss_u 0.9146 (0.8695) acc_u 6.2500 (15.3125) lr 1.1253e-03 eta 0:00:18
epoch [94/200] batch [25/60] time 0.410 (0.457) data 0.278 (0.325) loss_u loss_u 0.9102 (0.8672) acc_u 6.2500 (15.6250) lr 1.1253e-03 eta 0:00:15
epoch [94/200] batch [30/60] time 0.552 (0.458) data 0.421 (0.326) loss_u loss_u 0.7896 (0.8635) acc_u 21.8750 (16.5625) lr 1.1253e-03 eta 0:00:13
epoch [94/200] batch [35/60] time 0.532 (0.457) data 0.401 (0.325) loss_u loss_u 0.8545 (0.8567) acc_u 25.0000 (17.6786) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [40/60] time 0.449 (0.456) data 0.319 (0.325) loss_u loss_u 0.8257 (0.8529) acc_u 31.2500 (18.3594) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [45/60] time 0.549 (0.458) data 0.419 (0.327) loss_u loss_u 0.8354 (0.8542) acc_u 18.7500 (17.9861) lr 1.1253e-03 eta 0:00:06
epoch [94/200] batch [50/60] time 0.381 (0.455) data 0.250 (0.323) loss_u loss_u 0.9009 (0.8570) acc_u 18.7500 (17.7500) lr 1.1253e-03 eta 0:00:04
epoch [94/200] batch [55/60] time 0.374 (0.450) data 0.242 (0.319) loss_u loss_u 0.8574 (0.8564) acc_u 15.6250 (17.5568) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [60/60] time 0.608 (0.451) data 0.477 (0.320) loss_u loss_u 0.7671 (0.8557) acc_u 25.0000 (17.4479) lr 1.1253e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1510
confident_label rate tensor(0.3801, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1192
clean true:1119
clean false:73
clean_rate:0.9387583892617449
noisy true:507
noisy false:1437
after delete: len(clean_dataset) 1192
after delete: len(noisy_dataset) 1944
epoch [95/200] batch [5/37] time 0.630 (0.506) data 0.499 (0.374) loss_x loss_x 1.0312 (1.1771) acc_x 65.6250 (68.7500) lr 1.1097e-03 eta 0:00:16
epoch [95/200] batch [10/37] time 0.402 (0.507) data 0.271 (0.375) loss_x loss_x 1.2607 (1.2535) acc_x 68.7500 (66.8750) lr 1.1097e-03 eta 0:00:13
epoch [95/200] batch [15/37] time 0.530 (0.516) data 0.399 (0.385) loss_x loss_x 0.8481 (1.2186) acc_x 78.1250 (69.1667) lr 1.1097e-03 eta 0:00:11
epoch [95/200] batch [20/37] time 0.399 (0.483) data 0.267 (0.351) loss_x loss_x 1.2246 (1.2282) acc_x 75.0000 (69.5312) lr 1.1097e-03 eta 0:00:08
epoch [95/200] batch [25/37] time 0.484 (0.467) data 0.353 (0.336) loss_x loss_x 0.8921 (1.2103) acc_x 84.3750 (69.8750) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [30/37] time 0.470 (0.472) data 0.339 (0.340) loss_x loss_x 1.1152 (1.1899) acc_x 71.8750 (70.0000) lr 1.1097e-03 eta 0:00:03
epoch [95/200] batch [35/37] time 0.446 (0.469) data 0.315 (0.338) loss_x loss_x 1.7617 (1.1982) acc_x 56.2500 (69.0179) lr 1.1097e-03 eta 0:00:00
epoch [95/200] batch [5/60] time 0.492 (0.469) data 0.361 (0.338) loss_u loss_u 0.8242 (0.8758) acc_u 21.8750 (14.3750) lr 1.1097e-03 eta 0:00:25
epoch [95/200] batch [10/60] time 0.390 (0.465) data 0.258 (0.334) loss_u loss_u 0.8750 (0.8760) acc_u 25.0000 (14.6875) lr 1.1097e-03 eta 0:00:23
epoch [95/200] batch [15/60] time 0.341 (0.462) data 0.209 (0.331) loss_u loss_u 0.8384 (0.8686) acc_u 21.8750 (15.8333) lr 1.1097e-03 eta 0:00:20
epoch [95/200] batch [20/60] time 0.392 (0.457) data 0.260 (0.326) loss_u loss_u 0.8525 (0.8617) acc_u 12.5000 (16.2500) lr 1.1097e-03 eta 0:00:18
epoch [95/200] batch [25/60] time 0.450 (0.458) data 0.318 (0.327) loss_u loss_u 0.8760 (0.8589) acc_u 15.6250 (17.1250) lr 1.1097e-03 eta 0:00:16
epoch [95/200] batch [30/60] time 0.480 (0.457) data 0.348 (0.325) loss_u loss_u 0.8530 (0.8579) acc_u 15.6250 (16.8750) lr 1.1097e-03 eta 0:00:13
epoch [95/200] batch [35/60] time 0.560 (0.458) data 0.427 (0.326) loss_u loss_u 0.8662 (0.8566) acc_u 18.7500 (17.4107) lr 1.1097e-03 eta 0:00:11
epoch [95/200] batch [40/60] time 0.437 (0.457) data 0.306 (0.326) loss_u loss_u 0.9229 (0.8584) acc_u 12.5000 (17.4219) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [45/60] time 0.536 (0.459) data 0.403 (0.328) loss_u loss_u 0.7188 (0.8545) acc_u 34.3750 (17.8472) lr 1.1097e-03 eta 0:00:06
epoch [95/200] batch [50/60] time 0.374 (0.459) data 0.242 (0.327) loss_u loss_u 0.8828 (0.8565) acc_u 9.3750 (17.4375) lr 1.1097e-03 eta 0:00:04
epoch [95/200] batch [55/60] time 0.414 (0.455) data 0.282 (0.323) loss_u loss_u 0.7979 (0.8540) acc_u 25.0000 (17.6705) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [60/60] time 0.502 (0.454) data 0.371 (0.322) loss_u loss_u 0.8379 (0.8541) acc_u 18.7500 (17.6562) lr 1.1097e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1469
confident_label rate tensor(0.3814, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1196
clean true:1133
clean false:63
clean_rate:0.947324414715719
noisy true:534
noisy false:1406
after delete: len(clean_dataset) 1196
after delete: len(noisy_dataset) 1940
epoch [96/200] batch [5/37] time 0.478 (0.432) data 0.347 (0.301) loss_x loss_x 1.0479 (1.1452) acc_x 62.5000 (70.0000) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [10/37] time 0.372 (0.436) data 0.242 (0.305) loss_x loss_x 1.7432 (1.2833) acc_x 59.3750 (65.9375) lr 1.0941e-03 eta 0:00:11
epoch [96/200] batch [15/37] time 0.377 (0.448) data 0.245 (0.317) loss_x loss_x 1.1934 (1.2589) acc_x 65.6250 (66.6667) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [20/37] time 0.452 (0.464) data 0.321 (0.333) loss_x loss_x 1.3525 (1.2845) acc_x 68.7500 (66.0938) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [25/37] time 0.511 (0.463) data 0.380 (0.332) loss_x loss_x 1.0967 (1.2741) acc_x 75.0000 (66.6250) lr 1.0941e-03 eta 0:00:05
epoch [96/200] batch [30/37] time 0.416 (0.464) data 0.285 (0.333) loss_x loss_x 1.0117 (1.2372) acc_x 78.1250 (68.3333) lr 1.0941e-03 eta 0:00:03
epoch [96/200] batch [35/37] time 0.506 (0.467) data 0.376 (0.336) loss_x loss_x 0.8779 (1.2173) acc_x 71.8750 (68.7500) lr 1.0941e-03 eta 0:00:00
epoch [96/200] batch [5/60] time 0.436 (0.462) data 0.303 (0.331) loss_u loss_u 0.7944 (0.8409) acc_u 25.0000 (18.1250) lr 1.0941e-03 eta 0:00:25
epoch [96/200] batch [10/60] time 0.536 (0.460) data 0.404 (0.329) loss_u loss_u 0.8594 (0.8551) acc_u 18.7500 (17.5000) lr 1.0941e-03 eta 0:00:22
epoch [96/200] batch [15/60] time 0.327 (0.458) data 0.196 (0.327) loss_u loss_u 0.8569 (0.8481) acc_u 15.6250 (18.1250) lr 1.0941e-03 eta 0:00:20
epoch [96/200] batch [20/60] time 0.387 (0.455) data 0.254 (0.324) loss_u loss_u 0.8325 (0.8443) acc_u 18.7500 (18.1250) lr 1.0941e-03 eta 0:00:18
epoch [96/200] batch [25/60] time 0.621 (0.458) data 0.490 (0.326) loss_u loss_u 0.8608 (0.8431) acc_u 15.6250 (19.0000) lr 1.0941e-03 eta 0:00:16
epoch [96/200] batch [30/60] time 0.611 (0.458) data 0.480 (0.326) loss_u loss_u 0.8623 (0.8451) acc_u 15.6250 (18.8542) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [35/60] time 0.328 (0.458) data 0.197 (0.327) loss_u loss_u 0.8291 (0.8464) acc_u 25.0000 (18.6607) lr 1.0941e-03 eta 0:00:11
epoch [96/200] batch [40/60] time 0.443 (0.454) data 0.312 (0.323) loss_u loss_u 0.7148 (0.8451) acc_u 37.5000 (18.9844) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [45/60] time 0.398 (0.455) data 0.268 (0.324) loss_u loss_u 0.8110 (0.8459) acc_u 31.2500 (19.4444) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [50/60] time 0.569 (0.453) data 0.437 (0.322) loss_u loss_u 0.8594 (0.8473) acc_u 18.7500 (19.1875) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [55/60] time 0.610 (0.454) data 0.479 (0.323) loss_u loss_u 0.8535 (0.8495) acc_u 18.7500 (18.7500) lr 1.0941e-03 eta 0:00:02
epoch [96/200] batch [60/60] time 0.413 (0.453) data 0.281 (0.321) loss_u loss_u 0.8862 (0.8518) acc_u 15.6250 (18.4375) lr 1.0941e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1481
confident_label rate tensor(0.3798, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1191
clean true:1116
clean false:75
clean_rate:0.9370277078085643
noisy true:539
noisy false:1406
after delete: len(clean_dataset) 1191
after delete: len(noisy_dataset) 1945
epoch [97/200] batch [5/37] time 0.395 (0.470) data 0.263 (0.339) loss_x loss_x 1.5137 (1.2402) acc_x 65.6250 (67.5000) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [10/37] time 0.341 (0.485) data 0.210 (0.353) loss_x loss_x 1.0908 (1.1622) acc_x 71.8750 (69.3750) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [15/37] time 0.383 (0.472) data 0.252 (0.341) loss_x loss_x 1.4170 (1.2415) acc_x 71.8750 (68.7500) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [20/37] time 0.475 (0.470) data 0.343 (0.339) loss_x loss_x 1.3838 (1.1915) acc_x 75.0000 (70.4688) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [25/37] time 0.439 (0.462) data 0.308 (0.331) loss_x loss_x 2.0488 (1.2591) acc_x 59.3750 (69.3750) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [30/37] time 0.469 (0.459) data 0.338 (0.328) loss_x loss_x 1.0771 (1.2033) acc_x 75.0000 (71.0417) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [35/37] time 0.449 (0.459) data 0.318 (0.328) loss_x loss_x 1.0664 (1.1880) acc_x 68.7500 (70.2679) lr 1.0785e-03 eta 0:00:00
epoch [97/200] batch [5/60] time 0.460 (0.463) data 0.329 (0.332) loss_u loss_u 0.8896 (0.8386) acc_u 15.6250 (18.7500) lr 1.0785e-03 eta 0:00:25
epoch [97/200] batch [10/60] time 0.443 (0.459) data 0.312 (0.328) loss_u loss_u 0.9185 (0.8463) acc_u 9.3750 (18.7500) lr 1.0785e-03 eta 0:00:22
epoch [97/200] batch [15/60] time 0.336 (0.457) data 0.204 (0.326) loss_u loss_u 0.8857 (0.8469) acc_u 12.5000 (18.7500) lr 1.0785e-03 eta 0:00:20
epoch [97/200] batch [20/60] time 0.475 (0.456) data 0.343 (0.324) loss_u loss_u 0.8438 (0.8451) acc_u 21.8750 (19.2188) lr 1.0785e-03 eta 0:00:18
epoch [97/200] batch [25/60] time 0.412 (0.455) data 0.281 (0.323) loss_u loss_u 0.7651 (0.8395) acc_u 25.0000 (20.1250) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [30/60] time 0.419 (0.453) data 0.288 (0.322) loss_u loss_u 0.8228 (0.8464) acc_u 21.8750 (19.0625) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [35/60] time 0.345 (0.457) data 0.214 (0.326) loss_u loss_u 0.8174 (0.8462) acc_u 31.2500 (19.4643) lr 1.0785e-03 eta 0:00:11
epoch [97/200] batch [40/60] time 0.346 (0.452) data 0.215 (0.321) loss_u loss_u 0.8950 (0.8500) acc_u 12.5000 (18.9844) lr 1.0785e-03 eta 0:00:09
epoch [97/200] batch [45/60] time 0.699 (0.455) data 0.565 (0.323) loss_u loss_u 0.7764 (0.8464) acc_u 25.0000 (19.3750) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [50/60] time 0.357 (0.456) data 0.225 (0.325) loss_u loss_u 0.7773 (0.8412) acc_u 31.2500 (20.1250) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [55/60] time 0.385 (0.457) data 0.254 (0.326) loss_u loss_u 0.8257 (0.8416) acc_u 18.7500 (19.8295) lr 1.0785e-03 eta 0:00:02
epoch [97/200] batch [60/60] time 0.477 (0.456) data 0.346 (0.324) loss_u loss_u 0.9644 (0.8451) acc_u 3.1250 (19.2188) lr 1.0785e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1459
confident_label rate tensor(0.3903, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1224
clean true:1155
clean false:69
clean_rate:0.9436274509803921
noisy true:522
noisy false:1390
after delete: len(clean_dataset) 1224
after delete: len(noisy_dataset) 1912
epoch [98/200] batch [5/38] time 0.441 (0.457) data 0.311 (0.326) loss_x loss_x 1.5596 (1.2668) acc_x 75.0000 (71.8750) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [10/38] time 0.430 (0.465) data 0.299 (0.334) loss_x loss_x 1.2852 (1.1879) acc_x 71.8750 (72.8125) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [15/38] time 0.597 (0.462) data 0.466 (0.331) loss_x loss_x 1.8555 (1.1896) acc_x 56.2500 (71.6667) lr 1.0628e-03 eta 0:00:10
epoch [98/200] batch [20/38] time 0.594 (0.468) data 0.463 (0.337) loss_x loss_x 1.1514 (1.1613) acc_x 62.5000 (71.2500) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [25/38] time 0.394 (0.457) data 0.263 (0.326) loss_x loss_x 0.8921 (1.1302) acc_x 78.1250 (72.1250) lr 1.0628e-03 eta 0:00:05
epoch [98/200] batch [30/38] time 0.419 (0.462) data 0.287 (0.331) loss_x loss_x 1.0352 (1.1549) acc_x 78.1250 (71.4583) lr 1.0628e-03 eta 0:00:03
epoch [98/200] batch [35/38] time 0.489 (0.463) data 0.358 (0.332) loss_x loss_x 0.7725 (1.1413) acc_x 75.0000 (71.2500) lr 1.0628e-03 eta 0:00:01
epoch [98/200] batch [5/59] time 0.499 (0.465) data 0.367 (0.334) loss_u loss_u 0.9023 (0.8626) acc_u 12.5000 (16.8750) lr 1.0628e-03 eta 0:00:25
epoch [98/200] batch [10/59] time 0.335 (0.457) data 0.204 (0.326) loss_u loss_u 0.8608 (0.8468) acc_u 21.8750 (19.3750) lr 1.0628e-03 eta 0:00:22
epoch [98/200] batch [15/59] time 0.411 (0.456) data 0.279 (0.325) loss_u loss_u 0.8354 (0.8514) acc_u 21.8750 (18.3333) lr 1.0628e-03 eta 0:00:20
epoch [98/200] batch [20/59] time 0.495 (0.455) data 0.363 (0.324) loss_u loss_u 0.8901 (0.8517) acc_u 12.5000 (18.2812) lr 1.0628e-03 eta 0:00:17
epoch [98/200] batch [25/59] time 0.352 (0.455) data 0.220 (0.324) loss_u loss_u 0.8218 (0.8465) acc_u 28.1250 (19.2500) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [30/59] time 0.428 (0.451) data 0.297 (0.320) loss_u loss_u 0.7666 (0.8469) acc_u 31.2500 (19.0625) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [35/59] time 0.621 (0.451) data 0.490 (0.320) loss_u loss_u 0.8545 (0.8520) acc_u 21.8750 (18.5714) lr 1.0628e-03 eta 0:00:10
epoch [98/200] batch [40/59] time 0.368 (0.450) data 0.238 (0.318) loss_u loss_u 0.7969 (0.8541) acc_u 21.8750 (18.3594) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [45/59] time 0.357 (0.445) data 0.225 (0.314) loss_u loss_u 0.8047 (0.8529) acc_u 21.8750 (18.3333) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [50/59] time 0.390 (0.445) data 0.258 (0.314) loss_u loss_u 0.8638 (0.8541) acc_u 15.6250 (18.1250) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [55/59] time 0.387 (0.445) data 0.256 (0.313) loss_u loss_u 0.8428 (0.8524) acc_u 12.5000 (18.2955) lr 1.0628e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1528
confident_label rate tensor(0.3753, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1177
clean true:1098
clean false:79
clean_rate:0.9328802039082413
noisy true:510
noisy false:1449
after delete: len(clean_dataset) 1177
after delete: len(noisy_dataset) 1959
epoch [99/200] batch [5/36] time 0.490 (0.476) data 0.360 (0.346) loss_x loss_x 1.4062 (1.0792) acc_x 65.6250 (70.6250) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [10/36] time 0.488 (0.473) data 0.358 (0.343) loss_x loss_x 1.1621 (1.0292) acc_x 68.7500 (72.8125) lr 1.0471e-03 eta 0:00:12
epoch [99/200] batch [15/36] time 0.436 (0.464) data 0.306 (0.334) loss_x loss_x 1.3223 (1.0914) acc_x 65.6250 (71.8750) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [20/36] time 0.466 (0.471) data 0.336 (0.340) loss_x loss_x 0.9058 (1.1139) acc_x 65.6250 (70.0000) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [25/36] time 0.440 (0.465) data 0.310 (0.335) loss_x loss_x 1.5225 (1.1183) acc_x 68.7500 (70.6250) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [30/36] time 0.523 (0.463) data 0.392 (0.333) loss_x loss_x 1.0576 (1.1214) acc_x 71.8750 (70.1042) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [35/36] time 0.531 (0.464) data 0.401 (0.334) loss_x loss_x 0.9863 (1.1349) acc_x 71.8750 (69.8214) lr 1.0471e-03 eta 0:00:00
epoch [99/200] batch [5/61] time 0.351 (0.457) data 0.220 (0.327) loss_u loss_u 0.8926 (0.8487) acc_u 9.3750 (20.0000) lr 1.0471e-03 eta 0:00:25
epoch [99/200] batch [10/61] time 0.997 (0.469) data 0.866 (0.339) loss_u loss_u 0.8657 (0.8435) acc_u 15.6250 (20.6250) lr 1.0471e-03 eta 0:00:23
epoch [99/200] batch [15/61] time 0.404 (0.468) data 0.269 (0.338) loss_u loss_u 0.8892 (0.8540) acc_u 15.6250 (18.3333) lr 1.0471e-03 eta 0:00:21
epoch [99/200] batch [20/61] time 0.329 (0.471) data 0.197 (0.340) loss_u loss_u 0.8535 (0.8477) acc_u 18.7500 (19.6875) lr 1.0471e-03 eta 0:00:19
epoch [99/200] batch [25/61] time 0.497 (0.469) data 0.366 (0.338) loss_u loss_u 0.7891 (0.8436) acc_u 21.8750 (19.6250) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [30/61] time 0.555 (0.471) data 0.423 (0.340) loss_u loss_u 0.8784 (0.8411) acc_u 12.5000 (20.0000) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [35/61] time 0.452 (0.474) data 0.321 (0.343) loss_u loss_u 0.8589 (0.8378) acc_u 15.6250 (20.1786) lr 1.0471e-03 eta 0:00:12
epoch [99/200] batch [40/61] time 0.471 (0.475) data 0.334 (0.343) loss_u loss_u 0.8047 (0.8374) acc_u 21.8750 (20.0781) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [45/61] time 0.487 (0.474) data 0.353 (0.342) loss_u loss_u 0.9268 (0.8397) acc_u 12.5000 (19.7222) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [50/61] time 0.460 (0.478) data 0.325 (0.346) loss_u loss_u 0.8623 (0.8397) acc_u 15.6250 (19.8125) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [55/61] time 0.686 (0.480) data 0.554 (0.348) loss_u loss_u 0.8813 (0.8397) acc_u 12.5000 (19.5455) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [60/61] time 0.460 (0.477) data 0.329 (0.345) loss_u loss_u 0.8223 (0.8387) acc_u 21.8750 (19.7396) lr 1.0471e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1463
confident_label rate tensor(0.3855, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1209
clean true:1134
clean false:75
clean_rate:0.9379652605459057
noisy true:539
noisy false:1388
all clean rate:  [0.9052369077306733, 0.952076677316294, 0.9498207885304659, 0.950517836593786, 0.9479289940828403, 0.9459459459459459, 0.9539978094194962, 0.9462719298245614, 0.9543937708565072, 0.9406593406593406, 0.9561497326203209, 0.9495798319327731, 0.9473118279569892, 0.9441517386722866, 0.9428571428571428, 0.9402834008097166, 0.9444444444444444, 0.9409937888198758, 0.9479591836734694, 0.9325153374233128, 0.9303534303534303, 0.9409448818897638, 0.9415262636273538, 0.9355797819623389, 0.9270729270729271, 0.9374379344587885, 0.9266144814090019, 0.9369458128078818, 0.9300095877277086, 0.9463414634146341, 0.9442843419788665, 0.9429133858267716, 0.9460539460539461, 0.9250493096646942, 0.9468779123951537, 0.9394812680115274, 0.9289055191768008, 0.9391385767790262, 0.9388523047977423, 0.9459706959706959, 0.9411764705882353, 0.9347426470588235, 0.9354838709677419, 0.9385474860335196, 0.9322964318389753, 0.9471285323609845, 0.9398359161349134, 0.9478098788443616, 0.9446428571428571, 0.9439252336448598, 0.9409171075837742, 0.93646408839779, 0.9440937781785392, 0.9398148148148148, 0.9346642468239564, 0.9467613132209406, 0.9357959542656112, 0.9447513812154696, 0.9348214285714286, 0.9336283185840708, 0.9351771823681936, 0.9371633752244165, 0.941747572815534, 0.9386666666666666, 0.9405306495882891, 0.9351771823681936, 0.9288194444444444, 0.9363957597173145, 0.9457831325301205, 0.9344405594405595, 0.9250217959895379, 0.940193491644679, 0.9426644182124789, 0.9352014010507881, 0.9401709401709402, 0.9247219846022241, 0.9346733668341709, 0.9432684165961049, 0.9369817578772802, 0.9424216765453006, 0.9322607959356477, 0.9325153374233128, 0.9331641285956007, 0.9417152373022482, 0.9290484140233722, 0.9380378657487092, 0.9313559322033899, 0.9308016877637131, 0.9386189258312021, 0.9308641975308642, 0.9376068376068376, 0.9319502074688797, 0.9339388934764657, 0.9323308270676691, 0.9387583892617449, 0.947324414715719, 0.9370277078085643, 0.9436274509803921, 0.9328802039082413, 0.9379652605459057]
after delete: len(clean_dataset) 1209
after delete: len(noisy_dataset) 1927
epoch [100/200] batch [5/37] time 0.574 (0.481) data 0.444 (0.350) loss_x loss_x 0.7319 (1.3255) acc_x 81.2500 (68.7500) lr 1.0314e-03 eta 0:00:15
epoch [100/200] batch [10/37] time 0.446 (0.500) data 0.315 (0.370) loss_x loss_x 1.6094 (1.2881) acc_x 53.1250 (67.8125) lr 1.0314e-03 eta 0:00:13
epoch [100/200] batch [15/37] time 0.405 (0.487) data 0.274 (0.356) loss_x loss_x 0.7202 (1.1636) acc_x 84.3750 (71.2500) lr 1.0314e-03 eta 0:00:10
epoch [100/200] batch [20/37] time 0.382 (0.472) data 0.251 (0.341) loss_x loss_x 0.9868 (1.1966) acc_x 75.0000 (70.1562) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [25/37] time 0.425 (0.460) data 0.295 (0.330) loss_x loss_x 1.6943 (1.2457) acc_x 56.2500 (68.1250) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [30/37] time 0.485 (0.458) data 0.355 (0.327) loss_x loss_x 1.3281 (1.2209) acc_x 65.6250 (68.8542) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [35/37] time 0.452 (0.455) data 0.322 (0.324) loss_x loss_x 1.3086 (1.2535) acc_x 62.5000 (68.3929) lr 1.0314e-03 eta 0:00:00
epoch [100/200] batch [5/60] time 0.357 (0.450) data 0.226 (0.319) loss_u loss_u 0.8564 (0.8850) acc_u 18.7500 (15.6250) lr 1.0314e-03 eta 0:00:24
epoch [100/200] batch [10/60] time 0.473 (0.451) data 0.342 (0.321) loss_u loss_u 0.8003 (0.8705) acc_u 25.0000 (17.1875) lr 1.0314e-03 eta 0:00:22
epoch [100/200] batch [15/60] time 0.483 (0.452) data 0.352 (0.321) loss_u loss_u 0.9058 (0.8600) acc_u 12.5000 (17.9167) lr 1.0314e-03 eta 0:00:20
epoch [100/200] batch [20/60] time 0.383 (0.448) data 0.248 (0.317) loss_u loss_u 0.8584 (0.8578) acc_u 9.3750 (17.3438) lr 1.0314e-03 eta 0:00:17
epoch [100/200] batch [25/60] time 0.505 (0.451) data 0.373 (0.320) loss_u loss_u 0.8525 (0.8531) acc_u 15.6250 (17.7500) lr 1.0314e-03 eta 0:00:15
epoch [100/200] batch [30/60] time 0.427 (0.448) data 0.297 (0.317) loss_u loss_u 0.8682 (0.8551) acc_u 15.6250 (17.2917) lr 1.0314e-03 eta 0:00:13
epoch [100/200] batch [35/60] time 0.498 (0.446) data 0.367 (0.315) loss_u loss_u 0.7861 (0.8535) acc_u 25.0000 (17.5893) lr 1.0314e-03 eta 0:00:11
epoch [100/200] batch [40/60] time 0.549 (0.446) data 0.418 (0.315) loss_u loss_u 0.7852 (0.8478) acc_u 25.0000 (18.2812) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [45/60] time 0.335 (0.445) data 0.204 (0.314) loss_u loss_u 0.8174 (0.8465) acc_u 28.1250 (18.6111) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [50/60] time 0.508 (0.446) data 0.377 (0.315) loss_u loss_u 0.8613 (0.8490) acc_u 15.6250 (18.1875) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [55/60] time 0.549 (0.446) data 0.418 (0.315) loss_u loss_u 0.8315 (0.8485) acc_u 21.8750 (18.5227) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [60/60] time 0.423 (0.451) data 0.292 (0.320) loss_u loss_u 0.7383 (0.8469) acc_u 40.6250 (18.6979) lr 1.0314e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1477
confident_label rate tensor(0.3855, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1209
clean true:1134
clean false:75
clean_rate:0.9379652605459057
noisy true:525
noisy false:1402
after delete: len(clean_dataset) 1209
after delete: len(noisy_dataset) 1927
epoch [101/200] batch [5/37] time 0.530 (0.537) data 0.399 (0.406) loss_x loss_x 1.4639 (0.9839) acc_x 68.7500 (76.2500) lr 1.0157e-03 eta 0:00:17
epoch [101/200] batch [10/37] time 0.468 (0.487) data 0.337 (0.356) loss_x loss_x 0.8223 (1.0414) acc_x 78.1250 (72.8125) lr 1.0157e-03 eta 0:00:13
epoch [101/200] batch [15/37] time 0.474 (0.491) data 0.343 (0.360) loss_x loss_x 1.0859 (1.0873) acc_x 65.6250 (72.2917) lr 1.0157e-03 eta 0:00:10
epoch [101/200] batch [20/37] time 0.384 (0.480) data 0.252 (0.349) loss_x loss_x 1.0420 (1.0755) acc_x 71.8750 (72.5000) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [25/37] time 0.459 (0.474) data 0.328 (0.343) loss_x loss_x 0.8574 (1.0887) acc_x 78.1250 (72.0000) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [30/37] time 0.407 (0.472) data 0.277 (0.341) loss_x loss_x 1.2305 (1.0794) acc_x 71.8750 (72.2917) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [35/37] time 0.365 (0.462) data 0.235 (0.332) loss_x loss_x 1.1445 (1.1021) acc_x 71.8750 (71.5179) lr 1.0157e-03 eta 0:00:00
epoch [101/200] batch [5/60] time 0.463 (0.464) data 0.333 (0.333) loss_u loss_u 0.8447 (0.8715) acc_u 15.6250 (13.1250) lr 1.0157e-03 eta 0:00:25
epoch [101/200] batch [10/60] time 0.421 (0.459) data 0.289 (0.329) loss_u loss_u 0.8643 (0.8739) acc_u 15.6250 (14.3750) lr 1.0157e-03 eta 0:00:22
epoch [101/200] batch [15/60] time 0.401 (0.455) data 0.269 (0.324) loss_u loss_u 0.7822 (0.8632) acc_u 28.1250 (16.4583) lr 1.0157e-03 eta 0:00:20
epoch [101/200] batch [20/60] time 0.429 (0.453) data 0.297 (0.322) loss_u loss_u 0.8398 (0.8667) acc_u 18.7500 (16.4062) lr 1.0157e-03 eta 0:00:18
epoch [101/200] batch [25/60] time 0.413 (0.450) data 0.280 (0.319) loss_u loss_u 0.8530 (0.8650) acc_u 15.6250 (16.5000) lr 1.0157e-03 eta 0:00:15
epoch [101/200] batch [30/60] time 0.449 (0.449) data 0.317 (0.318) loss_u loss_u 0.8501 (0.8693) acc_u 12.5000 (16.1458) lr 1.0157e-03 eta 0:00:13
epoch [101/200] batch [35/60] time 0.343 (0.447) data 0.212 (0.316) loss_u loss_u 0.8857 (0.8685) acc_u 12.5000 (16.0714) lr 1.0157e-03 eta 0:00:11
epoch [101/200] batch [40/60] time 0.548 (0.447) data 0.416 (0.316) loss_u loss_u 0.7686 (0.8619) acc_u 25.0000 (17.1875) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [45/60] time 0.403 (0.452) data 0.272 (0.320) loss_u loss_u 0.8218 (0.8587) acc_u 18.7500 (17.5000) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [50/60] time 0.395 (0.452) data 0.263 (0.321) loss_u loss_u 0.8184 (0.8577) acc_u 25.0000 (17.4375) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [55/60] time 0.581 (0.455) data 0.449 (0.324) loss_u loss_u 0.9434 (0.8579) acc_u 3.1250 (17.5568) lr 1.0157e-03 eta 0:00:02
epoch [101/200] batch [60/60] time 0.405 (0.454) data 0.274 (0.323) loss_u loss_u 0.8887 (0.8562) acc_u 9.3750 (17.6562) lr 1.0157e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.4021, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1261
clean true:1184
clean false:77
clean_rate:0.9389373513084853
noisy true:518
noisy false:1357
after delete: len(clean_dataset) 1261
after delete: len(noisy_dataset) 1875
epoch [102/200] batch [5/39] time 0.352 (0.434) data 0.222 (0.303) loss_x loss_x 1.3867 (1.2385) acc_x 59.3750 (63.7500) lr 1.0000e-03 eta 0:00:14
epoch [102/200] batch [10/39] time 0.444 (0.432) data 0.313 (0.301) loss_x loss_x 1.4453 (1.1710) acc_x 59.3750 (69.6875) lr 1.0000e-03 eta 0:00:12
epoch [102/200] batch [15/39] time 0.541 (0.447) data 0.411 (0.317) loss_x loss_x 1.1396 (1.1258) acc_x 75.0000 (71.2500) lr 1.0000e-03 eta 0:00:10
epoch [102/200] batch [20/39] time 0.374 (0.440) data 0.243 (0.309) loss_x loss_x 0.7852 (1.0913) acc_x 75.0000 (71.7188) lr 1.0000e-03 eta 0:00:08
epoch [102/200] batch [25/39] time 0.355 (0.445) data 0.224 (0.314) loss_x loss_x 0.8638 (1.1291) acc_x 71.8750 (70.8750) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [30/39] time 0.405 (0.447) data 0.274 (0.316) loss_x loss_x 1.2227 (1.1499) acc_x 71.8750 (70.5208) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [35/39] time 0.464 (0.455) data 0.331 (0.325) loss_x loss_x 1.0273 (1.1605) acc_x 56.2500 (69.9107) lr 1.0000e-03 eta 0:00:01
epoch [102/200] batch [5/58] time 0.400 (0.462) data 0.268 (0.331) loss_u loss_u 0.8413 (0.8680) acc_u 21.8750 (16.8750) lr 1.0000e-03 eta 0:00:24
epoch [102/200] batch [10/58] time 0.490 (0.458) data 0.360 (0.327) loss_u loss_u 0.9028 (0.8463) acc_u 12.5000 (20.6250) lr 1.0000e-03 eta 0:00:21
epoch [102/200] batch [15/58] time 0.502 (0.455) data 0.369 (0.325) loss_u loss_u 0.8530 (0.8497) acc_u 18.7500 (20.4167) lr 1.0000e-03 eta 0:00:19
epoch [102/200] batch [20/58] time 0.517 (0.451) data 0.386 (0.320) loss_u loss_u 0.8545 (0.8622) acc_u 18.7500 (18.4375) lr 1.0000e-03 eta 0:00:17
epoch [102/200] batch [25/58] time 0.520 (0.454) data 0.387 (0.323) loss_u loss_u 0.7769 (0.8533) acc_u 25.0000 (19.2500) lr 1.0000e-03 eta 0:00:14
epoch [102/200] batch [30/58] time 0.448 (0.451) data 0.316 (0.319) loss_u loss_u 0.8926 (0.8606) acc_u 9.3750 (17.9167) lr 1.0000e-03 eta 0:00:12
epoch [102/200] batch [35/58] time 0.423 (0.453) data 0.291 (0.321) loss_u loss_u 0.8901 (0.8594) acc_u 15.6250 (18.1250) lr 1.0000e-03 eta 0:00:10
epoch [102/200] batch [40/58] time 0.362 (0.451) data 0.230 (0.320) loss_u loss_u 0.7808 (0.8577) acc_u 25.0000 (18.2031) lr 1.0000e-03 eta 0:00:08
epoch [102/200] batch [45/58] time 0.358 (0.449) data 0.226 (0.318) loss_u loss_u 0.8823 (0.8606) acc_u 15.6250 (17.8472) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [50/58] time 0.405 (0.449) data 0.273 (0.317) loss_u loss_u 0.9248 (0.8614) acc_u 3.1250 (17.6250) lr 1.0000e-03 eta 0:00:03
epoch [102/200] batch [55/58] time 0.379 (0.452) data 0.247 (0.320) loss_u loss_u 0.8447 (0.8622) acc_u 12.5000 (17.3864) lr 1.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1477
confident_label rate tensor(0.3884, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1218
clean true:1133
clean false:85
clean_rate:0.9302134646962233
noisy true:526
noisy false:1392
after delete: len(clean_dataset) 1218
after delete: len(noisy_dataset) 1918
epoch [103/200] batch [5/38] time 0.452 (0.452) data 0.322 (0.321) loss_x loss_x 1.0723 (1.2607) acc_x 81.2500 (70.6250) lr 9.8429e-04 eta 0:00:14
epoch [103/200] batch [10/38] time 0.374 (0.476) data 0.244 (0.346) loss_x loss_x 1.1660 (1.2837) acc_x 65.6250 (66.8750) lr 9.8429e-04 eta 0:00:13
epoch [103/200] batch [15/38] time 0.362 (0.457) data 0.231 (0.327) loss_x loss_x 1.1895 (1.3209) acc_x 65.6250 (66.0417) lr 9.8429e-04 eta 0:00:10
epoch [103/200] batch [20/38] time 0.565 (0.471) data 0.434 (0.340) loss_x loss_x 1.5498 (1.2768) acc_x 71.8750 (67.0312) lr 9.8429e-04 eta 0:00:08
epoch [103/200] batch [25/38] time 0.475 (0.463) data 0.343 (0.332) loss_x loss_x 0.9053 (1.2852) acc_x 71.8750 (66.2500) lr 9.8429e-04 eta 0:00:06
epoch [103/200] batch [30/38] time 0.363 (0.453) data 0.233 (0.322) loss_x loss_x 1.4658 (1.2898) acc_x 71.8750 (66.9792) lr 9.8429e-04 eta 0:00:03
epoch [103/200] batch [35/38] time 0.406 (0.447) data 0.275 (0.317) loss_x loss_x 1.3701 (1.2707) acc_x 65.6250 (67.5000) lr 9.8429e-04 eta 0:00:01
epoch [103/200] batch [5/59] time 0.364 (0.447) data 0.232 (0.316) loss_u loss_u 0.7988 (0.8287) acc_u 25.0000 (20.0000) lr 9.8429e-04 eta 0:00:24
epoch [103/200] batch [10/59] time 0.498 (0.445) data 0.366 (0.314) loss_u loss_u 0.8511 (0.8475) acc_u 12.5000 (18.1250) lr 9.8429e-04 eta 0:00:21
epoch [103/200] batch [15/59] time 0.389 (0.446) data 0.259 (0.316) loss_u loss_u 0.8828 (0.8524) acc_u 12.5000 (18.1250) lr 9.8429e-04 eta 0:00:19
epoch [103/200] batch [20/59] time 0.373 (0.446) data 0.242 (0.315) loss_u loss_u 0.8984 (0.8473) acc_u 9.3750 (18.7500) lr 9.8429e-04 eta 0:00:17
epoch [103/200] batch [25/59] time 0.410 (0.444) data 0.278 (0.313) loss_u loss_u 0.8496 (0.8496) acc_u 18.7500 (18.7500) lr 9.8429e-04 eta 0:00:15
epoch [103/200] batch [30/59] time 0.558 (0.445) data 0.425 (0.314) loss_u loss_u 0.8350 (0.8527) acc_u 18.7500 (18.3333) lr 9.8429e-04 eta 0:00:12
epoch [103/200] batch [35/59] time 0.599 (0.458) data 0.467 (0.327) loss_u loss_u 0.8125 (0.8485) acc_u 21.8750 (18.8393) lr 9.8429e-04 eta 0:00:10
epoch [103/200] batch [40/59] time 0.509 (0.461) data 0.377 (0.330) loss_u loss_u 0.8730 (0.8482) acc_u 12.5000 (18.5938) lr 9.8429e-04 eta 0:00:08
epoch [103/200] batch [45/59] time 0.425 (0.462) data 0.293 (0.331) loss_u loss_u 0.7773 (0.8499) acc_u 28.1250 (18.6111) lr 9.8429e-04 eta 0:00:06
epoch [103/200] batch [50/59] time 0.413 (0.460) data 0.281 (0.329) loss_u loss_u 0.8467 (0.8509) acc_u 12.5000 (18.3750) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [55/59] time 0.481 (0.459) data 0.349 (0.327) loss_u loss_u 0.8433 (0.8531) acc_u 18.7500 (17.9545) lr 9.8429e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1508
confident_label rate tensor(0.3753, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1177
clean true:1101
clean false:76
clean_rate:0.935429056924384
noisy true:527
noisy false:1432
after delete: len(clean_dataset) 1177
after delete: len(noisy_dataset) 1959
epoch [104/200] batch [5/36] time 0.383 (0.466) data 0.252 (0.336) loss_x loss_x 1.1738 (0.9537) acc_x 65.6250 (73.7500) lr 9.6859e-04 eta 0:00:14
epoch [104/200] batch [10/36] time 0.574 (0.491) data 0.444 (0.361) loss_x loss_x 1.2949 (0.9903) acc_x 71.8750 (75.0000) lr 9.6859e-04 eta 0:00:12
epoch [104/200] batch [15/36] time 0.431 (0.489) data 0.301 (0.359) loss_x loss_x 1.0039 (0.9848) acc_x 71.8750 (75.4167) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [20/36] time 0.372 (0.481) data 0.241 (0.350) loss_x loss_x 1.0137 (1.0316) acc_x 75.0000 (74.3750) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [25/36] time 0.368 (0.481) data 0.237 (0.351) loss_x loss_x 1.1631 (1.0350) acc_x 71.8750 (74.8750) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [30/36] time 0.502 (0.473) data 0.371 (0.342) loss_x loss_x 0.6348 (1.0227) acc_x 84.3750 (74.7917) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [35/36] time 0.408 (0.462) data 0.277 (0.331) loss_x loss_x 0.7900 (1.0517) acc_x 75.0000 (73.6607) lr 9.6859e-04 eta 0:00:00
epoch [104/200] batch [5/61] time 0.620 (0.471) data 0.488 (0.340) loss_u loss_u 0.8433 (0.8593) acc_u 15.6250 (18.7500) lr 9.6859e-04 eta 0:00:26
epoch [104/200] batch [10/61] time 0.499 (0.466) data 0.367 (0.336) loss_u loss_u 0.8369 (0.8627) acc_u 18.7500 (17.5000) lr 9.6859e-04 eta 0:00:23
epoch [104/200] batch [15/61] time 0.427 (0.463) data 0.295 (0.332) loss_u loss_u 0.7925 (0.8586) acc_u 21.8750 (17.7083) lr 9.6859e-04 eta 0:00:21
epoch [104/200] batch [20/61] time 0.470 (0.460) data 0.339 (0.329) loss_u loss_u 0.7969 (0.8513) acc_u 25.0000 (18.5938) lr 9.6859e-04 eta 0:00:18
epoch [104/200] batch [25/61] time 0.471 (0.456) data 0.340 (0.325) loss_u loss_u 0.8960 (0.8492) acc_u 12.5000 (18.7500) lr 9.6859e-04 eta 0:00:16
epoch [104/200] batch [30/61] time 0.446 (0.454) data 0.315 (0.323) loss_u loss_u 0.7979 (0.8486) acc_u 21.8750 (18.9583) lr 9.6859e-04 eta 0:00:14
epoch [104/200] batch [35/61] time 0.471 (0.454) data 0.339 (0.323) loss_u loss_u 0.8506 (0.8490) acc_u 21.8750 (19.2857) lr 9.6859e-04 eta 0:00:11
epoch [104/200] batch [40/61] time 0.359 (0.456) data 0.227 (0.325) loss_u loss_u 0.8955 (0.8505) acc_u 12.5000 (19.0625) lr 9.6859e-04 eta 0:00:09
epoch [104/200] batch [45/61] time 0.451 (0.453) data 0.319 (0.322) loss_u loss_u 0.8921 (0.8514) acc_u 9.3750 (18.5417) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [50/61] time 0.414 (0.451) data 0.282 (0.319) loss_u loss_u 0.9434 (0.8540) acc_u 6.2500 (18.1875) lr 9.6859e-04 eta 0:00:04
epoch [104/200] batch [55/61] time 0.444 (0.451) data 0.312 (0.319) loss_u loss_u 0.7930 (0.8505) acc_u 25.0000 (18.6932) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [60/61] time 0.626 (0.454) data 0.495 (0.323) loss_u loss_u 0.8276 (0.8488) acc_u 21.8750 (18.7500) lr 9.6859e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1497
confident_label rate tensor(0.3849, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1207
clean true:1133
clean false:74
clean_rate:0.9386909693454847
noisy true:506
noisy false:1423
after delete: len(clean_dataset) 1207
after delete: len(noisy_dataset) 1929
epoch [105/200] batch [5/37] time 0.529 (0.483) data 0.398 (0.352) loss_x loss_x 0.8496 (1.1666) acc_x 75.0000 (73.1250) lr 9.5289e-04 eta 0:00:15
epoch [105/200] batch [10/37] time 0.386 (0.457) data 0.255 (0.326) loss_x loss_x 1.0186 (1.1508) acc_x 65.6250 (71.2500) lr 9.5289e-04 eta 0:00:12
epoch [105/200] batch [15/37] time 0.426 (0.453) data 0.295 (0.323) loss_x loss_x 1.0322 (1.1615) acc_x 75.0000 (71.0417) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [20/37] time 0.494 (0.450) data 0.363 (0.319) loss_x loss_x 1.2578 (1.1848) acc_x 62.5000 (70.6250) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [25/37] time 0.485 (0.456) data 0.354 (0.325) loss_x loss_x 1.3145 (1.1904) acc_x 71.8750 (71.2500) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [30/37] time 0.614 (0.466) data 0.483 (0.335) loss_x loss_x 1.3057 (1.2188) acc_x 65.6250 (70.1042) lr 9.5289e-04 eta 0:00:03
epoch [105/200] batch [35/37] time 0.449 (0.463) data 0.318 (0.332) loss_x loss_x 1.3418 (1.2283) acc_x 68.7500 (69.5536) lr 9.5289e-04 eta 0:00:00
epoch [105/200] batch [5/60] time 0.450 (0.460) data 0.319 (0.329) loss_u loss_u 0.7173 (0.8336) acc_u 34.3750 (18.7500) lr 9.5289e-04 eta 0:00:25
epoch [105/200] batch [10/60] time 0.707 (0.463) data 0.575 (0.332) loss_u loss_u 0.7754 (0.8355) acc_u 21.8750 (19.3750) lr 9.5289e-04 eta 0:00:23
epoch [105/200] batch [15/60] time 0.401 (0.468) data 0.270 (0.337) loss_u loss_u 0.8945 (0.8424) acc_u 15.6250 (18.7500) lr 9.5289e-04 eta 0:00:21
epoch [105/200] batch [20/60] time 0.344 (0.467) data 0.211 (0.335) loss_u loss_u 0.8433 (0.8466) acc_u 25.0000 (19.0625) lr 9.5289e-04 eta 0:00:18
epoch [105/200] batch [25/60] time 0.340 (0.466) data 0.208 (0.335) loss_u loss_u 0.8164 (0.8513) acc_u 21.8750 (18.5000) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [30/60] time 0.449 (0.463) data 0.317 (0.332) loss_u loss_u 0.8843 (0.8503) acc_u 15.6250 (18.6458) lr 9.5289e-04 eta 0:00:13
epoch [105/200] batch [35/60] time 0.475 (0.463) data 0.344 (0.331) loss_u loss_u 0.8193 (0.8503) acc_u 15.6250 (18.3036) lr 9.5289e-04 eta 0:00:11
epoch [105/200] batch [40/60] time 0.348 (0.462) data 0.217 (0.331) loss_u loss_u 0.7500 (0.8451) acc_u 34.3750 (18.8281) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [45/60] time 0.406 (0.457) data 0.275 (0.326) loss_u loss_u 0.8877 (0.8475) acc_u 12.5000 (18.4722) lr 9.5289e-04 eta 0:00:06
epoch [105/200] batch [50/60] time 0.414 (0.455) data 0.283 (0.324) loss_u loss_u 0.8662 (0.8456) acc_u 18.7500 (18.8750) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [55/60] time 0.361 (0.452) data 0.230 (0.320) loss_u loss_u 0.8462 (0.8444) acc_u 15.6250 (18.9773) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [60/60] time 0.460 (0.456) data 0.329 (0.324) loss_u loss_u 0.8931 (0.8433) acc_u 12.5000 (19.2708) lr 9.5289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1465
confident_label rate tensor(0.3893, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1221
clean true:1145
clean false:76
clean_rate:0.9377559377559378
noisy true:526
noisy false:1389
after delete: len(clean_dataset) 1221
after delete: len(noisy_dataset) 1915
epoch [106/200] batch [5/38] time 0.365 (0.476) data 0.234 (0.344) loss_x loss_x 1.1240 (1.0769) acc_x 68.7500 (73.1250) lr 9.3721e-04 eta 0:00:15
epoch [106/200] batch [10/38] time 0.483 (0.452) data 0.351 (0.321) loss_x loss_x 1.2725 (1.0972) acc_x 71.8750 (71.8750) lr 9.3721e-04 eta 0:00:12
epoch [106/200] batch [15/38] time 0.430 (0.476) data 0.299 (0.345) loss_x loss_x 1.5605 (1.2038) acc_x 59.3750 (70.0000) lr 9.3721e-04 eta 0:00:10
epoch [106/200] batch [20/38] time 0.400 (0.472) data 0.269 (0.341) loss_x loss_x 0.8394 (1.2026) acc_x 81.2500 (70.1562) lr 9.3721e-04 eta 0:00:08
epoch [106/200] batch [25/38] time 0.433 (0.471) data 0.303 (0.340) loss_x loss_x 1.0361 (1.2246) acc_x 71.8750 (69.2500) lr 9.3721e-04 eta 0:00:06
epoch [106/200] batch [30/38] time 0.508 (0.474) data 0.378 (0.343) loss_x loss_x 1.3936 (1.1993) acc_x 68.7500 (69.7917) lr 9.3721e-04 eta 0:00:03
epoch [106/200] batch [35/38] time 0.410 (0.475) data 0.279 (0.344) loss_x loss_x 1.1133 (1.2065) acc_x 75.0000 (69.9107) lr 9.3721e-04 eta 0:00:01
epoch [106/200] batch [5/59] time 0.344 (0.466) data 0.213 (0.335) loss_u loss_u 0.8237 (0.8279) acc_u 28.1250 (22.5000) lr 9.3721e-04 eta 0:00:25
epoch [106/200] batch [10/59] time 0.454 (0.465) data 0.322 (0.334) loss_u loss_u 0.8984 (0.8313) acc_u 15.6250 (23.1250) lr 9.3721e-04 eta 0:00:22
epoch [106/200] batch [15/59] time 0.445 (0.465) data 0.313 (0.333) loss_u loss_u 0.7710 (0.8344) acc_u 21.8750 (22.0833) lr 9.3721e-04 eta 0:00:20
epoch [106/200] batch [20/59] time 0.375 (0.459) data 0.243 (0.328) loss_u loss_u 0.8623 (0.8414) acc_u 15.6250 (20.9375) lr 9.3721e-04 eta 0:00:17
epoch [106/200] batch [25/59] time 0.438 (0.457) data 0.306 (0.326) loss_u loss_u 0.7637 (0.8348) acc_u 25.0000 (21.1250) lr 9.3721e-04 eta 0:00:15
epoch [106/200] batch [30/59] time 0.413 (0.454) data 0.281 (0.323) loss_u loss_u 0.8564 (0.8346) acc_u 21.8750 (21.2500) lr 9.3721e-04 eta 0:00:13
epoch [106/200] batch [35/59] time 0.537 (0.455) data 0.405 (0.324) loss_u loss_u 0.8955 (0.8406) acc_u 15.6250 (20.3571) lr 9.3721e-04 eta 0:00:10
epoch [106/200] batch [40/59] time 0.431 (0.455) data 0.299 (0.323) loss_u loss_u 0.8296 (0.8404) acc_u 21.8750 (20.3125) lr 9.3721e-04 eta 0:00:08
epoch [106/200] batch [45/59] time 0.502 (0.455) data 0.370 (0.324) loss_u loss_u 0.8984 (0.8449) acc_u 12.5000 (19.5833) lr 9.3721e-04 eta 0:00:06
epoch [106/200] batch [50/59] time 0.580 (0.460) data 0.447 (0.328) loss_u loss_u 0.8794 (0.8427) acc_u 15.6250 (19.7500) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [55/59] time 0.382 (0.458) data 0.250 (0.327) loss_u loss_u 0.7676 (0.8418) acc_u 28.1250 (19.8295) lr 9.3721e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1451
confident_label rate tensor(0.3989, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1251
clean true:1169
clean false:82
clean_rate:0.9344524380495604
noisy true:516
noisy false:1369
after delete: len(clean_dataset) 1251
after delete: len(noisy_dataset) 1885
epoch [107/200] batch [5/39] time 0.454 (0.500) data 0.324 (0.369) loss_x loss_x 1.2412 (1.1394) acc_x 59.3750 (73.7500) lr 9.2154e-04 eta 0:00:16
epoch [107/200] batch [10/39] time 0.469 (0.512) data 0.338 (0.381) loss_x loss_x 1.3857 (1.2535) acc_x 68.7500 (70.3125) lr 9.2154e-04 eta 0:00:14
epoch [107/200] batch [15/39] time 0.346 (0.485) data 0.215 (0.354) loss_x loss_x 1.4551 (1.2116) acc_x 62.5000 (71.0417) lr 9.2154e-04 eta 0:00:11
epoch [107/200] batch [20/39] time 0.409 (0.501) data 0.278 (0.370) loss_x loss_x 1.1592 (1.1636) acc_x 68.7500 (71.5625) lr 9.2154e-04 eta 0:00:09
epoch [107/200] batch [25/39] time 0.427 (0.499) data 0.296 (0.368) loss_x loss_x 1.4316 (1.1811) acc_x 65.6250 (70.7500) lr 9.2154e-04 eta 0:00:06
epoch [107/200] batch [30/39] time 0.562 (0.495) data 0.431 (0.364) loss_x loss_x 1.1426 (1.1846) acc_x 78.1250 (70.3125) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [35/39] time 0.379 (0.480) data 0.249 (0.349) loss_x loss_x 1.2637 (1.1970) acc_x 68.7500 (70.2679) lr 9.2154e-04 eta 0:00:01
epoch [107/200] batch [5/58] time 0.493 (0.477) data 0.360 (0.346) loss_u loss_u 0.8345 (0.8228) acc_u 18.7500 (20.6250) lr 9.2154e-04 eta 0:00:25
epoch [107/200] batch [10/58] time 0.329 (0.477) data 0.197 (0.346) loss_u loss_u 0.8091 (0.8422) acc_u 21.8750 (18.1250) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [15/58] time 0.490 (0.481) data 0.358 (0.350) loss_u loss_u 0.8687 (0.8549) acc_u 18.7500 (17.9167) lr 9.2154e-04 eta 0:00:20
epoch [107/200] batch [20/58] time 0.387 (0.475) data 0.255 (0.344) loss_u loss_u 0.8579 (0.8490) acc_u 18.7500 (18.9062) lr 9.2154e-04 eta 0:00:18
epoch [107/200] batch [25/58] time 0.404 (0.467) data 0.273 (0.336) loss_u loss_u 0.8008 (0.8453) acc_u 28.1250 (19.8750) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [30/58] time 0.518 (0.465) data 0.386 (0.333) loss_u loss_u 0.8931 (0.8485) acc_u 12.5000 (19.2708) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [35/58] time 0.469 (0.462) data 0.337 (0.331) loss_u loss_u 0.9019 (0.8488) acc_u 12.5000 (19.2857) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [40/58] time 0.394 (0.459) data 0.264 (0.328) loss_u loss_u 0.8340 (0.8506) acc_u 21.8750 (19.2969) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [45/58] time 0.384 (0.459) data 0.254 (0.327) loss_u loss_u 0.9150 (0.8527) acc_u 6.2500 (18.7500) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [50/58] time 0.387 (0.458) data 0.255 (0.327) loss_u loss_u 0.8594 (0.8531) acc_u 18.7500 (18.6875) lr 9.2154e-04 eta 0:00:03
epoch [107/200] batch [55/58] time 0.458 (0.459) data 0.327 (0.328) loss_u loss_u 0.8335 (0.8490) acc_u 21.8750 (19.2614) lr 9.2154e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1459
confident_label rate tensor(0.3881, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1217
clean true:1135
clean false:82
clean_rate:0.932621199671323
noisy true:542
noisy false:1377
after delete: len(clean_dataset) 1217
after delete: len(noisy_dataset) 1919
epoch [108/200] batch [5/38] time 0.428 (0.499) data 0.298 (0.367) loss_x loss_x 1.0381 (1.1434) acc_x 75.0000 (76.2500) lr 9.0589e-04 eta 0:00:16
epoch [108/200] batch [10/38] time 0.380 (0.475) data 0.249 (0.344) loss_x loss_x 1.5586 (1.1803) acc_x 53.1250 (72.5000) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [15/38] time 0.436 (0.482) data 0.306 (0.351) loss_x loss_x 0.9434 (1.1279) acc_x 75.0000 (72.5000) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [20/38] time 0.421 (0.472) data 0.290 (0.341) loss_x loss_x 1.4365 (1.1566) acc_x 71.8750 (72.8125) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [25/38] time 0.378 (0.471) data 0.248 (0.340) loss_x loss_x 1.1133 (1.1381) acc_x 68.7500 (72.3750) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [30/38] time 0.452 (0.462) data 0.322 (0.332) loss_x loss_x 1.1865 (1.1180) acc_x 71.8750 (72.7083) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [35/38] time 0.488 (0.462) data 0.357 (0.331) loss_x loss_x 1.0742 (1.1095) acc_x 68.7500 (72.6786) lr 9.0589e-04 eta 0:00:01
epoch [108/200] batch [5/59] time 0.383 (0.469) data 0.251 (0.338) loss_u loss_u 0.8223 (0.8560) acc_u 25.0000 (20.6250) lr 9.0589e-04 eta 0:00:25
epoch [108/200] batch [10/59] time 0.445 (0.467) data 0.312 (0.336) loss_u loss_u 0.8257 (0.8362) acc_u 21.8750 (21.8750) lr 9.0589e-04 eta 0:00:22
epoch [108/200] batch [15/59] time 0.457 (0.467) data 0.325 (0.336) loss_u loss_u 0.8511 (0.8383) acc_u 18.7500 (21.2500) lr 9.0589e-04 eta 0:00:20
epoch [108/200] batch [20/59] time 0.374 (0.463) data 0.240 (0.332) loss_u loss_u 0.9561 (0.8467) acc_u 6.2500 (20.0000) lr 9.0589e-04 eta 0:00:18
epoch [108/200] batch [25/59] time 0.418 (0.461) data 0.286 (0.329) loss_u loss_u 0.9092 (0.8511) acc_u 9.3750 (19.2500) lr 9.0589e-04 eta 0:00:15
epoch [108/200] batch [30/59] time 0.472 (0.463) data 0.341 (0.332) loss_u loss_u 0.8477 (0.8560) acc_u 12.5000 (18.3333) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [35/59] time 0.376 (0.461) data 0.245 (0.330) loss_u loss_u 0.8179 (0.8569) acc_u 31.2500 (18.3929) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [40/59] time 0.327 (0.456) data 0.191 (0.325) loss_u loss_u 0.9023 (0.8568) acc_u 12.5000 (18.2031) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [45/59] time 0.500 (0.456) data 0.369 (0.325) loss_u loss_u 0.7720 (0.8533) acc_u 25.0000 (18.4028) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [50/59] time 0.378 (0.456) data 0.248 (0.325) loss_u loss_u 0.8286 (0.8554) acc_u 28.1250 (18.1250) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [55/59] time 0.617 (0.458) data 0.485 (0.327) loss_u loss_u 0.8052 (0.8537) acc_u 28.1250 (18.3523) lr 9.0589e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1494
confident_label rate tensor(0.3852, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1208
clean true:1123
clean false:85
clean_rate:0.929635761589404
noisy true:519
noisy false:1409
after delete: len(clean_dataset) 1208
after delete: len(noisy_dataset) 1928
epoch [109/200] batch [5/37] time 0.548 (0.437) data 0.416 (0.305) loss_x loss_x 1.2891 (1.3373) acc_x 65.6250 (69.3750) lr 8.9027e-04 eta 0:00:13
epoch [109/200] batch [10/37] time 0.465 (0.445) data 0.334 (0.314) loss_x loss_x 1.2012 (1.3029) acc_x 78.1250 (68.7500) lr 8.9027e-04 eta 0:00:12
epoch [109/200] batch [15/37] time 0.392 (0.451) data 0.262 (0.320) loss_x loss_x 1.1504 (1.2428) acc_x 68.7500 (69.5833) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [20/37] time 0.392 (0.453) data 0.261 (0.322) loss_x loss_x 1.1104 (1.2190) acc_x 75.0000 (70.3125) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [25/37] time 0.344 (0.446) data 0.213 (0.315) loss_x loss_x 1.5693 (1.2206) acc_x 62.5000 (69.7500) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [30/37] time 0.745 (0.471) data 0.613 (0.340) loss_x loss_x 0.6143 (1.1731) acc_x 81.2500 (70.9375) lr 8.9027e-04 eta 0:00:03
epoch [109/200] batch [35/37] time 0.437 (0.466) data 0.303 (0.335) loss_x loss_x 1.3320 (1.1951) acc_x 59.3750 (70.1786) lr 8.9027e-04 eta 0:00:00
epoch [109/200] batch [5/60] time 0.437 (0.472) data 0.305 (0.341) loss_u loss_u 0.8472 (0.8381) acc_u 18.7500 (20.0000) lr 8.9027e-04 eta 0:00:25
epoch [109/200] batch [10/60] time 0.456 (0.470) data 0.323 (0.339) loss_u loss_u 0.9463 (0.8625) acc_u 3.1250 (16.5625) lr 8.9027e-04 eta 0:00:23
epoch [109/200] batch [15/60] time 0.600 (0.476) data 0.468 (0.345) loss_u loss_u 0.8315 (0.8566) acc_u 25.0000 (17.9167) lr 8.9027e-04 eta 0:00:21
epoch [109/200] batch [20/60] time 0.479 (0.478) data 0.344 (0.346) loss_u loss_u 0.7910 (0.8521) acc_u 28.1250 (18.5938) lr 8.9027e-04 eta 0:00:19
epoch [109/200] batch [25/60] time 0.495 (0.479) data 0.364 (0.347) loss_u loss_u 0.8892 (0.8566) acc_u 15.6250 (17.7500) lr 8.9027e-04 eta 0:00:16
epoch [109/200] batch [30/60] time 0.534 (0.476) data 0.402 (0.344) loss_u loss_u 0.9009 (0.8518) acc_u 12.5000 (18.3333) lr 8.9027e-04 eta 0:00:14
epoch [109/200] batch [35/60] time 0.504 (0.478) data 0.372 (0.346) loss_u loss_u 0.7681 (0.8499) acc_u 34.3750 (18.7500) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [40/60] time 0.377 (0.477) data 0.246 (0.345) loss_u loss_u 0.7842 (0.8498) acc_u 25.0000 (18.6719) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [45/60] time 0.422 (0.473) data 0.291 (0.341) loss_u loss_u 0.8716 (0.8528) acc_u 15.6250 (18.2639) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [50/60] time 0.432 (0.473) data 0.300 (0.341) loss_u loss_u 0.8501 (0.8509) acc_u 15.6250 (18.3750) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [55/60] time 0.424 (0.472) data 0.292 (0.340) loss_u loss_u 0.8340 (0.8507) acc_u 18.7500 (18.4091) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [60/60] time 0.321 (0.467) data 0.189 (0.336) loss_u loss_u 0.9170 (0.8527) acc_u 9.3750 (18.0729) lr 8.9027e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1459
confident_label rate tensor(0.3878, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1216
clean true:1153
clean false:63
clean_rate:0.9481907894736842
noisy true:524
noisy false:1396
after delete: len(clean_dataset) 1216
after delete: len(noisy_dataset) 1920
epoch [110/200] batch [5/38] time 0.614 (0.471) data 0.483 (0.340) loss_x loss_x 1.1045 (1.1916) acc_x 68.7500 (70.6250) lr 8.7467e-04 eta 0:00:15
epoch [110/200] batch [10/38] time 0.480 (0.459) data 0.348 (0.328) loss_x loss_x 1.0479 (1.1896) acc_x 75.0000 (70.9375) lr 8.7467e-04 eta 0:00:12
epoch [110/200] batch [15/38] time 0.491 (0.463) data 0.361 (0.332) loss_x loss_x 1.4736 (1.1717) acc_x 62.5000 (72.0833) lr 8.7467e-04 eta 0:00:10
epoch [110/200] batch [20/38] time 0.479 (0.469) data 0.349 (0.339) loss_x loss_x 0.7583 (1.1466) acc_x 78.1250 (72.8125) lr 8.7467e-04 eta 0:00:08
epoch [110/200] batch [25/38] time 0.590 (0.471) data 0.459 (0.341) loss_x loss_x 1.1377 (1.1453) acc_x 62.5000 (71.7500) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [30/38] time 0.424 (0.471) data 0.294 (0.341) loss_x loss_x 1.1592 (1.1716) acc_x 68.7500 (70.7292) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [35/38] time 0.571 (0.479) data 0.441 (0.348) loss_x loss_x 0.7212 (1.1780) acc_x 81.2500 (71.1607) lr 8.7467e-04 eta 0:00:01
epoch [110/200] batch [5/60] time 0.456 (0.466) data 0.325 (0.335) loss_u loss_u 0.8296 (0.8479) acc_u 21.8750 (18.1250) lr 8.7467e-04 eta 0:00:25
epoch [110/200] batch [10/60] time 0.447 (0.465) data 0.316 (0.335) loss_u loss_u 0.7832 (0.8418) acc_u 31.2500 (20.0000) lr 8.7467e-04 eta 0:00:23
epoch [110/200] batch [15/60] time 0.422 (0.465) data 0.290 (0.334) loss_u loss_u 0.8062 (0.8435) acc_u 25.0000 (19.5833) lr 8.7467e-04 eta 0:00:20
epoch [110/200] batch [20/60] time 0.397 (0.466) data 0.265 (0.335) loss_u loss_u 0.8145 (0.8423) acc_u 21.8750 (19.3750) lr 8.7467e-04 eta 0:00:18
epoch [110/200] batch [25/60] time 0.566 (0.471) data 0.434 (0.340) loss_u loss_u 0.8149 (0.8441) acc_u 25.0000 (19.1250) lr 8.7467e-04 eta 0:00:16
epoch [110/200] batch [30/60] time 0.569 (0.471) data 0.437 (0.340) loss_u loss_u 0.8291 (0.8438) acc_u 18.7500 (18.8542) lr 8.7467e-04 eta 0:00:14
epoch [110/200] batch [35/60] time 0.492 (0.475) data 0.360 (0.344) loss_u loss_u 0.8208 (0.8453) acc_u 25.0000 (18.9286) lr 8.7467e-04 eta 0:00:11
epoch [110/200] batch [40/60] time 0.357 (0.470) data 0.226 (0.339) loss_u loss_u 0.8535 (0.8455) acc_u 18.7500 (18.9062) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [45/60] time 0.428 (0.466) data 0.296 (0.335) loss_u loss_u 0.8750 (0.8494) acc_u 15.6250 (18.5417) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [50/60] time 0.402 (0.464) data 0.271 (0.332) loss_u loss_u 0.8726 (0.8529) acc_u 15.6250 (18.2500) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [55/60] time 0.437 (0.461) data 0.305 (0.329) loss_u loss_u 0.8101 (0.8527) acc_u 25.0000 (18.2955) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [60/60] time 0.468 (0.459) data 0.336 (0.327) loss_u loss_u 0.9150 (0.8537) acc_u 6.2500 (18.2292) lr 8.7467e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1428
confident_label rate tensor(0.4008, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1257
clean true:1177
clean false:80
clean_rate:0.9363564041368337
noisy true:531
noisy false:1348
after delete: len(clean_dataset) 1257
after delete: len(noisy_dataset) 1879
epoch [111/200] batch [5/39] time 0.386 (0.447) data 0.255 (0.316) loss_x loss_x 0.8423 (1.0083) acc_x 84.3750 (78.1250) lr 8.5910e-04 eta 0:00:15
epoch [111/200] batch [10/39] time 0.465 (0.472) data 0.334 (0.341) loss_x loss_x 1.3545 (1.0516) acc_x 75.0000 (75.6250) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [15/39] time 0.536 (0.468) data 0.402 (0.337) loss_x loss_x 1.2090 (1.0189) acc_x 71.8750 (75.4167) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [20/39] time 0.483 (0.486) data 0.352 (0.355) loss_x loss_x 1.6758 (1.1030) acc_x 62.5000 (73.4375) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [25/39] time 0.378 (0.482) data 0.248 (0.351) loss_x loss_x 1.0059 (1.0737) acc_x 81.2500 (74.5000) lr 8.5910e-04 eta 0:00:06
epoch [111/200] batch [30/39] time 0.486 (0.474) data 0.355 (0.344) loss_x loss_x 1.1807 (1.0958) acc_x 62.5000 (73.4375) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [35/39] time 0.514 (0.470) data 0.383 (0.339) loss_x loss_x 1.0674 (1.1040) acc_x 78.1250 (72.9464) lr 8.5910e-04 eta 0:00:01
epoch [111/200] batch [5/58] time 0.415 (0.470) data 0.284 (0.340) loss_u loss_u 0.8887 (0.8777) acc_u 15.6250 (16.8750) lr 8.5910e-04 eta 0:00:24
epoch [111/200] batch [10/58] time 0.471 (0.475) data 0.339 (0.344) loss_u loss_u 0.8154 (0.8633) acc_u 21.8750 (17.1875) lr 8.5910e-04 eta 0:00:22
epoch [111/200] batch [15/58] time 0.478 (0.476) data 0.344 (0.345) loss_u loss_u 0.8022 (0.8624) acc_u 28.1250 (17.2917) lr 8.5910e-04 eta 0:00:20
epoch [111/200] batch [20/58] time 0.436 (0.473) data 0.304 (0.342) loss_u loss_u 0.9097 (0.8648) acc_u 12.5000 (16.7188) lr 8.5910e-04 eta 0:00:17
epoch [111/200] batch [25/58] time 0.415 (0.471) data 0.284 (0.340) loss_u loss_u 0.8345 (0.8586) acc_u 18.7500 (17.6250) lr 8.5910e-04 eta 0:00:15
epoch [111/200] batch [30/58] time 0.402 (0.469) data 0.270 (0.338) loss_u loss_u 0.8550 (0.8533) acc_u 18.7500 (18.3333) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [35/58] time 0.518 (0.466) data 0.388 (0.334) loss_u loss_u 0.7852 (0.8527) acc_u 21.8750 (18.5714) lr 8.5910e-04 eta 0:00:10
epoch [111/200] batch [40/58] time 0.402 (0.465) data 0.270 (0.334) loss_u loss_u 0.8413 (0.8566) acc_u 18.7500 (17.8906) lr 8.5910e-04 eta 0:00:08
epoch [111/200] batch [45/58] time 0.613 (0.465) data 0.481 (0.334) loss_u loss_u 0.8550 (0.8572) acc_u 21.8750 (17.6389) lr 8.5910e-04 eta 0:00:06
epoch [111/200] batch [50/58] time 0.425 (0.463) data 0.294 (0.332) loss_u loss_u 0.8472 (0.8589) acc_u 21.8750 (17.3750) lr 8.5910e-04 eta 0:00:03
epoch [111/200] batch [55/58] time 0.560 (0.465) data 0.428 (0.334) loss_u loss_u 0.8677 (0.8581) acc_u 12.5000 (17.3295) lr 8.5910e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1457
confident_label rate tensor(0.3967, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1244
clean true:1166
clean false:78
clean_rate:0.9372990353697749
noisy true:513
noisy false:1379
after delete: len(clean_dataset) 1244
after delete: len(noisy_dataset) 1892
epoch [112/200] batch [5/38] time 0.431 (0.433) data 0.300 (0.303) loss_x loss_x 1.3740 (1.2604) acc_x 62.5000 (69.3750) lr 8.4357e-04 eta 0:00:14
epoch [112/200] batch [10/38] time 0.554 (0.460) data 0.424 (0.329) loss_x loss_x 1.6426 (1.2985) acc_x 53.1250 (67.5000) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [15/38] time 0.385 (0.449) data 0.255 (0.318) loss_x loss_x 1.0557 (1.2244) acc_x 71.8750 (69.7917) lr 8.4357e-04 eta 0:00:10
epoch [112/200] batch [20/38] time 0.338 (0.455) data 0.208 (0.324) loss_x loss_x 0.7622 (1.1322) acc_x 78.1250 (72.1875) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [25/38] time 0.501 (0.459) data 0.371 (0.328) loss_x loss_x 0.6738 (1.1271) acc_x 84.3750 (72.6250) lr 8.4357e-04 eta 0:00:05
epoch [112/200] batch [30/38] time 0.371 (0.460) data 0.241 (0.330) loss_x loss_x 1.3799 (1.1305) acc_x 59.3750 (71.7708) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [35/38] time 0.586 (0.463) data 0.456 (0.332) loss_x loss_x 0.9829 (1.1297) acc_x 68.7500 (71.6964) lr 8.4357e-04 eta 0:00:01
epoch [112/200] batch [5/59] time 0.471 (0.459) data 0.339 (0.328) loss_u loss_u 0.8584 (0.8538) acc_u 12.5000 (18.1250) lr 8.4357e-04 eta 0:00:24
epoch [112/200] batch [10/59] time 0.525 (0.461) data 0.391 (0.330) loss_u loss_u 0.9497 (0.8662) acc_u 3.1250 (15.9375) lr 8.4357e-04 eta 0:00:22
epoch [112/200] batch [15/59] time 0.400 (0.460) data 0.268 (0.329) loss_u loss_u 0.8770 (0.8564) acc_u 21.8750 (17.5000) lr 8.4357e-04 eta 0:00:20
epoch [112/200] batch [20/59] time 0.468 (0.460) data 0.336 (0.329) loss_u loss_u 0.8511 (0.8552) acc_u 15.6250 (17.5000) lr 8.4357e-04 eta 0:00:17
epoch [112/200] batch [25/59] time 0.496 (0.462) data 0.365 (0.331) loss_u loss_u 0.9297 (0.8583) acc_u 6.2500 (17.1250) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [30/59] time 0.638 (0.465) data 0.506 (0.334) loss_u loss_u 0.8081 (0.8587) acc_u 25.0000 (17.0833) lr 8.4357e-04 eta 0:00:13
epoch [112/200] batch [35/59] time 0.415 (0.464) data 0.283 (0.333) loss_u loss_u 0.8540 (0.8571) acc_u 12.5000 (17.0536) lr 8.4357e-04 eta 0:00:11
epoch [112/200] batch [40/59] time 0.412 (0.461) data 0.281 (0.330) loss_u loss_u 0.8335 (0.8571) acc_u 21.8750 (17.4219) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [45/59] time 0.384 (0.459) data 0.252 (0.328) loss_u loss_u 0.8774 (0.8582) acc_u 12.5000 (17.1528) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [50/59] time 0.379 (0.458) data 0.247 (0.327) loss_u loss_u 0.9067 (0.8560) acc_u 12.5000 (17.5625) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [55/59] time 0.459 (0.460) data 0.327 (0.329) loss_u loss_u 0.7681 (0.8533) acc_u 21.8750 (17.7841) lr 8.4357e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1484
confident_label rate tensor(0.3884, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1218
clean true:1135
clean false:83
clean_rate:0.9318555008210181
noisy true:517
noisy false:1401
after delete: len(clean_dataset) 1218
after delete: len(noisy_dataset) 1918
epoch [113/200] batch [5/38] time 0.531 (0.576) data 0.399 (0.445) loss_x loss_x 0.7617 (1.0021) acc_x 75.0000 (74.3750) lr 8.2807e-04 eta 0:00:19
epoch [113/200] batch [10/38] time 0.643 (0.531) data 0.512 (0.400) loss_x loss_x 1.2021 (1.1395) acc_x 65.6250 (72.1875) lr 8.2807e-04 eta 0:00:14
epoch [113/200] batch [15/38] time 0.422 (0.514) data 0.291 (0.383) loss_x loss_x 0.8975 (1.0919) acc_x 81.2500 (73.5417) lr 8.2807e-04 eta 0:00:11
epoch [113/200] batch [20/38] time 0.429 (0.501) data 0.299 (0.370) loss_x loss_x 0.7378 (1.1242) acc_x 78.1250 (71.8750) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [25/38] time 0.419 (0.492) data 0.288 (0.361) loss_x loss_x 1.1113 (1.1306) acc_x 78.1250 (72.1250) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [30/38] time 0.598 (0.483) data 0.467 (0.352) loss_x loss_x 1.4160 (1.1655) acc_x 68.7500 (71.6667) lr 8.2807e-04 eta 0:00:03
epoch [113/200] batch [35/38] time 0.345 (0.480) data 0.215 (0.349) loss_x loss_x 0.6621 (1.1532) acc_x 81.2500 (71.6071) lr 8.2807e-04 eta 0:00:01
epoch [113/200] batch [5/59] time 0.480 (0.465) data 0.349 (0.335) loss_u loss_u 0.7573 (0.8434) acc_u 25.0000 (17.5000) lr 8.2807e-04 eta 0:00:25
epoch [113/200] batch [10/59] time 0.389 (0.461) data 0.258 (0.330) loss_u loss_u 0.8408 (0.8457) acc_u 21.8750 (18.4375) lr 8.2807e-04 eta 0:00:22
epoch [113/200] batch [15/59] time 0.523 (0.463) data 0.391 (0.332) loss_u loss_u 0.8721 (0.8385) acc_u 12.5000 (19.3750) lr 8.2807e-04 eta 0:00:20
epoch [113/200] batch [20/59] time 0.420 (0.458) data 0.288 (0.327) loss_u loss_u 0.8535 (0.8335) acc_u 9.3750 (19.2188) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [25/59] time 0.502 (0.459) data 0.370 (0.328) loss_u loss_u 0.9272 (0.8410) acc_u 9.3750 (18.6250) lr 8.2807e-04 eta 0:00:15
epoch [113/200] batch [30/59] time 0.485 (0.460) data 0.354 (0.329) loss_u loss_u 0.7788 (0.8436) acc_u 34.3750 (18.9583) lr 8.2807e-04 eta 0:00:13
epoch [113/200] batch [35/59] time 0.441 (0.455) data 0.308 (0.323) loss_u loss_u 0.9434 (0.8462) acc_u 6.2500 (18.7500) lr 8.2807e-04 eta 0:00:10
epoch [113/200] batch [40/59] time 0.445 (0.453) data 0.313 (0.322) loss_u loss_u 0.8066 (0.8461) acc_u 25.0000 (18.6719) lr 8.2807e-04 eta 0:00:08
epoch [113/200] batch [45/59] time 0.389 (0.451) data 0.256 (0.320) loss_u loss_u 0.8843 (0.8493) acc_u 15.6250 (18.3333) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [50/59] time 0.418 (0.455) data 0.285 (0.323) loss_u loss_u 0.9023 (0.8515) acc_u 12.5000 (18.0625) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [55/59] time 0.501 (0.454) data 0.368 (0.323) loss_u loss_u 0.8228 (0.8489) acc_u 21.8750 (18.3523) lr 8.2807e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1450
confident_label rate tensor(0.3957, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1241
clean true:1160
clean false:81
clean_rate:0.934730056406124
noisy true:526
noisy false:1369
after delete: len(clean_dataset) 1241
after delete: len(noisy_dataset) 1895
epoch [114/200] batch [5/38] time 0.421 (0.475) data 0.290 (0.344) loss_x loss_x 0.7397 (1.0509) acc_x 84.3750 (70.6250) lr 8.1262e-04 eta 0:00:15
epoch [114/200] batch [10/38] time 0.416 (0.457) data 0.285 (0.326) loss_x loss_x 1.5059 (1.1236) acc_x 68.7500 (69.0625) lr 8.1262e-04 eta 0:00:12
epoch [114/200] batch [15/38] time 0.522 (0.452) data 0.392 (0.321) loss_x loss_x 1.0322 (1.1202) acc_x 71.8750 (69.7917) lr 8.1262e-04 eta 0:00:10
epoch [114/200] batch [20/38] time 0.415 (0.448) data 0.285 (0.317) loss_x loss_x 1.4932 (1.1735) acc_x 62.5000 (69.3750) lr 8.1262e-04 eta 0:00:08
epoch [114/200] batch [25/38] time 0.433 (0.448) data 0.302 (0.317) loss_x loss_x 1.1670 (1.1609) acc_x 75.0000 (70.3750) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [30/38] time 0.413 (0.452) data 0.283 (0.321) loss_x loss_x 0.9531 (1.1554) acc_x 75.0000 (70.1042) lr 8.1262e-04 eta 0:00:03
epoch [114/200] batch [35/38] time 0.565 (0.461) data 0.433 (0.330) loss_x loss_x 0.8730 (1.1578) acc_x 75.0000 (70.1786) lr 8.1262e-04 eta 0:00:01
epoch [114/200] batch [5/59] time 0.623 (0.475) data 0.491 (0.344) loss_u loss_u 0.8174 (0.8671) acc_u 31.2500 (15.6250) lr 8.1262e-04 eta 0:00:25
epoch [114/200] batch [10/59] time 0.418 (0.473) data 0.287 (0.342) loss_u loss_u 0.8062 (0.8409) acc_u 21.8750 (19.0625) lr 8.1262e-04 eta 0:00:23
epoch [114/200] batch [15/59] time 0.403 (0.472) data 0.272 (0.341) loss_u loss_u 0.8193 (0.8452) acc_u 21.8750 (18.5417) lr 8.1262e-04 eta 0:00:20
epoch [114/200] batch [20/59] time 0.546 (0.472) data 0.414 (0.341) loss_u loss_u 0.9141 (0.8458) acc_u 9.3750 (18.4375) lr 8.1262e-04 eta 0:00:18
epoch [114/200] batch [25/59] time 0.542 (0.473) data 0.410 (0.342) loss_u loss_u 0.9165 (0.8466) acc_u 12.5000 (18.1250) lr 8.1262e-04 eta 0:00:16
epoch [114/200] batch [30/59] time 0.502 (0.472) data 0.371 (0.341) loss_u loss_u 0.8271 (0.8429) acc_u 18.7500 (18.8542) lr 8.1262e-04 eta 0:00:13
epoch [114/200] batch [35/59] time 0.397 (0.470) data 0.265 (0.338) loss_u loss_u 0.8735 (0.8448) acc_u 21.8750 (19.1964) lr 8.1262e-04 eta 0:00:11
epoch [114/200] batch [40/59] time 0.423 (0.471) data 0.293 (0.340) loss_u loss_u 0.8408 (0.8435) acc_u 18.7500 (19.3750) lr 8.1262e-04 eta 0:00:08
epoch [114/200] batch [45/59] time 0.462 (0.471) data 0.331 (0.340) loss_u loss_u 0.8794 (0.8478) acc_u 21.8750 (18.8889) lr 8.1262e-04 eta 0:00:06
epoch [114/200] batch [50/59] time 0.411 (0.469) data 0.279 (0.338) loss_u loss_u 0.7529 (0.8479) acc_u 28.1250 (18.7500) lr 8.1262e-04 eta 0:00:04
epoch [114/200] batch [55/59] time 0.405 (0.468) data 0.273 (0.337) loss_u loss_u 0.9307 (0.8487) acc_u 6.2500 (18.6932) lr 8.1262e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1436
confident_label rate tensor(0.3948, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1238
clean true:1163
clean false:75
clean_rate:0.9394184168012925
noisy true:537
noisy false:1361
after delete: len(clean_dataset) 1238
after delete: len(noisy_dataset) 1898
epoch [115/200] batch [5/38] time 0.563 (0.445) data 0.431 (0.314) loss_x loss_x 0.7681 (1.1896) acc_x 84.3750 (72.5000) lr 7.9721e-04 eta 0:00:14
epoch [115/200] batch [10/38] time 0.443 (0.471) data 0.313 (0.340) loss_x loss_x 0.9956 (1.1665) acc_x 71.8750 (70.9375) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [15/38] time 0.456 (0.474) data 0.326 (0.343) loss_x loss_x 1.6543 (1.1850) acc_x 59.3750 (70.8333) lr 7.9721e-04 eta 0:00:10
epoch [115/200] batch [20/38] time 0.464 (0.475) data 0.335 (0.344) loss_x loss_x 1.0625 (1.2041) acc_x 68.7500 (71.8750) lr 7.9721e-04 eta 0:00:08
epoch [115/200] batch [25/38] time 0.481 (0.480) data 0.351 (0.350) loss_x loss_x 1.6631 (1.2368) acc_x 65.6250 (71.1250) lr 7.9721e-04 eta 0:00:06
epoch [115/200] batch [30/38] time 0.592 (0.488) data 0.461 (0.357) loss_x loss_x 1.1523 (1.2155) acc_x 71.8750 (71.1458) lr 7.9721e-04 eta 0:00:03
epoch [115/200] batch [35/38] time 0.590 (0.490) data 0.459 (0.359) loss_x loss_x 1.0352 (1.1991) acc_x 75.0000 (71.6964) lr 7.9721e-04 eta 0:00:01
epoch [115/200] batch [5/59] time 0.459 (0.479) data 0.327 (0.348) loss_u loss_u 0.9365 (0.8680) acc_u 9.3750 (16.2500) lr 7.9721e-04 eta 0:00:25
epoch [115/200] batch [10/59] time 0.366 (0.482) data 0.235 (0.351) loss_u loss_u 0.8931 (0.8569) acc_u 12.5000 (16.8750) lr 7.9721e-04 eta 0:00:23
epoch [115/200] batch [15/59] time 0.450 (0.484) data 0.317 (0.353) loss_u loss_u 0.8105 (0.8517) acc_u 31.2500 (18.1250) lr 7.9721e-04 eta 0:00:21
epoch [115/200] batch [20/59] time 0.404 (0.478) data 0.273 (0.347) loss_u loss_u 0.7505 (0.8404) acc_u 31.2500 (20.0000) lr 7.9721e-04 eta 0:00:18
epoch [115/200] batch [25/59] time 0.441 (0.473) data 0.309 (0.341) loss_u loss_u 0.8525 (0.8413) acc_u 18.7500 (19.8750) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [30/59] time 0.541 (0.473) data 0.410 (0.341) loss_u loss_u 0.8740 (0.8457) acc_u 15.6250 (19.4792) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [35/59] time 0.562 (0.469) data 0.431 (0.338) loss_u loss_u 0.7832 (0.8406) acc_u 31.2500 (20.1786) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [40/59] time 0.443 (0.467) data 0.313 (0.335) loss_u loss_u 0.8438 (0.8420) acc_u 18.7500 (19.9219) lr 7.9721e-04 eta 0:00:08
epoch [115/200] batch [45/59] time 0.486 (0.464) data 0.355 (0.333) loss_u loss_u 0.8765 (0.8449) acc_u 18.7500 (19.6528) lr 7.9721e-04 eta 0:00:06
epoch [115/200] batch [50/59] time 0.400 (0.462) data 0.269 (0.331) loss_u loss_u 0.8384 (0.8438) acc_u 18.7500 (19.8750) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [55/59] time 0.452 (0.464) data 0.320 (0.332) loss_u loss_u 0.8330 (0.8439) acc_u 25.0000 (19.7159) lr 7.9721e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1435
confident_label rate tensor(0.3970, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1245
clean true:1163
clean false:82
clean_rate:0.934136546184739
noisy true:538
noisy false:1353
after delete: len(clean_dataset) 1245
after delete: len(noisy_dataset) 1891
epoch [116/200] batch [5/38] time 0.404 (0.451) data 0.274 (0.321) loss_x loss_x 0.8115 (1.2748) acc_x 81.2500 (66.8750) lr 7.8186e-04 eta 0:00:14
epoch [116/200] batch [10/38] time 0.552 (0.465) data 0.419 (0.334) loss_x loss_x 0.7822 (1.1775) acc_x 78.1250 (69.0625) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [15/38] time 0.405 (0.486) data 0.274 (0.355) loss_x loss_x 1.3047 (1.0879) acc_x 43.7500 (70.8333) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [20/38] time 0.474 (0.477) data 0.343 (0.346) loss_x loss_x 1.1328 (1.1311) acc_x 71.8750 (70.1562) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [25/38] time 0.680 (0.483) data 0.549 (0.352) loss_x loss_x 1.1689 (1.1427) acc_x 78.1250 (70.7500) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [30/38] time 0.338 (0.488) data 0.208 (0.357) loss_x loss_x 1.2295 (1.1420) acc_x 78.1250 (70.5208) lr 7.8186e-04 eta 0:00:03
epoch [116/200] batch [35/38] time 0.499 (0.487) data 0.369 (0.356) loss_x loss_x 1.3174 (1.1430) acc_x 62.5000 (70.9821) lr 7.8186e-04 eta 0:00:01
epoch [116/200] batch [5/59] time 0.518 (0.479) data 0.387 (0.348) loss_u loss_u 0.7339 (0.8289) acc_u 31.2500 (21.2500) lr 7.8186e-04 eta 0:00:25
epoch [116/200] batch [10/59] time 0.477 (0.474) data 0.344 (0.343) loss_u loss_u 0.8403 (0.8527) acc_u 18.7500 (17.5000) lr 7.8186e-04 eta 0:00:23
epoch [116/200] batch [15/59] time 0.429 (0.467) data 0.298 (0.336) loss_u loss_u 0.8906 (0.8570) acc_u 15.6250 (17.2917) lr 7.8186e-04 eta 0:00:20
epoch [116/200] batch [20/59] time 0.485 (0.464) data 0.354 (0.333) loss_u loss_u 0.8945 (0.8537) acc_u 12.5000 (17.6562) lr 7.8186e-04 eta 0:00:18
epoch [116/200] batch [25/59] time 0.389 (0.462) data 0.259 (0.331) loss_u loss_u 0.8892 (0.8566) acc_u 12.5000 (17.1250) lr 7.8186e-04 eta 0:00:15
epoch [116/200] batch [30/59] time 0.556 (0.460) data 0.426 (0.329) loss_u loss_u 0.8838 (0.8548) acc_u 15.6250 (17.5000) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [35/59] time 0.422 (0.460) data 0.290 (0.329) loss_u loss_u 0.8145 (0.8529) acc_u 34.3750 (18.3929) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [40/59] time 0.540 (0.463) data 0.407 (0.332) loss_u loss_u 0.8672 (0.8530) acc_u 21.8750 (18.2031) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [45/59] time 0.460 (0.464) data 0.327 (0.332) loss_u loss_u 0.8838 (0.8501) acc_u 12.5000 (18.4028) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [50/59] time 0.440 (0.464) data 0.307 (0.333) loss_u loss_u 0.8779 (0.8505) acc_u 15.6250 (18.6875) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [55/59] time 0.454 (0.461) data 0.322 (0.330) loss_u loss_u 0.8975 (0.8537) acc_u 12.5000 (18.1818) lr 7.8186e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1469
confident_label rate tensor(0.3945, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1237
clean true:1160
clean false:77
clean_rate:0.9377526273241714
noisy true:507
noisy false:1392
after delete: len(clean_dataset) 1237
after delete: len(noisy_dataset) 1899
epoch [117/200] batch [5/38] time 0.408 (0.458) data 0.277 (0.327) loss_x loss_x 1.2061 (0.9958) acc_x 71.8750 (75.6250) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [10/38] time 0.423 (0.452) data 0.291 (0.321) loss_x loss_x 1.2695 (1.0989) acc_x 75.0000 (73.1250) lr 7.6655e-04 eta 0:00:12
epoch [117/200] batch [15/38] time 0.423 (0.474) data 0.290 (0.342) loss_x loss_x 1.4756 (1.1585) acc_x 65.6250 (71.6667) lr 7.6655e-04 eta 0:00:10
epoch [117/200] batch [20/38] time 0.436 (0.504) data 0.305 (0.372) loss_x loss_x 1.1631 (1.1613) acc_x 75.0000 (70.9375) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [25/38] time 0.379 (0.502) data 0.248 (0.371) loss_x loss_x 1.0918 (1.1235) acc_x 78.1250 (72.7500) lr 7.6655e-04 eta 0:00:06
epoch [117/200] batch [30/38] time 0.443 (0.499) data 0.313 (0.368) loss_x loss_x 1.3418 (1.1650) acc_x 68.7500 (71.7708) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [35/38] time 0.350 (0.493) data 0.219 (0.362) loss_x loss_x 1.6182 (1.1648) acc_x 56.2500 (71.6071) lr 7.6655e-04 eta 0:00:01
epoch [117/200] batch [5/59] time 0.491 (0.487) data 0.360 (0.356) loss_u loss_u 0.9116 (0.8743) acc_u 9.3750 (13.7500) lr 7.6655e-04 eta 0:00:26
epoch [117/200] batch [10/59] time 0.363 (0.482) data 0.232 (0.351) loss_u loss_u 0.8623 (0.8834) acc_u 15.6250 (13.4375) lr 7.6655e-04 eta 0:00:23
epoch [117/200] batch [15/59] time 0.476 (0.480) data 0.344 (0.349) loss_u loss_u 0.8862 (0.8734) acc_u 18.7500 (14.7917) lr 7.6655e-04 eta 0:00:21
epoch [117/200] batch [20/59] time 0.503 (0.478) data 0.371 (0.346) loss_u loss_u 0.8896 (0.8681) acc_u 12.5000 (16.0938) lr 7.6655e-04 eta 0:00:18
epoch [117/200] batch [25/59] time 0.520 (0.477) data 0.388 (0.346) loss_u loss_u 0.8501 (0.8664) acc_u 21.8750 (16.2500) lr 7.6655e-04 eta 0:00:16
epoch [117/200] batch [30/59] time 0.444 (0.473) data 0.313 (0.341) loss_u loss_u 0.8730 (0.8641) acc_u 15.6250 (16.8750) lr 7.6655e-04 eta 0:00:13
epoch [117/200] batch [35/59] time 0.436 (0.470) data 0.305 (0.339) loss_u loss_u 0.8462 (0.8570) acc_u 21.8750 (17.7679) lr 7.6655e-04 eta 0:00:11
epoch [117/200] batch [40/59] time 0.347 (0.470) data 0.214 (0.339) loss_u loss_u 0.9009 (0.8571) acc_u 9.3750 (17.5781) lr 7.6655e-04 eta 0:00:08
epoch [117/200] batch [45/59] time 0.423 (0.468) data 0.290 (0.337) loss_u loss_u 0.9023 (0.8563) acc_u 12.5000 (17.7083) lr 7.6655e-04 eta 0:00:06
epoch [117/200] batch [50/59] time 0.487 (0.467) data 0.356 (0.335) loss_u loss_u 0.7563 (0.8542) acc_u 28.1250 (18.0625) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [55/59] time 0.384 (0.464) data 0.253 (0.333) loss_u loss_u 0.8477 (0.8492) acc_u 21.8750 (18.8636) lr 7.6655e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1460
confident_label rate tensor(0.3957, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1241
clean true:1169
clean false:72
clean_rate:0.9419822723609992
noisy true:507
noisy false:1388
after delete: len(clean_dataset) 1241
after delete: len(noisy_dataset) 1895
epoch [118/200] batch [5/38] time 0.386 (0.497) data 0.255 (0.367) loss_x loss_x 1.0674 (1.1435) acc_x 78.1250 (71.8750) lr 7.5131e-04 eta 0:00:16
epoch [118/200] batch [10/38] time 0.410 (0.482) data 0.279 (0.351) loss_x loss_x 1.1865 (1.2572) acc_x 65.6250 (67.1875) lr 7.5131e-04 eta 0:00:13
epoch [118/200] batch [15/38] time 0.459 (0.469) data 0.329 (0.338) loss_x loss_x 1.3311 (1.1707) acc_x 65.6250 (70.4167) lr 7.5131e-04 eta 0:00:10
epoch [118/200] batch [20/38] time 0.401 (0.460) data 0.271 (0.330) loss_x loss_x 1.4609 (1.2215) acc_x 62.5000 (70.1562) lr 7.5131e-04 eta 0:00:08
epoch [118/200] batch [25/38] time 0.655 (0.466) data 0.524 (0.335) loss_x loss_x 1.3535 (1.2263) acc_x 71.8750 (70.2500) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [30/38] time 0.405 (0.459) data 0.274 (0.329) loss_x loss_x 1.0186 (1.2198) acc_x 81.2500 (70.1042) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [35/38] time 0.523 (0.457) data 0.391 (0.326) loss_x loss_x 1.9023 (1.2383) acc_x 56.2500 (70.2679) lr 7.5131e-04 eta 0:00:01
epoch [118/200] batch [5/59] time 0.505 (0.457) data 0.371 (0.326) loss_u loss_u 0.8750 (0.8571) acc_u 15.6250 (17.5000) lr 7.5131e-04 eta 0:00:24
epoch [118/200] batch [10/59] time 0.373 (0.450) data 0.241 (0.319) loss_u loss_u 0.8232 (0.8358) acc_u 18.7500 (19.6875) lr 7.5131e-04 eta 0:00:22
epoch [118/200] batch [15/59] time 0.460 (0.452) data 0.329 (0.320) loss_u loss_u 0.8115 (0.8361) acc_u 21.8750 (19.5833) lr 7.5131e-04 eta 0:00:19
epoch [118/200] batch [20/59] time 0.458 (0.452) data 0.326 (0.320) loss_u loss_u 0.8623 (0.8477) acc_u 15.6250 (17.9688) lr 7.5131e-04 eta 0:00:17
epoch [118/200] batch [25/59] time 0.502 (0.452) data 0.369 (0.320) loss_u loss_u 0.8813 (0.8558) acc_u 15.6250 (17.2500) lr 7.5131e-04 eta 0:00:15
epoch [118/200] batch [30/59] time 0.737 (0.459) data 0.605 (0.328) loss_u loss_u 0.8677 (0.8585) acc_u 15.6250 (17.0833) lr 7.5131e-04 eta 0:00:13
epoch [118/200] batch [35/59] time 0.376 (0.459) data 0.245 (0.327) loss_u loss_u 0.8481 (0.8545) acc_u 18.7500 (17.3214) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [40/59] time 0.495 (0.457) data 0.363 (0.326) loss_u loss_u 0.8022 (0.8521) acc_u 25.0000 (17.8906) lr 7.5131e-04 eta 0:00:08
epoch [118/200] batch [45/59] time 0.440 (0.455) data 0.309 (0.324) loss_u loss_u 0.8389 (0.8531) acc_u 18.7500 (17.9167) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [50/59] time 0.362 (0.452) data 0.231 (0.321) loss_u loss_u 0.8071 (0.8535) acc_u 28.1250 (18.1250) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [55/59] time 0.377 (0.449) data 0.245 (0.318) loss_u loss_u 0.8867 (0.8546) acc_u 18.7500 (18.1818) lr 7.5131e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1457
confident_label rate tensor(0.3935, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1234
clean true:1146
clean false:88
clean_rate:0.9286871961102107
noisy true:533
noisy false:1369
after delete: len(clean_dataset) 1234
after delete: len(noisy_dataset) 1902
epoch [119/200] batch [5/38] time 0.333 (0.418) data 0.203 (0.288) loss_x loss_x 1.3740 (1.1296) acc_x 71.8750 (68.1250) lr 7.3613e-04 eta 0:00:13
epoch [119/200] batch [10/38] time 0.342 (0.423) data 0.212 (0.292) loss_x loss_x 1.1699 (1.1753) acc_x 65.6250 (68.7500) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [15/38] time 0.577 (0.425) data 0.446 (0.294) loss_x loss_x 1.0088 (1.0765) acc_x 71.8750 (71.8750) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [20/38] time 0.606 (0.444) data 0.475 (0.314) loss_x loss_x 1.0000 (1.0951) acc_x 75.0000 (72.5000) lr 7.3613e-04 eta 0:00:07
epoch [119/200] batch [25/38] time 0.403 (0.456) data 0.272 (0.325) loss_x loss_x 1.3838 (1.0869) acc_x 68.7500 (72.8750) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [30/38] time 0.471 (0.461) data 0.340 (0.330) loss_x loss_x 0.7227 (1.0926) acc_x 78.1250 (72.6042) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [35/38] time 0.511 (0.465) data 0.380 (0.334) loss_x loss_x 1.1279 (1.0883) acc_x 65.6250 (72.5893) lr 7.3613e-04 eta 0:00:01
epoch [119/200] batch [5/59] time 0.418 (0.461) data 0.286 (0.330) loss_u loss_u 0.8828 (0.8607) acc_u 12.5000 (16.2500) lr 7.3613e-04 eta 0:00:24
epoch [119/200] batch [10/59] time 0.410 (0.460) data 0.278 (0.329) loss_u loss_u 0.8887 (0.8422) acc_u 9.3750 (18.1250) lr 7.3613e-04 eta 0:00:22
epoch [119/200] batch [15/59] time 0.693 (0.465) data 0.562 (0.333) loss_u loss_u 0.8018 (0.8347) acc_u 28.1250 (20.4167) lr 7.3613e-04 eta 0:00:20
epoch [119/200] batch [20/59] time 0.456 (0.465) data 0.325 (0.334) loss_u loss_u 0.8916 (0.8315) acc_u 9.3750 (20.7812) lr 7.3613e-04 eta 0:00:18
epoch [119/200] batch [25/59] time 0.460 (0.465) data 0.328 (0.334) loss_u loss_u 0.8550 (0.8362) acc_u 12.5000 (19.8750) lr 7.3613e-04 eta 0:00:15
epoch [119/200] batch [30/59] time 0.364 (0.461) data 0.233 (0.330) loss_u loss_u 0.9438 (0.8390) acc_u 3.1250 (19.3750) lr 7.3613e-04 eta 0:00:13
epoch [119/200] batch [35/59] time 0.425 (0.464) data 0.294 (0.333) loss_u loss_u 0.8174 (0.8387) acc_u 21.8750 (19.3750) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [40/59] time 0.452 (0.459) data 0.321 (0.328) loss_u loss_u 0.9082 (0.8406) acc_u 12.5000 (19.1406) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [45/59] time 0.421 (0.456) data 0.290 (0.325) loss_u loss_u 0.8535 (0.8428) acc_u 21.8750 (19.2361) lr 7.3613e-04 eta 0:00:06
epoch [119/200] batch [50/59] time 0.419 (0.455) data 0.288 (0.323) loss_u loss_u 0.8306 (0.8412) acc_u 21.8750 (19.5000) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [55/59] time 0.353 (0.451) data 0.223 (0.320) loss_u loss_u 0.8027 (0.8418) acc_u 25.0000 (19.3750) lr 7.3613e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1450
confident_label rate tensor(0.3922, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1230
clean true:1160
clean false:70
clean_rate:0.943089430894309
noisy true:526
noisy false:1380
after delete: len(clean_dataset) 1230
after delete: len(noisy_dataset) 1906
epoch [120/200] batch [5/38] time 0.598 (0.435) data 0.468 (0.304) loss_x loss_x 1.0898 (1.4113) acc_x 71.8750 (66.2500) lr 7.2101e-04 eta 0:00:14
epoch [120/200] batch [10/38] time 0.427 (0.492) data 0.296 (0.361) loss_x loss_x 0.8110 (1.2348) acc_x 75.0000 (69.0625) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [15/38] time 0.446 (0.483) data 0.315 (0.352) loss_x loss_x 1.0117 (1.1971) acc_x 71.8750 (69.1667) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [20/38] time 0.446 (0.474) data 0.315 (0.343) loss_x loss_x 1.1045 (1.2008) acc_x 65.6250 (68.9062) lr 7.2101e-04 eta 0:00:08
epoch [120/200] batch [25/38] time 0.421 (0.467) data 0.290 (0.336) loss_x loss_x 0.9463 (1.2152) acc_x 75.0000 (67.7500) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [30/38] time 0.555 (0.468) data 0.424 (0.337) loss_x loss_x 1.5156 (1.2175) acc_x 68.7500 (68.7500) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [35/38] time 0.405 (0.460) data 0.274 (0.329) loss_x loss_x 1.0742 (1.2108) acc_x 81.2500 (69.0179) lr 7.2101e-04 eta 0:00:01
epoch [120/200] batch [5/59] time 0.515 (0.457) data 0.383 (0.326) loss_u loss_u 0.8125 (0.8464) acc_u 21.8750 (19.3750) lr 7.2101e-04 eta 0:00:24
epoch [120/200] batch [10/59] time 0.439 (0.454) data 0.308 (0.323) loss_u loss_u 0.8101 (0.8458) acc_u 25.0000 (18.4375) lr 7.2101e-04 eta 0:00:22
epoch [120/200] batch [15/59] time 0.370 (0.450) data 0.239 (0.319) loss_u loss_u 0.8247 (0.8459) acc_u 21.8750 (18.7500) lr 7.2101e-04 eta 0:00:19
epoch [120/200] batch [20/59] time 0.400 (0.452) data 0.269 (0.321) loss_u loss_u 0.7915 (0.8395) acc_u 28.1250 (19.3750) lr 7.2101e-04 eta 0:00:17
epoch [120/200] batch [25/59] time 0.450 (0.458) data 0.319 (0.327) loss_u loss_u 0.7876 (0.8418) acc_u 25.0000 (19.2500) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [30/59] time 0.387 (0.460) data 0.255 (0.329) loss_u loss_u 0.8535 (0.8444) acc_u 21.8750 (18.8542) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [35/59] time 0.391 (0.458) data 0.260 (0.327) loss_u loss_u 0.9092 (0.8490) acc_u 9.3750 (18.1250) lr 7.2101e-04 eta 0:00:10
epoch [120/200] batch [40/59] time 0.415 (0.457) data 0.284 (0.326) loss_u loss_u 0.8174 (0.8515) acc_u 18.7500 (18.0469) lr 7.2101e-04 eta 0:00:08
epoch [120/200] batch [45/59] time 0.378 (0.453) data 0.246 (0.322) loss_u loss_u 0.8389 (0.8527) acc_u 18.7500 (17.8472) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [50/59] time 0.461 (0.453) data 0.330 (0.322) loss_u loss_u 0.9048 (0.8526) acc_u 12.5000 (17.9375) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [55/59] time 0.454 (0.451) data 0.323 (0.320) loss_u loss_u 0.8711 (0.8540) acc_u 12.5000 (17.6705) lr 7.2101e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1456
confident_label rate tensor(0.3925, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1231
clean true:1147
clean false:84
clean_rate:0.9317627944760357
noisy true:533
noisy false:1372
after delete: len(clean_dataset) 1231
after delete: len(noisy_dataset) 1905
epoch [121/200] batch [5/38] time 0.400 (0.414) data 0.270 (0.284) loss_x loss_x 1.2285 (1.2050) acc_x 75.0000 (72.5000) lr 7.0596e-04 eta 0:00:13
epoch [121/200] batch [10/38] time 0.576 (0.423) data 0.445 (0.292) loss_x loss_x 1.4414 (1.1862) acc_x 65.6250 (73.4375) lr 7.0596e-04 eta 0:00:11
epoch [121/200] batch [15/38] time 0.668 (0.449) data 0.538 (0.318) loss_x loss_x 1.1914 (1.1577) acc_x 65.6250 (72.5000) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [20/38] time 0.568 (0.464) data 0.437 (0.333) loss_x loss_x 1.0605 (1.1721) acc_x 62.5000 (70.7812) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [25/38] time 0.630 (0.472) data 0.500 (0.341) loss_x loss_x 1.2627 (1.2106) acc_x 71.8750 (70.0000) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [30/38] time 0.427 (0.467) data 0.297 (0.337) loss_x loss_x 0.9707 (1.1757) acc_x 78.1250 (70.9375) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [35/38] time 0.402 (0.465) data 0.271 (0.334) loss_x loss_x 0.7632 (1.1705) acc_x 84.3750 (70.8036) lr 7.0596e-04 eta 0:00:01
epoch [121/200] batch [5/59] time 0.365 (0.474) data 0.234 (0.343) loss_u loss_u 0.8843 (0.8446) acc_u 12.5000 (21.8750) lr 7.0596e-04 eta 0:00:25
epoch [121/200] batch [10/59] time 0.377 (0.469) data 0.246 (0.338) loss_u loss_u 0.9282 (0.8396) acc_u 6.2500 (21.2500) lr 7.0596e-04 eta 0:00:22
epoch [121/200] batch [15/59] time 0.416 (0.466) data 0.284 (0.335) loss_u loss_u 0.8184 (0.8331) acc_u 21.8750 (21.8750) lr 7.0596e-04 eta 0:00:20
epoch [121/200] batch [20/59] time 0.392 (0.463) data 0.262 (0.332) loss_u loss_u 0.9126 (0.8357) acc_u 12.5000 (21.5625) lr 7.0596e-04 eta 0:00:18
epoch [121/200] batch [25/59] time 0.324 (0.461) data 0.194 (0.330) loss_u loss_u 0.8467 (0.8369) acc_u 15.6250 (21.2500) lr 7.0596e-04 eta 0:00:15
epoch [121/200] batch [30/59] time 0.402 (0.460) data 0.271 (0.329) loss_u loss_u 0.8315 (0.8378) acc_u 21.8750 (20.5208) lr 7.0596e-04 eta 0:00:13
epoch [121/200] batch [35/59] time 0.375 (0.459) data 0.244 (0.328) loss_u loss_u 0.7803 (0.8384) acc_u 25.0000 (20.1786) lr 7.0596e-04 eta 0:00:11
epoch [121/200] batch [40/59] time 0.385 (0.459) data 0.253 (0.328) loss_u loss_u 0.8564 (0.8435) acc_u 18.7500 (19.6875) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [45/59] time 0.446 (0.456) data 0.314 (0.325) loss_u loss_u 0.8120 (0.8429) acc_u 25.0000 (19.6528) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [50/59] time 0.434 (0.455) data 0.302 (0.323) loss_u loss_u 0.8813 (0.8449) acc_u 15.6250 (19.3750) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [55/59] time 0.400 (0.454) data 0.269 (0.323) loss_u loss_u 0.8364 (0.8439) acc_u 21.8750 (19.3750) lr 7.0596e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1461
confident_label rate tensor(0.3970, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1245
clean true:1159
clean false:86
clean_rate:0.9309236947791165
noisy true:516
noisy false:1375
after delete: len(clean_dataset) 1245
after delete: len(noisy_dataset) 1891
epoch [122/200] batch [5/38] time 0.359 (0.388) data 0.229 (0.257) loss_x loss_x 0.6694 (1.0629) acc_x 78.1250 (70.6250) lr 6.9098e-04 eta 0:00:12
epoch [122/200] batch [10/38] time 0.352 (0.430) data 0.222 (0.300) loss_x loss_x 0.7485 (1.0000) acc_x 87.5000 (75.3125) lr 6.9098e-04 eta 0:00:12
epoch [122/200] batch [15/38] time 0.389 (0.445) data 0.258 (0.314) loss_x loss_x 1.5127 (1.0318) acc_x 71.8750 (76.0417) lr 6.9098e-04 eta 0:00:10
epoch [122/200] batch [20/38] time 0.516 (0.465) data 0.385 (0.334) loss_x loss_x 1.3037 (1.0635) acc_x 62.5000 (74.6875) lr 6.9098e-04 eta 0:00:08
epoch [122/200] batch [25/38] time 0.431 (0.469) data 0.300 (0.338) loss_x loss_x 1.1768 (1.0725) acc_x 75.0000 (73.8750) lr 6.9098e-04 eta 0:00:06
epoch [122/200] batch [30/38] time 0.562 (0.479) data 0.430 (0.348) loss_x loss_x 1.2939 (1.0695) acc_x 71.8750 (73.5417) lr 6.9098e-04 eta 0:00:03
epoch [122/200] batch [35/38] time 0.506 (0.484) data 0.375 (0.353) loss_x loss_x 1.5059 (1.1077) acc_x 59.3750 (73.1250) lr 6.9098e-04 eta 0:00:01
epoch [122/200] batch [5/59] time 0.354 (0.473) data 0.223 (0.342) loss_u loss_u 0.7925 (0.7995) acc_u 34.3750 (25.6250) lr 6.9098e-04 eta 0:00:25
epoch [122/200] batch [10/59] time 0.555 (0.473) data 0.424 (0.342) loss_u loss_u 0.9058 (0.8327) acc_u 12.5000 (20.3125) lr 6.9098e-04 eta 0:00:23
epoch [122/200] batch [15/59] time 0.391 (0.473) data 0.259 (0.343) loss_u loss_u 0.8872 (0.8399) acc_u 15.6250 (19.7917) lr 6.9098e-04 eta 0:00:20
epoch [122/200] batch [20/59] time 0.399 (0.476) data 0.268 (0.345) loss_u loss_u 0.7178 (0.8303) acc_u 31.2500 (20.4688) lr 6.9098e-04 eta 0:00:18
epoch [122/200] batch [25/59] time 0.355 (0.473) data 0.223 (0.342) loss_u loss_u 0.8706 (0.8341) acc_u 12.5000 (19.7500) lr 6.9098e-04 eta 0:00:16
epoch [122/200] batch [30/59] time 0.486 (0.468) data 0.354 (0.337) loss_u loss_u 0.8364 (0.8326) acc_u 25.0000 (20.5208) lr 6.9098e-04 eta 0:00:13
epoch [122/200] batch [35/59] time 0.347 (0.463) data 0.217 (0.332) loss_u loss_u 0.7969 (0.8365) acc_u 25.0000 (19.8214) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [40/59] time 0.387 (0.458) data 0.256 (0.327) loss_u loss_u 0.8564 (0.8383) acc_u 12.5000 (19.6094) lr 6.9098e-04 eta 0:00:08
epoch [122/200] batch [45/59] time 0.420 (0.457) data 0.290 (0.326) loss_u loss_u 0.6890 (0.8336) acc_u 40.6250 (20.2083) lr 6.9098e-04 eta 0:00:06
epoch [122/200] batch [50/59] time 0.425 (0.456) data 0.294 (0.325) loss_u loss_u 0.8228 (0.8358) acc_u 28.1250 (20.1875) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [55/59] time 0.352 (0.456) data 0.222 (0.325) loss_u loss_u 0.8726 (0.8389) acc_u 15.6250 (20.0000) lr 6.9098e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1467
confident_label rate tensor(0.3973, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1246
clean true:1148
clean false:98
clean_rate:0.9213483146067416
noisy true:521
noisy false:1369
after delete: len(clean_dataset) 1246
after delete: len(noisy_dataset) 1890
epoch [123/200] batch [5/38] time 0.476 (0.444) data 0.345 (0.313) loss_x loss_x 0.9766 (0.9530) acc_x 75.0000 (76.8750) lr 6.7608e-04 eta 0:00:14
epoch [123/200] batch [10/38] time 0.593 (0.483) data 0.462 (0.352) loss_x loss_x 1.2803 (1.1119) acc_x 71.8750 (74.3750) lr 6.7608e-04 eta 0:00:13
epoch [123/200] batch [15/38] time 0.356 (0.490) data 0.226 (0.359) loss_x loss_x 0.6548 (1.1074) acc_x 84.3750 (74.1667) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [20/38] time 0.330 (0.477) data 0.199 (0.346) loss_x loss_x 1.1924 (1.0682) acc_x 65.6250 (74.2188) lr 6.7608e-04 eta 0:00:08
epoch [123/200] batch [25/38] time 0.395 (0.465) data 0.264 (0.334) loss_x loss_x 1.2178 (1.1000) acc_x 56.2500 (73.2500) lr 6.7608e-04 eta 0:00:06
epoch [123/200] batch [30/38] time 0.439 (0.466) data 0.308 (0.335) loss_x loss_x 0.8350 (1.0626) acc_x 78.1250 (74.1667) lr 6.7608e-04 eta 0:00:03
epoch [123/200] batch [35/38] time 0.629 (0.468) data 0.499 (0.337) loss_x loss_x 1.5928 (1.0876) acc_x 62.5000 (73.6607) lr 6.7608e-04 eta 0:00:01
epoch [123/200] batch [5/59] time 0.400 (0.471) data 0.269 (0.340) loss_u loss_u 0.7471 (0.8479) acc_u 31.2500 (18.1250) lr 6.7608e-04 eta 0:00:25
epoch [123/200] batch [10/59] time 0.418 (0.466) data 0.286 (0.335) loss_u loss_u 0.8086 (0.8530) acc_u 28.1250 (18.1250) lr 6.7608e-04 eta 0:00:22
epoch [123/200] batch [15/59] time 0.410 (0.462) data 0.277 (0.331) loss_u loss_u 0.8110 (0.8400) acc_u 18.7500 (19.7917) lr 6.7608e-04 eta 0:00:20
epoch [123/200] batch [20/59] time 0.472 (0.460) data 0.341 (0.329) loss_u loss_u 0.7046 (0.8352) acc_u 40.6250 (21.5625) lr 6.7608e-04 eta 0:00:17
epoch [123/200] batch [25/59] time 0.507 (0.461) data 0.374 (0.329) loss_u loss_u 0.8599 (0.8385) acc_u 18.7500 (20.8750) lr 6.7608e-04 eta 0:00:15
epoch [123/200] batch [30/59] time 0.386 (0.457) data 0.255 (0.326) loss_u loss_u 0.8423 (0.8412) acc_u 15.6250 (20.4167) lr 6.7608e-04 eta 0:00:13
epoch [123/200] batch [35/59] time 0.381 (0.459) data 0.250 (0.328) loss_u loss_u 0.8740 (0.8397) acc_u 15.6250 (20.3571) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [40/59] time 0.414 (0.457) data 0.283 (0.326) loss_u loss_u 0.9023 (0.8416) acc_u 15.6250 (20.2344) lr 6.7608e-04 eta 0:00:08
epoch [123/200] batch [45/59] time 0.489 (0.456) data 0.356 (0.324) loss_u loss_u 0.8652 (0.8430) acc_u 15.6250 (20.0694) lr 6.7608e-04 eta 0:00:06
epoch [123/200] batch [50/59] time 0.470 (0.458) data 0.339 (0.326) loss_u loss_u 0.8330 (0.8421) acc_u 18.7500 (20.0625) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [55/59] time 0.371 (0.456) data 0.239 (0.324) loss_u loss_u 0.8906 (0.8450) acc_u 9.3750 (19.4886) lr 6.7608e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1464
confident_label rate tensor(0.3932, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1233
clean true:1150
clean false:83
clean_rate:0.9326845093268451
noisy true:522
noisy false:1381
after delete: len(clean_dataset) 1233
after delete: len(noisy_dataset) 1903
epoch [124/200] batch [5/38] time 0.492 (0.449) data 0.362 (0.318) loss_x loss_x 1.0508 (1.0703) acc_x 71.8750 (75.0000) lr 6.6126e-04 eta 0:00:14
epoch [124/200] batch [10/38] time 0.456 (0.444) data 0.325 (0.313) loss_x loss_x 1.2324 (1.1106) acc_x 78.1250 (72.1875) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [15/38] time 0.405 (0.436) data 0.274 (0.306) loss_x loss_x 0.9268 (1.1295) acc_x 68.7500 (71.4583) lr 6.6126e-04 eta 0:00:10
epoch [124/200] batch [20/38] time 0.500 (0.457) data 0.370 (0.326) loss_x loss_x 1.2168 (1.0951) acc_x 62.5000 (72.5000) lr 6.6126e-04 eta 0:00:08
epoch [124/200] batch [25/38] time 0.541 (0.465) data 0.411 (0.334) loss_x loss_x 1.6758 (1.1316) acc_x 59.3750 (71.2500) lr 6.6126e-04 eta 0:00:06
epoch [124/200] batch [30/38] time 0.379 (0.468) data 0.249 (0.337) loss_x loss_x 1.2617 (1.1610) acc_x 71.8750 (71.1458) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [35/38] time 0.451 (0.466) data 0.316 (0.335) loss_x loss_x 1.5996 (1.1615) acc_x 62.5000 (70.8036) lr 6.6126e-04 eta 0:00:01
epoch [124/200] batch [5/59] time 0.551 (0.469) data 0.420 (0.338) loss_u loss_u 0.8540 (0.8365) acc_u 15.6250 (19.3750) lr 6.6126e-04 eta 0:00:25
epoch [124/200] batch [10/59] time 0.431 (0.465) data 0.299 (0.334) loss_u loss_u 0.7046 (0.8336) acc_u 37.5000 (20.6250) lr 6.6126e-04 eta 0:00:22
epoch [124/200] batch [15/59] time 0.376 (0.462) data 0.245 (0.331) loss_u loss_u 0.7534 (0.8302) acc_u 34.3750 (22.5000) lr 6.6126e-04 eta 0:00:20
epoch [124/200] batch [20/59] time 0.335 (0.460) data 0.203 (0.329) loss_u loss_u 0.8530 (0.8335) acc_u 15.6250 (22.3438) lr 6.6126e-04 eta 0:00:17
epoch [124/200] batch [25/59] time 0.551 (0.458) data 0.420 (0.327) loss_u loss_u 0.8408 (0.8316) acc_u 21.8750 (22.1250) lr 6.6126e-04 eta 0:00:15
epoch [124/200] batch [30/59] time 0.386 (0.461) data 0.254 (0.330) loss_u loss_u 0.8887 (0.8350) acc_u 15.6250 (21.5625) lr 6.6126e-04 eta 0:00:13
epoch [124/200] batch [35/59] time 0.453 (0.460) data 0.323 (0.328) loss_u loss_u 0.7695 (0.8358) acc_u 28.1250 (21.2500) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [40/59] time 0.340 (0.458) data 0.208 (0.327) loss_u loss_u 0.8789 (0.8422) acc_u 15.6250 (20.7031) lr 6.6126e-04 eta 0:00:08
epoch [124/200] batch [45/59] time 0.327 (0.455) data 0.195 (0.324) loss_u loss_u 0.8921 (0.8422) acc_u 15.6250 (20.6250) lr 6.6126e-04 eta 0:00:06
epoch [124/200] batch [50/59] time 0.504 (0.453) data 0.372 (0.322) loss_u loss_u 0.8657 (0.8434) acc_u 12.5000 (20.3125) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [55/59] time 0.632 (0.456) data 0.500 (0.325) loss_u loss_u 0.8628 (0.8428) acc_u 18.7500 (20.4545) lr 6.6126e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1498
confident_label rate tensor(0.3827, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1200
clean true:1122
clean false:78
clean_rate:0.935
noisy true:516
noisy false:1420
after delete: len(clean_dataset) 1200
after delete: len(noisy_dataset) 1936
epoch [125/200] batch [5/37] time 0.461 (0.469) data 0.330 (0.338) loss_x loss_x 1.0039 (1.2653) acc_x 81.2500 (70.0000) lr 6.4653e-04 eta 0:00:15
epoch [125/200] batch [10/37] time 0.430 (0.483) data 0.300 (0.353) loss_x loss_x 0.9731 (1.1457) acc_x 75.0000 (73.4375) lr 6.4653e-04 eta 0:00:13
epoch [125/200] batch [15/37] time 0.492 (0.465) data 0.362 (0.335) loss_x loss_x 0.9253 (1.1134) acc_x 78.1250 (73.3333) lr 6.4653e-04 eta 0:00:10
epoch [125/200] batch [20/37] time 0.403 (0.445) data 0.273 (0.314) loss_x loss_x 1.0625 (1.1145) acc_x 78.1250 (73.4375) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [25/37] time 0.699 (0.462) data 0.568 (0.331) loss_x loss_x 1.0332 (1.1162) acc_x 71.8750 (72.8750) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [30/37] time 0.473 (0.462) data 0.342 (0.332) loss_x loss_x 1.2480 (1.1500) acc_x 62.5000 (71.7708) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [35/37] time 0.582 (0.464) data 0.451 (0.333) loss_x loss_x 1.0918 (1.1658) acc_x 75.0000 (71.4286) lr 6.4653e-04 eta 0:00:00
epoch [125/200] batch [5/60] time 0.535 (0.461) data 0.404 (0.331) loss_u loss_u 0.8135 (0.8489) acc_u 28.1250 (21.2500) lr 6.4653e-04 eta 0:00:25
epoch [125/200] batch [10/60] time 0.383 (0.457) data 0.252 (0.326) loss_u loss_u 0.8237 (0.8441) acc_u 25.0000 (21.2500) lr 6.4653e-04 eta 0:00:22
epoch [125/200] batch [15/60] time 0.373 (0.455) data 0.242 (0.324) loss_u loss_u 0.8926 (0.8509) acc_u 18.7500 (20.0000) lr 6.4653e-04 eta 0:00:20
epoch [125/200] batch [20/60] time 0.380 (0.450) data 0.247 (0.319) loss_u loss_u 0.8633 (0.8497) acc_u 12.5000 (19.6875) lr 6.4653e-04 eta 0:00:18
epoch [125/200] batch [25/60] time 0.543 (0.454) data 0.412 (0.323) loss_u loss_u 0.8340 (0.8439) acc_u 15.6250 (20.2500) lr 6.4653e-04 eta 0:00:15
epoch [125/200] batch [30/60] time 0.468 (0.452) data 0.338 (0.321) loss_u loss_u 0.7744 (0.8441) acc_u 28.1250 (20.4167) lr 6.4653e-04 eta 0:00:13
epoch [125/200] batch [35/60] time 0.447 (0.452) data 0.317 (0.321) loss_u loss_u 0.8701 (0.8450) acc_u 12.5000 (19.9107) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [40/60] time 0.610 (0.451) data 0.478 (0.320) loss_u loss_u 0.9092 (0.8428) acc_u 6.2500 (19.9219) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [45/60] time 0.395 (0.447) data 0.264 (0.316) loss_u loss_u 0.7725 (0.8361) acc_u 28.1250 (20.8333) lr 6.4653e-04 eta 0:00:06
epoch [125/200] batch [50/60] time 0.499 (0.447) data 0.367 (0.316) loss_u loss_u 0.8892 (0.8393) acc_u 12.5000 (20.2500) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [55/60] time 0.457 (0.447) data 0.325 (0.315) loss_u loss_u 0.8071 (0.8391) acc_u 28.1250 (20.2273) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [60/60] time 0.430 (0.447) data 0.298 (0.316) loss_u loss_u 0.8657 (0.8372) acc_u 15.6250 (20.3646) lr 6.4653e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.3983, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1249
clean true:1167
clean false:82
clean_rate:0.9343474779823859
noisy true:528
noisy false:1359
after delete: len(clean_dataset) 1249
after delete: len(noisy_dataset) 1887
epoch [126/200] batch [5/39] time 0.598 (0.483) data 0.467 (0.351) loss_x loss_x 0.9912 (1.1534) acc_x 68.7500 (73.1250) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [10/39] time 0.371 (0.491) data 0.240 (0.360) loss_x loss_x 0.7729 (1.0908) acc_x 78.1250 (73.7500) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [15/39] time 0.493 (0.469) data 0.361 (0.338) loss_x loss_x 1.5000 (1.1186) acc_x 56.2500 (72.0833) lr 6.3188e-04 eta 0:00:11
epoch [126/200] batch [20/39] time 0.438 (0.476) data 0.307 (0.344) loss_x loss_x 1.0400 (1.1332) acc_x 65.6250 (70.0000) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [25/39] time 0.460 (0.465) data 0.330 (0.334) loss_x loss_x 1.1006 (1.1201) acc_x 78.1250 (71.6250) lr 6.3188e-04 eta 0:00:06
epoch [126/200] batch [30/39] time 0.499 (0.478) data 0.368 (0.346) loss_x loss_x 1.0693 (1.1413) acc_x 71.8750 (71.4583) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [35/39] time 0.387 (0.469) data 0.255 (0.338) loss_x loss_x 0.9585 (1.1211) acc_x 71.8750 (72.0536) lr 6.3188e-04 eta 0:00:01
epoch [126/200] batch [5/58] time 0.476 (0.469) data 0.346 (0.338) loss_u loss_u 0.8296 (0.8054) acc_u 21.8750 (24.3750) lr 6.3188e-04 eta 0:00:24
epoch [126/200] batch [10/58] time 0.366 (0.471) data 0.234 (0.340) loss_u loss_u 0.8408 (0.8127) acc_u 21.8750 (23.7500) lr 6.3188e-04 eta 0:00:22
epoch [126/200] batch [15/58] time 0.333 (0.465) data 0.201 (0.333) loss_u loss_u 0.8379 (0.8206) acc_u 15.6250 (22.9167) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [20/58] time 0.365 (0.459) data 0.234 (0.328) loss_u loss_u 0.9321 (0.8365) acc_u 6.2500 (21.0938) lr 6.3188e-04 eta 0:00:17
epoch [126/200] batch [25/58] time 0.582 (0.458) data 0.450 (0.327) loss_u loss_u 0.8960 (0.8475) acc_u 12.5000 (19.2500) lr 6.3188e-04 eta 0:00:15
epoch [126/200] batch [30/58] time 0.575 (0.461) data 0.443 (0.330) loss_u loss_u 0.9282 (0.8511) acc_u 12.5000 (18.4375) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [35/58] time 0.392 (0.458) data 0.260 (0.326) loss_u loss_u 0.9194 (0.8549) acc_u 9.3750 (17.9464) lr 6.3188e-04 eta 0:00:10
epoch [126/200] batch [40/58] time 0.516 (0.458) data 0.385 (0.326) loss_u loss_u 0.9111 (0.8535) acc_u 12.5000 (18.1250) lr 6.3188e-04 eta 0:00:08
epoch [126/200] batch [45/58] time 0.480 (0.457) data 0.349 (0.326) loss_u loss_u 0.8682 (0.8495) acc_u 15.6250 (18.4028) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [50/58] time 0.300 (0.453) data 0.169 (0.322) loss_u loss_u 0.8628 (0.8477) acc_u 15.6250 (18.7500) lr 6.3188e-04 eta 0:00:03
epoch [126/200] batch [55/58] time 0.499 (0.453) data 0.364 (0.321) loss_u loss_u 0.8726 (0.8492) acc_u 15.6250 (18.5227) lr 6.3188e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1454
confident_label rate tensor(0.3919, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1229
clean true:1153
clean false:76
clean_rate:0.9381611065907242
noisy true:529
noisy false:1378
after delete: len(clean_dataset) 1229
after delete: len(noisy_dataset) 1907
epoch [127/200] batch [5/38] time 0.458 (0.499) data 0.327 (0.368) loss_x loss_x 0.9502 (1.0979) acc_x 75.0000 (70.0000) lr 6.1732e-04 eta 0:00:16
epoch [127/200] batch [10/38] time 0.503 (0.487) data 0.373 (0.356) loss_x loss_x 0.9507 (1.1341) acc_x 84.3750 (70.0000) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [15/38] time 0.604 (0.483) data 0.474 (0.352) loss_x loss_x 0.9292 (1.0976) acc_x 75.0000 (71.6667) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [20/38] time 0.417 (0.469) data 0.286 (0.339) loss_x loss_x 1.5078 (1.1413) acc_x 50.0000 (70.3125) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [25/38] time 0.432 (0.464) data 0.300 (0.334) loss_x loss_x 0.9688 (1.1322) acc_x 71.8750 (69.6250) lr 6.1732e-04 eta 0:00:06
epoch [127/200] batch [30/38] time 0.473 (0.463) data 0.342 (0.333) loss_x loss_x 0.8911 (1.1428) acc_x 75.0000 (69.4792) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [35/38] time 0.451 (0.460) data 0.320 (0.329) loss_x loss_x 1.0547 (1.1325) acc_x 68.7500 (69.9107) lr 6.1732e-04 eta 0:00:01
epoch [127/200] batch [5/59] time 0.400 (0.455) data 0.268 (0.324) loss_u loss_u 0.7534 (0.8361) acc_u 31.2500 (21.2500) lr 6.1732e-04 eta 0:00:24
epoch [127/200] batch [10/59] time 0.537 (0.461) data 0.406 (0.330) loss_u loss_u 0.8765 (0.8336) acc_u 15.6250 (20.3125) lr 6.1732e-04 eta 0:00:22
epoch [127/200] batch [15/59] time 0.472 (0.469) data 0.342 (0.338) loss_u loss_u 0.8271 (0.8395) acc_u 25.0000 (19.3750) lr 6.1732e-04 eta 0:00:20
epoch [127/200] batch [20/59] time 0.475 (0.465) data 0.343 (0.333) loss_u loss_u 0.9238 (0.8487) acc_u 12.5000 (18.5938) lr 6.1732e-04 eta 0:00:18
epoch [127/200] batch [25/59] time 0.445 (0.466) data 0.314 (0.334) loss_u loss_u 0.8545 (0.8527) acc_u 18.7500 (18.2500) lr 6.1732e-04 eta 0:00:15
epoch [127/200] batch [30/59] time 0.386 (0.461) data 0.255 (0.330) loss_u loss_u 0.9077 (0.8525) acc_u 6.2500 (18.3333) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [35/59] time 0.431 (0.459) data 0.300 (0.328) loss_u loss_u 0.8042 (0.8460) acc_u 28.1250 (19.2857) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [40/59] time 0.435 (0.459) data 0.304 (0.328) loss_u loss_u 0.8184 (0.8463) acc_u 18.7500 (19.3750) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [45/59] time 0.413 (0.457) data 0.282 (0.326) loss_u loss_u 0.8652 (0.8464) acc_u 18.7500 (19.2361) lr 6.1732e-04 eta 0:00:06
epoch [127/200] batch [50/59] time 0.380 (0.456) data 0.249 (0.325) loss_u loss_u 0.8096 (0.8474) acc_u 28.1250 (19.1875) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [55/59] time 0.405 (0.455) data 0.274 (0.324) loss_u loss_u 0.8296 (0.8467) acc_u 25.0000 (19.2614) lr 6.1732e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1498
confident_label rate tensor(0.3842, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1205
clean true:1118
clean false:87
clean_rate:0.9278008298755187
noisy true:520
noisy false:1411
after delete: len(clean_dataset) 1205
after delete: len(noisy_dataset) 1931
epoch [128/200] batch [5/37] time 0.454 (0.586) data 0.323 (0.454) loss_x loss_x 0.5391 (1.1938) acc_x 93.7500 (69.3750) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [10/37] time 0.590 (0.530) data 0.459 (0.398) loss_x loss_x 0.9546 (1.1386) acc_x 75.0000 (71.2500) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [15/37] time 0.448 (0.488) data 0.317 (0.357) loss_x loss_x 0.8550 (1.1410) acc_x 71.8750 (71.2500) lr 6.0285e-04 eta 0:00:10
epoch [128/200] batch [20/37] time 0.408 (0.479) data 0.277 (0.347) loss_x loss_x 0.9307 (1.1177) acc_x 71.8750 (72.1875) lr 6.0285e-04 eta 0:00:08
epoch [128/200] batch [25/37] time 0.499 (0.481) data 0.368 (0.349) loss_x loss_x 0.9932 (1.1199) acc_x 71.8750 (71.5000) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [30/37] time 0.425 (0.479) data 0.294 (0.348) loss_x loss_x 0.9556 (1.1406) acc_x 78.1250 (70.7292) lr 6.0285e-04 eta 0:00:03
epoch [128/200] batch [35/37] time 0.400 (0.479) data 0.269 (0.348) loss_x loss_x 0.8052 (1.1353) acc_x 71.8750 (71.0714) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/60] time 0.347 (0.480) data 0.215 (0.348) loss_u loss_u 0.8398 (0.8260) acc_u 15.6250 (21.8750) lr 6.0285e-04 eta 0:00:26
epoch [128/200] batch [10/60] time 0.413 (0.473) data 0.282 (0.341) loss_u loss_u 0.8208 (0.8428) acc_u 21.8750 (20.3125) lr 6.0285e-04 eta 0:00:23
epoch [128/200] batch [15/60] time 0.457 (0.471) data 0.326 (0.339) loss_u loss_u 0.7393 (0.8447) acc_u 28.1250 (20.6250) lr 6.0285e-04 eta 0:00:21
epoch [128/200] batch [20/60] time 0.520 (0.469) data 0.389 (0.338) loss_u loss_u 0.8188 (0.8397) acc_u 25.0000 (21.2500) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [25/60] time 0.480 (0.466) data 0.349 (0.335) loss_u loss_u 0.7881 (0.8434) acc_u 28.1250 (20.3750) lr 6.0285e-04 eta 0:00:16
epoch [128/200] batch [30/60] time 0.387 (0.464) data 0.256 (0.333) loss_u loss_u 0.8691 (0.8460) acc_u 21.8750 (20.1042) lr 6.0285e-04 eta 0:00:13
epoch [128/200] batch [35/60] time 0.461 (0.462) data 0.331 (0.331) loss_u loss_u 0.8623 (0.8470) acc_u 18.7500 (19.9107) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [40/60] time 0.455 (0.460) data 0.324 (0.329) loss_u loss_u 0.8252 (0.8396) acc_u 21.8750 (20.7031) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [45/60] time 0.454 (0.459) data 0.323 (0.328) loss_u loss_u 0.8594 (0.8399) acc_u 15.6250 (20.5556) lr 6.0285e-04 eta 0:00:06
epoch [128/200] batch [50/60] time 0.586 (0.459) data 0.454 (0.328) loss_u loss_u 0.8408 (0.8386) acc_u 15.6250 (20.5000) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [55/60] time 0.491 (0.457) data 0.361 (0.326) loss_u loss_u 0.8389 (0.8400) acc_u 21.8750 (20.1136) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [60/60] time 0.415 (0.456) data 0.283 (0.325) loss_u loss_u 0.8140 (0.8376) acc_u 21.8750 (20.2604) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1456
confident_label rate tensor(0.3954, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1240
clean true:1161
clean false:79
clean_rate:0.9362903225806452
noisy true:519
noisy false:1377
after delete: len(clean_dataset) 1240
after delete: len(noisy_dataset) 1896
epoch [129/200] batch [5/38] time 0.493 (0.485) data 0.362 (0.354) loss_x loss_x 1.4346 (1.3547) acc_x 65.6250 (68.1250) lr 5.8849e-04 eta 0:00:16
epoch [129/200] batch [10/38] time 0.518 (0.473) data 0.387 (0.342) loss_x loss_x 1.3438 (1.1252) acc_x 56.2500 (72.8125) lr 5.8849e-04 eta 0:00:13
epoch [129/200] batch [15/38] time 0.499 (0.467) data 0.368 (0.336) loss_x loss_x 0.7041 (1.1396) acc_x 84.3750 (75.0000) lr 5.8849e-04 eta 0:00:10
epoch [129/200] batch [20/38] time 0.467 (0.465) data 0.336 (0.334) loss_x loss_x 1.0713 (1.1840) acc_x 71.8750 (72.6562) lr 5.8849e-04 eta 0:00:08
epoch [129/200] batch [25/38] time 0.384 (0.463) data 0.253 (0.332) loss_x loss_x 0.5518 (1.1407) acc_x 87.5000 (73.2500) lr 5.8849e-04 eta 0:00:06
epoch [129/200] batch [30/38] time 0.453 (0.459) data 0.322 (0.328) loss_x loss_x 1.8525 (1.1489) acc_x 53.1250 (72.5000) lr 5.8849e-04 eta 0:00:03
epoch [129/200] batch [35/38] time 0.523 (0.458) data 0.392 (0.327) loss_x loss_x 0.9487 (1.1512) acc_x 78.1250 (72.4107) lr 5.8849e-04 eta 0:00:01
epoch [129/200] batch [5/59] time 0.364 (0.462) data 0.231 (0.331) loss_u loss_u 0.8760 (0.8463) acc_u 12.5000 (16.8750) lr 5.8849e-04 eta 0:00:24
epoch [129/200] batch [10/59] time 0.515 (0.458) data 0.383 (0.326) loss_u loss_u 0.8901 (0.8686) acc_u 15.6250 (15.3125) lr 5.8849e-04 eta 0:00:22
epoch [129/200] batch [15/59] time 0.458 (0.462) data 0.326 (0.331) loss_u loss_u 0.8477 (0.8593) acc_u 15.6250 (16.6667) lr 5.8849e-04 eta 0:00:20
epoch [129/200] batch [20/59] time 0.585 (0.461) data 0.454 (0.329) loss_u loss_u 0.8003 (0.8534) acc_u 25.0000 (17.3438) lr 5.8849e-04 eta 0:00:17
epoch [129/200] batch [25/59] time 0.482 (0.461) data 0.350 (0.329) loss_u loss_u 0.8750 (0.8548) acc_u 12.5000 (17.0000) lr 5.8849e-04 eta 0:00:15
epoch [129/200] batch [30/59] time 0.334 (0.456) data 0.202 (0.324) loss_u loss_u 0.7847 (0.8487) acc_u 25.0000 (17.6042) lr 5.8849e-04 eta 0:00:13
epoch [129/200] batch [35/59] time 0.313 (0.455) data 0.181 (0.324) loss_u loss_u 0.7720 (0.8479) acc_u 34.3750 (17.8571) lr 5.8849e-04 eta 0:00:10
epoch [129/200] batch [40/59] time 0.379 (0.451) data 0.248 (0.320) loss_u loss_u 0.9072 (0.8521) acc_u 9.3750 (17.5781) lr 5.8849e-04 eta 0:00:08
epoch [129/200] batch [45/59] time 0.338 (0.450) data 0.207 (0.319) loss_u loss_u 0.8057 (0.8504) acc_u 25.0000 (18.1250) lr 5.8849e-04 eta 0:00:06
epoch [129/200] batch [50/59] time 0.380 (0.451) data 0.249 (0.320) loss_u loss_u 0.8765 (0.8494) acc_u 15.6250 (18.4375) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [55/59] time 0.415 (0.448) data 0.283 (0.317) loss_u loss_u 0.7837 (0.8467) acc_u 28.1250 (18.8068) lr 5.8849e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1418
confident_label rate tensor(0.3964, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1243
clean true:1163
clean false:80
clean_rate:0.9356395816572808
noisy true:555
noisy false:1338
after delete: len(clean_dataset) 1243
after delete: len(noisy_dataset) 1893
epoch [130/200] batch [5/38] time 0.544 (0.550) data 0.414 (0.419) loss_x loss_x 1.1465 (1.1782) acc_x 62.5000 (65.6250) lr 5.7422e-04 eta 0:00:18
epoch [130/200] batch [10/38] time 0.408 (0.518) data 0.278 (0.387) loss_x loss_x 1.7666 (1.2833) acc_x 62.5000 (68.1250) lr 5.7422e-04 eta 0:00:14
epoch [130/200] batch [15/38] time 0.402 (0.508) data 0.271 (0.377) loss_x loss_x 1.6387 (1.2975) acc_x 62.5000 (67.0833) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [20/38] time 0.518 (0.507) data 0.388 (0.377) loss_x loss_x 0.7700 (1.2314) acc_x 78.1250 (68.2812) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [25/38] time 0.605 (0.493) data 0.474 (0.363) loss_x loss_x 1.4434 (1.1946) acc_x 56.2500 (69.0000) lr 5.7422e-04 eta 0:00:06
epoch [130/200] batch [30/38] time 0.497 (0.486) data 0.366 (0.355) loss_x loss_x 0.9429 (1.1718) acc_x 81.2500 (70.2083) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [35/38] time 0.557 (0.486) data 0.426 (0.356) loss_x loss_x 1.2158 (1.1634) acc_x 75.0000 (70.9821) lr 5.7422e-04 eta 0:00:01
epoch [130/200] batch [5/59] time 0.375 (0.483) data 0.244 (0.352) loss_u loss_u 0.8774 (0.8244) acc_u 15.6250 (21.2500) lr 5.7422e-04 eta 0:00:26
epoch [130/200] batch [10/59] time 0.552 (0.477) data 0.421 (0.346) loss_u loss_u 0.8540 (0.8289) acc_u 18.7500 (20.6250) lr 5.7422e-04 eta 0:00:23
epoch [130/200] batch [15/59] time 0.479 (0.472) data 0.349 (0.341) loss_u loss_u 0.8574 (0.8395) acc_u 21.8750 (20.4167) lr 5.7422e-04 eta 0:00:20
epoch [130/200] batch [20/59] time 0.481 (0.470) data 0.350 (0.340) loss_u loss_u 0.9023 (0.8432) acc_u 12.5000 (20.0000) lr 5.7422e-04 eta 0:00:18
epoch [130/200] batch [25/59] time 0.462 (0.466) data 0.331 (0.335) loss_u loss_u 0.9082 (0.8463) acc_u 6.2500 (18.8750) lr 5.7422e-04 eta 0:00:15
epoch [130/200] batch [30/59] time 0.385 (0.465) data 0.254 (0.334) loss_u loss_u 0.8428 (0.8448) acc_u 15.6250 (18.9583) lr 5.7422e-04 eta 0:00:13
epoch [130/200] batch [35/59] time 0.385 (0.465) data 0.251 (0.334) loss_u loss_u 0.7988 (0.8418) acc_u 21.8750 (19.3750) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [40/59] time 0.444 (0.465) data 0.312 (0.334) loss_u loss_u 0.8467 (0.8454) acc_u 15.6250 (18.9844) lr 5.7422e-04 eta 0:00:08
epoch [130/200] batch [45/59] time 0.585 (0.463) data 0.455 (0.332) loss_u loss_u 0.7822 (0.8434) acc_u 28.1250 (19.2361) lr 5.7422e-04 eta 0:00:06
epoch [130/200] batch [50/59] time 0.331 (0.462) data 0.200 (0.331) loss_u loss_u 0.8594 (0.8449) acc_u 15.6250 (19.2500) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [55/59] time 0.484 (0.461) data 0.352 (0.329) loss_u loss_u 0.9204 (0.8469) acc_u 12.5000 (19.0909) lr 5.7422e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1447
confident_label rate tensor(0.3964, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1243
clean true:1163
clean false:80
clean_rate:0.9356395816572808
noisy true:526
noisy false:1367
after delete: len(clean_dataset) 1243
after delete: len(noisy_dataset) 1893
epoch [131/200] batch [5/38] time 0.454 (0.431) data 0.324 (0.300) loss_x loss_x 1.0049 (0.9628) acc_x 78.1250 (73.1250) lr 5.6006e-04 eta 0:00:14
epoch [131/200] batch [10/38] time 0.410 (0.452) data 0.280 (0.322) loss_x loss_x 1.2178 (1.0437) acc_x 78.1250 (73.7500) lr 5.6006e-04 eta 0:00:12
epoch [131/200] batch [15/38] time 0.397 (0.451) data 0.266 (0.320) loss_x loss_x 0.9312 (1.0360) acc_x 68.7500 (73.3333) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [20/38] time 0.428 (0.445) data 0.298 (0.315) loss_x loss_x 1.0791 (1.0033) acc_x 65.6250 (73.7500) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [25/38] time 0.506 (0.463) data 0.376 (0.332) loss_x loss_x 1.1250 (1.0391) acc_x 68.7500 (73.6250) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [30/38] time 0.439 (0.458) data 0.309 (0.327) loss_x loss_x 1.0869 (1.0573) acc_x 68.7500 (73.0208) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [35/38] time 0.471 (0.462) data 0.340 (0.331) loss_x loss_x 1.3906 (1.0755) acc_x 59.3750 (72.2321) lr 5.6006e-04 eta 0:00:01
epoch [131/200] batch [5/59] time 0.532 (0.467) data 0.401 (0.336) loss_u loss_u 0.8867 (0.8159) acc_u 15.6250 (25.0000) lr 5.6006e-04 eta 0:00:25
epoch [131/200] batch [10/59] time 0.429 (0.467) data 0.297 (0.336) loss_u loss_u 0.7686 (0.8268) acc_u 25.0000 (21.2500) lr 5.6006e-04 eta 0:00:22
epoch [131/200] batch [15/59] time 0.404 (0.473) data 0.274 (0.342) loss_u loss_u 0.8989 (0.8326) acc_u 12.5000 (20.2083) lr 5.6006e-04 eta 0:00:20
epoch [131/200] batch [20/59] time 0.377 (0.468) data 0.245 (0.337) loss_u loss_u 0.8857 (0.8463) acc_u 12.5000 (18.1250) lr 5.6006e-04 eta 0:00:18
epoch [131/200] batch [25/59] time 0.359 (0.466) data 0.228 (0.335) loss_u loss_u 0.8599 (0.8489) acc_u 15.6250 (18.0000) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [30/59] time 0.360 (0.462) data 0.230 (0.331) loss_u loss_u 0.8115 (0.8474) acc_u 31.2500 (18.7500) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [35/59] time 0.416 (0.459) data 0.285 (0.328) loss_u loss_u 0.8423 (0.8440) acc_u 21.8750 (19.6429) lr 5.6006e-04 eta 0:00:11
epoch [131/200] batch [40/59] time 0.441 (0.458) data 0.310 (0.327) loss_u loss_u 0.8896 (0.8482) acc_u 12.5000 (18.9062) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [45/59] time 0.445 (0.458) data 0.313 (0.327) loss_u loss_u 0.8594 (0.8492) acc_u 15.6250 (18.8889) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [50/59] time 0.432 (0.460) data 0.301 (0.329) loss_u loss_u 0.8281 (0.8458) acc_u 21.8750 (19.0625) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [55/59] time 0.466 (0.458) data 0.335 (0.327) loss_u loss_u 0.8169 (0.8439) acc_u 25.0000 (19.4886) lr 5.6006e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1394
confident_label rate tensor(0.4139, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1298
clean true:1219
clean false:79
clean_rate:0.9391371340523883
noisy true:523
noisy false:1315
after delete: len(clean_dataset) 1298
after delete: len(noisy_dataset) 1838
epoch [132/200] batch [5/40] time 0.388 (0.490) data 0.256 (0.359) loss_x loss_x 1.0088 (1.0116) acc_x 71.8750 (71.8750) lr 5.4601e-04 eta 0:00:17
epoch [132/200] batch [10/40] time 0.474 (0.511) data 0.343 (0.380) loss_x loss_x 0.9248 (1.0319) acc_x 68.7500 (70.0000) lr 5.4601e-04 eta 0:00:15
epoch [132/200] batch [15/40] time 0.434 (0.479) data 0.303 (0.348) loss_x loss_x 1.1562 (1.0720) acc_x 65.6250 (70.8333) lr 5.4601e-04 eta 0:00:11
epoch [132/200] batch [20/40] time 0.506 (0.482) data 0.375 (0.350) loss_x loss_x 1.5576 (1.1033) acc_x 71.8750 (71.5625) lr 5.4601e-04 eta 0:00:09
epoch [132/200] batch [25/40] time 0.452 (0.481) data 0.321 (0.350) loss_x loss_x 0.8374 (1.1147) acc_x 78.1250 (71.5000) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [30/40] time 0.380 (0.472) data 0.248 (0.341) loss_x loss_x 1.0283 (1.1242) acc_x 78.1250 (72.0833) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [35/40] time 0.459 (0.468) data 0.328 (0.337) loss_x loss_x 0.8164 (1.1306) acc_x 81.2500 (71.9643) lr 5.4601e-04 eta 0:00:02
epoch [132/200] batch [40/40] time 0.485 (0.472) data 0.354 (0.341) loss_x loss_x 1.2295 (1.1164) acc_x 71.8750 (72.2656) lr 5.4601e-04 eta 0:00:00
epoch [132/200] batch [5/57] time 0.494 (0.470) data 0.362 (0.339) loss_u loss_u 0.9033 (0.8645) acc_u 15.6250 (17.5000) lr 5.4601e-04 eta 0:00:24
epoch [132/200] batch [10/57] time 0.463 (0.468) data 0.327 (0.337) loss_u loss_u 0.8379 (0.8620) acc_u 15.6250 (16.5625) lr 5.4601e-04 eta 0:00:21
epoch [132/200] batch [15/57] time 0.463 (0.464) data 0.332 (0.333) loss_u loss_u 0.8335 (0.8627) acc_u 18.7500 (15.8333) lr 5.4601e-04 eta 0:00:19
epoch [132/200] batch [20/57] time 0.429 (0.456) data 0.298 (0.324) loss_u loss_u 0.7710 (0.8583) acc_u 28.1250 (16.8750) lr 5.4601e-04 eta 0:00:16
epoch [132/200] batch [25/57] time 0.432 (0.455) data 0.300 (0.324) loss_u loss_u 0.8125 (0.8611) acc_u 25.0000 (16.5000) lr 5.4601e-04 eta 0:00:14
epoch [132/200] batch [30/57] time 0.620 (0.463) data 0.488 (0.332) loss_u loss_u 0.8755 (0.8594) acc_u 18.7500 (16.8750) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [35/57] time 0.429 (0.462) data 0.298 (0.331) loss_u loss_u 0.8291 (0.8566) acc_u 18.7500 (17.1429) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [40/57] time 0.473 (0.462) data 0.341 (0.330) loss_u loss_u 0.9106 (0.8526) acc_u 6.2500 (17.6562) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [45/57] time 0.439 (0.462) data 0.307 (0.331) loss_u loss_u 0.8345 (0.8523) acc_u 21.8750 (17.9167) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [50/57] time 0.334 (0.460) data 0.202 (0.329) loss_u loss_u 0.8564 (0.8543) acc_u 12.5000 (17.5625) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [55/57] time 0.372 (0.461) data 0.240 (0.329) loss_u loss_u 0.9302 (0.8549) acc_u 6.2500 (17.5568) lr 5.4601e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1436
confident_label rate tensor(0.4047, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1269
clean true:1175
clean false:94
clean_rate:0.9259259259259259
noisy true:525
noisy false:1342
after delete: len(clean_dataset) 1269
after delete: len(noisy_dataset) 1867
epoch [133/200] batch [5/39] time 0.398 (0.399) data 0.268 (0.269) loss_x loss_x 0.8994 (1.1534) acc_x 65.6250 (66.8750) lr 5.3207e-04 eta 0:00:13
epoch [133/200] batch [10/39] time 0.453 (0.463) data 0.322 (0.332) loss_x loss_x 0.9839 (1.0678) acc_x 81.2500 (71.5625) lr 5.3207e-04 eta 0:00:13
epoch [133/200] batch [15/39] time 0.423 (0.474) data 0.291 (0.343) loss_x loss_x 0.9390 (1.0501) acc_x 78.1250 (73.1250) lr 5.3207e-04 eta 0:00:11
epoch [133/200] batch [20/39] time 0.441 (0.477) data 0.310 (0.346) loss_x loss_x 1.5732 (1.0777) acc_x 62.5000 (73.1250) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [25/39] time 0.373 (0.480) data 0.242 (0.348) loss_x loss_x 1.0195 (1.0671) acc_x 75.0000 (73.6250) lr 5.3207e-04 eta 0:00:06
epoch [133/200] batch [30/39] time 0.330 (0.473) data 0.199 (0.342) loss_x loss_x 0.9824 (1.0417) acc_x 65.6250 (73.3333) lr 5.3207e-04 eta 0:00:04
epoch [133/200] batch [35/39] time 0.506 (0.472) data 0.375 (0.341) loss_x loss_x 1.2920 (1.0430) acc_x 62.5000 (72.6786) lr 5.3207e-04 eta 0:00:01
epoch [133/200] batch [5/58] time 0.447 (0.461) data 0.315 (0.330) loss_u loss_u 0.8281 (0.8446) acc_u 25.0000 (19.3750) lr 5.3207e-04 eta 0:00:24
epoch [133/200] batch [10/58] time 0.388 (0.459) data 0.256 (0.328) loss_u loss_u 0.9009 (0.8482) acc_u 15.6250 (18.7500) lr 5.3207e-04 eta 0:00:22
epoch [133/200] batch [15/58] time 0.399 (0.457) data 0.267 (0.326) loss_u loss_u 0.9326 (0.8599) acc_u 6.2500 (17.7083) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [20/58] time 0.416 (0.455) data 0.285 (0.324) loss_u loss_u 0.8721 (0.8552) acc_u 21.8750 (18.4375) lr 5.3207e-04 eta 0:00:17
epoch [133/200] batch [25/58] time 0.348 (0.457) data 0.217 (0.326) loss_u loss_u 0.8384 (0.8537) acc_u 21.8750 (18.3750) lr 5.3207e-04 eta 0:00:15
epoch [133/200] batch [30/58] time 0.449 (0.457) data 0.318 (0.326) loss_u loss_u 0.8774 (0.8520) acc_u 12.5000 (18.5417) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [35/58] time 0.323 (0.453) data 0.192 (0.322) loss_u loss_u 0.8755 (0.8531) acc_u 18.7500 (18.1250) lr 5.3207e-04 eta 0:00:10
epoch [133/200] batch [40/58] time 0.306 (0.451) data 0.174 (0.320) loss_u loss_u 0.7666 (0.8501) acc_u 25.0000 (18.3594) lr 5.3207e-04 eta 0:00:08
epoch [133/200] batch [45/58] time 0.470 (0.451) data 0.338 (0.320) loss_u loss_u 0.8242 (0.8494) acc_u 18.7500 (18.5417) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [50/58] time 0.386 (0.450) data 0.255 (0.319) loss_u loss_u 0.8296 (0.8477) acc_u 18.7500 (18.8125) lr 5.3207e-04 eta 0:00:03
epoch [133/200] batch [55/58] time 0.573 (0.454) data 0.441 (0.322) loss_u loss_u 0.8223 (0.8471) acc_u 25.0000 (19.0341) lr 5.3207e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1417
confident_label rate tensor(0.4047, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1269
clean true:1185
clean false:84
clean_rate:0.933806146572104
noisy true:534
noisy false:1333
after delete: len(clean_dataset) 1269
after delete: len(noisy_dataset) 1867
epoch [134/200] batch [5/39] time 0.387 (0.455) data 0.255 (0.324) loss_x loss_x 1.0410 (1.2115) acc_x 71.8750 (69.3750) lr 5.1825e-04 eta 0:00:15
epoch [134/200] batch [10/39] time 0.420 (0.467) data 0.289 (0.335) loss_x loss_x 1.0947 (1.2210) acc_x 78.1250 (71.2500) lr 5.1825e-04 eta 0:00:13
epoch [134/200] batch [15/39] time 0.416 (0.475) data 0.285 (0.343) loss_x loss_x 1.0029 (1.1052) acc_x 71.8750 (73.3333) lr 5.1825e-04 eta 0:00:11
epoch [134/200] batch [20/39] time 0.574 (0.479) data 0.443 (0.348) loss_x loss_x 0.8130 (1.0812) acc_x 84.3750 (73.9062) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [25/39] time 0.563 (0.479) data 0.432 (0.348) loss_x loss_x 1.5381 (1.0944) acc_x 65.6250 (73.3750) lr 5.1825e-04 eta 0:00:06
epoch [134/200] batch [30/39] time 0.444 (0.469) data 0.313 (0.337) loss_x loss_x 1.1514 (1.0917) acc_x 71.8750 (73.5417) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [35/39] time 0.452 (0.467) data 0.322 (0.336) loss_x loss_x 0.9365 (1.0662) acc_x 84.3750 (73.7500) lr 5.1825e-04 eta 0:00:01
epoch [134/200] batch [5/58] time 0.482 (0.466) data 0.350 (0.335) loss_u loss_u 0.7612 (0.8253) acc_u 25.0000 (19.3750) lr 5.1825e-04 eta 0:00:24
epoch [134/200] batch [10/58] time 0.412 (0.465) data 0.280 (0.334) loss_u loss_u 0.8525 (0.8462) acc_u 15.6250 (18.1250) lr 5.1825e-04 eta 0:00:22
epoch [134/200] batch [15/58] time 0.432 (0.464) data 0.299 (0.333) loss_u loss_u 0.8413 (0.8448) acc_u 25.0000 (18.7500) lr 5.1825e-04 eta 0:00:19
epoch [134/200] batch [20/58] time 0.341 (0.463) data 0.209 (0.331) loss_u loss_u 0.8530 (0.8568) acc_u 15.6250 (17.0312) lr 5.1825e-04 eta 0:00:17
epoch [134/200] batch [25/58] time 0.426 (0.464) data 0.295 (0.333) loss_u loss_u 0.8447 (0.8557) acc_u 18.7500 (17.6250) lr 5.1825e-04 eta 0:00:15
epoch [134/200] batch [30/58] time 0.364 (0.464) data 0.233 (0.333) loss_u loss_u 0.8853 (0.8599) acc_u 15.6250 (16.8750) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [35/58] time 0.614 (0.464) data 0.483 (0.332) loss_u loss_u 0.9062 (0.8571) acc_u 9.3750 (17.0536) lr 5.1825e-04 eta 0:00:10
epoch [134/200] batch [40/58] time 0.348 (0.461) data 0.217 (0.330) loss_u loss_u 0.7744 (0.8520) acc_u 31.2500 (17.6562) lr 5.1825e-04 eta 0:00:08
epoch [134/200] batch [45/58] time 0.467 (0.462) data 0.337 (0.331) loss_u loss_u 0.9023 (0.8508) acc_u 12.5000 (18.1944) lr 5.1825e-04 eta 0:00:06
epoch [134/200] batch [50/58] time 0.377 (0.459) data 0.246 (0.327) loss_u loss_u 0.8301 (0.8501) acc_u 21.8750 (18.2500) lr 5.1825e-04 eta 0:00:03
epoch [134/200] batch [55/58] time 0.545 (0.460) data 0.414 (0.328) loss_u loss_u 0.8838 (0.8530) acc_u 18.7500 (18.0682) lr 5.1825e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1425
confident_label rate tensor(0.4005, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1256
clean true:1175
clean false:81
clean_rate:0.9355095541401274
noisy true:536
noisy false:1344
after delete: len(clean_dataset) 1256
after delete: len(noisy_dataset) 1880
epoch [135/200] batch [5/39] time 0.360 (0.487) data 0.229 (0.356) loss_x loss_x 1.4814 (1.0229) acc_x 62.5000 (74.3750) lr 5.0454e-04 eta 0:00:16
epoch [135/200] batch [10/39] time 0.391 (0.458) data 0.260 (0.327) loss_x loss_x 0.7925 (1.0863) acc_x 81.2500 (73.1250) lr 5.0454e-04 eta 0:00:13
epoch [135/200] batch [15/39] time 0.533 (0.452) data 0.402 (0.321) loss_x loss_x 0.4688 (1.1188) acc_x 87.5000 (73.1250) lr 5.0454e-04 eta 0:00:10
epoch [135/200] batch [20/39] time 0.515 (0.450) data 0.385 (0.319) loss_x loss_x 0.7651 (1.1212) acc_x 84.3750 (73.5938) lr 5.0454e-04 eta 0:00:08
epoch [135/200] batch [25/39] time 0.593 (0.452) data 0.463 (0.321) loss_x loss_x 1.1914 (1.1154) acc_x 62.5000 (73.6250) lr 5.0454e-04 eta 0:00:06
epoch [135/200] batch [30/39] time 0.531 (0.457) data 0.401 (0.327) loss_x loss_x 0.9365 (1.1037) acc_x 75.0000 (74.0625) lr 5.0454e-04 eta 0:00:04
epoch [135/200] batch [35/39] time 0.586 (0.467) data 0.456 (0.336) loss_x loss_x 1.1309 (1.1221) acc_x 62.5000 (72.5893) lr 5.0454e-04 eta 0:00:01
epoch [135/200] batch [5/58] time 0.391 (0.465) data 0.260 (0.334) loss_u loss_u 0.8965 (0.8762) acc_u 12.5000 (13.7500) lr 5.0454e-04 eta 0:00:24
epoch [135/200] batch [10/58] time 0.649 (0.471) data 0.517 (0.340) loss_u loss_u 0.8862 (0.8593) acc_u 9.3750 (15.3125) lr 5.0454e-04 eta 0:00:22
epoch [135/200] batch [15/58] time 0.448 (0.469) data 0.317 (0.338) loss_u loss_u 0.8926 (0.8656) acc_u 15.6250 (15.2083) lr 5.0454e-04 eta 0:00:20
epoch [135/200] batch [20/58] time 0.606 (0.471) data 0.475 (0.340) loss_u loss_u 0.8389 (0.8570) acc_u 18.7500 (16.8750) lr 5.0454e-04 eta 0:00:17
epoch [135/200] batch [25/58] time 0.483 (0.470) data 0.348 (0.339) loss_u loss_u 0.8589 (0.8584) acc_u 18.7500 (16.5000) lr 5.0454e-04 eta 0:00:15
epoch [135/200] batch [30/58] time 0.419 (0.472) data 0.289 (0.340) loss_u loss_u 0.7827 (0.8512) acc_u 34.3750 (17.6042) lr 5.0454e-04 eta 0:00:13
epoch [135/200] batch [35/58] time 0.369 (0.468) data 0.238 (0.336) loss_u loss_u 0.9414 (0.8539) acc_u 6.2500 (17.5893) lr 5.0454e-04 eta 0:00:10
epoch [135/200] batch [40/58] time 0.459 (0.464) data 0.328 (0.333) loss_u loss_u 0.8979 (0.8530) acc_u 12.5000 (17.7344) lr 5.0454e-04 eta 0:00:08
epoch [135/200] batch [45/58] time 0.326 (0.461) data 0.195 (0.330) loss_u loss_u 0.8057 (0.8534) acc_u 28.1250 (17.9861) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [50/58] time 0.509 (0.462) data 0.378 (0.330) loss_u loss_u 0.8530 (0.8541) acc_u 15.6250 (17.8125) lr 5.0454e-04 eta 0:00:03
epoch [135/200] batch [55/58] time 0.484 (0.462) data 0.354 (0.331) loss_u loss_u 0.8643 (0.8570) acc_u 9.3750 (17.4432) lr 5.0454e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1395
confident_label rate tensor(0.4069, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1276
clean true:1196
clean false:80
clean_rate:0.9373040752351097
noisy true:545
noisy false:1315
after delete: len(clean_dataset) 1276
after delete: len(noisy_dataset) 1860
epoch [136/200] batch [5/39] time 0.486 (0.478) data 0.354 (0.348) loss_x loss_x 1.1172 (0.9966) acc_x 71.8750 (76.2500) lr 4.9096e-04 eta 0:00:16
epoch [136/200] batch [10/39] time 0.483 (0.514) data 0.353 (0.383) loss_x loss_x 0.7314 (1.0414) acc_x 84.3750 (73.7500) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [15/39] time 0.628 (0.531) data 0.496 (0.400) loss_x loss_x 1.0703 (1.0189) acc_x 68.7500 (74.1667) lr 4.9096e-04 eta 0:00:12
epoch [136/200] batch [20/39] time 0.476 (0.533) data 0.345 (0.402) loss_x loss_x 0.7876 (0.9979) acc_x 75.0000 (73.7500) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [25/39] time 0.353 (0.522) data 0.222 (0.391) loss_x loss_x 1.0146 (1.0469) acc_x 68.7500 (73.2500) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [30/39] time 0.413 (0.512) data 0.282 (0.381) loss_x loss_x 0.9238 (1.0809) acc_x 81.2500 (72.0833) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [35/39] time 0.505 (0.507) data 0.374 (0.376) loss_x loss_x 0.8149 (1.0758) acc_x 81.2500 (72.1429) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [5/58] time 0.490 (0.503) data 0.358 (0.371) loss_u loss_u 0.8618 (0.8758) acc_u 18.7500 (16.8750) lr 4.9096e-04 eta 0:00:26
epoch [136/200] batch [10/58] time 0.388 (0.493) data 0.256 (0.362) loss_u loss_u 0.8262 (0.8503) acc_u 18.7500 (19.0625) lr 4.9096e-04 eta 0:00:23
epoch [136/200] batch [15/58] time 0.351 (0.486) data 0.219 (0.354) loss_u loss_u 0.8564 (0.8520) acc_u 18.7500 (18.3333) lr 4.9096e-04 eta 0:00:20
epoch [136/200] batch [20/58] time 0.441 (0.477) data 0.310 (0.346) loss_u loss_u 0.9111 (0.8478) acc_u 6.2500 (19.0625) lr 4.9096e-04 eta 0:00:18
epoch [136/200] batch [25/58] time 0.403 (0.471) data 0.272 (0.340) loss_u loss_u 0.7944 (0.8425) acc_u 31.2500 (20.0000) lr 4.9096e-04 eta 0:00:15
epoch [136/200] batch [30/58] time 0.377 (0.468) data 0.246 (0.336) loss_u loss_u 0.7622 (0.8433) acc_u 25.0000 (19.4792) lr 4.9096e-04 eta 0:00:13
epoch [136/200] batch [35/58] time 0.490 (0.468) data 0.359 (0.337) loss_u loss_u 0.8462 (0.8441) acc_u 15.6250 (19.2857) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [40/58] time 0.444 (0.465) data 0.312 (0.334) loss_u loss_u 0.7681 (0.8453) acc_u 28.1250 (18.9844) lr 4.9096e-04 eta 0:00:08
epoch [136/200] batch [45/58] time 0.353 (0.463) data 0.221 (0.332) loss_u loss_u 0.8945 (0.8475) acc_u 9.3750 (18.4722) lr 4.9096e-04 eta 0:00:06
epoch [136/200] batch [50/58] time 0.360 (0.462) data 0.228 (0.331) loss_u loss_u 0.7163 (0.8448) acc_u 34.3750 (18.8125) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [55/58] time 0.608 (0.467) data 0.473 (0.336) loss_u loss_u 0.9131 (0.8462) acc_u 9.3750 (18.8636) lr 4.9096e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1431
confident_label rate tensor(0.4075, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1278
clean true:1178
clean false:100
clean_rate:0.9217527386541471
noisy true:527
noisy false:1331
after delete: len(clean_dataset) 1278
after delete: len(noisy_dataset) 1858
epoch [137/200] batch [5/39] time 0.346 (0.416) data 0.215 (0.286) loss_x loss_x 1.3418 (1.1514) acc_x 65.6250 (69.3750) lr 4.7750e-04 eta 0:00:14
epoch [137/200] batch [10/39] time 0.459 (0.427) data 0.329 (0.297) loss_x loss_x 1.0557 (1.1406) acc_x 71.8750 (70.3125) lr 4.7750e-04 eta 0:00:12
epoch [137/200] batch [15/39] time 0.385 (0.439) data 0.254 (0.308) loss_x loss_x 0.9526 (1.0958) acc_x 78.1250 (72.0833) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [20/39] time 0.492 (0.443) data 0.362 (0.313) loss_x loss_x 0.7388 (1.0973) acc_x 78.1250 (72.3438) lr 4.7750e-04 eta 0:00:08
epoch [137/200] batch [25/39] time 0.474 (0.447) data 0.345 (0.316) loss_x loss_x 0.7217 (1.1267) acc_x 78.1250 (71.2500) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [30/39] time 0.480 (0.458) data 0.350 (0.328) loss_x loss_x 1.0322 (1.1194) acc_x 65.6250 (70.7292) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [35/39] time 0.692 (0.464) data 0.561 (0.333) loss_x loss_x 0.8330 (1.0984) acc_x 75.0000 (70.4464) lr 4.7750e-04 eta 0:00:01
epoch [137/200] batch [5/58] time 0.558 (0.472) data 0.427 (0.342) loss_u loss_u 0.9062 (0.8137) acc_u 12.5000 (23.7500) lr 4.7750e-04 eta 0:00:25
epoch [137/200] batch [10/58] time 0.436 (0.469) data 0.306 (0.339) loss_u loss_u 0.7637 (0.8317) acc_u 31.2500 (21.5625) lr 4.7750e-04 eta 0:00:22
epoch [137/200] batch [15/58] time 0.334 (0.466) data 0.203 (0.336) loss_u loss_u 0.8999 (0.8454) acc_u 15.6250 (19.5833) lr 4.7750e-04 eta 0:00:20
epoch [137/200] batch [20/58] time 0.422 (0.463) data 0.291 (0.332) loss_u loss_u 0.8940 (0.8493) acc_u 15.6250 (19.6875) lr 4.7750e-04 eta 0:00:17
epoch [137/200] batch [25/58] time 0.433 (0.461) data 0.301 (0.330) loss_u loss_u 0.9634 (0.8478) acc_u 0.0000 (19.1250) lr 4.7750e-04 eta 0:00:15
epoch [137/200] batch [30/58] time 0.286 (0.462) data 0.155 (0.331) loss_u loss_u 0.8384 (0.8477) acc_u 21.8750 (19.0625) lr 4.7750e-04 eta 0:00:12
epoch [137/200] batch [35/58] time 0.384 (0.459) data 0.253 (0.328) loss_u loss_u 0.8760 (0.8467) acc_u 15.6250 (19.0179) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [40/58] time 0.548 (0.459) data 0.417 (0.328) loss_u loss_u 0.8447 (0.8468) acc_u 15.6250 (18.9062) lr 4.7750e-04 eta 0:00:08
epoch [137/200] batch [45/58] time 0.438 (0.458) data 0.306 (0.327) loss_u loss_u 0.8750 (0.8474) acc_u 12.5000 (19.0278) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [50/58] time 0.435 (0.459) data 0.304 (0.328) loss_u loss_u 0.8979 (0.8497) acc_u 12.5000 (18.8125) lr 4.7750e-04 eta 0:00:03
epoch [137/200] batch [55/58] time 0.476 (0.459) data 0.345 (0.328) loss_u loss_u 0.8965 (0.8490) acc_u 9.3750 (18.9205) lr 4.7750e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1429
confident_label rate tensor(0.3992, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1252
clean true:1176
clean false:76
clean_rate:0.939297124600639
noisy true:531
noisy false:1353
after delete: len(clean_dataset) 1252
after delete: len(noisy_dataset) 1884
epoch [138/200] batch [5/39] time 0.480 (0.536) data 0.348 (0.405) loss_x loss_x 0.8032 (1.2175) acc_x 78.1250 (68.7500) lr 4.6417e-04 eta 0:00:18
epoch [138/200] batch [10/39] time 0.522 (0.497) data 0.391 (0.366) loss_x loss_x 1.3574 (1.2059) acc_x 59.3750 (70.6250) lr 4.6417e-04 eta 0:00:14
epoch [138/200] batch [15/39] time 0.530 (0.501) data 0.400 (0.371) loss_x loss_x 1.5127 (1.2312) acc_x 65.6250 (69.5833) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [20/39] time 0.427 (0.491) data 0.297 (0.361) loss_x loss_x 0.5942 (1.1766) acc_x 90.6250 (71.4062) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [25/39] time 0.388 (0.479) data 0.257 (0.349) loss_x loss_x 1.1084 (1.1469) acc_x 62.5000 (70.7500) lr 4.6417e-04 eta 0:00:06
epoch [138/200] batch [30/39] time 0.425 (0.472) data 0.295 (0.341) loss_x loss_x 1.0576 (1.1550) acc_x 71.8750 (70.6250) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [35/39] time 0.425 (0.468) data 0.295 (0.338) loss_x loss_x 1.2520 (1.1313) acc_x 59.3750 (71.2500) lr 4.6417e-04 eta 0:00:01
epoch [138/200] batch [5/58] time 0.366 (0.462) data 0.234 (0.331) loss_u loss_u 0.7944 (0.8243) acc_u 25.0000 (22.5000) lr 4.6417e-04 eta 0:00:24
epoch [138/200] batch [10/58] time 0.428 (0.467) data 0.296 (0.336) loss_u loss_u 0.8228 (0.8224) acc_u 18.7500 (22.1875) lr 4.6417e-04 eta 0:00:22
epoch [138/200] batch [15/58] time 0.442 (0.468) data 0.311 (0.337) loss_u loss_u 0.8862 (0.8412) acc_u 18.7500 (20.6250) lr 4.6417e-04 eta 0:00:20
epoch [138/200] batch [20/58] time 0.461 (0.464) data 0.328 (0.333) loss_u loss_u 0.9199 (0.8545) acc_u 12.5000 (18.2812) lr 4.6417e-04 eta 0:00:17
epoch [138/200] batch [25/58] time 0.383 (0.463) data 0.252 (0.332) loss_u loss_u 0.8613 (0.8549) acc_u 21.8750 (18.7500) lr 4.6417e-04 eta 0:00:15
epoch [138/200] batch [30/58] time 0.546 (0.465) data 0.413 (0.334) loss_u loss_u 0.8657 (0.8488) acc_u 15.6250 (19.3750) lr 4.6417e-04 eta 0:00:13
epoch [138/200] batch [35/58] time 0.361 (0.462) data 0.229 (0.331) loss_u loss_u 0.8022 (0.8464) acc_u 28.1250 (19.6429) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [40/58] time 0.367 (0.461) data 0.236 (0.330) loss_u loss_u 0.9038 (0.8436) acc_u 12.5000 (19.9219) lr 4.6417e-04 eta 0:00:08
epoch [138/200] batch [45/58] time 0.476 (0.460) data 0.342 (0.329) loss_u loss_u 0.8237 (0.8407) acc_u 25.0000 (20.1389) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [50/58] time 0.392 (0.461) data 0.261 (0.330) loss_u loss_u 0.9067 (0.8382) acc_u 12.5000 (20.3750) lr 4.6417e-04 eta 0:00:03
epoch [138/200] batch [55/58] time 0.461 (0.462) data 0.329 (0.330) loss_u loss_u 0.7798 (0.8410) acc_u 28.1250 (19.8295) lr 4.6417e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1429
confident_label rate tensor(0.3983, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1249
clean true:1166
clean false:83
clean_rate:0.933546837469976
noisy true:541
noisy false:1346
after delete: len(clean_dataset) 1249
after delete: len(noisy_dataset) 1887
epoch [139/200] batch [5/39] time 0.392 (0.467) data 0.262 (0.337) loss_x loss_x 1.1904 (1.1664) acc_x 71.8750 (68.7500) lr 4.5098e-04 eta 0:00:15
epoch [139/200] batch [10/39] time 0.351 (0.465) data 0.221 (0.334) loss_x loss_x 1.1562 (1.1904) acc_x 71.8750 (68.4375) lr 4.5098e-04 eta 0:00:13
epoch [139/200] batch [15/39] time 0.458 (0.464) data 0.327 (0.334) loss_x loss_x 0.6235 (1.1759) acc_x 87.5000 (69.7917) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [20/39] time 0.595 (0.474) data 0.465 (0.343) loss_x loss_x 0.9136 (1.1452) acc_x 78.1250 (71.4062) lr 4.5098e-04 eta 0:00:08
epoch [139/200] batch [25/39] time 0.471 (0.466) data 0.341 (0.335) loss_x loss_x 1.1230 (1.1497) acc_x 75.0000 (71.2500) lr 4.5098e-04 eta 0:00:06
epoch [139/200] batch [30/39] time 0.449 (0.464) data 0.318 (0.334) loss_x loss_x 1.5889 (1.1538) acc_x 65.6250 (71.1458) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [35/39] time 0.410 (0.457) data 0.280 (0.327) loss_x loss_x 1.6709 (1.1768) acc_x 46.8750 (70.5357) lr 4.5098e-04 eta 0:00:01
epoch [139/200] batch [5/58] time 0.422 (0.456) data 0.291 (0.325) loss_u loss_u 0.8315 (0.8263) acc_u 18.7500 (20.0000) lr 4.5098e-04 eta 0:00:24
epoch [139/200] batch [10/58] time 0.416 (0.455) data 0.283 (0.324) loss_u loss_u 0.8135 (0.8323) acc_u 18.7500 (18.7500) lr 4.5098e-04 eta 0:00:21
epoch [139/200] batch [15/58] time 0.443 (0.456) data 0.312 (0.325) loss_u loss_u 0.8164 (0.8427) acc_u 34.3750 (18.5417) lr 4.5098e-04 eta 0:00:19
epoch [139/200] batch [20/58] time 0.394 (0.452) data 0.263 (0.321) loss_u loss_u 0.8989 (0.8354) acc_u 9.3750 (19.2188) lr 4.5098e-04 eta 0:00:17
epoch [139/200] batch [25/58] time 0.365 (0.450) data 0.233 (0.319) loss_u loss_u 0.8125 (0.8354) acc_u 18.7500 (19.2500) lr 4.5098e-04 eta 0:00:14
epoch [139/200] batch [30/58] time 0.346 (0.454) data 0.214 (0.323) loss_u loss_u 0.7295 (0.8251) acc_u 34.3750 (20.5208) lr 4.5098e-04 eta 0:00:12
epoch [139/200] batch [35/58] time 0.457 (0.457) data 0.322 (0.326) loss_u loss_u 0.8799 (0.8287) acc_u 9.3750 (20.2679) lr 4.5098e-04 eta 0:00:10
epoch [139/200] batch [40/58] time 0.545 (0.456) data 0.411 (0.325) loss_u loss_u 0.7915 (0.8298) acc_u 21.8750 (20.0781) lr 4.5098e-04 eta 0:00:08
epoch [139/200] batch [45/58] time 0.451 (0.462) data 0.319 (0.330) loss_u loss_u 0.9331 (0.8342) acc_u 9.3750 (19.7222) lr 4.5098e-04 eta 0:00:06
epoch [139/200] batch [50/58] time 0.453 (0.466) data 0.318 (0.334) loss_u loss_u 0.8379 (0.8363) acc_u 21.8750 (19.7500) lr 4.5098e-04 eta 0:00:03
epoch [139/200] batch [55/58] time 0.615 (0.465) data 0.482 (0.334) loss_u loss_u 0.8208 (0.8388) acc_u 18.7500 (19.3750) lr 4.5098e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.4037, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1266
clean true:1187
clean false:79
clean_rate:0.9375987361769352
noisy true:529
noisy false:1341
after delete: len(clean_dataset) 1266
after delete: len(noisy_dataset) 1870
epoch [140/200] batch [5/39] time 0.427 (0.460) data 0.296 (0.329) loss_x loss_x 0.9966 (1.0797) acc_x 75.0000 (76.8750) lr 4.3792e-04 eta 0:00:15
epoch [140/200] batch [10/39] time 0.422 (0.458) data 0.292 (0.327) loss_x loss_x 1.2637 (1.0988) acc_x 68.7500 (75.9375) lr 4.3792e-04 eta 0:00:13
epoch [140/200] batch [15/39] time 0.378 (0.445) data 0.247 (0.314) loss_x loss_x 1.2002 (1.0954) acc_x 71.8750 (74.7917) lr 4.3792e-04 eta 0:00:10
epoch [140/200] batch [20/39] time 0.466 (0.438) data 0.335 (0.307) loss_x loss_x 1.0537 (1.1196) acc_x 75.0000 (73.9062) lr 4.3792e-04 eta 0:00:08
epoch [140/200] batch [25/39] time 0.361 (0.442) data 0.230 (0.312) loss_x loss_x 0.9116 (1.0931) acc_x 78.1250 (74.6250) lr 4.3792e-04 eta 0:00:06
epoch [140/200] batch [30/39] time 0.507 (0.445) data 0.377 (0.314) loss_x loss_x 1.5820 (1.1050) acc_x 50.0000 (73.5417) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [35/39] time 0.356 (0.453) data 0.226 (0.323) loss_x loss_x 0.9839 (1.1108) acc_x 81.2500 (73.3036) lr 4.3792e-04 eta 0:00:01
epoch [140/200] batch [5/58] time 0.402 (0.453) data 0.270 (0.323) loss_u loss_u 0.8613 (0.8439) acc_u 12.5000 (20.0000) lr 4.3792e-04 eta 0:00:24
epoch [140/200] batch [10/58] time 0.469 (0.458) data 0.337 (0.327) loss_u loss_u 0.8628 (0.8538) acc_u 12.5000 (17.8125) lr 4.3792e-04 eta 0:00:21
epoch [140/200] batch [15/58] time 0.528 (0.455) data 0.397 (0.324) loss_u loss_u 0.9341 (0.8496) acc_u 6.2500 (18.5417) lr 4.3792e-04 eta 0:00:19
epoch [140/200] batch [20/58] time 0.475 (0.451) data 0.343 (0.320) loss_u loss_u 0.9087 (0.8507) acc_u 12.5000 (18.9062) lr 4.3792e-04 eta 0:00:17
epoch [140/200] batch [25/58] time 0.439 (0.452) data 0.307 (0.321) loss_u loss_u 0.9014 (0.8579) acc_u 12.5000 (18.3750) lr 4.3792e-04 eta 0:00:14
epoch [140/200] batch [30/58] time 0.521 (0.450) data 0.388 (0.319) loss_u loss_u 0.8198 (0.8548) acc_u 21.8750 (18.6458) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [35/58] time 0.544 (0.453) data 0.408 (0.322) loss_u loss_u 0.8306 (0.8488) acc_u 15.6250 (18.9286) lr 4.3792e-04 eta 0:00:10
epoch [140/200] batch [40/58] time 0.459 (0.452) data 0.329 (0.321) loss_u loss_u 0.8848 (0.8510) acc_u 18.7500 (18.9844) lr 4.3792e-04 eta 0:00:08
epoch [140/200] batch [45/58] time 0.529 (0.453) data 0.398 (0.322) loss_u loss_u 0.8398 (0.8485) acc_u 25.0000 (19.0972) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [50/58] time 0.586 (0.456) data 0.454 (0.324) loss_u loss_u 0.8652 (0.8515) acc_u 18.7500 (18.5625) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [55/58] time 0.424 (0.454) data 0.293 (0.322) loss_u loss_u 0.8706 (0.8538) acc_u 12.5000 (18.1818) lr 4.3792e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1443
confident_label rate tensor(0.4072, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1277
clean true:1179
clean false:98
clean_rate:0.9232576350822239
noisy true:514
noisy false:1345
after delete: len(clean_dataset) 1277
after delete: len(noisy_dataset) 1859
epoch [141/200] batch [5/39] time 0.371 (0.429) data 0.240 (0.298) loss_x loss_x 1.0898 (1.0281) acc_x 71.8750 (76.2500) lr 4.2499e-04 eta 0:00:14
epoch [141/200] batch [10/39] time 0.368 (0.439) data 0.237 (0.308) loss_x loss_x 0.8799 (1.0528) acc_x 81.2500 (74.3750) lr 4.2499e-04 eta 0:00:12
epoch [141/200] batch [15/39] time 0.441 (0.450) data 0.309 (0.319) loss_x loss_x 0.6548 (1.0436) acc_x 81.2500 (74.1667) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [20/39] time 0.513 (0.470) data 0.374 (0.339) loss_x loss_x 1.5439 (1.1075) acc_x 56.2500 (72.0312) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [25/39] time 0.539 (0.466) data 0.409 (0.335) loss_x loss_x 0.5532 (1.0927) acc_x 84.3750 (72.3750) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [30/39] time 0.445 (0.460) data 0.314 (0.328) loss_x loss_x 1.0918 (1.1132) acc_x 75.0000 (71.8750) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [35/39] time 0.485 (0.464) data 0.353 (0.333) loss_x loss_x 1.2646 (1.1031) acc_x 62.5000 (72.3214) lr 4.2499e-04 eta 0:00:01
epoch [141/200] batch [5/58] time 0.362 (0.457) data 0.230 (0.326) loss_u loss_u 0.9429 (0.8827) acc_u 6.2500 (12.5000) lr 4.2499e-04 eta 0:00:24
epoch [141/200] batch [10/58] time 0.377 (0.458) data 0.246 (0.326) loss_u loss_u 0.8765 (0.8622) acc_u 21.8750 (16.5625) lr 4.2499e-04 eta 0:00:21
epoch [141/200] batch [15/58] time 0.467 (0.458) data 0.335 (0.327) loss_u loss_u 0.8398 (0.8630) acc_u 18.7500 (15.8333) lr 4.2499e-04 eta 0:00:19
epoch [141/200] batch [20/58] time 0.373 (0.460) data 0.242 (0.329) loss_u loss_u 0.8525 (0.8609) acc_u 15.6250 (16.4062) lr 4.2499e-04 eta 0:00:17
epoch [141/200] batch [25/58] time 0.380 (0.457) data 0.247 (0.326) loss_u loss_u 0.8325 (0.8485) acc_u 25.0000 (18.2500) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [30/58] time 0.409 (0.456) data 0.277 (0.325) loss_u loss_u 0.8989 (0.8469) acc_u 12.5000 (18.3333) lr 4.2499e-04 eta 0:00:12
epoch [141/200] batch [35/58] time 0.483 (0.454) data 0.352 (0.323) loss_u loss_u 0.8809 (0.8442) acc_u 12.5000 (18.8393) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [40/58] time 0.393 (0.456) data 0.261 (0.324) loss_u loss_u 0.8062 (0.8400) acc_u 18.7500 (19.3750) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [45/58] time 0.403 (0.456) data 0.272 (0.325) loss_u loss_u 0.8579 (0.8396) acc_u 18.7500 (19.4444) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [50/58] time 0.378 (0.464) data 0.247 (0.332) loss_u loss_u 0.9336 (0.8420) acc_u 6.2500 (19.1250) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [55/58] time 0.460 (0.463) data 0.330 (0.332) loss_u loss_u 0.8315 (0.8428) acc_u 15.6250 (18.9773) lr 4.2499e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1419
confident_label rate tensor(0.4082, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1280
clean true:1196
clean false:84
clean_rate:0.934375
noisy true:521
noisy false:1335
after delete: len(clean_dataset) 1280
after delete: len(noisy_dataset) 1856
epoch [142/200] batch [5/40] time 0.631 (0.488) data 0.501 (0.357) loss_x loss_x 1.5449 (1.1703) acc_x 65.6250 (73.1250) lr 4.1221e-04 eta 0:00:17
epoch [142/200] batch [10/40] time 0.523 (0.473) data 0.391 (0.342) loss_x loss_x 0.8965 (1.1532) acc_x 78.1250 (73.1250) lr 4.1221e-04 eta 0:00:14
epoch [142/200] batch [15/40] time 0.439 (0.472) data 0.308 (0.342) loss_x loss_x 1.3125 (1.1496) acc_x 65.6250 (72.0833) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [20/40] time 0.557 (0.488) data 0.425 (0.357) loss_x loss_x 0.9785 (1.1366) acc_x 84.3750 (72.5000) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [25/40] time 0.521 (0.478) data 0.390 (0.347) loss_x loss_x 0.7881 (1.0924) acc_x 75.0000 (73.0000) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [30/40] time 0.439 (0.473) data 0.309 (0.343) loss_x loss_x 1.4824 (1.0918) acc_x 65.6250 (72.6042) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [35/40] time 0.459 (0.478) data 0.328 (0.348) loss_x loss_x 1.0234 (1.1092) acc_x 81.2500 (72.7679) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [40/40] time 0.467 (0.479) data 0.335 (0.348) loss_x loss_x 1.0488 (1.1195) acc_x 65.6250 (72.0312) lr 4.1221e-04 eta 0:00:00
epoch [142/200] batch [5/58] time 0.451 (0.482) data 0.319 (0.351) loss_u loss_u 0.8574 (0.8392) acc_u 15.6250 (17.5000) lr 4.1221e-04 eta 0:00:25
epoch [142/200] batch [10/58] time 0.606 (0.480) data 0.474 (0.349) loss_u loss_u 0.9355 (0.8521) acc_u 9.3750 (16.5625) lr 4.1221e-04 eta 0:00:23
epoch [142/200] batch [15/58] time 0.386 (0.478) data 0.255 (0.347) loss_u loss_u 0.8618 (0.8441) acc_u 15.6250 (18.3333) lr 4.1221e-04 eta 0:00:20
epoch [142/200] batch [20/58] time 0.588 (0.476) data 0.456 (0.345) loss_u loss_u 0.8179 (0.8438) acc_u 21.8750 (19.0625) lr 4.1221e-04 eta 0:00:18
epoch [142/200] batch [25/58] time 0.362 (0.468) data 0.231 (0.337) loss_u loss_u 0.7861 (0.8420) acc_u 25.0000 (19.3750) lr 4.1221e-04 eta 0:00:15
epoch [142/200] batch [30/58] time 0.458 (0.468) data 0.326 (0.337) loss_u loss_u 0.8638 (0.8428) acc_u 18.7500 (19.4792) lr 4.1221e-04 eta 0:00:13
epoch [142/200] batch [35/58] time 0.437 (0.468) data 0.307 (0.337) loss_u loss_u 0.8555 (0.8430) acc_u 15.6250 (19.4643) lr 4.1221e-04 eta 0:00:10
epoch [142/200] batch [40/58] time 0.347 (0.464) data 0.215 (0.333) loss_u loss_u 0.9160 (0.8442) acc_u 15.6250 (19.2188) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [45/58] time 0.413 (0.459) data 0.283 (0.328) loss_u loss_u 0.8774 (0.8439) acc_u 15.6250 (19.3056) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [50/58] time 0.506 (0.459) data 0.375 (0.328) loss_u loss_u 0.8057 (0.8418) acc_u 21.8750 (19.4375) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [55/58] time 0.508 (0.459) data 0.377 (0.328) loss_u loss_u 0.8042 (0.8445) acc_u 18.7500 (18.8636) lr 4.1221e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1446
confident_label rate tensor(0.3967, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1244
clean true:1168
clean false:76
clean_rate:0.9389067524115756
noisy true:522
noisy false:1370
after delete: len(clean_dataset) 1244
after delete: len(noisy_dataset) 1892
epoch [143/200] batch [5/38] time 0.405 (0.571) data 0.271 (0.438) loss_x loss_x 1.1660 (1.3495) acc_x 65.6250 (64.3750) lr 3.9958e-04 eta 0:00:18
epoch [143/200] batch [10/38] time 0.364 (0.549) data 0.234 (0.417) loss_x loss_x 1.0908 (1.3149) acc_x 68.7500 (68.1250) lr 3.9958e-04 eta 0:00:15
epoch [143/200] batch [15/38] time 0.361 (0.519) data 0.231 (0.388) loss_x loss_x 1.3154 (1.2473) acc_x 68.7500 (69.5833) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [20/38] time 0.412 (0.497) data 0.281 (0.366) loss_x loss_x 1.0771 (1.2423) acc_x 75.0000 (69.6875) lr 3.9958e-04 eta 0:00:08
epoch [143/200] batch [25/38] time 0.469 (0.490) data 0.338 (0.359) loss_x loss_x 1.0312 (1.1641) acc_x 75.0000 (71.0000) lr 3.9958e-04 eta 0:00:06
epoch [143/200] batch [30/38] time 0.416 (0.485) data 0.286 (0.354) loss_x loss_x 1.4307 (1.1473) acc_x 65.6250 (71.9792) lr 3.9958e-04 eta 0:00:03
epoch [143/200] batch [35/38] time 0.421 (0.478) data 0.291 (0.347) loss_x loss_x 0.8926 (1.1341) acc_x 81.2500 (72.3214) lr 3.9958e-04 eta 0:00:01
epoch [143/200] batch [5/59] time 0.464 (0.477) data 0.334 (0.346) loss_u loss_u 0.8164 (0.8517) acc_u 31.2500 (20.6250) lr 3.9958e-04 eta 0:00:25
epoch [143/200] batch [10/59] time 0.425 (0.473) data 0.294 (0.342) loss_u loss_u 0.8433 (0.8542) acc_u 15.6250 (20.6250) lr 3.9958e-04 eta 0:00:23
epoch [143/200] batch [15/59] time 0.459 (0.470) data 0.329 (0.338) loss_u loss_u 0.8232 (0.8488) acc_u 21.8750 (20.4167) lr 3.9958e-04 eta 0:00:20
epoch [143/200] batch [20/59] time 0.400 (0.463) data 0.268 (0.332) loss_u loss_u 0.7261 (0.8427) acc_u 31.2500 (20.3125) lr 3.9958e-04 eta 0:00:18
epoch [143/200] batch [25/59] time 0.386 (0.458) data 0.255 (0.327) loss_u loss_u 0.8936 (0.8488) acc_u 12.5000 (19.1250) lr 3.9958e-04 eta 0:00:15
epoch [143/200] batch [30/59] time 0.375 (0.461) data 0.244 (0.329) loss_u loss_u 0.9209 (0.8487) acc_u 12.5000 (19.0625) lr 3.9958e-04 eta 0:00:13
epoch [143/200] batch [35/59] time 0.436 (0.457) data 0.304 (0.326) loss_u loss_u 0.9067 (0.8506) acc_u 18.7500 (19.1071) lr 3.9958e-04 eta 0:00:10
epoch [143/200] batch [40/59] time 0.418 (0.455) data 0.287 (0.324) loss_u loss_u 0.9121 (0.8498) acc_u 12.5000 (18.9844) lr 3.9958e-04 eta 0:00:08
epoch [143/200] batch [45/59] time 0.619 (0.458) data 0.488 (0.326) loss_u loss_u 0.8296 (0.8464) acc_u 28.1250 (19.2361) lr 3.9958e-04 eta 0:00:06
epoch [143/200] batch [50/59] time 0.552 (0.462) data 0.421 (0.331) loss_u loss_u 0.7524 (0.8456) acc_u 34.3750 (19.1875) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [55/59] time 0.458 (0.463) data 0.328 (0.332) loss_u loss_u 0.8877 (0.8449) acc_u 6.2500 (19.1477) lr 3.9958e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1448
confident_label rate tensor(0.4021, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1261
clean true:1174
clean false:87
clean_rate:0.9310071371927042
noisy true:514
noisy false:1361
after delete: len(clean_dataset) 1261
after delete: len(noisy_dataset) 1875
epoch [144/200] batch [5/39] time 0.551 (0.461) data 0.421 (0.331) loss_x loss_x 1.3740 (1.3600) acc_x 68.7500 (68.7500) lr 3.8709e-04 eta 0:00:15
epoch [144/200] batch [10/39] time 0.399 (0.483) data 0.269 (0.352) loss_x loss_x 0.8672 (1.1803) acc_x 81.2500 (71.8750) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [15/39] time 0.506 (0.502) data 0.375 (0.371) loss_x loss_x 1.3926 (1.1656) acc_x 65.6250 (71.8750) lr 3.8709e-04 eta 0:00:12
epoch [144/200] batch [20/39] time 0.554 (0.497) data 0.423 (0.367) loss_x loss_x 1.0996 (1.1295) acc_x 75.0000 (72.8125) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [25/39] time 0.381 (0.486) data 0.251 (0.356) loss_x loss_x 1.5166 (1.1672) acc_x 65.6250 (72.5000) lr 3.8709e-04 eta 0:00:06
epoch [144/200] batch [30/39] time 0.453 (0.486) data 0.323 (0.356) loss_x loss_x 0.9209 (1.1684) acc_x 71.8750 (71.7708) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [35/39] time 0.461 (0.488) data 0.331 (0.357) loss_x loss_x 1.1660 (1.1619) acc_x 71.8750 (71.6071) lr 3.8709e-04 eta 0:00:01
epoch [144/200] batch [5/58] time 0.539 (0.482) data 0.407 (0.352) loss_u loss_u 0.8193 (0.8233) acc_u 25.0000 (20.6250) lr 3.8709e-04 eta 0:00:25
epoch [144/200] batch [10/58] time 0.362 (0.473) data 0.231 (0.343) loss_u loss_u 0.8340 (0.8332) acc_u 25.0000 (20.6250) lr 3.8709e-04 eta 0:00:22
epoch [144/200] batch [15/58] time 0.382 (0.471) data 0.251 (0.340) loss_u loss_u 0.7905 (0.8335) acc_u 21.8750 (21.0417) lr 3.8709e-04 eta 0:00:20
epoch [144/200] batch [20/58] time 0.450 (0.468) data 0.319 (0.337) loss_u loss_u 0.8354 (0.8318) acc_u 21.8750 (21.2500) lr 3.8709e-04 eta 0:00:17
epoch [144/200] batch [25/58] time 0.426 (0.466) data 0.294 (0.335) loss_u loss_u 0.8931 (0.8301) acc_u 6.2500 (21.0000) lr 3.8709e-04 eta 0:00:15
epoch [144/200] batch [30/58] time 0.424 (0.462) data 0.293 (0.331) loss_u loss_u 0.8433 (0.8375) acc_u 12.5000 (19.7917) lr 3.8709e-04 eta 0:00:12
epoch [144/200] batch [35/58] time 0.467 (0.462) data 0.336 (0.332) loss_u loss_u 0.8423 (0.8409) acc_u 12.5000 (19.1071) lr 3.8709e-04 eta 0:00:10
epoch [144/200] batch [40/58] time 0.435 (0.461) data 0.303 (0.330) loss_u loss_u 0.8306 (0.8405) acc_u 18.7500 (19.4531) lr 3.8709e-04 eta 0:00:08
epoch [144/200] batch [45/58] time 0.440 (0.460) data 0.309 (0.329) loss_u loss_u 0.8623 (0.8423) acc_u 18.7500 (19.3750) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [50/58] time 0.456 (0.458) data 0.324 (0.327) loss_u loss_u 0.8022 (0.8435) acc_u 25.0000 (19.1250) lr 3.8709e-04 eta 0:00:03
epoch [144/200] batch [55/58] time 0.493 (0.456) data 0.362 (0.325) loss_u loss_u 0.8745 (0.8444) acc_u 15.6250 (19.0341) lr 3.8709e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1455
confident_label rate tensor(0.4034, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1265
clean true:1177
clean false:88
clean_rate:0.9304347826086956
noisy true:504
noisy false:1367
after delete: len(clean_dataset) 1265
after delete: len(noisy_dataset) 1871
epoch [145/200] batch [5/39] time 0.500 (0.515) data 0.369 (0.385) loss_x loss_x 1.4121 (1.0793) acc_x 65.6250 (72.5000) lr 3.7476e-04 eta 0:00:17
epoch [145/200] batch [10/39] time 0.525 (0.500) data 0.394 (0.370) loss_x loss_x 1.0928 (1.0773) acc_x 78.1250 (73.7500) lr 3.7476e-04 eta 0:00:14
epoch [145/200] batch [15/39] time 0.440 (0.503) data 0.310 (0.372) loss_x loss_x 1.1055 (1.0497) acc_x 71.8750 (73.9583) lr 3.7476e-04 eta 0:00:12
epoch [145/200] batch [20/39] time 0.488 (0.498) data 0.357 (0.368) loss_x loss_x 1.1123 (1.0286) acc_x 78.1250 (75.7812) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [25/39] time 0.546 (0.500) data 0.415 (0.369) loss_x loss_x 1.2129 (1.0526) acc_x 71.8750 (75.2500) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [30/39] time 0.433 (0.502) data 0.302 (0.371) loss_x loss_x 0.9429 (1.0249) acc_x 78.1250 (75.4167) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [35/39] time 0.476 (0.496) data 0.345 (0.365) loss_x loss_x 0.7700 (1.0183) acc_x 84.3750 (75.5357) lr 3.7476e-04 eta 0:00:01
epoch [145/200] batch [5/58] time 0.633 (0.488) data 0.502 (0.358) loss_u loss_u 0.7661 (0.7764) acc_u 28.1250 (26.2500) lr 3.7476e-04 eta 0:00:25
epoch [145/200] batch [10/58] time 0.359 (0.481) data 0.228 (0.351) loss_u loss_u 0.8452 (0.8227) acc_u 21.8750 (22.1875) lr 3.7476e-04 eta 0:00:23
epoch [145/200] batch [15/58] time 0.481 (0.476) data 0.350 (0.345) loss_u loss_u 0.8735 (0.8328) acc_u 15.6250 (20.6250) lr 3.7476e-04 eta 0:00:20
epoch [145/200] batch [20/58] time 0.400 (0.471) data 0.268 (0.340) loss_u loss_u 0.7925 (0.8361) acc_u 21.8750 (20.3125) lr 3.7476e-04 eta 0:00:17
epoch [145/200] batch [25/58] time 0.478 (0.477) data 0.343 (0.346) loss_u loss_u 0.8706 (0.8366) acc_u 15.6250 (20.3750) lr 3.7476e-04 eta 0:00:15
epoch [145/200] batch [30/58] time 0.467 (0.475) data 0.336 (0.344) loss_u loss_u 0.8257 (0.8395) acc_u 21.8750 (20.0000) lr 3.7476e-04 eta 0:00:13
epoch [145/200] batch [35/58] time 0.421 (0.476) data 0.289 (0.345) loss_u loss_u 0.8252 (0.8387) acc_u 18.7500 (19.8214) lr 3.7476e-04 eta 0:00:10
epoch [145/200] batch [40/58] time 0.350 (0.475) data 0.219 (0.344) loss_u loss_u 0.7920 (0.8361) acc_u 21.8750 (20.0781) lr 3.7476e-04 eta 0:00:08
epoch [145/200] batch [45/58] time 0.425 (0.470) data 0.294 (0.339) loss_u loss_u 0.8931 (0.8400) acc_u 12.5000 (19.8611) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [50/58] time 0.456 (0.469) data 0.325 (0.338) loss_u loss_u 0.8726 (0.8432) acc_u 15.6250 (19.5000) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [55/58] time 0.418 (0.467) data 0.287 (0.336) loss_u loss_u 0.8804 (0.8409) acc_u 18.7500 (20.1136) lr 3.7476e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1394
confident_label rate tensor(0.4190, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1314
clean true:1223
clean false:91
clean_rate:0.9307458143074582
noisy true:519
noisy false:1303
after delete: len(clean_dataset) 1314
after delete: len(noisy_dataset) 1822
epoch [146/200] batch [5/41] time 0.462 (0.512) data 0.331 (0.381) loss_x loss_x 1.1914 (1.1620) acc_x 56.2500 (66.2500) lr 3.6258e-04 eta 0:00:18
epoch [146/200] batch [10/41] time 0.531 (0.501) data 0.400 (0.370) loss_x loss_x 1.3457 (1.1972) acc_x 65.6250 (67.5000) lr 3.6258e-04 eta 0:00:15
epoch [146/200] batch [15/41] time 0.506 (0.502) data 0.375 (0.371) loss_x loss_x 1.1455 (1.1380) acc_x 68.7500 (69.5833) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [20/41] time 0.510 (0.492) data 0.380 (0.361) loss_x loss_x 0.7310 (1.0842) acc_x 87.5000 (71.7188) lr 3.6258e-04 eta 0:00:10
epoch [146/200] batch [25/41] time 0.541 (0.478) data 0.411 (0.347) loss_x loss_x 1.0732 (1.0907) acc_x 65.6250 (71.1250) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [30/41] time 0.429 (0.474) data 0.297 (0.343) loss_x loss_x 1.2666 (1.1209) acc_x 59.3750 (70.0000) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [35/41] time 0.410 (0.471) data 0.280 (0.340) loss_x loss_x 1.5791 (1.1095) acc_x 59.3750 (70.2679) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [40/41] time 0.444 (0.464) data 0.313 (0.334) loss_x loss_x 0.9551 (1.1344) acc_x 81.2500 (70.0781) lr 3.6258e-04 eta 0:00:00
epoch [146/200] batch [5/56] time 0.682 (0.469) data 0.549 (0.338) loss_u loss_u 0.9165 (0.8323) acc_u 12.5000 (20.0000) lr 3.6258e-04 eta 0:00:23
epoch [146/200] batch [10/56] time 0.558 (0.469) data 0.420 (0.338) loss_u loss_u 0.8599 (0.8502) acc_u 12.5000 (17.5000) lr 3.6258e-04 eta 0:00:21
epoch [146/200] batch [15/56] time 0.509 (0.467) data 0.378 (0.336) loss_u loss_u 0.8008 (0.8440) acc_u 21.8750 (18.3333) lr 3.6258e-04 eta 0:00:19
epoch [146/200] batch [20/56] time 0.559 (0.473) data 0.429 (0.342) loss_u loss_u 0.9087 (0.8529) acc_u 12.5000 (17.5000) lr 3.6258e-04 eta 0:00:17
epoch [146/200] batch [25/56] time 0.479 (0.472) data 0.349 (0.341) loss_u loss_u 0.8789 (0.8594) acc_u 15.6250 (16.5000) lr 3.6258e-04 eta 0:00:14
epoch [146/200] batch [30/56] time 0.438 (0.467) data 0.307 (0.336) loss_u loss_u 0.7603 (0.8561) acc_u 28.1250 (17.2917) lr 3.6258e-04 eta 0:00:12
epoch [146/200] batch [35/56] time 0.732 (0.469) data 0.601 (0.338) loss_u loss_u 0.7686 (0.8538) acc_u 37.5000 (17.7679) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [40/56] time 0.390 (0.468) data 0.258 (0.337) loss_u loss_u 0.8867 (0.8545) acc_u 12.5000 (17.8125) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [45/56] time 0.409 (0.467) data 0.277 (0.336) loss_u loss_u 0.7954 (0.8549) acc_u 21.8750 (17.9861) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [50/56] time 0.614 (0.467) data 0.483 (0.336) loss_u loss_u 0.8057 (0.8537) acc_u 25.0000 (18.1875) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [55/56] time 0.417 (0.465) data 0.287 (0.334) loss_u loss_u 0.8120 (0.8509) acc_u 21.8750 (18.6364) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1422
confident_label rate tensor(0.4031, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1264
clean true:1181
clean false:83
clean_rate:0.9343354430379747
noisy true:533
noisy false:1339
after delete: len(clean_dataset) 1264
after delete: len(noisy_dataset) 1872
epoch [147/200] batch [5/39] time 0.346 (0.437) data 0.216 (0.307) loss_x loss_x 0.8774 (1.1190) acc_x 71.8750 (73.7500) lr 3.5055e-04 eta 0:00:14
epoch [147/200] batch [10/39] time 0.421 (0.445) data 0.290 (0.314) loss_x loss_x 1.1123 (1.1568) acc_x 71.8750 (71.2500) lr 3.5055e-04 eta 0:00:12
epoch [147/200] batch [15/39] time 0.593 (0.463) data 0.462 (0.332) loss_x loss_x 0.9683 (1.1521) acc_x 68.7500 (70.8333) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [20/39] time 0.456 (0.463) data 0.324 (0.332) loss_x loss_x 1.2021 (1.1749) acc_x 68.7500 (70.7812) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [25/39] time 0.532 (0.467) data 0.401 (0.337) loss_x loss_x 1.0537 (1.0960) acc_x 71.8750 (71.6250) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [30/39] time 0.455 (0.466) data 0.325 (0.335) loss_x loss_x 1.6104 (1.1090) acc_x 65.6250 (71.4583) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [35/39] time 0.608 (0.467) data 0.478 (0.337) loss_x loss_x 1.4609 (1.1162) acc_x 71.8750 (71.3393) lr 3.5055e-04 eta 0:00:01
epoch [147/200] batch [5/58] time 0.495 (0.475) data 0.364 (0.344) loss_u loss_u 0.8701 (0.8575) acc_u 15.6250 (20.6250) lr 3.5055e-04 eta 0:00:25
epoch [147/200] batch [10/58] time 0.500 (0.480) data 0.368 (0.349) loss_u loss_u 0.7808 (0.8324) acc_u 28.1250 (23.1250) lr 3.5055e-04 eta 0:00:23
epoch [147/200] batch [15/58] time 0.421 (0.476) data 0.290 (0.345) loss_u loss_u 0.8506 (0.8348) acc_u 18.7500 (22.5000) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [20/58] time 0.471 (0.475) data 0.339 (0.344) loss_u loss_u 0.8008 (0.8414) acc_u 28.1250 (21.2500) lr 3.5055e-04 eta 0:00:18
epoch [147/200] batch [25/58] time 0.704 (0.478) data 0.572 (0.347) loss_u loss_u 0.8691 (0.8469) acc_u 21.8750 (20.6250) lr 3.5055e-04 eta 0:00:15
epoch [147/200] batch [30/58] time 0.371 (0.473) data 0.239 (0.342) loss_u loss_u 0.8979 (0.8483) acc_u 9.3750 (20.6250) lr 3.5055e-04 eta 0:00:13
epoch [147/200] batch [35/58] time 0.510 (0.473) data 0.378 (0.342) loss_u loss_u 0.8535 (0.8483) acc_u 18.7500 (20.1786) lr 3.5055e-04 eta 0:00:10
epoch [147/200] batch [40/58] time 0.512 (0.471) data 0.379 (0.340) loss_u loss_u 0.9033 (0.8460) acc_u 12.5000 (20.2344) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [45/58] time 0.392 (0.470) data 0.262 (0.339) loss_u loss_u 0.8970 (0.8474) acc_u 18.7500 (20.2778) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [50/58] time 0.451 (0.469) data 0.319 (0.338) loss_u loss_u 0.8398 (0.8485) acc_u 18.7500 (19.9375) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [55/58] time 0.450 (0.467) data 0.319 (0.336) loss_u loss_u 0.8643 (0.8477) acc_u 15.6250 (19.8295) lr 3.5055e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1425
confident_label rate tensor(0.4024, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1262
clean true:1172
clean false:90
clean_rate:0.9286846275752774
noisy true:539
noisy false:1335
after delete: len(clean_dataset) 1262
after delete: len(noisy_dataset) 1874
epoch [148/200] batch [5/39] time 0.437 (0.493) data 0.307 (0.363) loss_x loss_x 1.6855 (1.2946) acc_x 62.5000 (70.6250) lr 3.3869e-04 eta 0:00:16
epoch [148/200] batch [10/39] time 0.570 (0.488) data 0.439 (0.357) loss_x loss_x 0.7065 (1.1964) acc_x 87.5000 (73.1250) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [15/39] time 0.426 (0.477) data 0.296 (0.347) loss_x loss_x 0.5117 (1.1444) acc_x 90.6250 (73.3333) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [20/39] time 0.478 (0.475) data 0.348 (0.345) loss_x loss_x 1.4492 (1.1378) acc_x 71.8750 (72.6562) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [25/39] time 0.734 (0.485) data 0.604 (0.355) loss_x loss_x 0.8281 (1.1106) acc_x 78.1250 (72.8750) lr 3.3869e-04 eta 0:00:06
epoch [148/200] batch [30/39] time 0.413 (0.482) data 0.282 (0.351) loss_x loss_x 1.7803 (1.1223) acc_x 65.6250 (73.0208) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [35/39] time 0.516 (0.480) data 0.386 (0.350) loss_x loss_x 1.1279 (1.1160) acc_x 75.0000 (72.9464) lr 3.3869e-04 eta 0:00:01
epoch [148/200] batch [5/58] time 0.458 (0.478) data 0.326 (0.347) loss_u loss_u 0.8735 (0.8593) acc_u 18.7500 (20.0000) lr 3.3869e-04 eta 0:00:25
epoch [148/200] batch [10/58] time 0.437 (0.472) data 0.305 (0.342) loss_u loss_u 0.7939 (0.8338) acc_u 25.0000 (21.8750) lr 3.3869e-04 eta 0:00:22
epoch [148/200] batch [15/58] time 0.365 (0.467) data 0.234 (0.336) loss_u loss_u 0.9150 (0.8414) acc_u 9.3750 (20.6250) lr 3.3869e-04 eta 0:00:20
epoch [148/200] batch [20/58] time 0.526 (0.465) data 0.395 (0.334) loss_u loss_u 0.8154 (0.8380) acc_u 28.1250 (21.2500) lr 3.3869e-04 eta 0:00:17
epoch [148/200] batch [25/58] time 0.551 (0.464) data 0.420 (0.333) loss_u loss_u 0.8071 (0.8448) acc_u 28.1250 (20.6250) lr 3.3869e-04 eta 0:00:15
epoch [148/200] batch [30/58] time 0.514 (0.469) data 0.383 (0.338) loss_u loss_u 0.7808 (0.8429) acc_u 21.8750 (20.3125) lr 3.3869e-04 eta 0:00:13
epoch [148/200] batch [35/58] time 0.514 (0.470) data 0.383 (0.340) loss_u loss_u 0.7617 (0.8445) acc_u 31.2500 (20.1786) lr 3.3869e-04 eta 0:00:10
epoch [148/200] batch [40/58] time 0.436 (0.468) data 0.305 (0.337) loss_u loss_u 0.8247 (0.8460) acc_u 18.7500 (19.7656) lr 3.3869e-04 eta 0:00:08
epoch [148/200] batch [45/58] time 0.431 (0.467) data 0.299 (0.336) loss_u loss_u 0.8667 (0.8468) acc_u 18.7500 (19.5833) lr 3.3869e-04 eta 0:00:06
epoch [148/200] batch [50/58] time 0.519 (0.467) data 0.389 (0.336) loss_u loss_u 0.8755 (0.8493) acc_u 9.3750 (19.1875) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [55/58] time 0.427 (0.465) data 0.296 (0.334) loss_u loss_u 0.8237 (0.8472) acc_u 25.0000 (19.3182) lr 3.3869e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1444
confident_label rate tensor(0.3964, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1243
clean true:1158
clean false:85
clean_rate:0.9316170555108608
noisy true:534
noisy false:1359
after delete: len(clean_dataset) 1243
after delete: len(noisy_dataset) 1893
epoch [149/200] batch [5/38] time 0.484 (0.468) data 0.352 (0.337) loss_x loss_x 1.2637 (0.8807) acc_x 68.7500 (77.5000) lr 3.2699e-04 eta 0:00:15
epoch [149/200] batch [10/38] time 0.540 (0.481) data 0.409 (0.350) loss_x loss_x 1.0869 (1.0314) acc_x 71.8750 (75.0000) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [15/38] time 0.543 (0.477) data 0.412 (0.346) loss_x loss_x 0.8862 (1.0353) acc_x 81.2500 (75.8333) lr 3.2699e-04 eta 0:00:10
epoch [149/200] batch [20/38] time 0.520 (0.494) data 0.389 (0.363) loss_x loss_x 0.9565 (1.0488) acc_x 75.0000 (74.8438) lr 3.2699e-04 eta 0:00:08
epoch [149/200] batch [25/38] time 0.523 (0.511) data 0.392 (0.380) loss_x loss_x 1.0674 (1.0245) acc_x 71.8750 (74.7500) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [30/38] time 0.415 (0.512) data 0.284 (0.381) loss_x loss_x 0.9180 (1.0247) acc_x 87.5000 (75.1042) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [35/38] time 0.408 (0.498) data 0.277 (0.367) loss_x loss_x 0.8735 (1.0062) acc_x 78.1250 (75.7143) lr 3.2699e-04 eta 0:00:01
epoch [149/200] batch [5/59] time 0.367 (0.498) data 0.236 (0.367) loss_u loss_u 0.8525 (0.8661) acc_u 15.6250 (17.5000) lr 3.2699e-04 eta 0:00:26
epoch [149/200] batch [10/59] time 0.469 (0.497) data 0.337 (0.366) loss_u loss_u 0.7607 (0.8339) acc_u 31.2500 (21.2500) lr 3.2699e-04 eta 0:00:24
epoch [149/200] batch [15/59] time 0.479 (0.504) data 0.348 (0.373) loss_u loss_u 0.8389 (0.8410) acc_u 25.0000 (21.0417) lr 3.2699e-04 eta 0:00:22
epoch [149/200] batch [20/59] time 0.442 (0.501) data 0.310 (0.369) loss_u loss_u 0.8052 (0.8337) acc_u 28.1250 (22.1875) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [25/59] time 0.320 (0.496) data 0.188 (0.365) loss_u loss_u 0.8281 (0.8358) acc_u 25.0000 (21.5000) lr 3.2699e-04 eta 0:00:16
epoch [149/200] batch [30/59] time 0.316 (0.491) data 0.186 (0.360) loss_u loss_u 0.8740 (0.8338) acc_u 15.6250 (21.4583) lr 3.2699e-04 eta 0:00:14
epoch [149/200] batch [35/59] time 0.474 (0.485) data 0.344 (0.354) loss_u loss_u 0.8477 (0.8329) acc_u 15.6250 (21.5179) lr 3.2699e-04 eta 0:00:11
epoch [149/200] batch [40/59] time 0.533 (0.487) data 0.402 (0.355) loss_u loss_u 0.8779 (0.8358) acc_u 15.6250 (20.7812) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [45/59] time 0.358 (0.480) data 0.226 (0.349) loss_u loss_u 0.8560 (0.8385) acc_u 15.6250 (20.2083) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [50/59] time 0.412 (0.477) data 0.281 (0.346) loss_u loss_u 0.8721 (0.8424) acc_u 15.6250 (19.5625) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [55/59] time 0.373 (0.473) data 0.241 (0.342) loss_u loss_u 0.8813 (0.8427) acc_u 15.6250 (19.7159) lr 3.2699e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.4133, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1296
clean true:1209
clean false:87
clean_rate:0.9328703703703703
noisy true:507
noisy false:1333
after delete: len(clean_dataset) 1296
after delete: len(noisy_dataset) 1840
epoch [150/200] batch [5/40] time 0.410 (0.517) data 0.279 (0.385) loss_x loss_x 1.2969 (1.1223) acc_x 68.7500 (71.8750) lr 3.1545e-04 eta 0:00:18
epoch [150/200] batch [10/40] time 0.430 (0.463) data 0.299 (0.332) loss_x loss_x 1.8584 (1.1505) acc_x 53.1250 (71.5625) lr 3.1545e-04 eta 0:00:13
epoch [150/200] batch [15/40] time 0.490 (0.449) data 0.359 (0.318) loss_x loss_x 1.2041 (1.1332) acc_x 68.7500 (72.7083) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [20/40] time 0.525 (0.441) data 0.394 (0.310) loss_x loss_x 1.8584 (1.1476) acc_x 68.7500 (73.5938) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [25/40] time 0.533 (0.459) data 0.402 (0.328) loss_x loss_x 0.7139 (1.1420) acc_x 84.3750 (73.8750) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [30/40] time 0.364 (0.459) data 0.233 (0.328) loss_x loss_x 1.1982 (1.1508) acc_x 68.7500 (73.1250) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [35/40] time 0.397 (0.459) data 0.266 (0.328) loss_x loss_x 1.1807 (1.1418) acc_x 65.6250 (72.5893) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [40/40] time 0.611 (0.468) data 0.480 (0.337) loss_x loss_x 0.8311 (1.1148) acc_x 75.0000 (72.6562) lr 3.1545e-04 eta 0:00:00
epoch [150/200] batch [5/57] time 0.360 (0.473) data 0.229 (0.342) loss_u loss_u 0.8374 (0.8404) acc_u 18.7500 (18.1250) lr 3.1545e-04 eta 0:00:24
epoch [150/200] batch [10/57] time 0.453 (0.470) data 0.322 (0.339) loss_u loss_u 0.8306 (0.8390) acc_u 18.7500 (18.7500) lr 3.1545e-04 eta 0:00:22
epoch [150/200] batch [15/57] time 0.540 (0.475) data 0.409 (0.344) loss_u loss_u 0.8701 (0.8522) acc_u 12.5000 (17.2917) lr 3.1545e-04 eta 0:00:19
epoch [150/200] batch [20/57] time 0.378 (0.470) data 0.247 (0.339) loss_u loss_u 0.8784 (0.8608) acc_u 15.6250 (16.2500) lr 3.1545e-04 eta 0:00:17
epoch [150/200] batch [25/57] time 0.425 (0.468) data 0.293 (0.337) loss_u loss_u 0.8193 (0.8585) acc_u 21.8750 (16.7500) lr 3.1545e-04 eta 0:00:14
epoch [150/200] batch [30/57] time 0.419 (0.468) data 0.288 (0.337) loss_u loss_u 0.7881 (0.8559) acc_u 28.1250 (17.6042) lr 3.1545e-04 eta 0:00:12
epoch [150/200] batch [35/57] time 0.348 (0.464) data 0.218 (0.333) loss_u loss_u 0.8081 (0.8548) acc_u 18.7500 (17.6786) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [40/57] time 0.440 (0.462) data 0.309 (0.331) loss_u loss_u 0.8652 (0.8513) acc_u 15.6250 (18.2812) lr 3.1545e-04 eta 0:00:07
epoch [150/200] batch [45/57] time 0.428 (0.458) data 0.297 (0.327) loss_u loss_u 0.8730 (0.8493) acc_u 12.5000 (18.2639) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [50/57] time 0.464 (0.454) data 0.333 (0.323) loss_u loss_u 0.8867 (0.8492) acc_u 12.5000 (18.1250) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [55/57] time 0.529 (0.451) data 0.397 (0.320) loss_u loss_u 0.8218 (0.8502) acc_u 21.8750 (18.0682) lr 3.1545e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1428
confident_label rate tensor(0.3992, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1252
clean true:1167
clean false:85
clean_rate:0.9321086261980831
noisy true:541
noisy false:1343
after delete: len(clean_dataset) 1252
after delete: len(noisy_dataset) 1884
epoch [151/200] batch [5/39] time 0.459 (0.497) data 0.328 (0.367) loss_x loss_x 1.1172 (1.1264) acc_x 71.8750 (71.8750) lr 3.0409e-04 eta 0:00:16
epoch [151/200] batch [10/39] time 0.609 (0.512) data 0.477 (0.381) loss_x loss_x 1.0225 (1.0477) acc_x 75.0000 (74.0625) lr 3.0409e-04 eta 0:00:14
epoch [151/200] batch [15/39] time 0.531 (0.509) data 0.400 (0.379) loss_x loss_x 0.9238 (1.0860) acc_x 65.6250 (72.7083) lr 3.0409e-04 eta 0:00:12
epoch [151/200] batch [20/39] time 0.387 (0.501) data 0.257 (0.370) loss_x loss_x 1.2207 (1.0919) acc_x 78.1250 (73.2812) lr 3.0409e-04 eta 0:00:09
epoch [151/200] batch [25/39] time 0.461 (0.507) data 0.330 (0.376) loss_x loss_x 1.0811 (1.0634) acc_x 81.2500 (74.5000) lr 3.0409e-04 eta 0:00:07
epoch [151/200] batch [30/39] time 0.680 (0.501) data 0.548 (0.371) loss_x loss_x 0.9185 (1.0796) acc_x 75.0000 (73.8542) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [35/39] time 0.414 (0.492) data 0.283 (0.361) loss_x loss_x 1.0977 (1.0572) acc_x 65.6250 (74.1071) lr 3.0409e-04 eta 0:00:01
epoch [151/200] batch [5/58] time 0.454 (0.487) data 0.322 (0.356) loss_u loss_u 0.8857 (0.8677) acc_u 12.5000 (15.6250) lr 3.0409e-04 eta 0:00:25
epoch [151/200] batch [10/58] time 0.501 (0.489) data 0.369 (0.358) loss_u loss_u 0.8110 (0.8533) acc_u 21.8750 (17.8125) lr 3.0409e-04 eta 0:00:23
epoch [151/200] batch [15/58] time 0.368 (0.486) data 0.237 (0.355) loss_u loss_u 0.8682 (0.8445) acc_u 12.5000 (18.7500) lr 3.0409e-04 eta 0:00:20
epoch [151/200] batch [20/58] time 0.496 (0.480) data 0.364 (0.349) loss_u loss_u 0.8550 (0.8480) acc_u 15.6250 (18.2812) lr 3.0409e-04 eta 0:00:18
epoch [151/200] batch [25/58] time 0.436 (0.473) data 0.304 (0.342) loss_u loss_u 0.7729 (0.8413) acc_u 25.0000 (18.7500) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [30/58] time 0.422 (0.468) data 0.290 (0.337) loss_u loss_u 0.8403 (0.8417) acc_u 15.6250 (18.6458) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [35/58] time 0.412 (0.465) data 0.282 (0.334) loss_u loss_u 0.9009 (0.8445) acc_u 9.3750 (18.4821) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [40/58] time 0.457 (0.469) data 0.323 (0.338) loss_u loss_u 0.8271 (0.8411) acc_u 18.7500 (19.0625) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [45/58] time 0.551 (0.471) data 0.421 (0.340) loss_u loss_u 0.7861 (0.8437) acc_u 28.1250 (18.8194) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [50/58] time 0.480 (0.470) data 0.350 (0.338) loss_u loss_u 0.8823 (0.8440) acc_u 15.6250 (18.6875) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [55/58] time 0.504 (0.470) data 0.373 (0.339) loss_u loss_u 0.7681 (0.8424) acc_u 34.3750 (18.9773) lr 3.0409e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1409
confident_label rate tensor(0.4040, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1267
clean true:1192
clean false:75
clean_rate:0.9408050513022889
noisy true:535
noisy false:1334
after delete: len(clean_dataset) 1267
after delete: len(noisy_dataset) 1869
epoch [152/200] batch [5/39] time 0.373 (0.440) data 0.243 (0.309) loss_x loss_x 0.5264 (0.9505) acc_x 87.5000 (79.3750) lr 2.9289e-04 eta 0:00:14
epoch [152/200] batch [10/39] time 0.438 (0.428) data 0.308 (0.297) loss_x loss_x 0.8896 (0.9498) acc_x 68.7500 (76.2500) lr 2.9289e-04 eta 0:00:12
epoch [152/200] batch [15/39] time 0.604 (0.444) data 0.474 (0.314) loss_x loss_x 1.3730 (0.9583) acc_x 68.7500 (76.8750) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [20/39] time 0.404 (0.441) data 0.273 (0.310) loss_x loss_x 1.1162 (1.0130) acc_x 68.7500 (75.3125) lr 2.9289e-04 eta 0:00:08
epoch [152/200] batch [25/39] time 0.500 (0.452) data 0.370 (0.321) loss_x loss_x 1.4004 (1.0353) acc_x 65.6250 (74.3750) lr 2.9289e-04 eta 0:00:06
epoch [152/200] batch [30/39] time 0.517 (0.462) data 0.385 (0.331) loss_x loss_x 1.3213 (1.0614) acc_x 68.7500 (73.3333) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [35/39] time 0.429 (0.474) data 0.298 (0.344) loss_x loss_x 1.1309 (1.0810) acc_x 71.8750 (73.1250) lr 2.9289e-04 eta 0:00:01
epoch [152/200] batch [5/58] time 0.498 (0.473) data 0.366 (0.343) loss_u loss_u 0.8135 (0.8046) acc_u 25.0000 (25.6250) lr 2.9289e-04 eta 0:00:25
epoch [152/200] batch [10/58] time 0.484 (0.473) data 0.353 (0.342) loss_u loss_u 0.8828 (0.8274) acc_u 12.5000 (21.2500) lr 2.9289e-04 eta 0:00:22
epoch [152/200] batch [15/58] time 0.361 (0.472) data 0.230 (0.341) loss_u loss_u 0.8286 (0.8361) acc_u 21.8750 (20.4167) lr 2.9289e-04 eta 0:00:20
epoch [152/200] batch [20/58] time 0.343 (0.467) data 0.211 (0.337) loss_u loss_u 0.8125 (0.8355) acc_u 21.8750 (20.3125) lr 2.9289e-04 eta 0:00:17
epoch [152/200] batch [25/58] time 0.549 (0.464) data 0.418 (0.334) loss_u loss_u 0.8345 (0.8384) acc_u 21.8750 (20.5000) lr 2.9289e-04 eta 0:00:15
epoch [152/200] batch [30/58] time 0.473 (0.463) data 0.340 (0.332) loss_u loss_u 0.8364 (0.8425) acc_u 18.7500 (20.0000) lr 2.9289e-04 eta 0:00:12
epoch [152/200] batch [35/58] time 0.406 (0.465) data 0.275 (0.334) loss_u loss_u 0.9146 (0.8508) acc_u 12.5000 (18.8393) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [40/58] time 0.422 (0.466) data 0.290 (0.335) loss_u loss_u 0.8574 (0.8511) acc_u 18.7500 (19.1406) lr 2.9289e-04 eta 0:00:08
epoch [152/200] batch [45/58] time 0.359 (0.464) data 0.227 (0.333) loss_u loss_u 0.8877 (0.8528) acc_u 18.7500 (18.8194) lr 2.9289e-04 eta 0:00:06
epoch [152/200] batch [50/58] time 0.406 (0.461) data 0.275 (0.330) loss_u loss_u 0.7334 (0.8478) acc_u 34.3750 (19.4375) lr 2.9289e-04 eta 0:00:03
epoch [152/200] batch [55/58] time 0.359 (0.459) data 0.227 (0.327) loss_u loss_u 0.7622 (0.8497) acc_u 31.2500 (19.2614) lr 2.9289e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1432
confident_label rate tensor(0.4047, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1269
clean true:1187
clean false:82
clean_rate:0.9353821907013397
noisy true:517
noisy false:1350
after delete: len(clean_dataset) 1269
after delete: len(noisy_dataset) 1867
epoch [153/200] batch [5/39] time 0.460 (0.463) data 0.328 (0.332) loss_x loss_x 1.0615 (1.1275) acc_x 78.1250 (75.0000) lr 2.8187e-04 eta 0:00:15
epoch [153/200] batch [10/39] time 0.450 (0.443) data 0.319 (0.311) loss_x loss_x 0.8940 (1.0270) acc_x 75.0000 (75.0000) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [15/39] time 0.412 (0.444) data 0.281 (0.313) loss_x loss_x 1.1719 (1.0726) acc_x 71.8750 (74.7917) lr 2.8187e-04 eta 0:00:10
epoch [153/200] batch [20/39] time 0.434 (0.430) data 0.302 (0.299) loss_x loss_x 1.2432 (1.0900) acc_x 62.5000 (74.2188) lr 2.8187e-04 eta 0:00:08
epoch [153/200] batch [25/39] time 0.529 (0.440) data 0.398 (0.309) loss_x loss_x 0.7510 (1.0871) acc_x 81.2500 (74.5000) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [30/39] time 0.577 (0.449) data 0.447 (0.318) loss_x loss_x 1.4365 (1.0676) acc_x 65.6250 (74.2708) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [35/39] time 0.513 (0.462) data 0.382 (0.331) loss_x loss_x 0.8325 (1.0878) acc_x 78.1250 (73.9286) lr 2.8187e-04 eta 0:00:01
epoch [153/200] batch [5/58] time 0.398 (0.456) data 0.266 (0.325) loss_u loss_u 0.8589 (0.8338) acc_u 12.5000 (16.8750) lr 2.8187e-04 eta 0:00:24
epoch [153/200] batch [10/58] time 0.329 (0.456) data 0.198 (0.325) loss_u loss_u 0.7725 (0.8402) acc_u 31.2500 (20.0000) lr 2.8187e-04 eta 0:00:21
epoch [153/200] batch [15/58] time 0.364 (0.453) data 0.233 (0.322) loss_u loss_u 0.8555 (0.8389) acc_u 21.8750 (20.2083) lr 2.8187e-04 eta 0:00:19
epoch [153/200] batch [20/58] time 0.401 (0.453) data 0.270 (0.322) loss_u loss_u 0.9150 (0.8444) acc_u 12.5000 (19.3750) lr 2.8187e-04 eta 0:00:17
epoch [153/200] batch [25/58] time 0.573 (0.454) data 0.442 (0.323) loss_u loss_u 0.8950 (0.8459) acc_u 12.5000 (19.7500) lr 2.8187e-04 eta 0:00:14
epoch [153/200] batch [30/58] time 0.370 (0.451) data 0.239 (0.320) loss_u loss_u 0.9053 (0.8468) acc_u 9.3750 (19.1667) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [35/58] time 0.394 (0.453) data 0.263 (0.322) loss_u loss_u 0.9351 (0.8504) acc_u 6.2500 (18.7500) lr 2.8187e-04 eta 0:00:10
epoch [153/200] batch [40/58] time 0.552 (0.458) data 0.420 (0.327) loss_u loss_u 0.8589 (0.8491) acc_u 18.7500 (18.9844) lr 2.8187e-04 eta 0:00:08
epoch [153/200] batch [45/58] time 0.469 (0.457) data 0.338 (0.326) loss_u loss_u 0.8218 (0.8479) acc_u 18.7500 (19.3750) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [50/58] time 0.386 (0.458) data 0.255 (0.327) loss_u loss_u 0.8809 (0.8493) acc_u 12.5000 (19.2500) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [55/58] time 0.315 (0.456) data 0.183 (0.325) loss_u loss_u 0.7778 (0.8473) acc_u 28.1250 (19.3182) lr 2.8187e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1437
confident_label rate tensor(0.4094, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1284
clean true:1188
clean false:96
clean_rate:0.9252336448598131
noisy true:511
noisy false:1341
after delete: len(clean_dataset) 1284
after delete: len(noisy_dataset) 1852
epoch [154/200] batch [5/40] time 0.426 (0.416) data 0.295 (0.285) loss_x loss_x 1.1084 (1.0346) acc_x 78.1250 (74.3750) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [10/40] time 0.431 (0.411) data 0.299 (0.280) loss_x loss_x 0.9937 (1.0796) acc_x 71.8750 (74.0625) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [15/40] time 0.509 (0.433) data 0.379 (0.302) loss_x loss_x 1.1787 (1.0753) acc_x 59.3750 (70.6250) lr 2.7103e-04 eta 0:00:10
epoch [154/200] batch [20/40] time 0.460 (0.443) data 0.330 (0.312) loss_x loss_x 1.6455 (1.1221) acc_x 71.8750 (70.7812) lr 2.7103e-04 eta 0:00:08
epoch [154/200] batch [25/40] time 0.401 (0.449) data 0.271 (0.318) loss_x loss_x 1.3203 (1.1327) acc_x 65.6250 (71.2500) lr 2.7103e-04 eta 0:00:06
epoch [154/200] batch [30/40] time 0.380 (0.451) data 0.250 (0.321) loss_x loss_x 1.2803 (1.1159) acc_x 65.6250 (72.2917) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [35/40] time 0.485 (0.453) data 0.355 (0.322) loss_x loss_x 0.6807 (1.0824) acc_x 78.1250 (72.6786) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [40/40] time 0.371 (0.461) data 0.240 (0.330) loss_x loss_x 1.3594 (1.0847) acc_x 68.7500 (72.2656) lr 2.7103e-04 eta 0:00:00
epoch [154/200] batch [5/57] time 0.513 (0.458) data 0.381 (0.327) loss_u loss_u 0.8555 (0.8552) acc_u 18.7500 (16.8750) lr 2.7103e-04 eta 0:00:23
epoch [154/200] batch [10/57] time 0.379 (0.465) data 0.249 (0.334) loss_u loss_u 0.9058 (0.8643) acc_u 9.3750 (15.3125) lr 2.7103e-04 eta 0:00:21
epoch [154/200] batch [15/57] time 0.442 (0.466) data 0.310 (0.335) loss_u loss_u 0.9199 (0.8607) acc_u 6.2500 (15.6250) lr 2.7103e-04 eta 0:00:19
epoch [154/200] batch [20/57] time 0.491 (0.465) data 0.360 (0.334) loss_u loss_u 0.9146 (0.8618) acc_u 12.5000 (16.5625) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [25/57] time 0.392 (0.463) data 0.261 (0.332) loss_u loss_u 0.7827 (0.8594) acc_u 31.2500 (17.0000) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [30/57] time 0.412 (0.468) data 0.281 (0.337) loss_u loss_u 0.8213 (0.8572) acc_u 25.0000 (17.3958) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [35/57] time 0.733 (0.471) data 0.601 (0.340) loss_u loss_u 0.8813 (0.8626) acc_u 9.3750 (16.6071) lr 2.7103e-04 eta 0:00:10
epoch [154/200] batch [40/57] time 0.489 (0.468) data 0.357 (0.337) loss_u loss_u 0.8687 (0.8562) acc_u 21.8750 (17.5000) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [45/57] time 0.390 (0.466) data 0.258 (0.335) loss_u loss_u 0.9399 (0.8617) acc_u 6.2500 (16.5972) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [50/57] time 0.388 (0.467) data 0.253 (0.335) loss_u loss_u 0.9038 (0.8584) acc_u 21.8750 (17.0625) lr 2.7103e-04 eta 0:00:03
epoch [154/200] batch [55/57] time 0.460 (0.466) data 0.328 (0.335) loss_u loss_u 0.8770 (0.8590) acc_u 9.3750 (16.8750) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1462
confident_label rate tensor(0.3983, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1249
clean true:1160
clean false:89
clean_rate:0.9287429943955164
noisy true:514
noisy false:1373
after delete: len(clean_dataset) 1249
after delete: len(noisy_dataset) 1887
epoch [155/200] batch [5/39] time 0.498 (0.510) data 0.368 (0.379) loss_x loss_x 1.0801 (0.9960) acc_x 81.2500 (75.6250) lr 2.6037e-04 eta 0:00:17
epoch [155/200] batch [10/39] time 0.440 (0.474) data 0.310 (0.343) loss_x loss_x 1.1465 (1.0689) acc_x 62.5000 (72.8125) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [15/39] time 0.405 (0.462) data 0.275 (0.332) loss_x loss_x 1.4551 (1.1178) acc_x 68.7500 (71.8750) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [20/39] time 0.422 (0.454) data 0.291 (0.324) loss_x loss_x 0.9058 (1.1409) acc_x 78.1250 (71.7188) lr 2.6037e-04 eta 0:00:08
epoch [155/200] batch [25/39] time 0.426 (0.455) data 0.296 (0.324) loss_x loss_x 1.7959 (1.1614) acc_x 65.6250 (71.3750) lr 2.6037e-04 eta 0:00:06
epoch [155/200] batch [30/39] time 0.495 (0.456) data 0.363 (0.325) loss_x loss_x 1.0420 (1.1438) acc_x 81.2500 (72.0833) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [35/39] time 0.544 (0.457) data 0.412 (0.327) loss_x loss_x 0.9146 (1.1174) acc_x 75.0000 (72.5893) lr 2.6037e-04 eta 0:00:01
epoch [155/200] batch [5/58] time 0.402 (0.457) data 0.272 (0.327) loss_u loss_u 0.9233 (0.8523) acc_u 15.6250 (20.0000) lr 2.6037e-04 eta 0:00:24
epoch [155/200] batch [10/58] time 0.540 (0.458) data 0.409 (0.327) loss_u loss_u 0.8252 (0.8531) acc_u 18.7500 (19.0625) lr 2.6037e-04 eta 0:00:21
epoch [155/200] batch [15/58] time 0.408 (0.453) data 0.277 (0.322) loss_u loss_u 0.8750 (0.8646) acc_u 15.6250 (17.0833) lr 2.6037e-04 eta 0:00:19
epoch [155/200] batch [20/58] time 0.528 (0.455) data 0.397 (0.324) loss_u loss_u 0.8652 (0.8580) acc_u 21.8750 (18.1250) lr 2.6037e-04 eta 0:00:17
epoch [155/200] batch [25/58] time 0.443 (0.458) data 0.311 (0.327) loss_u loss_u 0.7852 (0.8447) acc_u 28.1250 (19.8750) lr 2.6037e-04 eta 0:00:15
epoch [155/200] batch [30/58] time 0.484 (0.460) data 0.353 (0.329) loss_u loss_u 0.7451 (0.8363) acc_u 28.1250 (20.6250) lr 2.6037e-04 eta 0:00:12
epoch [155/200] batch [35/58] time 0.396 (0.458) data 0.266 (0.327) loss_u loss_u 0.9189 (0.8377) acc_u 6.2500 (20.2679) lr 2.6037e-04 eta 0:00:10
epoch [155/200] batch [40/58] time 0.394 (0.458) data 0.262 (0.327) loss_u loss_u 0.8315 (0.8369) acc_u 18.7500 (20.1562) lr 2.6037e-04 eta 0:00:08
epoch [155/200] batch [45/58] time 0.563 (0.460) data 0.432 (0.329) loss_u loss_u 0.9087 (0.8399) acc_u 9.3750 (19.7917) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [50/58] time 0.364 (0.459) data 0.234 (0.328) loss_u loss_u 0.8657 (0.8397) acc_u 15.6250 (19.5625) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [55/58] time 0.378 (0.459) data 0.246 (0.328) loss_u loss_u 0.8682 (0.8414) acc_u 15.6250 (19.4318) lr 2.6037e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1442
confident_label rate tensor(0.4098, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1285
clean true:1180
clean false:105
clean_rate:0.9182879377431906
noisy true:514
noisy false:1337
after delete: len(clean_dataset) 1285
after delete: len(noisy_dataset) 1851
epoch [156/200] batch [5/40] time 0.719 (0.529) data 0.588 (0.398) loss_x loss_x 1.4795 (0.9877) acc_x 62.5000 (75.0000) lr 2.4989e-04 eta 0:00:18
epoch [156/200] batch [10/40] time 0.371 (0.480) data 0.240 (0.348) loss_x loss_x 1.1172 (1.0263) acc_x 68.7500 (73.1250) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [15/40] time 0.444 (0.481) data 0.309 (0.349) loss_x loss_x 1.2627 (1.0634) acc_x 78.1250 (72.5000) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [20/40] time 0.502 (0.486) data 0.371 (0.355) loss_x loss_x 0.9360 (1.0800) acc_x 78.1250 (72.0312) lr 2.4989e-04 eta 0:00:09
epoch [156/200] batch [25/40] time 0.498 (0.486) data 0.368 (0.354) loss_x loss_x 1.1904 (1.0426) acc_x 68.7500 (73.2500) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [30/40] time 0.476 (0.481) data 0.344 (0.349) loss_x loss_x 1.3516 (1.0667) acc_x 71.8750 (72.8125) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [35/40] time 0.452 (0.477) data 0.322 (0.346) loss_x loss_x 0.8940 (1.0673) acc_x 75.0000 (72.5893) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [40/40] time 0.573 (0.479) data 0.442 (0.348) loss_x loss_x 1.0156 (1.0566) acc_x 75.0000 (72.9688) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/57] time 0.528 (0.478) data 0.397 (0.346) loss_u loss_u 0.8931 (0.8636) acc_u 9.3750 (15.0000) lr 2.4989e-04 eta 0:00:24
epoch [156/200] batch [10/57] time 0.568 (0.473) data 0.437 (0.342) loss_u loss_u 0.8413 (0.8532) acc_u 12.5000 (16.5625) lr 2.4989e-04 eta 0:00:22
epoch [156/200] batch [15/57] time 0.411 (0.477) data 0.279 (0.346) loss_u loss_u 0.8105 (0.8513) acc_u 25.0000 (16.8750) lr 2.4989e-04 eta 0:00:20
epoch [156/200] batch [20/57] time 0.425 (0.476) data 0.294 (0.345) loss_u loss_u 0.8618 (0.8507) acc_u 18.7500 (17.6562) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [25/57] time 0.528 (0.472) data 0.397 (0.341) loss_u loss_u 0.8105 (0.8577) acc_u 21.8750 (16.8750) lr 2.4989e-04 eta 0:00:15
epoch [156/200] batch [30/57] time 0.464 (0.476) data 0.332 (0.344) loss_u loss_u 0.7930 (0.8434) acc_u 21.8750 (18.4375) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [35/57] time 0.460 (0.474) data 0.330 (0.342) loss_u loss_u 0.8999 (0.8426) acc_u 12.5000 (18.4821) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [40/57] time 0.884 (0.479) data 0.753 (0.348) loss_u loss_u 0.8589 (0.8438) acc_u 21.8750 (18.7500) lr 2.4989e-04 eta 0:00:08
epoch [156/200] batch [45/57] time 0.557 (0.478) data 0.425 (0.347) loss_u loss_u 0.8076 (0.8428) acc_u 21.8750 (18.8194) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [50/57] time 0.582 (0.481) data 0.451 (0.350) loss_u loss_u 0.7900 (0.8431) acc_u 25.0000 (18.7500) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [55/57] time 0.455 (0.480) data 0.324 (0.348) loss_u loss_u 0.9058 (0.8468) acc_u 9.3750 (18.1250) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1425
confident_label rate tensor(0.4082, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1280
clean true:1196
clean false:84
clean_rate:0.934375
noisy true:515
noisy false:1341
after delete: len(clean_dataset) 1280
after delete: len(noisy_dataset) 1856
epoch [157/200] batch [5/40] time 0.689 (0.562) data 0.558 (0.430) loss_x loss_x 1.2246 (1.0660) acc_x 62.5000 (71.2500) lr 2.3959e-04 eta 0:00:19
epoch [157/200] batch [10/40] time 0.378 (0.530) data 0.247 (0.399) loss_x loss_x 1.2881 (1.1076) acc_x 65.6250 (69.3750) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [15/40] time 0.705 (0.517) data 0.574 (0.386) loss_x loss_x 1.0566 (1.0363) acc_x 71.8750 (72.0833) lr 2.3959e-04 eta 0:00:12
epoch [157/200] batch [20/40] time 0.327 (0.493) data 0.197 (0.362) loss_x loss_x 0.8516 (1.0657) acc_x 84.3750 (72.3438) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [25/40] time 0.450 (0.489) data 0.320 (0.358) loss_x loss_x 0.9399 (1.0538) acc_x 75.0000 (72.6250) lr 2.3959e-04 eta 0:00:07
epoch [157/200] batch [30/40] time 0.401 (0.489) data 0.271 (0.358) loss_x loss_x 1.0713 (1.0809) acc_x 81.2500 (72.0833) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [35/40] time 0.422 (0.480) data 0.291 (0.349) loss_x loss_x 1.1719 (1.0581) acc_x 71.8750 (72.8571) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [40/40] time 0.414 (0.476) data 0.284 (0.345) loss_x loss_x 1.3477 (1.0575) acc_x 59.3750 (73.3594) lr 2.3959e-04 eta 0:00:00
epoch [157/200] batch [5/58] time 0.370 (0.480) data 0.238 (0.350) loss_u loss_u 0.8623 (0.8319) acc_u 15.6250 (20.6250) lr 2.3959e-04 eta 0:00:25
epoch [157/200] batch [10/58] time 0.453 (0.477) data 0.322 (0.346) loss_u loss_u 0.8647 (0.8307) acc_u 15.6250 (21.5625) lr 2.3959e-04 eta 0:00:22
epoch [157/200] batch [15/58] time 0.584 (0.479) data 0.452 (0.348) loss_u loss_u 0.8901 (0.8338) acc_u 15.6250 (21.0417) lr 2.3959e-04 eta 0:00:20
epoch [157/200] batch [20/58] time 0.336 (0.474) data 0.204 (0.343) loss_u loss_u 0.8286 (0.8382) acc_u 25.0000 (21.0938) lr 2.3959e-04 eta 0:00:18
epoch [157/200] batch [25/58] time 0.502 (0.474) data 0.371 (0.343) loss_u loss_u 0.8374 (0.8418) acc_u 18.7500 (20.8750) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [30/58] time 0.339 (0.468) data 0.207 (0.337) loss_u loss_u 0.8140 (0.8387) acc_u 25.0000 (21.1458) lr 2.3959e-04 eta 0:00:13
epoch [157/200] batch [35/58] time 0.644 (0.471) data 0.508 (0.340) loss_u loss_u 0.7632 (0.8392) acc_u 34.3750 (21.0714) lr 2.3959e-04 eta 0:00:10
epoch [157/200] batch [40/58] time 0.460 (0.470) data 0.329 (0.338) loss_u loss_u 0.7910 (0.8377) acc_u 21.8750 (21.2500) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [45/58] time 0.417 (0.468) data 0.286 (0.337) loss_u loss_u 0.9316 (0.8415) acc_u 6.2500 (20.4861) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [50/58] time 0.390 (0.468) data 0.260 (0.337) loss_u loss_u 0.7817 (0.8419) acc_u 21.8750 (20.1250) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [55/58] time 0.441 (0.465) data 0.310 (0.334) loss_u loss_u 0.9033 (0.8445) acc_u 9.3750 (19.5455) lr 2.3959e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1414
confident_label rate tensor(0.4021, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1261
clean true:1171
clean false:90
clean_rate:0.9286280729579699
noisy true:551
noisy false:1324
after delete: len(clean_dataset) 1261
after delete: len(noisy_dataset) 1875
epoch [158/200] batch [5/39] time 0.480 (0.478) data 0.349 (0.347) loss_x loss_x 1.3145 (1.0134) acc_x 75.0000 (76.8750) lr 2.2949e-04 eta 0:00:16
epoch [158/200] batch [10/39] time 0.430 (0.474) data 0.299 (0.344) loss_x loss_x 1.2715 (0.9799) acc_x 62.5000 (77.5000) lr 2.2949e-04 eta 0:00:13
epoch [158/200] batch [15/39] time 0.371 (0.476) data 0.240 (0.346) loss_x loss_x 1.3008 (0.9668) acc_x 71.8750 (76.8750) lr 2.2949e-04 eta 0:00:11
epoch [158/200] batch [20/39] time 0.434 (0.469) data 0.304 (0.338) loss_x loss_x 0.9951 (0.9625) acc_x 71.8750 (75.3125) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [25/39] time 0.647 (0.484) data 0.517 (0.353) loss_x loss_x 1.2285 (1.0162) acc_x 75.0000 (74.6250) lr 2.2949e-04 eta 0:00:06
epoch [158/200] batch [30/39] time 0.437 (0.480) data 0.305 (0.349) loss_x loss_x 1.2236 (1.0332) acc_x 71.8750 (74.5833) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [35/39] time 0.405 (0.477) data 0.275 (0.346) loss_x loss_x 1.0293 (1.0415) acc_x 78.1250 (74.9107) lr 2.2949e-04 eta 0:00:01
epoch [158/200] batch [5/58] time 0.400 (0.471) data 0.269 (0.341) loss_u loss_u 0.8354 (0.8269) acc_u 21.8750 (20.0000) lr 2.2949e-04 eta 0:00:24
epoch [158/200] batch [10/58] time 0.371 (0.473) data 0.239 (0.342) loss_u loss_u 0.8770 (0.8400) acc_u 9.3750 (18.4375) lr 2.2949e-04 eta 0:00:22
epoch [158/200] batch [15/58] time 0.464 (0.474) data 0.333 (0.343) loss_u loss_u 0.8359 (0.8325) acc_u 18.7500 (20.2083) lr 2.2949e-04 eta 0:00:20
epoch [158/200] batch [20/58] time 0.390 (0.471) data 0.260 (0.340) loss_u loss_u 0.8506 (0.8335) acc_u 21.8750 (20.6250) lr 2.2949e-04 eta 0:00:17
epoch [158/200] batch [25/58] time 0.557 (0.469) data 0.426 (0.338) loss_u loss_u 0.8379 (0.8356) acc_u 25.0000 (21.0000) lr 2.2949e-04 eta 0:00:15
epoch [158/200] batch [30/58] time 0.380 (0.467) data 0.248 (0.336) loss_u loss_u 0.8945 (0.8334) acc_u 15.6250 (21.5625) lr 2.2949e-04 eta 0:00:13
epoch [158/200] batch [35/58] time 0.565 (0.467) data 0.435 (0.336) loss_u loss_u 0.8457 (0.8367) acc_u 12.5000 (20.7143) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [40/58] time 0.474 (0.466) data 0.343 (0.335) loss_u loss_u 0.8965 (0.8388) acc_u 9.3750 (20.0781) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [45/58] time 0.350 (0.461) data 0.219 (0.330) loss_u loss_u 0.9087 (0.8413) acc_u 12.5000 (19.8611) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [50/58] time 0.473 (0.459) data 0.341 (0.328) loss_u loss_u 0.8755 (0.8397) acc_u 15.6250 (20.0000) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [55/58] time 0.377 (0.458) data 0.247 (0.327) loss_u loss_u 0.9043 (0.8428) acc_u 12.5000 (19.4318) lr 2.2949e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1427
confident_label rate tensor(0.4043, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1268
clean true:1176
clean false:92
clean_rate:0.9274447949526814
noisy true:533
noisy false:1335
after delete: len(clean_dataset) 1268
after delete: len(noisy_dataset) 1868
epoch [159/200] batch [5/39] time 0.439 (0.554) data 0.308 (0.422) loss_x loss_x 1.0049 (1.0432) acc_x 78.1250 (71.2500) lr 2.1957e-04 eta 0:00:18
epoch [159/200] batch [10/39] time 0.491 (0.500) data 0.360 (0.368) loss_x loss_x 1.4082 (1.0819) acc_x 65.6250 (71.5625) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [15/39] time 0.876 (0.516) data 0.745 (0.385) loss_x loss_x 0.9785 (1.0086) acc_x 81.2500 (73.7500) lr 2.1957e-04 eta 0:00:12
epoch [159/200] batch [20/39] time 0.487 (0.511) data 0.356 (0.380) loss_x loss_x 1.1592 (0.9845) acc_x 68.7500 (74.8438) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [25/39] time 0.580 (0.512) data 0.449 (0.381) loss_x loss_x 1.1172 (1.0642) acc_x 71.8750 (72.8750) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [30/39] time 0.368 (0.503) data 0.238 (0.372) loss_x loss_x 1.0752 (1.0821) acc_x 71.8750 (73.1250) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [35/39] time 0.532 (0.498) data 0.402 (0.367) loss_x loss_x 1.3232 (1.1035) acc_x 59.3750 (72.5893) lr 2.1957e-04 eta 0:00:01
epoch [159/200] batch [5/58] time 0.523 (0.486) data 0.393 (0.355) loss_u loss_u 0.8047 (0.8584) acc_u 21.8750 (15.6250) lr 2.1957e-04 eta 0:00:25
epoch [159/200] batch [10/58] time 0.478 (0.476) data 0.346 (0.346) loss_u loss_u 0.8452 (0.8551) acc_u 21.8750 (16.2500) lr 2.1957e-04 eta 0:00:22
epoch [159/200] batch [15/58] time 0.500 (0.475) data 0.368 (0.344) loss_u loss_u 0.9229 (0.8491) acc_u 6.2500 (16.4583) lr 2.1957e-04 eta 0:00:20
epoch [159/200] batch [20/58] time 0.422 (0.477) data 0.290 (0.346) loss_u loss_u 0.9307 (0.8528) acc_u 12.5000 (16.7188) lr 2.1957e-04 eta 0:00:18
epoch [159/200] batch [25/58] time 0.522 (0.479) data 0.391 (0.347) loss_u loss_u 0.8608 (0.8485) acc_u 15.6250 (17.7500) lr 2.1957e-04 eta 0:00:15
epoch [159/200] batch [30/58] time 0.573 (0.478) data 0.441 (0.347) loss_u loss_u 0.8184 (0.8480) acc_u 25.0000 (17.9167) lr 2.1957e-04 eta 0:00:13
epoch [159/200] batch [35/58] time 0.392 (0.474) data 0.262 (0.343) loss_u loss_u 0.8013 (0.8483) acc_u 28.1250 (18.3929) lr 2.1957e-04 eta 0:00:10
epoch [159/200] batch [40/58] time 0.391 (0.471) data 0.260 (0.340) loss_u loss_u 0.8257 (0.8466) acc_u 21.8750 (18.5938) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [45/58] time 0.473 (0.472) data 0.342 (0.341) loss_u loss_u 0.8853 (0.8477) acc_u 15.6250 (18.5417) lr 2.1957e-04 eta 0:00:06
epoch [159/200] batch [50/58] time 0.436 (0.472) data 0.305 (0.341) loss_u loss_u 0.8335 (0.8479) acc_u 15.6250 (18.3125) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [55/58] time 0.409 (0.470) data 0.277 (0.339) loss_u loss_u 0.7476 (0.8473) acc_u 34.3750 (18.3523) lr 2.1957e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1419
confident_label rate tensor(0.4043, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1268
clean true:1183
clean false:85
clean_rate:0.9329652996845426
noisy true:534
noisy false:1334
after delete: len(clean_dataset) 1268
after delete: len(noisy_dataset) 1868
epoch [160/200] batch [5/39] time 0.426 (0.513) data 0.295 (0.382) loss_x loss_x 0.8120 (1.0810) acc_x 78.1250 (72.5000) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [10/39] time 0.486 (0.503) data 0.355 (0.372) loss_x loss_x 0.9971 (1.1430) acc_x 78.1250 (71.5625) lr 2.0984e-04 eta 0:00:14
epoch [160/200] batch [15/39] time 0.482 (0.501) data 0.352 (0.370) loss_x loss_x 1.1230 (1.0897) acc_x 78.1250 (73.3333) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [20/39] time 0.407 (0.481) data 0.277 (0.350) loss_x loss_x 0.9712 (1.0446) acc_x 75.0000 (74.8438) lr 2.0984e-04 eta 0:00:09
epoch [160/200] batch [25/39] time 0.421 (0.483) data 0.291 (0.352) loss_x loss_x 0.7812 (1.0544) acc_x 84.3750 (74.7500) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [30/39] time 0.451 (0.485) data 0.319 (0.354) loss_x loss_x 0.9321 (1.0338) acc_x 81.2500 (75.2083) lr 2.0984e-04 eta 0:00:04
epoch [160/200] batch [35/39] time 0.482 (0.489) data 0.351 (0.358) loss_x loss_x 0.8833 (1.0296) acc_x 81.2500 (74.9107) lr 2.0984e-04 eta 0:00:01
epoch [160/200] batch [5/58] time 0.373 (0.477) data 0.243 (0.346) loss_u loss_u 0.7783 (0.8325) acc_u 28.1250 (20.0000) lr 2.0984e-04 eta 0:00:25
epoch [160/200] batch [10/58] time 0.377 (0.475) data 0.247 (0.344) loss_u loss_u 0.8721 (0.8399) acc_u 15.6250 (19.0625) lr 2.0984e-04 eta 0:00:22
epoch [160/200] batch [15/58] time 0.580 (0.476) data 0.449 (0.345) loss_u loss_u 0.7959 (0.8275) acc_u 25.0000 (20.4167) lr 2.0984e-04 eta 0:00:20
epoch [160/200] batch [20/58] time 0.374 (0.472) data 0.242 (0.341) loss_u loss_u 0.8560 (0.8301) acc_u 18.7500 (20.0000) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [25/58] time 0.385 (0.470) data 0.254 (0.339) loss_u loss_u 0.7549 (0.8264) acc_u 34.3750 (21.2500) lr 2.0984e-04 eta 0:00:15
epoch [160/200] batch [30/58] time 0.395 (0.468) data 0.263 (0.337) loss_u loss_u 0.9204 (0.8318) acc_u 3.1250 (20.2083) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [35/58] time 0.353 (0.466) data 0.221 (0.335) loss_u loss_u 0.8521 (0.8314) acc_u 12.5000 (20.4464) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [40/58] time 0.471 (0.468) data 0.339 (0.337) loss_u loss_u 0.9668 (0.8377) acc_u 3.1250 (19.6094) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [45/58] time 0.562 (0.473) data 0.431 (0.342) loss_u loss_u 0.7856 (0.8354) acc_u 25.0000 (19.8611) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [50/58] time 0.492 (0.476) data 0.362 (0.345) loss_u loss_u 0.8691 (0.8390) acc_u 15.6250 (19.3750) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [55/58] time 0.503 (0.476) data 0.372 (0.345) loss_u loss_u 0.8965 (0.8421) acc_u 15.6250 (19.2045) lr 2.0984e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.4031, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1264
clean true:1174
clean false:90
clean_rate:0.9287974683544303
noisy true:521
noisy false:1351
after delete: len(clean_dataset) 1264
after delete: len(noisy_dataset) 1872
epoch [161/200] batch [5/39] time 0.356 (0.437) data 0.225 (0.307) loss_x loss_x 0.8818 (1.1383) acc_x 78.1250 (71.8750) lr 2.0032e-04 eta 0:00:14
epoch [161/200] batch [10/39] time 0.448 (0.461) data 0.318 (0.330) loss_x loss_x 0.9893 (1.0528) acc_x 78.1250 (74.3750) lr 2.0032e-04 eta 0:00:13
epoch [161/200] batch [15/39] time 0.387 (0.447) data 0.257 (0.316) loss_x loss_x 1.4512 (1.1096) acc_x 59.3750 (72.9167) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [20/39] time 0.626 (0.450) data 0.495 (0.320) loss_x loss_x 0.7344 (1.0508) acc_x 75.0000 (73.9062) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [25/39] time 0.398 (0.451) data 0.267 (0.321) loss_x loss_x 0.9390 (1.1136) acc_x 78.1250 (71.8750) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [30/39] time 0.518 (0.452) data 0.388 (0.322) loss_x loss_x 0.9634 (1.0929) acc_x 71.8750 (72.2917) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [35/39] time 0.472 (0.450) data 0.342 (0.320) loss_x loss_x 1.1348 (1.0816) acc_x 62.5000 (72.1429) lr 2.0032e-04 eta 0:00:01
epoch [161/200] batch [5/58] time 0.397 (0.454) data 0.266 (0.323) loss_u loss_u 0.8047 (0.8415) acc_u 18.7500 (17.5000) lr 2.0032e-04 eta 0:00:24
epoch [161/200] batch [10/58] time 0.344 (0.450) data 0.213 (0.319) loss_u loss_u 0.9170 (0.8456) acc_u 6.2500 (17.1875) lr 2.0032e-04 eta 0:00:21
epoch [161/200] batch [15/58] time 0.409 (0.452) data 0.278 (0.321) loss_u loss_u 0.8389 (0.8499) acc_u 18.7500 (16.8750) lr 2.0032e-04 eta 0:00:19
epoch [161/200] batch [20/58] time 0.529 (0.454) data 0.397 (0.323) loss_u loss_u 0.8750 (0.8515) acc_u 18.7500 (17.1875) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [25/58] time 0.456 (0.455) data 0.324 (0.324) loss_u loss_u 0.8594 (0.8445) acc_u 15.6250 (18.3750) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [30/58] time 0.532 (0.458) data 0.400 (0.327) loss_u loss_u 0.8345 (0.8436) acc_u 15.6250 (18.3333) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [35/58] time 0.524 (0.460) data 0.391 (0.329) loss_u loss_u 0.7520 (0.8423) acc_u 37.5000 (19.0179) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [40/58] time 0.618 (0.462) data 0.487 (0.331) loss_u loss_u 0.7544 (0.8392) acc_u 31.2500 (19.2188) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [45/58] time 0.442 (0.458) data 0.309 (0.327) loss_u loss_u 0.8691 (0.8420) acc_u 15.6250 (18.8889) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [50/58] time 0.447 (0.456) data 0.316 (0.325) loss_u loss_u 0.7773 (0.8399) acc_u 28.1250 (19.3125) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [55/58] time 0.387 (0.455) data 0.255 (0.324) loss_u loss_u 0.8438 (0.8416) acc_u 21.8750 (19.2045) lr 2.0032e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1418
confident_label rate tensor(0.4015, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1259
clean true:1182
clean false:77
clean_rate:0.9388403494837172
noisy true:536
noisy false:1341
after delete: len(clean_dataset) 1259
after delete: len(noisy_dataset) 1877
epoch [162/200] batch [5/39] time 0.563 (0.457) data 0.432 (0.327) loss_x loss_x 0.7876 (0.9924) acc_x 81.2500 (78.1250) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [10/39] time 0.425 (0.498) data 0.294 (0.367) loss_x loss_x 1.6152 (1.1267) acc_x 65.6250 (71.5625) lr 1.9098e-04 eta 0:00:14
epoch [162/200] batch [15/39] time 0.448 (0.481) data 0.317 (0.351) loss_x loss_x 0.9653 (1.0492) acc_x 75.0000 (73.3333) lr 1.9098e-04 eta 0:00:11
epoch [162/200] batch [20/39] time 0.523 (0.475) data 0.393 (0.344) loss_x loss_x 0.8755 (1.0511) acc_x 68.7500 (72.1875) lr 1.9098e-04 eta 0:00:09
epoch [162/200] batch [25/39] time 0.440 (0.479) data 0.310 (0.348) loss_x loss_x 1.1465 (1.0519) acc_x 59.3750 (72.3750) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [30/39] time 0.479 (0.475) data 0.349 (0.344) loss_x loss_x 0.6392 (1.0280) acc_x 81.2500 (72.8125) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [35/39] time 0.461 (0.479) data 0.331 (0.349) loss_x loss_x 0.7402 (1.0615) acc_x 81.2500 (71.8750) lr 1.9098e-04 eta 0:00:01
epoch [162/200] batch [5/58] time 0.450 (0.475) data 0.318 (0.344) loss_u loss_u 0.8701 (0.8413) acc_u 18.7500 (18.1250) lr 1.9098e-04 eta 0:00:25
epoch [162/200] batch [10/58] time 0.371 (0.472) data 0.239 (0.341) loss_u loss_u 0.7856 (0.8357) acc_u 25.0000 (20.0000) lr 1.9098e-04 eta 0:00:22
epoch [162/200] batch [15/58] time 0.487 (0.473) data 0.356 (0.342) loss_u loss_u 0.8550 (0.8408) acc_u 15.6250 (19.5833) lr 1.9098e-04 eta 0:00:20
epoch [162/200] batch [20/58] time 0.415 (0.472) data 0.284 (0.341) loss_u loss_u 0.7202 (0.8427) acc_u 34.3750 (19.5312) lr 1.9098e-04 eta 0:00:17
epoch [162/200] batch [25/58] time 0.404 (0.469) data 0.272 (0.338) loss_u loss_u 0.8813 (0.8500) acc_u 12.5000 (18.5000) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [30/58] time 0.411 (0.469) data 0.280 (0.338) loss_u loss_u 0.8013 (0.8493) acc_u 25.0000 (18.6458) lr 1.9098e-04 eta 0:00:13
epoch [162/200] batch [35/58] time 0.624 (0.479) data 0.493 (0.348) loss_u loss_u 0.8901 (0.8518) acc_u 15.6250 (18.3929) lr 1.9098e-04 eta 0:00:11
epoch [162/200] batch [40/58] time 0.374 (0.476) data 0.242 (0.345) loss_u loss_u 0.8975 (0.8505) acc_u 9.3750 (18.5938) lr 1.9098e-04 eta 0:00:08
epoch [162/200] batch [45/58] time 0.381 (0.475) data 0.249 (0.344) loss_u loss_u 0.8003 (0.8459) acc_u 25.0000 (19.1667) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [50/58] time 0.380 (0.472) data 0.248 (0.340) loss_u loss_u 0.8281 (0.8444) acc_u 25.0000 (19.5000) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [55/58] time 0.398 (0.468) data 0.266 (0.336) loss_u loss_u 0.8325 (0.8448) acc_u 21.8750 (19.4886) lr 1.9098e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1399
confident_label rate tensor(0.4059, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1273
clean true:1193
clean false:80
clean_rate:0.9371563236449332
noisy true:544
noisy false:1319
after delete: len(clean_dataset) 1273
after delete: len(noisy_dataset) 1863
epoch [163/200] batch [5/39] time 0.570 (0.532) data 0.439 (0.401) loss_x loss_x 0.6685 (0.9729) acc_x 84.3750 (75.0000) lr 1.8185e-04 eta 0:00:18
epoch [163/200] batch [10/39] time 0.494 (0.521) data 0.363 (0.390) loss_x loss_x 0.9761 (1.0040) acc_x 81.2500 (77.5000) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [15/39] time 0.445 (0.509) data 0.315 (0.379) loss_x loss_x 1.1328 (1.0752) acc_x 71.8750 (75.8333) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [20/39] time 0.508 (0.508) data 0.377 (0.377) loss_x loss_x 0.7505 (1.0567) acc_x 84.3750 (75.6250) lr 1.8185e-04 eta 0:00:09
epoch [163/200] batch [25/39] time 0.474 (0.505) data 0.343 (0.374) loss_x loss_x 1.0977 (1.0628) acc_x 71.8750 (74.6250) lr 1.8185e-04 eta 0:00:07
epoch [163/200] batch [30/39] time 0.444 (0.499) data 0.313 (0.368) loss_x loss_x 1.2676 (1.0612) acc_x 78.1250 (74.4792) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [35/39] time 0.559 (0.495) data 0.428 (0.364) loss_x loss_x 1.1494 (1.0523) acc_x 65.6250 (74.3750) lr 1.8185e-04 eta 0:00:01
epoch [163/200] batch [5/58] time 0.461 (0.487) data 0.331 (0.356) loss_u loss_u 0.9048 (0.8547) acc_u 12.5000 (16.8750) lr 1.8185e-04 eta 0:00:25
epoch [163/200] batch [10/58] time 0.482 (0.484) data 0.352 (0.353) loss_u loss_u 0.7549 (0.8442) acc_u 31.2500 (18.7500) lr 1.8185e-04 eta 0:00:23
epoch [163/200] batch [15/58] time 0.354 (0.480) data 0.223 (0.349) loss_u loss_u 0.8086 (0.8388) acc_u 25.0000 (19.5833) lr 1.8185e-04 eta 0:00:20
epoch [163/200] batch [20/58] time 0.526 (0.481) data 0.394 (0.350) loss_u loss_u 0.8257 (0.8477) acc_u 21.8750 (18.5938) lr 1.8185e-04 eta 0:00:18
epoch [163/200] batch [25/58] time 0.805 (0.483) data 0.673 (0.352) loss_u loss_u 0.8311 (0.8447) acc_u 25.0000 (19.1250) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [30/58] time 0.570 (0.479) data 0.438 (0.348) loss_u loss_u 0.9321 (0.8403) acc_u 6.2500 (19.7917) lr 1.8185e-04 eta 0:00:13
epoch [163/200] batch [35/58] time 0.381 (0.476) data 0.250 (0.345) loss_u loss_u 0.8555 (0.8424) acc_u 15.6250 (19.4643) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [40/58] time 0.415 (0.473) data 0.284 (0.341) loss_u loss_u 0.8218 (0.8437) acc_u 28.1250 (19.5312) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [45/58] time 0.365 (0.472) data 0.234 (0.341) loss_u loss_u 0.8975 (0.8436) acc_u 9.3750 (19.5833) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [50/58] time 0.373 (0.470) data 0.243 (0.338) loss_u loss_u 0.8999 (0.8461) acc_u 12.5000 (19.4375) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [55/58] time 0.505 (0.470) data 0.373 (0.339) loss_u loss_u 0.8242 (0.8460) acc_u 21.8750 (19.4886) lr 1.8185e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1419
confident_label rate tensor(0.4098, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1285
clean true:1194
clean false:91
clean_rate:0.9291828793774319
noisy true:523
noisy false:1328
after delete: len(clean_dataset) 1285
after delete: len(noisy_dataset) 1851
epoch [164/200] batch [5/40] time 0.561 (0.515) data 0.431 (0.384) loss_x loss_x 1.0732 (0.8460) acc_x 71.8750 (76.2500) lr 1.7292e-04 eta 0:00:18
epoch [164/200] batch [10/40] time 0.517 (0.494) data 0.386 (0.363) loss_x loss_x 1.3145 (1.0085) acc_x 68.7500 (74.3750) lr 1.7292e-04 eta 0:00:14
epoch [164/200] batch [15/40] time 0.444 (0.468) data 0.314 (0.338) loss_x loss_x 0.9199 (1.0599) acc_x 81.2500 (75.0000) lr 1.7292e-04 eta 0:00:11
epoch [164/200] batch [20/40] time 0.457 (0.459) data 0.326 (0.328) loss_x loss_x 1.3184 (1.0773) acc_x 65.6250 (74.5312) lr 1.7292e-04 eta 0:00:09
epoch [164/200] batch [25/40] time 0.385 (0.459) data 0.254 (0.328) loss_x loss_x 0.9316 (1.0671) acc_x 75.0000 (74.7500) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [30/40] time 0.453 (0.459) data 0.322 (0.328) loss_x loss_x 1.2637 (1.0687) acc_x 65.6250 (74.5833) lr 1.7292e-04 eta 0:00:04
epoch [164/200] batch [35/40] time 0.695 (0.472) data 0.565 (0.342) loss_x loss_x 1.2793 (1.0427) acc_x 68.7500 (75.6250) lr 1.7292e-04 eta 0:00:02
epoch [164/200] batch [40/40] time 0.739 (0.489) data 0.608 (0.358) loss_x loss_x 0.8774 (1.0571) acc_x 87.5000 (75.4688) lr 1.7292e-04 eta 0:00:00
epoch [164/200] batch [5/57] time 0.449 (0.482) data 0.316 (0.351) loss_u loss_u 0.8613 (0.8264) acc_u 12.5000 (22.5000) lr 1.7292e-04 eta 0:00:25
epoch [164/200] batch [10/57] time 0.390 (0.484) data 0.258 (0.353) loss_u loss_u 0.7456 (0.8412) acc_u 28.1250 (20.0000) lr 1.7292e-04 eta 0:00:22
epoch [164/200] batch [15/57] time 0.672 (0.486) data 0.540 (0.355) loss_u loss_u 0.8511 (0.8539) acc_u 18.7500 (18.3333) lr 1.7292e-04 eta 0:00:20
epoch [164/200] batch [20/57] time 0.523 (0.486) data 0.391 (0.354) loss_u loss_u 0.8325 (0.8475) acc_u 21.8750 (19.3750) lr 1.7292e-04 eta 0:00:17
epoch [164/200] batch [25/57] time 0.375 (0.480) data 0.243 (0.348) loss_u loss_u 0.8149 (0.8448) acc_u 21.8750 (19.2500) lr 1.7292e-04 eta 0:00:15
epoch [164/200] batch [30/57] time 0.406 (0.475) data 0.275 (0.343) loss_u loss_u 0.8428 (0.8450) acc_u 15.6250 (19.0625) lr 1.7292e-04 eta 0:00:12
epoch [164/200] batch [35/57] time 0.445 (0.474) data 0.313 (0.342) loss_u loss_u 0.8472 (0.8454) acc_u 18.7500 (18.9286) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [40/57] time 0.525 (0.473) data 0.394 (0.342) loss_u loss_u 0.8101 (0.8456) acc_u 21.8750 (18.9062) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [45/57] time 0.548 (0.475) data 0.417 (0.344) loss_u loss_u 0.8604 (0.8464) acc_u 15.6250 (18.7500) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [50/57] time 0.440 (0.472) data 0.308 (0.340) loss_u loss_u 0.8354 (0.8453) acc_u 18.7500 (18.8750) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [55/57] time 0.356 (0.470) data 0.225 (0.339) loss_u loss_u 0.8291 (0.8469) acc_u 15.6250 (18.5795) lr 1.7292e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1432
confident_label rate tensor(0.4056, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1272
clean true:1191
clean false:81
clean_rate:0.9363207547169812
noisy true:513
noisy false:1351
after delete: len(clean_dataset) 1272
after delete: len(noisy_dataset) 1864
epoch [165/200] batch [5/39] time 0.657 (0.534) data 0.527 (0.403) loss_x loss_x 0.9688 (0.9713) acc_x 78.1250 (76.8750) lr 1.6419e-04 eta 0:00:18
epoch [165/200] batch [10/39] time 0.421 (0.511) data 0.291 (0.380) loss_x loss_x 1.1943 (1.0132) acc_x 65.6250 (74.0625) lr 1.6419e-04 eta 0:00:14
epoch [165/200] batch [15/39] time 0.507 (0.498) data 0.377 (0.367) loss_x loss_x 0.8281 (0.9845) acc_x 81.2500 (75.0000) lr 1.6419e-04 eta 0:00:11
epoch [165/200] batch [20/39] time 0.465 (0.496) data 0.334 (0.365) loss_x loss_x 1.5059 (1.0664) acc_x 65.6250 (73.7500) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [25/39] time 0.418 (0.488) data 0.287 (0.358) loss_x loss_x 1.2852 (1.0600) acc_x 68.7500 (73.7500) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [30/39] time 0.429 (0.487) data 0.299 (0.356) loss_x loss_x 1.2715 (1.0687) acc_x 62.5000 (72.8125) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [35/39] time 0.432 (0.482) data 0.300 (0.351) loss_x loss_x 1.5322 (1.0660) acc_x 71.8750 (73.3929) lr 1.6419e-04 eta 0:00:01
epoch [165/200] batch [5/58] time 0.527 (0.486) data 0.396 (0.355) loss_u loss_u 0.9053 (0.8479) acc_u 12.5000 (19.3750) lr 1.6419e-04 eta 0:00:25
epoch [165/200] batch [10/58] time 0.404 (0.481) data 0.273 (0.350) loss_u loss_u 0.8096 (0.8409) acc_u 28.1250 (19.0625) lr 1.6419e-04 eta 0:00:23
epoch [165/200] batch [15/58] time 0.495 (0.480) data 0.363 (0.349) loss_u loss_u 0.8755 (0.8533) acc_u 15.6250 (17.2917) lr 1.6419e-04 eta 0:00:20
epoch [165/200] batch [20/58] time 0.481 (0.477) data 0.349 (0.346) loss_u loss_u 0.8447 (0.8473) acc_u 18.7500 (17.9688) lr 1.6419e-04 eta 0:00:18
epoch [165/200] batch [25/58] time 0.325 (0.475) data 0.194 (0.344) loss_u loss_u 0.8550 (0.8523) acc_u 12.5000 (17.7500) lr 1.6419e-04 eta 0:00:15
epoch [165/200] batch [30/58] time 0.359 (0.467) data 0.228 (0.336) loss_u loss_u 0.8521 (0.8532) acc_u 21.8750 (17.7083) lr 1.6419e-04 eta 0:00:13
epoch [165/200] batch [35/58] time 0.495 (0.467) data 0.363 (0.336) loss_u loss_u 0.8398 (0.8556) acc_u 21.8750 (17.2321) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [40/58] time 0.394 (0.464) data 0.263 (0.333) loss_u loss_u 0.9185 (0.8565) acc_u 6.2500 (17.1094) lr 1.6419e-04 eta 0:00:08
epoch [165/200] batch [45/58] time 0.401 (0.459) data 0.271 (0.328) loss_u loss_u 0.8125 (0.8555) acc_u 21.8750 (17.0833) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [50/58] time 0.416 (0.457) data 0.285 (0.326) loss_u loss_u 0.8545 (0.8543) acc_u 12.5000 (17.2500) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [55/58] time 0.420 (0.458) data 0.290 (0.327) loss_u loss_u 0.7910 (0.8496) acc_u 28.1250 (17.8977) lr 1.6419e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1405
confident_label rate tensor(0.4082, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1280
clean true:1194
clean false:86
clean_rate:0.9328125
noisy true:537
noisy false:1319
after delete: len(clean_dataset) 1280
after delete: len(noisy_dataset) 1856
epoch [166/200] batch [5/40] time 0.426 (0.505) data 0.295 (0.374) loss_x loss_x 1.0557 (1.1111) acc_x 81.2500 (73.1250) lr 1.5567e-04 eta 0:00:17
epoch [166/200] batch [10/40] time 0.548 (0.475) data 0.418 (0.344) loss_x loss_x 1.0166 (1.0775) acc_x 71.8750 (73.7500) lr 1.5567e-04 eta 0:00:14
epoch [166/200] batch [15/40] time 0.428 (0.483) data 0.297 (0.352) loss_x loss_x 1.2041 (1.1056) acc_x 65.6250 (71.4583) lr 1.5567e-04 eta 0:00:12
epoch [166/200] batch [20/40] time 0.509 (0.484) data 0.380 (0.353) loss_x loss_x 1.0498 (1.0837) acc_x 75.0000 (72.5000) lr 1.5567e-04 eta 0:00:09
epoch [166/200] batch [25/40] time 0.458 (0.482) data 0.326 (0.351) loss_x loss_x 0.7876 (1.0658) acc_x 78.1250 (73.0000) lr 1.5567e-04 eta 0:00:07
epoch [166/200] batch [30/40] time 0.642 (0.496) data 0.511 (0.365) loss_x loss_x 1.0107 (1.0797) acc_x 81.2500 (72.8125) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [35/40] time 0.431 (0.497) data 0.300 (0.366) loss_x loss_x 1.1123 (1.0921) acc_x 78.1250 (73.5714) lr 1.5567e-04 eta 0:00:02
epoch [166/200] batch [40/40] time 0.377 (0.494) data 0.247 (0.363) loss_x loss_x 1.1982 (1.0988) acc_x 62.5000 (73.2812) lr 1.5567e-04 eta 0:00:00
epoch [166/200] batch [5/58] time 0.454 (0.484) data 0.324 (0.353) loss_u loss_u 0.8501 (0.8560) acc_u 15.6250 (16.2500) lr 1.5567e-04 eta 0:00:25
epoch [166/200] batch [10/58] time 0.376 (0.477) data 0.245 (0.347) loss_u loss_u 0.8438 (0.8411) acc_u 18.7500 (19.3750) lr 1.5567e-04 eta 0:00:22
epoch [166/200] batch [15/58] time 0.457 (0.472) data 0.326 (0.341) loss_u loss_u 0.7959 (0.8292) acc_u 21.8750 (20.2083) lr 1.5567e-04 eta 0:00:20
epoch [166/200] batch [20/58] time 0.352 (0.466) data 0.222 (0.335) loss_u loss_u 0.8950 (0.8365) acc_u 18.7500 (19.6875) lr 1.5567e-04 eta 0:00:17
epoch [166/200] batch [25/58] time 0.513 (0.466) data 0.381 (0.335) loss_u loss_u 0.8281 (0.8353) acc_u 18.7500 (19.7500) lr 1.5567e-04 eta 0:00:15
epoch [166/200] batch [30/58] time 0.398 (0.468) data 0.268 (0.338) loss_u loss_u 0.7944 (0.8316) acc_u 25.0000 (20.0000) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [35/58] time 0.483 (0.468) data 0.351 (0.337) loss_u loss_u 0.8604 (0.8370) acc_u 21.8750 (19.4643) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [40/58] time 0.495 (0.465) data 0.364 (0.334) loss_u loss_u 0.8145 (0.8370) acc_u 21.8750 (19.5312) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [45/58] time 0.681 (0.467) data 0.551 (0.336) loss_u loss_u 0.8921 (0.8397) acc_u 15.6250 (19.2361) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [50/58] time 0.721 (0.467) data 0.590 (0.335) loss_u loss_u 0.8896 (0.8390) acc_u 12.5000 (19.4375) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [55/58] time 0.512 (0.469) data 0.380 (0.338) loss_u loss_u 0.9097 (0.8413) acc_u 9.3750 (19.0341) lr 1.5567e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1431
confident_label rate tensor(0.4050, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1270
clean true:1188
clean false:82
clean_rate:0.9354330708661417
noisy true:517
noisy false:1349
after delete: len(clean_dataset) 1270
after delete: len(noisy_dataset) 1866
epoch [167/200] batch [5/39] time 0.516 (0.505) data 0.385 (0.374) loss_x loss_x 1.0020 (1.1892) acc_x 78.1250 (72.5000) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [10/39] time 0.441 (0.493) data 0.310 (0.362) loss_x loss_x 1.0283 (1.0411) acc_x 75.0000 (75.0000) lr 1.4736e-04 eta 0:00:14
epoch [167/200] batch [15/39] time 0.418 (0.492) data 0.287 (0.361) loss_x loss_x 1.5420 (1.1303) acc_x 68.7500 (73.1250) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [20/39] time 0.504 (0.499) data 0.373 (0.368) loss_x loss_x 1.6602 (1.1311) acc_x 68.7500 (73.2812) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [25/39] time 0.354 (0.488) data 0.223 (0.357) loss_x loss_x 1.0576 (1.0973) acc_x 75.0000 (73.5000) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [30/39] time 0.442 (0.503) data 0.311 (0.372) loss_x loss_x 0.9097 (1.1053) acc_x 75.0000 (73.5417) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [35/39] time 0.530 (0.504) data 0.399 (0.373) loss_x loss_x 1.0078 (1.1114) acc_x 81.2500 (74.1071) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [5/58] time 0.472 (0.490) data 0.340 (0.359) loss_u loss_u 0.8228 (0.8163) acc_u 21.8750 (24.3750) lr 1.4736e-04 eta 0:00:25
epoch [167/200] batch [10/58] time 0.532 (0.488) data 0.400 (0.357) loss_u loss_u 0.8740 (0.8321) acc_u 12.5000 (22.1875) lr 1.4736e-04 eta 0:00:23
epoch [167/200] batch [15/58] time 0.450 (0.491) data 0.318 (0.360) loss_u loss_u 0.8740 (0.8375) acc_u 15.6250 (21.0417) lr 1.4736e-04 eta 0:00:21
epoch [167/200] batch [20/58] time 0.500 (0.490) data 0.369 (0.359) loss_u loss_u 0.8481 (0.8403) acc_u 18.7500 (20.1562) lr 1.4736e-04 eta 0:00:18
epoch [167/200] batch [25/58] time 0.489 (0.487) data 0.357 (0.356) loss_u loss_u 0.8530 (0.8390) acc_u 12.5000 (20.3750) lr 1.4736e-04 eta 0:00:16
epoch [167/200] batch [30/58] time 0.526 (0.490) data 0.315 (0.356) loss_u loss_u 0.7725 (0.8393) acc_u 28.1250 (20.7292) lr 1.4736e-04 eta 0:00:13
epoch [167/200] batch [35/58] time 0.611 (0.495) data 0.396 (0.355) loss_u loss_u 0.8462 (0.8434) acc_u 21.8750 (19.9107) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [40/58] time 0.556 (0.504) data 0.336 (0.360) loss_u loss_u 0.8721 (0.8457) acc_u 21.8750 (19.8438) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [45/58] time 0.647 (0.505) data 0.434 (0.357) loss_u loss_u 0.8677 (0.8442) acc_u 12.5000 (19.7917) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [50/58] time 0.564 (0.505) data 0.345 (0.353) loss_u loss_u 0.8325 (0.8434) acc_u 21.8750 (19.6875) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [55/58] time 0.490 (0.506) data 0.276 (0.351) loss_u loss_u 0.8701 (0.8440) acc_u 12.5000 (19.3182) lr 1.4736e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1423
confident_label rate tensor(0.4091, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1283
clean true:1192
clean false:91
clean_rate:0.9290724863600935
noisy true:521
noisy false:1332
after delete: len(clean_dataset) 1283
after delete: len(noisy_dataset) 1853
epoch [168/200] batch [5/40] time 0.681 (0.561) data 0.462 (0.347) loss_x loss_x 1.1768 (1.2930) acc_x 59.3750 (65.6250) lr 1.3926e-04 eta 0:00:19
epoch [168/200] batch [10/40] time 0.662 (0.545) data 0.452 (0.332) loss_x loss_x 0.7593 (1.1441) acc_x 81.2500 (70.9375) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [15/40] time 0.656 (0.576) data 0.441 (0.366) loss_x loss_x 0.9751 (1.1267) acc_x 78.1250 (71.6667) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [20/40] time 0.483 (0.566) data 0.269 (0.356) loss_x loss_x 1.2148 (1.0859) acc_x 75.0000 (72.0312) lr 1.3926e-04 eta 0:00:11
epoch [168/200] batch [25/40] time 0.647 (0.570) data 0.436 (0.357) loss_x loss_x 1.2520 (1.0656) acc_x 78.1250 (72.6250) lr 1.3926e-04 eta 0:00:08
epoch [168/200] batch [30/40] time 0.511 (0.567) data 0.299 (0.355) loss_x loss_x 1.1240 (1.0842) acc_x 71.8750 (72.8125) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [35/40] time 0.508 (0.561) data 0.294 (0.348) loss_x loss_x 1.3867 (1.1394) acc_x 65.6250 (71.4286) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [40/40] time 0.521 (0.568) data 0.309 (0.355) loss_x loss_x 0.9351 (1.1094) acc_x 75.0000 (71.6406) lr 1.3926e-04 eta 0:00:00
epoch [168/200] batch [5/57] time 0.438 (0.558) data 0.225 (0.345) loss_u loss_u 0.9136 (0.8453) acc_u 12.5000 (20.0000) lr 1.3926e-04 eta 0:00:29
epoch [168/200] batch [10/57] time 0.462 (0.552) data 0.250 (0.341) loss_u loss_u 0.8921 (0.8383) acc_u 12.5000 (20.6250) lr 1.3926e-04 eta 0:00:25
epoch [168/200] batch [15/57] time 0.439 (0.547) data 0.229 (0.335) loss_u loss_u 0.8340 (0.8296) acc_u 18.7500 (21.0417) lr 1.3926e-04 eta 0:00:22
epoch [168/200] batch [20/57] time 0.698 (0.546) data 0.501 (0.334) loss_u loss_u 0.8315 (0.8370) acc_u 21.8750 (20.0000) lr 1.3926e-04 eta 0:00:20
epoch [168/200] batch [25/57] time 0.600 (0.545) data 0.387 (0.334) loss_u loss_u 0.8696 (0.8432) acc_u 15.6250 (19.6250) lr 1.3926e-04 eta 0:00:17
epoch [168/200] batch [30/57] time 0.608 (0.545) data 0.399 (0.335) loss_u loss_u 0.8364 (0.8435) acc_u 15.6250 (19.6875) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [35/57] time 0.514 (0.545) data 0.297 (0.336) loss_u loss_u 0.9272 (0.8459) acc_u 3.1250 (19.1071) lr 1.3926e-04 eta 0:00:11
epoch [168/200] batch [40/57] time 0.461 (0.542) data 0.248 (0.333) loss_u loss_u 0.8813 (0.8479) acc_u 15.6250 (19.0625) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [45/57] time 0.525 (0.546) data 0.315 (0.336) loss_u loss_u 0.8486 (0.8464) acc_u 15.6250 (19.1667) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [50/57] time 0.551 (0.544) data 0.336 (0.334) loss_u loss_u 0.8335 (0.8468) acc_u 15.6250 (18.9375) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [55/57] time 0.492 (0.544) data 0.275 (0.334) loss_u loss_u 0.8511 (0.8470) acc_u 18.7500 (18.8636) lr 1.3926e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1424
confident_label rate tensor(0.4104, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1287
clean true:1190
clean false:97
clean_rate:0.9246309246309247
noisy true:522
noisy false:1327
after delete: len(clean_dataset) 1287
after delete: len(noisy_dataset) 1849
epoch [169/200] batch [5/40] time 0.620 (0.553) data 0.411 (0.343) loss_x loss_x 0.9712 (0.9252) acc_x 78.1250 (75.6250) lr 1.3137e-04 eta 0:00:19
epoch [169/200] batch [10/40] time 0.513 (0.534) data 0.304 (0.321) loss_x loss_x 1.3809 (1.0290) acc_x 62.5000 (73.7500) lr 1.3137e-04 eta 0:00:16
epoch [169/200] batch [15/40] time 0.561 (0.544) data 0.349 (0.331) loss_x loss_x 0.9697 (0.9667) acc_x 62.5000 (74.5833) lr 1.3137e-04 eta 0:00:13
epoch [169/200] batch [20/40] time 0.590 (0.558) data 0.376 (0.345) loss_x loss_x 1.0576 (1.0138) acc_x 71.8750 (74.0625) lr 1.3137e-04 eta 0:00:11
epoch [169/200] batch [25/40] time 0.529 (0.550) data 0.312 (0.341) loss_x loss_x 1.4980 (1.0577) acc_x 62.5000 (73.2500) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [30/40] time 0.679 (0.558) data 0.466 (0.348) loss_x loss_x 0.6636 (1.0614) acc_x 87.5000 (73.5417) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [35/40] time 0.561 (0.559) data 0.343 (0.349) loss_x loss_x 0.6108 (1.0399) acc_x 93.7500 (73.7500) lr 1.3137e-04 eta 0:00:02
epoch [169/200] batch [40/40] time 0.568 (0.557) data 0.355 (0.346) loss_x loss_x 1.0049 (1.0532) acc_x 75.0000 (73.4375) lr 1.3137e-04 eta 0:00:00
epoch [169/200] batch [5/57] time 0.533 (0.555) data 0.319 (0.343) loss_u loss_u 0.8647 (0.8604) acc_u 12.5000 (16.8750) lr 1.3137e-04 eta 0:00:28
epoch [169/200] batch [10/57] time 0.609 (0.556) data 0.395 (0.344) loss_u loss_u 0.7852 (0.8492) acc_u 21.8750 (18.1250) lr 1.3137e-04 eta 0:00:26
epoch [169/200] batch [15/57] time 0.370 (0.555) data 0.239 (0.345) loss_u loss_u 0.7803 (0.8532) acc_u 28.1250 (18.1250) lr 1.3137e-04 eta 0:00:23
epoch [169/200] batch [20/57] time 0.498 (0.558) data 0.286 (0.347) loss_u loss_u 0.8467 (0.8456) acc_u 18.7500 (19.3750) lr 1.3137e-04 eta 0:00:20
epoch [169/200] batch [25/57] time 0.550 (0.556) data 0.339 (0.345) loss_u loss_u 0.8315 (0.8441) acc_u 18.7500 (19.3750) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [30/57] time 0.607 (0.552) data 0.395 (0.341) loss_u loss_u 0.8223 (0.8474) acc_u 21.8750 (18.9583) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [35/57] time 0.594 (0.550) data 0.384 (0.339) loss_u loss_u 0.7158 (0.8462) acc_u 34.3750 (18.9286) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [40/57] time 0.485 (0.548) data 0.271 (0.337) loss_u loss_u 0.8203 (0.8460) acc_u 18.7500 (18.9844) lr 1.3137e-04 eta 0:00:09
epoch [169/200] batch [45/57] time 0.556 (0.547) data 0.340 (0.336) loss_u loss_u 0.8828 (0.8438) acc_u 9.3750 (19.0972) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [50/57] time 0.558 (0.545) data 0.427 (0.335) loss_u loss_u 0.8335 (0.8482) acc_u 25.0000 (18.5625) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [55/57] time 0.719 (0.544) data 0.507 (0.334) loss_u loss_u 0.9023 (0.8493) acc_u 9.3750 (18.4091) lr 1.3137e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1404
confident_label rate tensor(0.4078, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1279
clean true:1199
clean false:80
clean_rate:0.9374511336982018
noisy true:533
noisy false:1324
after delete: len(clean_dataset) 1279
after delete: len(noisy_dataset) 1857
epoch [170/200] batch [5/39] time 0.525 (0.542) data 0.311 (0.329) loss_x loss_x 0.8926 (1.0070) acc_x 87.5000 (76.2500) lr 1.2369e-04 eta 0:00:18
epoch [170/200] batch [10/39] time 0.564 (0.545) data 0.352 (0.332) loss_x loss_x 1.3418 (1.1285) acc_x 62.5000 (72.5000) lr 1.2369e-04 eta 0:00:15
epoch [170/200] batch [15/39] time 0.599 (0.548) data 0.386 (0.335) loss_x loss_x 0.9014 (1.0198) acc_x 78.1250 (76.0417) lr 1.2369e-04 eta 0:00:13
epoch [170/200] batch [20/39] time 0.556 (0.546) data 0.347 (0.337) loss_x loss_x 0.8413 (0.9670) acc_x 78.1250 (76.4062) lr 1.2369e-04 eta 0:00:10
epoch [170/200] batch [25/39] time 0.637 (0.548) data 0.423 (0.341) loss_x loss_x 1.0908 (0.9620) acc_x 78.1250 (77.1250) lr 1.2369e-04 eta 0:00:07
epoch [170/200] batch [30/39] time 0.524 (0.544) data 0.315 (0.337) loss_x loss_x 0.6982 (0.9353) acc_x 78.1250 (77.6042) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [35/39] time 0.534 (0.548) data 0.322 (0.340) loss_x loss_x 0.8516 (0.9643) acc_x 81.2500 (76.4286) lr 1.2369e-04 eta 0:00:02
epoch [170/200] batch [5/58] time 0.569 (0.547) data 0.358 (0.340) loss_u loss_u 0.8545 (0.8421) acc_u 15.6250 (20.0000) lr 1.2369e-04 eta 0:00:28
epoch [170/200] batch [10/58] time 0.488 (0.545) data 0.273 (0.337) loss_u loss_u 0.7144 (0.8368) acc_u 40.6250 (20.6250) lr 1.2369e-04 eta 0:00:26
epoch [170/200] batch [15/58] time 0.850 (0.546) data 0.635 (0.338) loss_u loss_u 0.9292 (0.8424) acc_u 9.3750 (20.4167) lr 1.2369e-04 eta 0:00:23
epoch [170/200] batch [20/58] time 0.457 (0.540) data 0.243 (0.333) loss_u loss_u 0.8345 (0.8521) acc_u 15.6250 (18.2812) lr 1.2369e-04 eta 0:00:20
epoch [170/200] batch [25/58] time 0.493 (0.539) data 0.274 (0.332) loss_u loss_u 0.8291 (0.8588) acc_u 18.7500 (17.3750) lr 1.2369e-04 eta 0:00:17
epoch [170/200] batch [30/58] time 0.503 (0.537) data 0.290 (0.329) loss_u loss_u 0.8643 (0.8587) acc_u 15.6250 (17.2917) lr 1.2369e-04 eta 0:00:15
epoch [170/200] batch [35/58] time 0.502 (0.536) data 0.289 (0.329) loss_u loss_u 0.7515 (0.8553) acc_u 34.3750 (18.0357) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [40/58] time 0.493 (0.536) data 0.283 (0.329) loss_u loss_u 0.7964 (0.8526) acc_u 31.2500 (18.5156) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [45/58] time 0.442 (0.535) data 0.228 (0.327) loss_u loss_u 0.8047 (0.8478) acc_u 25.0000 (19.1667) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [50/58] time 0.664 (0.539) data 0.454 (0.330) loss_u loss_u 0.8560 (0.8474) acc_u 15.6250 (19.3125) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [55/58] time 0.473 (0.541) data 0.261 (0.332) loss_u loss_u 0.8887 (0.8472) acc_u 9.3750 (19.2045) lr 1.2369e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1397
confident_label rate tensor(0.4212, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1321
clean true:1228
clean false:93
clean_rate:0.9295987887963664
noisy true:511
noisy false:1304
after delete: len(clean_dataset) 1321
after delete: len(noisy_dataset) 1815
epoch [171/200] batch [5/41] time 0.492 (0.539) data 0.279 (0.323) loss_x loss_x 0.6753 (0.8814) acc_x 84.3750 (76.2500) lr 1.1623e-04 eta 0:00:19
epoch [171/200] batch [10/41] time 0.506 (0.539) data 0.299 (0.325) loss_x loss_x 0.4954 (1.0061) acc_x 87.5000 (73.7500) lr 1.1623e-04 eta 0:00:16
epoch [171/200] batch [15/41] time 0.500 (0.563) data 0.287 (0.350) loss_x loss_x 1.0098 (1.0060) acc_x 75.0000 (73.9583) lr 1.1623e-04 eta 0:00:14
epoch [171/200] batch [20/41] time 0.547 (0.580) data 0.337 (0.370) loss_x loss_x 1.4453 (1.0289) acc_x 71.8750 (73.7500) lr 1.1623e-04 eta 0:00:12
epoch [171/200] batch [25/41] time 0.517 (0.583) data 0.304 (0.374) loss_x loss_x 0.6855 (1.0416) acc_x 78.1250 (73.1250) lr 1.1623e-04 eta 0:00:09
epoch [171/200] batch [30/41] time 0.501 (0.570) data 0.287 (0.360) loss_x loss_x 1.3271 (1.0591) acc_x 65.6250 (73.1250) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [35/41] time 0.465 (0.560) data 0.256 (0.353) loss_x loss_x 0.5552 (1.0637) acc_x 90.6250 (73.2143) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [40/41] time 0.509 (0.564) data 0.298 (0.356) loss_x loss_x 1.4951 (1.0535) acc_x 78.1250 (73.1250) lr 1.1623e-04 eta 0:00:00
epoch [171/200] batch [5/56] time 0.560 (0.559) data 0.342 (0.351) loss_u loss_u 0.8701 (0.8529) acc_u 18.7500 (18.1250) lr 1.1623e-04 eta 0:00:28
epoch [171/200] batch [10/56] time 0.637 (0.558) data 0.422 (0.350) loss_u loss_u 0.8184 (0.8462) acc_u 21.8750 (18.1250) lr 1.1623e-04 eta 0:00:25
epoch [171/200] batch [15/56] time 0.502 (0.556) data 0.287 (0.348) loss_u loss_u 0.8838 (0.8435) acc_u 21.8750 (18.9583) lr 1.1623e-04 eta 0:00:22
epoch [171/200] batch [20/56] time 0.541 (0.553) data 0.326 (0.344) loss_u loss_u 0.7788 (0.8317) acc_u 28.1250 (20.6250) lr 1.1623e-04 eta 0:00:19
epoch [171/200] batch [25/56] time 0.533 (0.548) data 0.314 (0.340) loss_u loss_u 0.9077 (0.8388) acc_u 9.3750 (19.3750) lr 1.1623e-04 eta 0:00:16
epoch [171/200] batch [30/56] time 0.540 (0.545) data 0.324 (0.336) loss_u loss_u 0.8872 (0.8470) acc_u 12.5000 (18.3333) lr 1.1623e-04 eta 0:00:14
epoch [171/200] batch [35/56] time 0.559 (0.547) data 0.347 (0.338) loss_u loss_u 0.8691 (0.8506) acc_u 21.8750 (17.6786) lr 1.1623e-04 eta 0:00:11
epoch [171/200] batch [40/56] time 0.636 (0.549) data 0.423 (0.339) loss_u loss_u 0.8726 (0.8518) acc_u 18.7500 (17.8125) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [45/56] time 0.629 (0.551) data 0.413 (0.341) loss_u loss_u 0.7393 (0.8497) acc_u 28.1250 (17.9861) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [50/56] time 0.535 (0.550) data 0.403 (0.341) loss_u loss_u 0.8911 (0.8505) acc_u 15.6250 (17.9375) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [55/56] time 0.578 (0.554) data 0.367 (0.345) loss_u loss_u 0.8350 (0.8488) acc_u 18.7500 (18.0114) lr 1.1623e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1410
confident_label rate tensor(0.4091, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1283
clean true:1196
clean false:87
clean_rate:0.9321901792673422
noisy true:530
noisy false:1323
after delete: len(clean_dataset) 1283
after delete: len(noisy_dataset) 1853
epoch [172/200] batch [5/40] time 0.554 (0.522) data 0.344 (0.312) loss_x loss_x 0.8774 (1.0733) acc_x 81.2500 (72.5000) lr 1.0899e-04 eta 0:00:18
epoch [172/200] batch [10/40] time 0.568 (0.544) data 0.357 (0.339) loss_x loss_x 0.5332 (1.0588) acc_x 81.2500 (71.5625) lr 1.0899e-04 eta 0:00:16
epoch [172/200] batch [15/40] time 0.566 (0.559) data 0.435 (0.356) loss_x loss_x 0.9463 (1.0447) acc_x 71.8750 (73.5417) lr 1.0899e-04 eta 0:00:13
epoch [172/200] batch [20/40] time 0.482 (0.572) data 0.271 (0.368) loss_x loss_x 0.8081 (1.0069) acc_x 78.1250 (75.3125) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [25/40] time 0.568 (0.569) data 0.359 (0.363) loss_x loss_x 1.1035 (0.9900) acc_x 71.8750 (75.2500) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [30/40] time 0.639 (0.566) data 0.415 (0.359) loss_x loss_x 0.9429 (0.9810) acc_x 71.8750 (75.0000) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [35/40] time 0.593 (0.565) data 0.377 (0.357) loss_x loss_x 1.5195 (1.0158) acc_x 65.6250 (74.4643) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [40/40] time 0.575 (0.569) data 0.360 (0.362) loss_x loss_x 1.4180 (1.0243) acc_x 68.7500 (74.2188) lr 1.0899e-04 eta 0:00:00
epoch [172/200] batch [5/57] time 0.486 (0.563) data 0.273 (0.355) loss_u loss_u 0.8687 (0.8671) acc_u 12.5000 (16.2500) lr 1.0899e-04 eta 0:00:29
epoch [172/200] batch [10/57] time 0.588 (0.562) data 0.374 (0.354) loss_u loss_u 0.9219 (0.8532) acc_u 9.3750 (17.8125) lr 1.0899e-04 eta 0:00:26
epoch [172/200] batch [15/57] time 0.563 (0.557) data 0.349 (0.348) loss_u loss_u 0.8496 (0.8471) acc_u 18.7500 (18.1250) lr 1.0899e-04 eta 0:00:23
epoch [172/200] batch [20/57] time 0.578 (0.554) data 0.366 (0.346) loss_u loss_u 0.8223 (0.8504) acc_u 18.7500 (17.1875) lr 1.0899e-04 eta 0:00:20
epoch [172/200] batch [25/57] time 0.475 (0.549) data 0.343 (0.342) loss_u loss_u 0.7842 (0.8458) acc_u 31.2500 (18.2500) lr 1.0899e-04 eta 0:00:17
epoch [172/200] batch [30/57] time 0.679 (0.553) data 0.465 (0.345) loss_u loss_u 0.8833 (0.8489) acc_u 12.5000 (18.1250) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [35/57] time 0.606 (0.549) data 0.394 (0.341) loss_u loss_u 0.8618 (0.8497) acc_u 12.5000 (17.8571) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [40/57] time 0.557 (0.552) data 0.343 (0.343) loss_u loss_u 0.8691 (0.8506) acc_u 21.8750 (18.0469) lr 1.0899e-04 eta 0:00:09
epoch [172/200] batch [45/57] time 0.584 (0.548) data 0.372 (0.339) loss_u loss_u 0.8926 (0.8540) acc_u 15.6250 (17.6389) lr 1.0899e-04 eta 0:00:06
epoch [172/200] batch [50/57] time 0.563 (0.549) data 0.349 (0.340) loss_u loss_u 0.8457 (0.8503) acc_u 18.7500 (18.1250) lr 1.0899e-04 eta 0:00:03
epoch [172/200] batch [55/57] time 0.450 (0.547) data 0.305 (0.338) loss_u loss_u 0.7881 (0.8501) acc_u 31.2500 (18.3523) lr 1.0899e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1395
confident_label rate tensor(0.4216, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1322
clean true:1232
clean false:90
clean_rate:0.9319213313161876
noisy true:509
noisy false:1305
after delete: len(clean_dataset) 1322
after delete: len(noisy_dataset) 1814
epoch [173/200] batch [5/41] time 0.629 (0.616) data 0.418 (0.404) loss_x loss_x 1.1436 (1.0194) acc_x 81.2500 (71.8750) lr 1.0197e-04 eta 0:00:22
epoch [173/200] batch [10/41] time 0.507 (0.588) data 0.295 (0.376) loss_x loss_x 1.2803 (1.0983) acc_x 71.8750 (71.8750) lr 1.0197e-04 eta 0:00:18
epoch [173/200] batch [15/41] time 0.636 (0.567) data 0.426 (0.357) loss_x loss_x 1.4541 (1.0836) acc_x 65.6250 (72.0833) lr 1.0197e-04 eta 0:00:14
epoch [173/200] batch [20/41] time 0.527 (0.564) data 0.315 (0.353) loss_x loss_x 1.0557 (1.1248) acc_x 71.8750 (72.0312) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [25/41] time 0.496 (0.569) data 0.284 (0.358) loss_x loss_x 1.2578 (1.1413) acc_x 68.7500 (71.8750) lr 1.0197e-04 eta 0:00:09
epoch [173/200] batch [30/41] time 0.627 (0.575) data 0.416 (0.363) loss_x loss_x 1.3369 (1.1481) acc_x 62.5000 (71.9792) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [35/41] time 0.436 (0.564) data 0.305 (0.355) loss_x loss_x 1.2686 (1.1646) acc_x 68.7500 (71.5179) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [40/41] time 0.599 (0.563) data 0.386 (0.354) loss_x loss_x 0.9038 (1.1274) acc_x 75.0000 (72.1094) lr 1.0197e-04 eta 0:00:00
epoch [173/200] batch [5/56] time 0.525 (0.560) data 0.310 (0.350) loss_u loss_u 0.8203 (0.8613) acc_u 15.6250 (12.5000) lr 1.0197e-04 eta 0:00:28
epoch [173/200] batch [10/56] time 0.608 (0.564) data 0.395 (0.354) loss_u loss_u 0.9067 (0.8680) acc_u 6.2500 (13.1250) lr 1.0197e-04 eta 0:00:25
epoch [173/200] batch [15/56] time 0.473 (0.563) data 0.257 (0.352) loss_u loss_u 0.8340 (0.8533) acc_u 21.8750 (15.8333) lr 1.0197e-04 eta 0:00:23
epoch [173/200] batch [20/56] time 0.499 (0.560) data 0.285 (0.349) loss_u loss_u 0.8857 (0.8457) acc_u 15.6250 (17.0312) lr 1.0197e-04 eta 0:00:20
epoch [173/200] batch [25/56] time 0.455 (0.560) data 0.242 (0.348) loss_u loss_u 0.8955 (0.8481) acc_u 15.6250 (17.5000) lr 1.0197e-04 eta 0:00:17
epoch [173/200] batch [30/56] time 0.386 (0.555) data 0.218 (0.343) loss_u loss_u 0.8276 (0.8457) acc_u 18.7500 (17.8125) lr 1.0197e-04 eta 0:00:14
epoch [173/200] batch [35/56] time 0.641 (0.558) data 0.425 (0.346) loss_u loss_u 0.9258 (0.8502) acc_u 6.2500 (17.2321) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [40/56] time 0.695 (0.556) data 0.484 (0.344) loss_u loss_u 0.9097 (0.8512) acc_u 9.3750 (17.3438) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [45/56] time 0.678 (0.555) data 0.466 (0.343) loss_u loss_u 0.8628 (0.8529) acc_u 18.7500 (17.2917) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [50/56] time 0.491 (0.550) data 0.276 (0.339) loss_u loss_u 0.8940 (0.8474) acc_u 9.3750 (18.0000) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [55/56] time 0.521 (0.550) data 0.306 (0.339) loss_u loss_u 0.8027 (0.8473) acc_u 25.0000 (18.0682) lr 1.0197e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1424
confident_label rate tensor(0.4037, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1266
clean true:1178
clean false:88
clean_rate:0.9304897314375987
noisy true:534
noisy false:1336
after delete: len(clean_dataset) 1266
after delete: len(noisy_dataset) 1870
epoch [174/200] batch [5/39] time 0.568 (0.570) data 0.359 (0.359) loss_x loss_x 0.8384 (1.0063) acc_x 68.7500 (70.6250) lr 9.5173e-05 eta 0:00:19
epoch [174/200] batch [10/39] time 0.586 (0.564) data 0.377 (0.353) loss_x loss_x 1.4629 (1.0579) acc_x 68.7500 (72.5000) lr 9.5173e-05 eta 0:00:16
epoch [174/200] batch [15/39] time 0.731 (0.567) data 0.520 (0.356) loss_x loss_x 1.0068 (1.0457) acc_x 78.1250 (74.3750) lr 9.5173e-05 eta 0:00:13
epoch [174/200] batch [20/39] time 0.495 (0.560) data 0.283 (0.349) loss_x loss_x 0.9150 (1.0828) acc_x 81.2500 (73.1250) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [25/39] time 0.620 (0.572) data 0.403 (0.361) loss_x loss_x 0.9912 (1.1088) acc_x 81.2500 (72.3750) lr 9.5173e-05 eta 0:00:08
epoch [174/200] batch [30/39] time 0.609 (0.573) data 0.395 (0.361) loss_x loss_x 0.7188 (1.1024) acc_x 87.5000 (72.7083) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [35/39] time 0.384 (0.565) data 0.227 (0.355) loss_x loss_x 1.4170 (1.1010) acc_x 71.8750 (73.2143) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [5/58] time 0.607 (0.571) data 0.389 (0.360) loss_u loss_u 0.7915 (0.8357) acc_u 25.0000 (20.6250) lr 9.5173e-05 eta 0:00:30
epoch [174/200] batch [10/58] time 0.464 (0.567) data 0.247 (0.355) loss_u loss_u 0.7671 (0.8415) acc_u 25.0000 (18.1250) lr 9.5173e-05 eta 0:00:27
epoch [174/200] batch [15/58] time 0.563 (0.565) data 0.349 (0.353) loss_u loss_u 0.8291 (0.8401) acc_u 21.8750 (19.3750) lr 9.5173e-05 eta 0:00:24
epoch [174/200] batch [20/58] time 0.670 (0.563) data 0.453 (0.351) loss_u loss_u 0.8857 (0.8451) acc_u 12.5000 (18.7500) lr 9.5173e-05 eta 0:00:21
epoch [174/200] batch [25/58] time 0.495 (0.567) data 0.287 (0.355) loss_u loss_u 0.8711 (0.8438) acc_u 25.0000 (18.8750) lr 9.5173e-05 eta 0:00:18
epoch [174/200] batch [30/58] time 0.475 (0.565) data 0.264 (0.353) loss_u loss_u 0.8223 (0.8445) acc_u 21.8750 (18.7500) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [35/58] time 0.563 (0.568) data 0.353 (0.355) loss_u loss_u 0.9810 (0.8492) acc_u 0.0000 (17.8571) lr 9.5173e-05 eta 0:00:13
epoch [174/200] batch [40/58] time 0.454 (0.565) data 0.244 (0.353) loss_u loss_u 0.7065 (0.8447) acc_u 31.2500 (18.5938) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [45/58] time 0.590 (0.566) data 0.371 (0.354) loss_u loss_u 0.8994 (0.8481) acc_u 12.5000 (18.2639) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [50/58] time 0.458 (0.563) data 0.271 (0.351) loss_u loss_u 0.8174 (0.8430) acc_u 28.1250 (19.2500) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [55/58] time 0.514 (0.559) data 0.304 (0.347) loss_u loss_u 0.8677 (0.8422) acc_u 12.5000 (19.4318) lr 9.5173e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1415
confident_label rate tensor(0.4104, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1287
clean true:1197
clean false:90
clean_rate:0.9300699300699301
noisy true:524
noisy false:1325
after delete: len(clean_dataset) 1287
after delete: len(noisy_dataset) 1849
epoch [175/200] batch [5/40] time 0.518 (0.533) data 0.306 (0.320) loss_x loss_x 1.2314 (1.1461) acc_x 65.6250 (68.1250) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [10/40] time 0.512 (0.539) data 0.299 (0.326) loss_x loss_x 1.2607 (1.0928) acc_x 68.7500 (71.2500) lr 8.8597e-05 eta 0:00:16
epoch [175/200] batch [15/40] time 0.391 (0.547) data 0.261 (0.339) loss_x loss_x 0.6807 (1.0656) acc_x 78.1250 (71.8750) lr 8.8597e-05 eta 0:00:13
epoch [175/200] batch [20/40] time 0.581 (0.560) data 0.366 (0.350) loss_x loss_x 0.6108 (1.0591) acc_x 81.2500 (72.3438) lr 8.8597e-05 eta 0:00:11
epoch [175/200] batch [25/40] time 0.562 (0.556) data 0.352 (0.346) loss_x loss_x 1.0967 (1.0734) acc_x 78.1250 (71.8750) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [30/40] time 0.551 (0.548) data 0.343 (0.338) loss_x loss_x 1.0801 (1.0450) acc_x 78.1250 (73.5417) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [35/40] time 0.538 (0.555) data 0.324 (0.345) loss_x loss_x 0.8257 (1.0598) acc_x 81.2500 (73.1250) lr 8.8597e-05 eta 0:00:02
epoch [175/200] batch [40/40] time 0.506 (0.553) data 0.294 (0.342) loss_x loss_x 1.0000 (1.0808) acc_x 68.7500 (72.5781) lr 8.8597e-05 eta 0:00:00
epoch [175/200] batch [5/57] time 0.430 (0.548) data 0.217 (0.337) loss_u loss_u 0.8042 (0.8454) acc_u 18.7500 (16.2500) lr 8.8597e-05 eta 0:00:28
epoch [175/200] batch [10/57] time 0.504 (0.554) data 0.290 (0.344) loss_u loss_u 0.7573 (0.8409) acc_u 28.1250 (18.4375) lr 8.8597e-05 eta 0:00:26
epoch [175/200] batch [15/57] time 0.655 (0.554) data 0.446 (0.344) loss_u loss_u 0.8735 (0.8455) acc_u 18.7500 (18.5417) lr 8.8597e-05 eta 0:00:23
epoch [175/200] batch [20/57] time 0.479 (0.551) data 0.281 (0.341) loss_u loss_u 0.8296 (0.8457) acc_u 21.8750 (18.9062) lr 8.8597e-05 eta 0:00:20
epoch [175/200] batch [25/57] time 0.505 (0.552) data 0.295 (0.341) loss_u loss_u 0.8750 (0.8452) acc_u 15.6250 (19.3750) lr 8.8597e-05 eta 0:00:17
epoch [175/200] batch [30/57] time 0.504 (0.550) data 0.290 (0.339) loss_u loss_u 0.8076 (0.8417) acc_u 21.8750 (19.4792) lr 8.8597e-05 eta 0:00:14
epoch [175/200] batch [35/57] time 0.535 (0.554) data 0.325 (0.343) loss_u loss_u 0.8442 (0.8423) acc_u 21.8750 (19.6429) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [40/57] time 0.413 (0.553) data 0.202 (0.342) loss_u loss_u 0.8462 (0.8410) acc_u 25.0000 (19.8438) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [45/57] time 0.543 (0.551) data 0.327 (0.340) loss_u loss_u 0.8037 (0.8429) acc_u 28.1250 (19.5139) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [50/57] time 0.539 (0.550) data 0.327 (0.339) loss_u loss_u 0.8252 (0.8428) acc_u 18.7500 (19.4375) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [55/57] time 0.463 (0.550) data 0.247 (0.339) loss_u loss_u 0.7656 (0.8406) acc_u 31.2500 (19.8295) lr 8.8597e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1400
confident_label rate tensor(0.4133, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1296
clean true:1205
clean false:91
clean_rate:0.9297839506172839
noisy true:531
noisy false:1309
after delete: len(clean_dataset) 1296
after delete: len(noisy_dataset) 1840
epoch [176/200] batch [5/40] time 0.482 (0.580) data 0.270 (0.369) loss_x loss_x 1.5537 (1.1907) acc_x 68.7500 (72.5000) lr 8.2245e-05 eta 0:00:20
epoch [176/200] batch [10/40] time 0.807 (0.615) data 0.595 (0.408) loss_x loss_x 1.2158 (1.1685) acc_x 78.1250 (72.8125) lr 8.2245e-05 eta 0:00:18
epoch [176/200] batch [15/40] time 0.638 (0.592) data 0.425 (0.384) loss_x loss_x 1.1641 (1.1314) acc_x 68.7500 (73.1250) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [20/40] time 0.563 (0.579) data 0.348 (0.370) loss_x loss_x 1.1777 (1.1121) acc_x 78.1250 (73.2812) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [25/40] time 0.676 (0.576) data 0.467 (0.369) loss_x loss_x 0.8911 (1.1140) acc_x 78.1250 (73.3750) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [30/40] time 0.581 (0.570) data 0.371 (0.362) loss_x loss_x 1.1426 (1.1028) acc_x 71.8750 (73.2292) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [35/40] time 0.672 (0.573) data 0.463 (0.366) loss_x loss_x 1.2793 (1.1292) acc_x 65.6250 (72.3214) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [40/40] time 0.537 (0.568) data 0.324 (0.361) loss_x loss_x 0.7617 (1.1018) acc_x 81.2500 (73.2812) lr 8.2245e-05 eta 0:00:00
epoch [176/200] batch [5/57] time 0.477 (0.565) data 0.267 (0.357) loss_u loss_u 0.8984 (0.8605) acc_u 12.5000 (16.8750) lr 8.2245e-05 eta 0:00:29
epoch [176/200] batch [10/57] time 0.549 (0.570) data 0.335 (0.361) loss_u loss_u 0.7397 (0.8408) acc_u 28.1250 (19.0625) lr 8.2245e-05 eta 0:00:26
epoch [176/200] batch [15/57] time 0.510 (0.567) data 0.296 (0.357) loss_u loss_u 0.8779 (0.8277) acc_u 12.5000 (20.8333) lr 8.2245e-05 eta 0:00:23
epoch [176/200] batch [20/57] time 0.421 (0.560) data 0.211 (0.350) loss_u loss_u 0.8984 (0.8323) acc_u 15.6250 (20.3125) lr 8.2245e-05 eta 0:00:20
epoch [176/200] batch [25/57] time 0.465 (0.555) data 0.252 (0.345) loss_u loss_u 0.7866 (0.8381) acc_u 28.1250 (19.6250) lr 8.2245e-05 eta 0:00:17
epoch [176/200] batch [30/57] time 0.580 (0.557) data 0.338 (0.347) loss_u loss_u 0.7871 (0.8405) acc_u 28.1250 (19.1667) lr 8.2245e-05 eta 0:00:15
epoch [176/200] batch [35/57] time 0.587 (0.556) data 0.372 (0.346) loss_u loss_u 0.9438 (0.8417) acc_u 3.1250 (18.9286) lr 8.2245e-05 eta 0:00:12
epoch [176/200] batch [40/57] time 0.713 (0.559) data 0.494 (0.349) loss_u loss_u 0.8950 (0.8426) acc_u 9.3750 (19.2188) lr 8.2245e-05 eta 0:00:09
epoch [176/200] batch [45/57] time 0.476 (0.556) data 0.261 (0.346) loss_u loss_u 0.8291 (0.8430) acc_u 21.8750 (19.0278) lr 8.2245e-05 eta 0:00:06
epoch [176/200] batch [50/57] time 0.486 (0.554) data 0.272 (0.344) loss_u loss_u 0.8242 (0.8428) acc_u 21.8750 (19.0000) lr 8.2245e-05 eta 0:00:03
epoch [176/200] batch [55/57] time 0.463 (0.551) data 0.249 (0.340) loss_u loss_u 0.8550 (0.8432) acc_u 15.6250 (19.1477) lr 8.2245e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1431
confident_label rate tensor(0.4027, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1263
clean true:1186
clean false:77
clean_rate:0.9390340459224069
noisy true:519
noisy false:1354
after delete: len(clean_dataset) 1263
after delete: len(noisy_dataset) 1873
epoch [177/200] batch [5/39] time 0.509 (0.533) data 0.300 (0.322) loss_x loss_x 1.0078 (1.0039) acc_x 75.0000 (74.3750) lr 7.6120e-05 eta 0:00:18
epoch [177/200] batch [10/39] time 0.508 (0.524) data 0.298 (0.312) loss_x loss_x 0.5967 (0.9483) acc_x 84.3750 (76.2500) lr 7.6120e-05 eta 0:00:15
epoch [177/200] batch [15/39] time 0.486 (0.533) data 0.272 (0.322) loss_x loss_x 0.7354 (0.9332) acc_x 84.3750 (77.0833) lr 7.6120e-05 eta 0:00:12
epoch [177/200] batch [20/39] time 0.469 (0.525) data 0.255 (0.317) loss_x loss_x 1.2236 (0.9496) acc_x 75.0000 (76.7188) lr 7.6120e-05 eta 0:00:09
epoch [177/200] batch [25/39] time 0.390 (0.523) data 0.260 (0.317) loss_x loss_x 1.1133 (0.9655) acc_x 75.0000 (76.2500) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [30/39] time 0.664 (0.529) data 0.454 (0.322) loss_x loss_x 1.3477 (0.9915) acc_x 71.8750 (75.3125) lr 7.6120e-05 eta 0:00:04
epoch [177/200] batch [35/39] time 0.578 (0.527) data 0.366 (0.319) loss_x loss_x 1.0322 (0.9764) acc_x 75.0000 (75.8929) lr 7.6120e-05 eta 0:00:02
epoch [177/200] batch [5/58] time 0.638 (0.526) data 0.425 (0.317) loss_u loss_u 0.8760 (0.8509) acc_u 15.6250 (20.6250) lr 7.6120e-05 eta 0:00:27
epoch [177/200] batch [10/58] time 0.499 (0.526) data 0.284 (0.317) loss_u loss_u 0.8457 (0.8464) acc_u 25.0000 (21.2500) lr 7.6120e-05 eta 0:00:25
epoch [177/200] batch [15/58] time 0.476 (0.531) data 0.262 (0.320) loss_u loss_u 0.8232 (0.8519) acc_u 25.0000 (19.5833) lr 7.6120e-05 eta 0:00:22
epoch [177/200] batch [20/58] time 0.454 (0.525) data 0.240 (0.316) loss_u loss_u 0.8208 (0.8466) acc_u 28.1250 (20.4688) lr 7.6120e-05 eta 0:00:19
epoch [177/200] batch [25/58] time 0.791 (0.532) data 0.578 (0.323) loss_u loss_u 0.7842 (0.8440) acc_u 28.1250 (20.5000) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [30/58] time 0.539 (0.531) data 0.328 (0.322) loss_u loss_u 0.8491 (0.8481) acc_u 15.6250 (19.4792) lr 7.6120e-05 eta 0:00:14
epoch [177/200] batch [35/58] time 0.439 (0.533) data 0.225 (0.324) loss_u loss_u 0.8911 (0.8499) acc_u 15.6250 (19.3750) lr 7.6120e-05 eta 0:00:12
epoch [177/200] batch [40/58] time 0.804 (0.540) data 0.667 (0.332) loss_u loss_u 0.8647 (0.8492) acc_u 12.5000 (19.1406) lr 7.6120e-05 eta 0:00:09
epoch [177/200] batch [45/58] time 0.695 (0.542) data 0.481 (0.334) loss_u loss_u 0.8286 (0.8478) acc_u 18.7500 (19.1667) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [50/58] time 0.453 (0.543) data 0.242 (0.335) loss_u loss_u 0.8403 (0.8478) acc_u 21.8750 (19.1250) lr 7.6120e-05 eta 0:00:04
epoch [177/200] batch [55/58] time 0.614 (0.544) data 0.400 (0.336) loss_u loss_u 0.8643 (0.8469) acc_u 18.7500 (19.3750) lr 7.6120e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.4062, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1274
clean true:1198
clean false:76
clean_rate:0.9403453689167975
noisy true:518
noisy false:1344
after delete: len(clean_dataset) 1274
after delete: len(noisy_dataset) 1862
epoch [178/200] batch [5/39] time 0.572 (0.565) data 0.358 (0.342) loss_x loss_x 0.6377 (0.9308) acc_x 81.2500 (77.5000) lr 7.0224e-05 eta 0:00:19
epoch [178/200] batch [10/39] time 0.535 (0.557) data 0.321 (0.338) loss_x loss_x 1.1309 (0.9536) acc_x 68.7500 (76.8750) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [15/39] time 0.518 (0.554) data 0.304 (0.338) loss_x loss_x 1.2168 (1.0128) acc_x 68.7500 (75.4167) lr 7.0224e-05 eta 0:00:13
epoch [178/200] batch [20/39] time 0.402 (0.555) data 0.271 (0.343) loss_x loss_x 1.5957 (1.0615) acc_x 62.5000 (74.8438) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [25/39] time 0.641 (0.564) data 0.428 (0.352) loss_x loss_x 1.2510 (1.0689) acc_x 68.7500 (75.2500) lr 7.0224e-05 eta 0:00:07
epoch [178/200] batch [30/39] time 0.559 (0.559) data 0.345 (0.347) loss_x loss_x 1.4844 (1.0985) acc_x 65.6250 (74.2708) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [35/39] time 0.531 (0.553) data 0.318 (0.343) loss_x loss_x 1.0947 (1.0768) acc_x 75.0000 (74.5536) lr 7.0224e-05 eta 0:00:02
epoch [178/200] batch [5/58] time 0.536 (0.554) data 0.324 (0.344) loss_u loss_u 0.8374 (0.8351) acc_u 21.8750 (20.6250) lr 7.0224e-05 eta 0:00:29
epoch [178/200] batch [10/58] time 0.651 (0.553) data 0.433 (0.342) loss_u loss_u 0.8218 (0.8392) acc_u 18.7500 (20.6250) lr 7.0224e-05 eta 0:00:26
epoch [178/200] batch [15/58] time 0.571 (0.557) data 0.360 (0.348) loss_u loss_u 0.8398 (0.8447) acc_u 15.6250 (19.1667) lr 7.0224e-05 eta 0:00:23
epoch [178/200] batch [20/58] time 0.619 (0.555) data 0.406 (0.345) loss_u loss_u 0.8301 (0.8414) acc_u 18.7500 (19.0625) lr 7.0224e-05 eta 0:00:21
epoch [178/200] batch [25/58] time 0.638 (0.554) data 0.420 (0.344) loss_u loss_u 0.8340 (0.8410) acc_u 15.6250 (18.7500) lr 7.0224e-05 eta 0:00:18
epoch [178/200] batch [30/58] time 0.622 (0.555) data 0.412 (0.345) loss_u loss_u 0.8115 (0.8382) acc_u 18.7500 (19.0625) lr 7.0224e-05 eta 0:00:15
epoch [178/200] batch [35/58] time 0.460 (0.550) data 0.245 (0.340) loss_u loss_u 0.9277 (0.8429) acc_u 12.5000 (18.8393) lr 7.0224e-05 eta 0:00:12
epoch [178/200] batch [40/58] time 0.775 (0.554) data 0.564 (0.344) loss_u loss_u 0.8198 (0.8396) acc_u 18.7500 (19.4531) lr 7.0224e-05 eta 0:00:09
epoch [178/200] batch [45/58] time 0.588 (0.551) data 0.376 (0.341) loss_u loss_u 0.7485 (0.8369) acc_u 28.1250 (19.7222) lr 7.0224e-05 eta 0:00:07
epoch [178/200] batch [50/58] time 0.598 (0.552) data 0.380 (0.342) loss_u loss_u 0.8154 (0.8362) acc_u 28.1250 (19.9375) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [55/58] time 0.532 (0.550) data 0.334 (0.340) loss_u loss_u 0.8159 (0.8399) acc_u 21.8750 (19.5455) lr 7.0224e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1411
confident_label rate tensor(0.4066, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1275
clean true:1179
clean false:96
clean_rate:0.9247058823529412
noisy true:546
noisy false:1315
after delete: len(clean_dataset) 1275
after delete: len(noisy_dataset) 1861
epoch [179/200] batch [5/39] time 0.587 (0.525) data 0.376 (0.310) loss_x loss_x 0.7808 (1.0297) acc_x 84.3750 (76.2500) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [10/39] time 0.420 (0.503) data 0.205 (0.295) loss_x loss_x 0.9692 (1.0765) acc_x 62.5000 (71.8750) lr 6.4556e-05 eta 0:00:14
epoch [179/200] batch [15/39] time 0.572 (0.543) data 0.442 (0.338) loss_x loss_x 0.9492 (1.0092) acc_x 71.8750 (73.5417) lr 6.4556e-05 eta 0:00:13
epoch [179/200] batch [20/39] time 0.539 (0.554) data 0.329 (0.348) loss_x loss_x 1.0430 (1.0634) acc_x 75.0000 (73.2812) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [25/39] time 0.464 (0.550) data 0.255 (0.343) loss_x loss_x 1.2002 (1.0667) acc_x 62.5000 (73.2500) lr 6.4556e-05 eta 0:00:07
epoch [179/200] batch [30/39] time 0.583 (0.544) data 0.375 (0.338) loss_x loss_x 0.9946 (1.0628) acc_x 81.2500 (74.2708) lr 6.4556e-05 eta 0:00:04
epoch [179/200] batch [35/39] time 0.566 (0.546) data 0.354 (0.340) loss_x loss_x 1.0449 (1.0616) acc_x 65.6250 (73.9286) lr 6.4556e-05 eta 0:00:02
epoch [179/200] batch [5/58] time 0.614 (0.537) data 0.399 (0.330) loss_u loss_u 0.8608 (0.8310) acc_u 18.7500 (21.8750) lr 6.4556e-05 eta 0:00:28
epoch [179/200] batch [10/58] time 0.610 (0.539) data 0.396 (0.330) loss_u loss_u 0.8457 (0.8502) acc_u 18.7500 (18.7500) lr 6.4556e-05 eta 0:00:25
epoch [179/200] batch [15/58] time 0.531 (0.543) data 0.318 (0.334) loss_u loss_u 0.8516 (0.8421) acc_u 21.8750 (20.2083) lr 6.4556e-05 eta 0:00:23
epoch [179/200] batch [20/58] time 0.508 (0.543) data 0.288 (0.333) loss_u loss_u 0.8765 (0.8387) acc_u 12.5000 (20.4688) lr 6.4556e-05 eta 0:00:20
epoch [179/200] batch [25/58] time 0.464 (0.539) data 0.333 (0.331) loss_u loss_u 0.8008 (0.8400) acc_u 28.1250 (20.2500) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [30/58] time 0.591 (0.540) data 0.378 (0.331) loss_u loss_u 0.9253 (0.8407) acc_u 12.5000 (20.4167) lr 6.4556e-05 eta 0:00:15
epoch [179/200] batch [35/58] time 0.566 (0.538) data 0.354 (0.329) loss_u loss_u 0.9302 (0.8368) acc_u 3.1250 (20.5357) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [40/58] time 0.509 (0.537) data 0.297 (0.328) loss_u loss_u 0.8721 (0.8349) acc_u 15.6250 (20.7031) lr 6.4556e-05 eta 0:00:09
epoch [179/200] batch [45/58] time 0.557 (0.537) data 0.343 (0.327) loss_u loss_u 0.7910 (0.8377) acc_u 21.8750 (20.2083) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [50/58] time 0.649 (0.539) data 0.439 (0.329) loss_u loss_u 0.7798 (0.8371) acc_u 25.0000 (20.3125) lr 6.4556e-05 eta 0:00:04
epoch [179/200] batch [55/58] time 0.481 (0.540) data 0.269 (0.330) loss_u loss_u 0.8911 (0.8394) acc_u 15.6250 (19.8864) lr 6.4556e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.4126, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1294
clean true:1206
clean false:88
clean_rate:0.9319938176197836
noisy true:517
noisy false:1325
after delete: len(clean_dataset) 1294
after delete: len(noisy_dataset) 1842
epoch [180/200] batch [5/40] time 0.491 (0.515) data 0.284 (0.305) loss_x loss_x 1.3018 (1.0334) acc_x 75.0000 (75.6250) lr 5.9119e-05 eta 0:00:18
epoch [180/200] batch [10/40] time 0.456 (0.523) data 0.245 (0.312) loss_x loss_x 0.9033 (1.0173) acc_x 71.8750 (75.0000) lr 5.9119e-05 eta 0:00:15
epoch [180/200] batch [15/40] time 0.473 (0.525) data 0.264 (0.314) loss_x loss_x 1.3066 (1.0140) acc_x 71.8750 (74.3750) lr 5.9119e-05 eta 0:00:13
epoch [180/200] batch [20/40] time 0.468 (0.527) data 0.257 (0.317) loss_x loss_x 1.3809 (1.0791) acc_x 75.0000 (72.9688) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [25/40] time 0.744 (0.538) data 0.529 (0.328) loss_x loss_x 1.0205 (1.0632) acc_x 75.0000 (74.0000) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [30/40] time 0.756 (0.547) data 0.543 (0.336) loss_x loss_x 0.4302 (1.0462) acc_x 87.5000 (74.2708) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [35/40] time 0.451 (0.537) data 0.240 (0.329) loss_x loss_x 1.6914 (1.0777) acc_x 56.2500 (73.7500) lr 5.9119e-05 eta 0:00:02
epoch [180/200] batch [40/40] time 0.504 (0.538) data 0.295 (0.329) loss_x loss_x 1.4551 (1.0726) acc_x 62.5000 (73.7500) lr 5.9119e-05 eta 0:00:00
epoch [180/200] batch [5/57] time 0.490 (0.534) data 0.277 (0.324) loss_u loss_u 0.7988 (0.8100) acc_u 25.0000 (23.7500) lr 5.9119e-05 eta 0:00:27
epoch [180/200] batch [10/57] time 0.501 (0.534) data 0.291 (0.324) loss_u loss_u 0.8389 (0.8321) acc_u 18.7500 (20.6250) lr 5.9119e-05 eta 0:00:25
epoch [180/200] batch [15/57] time 0.638 (0.538) data 0.424 (0.328) loss_u loss_u 0.7749 (0.8266) acc_u 25.0000 (21.6667) lr 5.9119e-05 eta 0:00:22
epoch [180/200] batch [20/57] time 0.680 (0.538) data 0.465 (0.328) loss_u loss_u 0.9121 (0.8390) acc_u 12.5000 (19.8438) lr 5.9119e-05 eta 0:00:19
epoch [180/200] batch [25/57] time 0.531 (0.537) data 0.322 (0.328) loss_u loss_u 0.9458 (0.8450) acc_u 6.2500 (18.8750) lr 5.9119e-05 eta 0:00:17
epoch [180/200] batch [30/57] time 0.567 (0.545) data 0.357 (0.335) loss_u loss_u 0.8022 (0.8457) acc_u 25.0000 (18.6458) lr 5.9119e-05 eta 0:00:14
epoch [180/200] batch [35/57] time 0.599 (0.544) data 0.390 (0.334) loss_u loss_u 0.7578 (0.8450) acc_u 34.3750 (19.1071) lr 5.9119e-05 eta 0:00:11
epoch [180/200] batch [40/57] time 0.498 (0.543) data 0.288 (0.334) loss_u loss_u 0.8545 (0.8409) acc_u 15.6250 (19.6875) lr 5.9119e-05 eta 0:00:09
epoch [180/200] batch [45/57] time 0.490 (0.542) data 0.277 (0.333) loss_u loss_u 0.8838 (0.8409) acc_u 12.5000 (20.0694) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [50/57] time 0.474 (0.539) data 0.255 (0.329) loss_u loss_u 0.7773 (0.8389) acc_u 31.2500 (20.3750) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [55/57] time 0.638 (0.539) data 0.427 (0.329) loss_u loss_u 0.8647 (0.8412) acc_u 12.5000 (19.8864) lr 5.9119e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1359
confident_label rate tensor(0.4247, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1332
clean true:1232
clean false:100
clean_rate:0.924924924924925
noisy true:545
noisy false:1259
after delete: len(clean_dataset) 1332
after delete: len(noisy_dataset) 1804
epoch [181/200] batch [5/41] time 0.443 (0.530) data 0.229 (0.317) loss_x loss_x 0.8862 (1.0961) acc_x 75.0000 (71.8750) lr 5.3915e-05 eta 0:00:19
epoch [181/200] batch [10/41] time 0.620 (0.554) data 0.410 (0.341) loss_x loss_x 0.7334 (1.0118) acc_x 84.3750 (74.6875) lr 5.3915e-05 eta 0:00:17
epoch [181/200] batch [15/41] time 0.548 (0.561) data 0.333 (0.348) loss_x loss_x 0.9683 (0.9708) acc_x 71.8750 (75.0000) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [20/41] time 0.537 (0.568) data 0.327 (0.358) loss_x loss_x 1.1680 (0.9956) acc_x 75.0000 (75.0000) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [25/41] time 0.621 (0.569) data 0.410 (0.361) loss_x loss_x 0.6602 (0.9754) acc_x 84.3750 (76.2500) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [30/41] time 0.485 (0.559) data 0.268 (0.350) loss_x loss_x 1.3701 (1.0176) acc_x 65.6250 (75.0000) lr 5.3915e-05 eta 0:00:06
epoch [181/200] batch [35/41] time 0.552 (0.556) data 0.341 (0.347) loss_x loss_x 1.5244 (1.0358) acc_x 59.3750 (74.9107) lr 5.3915e-05 eta 0:00:03
epoch [181/200] batch [40/41] time 0.436 (0.549) data 0.222 (0.340) loss_x loss_x 0.9707 (1.0525) acc_x 68.7500 (74.2188) lr 5.3915e-05 eta 0:00:00
epoch [181/200] batch [5/56] time 0.493 (0.542) data 0.282 (0.331) loss_u loss_u 0.8657 (0.8427) acc_u 15.6250 (18.7500) lr 5.3915e-05 eta 0:00:27
epoch [181/200] batch [10/56] time 0.557 (0.544) data 0.343 (0.333) loss_u loss_u 0.8965 (0.8499) acc_u 9.3750 (17.8125) lr 5.3915e-05 eta 0:00:25
epoch [181/200] batch [15/56] time 0.550 (0.539) data 0.330 (0.327) loss_u loss_u 0.8335 (0.8496) acc_u 25.0000 (19.3750) lr 5.3915e-05 eta 0:00:22
epoch [181/200] batch [20/56] time 0.624 (0.543) data 0.410 (0.331) loss_u loss_u 0.8574 (0.8485) acc_u 15.6250 (18.5938) lr 5.3915e-05 eta 0:00:19
epoch [181/200] batch [25/56] time 0.484 (0.544) data 0.270 (0.332) loss_u loss_u 0.8320 (0.8412) acc_u 28.1250 (20.1250) lr 5.3915e-05 eta 0:00:16
epoch [181/200] batch [30/56] time 0.876 (0.551) data 0.661 (0.338) loss_u loss_u 0.9517 (0.8505) acc_u 3.1250 (18.5417) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [35/56] time 0.479 (0.551) data 0.264 (0.338) loss_u loss_u 0.7935 (0.8530) acc_u 18.7500 (18.1250) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [40/56] time 0.448 (0.548) data 0.239 (0.335) loss_u loss_u 0.9150 (0.8518) acc_u 9.3750 (18.3594) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [45/56] time 0.555 (0.547) data 0.345 (0.334) loss_u loss_u 0.8691 (0.8536) acc_u 12.5000 (17.9861) lr 5.3915e-05 eta 0:00:06
epoch [181/200] batch [50/56] time 0.476 (0.543) data 0.264 (0.331) loss_u loss_u 0.7769 (0.8526) acc_u 34.3750 (18.3750) lr 5.3915e-05 eta 0:00:03
epoch [181/200] batch [55/56] time 0.493 (0.544) data 0.284 (0.332) loss_u loss_u 0.8115 (0.8485) acc_u 28.1250 (18.9773) lr 5.3915e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1421
confident_label rate tensor(0.4120, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1292
clean true:1201
clean false:91
clean_rate:0.9295665634674922
noisy true:514
noisy false:1330
after delete: len(clean_dataset) 1292
after delete: len(noisy_dataset) 1844
epoch [182/200] batch [5/40] time 0.647 (0.581) data 0.435 (0.367) loss_x loss_x 1.6104 (1.1693) acc_x 62.5000 (70.6250) lr 4.8943e-05 eta 0:00:20
epoch [182/200] batch [10/40] time 0.498 (0.561) data 0.284 (0.348) loss_x loss_x 1.3418 (1.1542) acc_x 71.8750 (71.5625) lr 4.8943e-05 eta 0:00:16
epoch [182/200] batch [15/40] time 0.664 (0.561) data 0.452 (0.352) loss_x loss_x 0.6973 (1.0716) acc_x 81.2500 (72.9167) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [20/40] time 0.462 (0.540) data 0.251 (0.330) loss_x loss_x 0.7974 (1.1024) acc_x 78.1250 (72.5000) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [25/40] time 0.438 (0.537) data 0.228 (0.327) loss_x loss_x 1.0039 (1.0749) acc_x 65.6250 (72.3750) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [30/40] time 0.532 (0.537) data 0.321 (0.327) loss_x loss_x 1.5820 (1.1152) acc_x 68.7500 (71.6667) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [35/40] time 0.563 (0.538) data 0.352 (0.327) loss_x loss_x 0.9897 (1.1101) acc_x 68.7500 (71.8750) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [40/40] time 0.604 (0.549) data 0.389 (0.338) loss_x loss_x 1.8906 (1.1512) acc_x 59.3750 (71.0156) lr 4.8943e-05 eta 0:00:00
epoch [182/200] batch [5/57] time 0.576 (0.543) data 0.365 (0.333) loss_u loss_u 0.7563 (0.8313) acc_u 28.1250 (18.7500) lr 4.8943e-05 eta 0:00:28
epoch [182/200] batch [10/57] time 0.674 (0.546) data 0.463 (0.336) loss_u loss_u 0.8438 (0.8212) acc_u 15.6250 (20.6250) lr 4.8943e-05 eta 0:00:25
epoch [182/200] batch [15/57] time 0.585 (0.551) data 0.370 (0.340) loss_u loss_u 0.7690 (0.8117) acc_u 31.2500 (22.9167) lr 4.8943e-05 eta 0:00:23
epoch [182/200] batch [20/57] time 0.543 (0.552) data 0.330 (0.341) loss_u loss_u 0.8774 (0.8216) acc_u 9.3750 (21.0938) lr 4.8943e-05 eta 0:00:20
epoch [182/200] batch [25/57] time 0.718 (0.553) data 0.586 (0.343) loss_u loss_u 0.7808 (0.8223) acc_u 28.1250 (21.0000) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [30/57] time 0.469 (0.551) data 0.259 (0.342) loss_u loss_u 0.8638 (0.8268) acc_u 18.7500 (20.6250) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [35/57] time 0.462 (0.548) data 0.249 (0.338) loss_u loss_u 0.8076 (0.8256) acc_u 21.8750 (20.8929) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [40/57] time 0.516 (0.546) data 0.307 (0.336) loss_u loss_u 0.7773 (0.8275) acc_u 28.1250 (20.7812) lr 4.8943e-05 eta 0:00:09
epoch [182/200] batch [45/57] time 0.448 (0.542) data 0.231 (0.332) loss_u loss_u 0.7495 (0.8298) acc_u 31.2500 (20.9722) lr 4.8943e-05 eta 0:00:06
epoch [182/200] batch [50/57] time 0.495 (0.540) data 0.281 (0.330) loss_u loss_u 0.8311 (0.8309) acc_u 18.7500 (20.6875) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [55/57] time 0.486 (0.538) data 0.271 (0.327) loss_u loss_u 0.9097 (0.8333) acc_u 12.5000 (20.1136) lr 4.8943e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1390
confident_label rate tensor(0.4203, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1318
clean true:1226
clean false:92
clean_rate:0.9301972685887708
noisy true:520
noisy false:1298
after delete: len(clean_dataset) 1318
after delete: len(noisy_dataset) 1818
epoch [183/200] batch [5/41] time 0.740 (0.591) data 0.526 (0.378) loss_x loss_x 1.1719 (1.0651) acc_x 65.6250 (71.2500) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [10/41] time 0.507 (0.536) data 0.295 (0.323) loss_x loss_x 0.9473 (0.9836) acc_x 71.8750 (73.4375) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [15/41] time 0.579 (0.546) data 0.368 (0.333) loss_x loss_x 0.9175 (0.9919) acc_x 78.1250 (74.7917) lr 4.4207e-05 eta 0:00:14
epoch [183/200] batch [20/41] time 0.521 (0.535) data 0.291 (0.324) loss_x loss_x 0.8594 (0.9789) acc_x 78.1250 (75.7812) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [25/41] time 0.439 (0.527) data 0.227 (0.316) loss_x loss_x 1.2275 (1.0041) acc_x 71.8750 (75.1250) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [30/41] time 0.424 (0.522) data 0.211 (0.310) loss_x loss_x 0.6133 (0.9857) acc_x 84.3750 (75.1042) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [35/41] time 0.607 (0.524) data 0.394 (0.315) loss_x loss_x 0.8145 (0.9686) acc_x 78.1250 (75.8036) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [40/41] time 0.563 (0.527) data 0.353 (0.317) loss_x loss_x 1.3857 (0.9962) acc_x 62.5000 (75.1562) lr 4.4207e-05 eta 0:00:00
epoch [183/200] batch [5/56] time 0.592 (0.529) data 0.371 (0.318) loss_u loss_u 0.8960 (0.8666) acc_u 12.5000 (14.3750) lr 4.4207e-05 eta 0:00:26
epoch [183/200] batch [10/56] time 0.512 (0.526) data 0.297 (0.315) loss_u loss_u 0.8652 (0.8594) acc_u 21.8750 (16.8750) lr 4.4207e-05 eta 0:00:24
epoch [183/200] batch [15/56] time 0.618 (0.527) data 0.403 (0.316) loss_u loss_u 0.9072 (0.8628) acc_u 9.3750 (16.4583) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [20/56] time 0.506 (0.527) data 0.294 (0.316) loss_u loss_u 0.8643 (0.8631) acc_u 12.5000 (16.7188) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [25/56] time 0.491 (0.524) data 0.262 (0.312) loss_u loss_u 0.8252 (0.8586) acc_u 18.7500 (17.3750) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [30/56] time 0.766 (0.529) data 0.553 (0.317) loss_u loss_u 0.7905 (0.8553) acc_u 18.7500 (17.2917) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [35/56] time 0.516 (0.528) data 0.300 (0.316) loss_u loss_u 0.9043 (0.8557) acc_u 12.5000 (17.6786) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [40/56] time 0.555 (0.534) data 0.339 (0.321) loss_u loss_u 0.8511 (0.8565) acc_u 15.6250 (17.4219) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [45/56] time 0.535 (0.531) data 0.298 (0.319) loss_u loss_u 0.8120 (0.8545) acc_u 25.0000 (17.6389) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [50/56] time 0.479 (0.531) data 0.259 (0.318) loss_u loss_u 0.8657 (0.8535) acc_u 15.6250 (17.9375) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [55/56] time 0.623 (0.531) data 0.415 (0.319) loss_u loss_u 0.8574 (0.8533) acc_u 15.6250 (17.8977) lr 4.4207e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1367
confident_label rate tensor(0.4212, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1321
clean true:1231
clean false:90
clean_rate:0.9318697956093869
noisy true:538
noisy false:1277
after delete: len(clean_dataset) 1321
after delete: len(noisy_dataset) 1815
epoch [184/200] batch [5/41] time 0.681 (0.568) data 0.470 (0.354) loss_x loss_x 0.7808 (1.1526) acc_x 78.1250 (69.3750) lr 3.9706e-05 eta 0:00:20
epoch [184/200] batch [10/41] time 0.547 (0.560) data 0.337 (0.351) loss_x loss_x 1.2051 (1.1027) acc_x 68.7500 (70.9375) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [15/41] time 0.632 (0.557) data 0.421 (0.347) loss_x loss_x 1.0439 (1.0798) acc_x 75.0000 (71.4583) lr 3.9706e-05 eta 0:00:14
epoch [184/200] batch [20/41] time 0.584 (0.568) data 0.372 (0.358) loss_x loss_x 1.7588 (1.1214) acc_x 65.6250 (71.0938) lr 3.9706e-05 eta 0:00:11
epoch [184/200] batch [25/41] time 0.462 (0.558) data 0.245 (0.347) loss_x loss_x 0.6191 (1.1619) acc_x 90.6250 (71.6250) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [30/41] time 0.497 (0.550) data 0.286 (0.339) loss_x loss_x 0.8301 (1.1225) acc_x 81.2500 (72.2917) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [35/41] time 0.507 (0.549) data 0.297 (0.341) loss_x loss_x 1.1143 (1.1302) acc_x 71.8750 (72.1429) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [40/41] time 0.568 (0.546) data 0.356 (0.337) loss_x loss_x 1.2979 (1.1090) acc_x 71.8750 (72.7344) lr 3.9706e-05 eta 0:00:00
epoch [184/200] batch [5/56] time 0.440 (0.540) data 0.226 (0.330) loss_u loss_u 0.8823 (0.8523) acc_u 15.6250 (16.8750) lr 3.9706e-05 eta 0:00:27
epoch [184/200] batch [10/56] time 0.520 (0.542) data 0.309 (0.332) loss_u loss_u 0.8687 (0.8747) acc_u 15.6250 (14.6875) lr 3.9706e-05 eta 0:00:24
epoch [184/200] batch [15/56] time 0.533 (0.540) data 0.321 (0.329) loss_u loss_u 0.7993 (0.8607) acc_u 25.0000 (16.6667) lr 3.9706e-05 eta 0:00:22
epoch [184/200] batch [20/56] time 0.492 (0.532) data 0.279 (0.323) loss_u loss_u 0.7998 (0.8542) acc_u 25.0000 (17.5000) lr 3.9706e-05 eta 0:00:19
epoch [184/200] batch [25/56] time 0.619 (0.534) data 0.359 (0.324) loss_u loss_u 0.8462 (0.8576) acc_u 15.6250 (16.7500) lr 3.9706e-05 eta 0:00:16
epoch [184/200] batch [30/56] time 0.588 (0.535) data 0.374 (0.325) loss_u loss_u 0.8740 (0.8608) acc_u 15.6250 (16.3542) lr 3.9706e-05 eta 0:00:13
epoch [184/200] batch [35/56] time 0.451 (0.534) data 0.237 (0.324) loss_u loss_u 0.6997 (0.8528) acc_u 40.6250 (17.4107) lr 3.9706e-05 eta 0:00:11
epoch [184/200] batch [40/56] time 0.514 (0.533) data 0.297 (0.322) loss_u loss_u 0.8301 (0.8516) acc_u 21.8750 (17.7344) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [45/56] time 0.648 (0.533) data 0.434 (0.323) loss_u loss_u 0.8755 (0.8501) acc_u 18.7500 (18.2639) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [50/56] time 0.544 (0.532) data 0.330 (0.321) loss_u loss_u 0.8975 (0.8503) acc_u 9.3750 (18.0000) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [55/56] time 0.670 (0.533) data 0.459 (0.324) loss_u loss_u 0.8423 (0.8509) acc_u 18.7500 (17.9545) lr 3.9706e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1401
confident_label rate tensor(0.4043, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1268
clean true:1187
clean false:81
clean_rate:0.9361198738170347
noisy true:548
noisy false:1320
after delete: len(clean_dataset) 1268
after delete: len(noisy_dataset) 1868
epoch [185/200] batch [5/39] time 0.385 (0.539) data 0.255 (0.345) loss_x loss_x 1.1377 (1.0931) acc_x 75.0000 (76.8750) lr 3.5443e-05 eta 0:00:18
epoch [185/200] batch [10/39] time 0.528 (0.522) data 0.318 (0.319) loss_x loss_x 0.8711 (1.1076) acc_x 68.7500 (73.7500) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [15/39] time 0.533 (0.541) data 0.321 (0.335) loss_x loss_x 0.8364 (1.0386) acc_x 81.2500 (75.0000) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [20/39] time 0.505 (0.535) data 0.296 (0.331) loss_x loss_x 1.4961 (1.0522) acc_x 65.6250 (74.6875) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [25/39] time 0.498 (0.527) data 0.289 (0.322) loss_x loss_x 0.8047 (1.0677) acc_x 81.2500 (74.6250) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [30/39] time 0.808 (0.538) data 0.600 (0.332) loss_x loss_x 1.2012 (1.0785) acc_x 68.7500 (74.5833) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [35/39] time 0.531 (0.542) data 0.321 (0.335) loss_x loss_x 0.8042 (1.0570) acc_x 78.1250 (74.8214) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [5/58] time 0.473 (0.535) data 0.260 (0.328) loss_u loss_u 0.8887 (0.8748) acc_u 6.2500 (13.7500) lr 3.5443e-05 eta 0:00:28
epoch [185/200] batch [10/58] time 0.708 (0.533) data 0.497 (0.326) loss_u loss_u 0.8301 (0.8583) acc_u 21.8750 (16.5625) lr 3.5443e-05 eta 0:00:25
epoch [185/200] batch [15/58] time 0.536 (0.532) data 0.319 (0.324) loss_u loss_u 0.8149 (0.8507) acc_u 18.7500 (18.3333) lr 3.5443e-05 eta 0:00:22
epoch [185/200] batch [20/58] time 0.778 (0.533) data 0.561 (0.324) loss_u loss_u 0.8291 (0.8482) acc_u 25.0000 (19.2188) lr 3.5443e-05 eta 0:00:20
epoch [185/200] batch [25/58] time 0.553 (0.536) data 0.342 (0.328) loss_u loss_u 0.8911 (0.8498) acc_u 12.5000 (19.1250) lr 3.5443e-05 eta 0:00:17
epoch [185/200] batch [30/58] time 0.668 (0.534) data 0.456 (0.327) loss_u loss_u 0.8247 (0.8524) acc_u 25.0000 (18.5417) lr 3.5443e-05 eta 0:00:14
epoch [185/200] batch [35/58] time 0.541 (0.533) data 0.327 (0.325) loss_u loss_u 0.9326 (0.8468) acc_u 9.3750 (19.2857) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [40/58] time 0.527 (0.534) data 0.312 (0.326) loss_u loss_u 0.8081 (0.8418) acc_u 21.8750 (20.0000) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [45/58] time 0.490 (0.536) data 0.278 (0.327) loss_u loss_u 0.8125 (0.8422) acc_u 25.0000 (19.9306) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [50/58] time 0.503 (0.534) data 0.289 (0.325) loss_u loss_u 0.8428 (0.8420) acc_u 15.6250 (19.8750) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [55/58] time 0.467 (0.533) data 0.253 (0.325) loss_u loss_u 0.7856 (0.8382) acc_u 25.0000 (20.2841) lr 3.5443e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1382
confident_label rate tensor(0.4152, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1302
clean true:1213
clean false:89
clean_rate:0.9316436251920123
noisy true:541
noisy false:1293
after delete: len(clean_dataset) 1302
after delete: len(noisy_dataset) 1834
epoch [186/200] batch [5/40] time 0.520 (0.580) data 0.310 (0.375) loss_x loss_x 1.4980 (1.0583) acc_x 65.6250 (75.6250) lr 3.1417e-05 eta 0:00:20
epoch [186/200] batch [10/40] time 0.547 (0.562) data 0.333 (0.354) loss_x loss_x 0.8569 (1.1339) acc_x 78.1250 (74.6875) lr 3.1417e-05 eta 0:00:16
epoch [186/200] batch [15/40] time 0.509 (0.555) data 0.301 (0.346) loss_x loss_x 0.9121 (1.1110) acc_x 78.1250 (73.9583) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [20/40] time 0.501 (0.560) data 0.290 (0.354) loss_x loss_x 1.4346 (1.1173) acc_x 65.6250 (73.2812) lr 3.1417e-05 eta 0:00:11
epoch [186/200] batch [25/40] time 0.620 (0.570) data 0.401 (0.363) loss_x loss_x 1.1416 (1.1085) acc_x 62.5000 (72.6250) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [30/40] time 0.680 (0.581) data 0.469 (0.373) loss_x loss_x 0.8218 (1.0674) acc_x 75.0000 (73.1250) lr 3.1417e-05 eta 0:00:05
epoch [186/200] batch [35/40] time 0.512 (0.582) data 0.301 (0.373) loss_x loss_x 1.0918 (1.0688) acc_x 68.7500 (73.4821) lr 3.1417e-05 eta 0:00:02
epoch [186/200] batch [40/40] time 0.508 (0.582) data 0.293 (0.373) loss_x loss_x 1.6631 (1.0901) acc_x 62.5000 (72.8125) lr 3.1417e-05 eta 0:00:00
epoch [186/200] batch [5/57] time 0.529 (0.575) data 0.267 (0.364) loss_u loss_u 0.8477 (0.8609) acc_u 18.7500 (19.3750) lr 3.1417e-05 eta 0:00:29
epoch [186/200] batch [10/57] time 0.767 (0.571) data 0.552 (0.360) loss_u loss_u 0.8911 (0.8469) acc_u 12.5000 (19.3750) lr 3.1417e-05 eta 0:00:26
epoch [186/200] batch [15/57] time 0.454 (0.568) data 0.239 (0.356) loss_u loss_u 0.8379 (0.8532) acc_u 18.7500 (17.7083) lr 3.1417e-05 eta 0:00:23
epoch [186/200] batch [20/57] time 0.492 (0.564) data 0.276 (0.352) loss_u loss_u 0.8872 (0.8471) acc_u 12.5000 (18.7500) lr 3.1417e-05 eta 0:00:20
epoch [186/200] batch [25/57] time 0.495 (0.559) data 0.278 (0.347) loss_u loss_u 0.8252 (0.8457) acc_u 21.8750 (19.1250) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [30/57] time 0.475 (0.555) data 0.262 (0.344) loss_u loss_u 0.8418 (0.8468) acc_u 18.7500 (18.9583) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [35/57] time 0.504 (0.551) data 0.286 (0.339) loss_u loss_u 0.8730 (0.8494) acc_u 15.6250 (18.7500) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [40/57] time 0.436 (0.548) data 0.221 (0.337) loss_u loss_u 0.8716 (0.8512) acc_u 15.6250 (18.2812) lr 3.1417e-05 eta 0:00:09
epoch [186/200] batch [45/57] time 0.599 (0.549) data 0.383 (0.338) loss_u loss_u 0.8740 (0.8494) acc_u 9.3750 (18.1250) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [50/57] time 0.496 (0.550) data 0.286 (0.340) loss_u loss_u 0.8140 (0.8492) acc_u 21.8750 (18.1875) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [55/57] time 0.429 (0.547) data 0.226 (0.337) loss_u loss_u 0.9033 (0.8462) acc_u 12.5000 (18.6364) lr 3.1417e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1419
confident_label rate tensor(0.4168, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1307
clean true:1216
clean false:91
clean_rate:0.9303749043611323
noisy true:501
noisy false:1328
after delete: len(clean_dataset) 1307
after delete: len(noisy_dataset) 1829
epoch [187/200] batch [5/40] time 0.595 (0.565) data 0.383 (0.370) loss_x loss_x 0.8110 (1.0488) acc_x 78.1250 (71.2500) lr 2.7630e-05 eta 0:00:19
epoch [187/200] batch [10/40] time 0.646 (0.543) data 0.429 (0.339) loss_x loss_x 1.1084 (1.0541) acc_x 75.0000 (73.4375) lr 2.7630e-05 eta 0:00:16
epoch [187/200] batch [15/40] time 0.465 (0.526) data 0.251 (0.319) loss_x loss_x 1.2168 (1.0349) acc_x 65.6250 (73.9583) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [20/40] time 0.521 (0.528) data 0.304 (0.319) loss_x loss_x 0.9229 (1.0363) acc_x 68.7500 (73.5938) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [25/40] time 0.480 (0.533) data 0.314 (0.324) loss_x loss_x 1.1357 (1.0094) acc_x 81.2500 (74.3750) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [30/40] time 0.505 (0.529) data 0.295 (0.320) loss_x loss_x 1.6758 (1.0693) acc_x 65.6250 (73.1250) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [35/40] time 0.620 (0.541) data 0.402 (0.332) loss_x loss_x 0.5625 (1.0781) acc_x 78.1250 (72.8571) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [40/40] time 0.467 (0.546) data 0.258 (0.336) loss_x loss_x 0.5645 (1.0376) acc_x 84.3750 (74.1406) lr 2.7630e-05 eta 0:00:00
epoch [187/200] batch [5/57] time 0.601 (0.542) data 0.381 (0.332) loss_u loss_u 0.9316 (0.8920) acc_u 6.2500 (12.5000) lr 2.7630e-05 eta 0:00:28
epoch [187/200] batch [10/57] time 0.384 (0.544) data 0.169 (0.333) loss_u loss_u 0.7925 (0.8424) acc_u 25.0000 (18.7500) lr 2.7630e-05 eta 0:00:25
epoch [187/200] batch [15/57] time 0.514 (0.539) data 0.302 (0.329) loss_u loss_u 0.8066 (0.8352) acc_u 18.7500 (19.3750) lr 2.7630e-05 eta 0:00:22
epoch [187/200] batch [20/57] time 0.478 (0.539) data 0.259 (0.328) loss_u loss_u 0.8760 (0.8367) acc_u 15.6250 (19.8438) lr 2.7630e-05 eta 0:00:19
epoch [187/200] batch [25/57] time 0.457 (0.535) data 0.236 (0.325) loss_u loss_u 0.9121 (0.8375) acc_u 12.5000 (19.8750) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [30/57] time 0.519 (0.538) data 0.308 (0.327) loss_u loss_u 0.9146 (0.8346) acc_u 9.3750 (20.4167) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [35/57] time 0.627 (0.537) data 0.416 (0.326) loss_u loss_u 0.7520 (0.8354) acc_u 31.2500 (20.4464) lr 2.7630e-05 eta 0:00:11
epoch [187/200] batch [40/57] time 0.583 (0.538) data 0.366 (0.327) loss_u loss_u 0.7402 (0.8349) acc_u 31.2500 (20.3906) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [45/57] time 0.495 (0.534) data 0.284 (0.324) loss_u loss_u 0.9214 (0.8378) acc_u 9.3750 (20.0000) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [50/57] time 0.443 (0.533) data 0.232 (0.323) loss_u loss_u 0.8809 (0.8386) acc_u 12.5000 (19.7500) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [55/57] time 0.550 (0.533) data 0.339 (0.323) loss_u loss_u 0.8047 (0.8401) acc_u 31.2500 (19.6023) lr 2.7630e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1402
confident_label rate tensor(0.4174, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1309
clean true:1215
clean false:94
clean_rate:0.9281894576012223
noisy true:519
noisy false:1308
after delete: len(clean_dataset) 1309
after delete: len(noisy_dataset) 1827
epoch [188/200] batch [5/40] time 0.549 (0.601) data 0.334 (0.387) loss_x loss_x 1.0049 (0.9464) acc_x 81.2500 (81.2500) lr 2.4083e-05 eta 0:00:21
epoch [188/200] batch [10/40] time 0.514 (0.560) data 0.299 (0.355) loss_x loss_x 1.7773 (1.1332) acc_x 50.0000 (76.5625) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [15/40] time 0.588 (0.552) data 0.376 (0.344) loss_x loss_x 0.8369 (1.0791) acc_x 81.2500 (76.2500) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [20/40] time 0.486 (0.550) data 0.273 (0.345) loss_x loss_x 0.9878 (1.0571) acc_x 71.8750 (76.0938) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [25/40] time 0.588 (0.548) data 0.370 (0.341) loss_x loss_x 0.9565 (1.0427) acc_x 81.2500 (75.8750) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [30/40] time 0.428 (0.537) data 0.216 (0.329) loss_x loss_x 0.8467 (1.0491) acc_x 75.0000 (75.0000) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [35/40] time 0.592 (0.537) data 0.382 (0.331) loss_x loss_x 1.0684 (1.0554) acc_x 68.7500 (74.4643) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [40/40] time 0.648 (0.535) data 0.435 (0.327) loss_x loss_x 0.7749 (1.0373) acc_x 87.5000 (74.7656) lr 2.4083e-05 eta 0:00:00
epoch [188/200] batch [5/57] time 0.489 (0.541) data 0.279 (0.333) loss_u loss_u 0.8486 (0.8696) acc_u 18.7500 (19.3750) lr 2.4083e-05 eta 0:00:28
epoch [188/200] batch [10/57] time 0.495 (0.539) data 0.285 (0.330) loss_u loss_u 0.8989 (0.8721) acc_u 15.6250 (17.1875) lr 2.4083e-05 eta 0:00:25
epoch [188/200] batch [15/57] time 0.501 (0.536) data 0.288 (0.327) loss_u loss_u 0.8091 (0.8661) acc_u 21.8750 (16.8750) lr 2.4083e-05 eta 0:00:22
epoch [188/200] batch [20/57] time 0.606 (0.534) data 0.392 (0.326) loss_u loss_u 0.8159 (0.8555) acc_u 25.0000 (18.4375) lr 2.4083e-05 eta 0:00:19
epoch [188/200] batch [25/57] time 0.552 (0.535) data 0.341 (0.326) loss_u loss_u 0.7642 (0.8502) acc_u 31.2500 (18.2500) lr 2.4083e-05 eta 0:00:17
epoch [188/200] batch [30/57] time 0.571 (0.532) data 0.360 (0.324) loss_u loss_u 0.8442 (0.8492) acc_u 12.5000 (18.0208) lr 2.4083e-05 eta 0:00:14
epoch [188/200] batch [35/57] time 0.513 (0.535) data 0.302 (0.326) loss_u loss_u 0.7852 (0.8481) acc_u 28.1250 (18.3036) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [40/57] time 0.722 (0.541) data 0.503 (0.332) loss_u loss_u 0.8110 (0.8492) acc_u 15.6250 (18.2031) lr 2.4083e-05 eta 0:00:09
epoch [188/200] batch [45/57] time 0.615 (0.545) data 0.401 (0.336) loss_u loss_u 0.9131 (0.8521) acc_u 12.5000 (17.7778) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [50/57] time 0.496 (0.543) data 0.279 (0.334) loss_u loss_u 0.8550 (0.8530) acc_u 18.7500 (17.8750) lr 2.4083e-05 eta 0:00:03
epoch [188/200] batch [55/57] time 0.530 (0.542) data 0.313 (0.333) loss_u loss_u 0.8501 (0.8502) acc_u 18.7500 (18.2955) lr 2.4083e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1401
confident_label rate tensor(0.4085, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1281
clean true:1195
clean false:86
clean_rate:0.9328649492583919
noisy true:540
noisy false:1315
after delete: len(clean_dataset) 1281
after delete: len(noisy_dataset) 1855
epoch [189/200] batch [5/40] time 0.487 (0.527) data 0.274 (0.314) loss_x loss_x 1.0098 (1.1040) acc_x 81.2500 (78.1250) lr 2.0777e-05 eta 0:00:18
epoch [189/200] batch [10/40] time 0.511 (0.568) data 0.290 (0.355) loss_x loss_x 1.2637 (1.0813) acc_x 68.7500 (74.3750) lr 2.0777e-05 eta 0:00:17
epoch [189/200] batch [15/40] time 0.490 (0.554) data 0.279 (0.341) loss_x loss_x 1.2676 (1.0815) acc_x 81.2500 (75.0000) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [20/40] time 0.500 (0.548) data 0.291 (0.336) loss_x loss_x 0.8599 (1.0779) acc_x 75.0000 (75.0000) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [25/40] time 0.524 (0.545) data 0.311 (0.332) loss_x loss_x 0.9844 (1.0741) acc_x 71.8750 (74.6250) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [30/40] time 0.603 (0.550) data 0.392 (0.338) loss_x loss_x 1.2119 (1.0586) acc_x 75.0000 (75.3125) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [35/40] time 0.563 (0.568) data 0.355 (0.356) loss_x loss_x 1.2178 (1.0879) acc_x 75.0000 (74.4643) lr 2.0777e-05 eta 0:00:02
epoch [189/200] batch [40/40] time 0.472 (0.558) data 0.263 (0.346) loss_x loss_x 0.9331 (1.0699) acc_x 71.8750 (74.5312) lr 2.0777e-05 eta 0:00:00
epoch [189/200] batch [5/57] time 0.583 (0.561) data 0.373 (0.348) loss_u loss_u 0.8677 (0.8524) acc_u 12.5000 (18.7500) lr 2.0777e-05 eta 0:00:29
epoch [189/200] batch [10/57] time 0.497 (0.556) data 0.280 (0.343) loss_u loss_u 0.7676 (0.8483) acc_u 31.2500 (19.3750) lr 2.0777e-05 eta 0:00:26
epoch [189/200] batch [15/57] time 0.527 (0.554) data 0.314 (0.342) loss_u loss_u 0.7637 (0.8326) acc_u 34.3750 (21.4583) lr 2.0777e-05 eta 0:00:23
epoch [189/200] batch [20/57] time 0.635 (0.556) data 0.418 (0.343) loss_u loss_u 0.7686 (0.8321) acc_u 31.2500 (21.0938) lr 2.0777e-05 eta 0:00:20
epoch [189/200] batch [25/57] time 0.601 (0.558) data 0.388 (0.346) loss_u loss_u 0.7754 (0.8343) acc_u 28.1250 (20.7500) lr 2.0777e-05 eta 0:00:17
epoch [189/200] batch [30/57] time 0.439 (0.554) data 0.222 (0.343) loss_u loss_u 0.8950 (0.8340) acc_u 9.3750 (20.4167) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [35/57] time 0.641 (0.556) data 0.426 (0.344) loss_u loss_u 0.8369 (0.8368) acc_u 18.7500 (19.9107) lr 2.0777e-05 eta 0:00:12
epoch [189/200] batch [40/57] time 0.417 (0.553) data 0.201 (0.341) loss_u loss_u 0.8672 (0.8395) acc_u 21.8750 (19.6094) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [45/57] time 0.567 (0.556) data 0.349 (0.344) loss_u loss_u 0.8726 (0.8420) acc_u 15.6250 (19.3056) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [50/57] time 0.529 (0.552) data 0.313 (0.340) loss_u loss_u 0.8579 (0.8422) acc_u 21.8750 (19.5625) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [55/57] time 0.582 (0.551) data 0.369 (0.340) loss_u loss_u 0.8364 (0.8426) acc_u 18.7500 (19.4886) lr 2.0777e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1419
confident_label rate tensor(0.4069, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1276
clean true:1191
clean false:85
clean_rate:0.9333855799373041
noisy true:526
noisy false:1334
after delete: len(clean_dataset) 1276
after delete: len(noisy_dataset) 1860
epoch [190/200] batch [5/39] time 0.536 (0.555) data 0.326 (0.343) loss_x loss_x 0.9839 (1.0807) acc_x 81.2500 (76.2500) lr 1.7713e-05 eta 0:00:18
epoch [190/200] batch [10/39] time 0.545 (0.549) data 0.332 (0.336) loss_x loss_x 1.1885 (1.0147) acc_x 78.1250 (77.5000) lr 1.7713e-05 eta 0:00:15
epoch [190/200] batch [15/39] time 0.599 (0.553) data 0.387 (0.341) loss_x loss_x 0.8716 (0.9787) acc_x 78.1250 (78.3333) lr 1.7713e-05 eta 0:00:13
epoch [190/200] batch [20/39] time 0.528 (0.547) data 0.304 (0.337) loss_x loss_x 0.4065 (0.9931) acc_x 87.5000 (78.1250) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [25/39] time 0.742 (0.556) data 0.528 (0.346) loss_x loss_x 0.6938 (0.9845) acc_x 81.2500 (77.5000) lr 1.7713e-05 eta 0:00:07
epoch [190/200] batch [30/39] time 0.475 (0.547) data 0.260 (0.336) loss_x loss_x 1.7754 (1.0205) acc_x 59.3750 (75.8333) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [35/39] time 0.587 (0.546) data 0.376 (0.335) loss_x loss_x 0.6914 (0.9937) acc_x 78.1250 (75.9821) lr 1.7713e-05 eta 0:00:02
epoch [190/200] batch [5/58] time 0.638 (0.537) data 0.425 (0.327) loss_u loss_u 0.7700 (0.8539) acc_u 34.3750 (18.7500) lr 1.7713e-05 eta 0:00:28
epoch [190/200] batch [10/58] time 0.391 (0.535) data 0.176 (0.325) loss_u loss_u 0.8516 (0.8499) acc_u 15.6250 (19.3750) lr 1.7713e-05 eta 0:00:25
epoch [190/200] batch [15/58] time 0.403 (0.529) data 0.188 (0.317) loss_u loss_u 0.8291 (0.8487) acc_u 21.8750 (19.1667) lr 1.7713e-05 eta 0:00:22
epoch [190/200] batch [20/58] time 0.639 (0.531) data 0.426 (0.319) loss_u loss_u 0.7944 (0.8361) acc_u 21.8750 (19.8438) lr 1.7713e-05 eta 0:00:20
epoch [190/200] batch [25/58] time 0.650 (0.531) data 0.435 (0.318) loss_u loss_u 0.8174 (0.8440) acc_u 25.0000 (18.8750) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [30/58] time 0.520 (0.527) data 0.307 (0.315) loss_u loss_u 0.7417 (0.8449) acc_u 28.1250 (18.4375) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [35/58] time 0.407 (0.527) data 0.188 (0.315) loss_u loss_u 0.8989 (0.8452) acc_u 9.3750 (18.6607) lr 1.7713e-05 eta 0:00:12
epoch [190/200] batch [40/58] time 0.596 (0.528) data 0.381 (0.315) loss_u loss_u 0.8701 (0.8450) acc_u 12.5000 (18.5156) lr 1.7713e-05 eta 0:00:09
epoch [190/200] batch [45/58] time 0.632 (0.530) data 0.422 (0.318) loss_u loss_u 0.8442 (0.8452) acc_u 18.7500 (18.4028) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [50/58] time 0.492 (0.535) data 0.274 (0.323) loss_u loss_u 0.9019 (0.8465) acc_u 12.5000 (18.1875) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [55/58] time 0.635 (0.534) data 0.419 (0.322) loss_u loss_u 0.8457 (0.8427) acc_u 15.6250 (18.7500) lr 1.7713e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1391
confident_label rate tensor(0.4145, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1300
clean true:1208
clean false:92
clean_rate:0.9292307692307692
noisy true:537
noisy false:1299
after delete: len(clean_dataset) 1300
after delete: len(noisy_dataset) 1836
epoch [191/200] batch [5/40] time 0.469 (0.559) data 0.257 (0.345) loss_x loss_x 0.6255 (0.8941) acc_x 78.1250 (75.6250) lr 1.4891e-05 eta 0:00:19
epoch [191/200] batch [10/40] time 0.525 (0.552) data 0.311 (0.345) loss_x loss_x 0.8979 (1.0573) acc_x 68.7500 (72.5000) lr 1.4891e-05 eta 0:00:16
epoch [191/200] batch [15/40] time 0.547 (0.535) data 0.329 (0.326) loss_x loss_x 1.3701 (1.0534) acc_x 68.7500 (72.2917) lr 1.4891e-05 eta 0:00:13
epoch [191/200] batch [20/40] time 0.549 (0.546) data 0.337 (0.340) loss_x loss_x 1.1650 (1.1100) acc_x 68.7500 (71.2500) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [25/40] time 0.554 (0.544) data 0.340 (0.337) loss_x loss_x 1.3262 (1.1286) acc_x 65.6250 (70.5000) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [30/40] time 0.431 (0.540) data 0.219 (0.331) loss_x loss_x 1.0156 (1.0887) acc_x 87.5000 (72.0833) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [35/40] time 0.462 (0.533) data 0.251 (0.324) loss_x loss_x 0.7104 (1.0679) acc_x 84.3750 (72.7679) lr 1.4891e-05 eta 0:00:02
epoch [191/200] batch [40/40] time 0.571 (0.531) data 0.352 (0.320) loss_x loss_x 1.0156 (1.0821) acc_x 78.1250 (72.6562) lr 1.4891e-05 eta 0:00:00
epoch [191/200] batch [5/57] time 0.480 (0.527) data 0.264 (0.317) loss_u loss_u 0.8486 (0.8270) acc_u 15.6250 (20.0000) lr 1.4891e-05 eta 0:00:27
epoch [191/200] batch [10/57] time 0.414 (0.526) data 0.200 (0.316) loss_u loss_u 0.9058 (0.8334) acc_u 12.5000 (18.7500) lr 1.4891e-05 eta 0:00:24
epoch [191/200] batch [15/57] time 0.502 (0.522) data 0.287 (0.312) loss_u loss_u 0.8979 (0.8412) acc_u 9.3750 (17.7083) lr 1.4891e-05 eta 0:00:21
epoch [191/200] batch [20/57] time 0.512 (0.524) data 0.346 (0.314) loss_u loss_u 0.9253 (0.8500) acc_u 9.3750 (17.0312) lr 1.4891e-05 eta 0:00:19
epoch [191/200] batch [25/57] time 0.636 (0.525) data 0.424 (0.315) loss_u loss_u 0.8208 (0.8466) acc_u 25.0000 (18.0000) lr 1.4891e-05 eta 0:00:16
epoch [191/200] batch [30/57] time 0.478 (0.521) data 0.266 (0.311) loss_u loss_u 0.7549 (0.8427) acc_u 31.2500 (18.4375) lr 1.4891e-05 eta 0:00:14
epoch [191/200] batch [35/57] time 0.579 (0.521) data 0.365 (0.310) loss_u loss_u 0.8164 (0.8446) acc_u 18.7500 (18.2143) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [40/57] time 0.630 (0.526) data 0.370 (0.315) loss_u loss_u 0.8550 (0.8440) acc_u 18.7500 (18.3594) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [45/57] time 0.538 (0.523) data 0.325 (0.312) loss_u loss_u 0.8013 (0.8377) acc_u 31.2500 (19.5139) lr 1.4891e-05 eta 0:00:06
epoch [191/200] batch [50/57] time 0.557 (0.522) data 0.344 (0.311) loss_u loss_u 0.8843 (0.8411) acc_u 12.5000 (19.1875) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [55/57] time 0.584 (0.523) data 0.371 (0.311) loss_u loss_u 0.7891 (0.8431) acc_u 25.0000 (19.0341) lr 1.4891e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1411
confident_label rate tensor(0.4114, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1290
clean true:1202
clean false:88
clean_rate:0.931782945736434
noisy true:523
noisy false:1323
after delete: len(clean_dataset) 1290
after delete: len(noisy_dataset) 1846
epoch [192/200] batch [5/40] time 0.561 (0.506) data 0.350 (0.292) loss_x loss_x 1.2002 (1.0235) acc_x 62.5000 (72.5000) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [10/40] time 0.454 (0.510) data 0.243 (0.297) loss_x loss_x 1.3926 (1.0983) acc_x 71.8750 (71.2500) lr 1.2312e-05 eta 0:00:15
epoch [192/200] batch [15/40] time 0.494 (0.508) data 0.281 (0.295) loss_x loss_x 1.0381 (1.0329) acc_x 75.0000 (73.3333) lr 1.2312e-05 eta 0:00:12
epoch [192/200] batch [20/40] time 0.734 (0.521) data 0.521 (0.307) loss_x loss_x 1.3535 (1.0718) acc_x 71.8750 (73.1250) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [25/40] time 0.447 (0.526) data 0.234 (0.312) loss_x loss_x 1.4414 (1.0590) acc_x 65.6250 (74.3750) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [30/40] time 0.581 (0.523) data 0.365 (0.312) loss_x loss_x 0.8096 (1.0381) acc_x 78.1250 (74.5833) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [35/40] time 0.655 (0.529) data 0.423 (0.318) loss_x loss_x 1.0557 (1.0140) acc_x 81.2500 (75.4464) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [40/40] time 0.609 (0.530) data 0.393 (0.319) loss_x loss_x 0.7173 (1.0095) acc_x 84.3750 (75.8594) lr 1.2312e-05 eta 0:00:00
epoch [192/200] batch [5/57] time 0.495 (0.528) data 0.285 (0.316) loss_u loss_u 0.7788 (0.8589) acc_u 25.0000 (17.5000) lr 1.2312e-05 eta 0:00:27
epoch [192/200] batch [10/57] time 0.471 (0.526) data 0.259 (0.314) loss_u loss_u 0.9292 (0.8653) acc_u 3.1250 (15.0000) lr 1.2312e-05 eta 0:00:24
epoch [192/200] batch [15/57] time 0.570 (0.530) data 0.360 (0.318) loss_u loss_u 0.8213 (0.8635) acc_u 18.7500 (15.6250) lr 1.2312e-05 eta 0:00:22
epoch [192/200] batch [20/57] time 0.499 (0.527) data 0.288 (0.315) loss_u loss_u 0.8472 (0.8597) acc_u 18.7500 (16.2500) lr 1.2312e-05 eta 0:00:19
epoch [192/200] batch [25/57] time 0.515 (0.527) data 0.303 (0.316) loss_u loss_u 0.8735 (0.8610) acc_u 12.5000 (15.7500) lr 1.2312e-05 eta 0:00:16
epoch [192/200] batch [30/57] time 0.605 (0.526) data 0.395 (0.315) loss_u loss_u 0.7520 (0.8530) acc_u 25.0000 (16.7708) lr 1.2312e-05 eta 0:00:14
epoch [192/200] batch [35/57] time 0.535 (0.524) data 0.323 (0.313) loss_u loss_u 0.7881 (0.8482) acc_u 25.0000 (17.7679) lr 1.2312e-05 eta 0:00:11
epoch [192/200] batch [40/57] time 0.502 (0.523) data 0.292 (0.312) loss_u loss_u 0.8521 (0.8492) acc_u 21.8750 (17.8906) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [45/57] time 0.481 (0.520) data 0.271 (0.310) loss_u loss_u 0.8105 (0.8460) acc_u 21.8750 (18.1250) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [50/57] time 0.541 (0.521) data 0.331 (0.312) loss_u loss_u 0.8184 (0.8469) acc_u 21.8750 (17.8750) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [55/57] time 0.532 (0.523) data 0.318 (0.313) loss_u loss_u 0.8315 (0.8460) acc_u 21.8750 (18.1818) lr 1.2312e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1408
confident_label rate tensor(0.4142, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1299
clean true:1211
clean false:88
clean_rate:0.9322555812163202
noisy true:517
noisy false:1320
after delete: len(clean_dataset) 1299
after delete: len(noisy_dataset) 1837
epoch [193/200] batch [5/40] time 0.691 (0.545) data 0.478 (0.333) loss_x loss_x 1.1826 (1.1776) acc_x 78.1250 (73.7500) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [10/40] time 0.490 (0.517) data 0.278 (0.305) loss_x loss_x 0.9878 (1.1567) acc_x 68.7500 (73.7500) lr 9.9763e-06 eta 0:00:15
epoch [193/200] batch [15/40] time 0.500 (0.522) data 0.291 (0.316) loss_x loss_x 1.5186 (1.1757) acc_x 59.3750 (72.9167) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [20/40] time 0.524 (0.528) data 0.312 (0.320) loss_x loss_x 0.7271 (1.1339) acc_x 81.2500 (73.4375) lr 9.9763e-06 eta 0:00:10
epoch [193/200] batch [25/40] time 0.533 (0.522) data 0.320 (0.314) loss_x loss_x 1.0811 (1.1105) acc_x 71.8750 (73.5000) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [30/40] time 0.572 (0.526) data 0.358 (0.317) loss_x loss_x 0.6704 (1.0594) acc_x 87.5000 (74.2708) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [35/40] time 0.636 (0.529) data 0.423 (0.320) loss_x loss_x 0.6636 (1.0174) acc_x 84.3750 (75.2679) lr 9.9763e-06 eta 0:00:02
epoch [193/200] batch [40/40] time 0.603 (0.534) data 0.394 (0.326) loss_x loss_x 1.0342 (1.0386) acc_x 81.2500 (75.0000) lr 9.9763e-06 eta 0:00:00
epoch [193/200] batch [5/57] time 0.569 (0.528) data 0.358 (0.320) loss_u loss_u 0.9316 (0.8409) acc_u 9.3750 (22.5000) lr 9.9763e-06 eta 0:00:27
epoch [193/200] batch [10/57] time 0.564 (0.529) data 0.349 (0.321) loss_u loss_u 0.8623 (0.8282) acc_u 18.7500 (22.5000) lr 9.9763e-06 eta 0:00:24
epoch [193/200] batch [15/57] time 0.465 (0.529) data 0.254 (0.320) loss_u loss_u 0.8887 (0.8432) acc_u 12.5000 (19.5833) lr 9.9763e-06 eta 0:00:22
epoch [193/200] batch [20/57] time 0.502 (0.524) data 0.290 (0.314) loss_u loss_u 0.8276 (0.8408) acc_u 15.6250 (19.3750) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [25/57] time 0.478 (0.522) data 0.264 (0.311) loss_u loss_u 0.8999 (0.8450) acc_u 12.5000 (18.7500) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [30/57] time 0.472 (0.524) data 0.259 (0.313) loss_u loss_u 0.8828 (0.8491) acc_u 12.5000 (18.0208) lr 9.9763e-06 eta 0:00:14
epoch [193/200] batch [35/57] time 0.596 (0.526) data 0.378 (0.316) loss_u loss_u 0.8560 (0.8493) acc_u 18.7500 (18.1250) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [40/57] time 0.495 (0.524) data 0.282 (0.314) loss_u loss_u 0.9058 (0.8500) acc_u 12.5000 (18.2812) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [45/57] time 0.463 (0.522) data 0.250 (0.312) loss_u loss_u 0.8755 (0.8476) acc_u 15.6250 (18.5417) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [50/57] time 0.486 (0.521) data 0.272 (0.311) loss_u loss_u 0.7573 (0.8446) acc_u 28.1250 (18.8750) lr 9.9763e-06 eta 0:00:03
epoch [193/200] batch [55/57] time 0.607 (0.521) data 0.390 (0.311) loss_u loss_u 0.8086 (0.8442) acc_u 21.8750 (18.9205) lr 9.9763e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1368
confident_label rate tensor(0.4136, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1297
clean true:1216
clean false:81
clean_rate:0.9375481881264457
noisy true:552
noisy false:1287
after delete: len(clean_dataset) 1297
after delete: len(noisy_dataset) 1839
epoch [194/200] batch [5/40] time 0.581 (0.543) data 0.371 (0.332) loss_x loss_x 1.6631 (1.1898) acc_x 59.3750 (71.8750) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [10/40] time 0.542 (0.526) data 0.330 (0.315) loss_x loss_x 0.7920 (1.0489) acc_x 71.8750 (72.5000) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [15/40] time 0.454 (0.518) data 0.243 (0.310) loss_x loss_x 1.3340 (1.0529) acc_x 65.6250 (72.2917) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [20/40] time 0.549 (0.516) data 0.341 (0.307) loss_x loss_x 1.3975 (1.0713) acc_x 75.0000 (72.5000) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [25/40] time 0.460 (0.525) data 0.248 (0.316) loss_x loss_x 0.9297 (1.0545) acc_x 68.7500 (73.0000) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [30/40] time 0.355 (0.523) data 0.225 (0.317) loss_x loss_x 1.3691 (1.0680) acc_x 65.6250 (71.9792) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [35/40] time 0.628 (0.530) data 0.418 (0.323) loss_x loss_x 1.0391 (1.0695) acc_x 75.0000 (72.4107) lr 7.8853e-06 eta 0:00:02
epoch [194/200] batch [40/40] time 0.677 (0.528) data 0.461 (0.321) loss_x loss_x 1.1865 (1.0444) acc_x 65.6250 (72.7344) lr 7.8853e-06 eta 0:00:00
epoch [194/200] batch [5/57] time 0.629 (0.528) data 0.418 (0.319) loss_u loss_u 0.8857 (0.8377) acc_u 12.5000 (17.5000) lr 7.8853e-06 eta 0:00:27
epoch [194/200] batch [10/57] time 0.467 (0.522) data 0.252 (0.315) loss_u loss_u 0.8359 (0.8287) acc_u 25.0000 (20.0000) lr 7.8853e-06 eta 0:00:24
epoch [194/200] batch [15/57] time 0.475 (0.523) data 0.257 (0.315) loss_u loss_u 0.8184 (0.8381) acc_u 21.8750 (18.3333) lr 7.8853e-06 eta 0:00:21
epoch [194/200] batch [20/57] time 0.575 (0.522) data 0.361 (0.314) loss_u loss_u 0.8579 (0.8435) acc_u 15.6250 (18.4375) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [25/57] time 0.530 (0.523) data 0.316 (0.315) loss_u loss_u 0.9023 (0.8473) acc_u 9.3750 (18.2500) lr 7.8853e-06 eta 0:00:16
epoch [194/200] batch [30/57] time 0.446 (0.523) data 0.231 (0.314) loss_u loss_u 0.8223 (0.8451) acc_u 21.8750 (18.5417) lr 7.8853e-06 eta 0:00:14
epoch [194/200] batch [35/57] time 0.426 (0.522) data 0.212 (0.312) loss_u loss_u 0.7983 (0.8414) acc_u 21.8750 (18.7500) lr 7.8853e-06 eta 0:00:11
epoch [194/200] batch [40/57] time 0.398 (0.519) data 0.266 (0.311) loss_u loss_u 0.8442 (0.8426) acc_u 18.7500 (18.8281) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [45/57] time 0.531 (0.519) data 0.318 (0.310) loss_u loss_u 0.8027 (0.8435) acc_u 25.0000 (18.8889) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [50/57] time 0.528 (0.519) data 0.310 (0.310) loss_u loss_u 0.8652 (0.8430) acc_u 15.6250 (19.0625) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [55/57] time 0.537 (0.515) data 0.322 (0.307) loss_u loss_u 0.8438 (0.8416) acc_u 25.0000 (19.3182) lr 7.8853e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1404
confident_label rate tensor(0.4059, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1273
clean true:1180
clean false:93
clean_rate:0.9269442262372348
noisy true:552
noisy false:1311
after delete: len(clean_dataset) 1273
after delete: len(noisy_dataset) 1863
epoch [195/200] batch [5/39] time 0.543 (0.552) data 0.333 (0.338) loss_x loss_x 0.9888 (0.8368) acc_x 68.7500 (77.5000) lr 6.0390e-06 eta 0:00:18
epoch [195/200] batch [10/39] time 0.575 (0.542) data 0.364 (0.329) loss_x loss_x 1.3037 (0.9777) acc_x 62.5000 (73.1250) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [15/39] time 0.510 (0.534) data 0.297 (0.321) loss_x loss_x 1.2686 (1.0304) acc_x 68.7500 (72.5000) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [20/39] time 0.420 (0.525) data 0.289 (0.317) loss_x loss_x 0.9551 (1.0375) acc_x 75.0000 (72.6562) lr 6.0390e-06 eta 0:00:09
epoch [195/200] batch [25/39] time 0.534 (0.526) data 0.321 (0.317) loss_x loss_x 1.1182 (0.9921) acc_x 75.0000 (74.3750) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [30/39] time 0.510 (0.521) data 0.297 (0.311) loss_x loss_x 1.0635 (0.9896) acc_x 68.7500 (75.0000) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [35/39] time 0.590 (0.516) data 0.375 (0.308) loss_x loss_x 1.0654 (1.0147) acc_x 78.1250 (74.6429) lr 6.0390e-06 eta 0:00:02
epoch [195/200] batch [5/58] time 0.496 (0.517) data 0.278 (0.308) loss_u loss_u 0.9033 (0.8373) acc_u 12.5000 (20.6250) lr 6.0390e-06 eta 0:00:27
epoch [195/200] batch [10/58] time 0.473 (0.515) data 0.258 (0.306) loss_u loss_u 0.8438 (0.8441) acc_u 21.8750 (19.6875) lr 6.0390e-06 eta 0:00:24
epoch [195/200] batch [15/58] time 0.453 (0.512) data 0.238 (0.302) loss_u loss_u 0.8936 (0.8464) acc_u 15.6250 (19.3750) lr 6.0390e-06 eta 0:00:22
epoch [195/200] batch [20/58] time 0.505 (0.509) data 0.293 (0.301) loss_u loss_u 0.7935 (0.8457) acc_u 31.2500 (19.5312) lr 6.0390e-06 eta 0:00:19
epoch [195/200] batch [25/58] time 0.511 (0.510) data 0.298 (0.301) loss_u loss_u 0.9116 (0.8505) acc_u 9.3750 (18.8750) lr 6.0390e-06 eta 0:00:16
epoch [195/200] batch [30/58] time 0.403 (0.509) data 0.233 (0.301) loss_u loss_u 0.8647 (0.8457) acc_u 12.5000 (19.3750) lr 6.0390e-06 eta 0:00:14
epoch [195/200] batch [35/58] time 0.446 (0.511) data 0.235 (0.302) loss_u loss_u 0.8081 (0.8410) acc_u 21.8750 (20.2679) lr 6.0390e-06 eta 0:00:11
epoch [195/200] batch [40/58] time 0.652 (0.515) data 0.439 (0.306) loss_u loss_u 0.8833 (0.8403) acc_u 12.5000 (20.2344) lr 6.0390e-06 eta 0:00:09
epoch [195/200] batch [45/58] time 0.510 (0.519) data 0.299 (0.311) loss_u loss_u 0.8799 (0.8412) acc_u 18.7500 (20.1389) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [50/58] time 0.444 (0.517) data 0.231 (0.308) loss_u loss_u 0.8994 (0.8436) acc_u 15.6250 (19.8750) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [55/58] time 0.497 (0.517) data 0.286 (0.310) loss_u loss_u 0.8398 (0.8445) acc_u 18.7500 (19.8295) lr 6.0390e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1463
confident_label rate tensor(0.3957, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1241
clean true:1153
clean false:88
clean_rate:0.9290894439967768
noisy true:520
noisy false:1375
after delete: len(clean_dataset) 1241
after delete: len(noisy_dataset) 1895
epoch [196/200] batch [5/38] time 0.519 (0.584) data 0.308 (0.373) loss_x loss_x 1.2676 (1.1216) acc_x 68.7500 (71.8750) lr 4.4380e-06 eta 0:00:19
epoch [196/200] batch [10/38] time 0.524 (0.554) data 0.314 (0.344) loss_x loss_x 1.0713 (1.1418) acc_x 68.7500 (71.5625) lr 4.4380e-06 eta 0:00:15
epoch [196/200] batch [15/38] time 0.456 (0.540) data 0.267 (0.331) loss_x loss_x 1.0986 (1.0995) acc_x 68.7500 (72.0833) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [20/38] time 0.691 (0.556) data 0.476 (0.346) loss_x loss_x 1.2314 (1.0875) acc_x 62.5000 (70.9375) lr 4.4380e-06 eta 0:00:10
epoch [196/200] batch [25/38] time 0.476 (0.552) data 0.265 (0.340) loss_x loss_x 1.3662 (1.0969) acc_x 68.7500 (72.2500) lr 4.4380e-06 eta 0:00:07
epoch [196/200] batch [30/38] time 0.699 (0.549) data 0.483 (0.337) loss_x loss_x 1.1855 (1.1064) acc_x 65.6250 (72.2917) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [35/38] time 0.492 (0.549) data 0.279 (0.337) loss_x loss_x 1.2422 (1.1024) acc_x 65.6250 (72.4107) lr 4.4380e-06 eta 0:00:01
epoch [196/200] batch [5/59] time 0.592 (0.543) data 0.379 (0.330) loss_u loss_u 0.7720 (0.7867) acc_u 34.3750 (27.5000) lr 4.4380e-06 eta 0:00:29
epoch [196/200] batch [10/59] time 0.589 (0.547) data 0.373 (0.334) loss_u loss_u 0.7861 (0.8133) acc_u 28.1250 (24.0625) lr 4.4380e-06 eta 0:00:26
epoch [196/200] batch [15/59] time 0.513 (0.543) data 0.301 (0.330) loss_u loss_u 0.7983 (0.8131) acc_u 31.2500 (23.9583) lr 4.4380e-06 eta 0:00:23
epoch [196/200] batch [20/59] time 0.498 (0.542) data 0.287 (0.329) loss_u loss_u 0.7573 (0.8179) acc_u 28.1250 (22.9688) lr 4.4380e-06 eta 0:00:21
epoch [196/200] batch [25/59] time 0.506 (0.538) data 0.294 (0.325) loss_u loss_u 0.9326 (0.8261) acc_u 9.3750 (21.8750) lr 4.4380e-06 eta 0:00:18
epoch [196/200] batch [30/59] time 0.515 (0.534) data 0.302 (0.322) loss_u loss_u 0.8486 (0.8303) acc_u 15.6250 (21.2500) lr 4.4380e-06 eta 0:00:15
epoch [196/200] batch [35/59] time 0.541 (0.531) data 0.329 (0.318) loss_u loss_u 0.7764 (0.8335) acc_u 28.1250 (20.9821) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [40/59] time 0.528 (0.534) data 0.315 (0.320) loss_u loss_u 0.7095 (0.8282) acc_u 31.2500 (21.5625) lr 4.4380e-06 eta 0:00:10
epoch [196/200] batch [45/59] time 0.583 (0.538) data 0.366 (0.324) loss_u loss_u 0.8516 (0.8288) acc_u 15.6250 (21.3194) lr 4.4380e-06 eta 0:00:07
epoch [196/200] batch [50/59] time 0.454 (0.536) data 0.243 (0.322) loss_u loss_u 0.8604 (0.8316) acc_u 12.5000 (20.6250) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [55/59] time 0.463 (0.530) data 0.248 (0.318) loss_u loss_u 0.8774 (0.8320) acc_u 15.6250 (20.5682) lr 4.4380e-06 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1350
confident_label rate tensor(0.4235, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1328
clean true:1239
clean false:89
clean_rate:0.9329819277108434
noisy true:547
noisy false:1261
after delete: len(clean_dataset) 1328
after delete: len(noisy_dataset) 1808
epoch [197/200] batch [5/41] time 0.447 (0.504) data 0.238 (0.288) loss_x loss_x 1.0537 (1.2066) acc_x 75.0000 (69.3750) lr 3.0827e-06 eta 0:00:18
epoch [197/200] batch [10/41] time 0.409 (0.535) data 0.198 (0.321) loss_x loss_x 1.2061 (1.1121) acc_x 71.8750 (72.5000) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [15/41] time 0.499 (0.523) data 0.284 (0.312) loss_x loss_x 1.0479 (1.0787) acc_x 75.0000 (74.3750) lr 3.0827e-06 eta 0:00:13
epoch [197/200] batch [20/41] time 0.565 (0.532) data 0.356 (0.321) loss_x loss_x 0.9253 (1.0235) acc_x 78.1250 (75.1562) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [25/41] time 0.471 (0.533) data 0.259 (0.324) loss_x loss_x 0.9575 (1.0326) acc_x 78.1250 (74.2500) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [30/41] time 0.533 (0.526) data 0.321 (0.318) loss_x loss_x 0.5234 (1.0255) acc_x 87.5000 (74.3750) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [35/41] time 0.415 (0.525) data 0.202 (0.316) loss_x loss_x 1.1631 (1.0569) acc_x 78.1250 (73.5714) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [40/41] time 0.576 (0.528) data 0.365 (0.321) loss_x loss_x 1.3652 (1.0435) acc_x 68.7500 (74.4531) lr 3.0827e-06 eta 0:00:00
epoch [197/200] batch [5/56] time 0.522 (0.527) data 0.307 (0.319) loss_u loss_u 0.9062 (0.8572) acc_u 9.3750 (17.5000) lr 3.0827e-06 eta 0:00:26
epoch [197/200] batch [10/56] time 0.489 (0.523) data 0.277 (0.315) loss_u loss_u 0.9316 (0.8638) acc_u 9.3750 (17.1875) lr 3.0827e-06 eta 0:00:24
epoch [197/200] batch [15/56] time 0.631 (0.521) data 0.420 (0.312) loss_u loss_u 0.8257 (0.8547) acc_u 18.7500 (18.1250) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [20/56] time 0.452 (0.518) data 0.240 (0.309) loss_u loss_u 0.8184 (0.8501) acc_u 25.0000 (19.0625) lr 3.0827e-06 eta 0:00:18
epoch [197/200] batch [25/56] time 0.551 (0.516) data 0.337 (0.307) loss_u loss_u 0.7686 (0.8474) acc_u 28.1250 (19.2500) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [30/56] time 0.529 (0.518) data 0.314 (0.308) loss_u loss_u 0.8584 (0.8445) acc_u 15.6250 (19.4792) lr 3.0827e-06 eta 0:00:13
epoch [197/200] batch [35/56] time 0.545 (0.515) data 0.333 (0.306) loss_u loss_u 0.7456 (0.8396) acc_u 28.1250 (19.7321) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [40/56] time 0.404 (0.514) data 0.194 (0.305) loss_u loss_u 0.8755 (0.8399) acc_u 15.6250 (19.6094) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [45/56] time 0.430 (0.513) data 0.216 (0.304) loss_u loss_u 0.8472 (0.8396) acc_u 15.6250 (19.3750) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [50/56] time 0.527 (0.511) data 0.316 (0.302) loss_u loss_u 0.7798 (0.8409) acc_u 25.0000 (19.3125) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [55/56] time 0.489 (0.513) data 0.274 (0.304) loss_u loss_u 0.9243 (0.8452) acc_u 9.3750 (18.9773) lr 3.0827e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1397
confident_label rate tensor(0.4098, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1285
clean true:1198
clean false:87
clean_rate:0.932295719844358
noisy true:541
noisy false:1310
after delete: len(clean_dataset) 1285
after delete: len(noisy_dataset) 1851
epoch [198/200] batch [5/40] time 0.568 (0.553) data 0.354 (0.342) loss_x loss_x 0.6235 (0.8910) acc_x 75.0000 (76.8750) lr 1.9733e-06 eta 0:00:19
epoch [198/200] batch [10/40] time 0.412 (0.510) data 0.204 (0.299) loss_x loss_x 0.7593 (0.8867) acc_x 81.2500 (76.2500) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [15/40] time 0.409 (0.499) data 0.272 (0.292) loss_x loss_x 1.3018 (0.9305) acc_x 71.8750 (76.4583) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [20/40] time 0.507 (0.496) data 0.293 (0.288) loss_x loss_x 0.9062 (0.9743) acc_x 78.1250 (75.4688) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [25/40] time 0.621 (0.497) data 0.364 (0.286) loss_x loss_x 1.2744 (0.9873) acc_x 75.0000 (75.2500) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [30/40] time 0.538 (0.507) data 0.328 (0.296) loss_x loss_x 0.8120 (0.9699) acc_x 78.1250 (75.7292) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [35/40] time 0.514 (0.507) data 0.302 (0.296) loss_x loss_x 1.5498 (0.9752) acc_x 56.2500 (75.4464) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [40/40] time 0.486 (0.509) data 0.272 (0.297) loss_x loss_x 1.1670 (0.9820) acc_x 75.0000 (76.0938) lr 1.9733e-06 eta 0:00:00
epoch [198/200] batch [5/57] time 0.467 (0.511) data 0.251 (0.299) loss_u loss_u 0.7593 (0.8026) acc_u 28.1250 (24.3750) lr 1.9733e-06 eta 0:00:26
epoch [198/200] batch [10/57] time 0.552 (0.508) data 0.336 (0.296) loss_u loss_u 0.7983 (0.8184) acc_u 25.0000 (22.1875) lr 1.9733e-06 eta 0:00:23
epoch [198/200] batch [15/57] time 0.336 (0.503) data 0.205 (0.292) loss_u loss_u 0.8628 (0.8231) acc_u 15.6250 (21.8750) lr 1.9733e-06 eta 0:00:21
epoch [198/200] batch [20/57] time 0.434 (0.503) data 0.221 (0.292) loss_u loss_u 0.7759 (0.8229) acc_u 25.0000 (21.5625) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [25/57] time 0.485 (0.504) data 0.273 (0.293) loss_u loss_u 0.8228 (0.8253) acc_u 21.8750 (21.3750) lr 1.9733e-06 eta 0:00:16
epoch [198/200] batch [30/57] time 0.489 (0.506) data 0.278 (0.294) loss_u loss_u 0.9243 (0.8351) acc_u 9.3750 (20.6250) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [35/57] time 0.548 (0.510) data 0.337 (0.298) loss_u loss_u 0.8691 (0.8366) acc_u 12.5000 (20.6250) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [40/57] time 0.766 (0.515) data 0.553 (0.303) loss_u loss_u 0.8125 (0.8391) acc_u 28.1250 (20.2344) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [45/57] time 0.381 (0.512) data 0.166 (0.301) loss_u loss_u 0.8638 (0.8391) acc_u 18.7500 (20.2778) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [50/57] time 0.508 (0.512) data 0.291 (0.302) loss_u loss_u 0.7915 (0.8356) acc_u 21.8750 (20.6875) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [55/57] time 0.479 (0.510) data 0.268 (0.299) loss_u loss_u 0.7778 (0.8345) acc_u 25.0000 (20.6818) lr 1.9733e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1399
confident_label rate tensor(0.4139, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1298
clean true:1205
clean false:93
clean_rate:0.9283513097072419
noisy true:532
noisy false:1306
after delete: len(clean_dataset) 1298
after delete: len(noisy_dataset) 1838
epoch [199/200] batch [5/40] time 0.498 (0.476) data 0.287 (0.281) loss_x loss_x 1.4609 (1.3114) acc_x 75.0000 (75.6250) lr 1.1101e-06 eta 0:00:16
epoch [199/200] batch [10/40] time 0.500 (0.491) data 0.290 (0.288) loss_x loss_x 1.1445 (1.2374) acc_x 65.6250 (73.1250) lr 1.1101e-06 eta 0:00:14
epoch [199/200] batch [15/40] time 0.464 (0.506) data 0.253 (0.300) loss_x loss_x 0.7725 (1.1492) acc_x 81.2500 (74.5833) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [20/40] time 0.526 (0.529) data 0.318 (0.323) loss_x loss_x 0.8320 (1.1070) acc_x 81.2500 (73.9062) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [25/40] time 0.451 (0.525) data 0.239 (0.316) loss_x loss_x 0.8262 (1.0925) acc_x 78.1250 (73.3750) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [30/40] time 0.717 (0.532) data 0.507 (0.323) loss_x loss_x 0.9478 (1.0531) acc_x 81.2500 (74.1667) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [35/40] time 0.462 (0.528) data 0.251 (0.318) loss_x loss_x 1.4756 (1.0536) acc_x 59.3750 (73.5714) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [40/40] time 0.556 (0.521) data 0.347 (0.312) loss_x loss_x 0.9424 (1.0500) acc_x 71.8750 (73.4375) lr 1.1101e-06 eta 0:00:00
epoch [199/200] batch [5/57] time 0.424 (0.518) data 0.215 (0.309) loss_u loss_u 0.8765 (0.8494) acc_u 18.7500 (19.3750) lr 1.1101e-06 eta 0:00:26
epoch [199/200] batch [10/57] time 0.444 (0.518) data 0.230 (0.308) loss_u loss_u 0.8506 (0.8523) acc_u 21.8750 (19.0625) lr 1.1101e-06 eta 0:00:24
epoch [199/200] batch [15/57] time 0.607 (0.518) data 0.396 (0.308) loss_u loss_u 0.8916 (0.8586) acc_u 9.3750 (17.9167) lr 1.1101e-06 eta 0:00:21
epoch [199/200] batch [20/57] time 0.479 (0.518) data 0.264 (0.308) loss_u loss_u 0.8530 (0.8499) acc_u 15.6250 (19.0625) lr 1.1101e-06 eta 0:00:19
epoch [199/200] batch [25/57] time 0.505 (0.519) data 0.293 (0.310) loss_u loss_u 0.8872 (0.8465) acc_u 18.7500 (19.5000) lr 1.1101e-06 eta 0:00:16
epoch [199/200] batch [30/57] time 0.492 (0.517) data 0.274 (0.309) loss_u loss_u 0.7373 (0.8489) acc_u 34.3750 (19.3750) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [35/57] time 0.573 (0.516) data 0.355 (0.308) loss_u loss_u 0.9092 (0.8474) acc_u 12.5000 (19.7321) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [40/57] time 0.583 (0.516) data 0.373 (0.308) loss_u loss_u 0.8408 (0.8501) acc_u 12.5000 (19.1406) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [45/57] time 0.474 (0.514) data 0.265 (0.306) loss_u loss_u 0.8408 (0.8505) acc_u 18.7500 (19.0972) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [50/57] time 0.498 (0.510) data 0.284 (0.302) loss_u loss_u 0.8755 (0.8529) acc_u 12.5000 (18.6250) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [55/57] time 0.548 (0.509) data 0.332 (0.300) loss_u loss_u 0.8906 (0.8497) acc_u 15.6250 (19.0341) lr 1.1101e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1385
confident_label rate tensor(0.4155, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1303
clean true:1217
clean false:86
clean_rate:0.9339984650805833
noisy true:534
noisy false:1299
after delete: len(clean_dataset) 1303
after delete: len(noisy_dataset) 1833
epoch [200/200] batch [5/40] time 0.700 (0.573) data 0.488 (0.361) loss_x loss_x 0.9067 (0.9498) acc_x 81.2500 (77.5000) lr 4.9344e-07 eta 0:00:20
epoch [200/200] batch [10/40] time 0.529 (0.548) data 0.314 (0.335) loss_x loss_x 1.5371 (0.9855) acc_x 50.0000 (74.6875) lr 4.9344e-07 eta 0:00:16
epoch [200/200] batch [15/40] time 0.533 (0.535) data 0.316 (0.322) loss_x loss_x 1.3379 (1.0223) acc_x 62.5000 (74.1667) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [20/40] time 0.518 (0.530) data 0.308 (0.317) loss_x loss_x 1.1455 (1.0622) acc_x 75.0000 (73.9062) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [25/40] time 0.488 (0.518) data 0.277 (0.308) loss_x loss_x 0.9937 (1.0778) acc_x 71.8750 (73.2500) lr 4.9344e-07 eta 0:00:07
epoch [200/200] batch [30/40] time 0.502 (0.520) data 0.292 (0.309) loss_x loss_x 0.9775 (1.0459) acc_x 71.8750 (73.8542) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [35/40] time 0.575 (0.517) data 0.365 (0.309) loss_x loss_x 0.5747 (1.0329) acc_x 87.5000 (74.3750) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [40/40] time 0.510 (0.516) data 0.297 (0.308) loss_x loss_x 0.7593 (1.0194) acc_x 75.0000 (74.6875) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [5/57] time 0.482 (0.514) data 0.351 (0.307) loss_u loss_u 0.7700 (0.8368) acc_u 28.1250 (20.6250) lr 4.9344e-07 eta 0:00:26
epoch [200/200] batch [10/57] time 0.470 (0.514) data 0.259 (0.307) loss_u loss_u 0.9243 (0.8447) acc_u 6.2500 (19.6875) lr 4.9344e-07 eta 0:00:24
epoch [200/200] batch [15/57] time 0.471 (0.512) data 0.255 (0.304) loss_u loss_u 0.8936 (0.8478) acc_u 9.3750 (18.9583) lr 4.9344e-07 eta 0:00:21
epoch [200/200] batch [20/57] time 0.613 (0.511) data 0.404 (0.303) loss_u loss_u 0.9658 (0.8541) acc_u 3.1250 (17.8125) lr 4.9344e-07 eta 0:00:18
epoch [200/200] batch [25/57] time 0.493 (0.510) data 0.280 (0.302) loss_u loss_u 0.8384 (0.8580) acc_u 15.6250 (17.2500) lr 4.9344e-07 eta 0:00:16
epoch [200/200] batch [30/57] time 0.547 (0.511) data 0.336 (0.303) loss_u loss_u 0.7769 (0.8598) acc_u 25.0000 (16.8750) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [35/57] time 0.476 (0.510) data 0.265 (0.302) loss_u loss_u 0.8057 (0.8578) acc_u 25.0000 (17.4107) lr 4.9344e-07 eta 0:00:11
epoch [200/200] batch [40/57] time 0.450 (0.510) data 0.238 (0.303) loss_u loss_u 0.7954 (0.8552) acc_u 21.8750 (17.5000) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [45/57] time 0.428 (0.510) data 0.213 (0.302) loss_u loss_u 0.8940 (0.8559) acc_u 12.5000 (17.2222) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [50/57] time 0.453 (0.509) data 0.243 (0.301) loss_u loss_u 0.8735 (0.8530) acc_u 15.6250 (17.3750) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [55/57] time 0.494 (0.509) data 0.285 (0.302) loss_u loss_u 0.8462 (0.8505) acc_u 18.7500 (17.7273) lr 4.9344e-07 eta 0:00:01
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.375/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 5,173
* accuracy: 64.3%
* error: 35.7%
* macro_f1: 62.6%
Elapsed: 5:21:34
