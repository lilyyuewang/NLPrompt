***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.125', 'DATASET.NOISE_TYPE', 'sym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.125/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.125
  NOISE_TYPE: sym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.125/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.125/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.1591, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 499
clean true:498
clean false:1
clean_rate:0.9979959919839679
noisy true:67
noisy false:2570
after delete: len(clean_dataset) 499
after delete: len(noisy_dataset) 2637
epoch [1/200] batch [5/15] time 0.588 (0.519) data 0.458 (0.366) loss_x loss_x 3.2227 (3.0641) acc_x 28.1250 (32.5000) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [10/15] time 0.683 (0.551) data 0.551 (0.410) loss_x loss_x 2.6113 (2.8531) acc_x 34.3750 (36.8750) lr 1.0000e-05 eta 0:00:02
epoch [1/200] batch [15/15] time 0.581 (0.554) data 0.448 (0.416) loss_x loss_x 2.6328 (2.6921) acc_x 37.5000 (41.0417) lr 1.0000e-05 eta 0:00:00
epoch [1/200] batch [5/82] time 0.454 (0.541) data 0.323 (0.404) loss_u loss_u 0.9395 (0.9414) acc_u 9.3750 (8.7500) lr 1.0000e-05 eta 0:00:41
epoch [1/200] batch [10/82] time 0.399 (0.514) data 0.267 (0.378) loss_u loss_u 0.8994 (0.9309) acc_u 12.5000 (12.5000) lr 1.0000e-05 eta 0:00:37
epoch [1/200] batch [15/82] time 0.444 (0.502) data 0.314 (0.366) loss_u loss_u 0.8638 (0.9208) acc_u 28.1250 (14.3750) lr 1.0000e-05 eta 0:00:33
epoch [1/200] batch [20/82] time 0.450 (0.503) data 0.320 (0.368) loss_u loss_u 0.8887 (0.9096) acc_u 15.6250 (16.5625) lr 1.0000e-05 eta 0:00:31
epoch [1/200] batch [25/82] time 0.396 (0.490) data 0.265 (0.356) loss_u loss_u 0.9253 (0.9099) acc_u 18.7500 (16.6250) lr 1.0000e-05 eta 0:00:27
epoch [1/200] batch [30/82] time 0.438 (0.486) data 0.308 (0.352) loss_u loss_u 0.8872 (0.9073) acc_u 15.6250 (16.9792) lr 1.0000e-05 eta 0:00:25
epoch [1/200] batch [35/82] time 0.509 (0.484) data 0.378 (0.350) loss_u loss_u 0.8896 (0.9060) acc_u 15.6250 (17.0536) lr 1.0000e-05 eta 0:00:22
epoch [1/200] batch [40/82] time 0.365 (0.477) data 0.233 (0.343) loss_u loss_u 0.8652 (0.9034) acc_u 21.8750 (17.4219) lr 1.0000e-05 eta 0:00:20
epoch [1/200] batch [45/82] time 0.368 (0.470) data 0.238 (0.336) loss_u loss_u 0.8682 (0.9014) acc_u 15.6250 (17.5694) lr 1.0000e-05 eta 0:00:17
epoch [1/200] batch [50/82] time 0.557 (0.470) data 0.426 (0.337) loss_u loss_u 0.8672 (0.8977) acc_u 28.1250 (18.4375) lr 1.0000e-05 eta 0:00:15
epoch [1/200] batch [55/82] time 0.543 (0.472) data 0.411 (0.339) loss_u loss_u 0.9072 (0.8979) acc_u 9.3750 (18.2386) lr 1.0000e-05 eta 0:00:12
epoch [1/200] batch [60/82] time 0.365 (0.471) data 0.233 (0.338) loss_u loss_u 0.9062 (0.8966) acc_u 15.6250 (18.4375) lr 1.0000e-05 eta 0:00:10
epoch [1/200] batch [65/82] time 0.486 (0.474) data 0.355 (0.341) loss_u loss_u 0.8560 (0.8946) acc_u 31.2500 (18.7019) lr 1.0000e-05 eta 0:00:08
epoch [1/200] batch [70/82] time 0.480 (0.480) data 0.348 (0.347) loss_u loss_u 0.8315 (0.8931) acc_u 34.3750 (19.0179) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [75/82] time 0.328 (0.475) data 0.196 (0.343) loss_u loss_u 0.8945 (0.8923) acc_u 15.6250 (18.9167) lr 1.0000e-05 eta 0:00:03
epoch [1/200] batch [80/82] time 0.507 (0.474) data 0.371 (0.342) loss_u loss_u 0.8550 (0.8901) acc_u 15.6250 (19.2969) lr 1.0000e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2086
confident_label rate tensor(0.2959, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 928
clean true:928
clean false:0
clean_rate:1.0
noisy true:122
noisy false:2086
after delete: len(clean_dataset) 928
after delete: len(noisy_dataset) 2208
epoch [2/200] batch [5/29] time 0.511 (0.527) data 0.380 (0.395) loss_x loss_x 1.1162 (1.6189) acc_x 71.8750 (64.3750) lr 2.0000e-03 eta 0:00:12
epoch [2/200] batch [10/29] time 0.512 (0.550) data 0.380 (0.419) loss_x loss_x 2.0820 (1.6356) acc_x 50.0000 (60.6250) lr 2.0000e-03 eta 0:00:10
epoch [2/200] batch [15/29] time 0.455 (0.516) data 0.323 (0.385) loss_x loss_x 1.5811 (1.6166) acc_x 59.3750 (59.5833) lr 2.0000e-03 eta 0:00:07
epoch [2/200] batch [20/29] time 0.415 (0.489) data 0.283 (0.358) loss_x loss_x 1.8193 (1.6410) acc_x 53.1250 (58.4375) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [25/29] time 0.395 (0.479) data 0.264 (0.348) loss_x loss_x 1.9199 (1.6433) acc_x 53.1250 (57.5000) lr 2.0000e-03 eta 0:00:01
epoch [2/200] batch [5/69] time 0.409 (0.483) data 0.278 (0.352) loss_u loss_u 0.6997 (0.7980) acc_u 46.8750 (29.3750) lr 2.0000e-03 eta 0:00:30
epoch [2/200] batch [10/69] time 0.416 (0.480) data 0.283 (0.348) loss_u loss_u 0.8271 (0.8187) acc_u 21.8750 (25.3125) lr 2.0000e-03 eta 0:00:28
epoch [2/200] batch [15/69] time 0.404 (0.471) data 0.271 (0.339) loss_u loss_u 0.8882 (0.8313) acc_u 18.7500 (24.3750) lr 2.0000e-03 eta 0:00:25
epoch [2/200] batch [20/69] time 0.407 (0.467) data 0.277 (0.335) loss_u loss_u 0.8374 (0.8259) acc_u 18.7500 (24.5312) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [25/69] time 0.360 (0.466) data 0.229 (0.334) loss_u loss_u 0.8164 (0.8188) acc_u 25.0000 (25.3750) lr 2.0000e-03 eta 0:00:20
epoch [2/200] batch [30/69] time 0.595 (0.468) data 0.458 (0.336) loss_u loss_u 0.7754 (0.8116) acc_u 34.3750 (26.0417) lr 2.0000e-03 eta 0:00:18
epoch [2/200] batch [35/69] time 0.399 (0.465) data 0.262 (0.334) loss_u loss_u 0.9058 (0.8154) acc_u 12.5000 (25.3571) lr 2.0000e-03 eta 0:00:15
epoch [2/200] batch [40/69] time 0.473 (0.465) data 0.341 (0.333) loss_u loss_u 0.8579 (0.8120) acc_u 18.7500 (25.9375) lr 2.0000e-03 eta 0:00:13
epoch [2/200] batch [45/69] time 0.378 (0.461) data 0.247 (0.329) loss_u loss_u 0.7695 (0.8094) acc_u 34.3750 (26.3889) lr 2.0000e-03 eta 0:00:11
epoch [2/200] batch [50/69] time 0.377 (0.457) data 0.246 (0.325) loss_u loss_u 0.8096 (0.8094) acc_u 28.1250 (26.6250) lr 2.0000e-03 eta 0:00:08
epoch [2/200] batch [55/69] time 0.457 (0.459) data 0.324 (0.327) loss_u loss_u 0.8862 (0.8092) acc_u 15.6250 (26.4205) lr 2.0000e-03 eta 0:00:06
epoch [2/200] batch [60/69] time 0.547 (0.460) data 0.414 (0.328) loss_u loss_u 0.7441 (0.8103) acc_u 37.5000 (26.3542) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [65/69] time 0.462 (0.460) data 0.329 (0.328) loss_u loss_u 0.8721 (0.8127) acc_u 12.5000 (25.8173) lr 2.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1839
confident_label rate tensor(0.3683, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1155
clean true:1154
clean false:1
clean_rate:0.9991341991341991
noisy true:143
noisy false:1838
after delete: len(clean_dataset) 1155
after delete: len(noisy_dataset) 1981
epoch [3/200] batch [5/36] time 0.725 (0.540) data 0.593 (0.409) loss_x loss_x 1.5146 (1.5904) acc_x 50.0000 (59.3750) lr 1.9999e-03 eta 0:00:16
epoch [3/200] batch [10/36] time 0.424 (0.506) data 0.292 (0.374) loss_x loss_x 1.3867 (1.6582) acc_x 71.8750 (60.0000) lr 1.9999e-03 eta 0:00:13
epoch [3/200] batch [15/36] time 0.461 (0.494) data 0.330 (0.362) loss_x loss_x 1.2578 (1.5672) acc_x 65.6250 (62.2917) lr 1.9999e-03 eta 0:00:10
epoch [3/200] batch [20/36] time 0.506 (0.497) data 0.374 (0.365) loss_x loss_x 1.8066 (1.5504) acc_x 56.2500 (61.7188) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [25/36] time 0.421 (0.484) data 0.290 (0.352) loss_x loss_x 1.5596 (1.5696) acc_x 65.6250 (61.5000) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [30/36] time 0.419 (0.478) data 0.287 (0.347) loss_x loss_x 1.9414 (1.5775) acc_x 46.8750 (60.5208) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [35/36] time 0.545 (0.473) data 0.414 (0.341) loss_x loss_x 1.9082 (1.5846) acc_x 46.8750 (59.7321) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/61] time 0.514 (0.472) data 0.381 (0.340) loss_u loss_u 0.8188 (0.8254) acc_u 25.0000 (23.7500) lr 1.9999e-03 eta 0:00:26
epoch [3/200] batch [10/61] time 0.547 (0.474) data 0.415 (0.342) loss_u loss_u 0.7651 (0.8233) acc_u 25.0000 (24.0625) lr 1.9999e-03 eta 0:00:24
epoch [3/200] batch [15/61] time 0.422 (0.471) data 0.291 (0.340) loss_u loss_u 0.9375 (0.8410) acc_u 9.3750 (22.2917) lr 1.9999e-03 eta 0:00:21
epoch [3/200] batch [20/61] time 0.504 (0.470) data 0.372 (0.338) loss_u loss_u 0.8525 (0.8435) acc_u 15.6250 (22.3438) lr 1.9999e-03 eta 0:00:19
epoch [3/200] batch [25/61] time 0.456 (0.468) data 0.325 (0.337) loss_u loss_u 0.8467 (0.8390) acc_u 21.8750 (23.3750) lr 1.9999e-03 eta 0:00:16
epoch [3/200] batch [30/61] time 0.498 (0.468) data 0.366 (0.336) loss_u loss_u 0.8330 (0.8401) acc_u 15.6250 (22.2917) lr 1.9999e-03 eta 0:00:14
epoch [3/200] batch [35/61] time 0.329 (0.465) data 0.198 (0.333) loss_u loss_u 0.8096 (0.8331) acc_u 28.1250 (23.1250) lr 1.9999e-03 eta 0:00:12
epoch [3/200] batch [40/61] time 0.549 (0.468) data 0.417 (0.336) loss_u loss_u 0.8901 (0.8347) acc_u 9.3750 (22.3438) lr 1.9999e-03 eta 0:00:09
epoch [3/200] batch [45/61] time 0.339 (0.463) data 0.208 (0.331) loss_u loss_u 0.8066 (0.8335) acc_u 28.1250 (22.7778) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [50/61] time 0.554 (0.463) data 0.422 (0.332) loss_u loss_u 0.8755 (0.8337) acc_u 12.5000 (22.3125) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [55/61] time 0.407 (0.461) data 0.276 (0.329) loss_u loss_u 0.8628 (0.8315) acc_u 12.5000 (22.5000) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [60/61] time 0.346 (0.458) data 0.215 (0.327) loss_u loss_u 0.8813 (0.8307) acc_u 18.7500 (22.5521) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1836
confident_label rate tensor(0.3670, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1151
clean true:1151
clean false:0
clean_rate:1.0
noisy true:149
noisy false:1836
after delete: len(clean_dataset) 1151
after delete: len(noisy_dataset) 1985
epoch [4/200] batch [5/35] time 0.376 (0.509) data 0.246 (0.377) loss_x loss_x 1.3799 (1.5977) acc_x 53.1250 (53.7500) lr 1.9995e-03 eta 0:00:15
epoch [4/200] batch [10/35] time 0.495 (0.479) data 0.363 (0.348) loss_x loss_x 1.5850 (1.6115) acc_x 56.2500 (56.8750) lr 1.9995e-03 eta 0:00:11
epoch [4/200] batch [15/35] time 0.427 (0.464) data 0.295 (0.333) loss_x loss_x 1.9316 (1.5680) acc_x 46.8750 (57.9167) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [20/35] time 0.435 (0.471) data 0.304 (0.340) loss_x loss_x 1.9736 (1.6158) acc_x 50.0000 (56.8750) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [25/35] time 0.549 (0.470) data 0.418 (0.339) loss_x loss_x 1.4395 (1.6063) acc_x 59.3750 (57.5000) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [30/35] time 0.407 (0.464) data 0.276 (0.333) loss_x loss_x 1.2988 (1.5917) acc_x 65.6250 (58.5417) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [35/35] time 0.504 (0.462) data 0.373 (0.331) loss_x loss_x 1.2891 (1.5749) acc_x 62.5000 (58.8393) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/62] time 0.405 (0.457) data 0.275 (0.326) loss_u loss_u 0.8350 (0.8534) acc_u 21.8750 (15.6250) lr 1.9995e-03 eta 0:00:26
epoch [4/200] batch [10/62] time 0.587 (0.457) data 0.454 (0.326) loss_u loss_u 0.7954 (0.8446) acc_u 25.0000 (16.8750) lr 1.9995e-03 eta 0:00:23
epoch [4/200] batch [15/62] time 0.514 (0.461) data 0.381 (0.329) loss_u loss_u 0.7803 (0.8364) acc_u 28.1250 (18.7500) lr 1.9995e-03 eta 0:00:21
epoch [4/200] batch [20/62] time 0.493 (0.457) data 0.360 (0.325) loss_u loss_u 0.8340 (0.8334) acc_u 25.0000 (20.3125) lr 1.9995e-03 eta 0:00:19
epoch [4/200] batch [25/62] time 0.386 (0.450) data 0.255 (0.319) loss_u loss_u 0.7808 (0.8318) acc_u 28.1250 (21.0000) lr 1.9995e-03 eta 0:00:16
epoch [4/200] batch [30/62] time 0.504 (0.454) data 0.373 (0.323) loss_u loss_u 0.8301 (0.8320) acc_u 21.8750 (21.3542) lr 1.9995e-03 eta 0:00:14
epoch [4/200] batch [35/62] time 0.450 (0.449) data 0.319 (0.317) loss_u loss_u 0.8555 (0.8312) acc_u 21.8750 (21.6964) lr 1.9995e-03 eta 0:00:12
epoch [4/200] batch [40/62] time 0.447 (0.451) data 0.315 (0.319) loss_u loss_u 0.8325 (0.8281) acc_u 18.7500 (22.1094) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [45/62] time 0.503 (0.453) data 0.372 (0.321) loss_u loss_u 0.8081 (0.8272) acc_u 21.8750 (22.0833) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [50/62] time 0.414 (0.451) data 0.282 (0.319) loss_u loss_u 0.8110 (0.8299) acc_u 25.0000 (21.8750) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [55/62] time 0.481 (0.451) data 0.350 (0.319) loss_u loss_u 0.8447 (0.8311) acc_u 21.8750 (21.6477) lr 1.9995e-03 eta 0:00:03
epoch [4/200] batch [60/62] time 0.451 (0.454) data 0.319 (0.322) loss_u loss_u 0.8057 (0.8303) acc_u 31.2500 (21.9792) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1795
confident_label rate tensor(0.3776, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1184
clean true:1181
clean false:3
clean_rate:0.9974662162162162
noisy true:160
noisy false:1792
after delete: len(clean_dataset) 1184
after delete: len(noisy_dataset) 1952
epoch [5/200] batch [5/37] time 0.433 (0.462) data 0.302 (0.331) loss_x loss_x 1.2920 (1.5527) acc_x 56.2500 (58.7500) lr 1.9989e-03 eta 0:00:14
epoch [5/200] batch [10/37] time 0.484 (0.477) data 0.353 (0.346) loss_x loss_x 1.7090 (1.5438) acc_x 65.6250 (60.6250) lr 1.9989e-03 eta 0:00:12
epoch [5/200] batch [15/37] time 0.441 (0.464) data 0.309 (0.333) loss_x loss_x 1.2207 (1.5217) acc_x 68.7500 (60.6250) lr 1.9989e-03 eta 0:00:10
epoch [5/200] batch [20/37] time 0.459 (0.465) data 0.327 (0.334) loss_x loss_x 1.8350 (1.5532) acc_x 59.3750 (58.5938) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [25/37] time 0.496 (0.458) data 0.366 (0.327) loss_x loss_x 1.1514 (1.5472) acc_x 59.3750 (58.2500) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [30/37] time 0.457 (0.464) data 0.326 (0.333) loss_x loss_x 1.4922 (1.5421) acc_x 62.5000 (58.5417) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [35/37] time 0.450 (0.460) data 0.319 (0.329) loss_x loss_x 1.3135 (1.5482) acc_x 68.7500 (58.5714) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/61] time 0.349 (0.461) data 0.217 (0.329) loss_u loss_u 0.8506 (0.8057) acc_u 12.5000 (23.7500) lr 1.9989e-03 eta 0:00:25
epoch [5/200] batch [10/61] time 0.420 (0.457) data 0.288 (0.326) loss_u loss_u 0.8467 (0.8310) acc_u 25.0000 (21.5625) lr 1.9989e-03 eta 0:00:23
epoch [5/200] batch [15/61] time 0.372 (0.453) data 0.240 (0.321) loss_u loss_u 0.8364 (0.8233) acc_u 25.0000 (23.1250) lr 1.9989e-03 eta 0:00:20
epoch [5/200] batch [20/61] time 0.494 (0.455) data 0.362 (0.323) loss_u loss_u 0.8501 (0.8322) acc_u 12.5000 (21.2500) lr 1.9989e-03 eta 0:00:18
epoch [5/200] batch [25/61] time 0.419 (0.453) data 0.287 (0.321) loss_u loss_u 0.8594 (0.8356) acc_u 18.7500 (21.0000) lr 1.9989e-03 eta 0:00:16
epoch [5/200] batch [30/61] time 0.453 (0.456) data 0.319 (0.324) loss_u loss_u 0.7637 (0.8324) acc_u 25.0000 (21.1458) lr 1.9989e-03 eta 0:00:14
epoch [5/200] batch [35/61] time 0.485 (0.454) data 0.353 (0.323) loss_u loss_u 0.8584 (0.8353) acc_u 15.6250 (20.5357) lr 1.9989e-03 eta 0:00:11
epoch [5/200] batch [40/61] time 0.511 (0.457) data 0.379 (0.325) loss_u loss_u 0.8750 (0.8361) acc_u 9.3750 (20.3906) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [45/61] time 0.414 (0.461) data 0.282 (0.329) loss_u loss_u 0.9131 (0.8366) acc_u 9.3750 (20.4861) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [50/61] time 0.378 (0.458) data 0.245 (0.326) loss_u loss_u 0.8237 (0.8338) acc_u 31.2500 (20.9375) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [55/61] time 0.444 (0.456) data 0.312 (0.324) loss_u loss_u 0.8428 (0.8323) acc_u 25.0000 (21.5341) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [60/61] time 0.405 (0.455) data 0.272 (0.323) loss_u loss_u 0.8740 (0.8312) acc_u 15.6250 (21.6667) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1811
confident_label rate tensor(0.3728, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1169
clean true:1167
clean false:2
clean_rate:0.998289136013687
noisy true:158
noisy false:1809
after delete: len(clean_dataset) 1169
after delete: len(noisy_dataset) 1967
epoch [6/200] batch [5/36] time 0.443 (0.479) data 0.311 (0.347) loss_x loss_x 1.4512 (1.4670) acc_x 62.5000 (60.0000) lr 1.9980e-03 eta 0:00:14
epoch [6/200] batch [10/36] time 0.434 (0.455) data 0.303 (0.324) loss_x loss_x 1.8398 (1.5362) acc_x 53.1250 (57.1875) lr 1.9980e-03 eta 0:00:11
epoch [6/200] batch [15/36] time 0.477 (0.461) data 0.345 (0.330) loss_x loss_x 1.4736 (1.6072) acc_x 68.7500 (56.6667) lr 1.9980e-03 eta 0:00:09
epoch [6/200] batch [20/36] time 0.436 (0.482) data 0.305 (0.350) loss_x loss_x 1.8154 (1.6333) acc_x 46.8750 (55.9375) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [25/36] time 0.402 (0.480) data 0.270 (0.348) loss_x loss_x 1.2920 (1.5928) acc_x 71.8750 (57.1250) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [30/36] time 0.495 (0.477) data 0.364 (0.346) loss_x loss_x 1.7373 (1.6017) acc_x 62.5000 (56.9792) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [35/36] time 0.583 (0.478) data 0.453 (0.347) loss_x loss_x 1.4473 (1.5831) acc_x 65.6250 (57.5000) lr 1.9980e-03 eta 0:00:00
epoch [6/200] batch [5/61] time 0.409 (0.472) data 0.276 (0.341) loss_u loss_u 0.8364 (0.8163) acc_u 21.8750 (23.7500) lr 1.9980e-03 eta 0:00:26
epoch [6/200] batch [10/61] time 0.458 (0.465) data 0.326 (0.333) loss_u loss_u 0.7827 (0.8067) acc_u 21.8750 (25.6250) lr 1.9980e-03 eta 0:00:23
epoch [6/200] batch [15/61] time 0.341 (0.459) data 0.210 (0.327) loss_u loss_u 0.8145 (0.8183) acc_u 31.2500 (23.9583) lr 1.9980e-03 eta 0:00:21
epoch [6/200] batch [20/61] time 0.401 (0.457) data 0.269 (0.325) loss_u loss_u 0.8408 (0.8219) acc_u 15.6250 (23.4375) lr 1.9980e-03 eta 0:00:18
epoch [6/200] batch [25/61] time 0.373 (0.455) data 0.242 (0.324) loss_u loss_u 0.7671 (0.8163) acc_u 21.8750 (24.0000) lr 1.9980e-03 eta 0:00:16
epoch [6/200] batch [30/61] time 0.489 (0.461) data 0.359 (0.329) loss_u loss_u 0.8105 (0.8173) acc_u 18.7500 (23.6458) lr 1.9980e-03 eta 0:00:14
epoch [6/200] batch [35/61] time 0.559 (0.463) data 0.428 (0.331) loss_u loss_u 0.7876 (0.8187) acc_u 34.3750 (23.7500) lr 1.9980e-03 eta 0:00:12
epoch [6/200] batch [40/61] time 0.629 (0.463) data 0.497 (0.332) loss_u loss_u 0.8115 (0.8190) acc_u 18.7500 (23.8281) lr 1.9980e-03 eta 0:00:09
epoch [6/200] batch [45/61] time 0.407 (0.459) data 0.274 (0.328) loss_u loss_u 0.8335 (0.8203) acc_u 18.7500 (23.3333) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [50/61] time 0.454 (0.458) data 0.322 (0.327) loss_u loss_u 0.7144 (0.8173) acc_u 37.5000 (24.0000) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [55/61] time 0.418 (0.454) data 0.285 (0.323) loss_u loss_u 0.7988 (0.8162) acc_u 21.8750 (24.0341) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [60/61] time 0.428 (0.453) data 0.295 (0.321) loss_u loss_u 0.7817 (0.8157) acc_u 28.1250 (24.1667) lr 1.9980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1777
confident_label rate tensor(0.3871, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1214
clean true:1212
clean false:2
clean_rate:0.9983525535420099
noisy true:147
noisy false:1775
after delete: len(clean_dataset) 1214
after delete: len(noisy_dataset) 1922
epoch [7/200] batch [5/37] time 0.504 (0.436) data 0.374 (0.305) loss_x loss_x 1.4277 (1.5234) acc_x 65.6250 (59.3750) lr 1.9969e-03 eta 0:00:13
epoch [7/200] batch [10/37] time 0.689 (0.498) data 0.558 (0.367) loss_x loss_x 1.8779 (1.5266) acc_x 53.1250 (60.0000) lr 1.9969e-03 eta 0:00:13
epoch [7/200] batch [15/37] time 0.365 (0.482) data 0.234 (0.351) loss_x loss_x 1.8994 (1.5309) acc_x 40.6250 (61.0417) lr 1.9969e-03 eta 0:00:10
epoch [7/200] batch [20/37] time 0.414 (0.478) data 0.284 (0.347) loss_x loss_x 0.9170 (1.4869) acc_x 78.1250 (61.7188) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [25/37] time 0.488 (0.475) data 0.357 (0.344) loss_x loss_x 1.6592 (1.5136) acc_x 56.2500 (61.1250) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [30/37] time 0.447 (0.464) data 0.316 (0.332) loss_x loss_x 1.1426 (1.5142) acc_x 65.6250 (61.1458) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [35/37] time 0.445 (0.463) data 0.313 (0.331) loss_x loss_x 2.2539 (1.5184) acc_x 53.1250 (61.1607) lr 1.9969e-03 eta 0:00:00
epoch [7/200] batch [5/60] time 0.588 (0.463) data 0.457 (0.332) loss_u loss_u 0.7520 (0.8279) acc_u 34.3750 (23.7500) lr 1.9969e-03 eta 0:00:25
epoch [7/200] batch [10/60] time 0.441 (0.467) data 0.309 (0.335) loss_u loss_u 0.8447 (0.8015) acc_u 21.8750 (26.2500) lr 1.9969e-03 eta 0:00:23
epoch [7/200] batch [15/60] time 0.453 (0.461) data 0.321 (0.330) loss_u loss_u 0.7520 (0.8048) acc_u 34.3750 (25.8333) lr 1.9969e-03 eta 0:00:20
epoch [7/200] batch [20/60] time 0.412 (0.458) data 0.279 (0.327) loss_u loss_u 0.8086 (0.8061) acc_u 18.7500 (25.0000) lr 1.9969e-03 eta 0:00:18
epoch [7/200] batch [25/60] time 0.458 (0.466) data 0.327 (0.335) loss_u loss_u 0.8252 (0.8079) acc_u 25.0000 (24.5000) lr 1.9969e-03 eta 0:00:16
epoch [7/200] batch [30/60] time 0.682 (0.466) data 0.549 (0.334) loss_u loss_u 0.8164 (0.8095) acc_u 21.8750 (24.3750) lr 1.9969e-03 eta 0:00:13
epoch [7/200] batch [35/60] time 0.627 (0.467) data 0.496 (0.335) loss_u loss_u 0.8374 (0.8098) acc_u 28.1250 (24.4643) lr 1.9969e-03 eta 0:00:11
epoch [7/200] batch [40/60] time 0.351 (0.461) data 0.219 (0.330) loss_u loss_u 0.7427 (0.8135) acc_u 31.2500 (24.0625) lr 1.9969e-03 eta 0:00:09
epoch [7/200] batch [45/60] time 0.432 (0.457) data 0.300 (0.325) loss_u loss_u 0.8511 (0.8113) acc_u 12.5000 (24.0278) lr 1.9969e-03 eta 0:00:06
epoch [7/200] batch [50/60] time 0.509 (0.456) data 0.377 (0.324) loss_u loss_u 0.8726 (0.8152) acc_u 15.6250 (23.5625) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [55/60] time 0.403 (0.454) data 0.270 (0.322) loss_u loss_u 0.8247 (0.8160) acc_u 21.8750 (23.4091) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [60/60] time 0.552 (0.452) data 0.419 (0.320) loss_u loss_u 0.7856 (0.8156) acc_u 21.8750 (23.3854) lr 1.9969e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1775
confident_label rate tensor(0.3846, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1206
clean true:1206
clean false:0
clean_rate:1.0
noisy true:155
noisy false:1775
after delete: len(clean_dataset) 1206
after delete: len(noisy_dataset) 1930
epoch [8/200] batch [5/37] time 0.545 (0.533) data 0.414 (0.402) loss_x loss_x 1.5010 (1.5197) acc_x 71.8750 (62.5000) lr 1.9956e-03 eta 0:00:17
epoch [8/200] batch [10/37] time 0.407 (0.500) data 0.276 (0.368) loss_x loss_x 1.9600 (1.5821) acc_x 53.1250 (61.2500) lr 1.9956e-03 eta 0:00:13
epoch [8/200] batch [15/37] time 0.380 (0.490) data 0.249 (0.359) loss_x loss_x 1.7959 (1.6122) acc_x 46.8750 (60.6250) lr 1.9956e-03 eta 0:00:10
epoch [8/200] batch [20/37] time 0.577 (0.493) data 0.445 (0.361) loss_x loss_x 1.6797 (1.6050) acc_x 53.1250 (60.1562) lr 1.9956e-03 eta 0:00:08
epoch [8/200] batch [25/37] time 0.407 (0.497) data 0.277 (0.365) loss_x loss_x 1.4111 (1.5682) acc_x 53.1250 (60.1250) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [30/37] time 0.487 (0.497) data 0.356 (0.365) loss_x loss_x 1.1973 (1.5533) acc_x 53.1250 (59.5833) lr 1.9956e-03 eta 0:00:03
epoch [8/200] batch [35/37] time 0.487 (0.502) data 0.356 (0.371) loss_x loss_x 1.1836 (1.5265) acc_x 68.7500 (60.0893) lr 1.9956e-03 eta 0:00:01
epoch [8/200] batch [5/60] time 0.352 (0.489) data 0.220 (0.357) loss_u loss_u 0.8672 (0.8461) acc_u 12.5000 (18.1250) lr 1.9956e-03 eta 0:00:26
epoch [8/200] batch [10/60] time 0.366 (0.483) data 0.233 (0.351) loss_u loss_u 0.7324 (0.8275) acc_u 40.6250 (22.1875) lr 1.9956e-03 eta 0:00:24
epoch [8/200] batch [15/60] time 0.375 (0.478) data 0.243 (0.347) loss_u loss_u 0.7822 (0.8320) acc_u 37.5000 (21.6667) lr 1.9956e-03 eta 0:00:21
epoch [8/200] batch [20/60] time 0.425 (0.474) data 0.292 (0.342) loss_u loss_u 0.8369 (0.8355) acc_u 31.2500 (21.7188) lr 1.9956e-03 eta 0:00:18
epoch [8/200] batch [25/60] time 0.458 (0.470) data 0.326 (0.338) loss_u loss_u 0.7681 (0.8316) acc_u 28.1250 (22.6250) lr 1.9956e-03 eta 0:00:16
epoch [8/200] batch [30/60] time 0.429 (0.467) data 0.297 (0.335) loss_u loss_u 0.7769 (0.8281) acc_u 28.1250 (23.1250) lr 1.9956e-03 eta 0:00:14
epoch [8/200] batch [35/60] time 0.402 (0.463) data 0.270 (0.331) loss_u loss_u 0.8145 (0.8272) acc_u 25.0000 (23.2143) lr 1.9956e-03 eta 0:00:11
epoch [8/200] batch [40/60] time 0.469 (0.461) data 0.338 (0.329) loss_u loss_u 0.7710 (0.8265) acc_u 31.2500 (23.0469) lr 1.9956e-03 eta 0:00:09
epoch [8/200] batch [45/60] time 0.449 (0.462) data 0.317 (0.330) loss_u loss_u 0.8633 (0.8230) acc_u 9.3750 (23.4028) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [50/60] time 0.376 (0.459) data 0.243 (0.327) loss_u loss_u 0.8325 (0.8228) acc_u 18.7500 (23.4375) lr 1.9956e-03 eta 0:00:04
epoch [8/200] batch [55/60] time 0.503 (0.461) data 0.371 (0.329) loss_u loss_u 0.7773 (0.8222) acc_u 28.1250 (23.4091) lr 1.9956e-03 eta 0:00:02
epoch [8/200] batch [60/60] time 0.332 (0.459) data 0.199 (0.327) loss_u loss_u 0.8701 (0.8219) acc_u 9.3750 (23.1250) lr 1.9956e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1788
confident_label rate tensor(0.3807, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1194
clean true:1192
clean false:2
clean_rate:0.998324958123953
noisy true:156
noisy false:1786
after delete: len(clean_dataset) 1194
after delete: len(noisy_dataset) 1942
epoch [9/200] batch [5/37] time 0.563 (0.447) data 0.433 (0.316) loss_x loss_x 1.5195 (1.3658) acc_x 62.5000 (63.1250) lr 1.9940e-03 eta 0:00:14
epoch [9/200] batch [10/37] time 0.451 (0.464) data 0.320 (0.333) loss_x loss_x 1.3535 (1.3351) acc_x 62.5000 (64.3750) lr 1.9940e-03 eta 0:00:12
epoch [9/200] batch [15/37] time 0.404 (0.452) data 0.272 (0.320) loss_x loss_x 1.4990 (1.3388) acc_x 56.2500 (63.7500) lr 1.9940e-03 eta 0:00:09
epoch [9/200] batch [20/37] time 0.458 (0.466) data 0.326 (0.335) loss_x loss_x 1.4277 (1.4034) acc_x 68.7500 (63.1250) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [25/37] time 0.685 (0.492) data 0.554 (0.361) loss_x loss_x 1.4834 (1.3958) acc_x 65.6250 (63.1250) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [30/37] time 0.360 (0.482) data 0.229 (0.351) loss_x loss_x 2.0488 (1.4499) acc_x 65.6250 (62.6042) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [35/37] time 0.486 (0.484) data 0.354 (0.352) loss_x loss_x 1.7402 (1.4432) acc_x 68.7500 (63.1250) lr 1.9940e-03 eta 0:00:00
epoch [9/200] batch [5/60] time 0.707 (0.484) data 0.575 (0.352) loss_u loss_u 0.7563 (0.7892) acc_u 25.0000 (27.5000) lr 1.9940e-03 eta 0:00:26
epoch [9/200] batch [10/60] time 0.479 (0.479) data 0.346 (0.347) loss_u loss_u 0.7993 (0.8106) acc_u 28.1250 (23.7500) lr 1.9940e-03 eta 0:00:23
epoch [9/200] batch [15/60] time 0.402 (0.481) data 0.270 (0.349) loss_u loss_u 0.8477 (0.8018) acc_u 31.2500 (27.2917) lr 1.9940e-03 eta 0:00:21
epoch [9/200] batch [20/60] time 0.531 (0.474) data 0.399 (0.342) loss_u loss_u 0.8784 (0.8073) acc_u 12.5000 (26.5625) lr 1.9940e-03 eta 0:00:18
epoch [9/200] batch [25/60] time 0.490 (0.472) data 0.358 (0.340) loss_u loss_u 0.8013 (0.8145) acc_u 25.0000 (25.2500) lr 1.9940e-03 eta 0:00:16
epoch [9/200] batch [30/60] time 0.428 (0.466) data 0.296 (0.334) loss_u loss_u 0.8740 (0.8227) acc_u 12.5000 (23.8542) lr 1.9940e-03 eta 0:00:13
epoch [9/200] batch [35/60] time 0.475 (0.470) data 0.342 (0.339) loss_u loss_u 0.8828 (0.8238) acc_u 9.3750 (23.5714) lr 1.9940e-03 eta 0:00:11
epoch [9/200] batch [40/60] time 0.355 (0.466) data 0.222 (0.334) loss_u loss_u 0.7612 (0.8205) acc_u 34.3750 (23.9844) lr 1.9940e-03 eta 0:00:09
epoch [9/200] batch [45/60] time 0.461 (0.469) data 0.329 (0.337) loss_u loss_u 0.8564 (0.8199) acc_u 15.6250 (24.3750) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [50/60] time 0.370 (0.463) data 0.237 (0.332) loss_u loss_u 0.7817 (0.8192) acc_u 31.2500 (24.5000) lr 1.9940e-03 eta 0:00:04
epoch [9/200] batch [55/60] time 0.436 (0.462) data 0.304 (0.330) loss_u loss_u 0.7871 (0.8185) acc_u 31.2500 (24.5455) lr 1.9940e-03 eta 0:00:02
epoch [9/200] batch [60/60] time 0.370 (0.459) data 0.237 (0.327) loss_u loss_u 0.8228 (0.8173) acc_u 25.0000 (24.9479) lr 1.9940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1759
confident_label rate tensor(0.3906, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1225
clean true:1224
clean false:1
clean_rate:0.9991836734693877
noisy true:153
noisy false:1758
after delete: len(clean_dataset) 1225
after delete: len(noisy_dataset) 1911
epoch [10/200] batch [5/38] time 0.457 (0.524) data 0.325 (0.392) loss_x loss_x 1.3457 (1.5775) acc_x 56.2500 (59.3750) lr 1.9921e-03 eta 0:00:17
epoch [10/200] batch [10/38] time 0.419 (0.523) data 0.287 (0.391) loss_x loss_x 2.0430 (1.5098) acc_x 50.0000 (62.5000) lr 1.9921e-03 eta 0:00:14
epoch [10/200] batch [15/38] time 0.557 (0.524) data 0.426 (0.392) loss_x loss_x 1.6357 (1.5033) acc_x 62.5000 (61.0417) lr 1.9921e-03 eta 0:00:12
epoch [10/200] batch [20/38] time 0.517 (0.505) data 0.385 (0.373) loss_x loss_x 1.4697 (1.4986) acc_x 56.2500 (60.1562) lr 1.9921e-03 eta 0:00:09
epoch [10/200] batch [25/38] time 0.475 (0.498) data 0.343 (0.366) loss_x loss_x 1.4844 (1.4940) acc_x 65.6250 (61.3750) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [30/38] time 0.451 (0.504) data 0.319 (0.372) loss_x loss_x 1.6836 (1.5117) acc_x 59.3750 (61.1458) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [35/38] time 0.428 (0.499) data 0.297 (0.367) loss_x loss_x 1.6523 (1.5237) acc_x 62.5000 (60.8929) lr 1.9921e-03 eta 0:00:01
epoch [10/200] batch [5/59] time 0.485 (0.490) data 0.353 (0.358) loss_u loss_u 0.7944 (0.8132) acc_u 28.1250 (25.0000) lr 1.9921e-03 eta 0:00:26
epoch [10/200] batch [10/59] time 0.410 (0.488) data 0.278 (0.356) loss_u loss_u 0.8618 (0.8334) acc_u 15.6250 (21.5625) lr 1.9921e-03 eta 0:00:23
epoch [10/200] batch [15/59] time 0.374 (0.484) data 0.241 (0.352) loss_u loss_u 0.7876 (0.8291) acc_u 31.2500 (22.9167) lr 1.9921e-03 eta 0:00:21
epoch [10/200] batch [20/59] time 0.485 (0.479) data 0.353 (0.347) loss_u loss_u 0.8052 (0.8278) acc_u 21.8750 (23.2812) lr 1.9921e-03 eta 0:00:18
epoch [10/200] batch [25/59] time 0.546 (0.480) data 0.412 (0.348) loss_u loss_u 0.7954 (0.8209) acc_u 34.3750 (24.1250) lr 1.9921e-03 eta 0:00:16
epoch [10/200] batch [30/59] time 0.429 (0.475) data 0.296 (0.343) loss_u loss_u 0.8535 (0.8225) acc_u 15.6250 (23.3333) lr 1.9921e-03 eta 0:00:13
epoch [10/200] batch [35/59] time 0.425 (0.472) data 0.293 (0.340) loss_u loss_u 0.9087 (0.8180) acc_u 9.3750 (23.1250) lr 1.9921e-03 eta 0:00:11
epoch [10/200] batch [40/59] time 0.416 (0.466) data 0.284 (0.334) loss_u loss_u 0.7959 (0.8162) acc_u 21.8750 (23.4375) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [45/59] time 0.433 (0.462) data 0.301 (0.330) loss_u loss_u 0.8628 (0.8152) acc_u 12.5000 (23.5417) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [50/59] time 0.430 (0.459) data 0.299 (0.327) loss_u loss_u 0.8677 (0.8163) acc_u 9.3750 (23.1875) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [55/59] time 0.412 (0.457) data 0.280 (0.324) loss_u loss_u 0.8740 (0.8190) acc_u 12.5000 (22.7841) lr 1.9921e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1747
confident_label rate tensor(0.3945, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1237
clean true:1236
clean false:1
clean_rate:0.9991915925626516
noisy true:153
noisy false:1746
after delete: len(clean_dataset) 1237
after delete: len(noisy_dataset) 1899
epoch [11/200] batch [5/38] time 0.427 (0.467) data 0.296 (0.336) loss_x loss_x 1.3721 (1.6992) acc_x 62.5000 (55.6250) lr 1.9900e-03 eta 0:00:15
epoch [11/200] batch [10/38] time 0.578 (0.475) data 0.448 (0.343) loss_x loss_x 1.5479 (1.5383) acc_x 53.1250 (60.0000) lr 1.9900e-03 eta 0:00:13
epoch [11/200] batch [15/38] time 0.627 (0.468) data 0.495 (0.336) loss_x loss_x 1.4014 (1.5695) acc_x 59.3750 (58.1250) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [20/38] time 0.486 (0.469) data 0.355 (0.338) loss_x loss_x 1.0771 (1.5374) acc_x 78.1250 (59.6875) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [25/38] time 0.412 (0.471) data 0.281 (0.340) loss_x loss_x 1.3389 (1.5412) acc_x 68.7500 (59.6250) lr 1.9900e-03 eta 0:00:06
epoch [11/200] batch [30/38] time 0.472 (0.481) data 0.341 (0.350) loss_x loss_x 1.4502 (1.5041) acc_x 65.6250 (61.0417) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [35/38] time 0.372 (0.475) data 0.241 (0.344) loss_x loss_x 1.6396 (1.4906) acc_x 56.2500 (61.0714) lr 1.9900e-03 eta 0:00:01
epoch [11/200] batch [5/59] time 0.359 (0.470) data 0.228 (0.338) loss_u loss_u 0.8276 (0.8396) acc_u 31.2500 (22.5000) lr 1.9900e-03 eta 0:00:25
epoch [11/200] batch [10/59] time 0.557 (0.466) data 0.425 (0.334) loss_u loss_u 0.7515 (0.8367) acc_u 37.5000 (22.5000) lr 1.9900e-03 eta 0:00:22
epoch [11/200] batch [15/59] time 0.446 (0.463) data 0.314 (0.331) loss_u loss_u 0.8286 (0.8291) acc_u 21.8750 (23.7500) lr 1.9900e-03 eta 0:00:20
epoch [11/200] batch [20/59] time 0.356 (0.464) data 0.224 (0.333) loss_u loss_u 0.8198 (0.8278) acc_u 25.0000 (23.7500) lr 1.9900e-03 eta 0:00:18
epoch [11/200] batch [25/59] time 0.412 (0.462) data 0.282 (0.331) loss_u loss_u 0.8818 (0.8283) acc_u 18.7500 (23.7500) lr 1.9900e-03 eta 0:00:15
epoch [11/200] batch [30/59] time 0.322 (0.458) data 0.191 (0.327) loss_u loss_u 0.8687 (0.8221) acc_u 21.8750 (24.8958) lr 1.9900e-03 eta 0:00:13
epoch [11/200] batch [35/59] time 0.482 (0.456) data 0.349 (0.324) loss_u loss_u 0.8618 (0.8193) acc_u 18.7500 (25.0000) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [40/59] time 0.327 (0.454) data 0.195 (0.323) loss_u loss_u 0.8320 (0.8195) acc_u 25.0000 (24.8438) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [45/59] time 0.564 (0.454) data 0.433 (0.322) loss_u loss_u 0.8257 (0.8214) acc_u 18.7500 (24.5139) lr 1.9900e-03 eta 0:00:06
epoch [11/200] batch [50/59] time 0.755 (0.458) data 0.623 (0.327) loss_u loss_u 0.7656 (0.8221) acc_u 34.3750 (24.2500) lr 1.9900e-03 eta 0:00:04
epoch [11/200] batch [55/59] time 0.440 (0.457) data 0.307 (0.326) loss_u loss_u 0.8716 (0.8202) acc_u 15.6250 (24.5455) lr 1.9900e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1686
confident_label rate tensor(0.4129, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1295
clean true:1294
clean false:1
clean_rate:0.9992277992277993
noisy true:156
noisy false:1685
after delete: len(clean_dataset) 1295
after delete: len(noisy_dataset) 1841
epoch [12/200] batch [5/40] time 0.477 (0.466) data 0.345 (0.335) loss_x loss_x 1.6270 (1.4107) acc_x 59.3750 (63.1250) lr 1.9877e-03 eta 0:00:16
epoch [12/200] batch [10/40] time 0.567 (0.490) data 0.436 (0.358) loss_x loss_x 1.3428 (1.5134) acc_x 78.1250 (61.2500) lr 1.9877e-03 eta 0:00:14
epoch [12/200] batch [15/40] time 0.416 (0.474) data 0.285 (0.343) loss_x loss_x 1.2832 (1.4612) acc_x 62.5000 (62.9167) lr 1.9877e-03 eta 0:00:11
epoch [12/200] batch [20/40] time 0.735 (0.488) data 0.605 (0.357) loss_x loss_x 1.3926 (1.4372) acc_x 68.7500 (63.7500) lr 1.9877e-03 eta 0:00:09
epoch [12/200] batch [25/40] time 0.409 (0.487) data 0.278 (0.356) loss_x loss_x 1.2178 (1.4345) acc_x 59.3750 (63.5000) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [30/40] time 0.475 (0.483) data 0.343 (0.352) loss_x loss_x 1.4561 (1.4389) acc_x 71.8750 (63.0208) lr 1.9877e-03 eta 0:00:04
epoch [12/200] batch [35/40] time 0.367 (0.484) data 0.235 (0.353) loss_x loss_x 1.6924 (1.4415) acc_x 46.8750 (62.5893) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [40/40] time 0.585 (0.485) data 0.453 (0.353) loss_x loss_x 1.4717 (1.4298) acc_x 65.6250 (63.1250) lr 1.9877e-03 eta 0:00:00
epoch [12/200] batch [5/57] time 0.400 (0.480) data 0.268 (0.349) loss_u loss_u 0.7783 (0.8261) acc_u 31.2500 (21.8750) lr 1.9877e-03 eta 0:00:24
epoch [12/200] batch [10/57] time 0.370 (0.479) data 0.238 (0.347) loss_u loss_u 0.8037 (0.8178) acc_u 21.8750 (21.5625) lr 1.9877e-03 eta 0:00:22
epoch [12/200] batch [15/57] time 0.394 (0.472) data 0.262 (0.340) loss_u loss_u 0.7490 (0.8109) acc_u 43.7500 (24.5833) lr 1.9877e-03 eta 0:00:19
epoch [12/200] batch [20/57] time 0.404 (0.470) data 0.272 (0.338) loss_u loss_u 0.8188 (0.8166) acc_u 25.0000 (23.7500) lr 1.9877e-03 eta 0:00:17
epoch [12/200] batch [25/57] time 0.370 (0.463) data 0.239 (0.332) loss_u loss_u 0.7920 (0.8191) acc_u 31.2500 (23.8750) lr 1.9877e-03 eta 0:00:14
epoch [12/200] batch [30/57] time 0.418 (0.462) data 0.285 (0.330) loss_u loss_u 0.7896 (0.8171) acc_u 25.0000 (23.8542) lr 1.9877e-03 eta 0:00:12
epoch [12/200] batch [35/57] time 0.403 (0.456) data 0.271 (0.324) loss_u loss_u 0.8477 (0.8156) acc_u 15.6250 (24.1964) lr 1.9877e-03 eta 0:00:10
epoch [12/200] batch [40/57] time 0.516 (0.460) data 0.384 (0.328) loss_u loss_u 0.8442 (0.8198) acc_u 28.1250 (24.0625) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [45/57] time 0.467 (0.458) data 0.334 (0.326) loss_u loss_u 0.8086 (0.8215) acc_u 28.1250 (23.6111) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [50/57] time 0.397 (0.460) data 0.265 (0.328) loss_u loss_u 0.9102 (0.8207) acc_u 6.2500 (23.8750) lr 1.9877e-03 eta 0:00:03
epoch [12/200] batch [55/57] time 0.417 (0.464) data 0.285 (0.332) loss_u loss_u 0.8843 (0.8185) acc_u 12.5000 (23.9773) lr 1.9877e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1694
confident_label rate tensor(0.4133, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1296
clean true:1294
clean false:2
clean_rate:0.9984567901234568
noisy true:148
noisy false:1692
after delete: len(clean_dataset) 1296
after delete: len(noisy_dataset) 1840
epoch [13/200] batch [5/40] time 0.427 (0.462) data 0.297 (0.331) loss_x loss_x 1.6885 (1.5670) acc_x 53.1250 (58.1250) lr 1.9851e-03 eta 0:00:16
epoch [13/200] batch [10/40] time 0.455 (0.509) data 0.324 (0.378) loss_x loss_x 1.5332 (1.4865) acc_x 56.2500 (58.4375) lr 1.9851e-03 eta 0:00:15
epoch [13/200] batch [15/40] time 0.429 (0.487) data 0.298 (0.355) loss_x loss_x 1.6436 (1.4682) acc_x 50.0000 (60.2083) lr 1.9851e-03 eta 0:00:12
epoch [13/200] batch [20/40] time 0.446 (0.475) data 0.315 (0.343) loss_x loss_x 1.6621 (1.4653) acc_x 53.1250 (60.9375) lr 1.9851e-03 eta 0:00:09
epoch [13/200] batch [25/40] time 0.459 (0.472) data 0.328 (0.341) loss_x loss_x 1.3311 (1.4991) acc_x 59.3750 (60.7500) lr 1.9851e-03 eta 0:00:07
epoch [13/200] batch [30/40] time 0.395 (0.472) data 0.265 (0.341) loss_x loss_x 1.6992 (1.5181) acc_x 56.2500 (60.4167) lr 1.9851e-03 eta 0:00:04
epoch [13/200] batch [35/40] time 0.361 (0.476) data 0.230 (0.344) loss_x loss_x 1.3633 (1.4848) acc_x 68.7500 (61.7857) lr 1.9851e-03 eta 0:00:02
epoch [13/200] batch [40/40] time 0.641 (0.476) data 0.510 (0.345) loss_x loss_x 1.0283 (1.4802) acc_x 81.2500 (62.1875) lr 1.9851e-03 eta 0:00:00
epoch [13/200] batch [5/57] time 0.614 (0.477) data 0.481 (0.345) loss_u loss_u 0.8389 (0.8148) acc_u 18.7500 (23.1250) lr 1.9851e-03 eta 0:00:24
epoch [13/200] batch [10/57] time 0.409 (0.480) data 0.277 (0.349) loss_u loss_u 0.7495 (0.7947) acc_u 37.5000 (27.1875) lr 1.9851e-03 eta 0:00:22
epoch [13/200] batch [15/57] time 0.475 (0.479) data 0.342 (0.347) loss_u loss_u 0.7832 (0.7993) acc_u 31.2500 (25.6250) lr 1.9851e-03 eta 0:00:20
epoch [13/200] batch [20/57] time 0.434 (0.470) data 0.303 (0.339) loss_u loss_u 0.7979 (0.8004) acc_u 28.1250 (26.0938) lr 1.9851e-03 eta 0:00:17
epoch [13/200] batch [25/57] time 0.465 (0.470) data 0.333 (0.339) loss_u loss_u 0.8384 (0.8070) acc_u 18.7500 (25.0000) lr 1.9851e-03 eta 0:00:15
epoch [13/200] batch [30/57] time 0.430 (0.468) data 0.298 (0.337) loss_u loss_u 0.8350 (0.8095) acc_u 21.8750 (24.3750) lr 1.9851e-03 eta 0:00:12
epoch [13/200] batch [35/57] time 0.492 (0.466) data 0.359 (0.334) loss_u loss_u 0.8237 (0.8143) acc_u 28.1250 (23.9286) lr 1.9851e-03 eta 0:00:10
epoch [13/200] batch [40/57] time 0.482 (0.465) data 0.351 (0.334) loss_u loss_u 0.7964 (0.8126) acc_u 28.1250 (24.3750) lr 1.9851e-03 eta 0:00:07
epoch [13/200] batch [45/57] time 0.318 (0.460) data 0.186 (0.329) loss_u loss_u 0.7812 (0.8105) acc_u 21.8750 (24.5139) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [50/57] time 0.438 (0.459) data 0.306 (0.327) loss_u loss_u 0.7583 (0.8099) acc_u 31.2500 (24.7500) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [55/57] time 0.387 (0.458) data 0.255 (0.326) loss_u loss_u 0.8301 (0.8080) acc_u 21.8750 (25.0000) lr 1.9851e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1691
confident_label rate tensor(0.4107, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1288
clean true:1287
clean false:1
clean_rate:0.9992236024844721
noisy true:158
noisy false:1690
after delete: len(clean_dataset) 1288
after delete: len(noisy_dataset) 1848
epoch [14/200] batch [5/40] time 0.436 (0.410) data 0.305 (0.278) loss_x loss_x 1.4180 (1.5699) acc_x 50.0000 (57.5000) lr 1.9823e-03 eta 0:00:14
epoch [14/200] batch [10/40] time 0.425 (0.441) data 0.294 (0.309) loss_x loss_x 1.3848 (1.5526) acc_x 71.8750 (59.6875) lr 1.9823e-03 eta 0:00:13
epoch [14/200] batch [15/40] time 0.436 (0.446) data 0.306 (0.315) loss_x loss_x 1.4033 (1.5251) acc_x 62.5000 (60.4167) lr 1.9823e-03 eta 0:00:11
epoch [14/200] batch [20/40] time 0.350 (0.437) data 0.218 (0.306) loss_x loss_x 1.1084 (1.5444) acc_x 75.0000 (59.6875) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [25/40] time 0.441 (0.441) data 0.310 (0.309) loss_x loss_x 1.0654 (1.5308) acc_x 62.5000 (59.1250) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [30/40] time 0.474 (0.455) data 0.342 (0.324) loss_x loss_x 1.5410 (1.5124) acc_x 65.6250 (60.4167) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [35/40] time 0.473 (0.461) data 0.341 (0.330) loss_x loss_x 1.4004 (1.5004) acc_x 59.3750 (60.7143) lr 1.9823e-03 eta 0:00:02
epoch [14/200] batch [40/40] time 0.500 (0.463) data 0.369 (0.332) loss_x loss_x 1.3623 (1.4860) acc_x 62.5000 (61.0156) lr 1.9823e-03 eta 0:00:00
epoch [14/200] batch [5/57] time 0.448 (0.456) data 0.317 (0.325) loss_u loss_u 0.7368 (0.8302) acc_u 40.6250 (26.2500) lr 1.9823e-03 eta 0:00:23
epoch [14/200] batch [10/57] time 0.421 (0.454) data 0.289 (0.323) loss_u loss_u 0.7363 (0.8117) acc_u 34.3750 (27.1875) lr 1.9823e-03 eta 0:00:21
epoch [14/200] batch [15/57] time 0.521 (0.455) data 0.389 (0.324) loss_u loss_u 0.7520 (0.8128) acc_u 34.3750 (26.4583) lr 1.9823e-03 eta 0:00:19
epoch [14/200] batch [20/57] time 0.411 (0.454) data 0.280 (0.323) loss_u loss_u 0.7676 (0.8109) acc_u 31.2500 (25.6250) lr 1.9823e-03 eta 0:00:16
epoch [14/200] batch [25/57] time 0.533 (0.457) data 0.401 (0.326) loss_u loss_u 0.8403 (0.8182) acc_u 25.0000 (24.8750) lr 1.9823e-03 eta 0:00:14
epoch [14/200] batch [30/57] time 0.666 (0.460) data 0.534 (0.328) loss_u loss_u 0.8564 (0.8185) acc_u 15.6250 (24.5833) lr 1.9823e-03 eta 0:00:12
epoch [14/200] batch [35/57] time 0.429 (0.463) data 0.296 (0.331) loss_u loss_u 0.8301 (0.8105) acc_u 28.1250 (25.5357) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [40/57] time 0.409 (0.459) data 0.277 (0.328) loss_u loss_u 0.8262 (0.8102) acc_u 21.8750 (25.3906) lr 1.9823e-03 eta 0:00:07
epoch [14/200] batch [45/57] time 0.442 (0.459) data 0.311 (0.328) loss_u loss_u 0.7705 (0.8074) acc_u 34.3750 (26.1806) lr 1.9823e-03 eta 0:00:05
epoch [14/200] batch [50/57] time 0.431 (0.463) data 0.300 (0.331) loss_u loss_u 0.7925 (0.8082) acc_u 25.0000 (25.5625) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [55/57] time 0.470 (0.462) data 0.338 (0.330) loss_u loss_u 0.7451 (0.8067) acc_u 34.3750 (25.7386) lr 1.9823e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1666
confident_label rate tensor(0.4136, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1297
clean true:1296
clean false:1
clean_rate:0.9992289899768697
noisy true:174
noisy false:1665
after delete: len(clean_dataset) 1297
after delete: len(noisy_dataset) 1839
epoch [15/200] batch [5/40] time 0.508 (0.460) data 0.376 (0.328) loss_x loss_x 1.7412 (1.2615) acc_x 68.7500 (68.7500) lr 1.9792e-03 eta 0:00:16
epoch [15/200] batch [10/40] time 0.522 (0.481) data 0.391 (0.349) loss_x loss_x 1.3730 (1.2811) acc_x 59.3750 (66.8750) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [15/40] time 0.497 (0.481) data 0.366 (0.349) loss_x loss_x 1.2871 (1.3714) acc_x 56.2500 (64.1667) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [20/40] time 0.611 (0.492) data 0.478 (0.361) loss_x loss_x 1.2871 (1.3779) acc_x 71.8750 (64.3750) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [25/40] time 0.376 (0.484) data 0.244 (0.353) loss_x loss_x 1.3643 (1.3770) acc_x 65.6250 (64.5000) lr 1.9792e-03 eta 0:00:07
epoch [15/200] batch [30/40] time 0.348 (0.479) data 0.216 (0.347) loss_x loss_x 1.3096 (1.3944) acc_x 68.7500 (63.9583) lr 1.9792e-03 eta 0:00:04
epoch [15/200] batch [35/40] time 0.495 (0.483) data 0.363 (0.352) loss_x loss_x 1.0527 (1.3953) acc_x 68.7500 (63.4821) lr 1.9792e-03 eta 0:00:02
epoch [15/200] batch [40/40] time 0.477 (0.481) data 0.346 (0.349) loss_x loss_x 1.3643 (1.3992) acc_x 59.3750 (63.5938) lr 1.9792e-03 eta 0:00:00
epoch [15/200] batch [5/57] time 0.408 (0.473) data 0.275 (0.341) loss_u loss_u 0.8179 (0.8201) acc_u 28.1250 (23.1250) lr 1.9792e-03 eta 0:00:24
epoch [15/200] batch [10/57] time 0.364 (0.472) data 0.232 (0.340) loss_u loss_u 0.8271 (0.8137) acc_u 31.2500 (25.3125) lr 1.9792e-03 eta 0:00:22
epoch [15/200] batch [15/57] time 0.588 (0.468) data 0.455 (0.336) loss_u loss_u 0.8467 (0.8062) acc_u 21.8750 (26.6667) lr 1.9792e-03 eta 0:00:19
epoch [15/200] batch [20/57] time 0.374 (0.472) data 0.241 (0.340) loss_u loss_u 0.7798 (0.8068) acc_u 31.2500 (26.2500) lr 1.9792e-03 eta 0:00:17
epoch [15/200] batch [25/57] time 0.349 (0.466) data 0.216 (0.335) loss_u loss_u 0.8911 (0.8173) acc_u 9.3750 (24.2500) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [30/57] time 0.576 (0.466) data 0.444 (0.334) loss_u loss_u 0.7881 (0.8170) acc_u 28.1250 (24.7917) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [35/57] time 0.554 (0.466) data 0.422 (0.334) loss_u loss_u 0.8130 (0.8132) acc_u 25.0000 (25.0893) lr 1.9792e-03 eta 0:00:10
epoch [15/200] batch [40/57] time 0.386 (0.466) data 0.254 (0.334) loss_u loss_u 0.8789 (0.8158) acc_u 15.6250 (24.7656) lr 1.9792e-03 eta 0:00:07
epoch [15/200] batch [45/57] time 0.336 (0.462) data 0.205 (0.330) loss_u loss_u 0.7554 (0.8138) acc_u 34.3750 (25.1389) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [50/57] time 0.557 (0.462) data 0.425 (0.331) loss_u loss_u 0.7861 (0.8102) acc_u 34.3750 (25.8750) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [55/57] time 0.364 (0.460) data 0.232 (0.328) loss_u loss_u 0.8711 (0.8112) acc_u 12.5000 (25.5114) lr 1.9792e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1681
confident_label rate tensor(0.4110, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1289
clean true:1285
clean false:4
clean_rate:0.9968968192397207
noisy true:170
noisy false:1677
after delete: len(clean_dataset) 1289
after delete: len(noisy_dataset) 1847
epoch [16/200] batch [5/40] time 0.454 (0.518) data 0.322 (0.386) loss_x loss_x 1.4639 (1.0818) acc_x 68.7500 (75.6250) lr 1.9759e-03 eta 0:00:18
epoch [16/200] batch [10/40] time 0.561 (0.512) data 0.429 (0.380) loss_x loss_x 1.1924 (1.1814) acc_x 59.3750 (70.0000) lr 1.9759e-03 eta 0:00:15
epoch [16/200] batch [15/40] time 0.459 (0.507) data 0.328 (0.375) loss_x loss_x 0.6367 (1.1921) acc_x 81.2500 (68.5417) lr 1.9759e-03 eta 0:00:12
epoch [16/200] batch [20/40] time 0.579 (0.494) data 0.447 (0.362) loss_x loss_x 0.8730 (1.2488) acc_x 84.3750 (67.6562) lr 1.9759e-03 eta 0:00:09
epoch [16/200] batch [25/40] time 0.452 (0.491) data 0.321 (0.359) loss_x loss_x 1.7871 (1.3001) acc_x 59.3750 (66.5000) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [30/40] time 0.548 (0.495) data 0.414 (0.363) loss_x loss_x 1.4912 (1.3055) acc_x 53.1250 (66.4583) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [35/40] time 0.521 (0.498) data 0.389 (0.366) loss_x loss_x 1.8271 (1.3405) acc_x 56.2500 (65.4464) lr 1.9759e-03 eta 0:00:02
epoch [16/200] batch [40/40] time 0.460 (0.495) data 0.329 (0.363) loss_x loss_x 1.7197 (1.3658) acc_x 59.3750 (64.6875) lr 1.9759e-03 eta 0:00:00
epoch [16/200] batch [5/57] time 0.410 (0.494) data 0.278 (0.362) loss_u loss_u 0.8506 (0.8203) acc_u 21.8750 (26.2500) lr 1.9759e-03 eta 0:00:25
epoch [16/200] batch [10/57] time 0.528 (0.488) data 0.397 (0.357) loss_u loss_u 0.8203 (0.8160) acc_u 31.2500 (26.8750) lr 1.9759e-03 eta 0:00:22
epoch [16/200] batch [15/57] time 0.363 (0.485) data 0.231 (0.353) loss_u loss_u 0.8330 (0.8180) acc_u 18.7500 (25.8333) lr 1.9759e-03 eta 0:00:20
epoch [16/200] batch [20/57] time 0.390 (0.481) data 0.257 (0.349) loss_u loss_u 0.7285 (0.8209) acc_u 37.5000 (25.0000) lr 1.9759e-03 eta 0:00:17
epoch [16/200] batch [25/57] time 0.405 (0.478) data 0.273 (0.346) loss_u loss_u 0.8486 (0.8221) acc_u 18.7500 (24.2500) lr 1.9759e-03 eta 0:00:15
epoch [16/200] batch [30/57] time 0.407 (0.476) data 0.275 (0.344) loss_u loss_u 0.7192 (0.8157) acc_u 37.5000 (25.3125) lr 1.9759e-03 eta 0:00:12
epoch [16/200] batch [35/57] time 0.447 (0.475) data 0.315 (0.343) loss_u loss_u 0.7773 (0.8135) acc_u 28.1250 (25.3571) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [40/57] time 0.399 (0.473) data 0.266 (0.342) loss_u loss_u 0.7925 (0.8129) acc_u 25.0000 (25.5469) lr 1.9759e-03 eta 0:00:08
epoch [16/200] batch [45/57] time 0.677 (0.479) data 0.544 (0.347) loss_u loss_u 0.8779 (0.8133) acc_u 15.6250 (25.3472) lr 1.9759e-03 eta 0:00:05
epoch [16/200] batch [50/57] time 0.488 (0.477) data 0.356 (0.345) loss_u loss_u 0.8428 (0.8145) acc_u 18.7500 (25.0000) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [55/57] time 0.441 (0.476) data 0.309 (0.344) loss_u loss_u 0.8765 (0.8142) acc_u 18.7500 (25.1705) lr 1.9759e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1700
confident_label rate tensor(0.4110, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1289
clean true:1285
clean false:4
clean_rate:0.9968968192397207
noisy true:151
noisy false:1696
after delete: len(clean_dataset) 1289
after delete: len(noisy_dataset) 1847
epoch [17/200] batch [5/40] time 0.428 (0.463) data 0.297 (0.331) loss_x loss_x 1.8740 (1.4762) acc_x 53.1250 (61.8750) lr 1.9724e-03 eta 0:00:16
epoch [17/200] batch [10/40] time 0.450 (0.453) data 0.319 (0.321) loss_x loss_x 2.1289 (1.3926) acc_x 59.3750 (64.3750) lr 1.9724e-03 eta 0:00:13
epoch [17/200] batch [15/40] time 0.419 (0.449) data 0.287 (0.318) loss_x loss_x 1.3691 (1.4183) acc_x 71.8750 (63.1250) lr 1.9724e-03 eta 0:00:11
epoch [17/200] batch [20/40] time 0.443 (0.447) data 0.313 (0.316) loss_x loss_x 1.3936 (1.3803) acc_x 59.3750 (63.2812) lr 1.9724e-03 eta 0:00:08
epoch [17/200] batch [25/40] time 0.451 (0.442) data 0.321 (0.311) loss_x loss_x 1.5908 (1.4510) acc_x 56.2500 (61.0000) lr 1.9724e-03 eta 0:00:06
epoch [17/200] batch [30/40] time 0.607 (0.452) data 0.475 (0.320) loss_x loss_x 1.6270 (1.4858) acc_x 62.5000 (61.0417) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [35/40] time 0.611 (0.464) data 0.480 (0.333) loss_x loss_x 1.5693 (1.4991) acc_x 65.6250 (60.9821) lr 1.9724e-03 eta 0:00:02
epoch [17/200] batch [40/40] time 0.445 (0.466) data 0.314 (0.335) loss_x loss_x 1.1943 (1.4866) acc_x 62.5000 (61.4844) lr 1.9724e-03 eta 0:00:00
epoch [17/200] batch [5/57] time 0.352 (0.464) data 0.222 (0.333) loss_u loss_u 0.8667 (0.8060) acc_u 15.6250 (25.0000) lr 1.9724e-03 eta 0:00:24
epoch [17/200] batch [10/57] time 0.512 (0.462) data 0.380 (0.331) loss_u loss_u 0.7930 (0.8100) acc_u 31.2500 (25.3125) lr 1.9724e-03 eta 0:00:21
epoch [17/200] batch [15/57] time 0.434 (0.463) data 0.301 (0.331) loss_u loss_u 0.7378 (0.8038) acc_u 31.2500 (26.4583) lr 1.9724e-03 eta 0:00:19
epoch [17/200] batch [20/57] time 0.435 (0.462) data 0.302 (0.330) loss_u loss_u 0.7549 (0.7960) acc_u 34.3750 (27.6562) lr 1.9724e-03 eta 0:00:17
epoch [17/200] batch [25/57] time 0.399 (0.457) data 0.267 (0.326) loss_u loss_u 0.7329 (0.7974) acc_u 40.6250 (27.3750) lr 1.9724e-03 eta 0:00:14
epoch [17/200] batch [30/57] time 0.461 (0.453) data 0.330 (0.321) loss_u loss_u 0.7939 (0.7990) acc_u 31.2500 (27.2917) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [35/57] time 0.531 (0.453) data 0.398 (0.322) loss_u loss_u 0.8066 (0.7994) acc_u 25.0000 (26.8750) lr 1.9724e-03 eta 0:00:09
epoch [17/200] batch [40/57] time 0.384 (0.455) data 0.252 (0.323) loss_u loss_u 0.7783 (0.8002) acc_u 31.2500 (26.6406) lr 1.9724e-03 eta 0:00:07
epoch [17/200] batch [45/57] time 0.446 (0.452) data 0.315 (0.320) loss_u loss_u 0.7310 (0.7984) acc_u 28.1250 (26.5278) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [50/57] time 0.377 (0.448) data 0.245 (0.317) loss_u loss_u 0.8467 (0.7988) acc_u 18.7500 (26.6875) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [55/57] time 0.523 (0.451) data 0.392 (0.319) loss_u loss_u 0.7163 (0.8011) acc_u 37.5000 (26.3636) lr 1.9724e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1658
confident_label rate tensor(0.4187, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1313
clean true:1311
clean false:2
clean_rate:0.9984767707539984
noisy true:167
noisy false:1656
after delete: len(clean_dataset) 1313
after delete: len(noisy_dataset) 1823
epoch [18/200] batch [5/41] time 0.411 (0.474) data 0.278 (0.342) loss_x loss_x 1.0449 (1.2883) acc_x 71.8750 (71.2500) lr 1.9686e-03 eta 0:00:17
epoch [18/200] batch [10/41] time 0.567 (0.477) data 0.435 (0.345) loss_x loss_x 1.4111 (1.2873) acc_x 59.3750 (68.4375) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [15/41] time 0.494 (0.490) data 0.363 (0.358) loss_x loss_x 1.1729 (1.3276) acc_x 62.5000 (66.2500) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [20/41] time 0.546 (0.486) data 0.415 (0.355) loss_x loss_x 1.3428 (1.3384) acc_x 59.3750 (66.0938) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [25/41] time 0.371 (0.478) data 0.241 (0.347) loss_x loss_x 1.0000 (1.3372) acc_x 75.0000 (66.2500) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [30/41] time 0.496 (0.482) data 0.365 (0.351) loss_x loss_x 1.4756 (1.3779) acc_x 50.0000 (64.3750) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [35/41] time 0.392 (0.472) data 0.259 (0.341) loss_x loss_x 1.1270 (1.3951) acc_x 75.0000 (64.3750) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [40/41] time 0.538 (0.477) data 0.407 (0.346) loss_x loss_x 1.7930 (1.4071) acc_x 62.5000 (64.4531) lr 1.9686e-03 eta 0:00:00
epoch [18/200] batch [5/56] time 0.452 (0.475) data 0.320 (0.344) loss_u loss_u 0.8022 (0.8171) acc_u 25.0000 (24.3750) lr 1.9686e-03 eta 0:00:24
epoch [18/200] batch [10/56] time 0.560 (0.472) data 0.428 (0.341) loss_u loss_u 0.7959 (0.7980) acc_u 28.1250 (26.8750) lr 1.9686e-03 eta 0:00:21
epoch [18/200] batch [15/56] time 0.413 (0.467) data 0.281 (0.336) loss_u loss_u 0.8105 (0.8032) acc_u 21.8750 (26.0417) lr 1.9686e-03 eta 0:00:19
epoch [18/200] batch [20/56] time 0.410 (0.466) data 0.278 (0.335) loss_u loss_u 0.8198 (0.8074) acc_u 15.6250 (24.6875) lr 1.9686e-03 eta 0:00:16
epoch [18/200] batch [25/56] time 0.364 (0.462) data 0.231 (0.331) loss_u loss_u 0.7686 (0.8003) acc_u 34.3750 (25.3750) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [30/56] time 0.450 (0.463) data 0.318 (0.332) loss_u loss_u 0.8711 (0.8028) acc_u 15.6250 (25.2083) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [35/56] time 0.500 (0.462) data 0.367 (0.331) loss_u loss_u 0.7871 (0.8059) acc_u 31.2500 (25.2679) lr 1.9686e-03 eta 0:00:09
epoch [18/200] batch [40/56] time 0.483 (0.459) data 0.352 (0.327) loss_u loss_u 0.7505 (0.8070) acc_u 28.1250 (24.8438) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [45/56] time 0.656 (0.465) data 0.524 (0.333) loss_u loss_u 0.7896 (0.8035) acc_u 25.0000 (25.2083) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [50/56] time 0.536 (0.467) data 0.403 (0.336) loss_u loss_u 0.8643 (0.8033) acc_u 21.8750 (25.4375) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [55/56] time 0.341 (0.463) data 0.210 (0.331) loss_u loss_u 0.7720 (0.8052) acc_u 31.2500 (25.3977) lr 1.9686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1620
confident_label rate tensor(0.4305, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1350
clean true:1348
clean false:2
clean_rate:0.9985185185185185
noisy true:168
noisy false:1618
after delete: len(clean_dataset) 1350
after delete: len(noisy_dataset) 1786
epoch [19/200] batch [5/42] time 0.724 (0.514) data 0.592 (0.382) loss_x loss_x 1.7471 (1.3709) acc_x 59.3750 (63.7500) lr 1.9646e-03 eta 0:00:19
epoch [19/200] batch [10/42] time 0.464 (0.514) data 0.332 (0.382) loss_x loss_x 1.5166 (1.4143) acc_x 65.6250 (65.0000) lr 1.9646e-03 eta 0:00:16
epoch [19/200] batch [15/42] time 0.457 (0.508) data 0.326 (0.376) loss_x loss_x 1.2236 (1.3325) acc_x 68.7500 (67.0833) lr 1.9646e-03 eta 0:00:13
epoch [19/200] batch [20/42] time 0.522 (0.506) data 0.390 (0.375) loss_x loss_x 1.1455 (1.3557) acc_x 71.8750 (67.5000) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [25/42] time 0.514 (0.504) data 0.382 (0.372) loss_x loss_x 1.4697 (1.3733) acc_x 62.5000 (65.8750) lr 1.9646e-03 eta 0:00:08
epoch [19/200] batch [30/42] time 0.442 (0.502) data 0.310 (0.371) loss_x loss_x 1.2822 (1.3827) acc_x 71.8750 (65.7292) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [35/42] time 0.486 (0.502) data 0.355 (0.370) loss_x loss_x 1.2266 (1.4125) acc_x 68.7500 (65.4464) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [40/42] time 0.420 (0.492) data 0.288 (0.361) loss_x loss_x 1.8379 (1.4370) acc_x 53.1250 (65.0000) lr 1.9646e-03 eta 0:00:00
epoch [19/200] batch [5/55] time 0.499 (0.487) data 0.367 (0.355) loss_u loss_u 0.7959 (0.7941) acc_u 21.8750 (23.1250) lr 1.9646e-03 eta 0:00:24
epoch [19/200] batch [10/55] time 0.408 (0.483) data 0.276 (0.351) loss_u loss_u 0.8745 (0.8075) acc_u 18.7500 (23.7500) lr 1.9646e-03 eta 0:00:21
epoch [19/200] batch [15/55] time 0.357 (0.482) data 0.225 (0.350) loss_u loss_u 0.7998 (0.8055) acc_u 21.8750 (24.3750) lr 1.9646e-03 eta 0:00:19
epoch [19/200] batch [20/55] time 0.512 (0.481) data 0.379 (0.349) loss_u loss_u 0.8750 (0.8163) acc_u 12.5000 (22.5000) lr 1.9646e-03 eta 0:00:16
epoch [19/200] batch [25/55] time 0.401 (0.477) data 0.268 (0.345) loss_u loss_u 0.7812 (0.8181) acc_u 28.1250 (22.2500) lr 1.9646e-03 eta 0:00:14
epoch [19/200] batch [30/55] time 0.394 (0.477) data 0.261 (0.345) loss_u loss_u 0.7925 (0.8162) acc_u 31.2500 (23.0208) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [35/55] time 0.471 (0.475) data 0.339 (0.344) loss_u loss_u 0.8105 (0.8195) acc_u 28.1250 (23.0357) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [40/55] time 0.357 (0.472) data 0.225 (0.341) loss_u loss_u 0.8003 (0.8175) acc_u 25.0000 (23.2031) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [45/55] time 0.318 (0.470) data 0.186 (0.338) loss_u loss_u 0.8975 (0.8215) acc_u 15.6250 (22.7083) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [50/55] time 0.528 (0.471) data 0.395 (0.339) loss_u loss_u 0.8311 (0.8216) acc_u 28.1250 (22.5625) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [55/55] time 0.434 (0.465) data 0.302 (0.334) loss_u loss_u 0.7144 (0.8156) acc_u 28.1250 (23.2386) lr 1.9646e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1681
confident_label rate tensor(0.4158, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1304
clean true:1303
clean false:1
clean_rate:0.9992331288343558
noisy true:152
noisy false:1680
after delete: len(clean_dataset) 1304
after delete: len(noisy_dataset) 1832
epoch [20/200] batch [5/40] time 0.513 (0.467) data 0.382 (0.335) loss_x loss_x 1.0898 (1.4158) acc_x 71.8750 (63.7500) lr 1.9603e-03 eta 0:00:16
epoch [20/200] batch [10/40] time 0.401 (0.485) data 0.270 (0.354) loss_x loss_x 1.5615 (1.4018) acc_x 56.2500 (63.7500) lr 1.9603e-03 eta 0:00:14
epoch [20/200] batch [15/40] time 0.384 (0.476) data 0.254 (0.345) loss_x loss_x 1.5498 (1.4052) acc_x 56.2500 (63.3333) lr 1.9603e-03 eta 0:00:11
epoch [20/200] batch [20/40] time 0.509 (0.490) data 0.377 (0.358) loss_x loss_x 0.9028 (1.3476) acc_x 71.8750 (64.8438) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [25/40] time 0.586 (0.502) data 0.455 (0.371) loss_x loss_x 1.5527 (1.3737) acc_x 59.3750 (63.8750) lr 1.9603e-03 eta 0:00:07
epoch [20/200] batch [30/40] time 0.383 (0.491) data 0.251 (0.360) loss_x loss_x 1.1748 (1.3549) acc_x 65.6250 (63.9583) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [35/40] time 0.450 (0.496) data 0.319 (0.364) loss_x loss_x 1.0010 (1.3843) acc_x 71.8750 (63.6607) lr 1.9603e-03 eta 0:00:02
epoch [20/200] batch [40/40] time 0.520 (0.496) data 0.389 (0.365) loss_x loss_x 1.3086 (1.4011) acc_x 65.6250 (63.5938) lr 1.9603e-03 eta 0:00:00
epoch [20/200] batch [5/57] time 0.389 (0.492) data 0.256 (0.361) loss_u loss_u 0.8433 (0.7788) acc_u 21.8750 (28.1250) lr 1.9603e-03 eta 0:00:25
epoch [20/200] batch [10/57] time 0.434 (0.487) data 0.302 (0.356) loss_u loss_u 0.8062 (0.7860) acc_u 21.8750 (27.8125) lr 1.9603e-03 eta 0:00:22
epoch [20/200] batch [15/57] time 0.402 (0.483) data 0.270 (0.352) loss_u loss_u 0.7192 (0.7799) acc_u 40.6250 (29.3750) lr 1.9603e-03 eta 0:00:20
epoch [20/200] batch [20/57] time 0.467 (0.476) data 0.336 (0.345) loss_u loss_u 0.7827 (0.7841) acc_u 31.2500 (29.0625) lr 1.9603e-03 eta 0:00:17
epoch [20/200] batch [25/57] time 0.466 (0.474) data 0.334 (0.342) loss_u loss_u 0.7363 (0.7848) acc_u 31.2500 (29.0000) lr 1.9603e-03 eta 0:00:15
epoch [20/200] batch [30/57] time 0.410 (0.473) data 0.279 (0.341) loss_u loss_u 0.8657 (0.7895) acc_u 12.5000 (28.0208) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [35/57] time 0.644 (0.475) data 0.512 (0.343) loss_u loss_u 0.8071 (0.7951) acc_u 28.1250 (27.3214) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [40/57] time 0.347 (0.474) data 0.215 (0.342) loss_u loss_u 0.8145 (0.7972) acc_u 18.7500 (26.6406) lr 1.9603e-03 eta 0:00:08
epoch [20/200] batch [45/57] time 0.460 (0.470) data 0.328 (0.338) loss_u loss_u 0.8398 (0.8005) acc_u 21.8750 (26.3194) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [50/57] time 0.522 (0.470) data 0.391 (0.338) loss_u loss_u 0.8032 (0.7999) acc_u 28.1250 (26.4375) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [55/57] time 0.460 (0.470) data 0.329 (0.338) loss_u loss_u 0.8267 (0.8019) acc_u 25.0000 (26.2500) lr 1.9603e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1650
confident_label rate tensor(0.4260, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1336
clean true:1335
clean false:1
clean_rate:0.999251497005988
noisy true:151
noisy false:1649
after delete: len(clean_dataset) 1336
after delete: len(noisy_dataset) 1800
epoch [21/200] batch [5/41] time 0.400 (0.552) data 0.269 (0.421) loss_x loss_x 1.1416 (1.4195) acc_x 71.8750 (65.6250) lr 1.9558e-03 eta 0:00:19
epoch [21/200] batch [10/41] time 0.524 (0.497) data 0.392 (0.365) loss_x loss_x 0.9780 (1.4263) acc_x 71.8750 (64.3750) lr 1.9558e-03 eta 0:00:15
epoch [21/200] batch [15/41] time 0.525 (0.500) data 0.394 (0.368) loss_x loss_x 1.0889 (1.3435) acc_x 68.7500 (64.5833) lr 1.9558e-03 eta 0:00:13
epoch [21/200] batch [20/41] time 0.484 (0.497) data 0.352 (0.365) loss_x loss_x 1.8857 (1.3908) acc_x 53.1250 (64.0625) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [25/41] time 0.497 (0.494) data 0.365 (0.363) loss_x loss_x 1.2871 (1.3607) acc_x 59.3750 (64.1250) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [30/41] time 0.530 (0.493) data 0.398 (0.361) loss_x loss_x 1.4717 (1.3489) acc_x 59.3750 (64.7917) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [35/41] time 0.435 (0.495) data 0.303 (0.364) loss_x loss_x 1.1230 (1.3543) acc_x 68.7500 (64.8214) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [40/41] time 0.414 (0.493) data 0.282 (0.361) loss_x loss_x 1.9111 (1.3639) acc_x 53.1250 (65.0781) lr 1.9558e-03 eta 0:00:00
epoch [21/200] batch [5/56] time 0.453 (0.489) data 0.322 (0.358) loss_u loss_u 0.7656 (0.8170) acc_u 37.5000 (21.8750) lr 1.9558e-03 eta 0:00:24
epoch [21/200] batch [10/56] time 0.391 (0.481) data 0.259 (0.350) loss_u loss_u 0.9258 (0.8136) acc_u 9.3750 (23.1250) lr 1.9558e-03 eta 0:00:22
epoch [21/200] batch [15/56] time 0.462 (0.480) data 0.329 (0.349) loss_u loss_u 0.8188 (0.8043) acc_u 25.0000 (25.4167) lr 1.9558e-03 eta 0:00:19
epoch [21/200] batch [20/56] time 0.587 (0.483) data 0.454 (0.351) loss_u loss_u 0.8726 (0.8054) acc_u 12.5000 (25.4688) lr 1.9558e-03 eta 0:00:17
epoch [21/200] batch [25/56] time 0.499 (0.486) data 0.367 (0.355) loss_u loss_u 0.8535 (0.8047) acc_u 28.1250 (26.5000) lr 1.9558e-03 eta 0:00:15
epoch [21/200] batch [30/56] time 0.326 (0.485) data 0.194 (0.353) loss_u loss_u 0.8481 (0.8048) acc_u 18.7500 (26.0417) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [35/56] time 0.521 (0.483) data 0.388 (0.351) loss_u loss_u 0.7534 (0.8043) acc_u 37.5000 (26.2500) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [40/56] time 0.368 (0.478) data 0.237 (0.346) loss_u loss_u 0.8989 (0.8017) acc_u 15.6250 (26.4844) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [45/56] time 0.346 (0.478) data 0.215 (0.346) loss_u loss_u 0.8228 (0.8047) acc_u 21.8750 (26.0417) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [50/56] time 0.442 (0.479) data 0.310 (0.347) loss_u loss_u 0.7900 (0.8054) acc_u 31.2500 (25.7500) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [55/56] time 0.524 (0.480) data 0.391 (0.348) loss_u loss_u 0.8438 (0.8067) acc_u 28.1250 (25.4545) lr 1.9558e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1635
confident_label rate tensor(0.4289, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1345
clean true:1343
clean false:2
clean_rate:0.9985130111524163
noisy true:158
noisy false:1633
after delete: len(clean_dataset) 1345
after delete: len(noisy_dataset) 1791
epoch [22/200] batch [5/42] time 0.603 (0.506) data 0.471 (0.374) loss_x loss_x 1.5596 (1.3744) acc_x 59.3750 (63.7500) lr 1.9511e-03 eta 0:00:18
epoch [22/200] batch [10/42] time 0.555 (0.507) data 0.424 (0.375) loss_x loss_x 1.5107 (1.3246) acc_x 53.1250 (64.3750) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [15/42] time 0.387 (0.498) data 0.255 (0.366) loss_x loss_x 2.0195 (1.3446) acc_x 46.8750 (63.5417) lr 1.9511e-03 eta 0:00:13
epoch [22/200] batch [20/42] time 0.533 (0.515) data 0.401 (0.383) loss_x loss_x 2.0273 (1.3618) acc_x 46.8750 (62.9688) lr 1.9511e-03 eta 0:00:11
epoch [22/200] batch [25/42] time 0.397 (0.499) data 0.266 (0.368) loss_x loss_x 1.8213 (1.3977) acc_x 59.3750 (62.5000) lr 1.9511e-03 eta 0:00:08
epoch [22/200] batch [30/42] time 0.451 (0.498) data 0.319 (0.366) loss_x loss_x 2.2578 (1.4211) acc_x 46.8750 (62.3958) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [35/42] time 0.424 (0.505) data 0.293 (0.373) loss_x loss_x 1.3887 (1.4109) acc_x 78.1250 (62.6786) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [40/42] time 0.417 (0.497) data 0.285 (0.365) loss_x loss_x 1.2461 (1.3997) acc_x 53.1250 (62.6562) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/55] time 0.473 (0.487) data 0.341 (0.355) loss_u loss_u 0.7222 (0.8025) acc_u 43.7500 (27.5000) lr 1.9511e-03 eta 0:00:24
epoch [22/200] batch [10/55] time 0.369 (0.490) data 0.236 (0.359) loss_u loss_u 0.8501 (0.8078) acc_u 25.0000 (26.2500) lr 1.9511e-03 eta 0:00:22
epoch [22/200] batch [15/55] time 0.394 (0.486) data 0.261 (0.354) loss_u loss_u 0.7334 (0.8035) acc_u 43.7500 (27.9167) lr 1.9511e-03 eta 0:00:19
epoch [22/200] batch [20/55] time 0.383 (0.485) data 0.251 (0.353) loss_u loss_u 0.7886 (0.7939) acc_u 28.1250 (28.7500) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [25/55] time 0.495 (0.480) data 0.363 (0.348) loss_u loss_u 0.8604 (0.7983) acc_u 12.5000 (27.1250) lr 1.9511e-03 eta 0:00:14
epoch [22/200] batch [30/55] time 0.413 (0.481) data 0.280 (0.349) loss_u loss_u 0.8247 (0.8001) acc_u 15.6250 (26.3542) lr 1.9511e-03 eta 0:00:12
epoch [22/200] batch [35/55] time 0.407 (0.477) data 0.275 (0.345) loss_u loss_u 0.7144 (0.7971) acc_u 31.2500 (26.6071) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [40/55] time 0.420 (0.477) data 0.288 (0.345) loss_u loss_u 0.8721 (0.7960) acc_u 6.2500 (26.4844) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [45/55] time 0.474 (0.477) data 0.342 (0.345) loss_u loss_u 0.7798 (0.7979) acc_u 25.0000 (25.9028) lr 1.9511e-03 eta 0:00:04
epoch [22/200] batch [50/55] time 0.387 (0.474) data 0.254 (0.342) loss_u loss_u 0.8345 (0.7993) acc_u 15.6250 (25.2500) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [55/55] time 0.442 (0.472) data 0.311 (0.340) loss_u loss_u 0.8232 (0.7999) acc_u 21.8750 (25.1705) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1630
confident_label rate tensor(0.4276, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1341
clean true:1339
clean false:2
clean_rate:0.9985085756897838
noisy true:167
noisy false:1628
after delete: len(clean_dataset) 1341
after delete: len(noisy_dataset) 1795
epoch [23/200] batch [5/41] time 0.581 (0.509) data 0.449 (0.377) loss_x loss_x 0.9766 (1.3896) acc_x 75.0000 (67.5000) lr 1.9461e-03 eta 0:00:18
epoch [23/200] batch [10/41] time 0.387 (0.486) data 0.256 (0.354) loss_x loss_x 1.7861 (1.4885) acc_x 56.2500 (66.2500) lr 1.9461e-03 eta 0:00:15
epoch [23/200] batch [15/41] time 0.374 (0.473) data 0.242 (0.341) loss_x loss_x 1.6201 (1.4299) acc_x 65.6250 (66.8750) lr 1.9461e-03 eta 0:00:12
epoch [23/200] batch [20/41] time 0.483 (0.468) data 0.351 (0.337) loss_x loss_x 1.7568 (1.4738) acc_x 50.0000 (65.4688) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [25/41] time 0.491 (0.473) data 0.360 (0.341) loss_x loss_x 2.0547 (1.4770) acc_x 50.0000 (64.6250) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [30/41] time 0.433 (0.466) data 0.302 (0.334) loss_x loss_x 1.4209 (1.5025) acc_x 53.1250 (63.4375) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [35/41] time 0.479 (0.469) data 0.347 (0.338) loss_x loss_x 1.4580 (1.4828) acc_x 56.2500 (63.3929) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [40/41] time 0.506 (0.473) data 0.374 (0.342) loss_x loss_x 0.9893 (1.4527) acc_x 71.8750 (63.1250) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/56] time 0.467 (0.469) data 0.335 (0.337) loss_u loss_u 0.8833 (0.7647) acc_u 18.7500 (31.2500) lr 1.9461e-03 eta 0:00:23
epoch [23/200] batch [10/56] time 0.626 (0.478) data 0.495 (0.347) loss_u loss_u 0.7837 (0.7813) acc_u 31.2500 (29.6875) lr 1.9461e-03 eta 0:00:22
epoch [23/200] batch [15/56] time 0.391 (0.473) data 0.259 (0.341) loss_u loss_u 0.7480 (0.7789) acc_u 28.1250 (28.3333) lr 1.9461e-03 eta 0:00:19
epoch [23/200] batch [20/56] time 0.430 (0.469) data 0.297 (0.337) loss_u loss_u 0.7681 (0.7875) acc_u 31.2500 (26.7188) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [25/56] time 0.364 (0.463) data 0.231 (0.331) loss_u loss_u 0.8936 (0.7941) acc_u 21.8750 (26.0000) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [30/56] time 0.632 (0.464) data 0.501 (0.332) loss_u loss_u 0.8027 (0.8006) acc_u 25.0000 (25.3125) lr 1.9461e-03 eta 0:00:12
epoch [23/200] batch [35/56] time 0.420 (0.469) data 0.287 (0.337) loss_u loss_u 0.8696 (0.7990) acc_u 12.5000 (25.9821) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [40/56] time 0.661 (0.474) data 0.528 (0.342) loss_u loss_u 0.7769 (0.7998) acc_u 31.2500 (25.7812) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [45/56] time 0.468 (0.471) data 0.336 (0.339) loss_u loss_u 0.8584 (0.8011) acc_u 21.8750 (25.8333) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [50/56] time 0.382 (0.470) data 0.250 (0.338) loss_u loss_u 0.8369 (0.8051) acc_u 21.8750 (25.3125) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [55/56] time 0.459 (0.467) data 0.327 (0.335) loss_u loss_u 0.8701 (0.8068) acc_u 18.7500 (25.1136) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1601
confident_label rate tensor(0.4416, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1385
clean true:1381
clean false:4
clean_rate:0.9971119133574007
noisy true:154
noisy false:1597
after delete: len(clean_dataset) 1385
after delete: len(noisy_dataset) 1751
epoch [24/200] batch [5/43] time 0.385 (0.473) data 0.254 (0.341) loss_x loss_x 1.8311 (1.5838) acc_x 62.5000 (59.3750) lr 1.9409e-03 eta 0:00:17
epoch [24/200] batch [10/43] time 0.550 (0.490) data 0.418 (0.358) loss_x loss_x 1.8555 (1.5831) acc_x 53.1250 (59.3750) lr 1.9409e-03 eta 0:00:16
epoch [24/200] batch [15/43] time 0.413 (0.471) data 0.282 (0.340) loss_x loss_x 1.1699 (1.5327) acc_x 68.7500 (59.7917) lr 1.9409e-03 eta 0:00:13
epoch [24/200] batch [20/43] time 0.464 (0.470) data 0.332 (0.339) loss_x loss_x 1.4570 (1.5196) acc_x 68.7500 (60.7812) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [25/43] time 0.507 (0.474) data 0.377 (0.342) loss_x loss_x 1.0635 (1.4547) acc_x 71.8750 (62.2500) lr 1.9409e-03 eta 0:00:08
epoch [24/200] batch [30/43] time 0.506 (0.472) data 0.374 (0.341) loss_x loss_x 1.8945 (1.4651) acc_x 56.2500 (62.0833) lr 1.9409e-03 eta 0:00:06
epoch [24/200] batch [35/43] time 0.601 (0.482) data 0.468 (0.350) loss_x loss_x 1.5186 (1.4582) acc_x 62.5000 (62.2321) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [40/43] time 0.437 (0.481) data 0.305 (0.349) loss_x loss_x 1.9395 (1.4565) acc_x 59.3750 (62.3438) lr 1.9409e-03 eta 0:00:01
epoch [24/200] batch [5/54] time 0.618 (0.483) data 0.487 (0.352) loss_u loss_u 0.8501 (0.8099) acc_u 15.6250 (25.6250) lr 1.9409e-03 eta 0:00:23
epoch [24/200] batch [10/54] time 0.458 (0.479) data 0.326 (0.347) loss_u loss_u 0.7856 (0.7674) acc_u 21.8750 (29.6875) lr 1.9409e-03 eta 0:00:21
epoch [24/200] batch [15/54] time 0.364 (0.472) data 0.232 (0.340) loss_u loss_u 0.8369 (0.7896) acc_u 21.8750 (26.2500) lr 1.9409e-03 eta 0:00:18
epoch [24/200] batch [20/54] time 0.454 (0.472) data 0.323 (0.340) loss_u loss_u 0.8579 (0.8011) acc_u 15.6250 (24.5312) lr 1.9409e-03 eta 0:00:16
epoch [24/200] batch [25/54] time 0.523 (0.474) data 0.390 (0.342) loss_u loss_u 0.8242 (0.8079) acc_u 25.0000 (23.7500) lr 1.9409e-03 eta 0:00:13
epoch [24/200] batch [30/54] time 0.350 (0.471) data 0.218 (0.339) loss_u loss_u 0.8369 (0.8093) acc_u 15.6250 (23.0208) lr 1.9409e-03 eta 0:00:11
epoch [24/200] batch [35/54] time 0.431 (0.470) data 0.298 (0.338) loss_u loss_u 0.8857 (0.8086) acc_u 18.7500 (23.5714) lr 1.9409e-03 eta 0:00:08
epoch [24/200] batch [40/54] time 0.398 (0.468) data 0.266 (0.336) loss_u loss_u 0.8125 (0.8066) acc_u 21.8750 (24.2969) lr 1.9409e-03 eta 0:00:06
epoch [24/200] batch [45/54] time 0.363 (0.468) data 0.231 (0.336) loss_u loss_u 0.7866 (0.8062) acc_u 25.0000 (24.0972) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [50/54] time 0.496 (0.469) data 0.363 (0.337) loss_u loss_u 0.8032 (0.8050) acc_u 21.8750 (24.4375) lr 1.9409e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1603
confident_label rate tensor(0.4385, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1375
clean true:1373
clean false:2
clean_rate:0.9985454545454545
noisy true:160
noisy false:1601
after delete: len(clean_dataset) 1375
after delete: len(noisy_dataset) 1761
epoch [25/200] batch [5/42] time 0.546 (0.489) data 0.415 (0.358) loss_x loss_x 1.0869 (1.2780) acc_x 78.1250 (70.0000) lr 1.9354e-03 eta 0:00:18
epoch [25/200] batch [10/42] time 0.359 (0.464) data 0.227 (0.332) loss_x loss_x 1.5332 (1.2162) acc_x 65.6250 (70.9375) lr 1.9354e-03 eta 0:00:14
epoch [25/200] batch [15/42] time 0.511 (0.451) data 0.379 (0.319) loss_x loss_x 1.2363 (1.3042) acc_x 65.6250 (69.3750) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [20/42] time 0.472 (0.453) data 0.341 (0.321) loss_x loss_x 1.2441 (1.3541) acc_x 62.5000 (68.4375) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [25/42] time 0.427 (0.448) data 0.296 (0.316) loss_x loss_x 1.5684 (1.3433) acc_x 53.1250 (68.5000) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [30/42] time 0.577 (0.457) data 0.445 (0.325) loss_x loss_x 1.6094 (1.3549) acc_x 53.1250 (67.5000) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [35/42] time 0.548 (0.465) data 0.416 (0.333) loss_x loss_x 1.1963 (1.3349) acc_x 65.6250 (66.9643) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [40/42] time 0.561 (0.469) data 0.429 (0.337) loss_x loss_x 1.7812 (1.3516) acc_x 53.1250 (66.0938) lr 1.9354e-03 eta 0:00:00
epoch [25/200] batch [5/55] time 0.592 (0.464) data 0.460 (0.332) loss_u loss_u 0.7925 (0.7914) acc_u 25.0000 (25.6250) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [10/55] time 0.392 (0.457) data 0.260 (0.325) loss_u loss_u 0.8154 (0.7920) acc_u 21.8750 (26.5625) lr 1.9354e-03 eta 0:00:20
epoch [25/200] batch [15/55] time 0.405 (0.455) data 0.272 (0.324) loss_u loss_u 0.7495 (0.7901) acc_u 34.3750 (26.4583) lr 1.9354e-03 eta 0:00:18
epoch [25/200] batch [20/55] time 0.455 (0.453) data 0.322 (0.321) loss_u loss_u 0.8423 (0.7913) acc_u 18.7500 (26.4062) lr 1.9354e-03 eta 0:00:15
epoch [25/200] batch [25/55] time 0.514 (0.451) data 0.380 (0.319) loss_u loss_u 0.8428 (0.8008) acc_u 12.5000 (25.1250) lr 1.9354e-03 eta 0:00:13
epoch [25/200] batch [30/55] time 0.430 (0.454) data 0.298 (0.322) loss_u loss_u 0.8667 (0.8092) acc_u 12.5000 (24.0625) lr 1.9354e-03 eta 0:00:11
epoch [25/200] batch [35/55] time 0.411 (0.453) data 0.279 (0.321) loss_u loss_u 0.8926 (0.8095) acc_u 12.5000 (24.5536) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [40/55] time 0.374 (0.452) data 0.242 (0.320) loss_u loss_u 0.8096 (0.8112) acc_u 28.1250 (24.6875) lr 1.9354e-03 eta 0:00:06
epoch [25/200] batch [45/55] time 0.500 (0.454) data 0.370 (0.322) loss_u loss_u 0.7861 (0.8102) acc_u 28.1250 (24.7917) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [50/55] time 0.450 (0.454) data 0.319 (0.322) loss_u loss_u 0.7754 (0.8071) acc_u 25.0000 (24.9375) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [55/55] time 0.667 (0.455) data 0.536 (0.324) loss_u loss_u 0.7920 (0.8088) acc_u 28.1250 (24.5455) lr 1.9354e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1569
confident_label rate tensor(0.4452, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1396
clean true:1395
clean false:1
clean_rate:0.9992836676217765
noisy true:172
noisy false:1568
after delete: len(clean_dataset) 1396
after delete: len(noisy_dataset) 1740
epoch [26/200] batch [5/43] time 0.445 (0.504) data 0.314 (0.373) loss_x loss_x 1.6621 (1.3854) acc_x 65.6250 (64.3750) lr 1.9298e-03 eta 0:00:19
epoch [26/200] batch [10/43] time 0.623 (0.504) data 0.492 (0.373) loss_x loss_x 1.2754 (1.2275) acc_x 62.5000 (66.8750) lr 1.9298e-03 eta 0:00:16
epoch [26/200] batch [15/43] time 0.458 (0.513) data 0.327 (0.382) loss_x loss_x 0.7822 (1.1916) acc_x 84.3750 (68.1250) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [20/43] time 0.398 (0.505) data 0.268 (0.374) loss_x loss_x 1.5928 (1.2624) acc_x 50.0000 (66.0938) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [25/43] time 0.476 (0.512) data 0.345 (0.381) loss_x loss_x 1.3135 (1.3104) acc_x 56.2500 (65.0000) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [30/43] time 0.579 (0.508) data 0.447 (0.377) loss_x loss_x 1.2754 (1.3120) acc_x 68.7500 (65.1042) lr 1.9298e-03 eta 0:00:06
epoch [26/200] batch [35/43] time 0.369 (0.497) data 0.238 (0.366) loss_x loss_x 1.4102 (1.3067) acc_x 71.8750 (65.6250) lr 1.9298e-03 eta 0:00:03
epoch [26/200] batch [40/43] time 0.491 (0.498) data 0.360 (0.367) loss_x loss_x 1.4775 (1.3052) acc_x 62.5000 (65.4688) lr 1.9298e-03 eta 0:00:01
epoch [26/200] batch [5/54] time 0.365 (0.481) data 0.234 (0.349) loss_u loss_u 0.8008 (0.8174) acc_u 28.1250 (26.8750) lr 1.9298e-03 eta 0:00:23
epoch [26/200] batch [10/54] time 0.379 (0.478) data 0.248 (0.347) loss_u loss_u 0.8198 (0.8206) acc_u 28.1250 (26.5625) lr 1.9298e-03 eta 0:00:21
epoch [26/200] batch [15/54] time 0.458 (0.474) data 0.326 (0.343) loss_u loss_u 0.8315 (0.8052) acc_u 15.6250 (27.5000) lr 1.9298e-03 eta 0:00:18
epoch [26/200] batch [20/54] time 0.384 (0.471) data 0.253 (0.339) loss_u loss_u 0.8066 (0.8133) acc_u 25.0000 (26.0938) lr 1.9298e-03 eta 0:00:15
epoch [26/200] batch [25/54] time 0.408 (0.469) data 0.276 (0.337) loss_u loss_u 0.8218 (0.8116) acc_u 18.7500 (26.0000) lr 1.9298e-03 eta 0:00:13
epoch [26/200] batch [30/54] time 0.448 (0.470) data 0.316 (0.338) loss_u loss_u 0.8481 (0.8138) acc_u 21.8750 (25.2083) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [35/54] time 0.486 (0.469) data 0.353 (0.337) loss_u loss_u 0.7881 (0.8143) acc_u 34.3750 (25.3571) lr 1.9298e-03 eta 0:00:08
epoch [26/200] batch [40/54] time 0.463 (0.469) data 0.331 (0.337) loss_u loss_u 0.7998 (0.8100) acc_u 21.8750 (25.7812) lr 1.9298e-03 eta 0:00:06
epoch [26/200] batch [45/54] time 0.653 (0.472) data 0.520 (0.340) loss_u loss_u 0.7925 (0.8091) acc_u 31.2500 (25.8333) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [50/54] time 0.414 (0.473) data 0.283 (0.341) loss_u loss_u 0.7285 (0.8062) acc_u 28.1250 (25.7500) lr 1.9298e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1551
confident_label rate tensor(0.4490, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1408
clean true:1406
clean false:2
clean_rate:0.9985795454545454
noisy true:179
noisy false:1549
after delete: len(clean_dataset) 1408
after delete: len(noisy_dataset) 1728
epoch [27/200] batch [5/44] time 0.441 (0.459) data 0.309 (0.327) loss_x loss_x 1.7344 (1.3883) acc_x 46.8750 (62.5000) lr 1.9239e-03 eta 0:00:17
epoch [27/200] batch [10/44] time 0.448 (0.450) data 0.317 (0.319) loss_x loss_x 1.2773 (1.4782) acc_x 65.6250 (61.2500) lr 1.9239e-03 eta 0:00:15
epoch [27/200] batch [15/44] time 0.393 (0.444) data 0.262 (0.313) loss_x loss_x 1.1689 (1.4606) acc_x 68.7500 (62.9167) lr 1.9239e-03 eta 0:00:12
epoch [27/200] batch [20/44] time 0.394 (0.447) data 0.263 (0.316) loss_x loss_x 1.5986 (1.4019) acc_x 56.2500 (63.4375) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [25/44] time 0.379 (0.445) data 0.247 (0.314) loss_x loss_x 1.1504 (1.3738) acc_x 68.7500 (63.7500) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [30/44] time 0.514 (0.458) data 0.382 (0.326) loss_x loss_x 1.2314 (1.3721) acc_x 68.7500 (63.5417) lr 1.9239e-03 eta 0:00:06
epoch [27/200] batch [35/44] time 0.490 (0.457) data 0.358 (0.326) loss_x loss_x 1.4131 (1.3885) acc_x 65.6250 (63.3929) lr 1.9239e-03 eta 0:00:04
epoch [27/200] batch [40/44] time 0.537 (0.459) data 0.405 (0.327) loss_x loss_x 1.0068 (1.4021) acc_x 68.7500 (63.2812) lr 1.9239e-03 eta 0:00:01
epoch [27/200] batch [5/54] time 0.496 (0.461) data 0.365 (0.329) loss_u loss_u 0.7773 (0.8164) acc_u 25.0000 (20.6250) lr 1.9239e-03 eta 0:00:22
epoch [27/200] batch [10/54] time 0.453 (0.463) data 0.321 (0.332) loss_u loss_u 0.8145 (0.7966) acc_u 25.0000 (25.6250) lr 1.9239e-03 eta 0:00:20
epoch [27/200] batch [15/54] time 0.429 (0.463) data 0.297 (0.331) loss_u loss_u 0.7803 (0.7869) acc_u 34.3750 (28.3333) lr 1.9239e-03 eta 0:00:18
epoch [27/200] batch [20/54] time 0.564 (0.463) data 0.433 (0.331) loss_u loss_u 0.7910 (0.7868) acc_u 28.1250 (28.7500) lr 1.9239e-03 eta 0:00:15
epoch [27/200] batch [25/54] time 0.499 (0.466) data 0.366 (0.334) loss_u loss_u 0.8003 (0.7922) acc_u 25.0000 (27.6250) lr 1.9239e-03 eta 0:00:13
epoch [27/200] batch [30/54] time 0.398 (0.461) data 0.266 (0.329) loss_u loss_u 0.8442 (0.7955) acc_u 21.8750 (26.7708) lr 1.9239e-03 eta 0:00:11
epoch [27/200] batch [35/54] time 0.393 (0.458) data 0.261 (0.326) loss_u loss_u 0.7744 (0.7953) acc_u 31.2500 (26.7857) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [40/54] time 0.401 (0.462) data 0.269 (0.330) loss_u loss_u 0.8018 (0.7973) acc_u 25.0000 (26.4844) lr 1.9239e-03 eta 0:00:06
epoch [27/200] batch [45/54] time 0.479 (0.464) data 0.347 (0.332) loss_u loss_u 0.8345 (0.8019) acc_u 21.8750 (25.9028) lr 1.9239e-03 eta 0:00:04
epoch [27/200] batch [50/54] time 0.391 (0.462) data 0.258 (0.330) loss_u loss_u 0.7236 (0.8004) acc_u 31.2500 (25.9375) lr 1.9239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1584
confident_label rate tensor(0.4448, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1395
clean true:1394
clean false:1
clean_rate:0.9992831541218637
noisy true:158
noisy false:1583
after delete: len(clean_dataset) 1395
after delete: len(noisy_dataset) 1741
epoch [28/200] batch [5/43] time 0.409 (0.470) data 0.279 (0.338) loss_x loss_x 1.1816 (1.1331) acc_x 78.1250 (72.5000) lr 1.9178e-03 eta 0:00:17
epoch [28/200] batch [10/43] time 0.444 (0.509) data 0.313 (0.377) loss_x loss_x 1.3359 (1.2192) acc_x 65.6250 (69.6875) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [15/43] time 0.376 (0.473) data 0.244 (0.342) loss_x loss_x 0.9731 (1.1859) acc_x 71.8750 (68.7500) lr 1.9178e-03 eta 0:00:13
epoch [28/200] batch [20/43] time 0.432 (0.468) data 0.301 (0.336) loss_x loss_x 1.1973 (1.1553) acc_x 71.8750 (69.6875) lr 1.9178e-03 eta 0:00:10
epoch [28/200] batch [25/43] time 0.393 (0.462) data 0.262 (0.331) loss_x loss_x 1.2725 (1.1771) acc_x 62.5000 (69.2500) lr 1.9178e-03 eta 0:00:08
epoch [28/200] batch [30/43] time 0.489 (0.459) data 0.358 (0.328) loss_x loss_x 1.7383 (1.2399) acc_x 53.1250 (69.4792) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [35/43] time 0.450 (0.461) data 0.319 (0.330) loss_x loss_x 1.0977 (1.2457) acc_x 71.8750 (69.1964) lr 1.9178e-03 eta 0:00:03
epoch [28/200] batch [40/43] time 0.448 (0.465) data 0.318 (0.334) loss_x loss_x 1.2188 (1.2437) acc_x 59.3750 (68.9062) lr 1.9178e-03 eta 0:00:01
epoch [28/200] batch [5/54] time 0.377 (0.467) data 0.247 (0.336) loss_u loss_u 0.7949 (0.7757) acc_u 31.2500 (30.0000) lr 1.9178e-03 eta 0:00:22
epoch [28/200] batch [10/54] time 0.326 (0.464) data 0.194 (0.333) loss_u loss_u 0.9365 (0.8060) acc_u 6.2500 (25.9375) lr 1.9178e-03 eta 0:00:20
epoch [28/200] batch [15/54] time 0.449 (0.461) data 0.317 (0.329) loss_u loss_u 0.7524 (0.8013) acc_u 34.3750 (26.6667) lr 1.9178e-03 eta 0:00:17
epoch [28/200] batch [20/54] time 0.390 (0.460) data 0.259 (0.329) loss_u loss_u 0.8452 (0.8097) acc_u 18.7500 (25.6250) lr 1.9178e-03 eta 0:00:15
epoch [28/200] batch [25/54] time 0.434 (0.458) data 0.300 (0.327) loss_u loss_u 0.7998 (0.8083) acc_u 31.2500 (26.0000) lr 1.9178e-03 eta 0:00:13
epoch [28/200] batch [30/54] time 0.442 (0.456) data 0.311 (0.324) loss_u loss_u 0.7852 (0.8049) acc_u 21.8750 (25.9375) lr 1.9178e-03 eta 0:00:10
epoch [28/200] batch [35/54] time 0.603 (0.458) data 0.472 (0.327) loss_u loss_u 0.8066 (0.8043) acc_u 34.3750 (26.2500) lr 1.9178e-03 eta 0:00:08
epoch [28/200] batch [40/54] time 0.550 (0.461) data 0.418 (0.330) loss_u loss_u 0.7808 (0.8023) acc_u 25.0000 (26.3281) lr 1.9178e-03 eta 0:00:06
epoch [28/200] batch [45/54] time 0.444 (0.461) data 0.314 (0.330) loss_u loss_u 0.7124 (0.8018) acc_u 28.1250 (26.2500) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [50/54] time 0.349 (0.456) data 0.218 (0.324) loss_u loss_u 0.8047 (0.8018) acc_u 21.8750 (26.3750) lr 1.9178e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1579
confident_label rate tensor(0.4474, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1403
clean true:1402
clean false:1
clean_rate:0.9992872416250891
noisy true:155
noisy false:1578
after delete: len(clean_dataset) 1403
after delete: len(noisy_dataset) 1733
epoch [29/200] batch [5/43] time 0.454 (0.465) data 0.322 (0.334) loss_x loss_x 1.3799 (1.4469) acc_x 71.8750 (65.0000) lr 1.9114e-03 eta 0:00:17
epoch [29/200] batch [10/43] time 0.434 (0.463) data 0.302 (0.332) loss_x loss_x 1.7197 (1.4055) acc_x 56.2500 (64.3750) lr 1.9114e-03 eta 0:00:15
epoch [29/200] batch [15/43] time 0.539 (0.504) data 0.408 (0.373) loss_x loss_x 1.2021 (1.3930) acc_x 78.1250 (66.4583) lr 1.9114e-03 eta 0:00:14
epoch [29/200] batch [20/43] time 0.385 (0.497) data 0.254 (0.366) loss_x loss_x 1.4355 (1.3899) acc_x 56.2500 (66.5625) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [25/43] time 0.345 (0.478) data 0.214 (0.347) loss_x loss_x 0.6973 (1.3448) acc_x 90.6250 (67.6250) lr 1.9114e-03 eta 0:00:08
epoch [29/200] batch [30/43] time 0.489 (0.472) data 0.357 (0.341) loss_x loss_x 1.6738 (1.3829) acc_x 62.5000 (67.2917) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [35/43] time 0.406 (0.482) data 0.274 (0.351) loss_x loss_x 1.5293 (1.4222) acc_x 46.8750 (65.8929) lr 1.9114e-03 eta 0:00:03
epoch [29/200] batch [40/43] time 0.394 (0.481) data 0.263 (0.349) loss_x loss_x 1.4297 (1.4308) acc_x 59.3750 (64.8438) lr 1.9114e-03 eta 0:00:01
epoch [29/200] batch [5/54] time 0.395 (0.478) data 0.263 (0.347) loss_u loss_u 0.7754 (0.7630) acc_u 31.2500 (32.5000) lr 1.9114e-03 eta 0:00:23
epoch [29/200] batch [10/54] time 0.485 (0.474) data 0.353 (0.342) loss_u loss_u 0.8262 (0.7712) acc_u 25.0000 (30.3125) lr 1.9114e-03 eta 0:00:20
epoch [29/200] batch [15/54] time 0.434 (0.468) data 0.302 (0.337) loss_u loss_u 0.7954 (0.7883) acc_u 25.0000 (27.5000) lr 1.9114e-03 eta 0:00:18
epoch [29/200] batch [20/54] time 0.476 (0.465) data 0.344 (0.334) loss_u loss_u 0.7734 (0.7903) acc_u 25.0000 (27.0312) lr 1.9114e-03 eta 0:00:15
epoch [29/200] batch [25/54] time 0.475 (0.463) data 0.343 (0.332) loss_u loss_u 0.7861 (0.7885) acc_u 31.2500 (26.7500) lr 1.9114e-03 eta 0:00:13
epoch [29/200] batch [30/54] time 0.391 (0.462) data 0.258 (0.331) loss_u loss_u 0.8013 (0.7883) acc_u 25.0000 (26.9792) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [35/54] time 0.422 (0.462) data 0.292 (0.330) loss_u loss_u 0.8828 (0.7984) acc_u 9.3750 (25.2679) lr 1.9114e-03 eta 0:00:08
epoch [29/200] batch [40/54] time 0.388 (0.460) data 0.257 (0.328) loss_u loss_u 0.8281 (0.7991) acc_u 21.8750 (25.2344) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [45/54] time 0.426 (0.458) data 0.294 (0.326) loss_u loss_u 0.8232 (0.7983) acc_u 25.0000 (25.5556) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [50/54] time 0.533 (0.460) data 0.402 (0.328) loss_u loss_u 0.7642 (0.8002) acc_u 31.2500 (25.5625) lr 1.9114e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1564
confident_label rate tensor(0.4534, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1422
clean true:1419
clean false:3
clean_rate:0.9978902953586498
noisy true:153
noisy false:1561
after delete: len(clean_dataset) 1422
after delete: len(noisy_dataset) 1714
epoch [30/200] batch [5/44] time 0.427 (0.471) data 0.296 (0.339) loss_x loss_x 1.1533 (1.3818) acc_x 62.5000 (66.2500) lr 1.9048e-03 eta 0:00:18
epoch [30/200] batch [10/44] time 0.573 (0.484) data 0.442 (0.353) loss_x loss_x 0.9448 (1.2917) acc_x 75.0000 (69.0625) lr 1.9048e-03 eta 0:00:16
epoch [30/200] batch [15/44] time 0.597 (0.503) data 0.467 (0.372) loss_x loss_x 1.3115 (1.3184) acc_x 75.0000 (68.7500) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [20/44] time 0.450 (0.486) data 0.318 (0.355) loss_x loss_x 1.6279 (1.3738) acc_x 62.5000 (66.7188) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [25/44] time 0.526 (0.492) data 0.394 (0.360) loss_x loss_x 1.5859 (1.4202) acc_x 59.3750 (65.1250) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [30/44] time 0.442 (0.490) data 0.311 (0.359) loss_x loss_x 1.2412 (1.3904) acc_x 59.3750 (65.3125) lr 1.9048e-03 eta 0:00:06
epoch [30/200] batch [35/44] time 0.481 (0.490) data 0.348 (0.359) loss_x loss_x 1.2139 (1.4001) acc_x 78.1250 (64.8214) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [40/44] time 0.518 (0.495) data 0.386 (0.363) loss_x loss_x 1.8643 (1.4090) acc_x 53.1250 (64.7656) lr 1.9048e-03 eta 0:00:01
epoch [30/200] batch [5/53] time 0.437 (0.485) data 0.306 (0.354) loss_u loss_u 0.8330 (0.7945) acc_u 18.7500 (30.0000) lr 1.9048e-03 eta 0:00:23
epoch [30/200] batch [10/53] time 0.478 (0.482) data 0.344 (0.350) loss_u loss_u 0.7788 (0.8076) acc_u 25.0000 (27.1875) lr 1.9048e-03 eta 0:00:20
epoch [30/200] batch [15/53] time 0.538 (0.478) data 0.406 (0.346) loss_u loss_u 0.7544 (0.8076) acc_u 37.5000 (26.8750) lr 1.9048e-03 eta 0:00:18
epoch [30/200] batch [20/53] time 0.379 (0.475) data 0.247 (0.343) loss_u loss_u 0.8818 (0.8067) acc_u 12.5000 (26.7188) lr 1.9048e-03 eta 0:00:15
epoch [30/200] batch [25/53] time 0.475 (0.475) data 0.343 (0.343) loss_u loss_u 0.8032 (0.7984) acc_u 28.1250 (27.5000) lr 1.9048e-03 eta 0:00:13
epoch [30/200] batch [30/53] time 0.321 (0.471) data 0.189 (0.339) loss_u loss_u 0.8081 (0.7986) acc_u 18.7500 (26.8750) lr 1.9048e-03 eta 0:00:10
epoch [30/200] batch [35/53] time 0.394 (0.470) data 0.258 (0.338) loss_u loss_u 0.8184 (0.8000) acc_u 25.0000 (26.7857) lr 1.9048e-03 eta 0:00:08
epoch [30/200] batch [40/53] time 0.448 (0.474) data 0.315 (0.342) loss_u loss_u 0.8481 (0.7991) acc_u 21.8750 (27.0312) lr 1.9048e-03 eta 0:00:06
epoch [30/200] batch [45/53] time 0.404 (0.468) data 0.271 (0.336) loss_u loss_u 0.7837 (0.7990) acc_u 31.2500 (26.6667) lr 1.9048e-03 eta 0:00:03
epoch [30/200] batch [50/53] time 0.359 (0.466) data 0.227 (0.334) loss_u loss_u 0.7593 (0.7972) acc_u 28.1250 (26.8750) lr 1.9048e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1538
confident_label rate tensor(0.4573, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1434
clean true:1431
clean false:3
clean_rate:0.997907949790795
noisy true:167
noisy false:1535
after delete: len(clean_dataset) 1434
after delete: len(noisy_dataset) 1702
epoch [31/200] batch [5/44] time 0.425 (0.493) data 0.294 (0.362) loss_x loss_x 1.0566 (1.6033) acc_x 75.0000 (58.7500) lr 1.8980e-03 eta 0:00:19
epoch [31/200] batch [10/44] time 0.407 (0.499) data 0.277 (0.368) loss_x loss_x 0.8188 (1.3694) acc_x 75.0000 (63.7500) lr 1.8980e-03 eta 0:00:16
epoch [31/200] batch [15/44] time 0.428 (0.489) data 0.297 (0.358) loss_x loss_x 1.0537 (1.3447) acc_x 68.7500 (65.6250) lr 1.8980e-03 eta 0:00:14
epoch [31/200] batch [20/44] time 0.418 (0.478) data 0.287 (0.347) loss_x loss_x 1.0654 (1.3584) acc_x 78.1250 (65.9375) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [25/44] time 0.461 (0.471) data 0.329 (0.340) loss_x loss_x 1.3350 (1.3765) acc_x 62.5000 (65.3750) lr 1.8980e-03 eta 0:00:08
epoch [31/200] batch [30/44] time 0.644 (0.469) data 0.513 (0.338) loss_x loss_x 1.2734 (1.3653) acc_x 68.7500 (65.1042) lr 1.8980e-03 eta 0:00:06
epoch [31/200] batch [35/44] time 0.421 (0.471) data 0.290 (0.340) loss_x loss_x 1.4756 (1.3596) acc_x 59.3750 (64.8214) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [40/44] time 0.418 (0.471) data 0.286 (0.340) loss_x loss_x 1.2324 (1.3457) acc_x 59.3750 (64.5312) lr 1.8980e-03 eta 0:00:01
epoch [31/200] batch [5/53] time 0.618 (0.467) data 0.486 (0.335) loss_u loss_u 0.8042 (0.7854) acc_u 25.0000 (28.1250) lr 1.8980e-03 eta 0:00:22
epoch [31/200] batch [10/53] time 0.398 (0.461) data 0.267 (0.329) loss_u loss_u 0.6919 (0.7961) acc_u 43.7500 (26.2500) lr 1.8980e-03 eta 0:00:19
epoch [31/200] batch [15/53] time 0.376 (0.457) data 0.244 (0.326) loss_u loss_u 0.8560 (0.8009) acc_u 21.8750 (26.0417) lr 1.8980e-03 eta 0:00:17
epoch [31/200] batch [20/53] time 0.598 (0.459) data 0.462 (0.328) loss_u loss_u 0.8594 (0.7972) acc_u 21.8750 (26.2500) lr 1.8980e-03 eta 0:00:15
epoch [31/200] batch [25/53] time 0.382 (0.455) data 0.249 (0.323) loss_u loss_u 0.7222 (0.7967) acc_u 37.5000 (26.3750) lr 1.8980e-03 eta 0:00:12
epoch [31/200] batch [30/53] time 0.426 (0.457) data 0.294 (0.326) loss_u loss_u 0.8394 (0.8005) acc_u 25.0000 (25.8333) lr 1.8980e-03 eta 0:00:10
epoch [31/200] batch [35/53] time 0.492 (0.463) data 0.359 (0.331) loss_u loss_u 0.7266 (0.7981) acc_u 37.5000 (26.6071) lr 1.8980e-03 eta 0:00:08
epoch [31/200] batch [40/53] time 0.439 (0.461) data 0.308 (0.329) loss_u loss_u 0.7861 (0.7952) acc_u 31.2500 (27.1094) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [45/53] time 0.378 (0.459) data 0.247 (0.328) loss_u loss_u 0.8389 (0.7931) acc_u 18.7500 (27.2917) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [50/53] time 0.400 (0.457) data 0.268 (0.325) loss_u loss_u 0.7871 (0.7923) acc_u 25.0000 (27.3750) lr 1.8980e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1577
confident_label rate tensor(0.4439, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1392
clean true:1392
clean false:0
clean_rate:1.0
noisy true:167
noisy false:1577
after delete: len(clean_dataset) 1392
after delete: len(noisy_dataset) 1744
epoch [32/200] batch [5/43] time 0.403 (0.487) data 0.272 (0.356) loss_x loss_x 1.7979 (1.2058) acc_x 59.3750 (68.1250) lr 1.8910e-03 eta 0:00:18
epoch [32/200] batch [10/43] time 0.658 (0.502) data 0.527 (0.371) loss_x loss_x 1.1855 (1.1639) acc_x 78.1250 (70.0000) lr 1.8910e-03 eta 0:00:16
epoch [32/200] batch [15/43] time 0.474 (0.477) data 0.343 (0.346) loss_x loss_x 1.7529 (1.2467) acc_x 59.3750 (68.3333) lr 1.8910e-03 eta 0:00:13
epoch [32/200] batch [20/43] time 0.483 (0.470) data 0.351 (0.339) loss_x loss_x 1.4844 (1.2503) acc_x 68.7500 (67.8125) lr 1.8910e-03 eta 0:00:10
epoch [32/200] batch [25/43] time 0.437 (0.469) data 0.306 (0.338) loss_x loss_x 1.5693 (1.2636) acc_x 59.3750 (67.8750) lr 1.8910e-03 eta 0:00:08
epoch [32/200] batch [30/43] time 0.498 (0.469) data 0.367 (0.338) loss_x loss_x 1.6592 (1.2708) acc_x 65.6250 (68.3333) lr 1.8910e-03 eta 0:00:06
epoch [32/200] batch [35/43] time 0.517 (0.464) data 0.387 (0.333) loss_x loss_x 1.7354 (1.2952) acc_x 59.3750 (67.5893) lr 1.8910e-03 eta 0:00:03
epoch [32/200] batch [40/43] time 0.488 (0.468) data 0.356 (0.337) loss_x loss_x 1.5889 (1.3176) acc_x 56.2500 (66.8750) lr 1.8910e-03 eta 0:00:01
epoch [32/200] batch [5/54] time 0.452 (0.466) data 0.320 (0.335) loss_u loss_u 0.7554 (0.7674) acc_u 34.3750 (32.5000) lr 1.8910e-03 eta 0:00:22
epoch [32/200] batch [10/54] time 0.458 (0.467) data 0.326 (0.335) loss_u loss_u 0.8042 (0.7763) acc_u 31.2500 (30.6250) lr 1.8910e-03 eta 0:00:20
epoch [32/200] batch [15/54] time 0.519 (0.469) data 0.387 (0.338) loss_u loss_u 0.8560 (0.7930) acc_u 15.6250 (27.9167) lr 1.8910e-03 eta 0:00:18
epoch [32/200] batch [20/54] time 0.384 (0.470) data 0.252 (0.339) loss_u loss_u 0.8765 (0.7983) acc_u 12.5000 (26.7188) lr 1.8910e-03 eta 0:00:15
epoch [32/200] batch [25/54] time 0.438 (0.472) data 0.306 (0.340) loss_u loss_u 0.7695 (0.8012) acc_u 28.1250 (26.1250) lr 1.8910e-03 eta 0:00:13
epoch [32/200] batch [30/54] time 0.444 (0.467) data 0.313 (0.336) loss_u loss_u 0.8037 (0.8000) acc_u 25.0000 (26.2500) lr 1.8910e-03 eta 0:00:11
epoch [32/200] batch [35/54] time 0.435 (0.466) data 0.304 (0.334) loss_u loss_u 0.8887 (0.8009) acc_u 15.6250 (26.2500) lr 1.8910e-03 eta 0:00:08
epoch [32/200] batch [40/54] time 0.495 (0.465) data 0.363 (0.333) loss_u loss_u 0.7007 (0.7965) acc_u 34.3750 (26.7188) lr 1.8910e-03 eta 0:00:06
epoch [32/200] batch [45/54] time 0.465 (0.467) data 0.332 (0.335) loss_u loss_u 0.8428 (0.7975) acc_u 15.6250 (26.5278) lr 1.8910e-03 eta 0:00:04
epoch [32/200] batch [50/54] time 0.577 (0.469) data 0.445 (0.338) loss_u loss_u 0.7710 (0.7942) acc_u 25.0000 (26.6250) lr 1.8910e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1562
confident_label rate tensor(0.4487, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1407
clean true:1407
clean false:0
clean_rate:1.0
noisy true:167
noisy false:1562
after delete: len(clean_dataset) 1407
after delete: len(noisy_dataset) 1729
epoch [33/200] batch [5/43] time 0.495 (0.517) data 0.362 (0.385) loss_x loss_x 1.4902 (1.2562) acc_x 65.6250 (68.7500) lr 1.8838e-03 eta 0:00:19
epoch [33/200] batch [10/43] time 0.384 (0.494) data 0.252 (0.362) loss_x loss_x 0.9937 (1.2610) acc_x 75.0000 (66.8750) lr 1.8838e-03 eta 0:00:16
epoch [33/200] batch [15/43] time 0.425 (0.482) data 0.295 (0.351) loss_x loss_x 1.4717 (1.2986) acc_x 68.7500 (67.5000) lr 1.8838e-03 eta 0:00:13
epoch [33/200] batch [20/43] time 0.346 (0.464) data 0.216 (0.333) loss_x loss_x 1.5342 (1.2592) acc_x 71.8750 (67.9688) lr 1.8838e-03 eta 0:00:10
epoch [33/200] batch [25/43] time 0.432 (0.463) data 0.302 (0.332) loss_x loss_x 1.8691 (1.3080) acc_x 50.0000 (66.6250) lr 1.8838e-03 eta 0:00:08
epoch [33/200] batch [30/43] time 0.384 (0.463) data 0.253 (0.332) loss_x loss_x 1.3643 (1.3159) acc_x 68.7500 (67.2917) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [35/43] time 0.444 (0.462) data 0.313 (0.331) loss_x loss_x 1.1855 (1.3215) acc_x 68.7500 (66.9643) lr 1.8838e-03 eta 0:00:03
epoch [33/200] batch [40/43] time 0.599 (0.474) data 0.468 (0.343) loss_x loss_x 0.9385 (1.3232) acc_x 71.8750 (67.1875) lr 1.8838e-03 eta 0:00:01
epoch [33/200] batch [5/54] time 0.447 (0.470) data 0.315 (0.339) loss_u loss_u 0.8672 (0.7915) acc_u 15.6250 (24.3750) lr 1.8838e-03 eta 0:00:23
epoch [33/200] batch [10/54] time 0.413 (0.464) data 0.282 (0.333) loss_u loss_u 0.7559 (0.7963) acc_u 37.5000 (25.6250) lr 1.8838e-03 eta 0:00:20
epoch [33/200] batch [15/54] time 0.583 (0.471) data 0.453 (0.339) loss_u loss_u 0.7832 (0.7798) acc_u 21.8750 (27.5000) lr 1.8838e-03 eta 0:00:18
epoch [33/200] batch [20/54] time 0.376 (0.467) data 0.244 (0.335) loss_u loss_u 0.8335 (0.7921) acc_u 15.6250 (25.9375) lr 1.8838e-03 eta 0:00:15
epoch [33/200] batch [25/54] time 0.436 (0.464) data 0.304 (0.333) loss_u loss_u 0.8198 (0.7892) acc_u 21.8750 (26.0000) lr 1.8838e-03 eta 0:00:13
epoch [33/200] batch [30/54] time 0.414 (0.461) data 0.281 (0.329) loss_u loss_u 0.7656 (0.7928) acc_u 37.5000 (26.6667) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [35/54] time 0.517 (0.464) data 0.383 (0.332) loss_u loss_u 0.7515 (0.7936) acc_u 37.5000 (26.7857) lr 1.8838e-03 eta 0:00:08
epoch [33/200] batch [40/54] time 0.436 (0.462) data 0.305 (0.330) loss_u loss_u 0.8003 (0.7989) acc_u 25.0000 (26.0938) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [45/54] time 0.419 (0.460) data 0.287 (0.328) loss_u loss_u 0.7026 (0.7987) acc_u 43.7500 (26.3889) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [50/54] time 0.405 (0.458) data 0.274 (0.326) loss_u loss_u 0.8667 (0.8015) acc_u 15.6250 (25.9375) lr 1.8838e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1561
confident_label rate tensor(0.4518, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1417
clean true:1416
clean false:1
clean_rate:0.9992942836979535
noisy true:159
noisy false:1560
after delete: len(clean_dataset) 1417
after delete: len(noisy_dataset) 1719
epoch [34/200] batch [5/44] time 0.442 (0.568) data 0.310 (0.436) loss_x loss_x 1.0977 (1.1996) acc_x 78.1250 (70.0000) lr 1.8763e-03 eta 0:00:22
epoch [34/200] batch [10/44] time 0.586 (0.544) data 0.453 (0.412) loss_x loss_x 1.0762 (1.2516) acc_x 68.7500 (69.6875) lr 1.8763e-03 eta 0:00:18
epoch [34/200] batch [15/44] time 0.391 (0.536) data 0.260 (0.404) loss_x loss_x 1.0811 (1.2554) acc_x 71.8750 (69.7917) lr 1.8763e-03 eta 0:00:15
epoch [34/200] batch [20/44] time 0.635 (0.522) data 0.504 (0.390) loss_x loss_x 1.0273 (1.3130) acc_x 75.0000 (68.7500) lr 1.8763e-03 eta 0:00:12
epoch [34/200] batch [25/44] time 0.510 (0.511) data 0.379 (0.379) loss_x loss_x 1.5381 (1.3281) acc_x 59.3750 (67.5000) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [30/44] time 0.487 (0.500) data 0.356 (0.368) loss_x loss_x 1.6104 (1.3371) acc_x 59.3750 (66.7708) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [35/44] time 0.398 (0.494) data 0.268 (0.362) loss_x loss_x 1.1396 (1.3404) acc_x 68.7500 (66.6071) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [40/44] time 0.438 (0.487) data 0.307 (0.355) loss_x loss_x 1.4600 (1.3400) acc_x 59.3750 (66.1719) lr 1.8763e-03 eta 0:00:01
epoch [34/200] batch [5/53] time 0.468 (0.487) data 0.337 (0.355) loss_u loss_u 0.8047 (0.8342) acc_u 28.1250 (21.2500) lr 1.8763e-03 eta 0:00:23
epoch [34/200] batch [10/53] time 0.419 (0.489) data 0.288 (0.357) loss_u loss_u 0.8535 (0.8174) acc_u 15.6250 (22.5000) lr 1.8763e-03 eta 0:00:21
epoch [34/200] batch [15/53] time 0.445 (0.488) data 0.312 (0.356) loss_u loss_u 0.7349 (0.7952) acc_u 34.3750 (26.2500) lr 1.8763e-03 eta 0:00:18
epoch [34/200] batch [20/53] time 0.400 (0.490) data 0.268 (0.359) loss_u loss_u 0.8174 (0.7946) acc_u 25.0000 (26.7188) lr 1.8763e-03 eta 0:00:16
epoch [34/200] batch [25/53] time 0.362 (0.485) data 0.231 (0.353) loss_u loss_u 0.7993 (0.8001) acc_u 28.1250 (26.3750) lr 1.8763e-03 eta 0:00:13
epoch [34/200] batch [30/53] time 0.369 (0.483) data 0.238 (0.352) loss_u loss_u 0.8271 (0.8033) acc_u 15.6250 (25.5208) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [35/53] time 0.351 (0.484) data 0.220 (0.352) loss_u loss_u 0.7812 (0.7992) acc_u 25.0000 (26.3393) lr 1.8763e-03 eta 0:00:08
epoch [34/200] batch [40/53] time 0.510 (0.481) data 0.380 (0.349) loss_u loss_u 0.8315 (0.7991) acc_u 21.8750 (26.3281) lr 1.8763e-03 eta 0:00:06
epoch [34/200] batch [45/53] time 0.523 (0.480) data 0.390 (0.349) loss_u loss_u 0.8862 (0.7980) acc_u 6.2500 (25.9722) lr 1.8763e-03 eta 0:00:03
epoch [34/200] batch [50/53] time 0.378 (0.477) data 0.244 (0.346) loss_u loss_u 0.7686 (0.7972) acc_u 28.1250 (26.3125) lr 1.8763e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1520
confident_label rate tensor(0.4605, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1444
clean true:1443
clean false:1
clean_rate:0.9993074792243767
noisy true:173
noisy false:1519
after delete: len(clean_dataset) 1444
after delete: len(noisy_dataset) 1692
epoch [35/200] batch [5/45] time 0.435 (0.445) data 0.305 (0.314) loss_x loss_x 1.7422 (1.3115) acc_x 71.8750 (71.2500) lr 1.8686e-03 eta 0:00:17
epoch [35/200] batch [10/45] time 0.684 (0.470) data 0.554 (0.339) loss_x loss_x 0.7812 (1.1912) acc_x 84.3750 (72.8125) lr 1.8686e-03 eta 0:00:16
epoch [35/200] batch [15/45] time 0.508 (0.489) data 0.377 (0.358) loss_x loss_x 1.2148 (1.2216) acc_x 68.7500 (71.0417) lr 1.8686e-03 eta 0:00:14
epoch [35/200] batch [20/45] time 0.664 (0.498) data 0.534 (0.366) loss_x loss_x 1.3398 (1.2528) acc_x 65.6250 (70.1562) lr 1.8686e-03 eta 0:00:12
epoch [35/200] batch [25/45] time 0.449 (0.494) data 0.319 (0.363) loss_x loss_x 1.5293 (1.3133) acc_x 65.6250 (68.7500) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [30/45] time 0.427 (0.496) data 0.297 (0.365) loss_x loss_x 0.8901 (1.3102) acc_x 84.3750 (68.1250) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [35/45] time 0.433 (0.491) data 0.300 (0.360) loss_x loss_x 1.4990 (1.3191) acc_x 62.5000 (67.6786) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [40/45] time 0.498 (0.484) data 0.367 (0.353) loss_x loss_x 1.2637 (1.3382) acc_x 78.1250 (67.5000) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [45/45] time 0.445 (0.481) data 0.314 (0.350) loss_x loss_x 1.1836 (1.3167) acc_x 71.8750 (68.1944) lr 1.8686e-03 eta 0:00:00
epoch [35/200] batch [5/52] time 0.431 (0.473) data 0.300 (0.342) loss_u loss_u 0.8447 (0.7796) acc_u 25.0000 (30.6250) lr 1.8686e-03 eta 0:00:22
epoch [35/200] batch [10/52] time 0.422 (0.477) data 0.288 (0.346) loss_u loss_u 0.7690 (0.7985) acc_u 25.0000 (27.1875) lr 1.8686e-03 eta 0:00:20
epoch [35/200] batch [15/52] time 0.478 (0.477) data 0.345 (0.346) loss_u loss_u 0.7324 (0.7966) acc_u 31.2500 (27.2917) lr 1.8686e-03 eta 0:00:17
epoch [35/200] batch [20/52] time 0.357 (0.471) data 0.226 (0.340) loss_u loss_u 0.7500 (0.8020) acc_u 31.2500 (25.9375) lr 1.8686e-03 eta 0:00:15
epoch [35/200] batch [25/52] time 0.512 (0.474) data 0.380 (0.343) loss_u loss_u 0.7949 (0.8046) acc_u 28.1250 (26.2500) lr 1.8686e-03 eta 0:00:12
epoch [35/200] batch [30/52] time 0.376 (0.471) data 0.244 (0.340) loss_u loss_u 0.8560 (0.8057) acc_u 21.8750 (26.3542) lr 1.8686e-03 eta 0:00:10
epoch [35/200] batch [35/52] time 0.544 (0.472) data 0.411 (0.340) loss_u loss_u 0.8154 (0.8093) acc_u 15.6250 (25.1786) lr 1.8686e-03 eta 0:00:08
epoch [35/200] batch [40/52] time 0.456 (0.471) data 0.324 (0.340) loss_u loss_u 0.7852 (0.8047) acc_u 25.0000 (25.6250) lr 1.8686e-03 eta 0:00:05
epoch [35/200] batch [45/52] time 0.470 (0.469) data 0.338 (0.338) loss_u loss_u 0.7246 (0.8000) acc_u 31.2500 (25.9722) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [50/52] time 0.396 (0.468) data 0.264 (0.337) loss_u loss_u 0.7944 (0.8021) acc_u 31.2500 (25.9375) lr 1.8686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1528
confident_label rate tensor(0.4608, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1445
clean true:1444
clean false:1
clean_rate:0.9993079584775086
noisy true:164
noisy false:1527
after delete: len(clean_dataset) 1445
after delete: len(noisy_dataset) 1691
epoch [36/200] batch [5/45] time 0.480 (0.484) data 0.350 (0.353) loss_x loss_x 0.9033 (1.2697) acc_x 78.1250 (64.3750) lr 1.8607e-03 eta 0:00:19
epoch [36/200] batch [10/45] time 0.465 (0.467) data 0.334 (0.337) loss_x loss_x 1.3076 (1.3254) acc_x 59.3750 (62.8125) lr 1.8607e-03 eta 0:00:16
epoch [36/200] batch [15/45] time 0.572 (0.486) data 0.440 (0.355) loss_x loss_x 1.2607 (1.3846) acc_x 68.7500 (62.0833) lr 1.8607e-03 eta 0:00:14
epoch [36/200] batch [20/45] time 0.587 (0.493) data 0.456 (0.362) loss_x loss_x 1.9531 (1.4341) acc_x 59.3750 (62.9688) lr 1.8607e-03 eta 0:00:12
epoch [36/200] batch [25/45] time 0.568 (0.495) data 0.437 (0.364) loss_x loss_x 0.8921 (1.3850) acc_x 81.2500 (64.3750) lr 1.8607e-03 eta 0:00:09
epoch [36/200] batch [30/45] time 0.521 (0.488) data 0.390 (0.357) loss_x loss_x 1.3945 (1.3834) acc_x 68.7500 (64.4792) lr 1.8607e-03 eta 0:00:07
epoch [36/200] batch [35/45] time 0.501 (0.483) data 0.370 (0.352) loss_x loss_x 1.1445 (1.3624) acc_x 68.7500 (64.5536) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [40/45] time 0.509 (0.479) data 0.376 (0.348) loss_x loss_x 1.2461 (1.3470) acc_x 75.0000 (64.7656) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [45/45] time 0.447 (0.477) data 0.316 (0.346) loss_x loss_x 1.3398 (1.3301) acc_x 62.5000 (65.1389) lr 1.8607e-03 eta 0:00:00
epoch [36/200] batch [5/52] time 0.532 (0.472) data 0.400 (0.341) loss_u loss_u 0.8047 (0.7959) acc_u 28.1250 (28.7500) lr 1.8607e-03 eta 0:00:22
epoch [36/200] batch [10/52] time 0.523 (0.471) data 0.392 (0.340) loss_u loss_u 0.5879 (0.7733) acc_u 53.1250 (29.0625) lr 1.8607e-03 eta 0:00:19
epoch [36/200] batch [15/52] time 0.470 (0.471) data 0.338 (0.340) loss_u loss_u 0.8652 (0.7673) acc_u 18.7500 (30.6250) lr 1.8607e-03 eta 0:00:17
epoch [36/200] batch [20/52] time 0.491 (0.472) data 0.358 (0.341) loss_u loss_u 0.7896 (0.7731) acc_u 25.0000 (29.6875) lr 1.8607e-03 eta 0:00:15
epoch [36/200] batch [25/52] time 0.583 (0.474) data 0.453 (0.343) loss_u loss_u 0.7861 (0.7750) acc_u 28.1250 (29.2500) lr 1.8607e-03 eta 0:00:12
epoch [36/200] batch [30/52] time 0.348 (0.472) data 0.217 (0.341) loss_u loss_u 0.8013 (0.7775) acc_u 21.8750 (28.8542) lr 1.8607e-03 eta 0:00:10
epoch [36/200] batch [35/52] time 0.401 (0.469) data 0.268 (0.338) loss_u loss_u 0.8164 (0.7728) acc_u 21.8750 (29.4643) lr 1.8607e-03 eta 0:00:07
epoch [36/200] batch [40/52] time 0.465 (0.468) data 0.333 (0.337) loss_u loss_u 0.8188 (0.7803) acc_u 25.0000 (28.6719) lr 1.8607e-03 eta 0:00:05
epoch [36/200] batch [45/52] time 0.397 (0.467) data 0.265 (0.336) loss_u loss_u 0.8179 (0.7821) acc_u 25.0000 (28.5417) lr 1.8607e-03 eta 0:00:03
epoch [36/200] batch [50/52] time 0.488 (0.467) data 0.356 (0.336) loss_u loss_u 0.7783 (0.7825) acc_u 43.7500 (28.5000) lr 1.8607e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1511
confident_label rate tensor(0.4656, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1460
clean true:1460
clean false:0
clean_rate:1.0
noisy true:165
noisy false:1511
after delete: len(clean_dataset) 1460
after delete: len(noisy_dataset) 1676
epoch [37/200] batch [5/45] time 0.605 (0.497) data 0.473 (0.366) loss_x loss_x 1.0986 (1.1519) acc_x 78.1250 (72.5000) lr 1.8526e-03 eta 0:00:19
epoch [37/200] batch [10/45] time 0.591 (0.549) data 0.459 (0.417) loss_x loss_x 1.0264 (1.1243) acc_x 81.2500 (71.5625) lr 1.8526e-03 eta 0:00:19
epoch [37/200] batch [15/45] time 0.438 (0.523) data 0.306 (0.391) loss_x loss_x 1.2461 (1.2062) acc_x 56.2500 (69.1667) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [20/45] time 0.653 (0.528) data 0.521 (0.396) loss_x loss_x 1.6123 (1.2399) acc_x 53.1250 (68.7500) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [25/45] time 0.851 (0.546) data 0.719 (0.414) loss_x loss_x 1.3291 (1.2549) acc_x 65.6250 (68.8750) lr 1.8526e-03 eta 0:00:10
epoch [37/200] batch [30/45] time 0.523 (0.540) data 0.391 (0.408) loss_x loss_x 1.3838 (1.2403) acc_x 59.3750 (68.8542) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [35/45] time 0.372 (0.529) data 0.239 (0.397) loss_x loss_x 1.2344 (1.2375) acc_x 62.5000 (69.1071) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [40/45] time 0.472 (0.526) data 0.341 (0.395) loss_x loss_x 1.0859 (1.2631) acc_x 75.0000 (68.1250) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [45/45] time 0.551 (0.520) data 0.420 (0.388) loss_x loss_x 1.6807 (1.2874) acc_x 62.5000 (67.0139) lr 1.8526e-03 eta 0:00:00
epoch [37/200] batch [5/52] time 0.392 (0.516) data 0.260 (0.385) loss_u loss_u 0.8335 (0.8161) acc_u 21.8750 (26.8750) lr 1.8526e-03 eta 0:00:24
epoch [37/200] batch [10/52] time 0.431 (0.514) data 0.299 (0.383) loss_u loss_u 0.7700 (0.8090) acc_u 37.5000 (25.9375) lr 1.8526e-03 eta 0:00:21
epoch [37/200] batch [15/52] time 0.371 (0.512) data 0.240 (0.380) loss_u loss_u 0.7031 (0.8016) acc_u 37.5000 (26.6667) lr 1.8526e-03 eta 0:00:18
epoch [37/200] batch [20/52] time 0.426 (0.509) data 0.295 (0.377) loss_u loss_u 0.8843 (0.8045) acc_u 15.6250 (25.1562) lr 1.8526e-03 eta 0:00:16
epoch [37/200] batch [25/52] time 0.444 (0.505) data 0.312 (0.374) loss_u loss_u 0.7559 (0.8005) acc_u 31.2500 (25.3750) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [30/52] time 0.513 (0.502) data 0.380 (0.370) loss_u loss_u 0.8599 (0.7996) acc_u 18.7500 (25.7292) lr 1.8526e-03 eta 0:00:11
epoch [37/200] batch [35/52] time 0.423 (0.500) data 0.287 (0.368) loss_u loss_u 0.6885 (0.7968) acc_u 37.5000 (26.1607) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [40/52] time 0.431 (0.500) data 0.298 (0.368) loss_u loss_u 0.8291 (0.7972) acc_u 15.6250 (26.0156) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [45/52] time 0.421 (0.496) data 0.288 (0.364) loss_u loss_u 0.8047 (0.7940) acc_u 25.0000 (26.5972) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [50/52] time 0.444 (0.494) data 0.313 (0.362) loss_u loss_u 0.7681 (0.7913) acc_u 25.0000 (26.9375) lr 1.8526e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1505
confident_label rate tensor(0.4668, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1464
clean true:1460
clean false:4
clean_rate:0.9972677595628415
noisy true:171
noisy false:1501
after delete: len(clean_dataset) 1464
after delete: len(noisy_dataset) 1672
epoch [38/200] batch [5/45] time 0.486 (0.456) data 0.355 (0.324) loss_x loss_x 1.7197 (1.3104) acc_x 53.1250 (68.1250) lr 1.8443e-03 eta 0:00:18
epoch [38/200] batch [10/45] time 0.414 (0.442) data 0.282 (0.310) loss_x loss_x 1.7188 (1.2698) acc_x 65.6250 (69.6875) lr 1.8443e-03 eta 0:00:15
epoch [38/200] batch [15/45] time 0.521 (0.461) data 0.389 (0.329) loss_x loss_x 1.6348 (1.3488) acc_x 59.3750 (67.0833) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [20/45] time 0.387 (0.458) data 0.256 (0.326) loss_x loss_x 1.3633 (1.3056) acc_x 68.7500 (68.4375) lr 1.8443e-03 eta 0:00:11
epoch [38/200] batch [25/45] time 0.371 (0.465) data 0.240 (0.333) loss_x loss_x 1.3008 (1.3016) acc_x 71.8750 (67.7500) lr 1.8443e-03 eta 0:00:09
epoch [38/200] batch [30/45] time 0.562 (0.483) data 0.431 (0.352) loss_x loss_x 1.4893 (1.3038) acc_x 68.7500 (67.2917) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [35/45] time 0.492 (0.492) data 0.361 (0.361) loss_x loss_x 1.3672 (1.3406) acc_x 65.6250 (65.6250) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [40/45] time 0.456 (0.491) data 0.324 (0.359) loss_x loss_x 1.5371 (1.3567) acc_x 65.6250 (65.3906) lr 1.8443e-03 eta 0:00:02
epoch [38/200] batch [45/45] time 0.537 (0.492) data 0.406 (0.361) loss_x loss_x 0.8462 (1.3226) acc_x 81.2500 (66.3194) lr 1.8443e-03 eta 0:00:00
epoch [38/200] batch [5/52] time 0.366 (0.488) data 0.236 (0.357) loss_u loss_u 0.7588 (0.7797) acc_u 31.2500 (30.0000) lr 1.8443e-03 eta 0:00:22
epoch [38/200] batch [10/52] time 0.462 (0.484) data 0.330 (0.353) loss_u loss_u 0.8291 (0.7949) acc_u 15.6250 (26.8750) lr 1.8443e-03 eta 0:00:20
epoch [38/200] batch [15/52] time 0.485 (0.489) data 0.352 (0.358) loss_u loss_u 0.8423 (0.8007) acc_u 18.7500 (25.2083) lr 1.8443e-03 eta 0:00:18
epoch [38/200] batch [20/52] time 0.365 (0.484) data 0.233 (0.353) loss_u loss_u 0.7686 (0.7984) acc_u 21.8750 (24.8438) lr 1.8443e-03 eta 0:00:15
epoch [38/200] batch [25/52] time 0.440 (0.483) data 0.309 (0.351) loss_u loss_u 0.7988 (0.8022) acc_u 25.0000 (24.5000) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [30/52] time 0.474 (0.479) data 0.344 (0.348) loss_u loss_u 0.8174 (0.8026) acc_u 21.8750 (24.6875) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [35/52] time 0.409 (0.475) data 0.279 (0.343) loss_u loss_u 0.8120 (0.8065) acc_u 28.1250 (24.3750) lr 1.8443e-03 eta 0:00:08
epoch [38/200] batch [40/52] time 0.419 (0.470) data 0.288 (0.339) loss_u loss_u 0.8389 (0.8066) acc_u 21.8750 (24.2188) lr 1.8443e-03 eta 0:00:05
epoch [38/200] batch [45/52] time 0.439 (0.469) data 0.307 (0.337) loss_u loss_u 0.7373 (0.8072) acc_u 37.5000 (24.2361) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [50/52] time 0.404 (0.465) data 0.273 (0.334) loss_u loss_u 0.7935 (0.8040) acc_u 21.8750 (24.5000) lr 1.8443e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1538
confident_label rate tensor(0.4570, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1433
clean true:1432
clean false:1
clean_rate:0.9993021632937893
noisy true:166
noisy false:1537
after delete: len(clean_dataset) 1433
after delete: len(noisy_dataset) 1703
epoch [39/200] batch [5/44] time 0.542 (0.496) data 0.411 (0.365) loss_x loss_x 2.1074 (1.3051) acc_x 46.8750 (65.6250) lr 1.8358e-03 eta 0:00:19
epoch [39/200] batch [10/44] time 0.531 (0.479) data 0.400 (0.348) loss_x loss_x 0.9731 (1.2506) acc_x 81.2500 (67.5000) lr 1.8358e-03 eta 0:00:16
epoch [39/200] batch [15/44] time 0.441 (0.482) data 0.310 (0.351) loss_x loss_x 0.9028 (1.2731) acc_x 78.1250 (67.9167) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [20/44] time 0.561 (0.492) data 0.430 (0.361) loss_x loss_x 1.1875 (1.3303) acc_x 59.3750 (67.0312) lr 1.8358e-03 eta 0:00:11
epoch [39/200] batch [25/44] time 0.517 (0.497) data 0.387 (0.366) loss_x loss_x 0.9531 (1.3125) acc_x 65.6250 (66.3750) lr 1.8358e-03 eta 0:00:09
epoch [39/200] batch [30/44] time 0.386 (0.492) data 0.255 (0.361) loss_x loss_x 1.2852 (1.3363) acc_x 62.5000 (65.7292) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [35/44] time 0.427 (0.480) data 0.296 (0.349) loss_x loss_x 1.6270 (1.3599) acc_x 62.5000 (65.8036) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [40/44] time 0.380 (0.475) data 0.249 (0.344) loss_x loss_x 1.1504 (1.3446) acc_x 65.6250 (66.0156) lr 1.8358e-03 eta 0:00:01
epoch [39/200] batch [5/53] time 0.499 (0.467) data 0.367 (0.336) loss_u loss_u 0.8369 (0.8513) acc_u 15.6250 (18.7500) lr 1.8358e-03 eta 0:00:22
epoch [39/200] batch [10/53] time 0.473 (0.471) data 0.342 (0.340) loss_u loss_u 0.7920 (0.8252) acc_u 28.1250 (24.0625) lr 1.8358e-03 eta 0:00:20
epoch [39/200] batch [15/53] time 0.451 (0.469) data 0.319 (0.338) loss_u loss_u 0.7852 (0.8160) acc_u 37.5000 (25.4167) lr 1.8358e-03 eta 0:00:17
epoch [39/200] batch [20/53] time 0.772 (0.474) data 0.641 (0.343) loss_u loss_u 0.7881 (0.8098) acc_u 21.8750 (26.2500) lr 1.8358e-03 eta 0:00:15
epoch [39/200] batch [25/53] time 0.588 (0.474) data 0.456 (0.343) loss_u loss_u 0.7007 (0.8063) acc_u 37.5000 (26.3750) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [30/53] time 0.504 (0.474) data 0.372 (0.343) loss_u loss_u 0.7637 (0.7980) acc_u 31.2500 (27.5000) lr 1.8358e-03 eta 0:00:10
epoch [39/200] batch [35/53] time 0.445 (0.470) data 0.314 (0.339) loss_u loss_u 0.7900 (0.7983) acc_u 31.2500 (27.1429) lr 1.8358e-03 eta 0:00:08
epoch [39/200] batch [40/53] time 0.312 (0.467) data 0.180 (0.336) loss_u loss_u 0.7500 (0.7941) acc_u 31.2500 (27.6562) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [45/53] time 0.559 (0.466) data 0.427 (0.335) loss_u loss_u 0.7163 (0.7860) acc_u 37.5000 (28.8194) lr 1.8358e-03 eta 0:00:03
epoch [39/200] batch [50/53] time 0.360 (0.463) data 0.229 (0.332) loss_u loss_u 0.8149 (0.7880) acc_u 18.7500 (28.6250) lr 1.8358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1506
confident_label rate tensor(0.4652, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1459
clean true:1458
clean false:1
clean_rate:0.9993145990404386
noisy true:172
noisy false:1505
after delete: len(clean_dataset) 1459
after delete: len(noisy_dataset) 1677
epoch [40/200] batch [5/45] time 0.389 (0.443) data 0.259 (0.312) loss_x loss_x 0.9907 (1.3567) acc_x 71.8750 (63.7500) lr 1.8271e-03 eta 0:00:17
epoch [40/200] batch [10/45] time 0.674 (0.461) data 0.544 (0.330) loss_x loss_x 0.9956 (1.2653) acc_x 68.7500 (67.1875) lr 1.8271e-03 eta 0:00:16
epoch [40/200] batch [15/45] time 0.562 (0.467) data 0.432 (0.336) loss_x loss_x 1.7236 (1.3186) acc_x 56.2500 (66.2500) lr 1.8271e-03 eta 0:00:14
epoch [40/200] batch [20/45] time 0.499 (0.469) data 0.369 (0.339) loss_x loss_x 1.6846 (1.3381) acc_x 50.0000 (65.9375) lr 1.8271e-03 eta 0:00:11
epoch [40/200] batch [25/45] time 0.475 (0.461) data 0.345 (0.331) loss_x loss_x 0.9780 (1.3412) acc_x 71.8750 (65.6250) lr 1.8271e-03 eta 0:00:09
epoch [40/200] batch [30/45] time 0.462 (0.464) data 0.331 (0.334) loss_x loss_x 1.3213 (1.3323) acc_x 65.6250 (65.2083) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [35/45] time 0.449 (0.464) data 0.319 (0.334) loss_x loss_x 1.7646 (1.3503) acc_x 59.3750 (65.2679) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [40/45] time 0.385 (0.468) data 0.253 (0.337) loss_x loss_x 2.2910 (1.3594) acc_x 62.5000 (65.3125) lr 1.8271e-03 eta 0:00:02
epoch [40/200] batch [45/45] time 0.510 (0.479) data 0.379 (0.348) loss_x loss_x 1.8057 (1.3459) acc_x 59.3750 (65.3472) lr 1.8271e-03 eta 0:00:00
epoch [40/200] batch [5/52] time 0.403 (0.473) data 0.272 (0.342) loss_u loss_u 0.8760 (0.8233) acc_u 9.3750 (23.7500) lr 1.8271e-03 eta 0:00:22
epoch [40/200] batch [10/52] time 0.383 (0.470) data 0.252 (0.339) loss_u loss_u 0.8828 (0.8258) acc_u 12.5000 (21.2500) lr 1.8271e-03 eta 0:00:19
epoch [40/200] batch [15/52] time 0.407 (0.467) data 0.275 (0.336) loss_u loss_u 0.8462 (0.8219) acc_u 21.8750 (22.5000) lr 1.8271e-03 eta 0:00:17
epoch [40/200] batch [20/52] time 0.407 (0.468) data 0.275 (0.337) loss_u loss_u 0.8301 (0.8183) acc_u 21.8750 (22.9688) lr 1.8271e-03 eta 0:00:14
epoch [40/200] batch [25/52] time 0.518 (0.463) data 0.383 (0.332) loss_u loss_u 0.8159 (0.8119) acc_u 18.7500 (24.0000) lr 1.8271e-03 eta 0:00:12
epoch [40/200] batch [30/52] time 0.356 (0.462) data 0.224 (0.331) loss_u loss_u 0.7397 (0.8076) acc_u 40.6250 (24.3750) lr 1.8271e-03 eta 0:00:10
epoch [40/200] batch [35/52] time 0.330 (0.458) data 0.198 (0.327) loss_u loss_u 0.8838 (0.8069) acc_u 21.8750 (24.3750) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [40/52] time 0.556 (0.458) data 0.424 (0.327) loss_u loss_u 0.8071 (0.8007) acc_u 34.3750 (25.5469) lr 1.8271e-03 eta 0:00:05
epoch [40/200] batch [45/52] time 0.359 (0.456) data 0.227 (0.325) loss_u loss_u 0.8208 (0.8024) acc_u 21.8750 (25.0694) lr 1.8271e-03 eta 0:00:03
epoch [40/200] batch [50/52] time 0.515 (0.458) data 0.381 (0.326) loss_u loss_u 0.7974 (0.8016) acc_u 28.1250 (25.1875) lr 1.8271e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1522
confident_label rate tensor(0.4636, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1454
clean true:1453
clean false:1
clean_rate:0.9993122420907841
noisy true:161
noisy false:1521
after delete: len(clean_dataset) 1454
after delete: len(noisy_dataset) 1682
epoch [41/200] batch [5/45] time 0.467 (0.485) data 0.335 (0.354) loss_x loss_x 1.2383 (1.1656) acc_x 65.6250 (68.1250) lr 1.8181e-03 eta 0:00:19
epoch [41/200] batch [10/45] time 0.459 (0.487) data 0.327 (0.356) loss_x loss_x 1.4688 (1.1763) acc_x 62.5000 (69.3750) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [15/45] time 0.520 (0.465) data 0.388 (0.334) loss_x loss_x 0.9219 (1.1550) acc_x 71.8750 (68.9583) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [20/45] time 0.460 (0.460) data 0.329 (0.329) loss_x loss_x 1.0186 (1.2001) acc_x 71.8750 (66.5625) lr 1.8181e-03 eta 0:00:11
epoch [41/200] batch [25/45] time 0.578 (0.473) data 0.447 (0.342) loss_x loss_x 1.1494 (1.1684) acc_x 71.8750 (68.0000) lr 1.8181e-03 eta 0:00:09
epoch [41/200] batch [30/45] time 0.364 (0.480) data 0.233 (0.349) loss_x loss_x 1.7480 (1.2149) acc_x 59.3750 (67.7083) lr 1.8181e-03 eta 0:00:07
epoch [41/200] batch [35/45] time 0.438 (0.472) data 0.307 (0.341) loss_x loss_x 1.2930 (1.2447) acc_x 53.1250 (66.6071) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [40/45] time 0.555 (0.468) data 0.423 (0.337) loss_x loss_x 0.9414 (1.2457) acc_x 81.2500 (66.7969) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [45/45] time 0.553 (0.471) data 0.422 (0.340) loss_x loss_x 1.3418 (1.2636) acc_x 62.5000 (66.3889) lr 1.8181e-03 eta 0:00:00
epoch [41/200] batch [5/52] time 0.429 (0.464) data 0.297 (0.333) loss_u loss_u 0.8804 (0.8248) acc_u 12.5000 (21.2500) lr 1.8181e-03 eta 0:00:21
epoch [41/200] batch [10/52] time 0.467 (0.470) data 0.333 (0.339) loss_u loss_u 0.7622 (0.8152) acc_u 34.3750 (23.7500) lr 1.8181e-03 eta 0:00:19
epoch [41/200] batch [15/52] time 0.506 (0.477) data 0.373 (0.345) loss_u loss_u 0.8008 (0.8121) acc_u 21.8750 (23.5417) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [20/52] time 0.395 (0.482) data 0.264 (0.351) loss_u loss_u 0.7725 (0.8053) acc_u 34.3750 (24.8438) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [25/52] time 0.637 (0.484) data 0.505 (0.352) loss_u loss_u 0.7920 (0.8030) acc_u 21.8750 (25.0000) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [30/52] time 0.469 (0.482) data 0.337 (0.350) loss_u loss_u 0.7080 (0.8031) acc_u 31.2500 (24.8958) lr 1.8181e-03 eta 0:00:10
epoch [41/200] batch [35/52] time 0.345 (0.474) data 0.213 (0.342) loss_u loss_u 0.7998 (0.8015) acc_u 25.0000 (25.3571) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [40/52] time 0.462 (0.473) data 0.330 (0.342) loss_u loss_u 0.7319 (0.7932) acc_u 40.6250 (26.6406) lr 1.8181e-03 eta 0:00:05
epoch [41/200] batch [45/52] time 0.617 (0.473) data 0.485 (0.341) loss_u loss_u 0.6782 (0.7888) acc_u 37.5000 (27.5000) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [50/52] time 0.489 (0.471) data 0.357 (0.339) loss_u loss_u 0.8389 (0.7930) acc_u 25.0000 (27.0625) lr 1.8181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1486
confident_label rate tensor(0.4713, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1478
clean true:1475
clean false:3
clean_rate:0.9979702300405954
noisy true:175
noisy false:1483
after delete: len(clean_dataset) 1478
after delete: len(noisy_dataset) 1658
epoch [42/200] batch [5/46] time 0.611 (0.531) data 0.480 (0.399) loss_x loss_x 1.2842 (1.2859) acc_x 68.7500 (65.0000) lr 1.8090e-03 eta 0:00:21
epoch [42/200] batch [10/46] time 0.499 (0.500) data 0.367 (0.368) loss_x loss_x 1.0811 (1.2271) acc_x 81.2500 (68.4375) lr 1.8090e-03 eta 0:00:17
epoch [42/200] batch [15/46] time 0.561 (0.486) data 0.430 (0.355) loss_x loss_x 1.2959 (1.2224) acc_x 68.7500 (67.7083) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [20/46] time 0.427 (0.487) data 0.296 (0.356) loss_x loss_x 1.3516 (1.2592) acc_x 65.6250 (67.5000) lr 1.8090e-03 eta 0:00:12
epoch [42/200] batch [25/46] time 0.397 (0.496) data 0.266 (0.365) loss_x loss_x 1.3047 (1.2363) acc_x 68.7500 (68.0000) lr 1.8090e-03 eta 0:00:10
epoch [42/200] batch [30/46] time 0.430 (0.493) data 0.299 (0.361) loss_x loss_x 1.5654 (1.2857) acc_x 78.1250 (67.5000) lr 1.8090e-03 eta 0:00:07
epoch [42/200] batch [35/46] time 0.539 (0.494) data 0.409 (0.362) loss_x loss_x 1.3682 (1.2892) acc_x 53.1250 (66.9643) lr 1.8090e-03 eta 0:00:05
epoch [42/200] batch [40/46] time 0.454 (0.499) data 0.324 (0.368) loss_x loss_x 1.8965 (1.3073) acc_x 53.1250 (66.4062) lr 1.8090e-03 eta 0:00:02
epoch [42/200] batch [45/46] time 0.572 (0.502) data 0.442 (0.370) loss_x loss_x 1.4873 (1.2961) acc_x 65.6250 (66.3194) lr 1.8090e-03 eta 0:00:00
epoch [42/200] batch [5/51] time 0.457 (0.497) data 0.327 (0.366) loss_u loss_u 0.8481 (0.7981) acc_u 15.6250 (25.0000) lr 1.8090e-03 eta 0:00:22
epoch [42/200] batch [10/51] time 0.359 (0.492) data 0.227 (0.361) loss_u loss_u 0.7295 (0.7666) acc_u 31.2500 (29.3750) lr 1.8090e-03 eta 0:00:20
epoch [42/200] batch [15/51] time 0.366 (0.488) data 0.234 (0.357) loss_u loss_u 0.8247 (0.7769) acc_u 21.8750 (27.0833) lr 1.8090e-03 eta 0:00:17
epoch [42/200] batch [20/51] time 0.417 (0.483) data 0.286 (0.351) loss_u loss_u 0.7905 (0.7809) acc_u 28.1250 (27.3438) lr 1.8090e-03 eta 0:00:14
epoch [42/200] batch [25/51] time 0.407 (0.480) data 0.275 (0.348) loss_u loss_u 0.8701 (0.7875) acc_u 15.6250 (26.5000) lr 1.8090e-03 eta 0:00:12
epoch [42/200] batch [30/51] time 0.353 (0.476) data 0.221 (0.345) loss_u loss_u 0.7905 (0.7941) acc_u 28.1250 (25.9375) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [35/51] time 0.371 (0.472) data 0.240 (0.341) loss_u loss_u 0.8311 (0.7980) acc_u 28.1250 (25.7143) lr 1.8090e-03 eta 0:00:07
epoch [42/200] batch [40/51] time 0.338 (0.468) data 0.207 (0.337) loss_u loss_u 0.8008 (0.8001) acc_u 25.0000 (25.5469) lr 1.8090e-03 eta 0:00:05
epoch [42/200] batch [45/51] time 0.680 (0.468) data 0.549 (0.336) loss_u loss_u 0.7280 (0.8012) acc_u 31.2500 (25.3472) lr 1.8090e-03 eta 0:00:02
epoch [42/200] batch [50/51] time 0.428 (0.466) data 0.297 (0.335) loss_u loss_u 0.6279 (0.7980) acc_u 53.1250 (25.8125) lr 1.8090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1516
confident_label rate tensor(0.4617, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1448
clean true:1447
clean false:1
clean_rate:0.9993093922651933
noisy true:173
noisy false:1515
after delete: len(clean_dataset) 1448
after delete: len(noisy_dataset) 1688
epoch [43/200] batch [5/45] time 0.489 (0.480) data 0.358 (0.349) loss_x loss_x 1.3594 (1.1479) acc_x 65.6250 (71.2500) lr 1.7997e-03 eta 0:00:19
epoch [43/200] batch [10/45] time 0.488 (0.495) data 0.355 (0.364) loss_x loss_x 1.0928 (1.1780) acc_x 65.6250 (70.6250) lr 1.7997e-03 eta 0:00:17
epoch [43/200] batch [15/45] time 0.422 (0.480) data 0.290 (0.349) loss_x loss_x 1.3438 (1.2552) acc_x 62.5000 (68.5417) lr 1.7997e-03 eta 0:00:14
epoch [43/200] batch [20/45] time 0.417 (0.475) data 0.285 (0.344) loss_x loss_x 0.8721 (1.2303) acc_x 84.3750 (69.5312) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [25/45] time 0.370 (0.461) data 0.239 (0.330) loss_x loss_x 1.6143 (1.2468) acc_x 50.0000 (68.3750) lr 1.7997e-03 eta 0:00:09
epoch [43/200] batch [30/45] time 0.424 (0.460) data 0.293 (0.329) loss_x loss_x 1.4150 (1.2508) acc_x 65.6250 (68.6458) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [35/45] time 0.736 (0.467) data 0.604 (0.336) loss_x loss_x 1.2881 (1.2676) acc_x 65.6250 (67.7679) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [40/45] time 0.408 (0.462) data 0.277 (0.331) loss_x loss_x 1.1484 (1.2881) acc_x 71.8750 (67.6562) lr 1.7997e-03 eta 0:00:02
epoch [43/200] batch [45/45] time 0.512 (0.464) data 0.381 (0.332) loss_x loss_x 1.2900 (1.2767) acc_x 71.8750 (68.0556) lr 1.7997e-03 eta 0:00:00
epoch [43/200] batch [5/52] time 0.527 (0.462) data 0.394 (0.331) loss_u loss_u 0.8047 (0.8184) acc_u 28.1250 (24.3750) lr 1.7997e-03 eta 0:00:21
epoch [43/200] batch [10/52] time 0.453 (0.458) data 0.321 (0.326) loss_u loss_u 0.8169 (0.8089) acc_u 28.1250 (26.2500) lr 1.7997e-03 eta 0:00:19
epoch [43/200] batch [15/52] time 0.616 (0.462) data 0.484 (0.331) loss_u loss_u 0.8379 (0.8109) acc_u 18.7500 (25.2083) lr 1.7997e-03 eta 0:00:17
epoch [43/200] batch [20/52] time 0.485 (0.462) data 0.353 (0.330) loss_u loss_u 0.8066 (0.8042) acc_u 21.8750 (24.6875) lr 1.7997e-03 eta 0:00:14
epoch [43/200] batch [25/52] time 0.321 (0.464) data 0.189 (0.332) loss_u loss_u 0.7988 (0.8030) acc_u 28.1250 (25.5000) lr 1.7997e-03 eta 0:00:12
epoch [43/200] batch [30/52] time 0.506 (0.465) data 0.374 (0.333) loss_u loss_u 0.6909 (0.7922) acc_u 34.3750 (26.7708) lr 1.7997e-03 eta 0:00:10
epoch [43/200] batch [35/52] time 0.407 (0.462) data 0.275 (0.330) loss_u loss_u 0.7969 (0.7930) acc_u 28.1250 (26.8750) lr 1.7997e-03 eta 0:00:07
epoch [43/200] batch [40/52] time 0.598 (0.461) data 0.467 (0.329) loss_u loss_u 0.6982 (0.7886) acc_u 43.7500 (27.5000) lr 1.7997e-03 eta 0:00:05
epoch [43/200] batch [45/52] time 0.333 (0.458) data 0.202 (0.326) loss_u loss_u 0.7090 (0.7869) acc_u 37.5000 (27.8472) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [50/52] time 0.455 (0.455) data 0.323 (0.323) loss_u loss_u 0.8018 (0.7915) acc_u 31.2500 (27.2500) lr 1.7997e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1525
confident_label rate tensor(0.4592, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1440
clean true:1438
clean false:2
clean_rate:0.9986111111111111
noisy true:173
noisy false:1523
after delete: len(clean_dataset) 1440
after delete: len(noisy_dataset) 1696
epoch [44/200] batch [5/45] time 0.470 (0.430) data 0.339 (0.299) loss_x loss_x 1.5361 (1.3328) acc_x 62.5000 (72.5000) lr 1.7902e-03 eta 0:00:17
epoch [44/200] batch [10/45] time 0.335 (0.420) data 0.204 (0.289) loss_x loss_x 1.5029 (1.3078) acc_x 56.2500 (69.3750) lr 1.7902e-03 eta 0:00:14
epoch [44/200] batch [15/45] time 0.501 (0.434) data 0.371 (0.303) loss_x loss_x 0.9160 (1.2520) acc_x 75.0000 (69.5833) lr 1.7902e-03 eta 0:00:13
epoch [44/200] batch [20/45] time 0.417 (0.443) data 0.286 (0.312) loss_x loss_x 1.7324 (1.2735) acc_x 50.0000 (67.3438) lr 1.7902e-03 eta 0:00:11
epoch [44/200] batch [25/45] time 0.432 (0.454) data 0.300 (0.323) loss_x loss_x 1.1768 (1.2951) acc_x 81.2500 (67.2500) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [30/45] time 0.438 (0.461) data 0.306 (0.329) loss_x loss_x 1.5488 (1.2893) acc_x 65.6250 (68.0208) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [35/45] time 0.510 (0.460) data 0.379 (0.329) loss_x loss_x 1.2803 (1.3147) acc_x 65.6250 (67.1429) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [40/45] time 0.461 (0.468) data 0.331 (0.337) loss_x loss_x 1.3672 (1.3278) acc_x 65.6250 (66.8750) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [45/45] time 0.461 (0.472) data 0.329 (0.341) loss_x loss_x 0.8584 (1.3286) acc_x 78.1250 (66.4583) lr 1.7902e-03 eta 0:00:00
epoch [44/200] batch [5/53] time 0.320 (0.466) data 0.188 (0.335) loss_u loss_u 0.7031 (0.7579) acc_u 37.5000 (30.0000) lr 1.7902e-03 eta 0:00:22
epoch [44/200] batch [10/53] time 0.460 (0.463) data 0.326 (0.331) loss_u loss_u 0.7632 (0.7775) acc_u 28.1250 (26.2500) lr 1.7902e-03 eta 0:00:19
epoch [44/200] batch [15/53] time 0.399 (0.463) data 0.267 (0.331) loss_u loss_u 0.6504 (0.7785) acc_u 50.0000 (27.5000) lr 1.7902e-03 eta 0:00:17
epoch [44/200] batch [20/53] time 0.359 (0.457) data 0.227 (0.325) loss_u loss_u 0.8491 (0.7990) acc_u 21.8750 (25.7812) lr 1.7902e-03 eta 0:00:15
epoch [44/200] batch [25/53] time 0.419 (0.454) data 0.287 (0.323) loss_u loss_u 0.8076 (0.7975) acc_u 28.1250 (26.5000) lr 1.7902e-03 eta 0:00:12
epoch [44/200] batch [30/53] time 0.435 (0.454) data 0.304 (0.323) loss_u loss_u 0.7915 (0.7966) acc_u 34.3750 (26.6667) lr 1.7902e-03 eta 0:00:10
epoch [44/200] batch [35/53] time 0.381 (0.454) data 0.250 (0.323) loss_u loss_u 0.8379 (0.8029) acc_u 12.5000 (25.8929) lr 1.7902e-03 eta 0:00:08
epoch [44/200] batch [40/53] time 0.604 (0.455) data 0.473 (0.324) loss_u loss_u 0.7964 (0.8001) acc_u 21.8750 (26.1719) lr 1.7902e-03 eta 0:00:05
epoch [44/200] batch [45/53] time 0.481 (0.457) data 0.349 (0.325) loss_u loss_u 0.7803 (0.7972) acc_u 25.0000 (26.8750) lr 1.7902e-03 eta 0:00:03
epoch [44/200] batch [50/53] time 0.401 (0.458) data 0.271 (0.327) loss_u loss_u 0.7974 (0.7947) acc_u 28.1250 (27.4375) lr 1.7902e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1489
confident_label rate tensor(0.4700, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1474
clean true:1474
clean false:0
clean_rate:1.0
noisy true:173
noisy false:1489
after delete: len(clean_dataset) 1474
after delete: len(noisy_dataset) 1662
epoch [45/200] batch [5/46] time 0.653 (0.491) data 0.522 (0.360) loss_x loss_x 1.3467 (1.1636) acc_x 71.8750 (74.3750) lr 1.7804e-03 eta 0:00:20
epoch [45/200] batch [10/46] time 0.590 (0.493) data 0.459 (0.362) loss_x loss_x 0.8398 (1.2971) acc_x 75.0000 (69.3750) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [15/46] time 0.477 (0.475) data 0.346 (0.344) loss_x loss_x 1.3486 (1.3297) acc_x 62.5000 (67.7083) lr 1.7804e-03 eta 0:00:14
epoch [45/200] batch [20/46] time 0.585 (0.469) data 0.454 (0.338) loss_x loss_x 1.2236 (1.2782) acc_x 65.6250 (68.4375) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [25/46] time 0.557 (0.465) data 0.426 (0.334) loss_x loss_x 1.6309 (1.2850) acc_x 59.3750 (67.3750) lr 1.7804e-03 eta 0:00:09
epoch [45/200] batch [30/46] time 0.605 (0.468) data 0.475 (0.337) loss_x loss_x 1.2393 (1.2957) acc_x 59.3750 (66.7708) lr 1.7804e-03 eta 0:00:07
epoch [45/200] batch [35/46] time 0.448 (0.468) data 0.317 (0.337) loss_x loss_x 1.4600 (1.2904) acc_x 75.0000 (67.6786) lr 1.7804e-03 eta 0:00:05
epoch [45/200] batch [40/46] time 0.599 (0.485) data 0.468 (0.354) loss_x loss_x 0.8301 (1.2680) acc_x 84.3750 (68.0469) lr 1.7804e-03 eta 0:00:02
epoch [45/200] batch [45/46] time 0.436 (0.485) data 0.304 (0.354) loss_x loss_x 1.1592 (1.2643) acc_x 68.7500 (67.9167) lr 1.7804e-03 eta 0:00:00
epoch [45/200] batch [5/51] time 0.393 (0.481) data 0.261 (0.350) loss_u loss_u 0.7061 (0.7887) acc_u 37.5000 (26.2500) lr 1.7804e-03 eta 0:00:22
epoch [45/200] batch [10/51] time 0.384 (0.478) data 0.253 (0.347) loss_u loss_u 0.7856 (0.7957) acc_u 34.3750 (28.4375) lr 1.7804e-03 eta 0:00:19
epoch [45/200] batch [15/51] time 0.500 (0.477) data 0.368 (0.346) loss_u loss_u 0.8125 (0.7983) acc_u 37.5000 (28.5417) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [20/51] time 0.626 (0.477) data 0.496 (0.345) loss_u loss_u 0.9116 (0.7964) acc_u 9.3750 (27.6562) lr 1.7804e-03 eta 0:00:14
epoch [45/200] batch [25/51] time 0.550 (0.476) data 0.418 (0.345) loss_u loss_u 0.7637 (0.7900) acc_u 25.0000 (27.7500) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [30/51] time 0.340 (0.472) data 0.209 (0.340) loss_u loss_u 0.7734 (0.7784) acc_u 28.1250 (29.0625) lr 1.7804e-03 eta 0:00:09
epoch [45/200] batch [35/51] time 0.426 (0.468) data 0.294 (0.337) loss_u loss_u 0.8037 (0.7836) acc_u 25.0000 (28.0357) lr 1.7804e-03 eta 0:00:07
epoch [45/200] batch [40/51] time 0.413 (0.465) data 0.281 (0.333) loss_u loss_u 0.8364 (0.7865) acc_u 15.6250 (27.3438) lr 1.7804e-03 eta 0:00:05
epoch [45/200] batch [45/51] time 0.459 (0.465) data 0.328 (0.333) loss_u loss_u 0.7920 (0.7894) acc_u 31.2500 (27.0833) lr 1.7804e-03 eta 0:00:02
epoch [45/200] batch [50/51] time 0.434 (0.462) data 0.304 (0.331) loss_u loss_u 0.7422 (0.7844) acc_u 28.1250 (27.6875) lr 1.7804e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1493
confident_label rate tensor(0.4713, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1478
clean true:1476
clean false:2
clean_rate:0.9986468200270636
noisy true:167
noisy false:1491
after delete: len(clean_dataset) 1478
after delete: len(noisy_dataset) 1658
epoch [46/200] batch [5/46] time 0.439 (0.512) data 0.308 (0.381) loss_x loss_x 1.5332 (1.3826) acc_x 65.6250 (63.7500) lr 1.7705e-03 eta 0:00:20
epoch [46/200] batch [10/46] time 0.432 (0.499) data 0.300 (0.367) loss_x loss_x 1.1445 (1.2771) acc_x 65.6250 (65.3125) lr 1.7705e-03 eta 0:00:17
epoch [46/200] batch [15/46] time 0.439 (0.501) data 0.307 (0.370) loss_x loss_x 1.0312 (1.2295) acc_x 78.1250 (67.2917) lr 1.7705e-03 eta 0:00:15
epoch [46/200] batch [20/46] time 0.516 (0.502) data 0.385 (0.371) loss_x loss_x 1.4912 (1.2574) acc_x 62.5000 (67.9688) lr 1.7705e-03 eta 0:00:13
epoch [46/200] batch [25/46] time 0.484 (0.498) data 0.352 (0.366) loss_x loss_x 1.5957 (1.2642) acc_x 62.5000 (68.7500) lr 1.7705e-03 eta 0:00:10
epoch [46/200] batch [30/46] time 0.429 (0.492) data 0.298 (0.361) loss_x loss_x 0.9692 (1.2742) acc_x 75.0000 (68.2292) lr 1.7705e-03 eta 0:00:07
epoch [46/200] batch [35/46] time 0.462 (0.487) data 0.332 (0.355) loss_x loss_x 1.0713 (1.2801) acc_x 75.0000 (68.1250) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [40/46] time 0.392 (0.489) data 0.260 (0.357) loss_x loss_x 1.4795 (1.2844) acc_x 65.6250 (68.2812) lr 1.7705e-03 eta 0:00:02
epoch [46/200] batch [45/46] time 0.398 (0.482) data 0.268 (0.351) loss_x loss_x 1.5654 (1.3012) acc_x 65.6250 (67.9167) lr 1.7705e-03 eta 0:00:00
epoch [46/200] batch [5/51] time 0.462 (0.480) data 0.330 (0.349) loss_u loss_u 0.8091 (0.7745) acc_u 25.0000 (28.1250) lr 1.7705e-03 eta 0:00:22
epoch [46/200] batch [10/51] time 0.396 (0.476) data 0.265 (0.345) loss_u loss_u 0.7280 (0.7837) acc_u 28.1250 (27.1875) lr 1.7705e-03 eta 0:00:19
epoch [46/200] batch [15/51] time 0.477 (0.475) data 0.345 (0.343) loss_u loss_u 0.8521 (0.7869) acc_u 18.7500 (27.0833) lr 1.7705e-03 eta 0:00:17
epoch [46/200] batch [20/51] time 0.592 (0.476) data 0.461 (0.344) loss_u loss_u 0.7622 (0.7855) acc_u 40.6250 (28.1250) lr 1.7705e-03 eta 0:00:14
epoch [46/200] batch [25/51] time 0.423 (0.475) data 0.290 (0.344) loss_u loss_u 0.7563 (0.7932) acc_u 28.1250 (26.3750) lr 1.7705e-03 eta 0:00:12
epoch [46/200] batch [30/51] time 0.399 (0.474) data 0.268 (0.343) loss_u loss_u 0.8560 (0.7913) acc_u 21.8750 (26.8750) lr 1.7705e-03 eta 0:00:09
epoch [46/200] batch [35/51] time 0.491 (0.470) data 0.359 (0.339) loss_u loss_u 0.7539 (0.7956) acc_u 31.2500 (26.4286) lr 1.7705e-03 eta 0:00:07
epoch [46/200] batch [40/51] time 0.521 (0.472) data 0.390 (0.340) loss_u loss_u 0.5796 (0.7898) acc_u 46.8750 (27.1875) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [45/51] time 0.519 (0.471) data 0.387 (0.340) loss_u loss_u 0.8086 (0.7894) acc_u 34.3750 (27.5000) lr 1.7705e-03 eta 0:00:02
epoch [46/200] batch [50/51] time 0.437 (0.470) data 0.306 (0.339) loss_u loss_u 0.7827 (0.7908) acc_u 31.2500 (27.7500) lr 1.7705e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1489
confident_label rate tensor(0.4745, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1488
clean true:1488
clean false:0
clean_rate:1.0
noisy true:159
noisy false:1489
after delete: len(clean_dataset) 1488
after delete: len(noisy_dataset) 1648
epoch [47/200] batch [5/46] time 0.430 (0.480) data 0.299 (0.349) loss_x loss_x 1.4922 (1.3139) acc_x 62.5000 (66.2500) lr 1.7604e-03 eta 0:00:19
epoch [47/200] batch [10/46] time 0.402 (0.456) data 0.272 (0.325) loss_x loss_x 0.9644 (1.2481) acc_x 81.2500 (68.1250) lr 1.7604e-03 eta 0:00:16
epoch [47/200] batch [15/46] time 0.569 (0.467) data 0.439 (0.336) loss_x loss_x 1.0469 (1.2883) acc_x 71.8750 (66.6667) lr 1.7604e-03 eta 0:00:14
epoch [47/200] batch [20/46] time 0.500 (0.465) data 0.366 (0.334) loss_x loss_x 1.1172 (1.3171) acc_x 75.0000 (67.1875) lr 1.7604e-03 eta 0:00:12
epoch [47/200] batch [25/46] time 0.433 (0.462) data 0.302 (0.331) loss_x loss_x 1.1904 (1.2901) acc_x 71.8750 (66.7500) lr 1.7604e-03 eta 0:00:09
epoch [47/200] batch [30/46] time 0.425 (0.460) data 0.294 (0.328) loss_x loss_x 1.3662 (1.2896) acc_x 65.6250 (66.4583) lr 1.7604e-03 eta 0:00:07
epoch [47/200] batch [35/46] time 0.449 (0.462) data 0.317 (0.330) loss_x loss_x 1.0771 (1.2990) acc_x 71.8750 (66.6071) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [40/46] time 0.534 (0.471) data 0.403 (0.340) loss_x loss_x 1.1914 (1.2924) acc_x 65.6250 (66.3281) lr 1.7604e-03 eta 0:00:02
epoch [47/200] batch [45/46] time 0.615 (0.472) data 0.483 (0.340) loss_x loss_x 1.1084 (1.2876) acc_x 71.8750 (66.5278) lr 1.7604e-03 eta 0:00:00
epoch [47/200] batch [5/51] time 0.346 (0.464) data 0.214 (0.332) loss_u loss_u 0.7783 (0.8008) acc_u 25.0000 (27.5000) lr 1.7604e-03 eta 0:00:21
epoch [47/200] batch [10/51] time 0.609 (0.462) data 0.476 (0.330) loss_u loss_u 0.7427 (0.7802) acc_u 31.2500 (29.0625) lr 1.7604e-03 eta 0:00:18
epoch [47/200] batch [15/51] time 0.333 (0.457) data 0.201 (0.326) loss_u loss_u 0.7261 (0.7857) acc_u 34.3750 (28.9583) lr 1.7604e-03 eta 0:00:16
epoch [47/200] batch [20/51] time 0.379 (0.453) data 0.247 (0.321) loss_u loss_u 0.8638 (0.7877) acc_u 18.7500 (28.9062) lr 1.7604e-03 eta 0:00:14
epoch [47/200] batch [25/51] time 0.753 (0.461) data 0.621 (0.329) loss_u loss_u 0.7583 (0.7904) acc_u 31.2500 (28.0000) lr 1.7604e-03 eta 0:00:11
epoch [47/200] batch [30/51] time 0.434 (0.456) data 0.302 (0.325) loss_u loss_u 0.7402 (0.7863) acc_u 31.2500 (28.3333) lr 1.7604e-03 eta 0:00:09
epoch [47/200] batch [35/51] time 0.411 (0.452) data 0.279 (0.320) loss_u loss_u 0.9155 (0.7925) acc_u 9.3750 (27.7679) lr 1.7604e-03 eta 0:00:07
epoch [47/200] batch [40/51] time 0.441 (0.451) data 0.309 (0.319) loss_u loss_u 0.7607 (0.7933) acc_u 37.5000 (27.7344) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [45/51] time 0.463 (0.451) data 0.332 (0.319) loss_u loss_u 0.7754 (0.7922) acc_u 34.3750 (28.1250) lr 1.7604e-03 eta 0:00:02
epoch [47/200] batch [50/51] time 0.537 (0.451) data 0.406 (0.319) loss_u loss_u 0.8188 (0.7925) acc_u 15.6250 (27.7500) lr 1.7604e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1522
confident_label rate tensor(0.4646, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1457
clean true:1457
clean false:0
clean_rate:1.0
noisy true:157
noisy false:1522
after delete: len(clean_dataset) 1457
after delete: len(noisy_dataset) 1679
epoch [48/200] batch [5/45] time 0.468 (0.485) data 0.337 (0.354) loss_x loss_x 1.2715 (1.4324) acc_x 68.7500 (63.1250) lr 1.7501e-03 eta 0:00:19
epoch [48/200] batch [10/45] time 0.437 (0.467) data 0.306 (0.336) loss_x loss_x 0.9839 (1.3169) acc_x 68.7500 (65.9375) lr 1.7501e-03 eta 0:00:16
epoch [48/200] batch [15/45] time 0.647 (0.475) data 0.516 (0.344) loss_x loss_x 1.2197 (1.2682) acc_x 68.7500 (67.5000) lr 1.7501e-03 eta 0:00:14
epoch [48/200] batch [20/45] time 0.476 (0.481) data 0.345 (0.350) loss_x loss_x 0.9531 (1.2200) acc_x 78.1250 (68.9062) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [25/45] time 0.334 (0.471) data 0.203 (0.340) loss_x loss_x 1.3994 (1.2464) acc_x 68.7500 (68.3750) lr 1.7501e-03 eta 0:00:09
epoch [48/200] batch [30/45] time 0.462 (0.467) data 0.332 (0.335) loss_x loss_x 0.9468 (1.2459) acc_x 75.0000 (68.3333) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [35/45] time 0.542 (0.473) data 0.410 (0.341) loss_x loss_x 0.9790 (1.2547) acc_x 81.2500 (68.3929) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [40/45] time 0.485 (0.470) data 0.353 (0.339) loss_x loss_x 1.2402 (1.2618) acc_x 75.0000 (68.5938) lr 1.7501e-03 eta 0:00:02
epoch [48/200] batch [45/45] time 0.466 (0.473) data 0.335 (0.341) loss_x loss_x 1.1758 (1.2608) acc_x 75.0000 (68.6806) lr 1.7501e-03 eta 0:00:00
epoch [48/200] batch [5/52] time 0.406 (0.474) data 0.274 (0.343) loss_u loss_u 0.7227 (0.7521) acc_u 34.3750 (33.1250) lr 1.7501e-03 eta 0:00:22
epoch [48/200] batch [10/52] time 0.453 (0.470) data 0.321 (0.339) loss_u loss_u 0.8823 (0.7681) acc_u 12.5000 (30.3125) lr 1.7501e-03 eta 0:00:19
epoch [48/200] batch [15/52] time 0.561 (0.472) data 0.428 (0.341) loss_u loss_u 0.7188 (0.7772) acc_u 40.6250 (30.4167) lr 1.7501e-03 eta 0:00:17
epoch [48/200] batch [20/52] time 0.442 (0.468) data 0.310 (0.336) loss_u loss_u 0.8784 (0.7883) acc_u 9.3750 (28.1250) lr 1.7501e-03 eta 0:00:14
epoch [48/200] batch [25/52] time 0.532 (0.467) data 0.401 (0.336) loss_u loss_u 0.7290 (0.7839) acc_u 28.1250 (28.5000) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [30/52] time 0.471 (0.469) data 0.339 (0.338) loss_u loss_u 0.8242 (0.7823) acc_u 15.6250 (28.1250) lr 1.7501e-03 eta 0:00:10
epoch [48/200] batch [35/52] time 0.536 (0.469) data 0.404 (0.337) loss_u loss_u 0.6782 (0.7845) acc_u 37.5000 (28.0357) lr 1.7501e-03 eta 0:00:07
epoch [48/200] batch [40/52] time 0.405 (0.470) data 0.274 (0.339) loss_u loss_u 0.8408 (0.7850) acc_u 25.0000 (28.0469) lr 1.7501e-03 eta 0:00:05
epoch [48/200] batch [45/52] time 0.314 (0.467) data 0.182 (0.335) loss_u loss_u 0.8086 (0.7881) acc_u 25.0000 (27.6389) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [50/52] time 0.420 (0.464) data 0.288 (0.332) loss_u loss_u 0.7651 (0.7857) acc_u 34.3750 (27.8750) lr 1.7501e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1468
confident_label rate tensor(0.4809, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1508
clean true:1504
clean false:4
clean_rate:0.9973474801061007
noisy true:164
noisy false:1464
after delete: len(clean_dataset) 1508
after delete: len(noisy_dataset) 1628
epoch [49/200] batch [5/47] time 0.317 (0.460) data 0.186 (0.329) loss_x loss_x 1.7891 (1.3719) acc_x 62.5000 (66.2500) lr 1.7396e-03 eta 0:00:19
epoch [49/200] batch [10/47] time 0.400 (0.472) data 0.269 (0.341) loss_x loss_x 1.5879 (1.4109) acc_x 65.6250 (66.8750) lr 1.7396e-03 eta 0:00:17
epoch [49/200] batch [15/47] time 0.503 (0.483) data 0.372 (0.352) loss_x loss_x 1.3809 (1.3218) acc_x 65.6250 (68.1250) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [20/47] time 0.413 (0.468) data 0.282 (0.337) loss_x loss_x 1.4609 (1.2822) acc_x 65.6250 (69.0625) lr 1.7396e-03 eta 0:00:12
epoch [49/200] batch [25/47] time 0.467 (0.475) data 0.335 (0.344) loss_x loss_x 1.5146 (1.2650) acc_x 68.7500 (69.8750) lr 1.7396e-03 eta 0:00:10
epoch [49/200] batch [30/47] time 0.589 (0.485) data 0.458 (0.354) loss_x loss_x 1.2266 (1.2502) acc_x 71.8750 (69.7917) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [35/47] time 0.476 (0.477) data 0.345 (0.346) loss_x loss_x 1.4189 (1.3026) acc_x 68.7500 (68.7500) lr 1.7396e-03 eta 0:00:05
epoch [49/200] batch [40/47] time 0.498 (0.476) data 0.368 (0.344) loss_x loss_x 1.1123 (1.2921) acc_x 78.1250 (68.8281) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [45/47] time 0.510 (0.478) data 0.379 (0.347) loss_x loss_x 1.3584 (1.3073) acc_x 62.5000 (67.9167) lr 1.7396e-03 eta 0:00:00
epoch [49/200] batch [5/50] time 0.497 (0.474) data 0.365 (0.343) loss_u loss_u 0.7256 (0.7880) acc_u 31.2500 (25.0000) lr 1.7396e-03 eta 0:00:21
epoch [49/200] batch [10/50] time 0.359 (0.468) data 0.227 (0.337) loss_u loss_u 0.7646 (0.7827) acc_u 37.5000 (27.5000) lr 1.7396e-03 eta 0:00:18
epoch [49/200] batch [15/50] time 0.366 (0.466) data 0.235 (0.334) loss_u loss_u 0.7798 (0.7841) acc_u 25.0000 (28.3333) lr 1.7396e-03 eta 0:00:16
epoch [49/200] batch [20/50] time 0.666 (0.468) data 0.534 (0.336) loss_u loss_u 0.8594 (0.7879) acc_u 25.0000 (28.1250) lr 1.7396e-03 eta 0:00:14
epoch [49/200] batch [25/50] time 0.340 (0.468) data 0.208 (0.336) loss_u loss_u 0.8516 (0.7896) acc_u 15.6250 (27.7500) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [30/50] time 0.726 (0.466) data 0.594 (0.335) loss_u loss_u 0.7598 (0.7949) acc_u 28.1250 (26.5625) lr 1.7396e-03 eta 0:00:09
epoch [49/200] batch [35/50] time 0.328 (0.467) data 0.197 (0.336) loss_u loss_u 0.8394 (0.7955) acc_u 25.0000 (26.5179) lr 1.7396e-03 eta 0:00:07
epoch [49/200] batch [40/50] time 0.425 (0.468) data 0.293 (0.336) loss_u loss_u 0.8042 (0.7925) acc_u 21.8750 (26.5625) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [45/50] time 0.557 (0.466) data 0.425 (0.335) loss_u loss_u 0.7495 (0.7885) acc_u 37.5000 (27.1528) lr 1.7396e-03 eta 0:00:02
epoch [49/200] batch [50/50] time 0.402 (0.465) data 0.271 (0.333) loss_u loss_u 0.7798 (0.7871) acc_u 25.0000 (27.2500) lr 1.7396e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1451
confident_label rate tensor(0.4815, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1510
clean true:1510
clean false:0
clean_rate:1.0
noisy true:175
noisy false:1451
after delete: len(clean_dataset) 1510
after delete: len(noisy_dataset) 1626
epoch [50/200] batch [5/47] time 0.432 (0.463) data 0.302 (0.332) loss_x loss_x 1.0840 (1.2885) acc_x 75.0000 (67.5000) lr 1.7290e-03 eta 0:00:19
epoch [50/200] batch [10/47] time 0.494 (0.458) data 0.362 (0.327) loss_x loss_x 0.9795 (1.3560) acc_x 65.6250 (64.3750) lr 1.7290e-03 eta 0:00:16
epoch [50/200] batch [15/47] time 0.451 (0.453) data 0.320 (0.322) loss_x loss_x 0.9473 (1.2999) acc_x 75.0000 (66.0417) lr 1.7290e-03 eta 0:00:14
epoch [50/200] batch [20/47] time 0.585 (0.470) data 0.453 (0.339) loss_x loss_x 1.1514 (1.2393) acc_x 71.8750 (67.6562) lr 1.7290e-03 eta 0:00:12
epoch [50/200] batch [25/47] time 0.439 (0.463) data 0.307 (0.332) loss_x loss_x 1.6943 (1.2950) acc_x 65.6250 (66.5000) lr 1.7290e-03 eta 0:00:10
epoch [50/200] batch [30/47] time 0.510 (0.469) data 0.378 (0.338) loss_x loss_x 1.8340 (1.3158) acc_x 65.6250 (66.4583) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [35/47] time 0.583 (0.485) data 0.452 (0.354) loss_x loss_x 0.7290 (1.2884) acc_x 87.5000 (66.9643) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [40/47] time 0.465 (0.487) data 0.334 (0.356) loss_x loss_x 1.4238 (1.2722) acc_x 65.6250 (67.0312) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [45/47] time 0.444 (0.491) data 0.313 (0.360) loss_x loss_x 1.5869 (1.2759) acc_x 62.5000 (67.1528) lr 1.7290e-03 eta 0:00:00
epoch [50/200] batch [5/50] time 0.376 (0.483) data 0.244 (0.352) loss_u loss_u 0.7935 (0.8029) acc_u 31.2500 (24.3750) lr 1.7290e-03 eta 0:00:21
epoch [50/200] batch [10/50] time 0.499 (0.483) data 0.367 (0.352) loss_u loss_u 0.6865 (0.7758) acc_u 37.5000 (27.8125) lr 1.7290e-03 eta 0:00:19
epoch [50/200] batch [15/50] time 0.464 (0.481) data 0.332 (0.349) loss_u loss_u 0.8511 (0.7879) acc_u 25.0000 (27.2917) lr 1.7290e-03 eta 0:00:16
epoch [50/200] batch [20/50] time 0.386 (0.479) data 0.254 (0.348) loss_u loss_u 0.7651 (0.7859) acc_u 37.5000 (27.9688) lr 1.7290e-03 eta 0:00:14
epoch [50/200] batch [25/50] time 0.471 (0.478) data 0.338 (0.346) loss_u loss_u 0.7686 (0.7907) acc_u 34.3750 (27.1250) lr 1.7290e-03 eta 0:00:11
epoch [50/200] batch [30/50] time 0.468 (0.478) data 0.336 (0.346) loss_u loss_u 0.7969 (0.7926) acc_u 28.1250 (26.7708) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [35/50] time 0.439 (0.475) data 0.308 (0.344) loss_u loss_u 0.8003 (0.7929) acc_u 18.7500 (26.6071) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [40/50] time 0.451 (0.478) data 0.319 (0.347) loss_u loss_u 0.7832 (0.7928) acc_u 28.1250 (26.4844) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [45/50] time 0.429 (0.478) data 0.298 (0.346) loss_u loss_u 0.7524 (0.7906) acc_u 31.2500 (26.7361) lr 1.7290e-03 eta 0:00:02
epoch [50/200] batch [50/50] time 0.426 (0.478) data 0.295 (0.346) loss_u loss_u 0.8438 (0.7909) acc_u 28.1250 (27.0000) lr 1.7290e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1444
confident_label rate tensor(0.4872, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1528
clean true:1527
clean false:1
clean_rate:0.9993455497382199
noisy true:165
noisy false:1443
after delete: len(clean_dataset) 1528
after delete: len(noisy_dataset) 1608
epoch [51/200] batch [5/47] time 0.488 (0.509) data 0.357 (0.378) loss_x loss_x 1.5312 (1.4893) acc_x 62.5000 (63.7500) lr 1.7181e-03 eta 0:00:21
epoch [51/200] batch [10/47] time 0.503 (0.506) data 0.373 (0.376) loss_x loss_x 0.8452 (1.4056) acc_x 78.1250 (64.0625) lr 1.7181e-03 eta 0:00:18
epoch [51/200] batch [15/47] time 0.459 (0.494) data 0.328 (0.363) loss_x loss_x 1.4238 (1.3051) acc_x 62.5000 (67.9167) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [20/47] time 0.481 (0.484) data 0.350 (0.353) loss_x loss_x 1.2334 (1.3215) acc_x 71.8750 (67.0312) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [25/47] time 0.350 (0.476) data 0.218 (0.345) loss_x loss_x 0.8438 (1.3017) acc_x 81.2500 (67.1250) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [30/47] time 0.347 (0.468) data 0.216 (0.337) loss_x loss_x 1.5469 (1.2953) acc_x 62.5000 (67.1875) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [35/47] time 0.439 (0.470) data 0.308 (0.339) loss_x loss_x 1.0889 (1.2743) acc_x 78.1250 (67.4107) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [40/47] time 0.482 (0.464) data 0.351 (0.333) loss_x loss_x 0.8662 (1.2804) acc_x 71.8750 (66.9531) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [45/47] time 0.397 (0.463) data 0.265 (0.331) loss_x loss_x 1.9990 (1.2865) acc_x 59.3750 (67.0139) lr 1.7181e-03 eta 0:00:00
epoch [51/200] batch [5/50] time 0.373 (0.463) data 0.241 (0.332) loss_u loss_u 0.8389 (0.8222) acc_u 21.8750 (23.1250) lr 1.7181e-03 eta 0:00:20
epoch [51/200] batch [10/50] time 0.376 (0.460) data 0.244 (0.329) loss_u loss_u 0.7661 (0.8018) acc_u 34.3750 (26.5625) lr 1.7181e-03 eta 0:00:18
epoch [51/200] batch [15/50] time 0.553 (0.466) data 0.421 (0.334) loss_u loss_u 0.7158 (0.8065) acc_u 37.5000 (25.6250) lr 1.7181e-03 eta 0:00:16
epoch [51/200] batch [20/50] time 0.487 (0.463) data 0.355 (0.331) loss_u loss_u 0.7852 (0.8052) acc_u 25.0000 (24.8438) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [25/50] time 0.515 (0.465) data 0.385 (0.333) loss_u loss_u 0.7881 (0.8041) acc_u 25.0000 (25.0000) lr 1.7181e-03 eta 0:00:11
epoch [51/200] batch [30/50] time 0.366 (0.463) data 0.235 (0.332) loss_u loss_u 0.8462 (0.8050) acc_u 15.6250 (24.5833) lr 1.7181e-03 eta 0:00:09
epoch [51/200] batch [35/50] time 0.469 (0.463) data 0.338 (0.331) loss_u loss_u 0.8184 (0.8006) acc_u 31.2500 (25.5357) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [40/50] time 0.436 (0.463) data 0.304 (0.331) loss_u loss_u 0.8550 (0.8035) acc_u 12.5000 (25.1562) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [45/50] time 0.381 (0.465) data 0.248 (0.334) loss_u loss_u 0.7656 (0.7996) acc_u 31.2500 (25.9028) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [50/50] time 0.465 (0.464) data 0.333 (0.333) loss_u loss_u 0.7725 (0.7966) acc_u 25.0000 (26.2500) lr 1.7181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1480
confident_label rate tensor(0.4754, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1491
clean true:1490
clean false:1
clean_rate:0.9993293091884641
noisy true:166
noisy false:1479
after delete: len(clean_dataset) 1491
after delete: len(noisy_dataset) 1645
epoch [52/200] batch [5/46] time 0.451 (0.441) data 0.320 (0.310) loss_x loss_x 1.6738 (1.5564) acc_x 65.6250 (65.6250) lr 1.7071e-03 eta 0:00:18
epoch [52/200] batch [10/46] time 0.421 (0.428) data 0.290 (0.298) loss_x loss_x 1.2188 (1.3790) acc_x 71.8750 (68.1250) lr 1.7071e-03 eta 0:00:15
epoch [52/200] batch [15/46] time 0.549 (0.437) data 0.418 (0.306) loss_x loss_x 0.8452 (1.2774) acc_x 81.2500 (69.3750) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [20/46] time 0.348 (0.446) data 0.217 (0.316) loss_x loss_x 1.0479 (1.2400) acc_x 71.8750 (69.0625) lr 1.7071e-03 eta 0:00:11
epoch [52/200] batch [25/46] time 0.463 (0.464) data 0.331 (0.333) loss_x loss_x 1.8105 (1.2448) acc_x 59.3750 (68.7500) lr 1.7071e-03 eta 0:00:09
epoch [52/200] batch [30/46] time 0.599 (0.475) data 0.469 (0.344) loss_x loss_x 1.5078 (1.2375) acc_x 56.2500 (68.2292) lr 1.7071e-03 eta 0:00:07
epoch [52/200] batch [35/46] time 0.478 (0.478) data 0.347 (0.347) loss_x loss_x 1.6436 (1.2222) acc_x 62.5000 (68.9286) lr 1.7071e-03 eta 0:00:05
epoch [52/200] batch [40/46] time 0.511 (0.479) data 0.380 (0.348) loss_x loss_x 0.9478 (1.2205) acc_x 78.1250 (68.8281) lr 1.7071e-03 eta 0:00:02
epoch [52/200] batch [45/46] time 0.495 (0.485) data 0.363 (0.354) loss_x loss_x 0.9106 (1.2352) acc_x 75.0000 (68.7500) lr 1.7071e-03 eta 0:00:00
epoch [52/200] batch [5/51] time 0.402 (0.483) data 0.269 (0.352) loss_u loss_u 0.7749 (0.7736) acc_u 28.1250 (30.0000) lr 1.7071e-03 eta 0:00:22
epoch [52/200] batch [10/51] time 0.488 (0.480) data 0.356 (0.349) loss_u loss_u 0.8706 (0.7855) acc_u 21.8750 (29.6875) lr 1.7071e-03 eta 0:00:19
epoch [52/200] batch [15/51] time 0.506 (0.477) data 0.374 (0.346) loss_u loss_u 0.8267 (0.7866) acc_u 25.0000 (28.3333) lr 1.7071e-03 eta 0:00:17
epoch [52/200] batch [20/51] time 0.356 (0.474) data 0.224 (0.343) loss_u loss_u 0.7676 (0.7845) acc_u 31.2500 (27.9688) lr 1.7071e-03 eta 0:00:14
epoch [52/200] batch [25/51] time 0.366 (0.471) data 0.234 (0.340) loss_u loss_u 0.8853 (0.7853) acc_u 12.5000 (28.1250) lr 1.7071e-03 eta 0:00:12
epoch [52/200] batch [30/51] time 0.339 (0.472) data 0.208 (0.341) loss_u loss_u 0.7471 (0.7842) acc_u 28.1250 (28.1250) lr 1.7071e-03 eta 0:00:09
epoch [52/200] batch [35/51] time 0.358 (0.470) data 0.226 (0.339) loss_u loss_u 0.7837 (0.7822) acc_u 25.0000 (28.2143) lr 1.7071e-03 eta 0:00:07
epoch [52/200] batch [40/51] time 0.376 (0.467) data 0.244 (0.335) loss_u loss_u 0.7246 (0.7833) acc_u 34.3750 (28.1250) lr 1.7071e-03 eta 0:00:05
epoch [52/200] batch [45/51] time 0.538 (0.465) data 0.406 (0.333) loss_u loss_u 0.8535 (0.7825) acc_u 25.0000 (28.1944) lr 1.7071e-03 eta 0:00:02
epoch [52/200] batch [50/51] time 0.372 (0.463) data 0.240 (0.331) loss_u loss_u 0.8501 (0.7816) acc_u 18.7500 (28.0625) lr 1.7071e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1467
confident_label rate tensor(0.4802, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1506
clean true:1505
clean false:1
clean_rate:0.99933598937583
noisy true:164
noisy false:1466
after delete: len(clean_dataset) 1506
after delete: len(noisy_dataset) 1630
epoch [53/200] batch [5/47] time 0.411 (0.438) data 0.280 (0.307) loss_x loss_x 1.1816 (1.0339) acc_x 65.6250 (75.6250) lr 1.6959e-03 eta 0:00:18
epoch [53/200] batch [10/47] time 0.462 (0.455) data 0.329 (0.324) loss_x loss_x 1.3955 (1.2166) acc_x 68.7500 (70.3125) lr 1.6959e-03 eta 0:00:16
epoch [53/200] batch [15/47] time 0.440 (0.449) data 0.309 (0.318) loss_x loss_x 1.9639 (1.2560) acc_x 53.1250 (68.7500) lr 1.6959e-03 eta 0:00:14
epoch [53/200] batch [20/47] time 0.492 (0.452) data 0.361 (0.321) loss_x loss_x 1.1611 (1.2363) acc_x 75.0000 (69.3750) lr 1.6959e-03 eta 0:00:12
epoch [53/200] batch [25/47] time 0.447 (0.463) data 0.315 (0.332) loss_x loss_x 1.5254 (1.2274) acc_x 68.7500 (70.0000) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [30/47] time 0.474 (0.468) data 0.342 (0.336) loss_x loss_x 1.4473 (1.2549) acc_x 62.5000 (69.1667) lr 1.6959e-03 eta 0:00:07
epoch [53/200] batch [35/47] time 0.553 (0.463) data 0.422 (0.332) loss_x loss_x 1.0947 (1.2623) acc_x 75.0000 (68.5714) lr 1.6959e-03 eta 0:00:05
epoch [53/200] batch [40/47] time 0.506 (0.465) data 0.375 (0.334) loss_x loss_x 1.4131 (1.2787) acc_x 59.3750 (67.6562) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [45/47] time 0.464 (0.467) data 0.334 (0.336) loss_x loss_x 1.5352 (1.2917) acc_x 62.5000 (67.3611) lr 1.6959e-03 eta 0:00:00
epoch [53/200] batch [5/50] time 0.492 (0.472) data 0.361 (0.341) loss_u loss_u 0.8057 (0.8050) acc_u 28.1250 (23.7500) lr 1.6959e-03 eta 0:00:21
epoch [53/200] batch [10/50] time 0.505 (0.474) data 0.373 (0.342) loss_u loss_u 0.8320 (0.8022) acc_u 28.1250 (24.6875) lr 1.6959e-03 eta 0:00:18
epoch [53/200] batch [15/50] time 0.379 (0.471) data 0.247 (0.340) loss_u loss_u 0.7754 (0.8033) acc_u 31.2500 (24.7917) lr 1.6959e-03 eta 0:00:16
epoch [53/200] batch [20/50] time 0.428 (0.473) data 0.297 (0.342) loss_u loss_u 0.8027 (0.8056) acc_u 21.8750 (24.6875) lr 1.6959e-03 eta 0:00:14
epoch [53/200] batch [25/50] time 0.428 (0.468) data 0.297 (0.337) loss_u loss_u 0.7456 (0.7971) acc_u 34.3750 (26.0000) lr 1.6959e-03 eta 0:00:11
epoch [53/200] batch [30/50] time 0.396 (0.467) data 0.265 (0.336) loss_u loss_u 0.8257 (0.7946) acc_u 21.8750 (26.4583) lr 1.6959e-03 eta 0:00:09
epoch [53/200] batch [35/50] time 0.445 (0.467) data 0.313 (0.336) loss_u loss_u 0.8037 (0.7964) acc_u 25.0000 (26.4286) lr 1.6959e-03 eta 0:00:07
epoch [53/200] batch [40/50] time 0.480 (0.465) data 0.348 (0.334) loss_u loss_u 0.8130 (0.7941) acc_u 25.0000 (26.6406) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [45/50] time 0.560 (0.469) data 0.428 (0.338) loss_u loss_u 0.7939 (0.7917) acc_u 28.1250 (27.1528) lr 1.6959e-03 eta 0:00:02
epoch [53/200] batch [50/50] time 0.451 (0.466) data 0.320 (0.335) loss_u loss_u 0.7861 (0.7918) acc_u 28.1250 (27.6250) lr 1.6959e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1487
confident_label rate tensor(0.4723, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1481
clean true:1480
clean false:1
clean_rate:0.9993247805536799
noisy true:169
noisy false:1486
after delete: len(clean_dataset) 1481
after delete: len(noisy_dataset) 1655
epoch [54/200] batch [5/46] time 0.438 (0.474) data 0.307 (0.343) loss_x loss_x 2.1699 (1.5518) acc_x 40.6250 (62.5000) lr 1.6845e-03 eta 0:00:19
epoch [54/200] batch [10/46] time 0.495 (0.510) data 0.364 (0.379) loss_x loss_x 1.2773 (1.4365) acc_x 59.3750 (64.3750) lr 1.6845e-03 eta 0:00:18
epoch [54/200] batch [15/46] time 0.456 (0.498) data 0.325 (0.367) loss_x loss_x 1.4473 (1.3157) acc_x 53.1250 (67.0833) lr 1.6845e-03 eta 0:00:15
epoch [54/200] batch [20/46] time 0.533 (0.501) data 0.402 (0.369) loss_x loss_x 1.0986 (1.2389) acc_x 71.8750 (69.0625) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [25/46] time 0.423 (0.500) data 0.291 (0.368) loss_x loss_x 1.5303 (1.2423) acc_x 56.2500 (69.1250) lr 1.6845e-03 eta 0:00:10
epoch [54/200] batch [30/46] time 0.556 (0.503) data 0.425 (0.372) loss_x loss_x 1.3809 (1.2216) acc_x 68.7500 (69.5833) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [35/46] time 0.477 (0.505) data 0.346 (0.374) loss_x loss_x 1.4258 (1.2076) acc_x 65.6250 (70.0893) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [40/46] time 0.418 (0.495) data 0.287 (0.364) loss_x loss_x 0.7598 (1.1968) acc_x 75.0000 (70.0781) lr 1.6845e-03 eta 0:00:02
epoch [54/200] batch [45/46] time 0.436 (0.499) data 0.305 (0.368) loss_x loss_x 0.7710 (1.1952) acc_x 71.8750 (69.9306) lr 1.6845e-03 eta 0:00:00
epoch [54/200] batch [5/51] time 0.427 (0.489) data 0.295 (0.358) loss_u loss_u 0.7725 (0.8025) acc_u 31.2500 (27.5000) lr 1.6845e-03 eta 0:00:22
epoch [54/200] batch [10/51] time 0.510 (0.488) data 0.377 (0.357) loss_u loss_u 0.7383 (0.8052) acc_u 28.1250 (25.3125) lr 1.6845e-03 eta 0:00:20
epoch [54/200] batch [15/51] time 0.489 (0.485) data 0.358 (0.354) loss_u loss_u 0.7383 (0.7854) acc_u 31.2500 (27.7083) lr 1.6845e-03 eta 0:00:17
epoch [54/200] batch [20/51] time 0.452 (0.480) data 0.320 (0.349) loss_u loss_u 0.7651 (0.7866) acc_u 37.5000 (28.7500) lr 1.6845e-03 eta 0:00:14
epoch [54/200] batch [25/51] time 0.442 (0.482) data 0.310 (0.351) loss_u loss_u 0.7104 (0.7807) acc_u 40.6250 (30.1250) lr 1.6845e-03 eta 0:00:12
epoch [54/200] batch [30/51] time 0.465 (0.479) data 0.334 (0.348) loss_u loss_u 0.7837 (0.7819) acc_u 25.0000 (30.1042) lr 1.6845e-03 eta 0:00:10
epoch [54/200] batch [35/51] time 0.498 (0.476) data 0.367 (0.344) loss_u loss_u 0.7969 (0.7857) acc_u 18.7500 (29.0179) lr 1.6845e-03 eta 0:00:07
epoch [54/200] batch [40/51] time 0.458 (0.477) data 0.326 (0.345) loss_u loss_u 0.7930 (0.7839) acc_u 25.0000 (29.1406) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [45/51] time 0.421 (0.473) data 0.290 (0.341) loss_u loss_u 0.7412 (0.7861) acc_u 28.1250 (28.6111) lr 1.6845e-03 eta 0:00:02
epoch [54/200] batch [50/51] time 0.523 (0.472) data 0.391 (0.341) loss_u loss_u 0.8193 (0.7844) acc_u 25.0000 (28.9375) lr 1.6845e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1457
confident_label rate tensor(0.4786, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1501
clean true:1501
clean false:0
clean_rate:1.0
noisy true:178
noisy false:1457
after delete: len(clean_dataset) 1501
after delete: len(noisy_dataset) 1635
epoch [55/200] batch [5/46] time 0.458 (0.471) data 0.327 (0.340) loss_x loss_x 1.3350 (1.3938) acc_x 71.8750 (67.5000) lr 1.6730e-03 eta 0:00:19
epoch [55/200] batch [10/46] time 0.410 (0.460) data 0.278 (0.329) loss_x loss_x 1.5830 (1.4120) acc_x 59.3750 (67.5000) lr 1.6730e-03 eta 0:00:16
epoch [55/200] batch [15/46] time 0.410 (0.452) data 0.279 (0.321) loss_x loss_x 1.0049 (1.2518) acc_x 84.3750 (71.2500) lr 1.6730e-03 eta 0:00:13
epoch [55/200] batch [20/46] time 0.456 (0.453) data 0.324 (0.322) loss_x loss_x 1.0918 (1.2073) acc_x 68.7500 (72.0312) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [25/46] time 0.480 (0.460) data 0.349 (0.329) loss_x loss_x 1.2090 (1.2326) acc_x 65.6250 (71.6250) lr 1.6730e-03 eta 0:00:09
epoch [55/200] batch [30/46] time 0.543 (0.467) data 0.412 (0.336) loss_x loss_x 1.3115 (1.2144) acc_x 68.7500 (71.3542) lr 1.6730e-03 eta 0:00:07
epoch [55/200] batch [35/46] time 0.664 (0.474) data 0.532 (0.343) loss_x loss_x 0.9688 (1.2057) acc_x 75.0000 (71.0714) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [40/46] time 0.445 (0.474) data 0.314 (0.343) loss_x loss_x 1.1484 (1.2123) acc_x 68.7500 (70.8594) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [45/46] time 0.483 (0.474) data 0.352 (0.343) loss_x loss_x 1.1064 (1.2080) acc_x 62.5000 (70.4167) lr 1.6730e-03 eta 0:00:00
epoch [55/200] batch [5/51] time 0.412 (0.475) data 0.280 (0.344) loss_u loss_u 0.8149 (0.8247) acc_u 25.0000 (24.3750) lr 1.6730e-03 eta 0:00:21
epoch [55/200] batch [10/51] time 0.416 (0.476) data 0.286 (0.345) loss_u loss_u 0.7290 (0.8020) acc_u 34.3750 (28.1250) lr 1.6730e-03 eta 0:00:19
epoch [55/200] batch [15/51] time 0.367 (0.469) data 0.236 (0.338) loss_u loss_u 0.7393 (0.7812) acc_u 34.3750 (30.4167) lr 1.6730e-03 eta 0:00:16
epoch [55/200] batch [20/51] time 0.352 (0.464) data 0.219 (0.333) loss_u loss_u 0.8564 (0.7908) acc_u 15.6250 (28.1250) lr 1.6730e-03 eta 0:00:14
epoch [55/200] batch [25/51] time 0.399 (0.459) data 0.266 (0.328) loss_u loss_u 0.6973 (0.7809) acc_u 34.3750 (28.7500) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [30/51] time 0.393 (0.457) data 0.262 (0.325) loss_u loss_u 0.7993 (0.7744) acc_u 28.1250 (29.4792) lr 1.6730e-03 eta 0:00:09
epoch [55/200] batch [35/51] time 0.488 (0.457) data 0.356 (0.325) loss_u loss_u 0.7808 (0.7769) acc_u 28.1250 (29.1071) lr 1.6730e-03 eta 0:00:07
epoch [55/200] batch [40/51] time 0.579 (0.456) data 0.448 (0.324) loss_u loss_u 0.9082 (0.7797) acc_u 15.6250 (28.5156) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [45/51] time 0.502 (0.455) data 0.371 (0.324) loss_u loss_u 0.7095 (0.7788) acc_u 43.7500 (29.0972) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [50/51] time 0.465 (0.454) data 0.333 (0.322) loss_u loss_u 0.7573 (0.7771) acc_u 31.2500 (29.3750) lr 1.6730e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.4799, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1505
clean true:1504
clean false:1
clean_rate:0.9993355481727575
noisy true:191
noisy false:1440
after delete: len(clean_dataset) 1505
after delete: len(noisy_dataset) 1631
epoch [56/200] batch [5/47] time 0.468 (0.449) data 0.336 (0.318) loss_x loss_x 0.9204 (1.0320) acc_x 71.8750 (72.5000) lr 1.6613e-03 eta 0:00:18
epoch [56/200] batch [10/47] time 0.474 (0.464) data 0.343 (0.333) loss_x loss_x 1.0391 (0.9853) acc_x 78.1250 (73.7500) lr 1.6613e-03 eta 0:00:17
epoch [56/200] batch [15/47] time 0.368 (0.455) data 0.237 (0.324) loss_x loss_x 1.1016 (1.0583) acc_x 78.1250 (73.3333) lr 1.6613e-03 eta 0:00:14
epoch [56/200] batch [20/47] time 0.457 (0.473) data 0.325 (0.342) loss_x loss_x 1.4717 (1.1439) acc_x 65.6250 (71.2500) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [25/47] time 0.404 (0.468) data 0.273 (0.337) loss_x loss_x 1.5234 (1.2059) acc_x 62.5000 (69.8750) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [30/47] time 0.490 (0.474) data 0.359 (0.342) loss_x loss_x 1.1855 (1.2349) acc_x 65.6250 (69.3750) lr 1.6613e-03 eta 0:00:08
epoch [56/200] batch [35/47] time 0.555 (0.471) data 0.424 (0.340) loss_x loss_x 0.9595 (1.2045) acc_x 81.2500 (70.0000) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [40/47] time 0.476 (0.472) data 0.345 (0.341) loss_x loss_x 1.5967 (1.2239) acc_x 56.2500 (69.0625) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [45/47] time 0.458 (0.470) data 0.327 (0.339) loss_x loss_x 0.9888 (1.2211) acc_x 65.6250 (69.0972) lr 1.6613e-03 eta 0:00:00
epoch [56/200] batch [5/50] time 0.388 (0.466) data 0.255 (0.335) loss_u loss_u 0.8125 (0.7630) acc_u 21.8750 (30.6250) lr 1.6613e-03 eta 0:00:20
epoch [56/200] batch [10/50] time 0.503 (0.466) data 0.371 (0.335) loss_u loss_u 0.8267 (0.7674) acc_u 28.1250 (29.3750) lr 1.6613e-03 eta 0:00:18
epoch [56/200] batch [15/50] time 0.407 (0.465) data 0.275 (0.333) loss_u loss_u 0.7661 (0.7917) acc_u 25.0000 (26.0417) lr 1.6613e-03 eta 0:00:16
epoch [56/200] batch [20/50] time 0.589 (0.463) data 0.456 (0.332) loss_u loss_u 0.8438 (0.7916) acc_u 18.7500 (26.5625) lr 1.6613e-03 eta 0:00:13
epoch [56/200] batch [25/50] time 0.532 (0.462) data 0.400 (0.331) loss_u loss_u 0.7969 (0.7827) acc_u 34.3750 (28.3750) lr 1.6613e-03 eta 0:00:11
epoch [56/200] batch [30/50] time 0.455 (0.463) data 0.323 (0.331) loss_u loss_u 0.6772 (0.7749) acc_u 46.8750 (29.4792) lr 1.6613e-03 eta 0:00:09
epoch [56/200] batch [35/50] time 0.387 (0.462) data 0.254 (0.330) loss_u loss_u 0.7266 (0.7735) acc_u 37.5000 (29.8214) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [40/50] time 0.434 (0.461) data 0.302 (0.330) loss_u loss_u 0.7817 (0.7737) acc_u 25.0000 (30.1562) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [45/50] time 0.340 (0.459) data 0.208 (0.327) loss_u loss_u 0.8657 (0.7775) acc_u 9.3750 (29.4444) lr 1.6613e-03 eta 0:00:02
epoch [56/200] batch [50/50] time 0.459 (0.458) data 0.328 (0.327) loss_u loss_u 0.8433 (0.7808) acc_u 18.7500 (29.0000) lr 1.6613e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1442
confident_label rate tensor(0.4866, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1526
clean true:1525
clean false:1
clean_rate:0.9993446920052425
noisy true:169
noisy false:1441
after delete: len(clean_dataset) 1526
after delete: len(noisy_dataset) 1610
epoch [57/200] batch [5/47] time 0.413 (0.453) data 0.281 (0.322) loss_x loss_x 1.1729 (1.2484) acc_x 68.7500 (68.7500) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [10/47] time 0.496 (0.449) data 0.364 (0.318) loss_x loss_x 1.1357 (1.2072) acc_x 68.7500 (68.7500) lr 1.6494e-03 eta 0:00:16
epoch [57/200] batch [15/47] time 0.499 (0.467) data 0.367 (0.336) loss_x loss_x 1.1846 (1.2144) acc_x 71.8750 (69.3750) lr 1.6494e-03 eta 0:00:14
epoch [57/200] batch [20/47] time 0.559 (0.479) data 0.428 (0.348) loss_x loss_x 1.2148 (1.2014) acc_x 65.6250 (69.2188) lr 1.6494e-03 eta 0:00:12
epoch [57/200] batch [25/47] time 0.380 (0.473) data 0.250 (0.342) loss_x loss_x 0.8994 (1.2184) acc_x 78.1250 (69.2500) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [30/47] time 0.430 (0.475) data 0.299 (0.344) loss_x loss_x 1.0859 (1.2197) acc_x 62.5000 (68.7500) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [35/47] time 0.468 (0.471) data 0.336 (0.340) loss_x loss_x 1.3350 (1.2202) acc_x 65.6250 (68.6607) lr 1.6494e-03 eta 0:00:05
epoch [57/200] batch [40/47] time 0.471 (0.473) data 0.341 (0.342) loss_x loss_x 1.1260 (1.2421) acc_x 62.5000 (68.2812) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [45/47] time 0.445 (0.475) data 0.315 (0.344) loss_x loss_x 0.9941 (1.2231) acc_x 78.1250 (68.9583) lr 1.6494e-03 eta 0:00:00
epoch [57/200] batch [5/50] time 0.379 (0.471) data 0.247 (0.340) loss_u loss_u 0.7749 (0.7676) acc_u 25.0000 (28.1250) lr 1.6494e-03 eta 0:00:21
epoch [57/200] batch [10/50] time 0.558 (0.473) data 0.427 (0.342) loss_u loss_u 0.7681 (0.7819) acc_u 21.8750 (25.9375) lr 1.6494e-03 eta 0:00:18
epoch [57/200] batch [15/50] time 0.804 (0.476) data 0.673 (0.344) loss_u loss_u 0.6504 (0.7778) acc_u 43.7500 (26.6667) lr 1.6494e-03 eta 0:00:16
epoch [57/200] batch [20/50] time 0.426 (0.470) data 0.293 (0.339) loss_u loss_u 0.8784 (0.7818) acc_u 15.6250 (26.5625) lr 1.6494e-03 eta 0:00:14
epoch [57/200] batch [25/50] time 0.521 (0.474) data 0.390 (0.342) loss_u loss_u 0.8579 (0.7841) acc_u 18.7500 (27.1250) lr 1.6494e-03 eta 0:00:11
epoch [57/200] batch [30/50] time 0.372 (0.474) data 0.241 (0.342) loss_u loss_u 0.7505 (0.7811) acc_u 34.3750 (28.0208) lr 1.6494e-03 eta 0:00:09
epoch [57/200] batch [35/50] time 0.436 (0.472) data 0.304 (0.341) loss_u loss_u 0.8403 (0.7804) acc_u 21.8750 (27.9464) lr 1.6494e-03 eta 0:00:07
epoch [57/200] batch [40/50] time 0.409 (0.475) data 0.277 (0.344) loss_u loss_u 0.7412 (0.7809) acc_u 37.5000 (27.8125) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [45/50] time 0.390 (0.471) data 0.258 (0.339) loss_u loss_u 0.7671 (0.7834) acc_u 25.0000 (27.6389) lr 1.6494e-03 eta 0:00:02
epoch [57/200] batch [50/50] time 0.412 (0.467) data 0.281 (0.336) loss_u loss_u 0.8364 (0.7804) acc_u 21.8750 (28.1250) lr 1.6494e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1465
confident_label rate tensor(0.4790, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1502
clean true:1502
clean false:0
clean_rate:1.0
noisy true:169
noisy false:1465
after delete: len(clean_dataset) 1502
after delete: len(noisy_dataset) 1634
epoch [58/200] batch [5/46] time 0.467 (0.463) data 0.337 (0.332) loss_x loss_x 1.4795 (1.4244) acc_x 68.7500 (65.0000) lr 1.6374e-03 eta 0:00:18
epoch [58/200] batch [10/46] time 0.495 (0.460) data 0.364 (0.329) loss_x loss_x 0.6665 (1.3621) acc_x 87.5000 (66.2500) lr 1.6374e-03 eta 0:00:16
epoch [58/200] batch [15/46] time 0.412 (0.453) data 0.279 (0.322) loss_x loss_x 1.0137 (1.4292) acc_x 71.8750 (64.3750) lr 1.6374e-03 eta 0:00:14
epoch [58/200] batch [20/46] time 0.385 (0.447) data 0.254 (0.316) loss_x loss_x 1.1172 (1.3276) acc_x 68.7500 (66.7188) lr 1.6374e-03 eta 0:00:11
epoch [58/200] batch [25/46] time 0.700 (0.459) data 0.568 (0.328) loss_x loss_x 1.5107 (1.3383) acc_x 65.6250 (66.2500) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [30/46] time 0.449 (0.459) data 0.317 (0.328) loss_x loss_x 1.6191 (1.3623) acc_x 59.3750 (66.1458) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [35/46] time 0.394 (0.471) data 0.261 (0.340) loss_x loss_x 1.0811 (1.3409) acc_x 75.0000 (66.6071) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [40/46] time 0.489 (0.472) data 0.357 (0.341) loss_x loss_x 1.2549 (1.3391) acc_x 65.6250 (66.4844) lr 1.6374e-03 eta 0:00:02
epoch [58/200] batch [45/46] time 0.367 (0.464) data 0.237 (0.333) loss_x loss_x 1.0869 (1.3302) acc_x 68.7500 (66.6667) lr 1.6374e-03 eta 0:00:00
epoch [58/200] batch [5/51] time 0.503 (0.466) data 0.371 (0.335) loss_u loss_u 0.7798 (0.7713) acc_u 31.2500 (29.3750) lr 1.6374e-03 eta 0:00:21
epoch [58/200] batch [10/51] time 0.477 (0.469) data 0.345 (0.338) loss_u loss_u 0.8389 (0.7812) acc_u 21.8750 (28.1250) lr 1.6374e-03 eta 0:00:19
epoch [58/200] batch [15/51] time 0.426 (0.464) data 0.294 (0.332) loss_u loss_u 0.7788 (0.7799) acc_u 28.1250 (28.9583) lr 1.6374e-03 eta 0:00:16
epoch [58/200] batch [20/51] time 0.553 (0.469) data 0.422 (0.337) loss_u loss_u 0.7319 (0.7706) acc_u 31.2500 (30.0000) lr 1.6374e-03 eta 0:00:14
epoch [58/200] batch [25/51] time 0.488 (0.467) data 0.356 (0.335) loss_u loss_u 0.6758 (0.7647) acc_u 37.5000 (30.7500) lr 1.6374e-03 eta 0:00:12
epoch [58/200] batch [30/51] time 0.430 (0.465) data 0.299 (0.334) loss_u loss_u 0.8496 (0.7664) acc_u 21.8750 (30.7292) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [35/51] time 0.376 (0.463) data 0.244 (0.332) loss_u loss_u 0.8145 (0.7668) acc_u 31.2500 (30.7143) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [40/51] time 0.648 (0.466) data 0.517 (0.335) loss_u loss_u 0.7500 (0.7683) acc_u 34.3750 (30.5469) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [45/51] time 0.486 (0.465) data 0.355 (0.333) loss_u loss_u 0.8198 (0.7731) acc_u 18.7500 (29.9306) lr 1.6374e-03 eta 0:00:02
epoch [58/200] batch [50/51] time 0.474 (0.464) data 0.343 (0.333) loss_u loss_u 0.8276 (0.7750) acc_u 18.7500 (29.5000) lr 1.6374e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1408
confident_label rate tensor(0.4974, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1560
clean true:1559
clean false:1
clean_rate:0.9993589743589744
noisy true:169
noisy false:1407
after delete: len(clean_dataset) 1560
after delete: len(noisy_dataset) 1576
epoch [59/200] batch [5/48] time 0.387 (0.437) data 0.256 (0.306) loss_x loss_x 1.6250 (1.2048) acc_x 65.6250 (73.7500) lr 1.6252e-03 eta 0:00:18
epoch [59/200] batch [10/48] time 0.431 (0.485) data 0.300 (0.354) loss_x loss_x 1.3320 (1.1898) acc_x 59.3750 (71.8750) lr 1.6252e-03 eta 0:00:18
epoch [59/200] batch [15/48] time 0.528 (0.469) data 0.397 (0.338) loss_x loss_x 0.8921 (1.1924) acc_x 68.7500 (70.8333) lr 1.6252e-03 eta 0:00:15
epoch [59/200] batch [20/48] time 0.526 (0.487) data 0.396 (0.356) loss_x loss_x 1.0605 (1.2004) acc_x 75.0000 (70.0000) lr 1.6252e-03 eta 0:00:13
epoch [59/200] batch [25/48] time 0.429 (0.489) data 0.297 (0.358) loss_x loss_x 1.4004 (1.2217) acc_x 65.6250 (69.5000) lr 1.6252e-03 eta 0:00:11
epoch [59/200] batch [30/48] time 0.409 (0.492) data 0.278 (0.361) loss_x loss_x 1.5137 (1.2211) acc_x 68.7500 (69.7917) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [35/48] time 0.514 (0.486) data 0.382 (0.355) loss_x loss_x 1.5771 (1.2630) acc_x 53.1250 (69.1071) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [40/48] time 0.438 (0.482) data 0.308 (0.351) loss_x loss_x 2.5664 (1.2908) acc_x 34.3750 (68.0469) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [45/48] time 0.384 (0.486) data 0.253 (0.355) loss_x loss_x 1.0039 (1.2868) acc_x 75.0000 (67.9861) lr 1.6252e-03 eta 0:00:01
epoch [59/200] batch [5/49] time 0.412 (0.484) data 0.281 (0.352) loss_u loss_u 0.8589 (0.7604) acc_u 15.6250 (31.2500) lr 1.6252e-03 eta 0:00:21
epoch [59/200] batch [10/49] time 0.477 (0.486) data 0.345 (0.355) loss_u loss_u 0.8130 (0.7774) acc_u 28.1250 (28.4375) lr 1.6252e-03 eta 0:00:18
epoch [59/200] batch [15/49] time 0.466 (0.480) data 0.334 (0.349) loss_u loss_u 0.8667 (0.7883) acc_u 21.8750 (27.0833) lr 1.6252e-03 eta 0:00:16
epoch [59/200] batch [20/49] time 0.462 (0.475) data 0.327 (0.343) loss_u loss_u 0.7114 (0.7881) acc_u 31.2500 (27.5000) lr 1.6252e-03 eta 0:00:13
epoch [59/200] batch [25/49] time 0.351 (0.470) data 0.218 (0.339) loss_u loss_u 0.8501 (0.7895) acc_u 15.6250 (27.7500) lr 1.6252e-03 eta 0:00:11
epoch [59/200] batch [30/49] time 0.429 (0.466) data 0.297 (0.335) loss_u loss_u 0.8179 (0.7940) acc_u 28.1250 (27.9167) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [35/49] time 0.376 (0.465) data 0.245 (0.334) loss_u loss_u 0.8066 (0.7935) acc_u 28.1250 (27.9464) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [40/49] time 0.418 (0.467) data 0.286 (0.336) loss_u loss_u 0.7866 (0.7929) acc_u 28.1250 (28.2812) lr 1.6252e-03 eta 0:00:04
epoch [59/200] batch [45/49] time 0.443 (0.464) data 0.311 (0.333) loss_u loss_u 0.7861 (0.7938) acc_u 28.1250 (28.1250) lr 1.6252e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1443
confident_label rate tensor(0.4850, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1521
clean true:1519
clean false:2
clean_rate:0.9986850756081526
noisy true:174
noisy false:1441
after delete: len(clean_dataset) 1521
after delete: len(noisy_dataset) 1615
epoch [60/200] batch [5/47] time 0.506 (0.518) data 0.375 (0.385) loss_x loss_x 0.9717 (1.1196) acc_x 78.1250 (73.7500) lr 1.6129e-03 eta 0:00:21
epoch [60/200] batch [10/47] time 0.549 (0.506) data 0.417 (0.374) loss_x loss_x 1.1133 (1.1769) acc_x 75.0000 (72.1875) lr 1.6129e-03 eta 0:00:18
epoch [60/200] batch [15/47] time 0.383 (0.484) data 0.251 (0.352) loss_x loss_x 0.9121 (1.1875) acc_x 75.0000 (71.8750) lr 1.6129e-03 eta 0:00:15
epoch [60/200] batch [20/47] time 0.401 (0.471) data 0.271 (0.339) loss_x loss_x 1.0791 (1.2083) acc_x 75.0000 (71.0938) lr 1.6129e-03 eta 0:00:12
epoch [60/200] batch [25/47] time 0.479 (0.461) data 0.349 (0.330) loss_x loss_x 1.1777 (1.2181) acc_x 56.2500 (70.5000) lr 1.6129e-03 eta 0:00:10
epoch [60/200] batch [30/47] time 0.416 (0.457) data 0.285 (0.325) loss_x loss_x 0.7695 (1.1997) acc_x 87.5000 (70.8333) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [35/47] time 0.488 (0.456) data 0.357 (0.324) loss_x loss_x 1.1553 (1.2190) acc_x 68.7500 (70.2679) lr 1.6129e-03 eta 0:00:05
epoch [60/200] batch [40/47] time 0.603 (0.462) data 0.471 (0.331) loss_x loss_x 0.9761 (1.1846) acc_x 65.6250 (71.0156) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [45/47] time 0.469 (0.462) data 0.337 (0.330) loss_x loss_x 1.0596 (1.1953) acc_x 68.7500 (70.2778) lr 1.6129e-03 eta 0:00:00
epoch [60/200] batch [5/50] time 0.441 (0.463) data 0.309 (0.332) loss_u loss_u 0.8330 (0.8008) acc_u 18.7500 (25.6250) lr 1.6129e-03 eta 0:00:20
epoch [60/200] batch [10/50] time 0.497 (0.461) data 0.366 (0.330) loss_u loss_u 0.8589 (0.7835) acc_u 15.6250 (27.8125) lr 1.6129e-03 eta 0:00:18
epoch [60/200] batch [15/50] time 0.485 (0.465) data 0.355 (0.334) loss_u loss_u 0.8369 (0.7858) acc_u 21.8750 (26.8750) lr 1.6129e-03 eta 0:00:16
epoch [60/200] batch [20/50] time 0.454 (0.461) data 0.323 (0.330) loss_u loss_u 0.6807 (0.7878) acc_u 43.7500 (27.0312) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [25/50] time 0.500 (0.460) data 0.368 (0.329) loss_u loss_u 0.7764 (0.7865) acc_u 34.3750 (27.5000) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [30/50] time 0.371 (0.460) data 0.240 (0.329) loss_u loss_u 0.8804 (0.7926) acc_u 15.6250 (26.4583) lr 1.6129e-03 eta 0:00:09
epoch [60/200] batch [35/50] time 0.476 (0.457) data 0.345 (0.326) loss_u loss_u 0.8613 (0.7919) acc_u 18.7500 (26.8750) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [40/50] time 0.476 (0.456) data 0.344 (0.325) loss_u loss_u 0.7388 (0.7873) acc_u 34.3750 (27.8125) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [45/50] time 0.585 (0.458) data 0.454 (0.327) loss_u loss_u 0.8218 (0.7910) acc_u 18.7500 (27.1528) lr 1.6129e-03 eta 0:00:02
epoch [60/200] batch [50/50] time 0.357 (0.457) data 0.226 (0.326) loss_u loss_u 0.7778 (0.7922) acc_u 31.2500 (27.0000) lr 1.6129e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1467
confident_label rate tensor(0.4805, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1507
clean true:1507
clean false:0
clean_rate:1.0
noisy true:162
noisy false:1467
after delete: len(clean_dataset) 1507
after delete: len(noisy_dataset) 1629
epoch [61/200] batch [5/47] time 0.533 (0.514) data 0.402 (0.383) loss_x loss_x 0.6411 (1.0422) acc_x 81.2500 (71.8750) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [10/47] time 0.526 (0.502) data 0.394 (0.370) loss_x loss_x 0.9741 (1.0966) acc_x 59.3750 (69.6875) lr 1.6004e-03 eta 0:00:18
epoch [61/200] batch [15/47] time 0.460 (0.491) data 0.328 (0.360) loss_x loss_x 1.1846 (1.1091) acc_x 71.8750 (70.4167) lr 1.6004e-03 eta 0:00:15
epoch [61/200] batch [20/47] time 0.414 (0.481) data 0.283 (0.350) loss_x loss_x 0.9243 (1.1601) acc_x 78.1250 (69.5312) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [25/47] time 0.507 (0.471) data 0.376 (0.339) loss_x loss_x 1.1211 (1.1562) acc_x 75.0000 (70.2500) lr 1.6004e-03 eta 0:00:10
epoch [61/200] batch [30/47] time 0.603 (0.479) data 0.472 (0.348) loss_x loss_x 1.1494 (1.1494) acc_x 68.7500 (70.6250) lr 1.6004e-03 eta 0:00:08
epoch [61/200] batch [35/47] time 0.467 (0.475) data 0.335 (0.344) loss_x loss_x 1.2920 (1.1466) acc_x 59.3750 (70.1786) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [40/47] time 0.451 (0.469) data 0.320 (0.338) loss_x loss_x 1.1562 (1.1533) acc_x 62.5000 (69.9219) lr 1.6004e-03 eta 0:00:03
epoch [61/200] batch [45/47] time 0.525 (0.477) data 0.393 (0.345) loss_x loss_x 1.0938 (1.1649) acc_x 71.8750 (69.7222) lr 1.6004e-03 eta 0:00:00
epoch [61/200] batch [5/50] time 0.355 (0.471) data 0.224 (0.340) loss_u loss_u 0.8301 (0.7693) acc_u 31.2500 (31.2500) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [10/50] time 0.467 (0.467) data 0.336 (0.336) loss_u loss_u 0.7119 (0.7695) acc_u 31.2500 (31.5625) lr 1.6004e-03 eta 0:00:18
epoch [61/200] batch [15/50] time 0.387 (0.467) data 0.256 (0.335) loss_u loss_u 0.7866 (0.7831) acc_u 28.1250 (29.3750) lr 1.6004e-03 eta 0:00:16
epoch [61/200] batch [20/50] time 0.459 (0.465) data 0.327 (0.333) loss_u loss_u 0.7202 (0.7808) acc_u 40.6250 (28.9062) lr 1.6004e-03 eta 0:00:13
epoch [61/200] batch [25/50] time 0.583 (0.468) data 0.452 (0.336) loss_u loss_u 0.7246 (0.7775) acc_u 34.3750 (29.2500) lr 1.6004e-03 eta 0:00:11
epoch [61/200] batch [30/50] time 0.406 (0.466) data 0.275 (0.335) loss_u loss_u 0.7534 (0.7719) acc_u 28.1250 (29.5833) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [35/50] time 0.631 (0.466) data 0.500 (0.335) loss_u loss_u 0.7383 (0.7735) acc_u 34.3750 (28.8393) lr 1.6004e-03 eta 0:00:06
epoch [61/200] batch [40/50] time 0.439 (0.466) data 0.308 (0.335) loss_u loss_u 0.7832 (0.7760) acc_u 28.1250 (28.8281) lr 1.6004e-03 eta 0:00:04
epoch [61/200] batch [45/50] time 0.403 (0.468) data 0.271 (0.337) loss_u loss_u 0.8848 (0.7759) acc_u 18.7500 (29.1667) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [50/50] time 0.386 (0.468) data 0.255 (0.336) loss_u loss_u 0.8062 (0.7763) acc_u 21.8750 (29.1250) lr 1.6004e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1435
confident_label rate tensor(0.4837, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1517
clean true:1517
clean false:0
clean_rate:1.0
noisy true:184
noisy false:1435
after delete: len(clean_dataset) 1517
after delete: len(noisy_dataset) 1619
epoch [62/200] batch [5/47] time 0.427 (0.406) data 0.296 (0.275) loss_x loss_x 1.0635 (1.0205) acc_x 78.1250 (76.2500) lr 1.5878e-03 eta 0:00:17
epoch [62/200] batch [10/47] time 0.465 (0.425) data 0.333 (0.294) loss_x loss_x 1.0127 (1.0687) acc_x 75.0000 (73.7500) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [15/47] time 0.410 (0.433) data 0.279 (0.302) loss_x loss_x 1.2021 (1.1367) acc_x 71.8750 (70.6250) lr 1.5878e-03 eta 0:00:13
epoch [62/200] batch [20/47] time 0.520 (0.445) data 0.390 (0.314) loss_x loss_x 0.9878 (1.1224) acc_x 78.1250 (71.5625) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [25/47] time 0.389 (0.450) data 0.258 (0.319) loss_x loss_x 1.4326 (1.1768) acc_x 56.2500 (70.3750) lr 1.5878e-03 eta 0:00:09
epoch [62/200] batch [30/47] time 0.669 (0.470) data 0.538 (0.339) loss_x loss_x 1.0518 (1.1629) acc_x 81.2500 (71.1458) lr 1.5878e-03 eta 0:00:07
epoch [62/200] batch [35/47] time 0.440 (0.473) data 0.307 (0.343) loss_x loss_x 0.9287 (1.1304) acc_x 78.1250 (71.6071) lr 1.5878e-03 eta 0:00:05
epoch [62/200] batch [40/47] time 0.465 (0.471) data 0.334 (0.340) loss_x loss_x 0.9478 (1.1494) acc_x 75.0000 (71.2500) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [45/47] time 0.454 (0.470) data 0.322 (0.339) loss_x loss_x 1.6172 (1.1783) acc_x 56.2500 (70.4861) lr 1.5878e-03 eta 0:00:00
epoch [62/200] batch [5/50] time 0.481 (0.477) data 0.349 (0.346) loss_u loss_u 0.7485 (0.7573) acc_u 37.5000 (31.8750) lr 1.5878e-03 eta 0:00:21
epoch [62/200] batch [10/50] time 0.411 (0.478) data 0.280 (0.347) loss_u loss_u 0.7544 (0.7589) acc_u 31.2500 (30.3125) lr 1.5878e-03 eta 0:00:19
epoch [62/200] batch [15/50] time 0.421 (0.477) data 0.289 (0.346) loss_u loss_u 0.8105 (0.7728) acc_u 25.0000 (28.7500) lr 1.5878e-03 eta 0:00:16
epoch [62/200] batch [20/50] time 0.831 (0.483) data 0.699 (0.352) loss_u loss_u 0.7432 (0.7769) acc_u 28.1250 (29.0625) lr 1.5878e-03 eta 0:00:14
epoch [62/200] batch [25/50] time 0.406 (0.477) data 0.275 (0.346) loss_u loss_u 0.7314 (0.7800) acc_u 37.5000 (28.8750) lr 1.5878e-03 eta 0:00:11
epoch [62/200] batch [30/50] time 0.376 (0.476) data 0.245 (0.344) loss_u loss_u 0.7251 (0.7809) acc_u 34.3750 (28.1250) lr 1.5878e-03 eta 0:00:09
epoch [62/200] batch [35/50] time 0.487 (0.474) data 0.356 (0.343) loss_u loss_u 0.7378 (0.7799) acc_u 34.3750 (28.6607) lr 1.5878e-03 eta 0:00:07
epoch [62/200] batch [40/50] time 0.373 (0.472) data 0.241 (0.341) loss_u loss_u 0.8159 (0.7821) acc_u 25.0000 (29.0625) lr 1.5878e-03 eta 0:00:04
epoch [62/200] batch [45/50] time 0.382 (0.471) data 0.251 (0.340) loss_u loss_u 0.7617 (0.7802) acc_u 37.5000 (29.5139) lr 1.5878e-03 eta 0:00:02
epoch [62/200] batch [50/50] time 0.598 (0.472) data 0.467 (0.341) loss_u loss_u 0.7759 (0.7801) acc_u 21.8750 (29.3750) lr 1.5878e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.4844, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1519
clean true:1517
clean false:2
clean_rate:0.9986833443054641
noisy true:178
noisy false:1439
after delete: len(clean_dataset) 1519
after delete: len(noisy_dataset) 1617
epoch [63/200] batch [5/47] time 0.417 (0.523) data 0.287 (0.392) loss_x loss_x 1.4570 (1.2086) acc_x 59.3750 (70.6250) lr 1.5750e-03 eta 0:00:21
epoch [63/200] batch [10/47] time 0.519 (0.495) data 0.388 (0.364) loss_x loss_x 1.3066 (1.1788) acc_x 56.2500 (70.3125) lr 1.5750e-03 eta 0:00:18
epoch [63/200] batch [15/47] time 0.625 (0.493) data 0.494 (0.362) loss_x loss_x 1.7295 (1.2041) acc_x 50.0000 (69.5833) lr 1.5750e-03 eta 0:00:15
epoch [63/200] batch [20/47] time 0.446 (0.480) data 0.315 (0.349) loss_x loss_x 1.1621 (1.2072) acc_x 71.8750 (70.0000) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [25/47] time 0.412 (0.469) data 0.281 (0.338) loss_x loss_x 1.5361 (1.2457) acc_x 62.5000 (69.5000) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [30/47] time 0.357 (0.464) data 0.227 (0.333) loss_x loss_x 1.4639 (1.2838) acc_x 65.6250 (67.9167) lr 1.5750e-03 eta 0:00:07
epoch [63/200] batch [35/47] time 0.597 (0.469) data 0.465 (0.338) loss_x loss_x 1.3477 (1.3136) acc_x 62.5000 (66.9643) lr 1.5750e-03 eta 0:00:05
epoch [63/200] batch [40/47] time 0.384 (0.474) data 0.253 (0.343) loss_x loss_x 1.1738 (1.3089) acc_x 71.8750 (67.0312) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [45/47] time 0.413 (0.480) data 0.281 (0.348) loss_x loss_x 1.1953 (1.3115) acc_x 68.7500 (67.0833) lr 1.5750e-03 eta 0:00:00
epoch [63/200] batch [5/50] time 0.504 (0.473) data 0.373 (0.342) loss_u loss_u 0.8428 (0.8299) acc_u 12.5000 (18.7500) lr 1.5750e-03 eta 0:00:21
epoch [63/200] batch [10/50] time 0.520 (0.474) data 0.389 (0.343) loss_u loss_u 0.7803 (0.7878) acc_u 28.1250 (27.1875) lr 1.5750e-03 eta 0:00:18
epoch [63/200] batch [15/50] time 0.434 (0.472) data 0.301 (0.341) loss_u loss_u 0.7563 (0.7973) acc_u 28.1250 (25.0000) lr 1.5750e-03 eta 0:00:16
epoch [63/200] batch [20/50] time 0.394 (0.474) data 0.262 (0.343) loss_u loss_u 0.7705 (0.7891) acc_u 34.3750 (26.8750) lr 1.5750e-03 eta 0:00:14
epoch [63/200] batch [25/50] time 0.506 (0.473) data 0.375 (0.342) loss_u loss_u 0.8071 (0.7843) acc_u 25.0000 (28.2500) lr 1.5750e-03 eta 0:00:11
epoch [63/200] batch [30/50] time 0.362 (0.469) data 0.230 (0.338) loss_u loss_u 0.7305 (0.7814) acc_u 34.3750 (28.7500) lr 1.5750e-03 eta 0:00:09
epoch [63/200] batch [35/50] time 0.440 (0.467) data 0.308 (0.335) loss_u loss_u 0.8486 (0.7828) acc_u 25.0000 (28.3929) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [40/50] time 0.851 (0.469) data 0.719 (0.338) loss_u loss_u 0.7974 (0.7816) acc_u 25.0000 (28.5156) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [45/50] time 0.425 (0.470) data 0.295 (0.339) loss_u loss_u 0.7676 (0.7821) acc_u 21.8750 (28.1250) lr 1.5750e-03 eta 0:00:02
epoch [63/200] batch [50/50] time 0.527 (0.470) data 0.396 (0.338) loss_u loss_u 0.7725 (0.7823) acc_u 31.2500 (27.9375) lr 1.5750e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.4901, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1537
clean true:1536
clean false:1
clean_rate:0.9993493819128172
noisy true:180
noisy false:1419
after delete: len(clean_dataset) 1537
after delete: len(noisy_dataset) 1599
epoch [64/200] batch [5/48] time 0.422 (0.461) data 0.291 (0.330) loss_x loss_x 1.6123 (1.2618) acc_x 65.6250 (68.7500) lr 1.5621e-03 eta 0:00:19
epoch [64/200] batch [10/48] time 0.438 (0.451) data 0.307 (0.319) loss_x loss_x 1.4346 (1.3034) acc_x 53.1250 (64.6875) lr 1.5621e-03 eta 0:00:17
epoch [64/200] batch [15/48] time 0.356 (0.434) data 0.224 (0.303) loss_x loss_x 1.1006 (1.2669) acc_x 68.7500 (66.8750) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [20/48] time 0.415 (0.439) data 0.284 (0.308) loss_x loss_x 1.1055 (1.2437) acc_x 75.0000 (68.1250) lr 1.5621e-03 eta 0:00:12
epoch [64/200] batch [25/48] time 0.417 (0.437) data 0.286 (0.305) loss_x loss_x 1.6084 (1.2419) acc_x 56.2500 (68.5000) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [30/48] time 0.525 (0.445) data 0.394 (0.313) loss_x loss_x 1.0879 (1.2300) acc_x 71.8750 (68.7500) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [35/48] time 0.397 (0.456) data 0.266 (0.324) loss_x loss_x 1.3867 (1.2109) acc_x 62.5000 (69.4643) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [40/48] time 0.429 (0.459) data 0.297 (0.327) loss_x loss_x 2.1543 (1.2323) acc_x 59.3750 (69.2188) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [45/48] time 0.408 (0.456) data 0.277 (0.324) loss_x loss_x 0.9351 (1.2291) acc_x 65.6250 (69.1667) lr 1.5621e-03 eta 0:00:01
epoch [64/200] batch [5/49] time 0.432 (0.459) data 0.301 (0.328) loss_u loss_u 0.8008 (0.7644) acc_u 37.5000 (31.2500) lr 1.5621e-03 eta 0:00:20
epoch [64/200] batch [10/49] time 0.512 (0.456) data 0.381 (0.325) loss_u loss_u 0.7529 (0.7438) acc_u 34.3750 (33.7500) lr 1.5621e-03 eta 0:00:17
epoch [64/200] batch [15/49] time 0.520 (0.454) data 0.388 (0.322) loss_u loss_u 0.7910 (0.7527) acc_u 25.0000 (32.2917) lr 1.5621e-03 eta 0:00:15
epoch [64/200] batch [20/49] time 0.364 (0.451) data 0.232 (0.319) loss_u loss_u 0.8359 (0.7645) acc_u 15.6250 (30.3125) lr 1.5621e-03 eta 0:00:13
epoch [64/200] batch [25/49] time 0.406 (0.448) data 0.274 (0.317) loss_u loss_u 0.8345 (0.7661) acc_u 21.8750 (30.0000) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [30/49] time 0.403 (0.448) data 0.271 (0.317) loss_u loss_u 0.8188 (0.7729) acc_u 18.7500 (29.0625) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [35/49] time 0.415 (0.449) data 0.284 (0.317) loss_u loss_u 0.7930 (0.7760) acc_u 25.0000 (28.4821) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [40/49] time 0.375 (0.448) data 0.244 (0.317) loss_u loss_u 0.8047 (0.7772) acc_u 25.0000 (28.5156) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [45/49] time 0.340 (0.449) data 0.207 (0.317) loss_u loss_u 0.8027 (0.7802) acc_u 25.0000 (28.1944) lr 1.5621e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1453
confident_label rate tensor(0.4828, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1514
clean true:1513
clean false:1
clean_rate:0.9993394980184941
noisy true:170
noisy false:1452
after delete: len(clean_dataset) 1514
after delete: len(noisy_dataset) 1622
epoch [65/200] batch [5/47] time 0.416 (0.469) data 0.284 (0.338) loss_x loss_x 1.3955 (1.1137) acc_x 62.5000 (73.7500) lr 1.5490e-03 eta 0:00:19
epoch [65/200] batch [10/47] time 0.655 (0.505) data 0.522 (0.374) loss_x loss_x 1.0117 (1.0888) acc_x 75.0000 (71.5625) lr 1.5490e-03 eta 0:00:18
epoch [65/200] batch [15/47] time 0.599 (0.505) data 0.467 (0.373) loss_x loss_x 1.2422 (1.1943) acc_x 75.0000 (70.4167) lr 1.5490e-03 eta 0:00:16
epoch [65/200] batch [20/47] time 0.398 (0.494) data 0.266 (0.362) loss_x loss_x 1.2861 (1.2216) acc_x 65.6250 (69.0625) lr 1.5490e-03 eta 0:00:13
epoch [65/200] batch [25/47] time 0.387 (0.486) data 0.256 (0.354) loss_x loss_x 1.0430 (1.2172) acc_x 75.0000 (69.5000) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [30/47] time 0.408 (0.475) data 0.278 (0.343) loss_x loss_x 1.5830 (1.1949) acc_x 56.2500 (69.4792) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [35/47] time 0.645 (0.481) data 0.514 (0.349) loss_x loss_x 1.4561 (1.2369) acc_x 68.7500 (68.9286) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [40/47] time 0.604 (0.481) data 0.473 (0.349) loss_x loss_x 1.0713 (1.2207) acc_x 75.0000 (69.6094) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [45/47] time 0.497 (0.482) data 0.365 (0.350) loss_x loss_x 1.0381 (1.1926) acc_x 68.7500 (70.0694) lr 1.5490e-03 eta 0:00:00
epoch [65/200] batch [5/50] time 0.450 (0.471) data 0.318 (0.340) loss_u loss_u 0.7881 (0.7710) acc_u 31.2500 (30.0000) lr 1.5490e-03 eta 0:00:21
epoch [65/200] batch [10/50] time 0.445 (0.472) data 0.313 (0.340) loss_u loss_u 0.8433 (0.7985) acc_u 21.8750 (27.1875) lr 1.5490e-03 eta 0:00:18
epoch [65/200] batch [15/50] time 0.474 (0.470) data 0.343 (0.338) loss_u loss_u 0.6519 (0.7769) acc_u 43.7500 (29.7917) lr 1.5490e-03 eta 0:00:16
epoch [65/200] batch [20/50] time 0.474 (0.469) data 0.341 (0.338) loss_u loss_u 0.7944 (0.7714) acc_u 21.8750 (29.6875) lr 1.5490e-03 eta 0:00:14
epoch [65/200] batch [25/50] time 0.418 (0.467) data 0.286 (0.335) loss_u loss_u 0.7871 (0.7745) acc_u 31.2500 (29.6250) lr 1.5490e-03 eta 0:00:11
epoch [65/200] batch [30/50] time 0.314 (0.462) data 0.182 (0.330) loss_u loss_u 0.8345 (0.7823) acc_u 25.0000 (28.7500) lr 1.5490e-03 eta 0:00:09
epoch [65/200] batch [35/50] time 0.473 (0.464) data 0.340 (0.332) loss_u loss_u 0.7588 (0.7787) acc_u 28.1250 (28.6607) lr 1.5490e-03 eta 0:00:06
epoch [65/200] batch [40/50] time 0.621 (0.466) data 0.489 (0.334) loss_u loss_u 0.7412 (0.7767) acc_u 28.1250 (28.7500) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [45/50] time 0.408 (0.464) data 0.276 (0.332) loss_u loss_u 0.7148 (0.7748) acc_u 37.5000 (29.0972) lr 1.5490e-03 eta 0:00:02
epoch [65/200] batch [50/50] time 0.415 (0.461) data 0.284 (0.330) loss_u loss_u 0.8242 (0.7761) acc_u 25.0000 (29.1875) lr 1.5490e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1385
confident_label rate tensor(0.5051, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1584
clean true:1583
clean false:1
clean_rate:0.9993686868686869
noisy true:168
noisy false:1384
after delete: len(clean_dataset) 1584
after delete: len(noisy_dataset) 1552
epoch [66/200] batch [5/49] time 0.484 (0.521) data 0.352 (0.389) loss_x loss_x 1.1318 (1.3605) acc_x 62.5000 (65.0000) lr 1.5358e-03 eta 0:00:22
epoch [66/200] batch [10/49] time 0.490 (0.506) data 0.360 (0.374) loss_x loss_x 1.6855 (1.3879) acc_x 59.3750 (64.3750) lr 1.5358e-03 eta 0:00:19
epoch [66/200] batch [15/49] time 0.487 (0.500) data 0.356 (0.368) loss_x loss_x 1.3203 (1.3564) acc_x 65.6250 (65.6250) lr 1.5358e-03 eta 0:00:17
epoch [66/200] batch [20/49] time 0.529 (0.496) data 0.397 (0.364) loss_x loss_x 0.6382 (1.2953) acc_x 84.3750 (67.0312) lr 1.5358e-03 eta 0:00:14
epoch [66/200] batch [25/49] time 0.407 (0.498) data 0.276 (0.367) loss_x loss_x 1.3613 (1.2891) acc_x 59.3750 (67.0000) lr 1.5358e-03 eta 0:00:11
epoch [66/200] batch [30/49] time 0.404 (0.485) data 0.274 (0.354) loss_x loss_x 1.4004 (1.2753) acc_x 68.7500 (67.2917) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [35/49] time 0.587 (0.492) data 0.456 (0.361) loss_x loss_x 1.0596 (1.2576) acc_x 71.8750 (67.6786) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [40/49] time 0.400 (0.491) data 0.269 (0.360) loss_x loss_x 1.2891 (1.2732) acc_x 65.6250 (67.6562) lr 1.5358e-03 eta 0:00:04
epoch [66/200] batch [45/49] time 0.521 (0.491) data 0.390 (0.359) loss_x loss_x 1.1475 (1.2770) acc_x 62.5000 (67.0139) lr 1.5358e-03 eta 0:00:01
epoch [66/200] batch [5/48] time 0.894 (0.489) data 0.763 (0.358) loss_u loss_u 0.7881 (0.8013) acc_u 31.2500 (25.6250) lr 1.5358e-03 eta 0:00:21
epoch [66/200] batch [10/48] time 0.366 (0.487) data 0.234 (0.356) loss_u loss_u 0.7456 (0.7729) acc_u 34.3750 (31.2500) lr 1.5358e-03 eta 0:00:18
epoch [66/200] batch [15/48] time 0.399 (0.485) data 0.267 (0.353) loss_u loss_u 0.6953 (0.7648) acc_u 43.7500 (32.2917) lr 1.5358e-03 eta 0:00:15
epoch [66/200] batch [20/48] time 0.422 (0.479) data 0.291 (0.347) loss_u loss_u 0.8086 (0.7743) acc_u 18.7500 (30.3125) lr 1.5358e-03 eta 0:00:13
epoch [66/200] batch [25/48] time 0.471 (0.479) data 0.340 (0.347) loss_u loss_u 0.7603 (0.7812) acc_u 34.3750 (28.7500) lr 1.5358e-03 eta 0:00:11
epoch [66/200] batch [30/48] time 0.448 (0.478) data 0.315 (0.347) loss_u loss_u 0.6680 (0.7758) acc_u 37.5000 (28.7500) lr 1.5358e-03 eta 0:00:08
epoch [66/200] batch [35/48] time 0.541 (0.477) data 0.410 (0.346) loss_u loss_u 0.7515 (0.7803) acc_u 34.3750 (28.3929) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [40/48] time 0.593 (0.476) data 0.461 (0.345) loss_u loss_u 0.7881 (0.7790) acc_u 28.1250 (28.5938) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [45/48] time 0.452 (0.474) data 0.320 (0.343) loss_u loss_u 0.7871 (0.7826) acc_u 21.8750 (28.1250) lr 1.5358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1414
confident_label rate tensor(0.4923, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1544
clean true:1543
clean false:1
clean_rate:0.9993523316062176
noisy true:179
noisy false:1413
after delete: len(clean_dataset) 1544
after delete: len(noisy_dataset) 1592
epoch [67/200] batch [5/48] time 0.687 (0.627) data 0.556 (0.494) loss_x loss_x 1.3867 (1.1837) acc_x 68.7500 (71.8750) lr 1.5225e-03 eta 0:00:26
epoch [67/200] batch [10/48] time 0.437 (0.548) data 0.306 (0.416) loss_x loss_x 0.9185 (1.1657) acc_x 81.2500 (73.7500) lr 1.5225e-03 eta 0:00:20
epoch [67/200] batch [15/48] time 0.512 (0.548) data 0.380 (0.416) loss_x loss_x 0.9443 (1.1900) acc_x 75.0000 (71.8750) lr 1.5225e-03 eta 0:00:18
epoch [67/200] batch [20/48] time 0.411 (0.536) data 0.281 (0.404) loss_x loss_x 1.4688 (1.2484) acc_x 65.6250 (70.7812) lr 1.5225e-03 eta 0:00:15
epoch [67/200] batch [25/48] time 0.491 (0.518) data 0.359 (0.386) loss_x loss_x 0.9443 (1.2281) acc_x 65.6250 (70.2500) lr 1.5225e-03 eta 0:00:11
epoch [67/200] batch [30/48] time 0.479 (0.509) data 0.348 (0.377) loss_x loss_x 1.2998 (1.2186) acc_x 65.6250 (70.0000) lr 1.5225e-03 eta 0:00:09
epoch [67/200] batch [35/48] time 0.463 (0.512) data 0.332 (0.381) loss_x loss_x 1.0986 (1.2015) acc_x 78.1250 (70.6250) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [40/48] time 0.432 (0.499) data 0.299 (0.368) loss_x loss_x 1.1982 (1.1922) acc_x 75.0000 (70.6250) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [45/48] time 0.579 (0.495) data 0.448 (0.363) loss_x loss_x 0.8945 (1.1933) acc_x 75.0000 (70.4167) lr 1.5225e-03 eta 0:00:01
epoch [67/200] batch [5/49] time 0.569 (0.494) data 0.437 (0.362) loss_u loss_u 0.8032 (0.7850) acc_u 25.0000 (28.1250) lr 1.5225e-03 eta 0:00:21
epoch [67/200] batch [10/49] time 0.351 (0.489) data 0.218 (0.357) loss_u loss_u 0.7896 (0.7818) acc_u 25.0000 (28.1250) lr 1.5225e-03 eta 0:00:19
epoch [67/200] batch [15/49] time 0.372 (0.483) data 0.240 (0.351) loss_u loss_u 0.7974 (0.7893) acc_u 28.1250 (26.6667) lr 1.5225e-03 eta 0:00:16
epoch [67/200] batch [20/49] time 0.449 (0.475) data 0.316 (0.343) loss_u loss_u 0.7700 (0.7852) acc_u 28.1250 (28.1250) lr 1.5225e-03 eta 0:00:13
epoch [67/200] batch [25/49] time 0.468 (0.473) data 0.337 (0.341) loss_u loss_u 0.7925 (0.7827) acc_u 28.1250 (29.0000) lr 1.5225e-03 eta 0:00:11
epoch [67/200] batch [30/49] time 0.529 (0.474) data 0.398 (0.342) loss_u loss_u 0.8169 (0.7840) acc_u 25.0000 (28.8542) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [35/49] time 0.399 (0.472) data 0.266 (0.341) loss_u loss_u 0.8208 (0.7818) acc_u 25.0000 (29.1964) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [40/49] time 0.316 (0.470) data 0.183 (0.338) loss_u loss_u 0.7861 (0.7841) acc_u 34.3750 (28.6719) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [45/49] time 0.481 (0.472) data 0.345 (0.340) loss_u loss_u 0.7075 (0.7822) acc_u 34.3750 (28.9583) lr 1.5225e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1378
confident_label rate tensor(0.5048, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1583
clean true:1581
clean false:2
clean_rate:0.9987365761212887
noisy true:177
noisy false:1376
after delete: len(clean_dataset) 1583
after delete: len(noisy_dataset) 1553
epoch [68/200] batch [5/49] time 0.337 (0.431) data 0.206 (0.300) loss_x loss_x 1.2432 (1.4979) acc_x 59.3750 (60.6250) lr 1.5090e-03 eta 0:00:18
epoch [68/200] batch [10/49] time 0.442 (0.441) data 0.311 (0.310) loss_x loss_x 1.5391 (1.4329) acc_x 75.0000 (65.6250) lr 1.5090e-03 eta 0:00:17
epoch [68/200] batch [15/49] time 0.562 (0.468) data 0.431 (0.337) loss_x loss_x 1.3506 (1.3593) acc_x 65.6250 (67.0833) lr 1.5090e-03 eta 0:00:15
epoch [68/200] batch [20/49] time 0.610 (0.480) data 0.480 (0.349) loss_x loss_x 1.0039 (1.3025) acc_x 75.0000 (68.4375) lr 1.5090e-03 eta 0:00:13
epoch [68/200] batch [25/49] time 0.396 (0.466) data 0.264 (0.335) loss_x loss_x 1.0576 (1.3004) acc_x 65.6250 (67.6250) lr 1.5090e-03 eta 0:00:11
epoch [68/200] batch [30/49] time 0.489 (0.466) data 0.358 (0.335) loss_x loss_x 1.0801 (1.2862) acc_x 75.0000 (67.8125) lr 1.5090e-03 eta 0:00:08
epoch [68/200] batch [35/49] time 0.416 (0.462) data 0.285 (0.331) loss_x loss_x 1.5771 (1.2838) acc_x 59.3750 (67.6786) lr 1.5090e-03 eta 0:00:06
epoch [68/200] batch [40/49] time 0.565 (0.471) data 0.432 (0.340) loss_x loss_x 1.0088 (1.2682) acc_x 75.0000 (68.2031) lr 1.5090e-03 eta 0:00:04
epoch [68/200] batch [45/49] time 0.465 (0.468) data 0.333 (0.337) loss_x loss_x 0.9712 (1.2769) acc_x 71.8750 (68.0556) lr 1.5090e-03 eta 0:00:01
epoch [68/200] batch [5/48] time 0.878 (0.487) data 0.744 (0.355) loss_u loss_u 0.7627 (0.7760) acc_u 37.5000 (29.3750) lr 1.5090e-03 eta 0:00:20
epoch [68/200] batch [10/48] time 0.367 (0.483) data 0.233 (0.351) loss_u loss_u 0.8398 (0.7842) acc_u 15.6250 (27.8125) lr 1.5090e-03 eta 0:00:18
epoch [68/200] batch [15/48] time 0.433 (0.475) data 0.301 (0.344) loss_u loss_u 0.8618 (0.7981) acc_u 15.6250 (25.4167) lr 1.5090e-03 eta 0:00:15
epoch [68/200] batch [20/48] time 0.460 (0.474) data 0.329 (0.343) loss_u loss_u 0.7720 (0.7858) acc_u 31.2500 (27.1875) lr 1.5090e-03 eta 0:00:13
epoch [68/200] batch [25/48] time 0.414 (0.473) data 0.283 (0.341) loss_u loss_u 0.8408 (0.7852) acc_u 15.6250 (27.1250) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [30/48] time 0.490 (0.469) data 0.358 (0.337) loss_u loss_u 0.8076 (0.7874) acc_u 21.8750 (26.9792) lr 1.5090e-03 eta 0:00:08
epoch [68/200] batch [35/48] time 0.601 (0.469) data 0.469 (0.337) loss_u loss_u 0.7095 (0.7838) acc_u 34.3750 (27.2321) lr 1.5090e-03 eta 0:00:06
epoch [68/200] batch [40/48] time 0.470 (0.467) data 0.338 (0.335) loss_u loss_u 0.6958 (0.7850) acc_u 31.2500 (27.0312) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [45/48] time 0.411 (0.465) data 0.280 (0.333) loss_u loss_u 0.7153 (0.7838) acc_u 37.5000 (27.0833) lr 1.5090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1378
confident_label rate tensor(0.5057, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1586
clean true:1583
clean false:3
clean_rate:0.998108448928121
noisy true:175
noisy false:1375
after delete: len(clean_dataset) 1586
after delete: len(noisy_dataset) 1550
epoch [69/200] batch [5/49] time 0.574 (0.509) data 0.444 (0.378) loss_x loss_x 1.2412 (1.0659) acc_x 71.8750 (71.8750) lr 1.4955e-03 eta 0:00:22
epoch [69/200] batch [10/49] time 0.557 (0.499) data 0.424 (0.367) loss_x loss_x 1.0889 (1.0411) acc_x 68.7500 (71.8750) lr 1.4955e-03 eta 0:00:19
epoch [69/200] batch [15/49] time 0.430 (0.482) data 0.298 (0.351) loss_x loss_x 1.1641 (1.0497) acc_x 68.7500 (71.8750) lr 1.4955e-03 eta 0:00:16
epoch [69/200] batch [20/49] time 0.458 (0.483) data 0.327 (0.352) loss_x loss_x 1.7080 (1.1244) acc_x 68.7500 (71.0938) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [25/49] time 0.454 (0.481) data 0.323 (0.350) loss_x loss_x 1.3506 (1.1557) acc_x 71.8750 (71.1250) lr 1.4955e-03 eta 0:00:11
epoch [69/200] batch [30/49] time 0.444 (0.476) data 0.312 (0.344) loss_x loss_x 1.6348 (1.2052) acc_x 56.2500 (70.1042) lr 1.4955e-03 eta 0:00:09
epoch [69/200] batch [35/49] time 0.519 (0.477) data 0.388 (0.346) loss_x loss_x 1.0693 (1.1893) acc_x 68.7500 (70.0893) lr 1.4955e-03 eta 0:00:06
epoch [69/200] batch [40/49] time 0.443 (0.474) data 0.312 (0.343) loss_x loss_x 1.1953 (1.1633) acc_x 68.7500 (70.4688) lr 1.4955e-03 eta 0:00:04
epoch [69/200] batch [45/49] time 0.483 (0.477) data 0.351 (0.346) loss_x loss_x 0.9961 (1.1934) acc_x 75.0000 (69.7222) lr 1.4955e-03 eta 0:00:01
epoch [69/200] batch [5/48] time 0.399 (0.472) data 0.268 (0.341) loss_u loss_u 0.8599 (0.8363) acc_u 18.7500 (20.6250) lr 1.4955e-03 eta 0:00:20
epoch [69/200] batch [10/48] time 0.386 (0.472) data 0.254 (0.340) loss_u loss_u 0.7065 (0.8038) acc_u 43.7500 (28.4375) lr 1.4955e-03 eta 0:00:17
epoch [69/200] batch [15/48] time 0.360 (0.468) data 0.228 (0.336) loss_u loss_u 0.7358 (0.7871) acc_u 34.3750 (29.7917) lr 1.4955e-03 eta 0:00:15
epoch [69/200] batch [20/48] time 0.517 (0.462) data 0.386 (0.331) loss_u loss_u 0.7344 (0.7946) acc_u 34.3750 (28.5938) lr 1.4955e-03 eta 0:00:12
epoch [69/200] batch [25/48] time 0.457 (0.459) data 0.326 (0.328) loss_u loss_u 0.8721 (0.7954) acc_u 18.7500 (28.1250) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [30/48] time 0.460 (0.459) data 0.329 (0.328) loss_u loss_u 0.7778 (0.7876) acc_u 25.0000 (28.8542) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [35/48] time 0.468 (0.462) data 0.337 (0.331) loss_u loss_u 0.7422 (0.7892) acc_u 31.2500 (28.8393) lr 1.4955e-03 eta 0:00:06
epoch [69/200] batch [40/48] time 0.499 (0.466) data 0.367 (0.335) loss_u loss_u 0.7803 (0.7939) acc_u 31.2500 (28.1250) lr 1.4955e-03 eta 0:00:03
epoch [69/200] batch [45/48] time 0.399 (0.464) data 0.267 (0.333) loss_u loss_u 0.8896 (0.7929) acc_u 18.7500 (28.1944) lr 1.4955e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1418
confident_label rate tensor(0.4911, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1540
clean true:1540
clean false:0
clean_rate:1.0
noisy true:178
noisy false:1418
after delete: len(clean_dataset) 1540
after delete: len(noisy_dataset) 1596
epoch [70/200] batch [5/48] time 0.629 (0.538) data 0.498 (0.407) loss_x loss_x 0.8164 (1.1227) acc_x 81.2500 (69.3750) lr 1.4818e-03 eta 0:00:23
epoch [70/200] batch [10/48] time 0.440 (0.502) data 0.309 (0.370) loss_x loss_x 1.1855 (1.1324) acc_x 68.7500 (69.3750) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [15/48] time 0.528 (0.499) data 0.398 (0.368) loss_x loss_x 1.7217 (1.1598) acc_x 59.3750 (69.1667) lr 1.4818e-03 eta 0:00:16
epoch [70/200] batch [20/48] time 0.352 (0.485) data 0.222 (0.354) loss_x loss_x 1.2549 (1.1652) acc_x 65.6250 (68.7500) lr 1.4818e-03 eta 0:00:13
epoch [70/200] batch [25/48] time 0.431 (0.490) data 0.300 (0.359) loss_x loss_x 1.3438 (1.1180) acc_x 65.6250 (70.3750) lr 1.4818e-03 eta 0:00:11
epoch [70/200] batch [30/48] time 0.452 (0.490) data 0.321 (0.359) loss_x loss_x 0.6929 (1.1346) acc_x 90.6250 (70.6250) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [35/48] time 0.461 (0.487) data 0.330 (0.356) loss_x loss_x 0.8652 (1.1262) acc_x 78.1250 (70.3571) lr 1.4818e-03 eta 0:00:06
epoch [70/200] batch [40/48] time 0.470 (0.483) data 0.339 (0.352) loss_x loss_x 1.4873 (1.1470) acc_x 62.5000 (69.6875) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [45/48] time 0.517 (0.482) data 0.385 (0.351) loss_x loss_x 0.7388 (1.1512) acc_x 78.1250 (69.5139) lr 1.4818e-03 eta 0:00:01
epoch [70/200] batch [5/49] time 0.566 (0.482) data 0.435 (0.350) loss_u loss_u 0.6880 (0.7512) acc_u 40.6250 (31.2500) lr 1.4818e-03 eta 0:00:21
epoch [70/200] batch [10/49] time 0.579 (0.482) data 0.448 (0.351) loss_u loss_u 0.7910 (0.7706) acc_u 25.0000 (28.4375) lr 1.4818e-03 eta 0:00:18
epoch [70/200] batch [15/49] time 0.420 (0.480) data 0.288 (0.349) loss_u loss_u 0.7725 (0.7733) acc_u 28.1250 (28.3333) lr 1.4818e-03 eta 0:00:16
epoch [70/200] batch [20/49] time 0.389 (0.476) data 0.258 (0.345) loss_u loss_u 0.7651 (0.7726) acc_u 31.2500 (28.7500) lr 1.4818e-03 eta 0:00:13
epoch [70/200] batch [25/49] time 0.372 (0.470) data 0.241 (0.339) loss_u loss_u 0.7983 (0.7807) acc_u 34.3750 (27.7500) lr 1.4818e-03 eta 0:00:11
epoch [70/200] batch [30/49] time 0.469 (0.467) data 0.337 (0.336) loss_u loss_u 0.8188 (0.7817) acc_u 21.8750 (27.1875) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [35/49] time 0.370 (0.467) data 0.238 (0.335) loss_u loss_u 0.8188 (0.7819) acc_u 28.1250 (27.6786) lr 1.4818e-03 eta 0:00:06
epoch [70/200] batch [40/49] time 0.399 (0.465) data 0.268 (0.334) loss_u loss_u 0.7559 (0.7829) acc_u 34.3750 (27.5781) lr 1.4818e-03 eta 0:00:04
epoch [70/200] batch [45/49] time 0.443 (0.462) data 0.311 (0.331) loss_u loss_u 0.7188 (0.7797) acc_u 37.5000 (28.1250) lr 1.4818e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1433
confident_label rate tensor(0.4904, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1538
clean true:1536
clean false:2
clean_rate:0.9986996098829649
noisy true:167
noisy false:1431
after delete: len(clean_dataset) 1538
after delete: len(noisy_dataset) 1598
epoch [71/200] batch [5/48] time 0.491 (0.438) data 0.360 (0.307) loss_x loss_x 1.0820 (1.2038) acc_x 75.0000 (70.0000) lr 1.4679e-03 eta 0:00:18
epoch [71/200] batch [10/48] time 0.481 (0.449) data 0.350 (0.318) loss_x loss_x 1.5537 (1.2247) acc_x 59.3750 (69.0625) lr 1.4679e-03 eta 0:00:17
epoch [71/200] batch [15/48] time 0.384 (0.461) data 0.253 (0.329) loss_x loss_x 1.0088 (1.1831) acc_x 68.7500 (70.0000) lr 1.4679e-03 eta 0:00:15
epoch [71/200] batch [20/48] time 0.548 (0.473) data 0.417 (0.342) loss_x loss_x 1.1650 (1.1476) acc_x 71.8750 (70.0000) lr 1.4679e-03 eta 0:00:13
epoch [71/200] batch [25/48] time 0.420 (0.475) data 0.289 (0.344) loss_x loss_x 1.2305 (1.1655) acc_x 75.0000 (69.7500) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [30/48] time 0.414 (0.473) data 0.282 (0.342) loss_x loss_x 1.1709 (1.1430) acc_x 75.0000 (70.4167) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [35/48] time 0.465 (0.473) data 0.333 (0.342) loss_x loss_x 0.8232 (1.1373) acc_x 75.0000 (71.0714) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [40/48] time 0.401 (0.471) data 0.270 (0.340) loss_x loss_x 1.4756 (1.1512) acc_x 62.5000 (70.3906) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [45/48] time 0.413 (0.471) data 0.282 (0.340) loss_x loss_x 1.2598 (1.1535) acc_x 68.7500 (70.5556) lr 1.4679e-03 eta 0:00:01
epoch [71/200] batch [5/49] time 0.360 (0.462) data 0.229 (0.331) loss_u loss_u 0.7544 (0.8102) acc_u 34.3750 (23.1250) lr 1.4679e-03 eta 0:00:20
epoch [71/200] batch [10/49] time 0.517 (0.458) data 0.384 (0.327) loss_u loss_u 0.7378 (0.7792) acc_u 37.5000 (27.5000) lr 1.4679e-03 eta 0:00:17
epoch [71/200] batch [15/49] time 0.557 (0.460) data 0.426 (0.329) loss_u loss_u 0.7466 (0.7857) acc_u 31.2500 (26.6667) lr 1.4679e-03 eta 0:00:15
epoch [71/200] batch [20/49] time 0.416 (0.464) data 0.285 (0.332) loss_u loss_u 0.7930 (0.7796) acc_u 28.1250 (28.1250) lr 1.4679e-03 eta 0:00:13
epoch [71/200] batch [25/49] time 0.457 (0.461) data 0.326 (0.330) loss_u loss_u 0.6406 (0.7660) acc_u 50.0000 (30.0000) lr 1.4679e-03 eta 0:00:11
epoch [71/200] batch [30/49] time 0.364 (0.460) data 0.233 (0.329) loss_u loss_u 0.8345 (0.7697) acc_u 21.8750 (29.7917) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [35/49] time 0.501 (0.459) data 0.370 (0.328) loss_u loss_u 0.7725 (0.7748) acc_u 28.1250 (29.3750) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [40/49] time 0.354 (0.459) data 0.222 (0.328) loss_u loss_u 0.8018 (0.7750) acc_u 28.1250 (29.2188) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [45/49] time 0.505 (0.460) data 0.374 (0.329) loss_u loss_u 0.8457 (0.7803) acc_u 21.8750 (28.4722) lr 1.4679e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1419
confident_label rate tensor(0.4908, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1539
clean true:1537
clean false:2
clean_rate:0.9987004548408057
noisy true:180
noisy false:1417
after delete: len(clean_dataset) 1539
after delete: len(noisy_dataset) 1597
epoch [72/200] batch [5/48] time 0.419 (0.506) data 0.289 (0.375) loss_x loss_x 1.7764 (1.1054) acc_x 56.2500 (70.0000) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [10/48] time 0.478 (0.496) data 0.347 (0.365) loss_x loss_x 1.0293 (1.1745) acc_x 84.3750 (71.2500) lr 1.4540e-03 eta 0:00:18
epoch [72/200] batch [15/48] time 0.400 (0.486) data 0.269 (0.354) loss_x loss_x 1.2695 (1.2107) acc_x 62.5000 (69.3750) lr 1.4540e-03 eta 0:00:16
epoch [72/200] batch [20/48] time 0.472 (0.483) data 0.342 (0.352) loss_x loss_x 1.2295 (1.1968) acc_x 65.6250 (69.6875) lr 1.4540e-03 eta 0:00:13
epoch [72/200] batch [25/48] time 0.453 (0.487) data 0.321 (0.355) loss_x loss_x 1.1699 (1.2360) acc_x 75.0000 (69.2500) lr 1.4540e-03 eta 0:00:11
epoch [72/200] batch [30/48] time 0.484 (0.481) data 0.352 (0.350) loss_x loss_x 0.6216 (1.2035) acc_x 84.3750 (70.3125) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [35/48] time 0.439 (0.488) data 0.308 (0.357) loss_x loss_x 1.1865 (1.1954) acc_x 68.7500 (70.3571) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [40/48] time 0.479 (0.486) data 0.348 (0.355) loss_x loss_x 1.1992 (1.1958) acc_x 78.1250 (70.7812) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [45/48] time 0.441 (0.487) data 0.310 (0.355) loss_x loss_x 1.3945 (1.2060) acc_x 65.6250 (70.2083) lr 1.4540e-03 eta 0:00:01
epoch [72/200] batch [5/49] time 0.344 (0.479) data 0.213 (0.348) loss_u loss_u 0.8696 (0.8086) acc_u 18.7500 (25.6250) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [10/49] time 0.579 (0.477) data 0.447 (0.346) loss_u loss_u 0.8008 (0.8027) acc_u 25.0000 (25.6250) lr 1.4540e-03 eta 0:00:18
epoch [72/200] batch [15/49] time 0.385 (0.476) data 0.253 (0.344) loss_u loss_u 0.8418 (0.7887) acc_u 18.7500 (27.5000) lr 1.4540e-03 eta 0:00:16
epoch [72/200] batch [20/49] time 0.571 (0.473) data 0.438 (0.341) loss_u loss_u 0.6914 (0.7827) acc_u 40.6250 (28.7500) lr 1.4540e-03 eta 0:00:13
epoch [72/200] batch [25/49] time 0.570 (0.475) data 0.436 (0.343) loss_u loss_u 0.8320 (0.7816) acc_u 25.0000 (28.8750) lr 1.4540e-03 eta 0:00:11
epoch [72/200] batch [30/49] time 0.380 (0.472) data 0.248 (0.340) loss_u loss_u 0.7998 (0.7880) acc_u 25.0000 (27.9167) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [35/49] time 0.397 (0.469) data 0.266 (0.338) loss_u loss_u 0.6748 (0.7889) acc_u 40.6250 (28.1250) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [40/49] time 0.395 (0.468) data 0.263 (0.336) loss_u loss_u 0.7588 (0.7819) acc_u 28.1250 (28.8281) lr 1.4540e-03 eta 0:00:04
epoch [72/200] batch [45/49] time 0.469 (0.470) data 0.337 (0.338) loss_u loss_u 0.8413 (0.7804) acc_u 21.8750 (29.1667) lr 1.4540e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1360
confident_label rate tensor(0.5112, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1603
clean true:1600
clean false:3
clean_rate:0.9981285090455396
noisy true:176
noisy false:1357
after delete: len(clean_dataset) 1603
after delete: len(noisy_dataset) 1533
epoch [73/200] batch [5/50] time 0.402 (0.491) data 0.271 (0.359) loss_x loss_x 1.3086 (1.2406) acc_x 68.7500 (65.0000) lr 1.4399e-03 eta 0:00:22
epoch [73/200] batch [10/50] time 0.505 (0.514) data 0.373 (0.382) loss_x loss_x 0.9097 (1.1771) acc_x 71.8750 (66.8750) lr 1.4399e-03 eta 0:00:20
epoch [73/200] batch [15/50] time 0.447 (0.502) data 0.316 (0.370) loss_x loss_x 1.3838 (1.2703) acc_x 68.7500 (66.8750) lr 1.4399e-03 eta 0:00:17
epoch [73/200] batch [20/50] time 0.483 (0.509) data 0.352 (0.377) loss_x loss_x 1.2500 (1.2809) acc_x 71.8750 (66.5625) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [25/50] time 0.468 (0.514) data 0.336 (0.383) loss_x loss_x 1.2871 (1.2505) acc_x 75.0000 (67.5000) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [30/50] time 0.481 (0.508) data 0.349 (0.376) loss_x loss_x 1.3506 (1.2553) acc_x 68.7500 (67.2917) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [35/50] time 0.409 (0.515) data 0.277 (0.383) loss_x loss_x 0.8120 (1.2534) acc_x 84.3750 (67.7679) lr 1.4399e-03 eta 0:00:07
epoch [73/200] batch [40/50] time 0.582 (0.511) data 0.450 (0.379) loss_x loss_x 1.3398 (1.2533) acc_x 59.3750 (67.4219) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [45/50] time 0.472 (0.508) data 0.341 (0.377) loss_x loss_x 1.0000 (1.2538) acc_x 75.0000 (67.4306) lr 1.4399e-03 eta 0:00:02
epoch [73/200] batch [50/50] time 0.354 (0.497) data 0.223 (0.366) loss_x loss_x 0.9995 (1.2286) acc_x 68.7500 (67.8750) lr 1.4399e-03 eta 0:00:00
epoch [73/200] batch [5/47] time 0.500 (0.492) data 0.368 (0.360) loss_u loss_u 0.8506 (0.7877) acc_u 21.8750 (28.1250) lr 1.4399e-03 eta 0:00:20
epoch [73/200] batch [10/47] time 0.320 (0.488) data 0.189 (0.357) loss_u loss_u 0.7715 (0.7842) acc_u 34.3750 (28.1250) lr 1.4399e-03 eta 0:00:18
epoch [73/200] batch [15/47] time 0.458 (0.487) data 0.327 (0.355) loss_u loss_u 0.7969 (0.7826) acc_u 18.7500 (27.2917) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [20/47] time 0.532 (0.481) data 0.401 (0.349) loss_u loss_u 0.7881 (0.7801) acc_u 21.8750 (27.8125) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [25/47] time 0.439 (0.479) data 0.306 (0.348) loss_u loss_u 0.7373 (0.7790) acc_u 31.2500 (28.0000) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [30/47] time 0.368 (0.479) data 0.234 (0.348) loss_u loss_u 0.8833 (0.7806) acc_u 25.0000 (28.7500) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [35/47] time 0.411 (0.477) data 0.280 (0.346) loss_u loss_u 0.7817 (0.7825) acc_u 28.1250 (28.5714) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [40/47] time 0.397 (0.472) data 0.265 (0.340) loss_u loss_u 0.7266 (0.7791) acc_u 31.2500 (28.5938) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [45/47] time 0.379 (0.472) data 0.246 (0.340) loss_u loss_u 0.7949 (0.7801) acc_u 28.1250 (28.4722) lr 1.4399e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1416
confident_label rate tensor(0.4895, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1535
clean true:1534
clean false:1
clean_rate:0.9993485342019544
noisy true:186
noisy false:1415
after delete: len(clean_dataset) 1535
after delete: len(noisy_dataset) 1601
epoch [74/200] batch [5/47] time 0.534 (0.530) data 0.404 (0.398) loss_x loss_x 1.0840 (1.2460) acc_x 78.1250 (68.1250) lr 1.4258e-03 eta 0:00:22
epoch [74/200] batch [10/47] time 0.471 (0.502) data 0.339 (0.370) loss_x loss_x 1.3301 (1.3198) acc_x 65.6250 (66.2500) lr 1.4258e-03 eta 0:00:18
epoch [74/200] batch [15/47] time 0.459 (0.492) data 0.328 (0.360) loss_x loss_x 1.0195 (1.2535) acc_x 78.1250 (68.7500) lr 1.4258e-03 eta 0:00:15
epoch [74/200] batch [20/47] time 0.514 (0.489) data 0.383 (0.357) loss_x loss_x 0.9390 (1.2302) acc_x 71.8750 (70.0000) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [25/47] time 0.396 (0.486) data 0.265 (0.354) loss_x loss_x 1.0605 (1.1806) acc_x 65.6250 (70.6250) lr 1.4258e-03 eta 0:00:10
epoch [74/200] batch [30/47] time 0.445 (0.486) data 0.313 (0.355) loss_x loss_x 1.2979 (1.1725) acc_x 56.2500 (70.1042) lr 1.4258e-03 eta 0:00:08
epoch [74/200] batch [35/47] time 0.433 (0.486) data 0.302 (0.355) loss_x loss_x 1.6289 (1.1908) acc_x 59.3750 (69.9107) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [40/47] time 0.492 (0.481) data 0.360 (0.349) loss_x loss_x 1.1748 (1.2005) acc_x 71.8750 (69.2969) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [45/47] time 0.483 (0.482) data 0.351 (0.350) loss_x loss_x 1.5195 (1.2177) acc_x 65.6250 (68.8889) lr 1.4258e-03 eta 0:00:00
epoch [74/200] batch [5/50] time 0.376 (0.474) data 0.244 (0.343) loss_u loss_u 0.6328 (0.7338) acc_u 50.0000 (35.6250) lr 1.4258e-03 eta 0:00:21
epoch [74/200] batch [10/50] time 0.482 (0.471) data 0.350 (0.339) loss_u loss_u 0.7788 (0.7699) acc_u 25.0000 (28.4375) lr 1.4258e-03 eta 0:00:18
epoch [74/200] batch [15/50] time 0.408 (0.468) data 0.277 (0.336) loss_u loss_u 0.8594 (0.7841) acc_u 18.7500 (28.1250) lr 1.4258e-03 eta 0:00:16
epoch [74/200] batch [20/50] time 0.384 (0.464) data 0.253 (0.332) loss_u loss_u 0.7158 (0.7846) acc_u 34.3750 (27.8125) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [25/50] time 0.441 (0.461) data 0.310 (0.329) loss_u loss_u 0.6792 (0.7786) acc_u 34.3750 (28.3750) lr 1.4258e-03 eta 0:00:11
epoch [74/200] batch [30/50] time 0.407 (0.461) data 0.275 (0.329) loss_u loss_u 0.7202 (0.7789) acc_u 34.3750 (28.6458) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [35/50] time 0.384 (0.460) data 0.251 (0.328) loss_u loss_u 0.8096 (0.7834) acc_u 21.8750 (28.3036) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [40/50] time 0.628 (0.463) data 0.496 (0.331) loss_u loss_u 0.7046 (0.7787) acc_u 46.8750 (29.3750) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [45/50] time 0.459 (0.460) data 0.328 (0.328) loss_u loss_u 0.8359 (0.7820) acc_u 18.7500 (29.0278) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [50/50] time 0.502 (0.458) data 0.371 (0.327) loss_u loss_u 0.8076 (0.7816) acc_u 25.0000 (29.0625) lr 1.4258e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1377
confident_label rate tensor(0.5064, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1588
clean true:1588
clean false:0
clean_rate:1.0
noisy true:171
noisy false:1377
after delete: len(clean_dataset) 1588
after delete: len(noisy_dataset) 1548
epoch [75/200] batch [5/49] time 0.377 (0.534) data 0.245 (0.402) loss_x loss_x 1.1465 (1.2990) acc_x 78.1250 (66.2500) lr 1.4115e-03 eta 0:00:23
epoch [75/200] batch [10/49] time 0.423 (0.519) data 0.292 (0.387) loss_x loss_x 1.4189 (1.2702) acc_x 56.2500 (66.8750) lr 1.4115e-03 eta 0:00:20
epoch [75/200] batch [15/49] time 0.434 (0.500) data 0.302 (0.368) loss_x loss_x 1.4033 (1.2426) acc_x 62.5000 (68.9583) lr 1.4115e-03 eta 0:00:16
epoch [75/200] batch [20/49] time 0.566 (0.498) data 0.435 (0.367) loss_x loss_x 1.2920 (1.2113) acc_x 65.6250 (69.2188) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [25/49] time 0.367 (0.494) data 0.236 (0.363) loss_x loss_x 1.2100 (1.2033) acc_x 75.0000 (69.3750) lr 1.4115e-03 eta 0:00:11
epoch [75/200] batch [30/49] time 0.426 (0.496) data 0.295 (0.365) loss_x loss_x 1.3633 (1.1828) acc_x 62.5000 (70.0000) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [35/49] time 0.350 (0.491) data 0.219 (0.360) loss_x loss_x 1.2861 (1.1696) acc_x 68.7500 (70.0893) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [40/49] time 0.404 (0.488) data 0.272 (0.357) loss_x loss_x 1.2871 (1.1847) acc_x 68.7500 (69.3750) lr 1.4115e-03 eta 0:00:04
epoch [75/200] batch [45/49] time 0.530 (0.489) data 0.398 (0.358) loss_x loss_x 1.0225 (1.1744) acc_x 71.8750 (69.3750) lr 1.4115e-03 eta 0:00:01
epoch [75/200] batch [5/48] time 0.375 (0.486) data 0.244 (0.354) loss_u loss_u 0.7671 (0.7790) acc_u 21.8750 (26.8750) lr 1.4115e-03 eta 0:00:20
epoch [75/200] batch [10/48] time 0.543 (0.492) data 0.410 (0.360) loss_u loss_u 0.7798 (0.7866) acc_u 28.1250 (25.6250) lr 1.4115e-03 eta 0:00:18
epoch [75/200] batch [15/48] time 0.440 (0.485) data 0.307 (0.354) loss_u loss_u 0.9082 (0.8007) acc_u 15.6250 (25.4167) lr 1.4115e-03 eta 0:00:16
epoch [75/200] batch [20/48] time 0.390 (0.479) data 0.257 (0.348) loss_u loss_u 0.7725 (0.7948) acc_u 28.1250 (26.2500) lr 1.4115e-03 eta 0:00:13
epoch [75/200] batch [25/48] time 0.634 (0.479) data 0.499 (0.348) loss_u loss_u 0.6953 (0.7846) acc_u 40.6250 (27.6250) lr 1.4115e-03 eta 0:00:11
epoch [75/200] batch [30/48] time 0.407 (0.477) data 0.275 (0.345) loss_u loss_u 0.7891 (0.7840) acc_u 34.3750 (28.0208) lr 1.4115e-03 eta 0:00:08
epoch [75/200] batch [35/48] time 0.499 (0.471) data 0.367 (0.340) loss_u loss_u 0.7021 (0.7842) acc_u 37.5000 (27.8571) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [40/48] time 0.452 (0.468) data 0.320 (0.336) loss_u loss_u 0.7480 (0.7830) acc_u 34.3750 (28.0469) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [45/48] time 0.524 (0.465) data 0.392 (0.333) loss_u loss_u 0.7061 (0.7797) acc_u 34.3750 (28.3333) lr 1.4115e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1357
confident_label rate tensor(0.5080, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1593
clean true:1592
clean false:1
clean_rate:0.9993722536095417
noisy true:187
noisy false:1356
after delete: len(clean_dataset) 1593
after delete: len(noisy_dataset) 1543
epoch [76/200] batch [5/49] time 0.407 (0.460) data 0.277 (0.329) loss_x loss_x 0.7974 (1.1169) acc_x 81.2500 (70.0000) lr 1.3971e-03 eta 0:00:20
epoch [76/200] batch [10/49] time 0.766 (0.515) data 0.635 (0.384) loss_x loss_x 0.9297 (1.1444) acc_x 78.1250 (69.0625) lr 1.3971e-03 eta 0:00:20
epoch [76/200] batch [15/49] time 0.425 (0.478) data 0.295 (0.348) loss_x loss_x 1.0020 (1.1662) acc_x 75.0000 (68.9583) lr 1.3971e-03 eta 0:00:16
epoch [76/200] batch [20/49] time 0.572 (0.472) data 0.442 (0.342) loss_x loss_x 1.5010 (1.2019) acc_x 68.7500 (67.5000) lr 1.3971e-03 eta 0:00:13
epoch [76/200] batch [25/49] time 0.460 (0.470) data 0.329 (0.339) loss_x loss_x 1.3389 (1.1819) acc_x 56.2500 (68.0000) lr 1.3971e-03 eta 0:00:11
epoch [76/200] batch [30/49] time 0.383 (0.477) data 0.252 (0.346) loss_x loss_x 0.9634 (1.1983) acc_x 78.1250 (68.3333) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [35/49] time 0.589 (0.473) data 0.458 (0.342) loss_x loss_x 1.3936 (1.1960) acc_x 62.5000 (68.1250) lr 1.3971e-03 eta 0:00:06
epoch [76/200] batch [40/49] time 0.550 (0.470) data 0.419 (0.340) loss_x loss_x 1.4629 (1.1945) acc_x 62.5000 (68.1250) lr 1.3971e-03 eta 0:00:04
epoch [76/200] batch [45/49] time 0.422 (0.467) data 0.292 (0.336) loss_x loss_x 1.5938 (1.1916) acc_x 59.3750 (68.1250) lr 1.3971e-03 eta 0:00:01
epoch [76/200] batch [5/48] time 0.300 (0.463) data 0.169 (0.333) loss_u loss_u 0.7817 (0.7923) acc_u 34.3750 (28.1250) lr 1.3971e-03 eta 0:00:19
epoch [76/200] batch [10/48] time 0.356 (0.456) data 0.225 (0.325) loss_u loss_u 0.8745 (0.7723) acc_u 25.0000 (31.2500) lr 1.3971e-03 eta 0:00:17
epoch [76/200] batch [15/48] time 0.440 (0.454) data 0.309 (0.323) loss_u loss_u 0.8003 (0.7921) acc_u 34.3750 (28.7500) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [20/48] time 0.346 (0.451) data 0.215 (0.320) loss_u loss_u 0.7754 (0.7925) acc_u 31.2500 (28.2812) lr 1.3971e-03 eta 0:00:12
epoch [76/200] batch [25/48] time 0.402 (0.451) data 0.270 (0.320) loss_u loss_u 0.8374 (0.8005) acc_u 18.7500 (26.5000) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [30/48] time 0.415 (0.450) data 0.283 (0.319) loss_u loss_u 0.7490 (0.7944) acc_u 28.1250 (27.5000) lr 1.3971e-03 eta 0:00:08
epoch [76/200] batch [35/48] time 0.406 (0.450) data 0.273 (0.319) loss_u loss_u 0.8008 (0.7927) acc_u 21.8750 (27.9464) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [40/48] time 0.406 (0.448) data 0.274 (0.317) loss_u loss_u 0.8232 (0.7939) acc_u 25.0000 (27.7344) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [45/48] time 0.343 (0.449) data 0.210 (0.318) loss_u loss_u 0.7866 (0.7906) acc_u 25.0000 (27.8472) lr 1.3971e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1397
confident_label rate tensor(0.4997, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1567
clean true:1567
clean false:0
clean_rate:1.0
noisy true:172
noisy false:1397
after delete: len(clean_dataset) 1567
after delete: len(noisy_dataset) 1569
epoch [77/200] batch [5/48] time 0.456 (0.545) data 0.324 (0.413) loss_x loss_x 0.6440 (1.0953) acc_x 84.3750 (73.1250) lr 1.3827e-03 eta 0:00:23
epoch [77/200] batch [10/48] time 0.413 (0.508) data 0.282 (0.377) loss_x loss_x 0.9688 (1.0715) acc_x 71.8750 (71.8750) lr 1.3827e-03 eta 0:00:19
epoch [77/200] batch [15/48] time 0.497 (0.500) data 0.366 (0.369) loss_x loss_x 1.5498 (1.1310) acc_x 59.3750 (71.2500) lr 1.3827e-03 eta 0:00:16
epoch [77/200] batch [20/48] time 0.568 (0.495) data 0.437 (0.364) loss_x loss_x 0.9551 (1.1917) acc_x 78.1250 (70.1562) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [25/48] time 0.502 (0.488) data 0.371 (0.357) loss_x loss_x 1.2734 (1.1653) acc_x 65.6250 (70.1250) lr 1.3827e-03 eta 0:00:11
epoch [77/200] batch [30/48] time 0.424 (0.485) data 0.293 (0.353) loss_x loss_x 1.0146 (1.1917) acc_x 65.6250 (69.5833) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [35/48] time 0.454 (0.480) data 0.323 (0.348) loss_x loss_x 1.3838 (1.2071) acc_x 65.6250 (69.5536) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [40/48] time 0.358 (0.471) data 0.227 (0.340) loss_x loss_x 1.1553 (1.2128) acc_x 68.7500 (69.2969) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [45/48] time 0.332 (0.472) data 0.201 (0.340) loss_x loss_x 1.2051 (1.2181) acc_x 71.8750 (69.0972) lr 1.3827e-03 eta 0:00:01
epoch [77/200] batch [5/49] time 0.346 (0.462) data 0.214 (0.330) loss_u loss_u 0.7983 (0.7601) acc_u 25.0000 (28.7500) lr 1.3827e-03 eta 0:00:20
epoch [77/200] batch [10/49] time 0.594 (0.462) data 0.460 (0.330) loss_u loss_u 0.7769 (0.7771) acc_u 28.1250 (26.8750) lr 1.3827e-03 eta 0:00:18
epoch [77/200] batch [15/49] time 0.410 (0.460) data 0.278 (0.328) loss_u loss_u 0.8120 (0.7758) acc_u 25.0000 (27.5000) lr 1.3827e-03 eta 0:00:15
epoch [77/200] batch [20/49] time 0.465 (0.459) data 0.333 (0.328) loss_u loss_u 0.7236 (0.7780) acc_u 34.3750 (27.0312) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [25/49] time 0.433 (0.460) data 0.301 (0.328) loss_u loss_u 0.7959 (0.7790) acc_u 28.1250 (27.3750) lr 1.3827e-03 eta 0:00:11
epoch [77/200] batch [30/49] time 0.616 (0.463) data 0.485 (0.332) loss_u loss_u 0.7354 (0.7814) acc_u 31.2500 (27.0833) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [35/49] time 0.383 (0.462) data 0.252 (0.330) loss_u loss_u 0.8003 (0.7789) acc_u 34.3750 (28.2143) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [40/49] time 0.405 (0.460) data 0.273 (0.329) loss_u loss_u 0.7656 (0.7762) acc_u 28.1250 (28.5938) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [45/49] time 0.430 (0.459) data 0.299 (0.328) loss_u loss_u 0.8374 (0.7789) acc_u 18.7500 (28.3333) lr 1.3827e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1428
confident_label rate tensor(0.4888, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1533
clean true:1532
clean false:1
clean_rate:0.9993476842791911
noisy true:176
noisy false:1427
after delete: len(clean_dataset) 1533
after delete: len(noisy_dataset) 1603
epoch [78/200] batch [5/47] time 0.340 (0.475) data 0.209 (0.344) loss_x loss_x 1.0625 (1.2188) acc_x 78.1250 (71.2500) lr 1.3681e-03 eta 0:00:19
epoch [78/200] batch [10/47] time 0.459 (0.486) data 0.328 (0.355) loss_x loss_x 1.3594 (1.1370) acc_x 59.3750 (72.1875) lr 1.3681e-03 eta 0:00:17
epoch [78/200] batch [15/47] time 0.422 (0.491) data 0.291 (0.360) loss_x loss_x 1.4619 (1.1695) acc_x 62.5000 (71.4583) lr 1.3681e-03 eta 0:00:15
epoch [78/200] batch [20/47] time 0.432 (0.481) data 0.300 (0.349) loss_x loss_x 0.9048 (1.2115) acc_x 71.8750 (70.1562) lr 1.3681e-03 eta 0:00:12
epoch [78/200] batch [25/47] time 0.487 (0.486) data 0.356 (0.355) loss_x loss_x 1.2324 (1.1981) acc_x 62.5000 (70.2500) lr 1.3681e-03 eta 0:00:10
epoch [78/200] batch [30/47] time 0.477 (0.490) data 0.345 (0.358) loss_x loss_x 1.4365 (1.2380) acc_x 75.0000 (69.5833) lr 1.3681e-03 eta 0:00:08
epoch [78/200] batch [35/47] time 0.513 (0.489) data 0.382 (0.357) loss_x loss_x 1.2041 (1.2468) acc_x 56.2500 (68.3929) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [40/47] time 0.508 (0.486) data 0.377 (0.355) loss_x loss_x 1.1973 (1.2218) acc_x 65.6250 (68.9844) lr 1.3681e-03 eta 0:00:03
epoch [78/200] batch [45/47] time 0.379 (0.479) data 0.248 (0.347) loss_x loss_x 1.2998 (1.2044) acc_x 62.5000 (69.1667) lr 1.3681e-03 eta 0:00:00
epoch [78/200] batch [5/50] time 0.458 (0.480) data 0.325 (0.349) loss_u loss_u 0.7559 (0.7293) acc_u 37.5000 (35.6250) lr 1.3681e-03 eta 0:00:21
epoch [78/200] batch [10/50] time 0.482 (0.483) data 0.349 (0.351) loss_u loss_u 0.8081 (0.7469) acc_u 18.7500 (33.1250) lr 1.3681e-03 eta 0:00:19
epoch [78/200] batch [15/50] time 0.374 (0.482) data 0.243 (0.350) loss_u loss_u 0.7217 (0.7506) acc_u 34.3750 (32.7083) lr 1.3681e-03 eta 0:00:16
epoch [78/200] batch [20/50] time 0.459 (0.480) data 0.326 (0.349) loss_u loss_u 0.6895 (0.7617) acc_u 34.3750 (30.7812) lr 1.3681e-03 eta 0:00:14
epoch [78/200] batch [25/50] time 0.476 (0.475) data 0.344 (0.343) loss_u loss_u 0.7402 (0.7650) acc_u 31.2500 (29.7500) lr 1.3681e-03 eta 0:00:11
epoch [78/200] batch [30/50] time 0.559 (0.474) data 0.427 (0.343) loss_u loss_u 0.7842 (0.7640) acc_u 25.0000 (29.6875) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [35/50] time 0.410 (0.481) data 0.279 (0.349) loss_u loss_u 0.7471 (0.7662) acc_u 34.3750 (29.7321) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [40/50] time 0.471 (0.476) data 0.338 (0.344) loss_u loss_u 0.6792 (0.7647) acc_u 43.7500 (30.3125) lr 1.3681e-03 eta 0:00:04
epoch [78/200] batch [45/50] time 0.373 (0.474) data 0.242 (0.342) loss_u loss_u 0.7812 (0.7625) acc_u 31.2500 (30.9028) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [50/50] time 0.461 (0.473) data 0.330 (0.341) loss_u loss_u 0.6602 (0.7612) acc_u 46.8750 (31.0625) lr 1.3681e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1371
confident_label rate tensor(0.5064, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1588
clean true:1588
clean false:0
clean_rate:1.0
noisy true:177
noisy false:1371
after delete: len(clean_dataset) 1588
after delete: len(noisy_dataset) 1548
epoch [79/200] batch [5/49] time 0.522 (0.482) data 0.392 (0.352) loss_x loss_x 0.9077 (1.0456) acc_x 75.0000 (73.7500) lr 1.3535e-03 eta 0:00:21
epoch [79/200] batch [10/49] time 0.451 (0.466) data 0.319 (0.334) loss_x loss_x 0.8809 (1.0472) acc_x 78.1250 (72.1875) lr 1.3535e-03 eta 0:00:18
epoch [79/200] batch [15/49] time 0.477 (0.471) data 0.346 (0.340) loss_x loss_x 1.0723 (1.0900) acc_x 62.5000 (71.4583) lr 1.3535e-03 eta 0:00:16
epoch [79/200] batch [20/49] time 0.417 (0.466) data 0.286 (0.335) loss_x loss_x 1.3730 (1.1381) acc_x 68.7500 (70.3125) lr 1.3535e-03 eta 0:00:13
epoch [79/200] batch [25/49] time 0.392 (0.459) data 0.261 (0.328) loss_x loss_x 1.5723 (1.1492) acc_x 62.5000 (69.8750) lr 1.3535e-03 eta 0:00:11
epoch [79/200] batch [30/49] time 0.608 (0.473) data 0.477 (0.342) loss_x loss_x 1.2695 (1.1504) acc_x 62.5000 (70.0000) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [35/49] time 0.380 (0.475) data 0.248 (0.343) loss_x loss_x 1.0625 (1.1451) acc_x 75.0000 (70.1786) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [40/49] time 0.418 (0.470) data 0.287 (0.339) loss_x loss_x 0.9331 (1.1431) acc_x 75.0000 (70.4688) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [45/49] time 0.429 (0.474) data 0.298 (0.343) loss_x loss_x 1.2266 (1.1499) acc_x 68.7500 (70.7639) lr 1.3535e-03 eta 0:00:01
epoch [79/200] batch [5/48] time 0.349 (0.474) data 0.216 (0.343) loss_u loss_u 0.8535 (0.7830) acc_u 18.7500 (25.6250) lr 1.3535e-03 eta 0:00:20
epoch [79/200] batch [10/48] time 0.347 (0.467) data 0.214 (0.336) loss_u loss_u 0.7290 (0.7831) acc_u 25.0000 (27.1875) lr 1.3535e-03 eta 0:00:17
epoch [79/200] batch [15/48] time 0.383 (0.469) data 0.251 (0.337) loss_u loss_u 0.7964 (0.7668) acc_u 25.0000 (30.4167) lr 1.3535e-03 eta 0:00:15
epoch [79/200] batch [20/48] time 0.726 (0.471) data 0.594 (0.339) loss_u loss_u 0.7700 (0.7743) acc_u 28.1250 (29.0625) lr 1.3535e-03 eta 0:00:13
epoch [79/200] batch [25/48] time 0.373 (0.469) data 0.239 (0.338) loss_u loss_u 0.8184 (0.7746) acc_u 21.8750 (28.8750) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [30/48] time 0.386 (0.465) data 0.254 (0.333) loss_u loss_u 0.7681 (0.7811) acc_u 31.2500 (28.2292) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [35/48] time 0.542 (0.462) data 0.407 (0.331) loss_u loss_u 0.6646 (0.7738) acc_u 40.6250 (29.1071) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [40/48] time 0.518 (0.465) data 0.386 (0.333) loss_u loss_u 0.7065 (0.7733) acc_u 40.6250 (29.1406) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [45/48] time 0.467 (0.464) data 0.335 (0.332) loss_u loss_u 0.8076 (0.7768) acc_u 21.8750 (28.8194) lr 1.3535e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1375
confident_label rate tensor(0.5051, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1584
clean true:1584
clean false:0
clean_rate:1.0
noisy true:177
noisy false:1375
after delete: len(clean_dataset) 1584
after delete: len(noisy_dataset) 1552
epoch [80/200] batch [5/49] time 0.390 (0.486) data 0.260 (0.355) loss_x loss_x 1.3975 (1.1172) acc_x 68.7500 (75.0000) lr 1.3387e-03 eta 0:00:21
epoch [80/200] batch [10/49] time 0.569 (0.488) data 0.438 (0.357) loss_x loss_x 1.0264 (1.0863) acc_x 78.1250 (75.0000) lr 1.3387e-03 eta 0:00:19
epoch [80/200] batch [15/49] time 0.523 (0.481) data 0.391 (0.350) loss_x loss_x 1.6426 (1.1340) acc_x 62.5000 (74.7917) lr 1.3387e-03 eta 0:00:16
epoch [80/200] batch [20/49] time 0.499 (0.464) data 0.369 (0.333) loss_x loss_x 1.3564 (1.1350) acc_x 68.7500 (74.0625) lr 1.3387e-03 eta 0:00:13
epoch [80/200] batch [25/49] time 0.569 (0.467) data 0.438 (0.336) loss_x loss_x 1.0684 (1.1482) acc_x 68.7500 (73.7500) lr 1.3387e-03 eta 0:00:11
epoch [80/200] batch [30/49] time 0.431 (0.466) data 0.300 (0.335) loss_x loss_x 1.0391 (1.1466) acc_x 71.8750 (73.7500) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [35/49] time 0.594 (0.464) data 0.462 (0.333) loss_x loss_x 1.0117 (1.1763) acc_x 81.2500 (73.1250) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [40/49] time 0.617 (0.466) data 0.484 (0.334) loss_x loss_x 1.0625 (1.1945) acc_x 65.6250 (72.1094) lr 1.3387e-03 eta 0:00:04
epoch [80/200] batch [45/49] time 0.387 (0.467) data 0.256 (0.336) loss_x loss_x 1.4414 (1.2020) acc_x 71.8750 (71.7361) lr 1.3387e-03 eta 0:00:01
epoch [80/200] batch [5/48] time 0.572 (0.472) data 0.439 (0.341) loss_u loss_u 0.7422 (0.7410) acc_u 37.5000 (34.3750) lr 1.3387e-03 eta 0:00:20
epoch [80/200] batch [10/48] time 0.444 (0.468) data 0.312 (0.337) loss_u loss_u 0.8037 (0.7480) acc_u 28.1250 (33.4375) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [15/48] time 0.443 (0.465) data 0.311 (0.334) loss_u loss_u 0.8335 (0.7640) acc_u 21.8750 (31.0417) lr 1.3387e-03 eta 0:00:15
epoch [80/200] batch [20/48] time 0.755 (0.470) data 0.623 (0.338) loss_u loss_u 0.8081 (0.7703) acc_u 21.8750 (30.1562) lr 1.3387e-03 eta 0:00:13
epoch [80/200] batch [25/48] time 0.430 (0.466) data 0.299 (0.335) loss_u loss_u 0.8032 (0.7754) acc_u 25.0000 (29.5000) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [30/48] time 0.444 (0.463) data 0.312 (0.332) loss_u loss_u 0.7822 (0.7734) acc_u 28.1250 (29.2708) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [35/48] time 0.387 (0.463) data 0.256 (0.332) loss_u loss_u 0.7026 (0.7693) acc_u 37.5000 (29.5536) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [40/48] time 0.430 (0.461) data 0.298 (0.330) loss_u loss_u 0.8667 (0.7729) acc_u 18.7500 (28.9844) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [45/48] time 0.390 (0.460) data 0.259 (0.328) loss_u loss_u 0.7261 (0.7678) acc_u 34.3750 (29.4444) lr 1.3387e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1346
confident_label rate tensor(0.5092, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1597
clean true:1596
clean false:1
clean_rate:0.9993738259236068
noisy true:194
noisy false:1345
after delete: len(clean_dataset) 1597
after delete: len(noisy_dataset) 1539
epoch [81/200] batch [5/49] time 0.554 (0.476) data 0.422 (0.344) loss_x loss_x 1.7695 (1.3189) acc_x 59.3750 (67.5000) lr 1.3239e-03 eta 0:00:20
epoch [81/200] batch [10/49] time 0.466 (0.480) data 0.335 (0.348) loss_x loss_x 1.3057 (1.2951) acc_x 71.8750 (70.3125) lr 1.3239e-03 eta 0:00:18
epoch [81/200] batch [15/49] time 0.405 (0.460) data 0.274 (0.329) loss_x loss_x 0.8228 (1.2711) acc_x 84.3750 (69.5833) lr 1.3239e-03 eta 0:00:15
epoch [81/200] batch [20/49] time 0.380 (0.457) data 0.249 (0.326) loss_x loss_x 0.7554 (1.2570) acc_x 84.3750 (70.0000) lr 1.3239e-03 eta 0:00:13
epoch [81/200] batch [25/49] time 0.555 (0.463) data 0.424 (0.332) loss_x loss_x 1.2432 (1.2911) acc_x 68.7500 (69.1250) lr 1.3239e-03 eta 0:00:11
epoch [81/200] batch [30/49] time 0.437 (0.472) data 0.305 (0.341) loss_x loss_x 1.2451 (1.2799) acc_x 71.8750 (69.0625) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [35/49] time 0.420 (0.462) data 0.288 (0.331) loss_x loss_x 1.0146 (1.2841) acc_x 78.1250 (68.7500) lr 1.3239e-03 eta 0:00:06
epoch [81/200] batch [40/49] time 0.348 (0.462) data 0.216 (0.331) loss_x loss_x 1.0811 (1.2617) acc_x 75.0000 (69.3750) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [45/49] time 0.379 (0.466) data 0.248 (0.334) loss_x loss_x 1.7021 (1.2720) acc_x 59.3750 (68.9583) lr 1.3239e-03 eta 0:00:01
epoch [81/200] batch [5/48] time 0.325 (0.459) data 0.193 (0.327) loss_u loss_u 0.7227 (0.7674) acc_u 46.8750 (31.2500) lr 1.3239e-03 eta 0:00:19
epoch [81/200] batch [10/48] time 0.524 (0.464) data 0.393 (0.333) loss_u loss_u 0.7866 (0.7942) acc_u 31.2500 (26.8750) lr 1.3239e-03 eta 0:00:17
epoch [81/200] batch [15/48] time 0.468 (0.465) data 0.337 (0.333) loss_u loss_u 0.6665 (0.7896) acc_u 43.7500 (27.9167) lr 1.3239e-03 eta 0:00:15
epoch [81/200] batch [20/48] time 0.485 (0.466) data 0.353 (0.335) loss_u loss_u 0.8486 (0.7840) acc_u 18.7500 (28.4375) lr 1.3239e-03 eta 0:00:13
epoch [81/200] batch [25/48] time 0.430 (0.470) data 0.298 (0.338) loss_u loss_u 0.7251 (0.7818) acc_u 43.7500 (29.0000) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [30/48] time 0.658 (0.473) data 0.527 (0.342) loss_u loss_u 0.7930 (0.7830) acc_u 25.0000 (28.3333) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [35/48] time 0.515 (0.469) data 0.384 (0.338) loss_u loss_u 0.6978 (0.7802) acc_u 37.5000 (28.7500) lr 1.3239e-03 eta 0:00:06
epoch [81/200] batch [40/48] time 0.420 (0.470) data 0.289 (0.338) loss_u loss_u 0.6855 (0.7751) acc_u 40.6250 (29.3750) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [45/48] time 0.503 (0.469) data 0.371 (0.338) loss_u loss_u 0.7861 (0.7707) acc_u 25.0000 (29.7222) lr 1.3239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.4930, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1546
clean true:1545
clean false:1
clean_rate:0.999353169469599
noisy true:171
noisy false:1419
after delete: len(clean_dataset) 1546
after delete: len(noisy_dataset) 1590
epoch [82/200] batch [5/48] time 0.386 (0.462) data 0.254 (0.331) loss_x loss_x 1.6025 (1.4320) acc_x 62.5000 (62.5000) lr 1.3090e-03 eta 0:00:19
epoch [82/200] batch [10/48] time 0.519 (0.478) data 0.388 (0.347) loss_x loss_x 1.1367 (1.2189) acc_x 78.1250 (70.0000) lr 1.3090e-03 eta 0:00:18
epoch [82/200] batch [15/48] time 0.552 (0.467) data 0.421 (0.336) loss_x loss_x 0.5830 (1.1367) acc_x 84.3750 (70.6250) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [20/48] time 0.447 (0.462) data 0.316 (0.331) loss_x loss_x 1.3154 (1.1635) acc_x 71.8750 (70.6250) lr 1.3090e-03 eta 0:00:12
epoch [82/200] batch [25/48] time 0.534 (0.465) data 0.403 (0.334) loss_x loss_x 1.0684 (1.1891) acc_x 71.8750 (70.0000) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [30/48] time 0.393 (0.456) data 0.262 (0.325) loss_x loss_x 1.2568 (1.2149) acc_x 75.0000 (69.2708) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [35/48] time 0.529 (0.453) data 0.398 (0.322) loss_x loss_x 1.2197 (1.2061) acc_x 75.0000 (69.6429) lr 1.3090e-03 eta 0:00:05
epoch [82/200] batch [40/48] time 0.434 (0.460) data 0.302 (0.329) loss_x loss_x 1.3730 (1.2061) acc_x 68.7500 (70.0000) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [45/48] time 0.492 (0.458) data 0.361 (0.327) loss_x loss_x 1.1709 (1.1976) acc_x 71.8750 (70.4861) lr 1.3090e-03 eta 0:00:01
epoch [82/200] batch [5/49] time 0.544 (0.462) data 0.411 (0.331) loss_u loss_u 0.7754 (0.7825) acc_u 25.0000 (27.5000) lr 1.3090e-03 eta 0:00:20
epoch [82/200] batch [10/49] time 0.451 (0.465) data 0.319 (0.333) loss_u loss_u 0.8267 (0.7770) acc_u 28.1250 (28.1250) lr 1.3090e-03 eta 0:00:18
epoch [82/200] batch [15/49] time 0.397 (0.457) data 0.265 (0.326) loss_u loss_u 0.7705 (0.7819) acc_u 28.1250 (26.6667) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [20/49] time 0.409 (0.455) data 0.277 (0.323) loss_u loss_u 0.6650 (0.7744) acc_u 40.6250 (27.9688) lr 1.3090e-03 eta 0:00:13
epoch [82/200] batch [25/49] time 0.549 (0.454) data 0.417 (0.323) loss_u loss_u 0.7905 (0.7705) acc_u 31.2500 (28.6250) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [30/49] time 0.440 (0.456) data 0.309 (0.325) loss_u loss_u 0.7754 (0.7719) acc_u 25.0000 (28.4375) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [35/49] time 0.482 (0.457) data 0.351 (0.325) loss_u loss_u 0.8169 (0.7717) acc_u 28.1250 (28.9286) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [40/49] time 0.652 (0.456) data 0.521 (0.325) loss_u loss_u 0.6411 (0.7676) acc_u 40.6250 (29.1406) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [45/49] time 0.425 (0.460) data 0.291 (0.329) loss_u loss_u 0.7617 (0.7682) acc_u 28.1250 (28.8889) lr 1.3090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1390
confident_label rate tensor(0.5003, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1569
clean true:1568
clean false:1
clean_rate:0.9993626513702996
noisy true:178
noisy false:1389
after delete: len(clean_dataset) 1569
after delete: len(noisy_dataset) 1567
epoch [83/200] batch [5/49] time 0.457 (0.507) data 0.326 (0.376) loss_x loss_x 1.3594 (1.1159) acc_x 59.3750 (68.7500) lr 1.2940e-03 eta 0:00:22
epoch [83/200] batch [10/49] time 0.455 (0.500) data 0.324 (0.369) loss_x loss_x 1.2480 (1.0378) acc_x 56.2500 (72.1875) lr 1.2940e-03 eta 0:00:19
epoch [83/200] batch [15/49] time 0.541 (0.499) data 0.410 (0.368) loss_x loss_x 1.3213 (1.0961) acc_x 71.8750 (72.2917) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [20/49] time 0.465 (0.498) data 0.335 (0.367) loss_x loss_x 1.4580 (1.1544) acc_x 62.5000 (70.7812) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [25/49] time 0.449 (0.492) data 0.316 (0.361) loss_x loss_x 1.5693 (1.1540) acc_x 53.1250 (70.6250) lr 1.2940e-03 eta 0:00:11
epoch [83/200] batch [30/49] time 0.441 (0.500) data 0.310 (0.369) loss_x loss_x 0.9663 (1.1744) acc_x 75.0000 (70.2083) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [35/49] time 0.482 (0.501) data 0.351 (0.370) loss_x loss_x 1.4932 (1.1796) acc_x 59.3750 (70.2679) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [40/49] time 0.513 (0.499) data 0.378 (0.368) loss_x loss_x 0.7554 (1.1483) acc_x 75.0000 (70.8594) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [45/49] time 0.454 (0.495) data 0.323 (0.364) loss_x loss_x 0.7510 (1.1482) acc_x 81.2500 (70.7639) lr 1.2940e-03 eta 0:00:01
epoch [83/200] batch [5/48] time 0.428 (0.489) data 0.297 (0.358) loss_u loss_u 0.7808 (0.7754) acc_u 37.5000 (31.2500) lr 1.2940e-03 eta 0:00:21
epoch [83/200] batch [10/48] time 0.495 (0.485) data 0.362 (0.353) loss_u loss_u 0.7231 (0.7782) acc_u 34.3750 (30.6250) lr 1.2940e-03 eta 0:00:18
epoch [83/200] batch [15/48] time 0.820 (0.487) data 0.686 (0.355) loss_u loss_u 0.7749 (0.7868) acc_u 28.1250 (28.7500) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [20/48] time 0.440 (0.485) data 0.307 (0.353) loss_u loss_u 0.7798 (0.7891) acc_u 28.1250 (28.2812) lr 1.2940e-03 eta 0:00:13
epoch [83/200] batch [25/48] time 0.479 (0.485) data 0.347 (0.353) loss_u loss_u 0.7612 (0.7864) acc_u 31.2500 (28.2500) lr 1.2940e-03 eta 0:00:11
epoch [83/200] batch [30/48] time 0.480 (0.481) data 0.348 (0.349) loss_u loss_u 0.6577 (0.7831) acc_u 43.7500 (28.3333) lr 1.2940e-03 eta 0:00:08
epoch [83/200] batch [35/48] time 0.523 (0.483) data 0.392 (0.351) loss_u loss_u 0.7422 (0.7751) acc_u 37.5000 (29.1071) lr 1.2940e-03 eta 0:00:06
epoch [83/200] batch [40/48] time 0.376 (0.480) data 0.244 (0.348) loss_u loss_u 0.8628 (0.7812) acc_u 18.7500 (28.4375) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [45/48] time 0.408 (0.477) data 0.278 (0.345) loss_u loss_u 0.7549 (0.7787) acc_u 34.3750 (28.7500) lr 1.2940e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1368
confident_label rate tensor(0.5115, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1604
clean true:1603
clean false:1
clean_rate:0.9993765586034913
noisy true:165
noisy false:1367
after delete: len(clean_dataset) 1604
after delete: len(noisy_dataset) 1532
epoch [84/200] batch [5/50] time 0.427 (0.456) data 0.296 (0.325) loss_x loss_x 1.3857 (1.3551) acc_x 65.6250 (65.6250) lr 1.2790e-03 eta 0:00:20
epoch [84/200] batch [10/50] time 0.577 (0.477) data 0.446 (0.346) loss_x loss_x 1.8535 (1.3300) acc_x 56.2500 (66.2500) lr 1.2790e-03 eta 0:00:19
epoch [84/200] batch [15/50] time 0.437 (0.482) data 0.306 (0.351) loss_x loss_x 1.0137 (1.2594) acc_x 78.1250 (68.3333) lr 1.2790e-03 eta 0:00:16
epoch [84/200] batch [20/50] time 0.375 (0.477) data 0.244 (0.346) loss_x loss_x 1.2646 (1.2437) acc_x 59.3750 (68.7500) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [25/50] time 0.398 (0.467) data 0.267 (0.336) loss_x loss_x 0.9751 (1.2310) acc_x 62.5000 (67.8750) lr 1.2790e-03 eta 0:00:11
epoch [84/200] batch [30/50] time 0.346 (0.465) data 0.214 (0.334) loss_x loss_x 1.3516 (1.2295) acc_x 65.6250 (68.1250) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [35/50] time 0.458 (0.469) data 0.328 (0.338) loss_x loss_x 1.2891 (1.2520) acc_x 62.5000 (67.1429) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [40/50] time 0.443 (0.468) data 0.311 (0.337) loss_x loss_x 1.0029 (1.2372) acc_x 78.1250 (67.5781) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [45/50] time 0.492 (0.477) data 0.361 (0.346) loss_x loss_x 0.7354 (1.2148) acc_x 78.1250 (68.0556) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [50/50] time 0.402 (0.474) data 0.271 (0.343) loss_x loss_x 1.1914 (1.2295) acc_x 71.8750 (68.0000) lr 1.2790e-03 eta 0:00:00
epoch [84/200] batch [5/47] time 0.530 (0.473) data 0.399 (0.342) loss_u loss_u 0.7056 (0.7430) acc_u 37.5000 (30.6250) lr 1.2790e-03 eta 0:00:19
epoch [84/200] batch [10/47] time 0.428 (0.472) data 0.295 (0.340) loss_u loss_u 0.7651 (0.7840) acc_u 31.2500 (25.9375) lr 1.2790e-03 eta 0:00:17
epoch [84/200] batch [15/47] time 0.407 (0.466) data 0.275 (0.335) loss_u loss_u 0.7979 (0.7725) acc_u 25.0000 (27.5000) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [20/47] time 0.433 (0.467) data 0.302 (0.335) loss_u loss_u 0.8574 (0.7844) acc_u 12.5000 (26.2500) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [25/47] time 0.422 (0.467) data 0.291 (0.336) loss_u loss_u 0.7495 (0.7741) acc_u 28.1250 (27.2500) lr 1.2790e-03 eta 0:00:10
epoch [84/200] batch [30/47] time 0.330 (0.467) data 0.198 (0.335) loss_u loss_u 0.7773 (0.7759) acc_u 37.5000 (27.6042) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [35/47] time 0.469 (0.472) data 0.336 (0.340) loss_u loss_u 0.7637 (0.7747) acc_u 31.2500 (28.3036) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [40/47] time 0.533 (0.470) data 0.401 (0.339) loss_u loss_u 0.7065 (0.7715) acc_u 40.6250 (28.4375) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [45/47] time 0.498 (0.470) data 0.366 (0.338) loss_u loss_u 0.6548 (0.7710) acc_u 43.7500 (28.4722) lr 1.2790e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1397
confident_label rate tensor(0.4994, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1566
clean true:1565
clean false:1
clean_rate:0.9993614303959132
noisy true:174
noisy false:1396
after delete: len(clean_dataset) 1566
after delete: len(noisy_dataset) 1570
epoch [85/200] batch [5/48] time 0.455 (0.452) data 0.324 (0.320) loss_x loss_x 1.3633 (0.9383) acc_x 71.8750 (76.8750) lr 1.2639e-03 eta 0:00:19
epoch [85/200] batch [10/48] time 0.485 (0.462) data 0.354 (0.331) loss_x loss_x 1.0332 (0.9307) acc_x 78.1250 (77.1875) lr 1.2639e-03 eta 0:00:17
epoch [85/200] batch [15/48] time 0.509 (0.470) data 0.378 (0.339) loss_x loss_x 1.3525 (0.9974) acc_x 65.6250 (75.8333) lr 1.2639e-03 eta 0:00:15
epoch [85/200] batch [20/48] time 0.807 (0.483) data 0.676 (0.352) loss_x loss_x 0.8867 (1.0630) acc_x 71.8750 (73.7500) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [25/48] time 0.559 (0.494) data 0.426 (0.363) loss_x loss_x 1.2383 (1.0673) acc_x 71.8750 (73.7500) lr 1.2639e-03 eta 0:00:11
epoch [85/200] batch [30/48] time 0.463 (0.484) data 0.332 (0.353) loss_x loss_x 1.0938 (1.0760) acc_x 71.8750 (73.0208) lr 1.2639e-03 eta 0:00:08
epoch [85/200] batch [35/48] time 0.410 (0.486) data 0.279 (0.355) loss_x loss_x 1.0117 (1.0702) acc_x 78.1250 (73.2143) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [40/48] time 0.494 (0.488) data 0.362 (0.357) loss_x loss_x 0.8726 (1.0687) acc_x 84.3750 (73.5156) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [45/48] time 0.391 (0.488) data 0.260 (0.356) loss_x loss_x 1.2246 (1.0856) acc_x 75.0000 (72.8472) lr 1.2639e-03 eta 0:00:01
epoch [85/200] batch [5/49] time 0.339 (0.484) data 0.206 (0.353) loss_u loss_u 0.7505 (0.7711) acc_u 28.1250 (26.8750) lr 1.2639e-03 eta 0:00:21
epoch [85/200] batch [10/49] time 0.348 (0.476) data 0.217 (0.344) loss_u loss_u 0.6782 (0.7599) acc_u 50.0000 (32.1875) lr 1.2639e-03 eta 0:00:18
epoch [85/200] batch [15/49] time 0.482 (0.473) data 0.351 (0.342) loss_u loss_u 0.7690 (0.7662) acc_u 34.3750 (31.2500) lr 1.2639e-03 eta 0:00:16
epoch [85/200] batch [20/49] time 0.407 (0.468) data 0.275 (0.336) loss_u loss_u 0.7437 (0.7549) acc_u 31.2500 (32.5000) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [25/49] time 0.481 (0.476) data 0.349 (0.345) loss_u loss_u 0.7715 (0.7608) acc_u 31.2500 (31.2500) lr 1.2639e-03 eta 0:00:11
epoch [85/200] batch [30/49] time 0.534 (0.478) data 0.401 (0.347) loss_u loss_u 0.7632 (0.7622) acc_u 28.1250 (30.7292) lr 1.2639e-03 eta 0:00:09
epoch [85/200] batch [35/49] time 0.498 (0.477) data 0.367 (0.345) loss_u loss_u 0.7383 (0.7596) acc_u 37.5000 (30.7143) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [40/49] time 0.471 (0.476) data 0.339 (0.344) loss_u loss_u 0.8384 (0.7664) acc_u 21.8750 (29.7656) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [45/49] time 0.440 (0.474) data 0.309 (0.342) loss_u loss_u 0.8481 (0.7692) acc_u 21.8750 (29.4444) lr 1.2639e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1366
confident_label rate tensor(0.5048, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1583
clean true:1580
clean false:3
clean_rate:0.998104864181933
noisy true:190
noisy false:1363
after delete: len(clean_dataset) 1583
after delete: len(noisy_dataset) 1553
epoch [86/200] batch [5/49] time 0.525 (0.485) data 0.394 (0.354) loss_x loss_x 1.6270 (1.2553) acc_x 59.3750 (68.1250) lr 1.2487e-03 eta 0:00:21
epoch [86/200] batch [10/49] time 0.481 (0.473) data 0.350 (0.342) loss_x loss_x 0.9736 (1.1595) acc_x 71.8750 (70.3125) lr 1.2487e-03 eta 0:00:18
epoch [86/200] batch [15/49] time 0.396 (0.474) data 0.265 (0.343) loss_x loss_x 1.2441 (1.1721) acc_x 65.6250 (70.4167) lr 1.2487e-03 eta 0:00:16
epoch [86/200] batch [20/49] time 0.436 (0.470) data 0.305 (0.339) loss_x loss_x 1.3896 (1.1560) acc_x 71.8750 (71.5625) lr 1.2487e-03 eta 0:00:13
epoch [86/200] batch [25/49] time 0.560 (0.478) data 0.429 (0.347) loss_x loss_x 1.0996 (1.1443) acc_x 71.8750 (71.7500) lr 1.2487e-03 eta 0:00:11
epoch [86/200] batch [30/49] time 0.514 (0.472) data 0.383 (0.341) loss_x loss_x 1.1035 (1.1298) acc_x 65.6250 (71.2500) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [35/49] time 0.466 (0.473) data 0.335 (0.342) loss_x loss_x 1.5928 (1.1213) acc_x 62.5000 (71.3393) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [40/49] time 0.526 (0.481) data 0.395 (0.349) loss_x loss_x 1.1562 (1.1247) acc_x 78.1250 (71.7188) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [45/49] time 0.489 (0.481) data 0.357 (0.350) loss_x loss_x 1.4648 (1.1350) acc_x 65.6250 (71.3889) lr 1.2487e-03 eta 0:00:01
epoch [86/200] batch [5/48] time 0.397 (0.473) data 0.266 (0.341) loss_u loss_u 0.8066 (0.7783) acc_u 25.0000 (29.3750) lr 1.2487e-03 eta 0:00:20
epoch [86/200] batch [10/48] time 0.398 (0.479) data 0.267 (0.348) loss_u loss_u 0.7432 (0.7867) acc_u 25.0000 (27.5000) lr 1.2487e-03 eta 0:00:18
epoch [86/200] batch [15/48] time 0.480 (0.476) data 0.348 (0.345) loss_u loss_u 0.8047 (0.7776) acc_u 28.1250 (28.5417) lr 1.2487e-03 eta 0:00:15
epoch [86/200] batch [20/48] time 0.504 (0.473) data 0.372 (0.342) loss_u loss_u 0.6904 (0.7782) acc_u 43.7500 (28.4375) lr 1.2487e-03 eta 0:00:13
epoch [86/200] batch [25/48] time 0.385 (0.471) data 0.252 (0.340) loss_u loss_u 0.7373 (0.7775) acc_u 37.5000 (28.2500) lr 1.2487e-03 eta 0:00:10
epoch [86/200] batch [30/48] time 0.398 (0.469) data 0.267 (0.338) loss_u loss_u 0.7456 (0.7665) acc_u 31.2500 (29.5833) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [35/48] time 0.416 (0.469) data 0.284 (0.337) loss_u loss_u 0.7314 (0.7710) acc_u 31.2500 (29.1964) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [40/48] time 0.512 (0.467) data 0.379 (0.336) loss_u loss_u 0.7437 (0.7674) acc_u 34.3750 (29.4531) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [45/48] time 0.373 (0.467) data 0.241 (0.335) loss_u loss_u 0.8105 (0.7700) acc_u 31.2500 (29.1667) lr 1.2487e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1351
confident_label rate tensor(0.5182, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1625
clean true:1623
clean false:2
clean_rate:0.9987692307692307
noisy true:162
noisy false:1349
after delete: len(clean_dataset) 1625
after delete: len(noisy_dataset) 1511
epoch [87/200] batch [5/50] time 0.592 (0.550) data 0.462 (0.419) loss_x loss_x 1.8594 (1.1229) acc_x 65.6250 (75.0000) lr 1.2334e-03 eta 0:00:24
epoch [87/200] batch [10/50] time 0.488 (0.512) data 0.358 (0.382) loss_x loss_x 1.7363 (1.2771) acc_x 65.6250 (70.3125) lr 1.2334e-03 eta 0:00:20
epoch [87/200] batch [15/50] time 0.441 (0.497) data 0.311 (0.366) loss_x loss_x 1.0342 (1.2093) acc_x 68.7500 (70.8333) lr 1.2334e-03 eta 0:00:17
epoch [87/200] batch [20/50] time 0.391 (0.486) data 0.261 (0.355) loss_x loss_x 0.8447 (1.1596) acc_x 81.2500 (72.1875) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [25/50] time 0.504 (0.495) data 0.373 (0.364) loss_x loss_x 1.2861 (1.1571) acc_x 59.3750 (71.8750) lr 1.2334e-03 eta 0:00:12
epoch [87/200] batch [30/50] time 0.444 (0.494) data 0.313 (0.363) loss_x loss_x 0.8843 (1.1268) acc_x 68.7500 (72.2917) lr 1.2334e-03 eta 0:00:09
epoch [87/200] batch [35/50] time 0.468 (0.493) data 0.337 (0.362) loss_x loss_x 1.0732 (1.1283) acc_x 68.7500 (71.9643) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [40/50] time 0.510 (0.494) data 0.379 (0.363) loss_x loss_x 1.5039 (1.1386) acc_x 59.3750 (71.5625) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [45/50] time 0.459 (0.493) data 0.329 (0.362) loss_x loss_x 1.1611 (1.1330) acc_x 62.5000 (71.3889) lr 1.2334e-03 eta 0:00:02
epoch [87/200] batch [50/50] time 0.491 (0.496) data 0.360 (0.364) loss_x loss_x 1.6572 (1.1516) acc_x 59.3750 (71.2500) lr 1.2334e-03 eta 0:00:00
epoch [87/200] batch [5/47] time 0.420 (0.487) data 0.287 (0.356) loss_u loss_u 0.8452 (0.8042) acc_u 21.8750 (23.1250) lr 1.2334e-03 eta 0:00:20
epoch [87/200] batch [10/47] time 0.518 (0.485) data 0.386 (0.353) loss_u loss_u 0.7749 (0.7852) acc_u 31.2500 (25.9375) lr 1.2334e-03 eta 0:00:17
epoch [87/200] batch [15/47] time 0.418 (0.477) data 0.287 (0.346) loss_u loss_u 0.7974 (0.7804) acc_u 28.1250 (27.9167) lr 1.2334e-03 eta 0:00:15
epoch [87/200] batch [20/47] time 0.411 (0.474) data 0.280 (0.343) loss_u loss_u 0.8032 (0.7756) acc_u 25.0000 (28.1250) lr 1.2334e-03 eta 0:00:12
epoch [87/200] batch [25/47] time 0.517 (0.473) data 0.386 (0.342) loss_u loss_u 0.7744 (0.7702) acc_u 31.2500 (29.3750) lr 1.2334e-03 eta 0:00:10
epoch [87/200] batch [30/47] time 0.553 (0.477) data 0.422 (0.345) loss_u loss_u 0.9180 (0.7768) acc_u 9.3750 (27.9167) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [35/47] time 0.388 (0.477) data 0.256 (0.346) loss_u loss_u 0.8120 (0.7802) acc_u 28.1250 (28.0357) lr 1.2334e-03 eta 0:00:05
epoch [87/200] batch [40/47] time 0.457 (0.476) data 0.326 (0.345) loss_u loss_u 0.8062 (0.7852) acc_u 25.0000 (27.2656) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [45/47] time 0.340 (0.478) data 0.208 (0.346) loss_u loss_u 0.8208 (0.7878) acc_u 25.0000 (27.0833) lr 1.2334e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1370
confident_label rate tensor(0.5070, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1590
clean true:1589
clean false:1
clean_rate:0.9993710691823899
noisy true:177
noisy false:1369
after delete: len(clean_dataset) 1590
after delete: len(noisy_dataset) 1546
epoch [88/200] batch [5/49] time 0.486 (0.483) data 0.353 (0.351) loss_x loss_x 1.1270 (1.2468) acc_x 65.6250 (69.3750) lr 1.2181e-03 eta 0:00:21
epoch [88/200] batch [10/49] time 0.460 (0.459) data 0.329 (0.328) loss_x loss_x 1.0537 (1.3381) acc_x 71.8750 (66.5625) lr 1.2181e-03 eta 0:00:17
epoch [88/200] batch [15/49] time 0.452 (0.449) data 0.321 (0.318) loss_x loss_x 0.9883 (1.3596) acc_x 78.1250 (67.7083) lr 1.2181e-03 eta 0:00:15
epoch [88/200] batch [20/49] time 0.417 (0.445) data 0.286 (0.313) loss_x loss_x 0.7559 (1.2577) acc_x 81.2500 (69.5312) lr 1.2181e-03 eta 0:00:12
epoch [88/200] batch [25/49] time 0.505 (0.442) data 0.373 (0.310) loss_x loss_x 1.0625 (1.2784) acc_x 68.7500 (68.3750) lr 1.2181e-03 eta 0:00:10
epoch [88/200] batch [30/49] time 0.482 (0.455) data 0.351 (0.323) loss_x loss_x 1.1865 (1.2730) acc_x 71.8750 (68.2292) lr 1.2181e-03 eta 0:00:08
epoch [88/200] batch [35/49] time 0.448 (0.450) data 0.317 (0.319) loss_x loss_x 0.7676 (1.2463) acc_x 84.3750 (68.6607) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [40/49] time 0.394 (0.447) data 0.262 (0.315) loss_x loss_x 0.8589 (1.2261) acc_x 68.7500 (68.6719) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [45/49] time 0.536 (0.457) data 0.404 (0.326) loss_x loss_x 1.0381 (1.2144) acc_x 78.1250 (69.0972) lr 1.2181e-03 eta 0:00:01
epoch [88/200] batch [5/48] time 0.529 (0.451) data 0.399 (0.319) loss_u loss_u 0.7397 (0.7745) acc_u 34.3750 (26.8750) lr 1.2181e-03 eta 0:00:19
epoch [88/200] batch [10/48] time 0.410 (0.455) data 0.278 (0.324) loss_u loss_u 0.7661 (0.7686) acc_u 28.1250 (28.7500) lr 1.2181e-03 eta 0:00:17
epoch [88/200] batch [15/48] time 0.445 (0.456) data 0.314 (0.324) loss_u loss_u 0.7539 (0.7695) acc_u 28.1250 (28.1250) lr 1.2181e-03 eta 0:00:15
epoch [88/200] batch [20/48] time 0.509 (0.455) data 0.378 (0.323) loss_u loss_u 0.9146 (0.7816) acc_u 9.3750 (26.7188) lr 1.2181e-03 eta 0:00:12
epoch [88/200] batch [25/48] time 0.587 (0.455) data 0.455 (0.323) loss_u loss_u 0.7817 (0.7746) acc_u 31.2500 (27.8750) lr 1.2181e-03 eta 0:00:10
epoch [88/200] batch [30/48] time 0.709 (0.458) data 0.576 (0.326) loss_u loss_u 0.7544 (0.7750) acc_u 34.3750 (28.0208) lr 1.2181e-03 eta 0:00:08
epoch [88/200] batch [35/48] time 0.392 (0.455) data 0.260 (0.323) loss_u loss_u 0.7603 (0.7692) acc_u 31.2500 (28.8393) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [40/48] time 0.427 (0.454) data 0.295 (0.323) loss_u loss_u 0.7700 (0.7743) acc_u 31.2500 (28.6719) lr 1.2181e-03 eta 0:00:03
epoch [88/200] batch [45/48] time 0.409 (0.453) data 0.276 (0.322) loss_u loss_u 0.8135 (0.7741) acc_u 18.7500 (28.6806) lr 1.2181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1339
confident_label rate tensor(0.5128, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1608
clean true:1607
clean false:1
clean_rate:0.9993781094527363
noisy true:190
noisy false:1338
after delete: len(clean_dataset) 1608
after delete: len(noisy_dataset) 1528
epoch [89/200] batch [5/50] time 0.489 (0.522) data 0.359 (0.390) loss_x loss_x 1.6045 (1.1287) acc_x 68.7500 (73.1250) lr 1.2028e-03 eta 0:00:23
epoch [89/200] batch [10/50] time 0.523 (0.515) data 0.392 (0.383) loss_x loss_x 1.0020 (1.1910) acc_x 75.0000 (71.5625) lr 1.2028e-03 eta 0:00:20
epoch [89/200] batch [15/50] time 0.479 (0.485) data 0.348 (0.354) loss_x loss_x 0.6821 (1.1828) acc_x 84.3750 (72.0833) lr 1.2028e-03 eta 0:00:16
epoch [89/200] batch [20/50] time 0.475 (0.484) data 0.345 (0.353) loss_x loss_x 1.3691 (1.2058) acc_x 59.3750 (70.9375) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [25/50] time 0.475 (0.486) data 0.343 (0.354) loss_x loss_x 0.9785 (1.1581) acc_x 68.7500 (72.1250) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [30/50] time 0.546 (0.489) data 0.415 (0.358) loss_x loss_x 0.9800 (1.1826) acc_x 68.7500 (70.8333) lr 1.2028e-03 eta 0:00:09
epoch [89/200] batch [35/50] time 0.495 (0.484) data 0.364 (0.353) loss_x loss_x 0.7998 (1.1942) acc_x 81.2500 (70.7143) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [40/50] time 0.423 (0.481) data 0.292 (0.350) loss_x loss_x 1.2764 (1.1855) acc_x 68.7500 (70.3906) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [45/50] time 0.382 (0.474) data 0.252 (0.343) loss_x loss_x 1.1133 (1.2003) acc_x 75.0000 (69.9306) lr 1.2028e-03 eta 0:00:02
epoch [89/200] batch [50/50] time 0.409 (0.471) data 0.278 (0.340) loss_x loss_x 1.2979 (1.2061) acc_x 65.6250 (69.2500) lr 1.2028e-03 eta 0:00:00
epoch [89/200] batch [5/47] time 0.429 (0.469) data 0.297 (0.338) loss_u loss_u 0.7861 (0.7771) acc_u 28.1250 (28.1250) lr 1.2028e-03 eta 0:00:19
epoch [89/200] batch [10/47] time 0.516 (0.465) data 0.384 (0.334) loss_u loss_u 0.8750 (0.7835) acc_u 15.6250 (27.5000) lr 1.2028e-03 eta 0:00:17
epoch [89/200] batch [15/47] time 0.431 (0.466) data 0.299 (0.335) loss_u loss_u 0.7266 (0.7685) acc_u 34.3750 (30.0000) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [20/47] time 0.557 (0.474) data 0.425 (0.343) loss_u loss_u 0.7798 (0.7647) acc_u 21.8750 (29.0625) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [25/47] time 0.381 (0.469) data 0.249 (0.337) loss_u loss_u 0.7446 (0.7668) acc_u 34.3750 (29.5000) lr 1.2028e-03 eta 0:00:10
epoch [89/200] batch [30/47] time 0.443 (0.470) data 0.310 (0.338) loss_u loss_u 0.7773 (0.7700) acc_u 21.8750 (28.5417) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [35/47] time 0.660 (0.469) data 0.529 (0.338) loss_u loss_u 0.8022 (0.7740) acc_u 25.0000 (28.0357) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [40/47] time 0.466 (0.467) data 0.336 (0.336) loss_u loss_u 0.7769 (0.7733) acc_u 25.0000 (27.9688) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [45/47] time 0.516 (0.465) data 0.384 (0.333) loss_u loss_u 0.8291 (0.7761) acc_u 21.8750 (27.8472) lr 1.2028e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1376
confident_label rate tensor(0.5054, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1585
clean true:1584
clean false:1
clean_rate:0.9993690851735015
noisy true:176
noisy false:1375
after delete: len(clean_dataset) 1585
after delete: len(noisy_dataset) 1551
epoch [90/200] batch [5/49] time 0.426 (0.492) data 0.296 (0.361) loss_x loss_x 1.1260 (1.1008) acc_x 75.0000 (72.5000) lr 1.1874e-03 eta 0:00:21
epoch [90/200] batch [10/49] time 0.498 (0.466) data 0.367 (0.335) loss_x loss_x 1.5449 (1.1634) acc_x 75.0000 (74.3750) lr 1.1874e-03 eta 0:00:18
epoch [90/200] batch [15/49] time 0.417 (0.455) data 0.286 (0.323) loss_x loss_x 1.4668 (1.1251) acc_x 62.5000 (73.7500) lr 1.1874e-03 eta 0:00:15
epoch [90/200] batch [20/49] time 0.496 (0.452) data 0.364 (0.320) loss_x loss_x 0.8037 (1.1556) acc_x 87.5000 (73.1250) lr 1.1874e-03 eta 0:00:13
epoch [90/200] batch [25/49] time 0.436 (0.458) data 0.302 (0.326) loss_x loss_x 1.7188 (1.1687) acc_x 50.0000 (72.0000) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [30/49] time 0.466 (0.452) data 0.335 (0.321) loss_x loss_x 1.4033 (1.1601) acc_x 59.3750 (71.8750) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [35/49] time 0.480 (0.461) data 0.349 (0.329) loss_x loss_x 0.7280 (1.1506) acc_x 78.1250 (72.0536) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [40/49] time 0.461 (0.460) data 0.330 (0.329) loss_x loss_x 0.7124 (1.1411) acc_x 71.8750 (71.6406) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [45/49] time 0.560 (0.460) data 0.429 (0.329) loss_x loss_x 1.1953 (1.1615) acc_x 75.0000 (71.3194) lr 1.1874e-03 eta 0:00:01
epoch [90/200] batch [5/48] time 0.464 (0.461) data 0.332 (0.330) loss_u loss_u 0.6934 (0.6955) acc_u 43.7500 (41.8750) lr 1.1874e-03 eta 0:00:19
epoch [90/200] batch [10/48] time 0.391 (0.459) data 0.258 (0.328) loss_u loss_u 0.6875 (0.7393) acc_u 31.2500 (33.1250) lr 1.1874e-03 eta 0:00:17
epoch [90/200] batch [15/48] time 0.437 (0.459) data 0.304 (0.328) loss_u loss_u 0.7700 (0.7559) acc_u 34.3750 (31.4583) lr 1.1874e-03 eta 0:00:15
epoch [90/200] batch [20/48] time 0.447 (0.457) data 0.315 (0.325) loss_u loss_u 0.8091 (0.7564) acc_u 28.1250 (32.6562) lr 1.1874e-03 eta 0:00:12
epoch [90/200] batch [25/48] time 0.411 (0.453) data 0.278 (0.322) loss_u loss_u 0.7842 (0.7604) acc_u 28.1250 (32.2500) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [30/48] time 0.340 (0.454) data 0.208 (0.322) loss_u loss_u 0.7886 (0.7559) acc_u 28.1250 (32.7083) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [35/48] time 0.319 (0.451) data 0.188 (0.319) loss_u loss_u 0.7544 (0.7605) acc_u 31.2500 (31.8750) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [40/48] time 0.597 (0.451) data 0.465 (0.319) loss_u loss_u 0.8140 (0.7593) acc_u 21.8750 (31.7188) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [45/48] time 0.419 (0.453) data 0.288 (0.322) loss_u loss_u 0.8149 (0.7640) acc_u 31.2500 (31.0417) lr 1.1874e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1378
confident_label rate tensor(0.5045, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1582
clean true:1581
clean false:1
clean_rate:0.9993678887484198
noisy true:177
noisy false:1377
after delete: len(clean_dataset) 1582
after delete: len(noisy_dataset) 1554
epoch [91/200] batch [5/49] time 0.505 (0.479) data 0.374 (0.349) loss_x loss_x 1.0605 (0.9388) acc_x 71.8750 (76.8750) lr 1.1719e-03 eta 0:00:21
epoch [91/200] batch [10/49] time 0.440 (0.475) data 0.309 (0.344) loss_x loss_x 1.0137 (0.9131) acc_x 75.0000 (77.5000) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [15/49] time 0.496 (0.473) data 0.365 (0.342) loss_x loss_x 0.7837 (0.9446) acc_x 81.2500 (76.8750) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [20/49] time 0.391 (0.475) data 0.260 (0.344) loss_x loss_x 1.4062 (0.9849) acc_x 65.6250 (76.0938) lr 1.1719e-03 eta 0:00:13
epoch [91/200] batch [25/49] time 0.441 (0.481) data 0.310 (0.350) loss_x loss_x 1.5596 (1.0198) acc_x 59.3750 (74.8750) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [30/49] time 0.432 (0.481) data 0.301 (0.350) loss_x loss_x 1.4521 (1.0305) acc_x 68.7500 (74.3750) lr 1.1719e-03 eta 0:00:09
epoch [91/200] batch [35/49] time 0.387 (0.473) data 0.257 (0.342) loss_x loss_x 1.0840 (1.0573) acc_x 78.1250 (74.0179) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [40/49] time 0.465 (0.473) data 0.334 (0.342) loss_x loss_x 0.9893 (1.0618) acc_x 81.2500 (74.2188) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [45/49] time 0.413 (0.472) data 0.282 (0.341) loss_x loss_x 1.1396 (1.0846) acc_x 71.8750 (73.1944) lr 1.1719e-03 eta 0:00:01
epoch [91/200] batch [5/48] time 0.342 (0.474) data 0.212 (0.343) loss_u loss_u 0.9243 (0.7854) acc_u 6.2500 (26.8750) lr 1.1719e-03 eta 0:00:20
epoch [91/200] batch [10/48] time 0.452 (0.468) data 0.321 (0.337) loss_u loss_u 0.7153 (0.7732) acc_u 34.3750 (27.5000) lr 1.1719e-03 eta 0:00:17
epoch [91/200] batch [15/48] time 0.533 (0.467) data 0.401 (0.336) loss_u loss_u 0.7207 (0.7713) acc_u 31.2500 (27.5000) lr 1.1719e-03 eta 0:00:15
epoch [91/200] batch [20/48] time 0.466 (0.468) data 0.335 (0.336) loss_u loss_u 0.8179 (0.7711) acc_u 21.8750 (27.8125) lr 1.1719e-03 eta 0:00:13
epoch [91/200] batch [25/48] time 0.581 (0.465) data 0.450 (0.334) loss_u loss_u 0.7744 (0.7724) acc_u 21.8750 (27.6250) lr 1.1719e-03 eta 0:00:10
epoch [91/200] batch [30/48] time 0.542 (0.466) data 0.411 (0.334) loss_u loss_u 0.7158 (0.7661) acc_u 28.1250 (28.6458) lr 1.1719e-03 eta 0:00:08
epoch [91/200] batch [35/48] time 0.322 (0.464) data 0.191 (0.333) loss_u loss_u 0.6953 (0.7621) acc_u 40.6250 (29.2857) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [40/48] time 0.388 (0.466) data 0.257 (0.335) loss_u loss_u 0.7300 (0.7625) acc_u 28.1250 (28.9844) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [45/48] time 0.444 (0.465) data 0.313 (0.334) loss_u loss_u 0.9214 (0.7674) acc_u 12.5000 (28.4028) lr 1.1719e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1360
confident_label rate tensor(0.5150, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1615
clean true:1613
clean false:2
clean_rate:0.9987616099071207
noisy true:163
noisy false:1358
after delete: len(clean_dataset) 1615
after delete: len(noisy_dataset) 1521
epoch [92/200] batch [5/50] time 0.422 (0.416) data 0.291 (0.285) loss_x loss_x 0.7251 (1.3224) acc_x 78.1250 (65.6250) lr 1.1564e-03 eta 0:00:18
epoch [92/200] batch [10/50] time 0.611 (0.469) data 0.479 (0.338) loss_x loss_x 0.7461 (1.0934) acc_x 81.2500 (73.1250) lr 1.1564e-03 eta 0:00:18
epoch [92/200] batch [15/50] time 0.404 (0.462) data 0.272 (0.331) loss_x loss_x 1.4785 (1.1606) acc_x 68.7500 (72.0833) lr 1.1564e-03 eta 0:00:16
epoch [92/200] batch [20/50] time 0.439 (0.464) data 0.308 (0.333) loss_x loss_x 0.7246 (1.1203) acc_x 84.3750 (72.8125) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [25/50] time 0.546 (0.467) data 0.415 (0.336) loss_x loss_x 1.1934 (1.1437) acc_x 65.6250 (72.0000) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [30/50] time 0.513 (0.475) data 0.380 (0.344) loss_x loss_x 1.3340 (1.1401) acc_x 68.7500 (72.2917) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [35/50] time 0.369 (0.476) data 0.239 (0.345) loss_x loss_x 1.1963 (1.1538) acc_x 59.3750 (71.6964) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [40/50] time 0.420 (0.470) data 0.290 (0.339) loss_x loss_x 1.2793 (1.1599) acc_x 68.7500 (71.7188) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [45/50] time 0.465 (0.469) data 0.333 (0.338) loss_x loss_x 1.2148 (1.1958) acc_x 62.5000 (71.0417) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [50/50] time 0.437 (0.467) data 0.307 (0.336) loss_x loss_x 0.7578 (1.2079) acc_x 84.3750 (70.9375) lr 1.1564e-03 eta 0:00:00
epoch [92/200] batch [5/47] time 0.358 (0.462) data 0.227 (0.331) loss_u loss_u 0.7954 (0.7867) acc_u 37.5000 (31.2500) lr 1.1564e-03 eta 0:00:19
epoch [92/200] batch [10/47] time 0.365 (0.456) data 0.233 (0.325) loss_u loss_u 0.7129 (0.7749) acc_u 34.3750 (30.0000) lr 1.1564e-03 eta 0:00:16
epoch [92/200] batch [15/47] time 0.430 (0.457) data 0.297 (0.325) loss_u loss_u 0.7881 (0.7826) acc_u 25.0000 (28.9583) lr 1.1564e-03 eta 0:00:14
epoch [92/200] batch [20/47] time 0.514 (0.459) data 0.382 (0.327) loss_u loss_u 0.7847 (0.7815) acc_u 25.0000 (28.5938) lr 1.1564e-03 eta 0:00:12
epoch [92/200] batch [25/47] time 0.401 (0.457) data 0.270 (0.326) loss_u loss_u 0.8115 (0.7820) acc_u 25.0000 (28.1250) lr 1.1564e-03 eta 0:00:10
epoch [92/200] batch [30/47] time 0.450 (0.460) data 0.318 (0.328) loss_u loss_u 0.7695 (0.7780) acc_u 28.1250 (28.5417) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [35/47] time 0.647 (0.457) data 0.515 (0.326) loss_u loss_u 0.7588 (0.7794) acc_u 28.1250 (28.0357) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [40/47] time 0.391 (0.457) data 0.259 (0.326) loss_u loss_u 0.7046 (0.7790) acc_u 43.7500 (28.8281) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [45/47] time 0.463 (0.455) data 0.333 (0.324) loss_u loss_u 0.7856 (0.7798) acc_u 25.0000 (28.4722) lr 1.1564e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1340
confident_label rate tensor(0.5163, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1619
clean true:1615
clean false:4
clean_rate:0.9975293390982087
noisy true:181
noisy false:1336
after delete: len(clean_dataset) 1619
after delete: len(noisy_dataset) 1517
epoch [93/200] batch [5/50] time 0.460 (0.469) data 0.329 (0.338) loss_x loss_x 1.3770 (1.3138) acc_x 56.2500 (68.7500) lr 1.1409e-03 eta 0:00:21
epoch [93/200] batch [10/50] time 0.492 (0.472) data 0.361 (0.341) loss_x loss_x 1.5781 (1.2194) acc_x 68.7500 (70.3125) lr 1.1409e-03 eta 0:00:18
epoch [93/200] batch [15/50] time 0.527 (0.490) data 0.395 (0.358) loss_x loss_x 1.2549 (1.1747) acc_x 65.6250 (70.4167) lr 1.1409e-03 eta 0:00:17
epoch [93/200] batch [20/50] time 0.507 (0.485) data 0.376 (0.354) loss_x loss_x 1.4023 (1.2007) acc_x 75.0000 (70.4688) lr 1.1409e-03 eta 0:00:14
epoch [93/200] batch [25/50] time 0.518 (0.485) data 0.387 (0.354) loss_x loss_x 1.2666 (1.2432) acc_x 62.5000 (69.2500) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [30/50] time 0.398 (0.483) data 0.265 (0.352) loss_x loss_x 1.2900 (1.2408) acc_x 71.8750 (69.3750) lr 1.1409e-03 eta 0:00:09
epoch [93/200] batch [35/50] time 0.417 (0.487) data 0.286 (0.356) loss_x loss_x 1.1582 (1.2128) acc_x 78.1250 (69.6429) lr 1.1409e-03 eta 0:00:07
epoch [93/200] batch [40/50] time 0.428 (0.480) data 0.298 (0.349) loss_x loss_x 0.6484 (1.1913) acc_x 81.2500 (70.0781) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [45/50] time 0.475 (0.482) data 0.344 (0.351) loss_x loss_x 0.8164 (1.1803) acc_x 78.1250 (70.2778) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [50/50] time 0.484 (0.479) data 0.353 (0.348) loss_x loss_x 1.1934 (1.2088) acc_x 62.5000 (69.5625) lr 1.1409e-03 eta 0:00:00
epoch [93/200] batch [5/47] time 0.417 (0.474) data 0.286 (0.343) loss_u loss_u 0.7720 (0.7679) acc_u 31.2500 (31.8750) lr 1.1409e-03 eta 0:00:19
epoch [93/200] batch [10/47] time 0.415 (0.478) data 0.281 (0.346) loss_u loss_u 0.7842 (0.7593) acc_u 25.0000 (31.8750) lr 1.1409e-03 eta 0:00:17
epoch [93/200] batch [15/47] time 0.499 (0.474) data 0.367 (0.342) loss_u loss_u 0.7944 (0.7706) acc_u 21.8750 (30.4167) lr 1.1409e-03 eta 0:00:15
epoch [93/200] batch [20/47] time 0.424 (0.472) data 0.291 (0.341) loss_u loss_u 0.7178 (0.7718) acc_u 37.5000 (30.0000) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [25/47] time 0.379 (0.474) data 0.248 (0.342) loss_u loss_u 0.7559 (0.7719) acc_u 31.2500 (30.2500) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [30/47] time 0.440 (0.471) data 0.308 (0.339) loss_u loss_u 0.7930 (0.7720) acc_u 31.2500 (30.2083) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [35/47] time 0.424 (0.470) data 0.291 (0.338) loss_u loss_u 0.8711 (0.7736) acc_u 18.7500 (30.2679) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [40/47] time 0.402 (0.472) data 0.270 (0.341) loss_u loss_u 0.7188 (0.7755) acc_u 34.3750 (29.5312) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [45/47] time 0.489 (0.471) data 0.357 (0.340) loss_u loss_u 0.7568 (0.7760) acc_u 31.2500 (29.5833) lr 1.1409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1377
confident_label rate tensor(0.5064, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1588
clean true:1588
clean false:0
clean_rate:1.0
noisy true:171
noisy false:1377
after delete: len(clean_dataset) 1588
after delete: len(noisy_dataset) 1548
epoch [94/200] batch [5/49] time 0.619 (0.517) data 0.487 (0.385) loss_x loss_x 0.9673 (1.1073) acc_x 78.1250 (73.7500) lr 1.1253e-03 eta 0:00:22
epoch [94/200] batch [10/49] time 0.418 (0.480) data 0.287 (0.349) loss_x loss_x 1.2197 (1.0124) acc_x 68.7500 (75.0000) lr 1.1253e-03 eta 0:00:18
epoch [94/200] batch [15/49] time 0.407 (0.461) data 0.277 (0.330) loss_x loss_x 1.1582 (1.0503) acc_x 71.8750 (75.2083) lr 1.1253e-03 eta 0:00:15
epoch [94/200] batch [20/49] time 0.477 (0.452) data 0.346 (0.321) loss_x loss_x 1.2031 (1.0790) acc_x 75.0000 (75.0000) lr 1.1253e-03 eta 0:00:13
epoch [94/200] batch [25/49] time 0.536 (0.463) data 0.405 (0.332) loss_x loss_x 0.9775 (1.0731) acc_x 78.1250 (74.5000) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [30/49] time 0.475 (0.460) data 0.344 (0.329) loss_x loss_x 1.3154 (1.1103) acc_x 65.6250 (73.4375) lr 1.1253e-03 eta 0:00:08
epoch [94/200] batch [35/49] time 0.404 (0.458) data 0.274 (0.327) loss_x loss_x 0.8218 (1.0827) acc_x 71.8750 (74.0179) lr 1.1253e-03 eta 0:00:06
epoch [94/200] batch [40/49] time 0.530 (0.460) data 0.399 (0.329) loss_x loss_x 1.2295 (1.1025) acc_x 71.8750 (73.5938) lr 1.1253e-03 eta 0:00:04
epoch [94/200] batch [45/49] time 0.476 (0.468) data 0.345 (0.337) loss_x loss_x 1.5713 (1.1120) acc_x 65.6250 (73.4028) lr 1.1253e-03 eta 0:00:01
epoch [94/200] batch [5/48] time 0.345 (0.466) data 0.214 (0.335) loss_u loss_u 0.6597 (0.7163) acc_u 43.7500 (34.3750) lr 1.1253e-03 eta 0:00:20
epoch [94/200] batch [10/48] time 0.348 (0.465) data 0.216 (0.334) loss_u loss_u 0.7769 (0.7305) acc_u 31.2500 (33.7500) lr 1.1253e-03 eta 0:00:17
epoch [94/200] batch [15/48] time 0.438 (0.462) data 0.307 (0.330) loss_u loss_u 0.7500 (0.7455) acc_u 37.5000 (32.5000) lr 1.1253e-03 eta 0:00:15
epoch [94/200] batch [20/48] time 0.339 (0.458) data 0.208 (0.327) loss_u loss_u 0.7046 (0.7566) acc_u 34.3750 (30.7812) lr 1.1253e-03 eta 0:00:12
epoch [94/200] batch [25/48] time 0.508 (0.463) data 0.376 (0.332) loss_u loss_u 0.7783 (0.7705) acc_u 28.1250 (28.8750) lr 1.1253e-03 eta 0:00:10
epoch [94/200] batch [30/48] time 0.619 (0.467) data 0.488 (0.336) loss_u loss_u 0.8315 (0.7767) acc_u 25.0000 (27.9167) lr 1.1253e-03 eta 0:00:08
epoch [94/200] batch [35/48] time 0.437 (0.464) data 0.306 (0.333) loss_u loss_u 0.7583 (0.7759) acc_u 31.2500 (28.4821) lr 1.1253e-03 eta 0:00:06
epoch [94/200] batch [40/48] time 0.477 (0.462) data 0.346 (0.330) loss_u loss_u 0.7876 (0.7738) acc_u 25.0000 (28.6719) lr 1.1253e-03 eta 0:00:03
epoch [94/200] batch [45/48] time 0.612 (0.462) data 0.481 (0.331) loss_u loss_u 0.7393 (0.7728) acc_u 37.5000 (28.9583) lr 1.1253e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1333
confident_label rate tensor(0.5217, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1636
clean true:1632
clean false:4
clean_rate:0.9975550122249389
noisy true:171
noisy false:1329
after delete: len(clean_dataset) 1636
after delete: len(noisy_dataset) 1500
epoch [95/200] batch [5/51] time 0.484 (0.505) data 0.353 (0.374) loss_x loss_x 1.1826 (1.0091) acc_x 75.0000 (77.5000) lr 1.1097e-03 eta 0:00:23
epoch [95/200] batch [10/51] time 0.469 (0.488) data 0.338 (0.357) loss_x loss_x 1.7246 (1.2232) acc_x 68.7500 (73.4375) lr 1.1097e-03 eta 0:00:20
epoch [95/200] batch [15/51] time 0.479 (0.486) data 0.348 (0.356) loss_x loss_x 0.9873 (1.1836) acc_x 71.8750 (73.1250) lr 1.1097e-03 eta 0:00:17
epoch [95/200] batch [20/51] time 0.451 (0.486) data 0.321 (0.356) loss_x loss_x 0.8535 (1.1648) acc_x 78.1250 (72.9688) lr 1.1097e-03 eta 0:00:15
epoch [95/200] batch [25/51] time 0.440 (0.476) data 0.309 (0.345) loss_x loss_x 1.2031 (1.1568) acc_x 68.7500 (72.3750) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [30/51] time 0.432 (0.493) data 0.301 (0.362) loss_x loss_x 1.3271 (1.1632) acc_x 65.6250 (71.3542) lr 1.1097e-03 eta 0:00:10
epoch [95/200] batch [35/51] time 0.399 (0.486) data 0.269 (0.356) loss_x loss_x 1.2158 (1.1341) acc_x 75.0000 (71.8750) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [40/51] time 0.339 (0.478) data 0.208 (0.347) loss_x loss_x 1.1250 (1.1415) acc_x 81.2500 (71.2500) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [45/51] time 0.469 (0.475) data 0.339 (0.345) loss_x loss_x 0.9116 (1.1625) acc_x 75.0000 (70.5556) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [50/51] time 0.556 (0.478) data 0.426 (0.347) loss_x loss_x 1.3379 (1.1633) acc_x 62.5000 (70.6875) lr 1.1097e-03 eta 0:00:00
epoch [95/200] batch [5/46] time 0.433 (0.469) data 0.302 (0.338) loss_u loss_u 0.8643 (0.8045) acc_u 15.6250 (22.5000) lr 1.1097e-03 eta 0:00:19
epoch [95/200] batch [10/46] time 0.523 (0.465) data 0.391 (0.334) loss_u loss_u 0.6177 (0.7692) acc_u 46.8750 (30.3125) lr 1.1097e-03 eta 0:00:16
epoch [95/200] batch [15/46] time 0.520 (0.464) data 0.389 (0.333) loss_u loss_u 0.7505 (0.7810) acc_u 34.3750 (28.5417) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [20/46] time 0.655 (0.465) data 0.524 (0.334) loss_u loss_u 0.7808 (0.7835) acc_u 25.0000 (27.5000) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [25/46] time 0.638 (0.462) data 0.507 (0.331) loss_u loss_u 0.6753 (0.7799) acc_u 53.1250 (28.6250) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [30/46] time 0.361 (0.459) data 0.229 (0.328) loss_u loss_u 0.7524 (0.7799) acc_u 28.1250 (28.1250) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [35/46] time 0.463 (0.458) data 0.331 (0.327) loss_u loss_u 0.7666 (0.7788) acc_u 28.1250 (28.4821) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [40/46] time 0.470 (0.457) data 0.338 (0.326) loss_u loss_u 0.8281 (0.7812) acc_u 18.7500 (28.2031) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [45/46] time 0.581 (0.459) data 0.449 (0.328) loss_u loss_u 0.7734 (0.7818) acc_u 28.1250 (28.5417) lr 1.1097e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1373
confident_label rate tensor(0.5073, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1591
clean true:1589
clean false:2
clean_rate:0.9987429289754871
noisy true:174
noisy false:1371
after delete: len(clean_dataset) 1591
after delete: len(noisy_dataset) 1545
epoch [96/200] batch [5/49] time 0.478 (0.472) data 0.347 (0.341) loss_x loss_x 1.2119 (1.1474) acc_x 71.8750 (72.5000) lr 1.0941e-03 eta 0:00:20
epoch [96/200] batch [10/49] time 0.422 (0.491) data 0.292 (0.360) loss_x loss_x 1.4512 (1.1362) acc_x 62.5000 (72.8125) lr 1.0941e-03 eta 0:00:19
epoch [96/200] batch [15/49] time 0.460 (0.508) data 0.329 (0.377) loss_x loss_x 1.6807 (1.2600) acc_x 62.5000 (68.9583) lr 1.0941e-03 eta 0:00:17
epoch [96/200] batch [20/49] time 0.604 (0.502) data 0.473 (0.371) loss_x loss_x 0.7334 (1.2223) acc_x 81.2500 (69.3750) lr 1.0941e-03 eta 0:00:14
epoch [96/200] batch [25/49] time 0.553 (0.504) data 0.423 (0.372) loss_x loss_x 0.9526 (1.2188) acc_x 71.8750 (69.3750) lr 1.0941e-03 eta 0:00:12
epoch [96/200] batch [30/49] time 0.539 (0.504) data 0.408 (0.373) loss_x loss_x 1.3760 (1.2231) acc_x 59.3750 (68.9583) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [35/49] time 0.405 (0.497) data 0.275 (0.366) loss_x loss_x 0.9863 (1.2267) acc_x 68.7500 (68.3929) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [40/49] time 0.470 (0.492) data 0.339 (0.361) loss_x loss_x 1.6064 (1.2288) acc_x 53.1250 (67.8906) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [45/49] time 0.541 (0.488) data 0.409 (0.356) loss_x loss_x 0.5063 (1.2005) acc_x 87.5000 (68.5417) lr 1.0941e-03 eta 0:00:01
epoch [96/200] batch [5/48] time 0.483 (0.492) data 0.352 (0.361) loss_u loss_u 0.7466 (0.7591) acc_u 34.3750 (31.8750) lr 1.0941e-03 eta 0:00:21
epoch [96/200] batch [10/48] time 0.397 (0.492) data 0.266 (0.361) loss_u loss_u 0.7700 (0.7740) acc_u 34.3750 (31.5625) lr 1.0941e-03 eta 0:00:18
epoch [96/200] batch [15/48] time 0.373 (0.485) data 0.241 (0.354) loss_u loss_u 0.7803 (0.7840) acc_u 31.2500 (29.7917) lr 1.0941e-03 eta 0:00:16
epoch [96/200] batch [20/48] time 0.394 (0.480) data 0.262 (0.349) loss_u loss_u 0.7808 (0.7842) acc_u 28.1250 (28.5938) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [25/48] time 0.434 (0.478) data 0.303 (0.346) loss_u loss_u 0.6851 (0.7738) acc_u 46.8750 (29.8750) lr 1.0941e-03 eta 0:00:10
epoch [96/200] batch [30/48] time 0.443 (0.473) data 0.312 (0.342) loss_u loss_u 0.7954 (0.7725) acc_u 25.0000 (29.4792) lr 1.0941e-03 eta 0:00:08
epoch [96/200] batch [35/48] time 0.426 (0.468) data 0.294 (0.336) loss_u loss_u 0.7852 (0.7740) acc_u 28.1250 (28.8393) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [40/48] time 0.370 (0.468) data 0.239 (0.337) loss_u loss_u 0.7275 (0.7698) acc_u 43.7500 (29.5312) lr 1.0941e-03 eta 0:00:03
epoch [96/200] batch [45/48] time 0.429 (0.472) data 0.297 (0.341) loss_u loss_u 0.7881 (0.7753) acc_u 25.0000 (28.6111) lr 1.0941e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1365
confident_label rate tensor(0.5092, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1597
clean true:1597
clean false:0
clean_rate:1.0
noisy true:174
noisy false:1365
after delete: len(clean_dataset) 1597
after delete: len(noisy_dataset) 1539
epoch [97/200] batch [5/49] time 0.419 (0.399) data 0.289 (0.268) loss_x loss_x 1.2686 (1.2520) acc_x 62.5000 (68.1250) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [10/49] time 0.455 (0.461) data 0.323 (0.330) loss_x loss_x 1.1465 (1.2260) acc_x 75.0000 (70.3125) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [15/49] time 0.440 (0.463) data 0.310 (0.332) loss_x loss_x 1.3994 (1.2405) acc_x 62.5000 (70.4167) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [20/49] time 0.574 (0.477) data 0.443 (0.346) loss_x loss_x 1.3359 (1.2214) acc_x 59.3750 (70.6250) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [25/49] time 0.551 (0.478) data 0.420 (0.347) loss_x loss_x 1.1104 (1.2061) acc_x 71.8750 (70.2500) lr 1.0785e-03 eta 0:00:11
epoch [97/200] batch [30/49] time 0.405 (0.478) data 0.274 (0.347) loss_x loss_x 1.1865 (1.1950) acc_x 62.5000 (70.1042) lr 1.0785e-03 eta 0:00:09
epoch [97/200] batch [35/49] time 0.445 (0.486) data 0.315 (0.355) loss_x loss_x 1.1475 (1.2094) acc_x 75.0000 (70.0893) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [40/49] time 0.560 (0.485) data 0.430 (0.354) loss_x loss_x 2.2500 (1.2234) acc_x 46.8750 (69.6094) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [45/49] time 0.500 (0.479) data 0.369 (0.348) loss_x loss_x 1.1582 (1.2081) acc_x 68.7500 (69.5833) lr 1.0785e-03 eta 0:00:01
epoch [97/200] batch [5/48] time 0.399 (0.470) data 0.268 (0.339) loss_u loss_u 0.7759 (0.7688) acc_u 31.2500 (30.0000) lr 1.0785e-03 eta 0:00:20
epoch [97/200] batch [10/48] time 0.773 (0.474) data 0.641 (0.343) loss_u loss_u 0.7695 (0.7709) acc_u 34.3750 (29.6875) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [15/48] time 0.383 (0.475) data 0.251 (0.344) loss_u loss_u 0.7393 (0.7681) acc_u 25.0000 (29.3750) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [20/48] time 0.437 (0.477) data 0.306 (0.346) loss_u loss_u 0.8174 (0.7639) acc_u 18.7500 (29.5312) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [25/48] time 0.440 (0.473) data 0.309 (0.342) loss_u loss_u 0.7471 (0.7694) acc_u 25.0000 (28.5000) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [30/48] time 0.526 (0.475) data 0.392 (0.344) loss_u loss_u 0.7832 (0.7671) acc_u 21.8750 (29.0625) lr 1.0785e-03 eta 0:00:08
epoch [97/200] batch [35/48] time 0.477 (0.478) data 0.346 (0.347) loss_u loss_u 0.7388 (0.7617) acc_u 37.5000 (30.0000) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [40/48] time 0.391 (0.475) data 0.260 (0.344) loss_u loss_u 0.9175 (0.7669) acc_u 9.3750 (29.2969) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [45/48] time 0.437 (0.473) data 0.306 (0.341) loss_u loss_u 0.7593 (0.7681) acc_u 34.3750 (29.5833) lr 1.0785e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1339
confident_label rate tensor(0.5150, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1615
clean true:1614
clean false:1
clean_rate:0.9993808049535604
noisy true:183
noisy false:1338
after delete: len(clean_dataset) 1615
after delete: len(noisy_dataset) 1521
epoch [98/200] batch [5/50] time 0.372 (0.491) data 0.241 (0.360) loss_x loss_x 1.5234 (1.1923) acc_x 65.6250 (70.6250) lr 1.0628e-03 eta 0:00:22
epoch [98/200] batch [10/50] time 0.528 (0.479) data 0.396 (0.348) loss_x loss_x 0.9634 (1.2024) acc_x 75.0000 (68.4375) lr 1.0628e-03 eta 0:00:19
epoch [98/200] batch [15/50] time 0.411 (0.486) data 0.280 (0.355) loss_x loss_x 1.0117 (1.1535) acc_x 71.8750 (70.4167) lr 1.0628e-03 eta 0:00:17
epoch [98/200] batch [20/50] time 0.408 (0.480) data 0.277 (0.349) loss_x loss_x 0.6733 (1.1248) acc_x 90.6250 (72.5000) lr 1.0628e-03 eta 0:00:14
epoch [98/200] batch [25/50] time 0.416 (0.478) data 0.285 (0.347) loss_x loss_x 1.8535 (1.1348) acc_x 53.1250 (71.7500) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [30/50] time 0.474 (0.472) data 0.342 (0.341) loss_x loss_x 0.8130 (1.1261) acc_x 81.2500 (71.4583) lr 1.0628e-03 eta 0:00:09
epoch [98/200] batch [35/50] time 0.598 (0.470) data 0.466 (0.339) loss_x loss_x 0.9331 (1.1152) acc_x 84.3750 (71.7857) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [40/50] time 0.479 (0.472) data 0.349 (0.341) loss_x loss_x 1.4609 (1.1126) acc_x 68.7500 (72.0312) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [45/50] time 0.569 (0.480) data 0.438 (0.349) loss_x loss_x 1.0117 (1.1271) acc_x 75.0000 (71.5278) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [50/50] time 0.507 (0.482) data 0.375 (0.351) loss_x loss_x 0.9517 (1.1305) acc_x 78.1250 (71.8125) lr 1.0628e-03 eta 0:00:00
epoch [98/200] batch [5/47] time 0.444 (0.475) data 0.312 (0.343) loss_u loss_u 0.7808 (0.8053) acc_u 28.1250 (25.0000) lr 1.0628e-03 eta 0:00:19
epoch [98/200] batch [10/47] time 0.397 (0.474) data 0.265 (0.343) loss_u loss_u 0.7817 (0.8136) acc_u 34.3750 (24.0625) lr 1.0628e-03 eta 0:00:17
epoch [98/200] batch [15/47] time 0.408 (0.473) data 0.276 (0.342) loss_u loss_u 0.8286 (0.8078) acc_u 28.1250 (25.0000) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [20/47] time 0.464 (0.473) data 0.331 (0.341) loss_u loss_u 0.6680 (0.8063) acc_u 37.5000 (25.0000) lr 1.0628e-03 eta 0:00:12
epoch [98/200] batch [25/47] time 0.533 (0.470) data 0.400 (0.339) loss_u loss_u 0.7241 (0.7967) acc_u 34.3750 (26.0000) lr 1.0628e-03 eta 0:00:10
epoch [98/200] batch [30/47] time 0.555 (0.471) data 0.423 (0.340) loss_u loss_u 0.7603 (0.7966) acc_u 37.5000 (26.2500) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [35/47] time 0.492 (0.473) data 0.361 (0.341) loss_u loss_u 0.8149 (0.7890) acc_u 25.0000 (26.9643) lr 1.0628e-03 eta 0:00:05
epoch [98/200] batch [40/47] time 0.423 (0.472) data 0.290 (0.341) loss_u loss_u 0.6836 (0.7815) acc_u 43.7500 (27.8125) lr 1.0628e-03 eta 0:00:03
epoch [98/200] batch [45/47] time 0.624 (0.477) data 0.491 (0.345) loss_u loss_u 0.7793 (0.7810) acc_u 34.3750 (28.1944) lr 1.0628e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1347
confident_label rate tensor(0.5124, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1607
clean true:1607
clean false:0
clean_rate:1.0
noisy true:182
noisy false:1347
after delete: len(clean_dataset) 1607
after delete: len(noisy_dataset) 1529
epoch [99/200] batch [5/50] time 0.469 (0.472) data 0.339 (0.341) loss_x loss_x 1.2197 (0.9829) acc_x 59.3750 (76.2500) lr 1.0471e-03 eta 0:00:21
epoch [99/200] batch [10/50] time 0.442 (0.472) data 0.311 (0.341) loss_x loss_x 1.2734 (0.9294) acc_x 75.0000 (76.8750) lr 1.0471e-03 eta 0:00:18
epoch [99/200] batch [15/50] time 0.448 (0.475) data 0.316 (0.344) loss_x loss_x 0.8545 (0.9798) acc_x 81.2500 (76.4583) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [20/50] time 0.410 (0.477) data 0.278 (0.346) loss_x loss_x 1.3389 (1.0465) acc_x 68.7500 (74.8438) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [25/50] time 0.427 (0.486) data 0.296 (0.355) loss_x loss_x 1.4023 (1.0669) acc_x 62.5000 (74.0000) lr 1.0471e-03 eta 0:00:12
epoch [99/200] batch [30/50] time 0.410 (0.479) data 0.279 (0.348) loss_x loss_x 0.9233 (1.0482) acc_x 75.0000 (74.3750) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [35/50] time 0.449 (0.478) data 0.318 (0.347) loss_x loss_x 0.5820 (1.0487) acc_x 87.5000 (74.5536) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [40/50] time 0.561 (0.488) data 0.430 (0.357) loss_x loss_x 1.0352 (1.0598) acc_x 75.0000 (73.5156) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [45/50] time 0.527 (0.491) data 0.396 (0.359) loss_x loss_x 0.9136 (1.0695) acc_x 81.2500 (73.1250) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [50/50] time 0.407 (0.490) data 0.276 (0.359) loss_x loss_x 1.5078 (1.0861) acc_x 59.3750 (72.3125) lr 1.0471e-03 eta 0:00:00
epoch [99/200] batch [5/47] time 0.477 (0.483) data 0.345 (0.351) loss_u loss_u 0.8086 (0.7750) acc_u 28.1250 (27.5000) lr 1.0471e-03 eta 0:00:20
epoch [99/200] batch [10/47] time 0.437 (0.478) data 0.306 (0.347) loss_u loss_u 0.8052 (0.7527) acc_u 28.1250 (32.8125) lr 1.0471e-03 eta 0:00:17
epoch [99/200] batch [15/47] time 0.519 (0.481) data 0.387 (0.350) loss_u loss_u 0.7036 (0.7582) acc_u 40.6250 (31.4583) lr 1.0471e-03 eta 0:00:15
epoch [99/200] batch [20/47] time 0.381 (0.474) data 0.249 (0.343) loss_u loss_u 0.7168 (0.7653) acc_u 31.2500 (30.3125) lr 1.0471e-03 eta 0:00:12
epoch [99/200] batch [25/47] time 0.418 (0.472) data 0.287 (0.341) loss_u loss_u 0.7563 (0.7633) acc_u 34.3750 (30.2500) lr 1.0471e-03 eta 0:00:10
epoch [99/200] batch [30/47] time 0.460 (0.470) data 0.329 (0.339) loss_u loss_u 0.7041 (0.7553) acc_u 34.3750 (31.0417) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [35/47] time 0.477 (0.472) data 0.345 (0.341) loss_u loss_u 0.8350 (0.7512) acc_u 18.7500 (31.3393) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [40/47] time 0.414 (0.471) data 0.283 (0.340) loss_u loss_u 0.7832 (0.7549) acc_u 31.2500 (31.2500) lr 1.0471e-03 eta 0:00:03
epoch [99/200] batch [45/47] time 0.685 (0.476) data 0.553 (0.344) loss_u loss_u 0.8208 (0.7603) acc_u 21.8750 (30.7639) lr 1.0471e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1328
confident_label rate tensor(0.5182, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1625
clean true:1621
clean false:4
clean_rate:0.9975384615384615
noisy true:187
noisy false:1324
all clean rate:  [0.9979959919839679, 1.0, 0.9991341991341991, 1.0, 0.9974662162162162, 0.998289136013687, 0.9983525535420099, 1.0, 0.998324958123953, 0.9991836734693877, 0.9991915925626516, 0.9992277992277993, 0.9984567901234568, 0.9992236024844721, 0.9992289899768697, 0.9968968192397207, 0.9968968192397207, 0.9984767707539984, 0.9985185185185185, 0.9992331288343558, 0.999251497005988, 0.9985130111524163, 0.9985085756897838, 0.9971119133574007, 0.9985454545454545, 0.9992836676217765, 0.9985795454545454, 0.9992831541218637, 0.9992872416250891, 0.9978902953586498, 0.997907949790795, 1.0, 1.0, 0.9992942836979535, 0.9993074792243767, 0.9993079584775086, 1.0, 0.9972677595628415, 0.9993021632937893, 0.9993145990404386, 0.9993122420907841, 0.9979702300405954, 0.9993093922651933, 0.9986111111111111, 1.0, 0.9986468200270636, 1.0, 1.0, 0.9973474801061007, 1.0, 0.9993455497382199, 0.9993293091884641, 0.99933598937583, 0.9993247805536799, 1.0, 0.9993355481727575, 0.9993446920052425, 1.0, 0.9993589743589744, 0.9986850756081526, 1.0, 1.0, 0.9986833443054641, 0.9993493819128172, 0.9993394980184941, 0.9993686868686869, 0.9993523316062176, 0.9987365761212887, 0.998108448928121, 1.0, 0.9986996098829649, 0.9987004548408057, 0.9981285090455396, 0.9993485342019544, 1.0, 0.9993722536095417, 1.0, 0.9993476842791911, 1.0, 1.0, 0.9993738259236068, 0.999353169469599, 0.9993626513702996, 0.9993765586034913, 0.9993614303959132, 0.998104864181933, 0.9987692307692307, 0.9993710691823899, 0.9993781094527363, 0.9993690851735015, 0.9993678887484198, 0.9987616099071207, 0.9975293390982087, 1.0, 0.9975550122249389, 0.9987429289754871, 1.0, 0.9993808049535604, 1.0, 0.9975384615384615]
after delete: len(clean_dataset) 1625
after delete: len(noisy_dataset) 1511
epoch [100/200] batch [5/50] time 0.438 (0.451) data 0.307 (0.320) loss_x loss_x 1.3516 (1.0031) acc_x 62.5000 (74.3750) lr 1.0314e-03 eta 0:00:20
epoch [100/200] batch [10/50] time 0.407 (0.457) data 0.275 (0.326) loss_x loss_x 1.1553 (1.0938) acc_x 68.7500 (73.7500) lr 1.0314e-03 eta 0:00:18
epoch [100/200] batch [15/50] time 0.478 (0.481) data 0.347 (0.350) loss_x loss_x 1.3232 (1.0661) acc_x 65.6250 (73.5417) lr 1.0314e-03 eta 0:00:16
epoch [100/200] batch [20/50] time 0.418 (0.476) data 0.287 (0.345) loss_x loss_x 1.0156 (1.0646) acc_x 71.8750 (73.5938) lr 1.0314e-03 eta 0:00:14
epoch [100/200] batch [25/50] time 0.562 (0.482) data 0.431 (0.351) loss_x loss_x 1.5225 (1.0857) acc_x 59.3750 (72.8750) lr 1.0314e-03 eta 0:00:12
epoch [100/200] batch [30/50] time 0.425 (0.480) data 0.294 (0.349) loss_x loss_x 1.2266 (1.0998) acc_x 68.7500 (73.5417) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [35/50] time 0.550 (0.483) data 0.419 (0.351) loss_x loss_x 0.7793 (1.0770) acc_x 81.2500 (74.1071) lr 1.0314e-03 eta 0:00:07
epoch [100/200] batch [40/50] time 0.507 (0.484) data 0.376 (0.353) loss_x loss_x 0.9121 (1.0966) acc_x 78.1250 (73.8281) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [45/50] time 0.490 (0.487) data 0.359 (0.355) loss_x loss_x 1.2480 (1.1144) acc_x 68.7500 (73.2639) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [50/50] time 0.464 (0.489) data 0.332 (0.358) loss_x loss_x 1.0850 (1.1226) acc_x 71.8750 (73.0000) lr 1.0314e-03 eta 0:00:00
epoch [100/200] batch [5/47] time 0.561 (0.486) data 0.429 (0.355) loss_u loss_u 0.7734 (0.8131) acc_u 28.1250 (23.1250) lr 1.0314e-03 eta 0:00:20
epoch [100/200] batch [10/47] time 0.598 (0.486) data 0.466 (0.354) loss_u loss_u 0.6631 (0.7854) acc_u 50.0000 (25.9375) lr 1.0314e-03 eta 0:00:17
epoch [100/200] batch [15/47] time 0.398 (0.477) data 0.267 (0.346) loss_u loss_u 0.7993 (0.7728) acc_u 25.0000 (27.7083) lr 1.0314e-03 eta 0:00:15
epoch [100/200] batch [20/47] time 0.526 (0.476) data 0.395 (0.344) loss_u loss_u 0.7485 (0.7755) acc_u 31.2500 (27.5000) lr 1.0314e-03 eta 0:00:12
epoch [100/200] batch [25/47] time 0.398 (0.473) data 0.266 (0.341) loss_u loss_u 0.8076 (0.7690) acc_u 21.8750 (28.3750) lr 1.0314e-03 eta 0:00:10
epoch [100/200] batch [30/47] time 0.409 (0.471) data 0.277 (0.339) loss_u loss_u 0.7573 (0.7744) acc_u 31.2500 (27.9167) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [35/47] time 0.393 (0.469) data 0.261 (0.338) loss_u loss_u 0.8398 (0.7742) acc_u 15.6250 (27.7679) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [40/47] time 0.495 (0.470) data 0.363 (0.339) loss_u loss_u 0.8091 (0.7754) acc_u 28.1250 (27.6562) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [45/47] time 0.592 (0.470) data 0.461 (0.338) loss_u loss_u 0.7476 (0.7737) acc_u 31.2500 (28.1944) lr 1.0314e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1343
confident_label rate tensor(0.5169, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1621
clean true:1619
clean false:2
clean_rate:0.9987661937075879
noisy true:174
noisy false:1341
after delete: len(clean_dataset) 1621
after delete: len(noisy_dataset) 1515
epoch [101/200] batch [5/50] time 0.480 (0.439) data 0.350 (0.308) loss_x loss_x 1.3965 (1.2437) acc_x 68.7500 (68.1250) lr 1.0157e-03 eta 0:00:19
epoch [101/200] batch [10/50] time 0.413 (0.467) data 0.282 (0.337) loss_x loss_x 1.3193 (1.2114) acc_x 68.7500 (70.9375) lr 1.0157e-03 eta 0:00:18
epoch [101/200] batch [15/50] time 0.380 (0.466) data 0.248 (0.335) loss_x loss_x 0.7183 (1.1499) acc_x 81.2500 (71.6667) lr 1.0157e-03 eta 0:00:16
epoch [101/200] batch [20/50] time 0.495 (0.460) data 0.364 (0.329) loss_x loss_x 1.4316 (1.1613) acc_x 65.6250 (71.0938) lr 1.0157e-03 eta 0:00:13
epoch [101/200] batch [25/50] time 0.395 (0.461) data 0.264 (0.330) loss_x loss_x 1.5811 (1.1734) acc_x 53.1250 (70.6250) lr 1.0157e-03 eta 0:00:11
epoch [101/200] batch [30/50] time 0.430 (0.456) data 0.299 (0.324) loss_x loss_x 1.0928 (1.1561) acc_x 75.0000 (71.0417) lr 1.0157e-03 eta 0:00:09
epoch [101/200] batch [35/50] time 0.428 (0.457) data 0.297 (0.326) loss_x loss_x 1.0664 (1.1475) acc_x 78.1250 (71.4286) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [40/50] time 0.421 (0.458) data 0.290 (0.327) loss_x loss_x 1.1309 (1.1450) acc_x 65.6250 (71.1719) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [45/50] time 0.364 (0.456) data 0.233 (0.325) loss_x loss_x 0.6743 (1.1457) acc_x 87.5000 (71.1111) lr 1.0157e-03 eta 0:00:02
epoch [101/200] batch [50/50] time 0.387 (0.460) data 0.257 (0.329) loss_x loss_x 1.3770 (1.1437) acc_x 75.0000 (71.3750) lr 1.0157e-03 eta 0:00:00
epoch [101/200] batch [5/47] time 0.650 (0.456) data 0.519 (0.324) loss_u loss_u 0.6948 (0.7676) acc_u 37.5000 (30.6250) lr 1.0157e-03 eta 0:00:19
epoch [101/200] batch [10/47] time 0.440 (0.455) data 0.309 (0.324) loss_u loss_u 0.6885 (0.7686) acc_u 37.5000 (30.6250) lr 1.0157e-03 eta 0:00:16
epoch [101/200] batch [15/47] time 0.386 (0.454) data 0.255 (0.323) loss_u loss_u 0.8164 (0.7841) acc_u 21.8750 (28.9583) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [20/47] time 0.533 (0.452) data 0.400 (0.321) loss_u loss_u 0.7632 (0.7739) acc_u 31.2500 (30.3125) lr 1.0157e-03 eta 0:00:12
epoch [101/200] batch [25/47] time 0.712 (0.454) data 0.580 (0.322) loss_u loss_u 0.7207 (0.7710) acc_u 43.7500 (30.7500) lr 1.0157e-03 eta 0:00:09
epoch [101/200] batch [30/47] time 0.560 (0.455) data 0.428 (0.324) loss_u loss_u 0.7812 (0.7686) acc_u 28.1250 (30.4167) lr 1.0157e-03 eta 0:00:07
epoch [101/200] batch [35/47] time 0.450 (0.457) data 0.318 (0.326) loss_u loss_u 0.8184 (0.7730) acc_u 28.1250 (29.7321) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [40/47] time 0.357 (0.458) data 0.225 (0.327) loss_u loss_u 0.7744 (0.7720) acc_u 34.3750 (29.8438) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [45/47] time 0.444 (0.456) data 0.312 (0.324) loss_u loss_u 0.8281 (0.7723) acc_u 15.6250 (29.7917) lr 1.0157e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1326
confident_label rate tensor(0.5188, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1627
clean true:1623
clean false:4
clean_rate:0.9975414874001229
noisy true:187
noisy false:1322
after delete: len(clean_dataset) 1627
after delete: len(noisy_dataset) 1509
epoch [102/200] batch [5/50] time 0.562 (0.531) data 0.429 (0.399) loss_x loss_x 0.9448 (0.9812) acc_x 84.3750 (73.7500) lr 1.0000e-03 eta 0:00:23
epoch [102/200] batch [10/50] time 0.571 (0.568) data 0.439 (0.436) loss_x loss_x 1.5420 (1.1117) acc_x 59.3750 (70.6250) lr 1.0000e-03 eta 0:00:22
epoch [102/200] batch [15/50] time 0.576 (0.546) data 0.445 (0.414) loss_x loss_x 1.2832 (1.1114) acc_x 75.0000 (72.7083) lr 1.0000e-03 eta 0:00:19
epoch [102/200] batch [20/50] time 0.532 (0.536) data 0.401 (0.404) loss_x loss_x 1.9082 (1.1376) acc_x 50.0000 (72.1875) lr 1.0000e-03 eta 0:00:16
epoch [102/200] batch [25/50] time 0.570 (0.538) data 0.439 (0.406) loss_x loss_x 1.0508 (1.1549) acc_x 81.2500 (72.2500) lr 1.0000e-03 eta 0:00:13
epoch [102/200] batch [30/50] time 0.474 (0.535) data 0.342 (0.403) loss_x loss_x 1.2949 (1.1569) acc_x 78.1250 (71.8750) lr 1.0000e-03 eta 0:00:10
epoch [102/200] batch [35/50] time 0.404 (0.525) data 0.271 (0.393) loss_x loss_x 1.0898 (1.1505) acc_x 71.8750 (72.2321) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [40/50] time 0.505 (0.515) data 0.374 (0.383) loss_x loss_x 1.2725 (1.1614) acc_x 62.5000 (71.5625) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [45/50] time 0.486 (0.507) data 0.355 (0.375) loss_x loss_x 1.2725 (1.1660) acc_x 71.8750 (71.4583) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [50/50] time 0.424 (0.499) data 0.293 (0.367) loss_x loss_x 0.9082 (1.1626) acc_x 75.0000 (71.0625) lr 1.0000e-03 eta 0:00:00
epoch [102/200] batch [5/47] time 0.664 (0.499) data 0.533 (0.367) loss_u loss_u 0.7900 (0.7660) acc_u 21.8750 (31.8750) lr 1.0000e-03 eta 0:00:20
epoch [102/200] batch [10/47] time 0.413 (0.496) data 0.281 (0.364) loss_u loss_u 0.8086 (0.7633) acc_u 18.7500 (30.3125) lr 1.0000e-03 eta 0:00:18
epoch [102/200] batch [15/47] time 0.422 (0.491) data 0.291 (0.359) loss_u loss_u 0.8501 (0.7665) acc_u 15.6250 (30.2083) lr 1.0000e-03 eta 0:00:15
epoch [102/200] batch [20/47] time 0.469 (0.487) data 0.338 (0.355) loss_u loss_u 0.8047 (0.7675) acc_u 21.8750 (29.0625) lr 1.0000e-03 eta 0:00:13
epoch [102/200] batch [25/47] time 0.601 (0.488) data 0.469 (0.357) loss_u loss_u 0.6694 (0.7673) acc_u 43.7500 (29.0000) lr 1.0000e-03 eta 0:00:10
epoch [102/200] batch [30/47] time 0.446 (0.486) data 0.314 (0.355) loss_u loss_u 0.6982 (0.7635) acc_u 34.3750 (29.4792) lr 1.0000e-03 eta 0:00:08
epoch [102/200] batch [35/47] time 0.543 (0.484) data 0.411 (0.352) loss_u loss_u 0.7397 (0.7623) acc_u 31.2500 (29.5536) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [40/47] time 0.345 (0.482) data 0.214 (0.351) loss_u loss_u 0.7861 (0.7664) acc_u 25.0000 (29.1406) lr 1.0000e-03 eta 0:00:03
epoch [102/200] batch [45/47] time 0.520 (0.483) data 0.389 (0.351) loss_u loss_u 0.7939 (0.7675) acc_u 25.0000 (29.0278) lr 1.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1391
confident_label rate tensor(0.5019, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1574
clean true:1573
clean false:1
clean_rate:0.9993646759847522
noisy true:172
noisy false:1390
after delete: len(clean_dataset) 1574
after delete: len(noisy_dataset) 1562
epoch [103/200] batch [5/49] time 0.638 (0.485) data 0.507 (0.354) loss_x loss_x 0.7485 (0.9935) acc_x 78.1250 (74.3750) lr 9.8429e-04 eta 0:00:21
epoch [103/200] batch [10/49] time 0.403 (0.469) data 0.272 (0.338) loss_x loss_x 1.2256 (1.1265) acc_x 68.7500 (72.1875) lr 9.8429e-04 eta 0:00:18
epoch [103/200] batch [15/49] time 0.407 (0.465) data 0.277 (0.334) loss_x loss_x 1.2988 (1.1730) acc_x 71.8750 (71.6667) lr 9.8429e-04 eta 0:00:15
epoch [103/200] batch [20/49] time 0.408 (0.462) data 0.278 (0.331) loss_x loss_x 1.5869 (1.2063) acc_x 56.2500 (70.1562) lr 9.8429e-04 eta 0:00:13
epoch [103/200] batch [25/49] time 0.581 (0.466) data 0.450 (0.336) loss_x loss_x 1.2900 (1.1907) acc_x 65.6250 (70.2500) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [30/49] time 0.511 (0.476) data 0.380 (0.345) loss_x loss_x 0.8081 (1.1958) acc_x 71.8750 (70.4167) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [35/49] time 0.385 (0.468) data 0.253 (0.337) loss_x loss_x 1.0996 (1.1755) acc_x 71.8750 (70.4464) lr 9.8429e-04 eta 0:00:06
epoch [103/200] batch [40/49] time 0.538 (0.468) data 0.406 (0.337) loss_x loss_x 1.3701 (1.1585) acc_x 68.7500 (70.9375) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [45/49] time 0.580 (0.475) data 0.448 (0.344) loss_x loss_x 0.9902 (1.1452) acc_x 75.0000 (71.3194) lr 9.8429e-04 eta 0:00:01
epoch [103/200] batch [5/48] time 0.434 (0.481) data 0.302 (0.349) loss_u loss_u 0.7192 (0.7771) acc_u 34.3750 (29.3750) lr 9.8429e-04 eta 0:00:20
epoch [103/200] batch [10/48] time 0.384 (0.480) data 0.251 (0.348) loss_u loss_u 0.7959 (0.7724) acc_u 25.0000 (29.3750) lr 9.8429e-04 eta 0:00:18
epoch [103/200] batch [15/48] time 0.460 (0.483) data 0.327 (0.351) loss_u loss_u 0.7256 (0.7720) acc_u 31.2500 (29.3750) lr 9.8429e-04 eta 0:00:15
epoch [103/200] batch [20/48] time 0.369 (0.481) data 0.238 (0.349) loss_u loss_u 0.8057 (0.7697) acc_u 34.3750 (29.3750) lr 9.8429e-04 eta 0:00:13
epoch [103/200] batch [25/48] time 0.526 (0.480) data 0.393 (0.348) loss_u loss_u 0.7202 (0.7643) acc_u 31.2500 (30.6250) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [30/48] time 0.472 (0.479) data 0.341 (0.347) loss_u loss_u 0.8115 (0.7652) acc_u 21.8750 (29.7917) lr 9.8429e-04 eta 0:00:08
epoch [103/200] batch [35/48] time 0.546 (0.478) data 0.414 (0.346) loss_u loss_u 0.7397 (0.7579) acc_u 34.3750 (31.2500) lr 9.8429e-04 eta 0:00:06
epoch [103/200] batch [40/48] time 0.379 (0.473) data 0.248 (0.341) loss_u loss_u 0.7490 (0.7618) acc_u 28.1250 (30.8594) lr 9.8429e-04 eta 0:00:03
epoch [103/200] batch [45/48] time 0.375 (0.470) data 0.243 (0.338) loss_u loss_u 0.7539 (0.7651) acc_u 25.0000 (30.2778) lr 9.8429e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1308
confident_label rate tensor(0.5268, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1652
clean true:1649
clean false:3
clean_rate:0.9981840193704601
noisy true:179
noisy false:1305
after delete: len(clean_dataset) 1652
after delete: len(noisy_dataset) 1484
epoch [104/200] batch [5/51] time 0.609 (0.564) data 0.477 (0.431) loss_x loss_x 1.1279 (1.1240) acc_x 75.0000 (73.7500) lr 9.6859e-04 eta 0:00:25
epoch [104/200] batch [10/51] time 0.475 (0.548) data 0.344 (0.415) loss_x loss_x 1.5215 (1.0797) acc_x 71.8750 (74.3750) lr 9.6859e-04 eta 0:00:22
epoch [104/200] batch [15/51] time 0.521 (0.537) data 0.389 (0.405) loss_x loss_x 0.7515 (1.1081) acc_x 81.2500 (73.1250) lr 9.6859e-04 eta 0:00:19
epoch [104/200] batch [20/51] time 0.379 (0.510) data 0.248 (0.378) loss_x loss_x 0.6099 (1.1001) acc_x 87.5000 (73.1250) lr 9.6859e-04 eta 0:00:15
epoch [104/200] batch [25/51] time 0.454 (0.498) data 0.323 (0.366) loss_x loss_x 0.6147 (1.0703) acc_x 84.3750 (73.5000) lr 9.6859e-04 eta 0:00:12
epoch [104/200] batch [30/51] time 0.425 (0.502) data 0.294 (0.370) loss_x loss_x 1.3535 (1.1049) acc_x 65.6250 (72.8125) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [35/51] time 0.440 (0.490) data 0.309 (0.359) loss_x loss_x 1.5234 (1.1197) acc_x 62.5000 (72.3214) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [40/51] time 0.441 (0.489) data 0.310 (0.358) loss_x loss_x 1.3271 (1.1281) acc_x 68.7500 (72.0312) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [45/51] time 0.416 (0.484) data 0.285 (0.353) loss_x loss_x 1.3613 (1.1192) acc_x 68.7500 (72.2222) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [50/51] time 0.402 (0.485) data 0.270 (0.353) loss_x loss_x 1.2715 (1.1149) acc_x 78.1250 (72.4375) lr 9.6859e-04 eta 0:00:00
epoch [104/200] batch [5/46] time 0.520 (0.482) data 0.388 (0.350) loss_u loss_u 0.8413 (0.8043) acc_u 28.1250 (27.5000) lr 9.6859e-04 eta 0:00:19
epoch [104/200] batch [10/46] time 0.441 (0.481) data 0.309 (0.349) loss_u loss_u 0.8062 (0.7928) acc_u 21.8750 (26.5625) lr 9.6859e-04 eta 0:00:17
epoch [104/200] batch [15/46] time 0.342 (0.480) data 0.208 (0.349) loss_u loss_u 0.8521 (0.7924) acc_u 21.8750 (27.7083) lr 9.6859e-04 eta 0:00:14
epoch [104/200] batch [20/46] time 0.446 (0.479) data 0.314 (0.347) loss_u loss_u 0.8003 (0.7890) acc_u 28.1250 (27.8125) lr 9.6859e-04 eta 0:00:12
epoch [104/200] batch [25/46] time 0.711 (0.480) data 0.579 (0.348) loss_u loss_u 0.8105 (0.7790) acc_u 28.1250 (28.8750) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [30/46] time 0.455 (0.475) data 0.322 (0.343) loss_u loss_u 0.7827 (0.7737) acc_u 25.0000 (29.2708) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [35/46] time 0.380 (0.471) data 0.247 (0.339) loss_u loss_u 0.8042 (0.7759) acc_u 25.0000 (29.5536) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [40/46] time 0.508 (0.470) data 0.375 (0.338) loss_u loss_u 0.8062 (0.7768) acc_u 18.7500 (28.9844) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [45/46] time 0.397 (0.467) data 0.264 (0.335) loss_u loss_u 0.8027 (0.7756) acc_u 25.0000 (29.4444) lr 9.6859e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1319
confident_label rate tensor(0.5201, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1631
clean true:1631
clean false:0
clean_rate:1.0
noisy true:186
noisy false:1319
after delete: len(clean_dataset) 1631
after delete: len(noisy_dataset) 1505
epoch [105/200] batch [5/50] time 0.501 (0.495) data 0.369 (0.364) loss_x loss_x 1.1514 (1.0654) acc_x 62.5000 (73.1250) lr 9.5289e-04 eta 0:00:22
epoch [105/200] batch [10/50] time 0.454 (0.497) data 0.322 (0.365) loss_x loss_x 0.8408 (1.0588) acc_x 78.1250 (75.3125) lr 9.5289e-04 eta 0:00:19
epoch [105/200] batch [15/50] time 0.478 (0.484) data 0.346 (0.352) loss_x loss_x 1.4385 (1.0614) acc_x 59.3750 (74.3750) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [20/50] time 0.513 (0.494) data 0.381 (0.362) loss_x loss_x 1.1328 (1.0743) acc_x 71.8750 (74.2188) lr 9.5289e-04 eta 0:00:14
epoch [105/200] batch [25/50] time 0.540 (0.479) data 0.409 (0.348) loss_x loss_x 1.1025 (1.0596) acc_x 71.8750 (74.6250) lr 9.5289e-04 eta 0:00:11
epoch [105/200] batch [30/50] time 0.457 (0.478) data 0.326 (0.347) loss_x loss_x 0.7134 (1.0631) acc_x 84.3750 (74.5833) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [35/50] time 0.442 (0.477) data 0.311 (0.346) loss_x loss_x 1.2412 (1.1022) acc_x 75.0000 (73.3929) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [40/50] time 0.400 (0.471) data 0.269 (0.339) loss_x loss_x 1.3584 (1.1306) acc_x 56.2500 (72.4219) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [45/50] time 0.504 (0.467) data 0.373 (0.336) loss_x loss_x 1.2949 (1.1392) acc_x 68.7500 (72.2917) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [50/50] time 0.494 (0.464) data 0.364 (0.333) loss_x loss_x 1.7402 (1.1474) acc_x 56.2500 (71.8125) lr 9.5289e-04 eta 0:00:00
epoch [105/200] batch [5/47] time 0.343 (0.460) data 0.211 (0.329) loss_u loss_u 0.8340 (0.7662) acc_u 18.7500 (30.6250) lr 9.5289e-04 eta 0:00:19
epoch [105/200] batch [10/47] time 0.545 (0.455) data 0.413 (0.324) loss_u loss_u 0.7324 (0.7500) acc_u 34.3750 (33.1250) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [15/47] time 0.502 (0.455) data 0.370 (0.324) loss_u loss_u 0.8276 (0.7663) acc_u 18.7500 (30.6250) lr 9.5289e-04 eta 0:00:14
epoch [105/200] batch [20/47] time 0.480 (0.455) data 0.345 (0.324) loss_u loss_u 0.7021 (0.7666) acc_u 34.3750 (30.6250) lr 9.5289e-04 eta 0:00:12
epoch [105/200] batch [25/47] time 0.386 (0.458) data 0.255 (0.327) loss_u loss_u 0.7910 (0.7767) acc_u 31.2500 (29.5000) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [30/47] time 0.393 (0.454) data 0.262 (0.322) loss_u loss_u 0.7905 (0.7747) acc_u 34.3750 (29.6875) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [35/47] time 0.442 (0.452) data 0.310 (0.320) loss_u loss_u 0.7847 (0.7785) acc_u 28.1250 (29.1071) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [40/47] time 0.505 (0.457) data 0.373 (0.325) loss_u loss_u 0.7490 (0.7771) acc_u 34.3750 (29.2188) lr 9.5289e-04 eta 0:00:03
epoch [105/200] batch [45/47] time 0.592 (0.459) data 0.460 (0.327) loss_u loss_u 0.7661 (0.7781) acc_u 34.3750 (29.0278) lr 9.5289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1313
confident_label rate tensor(0.5226, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1639
clean true:1638
clean false:1
clean_rate:0.9993898718730934
noisy true:185
noisy false:1312
after delete: len(clean_dataset) 1639
after delete: len(noisy_dataset) 1497
epoch [106/200] batch [5/51] time 0.487 (0.438) data 0.356 (0.307) loss_x loss_x 0.9390 (1.1415) acc_x 75.0000 (70.6250) lr 9.3721e-04 eta 0:00:20
epoch [106/200] batch [10/51] time 0.484 (0.455) data 0.353 (0.324) loss_x loss_x 1.7881 (1.2193) acc_x 53.1250 (69.0625) lr 9.3721e-04 eta 0:00:18
epoch [106/200] batch [15/51] time 0.392 (0.452) data 0.261 (0.321) loss_x loss_x 1.2129 (1.1767) acc_x 71.8750 (70.6250) lr 9.3721e-04 eta 0:00:16
epoch [106/200] batch [20/51] time 0.393 (0.460) data 0.263 (0.329) loss_x loss_x 1.0381 (1.1360) acc_x 78.1250 (71.7188) lr 9.3721e-04 eta 0:00:14
epoch [106/200] batch [25/51] time 0.442 (0.453) data 0.312 (0.323) loss_x loss_x 0.7495 (1.1202) acc_x 71.8750 (71.5000) lr 9.3721e-04 eta 0:00:11
epoch [106/200] batch [30/51] time 0.411 (0.452) data 0.281 (0.322) loss_x loss_x 1.0879 (1.1522) acc_x 75.0000 (71.4583) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [35/51] time 0.528 (0.457) data 0.397 (0.327) loss_x loss_x 0.6553 (1.1082) acc_x 87.5000 (72.4107) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [40/51] time 0.412 (0.456) data 0.281 (0.325) loss_x loss_x 0.8462 (1.1509) acc_x 90.6250 (71.9531) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [45/51] time 0.551 (0.453) data 0.420 (0.322) loss_x loss_x 1.9678 (1.1794) acc_x 56.2500 (71.2500) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [50/51] time 0.386 (0.457) data 0.254 (0.326) loss_x loss_x 1.0020 (1.1883) acc_x 75.0000 (70.6250) lr 9.3721e-04 eta 0:00:00
epoch [106/200] batch [5/46] time 0.559 (0.457) data 0.424 (0.326) loss_u loss_u 0.8257 (0.7448) acc_u 18.7500 (30.0000) lr 9.3721e-04 eta 0:00:18
epoch [106/200] batch [10/46] time 0.453 (0.453) data 0.321 (0.322) loss_u loss_u 0.7964 (0.7722) acc_u 25.0000 (29.0625) lr 9.3721e-04 eta 0:00:16
epoch [106/200] batch [15/46] time 0.349 (0.453) data 0.217 (0.322) loss_u loss_u 0.7227 (0.7726) acc_u 25.0000 (28.5417) lr 9.3721e-04 eta 0:00:14
epoch [106/200] batch [20/46] time 0.387 (0.451) data 0.256 (0.320) loss_u loss_u 0.7847 (0.7573) acc_u 28.1250 (31.2500) lr 9.3721e-04 eta 0:00:11
epoch [106/200] batch [25/46] time 0.344 (0.447) data 0.213 (0.316) loss_u loss_u 0.8550 (0.7592) acc_u 21.8750 (30.7500) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [30/46] time 0.703 (0.448) data 0.572 (0.317) loss_u loss_u 0.7744 (0.7622) acc_u 25.0000 (30.1042) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [35/46] time 0.442 (0.448) data 0.310 (0.317) loss_u loss_u 0.8164 (0.7688) acc_u 18.7500 (29.1964) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [40/46] time 0.409 (0.446) data 0.277 (0.315) loss_u loss_u 0.8784 (0.7726) acc_u 12.5000 (29.2188) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [45/46] time 0.443 (0.445) data 0.311 (0.314) loss_u loss_u 0.7266 (0.7733) acc_u 40.6250 (29.5833) lr 9.3721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1306
confident_label rate tensor(0.5258, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1649
clean true:1646
clean false:3
clean_rate:0.9981807155852032
noisy true:184
noisy false:1303
after delete: len(clean_dataset) 1649
after delete: len(noisy_dataset) 1487
epoch [107/200] batch [5/51] time 0.454 (0.486) data 0.321 (0.355) loss_x loss_x 0.8335 (1.0095) acc_x 75.0000 (71.2500) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [10/51] time 0.423 (0.463) data 0.292 (0.332) loss_x loss_x 1.2383 (1.0452) acc_x 81.2500 (75.0000) lr 9.2154e-04 eta 0:00:18
epoch [107/200] batch [15/51] time 0.456 (0.473) data 0.325 (0.341) loss_x loss_x 2.0293 (1.1764) acc_x 56.2500 (71.8750) lr 9.2154e-04 eta 0:00:17
epoch [107/200] batch [20/51] time 0.490 (0.466) data 0.359 (0.334) loss_x loss_x 0.7783 (1.1278) acc_x 81.2500 (73.7500) lr 9.2154e-04 eta 0:00:14
epoch [107/200] batch [25/51] time 0.593 (0.467) data 0.460 (0.336) loss_x loss_x 1.0273 (1.1218) acc_x 84.3750 (73.8750) lr 9.2154e-04 eta 0:00:12
epoch [107/200] batch [30/51] time 0.538 (0.467) data 0.407 (0.336) loss_x loss_x 0.7827 (1.1362) acc_x 84.3750 (73.5417) lr 9.2154e-04 eta 0:00:09
epoch [107/200] batch [35/51] time 0.410 (0.464) data 0.279 (0.333) loss_x loss_x 0.6182 (1.1182) acc_x 90.6250 (73.7500) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [40/51] time 0.514 (0.467) data 0.382 (0.335) loss_x loss_x 1.2178 (1.1261) acc_x 68.7500 (73.2031) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [45/51] time 0.374 (0.466) data 0.243 (0.335) loss_x loss_x 0.7451 (1.1121) acc_x 84.3750 (73.2639) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [50/51] time 0.653 (0.462) data 0.522 (0.330) loss_x loss_x 1.5430 (1.1291) acc_x 53.1250 (72.1875) lr 9.2154e-04 eta 0:00:00
epoch [107/200] batch [5/46] time 0.341 (0.457) data 0.209 (0.325) loss_u loss_u 0.8955 (0.8041) acc_u 9.3750 (25.0000) lr 9.2154e-04 eta 0:00:18
epoch [107/200] batch [10/46] time 0.348 (0.451) data 0.216 (0.319) loss_u loss_u 0.8711 (0.7970) acc_u 18.7500 (24.6875) lr 9.2154e-04 eta 0:00:16
epoch [107/200] batch [15/46] time 0.365 (0.448) data 0.233 (0.316) loss_u loss_u 0.8560 (0.8080) acc_u 25.0000 (23.1250) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [20/46] time 0.354 (0.447) data 0.220 (0.315) loss_u loss_u 0.7393 (0.7885) acc_u 31.2500 (25.6250) lr 9.2154e-04 eta 0:00:11
epoch [107/200] batch [25/46] time 0.508 (0.446) data 0.375 (0.314) loss_u loss_u 0.7822 (0.7834) acc_u 28.1250 (26.5000) lr 9.2154e-04 eta 0:00:09
epoch [107/200] batch [30/46] time 0.546 (0.444) data 0.413 (0.312) loss_u loss_u 0.8062 (0.7853) acc_u 18.7500 (26.2500) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [35/46] time 0.448 (0.450) data 0.316 (0.318) loss_u loss_u 0.7441 (0.7792) acc_u 28.1250 (27.1429) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [40/46] time 0.575 (0.453) data 0.443 (0.321) loss_u loss_u 0.7363 (0.7762) acc_u 40.6250 (27.7344) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [45/46] time 0.383 (0.451) data 0.251 (0.320) loss_u loss_u 0.6768 (0.7723) acc_u 43.7500 (28.4028) lr 9.2154e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1304
confident_label rate tensor(0.5300, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1662
clean true:1660
clean false:2
clean_rate:0.9987966305655837
noisy true:172
noisy false:1302
after delete: len(clean_dataset) 1662
after delete: len(noisy_dataset) 1474
epoch [108/200] batch [5/51] time 0.431 (0.477) data 0.300 (0.346) loss_x loss_x 1.0615 (1.2285) acc_x 71.8750 (71.2500) lr 9.0589e-04 eta 0:00:21
epoch [108/200] batch [10/51] time 0.462 (0.449) data 0.330 (0.318) loss_x loss_x 0.9312 (1.1951) acc_x 78.1250 (72.5000) lr 9.0589e-04 eta 0:00:18
epoch [108/200] batch [15/51] time 0.463 (0.459) data 0.331 (0.327) loss_x loss_x 1.3242 (1.1691) acc_x 68.7500 (71.0417) lr 9.0589e-04 eta 0:00:16
epoch [108/200] batch [20/51] time 0.380 (0.451) data 0.250 (0.320) loss_x loss_x 1.3906 (1.1821) acc_x 68.7500 (70.4688) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [25/51] time 0.401 (0.442) data 0.271 (0.311) loss_x loss_x 1.1084 (1.1494) acc_x 78.1250 (71.3750) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [30/51] time 0.757 (0.459) data 0.626 (0.328) loss_x loss_x 0.9263 (1.1465) acc_x 75.0000 (71.3542) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [35/51] time 0.528 (0.458) data 0.397 (0.327) loss_x loss_x 0.7490 (1.1397) acc_x 75.0000 (70.7143) lr 9.0589e-04 eta 0:00:07
epoch [108/200] batch [40/51] time 0.385 (0.459) data 0.255 (0.328) loss_x loss_x 0.7134 (1.1218) acc_x 78.1250 (71.1719) lr 9.0589e-04 eta 0:00:05
epoch [108/200] batch [45/51] time 0.550 (0.454) data 0.417 (0.323) loss_x loss_x 0.9663 (1.1345) acc_x 78.1250 (71.2500) lr 9.0589e-04 eta 0:00:02
epoch [108/200] batch [50/51] time 0.449 (0.457) data 0.317 (0.326) loss_x loss_x 1.2783 (1.1435) acc_x 59.3750 (71.1875) lr 9.0589e-04 eta 0:00:00
epoch [108/200] batch [5/46] time 0.575 (0.465) data 0.443 (0.334) loss_u loss_u 0.8306 (0.7904) acc_u 21.8750 (28.7500) lr 9.0589e-04 eta 0:00:19
epoch [108/200] batch [10/46] time 0.363 (0.465) data 0.230 (0.333) loss_u loss_u 0.6753 (0.7632) acc_u 31.2500 (30.3125) lr 9.0589e-04 eta 0:00:16
epoch [108/200] batch [15/46] time 0.372 (0.461) data 0.239 (0.329) loss_u loss_u 0.8809 (0.7717) acc_u 12.5000 (29.5833) lr 9.0589e-04 eta 0:00:14
epoch [108/200] batch [20/46] time 0.565 (0.462) data 0.434 (0.331) loss_u loss_u 0.7158 (0.7732) acc_u 37.5000 (29.6875) lr 9.0589e-04 eta 0:00:12
epoch [108/200] batch [25/46] time 0.490 (0.464) data 0.359 (0.332) loss_u loss_u 0.7988 (0.7802) acc_u 21.8750 (28.7500) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [30/46] time 0.370 (0.461) data 0.238 (0.329) loss_u loss_u 0.7808 (0.7819) acc_u 28.1250 (28.2292) lr 9.0589e-04 eta 0:00:07
epoch [108/200] batch [35/46] time 0.429 (0.458) data 0.298 (0.327) loss_u loss_u 0.6953 (0.7730) acc_u 40.6250 (29.4643) lr 9.0589e-04 eta 0:00:05
epoch [108/200] batch [40/46] time 0.373 (0.456) data 0.243 (0.325) loss_u loss_u 0.7783 (0.7739) acc_u 28.1250 (29.2969) lr 9.0589e-04 eta 0:00:02
epoch [108/200] batch [45/46] time 0.347 (0.453) data 0.215 (0.321) loss_u loss_u 0.7881 (0.7734) acc_u 25.0000 (29.3750) lr 9.0589e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1315
confident_label rate tensor(0.5220, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1637
clean true:1635
clean false:2
clean_rate:0.9987782529016493
noisy true:186
noisy false:1313
after delete: len(clean_dataset) 1637
after delete: len(noisy_dataset) 1499
epoch [109/200] batch [5/51] time 0.509 (0.408) data 0.378 (0.277) loss_x loss_x 0.9526 (1.0772) acc_x 75.0000 (71.8750) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [10/51] time 0.474 (0.449) data 0.343 (0.318) loss_x loss_x 0.7622 (1.1184) acc_x 81.2500 (71.8750) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [15/51] time 0.427 (0.437) data 0.296 (0.306) loss_x loss_x 1.1992 (1.1778) acc_x 75.0000 (71.8750) lr 8.9027e-04 eta 0:00:15
epoch [109/200] batch [20/51] time 0.459 (0.437) data 0.327 (0.306) loss_x loss_x 1.9023 (1.2058) acc_x 56.2500 (71.4062) lr 8.9027e-04 eta 0:00:13
epoch [109/200] batch [25/51] time 0.578 (0.462) data 0.447 (0.331) loss_x loss_x 1.0254 (1.1949) acc_x 59.3750 (69.8750) lr 8.9027e-04 eta 0:00:12
epoch [109/200] batch [30/51] time 0.437 (0.465) data 0.306 (0.334) loss_x loss_x 1.1152 (1.1969) acc_x 78.1250 (70.4167) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [35/51] time 0.398 (0.453) data 0.267 (0.323) loss_x loss_x 0.9102 (1.2016) acc_x 78.1250 (70.1786) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [40/51] time 0.411 (0.457) data 0.280 (0.326) loss_x loss_x 1.1650 (1.1863) acc_x 68.7500 (69.8438) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [45/51] time 0.468 (0.465) data 0.336 (0.334) loss_x loss_x 0.9297 (1.1955) acc_x 71.8750 (69.7917) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [50/51] time 0.481 (0.469) data 0.351 (0.338) loss_x loss_x 0.9937 (1.1912) acc_x 68.7500 (69.7500) lr 8.9027e-04 eta 0:00:00
epoch [109/200] batch [5/46] time 0.408 (0.467) data 0.277 (0.336) loss_u loss_u 0.7275 (0.7771) acc_u 28.1250 (26.8750) lr 8.9027e-04 eta 0:00:19
epoch [109/200] batch [10/46] time 0.482 (0.466) data 0.349 (0.335) loss_u loss_u 0.7695 (0.7550) acc_u 31.2500 (31.8750) lr 8.9027e-04 eta 0:00:16
epoch [109/200] batch [15/46] time 0.473 (0.468) data 0.340 (0.337) loss_u loss_u 0.8135 (0.7699) acc_u 28.1250 (31.2500) lr 8.9027e-04 eta 0:00:14
epoch [109/200] batch [20/46] time 0.434 (0.462) data 0.302 (0.331) loss_u loss_u 0.8159 (0.7666) acc_u 25.0000 (31.7188) lr 8.9027e-04 eta 0:00:12
epoch [109/200] batch [25/46] time 0.354 (0.456) data 0.223 (0.325) loss_u loss_u 0.8208 (0.7740) acc_u 31.2500 (30.7500) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [30/46] time 0.414 (0.453) data 0.281 (0.321) loss_u loss_u 0.7070 (0.7730) acc_u 40.6250 (30.6250) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [35/46] time 0.476 (0.452) data 0.343 (0.320) loss_u loss_u 0.7583 (0.7758) acc_u 34.3750 (30.2679) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [40/46] time 0.444 (0.450) data 0.311 (0.318) loss_u loss_u 0.7100 (0.7736) acc_u 37.5000 (30.0781) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [45/46] time 0.481 (0.452) data 0.350 (0.321) loss_u loss_u 0.6650 (0.7675) acc_u 43.7500 (30.7639) lr 8.9027e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1317
confident_label rate tensor(0.5246, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1645
clean true:1642
clean false:3
clean_rate:0.998176291793313
noisy true:177
noisy false:1314
after delete: len(clean_dataset) 1645
after delete: len(noisy_dataset) 1491
epoch [110/200] batch [5/51] time 0.534 (0.465) data 0.402 (0.334) loss_x loss_x 0.7280 (1.2038) acc_x 87.5000 (69.3750) lr 8.7467e-04 eta 0:00:21
epoch [110/200] batch [10/51] time 0.396 (0.453) data 0.265 (0.322) loss_x loss_x 1.6113 (1.2513) acc_x 71.8750 (69.6875) lr 8.7467e-04 eta 0:00:18
epoch [110/200] batch [15/51] time 0.463 (0.454) data 0.332 (0.322) loss_x loss_x 1.5576 (1.2817) acc_x 56.2500 (68.5417) lr 8.7467e-04 eta 0:00:16
epoch [110/200] batch [20/51] time 0.438 (0.446) data 0.307 (0.314) loss_x loss_x 1.3340 (1.2634) acc_x 62.5000 (68.5938) lr 8.7467e-04 eta 0:00:13
epoch [110/200] batch [25/51] time 0.567 (0.449) data 0.435 (0.317) loss_x loss_x 0.9346 (1.2446) acc_x 71.8750 (68.1250) lr 8.7467e-04 eta 0:00:11
epoch [110/200] batch [30/51] time 0.498 (0.462) data 0.367 (0.331) loss_x loss_x 1.1006 (1.2156) acc_x 75.0000 (69.2708) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [35/51] time 0.434 (0.464) data 0.303 (0.333) loss_x loss_x 0.9512 (1.1963) acc_x 78.1250 (69.7321) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [40/51] time 0.564 (0.471) data 0.433 (0.340) loss_x loss_x 1.0693 (1.2017) acc_x 81.2500 (69.9219) lr 8.7467e-04 eta 0:00:05
epoch [110/200] batch [45/51] time 0.419 (0.471) data 0.288 (0.340) loss_x loss_x 0.9966 (1.1847) acc_x 62.5000 (70.0694) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [50/51] time 0.434 (0.467) data 0.303 (0.336) loss_x loss_x 1.2471 (1.1814) acc_x 65.6250 (70.0625) lr 8.7467e-04 eta 0:00:00
epoch [110/200] batch [5/46] time 0.397 (0.464) data 0.264 (0.333) loss_u loss_u 0.7217 (0.7475) acc_u 31.2500 (31.8750) lr 8.7467e-04 eta 0:00:19
epoch [110/200] batch [10/46] time 0.456 (0.463) data 0.324 (0.331) loss_u loss_u 0.8579 (0.7733) acc_u 15.6250 (27.5000) lr 8.7467e-04 eta 0:00:16
epoch [110/200] batch [15/46] time 0.365 (0.458) data 0.233 (0.326) loss_u loss_u 0.8525 (0.7729) acc_u 12.5000 (26.8750) lr 8.7467e-04 eta 0:00:14
epoch [110/200] batch [20/46] time 0.635 (0.462) data 0.503 (0.330) loss_u loss_u 0.7642 (0.7816) acc_u 31.2500 (26.5625) lr 8.7467e-04 eta 0:00:12
epoch [110/200] batch [25/46] time 0.480 (0.458) data 0.349 (0.327) loss_u loss_u 0.7466 (0.7785) acc_u 31.2500 (27.0000) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [30/46] time 0.426 (0.453) data 0.295 (0.321) loss_u loss_u 0.7134 (0.7741) acc_u 37.5000 (27.2917) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [35/46] time 0.437 (0.454) data 0.306 (0.322) loss_u loss_u 0.7891 (0.7761) acc_u 25.0000 (27.5000) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [40/46] time 0.460 (0.453) data 0.328 (0.321) loss_u loss_u 0.7988 (0.7783) acc_u 21.8750 (27.3438) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [45/46] time 0.380 (0.452) data 0.248 (0.321) loss_u loss_u 0.8071 (0.7794) acc_u 28.1250 (27.2917) lr 8.7467e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1279
confident_label rate tensor(0.5293, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1660
clean true:1659
clean false:1
clean_rate:0.9993975903614458
noisy true:198
noisy false:1278
after delete: len(clean_dataset) 1660
after delete: len(noisy_dataset) 1476
epoch [111/200] batch [5/51] time 0.372 (0.458) data 0.241 (0.327) loss_x loss_x 1.2021 (1.3641) acc_x 71.8750 (66.2500) lr 8.5910e-04 eta 0:00:21
epoch [111/200] batch [10/51] time 0.387 (0.461) data 0.256 (0.330) loss_x loss_x 1.3008 (1.2804) acc_x 68.7500 (68.4375) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [15/51] time 0.499 (0.467) data 0.369 (0.336) loss_x loss_x 0.7749 (1.1483) acc_x 81.2500 (71.2500) lr 8.5910e-04 eta 0:00:16
epoch [111/200] batch [20/51] time 0.547 (0.458) data 0.416 (0.327) loss_x loss_x 1.3086 (1.1715) acc_x 62.5000 (70.3125) lr 8.5910e-04 eta 0:00:14
epoch [111/200] batch [25/51] time 0.544 (0.460) data 0.413 (0.328) loss_x loss_x 0.8867 (1.1039) acc_x 68.7500 (71.6250) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [30/51] time 0.448 (0.459) data 0.317 (0.328) loss_x loss_x 0.5830 (1.0969) acc_x 90.6250 (72.3958) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [35/51] time 0.451 (0.456) data 0.319 (0.325) loss_x loss_x 0.9565 (1.1026) acc_x 71.8750 (72.3214) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [40/51] time 0.468 (0.465) data 0.336 (0.334) loss_x loss_x 0.9917 (1.1047) acc_x 68.7500 (72.1875) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [45/51] time 0.348 (0.460) data 0.217 (0.329) loss_x loss_x 0.7397 (1.0797) acc_x 81.2500 (72.9167) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [50/51] time 0.437 (0.456) data 0.306 (0.324) loss_x loss_x 1.0938 (1.0840) acc_x 68.7500 (72.4375) lr 8.5910e-04 eta 0:00:00
epoch [111/200] batch [5/46] time 0.390 (0.452) data 0.258 (0.321) loss_u loss_u 0.6938 (0.7588) acc_u 43.7500 (31.8750) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [10/46] time 0.453 (0.448) data 0.321 (0.317) loss_u loss_u 0.7607 (0.7791) acc_u 25.0000 (29.0625) lr 8.5910e-04 eta 0:00:16
epoch [111/200] batch [15/46] time 0.375 (0.445) data 0.242 (0.314) loss_u loss_u 0.8252 (0.7832) acc_u 25.0000 (28.5417) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [20/46] time 0.380 (0.448) data 0.247 (0.317) loss_u loss_u 0.7930 (0.7702) acc_u 25.0000 (30.1562) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [25/46] time 0.500 (0.447) data 0.368 (0.316) loss_u loss_u 0.8110 (0.7762) acc_u 21.8750 (29.7500) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [30/46] time 0.471 (0.448) data 0.339 (0.316) loss_u loss_u 0.8345 (0.7810) acc_u 18.7500 (28.6458) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [35/46] time 0.521 (0.448) data 0.389 (0.316) loss_u loss_u 0.7881 (0.7806) acc_u 31.2500 (28.5714) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [40/46] time 0.624 (0.448) data 0.493 (0.317) loss_u loss_u 0.7383 (0.7777) acc_u 31.2500 (28.5938) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [45/46] time 0.360 (0.445) data 0.228 (0.314) loss_u loss_u 0.8315 (0.7804) acc_u 25.0000 (28.4722) lr 8.5910e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1308
confident_label rate tensor(0.5261, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1650
clean true:1648
clean false:2
clean_rate:0.9987878787878788
noisy true:180
noisy false:1306
after delete: len(clean_dataset) 1650
after delete: len(noisy_dataset) 1486
epoch [112/200] batch [5/51] time 0.400 (0.433) data 0.270 (0.302) loss_x loss_x 1.2393 (1.3898) acc_x 62.5000 (63.7500) lr 8.4357e-04 eta 0:00:19
epoch [112/200] batch [10/51] time 0.334 (0.440) data 0.203 (0.310) loss_x loss_x 1.4248 (1.2819) acc_x 68.7500 (66.8750) lr 8.4357e-04 eta 0:00:18
epoch [112/200] batch [15/51] time 0.480 (0.453) data 0.349 (0.322) loss_x loss_x 0.9009 (1.2207) acc_x 71.8750 (68.7500) lr 8.4357e-04 eta 0:00:16
epoch [112/200] batch [20/51] time 0.353 (0.460) data 0.222 (0.329) loss_x loss_x 1.3955 (1.2198) acc_x 65.6250 (68.2812) lr 8.4357e-04 eta 0:00:14
epoch [112/200] batch [25/51] time 0.423 (0.449) data 0.292 (0.318) loss_x loss_x 1.1680 (1.1980) acc_x 75.0000 (69.5000) lr 8.4357e-04 eta 0:00:11
epoch [112/200] batch [30/51] time 0.372 (0.453) data 0.242 (0.322) loss_x loss_x 1.0742 (1.1860) acc_x 81.2500 (70.3125) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [35/51] time 0.445 (0.452) data 0.314 (0.321) loss_x loss_x 1.5811 (1.1748) acc_x 62.5000 (70.9821) lr 8.4357e-04 eta 0:00:07
epoch [112/200] batch [40/51] time 0.441 (0.449) data 0.310 (0.318) loss_x loss_x 1.3555 (1.1710) acc_x 65.6250 (70.8594) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [45/51] time 0.380 (0.444) data 0.249 (0.313) loss_x loss_x 1.1543 (1.1657) acc_x 71.8750 (70.6944) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [50/51] time 0.426 (0.446) data 0.295 (0.315) loss_x loss_x 1.5244 (1.1829) acc_x 62.5000 (69.9375) lr 8.4357e-04 eta 0:00:00
epoch [112/200] batch [5/46] time 0.426 (0.445) data 0.294 (0.314) loss_u loss_u 0.7915 (0.7960) acc_u 21.8750 (25.6250) lr 8.4357e-04 eta 0:00:18
epoch [112/200] batch [10/46] time 0.475 (0.440) data 0.344 (0.309) loss_u loss_u 0.8550 (0.7953) acc_u 18.7500 (26.5625) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [15/46] time 0.665 (0.445) data 0.533 (0.313) loss_u loss_u 0.7505 (0.7729) acc_u 34.3750 (29.5833) lr 8.4357e-04 eta 0:00:13
epoch [112/200] batch [20/46] time 0.398 (0.443) data 0.267 (0.312) loss_u loss_u 0.8037 (0.7743) acc_u 21.8750 (29.3750) lr 8.4357e-04 eta 0:00:11
epoch [112/200] batch [25/46] time 0.450 (0.441) data 0.317 (0.310) loss_u loss_u 0.7632 (0.7678) acc_u 34.3750 (30.1250) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [30/46] time 0.348 (0.442) data 0.216 (0.310) loss_u loss_u 0.6616 (0.7661) acc_u 46.8750 (30.5208) lr 8.4357e-04 eta 0:00:07
epoch [112/200] batch [35/46] time 0.472 (0.445) data 0.340 (0.314) loss_u loss_u 0.7988 (0.7676) acc_u 21.8750 (30.3571) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [40/46] time 0.522 (0.445) data 0.390 (0.314) loss_u loss_u 0.9058 (0.7701) acc_u 9.3750 (29.6875) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [45/46] time 0.349 (0.446) data 0.217 (0.315) loss_u loss_u 0.7002 (0.7662) acc_u 40.6250 (30.3472) lr 8.4357e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1296
confident_label rate tensor(0.5303, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1663
clean true:1660
clean false:3
clean_rate:0.9981960312687913
noisy true:180
noisy false:1293
after delete: len(clean_dataset) 1663
after delete: len(noisy_dataset) 1473
epoch [113/200] batch [5/51] time 0.325 (0.384) data 0.194 (0.253) loss_x loss_x 0.9365 (1.0892) acc_x 78.1250 (75.0000) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [10/51] time 0.418 (0.420) data 0.286 (0.289) loss_x loss_x 1.0859 (1.1789) acc_x 81.2500 (72.1875) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [15/51] time 0.404 (0.434) data 0.273 (0.303) loss_x loss_x 1.3301 (1.1692) acc_x 68.7500 (72.2917) lr 8.2807e-04 eta 0:00:15
epoch [113/200] batch [20/51] time 0.431 (0.452) data 0.300 (0.321) loss_x loss_x 1.4902 (1.1456) acc_x 71.8750 (72.9688) lr 8.2807e-04 eta 0:00:14
epoch [113/200] batch [25/51] time 0.421 (0.452) data 0.290 (0.321) loss_x loss_x 1.0791 (1.1368) acc_x 71.8750 (73.0000) lr 8.2807e-04 eta 0:00:11
epoch [113/200] batch [30/51] time 0.348 (0.441) data 0.217 (0.310) loss_x loss_x 1.0811 (1.1156) acc_x 71.8750 (73.2292) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [35/51] time 0.378 (0.448) data 0.247 (0.318) loss_x loss_x 0.8037 (1.0932) acc_x 84.3750 (73.6607) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [40/51] time 0.532 (0.450) data 0.400 (0.319) loss_x loss_x 1.4092 (1.0864) acc_x 59.3750 (73.4375) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [45/51] time 0.428 (0.451) data 0.298 (0.320) loss_x loss_x 1.2461 (1.0933) acc_x 68.7500 (73.1944) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [50/51] time 0.515 (0.451) data 0.384 (0.320) loss_x loss_x 1.1553 (1.1043) acc_x 65.6250 (72.9375) lr 8.2807e-04 eta 0:00:00
epoch [113/200] batch [5/46] time 0.376 (0.451) data 0.245 (0.320) loss_u loss_u 0.6758 (0.7428) acc_u 43.7500 (35.6250) lr 8.2807e-04 eta 0:00:18
epoch [113/200] batch [10/46] time 0.394 (0.449) data 0.262 (0.318) loss_u loss_u 0.7681 (0.7297) acc_u 34.3750 (36.5625) lr 8.2807e-04 eta 0:00:16
epoch [113/200] batch [15/46] time 0.653 (0.450) data 0.521 (0.319) loss_u loss_u 0.7627 (0.7383) acc_u 31.2500 (35.6250) lr 8.2807e-04 eta 0:00:13
epoch [113/200] batch [20/46] time 0.387 (0.447) data 0.256 (0.316) loss_u loss_u 0.8540 (0.7556) acc_u 25.0000 (32.6562) lr 8.2807e-04 eta 0:00:11
epoch [113/200] batch [25/46] time 0.381 (0.446) data 0.249 (0.315) loss_u loss_u 0.7676 (0.7445) acc_u 37.5000 (33.8750) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [30/46] time 0.491 (0.449) data 0.359 (0.318) loss_u loss_u 0.7715 (0.7521) acc_u 28.1250 (32.3958) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [35/46] time 0.395 (0.446) data 0.262 (0.315) loss_u loss_u 0.7607 (0.7542) acc_u 28.1250 (32.0536) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [40/46] time 0.428 (0.444) data 0.297 (0.313) loss_u loss_u 0.8379 (0.7565) acc_u 18.7500 (31.4062) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [45/46] time 0.357 (0.440) data 0.226 (0.309) loss_u loss_u 0.8130 (0.7588) acc_u 28.1250 (31.1806) lr 8.2807e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1336
confident_label rate tensor(0.5201, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1631
clean true:1630
clean false:1
clean_rate:0.9993868792152054
noisy true:170
noisy false:1335
after delete: len(clean_dataset) 1631
after delete: len(noisy_dataset) 1505
epoch [114/200] batch [5/50] time 0.591 (0.493) data 0.460 (0.362) loss_x loss_x 1.1543 (1.1584) acc_x 78.1250 (75.6250) lr 8.1262e-04 eta 0:00:22
epoch [114/200] batch [10/50] time 0.403 (0.488) data 0.272 (0.357) loss_x loss_x 1.4512 (1.2391) acc_x 71.8750 (73.4375) lr 8.1262e-04 eta 0:00:19
epoch [114/200] batch [15/50] time 0.536 (0.484) data 0.405 (0.354) loss_x loss_x 1.8125 (1.2358) acc_x 62.5000 (73.5417) lr 8.1262e-04 eta 0:00:16
epoch [114/200] batch [20/50] time 0.432 (0.473) data 0.300 (0.342) loss_x loss_x 0.8853 (1.1732) acc_x 78.1250 (73.1250) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [25/50] time 0.536 (0.472) data 0.405 (0.341) loss_x loss_x 0.7339 (1.1340) acc_x 84.3750 (73.1250) lr 8.1262e-04 eta 0:00:11
epoch [114/200] batch [30/50] time 0.453 (0.471) data 0.322 (0.340) loss_x loss_x 0.8882 (1.1452) acc_x 68.7500 (72.0833) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [35/50] time 0.427 (0.460) data 0.296 (0.329) loss_x loss_x 1.6396 (1.1415) acc_x 68.7500 (72.4107) lr 8.1262e-04 eta 0:00:06
epoch [114/200] batch [40/50] time 0.426 (0.463) data 0.295 (0.332) loss_x loss_x 0.9585 (1.1376) acc_x 75.0000 (72.4219) lr 8.1262e-04 eta 0:00:04
epoch [114/200] batch [45/50] time 0.364 (0.459) data 0.233 (0.328) loss_x loss_x 1.0029 (1.1261) acc_x 68.7500 (72.4306) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [50/50] time 0.588 (0.460) data 0.456 (0.329) loss_x loss_x 1.0479 (1.1212) acc_x 65.6250 (72.5625) lr 8.1262e-04 eta 0:00:00
epoch [114/200] batch [5/47] time 0.405 (0.457) data 0.274 (0.326) loss_u loss_u 0.7285 (0.7820) acc_u 31.2500 (26.2500) lr 8.1262e-04 eta 0:00:19
epoch [114/200] batch [10/47] time 0.572 (0.461) data 0.440 (0.330) loss_u loss_u 0.7944 (0.7694) acc_u 25.0000 (27.8125) lr 8.1262e-04 eta 0:00:17
epoch [114/200] batch [15/47] time 0.437 (0.457) data 0.305 (0.325) loss_u loss_u 0.7847 (0.7554) acc_u 28.1250 (30.4167) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [20/47] time 0.491 (0.460) data 0.360 (0.328) loss_u loss_u 0.8032 (0.7598) acc_u 28.1250 (30.7812) lr 8.1262e-04 eta 0:00:12
epoch [114/200] batch [25/47] time 0.487 (0.460) data 0.356 (0.329) loss_u loss_u 0.7041 (0.7570) acc_u 31.2500 (31.0000) lr 8.1262e-04 eta 0:00:10
epoch [114/200] batch [30/47] time 0.420 (0.461) data 0.286 (0.329) loss_u loss_u 0.8716 (0.7628) acc_u 12.5000 (29.8958) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [35/47] time 0.344 (0.454) data 0.211 (0.323) loss_u loss_u 0.7544 (0.7628) acc_u 31.2500 (30.0000) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [40/47] time 0.422 (0.451) data 0.290 (0.320) loss_u loss_u 0.7183 (0.7618) acc_u 34.3750 (30.4688) lr 8.1262e-04 eta 0:00:03
epoch [114/200] batch [45/47] time 0.431 (0.451) data 0.299 (0.320) loss_u loss_u 0.8267 (0.7674) acc_u 21.8750 (29.6528) lr 8.1262e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1317
confident_label rate tensor(0.5223, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1638
clean true:1636
clean false:2
clean_rate:0.9987789987789988
noisy true:183
noisy false:1315
after delete: len(clean_dataset) 1638
after delete: len(noisy_dataset) 1498
epoch [115/200] batch [5/51] time 0.448 (0.433) data 0.317 (0.302) loss_x loss_x 1.1494 (1.1229) acc_x 71.8750 (76.2500) lr 7.9721e-04 eta 0:00:19
epoch [115/200] batch [10/51] time 0.465 (0.442) data 0.334 (0.311) loss_x loss_x 0.8926 (1.1454) acc_x 75.0000 (73.4375) lr 7.9721e-04 eta 0:00:18
epoch [115/200] batch [15/51] time 0.403 (0.437) data 0.272 (0.305) loss_x loss_x 1.2295 (1.1935) acc_x 65.6250 (72.2917) lr 7.9721e-04 eta 0:00:15
epoch [115/200] batch [20/51] time 0.456 (0.447) data 0.325 (0.316) loss_x loss_x 1.0869 (1.1845) acc_x 78.1250 (70.9375) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [25/51] time 0.435 (0.448) data 0.303 (0.317) loss_x loss_x 1.0889 (1.1994) acc_x 75.0000 (70.8750) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [30/51] time 0.436 (0.459) data 0.304 (0.328) loss_x loss_x 1.1855 (1.1975) acc_x 75.0000 (70.6250) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [35/51] time 0.411 (0.453) data 0.279 (0.322) loss_x loss_x 1.0742 (1.1817) acc_x 75.0000 (70.8036) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [40/51] time 0.423 (0.449) data 0.293 (0.318) loss_x loss_x 0.8833 (1.1696) acc_x 75.0000 (71.0938) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [45/51] time 0.461 (0.445) data 0.329 (0.314) loss_x loss_x 1.3340 (1.1825) acc_x 65.6250 (70.8333) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [50/51] time 0.466 (0.455) data 0.334 (0.324) loss_x loss_x 1.1846 (1.1667) acc_x 71.8750 (71.3750) lr 7.9721e-04 eta 0:00:00
epoch [115/200] batch [5/46] time 0.427 (0.449) data 0.296 (0.318) loss_u loss_u 0.6943 (0.7577) acc_u 50.0000 (33.1250) lr 7.9721e-04 eta 0:00:18
epoch [115/200] batch [10/46] time 0.396 (0.450) data 0.264 (0.319) loss_u loss_u 0.7231 (0.7335) acc_u 34.3750 (35.9375) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [15/46] time 0.461 (0.449) data 0.329 (0.318) loss_u loss_u 0.7666 (0.7418) acc_u 25.0000 (35.2083) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [20/46] time 0.443 (0.450) data 0.311 (0.319) loss_u loss_u 0.7690 (0.7499) acc_u 34.3750 (33.7500) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [25/46] time 0.480 (0.452) data 0.348 (0.320) loss_u loss_u 0.9058 (0.7512) acc_u 12.5000 (32.8750) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [30/46] time 0.448 (0.450) data 0.316 (0.319) loss_u loss_u 0.8257 (0.7463) acc_u 18.7500 (33.4375) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [35/46] time 0.357 (0.446) data 0.226 (0.315) loss_u loss_u 0.7817 (0.7543) acc_u 31.2500 (32.0536) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [40/46] time 0.351 (0.446) data 0.219 (0.315) loss_u loss_u 0.6851 (0.7520) acc_u 37.5000 (32.1094) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [45/46] time 0.534 (0.446) data 0.401 (0.315) loss_u loss_u 0.8027 (0.7518) acc_u 21.8750 (32.0139) lr 7.9721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1324
confident_label rate tensor(0.5214, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1635
clean true:1634
clean false:1
clean_rate:0.999388379204893
noisy true:178
noisy false:1323
after delete: len(clean_dataset) 1635
after delete: len(noisy_dataset) 1501
epoch [116/200] batch [5/51] time 0.343 (0.442) data 0.212 (0.311) loss_x loss_x 1.1182 (0.8979) acc_x 71.8750 (78.1250) lr 7.8186e-04 eta 0:00:20
epoch [116/200] batch [10/51] time 0.506 (0.451) data 0.375 (0.320) loss_x loss_x 0.8706 (1.0253) acc_x 78.1250 (73.7500) lr 7.8186e-04 eta 0:00:18
epoch [116/200] batch [15/51] time 0.425 (0.458) data 0.295 (0.327) loss_x loss_x 0.9404 (1.0360) acc_x 71.8750 (72.5000) lr 7.8186e-04 eta 0:00:16
epoch [116/200] batch [20/51] time 0.380 (0.443) data 0.250 (0.312) loss_x loss_x 0.9595 (1.0333) acc_x 81.2500 (72.6562) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [25/51] time 0.542 (0.445) data 0.411 (0.314) loss_x loss_x 0.9194 (1.0417) acc_x 81.2500 (73.1250) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [30/51] time 0.418 (0.444) data 0.288 (0.313) loss_x loss_x 1.1865 (1.0639) acc_x 75.0000 (72.9167) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [35/51] time 0.382 (0.442) data 0.251 (0.311) loss_x loss_x 0.9785 (1.0514) acc_x 78.1250 (73.1250) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [40/51] time 0.501 (0.442) data 0.369 (0.311) loss_x loss_x 1.2354 (1.0761) acc_x 68.7500 (72.4219) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [45/51] time 0.404 (0.445) data 0.272 (0.314) loss_x loss_x 1.1221 (1.0752) acc_x 75.0000 (72.2222) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [50/51] time 0.592 (0.450) data 0.461 (0.319) loss_x loss_x 1.4258 (1.1013) acc_x 71.8750 (71.6875) lr 7.8186e-04 eta 0:00:00
epoch [116/200] batch [5/46] time 0.509 (0.456) data 0.378 (0.325) loss_u loss_u 0.7676 (0.7684) acc_u 25.0000 (27.5000) lr 7.8186e-04 eta 0:00:18
epoch [116/200] batch [10/46] time 0.510 (0.455) data 0.378 (0.324) loss_u loss_u 0.8687 (0.7863) acc_u 15.6250 (26.8750) lr 7.8186e-04 eta 0:00:16
epoch [116/200] batch [15/46] time 0.470 (0.455) data 0.339 (0.324) loss_u loss_u 0.7432 (0.7729) acc_u 34.3750 (30.0000) lr 7.8186e-04 eta 0:00:14
epoch [116/200] batch [20/46] time 0.658 (0.459) data 0.527 (0.328) loss_u loss_u 0.7422 (0.7621) acc_u 34.3750 (32.0312) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [25/46] time 0.438 (0.454) data 0.306 (0.323) loss_u loss_u 0.7812 (0.7645) acc_u 37.5000 (31.8750) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [30/46] time 0.375 (0.451) data 0.244 (0.320) loss_u loss_u 0.8574 (0.7646) acc_u 21.8750 (31.8750) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [35/46] time 0.378 (0.448) data 0.247 (0.317) loss_u loss_u 0.7026 (0.7612) acc_u 43.7500 (32.4107) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [40/46] time 0.450 (0.445) data 0.319 (0.314) loss_u loss_u 0.7471 (0.7642) acc_u 37.5000 (31.8750) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [45/46] time 0.314 (0.443) data 0.182 (0.312) loss_u loss_u 0.7534 (0.7660) acc_u 28.1250 (31.3889) lr 7.8186e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1339
confident_label rate tensor(0.5153, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1616
clean true:1615
clean false:1
clean_rate:0.9993811881188119
noisy true:182
noisy false:1338
after delete: len(clean_dataset) 1616
after delete: len(noisy_dataset) 1520
epoch [117/200] batch [5/50] time 0.610 (0.518) data 0.479 (0.387) loss_x loss_x 1.0615 (1.1459) acc_x 68.7500 (70.0000) lr 7.6655e-04 eta 0:00:23
epoch [117/200] batch [10/50] time 0.416 (0.520) data 0.285 (0.388) loss_x loss_x 1.6670 (1.1844) acc_x 68.7500 (70.9375) lr 7.6655e-04 eta 0:00:20
epoch [117/200] batch [15/50] time 0.410 (0.498) data 0.278 (0.367) loss_x loss_x 1.4658 (1.1218) acc_x 68.7500 (72.9167) lr 7.6655e-04 eta 0:00:17
epoch [117/200] batch [20/50] time 0.472 (0.476) data 0.338 (0.345) loss_x loss_x 0.6611 (1.1595) acc_x 84.3750 (71.8750) lr 7.6655e-04 eta 0:00:14
epoch [117/200] batch [25/50] time 0.473 (0.475) data 0.342 (0.343) loss_x loss_x 1.6309 (1.1813) acc_x 71.8750 (72.1250) lr 7.6655e-04 eta 0:00:11
epoch [117/200] batch [30/50] time 0.538 (0.479) data 0.407 (0.347) loss_x loss_x 1.1035 (1.1473) acc_x 65.6250 (72.6042) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [35/50] time 0.463 (0.486) data 0.332 (0.354) loss_x loss_x 1.1855 (1.1310) acc_x 65.6250 (73.0357) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [40/50] time 0.420 (0.487) data 0.289 (0.355) loss_x loss_x 1.0781 (1.1585) acc_x 75.0000 (72.2656) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [45/50] time 0.456 (0.485) data 0.325 (0.354) loss_x loss_x 0.7329 (1.1553) acc_x 68.7500 (71.3194) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [50/50] time 0.419 (0.481) data 0.288 (0.350) loss_x loss_x 0.9897 (1.1600) acc_x 62.5000 (70.6250) lr 7.6655e-04 eta 0:00:00
epoch [117/200] batch [5/47] time 0.455 (0.477) data 0.323 (0.345) loss_u loss_u 0.7705 (0.7625) acc_u 28.1250 (29.3750) lr 7.6655e-04 eta 0:00:20
epoch [117/200] batch [10/47] time 0.525 (0.475) data 0.394 (0.344) loss_u loss_u 0.7529 (0.7592) acc_u 37.5000 (30.0000) lr 7.6655e-04 eta 0:00:17
epoch [117/200] batch [15/47] time 0.481 (0.473) data 0.349 (0.342) loss_u loss_u 0.7788 (0.7701) acc_u 31.2500 (29.1667) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [20/47] time 0.420 (0.467) data 0.289 (0.335) loss_u loss_u 0.8555 (0.7628) acc_u 12.5000 (29.8438) lr 7.6655e-04 eta 0:00:12
epoch [117/200] batch [25/47] time 0.345 (0.462) data 0.213 (0.331) loss_u loss_u 0.7578 (0.7670) acc_u 31.2500 (28.8750) lr 7.6655e-04 eta 0:00:10
epoch [117/200] batch [30/47] time 0.555 (0.462) data 0.424 (0.331) loss_u loss_u 0.7798 (0.7680) acc_u 28.1250 (28.7500) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [35/47] time 0.412 (0.458) data 0.280 (0.327) loss_u loss_u 0.8037 (0.7737) acc_u 25.0000 (28.2143) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [40/47] time 0.556 (0.459) data 0.424 (0.328) loss_u loss_u 0.6343 (0.7694) acc_u 40.6250 (28.8281) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [45/47] time 0.462 (0.461) data 0.331 (0.329) loss_u loss_u 0.8208 (0.7677) acc_u 28.1250 (29.5139) lr 7.6655e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1331
confident_label rate tensor(0.5188, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1627
clean true:1624
clean false:3
clean_rate:0.9981561155500922
noisy true:181
noisy false:1328
after delete: len(clean_dataset) 1627
after delete: len(noisy_dataset) 1509
epoch [118/200] batch [5/50] time 0.503 (0.476) data 0.372 (0.344) loss_x loss_x 0.7852 (0.9334) acc_x 81.2500 (76.2500) lr 7.5131e-04 eta 0:00:21
epoch [118/200] batch [10/50] time 0.534 (0.474) data 0.403 (0.342) loss_x loss_x 1.0605 (1.0930) acc_x 78.1250 (72.8125) lr 7.5131e-04 eta 0:00:18
epoch [118/200] batch [15/50] time 0.400 (0.458) data 0.269 (0.327) loss_x loss_x 1.3877 (1.1462) acc_x 71.8750 (70.8333) lr 7.5131e-04 eta 0:00:16
epoch [118/200] batch [20/50] time 0.609 (0.456) data 0.478 (0.325) loss_x loss_x 1.2275 (1.1004) acc_x 62.5000 (72.0312) lr 7.5131e-04 eta 0:00:13
epoch [118/200] batch [25/50] time 0.507 (0.449) data 0.377 (0.318) loss_x loss_x 0.9976 (1.0907) acc_x 84.3750 (73.3750) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [30/50] time 0.700 (0.463) data 0.569 (0.332) loss_x loss_x 1.3750 (1.0984) acc_x 59.3750 (73.7500) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [35/50] time 0.459 (0.464) data 0.327 (0.333) loss_x loss_x 1.1035 (1.1035) acc_x 75.0000 (73.5714) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [40/50] time 0.340 (0.457) data 0.209 (0.326) loss_x loss_x 0.8276 (1.0839) acc_x 78.1250 (73.9062) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [45/50] time 0.395 (0.461) data 0.263 (0.330) loss_x loss_x 0.9561 (1.0792) acc_x 81.2500 (73.9583) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [50/50] time 0.432 (0.459) data 0.300 (0.328) loss_x loss_x 0.9653 (1.0660) acc_x 78.1250 (74.3750) lr 7.5131e-04 eta 0:00:00
epoch [118/200] batch [5/47] time 0.473 (0.451) data 0.341 (0.320) loss_u loss_u 0.8301 (0.7335) acc_u 28.1250 (36.8750) lr 7.5131e-04 eta 0:00:18
epoch [118/200] batch [10/47] time 0.416 (0.450) data 0.284 (0.318) loss_u loss_u 0.8027 (0.7569) acc_u 21.8750 (30.6250) lr 7.5131e-04 eta 0:00:16
epoch [118/200] batch [15/47] time 0.471 (0.453) data 0.340 (0.322) loss_u loss_u 0.7441 (0.7592) acc_u 31.2500 (29.5833) lr 7.5131e-04 eta 0:00:14
epoch [118/200] batch [20/47] time 0.382 (0.452) data 0.251 (0.321) loss_u loss_u 0.7515 (0.7606) acc_u 31.2500 (29.6875) lr 7.5131e-04 eta 0:00:12
epoch [118/200] batch [25/47] time 0.415 (0.448) data 0.283 (0.317) loss_u loss_u 0.7749 (0.7649) acc_u 31.2500 (29.1250) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [30/47] time 0.448 (0.445) data 0.315 (0.314) loss_u loss_u 0.8042 (0.7611) acc_u 21.8750 (29.3750) lr 7.5131e-04 eta 0:00:07
epoch [118/200] batch [35/47] time 0.461 (0.445) data 0.328 (0.314) loss_u loss_u 0.7573 (0.7628) acc_u 37.5000 (29.7321) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [40/47] time 0.376 (0.446) data 0.243 (0.314) loss_u loss_u 0.7949 (0.7646) acc_u 25.0000 (29.5312) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [45/47] time 0.506 (0.450) data 0.375 (0.318) loss_u loss_u 0.7158 (0.7627) acc_u 40.6250 (30.0694) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1327
confident_label rate tensor(0.5191, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1628
clean true:1628
clean false:0
clean_rate:1.0
noisy true:181
noisy false:1327
after delete: len(clean_dataset) 1628
after delete: len(noisy_dataset) 1508
epoch [119/200] batch [5/50] time 0.501 (0.441) data 0.370 (0.309) loss_x loss_x 1.0762 (1.1006) acc_x 68.7500 (73.1250) lr 7.3613e-04 eta 0:00:19
epoch [119/200] batch [10/50] time 0.550 (0.433) data 0.418 (0.301) loss_x loss_x 0.5576 (1.0981) acc_x 90.6250 (73.7500) lr 7.3613e-04 eta 0:00:17
epoch [119/200] batch [15/50] time 0.375 (0.427) data 0.244 (0.296) loss_x loss_x 0.8809 (1.1142) acc_x 71.8750 (73.3333) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [20/50] time 0.398 (0.425) data 0.267 (0.293) loss_x loss_x 0.6475 (1.1253) acc_x 87.5000 (73.4375) lr 7.3613e-04 eta 0:00:12
epoch [119/200] batch [25/50] time 0.487 (0.434) data 0.356 (0.303) loss_x loss_x 1.2510 (1.1137) acc_x 59.3750 (72.8750) lr 7.3613e-04 eta 0:00:10
epoch [119/200] batch [30/50] time 0.395 (0.433) data 0.264 (0.301) loss_x loss_x 1.1426 (1.1209) acc_x 71.8750 (72.2917) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [35/50] time 0.362 (0.432) data 0.231 (0.301) loss_x loss_x 0.9922 (1.1082) acc_x 68.7500 (72.4107) lr 7.3613e-04 eta 0:00:06
epoch [119/200] batch [40/50] time 0.468 (0.435) data 0.337 (0.304) loss_x loss_x 0.7773 (1.1077) acc_x 78.1250 (72.5000) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [45/50] time 0.435 (0.444) data 0.304 (0.313) loss_x loss_x 1.2588 (1.1296) acc_x 68.7500 (71.5972) lr 7.3613e-04 eta 0:00:02
epoch [119/200] batch [50/50] time 0.377 (0.445) data 0.246 (0.313) loss_x loss_x 0.8887 (1.1267) acc_x 78.1250 (71.7500) lr 7.3613e-04 eta 0:00:00
epoch [119/200] batch [5/47] time 0.499 (0.443) data 0.366 (0.312) loss_u loss_u 0.8018 (0.7684) acc_u 25.0000 (29.3750) lr 7.3613e-04 eta 0:00:18
epoch [119/200] batch [10/47] time 0.495 (0.444) data 0.362 (0.313) loss_u loss_u 0.7090 (0.7588) acc_u 40.6250 (32.1875) lr 7.3613e-04 eta 0:00:16
epoch [119/200] batch [15/47] time 0.564 (0.445) data 0.433 (0.313) loss_u loss_u 0.7378 (0.7520) acc_u 43.7500 (34.3750) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [20/47] time 0.424 (0.443) data 0.292 (0.311) loss_u loss_u 0.7100 (0.7562) acc_u 40.6250 (33.4375) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [25/47] time 0.358 (0.442) data 0.227 (0.311) loss_u loss_u 0.7847 (0.7590) acc_u 25.0000 (32.0000) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [30/47] time 0.384 (0.444) data 0.252 (0.312) loss_u loss_u 0.8081 (0.7601) acc_u 25.0000 (31.2500) lr 7.3613e-04 eta 0:00:07
epoch [119/200] batch [35/47] time 0.430 (0.443) data 0.297 (0.311) loss_u loss_u 0.7861 (0.7546) acc_u 25.0000 (31.5179) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [40/47] time 0.366 (0.443) data 0.234 (0.311) loss_u loss_u 0.6621 (0.7522) acc_u 40.6250 (31.6406) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [45/47] time 0.545 (0.444) data 0.412 (0.313) loss_u loss_u 0.7783 (0.7550) acc_u 25.0000 (31.1111) lr 7.3613e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1298
confident_label rate tensor(0.5290, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1659
clean true:1658
clean false:1
clean_rate:0.9993972272453285
noisy true:180
noisy false:1297
after delete: len(clean_dataset) 1659
after delete: len(noisy_dataset) 1477
epoch [120/200] batch [5/51] time 0.436 (0.422) data 0.305 (0.291) loss_x loss_x 0.8892 (1.0073) acc_x 75.0000 (74.3750) lr 7.2101e-04 eta 0:00:19
epoch [120/200] batch [10/51] time 0.481 (0.429) data 0.350 (0.298) loss_x loss_x 1.8330 (1.1760) acc_x 53.1250 (70.6250) lr 7.2101e-04 eta 0:00:17
epoch [120/200] batch [15/51] time 0.454 (0.464) data 0.323 (0.333) loss_x loss_x 0.8672 (1.1489) acc_x 71.8750 (70.4167) lr 7.2101e-04 eta 0:00:16
epoch [120/200] batch [20/51] time 0.436 (0.463) data 0.305 (0.332) loss_x loss_x 1.5039 (1.1599) acc_x 62.5000 (70.1562) lr 7.2101e-04 eta 0:00:14
epoch [120/200] batch [25/51] time 0.462 (0.455) data 0.331 (0.323) loss_x loss_x 1.2422 (1.1506) acc_x 78.1250 (70.0000) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [30/51] time 0.433 (0.446) data 0.302 (0.315) loss_x loss_x 1.0576 (1.1663) acc_x 78.1250 (70.0000) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [35/51] time 0.493 (0.449) data 0.362 (0.318) loss_x loss_x 1.2646 (1.1655) acc_x 65.6250 (70.2679) lr 7.2101e-04 eta 0:00:07
epoch [120/200] batch [40/51] time 0.392 (0.448) data 0.261 (0.317) loss_x loss_x 1.2637 (1.1630) acc_x 68.7500 (70.1562) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [45/51] time 0.475 (0.450) data 0.344 (0.319) loss_x loss_x 0.5684 (1.1555) acc_x 87.5000 (70.6250) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [50/51] time 0.496 (0.449) data 0.365 (0.318) loss_x loss_x 1.0430 (1.1530) acc_x 68.7500 (70.2500) lr 7.2101e-04 eta 0:00:00
epoch [120/200] batch [5/46] time 0.411 (0.448) data 0.279 (0.317) loss_u loss_u 0.7417 (0.7796) acc_u 37.5000 (30.6250) lr 7.2101e-04 eta 0:00:18
epoch [120/200] batch [10/46] time 0.381 (0.444) data 0.249 (0.313) loss_u loss_u 0.6235 (0.7416) acc_u 46.8750 (34.3750) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [15/46] time 0.431 (0.444) data 0.299 (0.313) loss_u loss_u 0.7617 (0.7508) acc_u 40.6250 (33.3333) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [20/46] time 0.447 (0.443) data 0.315 (0.312) loss_u loss_u 0.6680 (0.7500) acc_u 43.7500 (32.8125) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [25/46] time 0.422 (0.442) data 0.290 (0.311) loss_u loss_u 0.8433 (0.7574) acc_u 15.6250 (31.7500) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [30/46] time 0.464 (0.445) data 0.331 (0.314) loss_u loss_u 0.7930 (0.7697) acc_u 25.0000 (30.0000) lr 7.2101e-04 eta 0:00:07
epoch [120/200] batch [35/46] time 0.460 (0.444) data 0.327 (0.312) loss_u loss_u 0.7305 (0.7709) acc_u 34.3750 (30.1786) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [40/46] time 0.477 (0.444) data 0.345 (0.313) loss_u loss_u 0.8608 (0.7722) acc_u 18.7500 (29.9219) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [45/46] time 0.350 (0.443) data 0.218 (0.311) loss_u loss_u 0.7607 (0.7727) acc_u 34.3750 (30.1389) lr 7.2101e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1299
confident_label rate tensor(0.5303, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1663
clean true:1660
clean false:3
clean_rate:0.9981960312687913
noisy true:177
noisy false:1296
after delete: len(clean_dataset) 1663
after delete: len(noisy_dataset) 1473
epoch [121/200] batch [5/51] time 0.455 (0.468) data 0.324 (0.336) loss_x loss_x 1.3984 (1.2842) acc_x 56.2500 (65.0000) lr 7.0596e-04 eta 0:00:21
epoch [121/200] batch [10/51] time 0.432 (0.478) data 0.301 (0.347) loss_x loss_x 0.8770 (1.1655) acc_x 71.8750 (71.8750) lr 7.0596e-04 eta 0:00:19
epoch [121/200] batch [15/51] time 0.398 (0.459) data 0.267 (0.328) loss_x loss_x 0.9893 (1.1109) acc_x 78.1250 (72.7083) lr 7.0596e-04 eta 0:00:16
epoch [121/200] batch [20/51] time 0.476 (0.467) data 0.345 (0.336) loss_x loss_x 0.9106 (1.0896) acc_x 84.3750 (74.3750) lr 7.0596e-04 eta 0:00:14
epoch [121/200] batch [25/51] time 0.473 (0.462) data 0.342 (0.331) loss_x loss_x 1.6123 (1.1317) acc_x 53.1250 (73.3750) lr 7.0596e-04 eta 0:00:12
epoch [121/200] batch [30/51] time 0.412 (0.457) data 0.281 (0.326) loss_x loss_x 1.3838 (1.1605) acc_x 65.6250 (72.0833) lr 7.0596e-04 eta 0:00:09
epoch [121/200] batch [35/51] time 0.450 (0.452) data 0.319 (0.321) loss_x loss_x 1.3105 (1.1875) acc_x 71.8750 (70.8929) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [40/51] time 0.427 (0.450) data 0.297 (0.319) loss_x loss_x 1.2451 (1.1759) acc_x 75.0000 (71.4844) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [45/51] time 0.465 (0.452) data 0.334 (0.321) loss_x loss_x 0.7285 (1.1681) acc_x 78.1250 (71.3889) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [50/51] time 0.331 (0.448) data 0.201 (0.317) loss_x loss_x 0.9839 (1.1622) acc_x 71.8750 (71.3125) lr 7.0596e-04 eta 0:00:00
epoch [121/200] batch [5/46] time 0.516 (0.449) data 0.384 (0.318) loss_u loss_u 0.7720 (0.7749) acc_u 34.3750 (29.3750) lr 7.0596e-04 eta 0:00:18
epoch [121/200] batch [10/46] time 0.442 (0.447) data 0.311 (0.316) loss_u loss_u 0.7861 (0.7549) acc_u 25.0000 (30.3125) lr 7.0596e-04 eta 0:00:16
epoch [121/200] batch [15/46] time 0.377 (0.448) data 0.246 (0.317) loss_u loss_u 0.8145 (0.7563) acc_u 28.1250 (30.6250) lr 7.0596e-04 eta 0:00:13
epoch [121/200] batch [20/46] time 0.729 (0.453) data 0.598 (0.322) loss_u loss_u 0.6738 (0.7578) acc_u 46.8750 (30.7812) lr 7.0596e-04 eta 0:00:11
epoch [121/200] batch [25/46] time 0.525 (0.453) data 0.394 (0.322) loss_u loss_u 0.6562 (0.7544) acc_u 46.8750 (31.3750) lr 7.0596e-04 eta 0:00:09
epoch [121/200] batch [30/46] time 0.335 (0.448) data 0.204 (0.317) loss_u loss_u 0.7363 (0.7616) acc_u 31.2500 (30.2083) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [35/46] time 0.351 (0.446) data 0.219 (0.315) loss_u loss_u 0.7480 (0.7644) acc_u 34.3750 (29.8214) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [40/46] time 0.467 (0.449) data 0.335 (0.318) loss_u loss_u 0.8003 (0.7682) acc_u 31.2500 (29.3750) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [45/46] time 0.349 (0.448) data 0.217 (0.317) loss_u loss_u 0.8018 (0.7711) acc_u 21.8750 (28.8194) lr 7.0596e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1309
confident_label rate tensor(0.5255, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1648
clean true:1646
clean false:2
clean_rate:0.9987864077669902
noisy true:181
noisy false:1307
after delete: len(clean_dataset) 1648
after delete: len(noisy_dataset) 1488
epoch [122/200] batch [5/51] time 0.453 (0.493) data 0.322 (0.362) loss_x loss_x 1.0098 (0.9703) acc_x 81.2500 (75.0000) lr 6.9098e-04 eta 0:00:22
epoch [122/200] batch [10/51] time 0.406 (0.470) data 0.275 (0.340) loss_x loss_x 1.0830 (1.0407) acc_x 75.0000 (74.3750) lr 6.9098e-04 eta 0:00:19
epoch [122/200] batch [15/51] time 0.479 (0.461) data 0.349 (0.330) loss_x loss_x 0.7144 (0.9888) acc_x 81.2500 (75.2083) lr 6.9098e-04 eta 0:00:16
epoch [122/200] batch [20/51] time 0.526 (0.455) data 0.395 (0.324) loss_x loss_x 0.8765 (0.9753) acc_x 71.8750 (75.6250) lr 6.9098e-04 eta 0:00:14
epoch [122/200] batch [25/51] time 0.386 (0.455) data 0.255 (0.324) loss_x loss_x 1.5156 (1.0212) acc_x 56.2500 (74.8750) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [30/51] time 0.375 (0.453) data 0.244 (0.322) loss_x loss_x 0.7373 (1.0336) acc_x 81.2500 (74.7917) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [35/51] time 0.387 (0.452) data 0.256 (0.321) loss_x loss_x 1.1172 (1.0506) acc_x 71.8750 (73.7500) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [40/51] time 0.354 (0.448) data 0.223 (0.317) loss_x loss_x 1.6377 (1.0827) acc_x 62.5000 (73.1250) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [45/51] time 0.477 (0.447) data 0.346 (0.316) loss_x loss_x 0.6016 (1.0796) acc_x 81.2500 (73.0556) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [50/51] time 0.449 (0.445) data 0.318 (0.314) loss_x loss_x 0.8340 (1.0749) acc_x 81.2500 (73.4375) lr 6.9098e-04 eta 0:00:00
epoch [122/200] batch [5/46] time 0.517 (0.442) data 0.386 (0.311) loss_u loss_u 0.7212 (0.7523) acc_u 34.3750 (30.6250) lr 6.9098e-04 eta 0:00:18
epoch [122/200] batch [10/46] time 0.458 (0.439) data 0.327 (0.308) loss_u loss_u 0.6143 (0.7397) acc_u 53.1250 (32.5000) lr 6.9098e-04 eta 0:00:15
epoch [122/200] batch [15/46] time 0.453 (0.440) data 0.320 (0.309) loss_u loss_u 0.7524 (0.7502) acc_u 28.1250 (31.4583) lr 6.9098e-04 eta 0:00:13
epoch [122/200] batch [20/46] time 0.502 (0.441) data 0.368 (0.310) loss_u loss_u 0.7900 (0.7661) acc_u 25.0000 (30.0000) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [25/46] time 0.483 (0.444) data 0.350 (0.312) loss_u loss_u 0.7593 (0.7588) acc_u 43.7500 (31.3750) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [30/46] time 0.654 (0.448) data 0.521 (0.317) loss_u loss_u 0.7827 (0.7593) acc_u 28.1250 (31.2500) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [35/46] time 0.698 (0.450) data 0.566 (0.318) loss_u loss_u 0.7920 (0.7650) acc_u 25.0000 (30.3571) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [40/46] time 0.503 (0.450) data 0.371 (0.318) loss_u loss_u 0.7422 (0.7664) acc_u 34.3750 (30.0781) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [45/46] time 0.383 (0.447) data 0.251 (0.316) loss_u loss_u 0.8164 (0.7673) acc_u 21.8750 (30.0000) lr 6.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1289
confident_label rate tensor(0.5341, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1675
clean true:1672
clean false:3
clean_rate:0.9982089552238806
noisy true:175
noisy false:1286
after delete: len(clean_dataset) 1675
after delete: len(noisy_dataset) 1461
epoch [123/200] batch [5/52] time 0.449 (0.479) data 0.318 (0.347) loss_x loss_x 0.9243 (0.9595) acc_x 78.1250 (76.8750) lr 6.7608e-04 eta 0:00:22
epoch [123/200] batch [10/52] time 0.501 (0.468) data 0.368 (0.336) loss_x loss_x 1.2539 (1.0483) acc_x 56.2500 (71.8750) lr 6.7608e-04 eta 0:00:19
epoch [123/200] batch [15/52] time 0.441 (0.478) data 0.309 (0.346) loss_x loss_x 1.6201 (1.0543) acc_x 68.7500 (72.7083) lr 6.7608e-04 eta 0:00:17
epoch [123/200] batch [20/52] time 0.429 (0.483) data 0.297 (0.351) loss_x loss_x 0.7803 (1.0816) acc_x 81.2500 (72.9688) lr 6.7608e-04 eta 0:00:15
epoch [123/200] batch [25/52] time 0.587 (0.475) data 0.455 (0.343) loss_x loss_x 1.2012 (1.0740) acc_x 71.8750 (73.7500) lr 6.7608e-04 eta 0:00:12
epoch [123/200] batch [30/52] time 0.821 (0.484) data 0.688 (0.352) loss_x loss_x 0.8052 (1.0655) acc_x 81.2500 (73.9583) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [35/52] time 0.578 (0.485) data 0.446 (0.353) loss_x loss_x 0.7783 (1.0772) acc_x 84.3750 (73.6607) lr 6.7608e-04 eta 0:00:08
epoch [123/200] batch [40/52] time 0.405 (0.479) data 0.270 (0.347) loss_x loss_x 1.4365 (1.0907) acc_x 68.7500 (73.3594) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [45/52] time 0.607 (0.483) data 0.475 (0.351) loss_x loss_x 1.2412 (1.1092) acc_x 71.8750 (73.0556) lr 6.7608e-04 eta 0:00:03
epoch [123/200] batch [50/52] time 0.610 (0.488) data 0.480 (0.356) loss_x loss_x 1.0645 (1.1217) acc_x 81.2500 (72.5625) lr 6.7608e-04 eta 0:00:00
epoch [123/200] batch [5/45] time 0.435 (0.486) data 0.302 (0.354) loss_u loss_u 0.6787 (0.7537) acc_u 37.5000 (33.1250) lr 6.7608e-04 eta 0:00:19
epoch [123/200] batch [10/45] time 0.419 (0.483) data 0.286 (0.351) loss_u loss_u 0.7495 (0.7482) acc_u 25.0000 (31.5625) lr 6.7608e-04 eta 0:00:16
epoch [123/200] batch [15/45] time 0.313 (0.475) data 0.181 (0.343) loss_u loss_u 0.7954 (0.7586) acc_u 25.0000 (29.5833) lr 6.7608e-04 eta 0:00:14
epoch [123/200] batch [20/45] time 0.418 (0.474) data 0.286 (0.342) loss_u loss_u 0.8306 (0.7611) acc_u 21.8750 (28.7500) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [25/45] time 0.416 (0.472) data 0.284 (0.340) loss_u loss_u 0.7417 (0.7653) acc_u 40.6250 (28.8750) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [30/45] time 0.371 (0.468) data 0.239 (0.336) loss_u loss_u 0.7280 (0.7708) acc_u 34.3750 (28.4375) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [35/45] time 0.461 (0.467) data 0.328 (0.335) loss_u loss_u 0.8477 (0.7700) acc_u 18.7500 (28.6607) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [40/45] time 0.522 (0.466) data 0.389 (0.333) loss_u loss_u 0.8555 (0.7784) acc_u 15.6250 (27.5000) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [45/45] time 0.388 (0.463) data 0.256 (0.331) loss_u loss_u 0.7690 (0.7815) acc_u 31.2500 (27.0833) lr 6.7608e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1303
confident_label rate tensor(0.5306, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1664
clean true:1663
clean false:1
clean_rate:0.9993990384615384
noisy true:170
noisy false:1302
after delete: len(clean_dataset) 1664
after delete: len(noisy_dataset) 1472
epoch [124/200] batch [5/52] time 0.559 (0.506) data 0.428 (0.375) loss_x loss_x 1.1719 (1.1035) acc_x 65.6250 (70.6250) lr 6.6126e-04 eta 0:00:23
epoch [124/200] batch [10/52] time 0.537 (0.472) data 0.405 (0.340) loss_x loss_x 1.0498 (1.0847) acc_x 71.8750 (71.5625) lr 6.6126e-04 eta 0:00:19
epoch [124/200] batch [15/52] time 0.440 (0.462) data 0.309 (0.330) loss_x loss_x 0.9561 (1.0747) acc_x 75.0000 (71.2500) lr 6.6126e-04 eta 0:00:17
epoch [124/200] batch [20/52] time 0.511 (0.465) data 0.380 (0.334) loss_x loss_x 1.2080 (1.0655) acc_x 75.0000 (72.0312) lr 6.6126e-04 eta 0:00:14
epoch [124/200] batch [25/52] time 0.364 (0.457) data 0.233 (0.326) loss_x loss_x 1.5703 (1.0907) acc_x 65.6250 (72.2500) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [30/52] time 0.355 (0.448) data 0.224 (0.317) loss_x loss_x 0.8760 (1.0769) acc_x 75.0000 (72.7083) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [35/52] time 0.467 (0.454) data 0.336 (0.323) loss_x loss_x 1.2305 (1.1009) acc_x 71.8750 (72.5893) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [40/52] time 0.386 (0.451) data 0.255 (0.320) loss_x loss_x 1.4180 (1.0986) acc_x 56.2500 (72.6562) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [45/52] time 0.419 (0.451) data 0.288 (0.320) loss_x loss_x 1.0664 (1.1044) acc_x 68.7500 (72.1528) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [50/52] time 0.558 (0.455) data 0.427 (0.324) loss_x loss_x 0.9229 (1.0999) acc_x 71.8750 (72.2500) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/46] time 0.397 (0.456) data 0.266 (0.325) loss_u loss_u 0.7866 (0.7862) acc_u 28.1250 (29.3750) lr 6.6126e-04 eta 0:00:18
epoch [124/200] batch [10/46] time 0.410 (0.450) data 0.279 (0.319) loss_u loss_u 0.8613 (0.7749) acc_u 25.0000 (30.3125) lr 6.6126e-04 eta 0:00:16
epoch [124/200] batch [15/46] time 0.477 (0.450) data 0.345 (0.318) loss_u loss_u 0.7476 (0.7735) acc_u 31.2500 (29.1667) lr 6.6126e-04 eta 0:00:13
epoch [124/200] batch [20/46] time 0.528 (0.452) data 0.397 (0.321) loss_u loss_u 0.7266 (0.7665) acc_u 40.6250 (30.0000) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [25/46] time 0.472 (0.452) data 0.340 (0.320) loss_u loss_u 0.8213 (0.7588) acc_u 18.7500 (30.6250) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [30/46] time 0.352 (0.450) data 0.220 (0.319) loss_u loss_u 0.7661 (0.7615) acc_u 28.1250 (30.5208) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [35/46] time 0.495 (0.448) data 0.363 (0.317) loss_u loss_u 0.7573 (0.7663) acc_u 34.3750 (30.3571) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [40/46] time 0.378 (0.447) data 0.246 (0.316) loss_u loss_u 0.8735 (0.7726) acc_u 15.6250 (29.4531) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [45/46] time 0.398 (0.446) data 0.266 (0.314) loss_u loss_u 0.7563 (0.7723) acc_u 25.0000 (29.5139) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1296
confident_label rate tensor(0.5297, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1661
clean true:1658
clean false:3
clean_rate:0.9981938591210114
noisy true:182
noisy false:1293
after delete: len(clean_dataset) 1661
after delete: len(noisy_dataset) 1475
epoch [125/200] batch [5/51] time 0.429 (0.458) data 0.298 (0.327) loss_x loss_x 1.2207 (1.0934) acc_x 71.8750 (71.2500) lr 6.4653e-04 eta 0:00:21
epoch [125/200] batch [10/51] time 0.434 (0.442) data 0.303 (0.310) loss_x loss_x 1.1475 (1.0899) acc_x 75.0000 (72.8125) lr 6.4653e-04 eta 0:00:18
epoch [125/200] batch [15/51] time 0.420 (0.464) data 0.289 (0.332) loss_x loss_x 1.4814 (1.1150) acc_x 62.5000 (72.0833) lr 6.4653e-04 eta 0:00:16
epoch [125/200] batch [20/51] time 0.563 (0.465) data 0.430 (0.333) loss_x loss_x 0.6528 (1.1735) acc_x 81.2500 (70.9375) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [25/51] time 0.554 (0.463) data 0.422 (0.331) loss_x loss_x 0.5186 (1.1099) acc_x 81.2500 (72.5000) lr 6.4653e-04 eta 0:00:12
epoch [125/200] batch [30/51] time 0.420 (0.462) data 0.290 (0.331) loss_x loss_x 1.1680 (1.0859) acc_x 78.1250 (72.9167) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [35/51] time 0.510 (0.463) data 0.379 (0.332) loss_x loss_x 1.0938 (1.1185) acc_x 78.1250 (72.0536) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [40/51] time 0.393 (0.457) data 0.262 (0.326) loss_x loss_x 1.4932 (1.1443) acc_x 53.1250 (71.5625) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [45/51] time 0.373 (0.454) data 0.242 (0.323) loss_x loss_x 1.5166 (1.1535) acc_x 59.3750 (71.4583) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [50/51] time 0.412 (0.453) data 0.281 (0.322) loss_x loss_x 1.1855 (1.1542) acc_x 71.8750 (71.7500) lr 6.4653e-04 eta 0:00:00
epoch [125/200] batch [5/46] time 0.417 (0.447) data 0.287 (0.316) loss_u loss_u 0.8501 (0.8018) acc_u 21.8750 (26.8750) lr 6.4653e-04 eta 0:00:18
epoch [125/200] batch [10/46] time 0.570 (0.447) data 0.438 (0.316) loss_u loss_u 0.6943 (0.7684) acc_u 34.3750 (29.6875) lr 6.4653e-04 eta 0:00:16
epoch [125/200] batch [15/46] time 0.659 (0.448) data 0.528 (0.317) loss_u loss_u 0.7891 (0.7724) acc_u 25.0000 (29.5833) lr 6.4653e-04 eta 0:00:13
epoch [125/200] batch [20/46] time 0.396 (0.445) data 0.266 (0.313) loss_u loss_u 0.7812 (0.7684) acc_u 21.8750 (29.2188) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [25/46] time 0.712 (0.445) data 0.580 (0.314) loss_u loss_u 0.8291 (0.7771) acc_u 21.8750 (28.0000) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [30/46] time 0.506 (0.444) data 0.375 (0.313) loss_u loss_u 0.7852 (0.7765) acc_u 31.2500 (28.6458) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [35/46] time 0.419 (0.442) data 0.288 (0.311) loss_u loss_u 0.7700 (0.7762) acc_u 31.2500 (28.8393) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [40/46] time 0.398 (0.441) data 0.266 (0.310) loss_u loss_u 0.7490 (0.7742) acc_u 34.3750 (29.2188) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [45/46] time 0.489 (0.443) data 0.358 (0.312) loss_u loss_u 0.7314 (0.7759) acc_u 34.3750 (29.1667) lr 6.4653e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1304
confident_label rate tensor(0.5277, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1655
clean true:1651
clean false:4
clean_rate:0.997583081570997
noisy true:181
noisy false:1300
after delete: len(clean_dataset) 1655
after delete: len(noisy_dataset) 1481
epoch [126/200] batch [5/51] time 0.447 (0.428) data 0.315 (0.296) loss_x loss_x 0.5220 (0.9626) acc_x 90.6250 (73.7500) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [10/51] time 0.461 (0.475) data 0.330 (0.343) loss_x loss_x 1.0410 (1.0182) acc_x 68.7500 (72.5000) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [15/51] time 0.562 (0.461) data 0.432 (0.329) loss_x loss_x 1.0869 (1.0180) acc_x 75.0000 (71.6667) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [20/51] time 0.408 (0.458) data 0.277 (0.327) loss_x loss_x 1.1348 (1.0448) acc_x 75.0000 (71.7188) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [25/51] time 0.581 (0.464) data 0.450 (0.332) loss_x loss_x 0.9756 (1.0475) acc_x 71.8750 (72.0000) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [30/51] time 0.398 (0.458) data 0.266 (0.327) loss_x loss_x 1.2188 (1.0519) acc_x 65.6250 (71.9792) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [35/51] time 0.518 (0.457) data 0.387 (0.326) loss_x loss_x 1.4893 (1.0764) acc_x 65.6250 (72.0536) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [40/51] time 0.494 (0.452) data 0.362 (0.321) loss_x loss_x 0.7915 (1.0722) acc_x 78.1250 (72.1875) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [45/51] time 0.439 (0.462) data 0.307 (0.331) loss_x loss_x 0.9277 (1.0649) acc_x 78.1250 (72.2917) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [50/51] time 0.523 (0.464) data 0.392 (0.332) loss_x loss_x 0.8267 (1.0758) acc_x 78.1250 (72.0000) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/46] time 0.391 (0.461) data 0.261 (0.330) loss_u loss_u 0.7681 (0.7681) acc_u 31.2500 (28.7500) lr 6.3188e-04 eta 0:00:18
epoch [126/200] batch [10/46] time 0.461 (0.456) data 0.330 (0.325) loss_u loss_u 0.8428 (0.7729) acc_u 28.1250 (29.0625) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [15/46] time 0.500 (0.459) data 0.369 (0.327) loss_u loss_u 0.7090 (0.7651) acc_u 40.6250 (31.0417) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [20/46] time 0.411 (0.454) data 0.280 (0.323) loss_u loss_u 0.7998 (0.7700) acc_u 25.0000 (29.8438) lr 6.3188e-04 eta 0:00:11
epoch [126/200] batch [25/46] time 0.398 (0.451) data 0.266 (0.319) loss_u loss_u 0.7012 (0.7673) acc_u 46.8750 (30.6250) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [30/46] time 0.408 (0.449) data 0.276 (0.318) loss_u loss_u 0.6890 (0.7672) acc_u 37.5000 (30.2083) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [35/46] time 0.352 (0.446) data 0.221 (0.315) loss_u loss_u 0.6660 (0.7636) acc_u 37.5000 (30.1786) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [40/46] time 0.372 (0.444) data 0.242 (0.313) loss_u loss_u 0.7617 (0.7603) acc_u 31.2500 (30.7812) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [45/46] time 0.469 (0.445) data 0.338 (0.314) loss_u loss_u 0.7866 (0.7594) acc_u 25.0000 (30.6944) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1261
confident_label rate tensor(0.5424, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1701
clean true:1697
clean false:4
clean_rate:0.9976484420928865
noisy true:178
noisy false:1257
after delete: len(clean_dataset) 1701
after delete: len(noisy_dataset) 1435
epoch [127/200] batch [5/53] time 0.405 (0.475) data 0.273 (0.344) loss_x loss_x 1.4629 (1.1520) acc_x 56.2500 (68.7500) lr 6.1732e-04 eta 0:00:22
epoch [127/200] batch [10/53] time 0.357 (0.484) data 0.226 (0.352) loss_x loss_x 1.3389 (1.1274) acc_x 56.2500 (71.2500) lr 6.1732e-04 eta 0:00:20
epoch [127/200] batch [15/53] time 0.362 (0.489) data 0.231 (0.358) loss_x loss_x 0.6846 (1.1268) acc_x 75.0000 (71.6667) lr 6.1732e-04 eta 0:00:18
epoch [127/200] batch [20/53] time 0.662 (0.477) data 0.531 (0.346) loss_x loss_x 0.9468 (1.0703) acc_x 71.8750 (72.6562) lr 6.1732e-04 eta 0:00:15
epoch [127/200] batch [25/53] time 0.465 (0.477) data 0.334 (0.346) loss_x loss_x 1.0166 (1.0923) acc_x 62.5000 (72.3750) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [30/53] time 0.520 (0.479) data 0.389 (0.348) loss_x loss_x 0.7822 (1.0630) acc_x 81.2500 (72.8125) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [35/53] time 0.597 (0.484) data 0.466 (0.353) loss_x loss_x 1.4951 (1.0827) acc_x 56.2500 (72.6786) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [40/53] time 0.468 (0.484) data 0.338 (0.353) loss_x loss_x 1.3623 (1.1096) acc_x 62.5000 (72.0312) lr 6.1732e-04 eta 0:00:06
epoch [127/200] batch [45/53] time 0.504 (0.478) data 0.373 (0.347) loss_x loss_x 2.2773 (1.1438) acc_x 46.8750 (71.8750) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [50/53] time 0.358 (0.472) data 0.228 (0.341) loss_x loss_x 1.4395 (1.1398) acc_x 65.6250 (71.7500) lr 6.1732e-04 eta 0:00:01
epoch [127/200] batch [5/44] time 0.590 (0.469) data 0.455 (0.338) loss_u loss_u 0.7544 (0.7519) acc_u 28.1250 (31.2500) lr 6.1732e-04 eta 0:00:18
epoch [127/200] batch [10/44] time 0.343 (0.467) data 0.211 (0.336) loss_u loss_u 0.8618 (0.7708) acc_u 18.7500 (29.6875) lr 6.1732e-04 eta 0:00:15
epoch [127/200] batch [15/44] time 0.515 (0.466) data 0.384 (0.334) loss_u loss_u 0.6328 (0.7633) acc_u 50.0000 (30.2083) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [20/44] time 0.384 (0.464) data 0.253 (0.333) loss_u loss_u 0.8037 (0.7781) acc_u 34.3750 (28.9062) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [25/44] time 0.707 (0.465) data 0.574 (0.334) loss_u loss_u 0.8457 (0.7771) acc_u 25.0000 (29.0000) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [30/44] time 0.407 (0.464) data 0.276 (0.332) loss_u loss_u 0.8252 (0.7725) acc_u 25.0000 (29.2708) lr 6.1732e-04 eta 0:00:06
epoch [127/200] batch [35/44] time 0.348 (0.460) data 0.216 (0.328) loss_u loss_u 0.7827 (0.7786) acc_u 31.2500 (28.6607) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [40/44] time 0.370 (0.460) data 0.238 (0.328) loss_u loss_u 0.7783 (0.7809) acc_u 31.2500 (28.3594) lr 6.1732e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1286
confident_label rate tensor(0.5325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1670
clean true:1669
clean false:1
clean_rate:0.9994011976047904
noisy true:181
noisy false:1285
after delete: len(clean_dataset) 1670
after delete: len(noisy_dataset) 1466
epoch [128/200] batch [5/52] time 0.411 (0.400) data 0.280 (0.269) loss_x loss_x 0.8872 (0.9263) acc_x 78.1250 (74.3750) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [10/52] time 0.382 (0.435) data 0.250 (0.303) loss_x loss_x 0.8979 (0.9691) acc_x 84.3750 (75.6250) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [15/52] time 0.514 (0.456) data 0.383 (0.325) loss_x loss_x 1.3994 (1.0474) acc_x 53.1250 (73.9583) lr 6.0285e-04 eta 0:00:16
epoch [128/200] batch [20/52] time 0.432 (0.453) data 0.301 (0.322) loss_x loss_x 0.7983 (1.0452) acc_x 75.0000 (73.9062) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [25/52] time 0.375 (0.449) data 0.244 (0.318) loss_x loss_x 0.9722 (1.0274) acc_x 75.0000 (74.5000) lr 6.0285e-04 eta 0:00:12
epoch [128/200] batch [30/52] time 0.382 (0.450) data 0.251 (0.319) loss_x loss_x 0.5649 (1.0098) acc_x 87.5000 (75.1042) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [35/52] time 0.457 (0.456) data 0.325 (0.324) loss_x loss_x 1.4268 (1.0480) acc_x 68.7500 (73.9286) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [40/52] time 0.423 (0.458) data 0.291 (0.326) loss_x loss_x 1.2539 (1.0606) acc_x 53.1250 (73.1250) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [45/52] time 0.479 (0.461) data 0.347 (0.330) loss_x loss_x 1.1289 (1.0708) acc_x 65.6250 (72.7778) lr 6.0285e-04 eta 0:00:03
epoch [128/200] batch [50/52] time 0.347 (0.459) data 0.216 (0.328) loss_x loss_x 1.8955 (1.0704) acc_x 59.3750 (73.0000) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/45] time 0.365 (0.463) data 0.233 (0.332) loss_u loss_u 0.8228 (0.7842) acc_u 25.0000 (30.0000) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [10/45] time 0.470 (0.461) data 0.338 (0.330) loss_u loss_u 0.7803 (0.7869) acc_u 31.2500 (28.1250) lr 6.0285e-04 eta 0:00:16
epoch [128/200] batch [15/45] time 0.361 (0.459) data 0.229 (0.328) loss_u loss_u 0.7993 (0.7872) acc_u 18.7500 (27.7083) lr 6.0285e-04 eta 0:00:13
epoch [128/200] batch [20/45] time 0.443 (0.460) data 0.311 (0.328) loss_u loss_u 0.7563 (0.7795) acc_u 34.3750 (28.4375) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [25/45] time 0.350 (0.455) data 0.218 (0.323) loss_u loss_u 0.7524 (0.7788) acc_u 34.3750 (28.2500) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [30/45] time 0.428 (0.451) data 0.296 (0.320) loss_u loss_u 0.7622 (0.7740) acc_u 34.3750 (28.8542) lr 6.0285e-04 eta 0:00:06
epoch [128/200] batch [35/45] time 0.402 (0.453) data 0.270 (0.322) loss_u loss_u 0.7490 (0.7700) acc_u 28.1250 (29.5536) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [40/45] time 0.504 (0.451) data 0.372 (0.320) loss_u loss_u 0.6982 (0.7716) acc_u 31.2500 (29.2188) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [45/45] time 0.378 (0.448) data 0.247 (0.317) loss_u loss_u 0.8110 (0.7691) acc_u 21.8750 (29.2361) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1259
confident_label rate tensor(0.5360, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1681
clean true:1676
clean false:5
clean_rate:0.9970255800118977
noisy true:201
noisy false:1254
after delete: len(clean_dataset) 1681
after delete: len(noisy_dataset) 1455
epoch [129/200] batch [5/52] time 0.386 (0.498) data 0.254 (0.366) loss_x loss_x 1.0225 (0.9643) acc_x 65.6250 (76.2500) lr 5.8849e-04 eta 0:00:23
epoch [129/200] batch [10/52] time 0.447 (0.469) data 0.316 (0.338) loss_x loss_x 0.7139 (1.0584) acc_x 84.3750 (73.1250) lr 5.8849e-04 eta 0:00:19
epoch [129/200] batch [15/52] time 0.441 (0.458) data 0.311 (0.328) loss_x loss_x 1.1113 (1.0911) acc_x 71.8750 (73.9583) lr 5.8849e-04 eta 0:00:16
epoch [129/200] batch [20/52] time 0.465 (0.454) data 0.334 (0.324) loss_x loss_x 0.9893 (1.1173) acc_x 68.7500 (71.7188) lr 5.8849e-04 eta 0:00:14
epoch [129/200] batch [25/52] time 0.632 (0.465) data 0.500 (0.334) loss_x loss_x 1.4668 (1.1568) acc_x 62.5000 (71.1250) lr 5.8849e-04 eta 0:00:12
epoch [129/200] batch [30/52] time 0.493 (0.460) data 0.361 (0.329) loss_x loss_x 1.7070 (1.1975) acc_x 65.6250 (70.6250) lr 5.8849e-04 eta 0:00:10
epoch [129/200] batch [35/52] time 0.543 (0.462) data 0.412 (0.331) loss_x loss_x 1.1641 (1.2098) acc_x 65.6250 (70.0000) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [40/52] time 0.530 (0.462) data 0.399 (0.331) loss_x loss_x 1.2402 (1.2000) acc_x 78.1250 (70.5469) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [45/52] time 0.481 (0.462) data 0.350 (0.331) loss_x loss_x 0.8940 (1.2043) acc_x 81.2500 (70.6250) lr 5.8849e-04 eta 0:00:03
epoch [129/200] batch [50/52] time 0.409 (0.462) data 0.278 (0.331) loss_x loss_x 0.9199 (1.1875) acc_x 81.2500 (71.0000) lr 5.8849e-04 eta 0:00:00
epoch [129/200] batch [5/45] time 0.398 (0.456) data 0.265 (0.324) loss_u loss_u 0.6846 (0.7471) acc_u 40.6250 (31.2500) lr 5.8849e-04 eta 0:00:18
epoch [129/200] batch [10/45] time 0.467 (0.459) data 0.335 (0.328) loss_u loss_u 0.7700 (0.7562) acc_u 25.0000 (30.3125) lr 5.8849e-04 eta 0:00:16
epoch [129/200] batch [15/45] time 0.462 (0.461) data 0.330 (0.329) loss_u loss_u 0.7554 (0.7686) acc_u 28.1250 (28.5417) lr 5.8849e-04 eta 0:00:13
epoch [129/200] batch [20/45] time 0.350 (0.455) data 0.218 (0.324) loss_u loss_u 0.7817 (0.7708) acc_u 18.7500 (28.1250) lr 5.8849e-04 eta 0:00:11
epoch [129/200] batch [25/45] time 0.434 (0.454) data 0.303 (0.323) loss_u loss_u 0.6709 (0.7666) acc_u 37.5000 (28.8750) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [30/45] time 0.436 (0.452) data 0.304 (0.320) loss_u loss_u 0.6968 (0.7635) acc_u 37.5000 (29.1667) lr 5.8849e-04 eta 0:00:06
epoch [129/200] batch [35/45] time 0.363 (0.450) data 0.232 (0.318) loss_u loss_u 0.7969 (0.7710) acc_u 28.1250 (28.2143) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [40/45] time 0.476 (0.450) data 0.344 (0.318) loss_u loss_u 0.7344 (0.7666) acc_u 34.3750 (28.7500) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [45/45] time 0.398 (0.449) data 0.266 (0.318) loss_u loss_u 0.6685 (0.7622) acc_u 37.5000 (29.0278) lr 5.8849e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1263
confident_label rate tensor(0.5354, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1679
clean true:1677
clean false:2
clean_rate:0.9988088147706968
noisy true:196
noisy false:1261
after delete: len(clean_dataset) 1679
after delete: len(noisy_dataset) 1457
epoch [130/200] batch [5/52] time 0.577 (0.514) data 0.447 (0.382) loss_x loss_x 1.3848 (0.9767) acc_x 53.1250 (75.0000) lr 5.7422e-04 eta 0:00:24
epoch [130/200] batch [10/52] time 0.600 (0.514) data 0.469 (0.383) loss_x loss_x 1.0645 (1.0093) acc_x 84.3750 (73.7500) lr 5.7422e-04 eta 0:00:21
epoch [130/200] batch [15/52] time 0.457 (0.487) data 0.326 (0.356) loss_x loss_x 0.7139 (1.0211) acc_x 87.5000 (74.3750) lr 5.7422e-04 eta 0:00:18
epoch [130/200] batch [20/52] time 0.530 (0.475) data 0.399 (0.344) loss_x loss_x 0.7764 (1.0672) acc_x 87.5000 (72.8125) lr 5.7422e-04 eta 0:00:15
epoch [130/200] batch [25/52] time 0.449 (0.471) data 0.318 (0.340) loss_x loss_x 0.9448 (1.0485) acc_x 81.2500 (73.1250) lr 5.7422e-04 eta 0:00:12
epoch [130/200] batch [30/52] time 0.423 (0.467) data 0.292 (0.336) loss_x loss_x 1.2588 (1.0461) acc_x 75.0000 (72.9167) lr 5.7422e-04 eta 0:00:10
epoch [130/200] batch [35/52] time 0.421 (0.465) data 0.290 (0.334) loss_x loss_x 1.0762 (1.0533) acc_x 68.7500 (72.7679) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [40/52] time 0.491 (0.460) data 0.360 (0.329) loss_x loss_x 1.3008 (1.0505) acc_x 68.7500 (72.8125) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [45/52] time 0.464 (0.456) data 0.333 (0.325) loss_x loss_x 1.0615 (1.0493) acc_x 65.6250 (72.7778) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [50/52] time 0.403 (0.452) data 0.272 (0.321) loss_x loss_x 1.2871 (1.0826) acc_x 71.8750 (72.1875) lr 5.7422e-04 eta 0:00:00
epoch [130/200] batch [5/45] time 0.484 (0.449) data 0.352 (0.318) loss_u loss_u 0.7319 (0.7657) acc_u 31.2500 (28.7500) lr 5.7422e-04 eta 0:00:17
epoch [130/200] batch [10/45] time 0.493 (0.450) data 0.362 (0.319) loss_u loss_u 0.7466 (0.7748) acc_u 31.2500 (27.5000) lr 5.7422e-04 eta 0:00:15
epoch [130/200] batch [15/45] time 0.532 (0.448) data 0.400 (0.317) loss_u loss_u 0.7725 (0.7684) acc_u 28.1250 (28.5417) lr 5.7422e-04 eta 0:00:13
epoch [130/200] batch [20/45] time 0.401 (0.449) data 0.269 (0.318) loss_u loss_u 0.8467 (0.7783) acc_u 18.7500 (27.6562) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [25/45] time 0.393 (0.445) data 0.261 (0.314) loss_u loss_u 0.7271 (0.7755) acc_u 37.5000 (28.5000) lr 5.7422e-04 eta 0:00:08
epoch [130/200] batch [30/45] time 0.369 (0.442) data 0.237 (0.311) loss_u loss_u 0.7275 (0.7736) acc_u 37.5000 (29.2708) lr 5.7422e-04 eta 0:00:06
epoch [130/200] batch [35/45] time 0.549 (0.447) data 0.416 (0.316) loss_u loss_u 0.8281 (0.7667) acc_u 31.2500 (30.4464) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [40/45] time 0.522 (0.449) data 0.390 (0.318) loss_u loss_u 0.8486 (0.7700) acc_u 15.6250 (29.8438) lr 5.7422e-04 eta 0:00:02
epoch [130/200] batch [45/45] time 0.417 (0.447) data 0.284 (0.316) loss_u loss_u 0.7769 (0.7654) acc_u 34.3750 (30.4861) lr 5.7422e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1289
confident_label rate tensor(0.5322, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1669
clean true:1663
clean false:6
clean_rate:0.9964050329538646
noisy true:184
noisy false:1283
after delete: len(clean_dataset) 1669
after delete: len(noisy_dataset) 1467
epoch [131/200] batch [5/52] time 0.448 (0.487) data 0.317 (0.355) loss_x loss_x 1.4209 (1.1555) acc_x 71.8750 (73.7500) lr 5.6006e-04 eta 0:00:22
epoch [131/200] batch [10/52] time 0.425 (0.470) data 0.293 (0.338) loss_x loss_x 1.1865 (1.2315) acc_x 75.0000 (70.6250) lr 5.6006e-04 eta 0:00:19
epoch [131/200] batch [15/52] time 0.422 (0.479) data 0.291 (0.348) loss_x loss_x 1.0615 (1.1458) acc_x 78.1250 (72.0833) lr 5.6006e-04 eta 0:00:17
epoch [131/200] batch [20/52] time 0.468 (0.473) data 0.337 (0.342) loss_x loss_x 1.1777 (1.1402) acc_x 65.6250 (72.0312) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [25/52] time 0.443 (0.474) data 0.312 (0.343) loss_x loss_x 1.1562 (1.1427) acc_x 71.8750 (72.6250) lr 5.6006e-04 eta 0:00:12
epoch [131/200] batch [30/52] time 0.391 (0.461) data 0.260 (0.330) loss_x loss_x 0.5962 (1.1235) acc_x 84.3750 (73.3333) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [35/52] time 0.543 (0.469) data 0.409 (0.337) loss_x loss_x 1.2666 (1.1153) acc_x 65.6250 (73.5714) lr 5.6006e-04 eta 0:00:07
epoch [131/200] batch [40/52] time 0.357 (0.463) data 0.226 (0.331) loss_x loss_x 1.0215 (1.1320) acc_x 81.2500 (73.2812) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [45/52] time 0.453 (0.457) data 0.322 (0.326) loss_x loss_x 1.0127 (1.1140) acc_x 68.7500 (73.1944) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [50/52] time 0.398 (0.456) data 0.267 (0.325) loss_x loss_x 1.2344 (1.1136) acc_x 68.7500 (73.3750) lr 5.6006e-04 eta 0:00:00
epoch [131/200] batch [5/45] time 0.475 (0.455) data 0.343 (0.324) loss_u loss_u 0.5884 (0.7513) acc_u 50.0000 (33.1250) lr 5.6006e-04 eta 0:00:18
epoch [131/200] batch [10/45] time 0.402 (0.457) data 0.271 (0.325) loss_u loss_u 0.7368 (0.7462) acc_u 28.1250 (32.8125) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [15/45] time 0.401 (0.454) data 0.270 (0.322) loss_u loss_u 0.8457 (0.7639) acc_u 21.8750 (30.0000) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [20/45] time 0.437 (0.451) data 0.305 (0.320) loss_u loss_u 0.7832 (0.7659) acc_u 28.1250 (30.0000) lr 5.6006e-04 eta 0:00:11
epoch [131/200] batch [25/45] time 0.390 (0.450) data 0.257 (0.318) loss_u loss_u 0.8076 (0.7638) acc_u 21.8750 (30.1250) lr 5.6006e-04 eta 0:00:09
epoch [131/200] batch [30/45] time 0.385 (0.446) data 0.253 (0.315) loss_u loss_u 0.8477 (0.7664) acc_u 18.7500 (29.3750) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [35/45] time 0.472 (0.446) data 0.340 (0.315) loss_u loss_u 0.7236 (0.7605) acc_u 37.5000 (30.7143) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [40/45] time 0.381 (0.446) data 0.249 (0.314) loss_u loss_u 0.7344 (0.7611) acc_u 40.6250 (30.9375) lr 5.6006e-04 eta 0:00:02
epoch [131/200] batch [45/45] time 0.497 (0.448) data 0.365 (0.316) loss_u loss_u 0.6782 (0.7592) acc_u 34.3750 (31.0417) lr 5.6006e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1271
confident_label rate tensor(0.5430, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1703
clean true:1699
clean false:4
clean_rate:0.997651203758074
noisy true:166
noisy false:1267
after delete: len(clean_dataset) 1703
after delete: len(noisy_dataset) 1433
epoch [132/200] batch [5/53] time 0.457 (0.502) data 0.327 (0.371) loss_x loss_x 0.9204 (1.1876) acc_x 78.1250 (71.2500) lr 5.4601e-04 eta 0:00:24
epoch [132/200] batch [10/53] time 0.388 (0.455) data 0.258 (0.324) loss_x loss_x 1.5850 (1.1356) acc_x 62.5000 (73.1250) lr 5.4601e-04 eta 0:00:19
epoch [132/200] batch [15/53] time 0.336 (0.450) data 0.205 (0.320) loss_x loss_x 1.1895 (1.1319) acc_x 68.7500 (72.7083) lr 5.4601e-04 eta 0:00:17
epoch [132/200] batch [20/53] time 0.439 (0.441) data 0.308 (0.311) loss_x loss_x 0.8813 (1.1760) acc_x 68.7500 (70.4688) lr 5.4601e-04 eta 0:00:14
epoch [132/200] batch [25/53] time 0.431 (0.440) data 0.301 (0.310) loss_x loss_x 1.0625 (1.1495) acc_x 71.8750 (71.2500) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [30/53] time 0.424 (0.444) data 0.295 (0.314) loss_x loss_x 0.7642 (1.1305) acc_x 87.5000 (72.0833) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [35/53] time 0.501 (0.443) data 0.371 (0.313) loss_x loss_x 1.6543 (1.1477) acc_x 65.6250 (72.0536) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [40/53] time 0.423 (0.439) data 0.292 (0.309) loss_x loss_x 1.0791 (1.1217) acc_x 71.8750 (72.2656) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [45/53] time 0.425 (0.438) data 0.294 (0.307) loss_x loss_x 1.4297 (1.1082) acc_x 62.5000 (72.3611) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [50/53] time 0.411 (0.437) data 0.280 (0.306) loss_x loss_x 1.1562 (1.1190) acc_x 68.7500 (71.9375) lr 5.4601e-04 eta 0:00:01
epoch [132/200] batch [5/44] time 0.433 (0.435) data 0.301 (0.304) loss_u loss_u 0.7490 (0.7423) acc_u 31.2500 (34.3750) lr 5.4601e-04 eta 0:00:16
epoch [132/200] batch [10/44] time 0.541 (0.436) data 0.410 (0.305) loss_u loss_u 0.7964 (0.7608) acc_u 25.0000 (31.8750) lr 5.4601e-04 eta 0:00:14
epoch [132/200] batch [15/44] time 0.403 (0.439) data 0.272 (0.308) loss_u loss_u 0.7007 (0.7616) acc_u 40.6250 (30.6250) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [20/44] time 0.416 (0.441) data 0.284 (0.310) loss_u loss_u 0.7793 (0.7573) acc_u 31.2500 (31.2500) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [25/44] time 0.519 (0.443) data 0.387 (0.312) loss_u loss_u 0.7466 (0.7574) acc_u 28.1250 (31.1250) lr 5.4601e-04 eta 0:00:08
epoch [132/200] batch [30/44] time 0.351 (0.444) data 0.219 (0.313) loss_u loss_u 0.7930 (0.7641) acc_u 31.2500 (30.2083) lr 5.4601e-04 eta 0:00:06
epoch [132/200] batch [35/44] time 0.412 (0.443) data 0.280 (0.312) loss_u loss_u 0.7642 (0.7649) acc_u 25.0000 (30.0000) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [40/44] time 0.572 (0.444) data 0.441 (0.313) loss_u loss_u 0.8330 (0.7695) acc_u 21.8750 (29.5312) lr 5.4601e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1296
confident_label rate tensor(0.5300, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1662
clean true:1661
clean false:1
clean_rate:0.9993983152827918
noisy true:179
noisy false:1295
after delete: len(clean_dataset) 1662
after delete: len(noisy_dataset) 1474
epoch [133/200] batch [5/51] time 0.377 (0.414) data 0.246 (0.283) loss_x loss_x 0.8589 (1.1803) acc_x 81.2500 (71.2500) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [10/51] time 0.510 (0.424) data 0.380 (0.293) loss_x loss_x 0.8335 (1.2106) acc_x 78.1250 (70.0000) lr 5.3207e-04 eta 0:00:17
epoch [133/200] batch [15/51] time 0.379 (0.436) data 0.248 (0.305) loss_x loss_x 0.9985 (1.1585) acc_x 68.7500 (71.0417) lr 5.3207e-04 eta 0:00:15
epoch [133/200] batch [20/51] time 0.365 (0.446) data 0.234 (0.315) loss_x loss_x 1.0537 (1.1518) acc_x 78.1250 (71.2500) lr 5.3207e-04 eta 0:00:13
epoch [133/200] batch [25/51] time 0.430 (0.449) data 0.299 (0.318) loss_x loss_x 1.2451 (1.1372) acc_x 68.7500 (71.5000) lr 5.3207e-04 eta 0:00:11
epoch [133/200] batch [30/51] time 0.383 (0.447) data 0.252 (0.316) loss_x loss_x 0.6763 (1.0754) acc_x 84.3750 (73.1250) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [35/51] time 0.504 (0.454) data 0.373 (0.323) loss_x loss_x 0.9136 (1.0654) acc_x 84.3750 (73.3929) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [40/51] time 0.574 (0.458) data 0.443 (0.327) loss_x loss_x 1.1650 (1.0659) acc_x 75.0000 (73.5156) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [45/51] time 0.482 (0.460) data 0.349 (0.329) loss_x loss_x 1.2656 (1.0796) acc_x 65.6250 (73.2639) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [50/51] time 0.477 (0.463) data 0.345 (0.332) loss_x loss_x 1.2402 (1.0771) acc_x 71.8750 (72.8750) lr 5.3207e-04 eta 0:00:00
epoch [133/200] batch [5/46] time 0.598 (0.462) data 0.466 (0.331) loss_u loss_u 0.7778 (0.8058) acc_u 25.0000 (24.3750) lr 5.3207e-04 eta 0:00:18
epoch [133/200] batch [10/46] time 0.393 (0.467) data 0.260 (0.335) loss_u loss_u 0.7925 (0.7916) acc_u 28.1250 (25.6250) lr 5.3207e-04 eta 0:00:16
epoch [133/200] batch [15/46] time 0.390 (0.466) data 0.259 (0.334) loss_u loss_u 0.8110 (0.7809) acc_u 15.6250 (26.2500) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [20/46] time 0.326 (0.463) data 0.193 (0.332) loss_u loss_u 0.7183 (0.7753) acc_u 37.5000 (27.5000) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [25/46] time 0.402 (0.462) data 0.270 (0.330) loss_u loss_u 0.7539 (0.7720) acc_u 31.2500 (28.7500) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [30/46] time 0.360 (0.458) data 0.228 (0.326) loss_u loss_u 0.7534 (0.7698) acc_u 25.0000 (29.0625) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [35/46] time 0.462 (0.456) data 0.330 (0.325) loss_u loss_u 0.8462 (0.7679) acc_u 25.0000 (29.4643) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [40/46] time 0.428 (0.455) data 0.295 (0.323) loss_u loss_u 0.6528 (0.7610) acc_u 43.7500 (30.6250) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [45/46] time 0.455 (0.452) data 0.323 (0.320) loss_u loss_u 0.7300 (0.7582) acc_u 28.1250 (31.0417) lr 5.3207e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1319
confident_label rate tensor(0.5236, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1642
clean true:1639
clean false:3
clean_rate:0.9981729598051157
noisy true:178
noisy false:1316
after delete: len(clean_dataset) 1642
after delete: len(noisy_dataset) 1494
epoch [134/200] batch [5/51] time 0.409 (0.441) data 0.278 (0.309) loss_x loss_x 0.9707 (1.2439) acc_x 81.2500 (70.0000) lr 5.1825e-04 eta 0:00:20
epoch [134/200] batch [10/51] time 0.461 (0.448) data 0.330 (0.316) loss_x loss_x 1.1992 (1.2251) acc_x 71.8750 (71.2500) lr 5.1825e-04 eta 0:00:18
epoch [134/200] batch [15/51] time 0.483 (0.440) data 0.352 (0.309) loss_x loss_x 0.9966 (1.1647) acc_x 75.0000 (71.4583) lr 5.1825e-04 eta 0:00:15
epoch [134/200] batch [20/51] time 0.609 (0.453) data 0.478 (0.322) loss_x loss_x 1.4209 (1.1944) acc_x 50.0000 (69.5312) lr 5.1825e-04 eta 0:00:14
epoch [134/200] batch [25/51] time 0.482 (0.455) data 0.351 (0.324) loss_x loss_x 1.2061 (1.1925) acc_x 68.7500 (69.3750) lr 5.1825e-04 eta 0:00:11
epoch [134/200] batch [30/51] time 0.470 (0.453) data 0.339 (0.322) loss_x loss_x 1.1572 (1.1808) acc_x 68.7500 (69.6875) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [35/51] time 0.361 (0.449) data 0.230 (0.317) loss_x loss_x 1.2676 (1.1626) acc_x 68.7500 (70.7143) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [40/51] time 0.550 (0.454) data 0.419 (0.323) loss_x loss_x 0.8516 (1.1452) acc_x 81.2500 (71.4062) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [45/51] time 0.482 (0.457) data 0.351 (0.326) loss_x loss_x 1.2373 (1.1460) acc_x 65.6250 (71.4583) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [50/51] time 0.392 (0.453) data 0.261 (0.322) loss_x loss_x 1.1162 (1.1454) acc_x 68.7500 (71.3750) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/46] time 0.354 (0.456) data 0.222 (0.324) loss_u loss_u 0.7271 (0.7144) acc_u 31.2500 (36.8750) lr 5.1825e-04 eta 0:00:18
epoch [134/200] batch [10/46] time 0.444 (0.455) data 0.313 (0.324) loss_u loss_u 0.7383 (0.6997) acc_u 40.6250 (39.0625) lr 5.1825e-04 eta 0:00:16
epoch [134/200] batch [15/46] time 0.474 (0.455) data 0.343 (0.324) loss_u loss_u 0.7832 (0.7280) acc_u 31.2500 (35.0000) lr 5.1825e-04 eta 0:00:14
epoch [134/200] batch [20/46] time 0.432 (0.453) data 0.300 (0.322) loss_u loss_u 0.6992 (0.7317) acc_u 37.5000 (35.0000) lr 5.1825e-04 eta 0:00:11
epoch [134/200] batch [25/46] time 0.403 (0.453) data 0.271 (0.321) loss_u loss_u 0.7612 (0.7419) acc_u 34.3750 (34.1250) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [30/46] time 0.631 (0.453) data 0.499 (0.321) loss_u loss_u 0.7305 (0.7428) acc_u 34.3750 (33.5417) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [35/46] time 0.505 (0.452) data 0.373 (0.321) loss_u loss_u 0.8135 (0.7476) acc_u 21.8750 (32.7679) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [40/46] time 0.450 (0.450) data 0.319 (0.319) loss_u loss_u 0.6538 (0.7524) acc_u 43.7500 (32.2656) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [45/46] time 0.524 (0.451) data 0.392 (0.319) loss_u loss_u 0.7061 (0.7544) acc_u 40.6250 (32.0833) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1317
confident_label rate tensor(0.5242, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1644
clean true:1640
clean false:4
clean_rate:0.9975669099756691
noisy true:179
noisy false:1313
after delete: len(clean_dataset) 1644
after delete: len(noisy_dataset) 1492
epoch [135/200] batch [5/51] time 0.554 (0.475) data 0.422 (0.343) loss_x loss_x 1.0107 (0.9537) acc_x 75.0000 (78.1250) lr 5.0454e-04 eta 0:00:21
epoch [135/200] batch [10/51] time 0.437 (0.443) data 0.306 (0.312) loss_x loss_x 0.4780 (0.9206) acc_x 87.5000 (76.2500) lr 5.0454e-04 eta 0:00:18
epoch [135/200] batch [15/51] time 0.441 (0.468) data 0.310 (0.337) loss_x loss_x 1.2227 (0.9801) acc_x 65.6250 (73.7500) lr 5.0454e-04 eta 0:00:16
epoch [135/200] batch [20/51] time 0.544 (0.469) data 0.414 (0.338) loss_x loss_x 0.6382 (0.9861) acc_x 81.2500 (73.5938) lr 5.0454e-04 eta 0:00:14
epoch [135/200] batch [25/51] time 0.448 (0.460) data 0.318 (0.329) loss_x loss_x 1.7178 (1.0250) acc_x 53.1250 (72.3750) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [30/51] time 0.503 (0.465) data 0.372 (0.334) loss_x loss_x 0.9341 (1.0240) acc_x 84.3750 (73.0208) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [35/51] time 0.601 (0.464) data 0.471 (0.334) loss_x loss_x 1.4717 (1.0294) acc_x 65.6250 (72.7679) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [40/51] time 0.425 (0.463) data 0.294 (0.332) loss_x loss_x 0.7471 (1.0474) acc_x 81.2500 (72.3438) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [45/51] time 0.428 (0.463) data 0.296 (0.332) loss_x loss_x 1.3770 (1.0473) acc_x 68.7500 (72.7778) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [50/51] time 0.606 (0.463) data 0.475 (0.332) loss_x loss_x 1.1348 (1.0733) acc_x 78.1250 (72.3125) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/46] time 0.326 (0.458) data 0.194 (0.327) loss_u loss_u 0.7104 (0.7311) acc_u 37.5000 (31.8750) lr 5.0454e-04 eta 0:00:18
epoch [135/200] batch [10/46] time 0.490 (0.459) data 0.357 (0.328) loss_u loss_u 0.6465 (0.7464) acc_u 50.0000 (31.2500) lr 5.0454e-04 eta 0:00:16
epoch [135/200] batch [15/46] time 0.403 (0.454) data 0.271 (0.323) loss_u loss_u 0.7300 (0.7528) acc_u 34.3750 (31.2500) lr 5.0454e-04 eta 0:00:14
epoch [135/200] batch [20/46] time 0.412 (0.452) data 0.280 (0.321) loss_u loss_u 0.7896 (0.7517) acc_u 28.1250 (32.0312) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [25/46] time 0.333 (0.447) data 0.202 (0.316) loss_u loss_u 0.8008 (0.7492) acc_u 18.7500 (32.0000) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [30/46] time 0.531 (0.446) data 0.399 (0.315) loss_u loss_u 0.6191 (0.7463) acc_u 43.7500 (32.5000) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [35/46] time 0.447 (0.448) data 0.317 (0.317) loss_u loss_u 0.7480 (0.7535) acc_u 25.0000 (31.3393) lr 5.0454e-04 eta 0:00:04
epoch [135/200] batch [40/46] time 0.397 (0.445) data 0.265 (0.314) loss_u loss_u 0.7139 (0.7580) acc_u 40.6250 (31.0938) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [45/46] time 0.483 (0.447) data 0.351 (0.316) loss_u loss_u 0.8467 (0.7550) acc_u 21.8750 (31.4583) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1270
confident_label rate tensor(0.5370, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1684
clean true:1682
clean false:2
clean_rate:0.998812351543943
noisy true:184
noisy false:1268
after delete: len(clean_dataset) 1684
after delete: len(noisy_dataset) 1452
epoch [136/200] batch [5/52] time 0.511 (0.466) data 0.380 (0.334) loss_x loss_x 0.9668 (1.1301) acc_x 78.1250 (72.5000) lr 4.9096e-04 eta 0:00:21
epoch [136/200] batch [10/52] time 0.541 (0.449) data 0.410 (0.317) loss_x loss_x 1.0469 (0.9271) acc_x 71.8750 (77.8125) lr 4.9096e-04 eta 0:00:18
epoch [136/200] batch [15/52] time 0.377 (0.438) data 0.245 (0.306) loss_x loss_x 1.3701 (1.0486) acc_x 65.6250 (75.0000) lr 4.9096e-04 eta 0:00:16
epoch [136/200] batch [20/52] time 0.348 (0.439) data 0.217 (0.308) loss_x loss_x 1.3682 (1.0796) acc_x 62.5000 (73.2812) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [25/52] time 0.453 (0.440) data 0.322 (0.309) loss_x loss_x 1.2148 (1.1348) acc_x 65.6250 (72.5000) lr 4.9096e-04 eta 0:00:11
epoch [136/200] batch [30/52] time 0.435 (0.447) data 0.304 (0.315) loss_x loss_x 0.8052 (1.1199) acc_x 84.3750 (72.6042) lr 4.9096e-04 eta 0:00:09
epoch [136/200] batch [35/52] time 0.371 (0.448) data 0.240 (0.316) loss_x loss_x 0.8750 (1.1039) acc_x 84.3750 (73.3036) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [40/52] time 0.571 (0.449) data 0.440 (0.318) loss_x loss_x 1.0352 (1.1155) acc_x 62.5000 (72.3438) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [45/52] time 0.443 (0.454) data 0.312 (0.323) loss_x loss_x 0.8418 (1.0951) acc_x 84.3750 (72.8472) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [50/52] time 0.387 (0.452) data 0.256 (0.320) loss_x loss_x 1.2754 (1.0818) acc_x 68.7500 (73.1875) lr 4.9096e-04 eta 0:00:00
epoch [136/200] batch [5/45] time 0.460 (0.456) data 0.328 (0.324) loss_u loss_u 0.8286 (0.7654) acc_u 18.7500 (31.8750) lr 4.9096e-04 eta 0:00:18
epoch [136/200] batch [10/45] time 0.468 (0.454) data 0.336 (0.323) loss_u loss_u 0.7217 (0.7692) acc_u 31.2500 (28.7500) lr 4.9096e-04 eta 0:00:15
epoch [136/200] batch [15/45] time 0.334 (0.450) data 0.202 (0.319) loss_u loss_u 0.7124 (0.7629) acc_u 31.2500 (29.5833) lr 4.9096e-04 eta 0:00:13
epoch [136/200] batch [20/45] time 0.557 (0.450) data 0.426 (0.318) loss_u loss_u 0.7773 (0.7617) acc_u 25.0000 (29.0625) lr 4.9096e-04 eta 0:00:11
epoch [136/200] batch [25/45] time 0.436 (0.448) data 0.303 (0.316) loss_u loss_u 0.7788 (0.7598) acc_u 28.1250 (29.1250) lr 4.9096e-04 eta 0:00:08
epoch [136/200] batch [30/45] time 0.368 (0.447) data 0.236 (0.315) loss_u loss_u 0.6963 (0.7596) acc_u 37.5000 (29.6875) lr 4.9096e-04 eta 0:00:06
epoch [136/200] batch [35/45] time 0.446 (0.447) data 0.315 (0.316) loss_u loss_u 0.8164 (0.7661) acc_u 18.7500 (29.2857) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [40/45] time 0.485 (0.445) data 0.353 (0.313) loss_u loss_u 0.8232 (0.7653) acc_u 18.7500 (29.0625) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [45/45] time 0.545 (0.445) data 0.413 (0.314) loss_u loss_u 0.8081 (0.7663) acc_u 34.3750 (29.0972) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1303
confident_label rate tensor(0.5249, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1646
clean true:1643
clean false:3
clean_rate:0.9981773997569866
noisy true:190
noisy false:1300
after delete: len(clean_dataset) 1646
after delete: len(noisy_dataset) 1490
epoch [137/200] batch [5/51] time 0.422 (0.420) data 0.290 (0.288) loss_x loss_x 1.1094 (1.1693) acc_x 71.8750 (72.5000) lr 4.7750e-04 eta 0:00:19
epoch [137/200] batch [10/51] time 0.494 (0.465) data 0.361 (0.333) loss_x loss_x 0.6016 (1.0430) acc_x 90.6250 (75.9375) lr 4.7750e-04 eta 0:00:19
epoch [137/200] batch [15/51] time 0.550 (0.489) data 0.420 (0.358) loss_x loss_x 0.7119 (1.0789) acc_x 81.2500 (74.3750) lr 4.7750e-04 eta 0:00:17
epoch [137/200] batch [20/51] time 0.386 (0.477) data 0.255 (0.346) loss_x loss_x 0.8423 (1.1529) acc_x 71.8750 (72.3438) lr 4.7750e-04 eta 0:00:14
epoch [137/200] batch [25/51] time 0.431 (0.468) data 0.301 (0.336) loss_x loss_x 0.8276 (1.1253) acc_x 78.1250 (73.1250) lr 4.7750e-04 eta 0:00:12
epoch [137/200] batch [30/51] time 0.346 (0.461) data 0.216 (0.330) loss_x loss_x 1.4199 (1.1062) acc_x 65.6250 (73.6458) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [35/51] time 0.434 (0.469) data 0.302 (0.338) loss_x loss_x 0.5947 (1.0804) acc_x 87.5000 (74.0179) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [40/51] time 0.444 (0.465) data 0.313 (0.334) loss_x loss_x 1.3750 (1.0817) acc_x 65.6250 (73.6719) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [45/51] time 0.374 (0.463) data 0.243 (0.332) loss_x loss_x 0.7871 (1.0636) acc_x 81.2500 (73.8194) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [50/51] time 0.460 (0.463) data 0.328 (0.332) loss_x loss_x 1.0498 (1.0679) acc_x 75.0000 (74.0000) lr 4.7750e-04 eta 0:00:00
epoch [137/200] batch [5/46] time 0.366 (0.469) data 0.233 (0.338) loss_u loss_u 0.7231 (0.7590) acc_u 31.2500 (28.1250) lr 4.7750e-04 eta 0:00:19
epoch [137/200] batch [10/46] time 0.447 (0.468) data 0.314 (0.336) loss_u loss_u 0.7549 (0.7620) acc_u 34.3750 (30.9375) lr 4.7750e-04 eta 0:00:16
epoch [137/200] batch [15/46] time 0.402 (0.465) data 0.269 (0.334) loss_u loss_u 0.7959 (0.7535) acc_u 28.1250 (31.4583) lr 4.7750e-04 eta 0:00:14
epoch [137/200] batch [20/46] time 0.604 (0.464) data 0.471 (0.333) loss_u loss_u 0.7622 (0.7581) acc_u 37.5000 (31.0938) lr 4.7750e-04 eta 0:00:12
epoch [137/200] batch [25/46] time 0.460 (0.465) data 0.330 (0.333) loss_u loss_u 0.6738 (0.7598) acc_u 40.6250 (30.6250) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [30/46] time 0.523 (0.467) data 0.391 (0.335) loss_u loss_u 0.6797 (0.7592) acc_u 50.0000 (30.9375) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [35/46] time 0.561 (0.466) data 0.430 (0.334) loss_u loss_u 0.8184 (0.7614) acc_u 25.0000 (30.9821) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [40/46] time 0.375 (0.461) data 0.245 (0.330) loss_u loss_u 0.7510 (0.7583) acc_u 31.2500 (31.4844) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [45/46] time 0.395 (0.459) data 0.265 (0.328) loss_u loss_u 0.7188 (0.7583) acc_u 34.3750 (31.3889) lr 4.7750e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1327
confident_label rate tensor(0.5236, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1642
clean true:1640
clean false:2
clean_rate:0.9987819732034104
noisy true:169
noisy false:1325
after delete: len(clean_dataset) 1642
after delete: len(noisy_dataset) 1494
epoch [138/200] batch [5/51] time 0.428 (0.435) data 0.297 (0.304) loss_x loss_x 0.5898 (1.0820) acc_x 87.5000 (71.8750) lr 4.6417e-04 eta 0:00:20
epoch [138/200] batch [10/51] time 0.470 (0.443) data 0.339 (0.312) loss_x loss_x 1.1953 (1.1675) acc_x 75.0000 (70.9375) lr 4.6417e-04 eta 0:00:18
epoch [138/200] batch [15/51] time 0.435 (0.439) data 0.304 (0.308) loss_x loss_x 0.6104 (1.0590) acc_x 84.3750 (73.5417) lr 4.6417e-04 eta 0:00:15
epoch [138/200] batch [20/51] time 0.455 (0.447) data 0.323 (0.316) loss_x loss_x 1.1045 (1.0676) acc_x 65.6250 (73.5938) lr 4.6417e-04 eta 0:00:13
epoch [138/200] batch [25/51] time 0.599 (0.455) data 0.468 (0.324) loss_x loss_x 0.7725 (1.0661) acc_x 68.7500 (72.6250) lr 4.6417e-04 eta 0:00:11
epoch [138/200] batch [30/51] time 0.409 (0.450) data 0.279 (0.319) loss_x loss_x 0.7866 (1.0532) acc_x 87.5000 (73.4375) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [35/51] time 0.594 (0.448) data 0.463 (0.317) loss_x loss_x 0.6777 (1.0677) acc_x 78.1250 (72.6786) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [40/51] time 0.428 (0.443) data 0.298 (0.312) loss_x loss_x 1.2549 (1.0949) acc_x 62.5000 (72.3438) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [45/51] time 0.472 (0.443) data 0.340 (0.312) loss_x loss_x 0.9214 (1.0893) acc_x 78.1250 (72.7778) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [50/51] time 0.480 (0.441) data 0.349 (0.310) loss_x loss_x 1.2148 (1.1139) acc_x 71.8750 (72.1875) lr 4.6417e-04 eta 0:00:00
epoch [138/200] batch [5/46] time 0.372 (0.440) data 0.241 (0.309) loss_u loss_u 0.7881 (0.8144) acc_u 31.2500 (25.6250) lr 4.6417e-04 eta 0:00:18
epoch [138/200] batch [10/46] time 0.637 (0.444) data 0.505 (0.313) loss_u loss_u 0.7295 (0.7769) acc_u 34.3750 (27.8125) lr 4.6417e-04 eta 0:00:15
epoch [138/200] batch [15/46] time 0.377 (0.445) data 0.244 (0.314) loss_u loss_u 0.7354 (0.7660) acc_u 31.2500 (28.9583) lr 4.6417e-04 eta 0:00:13
epoch [138/200] batch [20/46] time 0.550 (0.447) data 0.417 (0.316) loss_u loss_u 0.8633 (0.7657) acc_u 18.7500 (29.0625) lr 4.6417e-04 eta 0:00:11
epoch [138/200] batch [25/46] time 0.352 (0.446) data 0.220 (0.314) loss_u loss_u 0.7788 (0.7608) acc_u 28.1250 (29.7500) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [30/46] time 0.500 (0.445) data 0.368 (0.314) loss_u loss_u 0.7246 (0.7639) acc_u 31.2500 (29.3750) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [35/46] time 0.394 (0.444) data 0.262 (0.313) loss_u loss_u 0.8901 (0.7645) acc_u 15.6250 (29.3750) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [40/46] time 0.498 (0.445) data 0.366 (0.313) loss_u loss_u 0.7153 (0.7659) acc_u 37.5000 (29.1406) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [45/46] time 0.391 (0.442) data 0.259 (0.310) loss_u loss_u 0.6812 (0.7617) acc_u 37.5000 (29.8611) lr 4.6417e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1300
confident_label rate tensor(0.5281, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1656
clean true:1654
clean false:2
clean_rate:0.998792270531401
noisy true:182
noisy false:1298
after delete: len(clean_dataset) 1656
after delete: len(noisy_dataset) 1480
epoch [139/200] batch [5/51] time 0.541 (0.475) data 0.411 (0.344) loss_x loss_x 0.9771 (1.1395) acc_x 75.0000 (71.8750) lr 4.5098e-04 eta 0:00:21
epoch [139/200] batch [10/51] time 0.419 (0.485) data 0.289 (0.354) loss_x loss_x 0.9927 (1.1774) acc_x 65.6250 (70.9375) lr 4.5098e-04 eta 0:00:19
epoch [139/200] batch [15/51] time 0.426 (0.459) data 0.295 (0.328) loss_x loss_x 1.5068 (1.2444) acc_x 65.6250 (69.3750) lr 4.5098e-04 eta 0:00:16
epoch [139/200] batch [20/51] time 0.399 (0.442) data 0.268 (0.311) loss_x loss_x 1.3389 (1.2161) acc_x 71.8750 (70.4688) lr 4.5098e-04 eta 0:00:13
epoch [139/200] batch [25/51] time 0.480 (0.438) data 0.349 (0.307) loss_x loss_x 2.0410 (1.2187) acc_x 59.3750 (70.7500) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [30/51] time 0.393 (0.443) data 0.262 (0.313) loss_x loss_x 1.6553 (1.2224) acc_x 65.6250 (70.3125) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [35/51] time 0.382 (0.446) data 0.251 (0.315) loss_x loss_x 1.3340 (1.2143) acc_x 71.8750 (70.6250) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [40/51] time 0.384 (0.453) data 0.253 (0.322) loss_x loss_x 0.9521 (1.1900) acc_x 68.7500 (70.8594) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [45/51] time 0.371 (0.447) data 0.240 (0.316) loss_x loss_x 1.1191 (1.1633) acc_x 65.6250 (71.0417) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [50/51] time 0.548 (0.453) data 0.417 (0.322) loss_x loss_x 0.8740 (1.1408) acc_x 81.2500 (71.6250) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/46] time 0.464 (0.453) data 0.332 (0.322) loss_u loss_u 0.7842 (0.8039) acc_u 31.2500 (26.2500) lr 4.5098e-04 eta 0:00:18
epoch [139/200] batch [10/46] time 0.411 (0.450) data 0.281 (0.319) loss_u loss_u 0.7495 (0.7851) acc_u 31.2500 (27.5000) lr 4.5098e-04 eta 0:00:16
epoch [139/200] batch [15/46] time 0.598 (0.453) data 0.466 (0.322) loss_u loss_u 0.6733 (0.7653) acc_u 34.3750 (29.1667) lr 4.5098e-04 eta 0:00:14
epoch [139/200] batch [20/46] time 0.600 (0.456) data 0.468 (0.325) loss_u loss_u 0.7744 (0.7660) acc_u 25.0000 (30.0000) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [25/46] time 0.456 (0.454) data 0.324 (0.323) loss_u loss_u 0.7769 (0.7686) acc_u 28.1250 (30.0000) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [30/46] time 0.537 (0.452) data 0.404 (0.321) loss_u loss_u 0.8159 (0.7666) acc_u 18.7500 (29.7917) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [35/46] time 0.557 (0.451) data 0.425 (0.320) loss_u loss_u 0.6348 (0.7635) acc_u 43.7500 (29.9107) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [40/46] time 0.408 (0.449) data 0.276 (0.317) loss_u loss_u 0.7749 (0.7621) acc_u 31.2500 (29.9219) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [45/46] time 0.424 (0.449) data 0.292 (0.317) loss_u loss_u 0.7979 (0.7607) acc_u 21.8750 (30.4167) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1288
confident_label rate tensor(0.5281, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1656
clean true:1654
clean false:2
clean_rate:0.998792270531401
noisy true:194
noisy false:1286
after delete: len(clean_dataset) 1656
after delete: len(noisy_dataset) 1480
epoch [140/200] batch [5/51] time 0.474 (0.434) data 0.343 (0.303) loss_x loss_x 0.7114 (0.9408) acc_x 81.2500 (74.3750) lr 4.3792e-04 eta 0:00:19
epoch [140/200] batch [10/51] time 0.378 (0.441) data 0.247 (0.310) loss_x loss_x 1.3906 (1.1421) acc_x 62.5000 (69.6875) lr 4.3792e-04 eta 0:00:18
epoch [140/200] batch [15/51] time 0.449 (0.460) data 0.319 (0.329) loss_x loss_x 1.3301 (1.1842) acc_x 65.6250 (69.3750) lr 4.3792e-04 eta 0:00:16
epoch [140/200] batch [20/51] time 0.346 (0.457) data 0.215 (0.326) loss_x loss_x 1.2266 (1.1617) acc_x 56.2500 (70.0000) lr 4.3792e-04 eta 0:00:14
epoch [140/200] batch [25/51] time 0.420 (0.452) data 0.289 (0.321) loss_x loss_x 0.6733 (1.1479) acc_x 81.2500 (70.7500) lr 4.3792e-04 eta 0:00:11
epoch [140/200] batch [30/51] time 0.339 (0.445) data 0.208 (0.314) loss_x loss_x 0.6079 (1.1171) acc_x 87.5000 (71.6667) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [35/51] time 0.464 (0.447) data 0.333 (0.316) loss_x loss_x 0.8950 (1.1112) acc_x 78.1250 (71.9643) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [40/51] time 0.430 (0.444) data 0.299 (0.313) loss_x loss_x 1.0234 (1.1047) acc_x 78.1250 (72.5000) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [45/51] time 0.500 (0.444) data 0.369 (0.313) loss_x loss_x 1.2705 (1.1255) acc_x 56.2500 (71.8056) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [50/51] time 0.483 (0.445) data 0.353 (0.314) loss_x loss_x 0.9731 (1.1032) acc_x 78.1250 (72.3125) lr 4.3792e-04 eta 0:00:00
epoch [140/200] batch [5/46] time 0.361 (0.443) data 0.229 (0.312) loss_u loss_u 0.8701 (0.7729) acc_u 18.7500 (28.1250) lr 4.3792e-04 eta 0:00:18
epoch [140/200] batch [10/46] time 0.385 (0.444) data 0.253 (0.313) loss_u loss_u 0.7144 (0.7576) acc_u 34.3750 (30.3125) lr 4.3792e-04 eta 0:00:15
epoch [140/200] batch [15/46] time 0.389 (0.443) data 0.258 (0.312) loss_u loss_u 0.7446 (0.7483) acc_u 43.7500 (31.2500) lr 4.3792e-04 eta 0:00:13
epoch [140/200] batch [20/46] time 0.391 (0.441) data 0.259 (0.310) loss_u loss_u 0.7759 (0.7526) acc_u 31.2500 (31.0938) lr 4.3792e-04 eta 0:00:11
epoch [140/200] batch [25/46] time 0.426 (0.441) data 0.294 (0.310) loss_u loss_u 0.8501 (0.7610) acc_u 15.6250 (30.3750) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [30/46] time 0.439 (0.440) data 0.306 (0.309) loss_u loss_u 0.7603 (0.7635) acc_u 25.0000 (30.2083) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [35/46] time 0.361 (0.441) data 0.229 (0.310) loss_u loss_u 0.8062 (0.7628) acc_u 25.0000 (30.0000) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [40/46] time 0.359 (0.442) data 0.226 (0.311) loss_u loss_u 0.8262 (0.7650) acc_u 25.0000 (30.0000) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [45/46] time 0.540 (0.441) data 0.408 (0.309) loss_u loss_u 0.7593 (0.7661) acc_u 28.1250 (29.8611) lr 4.3792e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1271
confident_label rate tensor(0.5376, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1686
clean true:1684
clean false:2
clean_rate:0.9988137603795967
noisy true:181
noisy false:1269
after delete: len(clean_dataset) 1686
after delete: len(noisy_dataset) 1450
epoch [141/200] batch [5/52] time 0.606 (0.524) data 0.475 (0.393) loss_x loss_x 1.0342 (1.0053) acc_x 68.7500 (74.3750) lr 4.2499e-04 eta 0:00:24
epoch [141/200] batch [10/52] time 0.446 (0.492) data 0.315 (0.360) loss_x loss_x 1.0762 (0.9805) acc_x 68.7500 (75.0000) lr 4.2499e-04 eta 0:00:20
epoch [141/200] batch [15/52] time 0.468 (0.480) data 0.336 (0.349) loss_x loss_x 1.2617 (0.9971) acc_x 71.8750 (74.7917) lr 4.2499e-04 eta 0:00:17
epoch [141/200] batch [20/52] time 0.362 (0.475) data 0.231 (0.343) loss_x loss_x 1.0010 (1.0081) acc_x 81.2500 (74.8438) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [25/52] time 0.554 (0.471) data 0.423 (0.339) loss_x loss_x 1.0557 (1.0243) acc_x 71.8750 (73.7500) lr 4.2499e-04 eta 0:00:12
epoch [141/200] batch [30/52] time 0.514 (0.466) data 0.383 (0.335) loss_x loss_x 1.2148 (1.0425) acc_x 68.7500 (72.5000) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [35/52] time 0.449 (0.465) data 0.318 (0.334) loss_x loss_x 0.8784 (1.0533) acc_x 75.0000 (72.0536) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [40/52] time 0.435 (0.461) data 0.304 (0.330) loss_x loss_x 0.9155 (1.0638) acc_x 81.2500 (71.9531) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [45/52] time 0.413 (0.459) data 0.282 (0.328) loss_x loss_x 0.9614 (1.0807) acc_x 68.7500 (71.5972) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [50/52] time 0.405 (0.457) data 0.274 (0.326) loss_x loss_x 1.0117 (1.0888) acc_x 78.1250 (71.7500) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/45] time 0.438 (0.460) data 0.307 (0.329) loss_u loss_u 0.6787 (0.7691) acc_u 37.5000 (28.1250) lr 4.2499e-04 eta 0:00:18
epoch [141/200] batch [10/45] time 0.453 (0.455) data 0.321 (0.323) loss_u loss_u 0.7300 (0.7647) acc_u 31.2500 (29.6875) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [15/45] time 0.488 (0.458) data 0.356 (0.327) loss_u loss_u 0.7603 (0.7526) acc_u 28.1250 (31.2500) lr 4.2499e-04 eta 0:00:13
epoch [141/200] batch [20/45] time 0.385 (0.452) data 0.254 (0.321) loss_u loss_u 0.8574 (0.7582) acc_u 18.7500 (30.4688) lr 4.2499e-04 eta 0:00:11
epoch [141/200] batch [25/45] time 0.362 (0.450) data 0.231 (0.318) loss_u loss_u 0.7456 (0.7567) acc_u 34.3750 (30.6250) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [30/45] time 0.413 (0.451) data 0.281 (0.319) loss_u loss_u 0.7529 (0.7648) acc_u 34.3750 (29.3750) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [35/45] time 0.416 (0.451) data 0.284 (0.319) loss_u loss_u 0.7383 (0.7652) acc_u 34.3750 (29.2857) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [40/45] time 0.628 (0.451) data 0.496 (0.319) loss_u loss_u 0.8198 (0.7672) acc_u 28.1250 (29.4531) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [45/45] time 0.487 (0.448) data 0.355 (0.317) loss_u loss_u 0.6865 (0.7568) acc_u 40.6250 (30.8333) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1290
confident_label rate tensor(0.5316, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1667
clean true:1664
clean false:3
clean_rate:0.9982003599280144
noisy true:182
noisy false:1287
after delete: len(clean_dataset) 1667
after delete: len(noisy_dataset) 1469
epoch [142/200] batch [5/52] time 0.498 (0.465) data 0.367 (0.334) loss_x loss_x 1.0703 (1.0059) acc_x 71.8750 (75.6250) lr 4.1221e-04 eta 0:00:21
epoch [142/200] batch [10/52] time 0.502 (0.482) data 0.371 (0.350) loss_x loss_x 0.9795 (1.0150) acc_x 65.6250 (73.4375) lr 4.1221e-04 eta 0:00:20
epoch [142/200] batch [15/52] time 0.423 (0.460) data 0.292 (0.329) loss_x loss_x 1.4102 (1.1098) acc_x 71.8750 (72.7083) lr 4.1221e-04 eta 0:00:17
epoch [142/200] batch [20/52] time 0.501 (0.457) data 0.370 (0.326) loss_x loss_x 0.7285 (1.1362) acc_x 84.3750 (72.8125) lr 4.1221e-04 eta 0:00:14
epoch [142/200] batch [25/52] time 0.517 (0.452) data 0.385 (0.321) loss_x loss_x 0.9897 (1.1372) acc_x 71.8750 (72.2500) lr 4.1221e-04 eta 0:00:12
epoch [142/200] batch [30/52] time 0.560 (0.461) data 0.427 (0.330) loss_x loss_x 1.0098 (1.1496) acc_x 78.1250 (71.8750) lr 4.1221e-04 eta 0:00:10
epoch [142/200] batch [35/52] time 0.414 (0.461) data 0.283 (0.330) loss_x loss_x 1.3154 (1.1411) acc_x 59.3750 (71.7857) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [40/52] time 0.399 (0.455) data 0.267 (0.324) loss_x loss_x 1.1162 (1.1450) acc_x 71.8750 (71.8750) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [45/52] time 0.440 (0.455) data 0.309 (0.324) loss_x loss_x 1.1133 (1.1374) acc_x 68.7500 (72.2222) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [50/52] time 0.431 (0.450) data 0.300 (0.318) loss_x loss_x 1.1943 (1.1364) acc_x 65.6250 (72.0625) lr 4.1221e-04 eta 0:00:00
epoch [142/200] batch [5/45] time 0.296 (0.446) data 0.165 (0.314) loss_u loss_u 0.7363 (0.7166) acc_u 34.3750 (36.8750) lr 4.1221e-04 eta 0:00:17
epoch [142/200] batch [10/45] time 0.387 (0.449) data 0.255 (0.317) loss_u loss_u 0.8140 (0.7474) acc_u 25.0000 (32.5000) lr 4.1221e-04 eta 0:00:15
epoch [142/200] batch [15/45] time 0.443 (0.447) data 0.311 (0.316) loss_u loss_u 0.6699 (0.7511) acc_u 40.6250 (32.2917) lr 4.1221e-04 eta 0:00:13
epoch [142/200] batch [20/45] time 0.476 (0.446) data 0.345 (0.314) loss_u loss_u 0.8125 (0.7544) acc_u 25.0000 (31.4062) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [25/45] time 0.373 (0.443) data 0.242 (0.311) loss_u loss_u 0.8218 (0.7630) acc_u 28.1250 (30.2500) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [30/45] time 0.487 (0.445) data 0.356 (0.314) loss_u loss_u 0.7539 (0.7609) acc_u 28.1250 (30.3125) lr 4.1221e-04 eta 0:00:06
epoch [142/200] batch [35/45] time 0.370 (0.443) data 0.238 (0.311) loss_u loss_u 0.7520 (0.7638) acc_u 31.2500 (30.0893) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [40/45] time 0.454 (0.443) data 0.323 (0.312) loss_u loss_u 0.7041 (0.7598) acc_u 40.6250 (30.9375) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [45/45] time 0.435 (0.446) data 0.303 (0.314) loss_u loss_u 0.7646 (0.7605) acc_u 28.1250 (30.6944) lr 4.1221e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1301
confident_label rate tensor(0.5284, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1657
clean true:1656
clean false:1
clean_rate:0.9993964996982498
noisy true:179
noisy false:1300
after delete: len(clean_dataset) 1657
after delete: len(noisy_dataset) 1479
epoch [143/200] batch [5/51] time 0.436 (0.523) data 0.305 (0.391) loss_x loss_x 0.7158 (0.9850) acc_x 81.2500 (75.0000) lr 3.9958e-04 eta 0:00:24
epoch [143/200] batch [10/51] time 0.540 (0.493) data 0.409 (0.362) loss_x loss_x 1.0527 (1.1458) acc_x 75.0000 (72.5000) lr 3.9958e-04 eta 0:00:20
epoch [143/200] batch [15/51] time 0.390 (0.476) data 0.259 (0.344) loss_x loss_x 1.2227 (1.2018) acc_x 68.7500 (70.2083) lr 3.9958e-04 eta 0:00:17
epoch [143/200] batch [20/51] time 0.406 (0.462) data 0.273 (0.331) loss_x loss_x 1.4346 (1.1711) acc_x 59.3750 (70.4688) lr 3.9958e-04 eta 0:00:14
epoch [143/200] batch [25/51] time 0.494 (0.464) data 0.363 (0.333) loss_x loss_x 1.1152 (1.1527) acc_x 81.2500 (71.2500) lr 3.9958e-04 eta 0:00:12
epoch [143/200] batch [30/51] time 0.382 (0.457) data 0.251 (0.326) loss_x loss_x 1.3721 (1.1611) acc_x 65.6250 (70.8333) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [35/51] time 0.341 (0.444) data 0.210 (0.313) loss_x loss_x 0.8564 (1.1765) acc_x 78.1250 (70.3571) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [40/51] time 0.463 (0.442) data 0.332 (0.311) loss_x loss_x 1.1816 (1.1726) acc_x 68.7500 (70.3125) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [45/51] time 0.589 (0.452) data 0.459 (0.321) loss_x loss_x 0.6719 (1.1527) acc_x 84.3750 (70.6250) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [50/51] time 0.484 (0.452) data 0.353 (0.321) loss_x loss_x 0.6528 (1.1279) acc_x 90.6250 (71.3750) lr 3.9958e-04 eta 0:00:00
epoch [143/200] batch [5/46] time 0.494 (0.451) data 0.362 (0.320) loss_u loss_u 0.7158 (0.7803) acc_u 40.6250 (28.7500) lr 3.9958e-04 eta 0:00:18
epoch [143/200] batch [10/46] time 0.394 (0.445) data 0.262 (0.314) loss_u loss_u 0.7178 (0.7828) acc_u 34.3750 (28.1250) lr 3.9958e-04 eta 0:00:16
epoch [143/200] batch [15/46] time 0.566 (0.442) data 0.434 (0.311) loss_u loss_u 0.7778 (0.7775) acc_u 31.2500 (27.9167) lr 3.9958e-04 eta 0:00:13
epoch [143/200] batch [20/46] time 0.395 (0.443) data 0.263 (0.311) loss_u loss_u 0.7324 (0.7721) acc_u 28.1250 (28.7500) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [25/46] time 0.404 (0.441) data 0.272 (0.309) loss_u loss_u 0.7539 (0.7653) acc_u 28.1250 (29.6250) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [30/46] time 0.410 (0.439) data 0.278 (0.308) loss_u loss_u 0.8115 (0.7666) acc_u 21.8750 (29.3750) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [35/46] time 0.476 (0.443) data 0.345 (0.311) loss_u loss_u 0.7593 (0.7686) acc_u 31.2500 (29.1964) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [40/46] time 0.458 (0.443) data 0.326 (0.312) loss_u loss_u 0.7598 (0.7677) acc_u 34.3750 (29.6875) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [45/46] time 0.351 (0.442) data 0.220 (0.311) loss_u loss_u 0.7900 (0.7638) acc_u 18.7500 (30.1389) lr 3.9958e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1263
confident_label rate tensor(0.5360, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1681
clean true:1678
clean false:3
clean_rate:0.9982153480071386
noisy true:195
noisy false:1260
after delete: len(clean_dataset) 1681
after delete: len(noisy_dataset) 1455
epoch [144/200] batch [5/52] time 0.354 (0.411) data 0.224 (0.281) loss_x loss_x 1.2803 (1.1948) acc_x 65.6250 (66.8750) lr 3.8709e-04 eta 0:00:19
epoch [144/200] batch [10/52] time 0.412 (0.431) data 0.281 (0.300) loss_x loss_x 0.9917 (1.0649) acc_x 65.6250 (69.0625) lr 3.8709e-04 eta 0:00:18
epoch [144/200] batch [15/52] time 0.332 (0.455) data 0.201 (0.324) loss_x loss_x 1.2832 (1.1221) acc_x 81.2500 (72.0833) lr 3.8709e-04 eta 0:00:16
epoch [144/200] batch [20/52] time 0.391 (0.439) data 0.261 (0.308) loss_x loss_x 1.7764 (1.1058) acc_x 59.3750 (72.3438) lr 3.8709e-04 eta 0:00:14
epoch [144/200] batch [25/52] time 0.490 (0.438) data 0.360 (0.308) loss_x loss_x 0.9106 (1.0889) acc_x 78.1250 (72.8750) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [30/52] time 0.581 (0.449) data 0.450 (0.318) loss_x loss_x 0.9365 (1.0805) acc_x 81.2500 (73.3333) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [35/52] time 0.415 (0.450) data 0.285 (0.319) loss_x loss_x 1.0264 (1.0418) acc_x 84.3750 (74.9107) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [40/52] time 0.419 (0.447) data 0.289 (0.316) loss_x loss_x 0.9766 (1.0506) acc_x 68.7500 (74.4531) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [45/52] time 0.597 (0.450) data 0.466 (0.320) loss_x loss_x 1.7080 (1.0518) acc_x 62.5000 (74.3750) lr 3.8709e-04 eta 0:00:03
epoch [144/200] batch [50/52] time 0.532 (0.453) data 0.400 (0.322) loss_x loss_x 1.0752 (1.0463) acc_x 71.8750 (74.4375) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/45] time 0.378 (0.451) data 0.246 (0.320) loss_u loss_u 0.6943 (0.7604) acc_u 43.7500 (31.8750) lr 3.8709e-04 eta 0:00:18
epoch [144/200] batch [10/45] time 0.462 (0.449) data 0.330 (0.318) loss_u loss_u 0.6797 (0.7437) acc_u 50.0000 (33.1250) lr 3.8709e-04 eta 0:00:15
epoch [144/200] batch [15/45] time 0.714 (0.456) data 0.582 (0.325) loss_u loss_u 0.7441 (0.7486) acc_u 34.3750 (32.0833) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [20/45] time 0.513 (0.455) data 0.381 (0.324) loss_u loss_u 0.6934 (0.7523) acc_u 37.5000 (32.0312) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [25/45] time 0.369 (0.453) data 0.237 (0.322) loss_u loss_u 0.8242 (0.7533) acc_u 18.7500 (31.3750) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [30/45] time 0.406 (0.451) data 0.275 (0.320) loss_u loss_u 0.6753 (0.7524) acc_u 40.6250 (31.3542) lr 3.8709e-04 eta 0:00:06
epoch [144/200] batch [35/45] time 0.452 (0.448) data 0.320 (0.316) loss_u loss_u 0.6797 (0.7527) acc_u 40.6250 (31.6071) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [40/45] time 0.359 (0.447) data 0.227 (0.316) loss_u loss_u 0.8262 (0.7550) acc_u 25.0000 (31.0938) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [45/45] time 0.420 (0.445) data 0.289 (0.314) loss_u loss_u 0.7817 (0.7542) acc_u 25.0000 (31.2500) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1288
confident_label rate tensor(0.5316, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1667
clean true:1665
clean false:2
clean_rate:0.9988002399520096
noisy true:183
noisy false:1286
after delete: len(clean_dataset) 1667
after delete: len(noisy_dataset) 1469
epoch [145/200] batch [5/52] time 0.392 (0.444) data 0.262 (0.314) loss_x loss_x 1.6182 (1.2524) acc_x 62.5000 (69.3750) lr 3.7476e-04 eta 0:00:20
epoch [145/200] batch [10/52] time 0.521 (0.450) data 0.391 (0.319) loss_x loss_x 1.0439 (1.1340) acc_x 68.7500 (73.7500) lr 3.7476e-04 eta 0:00:18
epoch [145/200] batch [15/52] time 0.418 (0.438) data 0.287 (0.307) loss_x loss_x 1.2891 (1.0693) acc_x 68.7500 (75.8333) lr 3.7476e-04 eta 0:00:16
epoch [145/200] batch [20/52] time 0.556 (0.440) data 0.425 (0.310) loss_x loss_x 1.3018 (1.0547) acc_x 62.5000 (75.6250) lr 3.7476e-04 eta 0:00:14
epoch [145/200] batch [25/52] time 0.385 (0.446) data 0.254 (0.315) loss_x loss_x 0.8950 (1.0671) acc_x 81.2500 (74.7500) lr 3.7476e-04 eta 0:00:12
epoch [145/200] batch [30/52] time 0.389 (0.461) data 0.258 (0.330) loss_x loss_x 1.4688 (1.0716) acc_x 65.6250 (74.1667) lr 3.7476e-04 eta 0:00:10
epoch [145/200] batch [35/52] time 0.385 (0.460) data 0.253 (0.329) loss_x loss_x 0.8750 (1.0708) acc_x 71.8750 (73.7500) lr 3.7476e-04 eta 0:00:07
epoch [145/200] batch [40/52] time 0.527 (0.457) data 0.396 (0.326) loss_x loss_x 1.2344 (1.0682) acc_x 65.6250 (73.7500) lr 3.7476e-04 eta 0:00:05
epoch [145/200] batch [45/52] time 0.564 (0.455) data 0.433 (0.324) loss_x loss_x 1.3975 (1.0869) acc_x 62.5000 (73.1944) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [50/52] time 0.340 (0.452) data 0.209 (0.321) loss_x loss_x 1.0322 (1.0910) acc_x 81.2500 (73.4375) lr 3.7476e-04 eta 0:00:00
epoch [145/200] batch [5/45] time 0.377 (0.450) data 0.245 (0.319) loss_u loss_u 0.7310 (0.7126) acc_u 28.1250 (34.3750) lr 3.7476e-04 eta 0:00:18
epoch [145/200] batch [10/45] time 0.464 (0.452) data 0.333 (0.321) loss_u loss_u 0.8159 (0.7494) acc_u 21.8750 (31.8750) lr 3.7476e-04 eta 0:00:15
epoch [145/200] batch [15/45] time 0.360 (0.448) data 0.228 (0.317) loss_u loss_u 0.7798 (0.7579) acc_u 21.8750 (29.7917) lr 3.7476e-04 eta 0:00:13
epoch [145/200] batch [20/45] time 0.558 (0.455) data 0.426 (0.323) loss_u loss_u 0.7202 (0.7651) acc_u 34.3750 (28.4375) lr 3.7476e-04 eta 0:00:11
epoch [145/200] batch [25/45] time 0.476 (0.456) data 0.345 (0.325) loss_u loss_u 0.7773 (0.7610) acc_u 28.1250 (29.2500) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [30/45] time 0.386 (0.455) data 0.255 (0.324) loss_u loss_u 0.8350 (0.7676) acc_u 18.7500 (28.8542) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [35/45] time 0.728 (0.457) data 0.597 (0.326) loss_u loss_u 0.6475 (0.7562) acc_u 43.7500 (30.0893) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [40/45] time 0.421 (0.456) data 0.290 (0.324) loss_u loss_u 0.8140 (0.7611) acc_u 25.0000 (29.8438) lr 3.7476e-04 eta 0:00:02
epoch [145/200] batch [45/45] time 0.326 (0.451) data 0.194 (0.320) loss_u loss_u 0.7588 (0.7580) acc_u 28.1250 (30.2083) lr 3.7476e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1295
confident_label rate tensor(0.5303, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1663
clean true:1661
clean false:2
clean_rate:0.9987973541791942
noisy true:180
noisy false:1293
after delete: len(clean_dataset) 1663
after delete: len(noisy_dataset) 1473
epoch [146/200] batch [5/51] time 0.500 (0.471) data 0.369 (0.340) loss_x loss_x 1.3379 (1.0065) acc_x 62.5000 (73.7500) lr 3.6258e-04 eta 0:00:21
epoch [146/200] batch [10/51] time 0.488 (0.456) data 0.357 (0.324) loss_x loss_x 0.8750 (0.9948) acc_x 75.0000 (72.1875) lr 3.6258e-04 eta 0:00:18
epoch [146/200] batch [15/51] time 0.452 (0.454) data 0.322 (0.323) loss_x loss_x 1.3076 (0.9962) acc_x 75.0000 (72.9167) lr 3.6258e-04 eta 0:00:16
epoch [146/200] batch [20/51] time 0.366 (0.447) data 0.235 (0.316) loss_x loss_x 1.2129 (1.0197) acc_x 68.7500 (72.3438) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [25/51] time 0.382 (0.444) data 0.249 (0.313) loss_x loss_x 1.4561 (1.0623) acc_x 68.7500 (71.8750) lr 3.6258e-04 eta 0:00:11
epoch [146/200] batch [30/51] time 0.488 (0.448) data 0.356 (0.317) loss_x loss_x 0.7671 (1.0844) acc_x 81.2500 (71.8750) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [35/51] time 0.401 (0.447) data 0.271 (0.316) loss_x loss_x 1.0830 (1.0722) acc_x 78.1250 (72.1429) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [40/51] time 0.414 (0.447) data 0.283 (0.316) loss_x loss_x 1.1152 (1.0822) acc_x 71.8750 (71.9531) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [45/51] time 0.498 (0.449) data 0.367 (0.318) loss_x loss_x 1.5020 (1.0767) acc_x 71.8750 (72.4306) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [50/51] time 0.451 (0.448) data 0.317 (0.317) loss_x loss_x 1.6357 (1.0804) acc_x 62.5000 (72.1250) lr 3.6258e-04 eta 0:00:00
epoch [146/200] batch [5/46] time 0.562 (0.455) data 0.430 (0.324) loss_u loss_u 0.7686 (0.7853) acc_u 34.3750 (29.3750) lr 3.6258e-04 eta 0:00:18
epoch [146/200] batch [10/46] time 0.437 (0.449) data 0.306 (0.318) loss_u loss_u 0.8066 (0.7752) acc_u 28.1250 (30.9375) lr 3.6258e-04 eta 0:00:16
epoch [146/200] batch [15/46] time 0.393 (0.447) data 0.261 (0.315) loss_u loss_u 0.6362 (0.7597) acc_u 43.7500 (32.9167) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [20/46] time 0.407 (0.446) data 0.276 (0.314) loss_u loss_u 0.8145 (0.7581) acc_u 18.7500 (32.3438) lr 3.6258e-04 eta 0:00:11
epoch [146/200] batch [25/46] time 0.562 (0.449) data 0.430 (0.317) loss_u loss_u 0.8516 (0.7566) acc_u 15.6250 (32.2500) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [30/46] time 0.431 (0.446) data 0.299 (0.315) loss_u loss_u 0.8457 (0.7595) acc_u 18.7500 (31.3542) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [35/46] time 0.358 (0.444) data 0.226 (0.312) loss_u loss_u 0.7222 (0.7597) acc_u 28.1250 (30.7143) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [40/46] time 0.382 (0.446) data 0.250 (0.315) loss_u loss_u 0.7515 (0.7645) acc_u 31.2500 (29.7656) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [45/46] time 0.402 (0.445) data 0.270 (0.313) loss_u loss_u 0.7817 (0.7620) acc_u 34.3750 (30.2778) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1280
confident_label rate tensor(0.5328, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1671
clean true:1670
clean false:1
clean_rate:0.9994015559545183
noisy true:186
noisy false:1279
after delete: len(clean_dataset) 1671
after delete: len(noisy_dataset) 1465
epoch [147/200] batch [5/52] time 0.534 (0.432) data 0.403 (0.301) loss_x loss_x 1.1387 (1.0772) acc_x 75.0000 (74.3750) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [10/52] time 0.357 (0.440) data 0.226 (0.309) loss_x loss_x 1.3389 (1.0865) acc_x 59.3750 (73.7500) lr 3.5055e-04 eta 0:00:18
epoch [147/200] batch [15/52] time 0.458 (0.440) data 0.326 (0.309) loss_x loss_x 1.1543 (1.0488) acc_x 78.1250 (74.5833) lr 3.5055e-04 eta 0:00:16
epoch [147/200] batch [20/52] time 0.573 (0.460) data 0.442 (0.329) loss_x loss_x 0.7334 (1.0958) acc_x 81.2500 (73.2812) lr 3.5055e-04 eta 0:00:14
epoch [147/200] batch [25/52] time 0.610 (0.472) data 0.479 (0.341) loss_x loss_x 1.3906 (1.0664) acc_x 68.7500 (74.2500) lr 3.5055e-04 eta 0:00:12
epoch [147/200] batch [30/52] time 0.423 (0.465) data 0.292 (0.334) loss_x loss_x 1.1016 (1.0762) acc_x 71.8750 (73.6458) lr 3.5055e-04 eta 0:00:10
epoch [147/200] batch [35/52] time 0.358 (0.458) data 0.227 (0.327) loss_x loss_x 0.9341 (1.0758) acc_x 78.1250 (73.5714) lr 3.5055e-04 eta 0:00:07
epoch [147/200] batch [40/52] time 0.618 (0.456) data 0.487 (0.326) loss_x loss_x 1.1387 (1.0695) acc_x 71.8750 (73.7500) lr 3.5055e-04 eta 0:00:05
epoch [147/200] batch [45/52] time 0.383 (0.449) data 0.252 (0.318) loss_x loss_x 1.6104 (1.0806) acc_x 71.8750 (73.4028) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [50/52] time 0.412 (0.446) data 0.281 (0.315) loss_x loss_x 1.0762 (1.0890) acc_x 78.1250 (73.5625) lr 3.5055e-04 eta 0:00:00
epoch [147/200] batch [5/45] time 0.381 (0.444) data 0.249 (0.313) loss_u loss_u 0.7573 (0.7675) acc_u 34.3750 (30.6250) lr 3.5055e-04 eta 0:00:17
epoch [147/200] batch [10/45] time 0.504 (0.447) data 0.373 (0.315) loss_u loss_u 0.7515 (0.7453) acc_u 31.2500 (32.5000) lr 3.5055e-04 eta 0:00:15
epoch [147/200] batch [15/45] time 0.340 (0.443) data 0.209 (0.312) loss_u loss_u 0.6509 (0.7442) acc_u 43.7500 (32.9167) lr 3.5055e-04 eta 0:00:13
epoch [147/200] batch [20/45] time 0.417 (0.441) data 0.287 (0.310) loss_u loss_u 0.8062 (0.7477) acc_u 25.0000 (32.0312) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [25/45] time 0.447 (0.444) data 0.315 (0.313) loss_u loss_u 0.8530 (0.7570) acc_u 21.8750 (30.5000) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [30/45] time 0.533 (0.449) data 0.401 (0.318) loss_u loss_u 0.7524 (0.7507) acc_u 28.1250 (31.0417) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [35/45] time 0.381 (0.445) data 0.248 (0.314) loss_u loss_u 0.7188 (0.7503) acc_u 31.2500 (31.3393) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [40/45] time 0.423 (0.443) data 0.293 (0.312) loss_u loss_u 0.7207 (0.7506) acc_u 37.5000 (31.4844) lr 3.5055e-04 eta 0:00:02
epoch [147/200] batch [45/45] time 0.500 (0.444) data 0.369 (0.313) loss_u loss_u 0.7300 (0.7473) acc_u 37.5000 (32.0139) lr 3.5055e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1308
confident_label rate tensor(0.5290, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1659
clean true:1656
clean false:3
clean_rate:0.9981916817359855
noisy true:172
noisy false:1305
after delete: len(clean_dataset) 1659
after delete: len(noisy_dataset) 1477
epoch [148/200] batch [5/51] time 0.347 (0.454) data 0.217 (0.323) loss_x loss_x 0.7656 (1.1441) acc_x 78.1250 (74.3750) lr 3.3869e-04 eta 0:00:20
epoch [148/200] batch [10/51] time 0.426 (0.458) data 0.295 (0.327) loss_x loss_x 0.8579 (0.9926) acc_x 75.0000 (75.9375) lr 3.3869e-04 eta 0:00:18
epoch [148/200] batch [15/51] time 0.435 (0.475) data 0.305 (0.344) loss_x loss_x 0.9575 (0.9982) acc_x 75.0000 (76.4583) lr 3.3869e-04 eta 0:00:17
epoch [148/200] batch [20/51] time 0.596 (0.475) data 0.465 (0.345) loss_x loss_x 1.5918 (1.0415) acc_x 65.6250 (74.6875) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [25/51] time 0.341 (0.467) data 0.211 (0.337) loss_x loss_x 0.6802 (0.9795) acc_x 78.1250 (75.5000) lr 3.3869e-04 eta 0:00:12
epoch [148/200] batch [30/51] time 0.485 (0.465) data 0.355 (0.335) loss_x loss_x 1.1895 (1.0133) acc_x 62.5000 (73.8542) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [35/51] time 0.490 (0.469) data 0.359 (0.338) loss_x loss_x 1.4551 (1.0152) acc_x 62.5000 (73.4821) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [40/51] time 0.587 (0.470) data 0.457 (0.339) loss_x loss_x 0.8838 (1.0118) acc_x 71.8750 (73.4375) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [45/51] time 0.404 (0.466) data 0.273 (0.336) loss_x loss_x 1.0586 (1.0381) acc_x 68.7500 (72.7778) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [50/51] time 0.418 (0.462) data 0.287 (0.332) loss_x loss_x 0.9580 (1.0310) acc_x 71.8750 (72.6875) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/46] time 0.471 (0.459) data 0.339 (0.328) loss_u loss_u 0.7368 (0.7342) acc_u 31.2500 (32.5000) lr 3.3869e-04 eta 0:00:18
epoch [148/200] batch [10/46] time 0.451 (0.460) data 0.318 (0.329) loss_u loss_u 0.7319 (0.7451) acc_u 34.3750 (31.2500) lr 3.3869e-04 eta 0:00:16
epoch [148/200] batch [15/46] time 0.473 (0.462) data 0.341 (0.331) loss_u loss_u 0.7871 (0.7556) acc_u 31.2500 (32.0833) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [20/46] time 0.419 (0.459) data 0.286 (0.328) loss_u loss_u 0.8013 (0.7570) acc_u 31.2500 (32.1875) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [25/46] time 0.362 (0.458) data 0.230 (0.327) loss_u loss_u 0.8755 (0.7553) acc_u 12.5000 (32.3750) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [30/46] time 0.805 (0.463) data 0.672 (0.332) loss_u loss_u 0.7910 (0.7581) acc_u 34.3750 (31.7708) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [35/46] time 0.485 (0.464) data 0.352 (0.332) loss_u loss_u 0.7144 (0.7580) acc_u 34.3750 (31.4286) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [40/46] time 0.349 (0.464) data 0.216 (0.332) loss_u loss_u 0.6743 (0.7561) acc_u 43.7500 (31.6406) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [45/46] time 0.561 (0.466) data 0.428 (0.334) loss_u loss_u 0.7349 (0.7593) acc_u 34.3750 (31.3194) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1271
confident_label rate tensor(0.5392, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1691
clean true:1689
clean false:2
clean_rate:0.9988172678888232
noisy true:176
noisy false:1269
after delete: len(clean_dataset) 1691
after delete: len(noisy_dataset) 1445
epoch [149/200] batch [5/52] time 0.461 (0.437) data 0.330 (0.306) loss_x loss_x 0.9263 (1.1039) acc_x 75.0000 (71.2500) lr 3.2699e-04 eta 0:00:20
epoch [149/200] batch [10/52] time 0.365 (0.423) data 0.234 (0.291) loss_x loss_x 1.0420 (1.0368) acc_x 75.0000 (72.5000) lr 3.2699e-04 eta 0:00:17
epoch [149/200] batch [15/52] time 0.518 (0.434) data 0.387 (0.303) loss_x loss_x 1.0322 (1.0444) acc_x 71.8750 (72.7083) lr 3.2699e-04 eta 0:00:16
epoch [149/200] batch [20/52] time 0.359 (0.425) data 0.228 (0.294) loss_x loss_x 1.0352 (1.0352) acc_x 75.0000 (73.1250) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [25/52] time 0.352 (0.424) data 0.221 (0.293) loss_x loss_x 1.0479 (1.0620) acc_x 78.1250 (72.8750) lr 3.2699e-04 eta 0:00:11
epoch [149/200] batch [30/52] time 0.682 (0.432) data 0.551 (0.302) loss_x loss_x 0.8408 (1.0655) acc_x 71.8750 (72.7083) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [35/52] time 0.471 (0.445) data 0.341 (0.314) loss_x loss_x 1.0254 (1.0855) acc_x 75.0000 (72.4107) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [40/52] time 0.497 (0.448) data 0.366 (0.317) loss_x loss_x 1.4102 (1.0838) acc_x 65.6250 (72.4219) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [45/52] time 0.493 (0.445) data 0.362 (0.314) loss_x loss_x 0.9609 (1.0655) acc_x 78.1250 (72.8472) lr 3.2699e-04 eta 0:00:03
epoch [149/200] batch [50/52] time 0.489 (0.446) data 0.358 (0.315) loss_x loss_x 1.7666 (1.0712) acc_x 46.8750 (72.3125) lr 3.2699e-04 eta 0:00:00
epoch [149/200] batch [5/45] time 0.493 (0.444) data 0.361 (0.313) loss_u loss_u 0.7002 (0.7761) acc_u 40.6250 (28.1250) lr 3.2699e-04 eta 0:00:17
epoch [149/200] batch [10/45] time 0.471 (0.445) data 0.339 (0.314) loss_u loss_u 0.7700 (0.7850) acc_u 34.3750 (27.5000) lr 3.2699e-04 eta 0:00:15
epoch [149/200] batch [15/45] time 0.356 (0.444) data 0.224 (0.312) loss_u loss_u 0.7593 (0.7717) acc_u 40.6250 (29.3750) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [20/45] time 0.343 (0.440) data 0.211 (0.308) loss_u loss_u 0.7651 (0.7747) acc_u 28.1250 (29.0625) lr 3.2699e-04 eta 0:00:10
epoch [149/200] batch [25/45] time 0.585 (0.442) data 0.453 (0.311) loss_u loss_u 0.7422 (0.7781) acc_u 28.1250 (28.8750) lr 3.2699e-04 eta 0:00:08
epoch [149/200] batch [30/45] time 0.538 (0.442) data 0.406 (0.311) loss_u loss_u 0.8037 (0.7760) acc_u 31.2500 (29.2708) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [35/45] time 0.353 (0.439) data 0.221 (0.308) loss_u loss_u 0.6738 (0.7705) acc_u 37.5000 (30.0000) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [40/45] time 0.397 (0.440) data 0.265 (0.309) loss_u loss_u 0.6929 (0.7681) acc_u 34.3750 (30.3125) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [45/45] time 0.356 (0.440) data 0.224 (0.309) loss_u loss_u 0.7905 (0.7743) acc_u 28.1250 (29.4444) lr 3.2699e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1272
confident_label rate tensor(0.5402, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1694
clean true:1689
clean false:5
clean_rate:0.9970484061393152
noisy true:175
noisy false:1267
after delete: len(clean_dataset) 1694
after delete: len(noisy_dataset) 1442
epoch [150/200] batch [5/52] time 0.424 (0.459) data 0.293 (0.328) loss_x loss_x 0.8120 (1.1779) acc_x 78.1250 (73.1250) lr 3.1545e-04 eta 0:00:21
epoch [150/200] batch [10/52] time 0.535 (0.459) data 0.405 (0.328) loss_x loss_x 0.8911 (1.1065) acc_x 78.1250 (73.7500) lr 3.1545e-04 eta 0:00:19
epoch [150/200] batch [15/52] time 0.505 (0.467) data 0.374 (0.336) loss_x loss_x 0.9497 (1.1013) acc_x 71.8750 (72.2917) lr 3.1545e-04 eta 0:00:17
epoch [150/200] batch [20/52] time 0.432 (0.456) data 0.301 (0.325) loss_x loss_x 1.5830 (1.1686) acc_x 62.5000 (71.8750) lr 3.1545e-04 eta 0:00:14
epoch [150/200] batch [25/52] time 0.491 (0.456) data 0.359 (0.325) loss_x loss_x 0.8892 (1.1088) acc_x 75.0000 (73.1250) lr 3.1545e-04 eta 0:00:12
epoch [150/200] batch [30/52] time 0.428 (0.450) data 0.297 (0.319) loss_x loss_x 0.9116 (1.0926) acc_x 71.8750 (73.3333) lr 3.1545e-04 eta 0:00:09
epoch [150/200] batch [35/52] time 0.419 (0.447) data 0.289 (0.316) loss_x loss_x 0.7368 (1.1021) acc_x 87.5000 (73.8393) lr 3.1545e-04 eta 0:00:07
epoch [150/200] batch [40/52] time 0.404 (0.450) data 0.274 (0.319) loss_x loss_x 1.0908 (1.1261) acc_x 68.7500 (73.8281) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [45/52] time 0.556 (0.452) data 0.425 (0.321) loss_x loss_x 0.8564 (1.1321) acc_x 78.1250 (73.5417) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [50/52] time 0.502 (0.449) data 0.371 (0.318) loss_x loss_x 1.2178 (1.1276) acc_x 68.7500 (73.6875) lr 3.1545e-04 eta 0:00:00
epoch [150/200] batch [5/45] time 0.414 (0.447) data 0.282 (0.316) loss_u loss_u 0.6436 (0.7192) acc_u 46.8750 (36.8750) lr 3.1545e-04 eta 0:00:17
epoch [150/200] batch [10/45] time 0.383 (0.449) data 0.249 (0.317) loss_u loss_u 0.7798 (0.7477) acc_u 25.0000 (31.8750) lr 3.1545e-04 eta 0:00:15
epoch [150/200] batch [15/45] time 0.494 (0.447) data 0.362 (0.316) loss_u loss_u 0.8364 (0.7416) acc_u 21.8750 (32.0833) lr 3.1545e-04 eta 0:00:13
epoch [150/200] batch [20/45] time 0.417 (0.448) data 0.285 (0.316) loss_u loss_u 0.8140 (0.7525) acc_u 21.8750 (30.1562) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [25/45] time 0.341 (0.446) data 0.209 (0.314) loss_u loss_u 0.7544 (0.7525) acc_u 28.1250 (30.0000) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [30/45] time 0.519 (0.446) data 0.387 (0.314) loss_u loss_u 0.8438 (0.7561) acc_u 21.8750 (30.0000) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [35/45] time 0.366 (0.442) data 0.235 (0.311) loss_u loss_u 0.8442 (0.7665) acc_u 18.7500 (28.8393) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [40/45] time 0.415 (0.442) data 0.284 (0.310) loss_u loss_u 0.7603 (0.7634) acc_u 28.1250 (29.0625) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [45/45] time 0.368 (0.442) data 0.236 (0.311) loss_u loss_u 0.7207 (0.7597) acc_u 37.5000 (29.7222) lr 3.1545e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1254
confident_label rate tensor(0.5411, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1697
clean true:1692
clean false:5
clean_rate:0.9970536240424278
noisy true:190
noisy false:1249
after delete: len(clean_dataset) 1697
after delete: len(noisy_dataset) 1439
epoch [151/200] batch [5/53] time 0.517 (0.461) data 0.386 (0.330) loss_x loss_x 1.1182 (1.0680) acc_x 71.8750 (73.7500) lr 3.0409e-04 eta 0:00:22
epoch [151/200] batch [10/53] time 0.427 (0.465) data 0.296 (0.334) loss_x loss_x 0.6694 (1.1139) acc_x 81.2500 (73.1250) lr 3.0409e-04 eta 0:00:20
epoch [151/200] batch [15/53] time 0.560 (0.468) data 0.428 (0.337) loss_x loss_x 0.9634 (1.0439) acc_x 78.1250 (74.5833) lr 3.0409e-04 eta 0:00:17
epoch [151/200] batch [20/53] time 0.594 (0.481) data 0.463 (0.350) loss_x loss_x 0.8301 (1.0217) acc_x 78.1250 (74.6875) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [25/53] time 0.408 (0.480) data 0.277 (0.348) loss_x loss_x 0.8477 (1.0348) acc_x 78.1250 (74.0000) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [30/53] time 0.432 (0.470) data 0.300 (0.339) loss_x loss_x 0.5142 (1.0217) acc_x 87.5000 (74.3750) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [35/53] time 0.407 (0.464) data 0.276 (0.333) loss_x loss_x 1.1533 (1.0336) acc_x 75.0000 (74.0179) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [40/53] time 0.517 (0.463) data 0.386 (0.332) loss_x loss_x 1.3584 (1.0333) acc_x 56.2500 (73.6719) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [45/53] time 0.539 (0.463) data 0.408 (0.332) loss_x loss_x 0.8345 (1.0167) acc_x 78.1250 (74.0278) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [50/53] time 0.420 (0.458) data 0.289 (0.326) loss_x loss_x 1.1143 (1.0217) acc_x 65.6250 (73.8125) lr 3.0409e-04 eta 0:00:01
epoch [151/200] batch [5/44] time 0.428 (0.448) data 0.296 (0.317) loss_u loss_u 0.6099 (0.7435) acc_u 56.2500 (36.2500) lr 3.0409e-04 eta 0:00:17
epoch [151/200] batch [10/44] time 0.458 (0.447) data 0.326 (0.316) loss_u loss_u 0.8203 (0.7490) acc_u 31.2500 (35.3125) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [15/44] time 0.467 (0.445) data 0.336 (0.313) loss_u loss_u 0.8071 (0.7551) acc_u 21.8750 (33.7500) lr 3.0409e-04 eta 0:00:12
epoch [151/200] batch [20/44] time 0.367 (0.440) data 0.235 (0.308) loss_u loss_u 0.6279 (0.7510) acc_u 43.7500 (33.2812) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [25/44] time 0.435 (0.439) data 0.304 (0.308) loss_u loss_u 0.7778 (0.7620) acc_u 40.6250 (32.1250) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [30/44] time 0.441 (0.442) data 0.309 (0.311) loss_u loss_u 0.7231 (0.7633) acc_u 37.5000 (31.8750) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [35/44] time 0.366 (0.439) data 0.233 (0.308) loss_u loss_u 0.7437 (0.7680) acc_u 34.3750 (30.8929) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [40/44] time 0.544 (0.444) data 0.410 (0.312) loss_u loss_u 0.6729 (0.7611) acc_u 37.5000 (31.7969) lr 3.0409e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1245
confident_label rate tensor(0.5453, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1710
clean true:1708
clean false:2
clean_rate:0.9988304093567252
noisy true:183
noisy false:1243
after delete: len(clean_dataset) 1710
after delete: len(noisy_dataset) 1426
epoch [152/200] batch [5/53] time 0.504 (0.418) data 0.373 (0.287) loss_x loss_x 0.8389 (0.9311) acc_x 75.0000 (76.8750) lr 2.9289e-04 eta 0:00:20
epoch [152/200] batch [10/53] time 0.396 (0.421) data 0.265 (0.290) loss_x loss_x 0.6680 (0.8438) acc_x 81.2500 (77.5000) lr 2.9289e-04 eta 0:00:18
epoch [152/200] batch [15/53] time 0.482 (0.434) data 0.351 (0.303) loss_x loss_x 0.9619 (0.9110) acc_x 71.8750 (74.7917) lr 2.9289e-04 eta 0:00:16
epoch [152/200] batch [20/53] time 0.625 (0.444) data 0.494 (0.313) loss_x loss_x 0.8867 (0.9370) acc_x 81.2500 (74.5312) lr 2.9289e-04 eta 0:00:14
epoch [152/200] batch [25/53] time 0.540 (0.452) data 0.409 (0.320) loss_x loss_x 1.1406 (0.9865) acc_x 71.8750 (74.3750) lr 2.9289e-04 eta 0:00:12
epoch [152/200] batch [30/53] time 0.448 (0.452) data 0.316 (0.320) loss_x loss_x 1.0742 (0.9937) acc_x 78.1250 (74.4792) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [35/53] time 0.428 (0.451) data 0.296 (0.320) loss_x loss_x 0.8545 (0.9938) acc_x 81.2500 (74.7321) lr 2.9289e-04 eta 0:00:08
epoch [152/200] batch [40/53] time 0.433 (0.455) data 0.302 (0.323) loss_x loss_x 0.7593 (1.0054) acc_x 78.1250 (74.2188) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [45/53] time 0.565 (0.461) data 0.434 (0.329) loss_x loss_x 1.4668 (1.0008) acc_x 68.7500 (74.5139) lr 2.9289e-04 eta 0:00:03
epoch [152/200] batch [50/53] time 0.452 (0.459) data 0.321 (0.328) loss_x loss_x 0.9575 (1.0036) acc_x 75.0000 (75.0000) lr 2.9289e-04 eta 0:00:01
epoch [152/200] batch [5/44] time 0.604 (0.457) data 0.472 (0.325) loss_u loss_u 0.7461 (0.7546) acc_u 28.1250 (30.0000) lr 2.9289e-04 eta 0:00:17
epoch [152/200] batch [10/44] time 0.502 (0.452) data 0.371 (0.321) loss_u loss_u 0.8413 (0.7507) acc_u 18.7500 (30.6250) lr 2.9289e-04 eta 0:00:15
epoch [152/200] batch [15/44] time 0.385 (0.450) data 0.253 (0.318) loss_u loss_u 0.7446 (0.7617) acc_u 31.2500 (29.1667) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [20/44] time 0.554 (0.452) data 0.423 (0.320) loss_u loss_u 0.7622 (0.7553) acc_u 34.3750 (30.4688) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [25/44] time 0.454 (0.451) data 0.322 (0.319) loss_u loss_u 0.7422 (0.7637) acc_u 40.6250 (29.3750) lr 2.9289e-04 eta 0:00:08
epoch [152/200] batch [30/44] time 0.510 (0.450) data 0.379 (0.319) loss_u loss_u 0.7720 (0.7594) acc_u 25.0000 (29.7917) lr 2.9289e-04 eta 0:00:06
epoch [152/200] batch [35/44] time 0.457 (0.451) data 0.326 (0.319) loss_u loss_u 0.8252 (0.7600) acc_u 28.1250 (29.9107) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [40/44] time 0.391 (0.450) data 0.259 (0.318) loss_u loss_u 0.7373 (0.7620) acc_u 37.5000 (30.0781) lr 2.9289e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1261
confident_label rate tensor(0.5376, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1686
clean true:1682
clean false:4
clean_rate:0.9976275207591934
noisy true:193
noisy false:1257
after delete: len(clean_dataset) 1686
after delete: len(noisy_dataset) 1450
epoch [153/200] batch [5/52] time 0.429 (0.439) data 0.298 (0.309) loss_x loss_x 1.4834 (0.9587) acc_x 62.5000 (76.2500) lr 2.8187e-04 eta 0:00:20
epoch [153/200] batch [10/52] time 0.415 (0.446) data 0.285 (0.316) loss_x loss_x 1.5488 (1.0131) acc_x 65.6250 (74.0625) lr 2.8187e-04 eta 0:00:18
epoch [153/200] batch [15/52] time 0.548 (0.457) data 0.418 (0.326) loss_x loss_x 1.1094 (1.0108) acc_x 71.8750 (73.3333) lr 2.8187e-04 eta 0:00:16
epoch [153/200] batch [20/52] time 0.417 (0.454) data 0.286 (0.323) loss_x loss_x 1.1143 (0.9933) acc_x 71.8750 (73.9062) lr 2.8187e-04 eta 0:00:14
epoch [153/200] batch [25/52] time 0.440 (0.446) data 0.309 (0.315) loss_x loss_x 0.8975 (1.0268) acc_x 84.3750 (73.8750) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [30/52] time 0.391 (0.447) data 0.260 (0.316) loss_x loss_x 1.1582 (1.0747) acc_x 65.6250 (72.5000) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [35/52] time 0.432 (0.440) data 0.301 (0.310) loss_x loss_x 1.4834 (1.0616) acc_x 62.5000 (72.8571) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [40/52] time 0.715 (0.453) data 0.584 (0.323) loss_x loss_x 0.7788 (1.0500) acc_x 81.2500 (73.3594) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [45/52] time 0.426 (0.451) data 0.295 (0.320) loss_x loss_x 1.4658 (1.0635) acc_x 71.8750 (73.3333) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [50/52] time 0.391 (0.448) data 0.260 (0.317) loss_x loss_x 0.7935 (1.0559) acc_x 81.2500 (73.8125) lr 2.8187e-04 eta 0:00:00
epoch [153/200] batch [5/45] time 0.513 (0.445) data 0.381 (0.314) loss_u loss_u 0.7822 (0.7537) acc_u 28.1250 (32.5000) lr 2.8187e-04 eta 0:00:17
epoch [153/200] batch [10/45] time 0.428 (0.441) data 0.298 (0.310) loss_u loss_u 0.8135 (0.7812) acc_u 28.1250 (29.0625) lr 2.8187e-04 eta 0:00:15
epoch [153/200] batch [15/45] time 0.441 (0.441) data 0.309 (0.310) loss_u loss_u 0.7437 (0.7700) acc_u 34.3750 (29.7917) lr 2.8187e-04 eta 0:00:13
epoch [153/200] batch [20/45] time 0.464 (0.440) data 0.332 (0.309) loss_u loss_u 0.7671 (0.7728) acc_u 37.5000 (29.8438) lr 2.8187e-04 eta 0:00:11
epoch [153/200] batch [25/45] time 0.374 (0.438) data 0.243 (0.307) loss_u loss_u 0.8262 (0.7685) acc_u 25.0000 (30.2500) lr 2.8187e-04 eta 0:00:08
epoch [153/200] batch [30/45] time 0.479 (0.437) data 0.346 (0.306) loss_u loss_u 0.6982 (0.7650) acc_u 31.2500 (30.3125) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [35/45] time 0.549 (0.441) data 0.416 (0.309) loss_u loss_u 0.7393 (0.7613) acc_u 37.5000 (30.8929) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [40/45] time 0.367 (0.444) data 0.235 (0.312) loss_u loss_u 0.8071 (0.7601) acc_u 18.7500 (30.7812) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [45/45] time 0.544 (0.449) data 0.411 (0.318) loss_u loss_u 0.6357 (0.7554) acc_u 50.0000 (31.5972) lr 2.8187e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1259
confident_label rate tensor(0.5395, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1692
clean true:1690
clean false:2
clean_rate:0.9988179669030733
noisy true:187
noisy false:1257
after delete: len(clean_dataset) 1692
after delete: len(noisy_dataset) 1444
epoch [154/200] batch [5/52] time 0.438 (0.437) data 0.307 (0.306) loss_x loss_x 1.0312 (1.0286) acc_x 78.1250 (73.7500) lr 2.7103e-04 eta 0:00:20
epoch [154/200] batch [10/52] time 0.608 (0.457) data 0.477 (0.326) loss_x loss_x 1.1445 (1.0209) acc_x 71.8750 (74.6875) lr 2.7103e-04 eta 0:00:19
epoch [154/200] batch [15/52] time 0.483 (0.461) data 0.352 (0.330) loss_x loss_x 1.2217 (1.0741) acc_x 71.8750 (74.5833) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [20/52] time 0.488 (0.455) data 0.357 (0.324) loss_x loss_x 1.2705 (1.1522) acc_x 78.1250 (73.1250) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [25/52] time 0.409 (0.455) data 0.278 (0.323) loss_x loss_x 1.2012 (1.1925) acc_x 68.7500 (70.1250) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [30/52] time 0.551 (0.452) data 0.418 (0.321) loss_x loss_x 1.1289 (1.1873) acc_x 78.1250 (70.5208) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [35/52] time 0.451 (0.462) data 0.320 (0.331) loss_x loss_x 1.2832 (1.1787) acc_x 65.6250 (70.8036) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [40/52] time 0.557 (0.473) data 0.426 (0.342) loss_x loss_x 0.5039 (1.1643) acc_x 93.7500 (71.5625) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [45/52] time 0.393 (0.466) data 0.262 (0.334) loss_x loss_x 0.8730 (1.1565) acc_x 78.1250 (71.8056) lr 2.7103e-04 eta 0:00:03
epoch [154/200] batch [50/52] time 0.360 (0.459) data 0.229 (0.328) loss_x loss_x 1.3008 (1.1571) acc_x 71.8750 (72.1875) lr 2.7103e-04 eta 0:00:00
epoch [154/200] batch [5/45] time 0.379 (0.453) data 0.246 (0.322) loss_u loss_u 0.7627 (0.7928) acc_u 28.1250 (27.5000) lr 2.7103e-04 eta 0:00:18
epoch [154/200] batch [10/45] time 0.450 (0.455) data 0.317 (0.323) loss_u loss_u 0.8130 (0.7743) acc_u 18.7500 (29.0625) lr 2.7103e-04 eta 0:00:15
epoch [154/200] batch [15/45] time 0.458 (0.455) data 0.325 (0.324) loss_u loss_u 0.7812 (0.7707) acc_u 28.1250 (30.0000) lr 2.7103e-04 eta 0:00:13
epoch [154/200] batch [20/45] time 0.462 (0.456) data 0.330 (0.324) loss_u loss_u 0.7300 (0.7566) acc_u 34.3750 (31.8750) lr 2.7103e-04 eta 0:00:11
epoch [154/200] batch [25/45] time 0.357 (0.456) data 0.224 (0.324) loss_u loss_u 0.8115 (0.7670) acc_u 25.0000 (30.5000) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [30/45] time 0.429 (0.454) data 0.297 (0.323) loss_u loss_u 0.7812 (0.7686) acc_u 31.2500 (30.4167) lr 2.7103e-04 eta 0:00:06
epoch [154/200] batch [35/45] time 0.418 (0.452) data 0.286 (0.320) loss_u loss_u 0.7739 (0.7713) acc_u 28.1250 (30.0000) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [40/45] time 0.398 (0.449) data 0.267 (0.317) loss_u loss_u 0.7573 (0.7661) acc_u 28.1250 (30.0781) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [45/45] time 0.550 (0.449) data 0.413 (0.317) loss_u loss_u 0.6738 (0.7591) acc_u 37.5000 (30.7639) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1280
confident_label rate tensor(0.5383, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1688
clean true:1687
clean false:1
clean_rate:0.9994075829383886
noisy true:169
noisy false:1279
after delete: len(clean_dataset) 1688
after delete: len(noisy_dataset) 1448
epoch [155/200] batch [5/52] time 0.483 (0.495) data 0.352 (0.363) loss_x loss_x 1.6787 (1.0964) acc_x 53.1250 (71.2500) lr 2.6037e-04 eta 0:00:23
epoch [155/200] batch [10/52] time 0.397 (0.473) data 0.266 (0.342) loss_x loss_x 1.5391 (1.1276) acc_x 65.6250 (71.2500) lr 2.6037e-04 eta 0:00:19
epoch [155/200] batch [15/52] time 0.469 (0.475) data 0.338 (0.344) loss_x loss_x 1.7686 (1.1235) acc_x 62.5000 (71.8750) lr 2.6037e-04 eta 0:00:17
epoch [155/200] batch [20/52] time 0.444 (0.466) data 0.313 (0.335) loss_x loss_x 1.5557 (1.1223) acc_x 53.1250 (71.5625) lr 2.6037e-04 eta 0:00:14
epoch [155/200] batch [25/52] time 0.365 (0.460) data 0.234 (0.329) loss_x loss_x 1.0527 (1.1219) acc_x 75.0000 (71.0000) lr 2.6037e-04 eta 0:00:12
epoch [155/200] batch [30/52] time 0.376 (0.454) data 0.245 (0.323) loss_x loss_x 1.1445 (1.1139) acc_x 68.7500 (71.5625) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [35/52] time 0.407 (0.454) data 0.276 (0.323) loss_x loss_x 0.9531 (1.1016) acc_x 81.2500 (72.2321) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [40/52] time 0.426 (0.457) data 0.295 (0.326) loss_x loss_x 1.3105 (1.1021) acc_x 71.8750 (72.3438) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [45/52] time 0.492 (0.456) data 0.361 (0.325) loss_x loss_x 1.1602 (1.1250) acc_x 75.0000 (72.1528) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [50/52] time 0.449 (0.455) data 0.318 (0.324) loss_x loss_x 1.7920 (1.1615) acc_x 59.3750 (71.5000) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/45] time 0.356 (0.452) data 0.224 (0.321) loss_u loss_u 0.8062 (0.7885) acc_u 25.0000 (27.5000) lr 2.6037e-04 eta 0:00:18
epoch [155/200] batch [10/45] time 0.384 (0.452) data 0.252 (0.321) loss_u loss_u 0.6948 (0.7658) acc_u 40.6250 (29.6875) lr 2.6037e-04 eta 0:00:15
epoch [155/200] batch [15/45] time 0.380 (0.452) data 0.248 (0.321) loss_u loss_u 0.6938 (0.7549) acc_u 40.6250 (31.0417) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [20/45] time 0.516 (0.455) data 0.383 (0.323) loss_u loss_u 0.7671 (0.7594) acc_u 31.2500 (29.6875) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [25/45] time 0.543 (0.457) data 0.410 (0.325) loss_u loss_u 0.7129 (0.7578) acc_u 34.3750 (30.3750) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [30/45] time 0.759 (0.461) data 0.625 (0.329) loss_u loss_u 0.8164 (0.7576) acc_u 18.7500 (30.4167) lr 2.6037e-04 eta 0:00:06
epoch [155/200] batch [35/45] time 0.556 (0.461) data 0.424 (0.329) loss_u loss_u 0.7231 (0.7537) acc_u 37.5000 (30.8929) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [40/45] time 0.373 (0.462) data 0.240 (0.331) loss_u loss_u 0.7192 (0.7539) acc_u 40.6250 (30.9375) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [45/45] time 0.661 (0.464) data 0.529 (0.332) loss_u loss_u 0.6714 (0.7524) acc_u 37.5000 (31.1806) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1250
confident_label rate tensor(0.5434, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1704
clean true:1701
clean false:3
clean_rate:0.9982394366197183
noisy true:185
noisy false:1247
after delete: len(clean_dataset) 1704
after delete: len(noisy_dataset) 1432
epoch [156/200] batch [5/53] time 0.523 (0.500) data 0.392 (0.368) loss_x loss_x 1.5967 (1.3162) acc_x 65.6250 (68.7500) lr 2.4989e-04 eta 0:00:23
epoch [156/200] batch [10/53] time 0.403 (0.480) data 0.272 (0.349) loss_x loss_x 0.6802 (1.1956) acc_x 84.3750 (70.9375) lr 2.4989e-04 eta 0:00:20
epoch [156/200] batch [15/53] time 0.402 (0.462) data 0.271 (0.331) loss_x loss_x 1.3223 (1.1240) acc_x 59.3750 (72.7083) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [20/53] time 0.536 (0.460) data 0.405 (0.329) loss_x loss_x 0.4309 (1.0542) acc_x 90.6250 (74.6875) lr 2.4989e-04 eta 0:00:15
epoch [156/200] batch [25/53] time 0.362 (0.458) data 0.230 (0.327) loss_x loss_x 0.8179 (1.0055) acc_x 84.3750 (76.8750) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [30/53] time 0.408 (0.463) data 0.276 (0.332) loss_x loss_x 1.3379 (1.0575) acc_x 62.5000 (75.4167) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [35/53] time 0.510 (0.465) data 0.378 (0.334) loss_x loss_x 0.5557 (1.0592) acc_x 87.5000 (74.6429) lr 2.4989e-04 eta 0:00:08
epoch [156/200] batch [40/53] time 0.472 (0.470) data 0.341 (0.338) loss_x loss_x 1.0420 (1.0451) acc_x 81.2500 (74.7656) lr 2.4989e-04 eta 0:00:06
epoch [156/200] batch [45/53] time 0.537 (0.470) data 0.405 (0.338) loss_x loss_x 0.5610 (1.0285) acc_x 84.3750 (75.0000) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [50/53] time 0.419 (0.475) data 0.288 (0.344) loss_x loss_x 0.9565 (1.0310) acc_x 78.1250 (75.0000) lr 2.4989e-04 eta 0:00:01
epoch [156/200] batch [5/44] time 0.502 (0.471) data 0.371 (0.340) loss_u loss_u 0.6343 (0.7279) acc_u 46.8750 (32.5000) lr 2.4989e-04 eta 0:00:18
epoch [156/200] batch [10/44] time 0.420 (0.470) data 0.288 (0.338) loss_u loss_u 0.7280 (0.7595) acc_u 40.6250 (30.0000) lr 2.4989e-04 eta 0:00:15
epoch [156/200] batch [15/44] time 0.395 (0.471) data 0.263 (0.339) loss_u loss_u 0.7900 (0.7778) acc_u 25.0000 (27.0833) lr 2.4989e-04 eta 0:00:13
epoch [156/200] batch [20/44] time 0.363 (0.469) data 0.230 (0.337) loss_u loss_u 0.7378 (0.7767) acc_u 37.5000 (28.1250) lr 2.4989e-04 eta 0:00:11
epoch [156/200] batch [25/44] time 0.462 (0.467) data 0.329 (0.336) loss_u loss_u 0.7197 (0.7753) acc_u 37.5000 (28.0000) lr 2.4989e-04 eta 0:00:08
epoch [156/200] batch [30/44] time 0.456 (0.471) data 0.323 (0.339) loss_u loss_u 0.6875 (0.7762) acc_u 37.5000 (27.8125) lr 2.4989e-04 eta 0:00:06
epoch [156/200] batch [35/44] time 0.459 (0.468) data 0.327 (0.337) loss_u loss_u 0.7739 (0.7703) acc_u 28.1250 (28.5714) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [40/44] time 0.410 (0.465) data 0.278 (0.333) loss_u loss_u 0.7710 (0.7696) acc_u 34.3750 (28.9844) lr 2.4989e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1260
confident_label rate tensor(0.5399, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1693
clean true:1692
clean false:1
clean_rate:0.9994093325457767
noisy true:184
noisy false:1259
after delete: len(clean_dataset) 1693
after delete: len(noisy_dataset) 1443
epoch [157/200] batch [5/52] time 0.418 (0.450) data 0.286 (0.318) loss_x loss_x 0.9448 (1.1603) acc_x 75.0000 (71.2500) lr 2.3959e-04 eta 0:00:21
epoch [157/200] batch [10/52] time 0.423 (0.456) data 0.292 (0.324) loss_x loss_x 1.1875 (1.1305) acc_x 71.8750 (70.3125) lr 2.3959e-04 eta 0:00:19
epoch [157/200] batch [15/52] time 0.447 (0.469) data 0.316 (0.337) loss_x loss_x 1.5352 (1.1112) acc_x 59.3750 (71.0417) lr 2.3959e-04 eta 0:00:17
epoch [157/200] batch [20/52] time 0.394 (0.477) data 0.263 (0.346) loss_x loss_x 0.8867 (1.1271) acc_x 71.8750 (69.5312) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [25/52] time 0.548 (0.467) data 0.417 (0.335) loss_x loss_x 1.1406 (1.1615) acc_x 65.6250 (69.3750) lr 2.3959e-04 eta 0:00:12
epoch [157/200] batch [30/52] time 0.445 (0.464) data 0.314 (0.333) loss_x loss_x 1.0166 (1.1304) acc_x 78.1250 (70.0000) lr 2.3959e-04 eta 0:00:10
epoch [157/200] batch [35/52] time 0.550 (0.471) data 0.418 (0.340) loss_x loss_x 1.4219 (1.1386) acc_x 75.0000 (70.6250) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [40/52] time 0.467 (0.466) data 0.336 (0.335) loss_x loss_x 1.2012 (1.1421) acc_x 71.8750 (70.7812) lr 2.3959e-04 eta 0:00:05
epoch [157/200] batch [45/52] time 0.362 (0.459) data 0.231 (0.328) loss_x loss_x 0.7607 (1.1325) acc_x 87.5000 (71.1806) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [50/52] time 0.350 (0.456) data 0.218 (0.325) loss_x loss_x 1.1650 (1.1455) acc_x 71.8750 (71.0000) lr 2.3959e-04 eta 0:00:00
epoch [157/200] batch [5/45] time 0.436 (0.453) data 0.304 (0.322) loss_u loss_u 0.7896 (0.7569) acc_u 31.2500 (34.3750) lr 2.3959e-04 eta 0:00:18
epoch [157/200] batch [10/45] time 0.525 (0.457) data 0.393 (0.325) loss_u loss_u 0.7788 (0.7361) acc_u 28.1250 (35.3125) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [15/45] time 0.533 (0.458) data 0.402 (0.326) loss_u loss_u 0.8213 (0.7477) acc_u 25.0000 (33.3333) lr 2.3959e-04 eta 0:00:13
epoch [157/200] batch [20/45] time 0.447 (0.456) data 0.315 (0.325) loss_u loss_u 0.7949 (0.7500) acc_u 31.2500 (33.2812) lr 2.3959e-04 eta 0:00:11
epoch [157/200] batch [25/45] time 0.495 (0.453) data 0.363 (0.321) loss_u loss_u 0.7168 (0.7481) acc_u 37.5000 (32.5000) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [30/45] time 0.415 (0.451) data 0.283 (0.319) loss_u loss_u 0.8081 (0.7507) acc_u 25.0000 (32.0833) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [35/45] time 0.371 (0.448) data 0.239 (0.316) loss_u loss_u 0.7827 (0.7538) acc_u 28.1250 (31.7857) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [40/45] time 0.428 (0.446) data 0.296 (0.315) loss_u loss_u 0.7666 (0.7481) acc_u 31.2500 (32.5000) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [45/45] time 0.420 (0.449) data 0.288 (0.317) loss_u loss_u 0.7524 (0.7502) acc_u 34.3750 (32.3611) lr 2.3959e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1224
confident_label rate tensor(0.5529, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1734
clean true:1731
clean false:3
clean_rate:0.9982698961937716
noisy true:181
noisy false:1221
after delete: len(clean_dataset) 1734
after delete: len(noisy_dataset) 1402
epoch [158/200] batch [5/54] time 0.480 (0.435) data 0.350 (0.305) loss_x loss_x 0.8315 (1.1433) acc_x 75.0000 (74.3750) lr 2.2949e-04 eta 0:00:21
epoch [158/200] batch [10/54] time 0.436 (0.437) data 0.305 (0.307) loss_x loss_x 1.1689 (1.0646) acc_x 65.6250 (74.3750) lr 2.2949e-04 eta 0:00:19
epoch [158/200] batch [15/54] time 0.440 (0.440) data 0.309 (0.309) loss_x loss_x 1.0156 (1.1290) acc_x 68.7500 (72.2917) lr 2.2949e-04 eta 0:00:17
epoch [158/200] batch [20/54] time 0.353 (0.434) data 0.222 (0.304) loss_x loss_x 1.2988 (1.1161) acc_x 68.7500 (71.7188) lr 2.2949e-04 eta 0:00:14
epoch [158/200] batch [25/54] time 0.402 (0.443) data 0.271 (0.312) loss_x loss_x 1.0898 (1.1075) acc_x 71.8750 (71.6250) lr 2.2949e-04 eta 0:00:12
epoch [158/200] batch [30/54] time 0.455 (0.450) data 0.324 (0.319) loss_x loss_x 1.1162 (1.1237) acc_x 78.1250 (71.6667) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [35/54] time 0.469 (0.452) data 0.338 (0.321) loss_x loss_x 0.9858 (1.1201) acc_x 75.0000 (72.3214) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [40/54] time 0.420 (0.456) data 0.289 (0.325) loss_x loss_x 1.3857 (1.1238) acc_x 59.3750 (72.2656) lr 2.2949e-04 eta 0:00:06
epoch [158/200] batch [45/54] time 0.507 (0.461) data 0.376 (0.330) loss_x loss_x 1.4756 (1.1464) acc_x 59.3750 (71.7361) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [50/54] time 0.353 (0.458) data 0.222 (0.327) loss_x loss_x 1.0312 (1.1190) acc_x 75.0000 (72.2500) lr 2.2949e-04 eta 0:00:01
epoch [158/200] batch [5/43] time 0.388 (0.451) data 0.257 (0.320) loss_u loss_u 0.7363 (0.7661) acc_u 31.2500 (28.7500) lr 2.2949e-04 eta 0:00:17
epoch [158/200] batch [10/43] time 0.438 (0.454) data 0.307 (0.323) loss_u loss_u 0.8003 (0.7773) acc_u 25.0000 (27.5000) lr 2.2949e-04 eta 0:00:14
epoch [158/200] batch [15/43] time 0.501 (0.450) data 0.371 (0.319) loss_u loss_u 0.6279 (0.7555) acc_u 56.2500 (30.8333) lr 2.2949e-04 eta 0:00:12
epoch [158/200] batch [20/43] time 0.345 (0.450) data 0.215 (0.319) loss_u loss_u 0.7744 (0.7537) acc_u 25.0000 (30.3125) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [25/43] time 0.499 (0.453) data 0.368 (0.322) loss_u loss_u 0.8413 (0.7631) acc_u 18.7500 (28.8750) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [30/43] time 0.414 (0.453) data 0.283 (0.322) loss_u loss_u 0.7896 (0.7669) acc_u 34.3750 (28.8542) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [35/43] time 0.441 (0.452) data 0.310 (0.321) loss_u loss_u 0.7793 (0.7673) acc_u 34.3750 (29.1964) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [40/43] time 0.440 (0.449) data 0.308 (0.318) loss_u loss_u 0.7554 (0.7674) acc_u 34.3750 (29.3750) lr 2.2949e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1259
confident_label rate tensor(0.5424, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1701
clean true:1700
clean false:1
clean_rate:0.9994121105232217
noisy true:177
noisy false:1258
after delete: len(clean_dataset) 1701
after delete: len(noisy_dataset) 1435
epoch [159/200] batch [5/53] time 0.389 (0.436) data 0.258 (0.305) loss_x loss_x 1.3779 (1.1061) acc_x 71.8750 (73.1250) lr 2.1957e-04 eta 0:00:20
epoch [159/200] batch [10/53] time 0.569 (0.450) data 0.438 (0.319) loss_x loss_x 1.2334 (0.9900) acc_x 68.7500 (78.1250) lr 2.1957e-04 eta 0:00:19
epoch [159/200] batch [15/53] time 0.457 (0.455) data 0.326 (0.324) loss_x loss_x 0.9883 (0.9877) acc_x 71.8750 (77.2917) lr 2.1957e-04 eta 0:00:17
epoch [159/200] batch [20/53] time 0.464 (0.453) data 0.333 (0.322) loss_x loss_x 1.3857 (1.0349) acc_x 68.7500 (75.9375) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [25/53] time 0.399 (0.445) data 0.268 (0.314) loss_x loss_x 1.1572 (1.0883) acc_x 65.6250 (74.1250) lr 2.1957e-04 eta 0:00:12
epoch [159/200] batch [30/53] time 0.481 (0.445) data 0.350 (0.314) loss_x loss_x 1.0469 (1.1090) acc_x 71.8750 (73.4375) lr 2.1957e-04 eta 0:00:10
epoch [159/200] batch [35/53] time 0.395 (0.452) data 0.264 (0.321) loss_x loss_x 0.7217 (1.0942) acc_x 81.2500 (74.5536) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [40/53] time 0.431 (0.448) data 0.300 (0.317) loss_x loss_x 0.8647 (1.0772) acc_x 71.8750 (74.2188) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [45/53] time 0.372 (0.447) data 0.238 (0.316) loss_x loss_x 1.2715 (1.0784) acc_x 68.7500 (73.5417) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [50/53] time 0.403 (0.448) data 0.272 (0.316) loss_x loss_x 1.2998 (1.0942) acc_x 62.5000 (72.8125) lr 2.1957e-04 eta 0:00:01
epoch [159/200] batch [5/44] time 0.484 (0.448) data 0.353 (0.317) loss_u loss_u 0.7393 (0.7481) acc_u 43.7500 (33.7500) lr 2.1957e-04 eta 0:00:17
epoch [159/200] batch [10/44] time 0.473 (0.449) data 0.341 (0.317) loss_u loss_u 0.6196 (0.7348) acc_u 46.8750 (33.7500) lr 2.1957e-04 eta 0:00:15
epoch [159/200] batch [15/44] time 0.483 (0.448) data 0.352 (0.317) loss_u loss_u 0.6450 (0.7376) acc_u 53.1250 (33.5417) lr 2.1957e-04 eta 0:00:12
epoch [159/200] batch [20/44] time 0.320 (0.444) data 0.188 (0.313) loss_u loss_u 0.7358 (0.7439) acc_u 37.5000 (32.3438) lr 2.1957e-04 eta 0:00:10
epoch [159/200] batch [25/44] time 0.404 (0.450) data 0.272 (0.319) loss_u loss_u 0.8555 (0.7519) acc_u 21.8750 (31.5000) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [30/44] time 0.659 (0.450) data 0.527 (0.319) loss_u loss_u 0.7642 (0.7534) acc_u 25.0000 (31.2500) lr 2.1957e-04 eta 0:00:06
epoch [159/200] batch [35/44] time 0.360 (0.447) data 0.228 (0.316) loss_u loss_u 0.7925 (0.7548) acc_u 28.1250 (30.6250) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [40/44] time 0.454 (0.446) data 0.322 (0.314) loss_u loss_u 0.7495 (0.7590) acc_u 28.1250 (30.2344) lr 2.1957e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1259
confident_label rate tensor(0.5354, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1679
clean true:1679
clean false:0
clean_rate:1.0
noisy true:198
noisy false:1259
after delete: len(clean_dataset) 1679
after delete: len(noisy_dataset) 1457
epoch [160/200] batch [5/52] time 0.406 (0.432) data 0.276 (0.302) loss_x loss_x 1.3750 (1.3627) acc_x 65.6250 (62.5000) lr 2.0984e-04 eta 0:00:20
epoch [160/200] batch [10/52] time 0.445 (0.453) data 0.315 (0.322) loss_x loss_x 0.5786 (1.2122) acc_x 81.2500 (69.0625) lr 2.0984e-04 eta 0:00:19
epoch [160/200] batch [15/52] time 0.548 (0.481) data 0.418 (0.350) loss_x loss_x 0.7466 (1.0802) acc_x 75.0000 (72.5000) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [20/52] time 0.422 (0.473) data 0.291 (0.342) loss_x loss_x 1.0889 (1.0342) acc_x 75.0000 (73.5938) lr 2.0984e-04 eta 0:00:15
epoch [160/200] batch [25/52] time 0.456 (0.461) data 0.325 (0.331) loss_x loss_x 0.7573 (1.0280) acc_x 81.2500 (73.6250) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [30/52] time 0.480 (0.472) data 0.350 (0.341) loss_x loss_x 0.9551 (1.0442) acc_x 71.8750 (73.4375) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [35/52] time 0.381 (0.464) data 0.251 (0.334) loss_x loss_x 0.9019 (1.0389) acc_x 75.0000 (73.4821) lr 2.0984e-04 eta 0:00:07
epoch [160/200] batch [40/52] time 0.443 (0.462) data 0.313 (0.332) loss_x loss_x 1.2754 (1.0524) acc_x 68.7500 (73.3594) lr 2.0984e-04 eta 0:00:05
epoch [160/200] batch [45/52] time 0.506 (0.468) data 0.376 (0.337) loss_x loss_x 1.0059 (1.0595) acc_x 81.2500 (73.1250) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [50/52] time 0.391 (0.459) data 0.261 (0.329) loss_x loss_x 1.6572 (1.0720) acc_x 75.0000 (73.3125) lr 2.0984e-04 eta 0:00:00
epoch [160/200] batch [5/45] time 0.367 (0.450) data 0.236 (0.320) loss_u loss_u 0.8394 (0.7626) acc_u 21.8750 (31.2500) lr 2.0984e-04 eta 0:00:18
epoch [160/200] batch [10/45] time 0.423 (0.453) data 0.291 (0.322) loss_u loss_u 0.7324 (0.7513) acc_u 40.6250 (32.8125) lr 2.0984e-04 eta 0:00:15
epoch [160/200] batch [15/45] time 0.484 (0.455) data 0.353 (0.324) loss_u loss_u 0.6680 (0.7409) acc_u 46.8750 (33.5417) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [20/45] time 0.438 (0.456) data 0.307 (0.325) loss_u loss_u 0.7158 (0.7470) acc_u 40.6250 (32.6562) lr 2.0984e-04 eta 0:00:11
epoch [160/200] batch [25/45] time 0.452 (0.454) data 0.320 (0.323) loss_u loss_u 0.7456 (0.7488) acc_u 34.3750 (32.5000) lr 2.0984e-04 eta 0:00:09
epoch [160/200] batch [30/45] time 0.395 (0.452) data 0.263 (0.321) loss_u loss_u 0.7544 (0.7511) acc_u 31.2500 (32.7083) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [35/45] time 0.457 (0.451) data 0.326 (0.320) loss_u loss_u 0.7666 (0.7530) acc_u 31.2500 (32.4107) lr 2.0984e-04 eta 0:00:04
epoch [160/200] batch [40/45] time 0.437 (0.450) data 0.305 (0.319) loss_u loss_u 0.7798 (0.7571) acc_u 34.3750 (32.1875) lr 2.0984e-04 eta 0:00:02
epoch [160/200] batch [45/45] time 0.373 (0.445) data 0.241 (0.314) loss_u loss_u 0.8071 (0.7529) acc_u 28.1250 (32.5694) lr 2.0984e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1260
confident_label rate tensor(0.5411, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1697
clean true:1695
clean false:2
clean_rate:0.9988214496169712
noisy true:181
noisy false:1258
after delete: len(clean_dataset) 1697
after delete: len(noisy_dataset) 1439
epoch [161/200] batch [5/53] time 0.378 (0.406) data 0.247 (0.275) loss_x loss_x 0.7437 (1.0054) acc_x 78.1250 (76.2500) lr 2.0032e-04 eta 0:00:19
epoch [161/200] batch [10/53] time 0.495 (0.434) data 0.364 (0.303) loss_x loss_x 1.3174 (1.0171) acc_x 68.7500 (76.5625) lr 2.0032e-04 eta 0:00:18
epoch [161/200] batch [15/53] time 0.420 (0.450) data 0.290 (0.319) loss_x loss_x 0.9893 (1.0749) acc_x 78.1250 (74.1667) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [20/53] time 0.384 (0.455) data 0.253 (0.324) loss_x loss_x 1.4072 (1.1250) acc_x 59.3750 (72.3438) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [25/53] time 0.418 (0.452) data 0.287 (0.321) loss_x loss_x 0.9111 (1.0925) acc_x 81.2500 (73.1250) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [30/53] time 0.531 (0.452) data 0.401 (0.322) loss_x loss_x 1.3887 (1.0955) acc_x 65.6250 (72.6042) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [35/53] time 0.418 (0.457) data 0.287 (0.326) loss_x loss_x 1.1426 (1.0923) acc_x 68.7500 (72.9464) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [40/53] time 0.375 (0.454) data 0.244 (0.323) loss_x loss_x 1.1543 (1.0883) acc_x 59.3750 (72.7344) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [45/53] time 0.459 (0.447) data 0.328 (0.316) loss_x loss_x 0.9653 (1.0847) acc_x 75.0000 (72.7778) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [50/53] time 0.428 (0.451) data 0.297 (0.320) loss_x loss_x 0.7798 (1.0912) acc_x 78.1250 (72.8125) lr 2.0032e-04 eta 0:00:01
epoch [161/200] batch [5/44] time 0.497 (0.456) data 0.365 (0.325) loss_u loss_u 0.7173 (0.7525) acc_u 34.3750 (30.6250) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [10/44] time 0.448 (0.455) data 0.315 (0.324) loss_u loss_u 0.7534 (0.7464) acc_u 34.3750 (32.8125) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [15/44] time 0.398 (0.452) data 0.266 (0.321) loss_u loss_u 0.7358 (0.7418) acc_u 34.3750 (33.3333) lr 2.0032e-04 eta 0:00:13
epoch [161/200] batch [20/44] time 0.422 (0.451) data 0.291 (0.320) loss_u loss_u 0.7637 (0.7508) acc_u 28.1250 (32.8125) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [25/44] time 0.334 (0.445) data 0.202 (0.314) loss_u loss_u 0.7324 (0.7564) acc_u 28.1250 (31.6250) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [30/44] time 0.384 (0.444) data 0.252 (0.312) loss_u loss_u 0.7920 (0.7642) acc_u 28.1250 (30.6250) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [35/44] time 0.448 (0.442) data 0.316 (0.311) loss_u loss_u 0.7964 (0.7602) acc_u 21.8750 (30.7143) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [40/44] time 0.559 (0.441) data 0.427 (0.310) loss_u loss_u 0.6792 (0.7579) acc_u 40.6250 (31.1719) lr 2.0032e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1241
confident_label rate tensor(0.5459, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1712
clean true:1712
clean false:0
clean_rate:1.0
noisy true:183
noisy false:1241
after delete: len(clean_dataset) 1712
after delete: len(noisy_dataset) 1424
epoch [162/200] batch [5/53] time 0.417 (0.478) data 0.286 (0.347) loss_x loss_x 0.7603 (0.8881) acc_x 81.2500 (79.3750) lr 1.9098e-04 eta 0:00:22
epoch [162/200] batch [10/53] time 0.414 (0.430) data 0.283 (0.299) loss_x loss_x 0.5215 (0.9051) acc_x 84.3750 (76.8750) lr 1.9098e-04 eta 0:00:18
epoch [162/200] batch [15/53] time 0.547 (0.445) data 0.416 (0.314) loss_x loss_x 0.6377 (0.9164) acc_x 81.2500 (77.0833) lr 1.9098e-04 eta 0:00:16
epoch [162/200] batch [20/53] time 0.449 (0.448) data 0.319 (0.317) loss_x loss_x 1.6367 (1.0036) acc_x 59.3750 (74.5312) lr 1.9098e-04 eta 0:00:14
epoch [162/200] batch [25/53] time 0.447 (0.456) data 0.316 (0.325) loss_x loss_x 0.7681 (1.0173) acc_x 71.8750 (73.7500) lr 1.9098e-04 eta 0:00:12
epoch [162/200] batch [30/53] time 0.411 (0.459) data 0.280 (0.328) loss_x loss_x 1.7090 (1.0450) acc_x 65.6250 (73.4375) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [35/53] time 0.450 (0.459) data 0.319 (0.328) loss_x loss_x 1.0361 (1.0629) acc_x 75.0000 (73.1250) lr 1.9098e-04 eta 0:00:08
epoch [162/200] batch [40/53] time 0.340 (0.459) data 0.208 (0.328) loss_x loss_x 1.3613 (1.0717) acc_x 65.6250 (72.3438) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [45/53] time 0.467 (0.453) data 0.336 (0.323) loss_x loss_x 0.7568 (1.0579) acc_x 84.3750 (72.7083) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [50/53] time 0.424 (0.451) data 0.294 (0.320) loss_x loss_x 1.2070 (1.0728) acc_x 68.7500 (72.1250) lr 1.9098e-04 eta 0:00:01
epoch [162/200] batch [5/44] time 0.405 (0.453) data 0.273 (0.322) loss_u loss_u 0.7515 (0.7591) acc_u 31.2500 (32.5000) lr 1.9098e-04 eta 0:00:17
epoch [162/200] batch [10/44] time 0.383 (0.457) data 0.252 (0.326) loss_u loss_u 0.7158 (0.7397) acc_u 40.6250 (33.4375) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [15/44] time 0.395 (0.452) data 0.265 (0.321) loss_u loss_u 0.7617 (0.7614) acc_u 28.1250 (30.8333) lr 1.9098e-04 eta 0:00:13
epoch [162/200] batch [20/44] time 0.405 (0.450) data 0.272 (0.320) loss_u loss_u 0.7993 (0.7671) acc_u 25.0000 (29.3750) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [25/44] time 0.344 (0.451) data 0.213 (0.319) loss_u loss_u 0.7861 (0.7659) acc_u 34.3750 (30.1250) lr 1.9098e-04 eta 0:00:08
epoch [162/200] batch [30/44] time 0.398 (0.450) data 0.266 (0.318) loss_u loss_u 0.7539 (0.7746) acc_u 34.3750 (28.5417) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [35/44] time 0.343 (0.448) data 0.211 (0.317) loss_u loss_u 0.8228 (0.7712) acc_u 21.8750 (28.5714) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [40/44] time 0.365 (0.447) data 0.233 (0.316) loss_u loss_u 0.7197 (0.7649) acc_u 34.3750 (29.8438) lr 1.9098e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1221
confident_label rate tensor(0.5462, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1713
clean true:1712
clean false:1
clean_rate:0.9994162288382954
noisy true:203
noisy false:1220
after delete: len(clean_dataset) 1713
after delete: len(noisy_dataset) 1423
epoch [163/200] batch [5/53] time 0.465 (0.452) data 0.334 (0.321) loss_x loss_x 1.6211 (1.4076) acc_x 50.0000 (65.6250) lr 1.8185e-04 eta 0:00:21
epoch [163/200] batch [10/53] time 0.488 (0.454) data 0.357 (0.323) loss_x loss_x 0.8999 (1.1795) acc_x 75.0000 (71.2500) lr 1.8185e-04 eta 0:00:19
epoch [163/200] batch [15/53] time 0.494 (0.449) data 0.362 (0.318) loss_x loss_x 1.1543 (1.1441) acc_x 65.6250 (72.5000) lr 1.8185e-04 eta 0:00:17
epoch [163/200] batch [20/53] time 0.431 (0.456) data 0.300 (0.325) loss_x loss_x 1.0293 (1.1059) acc_x 75.0000 (73.4375) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [25/53] time 0.378 (0.458) data 0.248 (0.327) loss_x loss_x 1.0215 (1.1129) acc_x 71.8750 (73.2500) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [30/53] time 0.367 (0.449) data 0.236 (0.318) loss_x loss_x 1.3252 (1.1067) acc_x 78.1250 (73.7500) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [35/53] time 0.380 (0.453) data 0.249 (0.321) loss_x loss_x 0.8638 (1.0930) acc_x 75.0000 (74.0179) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [40/53] time 0.418 (0.458) data 0.287 (0.327) loss_x loss_x 1.2656 (1.0879) acc_x 65.6250 (73.8281) lr 1.8185e-04 eta 0:00:05
epoch [163/200] batch [45/53] time 0.389 (0.451) data 0.258 (0.320) loss_x loss_x 1.5283 (1.1076) acc_x 53.1250 (73.3333) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [50/53] time 0.396 (0.447) data 0.265 (0.316) loss_x loss_x 0.9673 (1.0933) acc_x 78.1250 (73.6875) lr 1.8185e-04 eta 0:00:01
epoch [163/200] batch [5/44] time 0.322 (0.445) data 0.190 (0.314) loss_u loss_u 0.5859 (0.7346) acc_u 56.2500 (34.3750) lr 1.8185e-04 eta 0:00:17
epoch [163/200] batch [10/44] time 0.444 (0.444) data 0.312 (0.313) loss_u loss_u 0.6685 (0.7397) acc_u 50.0000 (35.0000) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [15/44] time 0.367 (0.442) data 0.235 (0.311) loss_u loss_u 0.7109 (0.7528) acc_u 37.5000 (32.5000) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [20/44] time 0.408 (0.441) data 0.276 (0.310) loss_u loss_u 0.8340 (0.7575) acc_u 18.7500 (31.4062) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [25/44] time 0.488 (0.442) data 0.356 (0.311) loss_u loss_u 0.6978 (0.7561) acc_u 40.6250 (31.5000) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [30/44] time 0.354 (0.441) data 0.222 (0.310) loss_u loss_u 0.7280 (0.7583) acc_u 34.3750 (31.0417) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [35/44] time 0.418 (0.439) data 0.286 (0.308) loss_u loss_u 0.6733 (0.7604) acc_u 43.7500 (30.7143) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [40/44] time 0.472 (0.443) data 0.340 (0.311) loss_u loss_u 0.6904 (0.7576) acc_u 43.7500 (31.1719) lr 1.8185e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1293
confident_label rate tensor(0.5357, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1680
clean true:1677
clean false:3
clean_rate:0.9982142857142857
noisy true:166
noisy false:1290
after delete: len(clean_dataset) 1680
after delete: len(noisy_dataset) 1456
epoch [164/200] batch [5/52] time 0.544 (0.474) data 0.413 (0.343) loss_x loss_x 0.8286 (0.9159) acc_x 87.5000 (80.6250) lr 1.7292e-04 eta 0:00:22
epoch [164/200] batch [10/52] time 0.416 (0.465) data 0.285 (0.334) loss_x loss_x 1.3701 (1.0756) acc_x 65.6250 (75.9375) lr 1.7292e-04 eta 0:00:19
epoch [164/200] batch [15/52] time 0.417 (0.450) data 0.286 (0.319) loss_x loss_x 0.7690 (1.0144) acc_x 75.0000 (74.7917) lr 1.7292e-04 eta 0:00:16
epoch [164/200] batch [20/52] time 0.586 (0.464) data 0.455 (0.333) loss_x loss_x 0.9468 (1.0219) acc_x 78.1250 (74.8438) lr 1.7292e-04 eta 0:00:14
epoch [164/200] batch [25/52] time 0.443 (0.453) data 0.313 (0.322) loss_x loss_x 0.7896 (1.0267) acc_x 84.3750 (74.5000) lr 1.7292e-04 eta 0:00:12
epoch [164/200] batch [30/52] time 0.478 (0.446) data 0.347 (0.315) loss_x loss_x 0.6152 (1.0410) acc_x 84.3750 (74.2708) lr 1.7292e-04 eta 0:00:09
epoch [164/200] batch [35/52] time 0.453 (0.439) data 0.322 (0.308) loss_x loss_x 1.4609 (1.0667) acc_x 62.5000 (73.8393) lr 1.7292e-04 eta 0:00:07
epoch [164/200] batch [40/52] time 0.368 (0.443) data 0.238 (0.313) loss_x loss_x 1.1611 (1.0411) acc_x 68.7500 (74.1406) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [45/52] time 0.428 (0.447) data 0.297 (0.316) loss_x loss_x 0.7603 (1.0386) acc_x 75.0000 (73.4722) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [50/52] time 0.551 (0.448) data 0.420 (0.317) loss_x loss_x 1.5488 (1.0513) acc_x 65.6250 (73.1250) lr 1.7292e-04 eta 0:00:00
epoch [164/200] batch [5/45] time 0.563 (0.454) data 0.433 (0.323) loss_u loss_u 0.7090 (0.7762) acc_u 37.5000 (28.7500) lr 1.7292e-04 eta 0:00:18
epoch [164/200] batch [10/45] time 0.756 (0.454) data 0.625 (0.323) loss_u loss_u 0.8057 (0.7766) acc_u 18.7500 (28.1250) lr 1.7292e-04 eta 0:00:15
epoch [164/200] batch [15/45] time 0.426 (0.451) data 0.295 (0.320) loss_u loss_u 0.7271 (0.7722) acc_u 31.2500 (28.9583) lr 1.7292e-04 eta 0:00:13
epoch [164/200] batch [20/45] time 0.416 (0.449) data 0.285 (0.318) loss_u loss_u 0.7363 (0.7550) acc_u 40.6250 (32.5000) lr 1.7292e-04 eta 0:00:11
epoch [164/200] batch [25/45] time 0.373 (0.447) data 0.241 (0.316) loss_u loss_u 0.7065 (0.7538) acc_u 40.6250 (32.5000) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [30/45] time 0.361 (0.444) data 0.228 (0.313) loss_u loss_u 0.7354 (0.7555) acc_u 34.3750 (32.0833) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [35/45] time 0.393 (0.446) data 0.260 (0.315) loss_u loss_u 0.6792 (0.7512) acc_u 40.6250 (32.6786) lr 1.7292e-04 eta 0:00:04
epoch [164/200] batch [40/45] time 0.379 (0.444) data 0.248 (0.313) loss_u loss_u 0.7827 (0.7546) acc_u 25.0000 (32.1875) lr 1.7292e-04 eta 0:00:02
epoch [164/200] batch [45/45] time 0.474 (0.444) data 0.338 (0.312) loss_u loss_u 0.8013 (0.7561) acc_u 21.8750 (32.0833) lr 1.7292e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1225
confident_label rate tensor(0.5459, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1712
clean true:1711
clean false:1
clean_rate:0.9994158878504673
noisy true:200
noisy false:1224
after delete: len(clean_dataset) 1712
after delete: len(noisy_dataset) 1424
epoch [165/200] batch [5/53] time 0.426 (0.474) data 0.294 (0.342) loss_x loss_x 0.5693 (0.7477) acc_x 84.3750 (80.6250) lr 1.6419e-04 eta 0:00:22
epoch [165/200] batch [10/53] time 0.449 (0.457) data 0.318 (0.326) loss_x loss_x 0.5889 (0.8691) acc_x 84.3750 (77.1875) lr 1.6419e-04 eta 0:00:19
epoch [165/200] batch [15/53] time 0.698 (0.466) data 0.566 (0.333) loss_x loss_x 1.0791 (0.9396) acc_x 75.0000 (76.4583) lr 1.6419e-04 eta 0:00:17
epoch [165/200] batch [20/53] time 0.417 (0.464) data 0.286 (0.332) loss_x loss_x 0.7095 (0.9810) acc_x 78.1250 (74.5312) lr 1.6419e-04 eta 0:00:15
epoch [165/200] batch [25/53] time 0.469 (0.456) data 0.338 (0.325) loss_x loss_x 1.6104 (1.0255) acc_x 71.8750 (73.6250) lr 1.6419e-04 eta 0:00:12
epoch [165/200] batch [30/53] time 0.394 (0.459) data 0.263 (0.328) loss_x loss_x 0.9229 (1.0531) acc_x 75.0000 (73.0208) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [35/53] time 0.349 (0.459) data 0.216 (0.327) loss_x loss_x 1.3945 (1.0649) acc_x 65.6250 (73.0357) lr 1.6419e-04 eta 0:00:08
epoch [165/200] batch [40/53] time 0.389 (0.455) data 0.259 (0.323) loss_x loss_x 1.3232 (1.0916) acc_x 65.6250 (72.4219) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [45/53] time 0.525 (0.454) data 0.394 (0.323) loss_x loss_x 0.8306 (1.1048) acc_x 78.1250 (72.0833) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [50/53] time 0.379 (0.449) data 0.248 (0.318) loss_x loss_x 1.3047 (1.1021) acc_x 59.3750 (71.7500) lr 1.6419e-04 eta 0:00:01
epoch [165/200] batch [5/44] time 0.449 (0.446) data 0.317 (0.315) loss_u loss_u 0.7217 (0.7602) acc_u 37.5000 (27.5000) lr 1.6419e-04 eta 0:00:17
epoch [165/200] batch [10/44] time 0.429 (0.447) data 0.297 (0.315) loss_u loss_u 0.8062 (0.7896) acc_u 21.8750 (25.3125) lr 1.6419e-04 eta 0:00:15
epoch [165/200] batch [15/44] time 0.432 (0.449) data 0.300 (0.317) loss_u loss_u 0.7466 (0.7818) acc_u 34.3750 (26.6667) lr 1.6419e-04 eta 0:00:13
epoch [165/200] batch [20/44] time 0.425 (0.448) data 0.293 (0.316) loss_u loss_u 0.6758 (0.7659) acc_u 37.5000 (28.2812) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [25/44] time 0.392 (0.445) data 0.261 (0.313) loss_u loss_u 0.7344 (0.7617) acc_u 37.5000 (29.1250) lr 1.6419e-04 eta 0:00:08
epoch [165/200] batch [30/44] time 0.336 (0.444) data 0.205 (0.313) loss_u loss_u 0.6646 (0.7550) acc_u 43.7500 (30.3125) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [35/44] time 0.508 (0.444) data 0.376 (0.313) loss_u loss_u 0.6431 (0.7483) acc_u 46.8750 (31.4286) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [40/44] time 0.413 (0.451) data 0.283 (0.319) loss_u loss_u 0.6968 (0.7493) acc_u 37.5000 (31.6406) lr 1.6419e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1253
confident_label rate tensor(0.5427, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1702
clean true:1700
clean false:2
clean_rate:0.9988249118683902
noisy true:183
noisy false:1251
after delete: len(clean_dataset) 1702
after delete: len(noisy_dataset) 1434
epoch [166/200] batch [5/53] time 0.498 (0.461) data 0.368 (0.330) loss_x loss_x 0.5508 (1.0898) acc_x 87.5000 (68.7500) lr 1.5567e-04 eta 0:00:22
epoch [166/200] batch [10/53] time 0.476 (0.470) data 0.345 (0.340) loss_x loss_x 1.2383 (1.0706) acc_x 65.6250 (70.3125) lr 1.5567e-04 eta 0:00:20
epoch [166/200] batch [15/53] time 0.393 (0.486) data 0.262 (0.356) loss_x loss_x 1.6289 (1.1192) acc_x 71.8750 (70.8333) lr 1.5567e-04 eta 0:00:18
epoch [166/200] batch [20/53] time 0.449 (0.471) data 0.318 (0.340) loss_x loss_x 1.0371 (1.0809) acc_x 78.1250 (72.3438) lr 1.5567e-04 eta 0:00:15
epoch [166/200] batch [25/53] time 0.411 (0.459) data 0.280 (0.328) loss_x loss_x 1.5088 (1.0771) acc_x 68.7500 (72.6250) lr 1.5567e-04 eta 0:00:12
epoch [166/200] batch [30/53] time 0.591 (0.455) data 0.460 (0.324) loss_x loss_x 1.0986 (1.0502) acc_x 68.7500 (73.2292) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [35/53] time 0.494 (0.464) data 0.363 (0.333) loss_x loss_x 0.9434 (1.0696) acc_x 78.1250 (72.6786) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [40/53] time 0.493 (0.469) data 0.362 (0.338) loss_x loss_x 0.6753 (1.0660) acc_x 81.2500 (72.8906) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [45/53] time 0.382 (0.470) data 0.251 (0.339) loss_x loss_x 1.4746 (1.1172) acc_x 71.8750 (72.0139) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [50/53] time 0.547 (0.470) data 0.416 (0.339) loss_x loss_x 1.0898 (1.1190) acc_x 75.0000 (72.1875) lr 1.5567e-04 eta 0:00:01
epoch [166/200] batch [5/44] time 0.499 (0.463) data 0.367 (0.331) loss_u loss_u 0.7656 (0.7367) acc_u 37.5000 (35.6250) lr 1.5567e-04 eta 0:00:18
epoch [166/200] batch [10/44] time 0.386 (0.457) data 0.254 (0.326) loss_u loss_u 0.7476 (0.7458) acc_u 34.3750 (34.6875) lr 1.5567e-04 eta 0:00:15
epoch [166/200] batch [15/44] time 0.529 (0.460) data 0.396 (0.329) loss_u loss_u 0.7544 (0.7424) acc_u 34.3750 (33.9583) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [20/44] time 0.576 (0.460) data 0.444 (0.329) loss_u loss_u 0.8047 (0.7458) acc_u 21.8750 (33.1250) lr 1.5567e-04 eta 0:00:11
epoch [166/200] batch [25/44] time 0.428 (0.459) data 0.296 (0.327) loss_u loss_u 0.8120 (0.7430) acc_u 25.0000 (33.6250) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [30/44] time 0.460 (0.457) data 0.327 (0.325) loss_u loss_u 0.7314 (0.7445) acc_u 34.3750 (33.3333) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [35/44] time 0.372 (0.458) data 0.241 (0.326) loss_u loss_u 0.7759 (0.7451) acc_u 25.0000 (32.9464) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [40/44] time 0.538 (0.455) data 0.406 (0.324) loss_u loss_u 0.7559 (0.7406) acc_u 31.2500 (33.3594) lr 1.5567e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1263
confident_label rate tensor(0.5402, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1694
clean true:1692
clean false:2
clean_rate:0.9988193624557261
noisy true:181
noisy false:1261
after delete: len(clean_dataset) 1694
after delete: len(noisy_dataset) 1442
epoch [167/200] batch [5/52] time 0.415 (0.425) data 0.284 (0.294) loss_x loss_x 0.7437 (0.8942) acc_x 81.2500 (78.7500) lr 1.4736e-04 eta 0:00:19
epoch [167/200] batch [10/52] time 0.459 (0.439) data 0.329 (0.308) loss_x loss_x 0.7363 (1.0560) acc_x 81.2500 (75.0000) lr 1.4736e-04 eta 0:00:18
epoch [167/200] batch [15/52] time 0.408 (0.460) data 0.278 (0.329) loss_x loss_x 1.2227 (1.0944) acc_x 65.6250 (73.7500) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [20/52] time 0.395 (0.448) data 0.264 (0.318) loss_x loss_x 0.8848 (1.0830) acc_x 78.1250 (72.9688) lr 1.4736e-04 eta 0:00:14
epoch [167/200] batch [25/52] time 0.416 (0.440) data 0.285 (0.309) loss_x loss_x 0.9424 (1.0698) acc_x 75.0000 (73.6250) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [30/52] time 0.675 (0.447) data 0.544 (0.316) loss_x loss_x 1.3359 (1.0755) acc_x 68.7500 (73.8542) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [35/52] time 0.457 (0.448) data 0.326 (0.318) loss_x loss_x 1.2402 (1.1173) acc_x 62.5000 (73.1250) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [40/52] time 0.428 (0.446) data 0.298 (0.316) loss_x loss_x 1.2949 (1.1173) acc_x 68.7500 (72.8125) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [45/52] time 0.359 (0.450) data 0.228 (0.319) loss_x loss_x 1.3242 (1.1044) acc_x 75.0000 (73.2639) lr 1.4736e-04 eta 0:00:03
epoch [167/200] batch [50/52] time 0.472 (0.446) data 0.340 (0.315) loss_x loss_x 1.4453 (1.1225) acc_x 68.7500 (72.8750) lr 1.4736e-04 eta 0:00:00
epoch [167/200] batch [5/45] time 0.553 (0.448) data 0.421 (0.317) loss_u loss_u 0.8115 (0.7901) acc_u 18.7500 (25.0000) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [10/45] time 0.529 (0.448) data 0.398 (0.318) loss_u loss_u 0.7773 (0.7594) acc_u 37.5000 (30.6250) lr 1.4736e-04 eta 0:00:15
epoch [167/200] batch [15/45] time 0.346 (0.453) data 0.216 (0.322) loss_u loss_u 0.6826 (0.7548) acc_u 43.7500 (31.4583) lr 1.4736e-04 eta 0:00:13
epoch [167/200] batch [20/45] time 0.336 (0.448) data 0.204 (0.317) loss_u loss_u 0.7515 (0.7555) acc_u 37.5000 (31.2500) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [25/45] time 0.487 (0.447) data 0.355 (0.315) loss_u loss_u 0.8008 (0.7539) acc_u 28.1250 (31.6250) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [30/45] time 0.436 (0.447) data 0.303 (0.316) loss_u loss_u 0.7764 (0.7548) acc_u 31.2500 (31.2500) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [35/45] time 0.584 (0.449) data 0.445 (0.317) loss_u loss_u 0.7876 (0.7598) acc_u 25.0000 (30.8036) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [40/45] time 0.389 (0.450) data 0.259 (0.318) loss_u loss_u 0.7866 (0.7597) acc_u 25.0000 (31.1719) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [45/45] time 0.439 (0.450) data 0.308 (0.319) loss_u loss_u 0.7241 (0.7604) acc_u 37.5000 (30.9028) lr 1.4736e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1274
confident_label rate tensor(0.5360, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1681
clean true:1681
clean false:0
clean_rate:1.0
noisy true:181
noisy false:1274
after delete: len(clean_dataset) 1681
after delete: len(noisy_dataset) 1455
epoch [168/200] batch [5/52] time 0.371 (0.434) data 0.241 (0.303) loss_x loss_x 1.1895 (1.1128) acc_x 71.8750 (74.3750) lr 1.3926e-04 eta 0:00:20
epoch [168/200] batch [10/52] time 0.404 (0.443) data 0.273 (0.312) loss_x loss_x 1.0889 (1.0872) acc_x 78.1250 (73.7500) lr 1.3926e-04 eta 0:00:18
epoch [168/200] batch [15/52] time 0.459 (0.447) data 0.329 (0.317) loss_x loss_x 0.9990 (1.0789) acc_x 71.8750 (73.3333) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [20/52] time 0.364 (0.438) data 0.233 (0.307) loss_x loss_x 1.5684 (1.0770) acc_x 68.7500 (73.1250) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [25/52] time 0.379 (0.439) data 0.249 (0.309) loss_x loss_x 0.7891 (1.0535) acc_x 75.0000 (73.7500) lr 1.3926e-04 eta 0:00:11
epoch [168/200] batch [30/52] time 0.392 (0.450) data 0.261 (0.319) loss_x loss_x 1.0029 (1.0420) acc_x 81.2500 (74.2708) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [35/52] time 0.487 (0.446) data 0.356 (0.315) loss_x loss_x 0.9819 (1.0722) acc_x 78.1250 (73.5714) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [40/52] time 0.615 (0.461) data 0.484 (0.330) loss_x loss_x 0.8613 (1.0779) acc_x 71.8750 (73.1250) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [45/52] time 0.477 (0.464) data 0.345 (0.333) loss_x loss_x 1.2891 (1.0739) acc_x 71.8750 (73.2639) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [50/52] time 0.440 (0.470) data 0.309 (0.339) loss_x loss_x 1.0273 (1.0644) acc_x 71.8750 (73.6250) lr 1.3926e-04 eta 0:00:00
epoch [168/200] batch [5/45] time 0.471 (0.469) data 0.340 (0.338) loss_u loss_u 0.7827 (0.7562) acc_u 25.0000 (31.8750) lr 1.3926e-04 eta 0:00:18
epoch [168/200] batch [10/45] time 0.439 (0.464) data 0.307 (0.332) loss_u loss_u 0.7798 (0.7393) acc_u 31.2500 (34.0625) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [15/45] time 0.340 (0.458) data 0.210 (0.327) loss_u loss_u 0.6255 (0.7420) acc_u 46.8750 (32.9167) lr 1.3926e-04 eta 0:00:13
epoch [168/200] batch [20/45] time 0.397 (0.455) data 0.267 (0.324) loss_u loss_u 0.7466 (0.7502) acc_u 31.2500 (32.0312) lr 1.3926e-04 eta 0:00:11
epoch [168/200] batch [25/45] time 0.393 (0.455) data 0.260 (0.324) loss_u loss_u 0.7939 (0.7562) acc_u 25.0000 (31.2500) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [30/45] time 0.459 (0.456) data 0.326 (0.324) loss_u loss_u 0.7329 (0.7577) acc_u 43.7500 (31.4583) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [35/45] time 0.414 (0.455) data 0.281 (0.324) loss_u loss_u 0.7637 (0.7576) acc_u 34.3750 (31.6964) lr 1.3926e-04 eta 0:00:04
epoch [168/200] batch [40/45] time 0.452 (0.456) data 0.319 (0.325) loss_u loss_u 0.8091 (0.7590) acc_u 18.7500 (31.3281) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [45/45] time 0.410 (0.455) data 0.280 (0.323) loss_u loss_u 0.7271 (0.7569) acc_u 31.2500 (31.6667) lr 1.3926e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1232
confident_label rate tensor(0.5513, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1729
clean true:1727
clean false:2
clean_rate:0.9988432620011567
noisy true:177
noisy false:1230
after delete: len(clean_dataset) 1729
after delete: len(noisy_dataset) 1407
epoch [169/200] batch [5/54] time 0.519 (0.502) data 0.388 (0.371) loss_x loss_x 1.1689 (0.9878) acc_x 62.5000 (75.6250) lr 1.3137e-04 eta 0:00:24
epoch [169/200] batch [10/54] time 0.422 (0.481) data 0.292 (0.351) loss_x loss_x 0.9150 (0.9609) acc_x 78.1250 (76.8750) lr 1.3137e-04 eta 0:00:21
epoch [169/200] batch [15/54] time 0.462 (0.462) data 0.332 (0.331) loss_x loss_x 0.8804 (1.0036) acc_x 81.2500 (75.0000) lr 1.3137e-04 eta 0:00:18
epoch [169/200] batch [20/54] time 0.452 (0.463) data 0.322 (0.332) loss_x loss_x 1.0010 (0.9821) acc_x 75.0000 (75.1562) lr 1.3137e-04 eta 0:00:15
epoch [169/200] batch [25/54] time 0.358 (0.464) data 0.226 (0.334) loss_x loss_x 1.0527 (0.9975) acc_x 71.8750 (74.8750) lr 1.3137e-04 eta 0:00:13
epoch [169/200] batch [30/54] time 0.547 (0.468) data 0.417 (0.337) loss_x loss_x 0.9409 (1.0228) acc_x 78.1250 (74.7917) lr 1.3137e-04 eta 0:00:11
epoch [169/200] batch [35/54] time 0.412 (0.467) data 0.282 (0.337) loss_x loss_x 0.9624 (1.0369) acc_x 75.0000 (73.9286) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [40/54] time 0.490 (0.465) data 0.360 (0.335) loss_x loss_x 0.8511 (1.0105) acc_x 78.1250 (74.6875) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [45/54] time 0.605 (0.466) data 0.474 (0.336) loss_x loss_x 0.6826 (0.9948) acc_x 87.5000 (75.0694) lr 1.3137e-04 eta 0:00:04
epoch [169/200] batch [50/54] time 0.489 (0.466) data 0.357 (0.336) loss_x loss_x 0.4490 (0.9776) acc_x 90.6250 (75.6250) lr 1.3137e-04 eta 0:00:01
epoch [169/200] batch [5/43] time 0.388 (0.461) data 0.254 (0.330) loss_u loss_u 0.8599 (0.7823) acc_u 25.0000 (26.2500) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [10/43] time 0.556 (0.463) data 0.424 (0.332) loss_u loss_u 0.7612 (0.7649) acc_u 28.1250 (28.4375) lr 1.3137e-04 eta 0:00:15
epoch [169/200] batch [15/43] time 0.348 (0.460) data 0.216 (0.329) loss_u loss_u 0.7495 (0.7632) acc_u 28.1250 (29.3750) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [20/43] time 0.563 (0.457) data 0.430 (0.326) loss_u loss_u 0.6851 (0.7568) acc_u 37.5000 (29.8438) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [25/43] time 0.721 (0.459) data 0.590 (0.328) loss_u loss_u 0.8013 (0.7554) acc_u 21.8750 (30.2500) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [30/43] time 0.368 (0.455) data 0.237 (0.324) loss_u loss_u 0.7837 (0.7596) acc_u 31.2500 (29.8958) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [35/43] time 0.377 (0.453) data 0.245 (0.322) loss_u loss_u 0.8315 (0.7634) acc_u 25.0000 (29.0179) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [40/43] time 0.396 (0.449) data 0.264 (0.318) loss_u loss_u 0.7959 (0.7653) acc_u 25.0000 (28.9062) lr 1.3137e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1286
confident_label rate tensor(0.5360, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1681
clean true:1679
clean false:2
clean_rate:0.998810232004759
noisy true:171
noisy false:1284
after delete: len(clean_dataset) 1681
after delete: len(noisy_dataset) 1455
epoch [170/200] batch [5/52] time 0.430 (0.424) data 0.299 (0.293) loss_x loss_x 0.9111 (1.0604) acc_x 81.2500 (71.8750) lr 1.2369e-04 eta 0:00:19
epoch [170/200] batch [10/52] time 0.435 (0.428) data 0.303 (0.297) loss_x loss_x 1.2207 (1.1307) acc_x 68.7500 (70.0000) lr 1.2369e-04 eta 0:00:17
epoch [170/200] batch [15/52] time 0.384 (0.440) data 0.253 (0.308) loss_x loss_x 1.2812 (1.1260) acc_x 65.6250 (70.4167) lr 1.2369e-04 eta 0:00:16
epoch [170/200] batch [20/52] time 0.527 (0.448) data 0.396 (0.317) loss_x loss_x 1.2656 (1.1032) acc_x 71.8750 (71.4062) lr 1.2369e-04 eta 0:00:14
epoch [170/200] batch [25/52] time 0.492 (0.447) data 0.361 (0.316) loss_x loss_x 1.2197 (1.1162) acc_x 65.6250 (70.8750) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [30/52] time 0.433 (0.446) data 0.302 (0.315) loss_x loss_x 1.6855 (1.1038) acc_x 62.5000 (71.3542) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [35/52] time 0.386 (0.443) data 0.254 (0.312) loss_x loss_x 1.2266 (1.1087) acc_x 65.6250 (71.4286) lr 1.2369e-04 eta 0:00:07
epoch [170/200] batch [40/52] time 0.464 (0.446) data 0.333 (0.315) loss_x loss_x 1.5352 (1.1171) acc_x 65.6250 (71.1719) lr 1.2369e-04 eta 0:00:05
epoch [170/200] batch [45/52] time 0.491 (0.448) data 0.360 (0.317) loss_x loss_x 1.1523 (1.1193) acc_x 71.8750 (71.3889) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [50/52] time 0.558 (0.455) data 0.426 (0.323) loss_x loss_x 0.8652 (1.1070) acc_x 75.0000 (71.3750) lr 1.2369e-04 eta 0:00:00
epoch [170/200] batch [5/45] time 0.386 (0.452) data 0.254 (0.321) loss_u loss_u 0.8452 (0.7256) acc_u 18.7500 (33.1250) lr 1.2369e-04 eta 0:00:18
epoch [170/200] batch [10/45] time 0.413 (0.449) data 0.281 (0.317) loss_u loss_u 0.6733 (0.7376) acc_u 43.7500 (32.1875) lr 1.2369e-04 eta 0:00:15
epoch [170/200] batch [15/45] time 0.439 (0.449) data 0.306 (0.317) loss_u loss_u 0.7354 (0.7437) acc_u 34.3750 (32.0833) lr 1.2369e-04 eta 0:00:13
epoch [170/200] batch [20/45] time 0.404 (0.448) data 0.271 (0.317) loss_u loss_u 0.7114 (0.7544) acc_u 31.2500 (30.7812) lr 1.2369e-04 eta 0:00:11
epoch [170/200] batch [25/45] time 0.403 (0.445) data 0.271 (0.314) loss_u loss_u 0.7876 (0.7585) acc_u 28.1250 (30.1250) lr 1.2369e-04 eta 0:00:08
epoch [170/200] batch [30/45] time 0.451 (0.444) data 0.319 (0.312) loss_u loss_u 0.7114 (0.7562) acc_u 37.5000 (30.6250) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [35/45] time 0.621 (0.444) data 0.485 (0.313) loss_u loss_u 0.7715 (0.7625) acc_u 28.1250 (29.6429) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [40/45] time 0.403 (0.453) data 0.271 (0.321) loss_u loss_u 0.6743 (0.7609) acc_u 46.8750 (30.1562) lr 1.2369e-04 eta 0:00:02
epoch [170/200] batch [45/45] time 0.401 (0.452) data 0.269 (0.321) loss_u loss_u 0.6606 (0.7561) acc_u 40.6250 (30.6250) lr 1.2369e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1236
confident_label rate tensor(0.5469, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1715
clean true:1713
clean false:2
clean_rate:0.9988338192419826
noisy true:187
noisy false:1234
after delete: len(clean_dataset) 1715
after delete: len(noisy_dataset) 1421
epoch [171/200] batch [5/53] time 0.386 (0.465) data 0.254 (0.333) loss_x loss_x 1.2051 (1.0643) acc_x 71.8750 (75.6250) lr 1.1623e-04 eta 0:00:22
epoch [171/200] batch [10/53] time 0.563 (0.483) data 0.431 (0.351) loss_x loss_x 1.1807 (1.1721) acc_x 71.8750 (72.8125) lr 1.1623e-04 eta 0:00:20
epoch [171/200] batch [15/53] time 0.424 (0.474) data 0.293 (0.342) loss_x loss_x 1.2695 (1.1602) acc_x 62.5000 (72.2917) lr 1.1623e-04 eta 0:00:17
epoch [171/200] batch [20/53] time 0.536 (0.480) data 0.405 (0.349) loss_x loss_x 0.7368 (1.1235) acc_x 87.5000 (73.1250) lr 1.1623e-04 eta 0:00:15
epoch [171/200] batch [25/53] time 0.402 (0.465) data 0.270 (0.333) loss_x loss_x 0.9233 (1.1384) acc_x 84.3750 (72.7500) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [30/53] time 0.406 (0.458) data 0.275 (0.327) loss_x loss_x 0.9399 (1.1274) acc_x 78.1250 (73.1250) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [35/53] time 0.469 (0.455) data 0.337 (0.323) loss_x loss_x 0.8013 (1.1327) acc_x 75.0000 (72.6786) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [40/53] time 0.382 (0.454) data 0.250 (0.323) loss_x loss_x 1.0947 (1.1242) acc_x 71.8750 (72.8125) lr 1.1623e-04 eta 0:00:05
epoch [171/200] batch [45/53] time 0.562 (0.458) data 0.430 (0.326) loss_x loss_x 1.4385 (1.1308) acc_x 75.0000 (72.6389) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [50/53] time 0.374 (0.458) data 0.244 (0.326) loss_x loss_x 0.8672 (1.1231) acc_x 81.2500 (73.0000) lr 1.1623e-04 eta 0:00:01
epoch [171/200] batch [5/44] time 0.377 (0.457) data 0.247 (0.325) loss_u loss_u 0.7178 (0.7441) acc_u 31.2500 (36.8750) lr 1.1623e-04 eta 0:00:17
epoch [171/200] batch [10/44] time 0.408 (0.450) data 0.276 (0.319) loss_u loss_u 0.6436 (0.7332) acc_u 43.7500 (35.6250) lr 1.1623e-04 eta 0:00:15
epoch [171/200] batch [15/44] time 0.697 (0.452) data 0.566 (0.321) loss_u loss_u 0.7446 (0.7480) acc_u 34.3750 (32.2917) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [20/44] time 0.625 (0.453) data 0.493 (0.322) loss_u loss_u 0.7393 (0.7505) acc_u 31.2500 (32.0312) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [25/44] time 0.418 (0.450) data 0.287 (0.319) loss_u loss_u 0.6948 (0.7481) acc_u 31.2500 (32.0000) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [30/44] time 0.430 (0.447) data 0.298 (0.315) loss_u loss_u 0.7031 (0.7479) acc_u 40.6250 (32.0833) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [35/44] time 0.454 (0.446) data 0.322 (0.314) loss_u loss_u 0.7451 (0.7504) acc_u 37.5000 (32.2321) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [40/44] time 0.428 (0.443) data 0.295 (0.311) loss_u loss_u 0.6733 (0.7463) acc_u 46.8750 (32.8125) lr 1.1623e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1309
confident_label rate tensor(0.5281, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1656
clean true:1653
clean false:3
clean_rate:0.9981884057971014
noisy true:174
noisy false:1306
after delete: len(clean_dataset) 1656
after delete: len(noisy_dataset) 1480
epoch [172/200] batch [5/51] time 0.561 (0.501) data 0.422 (0.367) loss_x loss_x 1.1152 (1.2877) acc_x 81.2500 (70.6250) lr 1.0899e-04 eta 0:00:23
epoch [172/200] batch [10/51] time 0.521 (0.485) data 0.390 (0.352) loss_x loss_x 0.8804 (1.1044) acc_x 68.7500 (72.8125) lr 1.0899e-04 eta 0:00:19
epoch [172/200] batch [15/51] time 0.661 (0.502) data 0.530 (0.370) loss_x loss_x 0.9243 (1.1075) acc_x 75.0000 (72.5000) lr 1.0899e-04 eta 0:00:18
epoch [172/200] batch [20/51] time 0.623 (0.493) data 0.493 (0.362) loss_x loss_x 1.2617 (1.0981) acc_x 71.8750 (73.2812) lr 1.0899e-04 eta 0:00:15
epoch [172/200] batch [25/51] time 0.485 (0.483) data 0.355 (0.351) loss_x loss_x 0.7041 (1.0674) acc_x 87.5000 (74.5000) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [30/51] time 0.462 (0.473) data 0.332 (0.341) loss_x loss_x 0.8281 (1.0462) acc_x 78.1250 (74.5833) lr 1.0899e-04 eta 0:00:09
epoch [172/200] batch [35/51] time 0.506 (0.471) data 0.376 (0.340) loss_x loss_x 0.8755 (1.0590) acc_x 84.3750 (75.0000) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [40/51] time 0.497 (0.476) data 0.366 (0.345) loss_x loss_x 0.9771 (1.0773) acc_x 71.8750 (74.0625) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [45/51] time 0.507 (0.485) data 0.377 (0.354) loss_x loss_x 1.0508 (1.0699) acc_x 75.0000 (74.0278) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [50/51] time 0.415 (0.483) data 0.285 (0.352) loss_x loss_x 1.7051 (1.0844) acc_x 62.5000 (73.8125) lr 1.0899e-04 eta 0:00:00
epoch [172/200] batch [5/46] time 0.329 (0.476) data 0.199 (0.345) loss_u loss_u 0.7192 (0.6900) acc_u 34.3750 (43.1250) lr 1.0899e-04 eta 0:00:19
epoch [172/200] batch [10/46] time 0.390 (0.472) data 0.260 (0.341) loss_u loss_u 0.6670 (0.7241) acc_u 37.5000 (36.5625) lr 1.0899e-04 eta 0:00:16
epoch [172/200] batch [15/46] time 0.565 (0.470) data 0.433 (0.339) loss_u loss_u 0.6670 (0.7351) acc_u 40.6250 (34.3750) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [20/46] time 0.398 (0.466) data 0.267 (0.335) loss_u loss_u 0.7100 (0.7384) acc_u 46.8750 (34.6875) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [25/46] time 0.364 (0.461) data 0.234 (0.330) loss_u loss_u 0.7012 (0.7454) acc_u 37.5000 (33.8750) lr 1.0899e-04 eta 0:00:09
epoch [172/200] batch [30/46] time 0.434 (0.457) data 0.302 (0.326) loss_u loss_u 0.7300 (0.7451) acc_u 40.6250 (33.8542) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [35/46] time 0.477 (0.455) data 0.346 (0.324) loss_u loss_u 0.6870 (0.7411) acc_u 37.5000 (34.1964) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [40/46] time 0.470 (0.455) data 0.339 (0.324) loss_u loss_u 0.7188 (0.7400) acc_u 37.5000 (34.1406) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [45/46] time 0.461 (0.453) data 0.331 (0.322) loss_u loss_u 0.7231 (0.7397) acc_u 37.5000 (33.9583) lr 1.0899e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1279
confident_label rate tensor(0.5316, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1667
clean true:1665
clean false:2
clean_rate:0.9988002399520096
noisy true:192
noisy false:1277
after delete: len(clean_dataset) 1667
after delete: len(noisy_dataset) 1469
epoch [173/200] batch [5/52] time 0.473 (0.431) data 0.342 (0.300) loss_x loss_x 1.1973 (1.1140) acc_x 71.8750 (75.0000) lr 1.0197e-04 eta 0:00:20
epoch [173/200] batch [10/52] time 0.353 (0.431) data 0.223 (0.300) loss_x loss_x 1.0596 (1.1035) acc_x 71.8750 (74.0625) lr 1.0197e-04 eta 0:00:18
epoch [173/200] batch [15/52] time 0.453 (0.436) data 0.323 (0.305) loss_x loss_x 0.6348 (1.0934) acc_x 81.2500 (73.3333) lr 1.0197e-04 eta 0:00:16
epoch [173/200] batch [20/52] time 0.390 (0.435) data 0.260 (0.305) loss_x loss_x 1.3301 (1.1339) acc_x 68.7500 (72.0312) lr 1.0197e-04 eta 0:00:13
epoch [173/200] batch [25/52] time 0.421 (0.438) data 0.289 (0.308) loss_x loss_x 1.1416 (1.1103) acc_x 78.1250 (73.2500) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [30/52] time 0.553 (0.449) data 0.422 (0.319) loss_x loss_x 0.9038 (1.1003) acc_x 75.0000 (72.9167) lr 1.0197e-04 eta 0:00:09
epoch [173/200] batch [35/52] time 0.386 (0.447) data 0.255 (0.317) loss_x loss_x 0.5571 (1.0652) acc_x 84.3750 (73.8393) lr 1.0197e-04 eta 0:00:07
epoch [173/200] batch [40/52] time 0.386 (0.439) data 0.255 (0.308) loss_x loss_x 0.8066 (1.0718) acc_x 78.1250 (73.6719) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [45/52] time 0.458 (0.441) data 0.327 (0.310) loss_x loss_x 1.1738 (1.0842) acc_x 71.8750 (73.1250) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [50/52] time 0.451 (0.436) data 0.320 (0.305) loss_x loss_x 1.2783 (1.0895) acc_x 71.8750 (72.7500) lr 1.0197e-04 eta 0:00:00
epoch [173/200] batch [5/45] time 0.373 (0.436) data 0.242 (0.306) loss_u loss_u 0.6572 (0.7072) acc_u 46.8750 (36.8750) lr 1.0197e-04 eta 0:00:17
epoch [173/200] batch [10/45] time 0.535 (0.437) data 0.404 (0.307) loss_u loss_u 0.7124 (0.7392) acc_u 40.6250 (33.7500) lr 1.0197e-04 eta 0:00:15
epoch [173/200] batch [15/45] time 0.390 (0.443) data 0.260 (0.313) loss_u loss_u 0.7827 (0.7336) acc_u 28.1250 (34.1667) lr 1.0197e-04 eta 0:00:13
epoch [173/200] batch [20/45] time 0.642 (0.442) data 0.510 (0.312) loss_u loss_u 0.8208 (0.7505) acc_u 31.2500 (32.9688) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [25/45] time 0.404 (0.442) data 0.273 (0.312) loss_u loss_u 0.7266 (0.7500) acc_u 37.5000 (33.0000) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [30/45] time 0.411 (0.442) data 0.279 (0.311) loss_u loss_u 0.8032 (0.7538) acc_u 25.0000 (32.5000) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [35/45] time 0.370 (0.440) data 0.239 (0.309) loss_u loss_u 0.8315 (0.7497) acc_u 31.2500 (32.9464) lr 1.0197e-04 eta 0:00:04
epoch [173/200] batch [40/45] time 0.530 (0.440) data 0.400 (0.309) loss_u loss_u 0.7578 (0.7505) acc_u 34.3750 (32.9688) lr 1.0197e-04 eta 0:00:02
epoch [173/200] batch [45/45] time 0.471 (0.439) data 0.340 (0.308) loss_u loss_u 0.6519 (0.7460) acc_u 37.5000 (33.2639) lr 1.0197e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1265
confident_label rate tensor(0.5376, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1686
clean true:1685
clean false:1
clean_rate:0.9994068801897983
noisy true:186
noisy false:1264
after delete: len(clean_dataset) 1686
after delete: len(noisy_dataset) 1450
epoch [174/200] batch [5/52] time 0.597 (0.526) data 0.466 (0.395) loss_x loss_x 0.8687 (0.9807) acc_x 75.0000 (71.8750) lr 9.5173e-05 eta 0:00:24
epoch [174/200] batch [10/52] time 0.487 (0.490) data 0.356 (0.359) loss_x loss_x 1.3066 (1.0919) acc_x 78.1250 (72.8125) lr 9.5173e-05 eta 0:00:20
epoch [174/200] batch [15/52] time 0.484 (0.478) data 0.352 (0.346) loss_x loss_x 0.9077 (1.0675) acc_x 84.3750 (72.7083) lr 9.5173e-05 eta 0:00:17
epoch [174/200] batch [20/52] time 0.462 (0.473) data 0.331 (0.342) loss_x loss_x 0.6704 (1.0473) acc_x 75.0000 (72.8125) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [25/52] time 0.403 (0.463) data 0.272 (0.332) loss_x loss_x 1.2939 (1.0760) acc_x 71.8750 (72.6250) lr 9.5173e-05 eta 0:00:12
epoch [174/200] batch [30/52] time 0.387 (0.456) data 0.257 (0.325) loss_x loss_x 1.3682 (1.0689) acc_x 71.8750 (72.6042) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [35/52] time 0.599 (0.457) data 0.468 (0.326) loss_x loss_x 0.9844 (1.0755) acc_x 78.1250 (72.2321) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [40/52] time 0.494 (0.461) data 0.363 (0.330) loss_x loss_x 1.2793 (1.0522) acc_x 68.7500 (72.8906) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [45/52] time 0.466 (0.464) data 0.335 (0.333) loss_x loss_x 0.9829 (1.0307) acc_x 75.0000 (73.3333) lr 9.5173e-05 eta 0:00:03
epoch [174/200] batch [50/52] time 0.355 (0.456) data 0.223 (0.325) loss_x loss_x 1.0098 (1.0374) acc_x 68.7500 (73.3750) lr 9.5173e-05 eta 0:00:00
epoch [174/200] batch [5/45] time 0.672 (0.453) data 0.540 (0.322) loss_u loss_u 0.7949 (0.7055) acc_u 18.7500 (34.3750) lr 9.5173e-05 eta 0:00:18
epoch [174/200] batch [10/45] time 0.336 (0.452) data 0.204 (0.321) loss_u loss_u 0.7090 (0.7096) acc_u 37.5000 (34.6875) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [15/45] time 0.374 (0.446) data 0.242 (0.315) loss_u loss_u 0.7495 (0.7326) acc_u 28.1250 (32.5000) lr 9.5173e-05 eta 0:00:13
epoch [174/200] batch [20/45] time 0.391 (0.443) data 0.259 (0.311) loss_u loss_u 0.7529 (0.7468) acc_u 34.3750 (31.5625) lr 9.5173e-05 eta 0:00:11
epoch [174/200] batch [25/45] time 0.420 (0.441) data 0.288 (0.309) loss_u loss_u 0.7183 (0.7406) acc_u 43.7500 (32.2500) lr 9.5173e-05 eta 0:00:08
epoch [174/200] batch [30/45] time 0.505 (0.443) data 0.372 (0.311) loss_u loss_u 0.7646 (0.7386) acc_u 31.2500 (33.0208) lr 9.5173e-05 eta 0:00:06
epoch [174/200] batch [35/45] time 0.465 (0.446) data 0.333 (0.314) loss_u loss_u 0.8076 (0.7398) acc_u 25.0000 (32.9464) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [40/45] time 0.411 (0.447) data 0.279 (0.315) loss_u loss_u 0.7173 (0.7386) acc_u 31.2500 (32.8906) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [45/45] time 0.441 (0.446) data 0.310 (0.315) loss_u loss_u 0.8193 (0.7413) acc_u 25.0000 (32.8472) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1219
confident_label rate tensor(0.5526, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1733
clean true:1733
clean false:0
clean_rate:1.0
noisy true:184
noisy false:1219
after delete: len(clean_dataset) 1733
after delete: len(noisy_dataset) 1403
epoch [175/200] batch [5/54] time 0.451 (0.432) data 0.320 (0.301) loss_x loss_x 1.4219 (1.1904) acc_x 62.5000 (70.6250) lr 8.8597e-05 eta 0:00:21
epoch [175/200] batch [10/54] time 0.354 (0.412) data 0.223 (0.280) loss_x loss_x 0.8218 (1.0391) acc_x 75.0000 (72.5000) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [15/54] time 0.487 (0.451) data 0.356 (0.320) loss_x loss_x 1.3652 (1.1418) acc_x 65.6250 (71.8750) lr 8.8597e-05 eta 0:00:17
epoch [175/200] batch [20/54] time 0.427 (0.445) data 0.295 (0.313) loss_x loss_x 1.0420 (1.1169) acc_x 78.1250 (72.1875) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [25/54] time 0.375 (0.438) data 0.242 (0.307) loss_x loss_x 0.9717 (1.0495) acc_x 62.5000 (73.3750) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [30/54] time 0.396 (0.440) data 0.265 (0.309) loss_x loss_x 1.2803 (1.0355) acc_x 71.8750 (73.8542) lr 8.8597e-05 eta 0:00:10
epoch [175/200] batch [35/54] time 0.446 (0.440) data 0.314 (0.309) loss_x loss_x 1.1182 (1.0469) acc_x 75.0000 (73.7500) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [40/54] time 0.417 (0.442) data 0.286 (0.311) loss_x loss_x 0.9341 (1.0328) acc_x 75.0000 (74.2969) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [45/54] time 0.511 (0.450) data 0.378 (0.319) loss_x loss_x 1.2021 (1.0186) acc_x 75.0000 (74.5833) lr 8.8597e-05 eta 0:00:04
epoch [175/200] batch [50/54] time 0.497 (0.451) data 0.366 (0.320) loss_x loss_x 1.4346 (1.0197) acc_x 65.6250 (74.3750) lr 8.8597e-05 eta 0:00:01
epoch [175/200] batch [5/43] time 0.496 (0.452) data 0.364 (0.321) loss_u loss_u 0.7944 (0.7936) acc_u 25.0000 (23.7500) lr 8.8597e-05 eta 0:00:17
epoch [175/200] batch [10/43] time 0.696 (0.455) data 0.564 (0.323) loss_u loss_u 0.8130 (0.7625) acc_u 15.6250 (29.0625) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [15/43] time 0.452 (0.451) data 0.321 (0.320) loss_u loss_u 0.6768 (0.7540) acc_u 46.8750 (30.0000) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [20/43] time 0.582 (0.450) data 0.450 (0.318) loss_u loss_u 0.7534 (0.7504) acc_u 34.3750 (31.0938) lr 8.8597e-05 eta 0:00:10
epoch [175/200] batch [25/43] time 0.491 (0.451) data 0.359 (0.320) loss_u loss_u 0.6270 (0.7394) acc_u 37.5000 (32.2500) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [30/43] time 0.439 (0.450) data 0.308 (0.319) loss_u loss_u 0.7593 (0.7427) acc_u 21.8750 (31.3542) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [35/43] time 0.357 (0.450) data 0.225 (0.318) loss_u loss_u 0.7637 (0.7443) acc_u 31.2500 (31.1607) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [40/43] time 0.423 (0.447) data 0.291 (0.316) loss_u loss_u 0.8076 (0.7484) acc_u 25.0000 (31.0938) lr 8.8597e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1244
confident_label rate tensor(0.5427, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1702
clean true:1699
clean false:3
clean_rate:0.9982373678025852
noisy true:193
noisy false:1241
after delete: len(clean_dataset) 1702
after delete: len(noisy_dataset) 1434
epoch [176/200] batch [5/53] time 0.450 (0.467) data 0.319 (0.335) loss_x loss_x 1.4014 (0.8913) acc_x 68.7500 (80.6250) lr 8.2245e-05 eta 0:00:22
epoch [176/200] batch [10/53] time 0.407 (0.437) data 0.276 (0.305) loss_x loss_x 0.9380 (1.0464) acc_x 75.0000 (74.6875) lr 8.2245e-05 eta 0:00:18
epoch [176/200] batch [15/53] time 0.408 (0.423) data 0.277 (0.291) loss_x loss_x 1.0947 (1.1312) acc_x 75.0000 (73.3333) lr 8.2245e-05 eta 0:00:16
epoch [176/200] batch [20/53] time 0.569 (0.437) data 0.438 (0.306) loss_x loss_x 0.9590 (1.1487) acc_x 75.0000 (73.1250) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [25/53] time 0.452 (0.441) data 0.322 (0.310) loss_x loss_x 0.7842 (1.1434) acc_x 81.2500 (73.8750) lr 8.2245e-05 eta 0:00:12
epoch [176/200] batch [30/53] time 0.503 (0.450) data 0.372 (0.319) loss_x loss_x 0.7202 (1.1035) acc_x 81.2500 (74.4792) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [35/53] time 0.407 (0.448) data 0.276 (0.316) loss_x loss_x 1.0137 (1.0953) acc_x 75.0000 (74.7321) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [40/53] time 0.453 (0.452) data 0.322 (0.321) loss_x loss_x 1.1162 (1.1067) acc_x 71.8750 (74.2969) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [45/53] time 0.443 (0.448) data 0.312 (0.317) loss_x loss_x 1.0352 (1.1049) acc_x 75.0000 (74.0278) lr 8.2245e-05 eta 0:00:03
epoch [176/200] batch [50/53] time 0.355 (0.444) data 0.224 (0.313) loss_x loss_x 1.1025 (1.1305) acc_x 68.7500 (73.0000) lr 8.2245e-05 eta 0:00:01
epoch [176/200] batch [5/44] time 0.410 (0.450) data 0.277 (0.319) loss_u loss_u 0.6958 (0.7478) acc_u 46.8750 (33.7500) lr 8.2245e-05 eta 0:00:17
epoch [176/200] batch [10/44] time 0.457 (0.451) data 0.326 (0.320) loss_u loss_u 0.6152 (0.7308) acc_u 53.1250 (35.3125) lr 8.2245e-05 eta 0:00:15
epoch [176/200] batch [15/44] time 0.501 (0.451) data 0.365 (0.319) loss_u loss_u 0.6543 (0.7210) acc_u 56.2500 (36.2500) lr 8.2245e-05 eta 0:00:13
epoch [176/200] batch [20/44] time 0.396 (0.452) data 0.264 (0.320) loss_u loss_u 0.7695 (0.7288) acc_u 34.3750 (35.4688) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [25/44] time 0.504 (0.451) data 0.372 (0.319) loss_u loss_u 0.7334 (0.7394) acc_u 31.2500 (34.1250) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [30/44] time 0.351 (0.450) data 0.219 (0.318) loss_u loss_u 0.6836 (0.7386) acc_u 37.5000 (34.0625) lr 8.2245e-05 eta 0:00:06
epoch [176/200] batch [35/44] time 0.449 (0.449) data 0.317 (0.318) loss_u loss_u 0.7593 (0.7370) acc_u 31.2500 (33.9286) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [40/44] time 0.324 (0.447) data 0.193 (0.315) loss_u loss_u 0.8574 (0.7486) acc_u 12.5000 (32.3438) lr 8.2245e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1263
confident_label rate tensor(0.5440, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1706
clean true:1704
clean false:2
clean_rate:0.9988276670574443
noisy true:169
noisy false:1261
after delete: len(clean_dataset) 1706
after delete: len(noisy_dataset) 1430
epoch [177/200] batch [5/53] time 0.483 (0.428) data 0.353 (0.298) loss_x loss_x 0.7852 (0.9287) acc_x 75.0000 (77.5000) lr 7.6120e-05 eta 0:00:20
epoch [177/200] batch [10/53] time 0.439 (0.423) data 0.308 (0.293) loss_x loss_x 0.9541 (0.9170) acc_x 75.0000 (77.5000) lr 7.6120e-05 eta 0:00:18
epoch [177/200] batch [15/53] time 0.559 (0.431) data 0.428 (0.300) loss_x loss_x 1.2705 (0.9706) acc_x 68.7500 (76.6667) lr 7.6120e-05 eta 0:00:16
epoch [177/200] batch [20/53] time 0.560 (0.441) data 0.429 (0.310) loss_x loss_x 0.6104 (0.9810) acc_x 84.3750 (76.0938) lr 7.6120e-05 eta 0:00:14
epoch [177/200] batch [25/53] time 0.404 (0.440) data 0.274 (0.309) loss_x loss_x 1.2246 (0.9953) acc_x 65.6250 (76.0000) lr 7.6120e-05 eta 0:00:12
epoch [177/200] batch [30/53] time 0.454 (0.436) data 0.323 (0.306) loss_x loss_x 1.1377 (1.0175) acc_x 68.7500 (75.2083) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [35/53] time 0.361 (0.433) data 0.230 (0.303) loss_x loss_x 0.8379 (1.0153) acc_x 81.2500 (75.2679) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [40/53] time 0.483 (0.444) data 0.352 (0.313) loss_x loss_x 1.0430 (1.0277) acc_x 75.0000 (74.8438) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [45/53] time 0.486 (0.443) data 0.355 (0.313) loss_x loss_x 1.0391 (1.0405) acc_x 81.2500 (74.5139) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [50/53] time 0.409 (0.444) data 0.278 (0.313) loss_x loss_x 0.7974 (1.0345) acc_x 78.1250 (74.7500) lr 7.6120e-05 eta 0:00:01
epoch [177/200] batch [5/44] time 0.363 (0.439) data 0.231 (0.308) loss_u loss_u 0.7637 (0.7698) acc_u 34.3750 (29.3750) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [10/44] time 0.367 (0.442) data 0.235 (0.311) loss_u loss_u 0.6890 (0.7554) acc_u 40.6250 (32.8125) lr 7.6120e-05 eta 0:00:15
epoch [177/200] batch [15/44] time 0.367 (0.435) data 0.235 (0.304) loss_u loss_u 0.7676 (0.7689) acc_u 34.3750 (30.2083) lr 7.6120e-05 eta 0:00:12
epoch [177/200] batch [20/44] time 0.382 (0.435) data 0.250 (0.304) loss_u loss_u 0.7949 (0.7776) acc_u 28.1250 (28.4375) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [25/44] time 0.404 (0.435) data 0.272 (0.304) loss_u loss_u 0.8228 (0.7720) acc_u 25.0000 (29.1250) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [30/44] time 0.326 (0.434) data 0.195 (0.302) loss_u loss_u 0.8335 (0.7725) acc_u 18.7500 (29.2708) lr 7.6120e-05 eta 0:00:06
epoch [177/200] batch [35/44] time 0.529 (0.435) data 0.397 (0.304) loss_u loss_u 0.7842 (0.7783) acc_u 28.1250 (28.3929) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [40/44] time 0.377 (0.436) data 0.246 (0.305) loss_u loss_u 0.8013 (0.7754) acc_u 25.0000 (28.9062) lr 7.6120e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1199
confident_label rate tensor(0.5571, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1747
clean true:1745
clean false:2
clean_rate:0.9988551803091014
noisy true:192
noisy false:1197
after delete: len(clean_dataset) 1747
after delete: len(noisy_dataset) 1389
epoch [178/200] batch [5/54] time 0.494 (0.441) data 0.363 (0.309) loss_x loss_x 1.0576 (1.1482) acc_x 75.0000 (71.2500) lr 7.0224e-05 eta 0:00:21
epoch [178/200] batch [10/54] time 0.521 (0.464) data 0.390 (0.333) loss_x loss_x 1.0840 (1.0990) acc_x 75.0000 (71.2500) lr 7.0224e-05 eta 0:00:20
epoch [178/200] batch [15/54] time 0.518 (0.495) data 0.386 (0.363) loss_x loss_x 0.9956 (1.0715) acc_x 78.1250 (71.8750) lr 7.0224e-05 eta 0:00:19
epoch [178/200] batch [20/54] time 0.400 (0.487) data 0.268 (0.355) loss_x loss_x 1.4844 (1.1105) acc_x 68.7500 (71.5625) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [25/54] time 0.447 (0.486) data 0.315 (0.354) loss_x loss_x 1.6709 (1.1319) acc_x 59.3750 (71.2500) lr 7.0224e-05 eta 0:00:14
epoch [178/200] batch [30/54] time 0.425 (0.484) data 0.294 (0.352) loss_x loss_x 0.5400 (1.1022) acc_x 78.1250 (71.1458) lr 7.0224e-05 eta 0:00:11
epoch [178/200] batch [35/54] time 0.322 (0.474) data 0.191 (0.343) loss_x loss_x 1.5703 (1.1225) acc_x 68.7500 (70.6250) lr 7.0224e-05 eta 0:00:09
epoch [178/200] batch [40/54] time 0.437 (0.478) data 0.306 (0.347) loss_x loss_x 0.7261 (1.1198) acc_x 81.2500 (70.4688) lr 7.0224e-05 eta 0:00:06
epoch [178/200] batch [45/54] time 0.562 (0.475) data 0.430 (0.343) loss_x loss_x 1.1533 (1.1085) acc_x 68.7500 (70.4167) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [50/54] time 0.310 (0.467) data 0.178 (0.335) loss_x loss_x 0.9009 (1.0959) acc_x 81.2500 (70.7500) lr 7.0224e-05 eta 0:00:01
epoch [178/200] batch [5/43] time 0.365 (0.461) data 0.234 (0.329) loss_u loss_u 0.7646 (0.7366) acc_u 31.2500 (36.8750) lr 7.0224e-05 eta 0:00:17
epoch [178/200] batch [10/43] time 0.420 (0.457) data 0.287 (0.326) loss_u loss_u 0.8535 (0.7678) acc_u 15.6250 (30.3125) lr 7.0224e-05 eta 0:00:15
epoch [178/200] batch [15/43] time 0.366 (0.453) data 0.234 (0.321) loss_u loss_u 0.7793 (0.7680) acc_u 25.0000 (29.5833) lr 7.0224e-05 eta 0:00:12
epoch [178/200] batch [20/43] time 0.358 (0.453) data 0.226 (0.322) loss_u loss_u 0.7900 (0.7665) acc_u 25.0000 (29.3750) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [25/43] time 0.349 (0.451) data 0.218 (0.319) loss_u loss_u 0.7920 (0.7669) acc_u 31.2500 (29.3750) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [30/43] time 0.805 (0.452) data 0.671 (0.321) loss_u loss_u 0.6895 (0.7601) acc_u 40.6250 (30.2083) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [35/43] time 0.441 (0.455) data 0.309 (0.323) loss_u loss_u 0.8423 (0.7652) acc_u 18.7500 (29.3750) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [40/43] time 0.405 (0.454) data 0.273 (0.322) loss_u loss_u 0.7104 (0.7674) acc_u 40.6250 (29.0625) lr 7.0224e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1245
confident_label rate tensor(0.5475, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1717
clean true:1716
clean false:1
clean_rate:0.9994175888177053
noisy true:175
noisy false:1244
after delete: len(clean_dataset) 1717
after delete: len(noisy_dataset) 1419
epoch [179/200] batch [5/53] time 0.360 (0.479) data 0.229 (0.348) loss_x loss_x 1.2070 (0.9647) acc_x 71.8750 (74.3750) lr 6.4556e-05 eta 0:00:22
epoch [179/200] batch [10/53] time 0.373 (0.445) data 0.242 (0.314) loss_x loss_x 0.8325 (0.9254) acc_x 75.0000 (75.0000) lr 6.4556e-05 eta 0:00:19
epoch [179/200] batch [15/53] time 0.400 (0.445) data 0.269 (0.315) loss_x loss_x 1.2988 (0.9435) acc_x 71.8750 (73.9583) lr 6.4556e-05 eta 0:00:16
epoch [179/200] batch [20/53] time 0.440 (0.451) data 0.309 (0.320) loss_x loss_x 0.9517 (0.9983) acc_x 68.7500 (72.5000) lr 6.4556e-05 eta 0:00:14
epoch [179/200] batch [25/53] time 0.483 (0.449) data 0.352 (0.319) loss_x loss_x 0.9741 (0.9878) acc_x 75.0000 (73.6250) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [30/53] time 0.465 (0.449) data 0.334 (0.318) loss_x loss_x 1.0039 (0.9899) acc_x 78.1250 (74.0625) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [35/53] time 0.478 (0.451) data 0.347 (0.320) loss_x loss_x 0.9644 (1.0040) acc_x 68.7500 (74.0179) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [40/53] time 0.392 (0.451) data 0.262 (0.320) loss_x loss_x 0.9697 (1.0257) acc_x 78.1250 (73.9062) lr 6.4556e-05 eta 0:00:05
epoch [179/200] batch [45/53] time 0.419 (0.447) data 0.289 (0.316) loss_x loss_x 0.9229 (1.0331) acc_x 78.1250 (74.1667) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [50/53] time 0.565 (0.445) data 0.435 (0.314) loss_x loss_x 0.8906 (1.0285) acc_x 75.0000 (74.0625) lr 6.4556e-05 eta 0:00:01
epoch [179/200] batch [5/44] time 0.461 (0.439) data 0.330 (0.308) loss_u loss_u 0.8345 (0.7764) acc_u 18.7500 (26.2500) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [10/44] time 0.321 (0.437) data 0.189 (0.306) loss_u loss_u 0.8091 (0.7807) acc_u 21.8750 (25.9375) lr 6.4556e-05 eta 0:00:14
epoch [179/200] batch [15/44] time 0.455 (0.438) data 0.324 (0.307) loss_u loss_u 0.6611 (0.7674) acc_u 40.6250 (27.5000) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [20/44] time 0.381 (0.435) data 0.249 (0.304) loss_u loss_u 0.7231 (0.7710) acc_u 28.1250 (26.7188) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [25/44] time 0.477 (0.433) data 0.345 (0.302) loss_u loss_u 0.7290 (0.7666) acc_u 34.3750 (27.1250) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [30/44] time 0.632 (0.435) data 0.501 (0.304) loss_u loss_u 0.8076 (0.7651) acc_u 21.8750 (27.9167) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [35/44] time 0.460 (0.436) data 0.328 (0.305) loss_u loss_u 0.7324 (0.7644) acc_u 37.5000 (28.5714) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [40/44] time 0.441 (0.439) data 0.310 (0.308) loss_u loss_u 0.7544 (0.7635) acc_u 40.6250 (29.1406) lr 6.4556e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1237
confident_label rate tensor(0.5456, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1711
clean true:1709
clean false:2
clean_rate:0.9988310929281122
noisy true:190
noisy false:1235
after delete: len(clean_dataset) 1711
after delete: len(noisy_dataset) 1425
epoch [180/200] batch [5/53] time 0.400 (0.423) data 0.269 (0.292) loss_x loss_x 1.2012 (0.8708) acc_x 75.0000 (78.7500) lr 5.9119e-05 eta 0:00:20
epoch [180/200] batch [10/53] time 0.301 (0.431) data 0.170 (0.299) loss_x loss_x 0.9150 (0.9788) acc_x 71.8750 (76.5625) lr 5.9119e-05 eta 0:00:18
epoch [180/200] batch [15/53] time 0.520 (0.448) data 0.389 (0.317) loss_x loss_x 0.9087 (1.0537) acc_x 68.7500 (73.7500) lr 5.9119e-05 eta 0:00:17
epoch [180/200] batch [20/53] time 0.563 (0.455) data 0.432 (0.324) loss_x loss_x 0.5903 (1.0194) acc_x 87.5000 (74.0625) lr 5.9119e-05 eta 0:00:15
epoch [180/200] batch [25/53] time 0.400 (0.454) data 0.269 (0.323) loss_x loss_x 0.8818 (1.0301) acc_x 78.1250 (73.5000) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [30/53] time 0.413 (0.454) data 0.282 (0.323) loss_x loss_x 0.9976 (1.0163) acc_x 78.1250 (74.0625) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [35/53] time 0.476 (0.451) data 0.345 (0.320) loss_x loss_x 1.0664 (1.0027) acc_x 75.0000 (74.8214) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [40/53] time 0.554 (0.452) data 0.423 (0.321) loss_x loss_x 1.2949 (1.0485) acc_x 75.0000 (73.7500) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [45/53] time 0.465 (0.449) data 0.334 (0.318) loss_x loss_x 0.9634 (1.0417) acc_x 81.2500 (73.8889) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [50/53] time 0.409 (0.447) data 0.278 (0.316) loss_x loss_x 1.0996 (1.0562) acc_x 71.8750 (73.6875) lr 5.9119e-05 eta 0:00:01
epoch [180/200] batch [5/44] time 0.339 (0.440) data 0.208 (0.309) loss_u loss_u 0.7144 (0.7730) acc_u 37.5000 (28.7500) lr 5.9119e-05 eta 0:00:17
epoch [180/200] batch [10/44] time 0.429 (0.441) data 0.298 (0.310) loss_u loss_u 0.7500 (0.7617) acc_u 28.1250 (30.6250) lr 5.9119e-05 eta 0:00:14
epoch [180/200] batch [15/44] time 0.470 (0.438) data 0.338 (0.307) loss_u loss_u 0.7808 (0.7685) acc_u 31.2500 (29.5833) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [20/44] time 0.404 (0.437) data 0.272 (0.306) loss_u loss_u 0.7119 (0.7637) acc_u 40.6250 (30.4688) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [25/44] time 0.391 (0.436) data 0.259 (0.305) loss_u loss_u 0.8550 (0.7629) acc_u 15.6250 (30.2500) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [30/44] time 0.385 (0.436) data 0.253 (0.305) loss_u loss_u 0.7314 (0.7565) acc_u 28.1250 (31.0417) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [35/44] time 0.416 (0.438) data 0.286 (0.307) loss_u loss_u 0.6592 (0.7537) acc_u 43.7500 (31.6071) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [40/44] time 0.474 (0.438) data 0.342 (0.307) loss_u loss_u 0.7012 (0.7503) acc_u 40.6250 (32.1094) lr 5.9119e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1289
confident_label rate tensor(0.5354, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1679
clean true:1677
clean false:2
clean_rate:0.9988088147706968
noisy true:170
noisy false:1287
after delete: len(clean_dataset) 1679
after delete: len(noisy_dataset) 1457
epoch [181/200] batch [5/52] time 0.477 (0.444) data 0.346 (0.313) loss_x loss_x 1.0645 (1.1265) acc_x 68.7500 (70.6250) lr 5.3915e-05 eta 0:00:20
epoch [181/200] batch [10/52] time 0.455 (0.451) data 0.324 (0.320) loss_x loss_x 0.7603 (1.0186) acc_x 75.0000 (72.8125) lr 5.3915e-05 eta 0:00:18
epoch [181/200] batch [15/52] time 0.472 (0.461) data 0.341 (0.330) loss_x loss_x 1.1973 (1.0424) acc_x 65.6250 (73.3333) lr 5.3915e-05 eta 0:00:17
epoch [181/200] batch [20/52] time 0.415 (0.464) data 0.284 (0.333) loss_x loss_x 0.7007 (1.0425) acc_x 84.3750 (74.2188) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [25/52] time 0.482 (0.454) data 0.351 (0.323) loss_x loss_x 1.0479 (1.0633) acc_x 68.7500 (73.5000) lr 5.3915e-05 eta 0:00:12
epoch [181/200] batch [30/52] time 0.562 (0.452) data 0.431 (0.321) loss_x loss_x 1.0000 (1.0771) acc_x 75.0000 (74.3750) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [35/52] time 0.552 (0.458) data 0.421 (0.327) loss_x loss_x 1.0361 (1.0803) acc_x 75.0000 (74.3750) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [40/52] time 0.450 (0.459) data 0.320 (0.328) loss_x loss_x 0.8643 (1.0664) acc_x 78.1250 (74.5312) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [45/52] time 0.377 (0.453) data 0.246 (0.322) loss_x loss_x 1.8301 (1.0749) acc_x 62.5000 (73.9583) lr 5.3915e-05 eta 0:00:03
epoch [181/200] batch [50/52] time 0.407 (0.452) data 0.276 (0.321) loss_x loss_x 1.3379 (1.0661) acc_x 65.6250 (74.1250) lr 5.3915e-05 eta 0:00:00
epoch [181/200] batch [5/45] time 0.672 (0.459) data 0.539 (0.327) loss_u loss_u 0.7642 (0.7741) acc_u 34.3750 (30.6250) lr 5.3915e-05 eta 0:00:18
epoch [181/200] batch [10/45] time 0.461 (0.457) data 0.329 (0.326) loss_u loss_u 0.6680 (0.7406) acc_u 37.5000 (32.1875) lr 5.3915e-05 eta 0:00:15
epoch [181/200] batch [15/45] time 0.417 (0.454) data 0.285 (0.322) loss_u loss_u 0.7236 (0.7347) acc_u 31.2500 (32.0833) lr 5.3915e-05 eta 0:00:13
epoch [181/200] batch [20/45] time 0.456 (0.453) data 0.325 (0.322) loss_u loss_u 0.7964 (0.7378) acc_u 37.5000 (32.5000) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [25/45] time 0.349 (0.450) data 0.218 (0.319) loss_u loss_u 0.7578 (0.7440) acc_u 34.3750 (32.2500) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [30/45] time 0.389 (0.449) data 0.258 (0.318) loss_u loss_u 0.8271 (0.7434) acc_u 25.0000 (32.8125) lr 5.3915e-05 eta 0:00:06
epoch [181/200] batch [35/45] time 0.401 (0.449) data 0.271 (0.317) loss_u loss_u 0.8105 (0.7431) acc_u 25.0000 (32.5000) lr 5.3915e-05 eta 0:00:04
epoch [181/200] batch [40/45] time 0.427 (0.449) data 0.292 (0.317) loss_u loss_u 0.7422 (0.7391) acc_u 31.2500 (33.2031) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [45/45] time 0.511 (0.451) data 0.380 (0.320) loss_u loss_u 0.8008 (0.7419) acc_u 25.0000 (32.6389) lr 5.3915e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1264
confident_label rate tensor(0.5437, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1705
clean true:1701
clean false:4
clean_rate:0.9976539589442815
noisy true:171
noisy false:1260
after delete: len(clean_dataset) 1705
after delete: len(noisy_dataset) 1431
epoch [182/200] batch [5/53] time 0.574 (0.457) data 0.444 (0.326) loss_x loss_x 0.9019 (1.0389) acc_x 68.7500 (73.7500) lr 4.8943e-05 eta 0:00:21
epoch [182/200] batch [10/53] time 0.502 (0.456) data 0.371 (0.325) loss_x loss_x 1.0928 (1.2203) acc_x 68.7500 (68.7500) lr 4.8943e-05 eta 0:00:19
epoch [182/200] batch [15/53] time 0.541 (0.465) data 0.409 (0.334) loss_x loss_x 1.6934 (1.1813) acc_x 46.8750 (70.6250) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [20/53] time 0.493 (0.456) data 0.362 (0.326) loss_x loss_x 0.9775 (1.1576) acc_x 78.1250 (71.2500) lr 4.8943e-05 eta 0:00:15
epoch [182/200] batch [25/53] time 0.496 (0.452) data 0.365 (0.322) loss_x loss_x 0.9517 (1.1449) acc_x 78.1250 (71.7500) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [30/53] time 0.384 (0.450) data 0.254 (0.319) loss_x loss_x 1.2080 (1.1199) acc_x 68.7500 (71.9792) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [35/53] time 0.428 (0.451) data 0.298 (0.321) loss_x loss_x 1.0811 (1.1222) acc_x 65.6250 (71.8750) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [40/53] time 0.477 (0.446) data 0.346 (0.315) loss_x loss_x 0.9478 (1.1122) acc_x 78.1250 (72.2656) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [45/53] time 0.424 (0.446) data 0.294 (0.315) loss_x loss_x 0.8452 (1.1035) acc_x 78.1250 (72.2917) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [50/53] time 0.523 (0.450) data 0.392 (0.319) loss_x loss_x 1.0625 (1.0994) acc_x 65.6250 (71.6250) lr 4.8943e-05 eta 0:00:01
epoch [182/200] batch [5/44] time 0.342 (0.449) data 0.210 (0.318) loss_u loss_u 0.7490 (0.7199) acc_u 37.5000 (36.8750) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [10/44] time 0.401 (0.449) data 0.269 (0.318) loss_u loss_u 0.7427 (0.7517) acc_u 37.5000 (31.8750) lr 4.8943e-05 eta 0:00:15
epoch [182/200] batch [15/44] time 0.427 (0.447) data 0.297 (0.316) loss_u loss_u 0.7520 (0.7448) acc_u 28.1250 (32.7083) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [20/44] time 0.467 (0.444) data 0.335 (0.313) loss_u loss_u 0.7627 (0.7465) acc_u 31.2500 (32.6562) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [25/44] time 0.320 (0.446) data 0.189 (0.315) loss_u loss_u 0.6333 (0.7378) acc_u 40.6250 (33.7500) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [30/44] time 0.484 (0.448) data 0.352 (0.316) loss_u loss_u 0.7378 (0.7437) acc_u 37.5000 (33.5417) lr 4.8943e-05 eta 0:00:06
epoch [182/200] batch [35/44] time 0.354 (0.443) data 0.222 (0.312) loss_u loss_u 0.8442 (0.7432) acc_u 12.5000 (32.9464) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [40/44] time 0.518 (0.442) data 0.386 (0.311) loss_u loss_u 0.7539 (0.7447) acc_u 40.6250 (32.9688) lr 4.8943e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1254
confident_label rate tensor(0.5430, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1703
clean true:1701
clean false:2
clean_rate:0.998825601879037
noisy true:181
noisy false:1252
after delete: len(clean_dataset) 1703
after delete: len(noisy_dataset) 1433
epoch [183/200] batch [5/53] time 0.409 (0.555) data 0.278 (0.423) loss_x loss_x 1.2881 (1.0840) acc_x 68.7500 (72.5000) lr 4.4207e-05 eta 0:00:26
epoch [183/200] batch [10/53] time 0.483 (0.508) data 0.351 (0.376) loss_x loss_x 1.4238 (1.0629) acc_x 71.8750 (74.0625) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [15/53] time 0.472 (0.490) data 0.339 (0.358) loss_x loss_x 1.0049 (1.0186) acc_x 78.1250 (76.6667) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [20/53] time 0.478 (0.478) data 0.346 (0.347) loss_x loss_x 0.4805 (0.9678) acc_x 90.6250 (77.9688) lr 4.4207e-05 eta 0:00:15
epoch [183/200] batch [25/53] time 0.658 (0.490) data 0.527 (0.358) loss_x loss_x 0.8120 (0.9924) acc_x 81.2500 (76.7500) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [30/53] time 0.533 (0.488) data 0.402 (0.356) loss_x loss_x 1.1436 (0.9863) acc_x 68.7500 (76.1458) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [35/53] time 0.427 (0.479) data 0.296 (0.347) loss_x loss_x 1.4404 (1.0262) acc_x 62.5000 (75.1786) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [40/53] time 0.387 (0.474) data 0.256 (0.342) loss_x loss_x 1.0527 (1.0341) acc_x 78.1250 (74.8438) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [45/53] time 0.568 (0.479) data 0.437 (0.347) loss_x loss_x 0.8691 (1.0349) acc_x 75.0000 (74.3750) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [50/53] time 0.423 (0.475) data 0.291 (0.344) loss_x loss_x 1.1152 (1.0590) acc_x 68.7500 (73.8125) lr 4.4207e-05 eta 0:00:01
epoch [183/200] batch [5/44] time 0.445 (0.474) data 0.312 (0.342) loss_u loss_u 0.7725 (0.7966) acc_u 28.1250 (26.8750) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [10/44] time 0.501 (0.474) data 0.369 (0.342) loss_u loss_u 0.7559 (0.7688) acc_u 37.5000 (30.6250) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [15/44] time 0.462 (0.475) data 0.329 (0.343) loss_u loss_u 0.8081 (0.7799) acc_u 18.7500 (27.7083) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [20/44] time 0.590 (0.475) data 0.459 (0.343) loss_u loss_u 0.7124 (0.7755) acc_u 31.2500 (27.8125) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [25/44] time 0.376 (0.472) data 0.245 (0.340) loss_u loss_u 0.7783 (0.7746) acc_u 25.0000 (27.5000) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [30/44] time 0.379 (0.471) data 0.247 (0.339) loss_u loss_u 0.7197 (0.7653) acc_u 34.3750 (29.0625) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [35/44] time 0.440 (0.471) data 0.307 (0.338) loss_u loss_u 0.7417 (0.7653) acc_u 34.3750 (29.1964) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [40/44] time 0.432 (0.471) data 0.301 (0.339) loss_u loss_u 0.8149 (0.7635) acc_u 21.8750 (29.3750) lr 4.4207e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1238
confident_label rate tensor(0.5491, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1722
clean true:1719
clean false:3
clean_rate:0.9982578397212544
noisy true:179
noisy false:1235
after delete: len(clean_dataset) 1722
after delete: len(noisy_dataset) 1414
epoch [184/200] batch [5/53] time 0.447 (0.455) data 0.317 (0.325) loss_x loss_x 0.7378 (0.9934) acc_x 81.2500 (72.5000) lr 3.9706e-05 eta 0:00:21
epoch [184/200] batch [10/53] time 0.454 (0.450) data 0.323 (0.319) loss_x loss_x 0.8076 (0.9911) acc_x 81.2500 (72.8125) lr 3.9706e-05 eta 0:00:19
epoch [184/200] batch [15/53] time 0.469 (0.449) data 0.336 (0.318) loss_x loss_x 1.2061 (1.0696) acc_x 71.8750 (71.6667) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [20/53] time 0.387 (0.461) data 0.255 (0.329) loss_x loss_x 1.2373 (1.0939) acc_x 78.1250 (71.2500) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [25/53] time 0.477 (0.461) data 0.345 (0.330) loss_x loss_x 0.8940 (1.0927) acc_x 75.0000 (71.0000) lr 3.9706e-05 eta 0:00:12
epoch [184/200] batch [30/53] time 0.511 (0.466) data 0.379 (0.334) loss_x loss_x 1.2764 (1.0678) acc_x 71.8750 (72.1875) lr 3.9706e-05 eta 0:00:10
epoch [184/200] batch [35/53] time 0.343 (0.456) data 0.211 (0.324) loss_x loss_x 1.3125 (1.0549) acc_x 68.7500 (72.7679) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [40/53] time 0.407 (0.449) data 0.276 (0.318) loss_x loss_x 1.2061 (1.0494) acc_x 68.7500 (72.7344) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [45/53] time 0.537 (0.448) data 0.405 (0.317) loss_x loss_x 1.3779 (1.0753) acc_x 71.8750 (72.4306) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [50/53] time 0.598 (0.455) data 0.466 (0.324) loss_x loss_x 0.8213 (1.0688) acc_x 78.1250 (72.8125) lr 3.9706e-05 eta 0:00:01
epoch [184/200] batch [5/44] time 0.455 (0.462) data 0.323 (0.330) loss_u loss_u 0.7339 (0.7344) acc_u 28.1250 (36.8750) lr 3.9706e-05 eta 0:00:18
epoch [184/200] batch [10/44] time 0.411 (0.461) data 0.280 (0.330) loss_u loss_u 0.7910 (0.7467) acc_u 18.7500 (32.1875) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [15/44] time 0.374 (0.457) data 0.242 (0.325) loss_u loss_u 0.7490 (0.7453) acc_u 31.2500 (31.4583) lr 3.9706e-05 eta 0:00:13
epoch [184/200] batch [20/44] time 0.353 (0.452) data 0.222 (0.320) loss_u loss_u 0.7280 (0.7395) acc_u 43.7500 (32.9688) lr 3.9706e-05 eta 0:00:10
epoch [184/200] batch [25/44] time 0.452 (0.452) data 0.320 (0.320) loss_u loss_u 0.7310 (0.7484) acc_u 37.5000 (31.6250) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [30/44] time 0.350 (0.450) data 0.218 (0.318) loss_u loss_u 0.7734 (0.7486) acc_u 28.1250 (32.6042) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [35/44] time 0.373 (0.448) data 0.242 (0.316) loss_u loss_u 0.7646 (0.7553) acc_u 31.2500 (31.8750) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [40/44] time 0.521 (0.449) data 0.389 (0.318) loss_u loss_u 0.7544 (0.7551) acc_u 28.1250 (31.9531) lr 3.9706e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1238
confident_label rate tensor(0.5478, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1718
clean true:1715
clean false:3
clean_rate:0.9982537834691502
noisy true:183
noisy false:1235
after delete: len(clean_dataset) 1718
after delete: len(noisy_dataset) 1418
epoch [185/200] batch [5/53] time 0.509 (0.486) data 0.376 (0.354) loss_x loss_x 0.8750 (1.0067) acc_x 78.1250 (75.0000) lr 3.5443e-05 eta 0:00:23
epoch [185/200] batch [10/53] time 0.378 (0.469) data 0.247 (0.337) loss_x loss_x 1.0078 (1.0135) acc_x 65.6250 (73.7500) lr 3.5443e-05 eta 0:00:20
epoch [185/200] batch [15/53] time 0.480 (0.472) data 0.349 (0.341) loss_x loss_x 0.6299 (1.0699) acc_x 81.2500 (72.5000) lr 3.5443e-05 eta 0:00:17
epoch [185/200] batch [20/53] time 0.491 (0.461) data 0.359 (0.330) loss_x loss_x 1.4600 (1.1190) acc_x 53.1250 (70.6250) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [25/53] time 0.502 (0.457) data 0.371 (0.325) loss_x loss_x 0.9238 (1.0758) acc_x 84.3750 (72.6250) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [30/53] time 0.511 (0.457) data 0.380 (0.326) loss_x loss_x 0.9302 (1.0508) acc_x 78.1250 (73.3333) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [35/53] time 0.452 (0.456) data 0.321 (0.325) loss_x loss_x 1.1094 (1.0621) acc_x 75.0000 (72.8571) lr 3.5443e-05 eta 0:00:08
epoch [185/200] batch [40/53] time 0.472 (0.465) data 0.341 (0.333) loss_x loss_x 1.5791 (1.0820) acc_x 71.8750 (72.7344) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [45/53] time 0.425 (0.469) data 0.293 (0.338) loss_x loss_x 0.9961 (1.1013) acc_x 71.8750 (72.2917) lr 3.5443e-05 eta 0:00:03
epoch [185/200] batch [50/53] time 0.594 (0.476) data 0.462 (0.345) loss_x loss_x 1.1562 (1.1064) acc_x 65.6250 (72.3750) lr 3.5443e-05 eta 0:00:01
epoch [185/200] batch [5/44] time 0.485 (0.476) data 0.353 (0.345) loss_u loss_u 0.7002 (0.7504) acc_u 40.6250 (31.2500) lr 3.5443e-05 eta 0:00:18
epoch [185/200] batch [10/44] time 0.642 (0.477) data 0.510 (0.345) loss_u loss_u 0.7539 (0.7690) acc_u 34.3750 (29.3750) lr 3.5443e-05 eta 0:00:16
epoch [185/200] batch [15/44] time 0.539 (0.472) data 0.407 (0.340) loss_u loss_u 0.7651 (0.7506) acc_u 37.5000 (32.7083) lr 3.5443e-05 eta 0:00:13
epoch [185/200] batch [20/44] time 0.375 (0.472) data 0.243 (0.340) loss_u loss_u 0.6675 (0.7427) acc_u 46.8750 (33.7500) lr 3.5443e-05 eta 0:00:11
epoch [185/200] batch [25/44] time 0.407 (0.471) data 0.276 (0.339) loss_u loss_u 0.8311 (0.7466) acc_u 28.1250 (33.3750) lr 3.5443e-05 eta 0:00:08
epoch [185/200] batch [30/44] time 0.488 (0.470) data 0.356 (0.338) loss_u loss_u 0.7104 (0.7430) acc_u 37.5000 (33.9583) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [35/44] time 0.450 (0.466) data 0.318 (0.334) loss_u loss_u 0.7871 (0.7470) acc_u 18.7500 (32.9464) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [40/44] time 0.356 (0.465) data 0.224 (0.333) loss_u loss_u 0.7739 (0.7541) acc_u 31.2500 (32.1875) lr 3.5443e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1262
confident_label rate tensor(0.5440, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1706
clean true:1704
clean false:2
clean_rate:0.9988276670574443
noisy true:170
noisy false:1260
after delete: len(clean_dataset) 1706
after delete: len(noisy_dataset) 1430
epoch [186/200] batch [5/53] time 0.464 (0.444) data 0.333 (0.312) loss_x loss_x 0.9995 (1.0494) acc_x 75.0000 (75.6250) lr 3.1417e-05 eta 0:00:21
epoch [186/200] batch [10/53] time 0.570 (0.457) data 0.439 (0.326) loss_x loss_x 0.8662 (1.0142) acc_x 78.1250 (75.3125) lr 3.1417e-05 eta 0:00:19
epoch [186/200] batch [15/53] time 0.412 (0.444) data 0.281 (0.312) loss_x loss_x 1.6367 (1.0587) acc_x 59.3750 (74.3750) lr 3.1417e-05 eta 0:00:16
epoch [186/200] batch [20/53] time 0.418 (0.443) data 0.287 (0.312) loss_x loss_x 0.8345 (1.0937) acc_x 78.1250 (73.4375) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [25/53] time 0.390 (0.436) data 0.259 (0.305) loss_x loss_x 0.9204 (1.0626) acc_x 75.0000 (73.8750) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [30/53] time 0.698 (0.455) data 0.567 (0.324) loss_x loss_x 0.9258 (1.0688) acc_x 65.6250 (73.5417) lr 3.1417e-05 eta 0:00:10
epoch [186/200] batch [35/53] time 0.365 (0.462) data 0.234 (0.331) loss_x loss_x 0.8423 (1.0593) acc_x 84.3750 (73.6607) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [40/53] time 0.474 (0.461) data 0.343 (0.329) loss_x loss_x 1.2070 (1.0872) acc_x 65.6250 (72.9688) lr 3.1417e-05 eta 0:00:05
epoch [186/200] batch [45/53] time 0.490 (0.460) data 0.357 (0.329) loss_x loss_x 0.5073 (1.0475) acc_x 90.6250 (73.9583) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [50/53] time 0.428 (0.461) data 0.294 (0.330) loss_x loss_x 0.9795 (1.0309) acc_x 68.7500 (74.1250) lr 3.1417e-05 eta 0:00:01
epoch [186/200] batch [5/44] time 0.454 (0.459) data 0.323 (0.328) loss_u loss_u 0.7339 (0.7755) acc_u 37.5000 (29.3750) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [10/44] time 0.426 (0.457) data 0.294 (0.326) loss_u loss_u 0.8877 (0.7804) acc_u 15.6250 (28.4375) lr 3.1417e-05 eta 0:00:15
epoch [186/200] batch [15/44] time 0.553 (0.453) data 0.420 (0.321) loss_u loss_u 0.7051 (0.7728) acc_u 40.6250 (29.7917) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [20/44] time 0.358 (0.453) data 0.226 (0.321) loss_u loss_u 0.8013 (0.7620) acc_u 28.1250 (31.4062) lr 3.1417e-05 eta 0:00:10
epoch [186/200] batch [25/44] time 0.398 (0.449) data 0.267 (0.317) loss_u loss_u 0.7407 (0.7630) acc_u 31.2500 (30.6250) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [30/44] time 0.402 (0.449) data 0.270 (0.318) loss_u loss_u 0.6655 (0.7586) acc_u 46.8750 (31.9792) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [35/44] time 0.520 (0.453) data 0.388 (0.321) loss_u loss_u 0.7388 (0.7562) acc_u 28.1250 (31.8750) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [40/44] time 0.443 (0.453) data 0.311 (0.322) loss_u loss_u 0.8042 (0.7579) acc_u 21.8750 (31.4844) lr 3.1417e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1243
confident_label rate tensor(0.5440, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1706
clean true:1704
clean false:2
clean_rate:0.9988276670574443
noisy true:189
noisy false:1241
after delete: len(clean_dataset) 1706
after delete: len(noisy_dataset) 1430
epoch [187/200] batch [5/53] time 0.392 (0.452) data 0.261 (0.321) loss_x loss_x 1.3545 (1.3033) acc_x 68.7500 (67.5000) lr 2.7630e-05 eta 0:00:21
epoch [187/200] batch [10/53] time 0.470 (0.469) data 0.340 (0.338) loss_x loss_x 1.0166 (1.1475) acc_x 75.0000 (71.5625) lr 2.7630e-05 eta 0:00:20
epoch [187/200] batch [15/53] time 0.466 (0.469) data 0.335 (0.338) loss_x loss_x 0.8633 (1.1011) acc_x 78.1250 (72.7083) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [20/53] time 0.506 (0.472) data 0.374 (0.341) loss_x loss_x 1.3076 (1.1264) acc_x 68.7500 (72.6562) lr 2.7630e-05 eta 0:00:15
epoch [187/200] batch [25/53] time 0.532 (0.477) data 0.401 (0.346) loss_x loss_x 0.8384 (1.1163) acc_x 71.8750 (72.6250) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [30/53] time 0.423 (0.466) data 0.291 (0.335) loss_x loss_x 2.0605 (1.1272) acc_x 56.2500 (72.0833) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [35/53] time 0.402 (0.467) data 0.272 (0.335) loss_x loss_x 0.9336 (1.1355) acc_x 75.0000 (72.4107) lr 2.7630e-05 eta 0:00:08
epoch [187/200] batch [40/53] time 0.422 (0.461) data 0.292 (0.330) loss_x loss_x 1.1084 (1.1464) acc_x 65.6250 (72.0312) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [45/53] time 0.520 (0.462) data 0.389 (0.331) loss_x loss_x 0.5205 (1.1279) acc_x 87.5000 (72.7083) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [50/53] time 0.380 (0.453) data 0.249 (0.322) loss_x loss_x 1.0752 (1.1374) acc_x 75.0000 (72.5625) lr 2.7630e-05 eta 0:00:01
epoch [187/200] batch [5/44] time 0.444 (0.454) data 0.312 (0.323) loss_u loss_u 0.7778 (0.7463) acc_u 31.2500 (33.7500) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [10/44] time 0.425 (0.451) data 0.294 (0.320) loss_u loss_u 0.6772 (0.7342) acc_u 37.5000 (34.0625) lr 2.7630e-05 eta 0:00:15
epoch [187/200] batch [15/44] time 0.546 (0.454) data 0.415 (0.322) loss_u loss_u 0.7422 (0.7517) acc_u 31.2500 (31.8750) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [20/44] time 0.412 (0.455) data 0.280 (0.323) loss_u loss_u 0.6670 (0.7454) acc_u 46.8750 (32.8125) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [25/44] time 0.418 (0.450) data 0.286 (0.318) loss_u loss_u 0.7998 (0.7515) acc_u 28.1250 (32.0000) lr 2.7630e-05 eta 0:00:08
epoch [187/200] batch [30/44] time 0.411 (0.447) data 0.279 (0.316) loss_u loss_u 0.7739 (0.7511) acc_u 25.0000 (31.5625) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [35/44] time 0.478 (0.447) data 0.347 (0.315) loss_u loss_u 0.7144 (0.7485) acc_u 34.3750 (31.6964) lr 2.7630e-05 eta 0:00:04
epoch [187/200] batch [40/44] time 0.444 (0.444) data 0.312 (0.313) loss_u loss_u 0.8140 (0.7494) acc_u 18.7500 (31.4844) lr 2.7630e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1210
confident_label rate tensor(0.5545, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1739
clean true:1737
clean false:2
clean_rate:0.9988499137435307
noisy true:189
noisy false:1208
after delete: len(clean_dataset) 1739
after delete: len(noisy_dataset) 1397
epoch [188/200] batch [5/54] time 0.426 (0.420) data 0.295 (0.289) loss_x loss_x 0.7539 (0.7851) acc_x 78.1250 (83.1250) lr 2.4083e-05 eta 0:00:20
epoch [188/200] batch [10/54] time 0.482 (0.426) data 0.351 (0.295) loss_x loss_x 1.1826 (0.9798) acc_x 71.8750 (75.0000) lr 2.4083e-05 eta 0:00:18
epoch [188/200] batch [15/54] time 0.449 (0.447) data 0.319 (0.316) loss_x loss_x 1.0723 (0.9811) acc_x 78.1250 (74.7917) lr 2.4083e-05 eta 0:00:17
epoch [188/200] batch [20/54] time 0.369 (0.455) data 0.239 (0.324) loss_x loss_x 1.0312 (0.9977) acc_x 71.8750 (74.3750) lr 2.4083e-05 eta 0:00:15
epoch [188/200] batch [25/54] time 0.514 (0.447) data 0.384 (0.316) loss_x loss_x 1.1855 (1.0083) acc_x 68.7500 (74.1250) lr 2.4083e-05 eta 0:00:12
epoch [188/200] batch [30/54] time 0.502 (0.450) data 0.371 (0.320) loss_x loss_x 1.6025 (1.0595) acc_x 62.5000 (73.6458) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [35/54] time 0.543 (0.453) data 0.412 (0.322) loss_x loss_x 1.2725 (1.0726) acc_x 71.8750 (73.2143) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [40/54] time 0.396 (0.452) data 0.265 (0.321) loss_x loss_x 0.9805 (1.0578) acc_x 68.7500 (72.9688) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [45/54] time 0.378 (0.445) data 0.247 (0.314) loss_x loss_x 1.0498 (1.0711) acc_x 71.8750 (72.5000) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [50/54] time 0.385 (0.446) data 0.254 (0.315) loss_x loss_x 0.8652 (1.0671) acc_x 78.1250 (72.7500) lr 2.4083e-05 eta 0:00:01
epoch [188/200] batch [5/43] time 0.383 (0.444) data 0.252 (0.313) loss_u loss_u 0.8999 (0.7949) acc_u 6.2500 (28.7500) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [10/43] time 0.607 (0.446) data 0.477 (0.315) loss_u loss_u 0.7896 (0.7815) acc_u 21.8750 (28.4375) lr 2.4083e-05 eta 0:00:14
epoch [188/200] batch [15/43] time 0.520 (0.445) data 0.390 (0.315) loss_u loss_u 0.7617 (0.7675) acc_u 28.1250 (29.3750) lr 2.4083e-05 eta 0:00:12
epoch [188/200] batch [20/43] time 0.334 (0.447) data 0.203 (0.316) loss_u loss_u 0.8418 (0.7748) acc_u 21.8750 (29.2188) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [25/43] time 0.407 (0.447) data 0.276 (0.316) loss_u loss_u 0.7627 (0.7703) acc_u 28.1250 (29.6250) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [30/43] time 0.385 (0.445) data 0.255 (0.314) loss_u loss_u 0.6372 (0.7643) acc_u 37.5000 (30.0000) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [35/43] time 0.412 (0.444) data 0.282 (0.313) loss_u loss_u 0.6763 (0.7642) acc_u 43.7500 (30.0000) lr 2.4083e-05 eta 0:00:03
epoch [188/200] batch [40/43] time 0.459 (0.442) data 0.328 (0.311) loss_u loss_u 0.8433 (0.7676) acc_u 15.6250 (29.3750) lr 2.4083e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1237
confident_label rate tensor(0.5517, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1730
clean true:1727
clean false:3
clean_rate:0.9982658959537573
noisy true:172
noisy false:1234
after delete: len(clean_dataset) 1730
after delete: len(noisy_dataset) 1406
epoch [189/200] batch [5/54] time 0.417 (0.490) data 0.287 (0.359) loss_x loss_x 0.9517 (1.0041) acc_x 68.7500 (75.6250) lr 2.0777e-05 eta 0:00:24
epoch [189/200] batch [10/54] time 0.378 (0.461) data 0.248 (0.330) loss_x loss_x 0.7510 (1.1085) acc_x 84.3750 (72.5000) lr 2.0777e-05 eta 0:00:20
epoch [189/200] batch [15/54] time 0.432 (0.451) data 0.302 (0.320) loss_x loss_x 1.3691 (1.0509) acc_x 65.6250 (73.7500) lr 2.0777e-05 eta 0:00:17
epoch [189/200] batch [20/54] time 0.498 (0.442) data 0.367 (0.312) loss_x loss_x 1.1855 (1.0349) acc_x 65.6250 (73.9062) lr 2.0777e-05 eta 0:00:15
epoch [189/200] batch [25/54] time 0.466 (0.450) data 0.335 (0.319) loss_x loss_x 0.8452 (1.0408) acc_x 75.0000 (74.7500) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [30/54] time 0.412 (0.444) data 0.281 (0.313) loss_x loss_x 0.7124 (1.0224) acc_x 84.3750 (75.5208) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [35/54] time 0.514 (0.449) data 0.383 (0.318) loss_x loss_x 1.1777 (1.0360) acc_x 75.0000 (74.4643) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [40/54] time 0.361 (0.446) data 0.231 (0.315) loss_x loss_x 1.5312 (1.0637) acc_x 65.6250 (74.1406) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [45/54] time 0.374 (0.445) data 0.243 (0.314) loss_x loss_x 0.8999 (1.0718) acc_x 90.6250 (74.3056) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [50/54] time 0.407 (0.444) data 0.276 (0.313) loss_x loss_x 0.7759 (1.0402) acc_x 78.1250 (74.8750) lr 2.0777e-05 eta 0:00:01
epoch [189/200] batch [5/43] time 0.419 (0.441) data 0.287 (0.310) loss_u loss_u 0.7041 (0.7521) acc_u 34.3750 (30.0000) lr 2.0777e-05 eta 0:00:16
epoch [189/200] batch [10/43] time 0.453 (0.440) data 0.322 (0.309) loss_u loss_u 0.7476 (0.7609) acc_u 37.5000 (32.5000) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [15/43] time 0.402 (0.438) data 0.270 (0.307) loss_u loss_u 0.7427 (0.7506) acc_u 31.2500 (32.2917) lr 2.0777e-05 eta 0:00:12
epoch [189/200] batch [20/43] time 0.386 (0.439) data 0.256 (0.308) loss_u loss_u 0.7476 (0.7496) acc_u 34.3750 (32.1875) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [25/43] time 0.450 (0.439) data 0.317 (0.308) loss_u loss_u 0.7910 (0.7561) acc_u 28.1250 (31.8750) lr 2.0777e-05 eta 0:00:07
epoch [189/200] batch [30/43] time 0.476 (0.441) data 0.345 (0.310) loss_u loss_u 0.7695 (0.7551) acc_u 21.8750 (32.1875) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [35/43] time 0.391 (0.443) data 0.261 (0.312) loss_u loss_u 0.7812 (0.7531) acc_u 25.0000 (31.9643) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [40/43] time 0.369 (0.442) data 0.239 (0.311) loss_u loss_u 0.7812 (0.7541) acc_u 25.0000 (31.9531) lr 2.0777e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1198
confident_label rate tensor(0.5580, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1750
clean true:1748
clean false:2
clean_rate:0.9988571428571429
noisy true:190
noisy false:1196
after delete: len(clean_dataset) 1750
after delete: len(noisy_dataset) 1386
epoch [190/200] batch [5/54] time 0.419 (0.437) data 0.286 (0.307) loss_x loss_x 1.1758 (1.0621) acc_x 65.6250 (73.7500) lr 1.7713e-05 eta 0:00:21
epoch [190/200] batch [10/54] time 0.458 (0.466) data 0.326 (0.335) loss_x loss_x 1.2744 (1.1392) acc_x 68.7500 (72.8125) lr 1.7713e-05 eta 0:00:20
epoch [190/200] batch [15/54] time 0.414 (0.458) data 0.283 (0.327) loss_x loss_x 1.5332 (1.1264) acc_x 59.3750 (72.7083) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [20/54] time 0.607 (0.476) data 0.476 (0.345) loss_x loss_x 1.1787 (1.0776) acc_x 75.0000 (73.4375) lr 1.7713e-05 eta 0:00:16
epoch [190/200] batch [25/54] time 0.460 (0.473) data 0.329 (0.342) loss_x loss_x 1.5566 (1.0783) acc_x 62.5000 (73.5000) lr 1.7713e-05 eta 0:00:13
epoch [190/200] batch [30/54] time 0.342 (0.470) data 0.211 (0.338) loss_x loss_x 1.4258 (1.0715) acc_x 62.5000 (73.3333) lr 1.7713e-05 eta 0:00:11
epoch [190/200] batch [35/54] time 0.438 (0.471) data 0.307 (0.340) loss_x loss_x 0.9463 (1.0341) acc_x 75.0000 (74.3750) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [40/54] time 0.421 (0.468) data 0.290 (0.337) loss_x loss_x 0.9199 (1.0227) acc_x 75.0000 (74.6875) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [45/54] time 0.607 (0.470) data 0.475 (0.339) loss_x loss_x 1.1260 (1.0204) acc_x 65.6250 (74.5833) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [50/54] time 0.404 (0.469) data 0.273 (0.338) loss_x loss_x 1.0518 (1.0201) acc_x 81.2500 (74.8750) lr 1.7713e-05 eta 0:00:01
epoch [190/200] batch [5/43] time 0.375 (0.464) data 0.243 (0.333) loss_u loss_u 0.7324 (0.7267) acc_u 34.3750 (34.3750) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [10/43] time 0.438 (0.463) data 0.306 (0.332) loss_u loss_u 0.7236 (0.7582) acc_u 37.5000 (31.5625) lr 1.7713e-05 eta 0:00:15
epoch [190/200] batch [15/43] time 0.394 (0.459) data 0.262 (0.328) loss_u loss_u 0.7720 (0.7565) acc_u 34.3750 (31.6667) lr 1.7713e-05 eta 0:00:12
epoch [190/200] batch [20/43] time 0.443 (0.458) data 0.312 (0.326) loss_u loss_u 0.7285 (0.7598) acc_u 37.5000 (31.4062) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [25/43] time 0.340 (0.454) data 0.209 (0.323) loss_u loss_u 0.6948 (0.7570) acc_u 31.2500 (31.1250) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [30/43] time 0.371 (0.450) data 0.240 (0.318) loss_u loss_u 0.7495 (0.7576) acc_u 34.3750 (31.2500) lr 1.7713e-05 eta 0:00:05
epoch [190/200] batch [35/43] time 0.438 (0.448) data 0.307 (0.316) loss_u loss_u 0.7632 (0.7512) acc_u 28.1250 (32.2321) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [40/43] time 0.580 (0.448) data 0.448 (0.317) loss_u loss_u 0.7383 (0.7602) acc_u 37.5000 (30.9375) lr 1.7713e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1244
confident_label rate tensor(0.5459, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1712
clean true:1711
clean false:1
clean_rate:0.9994158878504673
noisy true:181
noisy false:1243
after delete: len(clean_dataset) 1712
after delete: len(noisy_dataset) 1424
epoch [191/200] batch [5/53] time 0.331 (0.437) data 0.200 (0.306) loss_x loss_x 1.3789 (1.3550) acc_x 62.5000 (65.0000) lr 1.4891e-05 eta 0:00:20
epoch [191/200] batch [10/53] time 0.434 (0.451) data 0.303 (0.320) loss_x loss_x 1.1328 (1.2222) acc_x 75.0000 (69.0625) lr 1.4891e-05 eta 0:00:19
epoch [191/200] batch [15/53] time 0.436 (0.448) data 0.305 (0.317) loss_x loss_x 0.8149 (1.1895) acc_x 84.3750 (70.0000) lr 1.4891e-05 eta 0:00:17
epoch [191/200] batch [20/53] time 0.390 (0.453) data 0.259 (0.322) loss_x loss_x 0.9897 (1.1989) acc_x 75.0000 (70.1562) lr 1.4891e-05 eta 0:00:14
epoch [191/200] batch [25/53] time 0.459 (0.458) data 0.329 (0.327) loss_x loss_x 0.9785 (1.1779) acc_x 75.0000 (71.0000) lr 1.4891e-05 eta 0:00:12
epoch [191/200] batch [30/53] time 0.387 (0.456) data 0.257 (0.325) loss_x loss_x 1.2139 (1.2032) acc_x 71.8750 (70.0000) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [35/53] time 0.401 (0.451) data 0.270 (0.320) loss_x loss_x 1.0156 (1.1713) acc_x 68.7500 (70.6250) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [40/53] time 0.444 (0.457) data 0.313 (0.326) loss_x loss_x 1.1406 (1.1511) acc_x 59.3750 (71.1719) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [45/53] time 0.410 (0.452) data 0.280 (0.321) loss_x loss_x 0.6782 (1.1258) acc_x 84.3750 (71.9444) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [50/53] time 0.570 (0.453) data 0.439 (0.322) loss_x loss_x 1.3818 (1.1284) acc_x 62.5000 (71.7500) lr 1.4891e-05 eta 0:00:01
epoch [191/200] batch [5/44] time 0.364 (0.448) data 0.232 (0.317) loss_u loss_u 0.6348 (0.7370) acc_u 50.0000 (32.5000) lr 1.4891e-05 eta 0:00:17
epoch [191/200] batch [10/44] time 0.315 (0.446) data 0.184 (0.315) loss_u loss_u 0.8130 (0.7334) acc_u 21.8750 (32.5000) lr 1.4891e-05 eta 0:00:15
epoch [191/200] batch [15/44] time 0.383 (0.445) data 0.252 (0.313) loss_u loss_u 0.8198 (0.7448) acc_u 21.8750 (31.8750) lr 1.4891e-05 eta 0:00:12
epoch [191/200] batch [20/44] time 0.420 (0.444) data 0.289 (0.313) loss_u loss_u 0.7896 (0.7456) acc_u 28.1250 (32.6562) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [25/44] time 0.471 (0.444) data 0.339 (0.313) loss_u loss_u 0.7168 (0.7570) acc_u 28.1250 (30.7500) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [30/44] time 0.454 (0.442) data 0.322 (0.311) loss_u loss_u 0.7349 (0.7538) acc_u 40.6250 (31.4583) lr 1.4891e-05 eta 0:00:06
epoch [191/200] batch [35/44] time 0.613 (0.444) data 0.482 (0.313) loss_u loss_u 0.7734 (0.7560) acc_u 25.0000 (30.8036) lr 1.4891e-05 eta 0:00:04
epoch [191/200] batch [40/44] time 0.363 (0.443) data 0.232 (0.312) loss_u loss_u 0.8521 (0.7600) acc_u 18.7500 (30.5469) lr 1.4891e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1228
confident_label rate tensor(0.5501, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1725
clean true:1723
clean false:2
clean_rate:0.998840579710145
noisy true:185
noisy false:1226
after delete: len(clean_dataset) 1725
after delete: len(noisy_dataset) 1411
epoch [192/200] batch [5/53] time 0.489 (0.465) data 0.358 (0.333) loss_x loss_x 1.5068 (1.1884) acc_x 53.1250 (65.6250) lr 1.2312e-05 eta 0:00:22
epoch [192/200] batch [10/53] time 0.345 (0.472) data 0.213 (0.341) loss_x loss_x 0.7280 (1.2400) acc_x 75.0000 (67.5000) lr 1.2312e-05 eta 0:00:20
epoch [192/200] batch [15/53] time 0.413 (0.462) data 0.282 (0.331) loss_x loss_x 1.2461 (1.1731) acc_x 75.0000 (71.4583) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [20/53] time 0.516 (0.473) data 0.385 (0.342) loss_x loss_x 0.8311 (1.1659) acc_x 84.3750 (71.8750) lr 1.2312e-05 eta 0:00:15
epoch [192/200] batch [25/53] time 0.498 (0.471) data 0.367 (0.340) loss_x loss_x 1.4795 (1.1529) acc_x 68.7500 (71.8750) lr 1.2312e-05 eta 0:00:13
epoch [192/200] batch [30/53] time 0.458 (0.462) data 0.328 (0.331) loss_x loss_x 0.7544 (1.1218) acc_x 75.0000 (71.8750) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [35/53] time 0.447 (0.456) data 0.316 (0.325) loss_x loss_x 1.2666 (1.1137) acc_x 68.7500 (72.2321) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [40/53] time 0.408 (0.456) data 0.278 (0.325) loss_x loss_x 1.0342 (1.0881) acc_x 75.0000 (72.7344) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [45/53] time 0.344 (0.450) data 0.214 (0.319) loss_x loss_x 1.2139 (1.0973) acc_x 71.8750 (72.8472) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [50/53] time 0.508 (0.452) data 0.378 (0.321) loss_x loss_x 0.5679 (1.0751) acc_x 84.3750 (73.2500) lr 1.2312e-05 eta 0:00:01
epoch [192/200] batch [5/44] time 0.434 (0.451) data 0.302 (0.321) loss_u loss_u 0.7871 (0.7808) acc_u 25.0000 (28.1250) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [10/44] time 0.352 (0.452) data 0.220 (0.321) loss_u loss_u 0.7622 (0.7630) acc_u 28.1250 (29.6875) lr 1.2312e-05 eta 0:00:15
epoch [192/200] batch [15/44] time 0.644 (0.454) data 0.513 (0.323) loss_u loss_u 0.7734 (0.7485) acc_u 25.0000 (30.8333) lr 1.2312e-05 eta 0:00:13
epoch [192/200] batch [20/44] time 0.584 (0.452) data 0.452 (0.321) loss_u loss_u 0.7505 (0.7458) acc_u 31.2500 (31.4062) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [25/44] time 0.477 (0.450) data 0.345 (0.319) loss_u loss_u 0.7466 (0.7394) acc_u 28.1250 (32.0000) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [30/44] time 0.394 (0.449) data 0.262 (0.317) loss_u loss_u 0.7646 (0.7426) acc_u 28.1250 (32.2917) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [35/44] time 0.444 (0.446) data 0.313 (0.315) loss_u loss_u 0.7153 (0.7509) acc_u 34.3750 (31.2500) lr 1.2312e-05 eta 0:00:04
epoch [192/200] batch [40/44] time 0.473 (0.446) data 0.342 (0.315) loss_u loss_u 0.7983 (0.7544) acc_u 31.2500 (31.0156) lr 1.2312e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1202
confident_label rate tensor(0.5580, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1750
clean true:1748
clean false:2
clean_rate:0.9988571428571429
noisy true:186
noisy false:1200
after delete: len(clean_dataset) 1750
after delete: len(noisy_dataset) 1386
epoch [193/200] batch [5/54] time 0.495 (0.486) data 0.364 (0.354) loss_x loss_x 0.6787 (1.0263) acc_x 81.2500 (73.1250) lr 9.9763e-06 eta 0:00:23
epoch [193/200] batch [10/54] time 0.453 (0.453) data 0.322 (0.321) loss_x loss_x 0.8184 (0.9670) acc_x 78.1250 (73.4375) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [15/54] time 0.390 (0.453) data 0.258 (0.322) loss_x loss_x 1.7275 (1.0164) acc_x 59.3750 (72.9167) lr 9.9763e-06 eta 0:00:17
epoch [193/200] batch [20/54] time 0.563 (0.474) data 0.431 (0.342) loss_x loss_x 0.9390 (1.0166) acc_x 65.6250 (72.6562) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [25/54] time 0.415 (0.479) data 0.284 (0.347) loss_x loss_x 1.2393 (1.0494) acc_x 71.8750 (72.3750) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [30/54] time 0.454 (0.480) data 0.323 (0.348) loss_x loss_x 1.3457 (1.0577) acc_x 62.5000 (72.7083) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [35/54] time 0.475 (0.487) data 0.344 (0.355) loss_x loss_x 0.8955 (1.0477) acc_x 81.2500 (73.1250) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [40/54] time 0.451 (0.488) data 0.321 (0.357) loss_x loss_x 0.8584 (1.0253) acc_x 78.1250 (73.5156) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [45/54] time 0.451 (0.478) data 0.321 (0.347) loss_x loss_x 2.1309 (1.0389) acc_x 56.2500 (73.4028) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [50/54] time 0.438 (0.473) data 0.308 (0.342) loss_x loss_x 1.2803 (1.0375) acc_x 71.8750 (73.6875) lr 9.9763e-06 eta 0:00:01
epoch [193/200] batch [5/43] time 0.431 (0.463) data 0.299 (0.332) loss_u loss_u 0.8188 (0.7867) acc_u 28.1250 (30.0000) lr 9.9763e-06 eta 0:00:17
epoch [193/200] batch [10/43] time 0.495 (0.461) data 0.363 (0.330) loss_u loss_u 0.7690 (0.7860) acc_u 28.1250 (28.1250) lr 9.9763e-06 eta 0:00:15
epoch [193/200] batch [15/43] time 0.429 (0.465) data 0.297 (0.333) loss_u loss_u 0.8022 (0.7711) acc_u 25.0000 (29.3750) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [20/43] time 0.423 (0.461) data 0.291 (0.329) loss_u loss_u 0.6880 (0.7692) acc_u 43.7500 (29.3750) lr 9.9763e-06 eta 0:00:10
epoch [193/200] batch [25/43] time 0.445 (0.456) data 0.315 (0.324) loss_u loss_u 0.6357 (0.7650) acc_u 50.0000 (30.2500) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [30/43] time 0.324 (0.454) data 0.193 (0.323) loss_u loss_u 0.7705 (0.7595) acc_u 28.1250 (30.6250) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [35/43] time 0.475 (0.450) data 0.345 (0.319) loss_u loss_u 0.7915 (0.7620) acc_u 25.0000 (30.2679) lr 9.9763e-06 eta 0:00:03
epoch [193/200] batch [40/43] time 0.392 (0.453) data 0.261 (0.322) loss_u loss_u 0.6904 (0.7616) acc_u 40.6250 (30.1562) lr 9.9763e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1244
confident_label rate tensor(0.5450, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1709
clean true:1709
clean false:0
clean_rate:1.0
noisy true:183
noisy false:1244
after delete: len(clean_dataset) 1709
after delete: len(noisy_dataset) 1427
epoch [194/200] batch [5/53] time 0.663 (0.467) data 0.532 (0.336) loss_x loss_x 1.1562 (1.1430) acc_x 65.6250 (70.6250) lr 7.8853e-06 eta 0:00:22
epoch [194/200] batch [10/53] time 0.413 (0.457) data 0.282 (0.326) loss_x loss_x 0.7617 (0.9885) acc_x 84.3750 (74.0625) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [15/53] time 0.494 (0.452) data 0.363 (0.321) loss_x loss_x 0.9707 (1.0480) acc_x 78.1250 (73.9583) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [20/53] time 0.418 (0.455) data 0.287 (0.324) loss_x loss_x 0.8154 (1.0699) acc_x 78.1250 (73.4375) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [25/53] time 0.406 (0.449) data 0.275 (0.318) loss_x loss_x 1.1855 (1.0372) acc_x 75.0000 (74.0000) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [30/53] time 0.442 (0.444) data 0.311 (0.313) loss_x loss_x 1.2051 (1.0746) acc_x 68.7500 (73.3333) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [35/53] time 0.421 (0.448) data 0.290 (0.317) loss_x loss_x 1.0000 (1.0293) acc_x 78.1250 (74.4643) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [40/53] time 0.388 (0.446) data 0.257 (0.315) loss_x loss_x 1.2500 (1.0098) acc_x 68.7500 (74.8438) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [45/53] time 0.428 (0.448) data 0.297 (0.317) loss_x loss_x 1.1436 (1.0200) acc_x 78.1250 (74.9306) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [50/53] time 0.529 (0.451) data 0.398 (0.320) loss_x loss_x 1.0752 (1.0277) acc_x 81.2500 (74.6875) lr 7.8853e-06 eta 0:00:01
epoch [194/200] batch [5/44] time 0.514 (0.451) data 0.384 (0.320) loss_u loss_u 0.7505 (0.7674) acc_u 34.3750 (28.1250) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [10/44] time 0.433 (0.451) data 0.302 (0.320) loss_u loss_u 0.6572 (0.7510) acc_u 43.7500 (30.6250) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [15/44] time 0.434 (0.449) data 0.303 (0.318) loss_u loss_u 0.7461 (0.7583) acc_u 31.2500 (29.7917) lr 7.8853e-06 eta 0:00:13
epoch [194/200] batch [20/44] time 0.603 (0.449) data 0.471 (0.318) loss_u loss_u 0.7212 (0.7552) acc_u 37.5000 (30.1562) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [25/44] time 0.408 (0.450) data 0.276 (0.319) loss_u loss_u 0.6987 (0.7481) acc_u 37.5000 (30.7500) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [30/44] time 0.358 (0.450) data 0.227 (0.319) loss_u loss_u 0.8306 (0.7508) acc_u 21.8750 (30.5208) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [35/44] time 0.503 (0.450) data 0.371 (0.319) loss_u loss_u 0.6724 (0.7506) acc_u 37.5000 (30.7143) lr 7.8853e-06 eta 0:00:04
epoch [194/200] batch [40/44] time 0.372 (0.446) data 0.240 (0.315) loss_u loss_u 0.7754 (0.7555) acc_u 34.3750 (30.6250) lr 7.8853e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1204
confident_label rate tensor(0.5545, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1739
clean true:1738
clean false:1
clean_rate:0.9994249568717654
noisy true:194
noisy false:1203
after delete: len(clean_dataset) 1739
after delete: len(noisy_dataset) 1397
epoch [195/200] batch [5/54] time 0.426 (0.440) data 0.295 (0.309) loss_x loss_x 0.7246 (0.9883) acc_x 84.3750 (76.8750) lr 6.0390e-06 eta 0:00:21
epoch [195/200] batch [10/54] time 0.401 (0.458) data 0.269 (0.327) loss_x loss_x 1.0576 (1.0621) acc_x 75.0000 (74.6875) lr 6.0390e-06 eta 0:00:20
epoch [195/200] batch [15/54] time 0.448 (0.458) data 0.317 (0.327) loss_x loss_x 1.0254 (1.0020) acc_x 75.0000 (75.4167) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [20/54] time 0.399 (0.456) data 0.269 (0.325) loss_x loss_x 1.0020 (0.9888) acc_x 71.8750 (75.3125) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [25/54] time 0.571 (0.464) data 0.441 (0.333) loss_x loss_x 0.5186 (0.9738) acc_x 93.7500 (76.0000) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [30/54] time 0.598 (0.467) data 0.467 (0.336) loss_x loss_x 0.9351 (0.9743) acc_x 81.2500 (76.3542) lr 6.0390e-06 eta 0:00:11
epoch [195/200] batch [35/54] time 0.385 (0.460) data 0.254 (0.329) loss_x loss_x 1.0225 (0.9889) acc_x 75.0000 (76.2500) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [40/54] time 0.412 (0.458) data 0.281 (0.327) loss_x loss_x 0.9897 (0.9948) acc_x 78.1250 (76.4844) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [45/54] time 0.484 (0.457) data 0.351 (0.326) loss_x loss_x 1.0840 (1.0090) acc_x 71.8750 (75.9028) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [50/54] time 0.521 (0.460) data 0.388 (0.328) loss_x loss_x 0.8950 (1.0057) acc_x 84.3750 (76.1875) lr 6.0390e-06 eta 0:00:01
epoch [195/200] batch [5/43] time 0.423 (0.454) data 0.291 (0.323) loss_u loss_u 0.7520 (0.7536) acc_u 31.2500 (35.0000) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [10/43] time 0.363 (0.454) data 0.228 (0.323) loss_u loss_u 0.8569 (0.7623) acc_u 15.6250 (31.5625) lr 6.0390e-06 eta 0:00:14
epoch [195/200] batch [15/43] time 0.350 (0.456) data 0.218 (0.325) loss_u loss_u 0.6763 (0.7583) acc_u 46.8750 (31.4583) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [20/43] time 0.405 (0.450) data 0.273 (0.319) loss_u loss_u 0.7490 (0.7581) acc_u 28.1250 (30.7812) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [25/43] time 0.502 (0.448) data 0.369 (0.317) loss_u loss_u 0.7485 (0.7580) acc_u 28.1250 (30.7500) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [30/43] time 0.393 (0.452) data 0.261 (0.320) loss_u loss_u 0.8574 (0.7607) acc_u 21.8750 (30.9375) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [35/43] time 0.431 (0.450) data 0.299 (0.319) loss_u loss_u 0.7344 (0.7631) acc_u 34.3750 (30.4464) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [40/43] time 0.466 (0.450) data 0.334 (0.319) loss_u loss_u 0.8276 (0.7646) acc_u 18.7500 (30.3906) lr 6.0390e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1239
confident_label rate tensor(0.5469, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1715
clean true:1713
clean false:2
clean_rate:0.9988338192419826
noisy true:184
noisy false:1237
after delete: len(clean_dataset) 1715
after delete: len(noisy_dataset) 1421
epoch [196/200] batch [5/53] time 0.406 (0.456) data 0.275 (0.325) loss_x loss_x 1.2656 (1.1355) acc_x 65.6250 (70.0000) lr 4.4380e-06 eta 0:00:21
epoch [196/200] batch [10/53] time 0.449 (0.480) data 0.318 (0.349) loss_x loss_x 0.7993 (1.0825) acc_x 81.2500 (72.8125) lr 4.4380e-06 eta 0:00:20
epoch [196/200] batch [15/53] time 0.406 (0.500) data 0.275 (0.369) loss_x loss_x 0.7954 (1.0566) acc_x 78.1250 (71.8750) lr 4.4380e-06 eta 0:00:19
epoch [196/200] batch [20/53] time 0.526 (0.488) data 0.395 (0.356) loss_x loss_x 0.6904 (1.0832) acc_x 87.5000 (73.4375) lr 4.4380e-06 eta 0:00:16
epoch [196/200] batch [25/53] time 0.496 (0.483) data 0.366 (0.352) loss_x loss_x 1.1797 (1.1172) acc_x 75.0000 (72.7500) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [30/53] time 0.477 (0.483) data 0.346 (0.351) loss_x loss_x 1.6113 (1.1342) acc_x 65.6250 (72.6042) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [35/53] time 0.323 (0.470) data 0.192 (0.339) loss_x loss_x 0.9087 (1.1140) acc_x 71.8750 (72.9464) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [40/53] time 0.425 (0.470) data 0.294 (0.339) loss_x loss_x 1.7070 (1.1327) acc_x 53.1250 (72.0312) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [45/53] time 0.346 (0.465) data 0.215 (0.334) loss_x loss_x 1.0879 (1.1261) acc_x 78.1250 (72.3611) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [50/53] time 0.419 (0.461) data 0.288 (0.330) loss_x loss_x 1.4346 (1.1243) acc_x 68.7500 (72.7500) lr 4.4380e-06 eta 0:00:01
epoch [196/200] batch [5/44] time 0.405 (0.461) data 0.273 (0.330) loss_u loss_u 0.7056 (0.7282) acc_u 37.5000 (35.6250) lr 4.4380e-06 eta 0:00:17
epoch [196/200] batch [10/44] time 0.323 (0.456) data 0.191 (0.325) loss_u loss_u 0.6948 (0.7370) acc_u 37.5000 (33.7500) lr 4.4380e-06 eta 0:00:15
epoch [196/200] batch [15/44] time 0.298 (0.449) data 0.166 (0.318) loss_u loss_u 0.8125 (0.7486) acc_u 21.8750 (32.2917) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [20/44] time 0.369 (0.451) data 0.237 (0.320) loss_u loss_u 0.8018 (0.7515) acc_u 28.1250 (31.7188) lr 4.4380e-06 eta 0:00:10
epoch [196/200] batch [25/44] time 0.494 (0.451) data 0.362 (0.320) loss_u loss_u 0.6899 (0.7488) acc_u 40.6250 (33.0000) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [30/44] time 0.405 (0.448) data 0.273 (0.317) loss_u loss_u 0.7251 (0.7536) acc_u 34.3750 (32.0833) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [35/44] time 0.756 (0.453) data 0.621 (0.322) loss_u loss_u 0.7261 (0.7540) acc_u 34.3750 (31.6964) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [40/44] time 0.565 (0.454) data 0.433 (0.323) loss_u loss_u 0.6201 (0.7511) acc_u 40.6250 (32.1094) lr 4.4380e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1258
confident_label rate tensor(0.5424, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1701
clean true:1699
clean false:2
clean_rate:0.9988242210464433
noisy true:179
noisy false:1256
after delete: len(clean_dataset) 1701
after delete: len(noisy_dataset) 1435
epoch [197/200] batch [5/53] time 0.494 (0.514) data 0.363 (0.383) loss_x loss_x 0.8750 (1.1543) acc_x 78.1250 (72.5000) lr 3.0827e-06 eta 0:00:24
epoch [197/200] batch [10/53] time 0.585 (0.525) data 0.454 (0.394) loss_x loss_x 1.0088 (1.0997) acc_x 68.7500 (72.5000) lr 3.0827e-06 eta 0:00:22
epoch [197/200] batch [15/53] time 0.427 (0.500) data 0.295 (0.368) loss_x loss_x 1.5117 (1.0998) acc_x 62.5000 (72.2917) lr 3.0827e-06 eta 0:00:18
epoch [197/200] batch [20/53] time 0.452 (0.492) data 0.320 (0.360) loss_x loss_x 1.1865 (1.0746) acc_x 68.7500 (73.2812) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [25/53] time 0.468 (0.496) data 0.336 (0.365) loss_x loss_x 1.4717 (1.0783) acc_x 65.6250 (73.3750) lr 3.0827e-06 eta 0:00:13
epoch [197/200] batch [30/53] time 0.496 (0.495) data 0.365 (0.363) loss_x loss_x 0.9512 (1.0444) acc_x 71.8750 (73.9583) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [35/53] time 0.428 (0.486) data 0.297 (0.355) loss_x loss_x 0.9287 (1.0265) acc_x 75.0000 (74.4643) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [40/53] time 0.422 (0.482) data 0.290 (0.350) loss_x loss_x 1.2822 (1.0209) acc_x 71.8750 (74.7656) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [45/53] time 0.423 (0.477) data 0.292 (0.346) loss_x loss_x 1.0684 (1.0237) acc_x 65.6250 (74.5833) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [50/53] time 0.456 (0.474) data 0.324 (0.342) loss_x loss_x 1.3809 (1.0233) acc_x 68.7500 (74.6875) lr 3.0827e-06 eta 0:00:01
epoch [197/200] batch [5/44] time 0.495 (0.475) data 0.362 (0.343) loss_u loss_u 0.7344 (0.7209) acc_u 34.3750 (36.2500) lr 3.0827e-06 eta 0:00:18
epoch [197/200] batch [10/44] time 0.467 (0.473) data 0.334 (0.342) loss_u loss_u 0.8540 (0.7250) acc_u 18.7500 (35.3125) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [15/44] time 0.443 (0.470) data 0.311 (0.338) loss_u loss_u 0.7534 (0.7429) acc_u 31.2500 (33.5417) lr 3.0827e-06 eta 0:00:13
epoch [197/200] batch [20/44] time 0.350 (0.467) data 0.217 (0.335) loss_u loss_u 0.8047 (0.7391) acc_u 25.0000 (33.7500) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [25/44] time 0.411 (0.465) data 0.278 (0.333) loss_u loss_u 0.7729 (0.7424) acc_u 28.1250 (33.6250) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [30/44] time 0.298 (0.461) data 0.166 (0.329) loss_u loss_u 0.7173 (0.7468) acc_u 34.3750 (33.0208) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [35/44] time 0.778 (0.463) data 0.646 (0.331) loss_u loss_u 0.7007 (0.7496) acc_u 37.5000 (32.7679) lr 3.0827e-06 eta 0:00:04
epoch [197/200] batch [40/44] time 0.477 (0.465) data 0.344 (0.333) loss_u loss_u 0.7485 (0.7494) acc_u 31.2500 (32.9688) lr 3.0827e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1277
confident_label rate tensor(0.5360, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1681
clean true:1680
clean false:1
clean_rate:0.9994051160023796
noisy true:179
noisy false:1276
after delete: len(clean_dataset) 1681
after delete: len(noisy_dataset) 1455
epoch [198/200] batch [5/52] time 0.660 (0.553) data 0.529 (0.422) loss_x loss_x 1.3115 (1.3002) acc_x 71.8750 (71.8750) lr 1.9733e-06 eta 0:00:25
epoch [198/200] batch [10/52] time 0.386 (0.477) data 0.255 (0.346) loss_x loss_x 1.0195 (1.1454) acc_x 81.2500 (74.0625) lr 1.9733e-06 eta 0:00:20
epoch [198/200] batch [15/52] time 0.408 (0.491) data 0.277 (0.360) loss_x loss_x 1.3779 (1.1329) acc_x 59.3750 (73.7500) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [20/52] time 0.393 (0.475) data 0.261 (0.344) loss_x loss_x 1.2168 (1.1474) acc_x 68.7500 (73.2812) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [25/52] time 0.455 (0.459) data 0.323 (0.328) loss_x loss_x 0.9521 (1.1397) acc_x 81.2500 (73.0000) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [30/52] time 0.440 (0.451) data 0.308 (0.320) loss_x loss_x 1.2920 (1.1573) acc_x 71.8750 (72.6042) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [35/52] time 0.495 (0.451) data 0.363 (0.320) loss_x loss_x 1.2861 (1.1345) acc_x 68.7500 (72.8571) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [40/52] time 0.359 (0.452) data 0.227 (0.321) loss_x loss_x 0.9961 (1.1085) acc_x 75.0000 (73.4375) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [45/52] time 0.453 (0.451) data 0.322 (0.319) loss_x loss_x 1.0332 (1.0823) acc_x 78.1250 (73.8194) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [50/52] time 0.651 (0.456) data 0.520 (0.325) loss_x loss_x 1.0498 (1.0849) acc_x 68.7500 (73.5625) lr 1.9733e-06 eta 0:00:00
epoch [198/200] batch [5/45] time 0.449 (0.454) data 0.318 (0.323) loss_u loss_u 0.7773 (0.7530) acc_u 21.8750 (31.8750) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [10/45] time 0.425 (0.453) data 0.293 (0.321) loss_u loss_u 0.7808 (0.7714) acc_u 31.2500 (31.5625) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [15/45] time 0.510 (0.452) data 0.377 (0.321) loss_u loss_u 0.6709 (0.7532) acc_u 43.7500 (33.3333) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [20/45] time 0.445 (0.448) data 0.313 (0.317) loss_u loss_u 0.7573 (0.7563) acc_u 43.7500 (33.2812) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [25/45] time 0.630 (0.449) data 0.498 (0.318) loss_u loss_u 0.7998 (0.7626) acc_u 25.0000 (31.7500) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [30/45] time 0.411 (0.450) data 0.278 (0.319) loss_u loss_u 0.7183 (0.7558) acc_u 34.3750 (31.9792) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [35/45] time 0.476 (0.448) data 0.344 (0.316) loss_u loss_u 0.6909 (0.7599) acc_u 40.6250 (31.6964) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [40/45] time 0.369 (0.446) data 0.239 (0.314) loss_u loss_u 0.8286 (0.7583) acc_u 12.5000 (31.2500) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [45/45] time 0.446 (0.446) data 0.316 (0.314) loss_u loss_u 0.7505 (0.7530) acc_u 25.0000 (31.7361) lr 1.9733e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1247
confident_label rate tensor(0.5491, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1722
clean true:1720
clean false:2
clean_rate:0.9988385598141696
noisy true:169
noisy false:1245
after delete: len(clean_dataset) 1722
after delete: len(noisy_dataset) 1414
epoch [199/200] batch [5/53] time 0.332 (0.400) data 0.202 (0.269) loss_x loss_x 0.6523 (1.0195) acc_x 87.5000 (78.1250) lr 1.1101e-06 eta 0:00:19
epoch [199/200] batch [10/53] time 0.391 (0.428) data 0.260 (0.297) loss_x loss_x 1.1992 (1.0197) acc_x 68.7500 (75.3125) lr 1.1101e-06 eta 0:00:18
epoch [199/200] batch [15/53] time 0.406 (0.447) data 0.275 (0.316) loss_x loss_x 1.1543 (1.0428) acc_x 75.0000 (74.3750) lr 1.1101e-06 eta 0:00:16
epoch [199/200] batch [20/53] time 0.369 (0.454) data 0.238 (0.323) loss_x loss_x 1.5781 (1.0660) acc_x 50.0000 (73.4375) lr 1.1101e-06 eta 0:00:14
epoch [199/200] batch [25/53] time 0.669 (0.460) data 0.538 (0.329) loss_x loss_x 1.0400 (1.0307) acc_x 68.7500 (73.6250) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [30/53] time 0.410 (0.464) data 0.279 (0.333) loss_x loss_x 1.2373 (1.0155) acc_x 75.0000 (74.1667) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [35/53] time 0.433 (0.471) data 0.302 (0.340) loss_x loss_x 1.2549 (1.0146) acc_x 68.7500 (74.2857) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [40/53] time 0.401 (0.464) data 0.271 (0.333) loss_x loss_x 1.0029 (1.0034) acc_x 68.7500 (74.5312) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [45/53] time 0.395 (0.464) data 0.265 (0.333) loss_x loss_x 1.2109 (1.0269) acc_x 68.7500 (74.4444) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [50/53] time 0.435 (0.466) data 0.305 (0.335) loss_x loss_x 1.0898 (1.0183) acc_x 78.1250 (74.9375) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [5/44] time 0.342 (0.462) data 0.211 (0.331) loss_u loss_u 0.7251 (0.7640) acc_u 31.2500 (27.5000) lr 1.1101e-06 eta 0:00:18
epoch [199/200] batch [10/44] time 0.414 (0.462) data 0.282 (0.331) loss_u loss_u 0.7642 (0.7561) acc_u 31.2500 (29.0625) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [15/44] time 0.491 (0.463) data 0.360 (0.332) loss_u loss_u 0.7280 (0.7412) acc_u 31.2500 (31.4583) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [20/44] time 0.510 (0.460) data 0.378 (0.329) loss_u loss_u 0.6655 (0.7437) acc_u 37.5000 (31.4062) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [25/44] time 0.370 (0.456) data 0.240 (0.325) loss_u loss_u 0.7788 (0.7428) acc_u 25.0000 (31.2500) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [30/44] time 0.374 (0.452) data 0.243 (0.321) loss_u loss_u 0.8945 (0.7550) acc_u 15.6250 (30.5208) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [35/44] time 0.455 (0.454) data 0.325 (0.323) loss_u loss_u 0.7397 (0.7536) acc_u 28.1250 (30.7143) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [40/44] time 0.385 (0.452) data 0.254 (0.321) loss_u loss_u 0.7515 (0.7544) acc_u 28.1250 (30.6250) lr 1.1101e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1252
confident_label rate tensor(0.5430, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1703
clean true:1703
clean false:0
clean_rate:1.0
noisy true:181
noisy false:1252
after delete: len(clean_dataset) 1703
after delete: len(noisy_dataset) 1433
epoch [200/200] batch [5/53] time 0.549 (0.445) data 0.419 (0.315) loss_x loss_x 0.9346 (1.1114) acc_x 65.6250 (68.1250) lr 4.9344e-07 eta 0:00:21
epoch [200/200] batch [10/53] time 0.405 (0.453) data 0.274 (0.323) loss_x loss_x 0.6680 (1.0069) acc_x 78.1250 (71.2500) lr 4.9344e-07 eta 0:00:19
epoch [200/200] batch [15/53] time 0.437 (0.462) data 0.306 (0.331) loss_x loss_x 1.4492 (1.0747) acc_x 65.6250 (70.8333) lr 4.9344e-07 eta 0:00:17
epoch [200/200] batch [20/53] time 0.419 (0.458) data 0.287 (0.327) loss_x loss_x 1.1006 (1.0333) acc_x 71.8750 (73.2812) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [25/53] time 0.344 (0.453) data 0.213 (0.322) loss_x loss_x 0.8501 (1.0451) acc_x 78.1250 (73.1250) lr 4.9344e-07 eta 0:00:12
epoch [200/200] batch [30/53] time 0.418 (0.452) data 0.287 (0.321) loss_x loss_x 1.0625 (1.0079) acc_x 75.0000 (74.1667) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [35/53] time 0.572 (0.455) data 0.440 (0.324) loss_x loss_x 0.7739 (1.0104) acc_x 78.1250 (73.9286) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [40/53] time 0.386 (0.463) data 0.253 (0.332) loss_x loss_x 1.2090 (1.0205) acc_x 71.8750 (73.7500) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [45/53] time 0.377 (0.457) data 0.246 (0.326) loss_x loss_x 1.4180 (1.0612) acc_x 65.6250 (72.9861) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [50/53] time 0.449 (0.454) data 0.318 (0.323) loss_x loss_x 1.4072 (1.0859) acc_x 59.3750 (72.7500) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [5/44] time 0.504 (0.456) data 0.372 (0.325) loss_u loss_u 0.7378 (0.7553) acc_u 31.2500 (30.6250) lr 4.9344e-07 eta 0:00:17
epoch [200/200] batch [10/44] time 0.374 (0.450) data 0.242 (0.319) loss_u loss_u 0.7222 (0.7632) acc_u 37.5000 (28.4375) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [15/44] time 0.550 (0.450) data 0.418 (0.319) loss_u loss_u 0.7505 (0.7566) acc_u 34.3750 (30.2083) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [20/44] time 0.461 (0.448) data 0.329 (0.317) loss_u loss_u 0.6763 (0.7571) acc_u 40.6250 (30.6250) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [25/44] time 0.407 (0.450) data 0.275 (0.318) loss_u loss_u 0.7847 (0.7558) acc_u 34.3750 (31.3750) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [30/44] time 0.468 (0.453) data 0.337 (0.321) loss_u loss_u 0.6807 (0.7560) acc_u 43.7500 (31.5625) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [35/44] time 0.467 (0.452) data 0.336 (0.320) loss_u loss_u 0.8213 (0.7584) acc_u 25.0000 (31.2500) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [40/44] time 0.333 (0.452) data 0.201 (0.320) loss_u loss_u 0.7192 (0.7525) acc_u 40.6250 (31.7969) lr 4.9344e-07 eta 0:00:01
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.125/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 5,731
* accuracy: 71.3%
* error: 28.7%
* macro_f1: 70.4%
Elapsed: 5:04:44
