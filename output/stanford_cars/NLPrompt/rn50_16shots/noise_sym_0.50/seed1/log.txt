***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.5', 'DATASET.NOISE_TYPE', 'sym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.50/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.5
  NOISE_TYPE: sym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.50/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.50/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.0973, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 305
clean true:297
clean false:8
clean_rate:0.9737704918032787
noisy true:268
noisy false:2563
after delete: len(clean_dataset) 305
after delete: len(noisy_dataset) 2831
epoch [1/200] batch [5/9] time 0.446 (0.595) data 0.316 (0.443) loss_x loss_x 2.9082 (2.9152) acc_x 37.5000 (44.3750) lr 1.0000e-05 eta 0:00:02
epoch [1/200] batch [5/88] time 0.334 (0.513) data 0.203 (0.373) loss_u loss_u 0.9888 (0.9803) acc_u 0.0000 (3.7500) lr 1.0000e-05 eta 0:00:42
epoch [1/200] batch [10/88] time 0.479 (0.498) data 0.347 (0.360) loss_u loss_u 0.9766 (0.9695) acc_u 9.3750 (6.8750) lr 1.0000e-05 eta 0:00:38
epoch [1/200] batch [15/88] time 0.596 (0.500) data 0.465 (0.364) loss_u loss_u 0.9565 (0.9674) acc_u 9.3750 (7.5000) lr 1.0000e-05 eta 0:00:36
epoch [1/200] batch [20/88] time 0.460 (0.497) data 0.329 (0.361) loss_u loss_u 0.9536 (0.9661) acc_u 6.2500 (7.3438) lr 1.0000e-05 eta 0:00:33
epoch [1/200] batch [25/88] time 0.659 (0.496) data 0.528 (0.361) loss_u loss_u 0.9390 (0.9631) acc_u 15.6250 (8.0000) lr 1.0000e-05 eta 0:00:31
epoch [1/200] batch [30/88] time 0.477 (0.493) data 0.347 (0.359) loss_u loss_u 0.9800 (0.9646) acc_u 3.1250 (7.5000) lr 1.0000e-05 eta 0:00:28
epoch [1/200] batch [35/88] time 0.467 (0.483) data 0.335 (0.350) loss_u loss_u 0.9795 (0.9654) acc_u 0.0000 (7.0536) lr 1.0000e-05 eta 0:00:25
epoch [1/200] batch [40/88] time 0.434 (0.484) data 0.304 (0.351) loss_u loss_u 0.9658 (0.9653) acc_u 12.5000 (7.1875) lr 1.0000e-05 eta 0:00:23
epoch [1/200] batch [45/88] time 0.436 (0.483) data 0.305 (0.350) loss_u loss_u 0.9517 (0.9634) acc_u 12.5000 (7.5694) lr 1.0000e-05 eta 0:00:20
epoch [1/200] batch [50/88] time 0.387 (0.477) data 0.257 (0.344) loss_u loss_u 0.9790 (0.9627) acc_u 3.1250 (7.5000) lr 1.0000e-05 eta 0:00:18
epoch [1/200] batch [55/88] time 0.518 (0.475) data 0.386 (0.342) loss_u loss_u 0.9863 (0.9622) acc_u 6.2500 (7.5568) lr 1.0000e-05 eta 0:00:15
epoch [1/200] batch [60/88] time 0.516 (0.476) data 0.385 (0.343) loss_u loss_u 0.9473 (0.9607) acc_u 6.2500 (7.9167) lr 1.0000e-05 eta 0:00:13
epoch [1/200] batch [65/88] time 0.453 (0.479) data 0.322 (0.347) loss_u loss_u 0.9785 (0.9614) acc_u 0.0000 (7.5000) lr 1.0000e-05 eta 0:00:11
epoch [1/200] batch [70/88] time 0.637 (0.479) data 0.506 (0.346) loss_u loss_u 0.9492 (0.9613) acc_u 12.5000 (7.6339) lr 1.0000e-05 eta 0:00:08
epoch [1/200] batch [75/88] time 0.556 (0.485) data 0.425 (0.352) loss_u loss_u 0.9795 (0.9614) acc_u 3.1250 (7.6667) lr 1.0000e-05 eta 0:00:06
epoch [1/200] batch [80/88] time 0.519 (0.483) data 0.388 (0.350) loss_u loss_u 0.9775 (0.9613) acc_u 6.2500 (7.6562) lr 1.0000e-05 eta 0:00:03
epoch [1/200] batch [85/88] time 0.487 (0.482) data 0.356 (0.349) loss_u loss_u 0.9531 (0.9605) acc_u 9.3750 (7.7574) lr 1.0000e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2283
confident_label rate tensor(0.1346, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 422
clean true:418
clean false:4
clean_rate:0.990521327014218
noisy true:435
noisy false:2279
after delete: len(clean_dataset) 422
after delete: len(noisy_dataset) 2714
epoch [2/200] batch [5/13] time 0.355 (0.490) data 0.225 (0.348) loss_x loss_x 1.3672 (1.6861) acc_x 68.7500 (59.3750) lr 2.0000e-03 eta 0:00:03
epoch [2/200] batch [10/13] time 0.471 (0.487) data 0.340 (0.350) loss_x loss_x 1.9502 (1.8200) acc_x 50.0000 (55.0000) lr 2.0000e-03 eta 0:00:01
epoch [2/200] batch [5/84] time 0.385 (0.494) data 0.252 (0.359) loss_u loss_u 0.8838 (0.8943) acc_u 21.8750 (20.0000) lr 2.0000e-03 eta 0:00:39
epoch [2/200] batch [10/84] time 0.394 (0.475) data 0.263 (0.341) loss_u loss_u 0.9297 (0.8983) acc_u 12.5000 (17.5000) lr 2.0000e-03 eta 0:00:35
epoch [2/200] batch [15/84] time 0.334 (0.467) data 0.203 (0.334) loss_u loss_u 0.8638 (0.9004) acc_u 28.1250 (16.6667) lr 2.0000e-03 eta 0:00:32
epoch [2/200] batch [20/84] time 0.416 (0.462) data 0.285 (0.329) loss_u loss_u 0.8945 (0.8950) acc_u 15.6250 (16.7188) lr 2.0000e-03 eta 0:00:29
epoch [2/200] batch [25/84] time 0.433 (0.458) data 0.303 (0.325) loss_u loss_u 0.9253 (0.8996) acc_u 12.5000 (15.3750) lr 2.0000e-03 eta 0:00:27
epoch [2/200] batch [30/84] time 0.422 (0.453) data 0.292 (0.320) loss_u loss_u 0.9033 (0.8999) acc_u 12.5000 (15.0000) lr 2.0000e-03 eta 0:00:24
epoch [2/200] batch [35/84] time 0.341 (0.450) data 0.210 (0.317) loss_u loss_u 0.9121 (0.8954) acc_u 12.5000 (15.1786) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [40/84] time 0.402 (0.443) data 0.270 (0.311) loss_u loss_u 0.8906 (0.8964) acc_u 18.7500 (15.2344) lr 2.0000e-03 eta 0:00:19
epoch [2/200] batch [45/84] time 0.424 (0.442) data 0.292 (0.309) loss_u loss_u 0.8813 (0.8973) acc_u 18.7500 (15.2083) lr 2.0000e-03 eta 0:00:17
epoch [2/200] batch [50/84] time 0.784 (0.448) data 0.652 (0.316) loss_u loss_u 0.8940 (0.8954) acc_u 18.7500 (15.3125) lr 2.0000e-03 eta 0:00:15
epoch [2/200] batch [55/84] time 0.518 (0.453) data 0.387 (0.321) loss_u loss_u 0.9146 (0.8944) acc_u 6.2500 (15.1705) lr 2.0000e-03 eta 0:00:13
epoch [2/200] batch [60/84] time 0.498 (0.454) data 0.365 (0.322) loss_u loss_u 0.9111 (0.8972) acc_u 12.5000 (14.6875) lr 2.0000e-03 eta 0:00:10
epoch [2/200] batch [65/84] time 0.460 (0.455) data 0.328 (0.323) loss_u loss_u 0.8643 (0.8967) acc_u 15.6250 (14.6154) lr 2.0000e-03 eta 0:00:08
epoch [2/200] batch [70/84] time 0.445 (0.456) data 0.314 (0.324) loss_u loss_u 0.8667 (0.8970) acc_u 18.7500 (14.6429) lr 2.0000e-03 eta 0:00:06
epoch [2/200] batch [75/84] time 0.438 (0.455) data 0.307 (0.323) loss_u loss_u 0.8501 (0.8978) acc_u 28.1250 (14.6667) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [80/84] time 0.468 (0.456) data 0.337 (0.324) loss_u loss_u 0.8745 (0.8975) acc_u 18.7500 (14.7656) lr 2.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1867
confident_label rate tensor(0.2085, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 654
clean true:647
clean false:7
clean_rate:0.9892966360856269
noisy true:622
noisy false:1860
after delete: len(clean_dataset) 654
after delete: len(noisy_dataset) 2482
epoch [3/200] batch [5/20] time 0.589 (0.455) data 0.457 (0.324) loss_x loss_x 2.0254 (1.5537) acc_x 50.0000 (58.1250) lr 1.9999e-03 eta 0:00:06
epoch [3/200] batch [10/20] time 0.476 (0.465) data 0.344 (0.334) loss_x loss_x 2.0801 (1.6498) acc_x 40.6250 (55.6250) lr 1.9999e-03 eta 0:00:04
epoch [3/200] batch [15/20] time 0.525 (0.494) data 0.395 (0.363) loss_x loss_x 1.4014 (1.6186) acc_x 65.6250 (57.5000) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [20/20] time 0.464 (0.482) data 0.334 (0.351) loss_x loss_x 1.1875 (1.6487) acc_x 71.8750 (57.1875) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/77] time 0.443 (0.468) data 0.311 (0.337) loss_u loss_u 0.8960 (0.9136) acc_u 15.6250 (11.8750) lr 1.9999e-03 eta 0:00:33
epoch [3/200] batch [10/77] time 0.442 (0.464) data 0.310 (0.333) loss_u loss_u 0.9688 (0.9266) acc_u 6.2500 (9.6875) lr 1.9999e-03 eta 0:00:31
epoch [3/200] batch [15/77] time 0.425 (0.465) data 0.293 (0.334) loss_u loss_u 0.8857 (0.9331) acc_u 12.5000 (8.7500) lr 1.9999e-03 eta 0:00:28
epoch [3/200] batch [20/77] time 0.658 (0.465) data 0.526 (0.334) loss_u loss_u 0.9248 (0.9342) acc_u 12.5000 (8.4375) lr 1.9999e-03 eta 0:00:26
epoch [3/200] batch [25/77] time 0.394 (0.467) data 0.263 (0.336) loss_u loss_u 0.9058 (0.9320) acc_u 18.7500 (9.3750) lr 1.9999e-03 eta 0:00:24
epoch [3/200] batch [30/77] time 0.428 (0.473) data 0.296 (0.342) loss_u loss_u 0.9756 (0.9350) acc_u 3.1250 (8.9583) lr 1.9999e-03 eta 0:00:22
epoch [3/200] batch [35/77] time 0.496 (0.468) data 0.366 (0.337) loss_u loss_u 0.9531 (0.9330) acc_u 6.2500 (9.2857) lr 1.9999e-03 eta 0:00:19
epoch [3/200] batch [40/77] time 0.336 (0.465) data 0.206 (0.334) loss_u loss_u 0.9678 (0.9320) acc_u 0.0000 (9.4531) lr 1.9999e-03 eta 0:00:17
epoch [3/200] batch [45/77] time 0.407 (0.463) data 0.276 (0.331) loss_u loss_u 0.8516 (0.9301) acc_u 12.5000 (9.3750) lr 1.9999e-03 eta 0:00:14
epoch [3/200] batch [50/77] time 0.354 (0.457) data 0.223 (0.326) loss_u loss_u 0.8926 (0.9276) acc_u 12.5000 (9.6875) lr 1.9999e-03 eta 0:00:12
epoch [3/200] batch [55/77] time 0.367 (0.460) data 0.235 (0.328) loss_u loss_u 0.9751 (0.9276) acc_u 3.1250 (9.7159) lr 1.9999e-03 eta 0:00:10
epoch [3/200] batch [60/77] time 0.676 (0.463) data 0.546 (0.331) loss_u loss_u 0.8950 (0.9282) acc_u 12.5000 (9.6354) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [65/77] time 0.372 (0.464) data 0.241 (0.332) loss_u loss_u 0.8970 (0.9254) acc_u 18.7500 (10.0481) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [70/77] time 0.556 (0.467) data 0.425 (0.336) loss_u loss_u 0.9438 (0.9265) acc_u 6.2500 (9.9107) lr 1.9999e-03 eta 0:00:03
epoch [3/200] batch [75/77] time 0.356 (0.465) data 0.225 (0.334) loss_u loss_u 0.8569 (0.9250) acc_u 18.7500 (10.0833) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1855
confident_label rate tensor(0.2152, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 675
clean true:668
clean false:7
clean_rate:0.9896296296296296
noisy true:613
noisy false:1848
after delete: len(clean_dataset) 675
after delete: len(noisy_dataset) 2461
epoch [4/200] batch [5/21] time 0.386 (0.495) data 0.256 (0.364) loss_x loss_x 2.0684 (1.7146) acc_x 50.0000 (53.1250) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [10/21] time 0.417 (0.463) data 0.287 (0.332) loss_x loss_x 1.2930 (1.5878) acc_x 71.8750 (58.7500) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [15/21] time 0.421 (0.455) data 0.290 (0.324) loss_x loss_x 1.7666 (1.6162) acc_x 62.5000 (56.2500) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [20/21] time 0.413 (0.455) data 0.282 (0.325) loss_x loss_x 1.5898 (1.6129) acc_x 62.5000 (56.8750) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/76] time 0.549 (0.458) data 0.416 (0.327) loss_u loss_u 0.9229 (0.9420) acc_u 6.2500 (6.8750) lr 1.9995e-03 eta 0:00:32
epoch [4/200] batch [10/76] time 0.454 (0.454) data 0.323 (0.323) loss_u loss_u 0.9131 (0.9243) acc_u 9.3750 (9.0625) lr 1.9995e-03 eta 0:00:29
epoch [4/200] batch [15/76] time 0.572 (0.460) data 0.441 (0.329) loss_u loss_u 0.9385 (0.9174) acc_u 6.2500 (10.0000) lr 1.9995e-03 eta 0:00:28
epoch [4/200] batch [20/76] time 0.559 (0.471) data 0.427 (0.340) loss_u loss_u 0.9287 (0.9189) acc_u 6.2500 (9.6875) lr 1.9995e-03 eta 0:00:26
epoch [4/200] batch [25/76] time 0.415 (0.469) data 0.284 (0.338) loss_u loss_u 0.8989 (0.9160) acc_u 15.6250 (10.3750) lr 1.9995e-03 eta 0:00:23
epoch [4/200] batch [30/76] time 0.451 (0.470) data 0.319 (0.339) loss_u loss_u 0.9517 (0.9149) acc_u 3.1250 (10.4167) lr 1.9995e-03 eta 0:00:21
epoch [4/200] batch [35/76] time 0.461 (0.467) data 0.331 (0.336) loss_u loss_u 0.9106 (0.9177) acc_u 6.2500 (9.9107) lr 1.9995e-03 eta 0:00:19
epoch [4/200] batch [40/76] time 0.347 (0.461) data 0.216 (0.330) loss_u loss_u 0.9126 (0.9188) acc_u 18.7500 (10.0000) lr 1.9995e-03 eta 0:00:16
epoch [4/200] batch [45/76] time 0.436 (0.455) data 0.305 (0.324) loss_u loss_u 0.8896 (0.9192) acc_u 12.5000 (10.0000) lr 1.9995e-03 eta 0:00:14
epoch [4/200] batch [50/76] time 0.462 (0.454) data 0.331 (0.323) loss_u loss_u 0.9565 (0.9211) acc_u 6.2500 (9.7500) lr 1.9995e-03 eta 0:00:11
epoch [4/200] batch [55/76] time 0.371 (0.452) data 0.240 (0.321) loss_u loss_u 0.9268 (0.9185) acc_u 9.3750 (9.9432) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [60/76] time 0.432 (0.453) data 0.301 (0.322) loss_u loss_u 0.9175 (0.9191) acc_u 12.5000 (10.0521) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [65/76] time 0.416 (0.455) data 0.285 (0.324) loss_u loss_u 0.9531 (0.9206) acc_u 9.3750 (9.8077) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [70/76] time 0.552 (0.456) data 0.422 (0.325) loss_u loss_u 0.9082 (0.9186) acc_u 9.3750 (10.1786) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [75/76] time 0.440 (0.457) data 0.309 (0.326) loss_u loss_u 0.9233 (0.9197) acc_u 6.2500 (9.9583) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1849
confident_label rate tensor(0.2089, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 655
clean true:651
clean false:4
clean_rate:0.9938931297709923
noisy true:636
noisy false:1845
after delete: len(clean_dataset) 655
after delete: len(noisy_dataset) 2481
epoch [5/200] batch [5/20] time 0.513 (0.464) data 0.383 (0.333) loss_x loss_x 1.5732 (1.5055) acc_x 50.0000 (60.0000) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [10/20] time 0.384 (0.477) data 0.254 (0.347) loss_x loss_x 1.5273 (1.5299) acc_x 62.5000 (62.1875) lr 1.9989e-03 eta 0:00:04
epoch [5/200] batch [15/20] time 0.440 (0.465) data 0.309 (0.334) loss_x loss_x 1.6553 (1.5685) acc_x 65.6250 (61.4583) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [20/20] time 0.579 (0.479) data 0.448 (0.349) loss_x loss_x 1.3340 (1.5761) acc_x 53.1250 (60.1562) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/77] time 0.414 (0.467) data 0.283 (0.336) loss_u loss_u 0.9126 (0.9059) acc_u 9.3750 (12.5000) lr 1.9989e-03 eta 0:00:33
epoch [5/200] batch [10/77] time 0.324 (0.451) data 0.194 (0.321) loss_u loss_u 0.9292 (0.9132) acc_u 9.3750 (11.5625) lr 1.9989e-03 eta 0:00:30
epoch [5/200] batch [15/77] time 0.426 (0.446) data 0.295 (0.316) loss_u loss_u 0.9448 (0.9234) acc_u 9.3750 (10.0000) lr 1.9989e-03 eta 0:00:27
epoch [5/200] batch [20/77] time 0.405 (0.445) data 0.276 (0.314) loss_u loss_u 0.9443 (0.9212) acc_u 12.5000 (10.4688) lr 1.9989e-03 eta 0:00:25
epoch [5/200] batch [25/77] time 0.367 (0.443) data 0.236 (0.313) loss_u loss_u 0.9600 (0.9244) acc_u 3.1250 (9.8750) lr 1.9989e-03 eta 0:00:23
epoch [5/200] batch [30/77] time 0.586 (0.443) data 0.455 (0.313) loss_u loss_u 0.8960 (0.9234) acc_u 12.5000 (10.0000) lr 1.9989e-03 eta 0:00:20
epoch [5/200] batch [35/77] time 0.387 (0.445) data 0.256 (0.315) loss_u loss_u 0.9087 (0.9241) acc_u 15.6250 (10.1786) lr 1.9989e-03 eta 0:00:18
epoch [5/200] batch [40/77] time 0.428 (0.442) data 0.297 (0.312) loss_u loss_u 0.8774 (0.9235) acc_u 21.8750 (10.1562) lr 1.9989e-03 eta 0:00:16
epoch [5/200] batch [45/77] time 0.540 (0.443) data 0.409 (0.312) loss_u loss_u 0.9185 (0.9221) acc_u 9.3750 (10.4167) lr 1.9989e-03 eta 0:00:14
epoch [5/200] batch [50/77] time 0.391 (0.439) data 0.258 (0.308) loss_u loss_u 0.9604 (0.9229) acc_u 3.1250 (10.3125) lr 1.9989e-03 eta 0:00:11
epoch [5/200] batch [55/77] time 0.405 (0.442) data 0.273 (0.311) loss_u loss_u 0.9106 (0.9221) acc_u 12.5000 (10.5682) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [60/77] time 0.306 (0.439) data 0.174 (0.308) loss_u loss_u 0.9482 (0.9227) acc_u 6.2500 (10.5208) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [65/77] time 0.399 (0.441) data 0.268 (0.310) loss_u loss_u 0.8945 (0.9213) acc_u 15.6250 (10.7212) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [70/77] time 0.431 (0.443) data 0.299 (0.312) loss_u loss_u 0.9131 (0.9201) acc_u 12.5000 (10.8929) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [75/77] time 0.326 (0.444) data 0.194 (0.313) loss_u loss_u 0.9541 (0.9203) acc_u 0.0000 (10.8333) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1839
confident_label rate tensor(0.2127, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 667
clean true:662
clean false:5
clean_rate:0.992503748125937
noisy true:635
noisy false:1834
after delete: len(clean_dataset) 667
after delete: len(noisy_dataset) 2469
epoch [6/200] batch [5/20] time 0.457 (0.444) data 0.326 (0.313) loss_x loss_x 1.7178 (1.6807) acc_x 59.3750 (57.5000) lr 1.9980e-03 eta 0:00:06
epoch [6/200] batch [10/20] time 0.500 (0.445) data 0.370 (0.314) loss_x loss_x 1.7129 (1.6187) acc_x 40.6250 (57.1875) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [15/20] time 0.448 (0.445) data 0.318 (0.315) loss_x loss_x 1.4268 (1.6181) acc_x 62.5000 (57.5000) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [20/20] time 0.587 (0.462) data 0.457 (0.332) loss_x loss_x 1.9561 (1.6522) acc_x 59.3750 (56.2500) lr 1.9980e-03 eta 0:00:00
epoch [6/200] batch [5/77] time 0.507 (0.471) data 0.376 (0.340) loss_u loss_u 0.9502 (0.9325) acc_u 9.3750 (8.7500) lr 1.9980e-03 eta 0:00:33
epoch [6/200] batch [10/77] time 0.607 (0.465) data 0.477 (0.334) loss_u loss_u 0.9282 (0.9240) acc_u 9.3750 (9.6875) lr 1.9980e-03 eta 0:00:31
epoch [6/200] batch [15/77] time 0.426 (0.455) data 0.296 (0.324) loss_u loss_u 0.9180 (0.9238) acc_u 9.3750 (9.7917) lr 1.9980e-03 eta 0:00:28
epoch [6/200] batch [20/77] time 0.579 (0.460) data 0.446 (0.329) loss_u loss_u 0.9214 (0.9243) acc_u 15.6250 (10.1562) lr 1.9980e-03 eta 0:00:26
epoch [6/200] batch [25/77] time 0.487 (0.458) data 0.356 (0.327) loss_u loss_u 0.8862 (0.9216) acc_u 15.6250 (10.8750) lr 1.9980e-03 eta 0:00:23
epoch [6/200] batch [30/77] time 0.521 (0.456) data 0.390 (0.326) loss_u loss_u 0.9214 (0.9168) acc_u 12.5000 (11.6667) lr 1.9980e-03 eta 0:00:21
epoch [6/200] batch [35/77] time 0.431 (0.454) data 0.299 (0.323) loss_u loss_u 0.9346 (0.9197) acc_u 9.3750 (11.4286) lr 1.9980e-03 eta 0:00:19
epoch [6/200] batch [40/77] time 0.332 (0.449) data 0.200 (0.318) loss_u loss_u 0.9399 (0.9213) acc_u 9.3750 (11.1719) lr 1.9980e-03 eta 0:00:16
epoch [6/200] batch [45/77] time 0.498 (0.447) data 0.367 (0.316) loss_u loss_u 0.8965 (0.9191) acc_u 12.5000 (11.2500) lr 1.9980e-03 eta 0:00:14
epoch [6/200] batch [50/77] time 0.438 (0.447) data 0.307 (0.316) loss_u loss_u 0.9141 (0.9212) acc_u 12.5000 (10.9375) lr 1.9980e-03 eta 0:00:12
epoch [6/200] batch [55/77] time 0.569 (0.447) data 0.438 (0.316) loss_u loss_u 0.8135 (0.9204) acc_u 21.8750 (10.9091) lr 1.9980e-03 eta 0:00:09
epoch [6/200] batch [60/77] time 0.658 (0.451) data 0.527 (0.320) loss_u loss_u 0.9531 (0.9217) acc_u 9.3750 (10.8854) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [65/77] time 0.468 (0.448) data 0.337 (0.317) loss_u loss_u 0.9033 (0.9217) acc_u 9.3750 (10.7212) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [70/77] time 0.335 (0.448) data 0.203 (0.317) loss_u loss_u 0.9458 (0.9214) acc_u 6.2500 (10.8482) lr 1.9980e-03 eta 0:00:03
epoch [6/200] batch [75/77] time 0.412 (0.447) data 0.280 (0.316) loss_u loss_u 0.9053 (0.9218) acc_u 12.5000 (10.7083) lr 1.9980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1869
confident_label rate tensor(0.2111, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 662
clean true:657
clean false:5
clean_rate:0.9924471299093656
noisy true:610
noisy false:1864
after delete: len(clean_dataset) 662
after delete: len(noisy_dataset) 2474
epoch [7/200] batch [5/20] time 0.482 (0.523) data 0.351 (0.393) loss_x loss_x 1.1172 (1.4867) acc_x 59.3750 (61.2500) lr 1.9969e-03 eta 0:00:07
epoch [7/200] batch [10/20] time 0.405 (0.482) data 0.274 (0.351) loss_x loss_x 1.0498 (1.3984) acc_x 71.8750 (63.1250) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [15/20] time 0.630 (0.481) data 0.499 (0.350) loss_x loss_x 2.1211 (1.5357) acc_x 56.2500 (61.8750) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [20/20] time 0.493 (0.481) data 0.363 (0.350) loss_x loss_x 1.6836 (1.5802) acc_x 50.0000 (59.5312) lr 1.9969e-03 eta 0:00:00
epoch [7/200] batch [5/77] time 0.443 (0.466) data 0.313 (0.335) loss_u loss_u 0.9209 (0.9303) acc_u 9.3750 (8.1250) lr 1.9969e-03 eta 0:00:33
epoch [7/200] batch [10/77] time 0.366 (0.454) data 0.235 (0.323) loss_u loss_u 0.9575 (0.9375) acc_u 3.1250 (7.1875) lr 1.9969e-03 eta 0:00:30
epoch [7/200] batch [15/77] time 0.431 (0.454) data 0.300 (0.323) loss_u loss_u 0.9121 (0.9342) acc_u 6.2500 (7.0833) lr 1.9969e-03 eta 0:00:28
epoch [7/200] batch [20/77] time 0.401 (0.455) data 0.271 (0.324) loss_u loss_u 0.9468 (0.9279) acc_u 3.1250 (8.4375) lr 1.9969e-03 eta 0:00:25
epoch [7/200] batch [25/77] time 0.392 (0.458) data 0.260 (0.327) loss_u loss_u 0.9502 (0.9239) acc_u 6.2500 (9.2500) lr 1.9969e-03 eta 0:00:23
epoch [7/200] batch [30/77] time 0.522 (0.458) data 0.391 (0.327) loss_u loss_u 0.9014 (0.9232) acc_u 15.6250 (9.6875) lr 1.9969e-03 eta 0:00:21
epoch [7/200] batch [35/77] time 0.473 (0.459) data 0.343 (0.328) loss_u loss_u 0.9546 (0.9261) acc_u 6.2500 (9.3750) lr 1.9969e-03 eta 0:00:19
epoch [7/200] batch [40/77] time 0.471 (0.458) data 0.340 (0.327) loss_u loss_u 0.8911 (0.9237) acc_u 12.5000 (9.6094) lr 1.9969e-03 eta 0:00:16
epoch [7/200] batch [45/77] time 0.485 (0.459) data 0.353 (0.328) loss_u loss_u 0.8193 (0.9193) acc_u 25.0000 (10.0000) lr 1.9969e-03 eta 0:00:14
epoch [7/200] batch [50/77] time 0.479 (0.458) data 0.348 (0.327) loss_u loss_u 0.9131 (0.9211) acc_u 15.6250 (9.7500) lr 1.9969e-03 eta 0:00:12
epoch [7/200] batch [55/77] time 0.510 (0.459) data 0.379 (0.328) loss_u loss_u 0.9048 (0.9197) acc_u 12.5000 (9.9432) lr 1.9969e-03 eta 0:00:10
epoch [7/200] batch [60/77] time 0.379 (0.461) data 0.247 (0.330) loss_u loss_u 0.9038 (0.9218) acc_u 15.6250 (9.7396) lr 1.9969e-03 eta 0:00:07
epoch [7/200] batch [65/77] time 0.404 (0.459) data 0.274 (0.328) loss_u loss_u 0.9189 (0.9218) acc_u 9.3750 (9.6635) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [70/77] time 0.441 (0.457) data 0.310 (0.326) loss_u loss_u 0.9199 (0.9208) acc_u 12.5000 (9.9107) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [75/77] time 0.543 (0.457) data 0.412 (0.326) loss_u loss_u 0.9463 (0.9216) acc_u 9.3750 (9.8333) lr 1.9969e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1817
confident_label rate tensor(0.2223, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 697
clean true:690
clean false:7
clean_rate:0.9899569583931134
noisy true:629
noisy false:1810
after delete: len(clean_dataset) 697
after delete: len(noisy_dataset) 2439
epoch [8/200] batch [5/21] time 0.501 (0.499) data 0.371 (0.369) loss_x loss_x 1.9062 (1.7482) acc_x 46.8750 (55.0000) lr 1.9956e-03 eta 0:00:07
epoch [8/200] batch [10/21] time 0.438 (0.464) data 0.308 (0.334) loss_x loss_x 1.5859 (1.7133) acc_x 65.6250 (56.5625) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [15/21] time 0.468 (0.471) data 0.338 (0.340) loss_x loss_x 1.4414 (1.6555) acc_x 65.6250 (56.6667) lr 1.9956e-03 eta 0:00:02
epoch [8/200] batch [20/21] time 0.689 (0.485) data 0.559 (0.355) loss_x loss_x 1.7588 (1.6276) acc_x 56.2500 (57.1875) lr 1.9956e-03 eta 0:00:00
epoch [8/200] batch [5/76] time 0.567 (0.476) data 0.435 (0.346) loss_u loss_u 0.8613 (0.9082) acc_u 15.6250 (11.2500) lr 1.9956e-03 eta 0:00:33
epoch [8/200] batch [10/76] time 0.453 (0.466) data 0.321 (0.335) loss_u loss_u 0.8931 (0.9107) acc_u 12.5000 (10.0000) lr 1.9956e-03 eta 0:00:30
epoch [8/200] batch [15/76] time 0.452 (0.470) data 0.321 (0.339) loss_u loss_u 0.9448 (0.9135) acc_u 9.3750 (10.2083) lr 1.9956e-03 eta 0:00:28
epoch [8/200] batch [20/76] time 0.406 (0.470) data 0.273 (0.339) loss_u loss_u 0.9219 (0.9139) acc_u 12.5000 (10.7812) lr 1.9956e-03 eta 0:00:26
epoch [8/200] batch [25/76] time 0.431 (0.470) data 0.301 (0.339) loss_u loss_u 0.8374 (0.9133) acc_u 18.7500 (10.6250) lr 1.9956e-03 eta 0:00:23
epoch [8/200] batch [30/76] time 0.493 (0.474) data 0.362 (0.343) loss_u loss_u 0.9160 (0.9192) acc_u 6.2500 (9.6875) lr 1.9956e-03 eta 0:00:21
epoch [8/200] batch [35/76] time 0.510 (0.473) data 0.379 (0.342) loss_u loss_u 0.9365 (0.9187) acc_u 9.3750 (10.3571) lr 1.9956e-03 eta 0:00:19
epoch [8/200] batch [40/76] time 0.390 (0.471) data 0.258 (0.340) loss_u loss_u 0.9678 (0.9216) acc_u 6.2500 (9.9219) lr 1.9956e-03 eta 0:00:16
epoch [8/200] batch [45/76] time 0.362 (0.467) data 0.232 (0.336) loss_u loss_u 0.9551 (0.9204) acc_u 6.2500 (10.2778) lr 1.9956e-03 eta 0:00:14
epoch [8/200] batch [50/76] time 0.352 (0.467) data 0.220 (0.336) loss_u loss_u 0.9126 (0.9198) acc_u 12.5000 (10.6250) lr 1.9956e-03 eta 0:00:12
epoch [8/200] batch [55/76] time 0.419 (0.463) data 0.289 (0.332) loss_u loss_u 0.9648 (0.9215) acc_u 6.2500 (10.3409) lr 1.9956e-03 eta 0:00:09
epoch [8/200] batch [60/76] time 0.339 (0.460) data 0.208 (0.329) loss_u loss_u 0.9399 (0.9212) acc_u 3.1250 (10.4167) lr 1.9956e-03 eta 0:00:07
epoch [8/200] batch [65/76] time 0.471 (0.457) data 0.338 (0.326) loss_u loss_u 0.9141 (0.9187) acc_u 12.5000 (10.7692) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [70/76] time 0.363 (0.456) data 0.232 (0.325) loss_u loss_u 0.8911 (0.9181) acc_u 12.5000 (10.7143) lr 1.9956e-03 eta 0:00:02
epoch [8/200] batch [75/76] time 0.442 (0.455) data 0.311 (0.324) loss_u loss_u 0.9438 (0.9175) acc_u 9.3750 (10.7500) lr 1.9956e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1832
confident_label rate tensor(0.2178, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 683
clean true:673
clean false:10
clean_rate:0.9853587115666179
noisy true:631
noisy false:1822
after delete: len(clean_dataset) 683
after delete: len(noisy_dataset) 2453
epoch [9/200] batch [5/21] time 0.487 (0.502) data 0.357 (0.372) loss_x loss_x 1.6543 (1.3926) acc_x 62.5000 (66.8750) lr 1.9940e-03 eta 0:00:08
epoch [9/200] batch [10/21] time 0.431 (0.486) data 0.300 (0.356) loss_x loss_x 1.9971 (1.5220) acc_x 53.1250 (64.0625) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [15/21] time 0.492 (0.484) data 0.362 (0.353) loss_x loss_x 1.5137 (1.5426) acc_x 59.3750 (61.8750) lr 1.9940e-03 eta 0:00:02
epoch [9/200] batch [20/21] time 0.423 (0.472) data 0.293 (0.342) loss_x loss_x 1.7451 (1.5801) acc_x 50.0000 (60.1562) lr 1.9940e-03 eta 0:00:00
epoch [9/200] batch [5/76] time 0.515 (0.470) data 0.384 (0.339) loss_u loss_u 0.9058 (0.9246) acc_u 15.6250 (13.1250) lr 1.9940e-03 eta 0:00:33
epoch [9/200] batch [10/76] time 0.369 (0.469) data 0.239 (0.338) loss_u loss_u 0.9165 (0.9226) acc_u 12.5000 (13.1250) lr 1.9940e-03 eta 0:00:30
epoch [9/200] batch [15/76] time 0.447 (0.461) data 0.316 (0.331) loss_u loss_u 0.9414 (0.9216) acc_u 6.2500 (12.2917) lr 1.9940e-03 eta 0:00:28
epoch [9/200] batch [20/76] time 0.424 (0.453) data 0.292 (0.323) loss_u loss_u 0.9668 (0.9246) acc_u 3.1250 (11.4062) lr 1.9940e-03 eta 0:00:25
epoch [9/200] batch [25/76] time 0.497 (0.453) data 0.365 (0.322) loss_u loss_u 0.9224 (0.9199) acc_u 6.2500 (12.3750) lr 1.9940e-03 eta 0:00:23
epoch [9/200] batch [30/76] time 0.489 (0.456) data 0.358 (0.325) loss_u loss_u 0.8477 (0.9175) acc_u 18.7500 (12.1875) lr 1.9940e-03 eta 0:00:20
epoch [9/200] batch [35/76] time 0.449 (0.453) data 0.318 (0.322) loss_u loss_u 0.9561 (0.9142) acc_u 6.2500 (12.5000) lr 1.9940e-03 eta 0:00:18
epoch [9/200] batch [40/76] time 0.616 (0.453) data 0.485 (0.322) loss_u loss_u 0.9131 (0.9132) acc_u 15.6250 (12.6562) lr 1.9940e-03 eta 0:00:16
epoch [9/200] batch [45/76] time 0.624 (0.453) data 0.494 (0.322) loss_u loss_u 0.9116 (0.9156) acc_u 12.5000 (12.2222) lr 1.9940e-03 eta 0:00:14
epoch [9/200] batch [50/76] time 0.508 (0.457) data 0.377 (0.326) loss_u loss_u 0.8770 (0.9153) acc_u 18.7500 (12.1250) lr 1.9940e-03 eta 0:00:11
epoch [9/200] batch [55/76] time 0.495 (0.457) data 0.364 (0.327) loss_u loss_u 0.9121 (0.9157) acc_u 9.3750 (12.0455) lr 1.9940e-03 eta 0:00:09
epoch [9/200] batch [60/76] time 0.651 (0.460) data 0.521 (0.329) loss_u loss_u 0.9033 (0.9155) acc_u 9.3750 (11.8750) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [65/76] time 0.397 (0.456) data 0.267 (0.326) loss_u loss_u 0.9409 (0.9159) acc_u 9.3750 (11.7308) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [70/76] time 0.379 (0.456) data 0.248 (0.325) loss_u loss_u 0.8896 (0.9161) acc_u 15.6250 (11.6518) lr 1.9940e-03 eta 0:00:02
epoch [9/200] batch [75/76] time 0.391 (0.455) data 0.260 (0.325) loss_u loss_u 0.9331 (0.9186) acc_u 9.3750 (11.2500) lr 1.9940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1785
confident_label rate tensor(0.2312, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 725
clean true:719
clean false:6
clean_rate:0.9917241379310345
noisy true:632
noisy false:1779
after delete: len(clean_dataset) 725
after delete: len(noisy_dataset) 2411
epoch [10/200] batch [5/22] time 0.437 (0.492) data 0.306 (0.362) loss_x loss_x 1.1904 (1.5328) acc_x 75.0000 (65.6250) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [10/22] time 0.509 (0.459) data 0.377 (0.329) loss_x loss_x 1.6729 (1.5324) acc_x 56.2500 (62.5000) lr 1.9921e-03 eta 0:00:05
epoch [10/200] batch [15/22] time 0.383 (0.468) data 0.252 (0.337) loss_x loss_x 1.8896 (1.5009) acc_x 50.0000 (61.4583) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [20/22] time 0.573 (0.476) data 0.442 (0.346) loss_x loss_x 1.4307 (1.5092) acc_x 50.0000 (60.7812) lr 1.9921e-03 eta 0:00:00
epoch [10/200] batch [5/75] time 0.429 (0.467) data 0.298 (0.337) loss_u loss_u 0.8550 (0.9149) acc_u 21.8750 (11.2500) lr 1.9921e-03 eta 0:00:32
epoch [10/200] batch [10/75] time 0.627 (0.464) data 0.495 (0.333) loss_u loss_u 0.9136 (0.9104) acc_u 12.5000 (11.8750) lr 1.9921e-03 eta 0:00:30
epoch [10/200] batch [15/75] time 0.396 (0.457) data 0.266 (0.327) loss_u loss_u 0.9102 (0.9179) acc_u 15.6250 (10.4167) lr 1.9921e-03 eta 0:00:27
epoch [10/200] batch [20/75] time 0.412 (0.456) data 0.278 (0.325) loss_u loss_u 0.8926 (0.9153) acc_u 12.5000 (10.7812) lr 1.9921e-03 eta 0:00:25
epoch [10/200] batch [25/75] time 0.378 (0.456) data 0.247 (0.325) loss_u loss_u 0.8877 (0.9149) acc_u 21.8750 (11.1250) lr 1.9921e-03 eta 0:00:22
epoch [10/200] batch [30/75] time 0.663 (0.460) data 0.532 (0.329) loss_u loss_u 0.8940 (0.9109) acc_u 12.5000 (11.6667) lr 1.9921e-03 eta 0:00:20
epoch [10/200] batch [35/75] time 0.374 (0.460) data 0.244 (0.329) loss_u loss_u 0.9341 (0.9120) acc_u 6.2500 (11.6071) lr 1.9921e-03 eta 0:00:18
epoch [10/200] batch [40/75] time 0.555 (0.460) data 0.422 (0.329) loss_u loss_u 0.9180 (0.9114) acc_u 12.5000 (11.7969) lr 1.9921e-03 eta 0:00:16
epoch [10/200] batch [45/75] time 0.395 (0.459) data 0.264 (0.328) loss_u loss_u 0.9717 (0.9143) acc_u 3.1250 (11.4583) lr 1.9921e-03 eta 0:00:13
epoch [10/200] batch [50/75] time 0.455 (0.457) data 0.323 (0.326) loss_u loss_u 0.9346 (0.9156) acc_u 3.1250 (11.1875) lr 1.9921e-03 eta 0:00:11
epoch [10/200] batch [55/75] time 0.422 (0.456) data 0.291 (0.325) loss_u loss_u 0.9160 (0.9163) acc_u 12.5000 (10.9659) lr 1.9921e-03 eta 0:00:09
epoch [10/200] batch [60/75] time 0.433 (0.454) data 0.302 (0.323) loss_u loss_u 0.9565 (0.9165) acc_u 9.3750 (10.9375) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [65/75] time 0.375 (0.454) data 0.245 (0.323) loss_u loss_u 0.9028 (0.9168) acc_u 12.5000 (10.8654) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [70/75] time 0.442 (0.455) data 0.311 (0.324) loss_u loss_u 0.9614 (0.9191) acc_u 3.1250 (10.6250) lr 1.9921e-03 eta 0:00:02
epoch [10/200] batch [75/75] time 0.345 (0.451) data 0.214 (0.320) loss_u loss_u 0.8428 (0.9183) acc_u 15.6250 (10.6250) lr 1.9921e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1806
confident_label rate tensor(0.2226, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 698
clean true:695
clean false:3
clean_rate:0.995702005730659
noisy true:635
noisy false:1803
after delete: len(clean_dataset) 698
after delete: len(noisy_dataset) 2438
epoch [11/200] batch [5/21] time 0.382 (0.452) data 0.251 (0.321) loss_x loss_x 1.8584 (1.5676) acc_x 59.3750 (58.1250) lr 1.9900e-03 eta 0:00:07
epoch [11/200] batch [10/21] time 0.545 (0.450) data 0.414 (0.319) loss_x loss_x 1.2344 (1.5450) acc_x 71.8750 (60.3125) lr 1.9900e-03 eta 0:00:04
epoch [11/200] batch [15/21] time 0.603 (0.460) data 0.471 (0.329) loss_x loss_x 1.5352 (1.5848) acc_x 62.5000 (61.6667) lr 1.9900e-03 eta 0:00:02
epoch [11/200] batch [20/21] time 0.427 (0.464) data 0.296 (0.333) loss_x loss_x 1.6562 (1.6043) acc_x 53.1250 (60.6250) lr 1.9900e-03 eta 0:00:00
epoch [11/200] batch [5/76] time 0.344 (0.456) data 0.213 (0.325) loss_u loss_u 0.9761 (0.9220) acc_u 3.1250 (11.2500) lr 1.9900e-03 eta 0:00:32
epoch [11/200] batch [10/76] time 0.399 (0.445) data 0.268 (0.314) loss_u loss_u 0.8823 (0.9127) acc_u 12.5000 (12.5000) lr 1.9900e-03 eta 0:00:29
epoch [11/200] batch [15/76] time 0.454 (0.448) data 0.323 (0.317) loss_u loss_u 0.8892 (0.9123) acc_u 15.6250 (12.5000) lr 1.9900e-03 eta 0:00:27
epoch [11/200] batch [20/76] time 0.502 (0.444) data 0.370 (0.313) loss_u loss_u 0.9062 (0.9132) acc_u 12.5000 (12.0312) lr 1.9900e-03 eta 0:00:24
epoch [11/200] batch [25/76] time 0.468 (0.445) data 0.336 (0.314) loss_u loss_u 0.9307 (0.9116) acc_u 9.3750 (12.2500) lr 1.9900e-03 eta 0:00:22
epoch [11/200] batch [30/76] time 0.468 (0.448) data 0.337 (0.317) loss_u loss_u 0.8892 (0.9138) acc_u 12.5000 (11.7708) lr 1.9900e-03 eta 0:00:20
epoch [11/200] batch [35/76] time 0.480 (0.454) data 0.349 (0.323) loss_u loss_u 0.9639 (0.9157) acc_u 6.2500 (11.6071) lr 1.9900e-03 eta 0:00:18
epoch [11/200] batch [40/76] time 0.458 (0.452) data 0.327 (0.321) loss_u loss_u 0.9746 (0.9180) acc_u 0.0000 (11.0938) lr 1.9900e-03 eta 0:00:16
epoch [11/200] batch [45/76] time 0.555 (0.454) data 0.424 (0.323) loss_u loss_u 0.9258 (0.9167) acc_u 6.2500 (11.1806) lr 1.9900e-03 eta 0:00:14
epoch [11/200] batch [50/76] time 0.442 (0.459) data 0.310 (0.328) loss_u loss_u 0.9204 (0.9149) acc_u 12.5000 (11.3750) lr 1.9900e-03 eta 0:00:11
epoch [11/200] batch [55/76] time 0.461 (0.458) data 0.331 (0.326) loss_u loss_u 0.9434 (0.9157) acc_u 6.2500 (11.3068) lr 1.9900e-03 eta 0:00:09
epoch [11/200] batch [60/76] time 0.653 (0.460) data 0.521 (0.329) loss_u loss_u 0.9497 (0.9173) acc_u 6.2500 (11.0938) lr 1.9900e-03 eta 0:00:07
epoch [11/200] batch [65/76] time 0.467 (0.459) data 0.336 (0.328) loss_u loss_u 0.9126 (0.9168) acc_u 15.6250 (11.2019) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [70/76] time 0.359 (0.461) data 0.228 (0.330) loss_u loss_u 0.9585 (0.9185) acc_u 3.1250 (10.8929) lr 1.9900e-03 eta 0:00:02
epoch [11/200] batch [75/76] time 0.321 (0.458) data 0.191 (0.327) loss_u loss_u 0.9175 (0.9190) acc_u 12.5000 (10.8333) lr 1.9900e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1811
confident_label rate tensor(0.2156, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 676
clean true:673
clean false:3
clean_rate:0.9955621301775148
noisy true:652
noisy false:1808
after delete: len(clean_dataset) 676
after delete: len(noisy_dataset) 2460
epoch [12/200] batch [5/21] time 0.358 (0.443) data 0.227 (0.312) loss_x loss_x 1.3320 (1.4949) acc_x 68.7500 (58.1250) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [10/21] time 0.539 (0.471) data 0.408 (0.340) loss_x loss_x 1.7393 (1.4438) acc_x 56.2500 (60.0000) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [15/21] time 0.505 (0.487) data 0.375 (0.357) loss_x loss_x 1.8633 (1.4999) acc_x 46.8750 (59.3750) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [20/21] time 0.449 (0.474) data 0.319 (0.344) loss_x loss_x 1.5830 (1.4882) acc_x 62.5000 (61.2500) lr 1.9877e-03 eta 0:00:00
epoch [12/200] batch [5/76] time 0.369 (0.451) data 0.236 (0.320) loss_u loss_u 0.9482 (0.9318) acc_u 6.2500 (8.7500) lr 1.9877e-03 eta 0:00:31
epoch [12/200] batch [10/76] time 0.691 (0.462) data 0.560 (0.332) loss_u loss_u 0.9248 (0.9325) acc_u 9.3750 (10.0000) lr 1.9877e-03 eta 0:00:30
epoch [12/200] batch [15/76] time 0.536 (0.476) data 0.404 (0.345) loss_u loss_u 0.9316 (0.9349) acc_u 9.3750 (9.7917) lr 1.9877e-03 eta 0:00:29
epoch [12/200] batch [20/76] time 0.346 (0.475) data 0.214 (0.344) loss_u loss_u 0.8877 (0.9288) acc_u 12.5000 (10.3125) lr 1.9877e-03 eta 0:00:26
epoch [12/200] batch [25/76] time 0.544 (0.472) data 0.414 (0.342) loss_u loss_u 0.9648 (0.9316) acc_u 3.1250 (9.6250) lr 1.9877e-03 eta 0:00:24
epoch [12/200] batch [30/76] time 0.468 (0.474) data 0.338 (0.343) loss_u loss_u 0.9175 (0.9304) acc_u 15.6250 (10.3125) lr 1.9877e-03 eta 0:00:21
epoch [12/200] batch [35/76] time 0.385 (0.478) data 0.254 (0.347) loss_u loss_u 0.9277 (0.9279) acc_u 9.3750 (10.4464) lr 1.9877e-03 eta 0:00:19
epoch [12/200] batch [40/76] time 0.526 (0.479) data 0.395 (0.348) loss_u loss_u 0.9644 (0.9278) acc_u 6.2500 (10.1562) lr 1.9877e-03 eta 0:00:17
epoch [12/200] batch [45/76] time 0.470 (0.474) data 0.339 (0.343) loss_u loss_u 0.9194 (0.9292) acc_u 12.5000 (9.7917) lr 1.9877e-03 eta 0:00:14
epoch [12/200] batch [50/76] time 0.405 (0.468) data 0.275 (0.337) loss_u loss_u 0.8491 (0.9243) acc_u 18.7500 (10.5625) lr 1.9877e-03 eta 0:00:12
epoch [12/200] batch [55/76] time 0.372 (0.465) data 0.240 (0.334) loss_u loss_u 0.8872 (0.9223) acc_u 21.8750 (10.8523) lr 1.9877e-03 eta 0:00:09
epoch [12/200] batch [60/76] time 0.344 (0.464) data 0.212 (0.333) loss_u loss_u 0.9121 (0.9192) acc_u 6.2500 (11.0417) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [65/76] time 0.380 (0.463) data 0.249 (0.332) loss_u loss_u 0.9209 (0.9166) acc_u 9.3750 (11.4423) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [70/76] time 0.509 (0.464) data 0.378 (0.333) loss_u loss_u 0.8501 (0.9142) acc_u 18.7500 (11.6518) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [75/76] time 0.485 (0.465) data 0.353 (0.334) loss_u loss_u 0.9390 (0.9142) acc_u 9.3750 (11.5833) lr 1.9877e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1758
confident_label rate tensor(0.2245, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 704
clean true:700
clean false:4
clean_rate:0.9943181818181818
noisy true:678
noisy false:1754
after delete: len(clean_dataset) 704
after delete: len(noisy_dataset) 2432
epoch [13/200] batch [5/22] time 0.707 (0.489) data 0.576 (0.358) loss_x loss_x 1.2812 (1.4410) acc_x 68.7500 (60.0000) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [10/22] time 0.444 (0.506) data 0.314 (0.376) loss_x loss_x 1.9961 (1.4866) acc_x 37.5000 (57.5000) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [15/22] time 0.558 (0.499) data 0.428 (0.368) loss_x loss_x 1.8242 (1.4697) acc_x 65.6250 (60.4167) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [20/22] time 0.404 (0.498) data 0.273 (0.368) loss_x loss_x 1.5283 (1.4550) acc_x 68.7500 (62.1875) lr 1.9851e-03 eta 0:00:00
epoch [13/200] batch [5/76] time 0.618 (0.493) data 0.487 (0.362) loss_u loss_u 0.9282 (0.8978) acc_u 6.2500 (11.8750) lr 1.9851e-03 eta 0:00:34
epoch [13/200] batch [10/76] time 0.474 (0.485) data 0.342 (0.354) loss_u loss_u 0.8740 (0.9032) acc_u 15.6250 (10.9375) lr 1.9851e-03 eta 0:00:32
epoch [13/200] batch [15/76] time 0.358 (0.473) data 0.226 (0.342) loss_u loss_u 0.7910 (0.8979) acc_u 28.1250 (12.7083) lr 1.9851e-03 eta 0:00:28
epoch [13/200] batch [20/76] time 0.361 (0.475) data 0.231 (0.344) loss_u loss_u 0.9692 (0.9052) acc_u 3.1250 (11.8750) lr 1.9851e-03 eta 0:00:26
epoch [13/200] batch [25/76] time 0.524 (0.477) data 0.392 (0.346) loss_u loss_u 0.8809 (0.9041) acc_u 9.3750 (11.8750) lr 1.9851e-03 eta 0:00:24
epoch [13/200] batch [30/76] time 0.488 (0.474) data 0.357 (0.343) loss_u loss_u 0.9385 (0.9058) acc_u 12.5000 (11.7708) lr 1.9851e-03 eta 0:00:21
epoch [13/200] batch [35/76] time 0.440 (0.474) data 0.309 (0.343) loss_u loss_u 0.8994 (0.9079) acc_u 15.6250 (11.9643) lr 1.9851e-03 eta 0:00:19
epoch [13/200] batch [40/76] time 0.590 (0.478) data 0.458 (0.347) loss_u loss_u 0.9473 (0.9113) acc_u 6.2500 (11.6406) lr 1.9851e-03 eta 0:00:17
epoch [13/200] batch [45/76] time 0.453 (0.476) data 0.322 (0.345) loss_u loss_u 0.8721 (0.9124) acc_u 15.6250 (11.3889) lr 1.9851e-03 eta 0:00:14
epoch [13/200] batch [50/76] time 0.425 (0.476) data 0.293 (0.345) loss_u loss_u 0.8940 (0.9140) acc_u 12.5000 (11.2500) lr 1.9851e-03 eta 0:00:12
epoch [13/200] batch [55/76] time 0.445 (0.475) data 0.313 (0.344) loss_u loss_u 0.9229 (0.9147) acc_u 6.2500 (11.1364) lr 1.9851e-03 eta 0:00:09
epoch [13/200] batch [60/76] time 0.444 (0.471) data 0.313 (0.340) loss_u loss_u 0.8979 (0.9131) acc_u 9.3750 (11.3021) lr 1.9851e-03 eta 0:00:07
epoch [13/200] batch [65/76] time 0.397 (0.467) data 0.267 (0.336) loss_u loss_u 0.9526 (0.9158) acc_u 6.2500 (10.9135) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [70/76] time 0.457 (0.465) data 0.326 (0.333) loss_u loss_u 0.9229 (0.9156) acc_u 12.5000 (11.0268) lr 1.9851e-03 eta 0:00:02
epoch [13/200] batch [75/76] time 0.493 (0.463) data 0.361 (0.331) loss_u loss_u 0.9399 (0.9175) acc_u 6.2500 (10.7083) lr 1.9851e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1790
confident_label rate tensor(0.2286, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 717
clean true:710
clean false:7
clean_rate:0.9902370990237099
noisy true:636
noisy false:1783
after delete: len(clean_dataset) 717
after delete: len(noisy_dataset) 2419
epoch [14/200] batch [5/22] time 0.661 (0.535) data 0.531 (0.403) loss_x loss_x 1.7148 (1.5748) acc_x 62.5000 (64.3750) lr 1.9823e-03 eta 0:00:09
epoch [14/200] batch [10/22] time 0.429 (0.507) data 0.299 (0.376) loss_x loss_x 1.6787 (1.6021) acc_x 53.1250 (60.9375) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [15/22] time 0.444 (0.492) data 0.314 (0.361) loss_x loss_x 1.2900 (1.5322) acc_x 50.0000 (61.0417) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [20/22] time 0.468 (0.501) data 0.338 (0.370) loss_x loss_x 1.0977 (1.5195) acc_x 71.8750 (62.0312) lr 1.9823e-03 eta 0:00:01
epoch [14/200] batch [5/75] time 0.467 (0.483) data 0.336 (0.353) loss_u loss_u 0.8989 (0.9003) acc_u 6.2500 (12.5000) lr 1.9823e-03 eta 0:00:33
epoch [14/200] batch [10/75] time 0.503 (0.483) data 0.371 (0.352) loss_u loss_u 0.9214 (0.9072) acc_u 9.3750 (11.8750) lr 1.9823e-03 eta 0:00:31
epoch [14/200] batch [15/75] time 0.487 (0.471) data 0.355 (0.340) loss_u loss_u 0.9092 (0.9159) acc_u 12.5000 (10.8333) lr 1.9823e-03 eta 0:00:28
epoch [14/200] batch [20/75] time 0.516 (0.469) data 0.384 (0.338) loss_u loss_u 0.8604 (0.9103) acc_u 21.8750 (11.5625) lr 1.9823e-03 eta 0:00:25
epoch [14/200] batch [25/75] time 0.501 (0.465) data 0.369 (0.334) loss_u loss_u 0.9097 (0.9092) acc_u 9.3750 (11.6250) lr 1.9823e-03 eta 0:00:23
epoch [14/200] batch [30/75] time 0.428 (0.463) data 0.296 (0.332) loss_u loss_u 0.8955 (0.9118) acc_u 12.5000 (11.4583) lr 1.9823e-03 eta 0:00:20
epoch [14/200] batch [35/75] time 0.526 (0.463) data 0.394 (0.332) loss_u loss_u 0.9492 (0.9141) acc_u 6.2500 (11.1607) lr 1.9823e-03 eta 0:00:18
epoch [14/200] batch [40/75] time 0.450 (0.460) data 0.319 (0.329) loss_u loss_u 0.9219 (0.9138) acc_u 12.5000 (11.1719) lr 1.9823e-03 eta 0:00:16
epoch [14/200] batch [45/75] time 0.422 (0.454) data 0.291 (0.323) loss_u loss_u 0.9272 (0.9142) acc_u 9.3750 (10.9028) lr 1.9823e-03 eta 0:00:13
epoch [14/200] batch [50/75] time 0.430 (0.454) data 0.298 (0.323) loss_u loss_u 0.9263 (0.9167) acc_u 6.2500 (10.6250) lr 1.9823e-03 eta 0:00:11
epoch [14/200] batch [55/75] time 0.420 (0.462) data 0.288 (0.331) loss_u loss_u 0.9771 (0.9190) acc_u 0.0000 (10.3977) lr 1.9823e-03 eta 0:00:09
epoch [14/200] batch [60/75] time 0.453 (0.464) data 0.321 (0.333) loss_u loss_u 0.8809 (0.9189) acc_u 21.8750 (10.6250) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [65/75] time 0.473 (0.461) data 0.342 (0.330) loss_u loss_u 0.9136 (0.9183) acc_u 9.3750 (10.6731) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [70/75] time 0.421 (0.460) data 0.290 (0.329) loss_u loss_u 0.9058 (0.9197) acc_u 15.6250 (10.4464) lr 1.9823e-03 eta 0:00:02
epoch [14/200] batch [75/75] time 0.509 (0.463) data 0.379 (0.332) loss_u loss_u 0.9297 (0.9186) acc_u 9.3750 (10.6667) lr 1.9823e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1807
confident_label rate tensor(0.2277, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 714
clean true:707
clean false:7
clean_rate:0.9901960784313726
noisy true:622
noisy false:1800
after delete: len(clean_dataset) 714
after delete: len(noisy_dataset) 2422
epoch [15/200] batch [5/22] time 0.437 (0.501) data 0.305 (0.370) loss_x loss_x 1.4775 (1.3672) acc_x 56.2500 (63.1250) lr 1.9792e-03 eta 0:00:08
epoch [15/200] batch [10/22] time 0.528 (0.474) data 0.397 (0.343) loss_x loss_x 1.7490 (1.5399) acc_x 50.0000 (59.6875) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [15/22] time 0.544 (0.479) data 0.414 (0.348) loss_x loss_x 1.4111 (1.5310) acc_x 65.6250 (60.2083) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [20/22] time 0.630 (0.483) data 0.498 (0.352) loss_x loss_x 1.0488 (1.4309) acc_x 75.0000 (62.5000) lr 1.9792e-03 eta 0:00:00
epoch [15/200] batch [5/75] time 0.388 (0.479) data 0.257 (0.348) loss_u loss_u 0.8735 (0.9413) acc_u 15.6250 (6.8750) lr 1.9792e-03 eta 0:00:33
epoch [15/200] batch [10/75] time 0.423 (0.473) data 0.292 (0.342) loss_u loss_u 0.9282 (0.9306) acc_u 6.2500 (8.4375) lr 1.9792e-03 eta 0:00:30
epoch [15/200] batch [15/75] time 0.438 (0.475) data 0.305 (0.344) loss_u loss_u 0.8926 (0.9184) acc_u 12.5000 (10.4167) lr 1.9792e-03 eta 0:00:28
epoch [15/200] batch [20/75] time 0.386 (0.467) data 0.255 (0.336) loss_u loss_u 0.9155 (0.9192) acc_u 18.7500 (10.6250) lr 1.9792e-03 eta 0:00:25
epoch [15/200] batch [25/75] time 0.422 (0.465) data 0.291 (0.334) loss_u loss_u 0.8608 (0.9158) acc_u 12.5000 (10.7500) lr 1.9792e-03 eta 0:00:23
epoch [15/200] batch [30/75] time 0.505 (0.460) data 0.374 (0.329) loss_u loss_u 0.9302 (0.9174) acc_u 6.2500 (10.4167) lr 1.9792e-03 eta 0:00:20
epoch [15/200] batch [35/75] time 0.458 (0.465) data 0.328 (0.334) loss_u loss_u 0.9111 (0.9173) acc_u 9.3750 (10.4464) lr 1.9792e-03 eta 0:00:18
epoch [15/200] batch [40/75] time 0.464 (0.462) data 0.333 (0.331) loss_u loss_u 0.8613 (0.9135) acc_u 18.7500 (11.2500) lr 1.9792e-03 eta 0:00:16
epoch [15/200] batch [45/75] time 0.508 (0.459) data 0.377 (0.328) loss_u loss_u 0.9009 (0.9148) acc_u 9.3750 (11.0417) lr 1.9792e-03 eta 0:00:13
epoch [15/200] batch [50/75] time 0.381 (0.455) data 0.249 (0.324) loss_u loss_u 0.9805 (0.9151) acc_u 0.0000 (10.9375) lr 1.9792e-03 eta 0:00:11
epoch [15/200] batch [55/75] time 0.481 (0.453) data 0.350 (0.322) loss_u loss_u 0.8545 (0.9139) acc_u 12.5000 (10.9091) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [60/75] time 0.511 (0.456) data 0.380 (0.325) loss_u loss_u 0.9277 (0.9121) acc_u 12.5000 (11.1979) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [65/75] time 0.436 (0.461) data 0.303 (0.330) loss_u loss_u 0.9395 (0.9131) acc_u 12.5000 (11.1058) lr 1.9792e-03 eta 0:00:04
epoch [15/200] batch [70/75] time 0.627 (0.465) data 0.497 (0.334) loss_u loss_u 0.8735 (0.9126) acc_u 12.5000 (11.2054) lr 1.9792e-03 eta 0:00:02
epoch [15/200] batch [75/75] time 0.335 (0.468) data 0.203 (0.336) loss_u loss_u 0.9331 (0.9142) acc_u 9.3750 (11.0000) lr 1.9792e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1746
confident_label rate tensor(0.2353, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 738
clean true:736
clean false:2
clean_rate:0.997289972899729
noisy true:654
noisy false:1744
after delete: len(clean_dataset) 738
after delete: len(noisy_dataset) 2398
epoch [16/200] batch [5/23] time 0.383 (0.592) data 0.253 (0.461) loss_x loss_x 1.7979 (1.7004) acc_x 62.5000 (60.6250) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [10/23] time 0.478 (0.535) data 0.348 (0.404) loss_x loss_x 1.0254 (1.4955) acc_x 75.0000 (64.6875) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [15/23] time 0.527 (0.541) data 0.396 (0.410) loss_x loss_x 1.1055 (1.4713) acc_x 65.6250 (64.1667) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [20/23] time 0.464 (0.529) data 0.334 (0.398) loss_x loss_x 0.8740 (1.4009) acc_x 78.1250 (65.7812) lr 1.9759e-03 eta 0:00:01
epoch [16/200] batch [5/74] time 0.504 (0.511) data 0.374 (0.381) loss_u loss_u 0.8960 (0.9115) acc_u 12.5000 (11.2500) lr 1.9759e-03 eta 0:00:35
epoch [16/200] batch [10/74] time 0.456 (0.501) data 0.324 (0.370) loss_u loss_u 0.9414 (0.9145) acc_u 9.3750 (10.6250) lr 1.9759e-03 eta 0:00:32
epoch [16/200] batch [15/74] time 0.466 (0.489) data 0.334 (0.358) loss_u loss_u 0.8950 (0.9070) acc_u 15.6250 (11.4583) lr 1.9759e-03 eta 0:00:28
epoch [16/200] batch [20/74] time 0.433 (0.487) data 0.302 (0.356) loss_u loss_u 0.9067 (0.9053) acc_u 21.8750 (12.1875) lr 1.9759e-03 eta 0:00:26
epoch [16/200] batch [25/74] time 0.570 (0.485) data 0.439 (0.354) loss_u loss_u 0.9194 (0.9071) acc_u 9.3750 (11.6250) lr 1.9759e-03 eta 0:00:23
epoch [16/200] batch [30/74] time 0.560 (0.483) data 0.430 (0.352) loss_u loss_u 0.9043 (0.9101) acc_u 15.6250 (11.3542) lr 1.9759e-03 eta 0:00:21
epoch [16/200] batch [35/74] time 0.496 (0.481) data 0.365 (0.350) loss_u loss_u 0.9727 (0.9122) acc_u 6.2500 (11.0714) lr 1.9759e-03 eta 0:00:18
epoch [16/200] batch [40/74] time 0.543 (0.482) data 0.411 (0.350) loss_u loss_u 0.8779 (0.9103) acc_u 12.5000 (11.1719) lr 1.9759e-03 eta 0:00:16
epoch [16/200] batch [45/74] time 0.573 (0.479) data 0.443 (0.347) loss_u loss_u 0.8257 (0.9118) acc_u 25.0000 (11.0417) lr 1.9759e-03 eta 0:00:13
epoch [16/200] batch [50/74] time 0.395 (0.475) data 0.263 (0.344) loss_u loss_u 0.8862 (0.9133) acc_u 15.6250 (11.0625) lr 1.9759e-03 eta 0:00:11
epoch [16/200] batch [55/74] time 0.445 (0.471) data 0.313 (0.340) loss_u loss_u 0.9326 (0.9145) acc_u 12.5000 (11.0795) lr 1.9759e-03 eta 0:00:08
epoch [16/200] batch [60/74] time 0.386 (0.470) data 0.254 (0.339) loss_u loss_u 0.9185 (0.9146) acc_u 9.3750 (10.8854) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [65/74] time 0.435 (0.471) data 0.304 (0.340) loss_u loss_u 0.8765 (0.9131) acc_u 15.6250 (11.0577) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [70/74] time 0.642 (0.474) data 0.512 (0.343) loss_u loss_u 0.9111 (0.9148) acc_u 9.3750 (10.7143) lr 1.9759e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1772
confident_label rate tensor(0.2274, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 713
clean true:706
clean false:7
clean_rate:0.9901823281907434
noisy true:658
noisy false:1765
after delete: len(clean_dataset) 713
after delete: len(noisy_dataset) 2423
epoch [17/200] batch [5/22] time 0.581 (0.449) data 0.451 (0.318) loss_x loss_x 0.9849 (1.5366) acc_x 75.0000 (63.7500) lr 1.9724e-03 eta 0:00:07
epoch [17/200] batch [10/22] time 0.541 (0.473) data 0.410 (0.343) loss_x loss_x 1.1982 (1.5273) acc_x 65.6250 (60.3125) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [15/22] time 0.488 (0.471) data 0.357 (0.341) loss_x loss_x 1.9678 (1.5279) acc_x 56.2500 (60.8333) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [20/22] time 0.467 (0.482) data 0.332 (0.352) loss_x loss_x 1.1182 (1.5058) acc_x 78.1250 (61.5625) lr 1.9724e-03 eta 0:00:00
epoch [17/200] batch [5/75] time 0.724 (0.488) data 0.594 (0.357) loss_u loss_u 0.9819 (0.9370) acc_u 3.1250 (7.5000) lr 1.9724e-03 eta 0:00:34
epoch [17/200] batch [10/75] time 0.436 (0.480) data 0.304 (0.350) loss_u loss_u 0.8569 (0.9107) acc_u 25.0000 (11.8750) lr 1.9724e-03 eta 0:00:31
epoch [17/200] batch [15/75] time 0.358 (0.471) data 0.226 (0.340) loss_u loss_u 0.9194 (0.9140) acc_u 15.6250 (12.0833) lr 1.9724e-03 eta 0:00:28
epoch [17/200] batch [20/75] time 0.363 (0.467) data 0.232 (0.336) loss_u loss_u 0.8892 (0.9153) acc_u 12.5000 (11.5625) lr 1.9724e-03 eta 0:00:25
epoch [17/200] batch [25/75] time 0.373 (0.460) data 0.242 (0.329) loss_u loss_u 0.9399 (0.9163) acc_u 9.3750 (11.1250) lr 1.9724e-03 eta 0:00:23
epoch [17/200] batch [30/75] time 0.336 (0.456) data 0.204 (0.325) loss_u loss_u 0.9277 (0.9209) acc_u 9.3750 (10.4167) lr 1.9724e-03 eta 0:00:20
epoch [17/200] batch [35/75] time 0.533 (0.451) data 0.402 (0.320) loss_u loss_u 0.9009 (0.9219) acc_u 15.6250 (10.6250) lr 1.9724e-03 eta 0:00:18
epoch [17/200] batch [40/75] time 0.511 (0.447) data 0.381 (0.316) loss_u loss_u 0.8394 (0.9173) acc_u 18.7500 (11.0938) lr 1.9724e-03 eta 0:00:15
epoch [17/200] batch [45/75] time 0.401 (0.445) data 0.270 (0.314) loss_u loss_u 0.9346 (0.9146) acc_u 6.2500 (11.5278) lr 1.9724e-03 eta 0:00:13
epoch [17/200] batch [50/75] time 0.499 (0.445) data 0.368 (0.314) loss_u loss_u 0.9434 (0.9138) acc_u 6.2500 (11.4375) lr 1.9724e-03 eta 0:00:11
epoch [17/200] batch [55/75] time 0.442 (0.447) data 0.311 (0.316) loss_u loss_u 0.8452 (0.9106) acc_u 18.7500 (11.8182) lr 1.9724e-03 eta 0:00:08
epoch [17/200] batch [60/75] time 0.471 (0.446) data 0.339 (0.315) loss_u loss_u 0.8892 (0.9087) acc_u 15.6250 (12.0312) lr 1.9724e-03 eta 0:00:06
epoch [17/200] batch [65/75] time 0.459 (0.447) data 0.328 (0.316) loss_u loss_u 0.9424 (0.9098) acc_u 3.1250 (11.7308) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [70/75] time 0.522 (0.452) data 0.391 (0.321) loss_u loss_u 0.8994 (0.9101) acc_u 12.5000 (11.6964) lr 1.9724e-03 eta 0:00:02
epoch [17/200] batch [75/75] time 0.431 (0.450) data 0.299 (0.319) loss_u loss_u 0.9199 (0.9109) acc_u 9.3750 (11.5833) lr 1.9724e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1742
confident_label rate tensor(0.2398, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 752
clean true:746
clean false:6
clean_rate:0.9920212765957447
noisy true:648
noisy false:1736
after delete: len(clean_dataset) 752
after delete: len(noisy_dataset) 2384
epoch [18/200] batch [5/23] time 0.524 (0.465) data 0.393 (0.334) loss_x loss_x 1.1387 (1.3519) acc_x 71.8750 (66.8750) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [10/23] time 0.429 (0.432) data 0.297 (0.302) loss_x loss_x 1.3184 (1.3618) acc_x 68.7500 (65.6250) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [15/23] time 0.452 (0.452) data 0.322 (0.322) loss_x loss_x 1.1689 (1.3981) acc_x 62.5000 (64.5833) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [20/23] time 0.446 (0.458) data 0.315 (0.328) loss_x loss_x 1.7363 (1.4469) acc_x 62.5000 (64.5312) lr 1.9686e-03 eta 0:00:01
epoch [18/200] batch [5/74] time 0.493 (0.469) data 0.361 (0.338) loss_u loss_u 0.9150 (0.8927) acc_u 9.3750 (13.1250) lr 1.9686e-03 eta 0:00:32
epoch [18/200] batch [10/74] time 0.381 (0.468) data 0.249 (0.337) loss_u loss_u 0.8950 (0.9096) acc_u 18.7500 (12.1875) lr 1.9686e-03 eta 0:00:29
epoch [18/200] batch [15/74] time 0.380 (0.463) data 0.247 (0.332) loss_u loss_u 0.9409 (0.9205) acc_u 6.2500 (10.2083) lr 1.9686e-03 eta 0:00:27
epoch [18/200] batch [20/74] time 0.565 (0.474) data 0.434 (0.343) loss_u loss_u 0.8652 (0.9231) acc_u 18.7500 (9.8438) lr 1.9686e-03 eta 0:00:25
epoch [18/200] batch [25/74] time 0.468 (0.469) data 0.335 (0.338) loss_u loss_u 0.8965 (0.9171) acc_u 12.5000 (10.3750) lr 1.9686e-03 eta 0:00:22
epoch [18/200] batch [30/74] time 0.415 (0.465) data 0.283 (0.334) loss_u loss_u 0.8906 (0.9166) acc_u 12.5000 (10.5208) lr 1.9686e-03 eta 0:00:20
epoch [18/200] batch [35/74] time 0.392 (0.460) data 0.260 (0.329) loss_u loss_u 0.9170 (0.9204) acc_u 12.5000 (10.2679) lr 1.9686e-03 eta 0:00:17
epoch [18/200] batch [40/74] time 0.492 (0.457) data 0.361 (0.326) loss_u loss_u 0.9497 (0.9207) acc_u 9.3750 (10.3125) lr 1.9686e-03 eta 0:00:15
epoch [18/200] batch [45/74] time 0.378 (0.453) data 0.247 (0.322) loss_u loss_u 0.9492 (0.9203) acc_u 6.2500 (10.3472) lr 1.9686e-03 eta 0:00:13
epoch [18/200] batch [50/74] time 0.490 (0.452) data 0.359 (0.321) loss_u loss_u 0.9443 (0.9189) acc_u 6.2500 (10.4375) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [55/74] time 0.398 (0.454) data 0.267 (0.323) loss_u loss_u 0.8506 (0.9162) acc_u 15.6250 (10.5114) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [60/74] time 0.454 (0.457) data 0.324 (0.326) loss_u loss_u 0.9473 (0.9157) acc_u 9.3750 (10.6771) lr 1.9686e-03 eta 0:00:06
epoch [18/200] batch [65/74] time 0.431 (0.455) data 0.301 (0.324) loss_u loss_u 0.8975 (0.9168) acc_u 15.6250 (10.5288) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [70/74] time 0.503 (0.453) data 0.371 (0.322) loss_u loss_u 0.9082 (0.9153) acc_u 12.5000 (10.8929) lr 1.9686e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1728
confident_label rate tensor(0.2446, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 767
clean true:765
clean false:2
clean_rate:0.9973924380704041
noisy true:643
noisy false:1726
after delete: len(clean_dataset) 767
after delete: len(noisy_dataset) 2369
epoch [19/200] batch [5/23] time 0.684 (0.465) data 0.553 (0.333) loss_x loss_x 1.2725 (1.2859) acc_x 71.8750 (66.8750) lr 1.9646e-03 eta 0:00:08
epoch [19/200] batch [10/23] time 0.637 (0.478) data 0.506 (0.347) loss_x loss_x 1.5479 (1.3707) acc_x 59.3750 (63.1250) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [15/23] time 0.389 (0.463) data 0.258 (0.332) loss_x loss_x 1.4268 (1.3660) acc_x 46.8750 (62.9167) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [20/23] time 0.436 (0.464) data 0.305 (0.333) loss_x loss_x 1.5068 (1.3338) acc_x 56.2500 (64.3750) lr 1.9646e-03 eta 0:00:01
epoch [19/200] batch [5/74] time 0.474 (0.458) data 0.343 (0.328) loss_u loss_u 0.9229 (0.9303) acc_u 6.2500 (8.7500) lr 1.9646e-03 eta 0:00:31
epoch [19/200] batch [10/74] time 0.587 (0.460) data 0.457 (0.329) loss_u loss_u 0.8818 (0.9234) acc_u 15.6250 (10.0000) lr 1.9646e-03 eta 0:00:29
epoch [19/200] batch [15/74] time 0.348 (0.453) data 0.217 (0.322) loss_u loss_u 0.9844 (0.9287) acc_u 0.0000 (9.3750) lr 1.9646e-03 eta 0:00:26
epoch [19/200] batch [20/74] time 0.324 (0.442) data 0.193 (0.312) loss_u loss_u 0.8999 (0.9227) acc_u 6.2500 (9.8438) lr 1.9646e-03 eta 0:00:23
epoch [19/200] batch [25/74] time 0.438 (0.446) data 0.307 (0.315) loss_u loss_u 0.9458 (0.9188) acc_u 6.2500 (10.0000) lr 1.9646e-03 eta 0:00:21
epoch [19/200] batch [30/74] time 0.494 (0.457) data 0.362 (0.326) loss_u loss_u 0.9941 (0.9198) acc_u 0.0000 (10.1042) lr 1.9646e-03 eta 0:00:20
epoch [19/200] batch [35/74] time 0.576 (0.464) data 0.445 (0.333) loss_u loss_u 0.9312 (0.9173) acc_u 6.2500 (10.1786) lr 1.9646e-03 eta 0:00:18
epoch [19/200] batch [40/74] time 0.310 (0.462) data 0.178 (0.331) loss_u loss_u 0.8721 (0.9188) acc_u 18.7500 (9.8438) lr 1.9646e-03 eta 0:00:15
epoch [19/200] batch [45/74] time 0.513 (0.462) data 0.381 (0.331) loss_u loss_u 0.9746 (0.9194) acc_u 3.1250 (9.7222) lr 1.9646e-03 eta 0:00:13
epoch [19/200] batch [50/74] time 0.568 (0.462) data 0.436 (0.330) loss_u loss_u 0.9219 (0.9187) acc_u 12.5000 (10.0000) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [55/74] time 0.421 (0.459) data 0.290 (0.328) loss_u loss_u 0.9165 (0.9182) acc_u 6.2500 (10.0000) lr 1.9646e-03 eta 0:00:08
epoch [19/200] batch [60/74] time 0.694 (0.464) data 0.561 (0.333) loss_u loss_u 0.9639 (0.9173) acc_u 3.1250 (10.1042) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [65/74] time 0.439 (0.460) data 0.309 (0.329) loss_u loss_u 0.8901 (0.9181) acc_u 15.6250 (10.1442) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [70/74] time 0.443 (0.459) data 0.312 (0.327) loss_u loss_u 0.8896 (0.9182) acc_u 15.6250 (10.2679) lr 1.9646e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1754
confident_label rate tensor(0.2423, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 760
clean true:754
clean false:6
clean_rate:0.9921052631578947
noisy true:628
noisy false:1748
after delete: len(clean_dataset) 760
after delete: len(noisy_dataset) 2376
epoch [20/200] batch [5/23] time 0.566 (0.543) data 0.436 (0.412) loss_x loss_x 1.1758 (1.2488) acc_x 71.8750 (66.8750) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [10/23] time 0.424 (0.515) data 0.294 (0.385) loss_x loss_x 2.0137 (1.3997) acc_x 53.1250 (65.3125) lr 1.9603e-03 eta 0:00:06
epoch [20/200] batch [15/23] time 0.490 (0.500) data 0.360 (0.369) loss_x loss_x 1.4346 (1.4081) acc_x 65.6250 (64.7917) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [20/23] time 0.571 (0.508) data 0.440 (0.378) loss_x loss_x 1.5371 (1.3920) acc_x 56.2500 (65.6250) lr 1.9603e-03 eta 0:00:01
epoch [20/200] batch [5/74] time 0.446 (0.493) data 0.315 (0.362) loss_u loss_u 0.8940 (0.8807) acc_u 12.5000 (15.0000) lr 1.9603e-03 eta 0:00:34
epoch [20/200] batch [10/74] time 0.357 (0.481) data 0.226 (0.350) loss_u loss_u 0.9116 (0.9018) acc_u 9.3750 (11.5625) lr 1.9603e-03 eta 0:00:30
epoch [20/200] batch [15/74] time 0.472 (0.488) data 0.341 (0.357) loss_u loss_u 0.9302 (0.9065) acc_u 9.3750 (11.2500) lr 1.9603e-03 eta 0:00:28
epoch [20/200] batch [20/74] time 0.544 (0.487) data 0.412 (0.356) loss_u loss_u 0.9268 (0.9132) acc_u 12.5000 (10.3125) lr 1.9603e-03 eta 0:00:26
epoch [20/200] batch [25/74] time 0.519 (0.484) data 0.388 (0.353) loss_u loss_u 0.8779 (0.9122) acc_u 21.8750 (10.8750) lr 1.9603e-03 eta 0:00:23
epoch [20/200] batch [30/74] time 0.418 (0.483) data 0.286 (0.352) loss_u loss_u 0.8730 (0.9128) acc_u 18.7500 (10.9375) lr 1.9603e-03 eta 0:00:21
epoch [20/200] batch [35/74] time 0.423 (0.483) data 0.293 (0.352) loss_u loss_u 0.9341 (0.9151) acc_u 9.3750 (10.8929) lr 1.9603e-03 eta 0:00:18
epoch [20/200] batch [40/74] time 0.420 (0.485) data 0.289 (0.354) loss_u loss_u 0.9302 (0.9162) acc_u 12.5000 (10.7812) lr 1.9603e-03 eta 0:00:16
epoch [20/200] batch [45/74] time 0.374 (0.482) data 0.241 (0.351) loss_u loss_u 0.9385 (0.9142) acc_u 9.3750 (11.1111) lr 1.9603e-03 eta 0:00:13
epoch [20/200] batch [50/74] time 0.585 (0.477) data 0.455 (0.346) loss_u loss_u 0.9570 (0.9160) acc_u 6.2500 (10.9375) lr 1.9603e-03 eta 0:00:11
epoch [20/200] batch [55/74] time 0.448 (0.478) data 0.317 (0.347) loss_u loss_u 0.8896 (0.9146) acc_u 15.6250 (11.0795) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [60/74] time 0.421 (0.477) data 0.291 (0.346) loss_u loss_u 0.9399 (0.9165) acc_u 9.3750 (11.0417) lr 1.9603e-03 eta 0:00:06
epoch [20/200] batch [65/74] time 0.418 (0.479) data 0.288 (0.348) loss_u loss_u 0.8486 (0.9141) acc_u 21.8750 (11.2500) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [70/74] time 0.558 (0.479) data 0.426 (0.348) loss_u loss_u 0.8828 (0.9139) acc_u 21.8750 (11.2946) lr 1.9603e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1747
confident_label rate tensor(0.2446, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 767
clean true:760
clean false:7
clean_rate:0.9908735332464146
noisy true:629
noisy false:1740
after delete: len(clean_dataset) 767
after delete: len(noisy_dataset) 2369
epoch [21/200] batch [5/23] time 0.549 (0.502) data 0.417 (0.371) loss_x loss_x 1.6250 (1.4904) acc_x 59.3750 (61.2500) lr 1.9558e-03 eta 0:00:09
epoch [21/200] batch [10/23] time 0.440 (0.467) data 0.309 (0.336) loss_x loss_x 1.3711 (1.4019) acc_x 68.7500 (63.7500) lr 1.9558e-03 eta 0:00:06
epoch [21/200] batch [15/23] time 0.608 (0.490) data 0.477 (0.359) loss_x loss_x 2.0566 (1.4614) acc_x 59.3750 (64.1667) lr 1.9558e-03 eta 0:00:03
epoch [21/200] batch [20/23] time 0.433 (0.486) data 0.303 (0.356) loss_x loss_x 1.4580 (1.4576) acc_x 62.5000 (62.8125) lr 1.9558e-03 eta 0:00:01
epoch [21/200] batch [5/74] time 0.403 (0.476) data 0.271 (0.345) loss_u loss_u 0.9287 (0.9214) acc_u 6.2500 (8.7500) lr 1.9558e-03 eta 0:00:32
epoch [21/200] batch [10/74] time 0.386 (0.472) data 0.255 (0.341) loss_u loss_u 0.9126 (0.9208) acc_u 9.3750 (9.6875) lr 1.9558e-03 eta 0:00:30
epoch [21/200] batch [15/74] time 0.396 (0.472) data 0.263 (0.341) loss_u loss_u 0.8833 (0.9169) acc_u 12.5000 (10.6250) lr 1.9558e-03 eta 0:00:27
epoch [21/200] batch [20/74] time 0.532 (0.467) data 0.400 (0.336) loss_u loss_u 0.9199 (0.9195) acc_u 9.3750 (10.3125) lr 1.9558e-03 eta 0:00:25
epoch [21/200] batch [25/74] time 0.484 (0.470) data 0.352 (0.339) loss_u loss_u 0.8867 (0.9160) acc_u 15.6250 (11.0000) lr 1.9558e-03 eta 0:00:23
epoch [21/200] batch [30/74] time 0.521 (0.471) data 0.390 (0.340) loss_u loss_u 0.9688 (0.9210) acc_u 0.0000 (9.8958) lr 1.9558e-03 eta 0:00:20
epoch [21/200] batch [35/74] time 0.475 (0.476) data 0.345 (0.345) loss_u loss_u 0.9258 (0.9180) acc_u 6.2500 (10.4464) lr 1.9558e-03 eta 0:00:18
epoch [21/200] batch [40/74] time 0.605 (0.480) data 0.473 (0.348) loss_u loss_u 0.8970 (0.9170) acc_u 12.5000 (10.4688) lr 1.9558e-03 eta 0:00:16
epoch [21/200] batch [45/74] time 0.422 (0.474) data 0.290 (0.342) loss_u loss_u 0.9233 (0.9171) acc_u 9.3750 (10.6250) lr 1.9558e-03 eta 0:00:13
epoch [21/200] batch [50/74] time 0.491 (0.474) data 0.360 (0.343) loss_u loss_u 0.8628 (0.9176) acc_u 15.6250 (10.5000) lr 1.9558e-03 eta 0:00:11
epoch [21/200] batch [55/74] time 0.495 (0.473) data 0.365 (0.342) loss_u loss_u 0.9116 (0.9167) acc_u 15.6250 (10.5682) lr 1.9558e-03 eta 0:00:08
epoch [21/200] batch [60/74] time 0.461 (0.468) data 0.330 (0.337) loss_u loss_u 0.9424 (0.9183) acc_u 6.2500 (10.4167) lr 1.9558e-03 eta 0:00:06
epoch [21/200] batch [65/74] time 0.603 (0.475) data 0.472 (0.344) loss_u loss_u 0.9033 (0.9187) acc_u 12.5000 (10.1923) lr 1.9558e-03 eta 0:00:04
epoch [21/200] batch [70/74] time 0.427 (0.475) data 0.296 (0.344) loss_u loss_u 0.8706 (0.9175) acc_u 18.7500 (10.3571) lr 1.9558e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1735
confident_label rate tensor(0.2337, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 733
clean true:724
clean false:9
clean_rate:0.9877216916780355
noisy true:677
noisy false:1726
after delete: len(clean_dataset) 733
after delete: len(noisy_dataset) 2403
epoch [22/200] batch [5/22] time 0.422 (0.543) data 0.290 (0.412) loss_x loss_x 1.4971 (1.3463) acc_x 71.8750 (73.1250) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [10/22] time 0.514 (0.530) data 0.383 (0.399) loss_x loss_x 1.3652 (1.3315) acc_x 68.7500 (69.0625) lr 1.9511e-03 eta 0:00:06
epoch [22/200] batch [15/22] time 0.417 (0.497) data 0.287 (0.367) loss_x loss_x 1.1748 (1.3683) acc_x 68.7500 (67.0833) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [20/22] time 0.459 (0.480) data 0.329 (0.350) loss_x loss_x 1.4971 (1.3563) acc_x 59.3750 (67.8125) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/75] time 0.463 (0.468) data 0.332 (0.338) loss_u loss_u 0.9648 (0.9379) acc_u 3.1250 (8.1250) lr 1.9511e-03 eta 0:00:32
epoch [22/200] batch [10/75] time 0.391 (0.472) data 0.259 (0.341) loss_u loss_u 0.9165 (0.9231) acc_u 9.3750 (10.9375) lr 1.9511e-03 eta 0:00:30
epoch [22/200] batch [15/75] time 0.440 (0.467) data 0.308 (0.336) loss_u loss_u 0.9219 (0.9257) acc_u 9.3750 (10.6250) lr 1.9511e-03 eta 0:00:28
epoch [22/200] batch [20/75] time 0.610 (0.472) data 0.479 (0.341) loss_u loss_u 0.9517 (0.9239) acc_u 6.2500 (10.7812) lr 1.9511e-03 eta 0:00:25
epoch [22/200] batch [25/75] time 0.422 (0.468) data 0.292 (0.337) loss_u loss_u 0.9351 (0.9245) acc_u 6.2500 (10.2500) lr 1.9511e-03 eta 0:00:23
epoch [22/200] batch [30/75] time 0.433 (0.469) data 0.301 (0.338) loss_u loss_u 0.9136 (0.9224) acc_u 12.5000 (10.5208) lr 1.9511e-03 eta 0:00:21
epoch [22/200] batch [35/75] time 0.456 (0.467) data 0.325 (0.336) loss_u loss_u 0.9546 (0.9222) acc_u 0.0000 (10.3571) lr 1.9511e-03 eta 0:00:18
epoch [22/200] batch [40/75] time 0.524 (0.467) data 0.393 (0.337) loss_u loss_u 0.9272 (0.9216) acc_u 9.3750 (10.3906) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [45/75] time 0.386 (0.465) data 0.255 (0.334) loss_u loss_u 0.9517 (0.9218) acc_u 3.1250 (10.2083) lr 1.9511e-03 eta 0:00:13
epoch [22/200] batch [50/75] time 0.493 (0.468) data 0.363 (0.337) loss_u loss_u 0.9341 (0.9220) acc_u 6.2500 (10.1875) lr 1.9511e-03 eta 0:00:11
epoch [22/200] batch [55/75] time 0.458 (0.467) data 0.328 (0.336) loss_u loss_u 0.9395 (0.9199) acc_u 0.0000 (10.2841) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [60/75] time 0.494 (0.464) data 0.364 (0.334) loss_u loss_u 0.9424 (0.9195) acc_u 6.2500 (10.4688) lr 1.9511e-03 eta 0:00:06
epoch [22/200] batch [65/75] time 0.368 (0.462) data 0.237 (0.331) loss_u loss_u 0.9282 (0.9169) acc_u 6.2500 (10.6250) lr 1.9511e-03 eta 0:00:04
epoch [22/200] batch [70/75] time 0.715 (0.466) data 0.584 (0.335) loss_u loss_u 0.9146 (0.9154) acc_u 6.2500 (10.9375) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [75/75] time 0.426 (0.463) data 0.295 (0.332) loss_u loss_u 0.8652 (0.9160) acc_u 15.6250 (10.8750) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1754
confident_label rate tensor(0.2376, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 745
clean true:741
clean false:4
clean_rate:0.9946308724832215
noisy true:641
noisy false:1750
after delete: len(clean_dataset) 745
after delete: len(noisy_dataset) 2391
epoch [23/200] batch [5/23] time 0.460 (0.485) data 0.330 (0.354) loss_x loss_x 1.5830 (1.3197) acc_x 71.8750 (66.8750) lr 1.9461e-03 eta 0:00:08
epoch [23/200] batch [10/23] time 0.461 (0.458) data 0.330 (0.328) loss_x loss_x 1.6025 (1.4387) acc_x 53.1250 (63.4375) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [15/23] time 0.465 (0.465) data 0.335 (0.334) loss_x loss_x 1.2646 (1.4536) acc_x 75.0000 (64.1667) lr 1.9461e-03 eta 0:00:03
epoch [23/200] batch [20/23] time 0.529 (0.469) data 0.399 (0.339) loss_x loss_x 0.9702 (1.4410) acc_x 71.8750 (64.0625) lr 1.9461e-03 eta 0:00:01
epoch [23/200] batch [5/74] time 0.585 (0.483) data 0.454 (0.352) loss_u loss_u 0.8862 (0.9095) acc_u 15.6250 (13.1250) lr 1.9461e-03 eta 0:00:33
epoch [23/200] batch [10/74] time 0.508 (0.482) data 0.377 (0.351) loss_u loss_u 0.9863 (0.9112) acc_u 0.0000 (11.5625) lr 1.9461e-03 eta 0:00:30
epoch [23/200] batch [15/74] time 0.403 (0.483) data 0.272 (0.352) loss_u loss_u 0.9741 (0.9132) acc_u 3.1250 (10.8333) lr 1.9461e-03 eta 0:00:28
epoch [23/200] batch [20/74] time 0.448 (0.475) data 0.318 (0.344) loss_u loss_u 0.9580 (0.9150) acc_u 3.1250 (10.7812) lr 1.9461e-03 eta 0:00:25
epoch [23/200] batch [25/74] time 0.539 (0.472) data 0.408 (0.341) loss_u loss_u 0.8770 (0.9147) acc_u 12.5000 (10.7500) lr 1.9461e-03 eta 0:00:23
epoch [23/200] batch [30/74] time 0.430 (0.463) data 0.299 (0.332) loss_u loss_u 0.8911 (0.9162) acc_u 12.5000 (10.8333) lr 1.9461e-03 eta 0:00:20
epoch [23/200] batch [35/74] time 0.399 (0.462) data 0.266 (0.331) loss_u loss_u 0.9136 (0.9158) acc_u 9.3750 (10.9821) lr 1.9461e-03 eta 0:00:18
epoch [23/200] batch [40/74] time 0.481 (0.463) data 0.348 (0.332) loss_u loss_u 0.9038 (0.9176) acc_u 12.5000 (10.7812) lr 1.9461e-03 eta 0:00:15
epoch [23/200] batch [45/74] time 0.567 (0.468) data 0.436 (0.337) loss_u loss_u 0.8081 (0.9150) acc_u 25.0000 (11.1806) lr 1.9461e-03 eta 0:00:13
epoch [23/200] batch [50/74] time 0.472 (0.467) data 0.341 (0.336) loss_u loss_u 0.9565 (0.9148) acc_u 6.2500 (11.3750) lr 1.9461e-03 eta 0:00:11
epoch [23/200] batch [55/74] time 0.609 (0.470) data 0.478 (0.339) loss_u loss_u 0.9014 (0.9144) acc_u 12.5000 (11.3636) lr 1.9461e-03 eta 0:00:08
epoch [23/200] batch [60/74] time 0.419 (0.469) data 0.288 (0.338) loss_u loss_u 0.8726 (0.9141) acc_u 25.0000 (11.4062) lr 1.9461e-03 eta 0:00:06
epoch [23/200] batch [65/74] time 0.475 (0.472) data 0.344 (0.341) loss_u loss_u 0.9473 (0.9132) acc_u 15.6250 (11.7308) lr 1.9461e-03 eta 0:00:04
epoch [23/200] batch [70/74] time 0.420 (0.468) data 0.289 (0.337) loss_u loss_u 0.8628 (0.9120) acc_u 18.7500 (11.8750) lr 1.9461e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1713
confident_label rate tensor(0.2506, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 786
clean true:778
clean false:8
clean_rate:0.989821882951654
noisy true:645
noisy false:1705
after delete: len(clean_dataset) 786
after delete: len(noisy_dataset) 2350
epoch [24/200] batch [5/24] time 0.441 (0.566) data 0.311 (0.435) loss_x loss_x 1.5859 (1.3850) acc_x 53.1250 (63.7500) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [10/24] time 0.404 (0.531) data 0.274 (0.400) loss_x loss_x 1.7305 (1.4680) acc_x 56.2500 (60.0000) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [15/24] time 0.632 (0.511) data 0.499 (0.381) loss_x loss_x 1.5361 (1.4948) acc_x 59.3750 (60.6250) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [20/24] time 0.440 (0.495) data 0.309 (0.365) loss_x loss_x 1.0674 (1.4891) acc_x 81.2500 (61.4062) lr 1.9409e-03 eta 0:00:01
epoch [24/200] batch [5/73] time 0.382 (0.490) data 0.252 (0.359) loss_u loss_u 0.9326 (0.8937) acc_u 9.3750 (12.5000) lr 1.9409e-03 eta 0:00:33
epoch [24/200] batch [10/73] time 0.460 (0.485) data 0.328 (0.354) loss_u loss_u 0.9131 (0.9060) acc_u 6.2500 (11.5625) lr 1.9409e-03 eta 0:00:30
epoch [24/200] batch [15/73] time 0.508 (0.482) data 0.377 (0.351) loss_u loss_u 0.9297 (0.9172) acc_u 6.2500 (10.2083) lr 1.9409e-03 eta 0:00:27
epoch [24/200] batch [20/73] time 0.457 (0.482) data 0.326 (0.352) loss_u loss_u 0.9111 (0.9208) acc_u 12.5000 (10.1562) lr 1.9409e-03 eta 0:00:25
epoch [24/200] batch [25/73] time 0.518 (0.485) data 0.389 (0.354) loss_u loss_u 0.9453 (0.9199) acc_u 6.2500 (10.3750) lr 1.9409e-03 eta 0:00:23
epoch [24/200] batch [30/73] time 0.449 (0.482) data 0.319 (0.351) loss_u loss_u 0.8843 (0.9229) acc_u 15.6250 (10.0000) lr 1.9409e-03 eta 0:00:20
epoch [24/200] batch [35/73] time 0.443 (0.478) data 0.312 (0.348) loss_u loss_u 0.9600 (0.9204) acc_u 3.1250 (10.2679) lr 1.9409e-03 eta 0:00:18
epoch [24/200] batch [40/73] time 0.444 (0.477) data 0.315 (0.347) loss_u loss_u 0.9341 (0.9217) acc_u 9.3750 (10.1562) lr 1.9409e-03 eta 0:00:15
epoch [24/200] batch [45/73] time 0.370 (0.476) data 0.238 (0.346) loss_u loss_u 0.8896 (0.9209) acc_u 15.6250 (10.1389) lr 1.9409e-03 eta 0:00:13
epoch [24/200] batch [50/73] time 0.418 (0.473) data 0.284 (0.342) loss_u loss_u 0.9258 (0.9191) acc_u 12.5000 (10.6250) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [55/73] time 0.437 (0.472) data 0.306 (0.341) loss_u loss_u 0.9531 (0.9187) acc_u 6.2500 (10.3977) lr 1.9409e-03 eta 0:00:08
epoch [24/200] batch [60/73] time 0.508 (0.472) data 0.377 (0.341) loss_u loss_u 0.9253 (0.9184) acc_u 12.5000 (10.4167) lr 1.9409e-03 eta 0:00:06
epoch [24/200] batch [65/73] time 0.450 (0.469) data 0.319 (0.339) loss_u loss_u 0.8779 (0.9145) acc_u 12.5000 (11.0096) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [70/73] time 0.448 (0.468) data 0.317 (0.337) loss_u loss_u 0.9380 (0.9162) acc_u 3.1250 (10.7143) lr 1.9409e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1693
confident_label rate tensor(0.2522, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 791
clean true:783
clean false:8
clean_rate:0.9898862199747156
noisy true:660
noisy false:1685
after delete: len(clean_dataset) 791
after delete: len(noisy_dataset) 2345
epoch [25/200] batch [5/24] time 0.439 (0.461) data 0.308 (0.330) loss_x loss_x 2.3242 (1.6635) acc_x 59.3750 (61.8750) lr 1.9354e-03 eta 0:00:08
epoch [25/200] batch [10/24] time 0.505 (0.483) data 0.375 (0.352) loss_x loss_x 1.6924 (1.6563) acc_x 59.3750 (60.9375) lr 1.9354e-03 eta 0:00:06
epoch [25/200] batch [15/24] time 0.586 (0.469) data 0.456 (0.338) loss_x loss_x 1.2168 (1.5821) acc_x 62.5000 (61.0417) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [20/24] time 0.432 (0.472) data 0.302 (0.341) loss_x loss_x 1.6660 (1.5569) acc_x 59.3750 (60.6250) lr 1.9354e-03 eta 0:00:01
epoch [25/200] batch [5/73] time 0.339 (0.454) data 0.210 (0.323) loss_u loss_u 0.8608 (0.9062) acc_u 15.6250 (11.8750) lr 1.9354e-03 eta 0:00:30
epoch [25/200] batch [10/73] time 0.386 (0.455) data 0.256 (0.324) loss_u loss_u 0.9185 (0.9058) acc_u 12.5000 (12.8125) lr 1.9354e-03 eta 0:00:28
epoch [25/200] batch [15/73] time 0.413 (0.452) data 0.282 (0.321) loss_u loss_u 0.8877 (0.9124) acc_u 18.7500 (11.8750) lr 1.9354e-03 eta 0:00:26
epoch [25/200] batch [20/73] time 0.508 (0.451) data 0.377 (0.320) loss_u loss_u 0.8965 (0.9177) acc_u 9.3750 (10.9375) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [25/73] time 0.607 (0.452) data 0.476 (0.321) loss_u loss_u 0.9229 (0.9207) acc_u 9.3750 (10.6250) lr 1.9354e-03 eta 0:00:21
epoch [25/200] batch [30/73] time 0.397 (0.449) data 0.265 (0.318) loss_u loss_u 0.9077 (0.9207) acc_u 12.5000 (10.8333) lr 1.9354e-03 eta 0:00:19
epoch [25/200] batch [35/73] time 0.353 (0.442) data 0.223 (0.311) loss_u loss_u 0.9072 (0.9173) acc_u 12.5000 (11.1607) lr 1.9354e-03 eta 0:00:16
epoch [25/200] batch [40/73] time 0.542 (0.446) data 0.410 (0.315) loss_u loss_u 0.9795 (0.9188) acc_u 0.0000 (10.7031) lr 1.9354e-03 eta 0:00:14
epoch [25/200] batch [45/73] time 0.471 (0.451) data 0.340 (0.320) loss_u loss_u 0.9512 (0.9198) acc_u 3.1250 (10.5556) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [50/73] time 0.399 (0.446) data 0.268 (0.316) loss_u loss_u 0.9165 (0.9192) acc_u 6.2500 (10.5625) lr 1.9354e-03 eta 0:00:10
epoch [25/200] batch [55/73] time 0.571 (0.449) data 0.440 (0.318) loss_u loss_u 0.8940 (0.9176) acc_u 15.6250 (10.6818) lr 1.9354e-03 eta 0:00:08
epoch [25/200] batch [60/73] time 0.586 (0.451) data 0.454 (0.320) loss_u loss_u 0.9321 (0.9187) acc_u 9.3750 (10.4688) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [65/73] time 0.493 (0.449) data 0.363 (0.318) loss_u loss_u 0.9424 (0.9184) acc_u 9.3750 (10.5288) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [70/73] time 0.424 (0.450) data 0.291 (0.319) loss_u loss_u 0.9277 (0.9190) acc_u 6.2500 (10.5357) lr 1.9354e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1710
confident_label rate tensor(0.2484, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 779
clean true:774
clean false:5
clean_rate:0.993581514762516
noisy true:652
noisy false:1705
after delete: len(clean_dataset) 779
after delete: len(noisy_dataset) 2357
epoch [26/200] batch [5/24] time 0.814 (0.589) data 0.684 (0.457) loss_x loss_x 1.3887 (1.3906) acc_x 59.3750 (65.6250) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [10/24] time 0.591 (0.578) data 0.460 (0.447) loss_x loss_x 1.1777 (1.3530) acc_x 65.6250 (66.8750) lr 1.9298e-03 eta 0:00:08
epoch [26/200] batch [15/24] time 0.494 (0.530) data 0.364 (0.399) loss_x loss_x 1.2031 (1.3743) acc_x 62.5000 (65.8333) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [20/24] time 0.565 (0.520) data 0.434 (0.389) loss_x loss_x 0.8223 (1.3693) acc_x 75.0000 (65.3125) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [5/73] time 0.557 (0.495) data 0.426 (0.364) loss_u loss_u 0.8760 (0.9109) acc_u 21.8750 (13.7500) lr 1.9298e-03 eta 0:00:33
epoch [26/200] batch [10/73] time 0.445 (0.492) data 0.314 (0.361) loss_u loss_u 0.8979 (0.9200) acc_u 9.3750 (11.2500) lr 1.9298e-03 eta 0:00:31
epoch [26/200] batch [15/73] time 0.530 (0.501) data 0.399 (0.370) loss_u loss_u 0.8301 (0.9030) acc_u 21.8750 (13.1250) lr 1.9298e-03 eta 0:00:29
epoch [26/200] batch [20/73] time 0.414 (0.495) data 0.283 (0.364) loss_u loss_u 0.9482 (0.9094) acc_u 6.2500 (11.7188) lr 1.9298e-03 eta 0:00:26
epoch [26/200] batch [25/73] time 0.397 (0.490) data 0.267 (0.359) loss_u loss_u 0.9019 (0.9097) acc_u 9.3750 (11.5000) lr 1.9298e-03 eta 0:00:23
epoch [26/200] batch [30/73] time 0.466 (0.485) data 0.335 (0.354) loss_u loss_u 0.9092 (0.9080) acc_u 9.3750 (12.1875) lr 1.9298e-03 eta 0:00:20
epoch [26/200] batch [35/73] time 0.397 (0.482) data 0.265 (0.351) loss_u loss_u 0.9180 (0.9118) acc_u 12.5000 (11.6964) lr 1.9298e-03 eta 0:00:18
epoch [26/200] batch [40/73] time 0.432 (0.479) data 0.300 (0.348) loss_u loss_u 0.8926 (0.9141) acc_u 21.8750 (11.4062) lr 1.9298e-03 eta 0:00:15
epoch [26/200] batch [45/73] time 0.516 (0.480) data 0.385 (0.349) loss_u loss_u 0.8984 (0.9127) acc_u 18.7500 (11.6667) lr 1.9298e-03 eta 0:00:13
epoch [26/200] batch [50/73] time 0.500 (0.478) data 0.370 (0.347) loss_u loss_u 0.9390 (0.9126) acc_u 9.3750 (11.6250) lr 1.9298e-03 eta 0:00:10
epoch [26/200] batch [55/73] time 0.508 (0.480) data 0.377 (0.349) loss_u loss_u 0.9214 (0.9137) acc_u 12.5000 (11.4205) lr 1.9298e-03 eta 0:00:08
epoch [26/200] batch [60/73] time 0.454 (0.478) data 0.323 (0.347) loss_u loss_u 0.9014 (0.9137) acc_u 12.5000 (11.5104) lr 1.9298e-03 eta 0:00:06
epoch [26/200] batch [65/73] time 0.399 (0.481) data 0.267 (0.350) loss_u loss_u 0.9360 (0.9140) acc_u 9.3750 (11.4904) lr 1.9298e-03 eta 0:00:03
epoch [26/200] batch [70/73] time 0.440 (0.479) data 0.308 (0.348) loss_u loss_u 0.9541 (0.9149) acc_u 6.2500 (11.4732) lr 1.9298e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1713
confident_label rate tensor(0.2443, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 766
clean true:753
clean false:13
clean_rate:0.9830287206266318
noisy true:670
noisy false:1700
after delete: len(clean_dataset) 766
after delete: len(noisy_dataset) 2370
epoch [27/200] batch [5/23] time 0.596 (0.466) data 0.466 (0.335) loss_x loss_x 1.2090 (1.3568) acc_x 71.8750 (66.2500) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [10/23] time 0.585 (0.489) data 0.454 (0.358) loss_x loss_x 1.3037 (1.2638) acc_x 68.7500 (68.7500) lr 1.9239e-03 eta 0:00:06
epoch [27/200] batch [15/23] time 0.363 (0.468) data 0.232 (0.337) loss_x loss_x 1.6543 (1.3838) acc_x 62.5000 (67.2917) lr 1.9239e-03 eta 0:00:03
epoch [27/200] batch [20/23] time 0.457 (0.472) data 0.326 (0.341) loss_x loss_x 0.9570 (1.3921) acc_x 68.7500 (65.4688) lr 1.9239e-03 eta 0:00:01
epoch [27/200] batch [5/74] time 0.456 (0.480) data 0.324 (0.349) loss_u loss_u 0.8828 (0.8637) acc_u 15.6250 (20.6250) lr 1.9239e-03 eta 0:00:33
epoch [27/200] batch [10/74] time 0.441 (0.476) data 0.310 (0.345) loss_u loss_u 0.9482 (0.8794) acc_u 9.3750 (16.5625) lr 1.9239e-03 eta 0:00:30
epoch [27/200] batch [15/74] time 0.550 (0.472) data 0.418 (0.341) loss_u loss_u 0.8833 (0.8895) acc_u 12.5000 (14.1667) lr 1.9239e-03 eta 0:00:27
epoch [27/200] batch [20/74] time 0.383 (0.466) data 0.253 (0.335) loss_u loss_u 0.9126 (0.8991) acc_u 12.5000 (13.1250) lr 1.9239e-03 eta 0:00:25
epoch [27/200] batch [25/74] time 0.508 (0.468) data 0.377 (0.336) loss_u loss_u 0.8755 (0.9018) acc_u 15.6250 (12.6250) lr 1.9239e-03 eta 0:00:22
epoch [27/200] batch [30/74] time 0.654 (0.469) data 0.522 (0.338) loss_u loss_u 0.8965 (0.8987) acc_u 15.6250 (13.2292) lr 1.9239e-03 eta 0:00:20
epoch [27/200] batch [35/74] time 0.384 (0.464) data 0.252 (0.332) loss_u loss_u 0.9375 (0.9018) acc_u 9.3750 (12.5893) lr 1.9239e-03 eta 0:00:18
epoch [27/200] batch [40/74] time 0.534 (0.470) data 0.403 (0.339) loss_u loss_u 0.9307 (0.9036) acc_u 6.2500 (12.3438) lr 1.9239e-03 eta 0:00:15
epoch [27/200] batch [45/74] time 0.509 (0.469) data 0.379 (0.338) loss_u loss_u 0.8950 (0.9033) acc_u 12.5000 (12.2222) lr 1.9239e-03 eta 0:00:13
epoch [27/200] batch [50/74] time 0.433 (0.468) data 0.302 (0.337) loss_u loss_u 0.9268 (0.9038) acc_u 9.3750 (12.2500) lr 1.9239e-03 eta 0:00:11
epoch [27/200] batch [55/74] time 0.359 (0.461) data 0.229 (0.330) loss_u loss_u 0.9131 (0.9047) acc_u 12.5000 (12.1591) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [60/74] time 0.479 (0.463) data 0.348 (0.331) loss_u loss_u 0.9487 (0.9061) acc_u 6.2500 (11.8229) lr 1.9239e-03 eta 0:00:06
epoch [27/200] batch [65/74] time 0.395 (0.459) data 0.263 (0.328) loss_u loss_u 0.8940 (0.9066) acc_u 12.5000 (11.6827) lr 1.9239e-03 eta 0:00:04
epoch [27/200] batch [70/74] time 0.608 (0.458) data 0.477 (0.327) loss_u loss_u 0.9009 (0.9074) acc_u 18.7500 (11.6071) lr 1.9239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1717
confident_label rate tensor(0.2474, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 776
clean true:770
clean false:6
clean_rate:0.9922680412371134
noisy true:649
noisy false:1711
after delete: len(clean_dataset) 776
after delete: len(noisy_dataset) 2360
epoch [28/200] batch [5/24] time 0.373 (0.510) data 0.243 (0.380) loss_x loss_x 1.5322 (1.3609) acc_x 59.3750 (63.1250) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [10/24] time 0.409 (0.514) data 0.278 (0.384) loss_x loss_x 1.5215 (1.3613) acc_x 65.6250 (62.8125) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [15/24] time 0.604 (0.510) data 0.472 (0.380) loss_x loss_x 0.9678 (1.3446) acc_x 78.1250 (64.5833) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [20/24] time 0.459 (0.508) data 0.329 (0.378) loss_x loss_x 1.3496 (1.3534) acc_x 62.5000 (64.5312) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [5/73] time 0.383 (0.486) data 0.252 (0.355) loss_u loss_u 0.9053 (0.9196) acc_u 15.6250 (11.2500) lr 1.9178e-03 eta 0:00:33
epoch [28/200] batch [10/73] time 0.516 (0.486) data 0.386 (0.356) loss_u loss_u 0.8999 (0.9040) acc_u 12.5000 (12.8125) lr 1.9178e-03 eta 0:00:30
epoch [28/200] batch [15/73] time 0.380 (0.476) data 0.250 (0.345) loss_u loss_u 0.9233 (0.8998) acc_u 15.6250 (13.3333) lr 1.9178e-03 eta 0:00:27
epoch [28/200] batch [20/73] time 0.401 (0.477) data 0.270 (0.347) loss_u loss_u 0.8784 (0.8965) acc_u 18.7500 (13.7500) lr 1.9178e-03 eta 0:00:25
epoch [28/200] batch [25/73] time 0.533 (0.474) data 0.401 (0.343) loss_u loss_u 0.9233 (0.8962) acc_u 12.5000 (14.0000) lr 1.9178e-03 eta 0:00:22
epoch [28/200] batch [30/73] time 0.743 (0.474) data 0.611 (0.343) loss_u loss_u 0.9019 (0.9007) acc_u 15.6250 (13.2292) lr 1.9178e-03 eta 0:00:20
epoch [28/200] batch [35/73] time 0.390 (0.467) data 0.258 (0.336) loss_u loss_u 0.9277 (0.9030) acc_u 6.2500 (12.4107) lr 1.9178e-03 eta 0:00:17
epoch [28/200] batch [40/73] time 0.379 (0.464) data 0.249 (0.333) loss_u loss_u 0.9014 (0.9061) acc_u 12.5000 (11.9531) lr 1.9178e-03 eta 0:00:15
epoch [28/200] batch [45/73] time 0.419 (0.462) data 0.282 (0.331) loss_u loss_u 0.9360 (0.9076) acc_u 9.3750 (11.7361) lr 1.9178e-03 eta 0:00:12
epoch [28/200] batch [50/73] time 0.377 (0.458) data 0.246 (0.326) loss_u loss_u 0.9087 (0.9087) acc_u 9.3750 (11.6250) lr 1.9178e-03 eta 0:00:10
epoch [28/200] batch [55/73] time 0.531 (0.457) data 0.401 (0.326) loss_u loss_u 0.9199 (0.9103) acc_u 12.5000 (11.4773) lr 1.9178e-03 eta 0:00:08
epoch [28/200] batch [60/73] time 0.450 (0.456) data 0.319 (0.325) loss_u loss_u 0.9551 (0.9099) acc_u 9.3750 (11.4583) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [65/73] time 0.347 (0.458) data 0.216 (0.327) loss_u loss_u 0.9038 (0.9085) acc_u 12.5000 (11.5865) lr 1.9178e-03 eta 0:00:03
epoch [28/200] batch [70/73] time 0.403 (0.456) data 0.271 (0.325) loss_u loss_u 0.9565 (0.9107) acc_u 6.2500 (11.3839) lr 1.9178e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1698
confident_label rate tensor(0.2430, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 762
clean true:756
clean false:6
clean_rate:0.9921259842519685
noisy true:682
noisy false:1692
after delete: len(clean_dataset) 762
after delete: len(noisy_dataset) 2374
epoch [29/200] batch [5/23] time 0.585 (0.593) data 0.454 (0.462) loss_x loss_x 1.5020 (1.3305) acc_x 50.0000 (64.3750) lr 1.9114e-03 eta 0:00:10
epoch [29/200] batch [10/23] time 0.569 (0.542) data 0.438 (0.411) loss_x loss_x 2.1133 (1.4008) acc_x 56.2500 (63.1250) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [15/23] time 0.455 (0.518) data 0.324 (0.387) loss_x loss_x 1.0332 (1.3382) acc_x 78.1250 (65.6250) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [20/23] time 0.427 (0.501) data 0.296 (0.370) loss_x loss_x 1.3291 (1.3332) acc_x 50.0000 (65.0000) lr 1.9114e-03 eta 0:00:01
epoch [29/200] batch [5/74] time 0.347 (0.488) data 0.216 (0.357) loss_u loss_u 0.9243 (0.9243) acc_u 9.3750 (10.0000) lr 1.9114e-03 eta 0:00:33
epoch [29/200] batch [10/74] time 0.614 (0.487) data 0.482 (0.356) loss_u loss_u 0.8926 (0.9175) acc_u 12.5000 (10.0000) lr 1.9114e-03 eta 0:00:31
epoch [29/200] batch [15/74] time 0.349 (0.473) data 0.219 (0.342) loss_u loss_u 0.9297 (0.9189) acc_u 9.3750 (9.5833) lr 1.9114e-03 eta 0:00:27
epoch [29/200] batch [20/74] time 0.369 (0.464) data 0.238 (0.333) loss_u loss_u 0.9126 (0.9202) acc_u 12.5000 (9.0625) lr 1.9114e-03 eta 0:00:25
epoch [29/200] batch [25/74] time 0.391 (0.466) data 0.259 (0.335) loss_u loss_u 0.8955 (0.9212) acc_u 12.5000 (9.3750) lr 1.9114e-03 eta 0:00:22
epoch [29/200] batch [30/74] time 0.433 (0.466) data 0.301 (0.335) loss_u loss_u 0.9121 (0.9210) acc_u 12.5000 (9.4792) lr 1.9114e-03 eta 0:00:20
epoch [29/200] batch [35/74] time 0.348 (0.465) data 0.216 (0.334) loss_u loss_u 0.9082 (0.9204) acc_u 15.6250 (9.6429) lr 1.9114e-03 eta 0:00:18
epoch [29/200] batch [40/74] time 0.493 (0.465) data 0.363 (0.334) loss_u loss_u 0.9697 (0.9190) acc_u 0.0000 (9.6094) lr 1.9114e-03 eta 0:00:15
epoch [29/200] batch [45/74] time 0.505 (0.466) data 0.372 (0.335) loss_u loss_u 0.9907 (0.9151) acc_u 0.0000 (10.3472) lr 1.9114e-03 eta 0:00:13
epoch [29/200] batch [50/74] time 0.392 (0.465) data 0.261 (0.333) loss_u loss_u 0.8853 (0.9123) acc_u 12.5000 (10.5625) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [55/74] time 0.468 (0.469) data 0.338 (0.338) loss_u loss_u 0.9087 (0.9150) acc_u 15.6250 (10.3977) lr 1.9114e-03 eta 0:00:08
epoch [29/200] batch [60/74] time 0.479 (0.465) data 0.348 (0.334) loss_u loss_u 0.9609 (0.9158) acc_u 3.1250 (10.6250) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [65/74] time 0.469 (0.465) data 0.338 (0.334) loss_u loss_u 0.9014 (0.9135) acc_u 6.2500 (10.8173) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [70/74] time 0.532 (0.463) data 0.401 (0.332) loss_u loss_u 0.8921 (0.9141) acc_u 15.6250 (10.8929) lr 1.9114e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1712
confident_label rate tensor(0.2471, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 775
clean true:770
clean false:5
clean_rate:0.9935483870967742
noisy true:654
noisy false:1707
after delete: len(clean_dataset) 775
after delete: len(noisy_dataset) 2361
epoch [30/200] batch [5/24] time 0.456 (0.479) data 0.326 (0.348) loss_x loss_x 1.3770 (1.4033) acc_x 65.6250 (64.3750) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [10/24] time 0.429 (0.463) data 0.299 (0.332) loss_x loss_x 1.1953 (1.3983) acc_x 71.8750 (65.0000) lr 1.9048e-03 eta 0:00:06
epoch [30/200] batch [15/24] time 0.363 (0.468) data 0.232 (0.338) loss_x loss_x 1.3838 (1.3861) acc_x 65.6250 (65.8333) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [20/24] time 0.556 (0.477) data 0.426 (0.347) loss_x loss_x 1.7158 (1.3923) acc_x 65.6250 (66.0938) lr 1.9048e-03 eta 0:00:01
epoch [30/200] batch [5/73] time 0.653 (0.478) data 0.521 (0.347) loss_u loss_u 0.9272 (0.9115) acc_u 12.5000 (11.8750) lr 1.9048e-03 eta 0:00:32
epoch [30/200] batch [10/73] time 0.431 (0.473) data 0.301 (0.342) loss_u loss_u 0.9800 (0.9074) acc_u 3.1250 (11.5625) lr 1.9048e-03 eta 0:00:29
epoch [30/200] batch [15/73] time 0.480 (0.471) data 0.349 (0.340) loss_u loss_u 0.9072 (0.9037) acc_u 9.3750 (12.0833) lr 1.9048e-03 eta 0:00:27
epoch [30/200] batch [20/73] time 0.343 (0.466) data 0.211 (0.335) loss_u loss_u 0.9111 (0.9035) acc_u 18.7500 (12.6562) lr 1.9048e-03 eta 0:00:24
epoch [30/200] batch [25/73] time 0.472 (0.463) data 0.341 (0.332) loss_u loss_u 0.9438 (0.9024) acc_u 6.2500 (12.7500) lr 1.9048e-03 eta 0:00:22
epoch [30/200] batch [30/73] time 0.397 (0.465) data 0.265 (0.334) loss_u loss_u 0.9077 (0.9053) acc_u 12.5000 (12.2917) lr 1.9048e-03 eta 0:00:19
epoch [30/200] batch [35/73] time 0.477 (0.463) data 0.345 (0.332) loss_u loss_u 0.9502 (0.9086) acc_u 6.2500 (11.7857) lr 1.9048e-03 eta 0:00:17
epoch [30/200] batch [40/73] time 0.610 (0.468) data 0.480 (0.337) loss_u loss_u 0.9600 (0.9105) acc_u 6.2500 (11.5625) lr 1.9048e-03 eta 0:00:15
epoch [30/200] batch [45/73] time 0.436 (0.466) data 0.304 (0.335) loss_u loss_u 0.8838 (0.9113) acc_u 12.5000 (11.2500) lr 1.9048e-03 eta 0:00:13
epoch [30/200] batch [50/73] time 0.531 (0.466) data 0.401 (0.335) loss_u loss_u 0.8652 (0.9087) acc_u 15.6250 (11.4375) lr 1.9048e-03 eta 0:00:10
epoch [30/200] batch [55/73] time 0.332 (0.462) data 0.202 (0.331) loss_u loss_u 0.9355 (0.9093) acc_u 6.2500 (11.4773) lr 1.9048e-03 eta 0:00:08
epoch [30/200] batch [60/73] time 0.413 (0.459) data 0.281 (0.328) loss_u loss_u 0.8862 (0.9090) acc_u 21.8750 (11.8229) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [65/73] time 0.368 (0.456) data 0.236 (0.325) loss_u loss_u 0.9189 (0.9097) acc_u 18.7500 (11.8750) lr 1.9048e-03 eta 0:00:03
epoch [30/200] batch [70/73] time 0.600 (0.460) data 0.470 (0.328) loss_u loss_u 0.9175 (0.9099) acc_u 12.5000 (11.9196) lr 1.9048e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1668
confident_label rate tensor(0.2557, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 802
clean true:791
clean false:11
clean_rate:0.986284289276808
noisy true:677
noisy false:1657
after delete: len(clean_dataset) 802
after delete: len(noisy_dataset) 2334
epoch [31/200] batch [5/25] time 0.500 (0.470) data 0.369 (0.339) loss_x loss_x 1.0557 (1.3335) acc_x 75.0000 (68.1250) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [10/25] time 0.448 (0.473) data 0.318 (0.342) loss_x loss_x 0.9629 (1.3462) acc_x 78.1250 (68.7500) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [15/25] time 0.591 (0.473) data 0.460 (0.342) loss_x loss_x 1.3916 (1.3361) acc_x 68.7500 (68.3333) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [20/25] time 0.464 (0.472) data 0.333 (0.341) loss_x loss_x 1.4775 (1.3539) acc_x 59.3750 (67.5000) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [25/25] time 0.399 (0.463) data 0.270 (0.332) loss_x loss_x 2.0996 (1.4001) acc_x 46.8750 (65.7500) lr 1.8980e-03 eta 0:00:00
epoch [31/200] batch [5/72] time 0.566 (0.460) data 0.436 (0.330) loss_u loss_u 0.8555 (0.9110) acc_u 15.6250 (10.6250) lr 1.8980e-03 eta 0:00:30
epoch [31/200] batch [10/72] time 0.401 (0.456) data 0.269 (0.325) loss_u loss_u 0.9116 (0.9252) acc_u 6.2500 (8.1250) lr 1.8980e-03 eta 0:00:28
epoch [31/200] batch [15/72] time 0.402 (0.450) data 0.271 (0.319) loss_u loss_u 0.9751 (0.9332) acc_u 6.2500 (7.2917) lr 1.8980e-03 eta 0:00:25
epoch [31/200] batch [20/72] time 0.572 (0.448) data 0.442 (0.317) loss_u loss_u 0.9551 (0.9303) acc_u 6.2500 (8.2812) lr 1.8980e-03 eta 0:00:23
epoch [31/200] batch [25/72] time 0.327 (0.444) data 0.196 (0.313) loss_u loss_u 0.9258 (0.9312) acc_u 12.5000 (8.3750) lr 1.8980e-03 eta 0:00:20
epoch [31/200] batch [30/72] time 0.668 (0.452) data 0.538 (0.321) loss_u loss_u 0.9150 (0.9270) acc_u 9.3750 (8.8542) lr 1.8980e-03 eta 0:00:18
epoch [31/200] batch [35/72] time 0.476 (0.450) data 0.345 (0.319) loss_u loss_u 0.8979 (0.9260) acc_u 12.5000 (9.0179) lr 1.8980e-03 eta 0:00:16
epoch [31/200] batch [40/72] time 0.570 (0.451) data 0.440 (0.320) loss_u loss_u 0.9209 (0.9273) acc_u 12.5000 (8.9062) lr 1.8980e-03 eta 0:00:14
epoch [31/200] batch [45/72] time 0.406 (0.450) data 0.276 (0.319) loss_u loss_u 0.9316 (0.9230) acc_u 6.2500 (9.3750) lr 1.8980e-03 eta 0:00:12
epoch [31/200] batch [50/72] time 0.443 (0.448) data 0.311 (0.317) loss_u loss_u 0.8472 (0.9209) acc_u 18.7500 (9.7500) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [55/72] time 0.548 (0.451) data 0.418 (0.320) loss_u loss_u 0.9312 (0.9213) acc_u 12.5000 (9.7727) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [60/72] time 0.516 (0.450) data 0.384 (0.319) loss_u loss_u 0.9390 (0.9218) acc_u 9.3750 (9.9479) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [65/72] time 0.513 (0.448) data 0.382 (0.317) loss_u loss_u 0.9351 (0.9217) acc_u 6.2500 (9.8558) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [70/72] time 0.446 (0.447) data 0.315 (0.316) loss_u loss_u 0.9014 (0.9202) acc_u 9.3750 (10.0446) lr 1.8980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1653
confident_label rate tensor(0.2532, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 794
clean true:789
clean false:5
clean_rate:0.9937027707808564
noisy true:694
noisy false:1648
after delete: len(clean_dataset) 794
after delete: len(noisy_dataset) 2342
epoch [32/200] batch [5/24] time 0.520 (0.515) data 0.390 (0.384) loss_x loss_x 1.2188 (1.3426) acc_x 65.6250 (65.0000) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [10/24] time 0.470 (0.494) data 0.339 (0.363) loss_x loss_x 1.1982 (1.4052) acc_x 75.0000 (64.6875) lr 1.8910e-03 eta 0:00:06
epoch [32/200] batch [15/24] time 0.480 (0.475) data 0.348 (0.344) loss_x loss_x 1.9561 (1.4251) acc_x 50.0000 (64.7917) lr 1.8910e-03 eta 0:00:04
epoch [32/200] batch [20/24] time 0.399 (0.470) data 0.268 (0.339) loss_x loss_x 1.3936 (1.4387) acc_x 68.7500 (63.9062) lr 1.8910e-03 eta 0:00:01
epoch [32/200] batch [5/73] time 0.600 (0.462) data 0.468 (0.331) loss_u loss_u 0.9341 (0.9098) acc_u 6.2500 (10.6250) lr 1.8910e-03 eta 0:00:31
epoch [32/200] batch [10/73] time 0.356 (0.459) data 0.225 (0.328) loss_u loss_u 0.8960 (0.9139) acc_u 12.5000 (10.0000) lr 1.8910e-03 eta 0:00:28
epoch [32/200] batch [15/73] time 0.411 (0.455) data 0.280 (0.324) loss_u loss_u 0.9238 (0.9078) acc_u 9.3750 (11.0417) lr 1.8910e-03 eta 0:00:26
epoch [32/200] batch [20/73] time 0.395 (0.453) data 0.263 (0.322) loss_u loss_u 0.9492 (0.9091) acc_u 6.2500 (10.9375) lr 1.8910e-03 eta 0:00:24
epoch [32/200] batch [25/73] time 0.385 (0.447) data 0.253 (0.316) loss_u loss_u 0.8330 (0.9083) acc_u 21.8750 (11.1250) lr 1.8910e-03 eta 0:00:21
epoch [32/200] batch [30/73] time 0.448 (0.445) data 0.316 (0.314) loss_u loss_u 0.9136 (0.9113) acc_u 15.6250 (11.0417) lr 1.8910e-03 eta 0:00:19
epoch [32/200] batch [35/73] time 0.370 (0.451) data 0.239 (0.319) loss_u loss_u 0.9707 (0.9121) acc_u 6.2500 (10.8929) lr 1.8910e-03 eta 0:00:17
epoch [32/200] batch [40/73] time 0.414 (0.448) data 0.282 (0.317) loss_u loss_u 0.9126 (0.9110) acc_u 9.3750 (10.9375) lr 1.8910e-03 eta 0:00:14
epoch [32/200] batch [45/73] time 0.491 (0.452) data 0.360 (0.321) loss_u loss_u 0.9468 (0.9114) acc_u 3.1250 (10.9722) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [50/73] time 0.451 (0.457) data 0.319 (0.326) loss_u loss_u 0.9189 (0.9099) acc_u 6.2500 (11.1250) lr 1.8910e-03 eta 0:00:10
epoch [32/200] batch [55/73] time 0.435 (0.458) data 0.303 (0.327) loss_u loss_u 0.9033 (0.9105) acc_u 9.3750 (11.0795) lr 1.8910e-03 eta 0:00:08
epoch [32/200] batch [60/73] time 0.412 (0.457) data 0.280 (0.325) loss_u loss_u 0.9370 (0.9120) acc_u 9.3750 (10.8854) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [65/73] time 0.381 (0.455) data 0.250 (0.324) loss_u loss_u 0.8496 (0.9122) acc_u 25.0000 (11.1538) lr 1.8910e-03 eta 0:00:03
epoch [32/200] batch [70/73] time 0.616 (0.457) data 0.484 (0.326) loss_u loss_u 0.8979 (0.9115) acc_u 12.5000 (11.2054) lr 1.8910e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1718
confident_label rate tensor(0.2487, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 780
clean true:774
clean false:6
clean_rate:0.9923076923076923
noisy true:644
noisy false:1712
after delete: len(clean_dataset) 780
after delete: len(noisy_dataset) 2356
epoch [33/200] batch [5/24] time 0.411 (0.464) data 0.279 (0.333) loss_x loss_x 1.0742 (1.2707) acc_x 71.8750 (66.8750) lr 1.8838e-03 eta 0:00:08
epoch [33/200] batch [10/24] time 0.509 (0.486) data 0.378 (0.355) loss_x loss_x 1.2646 (1.2986) acc_x 65.6250 (67.5000) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [15/24] time 0.572 (0.502) data 0.440 (0.372) loss_x loss_x 1.1729 (1.2263) acc_x 65.6250 (69.5833) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [20/24] time 0.451 (0.507) data 0.321 (0.376) loss_x loss_x 1.3242 (1.2482) acc_x 68.7500 (68.5938) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [5/73] time 0.425 (0.499) data 0.294 (0.368) loss_u loss_u 0.8706 (0.9110) acc_u 15.6250 (11.8750) lr 1.8838e-03 eta 0:00:33
epoch [33/200] batch [10/73] time 0.385 (0.485) data 0.253 (0.354) loss_u loss_u 0.9150 (0.9062) acc_u 15.6250 (13.1250) lr 1.8838e-03 eta 0:00:30
epoch [33/200] batch [15/73] time 0.490 (0.478) data 0.357 (0.347) loss_u loss_u 0.8618 (0.9046) acc_u 18.7500 (12.7083) lr 1.8838e-03 eta 0:00:27
epoch [33/200] batch [20/73] time 0.381 (0.472) data 0.250 (0.341) loss_u loss_u 0.9277 (0.9074) acc_u 6.2500 (11.8750) lr 1.8838e-03 eta 0:00:25
epoch [33/200] batch [25/73] time 0.504 (0.476) data 0.373 (0.344) loss_u loss_u 0.9292 (0.9019) acc_u 18.7500 (13.1250) lr 1.8838e-03 eta 0:00:22
epoch [33/200] batch [30/73] time 0.464 (0.472) data 0.332 (0.341) loss_u loss_u 0.9102 (0.9050) acc_u 12.5000 (12.9167) lr 1.8838e-03 eta 0:00:20
epoch [33/200] batch [35/73] time 0.419 (0.469) data 0.287 (0.337) loss_u loss_u 0.9321 (0.9031) acc_u 9.3750 (12.8571) lr 1.8838e-03 eta 0:00:17
epoch [33/200] batch [40/73] time 0.628 (0.473) data 0.496 (0.342) loss_u loss_u 0.9473 (0.9064) acc_u 6.2500 (12.3438) lr 1.8838e-03 eta 0:00:15
epoch [33/200] batch [45/73] time 0.601 (0.475) data 0.470 (0.344) loss_u loss_u 0.9067 (0.9056) acc_u 9.3750 (12.4306) lr 1.8838e-03 eta 0:00:13
epoch [33/200] batch [50/73] time 0.450 (0.476) data 0.320 (0.345) loss_u loss_u 0.9307 (0.9077) acc_u 6.2500 (12.0625) lr 1.8838e-03 eta 0:00:10
epoch [33/200] batch [55/73] time 0.398 (0.472) data 0.267 (0.341) loss_u loss_u 0.9692 (0.9099) acc_u 0.0000 (11.7614) lr 1.8838e-03 eta 0:00:08
epoch [33/200] batch [60/73] time 0.599 (0.476) data 0.468 (0.345) loss_u loss_u 0.9214 (0.9099) acc_u 9.3750 (11.7188) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [65/73] time 0.403 (0.475) data 0.272 (0.344) loss_u loss_u 0.9326 (0.9098) acc_u 9.3750 (11.6346) lr 1.8838e-03 eta 0:00:03
epoch [33/200] batch [70/73] time 0.374 (0.474) data 0.241 (0.343) loss_u loss_u 0.8940 (0.9100) acc_u 15.6250 (11.6071) lr 1.8838e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1622
confident_label rate tensor(0.2666, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 836
clean true:827
clean false:9
clean_rate:0.9892344497607656
noisy true:687
noisy false:1613
after delete: len(clean_dataset) 836
after delete: len(noisy_dataset) 2300
epoch [34/200] batch [5/26] time 0.396 (0.496) data 0.266 (0.366) loss_x loss_x 1.3428 (1.4115) acc_x 62.5000 (62.5000) lr 1.8763e-03 eta 0:00:10
epoch [34/200] batch [10/26] time 0.360 (0.458) data 0.228 (0.327) loss_x loss_x 1.0117 (1.4376) acc_x 75.0000 (65.3125) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [15/26] time 0.451 (0.475) data 0.320 (0.344) loss_x loss_x 0.8672 (1.3890) acc_x 78.1250 (66.4583) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [20/26] time 0.501 (0.488) data 0.370 (0.357) loss_x loss_x 1.4365 (1.4436) acc_x 68.7500 (63.5938) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [25/26] time 0.407 (0.482) data 0.277 (0.351) loss_x loss_x 1.4404 (1.4536) acc_x 62.5000 (63.5000) lr 1.8763e-03 eta 0:00:00
epoch [34/200] batch [5/71] time 0.489 (0.472) data 0.358 (0.341) loss_u loss_u 0.9434 (0.9246) acc_u 6.2500 (9.3750) lr 1.8763e-03 eta 0:00:31
epoch [34/200] batch [10/71] time 0.476 (0.470) data 0.346 (0.339) loss_u loss_u 0.8970 (0.9144) acc_u 12.5000 (10.9375) lr 1.8763e-03 eta 0:00:28
epoch [34/200] batch [15/71] time 0.401 (0.465) data 0.269 (0.334) loss_u loss_u 0.8257 (0.9133) acc_u 28.1250 (11.6667) lr 1.8763e-03 eta 0:00:26
epoch [34/200] batch [20/71] time 0.628 (0.469) data 0.497 (0.338) loss_u loss_u 0.9570 (0.9190) acc_u 3.1250 (10.6250) lr 1.8763e-03 eta 0:00:23
epoch [34/200] batch [25/71] time 0.434 (0.472) data 0.304 (0.341) loss_u loss_u 0.9653 (0.9228) acc_u 6.2500 (10.0000) lr 1.8763e-03 eta 0:00:21
epoch [34/200] batch [30/71] time 0.404 (0.467) data 0.273 (0.336) loss_u loss_u 0.8057 (0.9150) acc_u 28.1250 (11.0417) lr 1.8763e-03 eta 0:00:19
epoch [34/200] batch [35/71] time 0.484 (0.466) data 0.353 (0.335) loss_u loss_u 0.8945 (0.9132) acc_u 15.6250 (11.3393) lr 1.8763e-03 eta 0:00:16
epoch [34/200] batch [40/71] time 0.410 (0.462) data 0.278 (0.331) loss_u loss_u 0.9009 (0.9114) acc_u 6.2500 (11.3281) lr 1.8763e-03 eta 0:00:14
epoch [34/200] batch [45/71] time 0.481 (0.466) data 0.350 (0.334) loss_u loss_u 0.9390 (0.9095) acc_u 6.2500 (11.5972) lr 1.8763e-03 eta 0:00:12
epoch [34/200] batch [50/71] time 0.496 (0.465) data 0.366 (0.334) loss_u loss_u 0.9224 (0.9098) acc_u 6.2500 (11.5625) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [55/71] time 0.472 (0.464) data 0.340 (0.333) loss_u loss_u 0.8613 (0.9122) acc_u 18.7500 (11.1932) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [60/71] time 0.371 (0.461) data 0.240 (0.330) loss_u loss_u 0.8979 (0.9120) acc_u 12.5000 (11.1979) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [65/71] time 0.360 (0.458) data 0.228 (0.327) loss_u loss_u 0.9243 (0.9128) acc_u 9.3750 (11.1538) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [70/71] time 0.463 (0.462) data 0.330 (0.331) loss_u loss_u 0.8613 (0.9104) acc_u 21.8750 (11.6964) lr 1.8763e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1640
confident_label rate tensor(0.2634, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 826
clean true:820
clean false:6
clean_rate:0.9927360774818402
noisy true:676
noisy false:1634
after delete: len(clean_dataset) 826
after delete: len(noisy_dataset) 2310
epoch [35/200] batch [5/25] time 0.624 (0.490) data 0.494 (0.359) loss_x loss_x 1.0156 (1.2641) acc_x 78.1250 (73.1250) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [10/25] time 0.650 (0.488) data 0.520 (0.357) loss_x loss_x 1.5859 (1.3131) acc_x 65.6250 (69.6875) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [15/25] time 0.386 (0.499) data 0.256 (0.368) loss_x loss_x 1.0908 (1.4055) acc_x 71.8750 (67.0833) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [20/25] time 0.494 (0.493) data 0.364 (0.362) loss_x loss_x 1.5273 (1.3745) acc_x 68.7500 (67.9688) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [25/25] time 0.397 (0.489) data 0.267 (0.359) loss_x loss_x 1.0625 (1.2960) acc_x 65.6250 (68.8750) lr 1.8686e-03 eta 0:00:00
epoch [35/200] batch [5/72] time 0.292 (0.478) data 0.161 (0.347) loss_u loss_u 0.9019 (0.8877) acc_u 15.6250 (14.3750) lr 1.8686e-03 eta 0:00:32
epoch [35/200] batch [10/72] time 0.447 (0.476) data 0.315 (0.345) loss_u loss_u 0.9072 (0.9076) acc_u 9.3750 (11.2500) lr 1.8686e-03 eta 0:00:29
epoch [35/200] batch [15/72] time 0.397 (0.468) data 0.265 (0.337) loss_u loss_u 0.9185 (0.9204) acc_u 9.3750 (10.0000) lr 1.8686e-03 eta 0:00:26
epoch [35/200] batch [20/72] time 0.478 (0.465) data 0.347 (0.334) loss_u loss_u 0.9072 (0.9162) acc_u 9.3750 (10.4688) lr 1.8686e-03 eta 0:00:24
epoch [35/200] batch [25/72] time 0.455 (0.463) data 0.323 (0.332) loss_u loss_u 0.8662 (0.9120) acc_u 21.8750 (11.0000) lr 1.8686e-03 eta 0:00:21
epoch [35/200] batch [30/72] time 0.534 (0.466) data 0.403 (0.335) loss_u loss_u 0.9194 (0.9169) acc_u 9.3750 (10.0000) lr 1.8686e-03 eta 0:00:19
epoch [35/200] batch [35/72] time 0.663 (0.470) data 0.532 (0.339) loss_u loss_u 0.9170 (0.9190) acc_u 9.3750 (9.6429) lr 1.8686e-03 eta 0:00:17
epoch [35/200] batch [40/72] time 0.456 (0.471) data 0.325 (0.339) loss_u loss_u 0.9375 (0.9199) acc_u 9.3750 (9.6094) lr 1.8686e-03 eta 0:00:15
epoch [35/200] batch [45/72] time 0.546 (0.472) data 0.415 (0.341) loss_u loss_u 0.9175 (0.9178) acc_u 12.5000 (10.0694) lr 1.8686e-03 eta 0:00:12
epoch [35/200] batch [50/72] time 0.417 (0.473) data 0.285 (0.341) loss_u loss_u 0.9438 (0.9161) acc_u 9.3750 (10.4375) lr 1.8686e-03 eta 0:00:10
epoch [35/200] batch [55/72] time 0.501 (0.474) data 0.371 (0.342) loss_u loss_u 0.9604 (0.9173) acc_u 6.2500 (10.5682) lr 1.8686e-03 eta 0:00:08
epoch [35/200] batch [60/72] time 0.553 (0.475) data 0.421 (0.344) loss_u loss_u 0.9385 (0.9161) acc_u 9.3750 (10.7812) lr 1.8686e-03 eta 0:00:05
epoch [35/200] batch [65/72] time 0.414 (0.472) data 0.281 (0.341) loss_u loss_u 0.9282 (0.9168) acc_u 9.3750 (10.5288) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [70/72] time 0.537 (0.470) data 0.406 (0.339) loss_u loss_u 0.9526 (0.9160) acc_u 9.3750 (10.8482) lr 1.8686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1597
confident_label rate tensor(0.2688, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 843
clean true:837
clean false:6
clean_rate:0.9928825622775801
noisy true:702
noisy false:1591
after delete: len(clean_dataset) 843
after delete: len(noisy_dataset) 2293
epoch [36/200] batch [5/26] time 0.498 (0.485) data 0.367 (0.347) loss_x loss_x 1.4473 (1.0708) acc_x 75.0000 (76.8750) lr 1.8607e-03 eta 0:00:10
epoch [36/200] batch [10/26] time 0.519 (0.503) data 0.389 (0.369) loss_x loss_x 1.1455 (1.1384) acc_x 65.6250 (71.8750) lr 1.8607e-03 eta 0:00:08
epoch [36/200] batch [15/26] time 0.449 (0.483) data 0.319 (0.350) loss_x loss_x 1.1230 (1.1749) acc_x 62.5000 (69.3750) lr 1.8607e-03 eta 0:00:05
epoch [36/200] batch [20/26] time 0.464 (0.481) data 0.334 (0.349) loss_x loss_x 1.2686 (1.2636) acc_x 75.0000 (67.1875) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [25/26] time 0.423 (0.475) data 0.293 (0.343) loss_x loss_x 1.2920 (1.3025) acc_x 65.6250 (65.6250) lr 1.8607e-03 eta 0:00:00
epoch [36/200] batch [5/71] time 0.411 (0.477) data 0.280 (0.345) loss_u loss_u 0.9395 (0.9116) acc_u 9.3750 (11.8750) lr 1.8607e-03 eta 0:00:31
epoch [36/200] batch [10/71] time 0.386 (0.467) data 0.256 (0.335) loss_u loss_u 0.8628 (0.9107) acc_u 15.6250 (10.9375) lr 1.8607e-03 eta 0:00:28
epoch [36/200] batch [15/71] time 0.497 (0.464) data 0.367 (0.332) loss_u loss_u 0.9385 (0.9165) acc_u 6.2500 (10.0000) lr 1.8607e-03 eta 0:00:25
epoch [36/200] batch [20/71] time 0.479 (0.462) data 0.349 (0.330) loss_u loss_u 0.8745 (0.9093) acc_u 15.6250 (11.2500) lr 1.8607e-03 eta 0:00:23
epoch [36/200] batch [25/71] time 0.403 (0.463) data 0.273 (0.331) loss_u loss_u 0.9644 (0.9156) acc_u 6.2500 (10.3750) lr 1.8607e-03 eta 0:00:21
epoch [36/200] batch [30/71] time 0.573 (0.465) data 0.443 (0.333) loss_u loss_u 0.9150 (0.9166) acc_u 15.6250 (10.4167) lr 1.8607e-03 eta 0:00:19
epoch [36/200] batch [35/71] time 0.400 (0.465) data 0.269 (0.334) loss_u loss_u 0.8877 (0.9155) acc_u 15.6250 (10.8036) lr 1.8607e-03 eta 0:00:16
epoch [36/200] batch [40/71] time 0.310 (0.460) data 0.180 (0.329) loss_u loss_u 0.9209 (0.9155) acc_u 15.6250 (10.8594) lr 1.8607e-03 eta 0:00:14
epoch [36/200] batch [45/71] time 0.538 (0.459) data 0.404 (0.327) loss_u loss_u 0.8901 (0.9144) acc_u 12.5000 (10.9722) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [50/71] time 0.509 (0.461) data 0.377 (0.330) loss_u loss_u 0.9253 (0.9140) acc_u 9.3750 (10.7500) lr 1.8607e-03 eta 0:00:09
epoch [36/200] batch [55/71] time 0.557 (0.465) data 0.425 (0.333) loss_u loss_u 0.8657 (0.9140) acc_u 18.7500 (10.7955) lr 1.8607e-03 eta 0:00:07
epoch [36/200] batch [60/71] time 0.442 (0.463) data 0.310 (0.332) loss_u loss_u 0.9541 (0.9138) acc_u 6.2500 (10.9375) lr 1.8607e-03 eta 0:00:05
epoch [36/200] batch [65/71] time 0.462 (0.462) data 0.330 (0.331) loss_u loss_u 0.9546 (0.9154) acc_u 3.1250 (10.6250) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [70/71] time 0.362 (0.462) data 0.230 (0.330) loss_u loss_u 0.9717 (0.9156) acc_u 0.0000 (10.5804) lr 1.8607e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1638
confident_label rate tensor(0.2634, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 826
clean true:819
clean false:7
clean_rate:0.9915254237288136
noisy true:679
noisy false:1631
after delete: len(clean_dataset) 826
after delete: len(noisy_dataset) 2310
epoch [37/200] batch [5/25] time 0.610 (0.496) data 0.479 (0.364) loss_x loss_x 1.2969 (1.3416) acc_x 62.5000 (65.6250) lr 1.8526e-03 eta 0:00:09
epoch [37/200] batch [10/25] time 0.424 (0.487) data 0.293 (0.355) loss_x loss_x 1.2109 (1.2884) acc_x 65.6250 (67.5000) lr 1.8526e-03 eta 0:00:07
epoch [37/200] batch [15/25] time 0.460 (0.485) data 0.329 (0.354) loss_x loss_x 0.8955 (1.2453) acc_x 84.3750 (70.0000) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [20/25] time 0.527 (0.482) data 0.396 (0.351) loss_x loss_x 1.5723 (1.3237) acc_x 56.2500 (66.7188) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [25/25] time 0.444 (0.486) data 0.312 (0.354) loss_x loss_x 1.5020 (1.3082) acc_x 68.7500 (67.1250) lr 1.8526e-03 eta 0:00:00
epoch [37/200] batch [5/72] time 0.488 (0.497) data 0.357 (0.366) loss_u loss_u 0.9224 (0.8972) acc_u 6.2500 (11.8750) lr 1.8526e-03 eta 0:00:33
epoch [37/200] batch [10/72] time 0.456 (0.487) data 0.326 (0.356) loss_u loss_u 0.9111 (0.9122) acc_u 12.5000 (10.9375) lr 1.8526e-03 eta 0:00:30
epoch [37/200] batch [15/72] time 0.473 (0.480) data 0.342 (0.348) loss_u loss_u 0.9541 (0.9122) acc_u 3.1250 (11.4583) lr 1.8526e-03 eta 0:00:27
epoch [37/200] batch [20/72] time 0.438 (0.477) data 0.306 (0.346) loss_u loss_u 0.9434 (0.9168) acc_u 9.3750 (10.6250) lr 1.8526e-03 eta 0:00:24
epoch [37/200] batch [25/72] time 0.471 (0.468) data 0.340 (0.337) loss_u loss_u 0.9854 (0.9176) acc_u 0.0000 (10.2500) lr 1.8526e-03 eta 0:00:22
epoch [37/200] batch [30/72] time 0.461 (0.465) data 0.330 (0.334) loss_u loss_u 0.9238 (0.9193) acc_u 6.2500 (9.8958) lr 1.8526e-03 eta 0:00:19
epoch [37/200] batch [35/72] time 0.386 (0.465) data 0.254 (0.334) loss_u loss_u 0.9565 (0.9199) acc_u 6.2500 (9.8214) lr 1.8526e-03 eta 0:00:17
epoch [37/200] batch [40/72] time 0.395 (0.463) data 0.265 (0.332) loss_u loss_u 0.9688 (0.9197) acc_u 9.3750 (9.7656) lr 1.8526e-03 eta 0:00:14
epoch [37/200] batch [45/72] time 0.377 (0.457) data 0.245 (0.325) loss_u loss_u 0.8804 (0.9176) acc_u 15.6250 (10.1389) lr 1.8526e-03 eta 0:00:12
epoch [37/200] batch [50/72] time 0.615 (0.458) data 0.485 (0.327) loss_u loss_u 0.9316 (0.9185) acc_u 12.5000 (10.1250) lr 1.8526e-03 eta 0:00:10
epoch [37/200] batch [55/72] time 0.517 (0.459) data 0.386 (0.327) loss_u loss_u 0.9170 (0.9157) acc_u 12.5000 (10.6250) lr 1.8526e-03 eta 0:00:07
epoch [37/200] batch [60/72] time 0.470 (0.458) data 0.338 (0.327) loss_u loss_u 0.8076 (0.9136) acc_u 18.7500 (10.9896) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [65/72] time 0.421 (0.457) data 0.288 (0.326) loss_u loss_u 0.9058 (0.9146) acc_u 12.5000 (10.9615) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [70/72] time 0.501 (0.461) data 0.369 (0.330) loss_u loss_u 0.9429 (0.9154) acc_u 12.5000 (11.1161) lr 1.8526e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1648
confident_label rate tensor(0.2529, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 793
clean true:786
clean false:7
clean_rate:0.9911727616645649
noisy true:702
noisy false:1641
after delete: len(clean_dataset) 793
after delete: len(noisy_dataset) 2343
epoch [38/200] batch [5/24] time 0.452 (0.512) data 0.322 (0.381) loss_x loss_x 0.8867 (1.4180) acc_x 81.2500 (64.3750) lr 1.8443e-03 eta 0:00:09
epoch [38/200] batch [10/24] time 0.369 (0.508) data 0.239 (0.377) loss_x loss_x 1.3926 (1.3449) acc_x 65.6250 (66.2500) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [15/24] time 0.450 (0.502) data 0.320 (0.372) loss_x loss_x 1.2520 (1.3251) acc_x 62.5000 (66.8750) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [20/24] time 0.429 (0.494) data 0.298 (0.364) loss_x loss_x 1.4512 (1.3154) acc_x 59.3750 (67.0312) lr 1.8443e-03 eta 0:00:01
epoch [38/200] batch [5/73] time 0.455 (0.497) data 0.325 (0.366) loss_u loss_u 0.9536 (0.9354) acc_u 9.3750 (9.3750) lr 1.8443e-03 eta 0:00:33
epoch [38/200] batch [10/73] time 0.393 (0.492) data 0.262 (0.362) loss_u loss_u 0.8911 (0.9286) acc_u 15.6250 (9.6875) lr 1.8443e-03 eta 0:00:31
epoch [38/200] batch [15/73] time 0.423 (0.502) data 0.292 (0.371) loss_u loss_u 0.8857 (0.9231) acc_u 15.6250 (10.2083) lr 1.8443e-03 eta 0:00:29
epoch [38/200] batch [20/73] time 0.445 (0.492) data 0.314 (0.362) loss_u loss_u 0.8901 (0.9166) acc_u 15.6250 (11.0938) lr 1.8443e-03 eta 0:00:26
epoch [38/200] batch [25/73] time 0.348 (0.483) data 0.217 (0.352) loss_u loss_u 0.8906 (0.9195) acc_u 15.6250 (11.2500) lr 1.8443e-03 eta 0:00:23
epoch [38/200] batch [30/73] time 0.401 (0.473) data 0.269 (0.342) loss_u loss_u 0.8335 (0.9201) acc_u 25.0000 (10.8333) lr 1.8443e-03 eta 0:00:20
epoch [38/200] batch [35/73] time 0.655 (0.472) data 0.524 (0.341) loss_u loss_u 0.8994 (0.9202) acc_u 12.5000 (10.5357) lr 1.8443e-03 eta 0:00:17
epoch [38/200] batch [40/73] time 0.453 (0.471) data 0.321 (0.340) loss_u loss_u 0.9360 (0.9190) acc_u 15.6250 (10.7812) lr 1.8443e-03 eta 0:00:15
epoch [38/200] batch [45/73] time 0.467 (0.467) data 0.336 (0.336) loss_u loss_u 0.8901 (0.9162) acc_u 12.5000 (10.9028) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [50/73] time 0.370 (0.463) data 0.238 (0.332) loss_u loss_u 0.9365 (0.9148) acc_u 9.3750 (11.2500) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [55/73] time 0.495 (0.466) data 0.362 (0.335) loss_u loss_u 0.9248 (0.9163) acc_u 9.3750 (10.9091) lr 1.8443e-03 eta 0:00:08
epoch [38/200] batch [60/73] time 0.485 (0.467) data 0.352 (0.336) loss_u loss_u 0.9741 (0.9164) acc_u 6.2500 (10.8333) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [65/73] time 0.360 (0.463) data 0.228 (0.332) loss_u loss_u 0.8975 (0.9131) acc_u 12.5000 (11.1058) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [70/73] time 0.725 (0.467) data 0.595 (0.336) loss_u loss_u 0.8599 (0.9107) acc_u 21.8750 (11.4732) lr 1.8443e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1674
confident_label rate tensor(0.2554, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 801
clean true:796
clean false:5
clean_rate:0.9937578027465668
noisy true:666
noisy false:1669
after delete: len(clean_dataset) 801
after delete: len(noisy_dataset) 2335
epoch [39/200] batch [5/25] time 0.430 (0.519) data 0.299 (0.388) loss_x loss_x 1.5361 (1.4118) acc_x 65.6250 (66.2500) lr 1.8358e-03 eta 0:00:10
epoch [39/200] batch [10/25] time 0.571 (0.509) data 0.441 (0.379) loss_x loss_x 0.8872 (1.3593) acc_x 78.1250 (66.8750) lr 1.8358e-03 eta 0:00:07
epoch [39/200] batch [15/25] time 0.451 (0.488) data 0.320 (0.357) loss_x loss_x 1.2910 (1.3023) acc_x 62.5000 (68.1250) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [20/25] time 0.435 (0.493) data 0.303 (0.363) loss_x loss_x 1.8086 (1.3322) acc_x 62.5000 (67.0312) lr 1.8358e-03 eta 0:00:02
epoch [39/200] batch [25/25] time 0.532 (0.488) data 0.402 (0.357) loss_x loss_x 1.4590 (1.3682) acc_x 59.3750 (66.2500) lr 1.8358e-03 eta 0:00:00
epoch [39/200] batch [5/72] time 0.361 (0.476) data 0.228 (0.346) loss_u loss_u 0.9526 (0.9286) acc_u 6.2500 (10.0000) lr 1.8358e-03 eta 0:00:31
epoch [39/200] batch [10/72] time 0.459 (0.471) data 0.326 (0.341) loss_u loss_u 0.9180 (0.9268) acc_u 15.6250 (10.6250) lr 1.8358e-03 eta 0:00:29
epoch [39/200] batch [15/72] time 0.409 (0.474) data 0.276 (0.343) loss_u loss_u 0.9077 (0.9143) acc_u 18.7500 (11.8750) lr 1.8358e-03 eta 0:00:26
epoch [39/200] batch [20/72] time 0.666 (0.472) data 0.534 (0.341) loss_u loss_u 0.9570 (0.9154) acc_u 6.2500 (11.7188) lr 1.8358e-03 eta 0:00:24
epoch [39/200] batch [25/72] time 0.380 (0.469) data 0.249 (0.338) loss_u loss_u 0.8755 (0.9119) acc_u 15.6250 (12.0000) lr 1.8358e-03 eta 0:00:22
epoch [39/200] batch [30/72] time 0.386 (0.468) data 0.255 (0.337) loss_u loss_u 0.9849 (0.9146) acc_u 6.2500 (11.8750) lr 1.8358e-03 eta 0:00:19
epoch [39/200] batch [35/72] time 0.470 (0.465) data 0.339 (0.334) loss_u loss_u 0.9009 (0.9130) acc_u 12.5000 (11.8750) lr 1.8358e-03 eta 0:00:17
epoch [39/200] batch [40/72] time 0.590 (0.463) data 0.455 (0.332) loss_u loss_u 0.9521 (0.9126) acc_u 6.2500 (12.1094) lr 1.8358e-03 eta 0:00:14
epoch [39/200] batch [45/72] time 0.458 (0.462) data 0.327 (0.331) loss_u loss_u 0.9604 (0.9119) acc_u 6.2500 (12.2917) lr 1.8358e-03 eta 0:00:12
epoch [39/200] batch [50/72] time 0.378 (0.461) data 0.247 (0.329) loss_u loss_u 0.9351 (0.9118) acc_u 6.2500 (12.1875) lr 1.8358e-03 eta 0:00:10
epoch [39/200] batch [55/72] time 0.396 (0.461) data 0.265 (0.330) loss_u loss_u 0.9683 (0.9127) acc_u 6.2500 (12.0455) lr 1.8358e-03 eta 0:00:07
epoch [39/200] batch [60/72] time 0.498 (0.462) data 0.366 (0.331) loss_u loss_u 0.9746 (0.9125) acc_u 3.1250 (11.9792) lr 1.8358e-03 eta 0:00:05
epoch [39/200] batch [65/72] time 0.432 (0.461) data 0.301 (0.330) loss_u loss_u 0.8872 (0.9120) acc_u 12.5000 (11.9231) lr 1.8358e-03 eta 0:00:03
epoch [39/200] batch [70/72] time 0.459 (0.459) data 0.328 (0.327) loss_u loss_u 0.9497 (0.9127) acc_u 6.2500 (11.8304) lr 1.8358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1661
confident_label rate tensor(0.2618, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 821
clean true:808
clean false:13
clean_rate:0.9841656516443362
noisy true:667
noisy false:1648
after delete: len(clean_dataset) 821
after delete: len(noisy_dataset) 2315
epoch [40/200] batch [5/25] time 0.507 (0.515) data 0.376 (0.385) loss_x loss_x 1.4092 (1.4311) acc_x 62.5000 (60.6250) lr 1.8271e-03 eta 0:00:10
epoch [40/200] batch [10/25] time 0.505 (0.513) data 0.373 (0.382) loss_x loss_x 1.3818 (1.4344) acc_x 59.3750 (64.0625) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [15/25] time 0.544 (0.512) data 0.414 (0.380) loss_x loss_x 1.0254 (1.4450) acc_x 68.7500 (63.5417) lr 1.8271e-03 eta 0:00:05
epoch [40/200] batch [20/25] time 0.427 (0.499) data 0.295 (0.368) loss_x loss_x 1.1553 (1.3836) acc_x 71.8750 (65.4688) lr 1.8271e-03 eta 0:00:02
epoch [40/200] batch [25/25] time 0.494 (0.493) data 0.363 (0.362) loss_x loss_x 1.7695 (1.4118) acc_x 59.3750 (64.7500) lr 1.8271e-03 eta 0:00:00
epoch [40/200] batch [5/72] time 0.302 (0.483) data 0.171 (0.352) loss_u loss_u 0.9346 (0.9254) acc_u 6.2500 (9.3750) lr 1.8271e-03 eta 0:00:32
epoch [40/200] batch [10/72] time 0.482 (0.479) data 0.351 (0.348) loss_u loss_u 0.9395 (0.9285) acc_u 9.3750 (9.3750) lr 1.8271e-03 eta 0:00:29
epoch [40/200] batch [15/72] time 0.447 (0.481) data 0.316 (0.349) loss_u loss_u 0.8359 (0.9188) acc_u 18.7500 (10.0000) lr 1.8271e-03 eta 0:00:27
epoch [40/200] batch [20/72] time 0.385 (0.480) data 0.254 (0.349) loss_u loss_u 0.9683 (0.9297) acc_u 6.2500 (8.7500) lr 1.8271e-03 eta 0:00:24
epoch [40/200] batch [25/72] time 0.641 (0.478) data 0.510 (0.347) loss_u loss_u 0.9336 (0.9298) acc_u 9.3750 (8.6250) lr 1.8271e-03 eta 0:00:22
epoch [40/200] batch [30/72] time 0.376 (0.476) data 0.245 (0.345) loss_u loss_u 0.9106 (0.9253) acc_u 15.6250 (9.4792) lr 1.8271e-03 eta 0:00:20
epoch [40/200] batch [35/72] time 0.493 (0.474) data 0.362 (0.343) loss_u loss_u 0.8940 (0.9252) acc_u 12.5000 (9.5536) lr 1.8271e-03 eta 0:00:17
epoch [40/200] batch [40/72] time 0.393 (0.471) data 0.263 (0.340) loss_u loss_u 0.9282 (0.9253) acc_u 3.1250 (9.2969) lr 1.8271e-03 eta 0:00:15
epoch [40/200] batch [45/72] time 0.456 (0.468) data 0.325 (0.337) loss_u loss_u 0.9097 (0.9245) acc_u 12.5000 (9.7917) lr 1.8271e-03 eta 0:00:12
epoch [40/200] batch [50/72] time 0.447 (0.470) data 0.316 (0.338) loss_u loss_u 0.9092 (0.9202) acc_u 9.3750 (10.2500) lr 1.8271e-03 eta 0:00:10
epoch [40/200] batch [55/72] time 0.489 (0.469) data 0.359 (0.338) loss_u loss_u 0.8813 (0.9184) acc_u 18.7500 (10.5114) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [60/72] time 0.382 (0.467) data 0.250 (0.336) loss_u loss_u 0.9473 (0.9186) acc_u 6.2500 (10.4167) lr 1.8271e-03 eta 0:00:05
epoch [40/200] batch [65/72] time 0.465 (0.468) data 0.334 (0.337) loss_u loss_u 0.9243 (0.9189) acc_u 6.2500 (10.3365) lr 1.8271e-03 eta 0:00:03
epoch [40/200] batch [70/72] time 0.527 (0.467) data 0.396 (0.335) loss_u loss_u 0.9253 (0.9184) acc_u 6.2500 (10.3125) lr 1.8271e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1604
confident_label rate tensor(0.2675, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 839
clean true:829
clean false:10
clean_rate:0.9880810488676997
noisy true:703
noisy false:1594
after delete: len(clean_dataset) 839
after delete: len(noisy_dataset) 2297
epoch [41/200] batch [5/26] time 0.427 (0.425) data 0.297 (0.295) loss_x loss_x 1.1357 (1.3572) acc_x 65.6250 (66.2500) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [10/26] time 0.679 (0.475) data 0.548 (0.344) loss_x loss_x 1.1152 (1.2440) acc_x 81.2500 (70.0000) lr 1.8181e-03 eta 0:00:07
epoch [41/200] batch [15/26] time 0.465 (0.492) data 0.335 (0.361) loss_x loss_x 1.6875 (1.3489) acc_x 56.2500 (67.0833) lr 1.8181e-03 eta 0:00:05
epoch [41/200] batch [20/26] time 0.472 (0.482) data 0.341 (0.351) loss_x loss_x 1.1455 (1.3463) acc_x 65.6250 (67.3438) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [25/26] time 0.470 (0.481) data 0.340 (0.351) loss_x loss_x 1.0244 (1.3546) acc_x 71.8750 (66.1250) lr 1.8181e-03 eta 0:00:00
epoch [41/200] batch [5/71] time 0.369 (0.478) data 0.239 (0.347) loss_u loss_u 0.8901 (0.9031) acc_u 15.6250 (13.7500) lr 1.8181e-03 eta 0:00:31
epoch [41/200] batch [10/71] time 0.510 (0.477) data 0.380 (0.346) loss_u loss_u 0.8716 (0.8985) acc_u 15.6250 (13.4375) lr 1.8181e-03 eta 0:00:29
epoch [41/200] batch [15/71] time 0.394 (0.479) data 0.264 (0.348) loss_u loss_u 0.8931 (0.9037) acc_u 15.6250 (12.5000) lr 1.8181e-03 eta 0:00:26
epoch [41/200] batch [20/71] time 0.479 (0.473) data 0.347 (0.342) loss_u loss_u 0.8853 (0.9043) acc_u 12.5000 (12.6562) lr 1.8181e-03 eta 0:00:24
epoch [41/200] batch [25/71] time 0.429 (0.470) data 0.298 (0.339) loss_u loss_u 0.9517 (0.9059) acc_u 6.2500 (12.7500) lr 1.8181e-03 eta 0:00:21
epoch [41/200] batch [30/71] time 0.337 (0.470) data 0.205 (0.340) loss_u loss_u 0.9053 (0.9067) acc_u 15.6250 (12.7083) lr 1.8181e-03 eta 0:00:19
epoch [41/200] batch [35/71] time 0.432 (0.468) data 0.300 (0.337) loss_u loss_u 0.9214 (0.9088) acc_u 9.3750 (12.5000) lr 1.8181e-03 eta 0:00:16
epoch [41/200] batch [40/71] time 0.450 (0.469) data 0.319 (0.338) loss_u loss_u 0.9600 (0.9087) acc_u 3.1250 (12.3438) lr 1.8181e-03 eta 0:00:14
epoch [41/200] batch [45/71] time 0.490 (0.468) data 0.359 (0.338) loss_u loss_u 0.9204 (0.9085) acc_u 9.3750 (12.4306) lr 1.8181e-03 eta 0:00:12
epoch [41/200] batch [50/71] time 0.481 (0.468) data 0.351 (0.337) loss_u loss_u 0.9443 (0.9105) acc_u 9.3750 (12.1250) lr 1.8181e-03 eta 0:00:09
epoch [41/200] batch [55/71] time 0.477 (0.469) data 0.346 (0.338) loss_u loss_u 0.9351 (0.9137) acc_u 9.3750 (11.5909) lr 1.8181e-03 eta 0:00:07
epoch [41/200] batch [60/71] time 0.416 (0.464) data 0.284 (0.333) loss_u loss_u 0.9409 (0.9143) acc_u 3.1250 (11.4583) lr 1.8181e-03 eta 0:00:05
epoch [41/200] batch [65/71] time 0.640 (0.465) data 0.509 (0.334) loss_u loss_u 0.8745 (0.9155) acc_u 15.6250 (11.2500) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [70/71] time 0.411 (0.463) data 0.280 (0.332) loss_u loss_u 0.9033 (0.9152) acc_u 12.5000 (11.2946) lr 1.8181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1646
confident_label rate tensor(0.2653, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 832
clean true:818
clean false:14
clean_rate:0.9831730769230769
noisy true:672
noisy false:1632
after delete: len(clean_dataset) 832
after delete: len(noisy_dataset) 2304
epoch [42/200] batch [5/26] time 0.604 (0.469) data 0.473 (0.339) loss_x loss_x 0.9834 (1.2627) acc_x 75.0000 (68.1250) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [10/26] time 0.619 (0.501) data 0.489 (0.371) loss_x loss_x 1.9131 (1.5168) acc_x 59.3750 (64.6875) lr 1.8090e-03 eta 0:00:08
epoch [42/200] batch [15/26] time 0.404 (0.501) data 0.274 (0.370) loss_x loss_x 1.3818 (1.4402) acc_x 68.7500 (65.8333) lr 1.8090e-03 eta 0:00:05
epoch [42/200] batch [20/26] time 0.328 (0.473) data 0.198 (0.342) loss_x loss_x 1.2334 (1.4177) acc_x 78.1250 (66.2500) lr 1.8090e-03 eta 0:00:02
epoch [42/200] batch [25/26] time 0.531 (0.475) data 0.401 (0.344) loss_x loss_x 1.7383 (1.4176) acc_x 46.8750 (65.8750) lr 1.8090e-03 eta 0:00:00
epoch [42/200] batch [5/72] time 0.428 (0.484) data 0.297 (0.354) loss_u loss_u 0.9619 (0.9146) acc_u 6.2500 (10.0000) lr 1.8090e-03 eta 0:00:32
epoch [42/200] batch [10/72] time 0.443 (0.479) data 0.313 (0.348) loss_u loss_u 0.9541 (0.9139) acc_u 6.2500 (10.0000) lr 1.8090e-03 eta 0:00:29
epoch [42/200] batch [15/72] time 0.459 (0.468) data 0.327 (0.338) loss_u loss_u 0.9272 (0.9243) acc_u 6.2500 (8.9583) lr 1.8090e-03 eta 0:00:26
epoch [42/200] batch [20/72] time 0.595 (0.471) data 0.464 (0.340) loss_u loss_u 0.8564 (0.9153) acc_u 18.7500 (10.3125) lr 1.8090e-03 eta 0:00:24
epoch [42/200] batch [25/72] time 0.437 (0.470) data 0.307 (0.339) loss_u loss_u 0.9243 (0.9189) acc_u 6.2500 (9.7500) lr 1.8090e-03 eta 0:00:22
epoch [42/200] batch [30/72] time 0.427 (0.466) data 0.297 (0.336) loss_u loss_u 0.8418 (0.9109) acc_u 28.1250 (11.0417) lr 1.8090e-03 eta 0:00:19
epoch [42/200] batch [35/72] time 0.381 (0.466) data 0.249 (0.335) loss_u loss_u 0.9536 (0.9118) acc_u 9.3750 (11.1607) lr 1.8090e-03 eta 0:00:17
epoch [42/200] batch [40/72] time 0.309 (0.463) data 0.178 (0.332) loss_u loss_u 0.9473 (0.9147) acc_u 9.3750 (10.9375) lr 1.8090e-03 eta 0:00:14
epoch [42/200] batch [45/72] time 0.459 (0.463) data 0.328 (0.332) loss_u loss_u 0.9385 (0.9162) acc_u 9.3750 (11.1111) lr 1.8090e-03 eta 0:00:12
epoch [42/200] batch [50/72] time 0.573 (0.465) data 0.441 (0.334) loss_u loss_u 0.9082 (0.9162) acc_u 12.5000 (11.3125) lr 1.8090e-03 eta 0:00:10
epoch [42/200] batch [55/72] time 0.634 (0.466) data 0.503 (0.335) loss_u loss_u 0.9180 (0.9169) acc_u 6.2500 (11.1364) lr 1.8090e-03 eta 0:00:07
epoch [42/200] batch [60/72] time 0.665 (0.471) data 0.534 (0.340) loss_u loss_u 0.8540 (0.9169) acc_u 18.7500 (11.0938) lr 1.8090e-03 eta 0:00:05
epoch [42/200] batch [65/72] time 0.406 (0.471) data 0.275 (0.340) loss_u loss_u 0.9282 (0.9172) acc_u 6.2500 (10.9615) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [70/72] time 0.360 (0.467) data 0.228 (0.336) loss_u loss_u 0.9009 (0.9159) acc_u 12.5000 (11.0714) lr 1.8090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1655
confident_label rate tensor(0.2545, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 798
clean true:789
clean false:9
clean_rate:0.9887218045112782
noisy true:692
noisy false:1646
after delete: len(clean_dataset) 798
after delete: len(noisy_dataset) 2338
epoch [43/200] batch [5/24] time 0.437 (0.436) data 0.307 (0.305) loss_x loss_x 1.5996 (1.3734) acc_x 53.1250 (63.7500) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [10/24] time 0.525 (0.457) data 0.394 (0.326) loss_x loss_x 1.0889 (1.4444) acc_x 75.0000 (65.3125) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [15/24] time 0.512 (0.479) data 0.381 (0.349) loss_x loss_x 1.3857 (1.3911) acc_x 68.7500 (66.4583) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [20/24] time 0.474 (0.493) data 0.344 (0.362) loss_x loss_x 0.9844 (1.3671) acc_x 71.8750 (66.5625) lr 1.7997e-03 eta 0:00:01
epoch [43/200] batch [5/73] time 0.514 (0.484) data 0.382 (0.353) loss_u loss_u 0.8496 (0.8944) acc_u 15.6250 (16.2500) lr 1.7997e-03 eta 0:00:32
epoch [43/200] batch [10/73] time 0.460 (0.492) data 0.327 (0.361) loss_u loss_u 0.9307 (0.9009) acc_u 12.5000 (13.7500) lr 1.7997e-03 eta 0:00:31
epoch [43/200] batch [15/73] time 0.503 (0.491) data 0.373 (0.360) loss_u loss_u 0.9785 (0.9134) acc_u 3.1250 (12.0833) lr 1.7997e-03 eta 0:00:28
epoch [43/200] batch [20/73] time 0.453 (0.486) data 0.321 (0.355) loss_u loss_u 0.9136 (0.9169) acc_u 12.5000 (11.2500) lr 1.7997e-03 eta 0:00:25
epoch [43/200] batch [25/73] time 0.366 (0.488) data 0.234 (0.357) loss_u loss_u 0.9136 (0.9152) acc_u 12.5000 (11.2500) lr 1.7997e-03 eta 0:00:23
epoch [43/200] batch [30/73] time 0.349 (0.478) data 0.217 (0.347) loss_u loss_u 0.8813 (0.9086) acc_u 18.7500 (12.5000) lr 1.7997e-03 eta 0:00:20
epoch [43/200] batch [35/73] time 0.529 (0.474) data 0.397 (0.343) loss_u loss_u 0.9092 (0.9086) acc_u 12.5000 (12.6786) lr 1.7997e-03 eta 0:00:18
epoch [43/200] batch [40/73] time 0.515 (0.479) data 0.383 (0.348) loss_u loss_u 0.9082 (0.9079) acc_u 18.7500 (12.6562) lr 1.7997e-03 eta 0:00:15
epoch [43/200] batch [45/73] time 0.539 (0.479) data 0.408 (0.348) loss_u loss_u 0.9424 (0.9097) acc_u 6.2500 (12.5000) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [50/73] time 0.524 (0.478) data 0.392 (0.347) loss_u loss_u 0.8569 (0.9096) acc_u 18.7500 (12.4375) lr 1.7997e-03 eta 0:00:10
epoch [43/200] batch [55/73] time 0.468 (0.477) data 0.337 (0.345) loss_u loss_u 0.9487 (0.9093) acc_u 3.1250 (12.3864) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [60/73] time 0.569 (0.477) data 0.439 (0.345) loss_u loss_u 0.8975 (0.9086) acc_u 15.6250 (12.3958) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [65/73] time 0.466 (0.477) data 0.335 (0.345) loss_u loss_u 0.9229 (0.9098) acc_u 6.2500 (12.1154) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [70/73] time 0.539 (0.475) data 0.408 (0.344) loss_u loss_u 0.9263 (0.9120) acc_u 6.2500 (11.6964) lr 1.7997e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1679
confident_label rate tensor(0.2529, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 793
clean true:784
clean false:9
clean_rate:0.9886506935687264
noisy true:673
noisy false:1670
after delete: len(clean_dataset) 793
after delete: len(noisy_dataset) 2343
epoch [44/200] batch [5/24] time 0.409 (0.435) data 0.280 (0.304) loss_x loss_x 1.0059 (1.3008) acc_x 78.1250 (65.6250) lr 1.7902e-03 eta 0:00:08
epoch [44/200] batch [10/24] time 0.437 (0.450) data 0.306 (0.319) loss_x loss_x 1.0713 (1.2995) acc_x 78.1250 (67.1875) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [15/24] time 0.473 (0.466) data 0.342 (0.335) loss_x loss_x 1.4102 (1.3870) acc_x 65.6250 (65.4167) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [20/24] time 0.393 (0.468) data 0.263 (0.338) loss_x loss_x 1.5430 (1.3386) acc_x 53.1250 (65.6250) lr 1.7902e-03 eta 0:00:01
epoch [44/200] batch [5/73] time 0.558 (0.484) data 0.427 (0.353) loss_u loss_u 0.9590 (0.9253) acc_u 3.1250 (8.7500) lr 1.7902e-03 eta 0:00:32
epoch [44/200] batch [10/73] time 0.427 (0.482) data 0.297 (0.351) loss_u loss_u 0.8623 (0.9218) acc_u 18.7500 (10.6250) lr 1.7902e-03 eta 0:00:30
epoch [44/200] batch [15/73] time 0.480 (0.475) data 0.348 (0.344) loss_u loss_u 0.9355 (0.9160) acc_u 9.3750 (11.4583) lr 1.7902e-03 eta 0:00:27
epoch [44/200] batch [20/73] time 0.389 (0.467) data 0.257 (0.336) loss_u loss_u 0.9443 (0.9107) acc_u 9.3750 (11.8750) lr 1.7902e-03 eta 0:00:24
epoch [44/200] batch [25/73] time 0.450 (0.460) data 0.318 (0.330) loss_u loss_u 0.9736 (0.9143) acc_u 6.2500 (11.6250) lr 1.7902e-03 eta 0:00:22
epoch [44/200] batch [30/73] time 0.348 (0.457) data 0.217 (0.326) loss_u loss_u 0.9580 (0.9137) acc_u 3.1250 (11.7708) lr 1.7902e-03 eta 0:00:19
epoch [44/200] batch [35/73] time 0.389 (0.453) data 0.257 (0.322) loss_u loss_u 0.8662 (0.9122) acc_u 12.5000 (11.6964) lr 1.7902e-03 eta 0:00:17
epoch [44/200] batch [40/73] time 0.432 (0.450) data 0.301 (0.319) loss_u loss_u 0.8486 (0.9114) acc_u 18.7500 (11.7969) lr 1.7902e-03 eta 0:00:14
epoch [44/200] batch [45/73] time 0.549 (0.454) data 0.418 (0.323) loss_u loss_u 0.9399 (0.9116) acc_u 6.2500 (11.5278) lr 1.7902e-03 eta 0:00:12
epoch [44/200] batch [50/73] time 0.481 (0.453) data 0.349 (0.322) loss_u loss_u 0.9414 (0.9135) acc_u 12.5000 (11.3125) lr 1.7902e-03 eta 0:00:10
epoch [44/200] batch [55/73] time 0.327 (0.449) data 0.196 (0.318) loss_u loss_u 0.8325 (0.9115) acc_u 21.8750 (11.5909) lr 1.7902e-03 eta 0:00:08
epoch [44/200] batch [60/73] time 0.410 (0.454) data 0.280 (0.323) loss_u loss_u 0.9077 (0.9114) acc_u 15.6250 (11.7708) lr 1.7902e-03 eta 0:00:05
epoch [44/200] batch [65/73] time 0.427 (0.458) data 0.295 (0.327) loss_u loss_u 0.9712 (0.9107) acc_u 0.0000 (11.8269) lr 1.7902e-03 eta 0:00:03
epoch [44/200] batch [70/73] time 0.561 (0.462) data 0.430 (0.331) loss_u loss_u 0.9331 (0.9111) acc_u 3.1250 (11.6964) lr 1.7902e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1673
confident_label rate tensor(0.2551, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 800
clean true:793
clean false:7
clean_rate:0.99125
noisy true:670
noisy false:1666
after delete: len(clean_dataset) 800
after delete: len(noisy_dataset) 2336
epoch [45/200] batch [5/25] time 0.509 (0.450) data 0.379 (0.319) loss_x loss_x 1.4238 (1.4858) acc_x 59.3750 (62.5000) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [10/25] time 0.596 (0.453) data 0.466 (0.323) loss_x loss_x 1.1152 (1.3850) acc_x 75.0000 (65.9375) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [15/25] time 0.521 (0.465) data 0.391 (0.335) loss_x loss_x 1.6592 (1.4034) acc_x 65.6250 (65.8333) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [20/25] time 0.453 (0.465) data 0.323 (0.335) loss_x loss_x 1.2803 (1.4004) acc_x 65.6250 (65.0000) lr 1.7804e-03 eta 0:00:02
epoch [45/200] batch [25/25] time 0.401 (0.458) data 0.270 (0.328) loss_x loss_x 1.4473 (1.4132) acc_x 65.6250 (65.1250) lr 1.7804e-03 eta 0:00:00
epoch [45/200] batch [5/73] time 0.498 (0.457) data 0.366 (0.327) loss_u loss_u 0.9336 (0.8872) acc_u 6.2500 (15.0000) lr 1.7804e-03 eta 0:00:31
epoch [45/200] batch [10/73] time 0.398 (0.452) data 0.266 (0.321) loss_u loss_u 0.9409 (0.8981) acc_u 6.2500 (13.1250) lr 1.7804e-03 eta 0:00:28
epoch [45/200] batch [15/73] time 0.471 (0.458) data 0.338 (0.327) loss_u loss_u 0.9136 (0.8992) acc_u 15.6250 (13.9583) lr 1.7804e-03 eta 0:00:26
epoch [45/200] batch [20/73] time 0.508 (0.454) data 0.377 (0.323) loss_u loss_u 0.9077 (0.8987) acc_u 15.6250 (14.3750) lr 1.7804e-03 eta 0:00:24
epoch [45/200] batch [25/73] time 0.441 (0.455) data 0.309 (0.324) loss_u loss_u 0.8965 (0.9024) acc_u 9.3750 (13.2500) lr 1.7804e-03 eta 0:00:21
epoch [45/200] batch [30/73] time 0.552 (0.455) data 0.419 (0.323) loss_u loss_u 0.9404 (0.9055) acc_u 9.3750 (13.1250) lr 1.7804e-03 eta 0:00:19
epoch [45/200] batch [35/73] time 0.433 (0.454) data 0.301 (0.323) loss_u loss_u 0.9351 (0.9093) acc_u 6.2500 (12.5000) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [40/73] time 0.397 (0.450) data 0.266 (0.319) loss_u loss_u 0.8843 (0.9092) acc_u 12.5000 (12.1875) lr 1.7804e-03 eta 0:00:14
epoch [45/200] batch [45/73] time 0.522 (0.452) data 0.391 (0.321) loss_u loss_u 0.8809 (0.9087) acc_u 18.7500 (12.3611) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [50/73] time 0.726 (0.455) data 0.594 (0.323) loss_u loss_u 0.8833 (0.9093) acc_u 15.6250 (12.4375) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [55/73] time 0.477 (0.454) data 0.345 (0.322) loss_u loss_u 0.9121 (0.9063) acc_u 9.3750 (12.8977) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [60/73] time 0.447 (0.453) data 0.316 (0.322) loss_u loss_u 0.9033 (0.9069) acc_u 12.5000 (12.7604) lr 1.7804e-03 eta 0:00:05
epoch [45/200] batch [65/73] time 0.428 (0.453) data 0.298 (0.321) loss_u loss_u 0.9385 (0.9078) acc_u 9.3750 (12.5481) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [70/73] time 0.395 (0.451) data 0.263 (0.320) loss_u loss_u 0.9258 (0.9091) acc_u 9.3750 (12.3214) lr 1.7804e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1605
confident_label rate tensor(0.2691, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 844
clean true:835
clean false:9
clean_rate:0.9893364928909952
noisy true:696
noisy false:1596
after delete: len(clean_dataset) 844
after delete: len(noisy_dataset) 2292
epoch [46/200] batch [5/26] time 0.461 (0.459) data 0.331 (0.329) loss_x loss_x 1.1338 (1.3252) acc_x 65.6250 (66.2500) lr 1.7705e-03 eta 0:00:09
epoch [46/200] batch [10/26] time 0.540 (0.460) data 0.410 (0.330) loss_x loss_x 1.2314 (1.3211) acc_x 75.0000 (68.1250) lr 1.7705e-03 eta 0:00:07
epoch [46/200] batch [15/26] time 0.542 (0.475) data 0.411 (0.345) loss_x loss_x 1.7314 (1.4079) acc_x 53.1250 (66.2500) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [20/26] time 0.420 (0.480) data 0.290 (0.349) loss_x loss_x 1.3232 (1.3843) acc_x 68.7500 (65.7812) lr 1.7705e-03 eta 0:00:02
epoch [46/200] batch [25/26] time 0.646 (0.495) data 0.515 (0.364) loss_x loss_x 1.5215 (1.3250) acc_x 62.5000 (65.8750) lr 1.7705e-03 eta 0:00:00
epoch [46/200] batch [5/71] time 0.609 (0.491) data 0.477 (0.360) loss_u loss_u 0.8447 (0.9127) acc_u 21.8750 (12.5000) lr 1.7705e-03 eta 0:00:32
epoch [46/200] batch [10/71] time 0.420 (0.496) data 0.289 (0.366) loss_u loss_u 0.9873 (0.9193) acc_u 0.0000 (10.6250) lr 1.7705e-03 eta 0:00:30
epoch [46/200] batch [15/71] time 0.506 (0.494) data 0.376 (0.363) loss_u loss_u 0.8691 (0.9186) acc_u 15.6250 (10.4167) lr 1.7705e-03 eta 0:00:27
epoch [46/200] batch [20/71] time 0.488 (0.489) data 0.357 (0.358) loss_u loss_u 0.8970 (0.9127) acc_u 12.5000 (11.0938) lr 1.7705e-03 eta 0:00:24
epoch [46/200] batch [25/71] time 0.370 (0.482) data 0.239 (0.351) loss_u loss_u 0.8408 (0.9143) acc_u 25.0000 (11.2500) lr 1.7705e-03 eta 0:00:22
epoch [46/200] batch [30/71] time 0.489 (0.477) data 0.357 (0.346) loss_u loss_u 0.9150 (0.9162) acc_u 9.3750 (10.5208) lr 1.7705e-03 eta 0:00:19
epoch [46/200] batch [35/71] time 0.401 (0.470) data 0.271 (0.339) loss_u loss_u 0.9263 (0.9195) acc_u 9.3750 (10.0893) lr 1.7705e-03 eta 0:00:16
epoch [46/200] batch [40/71] time 0.315 (0.468) data 0.184 (0.337) loss_u loss_u 0.9146 (0.9156) acc_u 6.2500 (10.6250) lr 1.7705e-03 eta 0:00:14
epoch [46/200] batch [45/71] time 0.393 (0.464) data 0.261 (0.333) loss_u loss_u 0.8921 (0.9150) acc_u 15.6250 (10.8333) lr 1.7705e-03 eta 0:00:12
epoch [46/200] batch [50/71] time 0.439 (0.461) data 0.308 (0.330) loss_u loss_u 0.9067 (0.9161) acc_u 12.5000 (10.8125) lr 1.7705e-03 eta 0:00:09
epoch [46/200] batch [55/71] time 0.395 (0.458) data 0.263 (0.327) loss_u loss_u 0.8384 (0.9150) acc_u 21.8750 (10.9659) lr 1.7705e-03 eta 0:00:07
epoch [46/200] batch [60/71] time 0.525 (0.458) data 0.393 (0.327) loss_u loss_u 0.9399 (0.9156) acc_u 9.3750 (10.7292) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [65/71] time 0.491 (0.459) data 0.360 (0.328) loss_u loss_u 0.9858 (0.9185) acc_u 3.1250 (10.2885) lr 1.7705e-03 eta 0:00:02
epoch [46/200] batch [70/71] time 0.533 (0.461) data 0.402 (0.330) loss_u loss_u 0.8931 (0.9178) acc_u 18.7500 (10.5357) lr 1.7705e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1634
confident_label rate tensor(0.2666, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 836
clean true:827
clean false:9
clean_rate:0.9892344497607656
noisy true:675
noisy false:1625
after delete: len(clean_dataset) 836
after delete: len(noisy_dataset) 2300
epoch [47/200] batch [5/26] time 0.460 (0.503) data 0.330 (0.372) loss_x loss_x 1.5303 (1.4215) acc_x 59.3750 (65.6250) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [10/26] time 0.422 (0.524) data 0.292 (0.393) loss_x loss_x 1.1006 (1.3600) acc_x 68.7500 (65.3125) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [15/26] time 0.650 (0.522) data 0.520 (0.391) loss_x loss_x 1.3779 (1.3320) acc_x 56.2500 (65.8333) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [20/26] time 0.430 (0.500) data 0.294 (0.369) loss_x loss_x 1.6904 (1.3770) acc_x 53.1250 (66.0938) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [25/26] time 0.386 (0.488) data 0.256 (0.357) loss_x loss_x 1.4404 (1.3937) acc_x 68.7500 (65.6250) lr 1.7604e-03 eta 0:00:00
epoch [47/200] batch [5/71] time 0.403 (0.487) data 0.272 (0.356) loss_u loss_u 0.8916 (0.9110) acc_u 15.6250 (11.2500) lr 1.7604e-03 eta 0:00:32
epoch [47/200] batch [10/71] time 0.384 (0.476) data 0.252 (0.345) loss_u loss_u 0.9878 (0.9206) acc_u 0.0000 (10.0000) lr 1.7604e-03 eta 0:00:29
epoch [47/200] batch [15/71] time 0.489 (0.474) data 0.357 (0.343) loss_u loss_u 0.8359 (0.9076) acc_u 15.6250 (11.6667) lr 1.7604e-03 eta 0:00:26
epoch [47/200] batch [20/71] time 0.473 (0.472) data 0.342 (0.340) loss_u loss_u 0.8911 (0.9088) acc_u 15.6250 (12.0312) lr 1.7604e-03 eta 0:00:24
epoch [47/200] batch [25/71] time 0.355 (0.469) data 0.223 (0.337) loss_u loss_u 0.9160 (0.9098) acc_u 9.3750 (12.0000) lr 1.7604e-03 eta 0:00:21
epoch [47/200] batch [30/71] time 0.392 (0.473) data 0.261 (0.342) loss_u loss_u 0.8994 (0.9090) acc_u 15.6250 (12.2917) lr 1.7604e-03 eta 0:00:19
epoch [47/200] batch [35/71] time 0.577 (0.473) data 0.445 (0.341) loss_u loss_u 0.9136 (0.9096) acc_u 15.6250 (12.4107) lr 1.7604e-03 eta 0:00:17
epoch [47/200] batch [40/71] time 0.363 (0.467) data 0.231 (0.336) loss_u loss_u 0.9346 (0.9103) acc_u 9.3750 (12.1875) lr 1.7604e-03 eta 0:00:14
epoch [47/200] batch [45/71] time 0.460 (0.466) data 0.328 (0.335) loss_u loss_u 0.9136 (0.9100) acc_u 12.5000 (12.1528) lr 1.7604e-03 eta 0:00:12
epoch [47/200] batch [50/71] time 0.441 (0.463) data 0.309 (0.332) loss_u loss_u 0.9277 (0.9110) acc_u 9.3750 (11.8125) lr 1.7604e-03 eta 0:00:09
epoch [47/200] batch [55/71] time 0.405 (0.466) data 0.273 (0.335) loss_u loss_u 0.8027 (0.9107) acc_u 28.1250 (11.9318) lr 1.7604e-03 eta 0:00:07
epoch [47/200] batch [60/71] time 0.445 (0.467) data 0.313 (0.335) loss_u loss_u 0.8799 (0.9095) acc_u 18.7500 (12.0833) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [65/71] time 0.528 (0.468) data 0.396 (0.336) loss_u loss_u 0.9331 (0.9113) acc_u 9.3750 (11.7308) lr 1.7604e-03 eta 0:00:02
epoch [47/200] batch [70/71] time 0.462 (0.466) data 0.331 (0.335) loss_u loss_u 0.8975 (0.9113) acc_u 18.7500 (11.7857) lr 1.7604e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1626
confident_label rate tensor(0.2586, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 811
clean true:801
clean false:10
clean_rate:0.9876695437731196
noisy true:709
noisy false:1616
after delete: len(clean_dataset) 811
after delete: len(noisy_dataset) 2325
epoch [48/200] batch [5/25] time 0.533 (0.491) data 0.402 (0.360) loss_x loss_x 1.1025 (1.1914) acc_x 71.8750 (69.3750) lr 1.7501e-03 eta 0:00:09
epoch [48/200] batch [10/25] time 0.488 (0.520) data 0.357 (0.389) loss_x loss_x 1.3848 (1.2277) acc_x 62.5000 (66.5625) lr 1.7501e-03 eta 0:00:07
epoch [48/200] batch [15/25] time 0.403 (0.507) data 0.272 (0.376) loss_x loss_x 1.0693 (1.2243) acc_x 78.1250 (67.9167) lr 1.7501e-03 eta 0:00:05
epoch [48/200] batch [20/25] time 0.488 (0.507) data 0.356 (0.376) loss_x loss_x 1.0674 (1.2416) acc_x 62.5000 (66.8750) lr 1.7501e-03 eta 0:00:02
epoch [48/200] batch [25/25] time 0.569 (0.512) data 0.438 (0.381) loss_x loss_x 1.0918 (1.3175) acc_x 71.8750 (65.7500) lr 1.7501e-03 eta 0:00:00
epoch [48/200] batch [5/72] time 0.490 (0.503) data 0.358 (0.372) loss_u loss_u 0.9443 (0.9209) acc_u 9.3750 (11.8750) lr 1.7501e-03 eta 0:00:33
epoch [48/200] batch [10/72] time 0.492 (0.491) data 0.361 (0.360) loss_u loss_u 0.8857 (0.9044) acc_u 18.7500 (13.7500) lr 1.7501e-03 eta 0:00:30
epoch [48/200] batch [15/72] time 0.459 (0.488) data 0.328 (0.357) loss_u loss_u 0.9189 (0.9079) acc_u 9.3750 (13.3333) lr 1.7501e-03 eta 0:00:27
epoch [48/200] batch [20/72] time 0.474 (0.493) data 0.343 (0.362) loss_u loss_u 0.9526 (0.9048) acc_u 9.3750 (13.7500) lr 1.7501e-03 eta 0:00:25
epoch [48/200] batch [25/72] time 0.369 (0.482) data 0.238 (0.351) loss_u loss_u 0.8633 (0.9085) acc_u 15.6250 (12.3750) lr 1.7501e-03 eta 0:00:22
epoch [48/200] batch [30/72] time 0.395 (0.473) data 0.264 (0.342) loss_u loss_u 0.8760 (0.9074) acc_u 18.7500 (12.6042) lr 1.7501e-03 eta 0:00:19
epoch [48/200] batch [35/72] time 0.410 (0.471) data 0.280 (0.340) loss_u loss_u 0.8623 (0.9054) acc_u 21.8750 (13.0357) lr 1.7501e-03 eta 0:00:17
epoch [48/200] batch [40/72] time 0.435 (0.467) data 0.303 (0.336) loss_u loss_u 0.9272 (0.9055) acc_u 9.3750 (12.8906) lr 1.7501e-03 eta 0:00:14
epoch [48/200] batch [45/72] time 0.389 (0.464) data 0.257 (0.333) loss_u loss_u 0.9185 (0.9078) acc_u 18.7500 (12.7778) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [50/72] time 0.477 (0.460) data 0.346 (0.329) loss_u loss_u 0.9419 (0.9088) acc_u 6.2500 (12.6250) lr 1.7501e-03 eta 0:00:10
epoch [48/200] batch [55/72] time 0.402 (0.461) data 0.271 (0.330) loss_u loss_u 0.9248 (0.9089) acc_u 6.2500 (12.4432) lr 1.7501e-03 eta 0:00:07
epoch [48/200] batch [60/72] time 0.470 (0.459) data 0.338 (0.328) loss_u loss_u 0.9268 (0.9098) acc_u 9.3750 (12.1875) lr 1.7501e-03 eta 0:00:05
epoch [48/200] batch [65/72] time 0.434 (0.456) data 0.303 (0.325) loss_u loss_u 0.9409 (0.9117) acc_u 9.3750 (11.9231) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [70/72] time 0.646 (0.458) data 0.515 (0.327) loss_u loss_u 0.9160 (0.9118) acc_u 9.3750 (11.9643) lr 1.7501e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1608
confident_label rate tensor(0.2640, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 828
clean true:819
clean false:9
clean_rate:0.9891304347826086
noisy true:709
noisy false:1599
after delete: len(clean_dataset) 828
after delete: len(noisy_dataset) 2308
epoch [49/200] batch [5/25] time 0.729 (0.612) data 0.595 (0.480) loss_x loss_x 1.0557 (1.2052) acc_x 75.0000 (70.6250) lr 1.7396e-03 eta 0:00:12
epoch [49/200] batch [10/25] time 0.414 (0.535) data 0.283 (0.404) loss_x loss_x 1.1260 (1.2279) acc_x 71.8750 (68.7500) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [15/25] time 0.379 (0.509) data 0.249 (0.378) loss_x loss_x 1.0391 (1.2240) acc_x 62.5000 (68.1250) lr 1.7396e-03 eta 0:00:05
epoch [49/200] batch [20/25] time 0.469 (0.496) data 0.339 (0.366) loss_x loss_x 1.3438 (1.2282) acc_x 68.7500 (67.0312) lr 1.7396e-03 eta 0:00:02
epoch [49/200] batch [25/25] time 0.403 (0.494) data 0.272 (0.363) loss_x loss_x 1.5781 (1.2982) acc_x 59.3750 (65.6250) lr 1.7396e-03 eta 0:00:00
epoch [49/200] batch [5/72] time 0.419 (0.484) data 0.289 (0.353) loss_u loss_u 0.9077 (0.9082) acc_u 12.5000 (10.6250) lr 1.7396e-03 eta 0:00:32
epoch [49/200] batch [10/72] time 0.397 (0.478) data 0.266 (0.347) loss_u loss_u 0.9316 (0.9112) acc_u 9.3750 (11.2500) lr 1.7396e-03 eta 0:00:29
epoch [49/200] batch [15/72] time 0.448 (0.484) data 0.318 (0.353) loss_u loss_u 0.8955 (0.9057) acc_u 15.6250 (12.7083) lr 1.7396e-03 eta 0:00:27
epoch [49/200] batch [20/72] time 0.395 (0.475) data 0.264 (0.345) loss_u loss_u 0.9312 (0.9097) acc_u 9.3750 (12.3438) lr 1.7396e-03 eta 0:00:24
epoch [49/200] batch [25/72] time 0.451 (0.470) data 0.320 (0.340) loss_u loss_u 0.9219 (0.9135) acc_u 9.3750 (11.6250) lr 1.7396e-03 eta 0:00:22
epoch [49/200] batch [30/72] time 0.416 (0.464) data 0.285 (0.333) loss_u loss_u 0.9473 (0.9145) acc_u 9.3750 (11.5625) lr 1.7396e-03 eta 0:00:19
epoch [49/200] batch [35/72] time 0.445 (0.465) data 0.314 (0.334) loss_u loss_u 0.9253 (0.9124) acc_u 9.3750 (11.9643) lr 1.7396e-03 eta 0:00:17
epoch [49/200] batch [40/72] time 0.364 (0.463) data 0.232 (0.332) loss_u loss_u 0.8540 (0.9106) acc_u 15.6250 (12.1094) lr 1.7396e-03 eta 0:00:14
epoch [49/200] batch [45/72] time 0.440 (0.461) data 0.308 (0.330) loss_u loss_u 0.9790 (0.9130) acc_u 9.3750 (11.9444) lr 1.7396e-03 eta 0:00:12
epoch [49/200] batch [50/72] time 0.553 (0.460) data 0.421 (0.329) loss_u loss_u 0.9102 (0.9128) acc_u 12.5000 (11.8750) lr 1.7396e-03 eta 0:00:10
epoch [49/200] batch [55/72] time 0.537 (0.459) data 0.401 (0.328) loss_u loss_u 0.8828 (0.9120) acc_u 12.5000 (12.0455) lr 1.7396e-03 eta 0:00:07
epoch [49/200] batch [60/72] time 0.561 (0.460) data 0.430 (0.328) loss_u loss_u 0.9009 (0.9127) acc_u 15.6250 (11.9792) lr 1.7396e-03 eta 0:00:05
epoch [49/200] batch [65/72] time 0.359 (0.457) data 0.228 (0.326) loss_u loss_u 0.9146 (0.9139) acc_u 9.3750 (11.7308) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [70/72] time 0.342 (0.457) data 0.210 (0.326) loss_u loss_u 0.9653 (0.9142) acc_u 6.2500 (11.5625) lr 1.7396e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1614
confident_label rate tensor(0.2707, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 849
clean true:841
clean false:8
clean_rate:0.9905771495877503
noisy true:681
noisy false:1606
after delete: len(clean_dataset) 849
after delete: len(noisy_dataset) 2287
epoch [50/200] batch [5/26] time 0.413 (0.464) data 0.282 (0.333) loss_x loss_x 1.5791 (1.3039) acc_x 62.5000 (69.3750) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [10/26] time 0.529 (0.474) data 0.399 (0.343) loss_x loss_x 1.1777 (1.2501) acc_x 71.8750 (69.6875) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [15/26] time 0.566 (0.487) data 0.436 (0.357) loss_x loss_x 1.3359 (1.2873) acc_x 59.3750 (67.9167) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [20/26] time 0.640 (0.506) data 0.509 (0.375) loss_x loss_x 1.6328 (1.3083) acc_x 68.7500 (68.5938) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [25/26] time 0.475 (0.494) data 0.344 (0.364) loss_x loss_x 1.2676 (1.3093) acc_x 68.7500 (69.0000) lr 1.7290e-03 eta 0:00:00
epoch [50/200] batch [5/71] time 0.438 (0.491) data 0.307 (0.360) loss_u loss_u 0.9312 (0.9248) acc_u 9.3750 (10.0000) lr 1.7290e-03 eta 0:00:32
epoch [50/200] batch [10/71] time 0.410 (0.485) data 0.278 (0.354) loss_u loss_u 0.8740 (0.9042) acc_u 12.5000 (11.5625) lr 1.7290e-03 eta 0:00:29
epoch [50/200] batch [15/71] time 0.483 (0.486) data 0.352 (0.355) loss_u loss_u 0.8926 (0.9064) acc_u 12.5000 (10.8333) lr 1.7290e-03 eta 0:00:27
epoch [50/200] batch [20/71] time 0.435 (0.479) data 0.303 (0.348) loss_u loss_u 0.8950 (0.9042) acc_u 9.3750 (11.2500) lr 1.7290e-03 eta 0:00:24
epoch [50/200] batch [25/71] time 0.421 (0.477) data 0.289 (0.345) loss_u loss_u 0.8950 (0.9109) acc_u 12.5000 (11.0000) lr 1.7290e-03 eta 0:00:21
epoch [50/200] batch [30/71] time 0.452 (0.477) data 0.321 (0.346) loss_u loss_u 0.9512 (0.9157) acc_u 6.2500 (10.5208) lr 1.7290e-03 eta 0:00:19
epoch [50/200] batch [35/71] time 0.476 (0.477) data 0.345 (0.346) loss_u loss_u 0.9458 (0.9173) acc_u 9.3750 (10.4464) lr 1.7290e-03 eta 0:00:17
epoch [50/200] batch [40/71] time 0.423 (0.482) data 0.291 (0.350) loss_u loss_u 0.9243 (0.9162) acc_u 12.5000 (10.7031) lr 1.7290e-03 eta 0:00:14
epoch [50/200] batch [45/71] time 0.412 (0.477) data 0.281 (0.346) loss_u loss_u 0.9424 (0.9176) acc_u 6.2500 (10.6944) lr 1.7290e-03 eta 0:00:12
epoch [50/200] batch [50/71] time 0.482 (0.476) data 0.350 (0.345) loss_u loss_u 0.8647 (0.9136) acc_u 15.6250 (11.2500) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [55/71] time 0.551 (0.474) data 0.419 (0.343) loss_u loss_u 0.9243 (0.9138) acc_u 12.5000 (11.2500) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [60/71] time 0.578 (0.472) data 0.446 (0.340) loss_u loss_u 0.9263 (0.9145) acc_u 6.2500 (11.0938) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [65/71] time 0.462 (0.471) data 0.329 (0.340) loss_u loss_u 0.8667 (0.9144) acc_u 15.6250 (11.1058) lr 1.7290e-03 eta 0:00:02
epoch [50/200] batch [70/71] time 0.503 (0.471) data 0.371 (0.340) loss_u loss_u 0.9551 (0.9145) acc_u 3.1250 (11.0714) lr 1.7290e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1606
confident_label rate tensor(0.2675, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 839
clean true:829
clean false:10
clean_rate:0.9880810488676997
noisy true:701
noisy false:1596
after delete: len(clean_dataset) 839
after delete: len(noisy_dataset) 2297
epoch [51/200] batch [5/26] time 0.448 (0.415) data 0.317 (0.284) loss_x loss_x 1.7383 (1.2607) acc_x 68.7500 (73.1250) lr 1.7181e-03 eta 0:00:08
epoch [51/200] batch [10/26] time 0.419 (0.400) data 0.289 (0.269) loss_x loss_x 1.4629 (1.3246) acc_x 62.5000 (69.6875) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [15/26] time 0.472 (0.423) data 0.340 (0.293) loss_x loss_x 1.5127 (1.3430) acc_x 56.2500 (67.2917) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [20/26] time 0.433 (0.424) data 0.302 (0.293) loss_x loss_x 1.2588 (1.3500) acc_x 68.7500 (66.4062) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [25/26] time 0.521 (0.433) data 0.390 (0.303) loss_x loss_x 1.3086 (1.3215) acc_x 62.5000 (67.1250) lr 1.7181e-03 eta 0:00:00
epoch [51/200] batch [5/71] time 0.401 (0.436) data 0.271 (0.305) loss_u loss_u 0.9150 (0.9145) acc_u 9.3750 (11.2500) lr 1.7181e-03 eta 0:00:28
epoch [51/200] batch [10/71] time 0.456 (0.447) data 0.326 (0.316) loss_u loss_u 0.9067 (0.9067) acc_u 12.5000 (10.6250) lr 1.7181e-03 eta 0:00:27
epoch [51/200] batch [15/71] time 0.298 (0.438) data 0.166 (0.308) loss_u loss_u 0.9043 (0.9109) acc_u 15.6250 (11.2500) lr 1.7181e-03 eta 0:00:24
epoch [51/200] batch [20/71] time 0.351 (0.434) data 0.219 (0.303) loss_u loss_u 0.9180 (0.9147) acc_u 18.7500 (10.9375) lr 1.7181e-03 eta 0:00:22
epoch [51/200] batch [25/71] time 0.476 (0.435) data 0.344 (0.304) loss_u loss_u 0.8970 (0.9134) acc_u 18.7500 (11.3750) lr 1.7181e-03 eta 0:00:19
epoch [51/200] batch [30/71] time 0.450 (0.438) data 0.320 (0.307) loss_u loss_u 0.9438 (0.9144) acc_u 6.2500 (11.3542) lr 1.7181e-03 eta 0:00:17
epoch [51/200] batch [35/71] time 0.476 (0.439) data 0.344 (0.308) loss_u loss_u 0.9370 (0.9144) acc_u 6.2500 (11.2500) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [40/71] time 0.410 (0.440) data 0.279 (0.309) loss_u loss_u 0.8540 (0.9133) acc_u 15.6250 (11.2500) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [45/71] time 0.662 (0.444) data 0.530 (0.313) loss_u loss_u 0.9092 (0.9099) acc_u 9.3750 (11.6667) lr 1.7181e-03 eta 0:00:11
epoch [51/200] batch [50/71] time 0.449 (0.442) data 0.317 (0.311) loss_u loss_u 0.9150 (0.9073) acc_u 9.3750 (12.1875) lr 1.7181e-03 eta 0:00:09
epoch [51/200] batch [55/71] time 0.463 (0.444) data 0.331 (0.313) loss_u loss_u 0.8770 (0.9076) acc_u 18.7500 (12.1023) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [60/71] time 0.452 (0.444) data 0.321 (0.313) loss_u loss_u 0.9214 (0.9103) acc_u 12.5000 (11.7188) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [65/71] time 0.451 (0.446) data 0.321 (0.315) loss_u loss_u 0.9180 (0.9105) acc_u 9.3750 (11.4904) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [70/71] time 0.373 (0.446) data 0.243 (0.315) loss_u loss_u 0.9395 (0.9102) acc_u 9.3750 (11.5625) lr 1.7181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1590
confident_label rate tensor(0.2739, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 859
clean true:844
clean false:15
clean_rate:0.9825378346915018
noisy true:702
noisy false:1575
after delete: len(clean_dataset) 859
after delete: len(noisy_dataset) 2277
epoch [52/200] batch [5/26] time 0.560 (0.563) data 0.430 (0.432) loss_x loss_x 1.1924 (1.2000) acc_x 75.0000 (66.8750) lr 1.7071e-03 eta 0:00:11
epoch [52/200] batch [10/26] time 0.402 (0.515) data 0.267 (0.384) loss_x loss_x 1.5449 (1.3012) acc_x 65.6250 (65.9375) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [15/26] time 0.436 (0.502) data 0.303 (0.370) loss_x loss_x 1.4443 (1.2662) acc_x 68.7500 (67.7083) lr 1.7071e-03 eta 0:00:05
epoch [52/200] batch [20/26] time 0.425 (0.480) data 0.294 (0.349) loss_x loss_x 1.9033 (1.3066) acc_x 59.3750 (67.0312) lr 1.7071e-03 eta 0:00:02
epoch [52/200] batch [25/26] time 0.612 (0.484) data 0.481 (0.353) loss_x loss_x 0.8691 (1.3180) acc_x 81.2500 (67.6250) lr 1.7071e-03 eta 0:00:00
epoch [52/200] batch [5/71] time 0.744 (0.489) data 0.612 (0.358) loss_u loss_u 0.9468 (0.9225) acc_u 6.2500 (10.0000) lr 1.7071e-03 eta 0:00:32
epoch [52/200] batch [10/71] time 0.337 (0.480) data 0.207 (0.349) loss_u loss_u 0.9341 (0.9187) acc_u 12.5000 (10.9375) lr 1.7071e-03 eta 0:00:29
epoch [52/200] batch [15/71] time 0.498 (0.483) data 0.366 (0.352) loss_u loss_u 0.8760 (0.9105) acc_u 15.6250 (12.7083) lr 1.7071e-03 eta 0:00:27
epoch [52/200] batch [20/71] time 0.513 (0.474) data 0.383 (0.343) loss_u loss_u 0.9336 (0.9142) acc_u 12.5000 (12.1875) lr 1.7071e-03 eta 0:00:24
epoch [52/200] batch [25/71] time 0.612 (0.473) data 0.481 (0.342) loss_u loss_u 0.9009 (0.9131) acc_u 12.5000 (12.5000) lr 1.7071e-03 eta 0:00:21
epoch [52/200] batch [30/71] time 0.430 (0.470) data 0.300 (0.339) loss_u loss_u 0.9653 (0.9151) acc_u 3.1250 (11.9792) lr 1.7071e-03 eta 0:00:19
epoch [52/200] batch [35/71] time 0.459 (0.470) data 0.328 (0.339) loss_u loss_u 0.9102 (0.9144) acc_u 9.3750 (11.8750) lr 1.7071e-03 eta 0:00:16
epoch [52/200] batch [40/71] time 0.428 (0.471) data 0.296 (0.339) loss_u loss_u 0.9580 (0.9136) acc_u 6.2500 (12.1875) lr 1.7071e-03 eta 0:00:14
epoch [52/200] batch [45/71] time 0.485 (0.470) data 0.353 (0.339) loss_u loss_u 0.8149 (0.9143) acc_u 28.1250 (12.0833) lr 1.7071e-03 eta 0:00:12
epoch [52/200] batch [50/71] time 0.476 (0.469) data 0.344 (0.338) loss_u loss_u 0.8735 (0.9133) acc_u 15.6250 (12.0625) lr 1.7071e-03 eta 0:00:09
epoch [52/200] batch [55/71] time 0.377 (0.468) data 0.246 (0.337) loss_u loss_u 0.9536 (0.9141) acc_u 6.2500 (11.8750) lr 1.7071e-03 eta 0:00:07
epoch [52/200] batch [60/71] time 0.451 (0.466) data 0.319 (0.335) loss_u loss_u 0.8809 (0.9132) acc_u 9.3750 (11.9792) lr 1.7071e-03 eta 0:00:05
epoch [52/200] batch [65/71] time 0.411 (0.465) data 0.279 (0.334) loss_u loss_u 0.8965 (0.9155) acc_u 9.3750 (11.4904) lr 1.7071e-03 eta 0:00:02
epoch [52/200] batch [70/71] time 0.402 (0.463) data 0.269 (0.332) loss_u loss_u 0.9194 (0.9157) acc_u 12.5000 (11.4286) lr 1.7071e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1596
confident_label rate tensor(0.2710, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 850
clean true:845
clean false:5
clean_rate:0.9941176470588236
noisy true:695
noisy false:1591
after delete: len(clean_dataset) 850
after delete: len(noisy_dataset) 2286
epoch [53/200] batch [5/26] time 0.441 (0.463) data 0.309 (0.333) loss_x loss_x 1.1279 (1.3754) acc_x 62.5000 (61.2500) lr 1.6959e-03 eta 0:00:09
epoch [53/200] batch [10/26] time 0.528 (0.481) data 0.397 (0.350) loss_x loss_x 0.9487 (1.3005) acc_x 71.8750 (64.3750) lr 1.6959e-03 eta 0:00:07
epoch [53/200] batch [15/26] time 0.531 (0.467) data 0.400 (0.336) loss_x loss_x 0.7334 (1.1941) acc_x 84.3750 (69.3750) lr 1.6959e-03 eta 0:00:05
epoch [53/200] batch [20/26] time 0.450 (0.467) data 0.320 (0.336) loss_x loss_x 1.4434 (1.2465) acc_x 65.6250 (67.8125) lr 1.6959e-03 eta 0:00:02
epoch [53/200] batch [25/26] time 0.483 (0.474) data 0.353 (0.343) loss_x loss_x 0.8423 (1.2340) acc_x 78.1250 (68.2500) lr 1.6959e-03 eta 0:00:00
epoch [53/200] batch [5/71] time 0.612 (0.468) data 0.481 (0.337) loss_u loss_u 0.9380 (0.9408) acc_u 9.3750 (8.7500) lr 1.6959e-03 eta 0:00:30
epoch [53/200] batch [10/71] time 0.357 (0.466) data 0.226 (0.335) loss_u loss_u 0.9673 (0.9373) acc_u 6.2500 (8.1250) lr 1.6959e-03 eta 0:00:28
epoch [53/200] batch [15/71] time 0.382 (0.457) data 0.250 (0.326) loss_u loss_u 0.9102 (0.9314) acc_u 12.5000 (8.5417) lr 1.6959e-03 eta 0:00:25
epoch [53/200] batch [20/71] time 0.354 (0.449) data 0.224 (0.318) loss_u loss_u 0.9663 (0.9344) acc_u 6.2500 (8.2812) lr 1.6959e-03 eta 0:00:22
epoch [53/200] batch [25/71] time 0.380 (0.446) data 0.250 (0.315) loss_u loss_u 0.8906 (0.9268) acc_u 15.6250 (9.3750) lr 1.6959e-03 eta 0:00:20
epoch [53/200] batch [30/71] time 0.504 (0.445) data 0.373 (0.314) loss_u loss_u 0.9204 (0.9239) acc_u 12.5000 (9.8958) lr 1.6959e-03 eta 0:00:18
epoch [53/200] batch [35/71] time 0.382 (0.449) data 0.251 (0.318) loss_u loss_u 0.9424 (0.9251) acc_u 3.1250 (9.4643) lr 1.6959e-03 eta 0:00:16
epoch [53/200] batch [40/71] time 0.391 (0.445) data 0.260 (0.314) loss_u loss_u 0.8872 (0.9228) acc_u 9.3750 (9.3750) lr 1.6959e-03 eta 0:00:13
epoch [53/200] batch [45/71] time 0.362 (0.449) data 0.230 (0.318) loss_u loss_u 0.9463 (0.9243) acc_u 9.3750 (9.3750) lr 1.6959e-03 eta 0:00:11
epoch [53/200] batch [50/71] time 0.381 (0.449) data 0.250 (0.318) loss_u loss_u 0.8740 (0.9203) acc_u 15.6250 (10.0625) lr 1.6959e-03 eta 0:00:09
epoch [53/200] batch [55/71] time 0.438 (0.450) data 0.306 (0.319) loss_u loss_u 0.9106 (0.9195) acc_u 9.3750 (10.0568) lr 1.6959e-03 eta 0:00:07
epoch [53/200] batch [60/71] time 0.430 (0.449) data 0.299 (0.318) loss_u loss_u 0.9473 (0.9205) acc_u 6.2500 (10.0000) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [65/71] time 0.319 (0.449) data 0.187 (0.318) loss_u loss_u 0.9380 (0.9207) acc_u 6.2500 (9.9038) lr 1.6959e-03 eta 0:00:02
epoch [53/200] batch [70/71] time 0.482 (0.450) data 0.350 (0.319) loss_u loss_u 0.9575 (0.9188) acc_u 6.2500 (10.2232) lr 1.6959e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1618
confident_label rate tensor(0.2653, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 832
clean true:823
clean false:9
clean_rate:0.9891826923076923
noisy true:695
noisy false:1609
after delete: len(clean_dataset) 832
after delete: len(noisy_dataset) 2304
epoch [54/200] batch [5/26] time 0.368 (0.458) data 0.238 (0.328) loss_x loss_x 1.7549 (1.4094) acc_x 59.3750 (70.0000) lr 1.6845e-03 eta 0:00:09
epoch [54/200] batch [10/26] time 0.546 (0.466) data 0.415 (0.335) loss_x loss_x 2.0234 (1.3999) acc_x 59.3750 (67.5000) lr 1.6845e-03 eta 0:00:07
epoch [54/200] batch [15/26] time 0.626 (0.476) data 0.495 (0.345) loss_x loss_x 1.2061 (1.3664) acc_x 62.5000 (68.1250) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [20/26] time 0.566 (0.468) data 0.435 (0.337) loss_x loss_x 1.3701 (1.3831) acc_x 78.1250 (66.8750) lr 1.6845e-03 eta 0:00:02
epoch [54/200] batch [25/26] time 0.439 (0.465) data 0.308 (0.335) loss_x loss_x 0.6772 (1.3293) acc_x 78.1250 (67.0000) lr 1.6845e-03 eta 0:00:00
epoch [54/200] batch [5/72] time 0.416 (0.459) data 0.284 (0.329) loss_u loss_u 0.8965 (0.9346) acc_u 9.3750 (8.1250) lr 1.6845e-03 eta 0:00:30
epoch [54/200] batch [10/72] time 0.455 (0.465) data 0.323 (0.334) loss_u loss_u 0.9097 (0.9120) acc_u 9.3750 (10.9375) lr 1.6845e-03 eta 0:00:28
epoch [54/200] batch [15/72] time 0.680 (0.468) data 0.549 (0.336) loss_u loss_u 0.9180 (0.9182) acc_u 12.5000 (10.0000) lr 1.6845e-03 eta 0:00:26
epoch [54/200] batch [20/72] time 0.323 (0.464) data 0.192 (0.333) loss_u loss_u 0.8813 (0.9091) acc_u 15.6250 (11.4062) lr 1.6845e-03 eta 0:00:24
epoch [54/200] batch [25/72] time 0.388 (0.462) data 0.257 (0.331) loss_u loss_u 0.9185 (0.9118) acc_u 12.5000 (11.6250) lr 1.6845e-03 eta 0:00:21
epoch [54/200] batch [30/72] time 0.420 (0.461) data 0.288 (0.330) loss_u loss_u 0.9619 (0.9100) acc_u 6.2500 (12.0833) lr 1.6845e-03 eta 0:00:19
epoch [54/200] batch [35/72] time 0.626 (0.461) data 0.495 (0.330) loss_u loss_u 0.9067 (0.9080) acc_u 12.5000 (12.2321) lr 1.6845e-03 eta 0:00:17
epoch [54/200] batch [40/72] time 0.616 (0.462) data 0.485 (0.331) loss_u loss_u 0.9316 (0.9070) acc_u 9.3750 (12.4219) lr 1.6845e-03 eta 0:00:14
epoch [54/200] batch [45/72] time 0.405 (0.462) data 0.273 (0.330) loss_u loss_u 0.9829 (0.9092) acc_u 3.1250 (12.2917) lr 1.6845e-03 eta 0:00:12
epoch [54/200] batch [50/72] time 0.371 (0.459) data 0.241 (0.328) loss_u loss_u 0.8438 (0.9082) acc_u 12.5000 (12.1875) lr 1.6845e-03 eta 0:00:10
epoch [54/200] batch [55/72] time 0.372 (0.463) data 0.237 (0.332) loss_u loss_u 0.9014 (0.9083) acc_u 12.5000 (11.9886) lr 1.6845e-03 eta 0:00:07
epoch [54/200] batch [60/72] time 0.343 (0.460) data 0.212 (0.329) loss_u loss_u 0.9087 (0.9101) acc_u 6.2500 (11.6146) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [65/72] time 0.405 (0.459) data 0.275 (0.328) loss_u loss_u 0.9277 (0.9097) acc_u 6.2500 (11.6346) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [70/72] time 0.511 (0.460) data 0.380 (0.328) loss_u loss_u 0.9214 (0.9104) acc_u 12.5000 (11.6518) lr 1.6845e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1584
confident_label rate tensor(0.2643, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 829
clean true:820
clean false:9
clean_rate:0.9891435464414958
noisy true:732
noisy false:1575
after delete: len(clean_dataset) 829
after delete: len(noisy_dataset) 2307
epoch [55/200] batch [5/25] time 0.425 (0.484) data 0.294 (0.353) loss_x loss_x 1.3418 (1.2951) acc_x 65.6250 (67.5000) lr 1.6730e-03 eta 0:00:09
epoch [55/200] batch [10/25] time 0.409 (0.499) data 0.278 (0.368) loss_x loss_x 1.0127 (1.2544) acc_x 75.0000 (67.5000) lr 1.6730e-03 eta 0:00:07
epoch [55/200] batch [15/25] time 0.444 (0.466) data 0.314 (0.336) loss_x loss_x 1.2090 (1.2190) acc_x 62.5000 (67.9167) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [20/25] time 0.504 (0.465) data 0.373 (0.335) loss_x loss_x 1.4639 (1.2427) acc_x 68.7500 (68.4375) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [25/25] time 0.438 (0.472) data 0.309 (0.342) loss_x loss_x 0.9702 (1.2383) acc_x 68.7500 (67.6250) lr 1.6730e-03 eta 0:00:00
epoch [55/200] batch [5/72] time 0.436 (0.465) data 0.305 (0.334) loss_u loss_u 0.9419 (0.8876) acc_u 6.2500 (13.1250) lr 1.6730e-03 eta 0:00:31
epoch [55/200] batch [10/72] time 0.576 (0.473) data 0.444 (0.342) loss_u loss_u 0.8862 (0.8988) acc_u 15.6250 (12.5000) lr 1.6730e-03 eta 0:00:29
epoch [55/200] batch [15/72] time 0.562 (0.475) data 0.431 (0.344) loss_u loss_u 0.8701 (0.9084) acc_u 12.5000 (10.6250) lr 1.6730e-03 eta 0:00:27
epoch [55/200] batch [20/72] time 0.404 (0.468) data 0.272 (0.337) loss_u loss_u 0.9370 (0.9092) acc_u 9.3750 (10.7812) lr 1.6730e-03 eta 0:00:24
epoch [55/200] batch [25/72] time 0.428 (0.469) data 0.298 (0.338) loss_u loss_u 0.9180 (0.9085) acc_u 12.5000 (11.0000) lr 1.6730e-03 eta 0:00:22
epoch [55/200] batch [30/72] time 0.413 (0.470) data 0.282 (0.339) loss_u loss_u 0.9326 (0.9063) acc_u 9.3750 (11.6667) lr 1.6730e-03 eta 0:00:19
epoch [55/200] batch [35/72] time 0.406 (0.463) data 0.275 (0.332) loss_u loss_u 0.8892 (0.9051) acc_u 12.5000 (11.9643) lr 1.6730e-03 eta 0:00:17
epoch [55/200] batch [40/72] time 0.415 (0.459) data 0.283 (0.328) loss_u loss_u 0.9663 (0.9102) acc_u 6.2500 (11.7188) lr 1.6730e-03 eta 0:00:14
epoch [55/200] batch [45/72] time 0.469 (0.464) data 0.337 (0.332) loss_u loss_u 0.8662 (0.9101) acc_u 15.6250 (12.0139) lr 1.6730e-03 eta 0:00:12
epoch [55/200] batch [50/72] time 0.532 (0.466) data 0.401 (0.334) loss_u loss_u 0.9692 (0.9117) acc_u 3.1250 (11.7500) lr 1.6730e-03 eta 0:00:10
epoch [55/200] batch [55/72] time 0.350 (0.461) data 0.220 (0.330) loss_u loss_u 0.9497 (0.9118) acc_u 9.3750 (11.8750) lr 1.6730e-03 eta 0:00:07
epoch [55/200] batch [60/72] time 0.326 (0.460) data 0.194 (0.328) loss_u loss_u 0.9136 (0.9126) acc_u 15.6250 (11.7188) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [65/72] time 0.425 (0.462) data 0.294 (0.331) loss_u loss_u 0.9092 (0.9119) acc_u 12.5000 (11.7788) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [70/72] time 0.407 (0.461) data 0.275 (0.330) loss_u loss_u 0.9229 (0.9114) acc_u 15.6250 (11.9196) lr 1.6730e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1570
confident_label rate tensor(0.2720, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 853
clean true:842
clean false:11
clean_rate:0.9871043376318874
noisy true:724
noisy false:1559
after delete: len(clean_dataset) 853
after delete: len(noisy_dataset) 2283
epoch [56/200] batch [5/26] time 0.419 (0.495) data 0.289 (0.364) loss_x loss_x 1.3369 (1.3539) acc_x 65.6250 (66.8750) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [10/26] time 0.403 (0.483) data 0.273 (0.353) loss_x loss_x 1.8467 (1.3343) acc_x 53.1250 (65.0000) lr 1.6613e-03 eta 0:00:07
epoch [56/200] batch [15/26] time 0.592 (0.465) data 0.462 (0.335) loss_x loss_x 1.1602 (1.3149) acc_x 65.6250 (64.3750) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [20/26] time 0.502 (0.457) data 0.371 (0.326) loss_x loss_x 1.1143 (1.2926) acc_x 71.8750 (65.4688) lr 1.6613e-03 eta 0:00:02
epoch [56/200] batch [25/26] time 0.395 (0.458) data 0.264 (0.327) loss_x loss_x 1.1689 (1.2536) acc_x 62.5000 (66.5000) lr 1.6613e-03 eta 0:00:00
epoch [56/200] batch [5/71] time 0.402 (0.459) data 0.271 (0.329) loss_u loss_u 0.7935 (0.8846) acc_u 21.8750 (12.5000) lr 1.6613e-03 eta 0:00:30
epoch [56/200] batch [10/71] time 0.492 (0.457) data 0.360 (0.326) loss_u loss_u 0.8984 (0.8906) acc_u 12.5000 (12.8125) lr 1.6613e-03 eta 0:00:27
epoch [56/200] batch [15/71] time 0.560 (0.453) data 0.429 (0.322) loss_u loss_u 0.9116 (0.8895) acc_u 12.5000 (13.9583) lr 1.6613e-03 eta 0:00:25
epoch [56/200] batch [20/71] time 0.424 (0.451) data 0.292 (0.321) loss_u loss_u 0.9238 (0.8936) acc_u 12.5000 (14.0625) lr 1.6613e-03 eta 0:00:23
epoch [56/200] batch [25/71] time 0.469 (0.454) data 0.337 (0.323) loss_u loss_u 0.8955 (0.9004) acc_u 15.6250 (13.1250) lr 1.6613e-03 eta 0:00:20
epoch [56/200] batch [30/71] time 0.499 (0.460) data 0.367 (0.329) loss_u loss_u 0.8799 (0.9016) acc_u 21.8750 (12.7083) lr 1.6613e-03 eta 0:00:18
epoch [56/200] batch [35/71] time 0.631 (0.459) data 0.499 (0.328) loss_u loss_u 0.9321 (0.9033) acc_u 6.2500 (12.2321) lr 1.6613e-03 eta 0:00:16
epoch [56/200] batch [40/71] time 0.419 (0.458) data 0.287 (0.327) loss_u loss_u 0.9517 (0.9049) acc_u 9.3750 (12.4219) lr 1.6613e-03 eta 0:00:14
epoch [56/200] batch [45/71] time 0.457 (0.458) data 0.326 (0.327) loss_u loss_u 0.9219 (0.9095) acc_u 9.3750 (11.9444) lr 1.6613e-03 eta 0:00:11
epoch [56/200] batch [50/71] time 0.422 (0.458) data 0.291 (0.326) loss_u loss_u 0.9595 (0.9123) acc_u 0.0000 (11.5625) lr 1.6613e-03 eta 0:00:09
epoch [56/200] batch [55/71] time 0.429 (0.458) data 0.298 (0.327) loss_u loss_u 0.8945 (0.9114) acc_u 12.5000 (11.5909) lr 1.6613e-03 eta 0:00:07
epoch [56/200] batch [60/71] time 0.401 (0.455) data 0.271 (0.324) loss_u loss_u 0.9448 (0.9109) acc_u 9.3750 (11.8229) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [65/71] time 0.511 (0.455) data 0.379 (0.324) loss_u loss_u 0.9019 (0.9106) acc_u 18.7500 (12.0192) lr 1.6613e-03 eta 0:00:02
epoch [56/200] batch [70/71] time 0.316 (0.456) data 0.186 (0.325) loss_u loss_u 0.9570 (0.9094) acc_u 6.2500 (12.1875) lr 1.6613e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1591
confident_label rate tensor(0.2768, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 868
clean true:860
clean false:8
clean_rate:0.9907834101382489
noisy true:685
noisy false:1583
after delete: len(clean_dataset) 868
after delete: len(noisy_dataset) 2268
epoch [57/200] batch [5/27] time 0.647 (0.544) data 0.514 (0.413) loss_x loss_x 1.2461 (1.3770) acc_x 75.0000 (65.0000) lr 1.6494e-03 eta 0:00:11
epoch [57/200] batch [10/27] time 0.392 (0.507) data 0.262 (0.375) loss_x loss_x 1.0703 (1.2294) acc_x 71.8750 (68.1250) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [15/27] time 0.496 (0.506) data 0.366 (0.375) loss_x loss_x 1.0859 (1.2545) acc_x 62.5000 (66.6667) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [20/27] time 0.376 (0.486) data 0.246 (0.355) loss_x loss_x 1.1211 (1.2413) acc_x 75.0000 (67.6562) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [25/27] time 0.551 (0.490) data 0.420 (0.360) loss_x loss_x 1.1846 (1.2216) acc_x 62.5000 (68.3750) lr 1.6494e-03 eta 0:00:00
epoch [57/200] batch [5/70] time 0.492 (0.483) data 0.361 (0.352) loss_u loss_u 0.9492 (0.9283) acc_u 6.2500 (7.5000) lr 1.6494e-03 eta 0:00:31
epoch [57/200] batch [10/70] time 0.476 (0.486) data 0.344 (0.355) loss_u loss_u 0.9077 (0.9211) acc_u 6.2500 (8.1250) lr 1.6494e-03 eta 0:00:29
epoch [57/200] batch [15/70] time 0.484 (0.484) data 0.352 (0.353) loss_u loss_u 0.8721 (0.9133) acc_u 15.6250 (10.2083) lr 1.6494e-03 eta 0:00:26
epoch [57/200] batch [20/70] time 0.516 (0.484) data 0.385 (0.353) loss_u loss_u 0.9497 (0.9072) acc_u 6.2500 (11.4062) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [25/70] time 0.400 (0.483) data 0.268 (0.351) loss_u loss_u 0.9316 (0.9087) acc_u 6.2500 (10.8750) lr 1.6494e-03 eta 0:00:21
epoch [57/200] batch [30/70] time 0.424 (0.479) data 0.292 (0.348) loss_u loss_u 0.9175 (0.9086) acc_u 12.5000 (10.8333) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [35/70] time 0.494 (0.478) data 0.363 (0.347) loss_u loss_u 0.9062 (0.9074) acc_u 9.3750 (11.1607) lr 1.6494e-03 eta 0:00:16
epoch [57/200] batch [40/70] time 0.371 (0.473) data 0.240 (0.342) loss_u loss_u 0.9502 (0.9097) acc_u 6.2500 (10.9375) lr 1.6494e-03 eta 0:00:14
epoch [57/200] batch [45/70] time 0.365 (0.467) data 0.234 (0.336) loss_u loss_u 0.8896 (0.9099) acc_u 12.5000 (10.9028) lr 1.6494e-03 eta 0:00:11
epoch [57/200] batch [50/70] time 0.396 (0.464) data 0.265 (0.333) loss_u loss_u 0.9160 (0.9109) acc_u 12.5000 (10.9375) lr 1.6494e-03 eta 0:00:09
epoch [57/200] batch [55/70] time 0.415 (0.466) data 0.284 (0.334) loss_u loss_u 0.9155 (0.9085) acc_u 9.3750 (11.1932) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [60/70] time 0.400 (0.465) data 0.268 (0.334) loss_u loss_u 0.9307 (0.9089) acc_u 12.5000 (11.2500) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [65/70] time 0.395 (0.466) data 0.263 (0.334) loss_u loss_u 0.8921 (0.9096) acc_u 12.5000 (11.1538) lr 1.6494e-03 eta 0:00:02
epoch [57/200] batch [70/70] time 0.698 (0.466) data 0.566 (0.334) loss_u loss_u 0.8921 (0.9098) acc_u 15.6250 (11.2054) lr 1.6494e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1596
confident_label rate tensor(0.2707, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 849
clean true:840
clean false:9
clean_rate:0.9893992932862191
noisy true:700
noisy false:1587
after delete: len(clean_dataset) 849
after delete: len(noisy_dataset) 2287
epoch [58/200] batch [5/26] time 0.378 (0.542) data 0.247 (0.412) loss_x loss_x 1.1230 (1.0870) acc_x 71.8750 (75.0000) lr 1.6374e-03 eta 0:00:11
epoch [58/200] batch [10/26] time 0.457 (0.506) data 0.326 (0.376) loss_x loss_x 1.9863 (1.2668) acc_x 62.5000 (69.3750) lr 1.6374e-03 eta 0:00:08
epoch [58/200] batch [15/26] time 0.453 (0.490) data 0.323 (0.359) loss_x loss_x 1.4746 (1.2486) acc_x 59.3750 (69.7917) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [20/26] time 0.540 (0.496) data 0.409 (0.365) loss_x loss_x 1.5713 (1.2788) acc_x 62.5000 (69.8438) lr 1.6374e-03 eta 0:00:02
epoch [58/200] batch [25/26] time 0.537 (0.496) data 0.407 (0.366) loss_x loss_x 1.1973 (1.2541) acc_x 81.2500 (70.8750) lr 1.6374e-03 eta 0:00:00
epoch [58/200] batch [5/71] time 0.444 (0.492) data 0.308 (0.361) loss_u loss_u 0.8145 (0.8781) acc_u 25.0000 (15.6250) lr 1.6374e-03 eta 0:00:32
epoch [58/200] batch [10/71] time 0.416 (0.481) data 0.285 (0.350) loss_u loss_u 0.8970 (0.9004) acc_u 9.3750 (13.1250) lr 1.6374e-03 eta 0:00:29
epoch [58/200] batch [15/71] time 0.481 (0.478) data 0.351 (0.348) loss_u loss_u 0.9478 (0.9098) acc_u 6.2500 (11.6667) lr 1.6374e-03 eta 0:00:26
epoch [58/200] batch [20/71] time 0.519 (0.475) data 0.387 (0.344) loss_u loss_u 0.8706 (0.9005) acc_u 15.6250 (13.1250) lr 1.6374e-03 eta 0:00:24
epoch [58/200] batch [25/71] time 0.388 (0.468) data 0.258 (0.337) loss_u loss_u 0.9058 (0.9047) acc_u 9.3750 (12.2500) lr 1.6374e-03 eta 0:00:21
epoch [58/200] batch [30/71] time 0.611 (0.471) data 0.480 (0.340) loss_u loss_u 0.8872 (0.9092) acc_u 15.6250 (11.7708) lr 1.6374e-03 eta 0:00:19
epoch [58/200] batch [35/71] time 0.402 (0.467) data 0.272 (0.336) loss_u loss_u 0.9429 (0.9081) acc_u 9.3750 (11.8750) lr 1.6374e-03 eta 0:00:16
epoch [58/200] batch [40/71] time 0.396 (0.462) data 0.265 (0.331) loss_u loss_u 0.9131 (0.9089) acc_u 9.3750 (11.6406) lr 1.6374e-03 eta 0:00:14
epoch [58/200] batch [45/71] time 0.428 (0.459) data 0.297 (0.328) loss_u loss_u 0.9385 (0.9069) acc_u 9.3750 (11.9444) lr 1.6374e-03 eta 0:00:11
epoch [58/200] batch [50/71] time 0.354 (0.459) data 0.223 (0.328) loss_u loss_u 0.8975 (0.9060) acc_u 18.7500 (12.3750) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [55/71] time 0.415 (0.460) data 0.285 (0.329) loss_u loss_u 0.9565 (0.9081) acc_u 3.1250 (12.1023) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [60/71] time 0.525 (0.462) data 0.394 (0.331) loss_u loss_u 0.9395 (0.9093) acc_u 6.2500 (11.7708) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [65/71] time 0.433 (0.460) data 0.302 (0.329) loss_u loss_u 0.8940 (0.9101) acc_u 12.5000 (11.6346) lr 1.6374e-03 eta 0:00:02
epoch [58/200] batch [70/71] time 0.401 (0.459) data 0.269 (0.328) loss_u loss_u 0.8384 (0.9079) acc_u 18.7500 (11.9196) lr 1.6374e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1556
confident_label rate tensor(0.2733, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 857
clean true:851
clean false:6
clean_rate:0.9929988331388565
noisy true:729
noisy false:1550
after delete: len(clean_dataset) 857
after delete: len(noisy_dataset) 2279
epoch [59/200] batch [5/26] time 0.521 (0.505) data 0.390 (0.374) loss_x loss_x 1.9219 (1.5016) acc_x 46.8750 (61.8750) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [10/26] time 0.475 (0.480) data 0.344 (0.349) loss_x loss_x 1.4473 (1.3603) acc_x 56.2500 (65.0000) lr 1.6252e-03 eta 0:00:07
epoch [59/200] batch [15/26] time 0.717 (0.500) data 0.587 (0.369) loss_x loss_x 1.6475 (1.3013) acc_x 62.5000 (66.4583) lr 1.6252e-03 eta 0:00:05
epoch [59/200] batch [20/26] time 0.470 (0.481) data 0.340 (0.350) loss_x loss_x 1.2744 (1.2894) acc_x 65.6250 (65.7812) lr 1.6252e-03 eta 0:00:02
epoch [59/200] batch [25/26] time 0.444 (0.475) data 0.314 (0.344) loss_x loss_x 1.4111 (1.2803) acc_x 68.7500 (66.5000) lr 1.6252e-03 eta 0:00:00
epoch [59/200] batch [5/71] time 0.570 (0.480) data 0.440 (0.349) loss_u loss_u 0.8828 (0.9095) acc_u 12.5000 (10.0000) lr 1.6252e-03 eta 0:00:31
epoch [59/200] batch [10/71] time 0.395 (0.481) data 0.264 (0.350) loss_u loss_u 0.9146 (0.8995) acc_u 9.3750 (10.9375) lr 1.6252e-03 eta 0:00:29
epoch [59/200] batch [15/71] time 0.483 (0.477) data 0.353 (0.346) loss_u loss_u 0.9351 (0.9049) acc_u 6.2500 (10.8333) lr 1.6252e-03 eta 0:00:26
epoch [59/200] batch [20/71] time 0.481 (0.474) data 0.350 (0.344) loss_u loss_u 0.9663 (0.9142) acc_u 3.1250 (9.8438) lr 1.6252e-03 eta 0:00:24
epoch [59/200] batch [25/71] time 0.488 (0.471) data 0.357 (0.340) loss_u loss_u 0.9395 (0.9192) acc_u 9.3750 (9.2500) lr 1.6252e-03 eta 0:00:21
epoch [59/200] batch [30/71] time 0.469 (0.472) data 0.338 (0.341) loss_u loss_u 0.9375 (0.9177) acc_u 6.2500 (9.6875) lr 1.6252e-03 eta 0:00:19
epoch [59/200] batch [35/71] time 0.710 (0.476) data 0.578 (0.345) loss_u loss_u 0.9717 (0.9168) acc_u 3.1250 (9.9107) lr 1.6252e-03 eta 0:00:17
epoch [59/200] batch [40/71] time 0.500 (0.473) data 0.368 (0.342) loss_u loss_u 0.9326 (0.9164) acc_u 12.5000 (10.0000) lr 1.6252e-03 eta 0:00:14
epoch [59/200] batch [45/71] time 0.510 (0.471) data 0.379 (0.340) loss_u loss_u 0.9458 (0.9180) acc_u 9.3750 (10.0694) lr 1.6252e-03 eta 0:00:12
epoch [59/200] batch [50/71] time 0.512 (0.469) data 0.380 (0.338) loss_u loss_u 0.8242 (0.9152) acc_u 18.7500 (10.3750) lr 1.6252e-03 eta 0:00:09
epoch [59/200] batch [55/71] time 0.434 (0.470) data 0.303 (0.339) loss_u loss_u 0.9360 (0.9135) acc_u 9.3750 (10.6250) lr 1.6252e-03 eta 0:00:07
epoch [59/200] batch [60/71] time 0.446 (0.469) data 0.316 (0.337) loss_u loss_u 0.9878 (0.9153) acc_u 0.0000 (10.5729) lr 1.6252e-03 eta 0:00:05
epoch [59/200] batch [65/71] time 0.395 (0.467) data 0.263 (0.336) loss_u loss_u 0.9824 (0.9149) acc_u 3.1250 (10.5769) lr 1.6252e-03 eta 0:00:02
epoch [59/200] batch [70/71] time 0.435 (0.466) data 0.303 (0.335) loss_u loss_u 0.8701 (0.9134) acc_u 12.5000 (10.7143) lr 1.6252e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1590
confident_label rate tensor(0.2714, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 851
clean true:847
clean false:4
clean_rate:0.9952996474735605
noisy true:699
noisy false:1586
after delete: len(clean_dataset) 851
after delete: len(noisy_dataset) 2285
epoch [60/200] batch [5/26] time 0.448 (0.485) data 0.317 (0.353) loss_x loss_x 1.1680 (1.3307) acc_x 68.7500 (63.1250) lr 1.6129e-03 eta 0:00:10
epoch [60/200] batch [10/26] time 0.397 (0.491) data 0.266 (0.360) loss_x loss_x 1.3730 (1.3083) acc_x 71.8750 (66.8750) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [15/26] time 0.449 (0.471) data 0.318 (0.340) loss_x loss_x 1.4590 (1.3135) acc_x 59.3750 (67.2917) lr 1.6129e-03 eta 0:00:05
epoch [60/200] batch [20/26] time 0.514 (0.462) data 0.383 (0.331) loss_x loss_x 1.5371 (1.3309) acc_x 68.7500 (67.3438) lr 1.6129e-03 eta 0:00:02
epoch [60/200] batch [25/26] time 0.407 (0.459) data 0.276 (0.328) loss_x loss_x 0.9023 (1.2727) acc_x 81.2500 (69.1250) lr 1.6129e-03 eta 0:00:00
epoch [60/200] batch [5/71] time 0.489 (0.459) data 0.358 (0.328) loss_u loss_u 0.9028 (0.9150) acc_u 12.5000 (13.1250) lr 1.6129e-03 eta 0:00:30
epoch [60/200] batch [10/71] time 0.517 (0.468) data 0.386 (0.337) loss_u loss_u 0.9834 (0.9175) acc_u 0.0000 (12.5000) lr 1.6129e-03 eta 0:00:28
epoch [60/200] batch [15/71] time 0.334 (0.460) data 0.202 (0.329) loss_u loss_u 0.9312 (0.9022) acc_u 12.5000 (13.9583) lr 1.6129e-03 eta 0:00:25
epoch [60/200] batch [20/71] time 0.442 (0.457) data 0.310 (0.326) loss_u loss_u 0.8799 (0.9064) acc_u 12.5000 (12.8125) lr 1.6129e-03 eta 0:00:23
epoch [60/200] batch [25/71] time 0.451 (0.449) data 0.318 (0.318) loss_u loss_u 0.8784 (0.9039) acc_u 15.6250 (12.7500) lr 1.6129e-03 eta 0:00:20
epoch [60/200] batch [30/71] time 0.519 (0.449) data 0.386 (0.318) loss_u loss_u 0.9419 (0.9093) acc_u 3.1250 (11.5625) lr 1.6129e-03 eta 0:00:18
epoch [60/200] batch [35/71] time 0.486 (0.454) data 0.350 (0.323) loss_u loss_u 0.9399 (0.9112) acc_u 3.1250 (11.2500) lr 1.6129e-03 eta 0:00:16
epoch [60/200] batch [40/71] time 0.434 (0.455) data 0.303 (0.323) loss_u loss_u 0.9736 (0.9109) acc_u 3.1250 (11.2500) lr 1.6129e-03 eta 0:00:14
epoch [60/200] batch [45/71] time 0.462 (0.454) data 0.331 (0.322) loss_u loss_u 0.8662 (0.9099) acc_u 18.7500 (11.3889) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [50/71] time 0.472 (0.452) data 0.341 (0.320) loss_u loss_u 0.8501 (0.9091) acc_u 21.8750 (11.5000) lr 1.6129e-03 eta 0:00:09
epoch [60/200] batch [55/71] time 0.369 (0.454) data 0.239 (0.322) loss_u loss_u 0.8872 (0.9112) acc_u 18.7500 (11.2500) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [60/71] time 0.425 (0.450) data 0.295 (0.319) loss_u loss_u 0.9092 (0.9121) acc_u 9.3750 (11.0938) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [65/71] time 0.376 (0.450) data 0.245 (0.319) loss_u loss_u 0.8491 (0.9109) acc_u 15.6250 (11.2019) lr 1.6129e-03 eta 0:00:02
epoch [60/200] batch [70/71] time 0.398 (0.451) data 0.266 (0.320) loss_u loss_u 0.9214 (0.9102) acc_u 9.3750 (11.3393) lr 1.6129e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1580
confident_label rate tensor(0.2739, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 859
clean true:855
clean false:4
clean_rate:0.9953434225844005
noisy true:701
noisy false:1576
after delete: len(clean_dataset) 859
after delete: len(noisy_dataset) 2277
epoch [61/200] batch [5/26] time 0.519 (0.523) data 0.387 (0.392) loss_x loss_x 1.7334 (1.5564) acc_x 56.2500 (58.7500) lr 1.6004e-03 eta 0:00:10
epoch [61/200] batch [10/26] time 0.502 (0.496) data 0.371 (0.365) loss_x loss_x 1.3926 (1.4178) acc_x 62.5000 (61.8750) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [15/26] time 0.402 (0.509) data 0.271 (0.379) loss_x loss_x 1.2021 (1.3031) acc_x 68.7500 (65.2083) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [20/26] time 0.505 (0.525) data 0.370 (0.394) loss_x loss_x 1.0322 (1.2417) acc_x 75.0000 (67.6562) lr 1.6004e-03 eta 0:00:03
epoch [61/200] batch [25/26] time 0.542 (0.518) data 0.412 (0.387) loss_x loss_x 1.3799 (1.2375) acc_x 56.2500 (67.1250) lr 1.6004e-03 eta 0:00:00
epoch [61/200] batch [5/71] time 0.503 (0.505) data 0.371 (0.374) loss_u loss_u 0.8872 (0.9110) acc_u 12.5000 (10.6250) lr 1.6004e-03 eta 0:00:33
epoch [61/200] batch [10/71] time 0.395 (0.505) data 0.264 (0.374) loss_u loss_u 0.9097 (0.9163) acc_u 6.2500 (9.0625) lr 1.6004e-03 eta 0:00:30
epoch [61/200] batch [15/71] time 0.404 (0.493) data 0.273 (0.362) loss_u loss_u 0.9058 (0.9012) acc_u 12.5000 (11.2500) lr 1.6004e-03 eta 0:00:27
epoch [61/200] batch [20/71] time 0.477 (0.487) data 0.347 (0.356) loss_u loss_u 0.8970 (0.8968) acc_u 12.5000 (12.0312) lr 1.6004e-03 eta 0:00:24
epoch [61/200] batch [25/71] time 0.430 (0.485) data 0.300 (0.354) loss_u loss_u 0.9434 (0.9025) acc_u 6.2500 (11.5000) lr 1.6004e-03 eta 0:00:22
epoch [61/200] batch [30/71] time 0.364 (0.477) data 0.232 (0.346) loss_u loss_u 0.9331 (0.8999) acc_u 6.2500 (12.0833) lr 1.6004e-03 eta 0:00:19
epoch [61/200] batch [35/71] time 0.465 (0.471) data 0.334 (0.341) loss_u loss_u 0.8691 (0.8977) acc_u 21.8750 (12.9464) lr 1.6004e-03 eta 0:00:16
epoch [61/200] batch [40/71] time 0.426 (0.468) data 0.294 (0.337) loss_u loss_u 0.8853 (0.9010) acc_u 12.5000 (12.5781) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [45/71] time 0.419 (0.466) data 0.287 (0.335) loss_u loss_u 0.9253 (0.9037) acc_u 9.3750 (12.3611) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [50/71] time 0.559 (0.464) data 0.428 (0.333) loss_u loss_u 0.9370 (0.9073) acc_u 12.5000 (12.1250) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [55/71] time 0.468 (0.465) data 0.337 (0.334) loss_u loss_u 0.9282 (0.9100) acc_u 6.2500 (11.7045) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [60/71] time 0.378 (0.467) data 0.246 (0.336) loss_u loss_u 0.8574 (0.9099) acc_u 12.5000 (11.6146) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [65/71] time 0.473 (0.466) data 0.342 (0.334) loss_u loss_u 0.9546 (0.9120) acc_u 6.2500 (11.2500) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [70/71] time 0.474 (0.466) data 0.344 (0.335) loss_u loss_u 0.7964 (0.9123) acc_u 28.1250 (11.2054) lr 1.6004e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1590
confident_label rate tensor(0.2774, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 870
clean true:861
clean false:9
clean_rate:0.9896551724137931
noisy true:685
noisy false:1581
after delete: len(clean_dataset) 870
after delete: len(noisy_dataset) 2266
epoch [62/200] batch [5/27] time 0.470 (0.470) data 0.340 (0.339) loss_x loss_x 1.4492 (1.2354) acc_x 65.6250 (68.7500) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [10/27] time 0.451 (0.467) data 0.320 (0.336) loss_x loss_x 1.1914 (1.2323) acc_x 65.6250 (68.1250) lr 1.5878e-03 eta 0:00:07
epoch [62/200] batch [15/27] time 0.441 (0.459) data 0.311 (0.328) loss_x loss_x 0.7642 (1.2524) acc_x 87.5000 (69.7917) lr 1.5878e-03 eta 0:00:05
epoch [62/200] batch [20/27] time 0.364 (0.468) data 0.232 (0.338) loss_x loss_x 1.3086 (1.2547) acc_x 65.6250 (69.3750) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [25/27] time 0.452 (0.475) data 0.321 (0.345) loss_x loss_x 1.0566 (1.2834) acc_x 78.1250 (69.0000) lr 1.5878e-03 eta 0:00:00
epoch [62/200] batch [5/70] time 0.349 (0.474) data 0.218 (0.343) loss_u loss_u 0.9575 (0.9269) acc_u 6.2500 (9.3750) lr 1.5878e-03 eta 0:00:30
epoch [62/200] batch [10/70] time 0.399 (0.471) data 0.268 (0.341) loss_u loss_u 0.8638 (0.9230) acc_u 15.6250 (10.0000) lr 1.5878e-03 eta 0:00:28
epoch [62/200] batch [15/70] time 0.457 (0.464) data 0.326 (0.333) loss_u loss_u 0.8857 (0.9140) acc_u 15.6250 (11.2500) lr 1.5878e-03 eta 0:00:25
epoch [62/200] batch [20/70] time 0.646 (0.467) data 0.516 (0.336) loss_u loss_u 0.9556 (0.9173) acc_u 6.2500 (10.6250) lr 1.5878e-03 eta 0:00:23
epoch [62/200] batch [25/70] time 0.364 (0.460) data 0.233 (0.329) loss_u loss_u 0.8936 (0.9190) acc_u 12.5000 (10.1250) lr 1.5878e-03 eta 0:00:20
epoch [62/200] batch [30/70] time 0.556 (0.461) data 0.424 (0.330) loss_u loss_u 0.8970 (0.9156) acc_u 12.5000 (10.7292) lr 1.5878e-03 eta 0:00:18
epoch [62/200] batch [35/70] time 0.478 (0.462) data 0.347 (0.331) loss_u loss_u 0.9834 (0.9187) acc_u 3.1250 (10.5357) lr 1.5878e-03 eta 0:00:16
epoch [62/200] batch [40/70] time 0.443 (0.459) data 0.312 (0.328) loss_u loss_u 0.8735 (0.9203) acc_u 25.0000 (10.6250) lr 1.5878e-03 eta 0:00:13
epoch [62/200] batch [45/70] time 0.416 (0.459) data 0.285 (0.328) loss_u loss_u 0.9351 (0.9196) acc_u 9.3750 (10.6944) lr 1.5878e-03 eta 0:00:11
epoch [62/200] batch [50/70] time 0.481 (0.459) data 0.349 (0.328) loss_u loss_u 0.8511 (0.9183) acc_u 15.6250 (10.6250) lr 1.5878e-03 eta 0:00:09
epoch [62/200] batch [55/70] time 0.536 (0.458) data 0.404 (0.327) loss_u loss_u 0.9365 (0.9171) acc_u 9.3750 (10.6818) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [60/70] time 0.404 (0.455) data 0.273 (0.324) loss_u loss_u 0.9741 (0.9167) acc_u 6.2500 (10.8333) lr 1.5878e-03 eta 0:00:04
epoch [62/200] batch [65/70] time 0.665 (0.457) data 0.532 (0.326) loss_u loss_u 0.8965 (0.9163) acc_u 12.5000 (10.8654) lr 1.5878e-03 eta 0:00:02
epoch [62/200] batch [70/70] time 0.367 (0.458) data 0.236 (0.327) loss_u loss_u 0.8848 (0.9148) acc_u 9.3750 (10.9375) lr 1.5878e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1568
confident_label rate tensor(0.2812, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 882
clean true:872
clean false:10
clean_rate:0.9886621315192744
noisy true:696
noisy false:1558
after delete: len(clean_dataset) 882
after delete: len(noisy_dataset) 2254
epoch [63/200] batch [5/27] time 0.547 (0.522) data 0.416 (0.391) loss_x loss_x 1.0977 (1.2688) acc_x 68.7500 (68.1250) lr 1.5750e-03 eta 0:00:11
epoch [63/200] batch [10/27] time 0.450 (0.496) data 0.319 (0.364) loss_x loss_x 1.1387 (1.2479) acc_x 71.8750 (68.7500) lr 1.5750e-03 eta 0:00:08
epoch [63/200] batch [15/27] time 0.437 (0.481) data 0.307 (0.350) loss_x loss_x 1.3320 (1.2068) acc_x 68.7500 (69.3750) lr 1.5750e-03 eta 0:00:05
epoch [63/200] batch [20/27] time 0.396 (0.474) data 0.266 (0.342) loss_x loss_x 1.7393 (1.2475) acc_x 50.0000 (68.1250) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [25/27] time 0.458 (0.466) data 0.328 (0.335) loss_x loss_x 1.5215 (1.2771) acc_x 53.1250 (66.6250) lr 1.5750e-03 eta 0:00:00
epoch [63/200] batch [5/70] time 0.406 (0.465) data 0.274 (0.334) loss_u loss_u 0.9434 (0.9135) acc_u 9.3750 (11.8750) lr 1.5750e-03 eta 0:00:30
epoch [63/200] batch [10/70] time 0.358 (0.459) data 0.227 (0.328) loss_u loss_u 0.9165 (0.9207) acc_u 9.3750 (10.9375) lr 1.5750e-03 eta 0:00:27
epoch [63/200] batch [15/70] time 0.355 (0.457) data 0.223 (0.326) loss_u loss_u 0.9399 (0.9246) acc_u 9.3750 (10.6250) lr 1.5750e-03 eta 0:00:25
epoch [63/200] batch [20/70] time 0.419 (0.454) data 0.288 (0.323) loss_u loss_u 0.8774 (0.9177) acc_u 12.5000 (11.4062) lr 1.5750e-03 eta 0:00:22
epoch [63/200] batch [25/70] time 0.497 (0.454) data 0.366 (0.323) loss_u loss_u 0.9473 (0.9190) acc_u 3.1250 (11.1250) lr 1.5750e-03 eta 0:00:20
epoch [63/200] batch [30/70] time 0.367 (0.451) data 0.236 (0.320) loss_u loss_u 0.9146 (0.9157) acc_u 15.6250 (11.7708) lr 1.5750e-03 eta 0:00:18
epoch [63/200] batch [35/70] time 0.435 (0.452) data 0.303 (0.321) loss_u loss_u 0.9307 (0.9156) acc_u 6.2500 (11.2500) lr 1.5750e-03 eta 0:00:15
epoch [63/200] batch [40/70] time 0.431 (0.455) data 0.299 (0.324) loss_u loss_u 0.9321 (0.9160) acc_u 6.2500 (11.0938) lr 1.5750e-03 eta 0:00:13
epoch [63/200] batch [45/70] time 0.529 (0.456) data 0.397 (0.325) loss_u loss_u 0.8188 (0.9135) acc_u 28.1250 (11.4583) lr 1.5750e-03 eta 0:00:11
epoch [63/200] batch [50/70] time 0.568 (0.457) data 0.434 (0.325) loss_u loss_u 0.8726 (0.9133) acc_u 15.6250 (11.5000) lr 1.5750e-03 eta 0:00:09
epoch [63/200] batch [55/70] time 0.361 (0.455) data 0.225 (0.324) loss_u loss_u 0.8936 (0.9136) acc_u 12.5000 (11.5341) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [60/70] time 0.405 (0.458) data 0.273 (0.327) loss_u loss_u 0.9131 (0.9137) acc_u 15.6250 (11.6146) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [65/70] time 0.410 (0.456) data 0.278 (0.324) loss_u loss_u 0.9204 (0.9140) acc_u 6.2500 (11.5865) lr 1.5750e-03 eta 0:00:02
epoch [63/200] batch [70/70] time 0.404 (0.457) data 0.273 (0.325) loss_u loss_u 0.9121 (0.9152) acc_u 15.6250 (11.4732) lr 1.5750e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1569
confident_label rate tensor(0.2736, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 858
clean true:847
clean false:11
clean_rate:0.9871794871794872
noisy true:720
noisy false:1558
after delete: len(clean_dataset) 858
after delete: len(noisy_dataset) 2278
epoch [64/200] batch [5/26] time 0.383 (0.503) data 0.253 (0.373) loss_x loss_x 1.5518 (1.2783) acc_x 53.1250 (65.0000) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [10/26] time 0.459 (0.490) data 0.328 (0.359) loss_x loss_x 1.5957 (1.2266) acc_x 56.2500 (67.5000) lr 1.5621e-03 eta 0:00:07
epoch [64/200] batch [15/26] time 0.457 (0.490) data 0.327 (0.359) loss_x loss_x 1.8164 (1.2759) acc_x 59.3750 (68.1250) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [20/26] time 0.319 (0.490) data 0.188 (0.359) loss_x loss_x 1.1074 (1.2490) acc_x 81.2500 (69.2188) lr 1.5621e-03 eta 0:00:02
epoch [64/200] batch [25/26] time 0.361 (0.479) data 0.231 (0.349) loss_x loss_x 1.0518 (1.2596) acc_x 65.6250 (68.6250) lr 1.5621e-03 eta 0:00:00
epoch [64/200] batch [5/71] time 0.367 (0.475) data 0.237 (0.345) loss_u loss_u 0.8921 (0.8952) acc_u 15.6250 (15.6250) lr 1.5621e-03 eta 0:00:31
epoch [64/200] batch [10/71] time 0.426 (0.465) data 0.295 (0.334) loss_u loss_u 0.8989 (0.9059) acc_u 15.6250 (13.4375) lr 1.5621e-03 eta 0:00:28
epoch [64/200] batch [15/71] time 0.477 (0.467) data 0.346 (0.336) loss_u loss_u 0.9238 (0.9080) acc_u 12.5000 (13.1250) lr 1.5621e-03 eta 0:00:26
epoch [64/200] batch [20/71] time 0.442 (0.470) data 0.311 (0.340) loss_u loss_u 0.9248 (0.9109) acc_u 12.5000 (12.3438) lr 1.5621e-03 eta 0:00:23
epoch [64/200] batch [25/71] time 0.367 (0.467) data 0.237 (0.336) loss_u loss_u 0.8594 (0.9095) acc_u 12.5000 (11.7500) lr 1.5621e-03 eta 0:00:21
epoch [64/200] batch [30/71] time 0.650 (0.465) data 0.519 (0.334) loss_u loss_u 0.9150 (0.9084) acc_u 9.3750 (11.6667) lr 1.5621e-03 eta 0:00:19
epoch [64/200] batch [35/71] time 0.358 (0.458) data 0.225 (0.328) loss_u loss_u 0.9360 (0.9114) acc_u 6.2500 (11.2500) lr 1.5621e-03 eta 0:00:16
epoch [64/200] batch [40/71] time 0.643 (0.461) data 0.512 (0.330) loss_u loss_u 0.9487 (0.9097) acc_u 6.2500 (11.6406) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [45/71] time 0.425 (0.458) data 0.295 (0.327) loss_u loss_u 0.9341 (0.9105) acc_u 3.1250 (11.4583) lr 1.5621e-03 eta 0:00:11
epoch [64/200] batch [50/71] time 0.394 (0.455) data 0.263 (0.324) loss_u loss_u 0.9097 (0.9117) acc_u 12.5000 (11.3750) lr 1.5621e-03 eta 0:00:09
epoch [64/200] batch [55/71] time 0.311 (0.450) data 0.180 (0.319) loss_u loss_u 0.9351 (0.9130) acc_u 6.2500 (11.1932) lr 1.5621e-03 eta 0:00:07
epoch [64/200] batch [60/71] time 0.456 (0.449) data 0.325 (0.318) loss_u loss_u 0.9517 (0.9123) acc_u 6.2500 (11.1979) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [65/71] time 0.463 (0.451) data 0.333 (0.320) loss_u loss_u 0.9092 (0.9102) acc_u 15.6250 (11.4423) lr 1.5621e-03 eta 0:00:02
epoch [64/200] batch [70/71] time 0.423 (0.451) data 0.293 (0.320) loss_u loss_u 0.8457 (0.9087) acc_u 28.1250 (11.9196) lr 1.5621e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1575
confident_label rate tensor(0.2631, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 825
clean true:816
clean false:9
clean_rate:0.9890909090909091
noisy true:745
noisy false:1566
after delete: len(clean_dataset) 825
after delete: len(noisy_dataset) 2311
epoch [65/200] batch [5/25] time 0.482 (0.457) data 0.353 (0.326) loss_x loss_x 1.0811 (1.2405) acc_x 68.7500 (65.6250) lr 1.5490e-03 eta 0:00:09
epoch [65/200] batch [10/25] time 0.529 (0.481) data 0.399 (0.351) loss_x loss_x 0.9766 (1.2600) acc_x 75.0000 (68.7500) lr 1.5490e-03 eta 0:00:07
epoch [65/200] batch [15/25] time 0.502 (0.480) data 0.371 (0.349) loss_x loss_x 1.0801 (1.2182) acc_x 75.0000 (70.0000) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [20/25] time 0.652 (0.490) data 0.522 (0.360) loss_x loss_x 1.0488 (1.2027) acc_x 78.1250 (70.6250) lr 1.5490e-03 eta 0:00:02
epoch [65/200] batch [25/25] time 0.443 (0.480) data 0.313 (0.349) loss_x loss_x 1.1143 (1.2250) acc_x 78.1250 (70.3750) lr 1.5490e-03 eta 0:00:00
epoch [65/200] batch [5/72] time 0.374 (0.474) data 0.243 (0.343) loss_u loss_u 0.8242 (0.8758) acc_u 28.1250 (18.1250) lr 1.5490e-03 eta 0:00:31
epoch [65/200] batch [10/72] time 0.567 (0.476) data 0.435 (0.345) loss_u loss_u 0.9570 (0.9000) acc_u 3.1250 (14.3750) lr 1.5490e-03 eta 0:00:29
epoch [65/200] batch [15/72] time 0.377 (0.466) data 0.245 (0.335) loss_u loss_u 0.8745 (0.9017) acc_u 15.6250 (13.3333) lr 1.5490e-03 eta 0:00:26
epoch [65/200] batch [20/72] time 0.422 (0.470) data 0.291 (0.339) loss_u loss_u 0.8950 (0.9048) acc_u 15.6250 (12.8125) lr 1.5490e-03 eta 0:00:24
epoch [65/200] batch [25/72] time 0.406 (0.469) data 0.275 (0.338) loss_u loss_u 0.9258 (0.9064) acc_u 9.3750 (12.6250) lr 1.5490e-03 eta 0:00:22
epoch [65/200] batch [30/72] time 0.395 (0.469) data 0.264 (0.338) loss_u loss_u 0.9287 (0.9091) acc_u 9.3750 (12.1875) lr 1.5490e-03 eta 0:00:19
epoch [65/200] batch [35/72] time 0.349 (0.468) data 0.217 (0.337) loss_u loss_u 0.8960 (0.9074) acc_u 12.5000 (12.4107) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [40/72] time 0.452 (0.464) data 0.320 (0.333) loss_u loss_u 0.9380 (0.9062) acc_u 9.3750 (12.5000) lr 1.5490e-03 eta 0:00:14
epoch [65/200] batch [45/72] time 0.474 (0.470) data 0.344 (0.339) loss_u loss_u 0.9507 (0.9048) acc_u 6.2500 (12.9167) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [50/72] time 0.410 (0.470) data 0.279 (0.339) loss_u loss_u 0.9282 (0.9063) acc_u 15.6250 (12.7500) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [55/72] time 0.489 (0.468) data 0.359 (0.337) loss_u loss_u 0.9551 (0.9074) acc_u 6.2500 (12.6705) lr 1.5490e-03 eta 0:00:07
epoch [65/200] batch [60/72] time 0.569 (0.466) data 0.438 (0.335) loss_u loss_u 0.8887 (0.9080) acc_u 18.7500 (12.6042) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [65/72] time 0.556 (0.465) data 0.423 (0.334) loss_u loss_u 0.8872 (0.9072) acc_u 12.5000 (12.6442) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [70/72] time 0.605 (0.464) data 0.473 (0.333) loss_u loss_u 0.8799 (0.9067) acc_u 12.5000 (12.5446) lr 1.5490e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1542
confident_label rate tensor(0.2790, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 875
clean true:868
clean false:7
clean_rate:0.992
noisy true:726
noisy false:1535
after delete: len(clean_dataset) 875
after delete: len(noisy_dataset) 2261
epoch [66/200] batch [5/27] time 0.458 (0.474) data 0.328 (0.344) loss_x loss_x 1.4199 (1.2785) acc_x 71.8750 (71.8750) lr 1.5358e-03 eta 0:00:10
epoch [66/200] batch [10/27] time 0.661 (0.534) data 0.530 (0.404) loss_x loss_x 0.9438 (1.1600) acc_x 75.0000 (71.8750) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [15/27] time 0.597 (0.510) data 0.466 (0.380) loss_x loss_x 0.7207 (1.1423) acc_x 84.3750 (71.2500) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [20/27] time 0.471 (0.502) data 0.340 (0.371) loss_x loss_x 1.1455 (1.1673) acc_x 65.6250 (70.6250) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [25/27] time 0.480 (0.490) data 0.349 (0.359) loss_x loss_x 1.0527 (1.2034) acc_x 71.8750 (70.6250) lr 1.5358e-03 eta 0:00:00
epoch [66/200] batch [5/70] time 0.489 (0.486) data 0.358 (0.355) loss_u loss_u 0.9175 (0.9152) acc_u 12.5000 (13.1250) lr 1.5358e-03 eta 0:00:31
epoch [66/200] batch [10/70] time 0.455 (0.475) data 0.323 (0.345) loss_u loss_u 0.9502 (0.9200) acc_u 6.2500 (11.5625) lr 1.5358e-03 eta 0:00:28
epoch [66/200] batch [15/70] time 0.443 (0.472) data 0.311 (0.341) loss_u loss_u 0.9404 (0.9187) acc_u 12.5000 (11.8750) lr 1.5358e-03 eta 0:00:25
epoch [66/200] batch [20/70] time 0.535 (0.474) data 0.404 (0.343) loss_u loss_u 0.8950 (0.9152) acc_u 12.5000 (11.8750) lr 1.5358e-03 eta 0:00:23
epoch [66/200] batch [25/70] time 0.473 (0.471) data 0.342 (0.340) loss_u loss_u 0.9346 (0.9154) acc_u 6.2500 (11.3750) lr 1.5358e-03 eta 0:00:21
epoch [66/200] batch [30/70] time 0.359 (0.467) data 0.228 (0.336) loss_u loss_u 0.9297 (0.9157) acc_u 6.2500 (11.1458) lr 1.5358e-03 eta 0:00:18
epoch [66/200] batch [35/70] time 0.437 (0.467) data 0.305 (0.336) loss_u loss_u 0.8345 (0.9147) acc_u 18.7500 (11.0714) lr 1.5358e-03 eta 0:00:16
epoch [66/200] batch [40/70] time 0.629 (0.469) data 0.497 (0.338) loss_u loss_u 0.9116 (0.9129) acc_u 9.3750 (11.2500) lr 1.5358e-03 eta 0:00:14
epoch [66/200] batch [45/70] time 0.505 (0.467) data 0.374 (0.336) loss_u loss_u 0.9302 (0.9130) acc_u 6.2500 (11.0417) lr 1.5358e-03 eta 0:00:11
epoch [66/200] batch [50/70] time 0.431 (0.466) data 0.300 (0.334) loss_u loss_u 0.9082 (0.9133) acc_u 12.5000 (11.0000) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [55/70] time 0.444 (0.465) data 0.312 (0.334) loss_u loss_u 0.9507 (0.9129) acc_u 6.2500 (11.0227) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [60/70] time 0.463 (0.465) data 0.332 (0.334) loss_u loss_u 0.8467 (0.9106) acc_u 25.0000 (11.5625) lr 1.5358e-03 eta 0:00:04
epoch [66/200] batch [65/70] time 0.529 (0.464) data 0.397 (0.333) loss_u loss_u 0.9707 (0.9133) acc_u 6.2500 (11.2981) lr 1.5358e-03 eta 0:00:02
epoch [66/200] batch [70/70] time 0.372 (0.468) data 0.241 (0.337) loss_u loss_u 0.9321 (0.9138) acc_u 6.2500 (11.1607) lr 1.5358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1532
confident_label rate tensor(0.2860, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 897
clean true:884
clean false:13
clean_rate:0.9855072463768116
noisy true:720
noisy false:1519
after delete: len(clean_dataset) 897
after delete: len(noisy_dataset) 2239
epoch [67/200] batch [5/28] time 0.536 (0.559) data 0.405 (0.429) loss_x loss_x 0.6997 (1.1250) acc_x 75.0000 (70.0000) lr 1.5225e-03 eta 0:00:12
epoch [67/200] batch [10/28] time 0.392 (0.527) data 0.259 (0.397) loss_x loss_x 1.4805 (1.2000) acc_x 50.0000 (69.3750) lr 1.5225e-03 eta 0:00:09
epoch [67/200] batch [15/28] time 0.439 (0.498) data 0.308 (0.367) loss_x loss_x 1.6260 (1.3238) acc_x 62.5000 (66.4583) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [20/28] time 0.426 (0.497) data 0.296 (0.366) loss_x loss_x 1.1943 (1.3363) acc_x 68.7500 (66.2500) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [25/28] time 0.373 (0.486) data 0.242 (0.356) loss_x loss_x 1.4463 (1.3286) acc_x 78.1250 (67.1250) lr 1.5225e-03 eta 0:00:01
epoch [67/200] batch [5/69] time 0.456 (0.497) data 0.325 (0.366) loss_u loss_u 0.9536 (0.9163) acc_u 3.1250 (10.6250) lr 1.5225e-03 eta 0:00:31
epoch [67/200] batch [10/69] time 0.475 (0.492) data 0.344 (0.361) loss_u loss_u 0.9663 (0.9146) acc_u 3.1250 (12.1875) lr 1.5225e-03 eta 0:00:29
epoch [67/200] batch [15/69] time 0.473 (0.498) data 0.342 (0.367) loss_u loss_u 0.9614 (0.9103) acc_u 3.1250 (12.2917) lr 1.5225e-03 eta 0:00:26
epoch [67/200] batch [20/69] time 0.360 (0.489) data 0.229 (0.358) loss_u loss_u 0.8828 (0.9121) acc_u 12.5000 (11.5625) lr 1.5225e-03 eta 0:00:23
epoch [67/200] batch [25/69] time 0.600 (0.484) data 0.469 (0.353) loss_u loss_u 0.9570 (0.9150) acc_u 3.1250 (11.1250) lr 1.5225e-03 eta 0:00:21
epoch [67/200] batch [30/69] time 0.295 (0.480) data 0.163 (0.349) loss_u loss_u 0.9829 (0.9142) acc_u 3.1250 (10.9375) lr 1.5225e-03 eta 0:00:18
epoch [67/200] batch [35/69] time 0.449 (0.476) data 0.318 (0.345) loss_u loss_u 0.8945 (0.9139) acc_u 12.5000 (10.9821) lr 1.5225e-03 eta 0:00:16
epoch [67/200] batch [40/69] time 0.459 (0.475) data 0.326 (0.344) loss_u loss_u 0.9424 (0.9144) acc_u 6.2500 (10.8594) lr 1.5225e-03 eta 0:00:13
epoch [67/200] batch [45/69] time 0.553 (0.475) data 0.421 (0.344) loss_u loss_u 0.8164 (0.9108) acc_u 25.0000 (11.2500) lr 1.5225e-03 eta 0:00:11
epoch [67/200] batch [50/69] time 0.372 (0.475) data 0.240 (0.344) loss_u loss_u 0.9902 (0.9112) acc_u 0.0000 (11.1250) lr 1.5225e-03 eta 0:00:09
epoch [67/200] batch [55/69] time 0.623 (0.480) data 0.490 (0.349) loss_u loss_u 0.8604 (0.9076) acc_u 15.6250 (11.8182) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [60/69] time 0.389 (0.481) data 0.258 (0.349) loss_u loss_u 0.9009 (0.9083) acc_u 12.5000 (11.7708) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [65/69] time 0.364 (0.476) data 0.233 (0.344) loss_u loss_u 0.9590 (0.9088) acc_u 9.3750 (11.8269) lr 1.5225e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1551
confident_label rate tensor(0.2854, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 895
clean true:885
clean false:10
clean_rate:0.9888268156424581
noisy true:700
noisy false:1541
after delete: len(clean_dataset) 895
after delete: len(noisy_dataset) 2241
epoch [68/200] batch [5/27] time 0.457 (0.513) data 0.326 (0.382) loss_x loss_x 0.8188 (1.1366) acc_x 81.2500 (72.5000) lr 1.5090e-03 eta 0:00:11
epoch [68/200] batch [10/27] time 0.407 (0.505) data 0.277 (0.375) loss_x loss_x 1.5469 (1.3682) acc_x 75.0000 (66.2500) lr 1.5090e-03 eta 0:00:08
epoch [68/200] batch [15/27] time 0.443 (0.488) data 0.312 (0.358) loss_x loss_x 1.5586 (1.3653) acc_x 59.3750 (66.2500) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [20/27] time 0.506 (0.492) data 0.375 (0.361) loss_x loss_x 1.2139 (1.2827) acc_x 68.7500 (68.2812) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [25/27] time 0.463 (0.479) data 0.332 (0.348) loss_x loss_x 1.4512 (1.2790) acc_x 59.3750 (68.5000) lr 1.5090e-03 eta 0:00:00
epoch [68/200] batch [5/70] time 0.465 (0.473) data 0.335 (0.343) loss_u loss_u 0.8491 (0.8896) acc_u 21.8750 (13.7500) lr 1.5090e-03 eta 0:00:30
epoch [68/200] batch [10/70] time 0.438 (0.472) data 0.307 (0.341) loss_u loss_u 0.8496 (0.8906) acc_u 18.7500 (14.0625) lr 1.5090e-03 eta 0:00:28
epoch [68/200] batch [15/70] time 0.456 (0.471) data 0.325 (0.341) loss_u loss_u 0.9224 (0.8985) acc_u 6.2500 (13.1250) lr 1.5090e-03 eta 0:00:25
epoch [68/200] batch [20/70] time 0.441 (0.464) data 0.311 (0.334) loss_u loss_u 0.9111 (0.9030) acc_u 12.5000 (12.8125) lr 1.5090e-03 eta 0:00:23
epoch [68/200] batch [25/70] time 0.710 (0.471) data 0.579 (0.340) loss_u loss_u 0.9507 (0.9062) acc_u 6.2500 (12.1250) lr 1.5090e-03 eta 0:00:21
epoch [68/200] batch [30/70] time 0.476 (0.469) data 0.345 (0.338) loss_u loss_u 0.9136 (0.9076) acc_u 12.5000 (11.9792) lr 1.5090e-03 eta 0:00:18
epoch [68/200] batch [35/70] time 0.439 (0.464) data 0.309 (0.333) loss_u loss_u 0.9409 (0.9080) acc_u 9.3750 (11.8750) lr 1.5090e-03 eta 0:00:16
epoch [68/200] batch [40/70] time 0.669 (0.464) data 0.538 (0.333) loss_u loss_u 0.9580 (0.9123) acc_u 6.2500 (11.1719) lr 1.5090e-03 eta 0:00:13
epoch [68/200] batch [45/70] time 0.520 (0.462) data 0.388 (0.331) loss_u loss_u 0.9287 (0.9110) acc_u 9.3750 (11.2500) lr 1.5090e-03 eta 0:00:11
epoch [68/200] batch [50/70] time 0.468 (0.459) data 0.335 (0.328) loss_u loss_u 0.9180 (0.9114) acc_u 12.5000 (11.0625) lr 1.5090e-03 eta 0:00:09
epoch [68/200] batch [55/70] time 0.432 (0.455) data 0.301 (0.324) loss_u loss_u 0.9106 (0.9118) acc_u 9.3750 (11.1932) lr 1.5090e-03 eta 0:00:06
epoch [68/200] batch [60/70] time 0.401 (0.453) data 0.270 (0.322) loss_u loss_u 0.9282 (0.9104) acc_u 6.2500 (11.4583) lr 1.5090e-03 eta 0:00:04
epoch [68/200] batch [65/70] time 0.433 (0.454) data 0.303 (0.323) loss_u loss_u 0.9556 (0.9112) acc_u 6.2500 (11.2981) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [70/70] time 0.702 (0.456) data 0.570 (0.325) loss_u loss_u 0.8452 (0.9102) acc_u 18.7500 (11.4286) lr 1.5090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1570
confident_label rate tensor(0.2720, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 853
clean true:847
clean false:6
clean_rate:0.9929660023446659
noisy true:719
noisy false:1564
after delete: len(clean_dataset) 853
after delete: len(noisy_dataset) 2283
epoch [69/200] batch [5/26] time 0.527 (0.496) data 0.391 (0.364) loss_x loss_x 0.9985 (1.2193) acc_x 78.1250 (69.3750) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [10/26] time 0.474 (0.497) data 0.344 (0.366) loss_x loss_x 1.0674 (1.1451) acc_x 75.0000 (72.8125) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [15/26] time 0.330 (0.464) data 0.200 (0.333) loss_x loss_x 0.9717 (1.2081) acc_x 75.0000 (69.7917) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [20/26] time 0.410 (0.455) data 0.280 (0.324) loss_x loss_x 1.2012 (1.2024) acc_x 68.7500 (69.6875) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [25/26] time 0.487 (0.453) data 0.357 (0.322) loss_x loss_x 0.8677 (1.1710) acc_x 81.2500 (70.2500) lr 1.4955e-03 eta 0:00:00
epoch [69/200] batch [5/71] time 0.402 (0.459) data 0.269 (0.329) loss_u loss_u 0.9404 (0.9278) acc_u 6.2500 (8.1250) lr 1.4955e-03 eta 0:00:30
epoch [69/200] batch [10/71] time 0.523 (0.465) data 0.391 (0.334) loss_u loss_u 0.8359 (0.9016) acc_u 21.8750 (11.5625) lr 1.4955e-03 eta 0:00:28
epoch [69/200] batch [15/71] time 0.416 (0.459) data 0.285 (0.328) loss_u loss_u 0.9526 (0.9115) acc_u 6.2500 (10.6250) lr 1.4955e-03 eta 0:00:25
epoch [69/200] batch [20/71] time 0.496 (0.456) data 0.366 (0.325) loss_u loss_u 0.9160 (0.9161) acc_u 12.5000 (10.3125) lr 1.4955e-03 eta 0:00:23
epoch [69/200] batch [25/71] time 0.470 (0.453) data 0.340 (0.322) loss_u loss_u 0.8472 (0.9179) acc_u 18.7500 (10.2500) lr 1.4955e-03 eta 0:00:20
epoch [69/200] batch [30/71] time 0.468 (0.455) data 0.337 (0.324) loss_u loss_u 0.9126 (0.9162) acc_u 9.3750 (10.5208) lr 1.4955e-03 eta 0:00:18
epoch [69/200] batch [35/71] time 0.386 (0.456) data 0.255 (0.325) loss_u loss_u 0.8760 (0.9134) acc_u 15.6250 (11.0714) lr 1.4955e-03 eta 0:00:16
epoch [69/200] batch [40/71] time 0.507 (0.459) data 0.374 (0.328) loss_u loss_u 0.9019 (0.9110) acc_u 9.3750 (11.3281) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [45/71] time 0.341 (0.453) data 0.210 (0.322) loss_u loss_u 0.9463 (0.9127) acc_u 3.1250 (11.1806) lr 1.4955e-03 eta 0:00:11
epoch [69/200] batch [50/71] time 0.439 (0.454) data 0.308 (0.323) loss_u loss_u 0.9062 (0.9129) acc_u 9.3750 (11.0625) lr 1.4955e-03 eta 0:00:09
epoch [69/200] batch [55/71] time 0.578 (0.460) data 0.447 (0.328) loss_u loss_u 0.9390 (0.9101) acc_u 9.3750 (11.5909) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [60/71] time 0.406 (0.460) data 0.275 (0.329) loss_u loss_u 0.9175 (0.9094) acc_u 12.5000 (11.5104) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [65/71] time 0.559 (0.463) data 0.427 (0.332) loss_u loss_u 0.8594 (0.9071) acc_u 18.7500 (11.8750) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [70/71] time 0.471 (0.461) data 0.339 (0.330) loss_u loss_u 0.8984 (0.9064) acc_u 12.5000 (12.0536) lr 1.4955e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1579
confident_label rate tensor(0.2726, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 855
clean true:846
clean false:9
clean_rate:0.9894736842105263
noisy true:711
noisy false:1570
after delete: len(clean_dataset) 855
after delete: len(noisy_dataset) 2281
epoch [70/200] batch [5/26] time 0.327 (0.473) data 0.197 (0.343) loss_x loss_x 0.9272 (1.1547) acc_x 68.7500 (68.7500) lr 1.4818e-03 eta 0:00:09
epoch [70/200] batch [10/26] time 0.502 (0.480) data 0.372 (0.350) loss_x loss_x 1.1357 (1.2305) acc_x 71.8750 (66.8750) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [15/26] time 0.541 (0.473) data 0.411 (0.343) loss_x loss_x 1.0469 (1.1851) acc_x 71.8750 (69.1667) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [20/26] time 0.463 (0.470) data 0.333 (0.339) loss_x loss_x 1.3076 (1.2432) acc_x 65.6250 (68.1250) lr 1.4818e-03 eta 0:00:02
epoch [70/200] batch [25/26] time 0.432 (0.470) data 0.302 (0.340) loss_x loss_x 1.2773 (1.2155) acc_x 75.0000 (69.1250) lr 1.4818e-03 eta 0:00:00
epoch [70/200] batch [5/71] time 0.317 (0.476) data 0.185 (0.345) loss_u loss_u 0.8560 (0.9022) acc_u 18.7500 (13.7500) lr 1.4818e-03 eta 0:00:31
epoch [70/200] batch [10/71] time 0.399 (0.477) data 0.267 (0.346) loss_u loss_u 0.8896 (0.9080) acc_u 12.5000 (12.1875) lr 1.4818e-03 eta 0:00:29
epoch [70/200] batch [15/71] time 0.391 (0.471) data 0.260 (0.340) loss_u loss_u 0.8271 (0.9024) acc_u 25.0000 (13.3333) lr 1.4818e-03 eta 0:00:26
epoch [70/200] batch [20/71] time 0.354 (0.465) data 0.222 (0.334) loss_u loss_u 0.9233 (0.9075) acc_u 9.3750 (12.8125) lr 1.4818e-03 eta 0:00:23
epoch [70/200] batch [25/71] time 0.723 (0.468) data 0.593 (0.337) loss_u loss_u 0.7900 (0.9080) acc_u 28.1250 (12.5000) lr 1.4818e-03 eta 0:00:21
epoch [70/200] batch [30/71] time 0.487 (0.466) data 0.355 (0.336) loss_u loss_u 0.9375 (0.9105) acc_u 12.5000 (12.2917) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [35/71] time 0.503 (0.468) data 0.372 (0.337) loss_u loss_u 0.9204 (0.9123) acc_u 9.3750 (11.9643) lr 1.4818e-03 eta 0:00:16
epoch [70/200] batch [40/71] time 0.378 (0.469) data 0.246 (0.338) loss_u loss_u 0.8677 (0.9109) acc_u 18.7500 (11.9531) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [45/71] time 0.376 (0.472) data 0.244 (0.341) loss_u loss_u 0.8662 (0.9070) acc_u 18.7500 (12.5694) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [50/71] time 0.528 (0.470) data 0.397 (0.340) loss_u loss_u 0.9531 (0.9077) acc_u 6.2500 (12.4375) lr 1.4818e-03 eta 0:00:09
epoch [70/200] batch [55/71] time 0.351 (0.472) data 0.221 (0.341) loss_u loss_u 0.9229 (0.9069) acc_u 9.3750 (12.6136) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [60/71] time 0.387 (0.468) data 0.256 (0.337) loss_u loss_u 0.9062 (0.9076) acc_u 9.3750 (12.3958) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [65/71] time 0.486 (0.467) data 0.355 (0.336) loss_u loss_u 0.8691 (0.9068) acc_u 12.5000 (12.4038) lr 1.4818e-03 eta 0:00:02
epoch [70/200] batch [70/71] time 0.500 (0.469) data 0.369 (0.338) loss_u loss_u 0.8628 (0.9085) acc_u 21.8750 (12.1875) lr 1.4818e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1567
confident_label rate tensor(0.2806, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 880
clean true:872
clean false:8
clean_rate:0.990909090909091
noisy true:697
noisy false:1559
after delete: len(clean_dataset) 880
after delete: len(noisy_dataset) 2256
epoch [71/200] batch [5/27] time 0.386 (0.426) data 0.256 (0.295) loss_x loss_x 1.0225 (1.2342) acc_x 78.1250 (69.3750) lr 1.4679e-03 eta 0:00:09
epoch [71/200] batch [10/27] time 0.509 (0.464) data 0.379 (0.333) loss_x loss_x 1.3223 (1.2595) acc_x 65.6250 (68.7500) lr 1.4679e-03 eta 0:00:07
epoch [71/200] batch [15/27] time 0.554 (0.472) data 0.423 (0.341) loss_x loss_x 0.8677 (1.2421) acc_x 81.2500 (68.7500) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [20/27] time 0.745 (0.476) data 0.615 (0.346) loss_x loss_x 1.0645 (1.2212) acc_x 78.1250 (70.1562) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [25/27] time 0.407 (0.467) data 0.276 (0.336) loss_x loss_x 1.3193 (1.2358) acc_x 65.6250 (69.5000) lr 1.4679e-03 eta 0:00:00
epoch [71/200] batch [5/70] time 0.356 (0.462) data 0.224 (0.331) loss_u loss_u 0.9380 (0.8977) acc_u 6.2500 (13.7500) lr 1.4679e-03 eta 0:00:30
epoch [71/200] batch [10/70] time 0.557 (0.463) data 0.425 (0.332) loss_u loss_u 0.9365 (0.9051) acc_u 9.3750 (12.5000) lr 1.4679e-03 eta 0:00:27
epoch [71/200] batch [15/70] time 0.469 (0.466) data 0.338 (0.335) loss_u loss_u 0.8804 (0.9049) acc_u 12.5000 (12.5000) lr 1.4679e-03 eta 0:00:25
epoch [71/200] batch [20/70] time 0.438 (0.467) data 0.308 (0.336) loss_u loss_u 0.8911 (0.9102) acc_u 18.7500 (12.0312) lr 1.4679e-03 eta 0:00:23
epoch [71/200] batch [25/70] time 0.478 (0.468) data 0.347 (0.337) loss_u loss_u 0.8472 (0.9053) acc_u 12.5000 (12.2500) lr 1.4679e-03 eta 0:00:21
epoch [71/200] batch [30/70] time 0.446 (0.465) data 0.316 (0.334) loss_u loss_u 0.9688 (0.9083) acc_u 3.1250 (12.0833) lr 1.4679e-03 eta 0:00:18
epoch [71/200] batch [35/70] time 0.387 (0.462) data 0.255 (0.331) loss_u loss_u 0.8975 (0.9079) acc_u 18.7500 (12.3214) lr 1.4679e-03 eta 0:00:16
epoch [71/200] batch [40/70] time 0.427 (0.459) data 0.295 (0.328) loss_u loss_u 0.9312 (0.9065) acc_u 12.5000 (12.4219) lr 1.4679e-03 eta 0:00:13
epoch [71/200] batch [45/70] time 0.496 (0.461) data 0.364 (0.330) loss_u loss_u 0.9521 (0.9088) acc_u 6.2500 (11.9444) lr 1.4679e-03 eta 0:00:11
epoch [71/200] batch [50/70] time 0.366 (0.458) data 0.235 (0.327) loss_u loss_u 0.8755 (0.9067) acc_u 12.5000 (12.1875) lr 1.4679e-03 eta 0:00:09
epoch [71/200] batch [55/70] time 0.489 (0.458) data 0.357 (0.327) loss_u loss_u 0.9448 (0.9055) acc_u 6.2500 (12.2159) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [60/70] time 0.333 (0.457) data 0.200 (0.326) loss_u loss_u 0.9546 (0.9080) acc_u 6.2500 (11.8229) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [65/70] time 0.522 (0.456) data 0.390 (0.325) loss_u loss_u 0.9331 (0.9094) acc_u 9.3750 (11.6346) lr 1.4679e-03 eta 0:00:02
epoch [71/200] batch [70/70] time 0.511 (0.460) data 0.379 (0.329) loss_u loss_u 0.8467 (0.9082) acc_u 18.7500 (11.7857) lr 1.4679e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1529
confident_label rate tensor(0.2822, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 885
clean true:877
clean false:8
clean_rate:0.9909604519774011
noisy true:730
noisy false:1521
after delete: len(clean_dataset) 885
after delete: len(noisy_dataset) 2251
epoch [72/200] batch [5/27] time 0.452 (0.494) data 0.322 (0.364) loss_x loss_x 1.1064 (1.0798) acc_x 75.0000 (70.0000) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [10/27] time 0.528 (0.477) data 0.397 (0.346) loss_x loss_x 0.9482 (1.0823) acc_x 75.0000 (71.5625) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [15/27] time 0.374 (0.475) data 0.242 (0.344) loss_x loss_x 1.1699 (1.0740) acc_x 75.0000 (73.1250) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [20/27] time 0.452 (0.472) data 0.321 (0.342) loss_x loss_x 1.2402 (1.0728) acc_x 65.6250 (73.4375) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [25/27] time 0.459 (0.480) data 0.328 (0.350) loss_x loss_x 1.1445 (1.0782) acc_x 71.8750 (72.8750) lr 1.4540e-03 eta 0:00:00
epoch [72/200] batch [5/70] time 0.499 (0.472) data 0.367 (0.341) loss_u loss_u 0.9536 (0.9133) acc_u 6.2500 (9.3750) lr 1.4540e-03 eta 0:00:30
epoch [72/200] batch [10/70] time 0.391 (0.471) data 0.261 (0.340) loss_u loss_u 0.9321 (0.9192) acc_u 6.2500 (9.3750) lr 1.4540e-03 eta 0:00:28
epoch [72/200] batch [15/70] time 0.701 (0.475) data 0.570 (0.344) loss_u loss_u 0.8960 (0.9237) acc_u 9.3750 (8.5417) lr 1.4540e-03 eta 0:00:26
epoch [72/200] batch [20/70] time 0.549 (0.484) data 0.418 (0.353) loss_u loss_u 0.8823 (0.9152) acc_u 12.5000 (10.3125) lr 1.4540e-03 eta 0:00:24
epoch [72/200] batch [25/70] time 0.454 (0.485) data 0.323 (0.355) loss_u loss_u 0.9165 (0.9181) acc_u 9.3750 (10.1250) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [30/70] time 0.455 (0.482) data 0.324 (0.351) loss_u loss_u 0.8823 (0.9215) acc_u 12.5000 (9.7917) lr 1.4540e-03 eta 0:00:19
epoch [72/200] batch [35/70] time 0.375 (0.473) data 0.242 (0.342) loss_u loss_u 0.9536 (0.9202) acc_u 3.1250 (9.9107) lr 1.4540e-03 eta 0:00:16
epoch [72/200] batch [40/70] time 0.449 (0.473) data 0.316 (0.342) loss_u loss_u 0.8770 (0.9184) acc_u 18.7500 (10.2344) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [45/70] time 0.447 (0.470) data 0.315 (0.339) loss_u loss_u 0.9380 (0.9194) acc_u 6.2500 (9.9306) lr 1.4540e-03 eta 0:00:11
epoch [72/200] batch [50/70] time 0.626 (0.471) data 0.496 (0.340) loss_u loss_u 0.8745 (0.9203) acc_u 12.5000 (10.0000) lr 1.4540e-03 eta 0:00:09
epoch [72/200] batch [55/70] time 0.723 (0.471) data 0.592 (0.340) loss_u loss_u 0.8354 (0.9160) acc_u 25.0000 (10.8523) lr 1.4540e-03 eta 0:00:07
epoch [72/200] batch [60/70] time 0.580 (0.471) data 0.448 (0.339) loss_u loss_u 0.8208 (0.9153) acc_u 25.0000 (10.9375) lr 1.4540e-03 eta 0:00:04
epoch [72/200] batch [65/70] time 0.514 (0.467) data 0.382 (0.336) loss_u loss_u 0.8936 (0.9159) acc_u 15.6250 (10.7692) lr 1.4540e-03 eta 0:00:02
epoch [72/200] batch [70/70] time 0.488 (0.468) data 0.358 (0.337) loss_u loss_u 0.9448 (0.9148) acc_u 9.3750 (10.8482) lr 1.4540e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1547
confident_label rate tensor(0.2755, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 864
clean true:858
clean false:6
clean_rate:0.9930555555555556
noisy true:731
noisy false:1541
after delete: len(clean_dataset) 864
after delete: len(noisy_dataset) 2272
epoch [73/200] batch [5/27] time 0.455 (0.502) data 0.323 (0.371) loss_x loss_x 1.2705 (1.1793) acc_x 65.6250 (69.3750) lr 1.4399e-03 eta 0:00:11
epoch [73/200] batch [10/27] time 0.477 (0.480) data 0.347 (0.349) loss_x loss_x 0.9761 (1.1833) acc_x 81.2500 (71.2500) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [15/27] time 0.419 (0.478) data 0.289 (0.347) loss_x loss_x 0.9185 (1.1522) acc_x 75.0000 (72.2917) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [20/27] time 0.545 (0.482) data 0.414 (0.352) loss_x loss_x 1.4873 (1.1637) acc_x 59.3750 (72.1875) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [25/27] time 0.369 (0.479) data 0.239 (0.349) loss_x loss_x 1.2891 (1.1828) acc_x 71.8750 (71.5000) lr 1.4399e-03 eta 0:00:00
epoch [73/200] batch [5/71] time 0.424 (0.471) data 0.293 (0.340) loss_u loss_u 0.8970 (0.9278) acc_u 12.5000 (10.0000) lr 1.4399e-03 eta 0:00:31
epoch [73/200] batch [10/71] time 0.482 (0.465) data 0.350 (0.334) loss_u loss_u 0.8926 (0.9187) acc_u 18.7500 (11.5625) lr 1.4399e-03 eta 0:00:28
epoch [73/200] batch [15/71] time 0.528 (0.475) data 0.398 (0.344) loss_u loss_u 0.9414 (0.9205) acc_u 6.2500 (10.8333) lr 1.4399e-03 eta 0:00:26
epoch [73/200] batch [20/71] time 0.487 (0.476) data 0.356 (0.346) loss_u loss_u 0.9111 (0.9232) acc_u 12.5000 (10.6250) lr 1.4399e-03 eta 0:00:24
epoch [73/200] batch [25/71] time 0.452 (0.474) data 0.321 (0.344) loss_u loss_u 0.9189 (0.9230) acc_u 15.6250 (10.8750) lr 1.4399e-03 eta 0:00:21
epoch [73/200] batch [30/71] time 0.456 (0.470) data 0.324 (0.339) loss_u loss_u 0.8140 (0.9170) acc_u 21.8750 (11.4583) lr 1.4399e-03 eta 0:00:19
epoch [73/200] batch [35/71] time 0.398 (0.466) data 0.267 (0.335) loss_u loss_u 0.8589 (0.9127) acc_u 15.6250 (11.5179) lr 1.4399e-03 eta 0:00:16
epoch [73/200] batch [40/71] time 0.408 (0.466) data 0.273 (0.336) loss_u loss_u 0.8765 (0.9100) acc_u 18.7500 (11.7969) lr 1.4399e-03 eta 0:00:14
epoch [73/200] batch [45/71] time 0.460 (0.468) data 0.326 (0.337) loss_u loss_u 0.8022 (0.9082) acc_u 28.1250 (12.0139) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [50/71] time 0.474 (0.465) data 0.342 (0.334) loss_u loss_u 0.9043 (0.9100) acc_u 12.5000 (11.8750) lr 1.4399e-03 eta 0:00:09
epoch [73/200] batch [55/71] time 0.468 (0.465) data 0.337 (0.334) loss_u loss_u 0.8599 (0.9077) acc_u 15.6250 (12.1023) lr 1.4399e-03 eta 0:00:07
epoch [73/200] batch [60/71] time 0.519 (0.465) data 0.388 (0.334) loss_u loss_u 0.9272 (0.9087) acc_u 12.5000 (12.1875) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [65/71] time 0.379 (0.464) data 0.248 (0.333) loss_u loss_u 0.9590 (0.9074) acc_u 6.2500 (12.2596) lr 1.4399e-03 eta 0:00:02
epoch [73/200] batch [70/71] time 0.445 (0.470) data 0.313 (0.339) loss_u loss_u 0.9199 (0.9067) acc_u 6.2500 (12.2768) lr 1.4399e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1543
confident_label rate tensor(0.2774, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 870
clean true:866
clean false:4
clean_rate:0.9954022988505747
noisy true:727
noisy false:1539
after delete: len(clean_dataset) 870
after delete: len(noisy_dataset) 2266
epoch [74/200] batch [5/27] time 0.459 (0.469) data 0.328 (0.338) loss_x loss_x 0.6982 (0.9807) acc_x 81.2500 (75.0000) lr 1.4258e-03 eta 0:00:10
epoch [74/200] batch [10/27] time 0.428 (0.473) data 0.297 (0.342) loss_x loss_x 0.9443 (1.0662) acc_x 78.1250 (72.1875) lr 1.4258e-03 eta 0:00:08
epoch [74/200] batch [15/27] time 0.521 (0.463) data 0.390 (0.332) loss_x loss_x 1.8682 (1.2077) acc_x 53.1250 (68.5417) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [20/27] time 0.479 (0.457) data 0.348 (0.327) loss_x loss_x 0.7690 (1.2137) acc_x 81.2500 (69.2188) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [25/27] time 0.440 (0.461) data 0.310 (0.330) loss_x loss_x 1.0879 (1.2053) acc_x 71.8750 (69.2500) lr 1.4258e-03 eta 0:00:00
epoch [74/200] batch [5/70] time 0.371 (0.457) data 0.241 (0.326) loss_u loss_u 0.9697 (0.9094) acc_u 0.0000 (9.3750) lr 1.4258e-03 eta 0:00:29
epoch [74/200] batch [10/70] time 0.425 (0.462) data 0.294 (0.331) loss_u loss_u 0.9087 (0.9109) acc_u 12.5000 (10.0000) lr 1.4258e-03 eta 0:00:27
epoch [74/200] batch [15/70] time 0.621 (0.465) data 0.490 (0.334) loss_u loss_u 0.9019 (0.9098) acc_u 9.3750 (10.4167) lr 1.4258e-03 eta 0:00:25
epoch [74/200] batch [20/70] time 0.831 (0.470) data 0.700 (0.339) loss_u loss_u 0.8169 (0.8992) acc_u 28.1250 (12.5000) lr 1.4258e-03 eta 0:00:23
epoch [74/200] batch [25/70] time 0.478 (0.467) data 0.346 (0.336) loss_u loss_u 0.8970 (0.9009) acc_u 18.7500 (12.2500) lr 1.4258e-03 eta 0:00:21
epoch [74/200] batch [30/70] time 0.415 (0.464) data 0.283 (0.333) loss_u loss_u 0.9458 (0.9009) acc_u 9.3750 (12.5000) lr 1.4258e-03 eta 0:00:18
epoch [74/200] batch [35/70] time 0.418 (0.461) data 0.287 (0.330) loss_u loss_u 0.9233 (0.8997) acc_u 12.5000 (12.9464) lr 1.4258e-03 eta 0:00:16
epoch [74/200] batch [40/70] time 0.447 (0.461) data 0.316 (0.330) loss_u loss_u 0.8369 (0.8981) acc_u 25.0000 (13.3594) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [45/70] time 0.408 (0.465) data 0.277 (0.334) loss_u loss_u 0.9229 (0.9016) acc_u 12.5000 (12.9167) lr 1.4258e-03 eta 0:00:11
epoch [74/200] batch [50/70] time 0.397 (0.463) data 0.266 (0.332) loss_u loss_u 0.8535 (0.9003) acc_u 21.8750 (13.0625) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [55/70] time 0.548 (0.465) data 0.416 (0.334) loss_u loss_u 0.9009 (0.9014) acc_u 15.6250 (13.0682) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [60/70] time 0.449 (0.465) data 0.317 (0.334) loss_u loss_u 0.9517 (0.9005) acc_u 6.2500 (13.1771) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [65/70] time 0.467 (0.465) data 0.336 (0.334) loss_u loss_u 0.9326 (0.9031) acc_u 6.2500 (12.8365) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [70/70] time 0.438 (0.465) data 0.307 (0.334) loss_u loss_u 0.8955 (0.9039) acc_u 15.6250 (12.6339) lr 1.4258e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1573
confident_label rate tensor(0.2790, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 875
clean true:867
clean false:8
clean_rate:0.9908571428571429
noisy true:696
noisy false:1565
after delete: len(clean_dataset) 875
after delete: len(noisy_dataset) 2261
epoch [75/200] batch [5/27] time 0.400 (0.488) data 0.270 (0.358) loss_x loss_x 0.9326 (1.1324) acc_x 71.8750 (70.0000) lr 1.4115e-03 eta 0:00:10
epoch [75/200] batch [10/27] time 0.482 (0.453) data 0.352 (0.323) loss_x loss_x 1.3320 (1.1748) acc_x 62.5000 (70.0000) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [15/27] time 0.480 (0.454) data 0.350 (0.324) loss_x loss_x 1.2432 (1.2639) acc_x 75.0000 (68.5417) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [20/27] time 0.422 (0.456) data 0.292 (0.326) loss_x loss_x 1.4639 (1.2299) acc_x 65.6250 (69.5312) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [25/27] time 0.449 (0.454) data 0.319 (0.324) loss_x loss_x 0.7744 (1.2074) acc_x 81.2500 (69.8750) lr 1.4115e-03 eta 0:00:00
epoch [75/200] batch [5/70] time 0.411 (0.458) data 0.281 (0.328) loss_u loss_u 0.8833 (0.8968) acc_u 9.3750 (12.5000) lr 1.4115e-03 eta 0:00:29
epoch [75/200] batch [10/70] time 0.420 (0.454) data 0.289 (0.324) loss_u loss_u 0.8809 (0.9085) acc_u 15.6250 (11.8750) lr 1.4115e-03 eta 0:00:27
epoch [75/200] batch [15/70] time 0.463 (0.449) data 0.333 (0.319) loss_u loss_u 0.9229 (0.9134) acc_u 9.3750 (10.8333) lr 1.4115e-03 eta 0:00:24
epoch [75/200] batch [20/70] time 0.392 (0.447) data 0.261 (0.316) loss_u loss_u 0.9497 (0.9161) acc_u 6.2500 (10.6250) lr 1.4115e-03 eta 0:00:22
epoch [75/200] batch [25/70] time 0.382 (0.444) data 0.252 (0.313) loss_u loss_u 0.8862 (0.9081) acc_u 12.5000 (11.7500) lr 1.4115e-03 eta 0:00:19
epoch [75/200] batch [30/70] time 0.368 (0.442) data 0.237 (0.312) loss_u loss_u 0.8462 (0.9017) acc_u 18.7500 (12.3958) lr 1.4115e-03 eta 0:00:17
epoch [75/200] batch [35/70] time 0.408 (0.440) data 0.277 (0.310) loss_u loss_u 0.9043 (0.8978) acc_u 9.3750 (13.0357) lr 1.4115e-03 eta 0:00:15
epoch [75/200] batch [40/70] time 0.412 (0.444) data 0.280 (0.314) loss_u loss_u 0.9263 (0.8983) acc_u 9.3750 (13.0469) lr 1.4115e-03 eta 0:00:13
epoch [75/200] batch [45/70] time 0.413 (0.448) data 0.282 (0.317) loss_u loss_u 0.8765 (0.8980) acc_u 18.7500 (13.0556) lr 1.4115e-03 eta 0:00:11
epoch [75/200] batch [50/70] time 0.399 (0.456) data 0.267 (0.325) loss_u loss_u 0.9385 (0.9021) acc_u 9.3750 (12.6250) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [55/70] time 0.372 (0.455) data 0.241 (0.325) loss_u loss_u 0.9888 (0.9065) acc_u 0.0000 (11.9318) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [60/70] time 0.487 (0.461) data 0.355 (0.330) loss_u loss_u 0.9517 (0.9076) acc_u 9.3750 (11.8229) lr 1.4115e-03 eta 0:00:04
epoch [75/200] batch [65/70] time 0.396 (0.460) data 0.264 (0.329) loss_u loss_u 0.8921 (0.9080) acc_u 12.5000 (11.7308) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [70/70] time 0.360 (0.458) data 0.228 (0.327) loss_u loss_u 0.9644 (0.9084) acc_u 3.1250 (11.6071) lr 1.4115e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1549
confident_label rate tensor(0.2841, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 891
clean true:886
clean false:5
clean_rate:0.9943883277216611
noisy true:701
noisy false:1544
after delete: len(clean_dataset) 891
after delete: len(noisy_dataset) 2245
epoch [76/200] batch [5/27] time 0.527 (0.517) data 0.396 (0.386) loss_x loss_x 1.1826 (1.1508) acc_x 71.8750 (72.5000) lr 1.3971e-03 eta 0:00:11
epoch [76/200] batch [10/27] time 0.460 (0.487) data 0.329 (0.357) loss_x loss_x 0.8315 (1.0963) acc_x 71.8750 (72.8125) lr 1.3971e-03 eta 0:00:08
epoch [76/200] batch [15/27] time 0.401 (0.476) data 0.270 (0.345) loss_x loss_x 1.4648 (1.1881) acc_x 68.7500 (72.2917) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [20/27] time 0.444 (0.489) data 0.313 (0.359) loss_x loss_x 0.4653 (1.1374) acc_x 87.5000 (72.8125) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [25/27] time 0.409 (0.487) data 0.279 (0.357) loss_x loss_x 1.1182 (1.2091) acc_x 75.0000 (71.5000) lr 1.3971e-03 eta 0:00:00
epoch [76/200] batch [5/70] time 0.448 (0.485) data 0.316 (0.355) loss_u loss_u 0.8843 (0.8759) acc_u 15.6250 (18.1250) lr 1.3971e-03 eta 0:00:31
epoch [76/200] batch [10/70] time 0.354 (0.473) data 0.223 (0.342) loss_u loss_u 0.8916 (0.9002) acc_u 15.6250 (14.0625) lr 1.3971e-03 eta 0:00:28
epoch [76/200] batch [15/70] time 0.470 (0.472) data 0.338 (0.341) loss_u loss_u 0.9604 (0.9068) acc_u 3.1250 (12.0833) lr 1.3971e-03 eta 0:00:25
epoch [76/200] batch [20/70] time 0.482 (0.476) data 0.349 (0.345) loss_u loss_u 0.9272 (0.9041) acc_u 9.3750 (12.3438) lr 1.3971e-03 eta 0:00:23
epoch [76/200] batch [25/70] time 0.519 (0.476) data 0.386 (0.344) loss_u loss_u 0.9487 (0.9046) acc_u 6.2500 (12.2500) lr 1.3971e-03 eta 0:00:21
epoch [76/200] batch [30/70] time 0.455 (0.474) data 0.323 (0.343) loss_u loss_u 0.9688 (0.9092) acc_u 3.1250 (11.8750) lr 1.3971e-03 eta 0:00:18
epoch [76/200] batch [35/70] time 0.449 (0.472) data 0.316 (0.340) loss_u loss_u 0.9038 (0.9117) acc_u 9.3750 (11.5179) lr 1.3971e-03 eta 0:00:16
epoch [76/200] batch [40/70] time 0.476 (0.467) data 0.344 (0.335) loss_u loss_u 0.8735 (0.9120) acc_u 15.6250 (11.4844) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [45/70] time 0.343 (0.467) data 0.210 (0.335) loss_u loss_u 0.9648 (0.9131) acc_u 6.2500 (11.3194) lr 1.3971e-03 eta 0:00:11
epoch [76/200] batch [50/70] time 0.521 (0.469) data 0.389 (0.337) loss_u loss_u 0.8618 (0.9126) acc_u 15.6250 (11.3750) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [55/70] time 0.563 (0.468) data 0.432 (0.336) loss_u loss_u 0.9409 (0.9154) acc_u 9.3750 (10.9659) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [60/70] time 0.358 (0.465) data 0.226 (0.333) loss_u loss_u 0.9497 (0.9166) acc_u 6.2500 (10.6771) lr 1.3971e-03 eta 0:00:04
epoch [76/200] batch [65/70] time 0.456 (0.463) data 0.323 (0.331) loss_u loss_u 0.8926 (0.9168) acc_u 15.6250 (10.5769) lr 1.3971e-03 eta 0:00:02
epoch [76/200] batch [70/70] time 0.396 (0.465) data 0.264 (0.333) loss_u loss_u 0.9072 (0.9133) acc_u 12.5000 (10.8929) lr 1.3971e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1537
confident_label rate tensor(0.2870, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 900
clean true:890
clean false:10
clean_rate:0.9888888888888889
noisy true:709
noisy false:1527
after delete: len(clean_dataset) 900
after delete: len(noisy_dataset) 2236
epoch [77/200] batch [5/28] time 0.538 (0.507) data 0.408 (0.377) loss_x loss_x 1.0283 (1.2020) acc_x 71.8750 (71.8750) lr 1.3827e-03 eta 0:00:11
epoch [77/200] batch [10/28] time 0.461 (0.489) data 0.330 (0.358) loss_x loss_x 1.0820 (1.2807) acc_x 75.0000 (70.0000) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [15/28] time 0.431 (0.482) data 0.300 (0.351) loss_x loss_x 1.6016 (1.3329) acc_x 59.3750 (68.9583) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [20/28] time 0.475 (0.488) data 0.344 (0.357) loss_x loss_x 1.0479 (1.3185) acc_x 62.5000 (68.5938) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [25/28] time 0.441 (0.476) data 0.310 (0.346) loss_x loss_x 1.3359 (1.3101) acc_x 65.6250 (68.5000) lr 1.3827e-03 eta 0:00:01
epoch [77/200] batch [5/69] time 0.455 (0.466) data 0.324 (0.335) loss_u loss_u 0.8579 (0.8960) acc_u 12.5000 (11.2500) lr 1.3827e-03 eta 0:00:29
epoch [77/200] batch [10/69] time 0.507 (0.466) data 0.376 (0.335) loss_u loss_u 0.9160 (0.8920) acc_u 9.3750 (13.1250) lr 1.3827e-03 eta 0:00:27
epoch [77/200] batch [15/69] time 0.434 (0.468) data 0.302 (0.337) loss_u loss_u 0.9175 (0.8916) acc_u 6.2500 (12.9167) lr 1.3827e-03 eta 0:00:25
epoch [77/200] batch [20/69] time 0.397 (0.473) data 0.264 (0.342) loss_u loss_u 0.9385 (0.8994) acc_u 9.3750 (12.1875) lr 1.3827e-03 eta 0:00:23
epoch [77/200] batch [25/69] time 0.381 (0.470) data 0.245 (0.339) loss_u loss_u 0.9121 (0.9000) acc_u 12.5000 (12.7500) lr 1.3827e-03 eta 0:00:20
epoch [77/200] batch [30/69] time 0.432 (0.466) data 0.300 (0.335) loss_u loss_u 0.8481 (0.9053) acc_u 18.7500 (12.0833) lr 1.3827e-03 eta 0:00:18
epoch [77/200] batch [35/69] time 0.454 (0.463) data 0.322 (0.332) loss_u loss_u 0.9072 (0.9050) acc_u 9.3750 (12.1429) lr 1.3827e-03 eta 0:00:15
epoch [77/200] batch [40/69] time 0.383 (0.459) data 0.251 (0.327) loss_u loss_u 0.8438 (0.9054) acc_u 21.8750 (12.0312) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [45/69] time 0.371 (0.457) data 0.240 (0.325) loss_u loss_u 0.8857 (0.9067) acc_u 12.5000 (11.7361) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [50/69] time 0.380 (0.457) data 0.248 (0.325) loss_u loss_u 0.7974 (0.9053) acc_u 25.0000 (11.9375) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [55/69] time 0.467 (0.457) data 0.336 (0.325) loss_u loss_u 0.9331 (0.9058) acc_u 6.2500 (11.9886) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [60/69] time 0.428 (0.458) data 0.297 (0.327) loss_u loss_u 0.9131 (0.9071) acc_u 9.3750 (11.8229) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [65/69] time 0.455 (0.457) data 0.324 (0.325) loss_u loss_u 0.9062 (0.9104) acc_u 9.3750 (11.2981) lr 1.3827e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1532
confident_label rate tensor(0.2848, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 893
clean true:887
clean false:6
clean_rate:0.9932810750279956
noisy true:717
noisy false:1526
after delete: len(clean_dataset) 893
after delete: len(noisy_dataset) 2243
epoch [78/200] batch [5/27] time 0.428 (0.456) data 0.297 (0.325) loss_x loss_x 1.0186 (1.1457) acc_x 75.0000 (70.0000) lr 1.3681e-03 eta 0:00:10
epoch [78/200] batch [10/27] time 0.521 (0.454) data 0.391 (0.323) loss_x loss_x 1.2012 (1.2382) acc_x 59.3750 (67.8125) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [15/27] time 0.459 (0.463) data 0.327 (0.332) loss_x loss_x 0.9399 (1.2489) acc_x 75.0000 (66.8750) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [20/27] time 0.546 (0.475) data 0.415 (0.344) loss_x loss_x 1.2969 (1.2788) acc_x 78.1250 (67.1875) lr 1.3681e-03 eta 0:00:03
epoch [78/200] batch [25/27] time 0.471 (0.476) data 0.340 (0.345) loss_x loss_x 0.9849 (1.2495) acc_x 78.1250 (68.1250) lr 1.3681e-03 eta 0:00:00
epoch [78/200] batch [5/70] time 0.459 (0.472) data 0.326 (0.341) loss_u loss_u 0.8916 (0.8953) acc_u 15.6250 (15.0000) lr 1.3681e-03 eta 0:00:30
epoch [78/200] batch [10/70] time 0.495 (0.472) data 0.364 (0.341) loss_u loss_u 0.8989 (0.8983) acc_u 12.5000 (14.0625) lr 1.3681e-03 eta 0:00:28
epoch [78/200] batch [15/70] time 0.568 (0.478) data 0.435 (0.347) loss_u loss_u 0.8979 (0.9066) acc_u 9.3750 (12.5000) lr 1.3681e-03 eta 0:00:26
epoch [78/200] batch [20/70] time 0.391 (0.479) data 0.261 (0.348) loss_u loss_u 0.8433 (0.9012) acc_u 21.8750 (13.1250) lr 1.3681e-03 eta 0:00:23
epoch [78/200] batch [25/70] time 0.418 (0.479) data 0.286 (0.348) loss_u loss_u 0.9395 (0.8977) acc_u 6.2500 (13.5000) lr 1.3681e-03 eta 0:00:21
epoch [78/200] batch [30/70] time 0.388 (0.473) data 0.256 (0.341) loss_u loss_u 0.9541 (0.8993) acc_u 6.2500 (13.1250) lr 1.3681e-03 eta 0:00:18
epoch [78/200] batch [35/70] time 0.400 (0.473) data 0.269 (0.342) loss_u loss_u 0.9077 (0.9031) acc_u 12.5000 (12.5000) lr 1.3681e-03 eta 0:00:16
epoch [78/200] batch [40/70] time 0.340 (0.466) data 0.209 (0.335) loss_u loss_u 0.9248 (0.9057) acc_u 9.3750 (12.1875) lr 1.3681e-03 eta 0:00:13
epoch [78/200] batch [45/70] time 0.521 (0.470) data 0.390 (0.339) loss_u loss_u 0.9453 (0.9069) acc_u 6.2500 (12.0139) lr 1.3681e-03 eta 0:00:11
epoch [78/200] batch [50/70] time 0.647 (0.471) data 0.516 (0.339) loss_u loss_u 0.9292 (0.9107) acc_u 6.2500 (11.5000) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [55/70] time 0.442 (0.473) data 0.310 (0.341) loss_u loss_u 0.8965 (0.9102) acc_u 9.3750 (11.4205) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [60/70] time 0.563 (0.473) data 0.433 (0.342) loss_u loss_u 0.9321 (0.9088) acc_u 6.2500 (11.7188) lr 1.3681e-03 eta 0:00:04
epoch [78/200] batch [65/70] time 0.393 (0.472) data 0.262 (0.341) loss_u loss_u 0.9297 (0.9106) acc_u 6.2500 (11.3942) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [70/70] time 0.327 (0.473) data 0.196 (0.341) loss_u loss_u 0.9087 (0.9103) acc_u 15.6250 (11.5625) lr 1.3681e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1459
confident_label rate tensor(0.3001, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 941
clean true:933
clean false:8
clean_rate:0.9914984059511158
noisy true:744
noisy false:1451
after delete: len(clean_dataset) 941
after delete: len(noisy_dataset) 2195
epoch [79/200] batch [5/29] time 0.593 (0.518) data 0.463 (0.387) loss_x loss_x 1.4902 (1.3404) acc_x 68.7500 (70.6250) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [10/29] time 0.341 (0.475) data 0.209 (0.344) loss_x loss_x 1.7314 (1.5115) acc_x 65.6250 (66.5625) lr 1.3535e-03 eta 0:00:09
epoch [79/200] batch [15/29] time 0.425 (0.495) data 0.294 (0.365) loss_x loss_x 1.1064 (1.4264) acc_x 78.1250 (67.7083) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [20/29] time 0.528 (0.482) data 0.397 (0.351) loss_x loss_x 1.2451 (1.3911) acc_x 71.8750 (68.4375) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [25/29] time 0.359 (0.479) data 0.229 (0.348) loss_x loss_x 1.5996 (1.3402) acc_x 62.5000 (69.3750) lr 1.3535e-03 eta 0:00:01
epoch [79/200] batch [5/68] time 0.350 (0.478) data 0.220 (0.348) loss_u loss_u 0.9575 (0.9355) acc_u 6.2500 (8.1250) lr 1.3535e-03 eta 0:00:30
epoch [79/200] batch [10/68] time 0.442 (0.472) data 0.311 (0.341) loss_u loss_u 0.9243 (0.9213) acc_u 9.3750 (10.0000) lr 1.3535e-03 eta 0:00:27
epoch [79/200] batch [15/68] time 0.427 (0.464) data 0.295 (0.334) loss_u loss_u 0.8877 (0.9182) acc_u 12.5000 (10.0000) lr 1.3535e-03 eta 0:00:24
epoch [79/200] batch [20/68] time 0.407 (0.460) data 0.275 (0.329) loss_u loss_u 0.9302 (0.9201) acc_u 9.3750 (10.0000) lr 1.3535e-03 eta 0:00:22
epoch [79/200] batch [25/68] time 0.426 (0.464) data 0.295 (0.333) loss_u loss_u 0.9595 (0.9265) acc_u 3.1250 (9.0000) lr 1.3535e-03 eta 0:00:19
epoch [79/200] batch [30/68] time 0.507 (0.462) data 0.376 (0.331) loss_u loss_u 0.9575 (0.9229) acc_u 6.2500 (9.7917) lr 1.3535e-03 eta 0:00:17
epoch [79/200] batch [35/68] time 0.623 (0.466) data 0.492 (0.335) loss_u loss_u 0.9585 (0.9214) acc_u 9.3750 (10.0893) lr 1.3535e-03 eta 0:00:15
epoch [79/200] batch [40/68] time 0.494 (0.466) data 0.363 (0.335) loss_u loss_u 0.9082 (0.9197) acc_u 12.5000 (10.3906) lr 1.3535e-03 eta 0:00:13
epoch [79/200] batch [45/68] time 0.399 (0.465) data 0.268 (0.334) loss_u loss_u 0.8906 (0.9170) acc_u 15.6250 (10.9028) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [50/68] time 0.411 (0.464) data 0.281 (0.333) loss_u loss_u 0.8926 (0.9191) acc_u 21.8750 (10.8125) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [55/68] time 0.500 (0.464) data 0.370 (0.333) loss_u loss_u 0.9253 (0.9210) acc_u 12.5000 (10.4545) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [60/68] time 0.540 (0.466) data 0.408 (0.335) loss_u loss_u 0.8667 (0.9195) acc_u 15.6250 (10.5208) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [65/68] time 0.375 (0.465) data 0.244 (0.334) loss_u loss_u 0.9517 (0.9209) acc_u 3.1250 (10.2404) lr 1.3535e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1547
confident_label rate tensor(0.2777, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 871
clean true:865
clean false:6
clean_rate:0.9931113662456946
noisy true:724
noisy false:1541
after delete: len(clean_dataset) 871
after delete: len(noisy_dataset) 2265
epoch [80/200] batch [5/27] time 0.358 (0.438) data 0.227 (0.307) loss_x loss_x 1.5264 (1.3331) acc_x 71.8750 (70.6250) lr 1.3387e-03 eta 0:00:09
epoch [80/200] batch [10/27] time 0.429 (0.438) data 0.298 (0.307) loss_x loss_x 1.2891 (1.3317) acc_x 78.1250 (70.3125) lr 1.3387e-03 eta 0:00:07
epoch [80/200] batch [15/27] time 0.377 (0.433) data 0.246 (0.302) loss_x loss_x 0.8633 (1.2082) acc_x 78.1250 (72.2917) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [20/27] time 0.468 (0.435) data 0.337 (0.304) loss_x loss_x 1.2305 (1.2268) acc_x 78.1250 (71.7188) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [25/27] time 0.453 (0.441) data 0.320 (0.310) loss_x loss_x 1.3447 (1.2217) acc_x 71.8750 (71.8750) lr 1.3387e-03 eta 0:00:00
epoch [80/200] batch [5/70] time 0.326 (0.438) data 0.194 (0.307) loss_u loss_u 0.8726 (0.9068) acc_u 15.6250 (11.8750) lr 1.3387e-03 eta 0:00:28
epoch [80/200] batch [10/70] time 0.338 (0.432) data 0.206 (0.301) loss_u loss_u 0.9033 (0.9007) acc_u 12.5000 (12.8125) lr 1.3387e-03 eta 0:00:25
epoch [80/200] batch [15/70] time 0.601 (0.435) data 0.469 (0.304) loss_u loss_u 0.8975 (0.9023) acc_u 12.5000 (12.5000) lr 1.3387e-03 eta 0:00:23
epoch [80/200] batch [20/70] time 0.541 (0.445) data 0.410 (0.314) loss_u loss_u 0.9321 (0.9015) acc_u 6.2500 (12.3438) lr 1.3387e-03 eta 0:00:22
epoch [80/200] batch [25/70] time 0.453 (0.448) data 0.322 (0.317) loss_u loss_u 0.8892 (0.9019) acc_u 12.5000 (12.2500) lr 1.3387e-03 eta 0:00:20
epoch [80/200] batch [30/70] time 0.462 (0.445) data 0.330 (0.314) loss_u loss_u 0.9849 (0.9081) acc_u 0.0000 (11.4583) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [35/70] time 0.438 (0.448) data 0.307 (0.316) loss_u loss_u 0.9092 (0.9082) acc_u 9.3750 (11.4286) lr 1.3387e-03 eta 0:00:15
epoch [80/200] batch [40/70] time 0.360 (0.446) data 0.229 (0.315) loss_u loss_u 0.9517 (0.9104) acc_u 6.2500 (10.9375) lr 1.3387e-03 eta 0:00:13
epoch [80/200] batch [45/70] time 0.417 (0.446) data 0.285 (0.314) loss_u loss_u 0.8984 (0.9102) acc_u 15.6250 (10.7639) lr 1.3387e-03 eta 0:00:11
epoch [80/200] batch [50/70] time 0.439 (0.450) data 0.308 (0.319) loss_u loss_u 0.9111 (0.9085) acc_u 15.6250 (11.1250) lr 1.3387e-03 eta 0:00:09
epoch [80/200] batch [55/70] time 0.627 (0.453) data 0.496 (0.321) loss_u loss_u 0.8433 (0.9062) acc_u 21.8750 (11.6477) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [60/70] time 0.415 (0.455) data 0.283 (0.324) loss_u loss_u 0.9180 (0.9043) acc_u 15.6250 (11.8229) lr 1.3387e-03 eta 0:00:04
epoch [80/200] batch [65/70] time 0.420 (0.455) data 0.289 (0.324) loss_u loss_u 0.9028 (0.9044) acc_u 12.5000 (11.7788) lr 1.3387e-03 eta 0:00:02
epoch [80/200] batch [70/70] time 0.446 (0.455) data 0.316 (0.323) loss_u loss_u 0.8818 (0.9045) acc_u 15.6250 (11.7857) lr 1.3387e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1555
confident_label rate tensor(0.2851, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 894
clean true:885
clean false:9
clean_rate:0.9899328859060402
noisy true:696
noisy false:1546
after delete: len(clean_dataset) 894
after delete: len(noisy_dataset) 2242
epoch [81/200] batch [5/27] time 0.447 (0.472) data 0.317 (0.342) loss_x loss_x 1.0869 (1.2717) acc_x 81.2500 (71.8750) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [10/27] time 0.617 (0.468) data 0.486 (0.337) loss_x loss_x 1.5889 (1.2244) acc_x 50.0000 (69.6875) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [15/27] time 0.429 (0.459) data 0.299 (0.328) loss_x loss_x 1.7715 (1.2662) acc_x 59.3750 (68.1250) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [20/27] time 0.475 (0.484) data 0.345 (0.353) loss_x loss_x 0.7749 (1.2135) acc_x 90.6250 (71.0938) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [25/27] time 0.486 (0.481) data 0.356 (0.350) loss_x loss_x 1.2178 (1.2287) acc_x 78.1250 (70.7500) lr 1.3239e-03 eta 0:00:00
epoch [81/200] batch [5/70] time 0.448 (0.482) data 0.317 (0.351) loss_u loss_u 0.9326 (0.9198) acc_u 9.3750 (11.8750) lr 1.3239e-03 eta 0:00:31
epoch [81/200] batch [10/70] time 0.486 (0.485) data 0.355 (0.354) loss_u loss_u 0.9512 (0.9214) acc_u 6.2500 (10.9375) lr 1.3239e-03 eta 0:00:29
epoch [81/200] batch [15/70] time 0.421 (0.488) data 0.291 (0.357) loss_u loss_u 0.9189 (0.9192) acc_u 12.5000 (11.0417) lr 1.3239e-03 eta 0:00:26
epoch [81/200] batch [20/70] time 0.554 (0.488) data 0.422 (0.357) loss_u loss_u 0.9536 (0.9187) acc_u 6.2500 (11.2500) lr 1.3239e-03 eta 0:00:24
epoch [81/200] batch [25/70] time 0.398 (0.479) data 0.267 (0.348) loss_u loss_u 0.9038 (0.9161) acc_u 12.5000 (11.3750) lr 1.3239e-03 eta 0:00:21
epoch [81/200] batch [30/70] time 0.432 (0.478) data 0.300 (0.347) loss_u loss_u 0.9502 (0.9183) acc_u 6.2500 (11.1458) lr 1.3239e-03 eta 0:00:19
epoch [81/200] batch [35/70] time 0.480 (0.477) data 0.349 (0.346) loss_u loss_u 0.8481 (0.9148) acc_u 18.7500 (11.4286) lr 1.3239e-03 eta 0:00:16
epoch [81/200] batch [40/70] time 0.383 (0.474) data 0.251 (0.343) loss_u loss_u 0.8677 (0.9155) acc_u 18.7500 (11.2500) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [45/70] time 0.577 (0.472) data 0.446 (0.340) loss_u loss_u 0.9238 (0.9141) acc_u 12.5000 (11.2500) lr 1.3239e-03 eta 0:00:11
epoch [81/200] batch [50/70] time 0.513 (0.471) data 0.382 (0.340) loss_u loss_u 0.9399 (0.9146) acc_u 12.5000 (11.4375) lr 1.3239e-03 eta 0:00:09
epoch [81/200] batch [55/70] time 0.425 (0.470) data 0.294 (0.339) loss_u loss_u 0.9600 (0.9157) acc_u 9.3750 (11.3068) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [60/70] time 0.413 (0.469) data 0.279 (0.338) loss_u loss_u 0.9102 (0.9147) acc_u 15.6250 (11.3021) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [65/70] time 0.395 (0.469) data 0.264 (0.337) loss_u loss_u 0.9531 (0.9154) acc_u 3.1250 (11.1058) lr 1.3239e-03 eta 0:00:02
epoch [81/200] batch [70/70] time 0.459 (0.470) data 0.327 (0.338) loss_u loss_u 0.8789 (0.9147) acc_u 12.5000 (11.2054) lr 1.3239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1559
confident_label rate tensor(0.2819, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 884
clean true:874
clean false:10
clean_rate:0.9886877828054299
noisy true:703
noisy false:1549
after delete: len(clean_dataset) 884
after delete: len(noisy_dataset) 2252
epoch [82/200] batch [5/27] time 0.511 (0.501) data 0.377 (0.370) loss_x loss_x 1.1621 (1.2904) acc_x 75.0000 (70.0000) lr 1.3090e-03 eta 0:00:11
epoch [82/200] batch [10/27] time 0.593 (0.550) data 0.463 (0.419) loss_x loss_x 1.1279 (1.3021) acc_x 65.6250 (68.7500) lr 1.3090e-03 eta 0:00:09
epoch [82/200] batch [15/27] time 0.383 (0.532) data 0.252 (0.401) loss_x loss_x 1.0391 (1.2551) acc_x 71.8750 (69.5833) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [20/27] time 0.549 (0.527) data 0.419 (0.396) loss_x loss_x 0.7188 (1.1941) acc_x 81.2500 (71.2500) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [25/27] time 0.521 (0.525) data 0.391 (0.394) loss_x loss_x 0.9644 (1.1564) acc_x 78.1250 (72.1250) lr 1.3090e-03 eta 0:00:01
epoch [82/200] batch [5/70] time 0.551 (0.503) data 0.419 (0.372) loss_u loss_u 0.8848 (0.9176) acc_u 15.6250 (10.0000) lr 1.3090e-03 eta 0:00:32
epoch [82/200] batch [10/70] time 0.461 (0.500) data 0.328 (0.369) loss_u loss_u 0.9458 (0.9122) acc_u 6.2500 (10.9375) lr 1.3090e-03 eta 0:00:30
epoch [82/200] batch [15/70] time 0.462 (0.492) data 0.330 (0.361) loss_u loss_u 0.9116 (0.9141) acc_u 12.5000 (11.0417) lr 1.3090e-03 eta 0:00:27
epoch [82/200] batch [20/70] time 0.350 (0.486) data 0.219 (0.355) loss_u loss_u 0.9502 (0.9064) acc_u 6.2500 (12.1875) lr 1.3090e-03 eta 0:00:24
epoch [82/200] batch [25/70] time 0.438 (0.481) data 0.306 (0.350) loss_u loss_u 0.9292 (0.9099) acc_u 9.3750 (11.7500) lr 1.3090e-03 eta 0:00:21
epoch [82/200] batch [30/70] time 0.579 (0.480) data 0.447 (0.349) loss_u loss_u 0.9370 (0.9056) acc_u 9.3750 (12.3958) lr 1.3090e-03 eta 0:00:19
epoch [82/200] batch [35/70] time 0.437 (0.475) data 0.305 (0.344) loss_u loss_u 0.9624 (0.9067) acc_u 6.2500 (12.0536) lr 1.3090e-03 eta 0:00:16
epoch [82/200] batch [40/70] time 0.436 (0.476) data 0.305 (0.345) loss_u loss_u 0.9688 (0.9098) acc_u 9.3750 (11.5625) lr 1.3090e-03 eta 0:00:14
epoch [82/200] batch [45/70] time 0.412 (0.471) data 0.280 (0.340) loss_u loss_u 0.8989 (0.9087) acc_u 12.5000 (11.7361) lr 1.3090e-03 eta 0:00:11
epoch [82/200] batch [50/70] time 0.321 (0.468) data 0.189 (0.337) loss_u loss_u 0.8379 (0.9087) acc_u 21.8750 (11.8750) lr 1.3090e-03 eta 0:00:09
epoch [82/200] batch [55/70] time 0.405 (0.466) data 0.273 (0.335) loss_u loss_u 0.8638 (0.9075) acc_u 18.7500 (12.0455) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [60/70] time 0.371 (0.465) data 0.239 (0.334) loss_u loss_u 0.8428 (0.9079) acc_u 25.0000 (12.0312) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [65/70] time 0.465 (0.463) data 0.334 (0.332) loss_u loss_u 0.9282 (0.9087) acc_u 6.2500 (11.7308) lr 1.3090e-03 eta 0:00:02
epoch [82/200] batch [70/70] time 0.606 (0.464) data 0.476 (0.333) loss_u loss_u 0.9614 (0.9095) acc_u 3.1250 (11.5625) lr 1.3090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.2943, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 923
clean true:916
clean false:7
clean_rate:0.9924160346695557
noisy true:703
noisy false:1510
after delete: len(clean_dataset) 923
after delete: len(noisy_dataset) 2213
epoch [83/200] batch [5/28] time 0.436 (0.429) data 0.306 (0.299) loss_x loss_x 1.0566 (1.2428) acc_x 71.8750 (70.6250) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [10/28] time 0.430 (0.454) data 0.299 (0.323) loss_x loss_x 1.9834 (1.3046) acc_x 62.5000 (68.7500) lr 1.2940e-03 eta 0:00:08
epoch [83/200] batch [15/28] time 0.463 (0.468) data 0.332 (0.338) loss_x loss_x 1.3213 (1.3087) acc_x 65.6250 (68.9583) lr 1.2940e-03 eta 0:00:06
epoch [83/200] batch [20/28] time 0.491 (0.475) data 0.361 (0.344) loss_x loss_x 1.1475 (1.2982) acc_x 68.7500 (68.9062) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [25/28] time 0.448 (0.478) data 0.317 (0.347) loss_x loss_x 0.9922 (1.2778) acc_x 75.0000 (69.6250) lr 1.2940e-03 eta 0:00:01
epoch [83/200] batch [5/69] time 0.544 (0.483) data 0.412 (0.353) loss_u loss_u 0.8975 (0.9160) acc_u 12.5000 (10.0000) lr 1.2940e-03 eta 0:00:30
epoch [83/200] batch [10/69] time 0.371 (0.488) data 0.240 (0.357) loss_u loss_u 0.9712 (0.9214) acc_u 0.0000 (8.7500) lr 1.2940e-03 eta 0:00:28
epoch [83/200] batch [15/69] time 0.541 (0.481) data 0.409 (0.350) loss_u loss_u 0.9917 (0.9226) acc_u 0.0000 (9.3750) lr 1.2940e-03 eta 0:00:25
epoch [83/200] batch [20/69] time 0.583 (0.477) data 0.451 (0.347) loss_u loss_u 0.9307 (0.9190) acc_u 6.2500 (9.8438) lr 1.2940e-03 eta 0:00:23
epoch [83/200] batch [25/69] time 0.597 (0.475) data 0.466 (0.344) loss_u loss_u 0.9604 (0.9219) acc_u 3.1250 (9.6250) lr 1.2940e-03 eta 0:00:20
epoch [83/200] batch [30/69] time 0.550 (0.478) data 0.419 (0.347) loss_u loss_u 0.9180 (0.9194) acc_u 9.3750 (10.0000) lr 1.2940e-03 eta 0:00:18
epoch [83/200] batch [35/69] time 0.421 (0.475) data 0.289 (0.344) loss_u loss_u 0.9546 (0.9191) acc_u 6.2500 (9.9107) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [40/69] time 0.477 (0.475) data 0.347 (0.344) loss_u loss_u 0.8564 (0.9176) acc_u 18.7500 (10.3125) lr 1.2940e-03 eta 0:00:13
epoch [83/200] batch [45/69] time 0.537 (0.475) data 0.405 (0.344) loss_u loss_u 0.9912 (0.9213) acc_u 0.0000 (9.7917) lr 1.2940e-03 eta 0:00:11
epoch [83/200] batch [50/69] time 0.465 (0.471) data 0.334 (0.340) loss_u loss_u 0.9785 (0.9201) acc_u 0.0000 (10.2500) lr 1.2940e-03 eta 0:00:08
epoch [83/200] batch [55/69] time 0.516 (0.473) data 0.385 (0.342) loss_u loss_u 0.9531 (0.9195) acc_u 6.2500 (10.3409) lr 1.2940e-03 eta 0:00:06
epoch [83/200] batch [60/69] time 0.328 (0.469) data 0.197 (0.338) loss_u loss_u 0.8896 (0.9184) acc_u 15.6250 (10.4688) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [65/69] time 0.462 (0.471) data 0.330 (0.340) loss_u loss_u 0.9282 (0.9148) acc_u 9.3750 (11.0096) lr 1.2940e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.2873, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 901
clean true:896
clean false:5
clean_rate:0.9944506104328524
noisy true:723
noisy false:1512
after delete: len(clean_dataset) 901
after delete: len(noisy_dataset) 2235
epoch [84/200] batch [5/28] time 0.542 (0.525) data 0.412 (0.395) loss_x loss_x 1.7451 (1.2856) acc_x 50.0000 (68.1250) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [10/28] time 0.663 (0.523) data 0.533 (0.392) loss_x loss_x 1.0254 (1.2204) acc_x 81.2500 (69.0625) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [15/28] time 0.480 (0.506) data 0.349 (0.375) loss_x loss_x 1.5088 (1.3041) acc_x 62.5000 (67.2917) lr 1.2790e-03 eta 0:00:06
epoch [84/200] batch [20/28] time 0.439 (0.495) data 0.309 (0.364) loss_x loss_x 0.6582 (1.2623) acc_x 84.3750 (68.2812) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [25/28] time 0.383 (0.488) data 0.253 (0.358) loss_x loss_x 1.0176 (1.2717) acc_x 75.0000 (67.8750) lr 1.2790e-03 eta 0:00:01
epoch [84/200] batch [5/69] time 0.425 (0.484) data 0.295 (0.353) loss_u loss_u 0.8950 (0.8749) acc_u 12.5000 (16.2500) lr 1.2790e-03 eta 0:00:30
epoch [84/200] batch [10/69] time 0.465 (0.489) data 0.333 (0.358) loss_u loss_u 0.8848 (0.8807) acc_u 12.5000 (15.3125) lr 1.2790e-03 eta 0:00:28
epoch [84/200] batch [15/69] time 0.530 (0.487) data 0.398 (0.356) loss_u loss_u 0.8984 (0.8877) acc_u 9.3750 (14.5833) lr 1.2790e-03 eta 0:00:26
epoch [84/200] batch [20/69] time 0.425 (0.480) data 0.295 (0.349) loss_u loss_u 0.9272 (0.8940) acc_u 9.3750 (13.2812) lr 1.2790e-03 eta 0:00:23
epoch [84/200] batch [25/69] time 0.512 (0.484) data 0.378 (0.353) loss_u loss_u 0.9336 (0.8989) acc_u 9.3750 (12.6250) lr 1.2790e-03 eta 0:00:21
epoch [84/200] batch [30/69] time 0.437 (0.479) data 0.307 (0.347) loss_u loss_u 0.9194 (0.9016) acc_u 12.5000 (12.6042) lr 1.2790e-03 eta 0:00:18
epoch [84/200] batch [35/69] time 0.437 (0.473) data 0.306 (0.342) loss_u loss_u 0.9150 (0.9014) acc_u 9.3750 (12.8571) lr 1.2790e-03 eta 0:00:16
epoch [84/200] batch [40/69] time 0.413 (0.473) data 0.283 (0.342) loss_u loss_u 0.8984 (0.9044) acc_u 12.5000 (12.6562) lr 1.2790e-03 eta 0:00:13
epoch [84/200] batch [45/69] time 0.389 (0.467) data 0.258 (0.336) loss_u loss_u 0.9644 (0.9068) acc_u 3.1250 (12.2222) lr 1.2790e-03 eta 0:00:11
epoch [84/200] batch [50/69] time 0.356 (0.463) data 0.225 (0.332) loss_u loss_u 0.8696 (0.9051) acc_u 18.7500 (12.5000) lr 1.2790e-03 eta 0:00:08
epoch [84/200] batch [55/69] time 0.455 (0.462) data 0.324 (0.331) loss_u loss_u 0.9624 (0.9088) acc_u 3.1250 (11.8750) lr 1.2790e-03 eta 0:00:06
epoch [84/200] batch [60/69] time 0.423 (0.463) data 0.291 (0.332) loss_u loss_u 0.8936 (0.9069) acc_u 12.5000 (11.9792) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [65/69] time 0.510 (0.461) data 0.379 (0.330) loss_u loss_u 0.9028 (0.9064) acc_u 12.5000 (12.0192) lr 1.2790e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1556
confident_label rate tensor(0.2800, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 878
clean true:871
clean false:7
clean_rate:0.9920273348519362
noisy true:709
noisy false:1549
after delete: len(clean_dataset) 878
after delete: len(noisy_dataset) 2258
epoch [85/200] batch [5/27] time 0.587 (0.577) data 0.456 (0.445) loss_x loss_x 1.3047 (1.2435) acc_x 65.6250 (65.0000) lr 1.2639e-03 eta 0:00:12
epoch [85/200] batch [10/27] time 0.562 (0.549) data 0.432 (0.418) loss_x loss_x 0.8350 (1.1882) acc_x 75.0000 (67.1875) lr 1.2639e-03 eta 0:00:09
epoch [85/200] batch [15/27] time 0.395 (0.520) data 0.264 (0.389) loss_x loss_x 1.3945 (1.1199) acc_x 65.6250 (69.1667) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [20/27] time 0.502 (0.497) data 0.372 (0.366) loss_x loss_x 1.0342 (1.1059) acc_x 71.8750 (69.0625) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [25/27] time 0.450 (0.485) data 0.318 (0.355) loss_x loss_x 1.2549 (1.1429) acc_x 78.1250 (69.2500) lr 1.2639e-03 eta 0:00:00
epoch [85/200] batch [5/70] time 0.501 (0.481) data 0.370 (0.350) loss_u loss_u 0.9380 (0.9187) acc_u 12.5000 (10.6250) lr 1.2639e-03 eta 0:00:31
epoch [85/200] batch [10/70] time 0.516 (0.480) data 0.385 (0.349) loss_u loss_u 0.9229 (0.9074) acc_u 9.3750 (12.8125) lr 1.2639e-03 eta 0:00:28
epoch [85/200] batch [15/70] time 0.402 (0.481) data 0.271 (0.350) loss_u loss_u 0.9087 (0.9001) acc_u 6.2500 (12.5000) lr 1.2639e-03 eta 0:00:26
epoch [85/200] batch [20/70] time 0.372 (0.480) data 0.241 (0.349) loss_u loss_u 0.9814 (0.9083) acc_u 0.0000 (11.8750) lr 1.2639e-03 eta 0:00:24
epoch [85/200] batch [25/70] time 0.487 (0.480) data 0.355 (0.350) loss_u loss_u 0.9824 (0.9121) acc_u 0.0000 (11.2500) lr 1.2639e-03 eta 0:00:21
epoch [85/200] batch [30/70] time 0.486 (0.480) data 0.354 (0.349) loss_u loss_u 0.9321 (0.9118) acc_u 12.5000 (11.6667) lr 1.2639e-03 eta 0:00:19
epoch [85/200] batch [35/70] time 0.434 (0.475) data 0.303 (0.344) loss_u loss_u 0.9038 (0.9109) acc_u 9.3750 (11.7857) lr 1.2639e-03 eta 0:00:16
epoch [85/200] batch [40/70] time 0.481 (0.476) data 0.350 (0.345) loss_u loss_u 0.9209 (0.9093) acc_u 9.3750 (12.1875) lr 1.2639e-03 eta 0:00:14
epoch [85/200] batch [45/70] time 0.364 (0.473) data 0.233 (0.342) loss_u loss_u 0.9346 (0.9090) acc_u 6.2500 (11.8750) lr 1.2639e-03 eta 0:00:11
epoch [85/200] batch [50/70] time 0.482 (0.473) data 0.351 (0.342) loss_u loss_u 0.9224 (0.9107) acc_u 9.3750 (11.5000) lr 1.2639e-03 eta 0:00:09
epoch [85/200] batch [55/70] time 0.450 (0.471) data 0.318 (0.340) loss_u loss_u 0.9292 (0.9088) acc_u 9.3750 (11.8182) lr 1.2639e-03 eta 0:00:07
epoch [85/200] batch [60/70] time 0.477 (0.469) data 0.346 (0.338) loss_u loss_u 0.9399 (0.9087) acc_u 6.2500 (11.8229) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [65/70] time 0.400 (0.471) data 0.268 (0.340) loss_u loss_u 0.9199 (0.9080) acc_u 9.3750 (11.8269) lr 1.2639e-03 eta 0:00:02
epoch [85/200] batch [70/70] time 0.498 (0.471) data 0.367 (0.340) loss_u loss_u 0.9448 (0.9092) acc_u 6.2500 (11.7411) lr 1.2639e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1540
confident_label rate tensor(0.2854, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 895
clean true:887
clean false:8
clean_rate:0.9910614525139665
noisy true:709
noisy false:1532
after delete: len(clean_dataset) 895
after delete: len(noisy_dataset) 2241
epoch [86/200] batch [5/27] time 0.549 (0.507) data 0.418 (0.376) loss_x loss_x 1.6650 (1.3280) acc_x 59.3750 (68.1250) lr 1.2487e-03 eta 0:00:11
epoch [86/200] batch [10/27] time 0.437 (0.493) data 0.306 (0.362) loss_x loss_x 1.4062 (1.3599) acc_x 56.2500 (65.9375) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [15/27] time 0.451 (0.489) data 0.320 (0.359) loss_x loss_x 0.8403 (1.2549) acc_x 87.5000 (67.9167) lr 1.2487e-03 eta 0:00:05
epoch [86/200] batch [20/27] time 0.413 (0.472) data 0.282 (0.341) loss_x loss_x 1.3311 (1.2328) acc_x 65.6250 (67.9688) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [25/27] time 0.484 (0.492) data 0.353 (0.361) loss_x loss_x 1.6016 (1.2350) acc_x 59.3750 (68.1250) lr 1.2487e-03 eta 0:00:00
epoch [86/200] batch [5/70] time 0.435 (0.486) data 0.304 (0.355) loss_u loss_u 0.8872 (0.9246) acc_u 15.6250 (8.7500) lr 1.2487e-03 eta 0:00:31
epoch [86/200] batch [10/70] time 0.404 (0.482) data 0.273 (0.351) loss_u loss_u 0.9731 (0.9154) acc_u 3.1250 (10.0000) lr 1.2487e-03 eta 0:00:28
epoch [86/200] batch [15/70] time 0.567 (0.488) data 0.435 (0.357) loss_u loss_u 0.9458 (0.9138) acc_u 6.2500 (10.8333) lr 1.2487e-03 eta 0:00:26
epoch [86/200] batch [20/70] time 0.437 (0.488) data 0.306 (0.357) loss_u loss_u 0.9424 (0.9233) acc_u 6.2500 (9.5312) lr 1.2487e-03 eta 0:00:24
epoch [86/200] batch [25/70] time 0.560 (0.488) data 0.428 (0.357) loss_u loss_u 0.9204 (0.9194) acc_u 12.5000 (9.8750) lr 1.2487e-03 eta 0:00:21
epoch [86/200] batch [30/70] time 0.509 (0.484) data 0.378 (0.353) loss_u loss_u 0.9482 (0.9117) acc_u 6.2500 (10.8333) lr 1.2487e-03 eta 0:00:19
epoch [86/200] batch [35/70] time 0.475 (0.485) data 0.344 (0.353) loss_u loss_u 0.9214 (0.9079) acc_u 9.3750 (11.6071) lr 1.2487e-03 eta 0:00:16
epoch [86/200] batch [40/70] time 0.497 (0.486) data 0.365 (0.355) loss_u loss_u 0.9473 (0.9127) acc_u 6.2500 (11.0156) lr 1.2487e-03 eta 0:00:14
epoch [86/200] batch [45/70] time 0.466 (0.485) data 0.335 (0.354) loss_u loss_u 0.9502 (0.9124) acc_u 3.1250 (10.9722) lr 1.2487e-03 eta 0:00:12
epoch [86/200] batch [50/70] time 0.378 (0.483) data 0.247 (0.352) loss_u loss_u 0.9253 (0.9116) acc_u 12.5000 (11.3125) lr 1.2487e-03 eta 0:00:09
epoch [86/200] batch [55/70] time 0.486 (0.481) data 0.354 (0.349) loss_u loss_u 0.8574 (0.9086) acc_u 21.8750 (11.7614) lr 1.2487e-03 eta 0:00:07
epoch [86/200] batch [60/70] time 0.466 (0.478) data 0.336 (0.346) loss_u loss_u 0.9653 (0.9074) acc_u 3.1250 (11.8750) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [65/70] time 0.376 (0.476) data 0.244 (0.345) loss_u loss_u 0.9082 (0.9088) acc_u 9.3750 (11.5865) lr 1.2487e-03 eta 0:00:02
epoch [86/200] batch [70/70] time 0.405 (0.478) data 0.275 (0.347) loss_u loss_u 0.9053 (0.9085) acc_u 18.7500 (11.6964) lr 1.2487e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1520
confident_label rate tensor(0.2879, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 903
clean true:900
clean false:3
clean_rate:0.9966777408637874
noisy true:716
noisy false:1517
after delete: len(clean_dataset) 903
after delete: len(noisy_dataset) 2233
epoch [87/200] batch [5/28] time 0.472 (0.489) data 0.342 (0.358) loss_x loss_x 0.9312 (1.1503) acc_x 78.1250 (74.3750) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [10/28] time 0.413 (0.495) data 0.283 (0.364) loss_x loss_x 1.0869 (1.1084) acc_x 62.5000 (73.4375) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [15/28] time 0.472 (0.475) data 0.341 (0.344) loss_x loss_x 1.2334 (1.0892) acc_x 71.8750 (73.3333) lr 1.2334e-03 eta 0:00:06
epoch [87/200] batch [20/28] time 0.394 (0.466) data 0.263 (0.336) loss_x loss_x 0.6602 (1.0781) acc_x 87.5000 (74.0625) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [25/28] time 0.388 (0.456) data 0.258 (0.325) loss_x loss_x 1.2070 (1.0505) acc_x 59.3750 (74.0000) lr 1.2334e-03 eta 0:00:01
epoch [87/200] batch [5/69] time 0.469 (0.465) data 0.338 (0.335) loss_u loss_u 0.9136 (0.9186) acc_u 12.5000 (13.1250) lr 1.2334e-03 eta 0:00:29
epoch [87/200] batch [10/69] time 0.326 (0.458) data 0.195 (0.328) loss_u loss_u 0.9126 (0.9116) acc_u 12.5000 (13.4375) lr 1.2334e-03 eta 0:00:27
epoch [87/200] batch [15/69] time 0.463 (0.465) data 0.332 (0.335) loss_u loss_u 0.8755 (0.9072) acc_u 18.7500 (13.3333) lr 1.2334e-03 eta 0:00:25
epoch [87/200] batch [20/69] time 0.532 (0.465) data 0.400 (0.335) loss_u loss_u 0.9116 (0.9029) acc_u 12.5000 (13.7500) lr 1.2334e-03 eta 0:00:22
epoch [87/200] batch [25/69] time 0.485 (0.463) data 0.353 (0.332) loss_u loss_u 0.8535 (0.9006) acc_u 18.7500 (13.6250) lr 1.2334e-03 eta 0:00:20
epoch [87/200] batch [30/69] time 0.414 (0.463) data 0.281 (0.332) loss_u loss_u 0.8745 (0.9006) acc_u 15.6250 (13.3333) lr 1.2334e-03 eta 0:00:18
epoch [87/200] batch [35/69] time 0.380 (0.460) data 0.249 (0.330) loss_u loss_u 0.8403 (0.9028) acc_u 18.7500 (13.0357) lr 1.2334e-03 eta 0:00:15
epoch [87/200] batch [40/69] time 0.509 (0.462) data 0.379 (0.331) loss_u loss_u 0.9307 (0.9054) acc_u 6.2500 (12.4219) lr 1.2334e-03 eta 0:00:13
epoch [87/200] batch [45/69] time 0.437 (0.463) data 0.306 (0.332) loss_u loss_u 0.9165 (0.9033) acc_u 9.3750 (12.5694) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [50/69] time 0.454 (0.461) data 0.322 (0.330) loss_u loss_u 0.8936 (0.9022) acc_u 15.6250 (12.8125) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [55/69] time 0.430 (0.462) data 0.298 (0.330) loss_u loss_u 0.9253 (0.9032) acc_u 9.3750 (12.7841) lr 1.2334e-03 eta 0:00:06
epoch [87/200] batch [60/69] time 0.476 (0.459) data 0.345 (0.328) loss_u loss_u 0.8804 (0.9045) acc_u 12.5000 (12.3958) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [65/69] time 0.540 (0.460) data 0.409 (0.329) loss_u loss_u 0.8896 (0.9046) acc_u 12.5000 (12.4038) lr 1.2334e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1520
confident_label rate tensor(0.2934, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 920
clean true:908
clean false:12
clean_rate:0.9869565217391304
noisy true:708
noisy false:1508
after delete: len(clean_dataset) 920
after delete: len(noisy_dataset) 2216
epoch [88/200] batch [5/28] time 0.580 (0.525) data 0.448 (0.393) loss_x loss_x 1.3906 (1.2261) acc_x 68.7500 (66.2500) lr 1.2181e-03 eta 0:00:12
epoch [88/200] batch [10/28] time 0.430 (0.513) data 0.299 (0.382) loss_x loss_x 1.1768 (1.2646) acc_x 71.8750 (69.0625) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [15/28] time 0.518 (0.506) data 0.387 (0.375) loss_x loss_x 0.8501 (1.2668) acc_x 75.0000 (67.9167) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [20/28] time 0.456 (0.491) data 0.326 (0.360) loss_x loss_x 1.0850 (1.2027) acc_x 65.6250 (69.2188) lr 1.2181e-03 eta 0:00:03
epoch [88/200] batch [25/28] time 0.365 (0.485) data 0.234 (0.354) loss_x loss_x 1.0557 (1.1916) acc_x 62.5000 (68.8750) lr 1.2181e-03 eta 0:00:01
epoch [88/200] batch [5/69] time 0.604 (0.480) data 0.472 (0.349) loss_u loss_u 0.9575 (0.9525) acc_u 3.1250 (5.0000) lr 1.2181e-03 eta 0:00:30
epoch [88/200] batch [10/69] time 0.451 (0.478) data 0.320 (0.347) loss_u loss_u 0.9170 (0.9262) acc_u 12.5000 (9.0625) lr 1.2181e-03 eta 0:00:28
epoch [88/200] batch [15/69] time 0.479 (0.476) data 0.347 (0.346) loss_u loss_u 0.8989 (0.9223) acc_u 12.5000 (9.5833) lr 1.2181e-03 eta 0:00:25
epoch [88/200] batch [20/69] time 0.463 (0.478) data 0.331 (0.347) loss_u loss_u 0.9194 (0.9210) acc_u 9.3750 (9.8438) lr 1.2181e-03 eta 0:00:23
epoch [88/200] batch [25/69] time 0.372 (0.475) data 0.240 (0.344) loss_u loss_u 0.9233 (0.9192) acc_u 12.5000 (10.1250) lr 1.2181e-03 eta 0:00:20
epoch [88/200] batch [30/69] time 0.478 (0.473) data 0.347 (0.342) loss_u loss_u 0.9043 (0.9182) acc_u 12.5000 (10.3125) lr 1.2181e-03 eta 0:00:18
epoch [88/200] batch [35/69] time 0.445 (0.474) data 0.315 (0.343) loss_u loss_u 0.8379 (0.9140) acc_u 18.7500 (10.9821) lr 1.2181e-03 eta 0:00:16
epoch [88/200] batch [40/69] time 0.432 (0.473) data 0.302 (0.342) loss_u loss_u 0.9775 (0.9141) acc_u 3.1250 (11.0156) lr 1.2181e-03 eta 0:00:13
epoch [88/200] batch [45/69] time 0.400 (0.477) data 0.269 (0.346) loss_u loss_u 0.8560 (0.9109) acc_u 18.7500 (11.2500) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [50/69] time 0.374 (0.474) data 0.243 (0.343) loss_u loss_u 0.9336 (0.9120) acc_u 12.5000 (11.1875) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [55/69] time 0.433 (0.472) data 0.301 (0.341) loss_u loss_u 0.9399 (0.9130) acc_u 6.2500 (11.1364) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [60/69] time 0.506 (0.470) data 0.375 (0.339) loss_u loss_u 0.9409 (0.9127) acc_u 6.2500 (11.3021) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [65/69] time 0.353 (0.468) data 0.222 (0.336) loss_u loss_u 0.9004 (0.9135) acc_u 15.6250 (11.3462) lr 1.2181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1483
confident_label rate tensor(0.2956, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 927
clean true:914
clean false:13
clean_rate:0.9859762675296656
noisy true:739
noisy false:1470
after delete: len(clean_dataset) 927
after delete: len(noisy_dataset) 2209
epoch [89/200] batch [5/28] time 0.522 (0.520) data 0.390 (0.389) loss_x loss_x 1.0547 (1.2057) acc_x 75.0000 (72.5000) lr 1.2028e-03 eta 0:00:11
epoch [89/200] batch [10/28] time 0.385 (0.490) data 0.254 (0.359) loss_x loss_x 1.5381 (1.1734) acc_x 65.6250 (73.1250) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [15/28] time 0.360 (0.487) data 0.230 (0.356) loss_x loss_x 1.0068 (1.2432) acc_x 71.8750 (71.2500) lr 1.2028e-03 eta 0:00:06
epoch [89/200] batch [20/28] time 0.556 (0.482) data 0.425 (0.351) loss_x loss_x 0.8169 (1.2130) acc_x 78.1250 (71.7188) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [25/28] time 0.465 (0.485) data 0.335 (0.354) loss_x loss_x 1.2422 (1.2042) acc_x 71.8750 (72.3750) lr 1.2028e-03 eta 0:00:01
epoch [89/200] batch [5/69] time 0.382 (0.470) data 0.251 (0.339) loss_u loss_u 0.8926 (0.9190) acc_u 12.5000 (8.7500) lr 1.2028e-03 eta 0:00:30
epoch [89/200] batch [10/69] time 0.393 (0.468) data 0.263 (0.337) loss_u loss_u 0.8599 (0.9132) acc_u 21.8750 (10.9375) lr 1.2028e-03 eta 0:00:27
epoch [89/200] batch [15/69] time 0.408 (0.465) data 0.277 (0.334) loss_u loss_u 0.9619 (0.9205) acc_u 6.2500 (10.2083) lr 1.2028e-03 eta 0:00:25
epoch [89/200] batch [20/69] time 0.465 (0.463) data 0.334 (0.332) loss_u loss_u 0.9331 (0.9138) acc_u 9.3750 (11.2500) lr 1.2028e-03 eta 0:00:22
epoch [89/200] batch [25/69] time 0.357 (0.459) data 0.226 (0.328) loss_u loss_u 0.9243 (0.9177) acc_u 6.2500 (10.6250) lr 1.2028e-03 eta 0:00:20
epoch [89/200] batch [30/69] time 0.508 (0.465) data 0.377 (0.334) loss_u loss_u 0.9634 (0.9163) acc_u 6.2500 (10.7292) lr 1.2028e-03 eta 0:00:18
epoch [89/200] batch [35/69] time 0.476 (0.469) data 0.345 (0.338) loss_u loss_u 0.9434 (0.9183) acc_u 3.1250 (10.4464) lr 1.2028e-03 eta 0:00:15
epoch [89/200] batch [40/69] time 0.432 (0.469) data 0.300 (0.338) loss_u loss_u 0.8652 (0.9168) acc_u 15.6250 (10.5469) lr 1.2028e-03 eta 0:00:13
epoch [89/200] batch [45/69] time 0.498 (0.468) data 0.366 (0.336) loss_u loss_u 0.9028 (0.9171) acc_u 12.5000 (10.4861) lr 1.2028e-03 eta 0:00:11
epoch [89/200] batch [50/69] time 0.540 (0.467) data 0.408 (0.336) loss_u loss_u 0.9233 (0.9165) acc_u 12.5000 (10.6875) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [55/69] time 0.431 (0.466) data 0.300 (0.335) loss_u loss_u 0.8721 (0.9160) acc_u 15.6250 (10.7955) lr 1.2028e-03 eta 0:00:06
epoch [89/200] batch [60/69] time 0.478 (0.466) data 0.346 (0.334) loss_u loss_u 0.8989 (0.9147) acc_u 12.5000 (10.9896) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [65/69] time 0.422 (0.465) data 0.290 (0.334) loss_u loss_u 0.9292 (0.9135) acc_u 12.5000 (11.2981) lr 1.2028e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1503
confident_label rate tensor(0.2982, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 935
clean true:922
clean false:13
clean_rate:0.986096256684492
noisy true:711
noisy false:1490
after delete: len(clean_dataset) 935
after delete: len(noisy_dataset) 2201
epoch [90/200] batch [5/29] time 0.386 (0.434) data 0.255 (0.303) loss_x loss_x 1.4951 (1.1878) acc_x 59.3750 (68.1250) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [10/29] time 0.471 (0.475) data 0.340 (0.344) loss_x loss_x 1.0635 (1.1338) acc_x 75.0000 (71.5625) lr 1.1874e-03 eta 0:00:09
epoch [90/200] batch [15/29] time 0.476 (0.467) data 0.345 (0.336) loss_x loss_x 1.4785 (1.1346) acc_x 65.6250 (71.2500) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [20/29] time 0.498 (0.484) data 0.367 (0.353) loss_x loss_x 1.9883 (1.1846) acc_x 56.2500 (70.7812) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [25/29] time 0.434 (0.476) data 0.303 (0.346) loss_x loss_x 1.1748 (1.1583) acc_x 71.8750 (71.3750) lr 1.1874e-03 eta 0:00:01
epoch [90/200] batch [5/68] time 0.636 (0.472) data 0.504 (0.341) loss_u loss_u 0.9663 (0.9023) acc_u 6.2500 (14.3750) lr 1.1874e-03 eta 0:00:29
epoch [90/200] batch [10/68] time 0.532 (0.471) data 0.401 (0.340) loss_u loss_u 0.9219 (0.9061) acc_u 9.3750 (12.5000) lr 1.1874e-03 eta 0:00:27
epoch [90/200] batch [15/68] time 0.538 (0.480) data 0.407 (0.349) loss_u loss_u 0.9717 (0.9250) acc_u 0.0000 (9.5833) lr 1.1874e-03 eta 0:00:25
epoch [90/200] batch [20/68] time 0.391 (0.477) data 0.260 (0.346) loss_u loss_u 0.9116 (0.9247) acc_u 15.6250 (9.5312) lr 1.1874e-03 eta 0:00:22
epoch [90/200] batch [25/68] time 0.345 (0.475) data 0.213 (0.344) loss_u loss_u 0.9531 (0.9242) acc_u 6.2500 (9.7500) lr 1.1874e-03 eta 0:00:20
epoch [90/200] batch [30/68] time 0.396 (0.472) data 0.265 (0.340) loss_u loss_u 0.8667 (0.9233) acc_u 15.6250 (9.7917) lr 1.1874e-03 eta 0:00:17
epoch [90/200] batch [35/68] time 0.381 (0.470) data 0.250 (0.338) loss_u loss_u 0.9604 (0.9244) acc_u 3.1250 (9.6429) lr 1.1874e-03 eta 0:00:15
epoch [90/200] batch [40/68] time 0.532 (0.466) data 0.401 (0.335) loss_u loss_u 0.8506 (0.9227) acc_u 15.6250 (9.8438) lr 1.1874e-03 eta 0:00:13
epoch [90/200] batch [45/68] time 0.591 (0.464) data 0.461 (0.333) loss_u loss_u 0.9170 (0.9201) acc_u 6.2500 (10.2778) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [50/68] time 0.347 (0.460) data 0.215 (0.329) loss_u loss_u 0.9175 (0.9178) acc_u 12.5000 (10.6875) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [55/68] time 0.507 (0.460) data 0.377 (0.329) loss_u loss_u 0.9922 (0.9169) acc_u 0.0000 (10.7955) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [60/68] time 0.575 (0.458) data 0.444 (0.327) loss_u loss_u 0.8638 (0.9170) acc_u 15.6250 (10.6771) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [65/68] time 0.379 (0.457) data 0.248 (0.326) loss_u loss_u 0.8818 (0.9155) acc_u 15.6250 (10.8173) lr 1.1874e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1514
confident_label rate tensor(0.2956, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 927
clean true:920
clean false:7
clean_rate:0.9924487594390508
noisy true:702
noisy false:1507
after delete: len(clean_dataset) 927
after delete: len(noisy_dataset) 2209
epoch [91/200] batch [5/28] time 0.505 (0.438) data 0.374 (0.307) loss_x loss_x 1.3643 (1.3566) acc_x 75.0000 (70.0000) lr 1.1719e-03 eta 0:00:10
epoch [91/200] batch [10/28] time 0.382 (0.431) data 0.251 (0.300) loss_x loss_x 1.3018 (1.3732) acc_x 75.0000 (69.0625) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [15/28] time 0.617 (0.446) data 0.487 (0.316) loss_x loss_x 1.1865 (1.2681) acc_x 81.2500 (72.2917) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [20/28] time 0.419 (0.461) data 0.288 (0.331) loss_x loss_x 0.9004 (1.2449) acc_x 71.8750 (72.0312) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [25/28] time 0.353 (0.455) data 0.221 (0.324) loss_x loss_x 1.3613 (1.2395) acc_x 65.6250 (71.1250) lr 1.1719e-03 eta 0:00:01
epoch [91/200] batch [5/69] time 0.636 (0.469) data 0.504 (0.338) loss_u loss_u 0.8779 (0.9057) acc_u 21.8750 (11.8750) lr 1.1719e-03 eta 0:00:29
epoch [91/200] batch [10/69] time 0.777 (0.470) data 0.646 (0.339) loss_u loss_u 0.8813 (0.9056) acc_u 12.5000 (11.5625) lr 1.1719e-03 eta 0:00:27
epoch [91/200] batch [15/69] time 0.638 (0.469) data 0.508 (0.338) loss_u loss_u 0.8853 (0.9028) acc_u 12.5000 (11.8750) lr 1.1719e-03 eta 0:00:25
epoch [91/200] batch [20/69] time 0.357 (0.471) data 0.226 (0.340) loss_u loss_u 0.8750 (0.9013) acc_u 12.5000 (11.8750) lr 1.1719e-03 eta 0:00:23
epoch [91/200] batch [25/69] time 0.441 (0.465) data 0.308 (0.335) loss_u loss_u 0.9160 (0.9007) acc_u 9.3750 (11.6250) lr 1.1719e-03 eta 0:00:20
epoch [91/200] batch [30/69] time 0.480 (0.463) data 0.349 (0.332) loss_u loss_u 0.9292 (0.9054) acc_u 9.3750 (11.2500) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [35/69] time 0.504 (0.467) data 0.373 (0.336) loss_u loss_u 0.9478 (0.9063) acc_u 6.2500 (11.2500) lr 1.1719e-03 eta 0:00:15
epoch [91/200] batch [40/69] time 0.455 (0.463) data 0.323 (0.332) loss_u loss_u 0.9097 (0.9094) acc_u 9.3750 (11.0156) lr 1.1719e-03 eta 0:00:13
epoch [91/200] batch [45/69] time 0.437 (0.463) data 0.306 (0.332) loss_u loss_u 0.9380 (0.9078) acc_u 12.5000 (11.3889) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [50/69] time 0.466 (0.461) data 0.334 (0.330) loss_u loss_u 0.8584 (0.9093) acc_u 21.8750 (11.3125) lr 1.1719e-03 eta 0:00:08
epoch [91/200] batch [55/69] time 0.358 (0.458) data 0.227 (0.326) loss_u loss_u 0.9771 (0.9092) acc_u 0.0000 (11.3068) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [60/69] time 0.487 (0.457) data 0.357 (0.326) loss_u loss_u 0.8706 (0.9085) acc_u 12.5000 (11.3542) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [65/69] time 0.456 (0.461) data 0.325 (0.330) loss_u loss_u 0.9141 (0.9099) acc_u 6.2500 (11.1058) lr 1.1719e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.2867, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 899
clean true:891
clean false:8
clean_rate:0.9911012235817576
noisy true:728
noisy false:1509
after delete: len(clean_dataset) 899
after delete: len(noisy_dataset) 2237
epoch [92/200] batch [5/28] time 0.396 (0.450) data 0.266 (0.319) loss_x loss_x 0.7827 (1.2132) acc_x 81.2500 (73.7500) lr 1.1564e-03 eta 0:00:10
epoch [92/200] batch [10/28] time 0.441 (0.456) data 0.310 (0.326) loss_x loss_x 0.8228 (1.3260) acc_x 81.2500 (69.3750) lr 1.1564e-03 eta 0:00:08
epoch [92/200] batch [15/28] time 0.538 (0.498) data 0.408 (0.368) loss_x loss_x 1.3818 (1.2714) acc_x 62.5000 (69.1667) lr 1.1564e-03 eta 0:00:06
epoch [92/200] batch [20/28] time 0.539 (0.495) data 0.409 (0.365) loss_x loss_x 0.6914 (1.2222) acc_x 78.1250 (70.6250) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [25/28] time 0.463 (0.485) data 0.333 (0.355) loss_x loss_x 1.2051 (1.1867) acc_x 71.8750 (70.7500) lr 1.1564e-03 eta 0:00:01
epoch [92/200] batch [5/69] time 0.361 (0.470) data 0.230 (0.339) loss_u loss_u 0.8125 (0.8802) acc_u 21.8750 (15.6250) lr 1.1564e-03 eta 0:00:30
epoch [92/200] batch [10/69] time 0.443 (0.463) data 0.311 (0.333) loss_u loss_u 0.9185 (0.8890) acc_u 15.6250 (15.9375) lr 1.1564e-03 eta 0:00:27
epoch [92/200] batch [15/69] time 0.406 (0.457) data 0.276 (0.327) loss_u loss_u 0.9165 (0.8839) acc_u 9.3750 (16.0417) lr 1.1564e-03 eta 0:00:24
epoch [92/200] batch [20/69] time 0.608 (0.455) data 0.477 (0.325) loss_u loss_u 0.8662 (0.8912) acc_u 15.6250 (14.5312) lr 1.1564e-03 eta 0:00:22
epoch [92/200] batch [25/69] time 0.536 (0.454) data 0.404 (0.324) loss_u loss_u 0.9448 (0.8932) acc_u 6.2500 (14.1250) lr 1.1564e-03 eta 0:00:19
epoch [92/200] batch [30/69] time 0.382 (0.454) data 0.252 (0.324) loss_u loss_u 0.9644 (0.8973) acc_u 3.1250 (13.4375) lr 1.1564e-03 eta 0:00:17
epoch [92/200] batch [35/69] time 0.415 (0.451) data 0.284 (0.320) loss_u loss_u 0.9839 (0.9051) acc_u 0.0000 (11.9643) lr 1.1564e-03 eta 0:00:15
epoch [92/200] batch [40/69] time 0.495 (0.452) data 0.364 (0.321) loss_u loss_u 0.8589 (0.9043) acc_u 21.8750 (12.1875) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [45/69] time 0.519 (0.453) data 0.387 (0.322) loss_u loss_u 0.9302 (0.9063) acc_u 9.3750 (11.8056) lr 1.1564e-03 eta 0:00:10
epoch [92/200] batch [50/69] time 0.477 (0.452) data 0.346 (0.321) loss_u loss_u 0.9336 (0.9066) acc_u 12.5000 (11.7500) lr 1.1564e-03 eta 0:00:08
epoch [92/200] batch [55/69] time 0.426 (0.456) data 0.293 (0.325) loss_u loss_u 0.8486 (0.9051) acc_u 15.6250 (11.8750) lr 1.1564e-03 eta 0:00:06
epoch [92/200] batch [60/69] time 0.490 (0.461) data 0.358 (0.330) loss_u loss_u 0.9312 (0.9076) acc_u 12.5000 (11.6667) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [65/69] time 0.364 (0.460) data 0.234 (0.330) loss_u loss_u 0.9561 (0.9080) acc_u 6.2500 (11.6346) lr 1.1564e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1483
confident_label rate tensor(0.2985, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 936
clean true:926
clean false:10
clean_rate:0.9893162393162394
noisy true:727
noisy false:1473
after delete: len(clean_dataset) 936
after delete: len(noisy_dataset) 2200
epoch [93/200] batch [5/29] time 0.460 (0.432) data 0.329 (0.301) loss_x loss_x 1.4863 (1.2555) acc_x 59.3750 (66.8750) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [10/29] time 0.381 (0.453) data 0.251 (0.322) loss_x loss_x 0.8320 (1.2267) acc_x 87.5000 (68.1250) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [15/29] time 0.400 (0.457) data 0.269 (0.327) loss_x loss_x 1.4385 (1.2713) acc_x 71.8750 (69.7917) lr 1.1409e-03 eta 0:00:06
epoch [93/200] batch [20/29] time 0.389 (0.448) data 0.258 (0.318) loss_x loss_x 1.0381 (1.2660) acc_x 68.7500 (69.6875) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [25/29] time 0.610 (0.448) data 0.480 (0.317) loss_x loss_x 1.1084 (1.2479) acc_x 71.8750 (69.7500) lr 1.1409e-03 eta 0:00:01
epoch [93/200] batch [5/68] time 0.430 (0.463) data 0.299 (0.332) loss_u loss_u 0.9956 (0.9121) acc_u 0.0000 (10.6250) lr 1.1409e-03 eta 0:00:29
epoch [93/200] batch [10/68] time 0.414 (0.466) data 0.283 (0.336) loss_u loss_u 0.9727 (0.9134) acc_u 3.1250 (11.8750) lr 1.1409e-03 eta 0:00:27
epoch [93/200] batch [15/68] time 0.547 (0.468) data 0.415 (0.338) loss_u loss_u 0.9443 (0.9131) acc_u 6.2500 (11.6667) lr 1.1409e-03 eta 0:00:24
epoch [93/200] batch [20/68] time 0.458 (0.476) data 0.326 (0.345) loss_u loss_u 0.9385 (0.9178) acc_u 6.2500 (10.6250) lr 1.1409e-03 eta 0:00:22
epoch [93/200] batch [25/68] time 0.450 (0.469) data 0.319 (0.338) loss_u loss_u 0.8931 (0.9163) acc_u 12.5000 (11.0000) lr 1.1409e-03 eta 0:00:20
epoch [93/200] batch [30/68] time 0.369 (0.461) data 0.237 (0.331) loss_u loss_u 0.9229 (0.9108) acc_u 9.3750 (11.7708) lr 1.1409e-03 eta 0:00:17
epoch [93/200] batch [35/68] time 0.386 (0.459) data 0.255 (0.328) loss_u loss_u 0.9360 (0.9123) acc_u 9.3750 (11.3393) lr 1.1409e-03 eta 0:00:15
epoch [93/200] batch [40/68] time 0.336 (0.457) data 0.204 (0.326) loss_u loss_u 0.9263 (0.9130) acc_u 9.3750 (11.1719) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [45/68] time 0.512 (0.455) data 0.382 (0.324) loss_u loss_u 0.9131 (0.9124) acc_u 12.5000 (11.1111) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [50/68] time 0.355 (0.454) data 0.224 (0.323) loss_u loss_u 0.9185 (0.9103) acc_u 12.5000 (11.3125) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [55/68] time 0.488 (0.454) data 0.356 (0.323) loss_u loss_u 0.9004 (0.9105) acc_u 9.3750 (11.3636) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [60/68] time 0.401 (0.450) data 0.271 (0.319) loss_u loss_u 0.8843 (0.9100) acc_u 18.7500 (11.5625) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [65/68] time 0.442 (0.451) data 0.311 (0.320) loss_u loss_u 0.9272 (0.9114) acc_u 9.3750 (11.4423) lr 1.1409e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1526
confident_label rate tensor(0.2895, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 908
clean true:898
clean false:10
clean_rate:0.9889867841409692
noisy true:712
noisy false:1516
after delete: len(clean_dataset) 908
after delete: len(noisy_dataset) 2228
epoch [94/200] batch [5/28] time 0.498 (0.458) data 0.367 (0.327) loss_x loss_x 1.1387 (1.3600) acc_x 78.1250 (63.1250) lr 1.1253e-03 eta 0:00:10
epoch [94/200] batch [10/28] time 0.496 (0.456) data 0.364 (0.325) loss_x loss_x 1.4102 (1.3290) acc_x 65.6250 (65.6250) lr 1.1253e-03 eta 0:00:08
epoch [94/200] batch [15/28] time 0.428 (0.464) data 0.297 (0.333) loss_x loss_x 0.9307 (1.2607) acc_x 71.8750 (68.1250) lr 1.1253e-03 eta 0:00:06
epoch [94/200] batch [20/28] time 0.552 (0.473) data 0.420 (0.342) loss_x loss_x 1.1436 (1.2562) acc_x 68.7500 (68.2812) lr 1.1253e-03 eta 0:00:03
epoch [94/200] batch [25/28] time 0.439 (0.472) data 0.308 (0.341) loss_x loss_x 1.2734 (1.2794) acc_x 65.6250 (68.5000) lr 1.1253e-03 eta 0:00:01
epoch [94/200] batch [5/69] time 0.398 (0.490) data 0.267 (0.359) loss_u loss_u 0.9326 (0.9190) acc_u 12.5000 (11.2500) lr 1.1253e-03 eta 0:00:31
epoch [94/200] batch [10/69] time 0.430 (0.485) data 0.299 (0.354) loss_u loss_u 0.9121 (0.9151) acc_u 9.3750 (10.3125) lr 1.1253e-03 eta 0:00:28
epoch [94/200] batch [15/69] time 0.507 (0.483) data 0.375 (0.352) loss_u loss_u 0.8105 (0.9066) acc_u 25.0000 (11.8750) lr 1.1253e-03 eta 0:00:26
epoch [94/200] batch [20/69] time 0.413 (0.483) data 0.283 (0.352) loss_u loss_u 0.8833 (0.9024) acc_u 12.5000 (11.7188) lr 1.1253e-03 eta 0:00:23
epoch [94/200] batch [25/69] time 0.521 (0.480) data 0.390 (0.349) loss_u loss_u 0.9097 (0.9046) acc_u 9.3750 (11.3750) lr 1.1253e-03 eta 0:00:21
epoch [94/200] batch [30/69] time 0.373 (0.473) data 0.243 (0.342) loss_u loss_u 0.9482 (0.9087) acc_u 6.2500 (10.8333) lr 1.1253e-03 eta 0:00:18
epoch [94/200] batch [35/69] time 0.614 (0.474) data 0.483 (0.343) loss_u loss_u 0.9170 (0.9069) acc_u 12.5000 (11.3393) lr 1.1253e-03 eta 0:00:16
epoch [94/200] batch [40/69] time 0.460 (0.471) data 0.329 (0.340) loss_u loss_u 0.9600 (0.9117) acc_u 6.2500 (10.7812) lr 1.1253e-03 eta 0:00:13
epoch [94/200] batch [45/69] time 0.375 (0.466) data 0.244 (0.335) loss_u loss_u 0.9448 (0.9136) acc_u 9.3750 (10.6944) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [50/69] time 0.433 (0.464) data 0.302 (0.333) loss_u loss_u 0.9346 (0.9133) acc_u 9.3750 (10.8750) lr 1.1253e-03 eta 0:00:08
epoch [94/200] batch [55/69] time 0.489 (0.463) data 0.359 (0.332) loss_u loss_u 0.8745 (0.9143) acc_u 12.5000 (10.7386) lr 1.1253e-03 eta 0:00:06
epoch [94/200] batch [60/69] time 0.473 (0.463) data 0.342 (0.332) loss_u loss_u 0.9604 (0.9127) acc_u 3.1250 (10.8854) lr 1.1253e-03 eta 0:00:04
epoch [94/200] batch [65/69] time 0.624 (0.465) data 0.492 (0.334) loss_u loss_u 0.9116 (0.9116) acc_u 12.5000 (11.1538) lr 1.1253e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1515
confident_label rate tensor(0.2876, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 902
clean true:896
clean false:6
clean_rate:0.9933481152993349
noisy true:725
noisy false:1509
after delete: len(clean_dataset) 902
after delete: len(noisy_dataset) 2234
epoch [95/200] batch [5/28] time 0.401 (0.492) data 0.271 (0.362) loss_x loss_x 0.7549 (1.0899) acc_x 81.2500 (73.7500) lr 1.1097e-03 eta 0:00:11
epoch [95/200] batch [10/28] time 0.388 (0.467) data 0.257 (0.337) loss_x loss_x 1.0410 (1.2016) acc_x 78.1250 (70.9375) lr 1.1097e-03 eta 0:00:08
epoch [95/200] batch [15/28] time 0.495 (0.460) data 0.363 (0.329) loss_x loss_x 1.1973 (1.2248) acc_x 59.3750 (69.3750) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [20/28] time 0.478 (0.474) data 0.348 (0.343) loss_x loss_x 1.1357 (1.1798) acc_x 65.6250 (69.8438) lr 1.1097e-03 eta 0:00:03
epoch [95/200] batch [25/28] time 0.584 (0.484) data 0.452 (0.353) loss_x loss_x 0.9834 (1.1550) acc_x 75.0000 (70.7500) lr 1.1097e-03 eta 0:00:01
epoch [95/200] batch [5/69] time 0.525 (0.486) data 0.394 (0.355) loss_u loss_u 0.8862 (0.9164) acc_u 15.6250 (11.2500) lr 1.1097e-03 eta 0:00:31
epoch [95/200] batch [10/69] time 0.379 (0.479) data 0.248 (0.348) loss_u loss_u 0.9355 (0.9106) acc_u 3.1250 (10.6250) lr 1.1097e-03 eta 0:00:28
epoch [95/200] batch [15/69] time 0.520 (0.476) data 0.389 (0.345) loss_u loss_u 0.9229 (0.9185) acc_u 9.3750 (9.7917) lr 1.1097e-03 eta 0:00:25
epoch [95/200] batch [20/69] time 0.433 (0.474) data 0.302 (0.343) loss_u loss_u 0.9316 (0.9211) acc_u 6.2500 (9.3750) lr 1.1097e-03 eta 0:00:23
epoch [95/200] batch [25/69] time 0.440 (0.474) data 0.309 (0.343) loss_u loss_u 0.8320 (0.9159) acc_u 15.6250 (9.8750) lr 1.1097e-03 eta 0:00:20
epoch [95/200] batch [30/69] time 0.646 (0.482) data 0.515 (0.351) loss_u loss_u 0.7998 (0.9079) acc_u 31.2500 (10.8333) lr 1.1097e-03 eta 0:00:18
epoch [95/200] batch [35/69] time 0.407 (0.481) data 0.276 (0.350) loss_u loss_u 0.9375 (0.9076) acc_u 6.2500 (11.1607) lr 1.1097e-03 eta 0:00:16
epoch [95/200] batch [40/69] time 0.377 (0.479) data 0.244 (0.348) loss_u loss_u 0.9644 (0.9123) acc_u 3.1250 (10.7031) lr 1.1097e-03 eta 0:00:13
epoch [95/200] batch [45/69] time 0.374 (0.475) data 0.243 (0.344) loss_u loss_u 0.9180 (0.9108) acc_u 12.5000 (10.8333) lr 1.1097e-03 eta 0:00:11
epoch [95/200] batch [50/69] time 0.386 (0.474) data 0.254 (0.343) loss_u loss_u 0.9297 (0.9097) acc_u 6.2500 (11.1250) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [55/69] time 0.424 (0.474) data 0.293 (0.343) loss_u loss_u 0.8354 (0.9098) acc_u 21.8750 (11.1364) lr 1.1097e-03 eta 0:00:06
epoch [95/200] batch [60/69] time 0.344 (0.473) data 0.213 (0.341) loss_u loss_u 0.9331 (0.9088) acc_u 6.2500 (11.3542) lr 1.1097e-03 eta 0:00:04
epoch [95/200] batch [65/69] time 0.418 (0.474) data 0.287 (0.343) loss_u loss_u 0.9214 (0.9090) acc_u 12.5000 (11.2981) lr 1.1097e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1497
confident_label rate tensor(0.2899, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 909
clean true:901
clean false:8
clean_rate:0.9911991199119912
noisy true:738
noisy false:1489
after delete: len(clean_dataset) 909
after delete: len(noisy_dataset) 2227
epoch [96/200] batch [5/28] time 0.352 (0.419) data 0.221 (0.288) loss_x loss_x 1.3135 (1.1709) acc_x 62.5000 (68.7500) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [10/28] time 0.471 (0.436) data 0.341 (0.306) loss_x loss_x 0.8574 (1.0900) acc_x 87.5000 (72.5000) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [15/28] time 0.395 (0.439) data 0.265 (0.309) loss_x loss_x 0.9199 (1.0702) acc_x 71.8750 (72.9167) lr 1.0941e-03 eta 0:00:05
epoch [96/200] batch [20/28] time 0.502 (0.455) data 0.371 (0.325) loss_x loss_x 1.5889 (1.1181) acc_x 68.7500 (71.5625) lr 1.0941e-03 eta 0:00:03
epoch [96/200] batch [25/28] time 0.404 (0.459) data 0.273 (0.328) loss_x loss_x 0.8213 (1.1118) acc_x 81.2500 (71.8750) lr 1.0941e-03 eta 0:00:01
epoch [96/200] batch [5/69] time 0.414 (0.454) data 0.284 (0.324) loss_u loss_u 0.8916 (0.9054) acc_u 12.5000 (12.5000) lr 1.0941e-03 eta 0:00:29
epoch [96/200] batch [10/69] time 0.467 (0.459) data 0.335 (0.328) loss_u loss_u 0.9146 (0.9215) acc_u 12.5000 (10.6250) lr 1.0941e-03 eta 0:00:27
epoch [96/200] batch [15/69] time 0.454 (0.454) data 0.321 (0.324) loss_u loss_u 0.7852 (0.9079) acc_u 25.0000 (12.5000) lr 1.0941e-03 eta 0:00:24
epoch [96/200] batch [20/69] time 0.399 (0.453) data 0.268 (0.322) loss_u loss_u 0.8979 (0.9084) acc_u 9.3750 (12.1875) lr 1.0941e-03 eta 0:00:22
epoch [96/200] batch [25/69] time 0.504 (0.457) data 0.374 (0.326) loss_u loss_u 0.9219 (0.9112) acc_u 6.2500 (11.5000) lr 1.0941e-03 eta 0:00:20
epoch [96/200] batch [30/69] time 0.504 (0.464) data 0.372 (0.333) loss_u loss_u 0.9146 (0.9135) acc_u 6.2500 (11.0417) lr 1.0941e-03 eta 0:00:18
epoch [96/200] batch [35/69] time 0.579 (0.463) data 0.446 (0.332) loss_u loss_u 0.8936 (0.9120) acc_u 15.6250 (11.5179) lr 1.0941e-03 eta 0:00:15
epoch [96/200] batch [40/69] time 0.453 (0.462) data 0.322 (0.331) loss_u loss_u 0.9077 (0.9120) acc_u 9.3750 (11.4844) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [45/69] time 0.475 (0.460) data 0.344 (0.329) loss_u loss_u 0.8760 (0.9089) acc_u 18.7500 (11.9444) lr 1.0941e-03 eta 0:00:11
epoch [96/200] batch [50/69] time 0.439 (0.456) data 0.309 (0.325) loss_u loss_u 0.9395 (0.9079) acc_u 6.2500 (11.9375) lr 1.0941e-03 eta 0:00:08
epoch [96/200] batch [55/69] time 0.513 (0.457) data 0.382 (0.326) loss_u loss_u 0.9458 (0.9083) acc_u 12.5000 (11.9318) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [60/69] time 0.522 (0.459) data 0.392 (0.328) loss_u loss_u 0.9258 (0.9080) acc_u 9.3750 (12.0312) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [65/69] time 0.384 (0.458) data 0.253 (0.327) loss_u loss_u 0.9014 (0.9061) acc_u 12.5000 (12.3558) lr 1.0941e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1460
confident_label rate tensor(0.2994, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 939
clean true:924
clean false:15
clean_rate:0.9840255591054313
noisy true:752
noisy false:1445
after delete: len(clean_dataset) 939
after delete: len(noisy_dataset) 2197
epoch [97/200] batch [5/29] time 0.386 (0.483) data 0.256 (0.352) loss_x loss_x 0.9580 (1.3084) acc_x 71.8750 (68.1250) lr 1.0785e-03 eta 0:00:11
epoch [97/200] batch [10/29] time 0.373 (0.458) data 0.242 (0.328) loss_x loss_x 1.1094 (1.2589) acc_x 75.0000 (70.3125) lr 1.0785e-03 eta 0:00:08
epoch [97/200] batch [15/29] time 0.420 (0.467) data 0.289 (0.336) loss_x loss_x 1.0869 (1.2470) acc_x 75.0000 (68.9583) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [20/29] time 0.530 (0.477) data 0.400 (0.346) loss_x loss_x 0.8359 (1.2360) acc_x 84.3750 (69.3750) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [25/29] time 0.602 (0.479) data 0.471 (0.348) loss_x loss_x 0.9619 (1.2537) acc_x 75.0000 (69.6250) lr 1.0785e-03 eta 0:00:01
epoch [97/200] batch [5/68] time 0.515 (0.482) data 0.383 (0.352) loss_u loss_u 0.9009 (0.9202) acc_u 15.6250 (11.2500) lr 1.0785e-03 eta 0:00:30
epoch [97/200] batch [10/68] time 0.344 (0.474) data 0.212 (0.343) loss_u loss_u 0.8750 (0.9067) acc_u 15.6250 (13.1250) lr 1.0785e-03 eta 0:00:27
epoch [97/200] batch [15/68] time 0.463 (0.471) data 0.331 (0.340) loss_u loss_u 0.9272 (0.9139) acc_u 12.5000 (12.0833) lr 1.0785e-03 eta 0:00:24
epoch [97/200] batch [20/68] time 0.442 (0.469) data 0.310 (0.338) loss_u loss_u 0.9370 (0.9157) acc_u 6.2500 (11.4062) lr 1.0785e-03 eta 0:00:22
epoch [97/200] batch [25/68] time 0.376 (0.471) data 0.244 (0.340) loss_u loss_u 0.9219 (0.9120) acc_u 9.3750 (11.6250) lr 1.0785e-03 eta 0:00:20
epoch [97/200] batch [30/68] time 0.404 (0.473) data 0.273 (0.342) loss_u loss_u 0.9688 (0.9151) acc_u 6.2500 (11.0417) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [35/68] time 0.496 (0.469) data 0.365 (0.339) loss_u loss_u 0.9243 (0.9165) acc_u 12.5000 (10.9821) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [40/68] time 0.510 (0.473) data 0.378 (0.343) loss_u loss_u 0.9502 (0.9176) acc_u 6.2500 (10.8594) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [45/68] time 0.521 (0.473) data 0.390 (0.342) loss_u loss_u 0.9248 (0.9163) acc_u 12.5000 (11.1806) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [50/68] time 0.378 (0.471) data 0.247 (0.340) loss_u loss_u 0.9365 (0.9166) acc_u 9.3750 (11.0625) lr 1.0785e-03 eta 0:00:08
epoch [97/200] batch [55/68] time 0.457 (0.473) data 0.326 (0.342) loss_u loss_u 0.9077 (0.9186) acc_u 9.3750 (10.6818) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [60/68] time 0.327 (0.472) data 0.196 (0.341) loss_u loss_u 0.8682 (0.9154) acc_u 21.8750 (11.2500) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [65/68] time 0.423 (0.473) data 0.291 (0.342) loss_u loss_u 0.9258 (0.9141) acc_u 12.5000 (11.3942) lr 1.0785e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1512
confident_label rate tensor(0.2930, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 919
clean true:913
clean false:6
clean_rate:0.9934711643090316
noisy true:711
noisy false:1506
after delete: len(clean_dataset) 919
after delete: len(noisy_dataset) 2217
epoch [98/200] batch [5/28] time 0.583 (0.513) data 0.452 (0.382) loss_x loss_x 0.7925 (1.1075) acc_x 81.2500 (70.0000) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [10/28] time 0.434 (0.481) data 0.302 (0.350) loss_x loss_x 1.6309 (1.1257) acc_x 65.6250 (70.6250) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [15/28] time 0.511 (0.474) data 0.380 (0.343) loss_x loss_x 0.8730 (1.1330) acc_x 75.0000 (71.4583) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [20/28] time 0.598 (0.477) data 0.467 (0.346) loss_x loss_x 1.0820 (1.1326) acc_x 59.3750 (70.7812) lr 1.0628e-03 eta 0:00:03
epoch [98/200] batch [25/28] time 0.391 (0.474) data 0.260 (0.343) loss_x loss_x 1.5742 (1.1743) acc_x 59.3750 (69.2500) lr 1.0628e-03 eta 0:00:01
epoch [98/200] batch [5/69] time 0.440 (0.472) data 0.309 (0.341) loss_u loss_u 0.8916 (0.9004) acc_u 18.7500 (13.1250) lr 1.0628e-03 eta 0:00:30
epoch [98/200] batch [10/69] time 0.438 (0.467) data 0.307 (0.336) loss_u loss_u 0.9546 (0.9278) acc_u 6.2500 (9.6875) lr 1.0628e-03 eta 0:00:27
epoch [98/200] batch [15/69] time 0.529 (0.467) data 0.397 (0.336) loss_u loss_u 0.9653 (0.9198) acc_u 6.2500 (10.6250) lr 1.0628e-03 eta 0:00:25
epoch [98/200] batch [20/69] time 0.463 (0.465) data 0.331 (0.334) loss_u loss_u 0.8911 (0.9152) acc_u 15.6250 (11.5625) lr 1.0628e-03 eta 0:00:22
epoch [98/200] batch [25/69] time 0.440 (0.461) data 0.307 (0.330) loss_u loss_u 0.9058 (0.9137) acc_u 12.5000 (12.0000) lr 1.0628e-03 eta 0:00:20
epoch [98/200] batch [30/69] time 0.371 (0.462) data 0.237 (0.330) loss_u loss_u 0.9536 (0.9163) acc_u 9.3750 (11.8750) lr 1.0628e-03 eta 0:00:18
epoch [98/200] batch [35/69] time 0.535 (0.462) data 0.404 (0.331) loss_u loss_u 0.8877 (0.9161) acc_u 12.5000 (11.6071) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [40/69] time 0.376 (0.459) data 0.244 (0.328) loss_u loss_u 0.9541 (0.9157) acc_u 9.3750 (11.5625) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [45/69] time 0.685 (0.463) data 0.554 (0.331) loss_u loss_u 0.8965 (0.9138) acc_u 15.6250 (11.8056) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [50/69] time 0.410 (0.458) data 0.279 (0.327) loss_u loss_u 0.8657 (0.9110) acc_u 18.7500 (12.1875) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [55/69] time 0.488 (0.456) data 0.356 (0.324) loss_u loss_u 0.8335 (0.9097) acc_u 25.0000 (12.2727) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [60/69] time 0.433 (0.456) data 0.302 (0.325) loss_u loss_u 0.8599 (0.9057) acc_u 18.7500 (12.9167) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [65/69] time 0.526 (0.461) data 0.394 (0.330) loss_u loss_u 0.9536 (0.9069) acc_u 9.3750 (12.7885) lr 1.0628e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1479
confident_label rate tensor(0.2953, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 926
clean true:918
clean false:8
clean_rate:0.9913606911447084
noisy true:739
noisy false:1471
after delete: len(clean_dataset) 926
after delete: len(noisy_dataset) 2210
epoch [99/200] batch [5/28] time 0.507 (0.469) data 0.377 (0.338) loss_x loss_x 0.8711 (1.2102) acc_x 84.3750 (73.7500) lr 1.0471e-03 eta 0:00:10
epoch [99/200] batch [10/28] time 0.608 (0.491) data 0.477 (0.360) loss_x loss_x 1.2207 (1.2218) acc_x 62.5000 (73.7500) lr 1.0471e-03 eta 0:00:08
epoch [99/200] batch [15/28] time 0.495 (0.506) data 0.364 (0.375) loss_x loss_x 1.7656 (1.2576) acc_x 62.5000 (71.8750) lr 1.0471e-03 eta 0:00:06
epoch [99/200] batch [20/28] time 0.529 (0.508) data 0.398 (0.378) loss_x loss_x 1.0928 (1.2327) acc_x 78.1250 (72.3438) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [25/28] time 0.439 (0.495) data 0.308 (0.364) loss_x loss_x 1.2520 (1.2139) acc_x 68.7500 (72.0000) lr 1.0471e-03 eta 0:00:01
epoch [99/200] batch [5/69] time 0.430 (0.480) data 0.298 (0.349) loss_u loss_u 0.8662 (0.8870) acc_u 18.7500 (15.6250) lr 1.0471e-03 eta 0:00:30
epoch [99/200] batch [10/69] time 0.651 (0.484) data 0.520 (0.354) loss_u loss_u 0.9082 (0.8999) acc_u 12.5000 (13.7500) lr 1.0471e-03 eta 0:00:28
epoch [99/200] batch [15/69] time 0.558 (0.481) data 0.427 (0.351) loss_u loss_u 0.9355 (0.9137) acc_u 6.2500 (11.0417) lr 1.0471e-03 eta 0:00:25
epoch [99/200] batch [20/69] time 0.407 (0.477) data 0.275 (0.346) loss_u loss_u 0.7817 (0.9086) acc_u 28.1250 (11.5625) lr 1.0471e-03 eta 0:00:23
epoch [99/200] batch [25/69] time 0.410 (0.473) data 0.278 (0.341) loss_u loss_u 0.9541 (0.9079) acc_u 6.2500 (11.5000) lr 1.0471e-03 eta 0:00:20
epoch [99/200] batch [30/69] time 0.451 (0.470) data 0.320 (0.339) loss_u loss_u 0.9077 (0.9066) acc_u 9.3750 (11.5625) lr 1.0471e-03 eta 0:00:18
epoch [99/200] batch [35/69] time 0.417 (0.470) data 0.286 (0.339) loss_u loss_u 0.9307 (0.9072) acc_u 6.2500 (11.4286) lr 1.0471e-03 eta 0:00:15
epoch [99/200] batch [40/69] time 0.347 (0.465) data 0.216 (0.334) loss_u loss_u 0.9331 (0.9082) acc_u 9.3750 (11.5625) lr 1.0471e-03 eta 0:00:13
epoch [99/200] batch [45/69] time 0.469 (0.464) data 0.338 (0.333) loss_u loss_u 0.9482 (0.9075) acc_u 9.3750 (11.5972) lr 1.0471e-03 eta 0:00:11
epoch [99/200] batch [50/69] time 0.466 (0.462) data 0.335 (0.331) loss_u loss_u 0.9087 (0.9095) acc_u 9.3750 (11.1250) lr 1.0471e-03 eta 0:00:08
epoch [99/200] batch [55/69] time 0.362 (0.462) data 0.230 (0.331) loss_u loss_u 0.9121 (0.9104) acc_u 12.5000 (11.0227) lr 1.0471e-03 eta 0:00:06
epoch [99/200] batch [60/69] time 0.449 (0.459) data 0.317 (0.328) loss_u loss_u 0.9009 (0.9113) acc_u 15.6250 (10.9375) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [65/69] time 0.611 (0.465) data 0.480 (0.334) loss_u loss_u 0.9048 (0.9108) acc_u 12.5000 (10.9615) lr 1.0471e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1469
confident_label rate tensor(0.2950, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 925
clean true:918
clean false:7
clean_rate:0.9924324324324324
noisy true:749
noisy false:1462
all clean rate:  [0.9737704918032787, 0.990521327014218, 0.9892966360856269, 0.9896296296296296, 0.9938931297709923, 0.992503748125937, 0.9924471299093656, 0.9899569583931134, 0.9853587115666179, 0.9917241379310345, 0.995702005730659, 0.9955621301775148, 0.9943181818181818, 0.9902370990237099, 0.9901960784313726, 0.997289972899729, 0.9901823281907434, 0.9920212765957447, 0.9973924380704041, 0.9921052631578947, 0.9908735332464146, 0.9877216916780355, 0.9946308724832215, 0.989821882951654, 0.9898862199747156, 0.993581514762516, 0.9830287206266318, 0.9922680412371134, 0.9921259842519685, 0.9935483870967742, 0.986284289276808, 0.9937027707808564, 0.9923076923076923, 0.9892344497607656, 0.9927360774818402, 0.9928825622775801, 0.9915254237288136, 0.9911727616645649, 0.9937578027465668, 0.9841656516443362, 0.9880810488676997, 0.9831730769230769, 0.9887218045112782, 0.9886506935687264, 0.99125, 0.9893364928909952, 0.9892344497607656, 0.9876695437731196, 0.9891304347826086, 0.9905771495877503, 0.9880810488676997, 0.9825378346915018, 0.9941176470588236, 0.9891826923076923, 0.9891435464414958, 0.9871043376318874, 0.9907834101382489, 0.9893992932862191, 0.9929988331388565, 0.9952996474735605, 0.9953434225844005, 0.9896551724137931, 0.9886621315192744, 0.9871794871794872, 0.9890909090909091, 0.992, 0.9855072463768116, 0.9888268156424581, 0.9929660023446659, 0.9894736842105263, 0.990909090909091, 0.9909604519774011, 0.9930555555555556, 0.9954022988505747, 0.9908571428571429, 0.9943883277216611, 0.9888888888888889, 0.9932810750279956, 0.9914984059511158, 0.9931113662456946, 0.9899328859060402, 0.9886877828054299, 0.9924160346695557, 0.9944506104328524, 0.9920273348519362, 0.9910614525139665, 0.9966777408637874, 0.9869565217391304, 0.9859762675296656, 0.986096256684492, 0.9924487594390508, 0.9911012235817576, 0.9893162393162394, 0.9889867841409692, 0.9933481152993349, 0.9911991199119912, 0.9840255591054313, 0.9934711643090316, 0.9913606911447084, 0.9924324324324324]
after delete: len(clean_dataset) 925
after delete: len(noisy_dataset) 2211
epoch [100/200] batch [5/28] time 0.672 (0.525) data 0.542 (0.394) loss_x loss_x 1.4141 (1.2734) acc_x 59.3750 (66.8750) lr 1.0314e-03 eta 0:00:12
epoch [100/200] batch [10/28] time 0.393 (0.504) data 0.262 (0.374) loss_x loss_x 1.3408 (1.1440) acc_x 68.7500 (70.9375) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [15/28] time 0.480 (0.488) data 0.349 (0.357) loss_x loss_x 0.9204 (1.2346) acc_x 81.2500 (71.0417) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [20/28] time 0.637 (0.492) data 0.506 (0.361) loss_x loss_x 1.1084 (1.1930) acc_x 75.0000 (71.5625) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [25/28] time 0.426 (0.493) data 0.296 (0.362) loss_x loss_x 1.5244 (1.1605) acc_x 68.7500 (71.7500) lr 1.0314e-03 eta 0:00:01
epoch [100/200] batch [5/69] time 0.516 (0.488) data 0.384 (0.357) loss_u loss_u 0.8745 (0.9181) acc_u 12.5000 (9.3750) lr 1.0314e-03 eta 0:00:31
epoch [100/200] batch [10/69] time 0.438 (0.492) data 0.307 (0.361) loss_u loss_u 0.8730 (0.9054) acc_u 18.7500 (11.2500) lr 1.0314e-03 eta 0:00:29
epoch [100/200] batch [15/69] time 0.365 (0.481) data 0.234 (0.350) loss_u loss_u 0.8774 (0.9032) acc_u 18.7500 (12.0833) lr 1.0314e-03 eta 0:00:25
epoch [100/200] batch [20/69] time 0.492 (0.478) data 0.362 (0.348) loss_u loss_u 0.8784 (0.9068) acc_u 12.5000 (11.0938) lr 1.0314e-03 eta 0:00:23
epoch [100/200] batch [25/69] time 0.563 (0.478) data 0.433 (0.347) loss_u loss_u 0.9697 (0.9129) acc_u 3.1250 (10.6250) lr 1.0314e-03 eta 0:00:21
epoch [100/200] batch [30/69] time 0.550 (0.481) data 0.415 (0.350) loss_u loss_u 0.9434 (0.9179) acc_u 6.2500 (10.0000) lr 1.0314e-03 eta 0:00:18
epoch [100/200] batch [35/69] time 0.476 (0.485) data 0.346 (0.354) loss_u loss_u 0.8794 (0.9178) acc_u 12.5000 (9.9107) lr 1.0314e-03 eta 0:00:16
epoch [100/200] batch [40/69] time 0.442 (0.483) data 0.310 (0.352) loss_u loss_u 0.9585 (0.9197) acc_u 6.2500 (9.7656) lr 1.0314e-03 eta 0:00:14
epoch [100/200] batch [45/69] time 0.429 (0.481) data 0.298 (0.350) loss_u loss_u 0.9478 (0.9200) acc_u 6.2500 (9.6528) lr 1.0314e-03 eta 0:00:11
epoch [100/200] batch [50/69] time 0.537 (0.479) data 0.406 (0.348) loss_u loss_u 0.9619 (0.9182) acc_u 6.2500 (10.0000) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [55/69] time 0.502 (0.476) data 0.370 (0.345) loss_u loss_u 0.8569 (0.9176) acc_u 21.8750 (10.2273) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [60/69] time 0.421 (0.476) data 0.289 (0.345) loss_u loss_u 0.8608 (0.9156) acc_u 18.7500 (10.5208) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [65/69] time 0.470 (0.477) data 0.339 (0.346) loss_u loss_u 0.9702 (0.9180) acc_u 3.1250 (10.2885) lr 1.0314e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1455
confident_label rate tensor(0.3023, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 948
clean true:939
clean false:9
clean_rate:0.990506329113924
noisy true:742
noisy false:1446
after delete: len(clean_dataset) 948
after delete: len(noisy_dataset) 2188
epoch [101/200] batch [5/29] time 0.542 (0.493) data 0.411 (0.362) loss_x loss_x 1.0166 (1.1165) acc_x 75.0000 (71.2500) lr 1.0157e-03 eta 0:00:11
epoch [101/200] batch [10/29] time 0.545 (0.466) data 0.414 (0.336) loss_x loss_x 1.6328 (1.1737) acc_x 62.5000 (71.2500) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [15/29] time 0.648 (0.480) data 0.517 (0.350) loss_x loss_x 0.9102 (1.1645) acc_x 78.1250 (71.4583) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [20/29] time 0.400 (0.480) data 0.270 (0.349) loss_x loss_x 1.0713 (1.1179) acc_x 68.7500 (71.4062) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [25/29] time 0.493 (0.469) data 0.363 (0.338) loss_x loss_x 1.0752 (1.1574) acc_x 68.7500 (70.0000) lr 1.0157e-03 eta 0:00:01
epoch [101/200] batch [5/68] time 0.466 (0.458) data 0.334 (0.327) loss_u loss_u 0.8740 (0.8775) acc_u 15.6250 (14.3750) lr 1.0157e-03 eta 0:00:28
epoch [101/200] batch [10/68] time 0.479 (0.460) data 0.347 (0.329) loss_u loss_u 0.9199 (0.8868) acc_u 12.5000 (14.0625) lr 1.0157e-03 eta 0:00:26
epoch [101/200] batch [15/68] time 0.387 (0.456) data 0.255 (0.325) loss_u loss_u 0.9312 (0.9004) acc_u 9.3750 (12.5000) lr 1.0157e-03 eta 0:00:24
epoch [101/200] batch [20/68] time 0.503 (0.454) data 0.373 (0.323) loss_u loss_u 0.9385 (0.9080) acc_u 6.2500 (11.5625) lr 1.0157e-03 eta 0:00:21
epoch [101/200] batch [25/68] time 0.679 (0.456) data 0.547 (0.325) loss_u loss_u 0.9033 (0.9093) acc_u 12.5000 (11.3750) lr 1.0157e-03 eta 0:00:19
epoch [101/200] batch [30/68] time 0.391 (0.455) data 0.261 (0.324) loss_u loss_u 0.9277 (0.9121) acc_u 9.3750 (11.1458) lr 1.0157e-03 eta 0:00:17
epoch [101/200] batch [35/68] time 0.385 (0.449) data 0.253 (0.318) loss_u loss_u 0.8965 (0.9119) acc_u 12.5000 (10.8929) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [40/68] time 0.391 (0.450) data 0.259 (0.319) loss_u loss_u 0.8848 (0.9090) acc_u 15.6250 (11.4062) lr 1.0157e-03 eta 0:00:12
epoch [101/200] batch [45/68] time 0.433 (0.452) data 0.301 (0.321) loss_u loss_u 0.8828 (0.9093) acc_u 15.6250 (11.3889) lr 1.0157e-03 eta 0:00:10
epoch [101/200] batch [50/68] time 0.599 (0.455) data 0.467 (0.323) loss_u loss_u 0.9111 (0.9108) acc_u 9.3750 (11.1250) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [55/68] time 0.520 (0.459) data 0.389 (0.328) loss_u loss_u 0.9268 (0.9110) acc_u 12.5000 (11.1932) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [60/68] time 0.485 (0.462) data 0.353 (0.330) loss_u loss_u 0.8442 (0.9094) acc_u 18.7500 (11.1979) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [65/68] time 0.316 (0.459) data 0.185 (0.328) loss_u loss_u 0.9375 (0.9100) acc_u 6.2500 (11.1058) lr 1.0157e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1496
confident_label rate tensor(0.2867, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 899
clean true:893
clean false:6
clean_rate:0.9933259176863182
noisy true:747
noisy false:1490
after delete: len(clean_dataset) 899
after delete: len(noisy_dataset) 2237
epoch [102/200] batch [5/28] time 0.500 (0.536) data 0.369 (0.405) loss_x loss_x 1.2959 (1.1729) acc_x 71.8750 (72.5000) lr 1.0000e-03 eta 0:00:12
epoch [102/200] batch [10/28] time 0.576 (0.542) data 0.445 (0.411) loss_x loss_x 1.1123 (1.1092) acc_x 75.0000 (75.6250) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [15/28] time 0.577 (0.534) data 0.447 (0.403) loss_x loss_x 1.0840 (1.1002) acc_x 75.0000 (75.6250) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [20/28] time 0.425 (0.517) data 0.294 (0.386) loss_x loss_x 0.9878 (1.0971) acc_x 84.3750 (74.8438) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [25/28] time 0.369 (0.494) data 0.238 (0.363) loss_x loss_x 1.1777 (1.1149) acc_x 62.5000 (73.7500) lr 1.0000e-03 eta 0:00:01
epoch [102/200] batch [5/69] time 0.759 (0.513) data 0.627 (0.382) loss_u loss_u 0.9253 (0.9265) acc_u 6.2500 (10.0000) lr 1.0000e-03 eta 0:00:32
epoch [102/200] batch [10/69] time 0.504 (0.504) data 0.373 (0.373) loss_u loss_u 0.9028 (0.9200) acc_u 15.6250 (11.8750) lr 1.0000e-03 eta 0:00:29
epoch [102/200] batch [15/69] time 0.456 (0.498) data 0.326 (0.367) loss_u loss_u 0.7856 (0.9084) acc_u 21.8750 (12.2917) lr 1.0000e-03 eta 0:00:26
epoch [102/200] batch [20/69] time 0.372 (0.491) data 0.242 (0.360) loss_u loss_u 0.9619 (0.9155) acc_u 6.2500 (11.4062) lr 1.0000e-03 eta 0:00:24
epoch [102/200] batch [25/69] time 0.537 (0.490) data 0.406 (0.359) loss_u loss_u 0.9688 (0.9089) acc_u 3.1250 (12.1250) lr 1.0000e-03 eta 0:00:21
epoch [102/200] batch [30/69] time 0.530 (0.497) data 0.398 (0.367) loss_u loss_u 0.8628 (0.9083) acc_u 15.6250 (11.8750) lr 1.0000e-03 eta 0:00:19
epoch [102/200] batch [35/69] time 0.523 (0.494) data 0.390 (0.363) loss_u loss_u 0.9375 (0.9077) acc_u 9.3750 (12.2321) lr 1.0000e-03 eta 0:00:16
epoch [102/200] batch [40/69] time 0.592 (0.494) data 0.461 (0.363) loss_u loss_u 0.8721 (0.9050) acc_u 15.6250 (12.4219) lr 1.0000e-03 eta 0:00:14
epoch [102/200] batch [45/69] time 0.380 (0.491) data 0.249 (0.360) loss_u loss_u 0.8706 (0.9047) acc_u 15.6250 (12.4306) lr 1.0000e-03 eta 0:00:11
epoch [102/200] batch [50/69] time 0.479 (0.493) data 0.346 (0.361) loss_u loss_u 0.9297 (0.9056) acc_u 9.3750 (12.1250) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [55/69] time 0.663 (0.493) data 0.532 (0.361) loss_u loss_u 0.9629 (0.9058) acc_u 6.2500 (12.1023) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [60/69] time 0.541 (0.494) data 0.407 (0.363) loss_u loss_u 0.9463 (0.9051) acc_u 6.2500 (12.1875) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [65/69] time 0.478 (0.493) data 0.348 (0.362) loss_u loss_u 0.9053 (0.9029) acc_u 9.3750 (12.4519) lr 1.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1469
confident_label rate tensor(0.3061, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 960
clean true:952
clean false:8
clean_rate:0.9916666666666667
noisy true:715
noisy false:1461
after delete: len(clean_dataset) 960
after delete: len(noisy_dataset) 2176
epoch [103/200] batch [5/30] time 0.430 (0.444) data 0.299 (0.313) loss_x loss_x 0.9912 (1.3461) acc_x 71.8750 (66.8750) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [10/30] time 0.577 (0.457) data 0.446 (0.327) loss_x loss_x 1.0078 (1.2504) acc_x 78.1250 (69.0625) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [15/30] time 0.436 (0.485) data 0.305 (0.355) loss_x loss_x 1.0869 (1.2675) acc_x 71.8750 (67.7083) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [20/30] time 0.520 (0.495) data 0.388 (0.365) loss_x loss_x 1.1963 (1.2310) acc_x 65.6250 (68.1250) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [25/30] time 0.473 (0.505) data 0.342 (0.375) loss_x loss_x 1.2363 (1.1994) acc_x 75.0000 (69.6250) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [30/30] time 0.367 (0.494) data 0.237 (0.363) loss_x loss_x 1.2900 (1.2156) acc_x 75.0000 (70.0000) lr 9.8429e-04 eta 0:00:00
epoch [103/200] batch [5/68] time 0.565 (0.495) data 0.433 (0.364) loss_u loss_u 0.9229 (0.9117) acc_u 6.2500 (11.2500) lr 9.8429e-04 eta 0:00:31
epoch [103/200] batch [10/68] time 0.468 (0.506) data 0.337 (0.375) loss_u loss_u 0.9453 (0.9045) acc_u 6.2500 (13.1250) lr 9.8429e-04 eta 0:00:29
epoch [103/200] batch [15/68] time 0.500 (0.502) data 0.368 (0.371) loss_u loss_u 0.9331 (0.9117) acc_u 9.3750 (11.8750) lr 9.8429e-04 eta 0:00:26
epoch [103/200] batch [20/68] time 0.527 (0.496) data 0.396 (0.365) loss_u loss_u 0.9189 (0.9161) acc_u 9.3750 (11.0938) lr 9.8429e-04 eta 0:00:23
epoch [103/200] batch [25/68] time 0.508 (0.500) data 0.377 (0.369) loss_u loss_u 0.9092 (0.9129) acc_u 15.6250 (11.5000) lr 9.8429e-04 eta 0:00:21
epoch [103/200] batch [30/68] time 0.368 (0.493) data 0.237 (0.362) loss_u loss_u 0.9546 (0.9186) acc_u 3.1250 (10.7292) lr 9.8429e-04 eta 0:00:18
epoch [103/200] batch [35/68] time 0.530 (0.493) data 0.399 (0.362) loss_u loss_u 0.9248 (0.9178) acc_u 12.5000 (10.8929) lr 9.8429e-04 eta 0:00:16
epoch [103/200] batch [40/68] time 0.380 (0.490) data 0.249 (0.359) loss_u loss_u 0.9351 (0.9170) acc_u 6.2500 (10.9375) lr 9.8429e-04 eta 0:00:13
epoch [103/200] batch [45/68] time 0.568 (0.489) data 0.436 (0.358) loss_u loss_u 0.9189 (0.9151) acc_u 9.3750 (10.9028) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [50/68] time 0.469 (0.487) data 0.338 (0.356) loss_u loss_u 0.8926 (0.9160) acc_u 9.3750 (10.5000) lr 9.8429e-04 eta 0:00:08
epoch [103/200] batch [55/68] time 0.401 (0.484) data 0.269 (0.353) loss_u loss_u 0.9634 (0.9165) acc_u 3.1250 (10.3409) lr 9.8429e-04 eta 0:00:06
epoch [103/200] batch [60/68] time 0.477 (0.481) data 0.345 (0.350) loss_u loss_u 0.8662 (0.9152) acc_u 12.5000 (10.4688) lr 9.8429e-04 eta 0:00:03
epoch [103/200] batch [65/68] time 0.465 (0.482) data 0.333 (0.351) loss_u loss_u 0.9507 (0.9151) acc_u 9.3750 (10.6250) lr 9.8429e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1509
confident_label rate tensor(0.2940, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 922
clean true:915
clean false:7
clean_rate:0.9924078091106291
noisy true:712
noisy false:1502
after delete: len(clean_dataset) 922
after delete: len(noisy_dataset) 2214
epoch [104/200] batch [5/28] time 0.451 (0.444) data 0.319 (0.313) loss_x loss_x 1.0156 (1.0835) acc_x 81.2500 (73.7500) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [10/28] time 0.457 (0.467) data 0.326 (0.337) loss_x loss_x 0.9292 (1.1173) acc_x 75.0000 (72.1875) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [15/28] time 0.515 (0.475) data 0.385 (0.344) loss_x loss_x 1.8291 (1.1938) acc_x 56.2500 (71.2500) lr 9.6859e-04 eta 0:00:06
epoch [104/200] batch [20/28] time 0.486 (0.479) data 0.356 (0.348) loss_x loss_x 1.2881 (1.2000) acc_x 68.7500 (71.5625) lr 9.6859e-04 eta 0:00:03
epoch [104/200] batch [25/28] time 0.612 (0.491) data 0.482 (0.361) loss_x loss_x 1.4072 (1.2159) acc_x 68.7500 (70.8750) lr 9.6859e-04 eta 0:00:01
epoch [104/200] batch [5/69] time 0.495 (0.490) data 0.364 (0.360) loss_u loss_u 0.8784 (0.8919) acc_u 15.6250 (17.5000) lr 9.6859e-04 eta 0:00:31
epoch [104/200] batch [10/69] time 0.391 (0.482) data 0.259 (0.351) loss_u loss_u 0.8833 (0.9061) acc_u 15.6250 (14.6875) lr 9.6859e-04 eta 0:00:28
epoch [104/200] batch [15/69] time 0.452 (0.481) data 0.321 (0.350) loss_u loss_u 0.9590 (0.9183) acc_u 6.2500 (12.2917) lr 9.6859e-04 eta 0:00:25
epoch [104/200] batch [20/69] time 0.546 (0.480) data 0.415 (0.349) loss_u loss_u 0.8999 (0.9145) acc_u 9.3750 (11.8750) lr 9.6859e-04 eta 0:00:23
epoch [104/200] batch [25/69] time 0.455 (0.483) data 0.323 (0.352) loss_u loss_u 0.8652 (0.9135) acc_u 18.7500 (11.8750) lr 9.6859e-04 eta 0:00:21
epoch [104/200] batch [30/69] time 0.531 (0.486) data 0.398 (0.354) loss_u loss_u 0.9482 (0.9150) acc_u 6.2500 (11.5625) lr 9.6859e-04 eta 0:00:18
epoch [104/200] batch [35/69] time 0.440 (0.489) data 0.309 (0.357) loss_u loss_u 0.8125 (0.9085) acc_u 28.1250 (12.4107) lr 9.6859e-04 eta 0:00:16
epoch [104/200] batch [40/69] time 0.691 (0.487) data 0.559 (0.355) loss_u loss_u 0.9116 (0.9098) acc_u 15.6250 (12.5000) lr 9.6859e-04 eta 0:00:14
epoch [104/200] batch [45/69] time 0.442 (0.487) data 0.311 (0.356) loss_u loss_u 0.9326 (0.9101) acc_u 9.3750 (12.4306) lr 9.6859e-04 eta 0:00:11
epoch [104/200] batch [50/69] time 0.445 (0.483) data 0.314 (0.351) loss_u loss_u 0.8027 (0.9068) acc_u 25.0000 (12.9375) lr 9.6859e-04 eta 0:00:09
epoch [104/200] batch [55/69] time 0.541 (0.481) data 0.409 (0.350) loss_u loss_u 0.8999 (0.9070) acc_u 12.5000 (12.6136) lr 9.6859e-04 eta 0:00:06
epoch [104/200] batch [60/69] time 0.515 (0.482) data 0.385 (0.351) loss_u loss_u 0.8857 (0.9073) acc_u 18.7500 (12.6562) lr 9.6859e-04 eta 0:00:04
epoch [104/200] batch [65/69] time 0.403 (0.480) data 0.272 (0.349) loss_u loss_u 0.8906 (0.9095) acc_u 15.6250 (12.3558) lr 9.6859e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1542
confident_label rate tensor(0.2864, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 898
clean true:892
clean false:6
clean_rate:0.9933184855233853
noisy true:702
noisy false:1536
after delete: len(clean_dataset) 898
after delete: len(noisy_dataset) 2238
epoch [105/200] batch [5/28] time 0.373 (0.467) data 0.242 (0.335) loss_x loss_x 1.0430 (0.9709) acc_x 84.3750 (79.3750) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [10/28] time 0.462 (0.457) data 0.330 (0.326) loss_x loss_x 0.9863 (0.9449) acc_x 75.0000 (78.7500) lr 9.5289e-04 eta 0:00:08
epoch [105/200] batch [15/28] time 0.539 (0.469) data 0.408 (0.337) loss_x loss_x 1.3926 (1.0774) acc_x 62.5000 (73.9583) lr 9.5289e-04 eta 0:00:06
epoch [105/200] batch [20/28] time 0.581 (0.475) data 0.451 (0.344) loss_x loss_x 0.9292 (1.0458) acc_x 71.8750 (74.3750) lr 9.5289e-04 eta 0:00:03
epoch [105/200] batch [25/28] time 0.493 (0.481) data 0.363 (0.350) loss_x loss_x 1.2617 (1.0595) acc_x 68.7500 (74.6250) lr 9.5289e-04 eta 0:00:01
epoch [105/200] batch [5/69] time 0.578 (0.480) data 0.447 (0.349) loss_u loss_u 0.8594 (0.8961) acc_u 12.5000 (11.8750) lr 9.5289e-04 eta 0:00:30
epoch [105/200] batch [10/69] time 0.470 (0.476) data 0.339 (0.345) loss_u loss_u 0.9336 (0.8901) acc_u 9.3750 (14.0625) lr 9.5289e-04 eta 0:00:28
epoch [105/200] batch [15/69] time 0.370 (0.467) data 0.239 (0.336) loss_u loss_u 0.9082 (0.8920) acc_u 15.6250 (14.5833) lr 9.5289e-04 eta 0:00:25
epoch [105/200] batch [20/69] time 0.466 (0.459) data 0.336 (0.328) loss_u loss_u 0.8892 (0.8864) acc_u 15.6250 (15.3125) lr 9.5289e-04 eta 0:00:22
epoch [105/200] batch [25/69] time 0.473 (0.457) data 0.341 (0.327) loss_u loss_u 0.8799 (0.8911) acc_u 15.6250 (14.5000) lr 9.5289e-04 eta 0:00:20
epoch [105/200] batch [30/69] time 0.352 (0.450) data 0.221 (0.319) loss_u loss_u 0.9199 (0.8910) acc_u 9.3750 (14.5833) lr 9.5289e-04 eta 0:00:17
epoch [105/200] batch [35/69] time 0.556 (0.455) data 0.425 (0.325) loss_u loss_u 0.9014 (0.8981) acc_u 12.5000 (13.3929) lr 9.5289e-04 eta 0:00:15
epoch [105/200] batch [40/69] time 0.299 (0.449) data 0.166 (0.318) loss_u loss_u 0.9077 (0.8957) acc_u 9.3750 (13.5938) lr 9.5289e-04 eta 0:00:13
epoch [105/200] batch [45/69] time 0.640 (0.448) data 0.508 (0.317) loss_u loss_u 0.8638 (0.8970) acc_u 18.7500 (13.4722) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [50/69] time 0.373 (0.449) data 0.242 (0.318) loss_u loss_u 0.8994 (0.8973) acc_u 15.6250 (13.5000) lr 9.5289e-04 eta 0:00:08
epoch [105/200] batch [55/69] time 0.444 (0.452) data 0.312 (0.321) loss_u loss_u 0.8892 (0.8970) acc_u 12.5000 (13.5795) lr 9.5289e-04 eta 0:00:06
epoch [105/200] batch [60/69] time 0.452 (0.452) data 0.321 (0.321) loss_u loss_u 0.9077 (0.8977) acc_u 12.5000 (13.4896) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [65/69] time 0.426 (0.452) data 0.296 (0.321) loss_u loss_u 0.9624 (0.9001) acc_u 6.2500 (13.2692) lr 9.5289e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1466
confident_label rate tensor(0.3039, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 953
clean true:943
clean false:10
clean_rate:0.9895068205666316
noisy true:727
noisy false:1456
after delete: len(clean_dataset) 953
after delete: len(noisy_dataset) 2183
epoch [106/200] batch [5/29] time 0.516 (0.431) data 0.385 (0.300) loss_x loss_x 1.1094 (1.1790) acc_x 71.8750 (70.0000) lr 9.3721e-04 eta 0:00:10
epoch [106/200] batch [10/29] time 0.457 (0.444) data 0.326 (0.313) loss_x loss_x 1.1084 (1.0721) acc_x 71.8750 (71.5625) lr 9.3721e-04 eta 0:00:08
epoch [106/200] batch [15/29] time 0.474 (0.452) data 0.344 (0.322) loss_x loss_x 0.9419 (1.1169) acc_x 78.1250 (71.4583) lr 9.3721e-04 eta 0:00:06
epoch [106/200] batch [20/29] time 0.622 (0.461) data 0.491 (0.331) loss_x loss_x 0.8892 (1.1178) acc_x 65.6250 (71.4062) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [25/29] time 0.452 (0.459) data 0.321 (0.329) loss_x loss_x 0.7734 (1.0931) acc_x 84.3750 (72.2500) lr 9.3721e-04 eta 0:00:01
epoch [106/200] batch [5/68] time 0.452 (0.459) data 0.321 (0.328) loss_u loss_u 0.9238 (0.9225) acc_u 9.3750 (8.1250) lr 9.3721e-04 eta 0:00:28
epoch [106/200] batch [10/68] time 0.514 (0.463) data 0.381 (0.333) loss_u loss_u 0.9302 (0.9124) acc_u 6.2500 (10.0000) lr 9.3721e-04 eta 0:00:26
epoch [106/200] batch [15/68] time 0.556 (0.471) data 0.424 (0.340) loss_u loss_u 0.9761 (0.9205) acc_u 3.1250 (9.1667) lr 9.3721e-04 eta 0:00:24
epoch [106/200] batch [20/68] time 0.450 (0.467) data 0.320 (0.336) loss_u loss_u 0.8638 (0.9152) acc_u 12.5000 (9.8438) lr 9.3721e-04 eta 0:00:22
epoch [106/200] batch [25/68] time 0.399 (0.463) data 0.267 (0.332) loss_u loss_u 0.9976 (0.9189) acc_u 0.0000 (9.2500) lr 9.3721e-04 eta 0:00:19
epoch [106/200] batch [30/68] time 0.363 (0.461) data 0.231 (0.330) loss_u loss_u 0.9067 (0.9193) acc_u 12.5000 (9.3750) lr 9.3721e-04 eta 0:00:17
epoch [106/200] batch [35/68] time 0.492 (0.464) data 0.360 (0.333) loss_u loss_u 0.9277 (0.9217) acc_u 9.3750 (9.2857) lr 9.3721e-04 eta 0:00:15
epoch [106/200] batch [40/68] time 0.400 (0.467) data 0.268 (0.336) loss_u loss_u 0.8389 (0.9171) acc_u 18.7500 (9.9219) lr 9.3721e-04 eta 0:00:13
epoch [106/200] batch [45/68] time 0.452 (0.465) data 0.322 (0.334) loss_u loss_u 0.9541 (0.9158) acc_u 6.2500 (10.2778) lr 9.3721e-04 eta 0:00:10
epoch [106/200] batch [50/68] time 0.558 (0.465) data 0.427 (0.334) loss_u loss_u 0.8652 (0.9125) acc_u 15.6250 (10.6250) lr 9.3721e-04 eta 0:00:08
epoch [106/200] batch [55/68] time 0.462 (0.465) data 0.332 (0.334) loss_u loss_u 0.8955 (0.9136) acc_u 15.6250 (10.5114) lr 9.3721e-04 eta 0:00:06
epoch [106/200] batch [60/68] time 0.435 (0.464) data 0.304 (0.333) loss_u loss_u 0.9204 (0.9102) acc_u 9.3750 (10.9375) lr 9.3721e-04 eta 0:00:03
epoch [106/200] batch [65/68] time 0.441 (0.466) data 0.310 (0.335) loss_u loss_u 0.8999 (0.9106) acc_u 9.3750 (10.9615) lr 9.3721e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1522
confident_label rate tensor(0.2934, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 920
clean true:908
clean false:12
clean_rate:0.9869565217391304
noisy true:706
noisy false:1510
after delete: len(clean_dataset) 920
after delete: len(noisy_dataset) 2216
epoch [107/200] batch [5/28] time 0.473 (0.451) data 0.343 (0.321) loss_x loss_x 0.8491 (1.1706) acc_x 78.1250 (73.1250) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [10/28] time 0.429 (0.470) data 0.298 (0.339) loss_x loss_x 1.1621 (1.2165) acc_x 65.6250 (70.9375) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [15/28] time 0.473 (0.472) data 0.343 (0.341) loss_x loss_x 1.1338 (1.1898) acc_x 68.7500 (71.6667) lr 9.2154e-04 eta 0:00:06
epoch [107/200] batch [20/28] time 0.436 (0.462) data 0.305 (0.332) loss_x loss_x 0.7241 (1.1682) acc_x 78.1250 (72.3438) lr 9.2154e-04 eta 0:00:03
epoch [107/200] batch [25/28] time 0.407 (0.465) data 0.277 (0.335) loss_x loss_x 1.2031 (1.1320) acc_x 68.7500 (73.1250) lr 9.2154e-04 eta 0:00:01
epoch [107/200] batch [5/69] time 0.717 (0.465) data 0.585 (0.335) loss_u loss_u 0.8052 (0.8836) acc_u 31.2500 (16.8750) lr 9.2154e-04 eta 0:00:29
epoch [107/200] batch [10/69] time 0.480 (0.464) data 0.349 (0.333) loss_u loss_u 0.8506 (0.9006) acc_u 18.7500 (13.7500) lr 9.2154e-04 eta 0:00:27
epoch [107/200] batch [15/69] time 0.453 (0.461) data 0.321 (0.330) loss_u loss_u 0.9189 (0.8943) acc_u 9.3750 (14.3750) lr 9.2154e-04 eta 0:00:24
epoch [107/200] batch [20/69] time 0.499 (0.458) data 0.367 (0.327) loss_u loss_u 0.7729 (0.8846) acc_u 28.1250 (14.8438) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [25/69] time 0.442 (0.453) data 0.311 (0.322) loss_u loss_u 0.9209 (0.8860) acc_u 9.3750 (14.7500) lr 9.2154e-04 eta 0:00:19
epoch [107/200] batch [30/69] time 0.416 (0.454) data 0.284 (0.323) loss_u loss_u 0.8750 (0.8911) acc_u 15.6250 (14.1667) lr 9.2154e-04 eta 0:00:17
epoch [107/200] batch [35/69] time 0.403 (0.453) data 0.271 (0.322) loss_u loss_u 0.9199 (0.8963) acc_u 9.3750 (13.5714) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [40/69] time 0.391 (0.450) data 0.259 (0.319) loss_u loss_u 0.9551 (0.8981) acc_u 9.3750 (13.4375) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [45/69] time 0.544 (0.452) data 0.412 (0.321) loss_u loss_u 0.9097 (0.9004) acc_u 9.3750 (12.9861) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [50/69] time 0.371 (0.450) data 0.239 (0.319) loss_u loss_u 0.9219 (0.9019) acc_u 12.5000 (12.8750) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [55/69] time 0.402 (0.450) data 0.270 (0.319) loss_u loss_u 0.9062 (0.9011) acc_u 9.3750 (13.0682) lr 9.2154e-04 eta 0:00:06
epoch [107/200] batch [60/69] time 0.463 (0.449) data 0.331 (0.318) loss_u loss_u 0.8755 (0.9014) acc_u 15.6250 (12.9167) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [65/69] time 0.384 (0.448) data 0.252 (0.316) loss_u loss_u 0.9146 (0.9005) acc_u 15.6250 (13.1731) lr 9.2154e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1482
confident_label rate tensor(0.2905, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 911
clean true:908
clean false:3
clean_rate:0.9967069154774972
noisy true:746
noisy false:1479
after delete: len(clean_dataset) 911
after delete: len(noisy_dataset) 2225
epoch [108/200] batch [5/28] time 0.393 (0.424) data 0.263 (0.293) loss_x loss_x 1.0752 (1.1004) acc_x 71.8750 (72.5000) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [10/28] time 0.461 (0.459) data 0.331 (0.328) loss_x loss_x 0.7520 (1.0529) acc_x 87.5000 (73.7500) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [15/28] time 0.497 (0.474) data 0.366 (0.343) loss_x loss_x 1.2246 (1.0573) acc_x 62.5000 (72.7083) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [20/28] time 0.430 (0.469) data 0.299 (0.338) loss_x loss_x 1.2832 (1.1201) acc_x 71.8750 (72.0312) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [25/28] time 0.417 (0.468) data 0.287 (0.337) loss_x loss_x 0.9355 (1.1224) acc_x 75.0000 (71.2500) lr 9.0589e-04 eta 0:00:01
epoch [108/200] batch [5/69] time 0.871 (0.473) data 0.739 (0.342) loss_u loss_u 0.9727 (0.9313) acc_u 3.1250 (8.1250) lr 9.0589e-04 eta 0:00:30
epoch [108/200] batch [10/69] time 0.653 (0.481) data 0.521 (0.350) loss_u loss_u 0.8701 (0.9104) acc_u 18.7500 (10.0000) lr 9.0589e-04 eta 0:00:28
epoch [108/200] batch [15/69] time 0.411 (0.477) data 0.281 (0.346) loss_u loss_u 0.8579 (0.9046) acc_u 18.7500 (11.2500) lr 9.0589e-04 eta 0:00:25
epoch [108/200] batch [20/69] time 0.388 (0.470) data 0.257 (0.339) loss_u loss_u 0.9175 (0.9066) acc_u 9.3750 (11.7188) lr 9.0589e-04 eta 0:00:23
epoch [108/200] batch [25/69] time 0.437 (0.471) data 0.302 (0.340) loss_u loss_u 0.9053 (0.9081) acc_u 12.5000 (11.8750) lr 9.0589e-04 eta 0:00:20
epoch [108/200] batch [30/69] time 0.441 (0.470) data 0.309 (0.339) loss_u loss_u 0.9468 (0.9089) acc_u 6.2500 (11.7708) lr 9.0589e-04 eta 0:00:18
epoch [108/200] batch [35/69] time 0.350 (0.469) data 0.218 (0.338) loss_u loss_u 0.8823 (0.9081) acc_u 15.6250 (11.9643) lr 9.0589e-04 eta 0:00:15
epoch [108/200] batch [40/69] time 0.465 (0.472) data 0.334 (0.340) loss_u loss_u 0.9126 (0.9071) acc_u 15.6250 (12.1094) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [45/69] time 0.432 (0.476) data 0.301 (0.344) loss_u loss_u 0.9307 (0.9090) acc_u 9.3750 (11.8750) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [50/69] time 0.451 (0.470) data 0.320 (0.339) loss_u loss_u 0.8740 (0.9079) acc_u 21.8750 (12.1250) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [55/69] time 0.434 (0.467) data 0.302 (0.336) loss_u loss_u 0.8345 (0.9061) acc_u 25.0000 (12.5000) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [60/69] time 0.630 (0.469) data 0.498 (0.338) loss_u loss_u 0.9053 (0.9047) acc_u 15.6250 (12.5000) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [65/69] time 0.523 (0.468) data 0.392 (0.337) loss_u loss_u 0.8638 (0.9021) acc_u 15.6250 (12.8365) lr 9.0589e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1482
confident_label rate tensor(0.2994, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 939
clean true:931
clean false:8
clean_rate:0.9914802981895634
noisy true:723
noisy false:1474
after delete: len(clean_dataset) 939
after delete: len(noisy_dataset) 2197
epoch [109/200] batch [5/29] time 0.483 (0.477) data 0.352 (0.346) loss_x loss_x 0.8589 (1.2115) acc_x 68.7500 (69.3750) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [10/29] time 0.459 (0.467) data 0.329 (0.337) loss_x loss_x 1.7920 (1.2308) acc_x 62.5000 (70.0000) lr 8.9027e-04 eta 0:00:08
epoch [109/200] batch [15/29] time 0.456 (0.481) data 0.325 (0.351) loss_x loss_x 1.0928 (1.2291) acc_x 71.8750 (70.4167) lr 8.9027e-04 eta 0:00:06
epoch [109/200] batch [20/29] time 0.545 (0.478) data 0.414 (0.347) loss_x loss_x 1.3418 (1.2128) acc_x 65.6250 (70.6250) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [25/29] time 0.480 (0.470) data 0.349 (0.339) loss_x loss_x 1.3398 (1.2177) acc_x 68.7500 (70.2500) lr 8.9027e-04 eta 0:00:01
epoch [109/200] batch [5/68] time 0.387 (0.468) data 0.256 (0.337) loss_u loss_u 0.9346 (0.9003) acc_u 6.2500 (11.2500) lr 8.9027e-04 eta 0:00:29
epoch [109/200] batch [10/68] time 0.465 (0.460) data 0.334 (0.330) loss_u loss_u 0.8965 (0.8993) acc_u 12.5000 (12.5000) lr 8.9027e-04 eta 0:00:26
epoch [109/200] batch [15/68] time 0.615 (0.466) data 0.485 (0.335) loss_u loss_u 0.9189 (0.9014) acc_u 9.3750 (13.3333) lr 8.9027e-04 eta 0:00:24
epoch [109/200] batch [20/68] time 0.369 (0.467) data 0.239 (0.336) loss_u loss_u 0.8911 (0.8962) acc_u 12.5000 (13.7500) lr 8.9027e-04 eta 0:00:22
epoch [109/200] batch [25/68] time 0.425 (0.470) data 0.295 (0.339) loss_u loss_u 0.9185 (0.8996) acc_u 9.3750 (13.2500) lr 8.9027e-04 eta 0:00:20
epoch [109/200] batch [30/68] time 0.442 (0.475) data 0.310 (0.345) loss_u loss_u 0.9043 (0.9047) acc_u 12.5000 (12.2917) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [35/68] time 0.409 (0.473) data 0.277 (0.342) loss_u loss_u 0.8677 (0.9064) acc_u 18.7500 (11.8750) lr 8.9027e-04 eta 0:00:15
epoch [109/200] batch [40/68] time 0.435 (0.470) data 0.304 (0.339) loss_u loss_u 0.8281 (0.9053) acc_u 25.0000 (12.1875) lr 8.9027e-04 eta 0:00:13
epoch [109/200] batch [45/68] time 0.477 (0.470) data 0.346 (0.339) loss_u loss_u 0.8730 (0.9054) acc_u 15.6250 (12.2917) lr 8.9027e-04 eta 0:00:10
epoch [109/200] batch [50/68] time 0.561 (0.468) data 0.428 (0.337) loss_u loss_u 0.8867 (0.9060) acc_u 12.5000 (12.1250) lr 8.9027e-04 eta 0:00:08
epoch [109/200] batch [55/68] time 0.478 (0.471) data 0.347 (0.340) loss_u loss_u 0.9248 (0.9058) acc_u 9.3750 (12.1591) lr 8.9027e-04 eta 0:00:06
epoch [109/200] batch [60/68] time 0.426 (0.471) data 0.296 (0.340) loss_u loss_u 0.8364 (0.9045) acc_u 18.7500 (12.2917) lr 8.9027e-04 eta 0:00:03
epoch [109/200] batch [65/68] time 0.468 (0.468) data 0.336 (0.337) loss_u loss_u 0.9180 (0.9052) acc_u 9.3750 (12.2115) lr 8.9027e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1480
confident_label rate tensor(0.2895, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 908
clean true:901
clean false:7
clean_rate:0.9922907488986784
noisy true:755
noisy false:1473
after delete: len(clean_dataset) 908
after delete: len(noisy_dataset) 2228
epoch [110/200] batch [5/28] time 0.404 (0.430) data 0.274 (0.299) loss_x loss_x 1.4912 (1.4389) acc_x 62.5000 (66.8750) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [10/28] time 0.539 (0.459) data 0.407 (0.328) loss_x loss_x 1.3203 (1.3363) acc_x 68.7500 (67.1875) lr 8.7467e-04 eta 0:00:08
epoch [110/200] batch [15/28] time 0.706 (0.468) data 0.575 (0.337) loss_x loss_x 0.5234 (1.1994) acc_x 87.5000 (71.2500) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [20/28] time 0.385 (0.457) data 0.254 (0.326) loss_x loss_x 1.0449 (1.1992) acc_x 71.8750 (70.7812) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [25/28] time 0.451 (0.460) data 0.321 (0.329) loss_x loss_x 1.2549 (1.1773) acc_x 71.8750 (71.3750) lr 8.7467e-04 eta 0:00:01
epoch [110/200] batch [5/69] time 0.416 (0.457) data 0.285 (0.326) loss_u loss_u 0.8799 (0.9012) acc_u 18.7500 (10.0000) lr 8.7467e-04 eta 0:00:29
epoch [110/200] batch [10/69] time 0.407 (0.456) data 0.277 (0.325) loss_u loss_u 0.9463 (0.9140) acc_u 6.2500 (10.0000) lr 8.7467e-04 eta 0:00:26
epoch [110/200] batch [15/69] time 0.391 (0.453) data 0.259 (0.322) loss_u loss_u 0.9341 (0.9178) acc_u 9.3750 (9.5833) lr 8.7467e-04 eta 0:00:24
epoch [110/200] batch [20/69] time 0.337 (0.449) data 0.205 (0.318) loss_u loss_u 0.9468 (0.9074) acc_u 3.1250 (11.2500) lr 8.7467e-04 eta 0:00:21
epoch [110/200] batch [25/69] time 0.349 (0.444) data 0.218 (0.313) loss_u loss_u 0.8877 (0.9040) acc_u 15.6250 (11.7500) lr 8.7467e-04 eta 0:00:19
epoch [110/200] batch [30/69] time 0.534 (0.449) data 0.401 (0.318) loss_u loss_u 0.8477 (0.8974) acc_u 18.7500 (12.8125) lr 8.7467e-04 eta 0:00:17
epoch [110/200] batch [35/69] time 0.440 (0.451) data 0.310 (0.320) loss_u loss_u 0.9375 (0.9011) acc_u 6.2500 (12.3214) lr 8.7467e-04 eta 0:00:15
epoch [110/200] batch [40/69] time 0.476 (0.454) data 0.344 (0.323) loss_u loss_u 0.8667 (0.9021) acc_u 18.7500 (12.3438) lr 8.7467e-04 eta 0:00:13
epoch [110/200] batch [45/69] time 0.389 (0.456) data 0.259 (0.325) loss_u loss_u 0.9419 (0.9032) acc_u 6.2500 (12.0833) lr 8.7467e-04 eta 0:00:10
epoch [110/200] batch [50/69] time 0.389 (0.456) data 0.259 (0.325) loss_u loss_u 0.9194 (0.9005) acc_u 9.3750 (12.3750) lr 8.7467e-04 eta 0:00:08
epoch [110/200] batch [55/69] time 0.447 (0.453) data 0.317 (0.322) loss_u loss_u 0.8662 (0.8993) acc_u 15.6250 (12.3864) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [60/69] time 0.476 (0.456) data 0.345 (0.325) loss_u loss_u 0.9351 (0.9012) acc_u 9.3750 (12.1354) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [65/69] time 0.586 (0.458) data 0.454 (0.327) loss_u loss_u 0.9346 (0.9028) acc_u 6.2500 (12.0673) lr 8.7467e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1482
confident_label rate tensor(0.3001, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 941
clean true:932
clean false:9
clean_rate:0.9904357066950054
noisy true:722
noisy false:1473
after delete: len(clean_dataset) 941
after delete: len(noisy_dataset) 2195
epoch [111/200] batch [5/29] time 0.607 (0.500) data 0.476 (0.369) loss_x loss_x 1.4004 (1.1910) acc_x 59.3750 (70.6250) lr 8.5910e-04 eta 0:00:12
epoch [111/200] batch [10/29] time 0.560 (0.517) data 0.429 (0.386) loss_x loss_x 1.3193 (1.2286) acc_x 65.6250 (71.2500) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [15/29] time 0.511 (0.499) data 0.380 (0.368) loss_x loss_x 1.0332 (1.1371) acc_x 75.0000 (73.3333) lr 8.5910e-04 eta 0:00:06
epoch [111/200] batch [20/29] time 0.528 (0.488) data 0.398 (0.357) loss_x loss_x 0.7573 (1.0659) acc_x 81.2500 (74.0625) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [25/29] time 0.483 (0.480) data 0.353 (0.350) loss_x loss_x 1.1934 (1.0704) acc_x 75.0000 (74.3750) lr 8.5910e-04 eta 0:00:01
epoch [111/200] batch [5/68] time 0.578 (0.471) data 0.447 (0.341) loss_u loss_u 0.9385 (0.8903) acc_u 6.2500 (15.0000) lr 8.5910e-04 eta 0:00:29
epoch [111/200] batch [10/68] time 0.449 (0.476) data 0.317 (0.345) loss_u loss_u 0.9404 (0.9136) acc_u 6.2500 (11.2500) lr 8.5910e-04 eta 0:00:27
epoch [111/200] batch [15/68] time 0.504 (0.474) data 0.372 (0.343) loss_u loss_u 0.9907 (0.9225) acc_u 0.0000 (10.0000) lr 8.5910e-04 eta 0:00:25
epoch [111/200] batch [20/68] time 0.385 (0.473) data 0.254 (0.342) loss_u loss_u 0.9653 (0.9192) acc_u 3.1250 (10.1562) lr 8.5910e-04 eta 0:00:22
epoch [111/200] batch [25/68] time 0.430 (0.470) data 0.298 (0.339) loss_u loss_u 0.9858 (0.9199) acc_u 0.0000 (9.8750) lr 8.5910e-04 eta 0:00:20
epoch [111/200] batch [30/68] time 0.484 (0.477) data 0.353 (0.346) loss_u loss_u 0.8662 (0.9175) acc_u 18.7500 (10.4167) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [35/68] time 0.603 (0.478) data 0.472 (0.347) loss_u loss_u 0.9126 (0.9179) acc_u 15.6250 (10.5357) lr 8.5910e-04 eta 0:00:15
epoch [111/200] batch [40/68] time 0.439 (0.476) data 0.307 (0.345) loss_u loss_u 0.8398 (0.9131) acc_u 15.6250 (11.0156) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [45/68] time 0.514 (0.475) data 0.384 (0.343) loss_u loss_u 0.8813 (0.9126) acc_u 12.5000 (10.9028) lr 8.5910e-04 eta 0:00:10
epoch [111/200] batch [50/68] time 0.443 (0.475) data 0.312 (0.344) loss_u loss_u 0.8984 (0.9154) acc_u 18.7500 (10.5625) lr 8.5910e-04 eta 0:00:08
epoch [111/200] batch [55/68] time 0.454 (0.473) data 0.324 (0.342) loss_u loss_u 0.9160 (0.9141) acc_u 9.3750 (10.5682) lr 8.5910e-04 eta 0:00:06
epoch [111/200] batch [60/68] time 0.689 (0.473) data 0.558 (0.342) loss_u loss_u 0.8896 (0.9143) acc_u 15.6250 (10.6250) lr 8.5910e-04 eta 0:00:03
epoch [111/200] batch [65/68] time 0.391 (0.473) data 0.259 (0.342) loss_u loss_u 0.9438 (0.9152) acc_u 9.3750 (10.6250) lr 8.5910e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1507
confident_label rate tensor(0.2969, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 931
clean true:925
clean false:6
clean_rate:0.9935553168635876
noisy true:704
noisy false:1501
after delete: len(clean_dataset) 931
after delete: len(noisy_dataset) 2205
epoch [112/200] batch [5/29] time 0.461 (0.516) data 0.330 (0.385) loss_x loss_x 0.5591 (1.1278) acc_x 84.3750 (73.1250) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [10/29] time 0.627 (0.498) data 0.497 (0.368) loss_x loss_x 0.9756 (1.1611) acc_x 75.0000 (72.1875) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [15/29] time 0.483 (0.487) data 0.352 (0.356) loss_x loss_x 0.8550 (1.1573) acc_x 75.0000 (71.8750) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [20/29] time 0.465 (0.480) data 0.334 (0.349) loss_x loss_x 1.1006 (1.1581) acc_x 62.5000 (71.4062) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [25/29] time 0.479 (0.469) data 0.349 (0.338) loss_x loss_x 1.4551 (1.1881) acc_x 65.6250 (71.1250) lr 8.4357e-04 eta 0:00:01
epoch [112/200] batch [5/68] time 0.397 (0.457) data 0.266 (0.326) loss_u loss_u 0.9351 (0.9139) acc_u 3.1250 (9.3750) lr 8.4357e-04 eta 0:00:28
epoch [112/200] batch [10/68] time 0.493 (0.464) data 0.362 (0.334) loss_u loss_u 0.8965 (0.9050) acc_u 15.6250 (10.6250) lr 8.4357e-04 eta 0:00:26
epoch [112/200] batch [15/68] time 0.371 (0.460) data 0.241 (0.330) loss_u loss_u 0.9702 (0.9088) acc_u 6.2500 (10.8333) lr 8.4357e-04 eta 0:00:24
epoch [112/200] batch [20/68] time 0.512 (0.463) data 0.382 (0.332) loss_u loss_u 0.9248 (0.9160) acc_u 9.3750 (10.4688) lr 8.4357e-04 eta 0:00:22
epoch [112/200] batch [25/68] time 0.469 (0.456) data 0.338 (0.326) loss_u loss_u 0.9219 (0.9154) acc_u 9.3750 (10.6250) lr 8.4357e-04 eta 0:00:19
epoch [112/200] batch [30/68] time 0.375 (0.458) data 0.244 (0.327) loss_u loss_u 0.9341 (0.9131) acc_u 9.3750 (11.0417) lr 8.4357e-04 eta 0:00:17
epoch [112/200] batch [35/68] time 0.410 (0.458) data 0.280 (0.327) loss_u loss_u 0.8579 (0.9126) acc_u 18.7500 (11.2500) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [40/68] time 0.430 (0.458) data 0.299 (0.328) loss_u loss_u 0.8691 (0.9042) acc_u 25.0000 (12.3438) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [45/68] time 0.546 (0.459) data 0.415 (0.328) loss_u loss_u 0.9189 (0.9067) acc_u 12.5000 (12.0833) lr 8.4357e-04 eta 0:00:10
epoch [112/200] batch [50/68] time 0.385 (0.457) data 0.254 (0.326) loss_u loss_u 0.9829 (0.9078) acc_u 3.1250 (11.8125) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [55/68] time 0.390 (0.456) data 0.260 (0.325) loss_u loss_u 0.8755 (0.9082) acc_u 18.7500 (11.8750) lr 8.4357e-04 eta 0:00:05
epoch [112/200] batch [60/68] time 0.429 (0.454) data 0.295 (0.323) loss_u loss_u 0.9395 (0.9093) acc_u 6.2500 (11.6146) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [65/68] time 0.384 (0.450) data 0.253 (0.320) loss_u loss_u 0.8389 (0.9077) acc_u 21.8750 (11.8750) lr 8.4357e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1506
confident_label rate tensor(0.2918, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 915
clean true:908
clean false:7
clean_rate:0.9923497267759562
noisy true:722
noisy false:1499
after delete: len(clean_dataset) 915
after delete: len(noisy_dataset) 2221
epoch [113/200] batch [5/28] time 0.481 (0.445) data 0.351 (0.314) loss_x loss_x 0.7627 (1.0464) acc_x 84.3750 (73.1250) lr 8.2807e-04 eta 0:00:10
epoch [113/200] batch [10/28] time 0.440 (0.487) data 0.310 (0.356) loss_x loss_x 1.3613 (1.0968) acc_x 71.8750 (74.0625) lr 8.2807e-04 eta 0:00:08
epoch [113/200] batch [15/28] time 0.425 (0.495) data 0.293 (0.364) loss_x loss_x 1.0293 (1.0617) acc_x 75.0000 (74.5833) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [20/28] time 0.531 (0.490) data 0.400 (0.359) loss_x loss_x 1.3408 (1.0684) acc_x 75.0000 (74.2188) lr 8.2807e-04 eta 0:00:03
epoch [113/200] batch [25/28] time 0.380 (0.480) data 0.250 (0.350) loss_x loss_x 1.6855 (1.1031) acc_x 56.2500 (72.5000) lr 8.2807e-04 eta 0:00:01
epoch [113/200] batch [5/69] time 0.338 (0.466) data 0.208 (0.336) loss_u loss_u 0.8706 (0.8811) acc_u 15.6250 (14.3750) lr 8.2807e-04 eta 0:00:29
epoch [113/200] batch [10/69] time 0.474 (0.459) data 0.343 (0.328) loss_u loss_u 0.9512 (0.8990) acc_u 9.3750 (13.4375) lr 8.2807e-04 eta 0:00:27
epoch [113/200] batch [15/69] time 0.451 (0.461) data 0.320 (0.330) loss_u loss_u 0.8535 (0.8907) acc_u 18.7500 (14.7917) lr 8.2807e-04 eta 0:00:24
epoch [113/200] batch [20/69] time 0.460 (0.463) data 0.329 (0.332) loss_u loss_u 0.9341 (0.8907) acc_u 12.5000 (14.6875) lr 8.2807e-04 eta 0:00:22
epoch [113/200] batch [25/69] time 0.475 (0.463) data 0.344 (0.332) loss_u loss_u 0.9106 (0.8954) acc_u 12.5000 (14.1250) lr 8.2807e-04 eta 0:00:20
epoch [113/200] batch [30/69] time 0.490 (0.458) data 0.359 (0.327) loss_u loss_u 0.9165 (0.8982) acc_u 9.3750 (13.4375) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [35/69] time 0.515 (0.455) data 0.383 (0.324) loss_u loss_u 0.8667 (0.9009) acc_u 18.7500 (13.1250) lr 8.2807e-04 eta 0:00:15
epoch [113/200] batch [40/69] time 0.452 (0.454) data 0.321 (0.323) loss_u loss_u 0.9492 (0.9044) acc_u 9.3750 (12.9688) lr 8.2807e-04 eta 0:00:13
epoch [113/200] batch [45/69] time 0.385 (0.459) data 0.253 (0.328) loss_u loss_u 0.9468 (0.9044) acc_u 6.2500 (13.0556) lr 8.2807e-04 eta 0:00:11
epoch [113/200] batch [50/69] time 0.506 (0.458) data 0.375 (0.327) loss_u loss_u 0.9321 (0.9071) acc_u 9.3750 (12.6875) lr 8.2807e-04 eta 0:00:08
epoch [113/200] batch [55/69] time 0.636 (0.462) data 0.505 (0.331) loss_u loss_u 0.8950 (0.9079) acc_u 18.7500 (12.5000) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [60/69] time 0.677 (0.467) data 0.546 (0.336) loss_u loss_u 0.8423 (0.9074) acc_u 21.8750 (12.3958) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [65/69] time 0.494 (0.466) data 0.362 (0.335) loss_u loss_u 0.8540 (0.9062) acc_u 21.8750 (12.6923) lr 8.2807e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1480
confident_label rate tensor(0.3033, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 951
clean true:940
clean false:11
clean_rate:0.9884332281808622
noisy true:716
noisy false:1469
after delete: len(clean_dataset) 951
after delete: len(noisy_dataset) 2185
epoch [114/200] batch [5/29] time 0.640 (0.478) data 0.509 (0.347) loss_x loss_x 1.0811 (0.9457) acc_x 75.0000 (76.8750) lr 8.1262e-04 eta 0:00:11
epoch [114/200] batch [10/29] time 0.427 (0.445) data 0.296 (0.314) loss_x loss_x 1.1377 (1.0191) acc_x 81.2500 (73.1250) lr 8.1262e-04 eta 0:00:08
epoch [114/200] batch [15/29] time 0.527 (0.453) data 0.396 (0.323) loss_x loss_x 0.7861 (1.0685) acc_x 84.3750 (73.3333) lr 8.1262e-04 eta 0:00:06
epoch [114/200] batch [20/29] time 0.457 (0.454) data 0.325 (0.323) loss_x loss_x 1.0479 (1.0813) acc_x 68.7500 (72.8125) lr 8.1262e-04 eta 0:00:04
epoch [114/200] batch [25/29] time 0.343 (0.445) data 0.213 (0.314) loss_x loss_x 1.3730 (1.1261) acc_x 62.5000 (71.8750) lr 8.1262e-04 eta 0:00:01
epoch [114/200] batch [5/68] time 0.449 (0.448) data 0.318 (0.317) loss_u loss_u 0.9263 (0.9255) acc_u 15.6250 (10.0000) lr 8.1262e-04 eta 0:00:28
epoch [114/200] batch [10/68] time 0.430 (0.448) data 0.299 (0.317) loss_u loss_u 0.9272 (0.9207) acc_u 9.3750 (10.6250) lr 8.1262e-04 eta 0:00:26
epoch [114/200] batch [15/68] time 0.361 (0.450) data 0.230 (0.319) loss_u loss_u 0.9214 (0.9180) acc_u 9.3750 (10.4167) lr 8.1262e-04 eta 0:00:23
epoch [114/200] batch [20/68] time 0.427 (0.450) data 0.295 (0.318) loss_u loss_u 0.9478 (0.9173) acc_u 9.3750 (10.7812) lr 8.1262e-04 eta 0:00:21
epoch [114/200] batch [25/68] time 0.475 (0.451) data 0.344 (0.320) loss_u loss_u 0.8711 (0.9084) acc_u 15.6250 (11.8750) lr 8.1262e-04 eta 0:00:19
epoch [114/200] batch [30/68] time 0.428 (0.450) data 0.297 (0.318) loss_u loss_u 0.8926 (0.9067) acc_u 12.5000 (11.9792) lr 8.1262e-04 eta 0:00:17
epoch [114/200] batch [35/68] time 0.403 (0.450) data 0.272 (0.319) loss_u loss_u 0.9307 (0.9077) acc_u 6.2500 (11.8750) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [40/68] time 0.511 (0.453) data 0.380 (0.322) loss_u loss_u 0.9263 (0.9027) acc_u 9.3750 (12.5000) lr 8.1262e-04 eta 0:00:12
epoch [114/200] batch [45/68] time 0.389 (0.455) data 0.257 (0.324) loss_u loss_u 0.9263 (0.9058) acc_u 6.2500 (11.8750) lr 8.1262e-04 eta 0:00:10
epoch [114/200] batch [50/68] time 0.414 (0.457) data 0.282 (0.326) loss_u loss_u 0.9370 (0.9063) acc_u 9.3750 (11.7500) lr 8.1262e-04 eta 0:00:08
epoch [114/200] batch [55/68] time 0.497 (0.457) data 0.366 (0.326) loss_u loss_u 0.8887 (0.9073) acc_u 12.5000 (11.5341) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [60/68] time 0.608 (0.464) data 0.478 (0.333) loss_u loss_u 0.9082 (0.9078) acc_u 15.6250 (11.6667) lr 8.1262e-04 eta 0:00:03
epoch [114/200] batch [65/68] time 0.570 (0.464) data 0.440 (0.333) loss_u loss_u 0.8740 (0.9053) acc_u 12.5000 (11.9712) lr 8.1262e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1508
confident_label rate tensor(0.3001, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 941
clean true:929
clean false:12
clean_rate:0.9872476089266737
noisy true:699
noisy false:1496
after delete: len(clean_dataset) 941
after delete: len(noisy_dataset) 2195
epoch [115/200] batch [5/29] time 0.548 (0.495) data 0.418 (0.365) loss_x loss_x 1.2080 (1.1004) acc_x 65.6250 (71.8750) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [10/29] time 0.459 (0.491) data 0.329 (0.360) loss_x loss_x 0.8892 (1.0910) acc_x 87.5000 (75.0000) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [15/29] time 0.513 (0.484) data 0.382 (0.354) loss_x loss_x 1.5762 (1.1233) acc_x 56.2500 (72.9167) lr 7.9721e-04 eta 0:00:06
epoch [115/200] batch [20/29] time 0.597 (0.481) data 0.466 (0.351) loss_x loss_x 0.8906 (1.1524) acc_x 78.1250 (72.0312) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [25/29] time 0.476 (0.482) data 0.345 (0.351) loss_x loss_x 1.3848 (1.1805) acc_x 75.0000 (72.2500) lr 7.9721e-04 eta 0:00:01
epoch [115/200] batch [5/68] time 0.308 (0.472) data 0.178 (0.341) loss_u loss_u 0.9800 (0.8908) acc_u 0.0000 (11.8750) lr 7.9721e-04 eta 0:00:29
epoch [115/200] batch [10/68] time 0.614 (0.478) data 0.484 (0.347) loss_u loss_u 0.8032 (0.8958) acc_u 21.8750 (10.9375) lr 7.9721e-04 eta 0:00:27
epoch [115/200] batch [15/68] time 0.510 (0.479) data 0.379 (0.348) loss_u loss_u 0.9404 (0.9103) acc_u 9.3750 (10.4167) lr 7.9721e-04 eta 0:00:25
epoch [115/200] batch [20/68] time 0.434 (0.476) data 0.304 (0.345) loss_u loss_u 0.8516 (0.9138) acc_u 15.6250 (10.0000) lr 7.9721e-04 eta 0:00:22
epoch [115/200] batch [25/68] time 0.589 (0.481) data 0.459 (0.350) loss_u loss_u 0.9258 (0.9158) acc_u 9.3750 (9.6250) lr 7.9721e-04 eta 0:00:20
epoch [115/200] batch [30/68] time 0.471 (0.480) data 0.341 (0.350) loss_u loss_u 0.9414 (0.9173) acc_u 3.1250 (9.6875) lr 7.9721e-04 eta 0:00:18
epoch [115/200] batch [35/68] time 0.359 (0.482) data 0.228 (0.351) loss_u loss_u 0.9370 (0.9169) acc_u 6.2500 (9.7321) lr 7.9721e-04 eta 0:00:15
epoch [115/200] batch [40/68] time 0.559 (0.480) data 0.428 (0.349) loss_u loss_u 0.9268 (0.9198) acc_u 9.3750 (9.5312) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [45/68] time 0.448 (0.477) data 0.317 (0.347) loss_u loss_u 0.8687 (0.9187) acc_u 18.7500 (9.5833) lr 7.9721e-04 eta 0:00:10
epoch [115/200] batch [50/68] time 0.443 (0.475) data 0.312 (0.344) loss_u loss_u 0.8296 (0.9145) acc_u 18.7500 (10.2500) lr 7.9721e-04 eta 0:00:08
epoch [115/200] batch [55/68] time 0.353 (0.471) data 0.221 (0.340) loss_u loss_u 0.9048 (0.9158) acc_u 12.5000 (10.1136) lr 7.9721e-04 eta 0:00:06
epoch [115/200] batch [60/68] time 0.388 (0.467) data 0.255 (0.336) loss_u loss_u 0.9385 (0.9148) acc_u 6.2500 (10.1562) lr 7.9721e-04 eta 0:00:03
epoch [115/200] batch [65/68] time 0.420 (0.466) data 0.289 (0.335) loss_u loss_u 0.8335 (0.9128) acc_u 28.1250 (10.6250) lr 7.9721e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1467
confident_label rate tensor(0.3026, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 949
clean true:941
clean false:8
clean_rate:0.9915700737618546
noisy true:728
noisy false:1459
after delete: len(clean_dataset) 949
after delete: len(noisy_dataset) 2187
epoch [116/200] batch [5/29] time 0.417 (0.500) data 0.286 (0.369) loss_x loss_x 1.0078 (1.2271) acc_x 68.7500 (73.1250) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [10/29] time 0.439 (0.466) data 0.309 (0.335) loss_x loss_x 0.9849 (1.1130) acc_x 75.0000 (75.0000) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [15/29] time 0.431 (0.476) data 0.300 (0.345) loss_x loss_x 1.0732 (1.1427) acc_x 87.5000 (75.2083) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [20/29] time 0.384 (0.462) data 0.253 (0.332) loss_x loss_x 0.8477 (1.0864) acc_x 78.1250 (75.4688) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [25/29] time 0.489 (0.477) data 0.359 (0.346) loss_x loss_x 1.1729 (1.1032) acc_x 78.1250 (75.2500) lr 7.8186e-04 eta 0:00:01
epoch [116/200] batch [5/68] time 0.381 (0.462) data 0.247 (0.332) loss_u loss_u 0.9009 (0.8745) acc_u 12.5000 (16.2500) lr 7.8186e-04 eta 0:00:29
epoch [116/200] batch [10/68] time 0.352 (0.460) data 0.220 (0.329) loss_u loss_u 0.9448 (0.9061) acc_u 9.3750 (12.5000) lr 7.8186e-04 eta 0:00:26
epoch [116/200] batch [15/68] time 0.544 (0.459) data 0.413 (0.328) loss_u loss_u 0.8530 (0.9047) acc_u 18.7500 (12.5000) lr 7.8186e-04 eta 0:00:24
epoch [116/200] batch [20/68] time 0.508 (0.460) data 0.376 (0.329) loss_u loss_u 0.9033 (0.9105) acc_u 15.6250 (11.8750) lr 7.8186e-04 eta 0:00:22
epoch [116/200] batch [25/68] time 0.410 (0.457) data 0.278 (0.326) loss_u loss_u 0.9819 (0.9145) acc_u 3.1250 (11.2500) lr 7.8186e-04 eta 0:00:19
epoch [116/200] batch [30/68] time 0.479 (0.462) data 0.348 (0.331) loss_u loss_u 0.8784 (0.9152) acc_u 15.6250 (10.9375) lr 7.8186e-04 eta 0:00:17
epoch [116/200] batch [35/68] time 0.546 (0.472) data 0.415 (0.340) loss_u loss_u 0.9360 (0.9166) acc_u 9.3750 (10.8036) lr 7.8186e-04 eta 0:00:15
epoch [116/200] batch [40/68] time 0.395 (0.471) data 0.264 (0.340) loss_u loss_u 0.9160 (0.9174) acc_u 12.5000 (10.6250) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [45/68] time 0.313 (0.468) data 0.183 (0.337) loss_u loss_u 0.8887 (0.9167) acc_u 12.5000 (10.6250) lr 7.8186e-04 eta 0:00:10
epoch [116/200] batch [50/68] time 0.350 (0.466) data 0.219 (0.335) loss_u loss_u 0.9214 (0.9165) acc_u 9.3750 (10.5000) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [55/68] time 0.342 (0.464) data 0.211 (0.333) loss_u loss_u 0.9160 (0.9130) acc_u 9.3750 (10.9659) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [60/68] time 0.538 (0.465) data 0.407 (0.334) loss_u loss_u 0.9551 (0.9119) acc_u 6.2500 (11.3021) lr 7.8186e-04 eta 0:00:03
epoch [116/200] batch [65/68] time 0.338 (0.463) data 0.207 (0.332) loss_u loss_u 0.8799 (0.9115) acc_u 15.6250 (11.1058) lr 7.8186e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1470
confident_label rate tensor(0.3020, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 947
clean true:940
clean false:7
clean_rate:0.9926082365364308
noisy true:726
noisy false:1463
after delete: len(clean_dataset) 947
after delete: len(noisy_dataset) 2189
epoch [117/200] batch [5/29] time 0.488 (0.484) data 0.357 (0.352) loss_x loss_x 1.3467 (1.2419) acc_x 62.5000 (65.0000) lr 7.6655e-04 eta 0:00:11
epoch [117/200] batch [10/29] time 0.634 (0.496) data 0.502 (0.365) loss_x loss_x 1.0889 (1.2041) acc_x 68.7500 (67.8125) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [15/29] time 0.600 (0.508) data 0.468 (0.377) loss_x loss_x 1.1475 (1.2506) acc_x 71.8750 (68.1250) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [20/29] time 0.563 (0.508) data 0.432 (0.377) loss_x loss_x 0.9893 (1.2132) acc_x 78.1250 (69.0625) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [25/29] time 0.422 (0.496) data 0.290 (0.365) loss_x loss_x 1.1016 (1.2002) acc_x 71.8750 (69.2500) lr 7.6655e-04 eta 0:00:01
epoch [117/200] batch [5/68] time 0.493 (0.477) data 0.361 (0.346) loss_u loss_u 0.8457 (0.9093) acc_u 18.7500 (12.5000) lr 7.6655e-04 eta 0:00:30
epoch [117/200] batch [10/68] time 0.460 (0.483) data 0.327 (0.352) loss_u loss_u 0.8647 (0.9114) acc_u 15.6250 (12.1875) lr 7.6655e-04 eta 0:00:28
epoch [117/200] batch [15/68] time 0.409 (0.479) data 0.276 (0.348) loss_u loss_u 0.9429 (0.9108) acc_u 6.2500 (12.2917) lr 7.6655e-04 eta 0:00:25
epoch [117/200] batch [20/68] time 0.579 (0.479) data 0.447 (0.347) loss_u loss_u 0.8999 (0.9115) acc_u 9.3750 (11.0938) lr 7.6655e-04 eta 0:00:22
epoch [117/200] batch [25/68] time 0.477 (0.479) data 0.346 (0.347) loss_u loss_u 0.9604 (0.9154) acc_u 6.2500 (10.8750) lr 7.6655e-04 eta 0:00:20
epoch [117/200] batch [30/68] time 0.398 (0.474) data 0.267 (0.343) loss_u loss_u 0.9487 (0.9183) acc_u 6.2500 (10.6250) lr 7.6655e-04 eta 0:00:18
epoch [117/200] batch [35/68] time 0.500 (0.472) data 0.369 (0.341) loss_u loss_u 0.9229 (0.9150) acc_u 9.3750 (10.9821) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [40/68] time 0.616 (0.475) data 0.485 (0.344) loss_u loss_u 0.8540 (0.9119) acc_u 15.6250 (11.3281) lr 7.6655e-04 eta 0:00:13
epoch [117/200] batch [45/68] time 0.556 (0.478) data 0.425 (0.346) loss_u loss_u 0.8521 (0.9109) acc_u 21.8750 (11.3889) lr 7.6655e-04 eta 0:00:10
epoch [117/200] batch [50/68] time 0.576 (0.477) data 0.445 (0.345) loss_u loss_u 0.9268 (0.9123) acc_u 12.5000 (11.3125) lr 7.6655e-04 eta 0:00:08
epoch [117/200] batch [55/68] time 0.469 (0.475) data 0.338 (0.344) loss_u loss_u 0.8975 (0.9126) acc_u 12.5000 (11.3636) lr 7.6655e-04 eta 0:00:06
epoch [117/200] batch [60/68] time 0.505 (0.472) data 0.375 (0.341) loss_u loss_u 0.9692 (0.9128) acc_u 3.1250 (11.2500) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [65/68] time 0.454 (0.473) data 0.323 (0.342) loss_u loss_u 0.8472 (0.9131) acc_u 18.7500 (11.1058) lr 7.6655e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1458
confident_label rate tensor(0.3128, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 981
clean true:973
clean false:8
clean_rate:0.9918450560652395
noisy true:705
noisy false:1450
after delete: len(clean_dataset) 981
after delete: len(noisy_dataset) 2155
epoch [118/200] batch [5/30] time 0.409 (0.473) data 0.280 (0.343) loss_x loss_x 0.9385 (1.2002) acc_x 71.8750 (70.6250) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [10/30] time 0.449 (0.461) data 0.319 (0.331) loss_x loss_x 0.7842 (1.1789) acc_x 78.1250 (70.9375) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [15/30] time 0.482 (0.459) data 0.352 (0.329) loss_x loss_x 1.1553 (1.1943) acc_x 68.7500 (71.6667) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [20/30] time 0.383 (0.460) data 0.251 (0.330) loss_x loss_x 1.5771 (1.1997) acc_x 65.6250 (72.1875) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [25/30] time 0.383 (0.455) data 0.252 (0.325) loss_x loss_x 1.0371 (1.1617) acc_x 65.6250 (71.8750) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [30/30] time 0.511 (0.454) data 0.381 (0.324) loss_x loss_x 1.1152 (1.1772) acc_x 78.1250 (71.8750) lr 7.5131e-04 eta 0:00:00
epoch [118/200] batch [5/67] time 0.632 (0.457) data 0.501 (0.326) loss_u loss_u 0.9180 (0.8969) acc_u 12.5000 (13.7500) lr 7.5131e-04 eta 0:00:28
epoch [118/200] batch [10/67] time 0.563 (0.460) data 0.431 (0.329) loss_u loss_u 0.9077 (0.9068) acc_u 12.5000 (13.1250) lr 7.5131e-04 eta 0:00:26
epoch [118/200] batch [15/67] time 0.354 (0.463) data 0.223 (0.332) loss_u loss_u 0.9346 (0.9111) acc_u 9.3750 (12.2917) lr 7.5131e-04 eta 0:00:24
epoch [118/200] batch [20/67] time 0.428 (0.464) data 0.296 (0.333) loss_u loss_u 0.9355 (0.9146) acc_u 9.3750 (11.7188) lr 7.5131e-04 eta 0:00:21
epoch [118/200] batch [25/67] time 0.470 (0.460) data 0.339 (0.329) loss_u loss_u 0.9033 (0.9122) acc_u 15.6250 (11.8750) lr 7.5131e-04 eta 0:00:19
epoch [118/200] batch [30/67] time 0.512 (0.459) data 0.381 (0.327) loss_u loss_u 0.9224 (0.9155) acc_u 9.3750 (11.3542) lr 7.5131e-04 eta 0:00:16
epoch [118/200] batch [35/67] time 0.570 (0.457) data 0.440 (0.326) loss_u loss_u 0.8916 (0.9125) acc_u 15.6250 (11.6071) lr 7.5131e-04 eta 0:00:14
epoch [118/200] batch [40/67] time 0.407 (0.454) data 0.277 (0.324) loss_u loss_u 0.8696 (0.9080) acc_u 15.6250 (12.1875) lr 7.5131e-04 eta 0:00:12
epoch [118/200] batch [45/67] time 0.433 (0.453) data 0.302 (0.322) loss_u loss_u 0.8906 (0.9098) acc_u 21.8750 (11.9444) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [50/67] time 0.376 (0.450) data 0.245 (0.319) loss_u loss_u 0.9111 (0.9127) acc_u 9.3750 (11.3750) lr 7.5131e-04 eta 0:00:07
epoch [118/200] batch [55/67] time 0.446 (0.450) data 0.315 (0.319) loss_u loss_u 0.9395 (0.9122) acc_u 9.3750 (11.3068) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [60/67] time 0.445 (0.449) data 0.315 (0.318) loss_u loss_u 0.9722 (0.9125) acc_u 9.3750 (11.3542) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [65/67] time 0.332 (0.452) data 0.200 (0.321) loss_u loss_u 0.8994 (0.9127) acc_u 12.5000 (11.3462) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1457
confident_label rate tensor(0.3020, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 947
clean true:941
clean false:6
clean_rate:0.9936642027455121
noisy true:738
noisy false:1451
after delete: len(clean_dataset) 947
after delete: len(noisy_dataset) 2189
epoch [119/200] batch [5/29] time 0.390 (0.410) data 0.259 (0.279) loss_x loss_x 1.8799 (1.2289) acc_x 59.3750 (70.0000) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [10/29] time 0.532 (0.457) data 0.401 (0.326) loss_x loss_x 1.5146 (1.2193) acc_x 65.6250 (69.6875) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [15/29] time 0.432 (0.455) data 0.302 (0.324) loss_x loss_x 1.0508 (1.2097) acc_x 75.0000 (69.1667) lr 7.3613e-04 eta 0:00:06
epoch [119/200] batch [20/29] time 0.325 (0.446) data 0.195 (0.315) loss_x loss_x 1.0723 (1.1248) acc_x 78.1250 (70.9375) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [25/29] time 0.466 (0.462) data 0.334 (0.331) loss_x loss_x 0.9434 (1.1036) acc_x 81.2500 (71.7500) lr 7.3613e-04 eta 0:00:01
epoch [119/200] batch [5/68] time 0.352 (0.469) data 0.221 (0.338) loss_u loss_u 0.9253 (0.9065) acc_u 6.2500 (11.2500) lr 7.3613e-04 eta 0:00:29
epoch [119/200] batch [10/68] time 0.453 (0.467) data 0.321 (0.336) loss_u loss_u 0.9224 (0.9039) acc_u 9.3750 (11.8750) lr 7.3613e-04 eta 0:00:27
epoch [119/200] batch [15/68] time 0.449 (0.463) data 0.318 (0.332) loss_u loss_u 0.8857 (0.9004) acc_u 15.6250 (13.1250) lr 7.3613e-04 eta 0:00:24
epoch [119/200] batch [20/68] time 0.482 (0.460) data 0.351 (0.329) loss_u loss_u 0.8169 (0.8945) acc_u 21.8750 (13.5938) lr 7.3613e-04 eta 0:00:22
epoch [119/200] batch [25/68] time 0.460 (0.456) data 0.328 (0.325) loss_u loss_u 0.9297 (0.9034) acc_u 6.2500 (12.2500) lr 7.3613e-04 eta 0:00:19
epoch [119/200] batch [30/68] time 0.592 (0.460) data 0.460 (0.329) loss_u loss_u 0.8442 (0.9011) acc_u 18.7500 (12.3958) lr 7.3613e-04 eta 0:00:17
epoch [119/200] batch [35/68] time 0.379 (0.457) data 0.247 (0.326) loss_u loss_u 0.8813 (0.9016) acc_u 12.5000 (12.5000) lr 7.3613e-04 eta 0:00:15
epoch [119/200] batch [40/68] time 0.416 (0.456) data 0.285 (0.324) loss_u loss_u 0.9478 (0.9033) acc_u 9.3750 (12.3438) lr 7.3613e-04 eta 0:00:12
epoch [119/200] batch [45/68] time 0.411 (0.452) data 0.276 (0.321) loss_u loss_u 0.9653 (0.9072) acc_u 3.1250 (12.0139) lr 7.3613e-04 eta 0:00:10
epoch [119/200] batch [50/68] time 0.403 (0.451) data 0.273 (0.320) loss_u loss_u 0.9648 (0.9084) acc_u 3.1250 (11.6875) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [55/68] time 0.382 (0.456) data 0.251 (0.325) loss_u loss_u 0.8936 (0.9083) acc_u 12.5000 (11.6477) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [60/68] time 0.374 (0.460) data 0.244 (0.329) loss_u loss_u 0.9775 (0.9109) acc_u 3.1250 (11.3542) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [65/68] time 0.355 (0.457) data 0.224 (0.326) loss_u loss_u 0.8809 (0.9099) acc_u 15.6250 (11.6346) lr 7.3613e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1513
confident_label rate tensor(0.3010, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 944
clean true:938
clean false:6
clean_rate:0.9936440677966102
noisy true:685
noisy false:1507
after delete: len(clean_dataset) 944
after delete: len(noisy_dataset) 2192
epoch [120/200] batch [5/29] time 0.495 (0.481) data 0.363 (0.350) loss_x loss_x 0.8462 (0.9823) acc_x 81.2500 (73.7500) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [10/29] time 0.441 (0.482) data 0.311 (0.351) loss_x loss_x 1.4414 (1.0817) acc_x 65.6250 (73.7500) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [15/29] time 0.418 (0.470) data 0.288 (0.340) loss_x loss_x 1.1035 (1.0791) acc_x 71.8750 (73.9583) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [20/29] time 0.468 (0.477) data 0.337 (0.347) loss_x loss_x 1.0527 (1.0694) acc_x 68.7500 (74.0625) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [25/29] time 0.566 (0.501) data 0.433 (0.370) loss_x loss_x 1.2764 (1.0865) acc_x 65.6250 (73.7500) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [5/68] time 0.415 (0.495) data 0.283 (0.364) loss_u loss_u 0.8818 (0.8949) acc_u 12.5000 (15.6250) lr 7.2101e-04 eta 0:00:31
epoch [120/200] batch [10/68] time 0.410 (0.489) data 0.279 (0.358) loss_u loss_u 0.8359 (0.8938) acc_u 15.6250 (14.6875) lr 7.2101e-04 eta 0:00:28
epoch [120/200] batch [15/68] time 0.362 (0.483) data 0.232 (0.351) loss_u loss_u 0.8896 (0.8934) acc_u 21.8750 (15.2083) lr 7.2101e-04 eta 0:00:25
epoch [120/200] batch [20/68] time 0.495 (0.482) data 0.365 (0.350) loss_u loss_u 0.9082 (0.8910) acc_u 12.5000 (15.1562) lr 7.2101e-04 eta 0:00:23
epoch [120/200] batch [25/68] time 0.576 (0.482) data 0.444 (0.351) loss_u loss_u 0.8491 (0.8910) acc_u 25.0000 (15.1250) lr 7.2101e-04 eta 0:00:20
epoch [120/200] batch [30/68] time 0.523 (0.480) data 0.392 (0.349) loss_u loss_u 0.9146 (0.8970) acc_u 9.3750 (14.2708) lr 7.2101e-04 eta 0:00:18
epoch [120/200] batch [35/68] time 0.469 (0.476) data 0.339 (0.345) loss_u loss_u 0.9082 (0.9015) acc_u 12.5000 (13.5714) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [40/68] time 0.326 (0.469) data 0.195 (0.338) loss_u loss_u 0.9556 (0.9027) acc_u 6.2500 (13.1250) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [45/68] time 0.595 (0.471) data 0.464 (0.340) loss_u loss_u 0.8970 (0.9023) acc_u 12.5000 (13.1250) lr 7.2101e-04 eta 0:00:10
epoch [120/200] batch [50/68] time 0.577 (0.476) data 0.446 (0.345) loss_u loss_u 0.8672 (0.9030) acc_u 15.6250 (12.7500) lr 7.2101e-04 eta 0:00:08
epoch [120/200] batch [55/68] time 0.344 (0.472) data 0.213 (0.341) loss_u loss_u 0.8804 (0.9026) acc_u 21.8750 (12.8977) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [60/68] time 0.443 (0.471) data 0.312 (0.340) loss_u loss_u 0.8794 (0.9031) acc_u 12.5000 (12.6562) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [65/68] time 0.400 (0.468) data 0.268 (0.337) loss_u loss_u 0.9258 (0.9057) acc_u 12.5000 (12.5000) lr 7.2101e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1501
confident_label rate tensor(0.2975, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 933
clean true:921
clean false:12
clean_rate:0.9871382636655949
noisy true:714
noisy false:1489
after delete: len(clean_dataset) 933
after delete: len(noisy_dataset) 2203
epoch [121/200] batch [5/29] time 0.475 (0.446) data 0.343 (0.315) loss_x loss_x 0.9868 (1.3251) acc_x 71.8750 (69.3750) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [10/29] time 0.448 (0.478) data 0.318 (0.347) loss_x loss_x 0.9878 (1.1251) acc_x 68.7500 (71.8750) lr 7.0596e-04 eta 0:00:09
epoch [121/200] batch [15/29] time 0.522 (0.485) data 0.392 (0.354) loss_x loss_x 0.7437 (1.0832) acc_x 75.0000 (72.7083) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [20/29] time 0.433 (0.473) data 0.303 (0.342) loss_x loss_x 1.2559 (1.1040) acc_x 65.6250 (72.3438) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [25/29] time 0.471 (0.480) data 0.340 (0.349) loss_x loss_x 1.6035 (1.1393) acc_x 68.7500 (71.6250) lr 7.0596e-04 eta 0:00:01
epoch [121/200] batch [5/68] time 0.508 (0.464) data 0.377 (0.333) loss_u loss_u 0.9331 (0.8853) acc_u 9.3750 (15.0000) lr 7.0596e-04 eta 0:00:29
epoch [121/200] batch [10/68] time 0.484 (0.467) data 0.353 (0.336) loss_u loss_u 0.9697 (0.8995) acc_u 3.1250 (13.4375) lr 7.0596e-04 eta 0:00:27
epoch [121/200] batch [15/68] time 0.424 (0.471) data 0.290 (0.340) loss_u loss_u 0.9072 (0.9056) acc_u 15.6250 (13.1250) lr 7.0596e-04 eta 0:00:24
epoch [121/200] batch [20/68] time 0.411 (0.475) data 0.280 (0.344) loss_u loss_u 0.8877 (0.9073) acc_u 9.3750 (12.3438) lr 7.0596e-04 eta 0:00:22
epoch [121/200] batch [25/68] time 0.510 (0.474) data 0.379 (0.343) loss_u loss_u 0.8379 (0.9056) acc_u 21.8750 (12.3750) lr 7.0596e-04 eta 0:00:20
epoch [121/200] batch [30/68] time 0.436 (0.476) data 0.305 (0.345) loss_u loss_u 0.9756 (0.9115) acc_u 3.1250 (11.5625) lr 7.0596e-04 eta 0:00:18
epoch [121/200] batch [35/68] time 0.451 (0.477) data 0.320 (0.346) loss_u loss_u 0.8633 (0.9059) acc_u 18.7500 (12.2321) lr 7.0596e-04 eta 0:00:15
epoch [121/200] batch [40/68] time 0.596 (0.475) data 0.465 (0.344) loss_u loss_u 0.9146 (0.9058) acc_u 6.2500 (12.0312) lr 7.0596e-04 eta 0:00:13
epoch [121/200] batch [45/68] time 0.472 (0.474) data 0.340 (0.343) loss_u loss_u 0.9277 (0.9065) acc_u 12.5000 (12.0833) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [50/68] time 0.410 (0.470) data 0.279 (0.339) loss_u loss_u 0.8926 (0.9025) acc_u 12.5000 (12.6250) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [55/68] time 0.559 (0.472) data 0.427 (0.340) loss_u loss_u 0.8623 (0.9044) acc_u 18.7500 (12.4432) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [60/68] time 0.431 (0.468) data 0.300 (0.337) loss_u loss_u 0.9214 (0.9049) acc_u 12.5000 (12.2917) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [65/68] time 0.362 (0.464) data 0.231 (0.333) loss_u loss_u 0.9795 (0.9052) acc_u 0.0000 (12.1635) lr 7.0596e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1469
confident_label rate tensor(0.3036, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 952
clean true:945
clean false:7
clean_rate:0.9926470588235294
noisy true:722
noisy false:1462
after delete: len(clean_dataset) 952
after delete: len(noisy_dataset) 2184
epoch [122/200] batch [5/29] time 0.414 (0.445) data 0.284 (0.315) loss_x loss_x 1.0508 (1.1144) acc_x 78.1250 (75.0000) lr 6.9098e-04 eta 0:00:10
epoch [122/200] batch [10/29] time 0.504 (0.458) data 0.374 (0.328) loss_x loss_x 0.9893 (1.0813) acc_x 78.1250 (74.3750) lr 6.9098e-04 eta 0:00:08
epoch [122/200] batch [15/29] time 0.495 (0.461) data 0.365 (0.330) loss_x loss_x 0.7344 (1.0522) acc_x 87.5000 (75.0000) lr 6.9098e-04 eta 0:00:06
epoch [122/200] batch [20/29] time 0.579 (0.486) data 0.449 (0.355) loss_x loss_x 1.1172 (1.0296) acc_x 75.0000 (75.0000) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [25/29] time 0.459 (0.486) data 0.329 (0.356) loss_x loss_x 0.8560 (1.0373) acc_x 78.1250 (74.7500) lr 6.9098e-04 eta 0:00:01
epoch [122/200] batch [5/68] time 0.366 (0.482) data 0.234 (0.351) loss_u loss_u 0.9678 (0.9374) acc_u 6.2500 (9.3750) lr 6.9098e-04 eta 0:00:30
epoch [122/200] batch [10/68] time 0.390 (0.484) data 0.259 (0.353) loss_u loss_u 0.9116 (0.9283) acc_u 9.3750 (9.6875) lr 6.9098e-04 eta 0:00:28
epoch [122/200] batch [15/68] time 0.442 (0.484) data 0.312 (0.353) loss_u loss_u 0.9482 (0.9282) acc_u 6.2500 (9.3750) lr 6.9098e-04 eta 0:00:25
epoch [122/200] batch [20/68] time 0.428 (0.481) data 0.297 (0.351) loss_u loss_u 0.9380 (0.9254) acc_u 6.2500 (9.6875) lr 6.9098e-04 eta 0:00:23
epoch [122/200] batch [25/68] time 0.377 (0.482) data 0.248 (0.351) loss_u loss_u 0.8447 (0.9144) acc_u 25.0000 (11.0000) lr 6.9098e-04 eta 0:00:20
epoch [122/200] batch [30/68] time 0.422 (0.479) data 0.291 (0.348) loss_u loss_u 0.9131 (0.9164) acc_u 9.3750 (10.8333) lr 6.9098e-04 eta 0:00:18
epoch [122/200] batch [35/68] time 0.496 (0.479) data 0.364 (0.349) loss_u loss_u 0.9253 (0.9128) acc_u 9.3750 (11.2500) lr 6.9098e-04 eta 0:00:15
epoch [122/200] batch [40/68] time 0.521 (0.484) data 0.390 (0.353) loss_u loss_u 0.9204 (0.9107) acc_u 12.5000 (11.4062) lr 6.9098e-04 eta 0:00:13
epoch [122/200] batch [45/68] time 0.381 (0.483) data 0.251 (0.352) loss_u loss_u 0.8813 (0.9094) acc_u 21.8750 (12.0139) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [50/68] time 0.393 (0.480) data 0.259 (0.349) loss_u loss_u 0.8921 (0.9083) acc_u 9.3750 (12.2500) lr 6.9098e-04 eta 0:00:08
epoch [122/200] batch [55/68] time 0.404 (0.477) data 0.272 (0.346) loss_u loss_u 0.9395 (0.9070) acc_u 9.3750 (12.4432) lr 6.9098e-04 eta 0:00:06
epoch [122/200] batch [60/68] time 0.483 (0.475) data 0.352 (0.344) loss_u loss_u 0.9185 (0.9073) acc_u 15.6250 (12.5000) lr 6.9098e-04 eta 0:00:03
epoch [122/200] batch [65/68] time 0.371 (0.471) data 0.240 (0.340) loss_u loss_u 0.9185 (0.9087) acc_u 12.5000 (12.3077) lr 6.9098e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1488
confident_label rate tensor(0.3055, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 958
clean true:948
clean false:10
clean_rate:0.9895615866388309
noisy true:700
noisy false:1478
after delete: len(clean_dataset) 958
after delete: len(noisy_dataset) 2178
epoch [123/200] batch [5/29] time 0.466 (0.505) data 0.335 (0.374) loss_x loss_x 0.6050 (0.8531) acc_x 90.6250 (80.6250) lr 6.7608e-04 eta 0:00:12
epoch [123/200] batch [10/29] time 0.432 (0.491) data 0.300 (0.360) loss_x loss_x 0.9307 (0.9972) acc_x 75.0000 (74.3750) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [15/29] time 0.531 (0.486) data 0.400 (0.355) loss_x loss_x 0.8950 (1.0125) acc_x 81.2500 (75.4167) lr 6.7608e-04 eta 0:00:06
epoch [123/200] batch [20/29] time 0.562 (0.513) data 0.431 (0.382) loss_x loss_x 1.1631 (1.0653) acc_x 62.5000 (75.1562) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [25/29] time 0.358 (0.493) data 0.227 (0.362) loss_x loss_x 0.7793 (1.0686) acc_x 84.3750 (75.2500) lr 6.7608e-04 eta 0:00:01
epoch [123/200] batch [5/68] time 0.599 (0.479) data 0.467 (0.348) loss_u loss_u 0.8901 (0.9036) acc_u 18.7500 (12.5000) lr 6.7608e-04 eta 0:00:30
epoch [123/200] batch [10/68] time 0.434 (0.473) data 0.302 (0.341) loss_u loss_u 0.9663 (0.9228) acc_u 3.1250 (10.0000) lr 6.7608e-04 eta 0:00:27
epoch [123/200] batch [15/68] time 0.362 (0.467) data 0.230 (0.336) loss_u loss_u 0.9556 (0.9260) acc_u 6.2500 (9.5833) lr 6.7608e-04 eta 0:00:24
epoch [123/200] batch [20/68] time 0.371 (0.461) data 0.237 (0.329) loss_u loss_u 0.9736 (0.9217) acc_u 3.1250 (9.8438) lr 6.7608e-04 eta 0:00:22
epoch [123/200] batch [25/68] time 0.460 (0.465) data 0.328 (0.334) loss_u loss_u 0.9395 (0.9262) acc_u 9.3750 (9.1250) lr 6.7608e-04 eta 0:00:19
epoch [123/200] batch [30/68] time 0.397 (0.460) data 0.265 (0.329) loss_u loss_u 0.8608 (0.9239) acc_u 18.7500 (9.3750) lr 6.7608e-04 eta 0:00:17
epoch [123/200] batch [35/68] time 0.438 (0.460) data 0.306 (0.329) loss_u loss_u 0.9409 (0.9226) acc_u 6.2500 (9.6429) lr 6.7608e-04 eta 0:00:15
epoch [123/200] batch [40/68] time 0.464 (0.458) data 0.332 (0.327) loss_u loss_u 0.7983 (0.9195) acc_u 25.0000 (9.9219) lr 6.7608e-04 eta 0:00:12
epoch [123/200] batch [45/68] time 0.401 (0.454) data 0.269 (0.323) loss_u loss_u 0.9272 (0.9185) acc_u 6.2500 (10.0694) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [50/68] time 0.527 (0.455) data 0.395 (0.324) loss_u loss_u 0.9116 (0.9183) acc_u 9.3750 (10.0000) lr 6.7608e-04 eta 0:00:08
epoch [123/200] batch [55/68] time 0.377 (0.456) data 0.244 (0.325) loss_u loss_u 0.9136 (0.9179) acc_u 15.6250 (10.2273) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [60/68] time 0.386 (0.457) data 0.254 (0.326) loss_u loss_u 0.8950 (0.9176) acc_u 18.7500 (10.4167) lr 6.7608e-04 eta 0:00:03
epoch [123/200] batch [65/68] time 0.411 (0.454) data 0.280 (0.323) loss_u loss_u 0.8633 (0.9159) acc_u 15.6250 (10.7212) lr 6.7608e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1432
confident_label rate tensor(0.3084, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 967
clean true:960
clean false:7
clean_rate:0.9927611168562565
noisy true:744
noisy false:1425
after delete: len(clean_dataset) 967
after delete: len(noisy_dataset) 2169
epoch [124/200] batch [5/30] time 0.604 (0.483) data 0.473 (0.351) loss_x loss_x 1.6533 (1.1943) acc_x 71.8750 (73.7500) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [10/30] time 0.516 (0.472) data 0.385 (0.341) loss_x loss_x 0.5894 (1.1063) acc_x 81.2500 (74.0625) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [15/30] time 0.453 (0.466) data 0.323 (0.335) loss_x loss_x 1.2832 (1.1331) acc_x 65.6250 (73.3333) lr 6.6126e-04 eta 0:00:06
epoch [124/200] batch [20/30] time 0.456 (0.468) data 0.324 (0.337) loss_x loss_x 1.3564 (1.1615) acc_x 71.8750 (71.8750) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [25/30] time 0.426 (0.464) data 0.295 (0.333) loss_x loss_x 1.0332 (1.1232) acc_x 71.8750 (72.7500) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [30/30] time 0.666 (0.471) data 0.535 (0.340) loss_x loss_x 0.9434 (1.1103) acc_x 75.0000 (73.2292) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/67] time 0.604 (0.474) data 0.473 (0.343) loss_u loss_u 0.8320 (0.8924) acc_u 18.7500 (13.1250) lr 6.6126e-04 eta 0:00:29
epoch [124/200] batch [10/67] time 0.330 (0.469) data 0.200 (0.338) loss_u loss_u 0.9087 (0.8983) acc_u 12.5000 (13.1250) lr 6.6126e-04 eta 0:00:26
epoch [124/200] batch [15/67] time 0.433 (0.467) data 0.302 (0.336) loss_u loss_u 0.9590 (0.8908) acc_u 6.2500 (13.7500) lr 6.6126e-04 eta 0:00:24
epoch [124/200] batch [20/67] time 0.451 (0.471) data 0.320 (0.340) loss_u loss_u 0.9438 (0.8969) acc_u 6.2500 (13.2812) lr 6.6126e-04 eta 0:00:22
epoch [124/200] batch [25/67] time 0.416 (0.469) data 0.285 (0.338) loss_u loss_u 0.9575 (0.8989) acc_u 3.1250 (12.6250) lr 6.6126e-04 eta 0:00:19
epoch [124/200] batch [30/67] time 0.397 (0.464) data 0.266 (0.333) loss_u loss_u 0.8823 (0.9028) acc_u 15.6250 (12.3958) lr 6.6126e-04 eta 0:00:17
epoch [124/200] batch [35/67] time 0.683 (0.464) data 0.553 (0.333) loss_u loss_u 0.9287 (0.9077) acc_u 9.3750 (11.7857) lr 6.6126e-04 eta 0:00:14
epoch [124/200] batch [40/67] time 0.405 (0.466) data 0.275 (0.335) loss_u loss_u 0.9062 (0.9095) acc_u 12.5000 (11.5625) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [45/67] time 0.608 (0.469) data 0.476 (0.337) loss_u loss_u 0.8994 (0.9114) acc_u 12.5000 (11.1806) lr 6.6126e-04 eta 0:00:10
epoch [124/200] batch [50/67] time 0.408 (0.468) data 0.277 (0.337) loss_u loss_u 0.9199 (0.9086) acc_u 9.3750 (11.6250) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [55/67] time 0.366 (0.465) data 0.235 (0.334) loss_u loss_u 0.9180 (0.9070) acc_u 12.5000 (12.0455) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [60/67] time 0.443 (0.461) data 0.313 (0.330) loss_u loss_u 0.9619 (0.9096) acc_u 3.1250 (11.6667) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [65/67] time 0.412 (0.460) data 0.281 (0.329) loss_u loss_u 0.9312 (0.9088) acc_u 12.5000 (11.8269) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1512
confident_label rate tensor(0.2899, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 909
clean true:903
clean false:6
clean_rate:0.9933993399339934
noisy true:721
noisy false:1506
after delete: len(clean_dataset) 909
after delete: len(noisy_dataset) 2227
epoch [125/200] batch [5/28] time 0.405 (0.455) data 0.274 (0.324) loss_x loss_x 1.2363 (1.3307) acc_x 62.5000 (65.6250) lr 6.4653e-04 eta 0:00:10
epoch [125/200] batch [10/28] time 0.506 (0.498) data 0.376 (0.367) loss_x loss_x 1.1279 (1.1767) acc_x 68.7500 (70.0000) lr 6.4653e-04 eta 0:00:08
epoch [125/200] batch [15/28] time 0.409 (0.479) data 0.278 (0.348) loss_x loss_x 1.1719 (1.2077) acc_x 65.6250 (69.3750) lr 6.4653e-04 eta 0:00:06
epoch [125/200] batch [20/28] time 0.435 (0.467) data 0.304 (0.336) loss_x loss_x 0.9546 (1.2136) acc_x 68.7500 (68.9062) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [25/28] time 0.457 (0.463) data 0.326 (0.333) loss_x loss_x 1.2305 (1.2055) acc_x 62.5000 (68.8750) lr 6.4653e-04 eta 0:00:01
epoch [125/200] batch [5/69] time 0.501 (0.470) data 0.370 (0.339) loss_u loss_u 0.9229 (0.8816) acc_u 9.3750 (15.0000) lr 6.4653e-04 eta 0:00:30
epoch [125/200] batch [10/69] time 0.496 (0.463) data 0.364 (0.332) loss_u loss_u 0.8569 (0.8893) acc_u 18.7500 (15.0000) lr 6.4653e-04 eta 0:00:27
epoch [125/200] batch [15/69] time 0.738 (0.466) data 0.606 (0.334) loss_u loss_u 0.8545 (0.8960) acc_u 18.7500 (13.9583) lr 6.4653e-04 eta 0:00:25
epoch [125/200] batch [20/69] time 0.429 (0.467) data 0.298 (0.336) loss_u loss_u 0.9395 (0.8975) acc_u 3.1250 (13.5938) lr 6.4653e-04 eta 0:00:22
epoch [125/200] batch [25/69] time 0.368 (0.464) data 0.237 (0.333) loss_u loss_u 0.8940 (0.9000) acc_u 9.3750 (13.0000) lr 6.4653e-04 eta 0:00:20
epoch [125/200] batch [30/69] time 0.417 (0.462) data 0.285 (0.330) loss_u loss_u 0.8955 (0.8999) acc_u 15.6250 (12.9167) lr 6.4653e-04 eta 0:00:18
epoch [125/200] batch [35/69] time 0.474 (0.461) data 0.342 (0.329) loss_u loss_u 0.9468 (0.9015) acc_u 6.2500 (12.5000) lr 6.4653e-04 eta 0:00:15
epoch [125/200] batch [40/69] time 0.578 (0.462) data 0.447 (0.330) loss_u loss_u 0.9214 (0.9010) acc_u 6.2500 (12.2656) lr 6.4653e-04 eta 0:00:13
epoch [125/200] batch [45/69] time 0.411 (0.466) data 0.279 (0.335) loss_u loss_u 0.8472 (0.8981) acc_u 15.6250 (12.6389) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [50/69] time 0.563 (0.471) data 0.430 (0.339) loss_u loss_u 0.8813 (0.8960) acc_u 15.6250 (12.9375) lr 6.4653e-04 eta 0:00:08
epoch [125/200] batch [55/69] time 0.557 (0.470) data 0.425 (0.339) loss_u loss_u 0.9062 (0.8962) acc_u 15.6250 (13.1818) lr 6.4653e-04 eta 0:00:06
epoch [125/200] batch [60/69] time 0.501 (0.468) data 0.369 (0.337) loss_u loss_u 0.8486 (0.8957) acc_u 15.6250 (13.2812) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [65/69] time 0.389 (0.467) data 0.255 (0.335) loss_u loss_u 0.9478 (0.8960) acc_u 6.2500 (13.2692) lr 6.4653e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1445
confident_label rate tensor(0.3099, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 972
clean true:960
clean false:12
clean_rate:0.9876543209876543
noisy true:731
noisy false:1433
after delete: len(clean_dataset) 972
after delete: len(noisy_dataset) 2164
epoch [126/200] batch [5/30] time 0.520 (0.523) data 0.390 (0.392) loss_x loss_x 0.7051 (1.0513) acc_x 84.3750 (75.6250) lr 6.3188e-04 eta 0:00:13
epoch [126/200] batch [10/30] time 0.445 (0.484) data 0.314 (0.353) loss_x loss_x 1.3721 (1.1471) acc_x 65.6250 (73.4375) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [15/30] time 0.533 (0.479) data 0.402 (0.348) loss_x loss_x 0.9707 (1.0349) acc_x 78.1250 (75.8333) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [20/30] time 0.591 (0.486) data 0.460 (0.355) loss_x loss_x 1.4170 (1.0350) acc_x 65.6250 (76.2500) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [25/30] time 0.471 (0.483) data 0.341 (0.352) loss_x loss_x 0.9116 (1.0488) acc_x 75.0000 (75.6250) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [30/30] time 0.476 (0.477) data 0.345 (0.346) loss_x loss_x 1.3350 (1.0655) acc_x 56.2500 (74.5833) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/67] time 0.454 (0.465) data 0.321 (0.334) loss_u loss_u 0.9463 (0.9229) acc_u 6.2500 (10.0000) lr 6.3188e-04 eta 0:00:28
epoch [126/200] batch [10/67] time 0.542 (0.467) data 0.410 (0.336) loss_u loss_u 0.8560 (0.8996) acc_u 15.6250 (11.8750) lr 6.3188e-04 eta 0:00:26
epoch [126/200] batch [15/67] time 0.567 (0.469) data 0.435 (0.338) loss_u loss_u 0.9097 (0.9136) acc_u 9.3750 (10.4167) lr 6.3188e-04 eta 0:00:24
epoch [126/200] batch [20/67] time 0.593 (0.473) data 0.462 (0.342) loss_u loss_u 0.8872 (0.9168) acc_u 15.6250 (10.4688) lr 6.3188e-04 eta 0:00:22
epoch [126/200] batch [25/67] time 0.498 (0.476) data 0.367 (0.345) loss_u loss_u 0.8936 (0.9211) acc_u 15.6250 (10.0000) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [30/67] time 0.388 (0.472) data 0.258 (0.341) loss_u loss_u 0.9058 (0.9194) acc_u 9.3750 (10.1042) lr 6.3188e-04 eta 0:00:17
epoch [126/200] batch [35/67] time 0.590 (0.471) data 0.460 (0.340) loss_u loss_u 0.9497 (0.9163) acc_u 3.1250 (10.4464) lr 6.3188e-04 eta 0:00:15
epoch [126/200] batch [40/67] time 0.369 (0.468) data 0.237 (0.337) loss_u loss_u 0.8643 (0.9135) acc_u 15.6250 (11.0156) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [45/67] time 0.437 (0.467) data 0.305 (0.336) loss_u loss_u 0.9727 (0.9161) acc_u 3.1250 (10.5556) lr 6.3188e-04 eta 0:00:10
epoch [126/200] batch [50/67] time 0.533 (0.466) data 0.402 (0.335) loss_u loss_u 0.8970 (0.9133) acc_u 12.5000 (11.1250) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [55/67] time 0.400 (0.466) data 0.268 (0.335) loss_u loss_u 0.8521 (0.9124) acc_u 21.8750 (11.1932) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [60/67] time 0.688 (0.468) data 0.556 (0.337) loss_u loss_u 0.9170 (0.9126) acc_u 12.5000 (11.5104) lr 6.3188e-04 eta 0:00:03
epoch [126/200] batch [65/67] time 0.463 (0.466) data 0.331 (0.335) loss_u loss_u 0.9448 (0.9138) acc_u 9.3750 (11.2981) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.3093, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 970
clean true:959
clean false:11
clean_rate:0.988659793814433
noisy true:743
noisy false:1423
after delete: len(clean_dataset) 970
after delete: len(noisy_dataset) 2166
epoch [127/200] batch [5/30] time 0.491 (0.456) data 0.360 (0.325) loss_x loss_x 1.2051 (1.1843) acc_x 75.0000 (73.7500) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [10/30] time 0.429 (0.440) data 0.298 (0.309) loss_x loss_x 1.3682 (1.1229) acc_x 68.7500 (72.1875) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [15/30] time 0.419 (0.433) data 0.287 (0.302) loss_x loss_x 0.8286 (1.1142) acc_x 84.3750 (72.0833) lr 6.1732e-04 eta 0:00:06
epoch [127/200] batch [20/30] time 0.491 (0.444) data 0.360 (0.313) loss_x loss_x 0.7095 (1.0713) acc_x 78.1250 (73.4375) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [25/30] time 0.569 (0.459) data 0.439 (0.329) loss_x loss_x 1.3564 (1.0721) acc_x 65.6250 (74.3750) lr 6.1732e-04 eta 0:00:02
epoch [127/200] batch [30/30] time 0.488 (0.456) data 0.358 (0.326) loss_x loss_x 1.1240 (1.0966) acc_x 62.5000 (72.8125) lr 6.1732e-04 eta 0:00:00
epoch [127/200] batch [5/67] time 0.395 (0.460) data 0.264 (0.329) loss_u loss_u 0.9077 (0.9159) acc_u 12.5000 (10.6250) lr 6.1732e-04 eta 0:00:28
epoch [127/200] batch [10/67] time 0.518 (0.468) data 0.388 (0.337) loss_u loss_u 0.9033 (0.9224) acc_u 12.5000 (9.6875) lr 6.1732e-04 eta 0:00:26
epoch [127/200] batch [15/67] time 0.640 (0.476) data 0.508 (0.345) loss_u loss_u 0.9595 (0.9221) acc_u 3.1250 (9.1667) lr 6.1732e-04 eta 0:00:24
epoch [127/200] batch [20/67] time 0.504 (0.474) data 0.372 (0.343) loss_u loss_u 0.9106 (0.9213) acc_u 12.5000 (9.6875) lr 6.1732e-04 eta 0:00:22
epoch [127/200] batch [25/67] time 0.343 (0.474) data 0.211 (0.343) loss_u loss_u 0.9678 (0.9200) acc_u 3.1250 (9.7500) lr 6.1732e-04 eta 0:00:19
epoch [127/200] batch [30/67] time 0.379 (0.472) data 0.248 (0.341) loss_u loss_u 0.8447 (0.9136) acc_u 21.8750 (10.6250) lr 6.1732e-04 eta 0:00:17
epoch [127/200] batch [35/67] time 0.452 (0.473) data 0.321 (0.342) loss_u loss_u 0.9424 (0.9142) acc_u 9.3750 (10.7143) lr 6.1732e-04 eta 0:00:15
epoch [127/200] batch [40/67] time 0.395 (0.472) data 0.264 (0.341) loss_u loss_u 0.9614 (0.9179) acc_u 6.2500 (10.3125) lr 6.1732e-04 eta 0:00:12
epoch [127/200] batch [45/67] time 0.376 (0.470) data 0.244 (0.339) loss_u loss_u 0.8672 (0.9165) acc_u 15.6250 (10.4167) lr 6.1732e-04 eta 0:00:10
epoch [127/200] batch [50/67] time 0.472 (0.472) data 0.340 (0.341) loss_u loss_u 0.8682 (0.9151) acc_u 18.7500 (10.5625) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [55/67] time 0.393 (0.468) data 0.263 (0.337) loss_u loss_u 0.8950 (0.9145) acc_u 15.6250 (10.7955) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [60/67] time 0.411 (0.466) data 0.280 (0.335) loss_u loss_u 0.9043 (0.9138) acc_u 12.5000 (11.0938) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [65/67] time 0.555 (0.469) data 0.424 (0.338) loss_u loss_u 0.8965 (0.9133) acc_u 15.6250 (11.2019) lr 6.1732e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1514
confident_label rate tensor(0.3058, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 959
clean true:948
clean false:11
clean_rate:0.9885297184567258
noisy true:674
noisy false:1503
after delete: len(clean_dataset) 959
after delete: len(noisy_dataset) 2177
epoch [128/200] batch [5/29] time 0.454 (0.474) data 0.323 (0.343) loss_x loss_x 1.2217 (1.1062) acc_x 68.7500 (75.0000) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [10/29] time 0.558 (0.484) data 0.426 (0.353) loss_x loss_x 0.6401 (1.0605) acc_x 84.3750 (75.6250) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [15/29] time 0.412 (0.475) data 0.282 (0.344) loss_x loss_x 0.7856 (1.0678) acc_x 84.3750 (75.6250) lr 6.0285e-04 eta 0:00:06
epoch [128/200] batch [20/29] time 0.586 (0.484) data 0.455 (0.354) loss_x loss_x 0.9790 (1.1154) acc_x 75.0000 (74.0625) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [25/29] time 0.396 (0.484) data 0.265 (0.353) loss_x loss_x 1.4316 (1.1332) acc_x 68.7500 (74.1250) lr 6.0285e-04 eta 0:00:01
epoch [128/200] batch [5/68] time 0.443 (0.483) data 0.312 (0.352) loss_u loss_u 0.8784 (0.9195) acc_u 12.5000 (11.2500) lr 6.0285e-04 eta 0:00:30
epoch [128/200] batch [10/68] time 0.385 (0.481) data 0.254 (0.350) loss_u loss_u 0.9419 (0.9208) acc_u 9.3750 (10.9375) lr 6.0285e-04 eta 0:00:27
epoch [128/200] batch [15/68] time 0.452 (0.475) data 0.318 (0.344) loss_u loss_u 0.9058 (0.9096) acc_u 9.3750 (11.8750) lr 6.0285e-04 eta 0:00:25
epoch [128/200] batch [20/68] time 0.434 (0.470) data 0.303 (0.339) loss_u loss_u 0.9165 (0.9085) acc_u 15.6250 (12.0312) lr 6.0285e-04 eta 0:00:22
epoch [128/200] batch [25/68] time 0.475 (0.467) data 0.344 (0.336) loss_u loss_u 0.8628 (0.9117) acc_u 15.6250 (11.6250) lr 6.0285e-04 eta 0:00:20
epoch [128/200] batch [30/68] time 0.402 (0.460) data 0.272 (0.329) loss_u loss_u 0.9072 (0.9152) acc_u 9.3750 (11.1458) lr 6.0285e-04 eta 0:00:17
epoch [128/200] batch [35/68] time 0.403 (0.461) data 0.271 (0.330) loss_u loss_u 0.8770 (0.9097) acc_u 18.7500 (11.6964) lr 6.0285e-04 eta 0:00:15
epoch [128/200] batch [40/68] time 0.490 (0.461) data 0.358 (0.329) loss_u loss_u 0.8994 (0.9071) acc_u 9.3750 (11.9531) lr 6.0285e-04 eta 0:00:12
epoch [128/200] batch [45/68] time 0.456 (0.461) data 0.324 (0.330) loss_u loss_u 0.9487 (0.9066) acc_u 6.2500 (11.9444) lr 6.0285e-04 eta 0:00:10
epoch [128/200] batch [50/68] time 0.468 (0.460) data 0.336 (0.329) loss_u loss_u 0.8955 (0.9075) acc_u 12.5000 (11.7500) lr 6.0285e-04 eta 0:00:08
epoch [128/200] batch [55/68] time 0.658 (0.462) data 0.527 (0.331) loss_u loss_u 0.8633 (0.9066) acc_u 15.6250 (11.9318) lr 6.0285e-04 eta 0:00:06
epoch [128/200] batch [60/68] time 0.388 (0.460) data 0.257 (0.329) loss_u loss_u 0.9116 (0.9068) acc_u 12.5000 (12.1354) lr 6.0285e-04 eta 0:00:03
epoch [128/200] batch [65/68] time 0.459 (0.463) data 0.328 (0.331) loss_u loss_u 0.9399 (0.9089) acc_u 6.2500 (11.8750) lr 6.0285e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1469
confident_label rate tensor(0.3023, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 948
clean true:936
clean false:12
clean_rate:0.9873417721518988
noisy true:731
noisy false:1457
after delete: len(clean_dataset) 948
after delete: len(noisy_dataset) 2188
epoch [129/200] batch [5/29] time 0.436 (0.494) data 0.305 (0.363) loss_x loss_x 1.1875 (1.2607) acc_x 71.8750 (71.2500) lr 5.8849e-04 eta 0:00:11
epoch [129/200] batch [10/29] time 0.496 (0.493) data 0.365 (0.362) loss_x loss_x 0.9292 (1.1994) acc_x 71.8750 (70.3125) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [15/29] time 0.484 (0.476) data 0.353 (0.345) loss_x loss_x 1.0381 (1.1558) acc_x 75.0000 (72.5000) lr 5.8849e-04 eta 0:00:06
epoch [129/200] batch [20/29] time 0.405 (0.486) data 0.275 (0.355) loss_x loss_x 1.1436 (1.1422) acc_x 71.8750 (72.3438) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [25/29] time 0.428 (0.489) data 0.297 (0.359) loss_x loss_x 1.0791 (1.1262) acc_x 71.8750 (72.1250) lr 5.8849e-04 eta 0:00:01
epoch [129/200] batch [5/68] time 0.506 (0.483) data 0.374 (0.352) loss_u loss_u 0.9253 (0.9195) acc_u 9.3750 (10.6250) lr 5.8849e-04 eta 0:00:30
epoch [129/200] batch [10/68] time 0.457 (0.489) data 0.326 (0.358) loss_u loss_u 0.8696 (0.9062) acc_u 15.6250 (12.1875) lr 5.8849e-04 eta 0:00:28
epoch [129/200] batch [15/68] time 0.361 (0.484) data 0.230 (0.353) loss_u loss_u 0.8750 (0.9038) acc_u 15.6250 (12.5000) lr 5.8849e-04 eta 0:00:25
epoch [129/200] batch [20/68] time 0.421 (0.482) data 0.291 (0.351) loss_u loss_u 0.8291 (0.9036) acc_u 28.1250 (12.9688) lr 5.8849e-04 eta 0:00:23
epoch [129/200] batch [25/68] time 0.527 (0.485) data 0.395 (0.354) loss_u loss_u 0.9272 (0.9004) acc_u 12.5000 (13.1250) lr 5.8849e-04 eta 0:00:20
epoch [129/200] batch [30/68] time 0.504 (0.483) data 0.372 (0.352) loss_u loss_u 0.8853 (0.9019) acc_u 15.6250 (12.7083) lr 5.8849e-04 eta 0:00:18
epoch [129/200] batch [35/68] time 0.358 (0.482) data 0.225 (0.351) loss_u loss_u 0.8804 (0.8953) acc_u 15.6250 (13.4821) lr 5.8849e-04 eta 0:00:15
epoch [129/200] batch [40/68] time 0.558 (0.482) data 0.426 (0.351) loss_u loss_u 0.8647 (0.8950) acc_u 18.7500 (13.5938) lr 5.8849e-04 eta 0:00:13
epoch [129/200] batch [45/68] time 0.486 (0.479) data 0.355 (0.348) loss_u loss_u 0.9131 (0.8999) acc_u 9.3750 (12.9861) lr 5.8849e-04 eta 0:00:11
epoch [129/200] batch [50/68] time 0.348 (0.479) data 0.217 (0.348) loss_u loss_u 0.9668 (0.9043) acc_u 0.0000 (12.3125) lr 5.8849e-04 eta 0:00:08
epoch [129/200] batch [55/68] time 0.374 (0.474) data 0.244 (0.343) loss_u loss_u 0.9346 (0.9048) acc_u 12.5000 (12.2727) lr 5.8849e-04 eta 0:00:06
epoch [129/200] batch [60/68] time 0.412 (0.470) data 0.281 (0.339) loss_u loss_u 0.9404 (0.9062) acc_u 6.2500 (12.0833) lr 5.8849e-04 eta 0:00:03
epoch [129/200] batch [65/68] time 0.373 (0.465) data 0.242 (0.334) loss_u loss_u 0.9023 (0.9066) acc_u 9.3750 (11.9712) lr 5.8849e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.3119, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 978
clean true:967
clean false:11
clean_rate:0.9887525562372188
noisy true:749
noisy false:1409
after delete: len(clean_dataset) 978
after delete: len(noisy_dataset) 2158
epoch [130/200] batch [5/30] time 0.478 (0.489) data 0.347 (0.358) loss_x loss_x 1.3623 (1.2326) acc_x 65.6250 (71.2500) lr 5.7422e-04 eta 0:00:12
epoch [130/200] batch [10/30] time 0.514 (0.489) data 0.384 (0.358) loss_x loss_x 1.4287 (1.1771) acc_x 71.8750 (74.0625) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [15/30] time 0.608 (0.506) data 0.476 (0.375) loss_x loss_x 1.6279 (1.1980) acc_x 59.3750 (72.7083) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [20/30] time 0.458 (0.507) data 0.327 (0.376) loss_x loss_x 1.2578 (1.2027) acc_x 71.8750 (72.0312) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [25/30] time 0.469 (0.492) data 0.339 (0.361) loss_x loss_x 1.4180 (1.1880) acc_x 62.5000 (72.5000) lr 5.7422e-04 eta 0:00:02
epoch [130/200] batch [30/30] time 0.448 (0.487) data 0.317 (0.356) loss_x loss_x 1.1797 (1.1906) acc_x 65.6250 (71.9792) lr 5.7422e-04 eta 0:00:00
epoch [130/200] batch [5/67] time 0.393 (0.481) data 0.262 (0.350) loss_u loss_u 0.8408 (0.8924) acc_u 21.8750 (13.7500) lr 5.7422e-04 eta 0:00:29
epoch [130/200] batch [10/67] time 0.491 (0.476) data 0.360 (0.345) loss_u loss_u 0.9165 (0.9118) acc_u 9.3750 (10.6250) lr 5.7422e-04 eta 0:00:27
epoch [130/200] batch [15/67] time 0.487 (0.474) data 0.356 (0.344) loss_u loss_u 0.8276 (0.8976) acc_u 18.7500 (12.0833) lr 5.7422e-04 eta 0:00:24
epoch [130/200] batch [20/67] time 0.387 (0.477) data 0.255 (0.347) loss_u loss_u 0.8438 (0.8990) acc_u 25.0000 (12.3438) lr 5.7422e-04 eta 0:00:22
epoch [130/200] batch [25/67] time 0.453 (0.480) data 0.321 (0.349) loss_u loss_u 0.9443 (0.9053) acc_u 9.3750 (11.8750) lr 5.7422e-04 eta 0:00:20
epoch [130/200] batch [30/67] time 0.822 (0.486) data 0.690 (0.355) loss_u loss_u 0.9521 (0.9063) acc_u 9.3750 (12.0833) lr 5.7422e-04 eta 0:00:17
epoch [130/200] batch [35/67] time 0.497 (0.482) data 0.367 (0.351) loss_u loss_u 0.8389 (0.9052) acc_u 21.8750 (12.2321) lr 5.7422e-04 eta 0:00:15
epoch [130/200] batch [40/67] time 0.407 (0.478) data 0.276 (0.347) loss_u loss_u 0.8701 (0.9076) acc_u 21.8750 (12.1094) lr 5.7422e-04 eta 0:00:12
epoch [130/200] batch [45/67] time 0.478 (0.475) data 0.345 (0.344) loss_u loss_u 0.9399 (0.9059) acc_u 9.3750 (12.3611) lr 5.7422e-04 eta 0:00:10
epoch [130/200] batch [50/67] time 0.472 (0.473) data 0.340 (0.341) loss_u loss_u 0.8965 (0.9081) acc_u 15.6250 (12.1875) lr 5.7422e-04 eta 0:00:08
epoch [130/200] batch [55/67] time 0.595 (0.474) data 0.462 (0.343) loss_u loss_u 0.9502 (0.9087) acc_u 3.1250 (11.9886) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [60/67] time 0.345 (0.469) data 0.214 (0.338) loss_u loss_u 0.9185 (0.9090) acc_u 15.6250 (11.9792) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [65/67] time 0.365 (0.466) data 0.233 (0.335) loss_u loss_u 0.9302 (0.9115) acc_u 9.3750 (11.5865) lr 5.7422e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1471
confident_label rate tensor(0.3013, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 945
clean true:939
clean false:6
clean_rate:0.9936507936507937
noisy true:726
noisy false:1465
after delete: len(clean_dataset) 945
after delete: len(noisy_dataset) 2191
epoch [131/200] batch [5/29] time 0.486 (0.441) data 0.356 (0.311) loss_x loss_x 1.2168 (1.0480) acc_x 68.7500 (75.6250) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [10/29] time 0.530 (0.440) data 0.400 (0.309) loss_x loss_x 1.1104 (1.0276) acc_x 68.7500 (75.6250) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [15/29] time 0.390 (0.465) data 0.260 (0.334) loss_x loss_x 0.8208 (1.0779) acc_x 78.1250 (74.1667) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [20/29] time 0.525 (0.487) data 0.394 (0.356) loss_x loss_x 0.8667 (1.0941) acc_x 78.1250 (73.7500) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [25/29] time 0.498 (0.487) data 0.366 (0.356) loss_x loss_x 1.3428 (1.0975) acc_x 71.8750 (73.7500) lr 5.6006e-04 eta 0:00:01
epoch [131/200] batch [5/68] time 0.502 (0.496) data 0.372 (0.365) loss_u loss_u 0.9282 (0.9126) acc_u 9.3750 (10.6250) lr 5.6006e-04 eta 0:00:31
epoch [131/200] batch [10/68] time 0.483 (0.495) data 0.351 (0.365) loss_u loss_u 0.8618 (0.8932) acc_u 18.7500 (14.0625) lr 5.6006e-04 eta 0:00:28
epoch [131/200] batch [15/68] time 0.407 (0.486) data 0.276 (0.355) loss_u loss_u 0.9126 (0.8977) acc_u 12.5000 (13.5417) lr 5.6006e-04 eta 0:00:25
epoch [131/200] batch [20/68] time 0.358 (0.484) data 0.228 (0.353) loss_u loss_u 0.8970 (0.8982) acc_u 12.5000 (13.7500) lr 5.6006e-04 eta 0:00:23
epoch [131/200] batch [25/68] time 0.482 (0.482) data 0.350 (0.351) loss_u loss_u 0.8770 (0.8986) acc_u 15.6250 (13.5000) lr 5.6006e-04 eta 0:00:20
epoch [131/200] batch [30/68] time 0.379 (0.478) data 0.248 (0.347) loss_u loss_u 0.9521 (0.9049) acc_u 3.1250 (12.3958) lr 5.6006e-04 eta 0:00:18
epoch [131/200] batch [35/68] time 0.445 (0.475) data 0.314 (0.343) loss_u loss_u 0.8711 (0.9053) acc_u 18.7500 (12.6786) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [40/68] time 0.414 (0.472) data 0.283 (0.341) loss_u loss_u 0.8726 (0.9055) acc_u 18.7500 (12.5781) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [45/68] time 0.659 (0.473) data 0.529 (0.342) loss_u loss_u 0.9072 (0.9076) acc_u 15.6250 (12.0833) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [50/68] time 0.348 (0.469) data 0.217 (0.338) loss_u loss_u 0.8486 (0.9079) acc_u 21.8750 (11.9375) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [55/68] time 0.585 (0.472) data 0.455 (0.341) loss_u loss_u 0.9219 (0.9096) acc_u 6.2500 (11.6477) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [60/68] time 0.464 (0.469) data 0.332 (0.338) loss_u loss_u 0.8730 (0.9079) acc_u 15.6250 (11.8750) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [65/68] time 0.462 (0.469) data 0.331 (0.338) loss_u loss_u 0.8931 (0.9084) acc_u 12.5000 (11.7788) lr 5.6006e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1451
confident_label rate tensor(0.3074, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 964
clean true:952
clean false:12
clean_rate:0.9875518672199171
noisy true:733
noisy false:1439
after delete: len(clean_dataset) 964
after delete: len(noisy_dataset) 2172
epoch [132/200] batch [5/30] time 0.413 (0.435) data 0.282 (0.304) loss_x loss_x 0.7896 (1.1628) acc_x 84.3750 (70.6250) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [10/30] time 0.581 (0.466) data 0.451 (0.335) loss_x loss_x 1.3555 (1.0950) acc_x 71.8750 (73.7500) lr 5.4601e-04 eta 0:00:09
epoch [132/200] batch [15/30] time 0.523 (0.513) data 0.393 (0.383) loss_x loss_x 1.0547 (1.1864) acc_x 81.2500 (72.0833) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [20/30] time 0.405 (0.488) data 0.275 (0.357) loss_x loss_x 1.5420 (1.2575) acc_x 71.8750 (71.4062) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [25/30] time 0.493 (0.489) data 0.362 (0.359) loss_x loss_x 1.2422 (1.2267) acc_x 78.1250 (71.7500) lr 5.4601e-04 eta 0:00:02
epoch [132/200] batch [30/30] time 0.393 (0.476) data 0.262 (0.345) loss_x loss_x 1.2881 (1.2289) acc_x 75.0000 (71.1458) lr 5.4601e-04 eta 0:00:00
epoch [132/200] batch [5/67] time 0.745 (0.478) data 0.615 (0.348) loss_u loss_u 0.8853 (0.9237) acc_u 15.6250 (11.2500) lr 5.4601e-04 eta 0:00:29
epoch [132/200] batch [10/67] time 0.374 (0.476) data 0.243 (0.346) loss_u loss_u 0.8682 (0.9107) acc_u 18.7500 (11.8750) lr 5.4601e-04 eta 0:00:27
epoch [132/200] batch [15/67] time 0.419 (0.471) data 0.287 (0.340) loss_u loss_u 0.8828 (0.9043) acc_u 15.6250 (12.2917) lr 5.4601e-04 eta 0:00:24
epoch [132/200] batch [20/67] time 0.469 (0.465) data 0.338 (0.334) loss_u loss_u 0.9419 (0.9094) acc_u 9.3750 (11.8750) lr 5.4601e-04 eta 0:00:21
epoch [132/200] batch [25/67] time 0.484 (0.460) data 0.353 (0.329) loss_u loss_u 0.9307 (0.9061) acc_u 9.3750 (12.6250) lr 5.4601e-04 eta 0:00:19
epoch [132/200] batch [30/67] time 0.490 (0.459) data 0.360 (0.329) loss_u loss_u 0.8999 (0.9069) acc_u 15.6250 (12.2917) lr 5.4601e-04 eta 0:00:16
epoch [132/200] batch [35/67] time 0.475 (0.459) data 0.345 (0.328) loss_u loss_u 0.8140 (0.9073) acc_u 25.0000 (12.4107) lr 5.4601e-04 eta 0:00:14
epoch [132/200] batch [40/67] time 0.416 (0.459) data 0.285 (0.329) loss_u loss_u 0.8784 (0.9065) acc_u 15.6250 (12.3438) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [45/67] time 0.439 (0.458) data 0.308 (0.327) loss_u loss_u 0.8823 (0.9087) acc_u 18.7500 (12.0833) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [50/67] time 0.434 (0.454) data 0.304 (0.324) loss_u loss_u 0.9287 (0.9081) acc_u 6.2500 (12.0000) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [55/67] time 0.452 (0.454) data 0.321 (0.323) loss_u loss_u 0.8726 (0.9067) acc_u 21.8750 (12.2159) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [60/67] time 0.524 (0.457) data 0.392 (0.326) loss_u loss_u 0.9585 (0.9056) acc_u 3.1250 (12.1875) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [65/67] time 0.547 (0.458) data 0.416 (0.327) loss_u loss_u 0.9136 (0.9044) acc_u 12.5000 (12.3558) lr 5.4601e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1400
confident_label rate tensor(0.3166, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 993
clean true:981
clean false:12
clean_rate:0.9879154078549849
noisy true:755
noisy false:1388
after delete: len(clean_dataset) 993
after delete: len(noisy_dataset) 2143
epoch [133/200] batch [5/31] time 0.489 (0.443) data 0.358 (0.312) loss_x loss_x 1.2617 (0.9854) acc_x 68.7500 (75.0000) lr 5.3207e-04 eta 0:00:11
epoch [133/200] batch [10/31] time 0.570 (0.467) data 0.440 (0.336) loss_x loss_x 1.6055 (1.0863) acc_x 56.2500 (73.4375) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [15/31] time 0.466 (0.467) data 0.335 (0.337) loss_x loss_x 1.0762 (1.0939) acc_x 71.8750 (73.1250) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [20/31] time 0.465 (0.458) data 0.333 (0.327) loss_x loss_x 0.7334 (1.0509) acc_x 84.3750 (73.7500) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [25/31] time 0.514 (0.456) data 0.383 (0.325) loss_x loss_x 1.3320 (1.1278) acc_x 65.6250 (72.0000) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [30/31] time 0.510 (0.466) data 0.380 (0.335) loss_x loss_x 0.9160 (1.1082) acc_x 78.1250 (72.3958) lr 5.3207e-04 eta 0:00:00
epoch [133/200] batch [5/66] time 0.494 (0.473) data 0.362 (0.342) loss_u loss_u 0.8965 (0.9125) acc_u 18.7500 (11.8750) lr 5.3207e-04 eta 0:00:28
epoch [133/200] batch [10/66] time 0.423 (0.472) data 0.291 (0.341) loss_u loss_u 0.9302 (0.9184) acc_u 6.2500 (10.9375) lr 5.3207e-04 eta 0:00:26
epoch [133/200] batch [15/66] time 0.412 (0.471) data 0.282 (0.340) loss_u loss_u 0.9521 (0.9126) acc_u 3.1250 (12.0833) lr 5.3207e-04 eta 0:00:24
epoch [133/200] batch [20/66] time 0.420 (0.475) data 0.289 (0.344) loss_u loss_u 0.8945 (0.9118) acc_u 12.5000 (11.5625) lr 5.3207e-04 eta 0:00:21
epoch [133/200] batch [25/66] time 0.499 (0.474) data 0.368 (0.343) loss_u loss_u 0.8989 (0.9132) acc_u 12.5000 (11.1250) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [30/66] time 0.495 (0.475) data 0.362 (0.344) loss_u loss_u 0.9829 (0.9109) acc_u 0.0000 (11.4583) lr 5.3207e-04 eta 0:00:17
epoch [133/200] batch [35/66] time 0.360 (0.471) data 0.229 (0.340) loss_u loss_u 0.9653 (0.9097) acc_u 3.1250 (11.6071) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [40/66] time 0.426 (0.470) data 0.295 (0.339) loss_u loss_u 0.9575 (0.9139) acc_u 3.1250 (10.9375) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [45/66] time 0.437 (0.472) data 0.306 (0.341) loss_u loss_u 0.9282 (0.9109) acc_u 9.3750 (11.3889) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [50/66] time 0.316 (0.472) data 0.185 (0.341) loss_u loss_u 0.8984 (0.9104) acc_u 12.5000 (11.4375) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [55/66] time 0.367 (0.470) data 0.234 (0.339) loss_u loss_u 0.9248 (0.9120) acc_u 9.3750 (11.3068) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [60/66] time 0.516 (0.470) data 0.384 (0.339) loss_u loss_u 0.9399 (0.9144) acc_u 6.2500 (10.9375) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [65/66] time 0.480 (0.468) data 0.349 (0.337) loss_u loss_u 0.8740 (0.9160) acc_u 12.5000 (10.6250) lr 5.3207e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1465
confident_label rate tensor(0.3119, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 978
clean true:964
clean false:14
clean_rate:0.9856850715746421
noisy true:707
noisy false:1451
after delete: len(clean_dataset) 978
after delete: len(noisy_dataset) 2158
epoch [134/200] batch [5/30] time 0.414 (0.481) data 0.283 (0.350) loss_x loss_x 1.2363 (1.4584) acc_x 71.8750 (64.3750) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [10/30] time 0.495 (0.477) data 0.364 (0.346) loss_x loss_x 0.8760 (1.3093) acc_x 78.1250 (67.8125) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [15/30] time 0.576 (0.496) data 0.445 (0.366) loss_x loss_x 1.3086 (1.2751) acc_x 62.5000 (68.7500) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [20/30] time 0.411 (0.479) data 0.281 (0.348) loss_x loss_x 1.2744 (1.2716) acc_x 65.6250 (68.5938) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [25/30] time 0.421 (0.478) data 0.290 (0.347) loss_x loss_x 1.2939 (1.2500) acc_x 59.3750 (69.2500) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [30/30] time 0.386 (0.472) data 0.256 (0.342) loss_x loss_x 0.7612 (1.2296) acc_x 78.1250 (70.1042) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/67] time 0.457 (0.474) data 0.327 (0.343) loss_u loss_u 0.8735 (0.9036) acc_u 15.6250 (13.7500) lr 5.1825e-04 eta 0:00:29
epoch [134/200] batch [10/67] time 0.425 (0.474) data 0.293 (0.343) loss_u loss_u 0.9307 (0.9167) acc_u 12.5000 (12.5000) lr 5.1825e-04 eta 0:00:27
epoch [134/200] batch [15/67] time 0.444 (0.473) data 0.312 (0.342) loss_u loss_u 0.8774 (0.9176) acc_u 15.6250 (11.2500) lr 5.1825e-04 eta 0:00:24
epoch [134/200] batch [20/67] time 0.394 (0.469) data 0.263 (0.338) loss_u loss_u 0.8794 (0.9083) acc_u 15.6250 (12.5000) lr 5.1825e-04 eta 0:00:22
epoch [134/200] batch [25/67] time 0.415 (0.473) data 0.284 (0.343) loss_u loss_u 0.8853 (0.9096) acc_u 18.7500 (12.0000) lr 5.1825e-04 eta 0:00:19
epoch [134/200] batch [30/67] time 0.632 (0.475) data 0.501 (0.344) loss_u loss_u 0.9253 (0.9102) acc_u 15.6250 (11.9792) lr 5.1825e-04 eta 0:00:17
epoch [134/200] batch [35/67] time 0.507 (0.475) data 0.375 (0.344) loss_u loss_u 0.8794 (0.9106) acc_u 15.6250 (12.0536) lr 5.1825e-04 eta 0:00:15
epoch [134/200] batch [40/67] time 0.488 (0.474) data 0.357 (0.343) loss_u loss_u 0.9590 (0.9131) acc_u 3.1250 (11.7969) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [45/67] time 0.400 (0.471) data 0.269 (0.340) loss_u loss_u 0.9053 (0.9118) acc_u 12.5000 (11.9444) lr 5.1825e-04 eta 0:00:10
epoch [134/200] batch [50/67] time 0.537 (0.470) data 0.406 (0.339) loss_u loss_u 0.9106 (0.9131) acc_u 12.5000 (11.8750) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [55/67] time 0.438 (0.467) data 0.305 (0.336) loss_u loss_u 0.9023 (0.9148) acc_u 12.5000 (11.4773) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [60/67] time 0.532 (0.470) data 0.400 (0.339) loss_u loss_u 0.9199 (0.9136) acc_u 9.3750 (11.6146) lr 5.1825e-04 eta 0:00:03
epoch [134/200] batch [65/67] time 0.709 (0.472) data 0.577 (0.341) loss_u loss_u 0.9429 (0.9143) acc_u 6.2500 (11.4904) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1468
confident_label rate tensor(0.3119, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 978
clean true:967
clean false:11
clean_rate:0.9887525562372188
noisy true:701
noisy false:1457
after delete: len(clean_dataset) 978
after delete: len(noisy_dataset) 2158
epoch [135/200] batch [5/30] time 0.424 (0.472) data 0.293 (0.341) loss_x loss_x 1.1807 (1.0760) acc_x 68.7500 (71.2500) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [10/30] time 0.575 (0.467) data 0.443 (0.336) loss_x loss_x 1.3008 (1.0975) acc_x 75.0000 (71.5625) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [15/30] time 0.436 (0.487) data 0.306 (0.356) loss_x loss_x 1.2764 (1.0601) acc_x 71.8750 (74.5833) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [20/30] time 0.486 (0.483) data 0.356 (0.352) loss_x loss_x 0.8862 (1.0262) acc_x 75.0000 (75.1562) lr 5.0454e-04 eta 0:00:04
epoch [135/200] batch [25/30] time 0.499 (0.485) data 0.370 (0.355) loss_x loss_x 0.9072 (1.0430) acc_x 81.2500 (74.8750) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [30/30] time 0.503 (0.487) data 0.373 (0.357) loss_x loss_x 0.7778 (1.0511) acc_x 75.0000 (74.6875) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/67] time 0.352 (0.481) data 0.220 (0.350) loss_u loss_u 0.8887 (0.9208) acc_u 21.8750 (11.2500) lr 5.0454e-04 eta 0:00:29
epoch [135/200] batch [10/67] time 0.358 (0.467) data 0.225 (0.337) loss_u loss_u 0.8960 (0.9110) acc_u 12.5000 (11.2500) lr 5.0454e-04 eta 0:00:26
epoch [135/200] batch [15/67] time 0.456 (0.467) data 0.326 (0.336) loss_u loss_u 0.9473 (0.9157) acc_u 9.3750 (10.8333) lr 5.0454e-04 eta 0:00:24
epoch [135/200] batch [20/67] time 0.385 (0.466) data 0.255 (0.335) loss_u loss_u 0.9351 (0.9105) acc_u 6.2500 (11.0938) lr 5.0454e-04 eta 0:00:21
epoch [135/200] batch [25/67] time 0.401 (0.458) data 0.267 (0.327) loss_u loss_u 0.8413 (0.9102) acc_u 21.8750 (11.5000) lr 5.0454e-04 eta 0:00:19
epoch [135/200] batch [30/67] time 0.470 (0.461) data 0.340 (0.330) loss_u loss_u 0.9243 (0.9131) acc_u 9.3750 (10.9375) lr 5.0454e-04 eta 0:00:17
epoch [135/200] batch [35/67] time 0.447 (0.460) data 0.315 (0.329) loss_u loss_u 0.9248 (0.9152) acc_u 9.3750 (10.6250) lr 5.0454e-04 eta 0:00:14
epoch [135/200] batch [40/67] time 0.505 (0.462) data 0.373 (0.331) loss_u loss_u 0.9019 (0.9096) acc_u 9.3750 (11.2500) lr 5.0454e-04 eta 0:00:12
epoch [135/200] batch [45/67] time 0.377 (0.461) data 0.246 (0.330) loss_u loss_u 0.9224 (0.9096) acc_u 12.5000 (11.3889) lr 5.0454e-04 eta 0:00:10
epoch [135/200] batch [50/67] time 0.483 (0.465) data 0.352 (0.334) loss_u loss_u 0.8198 (0.9093) acc_u 25.0000 (11.5000) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [55/67] time 0.667 (0.467) data 0.536 (0.336) loss_u loss_u 0.9341 (0.9079) acc_u 9.3750 (11.7614) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [60/67] time 0.428 (0.469) data 0.297 (0.338) loss_u loss_u 0.9551 (0.9074) acc_u 6.2500 (11.8750) lr 5.0454e-04 eta 0:00:03
epoch [135/200] batch [65/67] time 0.445 (0.468) data 0.313 (0.337) loss_u loss_u 0.9229 (0.9083) acc_u 9.3750 (11.8269) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1442
confident_label rate tensor(0.3141, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 985
clean true:973
clean false:12
clean_rate:0.9878172588832488
noisy true:721
noisy false:1430
after delete: len(clean_dataset) 985
after delete: len(noisy_dataset) 2151
epoch [136/200] batch [5/30] time 0.432 (0.452) data 0.301 (0.321) loss_x loss_x 0.6416 (0.8876) acc_x 84.3750 (77.5000) lr 4.9096e-04 eta 0:00:11
epoch [136/200] batch [10/30] time 0.541 (0.460) data 0.411 (0.329) loss_x loss_x 1.4346 (0.9799) acc_x 68.7500 (77.5000) lr 4.9096e-04 eta 0:00:09
epoch [136/200] batch [15/30] time 0.422 (0.468) data 0.292 (0.337) loss_x loss_x 1.0547 (0.9718) acc_x 75.0000 (78.1250) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [20/30] time 0.424 (0.464) data 0.294 (0.333) loss_x loss_x 1.3447 (1.0142) acc_x 65.6250 (76.0938) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [25/30] time 0.411 (0.471) data 0.280 (0.341) loss_x loss_x 1.7627 (1.0990) acc_x 68.7500 (74.6250) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [30/30] time 0.446 (0.465) data 0.316 (0.334) loss_x loss_x 1.1582 (1.0745) acc_x 65.6250 (74.2708) lr 4.9096e-04 eta 0:00:00
epoch [136/200] batch [5/67] time 0.423 (0.469) data 0.293 (0.339) loss_u loss_u 0.9165 (0.9244) acc_u 12.5000 (10.0000) lr 4.9096e-04 eta 0:00:29
epoch [136/200] batch [10/67] time 0.477 (0.471) data 0.346 (0.340) loss_u loss_u 0.8931 (0.9177) acc_u 9.3750 (10.0000) lr 4.9096e-04 eta 0:00:26
epoch [136/200] batch [15/67] time 0.411 (0.477) data 0.279 (0.346) loss_u loss_u 0.9585 (0.9276) acc_u 6.2500 (8.9583) lr 4.9096e-04 eta 0:00:24
epoch [136/200] batch [20/67] time 0.401 (0.475) data 0.270 (0.344) loss_u loss_u 0.7866 (0.9206) acc_u 21.8750 (9.8438) lr 4.9096e-04 eta 0:00:22
epoch [136/200] batch [25/67] time 0.455 (0.470) data 0.324 (0.340) loss_u loss_u 0.8413 (0.9156) acc_u 21.8750 (10.3750) lr 4.9096e-04 eta 0:00:19
epoch [136/200] batch [30/67] time 0.395 (0.467) data 0.263 (0.336) loss_u loss_u 0.8486 (0.9110) acc_u 18.7500 (11.1458) lr 4.9096e-04 eta 0:00:17
epoch [136/200] batch [35/67] time 0.469 (0.464) data 0.339 (0.333) loss_u loss_u 0.9238 (0.9113) acc_u 9.3750 (10.9821) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [40/67] time 0.414 (0.461) data 0.284 (0.330) loss_u loss_u 0.9775 (0.9118) acc_u 3.1250 (11.0938) lr 4.9096e-04 eta 0:00:12
epoch [136/200] batch [45/67] time 0.458 (0.457) data 0.327 (0.326) loss_u loss_u 0.9839 (0.9120) acc_u 3.1250 (11.2500) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [50/67] time 0.454 (0.455) data 0.323 (0.324) loss_u loss_u 0.9243 (0.9140) acc_u 9.3750 (11.0625) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [55/67] time 0.567 (0.457) data 0.437 (0.327) loss_u loss_u 0.9385 (0.9164) acc_u 9.3750 (10.7386) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [60/67] time 0.423 (0.462) data 0.292 (0.331) loss_u loss_u 0.9756 (0.9140) acc_u 0.0000 (11.0938) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [65/67] time 0.449 (0.461) data 0.317 (0.330) loss_u loss_u 0.9419 (0.9124) acc_u 6.2500 (11.1058) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1457
confident_label rate tensor(0.3052, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 957
clean true:950
clean false:7
clean_rate:0.9926854754440961
noisy true:729
noisy false:1450
after delete: len(clean_dataset) 957
after delete: len(noisy_dataset) 2179
epoch [137/200] batch [5/29] time 0.526 (0.491) data 0.395 (0.360) loss_x loss_x 1.1553 (1.0869) acc_x 68.7500 (73.1250) lr 4.7750e-04 eta 0:00:11
epoch [137/200] batch [10/29] time 0.497 (0.487) data 0.367 (0.356) loss_x loss_x 0.7871 (0.9575) acc_x 87.5000 (76.8750) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [15/29] time 0.434 (0.481) data 0.303 (0.351) loss_x loss_x 1.4873 (1.0047) acc_x 62.5000 (75.4167) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [20/29] time 0.460 (0.473) data 0.329 (0.343) loss_x loss_x 1.2979 (0.9962) acc_x 62.5000 (74.8438) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [25/29] time 0.569 (0.473) data 0.438 (0.343) loss_x loss_x 1.2373 (1.0744) acc_x 68.7500 (73.7500) lr 4.7750e-04 eta 0:00:01
epoch [137/200] batch [5/68] time 0.471 (0.474) data 0.340 (0.343) loss_u loss_u 0.9155 (0.8852) acc_u 12.5000 (15.0000) lr 4.7750e-04 eta 0:00:29
epoch [137/200] batch [10/68] time 0.536 (0.466) data 0.405 (0.335) loss_u loss_u 0.8926 (0.9031) acc_u 12.5000 (12.8125) lr 4.7750e-04 eta 0:00:27
epoch [137/200] batch [15/68] time 0.518 (0.458) data 0.387 (0.327) loss_u loss_u 0.9043 (0.8984) acc_u 9.3750 (13.3333) lr 4.7750e-04 eta 0:00:24
epoch [137/200] batch [20/68] time 0.438 (0.459) data 0.306 (0.328) loss_u loss_u 0.8901 (0.9002) acc_u 12.5000 (12.9688) lr 4.7750e-04 eta 0:00:22
epoch [137/200] batch [25/68] time 0.342 (0.454) data 0.210 (0.323) loss_u loss_u 0.9502 (0.9021) acc_u 6.2500 (12.8750) lr 4.7750e-04 eta 0:00:19
epoch [137/200] batch [30/68] time 0.400 (0.450) data 0.270 (0.320) loss_u loss_u 0.8486 (0.9002) acc_u 21.8750 (12.7083) lr 4.7750e-04 eta 0:00:17
epoch [137/200] batch [35/68] time 0.580 (0.456) data 0.448 (0.325) loss_u loss_u 0.9336 (0.9046) acc_u 6.2500 (12.1429) lr 4.7750e-04 eta 0:00:15
epoch [137/200] batch [40/68] time 0.453 (0.457) data 0.323 (0.326) loss_u loss_u 0.9233 (0.9033) acc_u 9.3750 (12.3438) lr 4.7750e-04 eta 0:00:12
epoch [137/200] batch [45/68] time 0.379 (0.453) data 0.247 (0.322) loss_u loss_u 0.8809 (0.9011) acc_u 12.5000 (12.5694) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [50/68] time 0.422 (0.452) data 0.290 (0.321) loss_u loss_u 0.8750 (0.9018) acc_u 15.6250 (12.4375) lr 4.7750e-04 eta 0:00:08
epoch [137/200] batch [55/68] time 0.464 (0.452) data 0.333 (0.321) loss_u loss_u 0.8623 (0.9026) acc_u 12.5000 (12.1591) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [60/68] time 0.412 (0.449) data 0.281 (0.318) loss_u loss_u 0.9561 (0.9038) acc_u 9.3750 (12.2917) lr 4.7750e-04 eta 0:00:03
epoch [137/200] batch [65/68] time 0.572 (0.450) data 0.440 (0.319) loss_u loss_u 0.8350 (0.9045) acc_u 18.7500 (12.1154) lr 4.7750e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1488
confident_label rate tensor(0.2950, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 925
clean true:920
clean false:5
clean_rate:0.9945945945945946
noisy true:728
noisy false:1483
after delete: len(clean_dataset) 925
after delete: len(noisy_dataset) 2211
epoch [138/200] batch [5/28] time 0.571 (0.462) data 0.441 (0.331) loss_x loss_x 1.1133 (1.1032) acc_x 68.7500 (70.6250) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [10/28] time 0.375 (0.447) data 0.244 (0.316) loss_x loss_x 0.8462 (1.0790) acc_x 81.2500 (73.4375) lr 4.6417e-04 eta 0:00:08
epoch [138/200] batch [15/28] time 0.489 (0.448) data 0.359 (0.317) loss_x loss_x 1.4434 (1.1387) acc_x 56.2500 (71.8750) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [20/28] time 0.528 (0.447) data 0.397 (0.316) loss_x loss_x 0.7031 (1.1505) acc_x 87.5000 (72.3438) lr 4.6417e-04 eta 0:00:03
epoch [138/200] batch [25/28] time 0.406 (0.454) data 0.274 (0.323) loss_x loss_x 0.6729 (1.1071) acc_x 84.3750 (73.3750) lr 4.6417e-04 eta 0:00:01
epoch [138/200] batch [5/69] time 0.433 (0.465) data 0.301 (0.334) loss_u loss_u 0.8340 (0.9333) acc_u 25.0000 (8.7500) lr 4.6417e-04 eta 0:00:29
epoch [138/200] batch [10/69] time 0.510 (0.472) data 0.379 (0.341) loss_u loss_u 0.8755 (0.9234) acc_u 12.5000 (9.6875) lr 4.6417e-04 eta 0:00:27
epoch [138/200] batch [15/69] time 0.433 (0.472) data 0.302 (0.340) loss_u loss_u 0.8628 (0.9124) acc_u 15.6250 (11.4583) lr 4.6417e-04 eta 0:00:25
epoch [138/200] batch [20/69] time 0.431 (0.467) data 0.301 (0.335) loss_u loss_u 0.8633 (0.9151) acc_u 15.6250 (10.9375) lr 4.6417e-04 eta 0:00:22
epoch [138/200] batch [25/69] time 0.406 (0.460) data 0.276 (0.329) loss_u loss_u 0.8813 (0.9117) acc_u 21.8750 (11.6250) lr 4.6417e-04 eta 0:00:20
epoch [138/200] batch [30/69] time 0.484 (0.460) data 0.353 (0.329) loss_u loss_u 0.9419 (0.9101) acc_u 3.1250 (11.4583) lr 4.6417e-04 eta 0:00:17
epoch [138/200] batch [35/69] time 0.426 (0.458) data 0.294 (0.327) loss_u loss_u 0.8882 (0.9127) acc_u 12.5000 (11.2500) lr 4.6417e-04 eta 0:00:15
epoch [138/200] batch [40/69] time 0.452 (0.456) data 0.321 (0.324) loss_u loss_u 0.8540 (0.9099) acc_u 15.6250 (11.4844) lr 4.6417e-04 eta 0:00:13
epoch [138/200] batch [45/69] time 0.424 (0.454) data 0.293 (0.323) loss_u loss_u 0.9009 (0.9085) acc_u 12.5000 (11.9444) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [50/69] time 0.470 (0.453) data 0.339 (0.321) loss_u loss_u 0.8584 (0.9062) acc_u 18.7500 (12.1875) lr 4.6417e-04 eta 0:00:08
epoch [138/200] batch [55/69] time 0.506 (0.455) data 0.375 (0.323) loss_u loss_u 0.9736 (0.9062) acc_u 3.1250 (12.2727) lr 4.6417e-04 eta 0:00:06
epoch [138/200] batch [60/69] time 0.465 (0.454) data 0.334 (0.323) loss_u loss_u 0.9131 (0.9014) acc_u 12.5000 (12.8646) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [65/69] time 0.487 (0.456) data 0.356 (0.325) loss_u loss_u 0.9268 (0.9010) acc_u 6.2500 (12.7885) lr 4.6417e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1452
confident_label rate tensor(0.3099, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 972
clean true:965
clean false:7
clean_rate:0.992798353909465
noisy true:719
noisy false:1445
after delete: len(clean_dataset) 972
after delete: len(noisy_dataset) 2164
epoch [139/200] batch [5/30] time 0.553 (0.471) data 0.421 (0.339) loss_x loss_x 0.9922 (0.9222) acc_x 71.8750 (79.3750) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [10/30] time 0.511 (0.475) data 0.379 (0.344) loss_x loss_x 1.5293 (1.1203) acc_x 68.7500 (74.0625) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [15/30] time 0.420 (0.478) data 0.290 (0.347) loss_x loss_x 0.9561 (1.1315) acc_x 81.2500 (74.1667) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [20/30] time 0.377 (0.481) data 0.246 (0.350) loss_x loss_x 1.0947 (1.1151) acc_x 71.8750 (73.9062) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [25/30] time 0.391 (0.472) data 0.261 (0.341) loss_x loss_x 1.2324 (1.1470) acc_x 71.8750 (73.5000) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [30/30] time 0.508 (0.472) data 0.378 (0.341) loss_x loss_x 0.8872 (1.1278) acc_x 84.3750 (73.7500) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/67] time 0.355 (0.466) data 0.224 (0.336) loss_u loss_u 0.9092 (0.9303) acc_u 9.3750 (8.7500) lr 4.5098e-04 eta 0:00:28
epoch [139/200] batch [10/67] time 0.401 (0.463) data 0.270 (0.332) loss_u loss_u 0.9634 (0.9223) acc_u 6.2500 (10.3125) lr 4.5098e-04 eta 0:00:26
epoch [139/200] batch [15/67] time 0.410 (0.468) data 0.279 (0.337) loss_u loss_u 0.7656 (0.9082) acc_u 25.0000 (11.8750) lr 4.5098e-04 eta 0:00:24
epoch [139/200] batch [20/67] time 0.535 (0.466) data 0.404 (0.335) loss_u loss_u 0.9360 (0.9107) acc_u 9.3750 (11.5625) lr 4.5098e-04 eta 0:00:21
epoch [139/200] batch [25/67] time 0.368 (0.463) data 0.237 (0.332) loss_u loss_u 0.9326 (0.9114) acc_u 9.3750 (11.6250) lr 4.5098e-04 eta 0:00:19
epoch [139/200] batch [30/67] time 0.507 (0.464) data 0.376 (0.333) loss_u loss_u 0.8911 (0.9149) acc_u 15.6250 (11.2500) lr 4.5098e-04 eta 0:00:17
epoch [139/200] batch [35/67] time 0.476 (0.462) data 0.345 (0.331) loss_u loss_u 0.8706 (0.9133) acc_u 15.6250 (11.5179) lr 4.5098e-04 eta 0:00:14
epoch [139/200] batch [40/67] time 0.474 (0.463) data 0.343 (0.332) loss_u loss_u 0.8569 (0.9120) acc_u 21.8750 (11.6406) lr 4.5098e-04 eta 0:00:12
epoch [139/200] batch [45/67] time 0.474 (0.463) data 0.342 (0.332) loss_u loss_u 0.9482 (0.9132) acc_u 9.3750 (11.7361) lr 4.5098e-04 eta 0:00:10
epoch [139/200] batch [50/67] time 0.430 (0.465) data 0.299 (0.334) loss_u loss_u 0.8633 (0.9126) acc_u 15.6250 (11.8125) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [55/67] time 0.414 (0.464) data 0.282 (0.333) loss_u loss_u 0.9155 (0.9117) acc_u 9.3750 (11.8750) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [60/67] time 0.385 (0.464) data 0.255 (0.332) loss_u loss_u 0.9272 (0.9134) acc_u 9.3750 (11.5625) lr 4.5098e-04 eta 0:00:03
epoch [139/200] batch [65/67] time 0.357 (0.467) data 0.224 (0.335) loss_u loss_u 0.8384 (0.9133) acc_u 15.6250 (11.4904) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1477
confident_label rate tensor(0.3055, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 958
clean true:950
clean false:8
clean_rate:0.9916492693110647
noisy true:709
noisy false:1469
after delete: len(clean_dataset) 958
after delete: len(noisy_dataset) 2178
epoch [140/200] batch [5/29] time 0.502 (0.519) data 0.372 (0.388) loss_x loss_x 1.1465 (0.9934) acc_x 68.7500 (76.2500) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [10/29] time 0.383 (0.480) data 0.252 (0.349) loss_x loss_x 0.7627 (1.0731) acc_x 81.2500 (75.0000) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [15/29] time 0.481 (0.491) data 0.351 (0.360) loss_x loss_x 0.9146 (1.0306) acc_x 75.0000 (76.0417) lr 4.3792e-04 eta 0:00:06
epoch [140/200] batch [20/29] time 0.480 (0.493) data 0.349 (0.363) loss_x loss_x 1.3525 (1.0831) acc_x 71.8750 (74.3750) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [25/29] time 0.424 (0.483) data 0.293 (0.352) loss_x loss_x 0.8232 (1.0741) acc_x 78.1250 (74.0000) lr 4.3792e-04 eta 0:00:01
epoch [140/200] batch [5/68] time 0.696 (0.483) data 0.565 (0.352) loss_u loss_u 0.8857 (0.9218) acc_u 12.5000 (8.7500) lr 4.3792e-04 eta 0:00:30
epoch [140/200] batch [10/68] time 0.388 (0.475) data 0.258 (0.345) loss_u loss_u 0.9614 (0.9170) acc_u 12.5000 (10.9375) lr 4.3792e-04 eta 0:00:27
epoch [140/200] batch [15/68] time 0.485 (0.472) data 0.354 (0.341) loss_u loss_u 0.8501 (0.9138) acc_u 15.6250 (11.4583) lr 4.3792e-04 eta 0:00:25
epoch [140/200] batch [20/68] time 0.535 (0.469) data 0.404 (0.338) loss_u loss_u 0.9072 (0.9134) acc_u 12.5000 (11.5625) lr 4.3792e-04 eta 0:00:22
epoch [140/200] batch [25/68] time 0.437 (0.468) data 0.305 (0.337) loss_u loss_u 0.9106 (0.9119) acc_u 9.3750 (11.6250) lr 4.3792e-04 eta 0:00:20
epoch [140/200] batch [30/68] time 0.402 (0.466) data 0.271 (0.335) loss_u loss_u 0.8262 (0.9087) acc_u 21.8750 (11.7708) lr 4.3792e-04 eta 0:00:17
epoch [140/200] batch [35/68] time 0.333 (0.466) data 0.202 (0.335) loss_u loss_u 0.8984 (0.9101) acc_u 12.5000 (11.6071) lr 4.3792e-04 eta 0:00:15
epoch [140/200] batch [40/68] time 0.476 (0.464) data 0.345 (0.333) loss_u loss_u 0.9634 (0.9125) acc_u 6.2500 (11.3281) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [45/68] time 0.387 (0.461) data 0.257 (0.330) loss_u loss_u 0.9014 (0.9147) acc_u 9.3750 (10.9722) lr 4.3792e-04 eta 0:00:10
epoch [140/200] batch [50/68] time 0.464 (0.462) data 0.333 (0.331) loss_u loss_u 0.8882 (0.9145) acc_u 12.5000 (11.0000) lr 4.3792e-04 eta 0:00:08
epoch [140/200] batch [55/68] time 0.462 (0.460) data 0.331 (0.329) loss_u loss_u 0.8916 (0.9137) acc_u 15.6250 (11.1932) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [60/68] time 0.376 (0.459) data 0.244 (0.328) loss_u loss_u 0.8428 (0.9139) acc_u 21.8750 (11.4062) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [65/68] time 0.435 (0.458) data 0.301 (0.326) loss_u loss_u 0.8784 (0.9116) acc_u 15.6250 (11.5865) lr 4.3792e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1450
confident_label rate tensor(0.3115, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 977
clean true:969
clean false:8
clean_rate:0.9918116683725691
noisy true:717
noisy false:1442
after delete: len(clean_dataset) 977
after delete: len(noisy_dataset) 2159
epoch [141/200] batch [5/30] time 0.601 (0.534) data 0.470 (0.403) loss_x loss_x 1.0996 (1.0981) acc_x 78.1250 (73.7500) lr 4.2499e-04 eta 0:00:13
epoch [141/200] batch [10/30] time 0.434 (0.532) data 0.301 (0.401) loss_x loss_x 0.8608 (1.1314) acc_x 75.0000 (72.8125) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [15/30] time 0.386 (0.525) data 0.256 (0.394) loss_x loss_x 1.1318 (1.1311) acc_x 81.2500 (73.3333) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [20/30] time 0.468 (0.510) data 0.337 (0.379) loss_x loss_x 1.0059 (1.1425) acc_x 75.0000 (72.6562) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [25/30] time 0.392 (0.495) data 0.262 (0.365) loss_x loss_x 1.5645 (1.1630) acc_x 53.1250 (71.5000) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [30/30] time 0.447 (0.485) data 0.316 (0.354) loss_x loss_x 1.0732 (1.1366) acc_x 75.0000 (72.3958) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/67] time 0.443 (0.476) data 0.312 (0.346) loss_u loss_u 0.8550 (0.8980) acc_u 21.8750 (14.3750) lr 4.2499e-04 eta 0:00:29
epoch [141/200] batch [10/67] time 0.380 (0.481) data 0.248 (0.351) loss_u loss_u 0.9121 (0.9033) acc_u 15.6250 (13.7500) lr 4.2499e-04 eta 0:00:27
epoch [141/200] batch [15/67] time 0.442 (0.481) data 0.311 (0.350) loss_u loss_u 0.9102 (0.9063) acc_u 12.5000 (13.5417) lr 4.2499e-04 eta 0:00:24
epoch [141/200] batch [20/67] time 0.483 (0.477) data 0.352 (0.346) loss_u loss_u 0.9019 (0.9068) acc_u 12.5000 (13.1250) lr 4.2499e-04 eta 0:00:22
epoch [141/200] batch [25/67] time 0.386 (0.471) data 0.256 (0.340) loss_u loss_u 0.8960 (0.9035) acc_u 18.7500 (13.5000) lr 4.2499e-04 eta 0:00:19
epoch [141/200] batch [30/67] time 0.545 (0.470) data 0.414 (0.339) loss_u loss_u 0.8989 (0.9044) acc_u 12.5000 (13.2292) lr 4.2499e-04 eta 0:00:17
epoch [141/200] batch [35/67] time 0.583 (0.468) data 0.451 (0.337) loss_u loss_u 0.8921 (0.9038) acc_u 9.3750 (12.9464) lr 4.2499e-04 eta 0:00:14
epoch [141/200] batch [40/67] time 0.740 (0.471) data 0.609 (0.340) loss_u loss_u 0.9277 (0.9069) acc_u 9.3750 (12.5781) lr 4.2499e-04 eta 0:00:12
epoch [141/200] batch [45/67] time 0.480 (0.472) data 0.348 (0.342) loss_u loss_u 0.8745 (0.9074) acc_u 18.7500 (12.3611) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [50/67] time 0.336 (0.468) data 0.205 (0.337) loss_u loss_u 0.9707 (0.9099) acc_u 3.1250 (11.9375) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [55/67] time 0.456 (0.467) data 0.324 (0.336) loss_u loss_u 0.8994 (0.9135) acc_u 12.5000 (11.5341) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [60/67] time 0.440 (0.468) data 0.306 (0.336) loss_u loss_u 0.9399 (0.9142) acc_u 9.3750 (11.3021) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [65/67] time 0.454 (0.467) data 0.322 (0.336) loss_u loss_u 0.7949 (0.9122) acc_u 25.0000 (11.6346) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1476
confident_label rate tensor(0.2997, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 940
clean true:932
clean false:8
clean_rate:0.9914893617021276
noisy true:728
noisy false:1468
after delete: len(clean_dataset) 940
after delete: len(noisy_dataset) 2196
epoch [142/200] batch [5/29] time 0.514 (0.487) data 0.383 (0.356) loss_x loss_x 1.1230 (1.0863) acc_x 68.7500 (71.8750) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [10/29] time 0.510 (0.468) data 0.379 (0.338) loss_x loss_x 0.4570 (1.0490) acc_x 87.5000 (73.7500) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [15/29] time 0.401 (0.464) data 0.271 (0.333) loss_x loss_x 0.8521 (1.0097) acc_x 78.1250 (74.5833) lr 4.1221e-04 eta 0:00:06
epoch [142/200] batch [20/29] time 0.460 (0.476) data 0.330 (0.345) loss_x loss_x 1.2012 (1.0354) acc_x 68.7500 (73.5938) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [25/29] time 0.413 (0.470) data 0.284 (0.340) loss_x loss_x 0.9614 (1.0176) acc_x 81.2500 (74.6250) lr 4.1221e-04 eta 0:00:01
epoch [142/200] batch [5/68] time 0.431 (0.455) data 0.300 (0.324) loss_u loss_u 0.8335 (0.8737) acc_u 21.8750 (16.2500) lr 4.1221e-04 eta 0:00:28
epoch [142/200] batch [10/68] time 0.492 (0.462) data 0.360 (0.331) loss_u loss_u 0.9175 (0.9017) acc_u 12.5000 (12.8125) lr 4.1221e-04 eta 0:00:26
epoch [142/200] batch [15/68] time 0.531 (0.463) data 0.400 (0.333) loss_u loss_u 0.8506 (0.8972) acc_u 21.8750 (13.1250) lr 4.1221e-04 eta 0:00:24
epoch [142/200] batch [20/68] time 0.427 (0.458) data 0.295 (0.327) loss_u loss_u 0.9893 (0.9052) acc_u 0.0000 (12.3438) lr 4.1221e-04 eta 0:00:21
epoch [142/200] batch [25/68] time 0.419 (0.455) data 0.287 (0.325) loss_u loss_u 0.9644 (0.9109) acc_u 3.1250 (11.2500) lr 4.1221e-04 eta 0:00:19
epoch [142/200] batch [30/68] time 0.468 (0.456) data 0.338 (0.325) loss_u loss_u 0.8730 (0.9064) acc_u 15.6250 (11.9792) lr 4.1221e-04 eta 0:00:17
epoch [142/200] batch [35/68] time 0.423 (0.457) data 0.293 (0.326) loss_u loss_u 0.8525 (0.9062) acc_u 31.2500 (12.5893) lr 4.1221e-04 eta 0:00:15
epoch [142/200] batch [40/68] time 0.374 (0.454) data 0.242 (0.324) loss_u loss_u 0.8892 (0.9055) acc_u 12.5000 (12.6562) lr 4.1221e-04 eta 0:00:12
epoch [142/200] batch [45/68] time 0.486 (0.455) data 0.356 (0.324) loss_u loss_u 0.9297 (0.9055) acc_u 9.3750 (12.7083) lr 4.1221e-04 eta 0:00:10
epoch [142/200] batch [50/68] time 0.594 (0.460) data 0.462 (0.329) loss_u loss_u 0.8643 (0.9036) acc_u 21.8750 (12.9375) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [55/68] time 0.506 (0.460) data 0.375 (0.329) loss_u loss_u 0.8936 (0.9056) acc_u 12.5000 (12.6136) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [60/68] time 0.647 (0.461) data 0.515 (0.330) loss_u loss_u 0.8857 (0.9040) acc_u 12.5000 (12.8125) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [65/68] time 0.454 (0.463) data 0.322 (0.332) loss_u loss_u 0.8545 (0.9019) acc_u 18.7500 (13.0769) lr 4.1221e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1467
confident_label rate tensor(0.3042, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 954
clean true:945
clean false:9
clean_rate:0.9905660377358491
noisy true:724
noisy false:1458
after delete: len(clean_dataset) 954
after delete: len(noisy_dataset) 2182
epoch [143/200] batch [5/29] time 0.416 (0.456) data 0.286 (0.326) loss_x loss_x 0.6353 (0.9766) acc_x 81.2500 (75.6250) lr 3.9958e-04 eta 0:00:10
epoch [143/200] batch [10/29] time 0.349 (0.455) data 0.219 (0.324) loss_x loss_x 1.1836 (1.0229) acc_x 71.8750 (75.6250) lr 3.9958e-04 eta 0:00:08
epoch [143/200] batch [15/29] time 0.387 (0.447) data 0.257 (0.316) loss_x loss_x 0.6831 (1.0646) acc_x 87.5000 (75.2083) lr 3.9958e-04 eta 0:00:06
epoch [143/200] batch [20/29] time 0.462 (0.455) data 0.331 (0.325) loss_x loss_x 0.5415 (1.0611) acc_x 84.3750 (75.3125) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [25/29] time 0.403 (0.454) data 0.272 (0.323) loss_x loss_x 1.2773 (1.0335) acc_x 68.7500 (75.5000) lr 3.9958e-04 eta 0:00:01
epoch [143/200] batch [5/68] time 0.401 (0.452) data 0.270 (0.321) loss_u loss_u 0.9678 (0.9100) acc_u 3.1250 (8.7500) lr 3.9958e-04 eta 0:00:28
epoch [143/200] batch [10/68] time 0.373 (0.444) data 0.242 (0.313) loss_u loss_u 0.8911 (0.8993) acc_u 15.6250 (11.8750) lr 3.9958e-04 eta 0:00:25
epoch [143/200] batch [15/68] time 0.479 (0.443) data 0.347 (0.312) loss_u loss_u 0.9385 (0.9067) acc_u 9.3750 (11.6667) lr 3.9958e-04 eta 0:00:23
epoch [143/200] batch [20/68] time 0.501 (0.451) data 0.369 (0.319) loss_u loss_u 0.8770 (0.9032) acc_u 15.6250 (12.0312) lr 3.9958e-04 eta 0:00:21
epoch [143/200] batch [25/68] time 0.422 (0.455) data 0.290 (0.324) loss_u loss_u 0.9434 (0.9096) acc_u 9.3750 (11.5000) lr 3.9958e-04 eta 0:00:19
epoch [143/200] batch [30/68] time 0.405 (0.453) data 0.273 (0.321) loss_u loss_u 0.8564 (0.9084) acc_u 18.7500 (12.0833) lr 3.9958e-04 eta 0:00:17
epoch [143/200] batch [35/68] time 0.454 (0.455) data 0.323 (0.323) loss_u loss_u 0.9438 (0.9053) acc_u 6.2500 (12.3214) lr 3.9958e-04 eta 0:00:15
epoch [143/200] batch [40/68] time 0.433 (0.458) data 0.303 (0.326) loss_u loss_u 0.9443 (0.9057) acc_u 6.2500 (12.0312) lr 3.9958e-04 eta 0:00:12
epoch [143/200] batch [45/68] time 0.359 (0.456) data 0.228 (0.325) loss_u loss_u 0.9590 (0.9032) acc_u 3.1250 (12.4306) lr 3.9958e-04 eta 0:00:10
epoch [143/200] batch [50/68] time 0.464 (0.456) data 0.333 (0.324) loss_u loss_u 0.7456 (0.9027) acc_u 34.3750 (12.7500) lr 3.9958e-04 eta 0:00:08
epoch [143/200] batch [55/68] time 0.768 (0.459) data 0.637 (0.327) loss_u loss_u 0.9282 (0.9050) acc_u 12.5000 (12.5568) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [60/68] time 0.383 (0.457) data 0.252 (0.325) loss_u loss_u 0.9087 (0.9066) acc_u 9.3750 (12.2396) lr 3.9958e-04 eta 0:00:03
epoch [143/200] batch [65/68] time 0.615 (0.456) data 0.483 (0.325) loss_u loss_u 0.9780 (0.9087) acc_u 3.1250 (12.0192) lr 3.9958e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1455
confident_label rate tensor(0.3115, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 977
clean true:967
clean false:10
clean_rate:0.9897645854657113
noisy true:714
noisy false:1445
after delete: len(clean_dataset) 977
after delete: len(noisy_dataset) 2159
epoch [144/200] batch [5/30] time 0.342 (0.399) data 0.210 (0.268) loss_x loss_x 1.2178 (1.2982) acc_x 68.7500 (68.1250) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [10/30] time 0.440 (0.464) data 0.310 (0.333) loss_x loss_x 1.5537 (1.3353) acc_x 68.7500 (65.6250) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [15/30] time 0.449 (0.467) data 0.317 (0.336) loss_x loss_x 0.9702 (1.2387) acc_x 84.3750 (69.5833) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [20/30] time 0.563 (0.469) data 0.432 (0.338) loss_x loss_x 1.3789 (1.2350) acc_x 53.1250 (69.5312) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [25/30] time 0.661 (0.482) data 0.530 (0.351) loss_x loss_x 1.0146 (1.2350) acc_x 71.8750 (69.8750) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [30/30] time 0.470 (0.477) data 0.339 (0.346) loss_x loss_x 1.1592 (1.2109) acc_x 81.2500 (71.0417) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/67] time 0.630 (0.480) data 0.498 (0.349) loss_u loss_u 0.9780 (0.9017) acc_u 3.1250 (11.2500) lr 3.8709e-04 eta 0:00:29
epoch [144/200] batch [10/67] time 0.357 (0.480) data 0.227 (0.349) loss_u loss_u 0.8857 (0.9041) acc_u 12.5000 (10.9375) lr 3.8709e-04 eta 0:00:27
epoch [144/200] batch [15/67] time 0.491 (0.483) data 0.359 (0.352) loss_u loss_u 0.9038 (0.9066) acc_u 9.3750 (10.4167) lr 3.8709e-04 eta 0:00:25
epoch [144/200] batch [20/67] time 0.719 (0.482) data 0.586 (0.351) loss_u loss_u 0.9849 (0.9099) acc_u 3.1250 (10.4688) lr 3.8709e-04 eta 0:00:22
epoch [144/200] batch [25/67] time 0.386 (0.478) data 0.254 (0.347) loss_u loss_u 0.9243 (0.9050) acc_u 9.3750 (11.3750) lr 3.8709e-04 eta 0:00:20
epoch [144/200] batch [30/67] time 0.513 (0.480) data 0.383 (0.349) loss_u loss_u 0.9307 (0.9080) acc_u 9.3750 (11.2500) lr 3.8709e-04 eta 0:00:17
epoch [144/200] batch [35/67] time 0.538 (0.481) data 0.407 (0.350) loss_u loss_u 0.8955 (0.9067) acc_u 9.3750 (11.5179) lr 3.8709e-04 eta 0:00:15
epoch [144/200] batch [40/67] time 0.452 (0.479) data 0.320 (0.348) loss_u loss_u 0.9302 (0.9073) acc_u 9.3750 (11.4844) lr 3.8709e-04 eta 0:00:12
epoch [144/200] batch [45/67] time 0.426 (0.476) data 0.294 (0.345) loss_u loss_u 0.9067 (0.9078) acc_u 9.3750 (11.4583) lr 3.8709e-04 eta 0:00:10
epoch [144/200] batch [50/67] time 0.563 (0.476) data 0.432 (0.345) loss_u loss_u 0.8506 (0.9065) acc_u 18.7500 (11.7500) lr 3.8709e-04 eta 0:00:08
epoch [144/200] batch [55/67] time 0.397 (0.474) data 0.266 (0.343) loss_u loss_u 0.8657 (0.9078) acc_u 15.6250 (11.5341) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [60/67] time 0.436 (0.472) data 0.305 (0.341) loss_u loss_u 0.9707 (0.9085) acc_u 6.2500 (11.5104) lr 3.8709e-04 eta 0:00:03
epoch [144/200] batch [65/67] time 0.501 (0.472) data 0.369 (0.341) loss_u loss_u 0.8848 (0.9077) acc_u 15.6250 (11.6827) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1477
confident_label rate tensor(0.3042, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 954
clean true:943
clean false:11
clean_rate:0.9884696016771488
noisy true:716
noisy false:1466
after delete: len(clean_dataset) 954
after delete: len(noisy_dataset) 2182
epoch [145/200] batch [5/29] time 0.518 (0.560) data 0.387 (0.429) loss_x loss_x 1.3057 (1.1772) acc_x 81.2500 (76.2500) lr 3.7476e-04 eta 0:00:13
epoch [145/200] batch [10/29] time 0.449 (0.493) data 0.318 (0.362) loss_x loss_x 1.1982 (1.1056) acc_x 68.7500 (75.6250) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [15/29] time 0.529 (0.485) data 0.399 (0.354) loss_x loss_x 1.1768 (1.0769) acc_x 68.7500 (74.7917) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [20/29] time 0.392 (0.466) data 0.262 (0.335) loss_x loss_x 1.1074 (1.0942) acc_x 84.3750 (74.5312) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [25/29] time 0.554 (0.474) data 0.424 (0.343) loss_x loss_x 0.7290 (1.0374) acc_x 78.1250 (76.1250) lr 3.7476e-04 eta 0:00:01
epoch [145/200] batch [5/68] time 0.420 (0.479) data 0.290 (0.349) loss_u loss_u 0.9121 (0.9171) acc_u 6.2500 (10.6250) lr 3.7476e-04 eta 0:00:30
epoch [145/200] batch [10/68] time 0.459 (0.477) data 0.330 (0.346) loss_u loss_u 0.9404 (0.9130) acc_u 9.3750 (10.9375) lr 3.7476e-04 eta 0:00:27
epoch [145/200] batch [15/68] time 0.382 (0.479) data 0.250 (0.349) loss_u loss_u 0.8315 (0.8987) acc_u 18.7500 (12.7083) lr 3.7476e-04 eta 0:00:25
epoch [145/200] batch [20/68] time 0.398 (0.474) data 0.266 (0.343) loss_u loss_u 0.8623 (0.9005) acc_u 15.6250 (12.1875) lr 3.7476e-04 eta 0:00:22
epoch [145/200] batch [25/68] time 0.493 (0.471) data 0.362 (0.340) loss_u loss_u 0.8647 (0.8958) acc_u 18.7500 (12.6250) lr 3.7476e-04 eta 0:00:20
epoch [145/200] batch [30/68] time 0.435 (0.467) data 0.303 (0.337) loss_u loss_u 0.8472 (0.8956) acc_u 15.6250 (12.5000) lr 3.7476e-04 eta 0:00:17
epoch [145/200] batch [35/68] time 0.534 (0.471) data 0.403 (0.341) loss_u loss_u 0.9224 (0.8980) acc_u 9.3750 (12.5000) lr 3.7476e-04 eta 0:00:15
epoch [145/200] batch [40/68] time 0.398 (0.473) data 0.267 (0.342) loss_u loss_u 0.9443 (0.8986) acc_u 9.3750 (12.5000) lr 3.7476e-04 eta 0:00:13
epoch [145/200] batch [45/68] time 0.625 (0.470) data 0.494 (0.339) loss_u loss_u 0.9219 (0.8985) acc_u 6.2500 (12.5694) lr 3.7476e-04 eta 0:00:10
epoch [145/200] batch [50/68] time 0.802 (0.470) data 0.671 (0.339) loss_u loss_u 0.8682 (0.8996) acc_u 15.6250 (12.3125) lr 3.7476e-04 eta 0:00:08
epoch [145/200] batch [55/68] time 0.364 (0.467) data 0.233 (0.336) loss_u loss_u 0.9072 (0.9004) acc_u 12.5000 (12.2159) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [60/68] time 0.373 (0.468) data 0.243 (0.337) loss_u loss_u 0.9727 (0.9001) acc_u 3.1250 (12.2917) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [65/68] time 0.472 (0.465) data 0.341 (0.334) loss_u loss_u 0.9829 (0.9012) acc_u 0.0000 (12.1154) lr 3.7476e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1387
confident_label rate tensor(0.3160, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 991
clean true:981
clean false:10
clean_rate:0.9899091826437941
noisy true:768
noisy false:1377
after delete: len(clean_dataset) 991
after delete: len(noisy_dataset) 2145
epoch [146/200] batch [5/30] time 0.529 (0.478) data 0.398 (0.348) loss_x loss_x 0.8599 (0.8524) acc_x 75.0000 (78.1250) lr 3.6258e-04 eta 0:00:11
epoch [146/200] batch [10/30] time 0.568 (0.474) data 0.437 (0.344) loss_x loss_x 0.8921 (0.8724) acc_x 78.1250 (78.1250) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [15/30] time 0.394 (0.480) data 0.264 (0.349) loss_x loss_x 1.0439 (0.9490) acc_x 71.8750 (76.2500) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [20/30] time 0.431 (0.480) data 0.300 (0.349) loss_x loss_x 1.8203 (1.0099) acc_x 56.2500 (75.3125) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [25/30] time 0.446 (0.478) data 0.315 (0.347) loss_x loss_x 1.3887 (1.0819) acc_x 56.2500 (73.3750) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [30/30] time 0.380 (0.474) data 0.250 (0.343) loss_x loss_x 1.8535 (1.1626) acc_x 62.5000 (72.0833) lr 3.6258e-04 eta 0:00:00
epoch [146/200] batch [5/67] time 0.518 (0.477) data 0.387 (0.346) loss_u loss_u 0.9751 (0.9037) acc_u 6.2500 (13.1250) lr 3.6258e-04 eta 0:00:29
epoch [146/200] batch [10/67] time 0.381 (0.478) data 0.250 (0.347) loss_u loss_u 0.8984 (0.9067) acc_u 12.5000 (12.1875) lr 3.6258e-04 eta 0:00:27
epoch [146/200] batch [15/67] time 0.438 (0.477) data 0.308 (0.346) loss_u loss_u 0.9600 (0.9095) acc_u 3.1250 (10.8333) lr 3.6258e-04 eta 0:00:24
epoch [146/200] batch [20/67] time 0.473 (0.480) data 0.342 (0.349) loss_u loss_u 0.8936 (0.9067) acc_u 12.5000 (11.2500) lr 3.6258e-04 eta 0:00:22
epoch [146/200] batch [25/67] time 0.385 (0.480) data 0.255 (0.349) loss_u loss_u 0.9087 (0.9063) acc_u 12.5000 (11.6250) lr 3.6258e-04 eta 0:00:20
epoch [146/200] batch [30/67] time 0.346 (0.482) data 0.214 (0.351) loss_u loss_u 0.9033 (0.9068) acc_u 9.3750 (11.5625) lr 3.6258e-04 eta 0:00:17
epoch [146/200] batch [35/67] time 0.402 (0.478) data 0.271 (0.347) loss_u loss_u 0.8169 (0.9074) acc_u 28.1250 (11.5179) lr 3.6258e-04 eta 0:00:15
epoch [146/200] batch [40/67] time 0.660 (0.480) data 0.529 (0.349) loss_u loss_u 0.8789 (0.9066) acc_u 15.6250 (11.5625) lr 3.6258e-04 eta 0:00:12
epoch [146/200] batch [45/67] time 0.354 (0.478) data 0.223 (0.347) loss_u loss_u 0.8540 (0.9070) acc_u 18.7500 (11.5972) lr 3.6258e-04 eta 0:00:10
epoch [146/200] batch [50/67] time 0.480 (0.475) data 0.349 (0.344) loss_u loss_u 0.9482 (0.9096) acc_u 6.2500 (11.3125) lr 3.6258e-04 eta 0:00:08
epoch [146/200] batch [55/67] time 0.448 (0.474) data 0.317 (0.343) loss_u loss_u 0.8950 (0.9103) acc_u 12.5000 (11.3068) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [60/67] time 0.460 (0.471) data 0.329 (0.340) loss_u loss_u 0.8955 (0.9116) acc_u 12.5000 (11.2500) lr 3.6258e-04 eta 0:00:03
epoch [146/200] batch [65/67] time 0.368 (0.469) data 0.237 (0.338) loss_u loss_u 0.9624 (0.9129) acc_u 6.2500 (11.1058) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1451
confident_label rate tensor(0.3109, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 975
clean true:966
clean false:9
clean_rate:0.9907692307692307
noisy true:719
noisy false:1442
after delete: len(clean_dataset) 975
after delete: len(noisy_dataset) 2161
epoch [147/200] batch [5/30] time 0.480 (0.417) data 0.349 (0.286) loss_x loss_x 0.7402 (1.0311) acc_x 81.2500 (73.7500) lr 3.5055e-04 eta 0:00:10
epoch [147/200] batch [10/30] time 0.433 (0.465) data 0.303 (0.334) loss_x loss_x 0.9385 (0.9829) acc_x 78.1250 (75.9375) lr 3.5055e-04 eta 0:00:09
epoch [147/200] batch [15/30] time 0.527 (0.475) data 0.397 (0.344) loss_x loss_x 0.6030 (0.9816) acc_x 84.3750 (76.0417) lr 3.5055e-04 eta 0:00:07
epoch [147/200] batch [20/30] time 0.481 (0.475) data 0.351 (0.345) loss_x loss_x 1.1543 (1.0212) acc_x 71.8750 (74.3750) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [25/30] time 0.503 (0.476) data 0.372 (0.345) loss_x loss_x 0.9897 (1.0022) acc_x 81.2500 (74.7500) lr 3.5055e-04 eta 0:00:02
epoch [147/200] batch [30/30] time 0.463 (0.478) data 0.332 (0.348) loss_x loss_x 1.0957 (1.0251) acc_x 75.0000 (73.6458) lr 3.5055e-04 eta 0:00:00
epoch [147/200] batch [5/67] time 0.546 (0.486) data 0.414 (0.356) loss_u loss_u 0.9004 (0.9198) acc_u 12.5000 (8.7500) lr 3.5055e-04 eta 0:00:30
epoch [147/200] batch [10/67] time 0.488 (0.486) data 0.357 (0.355) loss_u loss_u 0.8877 (0.9121) acc_u 15.6250 (11.2500) lr 3.5055e-04 eta 0:00:27
epoch [147/200] batch [15/67] time 0.365 (0.478) data 0.234 (0.347) loss_u loss_u 0.9526 (0.9126) acc_u 6.2500 (10.8333) lr 3.5055e-04 eta 0:00:24
epoch [147/200] batch [20/67] time 0.597 (0.482) data 0.466 (0.351) loss_u loss_u 0.9282 (0.9153) acc_u 9.3750 (10.1562) lr 3.5055e-04 eta 0:00:22
epoch [147/200] batch [25/67] time 0.470 (0.485) data 0.338 (0.354) loss_u loss_u 0.9248 (0.9163) acc_u 9.3750 (10.5000) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [30/67] time 0.641 (0.483) data 0.511 (0.353) loss_u loss_u 0.9473 (0.9193) acc_u 3.1250 (9.8958) lr 3.5055e-04 eta 0:00:17
epoch [147/200] batch [35/67] time 0.353 (0.483) data 0.222 (0.353) loss_u loss_u 0.9048 (0.9194) acc_u 12.5000 (9.8214) lr 3.5055e-04 eta 0:00:15
epoch [147/200] batch [40/67] time 0.471 (0.482) data 0.341 (0.351) loss_u loss_u 0.8413 (0.9146) acc_u 15.6250 (10.4688) lr 3.5055e-04 eta 0:00:13
epoch [147/200] batch [45/67] time 0.387 (0.480) data 0.256 (0.349) loss_u loss_u 0.8652 (0.9111) acc_u 12.5000 (10.6944) lr 3.5055e-04 eta 0:00:10
epoch [147/200] batch [50/67] time 0.484 (0.479) data 0.352 (0.348) loss_u loss_u 0.9688 (0.9133) acc_u 3.1250 (10.4375) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [55/67] time 0.459 (0.476) data 0.328 (0.345) loss_u loss_u 0.8599 (0.9124) acc_u 12.5000 (10.5682) lr 3.5055e-04 eta 0:00:05
epoch [147/200] batch [60/67] time 0.487 (0.473) data 0.357 (0.342) loss_u loss_u 0.9282 (0.9108) acc_u 6.2500 (10.7292) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [65/67] time 0.400 (0.469) data 0.269 (0.338) loss_u loss_u 0.9189 (0.9124) acc_u 9.3750 (10.6731) lr 3.5055e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1437
confident_label rate tensor(0.3106, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 974
clean true:967
clean false:7
clean_rate:0.9928131416837782
noisy true:732
noisy false:1430
after delete: len(clean_dataset) 974
after delete: len(noisy_dataset) 2162
epoch [148/200] batch [5/30] time 0.482 (0.458) data 0.350 (0.326) loss_x loss_x 1.1416 (1.0217) acc_x 78.1250 (74.3750) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [10/30] time 0.509 (0.457) data 0.378 (0.326) loss_x loss_x 0.5200 (0.9606) acc_x 90.6250 (77.1875) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [15/30] time 0.379 (0.458) data 0.249 (0.327) loss_x loss_x 1.0361 (0.9564) acc_x 81.2500 (77.7083) lr 3.3869e-04 eta 0:00:06
epoch [148/200] batch [20/30] time 0.397 (0.455) data 0.265 (0.324) loss_x loss_x 1.6387 (1.0075) acc_x 65.6250 (77.1875) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [25/30] time 0.533 (0.476) data 0.403 (0.345) loss_x loss_x 1.0908 (1.0133) acc_x 68.7500 (76.3750) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [30/30] time 0.352 (0.472) data 0.222 (0.341) loss_x loss_x 0.8901 (1.0120) acc_x 68.7500 (75.5208) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/67] time 0.409 (0.465) data 0.278 (0.334) loss_u loss_u 0.9404 (0.9020) acc_u 9.3750 (16.2500) lr 3.3869e-04 eta 0:00:28
epoch [148/200] batch [10/67] time 0.455 (0.471) data 0.323 (0.340) loss_u loss_u 0.8486 (0.8853) acc_u 18.7500 (15.9375) lr 3.3869e-04 eta 0:00:26
epoch [148/200] batch [15/67] time 0.331 (0.466) data 0.200 (0.335) loss_u loss_u 0.8550 (0.8993) acc_u 18.7500 (13.7500) lr 3.3869e-04 eta 0:00:24
epoch [148/200] batch [20/67] time 0.516 (0.468) data 0.385 (0.337) loss_u loss_u 0.9663 (0.9030) acc_u 6.2500 (12.6562) lr 3.3869e-04 eta 0:00:21
epoch [148/200] batch [25/67] time 0.454 (0.465) data 0.323 (0.334) loss_u loss_u 0.9316 (0.8990) acc_u 6.2500 (13.0000) lr 3.3869e-04 eta 0:00:19
epoch [148/200] batch [30/67] time 0.449 (0.469) data 0.317 (0.337) loss_u loss_u 0.8687 (0.8970) acc_u 15.6250 (13.1250) lr 3.3869e-04 eta 0:00:17
epoch [148/200] batch [35/67] time 0.436 (0.464) data 0.305 (0.333) loss_u loss_u 0.7827 (0.8937) acc_u 28.1250 (13.4821) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [40/67] time 0.447 (0.465) data 0.316 (0.334) loss_u loss_u 0.8647 (0.8976) acc_u 15.6250 (13.0469) lr 3.3869e-04 eta 0:00:12
epoch [148/200] batch [45/67] time 0.452 (0.472) data 0.322 (0.341) loss_u loss_u 0.8940 (0.8989) acc_u 15.6250 (12.8472) lr 3.3869e-04 eta 0:00:10
epoch [148/200] batch [50/67] time 0.409 (0.469) data 0.277 (0.338) loss_u loss_u 0.9292 (0.9009) acc_u 6.2500 (12.5000) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [55/67] time 0.402 (0.466) data 0.271 (0.335) loss_u loss_u 0.9595 (0.9037) acc_u 6.2500 (12.1591) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [60/67] time 0.467 (0.465) data 0.335 (0.334) loss_u loss_u 0.8984 (0.9036) acc_u 9.3750 (12.0833) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [65/67] time 0.374 (0.464) data 0.242 (0.333) loss_u loss_u 0.8486 (0.9015) acc_u 18.7500 (12.4038) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1467
confident_label rate tensor(0.3039, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 953
clean true:947
clean false:6
clean_rate:0.993704092339979
noisy true:722
noisy false:1461
after delete: len(clean_dataset) 953
after delete: len(noisy_dataset) 2183
epoch [149/200] batch [5/29] time 0.403 (0.473) data 0.272 (0.342) loss_x loss_x 0.7744 (1.1125) acc_x 81.2500 (73.1250) lr 3.2699e-04 eta 0:00:11
epoch [149/200] batch [10/29] time 0.395 (0.452) data 0.265 (0.322) loss_x loss_x 0.8096 (1.0798) acc_x 78.1250 (73.7500) lr 3.2699e-04 eta 0:00:08
epoch [149/200] batch [15/29] time 0.538 (0.475) data 0.407 (0.344) loss_x loss_x 1.0947 (1.1265) acc_x 75.0000 (73.1250) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [20/29] time 0.487 (0.477) data 0.357 (0.347) loss_x loss_x 0.9404 (1.1113) acc_x 81.2500 (72.9688) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [25/29] time 0.379 (0.478) data 0.248 (0.347) loss_x loss_x 1.2803 (1.0946) acc_x 71.8750 (74.2500) lr 3.2699e-04 eta 0:00:01
epoch [149/200] batch [5/68] time 0.528 (0.469) data 0.396 (0.339) loss_u loss_u 0.8950 (0.9057) acc_u 12.5000 (11.2500) lr 3.2699e-04 eta 0:00:29
epoch [149/200] batch [10/68] time 0.407 (0.460) data 0.277 (0.329) loss_u loss_u 0.9111 (0.9301) acc_u 9.3750 (8.4375) lr 3.2699e-04 eta 0:00:26
epoch [149/200] batch [15/68] time 0.513 (0.458) data 0.382 (0.327) loss_u loss_u 0.8560 (0.9164) acc_u 15.6250 (10.2083) lr 3.2699e-04 eta 0:00:24
epoch [149/200] batch [20/68] time 0.403 (0.455) data 0.272 (0.324) loss_u loss_u 0.9243 (0.9141) acc_u 9.3750 (10.7812) lr 3.2699e-04 eta 0:00:21
epoch [149/200] batch [25/68] time 0.389 (0.455) data 0.258 (0.324) loss_u loss_u 0.9209 (0.9074) acc_u 9.3750 (11.7500) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [30/68] time 0.588 (0.456) data 0.457 (0.325) loss_u loss_u 0.9546 (0.9098) acc_u 3.1250 (11.0417) lr 3.2699e-04 eta 0:00:17
epoch [149/200] batch [35/68] time 0.413 (0.453) data 0.282 (0.322) loss_u loss_u 0.9194 (0.9080) acc_u 9.3750 (10.9821) lr 3.2699e-04 eta 0:00:14
epoch [149/200] batch [40/68] time 0.459 (0.452) data 0.327 (0.321) loss_u loss_u 0.8887 (0.9071) acc_u 15.6250 (11.4062) lr 3.2699e-04 eta 0:00:12
epoch [149/200] batch [45/68] time 0.446 (0.451) data 0.314 (0.320) loss_u loss_u 0.9551 (0.9091) acc_u 6.2500 (11.1806) lr 3.2699e-04 eta 0:00:10
epoch [149/200] batch [50/68] time 0.507 (0.452) data 0.376 (0.321) loss_u loss_u 0.9072 (0.9110) acc_u 9.3750 (10.8750) lr 3.2699e-04 eta 0:00:08
epoch [149/200] batch [55/68] time 0.546 (0.456) data 0.416 (0.326) loss_u loss_u 0.9419 (0.9125) acc_u 9.3750 (10.7955) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [60/68] time 0.470 (0.456) data 0.338 (0.325) loss_u loss_u 0.9307 (0.9138) acc_u 6.2500 (10.6771) lr 3.2699e-04 eta 0:00:03
epoch [149/200] batch [65/68] time 0.549 (0.457) data 0.419 (0.326) loss_u loss_u 0.8843 (0.9104) acc_u 12.5000 (11.0577) lr 3.2699e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1451
confident_label rate tensor(0.3036, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 952
clean true:945
clean false:7
clean_rate:0.9926470588235294
noisy true:740
noisy false:1444
after delete: len(clean_dataset) 952
after delete: len(noisy_dataset) 2184
epoch [150/200] batch [5/29] time 0.486 (0.457) data 0.355 (0.326) loss_x loss_x 1.4365 (1.0336) acc_x 68.7500 (75.6250) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [10/29] time 0.424 (0.446) data 0.294 (0.315) loss_x loss_x 1.1279 (1.0266) acc_x 78.1250 (75.9375) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [15/29] time 0.419 (0.459) data 0.288 (0.328) loss_x loss_x 0.7080 (0.9431) acc_x 87.5000 (76.4583) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [20/29] time 0.482 (0.464) data 0.351 (0.333) loss_x loss_x 1.8428 (1.0233) acc_x 68.7500 (75.1562) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [25/29] time 0.420 (0.463) data 0.289 (0.332) loss_x loss_x 1.3154 (1.0321) acc_x 59.3750 (74.5000) lr 3.1545e-04 eta 0:00:01
epoch [150/200] batch [5/68] time 0.440 (0.457) data 0.308 (0.326) loss_u loss_u 0.8379 (0.9054) acc_u 18.7500 (10.6250) lr 3.1545e-04 eta 0:00:28
epoch [150/200] batch [10/68] time 0.620 (0.462) data 0.489 (0.331) loss_u loss_u 0.9126 (0.9043) acc_u 12.5000 (11.2500) lr 3.1545e-04 eta 0:00:26
epoch [150/200] batch [15/68] time 0.432 (0.461) data 0.301 (0.330) loss_u loss_u 0.8735 (0.9091) acc_u 18.7500 (11.2500) lr 3.1545e-04 eta 0:00:24
epoch [150/200] batch [20/68] time 0.412 (0.456) data 0.281 (0.325) loss_u loss_u 0.9365 (0.9098) acc_u 6.2500 (10.9375) lr 3.1545e-04 eta 0:00:21
epoch [150/200] batch [25/68] time 0.464 (0.463) data 0.332 (0.332) loss_u loss_u 0.8989 (0.9001) acc_u 12.5000 (12.3750) lr 3.1545e-04 eta 0:00:19
epoch [150/200] batch [30/68] time 0.505 (0.461) data 0.375 (0.330) loss_u loss_u 0.8682 (0.9005) acc_u 12.5000 (12.1875) lr 3.1545e-04 eta 0:00:17
epoch [150/200] batch [35/68] time 0.492 (0.462) data 0.360 (0.331) loss_u loss_u 0.8916 (0.9010) acc_u 12.5000 (12.2321) lr 3.1545e-04 eta 0:00:15
epoch [150/200] batch [40/68] time 0.393 (0.461) data 0.261 (0.330) loss_u loss_u 0.9277 (0.9041) acc_u 9.3750 (11.7188) lr 3.1545e-04 eta 0:00:12
epoch [150/200] batch [45/68] time 0.428 (0.461) data 0.296 (0.330) loss_u loss_u 0.9048 (0.9036) acc_u 12.5000 (11.8750) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [50/68] time 0.385 (0.461) data 0.253 (0.330) loss_u loss_u 0.8623 (0.9041) acc_u 18.7500 (12.0625) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [55/68] time 0.413 (0.461) data 0.282 (0.330) loss_u loss_u 0.8462 (0.9018) acc_u 21.8750 (12.3295) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [60/68] time 0.330 (0.460) data 0.198 (0.329) loss_u loss_u 0.9326 (0.9027) acc_u 6.2500 (12.1354) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [65/68] time 0.504 (0.458) data 0.374 (0.327) loss_u loss_u 0.8921 (0.9036) acc_u 12.5000 (11.9231) lr 3.1545e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1440
confident_label rate tensor(0.3103, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 973
clean true:965
clean false:8
clean_rate:0.9917780061664954
noisy true:731
noisy false:1432
after delete: len(clean_dataset) 973
after delete: len(noisy_dataset) 2163
epoch [151/200] batch [5/30] time 0.381 (0.425) data 0.251 (0.294) loss_x loss_x 1.4189 (1.1378) acc_x 68.7500 (73.7500) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [10/30] time 0.504 (0.450) data 0.373 (0.320) loss_x loss_x 0.9624 (1.0343) acc_x 75.0000 (75.9375) lr 3.0409e-04 eta 0:00:09
epoch [151/200] batch [15/30] time 0.561 (0.456) data 0.430 (0.326) loss_x loss_x 0.4988 (1.0052) acc_x 93.7500 (77.2917) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [20/30] time 0.522 (0.472) data 0.391 (0.342) loss_x loss_x 1.0117 (1.0423) acc_x 75.0000 (75.6250) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [25/30] time 0.471 (0.470) data 0.341 (0.340) loss_x loss_x 1.0020 (0.9903) acc_x 81.2500 (76.8750) lr 3.0409e-04 eta 0:00:02
epoch [151/200] batch [30/30] time 0.574 (0.474) data 0.443 (0.343) loss_x loss_x 1.0283 (0.9868) acc_x 75.0000 (76.7708) lr 3.0409e-04 eta 0:00:00
epoch [151/200] batch [5/67] time 0.461 (0.471) data 0.330 (0.340) loss_u loss_u 0.9263 (0.9021) acc_u 9.3750 (11.8750) lr 3.0409e-04 eta 0:00:29
epoch [151/200] batch [10/67] time 0.539 (0.473) data 0.406 (0.342) loss_u loss_u 0.8696 (0.9006) acc_u 18.7500 (12.8125) lr 3.0409e-04 eta 0:00:26
epoch [151/200] batch [15/67] time 0.822 (0.476) data 0.689 (0.345) loss_u loss_u 0.8911 (0.9044) acc_u 9.3750 (11.8750) lr 3.0409e-04 eta 0:00:24
epoch [151/200] batch [20/67] time 0.523 (0.475) data 0.391 (0.343) loss_u loss_u 0.9185 (0.9037) acc_u 15.6250 (12.1875) lr 3.0409e-04 eta 0:00:22
epoch [151/200] batch [25/67] time 0.465 (0.473) data 0.333 (0.342) loss_u loss_u 0.9019 (0.9012) acc_u 12.5000 (12.6250) lr 3.0409e-04 eta 0:00:19
epoch [151/200] batch [30/67] time 0.453 (0.474) data 0.321 (0.343) loss_u loss_u 0.8633 (0.9029) acc_u 15.6250 (12.1875) lr 3.0409e-04 eta 0:00:17
epoch [151/200] batch [35/67] time 0.428 (0.477) data 0.296 (0.346) loss_u loss_u 0.9263 (0.9046) acc_u 9.3750 (12.0536) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [40/67] time 0.463 (0.475) data 0.331 (0.343) loss_u loss_u 0.8657 (0.9045) acc_u 15.6250 (11.9531) lr 3.0409e-04 eta 0:00:12
epoch [151/200] batch [45/67] time 0.583 (0.476) data 0.450 (0.344) loss_u loss_u 0.8545 (0.9041) acc_u 18.7500 (11.8056) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [50/67] time 0.319 (0.471) data 0.187 (0.339) loss_u loss_u 0.8384 (0.9040) acc_u 18.7500 (11.8750) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [55/67] time 0.439 (0.468) data 0.308 (0.337) loss_u loss_u 0.8813 (0.9034) acc_u 12.5000 (11.8750) lr 3.0409e-04 eta 0:00:05
epoch [151/200] batch [60/67] time 0.399 (0.467) data 0.268 (0.336) loss_u loss_u 0.9209 (0.9062) acc_u 6.2500 (11.4583) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [65/67] time 0.473 (0.466) data 0.343 (0.334) loss_u loss_u 0.9082 (0.9071) acc_u 9.3750 (11.3462) lr 3.0409e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1409
confident_label rate tensor(0.3176, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 996
clean true:987
clean false:9
clean_rate:0.9909638554216867
noisy true:740
noisy false:1400
after delete: len(clean_dataset) 996
after delete: len(noisy_dataset) 2140
epoch [152/200] batch [5/31] time 0.487 (0.491) data 0.355 (0.359) loss_x loss_x 0.8467 (0.9782) acc_x 81.2500 (77.5000) lr 2.9289e-04 eta 0:00:12
epoch [152/200] batch [10/31] time 0.462 (0.502) data 0.332 (0.371) loss_x loss_x 0.4932 (1.0625) acc_x 87.5000 (75.6250) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [15/31] time 0.483 (0.478) data 0.352 (0.347) loss_x loss_x 0.7949 (1.0773) acc_x 84.3750 (74.7917) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [20/31] time 0.421 (0.470) data 0.291 (0.339) loss_x loss_x 0.9604 (1.1228) acc_x 71.8750 (72.6562) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [25/31] time 0.628 (0.489) data 0.498 (0.358) loss_x loss_x 1.7031 (1.1471) acc_x 65.6250 (71.8750) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [30/31] time 0.449 (0.483) data 0.319 (0.353) loss_x loss_x 1.2881 (1.1250) acc_x 71.8750 (71.9792) lr 2.9289e-04 eta 0:00:00
epoch [152/200] batch [5/66] time 0.442 (0.477) data 0.311 (0.346) loss_u loss_u 0.8672 (0.9123) acc_u 15.6250 (11.8750) lr 2.9289e-04 eta 0:00:29
epoch [152/200] batch [10/66] time 0.764 (0.489) data 0.632 (0.358) loss_u loss_u 0.8970 (0.9002) acc_u 9.3750 (12.1875) lr 2.9289e-04 eta 0:00:27
epoch [152/200] batch [15/66] time 0.521 (0.485) data 0.389 (0.354) loss_u loss_u 0.9058 (0.8979) acc_u 9.3750 (12.2917) lr 2.9289e-04 eta 0:00:24
epoch [152/200] batch [20/66] time 0.451 (0.481) data 0.321 (0.350) loss_u loss_u 0.9258 (0.9030) acc_u 9.3750 (11.7188) lr 2.9289e-04 eta 0:00:22
epoch [152/200] batch [25/66] time 0.457 (0.477) data 0.326 (0.346) loss_u loss_u 0.8735 (0.9053) acc_u 12.5000 (11.1250) lr 2.9289e-04 eta 0:00:19
epoch [152/200] batch [30/66] time 0.431 (0.474) data 0.300 (0.343) loss_u loss_u 0.9492 (0.9090) acc_u 6.2500 (10.5208) lr 2.9289e-04 eta 0:00:17
epoch [152/200] batch [35/66] time 0.371 (0.469) data 0.241 (0.338) loss_u loss_u 0.9028 (0.9071) acc_u 12.5000 (10.9821) lr 2.9289e-04 eta 0:00:14
epoch [152/200] batch [40/66] time 0.419 (0.467) data 0.288 (0.336) loss_u loss_u 0.9072 (0.9078) acc_u 12.5000 (10.7812) lr 2.9289e-04 eta 0:00:12
epoch [152/200] batch [45/66] time 0.395 (0.468) data 0.263 (0.337) loss_u loss_u 0.9629 (0.9109) acc_u 3.1250 (10.4167) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [50/66] time 0.431 (0.469) data 0.300 (0.338) loss_u loss_u 0.9541 (0.9121) acc_u 9.3750 (10.3125) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [55/66] time 0.347 (0.469) data 0.215 (0.338) loss_u loss_u 0.8735 (0.9114) acc_u 15.6250 (10.6250) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [60/66] time 0.533 (0.468) data 0.401 (0.337) loss_u loss_u 0.8813 (0.9125) acc_u 15.6250 (10.4688) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [65/66] time 0.422 (0.468) data 0.290 (0.337) loss_u loss_u 0.9102 (0.9114) acc_u 15.6250 (10.8173) lr 2.9289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.3173, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 995
clean true:985
clean false:10
clean_rate:0.9899497487437185
noisy true:738
noisy false:1403
after delete: len(clean_dataset) 995
after delete: len(noisy_dataset) 2141
epoch [153/200] batch [5/31] time 0.399 (0.490) data 0.267 (0.359) loss_x loss_x 0.8052 (1.0407) acc_x 75.0000 (73.7500) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [10/31] time 0.596 (0.495) data 0.466 (0.364) loss_x loss_x 1.3203 (1.0769) acc_x 68.7500 (74.3750) lr 2.8187e-04 eta 0:00:10
epoch [153/200] batch [15/31] time 0.486 (0.483) data 0.355 (0.352) loss_x loss_x 1.1328 (1.0912) acc_x 71.8750 (73.5417) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [20/31] time 0.565 (0.477) data 0.433 (0.347) loss_x loss_x 1.3262 (1.0835) acc_x 71.8750 (73.7500) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [25/31] time 0.402 (0.469) data 0.271 (0.338) loss_x loss_x 1.5117 (1.1138) acc_x 59.3750 (72.5000) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [30/31] time 0.488 (0.461) data 0.357 (0.330) loss_x loss_x 1.0205 (1.1045) acc_x 71.8750 (72.0833) lr 2.8187e-04 eta 0:00:00
epoch [153/200] batch [5/66] time 0.388 (0.453) data 0.257 (0.322) loss_u loss_u 0.9668 (0.9328) acc_u 6.2500 (7.5000) lr 2.8187e-04 eta 0:00:27
epoch [153/200] batch [10/66] time 0.845 (0.468) data 0.713 (0.336) loss_u loss_u 0.9224 (0.9042) acc_u 12.5000 (13.1250) lr 2.8187e-04 eta 0:00:26
epoch [153/200] batch [15/66] time 0.427 (0.468) data 0.296 (0.337) loss_u loss_u 0.9336 (0.9014) acc_u 9.3750 (13.5417) lr 2.8187e-04 eta 0:00:23
epoch [153/200] batch [20/66] time 0.562 (0.472) data 0.430 (0.341) loss_u loss_u 0.9937 (0.9076) acc_u 0.0000 (12.8125) lr 2.8187e-04 eta 0:00:21
epoch [153/200] batch [25/66] time 0.374 (0.467) data 0.243 (0.336) loss_u loss_u 0.9180 (0.9080) acc_u 9.3750 (12.3750) lr 2.8187e-04 eta 0:00:19
epoch [153/200] batch [30/66] time 0.528 (0.465) data 0.396 (0.334) loss_u loss_u 0.8438 (0.9077) acc_u 25.0000 (12.6042) lr 2.8187e-04 eta 0:00:16
epoch [153/200] batch [35/66] time 0.365 (0.463) data 0.233 (0.332) loss_u loss_u 0.9258 (0.9107) acc_u 15.6250 (12.3214) lr 2.8187e-04 eta 0:00:14
epoch [153/200] batch [40/66] time 0.466 (0.466) data 0.335 (0.335) loss_u loss_u 0.9395 (0.9118) acc_u 9.3750 (12.1094) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [45/66] time 0.693 (0.467) data 0.561 (0.336) loss_u loss_u 0.8438 (0.9092) acc_u 21.8750 (12.5000) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [50/66] time 0.407 (0.471) data 0.277 (0.339) loss_u loss_u 0.9531 (0.9079) acc_u 6.2500 (12.6875) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [55/66] time 0.483 (0.470) data 0.352 (0.339) loss_u loss_u 0.9692 (0.9095) acc_u 3.1250 (12.3864) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [60/66] time 0.372 (0.467) data 0.241 (0.336) loss_u loss_u 0.9194 (0.9114) acc_u 9.3750 (12.0833) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [65/66] time 0.452 (0.471) data 0.321 (0.340) loss_u loss_u 0.8955 (0.9130) acc_u 12.5000 (11.8750) lr 2.8187e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1433
confident_label rate tensor(0.3138, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 984
clean true:972
clean false:12
clean_rate:0.9878048780487805
noisy true:731
noisy false:1421
after delete: len(clean_dataset) 984
after delete: len(noisy_dataset) 2152
epoch [154/200] batch [5/30] time 0.421 (0.467) data 0.290 (0.336) loss_x loss_x 0.5903 (1.0928) acc_x 93.7500 (79.3750) lr 2.7103e-04 eta 0:00:11
epoch [154/200] batch [10/30] time 0.486 (0.470) data 0.356 (0.340) loss_x loss_x 1.5449 (1.1204) acc_x 62.5000 (75.6250) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [15/30] time 0.491 (0.481) data 0.360 (0.351) loss_x loss_x 0.9946 (1.0821) acc_x 81.2500 (75.8333) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [20/30] time 0.442 (0.478) data 0.311 (0.347) loss_x loss_x 1.5254 (1.1219) acc_x 71.8750 (75.0000) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [25/30] time 0.497 (0.477) data 0.366 (0.346) loss_x loss_x 0.8911 (1.1133) acc_x 84.3750 (75.1250) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [30/30] time 0.503 (0.479) data 0.373 (0.348) loss_x loss_x 1.1289 (1.1278) acc_x 75.0000 (74.8958) lr 2.7103e-04 eta 0:00:00
epoch [154/200] batch [5/67] time 0.412 (0.469) data 0.282 (0.338) loss_u loss_u 0.8926 (0.9063) acc_u 12.5000 (14.3750) lr 2.7103e-04 eta 0:00:29
epoch [154/200] batch [10/67] time 0.739 (0.471) data 0.608 (0.340) loss_u loss_u 0.9414 (0.9240) acc_u 6.2500 (10.9375) lr 2.7103e-04 eta 0:00:26
epoch [154/200] batch [15/67] time 0.453 (0.476) data 0.321 (0.345) loss_u loss_u 0.9194 (0.9141) acc_u 12.5000 (12.5000) lr 2.7103e-04 eta 0:00:24
epoch [154/200] batch [20/67] time 0.701 (0.478) data 0.570 (0.347) loss_u loss_u 0.9243 (0.9092) acc_u 12.5000 (13.4375) lr 2.7103e-04 eta 0:00:22
epoch [154/200] batch [25/67] time 0.481 (0.477) data 0.350 (0.346) loss_u loss_u 0.9463 (0.9052) acc_u 9.3750 (13.7500) lr 2.7103e-04 eta 0:00:20
epoch [154/200] batch [30/67] time 0.394 (0.471) data 0.262 (0.340) loss_u loss_u 0.8804 (0.9081) acc_u 12.5000 (12.8125) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [35/67] time 0.426 (0.469) data 0.295 (0.339) loss_u loss_u 0.9526 (0.9085) acc_u 9.3750 (12.7679) lr 2.7103e-04 eta 0:00:15
epoch [154/200] batch [40/67] time 0.413 (0.472) data 0.282 (0.341) loss_u loss_u 0.8477 (0.9076) acc_u 15.6250 (13.0469) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [45/67] time 0.643 (0.471) data 0.513 (0.340) loss_u loss_u 0.8975 (0.9083) acc_u 12.5000 (12.9167) lr 2.7103e-04 eta 0:00:10
epoch [154/200] batch [50/67] time 0.538 (0.469) data 0.406 (0.338) loss_u loss_u 0.8325 (0.9069) acc_u 21.8750 (13.1250) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [55/67] time 0.448 (0.467) data 0.317 (0.336) loss_u loss_u 0.8794 (0.9073) acc_u 18.7500 (13.0114) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [60/67] time 0.427 (0.467) data 0.296 (0.336) loss_u loss_u 0.9082 (0.9075) acc_u 15.6250 (12.9167) lr 2.7103e-04 eta 0:00:03
epoch [154/200] batch [65/67] time 0.415 (0.467) data 0.284 (0.336) loss_u loss_u 0.9194 (0.9081) acc_u 6.2500 (12.7404) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1410
confident_label rate tensor(0.3166, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 993
clean true:984
clean false:9
clean_rate:0.9909365558912386
noisy true:742
noisy false:1401
after delete: len(clean_dataset) 993
after delete: len(noisy_dataset) 2143
epoch [155/200] batch [5/31] time 0.381 (0.438) data 0.251 (0.307) loss_x loss_x 1.0713 (1.1002) acc_x 78.1250 (76.8750) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [10/31] time 0.510 (0.496) data 0.379 (0.365) loss_x loss_x 1.0479 (1.0222) acc_x 78.1250 (77.5000) lr 2.6037e-04 eta 0:00:10
epoch [155/200] batch [15/31] time 0.415 (0.482) data 0.283 (0.351) loss_x loss_x 0.7705 (1.0426) acc_x 78.1250 (76.2500) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [20/31] time 0.485 (0.474) data 0.354 (0.343) loss_x loss_x 0.8862 (1.0497) acc_x 68.7500 (74.6875) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [25/31] time 0.439 (0.477) data 0.308 (0.346) loss_x loss_x 1.2764 (1.0725) acc_x 71.8750 (74.6250) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [30/31] time 0.463 (0.484) data 0.333 (0.353) loss_x loss_x 0.7666 (1.0807) acc_x 87.5000 (74.6875) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/66] time 0.418 (0.484) data 0.287 (0.353) loss_u loss_u 0.8960 (0.9288) acc_u 12.5000 (8.1250) lr 2.6037e-04 eta 0:00:29
epoch [155/200] batch [10/66] time 0.502 (0.479) data 0.371 (0.349) loss_u loss_u 0.8633 (0.9142) acc_u 18.7500 (10.3125) lr 2.6037e-04 eta 0:00:26
epoch [155/200] batch [15/66] time 0.394 (0.475) data 0.264 (0.344) loss_u loss_u 0.9561 (0.9182) acc_u 6.2500 (9.7917) lr 2.6037e-04 eta 0:00:24
epoch [155/200] batch [20/66] time 0.395 (0.474) data 0.263 (0.343) loss_u loss_u 0.8716 (0.9147) acc_u 15.6250 (10.6250) lr 2.6037e-04 eta 0:00:21
epoch [155/200] batch [25/66] time 0.472 (0.473) data 0.342 (0.342) loss_u loss_u 0.9316 (0.9112) acc_u 9.3750 (11.0000) lr 2.6037e-04 eta 0:00:19
epoch [155/200] batch [30/66] time 0.605 (0.475) data 0.474 (0.344) loss_u loss_u 0.9180 (0.9069) acc_u 9.3750 (11.5625) lr 2.6037e-04 eta 0:00:17
epoch [155/200] batch [35/66] time 0.365 (0.472) data 0.233 (0.341) loss_u loss_u 0.9370 (0.9117) acc_u 9.3750 (10.8929) lr 2.6037e-04 eta 0:00:14
epoch [155/200] batch [40/66] time 0.872 (0.476) data 0.739 (0.345) loss_u loss_u 0.9233 (0.9095) acc_u 9.3750 (11.1719) lr 2.6037e-04 eta 0:00:12
epoch [155/200] batch [45/66] time 0.377 (0.473) data 0.244 (0.342) loss_u loss_u 0.9077 (0.9120) acc_u 12.5000 (10.9028) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [50/66] time 0.447 (0.471) data 0.316 (0.340) loss_u loss_u 0.9238 (0.9103) acc_u 6.2500 (11.0625) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [55/66] time 0.487 (0.472) data 0.356 (0.341) loss_u loss_u 0.9219 (0.9074) acc_u 9.3750 (11.4773) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [60/66] time 0.433 (0.471) data 0.302 (0.340) loss_u loss_u 0.8848 (0.9090) acc_u 18.7500 (11.3542) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [65/66] time 0.465 (0.472) data 0.335 (0.341) loss_u loss_u 0.9058 (0.9080) acc_u 9.3750 (11.3942) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1451
confident_label rate tensor(0.3077, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 965
clean true:956
clean false:9
clean_rate:0.9906735751295337
noisy true:729
noisy false:1442
after delete: len(clean_dataset) 965
after delete: len(noisy_dataset) 2171
epoch [156/200] batch [5/30] time 0.437 (0.472) data 0.306 (0.341) loss_x loss_x 1.0371 (1.4984) acc_x 71.8750 (63.7500) lr 2.4989e-04 eta 0:00:11
epoch [156/200] batch [10/30] time 0.453 (0.460) data 0.322 (0.330) loss_x loss_x 1.1533 (1.2651) acc_x 68.7500 (70.6250) lr 2.4989e-04 eta 0:00:09
epoch [156/200] batch [15/30] time 0.444 (0.451) data 0.314 (0.320) loss_x loss_x 1.3867 (1.2133) acc_x 56.2500 (72.5000) lr 2.4989e-04 eta 0:00:06
epoch [156/200] batch [20/30] time 0.420 (0.450) data 0.290 (0.320) loss_x loss_x 1.1025 (1.1431) acc_x 78.1250 (74.2188) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [25/30] time 0.560 (0.454) data 0.430 (0.323) loss_x loss_x 1.0557 (1.0981) acc_x 71.8750 (75.0000) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [30/30] time 0.459 (0.457) data 0.329 (0.326) loss_x loss_x 1.2451 (1.1100) acc_x 65.6250 (74.3750) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/67] time 0.407 (0.449) data 0.276 (0.318) loss_u loss_u 0.9307 (0.9168) acc_u 9.3750 (10.0000) lr 2.4989e-04 eta 0:00:27
epoch [156/200] batch [10/67] time 0.452 (0.451) data 0.322 (0.320) loss_u loss_u 0.8589 (0.8966) acc_u 18.7500 (12.8125) lr 2.4989e-04 eta 0:00:25
epoch [156/200] batch [15/67] time 0.696 (0.462) data 0.566 (0.331) loss_u loss_u 0.9834 (0.9110) acc_u 0.0000 (10.8333) lr 2.4989e-04 eta 0:00:24
epoch [156/200] batch [20/67] time 0.649 (0.466) data 0.518 (0.336) loss_u loss_u 0.8965 (0.9030) acc_u 9.3750 (12.0312) lr 2.4989e-04 eta 0:00:21
epoch [156/200] batch [25/67] time 0.360 (0.470) data 0.228 (0.339) loss_u loss_u 0.9238 (0.9065) acc_u 6.2500 (11.6250) lr 2.4989e-04 eta 0:00:19
epoch [156/200] batch [30/67] time 0.506 (0.468) data 0.374 (0.337) loss_u loss_u 0.8530 (0.9096) acc_u 21.8750 (11.1458) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [35/67] time 0.426 (0.470) data 0.295 (0.339) loss_u loss_u 0.9448 (0.9103) acc_u 6.2500 (11.1607) lr 2.4989e-04 eta 0:00:15
epoch [156/200] batch [40/67] time 0.481 (0.471) data 0.351 (0.340) loss_u loss_u 0.8931 (0.9085) acc_u 15.6250 (11.5625) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [45/67] time 0.459 (0.468) data 0.327 (0.337) loss_u loss_u 0.8994 (0.9068) acc_u 12.5000 (11.6667) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [50/67] time 0.506 (0.467) data 0.375 (0.336) loss_u loss_u 0.8315 (0.9043) acc_u 21.8750 (12.0625) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [55/67] time 0.511 (0.466) data 0.380 (0.335) loss_u loss_u 0.9302 (0.9046) acc_u 9.3750 (11.8750) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [60/67] time 0.319 (0.466) data 0.187 (0.335) loss_u loss_u 0.9385 (0.9081) acc_u 6.2500 (11.4583) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [65/67] time 0.479 (0.464) data 0.348 (0.333) loss_u loss_u 0.9653 (0.9084) acc_u 6.2500 (11.3462) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1456
confident_label rate tensor(0.3061, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 960
clean true:951
clean false:9
clean_rate:0.990625
noisy true:729
noisy false:1447
after delete: len(clean_dataset) 960
after delete: len(noisy_dataset) 2176
epoch [157/200] batch [5/30] time 0.471 (0.516) data 0.340 (0.384) loss_x loss_x 1.6221 (1.1642) acc_x 56.2500 (66.8750) lr 2.3959e-04 eta 0:00:12
epoch [157/200] batch [10/30] time 0.386 (0.487) data 0.256 (0.356) loss_x loss_x 1.0850 (1.0526) acc_x 78.1250 (71.5625) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [15/30] time 0.552 (0.476) data 0.421 (0.345) loss_x loss_x 0.7520 (1.0604) acc_x 78.1250 (70.6250) lr 2.3959e-04 eta 0:00:07
epoch [157/200] batch [20/30] time 0.712 (0.475) data 0.581 (0.344) loss_x loss_x 1.0469 (1.0715) acc_x 71.8750 (71.0938) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [25/30] time 0.516 (0.466) data 0.385 (0.335) loss_x loss_x 0.9219 (1.0674) acc_x 84.3750 (71.8750) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [30/30] time 0.417 (0.464) data 0.285 (0.333) loss_x loss_x 0.7515 (1.0562) acc_x 75.0000 (71.6667) lr 2.3959e-04 eta 0:00:00
epoch [157/200] batch [5/68] time 0.716 (0.467) data 0.586 (0.337) loss_u loss_u 0.8955 (0.9081) acc_u 15.6250 (12.5000) lr 2.3959e-04 eta 0:00:29
epoch [157/200] batch [10/68] time 0.623 (0.472) data 0.492 (0.341) loss_u loss_u 0.9731 (0.9159) acc_u 6.2500 (11.2500) lr 2.3959e-04 eta 0:00:27
epoch [157/200] batch [15/68] time 0.411 (0.468) data 0.280 (0.337) loss_u loss_u 0.9233 (0.9130) acc_u 12.5000 (11.8750) lr 2.3959e-04 eta 0:00:24
epoch [157/200] batch [20/68] time 0.474 (0.464) data 0.342 (0.333) loss_u loss_u 0.9429 (0.9124) acc_u 3.1250 (11.8750) lr 2.3959e-04 eta 0:00:22
epoch [157/200] batch [25/68] time 0.547 (0.470) data 0.417 (0.339) loss_u loss_u 0.9766 (0.9129) acc_u 3.1250 (11.6250) lr 2.3959e-04 eta 0:00:20
epoch [157/200] batch [30/68] time 0.507 (0.471) data 0.376 (0.340) loss_u loss_u 0.9009 (0.9113) acc_u 12.5000 (11.7708) lr 2.3959e-04 eta 0:00:17
epoch [157/200] batch [35/68] time 0.456 (0.470) data 0.325 (0.339) loss_u loss_u 0.8096 (0.9054) acc_u 21.8750 (12.5000) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [40/68] time 0.455 (0.473) data 0.323 (0.342) loss_u loss_u 0.9780 (0.9059) acc_u 3.1250 (12.1875) lr 2.3959e-04 eta 0:00:13
epoch [157/200] batch [45/68] time 0.483 (0.475) data 0.349 (0.344) loss_u loss_u 0.8779 (0.9052) acc_u 15.6250 (12.1528) lr 2.3959e-04 eta 0:00:10
epoch [157/200] batch [50/68] time 0.365 (0.469) data 0.233 (0.338) loss_u loss_u 0.9409 (0.9057) acc_u 9.3750 (12.1250) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [55/68] time 0.595 (0.468) data 0.464 (0.337) loss_u loss_u 0.8486 (0.9041) acc_u 25.0000 (12.4432) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [60/68] time 0.414 (0.468) data 0.283 (0.337) loss_u loss_u 0.8809 (0.9028) acc_u 12.5000 (12.5000) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [65/68] time 0.381 (0.467) data 0.250 (0.335) loss_u loss_u 0.9507 (0.9027) acc_u 9.3750 (12.5962) lr 2.3959e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1449
confident_label rate tensor(0.3033, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 951
clean true:943
clean false:8
clean_rate:0.9915878023133544
noisy true:744
noisy false:1441
after delete: len(clean_dataset) 951
after delete: len(noisy_dataset) 2185
epoch [158/200] batch [5/29] time 0.399 (0.474) data 0.269 (0.343) loss_x loss_x 1.0840 (1.0372) acc_x 62.5000 (71.2500) lr 2.2949e-04 eta 0:00:11
epoch [158/200] batch [10/29] time 0.419 (0.465) data 0.289 (0.335) loss_x loss_x 0.6772 (0.9507) acc_x 84.3750 (73.7500) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [15/29] time 0.457 (0.470) data 0.327 (0.339) loss_x loss_x 0.8535 (0.9636) acc_x 84.3750 (72.9167) lr 2.2949e-04 eta 0:00:06
epoch [158/200] batch [20/29] time 0.463 (0.464) data 0.333 (0.333) loss_x loss_x 2.0254 (1.0163) acc_x 50.0000 (72.3438) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [25/29] time 0.636 (0.489) data 0.505 (0.358) loss_x loss_x 0.4407 (0.9879) acc_x 84.3750 (73.1250) lr 2.2949e-04 eta 0:00:01
epoch [158/200] batch [5/68] time 0.368 (0.479) data 0.237 (0.348) loss_u loss_u 0.8857 (0.9101) acc_u 15.6250 (11.8750) lr 2.2949e-04 eta 0:00:30
epoch [158/200] batch [10/68] time 0.638 (0.481) data 0.507 (0.350) loss_u loss_u 0.8955 (0.9095) acc_u 15.6250 (12.8125) lr 2.2949e-04 eta 0:00:27
epoch [158/200] batch [15/68] time 0.447 (0.476) data 0.316 (0.345) loss_u loss_u 0.8887 (0.8984) acc_u 15.6250 (13.9583) lr 2.2949e-04 eta 0:00:25
epoch [158/200] batch [20/68] time 0.362 (0.468) data 0.230 (0.337) loss_u loss_u 0.8760 (0.8944) acc_u 15.6250 (14.6875) lr 2.2949e-04 eta 0:00:22
epoch [158/200] batch [25/68] time 0.543 (0.471) data 0.411 (0.339) loss_u loss_u 0.9404 (0.9026) acc_u 6.2500 (12.8750) lr 2.2949e-04 eta 0:00:20
epoch [158/200] batch [30/68] time 0.686 (0.476) data 0.555 (0.345) loss_u loss_u 0.9272 (0.9069) acc_u 9.3750 (12.5000) lr 2.2949e-04 eta 0:00:18
epoch [158/200] batch [35/68] time 0.501 (0.470) data 0.371 (0.339) loss_u loss_u 0.8711 (0.9089) acc_u 15.6250 (12.1429) lr 2.2949e-04 eta 0:00:15
epoch [158/200] batch [40/68] time 0.419 (0.466) data 0.288 (0.335) loss_u loss_u 0.9160 (0.9058) acc_u 9.3750 (12.5000) lr 2.2949e-04 eta 0:00:13
epoch [158/200] batch [45/68] time 0.384 (0.463) data 0.252 (0.332) loss_u loss_u 0.9429 (0.9045) acc_u 6.2500 (12.8472) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [50/68] time 0.466 (0.463) data 0.334 (0.332) loss_u loss_u 0.8594 (0.9025) acc_u 18.7500 (13.0000) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [55/68] time 0.603 (0.463) data 0.471 (0.332) loss_u loss_u 0.9404 (0.9029) acc_u 6.2500 (12.8977) lr 2.2949e-04 eta 0:00:06
epoch [158/200] batch [60/68] time 0.445 (0.468) data 0.313 (0.336) loss_u loss_u 0.8052 (0.8996) acc_u 21.8750 (13.2292) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [65/68] time 0.378 (0.466) data 0.246 (0.335) loss_u loss_u 0.9336 (0.8990) acc_u 6.2500 (13.2692) lr 2.2949e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1401
confident_label rate tensor(0.3237, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1015
clean true:1003
clean false:12
clean_rate:0.9881773399014778
noisy true:732
noisy false:1389
after delete: len(clean_dataset) 1015
after delete: len(noisy_dataset) 2121
epoch [159/200] batch [5/31] time 0.418 (0.452) data 0.287 (0.321) loss_x loss_x 0.6616 (0.8939) acc_x 93.7500 (78.1250) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [10/31] time 0.387 (0.440) data 0.256 (0.309) loss_x loss_x 1.3447 (0.9980) acc_x 71.8750 (75.9375) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [15/31] time 0.426 (0.483) data 0.296 (0.352) loss_x loss_x 0.9995 (0.9793) acc_x 71.8750 (76.8750) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [20/31] time 0.497 (0.479) data 0.367 (0.348) loss_x loss_x 1.8857 (1.1037) acc_x 65.6250 (74.6875) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [25/31] time 0.504 (0.471) data 0.374 (0.340) loss_x loss_x 1.0811 (1.1075) acc_x 71.8750 (74.8750) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [30/31] time 0.478 (0.469) data 0.348 (0.339) loss_x loss_x 0.9546 (1.1231) acc_x 78.1250 (74.0625) lr 2.1957e-04 eta 0:00:00
epoch [159/200] batch [5/66] time 0.517 (0.463) data 0.386 (0.332) loss_u loss_u 0.8643 (0.8809) acc_u 15.6250 (15.0000) lr 2.1957e-04 eta 0:00:28
epoch [159/200] batch [10/66] time 0.639 (0.474) data 0.508 (0.343) loss_u loss_u 0.9863 (0.8953) acc_u 0.0000 (13.1250) lr 2.1957e-04 eta 0:00:26
epoch [159/200] batch [15/66] time 0.526 (0.473) data 0.395 (0.342) loss_u loss_u 0.9199 (0.9001) acc_u 12.5000 (12.2917) lr 2.1957e-04 eta 0:00:24
epoch [159/200] batch [20/66] time 0.377 (0.465) data 0.246 (0.334) loss_u loss_u 0.9570 (0.8976) acc_u 3.1250 (12.6562) lr 2.1957e-04 eta 0:00:21
epoch [159/200] batch [25/66] time 0.363 (0.461) data 0.232 (0.330) loss_u loss_u 0.9175 (0.9035) acc_u 9.3750 (12.1250) lr 2.1957e-04 eta 0:00:18
epoch [159/200] batch [30/66] time 0.553 (0.458) data 0.422 (0.327) loss_u loss_u 0.8682 (0.8990) acc_u 15.6250 (12.7083) lr 2.1957e-04 eta 0:00:16
epoch [159/200] batch [35/66] time 0.407 (0.461) data 0.276 (0.330) loss_u loss_u 0.9106 (0.9018) acc_u 9.3750 (12.1429) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [40/66] time 0.463 (0.458) data 0.331 (0.327) loss_u loss_u 0.8833 (0.9046) acc_u 12.5000 (11.7188) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [45/66] time 0.455 (0.456) data 0.323 (0.324) loss_u loss_u 0.9136 (0.9078) acc_u 6.2500 (11.5278) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [50/66] time 0.506 (0.455) data 0.374 (0.324) loss_u loss_u 0.9160 (0.9098) acc_u 9.3750 (11.3125) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [55/66] time 0.561 (0.458) data 0.430 (0.327) loss_u loss_u 0.9004 (0.9096) acc_u 12.5000 (11.3068) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [60/66] time 0.553 (0.459) data 0.422 (0.328) loss_u loss_u 0.9312 (0.9119) acc_u 9.3750 (10.8854) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [65/66] time 0.413 (0.457) data 0.281 (0.326) loss_u loss_u 0.9551 (0.9121) acc_u 6.2500 (10.9615) lr 2.1957e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1439
confident_label rate tensor(0.3141, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 985
clean true:975
clean false:10
clean_rate:0.9898477157360406
noisy true:722
noisy false:1429
after delete: len(clean_dataset) 985
after delete: len(noisy_dataset) 2151
epoch [160/200] batch [5/30] time 0.507 (0.511) data 0.376 (0.381) loss_x loss_x 0.9463 (1.1153) acc_x 78.1250 (75.0000) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [10/30] time 0.446 (0.480) data 0.316 (0.349) loss_x loss_x 0.9502 (1.0548) acc_x 84.3750 (76.5625) lr 2.0984e-04 eta 0:00:09
epoch [160/200] batch [15/30] time 0.380 (0.478) data 0.250 (0.347) loss_x loss_x 1.4121 (1.0021) acc_x 71.8750 (78.3333) lr 2.0984e-04 eta 0:00:07
epoch [160/200] batch [20/30] time 0.408 (0.467) data 0.277 (0.336) loss_x loss_x 1.2637 (1.0118) acc_x 59.3750 (76.8750) lr 2.0984e-04 eta 0:00:04
epoch [160/200] batch [25/30] time 0.345 (0.463) data 0.215 (0.333) loss_x loss_x 1.2021 (1.0545) acc_x 75.0000 (75.3750) lr 2.0984e-04 eta 0:00:02
epoch [160/200] batch [30/30] time 0.510 (0.468) data 0.379 (0.338) loss_x loss_x 1.0391 (1.0410) acc_x 71.8750 (75.4167) lr 2.0984e-04 eta 0:00:00
epoch [160/200] batch [5/67] time 0.413 (0.458) data 0.280 (0.327) loss_u loss_u 0.9375 (0.8975) acc_u 9.3750 (13.1250) lr 2.0984e-04 eta 0:00:28
epoch [160/200] batch [10/67] time 0.403 (0.460) data 0.273 (0.328) loss_u loss_u 0.9482 (0.9072) acc_u 6.2500 (11.5625) lr 2.0984e-04 eta 0:00:26
epoch [160/200] batch [15/67] time 0.378 (0.458) data 0.248 (0.327) loss_u loss_u 0.8809 (0.9062) acc_u 12.5000 (11.6667) lr 2.0984e-04 eta 0:00:23
epoch [160/200] batch [20/67] time 0.622 (0.463) data 0.491 (0.332) loss_u loss_u 0.9111 (0.9110) acc_u 12.5000 (11.2500) lr 2.0984e-04 eta 0:00:21
epoch [160/200] batch [25/67] time 0.505 (0.462) data 0.374 (0.331) loss_u loss_u 0.9404 (0.9139) acc_u 9.3750 (10.7500) lr 2.0984e-04 eta 0:00:19
epoch [160/200] batch [30/67] time 0.408 (0.460) data 0.277 (0.329) loss_u loss_u 0.9004 (0.9136) acc_u 15.6250 (11.0417) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [35/67] time 0.507 (0.461) data 0.376 (0.330) loss_u loss_u 0.8379 (0.9134) acc_u 18.7500 (10.9821) lr 2.0984e-04 eta 0:00:14
epoch [160/200] batch [40/67] time 0.366 (0.458) data 0.234 (0.327) loss_u loss_u 0.8633 (0.9108) acc_u 18.7500 (11.4062) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [45/67] time 0.496 (0.457) data 0.363 (0.326) loss_u loss_u 0.9624 (0.9144) acc_u 3.1250 (10.8333) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [50/67] time 0.442 (0.457) data 0.312 (0.326) loss_u loss_u 0.8965 (0.9118) acc_u 12.5000 (11.1250) lr 2.0984e-04 eta 0:00:07
epoch [160/200] batch [55/67] time 0.368 (0.467) data 0.238 (0.336) loss_u loss_u 0.9263 (0.9092) acc_u 9.3750 (11.4205) lr 2.0984e-04 eta 0:00:05
epoch [160/200] batch [60/67] time 0.441 (0.465) data 0.310 (0.334) loss_u loss_u 0.9341 (0.9083) acc_u 9.3750 (11.6146) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [65/67] time 0.464 (0.463) data 0.333 (0.332) loss_u loss_u 0.8569 (0.9062) acc_u 18.7500 (11.8750) lr 2.0984e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1462
confident_label rate tensor(0.3147, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 987
clean true:978
clean false:9
clean_rate:0.9908814589665653
noisy true:696
noisy false:1453
after delete: len(clean_dataset) 987
after delete: len(noisy_dataset) 2149
epoch [161/200] batch [5/30] time 0.527 (0.470) data 0.396 (0.339) loss_x loss_x 0.8765 (0.9915) acc_x 78.1250 (76.8750) lr 2.0032e-04 eta 0:00:11
epoch [161/200] batch [10/30] time 0.475 (0.472) data 0.344 (0.342) loss_x loss_x 1.0029 (1.0915) acc_x 75.0000 (75.3125) lr 2.0032e-04 eta 0:00:09
epoch [161/200] batch [15/30] time 0.435 (0.461) data 0.304 (0.330) loss_x loss_x 1.3496 (1.0903) acc_x 68.7500 (74.3750) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [20/30] time 0.532 (0.473) data 0.402 (0.342) loss_x loss_x 1.2012 (1.1334) acc_x 68.7500 (73.4375) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [25/30] time 0.393 (0.470) data 0.262 (0.339) loss_x loss_x 1.3965 (1.1414) acc_x 65.6250 (73.1250) lr 2.0032e-04 eta 0:00:02
epoch [161/200] batch [30/30] time 0.454 (0.464) data 0.324 (0.333) loss_x loss_x 1.3369 (1.1284) acc_x 53.1250 (73.0208) lr 2.0032e-04 eta 0:00:00
epoch [161/200] batch [5/67] time 0.535 (0.473) data 0.404 (0.342) loss_u loss_u 0.9072 (0.9374) acc_u 9.3750 (7.5000) lr 2.0032e-04 eta 0:00:29
epoch [161/200] batch [10/67] time 0.443 (0.471) data 0.313 (0.341) loss_u loss_u 0.9077 (0.9220) acc_u 9.3750 (8.7500) lr 2.0032e-04 eta 0:00:26
epoch [161/200] batch [15/67] time 0.432 (0.466) data 0.301 (0.335) loss_u loss_u 0.8691 (0.9120) acc_u 21.8750 (11.4583) lr 2.0032e-04 eta 0:00:24
epoch [161/200] batch [20/67] time 0.371 (0.472) data 0.241 (0.342) loss_u loss_u 0.9097 (0.9094) acc_u 9.3750 (11.2500) lr 2.0032e-04 eta 0:00:22
epoch [161/200] batch [25/67] time 0.448 (0.468) data 0.318 (0.337) loss_u loss_u 0.9707 (0.9108) acc_u 3.1250 (11.0000) lr 2.0032e-04 eta 0:00:19
epoch [161/200] batch [30/67] time 0.385 (0.471) data 0.253 (0.340) loss_u loss_u 0.9800 (0.9115) acc_u 3.1250 (10.9375) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [35/67] time 0.396 (0.467) data 0.264 (0.336) loss_u loss_u 0.9331 (0.9094) acc_u 9.3750 (11.0714) lr 2.0032e-04 eta 0:00:14
epoch [161/200] batch [40/67] time 0.494 (0.465) data 0.363 (0.334) loss_u loss_u 0.9087 (0.9124) acc_u 12.5000 (10.7031) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [45/67] time 0.477 (0.462) data 0.346 (0.330) loss_u loss_u 0.9067 (0.9113) acc_u 9.3750 (10.9028) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [50/67] time 0.443 (0.460) data 0.310 (0.329) loss_u loss_u 0.8618 (0.9086) acc_u 15.6250 (11.2500) lr 2.0032e-04 eta 0:00:07
epoch [161/200] batch [55/67] time 0.388 (0.458) data 0.257 (0.326) loss_u loss_u 0.8896 (0.9090) acc_u 15.6250 (11.4205) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [60/67] time 0.443 (0.458) data 0.313 (0.327) loss_u loss_u 0.9092 (0.9086) acc_u 9.3750 (11.4583) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [65/67] time 0.407 (0.458) data 0.276 (0.327) loss_u loss_u 0.9175 (0.9101) acc_u 12.5000 (11.2500) lr 2.0032e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.3109, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 975
clean true:962
clean false:13
clean_rate:0.9866666666666667
noisy true:761
noisy false:1400
after delete: len(clean_dataset) 975
after delete: len(noisy_dataset) 2161
epoch [162/200] batch [5/30] time 0.463 (0.487) data 0.332 (0.356) loss_x loss_x 0.9043 (1.0452) acc_x 75.0000 (75.6250) lr 1.9098e-04 eta 0:00:12
epoch [162/200] batch [10/30] time 0.385 (0.458) data 0.254 (0.327) loss_x loss_x 1.1328 (0.9480) acc_x 68.7500 (76.8750) lr 1.9098e-04 eta 0:00:09
epoch [162/200] batch [15/30] time 0.504 (0.467) data 0.372 (0.336) loss_x loss_x 0.7759 (0.9702) acc_x 87.5000 (76.4583) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [20/30] time 0.499 (0.469) data 0.369 (0.338) loss_x loss_x 1.0527 (0.9996) acc_x 75.0000 (75.1562) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [25/30] time 0.623 (0.469) data 0.492 (0.338) loss_x loss_x 1.4404 (1.0099) acc_x 62.5000 (74.8750) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [30/30] time 0.548 (0.472) data 0.417 (0.341) loss_x loss_x 1.5264 (1.0467) acc_x 65.6250 (74.3750) lr 1.9098e-04 eta 0:00:00
epoch [162/200] batch [5/67] time 0.591 (0.478) data 0.459 (0.347) loss_u loss_u 0.7983 (0.9026) acc_u 25.0000 (11.2500) lr 1.9098e-04 eta 0:00:29
epoch [162/200] batch [10/67] time 0.367 (0.467) data 0.236 (0.336) loss_u loss_u 0.8291 (0.9051) acc_u 21.8750 (11.2500) lr 1.9098e-04 eta 0:00:26
epoch [162/200] batch [15/67] time 0.458 (0.464) data 0.327 (0.333) loss_u loss_u 0.8628 (0.8957) acc_u 15.6250 (12.7083) lr 1.9098e-04 eta 0:00:24
epoch [162/200] batch [20/67] time 0.385 (0.459) data 0.253 (0.328) loss_u loss_u 0.9316 (0.8963) acc_u 6.2500 (12.8125) lr 1.9098e-04 eta 0:00:21
epoch [162/200] batch [25/67] time 0.413 (0.457) data 0.282 (0.326) loss_u loss_u 0.9385 (0.8974) acc_u 9.3750 (12.6250) lr 1.9098e-04 eta 0:00:19
epoch [162/200] batch [30/67] time 0.405 (0.454) data 0.274 (0.323) loss_u loss_u 0.8477 (0.8981) acc_u 18.7500 (12.5000) lr 1.9098e-04 eta 0:00:16
epoch [162/200] batch [35/67] time 0.486 (0.454) data 0.355 (0.323) loss_u loss_u 0.9731 (0.8974) acc_u 3.1250 (12.6786) lr 1.9098e-04 eta 0:00:14
epoch [162/200] batch [40/67] time 0.387 (0.454) data 0.256 (0.323) loss_u loss_u 0.9565 (0.8993) acc_u 3.1250 (12.3438) lr 1.9098e-04 eta 0:00:12
epoch [162/200] batch [45/67] time 0.477 (0.453) data 0.345 (0.322) loss_u loss_u 0.9292 (0.9029) acc_u 6.2500 (11.8750) lr 1.9098e-04 eta 0:00:09
epoch [162/200] batch [50/67] time 0.427 (0.455) data 0.296 (0.324) loss_u loss_u 0.8408 (0.8992) acc_u 18.7500 (12.3750) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [55/67] time 0.388 (0.459) data 0.258 (0.328) loss_u loss_u 0.8589 (0.9002) acc_u 18.7500 (12.2159) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [60/67] time 0.554 (0.461) data 0.423 (0.330) loss_u loss_u 0.9170 (0.9025) acc_u 9.3750 (11.8750) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [65/67] time 0.522 (0.458) data 0.391 (0.327) loss_u loss_u 0.9565 (0.9042) acc_u 6.2500 (11.7308) lr 1.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1428
confident_label rate tensor(0.3080, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 966
clean true:959
clean false:7
clean_rate:0.9927536231884058
noisy true:749
noisy false:1421
after delete: len(clean_dataset) 966
after delete: len(noisy_dataset) 2170
epoch [163/200] batch [5/30] time 0.494 (0.491) data 0.365 (0.361) loss_x loss_x 0.8823 (1.0039) acc_x 71.8750 (74.3750) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [10/30] time 0.365 (0.461) data 0.234 (0.330) loss_x loss_x 0.8516 (1.0122) acc_x 81.2500 (75.3125) lr 1.8185e-04 eta 0:00:09
epoch [163/200] batch [15/30] time 0.440 (0.452) data 0.309 (0.322) loss_x loss_x 1.2607 (1.0531) acc_x 71.8750 (74.5833) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [20/30] time 0.470 (0.460) data 0.339 (0.330) loss_x loss_x 0.8418 (1.0432) acc_x 84.3750 (75.1562) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [25/30] time 0.538 (0.471) data 0.407 (0.340) loss_x loss_x 0.7290 (1.0615) acc_x 81.2500 (75.0000) lr 1.8185e-04 eta 0:00:02
epoch [163/200] batch [30/30] time 0.603 (0.473) data 0.473 (0.343) loss_x loss_x 1.1660 (1.0447) acc_x 71.8750 (75.1042) lr 1.8185e-04 eta 0:00:00
epoch [163/200] batch [5/67] time 0.470 (0.468) data 0.340 (0.337) loss_u loss_u 0.8818 (0.8918) acc_u 12.5000 (13.7500) lr 1.8185e-04 eta 0:00:29
epoch [163/200] batch [10/67] time 0.475 (0.466) data 0.345 (0.336) loss_u loss_u 0.8745 (0.9021) acc_u 18.7500 (13.1250) lr 1.8185e-04 eta 0:00:26
epoch [163/200] batch [15/67] time 0.480 (0.471) data 0.349 (0.340) loss_u loss_u 0.8125 (0.8884) acc_u 25.0000 (14.3750) lr 1.8185e-04 eta 0:00:24
epoch [163/200] batch [20/67] time 0.592 (0.474) data 0.460 (0.343) loss_u loss_u 0.8794 (0.8891) acc_u 21.8750 (14.3750) lr 1.8185e-04 eta 0:00:22
epoch [163/200] batch [25/67] time 0.455 (0.473) data 0.322 (0.342) loss_u loss_u 0.8389 (0.8896) acc_u 21.8750 (14.2500) lr 1.8185e-04 eta 0:00:19
epoch [163/200] batch [30/67] time 0.512 (0.476) data 0.380 (0.345) loss_u loss_u 0.9102 (0.8956) acc_u 12.5000 (13.5417) lr 1.8185e-04 eta 0:00:17
epoch [163/200] batch [35/67] time 0.507 (0.473) data 0.375 (0.342) loss_u loss_u 0.9038 (0.9000) acc_u 15.6250 (13.0357) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [40/67] time 0.596 (0.474) data 0.465 (0.343) loss_u loss_u 0.9292 (0.9030) acc_u 6.2500 (12.5000) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [45/67] time 0.386 (0.468) data 0.254 (0.337) loss_u loss_u 0.9014 (0.9006) acc_u 12.5000 (12.8472) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [50/67] time 0.545 (0.468) data 0.413 (0.337) loss_u loss_u 0.8604 (0.9004) acc_u 12.5000 (12.6250) lr 1.8185e-04 eta 0:00:07
epoch [163/200] batch [55/67] time 0.433 (0.468) data 0.300 (0.337) loss_u loss_u 0.8940 (0.9005) acc_u 12.5000 (12.6136) lr 1.8185e-04 eta 0:00:05
epoch [163/200] batch [60/67] time 0.370 (0.465) data 0.239 (0.334) loss_u loss_u 0.8989 (0.8994) acc_u 9.3750 (12.7083) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [65/67] time 0.443 (0.463) data 0.312 (0.332) loss_u loss_u 0.8276 (0.9002) acc_u 18.7500 (12.5962) lr 1.8185e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1410
confident_label rate tensor(0.3048, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 956
clean true:949
clean false:7
clean_rate:0.9926778242677824
noisy true:777
noisy false:1403
after delete: len(clean_dataset) 956
after delete: len(noisy_dataset) 2180
epoch [164/200] batch [5/29] time 0.392 (0.472) data 0.261 (0.341) loss_x loss_x 0.9141 (1.1695) acc_x 75.0000 (71.2500) lr 1.7292e-04 eta 0:00:11
epoch [164/200] batch [10/29] time 0.394 (0.446) data 0.264 (0.315) loss_x loss_x 0.7119 (1.1423) acc_x 84.3750 (73.7500) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [15/29] time 0.375 (0.442) data 0.245 (0.311) loss_x loss_x 1.0078 (1.0862) acc_x 65.6250 (72.7083) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [20/29] time 0.448 (0.439) data 0.318 (0.309) loss_x loss_x 0.7148 (1.0603) acc_x 81.2500 (73.1250) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [25/29] time 0.479 (0.439) data 0.348 (0.309) loss_x loss_x 0.9839 (1.0554) acc_x 65.6250 (73.5000) lr 1.7292e-04 eta 0:00:01
epoch [164/200] batch [5/68] time 0.407 (0.452) data 0.276 (0.321) loss_u loss_u 0.7876 (0.8793) acc_u 25.0000 (16.2500) lr 1.7292e-04 eta 0:00:28
epoch [164/200] batch [10/68] time 0.469 (0.453) data 0.337 (0.323) loss_u loss_u 0.8428 (0.9035) acc_u 18.7500 (12.8125) lr 1.7292e-04 eta 0:00:26
epoch [164/200] batch [15/68] time 0.607 (0.460) data 0.477 (0.329) loss_u loss_u 0.9370 (0.9090) acc_u 9.3750 (12.2917) lr 1.7292e-04 eta 0:00:24
epoch [164/200] batch [20/68] time 0.544 (0.468) data 0.414 (0.338) loss_u loss_u 0.9175 (0.9112) acc_u 9.3750 (11.7188) lr 1.7292e-04 eta 0:00:22
epoch [164/200] batch [25/68] time 0.600 (0.465) data 0.468 (0.335) loss_u loss_u 0.9126 (0.9089) acc_u 12.5000 (11.7500) lr 1.7292e-04 eta 0:00:20
epoch [164/200] batch [30/68] time 0.592 (0.466) data 0.461 (0.335) loss_u loss_u 0.8711 (0.9082) acc_u 15.6250 (11.7708) lr 1.7292e-04 eta 0:00:17
epoch [164/200] batch [35/68] time 0.361 (0.463) data 0.230 (0.332) loss_u loss_u 0.9077 (0.9079) acc_u 9.3750 (11.9643) lr 1.7292e-04 eta 0:00:15
epoch [164/200] batch [40/68] time 0.364 (0.461) data 0.233 (0.330) loss_u loss_u 0.8530 (0.9043) acc_u 15.6250 (12.5000) lr 1.7292e-04 eta 0:00:12
epoch [164/200] batch [45/68] time 0.434 (0.462) data 0.303 (0.331) loss_u loss_u 0.9082 (0.9003) acc_u 12.5000 (13.1250) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [50/68] time 0.433 (0.461) data 0.301 (0.330) loss_u loss_u 0.8730 (0.8967) acc_u 12.5000 (13.6250) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [55/68] time 0.498 (0.466) data 0.366 (0.335) loss_u loss_u 0.9048 (0.8999) acc_u 12.5000 (13.1250) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [60/68] time 0.375 (0.463) data 0.245 (0.332) loss_u loss_u 0.8926 (0.9006) acc_u 15.6250 (13.0729) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [65/68] time 0.391 (0.465) data 0.260 (0.334) loss_u loss_u 0.8979 (0.9002) acc_u 12.5000 (13.0288) lr 1.7292e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1437
confident_label rate tensor(0.3125, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 980
clean true:974
clean false:6
clean_rate:0.9938775510204082
noisy true:725
noisy false:1431
after delete: len(clean_dataset) 980
after delete: len(noisy_dataset) 2156
epoch [165/200] batch [5/30] time 0.452 (0.468) data 0.321 (0.337) loss_x loss_x 1.3408 (1.0511) acc_x 56.2500 (71.8750) lr 1.6419e-04 eta 0:00:11
epoch [165/200] batch [10/30] time 0.451 (0.467) data 0.319 (0.336) loss_x loss_x 1.0420 (1.1446) acc_x 62.5000 (70.6250) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [15/30] time 0.371 (0.470) data 0.241 (0.339) loss_x loss_x 1.9932 (1.1337) acc_x 50.0000 (71.0417) lr 1.6419e-04 eta 0:00:07
epoch [165/200] batch [20/30] time 0.499 (0.465) data 0.369 (0.334) loss_x loss_x 0.6274 (1.1189) acc_x 87.5000 (72.6562) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [25/30] time 0.571 (0.472) data 0.441 (0.341) loss_x loss_x 1.2412 (1.0822) acc_x 62.5000 (73.0000) lr 1.6419e-04 eta 0:00:02
epoch [165/200] batch [30/30] time 0.418 (0.466) data 0.288 (0.336) loss_x loss_x 0.7183 (1.0905) acc_x 84.3750 (72.5000) lr 1.6419e-04 eta 0:00:00
epoch [165/200] batch [5/67] time 0.352 (0.456) data 0.221 (0.325) loss_u loss_u 0.8608 (0.9224) acc_u 15.6250 (8.1250) lr 1.6419e-04 eta 0:00:28
epoch [165/200] batch [10/67] time 0.596 (0.459) data 0.464 (0.328) loss_u loss_u 0.9062 (0.9084) acc_u 9.3750 (11.5625) lr 1.6419e-04 eta 0:00:26
epoch [165/200] batch [15/67] time 0.374 (0.455) data 0.242 (0.324) loss_u loss_u 0.8975 (0.9112) acc_u 15.6250 (11.4583) lr 1.6419e-04 eta 0:00:23
epoch [165/200] batch [20/67] time 0.383 (0.454) data 0.252 (0.323) loss_u loss_u 0.9624 (0.9150) acc_u 6.2500 (11.0938) lr 1.6419e-04 eta 0:00:21
epoch [165/200] batch [25/67] time 0.447 (0.459) data 0.315 (0.328) loss_u loss_u 0.8711 (0.9148) acc_u 15.6250 (10.7500) lr 1.6419e-04 eta 0:00:19
epoch [165/200] batch [30/67] time 0.392 (0.460) data 0.260 (0.329) loss_u loss_u 0.8682 (0.9157) acc_u 18.7500 (10.5208) lr 1.6419e-04 eta 0:00:17
epoch [165/200] batch [35/67] time 0.394 (0.457) data 0.263 (0.326) loss_u loss_u 0.8687 (0.9124) acc_u 15.6250 (11.1607) lr 1.6419e-04 eta 0:00:14
epoch [165/200] batch [40/67] time 0.425 (0.454) data 0.294 (0.323) loss_u loss_u 0.9136 (0.9122) acc_u 9.3750 (11.1719) lr 1.6419e-04 eta 0:00:12
epoch [165/200] batch [45/67] time 0.601 (0.454) data 0.468 (0.323) loss_u loss_u 0.8633 (0.9101) acc_u 15.6250 (11.5278) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [50/67] time 0.465 (0.454) data 0.335 (0.322) loss_u loss_u 0.9175 (0.9079) acc_u 9.3750 (11.7500) lr 1.6419e-04 eta 0:00:07
epoch [165/200] batch [55/67] time 0.384 (0.453) data 0.253 (0.322) loss_u loss_u 0.8354 (0.9052) acc_u 18.7500 (11.9886) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [60/67] time 0.490 (0.455) data 0.360 (0.324) loss_u loss_u 0.9150 (0.9037) acc_u 6.2500 (12.1354) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [65/67] time 0.622 (0.456) data 0.490 (0.325) loss_u loss_u 0.8921 (0.9029) acc_u 12.5000 (12.0673) lr 1.6419e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1462
confident_label rate tensor(0.3115, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 977
clean true:968
clean false:9
clean_rate:0.9907881269191402
noisy true:706
noisy false:1453
after delete: len(clean_dataset) 977
after delete: len(noisy_dataset) 2159
epoch [166/200] batch [5/30] time 0.574 (0.486) data 0.443 (0.356) loss_x loss_x 0.9814 (0.9529) acc_x 75.0000 (76.2500) lr 1.5567e-04 eta 0:00:12
epoch [166/200] batch [10/30] time 0.396 (0.468) data 0.265 (0.337) loss_x loss_x 0.9468 (0.9865) acc_x 75.0000 (74.3750) lr 1.5567e-04 eta 0:00:09
epoch [166/200] batch [15/30] time 0.455 (0.464) data 0.325 (0.333) loss_x loss_x 1.1084 (0.9948) acc_x 78.1250 (75.4167) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [20/30] time 0.490 (0.457) data 0.359 (0.326) loss_x loss_x 1.8604 (1.0394) acc_x 59.3750 (74.6875) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [25/30] time 0.404 (0.453) data 0.273 (0.323) loss_x loss_x 1.1172 (1.0500) acc_x 78.1250 (74.6250) lr 1.5567e-04 eta 0:00:02
epoch [166/200] batch [30/30] time 0.564 (0.464) data 0.433 (0.333) loss_x loss_x 1.1406 (1.0670) acc_x 71.8750 (74.3750) lr 1.5567e-04 eta 0:00:00
epoch [166/200] batch [5/67] time 0.384 (0.461) data 0.253 (0.330) loss_u loss_u 0.9219 (0.8776) acc_u 9.3750 (15.0000) lr 1.5567e-04 eta 0:00:28
epoch [166/200] batch [10/67] time 0.477 (0.465) data 0.346 (0.334) loss_u loss_u 0.9248 (0.9013) acc_u 9.3750 (12.1875) lr 1.5567e-04 eta 0:00:26
epoch [166/200] batch [15/67] time 0.414 (0.462) data 0.283 (0.331) loss_u loss_u 0.9131 (0.9056) acc_u 9.3750 (11.2500) lr 1.5567e-04 eta 0:00:24
epoch [166/200] batch [20/67] time 0.427 (0.463) data 0.296 (0.332) loss_u loss_u 0.9463 (0.9082) acc_u 6.2500 (11.5625) lr 1.5567e-04 eta 0:00:21
epoch [166/200] batch [25/67] time 0.415 (0.461) data 0.283 (0.330) loss_u loss_u 0.9126 (0.9115) acc_u 9.3750 (11.1250) lr 1.5567e-04 eta 0:00:19
epoch [166/200] batch [30/67] time 0.585 (0.460) data 0.455 (0.329) loss_u loss_u 0.8979 (0.9153) acc_u 9.3750 (10.6250) lr 1.5567e-04 eta 0:00:17
epoch [166/200] batch [35/67] time 0.442 (0.461) data 0.310 (0.330) loss_u loss_u 0.8911 (0.9157) acc_u 18.7500 (10.8929) lr 1.5567e-04 eta 0:00:14
epoch [166/200] batch [40/67] time 0.420 (0.462) data 0.289 (0.331) loss_u loss_u 0.9009 (0.9173) acc_u 9.3750 (10.5469) lr 1.5567e-04 eta 0:00:12
epoch [166/200] batch [45/67] time 0.477 (0.463) data 0.345 (0.332) loss_u loss_u 0.8062 (0.9156) acc_u 25.0000 (10.8333) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [50/67] time 0.504 (0.462) data 0.371 (0.331) loss_u loss_u 0.8687 (0.9144) acc_u 15.6250 (10.9375) lr 1.5567e-04 eta 0:00:07
epoch [166/200] batch [55/67] time 0.354 (0.460) data 0.224 (0.329) loss_u loss_u 0.9141 (0.9157) acc_u 15.6250 (10.8523) lr 1.5567e-04 eta 0:00:05
epoch [166/200] batch [60/67] time 0.455 (0.457) data 0.323 (0.326) loss_u loss_u 0.9453 (0.9139) acc_u 3.1250 (11.1979) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [65/67] time 0.387 (0.460) data 0.257 (0.329) loss_u loss_u 0.8818 (0.9113) acc_u 15.6250 (11.5385) lr 1.5567e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1396
confident_label rate tensor(0.3176, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 996
clean true:987
clean false:9
clean_rate:0.9909638554216867
noisy true:753
noisy false:1387
after delete: len(clean_dataset) 996
after delete: len(noisy_dataset) 2140
epoch [167/200] batch [5/31] time 0.600 (0.470) data 0.470 (0.339) loss_x loss_x 0.7935 (1.3099) acc_x 84.3750 (68.7500) lr 1.4736e-04 eta 0:00:12
epoch [167/200] batch [10/31] time 0.376 (0.461) data 0.245 (0.330) loss_x loss_x 1.3818 (1.2360) acc_x 68.7500 (70.0000) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [15/31] time 0.500 (0.458) data 0.369 (0.327) loss_x loss_x 1.1709 (1.2220) acc_x 78.1250 (71.4583) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [20/31] time 0.627 (0.468) data 0.497 (0.338) loss_x loss_x 1.3184 (1.1531) acc_x 68.7500 (73.1250) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [25/31] time 0.436 (0.473) data 0.305 (0.342) loss_x loss_x 1.2773 (1.1147) acc_x 65.6250 (73.7500) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [30/31] time 0.392 (0.474) data 0.262 (0.343) loss_x loss_x 1.0586 (1.1012) acc_x 84.3750 (73.8542) lr 1.4736e-04 eta 0:00:00
epoch [167/200] batch [5/66] time 0.433 (0.467) data 0.303 (0.337) loss_u loss_u 0.8701 (0.9022) acc_u 15.6250 (12.5000) lr 1.4736e-04 eta 0:00:28
epoch [167/200] batch [10/66] time 0.405 (0.467) data 0.273 (0.336) loss_u loss_u 0.9668 (0.9111) acc_u 6.2500 (10.9375) lr 1.4736e-04 eta 0:00:26
epoch [167/200] batch [15/66] time 0.365 (0.464) data 0.234 (0.333) loss_u loss_u 0.9219 (0.9032) acc_u 12.5000 (12.2917) lr 1.4736e-04 eta 0:00:23
epoch [167/200] batch [20/66] time 0.439 (0.465) data 0.307 (0.334) loss_u loss_u 0.8789 (0.9064) acc_u 21.8750 (12.6562) lr 1.4736e-04 eta 0:00:21
epoch [167/200] batch [25/66] time 0.532 (0.476) data 0.401 (0.345) loss_u loss_u 0.8784 (0.9058) acc_u 21.8750 (12.8750) lr 1.4736e-04 eta 0:00:19
epoch [167/200] batch [30/66] time 0.419 (0.479) data 0.287 (0.348) loss_u loss_u 0.9497 (0.9069) acc_u 6.2500 (12.7083) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [35/66] time 0.457 (0.481) data 0.326 (0.349) loss_u loss_u 0.9326 (0.9114) acc_u 6.2500 (12.0536) lr 1.4736e-04 eta 0:00:14
epoch [167/200] batch [40/66] time 0.463 (0.477) data 0.332 (0.346) loss_u loss_u 0.8833 (0.9109) acc_u 18.7500 (12.1875) lr 1.4736e-04 eta 0:00:12
epoch [167/200] batch [45/66] time 0.517 (0.476) data 0.387 (0.345) loss_u loss_u 0.8955 (0.9111) acc_u 12.5000 (12.0139) lr 1.4736e-04 eta 0:00:09
epoch [167/200] batch [50/66] time 0.652 (0.473) data 0.520 (0.342) loss_u loss_u 0.8721 (0.9069) acc_u 15.6250 (12.5000) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [55/66] time 0.573 (0.476) data 0.442 (0.345) loss_u loss_u 0.9683 (0.9107) acc_u 6.2500 (12.0455) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [60/66] time 0.438 (0.476) data 0.308 (0.345) loss_u loss_u 0.7915 (0.9089) acc_u 25.0000 (12.2917) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [65/66] time 0.382 (0.473) data 0.252 (0.342) loss_u loss_u 0.9023 (0.9096) acc_u 9.3750 (12.1154) lr 1.4736e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1388
confident_label rate tensor(0.3237, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1015
clean true:1003
clean false:12
clean_rate:0.9881773399014778
noisy true:745
noisy false:1376
after delete: len(clean_dataset) 1015
after delete: len(noisy_dataset) 2121
epoch [168/200] batch [5/31] time 0.619 (0.513) data 0.489 (0.382) loss_x loss_x 1.0615 (1.1128) acc_x 71.8750 (72.5000) lr 1.3926e-04 eta 0:00:13
epoch [168/200] batch [10/31] time 0.392 (0.491) data 0.261 (0.360) loss_x loss_x 1.0605 (1.0306) acc_x 75.0000 (75.9375) lr 1.3926e-04 eta 0:00:10
epoch [168/200] batch [15/31] time 0.501 (0.484) data 0.371 (0.354) loss_x loss_x 0.5786 (0.9878) acc_x 93.7500 (76.6667) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [20/31] time 0.412 (0.472) data 0.282 (0.341) loss_x loss_x 0.9033 (1.0028) acc_x 81.2500 (75.6250) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [25/31] time 0.476 (0.477) data 0.345 (0.346) loss_x loss_x 0.6318 (1.0324) acc_x 90.6250 (74.8750) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [30/31] time 0.413 (0.478) data 0.283 (0.348) loss_x loss_x 1.2920 (1.0614) acc_x 65.6250 (73.8542) lr 1.3926e-04 eta 0:00:00
epoch [168/200] batch [5/66] time 0.837 (0.488) data 0.707 (0.358) loss_u loss_u 0.9673 (0.9082) acc_u 6.2500 (11.2500) lr 1.3926e-04 eta 0:00:29
epoch [168/200] batch [10/66] time 0.441 (0.488) data 0.309 (0.357) loss_u loss_u 0.9375 (0.9090) acc_u 9.3750 (11.5625) lr 1.3926e-04 eta 0:00:27
epoch [168/200] batch [15/66] time 0.448 (0.485) data 0.318 (0.355) loss_u loss_u 0.7729 (0.8938) acc_u 25.0000 (13.1250) lr 1.3926e-04 eta 0:00:24
epoch [168/200] batch [20/66] time 0.473 (0.491) data 0.339 (0.360) loss_u loss_u 0.8633 (0.8958) acc_u 21.8750 (12.9688) lr 1.3926e-04 eta 0:00:22
epoch [168/200] batch [25/66] time 0.367 (0.483) data 0.235 (0.352) loss_u loss_u 0.8945 (0.8967) acc_u 21.8750 (13.1250) lr 1.3926e-04 eta 0:00:19
epoch [168/200] batch [30/66] time 0.424 (0.477) data 0.293 (0.346) loss_u loss_u 0.8853 (0.9003) acc_u 15.6250 (12.7083) lr 1.3926e-04 eta 0:00:17
epoch [168/200] batch [35/66] time 0.450 (0.477) data 0.319 (0.346) loss_u loss_u 0.7466 (0.8975) acc_u 31.2500 (13.1250) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [40/66] time 0.443 (0.475) data 0.311 (0.344) loss_u loss_u 0.8989 (0.8972) acc_u 15.6250 (13.1250) lr 1.3926e-04 eta 0:00:12
epoch [168/200] batch [45/66] time 0.341 (0.471) data 0.209 (0.340) loss_u loss_u 0.9238 (0.9002) acc_u 6.2500 (12.9167) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [50/66] time 0.485 (0.471) data 0.353 (0.340) loss_u loss_u 0.9058 (0.9040) acc_u 15.6250 (12.5000) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [55/66] time 0.375 (0.470) data 0.245 (0.339) loss_u loss_u 0.8804 (0.9054) acc_u 18.7500 (12.2727) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [60/66] time 0.406 (0.469) data 0.275 (0.338) loss_u loss_u 0.8721 (0.9061) acc_u 18.7500 (12.1875) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [65/66] time 0.384 (0.469) data 0.253 (0.338) loss_u loss_u 0.8672 (0.9072) acc_u 15.6250 (11.9712) lr 1.3926e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1407
confident_label rate tensor(0.3119, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 978
clean true:974
clean false:4
clean_rate:0.9959100204498977
noisy true:755
noisy false:1403
after delete: len(clean_dataset) 978
after delete: len(noisy_dataset) 2158
epoch [169/200] batch [5/30] time 0.466 (0.496) data 0.335 (0.365) loss_x loss_x 0.9224 (0.8997) acc_x 78.1250 (80.6250) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [10/30] time 0.405 (0.466) data 0.274 (0.335) loss_x loss_x 1.0205 (0.9729) acc_x 71.8750 (77.5000) lr 1.3137e-04 eta 0:00:09
epoch [169/200] batch [15/30] time 0.559 (0.461) data 0.428 (0.330) loss_x loss_x 1.3037 (0.9970) acc_x 71.8750 (77.5000) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [20/30] time 0.520 (0.471) data 0.390 (0.340) loss_x loss_x 0.5684 (0.9918) acc_x 90.6250 (76.5625) lr 1.3137e-04 eta 0:00:04
epoch [169/200] batch [25/30] time 0.507 (0.474) data 0.376 (0.343) loss_x loss_x 1.0908 (1.0123) acc_x 78.1250 (76.3750) lr 1.3137e-04 eta 0:00:02
epoch [169/200] batch [30/30] time 0.414 (0.475) data 0.283 (0.344) loss_x loss_x 0.5532 (1.0053) acc_x 87.5000 (76.6667) lr 1.3137e-04 eta 0:00:00
epoch [169/200] batch [5/67] time 0.630 (0.469) data 0.498 (0.338) loss_u loss_u 0.9321 (0.9062) acc_u 3.1250 (10.6250) lr 1.3137e-04 eta 0:00:29
epoch [169/200] batch [10/67] time 0.551 (0.472) data 0.421 (0.342) loss_u loss_u 0.9458 (0.9145) acc_u 6.2500 (10.3125) lr 1.3137e-04 eta 0:00:26
epoch [169/200] batch [15/67] time 0.364 (0.476) data 0.233 (0.345) loss_u loss_u 0.8584 (0.9127) acc_u 21.8750 (10.8333) lr 1.3137e-04 eta 0:00:24
epoch [169/200] batch [20/67] time 0.426 (0.477) data 0.295 (0.346) loss_u loss_u 0.9438 (0.9131) acc_u 6.2500 (10.9375) lr 1.3137e-04 eta 0:00:22
epoch [169/200] batch [25/67] time 0.621 (0.475) data 0.490 (0.344) loss_u loss_u 0.9302 (0.9163) acc_u 9.3750 (10.3750) lr 1.3137e-04 eta 0:00:19
epoch [169/200] batch [30/67] time 0.466 (0.472) data 0.335 (0.341) loss_u loss_u 0.9556 (0.9147) acc_u 6.2500 (10.7292) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [35/67] time 0.420 (0.468) data 0.289 (0.337) loss_u loss_u 0.8770 (0.9132) acc_u 15.6250 (10.8929) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [40/67] time 0.444 (0.465) data 0.314 (0.334) loss_u loss_u 0.9033 (0.9104) acc_u 9.3750 (11.3281) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [45/67] time 0.467 (0.465) data 0.335 (0.334) loss_u loss_u 0.9434 (0.9137) acc_u 6.2500 (10.8333) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [50/67] time 0.487 (0.465) data 0.355 (0.334) loss_u loss_u 0.8677 (0.9116) acc_u 18.7500 (11.3125) lr 1.3137e-04 eta 0:00:07
epoch [169/200] batch [55/67] time 0.555 (0.466) data 0.424 (0.335) loss_u loss_u 0.9487 (0.9123) acc_u 6.2500 (11.3068) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [60/67] time 0.448 (0.472) data 0.316 (0.341) loss_u loss_u 0.9116 (0.9099) acc_u 9.3750 (11.6667) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [65/67] time 0.516 (0.474) data 0.385 (0.343) loss_u loss_u 0.9297 (0.9111) acc_u 9.3750 (11.4904) lr 1.3137e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1427
confident_label rate tensor(0.3090, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 969
clean true:959
clean false:10
clean_rate:0.9896800825593395
noisy true:750
noisy false:1417
after delete: len(clean_dataset) 969
after delete: len(noisy_dataset) 2167
epoch [170/200] batch [5/30] time 0.490 (0.502) data 0.358 (0.369) loss_x loss_x 1.2500 (1.1025) acc_x 68.7500 (75.0000) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [10/30] time 0.449 (0.477) data 0.318 (0.344) loss_x loss_x 1.0527 (1.1406) acc_x 68.7500 (73.4375) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [15/30] time 0.411 (0.472) data 0.280 (0.340) loss_x loss_x 0.5728 (1.0862) acc_x 84.3750 (74.3750) lr 1.2369e-04 eta 0:00:07
epoch [170/200] batch [20/30] time 0.432 (0.461) data 0.300 (0.329) loss_x loss_x 1.0439 (1.0678) acc_x 78.1250 (75.1562) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [25/30] time 0.379 (0.450) data 0.247 (0.319) loss_x loss_x 1.5967 (1.1115) acc_x 71.8750 (73.8750) lr 1.2369e-04 eta 0:00:02
epoch [170/200] batch [30/30] time 0.626 (0.471) data 0.495 (0.339) loss_x loss_x 0.8057 (1.1088) acc_x 78.1250 (73.5417) lr 1.2369e-04 eta 0:00:00
epoch [170/200] batch [5/67] time 0.368 (0.459) data 0.236 (0.328) loss_u loss_u 0.9316 (0.9048) acc_u 6.2500 (11.2500) lr 1.2369e-04 eta 0:00:28
epoch [170/200] batch [10/67] time 0.446 (0.458) data 0.316 (0.326) loss_u loss_u 0.8994 (0.9082) acc_u 12.5000 (11.2500) lr 1.2369e-04 eta 0:00:26
epoch [170/200] batch [15/67] time 0.412 (0.453) data 0.280 (0.321) loss_u loss_u 0.8633 (0.9108) acc_u 12.5000 (10.4167) lr 1.2369e-04 eta 0:00:23
epoch [170/200] batch [20/67] time 0.586 (0.467) data 0.454 (0.336) loss_u loss_u 0.8047 (0.9010) acc_u 25.0000 (11.8750) lr 1.2369e-04 eta 0:00:21
epoch [170/200] batch [25/67] time 0.431 (0.471) data 0.300 (0.340) loss_u loss_u 0.8906 (0.9022) acc_u 15.6250 (11.7500) lr 1.2369e-04 eta 0:00:19
epoch [170/200] batch [30/67] time 0.410 (0.468) data 0.279 (0.337) loss_u loss_u 0.8521 (0.9004) acc_u 18.7500 (12.0833) lr 1.2369e-04 eta 0:00:17
epoch [170/200] batch [35/67] time 0.456 (0.467) data 0.325 (0.335) loss_u loss_u 0.8271 (0.8985) acc_u 18.7500 (12.4107) lr 1.2369e-04 eta 0:00:14
epoch [170/200] batch [40/67] time 0.444 (0.465) data 0.312 (0.334) loss_u loss_u 0.9600 (0.9030) acc_u 9.3750 (11.9531) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [45/67] time 0.718 (0.465) data 0.587 (0.334) loss_u loss_u 0.8867 (0.9021) acc_u 15.6250 (12.2917) lr 1.2369e-04 eta 0:00:10
epoch [170/200] batch [50/67] time 0.485 (0.467) data 0.353 (0.336) loss_u loss_u 0.9561 (0.9043) acc_u 3.1250 (12.0625) lr 1.2369e-04 eta 0:00:07
epoch [170/200] batch [55/67] time 0.352 (0.465) data 0.222 (0.334) loss_u loss_u 0.9087 (0.9048) acc_u 12.5000 (11.9318) lr 1.2369e-04 eta 0:00:05
epoch [170/200] batch [60/67] time 0.591 (0.465) data 0.459 (0.333) loss_u loss_u 0.9404 (0.9037) acc_u 6.2500 (12.1354) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [65/67] time 0.404 (0.466) data 0.272 (0.335) loss_u loss_u 0.9585 (0.9047) acc_u 6.2500 (12.0673) lr 1.2369e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1407
confident_label rate tensor(0.3160, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 991
clean true:979
clean false:12
clean_rate:0.987891019172553
noisy true:750
noisy false:1395
after delete: len(clean_dataset) 991
after delete: len(noisy_dataset) 2145
epoch [171/200] batch [5/30] time 0.676 (0.539) data 0.546 (0.408) loss_x loss_x 1.2422 (0.9609) acc_x 71.8750 (78.1250) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [10/30] time 0.472 (0.496) data 0.341 (0.365) loss_x loss_x 1.3691 (1.0803) acc_x 65.6250 (75.0000) lr 1.1623e-04 eta 0:00:09
epoch [171/200] batch [15/30] time 0.448 (0.471) data 0.317 (0.340) loss_x loss_x 0.9038 (1.0270) acc_x 81.2500 (77.2917) lr 1.1623e-04 eta 0:00:07
epoch [171/200] batch [20/30] time 0.335 (0.464) data 0.204 (0.334) loss_x loss_x 0.8252 (0.9984) acc_x 81.2500 (77.3438) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [25/30] time 0.401 (0.457) data 0.270 (0.326) loss_x loss_x 1.0117 (1.0356) acc_x 75.0000 (76.3750) lr 1.1623e-04 eta 0:00:02
epoch [171/200] batch [30/30] time 0.514 (0.462) data 0.383 (0.331) loss_x loss_x 0.9219 (1.0323) acc_x 81.2500 (76.2500) lr 1.1623e-04 eta 0:00:00
epoch [171/200] batch [5/67] time 0.473 (0.456) data 0.340 (0.325) loss_u loss_u 0.8525 (0.8929) acc_u 28.1250 (16.2500) lr 1.1623e-04 eta 0:00:28
epoch [171/200] batch [10/67] time 0.400 (0.455) data 0.268 (0.325) loss_u loss_u 0.9438 (0.8920) acc_u 6.2500 (14.0625) lr 1.1623e-04 eta 0:00:25
epoch [171/200] batch [15/67] time 0.445 (0.456) data 0.313 (0.325) loss_u loss_u 0.8647 (0.8947) acc_u 21.8750 (13.9583) lr 1.1623e-04 eta 0:00:23
epoch [171/200] batch [20/67] time 0.512 (0.458) data 0.380 (0.327) loss_u loss_u 0.9385 (0.9027) acc_u 6.2500 (12.3438) lr 1.1623e-04 eta 0:00:21
epoch [171/200] batch [25/67] time 0.381 (0.451) data 0.250 (0.321) loss_u loss_u 0.9478 (0.9037) acc_u 9.3750 (12.6250) lr 1.1623e-04 eta 0:00:18
epoch [171/200] batch [30/67] time 0.415 (0.453) data 0.284 (0.322) loss_u loss_u 0.9482 (0.9049) acc_u 6.2500 (12.2917) lr 1.1623e-04 eta 0:00:16
epoch [171/200] batch [35/67] time 0.507 (0.451) data 0.376 (0.320) loss_u loss_u 0.9028 (0.9048) acc_u 9.3750 (12.4107) lr 1.1623e-04 eta 0:00:14
epoch [171/200] batch [40/67] time 0.454 (0.449) data 0.323 (0.318) loss_u loss_u 0.9717 (0.9102) acc_u 3.1250 (11.7188) lr 1.1623e-04 eta 0:00:12
epoch [171/200] batch [45/67] time 0.767 (0.457) data 0.635 (0.326) loss_u loss_u 0.8779 (0.9085) acc_u 9.3750 (11.7361) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [50/67] time 0.417 (0.456) data 0.286 (0.325) loss_u loss_u 0.8472 (0.9038) acc_u 18.7500 (12.5625) lr 1.1623e-04 eta 0:00:07
epoch [171/200] batch [55/67] time 0.401 (0.456) data 0.270 (0.325) loss_u loss_u 0.9116 (0.9047) acc_u 15.6250 (12.5000) lr 1.1623e-04 eta 0:00:05
epoch [171/200] batch [60/67] time 0.450 (0.456) data 0.319 (0.325) loss_u loss_u 0.9321 (0.9071) acc_u 9.3750 (12.1875) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [65/67] time 0.316 (0.454) data 0.185 (0.323) loss_u loss_u 0.7983 (0.9053) acc_u 25.0000 (12.4038) lr 1.1623e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1444
confident_label rate tensor(0.3147, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 987
clean true:980
clean false:7
clean_rate:0.9929078014184397
noisy true:712
noisy false:1437
after delete: len(clean_dataset) 987
after delete: len(noisy_dataset) 2149
epoch [172/200] batch [5/30] time 0.454 (0.538) data 0.323 (0.407) loss_x loss_x 1.0000 (1.0835) acc_x 78.1250 (73.7500) lr 1.0899e-04 eta 0:00:13
epoch [172/200] batch [10/30] time 0.468 (0.500) data 0.338 (0.370) loss_x loss_x 0.6270 (1.1066) acc_x 90.6250 (75.9375) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [15/30] time 0.429 (0.491) data 0.298 (0.360) loss_x loss_x 1.3369 (1.0662) acc_x 68.7500 (74.5833) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [20/30] time 0.499 (0.489) data 0.369 (0.358) loss_x loss_x 1.1709 (1.0652) acc_x 71.8750 (74.3750) lr 1.0899e-04 eta 0:00:04
epoch [172/200] batch [25/30] time 0.531 (0.487) data 0.400 (0.356) loss_x loss_x 1.0479 (1.0577) acc_x 78.1250 (74.2500) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [30/30] time 0.474 (0.482) data 0.343 (0.351) loss_x loss_x 0.7930 (1.0517) acc_x 81.2500 (74.5833) lr 1.0899e-04 eta 0:00:00
epoch [172/200] batch [5/67] time 0.459 (0.482) data 0.327 (0.351) loss_u loss_u 0.8872 (0.9045) acc_u 15.6250 (12.5000) lr 1.0899e-04 eta 0:00:29
epoch [172/200] batch [10/67] time 0.389 (0.490) data 0.257 (0.360) loss_u loss_u 0.9331 (0.9128) acc_u 9.3750 (11.2500) lr 1.0899e-04 eta 0:00:27
epoch [172/200] batch [15/67] time 0.599 (0.488) data 0.468 (0.357) loss_u loss_u 0.9727 (0.9167) acc_u 3.1250 (10.8333) lr 1.0899e-04 eta 0:00:25
epoch [172/200] batch [20/67] time 0.389 (0.481) data 0.257 (0.350) loss_u loss_u 0.9092 (0.9154) acc_u 9.3750 (11.0938) lr 1.0899e-04 eta 0:00:22
epoch [172/200] batch [25/67] time 0.445 (0.478) data 0.314 (0.347) loss_u loss_u 0.8862 (0.9123) acc_u 15.6250 (11.5000) lr 1.0899e-04 eta 0:00:20
epoch [172/200] batch [30/67] time 0.628 (0.477) data 0.497 (0.346) loss_u loss_u 0.9087 (0.9131) acc_u 12.5000 (11.3542) lr 1.0899e-04 eta 0:00:17
epoch [172/200] batch [35/67] time 0.437 (0.474) data 0.306 (0.343) loss_u loss_u 0.9307 (0.9134) acc_u 9.3750 (11.3393) lr 1.0899e-04 eta 0:00:15
epoch [172/200] batch [40/67] time 0.398 (0.473) data 0.266 (0.342) loss_u loss_u 0.9253 (0.9095) acc_u 9.3750 (12.1094) lr 1.0899e-04 eta 0:00:12
epoch [172/200] batch [45/67] time 0.375 (0.470) data 0.243 (0.339) loss_u loss_u 0.8970 (0.9096) acc_u 15.6250 (12.2222) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [50/67] time 0.447 (0.469) data 0.316 (0.338) loss_u loss_u 0.9390 (0.9109) acc_u 9.3750 (11.9375) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [55/67] time 0.423 (0.473) data 0.292 (0.342) loss_u loss_u 0.9478 (0.9088) acc_u 9.3750 (12.1023) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [60/67] time 0.338 (0.471) data 0.207 (0.340) loss_u loss_u 0.8271 (0.9073) acc_u 25.0000 (12.2396) lr 1.0899e-04 eta 0:00:03
epoch [172/200] batch [65/67] time 0.507 (0.472) data 0.376 (0.341) loss_u loss_u 0.9009 (0.9066) acc_u 15.6250 (12.3558) lr 1.0899e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1435
confident_label rate tensor(0.3144, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 986
clean true:973
clean false:13
clean_rate:0.986815415821501
noisy true:728
noisy false:1422
after delete: len(clean_dataset) 986
after delete: len(noisy_dataset) 2150
epoch [173/200] batch [5/30] time 0.495 (0.463) data 0.365 (0.332) loss_x loss_x 1.6318 (1.0653) acc_x 62.5000 (73.1250) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [10/30] time 0.496 (0.453) data 0.365 (0.323) loss_x loss_x 0.5054 (1.1682) acc_x 87.5000 (72.1875) lr 1.0197e-04 eta 0:00:09
epoch [173/200] batch [15/30] time 0.445 (0.457) data 0.314 (0.326) loss_x loss_x 1.1816 (1.1395) acc_x 75.0000 (72.7083) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [20/30] time 0.395 (0.463) data 0.264 (0.332) loss_x loss_x 1.3926 (1.1617) acc_x 71.8750 (72.5000) lr 1.0197e-04 eta 0:00:04
epoch [173/200] batch [25/30] time 0.523 (0.463) data 0.393 (0.333) loss_x loss_x 0.3650 (1.1372) acc_x 93.7500 (73.3750) lr 1.0197e-04 eta 0:00:02
epoch [173/200] batch [30/30] time 0.451 (0.474) data 0.320 (0.343) loss_x loss_x 0.7842 (1.1356) acc_x 84.3750 (73.6458) lr 1.0197e-04 eta 0:00:00
epoch [173/200] batch [5/67] time 0.445 (0.477) data 0.314 (0.346) loss_u loss_u 0.9023 (0.9029) acc_u 9.3750 (12.5000) lr 1.0197e-04 eta 0:00:29
epoch [173/200] batch [10/67] time 0.556 (0.478) data 0.425 (0.348) loss_u loss_u 0.9277 (0.9053) acc_u 9.3750 (12.8125) lr 1.0197e-04 eta 0:00:27
epoch [173/200] batch [15/67] time 0.379 (0.477) data 0.247 (0.346) loss_u loss_u 0.8672 (0.9038) acc_u 15.6250 (12.2917) lr 1.0197e-04 eta 0:00:24
epoch [173/200] batch [20/67] time 0.476 (0.478) data 0.344 (0.347) loss_u loss_u 0.9258 (0.9049) acc_u 9.3750 (12.1875) lr 1.0197e-04 eta 0:00:22
epoch [173/200] batch [25/67] time 0.535 (0.479) data 0.403 (0.348) loss_u loss_u 0.8911 (0.9042) acc_u 15.6250 (12.5000) lr 1.0197e-04 eta 0:00:20
epoch [173/200] batch [30/67] time 0.431 (0.475) data 0.300 (0.344) loss_u loss_u 0.9009 (0.9048) acc_u 12.5000 (12.2917) lr 1.0197e-04 eta 0:00:17
epoch [173/200] batch [35/67] time 0.369 (0.472) data 0.237 (0.341) loss_u loss_u 0.8477 (0.9046) acc_u 21.8750 (12.5000) lr 1.0197e-04 eta 0:00:15
epoch [173/200] batch [40/67] time 0.690 (0.473) data 0.559 (0.342) loss_u loss_u 0.9780 (0.9017) acc_u 3.1250 (12.7344) lr 1.0197e-04 eta 0:00:12
epoch [173/200] batch [45/67] time 0.737 (0.477) data 0.606 (0.346) loss_u loss_u 0.8945 (0.9035) acc_u 15.6250 (12.7083) lr 1.0197e-04 eta 0:00:10
epoch [173/200] batch [50/67] time 0.382 (0.473) data 0.250 (0.342) loss_u loss_u 0.8521 (0.9049) acc_u 18.7500 (12.5625) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [55/67] time 0.358 (0.470) data 0.226 (0.338) loss_u loss_u 0.9297 (0.9062) acc_u 9.3750 (12.3864) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [60/67] time 0.550 (0.470) data 0.418 (0.339) loss_u loss_u 0.8428 (0.9068) acc_u 21.8750 (12.2396) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [65/67] time 0.527 (0.474) data 0.396 (0.342) loss_u loss_u 0.9297 (0.9072) acc_u 12.5000 (12.2596) lr 1.0197e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1409
confident_label rate tensor(0.3128, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 981
clean true:965
clean false:16
clean_rate:0.9836901121304791
noisy true:762
noisy false:1393
after delete: len(clean_dataset) 981
after delete: len(noisy_dataset) 2155
epoch [174/200] batch [5/30] time 0.473 (0.497) data 0.343 (0.366) loss_x loss_x 0.9067 (1.1406) acc_x 84.3750 (71.2500) lr 9.5173e-05 eta 0:00:12
epoch [174/200] batch [10/30] time 0.415 (0.469) data 0.284 (0.338) loss_x loss_x 0.7646 (1.0377) acc_x 84.3750 (72.8125) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [15/30] time 0.739 (0.485) data 0.608 (0.354) loss_x loss_x 1.7363 (1.1575) acc_x 71.8750 (72.2917) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [20/30] time 0.416 (0.470) data 0.285 (0.339) loss_x loss_x 1.5459 (1.1480) acc_x 65.6250 (72.9688) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [25/30] time 0.377 (0.458) data 0.246 (0.327) loss_x loss_x 1.1562 (1.1410) acc_x 75.0000 (72.8750) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [30/30] time 0.394 (0.453) data 0.264 (0.322) loss_x loss_x 1.1709 (1.1306) acc_x 68.7500 (72.9167) lr 9.5173e-05 eta 0:00:00
epoch [174/200] batch [5/67] time 0.580 (0.463) data 0.448 (0.332) loss_u loss_u 0.9312 (0.9047) acc_u 12.5000 (11.8750) lr 9.5173e-05 eta 0:00:28
epoch [174/200] batch [10/67] time 0.485 (0.470) data 0.353 (0.339) loss_u loss_u 0.9170 (0.9171) acc_u 9.3750 (10.0000) lr 9.5173e-05 eta 0:00:26
epoch [174/200] batch [15/67] time 0.469 (0.468) data 0.338 (0.337) loss_u loss_u 0.8989 (0.9202) acc_u 12.5000 (10.2083) lr 9.5173e-05 eta 0:00:24
epoch [174/200] batch [20/67] time 0.368 (0.468) data 0.237 (0.337) loss_u loss_u 0.8218 (0.9109) acc_u 21.8750 (11.0938) lr 9.5173e-05 eta 0:00:21
epoch [174/200] batch [25/67] time 0.383 (0.467) data 0.252 (0.336) loss_u loss_u 0.8994 (0.9061) acc_u 12.5000 (11.6250) lr 9.5173e-05 eta 0:00:19
epoch [174/200] batch [30/67] time 0.388 (0.465) data 0.257 (0.333) loss_u loss_u 0.9155 (0.9054) acc_u 12.5000 (11.5625) lr 9.5173e-05 eta 0:00:17
epoch [174/200] batch [35/67] time 0.457 (0.463) data 0.326 (0.332) loss_u loss_u 0.8931 (0.9078) acc_u 12.5000 (11.4286) lr 9.5173e-05 eta 0:00:14
epoch [174/200] batch [40/67] time 0.382 (0.460) data 0.252 (0.329) loss_u loss_u 0.8901 (0.9061) acc_u 15.6250 (11.7969) lr 9.5173e-05 eta 0:00:12
epoch [174/200] batch [45/67] time 0.492 (0.465) data 0.362 (0.333) loss_u loss_u 0.9102 (0.9061) acc_u 12.5000 (12.0139) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [50/67] time 0.379 (0.463) data 0.247 (0.332) loss_u loss_u 0.9424 (0.9076) acc_u 6.2500 (11.9375) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [55/67] time 0.569 (0.466) data 0.437 (0.335) loss_u loss_u 0.9585 (0.9078) acc_u 3.1250 (11.7614) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [60/67] time 0.420 (0.463) data 0.287 (0.332) loss_u loss_u 0.8159 (0.9037) acc_u 25.0000 (12.2917) lr 9.5173e-05 eta 0:00:03
epoch [174/200] batch [65/67] time 0.672 (0.465) data 0.541 (0.334) loss_u loss_u 0.7676 (0.8999) acc_u 28.1250 (12.7404) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1426
confident_label rate tensor(0.3186, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 999
clean true:985
clean false:14
clean_rate:0.985985985985986
noisy true:725
noisy false:1412
after delete: len(clean_dataset) 999
after delete: len(noisy_dataset) 2137
epoch [175/200] batch [5/31] time 0.419 (0.439) data 0.289 (0.309) loss_x loss_x 0.8799 (1.1742) acc_x 81.2500 (71.8750) lr 8.8597e-05 eta 0:00:11
epoch [175/200] batch [10/31] time 0.403 (0.434) data 0.272 (0.304) loss_x loss_x 1.0586 (1.1377) acc_x 75.0000 (74.0625) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [15/31] time 0.383 (0.439) data 0.253 (0.309) loss_x loss_x 1.0303 (1.1025) acc_x 75.0000 (75.2083) lr 8.8597e-05 eta 0:00:07
epoch [175/200] batch [20/31] time 0.458 (0.451) data 0.328 (0.321) loss_x loss_x 0.8608 (1.0549) acc_x 81.2500 (76.5625) lr 8.8597e-05 eta 0:00:04
epoch [175/200] batch [25/31] time 0.454 (0.464) data 0.324 (0.333) loss_x loss_x 1.4043 (1.0712) acc_x 75.0000 (75.5000) lr 8.8597e-05 eta 0:00:02
epoch [175/200] batch [30/31] time 0.438 (0.460) data 0.308 (0.330) loss_x loss_x 1.1562 (1.0689) acc_x 65.6250 (74.7917) lr 8.8597e-05 eta 0:00:00
epoch [175/200] batch [5/66] time 0.541 (0.452) data 0.410 (0.321) loss_u loss_u 0.8535 (0.9072) acc_u 18.7500 (11.8750) lr 8.8597e-05 eta 0:00:27
epoch [175/200] batch [10/66] time 0.667 (0.456) data 0.537 (0.325) loss_u loss_u 0.8081 (0.9008) acc_u 25.0000 (13.4375) lr 8.8597e-05 eta 0:00:25
epoch [175/200] batch [15/66] time 0.350 (0.453) data 0.218 (0.322) loss_u loss_u 0.9258 (0.9096) acc_u 9.3750 (12.0833) lr 8.8597e-05 eta 0:00:23
epoch [175/200] batch [20/66] time 0.415 (0.450) data 0.279 (0.319) loss_u loss_u 0.9180 (0.9054) acc_u 12.5000 (12.6562) lr 8.8597e-05 eta 0:00:20
epoch [175/200] batch [25/66] time 0.796 (0.461) data 0.661 (0.330) loss_u loss_u 0.9097 (0.9099) acc_u 12.5000 (12.1250) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [30/66] time 0.400 (0.463) data 0.269 (0.332) loss_u loss_u 0.8975 (0.9092) acc_u 12.5000 (12.2917) lr 8.8597e-05 eta 0:00:16
epoch [175/200] batch [35/66] time 0.495 (0.464) data 0.363 (0.333) loss_u loss_u 0.8311 (0.9094) acc_u 25.0000 (12.2321) lr 8.8597e-05 eta 0:00:14
epoch [175/200] batch [40/66] time 0.483 (0.463) data 0.352 (0.332) loss_u loss_u 0.9551 (0.9089) acc_u 6.2500 (12.1094) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [45/66] time 0.471 (0.463) data 0.340 (0.332) loss_u loss_u 0.9131 (0.9097) acc_u 12.5000 (12.0139) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [50/66] time 0.746 (0.467) data 0.614 (0.336) loss_u loss_u 0.8750 (0.9052) acc_u 15.6250 (12.6875) lr 8.8597e-05 eta 0:00:07
epoch [175/200] batch [55/66] time 0.481 (0.470) data 0.348 (0.339) loss_u loss_u 0.9199 (0.9067) acc_u 9.3750 (12.4432) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [60/66] time 0.562 (0.471) data 0.431 (0.340) loss_u loss_u 0.9399 (0.9073) acc_u 9.3750 (12.3958) lr 8.8597e-05 eta 0:00:02
epoch [175/200] batch [65/66] time 0.530 (0.471) data 0.398 (0.340) loss_u loss_u 0.9395 (0.9072) acc_u 6.2500 (12.4038) lr 8.8597e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1435
confident_label rate tensor(0.3151, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 988
clean true:976
clean false:12
clean_rate:0.9878542510121457
noisy true:725
noisy false:1423
after delete: len(clean_dataset) 988
after delete: len(noisy_dataset) 2148
epoch [176/200] batch [5/30] time 0.365 (0.476) data 0.234 (0.346) loss_x loss_x 1.0996 (1.0671) acc_x 71.8750 (73.7500) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [10/30] time 0.648 (0.501) data 0.517 (0.370) loss_x loss_x 1.3008 (1.1746) acc_x 65.6250 (71.5625) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [15/30] time 0.556 (0.505) data 0.425 (0.375) loss_x loss_x 0.6787 (1.1460) acc_x 84.3750 (71.4583) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [20/30] time 0.406 (0.479) data 0.275 (0.349) loss_x loss_x 1.2207 (1.1456) acc_x 65.6250 (71.7188) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [25/30] time 0.457 (0.478) data 0.326 (0.347) loss_x loss_x 1.3291 (1.1772) acc_x 71.8750 (71.7500) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [30/30] time 0.470 (0.479) data 0.339 (0.348) loss_x loss_x 1.6045 (1.1689) acc_x 68.7500 (71.8750) lr 8.2245e-05 eta 0:00:00
epoch [176/200] batch [5/67] time 0.427 (0.472) data 0.295 (0.341) loss_u loss_u 0.9014 (0.9201) acc_u 12.5000 (11.2500) lr 8.2245e-05 eta 0:00:29
epoch [176/200] batch [10/67] time 0.411 (0.470) data 0.279 (0.339) loss_u loss_u 0.9180 (0.9335) acc_u 9.3750 (9.6875) lr 8.2245e-05 eta 0:00:26
epoch [176/200] batch [15/67] time 0.356 (0.461) data 0.225 (0.331) loss_u loss_u 0.8979 (0.9198) acc_u 12.5000 (11.2500) lr 8.2245e-05 eta 0:00:23
epoch [176/200] batch [20/67] time 0.395 (0.459) data 0.264 (0.328) loss_u loss_u 0.9053 (0.9191) acc_u 15.6250 (11.4062) lr 8.2245e-05 eta 0:00:21
epoch [176/200] batch [25/67] time 0.625 (0.462) data 0.495 (0.332) loss_u loss_u 0.9751 (0.9201) acc_u 3.1250 (11.0000) lr 8.2245e-05 eta 0:00:19
epoch [176/200] batch [30/67] time 0.451 (0.461) data 0.320 (0.330) loss_u loss_u 0.8950 (0.9159) acc_u 25.0000 (11.7708) lr 8.2245e-05 eta 0:00:17
epoch [176/200] batch [35/67] time 0.395 (0.461) data 0.262 (0.330) loss_u loss_u 0.8867 (0.9153) acc_u 15.6250 (11.8750) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [40/67] time 0.414 (0.467) data 0.283 (0.336) loss_u loss_u 0.8662 (0.9156) acc_u 15.6250 (11.6406) lr 8.2245e-05 eta 0:00:12
epoch [176/200] batch [45/67] time 0.393 (0.469) data 0.261 (0.338) loss_u loss_u 0.8721 (0.9148) acc_u 25.0000 (11.8750) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [50/67] time 0.502 (0.469) data 0.371 (0.337) loss_u loss_u 0.9092 (0.9173) acc_u 18.7500 (11.5000) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [55/67] time 0.482 (0.469) data 0.350 (0.338) loss_u loss_u 0.9360 (0.9153) acc_u 6.2500 (11.6477) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [60/67] time 0.456 (0.473) data 0.325 (0.342) loss_u loss_u 0.8486 (0.9139) acc_u 15.6250 (11.6667) lr 8.2245e-05 eta 0:00:03
epoch [176/200] batch [65/67] time 0.378 (0.470) data 0.246 (0.339) loss_u loss_u 0.9526 (0.9141) acc_u 6.2500 (11.6346) lr 8.2245e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1417
confident_label rate tensor(0.3249, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1019
clean true:1011
clean false:8
clean_rate:0.9921491658488715
noisy true:708
noisy false:1409
after delete: len(clean_dataset) 1019
after delete: len(noisy_dataset) 2117
epoch [177/200] batch [5/31] time 0.446 (0.506) data 0.316 (0.376) loss_x loss_x 1.0508 (1.2520) acc_x 71.8750 (73.7500) lr 7.6120e-05 eta 0:00:13
epoch [177/200] batch [10/31] time 0.580 (0.498) data 0.449 (0.367) loss_x loss_x 0.7510 (1.0916) acc_x 78.1250 (73.7500) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [15/31] time 0.497 (0.496) data 0.366 (0.366) loss_x loss_x 1.0332 (1.0945) acc_x 75.0000 (74.7917) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [20/31] time 0.468 (0.480) data 0.337 (0.349) loss_x loss_x 0.8657 (1.0818) acc_x 81.2500 (75.4688) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [25/31] time 0.411 (0.471) data 0.282 (0.340) loss_x loss_x 0.5713 (1.0496) acc_x 93.7500 (77.0000) lr 7.6120e-05 eta 0:00:02
epoch [177/200] batch [30/31] time 0.440 (0.473) data 0.309 (0.342) loss_x loss_x 1.4219 (1.0503) acc_x 78.1250 (76.8750) lr 7.6120e-05 eta 0:00:00
epoch [177/200] batch [5/66] time 0.374 (0.466) data 0.242 (0.335) loss_u loss_u 0.9468 (0.8967) acc_u 9.3750 (13.1250) lr 7.6120e-05 eta 0:00:28
epoch [177/200] batch [10/66] time 0.610 (0.470) data 0.479 (0.339) loss_u loss_u 0.8936 (0.9005) acc_u 15.6250 (13.4375) lr 7.6120e-05 eta 0:00:26
epoch [177/200] batch [15/66] time 0.538 (0.465) data 0.408 (0.334) loss_u loss_u 0.9165 (0.9009) acc_u 12.5000 (13.1250) lr 7.6120e-05 eta 0:00:23
epoch [177/200] batch [20/66] time 0.380 (0.461) data 0.249 (0.330) loss_u loss_u 0.9258 (0.9075) acc_u 12.5000 (12.0312) lr 7.6120e-05 eta 0:00:21
epoch [177/200] batch [25/66] time 0.410 (0.457) data 0.278 (0.326) loss_u loss_u 0.9561 (0.9102) acc_u 6.2500 (11.6250) lr 7.6120e-05 eta 0:00:18
epoch [177/200] batch [30/66] time 0.506 (0.456) data 0.375 (0.325) loss_u loss_u 0.8853 (0.9152) acc_u 15.6250 (11.1458) lr 7.6120e-05 eta 0:00:16
epoch [177/200] batch [35/66] time 0.514 (0.458) data 0.383 (0.327) loss_u loss_u 0.8984 (0.9165) acc_u 12.5000 (10.9821) lr 7.6120e-05 eta 0:00:14
epoch [177/200] batch [40/66] time 0.450 (0.458) data 0.319 (0.327) loss_u loss_u 0.9390 (0.9178) acc_u 9.3750 (10.7812) lr 7.6120e-05 eta 0:00:11
epoch [177/200] batch [45/66] time 0.499 (0.457) data 0.368 (0.326) loss_u loss_u 0.9307 (0.9193) acc_u 9.3750 (10.5556) lr 7.6120e-05 eta 0:00:09
epoch [177/200] batch [50/66] time 0.348 (0.456) data 0.217 (0.325) loss_u loss_u 0.8770 (0.9192) acc_u 15.6250 (10.5000) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [55/66] time 0.403 (0.457) data 0.271 (0.325) loss_u loss_u 0.9287 (0.9198) acc_u 9.3750 (10.5114) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [60/66] time 0.650 (0.459) data 0.519 (0.328) loss_u loss_u 0.8818 (0.9204) acc_u 18.7500 (10.5729) lr 7.6120e-05 eta 0:00:02
epoch [177/200] batch [65/66] time 0.433 (0.461) data 0.302 (0.329) loss_u loss_u 0.9468 (0.9187) acc_u 9.3750 (10.8654) lr 7.6120e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1447
confident_label rate tensor(0.3135, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 983
clean true:973
clean false:10
clean_rate:0.9898270600203459
noisy true:716
noisy false:1437
after delete: len(clean_dataset) 983
after delete: len(noisy_dataset) 2153
epoch [178/200] batch [5/30] time 0.497 (0.476) data 0.367 (0.345) loss_x loss_x 1.1943 (1.0062) acc_x 75.0000 (78.7500) lr 7.0224e-05 eta 0:00:11
epoch [178/200] batch [10/30] time 0.509 (0.472) data 0.378 (0.341) loss_x loss_x 0.3923 (0.9796) acc_x 93.7500 (78.4375) lr 7.0224e-05 eta 0:00:09
epoch [178/200] batch [15/30] time 0.519 (0.471) data 0.389 (0.340) loss_x loss_x 1.2559 (1.0223) acc_x 71.8750 (75.8333) lr 7.0224e-05 eta 0:00:07
epoch [178/200] batch [20/30] time 0.503 (0.478) data 0.373 (0.348) loss_x loss_x 1.2314 (1.0278) acc_x 71.8750 (76.0938) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [25/30] time 0.517 (0.478) data 0.387 (0.347) loss_x loss_x 1.4893 (1.0636) acc_x 71.8750 (75.5000) lr 7.0224e-05 eta 0:00:02
epoch [178/200] batch [30/30] time 0.615 (0.476) data 0.482 (0.345) loss_x loss_x 1.0264 (1.0430) acc_x 81.2500 (75.8333) lr 7.0224e-05 eta 0:00:00
epoch [178/200] batch [5/67] time 0.574 (0.475) data 0.443 (0.344) loss_u loss_u 0.9360 (0.8729) acc_u 9.3750 (17.5000) lr 7.0224e-05 eta 0:00:29
epoch [178/200] batch [10/67] time 0.422 (0.473) data 0.290 (0.343) loss_u loss_u 0.9253 (0.8990) acc_u 9.3750 (13.7500) lr 7.0224e-05 eta 0:00:26
epoch [178/200] batch [15/67] time 0.464 (0.474) data 0.332 (0.343) loss_u loss_u 0.8892 (0.9062) acc_u 15.6250 (12.2917) lr 7.0224e-05 eta 0:00:24
epoch [178/200] batch [20/67] time 0.423 (0.478) data 0.292 (0.347) loss_u loss_u 0.9253 (0.9081) acc_u 6.2500 (12.0312) lr 7.0224e-05 eta 0:00:22
epoch [178/200] batch [25/67] time 0.374 (0.474) data 0.243 (0.343) loss_u loss_u 0.9473 (0.9153) acc_u 3.1250 (11.3750) lr 7.0224e-05 eta 0:00:19
epoch [178/200] batch [30/67] time 0.427 (0.475) data 0.295 (0.344) loss_u loss_u 0.8809 (0.9150) acc_u 15.6250 (11.5625) lr 7.0224e-05 eta 0:00:17
epoch [178/200] batch [35/67] time 0.476 (0.478) data 0.345 (0.347) loss_u loss_u 0.9175 (0.9148) acc_u 12.5000 (11.5179) lr 7.0224e-05 eta 0:00:15
epoch [178/200] batch [40/67] time 0.462 (0.476) data 0.330 (0.345) loss_u loss_u 0.9243 (0.9156) acc_u 12.5000 (11.4062) lr 7.0224e-05 eta 0:00:12
epoch [178/200] batch [45/67] time 0.445 (0.474) data 0.311 (0.343) loss_u loss_u 0.9272 (0.9117) acc_u 9.3750 (12.0139) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [50/67] time 0.694 (0.476) data 0.562 (0.345) loss_u loss_u 0.8823 (0.9090) acc_u 12.5000 (12.3125) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [55/67] time 0.366 (0.474) data 0.235 (0.343) loss_u loss_u 0.8613 (0.9095) acc_u 18.7500 (12.2159) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [60/67] time 0.345 (0.473) data 0.214 (0.341) loss_u loss_u 0.9229 (0.9102) acc_u 12.5000 (12.0833) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [65/67] time 0.446 (0.470) data 0.314 (0.338) loss_u loss_u 0.9053 (0.9090) acc_u 9.3750 (12.2115) lr 7.0224e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1404
confident_label rate tensor(0.3157, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 990
clean true:979
clean false:11
clean_rate:0.9888888888888889
noisy true:753
noisy false:1393
after delete: len(clean_dataset) 990
after delete: len(noisy_dataset) 2146
epoch [179/200] batch [5/30] time 0.401 (0.496) data 0.271 (0.365) loss_x loss_x 0.9507 (1.2084) acc_x 75.0000 (68.7500) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [10/30] time 0.455 (0.516) data 0.324 (0.385) loss_x loss_x 1.0195 (1.0653) acc_x 71.8750 (71.5625) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [15/30] time 0.485 (0.495) data 0.354 (0.364) loss_x loss_x 1.4707 (1.0984) acc_x 59.3750 (71.4583) lr 6.4556e-05 eta 0:00:07
epoch [179/200] batch [20/30] time 0.363 (0.471) data 0.233 (0.340) loss_x loss_x 1.5566 (1.0875) acc_x 71.8750 (72.8125) lr 6.4556e-05 eta 0:00:04
epoch [179/200] batch [25/30] time 0.428 (0.463) data 0.297 (0.332) loss_x loss_x 1.0791 (1.0764) acc_x 71.8750 (73.5000) lr 6.4556e-05 eta 0:00:02
epoch [179/200] batch [30/30] time 0.465 (0.460) data 0.336 (0.329) loss_x loss_x 1.1367 (1.0938) acc_x 71.8750 (73.0208) lr 6.4556e-05 eta 0:00:00
epoch [179/200] batch [5/67] time 0.360 (0.450) data 0.230 (0.319) loss_u loss_u 0.8711 (0.8758) acc_u 15.6250 (13.7500) lr 6.4556e-05 eta 0:00:27
epoch [179/200] batch [10/67] time 0.452 (0.447) data 0.321 (0.316) loss_u loss_u 0.8872 (0.8882) acc_u 12.5000 (13.4375) lr 6.4556e-05 eta 0:00:25
epoch [179/200] batch [15/67] time 0.517 (0.448) data 0.387 (0.317) loss_u loss_u 0.9443 (0.9009) acc_u 3.1250 (12.0833) lr 6.4556e-05 eta 0:00:23
epoch [179/200] batch [20/67] time 0.555 (0.454) data 0.425 (0.324) loss_u loss_u 0.8433 (0.8988) acc_u 18.7500 (12.3438) lr 6.4556e-05 eta 0:00:21
epoch [179/200] batch [25/67] time 0.375 (0.450) data 0.244 (0.319) loss_u loss_u 0.9277 (0.9029) acc_u 9.3750 (11.7500) lr 6.4556e-05 eta 0:00:18
epoch [179/200] batch [30/67] time 0.647 (0.454) data 0.516 (0.323) loss_u loss_u 0.9512 (0.9062) acc_u 6.2500 (11.5625) lr 6.4556e-05 eta 0:00:16
epoch [179/200] batch [35/67] time 0.507 (0.455) data 0.377 (0.324) loss_u loss_u 0.8677 (0.9064) acc_u 15.6250 (11.5179) lr 6.4556e-05 eta 0:00:14
epoch [179/200] batch [40/67] time 0.403 (0.454) data 0.272 (0.323) loss_u loss_u 0.9307 (0.9099) acc_u 9.3750 (11.1719) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [45/67] time 0.364 (0.456) data 0.234 (0.325) loss_u loss_u 0.8979 (0.9068) acc_u 12.5000 (11.3889) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [50/67] time 0.469 (0.455) data 0.338 (0.324) loss_u loss_u 0.9043 (0.9067) acc_u 9.3750 (11.1875) lr 6.4556e-05 eta 0:00:07
epoch [179/200] batch [55/67] time 0.385 (0.451) data 0.254 (0.320) loss_u loss_u 0.9224 (0.9079) acc_u 9.3750 (11.0795) lr 6.4556e-05 eta 0:00:05
epoch [179/200] batch [60/67] time 0.422 (0.453) data 0.292 (0.322) loss_u loss_u 0.8823 (0.9083) acc_u 15.6250 (10.9896) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [65/67] time 0.437 (0.451) data 0.305 (0.320) loss_u loss_u 0.9741 (0.9067) acc_u 6.2500 (11.2981) lr 6.4556e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1426
confident_label rate tensor(0.3186, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 999
clean true:988
clean false:11
clean_rate:0.988988988988989
noisy true:722
noisy false:1415
after delete: len(clean_dataset) 999
after delete: len(noisy_dataset) 2137
epoch [180/200] batch [5/31] time 0.554 (0.477) data 0.424 (0.347) loss_x loss_x 0.9844 (0.9797) acc_x 75.0000 (73.1250) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [10/31] time 0.483 (0.487) data 0.352 (0.356) loss_x loss_x 1.1328 (0.9830) acc_x 71.8750 (75.0000) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [15/31] time 0.350 (0.489) data 0.220 (0.358) loss_x loss_x 0.7627 (1.0253) acc_x 78.1250 (75.0000) lr 5.9119e-05 eta 0:00:07
epoch [180/200] batch [20/31] time 0.474 (0.482) data 0.343 (0.351) loss_x loss_x 1.2314 (1.0207) acc_x 75.0000 (75.4688) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [25/31] time 0.387 (0.475) data 0.256 (0.344) loss_x loss_x 1.1045 (1.0445) acc_x 68.7500 (75.1250) lr 5.9119e-05 eta 0:00:02
epoch [180/200] batch [30/31] time 0.446 (0.465) data 0.316 (0.334) loss_x loss_x 0.8599 (1.0332) acc_x 71.8750 (74.7917) lr 5.9119e-05 eta 0:00:00
epoch [180/200] batch [5/66] time 0.516 (0.471) data 0.386 (0.340) loss_u loss_u 0.8247 (0.8751) acc_u 18.7500 (16.2500) lr 5.9119e-05 eta 0:00:28
epoch [180/200] batch [10/66] time 0.416 (0.470) data 0.285 (0.340) loss_u loss_u 0.9268 (0.8881) acc_u 9.3750 (14.3750) lr 5.9119e-05 eta 0:00:26
epoch [180/200] batch [15/66] time 0.517 (0.465) data 0.387 (0.334) loss_u loss_u 0.9438 (0.8890) acc_u 9.3750 (14.1667) lr 5.9119e-05 eta 0:00:23
epoch [180/200] batch [20/66] time 0.439 (0.469) data 0.309 (0.338) loss_u loss_u 0.9507 (0.8983) acc_u 0.0000 (12.8125) lr 5.9119e-05 eta 0:00:21
epoch [180/200] batch [25/66] time 0.426 (0.464) data 0.295 (0.333) loss_u loss_u 0.9233 (0.9008) acc_u 12.5000 (12.7500) lr 5.9119e-05 eta 0:00:19
epoch [180/200] batch [30/66] time 0.345 (0.459) data 0.213 (0.328) loss_u loss_u 0.9009 (0.8991) acc_u 12.5000 (12.8125) lr 5.9119e-05 eta 0:00:16
epoch [180/200] batch [35/66] time 0.419 (0.459) data 0.287 (0.328) loss_u loss_u 0.8706 (0.8967) acc_u 15.6250 (13.2143) lr 5.9119e-05 eta 0:00:14
epoch [180/200] batch [40/66] time 0.415 (0.458) data 0.284 (0.327) loss_u loss_u 0.9419 (0.9007) acc_u 6.2500 (12.8906) lr 5.9119e-05 eta 0:00:11
epoch [180/200] batch [45/66] time 0.640 (0.457) data 0.509 (0.326) loss_u loss_u 0.9351 (0.9058) acc_u 9.3750 (12.0833) lr 5.9119e-05 eta 0:00:09
epoch [180/200] batch [50/66] time 0.391 (0.457) data 0.261 (0.326) loss_u loss_u 0.8691 (0.9074) acc_u 21.8750 (12.0625) lr 5.9119e-05 eta 0:00:07
epoch [180/200] batch [55/66] time 0.412 (0.457) data 0.282 (0.326) loss_u loss_u 0.9443 (0.9073) acc_u 9.3750 (12.1591) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [60/66] time 0.432 (0.454) data 0.300 (0.323) loss_u loss_u 0.8481 (0.9077) acc_u 15.6250 (11.9792) lr 5.9119e-05 eta 0:00:02
epoch [180/200] batch [65/66] time 0.443 (0.451) data 0.313 (0.320) loss_u loss_u 0.9380 (0.9084) acc_u 9.3750 (11.8750) lr 5.9119e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1391
confident_label rate tensor(0.3246, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1018
clean true:1008
clean false:10
clean_rate:0.9901768172888016
noisy true:737
noisy false:1381
after delete: len(clean_dataset) 1018
after delete: len(noisy_dataset) 2118
epoch [181/200] batch [5/31] time 0.509 (0.447) data 0.379 (0.316) loss_x loss_x 1.3506 (1.2497) acc_x 62.5000 (70.0000) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [10/31] time 0.380 (0.456) data 0.249 (0.325) loss_x loss_x 1.1133 (1.2917) acc_x 71.8750 (70.9375) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [15/31] time 0.416 (0.466) data 0.286 (0.335) loss_x loss_x 0.8550 (1.1193) acc_x 71.8750 (73.1250) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [20/31] time 0.493 (0.465) data 0.362 (0.335) loss_x loss_x 1.5127 (1.1117) acc_x 62.5000 (72.9688) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [25/31] time 0.478 (0.463) data 0.347 (0.332) loss_x loss_x 1.4463 (1.0992) acc_x 59.3750 (72.5000) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [30/31] time 0.394 (0.462) data 0.264 (0.331) loss_x loss_x 0.7798 (1.0836) acc_x 81.2500 (72.9167) lr 5.3915e-05 eta 0:00:00
epoch [181/200] batch [5/66] time 0.518 (0.459) data 0.387 (0.329) loss_u loss_u 0.8506 (0.8708) acc_u 15.6250 (15.0000) lr 5.3915e-05 eta 0:00:28
epoch [181/200] batch [10/66] time 0.524 (0.452) data 0.393 (0.321) loss_u loss_u 0.9048 (0.8920) acc_u 12.5000 (12.5000) lr 5.3915e-05 eta 0:00:25
epoch [181/200] batch [15/66] time 0.432 (0.451) data 0.300 (0.320) loss_u loss_u 0.9590 (0.9069) acc_u 9.3750 (11.2500) lr 5.3915e-05 eta 0:00:23
epoch [181/200] batch [20/66] time 0.476 (0.452) data 0.345 (0.321) loss_u loss_u 0.9321 (0.9075) acc_u 9.3750 (11.4062) lr 5.3915e-05 eta 0:00:20
epoch [181/200] batch [25/66] time 0.450 (0.450) data 0.318 (0.319) loss_u loss_u 0.9492 (0.9107) acc_u 9.3750 (11.2500) lr 5.3915e-05 eta 0:00:18
epoch [181/200] batch [30/66] time 0.611 (0.453) data 0.479 (0.322) loss_u loss_u 0.9692 (0.9130) acc_u 3.1250 (11.0417) lr 5.3915e-05 eta 0:00:16
epoch [181/200] batch [35/66] time 0.479 (0.457) data 0.349 (0.326) loss_u loss_u 0.9214 (0.9150) acc_u 9.3750 (10.7143) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [40/66] time 0.411 (0.458) data 0.277 (0.327) loss_u loss_u 0.8604 (0.9133) acc_u 15.6250 (10.8594) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [45/66] time 0.454 (0.460) data 0.323 (0.329) loss_u loss_u 0.9570 (0.9124) acc_u 6.2500 (10.9722) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [50/66] time 0.421 (0.460) data 0.290 (0.329) loss_u loss_u 0.9229 (0.9108) acc_u 9.3750 (11.1875) lr 5.3915e-05 eta 0:00:07
epoch [181/200] batch [55/66] time 0.431 (0.457) data 0.299 (0.326) loss_u loss_u 0.9507 (0.9118) acc_u 6.2500 (11.0795) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [60/66] time 0.498 (0.458) data 0.368 (0.326) loss_u loss_u 0.8667 (0.9094) acc_u 15.6250 (11.5104) lr 5.3915e-05 eta 0:00:02
epoch [181/200] batch [65/66] time 0.588 (0.458) data 0.457 (0.327) loss_u loss_u 0.8926 (0.9099) acc_u 18.7500 (11.4904) lr 5.3915e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1424
confident_label rate tensor(0.3131, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 982
clean true:974
clean false:8
clean_rate:0.9918533604887984
noisy true:738
noisy false:1416
after delete: len(clean_dataset) 982
after delete: len(noisy_dataset) 2154
epoch [182/200] batch [5/30] time 0.448 (0.430) data 0.317 (0.300) loss_x loss_x 0.6650 (1.2126) acc_x 84.3750 (73.1250) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [10/30] time 0.405 (0.438) data 0.275 (0.308) loss_x loss_x 1.2002 (1.1330) acc_x 68.7500 (73.7500) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [15/30] time 0.464 (0.460) data 0.334 (0.329) loss_x loss_x 1.5547 (1.1237) acc_x 65.6250 (75.2083) lr 4.8943e-05 eta 0:00:06
epoch [182/200] batch [20/30] time 0.458 (0.483) data 0.327 (0.352) loss_x loss_x 1.1514 (1.1178) acc_x 84.3750 (76.0938) lr 4.8943e-05 eta 0:00:04
epoch [182/200] batch [25/30] time 0.612 (0.482) data 0.482 (0.351) loss_x loss_x 1.3037 (1.1005) acc_x 62.5000 (75.8750) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [30/30] time 0.624 (0.481) data 0.493 (0.350) loss_x loss_x 1.1660 (1.1285) acc_x 71.8750 (74.1667) lr 4.8943e-05 eta 0:00:00
epoch [182/200] batch [5/67] time 0.451 (0.476) data 0.320 (0.345) loss_u loss_u 0.8975 (0.9179) acc_u 15.6250 (11.2500) lr 4.8943e-05 eta 0:00:29
epoch [182/200] batch [10/67] time 0.453 (0.474) data 0.322 (0.343) loss_u loss_u 0.9072 (0.9089) acc_u 9.3750 (11.8750) lr 4.8943e-05 eta 0:00:26
epoch [182/200] batch [15/67] time 0.450 (0.474) data 0.319 (0.343) loss_u loss_u 0.9092 (0.9174) acc_u 15.6250 (11.2500) lr 4.8943e-05 eta 0:00:24
epoch [182/200] batch [20/67] time 0.676 (0.480) data 0.545 (0.349) loss_u loss_u 0.8330 (0.9095) acc_u 15.6250 (11.4062) lr 4.8943e-05 eta 0:00:22
epoch [182/200] batch [25/67] time 0.489 (0.482) data 0.355 (0.351) loss_u loss_u 0.8994 (0.9013) acc_u 12.5000 (12.8750) lr 4.8943e-05 eta 0:00:20
epoch [182/200] batch [30/67] time 0.429 (0.478) data 0.298 (0.347) loss_u loss_u 0.9038 (0.9020) acc_u 18.7500 (12.7083) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [35/67] time 0.438 (0.474) data 0.307 (0.343) loss_u loss_u 0.9395 (0.9057) acc_u 6.2500 (12.3214) lr 4.8943e-05 eta 0:00:15
epoch [182/200] batch [40/67] time 0.351 (0.469) data 0.221 (0.338) loss_u loss_u 0.9258 (0.9056) acc_u 9.3750 (12.2656) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [45/67] time 0.414 (0.469) data 0.282 (0.338) loss_u loss_u 0.9111 (0.9030) acc_u 12.5000 (12.7083) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [50/67] time 0.428 (0.466) data 0.295 (0.335) loss_u loss_u 0.9346 (0.9015) acc_u 9.3750 (12.8125) lr 4.8943e-05 eta 0:00:07
epoch [182/200] batch [55/67] time 0.352 (0.468) data 0.220 (0.337) loss_u loss_u 0.9248 (0.9036) acc_u 12.5000 (12.6705) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [60/67] time 0.505 (0.467) data 0.374 (0.336) loss_u loss_u 0.9688 (0.9073) acc_u 3.1250 (12.0833) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [65/67] time 0.380 (0.466) data 0.248 (0.335) loss_u loss_u 0.9653 (0.9067) acc_u 3.1250 (12.1635) lr 4.8943e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1421
confident_label rate tensor(0.3084, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 967
clean true:957
clean false:10
clean_rate:0.9896587383660806
noisy true:758
noisy false:1411
after delete: len(clean_dataset) 967
after delete: len(noisy_dataset) 2169
epoch [183/200] batch [5/30] time 0.399 (0.460) data 0.268 (0.329) loss_x loss_x 0.5698 (1.0694) acc_x 87.5000 (73.7500) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [10/30] time 0.400 (0.451) data 0.269 (0.321) loss_x loss_x 0.9546 (1.1847) acc_x 75.0000 (70.6250) lr 4.4207e-05 eta 0:00:09
epoch [183/200] batch [15/30] time 0.423 (0.446) data 0.293 (0.315) loss_x loss_x 1.1494 (1.2157) acc_x 71.8750 (70.8333) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [20/30] time 0.637 (0.464) data 0.507 (0.333) loss_x loss_x 0.5386 (1.1421) acc_x 81.2500 (72.5000) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [25/30] time 0.490 (0.473) data 0.359 (0.343) loss_x loss_x 0.9644 (1.0907) acc_x 71.8750 (73.8750) lr 4.4207e-05 eta 0:00:02
epoch [183/200] batch [30/30] time 0.522 (0.473) data 0.391 (0.343) loss_x loss_x 1.4395 (1.0723) acc_x 65.6250 (74.4792) lr 4.4207e-05 eta 0:00:00
epoch [183/200] batch [5/67] time 0.401 (0.474) data 0.270 (0.344) loss_u loss_u 0.8652 (0.9089) acc_u 15.6250 (13.1250) lr 4.4207e-05 eta 0:00:29
epoch [183/200] batch [10/67] time 0.488 (0.473) data 0.356 (0.343) loss_u loss_u 0.9258 (0.9119) acc_u 9.3750 (11.5625) lr 4.4207e-05 eta 0:00:26
epoch [183/200] batch [15/67] time 0.345 (0.466) data 0.213 (0.335) loss_u loss_u 0.8735 (0.9135) acc_u 15.6250 (11.0417) lr 4.4207e-05 eta 0:00:24
epoch [183/200] batch [20/67] time 0.509 (0.465) data 0.377 (0.334) loss_u loss_u 0.9722 (0.9152) acc_u 3.1250 (10.7812) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [25/67] time 0.464 (0.466) data 0.333 (0.335) loss_u loss_u 0.9175 (0.9099) acc_u 12.5000 (11.5000) lr 4.4207e-05 eta 0:00:19
epoch [183/200] batch [30/67] time 0.403 (0.461) data 0.272 (0.330) loss_u loss_u 0.9385 (0.9120) acc_u 6.2500 (11.2500) lr 4.4207e-05 eta 0:00:17
epoch [183/200] batch [35/67] time 0.866 (0.463) data 0.730 (0.332) loss_u loss_u 0.9609 (0.9116) acc_u 3.1250 (11.2500) lr 4.4207e-05 eta 0:00:14
epoch [183/200] batch [40/67] time 0.411 (0.463) data 0.279 (0.332) loss_u loss_u 0.9141 (0.9112) acc_u 9.3750 (11.4062) lr 4.4207e-05 eta 0:00:12
epoch [183/200] batch [45/67] time 0.469 (0.466) data 0.336 (0.335) loss_u loss_u 0.9307 (0.9092) acc_u 9.3750 (11.6667) lr 4.4207e-05 eta 0:00:10
epoch [183/200] batch [50/67] time 0.431 (0.465) data 0.298 (0.334) loss_u loss_u 0.8291 (0.9042) acc_u 21.8750 (12.3125) lr 4.4207e-05 eta 0:00:07
epoch [183/200] batch [55/67] time 0.469 (0.466) data 0.332 (0.334) loss_u loss_u 0.8560 (0.9028) acc_u 15.6250 (12.3864) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [60/67] time 0.502 (0.468) data 0.370 (0.336) loss_u loss_u 0.9487 (0.9045) acc_u 6.2500 (12.1875) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [65/67] time 0.453 (0.470) data 0.321 (0.339) loss_u loss_u 0.8618 (0.9024) acc_u 15.6250 (12.4038) lr 4.4207e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1423
confident_label rate tensor(0.3112, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 976
clean true:967
clean false:9
clean_rate:0.9907786885245902
noisy true:746
noisy false:1414
after delete: len(clean_dataset) 976
after delete: len(noisy_dataset) 2160
epoch [184/200] batch [5/30] time 0.440 (0.503) data 0.309 (0.371) loss_x loss_x 1.0166 (0.9203) acc_x 75.0000 (77.5000) lr 3.9706e-05 eta 0:00:12
epoch [184/200] batch [10/30] time 0.513 (0.511) data 0.383 (0.380) loss_x loss_x 0.5415 (0.9019) acc_x 90.6250 (77.8125) lr 3.9706e-05 eta 0:00:10
epoch [184/200] batch [15/30] time 0.426 (0.496) data 0.295 (0.365) loss_x loss_x 1.3740 (0.9682) acc_x 68.7500 (76.2500) lr 3.9706e-05 eta 0:00:07
epoch [184/200] batch [20/30] time 0.418 (0.485) data 0.288 (0.355) loss_x loss_x 1.3438 (1.0525) acc_x 62.5000 (73.7500) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [25/30] time 0.398 (0.470) data 0.268 (0.340) loss_x loss_x 1.4746 (1.0976) acc_x 75.0000 (72.7500) lr 3.9706e-05 eta 0:00:02
epoch [184/200] batch [30/30] time 0.563 (0.469) data 0.433 (0.338) loss_x loss_x 1.6729 (1.1132) acc_x 62.5000 (72.3958) lr 3.9706e-05 eta 0:00:00
epoch [184/200] batch [5/67] time 0.597 (0.474) data 0.466 (0.343) loss_u loss_u 0.8833 (0.9140) acc_u 18.7500 (12.5000) lr 3.9706e-05 eta 0:00:29
epoch [184/200] batch [10/67] time 0.408 (0.472) data 0.276 (0.341) loss_u loss_u 0.9077 (0.8996) acc_u 9.3750 (13.4375) lr 3.9706e-05 eta 0:00:26
epoch [184/200] batch [15/67] time 0.550 (0.479) data 0.418 (0.348) loss_u loss_u 0.8213 (0.9026) acc_u 21.8750 (12.5000) lr 3.9706e-05 eta 0:00:24
epoch [184/200] batch [20/67] time 0.432 (0.477) data 0.300 (0.345) loss_u loss_u 0.9009 (0.9047) acc_u 15.6250 (12.6562) lr 3.9706e-05 eta 0:00:22
epoch [184/200] batch [25/67] time 0.572 (0.479) data 0.441 (0.348) loss_u loss_u 0.9004 (0.9102) acc_u 12.5000 (12.0000) lr 3.9706e-05 eta 0:00:20
epoch [184/200] batch [30/67] time 0.400 (0.479) data 0.270 (0.348) loss_u loss_u 0.8374 (0.9094) acc_u 15.6250 (11.8750) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [35/67] time 0.412 (0.481) data 0.282 (0.350) loss_u loss_u 0.8877 (0.9062) acc_u 15.6250 (12.2321) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [40/67] time 0.382 (0.478) data 0.250 (0.347) loss_u loss_u 0.8755 (0.9069) acc_u 15.6250 (12.0312) lr 3.9706e-05 eta 0:00:12
epoch [184/200] batch [45/67] time 0.384 (0.475) data 0.251 (0.343) loss_u loss_u 0.8638 (0.9074) acc_u 15.6250 (11.8056) lr 3.9706e-05 eta 0:00:10
epoch [184/200] batch [50/67] time 0.456 (0.473) data 0.323 (0.342) loss_u loss_u 0.8696 (0.9090) acc_u 15.6250 (11.7500) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [55/67] time 0.424 (0.468) data 0.292 (0.337) loss_u loss_u 0.9346 (0.9088) acc_u 6.2500 (11.6477) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [60/67] time 0.400 (0.471) data 0.265 (0.339) loss_u loss_u 0.8345 (0.9067) acc_u 21.8750 (11.9271) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [65/67] time 0.737 (0.475) data 0.606 (0.344) loss_u loss_u 0.9019 (0.9067) acc_u 12.5000 (11.9231) lr 3.9706e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1395
confident_label rate tensor(0.3182, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 998
clean true:989
clean false:9
clean_rate:0.9909819639278558
noisy true:752
noisy false:1386
after delete: len(clean_dataset) 998
after delete: len(noisy_dataset) 2138
epoch [185/200] batch [5/31] time 0.513 (0.469) data 0.382 (0.339) loss_x loss_x 0.9355 (0.9944) acc_x 75.0000 (75.6250) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [10/31] time 0.466 (0.462) data 0.334 (0.331) loss_x loss_x 0.7920 (1.0304) acc_x 84.3750 (75.6250) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [15/31] time 0.593 (0.469) data 0.462 (0.338) loss_x loss_x 0.6514 (0.9687) acc_x 81.2500 (76.8750) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [20/31] time 0.456 (0.464) data 0.325 (0.333) loss_x loss_x 1.2705 (1.0421) acc_x 65.6250 (74.5312) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [25/31] time 0.432 (0.475) data 0.300 (0.344) loss_x loss_x 1.1748 (1.0446) acc_x 81.2500 (75.1250) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [30/31] time 0.348 (0.482) data 0.218 (0.351) loss_x loss_x 0.8403 (1.0400) acc_x 65.6250 (74.7917) lr 3.5443e-05 eta 0:00:00
epoch [185/200] batch [5/66] time 0.489 (0.488) data 0.358 (0.357) loss_u loss_u 0.8423 (0.8840) acc_u 18.7500 (14.3750) lr 3.5443e-05 eta 0:00:29
epoch [185/200] batch [10/66] time 0.393 (0.479) data 0.263 (0.348) loss_u loss_u 0.9014 (0.8996) acc_u 9.3750 (12.1875) lr 3.5443e-05 eta 0:00:26
epoch [185/200] batch [15/66] time 0.378 (0.479) data 0.246 (0.348) loss_u loss_u 0.9556 (0.9007) acc_u 6.2500 (11.6667) lr 3.5443e-05 eta 0:00:24
epoch [185/200] batch [20/66] time 0.383 (0.478) data 0.251 (0.347) loss_u loss_u 0.9741 (0.8979) acc_u 3.1250 (12.0312) lr 3.5443e-05 eta 0:00:22
epoch [185/200] batch [25/66] time 0.631 (0.482) data 0.499 (0.351) loss_u loss_u 0.8892 (0.8972) acc_u 12.5000 (12.2500) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [30/66] time 0.397 (0.479) data 0.266 (0.348) loss_u loss_u 0.9331 (0.8940) acc_u 12.5000 (12.7083) lr 3.5443e-05 eta 0:00:17
epoch [185/200] batch [35/66] time 0.623 (0.477) data 0.493 (0.346) loss_u loss_u 0.8628 (0.8972) acc_u 18.7500 (12.6786) lr 3.5443e-05 eta 0:00:14
epoch [185/200] batch [40/66] time 0.337 (0.476) data 0.207 (0.345) loss_u loss_u 0.9067 (0.9006) acc_u 12.5000 (12.5000) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [45/66] time 0.523 (0.476) data 0.391 (0.345) loss_u loss_u 0.9126 (0.9029) acc_u 12.5000 (12.2222) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [50/66] time 0.403 (0.476) data 0.271 (0.345) loss_u loss_u 0.8076 (0.9029) acc_u 25.0000 (12.2500) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [55/66] time 0.391 (0.473) data 0.259 (0.342) loss_u loss_u 0.9326 (0.9036) acc_u 6.2500 (12.1023) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [60/66] time 0.348 (0.472) data 0.216 (0.340) loss_u loss_u 0.8794 (0.9048) acc_u 18.7500 (11.9271) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [65/66] time 0.717 (0.474) data 0.586 (0.343) loss_u loss_u 0.9185 (0.9053) acc_u 9.3750 (11.8750) lr 3.5443e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1468
confident_label rate tensor(0.3061, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 960
clean true:954
clean false:6
clean_rate:0.99375
noisy true:714
noisy false:1462
after delete: len(clean_dataset) 960
after delete: len(noisy_dataset) 2176
epoch [186/200] batch [5/30] time 0.497 (0.478) data 0.366 (0.347) loss_x loss_x 0.7920 (1.0768) acc_x 81.2500 (73.7500) lr 3.1417e-05 eta 0:00:11
epoch [186/200] batch [10/30] time 0.437 (0.475) data 0.307 (0.344) loss_x loss_x 1.2021 (1.0308) acc_x 75.0000 (75.6250) lr 3.1417e-05 eta 0:00:09
epoch [186/200] batch [15/30] time 0.549 (0.476) data 0.418 (0.346) loss_x loss_x 0.8560 (1.0199) acc_x 71.8750 (75.4167) lr 3.1417e-05 eta 0:00:07
epoch [186/200] batch [20/30] time 0.482 (0.455) data 0.351 (0.325) loss_x loss_x 0.8843 (0.9696) acc_x 71.8750 (76.7188) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [25/30] time 0.424 (0.462) data 0.293 (0.331) loss_x loss_x 0.7236 (0.9570) acc_x 84.3750 (76.7500) lr 3.1417e-05 eta 0:00:02
epoch [186/200] batch [30/30] time 0.466 (0.457) data 0.335 (0.327) loss_x loss_x 1.1885 (0.9717) acc_x 71.8750 (76.6667) lr 3.1417e-05 eta 0:00:00
epoch [186/200] batch [5/68] time 0.345 (0.450) data 0.212 (0.320) loss_u loss_u 0.8857 (0.9118) acc_u 18.7500 (12.5000) lr 3.1417e-05 eta 0:00:28
epoch [186/200] batch [10/68] time 0.458 (0.450) data 0.326 (0.319) loss_u loss_u 0.9370 (0.9102) acc_u 6.2500 (12.5000) lr 3.1417e-05 eta 0:00:26
epoch [186/200] batch [15/68] time 0.493 (0.454) data 0.361 (0.323) loss_u loss_u 0.8535 (0.9011) acc_u 21.8750 (13.5417) lr 3.1417e-05 eta 0:00:24
epoch [186/200] batch [20/68] time 0.402 (0.460) data 0.269 (0.329) loss_u loss_u 0.9302 (0.9081) acc_u 6.2500 (12.1875) lr 3.1417e-05 eta 0:00:22
epoch [186/200] batch [25/68] time 0.335 (0.460) data 0.203 (0.329) loss_u loss_u 0.9600 (0.9079) acc_u 9.3750 (12.3750) lr 3.1417e-05 eta 0:00:19
epoch [186/200] batch [30/68] time 0.472 (0.462) data 0.342 (0.331) loss_u loss_u 0.8511 (0.9054) acc_u 15.6250 (12.8125) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [35/68] time 0.366 (0.463) data 0.235 (0.332) loss_u loss_u 0.9565 (0.9088) acc_u 6.2500 (12.3214) lr 3.1417e-05 eta 0:00:15
epoch [186/200] batch [40/68] time 0.353 (0.462) data 0.222 (0.331) loss_u loss_u 0.8540 (0.9091) acc_u 15.6250 (12.1094) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [45/68] time 0.574 (0.463) data 0.443 (0.332) loss_u loss_u 0.8633 (0.9059) acc_u 15.6250 (12.5694) lr 3.1417e-05 eta 0:00:10
epoch [186/200] batch [50/68] time 0.506 (0.464) data 0.374 (0.333) loss_u loss_u 0.8960 (0.9050) acc_u 12.5000 (12.5625) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [55/68] time 0.593 (0.462) data 0.461 (0.331) loss_u loss_u 0.9365 (0.9094) acc_u 6.2500 (11.8182) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [60/68] time 0.543 (0.460) data 0.411 (0.329) loss_u loss_u 0.9365 (0.9111) acc_u 12.5000 (11.6146) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [65/68] time 0.618 (0.461) data 0.487 (0.329) loss_u loss_u 0.9170 (0.9101) acc_u 12.5000 (11.7308) lr 3.1417e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1442
confident_label rate tensor(0.3087, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 968
clean true:959
clean false:9
clean_rate:0.9907024793388429
noisy true:735
noisy false:1433
after delete: len(clean_dataset) 968
after delete: len(noisy_dataset) 2168
epoch [187/200] batch [5/30] time 0.578 (0.454) data 0.447 (0.323) loss_x loss_x 0.7969 (0.9474) acc_x 78.1250 (78.1250) lr 2.7630e-05 eta 0:00:11
epoch [187/200] batch [10/30] time 0.369 (0.451) data 0.239 (0.321) loss_x loss_x 1.0352 (1.0186) acc_x 71.8750 (75.9375) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [15/30] time 0.575 (0.471) data 0.444 (0.340) loss_x loss_x 1.1729 (0.9993) acc_x 65.6250 (76.6667) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [20/30] time 0.508 (0.471) data 0.378 (0.340) loss_x loss_x 1.0625 (1.0331) acc_x 81.2500 (75.3125) lr 2.7630e-05 eta 0:00:04
epoch [187/200] batch [25/30] time 0.423 (0.471) data 0.292 (0.340) loss_x loss_x 1.2256 (1.0372) acc_x 59.3750 (73.8750) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [30/30] time 0.341 (0.465) data 0.211 (0.334) loss_x loss_x 1.4307 (1.0797) acc_x 81.2500 (73.4375) lr 2.7630e-05 eta 0:00:00
epoch [187/200] batch [5/67] time 0.352 (0.456) data 0.221 (0.325) loss_u loss_u 0.8613 (0.8910) acc_u 18.7500 (13.1250) lr 2.7630e-05 eta 0:00:28
epoch [187/200] batch [10/67] time 0.424 (0.457) data 0.293 (0.326) loss_u loss_u 0.9062 (0.9006) acc_u 18.7500 (13.1250) lr 2.7630e-05 eta 0:00:26
epoch [187/200] batch [15/67] time 0.469 (0.459) data 0.337 (0.328) loss_u loss_u 0.9341 (0.9013) acc_u 9.3750 (12.9167) lr 2.7630e-05 eta 0:00:23
epoch [187/200] batch [20/67] time 0.519 (0.455) data 0.388 (0.324) loss_u loss_u 0.9067 (0.8981) acc_u 12.5000 (13.2812) lr 2.7630e-05 eta 0:00:21
epoch [187/200] batch [25/67] time 0.355 (0.452) data 0.223 (0.320) loss_u loss_u 0.9468 (0.9023) acc_u 6.2500 (12.5000) lr 2.7630e-05 eta 0:00:18
epoch [187/200] batch [30/67] time 0.464 (0.449) data 0.333 (0.318) loss_u loss_u 0.8652 (0.9013) acc_u 18.7500 (13.0208) lr 2.7630e-05 eta 0:00:16
epoch [187/200] batch [35/67] time 0.551 (0.447) data 0.419 (0.316) loss_u loss_u 0.9185 (0.9060) acc_u 9.3750 (12.3214) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [40/67] time 0.463 (0.449) data 0.330 (0.318) loss_u loss_u 0.9175 (0.9043) acc_u 9.3750 (12.5781) lr 2.7630e-05 eta 0:00:12
epoch [187/200] batch [45/67] time 0.575 (0.452) data 0.444 (0.321) loss_u loss_u 0.9214 (0.9055) acc_u 9.3750 (12.3611) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [50/67] time 0.479 (0.453) data 0.347 (0.321) loss_u loss_u 0.8862 (0.9033) acc_u 12.5000 (12.5000) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [55/67] time 0.324 (0.450) data 0.193 (0.318) loss_u loss_u 0.9204 (0.9056) acc_u 12.5000 (12.1591) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [60/67] time 0.376 (0.449) data 0.244 (0.317) loss_u loss_u 0.9272 (0.9073) acc_u 6.2500 (11.8229) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [65/67] time 0.437 (0.449) data 0.306 (0.318) loss_u loss_u 0.9048 (0.9061) acc_u 9.3750 (12.0192) lr 2.7630e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1433
confident_label rate tensor(0.3154, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 989
clean true:983
clean false:6
clean_rate:0.993933265925177
noisy true:720
noisy false:1427
after delete: len(clean_dataset) 989
after delete: len(noisy_dataset) 2147
epoch [188/200] batch [5/30] time 0.488 (0.554) data 0.358 (0.423) loss_x loss_x 0.9995 (1.0442) acc_x 78.1250 (76.2500) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [10/30] time 0.475 (0.505) data 0.344 (0.374) loss_x loss_x 0.6616 (0.9841) acc_x 87.5000 (76.8750) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [15/30] time 0.500 (0.492) data 0.369 (0.361) loss_x loss_x 0.4473 (0.9868) acc_x 96.8750 (77.2917) lr 2.4083e-05 eta 0:00:07
epoch [188/200] batch [20/30] time 0.565 (0.483) data 0.435 (0.352) loss_x loss_x 0.9961 (1.0198) acc_x 78.1250 (76.0938) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [25/30] time 0.357 (0.480) data 0.226 (0.350) loss_x loss_x 1.2822 (1.0283) acc_x 68.7500 (75.8750) lr 2.4083e-05 eta 0:00:02
epoch [188/200] batch [30/30] time 0.495 (0.473) data 0.364 (0.342) loss_x loss_x 0.5908 (1.0058) acc_x 84.3750 (75.9375) lr 2.4083e-05 eta 0:00:00
epoch [188/200] batch [5/67] time 0.391 (0.465) data 0.259 (0.334) loss_u loss_u 0.8828 (0.9177) acc_u 15.6250 (10.6250) lr 2.4083e-05 eta 0:00:28
epoch [188/200] batch [10/67] time 0.473 (0.461) data 0.341 (0.329) loss_u loss_u 0.9048 (0.9125) acc_u 9.3750 (10.9375) lr 2.4083e-05 eta 0:00:26
epoch [188/200] batch [15/67] time 0.450 (0.458) data 0.319 (0.327) loss_u loss_u 0.9224 (0.9171) acc_u 15.6250 (10.6250) lr 2.4083e-05 eta 0:00:23
epoch [188/200] batch [20/67] time 0.360 (0.452) data 0.229 (0.321) loss_u loss_u 0.8867 (0.9194) acc_u 12.5000 (10.4688) lr 2.4083e-05 eta 0:00:21
epoch [188/200] batch [25/67] time 0.338 (0.453) data 0.207 (0.322) loss_u loss_u 0.9712 (0.9200) acc_u 6.2500 (10.3750) lr 2.4083e-05 eta 0:00:19
epoch [188/200] batch [30/67] time 0.452 (0.450) data 0.321 (0.319) loss_u loss_u 0.9146 (0.9206) acc_u 12.5000 (10.1042) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [35/67] time 0.495 (0.451) data 0.364 (0.320) loss_u loss_u 0.8701 (0.9158) acc_u 15.6250 (10.6250) lr 2.4083e-05 eta 0:00:14
epoch [188/200] batch [40/67] time 0.466 (0.458) data 0.335 (0.327) loss_u loss_u 0.9448 (0.9141) acc_u 9.3750 (10.9375) lr 2.4083e-05 eta 0:00:12
epoch [188/200] batch [45/67] time 0.523 (0.457) data 0.393 (0.326) loss_u loss_u 0.9189 (0.9142) acc_u 9.3750 (10.8333) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [50/67] time 0.503 (0.460) data 0.371 (0.329) loss_u loss_u 0.8867 (0.9119) acc_u 15.6250 (11.1250) lr 2.4083e-05 eta 0:00:07
epoch [188/200] batch [55/67] time 0.786 (0.464) data 0.654 (0.333) loss_u loss_u 0.9458 (0.9114) acc_u 6.2500 (11.2500) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [60/67] time 0.383 (0.463) data 0.252 (0.332) loss_u loss_u 0.8848 (0.9133) acc_u 15.6250 (10.9896) lr 2.4083e-05 eta 0:00:03
epoch [188/200] batch [65/67] time 0.463 (0.461) data 0.331 (0.330) loss_u loss_u 0.8408 (0.9104) acc_u 21.8750 (11.4904) lr 2.4083e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.3115, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 977
clean true:968
clean false:9
clean_rate:0.9907881269191402
noisy true:755
noisy false:1404
after delete: len(clean_dataset) 977
after delete: len(noisy_dataset) 2159
epoch [189/200] batch [5/30] time 0.366 (0.455) data 0.236 (0.324) loss_x loss_x 0.9395 (1.2719) acc_x 81.2500 (72.5000) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [10/30] time 0.413 (0.437) data 0.283 (0.306) loss_x loss_x 0.7852 (1.1690) acc_x 78.1250 (73.4375) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [15/30] time 0.477 (0.467) data 0.345 (0.336) loss_x loss_x 1.0254 (1.1520) acc_x 75.0000 (73.7500) lr 2.0777e-05 eta 0:00:07
epoch [189/200] batch [20/30] time 0.389 (0.457) data 0.258 (0.327) loss_x loss_x 1.6631 (1.1424) acc_x 56.2500 (73.5938) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [25/30] time 0.459 (0.459) data 0.328 (0.329) loss_x loss_x 1.1201 (1.1151) acc_x 78.1250 (74.0000) lr 2.0777e-05 eta 0:00:02
epoch [189/200] batch [30/30] time 0.464 (0.457) data 0.333 (0.327) loss_x loss_x 0.7344 (1.1181) acc_x 75.0000 (74.2708) lr 2.0777e-05 eta 0:00:00
epoch [189/200] batch [5/67] time 0.370 (0.456) data 0.238 (0.326) loss_u loss_u 0.9014 (0.8960) acc_u 15.6250 (13.7500) lr 2.0777e-05 eta 0:00:28
epoch [189/200] batch [10/67] time 0.518 (0.458) data 0.385 (0.328) loss_u loss_u 0.8618 (0.9082) acc_u 18.7500 (12.8125) lr 2.0777e-05 eta 0:00:26
epoch [189/200] batch [15/67] time 0.475 (0.458) data 0.342 (0.327) loss_u loss_u 0.9150 (0.9045) acc_u 9.3750 (12.7083) lr 2.0777e-05 eta 0:00:23
epoch [189/200] batch [20/67] time 0.377 (0.453) data 0.245 (0.322) loss_u loss_u 0.9082 (0.9064) acc_u 12.5000 (12.5000) lr 2.0777e-05 eta 0:00:21
epoch [189/200] batch [25/67] time 0.463 (0.453) data 0.331 (0.322) loss_u loss_u 0.9785 (0.9102) acc_u 3.1250 (12.0000) lr 2.0777e-05 eta 0:00:19
epoch [189/200] batch [30/67] time 0.401 (0.456) data 0.270 (0.325) loss_u loss_u 0.9463 (0.9096) acc_u 9.3750 (12.2917) lr 2.0777e-05 eta 0:00:16
epoch [189/200] batch [35/67] time 0.448 (0.455) data 0.318 (0.324) loss_u loss_u 0.9375 (0.9112) acc_u 6.2500 (11.8750) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [40/67] time 0.471 (0.462) data 0.341 (0.331) loss_u loss_u 0.9180 (0.9074) acc_u 9.3750 (12.1875) lr 2.0777e-05 eta 0:00:12
epoch [189/200] batch [45/67] time 0.385 (0.464) data 0.253 (0.333) loss_u loss_u 0.9233 (0.9053) acc_u 9.3750 (12.5000) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [50/67] time 0.395 (0.462) data 0.263 (0.331) loss_u loss_u 0.9307 (0.9041) acc_u 12.5000 (12.5000) lr 2.0777e-05 eta 0:00:07
epoch [189/200] batch [55/67] time 0.441 (0.461) data 0.309 (0.330) loss_u loss_u 0.9243 (0.9044) acc_u 9.3750 (12.4432) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [60/67] time 0.558 (0.464) data 0.427 (0.332) loss_u loss_u 0.9644 (0.9091) acc_u 6.2500 (11.8750) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [65/67] time 0.381 (0.462) data 0.249 (0.330) loss_u loss_u 0.9019 (0.9088) acc_u 12.5000 (11.9231) lr 2.0777e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.3093, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 970
clean true:965
clean false:5
clean_rate:0.9948453608247423
noisy true:737
noisy false:1429
after delete: len(clean_dataset) 970
after delete: len(noisy_dataset) 2166
epoch [190/200] batch [5/30] time 0.450 (0.458) data 0.320 (0.327) loss_x loss_x 0.8979 (1.1268) acc_x 75.0000 (70.0000) lr 1.7713e-05 eta 0:00:11
epoch [190/200] batch [10/30] time 0.481 (0.450) data 0.350 (0.319) loss_x loss_x 1.1523 (1.1288) acc_x 78.1250 (72.8125) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [15/30] time 0.395 (0.440) data 0.265 (0.310) loss_x loss_x 0.8955 (1.0729) acc_x 84.3750 (73.7500) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [20/30] time 0.461 (0.442) data 0.331 (0.311) loss_x loss_x 1.0156 (1.0365) acc_x 75.0000 (75.1562) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [25/30] time 0.398 (0.458) data 0.267 (0.328) loss_x loss_x 0.9438 (1.0119) acc_x 81.2500 (76.0000) lr 1.7713e-05 eta 0:00:02
epoch [190/200] batch [30/30] time 0.425 (0.463) data 0.293 (0.332) loss_x loss_x 1.0859 (1.0200) acc_x 78.1250 (75.9375) lr 1.7713e-05 eta 0:00:00
epoch [190/200] batch [5/67] time 0.371 (0.461) data 0.241 (0.330) loss_u loss_u 0.9204 (0.9273) acc_u 9.3750 (8.7500) lr 1.7713e-05 eta 0:00:28
epoch [190/200] batch [10/67] time 0.578 (0.461) data 0.447 (0.330) loss_u loss_u 0.8882 (0.9032) acc_u 15.6250 (11.8750) lr 1.7713e-05 eta 0:00:26
epoch [190/200] batch [15/67] time 0.416 (0.463) data 0.286 (0.333) loss_u loss_u 0.9307 (0.9032) acc_u 9.3750 (11.8750) lr 1.7713e-05 eta 0:00:24
epoch [190/200] batch [20/67] time 0.437 (0.461) data 0.306 (0.330) loss_u loss_u 0.9502 (0.9003) acc_u 3.1250 (12.1875) lr 1.7713e-05 eta 0:00:21
epoch [190/200] batch [25/67] time 0.588 (0.460) data 0.456 (0.329) loss_u loss_u 0.9175 (0.8939) acc_u 15.6250 (13.2500) lr 1.7713e-05 eta 0:00:19
epoch [190/200] batch [30/67] time 0.520 (0.460) data 0.388 (0.329) loss_u loss_u 0.8257 (0.8965) acc_u 18.7500 (12.8125) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [35/67] time 0.440 (0.459) data 0.308 (0.328) loss_u loss_u 0.9028 (0.8976) acc_u 9.3750 (12.7679) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [40/67] time 0.517 (0.459) data 0.386 (0.328) loss_u loss_u 0.9644 (0.8996) acc_u 6.2500 (12.4219) lr 1.7713e-05 eta 0:00:12
epoch [190/200] batch [45/67] time 0.429 (0.456) data 0.297 (0.325) loss_u loss_u 0.9722 (0.9036) acc_u 3.1250 (11.8056) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [50/67] time 0.422 (0.454) data 0.289 (0.323) loss_u loss_u 0.9321 (0.9013) acc_u 9.3750 (12.0000) lr 1.7713e-05 eta 0:00:07
epoch [190/200] batch [55/67] time 0.374 (0.451) data 0.243 (0.320) loss_u loss_u 0.9009 (0.9012) acc_u 12.5000 (12.1023) lr 1.7713e-05 eta 0:00:05
epoch [190/200] batch [60/67] time 0.439 (0.452) data 0.307 (0.321) loss_u loss_u 0.9360 (0.8999) acc_u 9.3750 (12.3958) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [65/67] time 0.509 (0.454) data 0.378 (0.323) loss_u loss_u 0.8496 (0.9003) acc_u 18.7500 (12.4038) lr 1.7713e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1442
confident_label rate tensor(0.3106, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 974
clean true:961
clean false:13
clean_rate:0.9866529774127311
noisy true:733
noisy false:1429
after delete: len(clean_dataset) 974
after delete: len(noisy_dataset) 2162
epoch [191/200] batch [5/30] time 0.420 (0.480) data 0.290 (0.350) loss_x loss_x 0.7998 (1.0024) acc_x 78.1250 (76.2500) lr 1.4891e-05 eta 0:00:12
epoch [191/200] batch [10/30] time 0.456 (0.461) data 0.326 (0.330) loss_x loss_x 1.0186 (0.9457) acc_x 62.5000 (76.8750) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [15/30] time 0.456 (0.469) data 0.326 (0.339) loss_x loss_x 1.6631 (1.0388) acc_x 62.5000 (74.7917) lr 1.4891e-05 eta 0:00:07
epoch [191/200] batch [20/30] time 0.453 (0.454) data 0.323 (0.323) loss_x loss_x 0.9741 (1.0402) acc_x 71.8750 (74.0625) lr 1.4891e-05 eta 0:00:04
epoch [191/200] batch [25/30] time 0.453 (0.451) data 0.323 (0.321) loss_x loss_x 1.3877 (1.0618) acc_x 62.5000 (73.1250) lr 1.4891e-05 eta 0:00:02
epoch [191/200] batch [30/30] time 0.445 (0.458) data 0.315 (0.328) loss_x loss_x 0.8286 (1.0540) acc_x 84.3750 (73.5417) lr 1.4891e-05 eta 0:00:00
epoch [191/200] batch [5/67] time 0.386 (0.451) data 0.255 (0.321) loss_u loss_u 0.9907 (0.9299) acc_u 0.0000 (8.1250) lr 1.4891e-05 eta 0:00:27
epoch [191/200] batch [10/67] time 0.580 (0.454) data 0.449 (0.323) loss_u loss_u 0.8740 (0.9269) acc_u 18.7500 (9.6875) lr 1.4891e-05 eta 0:00:25
epoch [191/200] batch [15/67] time 0.373 (0.449) data 0.243 (0.318) loss_u loss_u 0.9194 (0.9210) acc_u 6.2500 (10.2083) lr 1.4891e-05 eta 0:00:23
epoch [191/200] batch [20/67] time 0.493 (0.447) data 0.361 (0.316) loss_u loss_u 0.8262 (0.9136) acc_u 18.7500 (10.6250) lr 1.4891e-05 eta 0:00:20
epoch [191/200] batch [25/67] time 0.432 (0.442) data 0.301 (0.311) loss_u loss_u 0.8984 (0.9143) acc_u 12.5000 (10.2500) lr 1.4891e-05 eta 0:00:18
epoch [191/200] batch [30/67] time 0.494 (0.443) data 0.362 (0.313) loss_u loss_u 0.9854 (0.9118) acc_u 3.1250 (11.0417) lr 1.4891e-05 eta 0:00:16
epoch [191/200] batch [35/67] time 0.414 (0.441) data 0.283 (0.310) loss_u loss_u 0.8984 (0.9078) acc_u 12.5000 (11.2500) lr 1.4891e-05 eta 0:00:14
epoch [191/200] batch [40/67] time 0.446 (0.443) data 0.315 (0.312) loss_u loss_u 0.8784 (0.9095) acc_u 15.6250 (11.1719) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [45/67] time 0.505 (0.450) data 0.373 (0.319) loss_u loss_u 0.9346 (0.9091) acc_u 6.2500 (11.1111) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [50/67] time 0.495 (0.455) data 0.363 (0.324) loss_u loss_u 0.9292 (0.9107) acc_u 12.5000 (11.1250) lr 1.4891e-05 eta 0:00:07
epoch [191/200] batch [55/67] time 0.420 (0.455) data 0.288 (0.324) loss_u loss_u 0.9053 (0.9084) acc_u 12.5000 (11.4773) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [60/67] time 0.525 (0.456) data 0.393 (0.325) loss_u loss_u 0.9473 (0.9081) acc_u 6.2500 (11.4583) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [65/67] time 0.408 (0.456) data 0.275 (0.325) loss_u loss_u 0.9102 (0.9088) acc_u 15.6250 (11.5385) lr 1.4891e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1443
confident_label rate tensor(0.3160, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 991
clean true:982
clean false:9
clean_rate:0.9909182643794148
noisy true:711
noisy false:1434
after delete: len(clean_dataset) 991
after delete: len(noisy_dataset) 2145
epoch [192/200] batch [5/30] time 0.417 (0.434) data 0.286 (0.303) loss_x loss_x 0.9692 (1.3239) acc_x 75.0000 (65.0000) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [10/30] time 0.457 (0.467) data 0.326 (0.336) loss_x loss_x 1.1006 (1.2302) acc_x 81.2500 (69.0625) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [15/30] time 0.545 (0.485) data 0.414 (0.354) loss_x loss_x 0.8462 (1.1494) acc_x 75.0000 (71.4583) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [20/30] time 0.408 (0.475) data 0.278 (0.345) loss_x loss_x 1.4072 (1.1054) acc_x 68.7500 (72.5000) lr 1.2312e-05 eta 0:00:04
epoch [192/200] batch [25/30] time 0.612 (0.480) data 0.481 (0.350) loss_x loss_x 0.9512 (1.0904) acc_x 68.7500 (72.3750) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [30/30] time 0.511 (0.492) data 0.380 (0.361) loss_x loss_x 1.2529 (1.0943) acc_x 62.5000 (71.9792) lr 1.2312e-05 eta 0:00:00
epoch [192/200] batch [5/67] time 0.315 (0.481) data 0.184 (0.350) loss_u loss_u 0.9580 (0.9173) acc_u 6.2500 (10.0000) lr 1.2312e-05 eta 0:00:29
epoch [192/200] batch [10/67] time 0.325 (0.475) data 0.193 (0.344) loss_u loss_u 0.8999 (0.9092) acc_u 15.6250 (11.8750) lr 1.2312e-05 eta 0:00:27
epoch [192/200] batch [15/67] time 0.570 (0.475) data 0.440 (0.344) loss_u loss_u 0.8755 (0.9007) acc_u 15.6250 (13.5417) lr 1.2312e-05 eta 0:00:24
epoch [192/200] batch [20/67] time 0.511 (0.476) data 0.379 (0.345) loss_u loss_u 0.9209 (0.9018) acc_u 12.5000 (13.4375) lr 1.2312e-05 eta 0:00:22
epoch [192/200] batch [25/67] time 0.385 (0.469) data 0.254 (0.338) loss_u loss_u 0.9067 (0.9017) acc_u 9.3750 (13.2500) lr 1.2312e-05 eta 0:00:19
epoch [192/200] batch [30/67] time 0.505 (0.463) data 0.375 (0.332) loss_u loss_u 0.8921 (0.9039) acc_u 12.5000 (12.7083) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [35/67] time 0.685 (0.464) data 0.554 (0.333) loss_u loss_u 0.9722 (0.9033) acc_u 6.2500 (12.7679) lr 1.2312e-05 eta 0:00:14
epoch [192/200] batch [40/67] time 0.559 (0.466) data 0.428 (0.335) loss_u loss_u 0.9546 (0.9079) acc_u 3.1250 (11.9531) lr 1.2312e-05 eta 0:00:12
epoch [192/200] batch [45/67] time 0.552 (0.467) data 0.420 (0.336) loss_u loss_u 0.9399 (0.9070) acc_u 6.2500 (11.8750) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [50/67] time 0.497 (0.469) data 0.367 (0.338) loss_u loss_u 0.8906 (0.9048) acc_u 12.5000 (12.1250) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [55/67] time 0.355 (0.469) data 0.224 (0.338) loss_u loss_u 0.8896 (0.9056) acc_u 9.3750 (11.7614) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [60/67] time 0.342 (0.466) data 0.211 (0.335) loss_u loss_u 0.9326 (0.9057) acc_u 9.3750 (11.8229) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [65/67] time 0.344 (0.467) data 0.213 (0.336) loss_u loss_u 0.8760 (0.9042) acc_u 18.7500 (12.1154) lr 1.2312e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1373
confident_label rate tensor(0.3259, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1022
clean true:1013
clean false:9
clean_rate:0.9911937377690803
noisy true:750
noisy false:1364
after delete: len(clean_dataset) 1022
after delete: len(noisy_dataset) 2114
epoch [193/200] batch [5/31] time 0.562 (0.494) data 0.432 (0.363) loss_x loss_x 1.1816 (1.2667) acc_x 65.6250 (67.5000) lr 9.9763e-06 eta 0:00:12
epoch [193/200] batch [10/31] time 0.546 (0.508) data 0.415 (0.377) loss_x loss_x 1.6514 (1.1527) acc_x 65.6250 (72.5000) lr 9.9763e-06 eta 0:00:10
epoch [193/200] batch [15/31] time 0.437 (0.485) data 0.306 (0.354) loss_x loss_x 0.8428 (1.0937) acc_x 75.0000 (72.9167) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [20/31] time 0.409 (0.477) data 0.278 (0.346) loss_x loss_x 0.7368 (1.0664) acc_x 78.1250 (73.2812) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [25/31] time 0.464 (0.470) data 0.333 (0.339) loss_x loss_x 0.8481 (1.0297) acc_x 75.0000 (74.2500) lr 9.9763e-06 eta 0:00:02
epoch [193/200] batch [30/31] time 0.426 (0.469) data 0.295 (0.338) loss_x loss_x 1.1709 (1.0222) acc_x 75.0000 (74.3750) lr 9.9763e-06 eta 0:00:00
epoch [193/200] batch [5/66] time 0.439 (0.464) data 0.306 (0.333) loss_u loss_u 0.9497 (0.8900) acc_u 6.2500 (15.0000) lr 9.9763e-06 eta 0:00:28
epoch [193/200] batch [10/66] time 0.472 (0.460) data 0.341 (0.329) loss_u loss_u 0.9629 (0.9027) acc_u 6.2500 (13.4375) lr 9.9763e-06 eta 0:00:25
epoch [193/200] batch [15/66] time 0.454 (0.461) data 0.324 (0.330) loss_u loss_u 0.9141 (0.9056) acc_u 15.6250 (13.1250) lr 9.9763e-06 eta 0:00:23
epoch [193/200] batch [20/66] time 0.375 (0.467) data 0.244 (0.336) loss_u loss_u 0.9565 (0.9068) acc_u 3.1250 (12.1875) lr 9.9763e-06 eta 0:00:21
epoch [193/200] batch [25/66] time 0.471 (0.465) data 0.340 (0.334) loss_u loss_u 0.9414 (0.9079) acc_u 9.3750 (12.1250) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [30/66] time 0.385 (0.462) data 0.253 (0.331) loss_u loss_u 0.9258 (0.9106) acc_u 9.3750 (11.8750) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [35/66] time 0.592 (0.463) data 0.461 (0.332) loss_u loss_u 0.9570 (0.9049) acc_u 3.1250 (12.5893) lr 9.9763e-06 eta 0:00:14
epoch [193/200] batch [40/66] time 0.453 (0.463) data 0.323 (0.332) loss_u loss_u 0.8442 (0.9030) acc_u 18.7500 (12.7344) lr 9.9763e-06 eta 0:00:12
epoch [193/200] batch [45/66] time 0.451 (0.461) data 0.318 (0.330) loss_u loss_u 0.8892 (0.9011) acc_u 18.7500 (12.9167) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [50/66] time 0.630 (0.468) data 0.499 (0.337) loss_u loss_u 0.8643 (0.9019) acc_u 15.6250 (12.8750) lr 9.9763e-06 eta 0:00:07
epoch [193/200] batch [55/66] time 0.349 (0.469) data 0.217 (0.338) loss_u loss_u 0.9443 (0.9047) acc_u 6.2500 (12.4432) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [60/66] time 0.462 (0.468) data 0.330 (0.337) loss_u loss_u 0.9468 (0.9055) acc_u 9.3750 (12.3438) lr 9.9763e-06 eta 0:00:02
epoch [193/200] batch [65/66] time 0.513 (0.469) data 0.382 (0.338) loss_u loss_u 0.8838 (0.9067) acc_u 15.6250 (12.1635) lr 9.9763e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1475
confident_label rate tensor(0.3068, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 962
clean true:952
clean false:10
clean_rate:0.9896049896049897
noisy true:709
noisy false:1465
after delete: len(clean_dataset) 962
after delete: len(noisy_dataset) 2174
epoch [194/200] batch [5/30] time 0.594 (0.521) data 0.464 (0.391) loss_x loss_x 0.7578 (1.0034) acc_x 87.5000 (73.7500) lr 7.8853e-06 eta 0:00:13
epoch [194/200] batch [10/30] time 0.492 (0.495) data 0.361 (0.364) loss_x loss_x 1.4023 (1.1294) acc_x 62.5000 (71.5625) lr 7.8853e-06 eta 0:00:09
epoch [194/200] batch [15/30] time 0.504 (0.501) data 0.373 (0.371) loss_x loss_x 1.3467 (1.1614) acc_x 59.3750 (70.4167) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [20/30] time 0.422 (0.485) data 0.290 (0.355) loss_x loss_x 0.8721 (1.0886) acc_x 81.2500 (72.9688) lr 7.8853e-06 eta 0:00:04
epoch [194/200] batch [25/30] time 0.357 (0.464) data 0.226 (0.333) loss_x loss_x 1.5742 (1.1050) acc_x 65.6250 (73.2500) lr 7.8853e-06 eta 0:00:02
epoch [194/200] batch [30/30] time 0.460 (0.469) data 0.329 (0.338) loss_x loss_x 0.7124 (1.0785) acc_x 81.2500 (74.1667) lr 7.8853e-06 eta 0:00:00
epoch [194/200] batch [5/67] time 0.516 (0.466) data 0.385 (0.336) loss_u loss_u 0.8555 (0.8926) acc_u 18.7500 (15.6250) lr 7.8853e-06 eta 0:00:28
epoch [194/200] batch [10/67] time 0.404 (0.459) data 0.272 (0.328) loss_u loss_u 0.9292 (0.9053) acc_u 12.5000 (13.7500) lr 7.8853e-06 eta 0:00:26
epoch [194/200] batch [15/67] time 0.532 (0.464) data 0.401 (0.333) loss_u loss_u 0.8848 (0.8982) acc_u 15.6250 (14.1667) lr 7.8853e-06 eta 0:00:24
epoch [194/200] batch [20/67] time 0.506 (0.463) data 0.375 (0.333) loss_u loss_u 0.9219 (0.9010) acc_u 12.5000 (13.5938) lr 7.8853e-06 eta 0:00:21
epoch [194/200] batch [25/67] time 0.353 (0.458) data 0.221 (0.327) loss_u loss_u 0.9463 (0.9033) acc_u 6.2500 (12.7500) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [30/67] time 0.523 (0.458) data 0.392 (0.327) loss_u loss_u 0.8418 (0.9025) acc_u 18.7500 (13.0208) lr 7.8853e-06 eta 0:00:16
epoch [194/200] batch [35/67] time 0.363 (0.459) data 0.232 (0.328) loss_u loss_u 0.9722 (0.9051) acc_u 3.1250 (12.5000) lr 7.8853e-06 eta 0:00:14
epoch [194/200] batch [40/67] time 0.357 (0.461) data 0.226 (0.330) loss_u loss_u 0.8730 (0.9059) acc_u 15.6250 (12.3438) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [45/67] time 0.451 (0.459) data 0.317 (0.328) loss_u loss_u 0.9160 (0.9066) acc_u 9.3750 (12.2917) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [50/67] time 0.449 (0.460) data 0.318 (0.329) loss_u loss_u 0.7983 (0.9060) acc_u 31.2500 (12.3750) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [55/67] time 0.475 (0.462) data 0.344 (0.331) loss_u loss_u 0.9253 (0.9054) acc_u 9.3750 (12.3864) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [60/67] time 0.489 (0.460) data 0.359 (0.329) loss_u loss_u 0.8545 (0.9037) acc_u 15.6250 (12.6042) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [65/67] time 0.446 (0.460) data 0.315 (0.329) loss_u loss_u 0.9331 (0.9022) acc_u 9.3750 (12.6923) lr 7.8853e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1427
confident_label rate tensor(0.3198, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1003
clean true:994
clean false:9
clean_rate:0.9910269192422732
noisy true:715
noisy false:1418
after delete: len(clean_dataset) 1003
after delete: len(noisy_dataset) 2133
epoch [195/200] batch [5/31] time 0.459 (0.451) data 0.329 (0.320) loss_x loss_x 1.2207 (1.1442) acc_x 75.0000 (71.8750) lr 6.0390e-06 eta 0:00:11
epoch [195/200] batch [10/31] time 0.485 (0.502) data 0.355 (0.371) loss_x loss_x 1.1934 (1.0394) acc_x 75.0000 (75.3125) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [15/31] time 0.509 (0.497) data 0.379 (0.366) loss_x loss_x 1.4346 (1.1197) acc_x 71.8750 (73.5417) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [20/31] time 0.562 (0.504) data 0.432 (0.373) loss_x loss_x 0.8760 (1.0650) acc_x 78.1250 (75.3125) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [25/31] time 0.513 (0.498) data 0.383 (0.368) loss_x loss_x 1.0361 (1.0884) acc_x 78.1250 (75.1250) lr 6.0390e-06 eta 0:00:02
epoch [195/200] batch [30/31] time 0.468 (0.500) data 0.337 (0.369) loss_x loss_x 1.0342 (1.0328) acc_x 78.1250 (76.2500) lr 6.0390e-06 eta 0:00:00
epoch [195/200] batch [5/66] time 0.428 (0.491) data 0.298 (0.360) loss_u loss_u 0.9248 (0.9283) acc_u 9.3750 (10.0000) lr 6.0390e-06 eta 0:00:29
epoch [195/200] batch [10/66] time 0.618 (0.494) data 0.487 (0.364) loss_u loss_u 0.9365 (0.9138) acc_u 6.2500 (11.2500) lr 6.0390e-06 eta 0:00:27
epoch [195/200] batch [15/66] time 0.451 (0.484) data 0.318 (0.353) loss_u loss_u 0.8901 (0.9183) acc_u 12.5000 (10.4167) lr 6.0390e-06 eta 0:00:24
epoch [195/200] batch [20/66] time 0.329 (0.479) data 0.197 (0.348) loss_u loss_u 0.8374 (0.9101) acc_u 18.7500 (11.4062) lr 6.0390e-06 eta 0:00:22
epoch [195/200] batch [25/66] time 0.387 (0.475) data 0.256 (0.344) loss_u loss_u 0.8574 (0.9075) acc_u 18.7500 (11.7500) lr 6.0390e-06 eta 0:00:19
epoch [195/200] batch [30/66] time 0.549 (0.472) data 0.418 (0.341) loss_u loss_u 0.9136 (0.9061) acc_u 9.3750 (11.9792) lr 6.0390e-06 eta 0:00:16
epoch [195/200] batch [35/66] time 0.549 (0.479) data 0.419 (0.348) loss_u loss_u 0.7812 (0.9049) acc_u 31.2500 (11.9643) lr 6.0390e-06 eta 0:00:14
epoch [195/200] batch [40/66] time 0.470 (0.478) data 0.338 (0.347) loss_u loss_u 0.8647 (0.9045) acc_u 18.7500 (12.0312) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [45/66] time 0.510 (0.478) data 0.379 (0.346) loss_u loss_u 0.8564 (0.9039) acc_u 15.6250 (12.2222) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [50/66] time 0.374 (0.478) data 0.243 (0.347) loss_u loss_u 0.9497 (0.9048) acc_u 6.2500 (12.1250) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [55/66] time 0.489 (0.479) data 0.358 (0.348) loss_u loss_u 0.8564 (0.9054) acc_u 18.7500 (12.0455) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [60/66] time 0.460 (0.481) data 0.328 (0.350) loss_u loss_u 0.8970 (0.9055) acc_u 12.5000 (12.0312) lr 6.0390e-06 eta 0:00:02
epoch [195/200] batch [65/66] time 0.523 (0.481) data 0.391 (0.349) loss_u loss_u 0.9087 (0.9053) acc_u 12.5000 (12.1154) lr 6.0390e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1456
confident_label rate tensor(0.3131, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 982
clean true:974
clean false:8
clean_rate:0.9918533604887984
noisy true:706
noisy false:1448
after delete: len(clean_dataset) 982
after delete: len(noisy_dataset) 2154
epoch [196/200] batch [5/30] time 0.419 (0.444) data 0.289 (0.313) loss_x loss_x 1.2197 (1.0745) acc_x 68.7500 (73.1250) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [10/30] time 0.440 (0.442) data 0.309 (0.311) loss_x loss_x 0.8711 (1.1454) acc_x 81.2500 (73.1250) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [15/30] time 0.401 (0.440) data 0.270 (0.309) loss_x loss_x 0.8511 (1.1091) acc_x 81.2500 (74.5833) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [20/30] time 0.476 (0.456) data 0.346 (0.325) loss_x loss_x 1.4961 (1.1078) acc_x 68.7500 (74.3750) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [25/30] time 0.503 (0.460) data 0.372 (0.329) loss_x loss_x 0.8472 (1.1002) acc_x 81.2500 (74.6250) lr 4.4380e-06 eta 0:00:02
epoch [196/200] batch [30/30] time 0.428 (0.462) data 0.297 (0.331) loss_x loss_x 1.1660 (1.0870) acc_x 65.6250 (73.7500) lr 4.4380e-06 eta 0:00:00
epoch [196/200] batch [5/67] time 0.521 (0.461) data 0.389 (0.330) loss_u loss_u 0.8403 (0.9086) acc_u 15.6250 (10.6250) lr 4.4380e-06 eta 0:00:28
epoch [196/200] batch [10/67] time 0.436 (0.464) data 0.305 (0.333) loss_u loss_u 0.9297 (0.9033) acc_u 9.3750 (11.8750) lr 4.4380e-06 eta 0:00:26
epoch [196/200] batch [15/67] time 0.705 (0.470) data 0.574 (0.339) loss_u loss_u 0.9551 (0.8990) acc_u 3.1250 (11.8750) lr 4.4380e-06 eta 0:00:24
epoch [196/200] batch [20/67] time 0.480 (0.474) data 0.348 (0.343) loss_u loss_u 0.9688 (0.8986) acc_u 3.1250 (12.1875) lr 4.4380e-06 eta 0:00:22
epoch [196/200] batch [25/67] time 0.371 (0.472) data 0.239 (0.340) loss_u loss_u 0.9487 (0.9055) acc_u 6.2500 (11.8750) lr 4.4380e-06 eta 0:00:19
epoch [196/200] batch [30/67] time 0.575 (0.478) data 0.443 (0.347) loss_u loss_u 0.9097 (0.9055) acc_u 9.3750 (11.8750) lr 4.4380e-06 eta 0:00:17
epoch [196/200] batch [35/67] time 0.383 (0.473) data 0.249 (0.341) loss_u loss_u 0.9644 (0.9099) acc_u 6.2500 (11.4286) lr 4.4380e-06 eta 0:00:15
epoch [196/200] batch [40/67] time 0.397 (0.469) data 0.264 (0.338) loss_u loss_u 0.9272 (0.9094) acc_u 9.3750 (11.3281) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [45/67] time 0.358 (0.466) data 0.226 (0.335) loss_u loss_u 0.9390 (0.9118) acc_u 9.3750 (11.1111) lr 4.4380e-06 eta 0:00:10
epoch [196/200] batch [50/67] time 0.452 (0.466) data 0.321 (0.334) loss_u loss_u 0.9761 (0.9112) acc_u 3.1250 (11.1250) lr 4.4380e-06 eta 0:00:07
epoch [196/200] batch [55/67] time 0.495 (0.464) data 0.363 (0.333) loss_u loss_u 0.9136 (0.9091) acc_u 9.3750 (11.3636) lr 4.4380e-06 eta 0:00:05
epoch [196/200] batch [60/67] time 0.485 (0.463) data 0.354 (0.332) loss_u loss_u 0.8691 (0.9062) acc_u 18.7500 (11.8750) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [65/67] time 0.396 (0.460) data 0.264 (0.329) loss_u loss_u 0.9395 (0.9081) acc_u 6.2500 (11.7308) lr 4.4380e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1436
confident_label rate tensor(0.3103, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 973
clean true:964
clean false:9
clean_rate:0.9907502569373073
noisy true:736
noisy false:1427
after delete: len(clean_dataset) 973
after delete: len(noisy_dataset) 2163
epoch [197/200] batch [5/30] time 0.404 (0.435) data 0.275 (0.305) loss_x loss_x 1.3047 (1.0303) acc_x 78.1250 (78.7500) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [10/30] time 0.448 (0.419) data 0.317 (0.289) loss_x loss_x 0.8970 (0.9297) acc_x 78.1250 (79.3750) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [15/30] time 0.478 (0.441) data 0.347 (0.311) loss_x loss_x 1.3496 (0.9533) acc_x 62.5000 (78.1250) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [20/30] time 0.489 (0.451) data 0.358 (0.321) loss_x loss_x 0.6089 (0.9169) acc_x 84.3750 (78.9062) lr 3.0827e-06 eta 0:00:04
epoch [197/200] batch [25/30] time 0.427 (0.460) data 0.297 (0.329) loss_x loss_x 1.1836 (0.9338) acc_x 71.8750 (78.3750) lr 3.0827e-06 eta 0:00:02
epoch [197/200] batch [30/30] time 0.544 (0.462) data 0.413 (0.331) loss_x loss_x 1.4414 (0.9438) acc_x 75.0000 (78.4375) lr 3.0827e-06 eta 0:00:00
epoch [197/200] batch [5/67] time 0.452 (0.459) data 0.321 (0.328) loss_u loss_u 0.8457 (0.8750) acc_u 21.8750 (16.2500) lr 3.0827e-06 eta 0:00:28
epoch [197/200] batch [10/67] time 0.411 (0.454) data 0.281 (0.323) loss_u loss_u 0.8950 (0.9049) acc_u 12.5000 (12.5000) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [15/67] time 0.563 (0.453) data 0.432 (0.322) loss_u loss_u 0.8657 (0.8992) acc_u 15.6250 (12.5000) lr 3.0827e-06 eta 0:00:23
epoch [197/200] batch [20/67] time 0.367 (0.452) data 0.236 (0.322) loss_u loss_u 0.9165 (0.9096) acc_u 9.3750 (11.2500) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [25/67] time 0.542 (0.455) data 0.411 (0.324) loss_u loss_u 0.9497 (0.9129) acc_u 6.2500 (10.7500) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [30/67] time 0.336 (0.452) data 0.204 (0.321) loss_u loss_u 0.9126 (0.9098) acc_u 15.6250 (11.5625) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [35/67] time 0.547 (0.453) data 0.415 (0.322) loss_u loss_u 0.8472 (0.9050) acc_u 21.8750 (12.2321) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [40/67] time 0.540 (0.457) data 0.407 (0.326) loss_u loss_u 0.8955 (0.9033) acc_u 15.6250 (12.5000) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [45/67] time 0.416 (0.465) data 0.284 (0.334) loss_u loss_u 0.9355 (0.9071) acc_u 6.2500 (11.8750) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [50/67] time 0.622 (0.462) data 0.490 (0.331) loss_u loss_u 0.8677 (0.9058) acc_u 18.7500 (12.3750) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [55/67] time 0.451 (0.459) data 0.320 (0.328) loss_u loss_u 0.8550 (0.9042) acc_u 12.5000 (12.2727) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [60/67] time 0.462 (0.458) data 0.330 (0.327) loss_u loss_u 0.9058 (0.9036) acc_u 12.5000 (12.2917) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [65/67] time 0.501 (0.462) data 0.368 (0.331) loss_u loss_u 0.9023 (0.9056) acc_u 9.3750 (11.9712) lr 3.0827e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1441
confident_label rate tensor(0.3160, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 991
clean true:981
clean false:10
clean_rate:0.9899091826437941
noisy true:714
noisy false:1431
after delete: len(clean_dataset) 991
after delete: len(noisy_dataset) 2145
epoch [198/200] batch [5/30] time 0.502 (0.517) data 0.372 (0.386) loss_x loss_x 1.1738 (1.1186) acc_x 75.0000 (78.1250) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [10/30] time 0.472 (0.490) data 0.341 (0.359) loss_x loss_x 0.9897 (1.1185) acc_x 71.8750 (75.6250) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [15/30] time 0.419 (0.462) data 0.288 (0.331) loss_x loss_x 0.9473 (1.0782) acc_x 81.2500 (76.0417) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [20/30] time 0.698 (0.468) data 0.567 (0.337) loss_x loss_x 1.0850 (1.0721) acc_x 75.0000 (75.3125) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [25/30] time 0.463 (0.472) data 0.332 (0.341) loss_x loss_x 0.8745 (1.0673) acc_x 81.2500 (75.0000) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [30/30] time 0.579 (0.475) data 0.447 (0.344) loss_x loss_x 0.8516 (1.0618) acc_x 75.0000 (74.2708) lr 1.9733e-06 eta 0:00:00
epoch [198/200] batch [5/67] time 0.417 (0.470) data 0.285 (0.339) loss_u loss_u 0.9204 (0.9076) acc_u 9.3750 (14.3750) lr 1.9733e-06 eta 0:00:29
epoch [198/200] batch [10/67] time 0.361 (0.463) data 0.229 (0.331) loss_u loss_u 0.9214 (0.9074) acc_u 9.3750 (13.4375) lr 1.9733e-06 eta 0:00:26
epoch [198/200] batch [15/67] time 0.400 (0.460) data 0.269 (0.329) loss_u loss_u 0.9150 (0.8948) acc_u 9.3750 (13.9583) lr 1.9733e-06 eta 0:00:23
epoch [198/200] batch [20/67] time 0.408 (0.456) data 0.276 (0.325) loss_u loss_u 0.9121 (0.8969) acc_u 9.3750 (13.7500) lr 1.9733e-06 eta 0:00:21
epoch [198/200] batch [25/67] time 0.372 (0.460) data 0.239 (0.329) loss_u loss_u 0.9731 (0.9004) acc_u 3.1250 (12.8750) lr 1.9733e-06 eta 0:00:19
epoch [198/200] batch [30/67] time 0.337 (0.458) data 0.205 (0.326) loss_u loss_u 0.9468 (0.9045) acc_u 6.2500 (12.6042) lr 1.9733e-06 eta 0:00:16
epoch [198/200] batch [35/67] time 0.368 (0.454) data 0.237 (0.322) loss_u loss_u 0.8481 (0.9037) acc_u 18.7500 (12.8571) lr 1.9733e-06 eta 0:00:14
epoch [198/200] batch [40/67] time 0.542 (0.454) data 0.411 (0.323) loss_u loss_u 0.8730 (0.9052) acc_u 15.6250 (12.5781) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [45/67] time 0.498 (0.452) data 0.366 (0.320) loss_u loss_u 0.9336 (0.9025) acc_u 6.2500 (12.9167) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [50/67] time 0.587 (0.455) data 0.456 (0.324) loss_u loss_u 0.9360 (0.9041) acc_u 9.3750 (12.6875) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [55/67] time 0.514 (0.456) data 0.383 (0.325) loss_u loss_u 0.9668 (0.9048) acc_u 3.1250 (12.6705) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [60/67] time 0.704 (0.459) data 0.573 (0.327) loss_u loss_u 0.9155 (0.9067) acc_u 9.3750 (12.3958) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [65/67] time 0.562 (0.459) data 0.432 (0.328) loss_u loss_u 0.9058 (0.9062) acc_u 15.6250 (12.2596) lr 1.9733e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1436
confident_label rate tensor(0.3119, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 978
clean true:970
clean false:8
clean_rate:0.9918200408997955
noisy true:730
noisy false:1428
after delete: len(clean_dataset) 978
after delete: len(noisy_dataset) 2158
epoch [199/200] batch [5/30] time 0.378 (0.408) data 0.247 (0.277) loss_x loss_x 0.9624 (1.1340) acc_x 75.0000 (75.0000) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [10/30] time 0.668 (0.468) data 0.537 (0.338) loss_x loss_x 1.1221 (1.0618) acc_x 78.1250 (75.9375) lr 1.1101e-06 eta 0:00:09
epoch [199/200] batch [15/30] time 0.549 (0.488) data 0.418 (0.358) loss_x loss_x 0.8296 (1.0363) acc_x 75.0000 (75.4167) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [20/30] time 0.389 (0.482) data 0.259 (0.352) loss_x loss_x 0.9688 (1.0114) acc_x 71.8750 (75.0000) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [25/30] time 0.500 (0.476) data 0.369 (0.345) loss_x loss_x 0.9204 (0.9903) acc_x 75.0000 (75.5000) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [30/30] time 0.769 (0.483) data 0.638 (0.352) loss_x loss_x 0.9136 (1.0120) acc_x 78.1250 (75.1042) lr 1.1101e-06 eta 0:00:00
epoch [199/200] batch [5/67] time 0.431 (0.493) data 0.299 (0.362) loss_u loss_u 0.9009 (0.9145) acc_u 12.5000 (10.0000) lr 1.1101e-06 eta 0:00:30
epoch [199/200] batch [10/67] time 0.409 (0.492) data 0.278 (0.361) loss_u loss_u 0.8423 (0.9082) acc_u 15.6250 (10.6250) lr 1.1101e-06 eta 0:00:28
epoch [199/200] batch [15/67] time 0.472 (0.490) data 0.340 (0.359) loss_u loss_u 0.8037 (0.9039) acc_u 21.8750 (11.0417) lr 1.1101e-06 eta 0:00:25
epoch [199/200] batch [20/67] time 0.463 (0.485) data 0.331 (0.354) loss_u loss_u 0.8516 (0.8979) acc_u 15.6250 (12.0312) lr 1.1101e-06 eta 0:00:22
epoch [199/200] batch [25/67] time 0.494 (0.480) data 0.363 (0.349) loss_u loss_u 0.9019 (0.8999) acc_u 12.5000 (12.0000) lr 1.1101e-06 eta 0:00:20
epoch [199/200] batch [30/67] time 0.489 (0.475) data 0.357 (0.344) loss_u loss_u 0.9434 (0.9027) acc_u 6.2500 (11.6667) lr 1.1101e-06 eta 0:00:17
epoch [199/200] batch [35/67] time 0.533 (0.478) data 0.402 (0.347) loss_u loss_u 0.8462 (0.9001) acc_u 18.7500 (11.9643) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [40/67] time 0.483 (0.474) data 0.351 (0.343) loss_u loss_u 0.9590 (0.9013) acc_u 6.2500 (12.0312) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [45/67] time 0.337 (0.471) data 0.207 (0.340) loss_u loss_u 0.9287 (0.9016) acc_u 9.3750 (12.3611) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [50/67] time 0.449 (0.472) data 0.317 (0.341) loss_u loss_u 0.8413 (0.8966) acc_u 18.7500 (12.8125) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [55/67] time 0.452 (0.470) data 0.321 (0.339) loss_u loss_u 0.9302 (0.8984) acc_u 9.3750 (12.6136) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [60/67] time 0.384 (0.467) data 0.252 (0.336) loss_u loss_u 0.9297 (0.9002) acc_u 9.3750 (12.3958) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [65/67] time 0.523 (0.471) data 0.392 (0.340) loss_u loss_u 0.8691 (0.8996) acc_u 18.7500 (12.5000) lr 1.1101e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.3154, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 989
clean true:978
clean false:11
clean_rate:0.9888776541961577
noisy true:724
noisy false:1423
after delete: len(clean_dataset) 989
after delete: len(noisy_dataset) 2147
epoch [200/200] batch [5/30] time 0.540 (0.449) data 0.410 (0.318) loss_x loss_x 0.7251 (0.8205) acc_x 81.2500 (82.5000) lr 4.9344e-07 eta 0:00:11
epoch [200/200] batch [10/30] time 0.358 (0.450) data 0.227 (0.319) loss_x loss_x 1.4277 (0.9447) acc_x 71.8750 (79.6875) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [15/30] time 0.585 (0.454) data 0.455 (0.323) loss_x loss_x 0.8579 (0.9683) acc_x 81.2500 (77.2917) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [20/30] time 0.434 (0.447) data 0.304 (0.317) loss_x loss_x 1.0840 (0.9781) acc_x 68.7500 (75.7812) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [25/30] time 0.432 (0.445) data 0.302 (0.315) loss_x loss_x 1.0342 (0.9942) acc_x 68.7500 (74.8750) lr 4.9344e-07 eta 0:00:02
epoch [200/200] batch [30/30] time 0.427 (0.441) data 0.296 (0.310) loss_x loss_x 1.4570 (1.0006) acc_x 59.3750 (74.6875) lr 4.9344e-07 eta 0:00:00
epoch [200/200] batch [5/67] time 0.413 (0.444) data 0.282 (0.313) loss_u loss_u 0.7832 (0.8684) acc_u 28.1250 (15.6250) lr 4.9344e-07 eta 0:00:27
epoch [200/200] batch [10/67] time 0.419 (0.444) data 0.288 (0.313) loss_u loss_u 0.8691 (0.8814) acc_u 15.6250 (15.0000) lr 4.9344e-07 eta 0:00:25
epoch [200/200] batch [15/67] time 0.368 (0.439) data 0.236 (0.308) loss_u loss_u 0.9053 (0.8842) acc_u 12.5000 (14.5833) lr 4.9344e-07 eta 0:00:22
epoch [200/200] batch [20/67] time 0.444 (0.439) data 0.313 (0.309) loss_u loss_u 0.8867 (0.8962) acc_u 18.7500 (12.8125) lr 4.9344e-07 eta 0:00:20
epoch [200/200] batch [25/67] time 0.401 (0.441) data 0.266 (0.310) loss_u loss_u 0.8706 (0.9000) acc_u 15.6250 (12.2500) lr 4.9344e-07 eta 0:00:18
epoch [200/200] batch [30/67] time 0.408 (0.441) data 0.276 (0.310) loss_u loss_u 0.9253 (0.8998) acc_u 15.6250 (12.5000) lr 4.9344e-07 eta 0:00:16
epoch [200/200] batch [35/67] time 0.436 (0.439) data 0.303 (0.308) loss_u loss_u 0.8770 (0.9027) acc_u 21.8750 (12.4107) lr 4.9344e-07 eta 0:00:14
epoch [200/200] batch [40/67] time 0.343 (0.441) data 0.212 (0.310) loss_u loss_u 0.9502 (0.9062) acc_u 6.2500 (12.1094) lr 4.9344e-07 eta 0:00:11
epoch [200/200] batch [45/67] time 0.376 (0.438) data 0.245 (0.307) loss_u loss_u 0.9834 (0.9034) acc_u 3.1250 (12.3611) lr 4.9344e-07 eta 0:00:09
epoch [200/200] batch [50/67] time 0.369 (0.440) data 0.238 (0.309) loss_u loss_u 0.9585 (0.9052) acc_u 6.2500 (12.0000) lr 4.9344e-07 eta 0:00:07
epoch [200/200] batch [55/67] time 0.446 (0.441) data 0.314 (0.310) loss_u loss_u 0.8955 (0.9066) acc_u 12.5000 (11.8182) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [60/67] time 0.335 (0.440) data 0.204 (0.309) loss_u loss_u 0.7964 (0.9038) acc_u 25.0000 (12.1875) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [65/67] time 0.466 (0.440) data 0.334 (0.309) loss_u loss_u 0.9346 (0.9041) acc_u 6.2500 (12.1154) lr 4.9344e-07 eta 0:00:00
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.50/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 5,224
* accuracy: 65.0%
* error: 35.0%
* macro_f1: 63.5%
Elapsed: 5:06:47
