***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.625', 'DATASET.NOISE_TYPE', 'asym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.625/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.625
  NOISE_TYPE: asym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.625/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.625/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.0883, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 277
clean true:216
clean false:61
clean_rate:0.779783393501805
noisy true:349
noisy false:2510
after delete: len(clean_dataset) 277
after delete: len(noisy_dataset) 2859
epoch [1/200] batch [5/8] time 0.406 (0.517) data 0.276 (0.365) loss_x loss_x 2.8828 (2.9793) acc_x 46.8750 (43.7500) lr 1.0000e-05 eta 0:00:01
epoch [1/200] batch [5/89] time 0.452 (0.497) data 0.321 (0.357) loss_u loss_u 0.9775 (0.9762) acc_u 3.1250 (6.2500) lr 1.0000e-05 eta 0:00:41
epoch [1/200] batch [10/89] time 0.370 (0.491) data 0.240 (0.354) loss_u loss_u 0.9619 (0.9720) acc_u 6.2500 (5.9375) lr 1.0000e-05 eta 0:00:38
epoch [1/200] batch [15/89] time 0.523 (0.489) data 0.392 (0.353) loss_u loss_u 0.9609 (0.9708) acc_u 12.5000 (6.2500) lr 1.0000e-05 eta 0:00:36
epoch [1/200] batch [20/89] time 0.544 (0.485) data 0.413 (0.350) loss_u loss_u 0.9624 (0.9691) acc_u 9.3750 (7.3438) lr 1.0000e-05 eta 0:00:33
epoch [1/200] batch [25/89] time 0.427 (0.481) data 0.297 (0.346) loss_u loss_u 0.9473 (0.9661) acc_u 12.5000 (7.7500) lr 1.0000e-05 eta 0:00:30
epoch [1/200] batch [30/89] time 0.503 (0.480) data 0.373 (0.346) loss_u loss_u 0.9600 (0.9659) acc_u 3.1250 (7.1875) lr 1.0000e-05 eta 0:00:28
epoch [1/200] batch [35/89] time 0.519 (0.480) data 0.388 (0.346) loss_u loss_u 0.9712 (0.9656) acc_u 3.1250 (7.3214) lr 1.0000e-05 eta 0:00:25
epoch [1/200] batch [40/89] time 0.410 (0.475) data 0.280 (0.342) loss_u loss_u 0.9893 (0.9661) acc_u 0.0000 (6.7969) lr 1.0000e-05 eta 0:00:23
epoch [1/200] batch [45/89] time 0.468 (0.475) data 0.338 (0.342) loss_u loss_u 0.9404 (0.9636) acc_u 21.8750 (7.2222) lr 1.0000e-05 eta 0:00:20
epoch [1/200] batch [50/89] time 0.458 (0.476) data 0.327 (0.343) loss_u loss_u 0.9600 (0.9629) acc_u 3.1250 (7.2500) lr 1.0000e-05 eta 0:00:18
epoch [1/200] batch [55/89] time 0.508 (0.477) data 0.376 (0.344) loss_u loss_u 0.9590 (0.9633) acc_u 9.3750 (6.9318) lr 1.0000e-05 eta 0:00:16
epoch [1/200] batch [60/89] time 0.436 (0.479) data 0.304 (0.346) loss_u loss_u 0.9614 (0.9635) acc_u 6.2500 (6.7188) lr 1.0000e-05 eta 0:00:13
epoch [1/200] batch [65/89] time 0.520 (0.482) data 0.389 (0.349) loss_u loss_u 0.9385 (0.9631) acc_u 6.2500 (6.6827) lr 1.0000e-05 eta 0:00:11
epoch [1/200] batch [70/89] time 0.421 (0.481) data 0.290 (0.348) loss_u loss_u 0.9580 (0.9629) acc_u 6.2500 (6.7857) lr 1.0000e-05 eta 0:00:09
epoch [1/200] batch [75/89] time 0.422 (0.479) data 0.291 (0.346) loss_u loss_u 0.9731 (0.9629) acc_u 3.1250 (6.8333) lr 1.0000e-05 eta 0:00:06
epoch [1/200] batch [80/89] time 0.529 (0.480) data 0.397 (0.347) loss_u loss_u 0.9692 (0.9624) acc_u 9.3750 (6.9141) lr 1.0000e-05 eta 0:00:04
epoch [1/200] batch [85/89] time 0.539 (0.484) data 0.410 (0.351) loss_u loss_u 0.9487 (0.9624) acc_u 12.5000 (6.8382) lr 1.0000e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2292
confident_label rate tensor(0.1164, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 365
clean true:312
clean false:53
clean_rate:0.8547945205479452
noisy true:532
noisy false:2239
after delete: len(clean_dataset) 365
after delete: len(noisy_dataset) 2771
epoch [2/200] batch [5/11] time 0.459 (0.555) data 0.328 (0.419) loss_x loss_x 2.0781 (2.2080) acc_x 43.7500 (43.1250) lr 2.0000e-03 eta 0:00:03
epoch [2/200] batch [10/11] time 0.539 (0.515) data 0.408 (0.382) loss_x loss_x 1.9609 (2.0212) acc_x 31.2500 (44.6875) lr 2.0000e-03 eta 0:00:00
epoch [2/200] batch [5/86] time 0.452 (0.491) data 0.321 (0.358) loss_u loss_u 0.8623 (0.8952) acc_u 15.6250 (13.1250) lr 2.0000e-03 eta 0:00:39
epoch [2/200] batch [10/86] time 0.488 (0.486) data 0.357 (0.354) loss_u loss_u 0.9419 (0.9137) acc_u 3.1250 (10.6250) lr 2.0000e-03 eta 0:00:36
epoch [2/200] batch [15/86] time 0.422 (0.478) data 0.291 (0.345) loss_u loss_u 0.9062 (0.9112) acc_u 18.7500 (11.8750) lr 2.0000e-03 eta 0:00:33
epoch [2/200] batch [20/86] time 0.444 (0.475) data 0.313 (0.343) loss_u loss_u 0.9175 (0.9082) acc_u 15.6250 (12.1875) lr 2.0000e-03 eta 0:00:31
epoch [2/200] batch [25/86] time 0.378 (0.465) data 0.247 (0.333) loss_u loss_u 0.8892 (0.9056) acc_u 12.5000 (12.7500) lr 2.0000e-03 eta 0:00:28
epoch [2/200] batch [30/86] time 0.376 (0.460) data 0.245 (0.328) loss_u loss_u 0.9053 (0.9100) acc_u 12.5000 (12.0833) lr 2.0000e-03 eta 0:00:25
epoch [2/200] batch [35/86] time 0.399 (0.451) data 0.267 (0.319) loss_u loss_u 0.8794 (0.9102) acc_u 9.3750 (11.7857) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [40/86] time 0.404 (0.449) data 0.272 (0.317) loss_u loss_u 0.9048 (0.9083) acc_u 21.8750 (12.4219) lr 2.0000e-03 eta 0:00:20
epoch [2/200] batch [45/86] time 0.516 (0.453) data 0.385 (0.321) loss_u loss_u 0.9385 (0.9078) acc_u 6.2500 (12.2222) lr 2.0000e-03 eta 0:00:18
epoch [2/200] batch [50/86] time 0.369 (0.449) data 0.239 (0.318) loss_u loss_u 0.9360 (0.9066) acc_u 6.2500 (12.4375) lr 2.0000e-03 eta 0:00:16
epoch [2/200] batch [55/86] time 0.457 (0.448) data 0.327 (0.317) loss_u loss_u 0.8472 (0.9041) acc_u 18.7500 (12.7273) lr 2.0000e-03 eta 0:00:13
epoch [2/200] batch [60/86] time 0.547 (0.445) data 0.417 (0.314) loss_u loss_u 0.8511 (0.9025) acc_u 15.6250 (12.8125) lr 2.0000e-03 eta 0:00:11
epoch [2/200] batch [65/86] time 0.500 (0.444) data 0.368 (0.312) loss_u loss_u 0.9326 (0.9032) acc_u 6.2500 (12.6923) lr 2.0000e-03 eta 0:00:09
epoch [2/200] batch [70/86] time 0.334 (0.442) data 0.203 (0.311) loss_u loss_u 0.8794 (0.9035) acc_u 21.8750 (12.6786) lr 2.0000e-03 eta 0:00:07
epoch [2/200] batch [75/86] time 0.384 (0.441) data 0.254 (0.310) loss_u loss_u 0.8882 (0.9022) acc_u 18.7500 (12.9167) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [80/86] time 0.344 (0.439) data 0.213 (0.308) loss_u loss_u 0.9058 (0.9030) acc_u 9.3750 (12.7734) lr 2.0000e-03 eta 0:00:02
epoch [2/200] batch [85/86] time 0.376 (0.440) data 0.245 (0.309) loss_u loss_u 0.9023 (0.9031) acc_u 12.5000 (12.7941) lr 2.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1877
confident_label rate tensor(0.1805, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 566
clean true:487
clean false:79
clean_rate:0.8604240282685512
noisy true:772
noisy false:1798
after delete: len(clean_dataset) 566
after delete: len(noisy_dataset) 2570
epoch [3/200] batch [5/17] time 0.647 (0.532) data 0.513 (0.400) loss_x loss_x 1.4082 (1.6766) acc_x 59.3750 (53.7500) lr 1.9999e-03 eta 0:00:06
epoch [3/200] batch [10/17] time 0.427 (0.477) data 0.295 (0.346) loss_x loss_x 2.3457 (1.8009) acc_x 37.5000 (50.6250) lr 1.9999e-03 eta 0:00:03
epoch [3/200] batch [15/17] time 0.483 (0.462) data 0.353 (0.331) loss_x loss_x 1.4785 (1.7040) acc_x 50.0000 (52.2917) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/80] time 0.500 (0.477) data 0.369 (0.346) loss_u loss_u 0.8843 (0.8847) acc_u 15.6250 (14.3750) lr 1.9999e-03 eta 0:00:35
epoch [3/200] batch [10/80] time 0.572 (0.472) data 0.442 (0.340) loss_u loss_u 0.9517 (0.9041) acc_u 3.1250 (11.2500) lr 1.9999e-03 eta 0:00:33
epoch [3/200] batch [15/80] time 0.442 (0.467) data 0.312 (0.336) loss_u loss_u 0.9004 (0.9118) acc_u 9.3750 (9.7917) lr 1.9999e-03 eta 0:00:30
epoch [3/200] batch [20/80] time 0.558 (0.462) data 0.427 (0.331) loss_u loss_u 0.9175 (0.9140) acc_u 9.3750 (9.8438) lr 1.9999e-03 eta 0:00:27
epoch [3/200] batch [25/80] time 0.461 (0.458) data 0.330 (0.327) loss_u loss_u 0.9443 (0.9168) acc_u 6.2500 (9.6250) lr 1.9999e-03 eta 0:00:25
epoch [3/200] batch [30/80] time 0.538 (0.458) data 0.406 (0.327) loss_u loss_u 0.9541 (0.9169) acc_u 3.1250 (9.6875) lr 1.9999e-03 eta 0:00:22
epoch [3/200] batch [35/80] time 0.440 (0.457) data 0.308 (0.325) loss_u loss_u 0.9658 (0.9218) acc_u 0.0000 (9.1071) lr 1.9999e-03 eta 0:00:20
epoch [3/200] batch [40/80] time 0.423 (0.451) data 0.290 (0.320) loss_u loss_u 0.9194 (0.9208) acc_u 12.5000 (9.2188) lr 1.9999e-03 eta 0:00:18
epoch [3/200] batch [45/80] time 0.329 (0.446) data 0.198 (0.315) loss_u loss_u 0.9312 (0.9223) acc_u 6.2500 (9.0278) lr 1.9999e-03 eta 0:00:15
epoch [3/200] batch [50/80] time 0.516 (0.451) data 0.385 (0.319) loss_u loss_u 0.9556 (0.9220) acc_u 3.1250 (9.0625) lr 1.9999e-03 eta 0:00:13
epoch [3/200] batch [55/80] time 0.369 (0.449) data 0.239 (0.318) loss_u loss_u 0.9546 (0.9205) acc_u 9.3750 (9.4886) lr 1.9999e-03 eta 0:00:11
epoch [3/200] batch [60/80] time 0.603 (0.448) data 0.471 (0.317) loss_u loss_u 0.9326 (0.9216) acc_u 12.5000 (9.3229) lr 1.9999e-03 eta 0:00:08
epoch [3/200] batch [65/80] time 0.521 (0.452) data 0.391 (0.320) loss_u loss_u 0.7920 (0.9196) acc_u 28.1250 (9.6154) lr 1.9999e-03 eta 0:00:06
epoch [3/200] batch [70/80] time 0.484 (0.453) data 0.353 (0.321) loss_u loss_u 0.9453 (0.9191) acc_u 3.1250 (9.8214) lr 1.9999e-03 eta 0:00:04
epoch [3/200] batch [75/80] time 0.377 (0.449) data 0.245 (0.318) loss_u loss_u 0.9482 (0.9201) acc_u 6.2500 (9.6667) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [80/80] time 0.434 (0.447) data 0.302 (0.316) loss_u loss_u 0.8657 (0.9200) acc_u 15.6250 (9.6484) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1837
confident_label rate tensor(0.1773, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 556
clean true:484
clean false:72
clean_rate:0.8705035971223022
noisy true:815
noisy false:1765
after delete: len(clean_dataset) 556
after delete: len(noisy_dataset) 2580
epoch [4/200] batch [5/17] time 0.504 (0.468) data 0.374 (0.337) loss_x loss_x 2.3086 (2.0377) acc_x 40.6250 (50.0000) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [10/17] time 0.568 (0.486) data 0.437 (0.355) loss_x loss_x 1.9707 (1.9077) acc_x 56.2500 (52.5000) lr 1.9995e-03 eta 0:00:03
epoch [4/200] batch [15/17] time 0.573 (0.489) data 0.443 (0.358) loss_x loss_x 1.7559 (1.8792) acc_x 43.7500 (49.5833) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/80] time 0.631 (0.477) data 0.500 (0.347) loss_u loss_u 0.8862 (0.9241) acc_u 12.5000 (11.2500) lr 1.9995e-03 eta 0:00:35
epoch [4/200] batch [10/80] time 0.449 (0.485) data 0.315 (0.354) loss_u loss_u 0.9434 (0.9268) acc_u 3.1250 (10.3125) lr 1.9995e-03 eta 0:00:33
epoch [4/200] batch [15/80] time 0.556 (0.481) data 0.424 (0.349) loss_u loss_u 0.9082 (0.9303) acc_u 15.6250 (9.5833) lr 1.9995e-03 eta 0:00:31
epoch [4/200] batch [20/80] time 0.424 (0.480) data 0.292 (0.349) loss_u loss_u 0.8926 (0.9295) acc_u 12.5000 (9.2188) lr 1.9995e-03 eta 0:00:28
epoch [4/200] batch [25/80] time 0.413 (0.474) data 0.283 (0.343) loss_u loss_u 0.9434 (0.9267) acc_u 6.2500 (9.0000) lr 1.9995e-03 eta 0:00:26
epoch [4/200] batch [30/80] time 0.508 (0.472) data 0.375 (0.341) loss_u loss_u 0.8926 (0.9227) acc_u 18.7500 (9.6875) lr 1.9995e-03 eta 0:00:23
epoch [4/200] batch [35/80] time 0.432 (0.467) data 0.301 (0.336) loss_u loss_u 0.8745 (0.9190) acc_u 21.8750 (10.0893) lr 1.9995e-03 eta 0:00:21
epoch [4/200] batch [40/80] time 0.490 (0.466) data 0.359 (0.335) loss_u loss_u 0.9058 (0.9179) acc_u 9.3750 (10.0781) lr 1.9995e-03 eta 0:00:18
epoch [4/200] batch [45/80] time 0.291 (0.459) data 0.160 (0.328) loss_u loss_u 0.9277 (0.9194) acc_u 6.2500 (10.0694) lr 1.9995e-03 eta 0:00:16
epoch [4/200] batch [50/80] time 0.365 (0.461) data 0.234 (0.330) loss_u loss_u 0.9438 (0.9219) acc_u 3.1250 (9.4375) lr 1.9995e-03 eta 0:00:13
epoch [4/200] batch [55/80] time 0.474 (0.460) data 0.343 (0.328) loss_u loss_u 0.9414 (0.9227) acc_u 6.2500 (9.3750) lr 1.9995e-03 eta 0:00:11
epoch [4/200] batch [60/80] time 0.362 (0.459) data 0.231 (0.328) loss_u loss_u 0.9321 (0.9223) acc_u 6.2500 (9.3229) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [65/80] time 0.362 (0.457) data 0.231 (0.326) loss_u loss_u 0.8662 (0.9213) acc_u 18.7500 (9.4712) lr 1.9995e-03 eta 0:00:06
epoch [4/200] batch [70/80] time 0.459 (0.457) data 0.327 (0.326) loss_u loss_u 0.9365 (0.9216) acc_u 3.1250 (9.3304) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [75/80] time 0.363 (0.454) data 0.232 (0.323) loss_u loss_u 0.9277 (0.9205) acc_u 12.5000 (9.3750) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [80/80] time 0.373 (0.450) data 0.240 (0.319) loss_u loss_u 0.9697 (0.9206) acc_u 3.1250 (9.4922) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1836
confident_label rate tensor(0.1719, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 539
clean true:462
clean false:77
clean_rate:0.8571428571428571
noisy true:838
noisy false:1759
after delete: len(clean_dataset) 539
after delete: len(noisy_dataset) 2597
epoch [5/200] batch [5/16] time 0.469 (0.462) data 0.339 (0.331) loss_x loss_x 1.4424 (1.7461) acc_x 53.1250 (52.5000) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [10/16] time 0.683 (0.473) data 0.552 (0.342) loss_x loss_x 1.3984 (1.6612) acc_x 62.5000 (55.3125) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [15/16] time 0.409 (0.466) data 0.278 (0.335) loss_x loss_x 1.7021 (1.7412) acc_x 56.2500 (55.6250) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/81] time 0.456 (0.461) data 0.325 (0.330) loss_u loss_u 0.9302 (0.9322) acc_u 3.1250 (5.0000) lr 1.9989e-03 eta 0:00:35
epoch [5/200] batch [10/81] time 0.387 (0.454) data 0.255 (0.323) loss_u loss_u 0.9258 (0.9125) acc_u 9.3750 (10.0000) lr 1.9989e-03 eta 0:00:32
epoch [5/200] batch [15/81] time 0.469 (0.459) data 0.337 (0.328) loss_u loss_u 0.9268 (0.9178) acc_u 9.3750 (9.3750) lr 1.9989e-03 eta 0:00:30
epoch [5/200] batch [20/81] time 0.390 (0.461) data 0.259 (0.330) loss_u loss_u 0.9448 (0.9162) acc_u 6.2500 (9.8438) lr 1.9989e-03 eta 0:00:28
epoch [5/200] batch [25/81] time 0.367 (0.459) data 0.234 (0.328) loss_u loss_u 0.9512 (0.9200) acc_u 3.1250 (9.5000) lr 1.9989e-03 eta 0:00:25
epoch [5/200] batch [30/81] time 0.428 (0.454) data 0.296 (0.323) loss_u loss_u 0.9150 (0.9187) acc_u 9.3750 (10.1042) lr 1.9989e-03 eta 0:00:23
epoch [5/200] batch [35/81] time 0.408 (0.454) data 0.276 (0.322) loss_u loss_u 0.9312 (0.9159) acc_u 9.3750 (10.6250) lr 1.9989e-03 eta 0:00:20
epoch [5/200] batch [40/81] time 0.419 (0.457) data 0.287 (0.326) loss_u loss_u 0.8687 (0.9160) acc_u 15.6250 (10.3906) lr 1.9989e-03 eta 0:00:18
epoch [5/200] batch [45/81] time 0.502 (0.454) data 0.370 (0.322) loss_u loss_u 0.9351 (0.9168) acc_u 12.5000 (10.5556) lr 1.9989e-03 eta 0:00:16
epoch [5/200] batch [50/81] time 0.492 (0.454) data 0.362 (0.322) loss_u loss_u 0.9297 (0.9157) acc_u 9.3750 (10.6875) lr 1.9989e-03 eta 0:00:14
epoch [5/200] batch [55/81] time 0.452 (0.453) data 0.322 (0.322) loss_u loss_u 0.9419 (0.9155) acc_u 3.1250 (10.5114) lr 1.9989e-03 eta 0:00:11
epoch [5/200] batch [60/81] time 0.432 (0.451) data 0.300 (0.319) loss_u loss_u 0.9429 (0.9160) acc_u 6.2500 (10.7812) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [65/81] time 0.406 (0.449) data 0.274 (0.317) loss_u loss_u 0.9058 (0.9152) acc_u 12.5000 (10.8654) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [70/81] time 0.677 (0.451) data 0.544 (0.320) loss_u loss_u 0.9658 (0.9161) acc_u 0.0000 (10.8482) lr 1.9989e-03 eta 0:00:04
epoch [5/200] batch [75/81] time 0.415 (0.449) data 0.282 (0.317) loss_u loss_u 0.9316 (0.9146) acc_u 6.2500 (11.0000) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [80/81] time 0.556 (0.449) data 0.425 (0.318) loss_u loss_u 0.8936 (0.9149) acc_u 9.3750 (10.7422) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1885
confident_label rate tensor(0.1767, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 554
clean true:453
clean false:101
clean_rate:0.8176895306859205
noisy true:798
noisy false:1784
after delete: len(clean_dataset) 554
after delete: len(noisy_dataset) 2582
epoch [6/200] batch [5/17] time 0.414 (0.476) data 0.283 (0.345) loss_x loss_x 1.8721 (1.6809) acc_x 56.2500 (56.2500) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [10/17] time 0.402 (0.473) data 0.272 (0.342) loss_x loss_x 1.6455 (1.5718) acc_x 56.2500 (59.0625) lr 1.9980e-03 eta 0:00:03
epoch [6/200] batch [15/17] time 0.418 (0.457) data 0.289 (0.326) loss_x loss_x 2.1074 (1.6611) acc_x 43.7500 (57.2917) lr 1.9980e-03 eta 0:00:00
epoch [6/200] batch [5/80] time 0.361 (0.441) data 0.230 (0.310) loss_u loss_u 0.9307 (0.9152) acc_u 9.3750 (8.7500) lr 1.9980e-03 eta 0:00:33
epoch [6/200] batch [10/80] time 0.396 (0.446) data 0.264 (0.315) loss_u loss_u 0.9014 (0.9117) acc_u 15.6250 (10.3125) lr 1.9980e-03 eta 0:00:31
epoch [6/200] batch [15/80] time 0.491 (0.446) data 0.361 (0.315) loss_u loss_u 0.8989 (0.9066) acc_u 9.3750 (10.6250) lr 1.9980e-03 eta 0:00:28
epoch [6/200] batch [20/80] time 0.430 (0.440) data 0.299 (0.309) loss_u loss_u 0.8794 (0.9005) acc_u 9.3750 (11.7188) lr 1.9980e-03 eta 0:00:26
epoch [6/200] batch [25/80] time 0.403 (0.442) data 0.271 (0.311) loss_u loss_u 0.9609 (0.9033) acc_u 6.2500 (11.3750) lr 1.9980e-03 eta 0:00:24
epoch [6/200] batch [30/80] time 0.346 (0.439) data 0.215 (0.308) loss_u loss_u 0.9448 (0.9032) acc_u 3.1250 (11.3542) lr 1.9980e-03 eta 0:00:21
epoch [6/200] batch [35/80] time 0.478 (0.442) data 0.347 (0.312) loss_u loss_u 0.8794 (0.9053) acc_u 9.3750 (11.0714) lr 1.9980e-03 eta 0:00:19
epoch [6/200] batch [40/80] time 0.426 (0.445) data 0.296 (0.314) loss_u loss_u 0.9541 (0.9080) acc_u 3.1250 (10.4688) lr 1.9980e-03 eta 0:00:17
epoch [6/200] batch [45/80] time 0.495 (0.440) data 0.365 (0.309) loss_u loss_u 0.8545 (0.9073) acc_u 18.7500 (10.7639) lr 1.9980e-03 eta 0:00:15
epoch [6/200] batch [50/80] time 0.408 (0.441) data 0.277 (0.310) loss_u loss_u 0.9390 (0.9095) acc_u 6.2500 (10.5625) lr 1.9980e-03 eta 0:00:13
epoch [6/200] batch [55/80] time 0.455 (0.441) data 0.324 (0.310) loss_u loss_u 0.9634 (0.9103) acc_u 6.2500 (10.5682) lr 1.9980e-03 eta 0:00:11
epoch [6/200] batch [60/80] time 0.461 (0.446) data 0.330 (0.315) loss_u loss_u 0.9341 (0.9099) acc_u 9.3750 (10.7292) lr 1.9980e-03 eta 0:00:08
epoch [6/200] batch [65/80] time 0.450 (0.446) data 0.317 (0.315) loss_u loss_u 0.9712 (0.9100) acc_u 3.1250 (10.8173) lr 1.9980e-03 eta 0:00:06
epoch [6/200] batch [70/80] time 0.361 (0.445) data 0.230 (0.314) loss_u loss_u 0.8931 (0.9090) acc_u 15.6250 (10.9821) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [75/80] time 0.472 (0.444) data 0.340 (0.313) loss_u loss_u 0.8667 (0.9094) acc_u 15.6250 (10.8750) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [80/80] time 0.415 (0.440) data 0.283 (0.308) loss_u loss_u 0.8462 (0.9089) acc_u 21.8750 (10.9766) lr 1.9980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1840
confident_label rate tensor(0.1767, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 554
clean true:479
clean false:75
clean_rate:0.8646209386281588
noisy true:817
noisy false:1765
after delete: len(clean_dataset) 554
after delete: len(noisy_dataset) 2582
epoch [7/200] batch [5/17] time 0.474 (0.499) data 0.344 (0.368) loss_x loss_x 1.6201 (1.4998) acc_x 68.7500 (57.5000) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [10/17] time 0.489 (0.488) data 0.359 (0.358) loss_x loss_x 1.4268 (1.5519) acc_x 65.6250 (57.5000) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [15/17] time 0.669 (0.473) data 0.538 (0.342) loss_x loss_x 2.2793 (1.6661) acc_x 56.2500 (56.6667) lr 1.9969e-03 eta 0:00:00
epoch [7/200] batch [5/80] time 0.503 (0.460) data 0.372 (0.329) loss_u loss_u 0.8931 (0.9155) acc_u 15.6250 (9.3750) lr 1.9969e-03 eta 0:00:34
epoch [7/200] batch [10/80] time 0.362 (0.446) data 0.231 (0.315) loss_u loss_u 0.8760 (0.9083) acc_u 15.6250 (11.2500) lr 1.9969e-03 eta 0:00:31
epoch [7/200] batch [15/80] time 0.644 (0.452) data 0.513 (0.321) loss_u loss_u 0.9624 (0.9194) acc_u 0.0000 (9.7917) lr 1.9969e-03 eta 0:00:29
epoch [7/200] batch [20/80] time 0.485 (0.452) data 0.354 (0.322) loss_u loss_u 0.9399 (0.9164) acc_u 6.2500 (10.1562) lr 1.9969e-03 eta 0:00:27
epoch [7/200] batch [25/80] time 0.380 (0.447) data 0.248 (0.316) loss_u loss_u 0.9395 (0.9197) acc_u 9.3750 (10.1250) lr 1.9969e-03 eta 0:00:24
epoch [7/200] batch [30/80] time 0.403 (0.448) data 0.271 (0.317) loss_u loss_u 0.8745 (0.9221) acc_u 12.5000 (9.4792) lr 1.9969e-03 eta 0:00:22
epoch [7/200] batch [35/80] time 0.400 (0.446) data 0.268 (0.315) loss_u loss_u 0.9238 (0.9192) acc_u 6.2500 (9.7321) lr 1.9969e-03 eta 0:00:20
epoch [7/200] batch [40/80] time 0.436 (0.445) data 0.305 (0.313) loss_u loss_u 0.9438 (0.9198) acc_u 9.3750 (9.6094) lr 1.9969e-03 eta 0:00:17
epoch [7/200] batch [45/80] time 0.458 (0.445) data 0.327 (0.314) loss_u loss_u 0.8408 (0.9174) acc_u 12.5000 (9.6528) lr 1.9969e-03 eta 0:00:15
epoch [7/200] batch [50/80] time 0.352 (0.443) data 0.220 (0.312) loss_u loss_u 0.9253 (0.9182) acc_u 9.3750 (9.5000) lr 1.9969e-03 eta 0:00:13
epoch [7/200] batch [55/80] time 0.491 (0.444) data 0.360 (0.312) loss_u loss_u 0.9443 (0.9170) acc_u 9.3750 (9.8295) lr 1.9969e-03 eta 0:00:11
epoch [7/200] batch [60/80] time 0.584 (0.447) data 0.451 (0.316) loss_u loss_u 0.9297 (0.9187) acc_u 9.3750 (9.6875) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [65/80] time 0.369 (0.449) data 0.237 (0.317) loss_u loss_u 0.9106 (0.9195) acc_u 9.3750 (9.6154) lr 1.9969e-03 eta 0:00:06
epoch [7/200] batch [70/80] time 0.443 (0.451) data 0.311 (0.320) loss_u loss_u 0.9126 (0.9196) acc_u 6.2500 (9.5536) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [75/80] time 0.345 (0.451) data 0.215 (0.320) loss_u loss_u 0.8989 (0.9200) acc_u 12.5000 (9.4583) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [80/80] time 0.489 (0.450) data 0.358 (0.319) loss_u loss_u 0.8696 (0.9201) acc_u 18.7500 (9.3359) lr 1.9969e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1832
confident_label rate tensor(0.1865, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 585
clean true:502
clean false:83
clean_rate:0.8581196581196581
noisy true:802
noisy false:1749
after delete: len(clean_dataset) 585
after delete: len(noisy_dataset) 2551
epoch [8/200] batch [5/18] time 0.515 (0.525) data 0.385 (0.394) loss_x loss_x 1.8184 (1.6150) acc_x 53.1250 (56.8750) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [10/18] time 0.461 (0.476) data 0.330 (0.345) loss_x loss_x 1.6855 (1.6340) acc_x 56.2500 (56.5625) lr 1.9956e-03 eta 0:00:03
epoch [8/200] batch [15/18] time 0.350 (0.467) data 0.220 (0.337) loss_x loss_x 1.3096 (1.6890) acc_x 62.5000 (56.2500) lr 1.9956e-03 eta 0:00:01
epoch [8/200] batch [5/79] time 0.451 (0.469) data 0.321 (0.338) loss_u loss_u 0.9043 (0.9144) acc_u 12.5000 (11.8750) lr 1.9956e-03 eta 0:00:34
epoch [8/200] batch [10/79] time 0.430 (0.466) data 0.299 (0.335) loss_u loss_u 0.8760 (0.9215) acc_u 18.7500 (10.0000) lr 1.9956e-03 eta 0:00:32
epoch [8/200] batch [15/79] time 0.455 (0.456) data 0.325 (0.325) loss_u loss_u 0.9551 (0.9152) acc_u 6.2500 (10.0000) lr 1.9956e-03 eta 0:00:29
epoch [8/200] batch [20/79] time 0.441 (0.447) data 0.309 (0.316) loss_u loss_u 0.8887 (0.9115) acc_u 15.6250 (10.9375) lr 1.9956e-03 eta 0:00:26
epoch [8/200] batch [25/79] time 0.378 (0.446) data 0.246 (0.315) loss_u loss_u 0.8906 (0.9112) acc_u 9.3750 (10.7500) lr 1.9956e-03 eta 0:00:24
epoch [8/200] batch [30/79] time 0.520 (0.450) data 0.389 (0.319) loss_u loss_u 0.9478 (0.9123) acc_u 3.1250 (10.6250) lr 1.9956e-03 eta 0:00:22
epoch [8/200] batch [35/79] time 0.397 (0.447) data 0.265 (0.316) loss_u loss_u 0.8687 (0.9123) acc_u 18.7500 (10.6250) lr 1.9956e-03 eta 0:00:19
epoch [8/200] batch [40/79] time 0.546 (0.456) data 0.414 (0.325) loss_u loss_u 0.8706 (0.9115) acc_u 18.7500 (10.7031) lr 1.9956e-03 eta 0:00:17
epoch [8/200] batch [45/79] time 0.467 (0.458) data 0.336 (0.327) loss_u loss_u 0.9590 (0.9140) acc_u 3.1250 (10.3472) lr 1.9956e-03 eta 0:00:15
epoch [8/200] batch [50/79] time 0.366 (0.453) data 0.234 (0.322) loss_u loss_u 0.8403 (0.9124) acc_u 15.6250 (10.3750) lr 1.9956e-03 eta 0:00:13
epoch [8/200] batch [55/79] time 0.559 (0.451) data 0.427 (0.320) loss_u loss_u 0.9360 (0.9132) acc_u 3.1250 (10.3977) lr 1.9956e-03 eta 0:00:10
epoch [8/200] batch [60/79] time 0.342 (0.447) data 0.210 (0.316) loss_u loss_u 0.8828 (0.9137) acc_u 15.6250 (10.4688) lr 1.9956e-03 eta 0:00:08
epoch [8/200] batch [65/79] time 0.409 (0.445) data 0.279 (0.314) loss_u loss_u 0.9268 (0.9121) acc_u 15.6250 (10.8173) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [70/79] time 0.505 (0.446) data 0.373 (0.315) loss_u loss_u 0.9575 (0.9128) acc_u 6.2500 (10.7143) lr 1.9956e-03 eta 0:00:04
epoch [8/200] batch [75/79] time 0.372 (0.444) data 0.240 (0.313) loss_u loss_u 0.9570 (0.9134) acc_u 3.1250 (10.4167) lr 1.9956e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1806
confident_label rate tensor(0.1926, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 604
clean true:503
clean false:101
clean_rate:0.8327814569536424
noisy true:827
noisy false:1705
after delete: len(clean_dataset) 604
after delete: len(noisy_dataset) 2532
epoch [9/200] batch [5/18] time 0.421 (0.417) data 0.290 (0.286) loss_x loss_x 1.2588 (1.6727) acc_x 56.2500 (54.3750) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [10/18] time 0.455 (0.419) data 0.325 (0.289) loss_x loss_x 1.7695 (1.6183) acc_x 46.8750 (53.4375) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [15/18] time 0.393 (0.417) data 0.263 (0.287) loss_x loss_x 1.4111 (1.6077) acc_x 53.1250 (53.5417) lr 1.9940e-03 eta 0:00:01
epoch [9/200] batch [5/79] time 0.398 (0.429) data 0.267 (0.299) loss_u loss_u 0.9004 (0.9147) acc_u 12.5000 (10.0000) lr 1.9940e-03 eta 0:00:31
epoch [9/200] batch [10/79] time 0.627 (0.430) data 0.496 (0.299) loss_u loss_u 0.9009 (0.9146) acc_u 9.3750 (10.6250) lr 1.9940e-03 eta 0:00:29
epoch [9/200] batch [15/79] time 0.366 (0.429) data 0.236 (0.299) loss_u loss_u 0.9287 (0.9092) acc_u 6.2500 (10.6250) lr 1.9940e-03 eta 0:00:27
epoch [9/200] batch [20/79] time 0.389 (0.428) data 0.258 (0.297) loss_u loss_u 0.8652 (0.9091) acc_u 18.7500 (10.6250) lr 1.9940e-03 eta 0:00:25
epoch [9/200] batch [25/79] time 0.439 (0.436) data 0.308 (0.306) loss_u loss_u 0.9331 (0.9079) acc_u 9.3750 (10.8750) lr 1.9940e-03 eta 0:00:23
epoch [9/200] batch [30/79] time 0.384 (0.439) data 0.251 (0.308) loss_u loss_u 0.9243 (0.9087) acc_u 12.5000 (10.9375) lr 1.9940e-03 eta 0:00:21
epoch [9/200] batch [35/79] time 0.323 (0.439) data 0.193 (0.308) loss_u loss_u 0.9385 (0.9106) acc_u 6.2500 (10.7143) lr 1.9940e-03 eta 0:00:19
epoch [9/200] batch [40/79] time 0.371 (0.436) data 0.241 (0.305) loss_u loss_u 0.9580 (0.9125) acc_u 9.3750 (10.7812) lr 1.9940e-03 eta 0:00:16
epoch [9/200] batch [45/79] time 0.466 (0.435) data 0.335 (0.304) loss_u loss_u 0.9375 (0.9111) acc_u 6.2500 (10.9028) lr 1.9940e-03 eta 0:00:14
epoch [9/200] batch [50/79] time 0.423 (0.440) data 0.292 (0.309) loss_u loss_u 0.9282 (0.9111) acc_u 12.5000 (11.0000) lr 1.9940e-03 eta 0:00:12
epoch [9/200] batch [55/79] time 0.451 (0.438) data 0.316 (0.307) loss_u loss_u 0.9307 (0.9124) acc_u 3.1250 (10.5682) lr 1.9940e-03 eta 0:00:10
epoch [9/200] batch [60/79] time 0.498 (0.441) data 0.367 (0.310) loss_u loss_u 0.9131 (0.9140) acc_u 12.5000 (10.4167) lr 1.9940e-03 eta 0:00:08
epoch [9/200] batch [65/79] time 0.626 (0.443) data 0.494 (0.311) loss_u loss_u 0.9287 (0.9139) acc_u 12.5000 (10.4327) lr 1.9940e-03 eta 0:00:06
epoch [9/200] batch [70/79] time 0.442 (0.442) data 0.312 (0.311) loss_u loss_u 0.9307 (0.9146) acc_u 6.2500 (10.3125) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [75/79] time 0.390 (0.442) data 0.259 (0.311) loss_u loss_u 0.9395 (0.9140) acc_u 6.2500 (10.4583) lr 1.9940e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1854
confident_label rate tensor(0.1818, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 570
clean true:477
clean false:93
clean_rate:0.8368421052631579
noisy true:805
noisy false:1761
after delete: len(clean_dataset) 570
after delete: len(noisy_dataset) 2566
epoch [10/200] batch [5/17] time 0.401 (0.449) data 0.271 (0.319) loss_x loss_x 1.7559 (1.5291) acc_x 59.3750 (60.0000) lr 1.9921e-03 eta 0:00:05
epoch [10/200] batch [10/17] time 0.512 (0.455) data 0.382 (0.324) loss_x loss_x 1.0420 (1.4283) acc_x 68.7500 (61.5625) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [15/17] time 0.715 (0.468) data 0.584 (0.338) loss_x loss_x 1.2637 (1.4665) acc_x 59.3750 (59.1667) lr 1.9921e-03 eta 0:00:00
epoch [10/200] batch [5/80] time 0.452 (0.460) data 0.320 (0.329) loss_u loss_u 0.8804 (0.8697) acc_u 18.7500 (16.2500) lr 1.9921e-03 eta 0:00:34
epoch [10/200] batch [10/80] time 0.414 (0.462) data 0.282 (0.331) loss_u loss_u 0.9575 (0.9029) acc_u 3.1250 (12.1875) lr 1.9921e-03 eta 0:00:32
epoch [10/200] batch [15/80] time 0.464 (0.456) data 0.332 (0.325) loss_u loss_u 0.9785 (0.9091) acc_u 3.1250 (11.6667) lr 1.9921e-03 eta 0:00:29
epoch [10/200] batch [20/80] time 0.389 (0.448) data 0.258 (0.317) loss_u loss_u 0.9424 (0.9151) acc_u 3.1250 (10.7812) lr 1.9921e-03 eta 0:00:26
epoch [10/200] batch [25/80] time 0.348 (0.447) data 0.217 (0.316) loss_u loss_u 0.8926 (0.9156) acc_u 15.6250 (10.8750) lr 1.9921e-03 eta 0:00:24
epoch [10/200] batch [30/80] time 0.445 (0.446) data 0.313 (0.315) loss_u loss_u 0.9482 (0.9176) acc_u 3.1250 (10.3125) lr 1.9921e-03 eta 0:00:22
epoch [10/200] batch [35/80] time 0.731 (0.451) data 0.600 (0.320) loss_u loss_u 0.9316 (0.9175) acc_u 6.2500 (10.1786) lr 1.9921e-03 eta 0:00:20
epoch [10/200] batch [40/80] time 0.411 (0.452) data 0.279 (0.321) loss_u loss_u 0.9365 (0.9157) acc_u 12.5000 (10.4688) lr 1.9921e-03 eta 0:00:18
epoch [10/200] batch [45/80] time 0.366 (0.449) data 0.235 (0.318) loss_u loss_u 0.9204 (0.9152) acc_u 9.3750 (10.4167) lr 1.9921e-03 eta 0:00:15
epoch [10/200] batch [50/80] time 0.484 (0.447) data 0.354 (0.316) loss_u loss_u 0.8516 (0.9148) acc_u 18.7500 (10.4375) lr 1.9921e-03 eta 0:00:13
epoch [10/200] batch [55/80] time 0.409 (0.444) data 0.277 (0.313) loss_u loss_u 0.9141 (0.9139) acc_u 12.5000 (10.9659) lr 1.9921e-03 eta 0:00:11
epoch [10/200] batch [60/80] time 0.426 (0.443) data 0.294 (0.312) loss_u loss_u 0.9399 (0.9140) acc_u 9.3750 (11.1979) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [65/80] time 0.413 (0.439) data 0.282 (0.308) loss_u loss_u 0.9385 (0.9159) acc_u 9.3750 (10.9615) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [70/80] time 0.431 (0.439) data 0.301 (0.308) loss_u loss_u 0.9224 (0.9163) acc_u 6.2500 (10.6696) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [75/80] time 0.390 (0.437) data 0.258 (0.306) loss_u loss_u 0.9058 (0.9149) acc_u 9.3750 (10.9583) lr 1.9921e-03 eta 0:00:02
epoch [10/200] batch [80/80] time 0.397 (0.438) data 0.265 (0.307) loss_u loss_u 0.9028 (0.9135) acc_u 6.2500 (11.1328) lr 1.9921e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1778
confident_label rate tensor(0.1958, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 614
clean true:511
clean false:103
clean_rate:0.8322475570032574
noisy true:847
noisy false:1675
after delete: len(clean_dataset) 614
after delete: len(noisy_dataset) 2522
epoch [11/200] batch [5/19] time 0.356 (0.431) data 0.226 (0.301) loss_x loss_x 1.4902 (1.2426) acc_x 59.3750 (66.2500) lr 1.9900e-03 eta 0:00:06
epoch [11/200] batch [10/19] time 0.502 (0.454) data 0.372 (0.324) loss_x loss_x 1.8047 (1.4910) acc_x 50.0000 (60.0000) lr 1.9900e-03 eta 0:00:04
epoch [11/200] batch [15/19] time 0.463 (0.452) data 0.332 (0.322) loss_x loss_x 1.5801 (1.5714) acc_x 53.1250 (57.7083) lr 1.9900e-03 eta 0:00:01
epoch [11/200] batch [5/78] time 0.473 (0.450) data 0.341 (0.319) loss_u loss_u 0.9116 (0.9299) acc_u 12.5000 (10.0000) lr 1.9900e-03 eta 0:00:32
epoch [11/200] batch [10/78] time 0.426 (0.448) data 0.295 (0.318) loss_u loss_u 0.9663 (0.9306) acc_u 6.2500 (9.0625) lr 1.9900e-03 eta 0:00:30
epoch [11/200] batch [15/78] time 0.567 (0.456) data 0.436 (0.326) loss_u loss_u 0.9058 (0.9278) acc_u 9.3750 (8.9583) lr 1.9900e-03 eta 0:00:28
epoch [11/200] batch [20/78] time 0.455 (0.456) data 0.323 (0.325) loss_u loss_u 0.9365 (0.9302) acc_u 6.2500 (8.1250) lr 1.9900e-03 eta 0:00:26
epoch [11/200] batch [25/78] time 0.434 (0.453) data 0.303 (0.322) loss_u loss_u 0.8184 (0.9218) acc_u 28.1250 (9.6250) lr 1.9900e-03 eta 0:00:23
epoch [11/200] batch [30/78] time 0.325 (0.449) data 0.194 (0.318) loss_u loss_u 0.8789 (0.9188) acc_u 12.5000 (9.8958) lr 1.9900e-03 eta 0:00:21
epoch [11/200] batch [35/78] time 0.397 (0.441) data 0.265 (0.310) loss_u loss_u 0.9595 (0.9180) acc_u 3.1250 (9.7321) lr 1.9900e-03 eta 0:00:18
epoch [11/200] batch [40/78] time 0.424 (0.441) data 0.293 (0.310) loss_u loss_u 0.9072 (0.9174) acc_u 9.3750 (9.7656) lr 1.9900e-03 eta 0:00:16
epoch [11/200] batch [45/78] time 0.448 (0.446) data 0.317 (0.314) loss_u loss_u 0.9385 (0.9189) acc_u 6.2500 (9.5833) lr 1.9900e-03 eta 0:00:14
epoch [11/200] batch [50/78] time 0.507 (0.445) data 0.375 (0.313) loss_u loss_u 0.9209 (0.9187) acc_u 9.3750 (9.5625) lr 1.9900e-03 eta 0:00:12
epoch [11/200] batch [55/78] time 0.392 (0.447) data 0.261 (0.316) loss_u loss_u 0.9624 (0.9172) acc_u 3.1250 (9.6591) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [60/78] time 0.593 (0.447) data 0.463 (0.316) loss_u loss_u 0.9561 (0.9165) acc_u 3.1250 (9.7917) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [65/78] time 0.350 (0.445) data 0.220 (0.314) loss_u loss_u 0.9468 (0.9165) acc_u 6.2500 (9.9038) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [70/78] time 0.409 (0.446) data 0.277 (0.315) loss_u loss_u 0.9043 (0.9159) acc_u 9.3750 (9.9554) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [75/78] time 0.350 (0.446) data 0.219 (0.315) loss_u loss_u 0.9219 (0.9156) acc_u 9.3750 (10.0417) lr 1.9900e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1845
confident_label rate tensor(0.1827, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 573
clean true:487
clean false:86
clean_rate:0.849912739965096
noisy true:804
noisy false:1759
after delete: len(clean_dataset) 573
after delete: len(noisy_dataset) 2563
epoch [12/200] batch [5/17] time 0.423 (0.416) data 0.292 (0.285) loss_x loss_x 1.7881 (1.4697) acc_x 59.3750 (62.5000) lr 1.9877e-03 eta 0:00:04
epoch [12/200] batch [10/17] time 0.343 (0.421) data 0.212 (0.290) loss_x loss_x 2.1152 (1.5557) acc_x 43.7500 (61.2500) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [15/17] time 0.354 (0.448) data 0.224 (0.318) loss_x loss_x 1.5605 (1.5744) acc_x 59.3750 (60.0000) lr 1.9877e-03 eta 0:00:00
epoch [12/200] batch [5/80] time 0.477 (0.441) data 0.346 (0.311) loss_u loss_u 0.9199 (0.9453) acc_u 12.5000 (7.5000) lr 1.9877e-03 eta 0:00:33
epoch [12/200] batch [10/80] time 0.420 (0.433) data 0.289 (0.302) loss_u loss_u 0.9131 (0.9307) acc_u 6.2500 (9.3750) lr 1.9877e-03 eta 0:00:30
epoch [12/200] batch [15/80] time 0.401 (0.426) data 0.270 (0.296) loss_u loss_u 0.9150 (0.9255) acc_u 15.6250 (9.7917) lr 1.9877e-03 eta 0:00:27
epoch [12/200] batch [20/80] time 0.469 (0.431) data 0.338 (0.300) loss_u loss_u 0.8604 (0.9196) acc_u 18.7500 (11.2500) lr 1.9877e-03 eta 0:00:25
epoch [12/200] batch [25/80] time 0.488 (0.437) data 0.357 (0.307) loss_u loss_u 0.8574 (0.9159) acc_u 15.6250 (11.1250) lr 1.9877e-03 eta 0:00:24
epoch [12/200] batch [30/80] time 0.613 (0.445) data 0.481 (0.314) loss_u loss_u 0.9419 (0.9173) acc_u 9.3750 (10.8333) lr 1.9877e-03 eta 0:00:22
epoch [12/200] batch [35/80] time 0.570 (0.451) data 0.438 (0.320) loss_u loss_u 0.9385 (0.9220) acc_u 3.1250 (10.0000) lr 1.9877e-03 eta 0:00:20
epoch [12/200] batch [40/80] time 0.402 (0.450) data 0.271 (0.319) loss_u loss_u 0.9321 (0.9228) acc_u 6.2500 (9.6875) lr 1.9877e-03 eta 0:00:17
epoch [12/200] batch [45/80] time 0.376 (0.447) data 0.245 (0.317) loss_u loss_u 0.9312 (0.9202) acc_u 3.1250 (9.8611) lr 1.9877e-03 eta 0:00:15
epoch [12/200] batch [50/80] time 0.306 (0.447) data 0.176 (0.316) loss_u loss_u 0.9082 (0.9192) acc_u 9.3750 (9.9375) lr 1.9877e-03 eta 0:00:13
epoch [12/200] batch [55/80] time 0.417 (0.444) data 0.285 (0.313) loss_u loss_u 0.9268 (0.9184) acc_u 6.2500 (9.8295) lr 1.9877e-03 eta 0:00:11
epoch [12/200] batch [60/80] time 0.387 (0.443) data 0.255 (0.311) loss_u loss_u 0.8975 (0.9184) acc_u 9.3750 (9.7917) lr 1.9877e-03 eta 0:00:08
epoch [12/200] batch [65/80] time 0.317 (0.441) data 0.185 (0.310) loss_u loss_u 0.9331 (0.9182) acc_u 6.2500 (9.6635) lr 1.9877e-03 eta 0:00:06
epoch [12/200] batch [70/80] time 0.480 (0.442) data 0.348 (0.311) loss_u loss_u 0.9150 (0.9169) acc_u 6.2500 (9.8661) lr 1.9877e-03 eta 0:00:04
epoch [12/200] batch [75/80] time 0.532 (0.443) data 0.399 (0.312) loss_u loss_u 0.9487 (0.9185) acc_u 6.2500 (9.5417) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [80/80] time 0.426 (0.442) data 0.290 (0.311) loss_u loss_u 0.9194 (0.9180) acc_u 6.2500 (9.6094) lr 1.9877e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1823
confident_label rate tensor(0.1897, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 595
clean true:504
clean false:91
clean_rate:0.8470588235294118
noisy true:809
noisy false:1732
after delete: len(clean_dataset) 595
after delete: len(noisy_dataset) 2541
epoch [13/200] batch [5/18] time 0.508 (0.483) data 0.377 (0.352) loss_x loss_x 1.0107 (1.2909) acc_x 65.6250 (65.6250) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [10/18] time 0.458 (0.477) data 0.328 (0.346) loss_x loss_x 1.6494 (1.4661) acc_x 59.3750 (60.9375) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [15/18] time 0.636 (0.492) data 0.505 (0.361) loss_x loss_x 1.4990 (1.5076) acc_x 53.1250 (59.3750) lr 1.9851e-03 eta 0:00:01
epoch [13/200] batch [5/79] time 0.467 (0.484) data 0.335 (0.353) loss_u loss_u 0.8950 (0.9077) acc_u 9.3750 (9.3750) lr 1.9851e-03 eta 0:00:35
epoch [13/200] batch [10/79] time 0.363 (0.475) data 0.233 (0.345) loss_u loss_u 0.9365 (0.9091) acc_u 9.3750 (10.3125) lr 1.9851e-03 eta 0:00:32
epoch [13/200] batch [15/79] time 0.453 (0.467) data 0.322 (0.337) loss_u loss_u 0.9331 (0.9185) acc_u 9.3750 (9.7917) lr 1.9851e-03 eta 0:00:29
epoch [13/200] batch [20/79] time 0.467 (0.462) data 0.336 (0.331) loss_u loss_u 0.9502 (0.9138) acc_u 6.2500 (10.6250) lr 1.9851e-03 eta 0:00:27
epoch [13/200] batch [25/79] time 0.376 (0.462) data 0.245 (0.331) loss_u loss_u 0.9312 (0.9154) acc_u 6.2500 (10.5000) lr 1.9851e-03 eta 0:00:24
epoch [13/200] batch [30/79] time 0.390 (0.463) data 0.259 (0.332) loss_u loss_u 0.9526 (0.9195) acc_u 6.2500 (9.5833) lr 1.9851e-03 eta 0:00:22
epoch [13/200] batch [35/79] time 0.449 (0.458) data 0.317 (0.327) loss_u loss_u 0.9116 (0.9165) acc_u 12.5000 (10.1786) lr 1.9851e-03 eta 0:00:20
epoch [13/200] batch [40/79] time 0.651 (0.460) data 0.519 (0.329) loss_u loss_u 0.8794 (0.9174) acc_u 12.5000 (10.0000) lr 1.9851e-03 eta 0:00:17
epoch [13/200] batch [45/79] time 0.354 (0.456) data 0.222 (0.325) loss_u loss_u 0.8613 (0.9174) acc_u 18.7500 (9.9306) lr 1.9851e-03 eta 0:00:15
epoch [13/200] batch [50/79] time 0.377 (0.452) data 0.246 (0.321) loss_u loss_u 0.9453 (0.9187) acc_u 6.2500 (9.6875) lr 1.9851e-03 eta 0:00:13
epoch [13/200] batch [55/79] time 0.356 (0.448) data 0.225 (0.317) loss_u loss_u 0.9443 (0.9176) acc_u 3.1250 (9.7727) lr 1.9851e-03 eta 0:00:10
epoch [13/200] batch [60/79] time 0.305 (0.444) data 0.174 (0.313) loss_u loss_u 0.9556 (0.9168) acc_u 3.1250 (9.6875) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [65/79] time 0.375 (0.444) data 0.244 (0.313) loss_u loss_u 0.9038 (0.9161) acc_u 6.2500 (9.7115) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [70/79] time 0.483 (0.443) data 0.351 (0.312) loss_u loss_u 0.9028 (0.9158) acc_u 12.5000 (9.8214) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [75/79] time 0.451 (0.443) data 0.319 (0.311) loss_u loss_u 0.9287 (0.9163) acc_u 6.2500 (9.7917) lr 1.9851e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1836
confident_label rate tensor(0.1923, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 603
clean true:506
clean false:97
clean_rate:0.8391376451077943
noisy true:794
noisy false:1739
after delete: len(clean_dataset) 603
after delete: len(noisy_dataset) 2533
epoch [14/200] batch [5/18] time 0.484 (0.435) data 0.354 (0.304) loss_x loss_x 1.2656 (1.4857) acc_x 59.3750 (56.8750) lr 1.9823e-03 eta 0:00:05
epoch [14/200] batch [10/18] time 0.373 (0.441) data 0.241 (0.310) loss_x loss_x 1.5703 (1.6440) acc_x 59.3750 (54.6875) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [15/18] time 0.336 (0.449) data 0.205 (0.318) loss_x loss_x 1.7041 (1.6447) acc_x 50.0000 (54.7917) lr 1.9823e-03 eta 0:00:01
epoch [14/200] batch [5/79] time 0.376 (0.479) data 0.245 (0.348) loss_u loss_u 0.8960 (0.9116) acc_u 12.5000 (8.7500) lr 1.9823e-03 eta 0:00:35
epoch [14/200] batch [10/79] time 0.451 (0.470) data 0.320 (0.339) loss_u loss_u 0.9561 (0.9113) acc_u 0.0000 (8.4375) lr 1.9823e-03 eta 0:00:32
epoch [14/200] batch [15/79] time 0.337 (0.460) data 0.205 (0.330) loss_u loss_u 0.8628 (0.9116) acc_u 15.6250 (9.5833) lr 1.9823e-03 eta 0:00:29
epoch [14/200] batch [20/79] time 0.388 (0.468) data 0.255 (0.337) loss_u loss_u 0.9111 (0.9128) acc_u 9.3750 (10.1562) lr 1.9823e-03 eta 0:00:27
epoch [14/200] batch [25/79] time 0.480 (0.467) data 0.349 (0.336) loss_u loss_u 0.9624 (0.9120) acc_u 3.1250 (10.0000) lr 1.9823e-03 eta 0:00:25
epoch [14/200] batch [30/79] time 0.346 (0.466) data 0.215 (0.335) loss_u loss_u 0.9175 (0.9154) acc_u 6.2500 (9.4792) lr 1.9823e-03 eta 0:00:22
epoch [14/200] batch [35/79] time 0.429 (0.464) data 0.298 (0.333) loss_u loss_u 0.9219 (0.9149) acc_u 12.5000 (9.7321) lr 1.9823e-03 eta 0:00:20
epoch [14/200] batch [40/79] time 0.488 (0.463) data 0.356 (0.331) loss_u loss_u 0.7998 (0.9136) acc_u 25.0000 (9.8438) lr 1.9823e-03 eta 0:00:18
epoch [14/200] batch [45/79] time 0.447 (0.464) data 0.315 (0.332) loss_u loss_u 0.9272 (0.9146) acc_u 12.5000 (9.6528) lr 1.9823e-03 eta 0:00:15
epoch [14/200] batch [50/79] time 0.418 (0.460) data 0.286 (0.329) loss_u loss_u 0.8096 (0.9122) acc_u 25.0000 (9.7500) lr 1.9823e-03 eta 0:00:13
epoch [14/200] batch [55/79] time 0.425 (0.460) data 0.294 (0.329) loss_u loss_u 0.9565 (0.9136) acc_u 9.3750 (9.7159) lr 1.9823e-03 eta 0:00:11
epoch [14/200] batch [60/79] time 0.454 (0.457) data 0.323 (0.326) loss_u loss_u 0.9321 (0.9161) acc_u 6.2500 (9.4271) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [65/79] time 0.460 (0.458) data 0.329 (0.327) loss_u loss_u 0.9595 (0.9176) acc_u 6.2500 (9.3269) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [70/79] time 0.363 (0.457) data 0.232 (0.326) loss_u loss_u 0.9067 (0.9141) acc_u 6.2500 (9.7768) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [75/79] time 0.396 (0.459) data 0.264 (0.327) loss_u loss_u 0.8896 (0.9128) acc_u 9.3750 (10.0833) lr 1.9823e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1806
confident_label rate tensor(0.2022, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 634
clean true:535
clean false:99
clean_rate:0.8438485804416404
noisy true:795
noisy false:1707
after delete: len(clean_dataset) 634
after delete: len(noisy_dataset) 2502
epoch [15/200] batch [5/19] time 0.420 (0.489) data 0.289 (0.358) loss_x loss_x 1.3252 (1.4656) acc_x 62.5000 (60.0000) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [10/19] time 0.546 (0.483) data 0.416 (0.352) loss_x loss_x 1.4316 (1.4679) acc_x 56.2500 (59.0625) lr 1.9792e-03 eta 0:00:04
epoch [15/200] batch [15/19] time 0.400 (0.480) data 0.270 (0.349) loss_x loss_x 1.2744 (1.4409) acc_x 62.5000 (59.7917) lr 1.9792e-03 eta 0:00:01
epoch [15/200] batch [5/78] time 0.365 (0.469) data 0.234 (0.339) loss_u loss_u 0.9175 (0.9121) acc_u 9.3750 (6.8750) lr 1.9792e-03 eta 0:00:34
epoch [15/200] batch [10/78] time 0.359 (0.464) data 0.229 (0.333) loss_u loss_u 0.9556 (0.9156) acc_u 3.1250 (6.8750) lr 1.9792e-03 eta 0:00:31
epoch [15/200] batch [15/78] time 0.430 (0.462) data 0.299 (0.331) loss_u loss_u 0.9600 (0.9202) acc_u 6.2500 (6.8750) lr 1.9792e-03 eta 0:00:29
epoch [15/200] batch [20/78] time 0.489 (0.467) data 0.358 (0.336) loss_u loss_u 0.9644 (0.9240) acc_u 9.3750 (7.1875) lr 1.9792e-03 eta 0:00:27
epoch [15/200] batch [25/78] time 0.451 (0.470) data 0.320 (0.339) loss_u loss_u 0.8970 (0.9210) acc_u 15.6250 (8.2500) lr 1.9792e-03 eta 0:00:24
epoch [15/200] batch [30/78] time 0.408 (0.472) data 0.277 (0.341) loss_u loss_u 0.9302 (0.9216) acc_u 6.2500 (8.6458) lr 1.9792e-03 eta 0:00:22
epoch [15/200] batch [35/78] time 0.516 (0.474) data 0.385 (0.343) loss_u loss_u 0.8799 (0.9184) acc_u 15.6250 (9.1071) lr 1.9792e-03 eta 0:00:20
epoch [15/200] batch [40/78] time 0.473 (0.476) data 0.342 (0.345) loss_u loss_u 0.8867 (0.9179) acc_u 12.5000 (9.1406) lr 1.9792e-03 eta 0:00:18
epoch [15/200] batch [45/78] time 0.377 (0.470) data 0.247 (0.339) loss_u loss_u 0.9600 (0.9180) acc_u 3.1250 (9.3056) lr 1.9792e-03 eta 0:00:15
epoch [15/200] batch [50/78] time 0.523 (0.470) data 0.390 (0.339) loss_u loss_u 0.9199 (0.9181) acc_u 12.5000 (9.3125) lr 1.9792e-03 eta 0:00:13
epoch [15/200] batch [55/78] time 0.421 (0.469) data 0.290 (0.338) loss_u loss_u 0.9565 (0.9189) acc_u 6.2500 (9.3182) lr 1.9792e-03 eta 0:00:10
epoch [15/200] batch [60/78] time 0.372 (0.470) data 0.241 (0.339) loss_u loss_u 0.9219 (0.9211) acc_u 9.3750 (8.8542) lr 1.9792e-03 eta 0:00:08
epoch [15/200] batch [65/78] time 0.412 (0.468) data 0.280 (0.337) loss_u loss_u 0.9219 (0.9205) acc_u 12.5000 (8.9904) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [70/78] time 0.419 (0.465) data 0.288 (0.334) loss_u loss_u 0.9438 (0.9207) acc_u 3.1250 (8.7946) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [75/78] time 0.432 (0.465) data 0.301 (0.333) loss_u loss_u 0.9248 (0.9202) acc_u 12.5000 (8.9583) lr 1.9792e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1814
confident_label rate tensor(0.1869, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 586
clean true:498
clean false:88
clean_rate:0.8498293515358362
noisy true:824
noisy false:1726
after delete: len(clean_dataset) 586
after delete: len(noisy_dataset) 2550
epoch [16/200] batch [5/18] time 0.556 (0.532) data 0.425 (0.399) loss_x loss_x 1.6152 (1.4840) acc_x 50.0000 (59.3750) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [10/18] time 0.558 (0.505) data 0.428 (0.373) loss_x loss_x 1.1045 (1.4128) acc_x 71.8750 (64.0625) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [15/18] time 0.487 (0.493) data 0.357 (0.362) loss_x loss_x 1.4795 (1.3758) acc_x 62.5000 (64.3750) lr 1.9759e-03 eta 0:00:01
epoch [16/200] batch [5/79] time 0.406 (0.476) data 0.274 (0.345) loss_u loss_u 0.9492 (0.9095) acc_u 0.0000 (11.2500) lr 1.9759e-03 eta 0:00:35
epoch [16/200] batch [10/79] time 0.399 (0.469) data 0.269 (0.338) loss_u loss_u 0.9473 (0.9127) acc_u 3.1250 (10.3125) lr 1.9759e-03 eta 0:00:32
epoch [16/200] batch [15/79] time 0.492 (0.465) data 0.361 (0.334) loss_u loss_u 0.9224 (0.9149) acc_u 6.2500 (10.0000) lr 1.9759e-03 eta 0:00:29
epoch [16/200] batch [20/79] time 0.473 (0.460) data 0.341 (0.328) loss_u loss_u 0.9590 (0.9184) acc_u 3.1250 (9.5312) lr 1.9759e-03 eta 0:00:27
epoch [16/200] batch [25/79] time 0.489 (0.459) data 0.358 (0.328) loss_u loss_u 0.8940 (0.9147) acc_u 12.5000 (9.8750) lr 1.9759e-03 eta 0:00:24
epoch [16/200] batch [30/79] time 0.565 (0.468) data 0.434 (0.337) loss_u loss_u 0.9409 (0.9140) acc_u 0.0000 (9.5833) lr 1.9759e-03 eta 0:00:22
epoch [16/200] batch [35/79] time 0.459 (0.468) data 0.327 (0.337) loss_u loss_u 0.9385 (0.9163) acc_u 6.2500 (9.2857) lr 1.9759e-03 eta 0:00:20
epoch [16/200] batch [40/79] time 0.485 (0.464) data 0.354 (0.333) loss_u loss_u 0.9160 (0.9196) acc_u 12.5000 (9.0625) lr 1.9759e-03 eta 0:00:18
epoch [16/200] batch [45/79] time 0.411 (0.464) data 0.280 (0.332) loss_u loss_u 0.8784 (0.9197) acc_u 18.7500 (9.0972) lr 1.9759e-03 eta 0:00:15
epoch [16/200] batch [50/79] time 0.539 (0.461) data 0.408 (0.329) loss_u loss_u 0.9121 (0.9185) acc_u 9.3750 (9.3125) lr 1.9759e-03 eta 0:00:13
epoch [16/200] batch [55/79] time 0.359 (0.457) data 0.228 (0.326) loss_u loss_u 0.9170 (0.9185) acc_u 6.2500 (9.3750) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [60/79] time 0.524 (0.460) data 0.392 (0.329) loss_u loss_u 0.9492 (0.9171) acc_u 6.2500 (9.6354) lr 1.9759e-03 eta 0:00:08
epoch [16/200] batch [65/79] time 0.383 (0.459) data 0.252 (0.328) loss_u loss_u 0.9512 (0.9163) acc_u 3.1250 (9.8558) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [70/79] time 0.426 (0.460) data 0.295 (0.329) loss_u loss_u 0.9019 (0.9155) acc_u 15.6250 (10.2232) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [75/79] time 0.429 (0.460) data 0.298 (0.329) loss_u loss_u 0.8882 (0.9147) acc_u 15.6250 (10.4583) lr 1.9759e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1834
confident_label rate tensor(0.1894, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 594
clean true:500
clean false:94
clean_rate:0.8417508417508418
noisy true:802
noisy false:1740
after delete: len(clean_dataset) 594
after delete: len(noisy_dataset) 2542
epoch [17/200] batch [5/18] time 0.407 (0.477) data 0.275 (0.346) loss_x loss_x 1.2393 (1.4129) acc_x 62.5000 (62.5000) lr 1.9724e-03 eta 0:00:06
epoch [17/200] batch [10/18] time 0.362 (0.471) data 0.231 (0.340) loss_x loss_x 1.5342 (1.5876) acc_x 65.6250 (59.6875) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [15/18] time 0.404 (0.462) data 0.273 (0.331) loss_x loss_x 1.7773 (1.5971) acc_x 59.3750 (59.1667) lr 1.9724e-03 eta 0:00:01
epoch [17/200] batch [5/79] time 0.398 (0.457) data 0.267 (0.326) loss_u loss_u 0.8940 (0.9010) acc_u 15.6250 (13.1250) lr 1.9724e-03 eta 0:00:33
epoch [17/200] batch [10/79] time 0.441 (0.449) data 0.309 (0.318) loss_u loss_u 0.9751 (0.9107) acc_u 3.1250 (12.1875) lr 1.9724e-03 eta 0:00:31
epoch [17/200] batch [15/79] time 0.405 (0.456) data 0.274 (0.324) loss_u loss_u 0.9395 (0.9151) acc_u 6.2500 (11.2500) lr 1.9724e-03 eta 0:00:29
epoch [17/200] batch [20/79] time 0.383 (0.454) data 0.252 (0.323) loss_u loss_u 0.9399 (0.9145) acc_u 6.2500 (11.4062) lr 1.9724e-03 eta 0:00:26
epoch [17/200] batch [25/79] time 0.473 (0.453) data 0.341 (0.322) loss_u loss_u 0.9175 (0.9144) acc_u 15.6250 (11.3750) lr 1.9724e-03 eta 0:00:24
epoch [17/200] batch [30/79] time 0.524 (0.453) data 0.392 (0.321) loss_u loss_u 0.8887 (0.9138) acc_u 12.5000 (11.2500) lr 1.9724e-03 eta 0:00:22
epoch [17/200] batch [35/79] time 0.389 (0.456) data 0.259 (0.325) loss_u loss_u 0.8550 (0.9107) acc_u 18.7500 (11.6964) lr 1.9724e-03 eta 0:00:20
epoch [17/200] batch [40/79] time 0.428 (0.452) data 0.297 (0.321) loss_u loss_u 0.8882 (0.9104) acc_u 18.7500 (11.9531) lr 1.9724e-03 eta 0:00:17
epoch [17/200] batch [45/79] time 0.488 (0.455) data 0.358 (0.324) loss_u loss_u 0.9302 (0.9099) acc_u 6.2500 (12.0139) lr 1.9724e-03 eta 0:00:15
epoch [17/200] batch [50/79] time 0.657 (0.456) data 0.525 (0.324) loss_u loss_u 0.8960 (0.9077) acc_u 12.5000 (12.2500) lr 1.9724e-03 eta 0:00:13
epoch [17/200] batch [55/79] time 0.398 (0.457) data 0.266 (0.326) loss_u loss_u 0.8105 (0.9061) acc_u 21.8750 (12.2159) lr 1.9724e-03 eta 0:00:10
epoch [17/200] batch [60/79] time 0.455 (0.459) data 0.323 (0.328) loss_u loss_u 0.9263 (0.9065) acc_u 3.1250 (12.1354) lr 1.9724e-03 eta 0:00:08
epoch [17/200] batch [65/79] time 0.523 (0.459) data 0.391 (0.327) loss_u loss_u 0.9443 (0.9087) acc_u 0.0000 (11.7788) lr 1.9724e-03 eta 0:00:06
epoch [17/200] batch [70/79] time 0.466 (0.459) data 0.334 (0.328) loss_u loss_u 0.9326 (0.9088) acc_u 9.3750 (11.6964) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [75/79] time 0.569 (0.461) data 0.438 (0.329) loss_u loss_u 0.9385 (0.9079) acc_u 6.2500 (11.7083) lr 1.9724e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1823
confident_label rate tensor(0.1967, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 617
clean true:516
clean false:101
clean_rate:0.8363047001620746
noisy true:797
noisy false:1722
after delete: len(clean_dataset) 617
after delete: len(noisy_dataset) 2519
epoch [18/200] batch [5/19] time 0.526 (0.542) data 0.394 (0.411) loss_x loss_x 1.8037 (1.5383) acc_x 50.0000 (58.1250) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [10/19] time 0.391 (0.500) data 0.261 (0.369) loss_x loss_x 1.5840 (1.5330) acc_x 71.8750 (59.6875) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [15/19] time 0.560 (0.510) data 0.431 (0.379) loss_x loss_x 1.2334 (1.5762) acc_x 62.5000 (56.4583) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [5/78] time 0.403 (0.490) data 0.271 (0.359) loss_u loss_u 0.8853 (0.9038) acc_u 12.5000 (11.8750) lr 1.9686e-03 eta 0:00:35
epoch [18/200] batch [10/78] time 0.458 (0.487) data 0.327 (0.355) loss_u loss_u 0.9175 (0.9125) acc_u 9.3750 (10.0000) lr 1.9686e-03 eta 0:00:33
epoch [18/200] batch [15/78] time 0.808 (0.494) data 0.677 (0.362) loss_u loss_u 0.9395 (0.9179) acc_u 6.2500 (9.5833) lr 1.9686e-03 eta 0:00:31
epoch [18/200] batch [20/78] time 0.435 (0.485) data 0.304 (0.354) loss_u loss_u 0.9307 (0.9176) acc_u 9.3750 (10.0000) lr 1.9686e-03 eta 0:00:28
epoch [18/200] batch [25/78] time 0.424 (0.483) data 0.292 (0.351) loss_u loss_u 0.8550 (0.9189) acc_u 18.7500 (9.7500) lr 1.9686e-03 eta 0:00:25
epoch [18/200] batch [30/78] time 0.449 (0.476) data 0.317 (0.345) loss_u loss_u 0.9014 (0.9139) acc_u 15.6250 (10.6250) lr 1.9686e-03 eta 0:00:22
epoch [18/200] batch [35/78] time 0.394 (0.469) data 0.264 (0.338) loss_u loss_u 0.8613 (0.9127) acc_u 18.7500 (10.8929) lr 1.9686e-03 eta 0:00:20
epoch [18/200] batch [40/78] time 0.488 (0.470) data 0.357 (0.339) loss_u loss_u 0.9521 (0.9124) acc_u 3.1250 (10.9375) lr 1.9686e-03 eta 0:00:17
epoch [18/200] batch [45/78] time 0.356 (0.466) data 0.225 (0.335) loss_u loss_u 0.9185 (0.9137) acc_u 12.5000 (10.8333) lr 1.9686e-03 eta 0:00:15
epoch [18/200] batch [50/78] time 0.420 (0.462) data 0.288 (0.331) loss_u loss_u 0.9199 (0.9125) acc_u 9.3750 (11.1250) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [55/78] time 0.492 (0.463) data 0.362 (0.332) loss_u loss_u 0.9263 (0.9121) acc_u 6.2500 (11.0227) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [60/78] time 0.640 (0.467) data 0.510 (0.336) loss_u loss_u 0.9263 (0.9114) acc_u 3.1250 (11.1979) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [65/78] time 0.413 (0.470) data 0.281 (0.338) loss_u loss_u 0.9150 (0.9116) acc_u 9.3750 (11.0577) lr 1.9686e-03 eta 0:00:06
epoch [18/200] batch [70/78] time 0.501 (0.471) data 0.368 (0.340) loss_u loss_u 0.8867 (0.9122) acc_u 15.6250 (11.0714) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [75/78] time 0.398 (0.469) data 0.266 (0.338) loss_u loss_u 0.9419 (0.9126) acc_u 6.2500 (10.8750) lr 1.9686e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1791
confident_label rate tensor(0.2012, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 631
clean true:539
clean false:92
clean_rate:0.8541996830427893
noisy true:806
noisy false:1699
after delete: len(clean_dataset) 631
after delete: len(noisy_dataset) 2505
epoch [19/200] batch [5/19] time 0.541 (0.476) data 0.410 (0.345) loss_x loss_x 1.4033 (1.3525) acc_x 65.6250 (64.3750) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [10/19] time 0.362 (0.483) data 0.231 (0.352) loss_x loss_x 1.4043 (1.4953) acc_x 59.3750 (60.3125) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [15/19] time 0.401 (0.480) data 0.270 (0.349) loss_x loss_x 1.2275 (1.5630) acc_x 65.6250 (58.3333) lr 1.9646e-03 eta 0:00:01
epoch [19/200] batch [5/78] time 0.391 (0.463) data 0.259 (0.332) loss_u loss_u 0.9580 (0.9146) acc_u 3.1250 (10.6250) lr 1.9646e-03 eta 0:00:33
epoch [19/200] batch [10/78] time 0.427 (0.469) data 0.296 (0.338) loss_u loss_u 0.9380 (0.9153) acc_u 6.2500 (10.3125) lr 1.9646e-03 eta 0:00:31
epoch [19/200] batch [15/78] time 0.572 (0.467) data 0.441 (0.336) loss_u loss_u 0.8604 (0.9023) acc_u 18.7500 (11.6667) lr 1.9646e-03 eta 0:00:29
epoch [19/200] batch [20/78] time 0.694 (0.467) data 0.560 (0.336) loss_u loss_u 0.9253 (0.9008) acc_u 15.6250 (12.1875) lr 1.9646e-03 eta 0:00:27
epoch [19/200] batch [25/78] time 0.511 (0.470) data 0.380 (0.339) loss_u loss_u 0.9209 (0.8998) acc_u 12.5000 (12.2500) lr 1.9646e-03 eta 0:00:24
epoch [19/200] batch [30/78] time 0.446 (0.466) data 0.316 (0.335) loss_u loss_u 0.9023 (0.9011) acc_u 6.2500 (11.9792) lr 1.9646e-03 eta 0:00:22
epoch [19/200] batch [35/78] time 0.395 (0.463) data 0.263 (0.332) loss_u loss_u 0.9209 (0.9044) acc_u 12.5000 (11.7857) lr 1.9646e-03 eta 0:00:19
epoch [19/200] batch [40/78] time 0.475 (0.460) data 0.344 (0.329) loss_u loss_u 0.8857 (0.9064) acc_u 15.6250 (11.5625) lr 1.9646e-03 eta 0:00:17
epoch [19/200] batch [45/78] time 0.408 (0.461) data 0.277 (0.330) loss_u loss_u 0.9678 (0.9088) acc_u 0.0000 (11.1806) lr 1.9646e-03 eta 0:00:15
epoch [19/200] batch [50/78] time 0.443 (0.461) data 0.313 (0.330) loss_u loss_u 0.9399 (0.9086) acc_u 12.5000 (11.3125) lr 1.9646e-03 eta 0:00:12
epoch [19/200] batch [55/78] time 0.404 (0.463) data 0.273 (0.332) loss_u loss_u 0.8848 (0.9095) acc_u 12.5000 (11.1932) lr 1.9646e-03 eta 0:00:10
epoch [19/200] batch [60/78] time 0.479 (0.461) data 0.348 (0.330) loss_u loss_u 0.8604 (0.9083) acc_u 18.7500 (11.4062) lr 1.9646e-03 eta 0:00:08
epoch [19/200] batch [65/78] time 0.501 (0.464) data 0.369 (0.333) loss_u loss_u 0.9019 (0.9075) acc_u 12.5000 (11.4423) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [70/78] time 0.482 (0.464) data 0.351 (0.333) loss_u loss_u 0.9556 (0.9090) acc_u 3.1250 (11.2500) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [75/78] time 0.447 (0.464) data 0.315 (0.333) loss_u loss_u 0.8940 (0.9092) acc_u 12.5000 (11.2083) lr 1.9646e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1836
confident_label rate tensor(0.2076, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 651
clean true:544
clean false:107
clean_rate:0.8356374807987711
noisy true:756
noisy false:1729
after delete: len(clean_dataset) 651
after delete: len(noisy_dataset) 2485
epoch [20/200] batch [5/20] time 0.403 (0.512) data 0.272 (0.381) loss_x loss_x 1.4268 (1.3693) acc_x 59.3750 (61.8750) lr 1.9603e-03 eta 0:00:07
epoch [20/200] batch [10/20] time 0.398 (0.482) data 0.268 (0.351) loss_x loss_x 1.3877 (1.4403) acc_x 68.7500 (62.1875) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [15/20] time 0.611 (0.476) data 0.481 (0.345) loss_x loss_x 1.2656 (1.4618) acc_x 59.3750 (61.2500) lr 1.9603e-03 eta 0:00:02
epoch [20/200] batch [20/20] time 0.425 (0.467) data 0.295 (0.337) loss_x loss_x 1.6250 (1.5048) acc_x 59.3750 (59.6875) lr 1.9603e-03 eta 0:00:00
epoch [20/200] batch [5/77] time 0.511 (0.457) data 0.379 (0.326) loss_u loss_u 0.8682 (0.9038) acc_u 15.6250 (12.5000) lr 1.9603e-03 eta 0:00:32
epoch [20/200] batch [10/77] time 0.597 (0.460) data 0.466 (0.329) loss_u loss_u 0.9043 (0.9052) acc_u 12.5000 (12.1875) lr 1.9603e-03 eta 0:00:30
epoch [20/200] batch [15/77] time 0.469 (0.456) data 0.338 (0.325) loss_u loss_u 0.8447 (0.8940) acc_u 21.8750 (13.1250) lr 1.9603e-03 eta 0:00:28
epoch [20/200] batch [20/77] time 0.491 (0.457) data 0.360 (0.326) loss_u loss_u 0.9009 (0.8934) acc_u 12.5000 (13.4375) lr 1.9603e-03 eta 0:00:26
epoch [20/200] batch [25/77] time 0.392 (0.459) data 0.262 (0.328) loss_u loss_u 0.9375 (0.8955) acc_u 6.2500 (13.1250) lr 1.9603e-03 eta 0:00:23
epoch [20/200] batch [30/77] time 0.447 (0.458) data 0.316 (0.328) loss_u loss_u 0.9072 (0.8974) acc_u 12.5000 (12.9167) lr 1.9603e-03 eta 0:00:21
epoch [20/200] batch [35/77] time 0.484 (0.460) data 0.354 (0.329) loss_u loss_u 0.9243 (0.8982) acc_u 6.2500 (12.5893) lr 1.9603e-03 eta 0:00:19
epoch [20/200] batch [40/77] time 0.618 (0.461) data 0.488 (0.330) loss_u loss_u 0.8823 (0.8993) acc_u 9.3750 (12.5000) lr 1.9603e-03 eta 0:00:17
epoch [20/200] batch [45/77] time 0.405 (0.460) data 0.274 (0.329) loss_u loss_u 0.8735 (0.9027) acc_u 15.6250 (12.0139) lr 1.9603e-03 eta 0:00:14
epoch [20/200] batch [50/77] time 0.643 (0.459) data 0.511 (0.329) loss_u loss_u 0.9443 (0.9021) acc_u 6.2500 (12.0625) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [55/77] time 0.360 (0.458) data 0.229 (0.327) loss_u loss_u 0.9253 (0.9026) acc_u 6.2500 (11.9318) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [60/77] time 0.427 (0.459) data 0.296 (0.328) loss_u loss_u 0.9009 (0.9016) acc_u 15.6250 (12.0312) lr 1.9603e-03 eta 0:00:07
epoch [20/200] batch [65/77] time 0.425 (0.461) data 0.294 (0.331) loss_u loss_u 0.9097 (0.9027) acc_u 12.5000 (11.9231) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [70/77] time 0.414 (0.462) data 0.284 (0.331) loss_u loss_u 0.8936 (0.9041) acc_u 9.3750 (11.7857) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [75/77] time 0.385 (0.460) data 0.255 (0.330) loss_u loss_u 0.9058 (0.9060) acc_u 6.2500 (11.5417) lr 1.9603e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1796
confident_label rate tensor(0.2034, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 638
clean true:532
clean false:106
clean_rate:0.8338557993730408
noisy true:808
noisy false:1690
after delete: len(clean_dataset) 638
after delete: len(noisy_dataset) 2498
epoch [21/200] batch [5/19] time 0.498 (0.459) data 0.368 (0.328) loss_x loss_x 1.3730 (1.7443) acc_x 59.3750 (54.3750) lr 1.9558e-03 eta 0:00:06
epoch [21/200] batch [10/19] time 0.376 (0.448) data 0.246 (0.318) loss_x loss_x 1.7393 (1.6083) acc_x 50.0000 (56.8750) lr 1.9558e-03 eta 0:00:04
epoch [21/200] batch [15/19] time 0.534 (0.452) data 0.402 (0.321) loss_x loss_x 1.5078 (1.6331) acc_x 56.2500 (56.8750) lr 1.9558e-03 eta 0:00:01
epoch [21/200] batch [5/78] time 0.536 (0.453) data 0.405 (0.322) loss_u loss_u 0.9102 (0.9033) acc_u 9.3750 (10.6250) lr 1.9558e-03 eta 0:00:33
epoch [21/200] batch [10/78] time 0.447 (0.454) data 0.315 (0.323) loss_u loss_u 0.9331 (0.9096) acc_u 6.2500 (10.6250) lr 1.9558e-03 eta 0:00:30
epoch [21/200] batch [15/78] time 0.360 (0.444) data 0.229 (0.314) loss_u loss_u 0.9795 (0.9087) acc_u 0.0000 (10.4167) lr 1.9558e-03 eta 0:00:28
epoch [21/200] batch [20/78] time 0.475 (0.441) data 0.344 (0.310) loss_u loss_u 0.9121 (0.9081) acc_u 9.3750 (10.7812) lr 1.9558e-03 eta 0:00:25
epoch [21/200] batch [25/78] time 0.381 (0.442) data 0.251 (0.311) loss_u loss_u 0.9106 (0.9045) acc_u 15.6250 (11.5000) lr 1.9558e-03 eta 0:00:23
epoch [21/200] batch [30/78] time 0.586 (0.445) data 0.454 (0.314) loss_u loss_u 0.8896 (0.9036) acc_u 15.6250 (11.9792) lr 1.9558e-03 eta 0:00:21
epoch [21/200] batch [35/78] time 0.506 (0.447) data 0.375 (0.316) loss_u loss_u 0.9175 (0.9021) acc_u 9.3750 (12.0536) lr 1.9558e-03 eta 0:00:19
epoch [21/200] batch [40/78] time 0.519 (0.454) data 0.388 (0.323) loss_u loss_u 0.9058 (0.9041) acc_u 6.2500 (11.7969) lr 1.9558e-03 eta 0:00:17
epoch [21/200] batch [45/78] time 0.493 (0.458) data 0.362 (0.327) loss_u loss_u 0.9087 (0.9025) acc_u 9.3750 (12.0833) lr 1.9558e-03 eta 0:00:15
epoch [21/200] batch [50/78] time 0.594 (0.466) data 0.463 (0.335) loss_u loss_u 0.8853 (0.9020) acc_u 15.6250 (12.3750) lr 1.9558e-03 eta 0:00:13
epoch [21/200] batch [55/78] time 0.361 (0.468) data 0.229 (0.337) loss_u loss_u 0.9131 (0.9038) acc_u 6.2500 (11.9318) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [60/78] time 0.386 (0.468) data 0.253 (0.337) loss_u loss_u 0.8188 (0.9044) acc_u 28.1250 (11.9271) lr 1.9558e-03 eta 0:00:08
epoch [21/200] batch [65/78] time 0.379 (0.468) data 0.248 (0.337) loss_u loss_u 0.9297 (0.9061) acc_u 6.2500 (11.6827) lr 1.9558e-03 eta 0:00:06
epoch [21/200] batch [70/78] time 0.584 (0.471) data 0.453 (0.339) loss_u loss_u 0.9619 (0.9085) acc_u 6.2500 (11.4286) lr 1.9558e-03 eta 0:00:03
epoch [21/200] batch [75/78] time 0.432 (0.472) data 0.301 (0.341) loss_u loss_u 0.9277 (0.9082) acc_u 12.5000 (11.5000) lr 1.9558e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1819
confident_label rate tensor(0.2006, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 629
clean true:524
clean false:105
clean_rate:0.8330683624801272
noisy true:793
noisy false:1714
after delete: len(clean_dataset) 629
after delete: len(noisy_dataset) 2507
epoch [22/200] batch [5/19] time 0.703 (0.526) data 0.573 (0.396) loss_x loss_x 1.3789 (1.3939) acc_x 53.1250 (61.8750) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [10/19] time 0.637 (0.515) data 0.504 (0.384) loss_x loss_x 1.2227 (1.4583) acc_x 71.8750 (59.3750) lr 1.9511e-03 eta 0:00:04
epoch [22/200] batch [15/19] time 0.389 (0.498) data 0.259 (0.368) loss_x loss_x 1.5508 (1.5072) acc_x 65.6250 (58.5417) lr 1.9511e-03 eta 0:00:01
epoch [22/200] batch [5/78] time 0.351 (0.489) data 0.219 (0.358) loss_u loss_u 0.9238 (0.8978) acc_u 6.2500 (15.0000) lr 1.9511e-03 eta 0:00:35
epoch [22/200] batch [10/78] time 0.440 (0.481) data 0.309 (0.350) loss_u loss_u 0.8950 (0.9002) acc_u 15.6250 (14.0625) lr 1.9511e-03 eta 0:00:32
epoch [22/200] batch [15/78] time 0.331 (0.472) data 0.199 (0.341) loss_u loss_u 0.9233 (0.8990) acc_u 9.3750 (13.9583) lr 1.9511e-03 eta 0:00:29
epoch [22/200] batch [20/78] time 0.418 (0.478) data 0.287 (0.347) loss_u loss_u 0.9009 (0.8974) acc_u 15.6250 (13.5938) lr 1.9511e-03 eta 0:00:27
epoch [22/200] batch [25/78] time 0.348 (0.468) data 0.218 (0.337) loss_u loss_u 0.8877 (0.8985) acc_u 9.3750 (13.0000) lr 1.9511e-03 eta 0:00:24
epoch [22/200] batch [30/78] time 0.474 (0.467) data 0.343 (0.335) loss_u loss_u 0.9258 (0.9014) acc_u 6.2500 (12.3958) lr 1.9511e-03 eta 0:00:22
epoch [22/200] batch [35/78] time 0.498 (0.469) data 0.366 (0.338) loss_u loss_u 0.9526 (0.9033) acc_u 9.3750 (12.3214) lr 1.9511e-03 eta 0:00:20
epoch [22/200] batch [40/78] time 0.407 (0.465) data 0.275 (0.333) loss_u loss_u 0.9360 (0.9062) acc_u 6.2500 (11.7188) lr 1.9511e-03 eta 0:00:17
epoch [22/200] batch [45/78] time 0.354 (0.463) data 0.223 (0.332) loss_u loss_u 0.9448 (0.9073) acc_u 6.2500 (11.4583) lr 1.9511e-03 eta 0:00:15
epoch [22/200] batch [50/78] time 0.464 (0.466) data 0.332 (0.334) loss_u loss_u 0.8433 (0.9061) acc_u 21.8750 (11.6250) lr 1.9511e-03 eta 0:00:13
epoch [22/200] batch [55/78] time 0.492 (0.464) data 0.360 (0.332) loss_u loss_u 0.9639 (0.9065) acc_u 3.1250 (11.5909) lr 1.9511e-03 eta 0:00:10
epoch [22/200] batch [60/78] time 0.422 (0.463) data 0.290 (0.331) loss_u loss_u 0.9521 (0.9079) acc_u 3.1250 (11.4583) lr 1.9511e-03 eta 0:00:08
epoch [22/200] batch [65/78] time 0.473 (0.459) data 0.341 (0.328) loss_u loss_u 0.9600 (0.9080) acc_u 6.2500 (11.6346) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [70/78] time 0.368 (0.457) data 0.236 (0.326) loss_u loss_u 0.9722 (0.9088) acc_u 0.0000 (11.4286) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [75/78] time 0.402 (0.454) data 0.271 (0.322) loss_u loss_u 0.9453 (0.9104) acc_u 6.2500 (11.2083) lr 1.9511e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1763
confident_label rate tensor(0.2172, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 681
clean true:570
clean false:111
clean_rate:0.8370044052863436
noisy true:803
noisy false:1652
after delete: len(clean_dataset) 681
after delete: len(noisy_dataset) 2455
epoch [23/200] batch [5/21] time 0.357 (0.462) data 0.227 (0.332) loss_x loss_x 1.6328 (1.6250) acc_x 53.1250 (52.5000) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [10/21] time 0.594 (0.497) data 0.463 (0.367) loss_x loss_x 1.1875 (1.6110) acc_x 71.8750 (55.9375) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [15/21] time 0.525 (0.497) data 0.394 (0.366) loss_x loss_x 1.4844 (1.5859) acc_x 56.2500 (57.0833) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [20/21] time 0.504 (0.482) data 0.373 (0.351) loss_x loss_x 1.8037 (1.5767) acc_x 59.3750 (57.1875) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/76] time 0.477 (0.481) data 0.345 (0.350) loss_u loss_u 0.9629 (0.9013) acc_u 3.1250 (11.2500) lr 1.9461e-03 eta 0:00:34
epoch [23/200] batch [10/76] time 0.425 (0.478) data 0.293 (0.347) loss_u loss_u 0.9214 (0.9031) acc_u 9.3750 (12.5000) lr 1.9461e-03 eta 0:00:31
epoch [23/200] batch [15/76] time 0.434 (0.472) data 0.303 (0.341) loss_u loss_u 0.9307 (0.9021) acc_u 9.3750 (11.8750) lr 1.9461e-03 eta 0:00:28
epoch [23/200] batch [20/76] time 0.420 (0.475) data 0.289 (0.344) loss_u loss_u 0.9624 (0.9081) acc_u 0.0000 (10.4688) lr 1.9461e-03 eta 0:00:26
epoch [23/200] batch [25/76] time 0.392 (0.477) data 0.262 (0.346) loss_u loss_u 0.9458 (0.9140) acc_u 3.1250 (9.7500) lr 1.9461e-03 eta 0:00:24
epoch [23/200] batch [30/76] time 0.427 (0.470) data 0.296 (0.339) loss_u loss_u 0.9414 (0.9142) acc_u 9.3750 (9.7917) lr 1.9461e-03 eta 0:00:21
epoch [23/200] batch [35/76] time 0.366 (0.466) data 0.235 (0.335) loss_u loss_u 0.8691 (0.9136) acc_u 18.7500 (10.4464) lr 1.9461e-03 eta 0:00:19
epoch [23/200] batch [40/76] time 0.398 (0.469) data 0.267 (0.338) loss_u loss_u 0.9351 (0.9155) acc_u 3.1250 (10.0000) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [45/76] time 0.530 (0.468) data 0.398 (0.337) loss_u loss_u 0.8975 (0.9120) acc_u 15.6250 (10.4861) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [50/76] time 0.512 (0.468) data 0.380 (0.337) loss_u loss_u 0.9468 (0.9110) acc_u 6.2500 (10.7500) lr 1.9461e-03 eta 0:00:12
epoch [23/200] batch [55/76] time 0.607 (0.467) data 0.475 (0.336) loss_u loss_u 0.8525 (0.9113) acc_u 21.8750 (10.7386) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [60/76] time 0.437 (0.467) data 0.305 (0.336) loss_u loss_u 0.8877 (0.9120) acc_u 12.5000 (10.6250) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [65/76] time 0.359 (0.464) data 0.228 (0.332) loss_u loss_u 0.9517 (0.9112) acc_u 3.1250 (10.7212) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [70/76] time 0.528 (0.468) data 0.396 (0.336) loss_u loss_u 0.9087 (0.9125) acc_u 15.6250 (10.5357) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [75/76] time 0.326 (0.467) data 0.194 (0.335) loss_u loss_u 0.9072 (0.9120) acc_u 12.5000 (10.7500) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1788
confident_label rate tensor(0.2085, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 654
clean true:542
clean false:112
clean_rate:0.8287461773700305
noisy true:806
noisy false:1676
after delete: len(clean_dataset) 654
after delete: len(noisy_dataset) 2482
epoch [24/200] batch [5/20] time 0.487 (0.490) data 0.356 (0.359) loss_x loss_x 1.3076 (1.3627) acc_x 50.0000 (65.6250) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [10/20] time 0.406 (0.471) data 0.275 (0.341) loss_x loss_x 1.6602 (1.4205) acc_x 59.3750 (62.8125) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [15/20] time 0.435 (0.471) data 0.305 (0.340) loss_x loss_x 2.0605 (1.5379) acc_x 43.7500 (59.7917) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [20/20] time 0.471 (0.478) data 0.340 (0.348) loss_x loss_x 1.9434 (1.6006) acc_x 53.1250 (58.7500) lr 1.9409e-03 eta 0:00:00
epoch [24/200] batch [5/77] time 0.414 (0.478) data 0.283 (0.347) loss_u loss_u 0.8633 (0.9294) acc_u 15.6250 (7.5000) lr 1.9409e-03 eta 0:00:34
epoch [24/200] batch [10/77] time 0.563 (0.482) data 0.431 (0.351) loss_u loss_u 0.8672 (0.9206) acc_u 15.6250 (8.7500) lr 1.9409e-03 eta 0:00:32
epoch [24/200] batch [15/77] time 0.428 (0.476) data 0.296 (0.345) loss_u loss_u 0.9043 (0.9205) acc_u 15.6250 (9.3750) lr 1.9409e-03 eta 0:00:29
epoch [24/200] batch [20/77] time 0.474 (0.473) data 0.341 (0.342) loss_u loss_u 0.9448 (0.9226) acc_u 3.1250 (8.7500) lr 1.9409e-03 eta 0:00:26
epoch [24/200] batch [25/77] time 0.435 (0.471) data 0.305 (0.340) loss_u loss_u 0.9253 (0.9216) acc_u 9.3750 (9.1250) lr 1.9409e-03 eta 0:00:24
epoch [24/200] batch [30/77] time 0.367 (0.469) data 0.237 (0.338) loss_u loss_u 0.8501 (0.9150) acc_u 15.6250 (10.4167) lr 1.9409e-03 eta 0:00:22
epoch [24/200] batch [35/77] time 0.517 (0.464) data 0.387 (0.334) loss_u loss_u 0.8828 (0.9116) acc_u 12.5000 (10.8036) lr 1.9409e-03 eta 0:00:19
epoch [24/200] batch [40/77] time 0.485 (0.467) data 0.354 (0.336) loss_u loss_u 0.9238 (0.9110) acc_u 12.5000 (10.9375) lr 1.9409e-03 eta 0:00:17
epoch [24/200] batch [45/77] time 0.398 (0.464) data 0.267 (0.334) loss_u loss_u 0.9463 (0.9112) acc_u 6.2500 (10.7639) lr 1.9409e-03 eta 0:00:14
epoch [24/200] batch [50/77] time 0.530 (0.463) data 0.399 (0.332) loss_u loss_u 0.8672 (0.9085) acc_u 15.6250 (11.2500) lr 1.9409e-03 eta 0:00:12
epoch [24/200] batch [55/77] time 0.394 (0.462) data 0.262 (0.331) loss_u loss_u 0.9448 (0.9103) acc_u 3.1250 (10.7955) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [60/77] time 0.344 (0.460) data 0.213 (0.329) loss_u loss_u 0.8916 (0.9099) acc_u 12.5000 (11.0938) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [65/77] time 0.438 (0.459) data 0.306 (0.328) loss_u loss_u 0.8960 (0.9084) acc_u 12.5000 (11.1538) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [70/77] time 0.476 (0.458) data 0.343 (0.327) loss_u loss_u 0.9141 (0.9073) acc_u 6.2500 (11.2500) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [75/77] time 0.376 (0.460) data 0.245 (0.329) loss_u loss_u 0.9058 (0.9079) acc_u 15.6250 (11.2500) lr 1.9409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1834
confident_label rate tensor(0.2114, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 663
clean true:526
clean false:137
clean_rate:0.7933634992458521
noisy true:776
noisy false:1697
after delete: len(clean_dataset) 663
after delete: len(noisy_dataset) 2473
epoch [25/200] batch [5/20] time 0.389 (0.425) data 0.259 (0.294) loss_x loss_x 1.7520 (1.6748) acc_x 59.3750 (58.7500) lr 1.9354e-03 eta 0:00:06
epoch [25/200] batch [10/20] time 0.536 (0.433) data 0.405 (0.302) loss_x loss_x 1.2402 (1.6111) acc_x 59.3750 (59.6875) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [15/20] time 0.436 (0.444) data 0.305 (0.313) loss_x loss_x 1.5410 (1.6030) acc_x 50.0000 (59.1667) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [20/20] time 0.510 (0.453) data 0.378 (0.322) loss_x loss_x 1.8154 (1.6510) acc_x 53.1250 (57.3438) lr 1.9354e-03 eta 0:00:00
epoch [25/200] batch [5/77] time 0.483 (0.457) data 0.351 (0.326) loss_u loss_u 0.8750 (0.9271) acc_u 15.6250 (8.1250) lr 1.9354e-03 eta 0:00:32
epoch [25/200] batch [10/77] time 0.495 (0.457) data 0.363 (0.325) loss_u loss_u 0.9170 (0.9217) acc_u 6.2500 (8.4375) lr 1.9354e-03 eta 0:00:30
epoch [25/200] batch [15/77] time 0.485 (0.454) data 0.353 (0.323) loss_u loss_u 0.9395 (0.9264) acc_u 6.2500 (7.5000) lr 1.9354e-03 eta 0:00:28
epoch [25/200] batch [20/77] time 0.460 (0.452) data 0.328 (0.321) loss_u loss_u 0.9580 (0.9209) acc_u 9.3750 (9.2188) lr 1.9354e-03 eta 0:00:25
epoch [25/200] batch [25/77] time 0.562 (0.457) data 0.431 (0.325) loss_u loss_u 0.9214 (0.9207) acc_u 6.2500 (9.2500) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [30/77] time 0.650 (0.458) data 0.518 (0.326) loss_u loss_u 0.9282 (0.9207) acc_u 6.2500 (8.9583) lr 1.9354e-03 eta 0:00:21
epoch [25/200] batch [35/77] time 0.356 (0.465) data 0.224 (0.333) loss_u loss_u 0.9048 (0.9181) acc_u 9.3750 (9.1964) lr 1.9354e-03 eta 0:00:19
epoch [25/200] batch [40/77] time 0.409 (0.468) data 0.278 (0.336) loss_u loss_u 0.9150 (0.9150) acc_u 6.2500 (9.8438) lr 1.9354e-03 eta 0:00:17
epoch [25/200] batch [45/77] time 0.477 (0.463) data 0.347 (0.332) loss_u loss_u 0.9238 (0.9131) acc_u 6.2500 (10.4167) lr 1.9354e-03 eta 0:00:14
epoch [25/200] batch [50/77] time 0.586 (0.461) data 0.454 (0.330) loss_u loss_u 0.8545 (0.9133) acc_u 21.8750 (10.4375) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [55/77] time 0.537 (0.460) data 0.405 (0.329) loss_u loss_u 0.8799 (0.9124) acc_u 15.6250 (10.4545) lr 1.9354e-03 eta 0:00:10
epoch [25/200] batch [60/77] time 0.455 (0.463) data 0.323 (0.332) loss_u loss_u 0.9146 (0.9104) acc_u 9.3750 (10.7292) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [65/77] time 0.433 (0.463) data 0.302 (0.332) loss_u loss_u 0.9438 (0.9098) acc_u 0.0000 (10.7692) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [70/77] time 0.368 (0.461) data 0.237 (0.329) loss_u loss_u 0.9355 (0.9106) acc_u 3.1250 (10.5804) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [75/77] time 0.407 (0.461) data 0.273 (0.330) loss_u loss_u 0.9126 (0.9108) acc_u 9.3750 (10.5000) lr 1.9354e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1783
confident_label rate tensor(0.2162, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 678
clean true:554
clean false:124
clean_rate:0.8171091445427728
noisy true:799
noisy false:1659
after delete: len(clean_dataset) 678
after delete: len(noisy_dataset) 2458
epoch [26/200] batch [5/21] time 0.580 (0.447) data 0.449 (0.316) loss_x loss_x 1.9023 (1.6133) acc_x 46.8750 (54.3750) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [10/21] time 0.547 (0.472) data 0.417 (0.342) loss_x loss_x 1.4844 (1.4834) acc_x 62.5000 (59.3750) lr 1.9298e-03 eta 0:00:05
epoch [26/200] batch [15/21] time 0.398 (0.468) data 0.266 (0.337) loss_x loss_x 1.8613 (1.4190) acc_x 56.2500 (61.6667) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [20/21] time 0.435 (0.479) data 0.304 (0.349) loss_x loss_x 1.3193 (1.3729) acc_x 56.2500 (62.3438) lr 1.9298e-03 eta 0:00:00
epoch [26/200] batch [5/76] time 0.615 (0.486) data 0.484 (0.355) loss_u loss_u 0.9429 (0.9298) acc_u 6.2500 (11.2500) lr 1.9298e-03 eta 0:00:34
epoch [26/200] batch [10/76] time 0.407 (0.473) data 0.277 (0.342) loss_u loss_u 0.9131 (0.9183) acc_u 9.3750 (11.5625) lr 1.9298e-03 eta 0:00:31
epoch [26/200] batch [15/76] time 0.401 (0.461) data 0.270 (0.330) loss_u loss_u 0.9595 (0.9178) acc_u 3.1250 (10.8333) lr 1.9298e-03 eta 0:00:28
epoch [26/200] batch [20/76] time 0.429 (0.464) data 0.297 (0.333) loss_u loss_u 0.8516 (0.9128) acc_u 12.5000 (10.4688) lr 1.9298e-03 eta 0:00:26
epoch [26/200] batch [25/76] time 0.439 (0.458) data 0.308 (0.327) loss_u loss_u 0.9438 (0.9167) acc_u 6.2500 (9.5000) lr 1.9298e-03 eta 0:00:23
epoch [26/200] batch [30/76] time 0.371 (0.457) data 0.240 (0.326) loss_u loss_u 0.9175 (0.9178) acc_u 9.3750 (9.5833) lr 1.9298e-03 eta 0:00:21
epoch [26/200] batch [35/76] time 0.555 (0.454) data 0.424 (0.323) loss_u loss_u 0.8979 (0.9160) acc_u 15.6250 (10.3571) lr 1.9298e-03 eta 0:00:18
epoch [26/200] batch [40/76] time 0.562 (0.454) data 0.430 (0.323) loss_u loss_u 0.9370 (0.9159) acc_u 12.5000 (10.5469) lr 1.9298e-03 eta 0:00:16
epoch [26/200] batch [45/76] time 0.400 (0.453) data 0.269 (0.322) loss_u loss_u 0.9067 (0.9168) acc_u 15.6250 (10.3472) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [50/76] time 0.491 (0.452) data 0.359 (0.321) loss_u loss_u 0.8711 (0.9157) acc_u 18.7500 (10.5625) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [55/76] time 0.350 (0.451) data 0.218 (0.320) loss_u loss_u 0.8818 (0.9116) acc_u 15.6250 (11.2500) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [60/76] time 0.468 (0.450) data 0.336 (0.319) loss_u loss_u 0.8452 (0.9090) acc_u 25.0000 (11.6667) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [65/76] time 0.462 (0.453) data 0.330 (0.321) loss_u loss_u 0.9165 (0.9096) acc_u 9.3750 (11.4423) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [70/76] time 0.417 (0.453) data 0.285 (0.321) loss_u loss_u 0.8599 (0.9092) acc_u 18.7500 (11.3839) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [75/76] time 0.429 (0.451) data 0.297 (0.320) loss_u loss_u 0.8945 (0.9110) acc_u 15.6250 (10.9583) lr 1.9298e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1800
confident_label rate tensor(0.2133, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 669
clean true:537
clean false:132
clean_rate:0.8026905829596412
noisy true:799
noisy false:1668
after delete: len(clean_dataset) 669
after delete: len(noisy_dataset) 2467
epoch [27/200] batch [5/20] time 0.546 (0.502) data 0.415 (0.370) loss_x loss_x 1.5195 (1.4164) acc_x 65.6250 (64.3750) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [10/20] time 0.486 (0.463) data 0.356 (0.332) loss_x loss_x 1.4336 (1.3954) acc_x 65.6250 (64.3750) lr 1.9239e-03 eta 0:00:04
epoch [27/200] batch [15/20] time 0.410 (0.457) data 0.280 (0.326) loss_x loss_x 1.4209 (1.4896) acc_x 68.7500 (60.8333) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [20/20] time 0.520 (0.459) data 0.389 (0.328) loss_x loss_x 1.6758 (1.5226) acc_x 53.1250 (60.0000) lr 1.9239e-03 eta 0:00:00
epoch [27/200] batch [5/77] time 0.301 (0.448) data 0.170 (0.317) loss_u loss_u 0.9468 (0.9140) acc_u 9.3750 (11.2500) lr 1.9239e-03 eta 0:00:32
epoch [27/200] batch [10/77] time 0.440 (0.448) data 0.309 (0.317) loss_u loss_u 0.9243 (0.9111) acc_u 12.5000 (12.1875) lr 1.9239e-03 eta 0:00:30
epoch [27/200] batch [15/77] time 0.434 (0.442) data 0.303 (0.312) loss_u loss_u 0.8467 (0.9086) acc_u 18.7500 (12.2917) lr 1.9239e-03 eta 0:00:27
epoch [27/200] batch [20/77] time 0.435 (0.440) data 0.303 (0.310) loss_u loss_u 0.8843 (0.9094) acc_u 12.5000 (11.5625) lr 1.9239e-03 eta 0:00:25
epoch [27/200] batch [25/77] time 0.667 (0.448) data 0.535 (0.317) loss_u loss_u 0.8623 (0.9105) acc_u 21.8750 (11.5000) lr 1.9239e-03 eta 0:00:23
epoch [27/200] batch [30/77] time 0.388 (0.444) data 0.256 (0.313) loss_u loss_u 0.8955 (0.9098) acc_u 15.6250 (11.3542) lr 1.9239e-03 eta 0:00:20
epoch [27/200] batch [35/77] time 0.418 (0.444) data 0.287 (0.313) loss_u loss_u 0.8970 (0.9092) acc_u 15.6250 (11.7857) lr 1.9239e-03 eta 0:00:18
epoch [27/200] batch [40/77] time 0.373 (0.443) data 0.240 (0.312) loss_u loss_u 0.8867 (0.9109) acc_u 15.6250 (11.6406) lr 1.9239e-03 eta 0:00:16
epoch [27/200] batch [45/77] time 0.506 (0.450) data 0.373 (0.319) loss_u loss_u 0.9199 (0.9095) acc_u 6.2500 (11.5972) lr 1.9239e-03 eta 0:00:14
epoch [27/200] batch [50/77] time 0.391 (0.454) data 0.259 (0.323) loss_u loss_u 0.9219 (0.9062) acc_u 12.5000 (12.0000) lr 1.9239e-03 eta 0:00:12
epoch [27/200] batch [55/77] time 0.547 (0.458) data 0.415 (0.326) loss_u loss_u 0.8765 (0.9065) acc_u 15.6250 (11.9886) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [60/77] time 0.344 (0.454) data 0.213 (0.323) loss_u loss_u 0.9004 (0.9070) acc_u 12.5000 (11.8750) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [65/77] time 0.420 (0.452) data 0.289 (0.321) loss_u loss_u 0.9238 (0.9089) acc_u 6.2500 (11.4904) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [70/77] time 0.515 (0.454) data 0.385 (0.323) loss_u loss_u 0.9033 (0.9083) acc_u 12.5000 (11.4286) lr 1.9239e-03 eta 0:00:03
epoch [27/200] batch [75/77] time 0.414 (0.452) data 0.283 (0.321) loss_u loss_u 0.9277 (0.9088) acc_u 9.3750 (11.2917) lr 1.9239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1775
confident_label rate tensor(0.2239, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 702
clean true:573
clean false:129
clean_rate:0.8162393162393162
noisy true:788
noisy false:1646
after delete: len(clean_dataset) 702
after delete: len(noisy_dataset) 2434
epoch [28/200] batch [5/21] time 0.386 (0.450) data 0.255 (0.319) loss_x loss_x 1.7139 (1.5326) acc_x 65.6250 (64.3750) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [10/21] time 0.514 (0.440) data 0.383 (0.309) loss_x loss_x 1.2939 (1.4802) acc_x 68.7500 (64.0625) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [15/21] time 0.486 (0.449) data 0.353 (0.318) loss_x loss_x 1.8711 (1.5412) acc_x 50.0000 (60.6250) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [20/21] time 0.410 (0.469) data 0.279 (0.338) loss_x loss_x 1.2129 (1.5197) acc_x 59.3750 (59.8438) lr 1.9178e-03 eta 0:00:00
epoch [28/200] batch [5/76] time 0.434 (0.487) data 0.302 (0.356) loss_u loss_u 0.9854 (0.9322) acc_u 0.0000 (7.5000) lr 1.9178e-03 eta 0:00:34
epoch [28/200] batch [10/76] time 0.530 (0.477) data 0.399 (0.346) loss_u loss_u 0.9043 (0.9206) acc_u 12.5000 (9.0625) lr 1.9178e-03 eta 0:00:31
epoch [28/200] batch [15/76] time 0.444 (0.473) data 0.312 (0.342) loss_u loss_u 0.9292 (0.9159) acc_u 6.2500 (9.7917) lr 1.9178e-03 eta 0:00:28
epoch [28/200] batch [20/76] time 0.381 (0.469) data 0.249 (0.337) loss_u loss_u 0.8633 (0.9116) acc_u 18.7500 (10.6250) lr 1.9178e-03 eta 0:00:26
epoch [28/200] batch [25/76] time 0.370 (0.467) data 0.240 (0.336) loss_u loss_u 0.9165 (0.9094) acc_u 9.3750 (10.7500) lr 1.9178e-03 eta 0:00:23
epoch [28/200] batch [30/76] time 0.372 (0.459) data 0.239 (0.328) loss_u loss_u 0.8198 (0.9067) acc_u 18.7500 (10.8333) lr 1.9178e-03 eta 0:00:21
epoch [28/200] batch [35/76] time 0.429 (0.461) data 0.297 (0.330) loss_u loss_u 0.8633 (0.9059) acc_u 15.6250 (10.9821) lr 1.9178e-03 eta 0:00:18
epoch [28/200] batch [40/76] time 0.442 (0.458) data 0.311 (0.327) loss_u loss_u 0.9023 (0.9057) acc_u 12.5000 (11.2500) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [45/76] time 0.414 (0.458) data 0.283 (0.326) loss_u loss_u 0.8579 (0.9048) acc_u 28.1250 (11.5972) lr 1.9178e-03 eta 0:00:14
epoch [28/200] batch [50/76] time 0.526 (0.455) data 0.395 (0.324) loss_u loss_u 0.9526 (0.9036) acc_u 6.2500 (11.6875) lr 1.9178e-03 eta 0:00:11
epoch [28/200] batch [55/76] time 0.442 (0.456) data 0.309 (0.324) loss_u loss_u 0.9316 (0.9040) acc_u 12.5000 (11.7614) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [60/76] time 0.433 (0.460) data 0.301 (0.328) loss_u loss_u 0.9028 (0.9046) acc_u 9.3750 (11.6146) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [65/76] time 0.463 (0.458) data 0.333 (0.327) loss_u loss_u 0.9316 (0.9065) acc_u 6.2500 (11.2019) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [70/76] time 0.426 (0.459) data 0.294 (0.328) loss_u loss_u 0.9434 (0.9068) acc_u 9.3750 (11.2500) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [75/76] time 0.476 (0.459) data 0.345 (0.327) loss_u loss_u 0.9111 (0.9054) acc_u 9.3750 (11.3333) lr 1.9178e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1819
confident_label rate tensor(0.2159, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 677
clean true:543
clean false:134
clean_rate:0.8020679468242246
noisy true:774
noisy false:1685
after delete: len(clean_dataset) 677
after delete: len(noisy_dataset) 2459
epoch [29/200] batch [5/21] time 0.470 (0.458) data 0.339 (0.327) loss_x loss_x 1.8242 (1.6068) acc_x 56.2500 (60.0000) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [10/21] time 0.453 (0.472) data 0.323 (0.341) loss_x loss_x 1.4434 (1.5747) acc_x 62.5000 (60.3125) lr 1.9114e-03 eta 0:00:05
epoch [29/200] batch [15/21] time 0.417 (0.479) data 0.286 (0.348) loss_x loss_x 1.1758 (1.5185) acc_x 68.7500 (61.0417) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [20/21] time 0.486 (0.474) data 0.355 (0.343) loss_x loss_x 1.4248 (1.5192) acc_x 62.5000 (61.0938) lr 1.9114e-03 eta 0:00:00
epoch [29/200] batch [5/76] time 0.441 (0.464) data 0.310 (0.333) loss_u loss_u 0.8701 (0.8839) acc_u 18.7500 (13.7500) lr 1.9114e-03 eta 0:00:32
epoch [29/200] batch [10/76] time 0.454 (0.472) data 0.323 (0.341) loss_u loss_u 0.9175 (0.8953) acc_u 9.3750 (13.4375) lr 1.9114e-03 eta 0:00:31
epoch [29/200] batch [15/76] time 0.804 (0.483) data 0.672 (0.352) loss_u loss_u 0.9180 (0.9027) acc_u 12.5000 (12.5000) lr 1.9114e-03 eta 0:00:29
epoch [29/200] batch [20/76] time 0.548 (0.486) data 0.416 (0.355) loss_u loss_u 0.9204 (0.9134) acc_u 15.6250 (11.5625) lr 1.9114e-03 eta 0:00:27
epoch [29/200] batch [25/76] time 0.394 (0.479) data 0.263 (0.348) loss_u loss_u 0.8750 (0.9131) acc_u 12.5000 (11.1250) lr 1.9114e-03 eta 0:00:24
epoch [29/200] batch [30/76] time 0.683 (0.485) data 0.551 (0.354) loss_u loss_u 0.8784 (0.9104) acc_u 9.3750 (11.3542) lr 1.9114e-03 eta 0:00:22
epoch [29/200] batch [35/76] time 0.368 (0.481) data 0.238 (0.350) loss_u loss_u 0.9141 (0.9140) acc_u 9.3750 (10.8036) lr 1.9114e-03 eta 0:00:19
epoch [29/200] batch [40/76] time 0.402 (0.481) data 0.270 (0.350) loss_u loss_u 0.8901 (0.9121) acc_u 9.3750 (10.7031) lr 1.9114e-03 eta 0:00:17
epoch [29/200] batch [45/76] time 0.536 (0.478) data 0.405 (0.347) loss_u loss_u 0.9023 (0.9121) acc_u 6.2500 (10.8333) lr 1.9114e-03 eta 0:00:14
epoch [29/200] batch [50/76] time 0.441 (0.477) data 0.309 (0.346) loss_u loss_u 0.9204 (0.9113) acc_u 12.5000 (11.0625) lr 1.9114e-03 eta 0:00:12
epoch [29/200] batch [55/76] time 0.459 (0.476) data 0.327 (0.345) loss_u loss_u 0.8794 (0.9112) acc_u 18.7500 (11.1364) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [60/76] time 0.469 (0.474) data 0.337 (0.343) loss_u loss_u 0.8325 (0.9080) acc_u 25.0000 (11.6146) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [65/76] time 0.474 (0.475) data 0.343 (0.344) loss_u loss_u 0.9512 (0.9089) acc_u 3.1250 (11.3462) lr 1.9114e-03 eta 0:00:05
epoch [29/200] batch [70/76] time 0.513 (0.475) data 0.382 (0.344) loss_u loss_u 0.8921 (0.9085) acc_u 12.5000 (11.2946) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [75/76] time 0.356 (0.473) data 0.226 (0.342) loss_u loss_u 0.9395 (0.9097) acc_u 6.2500 (11.2083) lr 1.9114e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1893
confident_label rate tensor(0.2114, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 663
clean true:527
clean false:136
clean_rate:0.7948717948717948
noisy true:716
noisy false:1757
after delete: len(clean_dataset) 663
after delete: len(noisy_dataset) 2473
epoch [30/200] batch [5/20] time 0.451 (0.486) data 0.319 (0.355) loss_x loss_x 1.6572 (1.4795) acc_x 59.3750 (66.8750) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [10/20] time 0.432 (0.487) data 0.301 (0.355) loss_x loss_x 1.8096 (1.4241) acc_x 59.3750 (65.3125) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [15/20] time 0.422 (0.475) data 0.292 (0.344) loss_x loss_x 1.0010 (1.3946) acc_x 71.8750 (66.6667) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [20/20] time 0.358 (0.460) data 0.228 (0.329) loss_x loss_x 1.3838 (1.3930) acc_x 59.3750 (65.3125) lr 1.9048e-03 eta 0:00:00
epoch [30/200] batch [5/77] time 0.337 (0.445) data 0.206 (0.314) loss_u loss_u 0.9004 (0.9030) acc_u 12.5000 (13.1250) lr 1.9048e-03 eta 0:00:32
epoch [30/200] batch [10/77] time 0.447 (0.452) data 0.316 (0.321) loss_u loss_u 0.8750 (0.8931) acc_u 12.5000 (13.4375) lr 1.9048e-03 eta 0:00:30
epoch [30/200] batch [15/77] time 0.395 (0.449) data 0.263 (0.318) loss_u loss_u 0.9336 (0.8990) acc_u 6.2500 (12.2917) lr 1.9048e-03 eta 0:00:27
epoch [30/200] batch [20/77] time 0.396 (0.458) data 0.264 (0.327) loss_u loss_u 0.8711 (0.8993) acc_u 15.6250 (12.3438) lr 1.9048e-03 eta 0:00:26
epoch [30/200] batch [25/77] time 0.473 (0.465) data 0.342 (0.334) loss_u loss_u 0.9077 (0.9030) acc_u 6.2500 (11.7500) lr 1.9048e-03 eta 0:00:24
epoch [30/200] batch [30/77] time 0.430 (0.459) data 0.298 (0.328) loss_u loss_u 0.8237 (0.9023) acc_u 18.7500 (11.6667) lr 1.9048e-03 eta 0:00:21
epoch [30/200] batch [35/77] time 0.510 (0.460) data 0.379 (0.329) loss_u loss_u 0.9248 (0.8997) acc_u 6.2500 (11.9643) lr 1.9048e-03 eta 0:00:19
epoch [30/200] batch [40/77] time 0.517 (0.458) data 0.387 (0.327) loss_u loss_u 0.8833 (0.8986) acc_u 18.7500 (12.3438) lr 1.9048e-03 eta 0:00:16
epoch [30/200] batch [45/77] time 0.367 (0.452) data 0.236 (0.320) loss_u loss_u 0.9097 (0.8996) acc_u 15.6250 (12.2917) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [50/77] time 0.476 (0.456) data 0.344 (0.324) loss_u loss_u 0.9336 (0.9015) acc_u 6.2500 (12.0625) lr 1.9048e-03 eta 0:00:12
epoch [30/200] batch [55/77] time 0.442 (0.457) data 0.311 (0.325) loss_u loss_u 0.8262 (0.9004) acc_u 31.2500 (12.4432) lr 1.9048e-03 eta 0:00:10
epoch [30/200] batch [60/77] time 0.469 (0.457) data 0.335 (0.326) loss_u loss_u 0.9175 (0.8997) acc_u 6.2500 (12.5000) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [65/77] time 0.389 (0.456) data 0.257 (0.325) loss_u loss_u 0.8975 (0.9004) acc_u 15.6250 (12.4038) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [70/77] time 0.379 (0.453) data 0.248 (0.322) loss_u loss_u 0.8906 (0.9004) acc_u 12.5000 (12.5000) lr 1.9048e-03 eta 0:00:03
epoch [30/200] batch [75/77] time 0.376 (0.454) data 0.245 (0.323) loss_u loss_u 0.8447 (0.8996) acc_u 28.1250 (12.7500) lr 1.9048e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1771
confident_label rate tensor(0.2223, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 697
clean true:558
clean false:139
clean_rate:0.8005738880918221
noisy true:807
noisy false:1632
after delete: len(clean_dataset) 697
after delete: len(noisy_dataset) 2439
epoch [31/200] batch [5/21] time 0.575 (0.467) data 0.444 (0.336) loss_x loss_x 1.8594 (1.3539) acc_x 53.1250 (63.7500) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [10/21] time 0.426 (0.464) data 0.295 (0.333) loss_x loss_x 1.5469 (1.3276) acc_x 53.1250 (63.4375) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [15/21] time 0.532 (0.505) data 0.401 (0.374) loss_x loss_x 1.9258 (1.4183) acc_x 68.7500 (63.5417) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [20/21] time 0.422 (0.496) data 0.291 (0.365) loss_x loss_x 1.0479 (1.5051) acc_x 71.8750 (61.4062) lr 1.8980e-03 eta 0:00:00
epoch [31/200] batch [5/76] time 0.514 (0.490) data 0.382 (0.359) loss_u loss_u 0.9023 (0.8861) acc_u 9.3750 (15.0000) lr 1.8980e-03 eta 0:00:34
epoch [31/200] batch [10/76] time 0.537 (0.493) data 0.405 (0.362) loss_u loss_u 0.8989 (0.8958) acc_u 12.5000 (13.4375) lr 1.8980e-03 eta 0:00:32
epoch [31/200] batch [15/76] time 0.474 (0.487) data 0.343 (0.356) loss_u loss_u 0.9507 (0.9087) acc_u 9.3750 (12.7083) lr 1.8980e-03 eta 0:00:29
epoch [31/200] batch [20/76] time 0.464 (0.487) data 0.334 (0.356) loss_u loss_u 0.8584 (0.9069) acc_u 18.7500 (12.8125) lr 1.8980e-03 eta 0:00:27
epoch [31/200] batch [25/76] time 0.687 (0.494) data 0.557 (0.363) loss_u loss_u 0.9541 (0.9122) acc_u 3.1250 (12.1250) lr 1.8980e-03 eta 0:00:25
epoch [31/200] batch [30/76] time 0.462 (0.490) data 0.330 (0.359) loss_u loss_u 0.8853 (0.9111) acc_u 9.3750 (11.7708) lr 1.8980e-03 eta 0:00:22
epoch [31/200] batch [35/76] time 0.432 (0.484) data 0.301 (0.353) loss_u loss_u 0.9507 (0.9131) acc_u 6.2500 (11.4286) lr 1.8980e-03 eta 0:00:19
epoch [31/200] batch [40/76] time 0.402 (0.483) data 0.271 (0.352) loss_u loss_u 0.9194 (0.9149) acc_u 12.5000 (10.7812) lr 1.8980e-03 eta 0:00:17
epoch [31/200] batch [45/76] time 0.428 (0.479) data 0.297 (0.348) loss_u loss_u 0.8955 (0.9135) acc_u 12.5000 (10.7639) lr 1.8980e-03 eta 0:00:14
epoch [31/200] batch [50/76] time 0.477 (0.476) data 0.345 (0.345) loss_u loss_u 0.8569 (0.9107) acc_u 12.5000 (11.1875) lr 1.8980e-03 eta 0:00:12
epoch [31/200] batch [55/76] time 0.437 (0.472) data 0.306 (0.341) loss_u loss_u 0.9248 (0.9105) acc_u 15.6250 (11.2500) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [60/76] time 0.416 (0.475) data 0.286 (0.344) loss_u loss_u 0.9146 (0.9114) acc_u 6.2500 (10.8854) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [65/76] time 0.391 (0.475) data 0.259 (0.343) loss_u loss_u 0.9185 (0.9101) acc_u 9.3750 (11.0577) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [70/76] time 0.464 (0.474) data 0.333 (0.342) loss_u loss_u 0.9375 (0.9107) acc_u 6.2500 (10.9821) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [75/76] time 0.437 (0.474) data 0.306 (0.343) loss_u loss_u 0.9126 (0.9093) acc_u 12.5000 (11.2500) lr 1.8980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1796
confident_label rate tensor(0.2162, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 678
clean true:533
clean false:145
clean_rate:0.7861356932153393
noisy true:807
noisy false:1651
after delete: len(clean_dataset) 678
after delete: len(noisy_dataset) 2458
epoch [32/200] batch [5/21] time 0.470 (0.486) data 0.341 (0.355) loss_x loss_x 1.6250 (1.5051) acc_x 65.6250 (61.8750) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [10/21] time 0.531 (0.464) data 0.400 (0.333) loss_x loss_x 1.8584 (1.6614) acc_x 56.2500 (58.7500) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [15/21] time 0.435 (0.478) data 0.304 (0.348) loss_x loss_x 1.4287 (1.5719) acc_x 65.6250 (60.8333) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [20/21] time 0.650 (0.486) data 0.520 (0.356) loss_x loss_x 1.7607 (1.6115) acc_x 53.1250 (59.8438) lr 1.8910e-03 eta 0:00:00
epoch [32/200] batch [5/76] time 0.475 (0.478) data 0.343 (0.347) loss_u loss_u 0.9248 (0.9101) acc_u 9.3750 (12.5000) lr 1.8910e-03 eta 0:00:33
epoch [32/200] batch [10/76] time 0.413 (0.487) data 0.282 (0.357) loss_u loss_u 0.8403 (0.9094) acc_u 18.7500 (11.8750) lr 1.8910e-03 eta 0:00:32
epoch [32/200] batch [15/76] time 0.381 (0.484) data 0.249 (0.353) loss_u loss_u 0.9146 (0.9126) acc_u 15.6250 (11.4583) lr 1.8910e-03 eta 0:00:29
epoch [32/200] batch [20/76] time 0.467 (0.480) data 0.335 (0.350) loss_u loss_u 0.9053 (0.9138) acc_u 6.2500 (10.9375) lr 1.8910e-03 eta 0:00:26
epoch [32/200] batch [25/76] time 0.441 (0.480) data 0.309 (0.349) loss_u loss_u 0.8940 (0.9107) acc_u 12.5000 (11.0000) lr 1.8910e-03 eta 0:00:24
epoch [32/200] batch [30/76] time 0.373 (0.474) data 0.241 (0.343) loss_u loss_u 0.8818 (0.9116) acc_u 18.7500 (11.0417) lr 1.8910e-03 eta 0:00:21
epoch [32/200] batch [35/76] time 0.545 (0.480) data 0.413 (0.349) loss_u loss_u 0.9004 (0.9125) acc_u 12.5000 (10.7143) lr 1.8910e-03 eta 0:00:19
epoch [32/200] batch [40/76] time 0.356 (0.478) data 0.224 (0.347) loss_u loss_u 0.9243 (0.9131) acc_u 15.6250 (10.7031) lr 1.8910e-03 eta 0:00:17
epoch [32/200] batch [45/76] time 0.409 (0.472) data 0.279 (0.341) loss_u loss_u 0.9341 (0.9125) acc_u 6.2500 (10.5556) lr 1.8910e-03 eta 0:00:14
epoch [32/200] batch [50/76] time 0.499 (0.471) data 0.368 (0.340) loss_u loss_u 0.8955 (0.9118) acc_u 12.5000 (10.6250) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [55/76] time 0.457 (0.469) data 0.325 (0.338) loss_u loss_u 0.8643 (0.9114) acc_u 18.7500 (10.5682) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [60/76] time 0.453 (0.469) data 0.321 (0.338) loss_u loss_u 0.8999 (0.9114) acc_u 12.5000 (10.3646) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [65/76] time 0.581 (0.470) data 0.450 (0.339) loss_u loss_u 0.9053 (0.9117) acc_u 15.6250 (10.4327) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [70/76] time 0.429 (0.468) data 0.298 (0.337) loss_u loss_u 0.8745 (0.9098) acc_u 9.3750 (10.5804) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [75/76] time 0.443 (0.470) data 0.312 (0.339) loss_u loss_u 0.9131 (0.9095) acc_u 9.3750 (10.6250) lr 1.8910e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1787
confident_label rate tensor(0.2219, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 696
clean true:554
clean false:142
clean_rate:0.7959770114942529
noisy true:795
noisy false:1645
after delete: len(clean_dataset) 696
after delete: len(noisy_dataset) 2440
epoch [33/200] batch [5/21] time 0.376 (0.417) data 0.247 (0.287) loss_x loss_x 1.5957 (1.6988) acc_x 65.6250 (56.8750) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [10/21] time 0.419 (0.429) data 0.289 (0.299) loss_x loss_x 2.1094 (1.6488) acc_x 43.7500 (55.0000) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [15/21] time 0.525 (0.448) data 0.394 (0.318) loss_x loss_x 1.5068 (1.5796) acc_x 59.3750 (57.2917) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [20/21] time 0.541 (0.457) data 0.407 (0.326) loss_x loss_x 1.7070 (1.5217) acc_x 56.2500 (59.8438) lr 1.8838e-03 eta 0:00:00
epoch [33/200] batch [5/76] time 0.539 (0.480) data 0.407 (0.348) loss_u loss_u 0.8687 (0.8761) acc_u 12.5000 (15.6250) lr 1.8838e-03 eta 0:00:34
epoch [33/200] batch [10/76] time 0.372 (0.463) data 0.240 (0.331) loss_u loss_u 0.9644 (0.8892) acc_u 3.1250 (13.7500) lr 1.8838e-03 eta 0:00:30
epoch [33/200] batch [15/76] time 0.440 (0.459) data 0.307 (0.327) loss_u loss_u 0.9414 (0.9034) acc_u 0.0000 (11.6667) lr 1.8838e-03 eta 0:00:27
epoch [33/200] batch [20/76] time 0.719 (0.464) data 0.588 (0.332) loss_u loss_u 0.9653 (0.9071) acc_u 3.1250 (11.5625) lr 1.8838e-03 eta 0:00:25
epoch [33/200] batch [25/76] time 0.473 (0.461) data 0.340 (0.329) loss_u loss_u 0.8950 (0.9105) acc_u 12.5000 (10.8750) lr 1.8838e-03 eta 0:00:23
epoch [33/200] batch [30/76] time 0.428 (0.460) data 0.296 (0.328) loss_u loss_u 0.9390 (0.9139) acc_u 6.2500 (10.5208) lr 1.8838e-03 eta 0:00:21
epoch [33/200] batch [35/76] time 0.414 (0.458) data 0.281 (0.325) loss_u loss_u 0.9556 (0.9148) acc_u 6.2500 (10.0000) lr 1.8838e-03 eta 0:00:18
epoch [33/200] batch [40/76] time 0.351 (0.460) data 0.220 (0.328) loss_u loss_u 0.9072 (0.9148) acc_u 9.3750 (10.0000) lr 1.8838e-03 eta 0:00:16
epoch [33/200] batch [45/76] time 0.396 (0.459) data 0.262 (0.327) loss_u loss_u 0.8833 (0.9118) acc_u 15.6250 (10.3472) lr 1.8838e-03 eta 0:00:14
epoch [33/200] batch [50/76] time 0.392 (0.455) data 0.260 (0.323) loss_u loss_u 0.9126 (0.9125) acc_u 12.5000 (10.6250) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [55/76] time 0.598 (0.459) data 0.467 (0.327) loss_u loss_u 0.9150 (0.9126) acc_u 9.3750 (10.6250) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [60/76] time 0.499 (0.460) data 0.368 (0.328) loss_u loss_u 0.8940 (0.9131) acc_u 12.5000 (10.5729) lr 1.8838e-03 eta 0:00:07
epoch [33/200] batch [65/76] time 0.468 (0.460) data 0.336 (0.328) loss_u loss_u 0.8921 (0.9100) acc_u 12.5000 (11.0096) lr 1.8838e-03 eta 0:00:05
epoch [33/200] batch [70/76] time 0.371 (0.457) data 0.240 (0.325) loss_u loss_u 0.8994 (0.9100) acc_u 12.5000 (10.9821) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [75/76] time 0.429 (0.460) data 0.297 (0.328) loss_u loss_u 0.9316 (0.9083) acc_u 12.5000 (11.1250) lr 1.8838e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1835
confident_label rate tensor(0.2111, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 662
clean true:521
clean false:141
clean_rate:0.7870090634441088
noisy true:780
noisy false:1694
after delete: len(clean_dataset) 662
after delete: len(noisy_dataset) 2474
epoch [34/200] batch [5/20] time 0.387 (0.468) data 0.256 (0.337) loss_x loss_x 1.7705 (1.4176) acc_x 59.3750 (66.2500) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [10/20] time 0.427 (0.434) data 0.296 (0.303) loss_x loss_x 1.3955 (1.4186) acc_x 59.3750 (62.8125) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [15/20] time 0.595 (0.457) data 0.465 (0.327) loss_x loss_x 1.2285 (1.4493) acc_x 62.5000 (63.1250) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [20/20] time 0.497 (0.462) data 0.367 (0.331) loss_x loss_x 1.4746 (1.4969) acc_x 62.5000 (60.7812) lr 1.8763e-03 eta 0:00:00
epoch [34/200] batch [5/77] time 0.557 (0.467) data 0.427 (0.336) loss_u loss_u 0.9048 (0.9141) acc_u 12.5000 (10.0000) lr 1.8763e-03 eta 0:00:33
epoch [34/200] batch [10/77] time 0.407 (0.462) data 0.276 (0.331) loss_u loss_u 0.8999 (0.9157) acc_u 15.6250 (10.3125) lr 1.8763e-03 eta 0:00:30
epoch [34/200] batch [15/77] time 0.439 (0.458) data 0.308 (0.327) loss_u loss_u 0.8599 (0.9063) acc_u 21.8750 (11.4583) lr 1.8763e-03 eta 0:00:28
epoch [34/200] batch [20/77] time 0.371 (0.456) data 0.240 (0.325) loss_u loss_u 0.8970 (0.9012) acc_u 9.3750 (12.1875) lr 1.8763e-03 eta 0:00:25
epoch [34/200] batch [25/77] time 0.496 (0.459) data 0.365 (0.328) loss_u loss_u 0.8208 (0.8991) acc_u 28.1250 (12.1250) lr 1.8763e-03 eta 0:00:23
epoch [34/200] batch [30/77] time 0.623 (0.464) data 0.491 (0.333) loss_u loss_u 0.9531 (0.8968) acc_u 0.0000 (12.1875) lr 1.8763e-03 eta 0:00:21
epoch [34/200] batch [35/77] time 0.518 (0.465) data 0.386 (0.334) loss_u loss_u 0.8833 (0.8985) acc_u 15.6250 (12.0536) lr 1.8763e-03 eta 0:00:19
epoch [34/200] batch [40/77] time 0.460 (0.462) data 0.329 (0.331) loss_u loss_u 0.9126 (0.8997) acc_u 9.3750 (11.9531) lr 1.8763e-03 eta 0:00:17
epoch [34/200] batch [45/77] time 0.571 (0.464) data 0.438 (0.332) loss_u loss_u 0.9048 (0.9028) acc_u 12.5000 (11.5278) lr 1.8763e-03 eta 0:00:14
epoch [34/200] batch [50/77] time 0.386 (0.462) data 0.254 (0.331) loss_u loss_u 0.9438 (0.9018) acc_u 3.1250 (11.7500) lr 1.8763e-03 eta 0:00:12
epoch [34/200] batch [55/77] time 0.381 (0.460) data 0.249 (0.328) loss_u loss_u 0.8877 (0.8987) acc_u 15.6250 (12.2727) lr 1.8763e-03 eta 0:00:10
epoch [34/200] batch [60/77] time 0.468 (0.462) data 0.336 (0.331) loss_u loss_u 0.8633 (0.8986) acc_u 21.8750 (12.2917) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [65/77] time 0.410 (0.461) data 0.280 (0.330) loss_u loss_u 0.8491 (0.8992) acc_u 31.2500 (12.4519) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [70/77] time 0.520 (0.460) data 0.389 (0.329) loss_u loss_u 0.8862 (0.9004) acc_u 15.6250 (12.5000) lr 1.8763e-03 eta 0:00:03
epoch [34/200] batch [75/77] time 0.475 (0.462) data 0.345 (0.331) loss_u loss_u 0.8804 (0.8994) acc_u 21.8750 (12.7917) lr 1.8763e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1787
confident_label rate tensor(0.2229, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 699
clean true:552
clean false:147
clean_rate:0.7896995708154506
noisy true:797
noisy false:1640
after delete: len(clean_dataset) 699
after delete: len(noisy_dataset) 2437
epoch [35/200] batch [5/21] time 0.445 (0.486) data 0.313 (0.354) loss_x loss_x 1.0938 (1.5469) acc_x 65.6250 (58.7500) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [10/21] time 0.392 (0.495) data 0.260 (0.363) loss_x loss_x 1.2637 (1.4421) acc_x 75.0000 (62.5000) lr 1.8686e-03 eta 0:00:05
epoch [35/200] batch [15/21] time 0.462 (0.494) data 0.332 (0.362) loss_x loss_x 2.1699 (1.4730) acc_x 53.1250 (61.6667) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [20/21] time 0.544 (0.497) data 0.414 (0.365) loss_x loss_x 1.1865 (1.4898) acc_x 62.5000 (60.6250) lr 1.8686e-03 eta 0:00:00
epoch [35/200] batch [5/76] time 0.349 (0.486) data 0.218 (0.354) loss_u loss_u 0.8467 (0.8961) acc_u 18.7500 (11.2500) lr 1.8686e-03 eta 0:00:34
epoch [35/200] batch [10/76] time 0.428 (0.487) data 0.297 (0.356) loss_u loss_u 0.8955 (0.9058) acc_u 15.6250 (11.8750) lr 1.8686e-03 eta 0:00:32
epoch [35/200] batch [15/76] time 0.531 (0.487) data 0.399 (0.355) loss_u loss_u 0.9033 (0.9021) acc_u 6.2500 (11.2500) lr 1.8686e-03 eta 0:00:29
epoch [35/200] batch [20/76] time 0.458 (0.478) data 0.326 (0.347) loss_u loss_u 0.8711 (0.8979) acc_u 18.7500 (11.8750) lr 1.8686e-03 eta 0:00:26
epoch [35/200] batch [25/76] time 0.416 (0.479) data 0.285 (0.348) loss_u loss_u 0.9150 (0.9035) acc_u 9.3750 (11.5000) lr 1.8686e-03 eta 0:00:24
epoch [35/200] batch [30/76] time 0.448 (0.485) data 0.315 (0.353) loss_u loss_u 0.9062 (0.9038) acc_u 15.6250 (11.5625) lr 1.8686e-03 eta 0:00:22
epoch [35/200] batch [35/76] time 0.420 (0.483) data 0.289 (0.352) loss_u loss_u 0.8901 (0.9030) acc_u 12.5000 (11.7857) lr 1.8686e-03 eta 0:00:19
epoch [35/200] batch [40/76] time 0.609 (0.485) data 0.477 (0.353) loss_u loss_u 0.9131 (0.9032) acc_u 12.5000 (12.1094) lr 1.8686e-03 eta 0:00:17
epoch [35/200] batch [45/76] time 0.316 (0.483) data 0.184 (0.351) loss_u loss_u 0.9028 (0.9038) acc_u 9.3750 (11.9444) lr 1.8686e-03 eta 0:00:14
epoch [35/200] batch [50/76] time 0.458 (0.475) data 0.327 (0.343) loss_u loss_u 0.8696 (0.9029) acc_u 18.7500 (12.1875) lr 1.8686e-03 eta 0:00:12
epoch [35/200] batch [55/76] time 0.408 (0.475) data 0.277 (0.343) loss_u loss_u 0.8818 (0.9032) acc_u 21.8750 (12.2159) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [60/76] time 0.529 (0.477) data 0.397 (0.345) loss_u loss_u 0.8809 (0.9031) acc_u 15.6250 (12.3958) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [65/76] time 0.470 (0.478) data 0.338 (0.346) loss_u loss_u 0.9282 (0.9038) acc_u 9.3750 (12.1635) lr 1.8686e-03 eta 0:00:05
epoch [35/200] batch [70/76] time 0.418 (0.478) data 0.288 (0.347) loss_u loss_u 0.8882 (0.9021) acc_u 9.3750 (12.2768) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [75/76] time 0.517 (0.478) data 0.385 (0.346) loss_u loss_u 0.8701 (0.9035) acc_u 15.6250 (12.1667) lr 1.8686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1785
confident_label rate tensor(0.2267, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 711
clean true:563
clean false:148
clean_rate:0.7918424753867792
noisy true:788
noisy false:1637
after delete: len(clean_dataset) 711
after delete: len(noisy_dataset) 2425
epoch [36/200] batch [5/22] time 0.450 (0.528) data 0.318 (0.397) loss_x loss_x 0.7764 (1.5266) acc_x 81.2500 (66.8750) lr 1.8607e-03 eta 0:00:08
epoch [36/200] batch [10/22] time 0.420 (0.510) data 0.289 (0.379) loss_x loss_x 1.2852 (1.4159) acc_x 59.3750 (64.0625) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [15/22] time 0.531 (0.509) data 0.400 (0.379) loss_x loss_x 1.0156 (1.3491) acc_x 75.0000 (65.0000) lr 1.8607e-03 eta 0:00:03
epoch [36/200] batch [20/22] time 0.600 (0.513) data 0.469 (0.382) loss_x loss_x 1.4678 (1.4408) acc_x 53.1250 (62.3438) lr 1.8607e-03 eta 0:00:01
epoch [36/200] batch [5/75] time 0.380 (0.509) data 0.248 (0.378) loss_u loss_u 0.8594 (0.9062) acc_u 18.7500 (11.8750) lr 1.8607e-03 eta 0:00:35
epoch [36/200] batch [10/75] time 0.548 (0.509) data 0.417 (0.379) loss_u loss_u 0.8486 (0.9080) acc_u 18.7500 (11.8750) lr 1.8607e-03 eta 0:00:33
epoch [36/200] batch [15/75] time 0.394 (0.504) data 0.262 (0.373) loss_u loss_u 0.9072 (0.9124) acc_u 12.5000 (11.2500) lr 1.8607e-03 eta 0:00:30
epoch [36/200] batch [20/75] time 0.489 (0.499) data 0.357 (0.368) loss_u loss_u 0.9565 (0.9161) acc_u 6.2500 (10.3125) lr 1.8607e-03 eta 0:00:27
epoch [36/200] batch [25/75] time 0.325 (0.487) data 0.193 (0.356) loss_u loss_u 0.9741 (0.9156) acc_u 3.1250 (10.7500) lr 1.8607e-03 eta 0:00:24
epoch [36/200] batch [30/75] time 0.401 (0.481) data 0.270 (0.350) loss_u loss_u 0.8853 (0.9096) acc_u 9.3750 (11.4583) lr 1.8607e-03 eta 0:00:21
epoch [36/200] batch [35/75] time 0.364 (0.475) data 0.233 (0.343) loss_u loss_u 0.9438 (0.9069) acc_u 9.3750 (11.6964) lr 1.8607e-03 eta 0:00:18
epoch [36/200] batch [40/75] time 0.418 (0.472) data 0.286 (0.340) loss_u loss_u 0.9175 (0.9093) acc_u 12.5000 (11.1719) lr 1.8607e-03 eta 0:00:16
epoch [36/200] batch [45/75] time 0.510 (0.472) data 0.378 (0.341) loss_u loss_u 0.8457 (0.9072) acc_u 21.8750 (11.5278) lr 1.8607e-03 eta 0:00:14
epoch [36/200] batch [50/75] time 0.432 (0.472) data 0.301 (0.341) loss_u loss_u 0.8730 (0.9056) acc_u 21.8750 (11.6875) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [55/75] time 0.534 (0.474) data 0.403 (0.343) loss_u loss_u 0.8926 (0.9057) acc_u 18.7500 (11.8182) lr 1.8607e-03 eta 0:00:09
epoch [36/200] batch [60/75] time 0.445 (0.473) data 0.314 (0.341) loss_u loss_u 0.9272 (0.9067) acc_u 6.2500 (11.3021) lr 1.8607e-03 eta 0:00:07
epoch [36/200] batch [65/75] time 0.497 (0.472) data 0.366 (0.340) loss_u loss_u 0.8667 (0.9056) acc_u 18.7500 (11.5385) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [70/75] time 0.494 (0.471) data 0.363 (0.340) loss_u loss_u 0.9165 (0.9054) acc_u 9.3750 (11.4286) lr 1.8607e-03 eta 0:00:02
epoch [36/200] batch [75/75] time 0.404 (0.470) data 0.272 (0.339) loss_u loss_u 0.9033 (0.9062) acc_u 12.5000 (11.4167) lr 1.8607e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1774
confident_label rate tensor(0.2226, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 698
clean true:549
clean false:149
clean_rate:0.7865329512893983
noisy true:813
noisy false:1625
after delete: len(clean_dataset) 698
after delete: len(noisy_dataset) 2438
epoch [37/200] batch [5/21] time 0.525 (0.527) data 0.394 (0.396) loss_x loss_x 1.7842 (1.3846) acc_x 62.5000 (64.3750) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [10/21] time 0.379 (0.505) data 0.249 (0.374) loss_x loss_x 1.8496 (1.5399) acc_x 56.2500 (61.5625) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [15/21] time 0.450 (0.509) data 0.320 (0.378) loss_x loss_x 1.1719 (1.5091) acc_x 71.8750 (61.4583) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [20/21] time 0.403 (0.506) data 0.273 (0.375) loss_x loss_x 1.2061 (1.4873) acc_x 68.7500 (61.4062) lr 1.8526e-03 eta 0:00:00
epoch [37/200] batch [5/76] time 0.420 (0.483) data 0.290 (0.353) loss_u loss_u 0.9438 (0.9247) acc_u 9.3750 (7.5000) lr 1.8526e-03 eta 0:00:34
epoch [37/200] batch [10/76] time 0.588 (0.487) data 0.458 (0.357) loss_u loss_u 0.9277 (0.9030) acc_u 12.5000 (11.2500) lr 1.8526e-03 eta 0:00:32
epoch [37/200] batch [15/76] time 0.336 (0.477) data 0.205 (0.346) loss_u loss_u 0.9463 (0.8999) acc_u 6.2500 (11.4583) lr 1.8526e-03 eta 0:00:29
epoch [37/200] batch [20/76] time 0.404 (0.474) data 0.273 (0.343) loss_u loss_u 0.8940 (0.8994) acc_u 12.5000 (12.0312) lr 1.8526e-03 eta 0:00:26
epoch [37/200] batch [25/76] time 0.598 (0.470) data 0.466 (0.339) loss_u loss_u 0.9414 (0.9033) acc_u 9.3750 (11.5000) lr 1.8526e-03 eta 0:00:23
epoch [37/200] batch [30/76] time 0.473 (0.472) data 0.342 (0.342) loss_u loss_u 0.9526 (0.9022) acc_u 6.2500 (11.6667) lr 1.8526e-03 eta 0:00:21
epoch [37/200] batch [35/76] time 0.579 (0.484) data 0.448 (0.353) loss_u loss_u 0.9316 (0.9022) acc_u 6.2500 (11.5179) lr 1.8526e-03 eta 0:00:19
epoch [37/200] batch [40/76] time 0.507 (0.486) data 0.375 (0.355) loss_u loss_u 0.8530 (0.9032) acc_u 21.8750 (11.3281) lr 1.8526e-03 eta 0:00:17
epoch [37/200] batch [45/76] time 0.494 (0.481) data 0.363 (0.350) loss_u loss_u 0.9443 (0.9051) acc_u 3.1250 (10.9722) lr 1.8526e-03 eta 0:00:14
epoch [37/200] batch [50/76] time 0.492 (0.482) data 0.361 (0.350) loss_u loss_u 0.9258 (0.9038) acc_u 9.3750 (11.2500) lr 1.8526e-03 eta 0:00:12
epoch [37/200] batch [55/76] time 0.516 (0.483) data 0.385 (0.352) loss_u loss_u 0.9106 (0.9049) acc_u 12.5000 (11.0795) lr 1.8526e-03 eta 0:00:10
epoch [37/200] batch [60/76] time 0.511 (0.481) data 0.380 (0.350) loss_u loss_u 0.8667 (0.9040) acc_u 21.8750 (11.3542) lr 1.8526e-03 eta 0:00:07
epoch [37/200] batch [65/76] time 0.441 (0.477) data 0.309 (0.345) loss_u loss_u 0.8882 (0.9030) acc_u 9.3750 (11.5385) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [70/76] time 0.625 (0.478) data 0.494 (0.346) loss_u loss_u 0.8755 (0.9041) acc_u 18.7500 (11.6071) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [75/76] time 0.484 (0.476) data 0.352 (0.345) loss_u loss_u 0.9028 (0.9043) acc_u 12.5000 (11.5417) lr 1.8526e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1801
confident_label rate tensor(0.2325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 729
clean true:568
clean false:161
clean_rate:0.7791495198902606
noisy true:767
noisy false:1640
after delete: len(clean_dataset) 729
after delete: len(noisy_dataset) 2407
epoch [38/200] batch [5/22] time 0.633 (0.477) data 0.503 (0.346) loss_x loss_x 1.7422 (1.4039) acc_x 56.2500 (58.7500) lr 1.8443e-03 eta 0:00:08
epoch [38/200] batch [10/22] time 0.575 (0.484) data 0.445 (0.353) loss_x loss_x 1.3779 (1.3870) acc_x 62.5000 (60.6250) lr 1.8443e-03 eta 0:00:05
epoch [38/200] batch [15/22] time 0.443 (0.495) data 0.312 (0.364) loss_x loss_x 1.5273 (1.4433) acc_x 56.2500 (59.1667) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [20/22] time 0.426 (0.496) data 0.296 (0.366) loss_x loss_x 1.3691 (1.4500) acc_x 65.6250 (60.0000) lr 1.8443e-03 eta 0:00:00
epoch [38/200] batch [5/75] time 0.450 (0.494) data 0.319 (0.363) loss_u loss_u 0.9106 (0.8940) acc_u 15.6250 (15.6250) lr 1.8443e-03 eta 0:00:34
epoch [38/200] batch [10/75] time 0.446 (0.491) data 0.315 (0.360) loss_u loss_u 0.8647 (0.8999) acc_u 15.6250 (13.4375) lr 1.8443e-03 eta 0:00:31
epoch [38/200] batch [15/75] time 0.418 (0.481) data 0.286 (0.350) loss_u loss_u 0.9658 (0.8969) acc_u 6.2500 (13.1250) lr 1.8443e-03 eta 0:00:28
epoch [38/200] batch [20/75] time 0.316 (0.469) data 0.185 (0.338) loss_u loss_u 0.9600 (0.9069) acc_u 6.2500 (11.7188) lr 1.8443e-03 eta 0:00:25
epoch [38/200] batch [25/75] time 0.505 (0.459) data 0.373 (0.328) loss_u loss_u 0.8589 (0.9035) acc_u 15.6250 (11.8750) lr 1.8443e-03 eta 0:00:22
epoch [38/200] batch [30/75] time 0.864 (0.470) data 0.733 (0.339) loss_u loss_u 0.9043 (0.9079) acc_u 15.6250 (11.4583) lr 1.8443e-03 eta 0:00:21
epoch [38/200] batch [35/75] time 0.806 (0.476) data 0.675 (0.345) loss_u loss_u 0.9106 (0.9093) acc_u 12.5000 (11.2500) lr 1.8443e-03 eta 0:00:19
epoch [38/200] batch [40/75] time 0.479 (0.475) data 0.347 (0.344) loss_u loss_u 0.9561 (0.9123) acc_u 3.1250 (10.7812) lr 1.8443e-03 eta 0:00:16
epoch [38/200] batch [45/75] time 0.399 (0.476) data 0.267 (0.345) loss_u loss_u 0.8345 (0.9101) acc_u 25.0000 (11.1806) lr 1.8443e-03 eta 0:00:14
epoch [38/200] batch [50/75] time 0.474 (0.477) data 0.342 (0.345) loss_u loss_u 0.9487 (0.9077) acc_u 6.2500 (11.6875) lr 1.8443e-03 eta 0:00:11
epoch [38/200] batch [55/75] time 0.403 (0.474) data 0.271 (0.343) loss_u loss_u 0.9165 (0.9090) acc_u 6.2500 (11.4773) lr 1.8443e-03 eta 0:00:09
epoch [38/200] batch [60/75] time 0.411 (0.473) data 0.278 (0.342) loss_u loss_u 0.8423 (0.9077) acc_u 21.8750 (11.5104) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [65/75] time 0.485 (0.475) data 0.353 (0.343) loss_u loss_u 0.9224 (0.9089) acc_u 15.6250 (11.3942) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [70/75] time 0.386 (0.476) data 0.255 (0.344) loss_u loss_u 0.9551 (0.9107) acc_u 3.1250 (11.1161) lr 1.8443e-03 eta 0:00:02
epoch [38/200] batch [75/75] time 0.600 (0.476) data 0.469 (0.344) loss_u loss_u 0.9526 (0.9102) acc_u 9.3750 (11.2083) lr 1.8443e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1775
confident_label rate tensor(0.2181, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 684
clean true:565
clean false:119
clean_rate:0.8260233918128655
noisy true:796
noisy false:1656
after delete: len(clean_dataset) 684
after delete: len(noisy_dataset) 2452
epoch [39/200] batch [5/21] time 0.435 (0.474) data 0.304 (0.343) loss_x loss_x 1.4854 (1.4490) acc_x 56.2500 (63.1250) lr 1.8358e-03 eta 0:00:07
epoch [39/200] batch [10/21] time 0.592 (0.483) data 0.462 (0.352) loss_x loss_x 1.4658 (1.4758) acc_x 53.1250 (60.3125) lr 1.8358e-03 eta 0:00:05
epoch [39/200] batch [15/21] time 0.494 (0.477) data 0.364 (0.346) loss_x loss_x 1.7354 (1.5119) acc_x 59.3750 (60.8333) lr 1.8358e-03 eta 0:00:02
epoch [39/200] batch [20/21] time 0.511 (0.478) data 0.380 (0.348) loss_x loss_x 1.3965 (1.4628) acc_x 65.6250 (61.4062) lr 1.8358e-03 eta 0:00:00
epoch [39/200] batch [5/76] time 0.471 (0.469) data 0.339 (0.338) loss_u loss_u 0.9111 (0.9020) acc_u 9.3750 (11.2500) lr 1.8358e-03 eta 0:00:33
epoch [39/200] batch [10/76] time 0.416 (0.459) data 0.285 (0.328) loss_u loss_u 0.8730 (0.8986) acc_u 18.7500 (11.5625) lr 1.8358e-03 eta 0:00:30
epoch [39/200] batch [15/76] time 0.436 (0.455) data 0.301 (0.324) loss_u loss_u 0.8916 (0.9015) acc_u 15.6250 (11.6667) lr 1.8358e-03 eta 0:00:27
epoch [39/200] batch [20/76] time 0.366 (0.458) data 0.235 (0.327) loss_u loss_u 0.9707 (0.9083) acc_u 6.2500 (10.7812) lr 1.8358e-03 eta 0:00:25
epoch [39/200] batch [25/76] time 0.353 (0.458) data 0.220 (0.327) loss_u loss_u 0.8657 (0.9021) acc_u 18.7500 (11.6250) lr 1.8358e-03 eta 0:00:23
epoch [39/200] batch [30/76] time 0.394 (0.457) data 0.262 (0.326) loss_u loss_u 0.9028 (0.9035) acc_u 12.5000 (11.4583) lr 1.8358e-03 eta 0:00:21
epoch [39/200] batch [35/76] time 0.475 (0.458) data 0.342 (0.327) loss_u loss_u 0.9385 (0.9061) acc_u 3.1250 (11.2500) lr 1.8358e-03 eta 0:00:18
epoch [39/200] batch [40/76] time 0.384 (0.459) data 0.254 (0.327) loss_u loss_u 0.8994 (0.9047) acc_u 12.5000 (11.4844) lr 1.8358e-03 eta 0:00:16
epoch [39/200] batch [45/76] time 0.457 (0.456) data 0.325 (0.325) loss_u loss_u 0.8989 (0.9028) acc_u 6.2500 (11.5278) lr 1.8358e-03 eta 0:00:14
epoch [39/200] batch [50/76] time 0.375 (0.454) data 0.244 (0.322) loss_u loss_u 0.8989 (0.9010) acc_u 9.3750 (11.8125) lr 1.8358e-03 eta 0:00:11
epoch [39/200] batch [55/76] time 0.363 (0.453) data 0.232 (0.322) loss_u loss_u 0.9263 (0.9012) acc_u 9.3750 (11.8750) lr 1.8358e-03 eta 0:00:09
epoch [39/200] batch [60/76] time 0.441 (0.454) data 0.309 (0.323) loss_u loss_u 0.9248 (0.9020) acc_u 9.3750 (11.8750) lr 1.8358e-03 eta 0:00:07
epoch [39/200] batch [65/76] time 0.438 (0.456) data 0.307 (0.325) loss_u loss_u 0.8721 (0.9003) acc_u 12.5000 (12.0192) lr 1.8358e-03 eta 0:00:05
epoch [39/200] batch [70/76] time 0.565 (0.458) data 0.433 (0.327) loss_u loss_u 0.9268 (0.9004) acc_u 12.5000 (12.1875) lr 1.8358e-03 eta 0:00:02
epoch [39/200] batch [75/76] time 0.516 (0.457) data 0.385 (0.326) loss_u loss_u 0.8877 (0.9022) acc_u 12.5000 (11.9583) lr 1.8358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1767
confident_label rate tensor(0.2235, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 701
clean true:559
clean false:142
clean_rate:0.797432239657632
noisy true:810
noisy false:1625
after delete: len(clean_dataset) 701
after delete: len(noisy_dataset) 2435
epoch [40/200] batch [5/21] time 0.624 (0.544) data 0.493 (0.413) loss_x loss_x 1.1533 (1.2758) acc_x 68.7500 (64.3750) lr 1.8271e-03 eta 0:00:08
epoch [40/200] batch [10/21] time 0.442 (0.509) data 0.311 (0.377) loss_x loss_x 1.8955 (1.3305) acc_x 59.3750 (65.9375) lr 1.8271e-03 eta 0:00:05
epoch [40/200] batch [15/21] time 0.527 (0.514) data 0.396 (0.382) loss_x loss_x 1.5049 (1.4365) acc_x 71.8750 (63.3333) lr 1.8271e-03 eta 0:00:03
epoch [40/200] batch [20/21] time 0.490 (0.516) data 0.360 (0.384) loss_x loss_x 1.4150 (1.4035) acc_x 68.7500 (63.2812) lr 1.8271e-03 eta 0:00:00
epoch [40/200] batch [5/76] time 0.596 (0.501) data 0.465 (0.369) loss_u loss_u 0.9780 (0.9320) acc_u 0.0000 (6.2500) lr 1.8271e-03 eta 0:00:35
epoch [40/200] batch [10/76] time 0.379 (0.492) data 0.248 (0.361) loss_u loss_u 0.9053 (0.9169) acc_u 9.3750 (8.7500) lr 1.8271e-03 eta 0:00:32
epoch [40/200] batch [15/76] time 0.456 (0.487) data 0.325 (0.356) loss_u loss_u 0.9258 (0.9168) acc_u 12.5000 (9.1667) lr 1.8271e-03 eta 0:00:29
epoch [40/200] batch [20/76] time 0.492 (0.487) data 0.360 (0.356) loss_u loss_u 0.9365 (0.9119) acc_u 6.2500 (9.6875) lr 1.8271e-03 eta 0:00:27
epoch [40/200] batch [25/76] time 0.553 (0.481) data 0.421 (0.350) loss_u loss_u 0.8198 (0.9073) acc_u 21.8750 (10.5000) lr 1.8271e-03 eta 0:00:24
epoch [40/200] batch [30/76] time 0.501 (0.484) data 0.369 (0.352) loss_u loss_u 0.9248 (0.9079) acc_u 12.5000 (10.7292) lr 1.8271e-03 eta 0:00:22
epoch [40/200] batch [35/76] time 0.492 (0.488) data 0.362 (0.356) loss_u loss_u 0.9272 (0.9054) acc_u 9.3750 (11.2500) lr 1.8271e-03 eta 0:00:19
epoch [40/200] batch [40/76] time 0.754 (0.488) data 0.624 (0.357) loss_u loss_u 0.8921 (0.9031) acc_u 9.3750 (11.4844) lr 1.8271e-03 eta 0:00:17
epoch [40/200] batch [45/76] time 0.460 (0.484) data 0.328 (0.352) loss_u loss_u 0.9517 (0.9039) acc_u 3.1250 (11.3194) lr 1.8271e-03 eta 0:00:14
epoch [40/200] batch [50/76] time 0.387 (0.481) data 0.255 (0.350) loss_u loss_u 0.9629 (0.9074) acc_u 6.2500 (10.9375) lr 1.8271e-03 eta 0:00:12
epoch [40/200] batch [55/76] time 0.415 (0.477) data 0.282 (0.345) loss_u loss_u 0.9268 (0.9059) acc_u 6.2500 (11.1932) lr 1.8271e-03 eta 0:00:10
epoch [40/200] batch [60/76] time 0.579 (0.474) data 0.446 (0.343) loss_u loss_u 0.8975 (0.9061) acc_u 12.5000 (11.1979) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [65/76] time 0.474 (0.472) data 0.341 (0.340) loss_u loss_u 0.8447 (0.9059) acc_u 18.7500 (11.1058) lr 1.8271e-03 eta 0:00:05
epoch [40/200] batch [70/76] time 0.495 (0.472) data 0.364 (0.340) loss_u loss_u 0.8931 (0.9042) acc_u 15.6250 (11.2946) lr 1.8271e-03 eta 0:00:02
epoch [40/200] batch [75/76] time 0.461 (0.470) data 0.330 (0.339) loss_u loss_u 0.9346 (0.9026) acc_u 6.2500 (11.5833) lr 1.8271e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1786
confident_label rate tensor(0.2302, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 722
clean true:575
clean false:147
clean_rate:0.796398891966759
noisy true:775
noisy false:1639
after delete: len(clean_dataset) 722
after delete: len(noisy_dataset) 2414
epoch [41/200] batch [5/22] time 0.529 (0.483) data 0.398 (0.352) loss_x loss_x 1.4844 (1.2665) acc_x 53.1250 (66.2500) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [10/22] time 0.381 (0.470) data 0.250 (0.339) loss_x loss_x 1.1543 (1.3485) acc_x 68.7500 (62.1875) lr 1.8181e-03 eta 0:00:05
epoch [41/200] batch [15/22] time 0.368 (0.457) data 0.237 (0.326) loss_x loss_x 1.8135 (1.4117) acc_x 59.3750 (60.8333) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [20/22] time 0.443 (0.458) data 0.312 (0.327) loss_x loss_x 1.3359 (1.4280) acc_x 65.6250 (60.9375) lr 1.8181e-03 eta 0:00:00
epoch [41/200] batch [5/75] time 0.531 (0.459) data 0.400 (0.328) loss_u loss_u 0.8496 (0.9155) acc_u 21.8750 (10.6250) lr 1.8181e-03 eta 0:00:32
epoch [41/200] batch [10/75] time 0.436 (0.464) data 0.304 (0.333) loss_u loss_u 0.9683 (0.9144) acc_u 0.0000 (10.3125) lr 1.8181e-03 eta 0:00:30
epoch [41/200] batch [15/75] time 0.473 (0.458) data 0.340 (0.327) loss_u loss_u 0.9717 (0.9194) acc_u 6.2500 (10.0000) lr 1.8181e-03 eta 0:00:27
epoch [41/200] batch [20/75] time 0.527 (0.465) data 0.396 (0.333) loss_u loss_u 0.9028 (0.9128) acc_u 12.5000 (10.6250) lr 1.8181e-03 eta 0:00:25
epoch [41/200] batch [25/75] time 0.399 (0.472) data 0.269 (0.341) loss_u loss_u 0.9331 (0.9124) acc_u 6.2500 (10.5000) lr 1.8181e-03 eta 0:00:23
epoch [41/200] batch [30/75] time 0.383 (0.464) data 0.251 (0.332) loss_u loss_u 0.8999 (0.9099) acc_u 12.5000 (10.7292) lr 1.8181e-03 eta 0:00:20
epoch [41/200] batch [35/75] time 0.407 (0.463) data 0.276 (0.331) loss_u loss_u 0.9380 (0.9103) acc_u 3.1250 (10.6250) lr 1.8181e-03 eta 0:00:18
epoch [41/200] batch [40/75] time 0.485 (0.463) data 0.353 (0.331) loss_u loss_u 0.8706 (0.9088) acc_u 12.5000 (10.7031) lr 1.8181e-03 eta 0:00:16
epoch [41/200] batch [45/75] time 0.459 (0.462) data 0.327 (0.330) loss_u loss_u 0.9014 (0.9082) acc_u 9.3750 (10.9028) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [50/75] time 0.513 (0.461) data 0.381 (0.330) loss_u loss_u 0.8467 (0.9083) acc_u 18.7500 (10.8750) lr 1.8181e-03 eta 0:00:11
epoch [41/200] batch [55/75] time 0.353 (0.459) data 0.221 (0.327) loss_u loss_u 0.8750 (0.9085) acc_u 18.7500 (10.8523) lr 1.8181e-03 eta 0:00:09
epoch [41/200] batch [60/75] time 0.465 (0.458) data 0.334 (0.326) loss_u loss_u 0.9287 (0.9071) acc_u 9.3750 (11.1458) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [65/75] time 0.466 (0.460) data 0.334 (0.329) loss_u loss_u 0.9033 (0.9073) acc_u 9.3750 (11.1538) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [70/75] time 0.465 (0.460) data 0.334 (0.329) loss_u loss_u 0.9307 (0.9074) acc_u 6.2500 (11.1161) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [75/75] time 0.421 (0.459) data 0.290 (0.327) loss_u loss_u 0.9023 (0.9069) acc_u 18.7500 (11.4583) lr 1.8181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1782
confident_label rate tensor(0.2280, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 715
clean true:554
clean false:161
clean_rate:0.7748251748251749
noisy true:800
noisy false:1621
after delete: len(clean_dataset) 715
after delete: len(noisy_dataset) 2421
epoch [42/200] batch [5/22] time 0.408 (0.430) data 0.278 (0.299) loss_x loss_x 1.6543 (1.4816) acc_x 56.2500 (60.0000) lr 1.8090e-03 eta 0:00:07
epoch [42/200] batch [10/22] time 0.393 (0.414) data 0.263 (0.283) loss_x loss_x 1.5264 (1.5143) acc_x 59.3750 (60.9375) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [15/22] time 0.581 (0.446) data 0.451 (0.316) loss_x loss_x 1.7627 (1.4717) acc_x 46.8750 (60.0000) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [20/22] time 0.416 (0.442) data 0.285 (0.311) loss_x loss_x 2.0020 (1.5169) acc_x 46.8750 (59.0625) lr 1.8090e-03 eta 0:00:00
epoch [42/200] batch [5/75] time 0.507 (0.453) data 0.377 (0.322) loss_u loss_u 0.9629 (0.9237) acc_u 6.2500 (10.0000) lr 1.8090e-03 eta 0:00:31
epoch [42/200] batch [10/75] time 0.414 (0.457) data 0.284 (0.326) loss_u loss_u 0.9053 (0.9172) acc_u 9.3750 (10.0000) lr 1.8090e-03 eta 0:00:29
epoch [42/200] batch [15/75] time 0.423 (0.455) data 0.290 (0.324) loss_u loss_u 0.8809 (0.9094) acc_u 12.5000 (10.8333) lr 1.8090e-03 eta 0:00:27
epoch [42/200] batch [20/75] time 0.377 (0.455) data 0.245 (0.324) loss_u loss_u 0.9194 (0.9090) acc_u 6.2500 (10.4688) lr 1.8090e-03 eta 0:00:25
epoch [42/200] batch [25/75] time 0.528 (0.457) data 0.396 (0.326) loss_u loss_u 0.8921 (0.9114) acc_u 9.3750 (10.3750) lr 1.8090e-03 eta 0:00:22
epoch [42/200] batch [30/75] time 0.418 (0.459) data 0.285 (0.327) loss_u loss_u 0.9033 (0.9074) acc_u 9.3750 (11.1458) lr 1.8090e-03 eta 0:00:20
epoch [42/200] batch [35/75] time 0.382 (0.456) data 0.250 (0.325) loss_u loss_u 0.9312 (0.9071) acc_u 9.3750 (11.1607) lr 1.8090e-03 eta 0:00:18
epoch [42/200] batch [40/75] time 0.421 (0.457) data 0.290 (0.326) loss_u loss_u 0.8730 (0.9015) acc_u 15.6250 (11.9531) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [45/75] time 0.369 (0.453) data 0.239 (0.322) loss_u loss_u 0.8701 (0.8971) acc_u 18.7500 (12.7083) lr 1.8090e-03 eta 0:00:13
epoch [42/200] batch [50/75] time 0.472 (0.454) data 0.337 (0.322) loss_u loss_u 0.8784 (0.8972) acc_u 12.5000 (12.5625) lr 1.8090e-03 eta 0:00:11
epoch [42/200] batch [55/75] time 0.403 (0.458) data 0.271 (0.327) loss_u loss_u 0.9082 (0.8990) acc_u 9.3750 (12.3295) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [60/75] time 0.379 (0.456) data 0.246 (0.325) loss_u loss_u 0.8599 (0.8979) acc_u 18.7500 (12.6042) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [65/75] time 0.408 (0.461) data 0.276 (0.330) loss_u loss_u 0.8657 (0.8975) acc_u 18.7500 (12.7404) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [70/75] time 0.512 (0.460) data 0.379 (0.328) loss_u loss_u 0.8550 (0.8976) acc_u 18.7500 (12.7679) lr 1.8090e-03 eta 0:00:02
epoch [42/200] batch [75/75] time 0.653 (0.463) data 0.521 (0.331) loss_u loss_u 0.8687 (0.8978) acc_u 9.3750 (12.7083) lr 1.8090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1734
confident_label rate tensor(0.2436, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 764
clean true:612
clean false:152
clean_rate:0.8010471204188482
noisy true:790
noisy false:1582
after delete: len(clean_dataset) 764
after delete: len(noisy_dataset) 2372
epoch [43/200] batch [5/23] time 0.520 (0.452) data 0.389 (0.321) loss_x loss_x 1.0625 (1.3590) acc_x 78.1250 (69.3750) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [10/23] time 0.513 (0.467) data 0.383 (0.336) loss_x loss_x 1.2734 (1.4263) acc_x 68.7500 (65.0000) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [15/23] time 0.500 (0.479) data 0.370 (0.348) loss_x loss_x 1.8486 (1.4504) acc_x 56.2500 (62.9167) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [20/23] time 0.404 (0.479) data 0.274 (0.348) loss_x loss_x 1.3770 (1.4247) acc_x 56.2500 (62.6562) lr 1.7997e-03 eta 0:00:01
epoch [43/200] batch [5/74] time 0.453 (0.472) data 0.322 (0.341) loss_u loss_u 0.9106 (0.8979) acc_u 12.5000 (13.1250) lr 1.7997e-03 eta 0:00:32
epoch [43/200] batch [10/74] time 0.441 (0.468) data 0.309 (0.337) loss_u loss_u 0.8940 (0.9046) acc_u 12.5000 (13.7500) lr 1.7997e-03 eta 0:00:29
epoch [43/200] batch [15/74] time 0.362 (0.465) data 0.230 (0.334) loss_u loss_u 0.8359 (0.9087) acc_u 21.8750 (12.5000) lr 1.7997e-03 eta 0:00:27
epoch [43/200] batch [20/74] time 0.462 (0.467) data 0.331 (0.336) loss_u loss_u 0.8735 (0.9062) acc_u 18.7500 (12.5000) lr 1.7997e-03 eta 0:00:25
epoch [43/200] batch [25/74] time 0.541 (0.474) data 0.408 (0.343) loss_u loss_u 0.9126 (0.9092) acc_u 9.3750 (11.6250) lr 1.7997e-03 eta 0:00:23
epoch [43/200] batch [30/74] time 0.554 (0.475) data 0.421 (0.344) loss_u loss_u 0.9536 (0.9084) acc_u 3.1250 (11.1458) lr 1.7997e-03 eta 0:00:20
epoch [43/200] batch [35/74] time 0.422 (0.473) data 0.290 (0.342) loss_u loss_u 0.8892 (0.9127) acc_u 18.7500 (10.6250) lr 1.7997e-03 eta 0:00:18
epoch [43/200] batch [40/74] time 0.453 (0.471) data 0.321 (0.340) loss_u loss_u 0.9658 (0.9099) acc_u 0.0000 (10.8594) lr 1.7997e-03 eta 0:00:16
epoch [43/200] batch [45/74] time 0.483 (0.469) data 0.351 (0.338) loss_u loss_u 0.8716 (0.9111) acc_u 15.6250 (10.4861) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [50/74] time 0.510 (0.469) data 0.378 (0.337) loss_u loss_u 0.9189 (0.9092) acc_u 12.5000 (11.0625) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [55/74] time 0.432 (0.468) data 0.301 (0.336) loss_u loss_u 0.9521 (0.9076) acc_u 6.2500 (11.4205) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [60/74] time 0.396 (0.464) data 0.265 (0.333) loss_u loss_u 0.9731 (0.9065) acc_u 3.1250 (11.6146) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [65/74] time 0.389 (0.460) data 0.258 (0.328) loss_u loss_u 0.8276 (0.9052) acc_u 28.1250 (11.9231) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [70/74] time 0.473 (0.460) data 0.342 (0.328) loss_u loss_u 0.9150 (0.9068) acc_u 12.5000 (11.6964) lr 1.7997e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1801
confident_label rate tensor(0.2280, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 715
clean true:557
clean false:158
clean_rate:0.779020979020979
noisy true:778
noisy false:1643
after delete: len(clean_dataset) 715
after delete: len(noisy_dataset) 2421
epoch [44/200] batch [5/22] time 0.493 (0.503) data 0.362 (0.371) loss_x loss_x 1.4648 (1.3820) acc_x 53.1250 (58.7500) lr 1.7902e-03 eta 0:00:08
epoch [44/200] batch [10/22] time 0.428 (0.515) data 0.297 (0.384) loss_x loss_x 1.8887 (1.4682) acc_x 50.0000 (59.6875) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [15/22] time 0.361 (0.498) data 0.231 (0.367) loss_x loss_x 1.4072 (1.4365) acc_x 68.7500 (61.6667) lr 1.7902e-03 eta 0:00:03
epoch [44/200] batch [20/22] time 0.366 (0.491) data 0.235 (0.360) loss_x loss_x 1.4287 (1.4462) acc_x 68.7500 (62.8125) lr 1.7902e-03 eta 0:00:00
epoch [44/200] batch [5/75] time 0.546 (0.477) data 0.411 (0.346) loss_u loss_u 0.9468 (0.9193) acc_u 3.1250 (10.6250) lr 1.7902e-03 eta 0:00:33
epoch [44/200] batch [10/75] time 0.456 (0.475) data 0.323 (0.344) loss_u loss_u 0.9546 (0.9231) acc_u 6.2500 (10.3125) lr 1.7902e-03 eta 0:00:30
epoch [44/200] batch [15/75] time 0.490 (0.478) data 0.358 (0.347) loss_u loss_u 0.8486 (0.9160) acc_u 25.0000 (11.2500) lr 1.7902e-03 eta 0:00:28
epoch [44/200] batch [20/75] time 0.345 (0.469) data 0.213 (0.337) loss_u loss_u 0.8916 (0.9083) acc_u 15.6250 (12.3438) lr 1.7902e-03 eta 0:00:25
epoch [44/200] batch [25/75] time 0.396 (0.462) data 0.264 (0.331) loss_u loss_u 0.8901 (0.9048) acc_u 18.7500 (12.6250) lr 1.7902e-03 eta 0:00:23
epoch [44/200] batch [30/75] time 0.403 (0.462) data 0.272 (0.330) loss_u loss_u 0.8823 (0.9034) acc_u 9.3750 (12.5000) lr 1.7902e-03 eta 0:00:20
epoch [44/200] batch [35/75] time 0.413 (0.457) data 0.281 (0.325) loss_u loss_u 0.8633 (0.9023) acc_u 15.6250 (12.5000) lr 1.7902e-03 eta 0:00:18
epoch [44/200] batch [40/75] time 0.413 (0.463) data 0.282 (0.331) loss_u loss_u 0.9058 (0.9026) acc_u 12.5000 (12.5000) lr 1.7902e-03 eta 0:00:16
epoch [44/200] batch [45/75] time 0.409 (0.464) data 0.278 (0.333) loss_u loss_u 0.9531 (0.9031) acc_u 9.3750 (12.2222) lr 1.7902e-03 eta 0:00:13
epoch [44/200] batch [50/75] time 0.392 (0.463) data 0.261 (0.331) loss_u loss_u 0.8760 (0.9027) acc_u 18.7500 (12.3750) lr 1.7902e-03 eta 0:00:11
epoch [44/200] batch [55/75] time 0.474 (0.465) data 0.340 (0.333) loss_u loss_u 0.9385 (0.9016) acc_u 9.3750 (12.5000) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [60/75] time 0.428 (0.465) data 0.296 (0.333) loss_u loss_u 0.8345 (0.8976) acc_u 25.0000 (13.0729) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [65/75] time 0.446 (0.466) data 0.314 (0.334) loss_u loss_u 0.9004 (0.8979) acc_u 9.3750 (13.1731) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [70/75] time 0.493 (0.466) data 0.362 (0.335) loss_u loss_u 0.8818 (0.8996) acc_u 15.6250 (12.9018) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [75/75] time 0.483 (0.465) data 0.352 (0.333) loss_u loss_u 0.8926 (0.9007) acc_u 18.7500 (12.9167) lr 1.7902e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1739
confident_label rate tensor(0.2455, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 770
clean true:604
clean false:166
clean_rate:0.7844155844155845
noisy true:793
noisy false:1573
after delete: len(clean_dataset) 770
after delete: len(noisy_dataset) 2366
epoch [45/200] batch [5/24] time 0.672 (0.497) data 0.541 (0.366) loss_x loss_x 1.4355 (1.5359) acc_x 56.2500 (63.7500) lr 1.7804e-03 eta 0:00:09
epoch [45/200] batch [10/24] time 0.508 (0.486) data 0.377 (0.354) loss_x loss_x 1.1406 (1.4476) acc_x 68.7500 (60.9375) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [15/24] time 0.567 (0.478) data 0.436 (0.347) loss_x loss_x 1.5234 (1.3704) acc_x 62.5000 (62.9167) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [20/24] time 0.427 (0.481) data 0.297 (0.350) loss_x loss_x 1.5127 (1.4102) acc_x 56.2500 (61.4062) lr 1.7804e-03 eta 0:00:01
epoch [45/200] batch [5/73] time 0.372 (0.468) data 0.240 (0.337) loss_u loss_u 0.9346 (0.8806) acc_u 6.2500 (15.6250) lr 1.7804e-03 eta 0:00:31
epoch [45/200] batch [10/73] time 0.390 (0.467) data 0.259 (0.335) loss_u loss_u 0.9033 (0.8900) acc_u 9.3750 (14.0625) lr 1.7804e-03 eta 0:00:29
epoch [45/200] batch [15/73] time 0.384 (0.462) data 0.251 (0.331) loss_u loss_u 0.9380 (0.9067) acc_u 6.2500 (11.6667) lr 1.7804e-03 eta 0:00:26
epoch [45/200] batch [20/73] time 0.595 (0.462) data 0.464 (0.331) loss_u loss_u 0.9316 (0.9078) acc_u 6.2500 (11.4062) lr 1.7804e-03 eta 0:00:24
epoch [45/200] batch [25/73] time 0.409 (0.457) data 0.278 (0.326) loss_u loss_u 0.8340 (0.9009) acc_u 21.8750 (12.1250) lr 1.7804e-03 eta 0:00:21
epoch [45/200] batch [30/73] time 0.516 (0.462) data 0.385 (0.331) loss_u loss_u 0.8823 (0.9001) acc_u 12.5000 (12.1875) lr 1.7804e-03 eta 0:00:19
epoch [45/200] batch [35/73] time 0.538 (0.463) data 0.406 (0.331) loss_u loss_u 0.8789 (0.9036) acc_u 12.5000 (11.6071) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [40/73] time 0.407 (0.465) data 0.276 (0.334) loss_u loss_u 0.9634 (0.9084) acc_u 6.2500 (11.1719) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [45/73] time 0.401 (0.466) data 0.270 (0.335) loss_u loss_u 0.9463 (0.9114) acc_u 9.3750 (10.9722) lr 1.7804e-03 eta 0:00:13
epoch [45/200] batch [50/73] time 0.412 (0.463) data 0.281 (0.332) loss_u loss_u 0.8564 (0.9084) acc_u 18.7500 (11.1875) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [55/73] time 0.518 (0.464) data 0.386 (0.332) loss_u loss_u 0.8877 (0.9093) acc_u 15.6250 (10.7386) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [60/73] time 0.473 (0.467) data 0.342 (0.335) loss_u loss_u 0.7983 (0.9090) acc_u 25.0000 (10.9896) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [65/73] time 0.524 (0.465) data 0.394 (0.334) loss_u loss_u 0.9565 (0.9091) acc_u 3.1250 (10.9615) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [70/73] time 0.414 (0.463) data 0.282 (0.332) loss_u loss_u 0.9155 (0.9101) acc_u 9.3750 (10.7589) lr 1.7804e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1764
confident_label rate tensor(0.2360, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 740
clean true:599
clean false:141
clean_rate:0.8094594594594594
noisy true:773
noisy false:1623
after delete: len(clean_dataset) 740
after delete: len(noisy_dataset) 2396
epoch [46/200] batch [5/23] time 0.348 (0.408) data 0.217 (0.277) loss_x loss_x 2.1699 (1.5385) acc_x 56.2500 (61.8750) lr 1.7705e-03 eta 0:00:07
epoch [46/200] batch [10/23] time 0.523 (0.451) data 0.393 (0.320) loss_x loss_x 0.8350 (1.4125) acc_x 75.0000 (64.3750) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [15/23] time 0.498 (0.462) data 0.367 (0.332) loss_x loss_x 1.1006 (1.4248) acc_x 68.7500 (63.3333) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [20/23] time 0.374 (0.468) data 0.243 (0.338) loss_x loss_x 1.2754 (1.4494) acc_x 71.8750 (62.5000) lr 1.7705e-03 eta 0:00:01
epoch [46/200] batch [5/74] time 0.514 (0.461) data 0.382 (0.330) loss_u loss_u 0.9341 (0.9225) acc_u 9.3750 (10.0000) lr 1.7705e-03 eta 0:00:31
epoch [46/200] batch [10/74] time 0.444 (0.464) data 0.312 (0.333) loss_u loss_u 0.8926 (0.9186) acc_u 15.6250 (10.3125) lr 1.7705e-03 eta 0:00:29
epoch [46/200] batch [15/74] time 0.493 (0.466) data 0.361 (0.335) loss_u loss_u 0.9502 (0.9143) acc_u 3.1250 (10.8333) lr 1.7705e-03 eta 0:00:27
epoch [46/200] batch [20/74] time 0.409 (0.463) data 0.277 (0.332) loss_u loss_u 0.9546 (0.9146) acc_u 9.3750 (10.7812) lr 1.7705e-03 eta 0:00:25
epoch [46/200] batch [25/74] time 0.434 (0.458) data 0.303 (0.327) loss_u loss_u 0.9077 (0.9116) acc_u 9.3750 (11.1250) lr 1.7705e-03 eta 0:00:22
epoch [46/200] batch [30/74] time 0.442 (0.458) data 0.311 (0.326) loss_u loss_u 0.9751 (0.9115) acc_u 6.2500 (11.0417) lr 1.7705e-03 eta 0:00:20
epoch [46/200] batch [35/74] time 0.374 (0.455) data 0.242 (0.324) loss_u loss_u 0.9019 (0.9126) acc_u 12.5000 (10.8036) lr 1.7705e-03 eta 0:00:17
epoch [46/200] batch [40/74] time 0.510 (0.456) data 0.380 (0.325) loss_u loss_u 0.9399 (0.9127) acc_u 6.2500 (10.3125) lr 1.7705e-03 eta 0:00:15
epoch [46/200] batch [45/74] time 0.519 (0.457) data 0.388 (0.326) loss_u loss_u 0.9351 (0.9133) acc_u 6.2500 (10.2778) lr 1.7705e-03 eta 0:00:13
epoch [46/200] batch [50/74] time 0.408 (0.461) data 0.275 (0.330) loss_u loss_u 0.8848 (0.9115) acc_u 9.3750 (10.6250) lr 1.7705e-03 eta 0:00:11
epoch [46/200] batch [55/74] time 0.439 (0.460) data 0.308 (0.329) loss_u loss_u 0.9238 (0.9107) acc_u 9.3750 (10.6818) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [60/74] time 0.414 (0.461) data 0.283 (0.329) loss_u loss_u 0.8540 (0.9108) acc_u 21.8750 (10.8854) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [65/74] time 0.478 (0.462) data 0.348 (0.331) loss_u loss_u 0.8628 (0.9097) acc_u 15.6250 (11.0577) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [70/74] time 0.503 (0.462) data 0.371 (0.330) loss_u loss_u 0.8921 (0.9092) acc_u 12.5000 (11.2500) lr 1.7705e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1762
confident_label rate tensor(0.2376, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 745
clean true:591
clean false:154
clean_rate:0.7932885906040269
noisy true:783
noisy false:1608
after delete: len(clean_dataset) 745
after delete: len(noisy_dataset) 2391
epoch [47/200] batch [5/23] time 0.647 (0.485) data 0.516 (0.354) loss_x loss_x 1.1436 (1.3387) acc_x 68.7500 (67.5000) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [10/23] time 0.344 (0.447) data 0.213 (0.316) loss_x loss_x 1.5459 (1.4431) acc_x 62.5000 (63.4375) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [15/23] time 0.531 (0.465) data 0.400 (0.334) loss_x loss_x 1.4482 (1.4144) acc_x 62.5000 (63.9583) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [20/23] time 0.458 (0.476) data 0.327 (0.345) loss_x loss_x 1.0107 (1.3431) acc_x 65.6250 (65.6250) lr 1.7604e-03 eta 0:00:01
epoch [47/200] batch [5/74] time 0.579 (0.477) data 0.447 (0.346) loss_u loss_u 0.9414 (0.9286) acc_u 9.3750 (8.1250) lr 1.7604e-03 eta 0:00:32
epoch [47/200] batch [10/74] time 0.443 (0.466) data 0.311 (0.335) loss_u loss_u 0.8555 (0.9135) acc_u 21.8750 (11.2500) lr 1.7604e-03 eta 0:00:29
epoch [47/200] batch [15/74] time 0.402 (0.465) data 0.272 (0.334) loss_u loss_u 0.8916 (0.9028) acc_u 15.6250 (12.5000) lr 1.7604e-03 eta 0:00:27
epoch [47/200] batch [20/74] time 0.410 (0.461) data 0.278 (0.329) loss_u loss_u 0.8984 (0.9048) acc_u 21.8750 (12.9688) lr 1.7604e-03 eta 0:00:24
epoch [47/200] batch [25/74] time 0.588 (0.463) data 0.457 (0.332) loss_u loss_u 0.9019 (0.9020) acc_u 9.3750 (13.2500) lr 1.7604e-03 eta 0:00:22
epoch [47/200] batch [30/74] time 0.436 (0.460) data 0.304 (0.329) loss_u loss_u 0.8789 (0.9007) acc_u 12.5000 (13.0208) lr 1.7604e-03 eta 0:00:20
epoch [47/200] batch [35/74] time 0.397 (0.456) data 0.265 (0.324) loss_u loss_u 0.8276 (0.8998) acc_u 25.0000 (13.2143) lr 1.7604e-03 eta 0:00:17
epoch [47/200] batch [40/74] time 0.623 (0.458) data 0.491 (0.326) loss_u loss_u 0.9019 (0.9037) acc_u 12.5000 (12.3438) lr 1.7604e-03 eta 0:00:15
epoch [47/200] batch [45/74] time 0.550 (0.457) data 0.419 (0.325) loss_u loss_u 0.9038 (0.9040) acc_u 12.5000 (12.2917) lr 1.7604e-03 eta 0:00:13
epoch [47/200] batch [50/74] time 0.350 (0.458) data 0.218 (0.327) loss_u loss_u 0.9092 (0.9073) acc_u 9.3750 (11.8750) lr 1.7604e-03 eta 0:00:11
epoch [47/200] batch [55/74] time 0.449 (0.457) data 0.317 (0.326) loss_u loss_u 0.9707 (0.9091) acc_u 0.0000 (11.5341) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [60/74] time 0.450 (0.460) data 0.320 (0.329) loss_u loss_u 0.9312 (0.9060) acc_u 6.2500 (12.0312) lr 1.7604e-03 eta 0:00:06
epoch [47/200] batch [65/74] time 0.434 (0.458) data 0.302 (0.326) loss_u loss_u 0.9463 (0.9076) acc_u 3.1250 (11.5385) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [70/74] time 0.410 (0.453) data 0.279 (0.322) loss_u loss_u 0.9409 (0.9085) acc_u 3.1250 (11.2946) lr 1.7604e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1790
confident_label rate tensor(0.2315, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 726
clean true:560
clean false:166
clean_rate:0.7713498622589532
noisy true:786
noisy false:1624
after delete: len(clean_dataset) 726
after delete: len(noisy_dataset) 2410
epoch [48/200] batch [5/22] time 0.502 (0.484) data 0.371 (0.353) loss_x loss_x 1.9756 (1.4736) acc_x 46.8750 (61.8750) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [10/22] time 0.454 (0.467) data 0.323 (0.336) loss_x loss_x 0.9429 (1.3619) acc_x 75.0000 (65.9375) lr 1.7501e-03 eta 0:00:05
epoch [48/200] batch [15/22] time 0.625 (0.472) data 0.493 (0.341) loss_x loss_x 1.6055 (1.3548) acc_x 59.3750 (65.6250) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [20/22] time 0.618 (0.477) data 0.488 (0.346) loss_x loss_x 1.5283 (1.3427) acc_x 53.1250 (65.4688) lr 1.7501e-03 eta 0:00:00
epoch [48/200] batch [5/75] time 0.490 (0.476) data 0.359 (0.345) loss_u loss_u 0.8687 (0.8739) acc_u 15.6250 (15.6250) lr 1.7501e-03 eta 0:00:33
epoch [48/200] batch [10/75] time 0.377 (0.465) data 0.245 (0.334) loss_u loss_u 0.8730 (0.8828) acc_u 12.5000 (14.3750) lr 1.7501e-03 eta 0:00:30
epoch [48/200] batch [15/75] time 0.550 (0.471) data 0.418 (0.340) loss_u loss_u 0.8311 (0.8838) acc_u 21.8750 (14.3750) lr 1.7501e-03 eta 0:00:28
epoch [48/200] batch [20/75] time 0.328 (0.466) data 0.197 (0.335) loss_u loss_u 0.8809 (0.8893) acc_u 18.7500 (13.9062) lr 1.7501e-03 eta 0:00:25
epoch [48/200] batch [25/75] time 0.324 (0.456) data 0.191 (0.325) loss_u loss_u 0.8989 (0.8931) acc_u 12.5000 (13.0000) lr 1.7501e-03 eta 0:00:22
epoch [48/200] batch [30/75] time 0.460 (0.460) data 0.328 (0.329) loss_u loss_u 0.8452 (0.8873) acc_u 18.7500 (13.7500) lr 1.7501e-03 eta 0:00:20
epoch [48/200] batch [35/75] time 0.764 (0.467) data 0.632 (0.336) loss_u loss_u 0.9458 (0.8937) acc_u 3.1250 (12.6786) lr 1.7501e-03 eta 0:00:18
epoch [48/200] batch [40/75] time 0.449 (0.465) data 0.317 (0.334) loss_u loss_u 0.8350 (0.8885) acc_u 21.8750 (13.2031) lr 1.7501e-03 eta 0:00:16
epoch [48/200] batch [45/75] time 0.443 (0.467) data 0.311 (0.336) loss_u loss_u 0.7886 (0.8872) acc_u 28.1250 (13.4722) lr 1.7501e-03 eta 0:00:14
epoch [48/200] batch [50/75] time 0.517 (0.468) data 0.386 (0.337) loss_u loss_u 0.9688 (0.8896) acc_u 3.1250 (13.3750) lr 1.7501e-03 eta 0:00:11
epoch [48/200] batch [55/75] time 0.550 (0.469) data 0.418 (0.338) loss_u loss_u 0.9214 (0.8908) acc_u 12.5000 (13.2386) lr 1.7501e-03 eta 0:00:09
epoch [48/200] batch [60/75] time 0.422 (0.465) data 0.290 (0.334) loss_u loss_u 0.9482 (0.8929) acc_u 6.2500 (12.9688) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [65/75] time 0.495 (0.467) data 0.364 (0.336) loss_u loss_u 0.9409 (0.8956) acc_u 9.3750 (12.6923) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [70/75] time 0.433 (0.469) data 0.303 (0.338) loss_u loss_u 0.8755 (0.8962) acc_u 21.8750 (12.7232) lr 1.7501e-03 eta 0:00:02
epoch [48/200] batch [75/75] time 0.447 (0.469) data 0.317 (0.338) loss_u loss_u 0.9053 (0.8967) acc_u 9.3750 (12.6250) lr 1.7501e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1756
confident_label rate tensor(0.2360, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 740
clean true:574
clean false:166
clean_rate:0.7756756756756756
noisy true:806
noisy false:1590
after delete: len(clean_dataset) 740
after delete: len(noisy_dataset) 2396
epoch [49/200] batch [5/23] time 0.415 (0.509) data 0.284 (0.378) loss_x loss_x 1.1055 (1.2663) acc_x 75.0000 (70.6250) lr 1.7396e-03 eta 0:00:09
epoch [49/200] batch [10/23] time 0.377 (0.487) data 0.247 (0.356) loss_x loss_x 1.6816 (1.3055) acc_x 56.2500 (68.1250) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [15/23] time 0.378 (0.474) data 0.248 (0.343) loss_x loss_x 1.6631 (1.3431) acc_x 53.1250 (66.6667) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [20/23] time 0.643 (0.480) data 0.513 (0.350) loss_x loss_x 1.2949 (1.3511) acc_x 59.3750 (65.7812) lr 1.7396e-03 eta 0:00:01
epoch [49/200] batch [5/74] time 0.811 (0.493) data 0.680 (0.362) loss_u loss_u 0.8354 (0.8841) acc_u 25.0000 (15.0000) lr 1.7396e-03 eta 0:00:33
epoch [49/200] batch [10/74] time 0.387 (0.489) data 0.256 (0.358) loss_u loss_u 0.8872 (0.8808) acc_u 9.3750 (14.6875) lr 1.7396e-03 eta 0:00:31
epoch [49/200] batch [15/74] time 0.432 (0.483) data 0.301 (0.352) loss_u loss_u 0.9536 (0.8901) acc_u 6.2500 (13.9583) lr 1.7396e-03 eta 0:00:28
epoch [49/200] batch [20/74] time 0.319 (0.475) data 0.189 (0.344) loss_u loss_u 0.8770 (0.8961) acc_u 15.6250 (12.9688) lr 1.7396e-03 eta 0:00:25
epoch [49/200] batch [25/74] time 0.327 (0.471) data 0.196 (0.340) loss_u loss_u 0.8931 (0.8931) acc_u 12.5000 (13.5000) lr 1.7396e-03 eta 0:00:23
epoch [49/200] batch [30/74] time 0.427 (0.478) data 0.295 (0.347) loss_u loss_u 0.9302 (0.8975) acc_u 3.1250 (12.9167) lr 1.7396e-03 eta 0:00:21
epoch [49/200] batch [35/74] time 0.377 (0.474) data 0.247 (0.343) loss_u loss_u 0.8779 (0.8956) acc_u 15.6250 (13.2143) lr 1.7396e-03 eta 0:00:18
epoch [49/200] batch [40/74] time 0.426 (0.470) data 0.294 (0.339) loss_u loss_u 0.9634 (0.8953) acc_u 3.1250 (13.2812) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [45/74] time 0.423 (0.471) data 0.291 (0.340) loss_u loss_u 0.9282 (0.8968) acc_u 9.3750 (13.0556) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [50/74] time 0.338 (0.470) data 0.206 (0.339) loss_u loss_u 0.9375 (0.8977) acc_u 6.2500 (12.9375) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [55/74] time 0.393 (0.468) data 0.261 (0.337) loss_u loss_u 0.8770 (0.8956) acc_u 9.3750 (13.0682) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [60/74] time 0.408 (0.466) data 0.276 (0.334) loss_u loss_u 0.9277 (0.8969) acc_u 9.3750 (12.8125) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [65/74] time 0.511 (0.463) data 0.379 (0.331) loss_u loss_u 0.8149 (0.8959) acc_u 21.8750 (13.0769) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [70/74] time 0.387 (0.460) data 0.255 (0.328) loss_u loss_u 0.9199 (0.8967) acc_u 12.5000 (13.0804) lr 1.7396e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.2459, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 771
clean true:611
clean false:160
clean_rate:0.7924773022049286
noisy true:776
noisy false:1589
after delete: len(clean_dataset) 771
after delete: len(noisy_dataset) 2365
epoch [50/200] batch [5/24] time 0.499 (0.469) data 0.367 (0.337) loss_x loss_x 1.4707 (1.1805) acc_x 46.8750 (69.3750) lr 1.7290e-03 eta 0:00:08
epoch [50/200] batch [10/24] time 0.554 (0.501) data 0.422 (0.370) loss_x loss_x 1.3047 (1.2357) acc_x 68.7500 (65.9375) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [15/24] time 0.391 (0.480) data 0.260 (0.349) loss_x loss_x 1.4580 (1.2640) acc_x 71.8750 (66.6667) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [20/24] time 0.585 (0.481) data 0.455 (0.350) loss_x loss_x 1.5537 (1.3246) acc_x 53.1250 (65.3125) lr 1.7290e-03 eta 0:00:01
epoch [50/200] batch [5/73] time 0.348 (0.467) data 0.217 (0.336) loss_u loss_u 0.9546 (0.9019) acc_u 6.2500 (14.3750) lr 1.7290e-03 eta 0:00:31
epoch [50/200] batch [10/73] time 0.521 (0.466) data 0.391 (0.335) loss_u loss_u 0.8770 (0.9040) acc_u 21.8750 (13.7500) lr 1.7290e-03 eta 0:00:29
epoch [50/200] batch [15/73] time 0.485 (0.467) data 0.354 (0.336) loss_u loss_u 0.9097 (0.9056) acc_u 15.6250 (13.1250) lr 1.7290e-03 eta 0:00:27
epoch [50/200] batch [20/73] time 0.413 (0.462) data 0.283 (0.331) loss_u loss_u 0.9102 (0.9111) acc_u 15.6250 (11.8750) lr 1.7290e-03 eta 0:00:24
epoch [50/200] batch [25/73] time 0.437 (0.463) data 0.306 (0.332) loss_u loss_u 0.9165 (0.9066) acc_u 12.5000 (13.1250) lr 1.7290e-03 eta 0:00:22
epoch [50/200] batch [30/73] time 0.526 (0.463) data 0.396 (0.332) loss_u loss_u 0.9199 (0.9080) acc_u 9.3750 (12.5000) lr 1.7290e-03 eta 0:00:19
epoch [50/200] batch [35/73] time 0.326 (0.461) data 0.195 (0.330) loss_u loss_u 0.9097 (0.9094) acc_u 9.3750 (11.9643) lr 1.7290e-03 eta 0:00:17
epoch [50/200] batch [40/73] time 0.397 (0.458) data 0.266 (0.327) loss_u loss_u 0.8999 (0.9116) acc_u 12.5000 (11.2500) lr 1.7290e-03 eta 0:00:15
epoch [50/200] batch [45/73] time 0.436 (0.458) data 0.304 (0.327) loss_u loss_u 0.9355 (0.9148) acc_u 3.1250 (10.6250) lr 1.7290e-03 eta 0:00:12
epoch [50/200] batch [50/73] time 0.340 (0.453) data 0.208 (0.322) loss_u loss_u 0.8057 (0.9143) acc_u 28.1250 (10.6875) lr 1.7290e-03 eta 0:00:10
epoch [50/200] batch [55/73] time 0.601 (0.456) data 0.470 (0.325) loss_u loss_u 0.9023 (0.9149) acc_u 9.3750 (10.5114) lr 1.7290e-03 eta 0:00:08
epoch [50/200] batch [60/73] time 0.411 (0.456) data 0.280 (0.325) loss_u loss_u 0.9116 (0.9146) acc_u 12.5000 (10.5729) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [65/73] time 0.452 (0.454) data 0.321 (0.322) loss_u loss_u 0.9189 (0.9128) acc_u 15.6250 (10.8654) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [70/73] time 0.357 (0.455) data 0.226 (0.324) loss_u loss_u 0.9390 (0.9140) acc_u 6.2500 (10.5357) lr 1.7290e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1783
confident_label rate tensor(0.2286, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 717
clean true:560
clean false:157
clean_rate:0.7810320781032078
noisy true:793
noisy false:1626
after delete: len(clean_dataset) 717
after delete: len(noisy_dataset) 2419
epoch [51/200] batch [5/22] time 0.575 (0.448) data 0.444 (0.317) loss_x loss_x 1.4453 (1.1482) acc_x 59.3750 (73.7500) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [10/22] time 0.631 (0.474) data 0.501 (0.343) loss_x loss_x 1.0020 (1.2874) acc_x 84.3750 (70.3125) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [15/22] time 0.478 (0.464) data 0.347 (0.333) loss_x loss_x 1.1934 (1.2869) acc_x 68.7500 (69.3750) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [20/22] time 0.449 (0.471) data 0.318 (0.340) loss_x loss_x 1.0039 (1.3503) acc_x 75.0000 (67.5000) lr 1.7181e-03 eta 0:00:00
epoch [51/200] batch [5/75] time 0.462 (0.467) data 0.330 (0.337) loss_u loss_u 0.8872 (0.9045) acc_u 15.6250 (14.3750) lr 1.7181e-03 eta 0:00:32
epoch [51/200] batch [10/75] time 0.415 (0.467) data 0.284 (0.336) loss_u loss_u 0.9258 (0.8854) acc_u 9.3750 (16.5625) lr 1.7181e-03 eta 0:00:30
epoch [51/200] batch [15/75] time 0.398 (0.462) data 0.268 (0.331) loss_u loss_u 0.8940 (0.8914) acc_u 18.7500 (15.2083) lr 1.7181e-03 eta 0:00:27
epoch [51/200] batch [20/75] time 0.414 (0.461) data 0.282 (0.330) loss_u loss_u 0.8774 (0.8910) acc_u 18.7500 (15.3125) lr 1.7181e-03 eta 0:00:25
epoch [51/200] batch [25/75] time 0.424 (0.462) data 0.293 (0.330) loss_u loss_u 0.8857 (0.8965) acc_u 18.7500 (14.2500) lr 1.7181e-03 eta 0:00:23
epoch [51/200] batch [30/75] time 0.410 (0.463) data 0.278 (0.332) loss_u loss_u 0.9116 (0.8952) acc_u 9.3750 (14.0625) lr 1.7181e-03 eta 0:00:20
epoch [51/200] batch [35/75] time 0.446 (0.458) data 0.315 (0.327) loss_u loss_u 0.8472 (0.8960) acc_u 25.0000 (13.8393) lr 1.7181e-03 eta 0:00:18
epoch [51/200] batch [40/75] time 0.413 (0.461) data 0.281 (0.330) loss_u loss_u 0.9023 (0.8976) acc_u 9.3750 (13.3594) lr 1.7181e-03 eta 0:00:16
epoch [51/200] batch [45/75] time 0.472 (0.459) data 0.340 (0.327) loss_u loss_u 0.8765 (0.8986) acc_u 18.7500 (13.1944) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [50/75] time 0.478 (0.456) data 0.346 (0.324) loss_u loss_u 0.9121 (0.8980) acc_u 9.3750 (13.1875) lr 1.7181e-03 eta 0:00:11
epoch [51/200] batch [55/75] time 0.509 (0.457) data 0.377 (0.326) loss_u loss_u 0.8867 (0.8992) acc_u 9.3750 (12.8409) lr 1.7181e-03 eta 0:00:09
epoch [51/200] batch [60/75] time 0.466 (0.456) data 0.336 (0.325) loss_u loss_u 0.9199 (0.8971) acc_u 6.2500 (12.9688) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [65/75] time 0.450 (0.456) data 0.319 (0.325) loss_u loss_u 0.8115 (0.8971) acc_u 21.8750 (12.8846) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [70/75] time 0.441 (0.455) data 0.311 (0.323) loss_u loss_u 0.9336 (0.8961) acc_u 3.1250 (13.0357) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [75/75] time 0.489 (0.457) data 0.357 (0.326) loss_u loss_u 0.8916 (0.8954) acc_u 15.6250 (13.1250) lr 1.7181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1755
confident_label rate tensor(0.2436, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 764
clean true:588
clean false:176
clean_rate:0.7696335078534031
noisy true:793
noisy false:1579
after delete: len(clean_dataset) 764
after delete: len(noisy_dataset) 2372
epoch [52/200] batch [5/23] time 0.465 (0.524) data 0.335 (0.393) loss_x loss_x 1.1357 (1.5475) acc_x 71.8750 (63.1250) lr 1.7071e-03 eta 0:00:09
epoch [52/200] batch [10/23] time 0.419 (0.485) data 0.288 (0.355) loss_x loss_x 1.7334 (1.4405) acc_x 62.5000 (64.3750) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [15/23] time 0.433 (0.481) data 0.303 (0.351) loss_x loss_x 1.7529 (1.4692) acc_x 65.6250 (65.2083) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [20/23] time 0.399 (0.471) data 0.269 (0.340) loss_x loss_x 1.3760 (1.4761) acc_x 71.8750 (64.8438) lr 1.7071e-03 eta 0:00:01
epoch [52/200] batch [5/74] time 0.355 (0.451) data 0.224 (0.321) loss_u loss_u 0.8403 (0.8750) acc_u 21.8750 (16.2500) lr 1.7071e-03 eta 0:00:31
epoch [52/200] batch [10/74] time 0.415 (0.448) data 0.284 (0.317) loss_u loss_u 0.8784 (0.8871) acc_u 15.6250 (15.3125) lr 1.7071e-03 eta 0:00:28
epoch [52/200] batch [15/74] time 0.368 (0.444) data 0.235 (0.314) loss_u loss_u 0.9292 (0.8977) acc_u 6.2500 (13.5417) lr 1.7071e-03 eta 0:00:26
epoch [52/200] batch [20/74] time 0.526 (0.448) data 0.394 (0.318) loss_u loss_u 0.9229 (0.9011) acc_u 12.5000 (12.9688) lr 1.7071e-03 eta 0:00:24
epoch [52/200] batch [25/74] time 0.468 (0.449) data 0.337 (0.318) loss_u loss_u 0.9155 (0.8962) acc_u 6.2500 (13.5000) lr 1.7071e-03 eta 0:00:21
epoch [52/200] batch [30/74] time 0.596 (0.451) data 0.466 (0.320) loss_u loss_u 0.8887 (0.9008) acc_u 12.5000 (12.6042) lr 1.7071e-03 eta 0:00:19
epoch [52/200] batch [35/74] time 0.448 (0.456) data 0.317 (0.325) loss_u loss_u 0.8965 (0.9016) acc_u 15.6250 (12.6786) lr 1.7071e-03 eta 0:00:17
epoch [52/200] batch [40/74] time 0.391 (0.454) data 0.259 (0.323) loss_u loss_u 0.9087 (0.8993) acc_u 9.3750 (12.8906) lr 1.7071e-03 eta 0:00:15
epoch [52/200] batch [45/74] time 0.522 (0.453) data 0.390 (0.322) loss_u loss_u 0.9102 (0.8991) acc_u 15.6250 (12.8472) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [50/74] time 0.355 (0.454) data 0.224 (0.323) loss_u loss_u 0.9023 (0.9003) acc_u 15.6250 (12.8750) lr 1.7071e-03 eta 0:00:10
epoch [52/200] batch [55/74] time 0.467 (0.454) data 0.336 (0.324) loss_u loss_u 0.8354 (0.9006) acc_u 21.8750 (12.7273) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [60/74] time 0.511 (0.457) data 0.379 (0.326) loss_u loss_u 0.8804 (0.9017) acc_u 18.7500 (12.6042) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [65/74] time 0.455 (0.454) data 0.324 (0.323) loss_u loss_u 0.8789 (0.9027) acc_u 9.3750 (12.2115) lr 1.7071e-03 eta 0:00:04
epoch [52/200] batch [70/74] time 0.487 (0.453) data 0.356 (0.322) loss_u loss_u 0.8647 (0.9036) acc_u 15.6250 (12.1429) lr 1.7071e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1769
confident_label rate tensor(0.2433, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 763
clean true:598
clean false:165
clean_rate:0.783748361730013
noisy true:769
noisy false:1604
after delete: len(clean_dataset) 763
after delete: len(noisy_dataset) 2373
epoch [53/200] batch [5/23] time 0.367 (0.447) data 0.237 (0.316) loss_x loss_x 1.3037 (1.2535) acc_x 71.8750 (66.8750) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [10/23] time 0.491 (0.448) data 0.360 (0.317) loss_x loss_x 1.3789 (1.3722) acc_x 68.7500 (64.6875) lr 1.6959e-03 eta 0:00:05
epoch [53/200] batch [15/23] time 0.371 (0.442) data 0.240 (0.312) loss_x loss_x 1.3145 (1.3193) acc_x 68.7500 (64.7917) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [20/23] time 0.622 (0.456) data 0.492 (0.326) loss_x loss_x 1.4531 (1.3436) acc_x 62.5000 (64.3750) lr 1.6959e-03 eta 0:00:01
epoch [53/200] batch [5/74] time 0.440 (0.449) data 0.309 (0.318) loss_u loss_u 0.8872 (0.8768) acc_u 15.6250 (15.0000) lr 1.6959e-03 eta 0:00:30
epoch [53/200] batch [10/74] time 0.412 (0.447) data 0.281 (0.317) loss_u loss_u 0.9014 (0.8909) acc_u 15.6250 (14.0625) lr 1.6959e-03 eta 0:00:28
epoch [53/200] batch [15/74] time 0.405 (0.454) data 0.274 (0.323) loss_u loss_u 0.8892 (0.8931) acc_u 18.7500 (14.3750) lr 1.6959e-03 eta 0:00:26
epoch [53/200] batch [20/74] time 0.386 (0.455) data 0.254 (0.324) loss_u loss_u 0.9316 (0.8984) acc_u 9.3750 (13.5938) lr 1.6959e-03 eta 0:00:24
epoch [53/200] batch [25/74] time 0.487 (0.452) data 0.357 (0.321) loss_u loss_u 0.9155 (0.8991) acc_u 12.5000 (13.3750) lr 1.6959e-03 eta 0:00:22
epoch [53/200] batch [30/74] time 0.378 (0.449) data 0.246 (0.318) loss_u loss_u 0.9126 (0.9009) acc_u 18.7500 (13.4375) lr 1.6959e-03 eta 0:00:19
epoch [53/200] batch [35/74] time 0.457 (0.451) data 0.326 (0.320) loss_u loss_u 0.8960 (0.9011) acc_u 9.3750 (13.1250) lr 1.6959e-03 eta 0:00:17
epoch [53/200] batch [40/74] time 0.368 (0.446) data 0.238 (0.316) loss_u loss_u 0.8979 (0.9003) acc_u 18.7500 (13.2812) lr 1.6959e-03 eta 0:00:15
epoch [53/200] batch [45/74] time 0.502 (0.450) data 0.371 (0.319) loss_u loss_u 0.9614 (0.9007) acc_u 3.1250 (13.4722) lr 1.6959e-03 eta 0:00:13
epoch [53/200] batch [50/74] time 0.485 (0.447) data 0.353 (0.316) loss_u loss_u 0.8901 (0.9016) acc_u 12.5000 (13.4375) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [55/74] time 0.494 (0.449) data 0.363 (0.318) loss_u loss_u 0.8525 (0.8999) acc_u 12.5000 (13.4659) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [60/74] time 0.472 (0.449) data 0.342 (0.318) loss_u loss_u 0.8306 (0.8998) acc_u 15.6250 (13.3333) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [65/74] time 0.369 (0.451) data 0.238 (0.320) loss_u loss_u 0.9521 (0.9004) acc_u 6.2500 (13.1250) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [70/74] time 0.505 (0.451) data 0.374 (0.320) loss_u loss_u 0.9077 (0.9009) acc_u 15.6250 (12.9911) lr 1.6959e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1797
confident_label rate tensor(0.2385, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 748
clean true:592
clean false:156
clean_rate:0.7914438502673797
noisy true:747
noisy false:1641
after delete: len(clean_dataset) 748
after delete: len(noisy_dataset) 2388
epoch [54/200] batch [5/23] time 0.350 (0.453) data 0.219 (0.322) loss_x loss_x 1.1738 (1.3175) acc_x 71.8750 (66.2500) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [10/23] time 0.493 (0.458) data 0.362 (0.328) loss_x loss_x 1.5645 (1.3081) acc_x 56.2500 (65.3125) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [15/23] time 0.561 (0.493) data 0.430 (0.362) loss_x loss_x 2.1621 (1.3757) acc_x 40.6250 (63.3333) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [20/23] time 0.337 (0.475) data 0.204 (0.345) loss_x loss_x 1.9131 (1.3694) acc_x 65.6250 (64.6875) lr 1.6845e-03 eta 0:00:01
epoch [54/200] batch [5/74] time 0.611 (0.472) data 0.480 (0.341) loss_u loss_u 0.9102 (0.9172) acc_u 15.6250 (11.8750) lr 1.6845e-03 eta 0:00:32
epoch [54/200] batch [10/74] time 0.348 (0.469) data 0.218 (0.338) loss_u loss_u 0.8672 (0.9081) acc_u 25.0000 (13.7500) lr 1.6845e-03 eta 0:00:30
epoch [54/200] batch [15/74] time 0.453 (0.463) data 0.321 (0.332) loss_u loss_u 0.8174 (0.9032) acc_u 21.8750 (14.1667) lr 1.6845e-03 eta 0:00:27
epoch [54/200] batch [20/74] time 0.549 (0.469) data 0.418 (0.338) loss_u loss_u 0.8872 (0.9051) acc_u 18.7500 (13.7500) lr 1.6845e-03 eta 0:00:25
epoch [54/200] batch [25/74] time 0.427 (0.474) data 0.295 (0.343) loss_u loss_u 0.8706 (0.9042) acc_u 15.6250 (13.7500) lr 1.6845e-03 eta 0:00:23
epoch [54/200] batch [30/74] time 0.402 (0.469) data 0.270 (0.338) loss_u loss_u 0.8696 (0.9052) acc_u 12.5000 (13.3333) lr 1.6845e-03 eta 0:00:20
epoch [54/200] batch [35/74] time 0.529 (0.470) data 0.396 (0.339) loss_u loss_u 0.9390 (0.9049) acc_u 12.5000 (13.2143) lr 1.6845e-03 eta 0:00:18
epoch [54/200] batch [40/74] time 0.501 (0.470) data 0.369 (0.338) loss_u loss_u 0.8657 (0.9008) acc_u 15.6250 (13.7500) lr 1.6845e-03 eta 0:00:15
epoch [54/200] batch [45/74] time 0.472 (0.466) data 0.340 (0.334) loss_u loss_u 0.9219 (0.9021) acc_u 9.3750 (13.4028) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [50/74] time 0.502 (0.462) data 0.370 (0.330) loss_u loss_u 0.8823 (0.9012) acc_u 15.6250 (13.6250) lr 1.6845e-03 eta 0:00:11
epoch [54/200] batch [55/74] time 0.455 (0.460) data 0.324 (0.328) loss_u loss_u 0.8809 (0.9013) acc_u 15.6250 (13.3523) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [60/74] time 0.402 (0.460) data 0.271 (0.328) loss_u loss_u 0.9043 (0.9006) acc_u 9.3750 (13.2292) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [65/74] time 0.436 (0.461) data 0.304 (0.329) loss_u loss_u 0.9004 (0.9007) acc_u 9.3750 (13.0288) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [70/74] time 0.365 (0.458) data 0.233 (0.326) loss_u loss_u 0.9204 (0.9015) acc_u 9.3750 (12.9018) lr 1.6845e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1821
confident_label rate tensor(0.2302, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 722
clean true:560
clean false:162
clean_rate:0.775623268698061
noisy true:755
noisy false:1659
after delete: len(clean_dataset) 722
after delete: len(noisy_dataset) 2414
epoch [55/200] batch [5/22] time 0.440 (0.481) data 0.310 (0.350) loss_x loss_x 1.3760 (1.2982) acc_x 68.7500 (70.6250) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [10/22] time 0.410 (0.444) data 0.279 (0.314) loss_x loss_x 1.2881 (1.2942) acc_x 62.5000 (66.8750) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [15/22] time 0.487 (0.470) data 0.357 (0.339) loss_x loss_x 1.6836 (1.3398) acc_x 50.0000 (65.8333) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [20/22] time 0.452 (0.470) data 0.321 (0.339) loss_x loss_x 1.6846 (1.3351) acc_x 65.6250 (66.0938) lr 1.6730e-03 eta 0:00:00
epoch [55/200] batch [5/75] time 0.411 (0.458) data 0.280 (0.327) loss_u loss_u 0.8657 (0.8975) acc_u 18.7500 (13.1250) lr 1.6730e-03 eta 0:00:32
epoch [55/200] batch [10/75] time 0.401 (0.457) data 0.270 (0.326) loss_u loss_u 0.8853 (0.8983) acc_u 18.7500 (14.3750) lr 1.6730e-03 eta 0:00:29
epoch [55/200] batch [15/75] time 0.535 (0.464) data 0.405 (0.333) loss_u loss_u 0.9155 (0.8998) acc_u 9.3750 (12.9167) lr 1.6730e-03 eta 0:00:27
epoch [55/200] batch [20/75] time 0.376 (0.463) data 0.246 (0.332) loss_u loss_u 0.8857 (0.9018) acc_u 9.3750 (12.0312) lr 1.6730e-03 eta 0:00:25
epoch [55/200] batch [25/75] time 0.431 (0.466) data 0.301 (0.335) loss_u loss_u 0.8442 (0.8967) acc_u 21.8750 (12.5000) lr 1.6730e-03 eta 0:00:23
epoch [55/200] batch [30/75] time 0.520 (0.465) data 0.389 (0.334) loss_u loss_u 0.9082 (0.8961) acc_u 12.5000 (12.6042) lr 1.6730e-03 eta 0:00:20
epoch [55/200] batch [35/75] time 0.426 (0.471) data 0.294 (0.340) loss_u loss_u 0.9482 (0.8999) acc_u 3.1250 (12.1429) lr 1.6730e-03 eta 0:00:18
epoch [55/200] batch [40/75] time 0.382 (0.468) data 0.251 (0.336) loss_u loss_u 0.8813 (0.8979) acc_u 18.7500 (12.8125) lr 1.6730e-03 eta 0:00:16
epoch [55/200] batch [45/75] time 0.311 (0.463) data 0.180 (0.332) loss_u loss_u 0.8901 (0.8965) acc_u 9.3750 (12.7083) lr 1.6730e-03 eta 0:00:13
epoch [55/200] batch [50/75] time 0.382 (0.460) data 0.251 (0.329) loss_u loss_u 0.9258 (0.8979) acc_u 6.2500 (12.5000) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [55/75] time 0.436 (0.458) data 0.305 (0.327) loss_u loss_u 0.9082 (0.8994) acc_u 9.3750 (12.3295) lr 1.6730e-03 eta 0:00:09
epoch [55/200] batch [60/75] time 0.413 (0.465) data 0.281 (0.334) loss_u loss_u 0.8706 (0.8971) acc_u 15.6250 (12.7083) lr 1.6730e-03 eta 0:00:06
epoch [55/200] batch [65/75] time 0.626 (0.466) data 0.495 (0.335) loss_u loss_u 0.9390 (0.8980) acc_u 6.2500 (12.5481) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [70/75] time 0.469 (0.466) data 0.337 (0.335) loss_u loss_u 0.9434 (0.8983) acc_u 6.2500 (12.4554) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [75/75] time 0.397 (0.464) data 0.265 (0.333) loss_u loss_u 0.8774 (0.8978) acc_u 12.5000 (12.5000) lr 1.6730e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1709
confident_label rate tensor(0.2490, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 781
clean true:616
clean false:165
clean_rate:0.7887323943661971
noisy true:811
noisy false:1544
after delete: len(clean_dataset) 781
after delete: len(noisy_dataset) 2355
epoch [56/200] batch [5/24] time 0.535 (0.503) data 0.404 (0.372) loss_x loss_x 1.1914 (1.2092) acc_x 65.6250 (68.1250) lr 1.6613e-03 eta 0:00:09
epoch [56/200] batch [10/24] time 0.473 (0.475) data 0.343 (0.344) loss_x loss_x 1.2852 (1.3208) acc_x 59.3750 (61.8750) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [15/24] time 0.576 (0.498) data 0.444 (0.367) loss_x loss_x 0.7383 (1.2632) acc_x 78.1250 (65.0000) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [20/24] time 0.419 (0.480) data 0.288 (0.349) loss_x loss_x 1.8145 (1.3042) acc_x 56.2500 (64.6875) lr 1.6613e-03 eta 0:00:01
epoch [56/200] batch [5/73] time 0.364 (0.476) data 0.233 (0.345) loss_u loss_u 0.9111 (0.9185) acc_u 6.2500 (10.6250) lr 1.6613e-03 eta 0:00:32
epoch [56/200] batch [10/73] time 0.508 (0.474) data 0.376 (0.343) loss_u loss_u 0.8716 (0.9119) acc_u 15.6250 (11.2500) lr 1.6613e-03 eta 0:00:29
epoch [56/200] batch [15/73] time 0.473 (0.471) data 0.341 (0.340) loss_u loss_u 0.9541 (0.9135) acc_u 6.2500 (10.8333) lr 1.6613e-03 eta 0:00:27
epoch [56/200] batch [20/73] time 0.491 (0.471) data 0.361 (0.340) loss_u loss_u 0.9624 (0.9130) acc_u 3.1250 (10.9375) lr 1.6613e-03 eta 0:00:24
epoch [56/200] batch [25/73] time 0.462 (0.474) data 0.330 (0.343) loss_u loss_u 0.9316 (0.9084) acc_u 9.3750 (11.2500) lr 1.6613e-03 eta 0:00:22
epoch [56/200] batch [30/73] time 0.516 (0.476) data 0.384 (0.344) loss_u loss_u 0.8794 (0.9060) acc_u 15.6250 (11.6667) lr 1.6613e-03 eta 0:00:20
epoch [56/200] batch [35/73] time 0.502 (0.472) data 0.370 (0.341) loss_u loss_u 0.8911 (0.9045) acc_u 9.3750 (11.6964) lr 1.6613e-03 eta 0:00:17
epoch [56/200] batch [40/73] time 0.448 (0.467) data 0.317 (0.336) loss_u loss_u 0.8130 (0.9027) acc_u 21.8750 (11.7188) lr 1.6613e-03 eta 0:00:15
epoch [56/200] batch [45/73] time 0.404 (0.465) data 0.273 (0.333) loss_u loss_u 0.9473 (0.9032) acc_u 3.1250 (11.8750) lr 1.6613e-03 eta 0:00:13
epoch [56/200] batch [50/73] time 0.517 (0.463) data 0.386 (0.332) loss_u loss_u 0.9468 (0.9033) acc_u 6.2500 (11.9375) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [55/73] time 0.554 (0.464) data 0.423 (0.333) loss_u loss_u 0.9644 (0.9058) acc_u 0.0000 (11.4773) lr 1.6613e-03 eta 0:00:08
epoch [56/200] batch [60/73] time 0.437 (0.465) data 0.305 (0.333) loss_u loss_u 0.8965 (0.9069) acc_u 9.3750 (11.1979) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [65/73] time 0.520 (0.464) data 0.388 (0.333) loss_u loss_u 0.9019 (0.9063) acc_u 15.6250 (11.2500) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [70/73] time 0.403 (0.467) data 0.272 (0.335) loss_u loss_u 0.8872 (0.9072) acc_u 12.5000 (11.2054) lr 1.6613e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1787
confident_label rate tensor(0.2452, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 769
clean true:599
clean false:170
clean_rate:0.7789336801040312
noisy true:750
noisy false:1617
after delete: len(clean_dataset) 769
after delete: len(noisy_dataset) 2367
epoch [57/200] batch [5/24] time 0.470 (0.467) data 0.339 (0.336) loss_x loss_x 2.0957 (1.5297) acc_x 50.0000 (63.1250) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [10/24] time 0.412 (0.479) data 0.282 (0.348) loss_x loss_x 1.1455 (1.4056) acc_x 75.0000 (67.1875) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [15/24] time 0.398 (0.468) data 0.268 (0.337) loss_x loss_x 1.2549 (1.3840) acc_x 59.3750 (66.6667) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [20/24] time 0.490 (0.463) data 0.357 (0.333) loss_x loss_x 1.0576 (1.3932) acc_x 75.0000 (65.7812) lr 1.6494e-03 eta 0:00:01
epoch [57/200] batch [5/73] time 0.553 (0.455) data 0.423 (0.324) loss_u loss_u 0.8564 (0.8804) acc_u 12.5000 (13.1250) lr 1.6494e-03 eta 0:00:30
epoch [57/200] batch [10/73] time 0.449 (0.448) data 0.319 (0.318) loss_u loss_u 0.8853 (0.8889) acc_u 15.6250 (12.5000) lr 1.6494e-03 eta 0:00:28
epoch [57/200] batch [15/73] time 0.448 (0.455) data 0.317 (0.325) loss_u loss_u 0.8696 (0.8902) acc_u 12.5000 (12.5000) lr 1.6494e-03 eta 0:00:26
epoch [57/200] batch [20/73] time 0.520 (0.455) data 0.390 (0.325) loss_u loss_u 0.9292 (0.8984) acc_u 9.3750 (11.7188) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [25/73] time 0.419 (0.455) data 0.287 (0.325) loss_u loss_u 0.9097 (0.8924) acc_u 12.5000 (12.8750) lr 1.6494e-03 eta 0:00:21
epoch [57/200] batch [30/73] time 0.402 (0.456) data 0.271 (0.325) loss_u loss_u 0.9136 (0.8911) acc_u 12.5000 (13.2292) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [35/73] time 0.624 (0.464) data 0.493 (0.333) loss_u loss_u 0.9224 (0.8931) acc_u 9.3750 (13.2143) lr 1.6494e-03 eta 0:00:17
epoch [57/200] batch [40/73] time 0.486 (0.466) data 0.354 (0.335) loss_u loss_u 0.9219 (0.8956) acc_u 9.3750 (12.8906) lr 1.6494e-03 eta 0:00:15
epoch [57/200] batch [45/73] time 0.485 (0.465) data 0.354 (0.334) loss_u loss_u 0.8755 (0.8977) acc_u 12.5000 (12.4306) lr 1.6494e-03 eta 0:00:13
epoch [57/200] batch [50/73] time 0.387 (0.464) data 0.256 (0.333) loss_u loss_u 0.9238 (0.8981) acc_u 3.1250 (12.3125) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [55/73] time 0.402 (0.466) data 0.270 (0.335) loss_u loss_u 0.9307 (0.9009) acc_u 6.2500 (11.9886) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [60/73] time 0.470 (0.465) data 0.338 (0.334) loss_u loss_u 0.9297 (0.9003) acc_u 6.2500 (12.0833) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [65/73] time 0.554 (0.466) data 0.422 (0.335) loss_u loss_u 0.9414 (0.8992) acc_u 6.2500 (12.1154) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [70/73] time 0.424 (0.467) data 0.292 (0.336) loss_u loss_u 0.9248 (0.9000) acc_u 9.3750 (12.0089) lr 1.6494e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1795
confident_label rate tensor(0.2392, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 750
clean true:579
clean false:171
clean_rate:0.772
noisy true:762
noisy false:1624
after delete: len(clean_dataset) 750
after delete: len(noisy_dataset) 2386
epoch [58/200] batch [5/23] time 0.406 (0.519) data 0.273 (0.387) loss_x loss_x 1.0791 (1.3381) acc_x 78.1250 (63.7500) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [10/23] time 0.463 (0.488) data 0.333 (0.357) loss_x loss_x 1.9326 (1.3298) acc_x 56.2500 (64.3750) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [15/23] time 0.488 (0.482) data 0.358 (0.351) loss_x loss_x 1.1846 (1.2684) acc_x 59.3750 (65.0000) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [20/23] time 0.431 (0.473) data 0.302 (0.342) loss_x loss_x 2.0273 (1.3104) acc_x 56.2500 (63.9062) lr 1.6374e-03 eta 0:00:01
epoch [58/200] batch [5/74] time 0.574 (0.475) data 0.443 (0.344) loss_u loss_u 0.8799 (0.9023) acc_u 18.7500 (13.1250) lr 1.6374e-03 eta 0:00:32
epoch [58/200] batch [10/74] time 0.569 (0.475) data 0.438 (0.344) loss_u loss_u 0.9062 (0.9129) acc_u 9.3750 (10.3125) lr 1.6374e-03 eta 0:00:30
epoch [58/200] batch [15/74] time 0.500 (0.471) data 0.369 (0.340) loss_u loss_u 0.9155 (0.9199) acc_u 9.3750 (9.3750) lr 1.6374e-03 eta 0:00:27
epoch [58/200] batch [20/74] time 0.596 (0.468) data 0.465 (0.337) loss_u loss_u 0.9092 (0.9110) acc_u 12.5000 (10.9375) lr 1.6374e-03 eta 0:00:25
epoch [58/200] batch [25/74] time 0.658 (0.471) data 0.528 (0.340) loss_u loss_u 0.8467 (0.9075) acc_u 25.0000 (11.3750) lr 1.6374e-03 eta 0:00:23
epoch [58/200] batch [30/74] time 0.808 (0.478) data 0.676 (0.347) loss_u loss_u 0.8882 (0.9095) acc_u 15.6250 (11.2500) lr 1.6374e-03 eta 0:00:21
epoch [58/200] batch [35/74] time 0.459 (0.476) data 0.328 (0.345) loss_u loss_u 0.8237 (0.9031) acc_u 28.1250 (12.1429) lr 1.6374e-03 eta 0:00:18
epoch [58/200] batch [40/74] time 0.601 (0.485) data 0.469 (0.354) loss_u loss_u 0.8975 (0.9016) acc_u 9.3750 (12.1094) lr 1.6374e-03 eta 0:00:16
epoch [58/200] batch [45/74] time 0.516 (0.484) data 0.385 (0.353) loss_u loss_u 0.8623 (0.9012) acc_u 12.5000 (11.9444) lr 1.6374e-03 eta 0:00:14
epoch [58/200] batch [50/74] time 0.487 (0.485) data 0.356 (0.354) loss_u loss_u 0.9048 (0.9021) acc_u 15.6250 (11.9375) lr 1.6374e-03 eta 0:00:11
epoch [58/200] batch [55/74] time 0.358 (0.482) data 0.226 (0.350) loss_u loss_u 0.9561 (0.9027) acc_u 3.1250 (11.6477) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [60/74] time 0.404 (0.479) data 0.274 (0.347) loss_u loss_u 0.9209 (0.9021) acc_u 9.3750 (11.6667) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [65/74] time 0.425 (0.478) data 0.294 (0.346) loss_u loss_u 0.8989 (0.9016) acc_u 9.3750 (11.6346) lr 1.6374e-03 eta 0:00:04
epoch [58/200] batch [70/74] time 0.406 (0.475) data 0.275 (0.344) loss_u loss_u 0.8804 (0.9011) acc_u 12.5000 (11.7411) lr 1.6374e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1775
confident_label rate tensor(0.2372, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 744
clean true:572
clean false:172
clean_rate:0.7688172043010753
noisy true:789
noisy false:1603
after delete: len(clean_dataset) 744
after delete: len(noisy_dataset) 2392
epoch [59/200] batch [5/23] time 0.569 (0.478) data 0.438 (0.347) loss_x loss_x 1.4189 (1.1701) acc_x 65.6250 (69.3750) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [10/23] time 0.371 (0.436) data 0.241 (0.305) loss_x loss_x 1.4658 (1.3169) acc_x 65.6250 (67.5000) lr 1.6252e-03 eta 0:00:05
epoch [59/200] batch [15/23] time 0.489 (0.455) data 0.358 (0.325) loss_x loss_x 1.4902 (1.3302) acc_x 53.1250 (64.7917) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [20/23] time 0.428 (0.455) data 0.297 (0.325) loss_x loss_x 1.1396 (1.3343) acc_x 59.3750 (63.4375) lr 1.6252e-03 eta 0:00:01
epoch [59/200] batch [5/74] time 0.585 (0.467) data 0.455 (0.336) loss_u loss_u 0.9092 (0.8982) acc_u 9.3750 (13.1250) lr 1.6252e-03 eta 0:00:32
epoch [59/200] batch [10/74] time 0.383 (0.474) data 0.252 (0.344) loss_u loss_u 0.8696 (0.8980) acc_u 15.6250 (11.5625) lr 1.6252e-03 eta 0:00:30
epoch [59/200] batch [15/74] time 0.452 (0.474) data 0.322 (0.344) loss_u loss_u 0.8677 (0.8939) acc_u 18.7500 (12.7083) lr 1.6252e-03 eta 0:00:27
epoch [59/200] batch [20/74] time 0.556 (0.472) data 0.424 (0.341) loss_u loss_u 0.9180 (0.8999) acc_u 12.5000 (11.8750) lr 1.6252e-03 eta 0:00:25
epoch [59/200] batch [25/74] time 0.480 (0.478) data 0.350 (0.347) loss_u loss_u 0.9507 (0.9005) acc_u 6.2500 (11.6250) lr 1.6252e-03 eta 0:00:23
epoch [59/200] batch [30/74] time 0.489 (0.476) data 0.358 (0.345) loss_u loss_u 0.9370 (0.9001) acc_u 3.1250 (11.6667) lr 1.6252e-03 eta 0:00:20
epoch [59/200] batch [35/74] time 0.511 (0.476) data 0.380 (0.345) loss_u loss_u 0.9819 (0.9021) acc_u 0.0000 (11.6964) lr 1.6252e-03 eta 0:00:18
epoch [59/200] batch [40/74] time 0.445 (0.472) data 0.313 (0.341) loss_u loss_u 0.9253 (0.9006) acc_u 9.3750 (11.8750) lr 1.6252e-03 eta 0:00:16
epoch [59/200] batch [45/74] time 0.369 (0.467) data 0.237 (0.336) loss_u loss_u 0.9297 (0.9022) acc_u 6.2500 (11.7361) lr 1.6252e-03 eta 0:00:13
epoch [59/200] batch [50/74] time 0.440 (0.469) data 0.310 (0.338) loss_u loss_u 0.8955 (0.9010) acc_u 12.5000 (11.9375) lr 1.6252e-03 eta 0:00:11
epoch [59/200] batch [55/74] time 0.385 (0.466) data 0.255 (0.335) loss_u loss_u 0.9336 (0.9003) acc_u 3.1250 (11.9886) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [60/74] time 0.373 (0.466) data 0.242 (0.335) loss_u loss_u 0.9443 (0.9003) acc_u 9.3750 (12.1354) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [65/74] time 0.465 (0.466) data 0.335 (0.335) loss_u loss_u 0.8999 (0.8991) acc_u 15.6250 (12.3077) lr 1.6252e-03 eta 0:00:04
epoch [59/200] batch [70/74] time 0.514 (0.464) data 0.382 (0.333) loss_u loss_u 0.8853 (0.8980) acc_u 9.3750 (12.3661) lr 1.6252e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1754
confident_label rate tensor(0.2510, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 787
clean true:609
clean false:178
clean_rate:0.7738246505717916
noisy true:773
noisy false:1576
after delete: len(clean_dataset) 787
after delete: len(noisy_dataset) 2349
epoch [60/200] batch [5/24] time 0.475 (0.418) data 0.344 (0.287) loss_x loss_x 1.3828 (1.2877) acc_x 65.6250 (68.7500) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [10/24] time 0.468 (0.466) data 0.338 (0.335) loss_x loss_x 1.5459 (1.3426) acc_x 68.7500 (68.4375) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [15/24] time 0.537 (0.465) data 0.406 (0.334) loss_x loss_x 1.3789 (1.3315) acc_x 65.6250 (67.2917) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [20/24] time 0.414 (0.455) data 0.284 (0.325) loss_x loss_x 1.3076 (1.3534) acc_x 56.2500 (65.1562) lr 1.6129e-03 eta 0:00:01
epoch [60/200] batch [5/73] time 0.635 (0.456) data 0.504 (0.326) loss_u loss_u 0.8765 (0.8948) acc_u 9.3750 (11.2500) lr 1.6129e-03 eta 0:00:31
epoch [60/200] batch [10/73] time 0.460 (0.456) data 0.328 (0.325) loss_u loss_u 0.9185 (0.8999) acc_u 12.5000 (12.5000) lr 1.6129e-03 eta 0:00:28
epoch [60/200] batch [15/73] time 0.462 (0.455) data 0.331 (0.324) loss_u loss_u 0.9043 (0.9057) acc_u 15.6250 (11.8750) lr 1.6129e-03 eta 0:00:26
epoch [60/200] batch [20/73] time 0.445 (0.454) data 0.314 (0.323) loss_u loss_u 0.8936 (0.9052) acc_u 15.6250 (12.0312) lr 1.6129e-03 eta 0:00:24
epoch [60/200] batch [25/73] time 0.478 (0.456) data 0.347 (0.325) loss_u loss_u 0.9062 (0.9039) acc_u 12.5000 (12.3750) lr 1.6129e-03 eta 0:00:21
epoch [60/200] batch [30/73] time 0.475 (0.455) data 0.344 (0.324) loss_u loss_u 0.8105 (0.9025) acc_u 25.0000 (12.0833) lr 1.6129e-03 eta 0:00:19
epoch [60/200] batch [35/73] time 0.451 (0.452) data 0.321 (0.321) loss_u loss_u 0.9243 (0.9043) acc_u 6.2500 (11.8750) lr 1.6129e-03 eta 0:00:17
epoch [60/200] batch [40/73] time 0.604 (0.450) data 0.472 (0.318) loss_u loss_u 0.9204 (0.9060) acc_u 6.2500 (11.4844) lr 1.6129e-03 eta 0:00:14
epoch [60/200] batch [45/73] time 0.387 (0.451) data 0.254 (0.320) loss_u loss_u 0.9771 (0.9072) acc_u 0.0000 (11.2500) lr 1.6129e-03 eta 0:00:12
epoch [60/200] batch [50/73] time 0.440 (0.452) data 0.308 (0.321) loss_u loss_u 0.9106 (0.9080) acc_u 9.3750 (11.0625) lr 1.6129e-03 eta 0:00:10
epoch [60/200] batch [55/73] time 0.377 (0.455) data 0.246 (0.324) loss_u loss_u 0.8682 (0.9057) acc_u 15.6250 (11.4773) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [60/73] time 0.344 (0.456) data 0.212 (0.325) loss_u loss_u 0.8335 (0.9054) acc_u 21.8750 (11.4062) lr 1.6129e-03 eta 0:00:05
epoch [60/200] batch [65/73] time 0.683 (0.457) data 0.551 (0.326) loss_u loss_u 0.9067 (0.9040) acc_u 9.3750 (11.6346) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [70/73] time 0.418 (0.461) data 0.287 (0.330) loss_u loss_u 0.9478 (0.9054) acc_u 6.2500 (11.4286) lr 1.6129e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1807
confident_label rate tensor(0.2395, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 751
clean true:573
clean false:178
clean_rate:0.762982689747004
noisy true:756
noisy false:1629
after delete: len(clean_dataset) 751
after delete: len(noisy_dataset) 2385
epoch [61/200] batch [5/23] time 0.487 (0.514) data 0.356 (0.384) loss_x loss_x 1.2158 (1.1922) acc_x 59.3750 (66.8750) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [10/23] time 0.420 (0.499) data 0.290 (0.368) loss_x loss_x 1.0811 (1.2659) acc_x 62.5000 (65.9375) lr 1.6004e-03 eta 0:00:06
epoch [61/200] batch [15/23] time 0.433 (0.477) data 0.303 (0.347) loss_x loss_x 1.6924 (1.3324) acc_x 56.2500 (62.7083) lr 1.6004e-03 eta 0:00:03
epoch [61/200] batch [20/23] time 0.541 (0.494) data 0.411 (0.363) loss_x loss_x 1.3203 (1.3146) acc_x 59.3750 (63.2812) lr 1.6004e-03 eta 0:00:01
epoch [61/200] batch [5/74] time 0.515 (0.474) data 0.384 (0.343) loss_u loss_u 0.8594 (0.8868) acc_u 18.7500 (14.3750) lr 1.6004e-03 eta 0:00:32
epoch [61/200] batch [10/74] time 0.453 (0.478) data 0.322 (0.347) loss_u loss_u 0.9287 (0.9045) acc_u 6.2500 (12.1875) lr 1.6004e-03 eta 0:00:30
epoch [61/200] batch [15/74] time 0.441 (0.471) data 0.310 (0.339) loss_u loss_u 0.9155 (0.9020) acc_u 6.2500 (12.2917) lr 1.6004e-03 eta 0:00:27
epoch [61/200] batch [20/74] time 0.518 (0.468) data 0.386 (0.337) loss_u loss_u 0.9033 (0.9012) acc_u 15.6250 (12.5000) lr 1.6004e-03 eta 0:00:25
epoch [61/200] batch [25/74] time 0.613 (0.467) data 0.483 (0.336) loss_u loss_u 0.9019 (0.9010) acc_u 9.3750 (12.3750) lr 1.6004e-03 eta 0:00:22
epoch [61/200] batch [30/74] time 0.347 (0.464) data 0.215 (0.333) loss_u loss_u 0.9209 (0.8960) acc_u 12.5000 (12.8125) lr 1.6004e-03 eta 0:00:20
epoch [61/200] batch [35/74] time 0.483 (0.467) data 0.352 (0.336) loss_u loss_u 0.8481 (0.8962) acc_u 25.0000 (13.1250) lr 1.6004e-03 eta 0:00:18
epoch [61/200] batch [40/74] time 0.450 (0.468) data 0.318 (0.337) loss_u loss_u 0.8691 (0.8975) acc_u 15.6250 (13.0469) lr 1.6004e-03 eta 0:00:15
epoch [61/200] batch [45/74] time 0.723 (0.470) data 0.592 (0.339) loss_u loss_u 0.9028 (0.8986) acc_u 15.6250 (13.3333) lr 1.6004e-03 eta 0:00:13
epoch [61/200] batch [50/74] time 0.373 (0.469) data 0.242 (0.338) loss_u loss_u 0.9307 (0.9008) acc_u 3.1250 (12.8750) lr 1.6004e-03 eta 0:00:11
epoch [61/200] batch [55/74] time 0.420 (0.470) data 0.289 (0.338) loss_u loss_u 0.8926 (0.9005) acc_u 9.3750 (12.6136) lr 1.6004e-03 eta 0:00:08
epoch [61/200] batch [60/74] time 0.494 (0.468) data 0.361 (0.337) loss_u loss_u 0.8789 (0.8970) acc_u 15.6250 (13.0729) lr 1.6004e-03 eta 0:00:06
epoch [61/200] batch [65/74] time 0.393 (0.465) data 0.264 (0.333) loss_u loss_u 0.8066 (0.8961) acc_u 21.8750 (13.1731) lr 1.6004e-03 eta 0:00:04
epoch [61/200] batch [70/74] time 0.398 (0.464) data 0.268 (0.333) loss_u loss_u 0.8706 (0.8962) acc_u 15.6250 (13.1250) lr 1.6004e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1795
confident_label rate tensor(0.2443, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 766
clean true:586
clean false:180
clean_rate:0.7650130548302873
noisy true:755
noisy false:1615
after delete: len(clean_dataset) 766
after delete: len(noisy_dataset) 2370
epoch [62/200] batch [5/23] time 0.356 (0.441) data 0.226 (0.311) loss_x loss_x 1.3604 (1.1338) acc_x 68.7500 (69.3750) lr 1.5878e-03 eta 0:00:07
epoch [62/200] batch [10/23] time 0.536 (0.439) data 0.405 (0.308) loss_x loss_x 1.3789 (1.3125) acc_x 68.7500 (64.6875) lr 1.5878e-03 eta 0:00:05
epoch [62/200] batch [15/23] time 0.532 (0.433) data 0.400 (0.303) loss_x loss_x 1.7930 (1.3194) acc_x 56.2500 (65.2083) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [20/23] time 0.496 (0.452) data 0.366 (0.322) loss_x loss_x 1.0723 (1.3101) acc_x 68.7500 (65.9375) lr 1.5878e-03 eta 0:00:01
epoch [62/200] batch [5/74] time 0.514 (0.445) data 0.384 (0.315) loss_u loss_u 0.9351 (0.9137) acc_u 6.2500 (11.2500) lr 1.5878e-03 eta 0:00:30
epoch [62/200] batch [10/74] time 0.431 (0.446) data 0.299 (0.316) loss_u loss_u 0.9312 (0.9167) acc_u 9.3750 (9.6875) lr 1.5878e-03 eta 0:00:28
epoch [62/200] batch [15/74] time 0.659 (0.449) data 0.528 (0.319) loss_u loss_u 0.9072 (0.9105) acc_u 18.7500 (11.0417) lr 1.5878e-03 eta 0:00:26
epoch [62/200] batch [20/74] time 0.378 (0.447) data 0.247 (0.316) loss_u loss_u 0.8818 (0.9102) acc_u 12.5000 (10.9375) lr 1.5878e-03 eta 0:00:24
epoch [62/200] batch [25/74] time 0.476 (0.445) data 0.346 (0.315) loss_u loss_u 0.9253 (0.9111) acc_u 9.3750 (10.8750) lr 1.5878e-03 eta 0:00:21
epoch [62/200] batch [30/74] time 0.405 (0.442) data 0.274 (0.311) loss_u loss_u 0.9048 (0.9091) acc_u 15.6250 (11.4583) lr 1.5878e-03 eta 0:00:19
epoch [62/200] batch [35/74] time 0.509 (0.445) data 0.378 (0.314) loss_u loss_u 0.9097 (0.9080) acc_u 9.3750 (11.3393) lr 1.5878e-03 eta 0:00:17
epoch [62/200] batch [40/74] time 0.479 (0.443) data 0.348 (0.312) loss_u loss_u 0.8325 (0.9048) acc_u 21.8750 (11.7969) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [45/74] time 0.529 (0.446) data 0.397 (0.316) loss_u loss_u 0.9453 (0.9058) acc_u 6.2500 (11.8056) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [50/74] time 0.421 (0.449) data 0.290 (0.318) loss_u loss_u 0.9048 (0.9033) acc_u 12.5000 (12.1250) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [55/74] time 0.406 (0.449) data 0.275 (0.318) loss_u loss_u 0.8364 (0.9007) acc_u 21.8750 (12.6136) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [60/74] time 0.385 (0.446) data 0.254 (0.315) loss_u loss_u 0.9395 (0.9013) acc_u 6.2500 (12.4479) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [65/74] time 0.455 (0.447) data 0.325 (0.316) loss_u loss_u 0.8647 (0.9002) acc_u 15.6250 (12.7404) lr 1.5878e-03 eta 0:00:04
epoch [62/200] batch [70/74] time 0.437 (0.447) data 0.305 (0.316) loss_u loss_u 0.9419 (0.9002) acc_u 3.1250 (12.5893) lr 1.5878e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1803
confident_label rate tensor(0.2404, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 754
clean true:579
clean false:175
clean_rate:0.7679045092838196
noisy true:754
noisy false:1628
after delete: len(clean_dataset) 754
after delete: len(noisy_dataset) 2382
epoch [63/200] batch [5/23] time 0.389 (0.427) data 0.258 (0.296) loss_x loss_x 1.2139 (1.1649) acc_x 68.7500 (66.2500) lr 1.5750e-03 eta 0:00:07
epoch [63/200] batch [10/23] time 0.479 (0.489) data 0.348 (0.358) loss_x loss_x 1.5332 (1.2382) acc_x 68.7500 (69.6875) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [15/23] time 0.569 (0.485) data 0.439 (0.354) loss_x loss_x 1.1523 (1.3446) acc_x 75.0000 (68.7500) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [20/23] time 0.381 (0.469) data 0.252 (0.339) loss_x loss_x 1.2764 (1.3555) acc_x 71.8750 (67.9688) lr 1.5750e-03 eta 0:00:01
epoch [63/200] batch [5/74] time 0.385 (0.476) data 0.253 (0.345) loss_u loss_u 0.8857 (0.8792) acc_u 9.3750 (14.3750) lr 1.5750e-03 eta 0:00:32
epoch [63/200] batch [10/74] time 0.451 (0.468) data 0.319 (0.337) loss_u loss_u 0.8701 (0.8746) acc_u 12.5000 (15.6250) lr 1.5750e-03 eta 0:00:29
epoch [63/200] batch [15/74] time 0.467 (0.470) data 0.336 (0.339) loss_u loss_u 0.9185 (0.8852) acc_u 9.3750 (14.3750) lr 1.5750e-03 eta 0:00:27
epoch [63/200] batch [20/74] time 0.460 (0.470) data 0.327 (0.339) loss_u loss_u 0.9141 (0.8865) acc_u 9.3750 (14.2188) lr 1.5750e-03 eta 0:00:25
epoch [63/200] batch [25/74] time 0.417 (0.469) data 0.285 (0.337) loss_u loss_u 0.9731 (0.8938) acc_u 3.1250 (13.2500) lr 1.5750e-03 eta 0:00:22
epoch [63/200] batch [30/74] time 0.422 (0.470) data 0.292 (0.339) loss_u loss_u 0.9565 (0.8956) acc_u 0.0000 (12.9167) lr 1.5750e-03 eta 0:00:20
epoch [63/200] batch [35/74] time 0.456 (0.468) data 0.325 (0.337) loss_u loss_u 0.8740 (0.8919) acc_u 21.8750 (13.4821) lr 1.5750e-03 eta 0:00:18
epoch [63/200] batch [40/74] time 0.447 (0.465) data 0.316 (0.333) loss_u loss_u 0.9014 (0.8949) acc_u 21.8750 (13.2812) lr 1.5750e-03 eta 0:00:15
epoch [63/200] batch [45/74] time 0.398 (0.462) data 0.266 (0.331) loss_u loss_u 0.9121 (0.8966) acc_u 12.5000 (13.1944) lr 1.5750e-03 eta 0:00:13
epoch [63/200] batch [50/74] time 0.489 (0.461) data 0.357 (0.330) loss_u loss_u 0.9097 (0.8970) acc_u 15.6250 (13.3750) lr 1.5750e-03 eta 0:00:11
epoch [63/200] batch [55/74] time 0.376 (0.460) data 0.244 (0.329) loss_u loss_u 0.8179 (0.8975) acc_u 28.1250 (13.5227) lr 1.5750e-03 eta 0:00:08
epoch [63/200] batch [60/74] time 0.435 (0.462) data 0.303 (0.331) loss_u loss_u 0.8936 (0.8970) acc_u 12.5000 (13.6458) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [65/74] time 0.493 (0.464) data 0.362 (0.333) loss_u loss_u 0.9199 (0.8965) acc_u 12.5000 (13.8462) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [70/74] time 0.512 (0.465) data 0.380 (0.334) loss_u loss_u 0.9053 (0.8971) acc_u 12.5000 (13.7946) lr 1.5750e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1764
confident_label rate tensor(0.2519, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 790
clean true:600
clean false:190
clean_rate:0.759493670886076
noisy true:772
noisy false:1574
after delete: len(clean_dataset) 790
after delete: len(noisy_dataset) 2346
epoch [64/200] batch [5/24] time 0.475 (0.488) data 0.343 (0.356) loss_x loss_x 1.5635 (1.4996) acc_x 65.6250 (65.6250) lr 1.5621e-03 eta 0:00:09
epoch [64/200] batch [10/24] time 0.369 (0.461) data 0.238 (0.330) loss_x loss_x 1.2197 (1.3536) acc_x 71.8750 (69.0625) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [15/24] time 0.488 (0.476) data 0.357 (0.345) loss_x loss_x 1.0244 (1.3454) acc_x 68.7500 (68.5417) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [20/24] time 0.478 (0.482) data 0.347 (0.351) loss_x loss_x 1.8662 (1.3128) acc_x 53.1250 (69.6875) lr 1.5621e-03 eta 0:00:01
epoch [64/200] batch [5/73] time 0.515 (0.477) data 0.382 (0.346) loss_u loss_u 0.9458 (0.8940) acc_u 0.0000 (11.8750) lr 1.5621e-03 eta 0:00:32
epoch [64/200] batch [10/73] time 0.480 (0.493) data 0.347 (0.362) loss_u loss_u 0.8970 (0.8878) acc_u 21.8750 (13.7500) lr 1.5621e-03 eta 0:00:31
epoch [64/200] batch [15/73] time 0.504 (0.497) data 0.373 (0.366) loss_u loss_u 0.8354 (0.8946) acc_u 18.7500 (12.9167) lr 1.5621e-03 eta 0:00:28
epoch [64/200] batch [20/73] time 0.495 (0.501) data 0.363 (0.369) loss_u loss_u 0.9126 (0.9024) acc_u 15.6250 (12.5000) lr 1.5621e-03 eta 0:00:26
epoch [64/200] batch [25/73] time 0.455 (0.498) data 0.323 (0.367) loss_u loss_u 0.8604 (0.8987) acc_u 18.7500 (12.7500) lr 1.5621e-03 eta 0:00:23
epoch [64/200] batch [30/73] time 0.417 (0.490) data 0.286 (0.358) loss_u loss_u 0.9150 (0.8992) acc_u 6.2500 (12.6042) lr 1.5621e-03 eta 0:00:21
epoch [64/200] batch [35/73] time 0.522 (0.489) data 0.391 (0.357) loss_u loss_u 0.8848 (0.8979) acc_u 18.7500 (12.8571) lr 1.5621e-03 eta 0:00:18
epoch [64/200] batch [40/73] time 0.444 (0.481) data 0.312 (0.350) loss_u loss_u 0.8789 (0.8995) acc_u 18.7500 (12.8125) lr 1.5621e-03 eta 0:00:15
epoch [64/200] batch [45/73] time 0.492 (0.478) data 0.360 (0.347) loss_u loss_u 0.9253 (0.9025) acc_u 9.3750 (12.5000) lr 1.5621e-03 eta 0:00:13
epoch [64/200] batch [50/73] time 0.492 (0.479) data 0.361 (0.347) loss_u loss_u 0.9219 (0.9022) acc_u 9.3750 (12.4375) lr 1.5621e-03 eta 0:00:11
epoch [64/200] batch [55/73] time 0.510 (0.481) data 0.380 (0.349) loss_u loss_u 0.8564 (0.9021) acc_u 21.8750 (12.3864) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [60/73] time 0.464 (0.478) data 0.332 (0.347) loss_u loss_u 0.8960 (0.9040) acc_u 9.3750 (12.0312) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [65/73] time 0.374 (0.474) data 0.242 (0.343) loss_u loss_u 0.8838 (0.9025) acc_u 15.6250 (12.3077) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [70/73] time 0.306 (0.471) data 0.175 (0.339) loss_u loss_u 0.8623 (0.9037) acc_u 15.6250 (12.1429) lr 1.5621e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1788
confident_label rate tensor(0.2353, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 738
clean true:572
clean false:166
clean_rate:0.7750677506775068
noisy true:776
noisy false:1622
after delete: len(clean_dataset) 738
after delete: len(noisy_dataset) 2398
epoch [65/200] batch [5/23] time 0.458 (0.445) data 0.328 (0.314) loss_x loss_x 1.8379 (1.5387) acc_x 43.7500 (56.8750) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [10/23] time 0.533 (0.456) data 0.403 (0.326) loss_x loss_x 1.3965 (1.4150) acc_x 65.6250 (62.5000) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [15/23] time 0.397 (0.448) data 0.266 (0.317) loss_x loss_x 0.9785 (1.4133) acc_x 71.8750 (62.9167) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [20/23] time 0.400 (0.451) data 0.269 (0.321) loss_x loss_x 2.0449 (1.4173) acc_x 50.0000 (63.1250) lr 1.5490e-03 eta 0:00:01
epoch [65/200] batch [5/74] time 0.389 (0.444) data 0.257 (0.314) loss_u loss_u 0.9385 (0.8762) acc_u 6.2500 (15.0000) lr 1.5490e-03 eta 0:00:30
epoch [65/200] batch [10/74] time 0.677 (0.457) data 0.546 (0.326) loss_u loss_u 0.9009 (0.8950) acc_u 12.5000 (12.5000) lr 1.5490e-03 eta 0:00:29
epoch [65/200] batch [15/74] time 0.474 (0.458) data 0.343 (0.327) loss_u loss_u 0.9277 (0.8980) acc_u 9.3750 (12.0833) lr 1.5490e-03 eta 0:00:27
epoch [65/200] batch [20/74] time 0.434 (0.453) data 0.303 (0.322) loss_u loss_u 0.8682 (0.8915) acc_u 12.5000 (12.8125) lr 1.5490e-03 eta 0:00:24
epoch [65/200] batch [25/74] time 0.490 (0.448) data 0.360 (0.317) loss_u loss_u 0.9360 (0.8970) acc_u 9.3750 (11.8750) lr 1.5490e-03 eta 0:00:21
epoch [65/200] batch [30/74] time 0.660 (0.460) data 0.529 (0.329) loss_u loss_u 0.8657 (0.8932) acc_u 18.7500 (12.8125) lr 1.5490e-03 eta 0:00:20
epoch [65/200] batch [35/74] time 0.433 (0.458) data 0.301 (0.327) loss_u loss_u 0.9233 (0.8942) acc_u 12.5000 (12.8571) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [40/74] time 0.536 (0.457) data 0.403 (0.326) loss_u loss_u 0.9453 (0.8931) acc_u 3.1250 (12.9688) lr 1.5490e-03 eta 0:00:15
epoch [65/200] batch [45/74] time 0.363 (0.459) data 0.231 (0.328) loss_u loss_u 0.9067 (0.8980) acc_u 9.3750 (12.2917) lr 1.5490e-03 eta 0:00:13
epoch [65/200] batch [50/74] time 0.370 (0.459) data 0.239 (0.327) loss_u loss_u 0.9702 (0.8985) acc_u 0.0000 (12.1250) lr 1.5490e-03 eta 0:00:11
epoch [65/200] batch [55/74] time 0.412 (0.458) data 0.279 (0.326) loss_u loss_u 0.9185 (0.8963) acc_u 9.3750 (12.3295) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [60/74] time 0.530 (0.458) data 0.398 (0.327) loss_u loss_u 0.8350 (0.8958) acc_u 18.7500 (12.5000) lr 1.5490e-03 eta 0:00:06
epoch [65/200] batch [65/74] time 0.402 (0.454) data 0.270 (0.323) loss_u loss_u 0.8242 (0.8957) acc_u 25.0000 (12.5962) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [70/74] time 0.441 (0.453) data 0.310 (0.322) loss_u loss_u 0.8740 (0.8953) acc_u 15.6250 (12.6339) lr 1.5490e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1760
confident_label rate tensor(0.2430, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 762
clean true:603
clean false:159
clean_rate:0.7913385826771654
noisy true:773
noisy false:1601
after delete: len(clean_dataset) 762
after delete: len(noisy_dataset) 2374
epoch [66/200] batch [5/23] time 0.403 (0.452) data 0.272 (0.320) loss_x loss_x 0.9961 (1.3807) acc_x 65.6250 (65.0000) lr 1.5358e-03 eta 0:00:08
epoch [66/200] batch [10/23] time 0.481 (0.463) data 0.350 (0.332) loss_x loss_x 1.5898 (1.4239) acc_x 65.6250 (65.3125) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [15/23] time 0.413 (0.463) data 0.282 (0.332) loss_x loss_x 0.9160 (1.3722) acc_x 68.7500 (66.2500) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [20/23] time 0.529 (0.469) data 0.397 (0.338) loss_x loss_x 1.4941 (1.3935) acc_x 65.6250 (66.7188) lr 1.5358e-03 eta 0:00:01
epoch [66/200] batch [5/74] time 0.442 (0.473) data 0.311 (0.342) loss_u loss_u 0.8711 (0.8897) acc_u 15.6250 (16.2500) lr 1.5358e-03 eta 0:00:32
epoch [66/200] batch [10/74] time 0.404 (0.468) data 0.274 (0.337) loss_u loss_u 0.9702 (0.9000) acc_u 0.0000 (13.1250) lr 1.5358e-03 eta 0:00:29
epoch [66/200] batch [15/74] time 0.595 (0.473) data 0.463 (0.342) loss_u loss_u 0.8755 (0.9022) acc_u 12.5000 (12.2917) lr 1.5358e-03 eta 0:00:27
epoch [66/200] batch [20/74] time 0.531 (0.469) data 0.399 (0.338) loss_u loss_u 0.8936 (0.8985) acc_u 12.5000 (13.1250) lr 1.5358e-03 eta 0:00:25
epoch [66/200] batch [25/74] time 0.391 (0.469) data 0.260 (0.338) loss_u loss_u 0.9258 (0.9000) acc_u 9.3750 (12.6250) lr 1.5358e-03 eta 0:00:22
epoch [66/200] batch [30/74] time 0.425 (0.463) data 0.294 (0.331) loss_u loss_u 0.9307 (0.9035) acc_u 3.1250 (11.9792) lr 1.5358e-03 eta 0:00:20
epoch [66/200] batch [35/74] time 0.352 (0.462) data 0.221 (0.331) loss_u loss_u 0.8467 (0.9021) acc_u 15.6250 (12.2321) lr 1.5358e-03 eta 0:00:18
epoch [66/200] batch [40/74] time 0.495 (0.460) data 0.364 (0.329) loss_u loss_u 0.8950 (0.9038) acc_u 12.5000 (12.1094) lr 1.5358e-03 eta 0:00:15
epoch [66/200] batch [45/74] time 0.366 (0.456) data 0.235 (0.324) loss_u loss_u 0.8774 (0.9030) acc_u 12.5000 (12.0833) lr 1.5358e-03 eta 0:00:13
epoch [66/200] batch [50/74] time 0.398 (0.457) data 0.266 (0.326) loss_u loss_u 0.9136 (0.9011) acc_u 15.6250 (12.5000) lr 1.5358e-03 eta 0:00:10
epoch [66/200] batch [55/74] time 0.375 (0.453) data 0.243 (0.322) loss_u loss_u 0.9038 (0.9022) acc_u 6.2500 (12.2159) lr 1.5358e-03 eta 0:00:08
epoch [66/200] batch [60/74] time 0.590 (0.458) data 0.457 (0.327) loss_u loss_u 0.8965 (0.9017) acc_u 18.7500 (12.5521) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [65/74] time 0.398 (0.458) data 0.266 (0.326) loss_u loss_u 0.8696 (0.9030) acc_u 18.7500 (12.3077) lr 1.5358e-03 eta 0:00:04
epoch [66/200] batch [70/74] time 0.331 (0.454) data 0.199 (0.322) loss_u loss_u 0.8862 (0.9029) acc_u 15.6250 (12.3214) lr 1.5358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1792
confident_label rate tensor(0.2443, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 766
clean true:591
clean false:175
clean_rate:0.7715404699738904
noisy true:753
noisy false:1617
after delete: len(clean_dataset) 766
after delete: len(noisy_dataset) 2370
epoch [67/200] batch [5/23] time 0.472 (0.436) data 0.342 (0.304) loss_x loss_x 1.8291 (1.3640) acc_x 59.3750 (66.2500) lr 1.5225e-03 eta 0:00:07
epoch [67/200] batch [10/23] time 0.335 (0.447) data 0.205 (0.317) loss_x loss_x 1.0361 (1.2520) acc_x 75.0000 (68.4375) lr 1.5225e-03 eta 0:00:05
epoch [67/200] batch [15/23] time 0.351 (0.444) data 0.220 (0.313) loss_x loss_x 1.5732 (1.2989) acc_x 68.7500 (68.1250) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [20/23] time 0.438 (0.449) data 0.308 (0.318) loss_x loss_x 1.6074 (1.3698) acc_x 59.3750 (65.6250) lr 1.5225e-03 eta 0:00:01
epoch [67/200] batch [5/74] time 0.350 (0.450) data 0.217 (0.319) loss_u loss_u 0.9229 (0.9055) acc_u 6.2500 (11.8750) lr 1.5225e-03 eta 0:00:31
epoch [67/200] batch [10/74] time 0.406 (0.451) data 0.274 (0.320) loss_u loss_u 0.8965 (0.9116) acc_u 12.5000 (11.5625) lr 1.5225e-03 eta 0:00:28
epoch [67/200] batch [15/74] time 0.403 (0.461) data 0.271 (0.329) loss_u loss_u 0.9370 (0.9152) acc_u 9.3750 (11.0417) lr 1.5225e-03 eta 0:00:27
epoch [67/200] batch [20/74] time 0.361 (0.459) data 0.229 (0.328) loss_u loss_u 0.9229 (0.9062) acc_u 9.3750 (12.6562) lr 1.5225e-03 eta 0:00:24
epoch [67/200] batch [25/74] time 0.518 (0.466) data 0.385 (0.334) loss_u loss_u 0.9253 (0.9083) acc_u 9.3750 (12.0000) lr 1.5225e-03 eta 0:00:22
epoch [67/200] batch [30/74] time 0.368 (0.464) data 0.237 (0.333) loss_u loss_u 0.9653 (0.9073) acc_u 3.1250 (11.8750) lr 1.5225e-03 eta 0:00:20
epoch [67/200] batch [35/74] time 0.362 (0.465) data 0.229 (0.333) loss_u loss_u 0.9312 (0.9065) acc_u 6.2500 (11.8750) lr 1.5225e-03 eta 0:00:18
epoch [67/200] batch [40/74] time 0.464 (0.463) data 0.332 (0.331) loss_u loss_u 0.9414 (0.9032) acc_u 6.2500 (12.4219) lr 1.5225e-03 eta 0:00:15
epoch [67/200] batch [45/74] time 0.407 (0.463) data 0.275 (0.332) loss_u loss_u 0.9551 (0.9032) acc_u 3.1250 (12.4306) lr 1.5225e-03 eta 0:00:13
epoch [67/200] batch [50/74] time 0.496 (0.463) data 0.364 (0.331) loss_u loss_u 0.8853 (0.9017) acc_u 18.7500 (12.5625) lr 1.5225e-03 eta 0:00:11
epoch [67/200] batch [55/74] time 0.419 (0.462) data 0.287 (0.330) loss_u loss_u 0.8413 (0.9007) acc_u 15.6250 (12.7841) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [60/74] time 0.528 (0.465) data 0.397 (0.333) loss_u loss_u 0.8584 (0.8998) acc_u 18.7500 (12.9167) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [65/74] time 0.394 (0.464) data 0.264 (0.332) loss_u loss_u 0.9385 (0.9007) acc_u 9.3750 (12.7404) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [70/74] time 0.334 (0.461) data 0.203 (0.329) loss_u loss_u 0.8950 (0.9008) acc_u 18.7500 (12.6786) lr 1.5225e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1764
confident_label rate tensor(0.2586, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 811
clean true:630
clean false:181
clean_rate:0.7768187422934648
noisy true:742
noisy false:1583
after delete: len(clean_dataset) 811
after delete: len(noisy_dataset) 2325
epoch [68/200] batch [5/25] time 0.488 (0.534) data 0.356 (0.403) loss_x loss_x 1.1904 (1.3146) acc_x 65.6250 (68.1250) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [10/25] time 0.653 (0.536) data 0.520 (0.405) loss_x loss_x 1.0566 (1.2871) acc_x 59.3750 (67.1875) lr 1.5090e-03 eta 0:00:08
epoch [68/200] batch [15/25] time 0.458 (0.522) data 0.327 (0.391) loss_x loss_x 1.1875 (1.2843) acc_x 68.7500 (67.2917) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [20/25] time 0.630 (0.518) data 0.498 (0.386) loss_x loss_x 2.0664 (1.3676) acc_x 59.3750 (65.7812) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [25/25] time 0.399 (0.503) data 0.268 (0.372) loss_x loss_x 1.3428 (1.4013) acc_x 65.6250 (64.2500) lr 1.5090e-03 eta 0:00:00
epoch [68/200] batch [5/72] time 0.449 (0.494) data 0.317 (0.362) loss_u loss_u 0.9058 (0.9163) acc_u 9.3750 (8.7500) lr 1.5090e-03 eta 0:00:33
epoch [68/200] batch [10/72] time 0.378 (0.484) data 0.246 (0.353) loss_u loss_u 0.9214 (0.9150) acc_u 12.5000 (10.0000) lr 1.5090e-03 eta 0:00:30
epoch [68/200] batch [15/72] time 0.463 (0.476) data 0.331 (0.344) loss_u loss_u 0.8296 (0.9024) acc_u 18.7500 (12.0833) lr 1.5090e-03 eta 0:00:27
epoch [68/200] batch [20/72] time 0.374 (0.469) data 0.243 (0.337) loss_u loss_u 0.9360 (0.9011) acc_u 3.1250 (11.7188) lr 1.5090e-03 eta 0:00:24
epoch [68/200] batch [25/72] time 0.550 (0.471) data 0.418 (0.340) loss_u loss_u 0.8516 (0.9004) acc_u 18.7500 (11.8750) lr 1.5090e-03 eta 0:00:22
epoch [68/200] batch [30/72] time 0.352 (0.474) data 0.221 (0.343) loss_u loss_u 0.9072 (0.9068) acc_u 12.5000 (11.0417) lr 1.5090e-03 eta 0:00:19
epoch [68/200] batch [35/72] time 0.355 (0.471) data 0.223 (0.339) loss_u loss_u 0.8984 (0.9050) acc_u 9.3750 (11.3393) lr 1.5090e-03 eta 0:00:17
epoch [68/200] batch [40/72] time 0.408 (0.469) data 0.276 (0.337) loss_u loss_u 0.8877 (0.9073) acc_u 12.5000 (11.0156) lr 1.5090e-03 eta 0:00:14
epoch [68/200] batch [45/72] time 0.370 (0.466) data 0.236 (0.335) loss_u loss_u 0.8682 (0.9073) acc_u 15.6250 (10.9028) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [50/72] time 0.386 (0.468) data 0.254 (0.337) loss_u loss_u 0.8877 (0.9061) acc_u 21.8750 (11.3125) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [55/72] time 0.412 (0.464) data 0.281 (0.333) loss_u loss_u 0.9214 (0.9057) acc_u 9.3750 (11.4773) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [60/72] time 0.425 (0.461) data 0.294 (0.329) loss_u loss_u 0.9448 (0.9060) acc_u 6.2500 (11.4062) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [65/72] time 0.793 (0.463) data 0.663 (0.331) loss_u loss_u 0.8423 (0.9031) acc_u 21.8750 (11.9712) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [70/72] time 0.459 (0.464) data 0.329 (0.333) loss_u loss_u 0.9512 (0.9052) acc_u 6.2500 (11.5625) lr 1.5090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1776
confident_label rate tensor(0.2513, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 788
clean true:600
clean false:188
clean_rate:0.7614213197969543
noisy true:760
noisy false:1588
after delete: len(clean_dataset) 788
after delete: len(noisy_dataset) 2348
epoch [69/200] batch [5/24] time 0.487 (0.450) data 0.356 (0.319) loss_x loss_x 1.3271 (1.2365) acc_x 68.7500 (70.6250) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [10/24] time 0.442 (0.441) data 0.312 (0.310) loss_x loss_x 1.1074 (1.2234) acc_x 65.6250 (66.8750) lr 1.4955e-03 eta 0:00:06
epoch [69/200] batch [15/24] time 0.422 (0.444) data 0.292 (0.314) loss_x loss_x 1.5498 (1.2966) acc_x 62.5000 (65.0000) lr 1.4955e-03 eta 0:00:04
epoch [69/200] batch [20/24] time 0.409 (0.437) data 0.279 (0.307) loss_x loss_x 1.4883 (1.3166) acc_x 65.6250 (64.3750) lr 1.4955e-03 eta 0:00:01
epoch [69/200] batch [5/73] time 0.427 (0.446) data 0.295 (0.315) loss_u loss_u 0.9146 (0.8863) acc_u 9.3750 (11.2500) lr 1.4955e-03 eta 0:00:30
epoch [69/200] batch [10/73] time 0.317 (0.439) data 0.186 (0.308) loss_u loss_u 0.9092 (0.9026) acc_u 12.5000 (10.3125) lr 1.4955e-03 eta 0:00:27
epoch [69/200] batch [15/73] time 0.519 (0.449) data 0.388 (0.319) loss_u loss_u 0.9180 (0.9092) acc_u 6.2500 (10.0000) lr 1.4955e-03 eta 0:00:26
epoch [69/200] batch [20/73] time 0.399 (0.458) data 0.266 (0.327) loss_u loss_u 0.8750 (0.9097) acc_u 12.5000 (10.6250) lr 1.4955e-03 eta 0:00:24
epoch [69/200] batch [25/73] time 0.481 (0.457) data 0.350 (0.327) loss_u loss_u 0.8696 (0.9017) acc_u 18.7500 (12.1250) lr 1.4955e-03 eta 0:00:21
epoch [69/200] batch [30/73] time 0.353 (0.454) data 0.222 (0.323) loss_u loss_u 0.9277 (0.9006) acc_u 6.2500 (12.1875) lr 1.4955e-03 eta 0:00:19
epoch [69/200] batch [35/73] time 0.402 (0.453) data 0.272 (0.322) loss_u loss_u 0.9209 (0.9010) acc_u 12.5000 (12.5000) lr 1.4955e-03 eta 0:00:17
epoch [69/200] batch [40/73] time 0.448 (0.452) data 0.315 (0.321) loss_u loss_u 0.9365 (0.9030) acc_u 12.5000 (12.5000) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [45/73] time 0.529 (0.455) data 0.396 (0.324) loss_u loss_u 0.8887 (0.9026) acc_u 9.3750 (12.3611) lr 1.4955e-03 eta 0:00:12
epoch [69/200] batch [50/73] time 0.334 (0.457) data 0.203 (0.326) loss_u loss_u 0.9136 (0.9019) acc_u 12.5000 (12.3750) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [55/73] time 0.343 (0.456) data 0.211 (0.325) loss_u loss_u 0.9272 (0.9040) acc_u 12.5000 (12.1591) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [60/73] time 0.425 (0.456) data 0.293 (0.325) loss_u loss_u 0.8374 (0.9010) acc_u 18.7500 (12.3438) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [65/73] time 0.399 (0.452) data 0.267 (0.321) loss_u loss_u 0.9116 (0.9003) acc_u 6.2500 (12.4038) lr 1.4955e-03 eta 0:00:03
epoch [69/200] batch [70/73] time 0.435 (0.452) data 0.303 (0.321) loss_u loss_u 0.9102 (0.9010) acc_u 15.6250 (12.4107) lr 1.4955e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1739
confident_label rate tensor(0.2599, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 815
clean true:631
clean false:184
clean_rate:0.7742331288343558
noisy true:766
noisy false:1555
after delete: len(clean_dataset) 815
after delete: len(noisy_dataset) 2321
epoch [70/200] batch [5/25] time 0.505 (0.468) data 0.375 (0.338) loss_x loss_x 1.2295 (1.3488) acc_x 56.2500 (60.0000) lr 1.4818e-03 eta 0:00:09
epoch [70/200] batch [10/25] time 0.385 (0.450) data 0.255 (0.320) loss_x loss_x 2.0156 (1.4114) acc_x 43.7500 (59.0625) lr 1.4818e-03 eta 0:00:06
epoch [70/200] batch [15/25] time 0.464 (0.446) data 0.332 (0.316) loss_x loss_x 1.3633 (1.3475) acc_x 65.6250 (63.3333) lr 1.4818e-03 eta 0:00:04
epoch [70/200] batch [20/25] time 0.459 (0.450) data 0.329 (0.319) loss_x loss_x 1.1855 (1.3232) acc_x 75.0000 (64.6875) lr 1.4818e-03 eta 0:00:02
epoch [70/200] batch [25/25] time 0.493 (0.458) data 0.362 (0.328) loss_x loss_x 1.5928 (1.3746) acc_x 62.5000 (63.6250) lr 1.4818e-03 eta 0:00:00
epoch [70/200] batch [5/72] time 0.451 (0.450) data 0.320 (0.320) loss_u loss_u 0.9233 (0.9107) acc_u 9.3750 (10.6250) lr 1.4818e-03 eta 0:00:30
epoch [70/200] batch [10/72] time 0.432 (0.452) data 0.300 (0.322) loss_u loss_u 0.9082 (0.9130) acc_u 9.3750 (10.0000) lr 1.4818e-03 eta 0:00:28
epoch [70/200] batch [15/72] time 0.501 (0.452) data 0.369 (0.322) loss_u loss_u 0.8843 (0.9068) acc_u 15.6250 (11.4583) lr 1.4818e-03 eta 0:00:25
epoch [70/200] batch [20/72] time 0.612 (0.456) data 0.480 (0.326) loss_u loss_u 0.9355 (0.9111) acc_u 6.2500 (10.6250) lr 1.4818e-03 eta 0:00:23
epoch [70/200] batch [25/72] time 0.492 (0.459) data 0.360 (0.328) loss_u loss_u 0.8721 (0.9089) acc_u 15.6250 (11.1250) lr 1.4818e-03 eta 0:00:21
epoch [70/200] batch [30/72] time 0.462 (0.457) data 0.330 (0.326) loss_u loss_u 0.8447 (0.9072) acc_u 15.6250 (11.3542) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [35/72] time 0.403 (0.453) data 0.272 (0.322) loss_u loss_u 0.9414 (0.9087) acc_u 6.2500 (11.2500) lr 1.4818e-03 eta 0:00:16
epoch [70/200] batch [40/72] time 0.458 (0.452) data 0.327 (0.321) loss_u loss_u 0.8452 (0.9065) acc_u 25.0000 (11.7969) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [45/72] time 0.592 (0.452) data 0.460 (0.321) loss_u loss_u 0.9248 (0.9049) acc_u 12.5000 (12.0833) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [50/72] time 0.443 (0.450) data 0.312 (0.319) loss_u loss_u 0.9253 (0.9050) acc_u 3.1250 (12.0000) lr 1.4818e-03 eta 0:00:09
epoch [70/200] batch [55/72] time 0.392 (0.448) data 0.261 (0.317) loss_u loss_u 0.9312 (0.9043) acc_u 6.2500 (11.9886) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [60/72] time 0.414 (0.450) data 0.283 (0.318) loss_u loss_u 0.8306 (0.9025) acc_u 25.0000 (12.3958) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [65/72] time 0.415 (0.449) data 0.283 (0.318) loss_u loss_u 0.9370 (0.9020) acc_u 6.2500 (12.4519) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [70/72] time 0.434 (0.448) data 0.304 (0.317) loss_u loss_u 0.8735 (0.9007) acc_u 12.5000 (12.7232) lr 1.4818e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1782
confident_label rate tensor(0.2423, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 760
clean true:585
clean false:175
clean_rate:0.7697368421052632
noisy true:769
noisy false:1607
after delete: len(clean_dataset) 760
after delete: len(noisy_dataset) 2376
epoch [71/200] batch [5/23] time 0.535 (0.466) data 0.405 (0.335) loss_x loss_x 1.3789 (1.3061) acc_x 65.6250 (66.2500) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [10/23] time 0.410 (0.458) data 0.279 (0.327) loss_x loss_x 1.0264 (1.2938) acc_x 75.0000 (65.9375) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [15/23] time 0.478 (0.471) data 0.347 (0.340) loss_x loss_x 1.7510 (1.2554) acc_x 56.2500 (67.9167) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [20/23] time 0.474 (0.462) data 0.344 (0.331) loss_x loss_x 1.6211 (1.2766) acc_x 62.5000 (67.0312) lr 1.4679e-03 eta 0:00:01
epoch [71/200] batch [5/74] time 0.381 (0.462) data 0.250 (0.331) loss_u loss_u 0.9277 (0.8980) acc_u 12.5000 (15.6250) lr 1.4679e-03 eta 0:00:31
epoch [71/200] batch [10/74] time 0.437 (0.464) data 0.306 (0.333) loss_u loss_u 0.9404 (0.8958) acc_u 9.3750 (14.6875) lr 1.4679e-03 eta 0:00:29
epoch [71/200] batch [15/74] time 0.401 (0.457) data 0.270 (0.326) loss_u loss_u 0.9023 (0.8874) acc_u 12.5000 (15.0000) lr 1.4679e-03 eta 0:00:26
epoch [71/200] batch [20/74] time 0.568 (0.458) data 0.436 (0.327) loss_u loss_u 0.8667 (0.8878) acc_u 15.6250 (13.9062) lr 1.4679e-03 eta 0:00:24
epoch [71/200] batch [25/74] time 0.417 (0.453) data 0.285 (0.322) loss_u loss_u 0.9360 (0.8914) acc_u 6.2500 (13.6250) lr 1.4679e-03 eta 0:00:22
epoch [71/200] batch [30/74] time 0.428 (0.450) data 0.296 (0.319) loss_u loss_u 0.9453 (0.8936) acc_u 3.1250 (12.9167) lr 1.4679e-03 eta 0:00:19
epoch [71/200] batch [35/74] time 0.540 (0.451) data 0.409 (0.320) loss_u loss_u 0.9048 (0.8978) acc_u 12.5000 (12.5000) lr 1.4679e-03 eta 0:00:17
epoch [71/200] batch [40/74] time 0.401 (0.453) data 0.270 (0.322) loss_u loss_u 0.9189 (0.9015) acc_u 9.3750 (12.2656) lr 1.4679e-03 eta 0:00:15
epoch [71/200] batch [45/74] time 0.445 (0.453) data 0.314 (0.322) loss_u loss_u 0.9507 (0.9027) acc_u 9.3750 (12.2222) lr 1.4679e-03 eta 0:00:13
epoch [71/200] batch [50/74] time 0.457 (0.454) data 0.326 (0.322) loss_u loss_u 0.7773 (0.8978) acc_u 31.2500 (12.9375) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [55/74] time 0.489 (0.455) data 0.358 (0.324) loss_u loss_u 0.9458 (0.8978) acc_u 6.2500 (12.8409) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [60/74] time 0.445 (0.453) data 0.314 (0.322) loss_u loss_u 0.9131 (0.8961) acc_u 9.3750 (12.9688) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [65/74] time 0.477 (0.455) data 0.344 (0.324) loss_u loss_u 0.8599 (0.8949) acc_u 18.7500 (12.9327) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [70/74] time 0.397 (0.452) data 0.265 (0.321) loss_u loss_u 0.8481 (0.8931) acc_u 28.1250 (13.3482) lr 1.4679e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1755
confident_label rate tensor(0.2541, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 797
clean true:624
clean false:173
clean_rate:0.7829360100376411
noisy true:757
noisy false:1582
after delete: len(clean_dataset) 797
after delete: len(noisy_dataset) 2339
epoch [72/200] batch [5/24] time 0.527 (0.430) data 0.397 (0.300) loss_x loss_x 1.4404 (1.2609) acc_x 50.0000 (65.6250) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [10/24] time 0.526 (0.433) data 0.397 (0.303) loss_x loss_x 1.4160 (1.2917) acc_x 65.6250 (66.2500) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [15/24] time 0.548 (0.434) data 0.418 (0.304) loss_x loss_x 1.6211 (1.3116) acc_x 62.5000 (66.2500) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [20/24] time 0.438 (0.449) data 0.307 (0.319) loss_x loss_x 1.1826 (1.3505) acc_x 62.5000 (64.5312) lr 1.4540e-03 eta 0:00:01
epoch [72/200] batch [5/73] time 0.394 (0.455) data 0.263 (0.324) loss_u loss_u 0.8574 (0.8842) acc_u 15.6250 (13.1250) lr 1.4540e-03 eta 0:00:30
epoch [72/200] batch [10/73] time 0.432 (0.457) data 0.300 (0.327) loss_u loss_u 0.9243 (0.9037) acc_u 9.3750 (10.3125) lr 1.4540e-03 eta 0:00:28
epoch [72/200] batch [15/73] time 0.363 (0.450) data 0.233 (0.319) loss_u loss_u 0.9199 (0.9033) acc_u 3.1250 (11.4583) lr 1.4540e-03 eta 0:00:26
epoch [72/200] batch [20/73] time 0.509 (0.457) data 0.378 (0.326) loss_u loss_u 0.8779 (0.8980) acc_u 12.5000 (12.5000) lr 1.4540e-03 eta 0:00:24
epoch [72/200] batch [25/73] time 0.401 (0.454) data 0.270 (0.323) loss_u loss_u 0.9048 (0.9037) acc_u 12.5000 (11.8750) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [30/73] time 0.483 (0.452) data 0.351 (0.321) loss_u loss_u 0.8472 (0.9050) acc_u 15.6250 (11.5625) lr 1.4540e-03 eta 0:00:19
epoch [72/200] batch [35/73] time 0.418 (0.448) data 0.288 (0.317) loss_u loss_u 0.9023 (0.9045) acc_u 9.3750 (11.8750) lr 1.4540e-03 eta 0:00:17
epoch [72/200] batch [40/73] time 0.518 (0.444) data 0.388 (0.313) loss_u loss_u 0.9058 (0.9006) acc_u 9.3750 (12.2656) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [45/73] time 0.429 (0.445) data 0.299 (0.314) loss_u loss_u 0.8613 (0.9033) acc_u 18.7500 (11.8750) lr 1.4540e-03 eta 0:00:12
epoch [72/200] batch [50/73] time 0.408 (0.444) data 0.276 (0.313) loss_u loss_u 0.8789 (0.9032) acc_u 12.5000 (11.7500) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [55/73] time 0.357 (0.443) data 0.225 (0.312) loss_u loss_u 0.9136 (0.9024) acc_u 9.3750 (11.7614) lr 1.4540e-03 eta 0:00:07
epoch [72/200] batch [60/73] time 0.355 (0.440) data 0.225 (0.309) loss_u loss_u 0.9131 (0.9042) acc_u 12.5000 (11.5104) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [65/73] time 0.422 (0.443) data 0.290 (0.312) loss_u loss_u 0.9023 (0.9025) acc_u 15.6250 (11.7308) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [70/73] time 0.382 (0.444) data 0.250 (0.313) loss_u loss_u 0.8643 (0.9020) acc_u 21.8750 (11.9196) lr 1.4540e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1778
confident_label rate tensor(0.2506, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 786
clean true:598
clean false:188
clean_rate:0.7608142493638677
noisy true:760
noisy false:1590
after delete: len(clean_dataset) 786
after delete: len(noisy_dataset) 2350
epoch [73/200] batch [5/24] time 0.504 (0.486) data 0.374 (0.355) loss_x loss_x 1.7627 (1.4125) acc_x 59.3750 (65.0000) lr 1.4399e-03 eta 0:00:09
epoch [73/200] batch [10/24] time 0.339 (0.477) data 0.209 (0.346) loss_x loss_x 1.4580 (1.3798) acc_x 62.5000 (65.0000) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [15/24] time 0.402 (0.481) data 0.271 (0.350) loss_x loss_x 1.7227 (1.3599) acc_x 56.2500 (65.8333) lr 1.4399e-03 eta 0:00:04
epoch [73/200] batch [20/24] time 0.400 (0.472) data 0.270 (0.341) loss_x loss_x 1.3086 (1.3800) acc_x 62.5000 (65.1562) lr 1.4399e-03 eta 0:00:01
epoch [73/200] batch [5/73] time 0.395 (0.458) data 0.264 (0.327) loss_u loss_u 0.8633 (0.8748) acc_u 15.6250 (14.3750) lr 1.4399e-03 eta 0:00:31
epoch [73/200] batch [10/73] time 0.457 (0.471) data 0.327 (0.341) loss_u loss_u 0.9175 (0.8878) acc_u 6.2500 (13.1250) lr 1.4399e-03 eta 0:00:29
epoch [73/200] batch [15/73] time 0.378 (0.461) data 0.247 (0.330) loss_u loss_u 0.8794 (0.8894) acc_u 12.5000 (13.5417) lr 1.4399e-03 eta 0:00:26
epoch [73/200] batch [20/73] time 0.372 (0.461) data 0.241 (0.330) loss_u loss_u 0.9219 (0.8909) acc_u 12.5000 (13.7500) lr 1.4399e-03 eta 0:00:24
epoch [73/200] batch [25/73] time 0.612 (0.465) data 0.480 (0.334) loss_u loss_u 0.9131 (0.8952) acc_u 12.5000 (13.3750) lr 1.4399e-03 eta 0:00:22
epoch [73/200] batch [30/73] time 0.357 (0.462) data 0.226 (0.331) loss_u loss_u 0.8398 (0.8964) acc_u 12.5000 (12.9167) lr 1.4399e-03 eta 0:00:19
epoch [73/200] batch [35/73] time 0.402 (0.462) data 0.271 (0.331) loss_u loss_u 0.9575 (0.8966) acc_u 3.1250 (12.9464) lr 1.4399e-03 eta 0:00:17
epoch [73/200] batch [40/73] time 0.388 (0.458) data 0.257 (0.327) loss_u loss_u 0.8960 (0.8990) acc_u 9.3750 (12.6562) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [45/73] time 0.313 (0.451) data 0.181 (0.320) loss_u loss_u 0.9224 (0.8976) acc_u 9.3750 (12.9167) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [50/73] time 0.441 (0.450) data 0.310 (0.319) loss_u loss_u 0.9478 (0.8977) acc_u 6.2500 (13.0000) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [55/73] time 0.380 (0.447) data 0.248 (0.316) loss_u loss_u 0.9102 (0.8970) acc_u 9.3750 (13.0682) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [60/73] time 0.454 (0.448) data 0.322 (0.317) loss_u loss_u 0.8799 (0.8978) acc_u 12.5000 (13.0208) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [65/73] time 0.441 (0.449) data 0.309 (0.318) loss_u loss_u 0.8916 (0.8980) acc_u 12.5000 (12.9808) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [70/73] time 0.461 (0.447) data 0.330 (0.316) loss_u loss_u 0.9170 (0.9006) acc_u 12.5000 (12.6786) lr 1.4399e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1720
confident_label rate tensor(0.2669, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 837
clean true:645
clean false:192
clean_rate:0.7706093189964157
noisy true:771
noisy false:1528
after delete: len(clean_dataset) 837
after delete: len(noisy_dataset) 2299
epoch [74/200] batch [5/26] time 0.387 (0.472) data 0.257 (0.342) loss_x loss_x 1.4961 (1.2348) acc_x 46.8750 (69.3750) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [10/26] time 0.495 (0.464) data 0.364 (0.333) loss_x loss_x 1.0352 (1.1923) acc_x 71.8750 (69.6875) lr 1.4258e-03 eta 0:00:07
epoch [74/200] batch [15/26] time 0.385 (0.463) data 0.254 (0.332) loss_x loss_x 1.6025 (1.2029) acc_x 62.5000 (69.1667) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [20/26] time 0.510 (0.460) data 0.379 (0.329) loss_x loss_x 1.7666 (1.2524) acc_x 59.3750 (67.0312) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [25/26] time 0.461 (0.458) data 0.330 (0.327) loss_x loss_x 1.0898 (1.2926) acc_x 65.6250 (66.5000) lr 1.4258e-03 eta 0:00:00
epoch [74/200] batch [5/71] time 0.472 (0.456) data 0.340 (0.325) loss_u loss_u 0.9165 (0.9038) acc_u 9.3750 (10.6250) lr 1.4258e-03 eta 0:00:30
epoch [74/200] batch [10/71] time 0.629 (0.457) data 0.498 (0.326) loss_u loss_u 0.9180 (0.9023) acc_u 9.3750 (11.2500) lr 1.4258e-03 eta 0:00:27
epoch [74/200] batch [15/71] time 0.348 (0.446) data 0.218 (0.315) loss_u loss_u 0.9014 (0.9050) acc_u 9.3750 (11.2500) lr 1.4258e-03 eta 0:00:24
epoch [74/200] batch [20/71] time 0.464 (0.449) data 0.333 (0.317) loss_u loss_u 0.9492 (0.9113) acc_u 3.1250 (10.3125) lr 1.4258e-03 eta 0:00:22
epoch [74/200] batch [25/71] time 0.630 (0.451) data 0.498 (0.320) loss_u loss_u 0.8770 (0.9053) acc_u 15.6250 (11.5000) lr 1.4258e-03 eta 0:00:20
epoch [74/200] batch [30/71] time 0.454 (0.453) data 0.322 (0.322) loss_u loss_u 0.8643 (0.9092) acc_u 12.5000 (10.9375) lr 1.4258e-03 eta 0:00:18
epoch [74/200] batch [35/71] time 0.495 (0.456) data 0.363 (0.325) loss_u loss_u 0.9033 (0.9063) acc_u 9.3750 (11.4286) lr 1.4258e-03 eta 0:00:16
epoch [74/200] batch [40/71] time 0.501 (0.457) data 0.370 (0.326) loss_u loss_u 0.9258 (0.9068) acc_u 9.3750 (11.4062) lr 1.4258e-03 eta 0:00:14
epoch [74/200] batch [45/71] time 0.406 (0.453) data 0.275 (0.322) loss_u loss_u 0.8564 (0.9050) acc_u 18.7500 (11.5972) lr 1.4258e-03 eta 0:00:11
epoch [74/200] batch [50/71] time 0.341 (0.451) data 0.210 (0.320) loss_u loss_u 0.8511 (0.9042) acc_u 18.7500 (11.6875) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [55/71] time 0.376 (0.451) data 0.244 (0.319) loss_u loss_u 0.8911 (0.9031) acc_u 12.5000 (12.0455) lr 1.4258e-03 eta 0:00:07
epoch [74/200] batch [60/71] time 0.385 (0.450) data 0.253 (0.319) loss_u loss_u 0.9365 (0.9033) acc_u 9.3750 (12.0312) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [65/71] time 0.349 (0.451) data 0.216 (0.319) loss_u loss_u 0.8672 (0.9041) acc_u 15.6250 (11.8750) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [70/71] time 0.400 (0.450) data 0.268 (0.318) loss_u loss_u 0.8989 (0.9029) acc_u 15.6250 (12.0982) lr 1.4258e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1760
confident_label rate tensor(0.2570, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 806
clean true:617
clean false:189
clean_rate:0.7655086848635235
noisy true:759
noisy false:1571
after delete: len(clean_dataset) 806
after delete: len(noisy_dataset) 2330
epoch [75/200] batch [5/25] time 0.459 (0.465) data 0.328 (0.334) loss_x loss_x 0.9370 (1.1305) acc_x 84.3750 (71.8750) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [10/25] time 0.586 (0.465) data 0.456 (0.334) loss_x loss_x 1.9912 (1.2000) acc_x 53.1250 (70.3125) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [15/25] time 0.445 (0.453) data 0.314 (0.322) loss_x loss_x 1.1748 (1.2175) acc_x 68.7500 (67.9167) lr 1.4115e-03 eta 0:00:04
epoch [75/200] batch [20/25] time 0.330 (0.445) data 0.198 (0.314) loss_x loss_x 1.1152 (1.2686) acc_x 68.7500 (67.0312) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [25/25] time 0.429 (0.455) data 0.297 (0.324) loss_x loss_x 1.3252 (1.3143) acc_x 56.2500 (66.0000) lr 1.4115e-03 eta 0:00:00
epoch [75/200] batch [5/72] time 0.364 (0.448) data 0.233 (0.317) loss_u loss_u 0.9551 (0.9092) acc_u 3.1250 (12.5000) lr 1.4115e-03 eta 0:00:30
epoch [75/200] batch [10/72] time 0.370 (0.443) data 0.238 (0.311) loss_u loss_u 0.9717 (0.9092) acc_u 3.1250 (12.1875) lr 1.4115e-03 eta 0:00:27
epoch [75/200] batch [15/72] time 0.381 (0.446) data 0.250 (0.315) loss_u loss_u 0.8906 (0.9070) acc_u 12.5000 (12.0833) lr 1.4115e-03 eta 0:00:25
epoch [75/200] batch [20/72] time 0.453 (0.450) data 0.322 (0.319) loss_u loss_u 0.9165 (0.9096) acc_u 6.2500 (11.2500) lr 1.4115e-03 eta 0:00:23
epoch [75/200] batch [25/72] time 0.280 (0.444) data 0.149 (0.313) loss_u loss_u 0.9165 (0.9096) acc_u 9.3750 (11.5000) lr 1.4115e-03 eta 0:00:20
epoch [75/200] batch [30/72] time 0.410 (0.443) data 0.278 (0.312) loss_u loss_u 0.8560 (0.9036) acc_u 18.7500 (12.3958) lr 1.4115e-03 eta 0:00:18
epoch [75/200] batch [35/72] time 0.417 (0.439) data 0.286 (0.308) loss_u loss_u 0.8301 (0.9003) acc_u 21.8750 (12.8571) lr 1.4115e-03 eta 0:00:16
epoch [75/200] batch [40/72] time 0.390 (0.436) data 0.259 (0.305) loss_u loss_u 0.9302 (0.9019) acc_u 9.3750 (12.5000) lr 1.4115e-03 eta 0:00:13
epoch [75/200] batch [45/72] time 0.610 (0.438) data 0.479 (0.307) loss_u loss_u 0.9570 (0.9012) acc_u 6.2500 (12.6389) lr 1.4115e-03 eta 0:00:11
epoch [75/200] batch [50/72] time 0.603 (0.440) data 0.471 (0.308) loss_u loss_u 0.9165 (0.8999) acc_u 9.3750 (13.0000) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [55/72] time 0.405 (0.440) data 0.273 (0.309) loss_u loss_u 0.8989 (0.9008) acc_u 15.6250 (12.8977) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [60/72] time 0.712 (0.442) data 0.581 (0.311) loss_u loss_u 0.8438 (0.9012) acc_u 18.7500 (12.8125) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [65/72] time 0.546 (0.443) data 0.414 (0.312) loss_u loss_u 0.9404 (0.9027) acc_u 9.3750 (12.6442) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [70/72] time 0.453 (0.443) data 0.322 (0.312) loss_u loss_u 0.9136 (0.9022) acc_u 9.3750 (12.6786) lr 1.4115e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1755
confident_label rate tensor(0.2497, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 783
clean true:603
clean false:180
clean_rate:0.7701149425287356
noisy true:778
noisy false:1575
after delete: len(clean_dataset) 783
after delete: len(noisy_dataset) 2353
epoch [76/200] batch [5/24] time 0.399 (0.477) data 0.269 (0.346) loss_x loss_x 1.0010 (1.0515) acc_x 71.8750 (69.3750) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [10/24] time 0.425 (0.467) data 0.294 (0.336) loss_x loss_x 1.4785 (1.2678) acc_x 53.1250 (65.0000) lr 1.3971e-03 eta 0:00:06
epoch [76/200] batch [15/24] time 0.428 (0.460) data 0.298 (0.329) loss_x loss_x 1.1768 (1.2390) acc_x 71.8750 (67.7083) lr 1.3971e-03 eta 0:00:04
epoch [76/200] batch [20/24] time 0.420 (0.459) data 0.289 (0.328) loss_x loss_x 1.5684 (1.2778) acc_x 59.3750 (66.4062) lr 1.3971e-03 eta 0:00:01
epoch [76/200] batch [5/73] time 0.439 (0.458) data 0.307 (0.327) loss_u loss_u 0.9092 (0.8993) acc_u 12.5000 (12.5000) lr 1.3971e-03 eta 0:00:31
epoch [76/200] batch [10/73] time 0.335 (0.449) data 0.203 (0.318) loss_u loss_u 0.8418 (0.8983) acc_u 25.0000 (13.7500) lr 1.3971e-03 eta 0:00:28
epoch [76/200] batch [15/73] time 0.391 (0.452) data 0.260 (0.321) loss_u loss_u 0.9673 (0.8997) acc_u 3.1250 (13.5417) lr 1.3971e-03 eta 0:00:26
epoch [76/200] batch [20/73] time 0.499 (0.449) data 0.367 (0.318) loss_u loss_u 0.8706 (0.9017) acc_u 12.5000 (12.9688) lr 1.3971e-03 eta 0:00:23
epoch [76/200] batch [25/73] time 0.348 (0.442) data 0.217 (0.310) loss_u loss_u 0.9258 (0.9028) acc_u 3.1250 (13.0000) lr 1.3971e-03 eta 0:00:21
epoch [76/200] batch [30/73] time 0.510 (0.441) data 0.378 (0.310) loss_u loss_u 0.9399 (0.9041) acc_u 3.1250 (12.3958) lr 1.3971e-03 eta 0:00:18
epoch [76/200] batch [35/73] time 0.481 (0.440) data 0.349 (0.309) loss_u loss_u 0.8896 (0.8979) acc_u 18.7500 (13.0357) lr 1.3971e-03 eta 0:00:16
epoch [76/200] batch [40/73] time 0.427 (0.442) data 0.295 (0.310) loss_u loss_u 0.9702 (0.9007) acc_u 3.1250 (12.6562) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [45/73] time 0.497 (0.444) data 0.366 (0.312) loss_u loss_u 0.9409 (0.9014) acc_u 6.2500 (12.5000) lr 1.3971e-03 eta 0:00:12
epoch [76/200] batch [50/73] time 0.525 (0.443) data 0.394 (0.311) loss_u loss_u 0.8511 (0.8997) acc_u 21.8750 (12.6875) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [55/73] time 0.594 (0.443) data 0.463 (0.312) loss_u loss_u 0.8389 (0.8979) acc_u 18.7500 (12.8977) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [60/73] time 0.500 (0.444) data 0.368 (0.313) loss_u loss_u 0.8994 (0.8973) acc_u 12.5000 (12.9688) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [65/73] time 0.411 (0.444) data 0.280 (0.313) loss_u loss_u 0.8950 (0.8965) acc_u 15.6250 (13.0288) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [70/73] time 0.446 (0.445) data 0.313 (0.314) loss_u loss_u 0.8726 (0.8965) acc_u 18.7500 (13.1696) lr 1.3971e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1732
confident_label rate tensor(0.2615, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 820
clean true:636
clean false:184
clean_rate:0.775609756097561
noisy true:768
noisy false:1548
after delete: len(clean_dataset) 820
after delete: len(noisy_dataset) 2316
epoch [77/200] batch [5/25] time 0.408 (0.516) data 0.273 (0.384) loss_x loss_x 1.5791 (1.1512) acc_x 46.8750 (65.0000) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [10/25] time 0.453 (0.475) data 0.322 (0.343) loss_x loss_x 1.1318 (1.2718) acc_x 75.0000 (64.6875) lr 1.3827e-03 eta 0:00:07
epoch [77/200] batch [15/25] time 0.442 (0.460) data 0.312 (0.328) loss_x loss_x 1.4180 (1.2825) acc_x 53.1250 (65.2083) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [20/25] time 0.470 (0.454) data 0.338 (0.322) loss_x loss_x 1.4580 (1.2920) acc_x 68.7500 (65.9375) lr 1.3827e-03 eta 0:00:02
epoch [77/200] batch [25/25] time 0.489 (0.454) data 0.358 (0.322) loss_x loss_x 1.3984 (1.3452) acc_x 56.2500 (64.5000) lr 1.3827e-03 eta 0:00:00
epoch [77/200] batch [5/72] time 0.402 (0.447) data 0.270 (0.316) loss_u loss_u 0.8643 (0.8902) acc_u 18.7500 (12.5000) lr 1.3827e-03 eta 0:00:29
epoch [77/200] batch [10/72] time 0.600 (0.451) data 0.468 (0.320) loss_u loss_u 0.8770 (0.9062) acc_u 12.5000 (10.6250) lr 1.3827e-03 eta 0:00:27
epoch [77/200] batch [15/72] time 0.409 (0.449) data 0.274 (0.317) loss_u loss_u 0.9160 (0.9095) acc_u 9.3750 (10.8333) lr 1.3827e-03 eta 0:00:25
epoch [77/200] batch [20/72] time 0.397 (0.447) data 0.266 (0.316) loss_u loss_u 0.8906 (0.9090) acc_u 12.5000 (11.2500) lr 1.3827e-03 eta 0:00:23
epoch [77/200] batch [25/72] time 0.547 (0.448) data 0.414 (0.317) loss_u loss_u 0.8867 (0.9042) acc_u 12.5000 (11.6250) lr 1.3827e-03 eta 0:00:21
epoch [77/200] batch [30/72] time 0.358 (0.446) data 0.226 (0.315) loss_u loss_u 0.8599 (0.9043) acc_u 15.6250 (11.7708) lr 1.3827e-03 eta 0:00:18
epoch [77/200] batch [35/72] time 0.591 (0.446) data 0.457 (0.314) loss_u loss_u 0.9595 (0.9039) acc_u 6.2500 (11.6964) lr 1.3827e-03 eta 0:00:16
epoch [77/200] batch [40/72] time 0.374 (0.448) data 0.241 (0.316) loss_u loss_u 0.9492 (0.9057) acc_u 6.2500 (11.7969) lr 1.3827e-03 eta 0:00:14
epoch [77/200] batch [45/72] time 0.898 (0.456) data 0.767 (0.325) loss_u loss_u 0.8872 (0.9043) acc_u 12.5000 (12.1528) lr 1.3827e-03 eta 0:00:12
epoch [77/200] batch [50/72] time 0.438 (0.458) data 0.305 (0.326) loss_u loss_u 0.9502 (0.9063) acc_u 9.3750 (11.9375) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [55/72] time 0.385 (0.455) data 0.253 (0.323) loss_u loss_u 0.8872 (0.9063) acc_u 12.5000 (11.8750) lr 1.3827e-03 eta 0:00:07
epoch [77/200] batch [60/72] time 0.365 (0.451) data 0.233 (0.319) loss_u loss_u 0.9209 (0.9044) acc_u 9.3750 (12.2396) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [65/72] time 0.498 (0.450) data 0.366 (0.318) loss_u loss_u 0.8960 (0.9035) acc_u 6.2500 (12.2596) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [70/72] time 0.413 (0.448) data 0.281 (0.316) loss_u loss_u 0.9214 (0.9022) acc_u 6.2500 (12.3661) lr 1.3827e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1758
confident_label rate tensor(0.2423, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 760
clean true:590
clean false:170
clean_rate:0.7763157894736842
noisy true:788
noisy false:1588
after delete: len(clean_dataset) 760
after delete: len(noisy_dataset) 2376
epoch [78/200] batch [5/23] time 0.443 (0.459) data 0.312 (0.328) loss_x loss_x 1.0693 (1.3254) acc_x 78.1250 (67.5000) lr 1.3681e-03 eta 0:00:08
epoch [78/200] batch [10/23] time 0.609 (0.498) data 0.478 (0.366) loss_x loss_x 1.4404 (1.3258) acc_x 62.5000 (65.0000) lr 1.3681e-03 eta 0:00:06
epoch [78/200] batch [15/23] time 0.451 (0.466) data 0.320 (0.335) loss_x loss_x 1.0186 (1.3547) acc_x 71.8750 (64.1667) lr 1.3681e-03 eta 0:00:03
epoch [78/200] batch [20/23] time 0.452 (0.461) data 0.322 (0.330) loss_x loss_x 1.9629 (1.3797) acc_x 62.5000 (64.5312) lr 1.3681e-03 eta 0:00:01
epoch [78/200] batch [5/74] time 0.543 (0.463) data 0.412 (0.332) loss_u loss_u 0.8662 (0.8907) acc_u 15.6250 (13.7500) lr 1.3681e-03 eta 0:00:31
epoch [78/200] batch [10/74] time 0.373 (0.456) data 0.241 (0.325) loss_u loss_u 0.9102 (0.8940) acc_u 6.2500 (12.8125) lr 1.3681e-03 eta 0:00:29
epoch [78/200] batch [15/74] time 0.433 (0.453) data 0.301 (0.322) loss_u loss_u 0.9326 (0.8935) acc_u 12.5000 (13.1250) lr 1.3681e-03 eta 0:00:26
epoch [78/200] batch [20/74] time 0.516 (0.455) data 0.384 (0.324) loss_u loss_u 0.8882 (0.8994) acc_u 12.5000 (12.6562) lr 1.3681e-03 eta 0:00:24
epoch [78/200] batch [25/74] time 0.438 (0.451) data 0.306 (0.320) loss_u loss_u 0.8604 (0.8968) acc_u 21.8750 (12.7500) lr 1.3681e-03 eta 0:00:22
epoch [78/200] batch [30/74] time 0.493 (0.452) data 0.361 (0.321) loss_u loss_u 0.9087 (0.8906) acc_u 9.3750 (13.0208) lr 1.3681e-03 eta 0:00:19
epoch [78/200] batch [35/74] time 0.453 (0.452) data 0.321 (0.321) loss_u loss_u 0.8926 (0.8931) acc_u 12.5000 (12.4107) lr 1.3681e-03 eta 0:00:17
epoch [78/200] batch [40/74] time 0.375 (0.447) data 0.244 (0.316) loss_u loss_u 0.8940 (0.8964) acc_u 15.6250 (12.2656) lr 1.3681e-03 eta 0:00:15
epoch [78/200] batch [45/74] time 0.489 (0.449) data 0.358 (0.318) loss_u loss_u 0.9165 (0.8958) acc_u 9.3750 (12.4306) lr 1.3681e-03 eta 0:00:13
epoch [78/200] batch [50/74] time 0.350 (0.445) data 0.219 (0.314) loss_u loss_u 0.9023 (0.8965) acc_u 9.3750 (12.2500) lr 1.3681e-03 eta 0:00:10
epoch [78/200] batch [55/74] time 0.446 (0.444) data 0.315 (0.313) loss_u loss_u 0.9150 (0.8935) acc_u 9.3750 (12.7841) lr 1.3681e-03 eta 0:00:08
epoch [78/200] batch [60/74] time 0.363 (0.442) data 0.232 (0.310) loss_u loss_u 0.9155 (0.8956) acc_u 12.5000 (12.7083) lr 1.3681e-03 eta 0:00:06
epoch [78/200] batch [65/74] time 0.364 (0.441) data 0.233 (0.310) loss_u loss_u 0.9297 (0.8978) acc_u 6.2500 (12.4038) lr 1.3681e-03 eta 0:00:03
epoch [78/200] batch [70/74] time 0.500 (0.444) data 0.369 (0.313) loss_u loss_u 0.8350 (0.8967) acc_u 18.7500 (12.4107) lr 1.3681e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.2577, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 808
clean true:610
clean false:198
clean_rate:0.754950495049505
noisy true:777
noisy false:1551
after delete: len(clean_dataset) 808
after delete: len(noisy_dataset) 2328
epoch [79/200] batch [5/25] time 0.421 (0.426) data 0.291 (0.295) loss_x loss_x 1.0439 (1.3564) acc_x 71.8750 (66.2500) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [10/25] time 0.446 (0.423) data 0.316 (0.292) loss_x loss_x 0.9209 (1.2885) acc_x 65.6250 (65.6250) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [15/25] time 0.500 (0.453) data 0.370 (0.323) loss_x loss_x 1.0986 (1.3309) acc_x 62.5000 (64.7917) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [20/25] time 0.393 (0.463) data 0.262 (0.332) loss_x loss_x 0.9634 (1.2818) acc_x 78.1250 (66.8750) lr 1.3535e-03 eta 0:00:02
epoch [79/200] batch [25/25] time 0.612 (0.460) data 0.481 (0.329) loss_x loss_x 0.8584 (1.2547) acc_x 84.3750 (67.0000) lr 1.3535e-03 eta 0:00:00
epoch [79/200] batch [5/72] time 0.350 (0.459) data 0.219 (0.328) loss_u loss_u 0.9082 (0.8838) acc_u 12.5000 (13.1250) lr 1.3535e-03 eta 0:00:30
epoch [79/200] batch [10/72] time 0.548 (0.455) data 0.417 (0.325) loss_u loss_u 0.8691 (0.8855) acc_u 15.6250 (13.7500) lr 1.3535e-03 eta 0:00:28
epoch [79/200] batch [15/72] time 0.418 (0.447) data 0.287 (0.316) loss_u loss_u 0.8994 (0.8899) acc_u 12.5000 (12.9167) lr 1.3535e-03 eta 0:00:25
epoch [79/200] batch [20/72] time 0.462 (0.442) data 0.331 (0.311) loss_u loss_u 0.9155 (0.8917) acc_u 9.3750 (13.1250) lr 1.3535e-03 eta 0:00:22
epoch [79/200] batch [25/72] time 0.414 (0.441) data 0.283 (0.310) loss_u loss_u 0.8740 (0.8895) acc_u 21.8750 (14.0000) lr 1.3535e-03 eta 0:00:20
epoch [79/200] batch [30/72] time 0.549 (0.447) data 0.418 (0.316) loss_u loss_u 0.8823 (0.8878) acc_u 12.5000 (14.2708) lr 1.3535e-03 eta 0:00:18
epoch [79/200] batch [35/72] time 0.392 (0.448) data 0.261 (0.317) loss_u loss_u 0.8828 (0.8879) acc_u 15.6250 (14.1964) lr 1.3535e-03 eta 0:00:16
epoch [79/200] batch [40/72] time 0.370 (0.447) data 0.238 (0.316) loss_u loss_u 0.9331 (0.8897) acc_u 9.3750 (13.9062) lr 1.3535e-03 eta 0:00:14
epoch [79/200] batch [45/72] time 0.384 (0.446) data 0.253 (0.315) loss_u loss_u 0.9180 (0.8914) acc_u 9.3750 (13.6806) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [50/72] time 0.394 (0.448) data 0.262 (0.317) loss_u loss_u 0.9067 (0.8917) acc_u 9.3750 (13.4375) lr 1.3535e-03 eta 0:00:09
epoch [79/200] batch [55/72] time 0.412 (0.447) data 0.280 (0.316) loss_u loss_u 0.8423 (0.8909) acc_u 15.6250 (13.4659) lr 1.3535e-03 eta 0:00:07
epoch [79/200] batch [60/72] time 0.463 (0.450) data 0.331 (0.319) loss_u loss_u 0.8823 (0.8906) acc_u 15.6250 (13.5417) lr 1.3535e-03 eta 0:00:05
epoch [79/200] batch [65/72] time 0.460 (0.450) data 0.328 (0.319) loss_u loss_u 0.8994 (0.8929) acc_u 12.5000 (13.2212) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [70/72] time 0.501 (0.450) data 0.369 (0.319) loss_u loss_u 0.9009 (0.8948) acc_u 12.5000 (12.9911) lr 1.3535e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1743
confident_label rate tensor(0.2551, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 800
clean true:609
clean false:191
clean_rate:0.76125
noisy true:784
noisy false:1552
after delete: len(clean_dataset) 800
after delete: len(noisy_dataset) 2336
epoch [80/200] batch [5/25] time 0.456 (0.442) data 0.325 (0.311) loss_x loss_x 1.5049 (1.4430) acc_x 62.5000 (64.3750) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [10/25] time 0.421 (0.461) data 0.291 (0.330) loss_x loss_x 1.3408 (1.3664) acc_x 56.2500 (63.7500) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [15/25] time 0.462 (0.453) data 0.331 (0.322) loss_x loss_x 2.1367 (1.3758) acc_x 53.1250 (64.7917) lr 1.3387e-03 eta 0:00:04
epoch [80/200] batch [20/25] time 0.471 (0.456) data 0.340 (0.325) loss_x loss_x 1.2070 (1.3481) acc_x 65.6250 (65.1562) lr 1.3387e-03 eta 0:00:02
epoch [80/200] batch [25/25] time 0.432 (0.455) data 0.301 (0.324) loss_x loss_x 1.4414 (1.3373) acc_x 65.6250 (64.6250) lr 1.3387e-03 eta 0:00:00
epoch [80/200] batch [5/73] time 0.563 (0.457) data 0.431 (0.326) loss_u loss_u 0.9185 (0.8797) acc_u 9.3750 (15.6250) lr 1.3387e-03 eta 0:00:31
epoch [80/200] batch [10/73] time 0.500 (0.455) data 0.369 (0.324) loss_u loss_u 0.9326 (0.8933) acc_u 9.3750 (13.1250) lr 1.3387e-03 eta 0:00:28
epoch [80/200] batch [15/73] time 0.498 (0.463) data 0.367 (0.332) loss_u loss_u 0.9492 (0.8950) acc_u 3.1250 (12.5000) lr 1.3387e-03 eta 0:00:26
epoch [80/200] batch [20/73] time 0.386 (0.462) data 0.254 (0.331) loss_u loss_u 0.9380 (0.8956) acc_u 9.3750 (12.6562) lr 1.3387e-03 eta 0:00:24
epoch [80/200] batch [25/73] time 0.415 (0.462) data 0.281 (0.331) loss_u loss_u 0.8940 (0.8988) acc_u 12.5000 (11.7500) lr 1.3387e-03 eta 0:00:22
epoch [80/200] batch [30/73] time 0.372 (0.455) data 0.241 (0.324) loss_u loss_u 0.8374 (0.9016) acc_u 18.7500 (11.3542) lr 1.3387e-03 eta 0:00:19
epoch [80/200] batch [35/73] time 0.434 (0.456) data 0.302 (0.325) loss_u loss_u 0.8950 (0.9001) acc_u 9.3750 (11.7857) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [40/73] time 0.466 (0.454) data 0.334 (0.323) loss_u loss_u 0.9385 (0.9020) acc_u 3.1250 (11.4844) lr 1.3387e-03 eta 0:00:14
epoch [80/200] batch [45/73] time 0.492 (0.455) data 0.361 (0.323) loss_u loss_u 0.9478 (0.9028) acc_u 3.1250 (11.2500) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [50/73] time 0.599 (0.456) data 0.466 (0.324) loss_u loss_u 0.9009 (0.9009) acc_u 9.3750 (11.6250) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [55/73] time 0.398 (0.454) data 0.267 (0.322) loss_u loss_u 0.8828 (0.8979) acc_u 9.3750 (11.9318) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [60/73] time 0.567 (0.453) data 0.436 (0.322) loss_u loss_u 0.9375 (0.8981) acc_u 6.2500 (11.7188) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [65/73] time 0.363 (0.451) data 0.232 (0.320) loss_u loss_u 0.8584 (0.8977) acc_u 21.8750 (11.8750) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [70/73] time 0.461 (0.451) data 0.329 (0.320) loss_u loss_u 0.9155 (0.8966) acc_u 9.3750 (12.0089) lr 1.3387e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1764
confident_label rate tensor(0.2519, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 790
clean true:610
clean false:180
clean_rate:0.7721518987341772
noisy true:762
noisy false:1584
after delete: len(clean_dataset) 790
after delete: len(noisy_dataset) 2346
epoch [81/200] batch [5/24] time 0.385 (0.389) data 0.255 (0.258) loss_x loss_x 1.6953 (1.4297) acc_x 65.6250 (67.5000) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [10/24] time 0.424 (0.423) data 0.294 (0.292) loss_x loss_x 1.3174 (1.4245) acc_x 62.5000 (65.6250) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [15/24] time 0.392 (0.436) data 0.260 (0.305) loss_x loss_x 1.0479 (1.3521) acc_x 68.7500 (67.5000) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [20/24] time 0.436 (0.449) data 0.305 (0.318) loss_x loss_x 0.9717 (1.3740) acc_x 71.8750 (66.7188) lr 1.3239e-03 eta 0:00:01
epoch [81/200] batch [5/73] time 0.613 (0.446) data 0.482 (0.315) loss_u loss_u 0.8823 (0.9036) acc_u 12.5000 (10.6250) lr 1.3239e-03 eta 0:00:30
epoch [81/200] batch [10/73] time 0.484 (0.439) data 0.354 (0.308) loss_u loss_u 0.8999 (0.9190) acc_u 12.5000 (9.6875) lr 1.3239e-03 eta 0:00:27
epoch [81/200] batch [15/73] time 0.483 (0.434) data 0.351 (0.303) loss_u loss_u 0.9175 (0.9096) acc_u 6.2500 (10.2083) lr 1.3239e-03 eta 0:00:25
epoch [81/200] batch [20/73] time 0.459 (0.434) data 0.328 (0.303) loss_u loss_u 0.8853 (0.9029) acc_u 15.6250 (11.7188) lr 1.3239e-03 eta 0:00:23
epoch [81/200] batch [25/73] time 0.487 (0.440) data 0.356 (0.309) loss_u loss_u 0.8931 (0.9055) acc_u 12.5000 (11.2500) lr 1.3239e-03 eta 0:00:21
epoch [81/200] batch [30/73] time 0.360 (0.436) data 0.229 (0.304) loss_u loss_u 0.9106 (0.9055) acc_u 9.3750 (11.1458) lr 1.3239e-03 eta 0:00:18
epoch [81/200] batch [35/73] time 0.359 (0.435) data 0.228 (0.304) loss_u loss_u 0.8760 (0.9015) acc_u 18.7500 (11.9643) lr 1.3239e-03 eta 0:00:16
epoch [81/200] batch [40/73] time 0.508 (0.439) data 0.375 (0.308) loss_u loss_u 0.8799 (0.9012) acc_u 15.6250 (12.0312) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [45/73] time 0.365 (0.445) data 0.233 (0.314) loss_u loss_u 0.9058 (0.8996) acc_u 6.2500 (12.2222) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [50/73] time 0.439 (0.447) data 0.307 (0.316) loss_u loss_u 0.8701 (0.8999) acc_u 15.6250 (12.1875) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [55/73] time 0.589 (0.449) data 0.458 (0.318) loss_u loss_u 0.8540 (0.9008) acc_u 18.7500 (12.2727) lr 1.3239e-03 eta 0:00:08
epoch [81/200] batch [60/73] time 0.525 (0.451) data 0.394 (0.320) loss_u loss_u 0.8589 (0.9018) acc_u 21.8750 (12.2917) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [65/73] time 0.363 (0.451) data 0.231 (0.319) loss_u loss_u 0.9224 (0.9028) acc_u 9.3750 (12.0673) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [70/73] time 0.484 (0.448) data 0.353 (0.316) loss_u loss_u 0.8398 (0.9015) acc_u 21.8750 (12.1429) lr 1.3239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1768
confident_label rate tensor(0.2522, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 791
clean true:595
clean false:196
clean_rate:0.7522123893805309
noisy true:773
noisy false:1572
after delete: len(clean_dataset) 791
after delete: len(noisy_dataset) 2345
epoch [82/200] batch [5/24] time 0.520 (0.502) data 0.389 (0.371) loss_x loss_x 1.6582 (1.4305) acc_x 62.5000 (66.2500) lr 1.3090e-03 eta 0:00:09
epoch [82/200] batch [10/24] time 0.367 (0.469) data 0.236 (0.339) loss_x loss_x 1.2041 (1.2412) acc_x 75.0000 (70.0000) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [15/24] time 0.485 (0.477) data 0.355 (0.347) loss_x loss_x 1.9473 (1.2959) acc_x 53.1250 (69.3750) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [20/24] time 0.569 (0.477) data 0.439 (0.346) loss_x loss_x 1.2783 (1.3008) acc_x 75.0000 (69.5312) lr 1.3090e-03 eta 0:00:01
epoch [82/200] batch [5/73] time 0.404 (0.477) data 0.273 (0.346) loss_u loss_u 0.8965 (0.8795) acc_u 12.5000 (16.2500) lr 1.3090e-03 eta 0:00:32
epoch [82/200] batch [10/73] time 0.380 (0.474) data 0.249 (0.343) loss_u loss_u 0.8369 (0.8897) acc_u 21.8750 (14.3750) lr 1.3090e-03 eta 0:00:29
epoch [82/200] batch [15/73] time 0.594 (0.482) data 0.462 (0.351) loss_u loss_u 0.9355 (0.8921) acc_u 9.3750 (14.3750) lr 1.3090e-03 eta 0:00:27
epoch [82/200] batch [20/73] time 0.442 (0.474) data 0.311 (0.343) loss_u loss_u 0.9282 (0.8881) acc_u 9.3750 (14.8438) lr 1.3090e-03 eta 0:00:25
epoch [82/200] batch [25/73] time 0.395 (0.473) data 0.263 (0.342) loss_u loss_u 0.8994 (0.8844) acc_u 15.6250 (15.1250) lr 1.3090e-03 eta 0:00:22
epoch [82/200] batch [30/73] time 0.334 (0.466) data 0.202 (0.335) loss_u loss_u 0.8843 (0.8871) acc_u 18.7500 (14.6875) lr 1.3090e-03 eta 0:00:20
epoch [82/200] batch [35/73] time 0.476 (0.465) data 0.345 (0.334) loss_u loss_u 0.9272 (0.8901) acc_u 3.1250 (14.0179) lr 1.3090e-03 eta 0:00:17
epoch [82/200] batch [40/73] time 0.449 (0.462) data 0.318 (0.331) loss_u loss_u 0.8735 (0.8895) acc_u 18.7500 (13.9844) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [45/73] time 0.679 (0.465) data 0.547 (0.334) loss_u loss_u 0.9219 (0.8896) acc_u 9.3750 (13.8889) lr 1.3090e-03 eta 0:00:13
epoch [82/200] batch [50/73] time 0.424 (0.465) data 0.293 (0.334) loss_u loss_u 0.8823 (0.8914) acc_u 12.5000 (13.5000) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [55/73] time 0.473 (0.466) data 0.341 (0.335) loss_u loss_u 0.9497 (0.8931) acc_u 6.2500 (13.3523) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [60/73] time 0.387 (0.463) data 0.254 (0.332) loss_u loss_u 0.9272 (0.8949) acc_u 9.3750 (12.9688) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [65/73] time 0.503 (0.461) data 0.372 (0.329) loss_u loss_u 0.9204 (0.8954) acc_u 9.3750 (12.7885) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [70/73] time 0.402 (0.458) data 0.270 (0.327) loss_u loss_u 0.8911 (0.8954) acc_u 15.6250 (12.9464) lr 1.3090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1716
confident_label rate tensor(0.2618, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 821
clean true:622
clean false:199
clean_rate:0.7576126674786845
noisy true:798
noisy false:1517
after delete: len(clean_dataset) 821
after delete: len(noisy_dataset) 2315
epoch [83/200] batch [5/25] time 0.460 (0.455) data 0.329 (0.324) loss_x loss_x 1.0605 (1.3764) acc_x 71.8750 (65.6250) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [10/25] time 0.393 (0.435) data 0.262 (0.304) loss_x loss_x 1.3857 (1.3008) acc_x 71.8750 (67.1875) lr 1.2940e-03 eta 0:00:06
epoch [83/200] batch [15/25] time 0.373 (0.424) data 0.241 (0.293) loss_x loss_x 0.9478 (1.3104) acc_x 78.1250 (66.4583) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [20/25] time 0.456 (0.425) data 0.325 (0.294) loss_x loss_x 1.0381 (1.3031) acc_x 75.0000 (67.1875) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [25/25] time 0.391 (0.426) data 0.260 (0.295) loss_x loss_x 1.8984 (1.3351) acc_x 50.0000 (65.7500) lr 1.2940e-03 eta 0:00:00
epoch [83/200] batch [5/72] time 0.704 (0.425) data 0.574 (0.294) loss_u loss_u 0.9067 (0.8896) acc_u 15.6250 (15.6250) lr 1.2940e-03 eta 0:00:28
epoch [83/200] batch [10/72] time 0.478 (0.425) data 0.347 (0.294) loss_u loss_u 0.9194 (0.9119) acc_u 9.3750 (10.9375) lr 1.2940e-03 eta 0:00:26
epoch [83/200] batch [15/72] time 0.758 (0.442) data 0.627 (0.311) loss_u loss_u 0.9248 (0.9188) acc_u 12.5000 (10.0000) lr 1.2940e-03 eta 0:00:25
epoch [83/200] batch [20/72] time 0.551 (0.447) data 0.421 (0.316) loss_u loss_u 0.9258 (0.9241) acc_u 3.1250 (9.0625) lr 1.2940e-03 eta 0:00:23
epoch [83/200] batch [25/72] time 0.512 (0.454) data 0.379 (0.323) loss_u loss_u 0.9043 (0.9178) acc_u 18.7500 (10.1250) lr 1.2940e-03 eta 0:00:21
epoch [83/200] batch [30/72] time 0.422 (0.453) data 0.290 (0.322) loss_u loss_u 0.9229 (0.9161) acc_u 6.2500 (10.4167) lr 1.2940e-03 eta 0:00:19
epoch [83/200] batch [35/72] time 0.541 (0.454) data 0.410 (0.322) loss_u loss_u 0.8584 (0.9104) acc_u 15.6250 (10.8036) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [40/72] time 0.419 (0.450) data 0.289 (0.319) loss_u loss_u 0.8525 (0.9039) acc_u 21.8750 (11.5625) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [45/72] time 0.443 (0.450) data 0.312 (0.319) loss_u loss_u 0.9136 (0.9019) acc_u 9.3750 (11.9444) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [50/72] time 0.358 (0.450) data 0.226 (0.318) loss_u loss_u 0.9336 (0.9027) acc_u 6.2500 (11.8750) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [55/72] time 0.383 (0.448) data 0.251 (0.316) loss_u loss_u 0.9292 (0.9064) acc_u 6.2500 (11.4205) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [60/72] time 0.415 (0.446) data 0.284 (0.315) loss_u loss_u 0.9126 (0.9074) acc_u 9.3750 (11.2500) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [65/72] time 0.462 (0.449) data 0.331 (0.318) loss_u loss_u 0.8979 (0.9065) acc_u 15.6250 (11.5865) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [70/72] time 0.510 (0.449) data 0.378 (0.317) loss_u loss_u 0.8574 (0.9048) acc_u 18.7500 (11.7857) lr 1.2940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1754
confident_label rate tensor(0.2554, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 801
clean true:619
clean false:182
clean_rate:0.7727840199750312
noisy true:763
noisy false:1572
after delete: len(clean_dataset) 801
after delete: len(noisy_dataset) 2335
epoch [84/200] batch [5/25] time 0.482 (0.493) data 0.351 (0.362) loss_x loss_x 1.2764 (1.4230) acc_x 71.8750 (63.7500) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [10/25] time 0.395 (0.480) data 0.265 (0.349) loss_x loss_x 1.8232 (1.5063) acc_x 59.3750 (62.1875) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [15/25] time 0.428 (0.458) data 0.297 (0.327) loss_x loss_x 1.2412 (1.4545) acc_x 62.5000 (63.9583) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [20/25] time 0.582 (0.478) data 0.451 (0.347) loss_x loss_x 1.3242 (1.4288) acc_x 62.5000 (64.8438) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [25/25] time 0.449 (0.475) data 0.318 (0.345) loss_x loss_x 1.8418 (1.4017) acc_x 46.8750 (64.2500) lr 1.2790e-03 eta 0:00:00
epoch [84/200] batch [5/72] time 0.449 (0.468) data 0.319 (0.337) loss_u loss_u 0.8901 (0.8892) acc_u 12.5000 (13.1250) lr 1.2790e-03 eta 0:00:31
epoch [84/200] batch [10/72] time 0.576 (0.468) data 0.445 (0.337) loss_u loss_u 0.9473 (0.9052) acc_u 3.1250 (10.6250) lr 1.2790e-03 eta 0:00:29
epoch [84/200] batch [15/72] time 0.517 (0.465) data 0.387 (0.334) loss_u loss_u 0.9268 (0.9034) acc_u 6.2500 (11.2500) lr 1.2790e-03 eta 0:00:26
epoch [84/200] batch [20/72] time 0.401 (0.462) data 0.269 (0.331) loss_u loss_u 0.8853 (0.9033) acc_u 12.5000 (11.4062) lr 1.2790e-03 eta 0:00:24
epoch [84/200] batch [25/72] time 0.479 (0.460) data 0.347 (0.329) loss_u loss_u 0.9131 (0.9042) acc_u 6.2500 (10.8750) lr 1.2790e-03 eta 0:00:21
epoch [84/200] batch [30/72] time 0.474 (0.458) data 0.342 (0.327) loss_u loss_u 0.9180 (0.9054) acc_u 6.2500 (10.5208) lr 1.2790e-03 eta 0:00:19
epoch [84/200] batch [35/72] time 0.533 (0.459) data 0.401 (0.328) loss_u loss_u 0.8770 (0.8972) acc_u 18.7500 (11.6071) lr 1.2790e-03 eta 0:00:16
epoch [84/200] batch [40/72] time 0.391 (0.452) data 0.260 (0.321) loss_u loss_u 0.9131 (0.9007) acc_u 6.2500 (11.3281) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [45/72] time 0.421 (0.449) data 0.290 (0.318) loss_u loss_u 0.8765 (0.8980) acc_u 12.5000 (11.6667) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [50/72] time 0.389 (0.447) data 0.258 (0.316) loss_u loss_u 0.9497 (0.8985) acc_u 6.2500 (11.6250) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [55/72] time 0.395 (0.445) data 0.262 (0.314) loss_u loss_u 0.8730 (0.8994) acc_u 12.5000 (11.5909) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [60/72] time 0.482 (0.450) data 0.350 (0.318) loss_u loss_u 0.7939 (0.8976) acc_u 25.0000 (11.8229) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [65/72] time 0.504 (0.449) data 0.373 (0.318) loss_u loss_u 0.9302 (0.8989) acc_u 9.3750 (11.7308) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [70/72] time 0.470 (0.452) data 0.338 (0.320) loss_u loss_u 0.8789 (0.8965) acc_u 15.6250 (12.0982) lr 1.2790e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1772
confident_label rate tensor(0.2650, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 831
clean true:619
clean false:212
clean_rate:0.7448856799037304
noisy true:745
noisy false:1560
after delete: len(clean_dataset) 831
after delete: len(noisy_dataset) 2305
epoch [85/200] batch [5/25] time 0.511 (0.427) data 0.381 (0.296) loss_x loss_x 1.5537 (1.1811) acc_x 59.3750 (66.2500) lr 1.2639e-03 eta 0:00:08
epoch [85/200] batch [10/25] time 0.470 (0.448) data 0.340 (0.317) loss_x loss_x 1.0039 (1.1975) acc_x 68.7500 (66.5625) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [15/25] time 0.513 (0.466) data 0.383 (0.335) loss_x loss_x 2.0957 (1.2751) acc_x 46.8750 (65.8333) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [20/25] time 0.426 (0.467) data 0.296 (0.336) loss_x loss_x 1.7002 (1.2714) acc_x 59.3750 (66.4062) lr 1.2639e-03 eta 0:00:02
epoch [85/200] batch [25/25] time 0.400 (0.456) data 0.270 (0.326) loss_x loss_x 1.3037 (1.2895) acc_x 62.5000 (65.6250) lr 1.2639e-03 eta 0:00:00
epoch [85/200] batch [5/72] time 0.496 (0.459) data 0.365 (0.328) loss_u loss_u 0.8179 (0.8923) acc_u 25.0000 (14.3750) lr 1.2639e-03 eta 0:00:30
epoch [85/200] batch [10/72] time 0.418 (0.463) data 0.287 (0.332) loss_u loss_u 0.7939 (0.8843) acc_u 21.8750 (15.6250) lr 1.2639e-03 eta 0:00:28
epoch [85/200] batch [15/72] time 0.476 (0.459) data 0.344 (0.328) loss_u loss_u 0.8940 (0.8859) acc_u 12.5000 (14.7917) lr 1.2639e-03 eta 0:00:26
epoch [85/200] batch [20/72] time 0.402 (0.459) data 0.271 (0.328) loss_u loss_u 0.9170 (0.8862) acc_u 9.3750 (14.0625) lr 1.2639e-03 eta 0:00:23
epoch [85/200] batch [25/72] time 0.378 (0.455) data 0.247 (0.324) loss_u loss_u 0.8247 (0.8893) acc_u 18.7500 (13.5000) lr 1.2639e-03 eta 0:00:21
epoch [85/200] batch [30/72] time 0.445 (0.452) data 0.313 (0.321) loss_u loss_u 0.9438 (0.8948) acc_u 6.2500 (12.7083) lr 1.2639e-03 eta 0:00:18
epoch [85/200] batch [35/72] time 0.503 (0.453) data 0.372 (0.322) loss_u loss_u 0.9722 (0.8978) acc_u 0.0000 (12.3214) lr 1.2639e-03 eta 0:00:16
epoch [85/200] batch [40/72] time 0.340 (0.449) data 0.209 (0.318) loss_u loss_u 0.8643 (0.8962) acc_u 15.6250 (12.8125) lr 1.2639e-03 eta 0:00:14
epoch [85/200] batch [45/72] time 0.465 (0.452) data 0.333 (0.321) loss_u loss_u 0.8809 (0.8963) acc_u 12.5000 (13.0556) lr 1.2639e-03 eta 0:00:12
epoch [85/200] batch [50/72] time 0.416 (0.447) data 0.285 (0.316) loss_u loss_u 0.8604 (0.8954) acc_u 12.5000 (13.1250) lr 1.2639e-03 eta 0:00:09
epoch [85/200] batch [55/72] time 0.396 (0.448) data 0.265 (0.317) loss_u loss_u 0.9194 (0.8954) acc_u 9.3750 (13.1250) lr 1.2639e-03 eta 0:00:07
epoch [85/200] batch [60/72] time 0.393 (0.447) data 0.262 (0.316) loss_u loss_u 0.9502 (0.8964) acc_u 3.1250 (12.9167) lr 1.2639e-03 eta 0:00:05
epoch [85/200] batch [65/72] time 0.566 (0.447) data 0.436 (0.316) loss_u loss_u 0.8828 (0.8955) acc_u 12.5000 (13.0769) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [70/72] time 0.367 (0.448) data 0.235 (0.316) loss_u loss_u 0.9497 (0.8962) acc_u 3.1250 (12.9464) lr 1.2639e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1786
confident_label rate tensor(0.2551, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 800
clean true:592
clean false:208
clean_rate:0.74
noisy true:758
noisy false:1578
after delete: len(clean_dataset) 800
after delete: len(noisy_dataset) 2336
epoch [86/200] batch [5/25] time 0.383 (0.492) data 0.252 (0.361) loss_x loss_x 0.8926 (1.1716) acc_x 75.0000 (69.3750) lr 1.2487e-03 eta 0:00:09
epoch [86/200] batch [10/25] time 0.367 (0.454) data 0.236 (0.323) loss_x loss_x 0.7729 (1.1559) acc_x 90.6250 (71.2500) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [15/25] time 0.413 (0.465) data 0.283 (0.334) loss_x loss_x 1.3271 (1.1772) acc_x 65.6250 (70.4167) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [20/25] time 0.428 (0.468) data 0.296 (0.337) loss_x loss_x 1.2041 (1.2068) acc_x 75.0000 (69.3750) lr 1.2487e-03 eta 0:00:02
epoch [86/200] batch [25/25] time 0.430 (0.477) data 0.300 (0.346) loss_x loss_x 1.8389 (1.2672) acc_x 56.2500 (69.1250) lr 1.2487e-03 eta 0:00:00
epoch [86/200] batch [5/73] time 0.389 (0.475) data 0.257 (0.343) loss_u loss_u 0.8677 (0.9032) acc_u 18.7500 (11.8750) lr 1.2487e-03 eta 0:00:32
epoch [86/200] batch [10/73] time 0.362 (0.466) data 0.230 (0.335) loss_u loss_u 0.9463 (0.9157) acc_u 9.3750 (10.6250) lr 1.2487e-03 eta 0:00:29
epoch [86/200] batch [15/73] time 0.561 (0.461) data 0.429 (0.330) loss_u loss_u 0.8750 (0.9045) acc_u 18.7500 (12.2917) lr 1.2487e-03 eta 0:00:26
epoch [86/200] batch [20/73] time 0.442 (0.461) data 0.310 (0.330) loss_u loss_u 0.9565 (0.9019) acc_u 3.1250 (12.5000) lr 1.2487e-03 eta 0:00:24
epoch [86/200] batch [25/73] time 0.362 (0.462) data 0.231 (0.331) loss_u loss_u 0.9453 (0.9042) acc_u 6.2500 (12.3750) lr 1.2487e-03 eta 0:00:22
epoch [86/200] batch [30/73] time 0.545 (0.460) data 0.413 (0.328) loss_u loss_u 0.8828 (0.8994) acc_u 12.5000 (13.2292) lr 1.2487e-03 eta 0:00:19
epoch [86/200] batch [35/73] time 0.368 (0.453) data 0.236 (0.322) loss_u loss_u 0.9111 (0.9029) acc_u 9.3750 (12.5893) lr 1.2487e-03 eta 0:00:17
epoch [86/200] batch [40/73] time 0.382 (0.448) data 0.250 (0.317) loss_u loss_u 0.9067 (0.9029) acc_u 9.3750 (12.4219) lr 1.2487e-03 eta 0:00:14
epoch [86/200] batch [45/73] time 0.385 (0.446) data 0.254 (0.315) loss_u loss_u 0.9512 (0.9016) acc_u 6.2500 (12.7083) lr 1.2487e-03 eta 0:00:12
epoch [86/200] batch [50/73] time 0.453 (0.448) data 0.321 (0.316) loss_u loss_u 0.9233 (0.8997) acc_u 9.3750 (13.0625) lr 1.2487e-03 eta 0:00:10
epoch [86/200] batch [55/73] time 0.409 (0.450) data 0.277 (0.319) loss_u loss_u 0.8740 (0.9002) acc_u 12.5000 (13.0114) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [60/73] time 0.432 (0.451) data 0.300 (0.319) loss_u loss_u 0.9365 (0.8992) acc_u 9.3750 (13.0208) lr 1.2487e-03 eta 0:00:05
epoch [86/200] batch [65/73] time 0.549 (0.452) data 0.417 (0.321) loss_u loss_u 0.9185 (0.9001) acc_u 9.3750 (12.9808) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [70/73] time 0.441 (0.452) data 0.309 (0.320) loss_u loss_u 0.8901 (0.8992) acc_u 15.6250 (13.1250) lr 1.2487e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1766
confident_label rate tensor(0.2656, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 833
clean true:637
clean false:196
clean_rate:0.7647058823529411
noisy true:733
noisy false:1570
after delete: len(clean_dataset) 833
after delete: len(noisy_dataset) 2303
epoch [87/200] batch [5/26] time 0.454 (0.465) data 0.323 (0.335) loss_x loss_x 1.3291 (1.1480) acc_x 71.8750 (70.6250) lr 1.2334e-03 eta 0:00:09
epoch [87/200] batch [10/26] time 0.368 (0.439) data 0.238 (0.308) loss_x loss_x 1.2656 (1.1412) acc_x 68.7500 (71.2500) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [15/26] time 0.461 (0.425) data 0.331 (0.294) loss_x loss_x 1.0479 (1.1150) acc_x 65.6250 (71.0417) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [20/26] time 0.413 (0.422) data 0.282 (0.291) loss_x loss_x 1.2607 (1.1531) acc_x 65.6250 (71.0938) lr 1.2334e-03 eta 0:00:02
epoch [87/200] batch [25/26] time 0.479 (0.426) data 0.348 (0.296) loss_x loss_x 0.9888 (1.1896) acc_x 84.3750 (70.6250) lr 1.2334e-03 eta 0:00:00
epoch [87/200] batch [5/71] time 0.403 (0.441) data 0.271 (0.310) loss_u loss_u 0.9492 (0.9019) acc_u 6.2500 (13.1250) lr 1.2334e-03 eta 0:00:29
epoch [87/200] batch [10/71] time 0.379 (0.442) data 0.248 (0.311) loss_u loss_u 0.9053 (0.9069) acc_u 12.5000 (11.8750) lr 1.2334e-03 eta 0:00:26
epoch [87/200] batch [15/71] time 0.549 (0.445) data 0.418 (0.314) loss_u loss_u 0.9009 (0.9118) acc_u 15.6250 (11.2500) lr 1.2334e-03 eta 0:00:24
epoch [87/200] batch [20/71] time 0.533 (0.450) data 0.402 (0.319) loss_u loss_u 0.9023 (0.9079) acc_u 9.3750 (11.2500) lr 1.2334e-03 eta 0:00:22
epoch [87/200] batch [25/71] time 0.460 (0.449) data 0.329 (0.318) loss_u loss_u 0.9077 (0.9039) acc_u 9.3750 (12.0000) lr 1.2334e-03 eta 0:00:20
epoch [87/200] batch [30/71] time 0.365 (0.447) data 0.234 (0.316) loss_u loss_u 0.9355 (0.9055) acc_u 6.2500 (11.6667) lr 1.2334e-03 eta 0:00:18
epoch [87/200] batch [35/71] time 0.480 (0.447) data 0.349 (0.316) loss_u loss_u 0.9224 (0.9033) acc_u 9.3750 (11.6964) lr 1.2334e-03 eta 0:00:16
epoch [87/200] batch [40/71] time 0.387 (0.441) data 0.256 (0.310) loss_u loss_u 0.8779 (0.8985) acc_u 15.6250 (12.5781) lr 1.2334e-03 eta 0:00:13
epoch [87/200] batch [45/71] time 0.556 (0.441) data 0.424 (0.309) loss_u loss_u 0.9121 (0.8973) acc_u 12.5000 (12.7083) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [50/71] time 0.386 (0.441) data 0.255 (0.310) loss_u loss_u 0.8159 (0.8955) acc_u 25.0000 (12.9375) lr 1.2334e-03 eta 0:00:09
epoch [87/200] batch [55/71] time 0.475 (0.442) data 0.344 (0.310) loss_u loss_u 0.8843 (0.8949) acc_u 18.7500 (13.1250) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [60/71] time 0.547 (0.444) data 0.416 (0.313) loss_u loss_u 0.9004 (0.8936) acc_u 12.5000 (13.2292) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [65/71] time 0.591 (0.446) data 0.461 (0.315) loss_u loss_u 0.9404 (0.8936) acc_u 3.1250 (13.0288) lr 1.2334e-03 eta 0:00:02
epoch [87/200] batch [70/71] time 0.443 (0.445) data 0.313 (0.314) loss_u loss_u 0.8647 (0.8946) acc_u 15.6250 (12.8125) lr 1.2334e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1732
confident_label rate tensor(0.2682, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 841
clean true:637
clean false:204
clean_rate:0.7574316290130797
noisy true:767
noisy false:1528
after delete: len(clean_dataset) 841
after delete: len(noisy_dataset) 2295
epoch [88/200] batch [5/26] time 0.481 (0.481) data 0.350 (0.350) loss_x loss_x 1.2559 (1.2436) acc_x 75.0000 (68.1250) lr 1.2181e-03 eta 0:00:10
epoch [88/200] batch [10/26] time 0.574 (0.478) data 0.443 (0.347) loss_x loss_x 1.3643 (1.3171) acc_x 65.6250 (66.8750) lr 1.2181e-03 eta 0:00:07
epoch [88/200] batch [15/26] time 0.366 (0.484) data 0.235 (0.354) loss_x loss_x 0.9272 (1.3510) acc_x 81.2500 (66.4583) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [20/26] time 0.564 (0.482) data 0.434 (0.351) loss_x loss_x 1.2930 (1.3127) acc_x 68.7500 (66.5625) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [25/26] time 0.404 (0.473) data 0.274 (0.342) loss_x loss_x 1.4062 (1.3036) acc_x 56.2500 (66.2500) lr 1.2181e-03 eta 0:00:00
epoch [88/200] batch [5/71] time 0.524 (0.468) data 0.391 (0.337) loss_u loss_u 0.8320 (0.8602) acc_u 21.8750 (16.8750) lr 1.2181e-03 eta 0:00:30
epoch [88/200] batch [10/71] time 0.340 (0.463) data 0.207 (0.333) loss_u loss_u 0.8911 (0.8812) acc_u 9.3750 (14.3750) lr 1.2181e-03 eta 0:00:28
epoch [88/200] batch [15/71] time 0.379 (0.466) data 0.247 (0.334) loss_u loss_u 0.8687 (0.8900) acc_u 18.7500 (13.9583) lr 1.2181e-03 eta 0:00:26
epoch [88/200] batch [20/71] time 0.373 (0.457) data 0.241 (0.326) loss_u loss_u 0.9248 (0.8926) acc_u 6.2500 (13.7500) lr 1.2181e-03 eta 0:00:23
epoch [88/200] batch [25/71] time 0.400 (0.459) data 0.269 (0.328) loss_u loss_u 0.8740 (0.8885) acc_u 18.7500 (14.5000) lr 1.2181e-03 eta 0:00:21
epoch [88/200] batch [30/71] time 0.436 (0.461) data 0.305 (0.330) loss_u loss_u 0.9546 (0.8891) acc_u 6.2500 (14.4792) lr 1.2181e-03 eta 0:00:18
epoch [88/200] batch [35/71] time 0.481 (0.463) data 0.350 (0.332) loss_u loss_u 0.9136 (0.8864) acc_u 6.2500 (14.3750) lr 1.2181e-03 eta 0:00:16
epoch [88/200] batch [40/71] time 0.479 (0.461) data 0.346 (0.329) loss_u loss_u 0.9004 (0.8863) acc_u 9.3750 (14.2969) lr 1.2181e-03 eta 0:00:14
epoch [88/200] batch [45/71] time 0.428 (0.459) data 0.295 (0.328) loss_u loss_u 0.9492 (0.8886) acc_u 6.2500 (13.8194) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [50/71] time 0.427 (0.459) data 0.295 (0.327) loss_u loss_u 0.9116 (0.8862) acc_u 6.2500 (14.1250) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [55/71] time 0.676 (0.462) data 0.544 (0.330) loss_u loss_u 0.8955 (0.8871) acc_u 15.6250 (14.0341) lr 1.2181e-03 eta 0:00:07
epoch [88/200] batch [60/71] time 0.449 (0.459) data 0.316 (0.327) loss_u loss_u 0.8848 (0.8888) acc_u 12.5000 (13.8542) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [65/71] time 0.704 (0.463) data 0.572 (0.332) loss_u loss_u 0.8643 (0.8894) acc_u 18.7500 (13.7019) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [70/71] time 0.349 (0.463) data 0.218 (0.331) loss_u loss_u 0.9482 (0.8935) acc_u 6.2500 (13.1250) lr 1.2181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1799
confident_label rate tensor(0.2503, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 785
clean true:595
clean false:190
clean_rate:0.7579617834394905
noisy true:742
noisy false:1609
after delete: len(clean_dataset) 785
after delete: len(noisy_dataset) 2351
epoch [89/200] batch [5/24] time 0.467 (0.469) data 0.336 (0.338) loss_x loss_x 1.3438 (1.2798) acc_x 71.8750 (72.5000) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [10/24] time 0.543 (0.513) data 0.412 (0.383) loss_x loss_x 1.1797 (1.2152) acc_x 68.7500 (71.2500) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [15/24] time 0.389 (0.480) data 0.258 (0.349) loss_x loss_x 1.2900 (1.2246) acc_x 75.0000 (70.4167) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [20/24] time 0.359 (0.469) data 0.228 (0.339) loss_x loss_x 1.4199 (1.2786) acc_x 59.3750 (68.7500) lr 1.2028e-03 eta 0:00:01
epoch [89/200] batch [5/73] time 0.445 (0.462) data 0.315 (0.331) loss_u loss_u 0.8828 (0.9038) acc_u 18.7500 (13.1250) lr 1.2028e-03 eta 0:00:31
epoch [89/200] batch [10/73] time 0.327 (0.451) data 0.195 (0.320) loss_u loss_u 0.8940 (0.8852) acc_u 12.5000 (15.6250) lr 1.2028e-03 eta 0:00:28
epoch [89/200] batch [15/73] time 0.439 (0.451) data 0.307 (0.320) loss_u loss_u 0.9409 (0.8976) acc_u 0.0000 (12.5000) lr 1.2028e-03 eta 0:00:26
epoch [89/200] batch [20/73] time 0.425 (0.453) data 0.294 (0.322) loss_u loss_u 0.9194 (0.8970) acc_u 3.1250 (12.5000) lr 1.2028e-03 eta 0:00:24
epoch [89/200] batch [25/73] time 0.412 (0.452) data 0.281 (0.320) loss_u loss_u 0.9453 (0.8957) acc_u 6.2500 (12.5000) lr 1.2028e-03 eta 0:00:21
epoch [89/200] batch [30/73] time 0.388 (0.450) data 0.256 (0.319) loss_u loss_u 0.9580 (0.8983) acc_u 0.0000 (11.7708) lr 1.2028e-03 eta 0:00:19
epoch [89/200] batch [35/73] time 0.383 (0.449) data 0.250 (0.318) loss_u loss_u 0.9180 (0.8993) acc_u 12.5000 (11.9643) lr 1.2028e-03 eta 0:00:17
epoch [89/200] batch [40/73] time 0.505 (0.447) data 0.373 (0.316) loss_u loss_u 0.8740 (0.9008) acc_u 15.6250 (11.6406) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [45/73] time 0.462 (0.445) data 0.331 (0.313) loss_u loss_u 0.9292 (0.8963) acc_u 6.2500 (12.3611) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [50/73] time 0.376 (0.445) data 0.244 (0.314) loss_u loss_u 0.8979 (0.8972) acc_u 12.5000 (12.3125) lr 1.2028e-03 eta 0:00:10
epoch [89/200] batch [55/73] time 0.311 (0.442) data 0.179 (0.311) loss_u loss_u 0.8789 (0.8954) acc_u 15.6250 (12.5000) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [60/73] time 0.463 (0.442) data 0.332 (0.310) loss_u loss_u 0.8877 (0.8946) acc_u 9.3750 (12.4479) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [65/73] time 0.582 (0.445) data 0.449 (0.314) loss_u loss_u 0.8530 (0.8941) acc_u 15.6250 (12.5962) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [70/73] time 0.453 (0.445) data 0.322 (0.313) loss_u loss_u 0.9160 (0.8946) acc_u 15.6250 (12.6786) lr 1.2028e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1792
confident_label rate tensor(0.2494, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 782
clean true:585
clean false:197
clean_rate:0.7480818414322251
noisy true:759
noisy false:1595
after delete: len(clean_dataset) 782
after delete: len(noisy_dataset) 2354
epoch [90/200] batch [5/24] time 0.505 (0.469) data 0.374 (0.338) loss_x loss_x 0.6997 (1.1019) acc_x 81.2500 (72.5000) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [10/24] time 0.383 (0.497) data 0.253 (0.366) loss_x loss_x 1.1250 (1.2473) acc_x 71.8750 (68.4375) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [15/24] time 0.497 (0.475) data 0.366 (0.344) loss_x loss_x 1.6865 (1.2617) acc_x 65.6250 (68.3333) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [20/24] time 0.357 (0.465) data 0.226 (0.334) loss_x loss_x 1.4658 (1.3034) acc_x 62.5000 (67.5000) lr 1.1874e-03 eta 0:00:01
epoch [90/200] batch [5/73] time 0.499 (0.457) data 0.368 (0.327) loss_u loss_u 0.8872 (0.8977) acc_u 15.6250 (12.5000) lr 1.1874e-03 eta 0:00:31
epoch [90/200] batch [10/73] time 0.412 (0.453) data 0.281 (0.322) loss_u loss_u 0.8647 (0.8935) acc_u 15.6250 (12.8125) lr 1.1874e-03 eta 0:00:28
epoch [90/200] batch [15/73] time 0.434 (0.448) data 0.303 (0.316) loss_u loss_u 0.8848 (0.8901) acc_u 12.5000 (13.5417) lr 1.1874e-03 eta 0:00:25
epoch [90/200] batch [20/73] time 0.393 (0.449) data 0.262 (0.318) loss_u loss_u 0.9087 (0.8859) acc_u 12.5000 (14.2188) lr 1.1874e-03 eta 0:00:23
epoch [90/200] batch [25/73] time 0.452 (0.447) data 0.321 (0.316) loss_u loss_u 0.8970 (0.8874) acc_u 6.2500 (13.5000) lr 1.1874e-03 eta 0:00:21
epoch [90/200] batch [30/73] time 0.387 (0.449) data 0.255 (0.318) loss_u loss_u 0.8599 (0.8861) acc_u 18.7500 (13.5417) lr 1.1874e-03 eta 0:00:19
epoch [90/200] batch [35/73] time 0.449 (0.447) data 0.317 (0.316) loss_u loss_u 0.8760 (0.8871) acc_u 12.5000 (13.0357) lr 1.1874e-03 eta 0:00:16
epoch [90/200] batch [40/73] time 0.513 (0.447) data 0.381 (0.316) loss_u loss_u 0.8159 (0.8858) acc_u 31.2500 (13.6719) lr 1.1874e-03 eta 0:00:14
epoch [90/200] batch [45/73] time 0.503 (0.448) data 0.371 (0.317) loss_u loss_u 0.8452 (0.8829) acc_u 25.0000 (14.2361) lr 1.1874e-03 eta 0:00:12
epoch [90/200] batch [50/73] time 0.335 (0.446) data 0.204 (0.315) loss_u loss_u 0.8335 (0.8808) acc_u 15.6250 (14.5000) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [55/73] time 0.385 (0.447) data 0.254 (0.316) loss_u loss_u 0.8882 (0.8831) acc_u 15.6250 (14.1477) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [60/73] time 0.463 (0.448) data 0.331 (0.317) loss_u loss_u 0.8638 (0.8840) acc_u 12.5000 (13.9583) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [65/73] time 0.406 (0.447) data 0.274 (0.316) loss_u loss_u 0.8662 (0.8844) acc_u 15.6250 (13.8462) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [70/73] time 0.355 (0.445) data 0.223 (0.314) loss_u loss_u 0.8389 (0.8862) acc_u 18.7500 (13.6607) lr 1.1874e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1748
confident_label rate tensor(0.2592, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 813
clean true:608
clean false:205
clean_rate:0.7478474784747847
noisy true:780
noisy false:1543
after delete: len(clean_dataset) 813
after delete: len(noisy_dataset) 2323
epoch [91/200] batch [5/25] time 0.407 (0.445) data 0.275 (0.314) loss_x loss_x 1.2402 (1.2264) acc_x 78.1250 (74.3750) lr 1.1719e-03 eta 0:00:08
epoch [91/200] batch [10/25] time 0.471 (0.446) data 0.340 (0.315) loss_x loss_x 0.9307 (1.2202) acc_x 78.1250 (72.5000) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [15/25] time 0.430 (0.460) data 0.299 (0.330) loss_x loss_x 1.2256 (1.2463) acc_x 62.5000 (70.0000) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [20/25] time 0.477 (0.455) data 0.346 (0.324) loss_x loss_x 1.0576 (1.2469) acc_x 62.5000 (69.5312) lr 1.1719e-03 eta 0:00:02
epoch [91/200] batch [25/25] time 0.482 (0.456) data 0.351 (0.325) loss_x loss_x 1.0498 (1.2362) acc_x 65.6250 (69.0000) lr 1.1719e-03 eta 0:00:00
epoch [91/200] batch [5/72] time 0.432 (0.452) data 0.301 (0.321) loss_u loss_u 0.9287 (0.8905) acc_u 6.2500 (13.7500) lr 1.1719e-03 eta 0:00:30
epoch [91/200] batch [10/72] time 0.387 (0.443) data 0.257 (0.312) loss_u loss_u 0.9204 (0.9067) acc_u 3.1250 (10.0000) lr 1.1719e-03 eta 0:00:27
epoch [91/200] batch [15/72] time 0.378 (0.437) data 0.245 (0.306) loss_u loss_u 0.9062 (0.9069) acc_u 12.5000 (10.2083) lr 1.1719e-03 eta 0:00:24
epoch [91/200] batch [20/72] time 0.482 (0.439) data 0.350 (0.308) loss_u loss_u 0.7690 (0.9048) acc_u 25.0000 (10.4688) lr 1.1719e-03 eta 0:00:22
epoch [91/200] batch [25/72] time 0.422 (0.433) data 0.290 (0.302) loss_u loss_u 0.8789 (0.8957) acc_u 18.7500 (12.3750) lr 1.1719e-03 eta 0:00:20
epoch [91/200] batch [30/72] time 0.511 (0.436) data 0.379 (0.305) loss_u loss_u 0.8979 (0.8991) acc_u 15.6250 (12.1875) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [35/72] time 0.412 (0.442) data 0.281 (0.310) loss_u loss_u 0.9238 (0.9013) acc_u 9.3750 (11.8750) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [40/72] time 0.485 (0.443) data 0.354 (0.312) loss_u loss_u 0.9272 (0.9017) acc_u 9.3750 (11.6406) lr 1.1719e-03 eta 0:00:14
epoch [91/200] batch [45/72] time 0.611 (0.441) data 0.480 (0.310) loss_u loss_u 0.8916 (0.8975) acc_u 12.5000 (12.2917) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [50/72] time 0.360 (0.438) data 0.229 (0.307) loss_u loss_u 0.9189 (0.8986) acc_u 12.5000 (12.1250) lr 1.1719e-03 eta 0:00:09
epoch [91/200] batch [55/72] time 0.411 (0.439) data 0.281 (0.308) loss_u loss_u 0.9302 (0.8955) acc_u 3.1250 (12.5000) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [60/72] time 0.416 (0.438) data 0.286 (0.307) loss_u loss_u 0.9189 (0.8943) acc_u 9.3750 (12.7604) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [65/72] time 0.421 (0.440) data 0.289 (0.309) loss_u loss_u 0.8335 (0.8923) acc_u 18.7500 (12.9327) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [70/72] time 0.363 (0.438) data 0.231 (0.307) loss_u loss_u 0.8555 (0.8900) acc_u 15.6250 (13.2589) lr 1.1719e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1717
confident_label rate tensor(0.2761, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 866
clean true:667
clean false:199
clean_rate:0.7702078521939953
noisy true:752
noisy false:1518
after delete: len(clean_dataset) 866
after delete: len(noisy_dataset) 2270
epoch [92/200] batch [5/27] time 0.520 (0.490) data 0.390 (0.360) loss_x loss_x 1.3408 (1.2721) acc_x 62.5000 (67.5000) lr 1.1564e-03 eta 0:00:10
epoch [92/200] batch [10/27] time 0.582 (0.471) data 0.451 (0.341) loss_x loss_x 1.3242 (1.2981) acc_x 71.8750 (66.5625) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [15/27] time 0.383 (0.457) data 0.253 (0.327) loss_x loss_x 1.2188 (1.2563) acc_x 62.5000 (66.6667) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [20/27] time 0.487 (0.481) data 0.357 (0.351) loss_x loss_x 1.3066 (1.2646) acc_x 71.8750 (67.5000) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [25/27] time 0.403 (0.479) data 0.273 (0.349) loss_x loss_x 1.4512 (1.2911) acc_x 65.6250 (67.1250) lr 1.1564e-03 eta 0:00:00
epoch [92/200] batch [5/70] time 0.462 (0.465) data 0.330 (0.335) loss_u loss_u 0.9077 (0.8880) acc_u 15.6250 (14.3750) lr 1.1564e-03 eta 0:00:30
epoch [92/200] batch [10/70] time 0.479 (0.459) data 0.349 (0.329) loss_u loss_u 0.9097 (0.8792) acc_u 12.5000 (15.0000) lr 1.1564e-03 eta 0:00:27
epoch [92/200] batch [15/70] time 0.415 (0.452) data 0.284 (0.322) loss_u loss_u 0.9365 (0.8904) acc_u 6.2500 (13.5417) lr 1.1564e-03 eta 0:00:24
epoch [92/200] batch [20/70] time 0.424 (0.449) data 0.293 (0.318) loss_u loss_u 0.9131 (0.8901) acc_u 12.5000 (13.7500) lr 1.1564e-03 eta 0:00:22
epoch [92/200] batch [25/70] time 0.489 (0.446) data 0.358 (0.316) loss_u loss_u 0.9077 (0.8919) acc_u 18.7500 (13.6250) lr 1.1564e-03 eta 0:00:20
epoch [92/200] batch [30/70] time 0.463 (0.448) data 0.332 (0.317) loss_u loss_u 0.9233 (0.8934) acc_u 18.7500 (13.9583) lr 1.1564e-03 eta 0:00:17
epoch [92/200] batch [35/70] time 0.385 (0.446) data 0.254 (0.315) loss_u loss_u 0.8193 (0.8949) acc_u 21.8750 (13.3929) lr 1.1564e-03 eta 0:00:15
epoch [92/200] batch [40/70] time 0.469 (0.444) data 0.337 (0.313) loss_u loss_u 0.8892 (0.8977) acc_u 18.7500 (13.2031) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [45/70] time 0.411 (0.442) data 0.281 (0.312) loss_u loss_u 0.8594 (0.8977) acc_u 15.6250 (13.0556) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [50/70] time 0.415 (0.441) data 0.284 (0.310) loss_u loss_u 0.9106 (0.8973) acc_u 15.6250 (13.1250) lr 1.1564e-03 eta 0:00:08
epoch [92/200] batch [55/70] time 0.450 (0.440) data 0.318 (0.309) loss_u loss_u 0.9102 (0.8993) acc_u 9.3750 (12.7273) lr 1.1564e-03 eta 0:00:06
epoch [92/200] batch [60/70] time 0.439 (0.442) data 0.306 (0.311) loss_u loss_u 0.8994 (0.9003) acc_u 15.6250 (12.6562) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [65/70] time 0.405 (0.441) data 0.274 (0.310) loss_u loss_u 0.8735 (0.8987) acc_u 15.6250 (12.6923) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [70/70] time 0.405 (0.444) data 0.273 (0.313) loss_u loss_u 0.8877 (0.8986) acc_u 15.6250 (12.9018) lr 1.1564e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1754
confident_label rate tensor(0.2592, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 813
clean true:616
clean false:197
clean_rate:0.7576875768757687
noisy true:766
noisy false:1557
after delete: len(clean_dataset) 813
after delete: len(noisy_dataset) 2323
epoch [93/200] batch [5/25] time 0.352 (0.417) data 0.221 (0.286) loss_x loss_x 1.2031 (1.3043) acc_x 71.8750 (67.5000) lr 1.1409e-03 eta 0:00:08
epoch [93/200] batch [10/25] time 0.427 (0.418) data 0.296 (0.287) loss_x loss_x 1.5830 (1.4359) acc_x 46.8750 (62.1875) lr 1.1409e-03 eta 0:00:06
epoch [93/200] batch [15/25] time 0.470 (0.443) data 0.339 (0.312) loss_x loss_x 1.0049 (1.4569) acc_x 78.1250 (64.5833) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [20/25] time 0.517 (0.440) data 0.387 (0.310) loss_x loss_x 1.1387 (1.4185) acc_x 78.1250 (65.6250) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [25/25] time 0.381 (0.438) data 0.251 (0.307) loss_x loss_x 1.5186 (1.3583) acc_x 56.2500 (67.0000) lr 1.1409e-03 eta 0:00:00
epoch [93/200] batch [5/72] time 0.427 (0.433) data 0.296 (0.302) loss_u loss_u 0.9561 (0.8878) acc_u 6.2500 (13.1250) lr 1.1409e-03 eta 0:00:29
epoch [93/200] batch [10/72] time 0.396 (0.428) data 0.266 (0.297) loss_u loss_u 0.9160 (0.9001) acc_u 6.2500 (12.1875) lr 1.1409e-03 eta 0:00:26
epoch [93/200] batch [15/72] time 0.553 (0.433) data 0.422 (0.302) loss_u loss_u 0.8564 (0.8983) acc_u 15.6250 (12.5000) lr 1.1409e-03 eta 0:00:24
epoch [93/200] batch [20/72] time 0.334 (0.436) data 0.203 (0.305) loss_u loss_u 0.9258 (0.9012) acc_u 6.2500 (12.0312) lr 1.1409e-03 eta 0:00:22
epoch [93/200] batch [25/72] time 0.427 (0.441) data 0.296 (0.310) loss_u loss_u 0.8706 (0.8992) acc_u 21.8750 (12.5000) lr 1.1409e-03 eta 0:00:20
epoch [93/200] batch [30/72] time 0.300 (0.436) data 0.169 (0.305) loss_u loss_u 0.8740 (0.8983) acc_u 18.7500 (12.9167) lr 1.1409e-03 eta 0:00:18
epoch [93/200] batch [35/72] time 0.429 (0.437) data 0.298 (0.306) loss_u loss_u 0.8887 (0.8971) acc_u 18.7500 (13.0357) lr 1.1409e-03 eta 0:00:16
epoch [93/200] batch [40/72] time 0.471 (0.435) data 0.339 (0.304) loss_u loss_u 0.8877 (0.8964) acc_u 12.5000 (13.2812) lr 1.1409e-03 eta 0:00:13
epoch [93/200] batch [45/72] time 0.416 (0.436) data 0.284 (0.305) loss_u loss_u 0.9131 (0.8937) acc_u 9.3750 (13.6806) lr 1.1409e-03 eta 0:00:11
epoch [93/200] batch [50/72] time 0.508 (0.437) data 0.377 (0.306) loss_u loss_u 0.8516 (0.8930) acc_u 21.8750 (13.9375) lr 1.1409e-03 eta 0:00:09
epoch [93/200] batch [55/72] time 0.484 (0.440) data 0.354 (0.309) loss_u loss_u 0.9043 (0.8948) acc_u 12.5000 (13.6932) lr 1.1409e-03 eta 0:00:07
epoch [93/200] batch [60/72] time 0.407 (0.438) data 0.275 (0.307) loss_u loss_u 0.9375 (0.8949) acc_u 6.2500 (13.5417) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [65/72] time 0.418 (0.441) data 0.287 (0.309) loss_u loss_u 0.8677 (0.8934) acc_u 15.6250 (13.7500) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [70/72] time 0.382 (0.444) data 0.251 (0.312) loss_u loss_u 0.9370 (0.8952) acc_u 6.2500 (13.3482) lr 1.1409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1730
confident_label rate tensor(0.2695, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 845
clean true:653
clean false:192
clean_rate:0.7727810650887574
noisy true:753
noisy false:1538
after delete: len(clean_dataset) 845
after delete: len(noisy_dataset) 2291
epoch [94/200] batch [5/26] time 0.592 (0.490) data 0.460 (0.359) loss_x loss_x 1.0498 (1.1405) acc_x 75.0000 (70.0000) lr 1.1253e-03 eta 0:00:10
epoch [94/200] batch [10/26] time 0.545 (0.487) data 0.414 (0.356) loss_x loss_x 1.6074 (1.2547) acc_x 59.3750 (67.1875) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [15/26] time 0.437 (0.501) data 0.306 (0.369) loss_x loss_x 1.2617 (1.3300) acc_x 62.5000 (66.8750) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [20/26] time 0.449 (0.483) data 0.319 (0.352) loss_x loss_x 1.0811 (1.2883) acc_x 71.8750 (67.3438) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [25/26] time 0.504 (0.480) data 0.372 (0.349) loss_x loss_x 1.6025 (1.3114) acc_x 68.7500 (67.6250) lr 1.1253e-03 eta 0:00:00
epoch [94/200] batch [5/71] time 0.413 (0.477) data 0.281 (0.346) loss_u loss_u 0.9321 (0.9111) acc_u 6.2500 (12.5000) lr 1.1253e-03 eta 0:00:31
epoch [94/200] batch [10/71] time 0.369 (0.475) data 0.238 (0.344) loss_u loss_u 0.8936 (0.8985) acc_u 21.8750 (15.3125) lr 1.1253e-03 eta 0:00:28
epoch [94/200] batch [15/71] time 0.514 (0.470) data 0.383 (0.339) loss_u loss_u 0.9561 (0.9003) acc_u 3.1250 (14.1667) lr 1.1253e-03 eta 0:00:26
epoch [94/200] batch [20/71] time 0.380 (0.467) data 0.248 (0.336) loss_u loss_u 0.8823 (0.8948) acc_u 6.2500 (13.9062) lr 1.1253e-03 eta 0:00:23
epoch [94/200] batch [25/71] time 0.402 (0.469) data 0.271 (0.338) loss_u loss_u 0.9370 (0.8986) acc_u 6.2500 (13.1250) lr 1.1253e-03 eta 0:00:21
epoch [94/200] batch [30/71] time 0.537 (0.467) data 0.406 (0.336) loss_u loss_u 0.8745 (0.9002) acc_u 15.6250 (12.8125) lr 1.1253e-03 eta 0:00:19
epoch [94/200] batch [35/71] time 0.517 (0.467) data 0.386 (0.336) loss_u loss_u 0.8672 (0.8993) acc_u 18.7500 (12.8571) lr 1.1253e-03 eta 0:00:16
epoch [94/200] batch [40/71] time 0.386 (0.466) data 0.254 (0.335) loss_u loss_u 0.8892 (0.8977) acc_u 18.7500 (12.8125) lr 1.1253e-03 eta 0:00:14
epoch [94/200] batch [45/71] time 0.511 (0.465) data 0.378 (0.334) loss_u loss_u 0.8667 (0.8945) acc_u 15.6250 (13.4722) lr 1.1253e-03 eta 0:00:12
epoch [94/200] batch [50/71] time 0.436 (0.463) data 0.305 (0.332) loss_u loss_u 0.8203 (0.8934) acc_u 18.7500 (13.6250) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [55/71] time 0.449 (0.463) data 0.318 (0.332) loss_u loss_u 0.9756 (0.8965) acc_u 3.1250 (13.3523) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [60/71] time 0.380 (0.463) data 0.248 (0.331) loss_u loss_u 0.8623 (0.8946) acc_u 25.0000 (13.6979) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [65/71] time 0.445 (0.464) data 0.314 (0.333) loss_u loss_u 0.9111 (0.8949) acc_u 9.3750 (13.5577) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [70/71] time 0.398 (0.466) data 0.267 (0.334) loss_u loss_u 0.8813 (0.8959) acc_u 15.6250 (13.3929) lr 1.1253e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1800
confident_label rate tensor(0.2672, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 838
clean true:620
clean false:218
clean_rate:0.7398568019093079
noisy true:716
noisy false:1582
after delete: len(clean_dataset) 838
after delete: len(noisy_dataset) 2298
epoch [95/200] batch [5/26] time 0.525 (0.451) data 0.394 (0.320) loss_x loss_x 1.5322 (1.3010) acc_x 62.5000 (66.8750) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [10/26] time 0.340 (0.461) data 0.208 (0.330) loss_x loss_x 1.6211 (1.4197) acc_x 59.3750 (65.9375) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [15/26] time 0.441 (0.469) data 0.310 (0.338) loss_x loss_x 1.2588 (1.3714) acc_x 62.5000 (66.4583) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [20/26] time 0.504 (0.478) data 0.373 (0.347) loss_x loss_x 1.4775 (1.3670) acc_x 59.3750 (65.3125) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [25/26] time 0.366 (0.467) data 0.235 (0.337) loss_x loss_x 1.3750 (1.3770) acc_x 62.5000 (65.5000) lr 1.1097e-03 eta 0:00:00
epoch [95/200] batch [5/71] time 0.473 (0.458) data 0.341 (0.327) loss_u loss_u 0.9141 (0.8885) acc_u 9.3750 (13.7500) lr 1.1097e-03 eta 0:00:30
epoch [95/200] batch [10/71] time 0.416 (0.455) data 0.284 (0.324) loss_u loss_u 0.9346 (0.8965) acc_u 6.2500 (13.1250) lr 1.1097e-03 eta 0:00:27
epoch [95/200] batch [15/71] time 0.481 (0.457) data 0.350 (0.326) loss_u loss_u 0.8511 (0.8969) acc_u 18.7500 (12.7083) lr 1.1097e-03 eta 0:00:25
epoch [95/200] batch [20/71] time 0.564 (0.469) data 0.433 (0.338) loss_u loss_u 0.8931 (0.8944) acc_u 12.5000 (12.8125) lr 1.1097e-03 eta 0:00:23
epoch [95/200] batch [25/71] time 0.409 (0.466) data 0.277 (0.334) loss_u loss_u 0.8516 (0.8950) acc_u 18.7500 (12.8750) lr 1.1097e-03 eta 0:00:21
epoch [95/200] batch [30/71] time 0.524 (0.466) data 0.392 (0.335) loss_u loss_u 0.9292 (0.8955) acc_u 9.3750 (13.1250) lr 1.1097e-03 eta 0:00:19
epoch [95/200] batch [35/71] time 0.491 (0.465) data 0.360 (0.334) loss_u loss_u 0.8975 (0.8946) acc_u 15.6250 (13.0357) lr 1.1097e-03 eta 0:00:16
epoch [95/200] batch [40/71] time 0.385 (0.461) data 0.253 (0.330) loss_u loss_u 0.9131 (0.8960) acc_u 9.3750 (12.8125) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [45/71] time 0.460 (0.462) data 0.328 (0.331) loss_u loss_u 0.9058 (0.8961) acc_u 12.5000 (12.8472) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [50/71] time 0.608 (0.462) data 0.477 (0.330) loss_u loss_u 0.8730 (0.8973) acc_u 15.6250 (12.6250) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [55/71] time 0.347 (0.459) data 0.215 (0.328) loss_u loss_u 0.9346 (0.8984) acc_u 9.3750 (12.6136) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [60/71] time 0.375 (0.457) data 0.244 (0.326) loss_u loss_u 0.8105 (0.8956) acc_u 25.0000 (13.1771) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [65/71] time 0.391 (0.456) data 0.259 (0.324) loss_u loss_u 0.8623 (0.8916) acc_u 15.6250 (13.4135) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [70/71] time 0.383 (0.455) data 0.252 (0.323) loss_u loss_u 0.9180 (0.8926) acc_u 6.2500 (13.2143) lr 1.1097e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1760
confident_label rate tensor(0.2726, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 855
clean true:625
clean false:230
clean_rate:0.7309941520467836
noisy true:751
noisy false:1530
after delete: len(clean_dataset) 855
after delete: len(noisy_dataset) 2281
epoch [96/200] batch [5/26] time 0.624 (0.488) data 0.493 (0.357) loss_x loss_x 1.7041 (1.2799) acc_x 46.8750 (66.8750) lr 1.0941e-03 eta 0:00:10
epoch [96/200] batch [10/26] time 0.464 (0.466) data 0.333 (0.335) loss_x loss_x 1.8594 (1.3633) acc_x 62.5000 (66.2500) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [15/26] time 0.349 (0.445) data 0.217 (0.314) loss_x loss_x 1.3271 (1.3954) acc_x 71.8750 (66.6667) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [20/26] time 0.404 (0.455) data 0.273 (0.323) loss_x loss_x 1.5332 (1.3759) acc_x 68.7500 (66.4062) lr 1.0941e-03 eta 0:00:02
epoch [96/200] batch [25/26] time 0.483 (0.457) data 0.352 (0.326) loss_x loss_x 1.5586 (1.4082) acc_x 71.8750 (65.1250) lr 1.0941e-03 eta 0:00:00
epoch [96/200] batch [5/71] time 0.395 (0.451) data 0.263 (0.320) loss_u loss_u 0.8882 (0.8824) acc_u 15.6250 (16.2500) lr 1.0941e-03 eta 0:00:29
epoch [96/200] batch [10/71] time 0.379 (0.446) data 0.247 (0.315) loss_u loss_u 0.8477 (0.8727) acc_u 21.8750 (16.5625) lr 1.0941e-03 eta 0:00:27
epoch [96/200] batch [15/71] time 0.441 (0.447) data 0.310 (0.316) loss_u loss_u 0.9561 (0.8807) acc_u 3.1250 (15.0000) lr 1.0941e-03 eta 0:00:25
epoch [96/200] batch [20/71] time 0.550 (0.451) data 0.418 (0.320) loss_u loss_u 0.9229 (0.8914) acc_u 3.1250 (13.4375) lr 1.0941e-03 eta 0:00:23
epoch [96/200] batch [25/71] time 0.394 (0.453) data 0.262 (0.322) loss_u loss_u 0.8711 (0.8923) acc_u 18.7500 (13.5000) lr 1.0941e-03 eta 0:00:20
epoch [96/200] batch [30/71] time 0.670 (0.460) data 0.539 (0.329) loss_u loss_u 0.9072 (0.8993) acc_u 15.6250 (13.0208) lr 1.0941e-03 eta 0:00:18
epoch [96/200] batch [35/71] time 0.391 (0.456) data 0.259 (0.324) loss_u loss_u 0.8691 (0.8990) acc_u 25.0000 (13.3929) lr 1.0941e-03 eta 0:00:16
epoch [96/200] batch [40/71] time 0.421 (0.453) data 0.289 (0.322) loss_u loss_u 0.9033 (0.8973) acc_u 9.3750 (13.3594) lr 1.0941e-03 eta 0:00:14
epoch [96/200] batch [45/71] time 0.372 (0.450) data 0.241 (0.319) loss_u loss_u 0.8955 (0.8994) acc_u 12.5000 (13.0556) lr 1.0941e-03 eta 0:00:11
epoch [96/200] batch [50/71] time 0.530 (0.451) data 0.400 (0.319) loss_u loss_u 0.8960 (0.8980) acc_u 12.5000 (13.2500) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [55/71] time 0.382 (0.451) data 0.251 (0.320) loss_u loss_u 0.9331 (0.8988) acc_u 9.3750 (13.0682) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [60/71] time 0.400 (0.449) data 0.269 (0.318) loss_u loss_u 0.9370 (0.8975) acc_u 9.3750 (13.2292) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [65/71] time 0.475 (0.450) data 0.344 (0.318) loss_u loss_u 0.9092 (0.8967) acc_u 15.6250 (13.3654) lr 1.0941e-03 eta 0:00:02
epoch [96/200] batch [70/71] time 0.426 (0.448) data 0.295 (0.317) loss_u loss_u 0.8950 (0.8974) acc_u 15.6250 (13.3036) lr 1.0941e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1810
confident_label rate tensor(0.2548, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 799
clean true:595
clean false:204
clean_rate:0.7446808510638298
noisy true:731
noisy false:1606
after delete: len(clean_dataset) 799
after delete: len(noisy_dataset) 2337
epoch [97/200] batch [5/24] time 0.630 (0.529) data 0.499 (0.398) loss_x loss_x 1.6846 (1.1221) acc_x 65.6250 (73.7500) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [10/24] time 0.387 (0.480) data 0.257 (0.349) loss_x loss_x 0.6772 (1.1638) acc_x 87.5000 (73.1250) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [15/24] time 0.458 (0.464) data 0.328 (0.333) loss_x loss_x 1.2461 (1.1999) acc_x 71.8750 (73.1250) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [20/24] time 0.420 (0.463) data 0.289 (0.332) loss_x loss_x 1.2266 (1.1938) acc_x 68.7500 (72.1875) lr 1.0785e-03 eta 0:00:01
epoch [97/200] batch [5/73] time 0.380 (0.451) data 0.248 (0.320) loss_u loss_u 0.8608 (0.8765) acc_u 18.7500 (15.6250) lr 1.0785e-03 eta 0:00:30
epoch [97/200] batch [10/73] time 0.437 (0.448) data 0.305 (0.317) loss_u loss_u 0.9009 (0.8828) acc_u 9.3750 (14.0625) lr 1.0785e-03 eta 0:00:28
epoch [97/200] batch [15/73] time 0.525 (0.446) data 0.393 (0.315) loss_u loss_u 0.8970 (0.8870) acc_u 12.5000 (13.9583) lr 1.0785e-03 eta 0:00:25
epoch [97/200] batch [20/73] time 0.408 (0.439) data 0.276 (0.308) loss_u loss_u 0.9282 (0.8933) acc_u 6.2500 (12.6562) lr 1.0785e-03 eta 0:00:23
epoch [97/200] batch [25/73] time 0.449 (0.441) data 0.318 (0.310) loss_u loss_u 0.8467 (0.8885) acc_u 25.0000 (13.3750) lr 1.0785e-03 eta 0:00:21
epoch [97/200] batch [30/73] time 0.460 (0.441) data 0.330 (0.310) loss_u loss_u 0.8296 (0.8876) acc_u 25.0000 (13.7500) lr 1.0785e-03 eta 0:00:18
epoch [97/200] batch [35/73] time 0.374 (0.439) data 0.242 (0.308) loss_u loss_u 0.9644 (0.8900) acc_u 6.2500 (13.8393) lr 1.0785e-03 eta 0:00:16
epoch [97/200] batch [40/73] time 0.495 (0.442) data 0.364 (0.310) loss_u loss_u 0.8130 (0.8891) acc_u 25.0000 (13.9062) lr 1.0785e-03 eta 0:00:14
epoch [97/200] batch [45/73] time 0.342 (0.439) data 0.210 (0.308) loss_u loss_u 0.9502 (0.8894) acc_u 0.0000 (13.5417) lr 1.0785e-03 eta 0:00:12
epoch [97/200] batch [50/73] time 0.447 (0.442) data 0.315 (0.311) loss_u loss_u 0.9419 (0.8914) acc_u 6.2500 (13.3750) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [55/73] time 0.383 (0.444) data 0.253 (0.313) loss_u loss_u 0.8550 (0.8910) acc_u 18.7500 (13.5227) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [60/73] time 0.503 (0.445) data 0.372 (0.314) loss_u loss_u 0.8896 (0.8890) acc_u 15.6250 (13.8021) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [65/73] time 0.513 (0.446) data 0.381 (0.314) loss_u loss_u 0.8926 (0.8901) acc_u 6.2500 (13.6058) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [70/73] time 0.313 (0.441) data 0.182 (0.310) loss_u loss_u 0.8950 (0.8916) acc_u 15.6250 (13.4821) lr 1.0785e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1760
confident_label rate tensor(0.2755, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 864
clean true:639
clean false:225
clean_rate:0.7395833333333334
noisy true:737
noisy false:1535
after delete: len(clean_dataset) 864
after delete: len(noisy_dataset) 2272
epoch [98/200] batch [5/27] time 0.434 (0.490) data 0.303 (0.359) loss_x loss_x 0.8257 (1.2000) acc_x 71.8750 (66.2500) lr 1.0628e-03 eta 0:00:10
epoch [98/200] batch [10/27] time 0.461 (0.476) data 0.331 (0.346) loss_x loss_x 1.0850 (1.2150) acc_x 71.8750 (67.5000) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [15/27] time 0.533 (0.493) data 0.402 (0.362) loss_x loss_x 0.9370 (1.2168) acc_x 81.2500 (67.9167) lr 1.0628e-03 eta 0:00:05
epoch [98/200] batch [20/27] time 0.402 (0.470) data 0.270 (0.340) loss_x loss_x 1.0283 (1.1951) acc_x 71.8750 (68.5938) lr 1.0628e-03 eta 0:00:03
epoch [98/200] batch [25/27] time 0.370 (0.477) data 0.239 (0.347) loss_x loss_x 2.0469 (1.2398) acc_x 46.8750 (67.1250) lr 1.0628e-03 eta 0:00:00
epoch [98/200] batch [5/71] time 0.384 (0.468) data 0.252 (0.337) loss_u loss_u 0.9419 (0.9187) acc_u 3.1250 (10.0000) lr 1.0628e-03 eta 0:00:30
epoch [98/200] batch [10/71] time 0.416 (0.460) data 0.285 (0.329) loss_u loss_u 0.8711 (0.9075) acc_u 18.7500 (10.6250) lr 1.0628e-03 eta 0:00:28
epoch [98/200] batch [15/71] time 0.490 (0.463) data 0.360 (0.332) loss_u loss_u 0.9097 (0.9023) acc_u 12.5000 (11.8750) lr 1.0628e-03 eta 0:00:25
epoch [98/200] batch [20/71] time 0.471 (0.463) data 0.340 (0.332) loss_u loss_u 0.8877 (0.8995) acc_u 15.6250 (12.6562) lr 1.0628e-03 eta 0:00:23
epoch [98/200] batch [25/71] time 0.383 (0.461) data 0.251 (0.329) loss_u loss_u 0.9180 (0.8992) acc_u 12.5000 (12.5000) lr 1.0628e-03 eta 0:00:21
epoch [98/200] batch [30/71] time 0.360 (0.458) data 0.228 (0.327) loss_u loss_u 0.8706 (0.8975) acc_u 15.6250 (12.5000) lr 1.0628e-03 eta 0:00:18
epoch [98/200] batch [35/71] time 0.556 (0.458) data 0.425 (0.327) loss_u loss_u 0.9653 (0.9026) acc_u 3.1250 (11.6964) lr 1.0628e-03 eta 0:00:16
epoch [98/200] batch [40/71] time 0.508 (0.457) data 0.377 (0.326) loss_u loss_u 0.8628 (0.9015) acc_u 12.5000 (11.7188) lr 1.0628e-03 eta 0:00:14
epoch [98/200] batch [45/71] time 0.392 (0.456) data 0.259 (0.325) loss_u loss_u 0.8589 (0.9007) acc_u 18.7500 (12.0139) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [50/71] time 0.506 (0.458) data 0.375 (0.326) loss_u loss_u 0.9463 (0.9017) acc_u 6.2500 (12.0000) lr 1.0628e-03 eta 0:00:09
epoch [98/200] batch [55/71] time 0.410 (0.458) data 0.279 (0.327) loss_u loss_u 0.9404 (0.9031) acc_u 9.3750 (11.8182) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [60/71] time 0.403 (0.457) data 0.271 (0.326) loss_u loss_u 0.7822 (0.9003) acc_u 28.1250 (12.1354) lr 1.0628e-03 eta 0:00:05
epoch [98/200] batch [65/71] time 0.517 (0.458) data 0.386 (0.327) loss_u loss_u 0.9199 (0.9010) acc_u 9.3750 (12.1154) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [70/71] time 0.476 (0.460) data 0.345 (0.328) loss_u loss_u 0.8359 (0.8981) acc_u 21.8750 (12.5000) lr 1.0628e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1790
confident_label rate tensor(0.2701, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 847
clean true:620
clean false:227
clean_rate:0.731995277449823
noisy true:726
noisy false:1563
after delete: len(clean_dataset) 847
after delete: len(noisy_dataset) 2289
epoch [99/200] batch [5/26] time 0.601 (0.560) data 0.469 (0.428) loss_x loss_x 1.0820 (1.4949) acc_x 65.6250 (61.8750) lr 1.0471e-03 eta 0:00:11
epoch [99/200] batch [10/26] time 0.574 (0.548) data 0.443 (0.416) loss_x loss_x 0.8208 (1.3090) acc_x 78.1250 (66.8750) lr 1.0471e-03 eta 0:00:08
epoch [99/200] batch [15/26] time 0.452 (0.509) data 0.321 (0.377) loss_x loss_x 1.5283 (1.3262) acc_x 62.5000 (66.6667) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [20/26] time 0.443 (0.492) data 0.313 (0.361) loss_x loss_x 1.2461 (1.3061) acc_x 62.5000 (66.2500) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [25/26] time 0.374 (0.468) data 0.243 (0.337) loss_x loss_x 1.4346 (1.3148) acc_x 59.3750 (65.7500) lr 1.0471e-03 eta 0:00:00
epoch [99/200] batch [5/71] time 0.362 (0.467) data 0.229 (0.336) loss_u loss_u 0.9463 (0.8899) acc_u 6.2500 (13.1250) lr 1.0471e-03 eta 0:00:30
epoch [99/200] batch [10/71] time 0.406 (0.467) data 0.274 (0.336) loss_u loss_u 0.8882 (0.9007) acc_u 12.5000 (12.1875) lr 1.0471e-03 eta 0:00:28
epoch [99/200] batch [15/71] time 0.388 (0.463) data 0.256 (0.331) loss_u loss_u 0.8584 (0.9015) acc_u 18.7500 (11.8750) lr 1.0471e-03 eta 0:00:25
epoch [99/200] batch [20/71] time 0.486 (0.467) data 0.355 (0.335) loss_u loss_u 0.8950 (0.8983) acc_u 15.6250 (12.1875) lr 1.0471e-03 eta 0:00:23
epoch [99/200] batch [25/71] time 0.409 (0.463) data 0.277 (0.332) loss_u loss_u 0.9102 (0.8934) acc_u 12.5000 (12.8750) lr 1.0471e-03 eta 0:00:21
epoch [99/200] batch [30/71] time 0.533 (0.467) data 0.402 (0.336) loss_u loss_u 0.9170 (0.8981) acc_u 6.2500 (12.0833) lr 1.0471e-03 eta 0:00:19
epoch [99/200] batch [35/71] time 0.349 (0.460) data 0.218 (0.329) loss_u loss_u 0.9487 (0.9016) acc_u 6.2500 (11.8750) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [40/71] time 0.435 (0.461) data 0.304 (0.330) loss_u loss_u 0.8511 (0.8974) acc_u 15.6250 (12.4219) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [45/71] time 0.394 (0.462) data 0.263 (0.331) loss_u loss_u 0.8237 (0.8965) acc_u 25.0000 (12.6389) lr 1.0471e-03 eta 0:00:12
epoch [99/200] batch [50/71] time 0.533 (0.462) data 0.403 (0.331) loss_u loss_u 0.8623 (0.8949) acc_u 21.8750 (13.1875) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [55/71] time 0.325 (0.460) data 0.194 (0.329) loss_u loss_u 0.9468 (0.8969) acc_u 6.2500 (12.8977) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [60/71] time 0.379 (0.457) data 0.248 (0.326) loss_u loss_u 0.8589 (0.8946) acc_u 18.7500 (13.2812) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [65/71] time 0.322 (0.453) data 0.191 (0.322) loss_u loss_u 0.7939 (0.8928) acc_u 28.1250 (13.5096) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [70/71] time 0.430 (0.453) data 0.299 (0.322) loss_u loss_u 0.9321 (0.8945) acc_u 9.3750 (13.3482) lr 1.0471e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1768
confident_label rate tensor(0.2765, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 867
clean true:637
clean false:230
clean_rate:0.734717416378316
noisy true:731
noisy false:1538
all clean rate:  [0.779783393501805, 0.8547945205479452, 0.8604240282685512, 0.8705035971223022, 0.8571428571428571, 0.8176895306859205, 0.8646209386281588, 0.8581196581196581, 0.8327814569536424, 0.8368421052631579, 0.8322475570032574, 0.849912739965096, 0.8470588235294118, 0.8391376451077943, 0.8438485804416404, 0.8498293515358362, 0.8417508417508418, 0.8363047001620746, 0.8541996830427893, 0.8356374807987711, 0.8338557993730408, 0.8330683624801272, 0.8370044052863436, 0.8287461773700305, 0.7933634992458521, 0.8171091445427728, 0.8026905829596412, 0.8162393162393162, 0.8020679468242246, 0.7948717948717948, 0.8005738880918221, 0.7861356932153393, 0.7959770114942529, 0.7870090634441088, 0.7896995708154506, 0.7918424753867792, 0.7865329512893983, 0.7791495198902606, 0.8260233918128655, 0.797432239657632, 0.796398891966759, 0.7748251748251749, 0.8010471204188482, 0.779020979020979, 0.7844155844155845, 0.8094594594594594, 0.7932885906040269, 0.7713498622589532, 0.7756756756756756, 0.7924773022049286, 0.7810320781032078, 0.7696335078534031, 0.783748361730013, 0.7914438502673797, 0.775623268698061, 0.7887323943661971, 0.7789336801040312, 0.772, 0.7688172043010753, 0.7738246505717916, 0.762982689747004, 0.7650130548302873, 0.7679045092838196, 0.759493670886076, 0.7750677506775068, 0.7913385826771654, 0.7715404699738904, 0.7768187422934648, 0.7614213197969543, 0.7742331288343558, 0.7697368421052632, 0.7829360100376411, 0.7608142493638677, 0.7706093189964157, 0.7655086848635235, 0.7701149425287356, 0.775609756097561, 0.7763157894736842, 0.754950495049505, 0.76125, 0.7721518987341772, 0.7522123893805309, 0.7576126674786845, 0.7727840199750312, 0.7448856799037304, 0.74, 0.7647058823529411, 0.7574316290130797, 0.7579617834394905, 0.7480818414322251, 0.7478474784747847, 0.7702078521939953, 0.7576875768757687, 0.7727810650887574, 0.7398568019093079, 0.7309941520467836, 0.7446808510638298, 0.7395833333333334, 0.731995277449823, 0.734717416378316]
after delete: len(clean_dataset) 867
after delete: len(noisy_dataset) 2269
epoch [100/200] batch [5/27] time 0.445 (0.459) data 0.314 (0.328) loss_x loss_x 1.7100 (1.3838) acc_x 53.1250 (65.0000) lr 1.0314e-03 eta 0:00:10
epoch [100/200] batch [10/27] time 0.526 (0.482) data 0.395 (0.351) loss_x loss_x 1.1865 (1.3141) acc_x 71.8750 (68.4375) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [15/27] time 0.389 (0.455) data 0.258 (0.324) loss_x loss_x 0.9111 (1.3003) acc_x 75.0000 (68.3333) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [20/27] time 0.354 (0.454) data 0.224 (0.323) loss_x loss_x 1.2900 (1.2916) acc_x 59.3750 (67.8125) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [25/27] time 0.409 (0.445) data 0.279 (0.315) loss_x loss_x 1.0898 (1.2861) acc_x 68.7500 (67.1250) lr 1.0314e-03 eta 0:00:00
epoch [100/200] batch [5/70] time 0.542 (0.457) data 0.411 (0.327) loss_u loss_u 0.9170 (0.9112) acc_u 9.3750 (11.2500) lr 1.0314e-03 eta 0:00:29
epoch [100/200] batch [10/70] time 0.328 (0.458) data 0.198 (0.327) loss_u loss_u 0.8433 (0.8922) acc_u 18.7500 (12.8125) lr 1.0314e-03 eta 0:00:27
epoch [100/200] batch [15/70] time 0.423 (0.451) data 0.292 (0.320) loss_u loss_u 0.8711 (0.8816) acc_u 12.5000 (14.3750) lr 1.0314e-03 eta 0:00:24
epoch [100/200] batch [20/70] time 0.542 (0.454) data 0.412 (0.323) loss_u loss_u 0.8936 (0.8898) acc_u 18.7500 (13.4375) lr 1.0314e-03 eta 0:00:22
epoch [100/200] batch [25/70] time 0.357 (0.447) data 0.227 (0.316) loss_u loss_u 0.9258 (0.8923) acc_u 3.1250 (13.5000) lr 1.0314e-03 eta 0:00:20
epoch [100/200] batch [30/70] time 0.456 (0.450) data 0.326 (0.319) loss_u loss_u 0.9043 (0.8918) acc_u 9.3750 (13.2292) lr 1.0314e-03 eta 0:00:17
epoch [100/200] batch [35/70] time 0.429 (0.449) data 0.296 (0.318) loss_u loss_u 0.9668 (0.8978) acc_u 0.0000 (12.2321) lr 1.0314e-03 eta 0:00:15
epoch [100/200] batch [40/70] time 0.416 (0.449) data 0.284 (0.319) loss_u loss_u 0.9209 (0.8973) acc_u 6.2500 (12.4219) lr 1.0314e-03 eta 0:00:13
epoch [100/200] batch [45/70] time 0.618 (0.448) data 0.487 (0.318) loss_u loss_u 0.9302 (0.8990) acc_u 9.3750 (12.1528) lr 1.0314e-03 eta 0:00:11
epoch [100/200] batch [50/70] time 0.603 (0.449) data 0.472 (0.319) loss_u loss_u 0.9048 (0.8993) acc_u 12.5000 (12.2500) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [55/70] time 0.405 (0.447) data 0.273 (0.316) loss_u loss_u 0.8828 (0.9011) acc_u 15.6250 (12.0455) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [60/70] time 0.341 (0.444) data 0.209 (0.313) loss_u loss_u 0.8091 (0.8969) acc_u 25.0000 (12.6562) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [65/70] time 0.392 (0.443) data 0.261 (0.312) loss_u loss_u 0.8838 (0.8982) acc_u 18.7500 (12.5962) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [70/70] time 0.440 (0.446) data 0.308 (0.315) loss_u loss_u 0.8711 (0.8982) acc_u 15.6250 (12.5893) lr 1.0314e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1776
confident_label rate tensor(0.2573, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 807
clean true:600
clean false:207
clean_rate:0.7434944237918215
noisy true:760
noisy false:1569
after delete: len(clean_dataset) 807
after delete: len(noisy_dataset) 2329
epoch [101/200] batch [5/25] time 0.349 (0.427) data 0.219 (0.296) loss_x loss_x 1.1582 (1.2945) acc_x 75.0000 (71.2500) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [10/25] time 0.688 (0.462) data 0.558 (0.331) loss_x loss_x 1.6025 (1.3792) acc_x 56.2500 (67.5000) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [15/25] time 0.468 (0.462) data 0.338 (0.331) loss_x loss_x 1.1377 (1.3771) acc_x 62.5000 (66.4583) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [20/25] time 0.488 (0.445) data 0.358 (0.314) loss_x loss_x 1.2275 (1.2860) acc_x 71.8750 (68.9062) lr 1.0157e-03 eta 0:00:02
epoch [101/200] batch [25/25] time 0.381 (0.442) data 0.250 (0.311) loss_x loss_x 1.2451 (1.2863) acc_x 68.7500 (68.3750) lr 1.0157e-03 eta 0:00:00
epoch [101/200] batch [5/72] time 0.381 (0.438) data 0.249 (0.307) loss_u loss_u 0.8677 (0.9148) acc_u 15.6250 (11.2500) lr 1.0157e-03 eta 0:00:29
epoch [101/200] batch [10/72] time 0.408 (0.445) data 0.277 (0.314) loss_u loss_u 0.9146 (0.8896) acc_u 18.7500 (16.2500) lr 1.0157e-03 eta 0:00:27
epoch [101/200] batch [15/72] time 0.700 (0.453) data 0.569 (0.322) loss_u loss_u 0.9189 (0.8925) acc_u 15.6250 (15.6250) lr 1.0157e-03 eta 0:00:25
epoch [101/200] batch [20/72] time 0.385 (0.446) data 0.254 (0.315) loss_u loss_u 0.8320 (0.8851) acc_u 18.7500 (15.7812) lr 1.0157e-03 eta 0:00:23
epoch [101/200] batch [25/72] time 0.436 (0.442) data 0.304 (0.311) loss_u loss_u 0.8887 (0.8860) acc_u 12.5000 (15.3750) lr 1.0157e-03 eta 0:00:20
epoch [101/200] batch [30/72] time 0.422 (0.441) data 0.290 (0.310) loss_u loss_u 0.8262 (0.8817) acc_u 21.8750 (15.9375) lr 1.0157e-03 eta 0:00:18
epoch [101/200] batch [35/72] time 0.442 (0.439) data 0.311 (0.308) loss_u loss_u 0.9082 (0.8842) acc_u 12.5000 (15.5357) lr 1.0157e-03 eta 0:00:16
epoch [101/200] batch [40/72] time 0.399 (0.441) data 0.268 (0.310) loss_u loss_u 0.9219 (0.8831) acc_u 9.3750 (15.6250) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [45/72] time 0.385 (0.440) data 0.253 (0.309) loss_u loss_u 0.8320 (0.8857) acc_u 25.0000 (15.2778) lr 1.0157e-03 eta 0:00:11
epoch [101/200] batch [50/72] time 0.500 (0.440) data 0.367 (0.309) loss_u loss_u 0.9277 (0.8848) acc_u 6.2500 (15.2500) lr 1.0157e-03 eta 0:00:09
epoch [101/200] batch [55/72] time 0.427 (0.442) data 0.296 (0.310) loss_u loss_u 0.9102 (0.8850) acc_u 12.5000 (15.1705) lr 1.0157e-03 eta 0:00:07
epoch [101/200] batch [60/72] time 0.447 (0.439) data 0.315 (0.308) loss_u loss_u 0.9004 (0.8845) acc_u 9.3750 (15.0521) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [65/72] time 0.344 (0.438) data 0.213 (0.307) loss_u loss_u 0.8350 (0.8848) acc_u 15.6250 (15.1442) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [70/72] time 0.491 (0.441) data 0.360 (0.310) loss_u loss_u 0.8477 (0.8836) acc_u 21.8750 (15.3571) lr 1.0157e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1808
confident_label rate tensor(0.2640, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 828
clean true:619
clean false:209
clean_rate:0.7475845410628019
noisy true:709
noisy false:1599
after delete: len(clean_dataset) 828
after delete: len(noisy_dataset) 2308
epoch [102/200] batch [5/25] time 0.401 (0.394) data 0.270 (0.263) loss_x loss_x 1.4287 (1.0805) acc_x 59.3750 (71.8750) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [10/25] time 0.416 (0.422) data 0.285 (0.291) loss_x loss_x 1.2500 (1.1049) acc_x 68.7500 (72.8125) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [15/25] time 0.597 (0.445) data 0.466 (0.314) loss_x loss_x 1.1084 (1.1927) acc_x 68.7500 (71.2500) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [20/25] time 0.551 (0.469) data 0.419 (0.338) loss_x loss_x 1.0986 (1.1932) acc_x 68.7500 (70.3125) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [25/25] time 0.388 (0.453) data 0.256 (0.322) loss_x loss_x 0.6963 (1.2392) acc_x 81.2500 (69.1250) lr 1.0000e-03 eta 0:00:00
epoch [102/200] batch [5/72] time 0.327 (0.442) data 0.195 (0.311) loss_u loss_u 0.8008 (0.8938) acc_u 18.7500 (11.8750) lr 1.0000e-03 eta 0:00:29
epoch [102/200] batch [10/72] time 0.476 (0.440) data 0.345 (0.309) loss_u loss_u 0.8599 (0.8979) acc_u 15.6250 (12.5000) lr 1.0000e-03 eta 0:00:27
epoch [102/200] batch [15/72] time 0.404 (0.438) data 0.272 (0.307) loss_u loss_u 0.8652 (0.9018) acc_u 21.8750 (12.5000) lr 1.0000e-03 eta 0:00:24
epoch [102/200] batch [20/72] time 0.413 (0.447) data 0.282 (0.315) loss_u loss_u 0.8408 (0.8982) acc_u 25.0000 (12.8125) lr 1.0000e-03 eta 0:00:23
epoch [102/200] batch [25/72] time 0.395 (0.446) data 0.263 (0.314) loss_u loss_u 0.8442 (0.8913) acc_u 31.2500 (14.1250) lr 1.0000e-03 eta 0:00:20
epoch [102/200] batch [30/72] time 0.545 (0.447) data 0.414 (0.316) loss_u loss_u 0.9331 (0.8893) acc_u 9.3750 (14.1667) lr 1.0000e-03 eta 0:00:18
epoch [102/200] batch [35/72] time 0.417 (0.449) data 0.286 (0.318) loss_u loss_u 0.8618 (0.8884) acc_u 12.5000 (14.0179) lr 1.0000e-03 eta 0:00:16
epoch [102/200] batch [40/72] time 0.505 (0.448) data 0.373 (0.316) loss_u loss_u 0.8677 (0.8890) acc_u 18.7500 (13.9844) lr 1.0000e-03 eta 0:00:14
epoch [102/200] batch [45/72] time 0.518 (0.447) data 0.387 (0.315) loss_u loss_u 0.8550 (0.8901) acc_u 15.6250 (13.7500) lr 1.0000e-03 eta 0:00:12
epoch [102/200] batch [50/72] time 0.409 (0.446) data 0.277 (0.315) loss_u loss_u 0.8013 (0.8895) acc_u 31.2500 (13.8750) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [55/72] time 0.550 (0.446) data 0.418 (0.314) loss_u loss_u 0.8101 (0.8861) acc_u 28.1250 (14.2045) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [60/72] time 0.553 (0.445) data 0.421 (0.314) loss_u loss_u 0.9673 (0.8878) acc_u 3.1250 (13.9062) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [65/72] time 0.484 (0.448) data 0.351 (0.317) loss_u loss_u 0.9458 (0.8902) acc_u 9.3750 (13.5577) lr 1.0000e-03 eta 0:00:03
epoch [102/200] batch [70/72] time 0.565 (0.448) data 0.433 (0.317) loss_u loss_u 0.8853 (0.8921) acc_u 12.5000 (13.2589) lr 1.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1757
confident_label rate tensor(0.2730, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 856
clean true:634
clean false:222
clean_rate:0.7406542056074766
noisy true:745
noisy false:1535
after delete: len(clean_dataset) 856
after delete: len(noisy_dataset) 2280
epoch [103/200] batch [5/26] time 0.497 (0.562) data 0.367 (0.432) loss_x loss_x 1.0732 (1.1244) acc_x 68.7500 (73.1250) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [10/26] time 0.424 (0.509) data 0.293 (0.378) loss_x loss_x 1.6172 (1.2027) acc_x 68.7500 (70.6250) lr 9.8429e-04 eta 0:00:08
epoch [103/200] batch [15/26] time 0.391 (0.484) data 0.260 (0.353) loss_x loss_x 1.2812 (1.2305) acc_x 65.6250 (70.2083) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [20/26] time 0.389 (0.472) data 0.258 (0.341) loss_x loss_x 1.1768 (1.2625) acc_x 78.1250 (70.1562) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [25/26] time 0.360 (0.459) data 0.229 (0.328) loss_x loss_x 1.0674 (1.2440) acc_x 71.8750 (69.6250) lr 9.8429e-04 eta 0:00:00
epoch [103/200] batch [5/71] time 0.404 (0.448) data 0.274 (0.318) loss_u loss_u 0.8706 (0.9120) acc_u 15.6250 (10.0000) lr 9.8429e-04 eta 0:00:29
epoch [103/200] batch [10/71] time 0.556 (0.453) data 0.426 (0.323) loss_u loss_u 0.9419 (0.9040) acc_u 9.3750 (11.2500) lr 9.8429e-04 eta 0:00:27
epoch [103/200] batch [15/71] time 0.426 (0.447) data 0.295 (0.316) loss_u loss_u 0.9214 (0.9058) acc_u 6.2500 (11.2500) lr 9.8429e-04 eta 0:00:25
epoch [103/200] batch [20/71] time 0.467 (0.448) data 0.337 (0.317) loss_u loss_u 0.8833 (0.9102) acc_u 15.6250 (10.7812) lr 9.8429e-04 eta 0:00:22
epoch [103/200] batch [25/71] time 0.358 (0.445) data 0.227 (0.315) loss_u loss_u 0.9150 (0.9137) acc_u 18.7500 (10.8750) lr 9.8429e-04 eta 0:00:20
epoch [103/200] batch [30/71] time 0.418 (0.445) data 0.287 (0.314) loss_u loss_u 0.9355 (0.9113) acc_u 3.1250 (10.9375) lr 9.8429e-04 eta 0:00:18
epoch [103/200] batch [35/71] time 0.403 (0.446) data 0.272 (0.315) loss_u loss_u 0.9414 (0.9115) acc_u 9.3750 (10.8929) lr 9.8429e-04 eta 0:00:16
epoch [103/200] batch [40/71] time 0.507 (0.444) data 0.375 (0.313) loss_u loss_u 0.8501 (0.9068) acc_u 15.6250 (11.3281) lr 9.8429e-04 eta 0:00:13
epoch [103/200] batch [45/71] time 0.417 (0.444) data 0.286 (0.314) loss_u loss_u 0.9106 (0.9070) acc_u 9.3750 (11.1111) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [50/71] time 0.420 (0.445) data 0.291 (0.314) loss_u loss_u 0.9302 (0.9044) acc_u 9.3750 (11.7500) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [55/71] time 0.461 (0.444) data 0.331 (0.314) loss_u loss_u 0.9585 (0.9026) acc_u 6.2500 (11.9886) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [60/71] time 0.359 (0.444) data 0.228 (0.313) loss_u loss_u 0.9072 (0.9022) acc_u 12.5000 (12.0312) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [65/71] time 0.395 (0.441) data 0.264 (0.310) loss_u loss_u 0.8975 (0.9026) acc_u 12.5000 (11.8750) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [70/71] time 0.423 (0.440) data 0.292 (0.309) loss_u loss_u 0.8848 (0.9010) acc_u 6.2500 (11.7857) lr 9.8429e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1773
confident_label rate tensor(0.2666, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 836
clean true:621
clean false:215
clean_rate:0.742822966507177
noisy true:742
noisy false:1558
after delete: len(clean_dataset) 836
after delete: len(noisy_dataset) 2300
epoch [104/200] batch [5/26] time 0.508 (0.500) data 0.377 (0.369) loss_x loss_x 1.8145 (1.5486) acc_x 56.2500 (61.8750) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [10/26] time 0.483 (0.515) data 0.351 (0.383) loss_x loss_x 0.9551 (1.3246) acc_x 65.6250 (67.1875) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [15/26] time 0.457 (0.500) data 0.327 (0.369) loss_x loss_x 1.2207 (1.2937) acc_x 68.7500 (66.4583) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [20/26] time 0.465 (0.488) data 0.334 (0.357) loss_x loss_x 0.6875 (1.2366) acc_x 90.6250 (69.0625) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [25/26] time 0.444 (0.483) data 0.314 (0.352) loss_x loss_x 1.4268 (1.2391) acc_x 75.0000 (70.2500) lr 9.6859e-04 eta 0:00:00
epoch [104/200] batch [5/71] time 0.614 (0.481) data 0.484 (0.350) loss_u loss_u 0.8945 (0.8831) acc_u 18.7500 (16.8750) lr 9.6859e-04 eta 0:00:31
epoch [104/200] batch [10/71] time 0.390 (0.480) data 0.258 (0.349) loss_u loss_u 0.9292 (0.8829) acc_u 9.3750 (15.0000) lr 9.6859e-04 eta 0:00:29
epoch [104/200] batch [15/71] time 0.547 (0.474) data 0.415 (0.343) loss_u loss_u 0.8989 (0.8851) acc_u 15.6250 (14.7917) lr 9.6859e-04 eta 0:00:26
epoch [104/200] batch [20/71] time 0.518 (0.474) data 0.385 (0.343) loss_u loss_u 0.9458 (0.8835) acc_u 6.2500 (14.3750) lr 9.6859e-04 eta 0:00:24
epoch [104/200] batch [25/71] time 0.549 (0.473) data 0.417 (0.341) loss_u loss_u 0.8613 (0.8871) acc_u 15.6250 (13.8750) lr 9.6859e-04 eta 0:00:21
epoch [104/200] batch [30/71] time 0.378 (0.467) data 0.246 (0.335) loss_u loss_u 0.9399 (0.8891) acc_u 6.2500 (13.6458) lr 9.6859e-04 eta 0:00:19
epoch [104/200] batch [35/71] time 0.415 (0.471) data 0.282 (0.339) loss_u loss_u 0.9688 (0.8886) acc_u 3.1250 (13.9286) lr 9.6859e-04 eta 0:00:16
epoch [104/200] batch [40/71] time 0.418 (0.466) data 0.286 (0.334) loss_u loss_u 0.9102 (0.8918) acc_u 12.5000 (13.2812) lr 9.6859e-04 eta 0:00:14
epoch [104/200] batch [45/71] time 0.389 (0.462) data 0.257 (0.330) loss_u loss_u 0.8872 (0.8917) acc_u 15.6250 (13.3333) lr 9.6859e-04 eta 0:00:12
epoch [104/200] batch [50/71] time 0.492 (0.464) data 0.361 (0.332) loss_u loss_u 0.8638 (0.8928) acc_u 12.5000 (13.0000) lr 9.6859e-04 eta 0:00:09
epoch [104/200] batch [55/71] time 0.461 (0.464) data 0.329 (0.332) loss_u loss_u 0.8872 (0.8938) acc_u 15.6250 (12.8977) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [60/71] time 0.489 (0.470) data 0.358 (0.338) loss_u loss_u 0.9380 (0.8936) acc_u 6.2500 (13.0729) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [65/71] time 0.367 (0.471) data 0.235 (0.339) loss_u loss_u 0.8584 (0.8921) acc_u 21.8750 (13.3173) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [70/71] time 0.459 (0.467) data 0.326 (0.336) loss_u loss_u 0.9365 (0.8937) acc_u 6.2500 (13.2143) lr 9.6859e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1721
confident_label rate tensor(0.2707, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 849
clean true:633
clean false:216
clean_rate:0.7455830388692579
noisy true:782
noisy false:1505
after delete: len(clean_dataset) 849
after delete: len(noisy_dataset) 2287
epoch [105/200] batch [5/26] time 0.523 (0.467) data 0.392 (0.335) loss_x loss_x 0.7383 (1.0332) acc_x 84.3750 (71.2500) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [10/26] time 0.385 (0.477) data 0.253 (0.346) loss_x loss_x 1.4102 (1.2137) acc_x 59.3750 (68.7500) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [15/26] time 0.542 (0.478) data 0.410 (0.347) loss_x loss_x 1.4688 (1.2612) acc_x 68.7500 (67.9167) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [20/26] time 0.506 (0.482) data 0.374 (0.350) loss_x loss_x 0.9121 (1.2418) acc_x 71.8750 (68.4375) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [25/26] time 0.484 (0.480) data 0.353 (0.348) loss_x loss_x 0.7852 (1.2373) acc_x 81.2500 (68.0000) lr 9.5289e-04 eta 0:00:00
epoch [105/200] batch [5/71] time 0.464 (0.465) data 0.332 (0.334) loss_u loss_u 0.8896 (0.9022) acc_u 9.3750 (13.7500) lr 9.5289e-04 eta 0:00:30
epoch [105/200] batch [10/71] time 0.520 (0.458) data 0.389 (0.327) loss_u loss_u 0.9819 (0.8918) acc_u 3.1250 (15.6250) lr 9.5289e-04 eta 0:00:27
epoch [105/200] batch [15/71] time 0.416 (0.450) data 0.285 (0.318) loss_u loss_u 0.8584 (0.8998) acc_u 21.8750 (13.7500) lr 9.5289e-04 eta 0:00:25
epoch [105/200] batch [20/71] time 0.437 (0.451) data 0.305 (0.320) loss_u loss_u 0.8799 (0.8991) acc_u 15.6250 (14.0625) lr 9.5289e-04 eta 0:00:23
epoch [105/200] batch [25/71] time 0.778 (0.455) data 0.647 (0.324) loss_u loss_u 0.8984 (0.8965) acc_u 9.3750 (13.7500) lr 9.5289e-04 eta 0:00:20
epoch [105/200] batch [30/71] time 0.365 (0.455) data 0.234 (0.323) loss_u loss_u 0.9575 (0.8983) acc_u 3.1250 (13.5417) lr 9.5289e-04 eta 0:00:18
epoch [105/200] batch [35/71] time 0.586 (0.458) data 0.454 (0.326) loss_u loss_u 0.8696 (0.8975) acc_u 15.6250 (13.4821) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [40/71] time 0.486 (0.461) data 0.354 (0.329) loss_u loss_u 0.9326 (0.8968) acc_u 9.3750 (13.5938) lr 9.5289e-04 eta 0:00:14
epoch [105/200] batch [45/71] time 0.460 (0.458) data 0.329 (0.326) loss_u loss_u 0.8540 (0.8981) acc_u 15.6250 (13.3333) lr 9.5289e-04 eta 0:00:11
epoch [105/200] batch [50/71] time 0.430 (0.455) data 0.299 (0.323) loss_u loss_u 0.9175 (0.8961) acc_u 9.3750 (13.8125) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [55/71] time 0.420 (0.452) data 0.288 (0.320) loss_u loss_u 0.8682 (0.8956) acc_u 15.6250 (14.0341) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [60/71] time 0.489 (0.451) data 0.357 (0.320) loss_u loss_u 0.9336 (0.8952) acc_u 3.1250 (13.8542) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [65/71] time 0.437 (0.449) data 0.306 (0.318) loss_u loss_u 0.9014 (0.8944) acc_u 15.6250 (13.9423) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [70/71] time 0.581 (0.450) data 0.449 (0.319) loss_u loss_u 0.9111 (0.8926) acc_u 12.5000 (14.0625) lr 9.5289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1774
confident_label rate tensor(0.2688, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 843
clean true:618
clean false:225
clean_rate:0.7330960854092526
noisy true:744
noisy false:1549
after delete: len(clean_dataset) 843
after delete: len(noisy_dataset) 2293
epoch [106/200] batch [5/26] time 0.403 (0.466) data 0.273 (0.335) loss_x loss_x 0.9561 (1.1500) acc_x 75.0000 (70.0000) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [10/26] time 0.489 (0.463) data 0.359 (0.332) loss_x loss_x 1.4014 (1.1238) acc_x 65.6250 (71.8750) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [15/26] time 0.483 (0.462) data 0.352 (0.331) loss_x loss_x 1.3770 (1.1542) acc_x 62.5000 (71.0417) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [20/26] time 0.434 (0.475) data 0.304 (0.344) loss_x loss_x 1.0107 (1.2213) acc_x 75.0000 (69.5312) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [25/26] time 0.507 (0.467) data 0.376 (0.336) loss_x loss_x 1.4014 (1.2661) acc_x 71.8750 (69.1250) lr 9.3721e-04 eta 0:00:00
epoch [106/200] batch [5/71] time 0.459 (0.458) data 0.327 (0.327) loss_u loss_u 0.9502 (0.8840) acc_u 3.1250 (13.7500) lr 9.3721e-04 eta 0:00:30
epoch [106/200] batch [10/71] time 0.415 (0.452) data 0.283 (0.321) loss_u loss_u 0.9702 (0.9035) acc_u 3.1250 (10.3125) lr 9.3721e-04 eta 0:00:27
epoch [106/200] batch [15/71] time 0.404 (0.452) data 0.272 (0.320) loss_u loss_u 0.9775 (0.9044) acc_u 3.1250 (10.6250) lr 9.3721e-04 eta 0:00:25
epoch [106/200] batch [20/71] time 0.462 (0.450) data 0.330 (0.319) loss_u loss_u 0.8633 (0.9026) acc_u 15.6250 (11.2500) lr 9.3721e-04 eta 0:00:22
epoch [106/200] batch [25/71] time 0.472 (0.451) data 0.340 (0.320) loss_u loss_u 0.9038 (0.9036) acc_u 12.5000 (11.0000) lr 9.3721e-04 eta 0:00:20
epoch [106/200] batch [30/71] time 0.492 (0.446) data 0.360 (0.315) loss_u loss_u 0.9390 (0.9049) acc_u 6.2500 (11.1458) lr 9.3721e-04 eta 0:00:18
epoch [106/200] batch [35/71] time 0.467 (0.444) data 0.336 (0.313) loss_u loss_u 0.8677 (0.9019) acc_u 15.6250 (11.6964) lr 9.3721e-04 eta 0:00:15
epoch [106/200] batch [40/71] time 0.442 (0.444) data 0.311 (0.313) loss_u loss_u 0.9595 (0.9071) acc_u 3.1250 (10.8594) lr 9.3721e-04 eta 0:00:13
epoch [106/200] batch [45/71] time 0.408 (0.442) data 0.277 (0.311) loss_u loss_u 0.8843 (0.9003) acc_u 18.7500 (11.8750) lr 9.3721e-04 eta 0:00:11
epoch [106/200] batch [50/71] time 0.513 (0.443) data 0.381 (0.312) loss_u loss_u 0.8701 (0.9008) acc_u 15.6250 (11.7500) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [55/71] time 0.433 (0.446) data 0.301 (0.315) loss_u loss_u 0.8887 (0.8985) acc_u 9.3750 (12.1023) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [60/71] time 0.335 (0.443) data 0.204 (0.312) loss_u loss_u 0.9106 (0.8955) acc_u 9.3750 (12.6042) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [65/71] time 0.410 (0.442) data 0.279 (0.310) loss_u loss_u 0.9185 (0.8945) acc_u 12.5000 (12.8365) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [70/71] time 0.497 (0.442) data 0.366 (0.311) loss_u loss_u 0.8501 (0.8940) acc_u 25.0000 (13.1696) lr 9.3721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1726
confident_label rate tensor(0.2797, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 877
clean true:652
clean false:225
clean_rate:0.7434435575826682
noisy true:758
noisy false:1501
after delete: len(clean_dataset) 877
after delete: len(noisy_dataset) 2259
epoch [107/200] batch [5/27] time 0.365 (0.431) data 0.234 (0.301) loss_x loss_x 1.6982 (1.2865) acc_x 59.3750 (71.8750) lr 9.2154e-04 eta 0:00:09
epoch [107/200] batch [10/27] time 0.412 (0.450) data 0.281 (0.319) loss_x loss_x 1.7002 (1.2988) acc_x 53.1250 (67.1875) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [15/27] time 0.618 (0.452) data 0.488 (0.321) loss_x loss_x 1.1113 (1.2882) acc_x 62.5000 (66.8750) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [20/27] time 0.448 (0.441) data 0.317 (0.310) loss_x loss_x 1.5869 (1.2829) acc_x 62.5000 (68.1250) lr 9.2154e-04 eta 0:00:03
epoch [107/200] batch [25/27] time 0.393 (0.444) data 0.263 (0.313) loss_x loss_x 1.6162 (1.2444) acc_x 62.5000 (68.6250) lr 9.2154e-04 eta 0:00:00
epoch [107/200] batch [5/70] time 0.424 (0.450) data 0.292 (0.320) loss_u loss_u 0.8892 (0.8762) acc_u 9.3750 (15.0000) lr 9.2154e-04 eta 0:00:29
epoch [107/200] batch [10/70] time 0.486 (0.454) data 0.354 (0.323) loss_u loss_u 0.8848 (0.8985) acc_u 12.5000 (11.5625) lr 9.2154e-04 eta 0:00:27
epoch [107/200] batch [15/70] time 0.520 (0.455) data 0.388 (0.324) loss_u loss_u 0.9453 (0.9011) acc_u 6.2500 (12.0833) lr 9.2154e-04 eta 0:00:25
epoch [107/200] batch [20/70] time 0.437 (0.454) data 0.302 (0.323) loss_u loss_u 0.8555 (0.8993) acc_u 21.8750 (12.6562) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [25/70] time 0.489 (0.449) data 0.358 (0.317) loss_u loss_u 0.9019 (0.8949) acc_u 12.5000 (13.0000) lr 9.2154e-04 eta 0:00:20
epoch [107/200] batch [30/70] time 0.422 (0.450) data 0.291 (0.318) loss_u loss_u 0.8340 (0.8901) acc_u 21.8750 (13.5417) lr 9.2154e-04 eta 0:00:17
epoch [107/200] batch [35/70] time 0.455 (0.452) data 0.322 (0.320) loss_u loss_u 0.9150 (0.8935) acc_u 12.5000 (13.2143) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [40/70] time 0.390 (0.448) data 0.258 (0.317) loss_u loss_u 0.8975 (0.8958) acc_u 15.6250 (12.9688) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [45/70] time 0.449 (0.448) data 0.318 (0.317) loss_u loss_u 0.8643 (0.8967) acc_u 15.6250 (12.7778) lr 9.2154e-04 eta 0:00:11
epoch [107/200] batch [50/70] time 0.544 (0.445) data 0.413 (0.313) loss_u loss_u 0.9219 (0.8929) acc_u 6.2500 (13.0000) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [55/70] time 0.375 (0.446) data 0.243 (0.314) loss_u loss_u 0.8921 (0.8917) acc_u 9.3750 (13.0682) lr 9.2154e-04 eta 0:00:06
epoch [107/200] batch [60/70] time 0.454 (0.444) data 0.322 (0.313) loss_u loss_u 0.9272 (0.8915) acc_u 6.2500 (13.0729) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [65/70] time 0.517 (0.444) data 0.385 (0.313) loss_u loss_u 0.9233 (0.8907) acc_u 9.3750 (13.2692) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [70/70] time 0.411 (0.446) data 0.279 (0.314) loss_u loss_u 0.8784 (0.8897) acc_u 15.6250 (13.4375) lr 9.2154e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1751
confident_label rate tensor(0.2851, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 894
clean true:663
clean false:231
clean_rate:0.7416107382550335
noisy true:722
noisy false:1520
after delete: len(clean_dataset) 894
after delete: len(noisy_dataset) 2242
epoch [108/200] batch [5/27] time 0.355 (0.414) data 0.224 (0.283) loss_x loss_x 1.5029 (1.3185) acc_x 65.6250 (63.1250) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [10/27] time 0.387 (0.429) data 0.257 (0.298) loss_x loss_x 1.2021 (1.3226) acc_x 65.6250 (63.7500) lr 9.0589e-04 eta 0:00:07
epoch [108/200] batch [15/27] time 0.390 (0.446) data 0.260 (0.315) loss_x loss_x 1.2529 (1.3184) acc_x 65.6250 (64.7917) lr 9.0589e-04 eta 0:00:05
epoch [108/200] batch [20/27] time 0.544 (0.473) data 0.413 (0.342) loss_x loss_x 1.1797 (1.2966) acc_x 71.8750 (65.1562) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [25/27] time 0.448 (0.464) data 0.316 (0.333) loss_x loss_x 1.1182 (1.2980) acc_x 65.6250 (66.1250) lr 9.0589e-04 eta 0:00:00
epoch [108/200] batch [5/70] time 0.487 (0.478) data 0.356 (0.347) loss_u loss_u 0.9585 (0.9039) acc_u 3.1250 (10.6250) lr 9.0589e-04 eta 0:00:31
epoch [108/200] batch [10/70] time 0.373 (0.471) data 0.243 (0.340) loss_u loss_u 0.9102 (0.8944) acc_u 12.5000 (13.4375) lr 9.0589e-04 eta 0:00:28
epoch [108/200] batch [15/70] time 0.473 (0.462) data 0.342 (0.331) loss_u loss_u 0.8379 (0.8880) acc_u 15.6250 (14.5833) lr 9.0589e-04 eta 0:00:25
epoch [108/200] batch [20/70] time 0.471 (0.460) data 0.339 (0.329) loss_u loss_u 0.9067 (0.8900) acc_u 9.3750 (14.0625) lr 9.0589e-04 eta 0:00:22
epoch [108/200] batch [25/70] time 0.442 (0.462) data 0.310 (0.331) loss_u loss_u 0.9131 (0.8926) acc_u 12.5000 (14.1250) lr 9.0589e-04 eta 0:00:20
epoch [108/200] batch [30/70] time 0.407 (0.460) data 0.276 (0.329) loss_u loss_u 0.9258 (0.8985) acc_u 9.3750 (13.3333) lr 9.0589e-04 eta 0:00:18
epoch [108/200] batch [35/70] time 0.503 (0.458) data 0.371 (0.327) loss_u loss_u 0.9092 (0.8982) acc_u 12.5000 (13.3929) lr 9.0589e-04 eta 0:00:16
epoch [108/200] batch [40/70] time 0.490 (0.456) data 0.358 (0.325) loss_u loss_u 0.9609 (0.8992) acc_u 3.1250 (13.4375) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [45/70] time 0.392 (0.450) data 0.261 (0.318) loss_u loss_u 0.9614 (0.9009) acc_u 3.1250 (13.1250) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [50/70] time 0.471 (0.448) data 0.340 (0.317) loss_u loss_u 0.8999 (0.9010) acc_u 9.3750 (12.8750) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [55/70] time 0.382 (0.446) data 0.252 (0.314) loss_u loss_u 0.8213 (0.8995) acc_u 21.8750 (13.0114) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [60/70] time 0.385 (0.443) data 0.254 (0.312) loss_u loss_u 0.9497 (0.9007) acc_u 3.1250 (12.7604) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [65/70] time 0.418 (0.444) data 0.287 (0.313) loss_u loss_u 0.9565 (0.9018) acc_u 3.1250 (12.3558) lr 9.0589e-04 eta 0:00:02
epoch [108/200] batch [70/70] time 0.433 (0.445) data 0.303 (0.314) loss_u loss_u 0.9263 (0.9015) acc_u 9.3750 (12.3661) lr 9.0589e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1763
confident_label rate tensor(0.2819, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 884
clean true:647
clean false:237
clean_rate:0.7319004524886877
noisy true:726
noisy false:1526
after delete: len(clean_dataset) 884
after delete: len(noisy_dataset) 2252
epoch [109/200] batch [5/27] time 0.433 (0.418) data 0.303 (0.288) loss_x loss_x 0.7251 (1.2931) acc_x 81.2500 (70.6250) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [10/27] time 0.456 (0.465) data 0.326 (0.335) loss_x loss_x 1.4785 (1.2997) acc_x 68.7500 (70.3125) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [15/27] time 0.424 (0.463) data 0.294 (0.333) loss_x loss_x 1.4932 (1.2843) acc_x 56.2500 (68.3333) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [20/27] time 0.424 (0.454) data 0.294 (0.323) loss_x loss_x 1.3086 (1.2460) acc_x 65.6250 (69.8438) lr 8.9027e-04 eta 0:00:03
epoch [109/200] batch [25/27] time 0.470 (0.457) data 0.340 (0.327) loss_x loss_x 1.2861 (1.2784) acc_x 68.7500 (69.0000) lr 8.9027e-04 eta 0:00:00
epoch [109/200] batch [5/70] time 0.344 (0.452) data 0.212 (0.322) loss_u loss_u 0.9336 (0.8857) acc_u 15.6250 (13.7500) lr 8.9027e-04 eta 0:00:29
epoch [109/200] batch [10/70] time 0.459 (0.453) data 0.329 (0.323) loss_u loss_u 0.8438 (0.8997) acc_u 21.8750 (11.8750) lr 8.9027e-04 eta 0:00:27
epoch [109/200] batch [15/70] time 0.507 (0.447) data 0.377 (0.317) loss_u loss_u 0.8857 (0.8925) acc_u 6.2500 (12.2917) lr 8.9027e-04 eta 0:00:24
epoch [109/200] batch [20/70] time 0.549 (0.452) data 0.419 (0.321) loss_u loss_u 0.8687 (0.8946) acc_u 15.6250 (11.8750) lr 8.9027e-04 eta 0:00:22
epoch [109/200] batch [25/70] time 0.648 (0.453) data 0.516 (0.322) loss_u loss_u 0.8525 (0.8931) acc_u 18.7500 (12.7500) lr 8.9027e-04 eta 0:00:20
epoch [109/200] batch [30/70] time 0.455 (0.451) data 0.323 (0.321) loss_u loss_u 0.8994 (0.8917) acc_u 12.5000 (13.1250) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [35/70] time 0.443 (0.450) data 0.311 (0.319) loss_u loss_u 0.8574 (0.8900) acc_u 15.6250 (13.3929) lr 8.9027e-04 eta 0:00:15
epoch [109/200] batch [40/70] time 0.438 (0.451) data 0.308 (0.320) loss_u loss_u 0.9033 (0.8937) acc_u 6.2500 (12.8906) lr 8.9027e-04 eta 0:00:13
epoch [109/200] batch [45/70] time 0.370 (0.447) data 0.239 (0.316) loss_u loss_u 0.8325 (0.8938) acc_u 18.7500 (13.1250) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [50/70] time 0.594 (0.447) data 0.463 (0.317) loss_u loss_u 0.9546 (0.8953) acc_u 0.0000 (12.8750) lr 8.9027e-04 eta 0:00:08
epoch [109/200] batch [55/70] time 0.565 (0.447) data 0.435 (0.317) loss_u loss_u 0.8696 (0.8956) acc_u 18.7500 (12.9545) lr 8.9027e-04 eta 0:00:06
epoch [109/200] batch [60/70] time 0.431 (0.446) data 0.301 (0.315) loss_u loss_u 0.8794 (0.8946) acc_u 18.7500 (13.1771) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [65/70] time 0.402 (0.445) data 0.271 (0.314) loss_u loss_u 0.8823 (0.8947) acc_u 18.7500 (13.1250) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [70/70] time 0.408 (0.444) data 0.277 (0.314) loss_u loss_u 0.9307 (0.8955) acc_u 9.3750 (12.9464) lr 8.9027e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1714
confident_label rate tensor(0.2841, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 891
clean true:662
clean false:229
clean_rate:0.7429854096520763
noisy true:760
noisy false:1485
after delete: len(clean_dataset) 891
after delete: len(noisy_dataset) 2245
epoch [110/200] batch [5/27] time 0.498 (0.447) data 0.368 (0.317) loss_x loss_x 1.0371 (1.3107) acc_x 71.8750 (66.8750) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [10/27] time 0.443 (0.449) data 0.313 (0.319) loss_x loss_x 0.9263 (1.2020) acc_x 75.0000 (69.6875) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [15/27] time 0.462 (0.459) data 0.332 (0.329) loss_x loss_x 1.3086 (1.2582) acc_x 59.3750 (66.8750) lr 8.7467e-04 eta 0:00:05
epoch [110/200] batch [20/27] time 0.485 (0.456) data 0.354 (0.326) loss_x loss_x 1.0439 (1.2281) acc_x 78.1250 (68.7500) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [25/27] time 0.788 (0.465) data 0.658 (0.335) loss_x loss_x 1.6680 (1.2405) acc_x 59.3750 (68.8750) lr 8.7467e-04 eta 0:00:00
epoch [110/200] batch [5/70] time 0.355 (0.462) data 0.223 (0.331) loss_u loss_u 0.9419 (0.9184) acc_u 6.2500 (8.7500) lr 8.7467e-04 eta 0:00:29
epoch [110/200] batch [10/70] time 0.368 (0.458) data 0.236 (0.327) loss_u loss_u 0.9844 (0.9284) acc_u 3.1250 (9.0625) lr 8.7467e-04 eta 0:00:27
epoch [110/200] batch [15/70] time 0.467 (0.457) data 0.335 (0.326) loss_u loss_u 0.8960 (0.9183) acc_u 15.6250 (11.0417) lr 8.7467e-04 eta 0:00:25
epoch [110/200] batch [20/70] time 0.295 (0.457) data 0.164 (0.326) loss_u loss_u 0.9390 (0.9172) acc_u 12.5000 (11.2500) lr 8.7467e-04 eta 0:00:22
epoch [110/200] batch [25/70] time 0.634 (0.460) data 0.503 (0.330) loss_u loss_u 0.8032 (0.9082) acc_u 18.7500 (12.0000) lr 8.7467e-04 eta 0:00:20
epoch [110/200] batch [30/70] time 0.436 (0.456) data 0.304 (0.325) loss_u loss_u 0.8975 (0.9084) acc_u 18.7500 (12.1875) lr 8.7467e-04 eta 0:00:18
epoch [110/200] batch [35/70] time 0.330 (0.452) data 0.198 (0.321) loss_u loss_u 0.9038 (0.9109) acc_u 9.3750 (11.8750) lr 8.7467e-04 eta 0:00:15
epoch [110/200] batch [40/70] time 0.357 (0.450) data 0.226 (0.319) loss_u loss_u 0.9297 (0.9096) acc_u 12.5000 (12.2656) lr 8.7467e-04 eta 0:00:13
epoch [110/200] batch [45/70] time 0.588 (0.450) data 0.457 (0.319) loss_u loss_u 0.9165 (0.9099) acc_u 12.5000 (12.2917) lr 8.7467e-04 eta 0:00:11
epoch [110/200] batch [50/70] time 0.470 (0.450) data 0.339 (0.319) loss_u loss_u 0.8706 (0.9079) acc_u 15.6250 (12.4375) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [55/70] time 0.458 (0.452) data 0.326 (0.321) loss_u loss_u 0.9287 (0.9031) acc_u 6.2500 (12.8409) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [60/70] time 0.401 (0.450) data 0.270 (0.319) loss_u loss_u 0.9067 (0.9036) acc_u 12.5000 (12.7604) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [65/70] time 0.327 (0.448) data 0.195 (0.317) loss_u loss_u 0.9043 (0.9009) acc_u 12.5000 (13.1731) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [70/70] time 0.409 (0.446) data 0.279 (0.315) loss_u loss_u 0.8931 (0.9002) acc_u 12.5000 (13.2589) lr 8.7467e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1742
confident_label rate tensor(0.2701, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 847
clean true:628
clean false:219
clean_rate:0.7414403778040142
noisy true:766
noisy false:1523
after delete: len(clean_dataset) 847
after delete: len(noisy_dataset) 2289
epoch [111/200] batch [5/26] time 0.439 (0.487) data 0.308 (0.356) loss_x loss_x 1.2500 (1.2458) acc_x 62.5000 (65.6250) lr 8.5910e-04 eta 0:00:10
epoch [111/200] batch [10/26] time 0.536 (0.479) data 0.405 (0.349) loss_x loss_x 1.5020 (1.2654) acc_x 62.5000 (65.9375) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [15/26] time 0.474 (0.452) data 0.343 (0.321) loss_x loss_x 1.2021 (1.2229) acc_x 68.7500 (67.5000) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [20/26] time 0.578 (0.471) data 0.448 (0.341) loss_x loss_x 0.8428 (1.1699) acc_x 78.1250 (68.5938) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [25/26] time 0.420 (0.455) data 0.289 (0.325) loss_x loss_x 1.3906 (1.1863) acc_x 78.1250 (69.3750) lr 8.5910e-04 eta 0:00:00
epoch [111/200] batch [5/71] time 0.433 (0.451) data 0.301 (0.320) loss_u loss_u 0.8208 (0.8374) acc_u 18.7500 (21.2500) lr 8.5910e-04 eta 0:00:29
epoch [111/200] batch [10/71] time 0.472 (0.449) data 0.341 (0.318) loss_u loss_u 0.9248 (0.8684) acc_u 6.2500 (15.9375) lr 8.5910e-04 eta 0:00:27
epoch [111/200] batch [15/71] time 0.761 (0.453) data 0.631 (0.322) loss_u loss_u 0.8970 (0.8732) acc_u 12.5000 (15.0000) lr 8.5910e-04 eta 0:00:25
epoch [111/200] batch [20/71] time 0.366 (0.451) data 0.235 (0.320) loss_u loss_u 0.8232 (0.8745) acc_u 31.2500 (15.4688) lr 8.5910e-04 eta 0:00:23
epoch [111/200] batch [25/71] time 0.385 (0.447) data 0.255 (0.316) loss_u loss_u 0.9443 (0.8840) acc_u 6.2500 (14.1250) lr 8.5910e-04 eta 0:00:20
epoch [111/200] batch [30/71] time 0.381 (0.451) data 0.250 (0.320) loss_u loss_u 0.9268 (0.8912) acc_u 15.6250 (13.7500) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [35/71] time 0.361 (0.447) data 0.230 (0.316) loss_u loss_u 0.9263 (0.8915) acc_u 12.5000 (13.6607) lr 8.5910e-04 eta 0:00:16
epoch [111/200] batch [40/71] time 0.378 (0.446) data 0.246 (0.315) loss_u loss_u 0.8574 (0.8911) acc_u 18.7500 (13.4375) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [45/71] time 0.381 (0.442) data 0.251 (0.311) loss_u loss_u 0.8813 (0.8887) acc_u 12.5000 (13.7500) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [50/71] time 0.380 (0.443) data 0.251 (0.312) loss_u loss_u 0.9058 (0.8916) acc_u 9.3750 (13.3125) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [55/71] time 0.495 (0.444) data 0.365 (0.313) loss_u loss_u 0.8794 (0.8901) acc_u 15.6250 (13.5795) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [60/71] time 0.444 (0.443) data 0.314 (0.312) loss_u loss_u 0.9048 (0.8893) acc_u 15.6250 (13.8021) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [65/71] time 0.352 (0.440) data 0.220 (0.310) loss_u loss_u 0.9111 (0.8869) acc_u 9.3750 (14.1346) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [70/71] time 0.472 (0.439) data 0.341 (0.308) loss_u loss_u 0.9019 (0.8869) acc_u 12.5000 (14.0179) lr 8.5910e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1777
confident_label rate tensor(0.2774, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 870
clean true:644
clean false:226
clean_rate:0.7402298850574712
noisy true:715
noisy false:1551
after delete: len(clean_dataset) 870
after delete: len(noisy_dataset) 2266
epoch [112/200] batch [5/27] time 0.657 (0.565) data 0.525 (0.433) loss_x loss_x 1.0469 (1.1850) acc_x 65.6250 (67.5000) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [10/27] time 0.452 (0.544) data 0.320 (0.413) loss_x loss_x 1.2461 (1.2689) acc_x 68.7500 (65.3125) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [15/27] time 0.344 (0.528) data 0.213 (0.397) loss_x loss_x 1.0322 (1.2988) acc_x 68.7500 (64.5833) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [20/27] time 0.373 (0.499) data 0.240 (0.367) loss_x loss_x 1.0635 (1.3354) acc_x 71.8750 (65.1562) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [25/27] time 0.489 (0.496) data 0.359 (0.364) loss_x loss_x 2.0781 (1.3110) acc_x 56.2500 (66.6250) lr 8.4357e-04 eta 0:00:00
epoch [112/200] batch [5/70] time 0.521 (0.487) data 0.390 (0.356) loss_u loss_u 0.9023 (0.8739) acc_u 9.3750 (16.8750) lr 8.4357e-04 eta 0:00:31
epoch [112/200] batch [10/70] time 0.477 (0.488) data 0.347 (0.357) loss_u loss_u 0.8232 (0.8797) acc_u 21.8750 (14.6875) lr 8.4357e-04 eta 0:00:29
epoch [112/200] batch [15/70] time 0.499 (0.484) data 0.367 (0.352) loss_u loss_u 0.9053 (0.8843) acc_u 12.5000 (13.9583) lr 8.4357e-04 eta 0:00:26
epoch [112/200] batch [20/70] time 0.414 (0.477) data 0.282 (0.346) loss_u loss_u 0.8970 (0.8891) acc_u 15.6250 (13.4375) lr 8.4357e-04 eta 0:00:23
epoch [112/200] batch [25/70] time 0.479 (0.472) data 0.348 (0.341) loss_u loss_u 0.8403 (0.8887) acc_u 18.7500 (13.1250) lr 8.4357e-04 eta 0:00:21
epoch [112/200] batch [30/70] time 0.475 (0.468) data 0.344 (0.337) loss_u loss_u 0.9370 (0.8921) acc_u 9.3750 (12.7083) lr 8.4357e-04 eta 0:00:18
epoch [112/200] batch [35/70] time 0.374 (0.468) data 0.242 (0.336) loss_u loss_u 0.9097 (0.8924) acc_u 12.5000 (12.5893) lr 8.4357e-04 eta 0:00:16
epoch [112/200] batch [40/70] time 0.352 (0.466) data 0.220 (0.334) loss_u loss_u 0.9517 (0.8912) acc_u 6.2500 (12.8906) lr 8.4357e-04 eta 0:00:13
epoch [112/200] batch [45/70] time 0.331 (0.460) data 0.199 (0.329) loss_u loss_u 0.8599 (0.8916) acc_u 15.6250 (12.8472) lr 8.4357e-04 eta 0:00:11
epoch [112/200] batch [50/70] time 0.486 (0.459) data 0.354 (0.328) loss_u loss_u 0.8916 (0.8923) acc_u 12.5000 (12.7500) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [55/70] time 0.436 (0.457) data 0.305 (0.326) loss_u loss_u 0.8643 (0.8925) acc_u 15.6250 (12.8409) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [60/70] time 0.368 (0.456) data 0.237 (0.324) loss_u loss_u 0.8770 (0.8933) acc_u 15.6250 (12.8125) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [65/70] time 0.442 (0.458) data 0.310 (0.327) loss_u loss_u 0.9111 (0.8934) acc_u 15.6250 (12.9808) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [70/70] time 0.524 (0.459) data 0.391 (0.327) loss_u loss_u 0.8892 (0.8951) acc_u 21.8750 (12.8125) lr 8.4357e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1805
confident_label rate tensor(0.2730, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 856
clean true:625
clean false:231
clean_rate:0.7301401869158879
noisy true:706
noisy false:1574
after delete: len(clean_dataset) 856
after delete: len(noisy_dataset) 2280
epoch [113/200] batch [5/26] time 0.607 (0.476) data 0.476 (0.345) loss_x loss_x 0.7080 (1.2445) acc_x 75.0000 (69.3750) lr 8.2807e-04 eta 0:00:10
epoch [113/200] batch [10/26] time 0.440 (0.465) data 0.310 (0.334) loss_x loss_x 1.2021 (1.1480) acc_x 62.5000 (70.6250) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [15/26] time 0.463 (0.461) data 0.332 (0.330) loss_x loss_x 1.2734 (1.1495) acc_x 75.0000 (70.6250) lr 8.2807e-04 eta 0:00:05
epoch [113/200] batch [20/26] time 0.407 (0.447) data 0.276 (0.316) loss_x loss_x 1.1426 (1.1868) acc_x 65.6250 (69.3750) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [25/26] time 0.392 (0.455) data 0.261 (0.324) loss_x loss_x 1.7061 (1.2032) acc_x 62.5000 (68.7500) lr 8.2807e-04 eta 0:00:00
epoch [113/200] batch [5/71] time 0.356 (0.452) data 0.225 (0.321) loss_u loss_u 0.8760 (0.9010) acc_u 12.5000 (13.7500) lr 8.2807e-04 eta 0:00:29
epoch [113/200] batch [10/71] time 0.411 (0.451) data 0.279 (0.320) loss_u loss_u 0.8838 (0.8912) acc_u 18.7500 (14.6875) lr 8.2807e-04 eta 0:00:27
epoch [113/200] batch [15/71] time 0.379 (0.442) data 0.248 (0.311) loss_u loss_u 0.8901 (0.8960) acc_u 15.6250 (13.9583) lr 8.2807e-04 eta 0:00:24
epoch [113/200] batch [20/71] time 0.480 (0.442) data 0.348 (0.310) loss_u loss_u 0.8950 (0.8938) acc_u 12.5000 (14.2188) lr 8.2807e-04 eta 0:00:22
epoch [113/200] batch [25/71] time 0.377 (0.436) data 0.245 (0.305) loss_u loss_u 0.9023 (0.8988) acc_u 12.5000 (13.0000) lr 8.2807e-04 eta 0:00:20
epoch [113/200] batch [30/71] time 0.380 (0.435) data 0.249 (0.304) loss_u loss_u 0.9106 (0.8981) acc_u 9.3750 (12.9167) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [35/71] time 0.465 (0.436) data 0.333 (0.304) loss_u loss_u 0.8691 (0.8973) acc_u 18.7500 (12.9464) lr 8.2807e-04 eta 0:00:15
epoch [113/200] batch [40/71] time 0.351 (0.438) data 0.219 (0.306) loss_u loss_u 0.8403 (0.8957) acc_u 21.8750 (13.2031) lr 8.2807e-04 eta 0:00:13
epoch [113/200] batch [45/71] time 0.438 (0.433) data 0.306 (0.302) loss_u loss_u 0.8760 (0.8945) acc_u 15.6250 (13.2639) lr 8.2807e-04 eta 0:00:11
epoch [113/200] batch [50/71] time 0.365 (0.436) data 0.234 (0.304) loss_u loss_u 0.8579 (0.8944) acc_u 18.7500 (13.3125) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [55/71] time 0.436 (0.437) data 0.305 (0.305) loss_u loss_u 0.8735 (0.8937) acc_u 12.5000 (13.3523) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [60/71] time 0.404 (0.435) data 0.272 (0.304) loss_u loss_u 0.8555 (0.8926) acc_u 18.7500 (13.3333) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [65/71] time 0.440 (0.436) data 0.309 (0.305) loss_u loss_u 0.9175 (0.8936) acc_u 12.5000 (13.2212) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [70/71] time 0.456 (0.441) data 0.325 (0.309) loss_u loss_u 0.9136 (0.8934) acc_u 12.5000 (13.3036) lr 8.2807e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1723
confident_label rate tensor(0.2841, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 891
clean true:654
clean false:237
clean_rate:0.734006734006734
noisy true:759
noisy false:1486
after delete: len(clean_dataset) 891
after delete: len(noisy_dataset) 2245
epoch [114/200] batch [5/27] time 0.464 (0.463) data 0.334 (0.333) loss_x loss_x 1.3271 (1.3795) acc_x 65.6250 (63.7500) lr 8.1262e-04 eta 0:00:10
epoch [114/200] batch [10/27] time 0.408 (0.464) data 0.278 (0.334) loss_x loss_x 0.7412 (1.2159) acc_x 78.1250 (66.5625) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [15/27] time 0.767 (0.470) data 0.636 (0.339) loss_x loss_x 1.3359 (1.2137) acc_x 68.7500 (67.0833) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [20/27] time 0.400 (0.465) data 0.270 (0.334) loss_x loss_x 1.5088 (1.2653) acc_x 59.3750 (66.2500) lr 8.1262e-04 eta 0:00:03
epoch [114/200] batch [25/27] time 0.457 (0.477) data 0.326 (0.346) loss_x loss_x 1.4180 (1.2688) acc_x 59.3750 (66.7500) lr 8.1262e-04 eta 0:00:00
epoch [114/200] batch [5/70] time 0.444 (0.478) data 0.313 (0.348) loss_u loss_u 0.8682 (0.8935) acc_u 12.5000 (12.5000) lr 8.1262e-04 eta 0:00:31
epoch [114/200] batch [10/70] time 0.561 (0.476) data 0.430 (0.346) loss_u loss_u 0.8589 (0.8938) acc_u 15.6250 (12.8125) lr 8.1262e-04 eta 0:00:28
epoch [114/200] batch [15/70] time 0.393 (0.468) data 0.261 (0.337) loss_u loss_u 0.8687 (0.8993) acc_u 15.6250 (12.2917) lr 8.1262e-04 eta 0:00:25
epoch [114/200] batch [20/70] time 0.377 (0.466) data 0.242 (0.335) loss_u loss_u 0.9023 (0.9058) acc_u 12.5000 (11.4062) lr 8.1262e-04 eta 0:00:23
epoch [114/200] batch [25/70] time 0.434 (0.466) data 0.302 (0.335) loss_u loss_u 0.9336 (0.9100) acc_u 6.2500 (10.8750) lr 8.1262e-04 eta 0:00:20
epoch [114/200] batch [30/70] time 0.468 (0.469) data 0.336 (0.337) loss_u loss_u 0.9194 (0.9053) acc_u 12.5000 (11.6667) lr 8.1262e-04 eta 0:00:18
epoch [114/200] batch [35/70] time 0.402 (0.471) data 0.270 (0.339) loss_u loss_u 0.8413 (0.8992) acc_u 21.8750 (12.2321) lr 8.1262e-04 eta 0:00:16
epoch [114/200] batch [40/70] time 0.372 (0.465) data 0.240 (0.334) loss_u loss_u 0.8911 (0.9003) acc_u 12.5000 (12.0312) lr 8.1262e-04 eta 0:00:13
epoch [114/200] batch [45/70] time 0.357 (0.464) data 0.225 (0.333) loss_u loss_u 0.9370 (0.8986) acc_u 12.5000 (12.2917) lr 8.1262e-04 eta 0:00:11
epoch [114/200] batch [50/70] time 0.425 (0.464) data 0.292 (0.332) loss_u loss_u 0.8770 (0.8995) acc_u 18.7500 (12.3750) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [55/70] time 0.450 (0.461) data 0.319 (0.330) loss_u loss_u 0.7593 (0.8962) acc_u 31.2500 (12.7273) lr 8.1262e-04 eta 0:00:06
epoch [114/200] batch [60/70] time 0.376 (0.460) data 0.245 (0.328) loss_u loss_u 0.8940 (0.8962) acc_u 9.3750 (12.7083) lr 8.1262e-04 eta 0:00:04
epoch [114/200] batch [65/70] time 0.554 (0.462) data 0.423 (0.330) loss_u loss_u 0.9487 (0.8969) acc_u 6.2500 (12.6442) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [70/70] time 0.440 (0.459) data 0.308 (0.328) loss_u loss_u 0.8887 (0.8960) acc_u 12.5000 (12.6786) lr 8.1262e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1763
confident_label rate tensor(0.2701, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 847
clean true:628
clean false:219
clean_rate:0.7414403778040142
noisy true:745
noisy false:1544
after delete: len(clean_dataset) 847
after delete: len(noisy_dataset) 2289
epoch [115/200] batch [5/26] time 0.449 (0.433) data 0.318 (0.302) loss_x loss_x 0.9126 (1.1159) acc_x 71.8750 (67.5000) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [10/26] time 0.389 (0.416) data 0.259 (0.285) loss_x loss_x 1.8828 (1.2451) acc_x 53.1250 (66.2500) lr 7.9721e-04 eta 0:00:06
epoch [115/200] batch [15/26] time 0.590 (0.454) data 0.460 (0.323) loss_x loss_x 1.4443 (1.2782) acc_x 65.6250 (67.0833) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [20/26] time 0.445 (0.450) data 0.314 (0.320) loss_x loss_x 1.4463 (1.3162) acc_x 71.8750 (67.1875) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [25/26] time 0.367 (0.454) data 0.238 (0.324) loss_x loss_x 1.2969 (1.3038) acc_x 59.3750 (67.1250) lr 7.9721e-04 eta 0:00:00
epoch [115/200] batch [5/71] time 0.383 (0.447) data 0.251 (0.316) loss_u loss_u 0.8730 (0.8988) acc_u 15.6250 (11.2500) lr 7.9721e-04 eta 0:00:29
epoch [115/200] batch [10/71] time 0.403 (0.448) data 0.271 (0.318) loss_u loss_u 0.9395 (0.8960) acc_u 3.1250 (11.8750) lr 7.9721e-04 eta 0:00:27
epoch [115/200] batch [15/71] time 0.497 (0.456) data 0.365 (0.326) loss_u loss_u 0.8423 (0.8934) acc_u 18.7500 (12.0833) lr 7.9721e-04 eta 0:00:25
epoch [115/200] batch [20/71] time 0.395 (0.456) data 0.264 (0.325) loss_u loss_u 0.8564 (0.8992) acc_u 15.6250 (11.4062) lr 7.9721e-04 eta 0:00:23
epoch [115/200] batch [25/71] time 0.403 (0.455) data 0.271 (0.324) loss_u loss_u 0.8550 (0.8942) acc_u 21.8750 (12.7500) lr 7.9721e-04 eta 0:00:20
epoch [115/200] batch [30/71] time 0.454 (0.458) data 0.323 (0.327) loss_u loss_u 0.9448 (0.8935) acc_u 3.1250 (13.0208) lr 7.9721e-04 eta 0:00:18
epoch [115/200] batch [35/71] time 0.504 (0.456) data 0.372 (0.325) loss_u loss_u 0.8613 (0.8949) acc_u 18.7500 (12.9464) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [40/71] time 0.404 (0.457) data 0.273 (0.326) loss_u loss_u 0.8667 (0.8917) acc_u 12.5000 (13.3594) lr 7.9721e-04 eta 0:00:14
epoch [115/200] batch [45/71] time 0.384 (0.455) data 0.254 (0.324) loss_u loss_u 0.9448 (0.8956) acc_u 3.1250 (13.0556) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [50/71] time 0.405 (0.453) data 0.273 (0.321) loss_u loss_u 0.8662 (0.8962) acc_u 18.7500 (12.9375) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [55/71] time 0.381 (0.449) data 0.251 (0.317) loss_u loss_u 0.8701 (0.8950) acc_u 18.7500 (13.2386) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [60/71] time 0.401 (0.448) data 0.269 (0.317) loss_u loss_u 0.8906 (0.8944) acc_u 12.5000 (13.1250) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [65/71] time 0.450 (0.447) data 0.317 (0.315) loss_u loss_u 0.8887 (0.8956) acc_u 15.6250 (12.9808) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [70/71] time 0.426 (0.443) data 0.295 (0.311) loss_u loss_u 0.9277 (0.8957) acc_u 9.3750 (12.9911) lr 7.9721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1722
confident_label rate tensor(0.2832, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 888
clean true:655
clean false:233
clean_rate:0.7376126126126126
noisy true:759
noisy false:1489
after delete: len(clean_dataset) 888
after delete: len(noisy_dataset) 2248
epoch [116/200] batch [5/27] time 0.622 (0.522) data 0.490 (0.390) loss_x loss_x 1.1748 (1.1592) acc_x 71.8750 (71.2500) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [10/27] time 0.502 (0.499) data 0.370 (0.367) loss_x loss_x 1.0381 (1.1897) acc_x 75.0000 (70.3125) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [15/27] time 0.401 (0.479) data 0.270 (0.348) loss_x loss_x 1.9150 (1.2671) acc_x 53.1250 (68.3333) lr 7.8186e-04 eta 0:00:05
epoch [116/200] batch [20/27] time 0.403 (0.480) data 0.271 (0.348) loss_x loss_x 1.1113 (1.2275) acc_x 78.1250 (68.2812) lr 7.8186e-04 eta 0:00:03
epoch [116/200] batch [25/27] time 0.482 (0.472) data 0.351 (0.340) loss_x loss_x 1.3096 (1.2462) acc_x 71.8750 (67.2500) lr 7.8186e-04 eta 0:00:00
epoch [116/200] batch [5/70] time 0.407 (0.476) data 0.274 (0.344) loss_u loss_u 0.8823 (0.8703) acc_u 15.6250 (18.7500) lr 7.8186e-04 eta 0:00:30
epoch [116/200] batch [10/70] time 0.626 (0.484) data 0.493 (0.352) loss_u loss_u 0.8760 (0.8650) acc_u 18.7500 (18.7500) lr 7.8186e-04 eta 0:00:29
epoch [116/200] batch [15/70] time 0.575 (0.482) data 0.443 (0.350) loss_u loss_u 0.9067 (0.8649) acc_u 9.3750 (18.9583) lr 7.8186e-04 eta 0:00:26
epoch [116/200] batch [20/70] time 0.482 (0.482) data 0.351 (0.350) loss_u loss_u 0.8574 (0.8704) acc_u 18.7500 (17.5000) lr 7.8186e-04 eta 0:00:24
epoch [116/200] batch [25/70] time 0.351 (0.476) data 0.219 (0.344) loss_u loss_u 0.9409 (0.8794) acc_u 9.3750 (16.2500) lr 7.8186e-04 eta 0:00:21
epoch [116/200] batch [30/70] time 0.414 (0.470) data 0.282 (0.338) loss_u loss_u 0.8809 (0.8828) acc_u 15.6250 (15.6250) lr 7.8186e-04 eta 0:00:18
epoch [116/200] batch [35/70] time 0.353 (0.465) data 0.221 (0.333) loss_u loss_u 0.8931 (0.8834) acc_u 6.2500 (15.0893) lr 7.8186e-04 eta 0:00:16
epoch [116/200] batch [40/70] time 0.469 (0.466) data 0.337 (0.334) loss_u loss_u 0.9072 (0.8868) acc_u 9.3750 (14.9219) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [45/70] time 0.421 (0.462) data 0.290 (0.330) loss_u loss_u 0.8770 (0.8857) acc_u 12.5000 (14.7917) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [50/70] time 0.427 (0.461) data 0.296 (0.329) loss_u loss_u 0.9033 (0.8878) acc_u 12.5000 (14.4375) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [55/70] time 0.425 (0.463) data 0.291 (0.331) loss_u loss_u 0.9761 (0.8892) acc_u 6.2500 (14.3182) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [60/70] time 0.507 (0.461) data 0.375 (0.329) loss_u loss_u 0.8555 (0.8887) acc_u 21.8750 (14.4792) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [65/70] time 0.407 (0.459) data 0.276 (0.327) loss_u loss_u 0.9189 (0.8912) acc_u 9.3750 (14.0865) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [70/70] time 0.349 (0.457) data 0.217 (0.326) loss_u loss_u 0.7944 (0.8919) acc_u 28.1250 (14.0179) lr 7.8186e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1744
confident_label rate tensor(0.2803, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 879
clean true:632
clean false:247
clean_rate:0.7189988623435722
noisy true:760
noisy false:1497
after delete: len(clean_dataset) 879
after delete: len(noisy_dataset) 2257
epoch [117/200] batch [5/27] time 0.391 (0.440) data 0.259 (0.309) loss_x loss_x 1.4551 (1.5277) acc_x 53.1250 (63.1250) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [10/27] time 0.392 (0.433) data 0.261 (0.302) loss_x loss_x 0.9355 (1.4083) acc_x 71.8750 (65.0000) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [15/27] time 0.417 (0.464) data 0.287 (0.333) loss_x loss_x 1.5781 (1.3473) acc_x 71.8750 (67.7083) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [20/27] time 0.402 (0.463) data 0.272 (0.332) loss_x loss_x 1.2646 (1.2989) acc_x 59.3750 (68.4375) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [25/27] time 0.618 (0.455) data 0.488 (0.324) loss_x loss_x 1.0322 (1.2874) acc_x 75.0000 (68.5000) lr 7.6655e-04 eta 0:00:00
epoch [117/200] batch [5/70] time 0.440 (0.454) data 0.310 (0.323) loss_u loss_u 0.9175 (0.8855) acc_u 9.3750 (13.1250) lr 7.6655e-04 eta 0:00:29
epoch [117/200] batch [10/70] time 0.355 (0.447) data 0.225 (0.317) loss_u loss_u 0.8848 (0.8896) acc_u 12.5000 (12.8125) lr 7.6655e-04 eta 0:00:26
epoch [117/200] batch [15/70] time 0.506 (0.456) data 0.369 (0.325) loss_u loss_u 0.8955 (0.8872) acc_u 9.3750 (13.3333) lr 7.6655e-04 eta 0:00:25
epoch [117/200] batch [20/70] time 0.587 (0.461) data 0.457 (0.330) loss_u loss_u 0.8960 (0.8910) acc_u 9.3750 (12.9688) lr 7.6655e-04 eta 0:00:23
epoch [117/200] batch [25/70] time 0.354 (0.460) data 0.222 (0.330) loss_u loss_u 0.8618 (0.8932) acc_u 21.8750 (13.2500) lr 7.6655e-04 eta 0:00:20
epoch [117/200] batch [30/70] time 0.453 (0.456) data 0.322 (0.325) loss_u loss_u 0.9541 (0.8940) acc_u 6.2500 (13.4375) lr 7.6655e-04 eta 0:00:18
epoch [117/200] batch [35/70] time 0.367 (0.452) data 0.235 (0.321) loss_u loss_u 0.9453 (0.8979) acc_u 6.2500 (12.7679) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [40/70] time 0.364 (0.449) data 0.232 (0.318) loss_u loss_u 0.8423 (0.8969) acc_u 18.7500 (12.8906) lr 7.6655e-04 eta 0:00:13
epoch [117/200] batch [45/70] time 0.429 (0.450) data 0.296 (0.319) loss_u loss_u 0.8540 (0.8971) acc_u 15.6250 (12.7778) lr 7.6655e-04 eta 0:00:11
epoch [117/200] batch [50/70] time 0.379 (0.446) data 0.248 (0.315) loss_u loss_u 0.9248 (0.8980) acc_u 6.2500 (12.7500) lr 7.6655e-04 eta 0:00:08
epoch [117/200] batch [55/70] time 0.376 (0.446) data 0.245 (0.315) loss_u loss_u 0.8809 (0.8978) acc_u 9.3750 (12.6136) lr 7.6655e-04 eta 0:00:06
epoch [117/200] batch [60/70] time 0.439 (0.446) data 0.307 (0.315) loss_u loss_u 0.8706 (0.8979) acc_u 15.6250 (12.5000) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [65/70] time 0.426 (0.446) data 0.295 (0.315) loss_u loss_u 0.8926 (0.8989) acc_u 18.7500 (12.4519) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [70/70] time 0.409 (0.444) data 0.278 (0.313) loss_u loss_u 0.9463 (0.8987) acc_u 6.2500 (12.5893) lr 7.6655e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.2848, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 893
clean true:640
clean false:253
clean_rate:0.7166853303471444
noisy true:747
noisy false:1496
after delete: len(clean_dataset) 893
after delete: len(noisy_dataset) 2243
epoch [118/200] batch [5/27] time 0.641 (0.540) data 0.510 (0.409) loss_x loss_x 0.9634 (1.1802) acc_x 75.0000 (70.0000) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [10/27] time 0.490 (0.496) data 0.359 (0.365) loss_x loss_x 1.4072 (1.1524) acc_x 68.7500 (70.3125) lr 7.5131e-04 eta 0:00:08
epoch [118/200] batch [15/27] time 0.417 (0.473) data 0.286 (0.342) loss_x loss_x 0.9795 (1.1671) acc_x 68.7500 (69.5833) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [20/27] time 0.526 (0.472) data 0.395 (0.341) loss_x loss_x 1.3105 (1.1396) acc_x 65.6250 (70.3125) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [25/27] time 0.451 (0.463) data 0.321 (0.332) loss_x loss_x 1.5693 (1.1488) acc_x 62.5000 (70.2500) lr 7.5131e-04 eta 0:00:00
epoch [118/200] batch [5/70] time 0.382 (0.453) data 0.251 (0.322) loss_u loss_u 0.9541 (0.9135) acc_u 3.1250 (10.0000) lr 7.5131e-04 eta 0:00:29
epoch [118/200] batch [10/70] time 0.455 (0.450) data 0.323 (0.319) loss_u loss_u 0.7827 (0.8855) acc_u 28.1250 (14.0625) lr 7.5131e-04 eta 0:00:26
epoch [118/200] batch [15/70] time 0.445 (0.447) data 0.313 (0.316) loss_u loss_u 0.9277 (0.8931) acc_u 6.2500 (13.1250) lr 7.5131e-04 eta 0:00:24
epoch [118/200] batch [20/70] time 0.423 (0.446) data 0.292 (0.315) loss_u loss_u 0.9170 (0.8944) acc_u 9.3750 (13.1250) lr 7.5131e-04 eta 0:00:22
epoch [118/200] batch [25/70] time 0.393 (0.445) data 0.262 (0.314) loss_u loss_u 0.9282 (0.8958) acc_u 12.5000 (12.8750) lr 7.5131e-04 eta 0:00:20
epoch [118/200] batch [30/70] time 0.351 (0.440) data 0.219 (0.309) loss_u loss_u 0.8643 (0.8958) acc_u 15.6250 (12.8125) lr 7.5131e-04 eta 0:00:17
epoch [118/200] batch [35/70] time 0.416 (0.440) data 0.284 (0.309) loss_u loss_u 0.8970 (0.8979) acc_u 15.6250 (12.6786) lr 7.5131e-04 eta 0:00:15
epoch [118/200] batch [40/70] time 0.338 (0.442) data 0.207 (0.310) loss_u loss_u 0.9019 (0.8968) acc_u 12.5000 (12.9688) lr 7.5131e-04 eta 0:00:13
epoch [118/200] batch [45/70] time 0.431 (0.440) data 0.301 (0.309) loss_u loss_u 0.8667 (0.8956) acc_u 15.6250 (13.1250) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [50/70] time 0.424 (0.443) data 0.294 (0.312) loss_u loss_u 0.9478 (0.8973) acc_u 3.1250 (12.7500) lr 7.5131e-04 eta 0:00:08
epoch [118/200] batch [55/70] time 0.433 (0.445) data 0.302 (0.314) loss_u loss_u 0.8369 (0.8970) acc_u 21.8750 (12.7273) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [60/70] time 0.369 (0.445) data 0.237 (0.314) loss_u loss_u 0.9351 (0.8959) acc_u 6.2500 (12.8125) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [65/70] time 0.361 (0.444) data 0.230 (0.313) loss_u loss_u 0.9185 (0.8968) acc_u 6.2500 (12.7885) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [70/70] time 0.362 (0.443) data 0.232 (0.311) loss_u loss_u 0.9370 (0.8960) acc_u 9.3750 (13.0804) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1776
confident_label rate tensor(0.2784, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 873
clean true:647
clean false:226
clean_rate:0.7411225658648339
noisy true:713
noisy false:1550
after delete: len(clean_dataset) 873
after delete: len(noisy_dataset) 2263
epoch [119/200] batch [5/27] time 0.496 (0.530) data 0.365 (0.399) loss_x loss_x 1.7422 (1.2139) acc_x 62.5000 (70.0000) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [10/27] time 0.514 (0.507) data 0.382 (0.375) loss_x loss_x 1.2695 (1.2613) acc_x 68.7500 (67.1875) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [15/27] time 0.362 (0.471) data 0.231 (0.339) loss_x loss_x 1.4658 (1.2606) acc_x 62.5000 (68.3333) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [20/27] time 0.479 (0.474) data 0.348 (0.343) loss_x loss_x 1.0605 (1.2390) acc_x 65.6250 (69.3750) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [25/27] time 0.753 (0.486) data 0.621 (0.355) loss_x loss_x 1.4922 (1.1933) acc_x 62.5000 (69.8750) lr 7.3613e-04 eta 0:00:00
epoch [119/200] batch [5/70] time 0.395 (0.492) data 0.262 (0.360) loss_u loss_u 0.8779 (0.8991) acc_u 12.5000 (11.2500) lr 7.3613e-04 eta 0:00:31
epoch [119/200] batch [10/70] time 0.457 (0.484) data 0.325 (0.352) loss_u loss_u 0.8716 (0.9007) acc_u 18.7500 (12.5000) lr 7.3613e-04 eta 0:00:29
epoch [119/200] batch [15/70] time 0.410 (0.477) data 0.278 (0.345) loss_u loss_u 0.9165 (0.9004) acc_u 6.2500 (12.0833) lr 7.3613e-04 eta 0:00:26
epoch [119/200] batch [20/70] time 0.416 (0.475) data 0.285 (0.344) loss_u loss_u 0.8896 (0.9004) acc_u 12.5000 (11.8750) lr 7.3613e-04 eta 0:00:23
epoch [119/200] batch [25/70] time 0.425 (0.472) data 0.294 (0.340) loss_u loss_u 0.9048 (0.8984) acc_u 12.5000 (12.1250) lr 7.3613e-04 eta 0:00:21
epoch [119/200] batch [30/70] time 0.424 (0.474) data 0.292 (0.342) loss_u loss_u 0.9150 (0.8985) acc_u 9.3750 (12.1875) lr 7.3613e-04 eta 0:00:18
epoch [119/200] batch [35/70] time 0.388 (0.470) data 0.256 (0.338) loss_u loss_u 0.8745 (0.8944) acc_u 12.5000 (12.5000) lr 7.3613e-04 eta 0:00:16
epoch [119/200] batch [40/70] time 0.568 (0.471) data 0.437 (0.339) loss_u loss_u 0.9424 (0.8960) acc_u 3.1250 (12.1875) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [45/70] time 0.339 (0.469) data 0.207 (0.337) loss_u loss_u 0.8462 (0.8950) acc_u 18.7500 (12.3611) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [50/70] time 0.458 (0.464) data 0.328 (0.332) loss_u loss_u 0.9448 (0.8974) acc_u 6.2500 (12.0000) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [55/70] time 0.338 (0.460) data 0.208 (0.328) loss_u loss_u 0.9253 (0.8969) acc_u 12.5000 (12.1591) lr 7.3613e-04 eta 0:00:06
epoch [119/200] batch [60/70] time 0.453 (0.458) data 0.322 (0.326) loss_u loss_u 0.9082 (0.8975) acc_u 12.5000 (12.1354) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [65/70] time 0.378 (0.456) data 0.246 (0.325) loss_u loss_u 0.8706 (0.8978) acc_u 15.6250 (12.0673) lr 7.3613e-04 eta 0:00:02
epoch [119/200] batch [70/70] time 0.346 (0.456) data 0.214 (0.324) loss_u loss_u 0.9106 (0.8989) acc_u 9.3750 (12.0089) lr 7.3613e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1752
confident_label rate tensor(0.2787, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 874
clean true:650
clean false:224
clean_rate:0.7437070938215103
noisy true:734
noisy false:1528
after delete: len(clean_dataset) 874
after delete: len(noisy_dataset) 2262
epoch [120/200] batch [5/27] time 0.418 (0.460) data 0.288 (0.329) loss_x loss_x 1.2422 (1.1349) acc_x 65.6250 (70.0000) lr 7.2101e-04 eta 0:00:10
epoch [120/200] batch [10/27] time 0.488 (0.508) data 0.358 (0.377) loss_x loss_x 0.8384 (1.1057) acc_x 71.8750 (70.3125) lr 7.2101e-04 eta 0:00:08
epoch [120/200] batch [15/27] time 0.409 (0.469) data 0.279 (0.338) loss_x loss_x 1.1787 (1.1868) acc_x 78.1250 (70.0000) lr 7.2101e-04 eta 0:00:05
epoch [120/200] batch [20/27] time 0.471 (0.468) data 0.341 (0.337) loss_x loss_x 1.1455 (1.1750) acc_x 59.3750 (70.3125) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [25/27] time 0.654 (0.466) data 0.523 (0.335) loss_x loss_x 0.8218 (1.1591) acc_x 71.8750 (70.2500) lr 7.2101e-04 eta 0:00:00
epoch [120/200] batch [5/70] time 0.470 (0.462) data 0.337 (0.331) loss_u loss_u 0.8877 (0.9117) acc_u 15.6250 (12.5000) lr 7.2101e-04 eta 0:00:30
epoch [120/200] batch [10/70] time 0.541 (0.460) data 0.409 (0.329) loss_u loss_u 0.9048 (0.9088) acc_u 9.3750 (11.2500) lr 7.2101e-04 eta 0:00:27
epoch [120/200] batch [15/70] time 0.427 (0.452) data 0.296 (0.321) loss_u loss_u 0.8047 (0.8977) acc_u 21.8750 (12.9167) lr 7.2101e-04 eta 0:00:24
epoch [120/200] batch [20/70] time 0.488 (0.449) data 0.357 (0.318) loss_u loss_u 0.9341 (0.9058) acc_u 3.1250 (11.7188) lr 7.2101e-04 eta 0:00:22
epoch [120/200] batch [25/70] time 0.342 (0.447) data 0.211 (0.316) loss_u loss_u 0.8271 (0.8985) acc_u 21.8750 (12.5000) lr 7.2101e-04 eta 0:00:20
epoch [120/200] batch [30/70] time 0.372 (0.449) data 0.241 (0.318) loss_u loss_u 0.8872 (0.8969) acc_u 9.3750 (12.5000) lr 7.2101e-04 eta 0:00:17
epoch [120/200] batch [35/70] time 0.382 (0.446) data 0.250 (0.315) loss_u loss_u 0.8047 (0.8902) acc_u 25.0000 (13.5714) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [40/70] time 0.365 (0.446) data 0.233 (0.315) loss_u loss_u 0.8916 (0.8917) acc_u 15.6250 (13.2812) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [45/70] time 0.413 (0.445) data 0.281 (0.314) loss_u loss_u 0.8877 (0.8941) acc_u 12.5000 (12.9167) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [50/70] time 0.450 (0.445) data 0.319 (0.314) loss_u loss_u 0.9106 (0.8927) acc_u 6.2500 (13.1875) lr 7.2101e-04 eta 0:00:08
epoch [120/200] batch [55/70] time 0.385 (0.443) data 0.254 (0.312) loss_u loss_u 0.9390 (0.8940) acc_u 6.2500 (13.0682) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [60/70] time 0.504 (0.442) data 0.372 (0.311) loss_u loss_u 0.8970 (0.8960) acc_u 12.5000 (12.7604) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [65/70] time 0.352 (0.442) data 0.220 (0.311) loss_u loss_u 0.9326 (0.8974) acc_u 6.2500 (12.6442) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [70/70] time 0.477 (0.443) data 0.345 (0.311) loss_u loss_u 0.9355 (0.8977) acc_u 6.2500 (12.5893) lr 7.2101e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1787
confident_label rate tensor(0.2758, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 865
clean true:625
clean false:240
clean_rate:0.7225433526011561
noisy true:724
noisy false:1547
after delete: len(clean_dataset) 865
after delete: len(noisy_dataset) 2271
epoch [121/200] batch [5/27] time 0.376 (0.462) data 0.246 (0.331) loss_x loss_x 0.8813 (1.0619) acc_x 75.0000 (78.7500) lr 7.0596e-04 eta 0:00:10
epoch [121/200] batch [10/27] time 0.381 (0.442) data 0.250 (0.311) loss_x loss_x 1.5117 (1.2255) acc_x 75.0000 (70.9375) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [15/27] time 0.415 (0.452) data 0.285 (0.321) loss_x loss_x 1.2686 (1.1411) acc_x 65.6250 (72.5000) lr 7.0596e-04 eta 0:00:05
epoch [121/200] batch [20/27] time 0.485 (0.450) data 0.354 (0.319) loss_x loss_x 0.9189 (1.1562) acc_x 75.0000 (72.0312) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [25/27] time 0.426 (0.452) data 0.295 (0.321) loss_x loss_x 1.1768 (1.1492) acc_x 65.6250 (71.7500) lr 7.0596e-04 eta 0:00:00
epoch [121/200] batch [5/70] time 0.329 (0.445) data 0.198 (0.314) loss_u loss_u 0.9756 (0.9052) acc_u 6.2500 (11.2500) lr 7.0596e-04 eta 0:00:28
epoch [121/200] batch [10/70] time 0.396 (0.442) data 0.265 (0.311) loss_u loss_u 0.8682 (0.8975) acc_u 15.6250 (11.8750) lr 7.0596e-04 eta 0:00:26
epoch [121/200] batch [15/70] time 0.498 (0.447) data 0.367 (0.316) loss_u loss_u 0.9102 (0.9021) acc_u 9.3750 (11.8750) lr 7.0596e-04 eta 0:00:24
epoch [121/200] batch [20/70] time 0.518 (0.448) data 0.387 (0.317) loss_u loss_u 0.8887 (0.8983) acc_u 12.5000 (12.5000) lr 7.0596e-04 eta 0:00:22
epoch [121/200] batch [25/70] time 0.402 (0.446) data 0.271 (0.315) loss_u loss_u 0.9062 (0.8969) acc_u 9.3750 (12.7500) lr 7.0596e-04 eta 0:00:20
epoch [121/200] batch [30/70] time 0.341 (0.443) data 0.210 (0.312) loss_u loss_u 0.9482 (0.8966) acc_u 6.2500 (12.5000) lr 7.0596e-04 eta 0:00:17
epoch [121/200] batch [35/70] time 0.354 (0.441) data 0.223 (0.310) loss_u loss_u 0.9526 (0.8990) acc_u 9.3750 (12.4107) lr 7.0596e-04 eta 0:00:15
epoch [121/200] batch [40/70] time 0.350 (0.444) data 0.219 (0.313) loss_u loss_u 0.8774 (0.8959) acc_u 18.7500 (12.8125) lr 7.0596e-04 eta 0:00:13
epoch [121/200] batch [45/70] time 0.369 (0.442) data 0.237 (0.311) loss_u loss_u 0.9297 (0.8974) acc_u 6.2500 (12.5000) lr 7.0596e-04 eta 0:00:11
epoch [121/200] batch [50/70] time 0.440 (0.439) data 0.310 (0.308) loss_u loss_u 0.8877 (0.8976) acc_u 12.5000 (12.5000) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [55/70] time 0.416 (0.438) data 0.286 (0.307) loss_u loss_u 0.8516 (0.8972) acc_u 21.8750 (12.5568) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [60/70] time 0.418 (0.439) data 0.286 (0.308) loss_u loss_u 0.8525 (0.8980) acc_u 18.7500 (12.6042) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [65/70] time 0.427 (0.439) data 0.295 (0.308) loss_u loss_u 0.8530 (0.8937) acc_u 18.7500 (13.1250) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [70/70] time 0.493 (0.441) data 0.362 (0.310) loss_u loss_u 0.9019 (0.8930) acc_u 9.3750 (13.1696) lr 7.0596e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1771
confident_label rate tensor(0.2749, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 862
clean true:626
clean false:236
clean_rate:0.7262180974477959
noisy true:739
noisy false:1535
after delete: len(clean_dataset) 862
after delete: len(noisy_dataset) 2274
epoch [122/200] batch [5/26] time 0.476 (0.477) data 0.345 (0.346) loss_x loss_x 0.8730 (1.1842) acc_x 81.2500 (65.6250) lr 6.9098e-04 eta 0:00:10
epoch [122/200] batch [10/26] time 0.466 (0.461) data 0.335 (0.330) loss_x loss_x 1.0049 (1.1030) acc_x 78.1250 (69.0625) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [15/26] time 0.513 (0.478) data 0.382 (0.347) loss_x loss_x 0.8530 (1.0851) acc_x 71.8750 (70.8333) lr 6.9098e-04 eta 0:00:05
epoch [122/200] batch [20/26] time 0.480 (0.478) data 0.348 (0.347) loss_x loss_x 0.5459 (1.0318) acc_x 87.5000 (72.9688) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [25/26] time 0.416 (0.471) data 0.285 (0.340) loss_x loss_x 1.5439 (1.0833) acc_x 65.6250 (72.7500) lr 6.9098e-04 eta 0:00:00
epoch [122/200] batch [5/71] time 0.497 (0.478) data 0.364 (0.346) loss_u loss_u 0.9204 (0.9079) acc_u 6.2500 (10.6250) lr 6.9098e-04 eta 0:00:31
epoch [122/200] batch [10/71] time 0.516 (0.473) data 0.383 (0.342) loss_u loss_u 0.8062 (0.9002) acc_u 21.8750 (11.5625) lr 6.9098e-04 eta 0:00:28
epoch [122/200] batch [15/71] time 0.367 (0.465) data 0.235 (0.333) loss_u loss_u 0.9268 (0.9010) acc_u 6.2500 (11.8750) lr 6.9098e-04 eta 0:00:26
epoch [122/200] batch [20/71] time 0.370 (0.461) data 0.238 (0.330) loss_u loss_u 0.9268 (0.8981) acc_u 12.5000 (12.5000) lr 6.9098e-04 eta 0:00:23
epoch [122/200] batch [25/71] time 0.511 (0.463) data 0.379 (0.331) loss_u loss_u 0.8320 (0.8986) acc_u 21.8750 (12.7500) lr 6.9098e-04 eta 0:00:21
epoch [122/200] batch [30/71] time 0.378 (0.459) data 0.244 (0.328) loss_u loss_u 0.9365 (0.8969) acc_u 15.6250 (12.7083) lr 6.9098e-04 eta 0:00:18
epoch [122/200] batch [35/71] time 0.371 (0.456) data 0.238 (0.325) loss_u loss_u 0.8154 (0.8955) acc_u 21.8750 (12.8571) lr 6.9098e-04 eta 0:00:16
epoch [122/200] batch [40/71] time 0.435 (0.456) data 0.303 (0.324) loss_u loss_u 0.8765 (0.8960) acc_u 21.8750 (13.0469) lr 6.9098e-04 eta 0:00:14
epoch [122/200] batch [45/71] time 0.543 (0.454) data 0.411 (0.322) loss_u loss_u 0.8706 (0.8930) acc_u 15.6250 (13.4722) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [50/71] time 0.502 (0.454) data 0.367 (0.322) loss_u loss_u 0.8652 (0.8894) acc_u 18.7500 (14.0000) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [55/71] time 0.544 (0.455) data 0.411 (0.323) loss_u loss_u 0.8403 (0.8890) acc_u 21.8750 (14.0909) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [60/71] time 0.556 (0.454) data 0.424 (0.322) loss_u loss_u 0.9336 (0.8908) acc_u 6.2500 (13.6458) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [65/71] time 0.411 (0.453) data 0.280 (0.321) loss_u loss_u 0.7935 (0.8909) acc_u 28.1250 (13.6058) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [70/71] time 0.535 (0.453) data 0.404 (0.322) loss_u loss_u 0.9521 (0.8916) acc_u 9.3750 (13.5268) lr 6.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1728
confident_label rate tensor(0.2848, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 893
clean true:662
clean false:231
clean_rate:0.7413213885778276
noisy true:746
noisy false:1497
after delete: len(clean_dataset) 893
after delete: len(noisy_dataset) 2243
epoch [123/200] batch [5/27] time 0.462 (0.487) data 0.331 (0.356) loss_x loss_x 1.1416 (1.1397) acc_x 71.8750 (70.6250) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [10/27] time 0.514 (0.493) data 0.383 (0.363) loss_x loss_x 1.2617 (1.2812) acc_x 78.1250 (69.3750) lr 6.7608e-04 eta 0:00:08
epoch [123/200] batch [15/27] time 0.331 (0.463) data 0.200 (0.332) loss_x loss_x 1.7012 (1.2926) acc_x 53.1250 (67.2917) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [20/27] time 0.437 (0.447) data 0.306 (0.317) loss_x loss_x 1.3682 (1.2878) acc_x 68.7500 (66.7188) lr 6.7608e-04 eta 0:00:03
epoch [123/200] batch [25/27] time 0.384 (0.446) data 0.253 (0.315) loss_x loss_x 1.1406 (1.2441) acc_x 68.7500 (67.0000) lr 6.7608e-04 eta 0:00:00
epoch [123/200] batch [5/70] time 0.409 (0.452) data 0.278 (0.322) loss_u loss_u 0.9053 (0.8780) acc_u 12.5000 (15.0000) lr 6.7608e-04 eta 0:00:29
epoch [123/200] batch [10/70] time 0.414 (0.451) data 0.282 (0.320) loss_u loss_u 0.9038 (0.8910) acc_u 12.5000 (13.7500) lr 6.7608e-04 eta 0:00:27
epoch [123/200] batch [15/70] time 0.390 (0.453) data 0.259 (0.322) loss_u loss_u 0.8613 (0.8815) acc_u 15.6250 (13.7500) lr 6.7608e-04 eta 0:00:24
epoch [123/200] batch [20/70] time 0.524 (0.459) data 0.392 (0.328) loss_u loss_u 0.9663 (0.8892) acc_u 3.1250 (13.1250) lr 6.7608e-04 eta 0:00:22
epoch [123/200] batch [25/70] time 0.390 (0.450) data 0.258 (0.319) loss_u loss_u 0.9648 (0.8927) acc_u 3.1250 (13.1250) lr 6.7608e-04 eta 0:00:20
epoch [123/200] batch [30/70] time 0.351 (0.452) data 0.219 (0.320) loss_u loss_u 0.8936 (0.8867) acc_u 15.6250 (14.1667) lr 6.7608e-04 eta 0:00:18
epoch [123/200] batch [35/70] time 0.551 (0.450) data 0.420 (0.319) loss_u loss_u 0.9146 (0.8899) acc_u 12.5000 (13.7500) lr 6.7608e-04 eta 0:00:15
epoch [123/200] batch [40/70] time 0.444 (0.456) data 0.312 (0.325) loss_u loss_u 0.8604 (0.8915) acc_u 25.0000 (13.6719) lr 6.7608e-04 eta 0:00:13
epoch [123/200] batch [45/70] time 0.405 (0.457) data 0.273 (0.325) loss_u loss_u 0.9175 (0.8926) acc_u 9.3750 (13.3333) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [50/70] time 0.432 (0.454) data 0.297 (0.322) loss_u loss_u 0.8965 (0.8935) acc_u 12.5000 (13.1250) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [55/70] time 0.437 (0.454) data 0.305 (0.322) loss_u loss_u 0.9473 (0.8945) acc_u 6.2500 (13.0682) lr 6.7608e-04 eta 0:00:06
epoch [123/200] batch [60/70] time 0.351 (0.454) data 0.219 (0.322) loss_u loss_u 0.9189 (0.8957) acc_u 6.2500 (12.7604) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [65/70] time 0.447 (0.451) data 0.315 (0.320) loss_u loss_u 0.8892 (0.8963) acc_u 15.6250 (12.7885) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [70/70] time 0.367 (0.451) data 0.235 (0.319) loss_u loss_u 0.8877 (0.8970) acc_u 15.6250 (12.6786) lr 6.7608e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.2819, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 884
clean true:643
clean false:241
clean_rate:0.7273755656108597
noisy true:744
noisy false:1508
after delete: len(clean_dataset) 884
after delete: len(noisy_dataset) 2252
epoch [124/200] batch [5/27] time 0.631 (0.476) data 0.500 (0.346) loss_x loss_x 0.7056 (1.0218) acc_x 75.0000 (71.2500) lr 6.6126e-04 eta 0:00:10
epoch [124/200] batch [10/27] time 0.406 (0.468) data 0.276 (0.337) loss_x loss_x 1.6309 (1.2387) acc_x 65.6250 (65.0000) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [15/27] time 0.774 (0.480) data 0.644 (0.349) loss_x loss_x 1.2393 (1.2060) acc_x 68.7500 (67.7083) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [20/27] time 0.378 (0.466) data 0.248 (0.336) loss_x loss_x 1.1816 (1.2077) acc_x 65.6250 (66.8750) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [25/27] time 0.491 (0.461) data 0.361 (0.330) loss_x loss_x 0.9126 (1.1781) acc_x 81.2500 (68.5000) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/70] time 0.511 (0.459) data 0.380 (0.328) loss_u loss_u 0.9272 (0.9184) acc_u 6.2500 (8.7500) lr 6.6126e-04 eta 0:00:29
epoch [124/200] batch [10/70] time 0.517 (0.461) data 0.386 (0.330) loss_u loss_u 0.9023 (0.9163) acc_u 6.2500 (9.3750) lr 6.6126e-04 eta 0:00:27
epoch [124/200] batch [15/70] time 0.367 (0.454) data 0.236 (0.323) loss_u loss_u 0.8760 (0.9033) acc_u 12.5000 (11.6667) lr 6.6126e-04 eta 0:00:24
epoch [124/200] batch [20/70] time 0.331 (0.450) data 0.200 (0.319) loss_u loss_u 0.8584 (0.9002) acc_u 18.7500 (11.7188) lr 6.6126e-04 eta 0:00:22
epoch [124/200] batch [25/70] time 0.360 (0.447) data 0.228 (0.316) loss_u loss_u 0.9150 (0.8992) acc_u 9.3750 (12.1250) lr 6.6126e-04 eta 0:00:20
epoch [124/200] batch [30/70] time 0.433 (0.445) data 0.300 (0.314) loss_u loss_u 0.8433 (0.8951) acc_u 21.8750 (13.0208) lr 6.6126e-04 eta 0:00:17
epoch [124/200] batch [35/70] time 0.493 (0.451) data 0.360 (0.320) loss_u loss_u 0.8745 (0.8984) acc_u 18.7500 (12.6786) lr 6.6126e-04 eta 0:00:15
epoch [124/200] batch [40/70] time 0.639 (0.451) data 0.506 (0.320) loss_u loss_u 0.8896 (0.8992) acc_u 15.6250 (12.7344) lr 6.6126e-04 eta 0:00:13
epoch [124/200] batch [45/70] time 0.610 (0.455) data 0.477 (0.324) loss_u loss_u 0.9312 (0.9007) acc_u 6.2500 (12.3611) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [50/70] time 0.452 (0.460) data 0.319 (0.329) loss_u loss_u 0.8667 (0.8960) acc_u 15.6250 (12.8125) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [55/70] time 0.381 (0.459) data 0.249 (0.328) loss_u loss_u 0.9380 (0.8956) acc_u 9.3750 (12.8409) lr 6.6126e-04 eta 0:00:06
epoch [124/200] batch [60/70] time 0.637 (0.462) data 0.503 (0.330) loss_u loss_u 0.8398 (0.8954) acc_u 18.7500 (12.8646) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [65/70] time 0.460 (0.465) data 0.329 (0.333) loss_u loss_u 0.8442 (0.8950) acc_u 21.8750 (13.0769) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [70/70] time 0.474 (0.472) data 0.338 (0.340) loss_u loss_u 0.8989 (0.8947) acc_u 9.3750 (13.1696) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1764
confident_label rate tensor(0.2838, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 890
clean true:641
clean false:249
clean_rate:0.7202247191011236
noisy true:731
noisy false:1515
after delete: len(clean_dataset) 890
after delete: len(noisy_dataset) 2246
epoch [125/200] batch [5/27] time 0.505 (0.507) data 0.374 (0.375) loss_x loss_x 1.3174 (1.0415) acc_x 65.6250 (73.7500) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [10/27] time 0.377 (0.466) data 0.246 (0.335) loss_x loss_x 0.9624 (1.1188) acc_x 71.8750 (72.1875) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [15/27] time 0.373 (0.453) data 0.242 (0.322) loss_x loss_x 1.5566 (1.1657) acc_x 75.0000 (72.2917) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [20/27] time 0.371 (0.452) data 0.241 (0.321) loss_x loss_x 1.1211 (1.2102) acc_x 65.6250 (71.5625) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [25/27] time 0.602 (0.452) data 0.473 (0.321) loss_x loss_x 1.8857 (1.2454) acc_x 53.1250 (69.7500) lr 6.4653e-04 eta 0:00:00
epoch [125/200] batch [5/70] time 0.423 (0.449) data 0.292 (0.319) loss_u loss_u 0.9336 (0.9141) acc_u 6.2500 (10.0000) lr 6.4653e-04 eta 0:00:29
epoch [125/200] batch [10/70] time 0.675 (0.455) data 0.543 (0.324) loss_u loss_u 0.9478 (0.9153) acc_u 3.1250 (10.3125) lr 6.4653e-04 eta 0:00:27
epoch [125/200] batch [15/70] time 0.362 (0.454) data 0.227 (0.323) loss_u loss_u 0.9165 (0.9065) acc_u 9.3750 (10.8333) lr 6.4653e-04 eta 0:00:24
epoch [125/200] batch [20/70] time 0.434 (0.453) data 0.304 (0.323) loss_u loss_u 0.9204 (0.9053) acc_u 12.5000 (10.9375) lr 6.4653e-04 eta 0:00:22
epoch [125/200] batch [25/70] time 0.421 (0.453) data 0.290 (0.322) loss_u loss_u 0.8872 (0.9014) acc_u 18.7500 (12.2500) lr 6.4653e-04 eta 0:00:20
epoch [125/200] batch [30/70] time 0.328 (0.451) data 0.196 (0.320) loss_u loss_u 0.8950 (0.9002) acc_u 15.6250 (12.7083) lr 6.4653e-04 eta 0:00:18
epoch [125/200] batch [35/70] time 0.349 (0.447) data 0.218 (0.316) loss_u loss_u 0.9307 (0.8970) acc_u 9.3750 (12.9464) lr 6.4653e-04 eta 0:00:15
epoch [125/200] batch [40/70] time 0.438 (0.447) data 0.307 (0.316) loss_u loss_u 0.9297 (0.8949) acc_u 12.5000 (13.4375) lr 6.4653e-04 eta 0:00:13
epoch [125/200] batch [45/70] time 0.443 (0.447) data 0.311 (0.317) loss_u loss_u 0.8848 (0.8932) acc_u 15.6250 (13.7500) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [50/70] time 0.414 (0.444) data 0.284 (0.314) loss_u loss_u 0.9478 (0.8924) acc_u 6.2500 (13.6875) lr 6.4653e-04 eta 0:00:08
epoch [125/200] batch [55/70] time 0.411 (0.443) data 0.281 (0.312) loss_u loss_u 0.8628 (0.8935) acc_u 18.7500 (13.5227) lr 6.4653e-04 eta 0:00:06
epoch [125/200] batch [60/70] time 0.378 (0.441) data 0.246 (0.310) loss_u loss_u 0.9585 (0.8942) acc_u 6.2500 (13.5417) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [65/70] time 0.366 (0.442) data 0.234 (0.311) loss_u loss_u 0.8335 (0.8936) acc_u 21.8750 (13.5577) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [70/70] time 0.564 (0.442) data 0.434 (0.311) loss_u loss_u 0.8618 (0.8939) acc_u 21.8750 (13.4821) lr 6.4653e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1777
confident_label rate tensor(0.2749, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 862
clean true:631
clean false:231
clean_rate:0.7320185614849188
noisy true:728
noisy false:1546
after delete: len(clean_dataset) 862
after delete: len(noisy_dataset) 2274
epoch [126/200] batch [5/26] time 0.349 (0.427) data 0.217 (0.296) loss_x loss_x 1.2900 (1.2154) acc_x 53.1250 (68.7500) lr 6.3188e-04 eta 0:00:08
epoch [126/200] batch [10/26] time 0.478 (0.459) data 0.348 (0.328) loss_x loss_x 1.2354 (1.2344) acc_x 65.6250 (66.5625) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [15/26] time 0.339 (0.463) data 0.208 (0.332) loss_x loss_x 1.1680 (1.2369) acc_x 71.8750 (67.5000) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [20/26] time 0.488 (0.473) data 0.357 (0.343) loss_x loss_x 0.8228 (1.1950) acc_x 84.3750 (68.7500) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [25/26] time 0.366 (0.471) data 0.235 (0.340) loss_x loss_x 1.1982 (1.1914) acc_x 59.3750 (68.5000) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/71] time 0.500 (0.480) data 0.368 (0.349) loss_u loss_u 0.8291 (0.8640) acc_u 15.6250 (16.2500) lr 6.3188e-04 eta 0:00:31
epoch [126/200] batch [10/71] time 0.459 (0.470) data 0.327 (0.339) loss_u loss_u 0.8564 (0.8767) acc_u 12.5000 (15.3125) lr 6.3188e-04 eta 0:00:28
epoch [126/200] batch [15/71] time 0.584 (0.474) data 0.453 (0.343) loss_u loss_u 0.8691 (0.8896) acc_u 12.5000 (13.3333) lr 6.3188e-04 eta 0:00:26
epoch [126/200] batch [20/71] time 0.515 (0.472) data 0.384 (0.340) loss_u loss_u 0.9390 (0.9009) acc_u 6.2500 (11.8750) lr 6.3188e-04 eta 0:00:24
epoch [126/200] batch [25/71] time 0.385 (0.469) data 0.253 (0.338) loss_u loss_u 0.8770 (0.9004) acc_u 12.5000 (11.8750) lr 6.3188e-04 eta 0:00:21
epoch [126/200] batch [30/71] time 0.451 (0.469) data 0.320 (0.338) loss_u loss_u 0.9067 (0.8982) acc_u 12.5000 (12.2917) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [35/71] time 0.513 (0.467) data 0.382 (0.335) loss_u loss_u 0.8892 (0.8933) acc_u 12.5000 (12.7679) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [40/71] time 0.439 (0.462) data 0.307 (0.330) loss_u loss_u 0.8711 (0.8901) acc_u 18.7500 (13.0469) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [45/71] time 0.519 (0.467) data 0.388 (0.336) loss_u loss_u 0.9067 (0.8896) acc_u 12.5000 (13.1250) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [50/71] time 0.650 (0.466) data 0.520 (0.335) loss_u loss_u 0.9204 (0.8872) acc_u 9.3750 (13.5625) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [55/71] time 0.391 (0.461) data 0.260 (0.329) loss_u loss_u 0.9531 (0.8894) acc_u 3.1250 (13.3523) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [60/71] time 0.439 (0.457) data 0.307 (0.325) loss_u loss_u 0.9033 (0.8898) acc_u 12.5000 (13.3854) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [65/71] time 0.427 (0.453) data 0.296 (0.322) loss_u loss_u 0.8379 (0.8905) acc_u 18.7500 (13.2692) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [70/71] time 0.431 (0.452) data 0.299 (0.320) loss_u loss_u 0.8486 (0.8893) acc_u 18.7500 (13.3482) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1753
confident_label rate tensor(0.2809, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 881
clean true:639
clean false:242
clean_rate:0.7253121452894438
noisy true:744
noisy false:1511
after delete: len(clean_dataset) 881
after delete: len(noisy_dataset) 2255
epoch [127/200] batch [5/27] time 0.451 (0.465) data 0.318 (0.331) loss_x loss_x 1.2109 (1.1795) acc_x 65.6250 (70.0000) lr 6.1732e-04 eta 0:00:10
epoch [127/200] batch [10/27] time 0.433 (0.461) data 0.301 (0.328) loss_x loss_x 1.8135 (1.1851) acc_x 59.3750 (71.8750) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [15/27] time 0.564 (0.464) data 0.432 (0.331) loss_x loss_x 0.9565 (1.1346) acc_x 75.0000 (72.9167) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [20/27] time 0.411 (0.471) data 0.279 (0.338) loss_x loss_x 1.9727 (1.1884) acc_x 53.1250 (71.8750) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [25/27] time 0.469 (0.464) data 0.339 (0.332) loss_x loss_x 0.8125 (1.1671) acc_x 84.3750 (71.7500) lr 6.1732e-04 eta 0:00:00
epoch [127/200] batch [5/70] time 0.517 (0.472) data 0.384 (0.340) loss_u loss_u 0.8887 (0.9051) acc_u 12.5000 (12.5000) lr 6.1732e-04 eta 0:00:30
epoch [127/200] batch [10/70] time 0.436 (0.466) data 0.304 (0.334) loss_u loss_u 0.9531 (0.9086) acc_u 6.2500 (11.2500) lr 6.1732e-04 eta 0:00:27
epoch [127/200] batch [15/70] time 0.396 (0.469) data 0.264 (0.337) loss_u loss_u 0.9292 (0.9075) acc_u 9.3750 (11.8750) lr 6.1732e-04 eta 0:00:25
epoch [127/200] batch [20/70] time 0.509 (0.469) data 0.377 (0.337) loss_u loss_u 0.8979 (0.8975) acc_u 15.6250 (12.9688) lr 6.1732e-04 eta 0:00:23
epoch [127/200] batch [25/70] time 0.352 (0.463) data 0.221 (0.331) loss_u loss_u 0.9053 (0.8982) acc_u 6.2500 (12.5000) lr 6.1732e-04 eta 0:00:20
epoch [127/200] batch [30/70] time 0.376 (0.460) data 0.245 (0.328) loss_u loss_u 0.8301 (0.8926) acc_u 18.7500 (13.4375) lr 6.1732e-04 eta 0:00:18
epoch [127/200] batch [35/70] time 0.446 (0.459) data 0.314 (0.327) loss_u loss_u 0.8882 (0.8912) acc_u 9.3750 (13.5714) lr 6.1732e-04 eta 0:00:16
epoch [127/200] batch [40/70] time 0.751 (0.463) data 0.619 (0.332) loss_u loss_u 0.9067 (0.8916) acc_u 12.5000 (13.5938) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [45/70] time 0.436 (0.461) data 0.305 (0.329) loss_u loss_u 0.9219 (0.8913) acc_u 9.3750 (13.5417) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [50/70] time 0.551 (0.461) data 0.419 (0.329) loss_u loss_u 0.8999 (0.8895) acc_u 9.3750 (13.6875) lr 6.1732e-04 eta 0:00:09
epoch [127/200] batch [55/70] time 0.379 (0.456) data 0.248 (0.325) loss_u loss_u 0.9170 (0.8881) acc_u 12.5000 (14.0341) lr 6.1732e-04 eta 0:00:06
epoch [127/200] batch [60/70] time 0.443 (0.456) data 0.312 (0.324) loss_u loss_u 0.8911 (0.8886) acc_u 12.5000 (13.8021) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [65/70] time 0.315 (0.452) data 0.183 (0.320) loss_u loss_u 0.8413 (0.8888) acc_u 18.7500 (13.7500) lr 6.1732e-04 eta 0:00:02
epoch [127/200] batch [70/70] time 0.426 (0.449) data 0.295 (0.318) loss_u loss_u 0.9336 (0.8879) acc_u 9.3750 (13.8393) lr 6.1732e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1776
confident_label rate tensor(0.2758, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 865
clean true:633
clean false:232
clean_rate:0.7317919075144509
noisy true:727
noisy false:1544
after delete: len(clean_dataset) 865
after delete: len(noisy_dataset) 2271
epoch [128/200] batch [5/27] time 0.477 (0.455) data 0.346 (0.324) loss_x loss_x 1.1152 (1.0308) acc_x 65.6250 (73.1250) lr 6.0285e-04 eta 0:00:10
epoch [128/200] batch [10/27] time 0.495 (0.458) data 0.363 (0.327) loss_x loss_x 1.6582 (1.2109) acc_x 62.5000 (68.7500) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [15/27] time 0.458 (0.475) data 0.328 (0.345) loss_x loss_x 1.1807 (1.1413) acc_x 71.8750 (72.0833) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [20/27] time 0.406 (0.465) data 0.276 (0.335) loss_x loss_x 0.9414 (1.1024) acc_x 71.8750 (72.6562) lr 6.0285e-04 eta 0:00:03
epoch [128/200] batch [25/27] time 0.766 (0.475) data 0.635 (0.345) loss_x loss_x 0.9941 (1.1423) acc_x 68.7500 (71.1250) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/70] time 0.367 (0.469) data 0.236 (0.338) loss_u loss_u 0.9839 (0.9172) acc_u 3.1250 (9.3750) lr 6.0285e-04 eta 0:00:30
epoch [128/200] batch [10/70] time 0.406 (0.462) data 0.276 (0.332) loss_u loss_u 0.9194 (0.9046) acc_u 12.5000 (11.8750) lr 6.0285e-04 eta 0:00:27
epoch [128/200] batch [15/70] time 0.471 (0.454) data 0.340 (0.324) loss_u loss_u 0.9277 (0.9127) acc_u 6.2500 (10.8333) lr 6.0285e-04 eta 0:00:24
epoch [128/200] batch [20/70] time 0.324 (0.452) data 0.193 (0.321) loss_u loss_u 0.9517 (0.9052) acc_u 6.2500 (11.5625) lr 6.0285e-04 eta 0:00:22
epoch [128/200] batch [25/70] time 0.566 (0.450) data 0.434 (0.319) loss_u loss_u 0.8892 (0.8964) acc_u 12.5000 (12.5000) lr 6.0285e-04 eta 0:00:20
epoch [128/200] batch [30/70] time 0.431 (0.450) data 0.300 (0.319) loss_u loss_u 0.8867 (0.8934) acc_u 12.5000 (12.6042) lr 6.0285e-04 eta 0:00:17
epoch [128/200] batch [35/70] time 0.503 (0.450) data 0.371 (0.319) loss_u loss_u 0.8315 (0.8942) acc_u 21.8750 (12.4107) lr 6.0285e-04 eta 0:00:15
epoch [128/200] batch [40/70] time 0.402 (0.449) data 0.271 (0.318) loss_u loss_u 0.9072 (0.8948) acc_u 12.5000 (12.5000) lr 6.0285e-04 eta 0:00:13
epoch [128/200] batch [45/70] time 0.477 (0.451) data 0.345 (0.320) loss_u loss_u 0.9023 (0.8936) acc_u 12.5000 (12.7778) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [50/70] time 0.400 (0.446) data 0.268 (0.315) loss_u loss_u 0.8721 (0.8875) acc_u 12.5000 (13.5625) lr 6.0285e-04 eta 0:00:08
epoch [128/200] batch [55/70] time 0.393 (0.446) data 0.261 (0.315) loss_u loss_u 0.9028 (0.8866) acc_u 12.5000 (13.6932) lr 6.0285e-04 eta 0:00:06
epoch [128/200] batch [60/70] time 0.560 (0.450) data 0.429 (0.319) loss_u loss_u 0.9380 (0.8869) acc_u 6.2500 (13.6458) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [65/70] time 0.675 (0.456) data 0.542 (0.324) loss_u loss_u 0.9214 (0.8883) acc_u 9.3750 (13.6058) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [70/70] time 0.495 (0.460) data 0.363 (0.329) loss_u loss_u 0.8921 (0.8873) acc_u 12.5000 (13.8393) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1752
confident_label rate tensor(0.2806, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 880
clean true:646
clean false:234
clean_rate:0.7340909090909091
noisy true:738
noisy false:1518
after delete: len(clean_dataset) 880
after delete: len(noisy_dataset) 2256
epoch [129/200] batch [5/27] time 0.722 (0.496) data 0.592 (0.365) loss_x loss_x 1.3008 (1.1451) acc_x 71.8750 (68.7500) lr 5.8849e-04 eta 0:00:10
epoch [129/200] batch [10/27] time 0.345 (0.472) data 0.214 (0.341) loss_x loss_x 0.9438 (1.1456) acc_x 78.1250 (69.6875) lr 5.8849e-04 eta 0:00:08
epoch [129/200] batch [15/27] time 0.386 (0.460) data 0.256 (0.329) loss_x loss_x 1.2207 (1.1815) acc_x 68.7500 (68.5417) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [20/27] time 0.446 (0.470) data 0.315 (0.339) loss_x loss_x 1.1631 (1.2148) acc_x 68.7500 (67.6562) lr 5.8849e-04 eta 0:00:03
epoch [129/200] batch [25/27] time 0.452 (0.469) data 0.321 (0.339) loss_x loss_x 1.4082 (1.2138) acc_x 71.8750 (68.1250) lr 5.8849e-04 eta 0:00:00
epoch [129/200] batch [5/70] time 0.402 (0.469) data 0.270 (0.338) loss_u loss_u 0.8242 (0.8364) acc_u 18.7500 (20.0000) lr 5.8849e-04 eta 0:00:30
epoch [129/200] batch [10/70] time 0.434 (0.461) data 0.302 (0.330) loss_u loss_u 0.8804 (0.8486) acc_u 15.6250 (18.1250) lr 5.8849e-04 eta 0:00:27
epoch [129/200] batch [15/70] time 0.490 (0.457) data 0.359 (0.326) loss_u loss_u 0.9268 (0.8590) acc_u 9.3750 (17.5000) lr 5.8849e-04 eta 0:00:25
epoch [129/200] batch [20/70] time 0.411 (0.455) data 0.280 (0.324) loss_u loss_u 0.9229 (0.8676) acc_u 15.6250 (17.0312) lr 5.8849e-04 eta 0:00:22
epoch [129/200] batch [25/70] time 0.419 (0.452) data 0.288 (0.321) loss_u loss_u 0.8994 (0.8775) acc_u 15.6250 (15.8750) lr 5.8849e-04 eta 0:00:20
epoch [129/200] batch [30/70] time 0.390 (0.448) data 0.259 (0.316) loss_u loss_u 0.9092 (0.8833) acc_u 6.2500 (14.8958) lr 5.8849e-04 eta 0:00:17
epoch [129/200] batch [35/70] time 0.428 (0.450) data 0.297 (0.319) loss_u loss_u 0.9253 (0.8855) acc_u 6.2500 (14.4643) lr 5.8849e-04 eta 0:00:15
epoch [129/200] batch [40/70] time 0.411 (0.447) data 0.279 (0.316) loss_u loss_u 0.8477 (0.8863) acc_u 25.0000 (14.3750) lr 5.8849e-04 eta 0:00:13
epoch [129/200] batch [45/70] time 0.352 (0.446) data 0.220 (0.315) loss_u loss_u 0.8398 (0.8831) acc_u 18.7500 (14.5139) lr 5.8849e-04 eta 0:00:11
epoch [129/200] batch [50/70] time 0.433 (0.444) data 0.301 (0.313) loss_u loss_u 0.9062 (0.8848) acc_u 12.5000 (14.3750) lr 5.8849e-04 eta 0:00:08
epoch [129/200] batch [55/70] time 0.415 (0.443) data 0.283 (0.312) loss_u loss_u 0.8765 (0.8868) acc_u 18.7500 (14.0909) lr 5.8849e-04 eta 0:00:06
epoch [129/200] batch [60/70] time 0.433 (0.443) data 0.302 (0.311) loss_u loss_u 0.8887 (0.8871) acc_u 12.5000 (14.1146) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [65/70] time 0.584 (0.445) data 0.452 (0.314) loss_u loss_u 0.9551 (0.8891) acc_u 3.1250 (13.8942) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [70/70] time 0.463 (0.446) data 0.331 (0.314) loss_u loss_u 0.9351 (0.8893) acc_u 3.1250 (13.7500) lr 5.8849e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1748
confident_label rate tensor(0.2899, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 909
clean true:659
clean false:250
clean_rate:0.724972497249725
noisy true:729
noisy false:1498
after delete: len(clean_dataset) 909
after delete: len(noisy_dataset) 2227
epoch [130/200] batch [5/28] time 0.377 (0.434) data 0.246 (0.304) loss_x loss_x 1.2324 (1.2341) acc_x 68.7500 (68.7500) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [10/28] time 0.453 (0.433) data 0.322 (0.302) loss_x loss_x 0.8408 (1.1221) acc_x 78.1250 (70.3125) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [15/28] time 0.564 (0.453) data 0.433 (0.322) loss_x loss_x 1.0127 (1.1021) acc_x 68.7500 (69.7917) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [20/28] time 0.438 (0.471) data 0.308 (0.340) loss_x loss_x 1.2373 (1.1023) acc_x 65.6250 (70.4688) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [25/28] time 0.461 (0.467) data 0.331 (0.337) loss_x loss_x 0.9707 (1.1374) acc_x 81.2500 (69.7500) lr 5.7422e-04 eta 0:00:01
epoch [130/200] batch [5/69] time 0.406 (0.465) data 0.274 (0.335) loss_u loss_u 0.8887 (0.9134) acc_u 12.5000 (10.0000) lr 5.7422e-04 eta 0:00:29
epoch [130/200] batch [10/69] time 0.468 (0.461) data 0.337 (0.330) loss_u loss_u 0.9297 (0.9091) acc_u 6.2500 (10.3125) lr 5.7422e-04 eta 0:00:27
epoch [130/200] batch [15/69] time 0.453 (0.463) data 0.322 (0.332) loss_u loss_u 0.9575 (0.9122) acc_u 3.1250 (10.4167) lr 5.7422e-04 eta 0:00:25
epoch [130/200] batch [20/69] time 0.392 (0.458) data 0.259 (0.327) loss_u loss_u 0.9253 (0.9049) acc_u 6.2500 (11.2500) lr 5.7422e-04 eta 0:00:22
epoch [130/200] batch [25/69] time 0.531 (0.455) data 0.400 (0.324) loss_u loss_u 0.9033 (0.8965) acc_u 15.6250 (12.6250) lr 5.7422e-04 eta 0:00:20
epoch [130/200] batch [30/69] time 0.404 (0.454) data 0.273 (0.323) loss_u loss_u 0.8687 (0.8927) acc_u 15.6250 (13.3333) lr 5.7422e-04 eta 0:00:17
epoch [130/200] batch [35/69] time 0.494 (0.453) data 0.362 (0.322) loss_u loss_u 0.9463 (0.8960) acc_u 3.1250 (12.9464) lr 5.7422e-04 eta 0:00:15
epoch [130/200] batch [40/69] time 0.340 (0.451) data 0.209 (0.319) loss_u loss_u 0.9512 (0.8959) acc_u 6.2500 (12.9688) lr 5.7422e-04 eta 0:00:13
epoch [130/200] batch [45/69] time 0.427 (0.448) data 0.295 (0.317) loss_u loss_u 0.9438 (0.8984) acc_u 12.5000 (12.7778) lr 5.7422e-04 eta 0:00:10
epoch [130/200] batch [50/69] time 0.437 (0.449) data 0.306 (0.318) loss_u loss_u 0.9370 (0.8955) acc_u 6.2500 (13.1875) lr 5.7422e-04 eta 0:00:08
epoch [130/200] batch [55/69] time 0.529 (0.449) data 0.397 (0.318) loss_u loss_u 0.8486 (0.8940) acc_u 15.6250 (13.2955) lr 5.7422e-04 eta 0:00:06
epoch [130/200] batch [60/69] time 0.373 (0.447) data 0.241 (0.316) loss_u loss_u 0.9419 (0.8950) acc_u 6.2500 (13.0729) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [65/69] time 0.411 (0.446) data 0.280 (0.315) loss_u loss_u 0.9111 (0.8951) acc_u 12.5000 (12.9808) lr 5.7422e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1763
confident_label rate tensor(0.2765, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 867
clean true:635
clean false:232
clean_rate:0.7324106113033448
noisy true:738
noisy false:1531
after delete: len(clean_dataset) 867
after delete: len(noisy_dataset) 2269
epoch [131/200] batch [5/27] time 0.554 (0.460) data 0.424 (0.329) loss_x loss_x 0.9634 (0.9558) acc_x 84.3750 (76.2500) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [10/27] time 0.376 (0.439) data 0.246 (0.309) loss_x loss_x 0.9336 (1.1364) acc_x 71.8750 (73.4375) lr 5.6006e-04 eta 0:00:07
epoch [131/200] batch [15/27] time 0.386 (0.440) data 0.255 (0.309) loss_x loss_x 1.3467 (1.0956) acc_x 62.5000 (73.3333) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [20/27] time 0.423 (0.437) data 0.292 (0.306) loss_x loss_x 0.8071 (1.1151) acc_x 78.1250 (71.5625) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [25/27] time 0.404 (0.446) data 0.274 (0.315) loss_x loss_x 0.7593 (1.1363) acc_x 81.2500 (70.5000) lr 5.6006e-04 eta 0:00:00
epoch [131/200] batch [5/70] time 0.412 (0.447) data 0.280 (0.316) loss_u loss_u 0.9067 (0.8913) acc_u 9.3750 (12.5000) lr 5.6006e-04 eta 0:00:29
epoch [131/200] batch [10/70] time 0.460 (0.449) data 0.329 (0.318) loss_u loss_u 0.8994 (0.8750) acc_u 15.6250 (15.3125) lr 5.6006e-04 eta 0:00:26
epoch [131/200] batch [15/70] time 0.373 (0.446) data 0.243 (0.315) loss_u loss_u 0.9688 (0.8869) acc_u 0.0000 (13.7500) lr 5.6006e-04 eta 0:00:24
epoch [131/200] batch [20/70] time 0.466 (0.447) data 0.334 (0.316) loss_u loss_u 0.8179 (0.8827) acc_u 21.8750 (14.0625) lr 5.6006e-04 eta 0:00:22
epoch [131/200] batch [25/70] time 0.463 (0.448) data 0.331 (0.316) loss_u loss_u 0.8911 (0.8845) acc_u 21.8750 (14.0000) lr 5.6006e-04 eta 0:00:20
epoch [131/200] batch [30/70] time 0.417 (0.455) data 0.285 (0.324) loss_u loss_u 0.8740 (0.8899) acc_u 12.5000 (13.0208) lr 5.6006e-04 eta 0:00:18
epoch [131/200] batch [35/70] time 0.508 (0.456) data 0.375 (0.325) loss_u loss_u 0.8965 (0.8907) acc_u 15.6250 (13.1250) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [40/70] time 0.476 (0.455) data 0.343 (0.323) loss_u loss_u 0.9058 (0.8912) acc_u 12.5000 (12.9688) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [45/70] time 0.358 (0.453) data 0.226 (0.322) loss_u loss_u 0.8779 (0.8911) acc_u 18.7500 (12.9167) lr 5.6006e-04 eta 0:00:11
epoch [131/200] batch [50/70] time 0.494 (0.456) data 0.362 (0.325) loss_u loss_u 0.8838 (0.8911) acc_u 18.7500 (13.0625) lr 5.6006e-04 eta 0:00:09
epoch [131/200] batch [55/70] time 0.469 (0.456) data 0.339 (0.325) loss_u loss_u 0.8701 (0.8924) acc_u 15.6250 (12.8977) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [60/70] time 0.401 (0.456) data 0.269 (0.324) loss_u loss_u 0.8862 (0.8917) acc_u 15.6250 (13.1250) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [65/70] time 0.464 (0.454) data 0.332 (0.323) loss_u loss_u 0.9473 (0.8936) acc_u 6.2500 (12.9327) lr 5.6006e-04 eta 0:00:02
epoch [131/200] batch [70/70] time 0.374 (0.454) data 0.243 (0.323) loss_u loss_u 0.8818 (0.8932) acc_u 15.6250 (13.0804) lr 5.6006e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1786
confident_label rate tensor(0.2809, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 881
clean true:637
clean false:244
clean_rate:0.7230419977298524
noisy true:713
noisy false:1542
after delete: len(clean_dataset) 881
after delete: len(noisy_dataset) 2255
epoch [132/200] batch [5/27] time 0.352 (0.455) data 0.222 (0.324) loss_x loss_x 1.0830 (0.9920) acc_x 62.5000 (71.8750) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [10/27] time 0.477 (0.444) data 0.347 (0.313) loss_x loss_x 1.0303 (1.1624) acc_x 81.2500 (69.3750) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [15/27] time 0.586 (0.453) data 0.457 (0.322) loss_x loss_x 1.1953 (1.1445) acc_x 68.7500 (69.3750) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [20/27] time 0.522 (0.494) data 0.390 (0.363) loss_x loss_x 0.9624 (1.1629) acc_x 78.1250 (68.7500) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [25/27] time 0.438 (0.486) data 0.307 (0.355) loss_x loss_x 1.0752 (1.1603) acc_x 81.2500 (69.2500) lr 5.4601e-04 eta 0:00:00
epoch [132/200] batch [5/70] time 0.416 (0.476) data 0.283 (0.345) loss_u loss_u 0.9204 (0.8877) acc_u 9.3750 (12.5000) lr 5.4601e-04 eta 0:00:30
epoch [132/200] batch [10/70] time 0.423 (0.473) data 0.291 (0.342) loss_u loss_u 0.8511 (0.8762) acc_u 18.7500 (14.6875) lr 5.4601e-04 eta 0:00:28
epoch [132/200] batch [15/70] time 0.358 (0.464) data 0.227 (0.333) loss_u loss_u 0.8745 (0.8715) acc_u 15.6250 (15.6250) lr 5.4601e-04 eta 0:00:25
epoch [132/200] batch [20/70] time 0.369 (0.457) data 0.238 (0.325) loss_u loss_u 0.8467 (0.8724) acc_u 15.6250 (15.3125) lr 5.4601e-04 eta 0:00:22
epoch [132/200] batch [25/70] time 0.394 (0.454) data 0.262 (0.323) loss_u loss_u 0.8628 (0.8746) acc_u 18.7500 (15.5000) lr 5.4601e-04 eta 0:00:20
epoch [132/200] batch [30/70] time 0.603 (0.463) data 0.472 (0.331) loss_u loss_u 0.9111 (0.8723) acc_u 12.5000 (16.0417) lr 5.4601e-04 eta 0:00:18
epoch [132/200] batch [35/70] time 0.556 (0.461) data 0.425 (0.330) loss_u loss_u 0.9199 (0.8768) acc_u 9.3750 (15.6250) lr 5.4601e-04 eta 0:00:16
epoch [132/200] batch [40/70] time 0.413 (0.457) data 0.282 (0.326) loss_u loss_u 0.9438 (0.8792) acc_u 3.1250 (15.0781) lr 5.4601e-04 eta 0:00:13
epoch [132/200] batch [45/70] time 0.439 (0.458) data 0.307 (0.327) loss_u loss_u 0.9321 (0.8823) acc_u 9.3750 (14.5833) lr 5.4601e-04 eta 0:00:11
epoch [132/200] batch [50/70] time 0.456 (0.461) data 0.325 (0.330) loss_u loss_u 0.9355 (0.8873) acc_u 9.3750 (14.1875) lr 5.4601e-04 eta 0:00:09
epoch [132/200] batch [55/70] time 0.381 (0.459) data 0.249 (0.327) loss_u loss_u 0.8481 (0.8899) acc_u 12.5000 (13.6364) lr 5.4601e-04 eta 0:00:06
epoch [132/200] batch [60/70] time 0.559 (0.460) data 0.427 (0.328) loss_u loss_u 0.9102 (0.8905) acc_u 12.5000 (13.5417) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [65/70] time 0.429 (0.460) data 0.299 (0.329) loss_u loss_u 0.8940 (0.8892) acc_u 15.6250 (13.6058) lr 5.4601e-04 eta 0:00:02
epoch [132/200] batch [70/70] time 0.416 (0.459) data 0.286 (0.328) loss_u loss_u 0.8892 (0.8890) acc_u 18.7500 (13.7054) lr 5.4601e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1755
confident_label rate tensor(0.2905, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 911
clean true:674
clean false:237
clean_rate:0.7398463227222832
noisy true:707
noisy false:1518
after delete: len(clean_dataset) 911
after delete: len(noisy_dataset) 2225
epoch [133/200] batch [5/28] time 0.463 (0.487) data 0.333 (0.356) loss_x loss_x 0.6743 (1.1704) acc_x 87.5000 (71.2500) lr 5.3207e-04 eta 0:00:11
epoch [133/200] batch [10/28] time 0.443 (0.493) data 0.312 (0.363) loss_x loss_x 1.6934 (1.1086) acc_x 62.5000 (74.0625) lr 5.3207e-04 eta 0:00:08
epoch [133/200] batch [15/28] time 0.460 (0.482) data 0.329 (0.351) loss_x loss_x 1.3457 (1.1668) acc_x 65.6250 (71.4583) lr 5.3207e-04 eta 0:00:06
epoch [133/200] batch [20/28] time 0.426 (0.472) data 0.295 (0.341) loss_x loss_x 0.6885 (1.2146) acc_x 78.1250 (70.4688) lr 5.3207e-04 eta 0:00:03
epoch [133/200] batch [25/28] time 0.514 (0.464) data 0.383 (0.334) loss_x loss_x 1.1660 (1.1832) acc_x 62.5000 (70.2500) lr 5.3207e-04 eta 0:00:01
epoch [133/200] batch [5/69] time 0.358 (0.453) data 0.228 (0.322) loss_u loss_u 0.8696 (0.8994) acc_u 15.6250 (10.6250) lr 5.3207e-04 eta 0:00:28
epoch [133/200] batch [10/69] time 0.492 (0.446) data 0.361 (0.315) loss_u loss_u 0.8506 (0.8918) acc_u 21.8750 (13.1250) lr 5.3207e-04 eta 0:00:26
epoch [133/200] batch [15/69] time 0.415 (0.450) data 0.284 (0.319) loss_u loss_u 0.8599 (0.8817) acc_u 12.5000 (13.7500) lr 5.3207e-04 eta 0:00:24
epoch [133/200] batch [20/69] time 0.365 (0.446) data 0.234 (0.315) loss_u loss_u 0.9170 (0.8885) acc_u 9.3750 (13.4375) lr 5.3207e-04 eta 0:00:21
epoch [133/200] batch [25/69] time 0.415 (0.445) data 0.284 (0.314) loss_u loss_u 0.9141 (0.8875) acc_u 9.3750 (13.6250) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [30/69] time 0.576 (0.451) data 0.443 (0.320) loss_u loss_u 0.8960 (0.8896) acc_u 12.5000 (13.2292) lr 5.3207e-04 eta 0:00:17
epoch [133/200] batch [35/69] time 0.506 (0.452) data 0.373 (0.321) loss_u loss_u 0.9258 (0.8933) acc_u 6.2500 (12.8571) lr 5.3207e-04 eta 0:00:15
epoch [133/200] batch [40/69] time 0.408 (0.451) data 0.276 (0.320) loss_u loss_u 0.8408 (0.8938) acc_u 25.0000 (12.9688) lr 5.3207e-04 eta 0:00:13
epoch [133/200] batch [45/69] time 0.436 (0.452) data 0.304 (0.320) loss_u loss_u 0.9038 (0.8928) acc_u 12.5000 (13.1250) lr 5.3207e-04 eta 0:00:10
epoch [133/200] batch [50/69] time 0.416 (0.452) data 0.284 (0.321) loss_u loss_u 0.9053 (0.8953) acc_u 12.5000 (12.8125) lr 5.3207e-04 eta 0:00:08
epoch [133/200] batch [55/69] time 0.431 (0.454) data 0.299 (0.322) loss_u loss_u 0.9648 (0.8981) acc_u 6.2500 (12.4432) lr 5.3207e-04 eta 0:00:06
epoch [133/200] batch [60/69] time 0.444 (0.449) data 0.312 (0.318) loss_u loss_u 0.9189 (0.8982) acc_u 9.3750 (12.3958) lr 5.3207e-04 eta 0:00:04
epoch [133/200] batch [65/69] time 0.502 (0.449) data 0.370 (0.317) loss_u loss_u 0.8701 (0.8995) acc_u 12.5000 (12.2115) lr 5.3207e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1761
confident_label rate tensor(0.2790, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 875
clean true:653
clean false:222
clean_rate:0.7462857142857143
noisy true:722
noisy false:1539
after delete: len(clean_dataset) 875
after delete: len(noisy_dataset) 2261
epoch [134/200] batch [5/27] time 0.417 (0.440) data 0.287 (0.309) loss_x loss_x 1.2510 (1.2935) acc_x 71.8750 (66.2500) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [10/27] time 0.401 (0.450) data 0.270 (0.319) loss_x loss_x 1.0010 (1.2641) acc_x 71.8750 (67.1875) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [15/27] time 0.512 (0.455) data 0.381 (0.324) loss_x loss_x 1.4736 (1.2722) acc_x 53.1250 (67.2917) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [20/27] time 0.646 (0.454) data 0.515 (0.324) loss_x loss_x 1.0166 (1.2812) acc_x 71.8750 (67.1875) lr 5.1825e-04 eta 0:00:03
epoch [134/200] batch [25/27] time 0.442 (0.455) data 0.312 (0.324) loss_x loss_x 1.6797 (1.2998) acc_x 56.2500 (67.0000) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/70] time 0.449 (0.454) data 0.318 (0.323) loss_u loss_u 0.9087 (0.9032) acc_u 12.5000 (12.5000) lr 5.1825e-04 eta 0:00:29
epoch [134/200] batch [10/70] time 0.446 (0.452) data 0.314 (0.321) loss_u loss_u 0.9385 (0.8874) acc_u 6.2500 (14.6875) lr 5.1825e-04 eta 0:00:27
epoch [134/200] batch [15/70] time 0.401 (0.449) data 0.269 (0.318) loss_u loss_u 0.9609 (0.8954) acc_u 3.1250 (12.7083) lr 5.1825e-04 eta 0:00:24
epoch [134/200] batch [20/70] time 0.468 (0.448) data 0.337 (0.317) loss_u loss_u 0.8853 (0.8919) acc_u 12.5000 (13.1250) lr 5.1825e-04 eta 0:00:22
epoch [134/200] batch [25/70] time 0.623 (0.449) data 0.490 (0.318) loss_u loss_u 0.8789 (0.8891) acc_u 12.5000 (13.3750) lr 5.1825e-04 eta 0:00:20
epoch [134/200] batch [30/70] time 0.414 (0.449) data 0.283 (0.318) loss_u loss_u 0.9121 (0.8909) acc_u 9.3750 (13.4375) lr 5.1825e-04 eta 0:00:17
epoch [134/200] batch [35/70] time 0.344 (0.447) data 0.213 (0.316) loss_u loss_u 0.9067 (0.8865) acc_u 12.5000 (13.9286) lr 5.1825e-04 eta 0:00:15
epoch [134/200] batch [40/70] time 0.419 (0.445) data 0.286 (0.314) loss_u loss_u 0.8813 (0.8840) acc_u 12.5000 (14.1406) lr 5.1825e-04 eta 0:00:13
epoch [134/200] batch [45/70] time 0.382 (0.444) data 0.251 (0.313) loss_u loss_u 0.9004 (0.8859) acc_u 15.6250 (13.8889) lr 5.1825e-04 eta 0:00:11
epoch [134/200] batch [50/70] time 0.480 (0.442) data 0.348 (0.311) loss_u loss_u 0.9365 (0.8874) acc_u 9.3750 (13.8750) lr 5.1825e-04 eta 0:00:08
epoch [134/200] batch [55/70] time 0.438 (0.445) data 0.307 (0.314) loss_u loss_u 0.9038 (0.8877) acc_u 12.5000 (13.9773) lr 5.1825e-04 eta 0:00:06
epoch [134/200] batch [60/70] time 0.322 (0.443) data 0.190 (0.312) loss_u loss_u 0.9653 (0.8882) acc_u 0.0000 (13.7500) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [65/70] time 0.483 (0.443) data 0.351 (0.312) loss_u loss_u 0.9155 (0.8882) acc_u 6.2500 (13.6538) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [70/70] time 0.477 (0.444) data 0.346 (0.313) loss_u loss_u 0.9253 (0.8883) acc_u 15.6250 (13.6161) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1752
confident_label rate tensor(0.2825, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 886
clean true:638
clean false:248
clean_rate:0.7200902934537246
noisy true:746
noisy false:1504
after delete: len(clean_dataset) 886
after delete: len(noisy_dataset) 2250
epoch [135/200] batch [5/27] time 0.456 (0.457) data 0.326 (0.326) loss_x loss_x 0.8647 (1.2058) acc_x 78.1250 (68.1250) lr 5.0454e-04 eta 0:00:10
epoch [135/200] batch [10/27] time 0.390 (0.464) data 0.259 (0.333) loss_x loss_x 0.9019 (1.1719) acc_x 75.0000 (69.6875) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [15/27] time 0.406 (0.452) data 0.276 (0.321) loss_x loss_x 1.4668 (1.1443) acc_x 62.5000 (69.5833) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [20/27] time 0.555 (0.450) data 0.425 (0.319) loss_x loss_x 0.8350 (1.1636) acc_x 81.2500 (69.5312) lr 5.0454e-04 eta 0:00:03
epoch [135/200] batch [25/27] time 0.381 (0.448) data 0.250 (0.317) loss_x loss_x 1.6729 (1.2044) acc_x 65.6250 (68.8750) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/70] time 0.379 (0.455) data 0.248 (0.324) loss_u loss_u 0.8032 (0.8675) acc_u 18.7500 (12.5000) lr 5.0454e-04 eta 0:00:29
epoch [135/200] batch [10/70] time 0.429 (0.444) data 0.299 (0.313) loss_u loss_u 0.7974 (0.8820) acc_u 21.8750 (12.8125) lr 5.0454e-04 eta 0:00:26
epoch [135/200] batch [15/70] time 0.420 (0.438) data 0.290 (0.307) loss_u loss_u 0.8130 (0.8787) acc_u 25.0000 (14.1667) lr 5.0454e-04 eta 0:00:24
epoch [135/200] batch [20/70] time 0.607 (0.440) data 0.476 (0.309) loss_u loss_u 0.9263 (0.8822) acc_u 6.2500 (13.9062) lr 5.0454e-04 eta 0:00:21
epoch [135/200] batch [25/70] time 0.480 (0.442) data 0.350 (0.311) loss_u loss_u 0.8853 (0.8838) acc_u 15.6250 (14.1250) lr 5.0454e-04 eta 0:00:19
epoch [135/200] batch [30/70] time 0.405 (0.442) data 0.274 (0.311) loss_u loss_u 0.8491 (0.8842) acc_u 18.7500 (13.7500) lr 5.0454e-04 eta 0:00:17
epoch [135/200] batch [35/70] time 0.487 (0.444) data 0.354 (0.313) loss_u loss_u 0.8940 (0.8829) acc_u 12.5000 (14.1071) lr 5.0454e-04 eta 0:00:15
epoch [135/200] batch [40/70] time 0.408 (0.446) data 0.277 (0.315) loss_u loss_u 0.9058 (0.8849) acc_u 12.5000 (13.9844) lr 5.0454e-04 eta 0:00:13
epoch [135/200] batch [45/70] time 0.639 (0.453) data 0.508 (0.322) loss_u loss_u 0.9399 (0.8875) acc_u 6.2500 (13.8194) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [50/70] time 0.589 (0.455) data 0.457 (0.324) loss_u loss_u 0.8418 (0.8903) acc_u 15.6250 (13.3750) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [55/70] time 0.533 (0.454) data 0.401 (0.323) loss_u loss_u 0.9346 (0.8895) acc_u 3.1250 (13.5227) lr 5.0454e-04 eta 0:00:06
epoch [135/200] batch [60/70] time 0.486 (0.454) data 0.354 (0.323) loss_u loss_u 0.9097 (0.8895) acc_u 12.5000 (13.4896) lr 5.0454e-04 eta 0:00:04
epoch [135/200] batch [65/70] time 0.415 (0.454) data 0.284 (0.323) loss_u loss_u 0.8013 (0.8890) acc_u 25.0000 (13.6058) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [70/70] time 0.357 (0.451) data 0.225 (0.320) loss_u loss_u 0.9253 (0.8904) acc_u 12.5000 (13.5268) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1741
confident_label rate tensor(0.2841, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 891
clean true:658
clean false:233
clean_rate:0.7384960718294051
noisy true:737
noisy false:1508
after delete: len(clean_dataset) 891
after delete: len(noisy_dataset) 2245
epoch [136/200] batch [5/27] time 0.436 (0.465) data 0.305 (0.334) loss_x loss_x 1.2314 (1.2100) acc_x 71.8750 (73.7500) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [10/27] time 0.450 (0.439) data 0.320 (0.308) loss_x loss_x 1.2666 (1.1898) acc_x 65.6250 (72.1875) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [15/27] time 0.432 (0.467) data 0.300 (0.336) loss_x loss_x 1.0186 (1.1166) acc_x 68.7500 (71.8750) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [20/27] time 0.373 (0.464) data 0.244 (0.333) loss_x loss_x 1.8105 (1.1386) acc_x 56.2500 (71.7188) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [25/27] time 0.475 (0.459) data 0.344 (0.328) loss_x loss_x 1.3926 (1.1582) acc_x 62.5000 (71.5000) lr 4.9096e-04 eta 0:00:00
epoch [136/200] batch [5/70] time 0.438 (0.448) data 0.306 (0.317) loss_u loss_u 0.9150 (0.9151) acc_u 12.5000 (10.6250) lr 4.9096e-04 eta 0:00:29
epoch [136/200] batch [10/70] time 0.454 (0.446) data 0.322 (0.315) loss_u loss_u 0.9009 (0.9177) acc_u 12.5000 (10.0000) lr 4.9096e-04 eta 0:00:26
epoch [136/200] batch [15/70] time 0.434 (0.445) data 0.301 (0.314) loss_u loss_u 0.9409 (0.9150) acc_u 6.2500 (10.4167) lr 4.9096e-04 eta 0:00:24
epoch [136/200] batch [20/70] time 0.536 (0.447) data 0.402 (0.316) loss_u loss_u 0.9019 (0.9131) acc_u 12.5000 (10.7812) lr 4.9096e-04 eta 0:00:22
epoch [136/200] batch [25/70] time 0.410 (0.450) data 0.277 (0.319) loss_u loss_u 0.9102 (0.9116) acc_u 9.3750 (11.1250) lr 4.9096e-04 eta 0:00:20
epoch [136/200] batch [30/70] time 0.387 (0.448) data 0.256 (0.317) loss_u loss_u 0.9385 (0.9105) acc_u 3.1250 (11.4583) lr 4.9096e-04 eta 0:00:17
epoch [136/200] batch [35/70] time 0.466 (0.451) data 0.335 (0.319) loss_u loss_u 0.8950 (0.9071) acc_u 9.3750 (11.5179) lr 4.9096e-04 eta 0:00:15
epoch [136/200] batch [40/70] time 0.563 (0.455) data 0.432 (0.323) loss_u loss_u 0.8721 (0.9003) acc_u 15.6250 (12.5000) lr 4.9096e-04 eta 0:00:13
epoch [136/200] batch [45/70] time 0.463 (0.456) data 0.330 (0.325) loss_u loss_u 0.9307 (0.9000) acc_u 6.2500 (12.5000) lr 4.9096e-04 eta 0:00:11
epoch [136/200] batch [50/70] time 0.370 (0.456) data 0.238 (0.325) loss_u loss_u 0.8848 (0.9002) acc_u 12.5000 (12.3750) lr 4.9096e-04 eta 0:00:09
epoch [136/200] batch [55/70] time 0.455 (0.456) data 0.322 (0.325) loss_u loss_u 0.8003 (0.8971) acc_u 18.7500 (12.5568) lr 4.9096e-04 eta 0:00:06
epoch [136/200] batch [60/70] time 0.608 (0.459) data 0.475 (0.328) loss_u loss_u 0.8955 (0.8980) acc_u 12.5000 (12.6042) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [65/70] time 0.408 (0.461) data 0.276 (0.329) loss_u loss_u 0.8618 (0.8964) acc_u 15.6250 (12.8365) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [70/70] time 0.385 (0.462) data 0.252 (0.331) loss_u loss_u 0.9072 (0.8944) acc_u 15.6250 (13.1250) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1769
confident_label rate tensor(0.2800, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 878
clean true:636
clean false:242
clean_rate:0.724373576309795
noisy true:731
noisy false:1527
after delete: len(clean_dataset) 878
after delete: len(noisy_dataset) 2258
epoch [137/200] batch [5/27] time 0.407 (0.485) data 0.277 (0.354) loss_x loss_x 1.1768 (1.1747) acc_x 65.6250 (71.8750) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [10/27] time 0.538 (0.476) data 0.407 (0.344) loss_x loss_x 1.0693 (1.1681) acc_x 71.8750 (74.6875) lr 4.7750e-04 eta 0:00:08
epoch [137/200] batch [15/27] time 0.622 (0.502) data 0.492 (0.371) loss_x loss_x 0.9370 (1.1298) acc_x 75.0000 (73.7500) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [20/27] time 0.404 (0.482) data 0.273 (0.351) loss_x loss_x 1.3262 (1.1574) acc_x 68.7500 (73.7500) lr 4.7750e-04 eta 0:00:03
epoch [137/200] batch [25/27] time 0.395 (0.461) data 0.263 (0.330) loss_x loss_x 1.3965 (1.2061) acc_x 71.8750 (71.3750) lr 4.7750e-04 eta 0:00:00
epoch [137/200] batch [5/70] time 0.413 (0.468) data 0.281 (0.337) loss_u loss_u 0.9360 (0.8860) acc_u 6.2500 (13.1250) lr 4.7750e-04 eta 0:00:30
epoch [137/200] batch [10/70] time 0.370 (0.458) data 0.239 (0.327) loss_u loss_u 0.9302 (0.8877) acc_u 6.2500 (13.4375) lr 4.7750e-04 eta 0:00:27
epoch [137/200] batch [15/70] time 0.432 (0.457) data 0.301 (0.326) loss_u loss_u 0.8325 (0.8808) acc_u 21.8750 (14.5833) lr 4.7750e-04 eta 0:00:25
epoch [137/200] batch [20/70] time 0.437 (0.459) data 0.305 (0.327) loss_u loss_u 0.8970 (0.8808) acc_u 15.6250 (14.6875) lr 4.7750e-04 eta 0:00:22
epoch [137/200] batch [25/70] time 0.590 (0.461) data 0.460 (0.330) loss_u loss_u 0.8730 (0.8803) acc_u 18.7500 (14.8750) lr 4.7750e-04 eta 0:00:20
epoch [137/200] batch [30/70] time 0.432 (0.455) data 0.301 (0.324) loss_u loss_u 0.8911 (0.8804) acc_u 9.3750 (14.7917) lr 4.7750e-04 eta 0:00:18
epoch [137/200] batch [35/70] time 0.386 (0.454) data 0.254 (0.323) loss_u loss_u 0.8149 (0.8790) acc_u 15.6250 (15.0000) lr 4.7750e-04 eta 0:00:15
epoch [137/200] batch [40/70] time 0.432 (0.455) data 0.300 (0.323) loss_u loss_u 0.8096 (0.8811) acc_u 25.0000 (14.6094) lr 4.7750e-04 eta 0:00:13
epoch [137/200] batch [45/70] time 0.516 (0.454) data 0.385 (0.322) loss_u loss_u 0.9316 (0.8818) acc_u 9.3750 (14.5833) lr 4.7750e-04 eta 0:00:11
epoch [137/200] batch [50/70] time 0.549 (0.454) data 0.417 (0.322) loss_u loss_u 0.8926 (0.8839) acc_u 12.5000 (14.3125) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [55/70] time 0.525 (0.452) data 0.393 (0.321) loss_u loss_u 0.8765 (0.8818) acc_u 15.6250 (14.4318) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [60/70] time 0.380 (0.451) data 0.249 (0.319) loss_u loss_u 0.9482 (0.8837) acc_u 6.2500 (14.1667) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [65/70] time 0.458 (0.454) data 0.327 (0.322) loss_u loss_u 0.8555 (0.8833) acc_u 15.6250 (14.2308) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [70/70] time 0.381 (0.450) data 0.249 (0.319) loss_u loss_u 0.9385 (0.8846) acc_u 6.2500 (14.0625) lr 4.7750e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1811
confident_label rate tensor(0.2730, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 856
clean true:621
clean false:235
clean_rate:0.7254672897196262
noisy true:704
noisy false:1576
after delete: len(clean_dataset) 856
after delete: len(noisy_dataset) 2280
epoch [138/200] batch [5/26] time 0.423 (0.434) data 0.292 (0.303) loss_x loss_x 0.9868 (1.2513) acc_x 75.0000 (68.1250) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [10/26] time 0.566 (0.459) data 0.435 (0.328) loss_x loss_x 1.1152 (1.4250) acc_x 68.7500 (64.3750) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [15/26] time 0.613 (0.485) data 0.483 (0.354) loss_x loss_x 1.0400 (1.3271) acc_x 75.0000 (67.5000) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [20/26] time 0.474 (0.479) data 0.343 (0.348) loss_x loss_x 0.8979 (1.2715) acc_x 84.3750 (68.7500) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [25/26] time 0.465 (0.472) data 0.335 (0.342) loss_x loss_x 1.4883 (1.2541) acc_x 68.7500 (69.1250) lr 4.6417e-04 eta 0:00:00
epoch [138/200] batch [5/71] time 0.500 (0.463) data 0.370 (0.332) loss_u loss_u 0.9072 (0.8865) acc_u 9.3750 (14.3750) lr 4.6417e-04 eta 0:00:30
epoch [138/200] batch [10/71] time 0.359 (0.453) data 0.228 (0.322) loss_u loss_u 0.9287 (0.8899) acc_u 6.2500 (13.1250) lr 4.6417e-04 eta 0:00:27
epoch [138/200] batch [15/71] time 0.543 (0.464) data 0.413 (0.333) loss_u loss_u 0.8760 (0.8842) acc_u 18.7500 (13.9583) lr 4.6417e-04 eta 0:00:25
epoch [138/200] batch [20/71] time 0.384 (0.460) data 0.252 (0.330) loss_u loss_u 0.8188 (0.8818) acc_u 21.8750 (14.5312) lr 4.6417e-04 eta 0:00:23
epoch [138/200] batch [25/71] time 0.519 (0.455) data 0.387 (0.325) loss_u loss_u 0.8789 (0.8863) acc_u 18.7500 (13.7500) lr 4.6417e-04 eta 0:00:20
epoch [138/200] batch [30/71] time 0.444 (0.455) data 0.312 (0.324) loss_u loss_u 0.9106 (0.8852) acc_u 9.3750 (14.1667) lr 4.6417e-04 eta 0:00:18
epoch [138/200] batch [35/71] time 0.387 (0.451) data 0.255 (0.320) loss_u loss_u 0.9531 (0.8864) acc_u 3.1250 (13.9286) lr 4.6417e-04 eta 0:00:16
epoch [138/200] batch [40/71] time 0.427 (0.447) data 0.295 (0.316) loss_u loss_u 0.9282 (0.8904) acc_u 12.5000 (13.3594) lr 4.6417e-04 eta 0:00:13
epoch [138/200] batch [45/71] time 0.461 (0.444) data 0.329 (0.313) loss_u loss_u 0.8828 (0.8909) acc_u 18.7500 (13.6806) lr 4.6417e-04 eta 0:00:11
epoch [138/200] batch [50/71] time 0.447 (0.446) data 0.317 (0.315) loss_u loss_u 0.8804 (0.8922) acc_u 15.6250 (13.4375) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [55/71] time 0.361 (0.444) data 0.229 (0.313) loss_u loss_u 0.8413 (0.8902) acc_u 21.8750 (13.6364) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [60/71] time 0.445 (0.445) data 0.313 (0.314) loss_u loss_u 0.9233 (0.8889) acc_u 6.2500 (13.6458) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [65/71] time 0.487 (0.445) data 0.355 (0.313) loss_u loss_u 0.8394 (0.8869) acc_u 18.7500 (13.8462) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [70/71] time 0.631 (0.446) data 0.499 (0.315) loss_u loss_u 0.9077 (0.8879) acc_u 12.5000 (13.5714) lr 4.6417e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1756
confident_label rate tensor(0.2828, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 887
clean true:657
clean false:230
clean_rate:0.7406989853438557
noisy true:723
noisy false:1526
after delete: len(clean_dataset) 887
after delete: len(noisy_dataset) 2249
epoch [139/200] batch [5/27] time 0.456 (0.460) data 0.325 (0.330) loss_x loss_x 1.4082 (1.2651) acc_x 65.6250 (66.8750) lr 4.5098e-04 eta 0:00:10
epoch [139/200] batch [10/27] time 0.472 (0.456) data 0.341 (0.326) loss_x loss_x 1.1104 (1.1021) acc_x 71.8750 (71.5625) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [15/27] time 0.445 (0.467) data 0.313 (0.337) loss_x loss_x 0.8540 (1.1128) acc_x 84.3750 (71.0417) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [20/27] time 0.380 (0.477) data 0.250 (0.346) loss_x loss_x 0.9146 (1.1342) acc_x 81.2500 (70.7812) lr 4.5098e-04 eta 0:00:03
epoch [139/200] batch [25/27] time 0.517 (0.480) data 0.386 (0.349) loss_x loss_x 1.6152 (1.1899) acc_x 59.3750 (69.7500) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/70] time 0.446 (0.464) data 0.315 (0.333) loss_u loss_u 0.8296 (0.8649) acc_u 21.8750 (17.5000) lr 4.5098e-04 eta 0:00:30
epoch [139/200] batch [10/70] time 0.483 (0.465) data 0.351 (0.334) loss_u loss_u 0.8633 (0.8754) acc_u 18.7500 (15.9375) lr 4.5098e-04 eta 0:00:27
epoch [139/200] batch [15/70] time 0.436 (0.460) data 0.304 (0.329) loss_u loss_u 0.9170 (0.8862) acc_u 6.2500 (14.5833) lr 4.5098e-04 eta 0:00:25
epoch [139/200] batch [20/70] time 0.614 (0.460) data 0.483 (0.329) loss_u loss_u 0.8374 (0.8823) acc_u 21.8750 (15.1562) lr 4.5098e-04 eta 0:00:23
epoch [139/200] batch [25/70] time 0.538 (0.458) data 0.407 (0.327) loss_u loss_u 0.8843 (0.8820) acc_u 15.6250 (14.8750) lr 4.5098e-04 eta 0:00:20
epoch [139/200] batch [30/70] time 0.373 (0.455) data 0.242 (0.324) loss_u loss_u 0.8491 (0.8806) acc_u 18.7500 (15.0000) lr 4.5098e-04 eta 0:00:18
epoch [139/200] batch [35/70] time 0.438 (0.452) data 0.306 (0.321) loss_u loss_u 0.8477 (0.8824) acc_u 21.8750 (14.7321) lr 4.5098e-04 eta 0:00:15
epoch [139/200] batch [40/70] time 0.442 (0.448) data 0.311 (0.317) loss_u loss_u 0.8936 (0.8850) acc_u 12.5000 (14.2969) lr 4.5098e-04 eta 0:00:13
epoch [139/200] batch [45/70] time 0.354 (0.444) data 0.223 (0.313) loss_u loss_u 0.8911 (0.8853) acc_u 12.5000 (14.5833) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [50/70] time 0.440 (0.445) data 0.308 (0.313) loss_u loss_u 0.8423 (0.8842) acc_u 18.7500 (14.5000) lr 4.5098e-04 eta 0:00:08
epoch [139/200] batch [55/70] time 0.439 (0.445) data 0.308 (0.313) loss_u loss_u 0.8315 (0.8841) acc_u 21.8750 (14.5455) lr 4.5098e-04 eta 0:00:06
epoch [139/200] batch [60/70] time 0.450 (0.443) data 0.319 (0.312) loss_u loss_u 0.8320 (0.8838) acc_u 18.7500 (14.3750) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [65/70] time 0.423 (0.443) data 0.292 (0.312) loss_u loss_u 0.9087 (0.8876) acc_u 9.3750 (13.8462) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [70/70] time 0.374 (0.442) data 0.243 (0.311) loss_u loss_u 0.8550 (0.8880) acc_u 21.8750 (13.7946) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1743
confident_label rate tensor(0.2784, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 873
clean true:640
clean false:233
clean_rate:0.7331042382588774
noisy true:753
noisy false:1510
after delete: len(clean_dataset) 873
after delete: len(noisy_dataset) 2263
epoch [140/200] batch [5/27] time 0.428 (0.476) data 0.297 (0.346) loss_x loss_x 0.9951 (1.1667) acc_x 65.6250 (70.0000) lr 4.3792e-04 eta 0:00:10
epoch [140/200] batch [10/27] time 0.411 (0.495) data 0.281 (0.364) loss_x loss_x 0.8262 (1.0852) acc_x 65.6250 (70.6250) lr 4.3792e-04 eta 0:00:08
epoch [140/200] batch [15/27] time 0.450 (0.467) data 0.319 (0.337) loss_x loss_x 1.1074 (1.0832) acc_x 71.8750 (72.0833) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [20/27] time 0.508 (0.468) data 0.377 (0.337) loss_x loss_x 1.7021 (1.1625) acc_x 68.7500 (71.2500) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [25/27] time 0.487 (0.470) data 0.357 (0.339) loss_x loss_x 0.9561 (1.1615) acc_x 75.0000 (71.6250) lr 4.3792e-04 eta 0:00:00
epoch [140/200] batch [5/70] time 0.438 (0.457) data 0.307 (0.327) loss_u loss_u 0.8726 (0.8938) acc_u 15.6250 (11.8750) lr 4.3792e-04 eta 0:00:29
epoch [140/200] batch [10/70] time 0.385 (0.453) data 0.254 (0.322) loss_u loss_u 0.8872 (0.8912) acc_u 12.5000 (13.7500) lr 4.3792e-04 eta 0:00:27
epoch [140/200] batch [15/70] time 0.376 (0.442) data 0.245 (0.311) loss_u loss_u 0.8604 (0.8849) acc_u 18.7500 (14.7917) lr 4.3792e-04 eta 0:00:24
epoch [140/200] batch [20/70] time 0.465 (0.437) data 0.334 (0.306) loss_u loss_u 0.9229 (0.8829) acc_u 9.3750 (15.4688) lr 4.3792e-04 eta 0:00:21
epoch [140/200] batch [25/70] time 0.429 (0.438) data 0.297 (0.307) loss_u loss_u 0.7983 (0.8832) acc_u 21.8750 (14.8750) lr 4.3792e-04 eta 0:00:19
epoch [140/200] batch [30/70] time 0.463 (0.438) data 0.333 (0.307) loss_u loss_u 0.8564 (0.8829) acc_u 15.6250 (14.5833) lr 4.3792e-04 eta 0:00:17
epoch [140/200] batch [35/70] time 0.360 (0.442) data 0.228 (0.311) loss_u loss_u 0.8774 (0.8860) acc_u 12.5000 (13.9286) lr 4.3792e-04 eta 0:00:15
epoch [140/200] batch [40/70] time 0.527 (0.444) data 0.396 (0.313) loss_u loss_u 0.8662 (0.8848) acc_u 18.7500 (14.0625) lr 4.3792e-04 eta 0:00:13
epoch [140/200] batch [45/70] time 0.403 (0.446) data 0.272 (0.315) loss_u loss_u 0.8433 (0.8886) acc_u 18.7500 (13.4722) lr 4.3792e-04 eta 0:00:11
epoch [140/200] batch [50/70] time 0.403 (0.445) data 0.273 (0.314) loss_u loss_u 0.8833 (0.8895) acc_u 12.5000 (13.3125) lr 4.3792e-04 eta 0:00:08
epoch [140/200] batch [55/70] time 0.389 (0.442) data 0.258 (0.311) loss_u loss_u 0.9238 (0.8906) acc_u 9.3750 (13.2386) lr 4.3792e-04 eta 0:00:06
epoch [140/200] batch [60/70] time 0.395 (0.443) data 0.265 (0.312) loss_u loss_u 0.8457 (0.8893) acc_u 15.6250 (13.2812) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [65/70] time 0.315 (0.441) data 0.184 (0.310) loss_u loss_u 0.8926 (0.8887) acc_u 18.7500 (13.4135) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [70/70] time 0.478 (0.443) data 0.346 (0.312) loss_u loss_u 0.8970 (0.8896) acc_u 12.5000 (13.3036) lr 4.3792e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.2768, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 868
clean true:637
clean false:231
clean_rate:0.7338709677419355
noisy true:750
noisy false:1518
after delete: len(clean_dataset) 868
after delete: len(noisy_dataset) 2268
epoch [141/200] batch [5/27] time 0.421 (0.395) data 0.290 (0.265) loss_x loss_x 0.5840 (0.8734) acc_x 87.5000 (78.7500) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [10/27] time 0.430 (0.418) data 0.300 (0.288) loss_x loss_x 1.1094 (1.1160) acc_x 65.6250 (70.9375) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [15/27] time 0.564 (0.431) data 0.434 (0.302) loss_x loss_x 1.7139 (1.1906) acc_x 56.2500 (70.2083) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [20/27] time 0.412 (0.429) data 0.281 (0.299) loss_x loss_x 0.7407 (1.1734) acc_x 84.3750 (70.9375) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [25/27] time 0.538 (0.454) data 0.406 (0.324) loss_x loss_x 1.1084 (1.2026) acc_x 78.1250 (70.7500) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/70] time 0.420 (0.444) data 0.288 (0.313) loss_u loss_u 0.8081 (0.8740) acc_u 28.1250 (15.6250) lr 4.2499e-04 eta 0:00:28
epoch [141/200] batch [10/70] time 0.389 (0.443) data 0.257 (0.312) loss_u loss_u 0.9297 (0.8895) acc_u 6.2500 (15.0000) lr 4.2499e-04 eta 0:00:26
epoch [141/200] batch [15/70] time 0.409 (0.445) data 0.277 (0.314) loss_u loss_u 0.8979 (0.8902) acc_u 12.5000 (14.3750) lr 4.2499e-04 eta 0:00:24
epoch [141/200] batch [20/70] time 0.442 (0.443) data 0.311 (0.312) loss_u loss_u 0.8145 (0.8813) acc_u 28.1250 (15.6250) lr 4.2499e-04 eta 0:00:22
epoch [141/200] batch [25/70] time 0.466 (0.440) data 0.335 (0.309) loss_u loss_u 0.9004 (0.8846) acc_u 9.3750 (14.7500) lr 4.2499e-04 eta 0:00:19
epoch [141/200] batch [30/70] time 0.376 (0.437) data 0.246 (0.306) loss_u loss_u 0.9194 (0.8869) acc_u 9.3750 (14.3750) lr 4.2499e-04 eta 0:00:17
epoch [141/200] batch [35/70] time 0.380 (0.437) data 0.249 (0.306) loss_u loss_u 0.8599 (0.8880) acc_u 21.8750 (14.0179) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [40/70] time 0.447 (0.436) data 0.316 (0.305) loss_u loss_u 0.9448 (0.8906) acc_u 6.2500 (13.3594) lr 4.2499e-04 eta 0:00:13
epoch [141/200] batch [45/70] time 0.460 (0.438) data 0.329 (0.307) loss_u loss_u 0.8916 (0.8878) acc_u 15.6250 (13.6806) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [50/70] time 0.485 (0.441) data 0.353 (0.310) loss_u loss_u 0.8374 (0.8847) acc_u 21.8750 (14.1250) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [55/70] time 0.381 (0.441) data 0.249 (0.310) loss_u loss_u 0.9692 (0.8870) acc_u 3.1250 (13.8636) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [60/70] time 0.457 (0.443) data 0.326 (0.312) loss_u loss_u 0.8906 (0.8881) acc_u 9.3750 (13.6979) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [65/70] time 0.401 (0.443) data 0.269 (0.312) loss_u loss_u 0.8691 (0.8884) acc_u 18.7500 (13.7019) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [70/70] time 0.558 (0.446) data 0.426 (0.315) loss_u loss_u 0.8657 (0.8889) acc_u 15.6250 (13.6161) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1750
confident_label rate tensor(0.2812, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 882
clean true:631
clean false:251
clean_rate:0.7154195011337868
noisy true:755
noisy false:1499
after delete: len(clean_dataset) 882
after delete: len(noisy_dataset) 2254
epoch [142/200] batch [5/27] time 0.378 (0.451) data 0.247 (0.320) loss_x loss_x 1.0605 (1.1037) acc_x 71.8750 (73.1250) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [10/27] time 0.494 (0.446) data 0.364 (0.316) loss_x loss_x 0.9253 (1.0934) acc_x 78.1250 (73.7500) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [15/27] time 0.660 (0.451) data 0.530 (0.321) loss_x loss_x 0.7305 (1.1141) acc_x 81.2500 (72.0833) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [20/27] time 0.501 (0.448) data 0.370 (0.317) loss_x loss_x 1.3398 (1.1539) acc_x 75.0000 (70.7812) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [25/27] time 0.532 (0.449) data 0.401 (0.318) loss_x loss_x 1.2246 (1.1692) acc_x 68.7500 (70.7500) lr 4.1221e-04 eta 0:00:00
epoch [142/200] batch [5/70] time 0.546 (0.451) data 0.415 (0.321) loss_u loss_u 0.8213 (0.8906) acc_u 21.8750 (13.1250) lr 4.1221e-04 eta 0:00:29
epoch [142/200] batch [10/70] time 0.389 (0.448) data 0.257 (0.317) loss_u loss_u 0.9077 (0.8993) acc_u 12.5000 (12.1875) lr 4.1221e-04 eta 0:00:26
epoch [142/200] batch [15/70] time 0.533 (0.449) data 0.402 (0.318) loss_u loss_u 0.9263 (0.9035) acc_u 9.3750 (11.6667) lr 4.1221e-04 eta 0:00:24
epoch [142/200] batch [20/70] time 0.639 (0.451) data 0.506 (0.320) loss_u loss_u 0.9277 (0.9056) acc_u 9.3750 (11.4062) lr 4.1221e-04 eta 0:00:22
epoch [142/200] batch [25/70] time 0.571 (0.455) data 0.440 (0.324) loss_u loss_u 0.8745 (0.9022) acc_u 18.7500 (11.6250) lr 4.1221e-04 eta 0:00:20
epoch [142/200] batch [30/70] time 0.391 (0.457) data 0.261 (0.325) loss_u loss_u 0.8760 (0.9014) acc_u 15.6250 (11.8750) lr 4.1221e-04 eta 0:00:18
epoch [142/200] batch [35/70] time 0.445 (0.451) data 0.314 (0.320) loss_u loss_u 0.8887 (0.9016) acc_u 12.5000 (11.9643) lr 4.1221e-04 eta 0:00:15
epoch [142/200] batch [40/70] time 0.432 (0.452) data 0.300 (0.321) loss_u loss_u 0.9019 (0.8982) acc_u 15.6250 (12.8906) lr 4.1221e-04 eta 0:00:13
epoch [142/200] batch [45/70] time 0.481 (0.452) data 0.348 (0.321) loss_u loss_u 0.8662 (0.8982) acc_u 21.8750 (12.7778) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [50/70] time 0.380 (0.451) data 0.249 (0.320) loss_u loss_u 0.8691 (0.8975) acc_u 15.6250 (13.0000) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [55/70] time 0.398 (0.448) data 0.266 (0.316) loss_u loss_u 0.8984 (0.8970) acc_u 12.5000 (13.1818) lr 4.1221e-04 eta 0:00:06
epoch [142/200] batch [60/70] time 0.372 (0.446) data 0.241 (0.315) loss_u loss_u 0.8418 (0.8965) acc_u 18.7500 (13.1250) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [65/70] time 0.574 (0.446) data 0.442 (0.315) loss_u loss_u 0.9233 (0.8960) acc_u 6.2500 (13.0769) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [70/70] time 0.370 (0.444) data 0.239 (0.313) loss_u loss_u 0.9150 (0.8930) acc_u 9.3750 (13.3929) lr 4.1221e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1760
confident_label rate tensor(0.2854, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 895
clean true:657
clean false:238
clean_rate:0.7340782122905027
noisy true:719
noisy false:1522
after delete: len(clean_dataset) 895
after delete: len(noisy_dataset) 2241
epoch [143/200] batch [5/27] time 0.476 (0.501) data 0.346 (0.371) loss_x loss_x 1.6221 (1.2538) acc_x 46.8750 (66.8750) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [10/27] time 0.491 (0.486) data 0.360 (0.355) loss_x loss_x 1.1348 (1.2143) acc_x 81.2500 (69.3750) lr 3.9958e-04 eta 0:00:08
epoch [143/200] batch [15/27] time 0.413 (0.460) data 0.283 (0.329) loss_x loss_x 1.2168 (1.2610) acc_x 65.6250 (69.1667) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [20/27] time 0.702 (0.472) data 0.572 (0.342) loss_x loss_x 0.7129 (1.2304) acc_x 78.1250 (70.1562) lr 3.9958e-04 eta 0:00:03
epoch [143/200] batch [25/27] time 0.448 (0.473) data 0.317 (0.343) loss_x loss_x 0.9824 (1.2453) acc_x 71.8750 (68.7500) lr 3.9958e-04 eta 0:00:00
epoch [143/200] batch [5/70] time 0.467 (0.465) data 0.335 (0.335) loss_u loss_u 0.8765 (0.8696) acc_u 15.6250 (15.6250) lr 3.9958e-04 eta 0:00:30
epoch [143/200] batch [10/70] time 0.399 (0.459) data 0.268 (0.328) loss_u loss_u 0.8911 (0.8805) acc_u 12.5000 (14.6875) lr 3.9958e-04 eta 0:00:27
epoch [143/200] batch [15/70] time 0.473 (0.456) data 0.342 (0.325) loss_u loss_u 0.9409 (0.8883) acc_u 6.2500 (14.1667) lr 3.9958e-04 eta 0:00:25
epoch [143/200] batch [20/70] time 0.377 (0.450) data 0.246 (0.319) loss_u loss_u 0.9146 (0.8958) acc_u 6.2500 (12.9688) lr 3.9958e-04 eta 0:00:22
epoch [143/200] batch [25/70] time 0.376 (0.443) data 0.246 (0.312) loss_u loss_u 0.8398 (0.8888) acc_u 15.6250 (13.8750) lr 3.9958e-04 eta 0:00:19
epoch [143/200] batch [30/70] time 0.428 (0.446) data 0.298 (0.316) loss_u loss_u 0.9053 (0.8891) acc_u 9.3750 (13.7500) lr 3.9958e-04 eta 0:00:17
epoch [143/200] batch [35/70] time 0.477 (0.444) data 0.347 (0.313) loss_u loss_u 0.8789 (0.8909) acc_u 9.3750 (13.3929) lr 3.9958e-04 eta 0:00:15
epoch [143/200] batch [40/70] time 0.425 (0.444) data 0.293 (0.314) loss_u loss_u 0.8706 (0.8899) acc_u 15.6250 (13.4375) lr 3.9958e-04 eta 0:00:13
epoch [143/200] batch [45/70] time 0.407 (0.445) data 0.276 (0.314) loss_u loss_u 0.9053 (0.8920) acc_u 12.5000 (13.4028) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [50/70] time 0.427 (0.445) data 0.296 (0.314) loss_u loss_u 0.9019 (0.8948) acc_u 9.3750 (13.1250) lr 3.9958e-04 eta 0:00:08
epoch [143/200] batch [55/70] time 0.523 (0.447) data 0.393 (0.317) loss_u loss_u 0.9175 (0.8934) acc_u 9.3750 (13.2955) lr 3.9958e-04 eta 0:00:06
epoch [143/200] batch [60/70] time 0.367 (0.447) data 0.235 (0.317) loss_u loss_u 0.8667 (0.8919) acc_u 9.3750 (13.2292) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [65/70] time 0.410 (0.446) data 0.279 (0.315) loss_u loss_u 0.7979 (0.8886) acc_u 25.0000 (13.5577) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [70/70] time 0.415 (0.447) data 0.282 (0.316) loss_u loss_u 0.9111 (0.8898) acc_u 12.5000 (13.3482) lr 3.9958e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1772
confident_label rate tensor(0.2758, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 865
clean true:623
clean false:242
clean_rate:0.7202312138728324
noisy true:741
noisy false:1530
after delete: len(clean_dataset) 865
after delete: len(noisy_dataset) 2271
epoch [144/200] batch [5/27] time 0.465 (0.459) data 0.335 (0.329) loss_x loss_x 1.3730 (1.1526) acc_x 75.0000 (71.2500) lr 3.8709e-04 eta 0:00:10
epoch [144/200] batch [10/27] time 0.451 (0.466) data 0.320 (0.335) loss_x loss_x 1.2568 (1.1164) acc_x 65.6250 (70.0000) lr 3.8709e-04 eta 0:00:07
epoch [144/200] batch [15/27] time 0.384 (0.466) data 0.252 (0.336) loss_x loss_x 1.0420 (1.1022) acc_x 68.7500 (69.7917) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [20/27] time 0.307 (0.453) data 0.176 (0.323) loss_x loss_x 1.1436 (1.0748) acc_x 81.2500 (71.8750) lr 3.8709e-04 eta 0:00:03
epoch [144/200] batch [25/27] time 0.533 (0.451) data 0.403 (0.321) loss_x loss_x 0.9956 (1.0788) acc_x 71.8750 (71.8750) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/70] time 0.462 (0.448) data 0.332 (0.317) loss_u loss_u 0.8525 (0.8848) acc_u 15.6250 (12.5000) lr 3.8709e-04 eta 0:00:29
epoch [144/200] batch [10/70] time 0.456 (0.453) data 0.325 (0.322) loss_u loss_u 0.8936 (0.8805) acc_u 12.5000 (13.7500) lr 3.8709e-04 eta 0:00:27
epoch [144/200] batch [15/70] time 0.491 (0.451) data 0.361 (0.320) loss_u loss_u 0.9336 (0.8718) acc_u 6.2500 (14.5833) lr 3.8709e-04 eta 0:00:24
epoch [144/200] batch [20/70] time 0.395 (0.447) data 0.264 (0.316) loss_u loss_u 0.8193 (0.8767) acc_u 21.8750 (14.2188) lr 3.8709e-04 eta 0:00:22
epoch [144/200] batch [25/70] time 0.444 (0.443) data 0.312 (0.312) loss_u loss_u 0.8408 (0.8769) acc_u 21.8750 (14.8750) lr 3.8709e-04 eta 0:00:19
epoch [144/200] batch [30/70] time 0.460 (0.448) data 0.328 (0.317) loss_u loss_u 0.8281 (0.8776) acc_u 21.8750 (14.6875) lr 3.8709e-04 eta 0:00:17
epoch [144/200] batch [35/70] time 0.363 (0.447) data 0.233 (0.316) loss_u loss_u 0.8647 (0.8807) acc_u 12.5000 (14.1071) lr 3.8709e-04 eta 0:00:15
epoch [144/200] batch [40/70] time 0.454 (0.448) data 0.323 (0.317) loss_u loss_u 0.8975 (0.8826) acc_u 12.5000 (14.0625) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [45/70] time 0.443 (0.445) data 0.311 (0.314) loss_u loss_u 0.9092 (0.8830) acc_u 9.3750 (13.8194) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [50/70] time 0.491 (0.446) data 0.359 (0.315) loss_u loss_u 0.8950 (0.8822) acc_u 9.3750 (14.2500) lr 3.8709e-04 eta 0:00:08
epoch [144/200] batch [55/70] time 0.440 (0.447) data 0.308 (0.316) loss_u loss_u 0.8672 (0.8809) acc_u 18.7500 (14.6023) lr 3.8709e-04 eta 0:00:06
epoch [144/200] batch [60/70] time 0.395 (0.444) data 0.265 (0.313) loss_u loss_u 0.9155 (0.8838) acc_u 9.3750 (14.3750) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [65/70] time 0.375 (0.446) data 0.243 (0.315) loss_u loss_u 0.9062 (0.8863) acc_u 9.3750 (13.9904) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [70/70] time 0.371 (0.443) data 0.239 (0.312) loss_u loss_u 0.9155 (0.8875) acc_u 6.2500 (13.7946) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1780
confident_label rate tensor(0.2832, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 888
clean true:641
clean false:247
clean_rate:0.7218468468468469
noisy true:715
noisy false:1533
after delete: len(clean_dataset) 888
after delete: len(noisy_dataset) 2248
epoch [145/200] batch [5/27] time 0.705 (0.480) data 0.574 (0.349) loss_x loss_x 1.4473 (1.2283) acc_x 62.5000 (68.1250) lr 3.7476e-04 eta 0:00:10
epoch [145/200] batch [10/27] time 0.422 (0.470) data 0.292 (0.339) loss_x loss_x 0.9805 (1.0864) acc_x 75.0000 (72.1875) lr 3.7476e-04 eta 0:00:07
epoch [145/200] batch [15/27] time 0.337 (0.473) data 0.206 (0.342) loss_x loss_x 1.1182 (1.0987) acc_x 68.7500 (71.8750) lr 3.7476e-04 eta 0:00:05
epoch [145/200] batch [20/27] time 0.384 (0.458) data 0.255 (0.327) loss_x loss_x 1.2061 (1.0994) acc_x 71.8750 (72.0312) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [25/27] time 0.496 (0.450) data 0.366 (0.320) loss_x loss_x 1.0918 (1.0871) acc_x 75.0000 (72.7500) lr 3.7476e-04 eta 0:00:00
epoch [145/200] batch [5/70] time 0.504 (0.464) data 0.373 (0.333) loss_u loss_u 0.9312 (0.8595) acc_u 9.3750 (15.6250) lr 3.7476e-04 eta 0:00:30
epoch [145/200] batch [10/70] time 0.431 (0.463) data 0.300 (0.332) loss_u loss_u 0.9126 (0.8826) acc_u 12.5000 (14.6875) lr 3.7476e-04 eta 0:00:27
epoch [145/200] batch [15/70] time 0.823 (0.469) data 0.693 (0.338) loss_u loss_u 0.8945 (0.8824) acc_u 12.5000 (15.2083) lr 3.7476e-04 eta 0:00:25
epoch [145/200] batch [20/70] time 0.574 (0.467) data 0.443 (0.337) loss_u loss_u 0.8999 (0.8819) acc_u 12.5000 (14.8438) lr 3.7476e-04 eta 0:00:23
epoch [145/200] batch [25/70] time 0.446 (0.466) data 0.315 (0.335) loss_u loss_u 0.9419 (0.8863) acc_u 6.2500 (14.3750) lr 3.7476e-04 eta 0:00:20
epoch [145/200] batch [30/70] time 0.352 (0.459) data 0.221 (0.328) loss_u loss_u 0.8975 (0.8905) acc_u 15.6250 (13.6458) lr 3.7476e-04 eta 0:00:18
epoch [145/200] batch [35/70] time 0.368 (0.454) data 0.237 (0.323) loss_u loss_u 0.9507 (0.8932) acc_u 6.2500 (13.3929) lr 3.7476e-04 eta 0:00:15
epoch [145/200] batch [40/70] time 0.380 (0.451) data 0.249 (0.320) loss_u loss_u 0.9233 (0.8961) acc_u 9.3750 (12.8906) lr 3.7476e-04 eta 0:00:13
epoch [145/200] batch [45/70] time 0.371 (0.451) data 0.240 (0.320) loss_u loss_u 0.9160 (0.8952) acc_u 9.3750 (12.9861) lr 3.7476e-04 eta 0:00:11
epoch [145/200] batch [50/70] time 0.421 (0.450) data 0.290 (0.319) loss_u loss_u 0.8647 (0.8958) acc_u 15.6250 (12.8750) lr 3.7476e-04 eta 0:00:08
epoch [145/200] batch [55/70] time 0.451 (0.446) data 0.319 (0.315) loss_u loss_u 0.8481 (0.8935) acc_u 21.8750 (13.3523) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [60/70] time 0.447 (0.446) data 0.316 (0.315) loss_u loss_u 0.8896 (0.8923) acc_u 9.3750 (13.4375) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [65/70] time 0.406 (0.443) data 0.274 (0.312) loss_u loss_u 0.9326 (0.8937) acc_u 9.3750 (13.2212) lr 3.7476e-04 eta 0:00:02
epoch [145/200] batch [70/70] time 0.507 (0.445) data 0.376 (0.314) loss_u loss_u 0.8672 (0.8924) acc_u 15.6250 (13.3929) lr 3.7476e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1758
confident_label rate tensor(0.2860, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 897
clean true:652
clean false:245
clean_rate:0.7268673355629878
noisy true:726
noisy false:1513
after delete: len(clean_dataset) 897
after delete: len(noisy_dataset) 2239
epoch [146/200] batch [5/28] time 0.582 (0.453) data 0.451 (0.322) loss_x loss_x 1.0127 (1.1967) acc_x 75.0000 (68.7500) lr 3.6258e-04 eta 0:00:10
epoch [146/200] batch [10/28] time 0.395 (0.459) data 0.264 (0.328) loss_x loss_x 1.6992 (1.1330) acc_x 62.5000 (70.9375) lr 3.6258e-04 eta 0:00:08
epoch [146/200] batch [15/28] time 0.455 (0.454) data 0.324 (0.324) loss_x loss_x 0.9912 (1.0495) acc_x 75.0000 (72.2917) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [20/28] time 0.449 (0.453) data 0.318 (0.322) loss_x loss_x 0.8984 (1.0526) acc_x 75.0000 (72.9688) lr 3.6258e-04 eta 0:00:03
epoch [146/200] batch [25/28] time 0.441 (0.454) data 0.309 (0.323) loss_x loss_x 1.3037 (1.0828) acc_x 62.5000 (72.5000) lr 3.6258e-04 eta 0:00:01
epoch [146/200] batch [5/69] time 0.455 (0.461) data 0.323 (0.330) loss_u loss_u 0.8252 (0.8874) acc_u 15.6250 (12.5000) lr 3.6258e-04 eta 0:00:29
epoch [146/200] batch [10/69] time 0.294 (0.459) data 0.162 (0.328) loss_u loss_u 0.9077 (0.8769) acc_u 9.3750 (14.3750) lr 3.6258e-04 eta 0:00:27
epoch [146/200] batch [15/69] time 0.379 (0.461) data 0.247 (0.329) loss_u loss_u 0.8872 (0.8839) acc_u 12.5000 (13.3333) lr 3.6258e-04 eta 0:00:24
epoch [146/200] batch [20/69] time 0.420 (0.459) data 0.289 (0.327) loss_u loss_u 0.9121 (0.8970) acc_u 3.1250 (11.4062) lr 3.6258e-04 eta 0:00:22
epoch [146/200] batch [25/69] time 0.432 (0.456) data 0.299 (0.325) loss_u loss_u 0.8569 (0.8926) acc_u 12.5000 (11.8750) lr 3.6258e-04 eta 0:00:20
epoch [146/200] batch [30/69] time 0.443 (0.460) data 0.311 (0.329) loss_u loss_u 0.8462 (0.8882) acc_u 15.6250 (12.3958) lr 3.6258e-04 eta 0:00:17
epoch [146/200] batch [35/69] time 0.436 (0.457) data 0.304 (0.326) loss_u loss_u 0.8413 (0.8892) acc_u 18.7500 (12.6786) lr 3.6258e-04 eta 0:00:15
epoch [146/200] batch [40/69] time 0.363 (0.455) data 0.232 (0.324) loss_u loss_u 0.9668 (0.8941) acc_u 0.0000 (12.0312) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [45/69] time 0.439 (0.454) data 0.308 (0.323) loss_u loss_u 0.8770 (0.8927) acc_u 15.6250 (12.3611) lr 3.6258e-04 eta 0:00:10
epoch [146/200] batch [50/69] time 0.490 (0.453) data 0.358 (0.321) loss_u loss_u 0.8765 (0.8923) acc_u 12.5000 (12.5000) lr 3.6258e-04 eta 0:00:08
epoch [146/200] batch [55/69] time 0.579 (0.454) data 0.447 (0.322) loss_u loss_u 0.8389 (0.8904) acc_u 21.8750 (12.8409) lr 3.6258e-04 eta 0:00:06
epoch [146/200] batch [60/69] time 0.401 (0.452) data 0.269 (0.321) loss_u loss_u 0.9033 (0.8908) acc_u 9.3750 (12.7083) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [65/69] time 0.415 (0.453) data 0.281 (0.321) loss_u loss_u 0.9458 (0.8921) acc_u 9.3750 (12.5962) lr 3.6258e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1775
confident_label rate tensor(0.2889, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 906
clean true:645
clean false:261
clean_rate:0.7119205298013245
noisy true:716
noisy false:1514
after delete: len(clean_dataset) 906
after delete: len(noisy_dataset) 2230
epoch [147/200] batch [5/28] time 0.671 (0.510) data 0.540 (0.379) loss_x loss_x 1.5654 (1.2006) acc_x 62.5000 (73.7500) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [10/28] time 0.469 (0.490) data 0.338 (0.359) loss_x loss_x 1.1240 (1.2238) acc_x 65.6250 (70.3125) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [15/28] time 0.472 (0.475) data 0.342 (0.344) loss_x loss_x 1.4404 (1.2452) acc_x 62.5000 (69.3750) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [20/28] time 0.540 (0.487) data 0.409 (0.356) loss_x loss_x 1.4014 (1.1918) acc_x 62.5000 (71.0938) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [25/28] time 0.453 (0.486) data 0.322 (0.355) loss_x loss_x 1.2246 (1.1846) acc_x 68.7500 (71.0000) lr 3.5055e-04 eta 0:00:01
epoch [147/200] batch [5/69] time 0.376 (0.478) data 0.244 (0.347) loss_u loss_u 0.8818 (0.8678) acc_u 12.5000 (18.1250) lr 3.5055e-04 eta 0:00:30
epoch [147/200] batch [10/69] time 0.492 (0.471) data 0.361 (0.340) loss_u loss_u 0.9585 (0.8880) acc_u 6.2500 (13.7500) lr 3.5055e-04 eta 0:00:27
epoch [147/200] batch [15/69] time 0.407 (0.463) data 0.275 (0.332) loss_u loss_u 0.9189 (0.8934) acc_u 9.3750 (13.3333) lr 3.5055e-04 eta 0:00:25
epoch [147/200] batch [20/69] time 0.364 (0.455) data 0.233 (0.324) loss_u loss_u 0.8760 (0.8904) acc_u 12.5000 (13.1250) lr 3.5055e-04 eta 0:00:22
epoch [147/200] batch [25/69] time 0.449 (0.457) data 0.318 (0.326) loss_u loss_u 0.9170 (0.8951) acc_u 9.3750 (12.5000) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [30/69] time 0.379 (0.456) data 0.248 (0.325) loss_u loss_u 0.9277 (0.8933) acc_u 9.3750 (12.9167) lr 3.5055e-04 eta 0:00:17
epoch [147/200] batch [35/69] time 0.350 (0.456) data 0.219 (0.325) loss_u loss_u 0.9307 (0.8934) acc_u 9.3750 (12.9464) lr 3.5055e-04 eta 0:00:15
epoch [147/200] batch [40/69] time 0.420 (0.454) data 0.289 (0.323) loss_u loss_u 0.9209 (0.8917) acc_u 9.3750 (13.3594) lr 3.5055e-04 eta 0:00:13
epoch [147/200] batch [45/69] time 0.419 (0.454) data 0.287 (0.322) loss_u loss_u 0.9038 (0.8901) acc_u 15.6250 (13.6111) lr 3.5055e-04 eta 0:00:10
epoch [147/200] batch [50/69] time 0.453 (0.455) data 0.322 (0.324) loss_u loss_u 0.8979 (0.8907) acc_u 9.3750 (13.4375) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [55/69] time 0.405 (0.454) data 0.274 (0.323) loss_u loss_u 0.8633 (0.8906) acc_u 18.7500 (13.4659) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [60/69] time 0.415 (0.455) data 0.282 (0.324) loss_u loss_u 0.8867 (0.8919) acc_u 15.6250 (13.2292) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [65/69] time 0.387 (0.452) data 0.256 (0.321) loss_u loss_u 0.9297 (0.8908) acc_u 9.3750 (13.3654) lr 3.5055e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.2844, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 892
clean true:652
clean false:240
clean_rate:0.7309417040358744
noisy true:735
noisy false:1509
after delete: len(clean_dataset) 892
after delete: len(noisy_dataset) 2244
epoch [148/200] batch [5/27] time 0.479 (0.544) data 0.349 (0.414) loss_x loss_x 1.2441 (1.1493) acc_x 71.8750 (72.5000) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [10/27] time 0.487 (0.511) data 0.357 (0.380) loss_x loss_x 0.7256 (1.1335) acc_x 78.1250 (72.5000) lr 3.3869e-04 eta 0:00:08
epoch [148/200] batch [15/27] time 0.457 (0.492) data 0.327 (0.362) loss_x loss_x 0.8516 (1.0916) acc_x 81.2500 (72.9167) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [20/27] time 0.413 (0.477) data 0.283 (0.347) loss_x loss_x 1.2969 (1.0604) acc_x 68.7500 (73.1250) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [25/27] time 0.436 (0.473) data 0.306 (0.343) loss_x loss_x 1.1992 (1.0719) acc_x 68.7500 (72.6250) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/70] time 0.438 (0.467) data 0.307 (0.336) loss_u loss_u 0.8667 (0.8934) acc_u 21.8750 (15.0000) lr 3.3869e-04 eta 0:00:30
epoch [148/200] batch [10/70] time 0.477 (0.464) data 0.346 (0.334) loss_u loss_u 0.8677 (0.8845) acc_u 18.7500 (15.9375) lr 3.3869e-04 eta 0:00:27
epoch [148/200] batch [15/70] time 0.435 (0.458) data 0.304 (0.327) loss_u loss_u 0.9385 (0.8911) acc_u 9.3750 (13.7500) lr 3.3869e-04 eta 0:00:25
epoch [148/200] batch [20/70] time 0.390 (0.460) data 0.258 (0.329) loss_u loss_u 0.8433 (0.8868) acc_u 21.8750 (14.3750) lr 3.3869e-04 eta 0:00:22
epoch [148/200] batch [25/70] time 0.418 (0.454) data 0.285 (0.323) loss_u loss_u 0.8853 (0.8909) acc_u 15.6250 (13.6250) lr 3.3869e-04 eta 0:00:20
epoch [148/200] batch [30/70] time 0.385 (0.454) data 0.254 (0.323) loss_u loss_u 0.8452 (0.8880) acc_u 25.0000 (14.1667) lr 3.3869e-04 eta 0:00:18
epoch [148/200] batch [35/70] time 0.345 (0.455) data 0.214 (0.324) loss_u loss_u 0.8491 (0.8875) acc_u 21.8750 (14.2857) lr 3.3869e-04 eta 0:00:15
epoch [148/200] batch [40/70] time 0.480 (0.454) data 0.349 (0.323) loss_u loss_u 0.8550 (0.8879) acc_u 18.7500 (14.1406) lr 3.3869e-04 eta 0:00:13
epoch [148/200] batch [45/70] time 0.459 (0.451) data 0.327 (0.320) loss_u loss_u 0.8716 (0.8889) acc_u 15.6250 (13.9583) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [50/70] time 0.372 (0.450) data 0.241 (0.319) loss_u loss_u 0.8223 (0.8905) acc_u 18.7500 (13.5625) lr 3.3869e-04 eta 0:00:08
epoch [148/200] batch [55/70] time 0.462 (0.448) data 0.332 (0.317) loss_u loss_u 0.8652 (0.8905) acc_u 15.6250 (13.8636) lr 3.3869e-04 eta 0:00:06
epoch [148/200] batch [60/70] time 0.415 (0.447) data 0.284 (0.316) loss_u loss_u 0.9453 (0.8931) acc_u 9.3750 (13.4896) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [65/70] time 0.388 (0.447) data 0.256 (0.316) loss_u loss_u 0.9185 (0.8925) acc_u 6.2500 (13.4615) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [70/70] time 0.363 (0.448) data 0.231 (0.317) loss_u loss_u 0.8950 (0.8924) acc_u 12.5000 (13.4375) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1748
confident_label rate tensor(0.2819, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 884
clean true:650
clean false:234
clean_rate:0.7352941176470589
noisy true:738
noisy false:1514
after delete: len(clean_dataset) 884
after delete: len(noisy_dataset) 2252
epoch [149/200] batch [5/27] time 0.524 (0.456) data 0.393 (0.325) loss_x loss_x 1.5127 (1.2945) acc_x 62.5000 (63.7500) lr 3.2699e-04 eta 0:00:10
epoch [149/200] batch [10/27] time 0.352 (0.444) data 0.222 (0.313) loss_x loss_x 1.3154 (1.2062) acc_x 65.6250 (67.5000) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [15/27] time 0.492 (0.450) data 0.361 (0.320) loss_x loss_x 1.2090 (1.1250) acc_x 68.7500 (69.5833) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [20/27] time 0.486 (0.461) data 0.356 (0.331) loss_x loss_x 1.0234 (1.1114) acc_x 78.1250 (70.0000) lr 3.2699e-04 eta 0:00:03
epoch [149/200] batch [25/27] time 0.431 (0.457) data 0.300 (0.326) loss_x loss_x 1.5137 (1.1048) acc_x 68.7500 (71.0000) lr 3.2699e-04 eta 0:00:00
epoch [149/200] batch [5/70] time 0.560 (0.453) data 0.429 (0.322) loss_u loss_u 0.8506 (0.8858) acc_u 18.7500 (13.7500) lr 3.2699e-04 eta 0:00:29
epoch [149/200] batch [10/70] time 0.488 (0.452) data 0.357 (0.321) loss_u loss_u 0.8892 (0.8872) acc_u 12.5000 (13.7500) lr 3.2699e-04 eta 0:00:27
epoch [149/200] batch [15/70] time 0.504 (0.455) data 0.374 (0.324) loss_u loss_u 0.8564 (0.8766) acc_u 21.8750 (15.8333) lr 3.2699e-04 eta 0:00:25
epoch [149/200] batch [20/70] time 0.355 (0.449) data 0.224 (0.318) loss_u loss_u 0.9360 (0.8817) acc_u 6.2500 (15.3125) lr 3.2699e-04 eta 0:00:22
epoch [149/200] batch [25/70] time 0.445 (0.444) data 0.313 (0.314) loss_u loss_u 0.9600 (0.8910) acc_u 3.1250 (14.1250) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [30/70] time 0.387 (0.450) data 0.255 (0.319) loss_u loss_u 0.9224 (0.8894) acc_u 9.3750 (14.2708) lr 3.2699e-04 eta 0:00:17
epoch [149/200] batch [35/70] time 0.395 (0.446) data 0.263 (0.315) loss_u loss_u 0.8965 (0.8923) acc_u 15.6250 (13.9286) lr 3.2699e-04 eta 0:00:15
epoch [149/200] batch [40/70] time 0.453 (0.448) data 0.322 (0.317) loss_u loss_u 0.8721 (0.8898) acc_u 12.5000 (14.0625) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [45/70] time 0.414 (0.446) data 0.283 (0.315) loss_u loss_u 0.8516 (0.8893) acc_u 18.7500 (13.6806) lr 3.2699e-04 eta 0:00:11
epoch [149/200] batch [50/70] time 0.444 (0.448) data 0.313 (0.317) loss_u loss_u 0.9126 (0.8882) acc_u 12.5000 (13.8750) lr 3.2699e-04 eta 0:00:08
epoch [149/200] batch [55/70] time 0.381 (0.446) data 0.251 (0.315) loss_u loss_u 0.8911 (0.8866) acc_u 15.6250 (14.2614) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [60/70] time 0.530 (0.447) data 0.399 (0.316) loss_u loss_u 0.8887 (0.8850) acc_u 15.6250 (14.4271) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [65/70] time 0.376 (0.445) data 0.245 (0.314) loss_u loss_u 0.8003 (0.8825) acc_u 21.8750 (14.8077) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [70/70] time 0.462 (0.444) data 0.330 (0.313) loss_u loss_u 0.9185 (0.8831) acc_u 9.3750 (14.7768) lr 3.2699e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1773
confident_label rate tensor(0.2793, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 876
clean true:628
clean false:248
clean_rate:0.7168949771689498
noisy true:735
noisy false:1525
after delete: len(clean_dataset) 876
after delete: len(noisy_dataset) 2260
epoch [150/200] batch [5/27] time 0.360 (0.513) data 0.231 (0.383) loss_x loss_x 1.1475 (1.4485) acc_x 71.8750 (67.5000) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [10/27] time 0.503 (0.488) data 0.372 (0.358) loss_x loss_x 0.9639 (1.2593) acc_x 78.1250 (70.6250) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [15/27] time 0.512 (0.472) data 0.380 (0.341) loss_x loss_x 0.9292 (1.2016) acc_x 71.8750 (71.2500) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [20/27] time 0.488 (0.471) data 0.358 (0.341) loss_x loss_x 0.7163 (1.1608) acc_x 81.2500 (70.7812) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [25/27] time 0.382 (0.459) data 0.252 (0.329) loss_x loss_x 1.5029 (1.2189) acc_x 65.6250 (70.0000) lr 3.1545e-04 eta 0:00:00
epoch [150/200] batch [5/70] time 0.494 (0.457) data 0.364 (0.326) loss_u loss_u 0.9097 (0.8866) acc_u 18.7500 (16.8750) lr 3.1545e-04 eta 0:00:29
epoch [150/200] batch [10/70] time 0.316 (0.450) data 0.185 (0.319) loss_u loss_u 0.9141 (0.9007) acc_u 9.3750 (13.1250) lr 3.1545e-04 eta 0:00:26
epoch [150/200] batch [15/70] time 0.576 (0.448) data 0.445 (0.318) loss_u loss_u 0.8672 (0.8942) acc_u 18.7500 (13.7500) lr 3.1545e-04 eta 0:00:24
epoch [150/200] batch [20/70] time 0.605 (0.451) data 0.475 (0.320) loss_u loss_u 0.8921 (0.8950) acc_u 15.6250 (13.2812) lr 3.1545e-04 eta 0:00:22
epoch [150/200] batch [25/70] time 0.409 (0.444) data 0.278 (0.314) loss_u loss_u 0.9424 (0.8948) acc_u 9.3750 (13.5000) lr 3.1545e-04 eta 0:00:20
epoch [150/200] batch [30/70] time 0.423 (0.446) data 0.293 (0.316) loss_u loss_u 0.8447 (0.8909) acc_u 28.1250 (14.0625) lr 3.1545e-04 eta 0:00:17
epoch [150/200] batch [35/70] time 0.444 (0.445) data 0.313 (0.314) loss_u loss_u 0.7842 (0.8902) acc_u 28.1250 (14.1071) lr 3.1545e-04 eta 0:00:15
epoch [150/200] batch [40/70] time 0.367 (0.443) data 0.236 (0.313) loss_u loss_u 0.9038 (0.8885) acc_u 12.5000 (14.2188) lr 3.1545e-04 eta 0:00:13
epoch [150/200] batch [45/70] time 0.458 (0.442) data 0.326 (0.312) loss_u loss_u 0.9102 (0.8894) acc_u 12.5000 (13.8194) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [50/70] time 0.420 (0.441) data 0.288 (0.311) loss_u loss_u 0.8955 (0.8873) acc_u 15.6250 (14.0000) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [55/70] time 0.363 (0.441) data 0.233 (0.310) loss_u loss_u 0.8252 (0.8850) acc_u 18.7500 (14.2045) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [60/70] time 0.346 (0.437) data 0.215 (0.307) loss_u loss_u 0.8477 (0.8844) acc_u 21.8750 (14.5312) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [65/70] time 0.753 (0.440) data 0.621 (0.309) loss_u loss_u 0.8730 (0.8863) acc_u 9.3750 (14.1827) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [70/70] time 0.373 (0.441) data 0.243 (0.311) loss_u loss_u 0.8652 (0.8865) acc_u 18.7500 (14.1964) lr 3.1545e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1727
confident_label rate tensor(0.2832, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 888
clean true:648
clean false:240
clean_rate:0.7297297297297297
noisy true:761
noisy false:1487
after delete: len(clean_dataset) 888
after delete: len(noisy_dataset) 2248
epoch [151/200] batch [5/27] time 0.505 (0.504) data 0.374 (0.373) loss_x loss_x 1.5566 (1.1223) acc_x 53.1250 (68.1250) lr 3.0409e-04 eta 0:00:11
epoch [151/200] batch [10/27] time 0.360 (0.478) data 0.230 (0.347) loss_x loss_x 1.0361 (1.1712) acc_x 68.7500 (69.3750) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [15/27] time 0.470 (0.471) data 0.339 (0.340) loss_x loss_x 1.0410 (1.1323) acc_x 68.7500 (69.7917) lr 3.0409e-04 eta 0:00:05
epoch [151/200] batch [20/27] time 0.444 (0.477) data 0.314 (0.347) loss_x loss_x 1.2246 (1.1376) acc_x 78.1250 (70.6250) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [25/27] time 0.516 (0.469) data 0.386 (0.338) loss_x loss_x 1.0527 (1.1292) acc_x 71.8750 (70.8750) lr 3.0409e-04 eta 0:00:00
epoch [151/200] batch [5/70] time 0.510 (0.459) data 0.379 (0.329) loss_u loss_u 0.8906 (0.8991) acc_u 15.6250 (14.3750) lr 3.0409e-04 eta 0:00:29
epoch [151/200] batch [10/70] time 0.429 (0.453) data 0.297 (0.322) loss_u loss_u 0.7822 (0.8852) acc_u 28.1250 (16.8750) lr 3.0409e-04 eta 0:00:27
epoch [151/200] batch [15/70] time 0.439 (0.458) data 0.308 (0.327) loss_u loss_u 0.7974 (0.8786) acc_u 18.7500 (16.2500) lr 3.0409e-04 eta 0:00:25
epoch [151/200] batch [20/70] time 0.394 (0.452) data 0.263 (0.321) loss_u loss_u 0.9185 (0.8822) acc_u 12.5000 (15.7812) lr 3.0409e-04 eta 0:00:22
epoch [151/200] batch [25/70] time 0.420 (0.451) data 0.288 (0.320) loss_u loss_u 0.8555 (0.8889) acc_u 21.8750 (14.8750) lr 3.0409e-04 eta 0:00:20
epoch [151/200] batch [30/70] time 0.338 (0.449) data 0.207 (0.318) loss_u loss_u 0.8584 (0.8870) acc_u 15.6250 (15.1042) lr 3.0409e-04 eta 0:00:17
epoch [151/200] batch [35/70] time 0.410 (0.446) data 0.278 (0.315) loss_u loss_u 0.8154 (0.8847) acc_u 21.8750 (15.1786) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [40/70] time 0.409 (0.449) data 0.277 (0.317) loss_u loss_u 0.8970 (0.8850) acc_u 12.5000 (15.0000) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [45/70] time 0.489 (0.449) data 0.357 (0.318) loss_u loss_u 0.8799 (0.8838) acc_u 15.6250 (14.9306) lr 3.0409e-04 eta 0:00:11
epoch [151/200] batch [50/70] time 0.453 (0.451) data 0.322 (0.319) loss_u loss_u 0.8765 (0.8848) acc_u 18.7500 (14.8125) lr 3.0409e-04 eta 0:00:09
epoch [151/200] batch [55/70] time 0.383 (0.450) data 0.252 (0.319) loss_u loss_u 0.9297 (0.8861) acc_u 6.2500 (14.4318) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [60/70] time 0.444 (0.448) data 0.313 (0.317) loss_u loss_u 0.9526 (0.8860) acc_u 6.2500 (14.5833) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [65/70] time 0.418 (0.448) data 0.286 (0.316) loss_u loss_u 0.9170 (0.8871) acc_u 9.3750 (14.4231) lr 3.0409e-04 eta 0:00:02
epoch [151/200] batch [70/70] time 0.425 (0.447) data 0.293 (0.316) loss_u loss_u 0.9702 (0.8886) acc_u 3.1250 (14.1518) lr 3.0409e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1795
confident_label rate tensor(0.2809, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 881
clean true:639
clean false:242
clean_rate:0.7253121452894438
noisy true:702
noisy false:1553
after delete: len(clean_dataset) 881
after delete: len(noisy_dataset) 2255
epoch [152/200] batch [5/27] time 0.606 (0.470) data 0.476 (0.340) loss_x loss_x 0.9956 (1.2150) acc_x 75.0000 (70.6250) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [10/27] time 0.439 (0.449) data 0.309 (0.318) loss_x loss_x 1.1699 (1.1211) acc_x 62.5000 (71.8750) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [15/27] time 0.517 (0.447) data 0.387 (0.317) loss_x loss_x 1.1689 (1.1431) acc_x 71.8750 (70.6250) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [20/27] time 0.491 (0.440) data 0.361 (0.309) loss_x loss_x 0.8594 (1.1641) acc_x 78.1250 (70.0000) lr 2.9289e-04 eta 0:00:03
epoch [152/200] batch [25/27] time 0.398 (0.445) data 0.268 (0.315) loss_x loss_x 1.0947 (1.1634) acc_x 75.0000 (70.1250) lr 2.9289e-04 eta 0:00:00
epoch [152/200] batch [5/70] time 0.502 (0.457) data 0.371 (0.327) loss_u loss_u 0.9458 (0.8965) acc_u 9.3750 (14.3750) lr 2.9289e-04 eta 0:00:29
epoch [152/200] batch [10/70] time 0.412 (0.454) data 0.281 (0.323) loss_u loss_u 0.8564 (0.8857) acc_u 25.0000 (16.2500) lr 2.9289e-04 eta 0:00:27
epoch [152/200] batch [15/70] time 0.473 (0.454) data 0.341 (0.323) loss_u loss_u 0.9048 (0.8844) acc_u 12.5000 (15.8333) lr 2.9289e-04 eta 0:00:24
epoch [152/200] batch [20/70] time 0.399 (0.456) data 0.268 (0.326) loss_u loss_u 0.9590 (0.8920) acc_u 6.2500 (14.2188) lr 2.9289e-04 eta 0:00:22
epoch [152/200] batch [25/70] time 0.505 (0.458) data 0.374 (0.327) loss_u loss_u 0.9287 (0.8872) acc_u 6.2500 (14.3750) lr 2.9289e-04 eta 0:00:20
epoch [152/200] batch [30/70] time 0.360 (0.457) data 0.229 (0.326) loss_u loss_u 0.8530 (0.8892) acc_u 21.8750 (14.0625) lr 2.9289e-04 eta 0:00:18
epoch [152/200] batch [35/70] time 0.423 (0.455) data 0.291 (0.324) loss_u loss_u 0.9048 (0.8877) acc_u 9.3750 (14.1071) lr 2.9289e-04 eta 0:00:15
epoch [152/200] batch [40/70] time 0.578 (0.460) data 0.448 (0.329) loss_u loss_u 0.9131 (0.8863) acc_u 6.2500 (14.1406) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [45/70] time 0.423 (0.459) data 0.292 (0.328) loss_u loss_u 0.9023 (0.8832) acc_u 9.3750 (14.5139) lr 2.9289e-04 eta 0:00:11
epoch [152/200] batch [50/70] time 0.349 (0.452) data 0.219 (0.321) loss_u loss_u 0.8838 (0.8846) acc_u 12.5000 (14.2500) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [55/70] time 0.552 (0.452) data 0.422 (0.321) loss_u loss_u 0.8682 (0.8841) acc_u 18.7500 (14.3750) lr 2.9289e-04 eta 0:00:06
epoch [152/200] batch [60/70] time 0.367 (0.448) data 0.236 (0.317) loss_u loss_u 0.9414 (0.8850) acc_u 6.2500 (14.3229) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [65/70] time 0.434 (0.449) data 0.303 (0.318) loss_u loss_u 0.8896 (0.8859) acc_u 15.6250 (14.1346) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [70/70] time 0.376 (0.445) data 0.245 (0.314) loss_u loss_u 0.9419 (0.8852) acc_u 6.2500 (14.3304) lr 2.9289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1765
confident_label rate tensor(0.2892, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 907
clean true:662
clean false:245
clean_rate:0.7298787210584344
noisy true:709
noisy false:1520
after delete: len(clean_dataset) 907
after delete: len(noisy_dataset) 2229
epoch [153/200] batch [5/28] time 0.373 (0.438) data 0.242 (0.307) loss_x loss_x 1.3086 (1.2610) acc_x 62.5000 (69.3750) lr 2.8187e-04 eta 0:00:10
epoch [153/200] batch [10/28] time 0.552 (0.456) data 0.420 (0.325) loss_x loss_x 0.9326 (1.1300) acc_x 71.8750 (70.6250) lr 2.8187e-04 eta 0:00:08
epoch [153/200] batch [15/28] time 0.378 (0.478) data 0.247 (0.347) loss_x loss_x 1.1475 (1.1759) acc_x 62.5000 (68.9583) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [20/28] time 0.474 (0.485) data 0.345 (0.355) loss_x loss_x 1.0469 (1.1684) acc_x 68.7500 (69.2188) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [25/28] time 0.401 (0.470) data 0.270 (0.339) loss_x loss_x 1.4570 (1.1498) acc_x 68.7500 (70.7500) lr 2.8187e-04 eta 0:00:01
epoch [153/200] batch [5/69] time 0.517 (0.464) data 0.385 (0.333) loss_u loss_u 0.8931 (0.8770) acc_u 12.5000 (16.8750) lr 2.8187e-04 eta 0:00:29
epoch [153/200] batch [10/69] time 0.388 (0.458) data 0.257 (0.327) loss_u loss_u 0.8921 (0.8837) acc_u 15.6250 (15.6250) lr 2.8187e-04 eta 0:00:27
epoch [153/200] batch [15/69] time 0.498 (0.453) data 0.367 (0.322) loss_u loss_u 0.9141 (0.8867) acc_u 12.5000 (14.7917) lr 2.8187e-04 eta 0:00:24
epoch [153/200] batch [20/69] time 0.429 (0.450) data 0.297 (0.319) loss_u loss_u 0.7998 (0.8853) acc_u 28.1250 (14.5312) lr 2.8187e-04 eta 0:00:22
epoch [153/200] batch [25/69] time 0.356 (0.447) data 0.225 (0.316) loss_u loss_u 0.8809 (0.8856) acc_u 12.5000 (14.5000) lr 2.8187e-04 eta 0:00:19
epoch [153/200] batch [30/69] time 0.468 (0.445) data 0.336 (0.314) loss_u loss_u 0.9375 (0.8879) acc_u 12.5000 (14.1667) lr 2.8187e-04 eta 0:00:17
epoch [153/200] batch [35/69] time 0.456 (0.445) data 0.323 (0.313) loss_u loss_u 0.9067 (0.8843) acc_u 6.2500 (14.4643) lr 2.8187e-04 eta 0:00:15
epoch [153/200] batch [40/69] time 0.487 (0.446) data 0.356 (0.315) loss_u loss_u 0.9102 (0.8866) acc_u 9.3750 (14.2188) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [45/69] time 0.537 (0.446) data 0.405 (0.315) loss_u loss_u 0.9082 (0.8905) acc_u 9.3750 (13.7500) lr 2.8187e-04 eta 0:00:10
epoch [153/200] batch [50/69] time 0.387 (0.447) data 0.256 (0.315) loss_u loss_u 0.8896 (0.8905) acc_u 15.6250 (13.6250) lr 2.8187e-04 eta 0:00:08
epoch [153/200] batch [55/69] time 0.367 (0.445) data 0.235 (0.314) loss_u loss_u 0.8999 (0.8915) acc_u 9.3750 (13.5227) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [60/69] time 0.366 (0.441) data 0.234 (0.310) loss_u loss_u 0.8916 (0.8905) acc_u 18.7500 (13.7500) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [65/69] time 0.405 (0.441) data 0.274 (0.309) loss_u loss_u 0.8589 (0.8901) acc_u 15.6250 (13.7019) lr 2.8187e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1746
confident_label rate tensor(0.2848, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 893
clean true:649
clean false:244
clean_rate:0.7267637178051511
noisy true:741
noisy false:1502
after delete: len(clean_dataset) 893
after delete: len(noisy_dataset) 2243
epoch [154/200] batch [5/27] time 0.530 (0.419) data 0.400 (0.289) loss_x loss_x 1.3086 (1.0775) acc_x 68.7500 (73.7500) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [10/27] time 0.373 (0.447) data 0.243 (0.316) loss_x loss_x 0.7744 (1.1792) acc_x 87.5000 (72.1875) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [15/27] time 0.563 (0.451) data 0.433 (0.321) loss_x loss_x 1.3193 (1.1622) acc_x 71.8750 (72.9167) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [20/27] time 0.477 (0.447) data 0.346 (0.316) loss_x loss_x 1.1270 (1.1727) acc_x 68.7500 (71.8750) lr 2.7103e-04 eta 0:00:03
epoch [154/200] batch [25/27] time 0.534 (0.453) data 0.403 (0.322) loss_x loss_x 1.0088 (1.1525) acc_x 75.0000 (72.2500) lr 2.7103e-04 eta 0:00:00
epoch [154/200] batch [5/70] time 0.613 (0.470) data 0.481 (0.339) loss_u loss_u 0.8687 (0.8614) acc_u 15.6250 (15.6250) lr 2.7103e-04 eta 0:00:30
epoch [154/200] batch [10/70] time 0.518 (0.472) data 0.386 (0.341) loss_u loss_u 0.9111 (0.8788) acc_u 15.6250 (15.0000) lr 2.7103e-04 eta 0:00:28
epoch [154/200] batch [15/70] time 0.347 (0.464) data 0.215 (0.333) loss_u loss_u 0.9380 (0.8848) acc_u 12.5000 (14.7917) lr 2.7103e-04 eta 0:00:25
epoch [154/200] batch [20/70] time 0.377 (0.457) data 0.245 (0.326) loss_u loss_u 0.8823 (0.8847) acc_u 15.6250 (14.6875) lr 2.7103e-04 eta 0:00:22
epoch [154/200] batch [25/70] time 0.474 (0.455) data 0.342 (0.324) loss_u loss_u 0.8589 (0.8830) acc_u 12.5000 (14.7500) lr 2.7103e-04 eta 0:00:20
epoch [154/200] batch [30/70] time 0.421 (0.449) data 0.290 (0.318) loss_u loss_u 0.8979 (0.8852) acc_u 12.5000 (14.7917) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [35/70] time 0.458 (0.448) data 0.327 (0.317) loss_u loss_u 0.8633 (0.8824) acc_u 12.5000 (14.9107) lr 2.7103e-04 eta 0:00:15
epoch [154/200] batch [40/70] time 0.381 (0.447) data 0.249 (0.316) loss_u loss_u 0.9150 (0.8846) acc_u 6.2500 (14.2188) lr 2.7103e-04 eta 0:00:13
epoch [154/200] batch [45/70] time 0.412 (0.447) data 0.280 (0.316) loss_u loss_u 0.8311 (0.8856) acc_u 21.8750 (14.0278) lr 2.7103e-04 eta 0:00:11
epoch [154/200] batch [50/70] time 0.479 (0.446) data 0.347 (0.314) loss_u loss_u 0.9478 (0.8892) acc_u 6.2500 (13.3750) lr 2.7103e-04 eta 0:00:08
epoch [154/200] batch [55/70] time 0.424 (0.446) data 0.292 (0.315) loss_u loss_u 0.8706 (0.8877) acc_u 15.6250 (13.7500) lr 2.7103e-04 eta 0:00:06
epoch [154/200] batch [60/70] time 0.490 (0.446) data 0.359 (0.315) loss_u loss_u 0.9604 (0.8890) acc_u 3.1250 (13.6458) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [65/70] time 0.374 (0.444) data 0.242 (0.313) loss_u loss_u 0.9414 (0.8911) acc_u 9.3750 (13.4135) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [70/70] time 0.388 (0.444) data 0.257 (0.313) loss_u loss_u 0.9219 (0.8920) acc_u 9.3750 (13.1250) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1744
confident_label rate tensor(0.2854, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 895
clean true:647
clean false:248
clean_rate:0.7229050279329609
noisy true:745
noisy false:1496
after delete: len(clean_dataset) 895
after delete: len(noisy_dataset) 2241
epoch [155/200] batch [5/27] time 0.463 (0.461) data 0.332 (0.329) loss_x loss_x 1.2246 (1.2981) acc_x 65.6250 (67.5000) lr 2.6037e-04 eta 0:00:10
epoch [155/200] batch [10/27] time 0.548 (0.474) data 0.417 (0.343) loss_x loss_x 0.7388 (1.1857) acc_x 87.5000 (70.3125) lr 2.6037e-04 eta 0:00:08
epoch [155/200] batch [15/27] time 0.353 (0.470) data 0.224 (0.339) loss_x loss_x 1.4619 (1.1865) acc_x 65.6250 (69.5833) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [20/27] time 0.430 (0.458) data 0.299 (0.327) loss_x loss_x 0.9297 (1.1327) acc_x 75.0000 (70.6250) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [25/27] time 0.459 (0.458) data 0.328 (0.327) loss_x loss_x 1.2217 (1.0995) acc_x 62.5000 (71.1250) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/70] time 0.429 (0.458) data 0.298 (0.327) loss_u loss_u 0.9341 (0.8841) acc_u 9.3750 (15.0000) lr 2.6037e-04 eta 0:00:29
epoch [155/200] batch [10/70] time 0.346 (0.447) data 0.215 (0.317) loss_u loss_u 0.8584 (0.8967) acc_u 15.6250 (12.5000) lr 2.6037e-04 eta 0:00:26
epoch [155/200] batch [15/70] time 0.496 (0.444) data 0.364 (0.313) loss_u loss_u 0.9277 (0.8887) acc_u 6.2500 (13.9583) lr 2.6037e-04 eta 0:00:24
epoch [155/200] batch [20/70] time 0.529 (0.450) data 0.397 (0.319) loss_u loss_u 0.8994 (0.8807) acc_u 9.3750 (14.8438) lr 2.6037e-04 eta 0:00:22
epoch [155/200] batch [25/70] time 0.436 (0.448) data 0.304 (0.317) loss_u loss_u 0.7607 (0.8800) acc_u 31.2500 (15.0000) lr 2.6037e-04 eta 0:00:20
epoch [155/200] batch [30/70] time 0.574 (0.449) data 0.443 (0.318) loss_u loss_u 0.9224 (0.8850) acc_u 12.5000 (14.2708) lr 2.6037e-04 eta 0:00:17
epoch [155/200] batch [35/70] time 0.533 (0.451) data 0.401 (0.320) loss_u loss_u 0.7861 (0.8802) acc_u 28.1250 (15.0893) lr 2.6037e-04 eta 0:00:15
epoch [155/200] batch [40/70] time 0.478 (0.452) data 0.346 (0.321) loss_u loss_u 0.8545 (0.8809) acc_u 12.5000 (14.6094) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [45/70] time 0.475 (0.451) data 0.343 (0.320) loss_u loss_u 0.8940 (0.8789) acc_u 12.5000 (15.0694) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [50/70] time 0.470 (0.451) data 0.339 (0.320) loss_u loss_u 0.8633 (0.8806) acc_u 18.7500 (14.8125) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [55/70] time 0.539 (0.449) data 0.409 (0.318) loss_u loss_u 0.8613 (0.8797) acc_u 18.7500 (14.9432) lr 2.6037e-04 eta 0:00:06
epoch [155/200] batch [60/70] time 0.543 (0.450) data 0.412 (0.319) loss_u loss_u 0.9238 (0.8812) acc_u 6.2500 (14.5833) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [65/70] time 0.436 (0.448) data 0.305 (0.317) loss_u loss_u 0.9292 (0.8813) acc_u 9.3750 (14.5192) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [70/70] time 0.418 (0.447) data 0.288 (0.316) loss_u loss_u 0.9004 (0.8814) acc_u 12.5000 (14.6875) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1793
confident_label rate tensor(0.2851, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 894
clean true:646
clean false:248
clean_rate:0.7225950782997763
noisy true:697
noisy false:1545
after delete: len(clean_dataset) 894
after delete: len(noisy_dataset) 2242
epoch [156/200] batch [5/27] time 0.340 (0.410) data 0.209 (0.279) loss_x loss_x 1.0146 (1.1835) acc_x 75.0000 (71.8750) lr 2.4989e-04 eta 0:00:09
epoch [156/200] batch [10/27] time 0.367 (0.428) data 0.236 (0.298) loss_x loss_x 1.4092 (1.1799) acc_x 65.6250 (72.8125) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [15/27] time 0.672 (0.456) data 0.541 (0.325) loss_x loss_x 0.8252 (1.1972) acc_x 78.1250 (71.2500) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [20/27] time 0.509 (0.451) data 0.378 (0.321) loss_x loss_x 1.1895 (1.1483) acc_x 62.5000 (72.1875) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [25/27] time 0.599 (0.461) data 0.469 (0.330) loss_x loss_x 1.1904 (1.1056) acc_x 65.6250 (72.7500) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/70] time 0.379 (0.450) data 0.248 (0.319) loss_u loss_u 0.8584 (0.8862) acc_u 12.5000 (13.7500) lr 2.4989e-04 eta 0:00:29
epoch [156/200] batch [10/70] time 0.513 (0.447) data 0.383 (0.316) loss_u loss_u 0.9448 (0.8971) acc_u 3.1250 (13.1250) lr 2.4989e-04 eta 0:00:26
epoch [156/200] batch [15/70] time 0.390 (0.447) data 0.258 (0.316) loss_u loss_u 0.8096 (0.8888) acc_u 25.0000 (13.9583) lr 2.4989e-04 eta 0:00:24
epoch [156/200] batch [20/70] time 0.419 (0.443) data 0.288 (0.312) loss_u loss_u 0.8965 (0.8943) acc_u 18.7500 (13.7500) lr 2.4989e-04 eta 0:00:22
epoch [156/200] batch [25/70] time 0.571 (0.447) data 0.439 (0.316) loss_u loss_u 0.9473 (0.8991) acc_u 3.1250 (13.1250) lr 2.4989e-04 eta 0:00:20
epoch [156/200] batch [30/70] time 0.456 (0.444) data 0.325 (0.313) loss_u loss_u 0.9116 (0.8983) acc_u 12.5000 (13.1250) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [35/70] time 0.345 (0.441) data 0.214 (0.310) loss_u loss_u 0.9253 (0.8983) acc_u 12.5000 (13.1250) lr 2.4989e-04 eta 0:00:15
epoch [156/200] batch [40/70] time 0.461 (0.440) data 0.331 (0.309) loss_u loss_u 0.8555 (0.8962) acc_u 15.6250 (13.2812) lr 2.4989e-04 eta 0:00:13
epoch [156/200] batch [45/70] time 0.409 (0.440) data 0.278 (0.309) loss_u loss_u 0.8760 (0.8964) acc_u 12.5000 (13.2639) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [50/70] time 0.401 (0.437) data 0.269 (0.307) loss_u loss_u 0.7915 (0.8914) acc_u 28.1250 (13.7500) lr 2.4989e-04 eta 0:00:08
epoch [156/200] batch [55/70] time 0.397 (0.437) data 0.266 (0.306) loss_u loss_u 0.8848 (0.8921) acc_u 12.5000 (13.6364) lr 2.4989e-04 eta 0:00:06
epoch [156/200] batch [60/70] time 0.505 (0.439) data 0.374 (0.308) loss_u loss_u 0.9458 (0.8945) acc_u 12.5000 (13.4896) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [65/70] time 0.577 (0.438) data 0.446 (0.307) loss_u loss_u 0.8950 (0.8935) acc_u 12.5000 (13.6058) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [70/70] time 0.449 (0.440) data 0.317 (0.309) loss_u loss_u 0.8486 (0.8925) acc_u 21.8750 (13.7500) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1737
confident_label rate tensor(0.2857, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 896
clean true:661
clean false:235
clean_rate:0.7377232142857143
noisy true:738
noisy false:1502
after delete: len(clean_dataset) 896
after delete: len(noisy_dataset) 2240
epoch [157/200] batch [5/28] time 0.599 (0.470) data 0.468 (0.340) loss_x loss_x 0.9756 (1.1204) acc_x 71.8750 (72.5000) lr 2.3959e-04 eta 0:00:10
epoch [157/200] batch [10/28] time 0.414 (0.450) data 0.284 (0.320) loss_x loss_x 1.6699 (1.2261) acc_x 56.2500 (68.7500) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [15/28] time 0.566 (0.474) data 0.437 (0.344) loss_x loss_x 0.6841 (1.1366) acc_x 84.3750 (71.8750) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [20/28] time 0.516 (0.466) data 0.386 (0.336) loss_x loss_x 1.4727 (1.1389) acc_x 65.6250 (72.1875) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [25/28] time 0.561 (0.470) data 0.430 (0.340) loss_x loss_x 0.9463 (1.1099) acc_x 68.7500 (72.7500) lr 2.3959e-04 eta 0:00:01
epoch [157/200] batch [5/70] time 0.603 (0.465) data 0.472 (0.335) loss_u loss_u 0.9067 (0.9084) acc_u 18.7500 (10.6250) lr 2.3959e-04 eta 0:00:30
epoch [157/200] batch [10/70] time 0.499 (0.467) data 0.368 (0.336) loss_u loss_u 0.8735 (0.9079) acc_u 25.0000 (12.5000) lr 2.3959e-04 eta 0:00:28
epoch [157/200] batch [15/70] time 0.444 (0.463) data 0.314 (0.333) loss_u loss_u 0.8857 (0.9009) acc_u 9.3750 (12.7083) lr 2.3959e-04 eta 0:00:25
epoch [157/200] batch [20/70] time 0.330 (0.453) data 0.199 (0.323) loss_u loss_u 0.9229 (0.8879) acc_u 9.3750 (14.3750) lr 2.3959e-04 eta 0:00:22
epoch [157/200] batch [25/70] time 0.419 (0.447) data 0.289 (0.316) loss_u loss_u 0.8945 (0.8846) acc_u 12.5000 (14.5000) lr 2.3959e-04 eta 0:00:20
epoch [157/200] batch [30/70] time 0.388 (0.440) data 0.257 (0.309) loss_u loss_u 0.8306 (0.8841) acc_u 21.8750 (14.4792) lr 2.3959e-04 eta 0:00:17
epoch [157/200] batch [35/70] time 0.355 (0.435) data 0.223 (0.305) loss_u loss_u 0.8784 (0.8836) acc_u 12.5000 (14.3750) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [40/70] time 0.463 (0.438) data 0.332 (0.307) loss_u loss_u 0.8623 (0.8834) acc_u 15.6250 (14.2969) lr 2.3959e-04 eta 0:00:13
epoch [157/200] batch [45/70] time 0.419 (0.436) data 0.287 (0.306) loss_u loss_u 0.8486 (0.8855) acc_u 15.6250 (13.9583) lr 2.3959e-04 eta 0:00:10
epoch [157/200] batch [50/70] time 0.481 (0.437) data 0.350 (0.307) loss_u loss_u 0.9165 (0.8876) acc_u 12.5000 (13.8750) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [55/70] time 0.407 (0.437) data 0.275 (0.306) loss_u loss_u 0.8862 (0.8856) acc_u 15.6250 (14.2614) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [60/70] time 0.476 (0.437) data 0.345 (0.306) loss_u loss_u 0.9312 (0.8869) acc_u 9.3750 (14.0625) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [65/70] time 0.551 (0.438) data 0.420 (0.307) loss_u loss_u 0.8975 (0.8875) acc_u 9.3750 (13.8942) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [70/70] time 0.361 (0.437) data 0.230 (0.306) loss_u loss_u 0.9404 (0.8881) acc_u 9.3750 (13.7946) lr 2.3959e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1779
confident_label rate tensor(0.2950, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 925
clean true:658
clean false:267
clean_rate:0.7113513513513513
noisy true:699
noisy false:1512
after delete: len(clean_dataset) 925
after delete: len(noisy_dataset) 2211
epoch [158/200] batch [5/28] time 0.432 (0.428) data 0.301 (0.297) loss_x loss_x 0.9004 (0.9787) acc_x 78.1250 (73.1250) lr 2.2949e-04 eta 0:00:09
epoch [158/200] batch [10/28] time 0.595 (0.434) data 0.464 (0.303) loss_x loss_x 0.5869 (0.9707) acc_x 84.3750 (75.9375) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [15/28] time 0.396 (0.437) data 0.265 (0.306) loss_x loss_x 1.4971 (1.0714) acc_x 62.5000 (73.1250) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [20/28] time 0.459 (0.446) data 0.328 (0.315) loss_x loss_x 0.9858 (1.1045) acc_x 68.7500 (72.3438) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [25/28] time 0.449 (0.454) data 0.319 (0.323) loss_x loss_x 1.4844 (1.1353) acc_x 62.5000 (70.6250) lr 2.2949e-04 eta 0:00:01
epoch [158/200] batch [5/69] time 0.449 (0.450) data 0.317 (0.319) loss_u loss_u 0.8882 (0.8946) acc_u 12.5000 (11.2500) lr 2.2949e-04 eta 0:00:28
epoch [158/200] batch [10/69] time 0.455 (0.450) data 0.323 (0.319) loss_u loss_u 0.8740 (0.8846) acc_u 15.6250 (12.5000) lr 2.2949e-04 eta 0:00:26
epoch [158/200] batch [15/69] time 0.373 (0.448) data 0.242 (0.316) loss_u loss_u 0.8833 (0.8813) acc_u 12.5000 (12.7083) lr 2.2949e-04 eta 0:00:24
epoch [158/200] batch [20/69] time 0.484 (0.445) data 0.353 (0.314) loss_u loss_u 0.9189 (0.8771) acc_u 9.3750 (14.2188) lr 2.2949e-04 eta 0:00:21
epoch [158/200] batch [25/69] time 0.416 (0.447) data 0.284 (0.316) loss_u loss_u 0.9243 (0.8737) acc_u 9.3750 (15.2500) lr 2.2949e-04 eta 0:00:19
epoch [158/200] batch [30/69] time 0.449 (0.453) data 0.318 (0.322) loss_u loss_u 0.8545 (0.8803) acc_u 25.0000 (14.8958) lr 2.2949e-04 eta 0:00:17
epoch [158/200] batch [35/69] time 0.436 (0.450) data 0.304 (0.319) loss_u loss_u 0.9062 (0.8821) acc_u 15.6250 (14.8214) lr 2.2949e-04 eta 0:00:15
epoch [158/200] batch [40/69] time 0.435 (0.453) data 0.303 (0.322) loss_u loss_u 0.9575 (0.8833) acc_u 6.2500 (14.6094) lr 2.2949e-04 eta 0:00:13
epoch [158/200] batch [45/69] time 0.512 (0.455) data 0.380 (0.323) loss_u loss_u 0.8281 (0.8824) acc_u 21.8750 (14.5139) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [50/69] time 0.351 (0.451) data 0.219 (0.320) loss_u loss_u 0.8545 (0.8837) acc_u 18.7500 (14.2500) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [55/69] time 0.411 (0.449) data 0.279 (0.317) loss_u loss_u 0.9263 (0.8849) acc_u 6.2500 (13.9773) lr 2.2949e-04 eta 0:00:06
epoch [158/200] batch [60/69] time 0.480 (0.449) data 0.348 (0.318) loss_u loss_u 0.8774 (0.8854) acc_u 12.5000 (13.9583) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [65/69] time 0.330 (0.447) data 0.199 (0.316) loss_u loss_u 0.8975 (0.8863) acc_u 15.6250 (13.8942) lr 2.2949e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1771
confident_label rate tensor(0.2828, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 887
clean true:652
clean false:235
clean_rate:0.7350620067643743
noisy true:713
noisy false:1536
after delete: len(clean_dataset) 887
after delete: len(noisy_dataset) 2249
epoch [159/200] batch [5/27] time 0.646 (0.537) data 0.516 (0.407) loss_x loss_x 0.7656 (1.0648) acc_x 84.3750 (78.1250) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [10/27] time 0.352 (0.463) data 0.222 (0.333) loss_x loss_x 1.2061 (1.0983) acc_x 68.7500 (73.7500) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [15/27] time 0.447 (0.457) data 0.317 (0.327) loss_x loss_x 0.8242 (1.0532) acc_x 75.0000 (73.7500) lr 2.1957e-04 eta 0:00:05
epoch [159/200] batch [20/27] time 0.379 (0.455) data 0.248 (0.324) loss_x loss_x 0.6875 (1.0868) acc_x 78.1250 (71.8750) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [25/27] time 0.462 (0.449) data 0.332 (0.318) loss_x loss_x 1.2461 (1.0801) acc_x 68.7500 (71.2500) lr 2.1957e-04 eta 0:00:00
epoch [159/200] batch [5/70] time 0.512 (0.445) data 0.381 (0.315) loss_u loss_u 0.9360 (0.8949) acc_u 6.2500 (11.8750) lr 2.1957e-04 eta 0:00:28
epoch [159/200] batch [10/70] time 0.402 (0.448) data 0.271 (0.317) loss_u loss_u 0.8970 (0.8901) acc_u 12.5000 (11.8750) lr 2.1957e-04 eta 0:00:26
epoch [159/200] batch [15/70] time 0.413 (0.445) data 0.281 (0.314) loss_u loss_u 0.9111 (0.9002) acc_u 9.3750 (11.2500) lr 2.1957e-04 eta 0:00:24
epoch [159/200] batch [20/70] time 0.381 (0.444) data 0.249 (0.313) loss_u loss_u 0.8818 (0.8923) acc_u 12.5000 (12.5000) lr 2.1957e-04 eta 0:00:22
epoch [159/200] batch [25/70] time 0.505 (0.442) data 0.374 (0.311) loss_u loss_u 0.8799 (0.8848) acc_u 12.5000 (13.8750) lr 2.1957e-04 eta 0:00:19
epoch [159/200] batch [30/70] time 0.400 (0.439) data 0.269 (0.308) loss_u loss_u 0.8901 (0.8850) acc_u 15.6250 (14.1667) lr 2.1957e-04 eta 0:00:17
epoch [159/200] batch [35/70] time 0.348 (0.440) data 0.217 (0.309) loss_u loss_u 0.8901 (0.8856) acc_u 12.5000 (14.1964) lr 2.1957e-04 eta 0:00:15
epoch [159/200] batch [40/70] time 0.440 (0.439) data 0.309 (0.308) loss_u loss_u 0.8242 (0.8807) acc_u 25.0000 (14.6094) lr 2.1957e-04 eta 0:00:13
epoch [159/200] batch [45/70] time 0.356 (0.436) data 0.225 (0.305) loss_u loss_u 0.8428 (0.8826) acc_u 21.8750 (14.3056) lr 2.1957e-04 eta 0:00:10
epoch [159/200] batch [50/70] time 0.438 (0.437) data 0.306 (0.306) loss_u loss_u 0.8970 (0.8825) acc_u 9.3750 (14.2500) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [55/70] time 0.401 (0.437) data 0.270 (0.306) loss_u loss_u 0.8975 (0.8818) acc_u 9.3750 (14.3182) lr 2.1957e-04 eta 0:00:06
epoch [159/200] batch [60/70] time 0.404 (0.438) data 0.273 (0.306) loss_u loss_u 0.9238 (0.8838) acc_u 6.2500 (14.1146) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [65/70] time 0.636 (0.439) data 0.504 (0.308) loss_u loss_u 0.8081 (0.8835) acc_u 21.8750 (14.1827) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [70/70] time 0.472 (0.439) data 0.340 (0.307) loss_u loss_u 0.8760 (0.8839) acc_u 15.6250 (14.1071) lr 2.1957e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1747
confident_label rate tensor(0.2905, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 911
clean true:650
clean false:261
clean_rate:0.7135016465422612
noisy true:739
noisy false:1486
after delete: len(clean_dataset) 911
after delete: len(noisy_dataset) 2225
epoch [160/200] batch [5/28] time 0.640 (0.508) data 0.509 (0.378) loss_x loss_x 1.6807 (1.1531) acc_x 59.3750 (70.0000) lr 2.0984e-04 eta 0:00:11
epoch [160/200] batch [10/28] time 0.439 (0.485) data 0.308 (0.354) loss_x loss_x 1.2480 (1.2188) acc_x 75.0000 (68.4375) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [15/28] time 0.428 (0.468) data 0.297 (0.337) loss_x loss_x 0.8906 (1.2049) acc_x 78.1250 (70.0000) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [20/28] time 0.551 (0.459) data 0.421 (0.328) loss_x loss_x 0.7207 (1.1553) acc_x 84.3750 (71.8750) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [25/28] time 0.382 (0.458) data 0.251 (0.327) loss_x loss_x 0.8560 (1.0863) acc_x 78.1250 (73.5000) lr 2.0984e-04 eta 0:00:01
epoch [160/200] batch [5/69] time 0.529 (0.458) data 0.398 (0.328) loss_u loss_u 0.9214 (0.8952) acc_u 9.3750 (14.3750) lr 2.0984e-04 eta 0:00:29
epoch [160/200] batch [10/69] time 0.502 (0.460) data 0.371 (0.330) loss_u loss_u 0.9243 (0.8925) acc_u 9.3750 (14.6875) lr 2.0984e-04 eta 0:00:27
epoch [160/200] batch [15/69] time 0.436 (0.457) data 0.305 (0.326) loss_u loss_u 0.8613 (0.8831) acc_u 18.7500 (15.8333) lr 2.0984e-04 eta 0:00:24
epoch [160/200] batch [20/69] time 0.424 (0.457) data 0.293 (0.326) loss_u loss_u 0.8999 (0.8903) acc_u 9.3750 (14.2188) lr 2.0984e-04 eta 0:00:22
epoch [160/200] batch [25/69] time 0.488 (0.460) data 0.357 (0.329) loss_u loss_u 0.8940 (0.8884) acc_u 9.3750 (14.1250) lr 2.0984e-04 eta 0:00:20
epoch [160/200] batch [30/69] time 0.394 (0.454) data 0.264 (0.323) loss_u loss_u 0.9229 (0.8870) acc_u 6.2500 (14.2708) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [35/69] time 0.407 (0.452) data 0.277 (0.321) loss_u loss_u 0.8843 (0.8855) acc_u 18.7500 (14.6429) lr 2.0984e-04 eta 0:00:15
epoch [160/200] batch [40/69] time 0.355 (0.451) data 0.224 (0.320) loss_u loss_u 0.9492 (0.8884) acc_u 6.2500 (14.3750) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [45/69] time 0.493 (0.452) data 0.361 (0.321) loss_u loss_u 0.9141 (0.8900) acc_u 12.5000 (14.1667) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [50/69] time 0.356 (0.449) data 0.226 (0.318) loss_u loss_u 0.8955 (0.8894) acc_u 12.5000 (14.1250) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [55/69] time 0.411 (0.449) data 0.280 (0.318) loss_u loss_u 0.8398 (0.8884) acc_u 15.6250 (14.3182) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [60/69] time 0.523 (0.452) data 0.392 (0.321) loss_u loss_u 0.9312 (0.8887) acc_u 9.3750 (14.2708) lr 2.0984e-04 eta 0:00:04
epoch [160/200] batch [65/69] time 0.421 (0.452) data 0.290 (0.321) loss_u loss_u 0.9365 (0.8888) acc_u 6.2500 (14.1346) lr 2.0984e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1738
confident_label rate tensor(0.2911, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 913
clean true:670
clean false:243
clean_rate:0.7338444687842278
noisy true:728
noisy false:1495
after delete: len(clean_dataset) 913
after delete: len(noisy_dataset) 2223
epoch [161/200] batch [5/28] time 0.489 (0.476) data 0.357 (0.345) loss_x loss_x 0.9160 (1.3916) acc_x 78.1250 (66.8750) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [10/28] time 0.423 (0.464) data 0.293 (0.333) loss_x loss_x 0.7803 (1.3062) acc_x 81.2500 (69.3750) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [15/28] time 0.461 (0.450) data 0.330 (0.319) loss_x loss_x 0.9551 (1.2624) acc_x 68.7500 (68.9583) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [20/28] time 0.911 (0.494) data 0.780 (0.364) loss_x loss_x 1.7471 (1.2475) acc_x 62.5000 (68.9062) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [25/28] time 0.372 (0.479) data 0.242 (0.348) loss_x loss_x 1.1357 (1.2473) acc_x 75.0000 (69.0000) lr 2.0032e-04 eta 0:00:01
epoch [161/200] batch [5/69] time 0.355 (0.467) data 0.224 (0.336) loss_u loss_u 0.9116 (0.8895) acc_u 12.5000 (13.7500) lr 2.0032e-04 eta 0:00:29
epoch [161/200] batch [10/69] time 0.378 (0.459) data 0.247 (0.328) loss_u loss_u 0.9160 (0.8953) acc_u 12.5000 (13.1250) lr 2.0032e-04 eta 0:00:27
epoch [161/200] batch [15/69] time 0.507 (0.458) data 0.375 (0.327) loss_u loss_u 0.8271 (0.8895) acc_u 21.8750 (13.5417) lr 2.0032e-04 eta 0:00:24
epoch [161/200] batch [20/69] time 0.398 (0.451) data 0.266 (0.320) loss_u loss_u 0.9346 (0.8866) acc_u 9.3750 (14.2188) lr 2.0032e-04 eta 0:00:22
epoch [161/200] batch [25/69] time 0.349 (0.444) data 0.218 (0.313) loss_u loss_u 0.9009 (0.8841) acc_u 15.6250 (14.5000) lr 2.0032e-04 eta 0:00:19
epoch [161/200] batch [30/69] time 0.418 (0.445) data 0.287 (0.314) loss_u loss_u 0.9399 (0.8923) acc_u 6.2500 (13.4375) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [35/69] time 0.490 (0.447) data 0.359 (0.316) loss_u loss_u 0.8818 (0.8881) acc_u 15.6250 (14.1964) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [40/69] time 0.582 (0.445) data 0.451 (0.314) loss_u loss_u 0.9160 (0.8888) acc_u 9.3750 (13.9062) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [45/69] time 0.323 (0.444) data 0.191 (0.312) loss_u loss_u 0.9170 (0.8867) acc_u 12.5000 (14.1667) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [50/69] time 0.397 (0.446) data 0.265 (0.314) loss_u loss_u 0.9229 (0.8894) acc_u 9.3750 (13.7500) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [55/69] time 0.403 (0.444) data 0.273 (0.313) loss_u loss_u 0.8687 (0.8880) acc_u 18.7500 (14.0909) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [60/69] time 0.433 (0.445) data 0.301 (0.314) loss_u loss_u 0.9189 (0.8892) acc_u 6.2500 (13.9062) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [65/69] time 0.373 (0.444) data 0.242 (0.313) loss_u loss_u 0.8677 (0.8885) acc_u 15.6250 (13.9904) lr 2.0032e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1753
confident_label rate tensor(0.2918, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 915
clean true:656
clean false:259
clean_rate:0.7169398907103826
noisy true:727
noisy false:1494
after delete: len(clean_dataset) 915
after delete: len(noisy_dataset) 2221
epoch [162/200] batch [5/28] time 0.416 (0.443) data 0.285 (0.312) loss_x loss_x 1.4424 (1.2119) acc_x 65.6250 (71.8750) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [10/28] time 0.435 (0.440) data 0.304 (0.309) loss_x loss_x 0.8965 (1.1474) acc_x 84.3750 (72.8125) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [15/28] time 0.396 (0.431) data 0.266 (0.301) loss_x loss_x 0.9839 (1.1394) acc_x 75.0000 (72.7083) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [20/28] time 0.472 (0.437) data 0.342 (0.306) loss_x loss_x 1.3887 (1.1647) acc_x 68.7500 (72.0312) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [25/28] time 0.446 (0.440) data 0.315 (0.310) loss_x loss_x 1.0547 (1.1936) acc_x 81.2500 (72.2500) lr 1.9098e-04 eta 0:00:01
epoch [162/200] batch [5/69] time 0.337 (0.438) data 0.206 (0.308) loss_u loss_u 0.8179 (0.8634) acc_u 21.8750 (18.1250) lr 1.9098e-04 eta 0:00:28
epoch [162/200] batch [10/69] time 0.386 (0.440) data 0.255 (0.309) loss_u loss_u 0.9375 (0.8854) acc_u 9.3750 (15.0000) lr 1.9098e-04 eta 0:00:25
epoch [162/200] batch [15/69] time 0.446 (0.439) data 0.315 (0.308) loss_u loss_u 0.9258 (0.8834) acc_u 6.2500 (14.3750) lr 1.9098e-04 eta 0:00:23
epoch [162/200] batch [20/69] time 0.535 (0.440) data 0.401 (0.309) loss_u loss_u 0.9800 (0.8931) acc_u 3.1250 (13.5938) lr 1.9098e-04 eta 0:00:21
epoch [162/200] batch [25/69] time 0.369 (0.448) data 0.238 (0.317) loss_u loss_u 0.9380 (0.8939) acc_u 3.1250 (13.2500) lr 1.9098e-04 eta 0:00:19
epoch [162/200] batch [30/69] time 0.421 (0.450) data 0.289 (0.319) loss_u loss_u 0.8228 (0.8878) acc_u 28.1250 (14.0625) lr 1.9098e-04 eta 0:00:17
epoch [162/200] batch [35/69] time 0.492 (0.451) data 0.361 (0.320) loss_u loss_u 0.8486 (0.8866) acc_u 18.7500 (14.2857) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [40/69] time 0.447 (0.449) data 0.316 (0.318) loss_u loss_u 0.8003 (0.8847) acc_u 25.0000 (14.4531) lr 1.9098e-04 eta 0:00:13
epoch [162/200] batch [45/69] time 0.431 (0.446) data 0.300 (0.315) loss_u loss_u 0.8921 (0.8849) acc_u 15.6250 (14.5139) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [50/69] time 0.345 (0.445) data 0.214 (0.314) loss_u loss_u 0.8931 (0.8878) acc_u 15.6250 (14.1875) lr 1.9098e-04 eta 0:00:08
epoch [162/200] batch [55/69] time 0.425 (0.443) data 0.293 (0.312) loss_u loss_u 0.9185 (0.8897) acc_u 9.3750 (13.9773) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [60/69] time 0.393 (0.444) data 0.262 (0.313) loss_u loss_u 0.9229 (0.8904) acc_u 6.2500 (13.8542) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [65/69] time 0.403 (0.443) data 0.272 (0.312) loss_u loss_u 0.8721 (0.8896) acc_u 15.6250 (13.7981) lr 1.9098e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1732
confident_label rate tensor(0.2988, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 937
clean true:672
clean false:265
clean_rate:0.7171824973319103
noisy true:732
noisy false:1467
after delete: len(clean_dataset) 937
after delete: len(noisy_dataset) 2199
epoch [163/200] batch [5/29] time 0.482 (0.531) data 0.351 (0.399) loss_x loss_x 0.8130 (1.1474) acc_x 68.7500 (69.3750) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [10/29] time 0.453 (0.498) data 0.321 (0.366) loss_x loss_x 1.4121 (1.2217) acc_x 68.7500 (69.3750) lr 1.8185e-04 eta 0:00:09
epoch [163/200] batch [15/29] time 0.555 (0.492) data 0.423 (0.360) loss_x loss_x 1.4092 (1.2021) acc_x 62.5000 (69.1667) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [20/29] time 0.392 (0.469) data 0.261 (0.337) loss_x loss_x 1.3457 (1.2221) acc_x 62.5000 (69.6875) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [25/29] time 0.494 (0.479) data 0.362 (0.347) loss_x loss_x 0.9355 (1.2260) acc_x 75.0000 (69.7500) lr 1.8185e-04 eta 0:00:01
epoch [163/200] batch [5/68] time 0.391 (0.469) data 0.261 (0.337) loss_u loss_u 0.9072 (0.8603) acc_u 9.3750 (17.5000) lr 1.8185e-04 eta 0:00:29
epoch [163/200] batch [10/68] time 0.381 (0.460) data 0.249 (0.329) loss_u loss_u 0.8120 (0.8795) acc_u 25.0000 (14.3750) lr 1.8185e-04 eta 0:00:26
epoch [163/200] batch [15/68] time 0.528 (0.462) data 0.396 (0.331) loss_u loss_u 0.7935 (0.8705) acc_u 25.0000 (16.0417) lr 1.8185e-04 eta 0:00:24
epoch [163/200] batch [20/68] time 0.484 (0.457) data 0.351 (0.326) loss_u loss_u 0.9688 (0.8837) acc_u 6.2500 (14.3750) lr 1.8185e-04 eta 0:00:21
epoch [163/200] batch [25/68] time 0.503 (0.464) data 0.371 (0.332) loss_u loss_u 0.9600 (0.8895) acc_u 0.0000 (13.8750) lr 1.8185e-04 eta 0:00:19
epoch [163/200] batch [30/68] time 0.510 (0.463) data 0.379 (0.331) loss_u loss_u 0.9014 (0.8883) acc_u 9.3750 (13.7500) lr 1.8185e-04 eta 0:00:17
epoch [163/200] batch [35/68] time 0.354 (0.460) data 0.222 (0.328) loss_u loss_u 0.9390 (0.8885) acc_u 6.2500 (14.1071) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [40/68] time 0.429 (0.460) data 0.298 (0.329) loss_u loss_u 0.9023 (0.8914) acc_u 9.3750 (13.5938) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [45/68] time 0.421 (0.463) data 0.290 (0.332) loss_u loss_u 0.8687 (0.8914) acc_u 18.7500 (13.6111) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [50/68] time 0.470 (0.463) data 0.339 (0.331) loss_u loss_u 0.8564 (0.8907) acc_u 15.6250 (13.7500) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [55/68] time 0.396 (0.458) data 0.265 (0.327) loss_u loss_u 0.9067 (0.8917) acc_u 12.5000 (13.6932) lr 1.8185e-04 eta 0:00:05
epoch [163/200] batch [60/68] time 0.578 (0.459) data 0.448 (0.327) loss_u loss_u 0.9258 (0.8939) acc_u 12.5000 (13.3854) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [65/68] time 0.377 (0.457) data 0.246 (0.325) loss_u loss_u 0.8252 (0.8949) acc_u 18.7500 (13.1250) lr 1.8185e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1713
confident_label rate tensor(0.2870, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 900
clean true:659
clean false:241
clean_rate:0.7322222222222222
noisy true:764
noisy false:1472
after delete: len(clean_dataset) 900
after delete: len(noisy_dataset) 2236
epoch [164/200] batch [5/28] time 0.389 (0.441) data 0.259 (0.311) loss_x loss_x 1.1406 (0.9930) acc_x 68.7500 (76.2500) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [10/28] time 0.543 (0.458) data 0.413 (0.328) loss_x loss_x 1.6113 (1.1333) acc_x 65.6250 (72.5000) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [15/28] time 0.347 (0.430) data 0.217 (0.300) loss_x loss_x 1.5479 (1.1603) acc_x 65.6250 (72.5000) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [20/28] time 0.374 (0.441) data 0.244 (0.311) loss_x loss_x 1.2793 (1.1843) acc_x 71.8750 (71.0938) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [25/28] time 0.555 (0.460) data 0.425 (0.330) loss_x loss_x 1.1035 (1.1724) acc_x 71.8750 (71.3750) lr 1.7292e-04 eta 0:00:01
epoch [164/200] batch [5/69] time 0.433 (0.453) data 0.302 (0.323) loss_u loss_u 0.9277 (0.8939) acc_u 6.2500 (13.7500) lr 1.7292e-04 eta 0:00:29
epoch [164/200] batch [10/69] time 0.510 (0.452) data 0.379 (0.322) loss_u loss_u 0.9517 (0.8876) acc_u 0.0000 (14.3750) lr 1.7292e-04 eta 0:00:26
epoch [164/200] batch [15/69] time 0.417 (0.457) data 0.286 (0.326) loss_u loss_u 0.8569 (0.8809) acc_u 18.7500 (15.0000) lr 1.7292e-04 eta 0:00:24
epoch [164/200] batch [20/69] time 0.365 (0.454) data 0.233 (0.323) loss_u loss_u 0.8828 (0.8815) acc_u 12.5000 (14.6875) lr 1.7292e-04 eta 0:00:22
epoch [164/200] batch [25/69] time 0.528 (0.457) data 0.396 (0.326) loss_u loss_u 0.9116 (0.8895) acc_u 6.2500 (13.5000) lr 1.7292e-04 eta 0:00:20
epoch [164/200] batch [30/69] time 0.393 (0.458) data 0.263 (0.327) loss_u loss_u 0.8784 (0.8894) acc_u 18.7500 (13.6458) lr 1.7292e-04 eta 0:00:17
epoch [164/200] batch [35/69] time 0.436 (0.456) data 0.304 (0.325) loss_u loss_u 0.7993 (0.8879) acc_u 28.1250 (13.9286) lr 1.7292e-04 eta 0:00:15
epoch [164/200] batch [40/69] time 0.517 (0.455) data 0.387 (0.324) loss_u loss_u 0.8555 (0.8858) acc_u 12.5000 (14.0625) lr 1.7292e-04 eta 0:00:13
epoch [164/200] batch [45/69] time 0.410 (0.454) data 0.279 (0.323) loss_u loss_u 0.8472 (0.8834) acc_u 18.7500 (14.3056) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [50/69] time 0.439 (0.454) data 0.308 (0.323) loss_u loss_u 0.8965 (0.8834) acc_u 12.5000 (14.4375) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [55/69] time 0.479 (0.450) data 0.349 (0.319) loss_u loss_u 0.9067 (0.8855) acc_u 15.6250 (14.3182) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [60/69] time 0.459 (0.448) data 0.327 (0.317) loss_u loss_u 0.8564 (0.8852) acc_u 12.5000 (14.2188) lr 1.7292e-04 eta 0:00:04
epoch [164/200] batch [65/69] time 0.393 (0.446) data 0.262 (0.315) loss_u loss_u 0.9502 (0.8860) acc_u 9.3750 (14.1827) lr 1.7292e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1755
confident_label rate tensor(0.2825, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 886
clean true:630
clean false:256
clean_rate:0.7110609480812641
noisy true:751
noisy false:1499
after delete: len(clean_dataset) 886
after delete: len(noisy_dataset) 2250
epoch [165/200] batch [5/27] time 0.573 (0.480) data 0.442 (0.350) loss_x loss_x 0.9243 (1.1150) acc_x 71.8750 (73.1250) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [10/27] time 0.426 (0.446) data 0.296 (0.316) loss_x loss_x 1.0908 (1.1010) acc_x 65.6250 (72.5000) lr 1.6419e-04 eta 0:00:07
epoch [165/200] batch [15/27] time 0.500 (0.455) data 0.370 (0.325) loss_x loss_x 0.9258 (0.9848) acc_x 81.2500 (76.0417) lr 1.6419e-04 eta 0:00:05
epoch [165/200] batch [20/27] time 0.556 (0.452) data 0.426 (0.322) loss_x loss_x 0.9531 (1.0177) acc_x 78.1250 (75.6250) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [25/27] time 0.456 (0.443) data 0.326 (0.313) loss_x loss_x 1.1445 (1.0701) acc_x 68.7500 (74.1250) lr 1.6419e-04 eta 0:00:00
epoch [165/200] batch [5/70] time 0.426 (0.443) data 0.295 (0.312) loss_u loss_u 0.8838 (0.8521) acc_u 15.6250 (18.1250) lr 1.6419e-04 eta 0:00:28
epoch [165/200] batch [10/70] time 0.328 (0.439) data 0.197 (0.308) loss_u loss_u 0.9185 (0.8642) acc_u 9.3750 (17.1875) lr 1.6419e-04 eta 0:00:26
epoch [165/200] batch [15/70] time 0.423 (0.436) data 0.292 (0.306) loss_u loss_u 0.8784 (0.8805) acc_u 12.5000 (15.4167) lr 1.6419e-04 eta 0:00:23
epoch [165/200] batch [20/70] time 0.387 (0.435) data 0.255 (0.304) loss_u loss_u 0.8877 (0.8826) acc_u 12.5000 (15.0000) lr 1.6419e-04 eta 0:00:21
epoch [165/200] batch [25/70] time 0.364 (0.436) data 0.233 (0.305) loss_u loss_u 0.8911 (0.8815) acc_u 15.6250 (15.1250) lr 1.6419e-04 eta 0:00:19
epoch [165/200] batch [30/70] time 0.429 (0.438) data 0.298 (0.307) loss_u loss_u 0.9023 (0.8762) acc_u 12.5000 (16.0417) lr 1.6419e-04 eta 0:00:17
epoch [165/200] batch [35/70] time 0.376 (0.440) data 0.244 (0.309) loss_u loss_u 0.8994 (0.8755) acc_u 12.5000 (16.4286) lr 1.6419e-04 eta 0:00:15
epoch [165/200] batch [40/70] time 0.376 (0.441) data 0.243 (0.310) loss_u loss_u 0.8833 (0.8773) acc_u 12.5000 (15.8594) lr 1.6419e-04 eta 0:00:13
epoch [165/200] batch [45/70] time 0.433 (0.440) data 0.301 (0.310) loss_u loss_u 0.8882 (0.8820) acc_u 9.3750 (15.0694) lr 1.6419e-04 eta 0:00:11
epoch [165/200] batch [50/70] time 0.495 (0.444) data 0.365 (0.314) loss_u loss_u 0.8086 (0.8762) acc_u 25.0000 (15.6250) lr 1.6419e-04 eta 0:00:08
epoch [165/200] batch [55/70] time 0.464 (0.442) data 0.334 (0.312) loss_u loss_u 0.8911 (0.8759) acc_u 12.5000 (15.7386) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [60/70] time 0.371 (0.442) data 0.240 (0.311) loss_u loss_u 0.8921 (0.8767) acc_u 15.6250 (15.5729) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [65/70] time 0.428 (0.443) data 0.297 (0.312) loss_u loss_u 0.9126 (0.8784) acc_u 9.3750 (15.1923) lr 1.6419e-04 eta 0:00:02
epoch [165/200] batch [70/70] time 0.419 (0.442) data 0.288 (0.312) loss_u loss_u 0.8569 (0.8810) acc_u 18.7500 (14.8214) lr 1.6419e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1808
confident_label rate tensor(0.2892, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 907
clean true:642
clean false:265
clean_rate:0.7078280044101434
noisy true:686
noisy false:1543
after delete: len(clean_dataset) 907
after delete: len(noisy_dataset) 2229
epoch [166/200] batch [5/28] time 0.531 (0.448) data 0.400 (0.316) loss_x loss_x 0.8711 (1.0817) acc_x 68.7500 (71.2500) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [10/28] time 0.373 (0.523) data 0.241 (0.391) loss_x loss_x 1.0938 (1.1257) acc_x 71.8750 (71.8750) lr 1.5567e-04 eta 0:00:09
epoch [166/200] batch [15/28] time 0.578 (0.500) data 0.447 (0.368) loss_x loss_x 1.2324 (1.1385) acc_x 62.5000 (70.6250) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [20/28] time 0.384 (0.481) data 0.252 (0.350) loss_x loss_x 1.1328 (1.1422) acc_x 68.7500 (71.2500) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [25/28] time 0.326 (0.468) data 0.195 (0.337) loss_x loss_x 1.6396 (1.1570) acc_x 65.6250 (71.0000) lr 1.5567e-04 eta 0:00:01
epoch [166/200] batch [5/69] time 0.324 (0.463) data 0.192 (0.332) loss_u loss_u 0.9023 (0.8960) acc_u 9.3750 (11.2500) lr 1.5567e-04 eta 0:00:29
epoch [166/200] batch [10/69] time 0.425 (0.459) data 0.293 (0.328) loss_u loss_u 0.8921 (0.8935) acc_u 15.6250 (12.5000) lr 1.5567e-04 eta 0:00:27
epoch [166/200] batch [15/69] time 0.426 (0.455) data 0.295 (0.324) loss_u loss_u 0.9443 (0.8880) acc_u 6.2500 (13.1250) lr 1.5567e-04 eta 0:00:24
epoch [166/200] batch [20/69] time 0.480 (0.459) data 0.350 (0.328) loss_u loss_u 0.8677 (0.8911) acc_u 15.6250 (12.6562) lr 1.5567e-04 eta 0:00:22
epoch [166/200] batch [25/69] time 0.690 (0.461) data 0.558 (0.330) loss_u loss_u 0.8374 (0.8888) acc_u 21.8750 (13.2500) lr 1.5567e-04 eta 0:00:20
epoch [166/200] batch [30/69] time 0.630 (0.464) data 0.498 (0.332) loss_u loss_u 0.8691 (0.8956) acc_u 18.7500 (12.6042) lr 1.5567e-04 eta 0:00:18
epoch [166/200] batch [35/69] time 0.508 (0.469) data 0.376 (0.338) loss_u loss_u 0.8740 (0.8931) acc_u 15.6250 (13.0357) lr 1.5567e-04 eta 0:00:15
epoch [166/200] batch [40/69] time 0.485 (0.466) data 0.353 (0.335) loss_u loss_u 0.9033 (0.8935) acc_u 9.3750 (12.8125) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [45/69] time 0.406 (0.466) data 0.270 (0.334) loss_u loss_u 0.8662 (0.8946) acc_u 18.7500 (12.6389) lr 1.5567e-04 eta 0:00:11
epoch [166/200] batch [50/69] time 0.473 (0.467) data 0.340 (0.335) loss_u loss_u 0.9556 (0.8938) acc_u 6.2500 (12.9375) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [55/69] time 0.480 (0.469) data 0.348 (0.337) loss_u loss_u 0.9360 (0.8945) acc_u 9.3750 (12.6136) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [60/69] time 0.535 (0.469) data 0.403 (0.337) loss_u loss_u 0.8794 (0.8927) acc_u 18.7500 (12.9167) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [65/69] time 0.494 (0.470) data 0.362 (0.338) loss_u loss_u 0.9033 (0.8911) acc_u 12.5000 (13.2212) lr 1.5567e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1728
confident_label rate tensor(0.2812, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 882
clean true:642
clean false:240
clean_rate:0.7278911564625851
noisy true:766
noisy false:1488
after delete: len(clean_dataset) 882
after delete: len(noisy_dataset) 2254
epoch [167/200] batch [5/27] time 0.423 (0.463) data 0.292 (0.333) loss_x loss_x 1.0479 (1.0438) acc_x 75.0000 (79.3750) lr 1.4736e-04 eta 0:00:10
epoch [167/200] batch [10/27] time 0.324 (0.474) data 0.194 (0.344) loss_x loss_x 0.7612 (1.1996) acc_x 78.1250 (74.6875) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [15/27] time 0.397 (0.458) data 0.266 (0.327) loss_x loss_x 1.4590 (1.2855) acc_x 68.7500 (71.6667) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [20/27] time 0.422 (0.453) data 0.291 (0.322) loss_x loss_x 1.2217 (1.2176) acc_x 81.2500 (72.6562) lr 1.4736e-04 eta 0:00:03
epoch [167/200] batch [25/27] time 0.455 (0.457) data 0.324 (0.326) loss_x loss_x 1.2217 (1.2090) acc_x 68.7500 (72.5000) lr 1.4736e-04 eta 0:00:00
epoch [167/200] batch [5/70] time 0.465 (0.461) data 0.334 (0.330) loss_u loss_u 0.8950 (0.8837) acc_u 15.6250 (14.3750) lr 1.4736e-04 eta 0:00:29
epoch [167/200] batch [10/70] time 0.401 (0.456) data 0.269 (0.325) loss_u loss_u 0.9146 (0.8930) acc_u 12.5000 (12.8125) lr 1.4736e-04 eta 0:00:27
epoch [167/200] batch [15/70] time 0.432 (0.453) data 0.300 (0.322) loss_u loss_u 0.9458 (0.8930) acc_u 6.2500 (12.2917) lr 1.4736e-04 eta 0:00:24
epoch [167/200] batch [20/70] time 0.419 (0.447) data 0.287 (0.316) loss_u loss_u 0.8491 (0.8919) acc_u 21.8750 (12.9688) lr 1.4736e-04 eta 0:00:22
epoch [167/200] batch [25/70] time 0.376 (0.449) data 0.244 (0.318) loss_u loss_u 0.8940 (0.8951) acc_u 15.6250 (12.6250) lr 1.4736e-04 eta 0:00:20
epoch [167/200] batch [30/70] time 0.405 (0.446) data 0.273 (0.315) loss_u loss_u 0.9370 (0.8931) acc_u 12.5000 (13.3333) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [35/70] time 0.375 (0.449) data 0.243 (0.318) loss_u loss_u 0.9014 (0.8900) acc_u 9.3750 (13.7500) lr 1.4736e-04 eta 0:00:15
epoch [167/200] batch [40/70] time 0.335 (0.446) data 0.204 (0.315) loss_u loss_u 0.8818 (0.8861) acc_u 9.3750 (14.2188) lr 1.4736e-04 eta 0:00:13
epoch [167/200] batch [45/70] time 0.380 (0.443) data 0.248 (0.312) loss_u loss_u 0.8926 (0.8826) acc_u 9.3750 (14.5833) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [50/70] time 0.458 (0.445) data 0.327 (0.314) loss_u loss_u 0.9087 (0.8836) acc_u 12.5000 (14.3750) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [55/70] time 0.354 (0.444) data 0.222 (0.313) loss_u loss_u 0.8359 (0.8830) acc_u 25.0000 (14.4886) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [60/70] time 0.518 (0.450) data 0.386 (0.319) loss_u loss_u 0.7822 (0.8817) acc_u 31.2500 (14.5833) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [65/70] time 0.327 (0.448) data 0.195 (0.317) loss_u loss_u 0.9067 (0.8830) acc_u 12.5000 (14.4231) lr 1.4736e-04 eta 0:00:02
epoch [167/200] batch [70/70] time 0.428 (0.446) data 0.297 (0.314) loss_u loss_u 0.7925 (0.8825) acc_u 25.0000 (14.5982) lr 1.4736e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1757
confident_label rate tensor(0.2870, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 900
clean true:654
clean false:246
clean_rate:0.7266666666666667
noisy true:725
noisy false:1511
after delete: len(clean_dataset) 900
after delete: len(noisy_dataset) 2236
epoch [168/200] batch [5/28] time 0.475 (0.445) data 0.344 (0.314) loss_x loss_x 1.3496 (1.2301) acc_x 68.7500 (70.6250) lr 1.3926e-04 eta 0:00:10
epoch [168/200] batch [10/28] time 0.490 (0.445) data 0.359 (0.314) loss_x loss_x 1.6641 (1.2404) acc_x 62.5000 (71.2500) lr 1.3926e-04 eta 0:00:08
epoch [168/200] batch [15/28] time 0.483 (0.473) data 0.352 (0.343) loss_x loss_x 1.4707 (1.2308) acc_x 65.6250 (71.2500) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [20/28] time 0.447 (0.462) data 0.316 (0.331) loss_x loss_x 1.2344 (1.1993) acc_x 56.2500 (70.7812) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [25/28] time 0.578 (0.467) data 0.448 (0.336) loss_x loss_x 1.3018 (1.2275) acc_x 75.0000 (70.2500) lr 1.3926e-04 eta 0:00:01
epoch [168/200] batch [5/69] time 0.558 (0.482) data 0.426 (0.351) loss_u loss_u 0.8999 (0.8779) acc_u 9.3750 (11.8750) lr 1.3926e-04 eta 0:00:30
epoch [168/200] batch [10/69] time 0.348 (0.470) data 0.217 (0.339) loss_u loss_u 0.8604 (0.8779) acc_u 18.7500 (14.0625) lr 1.3926e-04 eta 0:00:27
epoch [168/200] batch [15/69] time 0.392 (0.461) data 0.260 (0.330) loss_u loss_u 0.8794 (0.8859) acc_u 21.8750 (14.1667) lr 1.3926e-04 eta 0:00:24
epoch [168/200] batch [20/69] time 0.465 (0.458) data 0.333 (0.327) loss_u loss_u 0.9263 (0.8879) acc_u 12.5000 (13.9062) lr 1.3926e-04 eta 0:00:22
epoch [168/200] batch [25/69] time 0.474 (0.457) data 0.343 (0.326) loss_u loss_u 0.8789 (0.8871) acc_u 9.3750 (13.7500) lr 1.3926e-04 eta 0:00:20
epoch [168/200] batch [30/69] time 0.332 (0.454) data 0.200 (0.323) loss_u loss_u 0.8862 (0.8857) acc_u 15.6250 (13.9583) lr 1.3926e-04 eta 0:00:17
epoch [168/200] batch [35/69] time 0.504 (0.455) data 0.372 (0.324) loss_u loss_u 0.9062 (0.8832) acc_u 12.5000 (14.3750) lr 1.3926e-04 eta 0:00:15
epoch [168/200] batch [40/69] time 0.461 (0.454) data 0.330 (0.323) loss_u loss_u 0.9473 (0.8843) acc_u 9.3750 (14.5312) lr 1.3926e-04 eta 0:00:13
epoch [168/200] batch [45/69] time 0.360 (0.451) data 0.229 (0.320) loss_u loss_u 0.8218 (0.8842) acc_u 15.6250 (14.4444) lr 1.3926e-04 eta 0:00:10
epoch [168/200] batch [50/69] time 0.420 (0.453) data 0.289 (0.322) loss_u loss_u 0.8774 (0.8849) acc_u 15.6250 (14.4375) lr 1.3926e-04 eta 0:00:08
epoch [168/200] batch [55/69] time 0.367 (0.448) data 0.235 (0.317) loss_u loss_u 0.9263 (0.8856) acc_u 15.6250 (14.6023) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [60/69] time 0.420 (0.453) data 0.288 (0.322) loss_u loss_u 0.8481 (0.8852) acc_u 18.7500 (14.4792) lr 1.3926e-04 eta 0:00:04
epoch [168/200] batch [65/69] time 0.478 (0.451) data 0.347 (0.320) loss_u loss_u 0.8794 (0.8858) acc_u 12.5000 (14.2788) lr 1.3926e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.2860, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 897
clean true:646
clean false:251
clean_rate:0.7201783723522854
noisy true:741
noisy false:1498
after delete: len(clean_dataset) 897
after delete: len(noisy_dataset) 2239
epoch [169/200] batch [5/28] time 0.676 (0.452) data 0.545 (0.321) loss_x loss_x 0.9614 (0.9121) acc_x 75.0000 (78.1250) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [10/28] time 0.300 (0.462) data 0.169 (0.331) loss_x loss_x 1.3916 (1.0951) acc_x 65.6250 (75.0000) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [15/28] time 0.476 (0.463) data 0.346 (0.332) loss_x loss_x 0.9673 (1.0440) acc_x 78.1250 (75.4167) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [20/28] time 0.401 (0.446) data 0.271 (0.315) loss_x loss_x 1.4639 (1.0284) acc_x 62.5000 (74.8438) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [25/28] time 0.484 (0.444) data 0.353 (0.314) loss_x loss_x 1.4766 (1.0404) acc_x 62.5000 (74.0000) lr 1.3137e-04 eta 0:00:01
epoch [169/200] batch [5/69] time 0.361 (0.434) data 0.229 (0.304) loss_u loss_u 0.9072 (0.9249) acc_u 9.3750 (9.3750) lr 1.3137e-04 eta 0:00:27
epoch [169/200] batch [10/69] time 0.466 (0.446) data 0.332 (0.316) loss_u loss_u 0.9253 (0.9129) acc_u 9.3750 (10.9375) lr 1.3137e-04 eta 0:00:26
epoch [169/200] batch [15/69] time 0.430 (0.452) data 0.298 (0.321) loss_u loss_u 0.9097 (0.9063) acc_u 9.3750 (10.0000) lr 1.3137e-04 eta 0:00:24
epoch [169/200] batch [20/69] time 0.448 (0.446) data 0.317 (0.315) loss_u loss_u 0.8882 (0.8972) acc_u 15.6250 (11.8750) lr 1.3137e-04 eta 0:00:21
epoch [169/200] batch [25/69] time 0.498 (0.446) data 0.367 (0.315) loss_u loss_u 0.8452 (0.8909) acc_u 21.8750 (13.0000) lr 1.3137e-04 eta 0:00:19
epoch [169/200] batch [30/69] time 0.460 (0.445) data 0.329 (0.314) loss_u loss_u 0.8735 (0.8944) acc_u 18.7500 (12.6042) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [35/69] time 0.384 (0.440) data 0.254 (0.310) loss_u loss_u 0.8496 (0.8911) acc_u 15.6250 (13.0357) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [40/69] time 0.383 (0.439) data 0.251 (0.308) loss_u loss_u 0.9575 (0.8914) acc_u 6.2500 (13.0469) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [45/69] time 0.382 (0.437) data 0.252 (0.306) loss_u loss_u 0.8193 (0.8869) acc_u 21.8750 (13.6111) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [50/69] time 0.536 (0.445) data 0.405 (0.314) loss_u loss_u 0.8911 (0.8850) acc_u 12.5000 (13.9375) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [55/69] time 0.479 (0.444) data 0.348 (0.313) loss_u loss_u 0.9331 (0.8861) acc_u 6.2500 (13.8636) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [60/69] time 0.467 (0.441) data 0.336 (0.310) loss_u loss_u 0.8701 (0.8862) acc_u 12.5000 (13.7500) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [65/69] time 0.378 (0.440) data 0.245 (0.309) loss_u loss_u 0.9272 (0.8885) acc_u 12.5000 (13.5577) lr 1.3137e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1726
confident_label rate tensor(0.2908, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 912
clean true:666
clean false:246
clean_rate:0.7302631578947368
noisy true:744
noisy false:1480
after delete: len(clean_dataset) 912
after delete: len(noisy_dataset) 2224
epoch [170/200] batch [5/28] time 0.474 (0.483) data 0.344 (0.353) loss_x loss_x 1.1006 (1.3536) acc_x 59.3750 (60.6250) lr 1.2369e-04 eta 0:00:11
epoch [170/200] batch [10/28] time 0.422 (0.484) data 0.291 (0.354) loss_x loss_x 0.7300 (1.1344) acc_x 87.5000 (68.7500) lr 1.2369e-04 eta 0:00:08
epoch [170/200] batch [15/28] time 0.426 (0.465) data 0.296 (0.335) loss_x loss_x 1.1650 (1.1245) acc_x 71.8750 (70.0000) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [20/28] time 0.486 (0.473) data 0.356 (0.342) loss_x loss_x 0.9761 (1.1236) acc_x 78.1250 (70.4688) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [25/28] time 0.428 (0.459) data 0.297 (0.328) loss_x loss_x 1.1660 (1.1397) acc_x 62.5000 (70.0000) lr 1.2369e-04 eta 0:00:01
epoch [170/200] batch [5/69] time 0.412 (0.455) data 0.283 (0.324) loss_u loss_u 0.9038 (0.8676) acc_u 12.5000 (19.3750) lr 1.2369e-04 eta 0:00:29
epoch [170/200] batch [10/69] time 0.389 (0.455) data 0.258 (0.325) loss_u loss_u 0.8521 (0.8694) acc_u 18.7500 (18.4375) lr 1.2369e-04 eta 0:00:26
epoch [170/200] batch [15/69] time 0.473 (0.450) data 0.342 (0.319) loss_u loss_u 0.9062 (0.8770) acc_u 12.5000 (16.6667) lr 1.2369e-04 eta 0:00:24
epoch [170/200] batch [20/69] time 0.532 (0.450) data 0.401 (0.320) loss_u loss_u 0.8325 (0.8774) acc_u 18.7500 (16.0938) lr 1.2369e-04 eta 0:00:22
epoch [170/200] batch [25/69] time 0.469 (0.447) data 0.337 (0.316) loss_u loss_u 0.9224 (0.8780) acc_u 9.3750 (15.6250) lr 1.2369e-04 eta 0:00:19
epoch [170/200] batch [30/69] time 0.621 (0.452) data 0.490 (0.321) loss_u loss_u 0.7964 (0.8762) acc_u 25.0000 (15.9375) lr 1.2369e-04 eta 0:00:17
epoch [170/200] batch [35/69] time 0.430 (0.449) data 0.299 (0.318) loss_u loss_u 0.9038 (0.8794) acc_u 9.3750 (15.3571) lr 1.2369e-04 eta 0:00:15
epoch [170/200] batch [40/69] time 0.383 (0.447) data 0.252 (0.316) loss_u loss_u 0.8252 (0.8813) acc_u 21.8750 (15.0000) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [45/69] time 0.399 (0.447) data 0.267 (0.316) loss_u loss_u 0.8579 (0.8809) acc_u 25.0000 (14.9306) lr 1.2369e-04 eta 0:00:10
epoch [170/200] batch [50/69] time 0.423 (0.445) data 0.292 (0.314) loss_u loss_u 0.8550 (0.8836) acc_u 18.7500 (14.6250) lr 1.2369e-04 eta 0:00:08
epoch [170/200] batch [55/69] time 0.581 (0.447) data 0.450 (0.316) loss_u loss_u 0.8701 (0.8853) acc_u 15.6250 (14.5455) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [60/69] time 0.447 (0.447) data 0.315 (0.316) loss_u loss_u 0.8560 (0.8851) acc_u 18.7500 (14.5312) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [65/69] time 0.386 (0.445) data 0.255 (0.314) loss_u loss_u 0.8882 (0.8855) acc_u 18.7500 (14.6154) lr 1.2369e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1768
confident_label rate tensor(0.2879, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 903
clean true:665
clean false:238
clean_rate:0.7364341085271318
noisy true:703
noisy false:1530
after delete: len(clean_dataset) 903
after delete: len(noisy_dataset) 2233
epoch [171/200] batch [5/28] time 0.388 (0.426) data 0.257 (0.295) loss_x loss_x 1.0020 (0.9131) acc_x 68.7500 (77.5000) lr 1.1623e-04 eta 0:00:09
epoch [171/200] batch [10/28] time 0.417 (0.438) data 0.287 (0.308) loss_x loss_x 1.1162 (1.0263) acc_x 71.8750 (76.5625) lr 1.1623e-04 eta 0:00:07
epoch [171/200] batch [15/28] time 0.360 (0.419) data 0.230 (0.289) loss_x loss_x 1.2861 (1.0904) acc_x 68.7500 (73.1250) lr 1.1623e-04 eta 0:00:05
epoch [171/200] batch [20/28] time 0.528 (0.432) data 0.397 (0.301) loss_x loss_x 0.9404 (1.0590) acc_x 75.0000 (73.2812) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [25/28] time 0.433 (0.437) data 0.302 (0.307) loss_x loss_x 1.0176 (1.0808) acc_x 75.0000 (73.0000) lr 1.1623e-04 eta 0:00:01
epoch [171/200] batch [5/69] time 0.358 (0.438) data 0.226 (0.307) loss_u loss_u 0.9023 (0.8910) acc_u 15.6250 (13.1250) lr 1.1623e-04 eta 0:00:28
epoch [171/200] batch [10/69] time 0.458 (0.437) data 0.327 (0.306) loss_u loss_u 0.9443 (0.8947) acc_u 6.2500 (13.1250) lr 1.1623e-04 eta 0:00:25
epoch [171/200] batch [15/69] time 0.500 (0.435) data 0.369 (0.304) loss_u loss_u 0.9458 (0.8991) acc_u 9.3750 (13.1250) lr 1.1623e-04 eta 0:00:23
epoch [171/200] batch [20/69] time 0.369 (0.432) data 0.237 (0.301) loss_u loss_u 0.8979 (0.8895) acc_u 9.3750 (13.9062) lr 1.1623e-04 eta 0:00:21
epoch [171/200] batch [25/69] time 0.436 (0.438) data 0.304 (0.307) loss_u loss_u 0.8965 (0.8891) acc_u 12.5000 (13.8750) lr 1.1623e-04 eta 0:00:19
epoch [171/200] batch [30/69] time 0.403 (0.437) data 0.273 (0.306) loss_u loss_u 0.8872 (0.8822) acc_u 12.5000 (14.7917) lr 1.1623e-04 eta 0:00:17
epoch [171/200] batch [35/69] time 0.404 (0.438) data 0.274 (0.307) loss_u loss_u 0.8872 (0.8862) acc_u 12.5000 (14.1964) lr 1.1623e-04 eta 0:00:14
epoch [171/200] batch [40/69] time 0.397 (0.441) data 0.266 (0.310) loss_u loss_u 0.9658 (0.8902) acc_u 3.1250 (13.6719) lr 1.1623e-04 eta 0:00:12
epoch [171/200] batch [45/69] time 0.696 (0.444) data 0.564 (0.313) loss_u loss_u 0.7656 (0.8878) acc_u 34.3750 (13.9583) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [50/69] time 0.416 (0.440) data 0.284 (0.309) loss_u loss_u 0.8936 (0.8865) acc_u 12.5000 (14.2500) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [55/69] time 0.350 (0.440) data 0.219 (0.309) loss_u loss_u 0.9170 (0.8866) acc_u 9.3750 (14.2614) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [60/69] time 0.488 (0.441) data 0.357 (0.310) loss_u loss_u 0.9556 (0.8889) acc_u 6.2500 (13.8542) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [65/69] time 0.340 (0.440) data 0.209 (0.309) loss_u loss_u 0.9326 (0.8893) acc_u 12.5000 (13.8942) lr 1.1623e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1748
confident_label rate tensor(0.2902, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 910
clean true:656
clean false:254
clean_rate:0.7208791208791209
noisy true:732
noisy false:1494
after delete: len(clean_dataset) 910
after delete: len(noisy_dataset) 2226
epoch [172/200] batch [5/28] time 0.645 (0.496) data 0.514 (0.365) loss_x loss_x 1.2881 (1.0834) acc_x 78.1250 (78.7500) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [10/28] time 0.468 (0.463) data 0.337 (0.332) loss_x loss_x 1.1250 (1.1300) acc_x 71.8750 (74.3750) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [15/28] time 0.704 (0.474) data 0.573 (0.343) loss_x loss_x 1.2910 (1.1859) acc_x 78.1250 (74.5833) lr 1.0899e-04 eta 0:00:06
epoch [172/200] batch [20/28] time 0.488 (0.474) data 0.356 (0.343) loss_x loss_x 0.7534 (1.1535) acc_x 75.0000 (73.4375) lr 1.0899e-04 eta 0:00:03
epoch [172/200] batch [25/28] time 0.569 (0.476) data 0.439 (0.345) loss_x loss_x 1.5752 (1.1911) acc_x 62.5000 (72.6250) lr 1.0899e-04 eta 0:00:01
epoch [172/200] batch [5/69] time 0.415 (0.458) data 0.285 (0.327) loss_u loss_u 0.9243 (0.8882) acc_u 9.3750 (12.5000) lr 1.0899e-04 eta 0:00:29
epoch [172/200] batch [10/69] time 0.407 (0.453) data 0.276 (0.323) loss_u loss_u 0.8955 (0.8941) acc_u 15.6250 (12.1875) lr 1.0899e-04 eta 0:00:26
epoch [172/200] batch [15/69] time 0.412 (0.451) data 0.280 (0.321) loss_u loss_u 0.8374 (0.8971) acc_u 21.8750 (11.6667) lr 1.0899e-04 eta 0:00:24
epoch [172/200] batch [20/69] time 0.429 (0.447) data 0.297 (0.316) loss_u loss_u 0.8896 (0.8928) acc_u 12.5000 (12.5000) lr 1.0899e-04 eta 0:00:21
epoch [172/200] batch [25/69] time 0.394 (0.444) data 0.261 (0.313) loss_u loss_u 0.9634 (0.8843) acc_u 0.0000 (14.1250) lr 1.0899e-04 eta 0:00:19
epoch [172/200] batch [30/69] time 0.446 (0.448) data 0.316 (0.317) loss_u loss_u 0.8882 (0.8862) acc_u 12.5000 (14.0625) lr 1.0899e-04 eta 0:00:17
epoch [172/200] batch [35/69] time 0.328 (0.444) data 0.196 (0.313) loss_u loss_u 0.8599 (0.8845) acc_u 15.6250 (14.2857) lr 1.0899e-04 eta 0:00:15
epoch [172/200] batch [40/69] time 0.446 (0.449) data 0.314 (0.317) loss_u loss_u 0.8965 (0.8831) acc_u 12.5000 (14.5312) lr 1.0899e-04 eta 0:00:13
epoch [172/200] batch [45/69] time 0.516 (0.450) data 0.384 (0.319) loss_u loss_u 0.8428 (0.8803) acc_u 21.8750 (14.9306) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [50/69] time 0.530 (0.450) data 0.398 (0.319) loss_u loss_u 0.9102 (0.8829) acc_u 12.5000 (14.5625) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [55/69] time 0.508 (0.448) data 0.375 (0.316) loss_u loss_u 0.9624 (0.8878) acc_u 3.1250 (13.9773) lr 1.0899e-04 eta 0:00:06
epoch [172/200] batch [60/69] time 0.407 (0.446) data 0.276 (0.315) loss_u loss_u 0.7983 (0.8863) acc_u 28.1250 (14.2708) lr 1.0899e-04 eta 0:00:04
epoch [172/200] batch [65/69] time 0.377 (0.447) data 0.245 (0.315) loss_u loss_u 0.9019 (0.8858) acc_u 12.5000 (14.3750) lr 1.0899e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1748
confident_label rate tensor(0.2848, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 893
clean true:646
clean false:247
clean_rate:0.723404255319149
noisy true:742
noisy false:1501
after delete: len(clean_dataset) 893
after delete: len(noisy_dataset) 2243
epoch [173/200] batch [5/27] time 0.398 (0.454) data 0.268 (0.323) loss_x loss_x 1.3652 (1.2952) acc_x 62.5000 (67.5000) lr 1.0197e-04 eta 0:00:09
epoch [173/200] batch [10/27] time 0.382 (0.478) data 0.251 (0.348) loss_x loss_x 1.2998 (1.3045) acc_x 68.7500 (68.1250) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [15/27] time 0.380 (0.458) data 0.250 (0.328) loss_x loss_x 1.5400 (1.3675) acc_x 68.7500 (67.5000) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [20/27] time 0.400 (0.452) data 0.269 (0.321) loss_x loss_x 0.8560 (1.2996) acc_x 71.8750 (68.9062) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [25/27] time 0.519 (0.461) data 0.388 (0.330) loss_x loss_x 1.1982 (1.2668) acc_x 65.6250 (69.2500) lr 1.0197e-04 eta 0:00:00
epoch [173/200] batch [5/70] time 0.450 (0.457) data 0.318 (0.326) loss_u loss_u 0.8799 (0.9133) acc_u 18.7500 (11.8750) lr 1.0197e-04 eta 0:00:29
epoch [173/200] batch [10/70] time 0.465 (0.452) data 0.333 (0.322) loss_u loss_u 0.9141 (0.8952) acc_u 12.5000 (14.0625) lr 1.0197e-04 eta 0:00:27
epoch [173/200] batch [15/70] time 0.427 (0.452) data 0.296 (0.321) loss_u loss_u 0.8960 (0.8872) acc_u 6.2500 (14.1667) lr 1.0197e-04 eta 0:00:24
epoch [173/200] batch [20/70] time 0.375 (0.448) data 0.244 (0.317) loss_u loss_u 0.9185 (0.8893) acc_u 9.3750 (14.2188) lr 1.0197e-04 eta 0:00:22
epoch [173/200] batch [25/70] time 0.390 (0.452) data 0.258 (0.321) loss_u loss_u 0.9424 (0.8922) acc_u 6.2500 (13.7500) lr 1.0197e-04 eta 0:00:20
epoch [173/200] batch [30/70] time 0.417 (0.451) data 0.286 (0.320) loss_u loss_u 0.8599 (0.8873) acc_u 18.7500 (14.2708) lr 1.0197e-04 eta 0:00:18
epoch [173/200] batch [35/70] time 0.349 (0.446) data 0.217 (0.315) loss_u loss_u 0.8242 (0.8872) acc_u 25.0000 (14.3750) lr 1.0197e-04 eta 0:00:15
epoch [173/200] batch [40/70] time 0.355 (0.441) data 0.225 (0.310) loss_u loss_u 0.8706 (0.8852) acc_u 15.6250 (14.6094) lr 1.0197e-04 eta 0:00:13
epoch [173/200] batch [45/70] time 0.420 (0.439) data 0.290 (0.308) loss_u loss_u 0.9038 (0.8863) acc_u 12.5000 (14.5139) lr 1.0197e-04 eta 0:00:10
epoch [173/200] batch [50/70] time 0.436 (0.438) data 0.305 (0.307) loss_u loss_u 0.9253 (0.8856) acc_u 12.5000 (14.6875) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [55/70] time 0.478 (0.437) data 0.348 (0.306) loss_u loss_u 0.8989 (0.8837) acc_u 12.5000 (15.1136) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [60/70] time 0.462 (0.436) data 0.331 (0.305) loss_u loss_u 0.8560 (0.8848) acc_u 18.7500 (14.9479) lr 1.0197e-04 eta 0:00:04
epoch [173/200] batch [65/70] time 0.394 (0.440) data 0.263 (0.309) loss_u loss_u 0.8882 (0.8855) acc_u 12.5000 (14.8077) lr 1.0197e-04 eta 0:00:02
epoch [173/200] batch [70/70] time 0.394 (0.439) data 0.262 (0.308) loss_u loss_u 0.8408 (0.8837) acc_u 18.7500 (15.0446) lr 1.0197e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1751
confident_label rate tensor(0.2918, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 915
clean true:671
clean false:244
clean_rate:0.7333333333333333
noisy true:714
noisy false:1507
after delete: len(clean_dataset) 915
after delete: len(noisy_dataset) 2221
epoch [174/200] batch [5/28] time 0.492 (0.398) data 0.361 (0.267) loss_x loss_x 1.2998 (1.0574) acc_x 75.0000 (74.3750) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [10/28] time 0.611 (0.454) data 0.480 (0.323) loss_x loss_x 0.9082 (1.0971) acc_x 75.0000 (72.5000) lr 9.5173e-05 eta 0:00:08
epoch [174/200] batch [15/28] time 0.451 (0.442) data 0.319 (0.311) loss_x loss_x 1.3584 (1.0969) acc_x 62.5000 (72.7083) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [20/28] time 0.515 (0.438) data 0.384 (0.307) loss_x loss_x 1.1816 (1.0932) acc_x 68.7500 (73.1250) lr 9.5173e-05 eta 0:00:03
epoch [174/200] batch [25/28] time 0.476 (0.443) data 0.346 (0.312) loss_x loss_x 0.7383 (1.0838) acc_x 84.3750 (73.3750) lr 9.5173e-05 eta 0:00:01
epoch [174/200] batch [5/69] time 0.526 (0.444) data 0.394 (0.313) loss_u loss_u 0.9316 (0.9208) acc_u 6.2500 (8.7500) lr 9.5173e-05 eta 0:00:28
epoch [174/200] batch [10/69] time 0.402 (0.443) data 0.270 (0.312) loss_u loss_u 0.9707 (0.9104) acc_u 0.0000 (10.0000) lr 9.5173e-05 eta 0:00:26
epoch [174/200] batch [15/69] time 0.471 (0.441) data 0.339 (0.310) loss_u loss_u 0.9336 (0.9105) acc_u 9.3750 (10.8333) lr 9.5173e-05 eta 0:00:23
epoch [174/200] batch [20/69] time 0.484 (0.445) data 0.353 (0.313) loss_u loss_u 0.9233 (0.9054) acc_u 9.3750 (11.4062) lr 9.5173e-05 eta 0:00:21
epoch [174/200] batch [25/69] time 0.422 (0.443) data 0.291 (0.312) loss_u loss_u 0.8750 (0.8964) acc_u 12.5000 (12.2500) lr 9.5173e-05 eta 0:00:19
epoch [174/200] batch [30/69] time 0.373 (0.449) data 0.242 (0.318) loss_u loss_u 0.7861 (0.8941) acc_u 31.2500 (12.8125) lr 9.5173e-05 eta 0:00:17
epoch [174/200] batch [35/69] time 0.525 (0.450) data 0.394 (0.318) loss_u loss_u 0.8848 (0.8952) acc_u 12.5000 (12.5893) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [40/69] time 0.459 (0.454) data 0.327 (0.323) loss_u loss_u 0.9111 (0.8913) acc_u 12.5000 (13.2031) lr 9.5173e-05 eta 0:00:13
epoch [174/200] batch [45/69] time 0.378 (0.450) data 0.247 (0.318) loss_u loss_u 0.9478 (0.8909) acc_u 3.1250 (13.0556) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [50/69] time 0.476 (0.452) data 0.345 (0.320) loss_u loss_u 0.8574 (0.8906) acc_u 21.8750 (13.3125) lr 9.5173e-05 eta 0:00:08
epoch [174/200] batch [55/69] time 0.410 (0.453) data 0.279 (0.321) loss_u loss_u 0.9258 (0.8931) acc_u 9.3750 (13.0682) lr 9.5173e-05 eta 0:00:06
epoch [174/200] batch [60/69] time 0.388 (0.450) data 0.256 (0.318) loss_u loss_u 0.8408 (0.8926) acc_u 21.8750 (13.1771) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [65/69] time 0.394 (0.450) data 0.263 (0.319) loss_u loss_u 0.8721 (0.8922) acc_u 18.7500 (13.3173) lr 9.5173e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1731
confident_label rate tensor(0.2911, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 913
clean true:667
clean false:246
clean_rate:0.7305585980284776
noisy true:738
noisy false:1485
after delete: len(clean_dataset) 913
after delete: len(noisy_dataset) 2223
epoch [175/200] batch [5/28] time 0.412 (0.434) data 0.281 (0.304) loss_x loss_x 1.5215 (1.1892) acc_x 68.7500 (75.0000) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [10/28] time 0.535 (0.448) data 0.404 (0.317) loss_x loss_x 1.0576 (1.0856) acc_x 75.0000 (75.9375) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [15/28] time 0.392 (0.440) data 0.260 (0.310) loss_x loss_x 0.5171 (1.0040) acc_x 81.2500 (76.4583) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [20/28] time 0.507 (0.450) data 0.376 (0.319) loss_x loss_x 0.9277 (1.0397) acc_x 71.8750 (74.6875) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [25/28] time 0.415 (0.453) data 0.284 (0.323) loss_x loss_x 1.1064 (1.0525) acc_x 81.2500 (74.3750) lr 8.8597e-05 eta 0:00:01
epoch [175/200] batch [5/69] time 0.798 (0.463) data 0.668 (0.332) loss_u loss_u 0.8706 (0.8758) acc_u 21.8750 (16.8750) lr 8.8597e-05 eta 0:00:29
epoch [175/200] batch [10/69] time 0.534 (0.460) data 0.403 (0.329) loss_u loss_u 0.9097 (0.8888) acc_u 12.5000 (14.3750) lr 8.8597e-05 eta 0:00:27
epoch [175/200] batch [15/69] time 0.461 (0.460) data 0.330 (0.329) loss_u loss_u 0.9409 (0.8952) acc_u 6.2500 (13.7500) lr 8.8597e-05 eta 0:00:24
epoch [175/200] batch [20/69] time 0.379 (0.459) data 0.248 (0.328) loss_u loss_u 0.8281 (0.8845) acc_u 21.8750 (14.6875) lr 8.8597e-05 eta 0:00:22
epoch [175/200] batch [25/69] time 0.384 (0.454) data 0.253 (0.323) loss_u loss_u 0.8989 (0.8899) acc_u 15.6250 (14.2500) lr 8.8597e-05 eta 0:00:19
epoch [175/200] batch [30/69] time 0.360 (0.448) data 0.229 (0.317) loss_u loss_u 0.9585 (0.8925) acc_u 3.1250 (13.7500) lr 8.8597e-05 eta 0:00:17
epoch [175/200] batch [35/69] time 0.370 (0.448) data 0.238 (0.317) loss_u loss_u 0.8110 (0.8893) acc_u 25.0000 (14.1071) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [40/69] time 0.467 (0.447) data 0.335 (0.315) loss_u loss_u 0.9497 (0.8911) acc_u 6.2500 (13.6719) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [45/69] time 0.533 (0.444) data 0.401 (0.313) loss_u loss_u 0.7754 (0.8903) acc_u 28.1250 (13.7500) lr 8.8597e-05 eta 0:00:10
epoch [175/200] batch [50/69] time 0.351 (0.446) data 0.220 (0.314) loss_u loss_u 0.8564 (0.8868) acc_u 12.5000 (14.0000) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [55/69] time 0.510 (0.445) data 0.379 (0.314) loss_u loss_u 0.8921 (0.8864) acc_u 12.5000 (14.0909) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [60/69] time 0.505 (0.450) data 0.373 (0.319) loss_u loss_u 0.8784 (0.8879) acc_u 15.6250 (13.9583) lr 8.8597e-05 eta 0:00:04
epoch [175/200] batch [65/69] time 0.430 (0.449) data 0.299 (0.318) loss_u loss_u 0.9521 (0.8880) acc_u 3.1250 (13.9423) lr 8.8597e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1716
confident_label rate tensor(0.2962, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 929
clean true:667
clean false:262
clean_rate:0.7179763186221744
noisy true:753
noisy false:1454
after delete: len(clean_dataset) 929
after delete: len(noisy_dataset) 2207
epoch [176/200] batch [5/29] time 0.405 (0.437) data 0.274 (0.306) loss_x loss_x 1.0557 (1.0287) acc_x 62.5000 (72.5000) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [10/29] time 0.328 (0.425) data 0.197 (0.295) loss_x loss_x 1.1455 (1.1834) acc_x 68.7500 (70.3125) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [15/29] time 0.452 (0.445) data 0.322 (0.314) loss_x loss_x 1.1836 (1.1673) acc_x 65.6250 (70.0000) lr 8.2245e-05 eta 0:00:06
epoch [176/200] batch [20/29] time 0.490 (0.454) data 0.359 (0.324) loss_x loss_x 1.1016 (1.1345) acc_x 68.7500 (69.8438) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [25/29] time 0.459 (0.461) data 0.329 (0.330) loss_x loss_x 1.0918 (1.1376) acc_x 68.7500 (69.7500) lr 8.2245e-05 eta 0:00:01
epoch [176/200] batch [5/68] time 0.338 (0.449) data 0.207 (0.318) loss_u loss_u 0.8804 (0.8948) acc_u 18.7500 (16.8750) lr 8.2245e-05 eta 0:00:28
epoch [176/200] batch [10/68] time 0.381 (0.445) data 0.249 (0.314) loss_u loss_u 0.9473 (0.8884) acc_u 9.3750 (15.6250) lr 8.2245e-05 eta 0:00:25
epoch [176/200] batch [15/68] time 0.396 (0.442) data 0.264 (0.311) loss_u loss_u 0.8638 (0.8855) acc_u 15.6250 (15.4167) lr 8.2245e-05 eta 0:00:23
epoch [176/200] batch [20/68] time 0.377 (0.446) data 0.246 (0.315) loss_u loss_u 0.8657 (0.8891) acc_u 15.6250 (14.6875) lr 8.2245e-05 eta 0:00:21
epoch [176/200] batch [25/68] time 0.328 (0.446) data 0.197 (0.315) loss_u loss_u 0.9399 (0.8899) acc_u 6.2500 (14.2500) lr 8.2245e-05 eta 0:00:19
epoch [176/200] batch [30/68] time 0.396 (0.443) data 0.266 (0.312) loss_u loss_u 0.9087 (0.8875) acc_u 12.5000 (14.6875) lr 8.2245e-05 eta 0:00:16
epoch [176/200] batch [35/68] time 0.486 (0.446) data 0.354 (0.315) loss_u loss_u 0.8667 (0.8874) acc_u 18.7500 (14.7321) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [40/68] time 0.370 (0.444) data 0.239 (0.312) loss_u loss_u 0.9385 (0.8878) acc_u 3.1250 (14.4531) lr 8.2245e-05 eta 0:00:12
epoch [176/200] batch [45/68] time 0.522 (0.443) data 0.390 (0.312) loss_u loss_u 0.9058 (0.8849) acc_u 12.5000 (14.7222) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [50/68] time 0.477 (0.445) data 0.344 (0.314) loss_u loss_u 0.7681 (0.8834) acc_u 28.1250 (14.6875) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [55/68] time 0.375 (0.446) data 0.243 (0.314) loss_u loss_u 0.8862 (0.8823) acc_u 12.5000 (14.8864) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [60/68] time 0.449 (0.450) data 0.316 (0.319) loss_u loss_u 0.8984 (0.8852) acc_u 12.5000 (14.3750) lr 8.2245e-05 eta 0:00:03
epoch [176/200] batch [65/68] time 0.470 (0.452) data 0.339 (0.320) loss_u loss_u 0.9316 (0.8852) acc_u 12.5000 (14.4712) lr 8.2245e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1726
confident_label rate tensor(0.2902, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 910
clean true:683
clean false:227
clean_rate:0.7505494505494505
noisy true:727
noisy false:1499
after delete: len(clean_dataset) 910
after delete: len(noisy_dataset) 2226
epoch [177/200] batch [5/28] time 0.416 (0.394) data 0.285 (0.264) loss_x loss_x 0.8428 (1.0740) acc_x 81.2500 (74.3750) lr 7.6120e-05 eta 0:00:09
epoch [177/200] batch [10/28] time 0.413 (0.419) data 0.283 (0.289) loss_x loss_x 1.2256 (1.1375) acc_x 75.0000 (72.1875) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [15/28] time 0.493 (0.427) data 0.362 (0.296) loss_x loss_x 1.0967 (1.1890) acc_x 62.5000 (70.8333) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [20/28] time 0.479 (0.432) data 0.348 (0.301) loss_x loss_x 1.5898 (1.2033) acc_x 71.8750 (70.9375) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [25/28] time 0.370 (0.447) data 0.239 (0.316) loss_x loss_x 0.7734 (1.1545) acc_x 78.1250 (72.2500) lr 7.6120e-05 eta 0:00:01
epoch [177/200] batch [5/69] time 0.334 (0.447) data 0.203 (0.316) loss_u loss_u 0.9409 (0.9150) acc_u 6.2500 (10.0000) lr 7.6120e-05 eta 0:00:28
epoch [177/200] batch [10/69] time 0.419 (0.450) data 0.288 (0.319) loss_u loss_u 0.9390 (0.8888) acc_u 9.3750 (14.0625) lr 7.6120e-05 eta 0:00:26
epoch [177/200] batch [15/69] time 0.401 (0.450) data 0.269 (0.319) loss_u loss_u 0.8818 (0.8914) acc_u 12.5000 (13.5417) lr 7.6120e-05 eta 0:00:24
epoch [177/200] batch [20/69] time 0.520 (0.450) data 0.389 (0.319) loss_u loss_u 0.8701 (0.8826) acc_u 18.7500 (15.1562) lr 7.6120e-05 eta 0:00:22
epoch [177/200] batch [25/69] time 0.502 (0.447) data 0.371 (0.316) loss_u loss_u 0.8955 (0.8894) acc_u 9.3750 (14.0000) lr 7.6120e-05 eta 0:00:19
epoch [177/200] batch [30/69] time 0.367 (0.442) data 0.235 (0.311) loss_u loss_u 0.8525 (0.8817) acc_u 15.6250 (15.0000) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [35/69] time 0.505 (0.441) data 0.374 (0.310) loss_u loss_u 0.9463 (0.8850) acc_u 6.2500 (14.5536) lr 7.6120e-05 eta 0:00:14
epoch [177/200] batch [40/69] time 0.513 (0.446) data 0.382 (0.314) loss_u loss_u 0.9399 (0.8877) acc_u 3.1250 (14.1406) lr 7.6120e-05 eta 0:00:12
epoch [177/200] batch [45/69] time 0.568 (0.447) data 0.437 (0.315) loss_u loss_u 0.9448 (0.8907) acc_u 6.2500 (13.8194) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [50/69] time 0.455 (0.444) data 0.324 (0.312) loss_u loss_u 0.9214 (0.8881) acc_u 12.5000 (14.1875) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [55/69] time 0.477 (0.443) data 0.345 (0.312) loss_u loss_u 0.8408 (0.8903) acc_u 28.1250 (14.1477) lr 7.6120e-05 eta 0:00:06
epoch [177/200] batch [60/69] time 0.526 (0.443) data 0.395 (0.312) loss_u loss_u 0.9087 (0.8896) acc_u 9.3750 (14.1146) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [65/69] time 0.452 (0.442) data 0.320 (0.310) loss_u loss_u 0.8115 (0.8860) acc_u 25.0000 (14.5192) lr 7.6120e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1738
confident_label rate tensor(0.2921, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 916
clean true:657
clean false:259
clean_rate:0.7172489082969432
noisy true:741
noisy false:1479
after delete: len(clean_dataset) 916
after delete: len(noisy_dataset) 2220
epoch [178/200] batch [5/28] time 0.462 (0.427) data 0.332 (0.296) loss_x loss_x 0.6318 (0.8366) acc_x 81.2500 (75.6250) lr 7.0224e-05 eta 0:00:09
epoch [178/200] batch [10/28] time 0.682 (0.459) data 0.552 (0.329) loss_x loss_x 1.0654 (0.9538) acc_x 75.0000 (74.3750) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [15/28] time 0.413 (0.444) data 0.282 (0.313) loss_x loss_x 1.2871 (1.0097) acc_x 65.6250 (72.5000) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [20/28] time 0.411 (0.454) data 0.281 (0.323) loss_x loss_x 0.9810 (1.0231) acc_x 68.7500 (71.7188) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [25/28] time 0.328 (0.455) data 0.197 (0.325) loss_x loss_x 1.4668 (1.1253) acc_x 62.5000 (70.0000) lr 7.0224e-05 eta 0:00:01
epoch [178/200] batch [5/69] time 0.414 (0.456) data 0.284 (0.326) loss_u loss_u 0.9062 (0.8778) acc_u 12.5000 (16.8750) lr 7.0224e-05 eta 0:00:29
epoch [178/200] batch [10/69] time 0.476 (0.454) data 0.345 (0.324) loss_u loss_u 0.9160 (0.8789) acc_u 12.5000 (16.2500) lr 7.0224e-05 eta 0:00:26
epoch [178/200] batch [15/69] time 0.359 (0.445) data 0.228 (0.315) loss_u loss_u 0.9170 (0.8885) acc_u 12.5000 (14.7917) lr 7.0224e-05 eta 0:00:24
epoch [178/200] batch [20/69] time 0.360 (0.442) data 0.229 (0.311) loss_u loss_u 0.8218 (0.8819) acc_u 15.6250 (14.8438) lr 7.0224e-05 eta 0:00:21
epoch [178/200] batch [25/69] time 0.419 (0.441) data 0.288 (0.310) loss_u loss_u 0.8789 (0.8759) acc_u 12.5000 (15.3750) lr 7.0224e-05 eta 0:00:19
epoch [178/200] batch [30/69] time 0.458 (0.440) data 0.327 (0.310) loss_u loss_u 0.8921 (0.8812) acc_u 12.5000 (14.4792) lr 7.0224e-05 eta 0:00:17
epoch [178/200] batch [35/69] time 0.491 (0.439) data 0.360 (0.308) loss_u loss_u 0.9150 (0.8859) acc_u 9.3750 (13.9286) lr 7.0224e-05 eta 0:00:14
epoch [178/200] batch [40/69] time 0.432 (0.436) data 0.301 (0.305) loss_u loss_u 0.8828 (0.8852) acc_u 15.6250 (14.2188) lr 7.0224e-05 eta 0:00:12
epoch [178/200] batch [45/69] time 0.367 (0.436) data 0.236 (0.305) loss_u loss_u 0.8794 (0.8840) acc_u 15.6250 (14.5833) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [50/69] time 0.409 (0.436) data 0.278 (0.305) loss_u loss_u 0.9375 (0.8833) acc_u 6.2500 (14.6250) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [55/69] time 0.316 (0.432) data 0.185 (0.301) loss_u loss_u 0.9482 (0.8829) acc_u 0.0000 (14.6023) lr 7.0224e-05 eta 0:00:06
epoch [178/200] batch [60/69] time 0.409 (0.435) data 0.276 (0.304) loss_u loss_u 0.8911 (0.8794) acc_u 15.6250 (15.0521) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [65/69] time 0.430 (0.436) data 0.298 (0.305) loss_u loss_u 0.8779 (0.8799) acc_u 15.6250 (14.8558) lr 7.0224e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1750
confident_label rate tensor(0.2889, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 906
clean true:658
clean false:248
clean_rate:0.7262693156732892
noisy true:728
noisy false:1502
after delete: len(clean_dataset) 906
after delete: len(noisy_dataset) 2230
epoch [179/200] batch [5/28] time 0.417 (0.427) data 0.287 (0.297) loss_x loss_x 0.9956 (0.9730) acc_x 75.0000 (77.5000) lr 6.4556e-05 eta 0:00:09
epoch [179/200] batch [10/28] time 0.463 (0.461) data 0.333 (0.330) loss_x loss_x 1.4170 (1.0790) acc_x 75.0000 (75.0000) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [15/28] time 0.395 (0.485) data 0.265 (0.354) loss_x loss_x 1.3281 (1.0954) acc_x 62.5000 (73.3333) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [20/28] time 0.436 (0.472) data 0.306 (0.341) loss_x loss_x 0.9790 (1.0340) acc_x 78.1250 (75.0000) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [25/28] time 0.404 (0.457) data 0.273 (0.326) loss_x loss_x 1.1045 (1.0489) acc_x 75.0000 (75.1250) lr 6.4556e-05 eta 0:00:01
epoch [179/200] batch [5/69] time 0.395 (0.455) data 0.264 (0.324) loss_u loss_u 0.8936 (0.8837) acc_u 6.2500 (15.6250) lr 6.4556e-05 eta 0:00:29
epoch [179/200] batch [10/69] time 0.680 (0.457) data 0.549 (0.327) loss_u loss_u 0.9009 (0.8871) acc_u 6.2500 (14.3750) lr 6.4556e-05 eta 0:00:26
epoch [179/200] batch [15/69] time 0.354 (0.450) data 0.222 (0.319) loss_u loss_u 0.8970 (0.8779) acc_u 15.6250 (16.4583) lr 6.4556e-05 eta 0:00:24
epoch [179/200] batch [20/69] time 0.345 (0.444) data 0.214 (0.313) loss_u loss_u 0.8721 (0.8881) acc_u 18.7500 (15.4688) lr 6.4556e-05 eta 0:00:21
epoch [179/200] batch [25/69] time 0.446 (0.443) data 0.315 (0.312) loss_u loss_u 0.8940 (0.8839) acc_u 6.2500 (15.8750) lr 6.4556e-05 eta 0:00:19
epoch [179/200] batch [30/69] time 0.403 (0.440) data 0.272 (0.309) loss_u loss_u 0.8389 (0.8777) acc_u 15.6250 (16.0417) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [35/69] time 0.365 (0.440) data 0.234 (0.309) loss_u loss_u 0.8423 (0.8783) acc_u 18.7500 (15.8036) lr 6.4556e-05 eta 0:00:14
epoch [179/200] batch [40/69] time 0.402 (0.439) data 0.271 (0.308) loss_u loss_u 0.9258 (0.8834) acc_u 9.3750 (15.0000) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [45/69] time 0.492 (0.441) data 0.361 (0.310) loss_u loss_u 0.9297 (0.8831) acc_u 9.3750 (14.7917) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [50/69] time 0.550 (0.443) data 0.419 (0.312) loss_u loss_u 0.9233 (0.8851) acc_u 12.5000 (14.6875) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [55/69] time 0.417 (0.444) data 0.286 (0.313) loss_u loss_u 0.8452 (0.8820) acc_u 18.7500 (15.2841) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [60/69] time 0.353 (0.443) data 0.221 (0.312) loss_u loss_u 0.9219 (0.8829) acc_u 9.3750 (15.1562) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [65/69] time 0.382 (0.442) data 0.252 (0.311) loss_u loss_u 0.8677 (0.8834) acc_u 18.7500 (15.0481) lr 6.4556e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1738
confident_label rate tensor(0.2899, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 909
clean true:645
clean false:264
clean_rate:0.7095709570957096
noisy true:753
noisy false:1474
after delete: len(clean_dataset) 909
after delete: len(noisy_dataset) 2227
epoch [180/200] batch [5/28] time 0.396 (0.518) data 0.265 (0.387) loss_x loss_x 1.0410 (1.2243) acc_x 75.0000 (71.2500) lr 5.9119e-05 eta 0:00:11
epoch [180/200] batch [10/28] time 0.521 (0.495) data 0.391 (0.364) loss_x loss_x 1.0459 (1.1932) acc_x 59.3750 (69.3750) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [15/28] time 0.466 (0.468) data 0.337 (0.338) loss_x loss_x 1.0576 (1.1750) acc_x 68.7500 (69.1667) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [20/28] time 0.520 (0.459) data 0.388 (0.329) loss_x loss_x 1.1182 (1.1359) acc_x 81.2500 (70.9375) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [25/28] time 0.485 (0.471) data 0.353 (0.340) loss_x loss_x 1.3594 (1.1647) acc_x 59.3750 (69.6250) lr 5.9119e-05 eta 0:00:01
epoch [180/200] batch [5/69] time 0.452 (0.470) data 0.322 (0.339) loss_u loss_u 0.8579 (0.8979) acc_u 15.6250 (11.8750) lr 5.9119e-05 eta 0:00:30
epoch [180/200] batch [10/69] time 0.413 (0.467) data 0.282 (0.336) loss_u loss_u 0.9468 (0.9015) acc_u 9.3750 (12.1875) lr 5.9119e-05 eta 0:00:27
epoch [180/200] batch [15/69] time 0.357 (0.457) data 0.227 (0.327) loss_u loss_u 0.8711 (0.8916) acc_u 18.7500 (13.5417) lr 5.9119e-05 eta 0:00:24
epoch [180/200] batch [20/69] time 0.340 (0.449) data 0.210 (0.319) loss_u loss_u 0.8662 (0.8863) acc_u 18.7500 (14.6875) lr 5.9119e-05 eta 0:00:22
epoch [180/200] batch [25/69] time 0.596 (0.448) data 0.464 (0.317) loss_u loss_u 0.8628 (0.8809) acc_u 18.7500 (15.1250) lr 5.9119e-05 eta 0:00:19
epoch [180/200] batch [30/69] time 0.526 (0.448) data 0.394 (0.317) loss_u loss_u 0.8550 (0.8810) acc_u 18.7500 (14.7917) lr 5.9119e-05 eta 0:00:17
epoch [180/200] batch [35/69] time 0.408 (0.445) data 0.276 (0.315) loss_u loss_u 0.8999 (0.8825) acc_u 18.7500 (15.0000) lr 5.9119e-05 eta 0:00:15
epoch [180/200] batch [40/69] time 0.590 (0.451) data 0.459 (0.320) loss_u loss_u 0.9463 (0.8837) acc_u 3.1250 (14.4531) lr 5.9119e-05 eta 0:00:13
epoch [180/200] batch [45/69] time 0.439 (0.451) data 0.307 (0.320) loss_u loss_u 0.8477 (0.8838) acc_u 25.0000 (14.7917) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [50/69] time 0.424 (0.446) data 0.293 (0.315) loss_u loss_u 0.8428 (0.8847) acc_u 21.8750 (14.8125) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [55/69] time 0.448 (0.446) data 0.317 (0.315) loss_u loss_u 0.8774 (0.8843) acc_u 12.5000 (14.7727) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [60/69] time 0.385 (0.444) data 0.254 (0.313) loss_u loss_u 0.8315 (0.8848) acc_u 18.7500 (14.5833) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [65/69] time 0.510 (0.444) data 0.378 (0.313) loss_u loss_u 0.9609 (0.8844) acc_u 3.1250 (14.5673) lr 5.9119e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1744
confident_label rate tensor(0.2883, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 904
clean true:642
clean false:262
clean_rate:0.7101769911504425
noisy true:750
noisy false:1482
after delete: len(clean_dataset) 904
after delete: len(noisy_dataset) 2232
epoch [181/200] batch [5/28] time 0.592 (0.624) data 0.461 (0.493) loss_x loss_x 1.4453 (1.1495) acc_x 65.6250 (70.6250) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [10/28] time 0.503 (0.545) data 0.372 (0.414) loss_x loss_x 1.0391 (1.1510) acc_x 75.0000 (70.9375) lr 5.3915e-05 eta 0:00:09
epoch [181/200] batch [15/28] time 0.391 (0.514) data 0.260 (0.383) loss_x loss_x 0.8154 (1.1439) acc_x 78.1250 (72.2917) lr 5.3915e-05 eta 0:00:06
epoch [181/200] batch [20/28] time 0.357 (0.495) data 0.226 (0.363) loss_x loss_x 1.1777 (1.1528) acc_x 68.7500 (71.7188) lr 5.3915e-05 eta 0:00:03
epoch [181/200] batch [25/28] time 0.463 (0.485) data 0.332 (0.354) loss_x loss_x 1.1826 (1.1689) acc_x 68.7500 (70.7500) lr 5.3915e-05 eta 0:00:01
epoch [181/200] batch [5/69] time 0.455 (0.484) data 0.322 (0.352) loss_u loss_u 0.8677 (0.8653) acc_u 18.7500 (16.2500) lr 5.3915e-05 eta 0:00:30
epoch [181/200] batch [10/69] time 0.401 (0.474) data 0.269 (0.343) loss_u loss_u 0.8901 (0.8604) acc_u 9.3750 (16.8750) lr 5.3915e-05 eta 0:00:27
epoch [181/200] batch [15/69] time 0.472 (0.477) data 0.340 (0.346) loss_u loss_u 0.9268 (0.8659) acc_u 12.5000 (17.2917) lr 5.3915e-05 eta 0:00:25
epoch [181/200] batch [20/69] time 0.384 (0.474) data 0.252 (0.342) loss_u loss_u 0.9058 (0.8740) acc_u 12.5000 (15.7812) lr 5.3915e-05 eta 0:00:23
epoch [181/200] batch [25/69] time 0.361 (0.466) data 0.229 (0.335) loss_u loss_u 0.8525 (0.8768) acc_u 21.8750 (16.0000) lr 5.3915e-05 eta 0:00:20
epoch [181/200] batch [30/69] time 0.414 (0.465) data 0.282 (0.333) loss_u loss_u 0.8716 (0.8841) acc_u 15.6250 (14.8958) lr 5.3915e-05 eta 0:00:18
epoch [181/200] batch [35/69] time 0.495 (0.464) data 0.363 (0.333) loss_u loss_u 0.8647 (0.8857) acc_u 15.6250 (14.3750) lr 5.3915e-05 eta 0:00:15
epoch [181/200] batch [40/69] time 0.718 (0.470) data 0.586 (0.339) loss_u loss_u 0.8237 (0.8868) acc_u 18.7500 (14.2188) lr 5.3915e-05 eta 0:00:13
epoch [181/200] batch [45/69] time 0.372 (0.467) data 0.240 (0.336) loss_u loss_u 0.9092 (0.8878) acc_u 12.5000 (14.1667) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [50/69] time 0.479 (0.469) data 0.346 (0.337) loss_u loss_u 0.9536 (0.8877) acc_u 3.1250 (14.0625) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [55/69] time 0.370 (0.469) data 0.237 (0.337) loss_u loss_u 0.8574 (0.8890) acc_u 18.7500 (13.8068) lr 5.3915e-05 eta 0:00:06
epoch [181/200] batch [60/69] time 0.444 (0.470) data 0.313 (0.338) loss_u loss_u 0.9512 (0.8877) acc_u 6.2500 (14.1667) lr 5.3915e-05 eta 0:00:04
epoch [181/200] batch [65/69] time 0.508 (0.470) data 0.377 (0.338) loss_u loss_u 0.9448 (0.8891) acc_u 9.3750 (14.0865) lr 5.3915e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1747
confident_label rate tensor(0.2911, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 913
clean true:659
clean false:254
clean_rate:0.7217962760131434
noisy true:730
noisy false:1493
after delete: len(clean_dataset) 913
after delete: len(noisy_dataset) 2223
epoch [182/200] batch [5/28] time 0.404 (0.470) data 0.274 (0.340) loss_x loss_x 0.8228 (1.2888) acc_x 75.0000 (68.7500) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [10/28] time 0.449 (0.460) data 0.318 (0.329) loss_x loss_x 0.9316 (1.1814) acc_x 68.7500 (71.8750) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [15/28] time 0.549 (0.472) data 0.418 (0.341) loss_x loss_x 0.8779 (1.1624) acc_x 59.3750 (71.6667) lr 4.8943e-05 eta 0:00:06
epoch [182/200] batch [20/28] time 0.398 (0.472) data 0.267 (0.342) loss_x loss_x 1.2793 (1.1875) acc_x 65.6250 (70.9375) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [25/28] time 0.400 (0.461) data 0.268 (0.331) loss_x loss_x 1.1426 (1.1555) acc_x 65.6250 (71.5000) lr 4.8943e-05 eta 0:00:01
epoch [182/200] batch [5/69] time 0.425 (0.450) data 0.294 (0.319) loss_u loss_u 0.8511 (0.8782) acc_u 18.7500 (16.2500) lr 4.8943e-05 eta 0:00:28
epoch [182/200] batch [10/69] time 0.398 (0.452) data 0.267 (0.321) loss_u loss_u 0.8672 (0.8732) acc_u 18.7500 (16.8750) lr 4.8943e-05 eta 0:00:26
epoch [182/200] batch [15/69] time 0.426 (0.454) data 0.294 (0.323) loss_u loss_u 0.8213 (0.8719) acc_u 25.0000 (16.0417) lr 4.8943e-05 eta 0:00:24
epoch [182/200] batch [20/69] time 0.385 (0.451) data 0.254 (0.320) loss_u loss_u 0.8838 (0.8713) acc_u 9.3750 (15.7812) lr 4.8943e-05 eta 0:00:22
epoch [182/200] batch [25/69] time 0.399 (0.449) data 0.268 (0.317) loss_u loss_u 0.8433 (0.8767) acc_u 21.8750 (15.1250) lr 4.8943e-05 eta 0:00:19
epoch [182/200] batch [30/69] time 0.325 (0.446) data 0.194 (0.315) loss_u loss_u 0.9717 (0.8840) acc_u 3.1250 (14.1667) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [35/69] time 0.544 (0.446) data 0.411 (0.315) loss_u loss_u 0.9614 (0.8870) acc_u 0.0000 (13.6607) lr 4.8943e-05 eta 0:00:15
epoch [182/200] batch [40/69] time 0.312 (0.446) data 0.181 (0.315) loss_u loss_u 0.8955 (0.8878) acc_u 12.5000 (13.6719) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [45/69] time 0.416 (0.447) data 0.285 (0.316) loss_u loss_u 0.9146 (0.8905) acc_u 12.5000 (13.4722) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [50/69] time 0.377 (0.445) data 0.246 (0.314) loss_u loss_u 0.8623 (0.8892) acc_u 25.0000 (13.8750) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [55/69] time 0.407 (0.444) data 0.276 (0.313) loss_u loss_u 0.8813 (0.8887) acc_u 15.6250 (13.9773) lr 4.8943e-05 eta 0:00:06
epoch [182/200] batch [60/69] time 0.349 (0.444) data 0.219 (0.313) loss_u loss_u 0.9028 (0.8874) acc_u 15.6250 (14.0104) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [65/69] time 0.385 (0.443) data 0.253 (0.312) loss_u loss_u 0.9209 (0.8897) acc_u 12.5000 (13.8462) lr 4.8943e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1745
confident_label rate tensor(0.3001, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 941
clean true:677
clean false:264
clean_rate:0.7194473963868225
noisy true:714
noisy false:1481
after delete: len(clean_dataset) 941
after delete: len(noisy_dataset) 2195
epoch [183/200] batch [5/29] time 0.421 (0.433) data 0.291 (0.302) loss_x loss_x 0.7456 (1.0743) acc_x 71.8750 (68.1250) lr 4.4207e-05 eta 0:00:10
epoch [183/200] batch [10/29] time 0.569 (0.453) data 0.438 (0.322) loss_x loss_x 0.9448 (1.0600) acc_x 71.8750 (70.6250) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [15/29] time 0.510 (0.455) data 0.380 (0.325) loss_x loss_x 0.9204 (1.1086) acc_x 71.8750 (71.4583) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [20/29] time 0.396 (0.464) data 0.265 (0.334) loss_x loss_x 0.9287 (1.0692) acc_x 75.0000 (72.1875) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [25/29] time 0.445 (0.458) data 0.314 (0.327) loss_x loss_x 1.0859 (1.0626) acc_x 75.0000 (72.6250) lr 4.4207e-05 eta 0:00:01
epoch [183/200] batch [5/68] time 0.365 (0.446) data 0.233 (0.316) loss_u loss_u 0.8975 (0.9061) acc_u 15.6250 (13.7500) lr 4.4207e-05 eta 0:00:28
epoch [183/200] batch [10/68] time 0.420 (0.440) data 0.288 (0.309) loss_u loss_u 0.9097 (0.9021) acc_u 9.3750 (12.8125) lr 4.4207e-05 eta 0:00:25
epoch [183/200] batch [15/68] time 0.391 (0.443) data 0.260 (0.312) loss_u loss_u 0.8818 (0.8988) acc_u 15.6250 (13.9583) lr 4.4207e-05 eta 0:00:23
epoch [183/200] batch [20/68] time 0.487 (0.442) data 0.354 (0.311) loss_u loss_u 0.9048 (0.8956) acc_u 9.3750 (13.7500) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [25/68] time 0.500 (0.447) data 0.367 (0.316) loss_u loss_u 0.8335 (0.8921) acc_u 18.7500 (13.7500) lr 4.4207e-05 eta 0:00:19
epoch [183/200] batch [30/68] time 0.443 (0.446) data 0.311 (0.315) loss_u loss_u 0.9468 (0.8926) acc_u 6.2500 (13.6458) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [35/68] time 0.381 (0.444) data 0.251 (0.313) loss_u loss_u 0.8794 (0.8907) acc_u 18.7500 (13.8393) lr 4.4207e-05 eta 0:00:14
epoch [183/200] batch [40/68] time 0.440 (0.447) data 0.307 (0.316) loss_u loss_u 0.9023 (0.8918) acc_u 12.5000 (13.6719) lr 4.4207e-05 eta 0:00:12
epoch [183/200] batch [45/68] time 0.307 (0.450) data 0.175 (0.318) loss_u loss_u 0.9023 (0.8924) acc_u 15.6250 (13.6806) lr 4.4207e-05 eta 0:00:10
epoch [183/200] batch [50/68] time 0.398 (0.448) data 0.267 (0.317) loss_u loss_u 0.9609 (0.8926) acc_u 3.1250 (13.8125) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [55/68] time 0.451 (0.450) data 0.320 (0.318) loss_u loss_u 0.8906 (0.8923) acc_u 18.7500 (14.0341) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [60/68] time 0.453 (0.451) data 0.322 (0.319) loss_u loss_u 0.8633 (0.8920) acc_u 18.7500 (13.9583) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [65/68] time 0.395 (0.449) data 0.264 (0.317) loss_u loss_u 0.9102 (0.8935) acc_u 12.5000 (13.8942) lr 4.4207e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.2940, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 922
clean true:666
clean false:256
clean_rate:0.7223427331887202
noisy true:721
noisy false:1493
after delete: len(clean_dataset) 922
after delete: len(noisy_dataset) 2214
epoch [184/200] batch [5/28] time 0.443 (0.428) data 0.309 (0.296) loss_x loss_x 1.1094 (1.3256) acc_x 65.6250 (68.7500) lr 3.9706e-05 eta 0:00:09
epoch [184/200] batch [10/28] time 0.467 (0.461) data 0.335 (0.329) loss_x loss_x 0.6992 (1.0973) acc_x 75.0000 (73.1250) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [15/28] time 0.464 (0.458) data 0.332 (0.326) loss_x loss_x 0.8945 (1.0735) acc_x 75.0000 (73.5417) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [20/28] time 0.478 (0.478) data 0.347 (0.346) loss_x loss_x 1.0244 (1.0916) acc_x 78.1250 (73.1250) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [25/28] time 0.439 (0.474) data 0.309 (0.343) loss_x loss_x 1.7275 (1.1487) acc_x 53.1250 (71.6250) lr 3.9706e-05 eta 0:00:01
epoch [184/200] batch [5/69] time 0.529 (0.471) data 0.397 (0.339) loss_u loss_u 0.9312 (0.8923) acc_u 9.3750 (13.7500) lr 3.9706e-05 eta 0:00:30
epoch [184/200] batch [10/69] time 0.470 (0.470) data 0.339 (0.338) loss_u loss_u 0.9048 (0.8890) acc_u 15.6250 (15.0000) lr 3.9706e-05 eta 0:00:27
epoch [184/200] batch [15/69] time 0.389 (0.465) data 0.257 (0.333) loss_u loss_u 0.8794 (0.8897) acc_u 9.3750 (13.9583) lr 3.9706e-05 eta 0:00:25
epoch [184/200] batch [20/69] time 0.389 (0.462) data 0.258 (0.331) loss_u loss_u 0.9409 (0.8958) acc_u 9.3750 (13.7500) lr 3.9706e-05 eta 0:00:22
epoch [184/200] batch [25/69] time 0.374 (0.464) data 0.243 (0.332) loss_u loss_u 0.8916 (0.8922) acc_u 12.5000 (14.0000) lr 3.9706e-05 eta 0:00:20
epoch [184/200] batch [30/69] time 0.435 (0.459) data 0.303 (0.328) loss_u loss_u 0.8325 (0.8861) acc_u 18.7500 (14.6875) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [35/69] time 0.414 (0.459) data 0.283 (0.328) loss_u loss_u 0.9346 (0.8872) acc_u 6.2500 (13.9286) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [40/69] time 0.423 (0.458) data 0.291 (0.327) loss_u loss_u 0.9272 (0.8883) acc_u 6.2500 (13.6719) lr 3.9706e-05 eta 0:00:13
epoch [184/200] batch [45/69] time 0.394 (0.460) data 0.263 (0.328) loss_u loss_u 0.8760 (0.8884) acc_u 21.8750 (13.9583) lr 3.9706e-05 eta 0:00:11
epoch [184/200] batch [50/69] time 0.462 (0.455) data 0.331 (0.324) loss_u loss_u 0.8877 (0.8906) acc_u 21.8750 (14.0625) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [55/69] time 0.364 (0.455) data 0.233 (0.324) loss_u loss_u 0.8921 (0.8892) acc_u 12.5000 (14.2045) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [60/69] time 0.439 (0.452) data 0.307 (0.321) loss_u loss_u 0.7642 (0.8869) acc_u 28.1250 (14.6354) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [65/69] time 0.618 (0.456) data 0.485 (0.325) loss_u loss_u 0.8906 (0.8867) acc_u 12.5000 (14.5192) lr 3.9706e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1751
confident_label rate tensor(0.2832, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 888
clean true:631
clean false:257
clean_rate:0.7105855855855856
noisy true:754
noisy false:1494
after delete: len(clean_dataset) 888
after delete: len(noisy_dataset) 2248
epoch [185/200] batch [5/27] time 0.387 (0.459) data 0.257 (0.328) loss_x loss_x 0.9312 (0.9241) acc_x 71.8750 (75.6250) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [10/27] time 0.479 (0.480) data 0.349 (0.349) loss_x loss_x 0.9307 (1.0923) acc_x 84.3750 (73.4375) lr 3.5443e-05 eta 0:00:08
epoch [185/200] batch [15/27] time 0.457 (0.460) data 0.326 (0.330) loss_x loss_x 0.6211 (1.0773) acc_x 84.3750 (73.7500) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [20/27] time 0.395 (0.452) data 0.265 (0.321) loss_x loss_x 0.8662 (1.0710) acc_x 75.0000 (73.2812) lr 3.5443e-05 eta 0:00:03
epoch [185/200] batch [25/27] time 0.432 (0.444) data 0.302 (0.314) loss_x loss_x 1.4922 (1.1200) acc_x 68.7500 (72.1250) lr 3.5443e-05 eta 0:00:00
epoch [185/200] batch [5/70] time 0.359 (0.444) data 0.228 (0.314) loss_u loss_u 0.9429 (0.9003) acc_u 3.1250 (11.2500) lr 3.5443e-05 eta 0:00:28
epoch [185/200] batch [10/70] time 0.481 (0.444) data 0.349 (0.313) loss_u loss_u 0.8896 (0.8893) acc_u 12.5000 (13.4375) lr 3.5443e-05 eta 0:00:26
epoch [185/200] batch [15/70] time 0.395 (0.440) data 0.265 (0.310) loss_u loss_u 0.9214 (0.8915) acc_u 6.2500 (12.9167) lr 3.5443e-05 eta 0:00:24
epoch [185/200] batch [20/70] time 0.466 (0.445) data 0.335 (0.314) loss_u loss_u 0.8813 (0.8870) acc_u 15.6250 (13.9062) lr 3.5443e-05 eta 0:00:22
epoch [185/200] batch [25/70] time 0.359 (0.442) data 0.229 (0.311) loss_u loss_u 0.8916 (0.8856) acc_u 12.5000 (14.2500) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [30/70] time 0.478 (0.442) data 0.346 (0.311) loss_u loss_u 0.8564 (0.8860) acc_u 18.7500 (14.6875) lr 3.5443e-05 eta 0:00:17
epoch [185/200] batch [35/70] time 0.515 (0.439) data 0.385 (0.309) loss_u loss_u 0.9487 (0.8863) acc_u 9.3750 (14.7321) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [40/70] time 0.371 (0.438) data 0.241 (0.307) loss_u loss_u 0.8809 (0.8854) acc_u 15.6250 (14.6875) lr 3.5443e-05 eta 0:00:13
epoch [185/200] batch [45/70] time 0.387 (0.439) data 0.256 (0.308) loss_u loss_u 0.8672 (0.8846) acc_u 15.6250 (14.7222) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [50/70] time 0.448 (0.442) data 0.318 (0.311) loss_u loss_u 0.8867 (0.8825) acc_u 18.7500 (15.1875) lr 3.5443e-05 eta 0:00:08
epoch [185/200] batch [55/70] time 0.495 (0.441) data 0.364 (0.310) loss_u loss_u 0.8369 (0.8818) acc_u 21.8750 (15.3409) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [60/70] time 0.429 (0.441) data 0.298 (0.310) loss_u loss_u 0.8906 (0.8794) acc_u 9.3750 (15.5729) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [65/70] time 0.517 (0.443) data 0.385 (0.312) loss_u loss_u 0.8384 (0.8788) acc_u 21.8750 (15.7212) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [70/70] time 0.440 (0.442) data 0.310 (0.311) loss_u loss_u 0.8232 (0.8780) acc_u 25.0000 (15.8036) lr 3.5443e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1770
confident_label rate tensor(0.2911, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 913
clean true:655
clean false:258
clean_rate:0.7174151150054765
noisy true:711
noisy false:1512
after delete: len(clean_dataset) 913
after delete: len(noisy_dataset) 2223
epoch [186/200] batch [5/28] time 0.684 (0.526) data 0.553 (0.394) loss_x loss_x 0.8667 (1.0246) acc_x 75.0000 (70.0000) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [10/28] time 0.466 (0.476) data 0.336 (0.345) loss_x loss_x 0.9478 (1.1339) acc_x 75.0000 (69.6875) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [15/28] time 0.362 (0.465) data 0.231 (0.334) loss_x loss_x 0.8506 (1.0980) acc_x 87.5000 (71.4583) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [20/28] time 0.549 (0.475) data 0.417 (0.344) loss_x loss_x 1.0791 (1.0937) acc_x 71.8750 (71.7188) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [25/28] time 0.690 (0.496) data 0.558 (0.365) loss_x loss_x 1.5430 (1.1359) acc_x 65.6250 (71.5000) lr 3.1417e-05 eta 0:00:01
epoch [186/200] batch [5/69] time 0.523 (0.498) data 0.391 (0.366) loss_u loss_u 0.8530 (0.8732) acc_u 21.8750 (18.1250) lr 3.1417e-05 eta 0:00:31
epoch [186/200] batch [10/69] time 0.425 (0.487) data 0.292 (0.355) loss_u loss_u 0.8286 (0.8661) acc_u 21.8750 (17.8125) lr 3.1417e-05 eta 0:00:28
epoch [186/200] batch [15/69] time 0.428 (0.487) data 0.296 (0.355) loss_u loss_u 0.9370 (0.8662) acc_u 3.1250 (17.2917) lr 3.1417e-05 eta 0:00:26
epoch [186/200] batch [20/69] time 0.409 (0.485) data 0.276 (0.353) loss_u loss_u 0.9121 (0.8802) acc_u 6.2500 (14.8438) lr 3.1417e-05 eta 0:00:23
epoch [186/200] batch [25/69] time 0.415 (0.482) data 0.282 (0.350) loss_u loss_u 0.8672 (0.8783) acc_u 18.7500 (14.8750) lr 3.1417e-05 eta 0:00:21
epoch [186/200] batch [30/69] time 0.474 (0.483) data 0.342 (0.350) loss_u loss_u 0.8848 (0.8812) acc_u 12.5000 (14.3750) lr 3.1417e-05 eta 0:00:18
epoch [186/200] batch [35/69] time 0.480 (0.486) data 0.348 (0.354) loss_u loss_u 0.8472 (0.8875) acc_u 18.7500 (13.7500) lr 3.1417e-05 eta 0:00:16
epoch [186/200] batch [40/69] time 0.341 (0.482) data 0.209 (0.350) loss_u loss_u 0.8398 (0.8875) acc_u 9.3750 (13.6719) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [45/69] time 0.469 (0.478) data 0.338 (0.346) loss_u loss_u 0.8789 (0.8877) acc_u 21.8750 (13.8194) lr 3.1417e-05 eta 0:00:11
epoch [186/200] batch [50/69] time 0.383 (0.476) data 0.251 (0.344) loss_u loss_u 0.8770 (0.8845) acc_u 15.6250 (14.3750) lr 3.1417e-05 eta 0:00:09
epoch [186/200] batch [55/69] time 0.402 (0.474) data 0.272 (0.342) loss_u loss_u 0.8691 (0.8831) acc_u 15.6250 (14.5455) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [60/69] time 0.586 (0.475) data 0.456 (0.343) loss_u loss_u 0.8574 (0.8821) acc_u 15.6250 (14.7917) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [65/69] time 0.363 (0.472) data 0.232 (0.340) loss_u loss_u 0.8682 (0.8810) acc_u 12.5000 (14.7115) lr 3.1417e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1765
confident_label rate tensor(0.2860, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 897
clean true:635
clean false:262
clean_rate:0.7079152731326644
noisy true:736
noisy false:1503
after delete: len(clean_dataset) 897
after delete: len(noisy_dataset) 2239
epoch [187/200] batch [5/28] time 0.359 (0.407) data 0.229 (0.276) loss_x loss_x 1.0342 (1.3328) acc_x 65.6250 (63.7500) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [10/28] time 0.432 (0.478) data 0.303 (0.347) loss_x loss_x 1.3203 (1.3119) acc_x 59.3750 (64.6875) lr 2.7630e-05 eta 0:00:08
epoch [187/200] batch [15/28] time 0.380 (0.465) data 0.249 (0.334) loss_x loss_x 1.4707 (1.2932) acc_x 68.7500 (67.2917) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [20/28] time 0.427 (0.462) data 0.296 (0.331) loss_x loss_x 1.7939 (1.2866) acc_x 50.0000 (67.0312) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [25/28] time 0.509 (0.453) data 0.379 (0.322) loss_x loss_x 1.5146 (1.3004) acc_x 65.6250 (67.0000) lr 2.7630e-05 eta 0:00:01
epoch [187/200] batch [5/69] time 0.429 (0.453) data 0.298 (0.322) loss_u loss_u 0.8262 (0.8632) acc_u 25.0000 (16.2500) lr 2.7630e-05 eta 0:00:28
epoch [187/200] batch [10/69] time 0.321 (0.447) data 0.191 (0.316) loss_u loss_u 0.9092 (0.8783) acc_u 9.3750 (15.0000) lr 2.7630e-05 eta 0:00:26
epoch [187/200] batch [15/69] time 0.418 (0.447) data 0.287 (0.316) loss_u loss_u 0.8447 (0.8728) acc_u 18.7500 (16.2500) lr 2.7630e-05 eta 0:00:24
epoch [187/200] batch [20/69] time 0.347 (0.441) data 0.216 (0.311) loss_u loss_u 0.9282 (0.8820) acc_u 6.2500 (15.1562) lr 2.7630e-05 eta 0:00:21
epoch [187/200] batch [25/69] time 0.432 (0.443) data 0.299 (0.312) loss_u loss_u 0.8848 (0.8845) acc_u 18.7500 (15.0000) lr 2.7630e-05 eta 0:00:19
epoch [187/200] batch [30/69] time 0.437 (0.447) data 0.306 (0.317) loss_u loss_u 0.9014 (0.8826) acc_u 9.3750 (15.3125) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [35/69] time 0.461 (0.449) data 0.330 (0.318) loss_u loss_u 0.8940 (0.8795) acc_u 18.7500 (15.7143) lr 2.7630e-05 eta 0:00:15
epoch [187/200] batch [40/69] time 0.412 (0.448) data 0.282 (0.318) loss_u loss_u 0.8838 (0.8803) acc_u 15.6250 (15.7812) lr 2.7630e-05 eta 0:00:12
epoch [187/200] batch [45/69] time 0.495 (0.447) data 0.364 (0.316) loss_u loss_u 0.8994 (0.8826) acc_u 9.3750 (15.2083) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [50/69] time 0.589 (0.448) data 0.456 (0.317) loss_u loss_u 0.9351 (0.8810) acc_u 9.3750 (15.3750) lr 2.7630e-05 eta 0:00:08
epoch [187/200] batch [55/69] time 0.480 (0.451) data 0.349 (0.320) loss_u loss_u 0.9238 (0.8829) acc_u 12.5000 (15.1705) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [60/69] time 0.409 (0.453) data 0.278 (0.322) loss_u loss_u 0.8569 (0.8822) acc_u 21.8750 (15.2604) lr 2.7630e-05 eta 0:00:04
epoch [187/200] batch [65/69] time 0.395 (0.449) data 0.266 (0.318) loss_u loss_u 0.9424 (0.8851) acc_u 3.1250 (14.9038) lr 2.7630e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1731
confident_label rate tensor(0.2959, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 928
clean true:679
clean false:249
clean_rate:0.7316810344827587
noisy true:726
noisy false:1482
after delete: len(clean_dataset) 928
after delete: len(noisy_dataset) 2208
epoch [188/200] batch [5/29] time 0.373 (0.420) data 0.243 (0.290) loss_x loss_x 1.6445 (1.2611) acc_x 68.7500 (71.2500) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [10/29] time 0.435 (0.430) data 0.305 (0.300) loss_x loss_x 0.7944 (1.1491) acc_x 81.2500 (72.8125) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [15/29] time 0.508 (0.441) data 0.378 (0.311) loss_x loss_x 1.1064 (1.1467) acc_x 71.8750 (71.4583) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [20/29] time 0.810 (0.464) data 0.679 (0.334) loss_x loss_x 0.8633 (1.1152) acc_x 68.7500 (71.8750) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [25/29] time 0.460 (0.462) data 0.329 (0.331) loss_x loss_x 0.9224 (1.1323) acc_x 75.0000 (70.8750) lr 2.4083e-05 eta 0:00:01
epoch [188/200] batch [5/69] time 0.419 (0.452) data 0.287 (0.322) loss_u loss_u 0.9512 (0.9147) acc_u 6.2500 (11.2500) lr 2.4083e-05 eta 0:00:28
epoch [188/200] batch [10/69] time 0.520 (0.451) data 0.389 (0.320) loss_u loss_u 0.8579 (0.9076) acc_u 15.6250 (11.2500) lr 2.4083e-05 eta 0:00:26
epoch [188/200] batch [15/69] time 0.428 (0.444) data 0.297 (0.313) loss_u loss_u 0.8691 (0.8962) acc_u 15.6250 (12.5000) lr 2.4083e-05 eta 0:00:23
epoch [188/200] batch [20/69] time 0.534 (0.450) data 0.405 (0.320) loss_u loss_u 0.9155 (0.8971) acc_u 9.3750 (12.1875) lr 2.4083e-05 eta 0:00:22
epoch [188/200] batch [25/69] time 0.402 (0.445) data 0.270 (0.314) loss_u loss_u 0.8447 (0.8878) acc_u 18.7500 (14.0000) lr 2.4083e-05 eta 0:00:19
epoch [188/200] batch [30/69] time 0.419 (0.445) data 0.288 (0.314) loss_u loss_u 0.8682 (0.8887) acc_u 15.6250 (13.5417) lr 2.4083e-05 eta 0:00:17
epoch [188/200] batch [35/69] time 0.461 (0.444) data 0.330 (0.314) loss_u loss_u 0.8662 (0.8871) acc_u 12.5000 (13.7500) lr 2.4083e-05 eta 0:00:15
epoch [188/200] batch [40/69] time 0.452 (0.443) data 0.321 (0.313) loss_u loss_u 0.9316 (0.8863) acc_u 6.2500 (13.9062) lr 2.4083e-05 eta 0:00:12
epoch [188/200] batch [45/69] time 0.394 (0.443) data 0.263 (0.312) loss_u loss_u 0.8711 (0.8855) acc_u 15.6250 (14.0278) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [50/69] time 0.337 (0.438) data 0.206 (0.308) loss_u loss_u 0.8301 (0.8847) acc_u 21.8750 (14.2500) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [55/69] time 0.427 (0.442) data 0.295 (0.311) loss_u loss_u 0.8950 (0.8875) acc_u 12.5000 (13.8636) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [60/69] time 0.369 (0.441) data 0.238 (0.310) loss_u loss_u 0.8110 (0.8878) acc_u 21.8750 (13.8021) lr 2.4083e-05 eta 0:00:03
epoch [188/200] batch [65/69] time 0.507 (0.441) data 0.376 (0.310) loss_u loss_u 0.7969 (0.8847) acc_u 25.0000 (14.1827) lr 2.4083e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1723
confident_label rate tensor(0.2956, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 927
clean true:682
clean false:245
clean_rate:0.7357065803667745
noisy true:731
noisy false:1478
after delete: len(clean_dataset) 927
after delete: len(noisy_dataset) 2209
epoch [189/200] batch [5/28] time 0.493 (0.525) data 0.363 (0.394) loss_x loss_x 1.0312 (1.2859) acc_x 81.2500 (69.3750) lr 2.0777e-05 eta 0:00:12
epoch [189/200] batch [10/28] time 0.550 (0.518) data 0.420 (0.387) loss_x loss_x 1.5762 (1.2550) acc_x 59.3750 (71.2500) lr 2.0777e-05 eta 0:00:09
epoch [189/200] batch [15/28] time 0.437 (0.486) data 0.306 (0.355) loss_x loss_x 0.9355 (1.1865) acc_x 68.7500 (71.4583) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [20/28] time 0.415 (0.465) data 0.285 (0.334) loss_x loss_x 0.9438 (1.2063) acc_x 71.8750 (70.3125) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [25/28] time 0.368 (0.449) data 0.237 (0.319) loss_x loss_x 1.7715 (1.2530) acc_x 59.3750 (69.1250) lr 2.0777e-05 eta 0:00:01
epoch [189/200] batch [5/69] time 0.388 (0.449) data 0.257 (0.318) loss_u loss_u 0.8369 (0.8678) acc_u 21.8750 (17.5000) lr 2.0777e-05 eta 0:00:28
epoch [189/200] batch [10/69] time 0.381 (0.449) data 0.250 (0.318) loss_u loss_u 0.8623 (0.8705) acc_u 15.6250 (16.5625) lr 2.0777e-05 eta 0:00:26
epoch [189/200] batch [15/69] time 0.375 (0.448) data 0.244 (0.318) loss_u loss_u 0.8184 (0.8763) acc_u 18.7500 (15.6250) lr 2.0777e-05 eta 0:00:24
epoch [189/200] batch [20/69] time 0.484 (0.443) data 0.352 (0.313) loss_u loss_u 0.8853 (0.8781) acc_u 12.5000 (15.0000) lr 2.0777e-05 eta 0:00:21
epoch [189/200] batch [25/69] time 0.400 (0.444) data 0.269 (0.313) loss_u loss_u 0.8901 (0.8793) acc_u 15.6250 (15.2500) lr 2.0777e-05 eta 0:00:19
epoch [189/200] batch [30/69] time 0.468 (0.443) data 0.337 (0.312) loss_u loss_u 0.8979 (0.8808) acc_u 9.3750 (15.1042) lr 2.0777e-05 eta 0:00:17
epoch [189/200] batch [35/69] time 0.387 (0.440) data 0.256 (0.309) loss_u loss_u 0.9121 (0.8799) acc_u 3.1250 (15.1786) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [40/69] time 0.383 (0.436) data 0.252 (0.305) loss_u loss_u 0.9302 (0.8814) acc_u 6.2500 (14.9219) lr 2.0777e-05 eta 0:00:12
epoch [189/200] batch [45/69] time 0.344 (0.435) data 0.212 (0.304) loss_u loss_u 0.8984 (0.8824) acc_u 18.7500 (14.9306) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [50/69] time 0.410 (0.437) data 0.279 (0.306) loss_u loss_u 0.8530 (0.8838) acc_u 18.7500 (14.8125) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [55/69] time 0.414 (0.436) data 0.282 (0.305) loss_u loss_u 0.8457 (0.8814) acc_u 18.7500 (15.0000) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [60/69] time 0.414 (0.435) data 0.283 (0.304) loss_u loss_u 0.9004 (0.8811) acc_u 12.5000 (14.8958) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [65/69] time 0.429 (0.437) data 0.298 (0.306) loss_u loss_u 0.8916 (0.8825) acc_u 12.5000 (14.6635) lr 2.0777e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1757
confident_label rate tensor(0.2895, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 908
clean true:659
clean false:249
clean_rate:0.7257709251101322
noisy true:720
noisy false:1508
after delete: len(clean_dataset) 908
after delete: len(noisy_dataset) 2228
epoch [190/200] batch [5/28] time 0.416 (0.497) data 0.285 (0.366) loss_x loss_x 1.2012 (1.2625) acc_x 71.8750 (68.1250) lr 1.7713e-05 eta 0:00:11
epoch [190/200] batch [10/28] time 0.506 (0.471) data 0.373 (0.339) loss_x loss_x 1.0811 (1.2038) acc_x 75.0000 (70.3125) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [15/28] time 0.530 (0.481) data 0.399 (0.350) loss_x loss_x 1.0039 (1.1279) acc_x 75.0000 (71.6667) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [20/28] time 0.428 (0.483) data 0.297 (0.352) loss_x loss_x 1.4766 (1.1396) acc_x 65.6250 (71.2500) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [25/28] time 0.737 (0.491) data 0.605 (0.360) loss_x loss_x 1.0928 (1.1030) acc_x 78.1250 (72.2500) lr 1.7713e-05 eta 0:00:01
epoch [190/200] batch [5/69] time 0.503 (0.479) data 0.372 (0.348) loss_u loss_u 0.8174 (0.8519) acc_u 25.0000 (20.0000) lr 1.7713e-05 eta 0:00:30
epoch [190/200] batch [10/69] time 0.389 (0.472) data 0.259 (0.341) loss_u loss_u 0.9146 (0.8800) acc_u 12.5000 (15.0000) lr 1.7713e-05 eta 0:00:27
epoch [190/200] batch [15/69] time 0.525 (0.469) data 0.394 (0.338) loss_u loss_u 0.8535 (0.8806) acc_u 18.7500 (15.0000) lr 1.7713e-05 eta 0:00:25
epoch [190/200] batch [20/69] time 0.335 (0.464) data 0.204 (0.332) loss_u loss_u 0.8916 (0.8756) acc_u 9.3750 (15.6250) lr 1.7713e-05 eta 0:00:22
epoch [190/200] batch [25/69] time 0.402 (0.461) data 0.271 (0.330) loss_u loss_u 0.9204 (0.8829) acc_u 6.2500 (14.7500) lr 1.7713e-05 eta 0:00:20
epoch [190/200] batch [30/69] time 0.303 (0.456) data 0.172 (0.324) loss_u loss_u 0.9214 (0.8852) acc_u 6.2500 (14.1667) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [35/69] time 0.449 (0.454) data 0.317 (0.323) loss_u loss_u 0.8994 (0.8869) acc_u 9.3750 (13.9286) lr 1.7713e-05 eta 0:00:15
epoch [190/200] batch [40/69] time 0.566 (0.454) data 0.436 (0.322) loss_u loss_u 0.8867 (0.8820) acc_u 12.5000 (14.4531) lr 1.7713e-05 eta 0:00:13
epoch [190/200] batch [45/69] time 0.538 (0.454) data 0.407 (0.323) loss_u loss_u 0.9521 (0.8844) acc_u 3.1250 (13.9583) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [50/69] time 0.389 (0.452) data 0.257 (0.320) loss_u loss_u 0.7969 (0.8830) acc_u 28.1250 (14.3125) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [55/69] time 0.355 (0.452) data 0.224 (0.321) loss_u loss_u 0.8804 (0.8833) acc_u 12.5000 (14.2045) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [60/69] time 0.411 (0.451) data 0.280 (0.319) loss_u loss_u 0.9771 (0.8865) acc_u 0.0000 (13.6979) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [65/69] time 0.376 (0.448) data 0.245 (0.317) loss_u loss_u 0.8535 (0.8860) acc_u 25.0000 (14.0385) lr 1.7713e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1781
confident_label rate tensor(0.2864, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 898
clean true:649
clean false:249
clean_rate:0.72271714922049
noisy true:706
noisy false:1532
after delete: len(clean_dataset) 898
after delete: len(noisy_dataset) 2238
epoch [191/200] batch [5/28] time 0.495 (0.429) data 0.364 (0.298) loss_x loss_x 1.1152 (1.0240) acc_x 75.0000 (73.7500) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [10/28] time 0.388 (0.436) data 0.257 (0.305) loss_x loss_x 1.1318 (0.9966) acc_x 65.6250 (74.0625) lr 1.4891e-05 eta 0:00:07
epoch [191/200] batch [15/28] time 0.449 (0.453) data 0.319 (0.322) loss_x loss_x 1.0312 (0.9982) acc_x 81.2500 (74.5833) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [20/28] time 0.473 (0.453) data 0.343 (0.322) loss_x loss_x 1.4844 (1.0259) acc_x 62.5000 (74.0625) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [25/28] time 0.419 (0.455) data 0.288 (0.325) loss_x loss_x 0.8809 (1.0692) acc_x 81.2500 (73.5000) lr 1.4891e-05 eta 0:00:01
epoch [191/200] batch [5/69] time 0.427 (0.457) data 0.296 (0.326) loss_u loss_u 0.9453 (0.8785) acc_u 3.1250 (13.1250) lr 1.4891e-05 eta 0:00:29
epoch [191/200] batch [10/69] time 0.452 (0.452) data 0.321 (0.321) loss_u loss_u 0.8218 (0.8784) acc_u 25.0000 (15.3125) lr 1.4891e-05 eta 0:00:26
epoch [191/200] batch [15/69] time 0.349 (0.448) data 0.219 (0.317) loss_u loss_u 0.9365 (0.8787) acc_u 9.3750 (15.6250) lr 1.4891e-05 eta 0:00:24
epoch [191/200] batch [20/69] time 0.426 (0.447) data 0.294 (0.317) loss_u loss_u 0.8267 (0.8820) acc_u 21.8750 (14.5312) lr 1.4891e-05 eta 0:00:21
epoch [191/200] batch [25/69] time 0.352 (0.446) data 0.221 (0.315) loss_u loss_u 0.9741 (0.8880) acc_u 3.1250 (13.8750) lr 1.4891e-05 eta 0:00:19
epoch [191/200] batch [30/69] time 0.500 (0.442) data 0.370 (0.312) loss_u loss_u 0.9624 (0.8894) acc_u 3.1250 (13.7500) lr 1.4891e-05 eta 0:00:17
epoch [191/200] batch [35/69] time 0.600 (0.444) data 0.469 (0.313) loss_u loss_u 0.7959 (0.8853) acc_u 21.8750 (14.1071) lr 1.4891e-05 eta 0:00:15
epoch [191/200] batch [40/69] time 0.574 (0.449) data 0.443 (0.318) loss_u loss_u 0.8569 (0.8864) acc_u 15.6250 (14.0625) lr 1.4891e-05 eta 0:00:13
epoch [191/200] batch [45/69] time 0.382 (0.449) data 0.251 (0.319) loss_u loss_u 0.9258 (0.8880) acc_u 6.2500 (13.6806) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [50/69] time 0.386 (0.445) data 0.256 (0.314) loss_u loss_u 0.8701 (0.8865) acc_u 18.7500 (13.9375) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [55/69] time 0.384 (0.442) data 0.254 (0.311) loss_u loss_u 0.8599 (0.8852) acc_u 15.6250 (14.0909) lr 1.4891e-05 eta 0:00:06
epoch [191/200] batch [60/69] time 0.372 (0.441) data 0.242 (0.310) loss_u loss_u 0.8911 (0.8840) acc_u 9.3750 (14.0104) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [65/69] time 0.431 (0.442) data 0.301 (0.311) loss_u loss_u 0.9268 (0.8839) acc_u 6.2500 (14.0385) lr 1.4891e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1735
confident_label rate tensor(0.2946, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 924
clean true:668
clean false:256
clean_rate:0.7229437229437229
noisy true:733
noisy false:1479
after delete: len(clean_dataset) 924
after delete: len(noisy_dataset) 2212
epoch [192/200] batch [5/28] time 0.514 (0.432) data 0.383 (0.301) loss_x loss_x 0.8003 (1.1812) acc_x 90.6250 (71.8750) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [10/28] time 0.406 (0.444) data 0.275 (0.313) loss_x loss_x 1.0400 (1.2686) acc_x 71.8750 (66.5625) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [15/28] time 0.368 (0.445) data 0.238 (0.314) loss_x loss_x 0.8384 (1.1759) acc_x 81.2500 (69.1667) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [20/28] time 0.425 (0.443) data 0.294 (0.313) loss_x loss_x 1.4707 (1.1575) acc_x 62.5000 (70.6250) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [25/28] time 0.541 (0.457) data 0.409 (0.326) loss_x loss_x 1.1338 (1.1388) acc_x 81.2500 (72.5000) lr 1.2312e-05 eta 0:00:01
epoch [192/200] batch [5/69] time 0.506 (0.458) data 0.375 (0.327) loss_u loss_u 0.9326 (0.9150) acc_u 9.3750 (9.3750) lr 1.2312e-05 eta 0:00:29
epoch [192/200] batch [10/69] time 0.517 (0.459) data 0.385 (0.328) loss_u loss_u 0.8506 (0.8947) acc_u 18.7500 (12.8125) lr 1.2312e-05 eta 0:00:27
epoch [192/200] batch [15/69] time 0.391 (0.455) data 0.260 (0.324) loss_u loss_u 0.9507 (0.8910) acc_u 9.3750 (13.5417) lr 1.2312e-05 eta 0:00:24
epoch [192/200] batch [20/69] time 0.422 (0.458) data 0.292 (0.327) loss_u loss_u 0.8960 (0.8903) acc_u 12.5000 (13.2812) lr 1.2312e-05 eta 0:00:22
epoch [192/200] batch [25/69] time 0.546 (0.455) data 0.416 (0.324) loss_u loss_u 0.8696 (0.8871) acc_u 15.6250 (14.2500) lr 1.2312e-05 eta 0:00:20
epoch [192/200] batch [30/69] time 0.637 (0.456) data 0.506 (0.326) loss_u loss_u 0.9380 (0.8860) acc_u 9.3750 (13.9583) lr 1.2312e-05 eta 0:00:17
epoch [192/200] batch [35/69] time 0.452 (0.457) data 0.320 (0.326) loss_u loss_u 0.9189 (0.8862) acc_u 15.6250 (14.2857) lr 1.2312e-05 eta 0:00:15
epoch [192/200] batch [40/69] time 0.450 (0.452) data 0.319 (0.322) loss_u loss_u 0.9199 (0.8913) acc_u 9.3750 (13.5156) lr 1.2312e-05 eta 0:00:13
epoch [192/200] batch [45/69] time 0.405 (0.452) data 0.273 (0.321) loss_u loss_u 0.9229 (0.8935) acc_u 9.3750 (13.1944) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [50/69] time 0.450 (0.451) data 0.319 (0.320) loss_u loss_u 0.8525 (0.8931) acc_u 15.6250 (13.2500) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [55/69] time 0.405 (0.448) data 0.274 (0.317) loss_u loss_u 0.9092 (0.8919) acc_u 12.5000 (13.2955) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [60/69] time 0.415 (0.448) data 0.283 (0.317) loss_u loss_u 0.8486 (0.8911) acc_u 21.8750 (13.4375) lr 1.2312e-05 eta 0:00:04
epoch [192/200] batch [65/69] time 0.541 (0.449) data 0.410 (0.318) loss_u loss_u 0.9355 (0.8934) acc_u 9.3750 (13.2692) lr 1.2312e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1712
confident_label rate tensor(0.2930, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 919
clean true:672
clean false:247
clean_rate:0.7312295973884657
noisy true:752
noisy false:1465
after delete: len(clean_dataset) 919
after delete: len(noisy_dataset) 2217
epoch [193/200] batch [5/28] time 0.434 (0.491) data 0.303 (0.360) loss_x loss_x 1.2021 (1.0793) acc_x 68.7500 (73.7500) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [10/28] time 0.415 (0.474) data 0.284 (0.343) loss_x loss_x 1.5986 (1.1168) acc_x 65.6250 (71.8750) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [15/28] time 0.432 (0.470) data 0.302 (0.340) loss_x loss_x 0.9702 (1.0686) acc_x 81.2500 (74.3750) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [20/28] time 0.444 (0.464) data 0.313 (0.333) loss_x loss_x 1.2988 (1.0963) acc_x 65.6250 (73.1250) lr 9.9763e-06 eta 0:00:03
epoch [193/200] batch [25/28] time 0.487 (0.470) data 0.356 (0.340) loss_x loss_x 1.3281 (1.1092) acc_x 62.5000 (71.6250) lr 9.9763e-06 eta 0:00:01
epoch [193/200] batch [5/69] time 0.362 (0.451) data 0.231 (0.321) loss_u loss_u 0.8682 (0.8855) acc_u 12.5000 (15.0000) lr 9.9763e-06 eta 0:00:28
epoch [193/200] batch [10/69] time 0.501 (0.462) data 0.369 (0.331) loss_u loss_u 0.8232 (0.8719) acc_u 21.8750 (15.9375) lr 9.9763e-06 eta 0:00:27
epoch [193/200] batch [15/69] time 0.391 (0.454) data 0.258 (0.323) loss_u loss_u 0.8345 (0.8748) acc_u 21.8750 (15.8333) lr 9.9763e-06 eta 0:00:24
epoch [193/200] batch [20/69] time 0.521 (0.455) data 0.389 (0.324) loss_u loss_u 0.8955 (0.8763) acc_u 15.6250 (15.9375) lr 9.9763e-06 eta 0:00:22
epoch [193/200] batch [25/69] time 0.382 (0.447) data 0.251 (0.316) loss_u loss_u 0.9062 (0.8757) acc_u 15.6250 (15.7500) lr 9.9763e-06 eta 0:00:19
epoch [193/200] batch [30/69] time 0.601 (0.450) data 0.469 (0.319) loss_u loss_u 0.9326 (0.8750) acc_u 9.3750 (15.6250) lr 9.9763e-06 eta 0:00:17
epoch [193/200] batch [35/69] time 0.487 (0.453) data 0.356 (0.322) loss_u loss_u 0.9185 (0.8791) acc_u 9.3750 (15.1786) lr 9.9763e-06 eta 0:00:15
epoch [193/200] batch [40/69] time 0.427 (0.452) data 0.296 (0.321) loss_u loss_u 0.9102 (0.8828) acc_u 12.5000 (14.7656) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [45/69] time 0.529 (0.452) data 0.397 (0.321) loss_u loss_u 0.8574 (0.8828) acc_u 12.5000 (14.6528) lr 9.9763e-06 eta 0:00:10
epoch [193/200] batch [50/69] time 0.448 (0.452) data 0.316 (0.320) loss_u loss_u 0.9146 (0.8851) acc_u 9.3750 (14.3125) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [55/69] time 0.374 (0.448) data 0.242 (0.317) loss_u loss_u 0.9331 (0.8859) acc_u 9.3750 (14.3750) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [60/69] time 0.588 (0.447) data 0.456 (0.316) loss_u loss_u 0.8257 (0.8861) acc_u 18.7500 (14.3750) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [65/69] time 0.445 (0.446) data 0.313 (0.315) loss_u loss_u 0.7847 (0.8837) acc_u 28.1250 (14.6635) lr 9.9763e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1755
confident_label rate tensor(0.2886, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 905
clean true:656
clean false:249
clean_rate:0.7248618784530386
noisy true:725
noisy false:1506
after delete: len(clean_dataset) 905
after delete: len(noisy_dataset) 2231
epoch [194/200] batch [5/28] time 0.437 (0.444) data 0.307 (0.314) loss_x loss_x 1.2422 (1.1357) acc_x 68.7500 (70.0000) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [10/28] time 0.622 (0.487) data 0.492 (0.357) loss_x loss_x 0.7422 (1.1166) acc_x 81.2500 (70.0000) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [15/28] time 0.392 (0.467) data 0.261 (0.337) loss_x loss_x 1.0625 (1.0642) acc_x 71.8750 (71.4583) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [20/28] time 0.559 (0.464) data 0.428 (0.333) loss_x loss_x 0.9697 (1.0903) acc_x 78.1250 (71.8750) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [25/28] time 0.336 (0.454) data 0.206 (0.324) loss_x loss_x 1.3369 (1.1175) acc_x 68.7500 (71.1250) lr 7.8853e-06 eta 0:00:01
epoch [194/200] batch [5/69] time 0.374 (0.446) data 0.243 (0.315) loss_u loss_u 0.8877 (0.8761) acc_u 15.6250 (15.0000) lr 7.8853e-06 eta 0:00:28
epoch [194/200] batch [10/69] time 0.390 (0.448) data 0.260 (0.317) loss_u loss_u 0.9209 (0.8817) acc_u 15.6250 (15.3125) lr 7.8853e-06 eta 0:00:26
epoch [194/200] batch [15/69] time 0.431 (0.441) data 0.301 (0.311) loss_u loss_u 0.9165 (0.8837) acc_u 15.6250 (15.4167) lr 7.8853e-06 eta 0:00:23
epoch [194/200] batch [20/69] time 0.424 (0.441) data 0.294 (0.310) loss_u loss_u 0.9678 (0.8921) acc_u 3.1250 (13.7500) lr 7.8853e-06 eta 0:00:21
epoch [194/200] batch [25/69] time 0.433 (0.438) data 0.301 (0.308) loss_u loss_u 0.9131 (0.8897) acc_u 15.6250 (14.1250) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [30/69] time 0.480 (0.438) data 0.349 (0.307) loss_u loss_u 0.9185 (0.8894) acc_u 9.3750 (14.1667) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [35/69] time 0.422 (0.437) data 0.291 (0.306) loss_u loss_u 0.8813 (0.8893) acc_u 9.3750 (13.8393) lr 7.8853e-06 eta 0:00:14
epoch [194/200] batch [40/69] time 0.405 (0.433) data 0.273 (0.303) loss_u loss_u 0.8647 (0.8884) acc_u 18.7500 (14.2969) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [45/69] time 0.428 (0.431) data 0.297 (0.301) loss_u loss_u 0.9414 (0.8906) acc_u 3.1250 (13.7500) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [50/69] time 0.475 (0.431) data 0.343 (0.300) loss_u loss_u 0.8882 (0.8875) acc_u 9.3750 (14.1250) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [55/69] time 0.519 (0.432) data 0.388 (0.302) loss_u loss_u 0.9546 (0.8888) acc_u 6.2500 (13.9773) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [60/69] time 0.652 (0.435) data 0.521 (0.304) loss_u loss_u 0.8926 (0.8901) acc_u 18.7500 (13.8542) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [65/69] time 0.493 (0.435) data 0.362 (0.305) loss_u loss_u 0.8389 (0.8912) acc_u 18.7500 (13.6538) lr 7.8853e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1745
confident_label rate tensor(0.2905, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 911
clean true:658
clean false:253
clean_rate:0.7222832052689352
noisy true:733
noisy false:1492
after delete: len(clean_dataset) 911
after delete: len(noisy_dataset) 2225
epoch [195/200] batch [5/28] time 0.437 (0.417) data 0.307 (0.287) loss_x loss_x 1.1260 (1.1847) acc_x 71.8750 (67.5000) lr 6.0390e-06 eta 0:00:09
epoch [195/200] batch [10/28] time 0.504 (0.427) data 0.373 (0.297) loss_x loss_x 0.9414 (1.1343) acc_x 71.8750 (70.9375) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [15/28] time 0.475 (0.431) data 0.345 (0.301) loss_x loss_x 1.1367 (1.1300) acc_x 68.7500 (71.2500) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [20/28] time 0.408 (0.429) data 0.278 (0.299) loss_x loss_x 0.8462 (1.1335) acc_x 75.0000 (70.1562) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [25/28] time 0.618 (0.448) data 0.487 (0.318) loss_x loss_x 0.7720 (1.1499) acc_x 75.0000 (69.7500) lr 6.0390e-06 eta 0:00:01
epoch [195/200] batch [5/69] time 0.369 (0.456) data 0.238 (0.326) loss_u loss_u 0.8584 (0.8794) acc_u 18.7500 (15.0000) lr 6.0390e-06 eta 0:00:29
epoch [195/200] batch [10/69] time 0.424 (0.453) data 0.293 (0.323) loss_u loss_u 0.8984 (0.8708) acc_u 12.5000 (15.9375) lr 6.0390e-06 eta 0:00:26
epoch [195/200] batch [15/69] time 0.439 (0.450) data 0.307 (0.319) loss_u loss_u 0.8599 (0.8780) acc_u 15.6250 (14.7917) lr 6.0390e-06 eta 0:00:24
epoch [195/200] batch [20/69] time 0.457 (0.442) data 0.321 (0.311) loss_u loss_u 0.8750 (0.8754) acc_u 15.6250 (15.0000) lr 6.0390e-06 eta 0:00:21
epoch [195/200] batch [25/69] time 0.456 (0.443) data 0.326 (0.312) loss_u loss_u 0.9790 (0.8814) acc_u 0.0000 (14.3750) lr 6.0390e-06 eta 0:00:19
epoch [195/200] batch [30/69] time 0.406 (0.441) data 0.274 (0.310) loss_u loss_u 0.8984 (0.8850) acc_u 15.6250 (14.4792) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [35/69] time 0.451 (0.442) data 0.319 (0.311) loss_u loss_u 0.9224 (0.8875) acc_u 9.3750 (14.3750) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [40/69] time 0.407 (0.439) data 0.275 (0.308) loss_u loss_u 0.8643 (0.8888) acc_u 15.6250 (14.1406) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [45/69] time 0.700 (0.441) data 0.567 (0.310) loss_u loss_u 0.8574 (0.8897) acc_u 25.0000 (14.0972) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [50/69] time 0.374 (0.443) data 0.243 (0.312) loss_u loss_u 0.8638 (0.8915) acc_u 15.6250 (13.6875) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [55/69] time 0.418 (0.444) data 0.287 (0.313) loss_u loss_u 0.8442 (0.8913) acc_u 18.7500 (13.6364) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [60/69] time 0.346 (0.442) data 0.215 (0.311) loss_u loss_u 0.8271 (0.8895) acc_u 25.0000 (13.9062) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [65/69] time 0.477 (0.442) data 0.346 (0.311) loss_u loss_u 0.7695 (0.8889) acc_u 18.7500 (13.7981) lr 6.0390e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1714
confident_label rate tensor(0.2886, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 905
clean true:670
clean false:235
clean_rate:0.7403314917127072
noisy true:752
noisy false:1479
after delete: len(clean_dataset) 905
after delete: len(noisy_dataset) 2231
epoch [196/200] batch [5/28] time 0.450 (0.479) data 0.320 (0.349) loss_x loss_x 1.0254 (0.9812) acc_x 75.0000 (75.0000) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [10/28] time 0.434 (0.473) data 0.304 (0.343) loss_x loss_x 0.9829 (1.0938) acc_x 71.8750 (72.1875) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [15/28] time 0.422 (0.465) data 0.293 (0.334) loss_x loss_x 1.1240 (1.0674) acc_x 68.7500 (73.1250) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [20/28] time 0.565 (0.470) data 0.434 (0.339) loss_x loss_x 1.1582 (1.1087) acc_x 75.0000 (71.8750) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [25/28] time 0.457 (0.456) data 0.327 (0.325) loss_x loss_x 2.3320 (1.1711) acc_x 53.1250 (70.7500) lr 4.4380e-06 eta 0:00:01
epoch [196/200] batch [5/69] time 0.346 (0.439) data 0.215 (0.309) loss_u loss_u 0.8403 (0.8659) acc_u 18.7500 (16.2500) lr 4.4380e-06 eta 0:00:28
epoch [196/200] batch [10/69] time 0.616 (0.445) data 0.485 (0.315) loss_u loss_u 0.9053 (0.8815) acc_u 9.3750 (13.7500) lr 4.4380e-06 eta 0:00:26
epoch [196/200] batch [15/69] time 0.377 (0.439) data 0.247 (0.308) loss_u loss_u 0.9146 (0.8847) acc_u 12.5000 (14.1667) lr 4.4380e-06 eta 0:00:23
epoch [196/200] batch [20/69] time 0.487 (0.436) data 0.356 (0.306) loss_u loss_u 0.8789 (0.8773) acc_u 21.8750 (15.1562) lr 4.4380e-06 eta 0:00:21
epoch [196/200] batch [25/69] time 0.443 (0.438) data 0.313 (0.307) loss_u loss_u 0.8843 (0.8808) acc_u 12.5000 (14.1250) lr 4.4380e-06 eta 0:00:19
epoch [196/200] batch [30/69] time 0.346 (0.435) data 0.215 (0.304) loss_u loss_u 0.8521 (0.8815) acc_u 18.7500 (14.3750) lr 4.4380e-06 eta 0:00:16
epoch [196/200] batch [35/69] time 0.352 (0.433) data 0.221 (0.303) loss_u loss_u 0.9243 (0.8788) acc_u 12.5000 (15.0893) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [40/69] time 0.542 (0.440) data 0.411 (0.309) loss_u loss_u 0.9551 (0.8815) acc_u 3.1250 (14.6094) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [45/69] time 0.349 (0.437) data 0.217 (0.306) loss_u loss_u 0.9028 (0.8843) acc_u 12.5000 (14.3056) lr 4.4380e-06 eta 0:00:10
epoch [196/200] batch [50/69] time 0.351 (0.436) data 0.220 (0.305) loss_u loss_u 0.9272 (0.8876) acc_u 6.2500 (13.7500) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [55/69] time 0.459 (0.439) data 0.328 (0.308) loss_u loss_u 0.9004 (0.8858) acc_u 12.5000 (14.1477) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [60/69] time 0.481 (0.447) data 0.350 (0.316) loss_u loss_u 0.8188 (0.8851) acc_u 31.2500 (14.3750) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [65/69] time 0.588 (0.452) data 0.457 (0.321) loss_u loss_u 0.9126 (0.8852) acc_u 9.3750 (14.3269) lr 4.4380e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1790
confident_label rate tensor(0.2908, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 912
clean true:651
clean false:261
clean_rate:0.7138157894736842
noisy true:695
noisy false:1529
after delete: len(clean_dataset) 912
after delete: len(noisy_dataset) 2224
epoch [197/200] batch [5/28] time 0.413 (0.490) data 0.283 (0.360) loss_x loss_x 0.9917 (1.2434) acc_x 71.8750 (68.7500) lr 3.0827e-06 eta 0:00:11
epoch [197/200] batch [10/28] time 0.344 (0.446) data 0.214 (0.316) loss_x loss_x 1.1377 (1.2725) acc_x 68.7500 (68.4375) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [15/28] time 0.349 (0.441) data 0.219 (0.310) loss_x loss_x 0.9790 (1.2108) acc_x 84.3750 (70.8333) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [20/28] time 0.407 (0.437) data 0.276 (0.307) loss_x loss_x 1.3145 (1.1815) acc_x 78.1250 (72.3438) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [25/28] time 0.592 (0.433) data 0.461 (0.302) loss_x loss_x 1.2539 (1.1855) acc_x 62.5000 (71.8750) lr 3.0827e-06 eta 0:00:01
epoch [197/200] batch [5/69] time 0.493 (0.439) data 0.362 (0.308) loss_u loss_u 0.8794 (0.8753) acc_u 18.7500 (15.6250) lr 3.0827e-06 eta 0:00:28
epoch [197/200] batch [10/69] time 0.443 (0.434) data 0.311 (0.304) loss_u loss_u 0.9053 (0.8863) acc_u 12.5000 (14.6875) lr 3.0827e-06 eta 0:00:25
epoch [197/200] batch [15/69] time 0.457 (0.435) data 0.326 (0.304) loss_u loss_u 0.8916 (0.8922) acc_u 15.6250 (13.9583) lr 3.0827e-06 eta 0:00:23
epoch [197/200] batch [20/69] time 0.460 (0.433) data 0.328 (0.303) loss_u loss_u 0.8711 (0.8902) acc_u 18.7500 (14.2188) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [25/69] time 0.384 (0.434) data 0.252 (0.303) loss_u loss_u 0.9067 (0.8957) acc_u 9.3750 (13.3750) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [30/69] time 0.468 (0.436) data 0.336 (0.305) loss_u loss_u 0.8433 (0.8940) acc_u 15.6250 (13.5417) lr 3.0827e-06 eta 0:00:17
epoch [197/200] batch [35/69] time 0.500 (0.439) data 0.368 (0.308) loss_u loss_u 0.9111 (0.8954) acc_u 15.6250 (13.3929) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [40/69] time 0.443 (0.440) data 0.310 (0.309) loss_u loss_u 0.8389 (0.8894) acc_u 25.0000 (14.1406) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [45/69] time 0.372 (0.442) data 0.240 (0.311) loss_u loss_u 0.8711 (0.8877) acc_u 12.5000 (14.1667) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [50/69] time 0.390 (0.442) data 0.258 (0.311) loss_u loss_u 0.8516 (0.8852) acc_u 18.7500 (14.3750) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [55/69] time 0.446 (0.440) data 0.312 (0.309) loss_u loss_u 0.9121 (0.8877) acc_u 9.3750 (13.9205) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [60/69] time 0.460 (0.440) data 0.330 (0.309) loss_u loss_u 0.8696 (0.8899) acc_u 18.7500 (13.6458) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [65/69] time 0.374 (0.440) data 0.242 (0.309) loss_u loss_u 0.8950 (0.8899) acc_u 15.6250 (13.5577) lr 3.0827e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1787
confident_label rate tensor(0.2886, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 905
clean true:653
clean false:252
clean_rate:0.7215469613259669
noisy true:696
noisy false:1535
after delete: len(clean_dataset) 905
after delete: len(noisy_dataset) 2231
epoch [198/200] batch [5/28] time 0.644 (0.494) data 0.514 (0.363) loss_x loss_x 0.9854 (1.0479) acc_x 71.8750 (70.6250) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [10/28] time 0.396 (0.467) data 0.264 (0.336) loss_x loss_x 0.7617 (1.0315) acc_x 81.2500 (72.8125) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [15/28] time 0.492 (0.471) data 0.362 (0.340) loss_x loss_x 1.0215 (1.0552) acc_x 78.1250 (72.7083) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [20/28] time 0.542 (0.471) data 0.413 (0.340) loss_x loss_x 1.1572 (1.0602) acc_x 78.1250 (73.2812) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [25/28] time 0.412 (0.458) data 0.283 (0.327) loss_x loss_x 1.0488 (1.0605) acc_x 78.1250 (74.3750) lr 1.9733e-06 eta 0:00:01
epoch [198/200] batch [5/69] time 0.363 (0.454) data 0.232 (0.323) loss_u loss_u 0.8906 (0.9062) acc_u 12.5000 (11.2500) lr 1.9733e-06 eta 0:00:29
epoch [198/200] batch [10/69] time 0.449 (0.454) data 0.318 (0.323) loss_u loss_u 0.8472 (0.8946) acc_u 21.8750 (12.8125) lr 1.9733e-06 eta 0:00:26
epoch [198/200] batch [15/69] time 0.547 (0.454) data 0.417 (0.323) loss_u loss_u 0.8740 (0.8886) acc_u 18.7500 (13.7500) lr 1.9733e-06 eta 0:00:24
epoch [198/200] batch [20/69] time 0.439 (0.454) data 0.309 (0.323) loss_u loss_u 0.8164 (0.8828) acc_u 28.1250 (15.1562) lr 1.9733e-06 eta 0:00:22
epoch [198/200] batch [25/69] time 0.383 (0.449) data 0.251 (0.318) loss_u loss_u 0.8398 (0.8825) acc_u 28.1250 (15.3750) lr 1.9733e-06 eta 0:00:19
epoch [198/200] batch [30/69] time 0.331 (0.451) data 0.200 (0.320) loss_u loss_u 0.9243 (0.8884) acc_u 12.5000 (14.5833) lr 1.9733e-06 eta 0:00:17
epoch [198/200] batch [35/69] time 0.475 (0.448) data 0.343 (0.317) loss_u loss_u 0.8716 (0.8864) acc_u 15.6250 (14.6429) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [40/69] time 0.401 (0.448) data 0.270 (0.317) loss_u loss_u 0.9229 (0.8860) acc_u 12.5000 (14.7656) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [45/69] time 0.467 (0.446) data 0.336 (0.315) loss_u loss_u 0.8628 (0.8865) acc_u 15.6250 (14.7222) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [50/69] time 0.380 (0.448) data 0.248 (0.317) loss_u loss_u 0.8584 (0.8859) acc_u 18.7500 (14.6875) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [55/69] time 0.315 (0.443) data 0.183 (0.312) loss_u loss_u 0.8174 (0.8843) acc_u 25.0000 (14.8295) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [60/69] time 0.497 (0.443) data 0.366 (0.312) loss_u loss_u 0.8647 (0.8857) acc_u 21.8750 (14.6875) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [65/69] time 0.484 (0.444) data 0.353 (0.313) loss_u loss_u 0.9082 (0.8860) acc_u 9.3750 (14.6154) lr 1.9733e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1753
confident_label rate tensor(0.2956, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 927
clean true:681
clean false:246
clean_rate:0.7346278317152104
noisy true:702
noisy false:1507
after delete: len(clean_dataset) 927
after delete: len(noisy_dataset) 2209
epoch [199/200] batch [5/28] time 0.370 (0.465) data 0.239 (0.334) loss_x loss_x 1.0332 (1.0903) acc_x 78.1250 (76.8750) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [10/28] time 0.435 (0.474) data 0.305 (0.343) loss_x loss_x 1.1719 (1.1674) acc_x 71.8750 (73.7500) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [15/28] time 0.410 (0.460) data 0.279 (0.329) loss_x loss_x 1.5850 (1.1258) acc_x 59.3750 (74.1667) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [20/28] time 0.402 (0.451) data 0.271 (0.320) loss_x loss_x 1.5908 (1.1375) acc_x 62.5000 (74.2188) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [25/28] time 0.454 (0.454) data 0.323 (0.323) loss_x loss_x 1.4736 (1.1359) acc_x 68.7500 (73.8750) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [5/69] time 0.437 (0.460) data 0.306 (0.329) loss_u loss_u 0.8501 (0.8842) acc_u 18.7500 (15.0000) lr 1.1101e-06 eta 0:00:29
epoch [199/200] batch [10/69] time 0.402 (0.469) data 0.271 (0.338) loss_u loss_u 0.8765 (0.8906) acc_u 15.6250 (14.3750) lr 1.1101e-06 eta 0:00:27
epoch [199/200] batch [15/69] time 0.657 (0.464) data 0.525 (0.333) loss_u loss_u 0.8813 (0.8958) acc_u 15.6250 (13.5417) lr 1.1101e-06 eta 0:00:25
epoch [199/200] batch [20/69] time 0.343 (0.455) data 0.212 (0.324) loss_u loss_u 0.8213 (0.8896) acc_u 21.8750 (14.5312) lr 1.1101e-06 eta 0:00:22
epoch [199/200] batch [25/69] time 0.511 (0.454) data 0.379 (0.323) loss_u loss_u 0.8999 (0.8924) acc_u 18.7500 (13.8750) lr 1.1101e-06 eta 0:00:19
epoch [199/200] batch [30/69] time 0.537 (0.455) data 0.405 (0.324) loss_u loss_u 0.9683 (0.8944) acc_u 6.2500 (13.6458) lr 1.1101e-06 eta 0:00:17
epoch [199/200] batch [35/69] time 0.496 (0.455) data 0.364 (0.323) loss_u loss_u 0.9443 (0.8951) acc_u 6.2500 (13.4821) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [40/69] time 0.508 (0.458) data 0.376 (0.326) loss_u loss_u 0.9175 (0.8908) acc_u 9.3750 (13.7500) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [45/69] time 0.381 (0.460) data 0.249 (0.329) loss_u loss_u 0.9175 (0.8911) acc_u 12.5000 (13.6806) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [50/69] time 0.359 (0.459) data 0.227 (0.327) loss_u loss_u 0.9419 (0.8898) acc_u 6.2500 (13.6875) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [55/69] time 0.429 (0.458) data 0.298 (0.326) loss_u loss_u 0.9185 (0.8927) acc_u 6.2500 (13.2386) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [60/69] time 0.530 (0.457) data 0.397 (0.325) loss_u loss_u 0.8838 (0.8942) acc_u 18.7500 (13.0208) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [65/69] time 0.508 (0.456) data 0.376 (0.325) loss_u loss_u 0.8877 (0.8935) acc_u 18.7500 (13.1250) lr 1.1101e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1725
confident_label rate tensor(0.2930, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 919
clean true:667
clean false:252
clean_rate:0.7257889009793254
noisy true:744
noisy false:1473
after delete: len(clean_dataset) 919
after delete: len(noisy_dataset) 2217
epoch [200/200] batch [5/28] time 0.325 (0.463) data 0.194 (0.332) loss_x loss_x 0.9688 (1.1057) acc_x 71.8750 (70.0000) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [10/28] time 0.415 (0.457) data 0.286 (0.326) loss_x loss_x 1.0381 (1.1491) acc_x 78.1250 (71.2500) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [15/28] time 0.351 (0.449) data 0.220 (0.319) loss_x loss_x 1.0059 (1.0885) acc_x 81.2500 (72.0833) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [20/28] time 0.475 (0.454) data 0.344 (0.323) loss_x loss_x 1.3564 (1.0829) acc_x 71.8750 (72.8125) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [25/28] time 0.638 (0.469) data 0.508 (0.339) loss_x loss_x 1.1865 (1.0892) acc_x 71.8750 (72.2500) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [5/69] time 0.385 (0.460) data 0.254 (0.329) loss_u loss_u 0.7925 (0.8809) acc_u 25.0000 (15.6250) lr 4.9344e-07 eta 0:00:29
epoch [200/200] batch [10/69] time 0.418 (0.456) data 0.283 (0.326) loss_u loss_u 0.8398 (0.8746) acc_u 15.6250 (15.9375) lr 4.9344e-07 eta 0:00:26
epoch [200/200] batch [15/69] time 0.459 (0.458) data 0.326 (0.327) loss_u loss_u 0.8936 (0.8819) acc_u 15.6250 (14.7917) lr 4.9344e-07 eta 0:00:24
epoch [200/200] batch [20/69] time 0.440 (0.458) data 0.309 (0.327) loss_u loss_u 0.9346 (0.8846) acc_u 9.3750 (14.3750) lr 4.9344e-07 eta 0:00:22
epoch [200/200] batch [25/69] time 0.358 (0.452) data 0.226 (0.321) loss_u loss_u 0.8608 (0.8781) acc_u 15.6250 (14.8750) lr 4.9344e-07 eta 0:00:19
epoch [200/200] batch [30/69] time 0.549 (0.451) data 0.417 (0.319) loss_u loss_u 0.9316 (0.8805) acc_u 9.3750 (14.6875) lr 4.9344e-07 eta 0:00:17
epoch [200/200] batch [35/69] time 0.398 (0.448) data 0.267 (0.317) loss_u loss_u 0.9087 (0.8793) acc_u 12.5000 (15.1786) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [40/69] time 0.360 (0.444) data 0.229 (0.313) loss_u loss_u 0.8945 (0.8808) acc_u 12.5000 (15.0000) lr 4.9344e-07 eta 0:00:12
epoch [200/200] batch [45/69] time 0.432 (0.443) data 0.301 (0.312) loss_u loss_u 0.9370 (0.8826) acc_u 6.2500 (14.9306) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [50/69] time 0.403 (0.441) data 0.272 (0.310) loss_u loss_u 0.8730 (0.8831) acc_u 21.8750 (15.1875) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [55/69] time 0.428 (0.443) data 0.297 (0.312) loss_u loss_u 0.8418 (0.8848) acc_u 18.7500 (14.8864) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [60/69] time 0.373 (0.442) data 0.242 (0.311) loss_u loss_u 0.8540 (0.8829) acc_u 18.7500 (15.0521) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [65/69] time 0.437 (0.442) data 0.306 (0.311) loss_u loss_u 0.8901 (0.8848) acc_u 9.3750 (14.8558) lr 4.9344e-07 eta 0:00:01
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.625/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 4,176
* accuracy: 51.9%
* error: 48.1%
* macro_f1: 49.5%
Elapsed: 5:01:44
