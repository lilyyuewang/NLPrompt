***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.625', 'DATASET.NOISE_TYPE', 'sym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.625/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.625
  NOISE_TYPE: sym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.625/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.625/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2574
confident_label rate tensor(0.0733, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 230
clean true:216
clean false:14
clean_rate:0.9391304347826087
noisy true:346
noisy false:2560
after delete: len(clean_dataset) 230
after delete: len(noisy_dataset) 2906
epoch [1/200] batch [5/7] time 0.641 (0.704) data 0.378 (0.427) loss_x loss_x 2.6992 (2.9805) acc_x 43.7500 (40.6250) lr 1.0000e-05 eta 0:00:01
epoch [1/200] batch [5/90] time 0.521 (0.590) data 0.242 (0.327) loss_u loss_u 0.9873 (0.9812) acc_u 3.1250 (6.2500) lr 1.0000e-05 eta 0:00:50
epoch [1/200] batch [10/90] time 0.537 (0.574) data 0.261 (0.308) loss_u loss_u 0.9746 (0.9797) acc_u 9.3750 (5.3125) lr 1.0000e-05 eta 0:00:45
epoch [1/200] batch [15/90] time 0.527 (0.574) data 0.330 (0.310) loss_u loss_u 0.9722 (0.9781) acc_u 12.5000 (5.8333) lr 1.0000e-05 eta 0:00:43
epoch [1/200] batch [20/90] time 0.551 (0.584) data 0.338 (0.326) loss_u loss_u 0.9946 (0.9797) acc_u 0.0000 (5.0000) lr 1.0000e-05 eta 0:00:40
epoch [1/200] batch [25/90] time 0.468 (0.576) data 0.269 (0.319) loss_u loss_u 0.9863 (0.9804) acc_u 6.2500 (4.6250) lr 1.0000e-05 eta 0:00:37
epoch [1/200] batch [30/90] time 0.532 (0.575) data 0.258 (0.315) loss_u loss_u 0.9863 (0.9799) acc_u 0.0000 (4.5833) lr 1.0000e-05 eta 0:00:34
epoch [1/200] batch [35/90] time 0.548 (0.573) data 0.336 (0.312) loss_u loss_u 0.9619 (0.9792) acc_u 9.3750 (4.7321) lr 1.0000e-05 eta 0:00:31
epoch [1/200] batch [40/90] time 0.588 (0.577) data 0.315 (0.315) loss_u loss_u 0.9766 (0.9788) acc_u 3.1250 (4.6094) lr 1.0000e-05 eta 0:00:28
epoch [1/200] batch [45/90] time 0.477 (0.576) data 0.208 (0.314) loss_u loss_u 0.9805 (0.9766) acc_u 0.0000 (4.5833) lr 1.0000e-05 eta 0:00:25
epoch [1/200] batch [50/90] time 0.537 (0.574) data 0.268 (0.314) loss_u loss_u 0.9858 (0.9762) acc_u 0.0000 (4.8125) lr 1.0000e-05 eta 0:00:22
epoch [1/200] batch [55/90] time 0.546 (0.579) data 0.275 (0.318) loss_u loss_u 0.9805 (0.9768) acc_u 6.2500 (4.6023) lr 1.0000e-05 eta 0:00:20
epoch [1/200] batch [60/90] time 0.761 (0.584) data 0.625 (0.325) loss_u loss_u 0.9829 (0.9769) acc_u 6.2500 (4.5833) lr 1.0000e-05 eta 0:00:17
epoch [1/200] batch [65/90] time 0.668 (0.585) data 0.389 (0.324) loss_u loss_u 0.9912 (0.9775) acc_u 3.1250 (4.5673) lr 1.0000e-05 eta 0:00:14
epoch [1/200] batch [70/90] time 0.480 (0.591) data 0.208 (0.331) loss_u loss_u 0.9878 (0.9774) acc_u 3.1250 (4.5536) lr 1.0000e-05 eta 0:00:11
epoch [1/200] batch [75/90] time 0.703 (0.594) data 0.435 (0.333) loss_u loss_u 0.9873 (0.9770) acc_u 0.0000 (4.5833) lr 1.0000e-05 eta 0:00:08
epoch [1/200] batch [80/90] time 0.570 (0.593) data 0.295 (0.331) loss_u loss_u 0.9697 (0.9765) acc_u 3.1250 (4.5312) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [85/90] time 0.577 (0.591) data 0.308 (0.328) loss_u loss_u 0.9805 (0.9765) acc_u 0.0000 (4.4118) lr 1.0000e-05 eta 0:00:02
epoch [1/200] batch [90/90] time 0.511 (0.588) data 0.229 (0.326) loss_u loss_u 0.9629 (0.9764) acc_u 9.3750 (4.4444) lr 1.0000e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2319
confident_label rate tensor(0.1004, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 315
clean true:310
clean false:5
clean_rate:0.9841269841269841
noisy true:507
noisy false:2314
after delete: len(clean_dataset) 315
after delete: len(noisy_dataset) 2821
epoch [2/200] batch [5/9] time 0.588 (0.577) data 0.312 (0.310) loss_x loss_x 1.4199 (1.9182) acc_x 59.3750 (53.7500) lr 2.0000e-03 eta 0:00:02
epoch [2/200] batch [5/88] time 0.478 (0.577) data 0.328 (0.329) loss_u loss_u 0.9580 (0.9565) acc_u 6.2500 (4.3750) lr 2.0000e-03 eta 0:00:47
epoch [2/200] batch [10/88] time 0.555 (0.594) data 0.278 (0.340) loss_u loss_u 0.9067 (0.9499) acc_u 12.5000 (5.9375) lr 2.0000e-03 eta 0:00:46
epoch [2/200] batch [15/88] time 0.590 (0.583) data 0.312 (0.330) loss_u loss_u 0.9775 (0.9452) acc_u 3.1250 (6.6667) lr 2.0000e-03 eta 0:00:42
epoch [2/200] batch [20/88] time 0.580 (0.591) data 0.304 (0.334) loss_u loss_u 0.9175 (0.9416) acc_u 9.3750 (7.6562) lr 2.0000e-03 eta 0:00:40
epoch [2/200] batch [25/88] time 0.500 (0.577) data 0.216 (0.321) loss_u loss_u 0.9785 (0.9385) acc_u 3.1250 (8.0000) lr 2.0000e-03 eta 0:00:36
epoch [2/200] batch [30/88] time 0.561 (0.577) data 0.290 (0.317) loss_u loss_u 0.9263 (0.9371) acc_u 12.5000 (8.3333) lr 2.0000e-03 eta 0:00:33
epoch [2/200] batch [35/88] time 0.542 (0.573) data 0.263 (0.312) loss_u loss_u 0.9473 (0.9349) acc_u 9.3750 (8.8393) lr 2.0000e-03 eta 0:00:30
epoch [2/200] batch [40/88] time 0.571 (0.570) data 0.299 (0.308) loss_u loss_u 0.9502 (0.9339) acc_u 3.1250 (8.9062) lr 2.0000e-03 eta 0:00:27
epoch [2/200] batch [45/88] time 0.701 (0.579) data 0.444 (0.316) loss_u loss_u 0.9023 (0.9319) acc_u 15.6250 (9.1667) lr 2.0000e-03 eta 0:00:24
epoch [2/200] batch [50/88] time 0.486 (0.581) data 0.291 (0.319) loss_u loss_u 0.9023 (0.9291) acc_u 9.3750 (9.5625) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [55/88] time 0.483 (0.581) data 0.207 (0.318) loss_u loss_u 0.8872 (0.9267) acc_u 9.3750 (9.7727) lr 2.0000e-03 eta 0:00:19
epoch [2/200] batch [60/88] time 0.761 (0.581) data 0.532 (0.319) loss_u loss_u 0.9048 (0.9255) acc_u 15.6250 (9.8958) lr 2.0000e-03 eta 0:00:16
epoch [2/200] batch [65/88] time 0.746 (0.588) data 0.469 (0.326) loss_u loss_u 0.9692 (0.9264) acc_u 6.2500 (9.9038) lr 2.0000e-03 eta 0:00:13
epoch [2/200] batch [70/88] time 0.543 (0.588) data 0.267 (0.327) loss_u loss_u 0.9741 (0.9269) acc_u 0.0000 (9.9107) lr 2.0000e-03 eta 0:00:10
epoch [2/200] batch [75/88] time 0.650 (0.583) data 0.381 (0.322) loss_u loss_u 0.9800 (0.9267) acc_u 3.1250 (9.7917) lr 2.0000e-03 eta 0:00:07
epoch [2/200] batch [80/88] time 0.554 (0.579) data 0.283 (0.320) loss_u loss_u 0.9478 (0.9275) acc_u 9.3750 (9.7266) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [85/88] time 0.568 (0.579) data 0.295 (0.318) loss_u loss_u 0.9229 (0.9270) acc_u 12.5000 (9.7794) lr 2.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1911
confident_label rate tensor(0.1511, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 474
clean true:466
clean false:8
clean_rate:0.9831223628691983
noisy true:759
noisy false:1903
after delete: len(clean_dataset) 474
after delete: len(noisy_dataset) 2662
epoch [3/200] batch [5/14] time 0.595 (0.623) data 0.324 (0.352) loss_x loss_x 2.0957 (1.6740) acc_x 40.6250 (50.0000) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [10/14] time 0.665 (0.596) data 0.390 (0.338) loss_x loss_x 2.0918 (1.7321) acc_x 37.5000 (51.5625) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [5/83] time 0.616 (0.611) data 0.336 (0.352) loss_u loss_u 0.9580 (0.9646) acc_u 6.2500 (3.7500) lr 1.9999e-03 eta 0:00:47
epoch [3/200] batch [10/83] time 0.594 (0.599) data 0.318 (0.342) loss_u loss_u 0.9160 (0.9519) acc_u 12.5000 (5.9375) lr 1.9999e-03 eta 0:00:43
epoch [3/200] batch [15/83] time 0.533 (0.592) data 0.252 (0.332) loss_u loss_u 0.9639 (0.9572) acc_u 3.1250 (4.5833) lr 1.9999e-03 eta 0:00:40
epoch [3/200] batch [20/83] time 0.532 (0.585) data 0.262 (0.328) loss_u loss_u 0.9355 (0.9566) acc_u 9.3750 (5.0000) lr 1.9999e-03 eta 0:00:36
epoch [3/200] batch [25/83] time 0.410 (0.581) data 0.278 (0.326) loss_u loss_u 0.9189 (0.9582) acc_u 12.5000 (5.0000) lr 1.9999e-03 eta 0:00:33
epoch [3/200] batch [30/83] time 0.349 (0.577) data 0.216 (0.323) loss_u loss_u 0.9595 (0.9559) acc_u 3.1250 (5.1042) lr 1.9999e-03 eta 0:00:30
epoch [3/200] batch [35/83] time 0.742 (0.582) data 0.464 (0.325) loss_u loss_u 0.9131 (0.9529) acc_u 9.3750 (5.5357) lr 1.9999e-03 eta 0:00:27
epoch [3/200] batch [40/83] time 0.601 (0.583) data 0.329 (0.325) loss_u loss_u 0.9932 (0.9538) acc_u 0.0000 (5.4688) lr 1.9999e-03 eta 0:00:25
epoch [3/200] batch [45/83] time 0.411 (0.577) data 0.240 (0.319) loss_u loss_u 0.9492 (0.9520) acc_u 6.2500 (5.7639) lr 1.9999e-03 eta 0:00:21
epoch [3/200] batch [50/83] time 0.614 (0.576) data 0.344 (0.317) loss_u loss_u 0.9287 (0.9506) acc_u 9.3750 (6.0625) lr 1.9999e-03 eta 0:00:19
epoch [3/200] batch [55/83] time 0.524 (0.573) data 0.242 (0.315) loss_u loss_u 0.9629 (0.9507) acc_u 6.2500 (6.3068) lr 1.9999e-03 eta 0:00:16
epoch [3/200] batch [60/83] time 0.529 (0.573) data 0.276 (0.315) loss_u loss_u 0.9409 (0.9505) acc_u 6.2500 (6.1979) lr 1.9999e-03 eta 0:00:13
epoch [3/200] batch [65/83] time 0.655 (0.577) data 0.381 (0.317) loss_u loss_u 0.9019 (0.9498) acc_u 15.6250 (6.2019) lr 1.9999e-03 eta 0:00:10
epoch [3/200] batch [70/83] time 0.547 (0.577) data 0.266 (0.318) loss_u loss_u 0.8940 (0.9493) acc_u 18.7500 (6.2946) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [75/83] time 0.543 (0.575) data 0.272 (0.315) loss_u loss_u 0.9243 (0.9487) acc_u 9.3750 (6.3333) lr 1.9999e-03 eta 0:00:04
epoch [3/200] batch [80/83] time 0.564 (0.574) data 0.294 (0.314) loss_u loss_u 0.9575 (0.9478) acc_u 3.1250 (6.4844) lr 1.9999e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1844
confident_label rate tensor(0.1543, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 484
clean true:476
clean false:8
clean_rate:0.9834710743801653
noisy true:816
noisy false:1836
after delete: len(clean_dataset) 484
after delete: len(noisy_dataset) 2652
epoch [4/200] batch [5/15] time 0.545 (0.598) data 0.274 (0.325) loss_x loss_x 1.4355 (1.4658) acc_x 53.1250 (60.6250) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [10/15] time 0.675 (0.578) data 0.399 (0.319) loss_x loss_x 2.5527 (1.5172) acc_x 34.3750 (58.4375) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [15/15] time 0.660 (0.598) data 0.387 (0.335) loss_x loss_x 1.8516 (1.5596) acc_x 40.6250 (58.3333) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/82] time 0.536 (0.583) data 0.271 (0.323) loss_u loss_u 0.9082 (0.9206) acc_u 12.5000 (11.8750) lr 1.9995e-03 eta 0:00:44
epoch [4/200] batch [10/82] time 0.572 (0.591) data 0.298 (0.329) loss_u loss_u 0.9087 (0.9289) acc_u 9.3750 (10.0000) lr 1.9995e-03 eta 0:00:42
epoch [4/200] batch [15/82] time 0.375 (0.582) data 0.243 (0.323) loss_u loss_u 0.9004 (0.9324) acc_u 15.6250 (9.7917) lr 1.9995e-03 eta 0:00:38
epoch [4/200] batch [20/82] time 0.523 (0.591) data 0.244 (0.330) loss_u loss_u 0.9707 (0.9397) acc_u 6.2500 (8.4375) lr 1.9995e-03 eta 0:00:36
epoch [4/200] batch [25/82] time 0.606 (0.590) data 0.331 (0.331) loss_u loss_u 0.8970 (0.9392) acc_u 12.5000 (8.2500) lr 1.9995e-03 eta 0:00:33
epoch [4/200] batch [30/82] time 0.505 (0.585) data 0.211 (0.326) loss_u loss_u 0.9644 (0.9418) acc_u 3.1250 (8.0208) lr 1.9995e-03 eta 0:00:30
epoch [4/200] batch [35/82] time 0.531 (0.585) data 0.253 (0.324) loss_u loss_u 0.9614 (0.9423) acc_u 3.1250 (7.6786) lr 1.9995e-03 eta 0:00:27
epoch [4/200] batch [40/82] time 0.536 (0.581) data 0.264 (0.321) loss_u loss_u 0.9453 (0.9418) acc_u 6.2500 (7.7344) lr 1.9995e-03 eta 0:00:24
epoch [4/200] batch [45/82] time 0.572 (0.582) data 0.306 (0.321) loss_u loss_u 0.9258 (0.9415) acc_u 15.6250 (7.9167) lr 1.9995e-03 eta 0:00:21
epoch [4/200] batch [50/82] time 0.657 (0.579) data 0.377 (0.320) loss_u loss_u 0.9448 (0.9426) acc_u 9.3750 (7.8125) lr 1.9995e-03 eta 0:00:18
epoch [4/200] batch [55/82] time 0.576 (0.576) data 0.304 (0.318) loss_u loss_u 0.9673 (0.9446) acc_u 3.1250 (7.5568) lr 1.9995e-03 eta 0:00:15
epoch [4/200] batch [60/82] time 0.526 (0.575) data 0.259 (0.316) loss_u loss_u 0.9316 (0.9454) acc_u 6.2500 (7.5000) lr 1.9995e-03 eta 0:00:12
epoch [4/200] batch [65/82] time 0.474 (0.569) data 0.200 (0.311) loss_u loss_u 0.9209 (0.9444) acc_u 12.5000 (7.5481) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [70/82] time 0.605 (0.569) data 0.327 (0.309) loss_u loss_u 0.9312 (0.9437) acc_u 9.3750 (7.5446) lr 1.9995e-03 eta 0:00:06
epoch [4/200] batch [75/82] time 0.421 (0.569) data 0.256 (0.310) loss_u loss_u 0.9614 (0.9440) acc_u 6.2500 (7.5833) lr 1.9995e-03 eta 0:00:03
epoch [4/200] batch [80/82] time 0.385 (0.567) data 0.254 (0.309) loss_u loss_u 0.9736 (0.9440) acc_u 3.1250 (7.5391) lr 1.9995e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1859
confident_label rate tensor(0.1588, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 498
clean true:495
clean false:3
clean_rate:0.9939759036144579
noisy true:782
noisy false:1856
after delete: len(clean_dataset) 498
after delete: len(noisy_dataset) 2638
epoch [5/200] batch [5/15] time 0.670 (0.642) data 0.409 (0.374) loss_x loss_x 1.3887 (1.3715) acc_x 65.6250 (61.2500) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [10/15] time 0.625 (0.595) data 0.352 (0.339) loss_x loss_x 2.7422 (1.6405) acc_x 37.5000 (55.3125) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [15/15] time 0.542 (0.575) data 0.342 (0.319) loss_x loss_x 1.7256 (1.6310) acc_x 59.3750 (56.2500) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/82] time 0.529 (0.574) data 0.298 (0.315) loss_u loss_u 0.9458 (0.9484) acc_u 6.2500 (6.2500) lr 1.9989e-03 eta 0:00:44
epoch [5/200] batch [10/82] time 0.560 (0.577) data 0.284 (0.315) loss_u loss_u 0.9868 (0.9541) acc_u 0.0000 (5.3125) lr 1.9989e-03 eta 0:00:41
epoch [5/200] batch [15/82] time 0.588 (0.579) data 0.307 (0.319) loss_u loss_u 0.9292 (0.9456) acc_u 9.3750 (7.0833) lr 1.9989e-03 eta 0:00:38
epoch [5/200] batch [20/82] time 0.566 (0.578) data 0.295 (0.321) loss_u loss_u 0.9731 (0.9481) acc_u 3.1250 (7.0312) lr 1.9989e-03 eta 0:00:35
epoch [5/200] batch [25/82] time 0.543 (0.574) data 0.346 (0.317) loss_u loss_u 0.9668 (0.9468) acc_u 3.1250 (7.2500) lr 1.9989e-03 eta 0:00:32
epoch [5/200] batch [30/82] time 0.604 (0.573) data 0.334 (0.315) loss_u loss_u 0.9634 (0.9479) acc_u 3.1250 (6.8750) lr 1.9989e-03 eta 0:00:29
epoch [5/200] batch [35/82] time 0.452 (0.576) data 0.179 (0.320) loss_u loss_u 0.9512 (0.9495) acc_u 9.3750 (6.7857) lr 1.9989e-03 eta 0:00:27
epoch [5/200] batch [40/82] time 0.481 (0.573) data 0.349 (0.317) loss_u loss_u 0.9194 (0.9499) acc_u 12.5000 (6.7188) lr 1.9989e-03 eta 0:00:24
epoch [5/200] batch [45/82] time 0.554 (0.573) data 0.280 (0.316) loss_u loss_u 0.9287 (0.9485) acc_u 6.2500 (6.8750) lr 1.9989e-03 eta 0:00:21
epoch [5/200] batch [50/82] time 0.574 (0.573) data 0.306 (0.317) loss_u loss_u 0.9609 (0.9487) acc_u 6.2500 (6.8750) lr 1.9989e-03 eta 0:00:18
epoch [5/200] batch [55/82] time 0.550 (0.576) data 0.279 (0.318) loss_u loss_u 0.9712 (0.9472) acc_u 3.1250 (7.2159) lr 1.9989e-03 eta 0:00:15
epoch [5/200] batch [60/82] time 0.567 (0.571) data 0.357 (0.316) loss_u loss_u 0.9102 (0.9473) acc_u 12.5000 (7.1354) lr 1.9989e-03 eta 0:00:12
epoch [5/200] batch [65/82] time 0.403 (0.570) data 0.271 (0.315) loss_u loss_u 0.9688 (0.9459) acc_u 3.1250 (7.4038) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [70/82] time 0.550 (0.568) data 0.362 (0.313) loss_u loss_u 0.9878 (0.9459) acc_u 0.0000 (7.4554) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [75/82] time 0.706 (0.570) data 0.430 (0.314) loss_u loss_u 0.9653 (0.9451) acc_u 3.1250 (7.4583) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [80/82] time 0.511 (0.568) data 0.242 (0.313) loss_u loss_u 0.9141 (0.9444) acc_u 9.3750 (7.5000) lr 1.9989e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1866
confident_label rate tensor(0.1620, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 508
clean true:497
clean false:11
clean_rate:0.9783464566929134
noisy true:773
noisy false:1855
after delete: len(clean_dataset) 508
after delete: len(noisy_dataset) 2628
epoch [6/200] batch [5/15] time 0.773 (0.642) data 0.522 (0.374) loss_x loss_x 1.2129 (1.3145) acc_x 68.7500 (62.5000) lr 1.9980e-03 eta 0:00:06
epoch [6/200] batch [10/15] time 0.623 (0.638) data 0.349 (0.369) loss_x loss_x 1.8672 (1.4299) acc_x 56.2500 (61.5625) lr 1.9980e-03 eta 0:00:03
epoch [6/200] batch [15/15] time 0.577 (0.612) data 0.437 (0.360) loss_x loss_x 1.9023 (1.5148) acc_x 59.3750 (60.2083) lr 1.9980e-03 eta 0:00:00
epoch [6/200] batch [5/82] time 0.726 (0.622) data 0.457 (0.365) loss_u loss_u 0.9004 (0.9408) acc_u 9.3750 (4.3750) lr 1.9980e-03 eta 0:00:47
epoch [6/200] batch [10/82] time 0.532 (0.617) data 0.257 (0.361) loss_u loss_u 0.9590 (0.9499) acc_u 6.2500 (4.6875) lr 1.9980e-03 eta 0:00:44
epoch [6/200] batch [15/82] time 0.567 (0.604) data 0.293 (0.351) loss_u loss_u 0.9448 (0.9468) acc_u 3.1250 (5.6250) lr 1.9980e-03 eta 0:00:40
epoch [6/200] batch [20/82] time 0.326 (0.596) data 0.195 (0.344) loss_u loss_u 0.9258 (0.9492) acc_u 9.3750 (5.4688) lr 1.9980e-03 eta 0:00:36
epoch [6/200] batch [25/82] time 0.605 (0.591) data 0.337 (0.336) loss_u loss_u 0.9619 (0.9488) acc_u 3.1250 (5.8750) lr 1.9980e-03 eta 0:00:33
epoch [6/200] batch [30/82] time 0.531 (0.585) data 0.251 (0.331) loss_u loss_u 0.9463 (0.9457) acc_u 6.2500 (6.2500) lr 1.9980e-03 eta 0:00:30
epoch [6/200] batch [35/82] time 0.610 (0.587) data 0.339 (0.331) loss_u loss_u 0.9463 (0.9457) acc_u 3.1250 (6.2500) lr 1.9980e-03 eta 0:00:27
epoch [6/200] batch [40/82] time 0.569 (0.587) data 0.297 (0.329) loss_u loss_u 0.9443 (0.9455) acc_u 6.2500 (6.3281) lr 1.9980e-03 eta 0:00:24
epoch [6/200] batch [45/82] time 0.970 (0.594) data 0.695 (0.338) loss_u loss_u 0.9561 (0.9461) acc_u 6.2500 (6.3889) lr 1.9980e-03 eta 0:00:21
epoch [6/200] batch [50/82] time 0.741 (0.601) data 0.471 (0.343) loss_u loss_u 0.9526 (0.9446) acc_u 6.2500 (6.6250) lr 1.9980e-03 eta 0:00:19
epoch [6/200] batch [55/82] time 0.567 (0.600) data 0.290 (0.341) loss_u loss_u 0.8960 (0.9442) acc_u 12.5000 (6.7614) lr 1.9980e-03 eta 0:00:16
epoch [6/200] batch [60/82] time 0.448 (0.596) data 0.316 (0.338) loss_u loss_u 0.9717 (0.9458) acc_u 0.0000 (6.5625) lr 1.9980e-03 eta 0:00:13
epoch [6/200] batch [65/82] time 0.444 (0.589) data 0.305 (0.339) loss_u loss_u 0.9868 (0.9463) acc_u 0.0000 (6.5385) lr 1.9980e-03 eta 0:00:10
epoch [6/200] batch [70/82] time 0.367 (0.579) data 0.236 (0.336) loss_u loss_u 0.9312 (0.9458) acc_u 9.3750 (6.6071) lr 1.9980e-03 eta 0:00:06
epoch [6/200] batch [75/82] time 0.792 (0.574) data 0.660 (0.337) loss_u loss_u 0.9360 (0.9431) acc_u 12.5000 (7.0833) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [80/82] time 0.342 (0.563) data 0.212 (0.332) loss_u loss_u 0.9053 (0.9411) acc_u 18.7500 (7.4609) lr 1.9980e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1809
confident_label rate tensor(0.1547, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 485
clean true:480
clean false:5
clean_rate:0.9896907216494846
noisy true:847
noisy false:1804
after delete: len(clean_dataset) 485
after delete: len(noisy_dataset) 2651
epoch [7/200] batch [5/15] time 0.415 (0.456) data 0.284 (0.325) loss_x loss_x 1.2686 (1.4937) acc_x 71.8750 (60.6250) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [10/15] time 0.442 (0.468) data 0.311 (0.337) loss_x loss_x 1.3008 (1.6220) acc_x 65.6250 (55.3125) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [15/15] time 0.594 (0.484) data 0.463 (0.353) loss_x loss_x 1.2822 (1.6096) acc_x 71.8750 (56.0417) lr 1.9969e-03 eta 0:00:00
epoch [7/200] batch [5/82] time 0.402 (0.461) data 0.270 (0.329) loss_u loss_u 0.9409 (0.9297) acc_u 6.2500 (9.3750) lr 1.9969e-03 eta 0:00:35
epoch [7/200] batch [10/82] time 0.498 (0.456) data 0.368 (0.325) loss_u loss_u 0.9126 (0.9404) acc_u 12.5000 (7.8125) lr 1.9969e-03 eta 0:00:32
epoch [7/200] batch [15/82] time 0.404 (0.464) data 0.273 (0.333) loss_u loss_u 0.9473 (0.9378) acc_u 6.2500 (7.7083) lr 1.9969e-03 eta 0:00:31
epoch [7/200] batch [20/82] time 0.504 (0.468) data 0.372 (0.337) loss_u loss_u 0.9727 (0.9396) acc_u 3.1250 (7.8125) lr 1.9969e-03 eta 0:00:29
epoch [7/200] batch [25/82] time 0.445 (0.462) data 0.314 (0.331) loss_u loss_u 0.9424 (0.9401) acc_u 6.2500 (8.1250) lr 1.9969e-03 eta 0:00:26
epoch [7/200] batch [30/82] time 0.407 (0.466) data 0.275 (0.334) loss_u loss_u 0.9707 (0.9384) acc_u 6.2500 (8.3333) lr 1.9969e-03 eta 0:00:24
epoch [7/200] batch [35/82] time 0.419 (0.468) data 0.289 (0.336) loss_u loss_u 0.9355 (0.9373) acc_u 9.3750 (8.6607) lr 1.9969e-03 eta 0:00:21
epoch [7/200] batch [40/82] time 0.422 (0.468) data 0.290 (0.337) loss_u loss_u 0.9360 (0.9371) acc_u 9.3750 (8.5156) lr 1.9969e-03 eta 0:00:19
epoch [7/200] batch [45/82] time 0.435 (0.470) data 0.304 (0.339) loss_u loss_u 0.9692 (0.9376) acc_u 3.1250 (8.4722) lr 1.9969e-03 eta 0:00:17
epoch [7/200] batch [50/82] time 0.357 (0.462) data 0.227 (0.331) loss_u loss_u 0.9551 (0.9392) acc_u 6.2500 (8.0625) lr 1.9969e-03 eta 0:00:14
epoch [7/200] batch [55/82] time 0.490 (0.461) data 0.358 (0.330) loss_u loss_u 0.9209 (0.9396) acc_u 9.3750 (8.0114) lr 1.9969e-03 eta 0:00:12
epoch [7/200] batch [60/82] time 0.438 (0.458) data 0.308 (0.327) loss_u loss_u 0.8994 (0.9383) acc_u 9.3750 (8.0729) lr 1.9969e-03 eta 0:00:10
epoch [7/200] batch [65/82] time 0.622 (0.462) data 0.492 (0.331) loss_u loss_u 0.9473 (0.9386) acc_u 6.2500 (7.9808) lr 1.9969e-03 eta 0:00:07
epoch [7/200] batch [70/82] time 0.373 (0.461) data 0.242 (0.330) loss_u loss_u 0.9282 (0.9383) acc_u 9.3750 (7.9911) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [75/82] time 0.443 (0.463) data 0.311 (0.332) loss_u loss_u 0.8950 (0.9391) acc_u 15.6250 (7.9583) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [80/82] time 0.606 (0.465) data 0.474 (0.334) loss_u loss_u 0.9629 (0.9393) acc_u 6.2500 (7.9297) lr 1.9969e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1841
confident_label rate tensor(0.1645, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 516
clean true:515
clean false:1
clean_rate:0.998062015503876
noisy true:780
noisy false:1840
after delete: len(clean_dataset) 516
after delete: len(noisy_dataset) 2620
epoch [8/200] batch [5/16] time 0.454 (0.485) data 0.323 (0.354) loss_x loss_x 1.4619 (1.4937) acc_x 50.0000 (60.6250) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [10/16] time 0.689 (0.523) data 0.558 (0.392) loss_x loss_x 1.7090 (1.5433) acc_x 50.0000 (56.8750) lr 1.9956e-03 eta 0:00:03
epoch [8/200] batch [15/16] time 0.367 (0.500) data 0.236 (0.369) loss_x loss_x 0.9263 (1.4731) acc_x 78.1250 (58.7500) lr 1.9956e-03 eta 0:00:00
epoch [8/200] batch [5/81] time 0.387 (0.501) data 0.256 (0.370) loss_u loss_u 0.9546 (0.9596) acc_u 6.2500 (4.3750) lr 1.9956e-03 eta 0:00:38
epoch [8/200] batch [10/81] time 0.359 (0.484) data 0.229 (0.353) loss_u loss_u 0.9414 (0.9596) acc_u 9.3750 (5.6250) lr 1.9956e-03 eta 0:00:34
epoch [8/200] batch [15/81] time 0.376 (0.470) data 0.244 (0.339) loss_u loss_u 0.9795 (0.9546) acc_u 0.0000 (5.6250) lr 1.9956e-03 eta 0:00:31
epoch [8/200] batch [20/81] time 0.400 (0.467) data 0.270 (0.336) loss_u loss_u 0.9307 (0.9527) acc_u 9.3750 (5.9375) lr 1.9956e-03 eta 0:00:28
epoch [8/200] batch [25/81] time 0.536 (0.460) data 0.404 (0.329) loss_u loss_u 0.9399 (0.9525) acc_u 9.3750 (5.7500) lr 1.9956e-03 eta 0:00:25
epoch [8/200] batch [30/81] time 0.479 (0.457) data 0.347 (0.326) loss_u loss_u 0.9526 (0.9500) acc_u 3.1250 (6.0417) lr 1.9956e-03 eta 0:00:23
epoch [8/200] batch [35/81] time 0.504 (0.457) data 0.373 (0.326) loss_u loss_u 0.9746 (0.9497) acc_u 0.0000 (5.8036) lr 1.9956e-03 eta 0:00:21
epoch [8/200] batch [40/81] time 0.504 (0.457) data 0.372 (0.326) loss_u loss_u 0.9399 (0.9484) acc_u 9.3750 (6.0938) lr 1.9956e-03 eta 0:00:18
epoch [8/200] batch [45/81] time 0.654 (0.462) data 0.523 (0.331) loss_u loss_u 0.9536 (0.9482) acc_u 3.1250 (6.1111) lr 1.9956e-03 eta 0:00:16
epoch [8/200] batch [50/81] time 0.424 (0.462) data 0.293 (0.330) loss_u loss_u 0.9253 (0.9476) acc_u 9.3750 (6.3750) lr 1.9956e-03 eta 0:00:14
epoch [8/200] batch [55/81] time 0.358 (0.460) data 0.227 (0.329) loss_u loss_u 0.9150 (0.9451) acc_u 6.2500 (6.5341) lr 1.9956e-03 eta 0:00:11
epoch [8/200] batch [60/81] time 0.428 (0.457) data 0.298 (0.326) loss_u loss_u 0.9585 (0.9455) acc_u 0.0000 (6.4583) lr 1.9956e-03 eta 0:00:09
epoch [8/200] batch [65/81] time 0.548 (0.456) data 0.416 (0.325) loss_u loss_u 0.9644 (0.9454) acc_u 3.1250 (6.4904) lr 1.9956e-03 eta 0:00:07
epoch [8/200] batch [70/81] time 0.627 (0.457) data 0.496 (0.326) loss_u loss_u 0.9585 (0.9453) acc_u 3.1250 (6.6071) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [75/81] time 0.652 (0.458) data 0.521 (0.327) loss_u loss_u 0.9683 (0.9453) acc_u 3.1250 (6.5833) lr 1.9956e-03 eta 0:00:02
epoch [8/200] batch [80/81] time 0.427 (0.455) data 0.296 (0.324) loss_u loss_u 0.9761 (0.9459) acc_u 3.1250 (6.5234) lr 1.9956e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1771
confident_label rate tensor(0.1674, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 525
clean true:517
clean false:8
clean_rate:0.9847619047619047
noisy true:848
noisy false:1763
after delete: len(clean_dataset) 525
after delete: len(noisy_dataset) 2611
epoch [9/200] batch [5/16] time 0.368 (0.455) data 0.237 (0.324) loss_x loss_x 2.0078 (1.6877) acc_x 53.1250 (59.3750) lr 1.9940e-03 eta 0:00:05
epoch [9/200] batch [10/16] time 0.434 (0.454) data 0.304 (0.323) loss_x loss_x 1.2695 (1.6310) acc_x 68.7500 (61.5625) lr 1.9940e-03 eta 0:00:02
epoch [9/200] batch [15/16] time 0.359 (0.465) data 0.229 (0.335) loss_x loss_x 1.2627 (1.6201) acc_x 65.6250 (61.8750) lr 1.9940e-03 eta 0:00:00
epoch [9/200] batch [5/81] time 0.405 (0.455) data 0.275 (0.325) loss_u loss_u 0.9463 (0.9556) acc_u 9.3750 (5.6250) lr 1.9940e-03 eta 0:00:34
epoch [9/200] batch [10/81] time 0.352 (0.448) data 0.220 (0.317) loss_u loss_u 0.9688 (0.9520) acc_u 3.1250 (6.5625) lr 1.9940e-03 eta 0:00:31
epoch [9/200] batch [15/81] time 0.600 (0.449) data 0.469 (0.318) loss_u loss_u 0.9570 (0.9531) acc_u 9.3750 (6.4583) lr 1.9940e-03 eta 0:00:29
epoch [9/200] batch [20/81] time 0.427 (0.440) data 0.296 (0.309) loss_u loss_u 0.9199 (0.9511) acc_u 12.5000 (6.2500) lr 1.9940e-03 eta 0:00:26
epoch [9/200] batch [25/81] time 0.366 (0.439) data 0.235 (0.308) loss_u loss_u 0.9487 (0.9498) acc_u 6.2500 (6.7500) lr 1.9940e-03 eta 0:00:24
epoch [9/200] batch [30/81] time 0.389 (0.439) data 0.257 (0.308) loss_u loss_u 0.9189 (0.9479) acc_u 12.5000 (7.0833) lr 1.9940e-03 eta 0:00:22
epoch [9/200] batch [35/81] time 0.462 (0.446) data 0.330 (0.315) loss_u loss_u 0.9517 (0.9463) acc_u 6.2500 (7.1429) lr 1.9940e-03 eta 0:00:20
epoch [9/200] batch [40/81] time 0.479 (0.448) data 0.349 (0.317) loss_u loss_u 0.9761 (0.9461) acc_u 3.1250 (7.1875) lr 1.9940e-03 eta 0:00:18
epoch [9/200] batch [45/81] time 0.442 (0.447) data 0.311 (0.316) loss_u loss_u 0.9678 (0.9454) acc_u 0.0000 (7.1528) lr 1.9940e-03 eta 0:00:16
epoch [9/200] batch [50/81] time 0.362 (0.448) data 0.232 (0.317) loss_u loss_u 0.9136 (0.9446) acc_u 9.3750 (7.1875) lr 1.9940e-03 eta 0:00:13
epoch [9/200] batch [55/81] time 0.390 (0.447) data 0.258 (0.316) loss_u loss_u 0.9648 (0.9443) acc_u 6.2500 (7.2159) lr 1.9940e-03 eta 0:00:11
epoch [9/200] batch [60/81] time 0.416 (0.445) data 0.284 (0.314) loss_u loss_u 0.9272 (0.9438) acc_u 9.3750 (7.1354) lr 1.9940e-03 eta 0:00:09
epoch [9/200] batch [65/81] time 0.442 (0.446) data 0.310 (0.315) loss_u loss_u 0.9751 (0.9445) acc_u 0.0000 (7.0673) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [70/81] time 0.451 (0.448) data 0.320 (0.317) loss_u loss_u 0.9463 (0.9455) acc_u 9.3750 (6.9643) lr 1.9940e-03 eta 0:00:04
epoch [9/200] batch [75/81] time 0.373 (0.449) data 0.241 (0.318) loss_u loss_u 0.9487 (0.9453) acc_u 3.1250 (7.0000) lr 1.9940e-03 eta 0:00:02
epoch [9/200] batch [80/81] time 0.405 (0.451) data 0.274 (0.320) loss_u loss_u 0.9390 (0.9446) acc_u 6.2500 (7.1484) lr 1.9940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1811
confident_label rate tensor(0.1575, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 494
clean true:488
clean false:6
clean_rate:0.9878542510121457
noisy true:837
noisy false:1805
after delete: len(clean_dataset) 494
after delete: len(noisy_dataset) 2642
epoch [10/200] batch [5/15] time 0.639 (0.497) data 0.508 (0.367) loss_x loss_x 1.6885 (1.4075) acc_x 50.0000 (58.1250) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [10/15] time 0.501 (0.471) data 0.370 (0.341) loss_x loss_x 1.7500 (1.5445) acc_x 56.2500 (58.1250) lr 1.9921e-03 eta 0:00:02
epoch [10/200] batch [15/15] time 0.487 (0.477) data 0.356 (0.346) loss_x loss_x 1.3428 (1.3823) acc_x 62.5000 (60.8333) lr 1.9921e-03 eta 0:00:00
epoch [10/200] batch [5/82] time 0.583 (0.482) data 0.449 (0.351) loss_u loss_u 0.9087 (0.9391) acc_u 9.3750 (6.2500) lr 1.9921e-03 eta 0:00:37
epoch [10/200] batch [10/82] time 0.762 (0.483) data 0.628 (0.352) loss_u loss_u 0.9438 (0.9347) acc_u 9.3750 (8.1250) lr 1.9921e-03 eta 0:00:34
epoch [10/200] batch [15/82] time 0.449 (0.492) data 0.318 (0.360) loss_u loss_u 0.9346 (0.9392) acc_u 6.2500 (7.5000) lr 1.9921e-03 eta 0:00:32
epoch [10/200] batch [20/82] time 0.471 (0.495) data 0.339 (0.363) loss_u loss_u 0.9668 (0.9422) acc_u 0.0000 (6.8750) lr 1.9921e-03 eta 0:00:30
epoch [10/200] batch [25/82] time 0.524 (0.491) data 0.392 (0.359) loss_u loss_u 0.9346 (0.9450) acc_u 9.3750 (6.5000) lr 1.9921e-03 eta 0:00:27
epoch [10/200] batch [30/82] time 0.409 (0.489) data 0.275 (0.357) loss_u loss_u 0.9893 (0.9478) acc_u 0.0000 (6.2500) lr 1.9921e-03 eta 0:00:25
epoch [10/200] batch [35/82] time 0.726 (0.494) data 0.592 (0.361) loss_u loss_u 0.9019 (0.9461) acc_u 12.5000 (6.6071) lr 1.9921e-03 eta 0:00:23
epoch [10/200] batch [40/82] time 0.518 (0.495) data 0.387 (0.363) loss_u loss_u 0.9585 (0.9450) acc_u 6.2500 (6.8750) lr 1.9921e-03 eta 0:00:20
epoch [10/200] batch [45/82] time 0.421 (0.491) data 0.287 (0.359) loss_u loss_u 0.9424 (0.9466) acc_u 6.2500 (6.5278) lr 1.9921e-03 eta 0:00:18
epoch [10/200] batch [50/82] time 0.505 (0.490) data 0.369 (0.357) loss_u loss_u 0.8774 (0.9438) acc_u 18.7500 (6.9375) lr 1.9921e-03 eta 0:00:15
epoch [10/200] batch [55/82] time 0.449 (0.491) data 0.317 (0.358) loss_u loss_u 0.9448 (0.9427) acc_u 6.2500 (7.2159) lr 1.9921e-03 eta 0:00:13
epoch [10/200] batch [60/82] time 0.438 (0.487) data 0.307 (0.354) loss_u loss_u 0.9487 (0.9420) acc_u 6.2500 (7.1875) lr 1.9921e-03 eta 0:00:10
epoch [10/200] batch [65/82] time 0.503 (0.485) data 0.371 (0.352) loss_u loss_u 0.9424 (0.9425) acc_u 3.1250 (7.0673) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [70/82] time 0.553 (0.487) data 0.422 (0.354) loss_u loss_u 0.9844 (0.9412) acc_u 0.0000 (7.2321) lr 1.9921e-03 eta 0:00:05
epoch [10/200] batch [75/82] time 0.461 (0.487) data 0.327 (0.354) loss_u loss_u 0.9570 (0.9426) acc_u 3.1250 (7.1667) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [80/82] time 0.646 (0.486) data 0.515 (0.353) loss_u loss_u 0.9644 (0.9420) acc_u 0.0000 (7.1094) lr 1.9921e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1823
confident_label rate tensor(0.1636, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 513
clean true:506
clean false:7
clean_rate:0.98635477582846
noisy true:807
noisy false:1816
after delete: len(clean_dataset) 513
after delete: len(noisy_dataset) 2623
epoch [11/200] batch [5/16] time 0.445 (0.538) data 0.315 (0.407) loss_x loss_x 1.3545 (1.4426) acc_x 68.7500 (62.5000) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [10/16] time 0.394 (0.484) data 0.263 (0.353) loss_x loss_x 2.3281 (1.5678) acc_x 43.7500 (60.0000) lr 1.9900e-03 eta 0:00:02
epoch [11/200] batch [15/16] time 0.448 (0.480) data 0.317 (0.349) loss_x loss_x 1.4561 (1.5795) acc_x 50.0000 (59.1667) lr 1.9900e-03 eta 0:00:00
epoch [11/200] batch [5/81] time 0.522 (0.483) data 0.390 (0.351) loss_u loss_u 0.9888 (0.9373) acc_u 0.0000 (8.7500) lr 1.9900e-03 eta 0:00:36
epoch [11/200] batch [10/81] time 0.459 (0.488) data 0.327 (0.357) loss_u loss_u 0.9087 (0.9470) acc_u 18.7500 (7.8125) lr 1.9900e-03 eta 0:00:34
epoch [11/200] batch [15/81] time 0.499 (0.479) data 0.368 (0.347) loss_u loss_u 0.8911 (0.9404) acc_u 15.6250 (8.3333) lr 1.9900e-03 eta 0:00:31
epoch [11/200] batch [20/81] time 0.375 (0.477) data 0.243 (0.345) loss_u loss_u 0.9683 (0.9399) acc_u 6.2500 (8.7500) lr 1.9900e-03 eta 0:00:29
epoch [11/200] batch [25/81] time 0.566 (0.484) data 0.435 (0.352) loss_u loss_u 0.9849 (0.9404) acc_u 0.0000 (8.3750) lr 1.9900e-03 eta 0:00:27
epoch [11/200] batch [30/81] time 0.573 (0.488) data 0.443 (0.356) loss_u loss_u 0.9468 (0.9389) acc_u 9.3750 (8.6458) lr 1.9900e-03 eta 0:00:24
epoch [11/200] batch [35/81] time 0.514 (0.485) data 0.384 (0.353) loss_u loss_u 0.9526 (0.9395) acc_u 6.2500 (8.5714) lr 1.9900e-03 eta 0:00:22
epoch [11/200] batch [40/81] time 0.498 (0.487) data 0.366 (0.355) loss_u loss_u 0.8979 (0.9401) acc_u 15.6250 (8.2812) lr 1.9900e-03 eta 0:00:19
epoch [11/200] batch [45/81] time 0.515 (0.485) data 0.383 (0.354) loss_u loss_u 0.9614 (0.9424) acc_u 3.1250 (7.9167) lr 1.9900e-03 eta 0:00:17
epoch [11/200] batch [50/81] time 0.434 (0.480) data 0.303 (0.349) loss_u loss_u 0.9150 (0.9401) acc_u 12.5000 (8.1250) lr 1.9900e-03 eta 0:00:14
epoch [11/200] batch [55/81] time 0.530 (0.480) data 0.398 (0.349) loss_u loss_u 0.9731 (0.9406) acc_u 6.2500 (7.9545) lr 1.9900e-03 eta 0:00:12
epoch [11/200] batch [60/81] time 0.429 (0.475) data 0.298 (0.344) loss_u loss_u 0.9185 (0.9417) acc_u 12.5000 (7.8125) lr 1.9900e-03 eta 0:00:09
epoch [11/200] batch [65/81] time 0.416 (0.474) data 0.284 (0.343) loss_u loss_u 0.9502 (0.9423) acc_u 6.2500 (7.6923) lr 1.9900e-03 eta 0:00:07
epoch [11/200] batch [70/81] time 0.327 (0.469) data 0.196 (0.337) loss_u loss_u 0.9546 (0.9419) acc_u 6.2500 (7.7232) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [75/81] time 0.385 (0.470) data 0.254 (0.339) loss_u loss_u 0.9458 (0.9426) acc_u 3.1250 (7.5417) lr 1.9900e-03 eta 0:00:02
epoch [11/200] batch [80/81] time 0.486 (0.466) data 0.355 (0.335) loss_u loss_u 0.8682 (0.9427) acc_u 18.7500 (7.5391) lr 1.9900e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1801
confident_label rate tensor(0.1629, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 511
clean true:502
clean false:9
clean_rate:0.9823874755381604
noisy true:833
noisy false:1792
after delete: len(clean_dataset) 511
after delete: len(noisy_dataset) 2625
epoch [12/200] batch [5/15] time 0.425 (0.396) data 0.294 (0.265) loss_x loss_x 1.1250 (1.4461) acc_x 75.0000 (62.5000) lr 1.9877e-03 eta 0:00:03
epoch [12/200] batch [10/15] time 0.414 (0.427) data 0.283 (0.296) loss_x loss_x 1.6768 (1.4598) acc_x 50.0000 (60.0000) lr 1.9877e-03 eta 0:00:02
epoch [12/200] batch [15/15] time 0.491 (0.465) data 0.361 (0.334) loss_x loss_x 1.5146 (1.5900) acc_x 62.5000 (57.9167) lr 1.9877e-03 eta 0:00:00
epoch [12/200] batch [5/82] time 0.567 (0.467) data 0.435 (0.336) loss_u loss_u 0.9678 (0.9368) acc_u 6.2500 (8.1250) lr 1.9877e-03 eta 0:00:35
epoch [12/200] batch [10/82] time 0.450 (0.464) data 0.320 (0.334) loss_u loss_u 0.9614 (0.9488) acc_u 6.2500 (6.8750) lr 1.9877e-03 eta 0:00:33
epoch [12/200] batch [15/82] time 0.440 (0.463) data 0.307 (0.332) loss_u loss_u 0.9482 (0.9466) acc_u 6.2500 (7.2917) lr 1.9877e-03 eta 0:00:31
epoch [12/200] batch [20/82] time 0.390 (0.461) data 0.259 (0.330) loss_u loss_u 0.9316 (0.9426) acc_u 9.3750 (7.9688) lr 1.9877e-03 eta 0:00:28
epoch [12/200] batch [25/82] time 0.551 (0.460) data 0.419 (0.329) loss_u loss_u 0.9521 (0.9439) acc_u 6.2500 (7.5000) lr 1.9877e-03 eta 0:00:26
epoch [12/200] batch [30/82] time 0.433 (0.459) data 0.301 (0.328) loss_u loss_u 0.9795 (0.9428) acc_u 3.1250 (7.3958) lr 1.9877e-03 eta 0:00:23
epoch [12/200] batch [35/82] time 0.793 (0.467) data 0.662 (0.336) loss_u loss_u 0.9185 (0.9425) acc_u 9.3750 (7.4107) lr 1.9877e-03 eta 0:00:21
epoch [12/200] batch [40/82] time 0.407 (0.462) data 0.276 (0.331) loss_u loss_u 0.9478 (0.9394) acc_u 6.2500 (7.7344) lr 1.9877e-03 eta 0:00:19
epoch [12/200] batch [45/82] time 0.383 (0.462) data 0.253 (0.331) loss_u loss_u 0.9629 (0.9398) acc_u 9.3750 (7.9861) lr 1.9877e-03 eta 0:00:17
epoch [12/200] batch [50/82] time 0.391 (0.460) data 0.260 (0.329) loss_u loss_u 0.9111 (0.9403) acc_u 12.5000 (8.0000) lr 1.9877e-03 eta 0:00:14
epoch [12/200] batch [55/82] time 0.449 (0.458) data 0.313 (0.327) loss_u loss_u 0.9512 (0.9393) acc_u 3.1250 (8.0682) lr 1.9877e-03 eta 0:00:12
epoch [12/200] batch [60/82] time 0.477 (0.461) data 0.344 (0.329) loss_u loss_u 0.9868 (0.9406) acc_u 0.0000 (7.7604) lr 1.9877e-03 eta 0:00:10
epoch [12/200] batch [65/82] time 0.533 (0.462) data 0.402 (0.331) loss_u loss_u 0.8555 (0.9396) acc_u 15.6250 (7.7885) lr 1.9877e-03 eta 0:00:07
epoch [12/200] batch [70/82] time 0.343 (0.459) data 0.213 (0.328) loss_u loss_u 0.9531 (0.9402) acc_u 3.1250 (7.8125) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [75/82] time 0.376 (0.456) data 0.244 (0.325) loss_u loss_u 0.9121 (0.9397) acc_u 12.5000 (7.9583) lr 1.9877e-03 eta 0:00:03
epoch [12/200] batch [80/82] time 0.409 (0.456) data 0.278 (0.325) loss_u loss_u 0.8726 (0.9402) acc_u 12.5000 (7.8125) lr 1.9877e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1822
confident_label rate tensor(0.1658, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 520
clean true:509
clean false:11
clean_rate:0.9788461538461538
noisy true:805
noisy false:1811
after delete: len(clean_dataset) 520
after delete: len(noisy_dataset) 2616
epoch [13/200] batch [5/16] time 0.547 (0.469) data 0.416 (0.338) loss_x loss_x 1.3350 (1.4840) acc_x 71.8750 (63.1250) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [10/16] time 0.391 (0.463) data 0.261 (0.332) loss_x loss_x 1.7266 (1.5504) acc_x 50.0000 (61.5625) lr 1.9851e-03 eta 0:00:02
epoch [13/200] batch [15/16] time 0.435 (0.451) data 0.304 (0.321) loss_x loss_x 1.2705 (1.5165) acc_x 71.8750 (62.7083) lr 1.9851e-03 eta 0:00:00
epoch [13/200] batch [5/81] time 0.446 (0.446) data 0.315 (0.315) loss_u loss_u 0.9409 (0.9387) acc_u 12.5000 (8.7500) lr 1.9851e-03 eta 0:00:33
epoch [13/200] batch [10/81] time 0.337 (0.445) data 0.207 (0.314) loss_u loss_u 0.9092 (0.9341) acc_u 12.5000 (9.0625) lr 1.9851e-03 eta 0:00:31
epoch [13/200] batch [15/81] time 0.402 (0.438) data 0.271 (0.307) loss_u loss_u 0.9448 (0.9415) acc_u 12.5000 (8.1250) lr 1.9851e-03 eta 0:00:28
epoch [13/200] batch [20/81] time 0.491 (0.449) data 0.360 (0.318) loss_u loss_u 0.8447 (0.9388) acc_u 21.8750 (8.9062) lr 1.9851e-03 eta 0:00:27
epoch [13/200] batch [25/81] time 0.448 (0.453) data 0.316 (0.321) loss_u loss_u 0.9224 (0.9311) acc_u 6.2500 (9.7500) lr 1.9851e-03 eta 0:00:25
epoch [13/200] batch [30/81] time 0.414 (0.453) data 0.283 (0.322) loss_u loss_u 0.9531 (0.9343) acc_u 6.2500 (9.0625) lr 1.9851e-03 eta 0:00:23
epoch [13/200] batch [35/81] time 0.518 (0.454) data 0.387 (0.323) loss_u loss_u 0.9141 (0.9326) acc_u 9.3750 (8.9286) lr 1.9851e-03 eta 0:00:20
epoch [13/200] batch [40/81] time 0.451 (0.454) data 0.320 (0.323) loss_u loss_u 0.9336 (0.9345) acc_u 9.3750 (8.5938) lr 1.9851e-03 eta 0:00:18
epoch [13/200] batch [45/81] time 0.477 (0.452) data 0.346 (0.321) loss_u loss_u 0.9419 (0.9341) acc_u 6.2500 (8.7500) lr 1.9851e-03 eta 0:00:16
epoch [13/200] batch [50/81] time 0.405 (0.452) data 0.272 (0.320) loss_u loss_u 0.9678 (0.9352) acc_u 3.1250 (8.5000) lr 1.9851e-03 eta 0:00:13
epoch [13/200] batch [55/81] time 0.421 (0.459) data 0.291 (0.327) loss_u loss_u 0.9399 (0.9369) acc_u 6.2500 (8.2955) lr 1.9851e-03 eta 0:00:11
epoch [13/200] batch [60/81] time 0.459 (0.462) data 0.328 (0.331) loss_u loss_u 0.9199 (0.9366) acc_u 9.3750 (8.3854) lr 1.9851e-03 eta 0:00:09
epoch [13/200] batch [65/81] time 0.525 (0.462) data 0.392 (0.331) loss_u loss_u 0.9424 (0.9365) acc_u 6.2500 (8.4615) lr 1.9851e-03 eta 0:00:07
epoch [13/200] batch [70/81] time 0.438 (0.463) data 0.307 (0.332) loss_u loss_u 0.9580 (0.9367) acc_u 6.2500 (8.3929) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [75/81] time 0.582 (0.463) data 0.450 (0.332) loss_u loss_u 0.9536 (0.9373) acc_u 6.2500 (8.2917) lr 1.9851e-03 eta 0:00:02
epoch [13/200] batch [80/81] time 0.404 (0.460) data 0.274 (0.329) loss_u loss_u 0.9316 (0.9381) acc_u 9.3750 (8.2031) lr 1.9851e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1783
confident_label rate tensor(0.1725, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 541
clean true:531
clean false:10
clean_rate:0.9815157116451017
noisy true:822
noisy false:1773
after delete: len(clean_dataset) 541
after delete: len(noisy_dataset) 2595
epoch [14/200] batch [5/16] time 0.458 (0.467) data 0.327 (0.337) loss_x loss_x 1.4922 (1.5488) acc_x 62.5000 (60.6250) lr 1.9823e-03 eta 0:00:05
epoch [14/200] batch [10/16] time 0.371 (0.460) data 0.240 (0.330) loss_x loss_x 1.4834 (1.6074) acc_x 62.5000 (60.0000) lr 1.9823e-03 eta 0:00:02
epoch [14/200] batch [15/16] time 0.406 (0.464) data 0.275 (0.333) loss_x loss_x 1.3682 (1.6008) acc_x 59.3750 (59.5833) lr 1.9823e-03 eta 0:00:00
epoch [14/200] batch [5/81] time 0.413 (0.455) data 0.282 (0.324) loss_u loss_u 0.9502 (0.9384) acc_u 6.2500 (6.8750) lr 1.9823e-03 eta 0:00:34
epoch [14/200] batch [10/81] time 0.529 (0.452) data 0.397 (0.321) loss_u loss_u 0.8940 (0.9375) acc_u 12.5000 (7.8125) lr 1.9823e-03 eta 0:00:32
epoch [14/200] batch [15/81] time 0.420 (0.449) data 0.289 (0.318) loss_u loss_u 0.9575 (0.9387) acc_u 6.2500 (7.9167) lr 1.9823e-03 eta 0:00:29
epoch [14/200] batch [20/81] time 0.363 (0.452) data 0.232 (0.321) loss_u loss_u 0.9048 (0.9337) acc_u 15.6250 (8.5938) lr 1.9823e-03 eta 0:00:27
epoch [14/200] batch [25/81] time 0.592 (0.452) data 0.461 (0.321) loss_u loss_u 0.9282 (0.9337) acc_u 9.3750 (8.8750) lr 1.9823e-03 eta 0:00:25
epoch [14/200] batch [30/81] time 0.370 (0.448) data 0.239 (0.317) loss_u loss_u 0.8970 (0.9319) acc_u 9.3750 (8.8542) lr 1.9823e-03 eta 0:00:22
epoch [14/200] batch [35/81] time 0.419 (0.447) data 0.288 (0.316) loss_u loss_u 0.9683 (0.9334) acc_u 0.0000 (8.5714) lr 1.9823e-03 eta 0:00:20
epoch [14/200] batch [40/81] time 0.728 (0.450) data 0.597 (0.319) loss_u loss_u 0.9507 (0.9358) acc_u 6.2500 (8.2031) lr 1.9823e-03 eta 0:00:18
epoch [14/200] batch [45/81] time 0.427 (0.449) data 0.296 (0.318) loss_u loss_u 0.9336 (0.9382) acc_u 12.5000 (8.1250) lr 1.9823e-03 eta 0:00:16
epoch [14/200] batch [50/81] time 0.513 (0.448) data 0.382 (0.317) loss_u loss_u 0.9570 (0.9382) acc_u 3.1250 (8.0625) lr 1.9823e-03 eta 0:00:13
epoch [14/200] batch [55/81] time 0.497 (0.446) data 0.366 (0.315) loss_u loss_u 0.9746 (0.9396) acc_u 3.1250 (7.8977) lr 1.9823e-03 eta 0:00:11
epoch [14/200] batch [60/81] time 0.406 (0.445) data 0.268 (0.314) loss_u loss_u 0.9565 (0.9400) acc_u 6.2500 (7.8646) lr 1.9823e-03 eta 0:00:09
epoch [14/200] batch [65/81] time 0.587 (0.452) data 0.455 (0.321) loss_u loss_u 0.9116 (0.9410) acc_u 9.3750 (7.6923) lr 1.9823e-03 eta 0:00:07
epoch [14/200] batch [70/81] time 0.426 (0.453) data 0.295 (0.321) loss_u loss_u 0.9678 (0.9429) acc_u 3.1250 (7.2768) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [75/81] time 0.542 (0.453) data 0.412 (0.322) loss_u loss_u 0.9600 (0.9429) acc_u 6.2500 (7.3750) lr 1.9823e-03 eta 0:00:02
epoch [14/200] batch [80/81] time 0.457 (0.454) data 0.325 (0.322) loss_u loss_u 0.9883 (0.9422) acc_u 0.0000 (7.5000) lr 1.9823e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1764
confident_label rate tensor(0.1703, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 534
clean true:522
clean false:12
clean_rate:0.9775280898876404
noisy true:850
noisy false:1752
after delete: len(clean_dataset) 534
after delete: len(noisy_dataset) 2602
epoch [15/200] batch [5/16] time 0.473 (0.518) data 0.342 (0.387) loss_x loss_x 1.7441 (1.4820) acc_x 62.5000 (63.1250) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [10/16] time 0.360 (0.490) data 0.229 (0.359) loss_x loss_x 1.6230 (1.4595) acc_x 46.8750 (61.8750) lr 1.9792e-03 eta 0:00:02
epoch [15/200] batch [15/16] time 0.466 (0.489) data 0.335 (0.358) loss_x loss_x 0.8174 (1.4514) acc_x 75.0000 (61.6667) lr 1.9792e-03 eta 0:00:00
epoch [15/200] batch [5/81] time 0.373 (0.467) data 0.241 (0.336) loss_u loss_u 0.9453 (0.9347) acc_u 3.1250 (6.8750) lr 1.9792e-03 eta 0:00:35
epoch [15/200] batch [10/81] time 0.519 (0.463) data 0.388 (0.332) loss_u loss_u 0.9390 (0.9333) acc_u 9.3750 (7.5000) lr 1.9792e-03 eta 0:00:32
epoch [15/200] batch [15/81] time 0.361 (0.452) data 0.230 (0.321) loss_u loss_u 0.9331 (0.9359) acc_u 12.5000 (7.9167) lr 1.9792e-03 eta 0:00:29
epoch [15/200] batch [20/81] time 0.408 (0.446) data 0.276 (0.315) loss_u loss_u 0.9819 (0.9393) acc_u 0.0000 (7.3438) lr 1.9792e-03 eta 0:00:27
epoch [15/200] batch [25/81] time 0.441 (0.455) data 0.308 (0.324) loss_u loss_u 0.9761 (0.9428) acc_u 9.3750 (7.1250) lr 1.9792e-03 eta 0:00:25
epoch [15/200] batch [30/81] time 0.332 (0.448) data 0.200 (0.317) loss_u loss_u 0.9707 (0.9424) acc_u 9.3750 (7.3958) lr 1.9792e-03 eta 0:00:22
epoch [15/200] batch [35/81] time 0.432 (0.453) data 0.302 (0.322) loss_u loss_u 0.9873 (0.9426) acc_u 0.0000 (7.3214) lr 1.9792e-03 eta 0:00:20
epoch [15/200] batch [40/81] time 0.542 (0.450) data 0.411 (0.319) loss_u loss_u 0.9619 (0.9448) acc_u 9.3750 (6.9531) lr 1.9792e-03 eta 0:00:18
epoch [15/200] batch [45/81] time 0.420 (0.451) data 0.289 (0.320) loss_u loss_u 0.9346 (0.9448) acc_u 12.5000 (7.0139) lr 1.9792e-03 eta 0:00:16
epoch [15/200] batch [50/81] time 0.429 (0.454) data 0.298 (0.322) loss_u loss_u 0.9131 (0.9434) acc_u 12.5000 (7.1875) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [55/81] time 0.465 (0.453) data 0.334 (0.321) loss_u loss_u 0.9600 (0.9446) acc_u 3.1250 (7.0455) lr 1.9792e-03 eta 0:00:11
epoch [15/200] batch [60/81] time 0.327 (0.451) data 0.196 (0.319) loss_u loss_u 0.9746 (0.9445) acc_u 3.1250 (6.9792) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [65/81] time 0.417 (0.451) data 0.285 (0.320) loss_u loss_u 0.9292 (0.9439) acc_u 3.1250 (6.9712) lr 1.9792e-03 eta 0:00:07
epoch [15/200] batch [70/81] time 0.399 (0.449) data 0.268 (0.318) loss_u loss_u 0.9229 (0.9438) acc_u 9.3750 (7.1429) lr 1.9792e-03 eta 0:00:04
epoch [15/200] batch [75/81] time 0.338 (0.450) data 0.207 (0.319) loss_u loss_u 0.9438 (0.9432) acc_u 6.2500 (7.1667) lr 1.9792e-03 eta 0:00:02
epoch [15/200] batch [80/81] time 0.436 (0.450) data 0.305 (0.318) loss_u loss_u 0.9351 (0.9401) acc_u 9.3750 (7.6172) lr 1.9792e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1791
confident_label rate tensor(0.1671, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 524
clean true:514
clean false:10
clean_rate:0.9809160305343512
noisy true:831
noisy false:1781
after delete: len(clean_dataset) 524
after delete: len(noisy_dataset) 2612
epoch [16/200] batch [5/16] time 0.510 (0.628) data 0.380 (0.497) loss_x loss_x 1.1934 (1.5424) acc_x 65.6250 (59.3750) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [10/16] time 0.653 (0.549) data 0.523 (0.418) loss_x loss_x 1.5781 (1.4460) acc_x 62.5000 (61.5625) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [15/16] time 0.418 (0.522) data 0.286 (0.391) loss_x loss_x 1.5654 (1.4435) acc_x 53.1250 (60.8333) lr 1.9759e-03 eta 0:00:00
epoch [16/200] batch [5/81] time 0.380 (0.495) data 0.249 (0.364) loss_u loss_u 0.8945 (0.9509) acc_u 15.6250 (6.2500) lr 1.9759e-03 eta 0:00:37
epoch [16/200] batch [10/81] time 0.432 (0.477) data 0.301 (0.346) loss_u loss_u 0.9321 (0.9525) acc_u 9.3750 (6.5625) lr 1.9759e-03 eta 0:00:33
epoch [16/200] batch [15/81] time 0.474 (0.465) data 0.343 (0.333) loss_u loss_u 0.9941 (0.9512) acc_u 0.0000 (6.6667) lr 1.9759e-03 eta 0:00:30
epoch [16/200] batch [20/81] time 0.482 (0.458) data 0.350 (0.327) loss_u loss_u 0.9497 (0.9421) acc_u 9.3750 (7.9688) lr 1.9759e-03 eta 0:00:27
epoch [16/200] batch [25/81] time 0.369 (0.464) data 0.237 (0.333) loss_u loss_u 0.9512 (0.9405) acc_u 6.2500 (8.2500) lr 1.9759e-03 eta 0:00:25
epoch [16/200] batch [30/81] time 0.450 (0.462) data 0.319 (0.330) loss_u loss_u 0.9556 (0.9415) acc_u 6.2500 (7.7083) lr 1.9759e-03 eta 0:00:23
epoch [16/200] batch [35/81] time 0.457 (0.458) data 0.326 (0.327) loss_u loss_u 0.9810 (0.9423) acc_u 3.1250 (7.4107) lr 1.9759e-03 eta 0:00:21
epoch [16/200] batch [40/81] time 0.341 (0.455) data 0.210 (0.324) loss_u loss_u 0.9087 (0.9406) acc_u 12.5000 (7.7344) lr 1.9759e-03 eta 0:00:18
epoch [16/200] batch [45/81] time 0.340 (0.448) data 0.208 (0.317) loss_u loss_u 0.9351 (0.9411) acc_u 9.3750 (7.7083) lr 1.9759e-03 eta 0:00:16
epoch [16/200] batch [50/81] time 0.513 (0.451) data 0.382 (0.319) loss_u loss_u 0.9424 (0.9400) acc_u 6.2500 (7.8750) lr 1.9759e-03 eta 0:00:13
epoch [16/200] batch [55/81] time 0.546 (0.451) data 0.413 (0.319) loss_u loss_u 0.9580 (0.9418) acc_u 6.2500 (7.7841) lr 1.9759e-03 eta 0:00:11
epoch [16/200] batch [60/81] time 0.387 (0.450) data 0.255 (0.318) loss_u loss_u 0.9878 (0.9416) acc_u 0.0000 (7.7083) lr 1.9759e-03 eta 0:00:09
epoch [16/200] batch [65/81] time 0.626 (0.454) data 0.494 (0.323) loss_u loss_u 0.9678 (0.9396) acc_u 6.2500 (7.9808) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [70/81] time 0.493 (0.452) data 0.362 (0.321) loss_u loss_u 0.9844 (0.9390) acc_u 3.1250 (8.0804) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [75/81] time 0.376 (0.454) data 0.244 (0.323) loss_u loss_u 0.8779 (0.9386) acc_u 18.7500 (8.1667) lr 1.9759e-03 eta 0:00:02
epoch [16/200] batch [80/81] time 0.371 (0.453) data 0.240 (0.322) loss_u loss_u 0.9629 (0.9391) acc_u 0.0000 (7.9688) lr 1.9759e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1747
confident_label rate tensor(0.1671, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 524
clean true:517
clean false:7
clean_rate:0.9866412213740458
noisy true:872
noisy false:1740
after delete: len(clean_dataset) 524
after delete: len(noisy_dataset) 2612
epoch [17/200] batch [5/16] time 0.507 (0.478) data 0.376 (0.347) loss_x loss_x 1.4570 (1.3165) acc_x 59.3750 (63.7500) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [10/16] time 0.381 (0.447) data 0.251 (0.316) loss_x loss_x 1.3633 (1.4534) acc_x 53.1250 (61.5625) lr 1.9724e-03 eta 0:00:02
epoch [17/200] batch [15/16] time 0.555 (0.471) data 0.424 (0.340) loss_x loss_x 1.1621 (1.4199) acc_x 71.8750 (61.4583) lr 1.9724e-03 eta 0:00:00
epoch [17/200] batch [5/81] time 0.457 (0.457) data 0.326 (0.326) loss_u loss_u 0.9409 (0.9446) acc_u 3.1250 (5.0000) lr 1.9724e-03 eta 0:00:34
epoch [17/200] batch [10/81] time 0.385 (0.461) data 0.253 (0.330) loss_u loss_u 0.9390 (0.9354) acc_u 6.2500 (6.5625) lr 1.9724e-03 eta 0:00:32
epoch [17/200] batch [15/81] time 0.355 (0.449) data 0.224 (0.318) loss_u loss_u 0.8574 (0.9298) acc_u 18.7500 (7.2917) lr 1.9724e-03 eta 0:00:29
epoch [17/200] batch [20/81] time 0.484 (0.458) data 0.353 (0.327) loss_u loss_u 0.9175 (0.9270) acc_u 9.3750 (7.8125) lr 1.9724e-03 eta 0:00:27
epoch [17/200] batch [25/81] time 0.456 (0.463) data 0.325 (0.332) loss_u loss_u 0.9639 (0.9314) acc_u 3.1250 (7.5000) lr 1.9724e-03 eta 0:00:25
epoch [17/200] batch [30/81] time 0.463 (0.454) data 0.333 (0.323) loss_u loss_u 0.9575 (0.9359) acc_u 6.2500 (7.1875) lr 1.9724e-03 eta 0:00:23
epoch [17/200] batch [35/81] time 0.461 (0.455) data 0.330 (0.324) loss_u loss_u 0.9834 (0.9386) acc_u 0.0000 (7.0536) lr 1.9724e-03 eta 0:00:20
epoch [17/200] batch [40/81] time 0.551 (0.460) data 0.421 (0.329) loss_u loss_u 0.9155 (0.9360) acc_u 9.3750 (7.5781) lr 1.9724e-03 eta 0:00:18
epoch [17/200] batch [45/81] time 0.430 (0.460) data 0.298 (0.329) loss_u loss_u 0.9629 (0.9387) acc_u 3.1250 (7.1528) lr 1.9724e-03 eta 0:00:16
epoch [17/200] batch [50/81] time 0.356 (0.458) data 0.224 (0.327) loss_u loss_u 0.9380 (0.9391) acc_u 6.2500 (7.0000) lr 1.9724e-03 eta 0:00:14
epoch [17/200] batch [55/81] time 0.435 (0.457) data 0.302 (0.326) loss_u loss_u 0.9297 (0.9369) acc_u 12.5000 (7.6136) lr 1.9724e-03 eta 0:00:11
epoch [17/200] batch [60/81] time 0.634 (0.457) data 0.504 (0.326) loss_u loss_u 0.9463 (0.9370) acc_u 6.2500 (7.5521) lr 1.9724e-03 eta 0:00:09
epoch [17/200] batch [65/81] time 0.442 (0.456) data 0.310 (0.325) loss_u loss_u 0.9717 (0.9369) acc_u 6.2500 (7.7404) lr 1.9724e-03 eta 0:00:07
epoch [17/200] batch [70/81] time 0.564 (0.458) data 0.432 (0.326) loss_u loss_u 0.9556 (0.9385) acc_u 6.2500 (7.5893) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [75/81] time 0.366 (0.459) data 0.234 (0.328) loss_u loss_u 0.9536 (0.9384) acc_u 6.2500 (7.6250) lr 1.9724e-03 eta 0:00:02
epoch [17/200] batch [80/81] time 0.427 (0.459) data 0.295 (0.328) loss_u loss_u 0.9512 (0.9375) acc_u 3.1250 (7.8906) lr 1.9724e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1809
confident_label rate tensor(0.1665, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 522
clean true:516
clean false:6
clean_rate:0.9885057471264368
noisy true:811
noisy false:1803
after delete: len(clean_dataset) 522
after delete: len(noisy_dataset) 2614
epoch [18/200] batch [5/16] time 0.406 (0.421) data 0.275 (0.290) loss_x loss_x 1.2119 (1.4484) acc_x 62.5000 (63.1250) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [10/16] time 0.528 (0.462) data 0.397 (0.331) loss_x loss_x 1.2979 (1.3116) acc_x 81.2500 (65.9375) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [15/16] time 0.373 (0.451) data 0.242 (0.320) loss_x loss_x 1.4473 (1.3715) acc_x 65.6250 (64.5833) lr 1.9686e-03 eta 0:00:00
epoch [18/200] batch [5/81] time 0.402 (0.443) data 0.272 (0.312) loss_u loss_u 0.9214 (0.9359) acc_u 15.6250 (10.6250) lr 1.9686e-03 eta 0:00:33
epoch [18/200] batch [10/81] time 0.471 (0.448) data 0.340 (0.317) loss_u loss_u 0.8989 (0.9253) acc_u 12.5000 (10.3125) lr 1.9686e-03 eta 0:00:31
epoch [18/200] batch [15/81] time 0.456 (0.454) data 0.323 (0.323) loss_u loss_u 0.8950 (0.9326) acc_u 15.6250 (9.3750) lr 1.9686e-03 eta 0:00:29
epoch [18/200] batch [20/81] time 0.363 (0.448) data 0.233 (0.317) loss_u loss_u 0.9541 (0.9326) acc_u 9.3750 (9.3750) lr 1.9686e-03 eta 0:00:27
epoch [18/200] batch [25/81] time 0.429 (0.450) data 0.297 (0.319) loss_u loss_u 0.8940 (0.9338) acc_u 18.7500 (9.6250) lr 1.9686e-03 eta 0:00:25
epoch [18/200] batch [30/81] time 0.394 (0.447) data 0.263 (0.316) loss_u loss_u 0.9341 (0.9349) acc_u 9.3750 (9.3750) lr 1.9686e-03 eta 0:00:22
epoch [18/200] batch [35/81] time 0.380 (0.443) data 0.248 (0.312) loss_u loss_u 0.9629 (0.9387) acc_u 3.1250 (8.6607) lr 1.9686e-03 eta 0:00:20
epoch [18/200] batch [40/81] time 0.368 (0.442) data 0.236 (0.311) loss_u loss_u 0.9624 (0.9384) acc_u 3.1250 (8.5156) lr 1.9686e-03 eta 0:00:18
epoch [18/200] batch [45/81] time 0.553 (0.445) data 0.421 (0.314) loss_u loss_u 0.9644 (0.9381) acc_u 3.1250 (8.6111) lr 1.9686e-03 eta 0:00:16
epoch [18/200] batch [50/81] time 0.671 (0.447) data 0.537 (0.315) loss_u loss_u 0.9204 (0.9385) acc_u 12.5000 (8.4375) lr 1.9686e-03 eta 0:00:13
epoch [18/200] batch [55/81] time 0.398 (0.456) data 0.265 (0.325) loss_u loss_u 0.9370 (0.9374) acc_u 9.3750 (8.6364) lr 1.9686e-03 eta 0:00:11
epoch [18/200] batch [60/81] time 0.379 (0.457) data 0.247 (0.326) loss_u loss_u 0.9917 (0.9394) acc_u 3.1250 (8.3854) lr 1.9686e-03 eta 0:00:09
epoch [18/200] batch [65/81] time 0.495 (0.457) data 0.364 (0.326) loss_u loss_u 0.9810 (0.9393) acc_u 3.1250 (8.4135) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [70/81] time 0.444 (0.457) data 0.313 (0.326) loss_u loss_u 0.9883 (0.9398) acc_u 0.0000 (8.2589) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [75/81] time 0.365 (0.457) data 0.234 (0.325) loss_u loss_u 0.9526 (0.9385) acc_u 6.2500 (8.4167) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [80/81] time 0.369 (0.452) data 0.237 (0.321) loss_u loss_u 0.9473 (0.9390) acc_u 6.2500 (8.3594) lr 1.9686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1763
confident_label rate tensor(0.1776, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 557
clean true:552
clean false:5
clean_rate:0.9910233393177738
noisy true:821
noisy false:1758
after delete: len(clean_dataset) 557
after delete: len(noisy_dataset) 2579
epoch [19/200] batch [5/17] time 0.608 (0.564) data 0.477 (0.433) loss_x loss_x 1.6396 (1.7271) acc_x 59.3750 (59.3750) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [10/17] time 0.435 (0.510) data 0.305 (0.380) loss_x loss_x 1.7891 (1.6501) acc_x 56.2500 (58.4375) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [15/17] time 0.452 (0.505) data 0.321 (0.374) loss_x loss_x 1.4766 (1.6627) acc_x 56.2500 (58.9583) lr 1.9646e-03 eta 0:00:01
epoch [19/200] batch [5/80] time 0.349 (0.498) data 0.217 (0.367) loss_u loss_u 0.9673 (0.9349) acc_u 3.1250 (8.1250) lr 1.9646e-03 eta 0:00:37
epoch [19/200] batch [10/80] time 0.391 (0.484) data 0.260 (0.353) loss_u loss_u 0.9419 (0.9378) acc_u 6.2500 (7.1875) lr 1.9646e-03 eta 0:00:33
epoch [19/200] batch [15/80] time 0.443 (0.481) data 0.313 (0.350) loss_u loss_u 0.9229 (0.9376) acc_u 12.5000 (8.1250) lr 1.9646e-03 eta 0:00:31
epoch [19/200] batch [20/80] time 0.404 (0.475) data 0.273 (0.344) loss_u loss_u 0.9302 (0.9377) acc_u 6.2500 (7.9688) lr 1.9646e-03 eta 0:00:28
epoch [19/200] batch [25/80] time 0.434 (0.473) data 0.304 (0.343) loss_u loss_u 0.8999 (0.9359) acc_u 18.7500 (8.2500) lr 1.9646e-03 eta 0:00:26
epoch [19/200] batch [30/80] time 0.400 (0.471) data 0.270 (0.340) loss_u loss_u 0.9233 (0.9352) acc_u 12.5000 (8.4375) lr 1.9646e-03 eta 0:00:23
epoch [19/200] batch [35/80] time 0.401 (0.469) data 0.269 (0.338) loss_u loss_u 0.9697 (0.9361) acc_u 6.2500 (8.4821) lr 1.9646e-03 eta 0:00:21
epoch [19/200] batch [40/80] time 0.495 (0.466) data 0.365 (0.335) loss_u loss_u 0.9800 (0.9375) acc_u 0.0000 (8.1250) lr 1.9646e-03 eta 0:00:18
epoch [19/200] batch [45/80] time 0.435 (0.462) data 0.305 (0.331) loss_u loss_u 0.9810 (0.9391) acc_u 0.0000 (7.9861) lr 1.9646e-03 eta 0:00:16
epoch [19/200] batch [50/80] time 0.408 (0.461) data 0.276 (0.330) loss_u loss_u 0.9507 (0.9399) acc_u 6.2500 (7.8125) lr 1.9646e-03 eta 0:00:13
epoch [19/200] batch [55/80] time 0.487 (0.460) data 0.355 (0.329) loss_u loss_u 0.9487 (0.9405) acc_u 3.1250 (7.6136) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [60/80] time 0.420 (0.459) data 0.289 (0.327) loss_u loss_u 0.9707 (0.9423) acc_u 3.1250 (7.2917) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [65/80] time 0.439 (0.454) data 0.307 (0.323) loss_u loss_u 0.9209 (0.9431) acc_u 9.3750 (7.1154) lr 1.9646e-03 eta 0:00:06
epoch [19/200] batch [70/80] time 0.433 (0.451) data 0.302 (0.320) loss_u loss_u 0.9536 (0.9423) acc_u 6.2500 (7.3214) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [75/80] time 0.567 (0.450) data 0.434 (0.319) loss_u loss_u 0.9365 (0.9422) acc_u 9.3750 (7.3750) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [80/80] time 0.432 (0.452) data 0.302 (0.321) loss_u loss_u 0.9419 (0.9416) acc_u 6.2500 (7.4219) lr 1.9646e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1763
confident_label rate tensor(0.1767, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 554
clean true:547
clean false:7
clean_rate:0.9873646209386282
noisy true:826
noisy false:1756
after delete: len(clean_dataset) 554
after delete: len(noisy_dataset) 2582
epoch [20/200] batch [5/17] time 0.312 (0.478) data 0.182 (0.348) loss_x loss_x 1.9629 (1.6365) acc_x 53.1250 (59.3750) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [10/17] time 0.413 (0.480) data 0.282 (0.349) loss_x loss_x 1.4814 (1.5229) acc_x 62.5000 (62.8125) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [15/17] time 0.692 (0.495) data 0.561 (0.364) loss_x loss_x 1.0615 (1.4967) acc_x 65.6250 (61.2500) lr 1.9603e-03 eta 0:00:00
epoch [20/200] batch [5/80] time 0.398 (0.478) data 0.267 (0.347) loss_u loss_u 0.9902 (0.9670) acc_u 0.0000 (4.3750) lr 1.9603e-03 eta 0:00:35
epoch [20/200] batch [10/80] time 0.357 (0.463) data 0.226 (0.332) loss_u loss_u 0.9336 (0.9593) acc_u 9.3750 (5.6250) lr 1.9603e-03 eta 0:00:32
epoch [20/200] batch [15/80] time 0.466 (0.456) data 0.335 (0.325) loss_u loss_u 0.9624 (0.9566) acc_u 3.1250 (5.4167) lr 1.9603e-03 eta 0:00:29
epoch [20/200] batch [20/80] time 0.441 (0.457) data 0.310 (0.326) loss_u loss_u 0.9536 (0.9524) acc_u 3.1250 (6.0938) lr 1.9603e-03 eta 0:00:27
epoch [20/200] batch [25/80] time 0.435 (0.448) data 0.304 (0.317) loss_u loss_u 0.8823 (0.9482) acc_u 15.6250 (6.7500) lr 1.9603e-03 eta 0:00:24
epoch [20/200] batch [30/80] time 0.474 (0.456) data 0.343 (0.325) loss_u loss_u 0.9058 (0.9454) acc_u 15.6250 (7.2917) lr 1.9603e-03 eta 0:00:22
epoch [20/200] batch [35/80] time 0.601 (0.461) data 0.468 (0.329) loss_u loss_u 0.9609 (0.9468) acc_u 3.1250 (7.0536) lr 1.9603e-03 eta 0:00:20
epoch [20/200] batch [40/80] time 0.415 (0.459) data 0.285 (0.328) loss_u loss_u 0.9287 (0.9453) acc_u 9.3750 (7.4219) lr 1.9603e-03 eta 0:00:18
epoch [20/200] batch [45/80] time 0.435 (0.457) data 0.300 (0.325) loss_u loss_u 0.9497 (0.9443) acc_u 3.1250 (7.5000) lr 1.9603e-03 eta 0:00:15
epoch [20/200] batch [50/80] time 0.534 (0.461) data 0.399 (0.329) loss_u loss_u 0.9277 (0.9425) acc_u 9.3750 (7.6875) lr 1.9603e-03 eta 0:00:13
epoch [20/200] batch [55/80] time 0.463 (0.460) data 0.331 (0.328) loss_u loss_u 0.8750 (0.9422) acc_u 15.6250 (7.7273) lr 1.9603e-03 eta 0:00:11
epoch [20/200] batch [60/80] time 0.411 (0.462) data 0.280 (0.330) loss_u loss_u 0.9380 (0.9427) acc_u 3.1250 (7.5521) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [65/80] time 0.513 (0.464) data 0.381 (0.332) loss_u loss_u 0.9248 (0.9435) acc_u 15.6250 (7.5962) lr 1.9603e-03 eta 0:00:06
epoch [20/200] batch [70/80] time 0.490 (0.465) data 0.358 (0.333) loss_u loss_u 0.9019 (0.9437) acc_u 15.6250 (7.5000) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [75/80] time 0.426 (0.465) data 0.295 (0.333) loss_u loss_u 0.9844 (0.9438) acc_u 0.0000 (7.2917) lr 1.9603e-03 eta 0:00:02
epoch [20/200] batch [80/80] time 0.370 (0.465) data 0.239 (0.333) loss_u loss_u 0.9517 (0.9440) acc_u 6.2500 (7.2266) lr 1.9603e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1789
confident_label rate tensor(0.1696, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 532
clean true:522
clean false:10
clean_rate:0.981203007518797
noisy true:825
noisy false:1779
after delete: len(clean_dataset) 532
after delete: len(noisy_dataset) 2604
epoch [21/200] batch [5/16] time 0.546 (0.502) data 0.415 (0.371) loss_x loss_x 1.2705 (1.2846) acc_x 65.6250 (65.0000) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [10/16] time 0.602 (0.507) data 0.471 (0.377) loss_x loss_x 1.6924 (1.4490) acc_x 56.2500 (61.5625) lr 1.9558e-03 eta 0:00:03
epoch [21/200] batch [15/16] time 0.384 (0.488) data 0.253 (0.357) loss_x loss_x 1.7168 (1.4989) acc_x 56.2500 (60.8333) lr 1.9558e-03 eta 0:00:00
epoch [21/200] batch [5/81] time 0.534 (0.495) data 0.404 (0.364) loss_u loss_u 0.9116 (0.9443) acc_u 12.5000 (5.6250) lr 1.9558e-03 eta 0:00:37
epoch [21/200] batch [10/81] time 0.363 (0.498) data 0.233 (0.367) loss_u loss_u 0.9492 (0.9454) acc_u 6.2500 (6.2500) lr 1.9558e-03 eta 0:00:35
epoch [21/200] batch [15/81] time 0.414 (0.488) data 0.283 (0.358) loss_u loss_u 0.9253 (0.9373) acc_u 12.5000 (7.9167) lr 1.9558e-03 eta 0:00:32
epoch [21/200] batch [20/81] time 0.423 (0.481) data 0.293 (0.350) loss_u loss_u 0.8999 (0.9320) acc_u 15.6250 (8.7500) lr 1.9558e-03 eta 0:00:29
epoch [21/200] batch [25/81] time 0.387 (0.473) data 0.256 (0.342) loss_u loss_u 0.9424 (0.9348) acc_u 9.3750 (8.1250) lr 1.9558e-03 eta 0:00:26
epoch [21/200] batch [30/81] time 0.526 (0.475) data 0.393 (0.344) loss_u loss_u 0.9541 (0.9382) acc_u 6.2500 (7.7083) lr 1.9558e-03 eta 0:00:24
epoch [21/200] batch [35/81] time 0.468 (0.471) data 0.336 (0.340) loss_u loss_u 0.9673 (0.9384) acc_u 6.2500 (8.0357) lr 1.9558e-03 eta 0:00:21
epoch [21/200] batch [40/81] time 0.527 (0.468) data 0.396 (0.337) loss_u loss_u 0.9229 (0.9391) acc_u 9.3750 (7.8906) lr 1.9558e-03 eta 0:00:19
epoch [21/200] batch [45/81] time 0.440 (0.468) data 0.309 (0.337) loss_u loss_u 0.9155 (0.9395) acc_u 9.3750 (7.9861) lr 1.9558e-03 eta 0:00:16
epoch [21/200] batch [50/81] time 0.449 (0.468) data 0.317 (0.336) loss_u loss_u 0.9468 (0.9389) acc_u 3.1250 (8.0000) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [55/81] time 0.400 (0.463) data 0.268 (0.332) loss_u loss_u 0.9575 (0.9399) acc_u 3.1250 (7.6136) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [60/81] time 0.478 (0.459) data 0.346 (0.328) loss_u loss_u 0.8569 (0.9374) acc_u 18.7500 (8.0208) lr 1.9558e-03 eta 0:00:09
epoch [21/200] batch [65/81] time 0.681 (0.462) data 0.550 (0.331) loss_u loss_u 0.9609 (0.9385) acc_u 3.1250 (7.8846) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [70/81] time 0.453 (0.460) data 0.323 (0.328) loss_u loss_u 0.9170 (0.9377) acc_u 9.3750 (7.9911) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [75/81] time 0.328 (0.459) data 0.197 (0.327) loss_u loss_u 0.9424 (0.9391) acc_u 6.2500 (7.7917) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [80/81] time 0.416 (0.458) data 0.284 (0.326) loss_u loss_u 0.8960 (0.9384) acc_u 12.5000 (7.9297) lr 1.9558e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1728
confident_label rate tensor(0.1834, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 575
clean true:566
clean false:9
clean_rate:0.9843478260869565
noisy true:842
noisy false:1719
after delete: len(clean_dataset) 575
after delete: len(noisy_dataset) 2561
epoch [22/200] batch [5/17] time 0.535 (0.470) data 0.403 (0.339) loss_x loss_x 1.9277 (1.1675) acc_x 59.3750 (68.7500) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [10/17] time 0.429 (0.465) data 0.298 (0.335) loss_x loss_x 1.6045 (1.2849) acc_x 50.0000 (64.6875) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [15/17] time 0.392 (0.461) data 0.262 (0.331) loss_x loss_x 1.1875 (1.2938) acc_x 75.0000 (65.0000) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/80] time 0.422 (0.454) data 0.291 (0.324) loss_u loss_u 0.9434 (0.9455) acc_u 6.2500 (6.2500) lr 1.9511e-03 eta 0:00:34
epoch [22/200] batch [10/80] time 0.347 (0.443) data 0.217 (0.312) loss_u loss_u 0.9453 (0.9424) acc_u 9.3750 (7.5000) lr 1.9511e-03 eta 0:00:31
epoch [22/200] batch [15/80] time 0.473 (0.437) data 0.342 (0.306) loss_u loss_u 0.9590 (0.9446) acc_u 3.1250 (6.8750) lr 1.9511e-03 eta 0:00:28
epoch [22/200] batch [20/80] time 0.390 (0.435) data 0.258 (0.304) loss_u loss_u 0.9468 (0.9429) acc_u 6.2500 (7.0312) lr 1.9511e-03 eta 0:00:26
epoch [22/200] batch [25/80] time 0.393 (0.437) data 0.261 (0.307) loss_u loss_u 0.9551 (0.9441) acc_u 3.1250 (6.7500) lr 1.9511e-03 eta 0:00:24
epoch [22/200] batch [30/80] time 0.378 (0.434) data 0.246 (0.303) loss_u loss_u 0.9683 (0.9414) acc_u 3.1250 (7.1875) lr 1.9511e-03 eta 0:00:21
epoch [22/200] batch [35/80] time 0.425 (0.437) data 0.293 (0.306) loss_u loss_u 0.8672 (0.9393) acc_u 15.6250 (7.2321) lr 1.9511e-03 eta 0:00:19
epoch [22/200] batch [40/80] time 0.545 (0.443) data 0.414 (0.312) loss_u loss_u 0.9580 (0.9418) acc_u 9.3750 (7.1094) lr 1.9511e-03 eta 0:00:17
epoch [22/200] batch [45/80] time 0.514 (0.444) data 0.383 (0.312) loss_u loss_u 0.9102 (0.9398) acc_u 12.5000 (7.5000) lr 1.9511e-03 eta 0:00:15
epoch [22/200] batch [50/80] time 0.427 (0.444) data 0.294 (0.313) loss_u loss_u 0.8799 (0.9399) acc_u 15.6250 (7.5625) lr 1.9511e-03 eta 0:00:13
epoch [22/200] batch [55/80] time 0.502 (0.446) data 0.371 (0.315) loss_u loss_u 0.9878 (0.9372) acc_u 3.1250 (7.7273) lr 1.9511e-03 eta 0:00:11
epoch [22/200] batch [60/80] time 0.398 (0.443) data 0.267 (0.312) loss_u loss_u 0.9707 (0.9381) acc_u 3.1250 (7.5521) lr 1.9511e-03 eta 0:00:08
epoch [22/200] batch [65/80] time 0.482 (0.442) data 0.350 (0.311) loss_u loss_u 0.9214 (0.9370) acc_u 9.3750 (7.7885) lr 1.9511e-03 eta 0:00:06
epoch [22/200] batch [70/80] time 0.415 (0.443) data 0.284 (0.312) loss_u loss_u 0.9619 (0.9358) acc_u 6.2500 (8.0357) lr 1.9511e-03 eta 0:00:04
epoch [22/200] batch [75/80] time 0.354 (0.442) data 0.222 (0.310) loss_u loss_u 0.9868 (0.9383) acc_u 0.0000 (7.6667) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [80/80] time 0.734 (0.444) data 0.603 (0.313) loss_u loss_u 0.9829 (0.9395) acc_u 0.0000 (7.4609) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1748
confident_label rate tensor(0.1741, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 546
clean true:534
clean false:12
clean_rate:0.978021978021978
noisy true:854
noisy false:1736
after delete: len(clean_dataset) 546
after delete: len(noisy_dataset) 2590
epoch [23/200] batch [5/17] time 0.498 (0.455) data 0.368 (0.324) loss_x loss_x 1.4746 (1.6432) acc_x 59.3750 (59.3750) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [10/17] time 0.562 (0.477) data 0.431 (0.346) loss_x loss_x 1.1094 (1.5932) acc_x 75.0000 (61.5625) lr 1.9461e-03 eta 0:00:03
epoch [23/200] batch [15/17] time 0.494 (0.471) data 0.362 (0.340) loss_x loss_x 1.7646 (1.5918) acc_x 59.3750 (61.8750) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/80] time 0.598 (0.474) data 0.467 (0.343) loss_u loss_u 0.9219 (0.9387) acc_u 12.5000 (8.1250) lr 1.9461e-03 eta 0:00:35
epoch [23/200] batch [10/80] time 0.528 (0.463) data 0.398 (0.332) loss_u loss_u 0.9829 (0.9365) acc_u 3.1250 (8.1250) lr 1.9461e-03 eta 0:00:32
epoch [23/200] batch [15/80] time 0.478 (0.456) data 0.346 (0.325) loss_u loss_u 0.9175 (0.9405) acc_u 12.5000 (7.9167) lr 1.9461e-03 eta 0:00:29
epoch [23/200] batch [20/80] time 0.531 (0.458) data 0.399 (0.327) loss_u loss_u 0.9751 (0.9439) acc_u 3.1250 (7.3438) lr 1.9461e-03 eta 0:00:27
epoch [23/200] batch [25/80] time 0.516 (0.456) data 0.385 (0.324) loss_u loss_u 0.9761 (0.9448) acc_u 3.1250 (7.3750) lr 1.9461e-03 eta 0:00:25
epoch [23/200] batch [30/80] time 0.367 (0.457) data 0.236 (0.326) loss_u loss_u 0.9102 (0.9434) acc_u 12.5000 (7.5000) lr 1.9461e-03 eta 0:00:22
epoch [23/200] batch [35/80] time 0.320 (0.447) data 0.189 (0.316) loss_u loss_u 0.8589 (0.9398) acc_u 18.7500 (8.0357) lr 1.9461e-03 eta 0:00:20
epoch [23/200] batch [40/80] time 0.373 (0.447) data 0.241 (0.316) loss_u loss_u 0.9780 (0.9415) acc_u 3.1250 (7.8906) lr 1.9461e-03 eta 0:00:17
epoch [23/200] batch [45/80] time 0.410 (0.447) data 0.279 (0.315) loss_u loss_u 0.9619 (0.9425) acc_u 3.1250 (7.5694) lr 1.9461e-03 eta 0:00:15
epoch [23/200] batch [50/80] time 0.482 (0.448) data 0.351 (0.316) loss_u loss_u 0.9346 (0.9425) acc_u 9.3750 (7.5000) lr 1.9461e-03 eta 0:00:13
epoch [23/200] batch [55/80] time 0.652 (0.451) data 0.522 (0.320) loss_u loss_u 0.9312 (0.9415) acc_u 9.3750 (7.8409) lr 1.9461e-03 eta 0:00:11
epoch [23/200] batch [60/80] time 0.520 (0.455) data 0.388 (0.324) loss_u loss_u 0.9092 (0.9419) acc_u 12.5000 (7.8125) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [65/80] time 0.374 (0.454) data 0.243 (0.323) loss_u loss_u 0.9478 (0.9425) acc_u 3.1250 (7.7404) lr 1.9461e-03 eta 0:00:06
epoch [23/200] batch [70/80] time 0.612 (0.454) data 0.481 (0.323) loss_u loss_u 0.9214 (0.9417) acc_u 15.6250 (8.0357) lr 1.9461e-03 eta 0:00:04
epoch [23/200] batch [75/80] time 0.422 (0.454) data 0.290 (0.323) loss_u loss_u 0.8745 (0.9412) acc_u 18.7500 (8.0833) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [80/80] time 0.378 (0.457) data 0.246 (0.326) loss_u loss_u 0.9458 (0.9414) acc_u 3.1250 (8.1250) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1758
confident_label rate tensor(0.1818, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 570
clean true:565
clean false:5
clean_rate:0.9912280701754386
noisy true:813
noisy false:1753
after delete: len(clean_dataset) 570
after delete: len(noisy_dataset) 2566
epoch [24/200] batch [5/17] time 0.367 (0.475) data 0.236 (0.344) loss_x loss_x 1.2344 (1.5279) acc_x 56.2500 (58.7500) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [10/17] time 0.411 (0.475) data 0.279 (0.344) loss_x loss_x 0.9917 (1.3879) acc_x 65.6250 (63.7500) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [15/17] time 0.483 (0.474) data 0.352 (0.343) loss_x loss_x 1.1094 (1.4090) acc_x 71.8750 (63.5417) lr 1.9409e-03 eta 0:00:00
epoch [24/200] batch [5/80] time 0.452 (0.467) data 0.320 (0.336) loss_u loss_u 0.9956 (0.9630) acc_u 0.0000 (3.1250) lr 1.9409e-03 eta 0:00:35
epoch [24/200] batch [10/80] time 0.340 (0.461) data 0.208 (0.329) loss_u loss_u 0.9497 (0.9617) acc_u 6.2500 (4.0625) lr 1.9409e-03 eta 0:00:32
epoch [24/200] batch [15/80] time 0.514 (0.457) data 0.382 (0.326) loss_u loss_u 0.9312 (0.9575) acc_u 6.2500 (4.3750) lr 1.9409e-03 eta 0:00:29
epoch [24/200] batch [20/80] time 0.523 (0.461) data 0.391 (0.330) loss_u loss_u 0.9233 (0.9519) acc_u 9.3750 (5.3125) lr 1.9409e-03 eta 0:00:27
epoch [24/200] batch [25/80] time 0.418 (0.460) data 0.286 (0.328) loss_u loss_u 0.8750 (0.9481) acc_u 15.6250 (5.7500) lr 1.9409e-03 eta 0:00:25
epoch [24/200] batch [30/80] time 0.354 (0.452) data 0.222 (0.321) loss_u loss_u 0.9365 (0.9479) acc_u 6.2500 (5.9375) lr 1.9409e-03 eta 0:00:22
epoch [24/200] batch [35/80] time 0.475 (0.449) data 0.342 (0.318) loss_u loss_u 0.9272 (0.9463) acc_u 12.5000 (6.5179) lr 1.9409e-03 eta 0:00:20
epoch [24/200] batch [40/80] time 0.513 (0.454) data 0.381 (0.322) loss_u loss_u 0.9058 (0.9458) acc_u 9.3750 (6.5625) lr 1.9409e-03 eta 0:00:18
epoch [24/200] batch [45/80] time 0.393 (0.461) data 0.262 (0.329) loss_u loss_u 0.9414 (0.9442) acc_u 12.5000 (7.1528) lr 1.9409e-03 eta 0:00:16
epoch [24/200] batch [50/80] time 0.547 (0.464) data 0.417 (0.333) loss_u loss_u 0.8662 (0.9432) acc_u 15.6250 (7.4375) lr 1.9409e-03 eta 0:00:13
epoch [24/200] batch [55/80] time 0.464 (0.459) data 0.332 (0.328) loss_u loss_u 0.9927 (0.9425) acc_u 0.0000 (7.2727) lr 1.9409e-03 eta 0:00:11
epoch [24/200] batch [60/80] time 0.466 (0.458) data 0.335 (0.327) loss_u loss_u 0.9429 (0.9428) acc_u 6.2500 (7.0833) lr 1.9409e-03 eta 0:00:09
epoch [24/200] batch [65/80] time 0.445 (0.456) data 0.313 (0.325) loss_u loss_u 0.9287 (0.9407) acc_u 9.3750 (7.3558) lr 1.9409e-03 eta 0:00:06
epoch [24/200] batch [70/80] time 0.420 (0.455) data 0.288 (0.323) loss_u loss_u 0.8931 (0.9415) acc_u 15.6250 (7.3661) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [75/80] time 0.439 (0.454) data 0.308 (0.323) loss_u loss_u 0.9336 (0.9415) acc_u 9.3750 (7.3333) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [80/80] time 0.384 (0.457) data 0.252 (0.325) loss_u loss_u 0.9585 (0.9418) acc_u 9.3750 (7.3438) lr 1.9409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1741
confident_label rate tensor(0.1805, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 566
clean true:557
clean false:9
clean_rate:0.9840989399293286
noisy true:838
noisy false:1732
after delete: len(clean_dataset) 566
after delete: len(noisy_dataset) 2570
epoch [25/200] batch [5/17] time 0.547 (0.508) data 0.416 (0.378) loss_x loss_x 1.3291 (1.2389) acc_x 71.8750 (73.1250) lr 1.9354e-03 eta 0:00:06
epoch [25/200] batch [10/17] time 0.490 (0.474) data 0.359 (0.343) loss_x loss_x 1.9717 (1.4670) acc_x 53.1250 (65.0000) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [15/17] time 0.382 (0.474) data 0.250 (0.343) loss_x loss_x 2.0723 (1.5173) acc_x 59.3750 (63.9583) lr 1.9354e-03 eta 0:00:00
epoch [25/200] batch [5/80] time 0.467 (0.474) data 0.336 (0.343) loss_u loss_u 0.9229 (0.9359) acc_u 9.3750 (8.7500) lr 1.9354e-03 eta 0:00:35
epoch [25/200] batch [10/80] time 0.484 (0.467) data 0.353 (0.336) loss_u loss_u 0.9336 (0.9387) acc_u 9.3750 (8.1250) lr 1.9354e-03 eta 0:00:32
epoch [25/200] batch [15/80] time 0.626 (0.470) data 0.495 (0.339) loss_u loss_u 0.9097 (0.9370) acc_u 9.3750 (8.3333) lr 1.9354e-03 eta 0:00:30
epoch [25/200] batch [20/80] time 0.413 (0.466) data 0.283 (0.334) loss_u loss_u 0.9033 (0.9385) acc_u 12.5000 (7.8125) lr 1.9354e-03 eta 0:00:27
epoch [25/200] batch [25/80] time 0.576 (0.463) data 0.445 (0.332) loss_u loss_u 0.9932 (0.9432) acc_u 0.0000 (7.6250) lr 1.9354e-03 eta 0:00:25
epoch [25/200] batch [30/80] time 0.489 (0.462) data 0.357 (0.331) loss_u loss_u 0.9619 (0.9419) acc_u 9.3750 (7.7083) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [35/80] time 0.603 (0.459) data 0.472 (0.328) loss_u loss_u 0.9521 (0.9413) acc_u 6.2500 (7.6786) lr 1.9354e-03 eta 0:00:20
epoch [25/200] batch [40/80] time 0.412 (0.458) data 0.280 (0.327) loss_u loss_u 0.9697 (0.9398) acc_u 3.1250 (7.9688) lr 1.9354e-03 eta 0:00:18
epoch [25/200] batch [45/80] time 0.412 (0.455) data 0.281 (0.324) loss_u loss_u 0.9673 (0.9382) acc_u 6.2500 (8.1250) lr 1.9354e-03 eta 0:00:15
epoch [25/200] batch [50/80] time 0.355 (0.454) data 0.224 (0.323) loss_u loss_u 0.9287 (0.9400) acc_u 9.3750 (7.8750) lr 1.9354e-03 eta 0:00:13
epoch [25/200] batch [55/80] time 0.572 (0.456) data 0.440 (0.325) loss_u loss_u 0.8970 (0.9392) acc_u 15.6250 (8.0682) lr 1.9354e-03 eta 0:00:11
epoch [25/200] batch [60/80] time 0.450 (0.457) data 0.319 (0.326) loss_u loss_u 0.9067 (0.9375) acc_u 9.3750 (8.2292) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [65/80] time 0.354 (0.454) data 0.222 (0.323) loss_u loss_u 0.9268 (0.9376) acc_u 9.3750 (8.2212) lr 1.9354e-03 eta 0:00:06
epoch [25/200] batch [70/80] time 0.394 (0.452) data 0.262 (0.321) loss_u loss_u 0.9302 (0.9386) acc_u 3.1250 (7.9911) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [75/80] time 0.452 (0.452) data 0.320 (0.321) loss_u loss_u 0.9609 (0.9382) acc_u 0.0000 (8.0417) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [80/80] time 0.520 (0.453) data 0.390 (0.322) loss_u loss_u 0.9663 (0.9387) acc_u 12.5000 (8.1250) lr 1.9354e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1744
confident_label rate tensor(0.1763, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 553
clean true:544
clean false:9
clean_rate:0.9837251356238698
noisy true:848
noisy false:1735
after delete: len(clean_dataset) 553
after delete: len(noisy_dataset) 2583
epoch [26/200] batch [5/17] time 0.419 (0.436) data 0.288 (0.305) loss_x loss_x 1.7588 (1.4231) acc_x 50.0000 (66.2500) lr 1.9298e-03 eta 0:00:05
epoch [26/200] batch [10/17] time 0.464 (0.466) data 0.333 (0.335) loss_x loss_x 1.7607 (1.5435) acc_x 56.2500 (63.4375) lr 1.9298e-03 eta 0:00:03
epoch [26/200] batch [15/17] time 0.477 (0.482) data 0.347 (0.351) loss_x loss_x 1.5068 (1.4822) acc_x 59.3750 (63.5417) lr 1.9298e-03 eta 0:00:00
epoch [26/200] batch [5/80] time 0.565 (0.474) data 0.434 (0.343) loss_u loss_u 0.9214 (0.9513) acc_u 15.6250 (8.1250) lr 1.9298e-03 eta 0:00:35
epoch [26/200] batch [10/80] time 0.343 (0.462) data 0.212 (0.331) loss_u loss_u 0.9404 (0.9514) acc_u 12.5000 (7.8125) lr 1.9298e-03 eta 0:00:32
epoch [26/200] batch [15/80] time 0.379 (0.456) data 0.247 (0.326) loss_u loss_u 0.9956 (0.9467) acc_u 0.0000 (8.7500) lr 1.9298e-03 eta 0:00:29
epoch [26/200] batch [20/80] time 0.354 (0.456) data 0.222 (0.324) loss_u loss_u 0.9331 (0.9396) acc_u 9.3750 (9.3750) lr 1.9298e-03 eta 0:00:27
epoch [26/200] batch [25/80] time 0.464 (0.458) data 0.332 (0.327) loss_u loss_u 0.9414 (0.9363) acc_u 3.1250 (9.5000) lr 1.9298e-03 eta 0:00:25
epoch [26/200] batch [30/80] time 0.386 (0.456) data 0.255 (0.325) loss_u loss_u 0.9185 (0.9345) acc_u 9.3750 (9.2708) lr 1.9298e-03 eta 0:00:22
epoch [26/200] batch [35/80] time 0.398 (0.455) data 0.265 (0.323) loss_u loss_u 0.8789 (0.9334) acc_u 15.6250 (9.1964) lr 1.9298e-03 eta 0:00:20
epoch [26/200] batch [40/80] time 0.444 (0.457) data 0.313 (0.326) loss_u loss_u 0.8833 (0.9337) acc_u 15.6250 (9.1406) lr 1.9298e-03 eta 0:00:18
epoch [26/200] batch [45/80] time 0.356 (0.459) data 0.225 (0.328) loss_u loss_u 0.9463 (0.9355) acc_u 6.2500 (8.6806) lr 1.9298e-03 eta 0:00:16
epoch [26/200] batch [50/80] time 0.391 (0.460) data 0.260 (0.329) loss_u loss_u 0.9287 (0.9376) acc_u 12.5000 (8.4375) lr 1.9298e-03 eta 0:00:13
epoch [26/200] batch [55/80] time 0.375 (0.462) data 0.244 (0.330) loss_u loss_u 0.9419 (0.9373) acc_u 6.2500 (8.4091) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [60/80] time 0.436 (0.459) data 0.305 (0.328) loss_u loss_u 0.9761 (0.9367) acc_u 3.1250 (8.4896) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [65/80] time 0.389 (0.456) data 0.258 (0.325) loss_u loss_u 0.9131 (0.9351) acc_u 12.5000 (8.5577) lr 1.9298e-03 eta 0:00:06
epoch [26/200] batch [70/80] time 0.418 (0.457) data 0.286 (0.326) loss_u loss_u 0.9351 (0.9355) acc_u 9.3750 (8.5714) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [75/80] time 0.418 (0.455) data 0.287 (0.324) loss_u loss_u 0.9492 (0.9363) acc_u 6.2500 (8.4583) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [80/80] time 0.513 (0.456) data 0.381 (0.325) loss_u loss_u 0.9287 (0.9356) acc_u 6.2500 (8.5156) lr 1.9298e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1707
confident_label rate tensor(0.1869, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 586
clean true:579
clean false:7
clean_rate:0.9880546075085325
noisy true:850
noisy false:1700
after delete: len(clean_dataset) 586
after delete: len(noisy_dataset) 2550
epoch [27/200] batch [5/18] time 0.613 (0.438) data 0.482 (0.307) loss_x loss_x 1.4619 (1.5166) acc_x 50.0000 (63.1250) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [10/18] time 0.391 (0.461) data 0.261 (0.330) loss_x loss_x 1.0508 (1.3991) acc_x 68.7500 (64.6875) lr 1.9239e-03 eta 0:00:03
epoch [27/200] batch [15/18] time 0.358 (0.466) data 0.227 (0.335) loss_x loss_x 1.1992 (1.4268) acc_x 62.5000 (62.9167) lr 1.9239e-03 eta 0:00:01
epoch [27/200] batch [5/79] time 0.489 (0.471) data 0.357 (0.340) loss_u loss_u 0.9463 (0.9530) acc_u 3.1250 (4.3750) lr 1.9239e-03 eta 0:00:34
epoch [27/200] batch [10/79] time 0.509 (0.471) data 0.376 (0.340) loss_u loss_u 0.9175 (0.9523) acc_u 9.3750 (5.0000) lr 1.9239e-03 eta 0:00:32
epoch [27/200] batch [15/79] time 0.412 (0.465) data 0.281 (0.334) loss_u loss_u 0.9199 (0.9463) acc_u 9.3750 (6.2500) lr 1.9239e-03 eta 0:00:29
epoch [27/200] batch [20/79] time 0.546 (0.470) data 0.414 (0.338) loss_u loss_u 0.8818 (0.9424) acc_u 18.7500 (7.3438) lr 1.9239e-03 eta 0:00:27
epoch [27/200] batch [25/79] time 0.347 (0.464) data 0.217 (0.333) loss_u loss_u 0.9517 (0.9444) acc_u 6.2500 (7.3750) lr 1.9239e-03 eta 0:00:25
epoch [27/200] batch [30/79] time 0.470 (0.461) data 0.338 (0.329) loss_u loss_u 0.9639 (0.9479) acc_u 6.2500 (6.9792) lr 1.9239e-03 eta 0:00:22
epoch [27/200] batch [35/79] time 0.501 (0.467) data 0.369 (0.336) loss_u loss_u 0.9727 (0.9480) acc_u 3.1250 (7.2321) lr 1.9239e-03 eta 0:00:20
epoch [27/200] batch [40/79] time 0.489 (0.462) data 0.357 (0.331) loss_u loss_u 0.9058 (0.9472) acc_u 12.5000 (7.2656) lr 1.9239e-03 eta 0:00:18
epoch [27/200] batch [45/79] time 0.388 (0.456) data 0.256 (0.325) loss_u loss_u 0.8970 (0.9438) acc_u 15.6250 (7.8472) lr 1.9239e-03 eta 0:00:15
epoch [27/200] batch [50/79] time 0.487 (0.455) data 0.355 (0.324) loss_u loss_u 0.9409 (0.9436) acc_u 6.2500 (7.6875) lr 1.9239e-03 eta 0:00:13
epoch [27/200] batch [55/79] time 0.444 (0.454) data 0.312 (0.322) loss_u loss_u 0.9404 (0.9444) acc_u 6.2500 (7.4432) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [60/79] time 0.375 (0.450) data 0.243 (0.319) loss_u loss_u 0.9614 (0.9441) acc_u 3.1250 (7.4479) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [65/79] time 0.502 (0.452) data 0.370 (0.320) loss_u loss_u 0.8892 (0.9439) acc_u 12.5000 (7.4519) lr 1.9239e-03 eta 0:00:06
epoch [27/200] batch [70/79] time 0.439 (0.453) data 0.307 (0.321) loss_u loss_u 0.9121 (0.9418) acc_u 12.5000 (7.7232) lr 1.9239e-03 eta 0:00:04
epoch [27/200] batch [75/79] time 0.518 (0.454) data 0.387 (0.322) loss_u loss_u 0.9478 (0.9418) acc_u 9.3750 (7.7083) lr 1.9239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1741
confident_label rate tensor(0.1795, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 563
clean true:560
clean false:3
clean_rate:0.9946714031971581
noisy true:835
noisy false:1738
after delete: len(clean_dataset) 563
after delete: len(noisy_dataset) 2573
epoch [28/200] batch [5/17] time 0.535 (0.515) data 0.404 (0.384) loss_x loss_x 1.1504 (1.3926) acc_x 78.1250 (70.0000) lr 1.9178e-03 eta 0:00:06
epoch [28/200] batch [10/17] time 0.447 (0.493) data 0.316 (0.362) loss_x loss_x 1.4541 (1.3360) acc_x 62.5000 (68.1250) lr 1.9178e-03 eta 0:00:03
epoch [28/200] batch [15/17] time 0.472 (0.488) data 0.341 (0.357) loss_x loss_x 1.5928 (1.3530) acc_x 56.2500 (67.0833) lr 1.9178e-03 eta 0:00:00
epoch [28/200] batch [5/80] time 0.505 (0.482) data 0.372 (0.351) loss_u loss_u 0.9360 (0.9499) acc_u 6.2500 (7.5000) lr 1.9178e-03 eta 0:00:36
epoch [28/200] batch [10/80] time 0.426 (0.469) data 0.294 (0.337) loss_u loss_u 0.9312 (0.9389) acc_u 6.2500 (7.8125) lr 1.9178e-03 eta 0:00:32
epoch [28/200] batch [15/80] time 0.430 (0.463) data 0.299 (0.331) loss_u loss_u 0.9238 (0.9373) acc_u 15.6250 (9.3750) lr 1.9178e-03 eta 0:00:30
epoch [28/200] batch [20/80] time 0.411 (0.464) data 0.278 (0.333) loss_u loss_u 0.8999 (0.9323) acc_u 9.3750 (9.3750) lr 1.9178e-03 eta 0:00:27
epoch [28/200] batch [25/80] time 0.361 (0.464) data 0.229 (0.333) loss_u loss_u 0.9512 (0.9363) acc_u 6.2500 (8.7500) lr 1.9178e-03 eta 0:00:25
epoch [28/200] batch [30/80] time 0.417 (0.461) data 0.287 (0.329) loss_u loss_u 0.9160 (0.9344) acc_u 12.5000 (9.1667) lr 1.9178e-03 eta 0:00:23
epoch [28/200] batch [35/80] time 0.580 (0.461) data 0.450 (0.330) loss_u loss_u 0.9297 (0.9336) acc_u 9.3750 (9.2857) lr 1.9178e-03 eta 0:00:20
epoch [28/200] batch [40/80] time 0.438 (0.457) data 0.308 (0.326) loss_u loss_u 0.9536 (0.9358) acc_u 3.1250 (9.0625) lr 1.9178e-03 eta 0:00:18
epoch [28/200] batch [45/80] time 0.422 (0.454) data 0.291 (0.322) loss_u loss_u 0.9834 (0.9375) acc_u 3.1250 (8.8889) lr 1.9178e-03 eta 0:00:15
epoch [28/200] batch [50/80] time 0.512 (0.453) data 0.380 (0.321) loss_u loss_u 0.9175 (0.9360) acc_u 9.3750 (9.1250) lr 1.9178e-03 eta 0:00:13
epoch [28/200] batch [55/80] time 0.432 (0.452) data 0.301 (0.320) loss_u loss_u 0.9673 (0.9346) acc_u 3.1250 (9.1477) lr 1.9178e-03 eta 0:00:11
epoch [28/200] batch [60/80] time 0.399 (0.449) data 0.268 (0.317) loss_u loss_u 0.9370 (0.9359) acc_u 6.2500 (8.9062) lr 1.9178e-03 eta 0:00:08
epoch [28/200] batch [65/80] time 0.397 (0.447) data 0.266 (0.316) loss_u loss_u 0.9585 (0.9351) acc_u 3.1250 (8.8462) lr 1.9178e-03 eta 0:00:06
epoch [28/200] batch [70/80] time 0.440 (0.446) data 0.309 (0.315) loss_u loss_u 0.9858 (0.9358) acc_u 3.1250 (8.7946) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [75/80] time 0.434 (0.445) data 0.302 (0.314) loss_u loss_u 0.9746 (0.9374) acc_u 3.1250 (8.5417) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [80/80] time 0.524 (0.445) data 0.392 (0.313) loss_u loss_u 0.9780 (0.9372) acc_u 3.1250 (8.5938) lr 1.9178e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1734
confident_label rate tensor(0.1837, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 576
clean true:568
clean false:8
clean_rate:0.9861111111111112
noisy true:834
noisy false:1726
after delete: len(clean_dataset) 576
after delete: len(noisy_dataset) 2560
epoch [29/200] batch [5/18] time 0.465 (0.516) data 0.334 (0.385) loss_x loss_x 0.8833 (1.1894) acc_x 81.2500 (71.8750) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [10/18] time 0.408 (0.482) data 0.276 (0.351) loss_x loss_x 1.6201 (1.3260) acc_x 50.0000 (65.0000) lr 1.9114e-03 eta 0:00:03
epoch [29/200] batch [15/18] time 0.337 (0.475) data 0.207 (0.344) loss_x loss_x 1.6123 (1.3953) acc_x 50.0000 (64.1667) lr 1.9114e-03 eta 0:00:01
epoch [29/200] batch [5/80] time 0.554 (0.474) data 0.422 (0.342) loss_u loss_u 0.9351 (0.9352) acc_u 9.3750 (8.1250) lr 1.9114e-03 eta 0:00:35
epoch [29/200] batch [10/80] time 0.712 (0.478) data 0.580 (0.346) loss_u loss_u 0.9663 (0.9422) acc_u 6.2500 (7.5000) lr 1.9114e-03 eta 0:00:33
epoch [29/200] batch [15/80] time 0.458 (0.471) data 0.326 (0.340) loss_u loss_u 0.8818 (0.9308) acc_u 15.6250 (9.1667) lr 1.9114e-03 eta 0:00:30
epoch [29/200] batch [20/80] time 0.424 (0.468) data 0.292 (0.336) loss_u loss_u 0.9648 (0.9297) acc_u 3.1250 (9.5312) lr 1.9114e-03 eta 0:00:28
epoch [29/200] batch [25/80] time 0.329 (0.454) data 0.197 (0.322) loss_u loss_u 0.9653 (0.9277) acc_u 0.0000 (9.6250) lr 1.9114e-03 eta 0:00:24
epoch [29/200] batch [30/80] time 0.442 (0.455) data 0.311 (0.324) loss_u loss_u 0.9717 (0.9317) acc_u 3.1250 (9.0625) lr 1.9114e-03 eta 0:00:22
epoch [29/200] batch [35/80] time 0.573 (0.457) data 0.441 (0.326) loss_u loss_u 0.9131 (0.9333) acc_u 9.3750 (8.6607) lr 1.9114e-03 eta 0:00:20
epoch [29/200] batch [40/80] time 0.459 (0.455) data 0.327 (0.324) loss_u loss_u 0.9663 (0.9349) acc_u 6.2500 (8.3594) lr 1.9114e-03 eta 0:00:18
epoch [29/200] batch [45/80] time 0.583 (0.455) data 0.451 (0.324) loss_u loss_u 0.9595 (0.9368) acc_u 3.1250 (7.9167) lr 1.9114e-03 eta 0:00:15
epoch [29/200] batch [50/80] time 0.495 (0.454) data 0.363 (0.322) loss_u loss_u 0.8975 (0.9372) acc_u 15.6250 (8.0000) lr 1.9114e-03 eta 0:00:13
epoch [29/200] batch [55/80] time 0.441 (0.453) data 0.309 (0.321) loss_u loss_u 0.8960 (0.9373) acc_u 15.6250 (8.0114) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [60/80] time 0.340 (0.451) data 0.208 (0.319) loss_u loss_u 0.9092 (0.9379) acc_u 15.6250 (8.0208) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [65/80] time 0.514 (0.451) data 0.382 (0.319) loss_u loss_u 0.9272 (0.9388) acc_u 9.3750 (7.9808) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [70/80] time 0.504 (0.451) data 0.371 (0.320) loss_u loss_u 0.9697 (0.9402) acc_u 3.1250 (7.8571) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [75/80] time 0.513 (0.451) data 0.382 (0.319) loss_u loss_u 0.9727 (0.9404) acc_u 3.1250 (7.9167) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [80/80] time 0.407 (0.450) data 0.274 (0.318) loss_u loss_u 0.9414 (0.9401) acc_u 3.1250 (8.0469) lr 1.9114e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1713
confident_label rate tensor(0.1834, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 575
clean true:565
clean false:10
clean_rate:0.9826086956521739
noisy true:858
noisy false:1703
after delete: len(clean_dataset) 575
after delete: len(noisy_dataset) 2561
epoch [30/200] batch [5/17] time 0.982 (0.652) data 0.847 (0.520) loss_x loss_x 1.6016 (1.5221) acc_x 59.3750 (63.1250) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [10/17] time 0.516 (0.580) data 0.384 (0.447) loss_x loss_x 1.9717 (1.5447) acc_x 40.6250 (61.2500) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [15/17] time 0.750 (0.572) data 0.618 (0.440) loss_x loss_x 1.9316 (1.5689) acc_x 43.7500 (59.7917) lr 1.9048e-03 eta 0:00:01
epoch [30/200] batch [5/80] time 0.525 (0.532) data 0.393 (0.400) loss_u loss_u 0.9478 (0.9382) acc_u 6.2500 (8.7500) lr 1.9048e-03 eta 0:00:39
epoch [30/200] batch [10/80] time 0.397 (0.515) data 0.265 (0.383) loss_u loss_u 0.9048 (0.9440) acc_u 9.3750 (7.8125) lr 1.9048e-03 eta 0:00:36
epoch [30/200] batch [15/80] time 0.480 (0.506) data 0.348 (0.374) loss_u loss_u 0.9209 (0.9389) acc_u 12.5000 (8.5417) lr 1.9048e-03 eta 0:00:32
epoch [30/200] batch [20/80] time 0.445 (0.494) data 0.314 (0.362) loss_u loss_u 0.9097 (0.9296) acc_u 9.3750 (9.3750) lr 1.9048e-03 eta 0:00:29
epoch [30/200] batch [25/80] time 0.461 (0.484) data 0.330 (0.352) loss_u loss_u 0.9048 (0.9296) acc_u 15.6250 (9.3750) lr 1.9048e-03 eta 0:00:26
epoch [30/200] batch [30/80] time 0.404 (0.477) data 0.272 (0.345) loss_u loss_u 0.9077 (0.9316) acc_u 15.6250 (8.9583) lr 1.9048e-03 eta 0:00:23
epoch [30/200] batch [35/80] time 0.395 (0.471) data 0.264 (0.339) loss_u loss_u 0.9336 (0.9345) acc_u 6.2500 (8.4821) lr 1.9048e-03 eta 0:00:21
epoch [30/200] batch [40/80] time 0.414 (0.471) data 0.283 (0.339) loss_u loss_u 0.9443 (0.9310) acc_u 6.2500 (8.9844) lr 1.9048e-03 eta 0:00:18
epoch [30/200] batch [45/80] time 0.529 (0.467) data 0.399 (0.336) loss_u loss_u 0.9297 (0.9333) acc_u 9.3750 (8.5417) lr 1.9048e-03 eta 0:00:16
epoch [30/200] batch [50/80] time 0.369 (0.466) data 0.238 (0.334) loss_u loss_u 0.8979 (0.9328) acc_u 18.7500 (8.6875) lr 1.9048e-03 eta 0:00:13
epoch [30/200] batch [55/80] time 0.454 (0.467) data 0.324 (0.335) loss_u loss_u 0.9087 (0.9333) acc_u 12.5000 (8.6364) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [60/80] time 0.491 (0.465) data 0.361 (0.334) loss_u loss_u 0.9824 (0.9352) acc_u 0.0000 (8.3854) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [65/80] time 0.395 (0.460) data 0.263 (0.329) loss_u loss_u 0.9409 (0.9349) acc_u 9.3750 (8.4615) lr 1.9048e-03 eta 0:00:06
epoch [30/200] batch [70/80] time 0.509 (0.460) data 0.377 (0.329) loss_u loss_u 0.9043 (0.9347) acc_u 12.5000 (8.4375) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [75/80] time 0.660 (0.464) data 0.530 (0.333) loss_u loss_u 0.9556 (0.9370) acc_u 6.2500 (8.0833) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [80/80] time 0.430 (0.464) data 0.299 (0.333) loss_u loss_u 0.9458 (0.9373) acc_u 6.2500 (8.1641) lr 1.9048e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1743
confident_label rate tensor(0.1849, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 580
clean true:568
clean false:12
clean_rate:0.9793103448275862
noisy true:825
noisy false:1731
after delete: len(clean_dataset) 580
after delete: len(noisy_dataset) 2556
epoch [31/200] batch [5/18] time 0.525 (0.460) data 0.394 (0.329) loss_x loss_x 1.1670 (1.0653) acc_x 68.7500 (70.6250) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [10/18] time 0.387 (0.461) data 0.255 (0.330) loss_x loss_x 1.2734 (1.3021) acc_x 56.2500 (65.0000) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [15/18] time 0.463 (0.475) data 0.332 (0.344) loss_x loss_x 1.3828 (1.3439) acc_x 65.6250 (64.1667) lr 1.8980e-03 eta 0:00:01
epoch [31/200] batch [5/79] time 0.446 (0.473) data 0.315 (0.341) loss_u loss_u 0.9717 (0.9573) acc_u 3.1250 (6.2500) lr 1.8980e-03 eta 0:00:34
epoch [31/200] batch [10/79] time 0.373 (0.461) data 0.241 (0.330) loss_u loss_u 0.9434 (0.9419) acc_u 6.2500 (7.8125) lr 1.8980e-03 eta 0:00:31
epoch [31/200] batch [15/79] time 0.381 (0.457) data 0.250 (0.325) loss_u loss_u 0.9673 (0.9306) acc_u 6.2500 (8.9583) lr 1.8980e-03 eta 0:00:29
epoch [31/200] batch [20/79] time 0.553 (0.455) data 0.421 (0.323) loss_u loss_u 0.9863 (0.9354) acc_u 0.0000 (8.1250) lr 1.8980e-03 eta 0:00:26
epoch [31/200] batch [25/79] time 0.405 (0.453) data 0.273 (0.322) loss_u loss_u 0.9692 (0.9371) acc_u 6.2500 (8.0000) lr 1.8980e-03 eta 0:00:24
epoch [31/200] batch [30/79] time 0.584 (0.457) data 0.454 (0.325) loss_u loss_u 0.9048 (0.9361) acc_u 12.5000 (8.2292) lr 1.8980e-03 eta 0:00:22
epoch [31/200] batch [35/79] time 0.408 (0.455) data 0.276 (0.324) loss_u loss_u 0.9409 (0.9400) acc_u 6.2500 (7.4107) lr 1.8980e-03 eta 0:00:20
epoch [31/200] batch [40/79] time 0.732 (0.466) data 0.601 (0.334) loss_u loss_u 0.9580 (0.9393) acc_u 6.2500 (7.5781) lr 1.8980e-03 eta 0:00:18
epoch [31/200] batch [45/79] time 0.438 (0.466) data 0.306 (0.335) loss_u loss_u 0.9565 (0.9426) acc_u 9.3750 (7.2917) lr 1.8980e-03 eta 0:00:15
epoch [31/200] batch [50/79] time 0.348 (0.463) data 0.216 (0.331) loss_u loss_u 0.9277 (0.9421) acc_u 12.5000 (7.5000) lr 1.8980e-03 eta 0:00:13
epoch [31/200] batch [55/79] time 0.517 (0.463) data 0.384 (0.332) loss_u loss_u 0.9624 (0.9443) acc_u 3.1250 (7.2159) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [60/79] time 0.527 (0.465) data 0.397 (0.333) loss_u loss_u 0.9058 (0.9437) acc_u 12.5000 (7.3438) lr 1.8980e-03 eta 0:00:08
epoch [31/200] batch [65/79] time 0.413 (0.464) data 0.281 (0.332) loss_u loss_u 0.9263 (0.9429) acc_u 12.5000 (7.4519) lr 1.8980e-03 eta 0:00:06
epoch [31/200] batch [70/79] time 0.450 (0.466) data 0.318 (0.335) loss_u loss_u 0.9033 (0.9445) acc_u 9.3750 (7.2768) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [75/79] time 0.415 (0.465) data 0.284 (0.334) loss_u loss_u 0.9209 (0.9441) acc_u 12.5000 (7.3750) lr 1.8980e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1702
confident_label rate tensor(0.1897, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 595
clean true:591
clean false:4
clean_rate:0.9932773109243698
noisy true:843
noisy false:1698
after delete: len(clean_dataset) 595
after delete: len(noisy_dataset) 2541
epoch [32/200] batch [5/18] time 0.437 (0.411) data 0.306 (0.280) loss_x loss_x 1.7871 (1.3891) acc_x 59.3750 (63.7500) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [10/18] time 0.671 (0.474) data 0.540 (0.343) loss_x loss_x 1.4512 (1.4744) acc_x 62.5000 (61.5625) lr 1.8910e-03 eta 0:00:03
epoch [32/200] batch [15/18] time 0.441 (0.497) data 0.310 (0.365) loss_x loss_x 1.1855 (1.4079) acc_x 65.6250 (62.7083) lr 1.8910e-03 eta 0:00:01
epoch [32/200] batch [5/79] time 0.602 (0.486) data 0.470 (0.355) loss_u loss_u 0.9292 (0.9277) acc_u 6.2500 (8.1250) lr 1.8910e-03 eta 0:00:35
epoch [32/200] batch [10/79] time 0.387 (0.471) data 0.256 (0.340) loss_u loss_u 0.9199 (0.9323) acc_u 12.5000 (8.4375) lr 1.8910e-03 eta 0:00:32
epoch [32/200] batch [15/79] time 0.408 (0.472) data 0.275 (0.341) loss_u loss_u 0.9141 (0.9303) acc_u 9.3750 (8.7500) lr 1.8910e-03 eta 0:00:30
epoch [32/200] batch [20/79] time 0.408 (0.465) data 0.275 (0.334) loss_u loss_u 0.9136 (0.9319) acc_u 15.6250 (9.0625) lr 1.8910e-03 eta 0:00:27
epoch [32/200] batch [25/79] time 0.460 (0.472) data 0.328 (0.341) loss_u loss_u 0.9321 (0.9345) acc_u 9.3750 (8.6250) lr 1.8910e-03 eta 0:00:25
epoch [32/200] batch [30/79] time 0.380 (0.465) data 0.249 (0.333) loss_u loss_u 0.9229 (0.9351) acc_u 9.3750 (8.3333) lr 1.8910e-03 eta 0:00:22
epoch [32/200] batch [35/79] time 0.463 (0.462) data 0.331 (0.330) loss_u loss_u 0.9282 (0.9366) acc_u 12.5000 (8.3929) lr 1.8910e-03 eta 0:00:20
epoch [32/200] batch [40/79] time 0.423 (0.460) data 0.293 (0.329) loss_u loss_u 0.9702 (0.9387) acc_u 6.2500 (8.2031) lr 1.8910e-03 eta 0:00:17
epoch [32/200] batch [45/79] time 0.497 (0.459) data 0.365 (0.328) loss_u loss_u 0.9810 (0.9392) acc_u 3.1250 (7.8472) lr 1.8910e-03 eta 0:00:15
epoch [32/200] batch [50/79] time 0.550 (0.460) data 0.418 (0.328) loss_u loss_u 0.9678 (0.9401) acc_u 3.1250 (7.6875) lr 1.8910e-03 eta 0:00:13
epoch [32/200] batch [55/79] time 0.355 (0.457) data 0.225 (0.325) loss_u loss_u 0.9116 (0.9410) acc_u 6.2500 (7.3864) lr 1.8910e-03 eta 0:00:10
epoch [32/200] batch [60/79] time 0.623 (0.456) data 0.491 (0.324) loss_u loss_u 0.9214 (0.9417) acc_u 9.3750 (7.3438) lr 1.8910e-03 eta 0:00:08
epoch [32/200] batch [65/79] time 0.375 (0.456) data 0.245 (0.324) loss_u loss_u 0.9839 (0.9418) acc_u 0.0000 (7.4038) lr 1.8910e-03 eta 0:00:06
epoch [32/200] batch [70/79] time 0.534 (0.457) data 0.402 (0.326) loss_u loss_u 0.9819 (0.9418) acc_u 0.0000 (7.4107) lr 1.8910e-03 eta 0:00:04
epoch [32/200] batch [75/79] time 0.559 (0.460) data 0.427 (0.328) loss_u loss_u 0.9351 (0.9412) acc_u 12.5000 (7.5833) lr 1.8910e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1716
confident_label rate tensor(0.1849, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 580
clean true:574
clean false:6
clean_rate:0.9896551724137931
noisy true:846
noisy false:1710
after delete: len(clean_dataset) 580
after delete: len(noisy_dataset) 2556
epoch [33/200] batch [5/18] time 0.473 (0.550) data 0.342 (0.418) loss_x loss_x 1.7383 (1.6203) acc_x 53.1250 (57.5000) lr 1.8838e-03 eta 0:00:07
epoch [33/200] batch [10/18] time 0.494 (0.557) data 0.363 (0.426) loss_x loss_x 0.9331 (1.4383) acc_x 68.7500 (63.4375) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [15/18] time 0.453 (0.511) data 0.322 (0.381) loss_x loss_x 1.5381 (1.4922) acc_x 59.3750 (63.1250) lr 1.8838e-03 eta 0:00:01
epoch [33/200] batch [5/79] time 0.347 (0.494) data 0.216 (0.363) loss_u loss_u 0.9673 (0.9579) acc_u 3.1250 (5.0000) lr 1.8838e-03 eta 0:00:36
epoch [33/200] batch [10/79] time 0.420 (0.480) data 0.289 (0.349) loss_u loss_u 0.9585 (0.9516) acc_u 6.2500 (5.6250) lr 1.8838e-03 eta 0:00:33
epoch [33/200] batch [15/79] time 0.579 (0.478) data 0.448 (0.347) loss_u loss_u 0.8682 (0.9482) acc_u 18.7500 (6.0417) lr 1.8838e-03 eta 0:00:30
epoch [33/200] batch [20/79] time 0.524 (0.475) data 0.392 (0.344) loss_u loss_u 0.9780 (0.9495) acc_u 3.1250 (5.9375) lr 1.8838e-03 eta 0:00:28
epoch [33/200] batch [25/79] time 0.397 (0.477) data 0.265 (0.346) loss_u loss_u 0.8921 (0.9434) acc_u 9.3750 (6.8750) lr 1.8838e-03 eta 0:00:25
epoch [33/200] batch [30/79] time 0.347 (0.471) data 0.216 (0.339) loss_u loss_u 0.9736 (0.9447) acc_u 6.2500 (7.1875) lr 1.8838e-03 eta 0:00:23
epoch [33/200] batch [35/79] time 0.635 (0.470) data 0.503 (0.339) loss_u loss_u 0.9004 (0.9410) acc_u 12.5000 (7.6786) lr 1.8838e-03 eta 0:00:20
epoch [33/200] batch [40/79] time 0.323 (0.468) data 0.191 (0.337) loss_u loss_u 0.9111 (0.9397) acc_u 12.5000 (7.9688) lr 1.8838e-03 eta 0:00:18
epoch [33/200] batch [45/79] time 0.337 (0.465) data 0.206 (0.334) loss_u loss_u 0.9395 (0.9391) acc_u 9.3750 (7.9167) lr 1.8838e-03 eta 0:00:15
epoch [33/200] batch [50/79] time 0.416 (0.464) data 0.284 (0.332) loss_u loss_u 0.9565 (0.9392) acc_u 6.2500 (7.9375) lr 1.8838e-03 eta 0:00:13
epoch [33/200] batch [55/79] time 0.529 (0.464) data 0.398 (0.332) loss_u loss_u 0.8794 (0.9390) acc_u 15.6250 (7.9545) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [60/79] time 0.412 (0.461) data 0.280 (0.330) loss_u loss_u 0.9941 (0.9405) acc_u 0.0000 (7.8646) lr 1.8838e-03 eta 0:00:08
epoch [33/200] batch [65/79] time 0.432 (0.458) data 0.300 (0.327) loss_u loss_u 0.8950 (0.9410) acc_u 15.6250 (7.7885) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [70/79] time 0.358 (0.457) data 0.227 (0.325) loss_u loss_u 0.9497 (0.9408) acc_u 3.1250 (7.8571) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [75/79] time 0.520 (0.456) data 0.389 (0.324) loss_u loss_u 0.9033 (0.9397) acc_u 15.6250 (8.0417) lr 1.8838e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1717
confident_label rate tensor(0.1865, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 585
clean true:572
clean false:13
clean_rate:0.9777777777777777
noisy true:847
noisy false:1704
after delete: len(clean_dataset) 585
after delete: len(noisy_dataset) 2551
epoch [34/200] batch [5/18] time 0.581 (0.538) data 0.450 (0.406) loss_x loss_x 0.9180 (1.3697) acc_x 84.3750 (68.7500) lr 1.8763e-03 eta 0:00:06
epoch [34/200] batch [10/18] time 0.441 (0.482) data 0.310 (0.351) loss_x loss_x 1.1836 (1.4816) acc_x 62.5000 (64.3750) lr 1.8763e-03 eta 0:00:03
epoch [34/200] batch [15/18] time 0.379 (0.477) data 0.248 (0.346) loss_x loss_x 1.7783 (1.5568) acc_x 68.7500 (61.8750) lr 1.8763e-03 eta 0:00:01
epoch [34/200] batch [5/79] time 0.482 (0.480) data 0.350 (0.348) loss_u loss_u 0.9810 (0.9331) acc_u 3.1250 (9.3750) lr 1.8763e-03 eta 0:00:35
epoch [34/200] batch [10/79] time 0.399 (0.475) data 0.267 (0.343) loss_u loss_u 0.9468 (0.9413) acc_u 6.2500 (7.8125) lr 1.8763e-03 eta 0:00:32
epoch [34/200] batch [15/79] time 0.394 (0.477) data 0.264 (0.346) loss_u loss_u 0.9370 (0.9390) acc_u 6.2500 (7.5000) lr 1.8763e-03 eta 0:00:30
epoch [34/200] batch [20/79] time 0.416 (0.468) data 0.285 (0.337) loss_u loss_u 0.8525 (0.9299) acc_u 15.6250 (8.5938) lr 1.8763e-03 eta 0:00:27
epoch [34/200] batch [25/79] time 0.530 (0.462) data 0.400 (0.331) loss_u loss_u 0.8643 (0.9329) acc_u 21.8750 (8.3750) lr 1.8763e-03 eta 0:00:24
epoch [34/200] batch [30/79] time 0.397 (0.461) data 0.267 (0.329) loss_u loss_u 0.9272 (0.9353) acc_u 6.2500 (8.1250) lr 1.8763e-03 eta 0:00:22
epoch [34/200] batch [35/79] time 0.499 (0.456) data 0.367 (0.325) loss_u loss_u 0.9546 (0.9329) acc_u 6.2500 (8.7500) lr 1.8763e-03 eta 0:00:20
epoch [34/200] batch [40/79] time 0.617 (0.459) data 0.485 (0.327) loss_u loss_u 0.9419 (0.9337) acc_u 9.3750 (8.7500) lr 1.8763e-03 eta 0:00:17
epoch [34/200] batch [45/79] time 0.386 (0.458) data 0.254 (0.326) loss_u loss_u 0.9414 (0.9326) acc_u 6.2500 (8.8889) lr 1.8763e-03 eta 0:00:15
epoch [34/200] batch [50/79] time 0.432 (0.459) data 0.300 (0.328) loss_u loss_u 0.9502 (0.9331) acc_u 6.2500 (8.8125) lr 1.8763e-03 eta 0:00:13
epoch [34/200] batch [55/79] time 0.581 (0.460) data 0.449 (0.329) loss_u loss_u 0.9478 (0.9349) acc_u 6.2500 (8.6364) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [60/79] time 0.562 (0.460) data 0.430 (0.328) loss_u loss_u 0.9854 (0.9372) acc_u 6.2500 (8.3333) lr 1.8763e-03 eta 0:00:08
epoch [34/200] batch [65/79] time 0.645 (0.466) data 0.512 (0.334) loss_u loss_u 0.9707 (0.9381) acc_u 6.2500 (8.1250) lr 1.8763e-03 eta 0:00:06
epoch [34/200] batch [70/79] time 0.488 (0.468) data 0.355 (0.337) loss_u loss_u 0.9146 (0.9384) acc_u 9.3750 (8.0357) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [75/79] time 0.387 (0.467) data 0.255 (0.335) loss_u loss_u 0.9399 (0.9393) acc_u 6.2500 (7.8333) lr 1.8763e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1718
confident_label rate tensor(0.1818, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 570
clean true:563
clean false:7
clean_rate:0.987719298245614
noisy true:855
noisy false:1711
after delete: len(clean_dataset) 570
after delete: len(noisy_dataset) 2566
epoch [35/200] batch [5/17] time 0.461 (0.429) data 0.331 (0.298) loss_x loss_x 1.4561 (1.3154) acc_x 68.7500 (68.1250) lr 1.8686e-03 eta 0:00:05
epoch [35/200] batch [10/17] time 0.439 (0.464) data 0.307 (0.333) loss_x loss_x 1.3398 (1.2682) acc_x 62.5000 (68.7500) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [15/17] time 0.488 (0.468) data 0.357 (0.338) loss_x loss_x 2.0840 (1.4088) acc_x 46.8750 (64.5833) lr 1.8686e-03 eta 0:00:00
epoch [35/200] batch [5/80] time 0.488 (0.478) data 0.356 (0.347) loss_u loss_u 0.9492 (0.9374) acc_u 9.3750 (10.0000) lr 1.8686e-03 eta 0:00:35
epoch [35/200] batch [10/80] time 0.449 (0.468) data 0.317 (0.337) loss_u loss_u 0.9912 (0.9473) acc_u 3.1250 (8.1250) lr 1.8686e-03 eta 0:00:32
epoch [35/200] batch [15/80] time 0.359 (0.461) data 0.229 (0.330) loss_u loss_u 0.9575 (0.9472) acc_u 3.1250 (7.5000) lr 1.8686e-03 eta 0:00:29
epoch [35/200] batch [20/80] time 0.562 (0.469) data 0.431 (0.338) loss_u loss_u 0.9868 (0.9499) acc_u 3.1250 (7.3438) lr 1.8686e-03 eta 0:00:28
epoch [35/200] batch [25/80] time 0.556 (0.469) data 0.423 (0.337) loss_u loss_u 0.9365 (0.9498) acc_u 6.2500 (7.0000) lr 1.8686e-03 eta 0:00:25
epoch [35/200] batch [30/80] time 0.339 (0.468) data 0.207 (0.336) loss_u loss_u 0.9409 (0.9485) acc_u 12.5000 (7.5000) lr 1.8686e-03 eta 0:00:23
epoch [35/200] batch [35/80] time 0.422 (0.470) data 0.290 (0.339) loss_u loss_u 0.9287 (0.9495) acc_u 6.2500 (7.3214) lr 1.8686e-03 eta 0:00:21
epoch [35/200] batch [40/80] time 0.412 (0.471) data 0.280 (0.340) loss_u loss_u 0.9590 (0.9499) acc_u 3.1250 (7.0312) lr 1.8686e-03 eta 0:00:18
epoch [35/200] batch [45/80] time 0.341 (0.469) data 0.209 (0.337) loss_u loss_u 0.8813 (0.9445) acc_u 15.6250 (7.7778) lr 1.8686e-03 eta 0:00:16
epoch [35/200] batch [50/80] time 0.475 (0.472) data 0.342 (0.340) loss_u loss_u 0.9419 (0.9443) acc_u 6.2500 (7.5625) lr 1.8686e-03 eta 0:00:14
epoch [35/200] batch [55/80] time 0.397 (0.474) data 0.266 (0.343) loss_u loss_u 0.9404 (0.9452) acc_u 6.2500 (7.3864) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [60/80] time 0.511 (0.471) data 0.379 (0.339) loss_u loss_u 0.9023 (0.9422) acc_u 12.5000 (7.9167) lr 1.8686e-03 eta 0:00:09
epoch [35/200] batch [65/80] time 0.466 (0.469) data 0.333 (0.338) loss_u loss_u 0.9399 (0.9407) acc_u 9.3750 (8.0769) lr 1.8686e-03 eta 0:00:07
epoch [35/200] batch [70/80] time 0.487 (0.467) data 0.355 (0.336) loss_u loss_u 0.8984 (0.9401) acc_u 9.3750 (8.0357) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [75/80] time 0.372 (0.467) data 0.241 (0.335) loss_u loss_u 0.8252 (0.9394) acc_u 18.7500 (8.0417) lr 1.8686e-03 eta 0:00:02
epoch [35/200] batch [80/80] time 0.498 (0.465) data 0.366 (0.334) loss_u loss_u 0.8994 (0.9406) acc_u 12.5000 (7.8906) lr 1.8686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1687
confident_label rate tensor(0.1920, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 602
clean true:595
clean false:7
clean_rate:0.9883720930232558
noisy true:854
noisy false:1680
after delete: len(clean_dataset) 602
after delete: len(noisy_dataset) 2534
epoch [36/200] batch [5/18] time 0.503 (0.451) data 0.372 (0.320) loss_x loss_x 1.7266 (1.3783) acc_x 62.5000 (64.3750) lr 1.8607e-03 eta 0:00:05
epoch [36/200] batch [10/18] time 0.417 (0.422) data 0.286 (0.291) loss_x loss_x 1.3975 (1.4342) acc_x 68.7500 (64.3750) lr 1.8607e-03 eta 0:00:03
epoch [36/200] batch [15/18] time 0.376 (0.430) data 0.245 (0.299) loss_x loss_x 1.5234 (1.4192) acc_x 65.6250 (66.0417) lr 1.8607e-03 eta 0:00:01
epoch [36/200] batch [5/79] time 0.556 (0.458) data 0.424 (0.327) loss_u loss_u 0.9346 (0.9505) acc_u 6.2500 (6.2500) lr 1.8607e-03 eta 0:00:33
epoch [36/200] batch [10/79] time 0.376 (0.450) data 0.246 (0.319) loss_u loss_u 0.9341 (0.9514) acc_u 6.2500 (6.2500) lr 1.8607e-03 eta 0:00:31
epoch [36/200] batch [15/79] time 0.558 (0.449) data 0.426 (0.318) loss_u loss_u 0.9526 (0.9532) acc_u 9.3750 (6.4583) lr 1.8607e-03 eta 0:00:28
epoch [36/200] batch [20/79] time 0.394 (0.458) data 0.263 (0.326) loss_u loss_u 0.9038 (0.9467) acc_u 15.6250 (7.3438) lr 1.8607e-03 eta 0:00:26
epoch [36/200] batch [25/79] time 0.450 (0.450) data 0.319 (0.319) loss_u loss_u 0.9395 (0.9464) acc_u 9.3750 (7.6250) lr 1.8607e-03 eta 0:00:24
epoch [36/200] batch [30/79] time 0.540 (0.453) data 0.408 (0.321) loss_u loss_u 0.9219 (0.9448) acc_u 12.5000 (7.8125) lr 1.8607e-03 eta 0:00:22
epoch [36/200] batch [35/79] time 0.366 (0.455) data 0.234 (0.323) loss_u loss_u 0.9272 (0.9452) acc_u 6.2500 (7.3214) lr 1.8607e-03 eta 0:00:20
epoch [36/200] batch [40/79] time 0.381 (0.454) data 0.249 (0.322) loss_u loss_u 0.9863 (0.9449) acc_u 0.0000 (7.1875) lr 1.8607e-03 eta 0:00:17
epoch [36/200] batch [45/79] time 0.440 (0.454) data 0.308 (0.322) loss_u loss_u 0.9595 (0.9462) acc_u 3.1250 (6.8750) lr 1.8607e-03 eta 0:00:15
epoch [36/200] batch [50/79] time 0.399 (0.451) data 0.267 (0.320) loss_u loss_u 0.9116 (0.9465) acc_u 12.5000 (6.6875) lr 1.8607e-03 eta 0:00:13
epoch [36/200] batch [55/79] time 0.308 (0.450) data 0.176 (0.319) loss_u loss_u 0.9653 (0.9454) acc_u 3.1250 (6.8750) lr 1.8607e-03 eta 0:00:10
epoch [36/200] batch [60/79] time 0.417 (0.450) data 0.285 (0.318) loss_u loss_u 0.9707 (0.9457) acc_u 6.2500 (6.9792) lr 1.8607e-03 eta 0:00:08
epoch [36/200] batch [65/79] time 0.535 (0.451) data 0.404 (0.319) loss_u loss_u 0.9258 (0.9464) acc_u 6.2500 (6.6827) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [70/79] time 0.401 (0.452) data 0.268 (0.320) loss_u loss_u 0.9741 (0.9464) acc_u 3.1250 (6.7857) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [75/79] time 0.516 (0.452) data 0.384 (0.320) loss_u loss_u 0.9238 (0.9434) acc_u 9.3750 (7.1667) lr 1.8607e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1727
confident_label rate tensor(0.1907, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 598
clean true:587
clean false:11
clean_rate:0.9816053511705686
noisy true:822
noisy false:1716
after delete: len(clean_dataset) 598
after delete: len(noisy_dataset) 2538
epoch [37/200] batch [5/18] time 0.771 (0.522) data 0.639 (0.391) loss_x loss_x 1.3350 (1.2320) acc_x 71.8750 (71.2500) lr 1.8526e-03 eta 0:00:06
epoch [37/200] batch [10/18] time 0.404 (0.499) data 0.274 (0.368) loss_x loss_x 2.0293 (1.2905) acc_x 56.2500 (69.0625) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [15/18] time 0.646 (0.493) data 0.516 (0.362) loss_x loss_x 1.8955 (1.3281) acc_x 50.0000 (67.7083) lr 1.8526e-03 eta 0:00:01
epoch [37/200] batch [5/79] time 0.471 (0.476) data 0.339 (0.345) loss_u loss_u 0.9312 (0.9214) acc_u 6.2500 (12.5000) lr 1.8526e-03 eta 0:00:35
epoch [37/200] batch [10/79] time 0.361 (0.468) data 0.230 (0.337) loss_u loss_u 0.9360 (0.9366) acc_u 6.2500 (8.7500) lr 1.8526e-03 eta 0:00:32
epoch [37/200] batch [15/79] time 0.425 (0.463) data 0.293 (0.332) loss_u loss_u 0.9365 (0.9331) acc_u 9.3750 (8.7500) lr 1.8526e-03 eta 0:00:29
epoch [37/200] batch [20/79] time 0.549 (0.466) data 0.418 (0.335) loss_u loss_u 0.9355 (0.9323) acc_u 9.3750 (8.7500) lr 1.8526e-03 eta 0:00:27
epoch [37/200] batch [25/79] time 0.642 (0.467) data 0.511 (0.335) loss_u loss_u 0.9634 (0.9363) acc_u 3.1250 (8.1250) lr 1.8526e-03 eta 0:00:25
epoch [37/200] batch [30/79] time 0.435 (0.471) data 0.304 (0.340) loss_u loss_u 0.9155 (0.9319) acc_u 9.3750 (8.4375) lr 1.8526e-03 eta 0:00:23
epoch [37/200] batch [35/79] time 0.388 (0.470) data 0.258 (0.338) loss_u loss_u 0.9409 (0.9372) acc_u 9.3750 (7.6786) lr 1.8526e-03 eta 0:00:20
epoch [37/200] batch [40/79] time 0.539 (0.469) data 0.408 (0.338) loss_u loss_u 0.9570 (0.9398) acc_u 3.1250 (7.4219) lr 1.8526e-03 eta 0:00:18
epoch [37/200] batch [45/79] time 0.388 (0.465) data 0.257 (0.334) loss_u loss_u 0.9819 (0.9418) acc_u 0.0000 (7.0139) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [50/79] time 0.431 (0.464) data 0.300 (0.333) loss_u loss_u 0.9053 (0.9418) acc_u 12.5000 (7.0000) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [55/79] time 0.499 (0.464) data 0.367 (0.332) loss_u loss_u 0.9194 (0.9402) acc_u 15.6250 (7.2727) lr 1.8526e-03 eta 0:00:11
epoch [37/200] batch [60/79] time 0.553 (0.462) data 0.421 (0.330) loss_u loss_u 0.9512 (0.9425) acc_u 9.3750 (6.9792) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [65/79] time 0.425 (0.460) data 0.293 (0.329) loss_u loss_u 0.9189 (0.9420) acc_u 9.3750 (6.9712) lr 1.8526e-03 eta 0:00:06
epoch [37/200] batch [70/79] time 0.467 (0.459) data 0.336 (0.328) loss_u loss_u 0.9302 (0.9412) acc_u 6.2500 (7.0982) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [75/79] time 0.392 (0.457) data 0.262 (0.326) loss_u loss_u 0.9814 (0.9408) acc_u 0.0000 (7.0833) lr 1.8526e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1721
confident_label rate tensor(0.1894, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 594
clean true:583
clean false:11
clean_rate:0.9814814814814815
noisy true:832
noisy false:1710
after delete: len(clean_dataset) 594
after delete: len(noisy_dataset) 2542
epoch [38/200] batch [5/18] time 0.521 (0.502) data 0.391 (0.370) loss_x loss_x 1.6748 (1.4062) acc_x 56.2500 (63.7500) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [10/18] time 0.514 (0.502) data 0.383 (0.370) loss_x loss_x 0.9351 (1.2811) acc_x 75.0000 (67.8125) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [15/18] time 0.422 (0.469) data 0.291 (0.338) loss_x loss_x 1.3271 (1.3047) acc_x 65.6250 (67.7083) lr 1.8443e-03 eta 0:00:01
epoch [38/200] batch [5/79] time 0.550 (0.491) data 0.417 (0.360) loss_u loss_u 0.9077 (0.9223) acc_u 9.3750 (9.3750) lr 1.8443e-03 eta 0:00:36
epoch [38/200] batch [10/79] time 0.438 (0.484) data 0.306 (0.352) loss_u loss_u 0.9453 (0.9169) acc_u 3.1250 (10.9375) lr 1.8443e-03 eta 0:00:33
epoch [38/200] batch [15/79] time 0.416 (0.487) data 0.284 (0.356) loss_u loss_u 0.9111 (0.9215) acc_u 9.3750 (9.7917) lr 1.8443e-03 eta 0:00:31
epoch [38/200] batch [20/79] time 0.363 (0.477) data 0.231 (0.345) loss_u loss_u 0.9805 (0.9331) acc_u 3.1250 (8.5938) lr 1.8443e-03 eta 0:00:28
epoch [38/200] batch [25/79] time 0.420 (0.472) data 0.288 (0.340) loss_u loss_u 0.9565 (0.9382) acc_u 3.1250 (7.8750) lr 1.8443e-03 eta 0:00:25
epoch [38/200] batch [30/79] time 0.661 (0.475) data 0.529 (0.343) loss_u loss_u 0.8979 (0.9364) acc_u 15.6250 (8.0208) lr 1.8443e-03 eta 0:00:23
epoch [38/200] batch [35/79] time 0.395 (0.467) data 0.263 (0.336) loss_u loss_u 0.9131 (0.9363) acc_u 9.3750 (8.1250) lr 1.8443e-03 eta 0:00:20
epoch [38/200] batch [40/79] time 0.385 (0.463) data 0.253 (0.331) loss_u loss_u 0.8813 (0.9364) acc_u 15.6250 (8.2812) lr 1.8443e-03 eta 0:00:18
epoch [38/200] batch [45/79] time 0.433 (0.459) data 0.302 (0.328) loss_u loss_u 0.9224 (0.9369) acc_u 9.3750 (8.1250) lr 1.8443e-03 eta 0:00:15
epoch [38/200] batch [50/79] time 0.438 (0.459) data 0.306 (0.327) loss_u loss_u 0.9785 (0.9378) acc_u 6.2500 (8.0625) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [55/79] time 0.371 (0.455) data 0.240 (0.324) loss_u loss_u 0.9844 (0.9383) acc_u 0.0000 (8.0114) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [60/79] time 0.717 (0.456) data 0.586 (0.325) loss_u loss_u 0.9062 (0.9387) acc_u 15.6250 (8.0729) lr 1.8443e-03 eta 0:00:08
epoch [38/200] batch [65/79] time 0.518 (0.453) data 0.385 (0.321) loss_u loss_u 0.9360 (0.9406) acc_u 9.3750 (7.8365) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [70/79] time 0.612 (0.460) data 0.481 (0.329) loss_u loss_u 0.9644 (0.9416) acc_u 3.1250 (7.6786) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [75/79] time 0.362 (0.459) data 0.231 (0.327) loss_u loss_u 0.9575 (0.9411) acc_u 6.2500 (7.7917) lr 1.8443e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1735
confident_label rate tensor(0.1910, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 599
clean true:587
clean false:12
clean_rate:0.9799666110183639
noisy true:814
noisy false:1723
after delete: len(clean_dataset) 599
after delete: len(noisy_dataset) 2537
epoch [39/200] batch [5/18] time 0.399 (0.456) data 0.268 (0.325) loss_x loss_x 1.2324 (1.4466) acc_x 71.8750 (65.0000) lr 1.8358e-03 eta 0:00:05
epoch [39/200] batch [10/18] time 0.404 (0.454) data 0.274 (0.324) loss_x loss_x 1.7529 (1.4574) acc_x 53.1250 (64.3750) lr 1.8358e-03 eta 0:00:03
epoch [39/200] batch [15/18] time 0.405 (0.456) data 0.274 (0.325) loss_x loss_x 1.1543 (1.3747) acc_x 68.7500 (66.6667) lr 1.8358e-03 eta 0:00:01
epoch [39/200] batch [5/79] time 0.503 (0.476) data 0.372 (0.346) loss_u loss_u 0.9634 (0.9424) acc_u 3.1250 (6.8750) lr 1.8358e-03 eta 0:00:35
epoch [39/200] batch [10/79] time 0.412 (0.473) data 0.281 (0.342) loss_u loss_u 0.9697 (0.9357) acc_u 3.1250 (7.8125) lr 1.8358e-03 eta 0:00:32
epoch [39/200] batch [15/79] time 0.491 (0.465) data 0.360 (0.334) loss_u loss_u 0.9668 (0.9362) acc_u 3.1250 (7.5000) lr 1.8358e-03 eta 0:00:29
epoch [39/200] batch [20/79] time 0.448 (0.465) data 0.317 (0.334) loss_u loss_u 0.9395 (0.9371) acc_u 6.2500 (7.5000) lr 1.8358e-03 eta 0:00:27
epoch [39/200] batch [25/79] time 0.616 (0.472) data 0.484 (0.341) loss_u loss_u 0.9272 (0.9392) acc_u 9.3750 (7.3750) lr 1.8358e-03 eta 0:00:25
epoch [39/200] batch [30/79] time 0.541 (0.473) data 0.408 (0.342) loss_u loss_u 0.9297 (0.9345) acc_u 6.2500 (7.9167) lr 1.8358e-03 eta 0:00:23
epoch [39/200] batch [35/79] time 0.437 (0.467) data 0.305 (0.336) loss_u loss_u 0.9692 (0.9373) acc_u 6.2500 (7.6786) lr 1.8358e-03 eta 0:00:20
epoch [39/200] batch [40/79] time 0.491 (0.466) data 0.359 (0.335) loss_u loss_u 0.9805 (0.9396) acc_u 0.0000 (7.4219) lr 1.8358e-03 eta 0:00:18
epoch [39/200] batch [45/79] time 0.426 (0.465) data 0.294 (0.334) loss_u loss_u 0.9189 (0.9381) acc_u 15.6250 (7.7778) lr 1.8358e-03 eta 0:00:15
epoch [39/200] batch [50/79] time 0.467 (0.463) data 0.335 (0.331) loss_u loss_u 0.9370 (0.9386) acc_u 9.3750 (7.8125) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [55/79] time 0.488 (0.463) data 0.356 (0.332) loss_u loss_u 0.9131 (0.9396) acc_u 12.5000 (7.6705) lr 1.8358e-03 eta 0:00:11
epoch [39/200] batch [60/79] time 0.394 (0.462) data 0.263 (0.331) loss_u loss_u 0.8516 (0.9388) acc_u 18.7500 (7.7604) lr 1.8358e-03 eta 0:00:08
epoch [39/200] batch [65/79] time 0.631 (0.463) data 0.500 (0.332) loss_u loss_u 0.9858 (0.9384) acc_u 3.1250 (7.6923) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [70/79] time 0.490 (0.459) data 0.359 (0.328) loss_u loss_u 0.9209 (0.9379) acc_u 9.3750 (7.7679) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [75/79] time 0.431 (0.457) data 0.299 (0.325) loss_u loss_u 0.9883 (0.9384) acc_u 0.0000 (7.5833) lr 1.8358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1706
confident_label rate tensor(0.1964, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 616
clean true:606
clean false:10
clean_rate:0.9837662337662337
noisy true:824
noisy false:1696
after delete: len(clean_dataset) 616
after delete: len(noisy_dataset) 2520
epoch [40/200] batch [5/19] time 0.533 (0.479) data 0.402 (0.348) loss_x loss_x 1.8730 (1.4992) acc_x 56.2500 (67.5000) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [10/19] time 0.521 (0.472) data 0.389 (0.341) loss_x loss_x 1.1191 (1.5119) acc_x 71.8750 (63.4375) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [15/19] time 0.587 (0.484) data 0.455 (0.353) loss_x loss_x 1.2080 (1.4235) acc_x 68.7500 (65.6250) lr 1.8271e-03 eta 0:00:01
epoch [40/200] batch [5/78] time 0.556 (0.484) data 0.424 (0.353) loss_u loss_u 0.9844 (0.9576) acc_u 0.0000 (5.6250) lr 1.8271e-03 eta 0:00:35
epoch [40/200] batch [10/78] time 0.424 (0.472) data 0.292 (0.341) loss_u loss_u 0.9634 (0.9571) acc_u 3.1250 (4.6875) lr 1.8271e-03 eta 0:00:32
epoch [40/200] batch [15/78] time 0.442 (0.465) data 0.310 (0.334) loss_u loss_u 0.9194 (0.9488) acc_u 12.5000 (5.6250) lr 1.8271e-03 eta 0:00:29
epoch [40/200] batch [20/78] time 0.472 (0.470) data 0.340 (0.338) loss_u loss_u 0.9438 (0.9382) acc_u 12.5000 (7.6562) lr 1.8271e-03 eta 0:00:27
epoch [40/200] batch [25/78] time 0.434 (0.465) data 0.302 (0.333) loss_u loss_u 0.9438 (0.9425) acc_u 6.2500 (6.7500) lr 1.8271e-03 eta 0:00:24
epoch [40/200] batch [30/78] time 0.561 (0.467) data 0.429 (0.335) loss_u loss_u 0.9326 (0.9418) acc_u 6.2500 (6.9792) lr 1.8271e-03 eta 0:00:22
epoch [40/200] batch [35/78] time 0.527 (0.465) data 0.395 (0.333) loss_u loss_u 0.9634 (0.9426) acc_u 9.3750 (7.0536) lr 1.8271e-03 eta 0:00:19
epoch [40/200] batch [40/78] time 0.471 (0.464) data 0.340 (0.332) loss_u loss_u 0.9712 (0.9437) acc_u 3.1250 (6.9531) lr 1.8271e-03 eta 0:00:17
epoch [40/200] batch [45/78] time 0.363 (0.462) data 0.231 (0.331) loss_u loss_u 0.9517 (0.9444) acc_u 6.2500 (6.9444) lr 1.8271e-03 eta 0:00:15
epoch [40/200] batch [50/78] time 0.361 (0.463) data 0.229 (0.331) loss_u loss_u 0.8975 (0.9444) acc_u 15.6250 (7.1875) lr 1.8271e-03 eta 0:00:12
epoch [40/200] batch [55/78] time 0.428 (0.460) data 0.296 (0.328) loss_u loss_u 0.9023 (0.9429) acc_u 12.5000 (7.5568) lr 1.8271e-03 eta 0:00:10
epoch [40/200] batch [60/78] time 0.500 (0.461) data 0.370 (0.329) loss_u loss_u 0.9370 (0.9439) acc_u 6.2500 (7.2917) lr 1.8271e-03 eta 0:00:08
epoch [40/200] batch [65/78] time 0.468 (0.463) data 0.336 (0.331) loss_u loss_u 0.9126 (0.9418) acc_u 15.6250 (7.5481) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [70/78] time 0.499 (0.465) data 0.367 (0.333) loss_u loss_u 0.9595 (0.9433) acc_u 3.1250 (7.3214) lr 1.8271e-03 eta 0:00:03
epoch [40/200] batch [75/78] time 0.358 (0.464) data 0.226 (0.333) loss_u loss_u 0.9028 (0.9431) acc_u 12.5000 (7.2917) lr 1.8271e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1694
confident_label rate tensor(0.1929, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 605
clean true:596
clean false:9
clean_rate:0.9851239669421488
noisy true:846
noisy false:1685
after delete: len(clean_dataset) 605
after delete: len(noisy_dataset) 2531
epoch [41/200] batch [5/18] time 0.376 (0.481) data 0.246 (0.351) loss_x loss_x 1.9609 (1.2901) acc_x 56.2500 (71.8750) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [10/18] time 0.680 (0.493) data 0.549 (0.362) loss_x loss_x 0.9839 (1.2341) acc_x 68.7500 (71.8750) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [15/18] time 0.537 (0.484) data 0.406 (0.353) loss_x loss_x 1.5107 (1.3278) acc_x 62.5000 (68.7500) lr 1.8181e-03 eta 0:00:01
epoch [41/200] batch [5/79] time 0.373 (0.468) data 0.242 (0.337) loss_u loss_u 0.9355 (0.9195) acc_u 6.2500 (11.8750) lr 1.8181e-03 eta 0:00:34
epoch [41/200] batch [10/79] time 0.417 (0.466) data 0.284 (0.335) loss_u loss_u 0.9023 (0.9216) acc_u 12.5000 (10.6250) lr 1.8181e-03 eta 0:00:32
epoch [41/200] batch [15/79] time 0.454 (0.465) data 0.324 (0.334) loss_u loss_u 0.9873 (0.9245) acc_u 3.1250 (10.2083) lr 1.8181e-03 eta 0:00:29
epoch [41/200] batch [20/79] time 0.448 (0.464) data 0.317 (0.332) loss_u loss_u 0.9653 (0.9303) acc_u 3.1250 (9.3750) lr 1.8181e-03 eta 0:00:27
epoch [41/200] batch [25/79] time 0.480 (0.467) data 0.347 (0.336) loss_u loss_u 0.9365 (0.9334) acc_u 6.2500 (9.0000) lr 1.8181e-03 eta 0:00:25
epoch [41/200] batch [30/79] time 0.330 (0.459) data 0.199 (0.328) loss_u loss_u 0.9688 (0.9352) acc_u 3.1250 (8.4375) lr 1.8181e-03 eta 0:00:22
epoch [41/200] batch [35/79] time 0.344 (0.458) data 0.213 (0.327) loss_u loss_u 0.9663 (0.9374) acc_u 3.1250 (8.2143) lr 1.8181e-03 eta 0:00:20
epoch [41/200] batch [40/79] time 0.527 (0.461) data 0.396 (0.330) loss_u loss_u 0.9209 (0.9360) acc_u 9.3750 (8.5156) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [45/79] time 0.424 (0.461) data 0.292 (0.329) loss_u loss_u 0.9434 (0.9369) acc_u 6.2500 (8.1944) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [50/79] time 0.438 (0.457) data 0.307 (0.326) loss_u loss_u 0.9644 (0.9374) acc_u 6.2500 (8.2500) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [55/79] time 0.473 (0.454) data 0.341 (0.322) loss_u loss_u 0.9136 (0.9387) acc_u 6.2500 (7.9545) lr 1.8181e-03 eta 0:00:10
epoch [41/200] batch [60/79] time 0.440 (0.454) data 0.307 (0.322) loss_u loss_u 0.9165 (0.9374) acc_u 15.6250 (8.3333) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [65/79] time 0.416 (0.452) data 0.283 (0.321) loss_u loss_u 0.9492 (0.9381) acc_u 6.2500 (8.2212) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [70/79] time 0.552 (0.453) data 0.421 (0.321) loss_u loss_u 0.9473 (0.9405) acc_u 9.3750 (7.9018) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [75/79] time 0.423 (0.452) data 0.290 (0.321) loss_u loss_u 0.9058 (0.9398) acc_u 9.3750 (7.8750) lr 1.8181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1693
confident_label rate tensor(0.1987, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 623
clean true:615
clean false:8
clean_rate:0.9871589085072231
noisy true:828
noisy false:1685
after delete: len(clean_dataset) 623
after delete: len(noisy_dataset) 2513
epoch [42/200] batch [5/19] time 0.351 (0.446) data 0.220 (0.315) loss_x loss_x 1.0391 (1.3853) acc_x 78.1250 (67.5000) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [10/19] time 0.561 (0.464) data 0.430 (0.333) loss_x loss_x 1.4268 (1.4154) acc_x 62.5000 (65.9375) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [15/19] time 0.482 (0.485) data 0.351 (0.354) loss_x loss_x 1.4336 (1.4115) acc_x 71.8750 (65.2083) lr 1.8090e-03 eta 0:00:01
epoch [42/200] batch [5/78] time 0.417 (0.506) data 0.285 (0.375) loss_u loss_u 0.8667 (0.9445) acc_u 15.6250 (6.8750) lr 1.8090e-03 eta 0:00:36
epoch [42/200] batch [10/78] time 0.538 (0.496) data 0.406 (0.365) loss_u loss_u 0.9502 (0.9413) acc_u 9.3750 (8.1250) lr 1.8090e-03 eta 0:00:33
epoch [42/200] batch [15/78] time 0.654 (0.492) data 0.521 (0.360) loss_u loss_u 0.8892 (0.9430) acc_u 12.5000 (7.2917) lr 1.8090e-03 eta 0:00:30
epoch [42/200] batch [20/78] time 0.328 (0.486) data 0.196 (0.355) loss_u loss_u 0.9658 (0.9426) acc_u 6.2500 (7.3438) lr 1.8090e-03 eta 0:00:28
epoch [42/200] batch [25/78] time 0.408 (0.480) data 0.276 (0.349) loss_u loss_u 0.9419 (0.9416) acc_u 9.3750 (7.6250) lr 1.8090e-03 eta 0:00:25
epoch [42/200] batch [30/78] time 0.501 (0.478) data 0.369 (0.347) loss_u loss_u 0.9575 (0.9383) acc_u 9.3750 (8.3333) lr 1.8090e-03 eta 0:00:22
epoch [42/200] batch [35/78] time 0.371 (0.477) data 0.240 (0.346) loss_u loss_u 0.9121 (0.9394) acc_u 15.6250 (8.3929) lr 1.8090e-03 eta 0:00:20
epoch [42/200] batch [40/78] time 0.445 (0.476) data 0.313 (0.345) loss_u loss_u 0.9165 (0.9374) acc_u 12.5000 (8.5938) lr 1.8090e-03 eta 0:00:18
epoch [42/200] batch [45/78] time 0.506 (0.480) data 0.374 (0.348) loss_u loss_u 0.9858 (0.9380) acc_u 3.1250 (8.3333) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [50/78] time 0.368 (0.476) data 0.236 (0.344) loss_u loss_u 0.9805 (0.9378) acc_u 0.0000 (8.1875) lr 1.8090e-03 eta 0:00:13
epoch [42/200] batch [55/78] time 0.467 (0.474) data 0.335 (0.343) loss_u loss_u 0.9678 (0.9389) acc_u 3.1250 (7.8977) lr 1.8090e-03 eta 0:00:10
epoch [42/200] batch [60/78] time 0.426 (0.477) data 0.295 (0.345) loss_u loss_u 0.9360 (0.9404) acc_u 6.2500 (7.6562) lr 1.8090e-03 eta 0:00:08
epoch [42/200] batch [65/78] time 0.363 (0.473) data 0.231 (0.341) loss_u loss_u 0.9346 (0.9413) acc_u 9.3750 (7.5962) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [70/78] time 0.431 (0.472) data 0.299 (0.340) loss_u loss_u 0.9175 (0.9399) acc_u 9.3750 (7.6786) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [75/78] time 0.361 (0.471) data 0.229 (0.339) loss_u loss_u 0.9805 (0.9409) acc_u 3.1250 (7.5833) lr 1.8090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1689
confident_label rate tensor(0.1869, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 586
clean true:579
clean false:7
clean_rate:0.9880546075085325
noisy true:868
noisy false:1682
after delete: len(clean_dataset) 586
after delete: len(noisy_dataset) 2550
epoch [43/200] batch [5/18] time 0.435 (0.458) data 0.304 (0.326) loss_x loss_x 1.3779 (1.5480) acc_x 59.3750 (60.0000) lr 1.7997e-03 eta 0:00:05
epoch [43/200] batch [10/18] time 0.617 (0.481) data 0.485 (0.349) loss_x loss_x 1.0518 (1.3646) acc_x 75.0000 (62.5000) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [15/18] time 0.399 (0.499) data 0.266 (0.367) loss_x loss_x 1.1885 (1.2925) acc_x 71.8750 (64.1667) lr 1.7997e-03 eta 0:00:01
epoch [43/200] batch [5/79] time 0.561 (0.491) data 0.429 (0.359) loss_u loss_u 0.9229 (0.9295) acc_u 9.3750 (8.1250) lr 1.7997e-03 eta 0:00:36
epoch [43/200] batch [10/79] time 0.333 (0.499) data 0.202 (0.366) loss_u loss_u 0.8755 (0.9278) acc_u 15.6250 (9.3750) lr 1.7997e-03 eta 0:00:34
epoch [43/200] batch [15/79] time 0.467 (0.491) data 0.336 (0.359) loss_u loss_u 0.9927 (0.9386) acc_u 0.0000 (8.1250) lr 1.7997e-03 eta 0:00:31
epoch [43/200] batch [20/79] time 0.441 (0.482) data 0.309 (0.350) loss_u loss_u 0.9175 (0.9417) acc_u 12.5000 (7.8125) lr 1.7997e-03 eta 0:00:28
epoch [43/200] batch [25/79] time 0.330 (0.472) data 0.199 (0.340) loss_u loss_u 0.9028 (0.9412) acc_u 9.3750 (7.8750) lr 1.7997e-03 eta 0:00:25
epoch [43/200] batch [30/79] time 0.465 (0.471) data 0.334 (0.339) loss_u loss_u 0.9116 (0.9409) acc_u 9.3750 (7.7083) lr 1.7997e-03 eta 0:00:23
epoch [43/200] batch [35/79] time 0.504 (0.474) data 0.372 (0.343) loss_u loss_u 0.9561 (0.9403) acc_u 6.2500 (7.6786) lr 1.7997e-03 eta 0:00:20
epoch [43/200] batch [40/79] time 0.341 (0.472) data 0.209 (0.340) loss_u loss_u 0.9272 (0.9422) acc_u 12.5000 (7.5781) lr 1.7997e-03 eta 0:00:18
epoch [43/200] batch [45/79] time 0.490 (0.469) data 0.358 (0.337) loss_u loss_u 0.9136 (0.9421) acc_u 9.3750 (7.6389) lr 1.7997e-03 eta 0:00:15
epoch [43/200] batch [50/79] time 0.475 (0.467) data 0.344 (0.335) loss_u loss_u 0.9272 (0.9403) acc_u 9.3750 (7.8750) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [55/79] time 0.375 (0.465) data 0.244 (0.333) loss_u loss_u 0.9717 (0.9410) acc_u 3.1250 (7.7841) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [60/79] time 0.406 (0.461) data 0.275 (0.329) loss_u loss_u 0.9419 (0.9423) acc_u 6.2500 (7.5521) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [65/79] time 0.525 (0.459) data 0.393 (0.327) loss_u loss_u 0.8931 (0.9406) acc_u 12.5000 (7.7885) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [70/79] time 0.564 (0.460) data 0.433 (0.328) loss_u loss_u 0.9160 (0.9387) acc_u 12.5000 (7.9911) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [75/79] time 0.561 (0.461) data 0.428 (0.329) loss_u loss_u 0.9272 (0.9366) acc_u 12.5000 (8.2500) lr 1.7997e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1685
confident_label rate tensor(0.1856, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 582
clean true:575
clean false:7
clean_rate:0.9879725085910653
noisy true:876
noisy false:1678
after delete: len(clean_dataset) 582
after delete: len(noisy_dataset) 2554
epoch [44/200] batch [5/18] time 0.577 (0.561) data 0.446 (0.430) loss_x loss_x 1.6221 (1.4514) acc_x 62.5000 (62.5000) lr 1.7902e-03 eta 0:00:07
epoch [44/200] batch [10/18] time 0.467 (0.517) data 0.335 (0.386) loss_x loss_x 1.3018 (1.4049) acc_x 75.0000 (64.3750) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [15/18] time 0.411 (0.520) data 0.280 (0.388) loss_x loss_x 1.2354 (1.4138) acc_x 65.6250 (63.9583) lr 1.7902e-03 eta 0:00:01
epoch [44/200] batch [5/79] time 0.482 (0.498) data 0.352 (0.367) loss_u loss_u 0.9863 (0.9481) acc_u 3.1250 (7.5000) lr 1.7902e-03 eta 0:00:36
epoch [44/200] batch [10/79] time 0.418 (0.488) data 0.286 (0.357) loss_u loss_u 0.8770 (0.9317) acc_u 15.6250 (10.3125) lr 1.7902e-03 eta 0:00:33
epoch [44/200] batch [15/79] time 0.450 (0.474) data 0.318 (0.343) loss_u loss_u 0.9634 (0.9362) acc_u 6.2500 (8.7500) lr 1.7902e-03 eta 0:00:30
epoch [44/200] batch [20/79] time 0.550 (0.471) data 0.417 (0.340) loss_u loss_u 0.9756 (0.9419) acc_u 3.1250 (7.8125) lr 1.7902e-03 eta 0:00:27
epoch [44/200] batch [25/79] time 0.706 (0.486) data 0.572 (0.355) loss_u loss_u 0.9668 (0.9395) acc_u 3.1250 (8.0000) lr 1.7902e-03 eta 0:00:26
epoch [44/200] batch [30/79] time 0.431 (0.494) data 0.299 (0.362) loss_u loss_u 0.8799 (0.9343) acc_u 12.5000 (8.9583) lr 1.7902e-03 eta 0:00:24
epoch [44/200] batch [35/79] time 0.341 (0.489) data 0.209 (0.357) loss_u loss_u 0.9854 (0.9366) acc_u 0.0000 (8.3929) lr 1.7902e-03 eta 0:00:21
epoch [44/200] batch [40/79] time 0.464 (0.487) data 0.332 (0.355) loss_u loss_u 0.8696 (0.9334) acc_u 15.6250 (8.9062) lr 1.7902e-03 eta 0:00:19
epoch [44/200] batch [45/79] time 0.348 (0.483) data 0.217 (0.351) loss_u loss_u 0.9565 (0.9345) acc_u 6.2500 (8.7500) lr 1.7902e-03 eta 0:00:16
epoch [44/200] batch [50/79] time 0.487 (0.480) data 0.355 (0.348) loss_u loss_u 0.9326 (0.9352) acc_u 9.3750 (8.6875) lr 1.7902e-03 eta 0:00:13
epoch [44/200] batch [55/79] time 0.322 (0.476) data 0.190 (0.344) loss_u loss_u 0.9004 (0.9348) acc_u 12.5000 (8.7500) lr 1.7902e-03 eta 0:00:11
epoch [44/200] batch [60/79] time 0.519 (0.474) data 0.388 (0.342) loss_u loss_u 0.9595 (0.9357) acc_u 6.2500 (8.6979) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [65/79] time 0.709 (0.474) data 0.578 (0.342) loss_u loss_u 0.9355 (0.9352) acc_u 9.3750 (8.7500) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [70/79] time 0.417 (0.471) data 0.285 (0.339) loss_u loss_u 0.9224 (0.9354) acc_u 9.3750 (8.6607) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [75/79] time 0.371 (0.467) data 0.239 (0.335) loss_u loss_u 0.9668 (0.9364) acc_u 3.1250 (8.4167) lr 1.7902e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1669
confident_label rate tensor(0.1983, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 622
clean true:612
clean false:10
clean_rate:0.9839228295819936
noisy true:855
noisy false:1659
after delete: len(clean_dataset) 622
after delete: len(noisy_dataset) 2514
epoch [45/200] batch [5/19] time 0.876 (0.572) data 0.743 (0.441) loss_x loss_x 0.8970 (1.1243) acc_x 75.0000 (70.0000) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [10/19] time 0.583 (0.552) data 0.451 (0.420) loss_x loss_x 1.1367 (1.1975) acc_x 71.8750 (70.3125) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [15/19] time 0.522 (0.543) data 0.392 (0.411) loss_x loss_x 0.9370 (1.3113) acc_x 75.0000 (68.7500) lr 1.7804e-03 eta 0:00:02
epoch [45/200] batch [5/78] time 0.492 (0.522) data 0.361 (0.390) loss_u loss_u 0.9087 (0.9430) acc_u 12.5000 (7.5000) lr 1.7804e-03 eta 0:00:38
epoch [45/200] batch [10/78] time 0.551 (0.509) data 0.419 (0.377) loss_u loss_u 0.9355 (0.9339) acc_u 9.3750 (7.8125) lr 1.7804e-03 eta 0:00:34
epoch [45/200] batch [15/78] time 0.323 (0.495) data 0.191 (0.363) loss_u loss_u 0.9482 (0.9282) acc_u 6.2500 (8.9583) lr 1.7804e-03 eta 0:00:31
epoch [45/200] batch [20/78] time 0.433 (0.484) data 0.301 (0.352) loss_u loss_u 0.8794 (0.9331) acc_u 18.7500 (8.5938) lr 1.7804e-03 eta 0:00:28
epoch [45/200] batch [25/78] time 0.454 (0.479) data 0.322 (0.347) loss_u loss_u 0.9175 (0.9357) acc_u 12.5000 (8.6250) lr 1.7804e-03 eta 0:00:25
epoch [45/200] batch [30/78] time 0.457 (0.473) data 0.326 (0.342) loss_u loss_u 0.9590 (0.9364) acc_u 6.2500 (8.6458) lr 1.7804e-03 eta 0:00:22
epoch [45/200] batch [35/78] time 0.505 (0.472) data 0.374 (0.340) loss_u loss_u 0.9146 (0.9366) acc_u 12.5000 (8.6607) lr 1.7804e-03 eta 0:00:20
epoch [45/200] batch [40/78] time 0.435 (0.467) data 0.304 (0.335) loss_u loss_u 0.9380 (0.9372) acc_u 9.3750 (8.5156) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [45/78] time 0.384 (0.467) data 0.253 (0.335) loss_u loss_u 0.9800 (0.9361) acc_u 6.2500 (8.7500) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [50/78] time 0.505 (0.466) data 0.374 (0.334) loss_u loss_u 0.9424 (0.9375) acc_u 9.3750 (8.6250) lr 1.7804e-03 eta 0:00:13
epoch [45/200] batch [55/78] time 0.343 (0.464) data 0.212 (0.333) loss_u loss_u 0.9175 (0.9374) acc_u 12.5000 (8.6364) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [60/78] time 0.465 (0.461) data 0.334 (0.330) loss_u loss_u 0.9385 (0.9383) acc_u 9.3750 (8.4896) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [65/78] time 0.612 (0.461) data 0.480 (0.330) loss_u loss_u 0.9819 (0.9387) acc_u 6.2500 (8.5577) lr 1.7804e-03 eta 0:00:05
epoch [45/200] batch [70/78] time 0.391 (0.460) data 0.260 (0.328) loss_u loss_u 0.9253 (0.9391) acc_u 9.3750 (8.5268) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [75/78] time 0.424 (0.458) data 0.292 (0.326) loss_u loss_u 0.8999 (0.9379) acc_u 9.3750 (8.6250) lr 1.7804e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1693
confident_label rate tensor(0.1980, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 621
clean true:613
clean false:8
clean_rate:0.9871175523349437
noisy true:830
noisy false:1685
after delete: len(clean_dataset) 621
after delete: len(noisy_dataset) 2515
epoch [46/200] batch [5/19] time 0.412 (0.493) data 0.280 (0.362) loss_x loss_x 1.4131 (1.4943) acc_x 56.2500 (58.7500) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [10/19] time 0.458 (0.487) data 0.328 (0.356) loss_x loss_x 1.1963 (1.3549) acc_x 78.1250 (64.0625) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [15/19] time 0.515 (0.488) data 0.385 (0.356) loss_x loss_x 1.4463 (1.3341) acc_x 68.7500 (64.7917) lr 1.7705e-03 eta 0:00:01
epoch [46/200] batch [5/78] time 0.387 (0.483) data 0.255 (0.352) loss_u loss_u 0.9399 (0.9394) acc_u 6.2500 (7.5000) lr 1.7705e-03 eta 0:00:35
epoch [46/200] batch [10/78] time 0.429 (0.475) data 0.298 (0.344) loss_u loss_u 0.9463 (0.9415) acc_u 6.2500 (7.5000) lr 1.7705e-03 eta 0:00:32
epoch [46/200] batch [15/78] time 0.355 (0.473) data 0.224 (0.341) loss_u loss_u 0.9683 (0.9395) acc_u 3.1250 (7.7083) lr 1.7705e-03 eta 0:00:29
epoch [46/200] batch [20/78] time 0.428 (0.474) data 0.296 (0.343) loss_u loss_u 0.9736 (0.9462) acc_u 3.1250 (6.8750) lr 1.7705e-03 eta 0:00:27
epoch [46/200] batch [25/78] time 0.385 (0.472) data 0.253 (0.341) loss_u loss_u 0.9302 (0.9435) acc_u 6.2500 (7.0000) lr 1.7705e-03 eta 0:00:25
epoch [46/200] batch [30/78] time 0.406 (0.474) data 0.275 (0.343) loss_u loss_u 0.8818 (0.9419) acc_u 15.6250 (7.1875) lr 1.7705e-03 eta 0:00:22
epoch [46/200] batch [35/78] time 0.422 (0.474) data 0.290 (0.343) loss_u loss_u 0.9375 (0.9415) acc_u 12.5000 (7.5000) lr 1.7705e-03 eta 0:00:20
epoch [46/200] batch [40/78] time 0.375 (0.473) data 0.244 (0.341) loss_u loss_u 0.9380 (0.9440) acc_u 12.5000 (7.3438) lr 1.7705e-03 eta 0:00:17
epoch [46/200] batch [45/78] time 0.430 (0.471) data 0.297 (0.339) loss_u loss_u 0.9751 (0.9445) acc_u 3.1250 (7.2222) lr 1.7705e-03 eta 0:00:15
epoch [46/200] batch [50/78] time 0.623 (0.474) data 0.490 (0.342) loss_u loss_u 0.9531 (0.9451) acc_u 3.1250 (7.1250) lr 1.7705e-03 eta 0:00:13
epoch [46/200] batch [55/78] time 0.712 (0.472) data 0.579 (0.341) loss_u loss_u 0.9229 (0.9432) acc_u 9.3750 (7.3295) lr 1.7705e-03 eta 0:00:10
epoch [46/200] batch [60/78] time 0.473 (0.471) data 0.341 (0.339) loss_u loss_u 0.9756 (0.9435) acc_u 3.1250 (7.2917) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [65/78] time 0.343 (0.465) data 0.211 (0.333) loss_u loss_u 0.9189 (0.9428) acc_u 6.2500 (7.3077) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [70/78] time 0.409 (0.462) data 0.279 (0.330) loss_u loss_u 0.9478 (0.9425) acc_u 6.2500 (7.4107) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [75/78] time 0.396 (0.459) data 0.263 (0.327) loss_u loss_u 0.9409 (0.9423) acc_u 6.2500 (7.3333) lr 1.7705e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1689
confident_label rate tensor(0.1967, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 617
clean true:608
clean false:9
clean_rate:0.9854132901134521
noisy true:839
noisy false:1680
after delete: len(clean_dataset) 617
after delete: len(noisy_dataset) 2519
epoch [47/200] batch [5/19] time 0.474 (0.509) data 0.342 (0.378) loss_x loss_x 1.1270 (1.3018) acc_x 75.0000 (68.1250) lr 1.7604e-03 eta 0:00:07
epoch [47/200] batch [10/19] time 0.468 (0.496) data 0.336 (0.364) loss_x loss_x 1.0498 (1.3559) acc_x 65.6250 (65.6250) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [15/19] time 0.479 (0.492) data 0.348 (0.361) loss_x loss_x 1.3008 (1.3697) acc_x 53.1250 (65.6250) lr 1.7604e-03 eta 0:00:01
epoch [47/200] batch [5/78] time 0.357 (0.483) data 0.225 (0.351) loss_u loss_u 0.9102 (0.9147) acc_u 9.3750 (11.8750) lr 1.7604e-03 eta 0:00:35
epoch [47/200] batch [10/78] time 0.393 (0.475) data 0.262 (0.343) loss_u loss_u 0.9443 (0.9138) acc_u 9.3750 (12.1875) lr 1.7604e-03 eta 0:00:32
epoch [47/200] batch [15/78] time 0.459 (0.473) data 0.327 (0.342) loss_u loss_u 0.9746 (0.9229) acc_u 3.1250 (10.2083) lr 1.7604e-03 eta 0:00:29
epoch [47/200] batch [20/78] time 0.403 (0.470) data 0.271 (0.339) loss_u loss_u 0.9497 (0.9261) acc_u 6.2500 (9.8438) lr 1.7604e-03 eta 0:00:27
epoch [47/200] batch [25/78] time 0.530 (0.466) data 0.397 (0.335) loss_u loss_u 0.9688 (0.9277) acc_u 3.1250 (9.7500) lr 1.7604e-03 eta 0:00:24
epoch [47/200] batch [30/78] time 0.551 (0.475) data 0.420 (0.343) loss_u loss_u 0.9521 (0.9302) acc_u 6.2500 (9.1667) lr 1.7604e-03 eta 0:00:22
epoch [47/200] batch [35/78] time 0.612 (0.485) data 0.479 (0.353) loss_u loss_u 0.9170 (0.9301) acc_u 12.5000 (9.0179) lr 1.7604e-03 eta 0:00:20
epoch [47/200] batch [40/78] time 0.486 (0.486) data 0.354 (0.354) loss_u loss_u 0.9639 (0.9311) acc_u 6.2500 (9.1406) lr 1.7604e-03 eta 0:00:18
epoch [47/200] batch [45/78] time 0.454 (0.481) data 0.322 (0.349) loss_u loss_u 0.9873 (0.9345) acc_u 0.0000 (8.5417) lr 1.7604e-03 eta 0:00:15
epoch [47/200] batch [50/78] time 0.427 (0.476) data 0.295 (0.344) loss_u loss_u 0.9160 (0.9348) acc_u 12.5000 (8.6250) lr 1.7604e-03 eta 0:00:13
epoch [47/200] batch [55/78] time 0.445 (0.471) data 0.314 (0.339) loss_u loss_u 0.9800 (0.9372) acc_u 0.0000 (8.2386) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [60/78] time 0.376 (0.469) data 0.245 (0.337) loss_u loss_u 0.9580 (0.9381) acc_u 6.2500 (8.1250) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [65/78] time 0.417 (0.464) data 0.286 (0.332) loss_u loss_u 0.9741 (0.9385) acc_u 0.0000 (7.9327) lr 1.7604e-03 eta 0:00:06
epoch [47/200] batch [70/78] time 0.365 (0.461) data 0.233 (0.329) loss_u loss_u 0.9429 (0.9384) acc_u 12.5000 (8.1250) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [75/78] time 0.435 (0.462) data 0.303 (0.330) loss_u loss_u 0.9531 (0.9387) acc_u 6.2500 (8.0417) lr 1.7604e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1724
confident_label rate tensor(0.1952, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 612
clean true:603
clean false:9
clean_rate:0.9852941176470589
noisy true:809
noisy false:1715
after delete: len(clean_dataset) 612
after delete: len(noisy_dataset) 2524
epoch [48/200] batch [5/19] time 0.565 (0.506) data 0.434 (0.375) loss_x loss_x 1.1465 (1.2006) acc_x 68.7500 (68.1250) lr 1.7501e-03 eta 0:00:07
epoch [48/200] batch [10/19] time 0.585 (0.519) data 0.454 (0.388) loss_x loss_x 1.4307 (1.2677) acc_x 50.0000 (67.8125) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [15/19] time 0.404 (0.492) data 0.272 (0.361) loss_x loss_x 1.5479 (1.2949) acc_x 62.5000 (67.2917) lr 1.7501e-03 eta 0:00:01
epoch [48/200] batch [5/78] time 0.373 (0.471) data 0.243 (0.340) loss_u loss_u 0.9087 (0.9026) acc_u 12.5000 (13.7500) lr 1.7501e-03 eta 0:00:34
epoch [48/200] batch [10/78] time 0.481 (0.477) data 0.349 (0.346) loss_u loss_u 0.9751 (0.9300) acc_u 6.2500 (10.3125) lr 1.7501e-03 eta 0:00:32
epoch [48/200] batch [15/78] time 0.340 (0.468) data 0.209 (0.337) loss_u loss_u 0.9741 (0.9389) acc_u 3.1250 (8.5417) lr 1.7501e-03 eta 0:00:29
epoch [48/200] batch [20/78] time 0.459 (0.475) data 0.328 (0.344) loss_u loss_u 0.9717 (0.9397) acc_u 3.1250 (8.1250) lr 1.7501e-03 eta 0:00:27
epoch [48/200] batch [25/78] time 0.440 (0.469) data 0.310 (0.338) loss_u loss_u 0.9482 (0.9386) acc_u 12.5000 (8.2500) lr 1.7501e-03 eta 0:00:24
epoch [48/200] batch [30/78] time 0.391 (0.467) data 0.260 (0.336) loss_u loss_u 0.9468 (0.9396) acc_u 9.3750 (8.3333) lr 1.7501e-03 eta 0:00:22
epoch [48/200] batch [35/78] time 0.405 (0.466) data 0.273 (0.335) loss_u loss_u 0.9336 (0.9400) acc_u 9.3750 (8.3036) lr 1.7501e-03 eta 0:00:20
epoch [48/200] batch [40/78] time 0.377 (0.466) data 0.246 (0.334) loss_u loss_u 0.9478 (0.9409) acc_u 6.2500 (8.2031) lr 1.7501e-03 eta 0:00:17
epoch [48/200] batch [45/78] time 0.431 (0.461) data 0.301 (0.330) loss_u loss_u 0.9390 (0.9414) acc_u 6.2500 (8.0556) lr 1.7501e-03 eta 0:00:15
epoch [48/200] batch [50/78] time 0.392 (0.459) data 0.261 (0.328) loss_u loss_u 0.9429 (0.9420) acc_u 6.2500 (8.0000) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [55/78] time 0.397 (0.455) data 0.266 (0.324) loss_u loss_u 0.9658 (0.9398) acc_u 3.1250 (8.3523) lr 1.7501e-03 eta 0:00:10
epoch [48/200] batch [60/78] time 0.441 (0.451) data 0.311 (0.320) loss_u loss_u 0.8696 (0.9385) acc_u 12.5000 (8.3854) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [65/78] time 0.463 (0.452) data 0.331 (0.321) loss_u loss_u 0.9688 (0.9397) acc_u 3.1250 (8.1731) lr 1.7501e-03 eta 0:00:05
epoch [48/200] batch [70/78] time 0.616 (0.453) data 0.482 (0.322) loss_u loss_u 0.9653 (0.9383) acc_u 0.0000 (8.2589) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [75/78] time 0.314 (0.454) data 0.183 (0.323) loss_u loss_u 0.9429 (0.9384) acc_u 3.1250 (8.1250) lr 1.7501e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1697
confident_label rate tensor(0.1926, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 604
clean true:596
clean false:8
clean_rate:0.9867549668874173
noisy true:843
noisy false:1689
after delete: len(clean_dataset) 604
after delete: len(noisy_dataset) 2532
epoch [49/200] batch [5/18] time 0.405 (0.438) data 0.275 (0.307) loss_x loss_x 1.0430 (1.1182) acc_x 68.7500 (73.7500) lr 1.7396e-03 eta 0:00:05
epoch [49/200] batch [10/18] time 0.358 (0.448) data 0.227 (0.318) loss_x loss_x 1.5537 (1.2024) acc_x 59.3750 (70.9375) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [15/18] time 0.455 (0.443) data 0.325 (0.312) loss_x loss_x 1.4580 (1.2625) acc_x 68.7500 (68.1250) lr 1.7396e-03 eta 0:00:01
epoch [49/200] batch [5/79] time 0.418 (0.445) data 0.286 (0.314) loss_u loss_u 0.9580 (0.9147) acc_u 3.1250 (11.2500) lr 1.7396e-03 eta 0:00:32
epoch [49/200] batch [10/79] time 0.669 (0.462) data 0.539 (0.331) loss_u loss_u 0.9473 (0.9272) acc_u 6.2500 (9.6875) lr 1.7396e-03 eta 0:00:31
epoch [49/200] batch [15/79] time 0.436 (0.458) data 0.306 (0.327) loss_u loss_u 0.9395 (0.9300) acc_u 12.5000 (9.5833) lr 1.7396e-03 eta 0:00:29
epoch [49/200] batch [20/79] time 0.350 (0.465) data 0.219 (0.334) loss_u loss_u 0.8940 (0.9246) acc_u 18.7500 (10.1562) lr 1.7396e-03 eta 0:00:27
epoch [49/200] batch [25/79] time 0.433 (0.467) data 0.302 (0.336) loss_u loss_u 0.9756 (0.9309) acc_u 3.1250 (9.2500) lr 1.7396e-03 eta 0:00:25
epoch [49/200] batch [30/79] time 0.372 (0.467) data 0.240 (0.336) loss_u loss_u 0.9312 (0.9330) acc_u 9.3750 (9.0625) lr 1.7396e-03 eta 0:00:22
epoch [49/200] batch [35/79] time 0.423 (0.465) data 0.292 (0.334) loss_u loss_u 0.9932 (0.9326) acc_u 0.0000 (9.1071) lr 1.7396e-03 eta 0:00:20
epoch [49/200] batch [40/79] time 0.329 (0.462) data 0.198 (0.331) loss_u loss_u 0.9521 (0.9367) acc_u 9.3750 (8.6719) lr 1.7396e-03 eta 0:00:18
epoch [49/200] batch [45/79] time 0.452 (0.462) data 0.320 (0.330) loss_u loss_u 0.8892 (0.9367) acc_u 12.5000 (8.4722) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [50/79] time 0.532 (0.464) data 0.399 (0.333) loss_u loss_u 0.9585 (0.9381) acc_u 3.1250 (8.3750) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [55/79] time 0.397 (0.460) data 0.266 (0.329) loss_u loss_u 0.8633 (0.9365) acc_u 15.6250 (8.3523) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [60/79] time 0.392 (0.459) data 0.259 (0.328) loss_u loss_u 0.9741 (0.9384) acc_u 3.1250 (8.1250) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [65/79] time 0.394 (0.456) data 0.261 (0.325) loss_u loss_u 0.9629 (0.9386) acc_u 3.1250 (8.0769) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [70/79] time 0.405 (0.453) data 0.273 (0.322) loss_u loss_u 0.9351 (0.9375) acc_u 6.2500 (8.1696) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [75/79] time 0.475 (0.451) data 0.343 (0.320) loss_u loss_u 0.9512 (0.9369) acc_u 9.3750 (8.2500) lr 1.7396e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1670
confident_label rate tensor(0.2028, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 636
clean true:628
clean false:8
clean_rate:0.9874213836477987
noisy true:838
noisy false:1662
after delete: len(clean_dataset) 636
after delete: len(noisy_dataset) 2500
epoch [50/200] batch [5/19] time 0.316 (0.414) data 0.185 (0.283) loss_x loss_x 1.2725 (1.2909) acc_x 68.7500 (63.1250) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [10/19] time 0.506 (0.478) data 0.376 (0.347) loss_x loss_x 1.5811 (1.3671) acc_x 65.6250 (65.3125) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [15/19] time 0.433 (0.470) data 0.303 (0.339) loss_x loss_x 1.6299 (1.4038) acc_x 53.1250 (64.1667) lr 1.7290e-03 eta 0:00:01
epoch [50/200] batch [5/78] time 0.457 (0.477) data 0.325 (0.346) loss_u loss_u 0.9741 (0.9609) acc_u 3.1250 (5.6250) lr 1.7290e-03 eta 0:00:34
epoch [50/200] batch [10/78] time 0.543 (0.476) data 0.411 (0.345) loss_u loss_u 0.9761 (0.9661) acc_u 3.1250 (4.6875) lr 1.7290e-03 eta 0:00:32
epoch [50/200] batch [15/78] time 0.369 (0.471) data 0.237 (0.340) loss_u loss_u 0.9482 (0.9595) acc_u 6.2500 (5.4167) lr 1.7290e-03 eta 0:00:29
epoch [50/200] batch [20/78] time 0.413 (0.468) data 0.282 (0.337) loss_u loss_u 0.9331 (0.9596) acc_u 6.2500 (5.1562) lr 1.7290e-03 eta 0:00:27
epoch [50/200] batch [25/78] time 0.354 (0.467) data 0.222 (0.335) loss_u loss_u 0.9282 (0.9520) acc_u 9.3750 (6.6250) lr 1.7290e-03 eta 0:00:24
epoch [50/200] batch [30/78] time 0.424 (0.466) data 0.291 (0.334) loss_u loss_u 0.9644 (0.9505) acc_u 6.2500 (6.9792) lr 1.7290e-03 eta 0:00:22
epoch [50/200] batch [35/78] time 0.404 (0.463) data 0.272 (0.331) loss_u loss_u 0.9790 (0.9515) acc_u 6.2500 (6.8750) lr 1.7290e-03 eta 0:00:19
epoch [50/200] batch [40/78] time 0.390 (0.456) data 0.258 (0.325) loss_u loss_u 0.9478 (0.9526) acc_u 9.3750 (6.9531) lr 1.7290e-03 eta 0:00:17
epoch [50/200] batch [45/78] time 0.428 (0.455) data 0.295 (0.324) loss_u loss_u 0.9907 (0.9526) acc_u 0.0000 (6.8056) lr 1.7290e-03 eta 0:00:15
epoch [50/200] batch [50/78] time 0.440 (0.457) data 0.307 (0.326) loss_u loss_u 0.9424 (0.9514) acc_u 6.2500 (7.0625) lr 1.7290e-03 eta 0:00:12
epoch [50/200] batch [55/78] time 0.519 (0.457) data 0.386 (0.325) loss_u loss_u 0.9775 (0.9510) acc_u 3.1250 (7.1591) lr 1.7290e-03 eta 0:00:10
epoch [50/200] batch [60/78] time 0.557 (0.464) data 0.425 (0.332) loss_u loss_u 0.9873 (0.9522) acc_u 0.0000 (6.9792) lr 1.7290e-03 eta 0:00:08
epoch [50/200] batch [65/78] time 0.521 (0.468) data 0.389 (0.336) loss_u loss_u 0.9253 (0.9504) acc_u 9.3750 (7.0673) lr 1.7290e-03 eta 0:00:06
epoch [50/200] batch [70/78] time 0.629 (0.472) data 0.497 (0.341) loss_u loss_u 0.9092 (0.9493) acc_u 12.5000 (7.1429) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [75/78] time 0.487 (0.471) data 0.352 (0.339) loss_u loss_u 0.9888 (0.9484) acc_u 0.0000 (7.2500) lr 1.7290e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1682
confident_label rate tensor(0.2003, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 628
clean true:617
clean false:11
clean_rate:0.982484076433121
noisy true:837
noisy false:1671
after delete: len(clean_dataset) 628
after delete: len(noisy_dataset) 2508
epoch [51/200] batch [5/19] time 0.467 (0.532) data 0.334 (0.400) loss_x loss_x 1.3652 (1.1339) acc_x 65.6250 (70.6250) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [10/19] time 0.475 (0.521) data 0.343 (0.389) loss_x loss_x 1.5723 (1.2702) acc_x 65.6250 (68.4375) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [15/19] time 0.444 (0.514) data 0.313 (0.383) loss_x loss_x 1.1094 (1.3191) acc_x 71.8750 (67.0833) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [5/78] time 0.476 (0.506) data 0.344 (0.374) loss_u loss_u 0.9473 (0.9631) acc_u 9.3750 (5.6250) lr 1.7181e-03 eta 0:00:36
epoch [51/200] batch [10/78] time 0.497 (0.501) data 0.366 (0.370) loss_u loss_u 0.9521 (0.9476) acc_u 6.2500 (6.5625) lr 1.7181e-03 eta 0:00:34
epoch [51/200] batch [15/78] time 0.384 (0.493) data 0.253 (0.362) loss_u loss_u 0.9688 (0.9521) acc_u 6.2500 (6.2500) lr 1.7181e-03 eta 0:00:31
epoch [51/200] batch [20/78] time 0.424 (0.491) data 0.292 (0.359) loss_u loss_u 0.9775 (0.9486) acc_u 6.2500 (6.7188) lr 1.7181e-03 eta 0:00:28
epoch [51/200] batch [25/78] time 0.441 (0.484) data 0.309 (0.352) loss_u loss_u 0.9204 (0.9462) acc_u 12.5000 (7.1250) lr 1.7181e-03 eta 0:00:25
epoch [51/200] batch [30/78] time 0.320 (0.475) data 0.189 (0.344) loss_u loss_u 0.9663 (0.9450) acc_u 6.2500 (7.5000) lr 1.7181e-03 eta 0:00:22
epoch [51/200] batch [35/78] time 0.465 (0.472) data 0.333 (0.340) loss_u loss_u 0.9434 (0.9447) acc_u 6.2500 (7.2321) lr 1.7181e-03 eta 0:00:20
epoch [51/200] batch [40/78] time 0.394 (0.466) data 0.262 (0.335) loss_u loss_u 0.9541 (0.9441) acc_u 3.1250 (7.1875) lr 1.7181e-03 eta 0:00:17
epoch [51/200] batch [45/78] time 0.493 (0.468) data 0.362 (0.337) loss_u loss_u 0.9517 (0.9462) acc_u 6.2500 (6.9444) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [50/78] time 0.375 (0.467) data 0.243 (0.336) loss_u loss_u 0.8848 (0.9451) acc_u 15.6250 (7.0625) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [55/78] time 0.476 (0.467) data 0.345 (0.335) loss_u loss_u 0.8901 (0.9438) acc_u 12.5000 (7.2727) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [60/78] time 0.440 (0.468) data 0.308 (0.336) loss_u loss_u 0.9219 (0.9435) acc_u 12.5000 (7.4479) lr 1.7181e-03 eta 0:00:08
epoch [51/200] batch [65/78] time 0.324 (0.466) data 0.191 (0.334) loss_u loss_u 0.9136 (0.9436) acc_u 12.5000 (7.4519) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [70/78] time 0.345 (0.464) data 0.213 (0.333) loss_u loss_u 0.9858 (0.9442) acc_u 3.1250 (7.5893) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [75/78] time 0.478 (0.465) data 0.347 (0.333) loss_u loss_u 0.9795 (0.9438) acc_u 0.0000 (7.6250) lr 1.7181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1727
confident_label rate tensor(0.1964, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 616
clean true:610
clean false:6
clean_rate:0.9902597402597403
noisy true:799
noisy false:1721
after delete: len(clean_dataset) 616
after delete: len(noisy_dataset) 2520
epoch [52/200] batch [5/19] time 0.446 (0.445) data 0.314 (0.314) loss_x loss_x 1.3545 (1.4688) acc_x 62.5000 (61.8750) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [10/19] time 0.554 (0.490) data 0.419 (0.357) loss_x loss_x 1.5098 (1.4564) acc_x 68.7500 (65.6250) lr 1.7071e-03 eta 0:00:04
epoch [52/200] batch [15/19] time 0.679 (0.494) data 0.548 (0.362) loss_x loss_x 1.1445 (1.3879) acc_x 71.8750 (66.4583) lr 1.7071e-03 eta 0:00:01
epoch [52/200] batch [5/78] time 0.521 (0.499) data 0.390 (0.367) loss_u loss_u 0.9277 (0.9267) acc_u 6.2500 (8.7500) lr 1.7071e-03 eta 0:00:36
epoch [52/200] batch [10/78] time 0.405 (0.492) data 0.274 (0.360) loss_u loss_u 0.9570 (0.9452) acc_u 6.2500 (6.5625) lr 1.7071e-03 eta 0:00:33
epoch [52/200] batch [15/78] time 0.508 (0.493) data 0.377 (0.362) loss_u loss_u 0.9575 (0.9465) acc_u 3.1250 (6.2500) lr 1.7071e-03 eta 0:00:31
epoch [52/200] batch [20/78] time 0.420 (0.483) data 0.288 (0.352) loss_u loss_u 0.9761 (0.9479) acc_u 6.2500 (6.4062) lr 1.7071e-03 eta 0:00:28
epoch [52/200] batch [25/78] time 0.429 (0.476) data 0.295 (0.344) loss_u loss_u 0.9028 (0.9424) acc_u 15.6250 (7.3750) lr 1.7071e-03 eta 0:00:25
epoch [52/200] batch [30/78] time 0.505 (0.478) data 0.374 (0.347) loss_u loss_u 0.9033 (0.9378) acc_u 15.6250 (7.9167) lr 1.7071e-03 eta 0:00:22
epoch [52/200] batch [35/78] time 0.662 (0.486) data 0.530 (0.354) loss_u loss_u 0.9644 (0.9396) acc_u 6.2500 (7.5893) lr 1.7071e-03 eta 0:00:20
epoch [52/200] batch [40/78] time 0.460 (0.493) data 0.329 (0.361) loss_u loss_u 0.9419 (0.9415) acc_u 9.3750 (7.3438) lr 1.7071e-03 eta 0:00:18
epoch [52/200] batch [45/78] time 0.514 (0.494) data 0.383 (0.362) loss_u loss_u 0.9521 (0.9427) acc_u 6.2500 (7.2917) lr 1.7071e-03 eta 0:00:16
epoch [52/200] batch [50/78] time 0.382 (0.488) data 0.250 (0.357) loss_u loss_u 0.9526 (0.9430) acc_u 9.3750 (7.5000) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [55/78] time 0.440 (0.487) data 0.308 (0.355) loss_u loss_u 0.9609 (0.9422) acc_u 6.2500 (7.6136) lr 1.7071e-03 eta 0:00:11
epoch [52/200] batch [60/78] time 0.555 (0.487) data 0.424 (0.356) loss_u loss_u 0.9277 (0.9411) acc_u 9.3750 (7.7083) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [65/78] time 0.495 (0.489) data 0.363 (0.357) loss_u loss_u 0.9219 (0.9411) acc_u 9.3750 (7.7404) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [70/78] time 0.431 (0.487) data 0.299 (0.355) loss_u loss_u 0.9731 (0.9412) acc_u 3.1250 (7.6786) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [75/78] time 0.407 (0.485) data 0.275 (0.353) loss_u loss_u 0.9497 (0.9404) acc_u 9.3750 (7.7917) lr 1.7071e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1717
confident_label rate tensor(0.1875, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 588
clean true:582
clean false:6
clean_rate:0.9897959183673469
noisy true:837
noisy false:1711
after delete: len(clean_dataset) 588
after delete: len(noisy_dataset) 2548
epoch [53/200] batch [5/18] time 0.491 (0.462) data 0.360 (0.331) loss_x loss_x 1.2334 (1.3211) acc_x 68.7500 (63.1250) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [10/18] time 0.468 (0.468) data 0.337 (0.337) loss_x loss_x 1.2715 (1.2508) acc_x 65.6250 (67.8125) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [15/18] time 0.412 (0.472) data 0.281 (0.341) loss_x loss_x 1.2471 (1.2517) acc_x 59.3750 (67.9167) lr 1.6959e-03 eta 0:00:01
epoch [53/200] batch [5/79] time 0.424 (0.493) data 0.293 (0.362) loss_u loss_u 0.9170 (0.9258) acc_u 9.3750 (10.0000) lr 1.6959e-03 eta 0:00:36
epoch [53/200] batch [10/79] time 0.444 (0.481) data 0.312 (0.350) loss_u loss_u 0.9341 (0.9151) acc_u 9.3750 (10.6250) lr 1.6959e-03 eta 0:00:33
epoch [53/200] batch [15/79] time 0.463 (0.477) data 0.331 (0.346) loss_u loss_u 0.9658 (0.9293) acc_u 3.1250 (8.5417) lr 1.6959e-03 eta 0:00:30
epoch [53/200] batch [20/79] time 0.421 (0.468) data 0.289 (0.337) loss_u loss_u 0.8896 (0.9324) acc_u 15.6250 (8.1250) lr 1.6959e-03 eta 0:00:27
epoch [53/200] batch [25/79] time 0.595 (0.466) data 0.463 (0.335) loss_u loss_u 0.9683 (0.9320) acc_u 3.1250 (8.3750) lr 1.6959e-03 eta 0:00:25
epoch [53/200] batch [30/79] time 0.541 (0.465) data 0.409 (0.334) loss_u loss_u 0.9053 (0.9322) acc_u 12.5000 (8.5417) lr 1.6959e-03 eta 0:00:22
epoch [53/200] batch [35/79] time 0.385 (0.463) data 0.253 (0.332) loss_u loss_u 0.9546 (0.9330) acc_u 6.2500 (8.4821) lr 1.6959e-03 eta 0:00:20
epoch [53/200] batch [40/79] time 0.483 (0.463) data 0.351 (0.332) loss_u loss_u 0.9766 (0.9342) acc_u 0.0000 (8.2812) lr 1.6959e-03 eta 0:00:18
epoch [53/200] batch [45/79] time 0.445 (0.466) data 0.313 (0.334) loss_u loss_u 0.9629 (0.9355) acc_u 3.1250 (8.1250) lr 1.6959e-03 eta 0:00:15
epoch [53/200] batch [50/79] time 0.472 (0.464) data 0.339 (0.332) loss_u loss_u 0.9575 (0.9353) acc_u 3.1250 (8.0625) lr 1.6959e-03 eta 0:00:13
epoch [53/200] batch [55/79] time 0.398 (0.463) data 0.266 (0.331) loss_u loss_u 0.9473 (0.9365) acc_u 9.3750 (7.7273) lr 1.6959e-03 eta 0:00:11
epoch [53/200] batch [60/79] time 0.407 (0.458) data 0.276 (0.326) loss_u loss_u 0.9287 (0.9366) acc_u 6.2500 (7.7083) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [65/79] time 0.469 (0.460) data 0.334 (0.329) loss_u loss_u 0.9663 (0.9379) acc_u 3.1250 (7.5000) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [70/79] time 0.359 (0.464) data 0.228 (0.332) loss_u loss_u 0.9863 (0.9394) acc_u 0.0000 (7.3661) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [75/79] time 0.388 (0.463) data 0.256 (0.331) loss_u loss_u 0.9595 (0.9406) acc_u 6.2500 (7.1667) lr 1.6959e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1695
confident_label rate tensor(0.2034, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 638
clean true:630
clean false:8
clean_rate:0.987460815047022
noisy true:811
noisy false:1687
after delete: len(clean_dataset) 638
after delete: len(noisy_dataset) 2498
epoch [54/200] batch [5/19] time 0.565 (0.534) data 0.434 (0.403) loss_x loss_x 1.2031 (1.3326) acc_x 65.6250 (65.6250) lr 1.6845e-03 eta 0:00:07
epoch [54/200] batch [10/19] time 0.436 (0.523) data 0.305 (0.391) loss_x loss_x 1.5361 (1.3985) acc_x 65.6250 (64.3750) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [15/19] time 0.452 (0.497) data 0.321 (0.366) loss_x loss_x 1.8633 (1.3830) acc_x 46.8750 (64.3750) lr 1.6845e-03 eta 0:00:01
epoch [54/200] batch [5/78] time 0.509 (0.496) data 0.376 (0.365) loss_u loss_u 0.9248 (0.9474) acc_u 12.5000 (6.8750) lr 1.6845e-03 eta 0:00:36
epoch [54/200] batch [10/78] time 0.469 (0.503) data 0.337 (0.372) loss_u loss_u 0.8911 (0.9455) acc_u 18.7500 (7.5000) lr 1.6845e-03 eta 0:00:34
epoch [54/200] batch [15/78] time 0.415 (0.490) data 0.284 (0.359) loss_u loss_u 0.9570 (0.9469) acc_u 6.2500 (7.7083) lr 1.6845e-03 eta 0:00:30
epoch [54/200] batch [20/78] time 0.558 (0.489) data 0.427 (0.358) loss_u loss_u 0.9072 (0.9408) acc_u 9.3750 (7.8125) lr 1.6845e-03 eta 0:00:28
epoch [54/200] batch [25/78] time 0.448 (0.481) data 0.318 (0.349) loss_u loss_u 0.9409 (0.9421) acc_u 9.3750 (7.7500) lr 1.6845e-03 eta 0:00:25
epoch [54/200] batch [30/78] time 0.446 (0.478) data 0.316 (0.347) loss_u loss_u 0.9600 (0.9428) acc_u 3.1250 (7.7083) lr 1.6845e-03 eta 0:00:22
epoch [54/200] batch [35/78] time 0.419 (0.477) data 0.288 (0.346) loss_u loss_u 0.9854 (0.9455) acc_u 3.1250 (7.2321) lr 1.6845e-03 eta 0:00:20
epoch [54/200] batch [40/78] time 0.489 (0.474) data 0.357 (0.343) loss_u loss_u 0.9292 (0.9457) acc_u 12.5000 (7.3438) lr 1.6845e-03 eta 0:00:18
epoch [54/200] batch [45/78] time 0.401 (0.471) data 0.268 (0.340) loss_u loss_u 0.9541 (0.9457) acc_u 3.1250 (7.0833) lr 1.6845e-03 eta 0:00:15
epoch [54/200] batch [50/78] time 0.412 (0.469) data 0.281 (0.338) loss_u loss_u 0.9702 (0.9440) acc_u 3.1250 (7.3125) lr 1.6845e-03 eta 0:00:13
epoch [54/200] batch [55/78] time 0.519 (0.470) data 0.389 (0.338) loss_u loss_u 0.8828 (0.9430) acc_u 15.6250 (7.5568) lr 1.6845e-03 eta 0:00:10
epoch [54/200] batch [60/78] time 0.529 (0.472) data 0.398 (0.341) loss_u loss_u 0.9761 (0.9427) acc_u 0.0000 (7.5521) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [65/78] time 0.482 (0.473) data 0.351 (0.342) loss_u loss_u 0.9648 (0.9425) acc_u 3.1250 (7.5481) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [70/78] time 0.373 (0.470) data 0.242 (0.339) loss_u loss_u 0.9585 (0.9424) acc_u 3.1250 (7.5446) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [75/78] time 0.437 (0.468) data 0.305 (0.337) loss_u loss_u 0.9771 (0.9432) acc_u 3.1250 (7.5417) lr 1.6845e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1683
confident_label rate tensor(0.1936, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 607
clean true:601
clean false:6
clean_rate:0.9901153212520593
noisy true:852
noisy false:1677
after delete: len(clean_dataset) 607
after delete: len(noisy_dataset) 2529
epoch [55/200] batch [5/18] time 0.588 (0.503) data 0.458 (0.372) loss_x loss_x 0.9507 (1.1091) acc_x 71.8750 (71.8750) lr 1.6730e-03 eta 0:00:06
epoch [55/200] batch [10/18] time 0.675 (0.472) data 0.542 (0.340) loss_x loss_x 1.3340 (1.2487) acc_x 65.6250 (68.4375) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [15/18] time 0.514 (0.489) data 0.383 (0.358) loss_x loss_x 1.0840 (1.2256) acc_x 68.7500 (69.1667) lr 1.6730e-03 eta 0:00:01
epoch [55/200] batch [5/79] time 0.391 (0.490) data 0.260 (0.358) loss_u loss_u 0.9346 (0.9499) acc_u 9.3750 (5.6250) lr 1.6730e-03 eta 0:00:36
epoch [55/200] batch [10/79] time 0.345 (0.483) data 0.213 (0.352) loss_u loss_u 0.9492 (0.9418) acc_u 6.2500 (7.5000) lr 1.6730e-03 eta 0:00:33
epoch [55/200] batch [15/79] time 0.508 (0.479) data 0.374 (0.348) loss_u loss_u 0.8560 (0.9366) acc_u 18.7500 (8.5417) lr 1.6730e-03 eta 0:00:30
epoch [55/200] batch [20/79] time 0.548 (0.478) data 0.416 (0.347) loss_u loss_u 0.8848 (0.9355) acc_u 21.8750 (9.0625) lr 1.6730e-03 eta 0:00:28
epoch [55/200] batch [25/79] time 0.392 (0.479) data 0.260 (0.347) loss_u loss_u 0.9092 (0.9401) acc_u 12.5000 (8.5000) lr 1.6730e-03 eta 0:00:25
epoch [55/200] batch [30/79] time 0.423 (0.473) data 0.291 (0.341) loss_u loss_u 0.8945 (0.9410) acc_u 18.7500 (8.5417) lr 1.6730e-03 eta 0:00:23
epoch [55/200] batch [35/79] time 0.417 (0.468) data 0.285 (0.336) loss_u loss_u 0.9453 (0.9401) acc_u 6.2500 (8.6607) lr 1.6730e-03 eta 0:00:20
epoch [55/200] batch [40/79] time 0.448 (0.466) data 0.317 (0.335) loss_u loss_u 0.9629 (0.9392) acc_u 3.1250 (8.5156) lr 1.6730e-03 eta 0:00:18
epoch [55/200] batch [45/79] time 0.685 (0.475) data 0.553 (0.343) loss_u loss_u 0.9551 (0.9378) acc_u 6.2500 (8.6111) lr 1.6730e-03 eta 0:00:16
epoch [55/200] batch [50/79] time 0.449 (0.476) data 0.317 (0.344) loss_u loss_u 0.8613 (0.9361) acc_u 21.8750 (8.9375) lr 1.6730e-03 eta 0:00:13
epoch [55/200] batch [55/79] time 0.544 (0.475) data 0.411 (0.343) loss_u loss_u 0.9541 (0.9360) acc_u 6.2500 (8.9205) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [60/79] time 0.546 (0.474) data 0.414 (0.342) loss_u loss_u 0.9258 (0.9368) acc_u 6.2500 (8.7500) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [65/79] time 0.600 (0.475) data 0.469 (0.343) loss_u loss_u 0.9302 (0.9375) acc_u 12.5000 (8.7019) lr 1.6730e-03 eta 0:00:06
epoch [55/200] batch [70/79] time 0.377 (0.473) data 0.245 (0.341) loss_u loss_u 0.9487 (0.9373) acc_u 6.2500 (8.7054) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [75/79] time 0.429 (0.474) data 0.297 (0.342) loss_u loss_u 0.9946 (0.9379) acc_u 0.0000 (8.6250) lr 1.6730e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1608
confident_label rate tensor(0.2111, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 662
clean true:650
clean false:12
clean_rate:0.9818731117824774
noisy true:878
noisy false:1596
after delete: len(clean_dataset) 662
after delete: len(noisy_dataset) 2474
epoch [56/200] batch [5/20] time 0.408 (0.441) data 0.277 (0.310) loss_x loss_x 1.5820 (1.6963) acc_x 71.8750 (62.5000) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [10/20] time 0.486 (0.455) data 0.354 (0.323) loss_x loss_x 1.3633 (1.5831) acc_x 71.8750 (64.6875) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [15/20] time 0.481 (0.473) data 0.350 (0.342) loss_x loss_x 1.0967 (1.5076) acc_x 84.3750 (66.6667) lr 1.6613e-03 eta 0:00:02
epoch [56/200] batch [20/20] time 0.483 (0.490) data 0.352 (0.359) loss_x loss_x 1.8506 (1.4696) acc_x 59.3750 (67.6562) lr 1.6613e-03 eta 0:00:00
epoch [56/200] batch [5/77] time 0.497 (0.482) data 0.365 (0.351) loss_u loss_u 0.9438 (0.9545) acc_u 6.2500 (5.6250) lr 1.6613e-03 eta 0:00:34
epoch [56/200] batch [10/77] time 0.452 (0.480) data 0.319 (0.348) loss_u loss_u 0.9658 (0.9488) acc_u 3.1250 (6.8750) lr 1.6613e-03 eta 0:00:32
epoch [56/200] batch [15/77] time 0.496 (0.474) data 0.364 (0.342) loss_u loss_u 0.9702 (0.9520) acc_u 6.2500 (6.2500) lr 1.6613e-03 eta 0:00:29
epoch [56/200] batch [20/77] time 0.437 (0.467) data 0.305 (0.336) loss_u loss_u 0.8994 (0.9488) acc_u 9.3750 (6.0938) lr 1.6613e-03 eta 0:00:26
epoch [56/200] batch [25/77] time 0.352 (0.464) data 0.221 (0.333) loss_u loss_u 0.9307 (0.9461) acc_u 6.2500 (6.2500) lr 1.6613e-03 eta 0:00:24
epoch [56/200] batch [30/77] time 0.471 (0.467) data 0.339 (0.336) loss_u loss_u 0.9399 (0.9459) acc_u 6.2500 (6.1458) lr 1.6613e-03 eta 0:00:21
epoch [56/200] batch [35/77] time 0.381 (0.461) data 0.249 (0.330) loss_u loss_u 0.9307 (0.9430) acc_u 3.1250 (6.6964) lr 1.6613e-03 eta 0:00:19
epoch [56/200] batch [40/77] time 0.582 (0.464) data 0.449 (0.332) loss_u loss_u 0.9492 (0.9428) acc_u 9.3750 (6.7969) lr 1.6613e-03 eta 0:00:17
epoch [56/200] batch [45/77] time 0.460 (0.466) data 0.330 (0.334) loss_u loss_u 0.9478 (0.9440) acc_u 9.3750 (6.6667) lr 1.6613e-03 eta 0:00:14
epoch [56/200] batch [50/77] time 0.500 (0.467) data 0.369 (0.335) loss_u loss_u 0.9277 (0.9453) acc_u 9.3750 (6.5000) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [55/77] time 0.645 (0.466) data 0.513 (0.334) loss_u loss_u 0.9604 (0.9454) acc_u 0.0000 (6.4205) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [60/77] time 0.410 (0.463) data 0.279 (0.332) loss_u loss_u 0.8921 (0.9435) acc_u 12.5000 (6.6667) lr 1.6613e-03 eta 0:00:07
epoch [56/200] batch [65/77] time 0.493 (0.461) data 0.362 (0.329) loss_u loss_u 0.8999 (0.9428) acc_u 18.7500 (6.9231) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [70/77] time 0.377 (0.460) data 0.245 (0.328) loss_u loss_u 0.9409 (0.9425) acc_u 6.2500 (6.9643) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [75/77] time 0.472 (0.460) data 0.341 (0.329) loss_u loss_u 0.9873 (0.9432) acc_u 0.0000 (6.8750) lr 1.6613e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1691
confident_label rate tensor(0.1955, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 613
clean true:604
clean false:9
clean_rate:0.9853181076672104
noisy true:841
noisy false:1682
after delete: len(clean_dataset) 613
after delete: len(noisy_dataset) 2523
epoch [57/200] batch [5/19] time 0.486 (0.494) data 0.354 (0.363) loss_x loss_x 1.2324 (1.3590) acc_x 68.7500 (66.2500) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [10/19] time 0.424 (0.474) data 0.293 (0.343) loss_x loss_x 1.2568 (1.3916) acc_x 59.3750 (65.3125) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [15/19] time 0.684 (0.498) data 0.552 (0.366) loss_x loss_x 1.5000 (1.3937) acc_x 65.6250 (65.8333) lr 1.6494e-03 eta 0:00:01
epoch [57/200] batch [5/78] time 0.379 (0.475) data 0.248 (0.343) loss_u loss_u 0.9038 (0.9271) acc_u 12.5000 (9.3750) lr 1.6494e-03 eta 0:00:34
epoch [57/200] batch [10/78] time 0.400 (0.461) data 0.268 (0.329) loss_u loss_u 0.9136 (0.9297) acc_u 9.3750 (8.7500) lr 1.6494e-03 eta 0:00:31
epoch [57/200] batch [15/78] time 0.659 (0.462) data 0.527 (0.331) loss_u loss_u 0.8545 (0.9337) acc_u 15.6250 (7.7083) lr 1.6494e-03 eta 0:00:29
epoch [57/200] batch [20/78] time 0.483 (0.460) data 0.351 (0.329) loss_u loss_u 0.8682 (0.9278) acc_u 15.6250 (8.1250) lr 1.6494e-03 eta 0:00:26
epoch [57/200] batch [25/78] time 0.487 (0.458) data 0.354 (0.327) loss_u loss_u 0.9160 (0.9291) acc_u 12.5000 (8.6250) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [30/78] time 0.409 (0.449) data 0.277 (0.318) loss_u loss_u 0.9199 (0.9302) acc_u 6.2500 (8.5417) lr 1.6494e-03 eta 0:00:21
epoch [57/200] batch [35/78] time 0.474 (0.457) data 0.342 (0.326) loss_u loss_u 0.9487 (0.9340) acc_u 6.2500 (8.3036) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [40/78] time 0.553 (0.457) data 0.422 (0.326) loss_u loss_u 0.9287 (0.9365) acc_u 9.3750 (8.1250) lr 1.6494e-03 eta 0:00:17
epoch [57/200] batch [45/78] time 0.565 (0.461) data 0.433 (0.329) loss_u loss_u 0.9155 (0.9375) acc_u 9.3750 (7.9861) lr 1.6494e-03 eta 0:00:15
epoch [57/200] batch [50/78] time 0.491 (0.460) data 0.359 (0.328) loss_u loss_u 0.9219 (0.9371) acc_u 9.3750 (8.2500) lr 1.6494e-03 eta 0:00:12
epoch [57/200] batch [55/78] time 0.394 (0.457) data 0.262 (0.326) loss_u loss_u 0.9702 (0.9385) acc_u 6.2500 (8.0682) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [60/78] time 0.436 (0.454) data 0.304 (0.322) loss_u loss_u 0.9404 (0.9389) acc_u 9.3750 (8.0729) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [65/78] time 0.382 (0.452) data 0.251 (0.321) loss_u loss_u 0.9634 (0.9405) acc_u 3.1250 (7.8365) lr 1.6494e-03 eta 0:00:05
epoch [57/200] batch [70/78] time 0.386 (0.453) data 0.255 (0.322) loss_u loss_u 0.8623 (0.9374) acc_u 18.7500 (8.3482) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [75/78] time 0.459 (0.456) data 0.327 (0.325) loss_u loss_u 0.9155 (0.9382) acc_u 12.5000 (8.2500) lr 1.6494e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1689
confident_label rate tensor(0.1913, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 600
clean true:593
clean false:7
clean_rate:0.9883333333333333
noisy true:854
noisy false:1682
after delete: len(clean_dataset) 600
after delete: len(noisy_dataset) 2536
epoch [58/200] batch [5/18] time 0.384 (0.467) data 0.253 (0.337) loss_x loss_x 1.1201 (1.2018) acc_x 75.0000 (69.3750) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [10/18] time 0.425 (0.470) data 0.294 (0.340) loss_x loss_x 1.4492 (1.2748) acc_x 65.6250 (68.4375) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [15/18] time 0.375 (0.453) data 0.245 (0.322) loss_x loss_x 2.0469 (1.3086) acc_x 56.2500 (67.5000) lr 1.6374e-03 eta 0:00:01
epoch [58/200] batch [5/79] time 0.445 (0.458) data 0.314 (0.327) loss_u loss_u 0.9009 (0.9323) acc_u 15.6250 (9.3750) lr 1.6374e-03 eta 0:00:33
epoch [58/200] batch [10/79] time 0.438 (0.453) data 0.306 (0.322) loss_u loss_u 0.8838 (0.9279) acc_u 18.7500 (9.6875) lr 1.6374e-03 eta 0:00:31
epoch [58/200] batch [15/79] time 0.464 (0.454) data 0.332 (0.324) loss_u loss_u 0.9565 (0.9383) acc_u 3.1250 (8.7500) lr 1.6374e-03 eta 0:00:29
epoch [58/200] batch [20/79] time 0.497 (0.449) data 0.365 (0.318) loss_u loss_u 0.9810 (0.9451) acc_u 3.1250 (7.6562) lr 1.6374e-03 eta 0:00:26
epoch [58/200] batch [25/79] time 0.402 (0.440) data 0.271 (0.309) loss_u loss_u 0.9434 (0.9479) acc_u 6.2500 (7.3750) lr 1.6374e-03 eta 0:00:23
epoch [58/200] batch [30/79] time 0.394 (0.444) data 0.263 (0.313) loss_u loss_u 0.9219 (0.9463) acc_u 12.5000 (7.3958) lr 1.6374e-03 eta 0:00:21
epoch [58/200] batch [35/79] time 0.431 (0.445) data 0.299 (0.314) loss_u loss_u 0.9160 (0.9418) acc_u 12.5000 (8.0357) lr 1.6374e-03 eta 0:00:19
epoch [58/200] batch [40/79] time 0.338 (0.447) data 0.205 (0.315) loss_u loss_u 0.8506 (0.9377) acc_u 15.6250 (8.3594) lr 1.6374e-03 eta 0:00:17
epoch [58/200] batch [45/79] time 0.639 (0.451) data 0.505 (0.319) loss_u loss_u 0.9443 (0.9362) acc_u 9.3750 (8.4028) lr 1.6374e-03 eta 0:00:15
epoch [58/200] batch [50/79] time 0.462 (0.451) data 0.330 (0.320) loss_u loss_u 0.8896 (0.9343) acc_u 12.5000 (8.5625) lr 1.6374e-03 eta 0:00:13
epoch [58/200] batch [55/79] time 0.379 (0.449) data 0.247 (0.318) loss_u loss_u 0.8877 (0.9326) acc_u 18.7500 (8.9773) lr 1.6374e-03 eta 0:00:10
epoch [58/200] batch [60/79] time 0.394 (0.450) data 0.262 (0.319) loss_u loss_u 0.9976 (0.9347) acc_u 0.0000 (8.5417) lr 1.6374e-03 eta 0:00:08
epoch [58/200] batch [65/79] time 0.388 (0.450) data 0.257 (0.319) loss_u loss_u 0.9316 (0.9361) acc_u 9.3750 (8.4615) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [70/79] time 0.464 (0.457) data 0.333 (0.326) loss_u loss_u 0.9150 (0.9340) acc_u 12.5000 (8.8393) lr 1.6374e-03 eta 0:00:04
epoch [58/200] batch [75/79] time 0.448 (0.454) data 0.317 (0.322) loss_u loss_u 0.9561 (0.9343) acc_u 9.3750 (8.8333) lr 1.6374e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1636
confident_label rate tensor(0.2038, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 639
clean true:628
clean false:11
clean_rate:0.9827856025039123
noisy true:872
noisy false:1625
after delete: len(clean_dataset) 639
after delete: len(noisy_dataset) 2497
epoch [59/200] batch [5/19] time 0.543 (0.559) data 0.411 (0.427) loss_x loss_x 1.9043 (1.5322) acc_x 53.1250 (60.6250) lr 1.6252e-03 eta 0:00:07
epoch [59/200] batch [10/19] time 0.633 (0.578) data 0.501 (0.446) loss_x loss_x 1.5547 (1.4930) acc_x 68.7500 (62.1875) lr 1.6252e-03 eta 0:00:05
epoch [59/200] batch [15/19] time 0.491 (0.530) data 0.359 (0.398) loss_x loss_x 1.0742 (1.4609) acc_x 78.1250 (63.1250) lr 1.6252e-03 eta 0:00:02
epoch [59/200] batch [5/78] time 0.512 (0.507) data 0.382 (0.376) loss_u loss_u 0.9414 (0.9583) acc_u 6.2500 (5.6250) lr 1.6252e-03 eta 0:00:37
epoch [59/200] batch [10/78] time 0.362 (0.492) data 0.230 (0.360) loss_u loss_u 0.9692 (0.9340) acc_u 0.0000 (8.1250) lr 1.6252e-03 eta 0:00:33
epoch [59/200] batch [15/78] time 0.426 (0.489) data 0.293 (0.357) loss_u loss_u 0.8955 (0.9373) acc_u 12.5000 (7.5000) lr 1.6252e-03 eta 0:00:30
epoch [59/200] batch [20/78] time 0.473 (0.487) data 0.343 (0.355) loss_u loss_u 0.9312 (0.9357) acc_u 6.2500 (7.8125) lr 1.6252e-03 eta 0:00:28
epoch [59/200] batch [25/78] time 0.452 (0.482) data 0.321 (0.350) loss_u loss_u 0.9785 (0.9397) acc_u 0.0000 (7.2500) lr 1.6252e-03 eta 0:00:25
epoch [59/200] batch [30/78] time 0.383 (0.486) data 0.251 (0.355) loss_u loss_u 0.9341 (0.9384) acc_u 9.3750 (7.2917) lr 1.6252e-03 eta 0:00:23
epoch [59/200] batch [35/78] time 0.459 (0.482) data 0.327 (0.351) loss_u loss_u 0.8950 (0.9405) acc_u 12.5000 (7.1429) lr 1.6252e-03 eta 0:00:20
epoch [59/200] batch [40/78] time 0.425 (0.479) data 0.294 (0.347) loss_u loss_u 0.9644 (0.9431) acc_u 3.1250 (6.7969) lr 1.6252e-03 eta 0:00:18
epoch [59/200] batch [45/78] time 0.392 (0.479) data 0.259 (0.348) loss_u loss_u 0.9346 (0.9424) acc_u 6.2500 (6.8750) lr 1.6252e-03 eta 0:00:15
epoch [59/200] batch [50/78] time 0.639 (0.476) data 0.508 (0.344) loss_u loss_u 0.9414 (0.9426) acc_u 9.3750 (6.8750) lr 1.6252e-03 eta 0:00:13
epoch [59/200] batch [55/78] time 0.485 (0.475) data 0.353 (0.343) loss_u loss_u 0.8979 (0.9411) acc_u 15.6250 (7.1023) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [60/78] time 0.429 (0.475) data 0.297 (0.343) loss_u loss_u 0.8906 (0.9399) acc_u 15.6250 (7.2917) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [65/78] time 0.427 (0.473) data 0.296 (0.341) loss_u loss_u 0.9312 (0.9410) acc_u 12.5000 (7.2115) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [70/78] time 0.376 (0.471) data 0.246 (0.340) loss_u loss_u 0.9463 (0.9403) acc_u 9.3750 (7.3661) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [75/78] time 0.498 (0.469) data 0.367 (0.338) loss_u loss_u 0.9526 (0.9407) acc_u 6.2500 (7.3750) lr 1.6252e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1662
confident_label rate tensor(0.2009, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 630
clean true:624
clean false:6
clean_rate:0.9904761904761905
noisy true:850
noisy false:1656
after delete: len(clean_dataset) 630
after delete: len(noisy_dataset) 2506
epoch [60/200] batch [5/19] time 0.608 (0.469) data 0.477 (0.337) loss_x loss_x 1.5732 (1.3381) acc_x 59.3750 (66.8750) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [10/19] time 0.491 (0.467) data 0.360 (0.336) loss_x loss_x 2.0254 (1.4347) acc_x 56.2500 (64.3750) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [15/19] time 0.411 (0.487) data 0.280 (0.356) loss_x loss_x 1.1807 (1.3772) acc_x 65.6250 (66.0417) lr 1.6129e-03 eta 0:00:01
epoch [60/200] batch [5/78] time 0.497 (0.476) data 0.365 (0.345) loss_u loss_u 0.9585 (0.9389) acc_u 6.2500 (8.7500) lr 1.6129e-03 eta 0:00:34
epoch [60/200] batch [10/78] time 0.370 (0.479) data 0.235 (0.348) loss_u loss_u 0.8818 (0.9255) acc_u 12.5000 (10.0000) lr 1.6129e-03 eta 0:00:32
epoch [60/200] batch [15/78] time 0.522 (0.484) data 0.390 (0.353) loss_u loss_u 0.9380 (0.9290) acc_u 9.3750 (9.7917) lr 1.6129e-03 eta 0:00:30
epoch [60/200] batch [20/78] time 0.522 (0.485) data 0.390 (0.353) loss_u loss_u 0.9746 (0.9406) acc_u 3.1250 (7.9688) lr 1.6129e-03 eta 0:00:28
epoch [60/200] batch [25/78] time 0.615 (0.489) data 0.482 (0.356) loss_u loss_u 0.9263 (0.9349) acc_u 6.2500 (8.6250) lr 1.6129e-03 eta 0:00:25
epoch [60/200] batch [30/78] time 0.446 (0.485) data 0.314 (0.353) loss_u loss_u 0.9409 (0.9318) acc_u 6.2500 (8.9583) lr 1.6129e-03 eta 0:00:23
epoch [60/200] batch [35/78] time 0.567 (0.486) data 0.435 (0.354) loss_u loss_u 0.9526 (0.9351) acc_u 6.2500 (8.5714) lr 1.6129e-03 eta 0:00:20
epoch [60/200] batch [40/78] time 0.551 (0.487) data 0.413 (0.355) loss_u loss_u 0.9751 (0.9369) acc_u 3.1250 (8.3594) lr 1.6129e-03 eta 0:00:18
epoch [60/200] batch [45/78] time 0.540 (0.487) data 0.408 (0.355) loss_u loss_u 0.9395 (0.9365) acc_u 9.3750 (8.4722) lr 1.6129e-03 eta 0:00:16
epoch [60/200] batch [50/78] time 0.538 (0.488) data 0.405 (0.355) loss_u loss_u 0.9722 (0.9372) acc_u 3.1250 (8.3125) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [55/78] time 0.446 (0.486) data 0.312 (0.353) loss_u loss_u 0.9229 (0.9366) acc_u 12.5000 (8.4091) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [60/78] time 0.453 (0.485) data 0.319 (0.352) loss_u loss_u 0.9741 (0.9347) acc_u 6.2500 (8.6458) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [65/78] time 0.370 (0.483) data 0.236 (0.350) loss_u loss_u 0.9502 (0.9354) acc_u 12.5000 (8.6058) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [70/78] time 0.385 (0.481) data 0.253 (0.348) loss_u loss_u 0.9707 (0.9366) acc_u 3.1250 (8.4821) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [75/78] time 0.335 (0.478) data 0.205 (0.346) loss_u loss_u 0.9404 (0.9374) acc_u 9.3750 (8.4167) lr 1.6129e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1637
confident_label rate tensor(0.2015, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 632
clean true:623
clean false:9
clean_rate:0.9857594936708861
noisy true:876
noisy false:1628
after delete: len(clean_dataset) 632
after delete: len(noisy_dataset) 2504
epoch [61/200] batch [5/19] time 0.365 (0.467) data 0.234 (0.336) loss_x loss_x 1.7305 (1.3992) acc_x 56.2500 (62.5000) lr 1.6004e-03 eta 0:00:06
epoch [61/200] batch [10/19] time 0.540 (0.474) data 0.409 (0.343) loss_x loss_x 1.2617 (1.2947) acc_x 65.6250 (67.5000) lr 1.6004e-03 eta 0:00:04
epoch [61/200] batch [15/19] time 0.559 (0.472) data 0.428 (0.341) loss_x loss_x 1.4414 (1.2659) acc_x 62.5000 (68.1250) lr 1.6004e-03 eta 0:00:01
epoch [61/200] batch [5/78] time 0.532 (0.473) data 0.400 (0.342) loss_u loss_u 0.9419 (0.9377) acc_u 9.3750 (6.2500) lr 1.6004e-03 eta 0:00:34
epoch [61/200] batch [10/78] time 0.398 (0.460) data 0.266 (0.329) loss_u loss_u 0.9170 (0.9341) acc_u 6.2500 (7.8125) lr 1.6004e-03 eta 0:00:31
epoch [61/200] batch [15/78] time 0.451 (0.461) data 0.319 (0.330) loss_u loss_u 0.9268 (0.9296) acc_u 12.5000 (8.5417) lr 1.6004e-03 eta 0:00:29
epoch [61/200] batch [20/78] time 0.433 (0.458) data 0.301 (0.327) loss_u loss_u 0.9473 (0.9319) acc_u 6.2500 (8.5938) lr 1.6004e-03 eta 0:00:26
epoch [61/200] batch [25/78] time 0.409 (0.453) data 0.277 (0.322) loss_u loss_u 0.9663 (0.9337) acc_u 6.2500 (8.1250) lr 1.6004e-03 eta 0:00:24
epoch [61/200] batch [30/78] time 0.568 (0.452) data 0.437 (0.321) loss_u loss_u 0.9453 (0.9386) acc_u 9.3750 (7.7083) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [35/78] time 0.396 (0.457) data 0.264 (0.326) loss_u loss_u 0.9058 (0.9364) acc_u 12.5000 (8.1250) lr 1.6004e-03 eta 0:00:19
epoch [61/200] batch [40/78] time 0.492 (0.455) data 0.360 (0.324) loss_u loss_u 0.9453 (0.9393) acc_u 12.5000 (7.9688) lr 1.6004e-03 eta 0:00:17
epoch [61/200] batch [45/78] time 0.332 (0.452) data 0.200 (0.321) loss_u loss_u 0.9434 (0.9380) acc_u 6.2500 (8.0556) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [50/78] time 0.558 (0.456) data 0.425 (0.325) loss_u loss_u 0.9595 (0.9396) acc_u 6.2500 (7.8750) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [55/78] time 0.496 (0.458) data 0.362 (0.326) loss_u loss_u 0.9263 (0.9387) acc_u 15.6250 (8.2386) lr 1.6004e-03 eta 0:00:10
epoch [61/200] batch [60/78] time 0.468 (0.456) data 0.337 (0.324) loss_u loss_u 0.9463 (0.9392) acc_u 9.3750 (8.1250) lr 1.6004e-03 eta 0:00:08
epoch [61/200] batch [65/78] time 0.546 (0.456) data 0.414 (0.324) loss_u loss_u 0.9248 (0.9398) acc_u 6.2500 (7.9808) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [70/78] time 0.532 (0.459) data 0.400 (0.328) loss_u loss_u 0.9580 (0.9398) acc_u 6.2500 (7.9911) lr 1.6004e-03 eta 0:00:03
epoch [61/200] batch [75/78] time 0.498 (0.457) data 0.367 (0.325) loss_u loss_u 0.9434 (0.9403) acc_u 6.2500 (7.9583) lr 1.6004e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1629
confident_label rate tensor(0.1999, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 627
clean true:620
clean false:7
clean_rate:0.988835725677831
noisy true:887
noisy false:1622
after delete: len(clean_dataset) 627
after delete: len(noisy_dataset) 2509
epoch [62/200] batch [5/19] time 0.462 (0.488) data 0.331 (0.357) loss_x loss_x 0.9370 (1.2833) acc_x 62.5000 (66.2500) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [10/19] time 0.487 (0.496) data 0.356 (0.365) loss_x loss_x 1.6885 (1.3143) acc_x 75.0000 (68.1250) lr 1.5878e-03 eta 0:00:04
epoch [62/200] batch [15/19] time 0.446 (0.465) data 0.315 (0.334) loss_x loss_x 0.7573 (1.2320) acc_x 75.0000 (70.0000) lr 1.5878e-03 eta 0:00:01
epoch [62/200] batch [5/78] time 0.528 (0.454) data 0.396 (0.322) loss_u loss_u 0.9619 (0.9472) acc_u 6.2500 (8.7500) lr 1.5878e-03 eta 0:00:33
epoch [62/200] batch [10/78] time 0.351 (0.468) data 0.219 (0.337) loss_u loss_u 0.9473 (0.9383) acc_u 9.3750 (9.6875) lr 1.5878e-03 eta 0:00:31
epoch [62/200] batch [15/78] time 0.521 (0.465) data 0.390 (0.334) loss_u loss_u 0.9487 (0.9346) acc_u 6.2500 (9.7917) lr 1.5878e-03 eta 0:00:29
epoch [62/200] batch [20/78] time 0.445 (0.460) data 0.314 (0.329) loss_u loss_u 0.8735 (0.9324) acc_u 18.7500 (10.1562) lr 1.5878e-03 eta 0:00:26
epoch [62/200] batch [25/78] time 0.375 (0.454) data 0.244 (0.323) loss_u loss_u 0.9263 (0.9311) acc_u 9.3750 (10.0000) lr 1.5878e-03 eta 0:00:24
epoch [62/200] batch [30/78] time 0.446 (0.458) data 0.315 (0.326) loss_u loss_u 0.9082 (0.9290) acc_u 12.5000 (9.8958) lr 1.5878e-03 eta 0:00:21
epoch [62/200] batch [35/78] time 0.521 (0.458) data 0.389 (0.327) loss_u loss_u 0.9165 (0.9275) acc_u 9.3750 (10.0893) lr 1.5878e-03 eta 0:00:19
epoch [62/200] batch [40/78] time 0.431 (0.455) data 0.299 (0.324) loss_u loss_u 0.9785 (0.9335) acc_u 0.0000 (9.2188) lr 1.5878e-03 eta 0:00:17
epoch [62/200] batch [45/78] time 0.518 (0.459) data 0.387 (0.328) loss_u loss_u 0.9648 (0.9350) acc_u 3.1250 (9.0278) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [50/78] time 0.455 (0.462) data 0.323 (0.330) loss_u loss_u 0.9614 (0.9368) acc_u 3.1250 (8.9375) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [55/78] time 0.452 (0.461) data 0.320 (0.330) loss_u loss_u 0.9492 (0.9383) acc_u 6.2500 (8.6932) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [60/78] time 0.581 (0.463) data 0.448 (0.331) loss_u loss_u 0.9282 (0.9382) acc_u 6.2500 (8.5938) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [65/78] time 0.400 (0.463) data 0.267 (0.331) loss_u loss_u 0.9399 (0.9363) acc_u 9.3750 (8.7500) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [70/78] time 0.398 (0.462) data 0.267 (0.330) loss_u loss_u 0.9697 (0.9364) acc_u 3.1250 (8.6161) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [75/78] time 0.625 (0.463) data 0.493 (0.332) loss_u loss_u 0.9971 (0.9381) acc_u 0.0000 (8.4167) lr 1.5878e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1677
confident_label rate tensor(0.2066, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 648
clean true:635
clean false:13
clean_rate:0.9799382716049383
noisy true:824
noisy false:1664
after delete: len(clean_dataset) 648
after delete: len(noisy_dataset) 2488
epoch [63/200] batch [5/20] time 0.508 (0.518) data 0.378 (0.387) loss_x loss_x 1.3799 (1.2348) acc_x 62.5000 (67.5000) lr 1.5750e-03 eta 0:00:07
epoch [63/200] batch [10/20] time 0.457 (0.480) data 0.326 (0.349) loss_x loss_x 1.1123 (1.1697) acc_x 71.8750 (69.0625) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [15/20] time 0.786 (0.485) data 0.656 (0.354) loss_x loss_x 1.6328 (1.2172) acc_x 59.3750 (67.2917) lr 1.5750e-03 eta 0:00:02
epoch [63/200] batch [20/20] time 0.507 (0.490) data 0.377 (0.359) loss_x loss_x 1.1182 (1.1920) acc_x 62.5000 (68.1250) lr 1.5750e-03 eta 0:00:00
epoch [63/200] batch [5/77] time 0.542 (0.489) data 0.410 (0.358) loss_u loss_u 0.9180 (0.9247) acc_u 12.5000 (10.6250) lr 1.5750e-03 eta 0:00:35
epoch [63/200] batch [10/77] time 0.364 (0.480) data 0.231 (0.348) loss_u loss_u 0.9438 (0.9300) acc_u 6.2500 (9.3750) lr 1.5750e-03 eta 0:00:32
epoch [63/200] batch [15/77] time 0.513 (0.475) data 0.381 (0.344) loss_u loss_u 0.9395 (0.9387) acc_u 9.3750 (8.5417) lr 1.5750e-03 eta 0:00:29
epoch [63/200] batch [20/77] time 0.558 (0.476) data 0.425 (0.345) loss_u loss_u 0.9214 (0.9358) acc_u 6.2500 (8.4375) lr 1.5750e-03 eta 0:00:27
epoch [63/200] batch [25/77] time 0.485 (0.471) data 0.353 (0.340) loss_u loss_u 0.9565 (0.9395) acc_u 6.2500 (7.8750) lr 1.5750e-03 eta 0:00:24
epoch [63/200] batch [30/77] time 0.434 (0.468) data 0.302 (0.337) loss_u loss_u 0.9507 (0.9402) acc_u 9.3750 (8.1250) lr 1.5750e-03 eta 0:00:21
epoch [63/200] batch [35/77] time 0.456 (0.466) data 0.324 (0.335) loss_u loss_u 0.8872 (0.9370) acc_u 12.5000 (8.5714) lr 1.5750e-03 eta 0:00:19
epoch [63/200] batch [40/77] time 0.470 (0.464) data 0.340 (0.332) loss_u loss_u 0.9668 (0.9390) acc_u 6.2500 (8.3594) lr 1.5750e-03 eta 0:00:17
epoch [63/200] batch [45/77] time 0.538 (0.465) data 0.406 (0.333) loss_u loss_u 0.9971 (0.9425) acc_u 0.0000 (7.7778) lr 1.5750e-03 eta 0:00:14
epoch [63/200] batch [50/77] time 0.424 (0.461) data 0.292 (0.330) loss_u loss_u 0.9023 (0.9394) acc_u 15.6250 (8.2500) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [55/77] time 0.379 (0.462) data 0.246 (0.330) loss_u loss_u 0.9551 (0.9399) acc_u 6.2500 (8.2386) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [60/77] time 0.454 (0.459) data 0.322 (0.327) loss_u loss_u 0.8984 (0.9407) acc_u 18.7500 (8.1250) lr 1.5750e-03 eta 0:00:07
epoch [63/200] batch [65/77] time 0.506 (0.461) data 0.374 (0.330) loss_u loss_u 0.9395 (0.9399) acc_u 6.2500 (8.2692) lr 1.5750e-03 eta 0:00:05
epoch [63/200] batch [70/77] time 0.403 (0.461) data 0.271 (0.329) loss_u loss_u 0.9409 (0.9404) acc_u 9.3750 (8.1696) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [75/77] time 0.506 (0.459) data 0.375 (0.328) loss_u loss_u 0.9512 (0.9402) acc_u 9.3750 (8.1250) lr 1.5750e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1699
confident_label rate tensor(0.1967, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 617
clean true:606
clean false:11
clean_rate:0.9821717990275527
noisy true:831
noisy false:1688
after delete: len(clean_dataset) 617
after delete: len(noisy_dataset) 2519
epoch [64/200] batch [5/19] time 0.436 (0.480) data 0.305 (0.350) loss_x loss_x 0.9385 (1.2571) acc_x 81.2500 (72.5000) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [10/19] time 0.606 (0.517) data 0.475 (0.387) loss_x loss_x 1.3018 (1.2567) acc_x 75.0000 (71.5625) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [15/19] time 0.414 (0.508) data 0.284 (0.377) loss_x loss_x 2.2520 (1.3066) acc_x 50.0000 (69.5833) lr 1.5621e-03 eta 0:00:02
epoch [64/200] batch [5/78] time 0.368 (0.464) data 0.238 (0.333) loss_u loss_u 0.9395 (0.9544) acc_u 12.5000 (6.8750) lr 1.5621e-03 eta 0:00:33
epoch [64/200] batch [10/78] time 0.455 (0.464) data 0.323 (0.333) loss_u loss_u 0.9595 (0.9458) acc_u 3.1250 (6.8750) lr 1.5621e-03 eta 0:00:31
epoch [64/200] batch [15/78] time 0.685 (0.461) data 0.554 (0.330) loss_u loss_u 0.9736 (0.9505) acc_u 3.1250 (6.0417) lr 1.5621e-03 eta 0:00:29
epoch [64/200] batch [20/78] time 0.465 (0.468) data 0.333 (0.336) loss_u loss_u 0.9482 (0.9512) acc_u 6.2500 (6.0938) lr 1.5621e-03 eta 0:00:27
epoch [64/200] batch [25/78] time 0.429 (0.462) data 0.296 (0.330) loss_u loss_u 0.9478 (0.9458) acc_u 9.3750 (7.0000) lr 1.5621e-03 eta 0:00:24
epoch [64/200] batch [30/78] time 0.383 (0.457) data 0.251 (0.325) loss_u loss_u 0.9688 (0.9421) acc_u 6.2500 (7.5000) lr 1.5621e-03 eta 0:00:21
epoch [64/200] batch [35/78] time 0.436 (0.457) data 0.305 (0.326) loss_u loss_u 0.9780 (0.9444) acc_u 3.1250 (7.3214) lr 1.5621e-03 eta 0:00:19
epoch [64/200] batch [40/78] time 0.406 (0.457) data 0.274 (0.326) loss_u loss_u 0.8887 (0.9431) acc_u 12.5000 (7.2656) lr 1.5621e-03 eta 0:00:17
epoch [64/200] batch [45/78] time 0.367 (0.453) data 0.235 (0.321) loss_u loss_u 0.8872 (0.9378) acc_u 15.6250 (7.9861) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [50/78] time 0.705 (0.456) data 0.572 (0.325) loss_u loss_u 0.9414 (0.9377) acc_u 6.2500 (8.0000) lr 1.5621e-03 eta 0:00:12
epoch [64/200] batch [55/78] time 0.424 (0.455) data 0.292 (0.323) loss_u loss_u 0.9370 (0.9377) acc_u 6.2500 (8.0682) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [60/78] time 0.426 (0.454) data 0.294 (0.322) loss_u loss_u 0.9678 (0.9396) acc_u 6.2500 (7.8125) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [65/78] time 0.502 (0.452) data 0.371 (0.320) loss_u loss_u 0.9668 (0.9414) acc_u 6.2500 (7.5000) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [70/78] time 0.476 (0.454) data 0.343 (0.322) loss_u loss_u 0.9297 (0.9401) acc_u 9.3750 (7.7232) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [75/78] time 0.570 (0.455) data 0.439 (0.323) loss_u loss_u 0.9253 (0.9380) acc_u 9.3750 (8.0417) lr 1.5621e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1650
confident_label rate tensor(0.1955, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 613
clean true:604
clean false:9
clean_rate:0.9853181076672104
noisy true:882
noisy false:1641
after delete: len(clean_dataset) 613
after delete: len(noisy_dataset) 2523
epoch [65/200] batch [5/19] time 0.466 (0.421) data 0.335 (0.290) loss_x loss_x 1.5322 (1.2054) acc_x 68.7500 (68.7500) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [10/19] time 0.675 (0.465) data 0.545 (0.334) loss_x loss_x 1.1367 (1.2086) acc_x 78.1250 (70.9375) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [15/19] time 0.455 (0.467) data 0.324 (0.336) loss_x loss_x 1.5234 (1.2742) acc_x 68.7500 (70.6250) lr 1.5490e-03 eta 0:00:01
epoch [65/200] batch [5/78] time 0.395 (0.464) data 0.264 (0.333) loss_u loss_u 0.9355 (0.9402) acc_u 9.3750 (8.1250) lr 1.5490e-03 eta 0:00:33
epoch [65/200] batch [10/78] time 0.410 (0.462) data 0.279 (0.331) loss_u loss_u 0.9263 (0.9366) acc_u 9.3750 (7.8125) lr 1.5490e-03 eta 0:00:31
epoch [65/200] batch [15/78] time 0.424 (0.465) data 0.294 (0.334) loss_u loss_u 0.9717 (0.9373) acc_u 0.0000 (8.1250) lr 1.5490e-03 eta 0:00:29
epoch [65/200] batch [20/78] time 0.383 (0.454) data 0.251 (0.323) loss_u loss_u 0.9297 (0.9354) acc_u 15.6250 (8.7500) lr 1.5490e-03 eta 0:00:26
epoch [65/200] batch [25/78] time 0.438 (0.452) data 0.306 (0.321) loss_u loss_u 0.9331 (0.9347) acc_u 6.2500 (9.0000) lr 1.5490e-03 eta 0:00:23
epoch [65/200] batch [30/78] time 0.548 (0.449) data 0.416 (0.318) loss_u loss_u 0.9541 (0.9381) acc_u 6.2500 (8.3333) lr 1.5490e-03 eta 0:00:21
epoch [65/200] batch [35/78] time 0.609 (0.457) data 0.477 (0.326) loss_u loss_u 0.9727 (0.9405) acc_u 3.1250 (7.9464) lr 1.5490e-03 eta 0:00:19
epoch [65/200] batch [40/78] time 0.498 (0.454) data 0.367 (0.323) loss_u loss_u 0.9858 (0.9423) acc_u 0.0000 (7.7344) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [45/78] time 0.364 (0.452) data 0.233 (0.321) loss_u loss_u 0.9922 (0.9407) acc_u 0.0000 (7.9861) lr 1.5490e-03 eta 0:00:14
epoch [65/200] batch [50/78] time 0.462 (0.450) data 0.329 (0.319) loss_u loss_u 0.9097 (0.9396) acc_u 15.6250 (8.1875) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [55/78] time 0.407 (0.450) data 0.275 (0.318) loss_u loss_u 0.9321 (0.9380) acc_u 3.1250 (8.2386) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [60/78] time 0.422 (0.454) data 0.290 (0.322) loss_u loss_u 0.9185 (0.9387) acc_u 12.5000 (8.1771) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [65/78] time 0.476 (0.453) data 0.346 (0.321) loss_u loss_u 0.9385 (0.9391) acc_u 6.2500 (8.2212) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [70/78] time 0.402 (0.453) data 0.271 (0.321) loss_u loss_u 0.9492 (0.9386) acc_u 6.2500 (8.2589) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [75/78] time 0.346 (0.451) data 0.214 (0.320) loss_u loss_u 0.9800 (0.9386) acc_u 0.0000 (8.2083) lr 1.5490e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1663
confident_label rate tensor(0.2098, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 658
clean true:648
clean false:10
clean_rate:0.9848024316109423
noisy true:825
noisy false:1653
after delete: len(clean_dataset) 658
after delete: len(noisy_dataset) 2478
epoch [66/200] batch [5/20] time 0.445 (0.511) data 0.314 (0.379) loss_x loss_x 0.9639 (1.1106) acc_x 71.8750 (72.5000) lr 1.5358e-03 eta 0:00:07
epoch [66/200] batch [10/20] time 0.537 (0.525) data 0.405 (0.394) loss_x loss_x 1.2080 (1.1832) acc_x 65.6250 (69.3750) lr 1.5358e-03 eta 0:00:05
epoch [66/200] batch [15/20] time 0.460 (0.492) data 0.330 (0.361) loss_x loss_x 1.4570 (1.2113) acc_x 65.6250 (68.5417) lr 1.5358e-03 eta 0:00:02
epoch [66/200] batch [20/20] time 0.566 (0.486) data 0.435 (0.355) loss_x loss_x 1.3135 (1.2876) acc_x 71.8750 (67.9688) lr 1.5358e-03 eta 0:00:00
epoch [66/200] batch [5/77] time 0.337 (0.471) data 0.206 (0.340) loss_u loss_u 0.9678 (0.9416) acc_u 3.1250 (8.1250) lr 1.5358e-03 eta 0:00:33
epoch [66/200] batch [10/77] time 0.508 (0.471) data 0.376 (0.339) loss_u loss_u 0.9199 (0.9409) acc_u 12.5000 (8.1250) lr 1.5358e-03 eta 0:00:31
epoch [66/200] batch [15/77] time 0.465 (0.463) data 0.333 (0.331) loss_u loss_u 0.9448 (0.9410) acc_u 3.1250 (7.9167) lr 1.5358e-03 eta 0:00:28
epoch [66/200] batch [20/77] time 0.506 (0.467) data 0.374 (0.335) loss_u loss_u 0.9341 (0.9422) acc_u 9.3750 (7.8125) lr 1.5358e-03 eta 0:00:26
epoch [66/200] batch [25/77] time 0.489 (0.462) data 0.357 (0.331) loss_u loss_u 0.9561 (0.9388) acc_u 6.2500 (8.3750) lr 1.5358e-03 eta 0:00:24
epoch [66/200] batch [30/77] time 0.378 (0.463) data 0.246 (0.331) loss_u loss_u 0.8989 (0.9392) acc_u 12.5000 (8.1250) lr 1.5358e-03 eta 0:00:21
epoch [66/200] batch [35/77] time 0.527 (0.466) data 0.394 (0.334) loss_u loss_u 0.9629 (0.9399) acc_u 6.2500 (7.9464) lr 1.5358e-03 eta 0:00:19
epoch [66/200] batch [40/77] time 0.472 (0.460) data 0.340 (0.328) loss_u loss_u 0.9766 (0.9408) acc_u 3.1250 (7.8906) lr 1.5358e-03 eta 0:00:17
epoch [66/200] batch [45/77] time 0.470 (0.461) data 0.337 (0.329) loss_u loss_u 0.9419 (0.9418) acc_u 6.2500 (7.7083) lr 1.5358e-03 eta 0:00:14
epoch [66/200] batch [50/77] time 0.482 (0.461) data 0.351 (0.330) loss_u loss_u 0.9526 (0.9420) acc_u 6.2500 (7.6250) lr 1.5358e-03 eta 0:00:12
epoch [66/200] batch [55/77] time 0.479 (0.460) data 0.347 (0.329) loss_u loss_u 0.9771 (0.9419) acc_u 3.1250 (7.7273) lr 1.5358e-03 eta 0:00:10
epoch [66/200] batch [60/77] time 0.476 (0.461) data 0.345 (0.329) loss_u loss_u 0.9585 (0.9439) acc_u 3.1250 (7.4479) lr 1.5358e-03 eta 0:00:07
epoch [66/200] batch [65/77] time 0.529 (0.459) data 0.398 (0.327) loss_u loss_u 0.9160 (0.9429) acc_u 9.3750 (7.4038) lr 1.5358e-03 eta 0:00:05
epoch [66/200] batch [70/77] time 0.470 (0.456) data 0.339 (0.324) loss_u loss_u 0.9888 (0.9435) acc_u 0.0000 (7.2321) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [75/77] time 0.384 (0.453) data 0.253 (0.322) loss_u loss_u 0.9360 (0.9437) acc_u 9.3750 (7.1667) lr 1.5358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1622
confident_label rate tensor(0.2022, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 634
clean true:628
clean false:6
clean_rate:0.9905362776025236
noisy true:886
noisy false:1616
after delete: len(clean_dataset) 634
after delete: len(noisy_dataset) 2502
epoch [67/200] batch [5/19] time 0.403 (0.549) data 0.272 (0.418) loss_x loss_x 1.4658 (1.3055) acc_x 65.6250 (68.1250) lr 1.5225e-03 eta 0:00:07
epoch [67/200] batch [10/19] time 0.428 (0.511) data 0.297 (0.380) loss_x loss_x 1.4766 (1.2668) acc_x 59.3750 (69.0625) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [15/19] time 0.448 (0.498) data 0.317 (0.367) loss_x loss_x 1.6211 (1.3556) acc_x 53.1250 (65.6250) lr 1.5225e-03 eta 0:00:01
epoch [67/200] batch [5/78] time 0.488 (0.477) data 0.357 (0.346) loss_u loss_u 0.9604 (0.9631) acc_u 6.2500 (5.0000) lr 1.5225e-03 eta 0:00:34
epoch [67/200] batch [10/78] time 0.368 (0.467) data 0.236 (0.336) loss_u loss_u 0.9912 (0.9337) acc_u 0.0000 (9.0625) lr 1.5225e-03 eta 0:00:31
epoch [67/200] batch [15/78] time 0.449 (0.460) data 0.317 (0.328) loss_u loss_u 0.9697 (0.9319) acc_u 3.1250 (9.1667) lr 1.5225e-03 eta 0:00:28
epoch [67/200] batch [20/78] time 0.358 (0.466) data 0.227 (0.334) loss_u loss_u 0.9492 (0.9371) acc_u 3.1250 (8.2812) lr 1.5225e-03 eta 0:00:27
epoch [67/200] batch [25/78] time 0.426 (0.462) data 0.294 (0.331) loss_u loss_u 0.9067 (0.9344) acc_u 9.3750 (8.7500) lr 1.5225e-03 eta 0:00:24
epoch [67/200] batch [30/78] time 0.378 (0.457) data 0.246 (0.326) loss_u loss_u 0.9795 (0.9386) acc_u 3.1250 (8.4375) lr 1.5225e-03 eta 0:00:21
epoch [67/200] batch [35/78] time 0.422 (0.453) data 0.290 (0.322) loss_u loss_u 0.9434 (0.9395) acc_u 12.5000 (8.4821) lr 1.5225e-03 eta 0:00:19
epoch [67/200] batch [40/78] time 0.320 (0.456) data 0.189 (0.325) loss_u loss_u 0.9702 (0.9396) acc_u 3.1250 (8.2812) lr 1.5225e-03 eta 0:00:17
epoch [67/200] batch [45/78] time 0.369 (0.458) data 0.238 (0.327) loss_u loss_u 0.9946 (0.9419) acc_u 0.0000 (8.0556) lr 1.5225e-03 eta 0:00:15
epoch [67/200] batch [50/78] time 0.476 (0.457) data 0.344 (0.326) loss_u loss_u 0.9536 (0.9434) acc_u 12.5000 (7.9375) lr 1.5225e-03 eta 0:00:12
epoch [67/200] batch [55/78] time 0.395 (0.458) data 0.262 (0.326) loss_u loss_u 0.9429 (0.9430) acc_u 9.3750 (7.9545) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [60/78] time 0.380 (0.456) data 0.248 (0.325) loss_u loss_u 0.9590 (0.9420) acc_u 6.2500 (8.1250) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [65/78] time 0.389 (0.453) data 0.257 (0.322) loss_u loss_u 0.9634 (0.9422) acc_u 6.2500 (8.1250) lr 1.5225e-03 eta 0:00:05
epoch [67/200] batch [70/78] time 0.344 (0.451) data 0.213 (0.319) loss_u loss_u 0.9561 (0.9435) acc_u 3.1250 (7.8125) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [75/78] time 0.463 (0.452) data 0.331 (0.321) loss_u loss_u 0.9937 (0.9436) acc_u 0.0000 (7.7500) lr 1.5225e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1592
confident_label rate tensor(0.2181, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 684
clean true:673
clean false:11
clean_rate:0.9839181286549707
noisy true:871
noisy false:1581
after delete: len(clean_dataset) 684
after delete: len(noisy_dataset) 2452
epoch [68/200] batch [5/21] time 0.383 (0.445) data 0.252 (0.314) loss_x loss_x 0.8882 (1.2899) acc_x 90.6250 (71.2500) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [10/21] time 0.626 (0.467) data 0.495 (0.336) loss_x loss_x 1.0195 (1.2955) acc_x 75.0000 (68.4375) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [15/21] time 0.493 (0.459) data 0.362 (0.328) loss_x loss_x 1.4141 (1.3186) acc_x 53.1250 (67.0833) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [20/21] time 0.489 (0.450) data 0.358 (0.319) loss_x loss_x 1.1055 (1.3361) acc_x 71.8750 (67.6562) lr 1.5090e-03 eta 0:00:00
epoch [68/200] batch [5/76] time 0.402 (0.446) data 0.270 (0.315) loss_u loss_u 0.9634 (0.9620) acc_u 6.2500 (5.6250) lr 1.5090e-03 eta 0:00:31
epoch [68/200] batch [10/76] time 0.397 (0.444) data 0.265 (0.313) loss_u loss_u 0.8945 (0.9516) acc_u 9.3750 (6.5625) lr 1.5090e-03 eta 0:00:29
epoch [68/200] batch [15/76] time 0.418 (0.446) data 0.284 (0.314) loss_u loss_u 0.9155 (0.9507) acc_u 9.3750 (6.8750) lr 1.5090e-03 eta 0:00:27
epoch [68/200] batch [20/76] time 0.453 (0.446) data 0.322 (0.315) loss_u loss_u 0.9097 (0.9472) acc_u 9.3750 (7.0312) lr 1.5090e-03 eta 0:00:24
epoch [68/200] batch [25/76] time 0.434 (0.440) data 0.302 (0.309) loss_u loss_u 0.9844 (0.9505) acc_u 0.0000 (6.2500) lr 1.5090e-03 eta 0:00:22
epoch [68/200] batch [30/76] time 0.439 (0.435) data 0.308 (0.304) loss_u loss_u 0.8672 (0.9443) acc_u 15.6250 (6.9792) lr 1.5090e-03 eta 0:00:20
epoch [68/200] batch [35/76] time 0.429 (0.440) data 0.298 (0.309) loss_u loss_u 0.9585 (0.9447) acc_u 3.1250 (6.7857) lr 1.5090e-03 eta 0:00:18
epoch [68/200] batch [40/76] time 0.392 (0.442) data 0.260 (0.311) loss_u loss_u 0.9487 (0.9434) acc_u 6.2500 (7.1094) lr 1.5090e-03 eta 0:00:15
epoch [68/200] batch [45/76] time 0.436 (0.450) data 0.304 (0.319) loss_u loss_u 0.9360 (0.9409) acc_u 6.2500 (7.4306) lr 1.5090e-03 eta 0:00:13
epoch [68/200] batch [50/76] time 0.515 (0.456) data 0.384 (0.324) loss_u loss_u 0.9604 (0.9422) acc_u 9.3750 (7.5000) lr 1.5090e-03 eta 0:00:11
epoch [68/200] batch [55/76] time 0.428 (0.453) data 0.296 (0.321) loss_u loss_u 0.9585 (0.9439) acc_u 6.2500 (7.1591) lr 1.5090e-03 eta 0:00:09
epoch [68/200] batch [60/76] time 0.386 (0.451) data 0.254 (0.319) loss_u loss_u 0.9365 (0.9435) acc_u 12.5000 (7.4479) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [65/76] time 0.441 (0.450) data 0.309 (0.318) loss_u loss_u 0.9893 (0.9456) acc_u 0.0000 (7.2115) lr 1.5090e-03 eta 0:00:04
epoch [68/200] batch [70/76] time 0.453 (0.450) data 0.321 (0.319) loss_u loss_u 0.9150 (0.9438) acc_u 12.5000 (7.5446) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [75/76] time 0.418 (0.449) data 0.287 (0.317) loss_u loss_u 0.9370 (0.9431) acc_u 9.3750 (7.6667) lr 1.5090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1623
confident_label rate tensor(0.2060, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 646
clean true:638
clean false:8
clean_rate:0.9876160990712074
noisy true:875
noisy false:1615
after delete: len(clean_dataset) 646
after delete: len(noisy_dataset) 2490
epoch [69/200] batch [5/20] time 0.392 (0.423) data 0.262 (0.292) loss_x loss_x 1.1162 (1.3531) acc_x 71.8750 (64.3750) lr 1.4955e-03 eta 0:00:06
epoch [69/200] batch [10/20] time 0.457 (0.447) data 0.326 (0.316) loss_x loss_x 0.8369 (1.2411) acc_x 75.0000 (66.5625) lr 1.4955e-03 eta 0:00:04
epoch [69/200] batch [15/20] time 0.549 (0.456) data 0.418 (0.325) loss_x loss_x 0.8320 (1.1971) acc_x 84.3750 (69.3750) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [20/20] time 0.451 (0.456) data 0.320 (0.325) loss_x loss_x 0.9761 (1.2356) acc_x 81.2500 (68.7500) lr 1.4955e-03 eta 0:00:00
epoch [69/200] batch [5/77] time 0.577 (0.472) data 0.444 (0.341) loss_u loss_u 0.9263 (0.9312) acc_u 9.3750 (8.7500) lr 1.4955e-03 eta 0:00:33
epoch [69/200] batch [10/77] time 0.495 (0.470) data 0.364 (0.339) loss_u loss_u 0.8984 (0.9228) acc_u 12.5000 (9.3750) lr 1.4955e-03 eta 0:00:31
epoch [69/200] batch [15/77] time 0.435 (0.463) data 0.304 (0.332) loss_u loss_u 0.9409 (0.9309) acc_u 6.2500 (8.5417) lr 1.4955e-03 eta 0:00:28
epoch [69/200] batch [20/77] time 0.517 (0.458) data 0.385 (0.327) loss_u loss_u 0.9731 (0.9328) acc_u 3.1250 (8.1250) lr 1.4955e-03 eta 0:00:26
epoch [69/200] batch [25/77] time 0.426 (0.460) data 0.295 (0.329) loss_u loss_u 0.9707 (0.9345) acc_u 3.1250 (8.1250) lr 1.4955e-03 eta 0:00:23
epoch [69/200] batch [30/77] time 0.399 (0.458) data 0.267 (0.327) loss_u loss_u 0.9678 (0.9379) acc_u 3.1250 (7.6042) lr 1.4955e-03 eta 0:00:21
epoch [69/200] batch [35/77] time 0.430 (0.455) data 0.299 (0.324) loss_u loss_u 0.9473 (0.9361) acc_u 9.3750 (7.8571) lr 1.4955e-03 eta 0:00:19
epoch [69/200] batch [40/77] time 0.452 (0.456) data 0.320 (0.324) loss_u loss_u 0.8330 (0.9353) acc_u 18.7500 (7.7344) lr 1.4955e-03 eta 0:00:16
epoch [69/200] batch [45/77] time 0.342 (0.451) data 0.209 (0.320) loss_u loss_u 0.9175 (0.9362) acc_u 18.7500 (7.9167) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [50/77] time 0.458 (0.451) data 0.326 (0.320) loss_u loss_u 0.9531 (0.9359) acc_u 12.5000 (8.3125) lr 1.4955e-03 eta 0:00:12
epoch [69/200] batch [55/77] time 0.413 (0.450) data 0.282 (0.319) loss_u loss_u 0.9453 (0.9357) acc_u 6.2500 (8.2955) lr 1.4955e-03 eta 0:00:09
epoch [69/200] batch [60/77] time 0.316 (0.449) data 0.184 (0.318) loss_u loss_u 0.8613 (0.9350) acc_u 18.7500 (8.4375) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [65/77] time 0.462 (0.449) data 0.330 (0.317) loss_u loss_u 0.9043 (0.9359) acc_u 12.5000 (8.2212) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [70/77] time 0.532 (0.454) data 0.401 (0.322) loss_u loss_u 0.9502 (0.9364) acc_u 9.3750 (8.2589) lr 1.4955e-03 eta 0:00:03
epoch [69/200] batch [75/77] time 0.420 (0.452) data 0.289 (0.321) loss_u loss_u 0.9424 (0.9368) acc_u 6.2500 (8.2917) lr 1.4955e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1614
confident_label rate tensor(0.2038, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 639
clean true:629
clean false:10
clean_rate:0.9843505477308294
noisy true:893
noisy false:1604
after delete: len(clean_dataset) 639
after delete: len(noisy_dataset) 2497
epoch [70/200] batch [5/19] time 0.420 (0.439) data 0.290 (0.308) loss_x loss_x 1.7842 (1.4488) acc_x 65.6250 (67.5000) lr 1.4818e-03 eta 0:00:06
epoch [70/200] batch [10/19] time 0.383 (0.459) data 0.252 (0.328) loss_x loss_x 1.1885 (1.3339) acc_x 62.5000 (67.1875) lr 1.4818e-03 eta 0:00:04
epoch [70/200] batch [15/19] time 0.427 (0.461) data 0.296 (0.330) loss_x loss_x 1.2871 (1.2789) acc_x 65.6250 (67.2917) lr 1.4818e-03 eta 0:00:01
epoch [70/200] batch [5/78] time 0.471 (0.476) data 0.339 (0.345) loss_u loss_u 0.9414 (0.9367) acc_u 9.3750 (8.1250) lr 1.4818e-03 eta 0:00:34
epoch [70/200] batch [10/78] time 0.399 (0.473) data 0.267 (0.342) loss_u loss_u 0.9590 (0.9491) acc_u 3.1250 (7.1875) lr 1.4818e-03 eta 0:00:32
epoch [70/200] batch [15/78] time 0.506 (0.472) data 0.375 (0.341) loss_u loss_u 0.9507 (0.9478) acc_u 9.3750 (7.2917) lr 1.4818e-03 eta 0:00:29
epoch [70/200] batch [20/78] time 0.385 (0.463) data 0.254 (0.332) loss_u loss_u 0.9321 (0.9423) acc_u 9.3750 (7.9688) lr 1.4818e-03 eta 0:00:26
epoch [70/200] batch [25/78] time 0.467 (0.461) data 0.335 (0.330) loss_u loss_u 0.8984 (0.9403) acc_u 15.6250 (8.2500) lr 1.4818e-03 eta 0:00:24
epoch [70/200] batch [30/78] time 0.551 (0.458) data 0.420 (0.326) loss_u loss_u 0.9653 (0.9445) acc_u 3.1250 (7.7083) lr 1.4818e-03 eta 0:00:21
epoch [70/200] batch [35/78] time 0.488 (0.456) data 0.358 (0.325) loss_u loss_u 0.9878 (0.9444) acc_u 3.1250 (7.6786) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [40/78] time 0.444 (0.453) data 0.312 (0.322) loss_u loss_u 0.8979 (0.9401) acc_u 12.5000 (7.9688) lr 1.4818e-03 eta 0:00:17
epoch [70/200] batch [45/78] time 0.429 (0.452) data 0.298 (0.320) loss_u loss_u 0.9849 (0.9392) acc_u 0.0000 (8.1944) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [50/78] time 0.412 (0.450) data 0.280 (0.319) loss_u loss_u 0.9248 (0.9388) acc_u 9.3750 (8.2500) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [55/78] time 0.534 (0.452) data 0.402 (0.321) loss_u loss_u 0.9385 (0.9387) acc_u 6.2500 (8.1818) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [60/78] time 0.403 (0.452) data 0.270 (0.320) loss_u loss_u 0.9819 (0.9382) acc_u 0.0000 (8.3333) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [65/78] time 0.397 (0.452) data 0.266 (0.321) loss_u loss_u 0.8809 (0.9368) acc_u 18.7500 (8.5577) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [70/78] time 0.411 (0.452) data 0.279 (0.321) loss_u loss_u 0.9937 (0.9375) acc_u 0.0000 (8.4375) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [75/78] time 0.440 (0.452) data 0.309 (0.320) loss_u loss_u 0.9429 (0.9372) acc_u 6.2500 (8.4583) lr 1.4818e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1583
confident_label rate tensor(0.2079, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 652
clean true:646
clean false:6
clean_rate:0.99079754601227
noisy true:907
noisy false:1577
after delete: len(clean_dataset) 652
after delete: len(noisy_dataset) 2484
epoch [71/200] batch [5/20] time 0.444 (0.441) data 0.313 (0.310) loss_x loss_x 1.2939 (1.1697) acc_x 78.1250 (75.0000) lr 1.4679e-03 eta 0:00:06
epoch [71/200] batch [10/20] time 0.461 (0.432) data 0.331 (0.301) loss_x loss_x 1.3838 (1.2398) acc_x 68.7500 (72.5000) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [15/20] time 0.789 (0.488) data 0.657 (0.357) loss_x loss_x 0.9917 (1.1950) acc_x 75.0000 (72.9167) lr 1.4679e-03 eta 0:00:02
epoch [71/200] batch [20/20] time 0.458 (0.474) data 0.327 (0.344) loss_x loss_x 0.8247 (1.1745) acc_x 71.8750 (72.0312) lr 1.4679e-03 eta 0:00:00
epoch [71/200] batch [5/77] time 0.419 (0.474) data 0.287 (0.343) loss_u loss_u 0.9229 (0.9328) acc_u 9.3750 (9.3750) lr 1.4679e-03 eta 0:00:34
epoch [71/200] batch [10/77] time 0.337 (0.467) data 0.206 (0.336) loss_u loss_u 0.9727 (0.9503) acc_u 6.2500 (6.8750) lr 1.4679e-03 eta 0:00:31
epoch [71/200] batch [15/77] time 0.474 (0.466) data 0.344 (0.335) loss_u loss_u 0.9561 (0.9487) acc_u 3.1250 (6.8750) lr 1.4679e-03 eta 0:00:28
epoch [71/200] batch [20/77] time 0.435 (0.463) data 0.304 (0.332) loss_u loss_u 0.9902 (0.9459) acc_u 0.0000 (7.1875) lr 1.4679e-03 eta 0:00:26
epoch [71/200] batch [25/77] time 0.422 (0.459) data 0.290 (0.328) loss_u loss_u 0.9946 (0.9448) acc_u 0.0000 (7.2500) lr 1.4679e-03 eta 0:00:23
epoch [71/200] batch [30/77] time 0.446 (0.454) data 0.314 (0.322) loss_u loss_u 0.9600 (0.9484) acc_u 6.2500 (6.6667) lr 1.4679e-03 eta 0:00:21
epoch [71/200] batch [35/77] time 0.400 (0.453) data 0.269 (0.322) loss_u loss_u 0.9189 (0.9451) acc_u 9.3750 (7.2321) lr 1.4679e-03 eta 0:00:19
epoch [71/200] batch [40/77] time 0.505 (0.454) data 0.373 (0.323) loss_u loss_u 0.9878 (0.9470) acc_u 3.1250 (7.1094) lr 1.4679e-03 eta 0:00:16
epoch [71/200] batch [45/77] time 0.438 (0.453) data 0.307 (0.322) loss_u loss_u 0.9712 (0.9494) acc_u 0.0000 (6.6667) lr 1.4679e-03 eta 0:00:14
epoch [71/200] batch [50/77] time 0.421 (0.453) data 0.289 (0.322) loss_u loss_u 0.9385 (0.9457) acc_u 9.3750 (7.2500) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [55/77] time 0.545 (0.459) data 0.413 (0.327) loss_u loss_u 0.9917 (0.9464) acc_u 0.0000 (7.0455) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [60/77] time 0.527 (0.464) data 0.394 (0.333) loss_u loss_u 0.8887 (0.9443) acc_u 12.5000 (7.2917) lr 1.4679e-03 eta 0:00:07
epoch [71/200] batch [65/77] time 0.512 (0.465) data 0.379 (0.333) loss_u loss_u 0.9326 (0.9427) acc_u 6.2500 (7.6442) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [70/77] time 0.354 (0.465) data 0.222 (0.333) loss_u loss_u 0.9590 (0.9432) acc_u 6.2500 (7.4554) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [75/77] time 0.458 (0.464) data 0.325 (0.332) loss_u loss_u 0.9448 (0.9429) acc_u 9.3750 (7.4583) lr 1.4679e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1601
confident_label rate tensor(0.2140, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 671
clean true:660
clean false:11
clean_rate:0.9836065573770492
noisy true:875
noisy false:1590
after delete: len(clean_dataset) 671
after delete: len(noisy_dataset) 2465
epoch [72/200] batch [5/20] time 0.458 (0.438) data 0.328 (0.307) loss_x loss_x 1.2002 (1.1710) acc_x 71.8750 (70.6250) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [10/20] time 0.440 (0.477) data 0.309 (0.346) loss_x loss_x 1.4951 (1.2538) acc_x 59.3750 (69.0625) lr 1.4540e-03 eta 0:00:04
epoch [72/200] batch [15/20] time 0.718 (0.506) data 0.587 (0.375) loss_x loss_x 1.6826 (1.2436) acc_x 62.5000 (70.0000) lr 1.4540e-03 eta 0:00:02
epoch [72/200] batch [20/20] time 0.370 (0.497) data 0.240 (0.366) loss_x loss_x 1.2266 (1.2573) acc_x 68.7500 (69.0625) lr 1.4540e-03 eta 0:00:00
epoch [72/200] batch [5/77] time 0.415 (0.490) data 0.284 (0.359) loss_u loss_u 0.9165 (0.9147) acc_u 9.3750 (10.0000) lr 1.4540e-03 eta 0:00:35
epoch [72/200] batch [10/77] time 0.423 (0.482) data 0.291 (0.351) loss_u loss_u 0.9624 (0.9241) acc_u 0.0000 (8.4375) lr 1.4540e-03 eta 0:00:32
epoch [72/200] batch [15/77] time 0.395 (0.470) data 0.263 (0.339) loss_u loss_u 0.9663 (0.9365) acc_u 3.1250 (6.8750) lr 1.4540e-03 eta 0:00:29
epoch [72/200] batch [20/77] time 0.421 (0.467) data 0.289 (0.336) loss_u loss_u 0.9365 (0.9402) acc_u 6.2500 (6.5625) lr 1.4540e-03 eta 0:00:26
epoch [72/200] batch [25/77] time 0.481 (0.466) data 0.349 (0.335) loss_u loss_u 0.9658 (0.9403) acc_u 6.2500 (6.8750) lr 1.4540e-03 eta 0:00:24
epoch [72/200] batch [30/77] time 0.477 (0.464) data 0.346 (0.333) loss_u loss_u 0.9595 (0.9409) acc_u 6.2500 (7.0833) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [35/77] time 0.374 (0.464) data 0.240 (0.332) loss_u loss_u 0.9092 (0.9400) acc_u 9.3750 (7.1429) lr 1.4540e-03 eta 0:00:19
epoch [72/200] batch [40/77] time 0.377 (0.463) data 0.246 (0.331) loss_u loss_u 0.8135 (0.9368) acc_u 21.8750 (7.5781) lr 1.4540e-03 eta 0:00:17
epoch [72/200] batch [45/77] time 0.419 (0.462) data 0.288 (0.330) loss_u loss_u 0.9751 (0.9375) acc_u 3.1250 (7.5694) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [50/77] time 0.479 (0.462) data 0.347 (0.331) loss_u loss_u 0.9482 (0.9385) acc_u 6.2500 (7.5000) lr 1.4540e-03 eta 0:00:12
epoch [72/200] batch [55/77] time 0.395 (0.463) data 0.263 (0.331) loss_u loss_u 0.9155 (0.9373) acc_u 9.3750 (7.7841) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [60/77] time 0.407 (0.460) data 0.276 (0.329) loss_u loss_u 0.8989 (0.9368) acc_u 15.6250 (7.9167) lr 1.4540e-03 eta 0:00:07
epoch [72/200] batch [65/77] time 0.425 (0.459) data 0.293 (0.327) loss_u loss_u 0.9150 (0.9384) acc_u 12.5000 (7.7885) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [70/77] time 0.512 (0.461) data 0.378 (0.330) loss_u loss_u 0.9502 (0.9405) acc_u 6.2500 (7.5446) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [75/77] time 0.413 (0.459) data 0.280 (0.327) loss_u loss_u 0.9458 (0.9383) acc_u 9.3750 (7.7917) lr 1.4540e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1598
confident_label rate tensor(0.2121, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 665
clean true:652
clean false:13
clean_rate:0.9804511278195489
noisy true:886
noisy false:1585
after delete: len(clean_dataset) 665
after delete: len(noisy_dataset) 2471
epoch [73/200] batch [5/20] time 0.396 (0.405) data 0.265 (0.273) loss_x loss_x 1.4678 (1.4496) acc_x 65.6250 (66.8750) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [10/20] time 0.621 (0.433) data 0.488 (0.301) loss_x loss_x 1.3789 (1.3778) acc_x 62.5000 (64.6875) lr 1.4399e-03 eta 0:00:04
epoch [73/200] batch [15/20] time 0.371 (0.451) data 0.240 (0.319) loss_x loss_x 1.3789 (1.3142) acc_x 65.6250 (67.7083) lr 1.4399e-03 eta 0:00:02
epoch [73/200] batch [20/20] time 0.621 (0.471) data 0.490 (0.339) loss_x loss_x 0.9136 (1.3194) acc_x 68.7500 (67.8125) lr 1.4399e-03 eta 0:00:00
epoch [73/200] batch [5/77] time 0.423 (0.471) data 0.292 (0.339) loss_u loss_u 0.9365 (0.9228) acc_u 6.2500 (10.6250) lr 1.4399e-03 eta 0:00:33
epoch [73/200] batch [10/77] time 0.429 (0.462) data 0.298 (0.331) loss_u loss_u 0.9307 (0.9207) acc_u 9.3750 (11.2500) lr 1.4399e-03 eta 0:00:30
epoch [73/200] batch [15/77] time 0.454 (0.465) data 0.322 (0.333) loss_u loss_u 0.9019 (0.9247) acc_u 9.3750 (10.0000) lr 1.4399e-03 eta 0:00:28
epoch [73/200] batch [20/77] time 0.429 (0.460) data 0.297 (0.328) loss_u loss_u 0.9346 (0.9349) acc_u 9.3750 (8.4375) lr 1.4399e-03 eta 0:00:26
epoch [73/200] batch [25/77] time 0.500 (0.462) data 0.368 (0.331) loss_u loss_u 0.9092 (0.9346) acc_u 15.6250 (8.7500) lr 1.4399e-03 eta 0:00:24
epoch [73/200] batch [30/77] time 0.487 (0.459) data 0.356 (0.327) loss_u loss_u 0.9302 (0.9314) acc_u 9.3750 (9.1667) lr 1.4399e-03 eta 0:00:21
epoch [73/200] batch [35/77] time 0.368 (0.454) data 0.237 (0.322) loss_u loss_u 0.9468 (0.9319) acc_u 9.3750 (9.1071) lr 1.4399e-03 eta 0:00:19
epoch [73/200] batch [40/77] time 0.464 (0.450) data 0.333 (0.318) loss_u loss_u 0.8589 (0.9333) acc_u 18.7500 (8.9062) lr 1.4399e-03 eta 0:00:16
epoch [73/200] batch [45/77] time 0.451 (0.448) data 0.321 (0.316) loss_u loss_u 0.9385 (0.9305) acc_u 9.3750 (9.4444) lr 1.4399e-03 eta 0:00:14
epoch [73/200] batch [50/77] time 0.509 (0.450) data 0.377 (0.318) loss_u loss_u 0.9824 (0.9332) acc_u 0.0000 (9.0625) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [55/77] time 0.427 (0.452) data 0.296 (0.321) loss_u loss_u 0.9678 (0.9320) acc_u 6.2500 (9.0341) lr 1.4399e-03 eta 0:00:09
epoch [73/200] batch [60/77] time 0.438 (0.451) data 0.306 (0.320) loss_u loss_u 0.9868 (0.9344) acc_u 3.1250 (8.8021) lr 1.4399e-03 eta 0:00:07
epoch [73/200] batch [65/77] time 0.476 (0.453) data 0.344 (0.321) loss_u loss_u 0.9351 (0.9340) acc_u 6.2500 (8.7981) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [70/77] time 0.554 (0.457) data 0.422 (0.325) loss_u loss_u 0.9688 (0.9350) acc_u 3.1250 (8.5714) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [75/77] time 0.550 (0.457) data 0.417 (0.326) loss_u loss_u 0.9824 (0.9360) acc_u 3.1250 (8.4167) lr 1.4399e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1650
confident_label rate tensor(0.2050, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 643
clean true:630
clean false:13
clean_rate:0.9797822706065319
noisy true:856
noisy false:1637
after delete: len(clean_dataset) 643
after delete: len(noisy_dataset) 2493
epoch [74/200] batch [5/20] time 0.430 (0.457) data 0.299 (0.326) loss_x loss_x 1.3623 (1.1742) acc_x 75.0000 (71.8750) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [10/20] time 0.650 (0.499) data 0.519 (0.368) loss_x loss_x 1.2373 (1.2066) acc_x 68.7500 (70.9375) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [15/20] time 0.550 (0.483) data 0.418 (0.352) loss_x loss_x 1.4912 (1.3149) acc_x 65.6250 (67.7083) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [20/20] time 0.415 (0.468) data 0.285 (0.337) loss_x loss_x 1.3320 (1.3218) acc_x 71.8750 (67.3438) lr 1.4258e-03 eta 0:00:00
epoch [74/200] batch [5/77] time 0.451 (0.469) data 0.321 (0.338) loss_u loss_u 0.9326 (0.9312) acc_u 6.2500 (8.7500) lr 1.4258e-03 eta 0:00:33
epoch [74/200] batch [10/77] time 0.440 (0.469) data 0.308 (0.338) loss_u loss_u 0.9517 (0.9408) acc_u 6.2500 (6.5625) lr 1.4258e-03 eta 0:00:31
epoch [74/200] batch [15/77] time 0.444 (0.469) data 0.312 (0.338) loss_u loss_u 0.9277 (0.9334) acc_u 9.3750 (8.1250) lr 1.4258e-03 eta 0:00:29
epoch [74/200] batch [20/77] time 0.422 (0.468) data 0.292 (0.337) loss_u loss_u 0.9258 (0.9334) acc_u 9.3750 (8.2812) lr 1.4258e-03 eta 0:00:26
epoch [74/200] batch [25/77] time 0.428 (0.467) data 0.297 (0.336) loss_u loss_u 0.9678 (0.9355) acc_u 6.2500 (8.2500) lr 1.4258e-03 eta 0:00:24
epoch [74/200] batch [30/77] time 0.512 (0.468) data 0.381 (0.336) loss_u loss_u 0.9810 (0.9357) acc_u 3.1250 (8.2292) lr 1.4258e-03 eta 0:00:21
epoch [74/200] batch [35/77] time 0.344 (0.462) data 0.211 (0.330) loss_u loss_u 0.8823 (0.9367) acc_u 12.5000 (7.9464) lr 1.4258e-03 eta 0:00:19
epoch [74/200] batch [40/77] time 0.555 (0.466) data 0.424 (0.334) loss_u loss_u 0.9253 (0.9366) acc_u 12.5000 (7.9688) lr 1.4258e-03 eta 0:00:17
epoch [74/200] batch [45/77] time 0.392 (0.463) data 0.261 (0.331) loss_u loss_u 0.9663 (0.9382) acc_u 3.1250 (7.9167) lr 1.4258e-03 eta 0:00:14
epoch [74/200] batch [50/77] time 0.361 (0.462) data 0.230 (0.331) loss_u loss_u 0.9307 (0.9378) acc_u 9.3750 (7.8750) lr 1.4258e-03 eta 0:00:12
epoch [74/200] batch [55/77] time 0.420 (0.459) data 0.288 (0.328) loss_u loss_u 0.8926 (0.9382) acc_u 15.6250 (7.7841) lr 1.4258e-03 eta 0:00:10
epoch [74/200] batch [60/77] time 0.394 (0.455) data 0.263 (0.324) loss_u loss_u 0.8535 (0.9364) acc_u 25.0000 (8.1250) lr 1.4258e-03 eta 0:00:07
epoch [74/200] batch [65/77] time 0.449 (0.452) data 0.318 (0.321) loss_u loss_u 0.9644 (0.9382) acc_u 3.1250 (7.9327) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [70/77] time 0.399 (0.452) data 0.268 (0.320) loss_u loss_u 0.9043 (0.9375) acc_u 12.5000 (8.0804) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [75/77] time 0.386 (0.451) data 0.254 (0.319) loss_u loss_u 0.8813 (0.9377) acc_u 18.7500 (8.0833) lr 1.4258e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1599
confident_label rate tensor(0.2117, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 664
clean true:657
clean false:7
clean_rate:0.9894578313253012
noisy true:880
noisy false:1592
after delete: len(clean_dataset) 664
after delete: len(noisy_dataset) 2472
epoch [75/200] batch [5/20] time 0.320 (0.438) data 0.189 (0.307) loss_x loss_x 1.8760 (1.4405) acc_x 50.0000 (63.7500) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [10/20] time 0.475 (0.456) data 0.344 (0.325) loss_x loss_x 1.2031 (1.3848) acc_x 68.7500 (67.8125) lr 1.4115e-03 eta 0:00:04
epoch [75/200] batch [15/20] time 1.075 (0.520) data 0.944 (0.389) loss_x loss_x 1.3291 (1.4130) acc_x 71.8750 (68.3333) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [20/20] time 0.383 (0.493) data 0.253 (0.362) loss_x loss_x 1.4629 (1.3973) acc_x 65.6250 (69.5312) lr 1.4115e-03 eta 0:00:00
epoch [75/200] batch [5/77] time 0.396 (0.487) data 0.265 (0.356) loss_u loss_u 0.9395 (0.9265) acc_u 9.3750 (10.6250) lr 1.4115e-03 eta 0:00:35
epoch [75/200] batch [10/77] time 0.458 (0.470) data 0.327 (0.339) loss_u loss_u 0.9067 (0.9264) acc_u 9.3750 (10.0000) lr 1.4115e-03 eta 0:00:31
epoch [75/200] batch [15/77] time 0.392 (0.465) data 0.260 (0.334) loss_u loss_u 0.9736 (0.9219) acc_u 3.1250 (11.0417) lr 1.4115e-03 eta 0:00:28
epoch [75/200] batch [20/77] time 0.465 (0.467) data 0.333 (0.336) loss_u loss_u 0.9419 (0.9273) acc_u 6.2500 (9.8438) lr 1.4115e-03 eta 0:00:26
epoch [75/200] batch [25/77] time 0.432 (0.466) data 0.300 (0.335) loss_u loss_u 0.8833 (0.9272) acc_u 15.6250 (9.6250) lr 1.4115e-03 eta 0:00:24
epoch [75/200] batch [30/77] time 0.362 (0.464) data 0.231 (0.332) loss_u loss_u 0.9722 (0.9301) acc_u 3.1250 (9.0625) lr 1.4115e-03 eta 0:00:21
epoch [75/200] batch [35/77] time 0.292 (0.459) data 0.161 (0.328) loss_u loss_u 0.9775 (0.9310) acc_u 0.0000 (8.8393) lr 1.4115e-03 eta 0:00:19
epoch [75/200] batch [40/77] time 0.491 (0.460) data 0.360 (0.329) loss_u loss_u 0.9546 (0.9305) acc_u 3.1250 (8.8281) lr 1.4115e-03 eta 0:00:17
epoch [75/200] batch [45/77] time 0.369 (0.460) data 0.237 (0.329) loss_u loss_u 0.9102 (0.9299) acc_u 12.5000 (9.0278) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [50/77] time 0.311 (0.457) data 0.179 (0.326) loss_u loss_u 0.9429 (0.9328) acc_u 9.3750 (8.6250) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [55/77] time 0.437 (0.454) data 0.305 (0.322) loss_u loss_u 0.9438 (0.9352) acc_u 9.3750 (8.2955) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [60/77] time 0.407 (0.453) data 0.276 (0.321) loss_u loss_u 0.9331 (0.9352) acc_u 6.2500 (8.3333) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [65/77] time 0.433 (0.454) data 0.301 (0.322) loss_u loss_u 0.9580 (0.9355) acc_u 3.1250 (8.2212) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [70/77] time 0.491 (0.452) data 0.358 (0.321) loss_u loss_u 0.8853 (0.9348) acc_u 18.7500 (8.4821) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [75/77] time 0.420 (0.452) data 0.288 (0.320) loss_u loss_u 0.9346 (0.9344) acc_u 6.2500 (8.4583) lr 1.4115e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1667
confident_label rate tensor(0.2098, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 658
clean true:646
clean false:12
clean_rate:0.9817629179331308
noisy true:823
noisy false:1655
after delete: len(clean_dataset) 658
after delete: len(noisy_dataset) 2478
epoch [76/200] batch [5/20] time 0.412 (0.548) data 0.280 (0.416) loss_x loss_x 1.2930 (1.3066) acc_x 62.5000 (63.7500) lr 1.3971e-03 eta 0:00:08
epoch [76/200] batch [10/20] time 0.460 (0.495) data 0.330 (0.364) loss_x loss_x 0.9248 (1.2047) acc_x 78.1250 (68.4375) lr 1.3971e-03 eta 0:00:04
epoch [76/200] batch [15/20] time 0.488 (0.480) data 0.357 (0.349) loss_x loss_x 1.0938 (1.1893) acc_x 68.7500 (69.5833) lr 1.3971e-03 eta 0:00:02
epoch [76/200] batch [20/20] time 0.516 (0.486) data 0.386 (0.355) loss_x loss_x 1.0996 (1.2084) acc_x 68.7500 (69.6875) lr 1.3971e-03 eta 0:00:00
epoch [76/200] batch [5/77] time 0.364 (0.475) data 0.233 (0.344) loss_u loss_u 0.9478 (0.9229) acc_u 3.1250 (8.1250) lr 1.3971e-03 eta 0:00:34
epoch [76/200] batch [10/77] time 0.465 (0.471) data 0.333 (0.340) loss_u loss_u 0.9419 (0.9322) acc_u 9.3750 (8.1250) lr 1.3971e-03 eta 0:00:31
epoch [76/200] batch [15/77] time 0.495 (0.470) data 0.364 (0.339) loss_u loss_u 0.9893 (0.9392) acc_u 0.0000 (7.2917) lr 1.3971e-03 eta 0:00:29
epoch [76/200] batch [20/77] time 0.401 (0.464) data 0.269 (0.333) loss_u loss_u 0.8882 (0.9369) acc_u 15.6250 (7.5000) lr 1.3971e-03 eta 0:00:26
epoch [76/200] batch [25/77] time 0.522 (0.461) data 0.391 (0.329) loss_u loss_u 0.9551 (0.9349) acc_u 12.5000 (8.1250) lr 1.3971e-03 eta 0:00:23
epoch [76/200] batch [30/77] time 0.431 (0.463) data 0.300 (0.332) loss_u loss_u 0.8906 (0.9301) acc_u 18.7500 (8.8542) lr 1.3971e-03 eta 0:00:21
epoch [76/200] batch [35/77] time 0.342 (0.457) data 0.210 (0.326) loss_u loss_u 0.9341 (0.9330) acc_u 6.2500 (8.3036) lr 1.3971e-03 eta 0:00:19
epoch [76/200] batch [40/77] time 0.648 (0.460) data 0.516 (0.328) loss_u loss_u 0.9438 (0.9349) acc_u 9.3750 (8.2812) lr 1.3971e-03 eta 0:00:17
epoch [76/200] batch [45/77] time 0.344 (0.458) data 0.212 (0.326) loss_u loss_u 0.9751 (0.9345) acc_u 6.2500 (8.5417) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [50/77] time 0.546 (0.456) data 0.414 (0.325) loss_u loss_u 0.9082 (0.9351) acc_u 15.6250 (8.5000) lr 1.3971e-03 eta 0:00:12
epoch [76/200] batch [55/77] time 0.644 (0.458) data 0.512 (0.327) loss_u loss_u 0.9473 (0.9358) acc_u 6.2500 (8.2955) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [60/77] time 0.481 (0.458) data 0.349 (0.326) loss_u loss_u 0.9170 (0.9369) acc_u 9.3750 (8.0729) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [65/77] time 0.688 (0.459) data 0.557 (0.327) loss_u loss_u 0.9468 (0.9371) acc_u 6.2500 (7.9808) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [70/77] time 0.552 (0.459) data 0.421 (0.328) loss_u loss_u 0.9282 (0.9388) acc_u 12.5000 (7.7232) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [75/77] time 0.391 (0.458) data 0.261 (0.326) loss_u loss_u 0.9565 (0.9379) acc_u 9.3750 (7.8750) lr 1.3971e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1630
confident_label rate tensor(0.2060, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 646
clean true:633
clean false:13
clean_rate:0.9798761609907121
noisy true:873
noisy false:1617
after delete: len(clean_dataset) 646
after delete: len(noisy_dataset) 2490
epoch [77/200] batch [5/20] time 0.447 (0.507) data 0.315 (0.376) loss_x loss_x 1.1201 (1.1631) acc_x 71.8750 (72.5000) lr 1.3827e-03 eta 0:00:07
epoch [77/200] batch [10/20] time 0.643 (0.502) data 0.513 (0.370) loss_x loss_x 1.1699 (1.2465) acc_x 68.7500 (70.3125) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [15/20] time 0.369 (0.467) data 0.238 (0.336) loss_x loss_x 1.1992 (1.2074) acc_x 71.8750 (71.2500) lr 1.3827e-03 eta 0:00:02
epoch [77/200] batch [20/20] time 0.447 (0.468) data 0.317 (0.337) loss_x loss_x 1.5078 (1.2801) acc_x 65.6250 (69.6875) lr 1.3827e-03 eta 0:00:00
epoch [77/200] batch [5/77] time 0.479 (0.469) data 0.347 (0.338) loss_u loss_u 0.9541 (0.9483) acc_u 9.3750 (8.7500) lr 1.3827e-03 eta 0:00:33
epoch [77/200] batch [10/77] time 0.535 (0.474) data 0.405 (0.343) loss_u loss_u 0.9556 (0.9464) acc_u 3.1250 (7.1875) lr 1.3827e-03 eta 0:00:31
epoch [77/200] batch [15/77] time 0.372 (0.468) data 0.240 (0.337) loss_u loss_u 0.9829 (0.9513) acc_u 0.0000 (6.2500) lr 1.3827e-03 eta 0:00:29
epoch [77/200] batch [20/77] time 0.589 (0.465) data 0.457 (0.333) loss_u loss_u 0.9214 (0.9504) acc_u 9.3750 (6.5625) lr 1.3827e-03 eta 0:00:26
epoch [77/200] batch [25/77] time 0.444 (0.465) data 0.312 (0.333) loss_u loss_u 0.9492 (0.9451) acc_u 3.1250 (7.1250) lr 1.3827e-03 eta 0:00:24
epoch [77/200] batch [30/77] time 0.436 (0.459) data 0.304 (0.327) loss_u loss_u 0.8877 (0.9411) acc_u 18.7500 (8.1250) lr 1.3827e-03 eta 0:00:21
epoch [77/200] batch [35/77] time 0.495 (0.459) data 0.364 (0.327) loss_u loss_u 0.9531 (0.9416) acc_u 6.2500 (8.2143) lr 1.3827e-03 eta 0:00:19
epoch [77/200] batch [40/77] time 0.383 (0.453) data 0.251 (0.321) loss_u loss_u 0.9224 (0.9396) acc_u 12.5000 (8.3594) lr 1.3827e-03 eta 0:00:16
epoch [77/200] batch [45/77] time 0.475 (0.452) data 0.344 (0.321) loss_u loss_u 0.9590 (0.9407) acc_u 3.1250 (8.1250) lr 1.3827e-03 eta 0:00:14
epoch [77/200] batch [50/77] time 0.474 (0.450) data 0.342 (0.319) loss_u loss_u 0.9575 (0.9415) acc_u 9.3750 (8.1875) lr 1.3827e-03 eta 0:00:12
epoch [77/200] batch [55/77] time 0.376 (0.445) data 0.244 (0.314) loss_u loss_u 0.9873 (0.9424) acc_u 0.0000 (7.8977) lr 1.3827e-03 eta 0:00:09
epoch [77/200] batch [60/77] time 0.486 (0.447) data 0.355 (0.316) loss_u loss_u 0.9468 (0.9420) acc_u 9.3750 (7.9688) lr 1.3827e-03 eta 0:00:07
epoch [77/200] batch [65/77] time 0.440 (0.450) data 0.308 (0.318) loss_u loss_u 0.9585 (0.9401) acc_u 6.2500 (8.3173) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [70/77] time 0.459 (0.453) data 0.328 (0.321) loss_u loss_u 0.8672 (0.9400) acc_u 15.6250 (8.2143) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [75/77] time 0.475 (0.451) data 0.343 (0.320) loss_u loss_u 0.9355 (0.9379) acc_u 9.3750 (8.5417) lr 1.3827e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1593
confident_label rate tensor(0.2168, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 680
clean true:670
clean false:10
clean_rate:0.9852941176470589
noisy true:873
noisy false:1583
after delete: len(clean_dataset) 680
after delete: len(noisy_dataset) 2456
epoch [78/200] batch [5/21] time 0.581 (0.458) data 0.451 (0.328) loss_x loss_x 1.4492 (1.4168) acc_x 62.5000 (60.6250) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [10/21] time 0.420 (0.475) data 0.289 (0.344) loss_x loss_x 1.1328 (1.2703) acc_x 65.6250 (65.9375) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [15/21] time 0.415 (0.467) data 0.285 (0.336) loss_x loss_x 1.1699 (1.2041) acc_x 68.7500 (68.5417) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [20/21] time 0.635 (0.470) data 0.504 (0.340) loss_x loss_x 1.1523 (1.2792) acc_x 75.0000 (67.6562) lr 1.3681e-03 eta 0:00:00
epoch [78/200] batch [5/76] time 0.438 (0.462) data 0.306 (0.330) loss_u loss_u 0.9253 (0.9314) acc_u 9.3750 (8.7500) lr 1.3681e-03 eta 0:00:32
epoch [78/200] batch [10/76] time 0.495 (0.456) data 0.364 (0.325) loss_u loss_u 0.9839 (0.9370) acc_u 6.2500 (9.0625) lr 1.3681e-03 eta 0:00:30
epoch [78/200] batch [15/76] time 0.625 (0.459) data 0.494 (0.328) loss_u loss_u 0.9561 (0.9432) acc_u 12.5000 (8.5417) lr 1.3681e-03 eta 0:00:28
epoch [78/200] batch [20/76] time 0.484 (0.465) data 0.351 (0.334) loss_u loss_u 0.9487 (0.9456) acc_u 3.1250 (7.6562) lr 1.3681e-03 eta 0:00:26
epoch [78/200] batch [25/76] time 0.535 (0.467) data 0.403 (0.335) loss_u loss_u 0.9956 (0.9459) acc_u 0.0000 (7.2500) lr 1.3681e-03 eta 0:00:23
epoch [78/200] batch [30/76] time 0.372 (0.467) data 0.240 (0.336) loss_u loss_u 0.9087 (0.9422) acc_u 12.5000 (7.7083) lr 1.3681e-03 eta 0:00:21
epoch [78/200] batch [35/76] time 0.492 (0.465) data 0.360 (0.334) loss_u loss_u 0.9536 (0.9431) acc_u 6.2500 (7.6786) lr 1.3681e-03 eta 0:00:19
epoch [78/200] batch [40/76] time 0.429 (0.464) data 0.299 (0.332) loss_u loss_u 0.9341 (0.9416) acc_u 9.3750 (7.8125) lr 1.3681e-03 eta 0:00:16
epoch [78/200] batch [45/76] time 0.556 (0.462) data 0.425 (0.331) loss_u loss_u 0.9429 (0.9427) acc_u 6.2500 (7.6389) lr 1.3681e-03 eta 0:00:14
epoch [78/200] batch [50/76] time 0.452 (0.462) data 0.322 (0.330) loss_u loss_u 0.9736 (0.9428) acc_u 0.0000 (7.3125) lr 1.3681e-03 eta 0:00:12
epoch [78/200] batch [55/76] time 0.417 (0.464) data 0.286 (0.332) loss_u loss_u 0.9761 (0.9448) acc_u 3.1250 (7.1023) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [60/76] time 0.378 (0.461) data 0.247 (0.329) loss_u loss_u 0.8696 (0.9445) acc_u 18.7500 (7.0833) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [65/76] time 0.393 (0.458) data 0.262 (0.327) loss_u loss_u 0.9590 (0.9457) acc_u 6.2500 (6.9712) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [70/76] time 0.455 (0.457) data 0.324 (0.325) loss_u loss_u 0.9214 (0.9439) acc_u 12.5000 (7.2321) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [75/76] time 0.434 (0.456) data 0.304 (0.324) loss_u loss_u 0.9243 (0.9437) acc_u 12.5000 (7.2917) lr 1.3681e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1584
confident_label rate tensor(0.2124, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 666
clean true:658
clean false:8
clean_rate:0.987987987987988
noisy true:894
noisy false:1576
after delete: len(clean_dataset) 666
after delete: len(noisy_dataset) 2470
epoch [79/200] batch [5/20] time 0.541 (0.454) data 0.410 (0.324) loss_x loss_x 0.6982 (1.1781) acc_x 90.6250 (72.5000) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [10/20] time 0.439 (0.491) data 0.309 (0.360) loss_x loss_x 0.9150 (1.3021) acc_x 78.1250 (69.3750) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [15/20] time 0.426 (0.476) data 0.295 (0.345) loss_x loss_x 1.5801 (1.2922) acc_x 59.3750 (68.7500) lr 1.3535e-03 eta 0:00:02
epoch [79/200] batch [20/20] time 0.433 (0.476) data 0.302 (0.344) loss_x loss_x 1.3154 (1.2660) acc_x 68.7500 (69.3750) lr 1.3535e-03 eta 0:00:00
epoch [79/200] batch [5/77] time 0.461 (0.481) data 0.329 (0.350) loss_u loss_u 0.8989 (0.9172) acc_u 18.7500 (13.1250) lr 1.3535e-03 eta 0:00:34
epoch [79/200] batch [10/77] time 0.367 (0.466) data 0.235 (0.335) loss_u loss_u 0.9580 (0.9348) acc_u 6.2500 (9.6875) lr 1.3535e-03 eta 0:00:31
epoch [79/200] batch [15/77] time 0.383 (0.454) data 0.252 (0.323) loss_u loss_u 0.9751 (0.9440) acc_u 3.1250 (7.7083) lr 1.3535e-03 eta 0:00:28
epoch [79/200] batch [20/77] time 0.484 (0.458) data 0.352 (0.327) loss_u loss_u 0.8838 (0.9414) acc_u 15.6250 (8.4375) lr 1.3535e-03 eta 0:00:26
epoch [79/200] batch [25/77] time 0.511 (0.454) data 0.379 (0.322) loss_u loss_u 0.9873 (0.9457) acc_u 0.0000 (7.6250) lr 1.3535e-03 eta 0:00:23
epoch [79/200] batch [30/77] time 0.556 (0.460) data 0.423 (0.329) loss_u loss_u 0.9116 (0.9407) acc_u 9.3750 (8.2292) lr 1.3535e-03 eta 0:00:21
epoch [79/200] batch [35/77] time 0.445 (0.460) data 0.314 (0.328) loss_u loss_u 0.9038 (0.9418) acc_u 12.5000 (7.8571) lr 1.3535e-03 eta 0:00:19
epoch [79/200] batch [40/77] time 0.377 (0.459) data 0.246 (0.327) loss_u loss_u 0.9873 (0.9401) acc_u 0.0000 (8.2031) lr 1.3535e-03 eta 0:00:16
epoch [79/200] batch [45/77] time 0.342 (0.454) data 0.211 (0.323) loss_u loss_u 0.9404 (0.9396) acc_u 6.2500 (8.3333) lr 1.3535e-03 eta 0:00:14
epoch [79/200] batch [50/77] time 0.469 (0.451) data 0.338 (0.320) loss_u loss_u 0.9229 (0.9380) acc_u 12.5000 (8.5625) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [55/77] time 0.523 (0.455) data 0.391 (0.323) loss_u loss_u 0.9595 (0.9365) acc_u 3.1250 (8.7500) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [60/77] time 0.419 (0.457) data 0.286 (0.326) loss_u loss_u 0.9253 (0.9350) acc_u 9.3750 (9.0104) lr 1.3535e-03 eta 0:00:07
epoch [79/200] batch [65/77] time 0.489 (0.458) data 0.356 (0.327) loss_u loss_u 0.9883 (0.9356) acc_u 3.1250 (8.8462) lr 1.3535e-03 eta 0:00:05
epoch [79/200] batch [70/77] time 0.452 (0.460) data 0.321 (0.328) loss_u loss_u 0.9448 (0.9344) acc_u 6.2500 (9.0179) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [75/77] time 0.493 (0.459) data 0.361 (0.327) loss_u loss_u 0.9316 (0.9352) acc_u 6.2500 (8.8750) lr 1.3535e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1582
confident_label rate tensor(0.2066, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 648
clean true:635
clean false:13
clean_rate:0.9799382716049383
noisy true:919
noisy false:1569
after delete: len(clean_dataset) 648
after delete: len(noisy_dataset) 2488
epoch [80/200] batch [5/20] time 0.506 (0.431) data 0.376 (0.300) loss_x loss_x 1.1602 (1.3559) acc_x 78.1250 (71.2500) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [10/20] time 0.515 (0.451) data 0.384 (0.320) loss_x loss_x 1.4805 (1.3756) acc_x 75.0000 (72.1875) lr 1.3387e-03 eta 0:00:04
epoch [80/200] batch [15/20] time 0.371 (0.456) data 0.240 (0.325) loss_x loss_x 1.2471 (1.3557) acc_x 71.8750 (70.6250) lr 1.3387e-03 eta 0:00:02
epoch [80/200] batch [20/20] time 0.486 (0.466) data 0.355 (0.335) loss_x loss_x 1.3154 (1.2918) acc_x 65.6250 (70.9375) lr 1.3387e-03 eta 0:00:00
epoch [80/200] batch [5/77] time 0.367 (0.457) data 0.235 (0.325) loss_u loss_u 0.9390 (0.9449) acc_u 6.2500 (4.3750) lr 1.3387e-03 eta 0:00:32
epoch [80/200] batch [10/77] time 0.365 (0.444) data 0.232 (0.313) loss_u loss_u 0.9385 (0.9379) acc_u 6.2500 (5.9375) lr 1.3387e-03 eta 0:00:29
epoch [80/200] batch [15/77] time 0.393 (0.453) data 0.262 (0.321) loss_u loss_u 0.9683 (0.9389) acc_u 0.0000 (6.4583) lr 1.3387e-03 eta 0:00:28
epoch [80/200] batch [20/77] time 0.616 (0.455) data 0.482 (0.323) loss_u loss_u 0.9805 (0.9386) acc_u 6.2500 (7.0312) lr 1.3387e-03 eta 0:00:25
epoch [80/200] batch [25/77] time 0.581 (0.463) data 0.448 (0.331) loss_u loss_u 0.9536 (0.9395) acc_u 9.3750 (7.2500) lr 1.3387e-03 eta 0:00:24
epoch [80/200] batch [30/77] time 0.384 (0.461) data 0.252 (0.330) loss_u loss_u 0.9341 (0.9393) acc_u 9.3750 (7.2917) lr 1.3387e-03 eta 0:00:21
epoch [80/200] batch [35/77] time 0.600 (0.468) data 0.468 (0.336) loss_u loss_u 0.9683 (0.9411) acc_u 3.1250 (7.2321) lr 1.3387e-03 eta 0:00:19
epoch [80/200] batch [40/77] time 0.475 (0.462) data 0.343 (0.330) loss_u loss_u 0.9556 (0.9398) acc_u 6.2500 (7.5000) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [45/77] time 0.577 (0.465) data 0.445 (0.333) loss_u loss_u 0.9551 (0.9400) acc_u 6.2500 (7.5000) lr 1.3387e-03 eta 0:00:14
epoch [80/200] batch [50/77] time 0.410 (0.462) data 0.279 (0.330) loss_u loss_u 0.9746 (0.9406) acc_u 3.1250 (7.6250) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [55/77] time 0.509 (0.464) data 0.377 (0.332) loss_u loss_u 0.9531 (0.9410) acc_u 6.2500 (7.6705) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [60/77] time 0.477 (0.462) data 0.345 (0.331) loss_u loss_u 0.9180 (0.9409) acc_u 9.3750 (7.6562) lr 1.3387e-03 eta 0:00:07
epoch [80/200] batch [65/77] time 0.398 (0.459) data 0.265 (0.327) loss_u loss_u 0.9106 (0.9394) acc_u 12.5000 (7.9327) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [70/77] time 0.522 (0.457) data 0.390 (0.325) loss_u loss_u 0.9092 (0.9390) acc_u 12.5000 (7.9018) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [75/77] time 0.498 (0.454) data 0.366 (0.322) loss_u loss_u 0.9365 (0.9385) acc_u 15.6250 (8.0000) lr 1.3387e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1611
confident_label rate tensor(0.2073, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 650
clean true:644
clean false:6
clean_rate:0.9907692307692307
noisy true:881
noisy false:1605
after delete: len(clean_dataset) 650
after delete: len(noisy_dataset) 2486
epoch [81/200] batch [5/20] time 0.487 (0.499) data 0.356 (0.367) loss_x loss_x 0.7114 (1.1157) acc_x 84.3750 (71.2500) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [10/20] time 0.398 (0.490) data 0.268 (0.359) loss_x loss_x 1.4854 (1.1103) acc_x 62.5000 (72.1875) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [15/20] time 0.547 (0.490) data 0.416 (0.359) loss_x loss_x 1.2422 (1.0852) acc_x 65.6250 (72.5000) lr 1.3239e-03 eta 0:00:02
epoch [81/200] batch [20/20] time 0.424 (0.474) data 0.294 (0.343) loss_x loss_x 1.2031 (1.1127) acc_x 68.7500 (71.7188) lr 1.3239e-03 eta 0:00:00
epoch [81/200] batch [5/77] time 0.349 (0.459) data 0.219 (0.328) loss_u loss_u 0.9336 (0.9354) acc_u 9.3750 (8.7500) lr 1.3239e-03 eta 0:00:33
epoch [81/200] batch [10/77] time 0.400 (0.459) data 0.269 (0.328) loss_u loss_u 0.8960 (0.9244) acc_u 15.6250 (10.0000) lr 1.3239e-03 eta 0:00:30
epoch [81/200] batch [15/77] time 0.430 (0.456) data 0.299 (0.325) loss_u loss_u 0.8560 (0.9271) acc_u 18.7500 (9.1667) lr 1.3239e-03 eta 0:00:28
epoch [81/200] batch [20/77] time 0.471 (0.461) data 0.339 (0.330) loss_u loss_u 0.8872 (0.9258) acc_u 18.7500 (9.3750) lr 1.3239e-03 eta 0:00:26
epoch [81/200] batch [25/77] time 0.403 (0.462) data 0.271 (0.331) loss_u loss_u 0.9365 (0.9298) acc_u 9.3750 (9.1250) lr 1.3239e-03 eta 0:00:24
epoch [81/200] batch [30/77] time 0.557 (0.460) data 0.426 (0.329) loss_u loss_u 0.9053 (0.9301) acc_u 12.5000 (9.1667) lr 1.3239e-03 eta 0:00:21
epoch [81/200] batch [35/77] time 0.334 (0.459) data 0.202 (0.328) loss_u loss_u 0.9580 (0.9318) acc_u 9.3750 (9.1071) lr 1.3239e-03 eta 0:00:19
epoch [81/200] batch [40/77] time 0.405 (0.455) data 0.274 (0.324) loss_u loss_u 0.8521 (0.9302) acc_u 18.7500 (9.2188) lr 1.3239e-03 eta 0:00:16
epoch [81/200] batch [45/77] time 0.359 (0.456) data 0.228 (0.325) loss_u loss_u 0.9448 (0.9330) acc_u 6.2500 (8.8194) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [50/77] time 0.396 (0.452) data 0.265 (0.321) loss_u loss_u 0.8887 (0.9336) acc_u 15.6250 (8.6875) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [55/77] time 0.461 (0.455) data 0.329 (0.324) loss_u loss_u 0.9526 (0.9335) acc_u 6.2500 (8.6364) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [60/77] time 0.337 (0.452) data 0.205 (0.321) loss_u loss_u 0.8774 (0.9332) acc_u 12.5000 (8.6458) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [65/77] time 0.694 (0.452) data 0.562 (0.321) loss_u loss_u 0.9688 (0.9341) acc_u 3.1250 (8.4615) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [70/77] time 0.410 (0.453) data 0.278 (0.322) loss_u loss_u 0.8945 (0.9343) acc_u 15.6250 (8.4821) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [75/77] time 0.351 (0.452) data 0.220 (0.321) loss_u loss_u 0.9536 (0.9359) acc_u 3.1250 (8.2083) lr 1.3239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1631
confident_label rate tensor(0.2191, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 687
clean true:672
clean false:15
clean_rate:0.9781659388646288
noisy true:833
noisy false:1616
after delete: len(clean_dataset) 687
after delete: len(noisy_dataset) 2449
epoch [82/200] batch [5/21] time 0.452 (0.445) data 0.321 (0.314) loss_x loss_x 1.6084 (1.3234) acc_x 59.3750 (71.8750) lr 1.3090e-03 eta 0:00:07
epoch [82/200] batch [10/21] time 0.488 (0.454) data 0.356 (0.323) loss_x loss_x 1.3330 (1.3295) acc_x 65.6250 (69.3750) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [15/21] time 0.433 (0.442) data 0.302 (0.311) loss_x loss_x 0.9331 (1.2723) acc_x 78.1250 (71.2500) lr 1.3090e-03 eta 0:00:02
epoch [82/200] batch [20/21] time 0.402 (0.446) data 0.270 (0.315) loss_x loss_x 1.8203 (1.2702) acc_x 68.7500 (71.8750) lr 1.3090e-03 eta 0:00:00
epoch [82/200] batch [5/76] time 0.433 (0.435) data 0.301 (0.304) loss_u loss_u 0.9600 (0.9399) acc_u 3.1250 (6.8750) lr 1.3090e-03 eta 0:00:30
epoch [82/200] batch [10/76] time 0.410 (0.427) data 0.279 (0.295) loss_u loss_u 0.9785 (0.9536) acc_u 3.1250 (5.6250) lr 1.3090e-03 eta 0:00:28
epoch [82/200] batch [15/76] time 0.416 (0.433) data 0.284 (0.302) loss_u loss_u 0.8667 (0.9422) acc_u 18.7500 (7.7083) lr 1.3090e-03 eta 0:00:26
epoch [82/200] batch [20/76] time 0.525 (0.434) data 0.393 (0.303) loss_u loss_u 0.9546 (0.9434) acc_u 6.2500 (7.5000) lr 1.3090e-03 eta 0:00:24
epoch [82/200] batch [25/76] time 0.447 (0.432) data 0.316 (0.301) loss_u loss_u 0.9038 (0.9421) acc_u 9.3750 (7.6250) lr 1.3090e-03 eta 0:00:22
epoch [82/200] batch [30/76] time 0.508 (0.435) data 0.377 (0.304) loss_u loss_u 0.9580 (0.9446) acc_u 3.1250 (7.3958) lr 1.3090e-03 eta 0:00:20
epoch [82/200] batch [35/76] time 0.564 (0.441) data 0.427 (0.310) loss_u loss_u 0.9531 (0.9448) acc_u 9.3750 (7.5893) lr 1.3090e-03 eta 0:00:18
epoch [82/200] batch [40/76] time 0.372 (0.439) data 0.241 (0.308) loss_u loss_u 0.9868 (0.9457) acc_u 0.0000 (7.4219) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [45/76] time 0.722 (0.446) data 0.590 (0.315) loss_u loss_u 0.9746 (0.9452) acc_u 6.2500 (7.4306) lr 1.3090e-03 eta 0:00:13
epoch [82/200] batch [50/76] time 0.592 (0.445) data 0.460 (0.314) loss_u loss_u 0.9482 (0.9446) acc_u 9.3750 (7.5000) lr 1.3090e-03 eta 0:00:11
epoch [82/200] batch [55/76] time 0.412 (0.451) data 0.280 (0.319) loss_u loss_u 0.8701 (0.9427) acc_u 15.6250 (7.6136) lr 1.3090e-03 eta 0:00:09
epoch [82/200] batch [60/76] time 0.377 (0.448) data 0.245 (0.317) loss_u loss_u 0.9556 (0.9410) acc_u 6.2500 (7.9167) lr 1.3090e-03 eta 0:00:07
epoch [82/200] batch [65/76] time 0.517 (0.447) data 0.387 (0.316) loss_u loss_u 0.9443 (0.9406) acc_u 6.2500 (7.9808) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [70/76] time 0.443 (0.449) data 0.312 (0.317) loss_u loss_u 0.9863 (0.9418) acc_u 6.2500 (7.9911) lr 1.3090e-03 eta 0:00:02
epoch [82/200] batch [75/76] time 0.492 (0.447) data 0.360 (0.316) loss_u loss_u 0.9907 (0.9428) acc_u 0.0000 (7.9583) lr 1.3090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1609
confident_label rate tensor(0.2092, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 656
clean true:650
clean false:6
clean_rate:0.9908536585365854
noisy true:877
noisy false:1603
after delete: len(clean_dataset) 656
after delete: len(noisy_dataset) 2480
epoch [83/200] batch [5/20] time 0.420 (0.463) data 0.289 (0.332) loss_x loss_x 1.3936 (1.2443) acc_x 75.0000 (71.2500) lr 1.2940e-03 eta 0:00:06
epoch [83/200] batch [10/20] time 0.466 (0.446) data 0.336 (0.315) loss_x loss_x 1.5576 (1.2686) acc_x 65.6250 (73.1250) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [15/20] time 0.462 (0.455) data 0.330 (0.324) loss_x loss_x 1.4219 (1.2970) acc_x 65.6250 (70.8333) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [20/20] time 0.451 (0.462) data 0.319 (0.331) loss_x loss_x 1.6465 (1.3039) acc_x 56.2500 (69.3750) lr 1.2940e-03 eta 0:00:00
epoch [83/200] batch [5/77] time 0.387 (0.465) data 0.256 (0.334) loss_u loss_u 0.9868 (0.9492) acc_u 3.1250 (6.8750) lr 1.2940e-03 eta 0:00:33
epoch [83/200] batch [10/77] time 0.437 (0.461) data 0.306 (0.330) loss_u loss_u 0.9839 (0.9339) acc_u 0.0000 (7.8125) lr 1.2940e-03 eta 0:00:30
epoch [83/200] batch [15/77] time 0.484 (0.462) data 0.354 (0.330) loss_u loss_u 0.9248 (0.9405) acc_u 9.3750 (7.5000) lr 1.2940e-03 eta 0:00:28
epoch [83/200] batch [20/77] time 0.465 (0.459) data 0.333 (0.327) loss_u loss_u 0.9170 (0.9359) acc_u 12.5000 (7.9688) lr 1.2940e-03 eta 0:00:26
epoch [83/200] batch [25/77] time 0.408 (0.459) data 0.277 (0.327) loss_u loss_u 0.9575 (0.9353) acc_u 9.3750 (8.3750) lr 1.2940e-03 eta 0:00:23
epoch [83/200] batch [30/77] time 0.464 (0.467) data 0.332 (0.335) loss_u loss_u 0.9653 (0.9399) acc_u 3.1250 (7.6042) lr 1.2940e-03 eta 0:00:21
epoch [83/200] batch [35/77] time 0.420 (0.465) data 0.289 (0.333) loss_u loss_u 0.9536 (0.9392) acc_u 3.1250 (7.5893) lr 1.2940e-03 eta 0:00:19
epoch [83/200] batch [40/77] time 0.399 (0.464) data 0.267 (0.332) loss_u loss_u 0.8457 (0.9344) acc_u 18.7500 (8.1250) lr 1.2940e-03 eta 0:00:17
epoch [83/200] batch [45/77] time 0.514 (0.463) data 0.381 (0.331) loss_u loss_u 0.9609 (0.9355) acc_u 9.3750 (7.9167) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [50/77] time 0.394 (0.461) data 0.263 (0.330) loss_u loss_u 0.9624 (0.9356) acc_u 3.1250 (8.0000) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [55/77] time 0.364 (0.462) data 0.234 (0.331) loss_u loss_u 0.8848 (0.9342) acc_u 15.6250 (8.1250) lr 1.2940e-03 eta 0:00:10
epoch [83/200] batch [60/77] time 0.391 (0.462) data 0.260 (0.331) loss_u loss_u 0.9678 (0.9347) acc_u 3.1250 (8.0729) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [65/77] time 0.371 (0.459) data 0.239 (0.327) loss_u loss_u 0.9009 (0.9341) acc_u 9.3750 (8.2212) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [70/77] time 0.648 (0.460) data 0.517 (0.328) loss_u loss_u 0.8994 (0.9345) acc_u 9.3750 (8.0804) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [75/77] time 0.464 (0.462) data 0.331 (0.331) loss_u loss_u 0.9463 (0.9355) acc_u 6.2500 (7.9583) lr 1.2940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1627
confident_label rate tensor(0.2098, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 658
clean true:650
clean false:8
clean_rate:0.9878419452887538
noisy true:859
noisy false:1619
after delete: len(clean_dataset) 658
after delete: len(noisy_dataset) 2478
epoch [84/200] batch [5/20] time 0.369 (0.457) data 0.238 (0.327) loss_x loss_x 1.0146 (1.3184) acc_x 81.2500 (68.7500) lr 1.2790e-03 eta 0:00:06
epoch [84/200] batch [10/20] time 0.457 (0.443) data 0.326 (0.312) loss_x loss_x 0.9399 (1.2416) acc_x 71.8750 (70.0000) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [15/20] time 0.462 (0.444) data 0.330 (0.313) loss_x loss_x 1.2969 (1.2339) acc_x 65.6250 (69.7917) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [20/20] time 0.484 (0.444) data 0.353 (0.313) loss_x loss_x 0.9331 (1.2307) acc_x 75.0000 (70.6250) lr 1.2790e-03 eta 0:00:00
epoch [84/200] batch [5/77] time 0.522 (0.449) data 0.390 (0.318) loss_u loss_u 0.9595 (0.9289) acc_u 3.1250 (8.7500) lr 1.2790e-03 eta 0:00:32
epoch [84/200] batch [10/77] time 0.593 (0.448) data 0.462 (0.317) loss_u loss_u 0.9922 (0.9458) acc_u 0.0000 (6.5625) lr 1.2790e-03 eta 0:00:30
epoch [84/200] batch [15/77] time 0.405 (0.443) data 0.274 (0.312) loss_u loss_u 0.9468 (0.9442) acc_u 6.2500 (6.6667) lr 1.2790e-03 eta 0:00:27
epoch [84/200] batch [20/77] time 0.405 (0.443) data 0.274 (0.312) loss_u loss_u 0.9077 (0.9381) acc_u 12.5000 (7.8125) lr 1.2790e-03 eta 0:00:25
epoch [84/200] batch [25/77] time 0.515 (0.445) data 0.385 (0.314) loss_u loss_u 0.9517 (0.9398) acc_u 6.2500 (7.6250) lr 1.2790e-03 eta 0:00:23
epoch [84/200] batch [30/77] time 0.468 (0.453) data 0.336 (0.322) loss_u loss_u 0.9160 (0.9407) acc_u 15.6250 (7.8125) lr 1.2790e-03 eta 0:00:21
epoch [84/200] batch [35/77] time 0.385 (0.449) data 0.254 (0.318) loss_u loss_u 0.9424 (0.9418) acc_u 9.3750 (7.9464) lr 1.2790e-03 eta 0:00:18
epoch [84/200] batch [40/77] time 0.394 (0.446) data 0.263 (0.315) loss_u loss_u 0.9941 (0.9382) acc_u 0.0000 (8.4375) lr 1.2790e-03 eta 0:00:16
epoch [84/200] batch [45/77] time 0.391 (0.443) data 0.261 (0.312) loss_u loss_u 0.8242 (0.9362) acc_u 21.8750 (8.6111) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [50/77] time 0.364 (0.441) data 0.232 (0.310) loss_u loss_u 0.9570 (0.9375) acc_u 3.1250 (8.3750) lr 1.2790e-03 eta 0:00:11
epoch [84/200] batch [55/77] time 0.591 (0.442) data 0.459 (0.311) loss_u loss_u 0.8857 (0.9339) acc_u 15.6250 (8.7500) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [60/77] time 0.614 (0.446) data 0.484 (0.315) loss_u loss_u 0.9414 (0.9351) acc_u 6.2500 (8.5417) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [65/77] time 0.370 (0.448) data 0.238 (0.317) loss_u loss_u 0.9048 (0.9361) acc_u 12.5000 (8.4615) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [70/77] time 0.413 (0.446) data 0.281 (0.315) loss_u loss_u 0.9038 (0.9350) acc_u 12.5000 (8.5268) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [75/77] time 0.521 (0.448) data 0.390 (0.317) loss_u loss_u 0.9141 (0.9351) acc_u 9.3750 (8.5000) lr 1.2790e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1617
confident_label rate tensor(0.2079, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 652
clean true:641
clean false:11
clean_rate:0.9831288343558282
noisy true:878
noisy false:1606
after delete: len(clean_dataset) 652
after delete: len(noisy_dataset) 2484
epoch [85/200] batch [5/20] time 0.450 (0.455) data 0.320 (0.324) loss_x loss_x 1.3359 (1.3666) acc_x 71.8750 (66.8750) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [10/20] time 0.375 (0.487) data 0.245 (0.356) loss_x loss_x 1.1934 (1.2623) acc_x 71.8750 (69.6875) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [15/20] time 0.437 (0.490) data 0.306 (0.359) loss_x loss_x 0.9189 (1.1689) acc_x 81.2500 (72.5000) lr 1.2639e-03 eta 0:00:02
epoch [85/200] batch [20/20] time 0.481 (0.478) data 0.350 (0.347) loss_x loss_x 1.5898 (1.2310) acc_x 62.5000 (69.8438) lr 1.2639e-03 eta 0:00:00
epoch [85/200] batch [5/77] time 0.468 (0.467) data 0.336 (0.336) loss_u loss_u 0.9731 (0.9262) acc_u 3.1250 (9.3750) lr 1.2639e-03 eta 0:00:33
epoch [85/200] batch [10/77] time 0.410 (0.461) data 0.278 (0.330) loss_u loss_u 0.9648 (0.9413) acc_u 6.2500 (7.8125) lr 1.2639e-03 eta 0:00:30
epoch [85/200] batch [15/77] time 0.544 (0.458) data 0.414 (0.327) loss_u loss_u 0.9277 (0.9373) acc_u 9.3750 (8.3333) lr 1.2639e-03 eta 0:00:28
epoch [85/200] batch [20/77] time 0.629 (0.459) data 0.498 (0.328) loss_u loss_u 0.9448 (0.9375) acc_u 12.5000 (8.4375) lr 1.2639e-03 eta 0:00:26
epoch [85/200] batch [25/77] time 0.485 (0.456) data 0.353 (0.325) loss_u loss_u 0.8901 (0.9339) acc_u 12.5000 (9.1250) lr 1.2639e-03 eta 0:00:23
epoch [85/200] batch [30/77] time 0.497 (0.457) data 0.367 (0.326) loss_u loss_u 0.9561 (0.9341) acc_u 6.2500 (8.9583) lr 1.2639e-03 eta 0:00:21
epoch [85/200] batch [35/77] time 0.368 (0.455) data 0.237 (0.324) loss_u loss_u 0.8901 (0.9342) acc_u 15.6250 (8.7500) lr 1.2639e-03 eta 0:00:19
epoch [85/200] batch [40/77] time 0.391 (0.450) data 0.260 (0.319) loss_u loss_u 0.9482 (0.9345) acc_u 9.3750 (8.5938) lr 1.2639e-03 eta 0:00:16
epoch [85/200] batch [45/77] time 0.410 (0.454) data 0.280 (0.323) loss_u loss_u 0.9468 (0.9340) acc_u 3.1250 (8.8194) lr 1.2639e-03 eta 0:00:14
epoch [85/200] batch [50/77] time 0.419 (0.453) data 0.288 (0.322) loss_u loss_u 0.9302 (0.9327) acc_u 12.5000 (9.0625) lr 1.2639e-03 eta 0:00:12
epoch [85/200] batch [55/77] time 0.465 (0.453) data 0.335 (0.322) loss_u loss_u 0.9277 (0.9321) acc_u 6.2500 (8.9773) lr 1.2639e-03 eta 0:00:09
epoch [85/200] batch [60/77] time 0.423 (0.453) data 0.292 (0.322) loss_u loss_u 0.9561 (0.9307) acc_u 9.3750 (9.2188) lr 1.2639e-03 eta 0:00:07
epoch [85/200] batch [65/77] time 0.843 (0.458) data 0.711 (0.327) loss_u loss_u 0.9492 (0.9324) acc_u 12.5000 (8.9904) lr 1.2639e-03 eta 0:00:05
epoch [85/200] batch [70/77] time 0.361 (0.456) data 0.230 (0.325) loss_u loss_u 0.9658 (0.9341) acc_u 6.2500 (8.8393) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [75/77] time 0.423 (0.453) data 0.291 (0.322) loss_u loss_u 0.9180 (0.9344) acc_u 9.3750 (8.7500) lr 1.2639e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1634
confident_label rate tensor(0.2121, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 665
clean true:652
clean false:13
clean_rate:0.9804511278195489
noisy true:850
noisy false:1621
after delete: len(clean_dataset) 665
after delete: len(noisy_dataset) 2471
epoch [86/200] batch [5/20] time 0.573 (0.465) data 0.443 (0.335) loss_x loss_x 1.5342 (1.1996) acc_x 62.5000 (71.2500) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [10/20] time 0.448 (0.455) data 0.317 (0.325) loss_x loss_x 1.2324 (1.1367) acc_x 71.8750 (72.5000) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [15/20] time 0.558 (0.476) data 0.427 (0.346) loss_x loss_x 0.8906 (1.1437) acc_x 78.1250 (72.0833) lr 1.2487e-03 eta 0:00:02
epoch [86/200] batch [20/20] time 0.469 (0.478) data 0.336 (0.348) loss_x loss_x 1.4062 (1.2049) acc_x 65.6250 (72.3438) lr 1.2487e-03 eta 0:00:00
epoch [86/200] batch [5/77] time 0.461 (0.485) data 0.331 (0.354) loss_u loss_u 0.8999 (0.9446) acc_u 15.6250 (10.0000) lr 1.2487e-03 eta 0:00:34
epoch [86/200] batch [10/77] time 0.503 (0.477) data 0.373 (0.346) loss_u loss_u 0.8809 (0.9296) acc_u 15.6250 (10.6250) lr 1.2487e-03 eta 0:00:31
epoch [86/200] batch [15/77] time 0.330 (0.461) data 0.199 (0.330) loss_u loss_u 0.9038 (0.9288) acc_u 9.3750 (10.0000) lr 1.2487e-03 eta 0:00:28
epoch [86/200] batch [20/77] time 0.418 (0.462) data 0.286 (0.331) loss_u loss_u 0.8887 (0.9239) acc_u 12.5000 (10.4688) lr 1.2487e-03 eta 0:00:26
epoch [86/200] batch [25/77] time 0.346 (0.456) data 0.214 (0.325) loss_u loss_u 0.9380 (0.9284) acc_u 6.2500 (9.5000) lr 1.2487e-03 eta 0:00:23
epoch [86/200] batch [30/77] time 0.350 (0.454) data 0.218 (0.323) loss_u loss_u 0.9917 (0.9340) acc_u 0.0000 (9.0625) lr 1.2487e-03 eta 0:00:21
epoch [86/200] batch [35/77] time 0.424 (0.454) data 0.293 (0.322) loss_u loss_u 0.9346 (0.9344) acc_u 6.2500 (8.9286) lr 1.2487e-03 eta 0:00:19
epoch [86/200] batch [40/77] time 0.392 (0.456) data 0.260 (0.325) loss_u loss_u 0.9375 (0.9344) acc_u 12.5000 (8.8281) lr 1.2487e-03 eta 0:00:16
epoch [86/200] batch [45/77] time 0.446 (0.456) data 0.315 (0.324) loss_u loss_u 0.9102 (0.9329) acc_u 9.3750 (9.2361) lr 1.2487e-03 eta 0:00:14
epoch [86/200] batch [50/77] time 0.575 (0.454) data 0.443 (0.323) loss_u loss_u 0.8945 (0.9340) acc_u 12.5000 (8.9375) lr 1.2487e-03 eta 0:00:12
epoch [86/200] batch [55/77] time 0.367 (0.450) data 0.235 (0.319) loss_u loss_u 0.9214 (0.9349) acc_u 6.2500 (8.6932) lr 1.2487e-03 eta 0:00:09
epoch [86/200] batch [60/77] time 0.449 (0.451) data 0.317 (0.320) loss_u loss_u 0.9146 (0.9340) acc_u 15.6250 (9.0104) lr 1.2487e-03 eta 0:00:07
epoch [86/200] batch [65/77] time 0.419 (0.448) data 0.287 (0.317) loss_u loss_u 0.9580 (0.9341) acc_u 3.1250 (8.8462) lr 1.2487e-03 eta 0:00:05
epoch [86/200] batch [70/77] time 0.409 (0.450) data 0.277 (0.319) loss_u loss_u 0.9966 (0.9342) acc_u 0.0000 (8.8839) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [75/77] time 0.423 (0.449) data 0.292 (0.318) loss_u loss_u 0.9087 (0.9347) acc_u 9.3750 (8.8333) lr 1.2487e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1570
confident_label rate tensor(0.2178, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 683
clean true:672
clean false:11
clean_rate:0.9838945827232797
noisy true:894
noisy false:1559
after delete: len(clean_dataset) 683
after delete: len(noisy_dataset) 2453
epoch [87/200] batch [5/21] time 0.457 (0.470) data 0.325 (0.338) loss_x loss_x 1.1582 (1.1928) acc_x 71.8750 (72.5000) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [10/21] time 0.337 (0.464) data 0.206 (0.333) loss_x loss_x 1.7256 (1.2171) acc_x 59.3750 (70.3125) lr 1.2334e-03 eta 0:00:05
epoch [87/200] batch [15/21] time 0.353 (0.446) data 0.222 (0.315) loss_x loss_x 0.7969 (1.2144) acc_x 84.3750 (71.2500) lr 1.2334e-03 eta 0:00:02
epoch [87/200] batch [20/21] time 0.347 (0.436) data 0.216 (0.305) loss_x loss_x 1.8672 (1.2302) acc_x 59.3750 (70.7812) lr 1.2334e-03 eta 0:00:00
epoch [87/200] batch [5/76] time 0.520 (0.455) data 0.388 (0.323) loss_u loss_u 0.9795 (0.9265) acc_u 0.0000 (8.7500) lr 1.2334e-03 eta 0:00:32
epoch [87/200] batch [10/76] time 0.469 (0.457) data 0.339 (0.326) loss_u loss_u 0.9795 (0.9426) acc_u 3.1250 (6.5625) lr 1.2334e-03 eta 0:00:30
epoch [87/200] batch [15/76] time 0.355 (0.463) data 0.224 (0.331) loss_u loss_u 0.8843 (0.9346) acc_u 12.5000 (7.7083) lr 1.2334e-03 eta 0:00:28
epoch [87/200] batch [20/76] time 0.407 (0.458) data 0.275 (0.326) loss_u loss_u 0.9556 (0.9364) acc_u 6.2500 (7.8125) lr 1.2334e-03 eta 0:00:25
epoch [87/200] batch [25/76] time 0.431 (0.456) data 0.299 (0.325) loss_u loss_u 0.8994 (0.9359) acc_u 12.5000 (8.2500) lr 1.2334e-03 eta 0:00:23
epoch [87/200] batch [30/76] time 0.413 (0.455) data 0.281 (0.324) loss_u loss_u 0.9829 (0.9376) acc_u 3.1250 (7.8125) lr 1.2334e-03 eta 0:00:20
epoch [87/200] batch [35/76] time 0.440 (0.456) data 0.308 (0.325) loss_u loss_u 0.9072 (0.9384) acc_u 12.5000 (7.7679) lr 1.2334e-03 eta 0:00:18
epoch [87/200] batch [40/76] time 0.370 (0.454) data 0.239 (0.322) loss_u loss_u 0.9497 (0.9390) acc_u 6.2500 (7.5781) lr 1.2334e-03 eta 0:00:16
epoch [87/200] batch [45/76] time 0.435 (0.453) data 0.303 (0.321) loss_u loss_u 0.9663 (0.9388) acc_u 3.1250 (7.7083) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [50/76] time 0.550 (0.460) data 0.418 (0.329) loss_u loss_u 0.9897 (0.9396) acc_u 0.0000 (7.5625) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [55/76] time 0.418 (0.459) data 0.286 (0.327) loss_u loss_u 0.9316 (0.9377) acc_u 6.2500 (7.7841) lr 1.2334e-03 eta 0:00:09
epoch [87/200] batch [60/76] time 0.365 (0.458) data 0.234 (0.327) loss_u loss_u 0.9800 (0.9387) acc_u 3.1250 (7.8646) lr 1.2334e-03 eta 0:00:07
epoch [87/200] batch [65/76] time 0.304 (0.453) data 0.173 (0.322) loss_u loss_u 0.9678 (0.9400) acc_u 3.1250 (7.7885) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [70/76] time 0.425 (0.453) data 0.294 (0.322) loss_u loss_u 0.9644 (0.9373) acc_u 6.2500 (8.0357) lr 1.2334e-03 eta 0:00:02
epoch [87/200] batch [75/76] time 0.414 (0.452) data 0.284 (0.321) loss_u loss_u 0.8696 (0.9333) acc_u 15.6250 (8.5000) lr 1.2334e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1601
confident_label rate tensor(0.2178, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 683
clean true:676
clean false:7
clean_rate:0.9897510980966325
noisy true:859
noisy false:1594
after delete: len(clean_dataset) 683
after delete: len(noisy_dataset) 2453
epoch [88/200] batch [5/21] time 0.409 (0.510) data 0.278 (0.379) loss_x loss_x 0.8564 (1.1939) acc_x 78.1250 (73.1250) lr 1.2181e-03 eta 0:00:08
epoch [88/200] batch [10/21] time 0.391 (0.502) data 0.261 (0.371) loss_x loss_x 0.9761 (1.2385) acc_x 75.0000 (71.5625) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [15/21] time 0.399 (0.487) data 0.268 (0.356) loss_x loss_x 1.4287 (1.2344) acc_x 68.7500 (71.6667) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [20/21] time 0.467 (0.487) data 0.336 (0.356) loss_x loss_x 1.0459 (1.1998) acc_x 65.6250 (71.5625) lr 1.2181e-03 eta 0:00:00
epoch [88/200] batch [5/76] time 0.436 (0.472) data 0.304 (0.340) loss_u loss_u 0.9639 (0.9502) acc_u 6.2500 (6.8750) lr 1.2181e-03 eta 0:00:33
epoch [88/200] batch [10/76] time 0.630 (0.476) data 0.498 (0.344) loss_u loss_u 0.9370 (0.9535) acc_u 6.2500 (5.6250) lr 1.2181e-03 eta 0:00:31
epoch [88/200] batch [15/76] time 0.610 (0.472) data 0.478 (0.340) loss_u loss_u 0.8921 (0.9514) acc_u 12.5000 (5.4167) lr 1.2181e-03 eta 0:00:28
epoch [88/200] batch [20/76] time 0.370 (0.465) data 0.239 (0.333) loss_u loss_u 0.9478 (0.9552) acc_u 6.2500 (5.0000) lr 1.2181e-03 eta 0:00:26
epoch [88/200] batch [25/76] time 0.381 (0.464) data 0.250 (0.332) loss_u loss_u 0.9380 (0.9523) acc_u 9.3750 (5.8750) lr 1.2181e-03 eta 0:00:23
epoch [88/200] batch [30/76] time 0.448 (0.461) data 0.317 (0.329) loss_u loss_u 0.9214 (0.9492) acc_u 12.5000 (6.6667) lr 1.2181e-03 eta 0:00:21
epoch [88/200] batch [35/76] time 0.436 (0.458) data 0.304 (0.326) loss_u loss_u 0.9429 (0.9476) acc_u 9.3750 (6.7857) lr 1.2181e-03 eta 0:00:18
epoch [88/200] batch [40/76] time 0.427 (0.453) data 0.296 (0.322) loss_u loss_u 0.9492 (0.9458) acc_u 6.2500 (7.1875) lr 1.2181e-03 eta 0:00:16
epoch [88/200] batch [45/76] time 0.508 (0.453) data 0.378 (0.322) loss_u loss_u 0.9521 (0.9467) acc_u 6.2500 (7.0139) lr 1.2181e-03 eta 0:00:14
epoch [88/200] batch [50/76] time 0.363 (0.452) data 0.230 (0.321) loss_u loss_u 0.9497 (0.9438) acc_u 9.3750 (7.5625) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [55/76] time 0.428 (0.449) data 0.296 (0.318) loss_u loss_u 0.9556 (0.9435) acc_u 6.2500 (7.5568) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [60/76] time 0.388 (0.447) data 0.256 (0.316) loss_u loss_u 0.9141 (0.9414) acc_u 12.5000 (7.6562) lr 1.2181e-03 eta 0:00:07
epoch [88/200] batch [65/76] time 0.381 (0.445) data 0.250 (0.313) loss_u loss_u 0.9565 (0.9426) acc_u 6.2500 (7.5962) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [70/76] time 0.453 (0.445) data 0.322 (0.313) loss_u loss_u 0.9648 (0.9419) acc_u 3.1250 (7.6786) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [75/76] time 0.512 (0.447) data 0.381 (0.316) loss_u loss_u 0.9302 (0.9427) acc_u 6.2500 (7.5000) lr 1.2181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1573
confident_label rate tensor(0.2159, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 677
clean true:666
clean false:11
clean_rate:0.983751846381093
noisy true:897
noisy false:1562
after delete: len(clean_dataset) 677
after delete: len(noisy_dataset) 2459
epoch [89/200] batch [5/21] time 0.545 (0.515) data 0.415 (0.385) loss_x loss_x 1.2207 (1.4172) acc_x 71.8750 (67.5000) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [10/21] time 0.486 (0.464) data 0.356 (0.334) loss_x loss_x 1.2295 (1.2590) acc_x 78.1250 (70.6250) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [15/21] time 0.523 (0.449) data 0.393 (0.318) loss_x loss_x 1.6318 (1.2965) acc_x 62.5000 (69.7917) lr 1.2028e-03 eta 0:00:02
epoch [89/200] batch [20/21] time 0.613 (0.448) data 0.482 (0.318) loss_x loss_x 1.2900 (1.2758) acc_x 68.7500 (69.5312) lr 1.2028e-03 eta 0:00:00
epoch [89/200] batch [5/76] time 0.482 (0.451) data 0.350 (0.320) loss_u loss_u 0.9360 (0.9384) acc_u 9.3750 (9.3750) lr 1.2028e-03 eta 0:00:32
epoch [89/200] batch [10/76] time 0.529 (0.454) data 0.398 (0.324) loss_u loss_u 0.8267 (0.9265) acc_u 21.8750 (10.6250) lr 1.2028e-03 eta 0:00:29
epoch [89/200] batch [15/76] time 0.540 (0.460) data 0.409 (0.329) loss_u loss_u 0.9888 (0.9338) acc_u 3.1250 (9.3750) lr 1.2028e-03 eta 0:00:28
epoch [89/200] batch [20/76] time 0.480 (0.451) data 0.349 (0.321) loss_u loss_u 0.9478 (0.9349) acc_u 6.2500 (8.9062) lr 1.2028e-03 eta 0:00:25
epoch [89/200] batch [25/76] time 0.411 (0.449) data 0.280 (0.318) loss_u loss_u 0.9858 (0.9307) acc_u 3.1250 (9.6250) lr 1.2028e-03 eta 0:00:22
epoch [89/200] batch [30/76] time 0.339 (0.443) data 0.207 (0.312) loss_u loss_u 0.9473 (0.9308) acc_u 6.2500 (9.5833) lr 1.2028e-03 eta 0:00:20
epoch [89/200] batch [35/76] time 0.383 (0.440) data 0.252 (0.309) loss_u loss_u 0.8936 (0.9317) acc_u 15.6250 (9.4643) lr 1.2028e-03 eta 0:00:18
epoch [89/200] batch [40/76] time 0.395 (0.439) data 0.264 (0.308) loss_u loss_u 0.9380 (0.9329) acc_u 15.6250 (9.2969) lr 1.2028e-03 eta 0:00:15
epoch [89/200] batch [45/76] time 0.721 (0.446) data 0.590 (0.315) loss_u loss_u 0.9336 (0.9318) acc_u 6.2500 (9.3750) lr 1.2028e-03 eta 0:00:13
epoch [89/200] batch [50/76] time 0.510 (0.448) data 0.379 (0.317) loss_u loss_u 0.9961 (0.9334) acc_u 0.0000 (9.0625) lr 1.2028e-03 eta 0:00:11
epoch [89/200] batch [55/76] time 0.427 (0.444) data 0.295 (0.313) loss_u loss_u 0.8994 (0.9327) acc_u 12.5000 (9.0909) lr 1.2028e-03 eta 0:00:09
epoch [89/200] batch [60/76] time 0.411 (0.444) data 0.280 (0.313) loss_u loss_u 0.9023 (0.9334) acc_u 12.5000 (8.9583) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [65/76] time 0.571 (0.445) data 0.440 (0.314) loss_u loss_u 0.8838 (0.9343) acc_u 15.6250 (8.7500) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [70/76] time 0.572 (0.447) data 0.440 (0.316) loss_u loss_u 0.9639 (0.9333) acc_u 3.1250 (8.6607) lr 1.2028e-03 eta 0:00:02
epoch [89/200] batch [75/76] time 0.395 (0.448) data 0.264 (0.317) loss_u loss_u 0.9575 (0.9355) acc_u 9.3750 (8.4583) lr 1.2028e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1608
confident_label rate tensor(0.2184, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 685
clean true:676
clean false:9
clean_rate:0.9868613138686131
noisy true:852
noisy false:1599
after delete: len(clean_dataset) 685
after delete: len(noisy_dataset) 2451
epoch [90/200] batch [5/21] time 0.359 (0.490) data 0.229 (0.359) loss_x loss_x 1.1406 (1.2571) acc_x 65.6250 (69.3750) lr 1.1874e-03 eta 0:00:07
epoch [90/200] batch [10/21] time 0.403 (0.485) data 0.272 (0.354) loss_x loss_x 1.2695 (1.1371) acc_x 71.8750 (70.9375) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [15/21] time 0.412 (0.466) data 0.280 (0.335) loss_x loss_x 0.8750 (1.1855) acc_x 78.1250 (70.0000) lr 1.1874e-03 eta 0:00:02
epoch [90/200] batch [20/21] time 0.406 (0.474) data 0.275 (0.343) loss_x loss_x 1.0166 (1.2015) acc_x 87.5000 (70.0000) lr 1.1874e-03 eta 0:00:00
epoch [90/200] batch [5/76] time 0.415 (0.471) data 0.283 (0.339) loss_u loss_u 0.9414 (0.9321) acc_u 3.1250 (6.8750) lr 1.1874e-03 eta 0:00:33
epoch [90/200] batch [10/76] time 0.472 (0.465) data 0.341 (0.334) loss_u loss_u 0.9106 (0.9332) acc_u 9.3750 (8.1250) lr 1.1874e-03 eta 0:00:30
epoch [90/200] batch [15/76] time 0.318 (0.453) data 0.187 (0.321) loss_u loss_u 0.9336 (0.9368) acc_u 12.5000 (7.9167) lr 1.1874e-03 eta 0:00:27
epoch [90/200] batch [20/76] time 0.440 (0.455) data 0.308 (0.324) loss_u loss_u 0.9775 (0.9427) acc_u 3.1250 (7.1875) lr 1.1874e-03 eta 0:00:25
epoch [90/200] batch [25/76] time 0.422 (0.453) data 0.291 (0.322) loss_u loss_u 0.8853 (0.9355) acc_u 15.6250 (8.3750) lr 1.1874e-03 eta 0:00:23
epoch [90/200] batch [30/76] time 0.502 (0.450) data 0.371 (0.319) loss_u loss_u 0.9424 (0.9373) acc_u 9.3750 (8.2292) lr 1.1874e-03 eta 0:00:20
epoch [90/200] batch [35/76] time 0.462 (0.451) data 0.329 (0.320) loss_u loss_u 0.9561 (0.9374) acc_u 3.1250 (8.0357) lr 1.1874e-03 eta 0:00:18
epoch [90/200] batch [40/76] time 0.405 (0.450) data 0.273 (0.319) loss_u loss_u 0.9746 (0.9388) acc_u 3.1250 (7.6562) lr 1.1874e-03 eta 0:00:16
epoch [90/200] batch [45/76] time 0.494 (0.453) data 0.362 (0.322) loss_u loss_u 0.8887 (0.9377) acc_u 9.3750 (7.7083) lr 1.1874e-03 eta 0:00:14
epoch [90/200] batch [50/76] time 0.462 (0.454) data 0.330 (0.323) loss_u loss_u 0.9395 (0.9386) acc_u 6.2500 (7.6875) lr 1.1874e-03 eta 0:00:11
epoch [90/200] batch [55/76] time 0.377 (0.453) data 0.247 (0.321) loss_u loss_u 0.9531 (0.9399) acc_u 6.2500 (7.5568) lr 1.1874e-03 eta 0:00:09
epoch [90/200] batch [60/76] time 0.398 (0.449) data 0.266 (0.318) loss_u loss_u 0.9614 (0.9417) acc_u 6.2500 (7.3958) lr 1.1874e-03 eta 0:00:07
epoch [90/200] batch [65/76] time 0.385 (0.447) data 0.253 (0.316) loss_u loss_u 0.9126 (0.9408) acc_u 9.3750 (7.6442) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [70/76] time 0.372 (0.448) data 0.240 (0.317) loss_u loss_u 0.9326 (0.9410) acc_u 6.2500 (7.5446) lr 1.1874e-03 eta 0:00:02
epoch [90/200] batch [75/76] time 0.458 (0.449) data 0.327 (0.317) loss_u loss_u 0.9092 (0.9418) acc_u 12.5000 (7.5000) lr 1.1874e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1611
confident_label rate tensor(0.2124, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 666
clean true:659
clean false:7
clean_rate:0.9894894894894894
noisy true:866
noisy false:1604
after delete: len(clean_dataset) 666
after delete: len(noisy_dataset) 2470
epoch [91/200] batch [5/20] time 0.515 (0.472) data 0.384 (0.342) loss_x loss_x 1.3721 (1.0705) acc_x 65.6250 (73.7500) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [10/20] time 0.394 (0.482) data 0.264 (0.351) loss_x loss_x 0.8682 (1.2059) acc_x 68.7500 (70.6250) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [15/20] time 0.577 (0.505) data 0.445 (0.374) loss_x loss_x 0.6787 (1.1554) acc_x 81.2500 (71.4583) lr 1.1719e-03 eta 0:00:02
epoch [91/200] batch [20/20] time 0.473 (0.491) data 0.339 (0.360) loss_x loss_x 1.1670 (1.1624) acc_x 65.6250 (70.7812) lr 1.1719e-03 eta 0:00:00
epoch [91/200] batch [5/77] time 0.451 (0.489) data 0.318 (0.357) loss_u loss_u 0.9482 (0.9524) acc_u 6.2500 (4.3750) lr 1.1719e-03 eta 0:00:35
epoch [91/200] batch [10/77] time 0.447 (0.481) data 0.315 (0.349) loss_u loss_u 0.9106 (0.9508) acc_u 12.5000 (5.9375) lr 1.1719e-03 eta 0:00:32
epoch [91/200] batch [15/77] time 0.529 (0.478) data 0.397 (0.346) loss_u loss_u 0.9219 (0.9504) acc_u 15.6250 (6.6667) lr 1.1719e-03 eta 0:00:29
epoch [91/200] batch [20/77] time 0.355 (0.470) data 0.223 (0.338) loss_u loss_u 0.9668 (0.9418) acc_u 3.1250 (7.6562) lr 1.1719e-03 eta 0:00:26
epoch [91/200] batch [25/77] time 0.485 (0.468) data 0.353 (0.336) loss_u loss_u 0.8921 (0.9349) acc_u 15.6250 (8.6250) lr 1.1719e-03 eta 0:00:24
epoch [91/200] batch [30/77] time 0.363 (0.468) data 0.233 (0.336) loss_u loss_u 0.9443 (0.9344) acc_u 12.5000 (8.8542) lr 1.1719e-03 eta 0:00:21
epoch [91/200] batch [35/77] time 0.560 (0.467) data 0.430 (0.335) loss_u loss_u 0.9604 (0.9388) acc_u 3.1250 (8.0357) lr 1.1719e-03 eta 0:00:19
epoch [91/200] batch [40/77] time 0.422 (0.464) data 0.290 (0.333) loss_u loss_u 0.9849 (0.9388) acc_u 0.0000 (7.9688) lr 1.1719e-03 eta 0:00:17
epoch [91/200] batch [45/77] time 0.421 (0.463) data 0.289 (0.331) loss_u loss_u 0.9434 (0.9382) acc_u 12.5000 (8.1944) lr 1.1719e-03 eta 0:00:14
epoch [91/200] batch [50/77] time 0.405 (0.464) data 0.272 (0.332) loss_u loss_u 0.9282 (0.9349) acc_u 9.3750 (8.5625) lr 1.1719e-03 eta 0:00:12
epoch [91/200] batch [55/77] time 0.470 (0.465) data 0.339 (0.334) loss_u loss_u 0.9790 (0.9353) acc_u 6.2500 (8.6932) lr 1.1719e-03 eta 0:00:10
epoch [91/200] batch [60/77] time 0.469 (0.466) data 0.339 (0.334) loss_u loss_u 0.9160 (0.9360) acc_u 15.6250 (8.6458) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [65/77] time 0.407 (0.464) data 0.277 (0.332) loss_u loss_u 0.9443 (0.9362) acc_u 9.3750 (8.6058) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [70/77] time 0.385 (0.463) data 0.254 (0.331) loss_u loss_u 0.9561 (0.9383) acc_u 6.2500 (8.2589) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [75/77] time 0.339 (0.462) data 0.209 (0.330) loss_u loss_u 0.9590 (0.9375) acc_u 9.3750 (8.4167) lr 1.1719e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1585
confident_label rate tensor(0.2213, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 694
clean true:681
clean false:13
clean_rate:0.9812680115273775
noisy true:870
noisy false:1572
after delete: len(clean_dataset) 694
after delete: len(noisy_dataset) 2442
epoch [92/200] batch [5/21] time 0.372 (0.469) data 0.241 (0.338) loss_x loss_x 1.5049 (1.3330) acc_x 62.5000 (70.6250) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [10/21] time 0.525 (0.473) data 0.394 (0.342) loss_x loss_x 1.3164 (1.2881) acc_x 71.8750 (70.6250) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [15/21] time 0.422 (0.473) data 0.291 (0.342) loss_x loss_x 0.9692 (1.1941) acc_x 78.1250 (72.5000) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [20/21] time 0.620 (0.475) data 0.490 (0.344) loss_x loss_x 1.1318 (1.2041) acc_x 78.1250 (71.5625) lr 1.1564e-03 eta 0:00:00
epoch [92/200] batch [5/76] time 0.559 (0.497) data 0.424 (0.366) loss_u loss_u 0.9678 (0.9360) acc_u 3.1250 (8.1250) lr 1.1564e-03 eta 0:00:35
epoch [92/200] batch [10/76] time 0.422 (0.487) data 0.290 (0.355) loss_u loss_u 0.9839 (0.9480) acc_u 0.0000 (6.5625) lr 1.1564e-03 eta 0:00:32
epoch [92/200] batch [15/76] time 0.477 (0.480) data 0.347 (0.349) loss_u loss_u 0.9551 (0.9486) acc_u 9.3750 (6.2500) lr 1.1564e-03 eta 0:00:29
epoch [92/200] batch [20/76] time 0.356 (0.476) data 0.225 (0.345) loss_u loss_u 0.9800 (0.9475) acc_u 3.1250 (6.7188) lr 1.1564e-03 eta 0:00:26
epoch [92/200] batch [25/76] time 0.359 (0.470) data 0.227 (0.339) loss_u loss_u 0.9028 (0.9424) acc_u 15.6250 (7.5000) lr 1.1564e-03 eta 0:00:23
epoch [92/200] batch [30/76] time 0.451 (0.470) data 0.320 (0.338) loss_u loss_u 0.9512 (0.9433) acc_u 6.2500 (7.3958) lr 1.1564e-03 eta 0:00:21
epoch [92/200] batch [35/76] time 0.454 (0.468) data 0.323 (0.337) loss_u loss_u 0.9570 (0.9445) acc_u 6.2500 (7.2321) lr 1.1564e-03 eta 0:00:19
epoch [92/200] batch [40/76] time 0.540 (0.468) data 0.410 (0.336) loss_u loss_u 0.9297 (0.9453) acc_u 9.3750 (7.2656) lr 1.1564e-03 eta 0:00:16
epoch [92/200] batch [45/76] time 0.546 (0.468) data 0.415 (0.337) loss_u loss_u 0.9038 (0.9435) acc_u 12.5000 (7.5694) lr 1.1564e-03 eta 0:00:14
epoch [92/200] batch [50/76] time 0.412 (0.467) data 0.281 (0.336) loss_u loss_u 0.9766 (0.9448) acc_u 3.1250 (7.3125) lr 1.1564e-03 eta 0:00:12
epoch [92/200] batch [55/76] time 0.540 (0.467) data 0.408 (0.336) loss_u loss_u 0.9883 (0.9456) acc_u 0.0000 (7.2159) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [60/76] time 0.414 (0.464) data 0.284 (0.333) loss_u loss_u 0.9663 (0.9448) acc_u 3.1250 (7.2917) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [65/76] time 0.388 (0.463) data 0.257 (0.332) loss_u loss_u 0.9673 (0.9444) acc_u 6.2500 (7.4519) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [70/76] time 0.469 (0.464) data 0.337 (0.333) loss_u loss_u 0.9771 (0.9445) acc_u 3.1250 (7.3661) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [75/76] time 0.414 (0.462) data 0.283 (0.331) loss_u loss_u 0.9204 (0.9451) acc_u 12.5000 (7.2083) lr 1.1564e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1610
confident_label rate tensor(0.2152, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 675
clean true:664
clean false:11
clean_rate:0.9837037037037037
noisy true:862
noisy false:1599
after delete: len(clean_dataset) 675
after delete: len(noisy_dataset) 2461
epoch [93/200] batch [5/21] time 0.440 (0.492) data 0.310 (0.361) loss_x loss_x 1.2480 (1.3877) acc_x 68.7500 (66.8750) lr 1.1409e-03 eta 0:00:07
epoch [93/200] batch [10/21] time 0.596 (0.477) data 0.466 (0.347) loss_x loss_x 1.1230 (1.3399) acc_x 65.6250 (67.1875) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [15/21] time 0.435 (0.461) data 0.305 (0.330) loss_x loss_x 1.2158 (1.2569) acc_x 75.0000 (69.7917) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [20/21] time 0.382 (0.464) data 0.252 (0.334) loss_x loss_x 1.0361 (1.2719) acc_x 78.1250 (70.1562) lr 1.1409e-03 eta 0:00:00
epoch [93/200] batch [5/76] time 0.399 (0.459) data 0.267 (0.328) loss_u loss_u 0.9609 (0.9667) acc_u 6.2500 (5.0000) lr 1.1409e-03 eta 0:00:32
epoch [93/200] batch [10/76] time 0.468 (0.452) data 0.337 (0.321) loss_u loss_u 0.9209 (0.9446) acc_u 9.3750 (7.8125) lr 1.1409e-03 eta 0:00:29
epoch [93/200] batch [15/76] time 0.403 (0.447) data 0.271 (0.316) loss_u loss_u 0.9971 (0.9499) acc_u 0.0000 (6.6667) lr 1.1409e-03 eta 0:00:27
epoch [93/200] batch [20/76] time 0.365 (0.447) data 0.233 (0.316) loss_u loss_u 0.9600 (0.9499) acc_u 6.2500 (6.7188) lr 1.1409e-03 eta 0:00:25
epoch [93/200] batch [25/76] time 0.465 (0.449) data 0.333 (0.318) loss_u loss_u 0.9209 (0.9462) acc_u 12.5000 (7.2500) lr 1.1409e-03 eta 0:00:22
epoch [93/200] batch [30/76] time 0.444 (0.448) data 0.313 (0.317) loss_u loss_u 0.8965 (0.9429) acc_u 18.7500 (7.6042) lr 1.1409e-03 eta 0:00:20
epoch [93/200] batch [35/76] time 0.435 (0.454) data 0.303 (0.323) loss_u loss_u 0.8779 (0.9386) acc_u 18.7500 (8.1250) lr 1.1409e-03 eta 0:00:18
epoch [93/200] batch [40/76] time 0.397 (0.451) data 0.266 (0.320) loss_u loss_u 0.8735 (0.9372) acc_u 21.8750 (8.3594) lr 1.1409e-03 eta 0:00:16
epoch [93/200] batch [45/76] time 0.413 (0.449) data 0.281 (0.317) loss_u loss_u 0.9604 (0.9409) acc_u 3.1250 (7.9167) lr 1.1409e-03 eta 0:00:13
epoch [93/200] batch [50/76] time 0.399 (0.454) data 0.267 (0.322) loss_u loss_u 0.9585 (0.9403) acc_u 9.3750 (8.0000) lr 1.1409e-03 eta 0:00:11
epoch [93/200] batch [55/76] time 0.479 (0.453) data 0.347 (0.321) loss_u loss_u 0.9346 (0.9405) acc_u 9.3750 (8.0114) lr 1.1409e-03 eta 0:00:09
epoch [93/200] batch [60/76] time 0.426 (0.456) data 0.294 (0.325) loss_u loss_u 0.9595 (0.9407) acc_u 6.2500 (7.9167) lr 1.1409e-03 eta 0:00:07
epoch [93/200] batch [65/76] time 0.366 (0.458) data 0.235 (0.327) loss_u loss_u 0.9307 (0.9415) acc_u 9.3750 (7.8846) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [70/76] time 0.376 (0.459) data 0.244 (0.327) loss_u loss_u 0.9683 (0.9404) acc_u 3.1250 (7.9464) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [75/76] time 0.442 (0.459) data 0.310 (0.328) loss_u loss_u 0.9316 (0.9407) acc_u 9.3750 (7.8333) lr 1.1409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1594
confident_label rate tensor(0.2188, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 686
clean true:676
clean false:10
clean_rate:0.9854227405247813
noisy true:866
noisy false:1584
after delete: len(clean_dataset) 686
after delete: len(noisy_dataset) 2450
epoch [94/200] batch [5/21] time 0.359 (0.493) data 0.229 (0.363) loss_x loss_x 1.5557 (1.3717) acc_x 71.8750 (66.8750) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [10/21] time 0.456 (0.474) data 0.326 (0.343) loss_x loss_x 0.8765 (1.2979) acc_x 75.0000 (68.4375) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [15/21] time 0.421 (0.467) data 0.291 (0.336) loss_x loss_x 1.0107 (1.2898) acc_x 62.5000 (67.0833) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [20/21] time 0.615 (0.470) data 0.483 (0.339) loss_x loss_x 1.0723 (1.2701) acc_x 65.6250 (67.6562) lr 1.1253e-03 eta 0:00:00
epoch [94/200] batch [5/76] time 0.437 (0.475) data 0.305 (0.343) loss_u loss_u 0.9253 (0.9301) acc_u 6.2500 (8.1250) lr 1.1253e-03 eta 0:00:33
epoch [94/200] batch [10/76] time 0.385 (0.465) data 0.254 (0.334) loss_u loss_u 0.9805 (0.9208) acc_u 6.2500 (10.3125) lr 1.1253e-03 eta 0:00:30
epoch [94/200] batch [15/76] time 0.429 (0.474) data 0.298 (0.343) loss_u loss_u 0.9385 (0.9286) acc_u 6.2500 (9.1667) lr 1.1253e-03 eta 0:00:28
epoch [94/200] batch [20/76] time 0.468 (0.467) data 0.338 (0.335) loss_u loss_u 0.8940 (0.9321) acc_u 12.5000 (8.2812) lr 1.1253e-03 eta 0:00:26
epoch [94/200] batch [25/76] time 0.474 (0.460) data 0.343 (0.329) loss_u loss_u 0.9209 (0.9301) acc_u 12.5000 (8.6250) lr 1.1253e-03 eta 0:00:23
epoch [94/200] batch [30/76] time 0.438 (0.455) data 0.307 (0.324) loss_u loss_u 0.9160 (0.9309) acc_u 9.3750 (8.6458) lr 1.1253e-03 eta 0:00:20
epoch [94/200] batch [35/76] time 0.393 (0.452) data 0.263 (0.321) loss_u loss_u 0.9546 (0.9331) acc_u 6.2500 (8.3929) lr 1.1253e-03 eta 0:00:18
epoch [94/200] batch [40/76] time 0.325 (0.454) data 0.193 (0.322) loss_u loss_u 0.9268 (0.9347) acc_u 9.3750 (8.1250) lr 1.1253e-03 eta 0:00:16
epoch [94/200] batch [45/76] time 0.543 (0.458) data 0.412 (0.326) loss_u loss_u 0.9917 (0.9343) acc_u 0.0000 (8.3333) lr 1.1253e-03 eta 0:00:14
epoch [94/200] batch [50/76] time 0.436 (0.456) data 0.304 (0.325) loss_u loss_u 0.9189 (0.9358) acc_u 9.3750 (8.1250) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [55/76] time 0.410 (0.452) data 0.278 (0.320) loss_u loss_u 0.9712 (0.9373) acc_u 3.1250 (7.9545) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [60/76] time 0.450 (0.449) data 0.319 (0.318) loss_u loss_u 0.9487 (0.9385) acc_u 6.2500 (7.6562) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [65/76] time 0.407 (0.450) data 0.277 (0.318) loss_u loss_u 0.9009 (0.9381) acc_u 15.6250 (7.6923) lr 1.1253e-03 eta 0:00:04
epoch [94/200] batch [70/76] time 0.441 (0.450) data 0.309 (0.319) loss_u loss_u 0.9375 (0.9389) acc_u 6.2500 (7.6339) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [75/76] time 0.452 (0.452) data 0.319 (0.320) loss_u loss_u 0.9331 (0.9403) acc_u 9.3750 (7.5000) lr 1.1253e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1604
confident_label rate tensor(0.2172, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 681
clean true:671
clean false:10
clean_rate:0.9853157121879589
noisy true:861
noisy false:1594
after delete: len(clean_dataset) 681
after delete: len(noisy_dataset) 2455
epoch [95/200] batch [5/21] time 0.533 (0.503) data 0.402 (0.372) loss_x loss_x 1.4336 (0.9926) acc_x 65.6250 (74.3750) lr 1.1097e-03 eta 0:00:08
epoch [95/200] batch [10/21] time 0.446 (0.511) data 0.316 (0.380) loss_x loss_x 0.8525 (1.1448) acc_x 75.0000 (71.2500) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [15/21] time 0.449 (0.497) data 0.318 (0.366) loss_x loss_x 1.0840 (1.1146) acc_x 71.8750 (72.5000) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [20/21] time 0.352 (0.482) data 0.221 (0.352) loss_x loss_x 0.7861 (1.1410) acc_x 87.5000 (72.5000) lr 1.1097e-03 eta 0:00:00
epoch [95/200] batch [5/76] time 0.345 (0.469) data 0.214 (0.338) loss_u loss_u 0.9419 (0.9374) acc_u 6.2500 (7.5000) lr 1.1097e-03 eta 0:00:33
epoch [95/200] batch [10/76] time 0.353 (0.458) data 0.222 (0.327) loss_u loss_u 0.9170 (0.9410) acc_u 12.5000 (7.1875) lr 1.1097e-03 eta 0:00:30
epoch [95/200] batch [15/76] time 0.481 (0.457) data 0.349 (0.326) loss_u loss_u 0.9351 (0.9355) acc_u 12.5000 (8.5417) lr 1.1097e-03 eta 0:00:27
epoch [95/200] batch [20/76] time 0.559 (0.459) data 0.428 (0.328) loss_u loss_u 0.9575 (0.9308) acc_u 3.1250 (9.0625) lr 1.1097e-03 eta 0:00:25
epoch [95/200] batch [25/76] time 0.463 (0.455) data 0.332 (0.324) loss_u loss_u 0.9014 (0.9344) acc_u 9.3750 (8.3750) lr 1.1097e-03 eta 0:00:23
epoch [95/200] batch [30/76] time 0.460 (0.452) data 0.329 (0.321) loss_u loss_u 0.9341 (0.9328) acc_u 9.3750 (8.7500) lr 1.1097e-03 eta 0:00:20
epoch [95/200] batch [35/76] time 0.511 (0.456) data 0.379 (0.325) loss_u loss_u 0.8354 (0.9327) acc_u 28.1250 (8.9286) lr 1.1097e-03 eta 0:00:18
epoch [95/200] batch [40/76] time 0.345 (0.454) data 0.212 (0.323) loss_u loss_u 0.9189 (0.9303) acc_u 9.3750 (9.1406) lr 1.1097e-03 eta 0:00:16
epoch [95/200] batch [45/76] time 0.414 (0.452) data 0.282 (0.321) loss_u loss_u 0.9614 (0.9300) acc_u 6.2500 (9.0972) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [50/76] time 0.281 (0.449) data 0.150 (0.317) loss_u loss_u 0.8872 (0.9306) acc_u 18.7500 (9.2500) lr 1.1097e-03 eta 0:00:11
epoch [95/200] batch [55/76] time 0.430 (0.453) data 0.299 (0.321) loss_u loss_u 0.9326 (0.9308) acc_u 9.3750 (9.2045) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [60/76] time 0.404 (0.451) data 0.272 (0.320) loss_u loss_u 0.9526 (0.9317) acc_u 9.3750 (9.1667) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [65/76] time 0.405 (0.453) data 0.274 (0.322) loss_u loss_u 0.9526 (0.9337) acc_u 6.2500 (8.8942) lr 1.1097e-03 eta 0:00:04
epoch [95/200] batch [70/76] time 0.391 (0.449) data 0.259 (0.318) loss_u loss_u 0.9575 (0.9345) acc_u 6.2500 (8.7054) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [75/76] time 0.399 (0.447) data 0.266 (0.315) loss_u loss_u 0.9600 (0.9342) acc_u 3.1250 (8.6250) lr 1.1097e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1576
confident_label rate tensor(0.2239, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 702
clean true:689
clean false:13
clean_rate:0.9814814814814815
noisy true:871
noisy false:1563
after delete: len(clean_dataset) 702
after delete: len(noisy_dataset) 2434
epoch [96/200] batch [5/21] time 0.468 (0.423) data 0.338 (0.293) loss_x loss_x 1.6982 (1.2802) acc_x 56.2500 (68.1250) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [10/21] time 0.362 (0.443) data 0.231 (0.312) loss_x loss_x 1.5820 (1.2448) acc_x 59.3750 (68.4375) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [15/21] time 0.400 (0.450) data 0.269 (0.319) loss_x loss_x 1.5332 (1.2968) acc_x 56.2500 (68.5417) lr 1.0941e-03 eta 0:00:02
epoch [96/200] batch [20/21] time 0.506 (0.460) data 0.375 (0.330) loss_x loss_x 1.7676 (1.2952) acc_x 62.5000 (68.5938) lr 1.0941e-03 eta 0:00:00
epoch [96/200] batch [5/76] time 0.376 (0.465) data 0.245 (0.334) loss_u loss_u 0.9409 (0.9513) acc_u 6.2500 (6.2500) lr 1.0941e-03 eta 0:00:33
epoch [96/200] batch [10/76] time 0.400 (0.457) data 0.269 (0.326) loss_u loss_u 0.9434 (0.9400) acc_u 9.3750 (8.1250) lr 1.0941e-03 eta 0:00:30
epoch [96/200] batch [15/76] time 0.376 (0.452) data 0.246 (0.321) loss_u loss_u 0.9204 (0.9406) acc_u 12.5000 (7.9167) lr 1.0941e-03 eta 0:00:27
epoch [96/200] batch [20/76] time 0.442 (0.450) data 0.311 (0.319) loss_u loss_u 0.9849 (0.9417) acc_u 3.1250 (8.2812) lr 1.0941e-03 eta 0:00:25
epoch [96/200] batch [25/76] time 0.352 (0.446) data 0.220 (0.314) loss_u loss_u 0.9673 (0.9413) acc_u 3.1250 (8.1250) lr 1.0941e-03 eta 0:00:22
epoch [96/200] batch [30/76] time 0.469 (0.448) data 0.338 (0.316) loss_u loss_u 0.9541 (0.9413) acc_u 6.2500 (8.1250) lr 1.0941e-03 eta 0:00:20
epoch [96/200] batch [35/76] time 0.360 (0.445) data 0.229 (0.314) loss_u loss_u 0.9834 (0.9408) acc_u 3.1250 (8.2143) lr 1.0941e-03 eta 0:00:18
epoch [96/200] batch [40/76] time 0.462 (0.447) data 0.330 (0.316) loss_u loss_u 0.9600 (0.9430) acc_u 3.1250 (7.7344) lr 1.0941e-03 eta 0:00:16
epoch [96/200] batch [45/76] time 0.416 (0.447) data 0.284 (0.315) loss_u loss_u 0.9131 (0.9434) acc_u 9.3750 (7.5694) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [50/76] time 0.365 (0.445) data 0.234 (0.314) loss_u loss_u 0.9873 (0.9454) acc_u 3.1250 (7.3750) lr 1.0941e-03 eta 0:00:11
epoch [96/200] batch [55/76] time 0.317 (0.443) data 0.185 (0.312) loss_u loss_u 0.9429 (0.9446) acc_u 6.2500 (7.3864) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [60/76] time 0.472 (0.445) data 0.340 (0.314) loss_u loss_u 0.9194 (0.9421) acc_u 15.6250 (7.7604) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [65/76] time 0.379 (0.444) data 0.248 (0.312) loss_u loss_u 0.9355 (0.9417) acc_u 9.3750 (7.9327) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [70/76] time 0.443 (0.448) data 0.311 (0.316) loss_u loss_u 0.9360 (0.9405) acc_u 6.2500 (7.9464) lr 1.0941e-03 eta 0:00:02
epoch [96/200] batch [75/76] time 0.518 (0.448) data 0.386 (0.317) loss_u loss_u 0.9844 (0.9411) acc_u 3.1250 (7.8333) lr 1.0941e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1562
confident_label rate tensor(0.2152, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 675
clean true:667
clean false:8
clean_rate:0.9881481481481481
noisy true:907
noisy false:1554
after delete: len(clean_dataset) 675
after delete: len(noisy_dataset) 2461
epoch [97/200] batch [5/21] time 0.376 (0.471) data 0.245 (0.340) loss_x loss_x 0.8911 (1.0209) acc_x 75.0000 (75.0000) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [10/21] time 0.382 (0.469) data 0.252 (0.339) loss_x loss_x 1.2842 (1.1558) acc_x 71.8750 (71.2500) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [15/21] time 0.368 (0.447) data 0.238 (0.317) loss_x loss_x 1.0400 (1.1700) acc_x 78.1250 (71.2500) lr 1.0785e-03 eta 0:00:02
epoch [97/200] batch [20/21] time 0.376 (0.445) data 0.246 (0.314) loss_x loss_x 1.3066 (1.1690) acc_x 62.5000 (70.4688) lr 1.0785e-03 eta 0:00:00
epoch [97/200] batch [5/76] time 0.506 (0.451) data 0.376 (0.320) loss_u loss_u 0.9448 (0.9511) acc_u 6.2500 (6.8750) lr 1.0785e-03 eta 0:00:32
epoch [97/200] batch [10/76] time 0.492 (0.448) data 0.360 (0.318) loss_u loss_u 0.9390 (0.9418) acc_u 6.2500 (8.1250) lr 1.0785e-03 eta 0:00:29
epoch [97/200] batch [15/76] time 0.528 (0.447) data 0.396 (0.317) loss_u loss_u 0.9014 (0.9378) acc_u 15.6250 (8.7500) lr 1.0785e-03 eta 0:00:27
epoch [97/200] batch [20/76] time 0.342 (0.446) data 0.211 (0.315) loss_u loss_u 0.9756 (0.9411) acc_u 3.1250 (8.2812) lr 1.0785e-03 eta 0:00:24
epoch [97/200] batch [25/76] time 0.431 (0.445) data 0.299 (0.315) loss_u loss_u 0.9409 (0.9426) acc_u 6.2500 (7.8750) lr 1.0785e-03 eta 0:00:22
epoch [97/200] batch [30/76] time 0.424 (0.448) data 0.293 (0.317) loss_u loss_u 0.9199 (0.9399) acc_u 6.2500 (8.1250) lr 1.0785e-03 eta 0:00:20
epoch [97/200] batch [35/76] time 0.357 (0.449) data 0.227 (0.318) loss_u loss_u 0.9097 (0.9390) acc_u 9.3750 (8.0357) lr 1.0785e-03 eta 0:00:18
epoch [97/200] batch [40/76] time 0.410 (0.447) data 0.277 (0.316) loss_u loss_u 0.9258 (0.9372) acc_u 9.3750 (8.3594) lr 1.0785e-03 eta 0:00:16
epoch [97/200] batch [45/76] time 0.516 (0.451) data 0.385 (0.320) loss_u loss_u 0.9429 (0.9365) acc_u 6.2500 (8.4028) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [50/76] time 0.494 (0.452) data 0.364 (0.321) loss_u loss_u 0.9302 (0.9383) acc_u 9.3750 (8.1250) lr 1.0785e-03 eta 0:00:11
epoch [97/200] batch [55/76] time 0.458 (0.448) data 0.328 (0.317) loss_u loss_u 0.9863 (0.9395) acc_u 3.1250 (8.0682) lr 1.0785e-03 eta 0:00:09
epoch [97/200] batch [60/76] time 0.490 (0.451) data 0.360 (0.320) loss_u loss_u 0.9219 (0.9390) acc_u 9.3750 (8.0729) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [65/76] time 0.437 (0.449) data 0.306 (0.318) loss_u loss_u 0.9585 (0.9377) acc_u 6.2500 (8.2212) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [70/76] time 0.439 (0.450) data 0.308 (0.319) loss_u loss_u 0.8701 (0.9369) acc_u 15.6250 (8.1696) lr 1.0785e-03 eta 0:00:02
epoch [97/200] batch [75/76] time 0.407 (0.448) data 0.277 (0.316) loss_u loss_u 0.9468 (0.9378) acc_u 3.1250 (8.0417) lr 1.0785e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1580
confident_label rate tensor(0.2191, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 687
clean true:678
clean false:9
clean_rate:0.9868995633187773
noisy true:878
noisy false:1571
after delete: len(clean_dataset) 687
after delete: len(noisy_dataset) 2449
epoch [98/200] batch [5/21] time 0.421 (0.409) data 0.290 (0.278) loss_x loss_x 0.8286 (1.0756) acc_x 71.8750 (71.2500) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [10/21] time 0.743 (0.446) data 0.612 (0.316) loss_x loss_x 1.5625 (1.3028) acc_x 59.3750 (66.2500) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [15/21] time 0.446 (0.450) data 0.315 (0.319) loss_x loss_x 1.1191 (1.3279) acc_x 75.0000 (67.2917) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [20/21] time 0.380 (0.443) data 0.250 (0.312) loss_x loss_x 1.6846 (1.3265) acc_x 62.5000 (67.8125) lr 1.0628e-03 eta 0:00:00
epoch [98/200] batch [5/76] time 0.409 (0.456) data 0.278 (0.326) loss_u loss_u 0.9526 (0.9525) acc_u 6.2500 (6.2500) lr 1.0628e-03 eta 0:00:32
epoch [98/200] batch [10/76] time 0.510 (0.453) data 0.379 (0.323) loss_u loss_u 0.9434 (0.9537) acc_u 6.2500 (6.5625) lr 1.0628e-03 eta 0:00:29
epoch [98/200] batch [15/76] time 0.369 (0.448) data 0.237 (0.317) loss_u loss_u 0.9487 (0.9475) acc_u 3.1250 (6.6667) lr 1.0628e-03 eta 0:00:27
epoch [98/200] batch [20/76] time 0.412 (0.443) data 0.280 (0.313) loss_u loss_u 0.9258 (0.9439) acc_u 9.3750 (6.8750) lr 1.0628e-03 eta 0:00:24
epoch [98/200] batch [25/76] time 0.459 (0.441) data 0.328 (0.310) loss_u loss_u 0.9917 (0.9446) acc_u 0.0000 (7.1250) lr 1.0628e-03 eta 0:00:22
epoch [98/200] batch [30/76] time 0.401 (0.440) data 0.270 (0.309) loss_u loss_u 0.9492 (0.9440) acc_u 6.2500 (7.2917) lr 1.0628e-03 eta 0:00:20
epoch [98/200] batch [35/76] time 0.297 (0.438) data 0.167 (0.307) loss_u loss_u 0.9512 (0.9452) acc_u 9.3750 (7.3214) lr 1.0628e-03 eta 0:00:17
epoch [98/200] batch [40/76] time 0.487 (0.441) data 0.356 (0.310) loss_u loss_u 0.9189 (0.9464) acc_u 6.2500 (7.0312) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [45/76] time 0.422 (0.443) data 0.290 (0.312) loss_u loss_u 0.9399 (0.9444) acc_u 12.5000 (7.4306) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [50/76] time 0.492 (0.444) data 0.360 (0.313) loss_u loss_u 0.9448 (0.9431) acc_u 6.2500 (7.4375) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [55/76] time 0.371 (0.447) data 0.239 (0.316) loss_u loss_u 0.9595 (0.9428) acc_u 0.0000 (7.2159) lr 1.0628e-03 eta 0:00:09
epoch [98/200] batch [60/76] time 0.378 (0.445) data 0.247 (0.314) loss_u loss_u 0.9609 (0.9444) acc_u 3.1250 (6.9792) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [65/76] time 0.388 (0.443) data 0.256 (0.312) loss_u loss_u 0.8677 (0.9437) acc_u 18.7500 (7.2115) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [70/76] time 0.484 (0.441) data 0.353 (0.310) loss_u loss_u 0.9282 (0.9438) acc_u 9.3750 (7.2321) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [75/76] time 0.468 (0.443) data 0.337 (0.312) loss_u loss_u 0.9746 (0.9445) acc_u 3.1250 (7.1667) lr 1.0628e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1571
confident_label rate tensor(0.2207, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 692
clean true:681
clean false:11
clean_rate:0.9841040462427746
noisy true:884
noisy false:1560
after delete: len(clean_dataset) 692
after delete: len(noisy_dataset) 2444
epoch [99/200] batch [5/21] time 0.611 (0.542) data 0.481 (0.411) loss_x loss_x 1.2236 (1.1830) acc_x 62.5000 (71.8750) lr 1.0471e-03 eta 0:00:08
epoch [99/200] batch [10/21] time 0.464 (0.479) data 0.333 (0.349) loss_x loss_x 1.1201 (1.2710) acc_x 75.0000 (70.9375) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [15/21] time 0.445 (0.469) data 0.315 (0.339) loss_x loss_x 1.5771 (1.2696) acc_x 62.5000 (69.3750) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [20/21] time 0.376 (0.475) data 0.244 (0.344) loss_x loss_x 0.7773 (1.1703) acc_x 87.5000 (71.7188) lr 1.0471e-03 eta 0:00:00
epoch [99/200] batch [5/76] time 0.463 (0.468) data 0.332 (0.338) loss_u loss_u 0.9189 (0.9436) acc_u 6.2500 (6.8750) lr 1.0471e-03 eta 0:00:33
epoch [99/200] batch [10/76] time 0.395 (0.455) data 0.264 (0.325) loss_u loss_u 0.8916 (0.9328) acc_u 12.5000 (8.1250) lr 1.0471e-03 eta 0:00:30
epoch [99/200] batch [15/76] time 0.498 (0.454) data 0.368 (0.323) loss_u loss_u 0.9307 (0.9381) acc_u 12.5000 (8.1250) lr 1.0471e-03 eta 0:00:27
epoch [99/200] batch [20/76] time 0.435 (0.453) data 0.303 (0.323) loss_u loss_u 0.9897 (0.9428) acc_u 0.0000 (7.5000) lr 1.0471e-03 eta 0:00:25
epoch [99/200] batch [25/76] time 0.354 (0.454) data 0.224 (0.323) loss_u loss_u 0.9185 (0.9392) acc_u 15.6250 (8.6250) lr 1.0471e-03 eta 0:00:23
epoch [99/200] batch [30/76] time 0.428 (0.453) data 0.297 (0.322) loss_u loss_u 0.9370 (0.9383) acc_u 6.2500 (8.6458) lr 1.0471e-03 eta 0:00:20
epoch [99/200] batch [35/76] time 0.509 (0.450) data 0.378 (0.319) loss_u loss_u 0.9082 (0.9366) acc_u 12.5000 (8.8393) lr 1.0471e-03 eta 0:00:18
epoch [99/200] batch [40/76] time 0.566 (0.458) data 0.435 (0.327) loss_u loss_u 0.9126 (0.9345) acc_u 12.5000 (9.2188) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [45/76] time 0.616 (0.464) data 0.484 (0.333) loss_u loss_u 0.8740 (0.9348) acc_u 15.6250 (8.9583) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [50/76] time 0.372 (0.461) data 0.241 (0.330) loss_u loss_u 0.9316 (0.9359) acc_u 6.2500 (8.5625) lr 1.0471e-03 eta 0:00:11
epoch [99/200] batch [55/76] time 0.366 (0.457) data 0.234 (0.326) loss_u loss_u 0.9653 (0.9377) acc_u 3.1250 (8.2955) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [60/76] time 0.533 (0.455) data 0.402 (0.324) loss_u loss_u 0.9868 (0.9405) acc_u 0.0000 (7.9167) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [65/76] time 0.416 (0.453) data 0.284 (0.321) loss_u loss_u 0.9512 (0.9407) acc_u 6.2500 (7.8846) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [70/76] time 0.398 (0.452) data 0.268 (0.321) loss_u loss_u 0.9517 (0.9392) acc_u 3.1250 (8.0804) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [75/76] time 0.422 (0.452) data 0.290 (0.321) loss_u loss_u 0.9497 (0.9381) acc_u 9.3750 (8.1667) lr 1.0471e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1593
confident_label rate tensor(0.2149, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 674
clean true:666
clean false:8
clean_rate:0.9881305637982196
noisy true:877
noisy false:1585
all clean rate:  [0.9391304347826087, 0.9841269841269841, 0.9831223628691983, 0.9834710743801653, 0.9939759036144579, 0.9783464566929134, 0.9896907216494846, 0.998062015503876, 0.9847619047619047, 0.9878542510121457, 0.98635477582846, 0.9823874755381604, 0.9788461538461538, 0.9815157116451017, 0.9775280898876404, 0.9809160305343512, 0.9866412213740458, 0.9885057471264368, 0.9910233393177738, 0.9873646209386282, 0.981203007518797, 0.9843478260869565, 0.978021978021978, 0.9912280701754386, 0.9840989399293286, 0.9837251356238698, 0.9880546075085325, 0.9946714031971581, 0.9861111111111112, 0.9826086956521739, 0.9793103448275862, 0.9932773109243698, 0.9896551724137931, 0.9777777777777777, 0.987719298245614, 0.9883720930232558, 0.9816053511705686, 0.9814814814814815, 0.9799666110183639, 0.9837662337662337, 0.9851239669421488, 0.9871589085072231, 0.9880546075085325, 0.9879725085910653, 0.9839228295819936, 0.9871175523349437, 0.9854132901134521, 0.9852941176470589, 0.9867549668874173, 0.9874213836477987, 0.982484076433121, 0.9902597402597403, 0.9897959183673469, 0.987460815047022, 0.9901153212520593, 0.9818731117824774, 0.9853181076672104, 0.9883333333333333, 0.9827856025039123, 0.9904761904761905, 0.9857594936708861, 0.988835725677831, 0.9799382716049383, 0.9821717990275527, 0.9853181076672104, 0.9848024316109423, 0.9905362776025236, 0.9839181286549707, 0.9876160990712074, 0.9843505477308294, 0.99079754601227, 0.9836065573770492, 0.9804511278195489, 0.9797822706065319, 0.9894578313253012, 0.9817629179331308, 0.9798761609907121, 0.9852941176470589, 0.987987987987988, 0.9799382716049383, 0.9907692307692307, 0.9781659388646288, 0.9908536585365854, 0.9878419452887538, 0.9831288343558282, 0.9804511278195489, 0.9838945827232797, 0.9897510980966325, 0.983751846381093, 0.9868613138686131, 0.9894894894894894, 0.9812680115273775, 0.9837037037037037, 0.9854227405247813, 0.9853157121879589, 0.9814814814814815, 0.9881481481481481, 0.9868995633187773, 0.9841040462427746, 0.9881305637982196]
after delete: len(clean_dataset) 674
after delete: len(noisy_dataset) 2462
epoch [100/200] batch [5/21] time 0.388 (0.511) data 0.257 (0.380) loss_x loss_x 0.8164 (0.9973) acc_x 84.3750 (80.6250) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [10/21] time 0.460 (0.478) data 0.328 (0.347) loss_x loss_x 1.0684 (1.0781) acc_x 81.2500 (79.0625) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [15/21] time 0.503 (0.458) data 0.373 (0.327) loss_x loss_x 1.2920 (1.1219) acc_x 56.2500 (75.2083) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [20/21] time 0.484 (0.466) data 0.353 (0.335) loss_x loss_x 1.4990 (1.1191) acc_x 59.3750 (74.5312) lr 1.0314e-03 eta 0:00:00
epoch [100/200] batch [5/76] time 0.372 (0.472) data 0.240 (0.341) loss_u loss_u 0.9292 (0.9104) acc_u 9.3750 (13.1250) lr 1.0314e-03 eta 0:00:33
epoch [100/200] batch [10/76] time 0.478 (0.470) data 0.346 (0.339) loss_u loss_u 0.9546 (0.9263) acc_u 3.1250 (10.3125) lr 1.0314e-03 eta 0:00:31
epoch [100/200] batch [15/76] time 0.501 (0.474) data 0.369 (0.343) loss_u loss_u 0.9648 (0.9328) acc_u 3.1250 (8.7500) lr 1.0314e-03 eta 0:00:28
epoch [100/200] batch [20/76] time 0.389 (0.470) data 0.257 (0.339) loss_u loss_u 0.9131 (0.9312) acc_u 12.5000 (9.2188) lr 1.0314e-03 eta 0:00:26
epoch [100/200] batch [25/76] time 0.397 (0.460) data 0.266 (0.329) loss_u loss_u 0.9463 (0.9339) acc_u 6.2500 (8.6250) lr 1.0314e-03 eta 0:00:23
epoch [100/200] batch [30/76] time 0.479 (0.458) data 0.349 (0.327) loss_u loss_u 0.9424 (0.9353) acc_u 6.2500 (8.2292) lr 1.0314e-03 eta 0:00:21
epoch [100/200] batch [35/76] time 0.410 (0.457) data 0.279 (0.326) loss_u loss_u 0.9062 (0.9324) acc_u 18.7500 (8.8393) lr 1.0314e-03 eta 0:00:18
epoch [100/200] batch [40/76] time 0.456 (0.458) data 0.325 (0.327) loss_u loss_u 0.9453 (0.9341) acc_u 9.3750 (8.6719) lr 1.0314e-03 eta 0:00:16
epoch [100/200] batch [45/76] time 0.415 (0.454) data 0.285 (0.323) loss_u loss_u 0.9106 (0.9353) acc_u 9.3750 (8.3333) lr 1.0314e-03 eta 0:00:14
epoch [100/200] batch [50/76] time 0.375 (0.451) data 0.244 (0.319) loss_u loss_u 0.8809 (0.9339) acc_u 18.7500 (8.7500) lr 1.0314e-03 eta 0:00:11
epoch [100/200] batch [55/76] time 0.453 (0.448) data 0.322 (0.317) loss_u loss_u 0.8784 (0.9327) acc_u 15.6250 (8.9773) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [60/76] time 0.356 (0.448) data 0.226 (0.316) loss_u loss_u 0.9165 (0.9331) acc_u 9.3750 (9.0104) lr 1.0314e-03 eta 0:00:07
epoch [100/200] batch [65/76] time 0.424 (0.446) data 0.293 (0.315) loss_u loss_u 0.9580 (0.9326) acc_u 6.2500 (9.0865) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [70/76] time 0.433 (0.445) data 0.302 (0.314) loss_u loss_u 0.9277 (0.9337) acc_u 12.5000 (8.9732) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [75/76] time 0.383 (0.447) data 0.252 (0.316) loss_u loss_u 0.9438 (0.9354) acc_u 3.1250 (8.6250) lr 1.0314e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1566
confident_label rate tensor(0.2261, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 709
clean true:703
clean false:6
clean_rate:0.9915373765867419
noisy true:867
noisy false:1560
after delete: len(clean_dataset) 709
after delete: len(noisy_dataset) 2427
epoch [101/200] batch [5/22] time 0.498 (0.487) data 0.368 (0.357) loss_x loss_x 1.0332 (1.1973) acc_x 78.1250 (76.2500) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [10/22] time 0.361 (0.454) data 0.231 (0.323) loss_x loss_x 0.9058 (1.1672) acc_x 81.2500 (75.3125) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [15/22] time 0.396 (0.458) data 0.266 (0.328) loss_x loss_x 1.0322 (1.1499) acc_x 81.2500 (74.5833) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [20/22] time 0.387 (0.453) data 0.257 (0.322) loss_x loss_x 1.5898 (1.1470) acc_x 65.6250 (74.0625) lr 1.0157e-03 eta 0:00:00
epoch [101/200] batch [5/75] time 0.602 (0.453) data 0.471 (0.322) loss_u loss_u 0.8901 (0.9266) acc_u 12.5000 (10.0000) lr 1.0157e-03 eta 0:00:31
epoch [101/200] batch [10/75] time 0.567 (0.446) data 0.436 (0.315) loss_u loss_u 0.9395 (0.9266) acc_u 6.2500 (8.7500) lr 1.0157e-03 eta 0:00:28
epoch [101/200] batch [15/75] time 0.358 (0.439) data 0.226 (0.308) loss_u loss_u 0.9463 (0.9233) acc_u 6.2500 (8.9583) lr 1.0157e-03 eta 0:00:26
epoch [101/200] batch [20/75] time 0.523 (0.453) data 0.391 (0.322) loss_u loss_u 0.9351 (0.9342) acc_u 6.2500 (7.6562) lr 1.0157e-03 eta 0:00:24
epoch [101/200] batch [25/75] time 0.556 (0.455) data 0.424 (0.324) loss_u loss_u 0.9526 (0.9356) acc_u 6.2500 (7.5000) lr 1.0157e-03 eta 0:00:22
epoch [101/200] batch [30/75] time 0.359 (0.452) data 0.229 (0.321) loss_u loss_u 0.9058 (0.9367) acc_u 9.3750 (7.3958) lr 1.0157e-03 eta 0:00:20
epoch [101/200] batch [35/75] time 0.440 (0.452) data 0.309 (0.320) loss_u loss_u 0.9355 (0.9385) acc_u 6.2500 (7.2321) lr 1.0157e-03 eta 0:00:18
epoch [101/200] batch [40/75] time 0.409 (0.453) data 0.278 (0.322) loss_u loss_u 0.9463 (0.9398) acc_u 6.2500 (7.2656) lr 1.0157e-03 eta 0:00:15
epoch [101/200] batch [45/75] time 0.432 (0.451) data 0.300 (0.320) loss_u loss_u 0.9336 (0.9415) acc_u 12.5000 (7.3611) lr 1.0157e-03 eta 0:00:13
epoch [101/200] batch [50/75] time 0.421 (0.450) data 0.290 (0.319) loss_u loss_u 0.9614 (0.9425) acc_u 3.1250 (7.2500) lr 1.0157e-03 eta 0:00:11
epoch [101/200] batch [55/75] time 0.455 (0.453) data 0.324 (0.322) loss_u loss_u 0.9365 (0.9418) acc_u 15.6250 (7.3864) lr 1.0157e-03 eta 0:00:09
epoch [101/200] batch [60/75] time 0.378 (0.453) data 0.246 (0.322) loss_u loss_u 0.9341 (0.9424) acc_u 6.2500 (7.3438) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [65/75] time 0.326 (0.451) data 0.195 (0.320) loss_u loss_u 0.9297 (0.9426) acc_u 9.3750 (7.4038) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [70/75] time 0.398 (0.451) data 0.267 (0.320) loss_u loss_u 0.9673 (0.9423) acc_u 3.1250 (7.4554) lr 1.0157e-03 eta 0:00:02
epoch [101/200] batch [75/75] time 0.428 (0.450) data 0.297 (0.319) loss_u loss_u 0.8877 (0.9416) acc_u 12.5000 (7.5833) lr 1.0157e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1587
confident_label rate tensor(0.2213, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 694
clean true:683
clean false:11
clean_rate:0.984149855907781
noisy true:866
noisy false:1576
after delete: len(clean_dataset) 694
after delete: len(noisy_dataset) 2442
epoch [102/200] batch [5/21] time 0.540 (0.478) data 0.409 (0.347) loss_x loss_x 0.9722 (1.0472) acc_x 78.1250 (71.8750) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [10/21] time 0.440 (0.469) data 0.307 (0.338) loss_x loss_x 1.0146 (1.0529) acc_x 78.1250 (73.7500) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [15/21] time 0.518 (0.460) data 0.388 (0.330) loss_x loss_x 0.8125 (1.0707) acc_x 81.2500 (73.1250) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [20/21] time 0.663 (0.495) data 0.533 (0.364) loss_x loss_x 1.5713 (1.1078) acc_x 75.0000 (73.1250) lr 1.0000e-03 eta 0:00:00
epoch [102/200] batch [5/76] time 0.470 (0.503) data 0.337 (0.372) loss_u loss_u 0.9531 (0.9479) acc_u 6.2500 (6.8750) lr 1.0000e-03 eta 0:00:35
epoch [102/200] batch [10/76] time 0.660 (0.510) data 0.528 (0.378) loss_u loss_u 0.9302 (0.9377) acc_u 12.5000 (9.0625) lr 1.0000e-03 eta 0:00:33
epoch [102/200] batch [15/76] time 0.574 (0.503) data 0.442 (0.372) loss_u loss_u 0.9492 (0.9355) acc_u 6.2500 (8.9583) lr 1.0000e-03 eta 0:00:30
epoch [102/200] batch [20/76] time 0.391 (0.503) data 0.257 (0.371) loss_u loss_u 0.9883 (0.9389) acc_u 0.0000 (8.4375) lr 1.0000e-03 eta 0:00:28
epoch [102/200] batch [25/76] time 0.375 (0.495) data 0.244 (0.364) loss_u loss_u 0.9297 (0.9402) acc_u 9.3750 (8.0000) lr 1.0000e-03 eta 0:00:25
epoch [102/200] batch [30/76] time 0.509 (0.489) data 0.378 (0.357) loss_u loss_u 0.9619 (0.9365) acc_u 3.1250 (8.3333) lr 1.0000e-03 eta 0:00:22
epoch [102/200] batch [35/76] time 0.373 (0.481) data 0.243 (0.349) loss_u loss_u 0.9712 (0.9383) acc_u 3.1250 (7.9464) lr 1.0000e-03 eta 0:00:19
epoch [102/200] batch [40/76] time 0.462 (0.477) data 0.331 (0.345) loss_u loss_u 0.9883 (0.9397) acc_u 0.0000 (7.9688) lr 1.0000e-03 eta 0:00:17
epoch [102/200] batch [45/76] time 0.374 (0.471) data 0.243 (0.339) loss_u loss_u 0.9409 (0.9411) acc_u 6.2500 (7.6389) lr 1.0000e-03 eta 0:00:14
epoch [102/200] batch [50/76] time 0.426 (0.466) data 0.295 (0.335) loss_u loss_u 0.9785 (0.9417) acc_u 0.0000 (7.4375) lr 1.0000e-03 eta 0:00:12
epoch [102/200] batch [55/76] time 0.419 (0.464) data 0.288 (0.333) loss_u loss_u 0.9663 (0.9403) acc_u 3.1250 (7.7273) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [60/76] time 0.419 (0.462) data 0.288 (0.331) loss_u loss_u 0.9287 (0.9401) acc_u 6.2500 (7.7604) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [65/76] time 0.406 (0.458) data 0.274 (0.327) loss_u loss_u 0.9224 (0.9391) acc_u 9.3750 (7.9327) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [70/76] time 0.365 (0.460) data 0.233 (0.328) loss_u loss_u 0.9199 (0.9392) acc_u 15.6250 (7.9911) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [75/76] time 0.411 (0.459) data 0.280 (0.327) loss_u loss_u 0.9307 (0.9392) acc_u 9.3750 (8.0417) lr 1.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1518
confident_label rate tensor(0.2325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 729
clean true:720
clean false:9
clean_rate:0.9876543209876543
noisy true:898
noisy false:1509
after delete: len(clean_dataset) 729
after delete: len(noisy_dataset) 2407
epoch [103/200] batch [5/22] time 0.412 (0.452) data 0.280 (0.321) loss_x loss_x 0.9692 (1.1699) acc_x 68.7500 (69.3750) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [10/22] time 0.539 (0.470) data 0.408 (0.339) loss_x loss_x 0.7793 (1.2228) acc_x 78.1250 (68.1250) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [15/22] time 0.397 (0.465) data 0.266 (0.334) loss_x loss_x 1.3506 (1.2174) acc_x 68.7500 (70.8333) lr 9.8429e-04 eta 0:00:03
epoch [103/200] batch [20/22] time 0.483 (0.471) data 0.352 (0.340) loss_x loss_x 1.4404 (1.2449) acc_x 65.6250 (69.5312) lr 9.8429e-04 eta 0:00:00
epoch [103/200] batch [5/75] time 0.356 (0.470) data 0.224 (0.339) loss_u loss_u 0.9487 (0.9366) acc_u 6.2500 (8.1250) lr 9.8429e-04 eta 0:00:32
epoch [103/200] batch [10/75] time 0.519 (0.460) data 0.388 (0.329) loss_u loss_u 0.9683 (0.9385) acc_u 3.1250 (8.7500) lr 9.8429e-04 eta 0:00:29
epoch [103/200] batch [15/75] time 0.363 (0.452) data 0.231 (0.321) loss_u loss_u 0.8662 (0.9294) acc_u 18.7500 (9.7917) lr 9.8429e-04 eta 0:00:27
epoch [103/200] batch [20/75] time 0.572 (0.459) data 0.439 (0.328) loss_u loss_u 0.9551 (0.9355) acc_u 9.3750 (9.2188) lr 9.8429e-04 eta 0:00:25
epoch [103/200] batch [25/75] time 0.566 (0.462) data 0.435 (0.330) loss_u loss_u 0.9561 (0.9405) acc_u 6.2500 (8.2500) lr 9.8429e-04 eta 0:00:23
epoch [103/200] batch [30/75] time 0.359 (0.461) data 0.229 (0.329) loss_u loss_u 0.9512 (0.9354) acc_u 6.2500 (8.6458) lr 9.8429e-04 eta 0:00:20
epoch [103/200] batch [35/75] time 0.375 (0.455) data 0.244 (0.323) loss_u loss_u 0.9600 (0.9392) acc_u 6.2500 (8.3036) lr 9.8429e-04 eta 0:00:18
epoch [103/200] batch [40/75] time 0.489 (0.461) data 0.357 (0.329) loss_u loss_u 0.9502 (0.9369) acc_u 3.1250 (8.3594) lr 9.8429e-04 eta 0:00:16
epoch [103/200] batch [45/75] time 0.458 (0.463) data 0.327 (0.331) loss_u loss_u 0.9624 (0.9399) acc_u 6.2500 (8.0556) lr 9.8429e-04 eta 0:00:13
epoch [103/200] batch [50/75] time 0.355 (0.460) data 0.223 (0.329) loss_u loss_u 0.9785 (0.9423) acc_u 3.1250 (7.7500) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [55/75] time 0.386 (0.458) data 0.255 (0.326) loss_u loss_u 0.9551 (0.9436) acc_u 6.2500 (7.5568) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [60/75] time 0.359 (0.459) data 0.227 (0.328) loss_u loss_u 0.8911 (0.9424) acc_u 15.6250 (7.6562) lr 9.8429e-04 eta 0:00:06
epoch [103/200] batch [65/75] time 0.400 (0.460) data 0.268 (0.329) loss_u loss_u 0.9463 (0.9432) acc_u 6.2500 (7.5962) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [70/75] time 0.442 (0.456) data 0.312 (0.325) loss_u loss_u 0.9663 (0.9434) acc_u 3.1250 (7.4554) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [75/75] time 0.511 (0.456) data 0.379 (0.324) loss_u loss_u 0.9438 (0.9423) acc_u 9.3750 (7.5833) lr 9.8429e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1535
confident_label rate tensor(0.2242, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 703
clean true:693
clean false:10
clean_rate:0.9857752489331437
noisy true:908
noisy false:1525
after delete: len(clean_dataset) 703
after delete: len(noisy_dataset) 2433
epoch [104/200] batch [5/21] time 0.371 (0.473) data 0.240 (0.342) loss_x loss_x 1.3281 (1.2490) acc_x 75.0000 (70.0000) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [10/21] time 0.457 (0.467) data 0.326 (0.336) loss_x loss_x 0.7573 (1.2271) acc_x 81.2500 (70.9375) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [15/21] time 0.506 (0.489) data 0.374 (0.358) loss_x loss_x 1.5059 (1.2621) acc_x 68.7500 (69.3750) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [20/21] time 0.558 (0.491) data 0.428 (0.360) loss_x loss_x 1.1689 (1.2144) acc_x 71.8750 (70.6250) lr 9.6859e-04 eta 0:00:00
epoch [104/200] batch [5/76] time 0.531 (0.477) data 0.399 (0.346) loss_u loss_u 0.9141 (0.9183) acc_u 12.5000 (10.6250) lr 9.6859e-04 eta 0:00:33
epoch [104/200] batch [10/76] time 0.406 (0.469) data 0.275 (0.338) loss_u loss_u 0.9619 (0.9307) acc_u 3.1250 (8.4375) lr 9.6859e-04 eta 0:00:30
epoch [104/200] batch [15/76] time 0.326 (0.461) data 0.194 (0.329) loss_u loss_u 0.9937 (0.9366) acc_u 0.0000 (7.5000) lr 9.6859e-04 eta 0:00:28
epoch [104/200] batch [20/76] time 0.373 (0.461) data 0.241 (0.329) loss_u loss_u 0.9653 (0.9431) acc_u 6.2500 (6.7188) lr 9.6859e-04 eta 0:00:25
epoch [104/200] batch [25/76] time 0.361 (0.454) data 0.230 (0.323) loss_u loss_u 0.9360 (0.9395) acc_u 9.3750 (7.5000) lr 9.6859e-04 eta 0:00:23
epoch [104/200] batch [30/76] time 0.432 (0.450) data 0.301 (0.318) loss_u loss_u 0.9751 (0.9387) acc_u 3.1250 (7.6042) lr 9.6859e-04 eta 0:00:20
epoch [104/200] batch [35/76] time 0.498 (0.452) data 0.367 (0.320) loss_u loss_u 0.9116 (0.9380) acc_u 12.5000 (7.6786) lr 9.6859e-04 eta 0:00:18
epoch [104/200] batch [40/76] time 0.423 (0.455) data 0.293 (0.323) loss_u loss_u 0.9473 (0.9405) acc_u 6.2500 (7.5000) lr 9.6859e-04 eta 0:00:16
epoch [104/200] batch [45/76] time 0.442 (0.452) data 0.312 (0.321) loss_u loss_u 0.9062 (0.9397) acc_u 12.5000 (7.7083) lr 9.6859e-04 eta 0:00:14
epoch [104/200] batch [50/76] time 0.389 (0.452) data 0.258 (0.321) loss_u loss_u 0.9233 (0.9390) acc_u 9.3750 (7.9375) lr 9.6859e-04 eta 0:00:11
epoch [104/200] batch [55/76] time 0.529 (0.453) data 0.397 (0.321) loss_u loss_u 0.9780 (0.9405) acc_u 0.0000 (7.6136) lr 9.6859e-04 eta 0:00:09
epoch [104/200] batch [60/76] time 0.442 (0.455) data 0.312 (0.324) loss_u loss_u 0.9849 (0.9406) acc_u 3.1250 (7.6562) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [65/76] time 0.348 (0.457) data 0.218 (0.326) loss_u loss_u 0.9580 (0.9398) acc_u 3.1250 (7.6923) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [70/76] time 0.484 (0.454) data 0.353 (0.323) loss_u loss_u 0.9673 (0.9405) acc_u 3.1250 (7.5893) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [75/76] time 0.429 (0.454) data 0.298 (0.323) loss_u loss_u 0.9653 (0.9411) acc_u 6.2500 (7.5417) lr 9.6859e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1553
confident_label rate tensor(0.2232, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 700
clean true:690
clean false:10
clean_rate:0.9857142857142858
noisy true:893
noisy false:1543
after delete: len(clean_dataset) 700
after delete: len(noisy_dataset) 2436
epoch [105/200] batch [5/21] time 0.588 (0.456) data 0.457 (0.325) loss_x loss_x 1.2236 (1.1500) acc_x 71.8750 (73.1250) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [10/21] time 0.520 (0.456) data 0.389 (0.325) loss_x loss_x 1.3896 (1.2122) acc_x 71.8750 (72.8125) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [15/21] time 0.366 (0.464) data 0.235 (0.333) loss_x loss_x 1.2383 (1.1928) acc_x 65.6250 (72.5000) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [20/21] time 0.455 (0.455) data 0.324 (0.324) loss_x loss_x 1.3945 (1.1843) acc_x 65.6250 (71.8750) lr 9.5289e-04 eta 0:00:00
epoch [105/200] batch [5/76] time 0.508 (0.462) data 0.375 (0.331) loss_u loss_u 0.9541 (0.9358) acc_u 6.2500 (9.3750) lr 9.5289e-04 eta 0:00:32
epoch [105/200] batch [10/76] time 0.451 (0.463) data 0.319 (0.331) loss_u loss_u 0.9497 (0.9372) acc_u 6.2500 (7.8125) lr 9.5289e-04 eta 0:00:30
epoch [105/200] batch [15/76] time 0.416 (0.463) data 0.284 (0.331) loss_u loss_u 0.8994 (0.9348) acc_u 15.6250 (8.7500) lr 9.5289e-04 eta 0:00:28
epoch [105/200] batch [20/76] time 0.453 (0.457) data 0.321 (0.325) loss_u loss_u 0.9458 (0.9348) acc_u 3.1250 (8.4375) lr 9.5289e-04 eta 0:00:25
epoch [105/200] batch [25/76] time 0.604 (0.461) data 0.472 (0.329) loss_u loss_u 0.8965 (0.9328) acc_u 12.5000 (8.6250) lr 9.5289e-04 eta 0:00:23
epoch [105/200] batch [30/76] time 0.458 (0.459) data 0.326 (0.327) loss_u loss_u 0.9531 (0.9338) acc_u 9.3750 (8.8542) lr 9.5289e-04 eta 0:00:21
epoch [105/200] batch [35/76] time 0.354 (0.461) data 0.222 (0.330) loss_u loss_u 0.9702 (0.9361) acc_u 3.1250 (8.5714) lr 9.5289e-04 eta 0:00:18
epoch [105/200] batch [40/76] time 0.432 (0.467) data 0.301 (0.335) loss_u loss_u 0.9824 (0.9405) acc_u 0.0000 (7.8906) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [45/76] time 0.380 (0.462) data 0.249 (0.330) loss_u loss_u 0.9248 (0.9405) acc_u 9.3750 (7.7083) lr 9.5289e-04 eta 0:00:14
epoch [105/200] batch [50/76] time 0.396 (0.461) data 0.266 (0.329) loss_u loss_u 0.9365 (0.9372) acc_u 9.3750 (8.3125) lr 9.5289e-04 eta 0:00:11
epoch [105/200] batch [55/76] time 0.423 (0.459) data 0.292 (0.328) loss_u loss_u 0.8970 (0.9373) acc_u 12.5000 (8.2386) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [60/76] time 0.450 (0.458) data 0.319 (0.327) loss_u loss_u 0.9375 (0.9364) acc_u 9.3750 (8.3854) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [65/76] time 0.496 (0.458) data 0.365 (0.327) loss_u loss_u 0.9673 (0.9371) acc_u 3.1250 (8.4135) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [70/76] time 0.345 (0.457) data 0.213 (0.325) loss_u loss_u 0.9590 (0.9368) acc_u 6.2500 (8.5268) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [75/76] time 0.570 (0.456) data 0.439 (0.325) loss_u loss_u 0.9048 (0.9360) acc_u 12.5000 (8.5833) lr 9.5289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1556
confident_label rate tensor(0.2235, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 701
clean true:693
clean false:8
clean_rate:0.9885877318116976
noisy true:887
noisy false:1548
after delete: len(clean_dataset) 701
after delete: len(noisy_dataset) 2435
epoch [106/200] batch [5/21] time 0.541 (0.569) data 0.410 (0.437) loss_x loss_x 1.1123 (1.0988) acc_x 75.0000 (74.3750) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [10/21] time 0.475 (0.527) data 0.344 (0.395) loss_x loss_x 1.2520 (1.0292) acc_x 68.7500 (75.6250) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [15/21] time 0.515 (0.528) data 0.384 (0.397) loss_x loss_x 0.7588 (1.0704) acc_x 84.3750 (73.7500) lr 9.3721e-04 eta 0:00:03
epoch [106/200] batch [20/21] time 0.622 (0.515) data 0.492 (0.384) loss_x loss_x 0.5737 (1.0488) acc_x 84.3750 (73.4375) lr 9.3721e-04 eta 0:00:00
epoch [106/200] batch [5/76] time 0.503 (0.499) data 0.372 (0.368) loss_u loss_u 0.8818 (0.9412) acc_u 15.6250 (7.5000) lr 9.3721e-04 eta 0:00:35
epoch [106/200] batch [10/76] time 0.421 (0.484) data 0.289 (0.352) loss_u loss_u 0.9277 (0.9437) acc_u 6.2500 (6.8750) lr 9.3721e-04 eta 0:00:31
epoch [106/200] batch [15/76] time 0.465 (0.474) data 0.333 (0.343) loss_u loss_u 0.9487 (0.9343) acc_u 9.3750 (8.1250) lr 9.3721e-04 eta 0:00:28
epoch [106/200] batch [20/76] time 0.449 (0.468) data 0.318 (0.337) loss_u loss_u 0.9722 (0.9412) acc_u 0.0000 (7.1875) lr 9.3721e-04 eta 0:00:26
epoch [106/200] batch [25/76] time 0.563 (0.466) data 0.432 (0.335) loss_u loss_u 0.9551 (0.9410) acc_u 6.2500 (7.5000) lr 9.3721e-04 eta 0:00:23
epoch [106/200] batch [30/76] time 0.400 (0.460) data 0.270 (0.328) loss_u loss_u 0.9365 (0.9420) acc_u 9.3750 (7.5000) lr 9.3721e-04 eta 0:00:21
epoch [106/200] batch [35/76] time 0.383 (0.461) data 0.251 (0.330) loss_u loss_u 0.9731 (0.9421) acc_u 3.1250 (7.5000) lr 9.3721e-04 eta 0:00:18
epoch [106/200] batch [40/76] time 0.465 (0.458) data 0.335 (0.327) loss_u loss_u 0.9409 (0.9404) acc_u 9.3750 (7.8125) lr 9.3721e-04 eta 0:00:16
epoch [106/200] batch [45/76] time 0.454 (0.456) data 0.322 (0.325) loss_u loss_u 0.9258 (0.9407) acc_u 9.3750 (7.7778) lr 9.3721e-04 eta 0:00:14
epoch [106/200] batch [50/76] time 0.462 (0.458) data 0.331 (0.327) loss_u loss_u 0.9155 (0.9401) acc_u 9.3750 (7.7500) lr 9.3721e-04 eta 0:00:11
epoch [106/200] batch [55/76] time 0.446 (0.457) data 0.315 (0.326) loss_u loss_u 0.9131 (0.9382) acc_u 12.5000 (8.0114) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [60/76] time 0.339 (0.455) data 0.208 (0.324) loss_u loss_u 0.9771 (0.9401) acc_u 0.0000 (7.8125) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [65/76] time 0.402 (0.456) data 0.271 (0.325) loss_u loss_u 0.9551 (0.9401) acc_u 6.2500 (7.7885) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [70/76] time 0.491 (0.462) data 0.360 (0.330) loss_u loss_u 0.9141 (0.9395) acc_u 12.5000 (7.9018) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [75/76] time 0.569 (0.461) data 0.438 (0.330) loss_u loss_u 0.9756 (0.9413) acc_u 3.1250 (7.7083) lr 9.3721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1587
confident_label rate tensor(0.2286, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 717
clean true:701
clean false:16
clean_rate:0.9776847977684798
noisy true:848
noisy false:1571
after delete: len(clean_dataset) 717
after delete: len(noisy_dataset) 2419
epoch [107/200] batch [5/22] time 0.379 (0.440) data 0.248 (0.309) loss_x loss_x 1.0547 (1.0386) acc_x 65.6250 (72.5000) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [10/22] time 0.399 (0.488) data 0.269 (0.358) loss_x loss_x 1.8438 (1.1007) acc_x 56.2500 (70.3125) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [15/22] time 0.431 (0.465) data 0.300 (0.334) loss_x loss_x 1.4219 (1.1660) acc_x 62.5000 (68.9583) lr 9.2154e-04 eta 0:00:03
epoch [107/200] batch [20/22] time 0.533 (0.454) data 0.403 (0.323) loss_x loss_x 1.5527 (1.1769) acc_x 62.5000 (69.2188) lr 9.2154e-04 eta 0:00:00
epoch [107/200] batch [5/75] time 0.401 (0.455) data 0.269 (0.324) loss_u loss_u 0.8481 (0.9186) acc_u 18.7500 (10.0000) lr 9.2154e-04 eta 0:00:31
epoch [107/200] batch [10/75] time 0.462 (0.456) data 0.331 (0.325) loss_u loss_u 0.9824 (0.9341) acc_u 3.1250 (8.1250) lr 9.2154e-04 eta 0:00:29
epoch [107/200] batch [15/75] time 0.484 (0.455) data 0.348 (0.324) loss_u loss_u 0.9053 (0.9306) acc_u 12.5000 (8.7500) lr 9.2154e-04 eta 0:00:27
epoch [107/200] batch [20/75] time 0.533 (0.454) data 0.402 (0.322) loss_u loss_u 0.9126 (0.9332) acc_u 12.5000 (8.2812) lr 9.2154e-04 eta 0:00:24
epoch [107/200] batch [25/75] time 0.449 (0.458) data 0.318 (0.326) loss_u loss_u 0.8687 (0.9345) acc_u 15.6250 (7.8750) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [30/75] time 0.475 (0.454) data 0.345 (0.322) loss_u loss_u 0.9336 (0.9381) acc_u 9.3750 (7.9167) lr 9.2154e-04 eta 0:00:20
epoch [107/200] batch [35/75] time 0.493 (0.455) data 0.362 (0.323) loss_u loss_u 0.9849 (0.9382) acc_u 3.1250 (7.9464) lr 9.2154e-04 eta 0:00:18
epoch [107/200] batch [40/75] time 0.425 (0.454) data 0.295 (0.323) loss_u loss_u 0.9707 (0.9374) acc_u 3.1250 (8.0469) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [45/75] time 0.369 (0.453) data 0.238 (0.321) loss_u loss_u 0.9697 (0.9353) acc_u 6.2500 (8.1250) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [50/75] time 0.376 (0.453) data 0.246 (0.322) loss_u loss_u 0.9312 (0.9372) acc_u 9.3750 (7.9375) lr 9.2154e-04 eta 0:00:11
epoch [107/200] batch [55/75] time 0.548 (0.452) data 0.417 (0.321) loss_u loss_u 0.8989 (0.9363) acc_u 12.5000 (8.1250) lr 9.2154e-04 eta 0:00:09
epoch [107/200] batch [60/75] time 0.540 (0.451) data 0.408 (0.320) loss_u loss_u 0.9727 (0.9385) acc_u 3.1250 (7.8646) lr 9.2154e-04 eta 0:00:06
epoch [107/200] batch [65/75] time 0.615 (0.454) data 0.485 (0.323) loss_u loss_u 0.9404 (0.9385) acc_u 6.2500 (7.8846) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [70/75] time 0.391 (0.452) data 0.260 (0.321) loss_u loss_u 0.9785 (0.9393) acc_u 0.0000 (7.8125) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [75/75] time 0.470 (0.452) data 0.339 (0.321) loss_u loss_u 0.9150 (0.9397) acc_u 12.5000 (7.7917) lr 9.2154e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1529
confident_label rate tensor(0.2267, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 711
clean true:701
clean false:10
clean_rate:0.9859353023909986
noisy true:906
noisy false:1519
after delete: len(clean_dataset) 711
after delete: len(noisy_dataset) 2425
epoch [108/200] batch [5/22] time 0.519 (0.497) data 0.388 (0.367) loss_x loss_x 0.7505 (1.2111) acc_x 84.3750 (72.5000) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [10/22] time 0.684 (0.516) data 0.553 (0.385) loss_x loss_x 1.1406 (1.2577) acc_x 68.7500 (70.0000) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [15/22] time 0.416 (0.511) data 0.285 (0.380) loss_x loss_x 1.5928 (1.2558) acc_x 68.7500 (70.6250) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [20/22] time 0.387 (0.492) data 0.257 (0.361) loss_x loss_x 0.9937 (1.2635) acc_x 75.0000 (70.7812) lr 9.0589e-04 eta 0:00:00
epoch [108/200] batch [5/75] time 0.405 (0.478) data 0.274 (0.347) loss_u loss_u 0.9619 (0.9493) acc_u 6.2500 (6.8750) lr 9.0589e-04 eta 0:00:33
epoch [108/200] batch [10/75] time 0.461 (0.472) data 0.329 (0.341) loss_u loss_u 0.9863 (0.9576) acc_u 3.1250 (6.5625) lr 9.0589e-04 eta 0:00:30
epoch [108/200] batch [15/75] time 0.560 (0.476) data 0.430 (0.345) loss_u loss_u 0.9702 (0.9478) acc_u 3.1250 (7.7083) lr 9.0589e-04 eta 0:00:28
epoch [108/200] batch [20/75] time 0.652 (0.475) data 0.521 (0.344) loss_u loss_u 0.9648 (0.9434) acc_u 6.2500 (8.5938) lr 9.0589e-04 eta 0:00:26
epoch [108/200] batch [25/75] time 0.371 (0.471) data 0.240 (0.341) loss_u loss_u 0.9365 (0.9414) acc_u 9.3750 (8.7500) lr 9.0589e-04 eta 0:00:23
epoch [108/200] batch [30/75] time 0.643 (0.471) data 0.512 (0.340) loss_u loss_u 0.9502 (0.9409) acc_u 6.2500 (8.4375) lr 9.0589e-04 eta 0:00:21
epoch [108/200] batch [35/75] time 0.351 (0.467) data 0.221 (0.336) loss_u loss_u 0.9858 (0.9455) acc_u 3.1250 (7.7679) lr 9.0589e-04 eta 0:00:18
epoch [108/200] batch [40/75] time 0.578 (0.470) data 0.448 (0.339) loss_u loss_u 0.9180 (0.9433) acc_u 12.5000 (8.1250) lr 9.0589e-04 eta 0:00:16
epoch [108/200] batch [45/75] time 0.349 (0.463) data 0.218 (0.332) loss_u loss_u 0.9468 (0.9429) acc_u 6.2500 (8.0556) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [50/75] time 0.405 (0.459) data 0.274 (0.328) loss_u loss_u 0.9487 (0.9421) acc_u 6.2500 (8.0000) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [55/75] time 0.436 (0.457) data 0.304 (0.326) loss_u loss_u 0.9434 (0.9421) acc_u 3.1250 (7.9545) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [60/75] time 0.640 (0.458) data 0.506 (0.327) loss_u loss_u 0.9453 (0.9428) acc_u 6.2500 (7.8125) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [65/75] time 0.652 (0.461) data 0.521 (0.330) loss_u loss_u 0.9399 (0.9425) acc_u 9.3750 (7.8365) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [70/75] time 0.551 (0.460) data 0.420 (0.328) loss_u loss_u 0.9692 (0.9432) acc_u 3.1250 (7.8125) lr 9.0589e-04 eta 0:00:02
epoch [108/200] batch [75/75] time 0.327 (0.459) data 0.196 (0.328) loss_u loss_u 0.9219 (0.9429) acc_u 9.3750 (7.8333) lr 9.0589e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1604
confident_label rate tensor(0.2197, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 689
clean true:678
clean false:11
clean_rate:0.9840348330914369
noisy true:854
noisy false:1593
after delete: len(clean_dataset) 689
after delete: len(noisy_dataset) 2447
epoch [109/200] batch [5/21] time 0.435 (0.433) data 0.304 (0.303) loss_x loss_x 1.2705 (1.3813) acc_x 81.2500 (73.7500) lr 8.9027e-04 eta 0:00:06
epoch [109/200] batch [10/21] time 0.461 (0.459) data 0.331 (0.328) loss_x loss_x 1.9834 (1.3433) acc_x 46.8750 (70.6250) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [15/21] time 0.406 (0.466) data 0.274 (0.335) loss_x loss_x 1.3994 (1.3545) acc_x 65.6250 (69.5833) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [20/21] time 0.445 (0.474) data 0.314 (0.344) loss_x loss_x 1.2305 (1.2948) acc_x 78.1250 (70.6250) lr 8.9027e-04 eta 0:00:00
epoch [109/200] batch [5/76] time 0.501 (0.468) data 0.369 (0.337) loss_u loss_u 0.9570 (0.9376) acc_u 6.2500 (7.5000) lr 8.9027e-04 eta 0:00:33
epoch [109/200] batch [10/76] time 0.398 (0.459) data 0.266 (0.329) loss_u loss_u 0.8857 (0.9355) acc_u 12.5000 (8.1250) lr 8.9027e-04 eta 0:00:30
epoch [109/200] batch [15/76] time 0.454 (0.462) data 0.319 (0.331) loss_u loss_u 0.9131 (0.9352) acc_u 9.3750 (8.3333) lr 8.9027e-04 eta 0:00:28
epoch [109/200] batch [20/76] time 0.427 (0.457) data 0.296 (0.326) loss_u loss_u 0.9702 (0.9345) acc_u 9.3750 (8.9062) lr 8.9027e-04 eta 0:00:25
epoch [109/200] batch [25/76] time 0.422 (0.459) data 0.289 (0.328) loss_u loss_u 0.9536 (0.9367) acc_u 6.2500 (8.5000) lr 8.9027e-04 eta 0:00:23
epoch [109/200] batch [30/76] time 0.377 (0.459) data 0.246 (0.328) loss_u loss_u 0.9888 (0.9379) acc_u 0.0000 (8.1250) lr 8.9027e-04 eta 0:00:21
epoch [109/200] batch [35/76] time 0.481 (0.459) data 0.351 (0.328) loss_u loss_u 0.9321 (0.9385) acc_u 9.3750 (8.0357) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [40/76] time 0.414 (0.454) data 0.282 (0.322) loss_u loss_u 0.9639 (0.9372) acc_u 3.1250 (8.1250) lr 8.9027e-04 eta 0:00:16
epoch [109/200] batch [45/76] time 0.588 (0.454) data 0.457 (0.322) loss_u loss_u 0.9673 (0.9380) acc_u 6.2500 (7.9167) lr 8.9027e-04 eta 0:00:14
epoch [109/200] batch [50/76] time 0.410 (0.450) data 0.279 (0.319) loss_u loss_u 0.9019 (0.9375) acc_u 12.5000 (7.8750) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [55/76] time 0.352 (0.447) data 0.221 (0.316) loss_u loss_u 0.9634 (0.9379) acc_u 3.1250 (8.0114) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [60/76] time 0.422 (0.447) data 0.291 (0.316) loss_u loss_u 0.8638 (0.9366) acc_u 15.6250 (8.1250) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [65/76] time 0.449 (0.446) data 0.317 (0.315) loss_u loss_u 0.9595 (0.9374) acc_u 6.2500 (8.0288) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [70/76] time 0.435 (0.444) data 0.303 (0.313) loss_u loss_u 0.9399 (0.9365) acc_u 9.3750 (8.1250) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [75/76] time 0.472 (0.447) data 0.340 (0.316) loss_u loss_u 0.9199 (0.9351) acc_u 9.3750 (8.2083) lr 8.9027e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1536
confident_label rate tensor(0.2267, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 711
clean true:701
clean false:10
clean_rate:0.9859353023909986
noisy true:899
noisy false:1526
after delete: len(clean_dataset) 711
after delete: len(noisy_dataset) 2425
epoch [110/200] batch [5/22] time 0.632 (0.506) data 0.501 (0.375) loss_x loss_x 1.3994 (1.1534) acc_x 75.0000 (70.6250) lr 8.7467e-04 eta 0:00:08
epoch [110/200] batch [10/22] time 0.400 (0.502) data 0.269 (0.371) loss_x loss_x 0.7427 (1.1715) acc_x 81.2500 (69.6875) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [15/22] time 0.377 (0.475) data 0.247 (0.344) loss_x loss_x 1.5498 (1.2008) acc_x 65.6250 (68.5417) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [20/22] time 0.537 (0.476) data 0.407 (0.346) loss_x loss_x 0.8394 (1.1896) acc_x 78.1250 (69.0625) lr 8.7467e-04 eta 0:00:00
epoch [110/200] batch [5/75] time 0.416 (0.466) data 0.285 (0.335) loss_u loss_u 0.9141 (0.9297) acc_u 9.3750 (8.1250) lr 8.7467e-04 eta 0:00:32
epoch [110/200] batch [10/75] time 0.313 (0.459) data 0.183 (0.328) loss_u loss_u 0.9600 (0.9270) acc_u 3.1250 (8.7500) lr 8.7467e-04 eta 0:00:29
epoch [110/200] batch [15/75] time 0.517 (0.465) data 0.387 (0.334) loss_u loss_u 0.9702 (0.9303) acc_u 0.0000 (8.3333) lr 8.7467e-04 eta 0:00:27
epoch [110/200] batch [20/75] time 0.476 (0.466) data 0.345 (0.335) loss_u loss_u 0.8828 (0.9353) acc_u 9.3750 (7.5000) lr 8.7467e-04 eta 0:00:25
epoch [110/200] batch [25/75] time 0.452 (0.468) data 0.320 (0.337) loss_u loss_u 0.8906 (0.9324) acc_u 15.6250 (8.5000) lr 8.7467e-04 eta 0:00:23
epoch [110/200] batch [30/75] time 0.368 (0.459) data 0.237 (0.328) loss_u loss_u 0.9565 (0.9354) acc_u 9.3750 (8.3333) lr 8.7467e-04 eta 0:00:20
epoch [110/200] batch [35/75] time 0.439 (0.455) data 0.308 (0.324) loss_u loss_u 0.9741 (0.9384) acc_u 6.2500 (7.9464) lr 8.7467e-04 eta 0:00:18
epoch [110/200] batch [40/75] time 0.565 (0.456) data 0.434 (0.325) loss_u loss_u 0.8730 (0.9379) acc_u 15.6250 (8.0469) lr 8.7467e-04 eta 0:00:15
epoch [110/200] batch [45/75] time 0.445 (0.459) data 0.314 (0.328) loss_u loss_u 0.9131 (0.9384) acc_u 9.3750 (7.9167) lr 8.7467e-04 eta 0:00:13
epoch [110/200] batch [50/75] time 0.376 (0.461) data 0.244 (0.330) loss_u loss_u 0.9834 (0.9395) acc_u 0.0000 (7.6875) lr 8.7467e-04 eta 0:00:11
epoch [110/200] batch [55/75] time 0.473 (0.460) data 0.341 (0.329) loss_u loss_u 0.9653 (0.9416) acc_u 6.2500 (7.4432) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [60/75] time 0.477 (0.458) data 0.345 (0.327) loss_u loss_u 0.8999 (0.9394) acc_u 18.7500 (7.7604) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [65/75] time 0.452 (0.457) data 0.315 (0.326) loss_u loss_u 0.9463 (0.9401) acc_u 9.3750 (7.6442) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [70/75] time 0.480 (0.458) data 0.349 (0.327) loss_u loss_u 0.9941 (0.9408) acc_u 0.0000 (7.5893) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [75/75] time 0.487 (0.458) data 0.356 (0.327) loss_u loss_u 0.9277 (0.9410) acc_u 9.3750 (7.5833) lr 8.7467e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1582
confident_label rate tensor(0.2207, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 692
clean true:682
clean false:10
clean_rate:0.9855491329479769
noisy true:872
noisy false:1572
after delete: len(clean_dataset) 692
after delete: len(noisy_dataset) 2444
epoch [111/200] batch [5/21] time 0.390 (0.456) data 0.260 (0.325) loss_x loss_x 1.3145 (1.2404) acc_x 65.6250 (70.0000) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [10/21] time 0.378 (0.468) data 0.248 (0.338) loss_x loss_x 1.5283 (1.1787) acc_x 56.2500 (71.2500) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [15/21] time 0.359 (0.463) data 0.228 (0.332) loss_x loss_x 1.5322 (1.1552) acc_x 56.2500 (71.6667) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [20/21] time 0.437 (0.465) data 0.307 (0.334) loss_x loss_x 0.8716 (1.1223) acc_x 75.0000 (71.8750) lr 8.5910e-04 eta 0:00:00
epoch [111/200] batch [5/76] time 0.434 (0.467) data 0.302 (0.336) loss_u loss_u 0.9629 (0.9228) acc_u 0.0000 (10.0000) lr 8.5910e-04 eta 0:00:33
epoch [111/200] batch [10/76] time 0.368 (0.462) data 0.236 (0.331) loss_u loss_u 0.9551 (0.9374) acc_u 3.1250 (7.5000) lr 8.5910e-04 eta 0:00:30
epoch [111/200] batch [15/76] time 0.504 (0.457) data 0.372 (0.326) loss_u loss_u 0.9082 (0.9342) acc_u 9.3750 (7.9167) lr 8.5910e-04 eta 0:00:27
epoch [111/200] batch [20/76] time 0.420 (0.456) data 0.288 (0.325) loss_u loss_u 0.9048 (0.9315) acc_u 12.5000 (8.1250) lr 8.5910e-04 eta 0:00:25
epoch [111/200] batch [25/76] time 0.461 (0.453) data 0.329 (0.322) loss_u loss_u 0.8857 (0.9305) acc_u 12.5000 (8.2500) lr 8.5910e-04 eta 0:00:23
epoch [111/200] batch [30/76] time 0.378 (0.449) data 0.247 (0.318) loss_u loss_u 0.9575 (0.9292) acc_u 6.2500 (8.6458) lr 8.5910e-04 eta 0:00:20
epoch [111/200] batch [35/76] time 0.367 (0.446) data 0.237 (0.315) loss_u loss_u 0.9355 (0.9305) acc_u 6.2500 (8.3929) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [40/76] time 0.371 (0.447) data 0.239 (0.316) loss_u loss_u 0.9595 (0.9349) acc_u 6.2500 (7.8906) lr 8.5910e-04 eta 0:00:16
epoch [111/200] batch [45/76] time 0.619 (0.448) data 0.487 (0.317) loss_u loss_u 0.8862 (0.9320) acc_u 12.5000 (8.5417) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [50/76] time 0.489 (0.453) data 0.358 (0.322) loss_u loss_u 0.8779 (0.9287) acc_u 18.7500 (9.0000) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [55/76] time 0.455 (0.456) data 0.325 (0.325) loss_u loss_u 0.9165 (0.9297) acc_u 12.5000 (8.9773) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [60/76] time 0.449 (0.456) data 0.319 (0.325) loss_u loss_u 0.9360 (0.9289) acc_u 12.5000 (9.0625) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [65/76] time 0.394 (0.454) data 0.262 (0.323) loss_u loss_u 0.9824 (0.9318) acc_u 3.1250 (8.6538) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [70/76] time 0.473 (0.452) data 0.342 (0.321) loss_u loss_u 0.9214 (0.9316) acc_u 9.3750 (8.6607) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [75/76] time 0.399 (0.452) data 0.268 (0.321) loss_u loss_u 0.9062 (0.9323) acc_u 12.5000 (8.6250) lr 8.5910e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1548
confident_label rate tensor(0.2312, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 725
clean true:714
clean false:11
clean_rate:0.9848275862068966
noisy true:874
noisy false:1537
after delete: len(clean_dataset) 725
after delete: len(noisy_dataset) 2411
epoch [112/200] batch [5/22] time 0.576 (0.483) data 0.445 (0.351) loss_x loss_x 1.3906 (1.1993) acc_x 68.7500 (68.7500) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [10/22] time 0.446 (0.512) data 0.316 (0.380) loss_x loss_x 1.3623 (1.2082) acc_x 65.6250 (70.6250) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [15/22] time 0.385 (0.488) data 0.255 (0.357) loss_x loss_x 0.8691 (1.2315) acc_x 71.8750 (69.3750) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [20/22] time 0.494 (0.479) data 0.363 (0.348) loss_x loss_x 1.0186 (1.2771) acc_x 71.8750 (68.4375) lr 8.4357e-04 eta 0:00:00
epoch [112/200] batch [5/75] time 0.331 (0.466) data 0.200 (0.335) loss_u loss_u 0.9536 (0.9456) acc_u 9.3750 (8.1250) lr 8.4357e-04 eta 0:00:32
epoch [112/200] batch [10/75] time 0.407 (0.464) data 0.275 (0.333) loss_u loss_u 0.9604 (0.9493) acc_u 6.2500 (6.8750) lr 8.4357e-04 eta 0:00:30
epoch [112/200] batch [15/75] time 0.430 (0.459) data 0.297 (0.328) loss_u loss_u 0.9326 (0.9428) acc_u 6.2500 (6.8750) lr 8.4357e-04 eta 0:00:27
epoch [112/200] batch [20/75] time 0.440 (0.463) data 0.308 (0.332) loss_u loss_u 0.9624 (0.9414) acc_u 3.1250 (6.7188) lr 8.4357e-04 eta 0:00:25
epoch [112/200] batch [25/75] time 0.366 (0.467) data 0.234 (0.335) loss_u loss_u 0.9731 (0.9468) acc_u 6.2500 (6.2500) lr 8.4357e-04 eta 0:00:23
epoch [112/200] batch [30/75] time 0.587 (0.466) data 0.456 (0.335) loss_u loss_u 0.9717 (0.9475) acc_u 3.1250 (6.2500) lr 8.4357e-04 eta 0:00:20
epoch [112/200] batch [35/75] time 0.432 (0.466) data 0.302 (0.335) loss_u loss_u 0.8784 (0.9426) acc_u 15.6250 (7.0536) lr 8.4357e-04 eta 0:00:18
epoch [112/200] batch [40/75] time 0.371 (0.465) data 0.239 (0.334) loss_u loss_u 0.9160 (0.9424) acc_u 9.3750 (7.0312) lr 8.4357e-04 eta 0:00:16
epoch [112/200] batch [45/75] time 0.415 (0.466) data 0.283 (0.335) loss_u loss_u 0.9443 (0.9417) acc_u 9.3750 (7.2917) lr 8.4357e-04 eta 0:00:13
epoch [112/200] batch [50/75] time 0.444 (0.466) data 0.313 (0.335) loss_u loss_u 0.8984 (0.9422) acc_u 15.6250 (7.3125) lr 8.4357e-04 eta 0:00:11
epoch [112/200] batch [55/75] time 0.500 (0.463) data 0.369 (0.331) loss_u loss_u 0.9453 (0.9408) acc_u 6.2500 (7.5000) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [60/75] time 0.545 (0.464) data 0.413 (0.333) loss_u loss_u 0.8779 (0.9392) acc_u 15.6250 (7.6042) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [65/75] time 0.378 (0.462) data 0.247 (0.331) loss_u loss_u 0.9248 (0.9400) acc_u 9.3750 (7.5000) lr 8.4357e-04 eta 0:00:04
epoch [112/200] batch [70/75] time 0.552 (0.466) data 0.421 (0.335) loss_u loss_u 0.8848 (0.9394) acc_u 15.6250 (7.5893) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [75/75] time 0.413 (0.463) data 0.283 (0.332) loss_u loss_u 0.9844 (0.9397) acc_u 0.0000 (7.5000) lr 8.4357e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1585
confident_label rate tensor(0.2245, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 704
clean true:693
clean false:11
clean_rate:0.984375
noisy true:858
noisy false:1574
after delete: len(clean_dataset) 704
after delete: len(noisy_dataset) 2432
epoch [113/200] batch [5/22] time 0.471 (0.457) data 0.340 (0.326) loss_x loss_x 1.1260 (1.2401) acc_x 68.7500 (68.7500) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [10/22] time 0.559 (0.490) data 0.428 (0.360) loss_x loss_x 0.9375 (1.1816) acc_x 78.1250 (70.6250) lr 8.2807e-04 eta 0:00:05
epoch [113/200] batch [15/22] time 0.404 (0.513) data 0.274 (0.383) loss_x loss_x 1.1025 (1.1500) acc_x 68.7500 (71.8750) lr 8.2807e-04 eta 0:00:03
epoch [113/200] batch [20/22] time 0.661 (0.508) data 0.530 (0.377) loss_x loss_x 1.2822 (1.1719) acc_x 71.8750 (70.7812) lr 8.2807e-04 eta 0:00:01
epoch [113/200] batch [5/76] time 0.383 (0.491) data 0.253 (0.360) loss_u loss_u 0.9731 (0.9280) acc_u 3.1250 (8.1250) lr 8.2807e-04 eta 0:00:34
epoch [113/200] batch [10/76] time 0.420 (0.480) data 0.288 (0.349) loss_u loss_u 0.8779 (0.9275) acc_u 21.8750 (9.3750) lr 8.2807e-04 eta 0:00:31
epoch [113/200] batch [15/76] time 0.547 (0.475) data 0.415 (0.344) loss_u loss_u 0.9775 (0.9376) acc_u 0.0000 (7.9167) lr 8.2807e-04 eta 0:00:28
epoch [113/200] batch [20/76] time 0.402 (0.474) data 0.271 (0.343) loss_u loss_u 0.9312 (0.9399) acc_u 9.3750 (7.6562) lr 8.2807e-04 eta 0:00:26
epoch [113/200] batch [25/76] time 0.677 (0.474) data 0.545 (0.343) loss_u loss_u 0.9443 (0.9457) acc_u 6.2500 (6.8750) lr 8.2807e-04 eta 0:00:24
epoch [113/200] batch [30/76] time 0.401 (0.477) data 0.270 (0.346) loss_u loss_u 0.9316 (0.9446) acc_u 9.3750 (7.1875) lr 8.2807e-04 eta 0:00:21
epoch [113/200] batch [35/76] time 0.344 (0.472) data 0.212 (0.341) loss_u loss_u 0.9268 (0.9429) acc_u 15.6250 (7.5000) lr 8.2807e-04 eta 0:00:19
epoch [113/200] batch [40/76] time 0.472 (0.469) data 0.340 (0.338) loss_u loss_u 0.9121 (0.9420) acc_u 9.3750 (7.5781) lr 8.2807e-04 eta 0:00:16
epoch [113/200] batch [45/76] time 0.471 (0.469) data 0.340 (0.338) loss_u loss_u 0.9009 (0.9410) acc_u 15.6250 (7.6389) lr 8.2807e-04 eta 0:00:14
epoch [113/200] batch [50/76] time 0.360 (0.468) data 0.229 (0.337) loss_u loss_u 0.9785 (0.9417) acc_u 3.1250 (7.6875) lr 8.2807e-04 eta 0:00:12
epoch [113/200] batch [55/76] time 0.395 (0.464) data 0.264 (0.333) loss_u loss_u 0.9404 (0.9419) acc_u 6.2500 (7.5568) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [60/76] time 0.521 (0.463) data 0.386 (0.332) loss_u loss_u 0.9097 (0.9411) acc_u 9.3750 (7.6042) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [65/76] time 0.368 (0.459) data 0.237 (0.328) loss_u loss_u 0.9111 (0.9375) acc_u 15.6250 (8.0769) lr 8.2807e-04 eta 0:00:05
epoch [113/200] batch [70/76] time 0.563 (0.460) data 0.433 (0.329) loss_u loss_u 0.9600 (0.9376) acc_u 9.3750 (8.1250) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [75/76] time 0.416 (0.457) data 0.286 (0.326) loss_u loss_u 0.8960 (0.9371) acc_u 15.6250 (8.1250) lr 8.2807e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1574
confident_label rate tensor(0.2181, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 684
clean true:674
clean false:10
clean_rate:0.9853801169590644
noisy true:888
noisy false:1564
after delete: len(clean_dataset) 684
after delete: len(noisy_dataset) 2452
epoch [114/200] batch [5/21] time 0.466 (0.478) data 0.336 (0.347) loss_x loss_x 0.7285 (1.1668) acc_x 81.2500 (70.6250) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [10/21] time 0.403 (0.471) data 0.270 (0.340) loss_x loss_x 1.4688 (1.1315) acc_x 62.5000 (70.6250) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [15/21] time 0.396 (0.474) data 0.266 (0.344) loss_x loss_x 1.1748 (1.1091) acc_x 68.7500 (72.0833) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [20/21] time 0.471 (0.464) data 0.340 (0.333) loss_x loss_x 0.8184 (1.1057) acc_x 78.1250 (72.6562) lr 8.1262e-04 eta 0:00:00
epoch [114/200] batch [5/76] time 0.467 (0.466) data 0.337 (0.335) loss_u loss_u 0.9746 (0.9467) acc_u 3.1250 (7.5000) lr 8.1262e-04 eta 0:00:33
epoch [114/200] batch [10/76] time 0.519 (0.460) data 0.387 (0.329) loss_u loss_u 0.9258 (0.9253) acc_u 9.3750 (10.0000) lr 8.1262e-04 eta 0:00:30
epoch [114/200] batch [15/76] time 0.488 (0.460) data 0.356 (0.329) loss_u loss_u 0.8701 (0.9254) acc_u 15.6250 (9.5833) lr 8.1262e-04 eta 0:00:28
epoch [114/200] batch [20/76] time 0.343 (0.462) data 0.210 (0.331) loss_u loss_u 0.9297 (0.9210) acc_u 12.5000 (10.1562) lr 8.1262e-04 eta 0:00:25
epoch [114/200] batch [25/76] time 0.504 (0.463) data 0.373 (0.332) loss_u loss_u 0.9600 (0.9272) acc_u 3.1250 (9.1250) lr 8.1262e-04 eta 0:00:23
epoch [114/200] batch [30/76] time 0.417 (0.462) data 0.286 (0.331) loss_u loss_u 0.9541 (0.9258) acc_u 9.3750 (9.5833) lr 8.1262e-04 eta 0:00:21
epoch [114/200] batch [35/76] time 0.553 (0.466) data 0.420 (0.335) loss_u loss_u 0.8896 (0.9277) acc_u 12.5000 (9.2857) lr 8.1262e-04 eta 0:00:19
epoch [114/200] batch [40/76] time 0.588 (0.469) data 0.456 (0.337) loss_u loss_u 0.9302 (0.9274) acc_u 12.5000 (9.3750) lr 8.1262e-04 eta 0:00:16
epoch [114/200] batch [45/76] time 0.460 (0.466) data 0.328 (0.334) loss_u loss_u 0.9595 (0.9309) acc_u 3.1250 (8.8194) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [50/76] time 0.503 (0.466) data 0.371 (0.335) loss_u loss_u 0.9438 (0.9319) acc_u 12.5000 (8.9375) lr 8.1262e-04 eta 0:00:12
epoch [114/200] batch [55/76] time 0.463 (0.466) data 0.332 (0.335) loss_u loss_u 0.9395 (0.9331) acc_u 9.3750 (8.8068) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [60/76] time 0.464 (0.469) data 0.331 (0.337) loss_u loss_u 0.9580 (0.9326) acc_u 3.1250 (8.9062) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [65/76] time 0.375 (0.470) data 0.243 (0.338) loss_u loss_u 0.9028 (0.9323) acc_u 12.5000 (8.8462) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [70/76] time 0.352 (0.466) data 0.220 (0.334) loss_u loss_u 0.9653 (0.9332) acc_u 3.1250 (8.7946) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [75/76] time 0.370 (0.462) data 0.239 (0.330) loss_u loss_u 0.9492 (0.9346) acc_u 3.1250 (8.5833) lr 8.1262e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1539
confident_label rate tensor(0.2229, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 699
clean true:692
clean false:7
clean_rate:0.9899856938483548
noisy true:905
noisy false:1532
after delete: len(clean_dataset) 699
after delete: len(noisy_dataset) 2437
epoch [115/200] batch [5/21] time 0.462 (0.532) data 0.332 (0.401) loss_x loss_x 1.0273 (0.9967) acc_x 78.1250 (73.7500) lr 7.9721e-04 eta 0:00:08
epoch [115/200] batch [10/21] time 0.612 (0.507) data 0.481 (0.376) loss_x loss_x 1.1875 (1.0676) acc_x 68.7500 (70.9375) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [15/21] time 0.433 (0.481) data 0.302 (0.351) loss_x loss_x 1.0234 (1.0998) acc_x 75.0000 (70.6250) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [20/21] time 0.429 (0.477) data 0.298 (0.346) loss_x loss_x 1.1768 (1.1421) acc_x 59.3750 (69.8438) lr 7.9721e-04 eta 0:00:00
epoch [115/200] batch [5/76] time 0.375 (0.468) data 0.244 (0.337) loss_u loss_u 0.9248 (0.9254) acc_u 9.3750 (9.3750) lr 7.9721e-04 eta 0:00:33
epoch [115/200] batch [10/76] time 0.391 (0.467) data 0.260 (0.336) loss_u loss_u 0.9595 (0.9398) acc_u 3.1250 (7.1875) lr 7.9721e-04 eta 0:00:30
epoch [115/200] batch [15/76] time 0.367 (0.463) data 0.235 (0.332) loss_u loss_u 0.9150 (0.9366) acc_u 9.3750 (7.5000) lr 7.9721e-04 eta 0:00:28
epoch [115/200] batch [20/76] time 0.641 (0.463) data 0.510 (0.332) loss_u loss_u 0.8511 (0.9351) acc_u 21.8750 (7.6562) lr 7.9721e-04 eta 0:00:25
epoch [115/200] batch [25/76] time 0.443 (0.457) data 0.312 (0.325) loss_u loss_u 0.9858 (0.9376) acc_u 3.1250 (7.7500) lr 7.9721e-04 eta 0:00:23
epoch [115/200] batch [30/76] time 0.442 (0.455) data 0.310 (0.324) loss_u loss_u 0.9722 (0.9336) acc_u 3.1250 (8.1250) lr 7.9721e-04 eta 0:00:20
epoch [115/200] batch [35/76] time 0.382 (0.460) data 0.248 (0.329) loss_u loss_u 0.9453 (0.9349) acc_u 9.3750 (8.0357) lr 7.9721e-04 eta 0:00:18
epoch [115/200] batch [40/76] time 0.402 (0.466) data 0.270 (0.335) loss_u loss_u 0.8950 (0.9354) acc_u 12.5000 (7.8906) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [45/76] time 0.408 (0.463) data 0.277 (0.331) loss_u loss_u 0.9331 (0.9366) acc_u 9.3750 (7.8472) lr 7.9721e-04 eta 0:00:14
epoch [115/200] batch [50/76] time 0.547 (0.460) data 0.415 (0.328) loss_u loss_u 0.9312 (0.9367) acc_u 9.3750 (7.8750) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [55/76] time 0.445 (0.456) data 0.315 (0.325) loss_u loss_u 0.9634 (0.9387) acc_u 6.2500 (7.6705) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [60/76] time 0.399 (0.456) data 0.268 (0.324) loss_u loss_u 0.9092 (0.9383) acc_u 9.3750 (7.7604) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [65/76] time 0.517 (0.457) data 0.387 (0.325) loss_u loss_u 0.9370 (0.9386) acc_u 9.3750 (7.7885) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [70/76] time 0.451 (0.455) data 0.321 (0.323) loss_u loss_u 0.9951 (0.9389) acc_u 0.0000 (7.7232) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [75/76] time 0.451 (0.456) data 0.320 (0.324) loss_u loss_u 0.9409 (0.9366) acc_u 6.2500 (8.0417) lr 7.9721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1572
confident_label rate tensor(0.2274, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 713
clean true:699
clean false:14
clean_rate:0.9803646563814866
noisy true:865
noisy false:1558
after delete: len(clean_dataset) 713
after delete: len(noisy_dataset) 2423
epoch [116/200] batch [5/22] time 0.476 (0.487) data 0.344 (0.355) loss_x loss_x 1.6045 (1.4409) acc_x 56.2500 (65.0000) lr 7.8186e-04 eta 0:00:08
epoch [116/200] batch [10/22] time 0.623 (0.483) data 0.492 (0.352) loss_x loss_x 0.6489 (1.2827) acc_x 90.6250 (69.3750) lr 7.8186e-04 eta 0:00:05
epoch [116/200] batch [15/22] time 0.354 (0.485) data 0.223 (0.354) loss_x loss_x 1.1455 (1.2056) acc_x 68.7500 (70.0000) lr 7.8186e-04 eta 0:00:03
epoch [116/200] batch [20/22] time 0.411 (0.486) data 0.280 (0.355) loss_x loss_x 1.0049 (1.1661) acc_x 71.8750 (71.2500) lr 7.8186e-04 eta 0:00:00
epoch [116/200] batch [5/75] time 0.489 (0.490) data 0.357 (0.358) loss_u loss_u 0.9834 (0.9576) acc_u 0.0000 (4.3750) lr 7.8186e-04 eta 0:00:34
epoch [116/200] batch [10/75] time 0.393 (0.476) data 0.262 (0.345) loss_u loss_u 0.8774 (0.9423) acc_u 12.5000 (6.2500) lr 7.8186e-04 eta 0:00:30
epoch [116/200] batch [15/75] time 0.453 (0.469) data 0.321 (0.337) loss_u loss_u 0.9629 (0.9448) acc_u 3.1250 (6.2500) lr 7.8186e-04 eta 0:00:28
epoch [116/200] batch [20/75] time 0.452 (0.465) data 0.321 (0.334) loss_u loss_u 0.9805 (0.9479) acc_u 3.1250 (6.4062) lr 7.8186e-04 eta 0:00:25
epoch [116/200] batch [25/75] time 0.405 (0.464) data 0.274 (0.332) loss_u loss_u 0.9346 (0.9449) acc_u 6.2500 (6.7500) lr 7.8186e-04 eta 0:00:23
epoch [116/200] batch [30/75] time 0.416 (0.458) data 0.285 (0.327) loss_u loss_u 0.9614 (0.9470) acc_u 6.2500 (6.3542) lr 7.8186e-04 eta 0:00:20
epoch [116/200] batch [35/75] time 0.475 (0.468) data 0.344 (0.337) loss_u loss_u 0.9536 (0.9479) acc_u 6.2500 (6.3393) lr 7.8186e-04 eta 0:00:18
epoch [116/200] batch [40/75] time 0.413 (0.468) data 0.282 (0.337) loss_u loss_u 0.8901 (0.9458) acc_u 18.7500 (6.7188) lr 7.8186e-04 eta 0:00:16
epoch [116/200] batch [45/75] time 0.582 (0.470) data 0.450 (0.339) loss_u loss_u 0.9067 (0.9423) acc_u 12.5000 (7.1528) lr 7.8186e-04 eta 0:00:14
epoch [116/200] batch [50/75] time 0.477 (0.468) data 0.345 (0.337) loss_u loss_u 0.9199 (0.9407) acc_u 9.3750 (7.3750) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [55/75] time 0.546 (0.468) data 0.413 (0.337) loss_u loss_u 0.8862 (0.9380) acc_u 12.5000 (7.7841) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [60/75] time 0.433 (0.468) data 0.300 (0.337) loss_u loss_u 0.9453 (0.9374) acc_u 9.3750 (7.7604) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [65/75] time 0.400 (0.466) data 0.269 (0.335) loss_u loss_u 0.9604 (0.9384) acc_u 6.2500 (7.5962) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [70/75] time 0.422 (0.464) data 0.291 (0.332) loss_u loss_u 0.9243 (0.9387) acc_u 9.3750 (7.5000) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [75/75] time 0.510 (0.463) data 0.379 (0.332) loss_u loss_u 0.9258 (0.9383) acc_u 6.2500 (7.5417) lr 7.8186e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1603
confident_label rate tensor(0.2165, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 679
clean true:668
clean false:11
clean_rate:0.9837997054491899
noisy true:865
noisy false:1592
after delete: len(clean_dataset) 679
after delete: len(noisy_dataset) 2457
epoch [117/200] batch [5/21] time 0.600 (0.427) data 0.469 (0.297) loss_x loss_x 0.4954 (1.1632) acc_x 90.6250 (74.3750) lr 7.6655e-04 eta 0:00:06
epoch [117/200] batch [10/21] time 0.429 (0.442) data 0.298 (0.311) loss_x loss_x 0.7954 (1.1517) acc_x 75.0000 (73.1250) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [15/21] time 0.529 (0.472) data 0.398 (0.341) loss_x loss_x 0.9644 (1.1756) acc_x 78.1250 (72.7083) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [20/21] time 0.467 (0.486) data 0.336 (0.356) loss_x loss_x 0.8579 (1.1763) acc_x 78.1250 (71.5625) lr 7.6655e-04 eta 0:00:00
epoch [117/200] batch [5/76] time 0.377 (0.478) data 0.244 (0.347) loss_u loss_u 0.9087 (0.9419) acc_u 12.5000 (6.8750) lr 7.6655e-04 eta 0:00:33
epoch [117/200] batch [10/76] time 0.437 (0.470) data 0.303 (0.339) loss_u loss_u 0.9482 (0.9325) acc_u 9.3750 (8.1250) lr 7.6655e-04 eta 0:00:31
epoch [117/200] batch [15/76] time 0.491 (0.469) data 0.359 (0.338) loss_u loss_u 0.8721 (0.9331) acc_u 21.8750 (9.1667) lr 7.6655e-04 eta 0:00:28
epoch [117/200] batch [20/76] time 0.560 (0.481) data 0.428 (0.349) loss_u loss_u 0.9482 (0.9298) acc_u 6.2500 (9.2188) lr 7.6655e-04 eta 0:00:26
epoch [117/200] batch [25/76] time 0.437 (0.477) data 0.305 (0.345) loss_u loss_u 0.9229 (0.9326) acc_u 6.2500 (8.7500) lr 7.6655e-04 eta 0:00:24
epoch [117/200] batch [30/76] time 0.709 (0.479) data 0.574 (0.347) loss_u loss_u 0.9712 (0.9359) acc_u 3.1250 (8.3333) lr 7.6655e-04 eta 0:00:22
epoch [117/200] batch [35/76] time 0.383 (0.475) data 0.251 (0.343) loss_u loss_u 0.9521 (0.9349) acc_u 6.2500 (8.3036) lr 7.6655e-04 eta 0:00:19
epoch [117/200] batch [40/76] time 0.446 (0.467) data 0.314 (0.336) loss_u loss_u 0.9888 (0.9398) acc_u 0.0000 (7.8125) lr 7.6655e-04 eta 0:00:16
epoch [117/200] batch [45/76] time 0.349 (0.463) data 0.217 (0.332) loss_u loss_u 0.9751 (0.9377) acc_u 3.1250 (8.1250) lr 7.6655e-04 eta 0:00:14
epoch [117/200] batch [50/76] time 0.528 (0.461) data 0.397 (0.329) loss_u loss_u 0.9858 (0.9379) acc_u 3.1250 (8.0000) lr 7.6655e-04 eta 0:00:11
epoch [117/200] batch [55/76] time 0.456 (0.460) data 0.325 (0.329) loss_u loss_u 0.9829 (0.9402) acc_u 3.1250 (7.6705) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [60/76] time 0.507 (0.461) data 0.375 (0.329) loss_u loss_u 0.9443 (0.9406) acc_u 6.2500 (7.6562) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [65/76] time 0.421 (0.463) data 0.290 (0.332) loss_u loss_u 0.9731 (0.9404) acc_u 3.1250 (7.7885) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [70/76] time 0.473 (0.465) data 0.341 (0.333) loss_u loss_u 0.9360 (0.9403) acc_u 9.3750 (7.7232) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [75/76] time 0.420 (0.465) data 0.288 (0.334) loss_u loss_u 0.9316 (0.9398) acc_u 9.3750 (7.8750) lr 7.6655e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1541
confident_label rate tensor(0.2283, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 716
clean true:705
clean false:11
clean_rate:0.9846368715083799
noisy true:890
noisy false:1530
after delete: len(clean_dataset) 716
after delete: len(noisy_dataset) 2420
epoch [118/200] batch [5/22] time 0.628 (0.538) data 0.496 (0.407) loss_x loss_x 0.9800 (1.0803) acc_x 78.1250 (71.2500) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [10/22] time 0.553 (0.538) data 0.422 (0.407) loss_x loss_x 1.3887 (1.2829) acc_x 71.8750 (68.4375) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [15/22] time 0.428 (0.509) data 0.297 (0.378) loss_x loss_x 0.9492 (1.2633) acc_x 78.1250 (70.0000) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [20/22] time 0.495 (0.504) data 0.364 (0.373) loss_x loss_x 1.5312 (1.2190) acc_x 62.5000 (70.6250) lr 7.5131e-04 eta 0:00:01
epoch [118/200] batch [5/75] time 0.518 (0.494) data 0.386 (0.363) loss_u loss_u 0.9463 (0.9547) acc_u 6.2500 (6.8750) lr 7.5131e-04 eta 0:00:34
epoch [118/200] batch [10/75] time 0.389 (0.490) data 0.258 (0.359) loss_u loss_u 0.9185 (0.9429) acc_u 15.6250 (8.7500) lr 7.5131e-04 eta 0:00:31
epoch [118/200] batch [15/75] time 0.386 (0.479) data 0.255 (0.348) loss_u loss_u 0.9678 (0.9435) acc_u 3.1250 (7.9167) lr 7.5131e-04 eta 0:00:28
epoch [118/200] batch [20/75] time 0.470 (0.474) data 0.337 (0.343) loss_u loss_u 0.9795 (0.9411) acc_u 6.2500 (8.4375) lr 7.5131e-04 eta 0:00:26
epoch [118/200] batch [25/75] time 0.392 (0.473) data 0.262 (0.341) loss_u loss_u 0.9551 (0.9423) acc_u 6.2500 (8.2500) lr 7.5131e-04 eta 0:00:23
epoch [118/200] batch [30/75] time 0.466 (0.472) data 0.334 (0.340) loss_u loss_u 0.8760 (0.9429) acc_u 12.5000 (7.8125) lr 7.5131e-04 eta 0:00:21
epoch [118/200] batch [35/75] time 0.452 (0.471) data 0.320 (0.339) loss_u loss_u 0.8486 (0.9405) acc_u 25.0000 (8.1250) lr 7.5131e-04 eta 0:00:18
epoch [118/200] batch [40/75] time 0.486 (0.467) data 0.355 (0.335) loss_u loss_u 0.9336 (0.9422) acc_u 6.2500 (7.8906) lr 7.5131e-04 eta 0:00:16
epoch [118/200] batch [45/75] time 0.478 (0.467) data 0.347 (0.335) loss_u loss_u 0.9673 (0.9408) acc_u 9.3750 (8.1944) lr 7.5131e-04 eta 0:00:13
epoch [118/200] batch [50/75] time 0.363 (0.462) data 0.233 (0.330) loss_u loss_u 0.9688 (0.9418) acc_u 0.0000 (7.8750) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [55/75] time 0.511 (0.464) data 0.381 (0.333) loss_u loss_u 0.9644 (0.9416) acc_u 6.2500 (7.9545) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [60/75] time 0.649 (0.467) data 0.517 (0.336) loss_u loss_u 0.9131 (0.9417) acc_u 9.3750 (7.8646) lr 7.5131e-04 eta 0:00:07
epoch [118/200] batch [65/75] time 0.443 (0.469) data 0.311 (0.337) loss_u loss_u 0.8970 (0.9419) acc_u 9.3750 (7.6923) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [70/75] time 0.653 (0.473) data 0.521 (0.341) loss_u loss_u 0.9468 (0.9416) acc_u 9.3750 (7.7679) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [75/75] time 0.455 (0.473) data 0.323 (0.342) loss_u loss_u 0.9551 (0.9417) acc_u 3.1250 (7.7500) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1544
confident_label rate tensor(0.2254, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 707
clean true:698
clean false:9
clean_rate:0.9872701555869873
noisy true:894
noisy false:1535
after delete: len(clean_dataset) 707
after delete: len(noisy_dataset) 2429
epoch [119/200] batch [5/22] time 0.426 (0.474) data 0.295 (0.343) loss_x loss_x 0.9229 (1.2689) acc_x 81.2500 (73.1250) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [10/22] time 0.452 (0.487) data 0.321 (0.356) loss_x loss_x 0.8081 (1.1221) acc_x 84.3750 (74.0625) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [15/22] time 0.466 (0.472) data 0.335 (0.341) loss_x loss_x 0.6943 (1.0728) acc_x 81.2500 (74.7917) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [20/22] time 0.435 (0.460) data 0.304 (0.329) loss_x loss_x 0.8921 (1.0450) acc_x 78.1250 (75.1562) lr 7.3613e-04 eta 0:00:00
epoch [119/200] batch [5/75] time 0.380 (0.459) data 0.248 (0.328) loss_u loss_u 0.9521 (0.9364) acc_u 6.2500 (8.1250) lr 7.3613e-04 eta 0:00:32
epoch [119/200] batch [10/75] time 0.430 (0.458) data 0.298 (0.327) loss_u loss_u 0.9565 (0.9380) acc_u 6.2500 (7.5000) lr 7.3613e-04 eta 0:00:29
epoch [119/200] batch [15/75] time 0.369 (0.456) data 0.238 (0.325) loss_u loss_u 0.9570 (0.9480) acc_u 6.2500 (6.6667) lr 7.3613e-04 eta 0:00:27
epoch [119/200] batch [20/75] time 0.663 (0.457) data 0.530 (0.326) loss_u loss_u 0.9326 (0.9459) acc_u 6.2500 (6.8750) lr 7.3613e-04 eta 0:00:25
epoch [119/200] batch [25/75] time 0.430 (0.467) data 0.299 (0.336) loss_u loss_u 0.9355 (0.9465) acc_u 6.2500 (6.7500) lr 7.3613e-04 eta 0:00:23
epoch [119/200] batch [30/75] time 0.404 (0.466) data 0.272 (0.335) loss_u loss_u 0.9268 (0.9485) acc_u 6.2500 (6.3542) lr 7.3613e-04 eta 0:00:20
epoch [119/200] batch [35/75] time 0.714 (0.470) data 0.583 (0.338) loss_u loss_u 0.9111 (0.9460) acc_u 15.6250 (6.8750) lr 7.3613e-04 eta 0:00:18
epoch [119/200] batch [40/75] time 0.375 (0.465) data 0.244 (0.334) loss_u loss_u 0.9097 (0.9427) acc_u 12.5000 (7.2656) lr 7.3613e-04 eta 0:00:16
epoch [119/200] batch [45/75] time 0.503 (0.467) data 0.372 (0.336) loss_u loss_u 0.9214 (0.9367) acc_u 6.2500 (8.1250) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [50/75] time 0.568 (0.468) data 0.437 (0.336) loss_u loss_u 0.9189 (0.9357) acc_u 9.3750 (8.1875) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [55/75] time 0.399 (0.465) data 0.268 (0.334) loss_u loss_u 0.9521 (0.9356) acc_u 6.2500 (8.2386) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [60/75] time 0.421 (0.462) data 0.289 (0.331) loss_u loss_u 0.9624 (0.9371) acc_u 3.1250 (8.1250) lr 7.3613e-04 eta 0:00:06
epoch [119/200] batch [65/75] time 0.396 (0.461) data 0.265 (0.330) loss_u loss_u 0.9404 (0.9382) acc_u 9.3750 (7.8846) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [70/75] time 0.396 (0.461) data 0.265 (0.329) loss_u loss_u 0.9141 (0.9355) acc_u 12.5000 (8.2143) lr 7.3613e-04 eta 0:00:02
epoch [119/200] batch [75/75] time 0.478 (0.463) data 0.346 (0.332) loss_u loss_u 0.9834 (0.9346) acc_u 3.1250 (8.4167) lr 7.3613e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1549
confident_label rate tensor(0.2328, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 730
clean true:715
clean false:15
clean_rate:0.9794520547945206
noisy true:872
noisy false:1534
after delete: len(clean_dataset) 730
after delete: len(noisy_dataset) 2406
epoch [120/200] batch [5/22] time 0.485 (0.486) data 0.355 (0.356) loss_x loss_x 0.7358 (1.0396) acc_x 75.0000 (73.1250) lr 7.2101e-04 eta 0:00:08
epoch [120/200] batch [10/22] time 0.415 (0.456) data 0.284 (0.325) loss_x loss_x 1.6123 (1.2582) acc_x 71.8750 (69.0625) lr 7.2101e-04 eta 0:00:05
epoch [120/200] batch [15/22] time 0.735 (0.477) data 0.604 (0.347) loss_x loss_x 1.4414 (1.2325) acc_x 68.7500 (70.6250) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [20/22] time 0.580 (0.480) data 0.450 (0.349) loss_x loss_x 1.0273 (1.2723) acc_x 78.1250 (70.9375) lr 7.2101e-04 eta 0:00:00
epoch [120/200] batch [5/75] time 0.401 (0.464) data 0.269 (0.334) loss_u loss_u 0.8975 (0.9411) acc_u 9.3750 (6.2500) lr 7.2101e-04 eta 0:00:32
epoch [120/200] batch [10/75] time 0.408 (0.459) data 0.277 (0.329) loss_u loss_u 0.9502 (0.9407) acc_u 6.2500 (6.5625) lr 7.2101e-04 eta 0:00:29
epoch [120/200] batch [15/75] time 0.417 (0.452) data 0.285 (0.322) loss_u loss_u 0.9741 (0.9414) acc_u 3.1250 (6.8750) lr 7.2101e-04 eta 0:00:27
epoch [120/200] batch [20/75] time 0.548 (0.455) data 0.417 (0.325) loss_u loss_u 0.9180 (0.9398) acc_u 15.6250 (7.6562) lr 7.2101e-04 eta 0:00:25
epoch [120/200] batch [25/75] time 0.487 (0.456) data 0.356 (0.325) loss_u loss_u 0.9668 (0.9406) acc_u 6.2500 (7.3750) lr 7.2101e-04 eta 0:00:22
epoch [120/200] batch [30/75] time 0.494 (0.454) data 0.362 (0.324) loss_u loss_u 0.9614 (0.9445) acc_u 6.2500 (6.9792) lr 7.2101e-04 eta 0:00:20
epoch [120/200] batch [35/75] time 0.461 (0.456) data 0.328 (0.325) loss_u loss_u 0.9453 (0.9459) acc_u 9.3750 (7.0536) lr 7.2101e-04 eta 0:00:18
epoch [120/200] batch [40/75] time 0.415 (0.456) data 0.283 (0.325) loss_u loss_u 0.9385 (0.9445) acc_u 6.2500 (7.1875) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [45/75] time 0.480 (0.457) data 0.349 (0.326) loss_u loss_u 0.8833 (0.9422) acc_u 21.8750 (7.5694) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [50/75] time 0.561 (0.457) data 0.426 (0.325) loss_u loss_u 0.9907 (0.9436) acc_u 0.0000 (7.3125) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [55/75] time 0.371 (0.459) data 0.239 (0.328) loss_u loss_u 0.9253 (0.9422) acc_u 12.5000 (7.5000) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [60/75] time 0.431 (0.457) data 0.300 (0.326) loss_u loss_u 0.9189 (0.9427) acc_u 12.5000 (7.3958) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [65/75] time 0.373 (0.456) data 0.242 (0.325) loss_u loss_u 0.9116 (0.9404) acc_u 12.5000 (7.6923) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [70/75] time 0.429 (0.453) data 0.297 (0.322) loss_u loss_u 0.9180 (0.9401) acc_u 12.5000 (7.7232) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [75/75] time 0.359 (0.451) data 0.228 (0.320) loss_u loss_u 0.9365 (0.9405) acc_u 6.2500 (7.7500) lr 7.2101e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1572
confident_label rate tensor(0.2194, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 688
clean true:676
clean false:12
clean_rate:0.9825581395348837
noisy true:888
noisy false:1560
after delete: len(clean_dataset) 688
after delete: len(noisy_dataset) 2448
epoch [121/200] batch [5/21] time 0.384 (0.476) data 0.254 (0.344) loss_x loss_x 1.2910 (1.2721) acc_x 71.8750 (70.6250) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [10/21] time 0.365 (0.473) data 0.234 (0.342) loss_x loss_x 1.0977 (1.1637) acc_x 65.6250 (71.2500) lr 7.0596e-04 eta 0:00:05
epoch [121/200] batch [15/21] time 0.375 (0.469) data 0.245 (0.338) loss_x loss_x 1.1523 (1.1470) acc_x 81.2500 (71.6667) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [20/21] time 0.461 (0.461) data 0.329 (0.330) loss_x loss_x 0.9731 (1.1726) acc_x 75.0000 (70.7812) lr 7.0596e-04 eta 0:00:00
epoch [121/200] batch [5/76] time 0.351 (0.456) data 0.221 (0.324) loss_u loss_u 0.9448 (0.9506) acc_u 6.2500 (6.8750) lr 7.0596e-04 eta 0:00:32
epoch [121/200] batch [10/76] time 0.504 (0.458) data 0.373 (0.327) loss_u loss_u 0.9155 (0.9489) acc_u 6.2500 (6.2500) lr 7.0596e-04 eta 0:00:30
epoch [121/200] batch [15/76] time 0.704 (0.464) data 0.573 (0.332) loss_u loss_u 0.8442 (0.9431) acc_u 18.7500 (7.2917) lr 7.0596e-04 eta 0:00:28
epoch [121/200] batch [20/76] time 0.409 (0.458) data 0.278 (0.327) loss_u loss_u 0.9077 (0.9430) acc_u 12.5000 (7.5000) lr 7.0596e-04 eta 0:00:25
epoch [121/200] batch [25/76] time 0.413 (0.460) data 0.282 (0.328) loss_u loss_u 0.8921 (0.9396) acc_u 15.6250 (8.0000) lr 7.0596e-04 eta 0:00:23
epoch [121/200] batch [30/76] time 0.424 (0.459) data 0.292 (0.328) loss_u loss_u 0.9365 (0.9407) acc_u 6.2500 (7.8125) lr 7.0596e-04 eta 0:00:21
epoch [121/200] batch [35/76] time 0.558 (0.463) data 0.427 (0.332) loss_u loss_u 0.9551 (0.9379) acc_u 6.2500 (8.2143) lr 7.0596e-04 eta 0:00:18
epoch [121/200] batch [40/76] time 0.402 (0.461) data 0.270 (0.329) loss_u loss_u 0.8921 (0.9349) acc_u 12.5000 (8.5938) lr 7.0596e-04 eta 0:00:16
epoch [121/200] batch [45/76] time 0.430 (0.461) data 0.299 (0.329) loss_u loss_u 0.9175 (0.9347) acc_u 9.3750 (8.5417) lr 7.0596e-04 eta 0:00:14
epoch [121/200] batch [50/76] time 0.436 (0.459) data 0.304 (0.327) loss_u loss_u 0.9160 (0.9376) acc_u 15.6250 (8.1875) lr 7.0596e-04 eta 0:00:11
epoch [121/200] batch [55/76] time 0.333 (0.453) data 0.203 (0.321) loss_u loss_u 0.9209 (0.9364) acc_u 9.3750 (8.2386) lr 7.0596e-04 eta 0:00:09
epoch [121/200] batch [60/76] time 0.369 (0.451) data 0.238 (0.320) loss_u loss_u 0.9263 (0.9349) acc_u 9.3750 (8.4896) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [65/76] time 0.443 (0.450) data 0.312 (0.319) loss_u loss_u 0.9053 (0.9340) acc_u 12.5000 (8.7019) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [70/76] time 0.521 (0.451) data 0.390 (0.320) loss_u loss_u 0.9253 (0.9329) acc_u 9.3750 (8.8839) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [75/76] time 0.345 (0.449) data 0.214 (0.317) loss_u loss_u 0.9688 (0.9321) acc_u 6.2500 (9.0000) lr 7.0596e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1543
confident_label rate tensor(0.2305, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 723
clean true:714
clean false:9
clean_rate:0.9875518672199171
noisy true:879
noisy false:1534
after delete: len(clean_dataset) 723
after delete: len(noisy_dataset) 2413
epoch [122/200] batch [5/22] time 0.443 (0.440) data 0.313 (0.310) loss_x loss_x 1.2051 (1.0926) acc_x 78.1250 (76.2500) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [10/22] time 0.526 (0.468) data 0.396 (0.337) loss_x loss_x 1.0107 (1.0233) acc_x 78.1250 (76.5625) lr 6.9098e-04 eta 0:00:05
epoch [122/200] batch [15/22] time 0.458 (0.492) data 0.327 (0.361) loss_x loss_x 0.7100 (1.0202) acc_x 81.2500 (75.8333) lr 6.9098e-04 eta 0:00:03
epoch [122/200] batch [20/22] time 0.366 (0.468) data 0.236 (0.338) loss_x loss_x 1.3867 (1.0987) acc_x 65.6250 (74.6875) lr 6.9098e-04 eta 0:00:00
epoch [122/200] batch [5/75] time 0.445 (0.460) data 0.315 (0.329) loss_u loss_u 0.9712 (0.9470) acc_u 3.1250 (5.6250) lr 6.9098e-04 eta 0:00:32
epoch [122/200] batch [10/75] time 0.403 (0.453) data 0.272 (0.322) loss_u loss_u 0.9697 (0.9487) acc_u 3.1250 (5.9375) lr 6.9098e-04 eta 0:00:29
epoch [122/200] batch [15/75] time 0.377 (0.449) data 0.245 (0.319) loss_u loss_u 0.9888 (0.9490) acc_u 0.0000 (6.4583) lr 6.9098e-04 eta 0:00:26
epoch [122/200] batch [20/75] time 0.444 (0.450) data 0.313 (0.320) loss_u loss_u 0.9302 (0.9444) acc_u 9.3750 (7.1875) lr 6.9098e-04 eta 0:00:24
epoch [122/200] batch [25/75] time 0.452 (0.448) data 0.320 (0.317) loss_u loss_u 0.9517 (0.9417) acc_u 9.3750 (7.5000) lr 6.9098e-04 eta 0:00:22
epoch [122/200] batch [30/75] time 0.507 (0.451) data 0.376 (0.320) loss_u loss_u 0.9570 (0.9427) acc_u 6.2500 (7.2917) lr 6.9098e-04 eta 0:00:20
epoch [122/200] batch [35/75] time 0.376 (0.444) data 0.242 (0.314) loss_u loss_u 0.9810 (0.9443) acc_u 3.1250 (7.1429) lr 6.9098e-04 eta 0:00:17
epoch [122/200] batch [40/75] time 0.445 (0.442) data 0.314 (0.311) loss_u loss_u 0.9629 (0.9428) acc_u 3.1250 (7.3438) lr 6.9098e-04 eta 0:00:15
epoch [122/200] batch [45/75] time 0.458 (0.444) data 0.323 (0.313) loss_u loss_u 0.9355 (0.9425) acc_u 9.3750 (7.5694) lr 6.9098e-04 eta 0:00:13
epoch [122/200] batch [50/75] time 0.477 (0.444) data 0.345 (0.313) loss_u loss_u 0.9604 (0.9418) acc_u 6.2500 (7.6875) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [55/75] time 0.463 (0.446) data 0.333 (0.315) loss_u loss_u 0.9326 (0.9420) acc_u 9.3750 (7.6705) lr 6.9098e-04 eta 0:00:08
epoch [122/200] batch [60/75] time 0.570 (0.450) data 0.439 (0.319) loss_u loss_u 0.9146 (0.9413) acc_u 15.6250 (7.8646) lr 6.9098e-04 eta 0:00:06
epoch [122/200] batch [65/75] time 0.390 (0.449) data 0.259 (0.318) loss_u loss_u 0.9253 (0.9388) acc_u 9.3750 (8.1250) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [70/75] time 0.415 (0.449) data 0.283 (0.318) loss_u loss_u 0.9365 (0.9386) acc_u 9.3750 (7.9911) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [75/75] time 0.493 (0.449) data 0.362 (0.318) loss_u loss_u 0.9448 (0.9387) acc_u 9.3750 (7.9583) lr 6.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1524
confident_label rate tensor(0.2290, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 718
clean true:707
clean false:11
clean_rate:0.9846796657381616
noisy true:905
noisy false:1513
after delete: len(clean_dataset) 718
after delete: len(noisy_dataset) 2418
epoch [123/200] batch [5/22] time 0.472 (0.456) data 0.341 (0.325) loss_x loss_x 1.5156 (1.2670) acc_x 68.7500 (70.6250) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [10/22] time 0.484 (0.458) data 0.354 (0.327) loss_x loss_x 0.9292 (1.2526) acc_x 84.3750 (70.0000) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [15/22] time 0.552 (0.480) data 0.422 (0.350) loss_x loss_x 0.9102 (1.0907) acc_x 81.2500 (73.3333) lr 6.7608e-04 eta 0:00:03
epoch [123/200] batch [20/22] time 0.690 (0.477) data 0.559 (0.346) loss_x loss_x 0.9438 (1.1437) acc_x 71.8750 (72.8125) lr 6.7608e-04 eta 0:00:00
epoch [123/200] batch [5/75] time 0.342 (0.462) data 0.211 (0.331) loss_u loss_u 0.9517 (0.9183) acc_u 6.2500 (10.6250) lr 6.7608e-04 eta 0:00:32
epoch [123/200] batch [10/75] time 0.526 (0.462) data 0.395 (0.332) loss_u loss_u 0.9399 (0.9317) acc_u 6.2500 (8.4375) lr 6.7608e-04 eta 0:00:30
epoch [123/200] batch [15/75] time 0.550 (0.466) data 0.418 (0.335) loss_u loss_u 0.9189 (0.9374) acc_u 12.5000 (8.1250) lr 6.7608e-04 eta 0:00:27
epoch [123/200] batch [20/75] time 0.627 (0.463) data 0.497 (0.332) loss_u loss_u 0.8911 (0.9302) acc_u 12.5000 (8.7500) lr 6.7608e-04 eta 0:00:25
epoch [123/200] batch [25/75] time 0.533 (0.464) data 0.401 (0.333) loss_u loss_u 0.9336 (0.9335) acc_u 9.3750 (8.6250) lr 6.7608e-04 eta 0:00:23
epoch [123/200] batch [30/75] time 0.397 (0.459) data 0.267 (0.328) loss_u loss_u 0.8955 (0.9355) acc_u 12.5000 (8.3333) lr 6.7608e-04 eta 0:00:20
epoch [123/200] batch [35/75] time 0.438 (0.453) data 0.306 (0.322) loss_u loss_u 0.9087 (0.9358) acc_u 15.6250 (8.3929) lr 6.7608e-04 eta 0:00:18
epoch [123/200] batch [40/75] time 0.413 (0.454) data 0.281 (0.323) loss_u loss_u 0.9927 (0.9353) acc_u 0.0000 (8.4375) lr 6.7608e-04 eta 0:00:15
epoch [123/200] batch [45/75] time 0.452 (0.455) data 0.320 (0.324) loss_u loss_u 0.9453 (0.9342) acc_u 6.2500 (8.6111) lr 6.7608e-04 eta 0:00:13
epoch [123/200] batch [50/75] time 0.457 (0.452) data 0.325 (0.321) loss_u loss_u 0.9399 (0.9373) acc_u 9.3750 (8.2500) lr 6.7608e-04 eta 0:00:11
epoch [123/200] batch [55/75] time 0.446 (0.456) data 0.314 (0.325) loss_u loss_u 0.9360 (0.9365) acc_u 9.3750 (8.2955) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [60/75] time 0.465 (0.454) data 0.333 (0.323) loss_u loss_u 0.9609 (0.9356) acc_u 3.1250 (8.3854) lr 6.7608e-04 eta 0:00:06
epoch [123/200] batch [65/75] time 0.461 (0.452) data 0.330 (0.321) loss_u loss_u 0.9751 (0.9365) acc_u 3.1250 (8.2212) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [70/75] time 0.365 (0.451) data 0.233 (0.320) loss_u loss_u 0.8926 (0.9364) acc_u 12.5000 (8.2589) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [75/75] time 0.445 (0.452) data 0.312 (0.321) loss_u loss_u 0.9570 (0.9364) acc_u 3.1250 (8.1250) lr 6.7608e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1544
confident_label rate tensor(0.2261, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 709
clean true:697
clean false:12
clean_rate:0.9830747531734838
noisy true:895
noisy false:1532
after delete: len(clean_dataset) 709
after delete: len(noisy_dataset) 2427
epoch [124/200] batch [5/22] time 0.403 (0.448) data 0.272 (0.318) loss_x loss_x 1.1797 (1.2006) acc_x 75.0000 (74.3750) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [10/22] time 0.469 (0.467) data 0.339 (0.336) loss_x loss_x 1.0352 (1.1763) acc_x 75.0000 (72.8125) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [15/22] time 0.452 (0.460) data 0.321 (0.329) loss_x loss_x 1.6104 (1.2036) acc_x 65.6250 (71.4583) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [20/22] time 0.354 (0.458) data 0.224 (0.327) loss_x loss_x 1.1094 (1.2065) acc_x 68.7500 (71.4062) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/75] time 0.449 (0.459) data 0.317 (0.329) loss_u loss_u 0.9858 (0.9433) acc_u 0.0000 (5.6250) lr 6.6126e-04 eta 0:00:32
epoch [124/200] batch [10/75] time 0.492 (0.463) data 0.360 (0.332) loss_u loss_u 0.8906 (0.9265) acc_u 12.5000 (7.5000) lr 6.6126e-04 eta 0:00:30
epoch [124/200] batch [15/75] time 0.335 (0.459) data 0.204 (0.328) loss_u loss_u 0.9712 (0.9252) acc_u 3.1250 (8.5417) lr 6.6126e-04 eta 0:00:27
epoch [124/200] batch [20/75] time 0.364 (0.455) data 0.233 (0.325) loss_u loss_u 0.9351 (0.9313) acc_u 6.2500 (8.1250) lr 6.6126e-04 eta 0:00:25
epoch [124/200] batch [25/75] time 0.463 (0.451) data 0.332 (0.321) loss_u loss_u 0.9399 (0.9323) acc_u 0.0000 (7.7500) lr 6.6126e-04 eta 0:00:22
epoch [124/200] batch [30/75] time 0.446 (0.445) data 0.315 (0.315) loss_u loss_u 0.9238 (0.9289) acc_u 9.3750 (8.7500) lr 6.6126e-04 eta 0:00:20
epoch [124/200] batch [35/75] time 0.416 (0.447) data 0.284 (0.316) loss_u loss_u 0.9146 (0.9306) acc_u 12.5000 (8.6607) lr 6.6126e-04 eta 0:00:17
epoch [124/200] batch [40/75] time 0.387 (0.446) data 0.255 (0.315) loss_u loss_u 0.9019 (0.9300) acc_u 12.5000 (8.5938) lr 6.6126e-04 eta 0:00:15
epoch [124/200] batch [45/75] time 0.506 (0.451) data 0.374 (0.319) loss_u loss_u 0.9473 (0.9299) acc_u 9.3750 (8.8194) lr 6.6126e-04 eta 0:00:13
epoch [124/200] batch [50/75] time 0.419 (0.452) data 0.288 (0.321) loss_u loss_u 0.8926 (0.9313) acc_u 9.3750 (8.3750) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [55/75] time 0.403 (0.454) data 0.273 (0.323) loss_u loss_u 0.9507 (0.9323) acc_u 6.2500 (8.1818) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [60/75] time 0.431 (0.453) data 0.300 (0.322) loss_u loss_u 0.8921 (0.9311) acc_u 12.5000 (8.2812) lr 6.6126e-04 eta 0:00:06
epoch [124/200] batch [65/75] time 0.415 (0.456) data 0.283 (0.325) loss_u loss_u 0.9277 (0.9322) acc_u 9.3750 (8.2212) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [70/75] time 0.531 (0.457) data 0.400 (0.325) loss_u loss_u 0.9180 (0.9321) acc_u 12.5000 (8.2589) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [75/75] time 0.419 (0.454) data 0.288 (0.323) loss_u loss_u 0.9272 (0.9330) acc_u 6.2500 (8.1667) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1548
confident_label rate tensor(0.2258, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 708
clean true:694
clean false:14
clean_rate:0.980225988700565
noisy true:894
noisy false:1534
after delete: len(clean_dataset) 708
after delete: len(noisy_dataset) 2428
epoch [125/200] batch [5/22] time 0.442 (0.463) data 0.311 (0.333) loss_x loss_x 0.6694 (1.2616) acc_x 87.5000 (73.1250) lr 6.4653e-04 eta 0:00:07
epoch [125/200] batch [10/22] time 0.485 (0.482) data 0.354 (0.352) loss_x loss_x 1.1191 (1.2029) acc_x 68.7500 (72.5000) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [15/22] time 0.451 (0.465) data 0.320 (0.334) loss_x loss_x 1.6133 (1.2024) acc_x 65.6250 (71.6667) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [20/22] time 0.470 (0.470) data 0.339 (0.340) loss_x loss_x 1.0850 (1.1355) acc_x 81.2500 (72.9688) lr 6.4653e-04 eta 0:00:00
epoch [125/200] batch [5/75] time 0.451 (0.461) data 0.321 (0.330) loss_u loss_u 0.9189 (0.9258) acc_u 9.3750 (9.3750) lr 6.4653e-04 eta 0:00:32
epoch [125/200] batch [10/75] time 0.437 (0.457) data 0.306 (0.326) loss_u loss_u 0.8662 (0.9272) acc_u 15.6250 (9.3750) lr 6.4653e-04 eta 0:00:29
epoch [125/200] batch [15/75] time 0.413 (0.450) data 0.281 (0.320) loss_u loss_u 0.9771 (0.9322) acc_u 3.1250 (8.3333) lr 6.4653e-04 eta 0:00:27
epoch [125/200] batch [20/75] time 0.469 (0.448) data 0.337 (0.317) loss_u loss_u 0.9097 (0.9321) acc_u 12.5000 (8.4375) lr 6.4653e-04 eta 0:00:24
epoch [125/200] batch [25/75] time 0.374 (0.442) data 0.241 (0.311) loss_u loss_u 0.9331 (0.9299) acc_u 9.3750 (8.5000) lr 6.4653e-04 eta 0:00:22
epoch [125/200] batch [30/75] time 0.385 (0.444) data 0.253 (0.313) loss_u loss_u 0.9292 (0.9339) acc_u 9.3750 (8.1250) lr 6.4653e-04 eta 0:00:19
epoch [125/200] batch [35/75] time 0.441 (0.445) data 0.309 (0.314) loss_u loss_u 0.9370 (0.9390) acc_u 6.2500 (7.6786) lr 6.4653e-04 eta 0:00:17
epoch [125/200] batch [40/75] time 0.522 (0.452) data 0.390 (0.320) loss_u loss_u 0.9668 (0.9417) acc_u 3.1250 (7.3438) lr 6.4653e-04 eta 0:00:15
epoch [125/200] batch [45/75] time 0.440 (0.454) data 0.309 (0.322) loss_u loss_u 0.9448 (0.9421) acc_u 3.1250 (7.2222) lr 6.4653e-04 eta 0:00:13
epoch [125/200] batch [50/75] time 0.589 (0.457) data 0.456 (0.326) loss_u loss_u 0.9941 (0.9435) acc_u 0.0000 (7.0000) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [55/75] time 0.519 (0.461) data 0.388 (0.329) loss_u loss_u 0.9238 (0.9440) acc_u 9.3750 (6.9318) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [60/75] time 0.429 (0.461) data 0.298 (0.329) loss_u loss_u 0.9399 (0.9441) acc_u 6.2500 (7.0312) lr 6.4653e-04 eta 0:00:06
epoch [125/200] batch [65/75] time 0.741 (0.463) data 0.609 (0.331) loss_u loss_u 0.9878 (0.9433) acc_u 0.0000 (7.0673) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [70/75] time 0.421 (0.463) data 0.290 (0.331) loss_u loss_u 0.9302 (0.9423) acc_u 9.3750 (7.1875) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [75/75] time 0.683 (0.464) data 0.551 (0.332) loss_u loss_u 0.9717 (0.9420) acc_u 6.2500 (7.2917) lr 6.4653e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1559
confident_label rate tensor(0.2277, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 714
clean true:704
clean false:10
clean_rate:0.9859943977591037
noisy true:873
noisy false:1549
after delete: len(clean_dataset) 714
after delete: len(noisy_dataset) 2422
epoch [126/200] batch [5/22] time 0.538 (0.464) data 0.407 (0.333) loss_x loss_x 1.0264 (1.1715) acc_x 68.7500 (68.7500) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [10/22] time 0.469 (0.505) data 0.338 (0.374) loss_x loss_x 1.0312 (1.0418) acc_x 71.8750 (71.5625) lr 6.3188e-04 eta 0:00:06
epoch [126/200] batch [15/22] time 0.537 (0.487) data 0.406 (0.355) loss_x loss_x 1.0830 (1.0470) acc_x 75.0000 (71.0417) lr 6.3188e-04 eta 0:00:03
epoch [126/200] batch [20/22] time 0.534 (0.492) data 0.403 (0.361) loss_x loss_x 1.4229 (1.0584) acc_x 68.7500 (71.7188) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/75] time 0.644 (0.497) data 0.512 (0.366) loss_u loss_u 0.9150 (0.9347) acc_u 12.5000 (9.3750) lr 6.3188e-04 eta 0:00:34
epoch [126/200] batch [10/75] time 0.410 (0.486) data 0.277 (0.355) loss_u loss_u 0.9102 (0.9264) acc_u 12.5000 (9.3750) lr 6.3188e-04 eta 0:00:31
epoch [126/200] batch [15/75] time 0.364 (0.473) data 0.233 (0.342) loss_u loss_u 0.9585 (0.9315) acc_u 3.1250 (8.5417) lr 6.3188e-04 eta 0:00:28
epoch [126/200] batch [20/75] time 0.440 (0.469) data 0.308 (0.337) loss_u loss_u 0.9355 (0.9325) acc_u 6.2500 (8.5938) lr 6.3188e-04 eta 0:00:25
epoch [126/200] batch [25/75] time 0.479 (0.466) data 0.347 (0.335) loss_u loss_u 0.9497 (0.9364) acc_u 6.2500 (8.1250) lr 6.3188e-04 eta 0:00:23
epoch [126/200] batch [30/75] time 0.525 (0.463) data 0.393 (0.331) loss_u loss_u 0.9458 (0.9357) acc_u 6.2500 (7.9167) lr 6.3188e-04 eta 0:00:20
epoch [126/200] batch [35/75] time 0.429 (0.464) data 0.298 (0.332) loss_u loss_u 0.9424 (0.9352) acc_u 6.2500 (8.1250) lr 6.3188e-04 eta 0:00:18
epoch [126/200] batch [40/75] time 0.396 (0.463) data 0.264 (0.331) loss_u loss_u 0.9858 (0.9370) acc_u 3.1250 (7.9688) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [45/75] time 0.529 (0.463) data 0.398 (0.331) loss_u loss_u 0.9570 (0.9385) acc_u 9.3750 (7.9861) lr 6.3188e-04 eta 0:00:13
epoch [126/200] batch [50/75] time 0.480 (0.463) data 0.348 (0.331) loss_u loss_u 0.9844 (0.9412) acc_u 0.0000 (7.6250) lr 6.3188e-04 eta 0:00:11
epoch [126/200] batch [55/75] time 0.473 (0.462) data 0.342 (0.331) loss_u loss_u 0.9199 (0.9420) acc_u 9.3750 (7.5000) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [60/75] time 0.376 (0.459) data 0.244 (0.328) loss_u loss_u 0.9736 (0.9433) acc_u 3.1250 (7.3958) lr 6.3188e-04 eta 0:00:06
epoch [126/200] batch [65/75] time 0.376 (0.458) data 0.243 (0.326) loss_u loss_u 0.9302 (0.9434) acc_u 9.3750 (7.5000) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [70/75] time 0.480 (0.456) data 0.349 (0.324) loss_u loss_u 0.9565 (0.9444) acc_u 6.2500 (7.3214) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [75/75] time 0.458 (0.456) data 0.326 (0.325) loss_u loss_u 0.8989 (0.9424) acc_u 12.5000 (7.5000) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1530
confident_label rate tensor(0.2290, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 718
clean true:703
clean false:15
clean_rate:0.979108635097493
noisy true:903
noisy false:1515
after delete: len(clean_dataset) 718
after delete: len(noisy_dataset) 2418
epoch [127/200] batch [5/22] time 0.595 (0.462) data 0.463 (0.330) loss_x loss_x 1.2109 (1.2115) acc_x 65.6250 (71.2500) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [10/22] time 0.439 (0.480) data 0.308 (0.349) loss_x loss_x 1.1182 (1.1030) acc_x 71.8750 (72.5000) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [15/22] time 0.379 (0.464) data 0.249 (0.333) loss_x loss_x 1.1367 (1.1189) acc_x 65.6250 (73.1250) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [20/22] time 0.380 (0.463) data 0.249 (0.332) loss_x loss_x 1.1748 (1.1361) acc_x 68.7500 (73.4375) lr 6.1732e-04 eta 0:00:00
epoch [127/200] batch [5/75] time 0.351 (0.451) data 0.221 (0.320) loss_u loss_u 0.9727 (0.9583) acc_u 3.1250 (5.6250) lr 6.1732e-04 eta 0:00:31
epoch [127/200] batch [10/75] time 0.374 (0.443) data 0.243 (0.312) loss_u loss_u 0.9502 (0.9480) acc_u 6.2500 (6.5625) lr 6.1732e-04 eta 0:00:28
epoch [127/200] batch [15/75] time 0.438 (0.443) data 0.306 (0.312) loss_u loss_u 0.9478 (0.9485) acc_u 6.2500 (6.6667) lr 6.1732e-04 eta 0:00:26
epoch [127/200] batch [20/75] time 0.473 (0.447) data 0.341 (0.316) loss_u loss_u 0.9917 (0.9468) acc_u 0.0000 (6.8750) lr 6.1732e-04 eta 0:00:24
epoch [127/200] batch [25/75] time 0.520 (0.444) data 0.387 (0.313) loss_u loss_u 0.9517 (0.9423) acc_u 6.2500 (7.2500) lr 6.1732e-04 eta 0:00:22
epoch [127/200] batch [30/75] time 0.433 (0.440) data 0.302 (0.309) loss_u loss_u 0.9209 (0.9417) acc_u 12.5000 (7.5000) lr 6.1732e-04 eta 0:00:19
epoch [127/200] batch [35/75] time 0.515 (0.444) data 0.382 (0.313) loss_u loss_u 0.9185 (0.9394) acc_u 9.3750 (7.7679) lr 6.1732e-04 eta 0:00:17
epoch [127/200] batch [40/75] time 0.387 (0.444) data 0.252 (0.313) loss_u loss_u 0.9492 (0.9391) acc_u 3.1250 (7.6562) lr 6.1732e-04 eta 0:00:15
epoch [127/200] batch [45/75] time 0.441 (0.446) data 0.308 (0.314) loss_u loss_u 0.9365 (0.9380) acc_u 12.5000 (7.7083) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [50/75] time 0.407 (0.445) data 0.275 (0.313) loss_u loss_u 0.9204 (0.9370) acc_u 6.2500 (7.6875) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [55/75] time 0.445 (0.446) data 0.313 (0.314) loss_u loss_u 0.8984 (0.9368) acc_u 9.3750 (7.6705) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [60/75] time 0.645 (0.451) data 0.514 (0.319) loss_u loss_u 0.9326 (0.9362) acc_u 12.5000 (7.9688) lr 6.1732e-04 eta 0:00:06
epoch [127/200] batch [65/75] time 0.546 (0.453) data 0.414 (0.321) loss_u loss_u 0.9795 (0.9371) acc_u 6.2500 (7.9808) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [70/75] time 0.390 (0.455) data 0.258 (0.323) loss_u loss_u 0.9307 (0.9370) acc_u 9.3750 (7.9018) lr 6.1732e-04 eta 0:00:02
epoch [127/200] batch [75/75] time 0.469 (0.453) data 0.338 (0.322) loss_u loss_u 0.9375 (0.9375) acc_u 6.2500 (7.8750) lr 6.1732e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1578
confident_label rate tensor(0.2267, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 711
clean true:700
clean false:11
clean_rate:0.9845288326300985
noisy true:858
noisy false:1567
after delete: len(clean_dataset) 711
after delete: len(noisy_dataset) 2425
epoch [128/200] batch [5/22] time 0.361 (0.461) data 0.230 (0.330) loss_x loss_x 1.1777 (1.1369) acc_x 71.8750 (73.1250) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [10/22] time 0.652 (0.477) data 0.521 (0.346) loss_x loss_x 1.0049 (1.0458) acc_x 78.1250 (74.3750) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [15/22] time 0.554 (0.476) data 0.424 (0.346) loss_x loss_x 1.3340 (1.1182) acc_x 62.5000 (73.1250) lr 6.0285e-04 eta 0:00:03
epoch [128/200] batch [20/22] time 0.507 (0.474) data 0.376 (0.343) loss_x loss_x 1.7324 (1.1989) acc_x 56.2500 (71.2500) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/75] time 0.534 (0.475) data 0.404 (0.344) loss_u loss_u 0.9023 (0.9412) acc_u 12.5000 (7.5000) lr 6.0285e-04 eta 0:00:33
epoch [128/200] batch [10/75] time 0.419 (0.471) data 0.287 (0.340) loss_u loss_u 0.9634 (0.9476) acc_u 6.2500 (6.2500) lr 6.0285e-04 eta 0:00:30
epoch [128/200] batch [15/75] time 0.479 (0.467) data 0.347 (0.336) loss_u loss_u 0.8965 (0.9473) acc_u 12.5000 (6.8750) lr 6.0285e-04 eta 0:00:28
epoch [128/200] batch [20/75] time 0.537 (0.471) data 0.405 (0.340) loss_u loss_u 0.9180 (0.9447) acc_u 6.2500 (7.1875) lr 6.0285e-04 eta 0:00:25
epoch [128/200] batch [25/75] time 0.380 (0.465) data 0.249 (0.333) loss_u loss_u 0.9639 (0.9437) acc_u 6.2500 (7.5000) lr 6.0285e-04 eta 0:00:23
epoch [128/200] batch [30/75] time 0.447 (0.462) data 0.316 (0.330) loss_u loss_u 0.8809 (0.9417) acc_u 12.5000 (7.6042) lr 6.0285e-04 eta 0:00:20
epoch [128/200] batch [35/75] time 0.398 (0.456) data 0.268 (0.325) loss_u loss_u 0.9360 (0.9422) acc_u 9.3750 (7.5000) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [40/75] time 0.348 (0.454) data 0.216 (0.323) loss_u loss_u 0.9580 (0.9417) acc_u 6.2500 (7.5781) lr 6.0285e-04 eta 0:00:15
epoch [128/200] batch [45/75] time 0.560 (0.453) data 0.427 (0.322) loss_u loss_u 0.9214 (0.9416) acc_u 9.3750 (7.6389) lr 6.0285e-04 eta 0:00:13
epoch [128/200] batch [50/75] time 0.420 (0.454) data 0.288 (0.323) loss_u loss_u 0.9312 (0.9419) acc_u 9.3750 (7.6250) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [55/75] time 0.406 (0.455) data 0.275 (0.324) loss_u loss_u 0.9106 (0.9414) acc_u 12.5000 (7.6136) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [60/75] time 0.396 (0.454) data 0.265 (0.322) loss_u loss_u 0.9346 (0.9410) acc_u 6.2500 (7.6562) lr 6.0285e-04 eta 0:00:06
epoch [128/200] batch [65/75] time 0.637 (0.457) data 0.506 (0.326) loss_u loss_u 0.9849 (0.9409) acc_u 0.0000 (7.5481) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [70/75] time 0.504 (0.459) data 0.373 (0.327) loss_u loss_u 0.9795 (0.9402) acc_u 3.1250 (7.6786) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [75/75] time 0.459 (0.456) data 0.327 (0.325) loss_u loss_u 0.8828 (0.9402) acc_u 18.7500 (7.7083) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1513
confident_label rate tensor(0.2299, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 721
clean true:709
clean false:12
clean_rate:0.9833564493758669
noisy true:914
noisy false:1501
after delete: len(clean_dataset) 721
after delete: len(noisy_dataset) 2415
epoch [129/200] batch [5/22] time 0.516 (0.441) data 0.386 (0.310) loss_x loss_x 1.0752 (1.1891) acc_x 71.8750 (69.3750) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [10/22] time 0.462 (0.459) data 0.332 (0.329) loss_x loss_x 1.9854 (1.2050) acc_x 65.6250 (71.8750) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [15/22] time 0.394 (0.456) data 0.264 (0.325) loss_x loss_x 1.5791 (1.1549) acc_x 62.5000 (72.5000) lr 5.8849e-04 eta 0:00:03
epoch [129/200] batch [20/22] time 0.342 (0.452) data 0.212 (0.321) loss_x loss_x 1.1045 (1.1709) acc_x 75.0000 (72.5000) lr 5.8849e-04 eta 0:00:00
epoch [129/200] batch [5/75] time 0.473 (0.459) data 0.342 (0.328) loss_u loss_u 0.8813 (0.9354) acc_u 15.6250 (8.7500) lr 5.8849e-04 eta 0:00:32
epoch [129/200] batch [10/75] time 0.320 (0.449) data 0.189 (0.318) loss_u loss_u 0.9043 (0.9434) acc_u 9.3750 (7.1875) lr 5.8849e-04 eta 0:00:29
epoch [129/200] batch [15/75] time 0.322 (0.445) data 0.191 (0.314) loss_u loss_u 0.8652 (0.9388) acc_u 18.7500 (7.9167) lr 5.8849e-04 eta 0:00:26
epoch [129/200] batch [20/75] time 0.444 (0.443) data 0.313 (0.312) loss_u loss_u 0.9678 (0.9417) acc_u 6.2500 (7.5000) lr 5.8849e-04 eta 0:00:24
epoch [129/200] batch [25/75] time 0.318 (0.436) data 0.187 (0.305) loss_u loss_u 0.9580 (0.9427) acc_u 3.1250 (7.3750) lr 5.8849e-04 eta 0:00:21
epoch [129/200] batch [30/75] time 0.473 (0.432) data 0.341 (0.301) loss_u loss_u 0.9644 (0.9417) acc_u 6.2500 (7.5000) lr 5.8849e-04 eta 0:00:19
epoch [129/200] batch [35/75] time 0.650 (0.439) data 0.515 (0.308) loss_u loss_u 0.9541 (0.9371) acc_u 9.3750 (8.0357) lr 5.8849e-04 eta 0:00:17
epoch [129/200] batch [40/75] time 0.435 (0.440) data 0.303 (0.309) loss_u loss_u 0.9678 (0.9410) acc_u 3.1250 (7.4219) lr 5.8849e-04 eta 0:00:15
epoch [129/200] batch [45/75] time 0.479 (0.443) data 0.348 (0.312) loss_u loss_u 0.9082 (0.9394) acc_u 9.3750 (7.7083) lr 5.8849e-04 eta 0:00:13
epoch [129/200] batch [50/75] time 0.453 (0.443) data 0.321 (0.312) loss_u loss_u 0.9707 (0.9414) acc_u 0.0000 (7.3750) lr 5.8849e-04 eta 0:00:11
epoch [129/200] batch [55/75] time 0.570 (0.449) data 0.440 (0.318) loss_u loss_u 0.9341 (0.9411) acc_u 9.3750 (7.5000) lr 5.8849e-04 eta 0:00:08
epoch [129/200] batch [60/75] time 0.372 (0.450) data 0.242 (0.319) loss_u loss_u 0.9575 (0.9420) acc_u 6.2500 (7.3438) lr 5.8849e-04 eta 0:00:06
epoch [129/200] batch [65/75] time 0.397 (0.449) data 0.267 (0.318) loss_u loss_u 0.9541 (0.9412) acc_u 3.1250 (7.4519) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [70/75] time 0.493 (0.452) data 0.363 (0.320) loss_u loss_u 0.9097 (0.9399) acc_u 9.3750 (7.5446) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [75/75] time 0.478 (0.452) data 0.346 (0.321) loss_u loss_u 0.8828 (0.9390) acc_u 15.6250 (7.7500) lr 5.8849e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1539
confident_label rate tensor(0.2305, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 723
clean true:712
clean false:11
clean_rate:0.9847856154910097
noisy true:885
noisy false:1528
after delete: len(clean_dataset) 723
after delete: len(noisy_dataset) 2413
epoch [130/200] batch [5/22] time 0.390 (0.434) data 0.259 (0.303) loss_x loss_x 1.4854 (1.1735) acc_x 65.6250 (72.5000) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [10/22] time 0.768 (0.493) data 0.633 (0.361) loss_x loss_x 1.3027 (1.1445) acc_x 68.7500 (72.8125) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [15/22] time 0.486 (0.484) data 0.356 (0.353) loss_x loss_x 1.1221 (1.1660) acc_x 71.8750 (73.1250) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [20/22] time 0.436 (0.476) data 0.305 (0.345) loss_x loss_x 0.7661 (1.1429) acc_x 87.5000 (73.5938) lr 5.7422e-04 eta 0:00:00
epoch [130/200] batch [5/75] time 0.552 (0.478) data 0.421 (0.347) loss_u loss_u 0.9121 (0.9362) acc_u 9.3750 (8.1250) lr 5.7422e-04 eta 0:00:33
epoch [130/200] batch [10/75] time 0.447 (0.477) data 0.317 (0.346) loss_u loss_u 0.9385 (0.9266) acc_u 9.3750 (9.3750) lr 5.7422e-04 eta 0:00:30
epoch [130/200] batch [15/75] time 0.529 (0.482) data 0.397 (0.351) loss_u loss_u 0.9663 (0.9334) acc_u 3.1250 (8.7500) lr 5.7422e-04 eta 0:00:28
epoch [130/200] batch [20/75] time 0.392 (0.474) data 0.260 (0.343) loss_u loss_u 0.9287 (0.9313) acc_u 9.3750 (9.0625) lr 5.7422e-04 eta 0:00:26
epoch [130/200] batch [25/75] time 0.417 (0.471) data 0.287 (0.340) loss_u loss_u 0.9805 (0.9309) acc_u 0.0000 (8.7500) lr 5.7422e-04 eta 0:00:23
epoch [130/200] batch [30/75] time 0.379 (0.470) data 0.247 (0.339) loss_u loss_u 0.9268 (0.9331) acc_u 9.3750 (8.4375) lr 5.7422e-04 eta 0:00:21
epoch [130/200] batch [35/75] time 0.436 (0.465) data 0.305 (0.334) loss_u loss_u 0.9805 (0.9372) acc_u 3.1250 (7.9464) lr 5.7422e-04 eta 0:00:18
epoch [130/200] batch [40/75] time 0.554 (0.464) data 0.421 (0.333) loss_u loss_u 0.9478 (0.9374) acc_u 6.2500 (7.9688) lr 5.7422e-04 eta 0:00:16
epoch [130/200] batch [45/75] time 0.473 (0.463) data 0.342 (0.331) loss_u loss_u 0.9658 (0.9370) acc_u 3.1250 (8.1250) lr 5.7422e-04 eta 0:00:13
epoch [130/200] batch [50/75] time 0.459 (0.465) data 0.328 (0.334) loss_u loss_u 0.9282 (0.9383) acc_u 9.3750 (8.0000) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [55/75] time 0.400 (0.465) data 0.269 (0.334) loss_u loss_u 0.9277 (0.9378) acc_u 9.3750 (8.0682) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [60/75] time 0.550 (0.465) data 0.419 (0.334) loss_u loss_u 0.9888 (0.9394) acc_u 0.0000 (7.9167) lr 5.7422e-04 eta 0:00:06
epoch [130/200] batch [65/75] time 0.418 (0.468) data 0.287 (0.336) loss_u loss_u 0.9136 (0.9393) acc_u 9.3750 (7.9808) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [70/75] time 0.354 (0.465) data 0.223 (0.334) loss_u loss_u 0.8779 (0.9394) acc_u 12.5000 (7.9018) lr 5.7422e-04 eta 0:00:02
epoch [130/200] batch [75/75] time 0.374 (0.463) data 0.243 (0.332) loss_u loss_u 0.9097 (0.9396) acc_u 12.5000 (7.9167) lr 5.7422e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1547
confident_label rate tensor(0.2293, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 719
clean true:707
clean false:12
clean_rate:0.9833101529902643
noisy true:882
noisy false:1535
after delete: len(clean_dataset) 719
after delete: len(noisy_dataset) 2417
epoch [131/200] batch [5/22] time 0.485 (0.477) data 0.354 (0.346) loss_x loss_x 1.0664 (1.1154) acc_x 62.5000 (73.1250) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [10/22] time 0.346 (0.459) data 0.215 (0.328) loss_x loss_x 1.1113 (1.1179) acc_x 65.6250 (72.1875) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [15/22] time 0.505 (0.460) data 0.374 (0.330) loss_x loss_x 0.9268 (1.1168) acc_x 84.3750 (72.9167) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [20/22] time 0.483 (0.467) data 0.353 (0.337) loss_x loss_x 0.8345 (1.0870) acc_x 75.0000 (73.2812) lr 5.6006e-04 eta 0:00:00
epoch [131/200] batch [5/75] time 0.401 (0.463) data 0.270 (0.333) loss_u loss_u 0.9160 (0.9382) acc_u 12.5000 (8.1250) lr 5.6006e-04 eta 0:00:32
epoch [131/200] batch [10/75] time 0.421 (0.456) data 0.291 (0.325) loss_u loss_u 0.9639 (0.9458) acc_u 6.2500 (6.8750) lr 5.6006e-04 eta 0:00:29
epoch [131/200] batch [15/75] time 0.434 (0.459) data 0.302 (0.329) loss_u loss_u 0.9209 (0.9418) acc_u 9.3750 (7.0833) lr 5.6006e-04 eta 0:00:27
epoch [131/200] batch [20/75] time 0.499 (0.455) data 0.369 (0.324) loss_u loss_u 0.9595 (0.9438) acc_u 6.2500 (7.0312) lr 5.6006e-04 eta 0:00:25
epoch [131/200] batch [25/75] time 0.517 (0.461) data 0.387 (0.330) loss_u loss_u 0.9224 (0.9418) acc_u 9.3750 (7.5000) lr 5.6006e-04 eta 0:00:23
epoch [131/200] batch [30/75] time 0.340 (0.455) data 0.209 (0.324) loss_u loss_u 0.9795 (0.9424) acc_u 3.1250 (7.2917) lr 5.6006e-04 eta 0:00:20
epoch [131/200] batch [35/75] time 0.367 (0.453) data 0.235 (0.322) loss_u loss_u 0.9658 (0.9401) acc_u 3.1250 (7.5000) lr 5.6006e-04 eta 0:00:18
epoch [131/200] batch [40/75] time 0.500 (0.453) data 0.368 (0.322) loss_u loss_u 0.9204 (0.9395) acc_u 9.3750 (7.5781) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [45/75] time 0.397 (0.452) data 0.267 (0.321) loss_u loss_u 0.8882 (0.9335) acc_u 12.5000 (8.3333) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [50/75] time 0.527 (0.450) data 0.396 (0.319) loss_u loss_u 0.9062 (0.9357) acc_u 9.3750 (8.0000) lr 5.6006e-04 eta 0:00:11
epoch [131/200] batch [55/75] time 0.448 (0.449) data 0.317 (0.318) loss_u loss_u 0.9136 (0.9345) acc_u 12.5000 (8.1250) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [60/75] time 0.626 (0.450) data 0.495 (0.319) loss_u loss_u 0.8960 (0.9347) acc_u 9.3750 (8.0208) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [65/75] time 0.419 (0.453) data 0.286 (0.322) loss_u loss_u 0.9302 (0.9347) acc_u 12.5000 (8.0769) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [70/75] time 0.468 (0.456) data 0.334 (0.325) loss_u loss_u 0.9224 (0.9348) acc_u 9.3750 (7.9911) lr 5.6006e-04 eta 0:00:02
epoch [131/200] batch [75/75] time 0.567 (0.458) data 0.433 (0.326) loss_u loss_u 0.8916 (0.9342) acc_u 12.5000 (8.0417) lr 5.6006e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1499
confident_label rate tensor(0.2382, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 747
clean true:732
clean false:15
clean_rate:0.9799196787148594
noisy true:905
noisy false:1484
after delete: len(clean_dataset) 747
after delete: len(noisy_dataset) 2389
epoch [132/200] batch [5/23] time 0.688 (0.571) data 0.558 (0.440) loss_x loss_x 1.0752 (1.1370) acc_x 71.8750 (71.8750) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [10/23] time 0.508 (0.519) data 0.378 (0.388) loss_x loss_x 1.7646 (1.2497) acc_x 59.3750 (69.0625) lr 5.4601e-04 eta 0:00:06
epoch [132/200] batch [15/23] time 0.465 (0.481) data 0.334 (0.350) loss_x loss_x 1.0625 (1.2516) acc_x 81.2500 (69.1667) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [20/23] time 0.503 (0.471) data 0.374 (0.341) loss_x loss_x 0.7495 (1.2072) acc_x 81.2500 (70.3125) lr 5.4601e-04 eta 0:00:01
epoch [132/200] batch [5/74] time 0.481 (0.462) data 0.349 (0.332) loss_u loss_u 0.9092 (0.9270) acc_u 12.5000 (8.7500) lr 5.4601e-04 eta 0:00:31
epoch [132/200] batch [10/74] time 0.479 (0.464) data 0.348 (0.333) loss_u loss_u 0.9751 (0.9435) acc_u 3.1250 (7.1875) lr 5.4601e-04 eta 0:00:29
epoch [132/200] batch [15/74] time 0.580 (0.470) data 0.449 (0.339) loss_u loss_u 0.9312 (0.9481) acc_u 12.5000 (6.8750) lr 5.4601e-04 eta 0:00:27
epoch [132/200] batch [20/74] time 0.372 (0.464) data 0.241 (0.333) loss_u loss_u 0.8760 (0.9422) acc_u 15.6250 (7.5000) lr 5.4601e-04 eta 0:00:25
epoch [132/200] batch [25/74] time 0.436 (0.462) data 0.305 (0.331) loss_u loss_u 0.9707 (0.9437) acc_u 3.1250 (7.3750) lr 5.4601e-04 eta 0:00:22
epoch [132/200] batch [30/74] time 0.409 (0.460) data 0.277 (0.329) loss_u loss_u 0.9395 (0.9408) acc_u 6.2500 (7.9167) lr 5.4601e-04 eta 0:00:20
epoch [132/200] batch [35/74] time 0.614 (0.461) data 0.479 (0.330) loss_u loss_u 0.9624 (0.9417) acc_u 6.2500 (7.6786) lr 5.4601e-04 eta 0:00:17
epoch [132/200] batch [40/74] time 0.551 (0.463) data 0.420 (0.332) loss_u loss_u 0.9629 (0.9411) acc_u 3.1250 (7.5000) lr 5.4601e-04 eta 0:00:15
epoch [132/200] batch [45/74] time 0.465 (0.462) data 0.333 (0.331) loss_u loss_u 0.8936 (0.9382) acc_u 15.6250 (7.9167) lr 5.4601e-04 eta 0:00:13
epoch [132/200] batch [50/74] time 0.387 (0.459) data 0.256 (0.328) loss_u loss_u 0.9824 (0.9406) acc_u 3.1250 (7.6250) lr 5.4601e-04 eta 0:00:11
epoch [132/200] batch [55/74] time 0.395 (0.458) data 0.265 (0.327) loss_u loss_u 0.9746 (0.9428) acc_u 3.1250 (7.2727) lr 5.4601e-04 eta 0:00:08
epoch [132/200] batch [60/74] time 0.427 (0.453) data 0.295 (0.322) loss_u loss_u 0.9614 (0.9438) acc_u 6.2500 (7.1354) lr 5.4601e-04 eta 0:00:06
epoch [132/200] batch [65/74] time 0.482 (0.454) data 0.350 (0.323) loss_u loss_u 0.9297 (0.9432) acc_u 12.5000 (7.3077) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [70/74] time 0.448 (0.453) data 0.318 (0.322) loss_u loss_u 0.8584 (0.9417) acc_u 25.0000 (7.5446) lr 5.4601e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1549
confident_label rate tensor(0.2210, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 693
clean true:683
clean false:10
clean_rate:0.9855699855699855
noisy true:904
noisy false:1539
after delete: len(clean_dataset) 693
after delete: len(noisy_dataset) 2443
epoch [133/200] batch [5/21] time 0.482 (0.482) data 0.352 (0.352) loss_x loss_x 0.5830 (0.9603) acc_x 81.2500 (75.6250) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [10/21] time 0.343 (0.457) data 0.212 (0.327) loss_x loss_x 1.4863 (1.1260) acc_x 59.3750 (71.5625) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [15/21] time 0.368 (0.459) data 0.238 (0.328) loss_x loss_x 1.4482 (1.0898) acc_x 62.5000 (71.8750) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [20/21] time 0.427 (0.456) data 0.297 (0.325) loss_x loss_x 0.9712 (1.1586) acc_x 78.1250 (70.9375) lr 5.3207e-04 eta 0:00:00
epoch [133/200] batch [5/76] time 0.438 (0.450) data 0.306 (0.319) loss_u loss_u 0.8989 (0.9006) acc_u 12.5000 (11.2500) lr 5.3207e-04 eta 0:00:31
epoch [133/200] batch [10/76] time 0.553 (0.456) data 0.421 (0.325) loss_u loss_u 0.9717 (0.9212) acc_u 6.2500 (10.0000) lr 5.3207e-04 eta 0:00:30
epoch [133/200] batch [15/76] time 0.765 (0.465) data 0.633 (0.334) loss_u loss_u 0.9028 (0.9220) acc_u 9.3750 (9.5833) lr 5.3207e-04 eta 0:00:28
epoch [133/200] batch [20/76] time 0.405 (0.473) data 0.274 (0.341) loss_u loss_u 0.9727 (0.9271) acc_u 3.1250 (9.0625) lr 5.3207e-04 eta 0:00:26
epoch [133/200] batch [25/76] time 0.440 (0.473) data 0.307 (0.341) loss_u loss_u 0.9424 (0.9294) acc_u 3.1250 (8.5000) lr 5.3207e-04 eta 0:00:24
epoch [133/200] batch [30/76] time 0.499 (0.472) data 0.367 (0.341) loss_u loss_u 0.9531 (0.9295) acc_u 6.2500 (8.6458) lr 5.3207e-04 eta 0:00:21
epoch [133/200] batch [35/76] time 0.482 (0.473) data 0.350 (0.341) loss_u loss_u 0.9209 (0.9315) acc_u 9.3750 (8.4821) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [40/76] time 0.373 (0.469) data 0.241 (0.338) loss_u loss_u 0.8989 (0.9293) acc_u 12.5000 (8.8281) lr 5.3207e-04 eta 0:00:16
epoch [133/200] batch [45/76] time 0.409 (0.470) data 0.278 (0.339) loss_u loss_u 0.9600 (0.9318) acc_u 3.1250 (8.4028) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [50/76] time 0.404 (0.473) data 0.273 (0.341) loss_u loss_u 0.9331 (0.9338) acc_u 9.3750 (8.1250) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [55/76] time 0.553 (0.473) data 0.423 (0.342) loss_u loss_u 0.9180 (0.9343) acc_u 9.3750 (8.0682) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [60/76] time 0.383 (0.471) data 0.253 (0.339) loss_u loss_u 0.9678 (0.9341) acc_u 3.1250 (8.1250) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [65/76] time 0.414 (0.471) data 0.283 (0.339) loss_u loss_u 0.9409 (0.9347) acc_u 6.2500 (7.9808) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [70/76] time 0.431 (0.468) data 0.300 (0.336) loss_u loss_u 0.8848 (0.9344) acc_u 18.7500 (8.1250) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [75/76] time 0.419 (0.467) data 0.287 (0.336) loss_u loss_u 0.9175 (0.9334) acc_u 9.3750 (8.2083) lr 5.3207e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1556
confident_label rate tensor(0.2261, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 709
clean true:700
clean false:9
clean_rate:0.9873060648801129
noisy true:880
noisy false:1547
after delete: len(clean_dataset) 709
after delete: len(noisy_dataset) 2427
epoch [134/200] batch [5/22] time 0.396 (0.412) data 0.266 (0.282) loss_x loss_x 1.6748 (1.2564) acc_x 59.3750 (72.5000) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [10/22] time 0.360 (0.458) data 0.230 (0.328) loss_x loss_x 1.0908 (1.1343) acc_x 78.1250 (73.1250) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [15/22] time 0.420 (0.446) data 0.290 (0.315) loss_x loss_x 1.4717 (1.1887) acc_x 68.7500 (73.5417) lr 5.1825e-04 eta 0:00:03
epoch [134/200] batch [20/22] time 0.395 (0.440) data 0.264 (0.309) loss_x loss_x 0.9766 (1.1904) acc_x 75.0000 (72.9688) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/75] time 0.419 (0.441) data 0.288 (0.310) loss_u loss_u 0.8931 (0.9281) acc_u 12.5000 (7.5000) lr 5.1825e-04 eta 0:00:30
epoch [134/200] batch [10/75] time 0.361 (0.444) data 0.229 (0.313) loss_u loss_u 0.9800 (0.9255) acc_u 0.0000 (8.1250) lr 5.1825e-04 eta 0:00:28
epoch [134/200] batch [15/75] time 0.399 (0.449) data 0.269 (0.318) loss_u loss_u 0.9565 (0.9302) acc_u 9.3750 (8.3333) lr 5.1825e-04 eta 0:00:26
epoch [134/200] batch [20/75] time 0.483 (0.444) data 0.353 (0.313) loss_u loss_u 0.9023 (0.9309) acc_u 15.6250 (8.5938) lr 5.1825e-04 eta 0:00:24
epoch [134/200] batch [25/75] time 0.391 (0.447) data 0.259 (0.316) loss_u loss_u 0.9658 (0.9300) acc_u 3.1250 (8.8750) lr 5.1825e-04 eta 0:00:22
epoch [134/200] batch [30/75] time 0.553 (0.450) data 0.422 (0.319) loss_u loss_u 0.9370 (0.9310) acc_u 9.3750 (8.7500) lr 5.1825e-04 eta 0:00:20
epoch [134/200] batch [35/75] time 0.480 (0.451) data 0.348 (0.320) loss_u loss_u 0.9346 (0.9309) acc_u 9.3750 (8.7500) lr 5.1825e-04 eta 0:00:18
epoch [134/200] batch [40/75] time 0.388 (0.454) data 0.257 (0.323) loss_u loss_u 0.9355 (0.9305) acc_u 6.2500 (8.7500) lr 5.1825e-04 eta 0:00:15
epoch [134/200] batch [45/75] time 0.536 (0.454) data 0.406 (0.323) loss_u loss_u 0.8745 (0.9305) acc_u 12.5000 (8.6111) lr 5.1825e-04 eta 0:00:13
epoch [134/200] batch [50/75] time 0.428 (0.456) data 0.297 (0.325) loss_u loss_u 0.9482 (0.9284) acc_u 3.1250 (8.8750) lr 5.1825e-04 eta 0:00:11
epoch [134/200] batch [55/75] time 0.519 (0.456) data 0.387 (0.325) loss_u loss_u 0.9512 (0.9301) acc_u 6.2500 (8.6932) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [60/75] time 0.370 (0.453) data 0.239 (0.322) loss_u loss_u 0.9048 (0.9289) acc_u 15.6250 (8.9583) lr 5.1825e-04 eta 0:00:06
epoch [134/200] batch [65/75] time 0.388 (0.454) data 0.257 (0.323) loss_u loss_u 0.9458 (0.9304) acc_u 9.3750 (8.8942) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [70/75] time 0.360 (0.453) data 0.229 (0.321) loss_u loss_u 0.9473 (0.9304) acc_u 6.2500 (8.8839) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [75/75] time 0.533 (0.453) data 0.403 (0.322) loss_u loss_u 0.9307 (0.9308) acc_u 9.3750 (8.7917) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1530
confident_label rate tensor(0.2325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 729
clean true:719
clean false:10
clean_rate:0.9862825788751715
noisy true:887
noisy false:1520
after delete: len(clean_dataset) 729
after delete: len(noisy_dataset) 2407
epoch [135/200] batch [5/22] time 0.457 (0.478) data 0.326 (0.347) loss_x loss_x 1.2754 (1.1022) acc_x 65.6250 (72.5000) lr 5.0454e-04 eta 0:00:08
epoch [135/200] batch [10/22] time 0.684 (0.500) data 0.552 (0.368) loss_x loss_x 1.5000 (1.1306) acc_x 68.7500 (73.4375) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [15/22] time 0.593 (0.498) data 0.463 (0.367) loss_x loss_x 1.1309 (1.1427) acc_x 65.6250 (71.6667) lr 5.0454e-04 eta 0:00:03
epoch [135/200] batch [20/22] time 0.378 (0.494) data 0.248 (0.363) loss_x loss_x 0.9883 (1.1487) acc_x 81.2500 (72.1875) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/75] time 0.516 (0.489) data 0.385 (0.358) loss_u loss_u 0.9492 (0.9361) acc_u 6.2500 (8.1250) lr 5.0454e-04 eta 0:00:34
epoch [135/200] batch [10/75] time 0.381 (0.478) data 0.249 (0.346) loss_u loss_u 0.8794 (0.9279) acc_u 15.6250 (9.6875) lr 5.0454e-04 eta 0:00:31
epoch [135/200] batch [15/75] time 0.386 (0.467) data 0.255 (0.336) loss_u loss_u 0.9731 (0.9340) acc_u 3.1250 (8.9583) lr 5.0454e-04 eta 0:00:28
epoch [135/200] batch [20/75] time 0.435 (0.470) data 0.303 (0.339) loss_u loss_u 0.9863 (0.9272) acc_u 0.0000 (9.6875) lr 5.0454e-04 eta 0:00:25
epoch [135/200] batch [25/75] time 0.517 (0.468) data 0.385 (0.337) loss_u loss_u 0.9214 (0.9254) acc_u 6.2500 (9.8750) lr 5.0454e-04 eta 0:00:23
epoch [135/200] batch [30/75] time 0.467 (0.465) data 0.336 (0.334) loss_u loss_u 0.9531 (0.9327) acc_u 6.2500 (8.9583) lr 5.0454e-04 eta 0:00:20
epoch [135/200] batch [35/75] time 0.358 (0.462) data 0.226 (0.331) loss_u loss_u 0.9741 (0.9334) acc_u 6.2500 (9.1071) lr 5.0454e-04 eta 0:00:18
epoch [135/200] batch [40/75] time 0.404 (0.457) data 0.273 (0.326) loss_u loss_u 0.9771 (0.9352) acc_u 3.1250 (8.8281) lr 5.0454e-04 eta 0:00:16
epoch [135/200] batch [45/75] time 0.430 (0.460) data 0.298 (0.328) loss_u loss_u 0.9629 (0.9379) acc_u 3.1250 (8.4722) lr 5.0454e-04 eta 0:00:13
epoch [135/200] batch [50/75] time 0.470 (0.461) data 0.338 (0.330) loss_u loss_u 0.8877 (0.9364) acc_u 15.6250 (8.6875) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [55/75] time 0.425 (0.464) data 0.295 (0.332) loss_u loss_u 0.9248 (0.9345) acc_u 12.5000 (8.9205) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [60/75] time 0.516 (0.464) data 0.384 (0.332) loss_u loss_u 0.9468 (0.9354) acc_u 6.2500 (8.7500) lr 5.0454e-04 eta 0:00:06
epoch [135/200] batch [65/75] time 0.319 (0.465) data 0.188 (0.334) loss_u loss_u 0.9434 (0.9365) acc_u 12.5000 (8.6538) lr 5.0454e-04 eta 0:00:04
epoch [135/200] batch [70/75] time 0.510 (0.465) data 0.379 (0.333) loss_u loss_u 0.9312 (0.9362) acc_u 9.3750 (8.6607) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [75/75] time 0.395 (0.461) data 0.264 (0.330) loss_u loss_u 0.9355 (0.9367) acc_u 9.3750 (8.5417) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1540
confident_label rate tensor(0.2232, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 700
clean true:691
clean false:9
clean_rate:0.9871428571428571
noisy true:905
noisy false:1531
after delete: len(clean_dataset) 700
after delete: len(noisy_dataset) 2436
epoch [136/200] batch [5/21] time 0.383 (0.429) data 0.252 (0.299) loss_x loss_x 0.9590 (0.9643) acc_x 78.1250 (76.8750) lr 4.9096e-04 eta 0:00:06
epoch [136/200] batch [10/21] time 0.401 (0.437) data 0.270 (0.307) loss_x loss_x 1.0430 (1.0960) acc_x 71.8750 (73.1250) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [15/21] time 0.514 (0.452) data 0.384 (0.321) loss_x loss_x 1.2510 (1.0809) acc_x 75.0000 (73.5417) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [20/21] time 0.315 (0.451) data 0.185 (0.320) loss_x loss_x 0.7280 (1.0856) acc_x 75.0000 (72.9688) lr 4.9096e-04 eta 0:00:00
epoch [136/200] batch [5/76] time 0.472 (0.471) data 0.341 (0.340) loss_u loss_u 0.8960 (0.9330) acc_u 18.7500 (10.0000) lr 4.9096e-04 eta 0:00:33
epoch [136/200] batch [10/76] time 0.489 (0.470) data 0.358 (0.339) loss_u loss_u 0.9385 (0.9345) acc_u 9.3750 (9.3750) lr 4.9096e-04 eta 0:00:31
epoch [136/200] batch [15/76] time 0.430 (0.473) data 0.299 (0.341) loss_u loss_u 0.9409 (0.9381) acc_u 9.3750 (8.9583) lr 4.9096e-04 eta 0:00:28
epoch [136/200] batch [20/76] time 0.425 (0.468) data 0.293 (0.337) loss_u loss_u 0.9351 (0.9348) acc_u 12.5000 (9.3750) lr 4.9096e-04 eta 0:00:26
epoch [136/200] batch [25/76] time 0.337 (0.463) data 0.205 (0.332) loss_u loss_u 0.9419 (0.9256) acc_u 9.3750 (10.1250) lr 4.9096e-04 eta 0:00:23
epoch [136/200] batch [30/76] time 0.527 (0.462) data 0.395 (0.331) loss_u loss_u 0.9937 (0.9285) acc_u 0.0000 (9.4792) lr 4.9096e-04 eta 0:00:21
epoch [136/200] batch [35/76] time 0.406 (0.456) data 0.276 (0.324) loss_u loss_u 0.9502 (0.9319) acc_u 9.3750 (9.1964) lr 4.9096e-04 eta 0:00:18
epoch [136/200] batch [40/76] time 0.503 (0.458) data 0.371 (0.326) loss_u loss_u 0.9243 (0.9329) acc_u 9.3750 (8.9844) lr 4.9096e-04 eta 0:00:16
epoch [136/200] batch [45/76] time 0.378 (0.457) data 0.246 (0.325) loss_u loss_u 0.9219 (0.9326) acc_u 6.2500 (8.8194) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [50/76] time 0.430 (0.453) data 0.298 (0.321) loss_u loss_u 0.9658 (0.9321) acc_u 3.1250 (8.7500) lr 4.9096e-04 eta 0:00:11
epoch [136/200] batch [55/76] time 0.476 (0.451) data 0.345 (0.320) loss_u loss_u 0.8896 (0.9303) acc_u 15.6250 (8.9773) lr 4.9096e-04 eta 0:00:09
epoch [136/200] batch [60/76] time 0.465 (0.452) data 0.334 (0.320) loss_u loss_u 0.9619 (0.9319) acc_u 3.1250 (8.6458) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [65/76] time 0.383 (0.452) data 0.253 (0.320) loss_u loss_u 0.9077 (0.9304) acc_u 9.3750 (8.8462) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [70/76] time 0.434 (0.452) data 0.303 (0.321) loss_u loss_u 0.9448 (0.9300) acc_u 6.2500 (8.8839) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [75/76] time 0.470 (0.453) data 0.339 (0.321) loss_u loss_u 0.9785 (0.9305) acc_u 3.1250 (8.8750) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1575
confident_label rate tensor(0.2168, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 680
clean true:669
clean false:11
clean_rate:0.9838235294117647
noisy true:892
noisy false:1564
after delete: len(clean_dataset) 680
after delete: len(noisy_dataset) 2456
epoch [137/200] batch [5/21] time 0.458 (0.468) data 0.327 (0.338) loss_x loss_x 1.6816 (1.1577) acc_x 53.1250 (73.7500) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [10/21] time 0.418 (0.451) data 0.287 (0.320) loss_x loss_x 1.5225 (1.1438) acc_x 68.7500 (72.5000) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [15/21] time 0.791 (0.475) data 0.660 (0.345) loss_x loss_x 0.8853 (1.1322) acc_x 75.0000 (71.8750) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [20/21] time 0.512 (0.478) data 0.382 (0.347) loss_x loss_x 1.0029 (1.1573) acc_x 71.8750 (71.2500) lr 4.7750e-04 eta 0:00:00
epoch [137/200] batch [5/76] time 0.440 (0.469) data 0.309 (0.339) loss_u loss_u 0.9141 (0.9113) acc_u 6.2500 (9.3750) lr 4.7750e-04 eta 0:00:33
epoch [137/200] batch [10/76] time 0.468 (0.465) data 0.338 (0.334) loss_u loss_u 0.9351 (0.9045) acc_u 12.5000 (10.9375) lr 4.7750e-04 eta 0:00:30
epoch [137/200] batch [15/76] time 0.369 (0.459) data 0.238 (0.328) loss_u loss_u 0.9385 (0.9079) acc_u 9.3750 (11.2500) lr 4.7750e-04 eta 0:00:27
epoch [137/200] batch [20/76] time 0.444 (0.455) data 0.313 (0.324) loss_u loss_u 0.8789 (0.9114) acc_u 18.7500 (11.0938) lr 4.7750e-04 eta 0:00:25
epoch [137/200] batch [25/76] time 0.390 (0.460) data 0.258 (0.329) loss_u loss_u 0.8745 (0.9153) acc_u 15.6250 (10.8750) lr 4.7750e-04 eta 0:00:23
epoch [137/200] batch [30/76] time 0.580 (0.459) data 0.447 (0.328) loss_u loss_u 0.9482 (0.9169) acc_u 3.1250 (10.7292) lr 4.7750e-04 eta 0:00:21
epoch [137/200] batch [35/76] time 0.460 (0.460) data 0.327 (0.329) loss_u loss_u 0.9150 (0.9190) acc_u 12.5000 (10.6250) lr 4.7750e-04 eta 0:00:18
epoch [137/200] batch [40/76] time 0.421 (0.458) data 0.290 (0.327) loss_u loss_u 0.9526 (0.9219) acc_u 6.2500 (10.1562) lr 4.7750e-04 eta 0:00:16
epoch [137/200] batch [45/76] time 0.575 (0.459) data 0.442 (0.328) loss_u loss_u 0.9688 (0.9254) acc_u 6.2500 (9.7222) lr 4.7750e-04 eta 0:00:14
epoch [137/200] batch [50/76] time 0.402 (0.456) data 0.270 (0.325) loss_u loss_u 0.9521 (0.9281) acc_u 6.2500 (9.2500) lr 4.7750e-04 eta 0:00:11
epoch [137/200] batch [55/76] time 0.624 (0.458) data 0.492 (0.327) loss_u loss_u 0.9346 (0.9283) acc_u 9.3750 (9.1477) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [60/76] time 0.390 (0.457) data 0.259 (0.326) loss_u loss_u 0.9766 (0.9267) acc_u 3.1250 (9.3229) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [65/76] time 0.430 (0.455) data 0.300 (0.324) loss_u loss_u 0.9116 (0.9271) acc_u 12.5000 (9.2308) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [70/76] time 0.351 (0.454) data 0.220 (0.322) loss_u loss_u 0.9170 (0.9279) acc_u 9.3750 (9.1964) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [75/76] time 0.567 (0.456) data 0.436 (0.325) loss_u loss_u 0.9585 (0.9284) acc_u 6.2500 (9.2500) lr 4.7750e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1526
confident_label rate tensor(0.2315, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 726
clean true:712
clean false:14
clean_rate:0.9807162534435262
noisy true:898
noisy false:1512
after delete: len(clean_dataset) 726
after delete: len(noisy_dataset) 2410
epoch [138/200] batch [5/22] time 0.393 (0.458) data 0.263 (0.328) loss_x loss_x 1.6699 (1.2861) acc_x 62.5000 (69.3750) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [10/22] time 0.401 (0.439) data 0.271 (0.309) loss_x loss_x 1.2686 (1.2123) acc_x 68.7500 (71.2500) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [15/22] time 0.569 (0.449) data 0.439 (0.319) loss_x loss_x 1.5273 (1.1743) acc_x 56.2500 (71.2500) lr 4.6417e-04 eta 0:00:03
epoch [138/200] batch [20/22] time 0.437 (0.472) data 0.305 (0.342) loss_x loss_x 1.3994 (1.2164) acc_x 71.8750 (69.3750) lr 4.6417e-04 eta 0:00:00
epoch [138/200] batch [5/75] time 0.400 (0.465) data 0.270 (0.334) loss_u loss_u 0.9277 (0.9141) acc_u 9.3750 (11.2500) lr 4.6417e-04 eta 0:00:32
epoch [138/200] batch [10/75] time 0.561 (0.461) data 0.429 (0.331) loss_u loss_u 0.8818 (0.9191) acc_u 12.5000 (9.6875) lr 4.6417e-04 eta 0:00:29
epoch [138/200] batch [15/75] time 0.574 (0.463) data 0.444 (0.333) loss_u loss_u 0.9614 (0.9301) acc_u 3.1250 (8.3333) lr 4.6417e-04 eta 0:00:27
epoch [138/200] batch [20/75] time 0.559 (0.473) data 0.427 (0.343) loss_u loss_u 0.9951 (0.9353) acc_u 3.1250 (7.9688) lr 4.6417e-04 eta 0:00:26
epoch [138/200] batch [25/75] time 0.422 (0.468) data 0.290 (0.337) loss_u loss_u 0.8926 (0.9351) acc_u 12.5000 (7.8750) lr 4.6417e-04 eta 0:00:23
epoch [138/200] batch [30/75] time 0.427 (0.462) data 0.295 (0.331) loss_u loss_u 0.9482 (0.9323) acc_u 6.2500 (8.5417) lr 4.6417e-04 eta 0:00:20
epoch [138/200] batch [35/75] time 0.431 (0.459) data 0.299 (0.328) loss_u loss_u 0.9678 (0.9338) acc_u 3.1250 (8.3036) lr 4.6417e-04 eta 0:00:18
epoch [138/200] batch [40/75] time 0.451 (0.463) data 0.320 (0.332) loss_u loss_u 0.9644 (0.9354) acc_u 3.1250 (7.9688) lr 4.6417e-04 eta 0:00:16
epoch [138/200] batch [45/75] time 0.563 (0.466) data 0.432 (0.334) loss_u loss_u 0.8931 (0.9323) acc_u 12.5000 (8.6111) lr 4.6417e-04 eta 0:00:13
epoch [138/200] batch [50/75] time 0.426 (0.462) data 0.295 (0.331) loss_u loss_u 0.9219 (0.9335) acc_u 9.3750 (8.5000) lr 4.6417e-04 eta 0:00:11
epoch [138/200] batch [55/75] time 0.465 (0.457) data 0.333 (0.326) loss_u loss_u 0.9385 (0.9334) acc_u 9.3750 (8.5795) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [60/75] time 0.428 (0.454) data 0.296 (0.323) loss_u loss_u 0.9307 (0.9333) acc_u 6.2500 (8.5417) lr 4.6417e-04 eta 0:00:06
epoch [138/200] batch [65/75] time 0.729 (0.458) data 0.598 (0.327) loss_u loss_u 0.9653 (0.9337) acc_u 6.2500 (8.6058) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [70/75] time 0.613 (0.461) data 0.481 (0.330) loss_u loss_u 0.9170 (0.9337) acc_u 9.3750 (8.5714) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [75/75] time 0.452 (0.460) data 0.320 (0.329) loss_u loss_u 0.9746 (0.9340) acc_u 6.2500 (8.5417) lr 4.6417e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1561
confident_label rate tensor(0.2242, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 703
clean true:688
clean false:15
clean_rate:0.9786628733997155
noisy true:887
noisy false:1546
after delete: len(clean_dataset) 703
after delete: len(noisy_dataset) 2433
epoch [139/200] batch [5/21] time 0.467 (0.472) data 0.336 (0.341) loss_x loss_x 0.6982 (1.0226) acc_x 78.1250 (78.1250) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [10/21] time 0.699 (0.492) data 0.568 (0.361) loss_x loss_x 1.1865 (1.1526) acc_x 71.8750 (75.0000) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [15/21] time 0.403 (0.486) data 0.272 (0.355) loss_x loss_x 1.1631 (1.1496) acc_x 78.1250 (75.0000) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [20/21] time 0.397 (0.486) data 0.266 (0.355) loss_x loss_x 0.9131 (1.1212) acc_x 87.5000 (75.9375) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/76] time 0.467 (0.481) data 0.335 (0.350) loss_u loss_u 0.9248 (0.9200) acc_u 9.3750 (10.0000) lr 4.5098e-04 eta 0:00:34
epoch [139/200] batch [10/76] time 0.564 (0.480) data 0.433 (0.348) loss_u loss_u 0.9634 (0.9379) acc_u 6.2500 (7.8125) lr 4.5098e-04 eta 0:00:31
epoch [139/200] batch [15/76] time 0.373 (0.474) data 0.241 (0.343) loss_u loss_u 0.9141 (0.9337) acc_u 9.3750 (8.1250) lr 4.5098e-04 eta 0:00:28
epoch [139/200] batch [20/76] time 0.327 (0.465) data 0.195 (0.334) loss_u loss_u 0.9312 (0.9305) acc_u 9.3750 (8.4375) lr 4.5098e-04 eta 0:00:26
epoch [139/200] batch [25/76] time 0.360 (0.465) data 0.228 (0.334) loss_u loss_u 0.9482 (0.9319) acc_u 9.3750 (8.5000) lr 4.5098e-04 eta 0:00:23
epoch [139/200] batch [30/76] time 0.464 (0.465) data 0.334 (0.333) loss_u loss_u 0.9229 (0.9354) acc_u 9.3750 (8.2292) lr 4.5098e-04 eta 0:00:21
epoch [139/200] batch [35/76] time 0.430 (0.459) data 0.296 (0.328) loss_u loss_u 0.9556 (0.9355) acc_u 9.3750 (8.3929) lr 4.5098e-04 eta 0:00:18
epoch [139/200] batch [40/76] time 0.367 (0.464) data 0.235 (0.333) loss_u loss_u 0.9380 (0.9360) acc_u 9.3750 (8.5156) lr 4.5098e-04 eta 0:00:16
epoch [139/200] batch [45/76] time 0.407 (0.462) data 0.275 (0.330) loss_u loss_u 0.8901 (0.9316) acc_u 12.5000 (9.0278) lr 4.5098e-04 eta 0:00:14
epoch [139/200] batch [50/76] time 0.463 (0.464) data 0.331 (0.332) loss_u loss_u 0.9639 (0.9335) acc_u 0.0000 (8.6875) lr 4.5098e-04 eta 0:00:12
epoch [139/200] batch [55/76] time 0.538 (0.464) data 0.406 (0.333) loss_u loss_u 0.8638 (0.9323) acc_u 15.6250 (8.6364) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [60/76] time 0.554 (0.467) data 0.422 (0.336) loss_u loss_u 0.9341 (0.9316) acc_u 6.2500 (8.8021) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [65/76] time 0.626 (0.470) data 0.494 (0.338) loss_u loss_u 0.9272 (0.9325) acc_u 9.3750 (8.7500) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [70/76] time 0.452 (0.467) data 0.319 (0.336) loss_u loss_u 0.9961 (0.9328) acc_u 0.0000 (8.7946) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [75/76] time 0.337 (0.465) data 0.206 (0.334) loss_u loss_u 0.9224 (0.9309) acc_u 9.3750 (8.9583) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1570
confident_label rate tensor(0.2309, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 724
clean true:709
clean false:15
clean_rate:0.9792817679558011
noisy true:857
noisy false:1555
after delete: len(clean_dataset) 724
after delete: len(noisy_dataset) 2412
epoch [140/200] batch [5/22] time 0.544 (0.445) data 0.413 (0.313) loss_x loss_x 0.8809 (1.0932) acc_x 75.0000 (76.8750) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [10/22] time 0.472 (0.477) data 0.341 (0.345) loss_x loss_x 1.4473 (1.2290) acc_x 62.5000 (72.8125) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [15/22] time 0.518 (0.497) data 0.387 (0.365) loss_x loss_x 1.1055 (1.2072) acc_x 78.1250 (73.7500) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [20/22] time 0.438 (0.492) data 0.308 (0.361) loss_x loss_x 0.8022 (1.1809) acc_x 81.2500 (73.1250) lr 4.3792e-04 eta 0:00:00
epoch [140/200] batch [5/75] time 0.411 (0.480) data 0.280 (0.349) loss_u loss_u 0.9814 (0.9523) acc_u 0.0000 (6.2500) lr 4.3792e-04 eta 0:00:33
epoch [140/200] batch [10/75] time 0.385 (0.477) data 0.253 (0.346) loss_u loss_u 0.9556 (0.9405) acc_u 6.2500 (7.5000) lr 4.3792e-04 eta 0:00:31
epoch [140/200] batch [15/75] time 0.400 (0.470) data 0.269 (0.338) loss_u loss_u 0.9517 (0.9327) acc_u 6.2500 (8.1250) lr 4.3792e-04 eta 0:00:28
epoch [140/200] batch [20/75] time 0.466 (0.466) data 0.334 (0.335) loss_u loss_u 0.9253 (0.9323) acc_u 9.3750 (8.2812) lr 4.3792e-04 eta 0:00:25
epoch [140/200] batch [25/75] time 0.517 (0.467) data 0.385 (0.335) loss_u loss_u 0.9756 (0.9390) acc_u 3.1250 (7.5000) lr 4.3792e-04 eta 0:00:23
epoch [140/200] batch [30/75] time 0.611 (0.469) data 0.480 (0.338) loss_u loss_u 0.9927 (0.9370) acc_u 0.0000 (7.8125) lr 4.3792e-04 eta 0:00:21
epoch [140/200] batch [35/75] time 0.425 (0.467) data 0.294 (0.336) loss_u loss_u 0.8989 (0.9374) acc_u 12.5000 (7.7679) lr 4.3792e-04 eta 0:00:18
epoch [140/200] batch [40/75] time 0.502 (0.468) data 0.371 (0.336) loss_u loss_u 0.9385 (0.9372) acc_u 6.2500 (7.8906) lr 4.3792e-04 eta 0:00:16
epoch [140/200] batch [45/75] time 0.450 (0.472) data 0.318 (0.341) loss_u loss_u 0.9697 (0.9372) acc_u 6.2500 (7.9167) lr 4.3792e-04 eta 0:00:14
epoch [140/200] batch [50/75] time 0.438 (0.471) data 0.306 (0.340) loss_u loss_u 0.9170 (0.9354) acc_u 12.5000 (8.3125) lr 4.3792e-04 eta 0:00:11
epoch [140/200] batch [55/75] time 0.460 (0.468) data 0.327 (0.336) loss_u loss_u 0.9360 (0.9363) acc_u 6.2500 (8.1250) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [60/75] time 0.449 (0.468) data 0.317 (0.336) loss_u loss_u 0.9595 (0.9375) acc_u 9.3750 (8.0729) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [65/75] time 0.476 (0.465) data 0.344 (0.334) loss_u loss_u 0.8364 (0.9359) acc_u 18.7500 (8.2692) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [70/75] time 0.364 (0.463) data 0.233 (0.331) loss_u loss_u 0.9463 (0.9365) acc_u 6.2500 (8.2143) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [75/75] time 0.426 (0.464) data 0.294 (0.332) loss_u loss_u 0.9077 (0.9369) acc_u 12.5000 (8.1667) lr 4.3792e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1526
confident_label rate tensor(0.2325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 729
clean true:717
clean false:12
clean_rate:0.9835390946502057
noisy true:893
noisy false:1514
after delete: len(clean_dataset) 729
after delete: len(noisy_dataset) 2407
epoch [141/200] batch [5/22] time 0.497 (0.463) data 0.366 (0.332) loss_x loss_x 1.3750 (1.1166) acc_x 62.5000 (70.6250) lr 4.2499e-04 eta 0:00:07
epoch [141/200] batch [10/22] time 0.490 (0.473) data 0.359 (0.342) loss_x loss_x 0.7764 (1.0336) acc_x 84.3750 (75.6250) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [15/22] time 0.604 (0.479) data 0.472 (0.349) loss_x loss_x 1.8223 (1.1108) acc_x 56.2500 (73.5417) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [20/22] time 0.344 (0.471) data 0.213 (0.340) loss_x loss_x 1.2725 (1.2088) acc_x 62.5000 (71.5625) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/75] time 0.404 (0.469) data 0.273 (0.338) loss_u loss_u 0.8813 (0.9321) acc_u 15.6250 (9.3750) lr 4.2499e-04 eta 0:00:32
epoch [141/200] batch [10/75] time 0.413 (0.466) data 0.282 (0.335) loss_u loss_u 0.9819 (0.9413) acc_u 3.1250 (7.5000) lr 4.2499e-04 eta 0:00:30
epoch [141/200] batch [15/75] time 0.476 (0.460) data 0.344 (0.329) loss_u loss_u 0.9678 (0.9403) acc_u 6.2500 (7.5000) lr 4.2499e-04 eta 0:00:27
epoch [141/200] batch [20/75] time 0.518 (0.455) data 0.386 (0.324) loss_u loss_u 0.9785 (0.9399) acc_u 0.0000 (7.5000) lr 4.2499e-04 eta 0:00:25
epoch [141/200] batch [25/75] time 0.384 (0.456) data 0.252 (0.325) loss_u loss_u 0.8984 (0.9336) acc_u 12.5000 (8.2500) lr 4.2499e-04 eta 0:00:22
epoch [141/200] batch [30/75] time 0.454 (0.456) data 0.322 (0.325) loss_u loss_u 0.8921 (0.9389) acc_u 12.5000 (7.6042) lr 4.2499e-04 eta 0:00:20
epoch [141/200] batch [35/75] time 0.414 (0.455) data 0.283 (0.324) loss_u loss_u 0.9497 (0.9388) acc_u 6.2500 (7.5000) lr 4.2499e-04 eta 0:00:18
epoch [141/200] batch [40/75] time 0.432 (0.453) data 0.302 (0.322) loss_u loss_u 0.8701 (0.9348) acc_u 15.6250 (8.1250) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [45/75] time 0.515 (0.457) data 0.384 (0.326) loss_u loss_u 0.8931 (0.9362) acc_u 12.5000 (8.0556) lr 4.2499e-04 eta 0:00:13
epoch [141/200] batch [50/75] time 0.516 (0.461) data 0.385 (0.330) loss_u loss_u 0.9741 (0.9391) acc_u 3.1250 (7.6875) lr 4.2499e-04 eta 0:00:11
epoch [141/200] batch [55/75] time 0.458 (0.461) data 0.328 (0.330) loss_u loss_u 0.9697 (0.9412) acc_u 6.2500 (7.5000) lr 4.2499e-04 eta 0:00:09
epoch [141/200] batch [60/75] time 0.465 (0.461) data 0.334 (0.330) loss_u loss_u 0.9736 (0.9410) acc_u 3.1250 (7.7083) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [65/75] time 0.376 (0.461) data 0.245 (0.330) loss_u loss_u 0.9624 (0.9407) acc_u 6.2500 (7.6923) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [70/75] time 0.427 (0.459) data 0.296 (0.328) loss_u loss_u 0.9106 (0.9402) acc_u 9.3750 (7.6339) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [75/75] time 0.333 (0.459) data 0.201 (0.328) loss_u loss_u 0.9600 (0.9397) acc_u 3.1250 (7.6667) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1543
confident_label rate tensor(0.2366, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 742
clean true:732
clean false:10
clean_rate:0.9865229110512129
noisy true:861
noisy false:1533
after delete: len(clean_dataset) 742
after delete: len(noisy_dataset) 2394
epoch [142/200] batch [5/23] time 0.586 (0.479) data 0.455 (0.348) loss_x loss_x 1.1025 (0.8740) acc_x 68.7500 (76.8750) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [10/23] time 0.426 (0.447) data 0.295 (0.316) loss_x loss_x 0.9590 (0.9864) acc_x 75.0000 (75.0000) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [15/23] time 0.509 (0.455) data 0.379 (0.324) loss_x loss_x 1.2100 (1.0519) acc_x 62.5000 (74.3750) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [20/23] time 0.684 (0.470) data 0.552 (0.340) loss_x loss_x 1.5479 (1.0722) acc_x 65.6250 (74.0625) lr 4.1221e-04 eta 0:00:01
epoch [142/200] batch [5/74] time 0.430 (0.466) data 0.299 (0.335) loss_u loss_u 0.9692 (0.9558) acc_u 3.1250 (6.2500) lr 4.1221e-04 eta 0:00:32
epoch [142/200] batch [10/74] time 0.704 (0.474) data 0.572 (0.342) loss_u loss_u 0.8999 (0.9507) acc_u 12.5000 (6.8750) lr 4.1221e-04 eta 0:00:30
epoch [142/200] batch [15/74] time 0.438 (0.470) data 0.306 (0.338) loss_u loss_u 0.9463 (0.9480) acc_u 9.3750 (7.5000) lr 4.1221e-04 eta 0:00:27
epoch [142/200] batch [20/74] time 0.456 (0.469) data 0.321 (0.338) loss_u loss_u 0.9561 (0.9466) acc_u 6.2500 (7.1875) lr 4.1221e-04 eta 0:00:25
epoch [142/200] batch [25/74] time 0.457 (0.473) data 0.324 (0.341) loss_u loss_u 0.9204 (0.9466) acc_u 9.3750 (7.1250) lr 4.1221e-04 eta 0:00:23
epoch [142/200] batch [30/74] time 0.525 (0.471) data 0.391 (0.339) loss_u loss_u 0.9941 (0.9485) acc_u 0.0000 (6.7708) lr 4.1221e-04 eta 0:00:20
epoch [142/200] batch [35/74] time 0.514 (0.473) data 0.382 (0.341) loss_u loss_u 0.8560 (0.9449) acc_u 18.7500 (7.4107) lr 4.1221e-04 eta 0:00:18
epoch [142/200] batch [40/74] time 0.454 (0.472) data 0.322 (0.340) loss_u loss_u 0.9536 (0.9451) acc_u 6.2500 (7.3438) lr 4.1221e-04 eta 0:00:16
epoch [142/200] batch [45/74] time 0.432 (0.470) data 0.300 (0.338) loss_u loss_u 0.9194 (0.9450) acc_u 9.3750 (7.3611) lr 4.1221e-04 eta 0:00:13
epoch [142/200] batch [50/74] time 0.571 (0.475) data 0.440 (0.343) loss_u loss_u 0.9858 (0.9448) acc_u 3.1250 (7.3125) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [55/74] time 0.559 (0.473) data 0.427 (0.341) loss_u loss_u 0.9663 (0.9447) acc_u 6.2500 (7.4432) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [60/74] time 0.407 (0.471) data 0.275 (0.338) loss_u loss_u 0.9106 (0.9411) acc_u 9.3750 (7.7604) lr 4.1221e-04 eta 0:00:06
epoch [142/200] batch [65/74] time 0.352 (0.468) data 0.220 (0.336) loss_u loss_u 0.9497 (0.9402) acc_u 6.2500 (7.8365) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [70/74] time 0.383 (0.466) data 0.253 (0.334) loss_u loss_u 0.8516 (0.9386) acc_u 18.7500 (7.9911) lr 4.1221e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1526
confident_label rate tensor(0.2337, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 733
clean true:721
clean false:12
clean_rate:0.9836289222373806
noisy true:889
noisy false:1514
after delete: len(clean_dataset) 733
after delete: len(noisy_dataset) 2403
epoch [143/200] batch [5/22] time 0.417 (0.439) data 0.285 (0.308) loss_x loss_x 1.1074 (1.1818) acc_x 65.6250 (70.0000) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [10/22] time 0.368 (0.438) data 0.237 (0.307) loss_x loss_x 0.8555 (1.0091) acc_x 75.0000 (75.0000) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [15/22] time 0.404 (0.444) data 0.273 (0.313) loss_x loss_x 1.1035 (0.9937) acc_x 68.7500 (76.2500) lr 3.9958e-04 eta 0:00:03
epoch [143/200] batch [20/22] time 0.393 (0.436) data 0.262 (0.305) loss_x loss_x 0.6694 (1.0406) acc_x 87.5000 (75.7812) lr 3.9958e-04 eta 0:00:00
epoch [143/200] batch [5/75] time 0.516 (0.469) data 0.383 (0.338) loss_u loss_u 0.8931 (0.9258) acc_u 9.3750 (9.3750) lr 3.9958e-04 eta 0:00:32
epoch [143/200] batch [10/75] time 0.358 (0.459) data 0.227 (0.327) loss_u loss_u 0.9004 (0.9332) acc_u 12.5000 (8.7500) lr 3.9958e-04 eta 0:00:29
epoch [143/200] batch [15/75] time 0.404 (0.467) data 0.273 (0.336) loss_u loss_u 0.9604 (0.9358) acc_u 6.2500 (7.9167) lr 3.9958e-04 eta 0:00:28
epoch [143/200] batch [20/75] time 0.530 (0.469) data 0.398 (0.337) loss_u loss_u 0.9526 (0.9380) acc_u 9.3750 (8.5938) lr 3.9958e-04 eta 0:00:25
epoch [143/200] batch [25/75] time 0.389 (0.461) data 0.256 (0.329) loss_u loss_u 0.9727 (0.9381) acc_u 3.1250 (8.1250) lr 3.9958e-04 eta 0:00:23
epoch [143/200] batch [30/75] time 0.501 (0.458) data 0.368 (0.327) loss_u loss_u 0.9536 (0.9329) acc_u 3.1250 (8.8542) lr 3.9958e-04 eta 0:00:20
epoch [143/200] batch [35/75] time 0.603 (0.459) data 0.471 (0.327) loss_u loss_u 0.9707 (0.9348) acc_u 3.1250 (8.4821) lr 3.9958e-04 eta 0:00:18
epoch [143/200] batch [40/75] time 0.369 (0.458) data 0.237 (0.326) loss_u loss_u 0.9434 (0.9350) acc_u 6.2500 (8.5938) lr 3.9958e-04 eta 0:00:16
epoch [143/200] batch [45/75] time 0.415 (0.458) data 0.284 (0.327) loss_u loss_u 0.9146 (0.9358) acc_u 9.3750 (8.4722) lr 3.9958e-04 eta 0:00:13
epoch [143/200] batch [50/75] time 0.362 (0.459) data 0.231 (0.328) loss_u loss_u 0.9492 (0.9346) acc_u 6.2500 (8.5625) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [55/75] time 0.424 (0.456) data 0.293 (0.325) loss_u loss_u 0.9370 (0.9360) acc_u 6.2500 (8.3523) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [60/75] time 0.489 (0.457) data 0.357 (0.326) loss_u loss_u 0.9238 (0.9358) acc_u 9.3750 (8.3333) lr 3.9958e-04 eta 0:00:06
epoch [143/200] batch [65/75] time 0.449 (0.456) data 0.317 (0.324) loss_u loss_u 0.9912 (0.9364) acc_u 0.0000 (8.3173) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [70/75] time 0.413 (0.455) data 0.282 (0.324) loss_u loss_u 0.9473 (0.9388) acc_u 6.2500 (8.0357) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [75/75] time 0.385 (0.455) data 0.252 (0.323) loss_u loss_u 0.9663 (0.9388) acc_u 3.1250 (8.0417) lr 3.9958e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1552
confident_label rate tensor(0.2328, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 730
clean true:714
clean false:16
clean_rate:0.9780821917808219
noisy true:870
noisy false:1536
after delete: len(clean_dataset) 730
after delete: len(noisy_dataset) 2406
epoch [144/200] batch [5/22] time 0.486 (0.482) data 0.356 (0.352) loss_x loss_x 1.1436 (1.0799) acc_x 71.8750 (76.8750) lr 3.8709e-04 eta 0:00:08
epoch [144/200] batch [10/22] time 0.534 (0.465) data 0.403 (0.335) loss_x loss_x 1.0303 (1.0482) acc_x 78.1250 (76.2500) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [15/22] time 0.400 (0.466) data 0.269 (0.335) loss_x loss_x 1.3818 (1.0544) acc_x 65.6250 (75.4167) lr 3.8709e-04 eta 0:00:03
epoch [144/200] batch [20/22] time 0.562 (0.464) data 0.431 (0.333) loss_x loss_x 1.2148 (1.1109) acc_x 68.7500 (74.6875) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/75] time 0.594 (0.465) data 0.463 (0.334) loss_u loss_u 0.9917 (0.9421) acc_u 0.0000 (6.2500) lr 3.8709e-04 eta 0:00:32
epoch [144/200] batch [10/75] time 0.344 (0.459) data 0.213 (0.328) loss_u loss_u 0.9360 (0.9305) acc_u 6.2500 (7.8125) lr 3.8709e-04 eta 0:00:29
epoch [144/200] batch [15/75] time 0.446 (0.458) data 0.314 (0.327) loss_u loss_u 0.9810 (0.9399) acc_u 3.1250 (7.0833) lr 3.8709e-04 eta 0:00:27
epoch [144/200] batch [20/75] time 0.417 (0.453) data 0.285 (0.321) loss_u loss_u 0.9307 (0.9449) acc_u 9.3750 (6.7188) lr 3.8709e-04 eta 0:00:24
epoch [144/200] batch [25/75] time 0.393 (0.450) data 0.260 (0.319) loss_u loss_u 0.9287 (0.9399) acc_u 9.3750 (7.7500) lr 3.8709e-04 eta 0:00:22
epoch [144/200] batch [30/75] time 0.516 (0.454) data 0.383 (0.322) loss_u loss_u 0.9673 (0.9404) acc_u 6.2500 (7.9167) lr 3.8709e-04 eta 0:00:20
epoch [144/200] batch [35/75] time 0.362 (0.455) data 0.231 (0.323) loss_u loss_u 0.9634 (0.9410) acc_u 3.1250 (7.6786) lr 3.8709e-04 eta 0:00:18
epoch [144/200] batch [40/75] time 0.525 (0.453) data 0.392 (0.322) loss_u loss_u 0.9219 (0.9374) acc_u 9.3750 (8.1250) lr 3.8709e-04 eta 0:00:15
epoch [144/200] batch [45/75] time 0.408 (0.454) data 0.277 (0.322) loss_u loss_u 0.9355 (0.9368) acc_u 9.3750 (8.1944) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [50/75] time 0.496 (0.454) data 0.364 (0.322) loss_u loss_u 0.9697 (0.9375) acc_u 3.1250 (8.0625) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [55/75] time 0.537 (0.454) data 0.406 (0.323) loss_u loss_u 0.9434 (0.9376) acc_u 12.5000 (8.1250) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [60/75] time 0.514 (0.459) data 0.382 (0.327) loss_u loss_u 0.9751 (0.9363) acc_u 3.1250 (8.2292) lr 3.8709e-04 eta 0:00:06
epoch [144/200] batch [65/75] time 0.509 (0.459) data 0.377 (0.328) loss_u loss_u 0.9780 (0.9373) acc_u 0.0000 (8.0769) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [70/75] time 0.392 (0.459) data 0.261 (0.327) loss_u loss_u 0.9102 (0.9357) acc_u 9.3750 (8.2143) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [75/75] time 0.437 (0.461) data 0.305 (0.329) loss_u loss_u 0.8589 (0.9357) acc_u 18.7500 (8.2500) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1587
confident_label rate tensor(0.2293, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 719
clean true:704
clean false:15
clean_rate:0.9791376912378303
noisy true:845
noisy false:1572
after delete: len(clean_dataset) 719
after delete: len(noisy_dataset) 2417
epoch [145/200] batch [5/22] time 0.519 (0.437) data 0.387 (0.306) loss_x loss_x 1.0332 (0.9869) acc_x 78.1250 (74.3750) lr 3.7476e-04 eta 0:00:07
epoch [145/200] batch [10/22] time 0.372 (0.453) data 0.241 (0.322) loss_x loss_x 0.7900 (0.9577) acc_x 81.2500 (75.9375) lr 3.7476e-04 eta 0:00:05
epoch [145/200] batch [15/22] time 0.533 (0.465) data 0.403 (0.334) loss_x loss_x 1.6377 (1.0404) acc_x 65.6250 (74.3750) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [20/22] time 0.522 (0.473) data 0.391 (0.342) loss_x loss_x 1.2324 (1.0463) acc_x 75.0000 (75.0000) lr 3.7476e-04 eta 0:00:00
epoch [145/200] batch [5/75] time 0.410 (0.466) data 0.278 (0.335) loss_u loss_u 0.9399 (0.9244) acc_u 6.2500 (8.7500) lr 3.7476e-04 eta 0:00:32
epoch [145/200] batch [10/75] time 0.559 (0.467) data 0.428 (0.336) loss_u loss_u 0.8350 (0.9261) acc_u 21.8750 (9.0625) lr 3.7476e-04 eta 0:00:30
epoch [145/200] batch [15/75] time 0.403 (0.463) data 0.271 (0.332) loss_u loss_u 0.9712 (0.9231) acc_u 3.1250 (9.3750) lr 3.7476e-04 eta 0:00:27
epoch [145/200] batch [20/75] time 0.485 (0.467) data 0.353 (0.336) loss_u loss_u 0.8755 (0.9112) acc_u 12.5000 (10.4688) lr 3.7476e-04 eta 0:00:25
epoch [145/200] batch [25/75] time 0.427 (0.467) data 0.295 (0.336) loss_u loss_u 0.8994 (0.9149) acc_u 12.5000 (10.1250) lr 3.7476e-04 eta 0:00:23
epoch [145/200] batch [30/75] time 0.463 (0.467) data 0.332 (0.336) loss_u loss_u 0.9277 (0.9192) acc_u 12.5000 (9.7917) lr 3.7476e-04 eta 0:00:21
epoch [145/200] batch [35/75] time 0.545 (0.465) data 0.414 (0.334) loss_u loss_u 0.9858 (0.9246) acc_u 0.0000 (9.0179) lr 3.7476e-04 eta 0:00:18
epoch [145/200] batch [40/75] time 0.416 (0.466) data 0.286 (0.335) loss_u loss_u 0.9312 (0.9244) acc_u 9.3750 (9.0625) lr 3.7476e-04 eta 0:00:16
epoch [145/200] batch [45/75] time 0.633 (0.467) data 0.500 (0.336) loss_u loss_u 0.8911 (0.9275) acc_u 15.6250 (8.6806) lr 3.7476e-04 eta 0:00:14
epoch [145/200] batch [50/75] time 0.419 (0.470) data 0.288 (0.339) loss_u loss_u 0.9565 (0.9274) acc_u 6.2500 (8.8125) lr 3.7476e-04 eta 0:00:11
epoch [145/200] batch [55/75] time 0.437 (0.467) data 0.305 (0.336) loss_u loss_u 0.9614 (0.9266) acc_u 3.1250 (8.9205) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [60/75] time 0.421 (0.466) data 0.290 (0.335) loss_u loss_u 0.9800 (0.9289) acc_u 3.1250 (8.6979) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [65/75] time 0.418 (0.468) data 0.287 (0.337) loss_u loss_u 0.9531 (0.9305) acc_u 6.2500 (8.5096) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [70/75] time 0.433 (0.465) data 0.302 (0.334) loss_u loss_u 0.9097 (0.9302) acc_u 9.3750 (8.4375) lr 3.7476e-04 eta 0:00:02
epoch [145/200] batch [75/75] time 0.416 (0.464) data 0.285 (0.333) loss_u loss_u 0.8584 (0.9296) acc_u 18.7500 (8.6250) lr 3.7476e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1502
confident_label rate tensor(0.2347, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 736
clean true:726
clean false:10
clean_rate:0.9864130434782609
noisy true:908
noisy false:1492
after delete: len(clean_dataset) 736
after delete: len(noisy_dataset) 2400
epoch [146/200] batch [5/23] time 0.742 (0.524) data 0.611 (0.393) loss_x loss_x 1.1953 (0.9981) acc_x 71.8750 (73.7500) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [10/23] time 0.427 (0.504) data 0.296 (0.373) loss_x loss_x 0.9473 (1.0140) acc_x 81.2500 (74.3750) lr 3.6258e-04 eta 0:00:06
epoch [146/200] batch [15/23] time 0.382 (0.499) data 0.251 (0.368) loss_x loss_x 1.7900 (1.0879) acc_x 59.3750 (74.1667) lr 3.6258e-04 eta 0:00:03
epoch [146/200] batch [20/23] time 0.417 (0.489) data 0.286 (0.358) loss_x loss_x 1.2939 (1.1368) acc_x 68.7500 (73.5938) lr 3.6258e-04 eta 0:00:01
epoch [146/200] batch [5/75] time 0.423 (0.480) data 0.290 (0.349) loss_u loss_u 0.9521 (0.9228) acc_u 6.2500 (9.3750) lr 3.6258e-04 eta 0:00:33
epoch [146/200] batch [10/75] time 0.641 (0.487) data 0.504 (0.356) loss_u loss_u 0.9624 (0.9260) acc_u 6.2500 (9.3750) lr 3.6258e-04 eta 0:00:31
epoch [146/200] batch [15/75] time 0.399 (0.485) data 0.267 (0.353) loss_u loss_u 0.9331 (0.9235) acc_u 6.2500 (9.3750) lr 3.6258e-04 eta 0:00:29
epoch [146/200] batch [20/75] time 0.495 (0.482) data 0.363 (0.351) loss_u loss_u 0.9683 (0.9240) acc_u 3.1250 (9.2188) lr 3.6258e-04 eta 0:00:26
epoch [146/200] batch [25/75] time 0.354 (0.477) data 0.223 (0.346) loss_u loss_u 0.9492 (0.9274) acc_u 12.5000 (9.3750) lr 3.6258e-04 eta 0:00:23
epoch [146/200] batch [30/75] time 0.397 (0.473) data 0.265 (0.341) loss_u loss_u 0.9561 (0.9290) acc_u 3.1250 (8.9583) lr 3.6258e-04 eta 0:00:21
epoch [146/200] batch [35/75] time 0.542 (0.472) data 0.410 (0.340) loss_u loss_u 0.9478 (0.9311) acc_u 6.2500 (8.8393) lr 3.6258e-04 eta 0:00:18
epoch [146/200] batch [40/75] time 0.366 (0.470) data 0.234 (0.339) loss_u loss_u 0.8945 (0.9319) acc_u 9.3750 (8.6719) lr 3.6258e-04 eta 0:00:16
epoch [146/200] batch [45/75] time 0.496 (0.468) data 0.364 (0.336) loss_u loss_u 0.9165 (0.9323) acc_u 15.6250 (8.8889) lr 3.6258e-04 eta 0:00:14
epoch [146/200] batch [50/75] time 0.544 (0.468) data 0.413 (0.336) loss_u loss_u 0.9897 (0.9340) acc_u 0.0000 (8.5625) lr 3.6258e-04 eta 0:00:11
epoch [146/200] batch [55/75] time 0.428 (0.464) data 0.296 (0.333) loss_u loss_u 0.9346 (0.9334) acc_u 6.2500 (8.5795) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [60/75] time 0.480 (0.461) data 0.349 (0.330) loss_u loss_u 0.9404 (0.9344) acc_u 12.5000 (8.4896) lr 3.6258e-04 eta 0:00:06
epoch [146/200] batch [65/75] time 0.536 (0.465) data 0.403 (0.333) loss_u loss_u 0.9526 (0.9350) acc_u 3.1250 (8.3173) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [70/75] time 0.424 (0.467) data 0.293 (0.335) loss_u loss_u 0.9736 (0.9355) acc_u 3.1250 (8.1696) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [75/75] time 0.459 (0.468) data 0.328 (0.336) loss_u loss_u 0.9951 (0.9360) acc_u 0.0000 (8.0417) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.2363, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 741
clean true:727
clean false:14
clean_rate:0.9811066126855601
noisy true:892
noisy false:1503
after delete: len(clean_dataset) 741
after delete: len(noisy_dataset) 2395
epoch [147/200] batch [5/23] time 0.418 (0.455) data 0.287 (0.324) loss_x loss_x 0.9688 (1.0988) acc_x 71.8750 (74.3750) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [10/23] time 0.463 (0.464) data 0.333 (0.333) loss_x loss_x 1.0889 (1.0069) acc_x 71.8750 (75.3125) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [15/23] time 0.457 (0.446) data 0.326 (0.315) loss_x loss_x 1.1133 (1.0386) acc_x 75.0000 (74.5833) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [20/23] time 0.628 (0.476) data 0.497 (0.345) loss_x loss_x 0.8008 (1.0010) acc_x 81.2500 (75.3125) lr 3.5055e-04 eta 0:00:01
epoch [147/200] batch [5/74] time 0.411 (0.463) data 0.279 (0.332) loss_u loss_u 0.9683 (0.9572) acc_u 6.2500 (5.6250) lr 3.5055e-04 eta 0:00:31
epoch [147/200] batch [10/74] time 0.521 (0.462) data 0.389 (0.331) loss_u loss_u 0.9287 (0.9294) acc_u 9.3750 (8.4375) lr 3.5055e-04 eta 0:00:29
epoch [147/200] batch [15/74] time 0.453 (0.461) data 0.321 (0.330) loss_u loss_u 0.9888 (0.9244) acc_u 0.0000 (8.9583) lr 3.5055e-04 eta 0:00:27
epoch [147/200] batch [20/74] time 0.495 (0.472) data 0.364 (0.341) loss_u loss_u 0.8555 (0.9237) acc_u 15.6250 (9.0625) lr 3.5055e-04 eta 0:00:25
epoch [147/200] batch [25/74] time 0.459 (0.475) data 0.327 (0.344) loss_u loss_u 0.9238 (0.9247) acc_u 12.5000 (9.1250) lr 3.5055e-04 eta 0:00:23
epoch [147/200] batch [30/74] time 0.417 (0.471) data 0.286 (0.339) loss_u loss_u 0.9399 (0.9256) acc_u 6.2500 (8.8542) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [35/74] time 0.439 (0.465) data 0.308 (0.333) loss_u loss_u 0.9736 (0.9290) acc_u 3.1250 (8.6607) lr 3.5055e-04 eta 0:00:18
epoch [147/200] batch [40/74] time 0.519 (0.475) data 0.388 (0.343) loss_u loss_u 0.9238 (0.9326) acc_u 9.3750 (8.3594) lr 3.5055e-04 eta 0:00:16
epoch [147/200] batch [45/74] time 0.343 (0.468) data 0.212 (0.336) loss_u loss_u 0.9722 (0.9333) acc_u 3.1250 (8.1944) lr 3.5055e-04 eta 0:00:13
epoch [147/200] batch [50/74] time 0.501 (0.469) data 0.369 (0.338) loss_u loss_u 0.9756 (0.9359) acc_u 6.2500 (7.9375) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [55/74] time 0.359 (0.468) data 0.228 (0.336) loss_u loss_u 0.9414 (0.9375) acc_u 6.2500 (7.7841) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [60/74] time 0.451 (0.465) data 0.320 (0.334) loss_u loss_u 0.8511 (0.9361) acc_u 18.7500 (7.9688) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [65/74] time 0.388 (0.463) data 0.257 (0.331) loss_u loss_u 0.9893 (0.9371) acc_u 0.0000 (7.9327) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [70/74] time 0.407 (0.463) data 0.276 (0.332) loss_u loss_u 0.9175 (0.9373) acc_u 12.5000 (7.9464) lr 3.5055e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1536
confident_label rate tensor(0.2251, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 706
clean true:694
clean false:12
clean_rate:0.9830028328611898
noisy true:906
noisy false:1524
after delete: len(clean_dataset) 706
after delete: len(noisy_dataset) 2430
epoch [148/200] batch [5/22] time 0.444 (0.538) data 0.314 (0.406) loss_x loss_x 0.9111 (0.8832) acc_x 78.1250 (79.3750) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [10/22] time 0.456 (0.521) data 0.325 (0.389) loss_x loss_x 0.9717 (1.0209) acc_x 78.1250 (75.6250) lr 3.3869e-04 eta 0:00:06
epoch [148/200] batch [15/22] time 0.458 (0.497) data 0.326 (0.365) loss_x loss_x 1.4209 (1.0986) acc_x 71.8750 (73.9583) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [20/22] time 0.488 (0.493) data 0.357 (0.362) loss_x loss_x 0.9790 (1.0687) acc_x 68.7500 (74.0625) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/75] time 0.430 (0.484) data 0.299 (0.353) loss_u loss_u 0.9712 (0.9314) acc_u 3.1250 (9.3750) lr 3.3869e-04 eta 0:00:33
epoch [148/200] batch [10/75] time 0.461 (0.483) data 0.329 (0.352) loss_u loss_u 0.9438 (0.9298) acc_u 9.3750 (10.3125) lr 3.3869e-04 eta 0:00:31
epoch [148/200] batch [15/75] time 0.489 (0.482) data 0.357 (0.350) loss_u loss_u 0.9346 (0.9322) acc_u 6.2500 (9.1667) lr 3.3869e-04 eta 0:00:28
epoch [148/200] batch [20/75] time 0.500 (0.484) data 0.365 (0.352) loss_u loss_u 0.9346 (0.9373) acc_u 9.3750 (8.7500) lr 3.3869e-04 eta 0:00:26
epoch [148/200] batch [25/75] time 0.622 (0.481) data 0.489 (0.349) loss_u loss_u 0.9575 (0.9382) acc_u 3.1250 (8.5000) lr 3.3869e-04 eta 0:00:24
epoch [148/200] batch [30/75] time 0.329 (0.474) data 0.197 (0.342) loss_u loss_u 0.9404 (0.9377) acc_u 6.2500 (8.3333) lr 3.3869e-04 eta 0:00:21
epoch [148/200] batch [35/75] time 0.447 (0.472) data 0.317 (0.341) loss_u loss_u 0.9043 (0.9364) acc_u 12.5000 (8.3036) lr 3.3869e-04 eta 0:00:18
epoch [148/200] batch [40/75] time 0.369 (0.469) data 0.238 (0.337) loss_u loss_u 0.9126 (0.9386) acc_u 9.3750 (8.0469) lr 3.3869e-04 eta 0:00:16
epoch [148/200] batch [45/75] time 0.599 (0.468) data 0.468 (0.337) loss_u loss_u 0.9316 (0.9391) acc_u 9.3750 (8.1250) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [50/75] time 0.621 (0.470) data 0.490 (0.338) loss_u loss_u 0.8701 (0.9387) acc_u 15.6250 (8.1250) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [55/75] time 0.506 (0.470) data 0.372 (0.338) loss_u loss_u 0.8853 (0.9371) acc_u 18.7500 (8.4091) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [60/75] time 0.596 (0.470) data 0.465 (0.339) loss_u loss_u 0.9082 (0.9363) acc_u 12.5000 (8.5417) lr 3.3869e-04 eta 0:00:07
epoch [148/200] batch [65/75] time 0.371 (0.467) data 0.240 (0.335) loss_u loss_u 0.9268 (0.9375) acc_u 9.3750 (8.4135) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [70/75] time 0.424 (0.469) data 0.293 (0.337) loss_u loss_u 0.9443 (0.9368) acc_u 6.2500 (8.4821) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [75/75] time 0.456 (0.473) data 0.325 (0.342) loss_u loss_u 0.8584 (0.9355) acc_u 18.7500 (8.6667) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1566
confident_label rate tensor(0.2270, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 712
clean true:699
clean false:13
clean_rate:0.9817415730337079
noisy true:871
noisy false:1553
after delete: len(clean_dataset) 712
after delete: len(noisy_dataset) 2424
epoch [149/200] batch [5/22] time 0.384 (0.424) data 0.253 (0.294) loss_x loss_x 0.9648 (1.1331) acc_x 71.8750 (71.8750) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [10/22] time 0.384 (0.451) data 0.253 (0.321) loss_x loss_x 0.8428 (1.1028) acc_x 75.0000 (72.5000) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [15/22] time 0.504 (0.466) data 0.374 (0.335) loss_x loss_x 1.0430 (1.0517) acc_x 78.1250 (74.1667) lr 3.2699e-04 eta 0:00:03
epoch [149/200] batch [20/22] time 0.417 (0.473) data 0.286 (0.343) loss_x loss_x 1.3359 (1.0957) acc_x 68.7500 (72.8125) lr 3.2699e-04 eta 0:00:00
epoch [149/200] batch [5/75] time 0.494 (0.476) data 0.363 (0.346) loss_u loss_u 0.9194 (0.9098) acc_u 6.2500 (10.0000) lr 3.2699e-04 eta 0:00:33
epoch [149/200] batch [10/75] time 0.470 (0.472) data 0.339 (0.341) loss_u loss_u 0.9487 (0.9248) acc_u 6.2500 (8.1250) lr 3.2699e-04 eta 0:00:30
epoch [149/200] batch [15/75] time 0.396 (0.469) data 0.265 (0.338) loss_u loss_u 0.9829 (0.9297) acc_u 0.0000 (8.1250) lr 3.2699e-04 eta 0:00:28
epoch [149/200] batch [20/75] time 0.440 (0.474) data 0.309 (0.343) loss_u loss_u 0.9478 (0.9373) acc_u 6.2500 (7.3438) lr 3.2699e-04 eta 0:00:26
epoch [149/200] batch [25/75] time 0.506 (0.478) data 0.374 (0.347) loss_u loss_u 0.9355 (0.9354) acc_u 9.3750 (7.8750) lr 3.2699e-04 eta 0:00:23
epoch [149/200] batch [30/75] time 0.414 (0.477) data 0.283 (0.346) loss_u loss_u 0.9736 (0.9353) acc_u 6.2500 (7.7083) lr 3.2699e-04 eta 0:00:21
epoch [149/200] batch [35/75] time 0.374 (0.476) data 0.242 (0.344) loss_u loss_u 0.9604 (0.9361) acc_u 6.2500 (7.6786) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [40/75] time 0.366 (0.470) data 0.235 (0.339) loss_u loss_u 0.9058 (0.9368) acc_u 9.3750 (7.5000) lr 3.2699e-04 eta 0:00:16
epoch [149/200] batch [45/75] time 0.432 (0.472) data 0.301 (0.341) loss_u loss_u 0.8496 (0.9302) acc_u 21.8750 (8.4722) lr 3.2699e-04 eta 0:00:14
epoch [149/200] batch [50/75] time 0.437 (0.470) data 0.305 (0.339) loss_u loss_u 0.9824 (0.9319) acc_u 0.0000 (8.1875) lr 3.2699e-04 eta 0:00:11
epoch [149/200] batch [55/75] time 0.531 (0.468) data 0.399 (0.337) loss_u loss_u 0.8560 (0.9302) acc_u 25.0000 (8.5795) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [60/75] time 0.429 (0.465) data 0.299 (0.334) loss_u loss_u 0.9443 (0.9310) acc_u 6.2500 (8.5417) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [65/75] time 0.383 (0.466) data 0.252 (0.335) loss_u loss_u 0.9404 (0.9304) acc_u 6.2500 (8.6058) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [70/75] time 0.480 (0.466) data 0.350 (0.335) loss_u loss_u 0.9375 (0.9302) acc_u 12.5000 (8.6607) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [75/75] time 0.561 (0.469) data 0.430 (0.337) loss_u loss_u 0.9146 (0.9313) acc_u 9.3750 (8.5417) lr 3.2699e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1521
confident_label rate tensor(0.2420, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 759
clean true:744
clean false:15
clean_rate:0.9802371541501976
noisy true:871
noisy false:1506
after delete: len(clean_dataset) 759
after delete: len(noisy_dataset) 2377
epoch [150/200] batch [5/23] time 0.531 (0.504) data 0.400 (0.373) loss_x loss_x 0.6284 (1.1581) acc_x 78.1250 (69.3750) lr 3.1545e-04 eta 0:00:09
epoch [150/200] batch [10/23] time 0.395 (0.476) data 0.265 (0.345) loss_x loss_x 0.9541 (1.1031) acc_x 75.0000 (71.8750) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [15/23] time 0.328 (0.454) data 0.198 (0.323) loss_x loss_x 1.0020 (1.0367) acc_x 71.8750 (73.5417) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [20/23] time 0.503 (0.448) data 0.373 (0.318) loss_x loss_x 1.1963 (1.0936) acc_x 75.0000 (72.8125) lr 3.1545e-04 eta 0:00:01
epoch [150/200] batch [5/74] time 0.395 (0.438) data 0.264 (0.308) loss_u loss_u 0.9312 (0.9492) acc_u 12.5000 (6.8750) lr 3.1545e-04 eta 0:00:30
epoch [150/200] batch [10/74] time 0.416 (0.443) data 0.285 (0.313) loss_u loss_u 0.9106 (0.9371) acc_u 9.3750 (8.1250) lr 3.1545e-04 eta 0:00:28
epoch [150/200] batch [15/74] time 0.337 (0.438) data 0.206 (0.308) loss_u loss_u 0.9595 (0.9391) acc_u 6.2500 (7.9167) lr 3.1545e-04 eta 0:00:25
epoch [150/200] batch [20/74] time 0.474 (0.440) data 0.342 (0.309) loss_u loss_u 0.9629 (0.9323) acc_u 6.2500 (8.9062) lr 3.1545e-04 eta 0:00:23
epoch [150/200] batch [25/74] time 0.463 (0.438) data 0.331 (0.307) loss_u loss_u 0.9653 (0.9384) acc_u 3.1250 (8.0000) lr 3.1545e-04 eta 0:00:21
epoch [150/200] batch [30/74] time 0.540 (0.437) data 0.409 (0.306) loss_u loss_u 0.9268 (0.9383) acc_u 9.3750 (8.2292) lr 3.1545e-04 eta 0:00:19
epoch [150/200] batch [35/74] time 0.518 (0.441) data 0.387 (0.310) loss_u loss_u 0.9233 (0.9383) acc_u 12.5000 (8.2143) lr 3.1545e-04 eta 0:00:17
epoch [150/200] batch [40/74] time 0.524 (0.446) data 0.394 (0.315) loss_u loss_u 0.9463 (0.9399) acc_u 6.2500 (7.8906) lr 3.1545e-04 eta 0:00:15
epoch [150/200] batch [45/74] time 0.456 (0.446) data 0.325 (0.315) loss_u loss_u 0.9575 (0.9432) acc_u 6.2500 (7.4306) lr 3.1545e-04 eta 0:00:12
epoch [150/200] batch [50/74] time 0.418 (0.448) data 0.287 (0.317) loss_u loss_u 0.9468 (0.9423) acc_u 12.5000 (7.5625) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [55/74] time 0.434 (0.451) data 0.304 (0.320) loss_u loss_u 0.9077 (0.9420) acc_u 9.3750 (7.5000) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [60/74] time 0.504 (0.451) data 0.371 (0.320) loss_u loss_u 0.9385 (0.9411) acc_u 12.5000 (7.6562) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [65/74] time 0.527 (0.454) data 0.396 (0.323) loss_u loss_u 0.8979 (0.9406) acc_u 15.6250 (7.5962) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [70/74] time 0.429 (0.454) data 0.297 (0.323) loss_u loss_u 0.9780 (0.9400) acc_u 3.1250 (7.6786) lr 3.1545e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1490
confident_label rate tensor(0.2331, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 731
clean true:720
clean false:11
clean_rate:0.9849521203830369
noisy true:926
noisy false:1479
after delete: len(clean_dataset) 731
after delete: len(noisy_dataset) 2405
epoch [151/200] batch [5/22] time 0.486 (0.475) data 0.355 (0.344) loss_x loss_x 1.2617 (1.1801) acc_x 68.7500 (72.5000) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [10/22] time 0.602 (0.473) data 0.472 (0.342) loss_x loss_x 1.2402 (1.0875) acc_x 68.7500 (72.5000) lr 3.0409e-04 eta 0:00:05
epoch [151/200] batch [15/22] time 0.476 (0.467) data 0.345 (0.336) loss_x loss_x 1.3223 (1.0427) acc_x 68.7500 (72.5000) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [20/22] time 0.442 (0.483) data 0.311 (0.352) loss_x loss_x 1.2842 (1.0495) acc_x 65.6250 (73.7500) lr 3.0409e-04 eta 0:00:00
epoch [151/200] batch [5/75] time 0.357 (0.478) data 0.225 (0.347) loss_u loss_u 0.9067 (0.9286) acc_u 12.5000 (10.6250) lr 3.0409e-04 eta 0:00:33
epoch [151/200] batch [10/75] time 0.431 (0.471) data 0.299 (0.340) loss_u loss_u 0.8989 (0.9228) acc_u 12.5000 (10.6250) lr 3.0409e-04 eta 0:00:30
epoch [151/200] batch [15/75] time 0.389 (0.473) data 0.257 (0.342) loss_u loss_u 0.9766 (0.9295) acc_u 3.1250 (9.5833) lr 3.0409e-04 eta 0:00:28
epoch [151/200] batch [20/75] time 0.518 (0.471) data 0.385 (0.340) loss_u loss_u 0.9302 (0.9300) acc_u 9.3750 (9.3750) lr 3.0409e-04 eta 0:00:25
epoch [151/200] batch [25/75] time 0.624 (0.471) data 0.493 (0.340) loss_u loss_u 0.8950 (0.9296) acc_u 12.5000 (9.2500) lr 3.0409e-04 eta 0:00:23
epoch [151/200] batch [30/75] time 0.451 (0.475) data 0.320 (0.344) loss_u loss_u 0.9526 (0.9312) acc_u 6.2500 (8.9583) lr 3.0409e-04 eta 0:00:21
epoch [151/200] batch [35/75] time 0.419 (0.470) data 0.288 (0.339) loss_u loss_u 0.9614 (0.9341) acc_u 6.2500 (8.6607) lr 3.0409e-04 eta 0:00:18
epoch [151/200] batch [40/75] time 0.465 (0.473) data 0.334 (0.342) loss_u loss_u 0.9390 (0.9367) acc_u 6.2500 (8.2812) lr 3.0409e-04 eta 0:00:16
epoch [151/200] batch [45/75] time 0.445 (0.471) data 0.313 (0.340) loss_u loss_u 0.9346 (0.9362) acc_u 6.2500 (8.4028) lr 3.0409e-04 eta 0:00:14
epoch [151/200] batch [50/75] time 0.516 (0.471) data 0.385 (0.340) loss_u loss_u 0.9312 (0.9371) acc_u 9.3750 (8.1875) lr 3.0409e-04 eta 0:00:11
epoch [151/200] batch [55/75] time 0.605 (0.473) data 0.473 (0.342) loss_u loss_u 0.9624 (0.9368) acc_u 6.2500 (8.1250) lr 3.0409e-04 eta 0:00:09
epoch [151/200] batch [60/75] time 0.571 (0.476) data 0.440 (0.345) loss_u loss_u 0.9170 (0.9356) acc_u 9.3750 (8.2812) lr 3.0409e-04 eta 0:00:07
epoch [151/200] batch [65/75] time 0.399 (0.475) data 0.268 (0.343) loss_u loss_u 0.8877 (0.9364) acc_u 15.6250 (8.3173) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [70/75] time 0.432 (0.475) data 0.300 (0.344) loss_u loss_u 0.9072 (0.9367) acc_u 12.5000 (8.3482) lr 3.0409e-04 eta 0:00:02
epoch [151/200] batch [75/75] time 0.505 (0.475) data 0.375 (0.344) loss_u loss_u 0.9141 (0.9363) acc_u 9.3750 (8.3333) lr 3.0409e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1521
confident_label rate tensor(0.2334, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 732
clean true:720
clean false:12
clean_rate:0.9836065573770492
noisy true:895
noisy false:1509
after delete: len(clean_dataset) 732
after delete: len(noisy_dataset) 2404
epoch [152/200] batch [5/22] time 0.424 (0.444) data 0.294 (0.314) loss_x loss_x 1.4053 (1.1856) acc_x 65.6250 (71.2500) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [10/22] time 0.483 (0.460) data 0.352 (0.329) loss_x loss_x 1.1338 (1.1149) acc_x 78.1250 (71.8750) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [15/22] time 0.394 (0.465) data 0.264 (0.335) loss_x loss_x 0.6152 (1.0830) acc_x 87.5000 (74.5833) lr 2.9289e-04 eta 0:00:03
epoch [152/200] batch [20/22] time 0.414 (0.480) data 0.283 (0.349) loss_x loss_x 1.4629 (1.0938) acc_x 62.5000 (73.9062) lr 2.9289e-04 eta 0:00:00
epoch [152/200] batch [5/75] time 0.527 (0.481) data 0.395 (0.350) loss_u loss_u 0.9390 (0.9355) acc_u 12.5000 (9.3750) lr 2.9289e-04 eta 0:00:33
epoch [152/200] batch [10/75] time 0.511 (0.480) data 0.379 (0.349) loss_u loss_u 0.9399 (0.9355) acc_u 6.2500 (9.0625) lr 2.9289e-04 eta 0:00:31
epoch [152/200] batch [15/75] time 0.464 (0.474) data 0.332 (0.343) loss_u loss_u 0.9629 (0.9339) acc_u 6.2500 (9.1667) lr 2.9289e-04 eta 0:00:28
epoch [152/200] batch [20/75] time 0.387 (0.469) data 0.255 (0.338) loss_u loss_u 0.9746 (0.9300) acc_u 6.2500 (9.6875) lr 2.9289e-04 eta 0:00:25
epoch [152/200] batch [25/75] time 0.372 (0.463) data 0.240 (0.332) loss_u loss_u 0.9106 (0.9324) acc_u 9.3750 (9.2500) lr 2.9289e-04 eta 0:00:23
epoch [152/200] batch [30/75] time 0.388 (0.462) data 0.258 (0.331) loss_u loss_u 0.9268 (0.9317) acc_u 12.5000 (9.4792) lr 2.9289e-04 eta 0:00:20
epoch [152/200] batch [35/75] time 0.492 (0.467) data 0.361 (0.336) loss_u loss_u 0.9170 (0.9348) acc_u 9.3750 (8.9286) lr 2.9289e-04 eta 0:00:18
epoch [152/200] batch [40/75] time 0.400 (0.463) data 0.268 (0.332) loss_u loss_u 0.9761 (0.9367) acc_u 3.1250 (8.6719) lr 2.9289e-04 eta 0:00:16
epoch [152/200] batch [45/75] time 0.423 (0.462) data 0.292 (0.330) loss_u loss_u 0.9512 (0.9398) acc_u 6.2500 (8.3333) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [50/75] time 0.423 (0.459) data 0.292 (0.327) loss_u loss_u 0.9473 (0.9398) acc_u 9.3750 (8.3125) lr 2.9289e-04 eta 0:00:11
epoch [152/200] batch [55/75] time 0.444 (0.460) data 0.312 (0.328) loss_u loss_u 0.9380 (0.9391) acc_u 6.2500 (8.2386) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [60/75] time 0.546 (0.461) data 0.414 (0.329) loss_u loss_u 0.9443 (0.9398) acc_u 6.2500 (7.9688) lr 2.9289e-04 eta 0:00:06
epoch [152/200] batch [65/75] time 0.486 (0.462) data 0.353 (0.331) loss_u loss_u 0.8662 (0.9371) acc_u 15.6250 (8.1731) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [70/75] time 0.421 (0.462) data 0.288 (0.331) loss_u loss_u 0.9561 (0.9359) acc_u 3.1250 (8.2143) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [75/75] time 0.462 (0.464) data 0.329 (0.333) loss_u loss_u 0.9316 (0.9362) acc_u 12.5000 (8.2083) lr 2.9289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1514
confident_label rate tensor(0.2379, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 746
clean true:736
clean false:10
clean_rate:0.9865951742627346
noisy true:886
noisy false:1504
after delete: len(clean_dataset) 746
after delete: len(noisy_dataset) 2390
epoch [153/200] batch [5/23] time 0.538 (0.493) data 0.407 (0.362) loss_x loss_x 1.0566 (1.0743) acc_x 71.8750 (73.7500) lr 2.8187e-04 eta 0:00:08
epoch [153/200] batch [10/23] time 0.361 (0.462) data 0.230 (0.331) loss_x loss_x 1.0059 (1.0820) acc_x 78.1250 (72.8125) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [15/23] time 0.371 (0.451) data 0.240 (0.321) loss_x loss_x 0.9126 (1.1283) acc_x 78.1250 (70.0000) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [20/23] time 0.549 (0.457) data 0.418 (0.326) loss_x loss_x 1.0410 (1.0877) acc_x 71.8750 (71.0938) lr 2.8187e-04 eta 0:00:01
epoch [153/200] batch [5/74] time 0.346 (0.457) data 0.214 (0.325) loss_u loss_u 0.9702 (0.9578) acc_u 3.1250 (5.0000) lr 2.8187e-04 eta 0:00:31
epoch [153/200] batch [10/74] time 0.609 (0.455) data 0.477 (0.324) loss_u loss_u 0.9009 (0.9481) acc_u 15.6250 (6.5625) lr 2.8187e-04 eta 0:00:29
epoch [153/200] batch [15/74] time 0.431 (0.451) data 0.300 (0.320) loss_u loss_u 0.8706 (0.9411) acc_u 18.7500 (7.7083) lr 2.8187e-04 eta 0:00:26
epoch [153/200] batch [20/74] time 0.470 (0.446) data 0.340 (0.315) loss_u loss_u 0.9409 (0.9443) acc_u 6.2500 (7.1875) lr 2.8187e-04 eta 0:00:24
epoch [153/200] batch [25/74] time 0.406 (0.445) data 0.275 (0.314) loss_u loss_u 0.9414 (0.9425) acc_u 6.2500 (7.3750) lr 2.8187e-04 eta 0:00:21
epoch [153/200] batch [30/74] time 0.499 (0.443) data 0.368 (0.312) loss_u loss_u 0.9473 (0.9369) acc_u 6.2500 (7.9167) lr 2.8187e-04 eta 0:00:19
epoch [153/200] batch [35/74] time 0.846 (0.451) data 0.713 (0.320) loss_u loss_u 0.9619 (0.9371) acc_u 3.1250 (7.9464) lr 2.8187e-04 eta 0:00:17
epoch [153/200] batch [40/74] time 0.363 (0.451) data 0.232 (0.319) loss_u loss_u 0.9258 (0.9367) acc_u 9.3750 (7.9688) lr 2.8187e-04 eta 0:00:15
epoch [153/200] batch [45/74] time 0.408 (0.449) data 0.276 (0.317) loss_u loss_u 0.9849 (0.9365) acc_u 3.1250 (7.9167) lr 2.8187e-04 eta 0:00:13
epoch [153/200] batch [50/74] time 0.534 (0.451) data 0.402 (0.320) loss_u loss_u 0.9790 (0.9383) acc_u 3.1250 (7.8125) lr 2.8187e-04 eta 0:00:10
epoch [153/200] batch [55/74] time 0.395 (0.449) data 0.263 (0.317) loss_u loss_u 0.9678 (0.9369) acc_u 3.1250 (7.8977) lr 2.8187e-04 eta 0:00:08
epoch [153/200] batch [60/74] time 0.571 (0.450) data 0.438 (0.318) loss_u loss_u 0.8916 (0.9354) acc_u 12.5000 (8.0729) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [65/74] time 0.494 (0.453) data 0.362 (0.322) loss_u loss_u 0.9580 (0.9357) acc_u 6.2500 (8.0288) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [70/74] time 0.463 (0.454) data 0.331 (0.323) loss_u loss_u 0.8599 (0.9353) acc_u 21.8750 (8.1696) lr 2.8187e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1530
confident_label rate tensor(0.2401, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 753
clean true:741
clean false:12
clean_rate:0.9840637450199203
noisy true:865
noisy false:1518
after delete: len(clean_dataset) 753
after delete: len(noisy_dataset) 2383
epoch [154/200] batch [5/23] time 0.416 (0.445) data 0.286 (0.315) loss_x loss_x 0.9419 (0.9684) acc_x 78.1250 (75.6250) lr 2.7103e-04 eta 0:00:08
epoch [154/200] batch [10/23] time 0.374 (0.467) data 0.244 (0.336) loss_x loss_x 1.2363 (1.0087) acc_x 65.6250 (73.4375) lr 2.7103e-04 eta 0:00:06
epoch [154/200] batch [15/23] time 0.575 (0.485) data 0.443 (0.354) loss_x loss_x 1.1475 (1.0649) acc_x 71.8750 (72.5000) lr 2.7103e-04 eta 0:00:03
epoch [154/200] batch [20/23] time 0.382 (0.482) data 0.250 (0.351) loss_x loss_x 1.1904 (1.1229) acc_x 75.0000 (71.7188) lr 2.7103e-04 eta 0:00:01
epoch [154/200] batch [5/74] time 0.384 (0.476) data 0.252 (0.345) loss_u loss_u 0.9580 (0.9446) acc_u 3.1250 (6.2500) lr 2.7103e-04 eta 0:00:32
epoch [154/200] batch [10/74] time 0.447 (0.473) data 0.315 (0.341) loss_u loss_u 0.9561 (0.9337) acc_u 3.1250 (7.5000) lr 2.7103e-04 eta 0:00:30
epoch [154/200] batch [15/74] time 0.452 (0.470) data 0.322 (0.339) loss_u loss_u 0.9570 (0.9414) acc_u 3.1250 (7.0833) lr 2.7103e-04 eta 0:00:27
epoch [154/200] batch [20/74] time 0.514 (0.474) data 0.383 (0.342) loss_u loss_u 0.9521 (0.9427) acc_u 6.2500 (6.8750) lr 2.7103e-04 eta 0:00:25
epoch [154/200] batch [25/74] time 0.438 (0.465) data 0.306 (0.334) loss_u loss_u 0.9932 (0.9475) acc_u 0.0000 (6.1250) lr 2.7103e-04 eta 0:00:22
epoch [154/200] batch [30/74] time 0.468 (0.461) data 0.336 (0.329) loss_u loss_u 0.9185 (0.9428) acc_u 9.3750 (6.6667) lr 2.7103e-04 eta 0:00:20
epoch [154/200] batch [35/74] time 0.426 (0.459) data 0.294 (0.328) loss_u loss_u 0.9277 (0.9404) acc_u 9.3750 (7.1429) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [40/74] time 0.402 (0.459) data 0.271 (0.327) loss_u loss_u 0.9092 (0.9418) acc_u 9.3750 (6.8750) lr 2.7103e-04 eta 0:00:15
epoch [154/200] batch [45/74] time 0.560 (0.462) data 0.427 (0.330) loss_u loss_u 0.9751 (0.9419) acc_u 6.2500 (7.0833) lr 2.7103e-04 eta 0:00:13
epoch [154/200] batch [50/74] time 0.342 (0.460) data 0.211 (0.328) loss_u loss_u 0.9331 (0.9385) acc_u 9.3750 (7.5000) lr 2.7103e-04 eta 0:00:11
epoch [154/200] batch [55/74] time 0.369 (0.457) data 0.238 (0.325) loss_u loss_u 0.9209 (0.9394) acc_u 12.5000 (7.5000) lr 2.7103e-04 eta 0:00:08
epoch [154/200] batch [60/74] time 0.439 (0.456) data 0.308 (0.325) loss_u loss_u 0.9048 (0.9393) acc_u 12.5000 (7.5000) lr 2.7103e-04 eta 0:00:06
epoch [154/200] batch [65/74] time 0.543 (0.459) data 0.409 (0.328) loss_u loss_u 0.9683 (0.9396) acc_u 3.1250 (7.4038) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [70/74] time 0.566 (0.464) data 0.433 (0.332) loss_u loss_u 0.9302 (0.9382) acc_u 9.3750 (7.5446) lr 2.7103e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1575
confident_label rate tensor(0.2286, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 717
clean true:707
clean false:10
clean_rate:0.9860529986052998
noisy true:854
noisy false:1565
after delete: len(clean_dataset) 717
after delete: len(noisy_dataset) 2419
epoch [155/200] batch [5/22] time 0.418 (0.474) data 0.287 (0.343) loss_x loss_x 0.6821 (1.1706) acc_x 87.5000 (75.0000) lr 2.6037e-04 eta 0:00:08
epoch [155/200] batch [10/22] time 0.447 (0.497) data 0.315 (0.366) loss_x loss_x 0.6226 (1.0681) acc_x 90.6250 (78.1250) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [15/22] time 0.501 (0.482) data 0.370 (0.351) loss_x loss_x 0.6855 (1.0543) acc_x 84.3750 (77.7083) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [20/22] time 0.440 (0.473) data 0.308 (0.342) loss_x loss_x 0.6201 (1.0313) acc_x 84.3750 (77.3438) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/75] time 0.525 (0.462) data 0.393 (0.330) loss_u loss_u 0.9409 (0.9639) acc_u 6.2500 (4.3750) lr 2.6037e-04 eta 0:00:32
epoch [155/200] batch [10/75] time 0.470 (0.457) data 0.337 (0.326) loss_u loss_u 0.9839 (0.9613) acc_u 0.0000 (5.0000) lr 2.6037e-04 eta 0:00:29
epoch [155/200] batch [15/75] time 0.454 (0.454) data 0.322 (0.323) loss_u loss_u 0.9917 (0.9438) acc_u 0.0000 (6.8750) lr 2.6037e-04 eta 0:00:27
epoch [155/200] batch [20/75] time 0.465 (0.459) data 0.332 (0.327) loss_u loss_u 0.8892 (0.9376) acc_u 12.5000 (7.6562) lr 2.6037e-04 eta 0:00:25
epoch [155/200] batch [25/75] time 0.447 (0.458) data 0.314 (0.327) loss_u loss_u 0.9194 (0.9358) acc_u 9.3750 (7.8750) lr 2.6037e-04 eta 0:00:22
epoch [155/200] batch [30/75] time 0.454 (0.457) data 0.322 (0.326) loss_u loss_u 0.9146 (0.9359) acc_u 15.6250 (8.0208) lr 2.6037e-04 eta 0:00:20
epoch [155/200] batch [35/75] time 0.428 (0.458) data 0.296 (0.327) loss_u loss_u 0.9614 (0.9378) acc_u 6.2500 (7.7679) lr 2.6037e-04 eta 0:00:18
epoch [155/200] batch [40/75] time 0.389 (0.459) data 0.257 (0.327) loss_u loss_u 0.9922 (0.9383) acc_u 0.0000 (7.6562) lr 2.6037e-04 eta 0:00:16
epoch [155/200] batch [45/75] time 0.623 (0.467) data 0.491 (0.335) loss_u loss_u 0.9609 (0.9401) acc_u 3.1250 (7.4306) lr 2.6037e-04 eta 0:00:14
epoch [155/200] batch [50/75] time 0.338 (0.471) data 0.207 (0.339) loss_u loss_u 0.8906 (0.9371) acc_u 15.6250 (7.9375) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [55/75] time 0.407 (0.467) data 0.275 (0.336) loss_u loss_u 0.8379 (0.9339) acc_u 21.8750 (8.2955) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [60/75] time 0.482 (0.468) data 0.351 (0.336) loss_u loss_u 0.9258 (0.9344) acc_u 9.3750 (8.3333) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [65/75] time 0.478 (0.468) data 0.346 (0.336) loss_u loss_u 0.9683 (0.9353) acc_u 3.1250 (8.2212) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [70/75] time 0.457 (0.468) data 0.325 (0.336) loss_u loss_u 0.9658 (0.9347) acc_u 6.2500 (8.2589) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [75/75] time 0.675 (0.476) data 0.543 (0.344) loss_u loss_u 0.9399 (0.9347) acc_u 6.2500 (8.2500) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1509
confident_label rate tensor(0.2296, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 720
clean true:708
clean false:12
clean_rate:0.9833333333333333
noisy true:919
noisy false:1497
after delete: len(clean_dataset) 720
after delete: len(noisy_dataset) 2416
epoch [156/200] batch [5/22] time 0.683 (0.594) data 0.552 (0.462) loss_x loss_x 1.4238 (1.3461) acc_x 71.8750 (66.8750) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [10/22] time 0.590 (0.567) data 0.459 (0.436) loss_x loss_x 0.8462 (1.1865) acc_x 78.1250 (73.4375) lr 2.4989e-04 eta 0:00:06
epoch [156/200] batch [15/22] time 0.365 (0.525) data 0.235 (0.394) loss_x loss_x 0.9434 (1.1279) acc_x 71.8750 (73.1250) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [20/22] time 0.416 (0.508) data 0.285 (0.377) loss_x loss_x 0.8657 (1.0779) acc_x 81.2500 (74.2188) lr 2.4989e-04 eta 0:00:01
epoch [156/200] batch [5/75] time 0.407 (0.498) data 0.276 (0.367) loss_u loss_u 0.9023 (0.9384) acc_u 9.3750 (6.8750) lr 2.4989e-04 eta 0:00:34
epoch [156/200] batch [10/75] time 0.528 (0.501) data 0.396 (0.370) loss_u loss_u 0.9268 (0.9453) acc_u 9.3750 (5.9375) lr 2.4989e-04 eta 0:00:32
epoch [156/200] batch [15/75] time 0.464 (0.497) data 0.330 (0.366) loss_u loss_u 0.9634 (0.9386) acc_u 3.1250 (7.7083) lr 2.4989e-04 eta 0:00:29
epoch [156/200] batch [20/75] time 0.556 (0.506) data 0.426 (0.375) loss_u loss_u 0.9429 (0.9383) acc_u 6.2500 (7.5000) lr 2.4989e-04 eta 0:00:27
epoch [156/200] batch [25/75] time 0.447 (0.496) data 0.316 (0.365) loss_u loss_u 0.8271 (0.9302) acc_u 15.6250 (8.3750) lr 2.4989e-04 eta 0:00:24
epoch [156/200] batch [30/75] time 0.465 (0.489) data 0.333 (0.358) loss_u loss_u 0.9839 (0.9324) acc_u 0.0000 (8.2292) lr 2.4989e-04 eta 0:00:22
epoch [156/200] batch [35/75] time 0.374 (0.485) data 0.243 (0.353) loss_u loss_u 0.9082 (0.9332) acc_u 12.5000 (8.1250) lr 2.4989e-04 eta 0:00:19
epoch [156/200] batch [40/75] time 0.368 (0.478) data 0.236 (0.347) loss_u loss_u 0.9380 (0.9322) acc_u 9.3750 (8.2812) lr 2.4989e-04 eta 0:00:16
epoch [156/200] batch [45/75] time 0.436 (0.479) data 0.305 (0.347) loss_u loss_u 0.9380 (0.9335) acc_u 9.3750 (8.0556) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [50/75] time 0.412 (0.475) data 0.282 (0.344) loss_u loss_u 0.9805 (0.9315) acc_u 6.2500 (8.3125) lr 2.4989e-04 eta 0:00:11
epoch [156/200] batch [55/75] time 0.339 (0.473) data 0.208 (0.341) loss_u loss_u 0.9761 (0.9324) acc_u 6.2500 (8.2955) lr 2.4989e-04 eta 0:00:09
epoch [156/200] batch [60/75] time 0.453 (0.473) data 0.323 (0.341) loss_u loss_u 0.9443 (0.9327) acc_u 6.2500 (8.3333) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [65/75] time 0.356 (0.471) data 0.225 (0.339) loss_u loss_u 0.9971 (0.9327) acc_u 0.0000 (8.2692) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [70/75] time 0.347 (0.468) data 0.212 (0.336) loss_u loss_u 0.9971 (0.9341) acc_u 0.0000 (8.0804) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [75/75] time 0.408 (0.468) data 0.277 (0.336) loss_u loss_u 0.9351 (0.9346) acc_u 6.2500 (8.0000) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1560
confident_label rate tensor(0.2344, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 735
clean true:723
clean false:12
clean_rate:0.9836734693877551
noisy true:853
noisy false:1548
after delete: len(clean_dataset) 735
after delete: len(noisy_dataset) 2401
epoch [157/200] batch [5/22] time 0.445 (0.483) data 0.315 (0.352) loss_x loss_x 0.7119 (1.1748) acc_x 84.3750 (70.6250) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [10/22] time 0.462 (0.495) data 0.331 (0.364) loss_x loss_x 0.5015 (1.0877) acc_x 90.6250 (74.0625) lr 2.3959e-04 eta 0:00:05
epoch [157/200] batch [15/22] time 0.439 (0.466) data 0.307 (0.336) loss_x loss_x 1.1992 (1.0840) acc_x 68.7500 (73.7500) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [20/22] time 0.633 (0.477) data 0.502 (0.347) loss_x loss_x 1.2207 (1.0852) acc_x 68.7500 (74.3750) lr 2.3959e-04 eta 0:00:00
epoch [157/200] batch [5/75] time 0.419 (0.461) data 0.288 (0.330) loss_u loss_u 0.9609 (0.9374) acc_u 6.2500 (8.1250) lr 2.3959e-04 eta 0:00:32
epoch [157/200] batch [10/75] time 0.377 (0.459) data 0.246 (0.328) loss_u loss_u 0.9409 (0.9377) acc_u 9.3750 (7.5000) lr 2.3959e-04 eta 0:00:29
epoch [157/200] batch [15/75] time 0.645 (0.464) data 0.514 (0.333) loss_u loss_u 0.9639 (0.9418) acc_u 6.2500 (7.9167) lr 2.3959e-04 eta 0:00:27
epoch [157/200] batch [20/75] time 0.394 (0.457) data 0.264 (0.326) loss_u loss_u 0.9746 (0.9415) acc_u 3.1250 (7.8125) lr 2.3959e-04 eta 0:00:25
epoch [157/200] batch [25/75] time 0.434 (0.456) data 0.302 (0.325) loss_u loss_u 0.8477 (0.9390) acc_u 21.8750 (8.1250) lr 2.3959e-04 eta 0:00:22
epoch [157/200] batch [30/75] time 0.413 (0.453) data 0.282 (0.322) loss_u loss_u 0.8979 (0.9355) acc_u 15.6250 (8.6458) lr 2.3959e-04 eta 0:00:20
epoch [157/200] batch [35/75] time 0.452 (0.451) data 0.319 (0.320) loss_u loss_u 0.9697 (0.9365) acc_u 3.1250 (8.3929) lr 2.3959e-04 eta 0:00:18
epoch [157/200] batch [40/75] time 0.440 (0.451) data 0.309 (0.320) loss_u loss_u 0.8862 (0.9365) acc_u 18.7500 (8.5938) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [45/75] time 0.812 (0.456) data 0.682 (0.325) loss_u loss_u 0.9385 (0.9364) acc_u 9.3750 (8.6111) lr 2.3959e-04 eta 0:00:13
epoch [157/200] batch [50/75] time 0.622 (0.458) data 0.490 (0.327) loss_u loss_u 0.9517 (0.9365) acc_u 3.1250 (8.5000) lr 2.3959e-04 eta 0:00:11
epoch [157/200] batch [55/75] time 0.397 (0.460) data 0.264 (0.329) loss_u loss_u 0.9395 (0.9362) acc_u 9.3750 (8.6364) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [60/75] time 0.359 (0.458) data 0.229 (0.327) loss_u loss_u 0.9243 (0.9365) acc_u 9.3750 (8.5417) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [65/75] time 0.423 (0.457) data 0.291 (0.326) loss_u loss_u 0.9106 (0.9362) acc_u 12.5000 (8.6058) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [70/75] time 0.533 (0.457) data 0.400 (0.326) loss_u loss_u 0.9634 (0.9375) acc_u 3.1250 (8.4821) lr 2.3959e-04 eta 0:00:02
epoch [157/200] batch [75/75] time 0.419 (0.454) data 0.286 (0.322) loss_u loss_u 0.8984 (0.9368) acc_u 18.7500 (8.5833) lr 2.3959e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1521
confident_label rate tensor(0.2334, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 732
clean true:717
clean false:15
clean_rate:0.9795081967213115
noisy true:898
noisy false:1506
after delete: len(clean_dataset) 732
after delete: len(noisy_dataset) 2404
epoch [158/200] batch [5/22] time 0.412 (0.446) data 0.281 (0.314) loss_x loss_x 0.8911 (0.9402) acc_x 78.1250 (78.1250) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [10/22] time 0.459 (0.460) data 0.326 (0.328) loss_x loss_x 0.6787 (1.0119) acc_x 84.3750 (76.5625) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [15/22] time 0.396 (0.469) data 0.265 (0.337) loss_x loss_x 0.7632 (1.0418) acc_x 84.3750 (75.6250) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [20/22] time 0.403 (0.478) data 0.266 (0.346) loss_x loss_x 1.5898 (1.1307) acc_x 71.8750 (74.0625) lr 2.2949e-04 eta 0:00:00
epoch [158/200] batch [5/75] time 0.407 (0.488) data 0.277 (0.355) loss_u loss_u 0.9746 (0.9446) acc_u 3.1250 (6.2500) lr 2.2949e-04 eta 0:00:34
epoch [158/200] batch [10/75] time 0.514 (0.490) data 0.384 (0.358) loss_u loss_u 0.9448 (0.9444) acc_u 6.2500 (7.1875) lr 2.2949e-04 eta 0:00:31
epoch [158/200] batch [15/75] time 0.474 (0.484) data 0.344 (0.352) loss_u loss_u 0.8652 (0.9316) acc_u 18.7500 (9.5833) lr 2.2949e-04 eta 0:00:29
epoch [158/200] batch [20/75] time 0.516 (0.482) data 0.384 (0.350) loss_u loss_u 0.9258 (0.9354) acc_u 6.2500 (9.2188) lr 2.2949e-04 eta 0:00:26
epoch [158/200] batch [25/75] time 0.450 (0.479) data 0.318 (0.347) loss_u loss_u 0.9565 (0.9344) acc_u 9.3750 (9.6250) lr 2.2949e-04 eta 0:00:23
epoch [158/200] batch [30/75] time 0.433 (0.473) data 0.302 (0.342) loss_u loss_u 0.9019 (0.9347) acc_u 12.5000 (9.3750) lr 2.2949e-04 eta 0:00:21
epoch [158/200] batch [35/75] time 0.413 (0.476) data 0.283 (0.345) loss_u loss_u 0.9307 (0.9362) acc_u 6.2500 (9.1964) lr 2.2949e-04 eta 0:00:19
epoch [158/200] batch [40/75] time 0.503 (0.480) data 0.372 (0.348) loss_u loss_u 0.9214 (0.9372) acc_u 12.5000 (8.9844) lr 2.2949e-04 eta 0:00:16
epoch [158/200] batch [45/75] time 0.473 (0.476) data 0.341 (0.344) loss_u loss_u 0.9717 (0.9368) acc_u 3.1250 (9.0278) lr 2.2949e-04 eta 0:00:14
epoch [158/200] batch [50/75] time 0.464 (0.475) data 0.332 (0.344) loss_u loss_u 0.8921 (0.9344) acc_u 15.6250 (9.2500) lr 2.2949e-04 eta 0:00:11
epoch [158/200] batch [55/75] time 0.420 (0.475) data 0.287 (0.343) loss_u loss_u 0.9082 (0.9346) acc_u 12.5000 (9.2614) lr 2.2949e-04 eta 0:00:09
epoch [158/200] batch [60/75] time 0.381 (0.473) data 0.251 (0.341) loss_u loss_u 0.9224 (0.9338) acc_u 9.3750 (9.3750) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [65/75] time 0.494 (0.471) data 0.363 (0.339) loss_u loss_u 0.9766 (0.9337) acc_u 6.2500 (9.4231) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [70/75] time 0.585 (0.473) data 0.454 (0.341) loss_u loss_u 0.9478 (0.9335) acc_u 6.2500 (9.3750) lr 2.2949e-04 eta 0:00:02
epoch [158/200] batch [75/75] time 0.413 (0.473) data 0.280 (0.341) loss_u loss_u 0.8989 (0.9325) acc_u 12.5000 (9.4583) lr 2.2949e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1521
confident_label rate tensor(0.2337, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 733
clean true:720
clean false:13
clean_rate:0.9822646657571623
noisy true:895
noisy false:1508
after delete: len(clean_dataset) 733
after delete: len(noisy_dataset) 2403
epoch [159/200] batch [5/22] time 0.596 (0.536) data 0.466 (0.405) loss_x loss_x 1.2285 (0.9046) acc_x 75.0000 (78.1250) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [10/22] time 0.445 (0.519) data 0.314 (0.388) loss_x loss_x 1.9863 (1.0772) acc_x 59.3750 (74.3750) lr 2.1957e-04 eta 0:00:06
epoch [159/200] batch [15/22] time 0.633 (0.501) data 0.502 (0.370) loss_x loss_x 0.9004 (1.0579) acc_x 75.0000 (74.7917) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [20/22] time 0.359 (0.482) data 0.227 (0.352) loss_x loss_x 1.5713 (1.1233) acc_x 62.5000 (74.2188) lr 2.1957e-04 eta 0:00:00
epoch [159/200] batch [5/75] time 0.463 (0.471) data 0.329 (0.339) loss_u loss_u 0.8730 (0.9331) acc_u 15.6250 (8.1250) lr 2.1957e-04 eta 0:00:32
epoch [159/200] batch [10/75] time 0.356 (0.469) data 0.225 (0.338) loss_u loss_u 0.9004 (0.9322) acc_u 15.6250 (9.0625) lr 2.1957e-04 eta 0:00:30
epoch [159/200] batch [15/75] time 0.314 (0.466) data 0.182 (0.335) loss_u loss_u 0.9575 (0.9280) acc_u 6.2500 (10.0000) lr 2.1957e-04 eta 0:00:27
epoch [159/200] batch [20/75] time 0.501 (0.467) data 0.369 (0.335) loss_u loss_u 0.9351 (0.9261) acc_u 6.2500 (10.1562) lr 2.1957e-04 eta 0:00:25
epoch [159/200] batch [25/75] time 0.539 (0.466) data 0.407 (0.334) loss_u loss_u 0.9736 (0.9313) acc_u 3.1250 (9.3750) lr 2.1957e-04 eta 0:00:23
epoch [159/200] batch [30/75] time 0.371 (0.465) data 0.239 (0.333) loss_u loss_u 0.8916 (0.9283) acc_u 12.5000 (9.4792) lr 2.1957e-04 eta 0:00:20
epoch [159/200] batch [35/75] time 0.418 (0.466) data 0.288 (0.335) loss_u loss_u 0.9814 (0.9306) acc_u 3.1250 (9.1071) lr 2.1957e-04 eta 0:00:18
epoch [159/200] batch [40/75] time 0.475 (0.469) data 0.343 (0.338) loss_u loss_u 0.9731 (0.9332) acc_u 6.2500 (8.8281) lr 2.1957e-04 eta 0:00:16
epoch [159/200] batch [45/75] time 0.472 (0.473) data 0.340 (0.341) loss_u loss_u 0.9131 (0.9351) acc_u 9.3750 (8.6111) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [50/75] time 0.467 (0.472) data 0.334 (0.341) loss_u loss_u 0.8936 (0.9336) acc_u 15.6250 (8.7500) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [55/75] time 0.418 (0.471) data 0.288 (0.339) loss_u loss_u 0.9468 (0.9339) acc_u 6.2500 (8.6364) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [60/75] time 0.519 (0.467) data 0.389 (0.336) loss_u loss_u 0.9316 (0.9349) acc_u 9.3750 (8.4896) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [65/75] time 0.684 (0.470) data 0.553 (0.339) loss_u loss_u 0.9038 (0.9349) acc_u 9.3750 (8.4135) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [70/75] time 0.490 (0.470) data 0.360 (0.339) loss_u loss_u 0.9521 (0.9351) acc_u 6.2500 (8.4375) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [75/75] time 0.467 (0.471) data 0.336 (0.340) loss_u loss_u 0.9541 (0.9365) acc_u 9.3750 (8.2500) lr 2.1957e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1494
confident_label rate tensor(0.2353, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 738
clean true:726
clean false:12
clean_rate:0.983739837398374
noisy true:916
noisy false:1482
after delete: len(clean_dataset) 738
after delete: len(noisy_dataset) 2398
epoch [160/200] batch [5/23] time 0.421 (0.542) data 0.290 (0.411) loss_x loss_x 1.3945 (1.1812) acc_x 75.0000 (70.0000) lr 2.0984e-04 eta 0:00:09
epoch [160/200] batch [10/23] time 0.401 (0.504) data 0.271 (0.373) loss_x loss_x 1.1885 (1.0316) acc_x 68.7500 (72.8125) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [15/23] time 0.442 (0.488) data 0.313 (0.357) loss_x loss_x 0.8652 (0.9646) acc_x 75.0000 (74.1667) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [20/23] time 0.474 (0.475) data 0.342 (0.344) loss_x loss_x 0.9858 (0.9811) acc_x 84.3750 (75.0000) lr 2.0984e-04 eta 0:00:01
epoch [160/200] batch [5/74] time 0.470 (0.484) data 0.338 (0.353) loss_u loss_u 0.9556 (0.9256) acc_u 6.2500 (10.0000) lr 2.0984e-04 eta 0:00:33
epoch [160/200] batch [10/74] time 0.459 (0.481) data 0.329 (0.350) loss_u loss_u 0.9282 (0.9448) acc_u 12.5000 (8.1250) lr 2.0984e-04 eta 0:00:30
epoch [160/200] batch [15/74] time 0.332 (0.470) data 0.201 (0.339) loss_u loss_u 0.8848 (0.9367) acc_u 12.5000 (8.7500) lr 2.0984e-04 eta 0:00:27
epoch [160/200] batch [20/74] time 0.596 (0.472) data 0.466 (0.341) loss_u loss_u 0.9268 (0.9335) acc_u 6.2500 (8.5938) lr 2.0984e-04 eta 0:00:25
epoch [160/200] batch [25/74] time 0.416 (0.469) data 0.284 (0.338) loss_u loss_u 0.9639 (0.9378) acc_u 3.1250 (8.0000) lr 2.0984e-04 eta 0:00:22
epoch [160/200] batch [30/74] time 0.411 (0.468) data 0.280 (0.337) loss_u loss_u 0.8872 (0.9353) acc_u 18.7500 (8.3333) lr 2.0984e-04 eta 0:00:20
epoch [160/200] batch [35/74] time 0.421 (0.465) data 0.289 (0.334) loss_u loss_u 0.9424 (0.9354) acc_u 6.2500 (8.5714) lr 2.0984e-04 eta 0:00:18
epoch [160/200] batch [40/74] time 0.495 (0.469) data 0.363 (0.338) loss_u loss_u 0.8770 (0.9354) acc_u 12.5000 (8.6719) lr 2.0984e-04 eta 0:00:15
epoch [160/200] batch [45/74] time 0.460 (0.467) data 0.329 (0.336) loss_u loss_u 0.9028 (0.9328) acc_u 12.5000 (8.9583) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [50/74] time 0.440 (0.474) data 0.308 (0.342) loss_u loss_u 0.9517 (0.9335) acc_u 6.2500 (8.8125) lr 2.0984e-04 eta 0:00:11
epoch [160/200] batch [55/74] time 0.349 (0.472) data 0.218 (0.341) loss_u loss_u 0.9360 (0.9321) acc_u 9.3750 (8.9205) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [60/74] time 0.497 (0.473) data 0.366 (0.342) loss_u loss_u 0.9375 (0.9341) acc_u 6.2500 (8.4896) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [65/74] time 0.461 (0.472) data 0.330 (0.341) loss_u loss_u 0.9844 (0.9352) acc_u 3.1250 (8.3654) lr 2.0984e-04 eta 0:00:04
epoch [160/200] batch [70/74] time 0.387 (0.472) data 0.254 (0.341) loss_u loss_u 0.9507 (0.9348) acc_u 9.3750 (8.4821) lr 2.0984e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1567
confident_label rate tensor(0.2337, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 733
clean true:723
clean false:10
clean_rate:0.9863574351978172
noisy true:846
noisy false:1557
after delete: len(clean_dataset) 733
after delete: len(noisy_dataset) 2403
epoch [161/200] batch [5/22] time 0.430 (0.423) data 0.300 (0.292) loss_x loss_x 0.9468 (1.1296) acc_x 78.1250 (72.5000) lr 2.0032e-04 eta 0:00:07
epoch [161/200] batch [10/22] time 0.509 (0.458) data 0.378 (0.327) loss_x loss_x 1.0889 (1.0788) acc_x 71.8750 (71.5625) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [15/22] time 0.448 (0.451) data 0.317 (0.321) loss_x loss_x 1.4502 (1.0741) acc_x 71.8750 (72.9167) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [20/22] time 0.448 (0.467) data 0.317 (0.337) loss_x loss_x 1.0977 (1.0465) acc_x 81.2500 (74.5312) lr 2.0032e-04 eta 0:00:00
epoch [161/200] batch [5/75] time 0.549 (0.486) data 0.417 (0.355) loss_u loss_u 0.9614 (0.9343) acc_u 3.1250 (8.7500) lr 2.0032e-04 eta 0:00:34
epoch [161/200] batch [10/75] time 0.350 (0.480) data 0.218 (0.349) loss_u loss_u 0.9126 (0.9389) acc_u 12.5000 (8.4375) lr 2.0032e-04 eta 0:00:31
epoch [161/200] batch [15/75] time 0.526 (0.474) data 0.394 (0.343) loss_u loss_u 0.9243 (0.9398) acc_u 12.5000 (8.1250) lr 2.0032e-04 eta 0:00:28
epoch [161/200] batch [20/75] time 0.456 (0.473) data 0.326 (0.341) loss_u loss_u 0.8779 (0.9318) acc_u 12.5000 (9.0625) lr 2.0032e-04 eta 0:00:25
epoch [161/200] batch [25/75] time 0.368 (0.464) data 0.238 (0.333) loss_u loss_u 0.9282 (0.9331) acc_u 9.3750 (8.7500) lr 2.0032e-04 eta 0:00:23
epoch [161/200] batch [30/75] time 0.600 (0.466) data 0.469 (0.335) loss_u loss_u 0.9834 (0.9328) acc_u 0.0000 (8.5417) lr 2.0032e-04 eta 0:00:20
epoch [161/200] batch [35/75] time 0.435 (0.463) data 0.305 (0.332) loss_u loss_u 0.9336 (0.9327) acc_u 6.2500 (8.6607) lr 2.0032e-04 eta 0:00:18
epoch [161/200] batch [40/75] time 0.407 (0.461) data 0.276 (0.329) loss_u loss_u 0.9834 (0.9362) acc_u 0.0000 (8.1250) lr 2.0032e-04 eta 0:00:16
epoch [161/200] batch [45/75] time 0.418 (0.457) data 0.286 (0.326) loss_u loss_u 0.9355 (0.9366) acc_u 9.3750 (8.0556) lr 2.0032e-04 eta 0:00:13
epoch [161/200] batch [50/75] time 0.548 (0.456) data 0.417 (0.325) loss_u loss_u 0.9307 (0.9368) acc_u 12.5000 (8.1250) lr 2.0032e-04 eta 0:00:11
epoch [161/200] batch [55/75] time 0.432 (0.455) data 0.301 (0.323) loss_u loss_u 0.9683 (0.9376) acc_u 3.1250 (8.0114) lr 2.0032e-04 eta 0:00:09
epoch [161/200] batch [60/75] time 0.484 (0.456) data 0.353 (0.325) loss_u loss_u 0.9072 (0.9385) acc_u 15.6250 (8.0729) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [65/75] time 0.464 (0.457) data 0.332 (0.326) loss_u loss_u 0.9814 (0.9384) acc_u 3.1250 (8.0288) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [70/75] time 0.393 (0.457) data 0.263 (0.326) loss_u loss_u 0.8828 (0.9363) acc_u 15.6250 (8.2143) lr 2.0032e-04 eta 0:00:02
epoch [161/200] batch [75/75] time 0.493 (0.458) data 0.362 (0.327) loss_u loss_u 0.9531 (0.9361) acc_u 3.1250 (8.1250) lr 2.0032e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1490
confident_label rate tensor(0.2318, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 727
clean true:714
clean false:13
clean_rate:0.9821182943603851
noisy true:932
noisy false:1477
after delete: len(clean_dataset) 727
after delete: len(noisy_dataset) 2409
epoch [162/200] batch [5/22] time 0.441 (0.507) data 0.309 (0.376) loss_x loss_x 0.7529 (1.0016) acc_x 87.5000 (76.8750) lr 1.9098e-04 eta 0:00:08
epoch [162/200] batch [10/22] time 0.593 (0.532) data 0.463 (0.401) loss_x loss_x 1.0449 (1.0205) acc_x 71.8750 (76.2500) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [15/22] time 0.367 (0.499) data 0.237 (0.368) loss_x loss_x 1.4053 (1.0931) acc_x 78.1250 (76.6667) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [20/22] time 0.435 (0.484) data 0.304 (0.353) loss_x loss_x 1.1680 (1.1099) acc_x 65.6250 (74.5312) lr 1.9098e-04 eta 0:00:00
epoch [162/200] batch [5/75] time 0.430 (0.497) data 0.298 (0.366) loss_u loss_u 0.9658 (0.9510) acc_u 6.2500 (6.8750) lr 1.9098e-04 eta 0:00:34
epoch [162/200] batch [10/75] time 0.363 (0.486) data 0.231 (0.355) loss_u loss_u 0.9663 (0.9404) acc_u 3.1250 (7.1875) lr 1.9098e-04 eta 0:00:31
epoch [162/200] batch [15/75] time 0.352 (0.484) data 0.220 (0.353) loss_u loss_u 0.9287 (0.9363) acc_u 12.5000 (8.1250) lr 1.9098e-04 eta 0:00:29
epoch [162/200] batch [20/75] time 0.386 (0.477) data 0.254 (0.346) loss_u loss_u 0.9468 (0.9319) acc_u 6.2500 (8.9062) lr 1.9098e-04 eta 0:00:26
epoch [162/200] batch [25/75] time 0.480 (0.479) data 0.349 (0.348) loss_u loss_u 0.9570 (0.9410) acc_u 3.1250 (7.5000) lr 1.9098e-04 eta 0:00:23
epoch [162/200] batch [30/75] time 0.405 (0.475) data 0.274 (0.344) loss_u loss_u 0.8857 (0.9387) acc_u 15.6250 (7.9167) lr 1.9098e-04 eta 0:00:21
epoch [162/200] batch [35/75] time 0.418 (0.473) data 0.287 (0.342) loss_u loss_u 0.8887 (0.9381) acc_u 12.5000 (7.9464) lr 1.9098e-04 eta 0:00:18
epoch [162/200] batch [40/75] time 0.466 (0.469) data 0.334 (0.338) loss_u loss_u 0.9102 (0.9336) acc_u 12.5000 (8.5938) lr 1.9098e-04 eta 0:00:16
epoch [162/200] batch [45/75] time 0.719 (0.472) data 0.587 (0.341) loss_u loss_u 0.8750 (0.9314) acc_u 18.7500 (8.8889) lr 1.9098e-04 eta 0:00:14
epoch [162/200] batch [50/75] time 0.484 (0.472) data 0.352 (0.341) loss_u loss_u 0.9077 (0.9315) acc_u 9.3750 (8.8750) lr 1.9098e-04 eta 0:00:11
epoch [162/200] batch [55/75] time 0.587 (0.472) data 0.455 (0.341) loss_u loss_u 0.9214 (0.9320) acc_u 9.3750 (8.8068) lr 1.9098e-04 eta 0:00:09
epoch [162/200] batch [60/75] time 0.379 (0.471) data 0.247 (0.340) loss_u loss_u 0.9224 (0.9319) acc_u 9.3750 (8.8542) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [65/75] time 0.659 (0.473) data 0.526 (0.342) loss_u loss_u 0.8926 (0.9298) acc_u 15.6250 (9.1827) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [70/75] time 0.394 (0.472) data 0.262 (0.341) loss_u loss_u 0.9858 (0.9308) acc_u 0.0000 (8.8839) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [75/75] time 0.510 (0.471) data 0.379 (0.340) loss_u loss_u 0.9189 (0.9321) acc_u 9.3750 (8.6667) lr 1.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1509
confident_label rate tensor(0.2357, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 739
clean true:727
clean false:12
clean_rate:0.9837618403247632
noisy true:900
noisy false:1497
after delete: len(clean_dataset) 739
after delete: len(noisy_dataset) 2397
epoch [163/200] batch [5/23] time 0.496 (0.488) data 0.365 (0.357) loss_x loss_x 0.7871 (0.8851) acc_x 87.5000 (75.6250) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [10/23] time 0.456 (0.466) data 0.326 (0.335) loss_x loss_x 1.5146 (1.0263) acc_x 62.5000 (73.7500) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [15/23] time 0.346 (0.458) data 0.215 (0.328) loss_x loss_x 1.2646 (1.0787) acc_x 71.8750 (73.1250) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [20/23] time 0.476 (0.451) data 0.343 (0.321) loss_x loss_x 1.3320 (1.1217) acc_x 65.6250 (72.5000) lr 1.8185e-04 eta 0:00:01
epoch [163/200] batch [5/74] time 0.512 (0.454) data 0.378 (0.322) loss_u loss_u 0.9326 (0.9517) acc_u 9.3750 (7.5000) lr 1.8185e-04 eta 0:00:31
epoch [163/200] batch [10/74] time 0.480 (0.457) data 0.347 (0.326) loss_u loss_u 0.8916 (0.9458) acc_u 15.6250 (6.8750) lr 1.8185e-04 eta 0:00:29
epoch [163/200] batch [15/74] time 0.566 (0.464) data 0.434 (0.333) loss_u loss_u 0.8628 (0.9439) acc_u 15.6250 (7.2917) lr 1.8185e-04 eta 0:00:27
epoch [163/200] batch [20/74] time 0.467 (0.461) data 0.336 (0.329) loss_u loss_u 0.9170 (0.9459) acc_u 9.3750 (6.8750) lr 1.8185e-04 eta 0:00:24
epoch [163/200] batch [25/74] time 0.622 (0.461) data 0.491 (0.329) loss_u loss_u 0.9160 (0.9375) acc_u 15.6250 (8.2500) lr 1.8185e-04 eta 0:00:22
epoch [163/200] batch [30/74] time 0.498 (0.466) data 0.366 (0.335) loss_u loss_u 0.9756 (0.9415) acc_u 3.1250 (7.7083) lr 1.8185e-04 eta 0:00:20
epoch [163/200] batch [35/74] time 0.488 (0.465) data 0.357 (0.334) loss_u loss_u 0.9072 (0.9415) acc_u 9.3750 (7.5000) lr 1.8185e-04 eta 0:00:18
epoch [163/200] batch [40/74] time 0.577 (0.465) data 0.446 (0.334) loss_u loss_u 0.9795 (0.9412) acc_u 0.0000 (7.3438) lr 1.8185e-04 eta 0:00:15
epoch [163/200] batch [45/74] time 0.399 (0.468) data 0.269 (0.336) loss_u loss_u 0.8823 (0.9390) acc_u 15.6250 (7.7083) lr 1.8185e-04 eta 0:00:13
epoch [163/200] batch [50/74] time 0.440 (0.468) data 0.308 (0.336) loss_u loss_u 0.9126 (0.9381) acc_u 9.3750 (7.8125) lr 1.8185e-04 eta 0:00:11
epoch [163/200] batch [55/74] time 0.575 (0.472) data 0.443 (0.341) loss_u loss_u 0.9414 (0.9384) acc_u 6.2500 (7.8409) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [60/74] time 0.565 (0.474) data 0.433 (0.343) loss_u loss_u 0.9810 (0.9395) acc_u 3.1250 (7.7083) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [65/74] time 0.395 (0.473) data 0.263 (0.341) loss_u loss_u 0.9297 (0.9380) acc_u 9.3750 (7.9327) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [70/74] time 0.439 (0.473) data 0.307 (0.342) loss_u loss_u 0.9585 (0.9376) acc_u 3.1250 (7.9464) lr 1.8185e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1487
confident_label rate tensor(0.2455, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 770
clean true:758
clean false:12
clean_rate:0.9844155844155844
noisy true:891
noisy false:1475
after delete: len(clean_dataset) 770
after delete: len(noisy_dataset) 2366
epoch [164/200] batch [5/24] time 0.426 (0.411) data 0.293 (0.280) loss_x loss_x 1.2793 (1.1881) acc_x 68.7500 (71.2500) lr 1.7292e-04 eta 0:00:07
epoch [164/200] batch [10/24] time 0.496 (0.461) data 0.365 (0.330) loss_x loss_x 1.1680 (1.1457) acc_x 75.0000 (72.8125) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [15/24] time 0.490 (0.470) data 0.360 (0.339) loss_x loss_x 0.8091 (1.0768) acc_x 71.8750 (73.3333) lr 1.7292e-04 eta 0:00:04
epoch [164/200] batch [20/24] time 0.586 (0.488) data 0.456 (0.357) loss_x loss_x 1.1133 (1.0623) acc_x 75.0000 (74.0625) lr 1.7292e-04 eta 0:00:01
epoch [164/200] batch [5/73] time 0.375 (0.468) data 0.243 (0.337) loss_u loss_u 0.9521 (0.9438) acc_u 9.3750 (8.7500) lr 1.7292e-04 eta 0:00:31
epoch [164/200] batch [10/73] time 0.371 (0.467) data 0.239 (0.336) loss_u loss_u 0.9351 (0.9360) acc_u 6.2500 (8.7500) lr 1.7292e-04 eta 0:00:29
epoch [164/200] batch [15/73] time 0.370 (0.457) data 0.239 (0.326) loss_u loss_u 0.9409 (0.9409) acc_u 6.2500 (7.9167) lr 1.7292e-04 eta 0:00:26
epoch [164/200] batch [20/73] time 0.625 (0.460) data 0.493 (0.329) loss_u loss_u 0.9512 (0.9278) acc_u 6.2500 (9.5312) lr 1.7292e-04 eta 0:00:24
epoch [164/200] batch [25/73] time 0.533 (0.461) data 0.400 (0.330) loss_u loss_u 0.9775 (0.9308) acc_u 6.2500 (9.0000) lr 1.7292e-04 eta 0:00:22
epoch [164/200] batch [30/73] time 0.449 (0.468) data 0.319 (0.336) loss_u loss_u 0.9189 (0.9336) acc_u 9.3750 (8.7500) lr 1.7292e-04 eta 0:00:20
epoch [164/200] batch [35/73] time 0.463 (0.473) data 0.332 (0.342) loss_u loss_u 0.9634 (0.9354) acc_u 6.2500 (8.4821) lr 1.7292e-04 eta 0:00:17
epoch [164/200] batch [40/73] time 0.442 (0.472) data 0.310 (0.341) loss_u loss_u 0.9272 (0.9333) acc_u 9.3750 (8.8281) lr 1.7292e-04 eta 0:00:15
epoch [164/200] batch [45/73] time 0.524 (0.470) data 0.394 (0.339) loss_u loss_u 0.9136 (0.9359) acc_u 9.3750 (8.3333) lr 1.7292e-04 eta 0:00:13
epoch [164/200] batch [50/73] time 0.346 (0.467) data 0.216 (0.335) loss_u loss_u 0.9468 (0.9350) acc_u 9.3750 (8.5000) lr 1.7292e-04 eta 0:00:10
epoch [164/200] batch [55/73] time 0.396 (0.466) data 0.265 (0.334) loss_u loss_u 0.9150 (0.9365) acc_u 9.3750 (8.2955) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [60/73] time 0.440 (0.464) data 0.308 (0.333) loss_u loss_u 0.9434 (0.9366) acc_u 6.2500 (8.2292) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [65/73] time 0.542 (0.463) data 0.412 (0.332) loss_u loss_u 0.9316 (0.9377) acc_u 9.3750 (8.0769) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [70/73] time 0.511 (0.463) data 0.379 (0.332) loss_u loss_u 0.9536 (0.9380) acc_u 6.2500 (8.0357) lr 1.7292e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1477
confident_label rate tensor(0.2353, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 738
clean true:727
clean false:11
clean_rate:0.9850948509485095
noisy true:932
noisy false:1466
after delete: len(clean_dataset) 738
after delete: len(noisy_dataset) 2398
epoch [165/200] batch [5/23] time 0.454 (0.542) data 0.321 (0.409) loss_x loss_x 1.2354 (1.1075) acc_x 75.0000 (71.2500) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [10/23] time 0.383 (0.506) data 0.251 (0.375) loss_x loss_x 0.8779 (1.1086) acc_x 81.2500 (71.2500) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [15/23] time 0.480 (0.478) data 0.349 (0.346) loss_x loss_x 1.2119 (1.1902) acc_x 62.5000 (70.6250) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [20/23] time 0.540 (0.492) data 0.408 (0.361) loss_x loss_x 1.2432 (1.1546) acc_x 75.0000 (71.5625) lr 1.6419e-04 eta 0:00:01
epoch [165/200] batch [5/74] time 0.581 (0.481) data 0.449 (0.349) loss_u loss_u 0.9175 (0.9091) acc_u 9.3750 (10.6250) lr 1.6419e-04 eta 0:00:33
epoch [165/200] batch [10/74] time 0.416 (0.482) data 0.284 (0.351) loss_u loss_u 0.9678 (0.9280) acc_u 3.1250 (9.0625) lr 1.6419e-04 eta 0:00:30
epoch [165/200] batch [15/74] time 0.413 (0.479) data 0.283 (0.348) loss_u loss_u 0.9453 (0.9288) acc_u 9.3750 (9.3750) lr 1.6419e-04 eta 0:00:28
epoch [165/200] batch [20/74] time 0.404 (0.469) data 0.273 (0.337) loss_u loss_u 0.9497 (0.9331) acc_u 6.2500 (8.5938) lr 1.6419e-04 eta 0:00:25
epoch [165/200] batch [25/74] time 0.449 (0.467) data 0.319 (0.336) loss_u loss_u 0.9507 (0.9354) acc_u 3.1250 (8.2500) lr 1.6419e-04 eta 0:00:22
epoch [165/200] batch [30/74] time 0.434 (0.460) data 0.303 (0.328) loss_u loss_u 0.9873 (0.9360) acc_u 3.1250 (8.2292) lr 1.6419e-04 eta 0:00:20
epoch [165/200] batch [35/74] time 0.424 (0.460) data 0.292 (0.329) loss_u loss_u 0.9155 (0.9365) acc_u 9.3750 (8.0357) lr 1.6419e-04 eta 0:00:17
epoch [165/200] batch [40/74] time 0.405 (0.457) data 0.274 (0.325) loss_u loss_u 0.9629 (0.9386) acc_u 6.2500 (7.8125) lr 1.6419e-04 eta 0:00:15
epoch [165/200] batch [45/74] time 0.370 (0.457) data 0.238 (0.325) loss_u loss_u 0.9268 (0.9365) acc_u 12.5000 (8.1944) lr 1.6419e-04 eta 0:00:13
epoch [165/200] batch [50/74] time 0.365 (0.454) data 0.234 (0.323) loss_u loss_u 0.9966 (0.9378) acc_u 0.0000 (8.1875) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [55/74] time 0.428 (0.455) data 0.297 (0.324) loss_u loss_u 0.9619 (0.9368) acc_u 3.1250 (8.3523) lr 1.6419e-04 eta 0:00:08
epoch [165/200] batch [60/74] time 0.448 (0.459) data 0.318 (0.327) loss_u loss_u 0.9136 (0.9359) acc_u 6.2500 (8.2812) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [65/74] time 0.430 (0.458) data 0.299 (0.327) loss_u loss_u 0.8999 (0.9352) acc_u 9.3750 (8.4615) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [70/74] time 0.463 (0.458) data 0.331 (0.327) loss_u loss_u 0.9355 (0.9358) acc_u 12.5000 (8.3929) lr 1.6419e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1487
confident_label rate tensor(0.2443, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 766
clean true:750
clean false:16
clean_rate:0.97911227154047
noisy true:899
noisy false:1471
after delete: len(clean_dataset) 766
after delete: len(noisy_dataset) 2370
epoch [166/200] batch [5/23] time 0.432 (0.451) data 0.301 (0.320) loss_x loss_x 0.8726 (0.9331) acc_x 75.0000 (75.6250) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [10/23] time 0.584 (0.463) data 0.453 (0.332) loss_x loss_x 1.1260 (1.0903) acc_x 78.1250 (73.1250) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [15/23] time 0.489 (0.464) data 0.359 (0.334) loss_x loss_x 1.4004 (1.1278) acc_x 71.8750 (72.9167) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [20/23] time 0.404 (0.459) data 0.274 (0.329) loss_x loss_x 1.2783 (1.1321) acc_x 71.8750 (73.9062) lr 1.5567e-04 eta 0:00:01
epoch [166/200] batch [5/74] time 0.604 (0.469) data 0.469 (0.338) loss_u loss_u 0.9937 (0.9409) acc_u 0.0000 (8.1250) lr 1.5567e-04 eta 0:00:32
epoch [166/200] batch [10/74] time 0.403 (0.477) data 0.272 (0.346) loss_u loss_u 0.9722 (0.9480) acc_u 3.1250 (6.5625) lr 1.5567e-04 eta 0:00:30
epoch [166/200] batch [15/74] time 0.438 (0.473) data 0.307 (0.342) loss_u loss_u 0.9272 (0.9433) acc_u 6.2500 (6.8750) lr 1.5567e-04 eta 0:00:27
epoch [166/200] batch [20/74] time 0.493 (0.468) data 0.361 (0.337) loss_u loss_u 0.9150 (0.9395) acc_u 9.3750 (7.3438) lr 1.5567e-04 eta 0:00:25
epoch [166/200] batch [25/74] time 0.374 (0.464) data 0.243 (0.333) loss_u loss_u 0.9629 (0.9429) acc_u 6.2500 (6.7500) lr 1.5567e-04 eta 0:00:22
epoch [166/200] batch [30/74] time 0.409 (0.457) data 0.278 (0.326) loss_u loss_u 0.9155 (0.9410) acc_u 9.3750 (7.0833) lr 1.5567e-04 eta 0:00:20
epoch [166/200] batch [35/74] time 0.529 (0.455) data 0.397 (0.324) loss_u loss_u 0.9829 (0.9437) acc_u 3.1250 (6.6964) lr 1.5567e-04 eta 0:00:17
epoch [166/200] batch [40/74] time 0.405 (0.454) data 0.274 (0.323) loss_u loss_u 0.9458 (0.9434) acc_u 6.2500 (6.7188) lr 1.5567e-04 eta 0:00:15
epoch [166/200] batch [45/74] time 0.603 (0.456) data 0.471 (0.325) loss_u loss_u 0.9492 (0.9436) acc_u 6.2500 (6.8750) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [50/74] time 0.408 (0.458) data 0.278 (0.327) loss_u loss_u 0.9502 (0.9423) acc_u 9.3750 (7.0625) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [55/74] time 0.395 (0.462) data 0.264 (0.331) loss_u loss_u 0.9028 (0.9424) acc_u 12.5000 (7.1023) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [60/74] time 0.366 (0.461) data 0.235 (0.330) loss_u loss_u 0.9487 (0.9431) acc_u 6.2500 (7.0312) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [65/74] time 0.410 (0.460) data 0.278 (0.329) loss_u loss_u 0.9531 (0.9443) acc_u 6.2500 (6.8750) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [70/74] time 0.476 (0.462) data 0.344 (0.331) loss_u loss_u 0.8809 (0.9438) acc_u 15.6250 (6.9643) lr 1.5567e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1531
confident_label rate tensor(0.2350, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 737
clean true:726
clean false:11
clean_rate:0.9850746268656716
noisy true:879
noisy false:1520
after delete: len(clean_dataset) 737
after delete: len(noisy_dataset) 2399
epoch [167/200] batch [5/23] time 0.380 (0.428) data 0.250 (0.298) loss_x loss_x 1.4932 (1.1042) acc_x 65.6250 (76.8750) lr 1.4736e-04 eta 0:00:07
epoch [167/200] batch [10/23] time 0.587 (0.438) data 0.456 (0.307) loss_x loss_x 1.0010 (1.1749) acc_x 71.8750 (70.9375) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [15/23] time 0.468 (0.448) data 0.338 (0.317) loss_x loss_x 1.3203 (1.1362) acc_x 59.3750 (71.2500) lr 1.4736e-04 eta 0:00:03
epoch [167/200] batch [20/23] time 0.513 (0.465) data 0.383 (0.334) loss_x loss_x 0.8271 (1.1191) acc_x 71.8750 (72.3438) lr 1.4736e-04 eta 0:00:01
epoch [167/200] batch [5/74] time 0.381 (0.452) data 0.249 (0.321) loss_u loss_u 0.9092 (0.9323) acc_u 9.3750 (8.7500) lr 1.4736e-04 eta 0:00:31
epoch [167/200] batch [10/74] time 0.419 (0.454) data 0.287 (0.323) loss_u loss_u 0.9287 (0.9334) acc_u 9.3750 (8.1250) lr 1.4736e-04 eta 0:00:29
epoch [167/200] batch [15/74] time 0.635 (0.456) data 0.502 (0.325) loss_u loss_u 0.9282 (0.9319) acc_u 6.2500 (7.9167) lr 1.4736e-04 eta 0:00:26
epoch [167/200] batch [20/74] time 0.380 (0.455) data 0.247 (0.324) loss_u loss_u 0.8765 (0.9253) acc_u 15.6250 (8.5938) lr 1.4736e-04 eta 0:00:24
epoch [167/200] batch [25/74] time 0.526 (0.454) data 0.395 (0.323) loss_u loss_u 0.9712 (0.9251) acc_u 3.1250 (9.0000) lr 1.4736e-04 eta 0:00:22
epoch [167/200] batch [30/74] time 0.448 (0.454) data 0.317 (0.323) loss_u loss_u 0.9751 (0.9264) acc_u 3.1250 (9.0625) lr 1.4736e-04 eta 0:00:19
epoch [167/200] batch [35/74] time 0.457 (0.458) data 0.326 (0.327) loss_u loss_u 0.9712 (0.9257) acc_u 3.1250 (9.1071) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [40/74] time 0.480 (0.460) data 0.348 (0.329) loss_u loss_u 0.9746 (0.9290) acc_u 3.1250 (8.6719) lr 1.4736e-04 eta 0:00:15
epoch [167/200] batch [45/74] time 0.867 (0.464) data 0.736 (0.333) loss_u loss_u 0.9424 (0.9299) acc_u 6.2500 (8.6111) lr 1.4736e-04 eta 0:00:13
epoch [167/200] batch [50/74] time 0.418 (0.461) data 0.287 (0.330) loss_u loss_u 0.9868 (0.9329) acc_u 3.1250 (8.3125) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [55/74] time 0.449 (0.460) data 0.314 (0.329) loss_u loss_u 0.9590 (0.9347) acc_u 6.2500 (8.1250) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [60/74] time 0.559 (0.463) data 0.428 (0.332) loss_u loss_u 0.8418 (0.9324) acc_u 21.8750 (8.4375) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [65/74] time 0.591 (0.464) data 0.460 (0.333) loss_u loss_u 0.9092 (0.9299) acc_u 12.5000 (8.7981) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [70/74] time 0.410 (0.465) data 0.279 (0.334) loss_u loss_u 0.8936 (0.9304) acc_u 18.7500 (8.8839) lr 1.4736e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1534
confident_label rate tensor(0.2305, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 723
clean true:707
clean false:16
clean_rate:0.9778699861687413
noisy true:895
noisy false:1518
after delete: len(clean_dataset) 723
after delete: len(noisy_dataset) 2413
epoch [168/200] batch [5/22] time 0.372 (0.447) data 0.242 (0.316) loss_x loss_x 1.5010 (1.1760) acc_x 65.6250 (70.0000) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [10/22] time 0.440 (0.470) data 0.310 (0.339) loss_x loss_x 1.3916 (1.1416) acc_x 59.3750 (70.9375) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [15/22] time 0.533 (0.480) data 0.400 (0.349) loss_x loss_x 1.0195 (1.1900) acc_x 68.7500 (70.6250) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [20/22] time 0.427 (0.463) data 0.297 (0.332) loss_x loss_x 1.1357 (1.1645) acc_x 75.0000 (72.8125) lr 1.3926e-04 eta 0:00:00
epoch [168/200] batch [5/75] time 0.503 (0.460) data 0.372 (0.329) loss_u loss_u 0.9062 (0.9389) acc_u 9.3750 (5.6250) lr 1.3926e-04 eta 0:00:32
epoch [168/200] batch [10/75] time 0.420 (0.454) data 0.289 (0.323) loss_u loss_u 0.9893 (0.9462) acc_u 0.0000 (5.9375) lr 1.3926e-04 eta 0:00:29
epoch [168/200] batch [15/75] time 0.469 (0.451) data 0.336 (0.320) loss_u loss_u 0.8979 (0.9461) acc_u 12.5000 (6.2500) lr 1.3926e-04 eta 0:00:27
epoch [168/200] batch [20/75] time 0.407 (0.457) data 0.275 (0.326) loss_u loss_u 0.9180 (0.9412) acc_u 9.3750 (6.5625) lr 1.3926e-04 eta 0:00:25
epoch [168/200] batch [25/75] time 0.437 (0.461) data 0.306 (0.330) loss_u loss_u 0.9399 (0.9440) acc_u 6.2500 (6.2500) lr 1.3926e-04 eta 0:00:23
epoch [168/200] batch [30/75] time 0.431 (0.455) data 0.299 (0.324) loss_u loss_u 0.8906 (0.9413) acc_u 18.7500 (6.9792) lr 1.3926e-04 eta 0:00:20
epoch [168/200] batch [35/75] time 0.430 (0.457) data 0.299 (0.326) loss_u loss_u 0.9155 (0.9406) acc_u 9.3750 (7.2321) lr 1.3926e-04 eta 0:00:18
epoch [168/200] batch [40/75] time 0.428 (0.458) data 0.297 (0.327) loss_u loss_u 0.9399 (0.9394) acc_u 6.2500 (7.4219) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [45/75] time 0.560 (0.458) data 0.428 (0.327) loss_u loss_u 0.9062 (0.9383) acc_u 12.5000 (7.7083) lr 1.3926e-04 eta 0:00:13
epoch [168/200] batch [50/75] time 0.436 (0.460) data 0.304 (0.328) loss_u loss_u 0.9468 (0.9384) acc_u 6.2500 (7.7500) lr 1.3926e-04 eta 0:00:11
epoch [168/200] batch [55/75] time 0.546 (0.462) data 0.413 (0.331) loss_u loss_u 0.9834 (0.9380) acc_u 6.2500 (7.8977) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [60/75] time 0.401 (0.462) data 0.270 (0.331) loss_u loss_u 0.8643 (0.9376) acc_u 15.6250 (7.9688) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [65/75] time 0.427 (0.460) data 0.293 (0.329) loss_u loss_u 0.9780 (0.9368) acc_u 3.1250 (8.0769) lr 1.3926e-04 eta 0:00:04
epoch [168/200] batch [70/75] time 0.422 (0.460) data 0.290 (0.329) loss_u loss_u 0.8965 (0.9370) acc_u 15.6250 (8.1696) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [75/75] time 0.414 (0.461) data 0.284 (0.329) loss_u loss_u 0.9014 (0.9352) acc_u 15.6250 (8.4167) lr 1.3926e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1449
confident_label rate tensor(0.2446, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 767
clean true:755
clean false:12
clean_rate:0.984354628422425
noisy true:932
noisy false:1437
after delete: len(clean_dataset) 767
after delete: len(noisy_dataset) 2369
epoch [169/200] batch [5/23] time 0.546 (0.445) data 0.414 (0.315) loss_x loss_x 0.8618 (0.9331) acc_x 84.3750 (77.5000) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [10/23] time 0.378 (0.468) data 0.247 (0.338) loss_x loss_x 1.3291 (0.9667) acc_x 65.6250 (76.2500) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [15/23] time 0.449 (0.470) data 0.318 (0.339) loss_x loss_x 0.8838 (0.9617) acc_x 81.2500 (77.0833) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [20/23] time 0.371 (0.477) data 0.241 (0.346) loss_x loss_x 1.2422 (1.0120) acc_x 71.8750 (76.0938) lr 1.3137e-04 eta 0:00:01
epoch [169/200] batch [5/74] time 0.443 (0.471) data 0.310 (0.340) loss_u loss_u 0.9321 (0.9563) acc_u 9.3750 (4.3750) lr 1.3137e-04 eta 0:00:32
epoch [169/200] batch [10/74] time 0.448 (0.477) data 0.316 (0.346) loss_u loss_u 0.9307 (0.9487) acc_u 6.2500 (5.6250) lr 1.3137e-04 eta 0:00:30
epoch [169/200] batch [15/74] time 0.694 (0.492) data 0.563 (0.361) loss_u loss_u 0.9761 (0.9331) acc_u 3.1250 (7.7083) lr 1.3137e-04 eta 0:00:29
epoch [169/200] batch [20/74] time 0.444 (0.488) data 0.312 (0.357) loss_u loss_u 0.9692 (0.9352) acc_u 3.1250 (7.6562) lr 1.3137e-04 eta 0:00:26
epoch [169/200] batch [25/74] time 0.559 (0.492) data 0.427 (0.361) loss_u loss_u 0.9502 (0.9386) acc_u 6.2500 (7.6250) lr 1.3137e-04 eta 0:00:24
epoch [169/200] batch [30/74] time 0.346 (0.489) data 0.214 (0.358) loss_u loss_u 0.9766 (0.9401) acc_u 0.0000 (7.2917) lr 1.3137e-04 eta 0:00:21
epoch [169/200] batch [35/74] time 0.317 (0.482) data 0.185 (0.351) loss_u loss_u 0.9824 (0.9403) acc_u 3.1250 (7.3214) lr 1.3137e-04 eta 0:00:18
epoch [169/200] batch [40/74] time 0.462 (0.478) data 0.330 (0.346) loss_u loss_u 0.9507 (0.9423) acc_u 6.2500 (7.1875) lr 1.3137e-04 eta 0:00:16
epoch [169/200] batch [45/74] time 0.389 (0.479) data 0.256 (0.347) loss_u loss_u 0.9395 (0.9427) acc_u 9.3750 (7.2917) lr 1.3137e-04 eta 0:00:13
epoch [169/200] batch [50/74] time 0.494 (0.478) data 0.361 (0.346) loss_u loss_u 0.8872 (0.9411) acc_u 12.5000 (7.5000) lr 1.3137e-04 eta 0:00:11
epoch [169/200] batch [55/74] time 0.368 (0.473) data 0.236 (0.341) loss_u loss_u 0.9336 (0.9421) acc_u 9.3750 (7.3864) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [60/74] time 0.557 (0.475) data 0.426 (0.344) loss_u loss_u 0.9414 (0.9411) acc_u 12.5000 (7.6562) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [65/74] time 0.485 (0.478) data 0.353 (0.346) loss_u loss_u 0.9351 (0.9401) acc_u 9.3750 (7.6923) lr 1.3137e-04 eta 0:00:04
epoch [169/200] batch [70/74] time 0.347 (0.475) data 0.215 (0.344) loss_u loss_u 0.9595 (0.9400) acc_u 6.2500 (7.7232) lr 1.3137e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1504
confident_label rate tensor(0.2439, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 765
clean true:750
clean false:15
clean_rate:0.9803921568627451
noisy true:882
noisy false:1489
after delete: len(clean_dataset) 765
after delete: len(noisy_dataset) 2371
epoch [170/200] batch [5/23] time 0.519 (0.539) data 0.388 (0.408) loss_x loss_x 1.1445 (0.8945) acc_x 78.1250 (76.8750) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [10/23] time 0.439 (0.500) data 0.308 (0.370) loss_x loss_x 1.4443 (1.0914) acc_x 62.5000 (74.3750) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [15/23] time 0.493 (0.492) data 0.362 (0.361) loss_x loss_x 1.4219 (1.1410) acc_x 59.3750 (72.5000) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [20/23] time 0.426 (0.475) data 0.295 (0.344) loss_x loss_x 1.2725 (1.1660) acc_x 71.8750 (72.5000) lr 1.2369e-04 eta 0:00:01
epoch [170/200] batch [5/74] time 0.400 (0.480) data 0.269 (0.349) loss_u loss_u 0.9141 (0.9458) acc_u 9.3750 (6.8750) lr 1.2369e-04 eta 0:00:33
epoch [170/200] batch [10/74] time 0.476 (0.473) data 0.343 (0.342) loss_u loss_u 0.9507 (0.9430) acc_u 6.2500 (7.5000) lr 1.2369e-04 eta 0:00:30
epoch [170/200] batch [15/74] time 0.566 (0.473) data 0.434 (0.342) loss_u loss_u 0.9341 (0.9441) acc_u 6.2500 (7.2917) lr 1.2369e-04 eta 0:00:27
epoch [170/200] batch [20/74] time 0.557 (0.475) data 0.425 (0.344) loss_u loss_u 0.9951 (0.9441) acc_u 0.0000 (7.3438) lr 1.2369e-04 eta 0:00:25
epoch [170/200] batch [25/74] time 0.497 (0.476) data 0.365 (0.345) loss_u loss_u 0.8945 (0.9401) acc_u 12.5000 (7.8750) lr 1.2369e-04 eta 0:00:23
epoch [170/200] batch [30/74] time 0.483 (0.470) data 0.350 (0.338) loss_u loss_u 0.9360 (0.9414) acc_u 9.3750 (7.9167) lr 1.2369e-04 eta 0:00:20
epoch [170/200] batch [35/74] time 0.520 (0.471) data 0.389 (0.339) loss_u loss_u 0.9336 (0.9425) acc_u 6.2500 (7.5000) lr 1.2369e-04 eta 0:00:18
epoch [170/200] batch [40/74] time 0.477 (0.475) data 0.346 (0.343) loss_u loss_u 0.9585 (0.9427) acc_u 3.1250 (7.3438) lr 1.2369e-04 eta 0:00:16
epoch [170/200] batch [45/74] time 0.383 (0.474) data 0.253 (0.342) loss_u loss_u 0.9600 (0.9438) acc_u 6.2500 (7.2917) lr 1.2369e-04 eta 0:00:13
epoch [170/200] batch [50/74] time 0.576 (0.475) data 0.444 (0.343) loss_u loss_u 0.9468 (0.9455) acc_u 9.3750 (7.0625) lr 1.2369e-04 eta 0:00:11
epoch [170/200] batch [55/74] time 0.420 (0.475) data 0.289 (0.344) loss_u loss_u 0.9526 (0.9460) acc_u 6.2500 (6.9886) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [60/74] time 0.455 (0.474) data 0.322 (0.342) loss_u loss_u 0.9214 (0.9448) acc_u 9.3750 (7.0833) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [65/74] time 0.446 (0.471) data 0.315 (0.339) loss_u loss_u 0.9741 (0.9466) acc_u 3.1250 (6.8750) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [70/74] time 0.376 (0.475) data 0.245 (0.344) loss_u loss_u 0.9609 (0.9453) acc_u 3.1250 (7.0536) lr 1.2369e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1519
confident_label rate tensor(0.2423, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 760
clean true:747
clean false:13
clean_rate:0.9828947368421053
noisy true:870
noisy false:1506
after delete: len(clean_dataset) 760
after delete: len(noisy_dataset) 2376
epoch [171/200] batch [5/23] time 0.491 (0.535) data 0.361 (0.404) loss_x loss_x 1.4395 (1.1619) acc_x 65.6250 (70.6250) lr 1.1623e-04 eta 0:00:09
epoch [171/200] batch [10/23] time 0.410 (0.506) data 0.278 (0.374) loss_x loss_x 0.9268 (1.0151) acc_x 81.2500 (75.3125) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [15/23] time 0.603 (0.498) data 0.472 (0.366) loss_x loss_x 1.5938 (1.0146) acc_x 68.7500 (75.8333) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [20/23] time 0.531 (0.499) data 0.399 (0.368) loss_x loss_x 1.0576 (0.9990) acc_x 78.1250 (76.8750) lr 1.1623e-04 eta 0:00:01
epoch [171/200] batch [5/74] time 0.498 (0.496) data 0.365 (0.364) loss_u loss_u 0.9526 (0.9015) acc_u 9.3750 (13.1250) lr 1.1623e-04 eta 0:00:34
epoch [171/200] batch [10/74] time 0.441 (0.485) data 0.308 (0.353) loss_u loss_u 0.9189 (0.9261) acc_u 9.3750 (8.7500) lr 1.1623e-04 eta 0:00:31
epoch [171/200] batch [15/74] time 0.597 (0.486) data 0.464 (0.355) loss_u loss_u 0.9551 (0.9258) acc_u 6.2500 (8.7500) lr 1.1623e-04 eta 0:00:28
epoch [171/200] batch [20/74] time 0.527 (0.483) data 0.396 (0.351) loss_u loss_u 0.9966 (0.9317) acc_u 0.0000 (8.4375) lr 1.1623e-04 eta 0:00:26
epoch [171/200] batch [25/74] time 0.379 (0.480) data 0.247 (0.348) loss_u loss_u 0.9810 (0.9304) acc_u 3.1250 (8.7500) lr 1.1623e-04 eta 0:00:23
epoch [171/200] batch [30/74] time 0.497 (0.478) data 0.363 (0.346) loss_u loss_u 0.9434 (0.9291) acc_u 6.2500 (8.7500) lr 1.1623e-04 eta 0:00:21
epoch [171/200] batch [35/74] time 0.410 (0.470) data 0.278 (0.338) loss_u loss_u 0.9692 (0.9274) acc_u 3.1250 (9.0179) lr 1.1623e-04 eta 0:00:18
epoch [171/200] batch [40/74] time 0.378 (0.470) data 0.246 (0.338) loss_u loss_u 0.8896 (0.9282) acc_u 12.5000 (8.9062) lr 1.1623e-04 eta 0:00:15
epoch [171/200] batch [45/74] time 0.414 (0.465) data 0.282 (0.333) loss_u loss_u 0.9595 (0.9290) acc_u 6.2500 (8.8194) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [50/74] time 0.510 (0.463) data 0.379 (0.332) loss_u loss_u 0.8926 (0.9317) acc_u 15.6250 (8.5625) lr 1.1623e-04 eta 0:00:11
epoch [171/200] batch [55/74] time 0.551 (0.465) data 0.420 (0.333) loss_u loss_u 0.9229 (0.9320) acc_u 6.2500 (8.4659) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [60/74] time 0.488 (0.467) data 0.356 (0.335) loss_u loss_u 0.9609 (0.9327) acc_u 3.1250 (8.3333) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [65/74] time 0.438 (0.466) data 0.306 (0.334) loss_u loss_u 0.9829 (0.9344) acc_u 3.1250 (8.2212) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [70/74] time 0.553 (0.469) data 0.421 (0.337) loss_u loss_u 0.8774 (0.9346) acc_u 15.6250 (8.1696) lr 1.1623e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1522
confident_label rate tensor(0.2411, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 756
clean true:741
clean false:15
clean_rate:0.9801587301587301
noisy true:873
noisy false:1507
after delete: len(clean_dataset) 756
after delete: len(noisy_dataset) 2380
epoch [172/200] batch [5/23] time 0.834 (0.647) data 0.560 (0.375) loss_x loss_x 1.1309 (1.2305) acc_x 62.5000 (67.5000) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [10/23] time 0.657 (0.629) data 0.384 (0.373) loss_x loss_x 0.6836 (1.0500) acc_x 84.3750 (70.9375) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [15/23] time 0.589 (0.615) data 0.316 (0.355) loss_x loss_x 1.3975 (1.0697) acc_x 62.5000 (71.8750) lr 1.0899e-04 eta 0:00:04
epoch [172/200] batch [20/23] time 0.565 (0.617) data 0.303 (0.362) loss_x loss_x 0.9424 (1.0371) acc_x 78.1250 (72.6562) lr 1.0899e-04 eta 0:00:01
epoch [172/200] batch [5/74] time 0.539 (0.604) data 0.266 (0.346) loss_u loss_u 0.9116 (0.9256) acc_u 6.2500 (10.0000) lr 1.0899e-04 eta 0:00:41
epoch [172/200] batch [10/74] time 0.534 (0.610) data 0.261 (0.349) loss_u loss_u 0.9355 (0.9287) acc_u 6.2500 (9.0625) lr 1.0899e-04 eta 0:00:39
epoch [172/200] batch [15/74] time 0.519 (0.608) data 0.245 (0.345) loss_u loss_u 0.9404 (0.9325) acc_u 12.5000 (8.9583) lr 1.0899e-04 eta 0:00:35
epoch [172/200] batch [20/74] time 0.618 (0.606) data 0.343 (0.342) loss_u loss_u 0.9229 (0.9325) acc_u 12.5000 (9.0625) lr 1.0899e-04 eta 0:00:32
epoch [172/200] batch [25/74] time 0.559 (0.605) data 0.307 (0.341) loss_u loss_u 0.8564 (0.9280) acc_u 18.7500 (9.6250) lr 1.0899e-04 eta 0:00:29
epoch [172/200] batch [30/74] time 0.599 (0.603) data 0.324 (0.339) loss_u loss_u 0.9707 (0.9218) acc_u 3.1250 (10.1042) lr 1.0899e-04 eta 0:00:26
epoch [172/200] batch [35/74] time 0.750 (0.607) data 0.483 (0.343) loss_u loss_u 0.9414 (0.9274) acc_u 9.3750 (9.2857) lr 1.0899e-04 eta 0:00:23
epoch [172/200] batch [40/74] time 0.558 (0.611) data 0.287 (0.348) loss_u loss_u 0.8789 (0.9263) acc_u 12.5000 (9.3750) lr 1.0899e-04 eta 0:00:20
epoch [172/200] batch [45/74] time 0.553 (0.612) data 0.287 (0.349) loss_u loss_u 0.9395 (0.9274) acc_u 9.3750 (9.2361) lr 1.0899e-04 eta 0:00:17
epoch [172/200] batch [50/74] time 0.517 (0.611) data 0.241 (0.349) loss_u loss_u 0.9077 (0.9266) acc_u 9.3750 (9.3125) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [55/74] time 0.619 (0.610) data 0.346 (0.347) loss_u loss_u 0.9189 (0.9282) acc_u 12.5000 (9.0909) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [60/74] time 0.436 (0.608) data 0.304 (0.346) loss_u loss_u 0.9331 (0.9298) acc_u 6.2500 (8.8021) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [65/74] time 0.569 (0.606) data 0.296 (0.343) loss_u loss_u 0.9082 (0.9301) acc_u 12.5000 (8.8942) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [70/74] time 0.593 (0.608) data 0.324 (0.345) loss_u loss_u 0.8848 (0.9309) acc_u 15.6250 (8.7500) lr 1.0899e-04 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1497
confident_label rate tensor(0.2401, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 753
clean true:739
clean false:14
clean_rate:0.9814077025232404
noisy true:900
noisy false:1483
after delete: len(clean_dataset) 753
after delete: len(noisy_dataset) 2383
epoch [173/200] batch [5/23] time 0.551 (0.620) data 0.284 (0.377) loss_x loss_x 1.5850 (1.1259) acc_x 59.3750 (72.5000) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [10/23] time 0.693 (0.634) data 0.432 (0.379) loss_x loss_x 1.0137 (1.1463) acc_x 75.0000 (71.2500) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [15/23] time 0.593 (0.636) data 0.322 (0.376) loss_x loss_x 1.1094 (1.1159) acc_x 71.8750 (72.2917) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [20/23] time 0.543 (0.653) data 0.275 (0.391) loss_x loss_x 1.0176 (1.1168) acc_x 75.0000 (72.8125) lr 1.0197e-04 eta 0:00:01
epoch [173/200] batch [5/74] time 0.609 (0.639) data 0.330 (0.379) loss_u loss_u 0.9277 (0.9607) acc_u 9.3750 (6.2500) lr 1.0197e-04 eta 0:00:44
epoch [173/200] batch [10/74] time 0.755 (0.634) data 0.488 (0.373) loss_u loss_u 0.9785 (0.9447) acc_u 3.1250 (6.8750) lr 1.0197e-04 eta 0:00:40
epoch [173/200] batch [15/74] time 0.579 (0.631) data 0.316 (0.368) loss_u loss_u 0.9458 (0.9521) acc_u 6.2500 (5.8333) lr 1.0197e-04 eta 0:00:37
epoch [173/200] batch [20/74] time 0.591 (0.624) data 0.325 (0.364) loss_u loss_u 0.9644 (0.9531) acc_u 6.2500 (5.6250) lr 1.0197e-04 eta 0:00:33
epoch [173/200] batch [25/74] time 0.588 (0.624) data 0.318 (0.363) loss_u loss_u 0.9146 (0.9523) acc_u 12.5000 (5.7500) lr 1.0197e-04 eta 0:00:30
epoch [173/200] batch [30/74] time 0.635 (0.619) data 0.367 (0.357) loss_u loss_u 0.8940 (0.9513) acc_u 12.5000 (6.1458) lr 1.0197e-04 eta 0:00:27
epoch [173/200] batch [35/74] time 0.658 (0.615) data 0.381 (0.354) loss_u loss_u 0.9858 (0.9476) acc_u 3.1250 (6.7857) lr 1.0197e-04 eta 0:00:23
epoch [173/200] batch [40/74] time 0.539 (0.615) data 0.261 (0.353) loss_u loss_u 0.9932 (0.9441) acc_u 0.0000 (7.3438) lr 1.0197e-04 eta 0:00:20
epoch [173/200] batch [45/74] time 0.644 (0.612) data 0.365 (0.351) loss_u loss_u 0.9082 (0.9420) acc_u 12.5000 (7.5694) lr 1.0197e-04 eta 0:00:17
epoch [173/200] batch [50/74] time 0.666 (0.613) data 0.400 (0.352) loss_u loss_u 0.9546 (0.9408) acc_u 6.2500 (7.6875) lr 1.0197e-04 eta 0:00:14
epoch [173/200] batch [55/74] time 0.825 (0.617) data 0.544 (0.355) loss_u loss_u 0.9453 (0.9408) acc_u 6.2500 (7.5568) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [60/74] time 0.534 (0.612) data 0.268 (0.351) loss_u loss_u 0.9233 (0.9403) acc_u 9.3750 (7.6562) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [65/74] time 0.506 (0.609) data 0.231 (0.348) loss_u loss_u 0.9136 (0.9386) acc_u 9.3750 (7.7885) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [70/74] time 0.480 (0.606) data 0.338 (0.345) loss_u loss_u 0.9634 (0.9379) acc_u 6.2500 (7.9018) lr 1.0197e-04 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1508
confident_label rate tensor(0.2337, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 733
clean true:717
clean false:16
clean_rate:0.9781718963165075
noisy true:911
noisy false:1492
after delete: len(clean_dataset) 733
after delete: len(noisy_dataset) 2403
epoch [174/200] batch [5/22] time 0.585 (0.625) data 0.310 (0.352) loss_x loss_x 1.0361 (0.9726) acc_x 81.2500 (80.0000) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [10/22] time 0.597 (0.616) data 0.326 (0.347) loss_x loss_x 1.0625 (1.0317) acc_x 71.8750 (75.9375) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [15/22] time 0.509 (0.568) data 0.272 (0.310) loss_x loss_x 1.0176 (1.1036) acc_x 75.0000 (73.3333) lr 9.5173e-05 eta 0:00:03
epoch [174/200] batch [20/22] time 0.649 (0.609) data 0.379 (0.347) loss_x loss_x 1.2812 (1.1115) acc_x 68.7500 (73.1250) lr 9.5173e-05 eta 0:00:01
epoch [174/200] batch [5/75] time 0.760 (0.609) data 0.490 (0.349) loss_u loss_u 0.9780 (0.9417) acc_u 3.1250 (8.1250) lr 9.5173e-05 eta 0:00:42
epoch [174/200] batch [10/75] time 0.698 (0.614) data 0.435 (0.352) loss_u loss_u 0.9556 (0.9464) acc_u 9.3750 (7.5000) lr 9.5173e-05 eta 0:00:39
epoch [174/200] batch [15/75] time 0.559 (0.607) data 0.279 (0.343) loss_u loss_u 0.8848 (0.9420) acc_u 12.5000 (7.5000) lr 9.5173e-05 eta 0:00:36
epoch [174/200] batch [20/75] time 0.566 (0.602) data 0.286 (0.340) loss_u loss_u 0.8691 (0.9354) acc_u 15.6250 (8.4375) lr 9.5173e-05 eta 0:00:33
epoch [174/200] batch [25/75] time 0.571 (0.602) data 0.299 (0.339) loss_u loss_u 0.9082 (0.9369) acc_u 9.3750 (8.3750) lr 9.5173e-05 eta 0:00:30
epoch [174/200] batch [30/75] time 0.486 (0.600) data 0.213 (0.339) loss_u loss_u 0.9111 (0.9374) acc_u 12.5000 (8.3333) lr 9.5173e-05 eta 0:00:27
epoch [174/200] batch [35/75] time 0.645 (0.600) data 0.371 (0.338) loss_u loss_u 0.9282 (0.9390) acc_u 9.3750 (8.0357) lr 9.5173e-05 eta 0:00:24
epoch [174/200] batch [40/75] time 0.635 (0.604) data 0.355 (0.341) loss_u loss_u 0.9644 (0.9376) acc_u 6.2500 (8.2812) lr 9.5173e-05 eta 0:00:21
epoch [174/200] batch [45/75] time 0.586 (0.601) data 0.311 (0.337) loss_u loss_u 0.9082 (0.9372) acc_u 12.5000 (8.2639) lr 9.5173e-05 eta 0:00:18
epoch [174/200] batch [50/75] time 0.530 (0.601) data 0.273 (0.336) loss_u loss_u 0.9058 (0.9358) acc_u 12.5000 (8.4375) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [55/75] time 0.552 (0.598) data 0.277 (0.334) loss_u loss_u 0.9033 (0.9319) acc_u 15.6250 (9.1477) lr 9.5173e-05 eta 0:00:11
epoch [174/200] batch [60/75] time 0.705 (0.599) data 0.431 (0.334) loss_u loss_u 0.9272 (0.9311) acc_u 9.3750 (9.1146) lr 9.5173e-05 eta 0:00:08
epoch [174/200] batch [65/75] time 0.652 (0.599) data 0.378 (0.334) loss_u loss_u 0.8984 (0.9321) acc_u 12.5000 (8.9423) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [70/75] time 0.620 (0.601) data 0.350 (0.337) loss_u loss_u 0.8911 (0.9308) acc_u 12.5000 (9.0625) lr 9.5173e-05 eta 0:00:03
epoch [174/200] batch [75/75] time 0.585 (0.601) data 0.336 (0.337) loss_u loss_u 0.9600 (0.9321) acc_u 3.1250 (8.8750) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1512
confident_label rate tensor(0.2337, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 733
clean true:725
clean false:8
clean_rate:0.9890859481582538
noisy true:899
noisy false:1504
after delete: len(clean_dataset) 733
after delete: len(noisy_dataset) 2403
epoch [175/200] batch [5/22] time 0.635 (0.637) data 0.361 (0.360) loss_x loss_x 0.9839 (1.0014) acc_x 65.6250 (74.3750) lr 8.8597e-05 eta 0:00:10
epoch [175/200] batch [10/22] time 0.667 (0.648) data 0.388 (0.375) loss_x loss_x 0.7881 (1.0741) acc_x 75.0000 (73.1250) lr 8.8597e-05 eta 0:00:07
epoch [175/200] batch [15/22] time 0.593 (0.638) data 0.331 (0.367) loss_x loss_x 1.0449 (1.0482) acc_x 71.8750 (73.7500) lr 8.8597e-05 eta 0:00:04
epoch [175/200] batch [20/22] time 0.530 (0.636) data 0.263 (0.372) loss_x loss_x 0.8760 (1.0173) acc_x 81.2500 (75.6250) lr 8.8597e-05 eta 0:00:01
epoch [175/200] batch [5/75] time 0.654 (0.633) data 0.389 (0.373) loss_u loss_u 0.8931 (0.9195) acc_u 9.3750 (9.3750) lr 8.8597e-05 eta 0:00:44
epoch [175/200] batch [10/75] time 0.612 (0.632) data 0.335 (0.370) loss_u loss_u 0.9463 (0.9231) acc_u 6.2500 (8.7500) lr 8.8597e-05 eta 0:00:41
epoch [175/200] batch [15/75] time 0.711 (0.630) data 0.443 (0.367) loss_u loss_u 0.9995 (0.9288) acc_u 0.0000 (7.9167) lr 8.8597e-05 eta 0:00:37
epoch [175/200] batch [20/75] time 0.747 (0.632) data 0.480 (0.369) loss_u loss_u 0.9199 (0.9297) acc_u 12.5000 (7.8125) lr 8.8597e-05 eta 0:00:34
epoch [175/200] batch [25/75] time 0.682 (0.632) data 0.417 (0.370) loss_u loss_u 0.9004 (0.9269) acc_u 12.5000 (8.3750) lr 8.8597e-05 eta 0:00:31
epoch [175/200] batch [30/75] time 0.602 (0.632) data 0.331 (0.369) loss_u loss_u 0.9531 (0.9263) acc_u 3.1250 (8.5417) lr 8.8597e-05 eta 0:00:28
epoch [175/200] batch [35/75] time 0.553 (0.629) data 0.285 (0.365) loss_u loss_u 0.9526 (0.9267) acc_u 6.2500 (8.6607) lr 8.8597e-05 eta 0:00:25
epoch [175/200] batch [40/75] time 0.632 (0.626) data 0.365 (0.362) loss_u loss_u 0.9375 (0.9304) acc_u 6.2500 (8.2031) lr 8.8597e-05 eta 0:00:21
epoch [175/200] batch [45/75] time 0.638 (0.624) data 0.362 (0.359) loss_u loss_u 0.9995 (0.9331) acc_u 0.0000 (7.9167) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [50/75] time 0.595 (0.620) data 0.320 (0.355) loss_u loss_u 0.9487 (0.9322) acc_u 6.2500 (8.0000) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [55/75] time 0.636 (0.621) data 0.368 (0.355) loss_u loss_u 0.8447 (0.9292) acc_u 18.7500 (8.5795) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [60/75] time 0.620 (0.619) data 0.353 (0.355) loss_u loss_u 0.9458 (0.9277) acc_u 6.2500 (8.9062) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [65/75] time 0.605 (0.619) data 0.339 (0.354) loss_u loss_u 0.9316 (0.9277) acc_u 9.3750 (9.0865) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [70/75] time 0.575 (0.616) data 0.306 (0.352) loss_u loss_u 0.9092 (0.9278) acc_u 9.3750 (9.0179) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [75/75] time 0.679 (0.616) data 0.408 (0.351) loss_u loss_u 0.9468 (0.9295) acc_u 6.2500 (8.7083) lr 8.8597e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1524
confident_label rate tensor(0.2423, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 760
clean true:744
clean false:16
clean_rate:0.9789473684210527
noisy true:868
noisy false:1508
after delete: len(clean_dataset) 760
after delete: len(noisy_dataset) 2376
epoch [176/200] batch [5/23] time 0.556 (0.601) data 0.284 (0.332) loss_x loss_x 1.1777 (1.1629) acc_x 75.0000 (71.8750) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [10/23] time 0.708 (0.595) data 0.471 (0.329) loss_x loss_x 0.8340 (1.1463) acc_x 75.0000 (72.5000) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [15/23] time 0.629 (0.598) data 0.366 (0.331) loss_x loss_x 0.6626 (1.0608) acc_x 84.3750 (74.1667) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [20/23] time 0.647 (0.617) data 0.382 (0.351) loss_x loss_x 0.5996 (1.0558) acc_x 84.3750 (74.5312) lr 8.2245e-05 eta 0:00:01
epoch [176/200] batch [5/74] time 0.687 (0.620) data 0.422 (0.355) loss_u loss_u 0.9551 (0.9292) acc_u 6.2500 (9.3750) lr 8.2245e-05 eta 0:00:42
epoch [176/200] batch [10/74] time 0.648 (0.628) data 0.387 (0.363) loss_u loss_u 0.8892 (0.9306) acc_u 12.5000 (9.3750) lr 8.2245e-05 eta 0:00:40
epoch [176/200] batch [15/74] time 0.500 (0.623) data 0.370 (0.362) loss_u loss_u 0.9424 (0.9364) acc_u 6.2500 (8.3333) lr 8.2245e-05 eta 0:00:36
epoch [176/200] batch [20/74] time 0.521 (0.623) data 0.256 (0.360) loss_u loss_u 0.8887 (0.9313) acc_u 15.6250 (9.0625) lr 8.2245e-05 eta 0:00:33
epoch [176/200] batch [25/74] time 0.548 (0.617) data 0.285 (0.353) loss_u loss_u 0.9014 (0.9289) acc_u 12.5000 (9.3750) lr 8.2245e-05 eta 0:00:30
epoch [176/200] batch [30/74] time 0.477 (0.610) data 0.201 (0.347) loss_u loss_u 0.9766 (0.9332) acc_u 3.1250 (8.7500) lr 8.2245e-05 eta 0:00:26
epoch [176/200] batch [35/74] time 0.646 (0.608) data 0.365 (0.345) loss_u loss_u 0.9429 (0.9367) acc_u 6.2500 (8.1250) lr 8.2245e-05 eta 0:00:23
epoch [176/200] batch [40/74] time 0.617 (0.605) data 0.346 (0.341) loss_u loss_u 0.9517 (0.9378) acc_u 9.3750 (7.8906) lr 8.2245e-05 eta 0:00:20
epoch [176/200] batch [45/74] time 0.487 (0.600) data 0.224 (0.337) loss_u loss_u 0.9600 (0.9405) acc_u 3.1250 (7.4306) lr 8.2245e-05 eta 0:00:17
epoch [176/200] batch [50/74] time 0.571 (0.600) data 0.298 (0.337) loss_u loss_u 0.9673 (0.9394) acc_u 3.1250 (7.6250) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [55/74] time 0.536 (0.600) data 0.406 (0.339) loss_u loss_u 0.9092 (0.9392) acc_u 12.5000 (7.6136) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [60/74] time 0.551 (0.599) data 0.274 (0.337) loss_u loss_u 0.9141 (0.9384) acc_u 12.5000 (7.6562) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [65/74] time 0.597 (0.601) data 0.329 (0.338) loss_u loss_u 0.9531 (0.9401) acc_u 9.3750 (7.5962) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [70/74] time 0.642 (0.603) data 0.372 (0.340) loss_u loss_u 0.9927 (0.9396) acc_u 0.0000 (7.5000) lr 8.2245e-05 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1488
confident_label rate tensor(0.2436, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 764
clean true:749
clean false:15
clean_rate:0.9803664921465969
noisy true:899
noisy false:1473
after delete: len(clean_dataset) 764
after delete: len(noisy_dataset) 2372
epoch [177/200] batch [5/23] time 0.459 (0.571) data 0.328 (0.333) loss_x loss_x 1.0576 (0.9806) acc_x 78.1250 (78.1250) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [10/23] time 0.698 (0.657) data 0.432 (0.403) loss_x loss_x 0.9370 (1.0182) acc_x 78.1250 (75.3125) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [15/23] time 0.580 (0.654) data 0.366 (0.398) loss_x loss_x 1.0859 (1.0291) acc_x 62.5000 (75.0000) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [20/23] time 0.581 (0.634) data 0.306 (0.381) loss_x loss_x 0.6465 (1.0222) acc_x 71.8750 (74.3750) lr 7.6120e-05 eta 0:00:01
epoch [177/200] batch [5/74] time 0.599 (0.618) data 0.324 (0.364) loss_u loss_u 0.9463 (0.9480) acc_u 6.2500 (6.2500) lr 7.6120e-05 eta 0:00:42
epoch [177/200] batch [10/74] time 0.589 (0.611) data 0.312 (0.354) loss_u loss_u 0.9390 (0.9444) acc_u 3.1250 (5.6250) lr 7.6120e-05 eta 0:00:39
epoch [177/200] batch [15/74] time 0.701 (0.604) data 0.428 (0.348) loss_u loss_u 0.9141 (0.9373) acc_u 9.3750 (6.2500) lr 7.6120e-05 eta 0:00:35
epoch [177/200] batch [20/74] time 0.597 (0.601) data 0.313 (0.343) loss_u loss_u 0.9414 (0.9320) acc_u 6.2500 (7.5000) lr 7.6120e-05 eta 0:00:32
epoch [177/200] batch [25/74] time 0.808 (0.608) data 0.530 (0.348) loss_u loss_u 0.9082 (0.9328) acc_u 15.6250 (7.7500) lr 7.6120e-05 eta 0:00:29
epoch [177/200] batch [30/74] time 0.548 (0.604) data 0.270 (0.344) loss_u loss_u 0.9619 (0.9319) acc_u 3.1250 (8.0208) lr 7.6120e-05 eta 0:00:26
epoch [177/200] batch [35/74] time 0.677 (0.600) data 0.416 (0.339) loss_u loss_u 0.9634 (0.9351) acc_u 6.2500 (7.8571) lr 7.6120e-05 eta 0:00:23
epoch [177/200] batch [40/74] time 0.463 (0.597) data 0.331 (0.337) loss_u loss_u 0.9302 (0.9374) acc_u 12.5000 (7.8125) lr 7.6120e-05 eta 0:00:20
epoch [177/200] batch [45/74] time 0.626 (0.601) data 0.357 (0.340) loss_u loss_u 0.9585 (0.9351) acc_u 9.3750 (8.1250) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [50/74] time 0.536 (0.602) data 0.265 (0.340) loss_u loss_u 0.8765 (0.9341) acc_u 15.6250 (8.2500) lr 7.6120e-05 eta 0:00:14
epoch [177/200] batch [55/74] time 0.660 (0.599) data 0.389 (0.339) loss_u loss_u 0.9390 (0.9317) acc_u 6.2500 (8.4659) lr 7.6120e-05 eta 0:00:11
epoch [177/200] batch [60/74] time 0.786 (0.603) data 0.521 (0.343) loss_u loss_u 0.9517 (0.9336) acc_u 9.3750 (8.2812) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [65/74] time 0.604 (0.604) data 0.342 (0.344) loss_u loss_u 0.9458 (0.9336) acc_u 6.2500 (8.3173) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [70/74] time 0.560 (0.603) data 0.429 (0.344) loss_u loss_u 0.9570 (0.9342) acc_u 6.2500 (8.2143) lr 7.6120e-05 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1487
confident_label rate tensor(0.2385, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 748
clean true:739
clean false:9
clean_rate:0.9879679144385026
noisy true:910
noisy false:1478
after delete: len(clean_dataset) 748
after delete: len(noisy_dataset) 2388
epoch [178/200] batch [5/23] time 0.646 (0.763) data 0.375 (0.490) loss_x loss_x 0.9165 (1.2116) acc_x 81.2500 (77.5000) lr 7.0224e-05 eta 0:00:13
epoch [178/200] batch [10/23] time 0.556 (0.682) data 0.277 (0.409) loss_x loss_x 1.3564 (1.1403) acc_x 68.7500 (76.5625) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [15/23] time 0.660 (0.649) data 0.388 (0.385) loss_x loss_x 1.4053 (1.1442) acc_x 71.8750 (75.8333) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [20/23] time 0.648 (0.643) data 0.381 (0.378) loss_x loss_x 0.9204 (1.1271) acc_x 68.7500 (75.6250) lr 7.0224e-05 eta 0:00:01
epoch [178/200] batch [5/74] time 0.548 (0.636) data 0.274 (0.369) loss_u loss_u 0.9619 (0.9540) acc_u 6.2500 (6.2500) lr 7.0224e-05 eta 0:00:43
epoch [178/200] batch [10/74] time 0.547 (0.628) data 0.282 (0.361) loss_u loss_u 0.9482 (0.9345) acc_u 9.3750 (8.7500) lr 7.0224e-05 eta 0:00:40
epoch [178/200] batch [15/74] time 0.669 (0.623) data 0.394 (0.356) loss_u loss_u 0.9727 (0.9266) acc_u 6.2500 (9.5833) lr 7.0224e-05 eta 0:00:36
epoch [178/200] batch [20/74] time 0.570 (0.617) data 0.292 (0.349) loss_u loss_u 0.9175 (0.9286) acc_u 12.5000 (9.3750) lr 7.0224e-05 eta 0:00:33
epoch [178/200] batch [25/74] time 0.575 (0.611) data 0.285 (0.345) loss_u loss_u 0.8560 (0.9292) acc_u 15.6250 (9.2500) lr 7.0224e-05 eta 0:00:29
epoch [178/200] batch [30/74] time 0.766 (0.610) data 0.493 (0.343) loss_u loss_u 0.9902 (0.9303) acc_u 0.0000 (9.0625) lr 7.0224e-05 eta 0:00:26
epoch [178/200] batch [35/74] time 0.620 (0.607) data 0.350 (0.342) loss_u loss_u 0.8628 (0.9272) acc_u 15.6250 (9.4643) lr 7.0224e-05 eta 0:00:23
epoch [178/200] batch [40/74] time 0.813 (0.607) data 0.534 (0.342) loss_u loss_u 0.9658 (0.9312) acc_u 6.2500 (9.0625) lr 7.0224e-05 eta 0:00:20
epoch [178/200] batch [45/74] time 0.622 (0.604) data 0.351 (0.338) loss_u loss_u 0.9355 (0.9323) acc_u 6.2500 (8.7500) lr 7.0224e-05 eta 0:00:17
epoch [178/200] batch [50/74] time 0.559 (0.603) data 0.286 (0.337) loss_u loss_u 0.9482 (0.9340) acc_u 6.2500 (8.6875) lr 7.0224e-05 eta 0:00:14
epoch [178/200] batch [55/74] time 0.657 (0.602) data 0.393 (0.338) loss_u loss_u 0.9619 (0.9357) acc_u 3.1250 (8.3523) lr 7.0224e-05 eta 0:00:11
epoch [178/200] batch [60/74] time 0.600 (0.604) data 0.324 (0.339) loss_u loss_u 0.9531 (0.9373) acc_u 6.2500 (8.1250) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [65/74] time 0.618 (0.603) data 0.340 (0.337) loss_u loss_u 0.9492 (0.9371) acc_u 12.5000 (8.3654) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [70/74] time 0.590 (0.602) data 0.318 (0.337) loss_u loss_u 0.9351 (0.9372) acc_u 6.2500 (8.2589) lr 7.0224e-05 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1468
confident_label rate tensor(0.2423, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 760
clean true:749
clean false:11
clean_rate:0.9855263157894737
noisy true:919
noisy false:1457
after delete: len(clean_dataset) 760
after delete: len(noisy_dataset) 2376
epoch [179/200] batch [5/23] time 0.559 (0.589) data 0.287 (0.317) loss_x loss_x 1.3867 (1.0699) acc_x 59.3750 (72.5000) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [10/23] time 0.725 (0.624) data 0.456 (0.353) loss_x loss_x 0.8887 (1.0615) acc_x 81.2500 (73.1250) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [15/23] time 0.360 (0.595) data 0.226 (0.342) loss_x loss_x 0.9385 (1.0387) acc_x 81.2500 (74.7917) lr 6.4556e-05 eta 0:00:04
epoch [179/200] batch [20/23] time 0.574 (0.608) data 0.316 (0.351) loss_x loss_x 0.9019 (1.0720) acc_x 81.2500 (74.2188) lr 6.4556e-05 eta 0:00:01
epoch [179/200] batch [5/74] time 0.511 (0.590) data 0.239 (0.335) loss_u loss_u 0.9214 (0.9301) acc_u 12.5000 (8.1250) lr 6.4556e-05 eta 0:00:40
epoch [179/200] batch [10/74] time 0.700 (0.596) data 0.425 (0.338) loss_u loss_u 0.9321 (0.9335) acc_u 9.3750 (8.1250) lr 6.4556e-05 eta 0:00:38
epoch [179/200] batch [15/74] time 0.527 (0.598) data 0.260 (0.338) loss_u loss_u 0.9160 (0.9292) acc_u 9.3750 (8.5417) lr 6.4556e-05 eta 0:00:35
epoch [179/200] batch [20/74] time 0.568 (0.594) data 0.275 (0.335) loss_u loss_u 0.8804 (0.9295) acc_u 12.5000 (8.5938) lr 6.4556e-05 eta 0:00:32
epoch [179/200] batch [25/74] time 0.788 (0.600) data 0.524 (0.340) loss_u loss_u 0.9067 (0.9327) acc_u 12.5000 (8.1250) lr 6.4556e-05 eta 0:00:29
epoch [179/200] batch [30/74] time 0.573 (0.598) data 0.398 (0.339) loss_u loss_u 0.9482 (0.9332) acc_u 6.2500 (8.0208) lr 6.4556e-05 eta 0:00:26
epoch [179/200] batch [35/74] time 0.536 (0.595) data 0.263 (0.334) loss_u loss_u 0.8906 (0.9328) acc_u 12.5000 (8.1250) lr 6.4556e-05 eta 0:00:23
epoch [179/200] batch [40/74] time 0.583 (0.593) data 0.313 (0.331) loss_u loss_u 0.9131 (0.9315) acc_u 9.3750 (8.2812) lr 6.4556e-05 eta 0:00:20
epoch [179/200] batch [45/74] time 0.509 (0.590) data 0.238 (0.330) loss_u loss_u 0.9204 (0.9317) acc_u 9.3750 (8.2639) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [50/74] time 0.625 (0.592) data 0.361 (0.331) loss_u loss_u 0.9536 (0.9352) acc_u 3.1250 (7.8125) lr 6.4556e-05 eta 0:00:14
epoch [179/200] batch [55/74] time 0.625 (0.593) data 0.361 (0.333) loss_u loss_u 0.9150 (0.9370) acc_u 12.5000 (7.5568) lr 6.4556e-05 eta 0:00:11
epoch [179/200] batch [60/74] time 0.595 (0.592) data 0.325 (0.332) loss_u loss_u 0.8369 (0.9353) acc_u 18.7500 (7.7604) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [65/74] time 0.516 (0.595) data 0.250 (0.334) loss_u loss_u 0.9121 (0.9366) acc_u 12.5000 (7.6923) lr 6.4556e-05 eta 0:00:05
epoch [179/200] batch [70/74] time 0.586 (0.597) data 0.308 (0.336) loss_u loss_u 0.9126 (0.9358) acc_u 9.3750 (7.8125) lr 6.4556e-05 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1525
confident_label rate tensor(0.2321, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 728
clean true:713
clean false:15
clean_rate:0.9793956043956044
noisy true:898
noisy false:1510
after delete: len(clean_dataset) 728
after delete: len(noisy_dataset) 2408
epoch [180/200] batch [5/22] time 0.604 (0.607) data 0.334 (0.337) loss_x loss_x 0.9844 (0.9427) acc_x 71.8750 (77.5000) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [10/22] time 0.921 (0.624) data 0.652 (0.355) loss_x loss_x 1.6377 (1.0920) acc_x 68.7500 (75.0000) lr 5.9119e-05 eta 0:00:07
epoch [180/200] batch [15/22] time 0.484 (0.623) data 0.218 (0.364) loss_x loss_x 1.0615 (1.0783) acc_x 68.7500 (75.2083) lr 5.9119e-05 eta 0:00:04
epoch [180/200] batch [20/22] time 0.583 (0.620) data 0.314 (0.358) loss_x loss_x 0.6553 (1.0277) acc_x 78.1250 (75.9375) lr 5.9119e-05 eta 0:00:01
epoch [180/200] batch [5/75] time 0.796 (0.631) data 0.524 (0.372) loss_u loss_u 0.9194 (0.9219) acc_u 9.3750 (8.7500) lr 5.9119e-05 eta 0:00:44
epoch [180/200] batch [10/75] time 0.431 (0.618) data 0.235 (0.360) loss_u loss_u 0.8379 (0.9177) acc_u 21.8750 (9.6875) lr 5.9119e-05 eta 0:00:40
epoch [180/200] batch [15/75] time 0.781 (0.623) data 0.520 (0.363) loss_u loss_u 0.9316 (0.9248) acc_u 9.3750 (9.7917) lr 5.9119e-05 eta 0:00:37
epoch [180/200] batch [20/75] time 0.619 (0.617) data 0.346 (0.356) loss_u loss_u 0.9307 (0.9304) acc_u 9.3750 (9.0625) lr 5.9119e-05 eta 0:00:33
epoch [180/200] batch [25/75] time 0.633 (0.614) data 0.362 (0.355) loss_u loss_u 0.9326 (0.9292) acc_u 9.3750 (8.8750) lr 5.9119e-05 eta 0:00:30
epoch [180/200] batch [30/75] time 0.744 (0.618) data 0.475 (0.357) loss_u loss_u 0.8916 (0.9297) acc_u 18.7500 (8.8542) lr 5.9119e-05 eta 0:00:27
epoch [180/200] batch [35/75] time 0.594 (0.614) data 0.324 (0.355) loss_u loss_u 0.9097 (0.9343) acc_u 9.3750 (8.2143) lr 5.9119e-05 eta 0:00:24
epoch [180/200] batch [40/75] time 0.588 (0.613) data 0.313 (0.353) loss_u loss_u 0.9629 (0.9329) acc_u 3.1250 (8.4375) lr 5.9119e-05 eta 0:00:21
epoch [180/200] batch [45/75] time 0.636 (0.612) data 0.364 (0.352) loss_u loss_u 0.9854 (0.9356) acc_u 3.1250 (8.3333) lr 5.9119e-05 eta 0:00:18
epoch [180/200] batch [50/75] time 0.513 (0.610) data 0.246 (0.350) loss_u loss_u 0.9771 (0.9345) acc_u 3.1250 (8.4375) lr 5.9119e-05 eta 0:00:15
epoch [180/200] batch [55/75] time 0.450 (0.606) data 0.182 (0.346) loss_u loss_u 0.9561 (0.9361) acc_u 6.2500 (8.1818) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [60/75] time 0.543 (0.606) data 0.269 (0.346) loss_u loss_u 0.8882 (0.9344) acc_u 12.5000 (8.2812) lr 5.9119e-05 eta 0:00:09
epoch [180/200] batch [65/75] time 0.660 (0.602) data 0.357 (0.341) loss_u loss_u 0.9321 (0.9338) acc_u 9.3750 (8.3173) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [70/75] time 0.590 (0.599) data 0.323 (0.337) loss_u loss_u 0.9414 (0.9349) acc_u 6.2500 (8.2143) lr 5.9119e-05 eta 0:00:02
epoch [180/200] batch [75/75] time 0.545 (0.599) data 0.271 (0.337) loss_u loss_u 0.9243 (0.9346) acc_u 12.5000 (8.2917) lr 5.9119e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.2353, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 738
clean true:730
clean false:8
clean_rate:0.989159891598916
noisy true:889
noisy false:1509
after delete: len(clean_dataset) 738
after delete: len(noisy_dataset) 2398
epoch [181/200] batch [5/23] time 0.522 (0.559) data 0.255 (0.291) loss_x loss_x 0.9717 (1.0479) acc_x 78.1250 (75.0000) lr 5.3915e-05 eta 0:00:10
epoch [181/200] batch [10/23] time 0.590 (0.641) data 0.323 (0.372) loss_x loss_x 1.1846 (1.0391) acc_x 68.7500 (73.7500) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [15/23] time 0.571 (0.640) data 0.307 (0.371) loss_x loss_x 1.1270 (1.0464) acc_x 75.0000 (74.7917) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [20/23] time 0.577 (0.619) data 0.308 (0.355) loss_x loss_x 0.7622 (0.9844) acc_x 84.3750 (76.5625) lr 5.3915e-05 eta 0:00:01
epoch [181/200] batch [5/74] time 0.602 (0.619) data 0.324 (0.357) loss_u loss_u 0.9634 (0.9366) acc_u 3.1250 (7.5000) lr 5.3915e-05 eta 0:00:42
epoch [181/200] batch [10/74] time 0.643 (0.625) data 0.373 (0.362) loss_u loss_u 0.9365 (0.9368) acc_u 9.3750 (7.8125) lr 5.3915e-05 eta 0:00:40
epoch [181/200] batch [15/74] time 0.507 (0.620) data 0.234 (0.356) loss_u loss_u 0.9067 (0.9337) acc_u 12.5000 (7.9167) lr 5.3915e-05 eta 0:00:36
epoch [181/200] batch [20/74] time 0.581 (0.612) data 0.306 (0.349) loss_u loss_u 0.9512 (0.9334) acc_u 6.2500 (8.1250) lr 5.3915e-05 eta 0:00:33
epoch [181/200] batch [25/74] time 0.815 (0.616) data 0.549 (0.353) loss_u loss_u 0.9424 (0.9296) acc_u 9.3750 (8.7500) lr 5.3915e-05 eta 0:00:30
epoch [181/200] batch [30/74] time 0.488 (0.609) data 0.355 (0.348) loss_u loss_u 0.9258 (0.9297) acc_u 6.2500 (8.6458) lr 5.3915e-05 eta 0:00:26
epoch [181/200] batch [35/74] time 0.596 (0.609) data 0.331 (0.347) loss_u loss_u 0.9160 (0.9267) acc_u 12.5000 (9.2857) lr 5.3915e-05 eta 0:00:23
epoch [181/200] batch [40/74] time 0.424 (0.611) data 0.254 (0.349) loss_u loss_u 0.9395 (0.9261) acc_u 6.2500 (9.3750) lr 5.3915e-05 eta 0:00:20
epoch [181/200] batch [45/74] time 0.523 (0.607) data 0.256 (0.345) loss_u loss_u 0.8730 (0.9258) acc_u 15.6250 (9.5139) lr 5.3915e-05 eta 0:00:17
epoch [181/200] batch [50/74] time 0.532 (0.605) data 0.259 (0.342) loss_u loss_u 0.9731 (0.9274) acc_u 6.2500 (9.3750) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [55/74] time 0.495 (0.602) data 0.224 (0.339) loss_u loss_u 0.9438 (0.9294) acc_u 6.2500 (9.1477) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [60/74] time 0.534 (0.599) data 0.270 (0.336) loss_u loss_u 0.9531 (0.9310) acc_u 6.2500 (9.0104) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [65/74] time 0.477 (0.596) data 0.208 (0.333) loss_u loss_u 0.9043 (0.9318) acc_u 12.5000 (8.8942) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [70/74] time 0.534 (0.597) data 0.349 (0.335) loss_u loss_u 0.9800 (0.9324) acc_u 3.1250 (8.7946) lr 5.3915e-05 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1484
confident_label rate tensor(0.2449, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 768
clean true:753
clean false:15
clean_rate:0.98046875
noisy true:899
noisy false:1469
after delete: len(clean_dataset) 768
after delete: len(noisy_dataset) 2368
epoch [182/200] batch [5/24] time 0.519 (0.633) data 0.248 (0.360) loss_x loss_x 1.3281 (1.1980) acc_x 65.6250 (71.8750) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [10/24] time 0.423 (0.611) data 0.292 (0.355) loss_x loss_x 0.8672 (1.1167) acc_x 78.1250 (74.3750) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [15/24] time 0.589 (0.612) data 0.317 (0.352) loss_x loss_x 0.6479 (1.0547) acc_x 87.5000 (76.2500) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [20/24] time 0.620 (0.616) data 0.350 (0.355) loss_x loss_x 0.8398 (1.0486) acc_x 75.0000 (75.1562) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [5/74] time 0.420 (0.621) data 0.289 (0.362) loss_u loss_u 0.8813 (0.9462) acc_u 15.6250 (6.8750) lr 4.8943e-05 eta 0:00:42
epoch [182/200] batch [10/74] time 0.684 (0.619) data 0.404 (0.358) loss_u loss_u 0.9595 (0.9451) acc_u 3.1250 (6.5625) lr 4.8943e-05 eta 0:00:39
epoch [182/200] batch [15/74] time 0.606 (0.618) data 0.332 (0.355) loss_u loss_u 0.9468 (0.9476) acc_u 9.3750 (6.6667) lr 4.8943e-05 eta 0:00:36
epoch [182/200] batch [20/74] time 0.554 (0.616) data 0.274 (0.352) loss_u loss_u 0.9858 (0.9481) acc_u 0.0000 (6.4062) lr 4.8943e-05 eta 0:00:33
epoch [182/200] batch [25/74] time 0.420 (0.608) data 0.288 (0.346) loss_u loss_u 0.9390 (0.9413) acc_u 6.2500 (7.1250) lr 4.8943e-05 eta 0:00:29
epoch [182/200] batch [30/74] time 0.544 (0.602) data 0.267 (0.339) loss_u loss_u 0.9829 (0.9429) acc_u 3.1250 (7.0833) lr 4.8943e-05 eta 0:00:26
epoch [182/200] batch [35/74] time 0.508 (0.600) data 0.232 (0.336) loss_u loss_u 0.9746 (0.9430) acc_u 3.1250 (7.0536) lr 4.8943e-05 eta 0:00:23
epoch [182/200] batch [40/74] time 0.424 (0.596) data 0.292 (0.334) loss_u loss_u 0.9619 (0.9442) acc_u 3.1250 (6.8750) lr 4.8943e-05 eta 0:00:20
epoch [182/200] batch [45/74] time 0.519 (0.596) data 0.241 (0.333) loss_u loss_u 0.9736 (0.9470) acc_u 3.1250 (6.5278) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [50/74] time 0.551 (0.596) data 0.289 (0.333) loss_u loss_u 0.9463 (0.9467) acc_u 6.2500 (6.5000) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [55/74] time 0.502 (0.591) data 0.237 (0.329) loss_u loss_u 0.9229 (0.9460) acc_u 12.5000 (6.8182) lr 4.8943e-05 eta 0:00:11
epoch [182/200] batch [60/74] time 0.516 (0.588) data 0.248 (0.325) loss_u loss_u 0.7964 (0.9436) acc_u 25.0000 (7.1354) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [65/74] time 0.457 (0.586) data 0.325 (0.324) loss_u loss_u 0.9941 (0.9447) acc_u 0.0000 (6.8750) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [70/74] time 0.546 (0.585) data 0.274 (0.323) loss_u loss_u 0.9697 (0.9436) acc_u 3.1250 (7.0536) lr 4.8943e-05 eta 0:00:02
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1465
confident_label rate tensor(0.2433, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 763
clean true:750
clean false:13
clean_rate:0.9829619921363041
noisy true:921
noisy false:1452
after delete: len(clean_dataset) 763
after delete: len(noisy_dataset) 2373
epoch [183/200] batch [5/23] time 0.540 (0.465) data 0.409 (0.334) loss_x loss_x 1.1680 (1.1836) acc_x 71.8750 (71.8750) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [10/23] time 0.526 (0.469) data 0.394 (0.338) loss_x loss_x 0.8701 (1.0803) acc_x 75.0000 (75.0000) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [15/23] time 0.515 (0.472) data 0.383 (0.342) loss_x loss_x 0.7114 (1.0522) acc_x 87.5000 (76.0417) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [20/23] time 0.870 (0.496) data 0.739 (0.365) loss_x loss_x 0.9653 (1.0264) acc_x 75.0000 (76.5625) lr 4.4207e-05 eta 0:00:01
epoch [183/200] batch [5/74] time 0.511 (0.499) data 0.379 (0.368) loss_u loss_u 0.9766 (0.9339) acc_u 3.1250 (8.1250) lr 4.4207e-05 eta 0:00:34
epoch [183/200] batch [10/74] time 0.505 (0.495) data 0.373 (0.364) loss_u loss_u 0.8682 (0.9407) acc_u 15.6250 (7.1875) lr 4.4207e-05 eta 0:00:31
epoch [183/200] batch [15/74] time 0.557 (0.491) data 0.427 (0.360) loss_u loss_u 0.9331 (0.9455) acc_u 9.3750 (6.6667) lr 4.4207e-05 eta 0:00:28
epoch [183/200] batch [20/74] time 0.394 (0.486) data 0.263 (0.355) loss_u loss_u 0.9355 (0.9468) acc_u 6.2500 (6.2500) lr 4.4207e-05 eta 0:00:26
epoch [183/200] batch [25/74] time 0.498 (0.482) data 0.367 (0.351) loss_u loss_u 0.9355 (0.9459) acc_u 9.3750 (6.6250) lr 4.4207e-05 eta 0:00:23
epoch [183/200] batch [30/74] time 0.564 (0.481) data 0.433 (0.350) loss_u loss_u 0.9849 (0.9461) acc_u 0.0000 (6.4583) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [35/74] time 0.423 (0.477) data 0.293 (0.345) loss_u loss_u 0.8647 (0.9414) acc_u 12.5000 (7.0536) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [40/74] time 0.433 (0.476) data 0.301 (0.344) loss_u loss_u 0.8599 (0.9375) acc_u 15.6250 (7.4219) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [45/74] time 0.374 (0.470) data 0.244 (0.339) loss_u loss_u 0.9492 (0.9374) acc_u 6.2500 (7.2917) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [50/74] time 0.532 (0.470) data 0.402 (0.339) loss_u loss_u 0.8936 (0.9349) acc_u 12.5000 (7.6875) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [55/74] time 0.458 (0.469) data 0.326 (0.338) loss_u loss_u 0.9805 (0.9360) acc_u 3.1250 (7.5000) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [60/74] time 0.450 (0.470) data 0.319 (0.339) loss_u loss_u 0.9399 (0.9365) acc_u 6.2500 (7.4479) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [65/74] time 0.500 (0.467) data 0.369 (0.336) loss_u loss_u 0.9678 (0.9363) acc_u 3.1250 (7.5481) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [70/74] time 0.512 (0.470) data 0.381 (0.338) loss_u loss_u 0.9243 (0.9369) acc_u 12.5000 (7.4554) lr 4.4207e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1508
confident_label rate tensor(0.2404, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 754
clean true:736
clean false:18
clean_rate:0.9761273209549072
noisy true:892
noisy false:1490
after delete: len(clean_dataset) 754
after delete: len(noisy_dataset) 2382
epoch [184/200] batch [5/23] time 0.645 (0.525) data 0.514 (0.394) loss_x loss_x 0.9458 (1.0438) acc_x 75.0000 (75.6250) lr 3.9706e-05 eta 0:00:09
epoch [184/200] batch [10/23] time 0.611 (0.514) data 0.480 (0.383) loss_x loss_x 1.2129 (1.0593) acc_x 65.6250 (75.6250) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [15/23] time 0.437 (0.478) data 0.307 (0.348) loss_x loss_x 0.8125 (1.1102) acc_x 75.0000 (73.5417) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [20/23] time 0.408 (0.487) data 0.278 (0.356) loss_x loss_x 1.0986 (1.0828) acc_x 71.8750 (73.9062) lr 3.9706e-05 eta 0:00:01
epoch [184/200] batch [5/74] time 0.419 (0.482) data 0.289 (0.352) loss_u loss_u 0.8867 (0.9556) acc_u 18.7500 (6.2500) lr 3.9706e-05 eta 0:00:33
epoch [184/200] batch [10/74] time 0.446 (0.479) data 0.313 (0.348) loss_u loss_u 0.9565 (0.9497) acc_u 6.2500 (6.5625) lr 3.9706e-05 eta 0:00:30
epoch [184/200] batch [15/74] time 0.376 (0.480) data 0.245 (0.350) loss_u loss_u 0.9111 (0.9438) acc_u 12.5000 (7.5000) lr 3.9706e-05 eta 0:00:28
epoch [184/200] batch [20/74] time 0.455 (0.473) data 0.324 (0.343) loss_u loss_u 0.8618 (0.9365) acc_u 15.6250 (8.1250) lr 3.9706e-05 eta 0:00:25
epoch [184/200] batch [25/74] time 0.336 (0.473) data 0.205 (0.342) loss_u loss_u 0.9741 (0.9393) acc_u 9.3750 (7.8750) lr 3.9706e-05 eta 0:00:23
epoch [184/200] batch [30/74] time 0.384 (0.471) data 0.253 (0.340) loss_u loss_u 0.9438 (0.9326) acc_u 6.2500 (8.5417) lr 3.9706e-05 eta 0:00:20
epoch [184/200] batch [35/74] time 0.356 (0.465) data 0.223 (0.334) loss_u loss_u 0.9521 (0.9315) acc_u 9.3750 (8.9286) lr 3.9706e-05 eta 0:00:18
epoch [184/200] batch [40/74] time 0.410 (0.461) data 0.280 (0.330) loss_u loss_u 0.9746 (0.9339) acc_u 0.0000 (8.6719) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [45/74] time 0.572 (0.462) data 0.441 (0.331) loss_u loss_u 0.9375 (0.9352) acc_u 6.2500 (8.4722) lr 3.9706e-05 eta 0:00:13
epoch [184/200] batch [50/74] time 0.622 (0.463) data 0.491 (0.332) loss_u loss_u 0.9937 (0.9370) acc_u 0.0000 (8.1875) lr 3.9706e-05 eta 0:00:11
epoch [184/200] batch [55/74] time 0.532 (0.463) data 0.399 (0.331) loss_u loss_u 0.8804 (0.9342) acc_u 15.6250 (8.5795) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [60/74] time 0.429 (0.461) data 0.298 (0.329) loss_u loss_u 0.9380 (0.9364) acc_u 6.2500 (8.2292) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [65/74] time 0.452 (0.457) data 0.321 (0.326) loss_u loss_u 0.9775 (0.9366) acc_u 3.1250 (8.1731) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [70/74] time 0.379 (0.456) data 0.247 (0.325) loss_u loss_u 0.9507 (0.9375) acc_u 6.2500 (7.9911) lr 3.9706e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1473
confident_label rate tensor(0.2443, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 766
clean true:755
clean false:11
clean_rate:0.9856396866840731
noisy true:908
noisy false:1462
after delete: len(clean_dataset) 766
after delete: len(noisy_dataset) 2370
epoch [185/200] batch [5/23] time 0.540 (0.464) data 0.408 (0.334) loss_x loss_x 1.0205 (1.1045) acc_x 78.1250 (72.5000) lr 3.5443e-05 eta 0:00:08
epoch [185/200] batch [10/23] time 0.458 (0.488) data 0.328 (0.357) loss_x loss_x 1.2852 (1.0632) acc_x 59.3750 (73.4375) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [15/23] time 0.355 (0.477) data 0.224 (0.347) loss_x loss_x 0.9448 (1.0511) acc_x 65.6250 (72.7083) lr 3.5443e-05 eta 0:00:03
epoch [185/200] batch [20/23] time 0.562 (0.481) data 0.432 (0.350) loss_x loss_x 0.8032 (1.0469) acc_x 84.3750 (73.5938) lr 3.5443e-05 eta 0:00:01
epoch [185/200] batch [5/74] time 0.458 (0.464) data 0.327 (0.333) loss_u loss_u 0.9287 (0.9474) acc_u 9.3750 (6.2500) lr 3.5443e-05 eta 0:00:32
epoch [185/200] batch [10/74] time 0.425 (0.460) data 0.293 (0.329) loss_u loss_u 0.9438 (0.9429) acc_u 6.2500 (6.8750) lr 3.5443e-05 eta 0:00:29
epoch [185/200] batch [15/74] time 0.415 (0.463) data 0.284 (0.332) loss_u loss_u 0.9805 (0.9454) acc_u 0.0000 (6.0417) lr 3.5443e-05 eta 0:00:27
epoch [185/200] batch [20/74] time 0.598 (0.465) data 0.467 (0.334) loss_u loss_u 0.9434 (0.9478) acc_u 9.3750 (5.9375) lr 3.5443e-05 eta 0:00:25
epoch [185/200] batch [25/74] time 0.395 (0.457) data 0.264 (0.326) loss_u loss_u 0.9746 (0.9443) acc_u 3.1250 (6.3750) lr 3.5443e-05 eta 0:00:22
epoch [185/200] batch [30/74] time 0.422 (0.452) data 0.291 (0.321) loss_u loss_u 0.9644 (0.9425) acc_u 6.2500 (6.7708) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [35/74] time 0.367 (0.447) data 0.235 (0.316) loss_u loss_u 0.9141 (0.9408) acc_u 12.5000 (6.8750) lr 3.5443e-05 eta 0:00:17
epoch [185/200] batch [40/74] time 0.428 (0.448) data 0.297 (0.317) loss_u loss_u 0.9189 (0.9414) acc_u 9.3750 (6.7969) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [45/74] time 0.409 (0.447) data 0.278 (0.316) loss_u loss_u 0.9609 (0.9413) acc_u 6.2500 (7.0139) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [50/74] time 0.424 (0.449) data 0.292 (0.317) loss_u loss_u 0.9302 (0.9399) acc_u 6.2500 (7.1875) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [55/74] time 0.440 (0.449) data 0.308 (0.318) loss_u loss_u 0.8901 (0.9408) acc_u 15.6250 (7.1591) lr 3.5443e-05 eta 0:00:08
epoch [185/200] batch [60/74] time 0.554 (0.454) data 0.423 (0.323) loss_u loss_u 0.9551 (0.9413) acc_u 6.2500 (7.1354) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [65/74] time 0.374 (0.457) data 0.242 (0.325) loss_u loss_u 0.9536 (0.9423) acc_u 6.2500 (7.0192) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [70/74] time 0.339 (0.454) data 0.208 (0.323) loss_u loss_u 0.9531 (0.9426) acc_u 6.2500 (7.0089) lr 3.5443e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1494
confident_label rate tensor(0.2385, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 748
clean true:733
clean false:15
clean_rate:0.9799465240641712
noisy true:909
noisy false:1479
after delete: len(clean_dataset) 748
after delete: len(noisy_dataset) 2388
epoch [186/200] batch [5/23] time 0.428 (0.464) data 0.297 (0.334) loss_x loss_x 1.6631 (1.1055) acc_x 68.7500 (73.7500) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [10/23] time 0.438 (0.474) data 0.308 (0.344) loss_x loss_x 1.5557 (1.1469) acc_x 65.6250 (71.8750) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [15/23] time 0.472 (0.473) data 0.342 (0.342) loss_x loss_x 1.2236 (1.1104) acc_x 78.1250 (72.5000) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [20/23] time 0.496 (0.471) data 0.366 (0.341) loss_x loss_x 1.7080 (1.1176) acc_x 53.1250 (72.8125) lr 3.1417e-05 eta 0:00:01
epoch [186/200] batch [5/74] time 0.388 (0.468) data 0.257 (0.337) loss_u loss_u 0.9033 (0.9216) acc_u 9.3750 (10.6250) lr 3.1417e-05 eta 0:00:32
epoch [186/200] batch [10/74] time 0.453 (0.475) data 0.320 (0.344) loss_u loss_u 0.8960 (0.9335) acc_u 12.5000 (8.7500) lr 3.1417e-05 eta 0:00:30
epoch [186/200] batch [15/74] time 0.517 (0.472) data 0.385 (0.341) loss_u loss_u 0.9277 (0.9298) acc_u 9.3750 (9.1667) lr 3.1417e-05 eta 0:00:27
epoch [186/200] batch [20/74] time 0.462 (0.462) data 0.332 (0.331) loss_u loss_u 0.9204 (0.9317) acc_u 9.3750 (8.5938) lr 3.1417e-05 eta 0:00:24
epoch [186/200] batch [25/74] time 0.468 (0.461) data 0.337 (0.330) loss_u loss_u 0.9355 (0.9319) acc_u 9.3750 (8.7500) lr 3.1417e-05 eta 0:00:22
epoch [186/200] batch [30/74] time 0.534 (0.460) data 0.403 (0.329) loss_u loss_u 0.9551 (0.9315) acc_u 6.2500 (8.6458) lr 3.1417e-05 eta 0:00:20
epoch [186/200] batch [35/74] time 0.492 (0.461) data 0.361 (0.330) loss_u loss_u 0.9355 (0.9329) acc_u 9.3750 (8.6607) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [40/74] time 0.380 (0.457) data 0.249 (0.326) loss_u loss_u 0.9683 (0.9337) acc_u 3.1250 (8.5938) lr 3.1417e-05 eta 0:00:15
epoch [186/200] batch [45/74] time 0.425 (0.457) data 0.294 (0.326) loss_u loss_u 0.8926 (0.9334) acc_u 15.6250 (8.7500) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [50/74] time 0.620 (0.458) data 0.489 (0.327) loss_u loss_u 0.9351 (0.9354) acc_u 9.3750 (8.5000) lr 3.1417e-05 eta 0:00:10
epoch [186/200] batch [55/74] time 0.432 (0.461) data 0.301 (0.330) loss_u loss_u 0.8901 (0.9337) acc_u 12.5000 (8.6364) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [60/74] time 0.551 (0.465) data 0.421 (0.333) loss_u loss_u 0.9370 (0.9345) acc_u 9.3750 (8.6458) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [65/74] time 0.393 (0.464) data 0.261 (0.333) loss_u loss_u 0.8960 (0.9330) acc_u 12.5000 (8.7981) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [70/74] time 0.517 (0.465) data 0.385 (0.334) loss_u loss_u 0.9561 (0.9340) acc_u 6.2500 (8.6607) lr 3.1417e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1488
confident_label rate tensor(0.2452, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 769
clean true:752
clean false:17
clean_rate:0.9778933680104032
noisy true:896
noisy false:1471
after delete: len(clean_dataset) 769
after delete: len(noisy_dataset) 2367
epoch [187/200] batch [5/24] time 0.386 (0.468) data 0.255 (0.338) loss_x loss_x 1.0420 (1.2141) acc_x 75.0000 (70.0000) lr 2.7630e-05 eta 0:00:08
epoch [187/200] batch [10/24] time 0.418 (0.482) data 0.287 (0.351) loss_x loss_x 0.6841 (1.0948) acc_x 78.1250 (70.0000) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [15/24] time 0.448 (0.482) data 0.317 (0.351) loss_x loss_x 1.1260 (1.1193) acc_x 78.1250 (70.4167) lr 2.7630e-05 eta 0:00:04
epoch [187/200] batch [20/24] time 0.825 (0.487) data 0.695 (0.357) loss_x loss_x 0.7935 (1.0813) acc_x 75.0000 (72.6562) lr 2.7630e-05 eta 0:00:01
epoch [187/200] batch [5/73] time 0.368 (0.474) data 0.237 (0.344) loss_u loss_u 0.9297 (0.9556) acc_u 6.2500 (4.3750) lr 2.7630e-05 eta 0:00:32
epoch [187/200] batch [10/73] time 0.384 (0.470) data 0.252 (0.340) loss_u loss_u 0.9795 (0.9565) acc_u 3.1250 (5.6250) lr 2.7630e-05 eta 0:00:29
epoch [187/200] batch [15/73] time 0.358 (0.466) data 0.228 (0.335) loss_u loss_u 0.9541 (0.9546) acc_u 3.1250 (5.6250) lr 2.7630e-05 eta 0:00:27
epoch [187/200] batch [20/73] time 0.450 (0.471) data 0.319 (0.340) loss_u loss_u 0.9126 (0.9519) acc_u 12.5000 (6.0938) lr 2.7630e-05 eta 0:00:24
epoch [187/200] batch [25/73] time 0.322 (0.470) data 0.191 (0.340) loss_u loss_u 0.8325 (0.9439) acc_u 21.8750 (7.5000) lr 2.7630e-05 eta 0:00:22
epoch [187/200] batch [30/73] time 0.661 (0.475) data 0.530 (0.344) loss_u loss_u 0.9653 (0.9483) acc_u 3.1250 (6.8750) lr 2.7630e-05 eta 0:00:20
epoch [187/200] batch [35/73] time 0.439 (0.472) data 0.307 (0.341) loss_u loss_u 0.9600 (0.9454) acc_u 6.2500 (7.3214) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [40/73] time 0.438 (0.469) data 0.306 (0.338) loss_u loss_u 0.8726 (0.9406) acc_u 18.7500 (7.8125) lr 2.7630e-05 eta 0:00:15
epoch [187/200] batch [45/73] time 0.506 (0.469) data 0.375 (0.338) loss_u loss_u 0.9839 (0.9408) acc_u 3.1250 (7.9861) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [50/73] time 0.440 (0.469) data 0.309 (0.338) loss_u loss_u 0.8843 (0.9397) acc_u 15.6250 (8.1875) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [55/73] time 0.413 (0.467) data 0.282 (0.336) loss_u loss_u 0.9331 (0.9406) acc_u 9.3750 (8.1818) lr 2.7630e-05 eta 0:00:08
epoch [187/200] batch [60/73] time 0.513 (0.466) data 0.381 (0.335) loss_u loss_u 0.9824 (0.9398) acc_u 0.0000 (8.1771) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [65/73] time 0.406 (0.463) data 0.274 (0.331) loss_u loss_u 0.9048 (0.9403) acc_u 12.5000 (8.0769) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [70/73] time 0.496 (0.462) data 0.365 (0.331) loss_u loss_u 0.9707 (0.9411) acc_u 3.1250 (7.9464) lr 2.7630e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1498
confident_label rate tensor(0.2388, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 749
clean true:733
clean false:16
clean_rate:0.9786381842456608
noisy true:905
noisy false:1482
after delete: len(clean_dataset) 749
after delete: len(noisy_dataset) 2387
epoch [188/200] batch [5/23] time 0.506 (0.487) data 0.376 (0.356) loss_x loss_x 1.3584 (1.2364) acc_x 71.8750 (68.1250) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [10/23] time 0.551 (0.485) data 0.421 (0.354) loss_x loss_x 0.9663 (1.1743) acc_x 78.1250 (70.0000) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [15/23] time 0.384 (0.496) data 0.254 (0.366) loss_x loss_x 1.0859 (1.1509) acc_x 68.7500 (71.0417) lr 2.4083e-05 eta 0:00:03
epoch [188/200] batch [20/23] time 0.423 (0.503) data 0.293 (0.372) loss_x loss_x 0.7690 (1.0980) acc_x 78.1250 (72.5000) lr 2.4083e-05 eta 0:00:01
epoch [188/200] batch [5/74] time 0.511 (0.486) data 0.380 (0.356) loss_u loss_u 0.9741 (0.9476) acc_u 3.1250 (5.6250) lr 2.4083e-05 eta 0:00:33
epoch [188/200] batch [10/74] time 0.450 (0.491) data 0.319 (0.361) loss_u loss_u 0.9136 (0.9346) acc_u 9.3750 (7.5000) lr 2.4083e-05 eta 0:00:31
epoch [188/200] batch [15/74] time 0.484 (0.487) data 0.352 (0.356) loss_u loss_u 0.9062 (0.9389) acc_u 12.5000 (7.2917) lr 2.4083e-05 eta 0:00:28
epoch [188/200] batch [20/74] time 0.409 (0.486) data 0.277 (0.355) loss_u loss_u 0.9023 (0.9336) acc_u 12.5000 (8.2812) lr 2.4083e-05 eta 0:00:26
epoch [188/200] batch [25/74] time 0.472 (0.485) data 0.340 (0.354) loss_u loss_u 0.9077 (0.9351) acc_u 12.5000 (8.3750) lr 2.4083e-05 eta 0:00:23
epoch [188/200] batch [30/74] time 0.494 (0.491) data 0.362 (0.360) loss_u loss_u 0.9067 (0.9330) acc_u 9.3750 (8.4375) lr 2.4083e-05 eta 0:00:21
epoch [188/200] batch [35/74] time 0.387 (0.492) data 0.255 (0.361) loss_u loss_u 0.8369 (0.9298) acc_u 21.8750 (9.1071) lr 2.4083e-05 eta 0:00:19
epoch [188/200] batch [40/74] time 0.457 (0.493) data 0.326 (0.362) loss_u loss_u 0.9482 (0.9320) acc_u 6.2500 (8.8281) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [45/74] time 0.431 (0.493) data 0.301 (0.362) loss_u loss_u 0.9653 (0.9316) acc_u 6.2500 (8.8194) lr 2.4083e-05 eta 0:00:14
epoch [188/200] batch [50/74] time 0.449 (0.490) data 0.318 (0.359) loss_u loss_u 0.9985 (0.9353) acc_u 0.0000 (8.3125) lr 2.4083e-05 eta 0:00:11
epoch [188/200] batch [55/74] time 0.567 (0.487) data 0.436 (0.356) loss_u loss_u 0.9502 (0.9352) acc_u 3.1250 (8.5227) lr 2.4083e-05 eta 0:00:09
epoch [188/200] batch [60/74] time 0.455 (0.485) data 0.324 (0.353) loss_u loss_u 0.9438 (0.9368) acc_u 6.2500 (8.3333) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [65/74] time 0.428 (0.482) data 0.296 (0.351) loss_u loss_u 0.8955 (0.9364) acc_u 12.5000 (8.2692) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [70/74] time 0.509 (0.480) data 0.377 (0.349) loss_u loss_u 0.9341 (0.9362) acc_u 6.2500 (8.2143) lr 2.4083e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1476
confident_label rate tensor(0.2417, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 758
clean true:744
clean false:14
clean_rate:0.9815303430079155
noisy true:916
noisy false:1462
after delete: len(clean_dataset) 758
after delete: len(noisy_dataset) 2378
epoch [189/200] batch [5/23] time 0.380 (0.422) data 0.250 (0.291) loss_x loss_x 0.9575 (0.9514) acc_x 78.1250 (76.2500) lr 2.0777e-05 eta 0:00:07
epoch [189/200] batch [10/23] time 0.439 (0.467) data 0.309 (0.336) loss_x loss_x 0.8945 (0.9779) acc_x 71.8750 (76.2500) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [15/23] time 0.388 (0.470) data 0.256 (0.339) loss_x loss_x 1.3545 (1.0447) acc_x 65.6250 (75.0000) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [20/23] time 0.340 (0.463) data 0.209 (0.332) loss_x loss_x 1.0938 (0.9914) acc_x 78.1250 (76.4062) lr 2.0777e-05 eta 0:00:01
epoch [189/200] batch [5/74] time 0.399 (0.459) data 0.268 (0.328) loss_u loss_u 0.9385 (0.9585) acc_u 6.2500 (3.7500) lr 2.0777e-05 eta 0:00:31
epoch [189/200] batch [10/74] time 0.403 (0.457) data 0.271 (0.326) loss_u loss_u 0.9849 (0.9554) acc_u 3.1250 (5.0000) lr 2.0777e-05 eta 0:00:29
epoch [189/200] batch [15/74] time 0.316 (0.452) data 0.184 (0.321) loss_u loss_u 0.9448 (0.9531) acc_u 6.2500 (5.6250) lr 2.0777e-05 eta 0:00:26
epoch [189/200] batch [20/74] time 0.503 (0.455) data 0.370 (0.324) loss_u loss_u 0.9604 (0.9548) acc_u 6.2500 (5.6250) lr 2.0777e-05 eta 0:00:24
epoch [189/200] batch [25/74] time 0.762 (0.461) data 0.629 (0.330) loss_u loss_u 0.9692 (0.9497) acc_u 9.3750 (6.6250) lr 2.0777e-05 eta 0:00:22
epoch [189/200] batch [30/74] time 0.347 (0.466) data 0.215 (0.335) loss_u loss_u 0.9121 (0.9464) acc_u 9.3750 (6.7708) lr 2.0777e-05 eta 0:00:20
epoch [189/200] batch [35/74] time 0.369 (0.459) data 0.237 (0.327) loss_u loss_u 0.9556 (0.9456) acc_u 3.1250 (6.7857) lr 2.0777e-05 eta 0:00:17
epoch [189/200] batch [40/74] time 0.431 (0.457) data 0.299 (0.325) loss_u loss_u 0.9590 (0.9422) acc_u 9.3750 (7.4219) lr 2.0777e-05 eta 0:00:15
epoch [189/200] batch [45/74] time 0.469 (0.457) data 0.337 (0.325) loss_u loss_u 0.9517 (0.9410) acc_u 3.1250 (7.5694) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [50/74] time 0.421 (0.458) data 0.288 (0.326) loss_u loss_u 0.9316 (0.9381) acc_u 12.5000 (7.8750) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [55/74] time 0.514 (0.463) data 0.383 (0.332) loss_u loss_u 0.9839 (0.9369) acc_u 3.1250 (8.0114) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [60/74] time 0.480 (0.460) data 0.349 (0.329) loss_u loss_u 0.8491 (0.9360) acc_u 15.6250 (8.0729) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [65/74] time 0.410 (0.460) data 0.277 (0.328) loss_u loss_u 0.9331 (0.9350) acc_u 6.2500 (8.2692) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [70/74] time 0.471 (0.460) data 0.339 (0.328) loss_u loss_u 0.9346 (0.9357) acc_u 6.2500 (8.2143) lr 2.0777e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1505
confident_label rate tensor(0.2369, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 743
clean true:734
clean false:9
clean_rate:0.9878869448183042
noisy true:897
noisy false:1496
after delete: len(clean_dataset) 743
after delete: len(noisy_dataset) 2393
epoch [190/200] batch [5/23] time 0.447 (0.551) data 0.316 (0.420) loss_x loss_x 1.2676 (1.2007) acc_x 75.0000 (74.3750) lr 1.7713e-05 eta 0:00:09
epoch [190/200] batch [10/23] time 0.481 (0.515) data 0.350 (0.384) loss_x loss_x 1.3301 (1.2542) acc_x 75.0000 (71.8750) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [15/23] time 0.668 (0.514) data 0.537 (0.383) loss_x loss_x 0.6372 (1.1832) acc_x 87.5000 (73.5417) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [20/23] time 0.491 (0.502) data 0.359 (0.371) loss_x loss_x 1.1074 (1.1582) acc_x 87.5000 (74.3750) lr 1.7713e-05 eta 0:00:01
epoch [190/200] batch [5/74] time 0.379 (0.485) data 0.249 (0.354) loss_u loss_u 0.9624 (0.9212) acc_u 6.2500 (8.7500) lr 1.7713e-05 eta 0:00:33
epoch [190/200] batch [10/74] time 0.454 (0.483) data 0.323 (0.351) loss_u loss_u 0.9399 (0.9253) acc_u 6.2500 (8.4375) lr 1.7713e-05 eta 0:00:30
epoch [190/200] batch [15/74] time 0.387 (0.472) data 0.255 (0.340) loss_u loss_u 0.9292 (0.9133) acc_u 9.3750 (9.7917) lr 1.7713e-05 eta 0:00:27
epoch [190/200] batch [20/74] time 0.349 (0.467) data 0.217 (0.336) loss_u loss_u 0.9531 (0.9194) acc_u 9.3750 (9.5312) lr 1.7713e-05 eta 0:00:25
epoch [190/200] batch [25/74] time 0.417 (0.464) data 0.285 (0.333) loss_u loss_u 0.9946 (0.9203) acc_u 0.0000 (9.5000) lr 1.7713e-05 eta 0:00:22
epoch [190/200] batch [30/74] time 0.687 (0.465) data 0.557 (0.334) loss_u loss_u 0.9619 (0.9234) acc_u 6.2500 (9.5833) lr 1.7713e-05 eta 0:00:20
epoch [190/200] batch [35/74] time 0.535 (0.468) data 0.403 (0.337) loss_u loss_u 0.9307 (0.9234) acc_u 6.2500 (9.5536) lr 1.7713e-05 eta 0:00:18
epoch [190/200] batch [40/74] time 0.494 (0.467) data 0.364 (0.335) loss_u loss_u 0.9912 (0.9258) acc_u 0.0000 (9.1406) lr 1.7713e-05 eta 0:00:15
epoch [190/200] batch [45/74] time 0.436 (0.467) data 0.305 (0.336) loss_u loss_u 0.9629 (0.9252) acc_u 3.1250 (9.3750) lr 1.7713e-05 eta 0:00:13
epoch [190/200] batch [50/74] time 0.527 (0.468) data 0.396 (0.337) loss_u loss_u 0.9541 (0.9247) acc_u 6.2500 (9.4375) lr 1.7713e-05 eta 0:00:11
epoch [190/200] batch [55/74] time 0.445 (0.468) data 0.314 (0.337) loss_u loss_u 0.9409 (0.9252) acc_u 6.2500 (9.4886) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [60/74] time 0.474 (0.468) data 0.343 (0.336) loss_u loss_u 0.9321 (0.9268) acc_u 9.3750 (9.2188) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [65/74] time 0.457 (0.463) data 0.325 (0.332) loss_u loss_u 0.9883 (0.9284) acc_u 0.0000 (9.0865) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [70/74] time 0.506 (0.463) data 0.374 (0.331) loss_u loss_u 0.9819 (0.9309) acc_u 0.0000 (8.7500) lr 1.7713e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1553
confident_label rate tensor(0.2344, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 735
clean true:720
clean false:15
clean_rate:0.9795918367346939
noisy true:863
noisy false:1538
after delete: len(clean_dataset) 735
after delete: len(noisy_dataset) 2401
epoch [191/200] batch [5/22] time 0.419 (0.474) data 0.289 (0.343) loss_x loss_x 0.5664 (0.9151) acc_x 87.5000 (76.2500) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [10/22] time 0.396 (0.478) data 0.265 (0.348) loss_x loss_x 0.6128 (0.9922) acc_x 87.5000 (75.9375) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [15/22] time 0.508 (0.455) data 0.378 (0.324) loss_x loss_x 1.0869 (1.0684) acc_x 71.8750 (73.1250) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [20/22] time 0.386 (0.464) data 0.256 (0.334) loss_x loss_x 0.9600 (1.0889) acc_x 71.8750 (72.3438) lr 1.4891e-05 eta 0:00:00
epoch [191/200] batch [5/75] time 0.508 (0.465) data 0.377 (0.334) loss_u loss_u 0.9111 (0.9159) acc_u 9.3750 (10.6250) lr 1.4891e-05 eta 0:00:32
epoch [191/200] batch [10/75] time 0.428 (0.459) data 0.297 (0.329) loss_u loss_u 0.9155 (0.9028) acc_u 12.5000 (12.1875) lr 1.4891e-05 eta 0:00:29
epoch [191/200] batch [15/75] time 0.399 (0.462) data 0.269 (0.331) loss_u loss_u 0.8730 (0.9083) acc_u 15.6250 (11.8750) lr 1.4891e-05 eta 0:00:27
epoch [191/200] batch [20/75] time 0.433 (0.466) data 0.302 (0.335) loss_u loss_u 0.9409 (0.9134) acc_u 6.2500 (11.4062) lr 1.4891e-05 eta 0:00:25
epoch [191/200] batch [25/75] time 0.495 (0.469) data 0.363 (0.337) loss_u loss_u 0.9272 (0.9141) acc_u 6.2500 (10.8750) lr 1.4891e-05 eta 0:00:23
epoch [191/200] batch [30/75] time 0.421 (0.466) data 0.290 (0.335) loss_u loss_u 0.8740 (0.9164) acc_u 15.6250 (10.6250) lr 1.4891e-05 eta 0:00:20
epoch [191/200] batch [35/75] time 0.334 (0.460) data 0.203 (0.329) loss_u loss_u 0.9336 (0.9194) acc_u 6.2500 (10.0000) lr 1.4891e-05 eta 0:00:18
epoch [191/200] batch [40/75] time 0.425 (0.460) data 0.294 (0.328) loss_u loss_u 0.9097 (0.9231) acc_u 9.3750 (9.3750) lr 1.4891e-05 eta 0:00:16
epoch [191/200] batch [45/75] time 0.596 (0.458) data 0.463 (0.326) loss_u loss_u 0.9639 (0.9257) acc_u 3.1250 (9.1667) lr 1.4891e-05 eta 0:00:13
epoch [191/200] batch [50/75] time 0.433 (0.466) data 0.301 (0.335) loss_u loss_u 0.9858 (0.9275) acc_u 0.0000 (8.8750) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [55/75] time 0.476 (0.465) data 0.344 (0.334) loss_u loss_u 0.9048 (0.9266) acc_u 12.5000 (9.0909) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [60/75] time 0.461 (0.464) data 0.330 (0.332) loss_u loss_u 0.9644 (0.9273) acc_u 3.1250 (9.0625) lr 1.4891e-05 eta 0:00:06
epoch [191/200] batch [65/75] time 0.446 (0.468) data 0.314 (0.336) loss_u loss_u 0.9629 (0.9291) acc_u 3.1250 (8.7981) lr 1.4891e-05 eta 0:00:04
epoch [191/200] batch [70/75] time 0.430 (0.468) data 0.298 (0.336) loss_u loss_u 0.9194 (0.9285) acc_u 12.5000 (8.9732) lr 1.4891e-05 eta 0:00:02
epoch [191/200] batch [75/75] time 0.454 (0.466) data 0.323 (0.335) loss_u loss_u 0.9482 (0.9301) acc_u 6.2500 (8.8333) lr 1.4891e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1527
confident_label rate tensor(0.2325, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 729
clean true:719
clean false:10
clean_rate:0.9862825788751715
noisy true:890
noisy false:1517
after delete: len(clean_dataset) 729
after delete: len(noisy_dataset) 2407
epoch [192/200] batch [5/22] time 0.637 (0.449) data 0.507 (0.318) loss_x loss_x 1.0410 (1.0407) acc_x 75.0000 (74.3750) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [10/22] time 0.513 (0.460) data 0.383 (0.329) loss_x loss_x 1.0713 (1.0961) acc_x 71.8750 (73.7500) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [15/22] time 0.383 (0.464) data 0.252 (0.333) loss_x loss_x 1.1250 (1.0846) acc_x 75.0000 (74.5833) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [20/22] time 0.498 (0.483) data 0.367 (0.352) loss_x loss_x 0.8677 (1.0882) acc_x 78.1250 (74.0625) lr 1.2312e-05 eta 0:00:00
epoch [192/200] batch [5/75] time 0.459 (0.465) data 0.326 (0.334) loss_u loss_u 0.9541 (0.9408) acc_u 6.2500 (7.5000) lr 1.2312e-05 eta 0:00:32
epoch [192/200] batch [10/75] time 0.402 (0.473) data 0.271 (0.342) loss_u loss_u 0.9414 (0.9397) acc_u 6.2500 (7.1875) lr 1.2312e-05 eta 0:00:30
epoch [192/200] batch [15/75] time 0.482 (0.469) data 0.351 (0.337) loss_u loss_u 0.9663 (0.9401) acc_u 3.1250 (7.2917) lr 1.2312e-05 eta 0:00:28
epoch [192/200] batch [20/75] time 0.501 (0.467) data 0.369 (0.336) loss_u loss_u 0.9390 (0.9360) acc_u 12.5000 (8.1250) lr 1.2312e-05 eta 0:00:25
epoch [192/200] batch [25/75] time 0.389 (0.469) data 0.258 (0.338) loss_u loss_u 0.9492 (0.9395) acc_u 6.2500 (7.7500) lr 1.2312e-05 eta 0:00:23
epoch [192/200] batch [30/75] time 0.620 (0.471) data 0.488 (0.339) loss_u loss_u 0.9810 (0.9405) acc_u 3.1250 (7.5000) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [35/75] time 0.448 (0.468) data 0.317 (0.337) loss_u loss_u 0.9307 (0.9383) acc_u 9.3750 (7.6786) lr 1.2312e-05 eta 0:00:18
epoch [192/200] batch [40/75] time 0.511 (0.469) data 0.380 (0.338) loss_u loss_u 0.8955 (0.9390) acc_u 15.6250 (7.7344) lr 1.2312e-05 eta 0:00:16
epoch [192/200] batch [45/75] time 0.393 (0.466) data 0.262 (0.335) loss_u loss_u 0.9263 (0.9386) acc_u 9.3750 (7.6389) lr 1.2312e-05 eta 0:00:13
epoch [192/200] batch [50/75] time 0.436 (0.463) data 0.305 (0.331) loss_u loss_u 0.9395 (0.9357) acc_u 6.2500 (8.0000) lr 1.2312e-05 eta 0:00:11
epoch [192/200] batch [55/75] time 0.409 (0.460) data 0.278 (0.328) loss_u loss_u 0.8301 (0.9334) acc_u 15.6250 (8.3523) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [60/75] time 0.455 (0.459) data 0.324 (0.328) loss_u loss_u 0.9985 (0.9361) acc_u 0.0000 (8.0729) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [65/75] time 0.648 (0.463) data 0.517 (0.332) loss_u loss_u 0.9092 (0.9340) acc_u 12.5000 (8.4135) lr 1.2312e-05 eta 0:00:04
epoch [192/200] batch [70/75] time 0.433 (0.463) data 0.301 (0.331) loss_u loss_u 0.9136 (0.9327) acc_u 9.3750 (8.6607) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [75/75] time 0.378 (0.464) data 0.245 (0.333) loss_u loss_u 0.9658 (0.9337) acc_u 6.2500 (8.5417) lr 1.2312e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1534
confident_label rate tensor(0.2411, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 756
clean true:740
clean false:16
clean_rate:0.9788359788359788
noisy true:862
noisy false:1518
after delete: len(clean_dataset) 756
after delete: len(noisy_dataset) 2380
epoch [193/200] batch [5/23] time 0.481 (0.512) data 0.350 (0.382) loss_x loss_x 1.4180 (1.2152) acc_x 62.5000 (68.7500) lr 9.9763e-06 eta 0:00:09
epoch [193/200] batch [10/23] time 0.451 (0.519) data 0.320 (0.389) loss_x loss_x 1.4893 (1.1697) acc_x 68.7500 (71.8750) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [15/23] time 0.506 (0.514) data 0.375 (0.383) loss_x loss_x 0.9810 (1.0820) acc_x 78.1250 (73.9583) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [20/23] time 0.522 (0.499) data 0.392 (0.368) loss_x loss_x 0.6890 (1.0422) acc_x 81.2500 (74.6875) lr 9.9763e-06 eta 0:00:01
epoch [193/200] batch [5/74] time 0.411 (0.492) data 0.279 (0.361) loss_u loss_u 0.8799 (0.9263) acc_u 15.6250 (9.3750) lr 9.9763e-06 eta 0:00:33
epoch [193/200] batch [10/74] time 0.454 (0.486) data 0.324 (0.355) loss_u loss_u 0.9434 (0.9295) acc_u 6.2500 (8.4375) lr 9.9763e-06 eta 0:00:31
epoch [193/200] batch [15/74] time 0.394 (0.485) data 0.263 (0.354) loss_u loss_u 0.8730 (0.9321) acc_u 15.6250 (8.1250) lr 9.9763e-06 eta 0:00:28
epoch [193/200] batch [20/74] time 0.402 (0.483) data 0.271 (0.352) loss_u loss_u 0.9395 (0.9346) acc_u 6.2500 (8.4375) lr 9.9763e-06 eta 0:00:26
epoch [193/200] batch [25/74] time 0.436 (0.477) data 0.304 (0.346) loss_u loss_u 0.8994 (0.9357) acc_u 15.6250 (8.2500) lr 9.9763e-06 eta 0:00:23
epoch [193/200] batch [30/74] time 0.388 (0.471) data 0.257 (0.340) loss_u loss_u 0.9448 (0.9342) acc_u 6.2500 (8.4375) lr 9.9763e-06 eta 0:00:20
epoch [193/200] batch [35/74] time 0.462 (0.473) data 0.331 (0.342) loss_u loss_u 0.9619 (0.9365) acc_u 6.2500 (8.2143) lr 9.9763e-06 eta 0:00:18
epoch [193/200] batch [40/74] time 0.492 (0.471) data 0.358 (0.340) loss_u loss_u 0.9883 (0.9385) acc_u 0.0000 (7.7344) lr 9.9763e-06 eta 0:00:16
epoch [193/200] batch [45/74] time 0.585 (0.472) data 0.453 (0.341) loss_u loss_u 0.9365 (0.9387) acc_u 9.3750 (7.5694) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [50/74] time 0.538 (0.472) data 0.406 (0.341) loss_u loss_u 0.9492 (0.9383) acc_u 6.2500 (7.6875) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [55/74] time 0.398 (0.467) data 0.266 (0.336) loss_u loss_u 0.9141 (0.9356) acc_u 12.5000 (8.0114) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [60/74] time 0.348 (0.464) data 0.218 (0.333) loss_u loss_u 0.9668 (0.9366) acc_u 6.2500 (7.9688) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [65/74] time 0.388 (0.463) data 0.257 (0.332) loss_u loss_u 0.9668 (0.9348) acc_u 3.1250 (8.3173) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [70/74] time 0.413 (0.461) data 0.283 (0.330) loss_u loss_u 0.9756 (0.9364) acc_u 3.1250 (8.1250) lr 9.9763e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1473
confident_label rate tensor(0.2494, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 782
clean true:764
clean false:18
clean_rate:0.9769820971867008
noisy true:899
noisy false:1455
after delete: len(clean_dataset) 782
after delete: len(noisy_dataset) 2354
epoch [194/200] batch [5/24] time 0.447 (0.445) data 0.316 (0.314) loss_x loss_x 1.1484 (1.0071) acc_x 75.0000 (76.2500) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [10/24] time 0.404 (0.445) data 0.274 (0.314) loss_x loss_x 0.9863 (1.1418) acc_x 71.8750 (73.1250) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [15/24] time 0.590 (0.460) data 0.459 (0.330) loss_x loss_x 1.4824 (1.1439) acc_x 65.6250 (73.3333) lr 7.8853e-06 eta 0:00:04
epoch [194/200] batch [20/24] time 0.551 (0.467) data 0.420 (0.337) loss_x loss_x 0.9258 (1.1258) acc_x 78.1250 (73.2812) lr 7.8853e-06 eta 0:00:01
epoch [194/200] batch [5/73] time 0.452 (0.457) data 0.321 (0.326) loss_u loss_u 0.9102 (0.9496) acc_u 12.5000 (6.2500) lr 7.8853e-06 eta 0:00:31
epoch [194/200] batch [10/73] time 0.490 (0.455) data 0.359 (0.324) loss_u loss_u 0.9346 (0.9461) acc_u 6.2500 (6.2500) lr 7.8853e-06 eta 0:00:28
epoch [194/200] batch [15/73] time 0.389 (0.450) data 0.258 (0.319) loss_u loss_u 0.8711 (0.9386) acc_u 18.7500 (7.2917) lr 7.8853e-06 eta 0:00:26
epoch [194/200] batch [20/73] time 0.435 (0.443) data 0.303 (0.312) loss_u loss_u 0.9644 (0.9424) acc_u 3.1250 (6.7188) lr 7.8853e-06 eta 0:00:23
epoch [194/200] batch [25/73] time 0.476 (0.443) data 0.345 (0.312) loss_u loss_u 0.9561 (0.9414) acc_u 6.2500 (6.7500) lr 7.8853e-06 eta 0:00:21
epoch [194/200] batch [30/73] time 0.484 (0.443) data 0.352 (0.312) loss_u loss_u 0.9536 (0.9428) acc_u 6.2500 (6.6667) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [35/73] time 0.457 (0.440) data 0.326 (0.309) loss_u loss_u 0.9155 (0.9408) acc_u 12.5000 (7.0536) lr 7.8853e-06 eta 0:00:16
epoch [194/200] batch [40/73] time 0.537 (0.441) data 0.405 (0.310) loss_u loss_u 0.9409 (0.9434) acc_u 6.2500 (6.8750) lr 7.8853e-06 eta 0:00:14
epoch [194/200] batch [45/73] time 0.435 (0.440) data 0.304 (0.309) loss_u loss_u 0.8979 (0.9443) acc_u 15.6250 (7.0139) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [50/73] time 0.350 (0.437) data 0.218 (0.306) loss_u loss_u 0.9678 (0.9444) acc_u 6.2500 (6.9375) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [55/73] time 0.396 (0.436) data 0.265 (0.305) loss_u loss_u 0.8716 (0.9453) acc_u 18.7500 (6.7614) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [60/73] time 0.480 (0.435) data 0.348 (0.304) loss_u loss_u 0.9673 (0.9464) acc_u 6.2500 (6.6146) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [65/73] time 0.704 (0.440) data 0.572 (0.308) loss_u loss_u 0.9604 (0.9440) acc_u 6.2500 (6.9712) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [70/73] time 0.336 (0.439) data 0.204 (0.308) loss_u loss_u 0.9004 (0.9433) acc_u 12.5000 (7.1875) lr 7.8853e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1477
confident_label rate tensor(0.2411, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 756
clean true:740
clean false:16
clean_rate:0.9788359788359788
noisy true:919
noisy false:1461
after delete: len(clean_dataset) 756
after delete: len(noisy_dataset) 2380
epoch [195/200] batch [5/23] time 0.424 (0.426) data 0.294 (0.295) loss_x loss_x 0.7002 (1.0440) acc_x 81.2500 (73.1250) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [10/23] time 0.435 (0.413) data 0.304 (0.282) loss_x loss_x 1.3398 (1.1497) acc_x 59.3750 (70.3125) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [15/23] time 0.415 (0.431) data 0.284 (0.300) loss_x loss_x 0.7266 (1.0733) acc_x 81.2500 (73.5417) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [20/23] time 0.421 (0.439) data 0.291 (0.308) loss_x loss_x 1.5322 (1.0882) acc_x 65.6250 (74.5312) lr 6.0390e-06 eta 0:00:01
epoch [195/200] batch [5/74] time 0.349 (0.451) data 0.217 (0.320) loss_u loss_u 0.9482 (0.9126) acc_u 6.2500 (9.3750) lr 6.0390e-06 eta 0:00:31
epoch [195/200] batch [10/74] time 0.520 (0.457) data 0.389 (0.326) loss_u loss_u 0.8696 (0.9160) acc_u 15.6250 (8.7500) lr 6.0390e-06 eta 0:00:29
epoch [195/200] batch [15/74] time 0.362 (0.456) data 0.231 (0.325) loss_u loss_u 0.9761 (0.9254) acc_u 3.1250 (8.1250) lr 6.0390e-06 eta 0:00:26
epoch [195/200] batch [20/74] time 0.374 (0.458) data 0.243 (0.327) loss_u loss_u 0.8906 (0.9241) acc_u 15.6250 (8.7500) lr 6.0390e-06 eta 0:00:24
epoch [195/200] batch [25/74] time 0.473 (0.454) data 0.342 (0.323) loss_u loss_u 0.9243 (0.9219) acc_u 9.3750 (9.3750) lr 6.0390e-06 eta 0:00:22
epoch [195/200] batch [30/74] time 0.362 (0.452) data 0.231 (0.321) loss_u loss_u 0.9277 (0.9254) acc_u 6.2500 (9.0625) lr 6.0390e-06 eta 0:00:19
epoch [195/200] batch [35/74] time 0.320 (0.447) data 0.189 (0.316) loss_u loss_u 0.9370 (0.9281) acc_u 9.3750 (8.7500) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [40/74] time 0.608 (0.449) data 0.476 (0.318) loss_u loss_u 0.9233 (0.9276) acc_u 9.3750 (8.7500) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [45/74] time 0.467 (0.450) data 0.336 (0.319) loss_u loss_u 0.9336 (0.9310) acc_u 6.2500 (8.3333) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [50/74] time 0.484 (0.447) data 0.353 (0.316) loss_u loss_u 0.9248 (0.9335) acc_u 9.3750 (8.0625) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [55/74] time 0.450 (0.446) data 0.319 (0.315) loss_u loss_u 0.9424 (0.9357) acc_u 6.2500 (7.7273) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [60/74] time 0.427 (0.443) data 0.296 (0.312) loss_u loss_u 0.9648 (0.9377) acc_u 3.1250 (7.5000) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [65/74] time 0.417 (0.442) data 0.285 (0.311) loss_u loss_u 0.9556 (0.9376) acc_u 9.3750 (7.5962) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [70/74] time 0.492 (0.440) data 0.361 (0.309) loss_u loss_u 0.9229 (0.9383) acc_u 6.2500 (7.4554) lr 6.0390e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1502
confident_label rate tensor(0.2436, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 764
clean true:750
clean false:14
clean_rate:0.981675392670157
noisy true:884
noisy false:1488
after delete: len(clean_dataset) 764
after delete: len(noisy_dataset) 2372
epoch [196/200] batch [5/23] time 0.393 (0.460) data 0.262 (0.329) loss_x loss_x 1.0400 (1.1349) acc_x 71.8750 (73.1250) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [10/23] time 0.383 (0.437) data 0.253 (0.306) loss_x loss_x 0.9507 (1.1660) acc_x 81.2500 (72.8125) lr 4.4380e-06 eta 0:00:05
epoch [196/200] batch [15/23] time 0.564 (0.461) data 0.433 (0.331) loss_x loss_x 1.0742 (1.0966) acc_x 75.0000 (74.3750) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [20/23] time 0.402 (0.455) data 0.271 (0.325) loss_x loss_x 1.4814 (1.1126) acc_x 62.5000 (74.3750) lr 4.4380e-06 eta 0:00:01
epoch [196/200] batch [5/74] time 0.539 (0.456) data 0.408 (0.325) loss_u loss_u 0.9175 (0.9412) acc_u 12.5000 (8.7500) lr 4.4380e-06 eta 0:00:31
epoch [196/200] batch [10/74] time 0.409 (0.457) data 0.277 (0.327) loss_u loss_u 0.9033 (0.9362) acc_u 9.3750 (8.7500) lr 4.4380e-06 eta 0:00:29
epoch [196/200] batch [15/74] time 0.425 (0.454) data 0.294 (0.323) loss_u loss_u 0.9175 (0.9340) acc_u 12.5000 (8.7500) lr 4.4380e-06 eta 0:00:26
epoch [196/200] batch [20/74] time 0.370 (0.453) data 0.239 (0.322) loss_u loss_u 0.9878 (0.9365) acc_u 0.0000 (8.2812) lr 4.4380e-06 eta 0:00:24
epoch [196/200] batch [25/74] time 0.374 (0.443) data 0.243 (0.312) loss_u loss_u 0.9443 (0.9384) acc_u 6.2500 (7.6250) lr 4.4380e-06 eta 0:00:21
epoch [196/200] batch [30/74] time 0.421 (0.444) data 0.290 (0.314) loss_u loss_u 0.9897 (0.9434) acc_u 6.2500 (7.2917) lr 4.4380e-06 eta 0:00:19
epoch [196/200] batch [35/74] time 0.386 (0.443) data 0.255 (0.312) loss_u loss_u 0.9146 (0.9416) acc_u 6.2500 (7.4107) lr 4.4380e-06 eta 0:00:17
epoch [196/200] batch [40/74] time 0.448 (0.441) data 0.317 (0.310) loss_u loss_u 0.9214 (0.9415) acc_u 9.3750 (7.3438) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [45/74] time 0.431 (0.440) data 0.300 (0.309) loss_u loss_u 0.9731 (0.9407) acc_u 3.1250 (7.3611) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [50/74] time 0.441 (0.439) data 0.310 (0.308) loss_u loss_u 0.9116 (0.9404) acc_u 9.3750 (7.3750) lr 4.4380e-06 eta 0:00:10
epoch [196/200] batch [55/74] time 0.454 (0.438) data 0.323 (0.307) loss_u loss_u 0.9438 (0.9403) acc_u 6.2500 (7.3295) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [60/74] time 0.419 (0.437) data 0.288 (0.306) loss_u loss_u 0.9785 (0.9395) acc_u 3.1250 (7.4479) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [65/74] time 0.585 (0.439) data 0.454 (0.308) loss_u loss_u 0.9199 (0.9389) acc_u 9.3750 (7.5000) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [70/74] time 0.485 (0.439) data 0.354 (0.308) loss_u loss_u 0.9766 (0.9399) acc_u 3.1250 (7.3214) lr 4.4380e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1520
confident_label rate tensor(0.2376, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 745
clean true:728
clean false:17
clean_rate:0.9771812080536912
noisy true:888
noisy false:1503
after delete: len(clean_dataset) 745
after delete: len(noisy_dataset) 2391
epoch [197/200] batch [5/23] time 0.514 (0.424) data 0.384 (0.293) loss_x loss_x 1.0293 (1.0416) acc_x 81.2500 (76.8750) lr 3.0827e-06 eta 0:00:07
epoch [197/200] batch [10/23] time 0.510 (0.450) data 0.379 (0.319) loss_x loss_x 1.0762 (1.0585) acc_x 78.1250 (75.0000) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [15/23] time 0.601 (0.460) data 0.470 (0.330) loss_x loss_x 1.3701 (1.0587) acc_x 75.0000 (74.7917) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [20/23] time 0.437 (0.455) data 0.306 (0.325) loss_x loss_x 1.0791 (1.1018) acc_x 71.8750 (74.6875) lr 3.0827e-06 eta 0:00:01
epoch [197/200] batch [5/74] time 0.414 (0.460) data 0.283 (0.329) loss_u loss_u 0.9673 (0.9459) acc_u 6.2500 (6.2500) lr 3.0827e-06 eta 0:00:31
epoch [197/200] batch [10/74] time 0.382 (0.457) data 0.250 (0.327) loss_u loss_u 0.9087 (0.9313) acc_u 12.5000 (8.4375) lr 3.0827e-06 eta 0:00:29
epoch [197/200] batch [15/74] time 0.439 (0.451) data 0.309 (0.321) loss_u loss_u 0.9683 (0.9341) acc_u 3.1250 (7.5000) lr 3.0827e-06 eta 0:00:26
epoch [197/200] batch [20/74] time 0.371 (0.446) data 0.241 (0.315) loss_u loss_u 0.9546 (0.9367) acc_u 6.2500 (7.0312) lr 3.0827e-06 eta 0:00:24
epoch [197/200] batch [25/74] time 0.377 (0.441) data 0.246 (0.310) loss_u loss_u 0.9146 (0.9322) acc_u 9.3750 (7.7500) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [30/74] time 0.397 (0.441) data 0.267 (0.310) loss_u loss_u 0.9229 (0.9344) acc_u 9.3750 (7.5000) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [35/74] time 0.433 (0.442) data 0.302 (0.312) loss_u loss_u 0.9731 (0.9338) acc_u 3.1250 (7.4107) lr 3.0827e-06 eta 0:00:17
epoch [197/200] batch [40/74] time 0.459 (0.439) data 0.328 (0.308) loss_u loss_u 0.9937 (0.9345) acc_u 0.0000 (7.4219) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [45/74] time 0.346 (0.437) data 0.215 (0.306) loss_u loss_u 0.8564 (0.9323) acc_u 12.5000 (7.7083) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [50/74] time 0.568 (0.435) data 0.437 (0.304) loss_u loss_u 0.9854 (0.9337) acc_u 3.1250 (7.5625) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [55/74] time 0.414 (0.434) data 0.283 (0.304) loss_u loss_u 0.9131 (0.9344) acc_u 12.5000 (7.6136) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [60/74] time 0.579 (0.436) data 0.448 (0.306) loss_u loss_u 0.9458 (0.9349) acc_u 6.2500 (7.6042) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [65/74] time 0.425 (0.436) data 0.294 (0.305) loss_u loss_u 0.8525 (0.9340) acc_u 25.0000 (7.8846) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [70/74] time 0.542 (0.437) data 0.410 (0.306) loss_u loss_u 0.8931 (0.9343) acc_u 12.5000 (7.8571) lr 3.0827e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1520
confident_label rate tensor(0.2353, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 738
clean true:725
clean false:13
clean_rate:0.9823848238482384
noisy true:891
noisy false:1507
after delete: len(clean_dataset) 738
after delete: len(noisy_dataset) 2398
epoch [198/200] batch [5/23] time 0.417 (0.447) data 0.287 (0.316) loss_x loss_x 1.6357 (1.2385) acc_x 59.3750 (71.2500) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [10/23] time 0.413 (0.449) data 0.282 (0.319) loss_x loss_x 0.7549 (1.1375) acc_x 78.1250 (75.0000) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [15/23] time 0.508 (0.439) data 0.378 (0.309) loss_x loss_x 0.8936 (1.1105) acc_x 78.1250 (75.2083) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [20/23] time 0.619 (0.463) data 0.488 (0.333) loss_x loss_x 0.8242 (1.0633) acc_x 81.2500 (75.7812) lr 1.9733e-06 eta 0:00:01
epoch [198/200] batch [5/74] time 0.384 (0.448) data 0.253 (0.317) loss_u loss_u 0.8955 (0.9246) acc_u 12.5000 (10.0000) lr 1.9733e-06 eta 0:00:30
epoch [198/200] batch [10/74] time 0.409 (0.444) data 0.278 (0.313) loss_u loss_u 0.9077 (0.9199) acc_u 9.3750 (10.6250) lr 1.9733e-06 eta 0:00:28
epoch [198/200] batch [15/74] time 0.453 (0.447) data 0.321 (0.316) loss_u loss_u 0.9263 (0.9224) acc_u 12.5000 (10.6250) lr 1.9733e-06 eta 0:00:26
epoch [198/200] batch [20/74] time 0.375 (0.446) data 0.244 (0.315) loss_u loss_u 0.9443 (0.9280) acc_u 9.3750 (9.8438) lr 1.9733e-06 eta 0:00:24
epoch [198/200] batch [25/74] time 0.410 (0.445) data 0.279 (0.314) loss_u loss_u 0.8979 (0.9262) acc_u 15.6250 (10.0000) lr 1.9733e-06 eta 0:00:21
epoch [198/200] batch [30/74] time 0.403 (0.443) data 0.272 (0.312) loss_u loss_u 0.9688 (0.9294) acc_u 3.1250 (9.3750) lr 1.9733e-06 eta 0:00:19
epoch [198/200] batch [35/74] time 0.351 (0.438) data 0.220 (0.307) loss_u loss_u 0.9170 (0.9283) acc_u 15.6250 (9.4643) lr 1.9733e-06 eta 0:00:17
epoch [198/200] batch [40/74] time 0.451 (0.438) data 0.319 (0.307) loss_u loss_u 0.8809 (0.9237) acc_u 15.6250 (10.0000) lr 1.9733e-06 eta 0:00:14
epoch [198/200] batch [45/74] time 0.319 (0.434) data 0.188 (0.303) loss_u loss_u 0.9380 (0.9266) acc_u 9.3750 (9.7222) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [50/74] time 0.494 (0.432) data 0.363 (0.301) loss_u loss_u 0.9326 (0.9249) acc_u 6.2500 (9.8125) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [55/74] time 0.460 (0.432) data 0.329 (0.301) loss_u loss_u 0.8574 (0.9251) acc_u 15.6250 (9.6023) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [60/74] time 0.531 (0.434) data 0.400 (0.303) loss_u loss_u 0.9102 (0.9256) acc_u 12.5000 (9.5833) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [65/74] time 0.430 (0.435) data 0.299 (0.304) loss_u loss_u 0.9478 (0.9271) acc_u 6.2500 (9.3269) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [70/74] time 0.426 (0.433) data 0.295 (0.302) loss_u loss_u 0.8916 (0.9288) acc_u 15.6250 (9.1518) lr 1.9733e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1471
confident_label rate tensor(0.2423, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 760
clean true:743
clean false:17
clean_rate:0.9776315789473684
noisy true:922
noisy false:1454
after delete: len(clean_dataset) 760
after delete: len(noisy_dataset) 2376
epoch [199/200] batch [5/23] time 0.487 (0.436) data 0.357 (0.306) loss_x loss_x 1.1133 (1.0970) acc_x 81.2500 (75.0000) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [10/23] time 0.445 (0.482) data 0.314 (0.352) loss_x loss_x 0.9116 (1.0076) acc_x 75.0000 (76.8750) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [15/23] time 0.530 (0.471) data 0.400 (0.341) loss_x loss_x 1.2910 (1.0747) acc_x 68.7500 (75.4167) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [20/23] time 0.374 (0.467) data 0.243 (0.337) loss_x loss_x 0.7422 (1.0553) acc_x 87.5000 (75.3125) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [5/74] time 0.428 (0.459) data 0.297 (0.328) loss_u loss_u 0.9492 (0.9433) acc_u 6.2500 (6.8750) lr 1.1101e-06 eta 0:00:31
epoch [199/200] batch [10/74] time 0.457 (0.455) data 0.326 (0.324) loss_u loss_u 0.9541 (0.9426) acc_u 3.1250 (6.2500) lr 1.1101e-06 eta 0:00:29
epoch [199/200] batch [15/74] time 0.563 (0.452) data 0.432 (0.321) loss_u loss_u 0.9434 (0.9464) acc_u 6.2500 (5.6250) lr 1.1101e-06 eta 0:00:26
epoch [199/200] batch [20/74] time 0.415 (0.447) data 0.284 (0.316) loss_u loss_u 0.9517 (0.9375) acc_u 6.2500 (6.7188) lr 1.1101e-06 eta 0:00:24
epoch [199/200] batch [25/74] time 0.422 (0.449) data 0.291 (0.318) loss_u loss_u 0.9248 (0.9383) acc_u 12.5000 (7.0000) lr 1.1101e-06 eta 0:00:22
epoch [199/200] batch [30/74] time 0.326 (0.444) data 0.195 (0.313) loss_u loss_u 0.9312 (0.9373) acc_u 9.3750 (7.2917) lr 1.1101e-06 eta 0:00:19
epoch [199/200] batch [35/74] time 0.473 (0.444) data 0.341 (0.313) loss_u loss_u 0.8521 (0.9324) acc_u 12.5000 (7.5893) lr 1.1101e-06 eta 0:00:17
epoch [199/200] batch [40/74] time 0.472 (0.446) data 0.341 (0.315) loss_u loss_u 0.9072 (0.9332) acc_u 15.6250 (7.8125) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [45/74] time 0.349 (0.443) data 0.218 (0.312) loss_u loss_u 0.8511 (0.9290) acc_u 18.7500 (8.4722) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [50/74] time 0.416 (0.440) data 0.285 (0.309) loss_u loss_u 0.9312 (0.9293) acc_u 6.2500 (8.5000) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [55/74] time 0.518 (0.441) data 0.387 (0.310) loss_u loss_u 0.9648 (0.9294) acc_u 3.1250 (8.6364) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [60/74] time 0.348 (0.440) data 0.217 (0.309) loss_u loss_u 0.9639 (0.9312) acc_u 3.1250 (8.3854) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [65/74] time 0.406 (0.440) data 0.275 (0.309) loss_u loss_u 0.9722 (0.9320) acc_u 3.1250 (8.4615) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [70/74] time 0.348 (0.438) data 0.217 (0.307) loss_u loss_u 0.9229 (0.9328) acc_u 6.2500 (8.2589) lr 1.1101e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1509
confident_label rate tensor(0.2382, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 747
clean true:733
clean false:14
clean_rate:0.9812583668005355
noisy true:894
noisy false:1495
after delete: len(clean_dataset) 747
after delete: len(noisy_dataset) 2389
epoch [200/200] batch [5/23] time 0.467 (0.504) data 0.337 (0.374) loss_x loss_x 0.8374 (1.0208) acc_x 87.5000 (75.6250) lr 4.9344e-07 eta 0:00:09
epoch [200/200] batch [10/23] time 0.368 (0.454) data 0.238 (0.323) loss_x loss_x 1.0293 (1.1869) acc_x 75.0000 (72.1875) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [15/23] time 0.345 (0.436) data 0.214 (0.305) loss_x loss_x 1.0947 (1.1059) acc_x 75.0000 (73.3333) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [20/23] time 0.393 (0.458) data 0.262 (0.328) loss_x loss_x 1.0137 (1.0548) acc_x 78.1250 (74.6875) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [5/74] time 0.449 (0.444) data 0.318 (0.314) loss_u loss_u 0.9736 (0.9403) acc_u 3.1250 (9.3750) lr 4.9344e-07 eta 0:00:30
epoch [200/200] batch [10/74] time 0.566 (0.443) data 0.435 (0.312) loss_u loss_u 0.9106 (0.9319) acc_u 12.5000 (10.0000) lr 4.9344e-07 eta 0:00:28
epoch [200/200] batch [15/74] time 0.367 (0.442) data 0.236 (0.311) loss_u loss_u 0.9321 (0.9373) acc_u 9.3750 (9.1667) lr 4.9344e-07 eta 0:00:26
epoch [200/200] batch [20/74] time 0.390 (0.436) data 0.259 (0.305) loss_u loss_u 0.9482 (0.9323) acc_u 6.2500 (9.8438) lr 4.9344e-07 eta 0:00:23
epoch [200/200] batch [25/74] time 0.358 (0.434) data 0.226 (0.303) loss_u loss_u 0.9351 (0.9313) acc_u 9.3750 (10.0000) lr 4.9344e-07 eta 0:00:21
epoch [200/200] batch [30/74] time 0.447 (0.442) data 0.316 (0.311) loss_u loss_u 0.9438 (0.9314) acc_u 9.3750 (9.6875) lr 4.9344e-07 eta 0:00:19
epoch [200/200] batch [35/74] time 0.396 (0.443) data 0.265 (0.312) loss_u loss_u 0.9736 (0.9355) acc_u 3.1250 (9.0179) lr 4.9344e-07 eta 0:00:17
epoch [200/200] batch [40/74] time 0.521 (0.443) data 0.390 (0.312) loss_u loss_u 0.9419 (0.9353) acc_u 9.3750 (9.0625) lr 4.9344e-07 eta 0:00:15
epoch [200/200] batch [45/74] time 0.423 (0.445) data 0.292 (0.314) loss_u loss_u 0.9897 (0.9365) acc_u 0.0000 (8.7500) lr 4.9344e-07 eta 0:00:12
epoch [200/200] batch [50/74] time 0.397 (0.439) data 0.266 (0.309) loss_u loss_u 0.8931 (0.9356) acc_u 15.6250 (8.8750) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [55/74] time 0.440 (0.442) data 0.309 (0.311) loss_u loss_u 0.8711 (0.9354) acc_u 18.7500 (8.9773) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [60/74] time 0.398 (0.441) data 0.267 (0.310) loss_u loss_u 0.9844 (0.9362) acc_u 0.0000 (8.7500) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [65/74] time 0.497 (0.440) data 0.366 (0.309) loss_u loss_u 0.8945 (0.9374) acc_u 15.6250 (8.7019) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [70/74] time 0.350 (0.439) data 0.219 (0.308) loss_u loss_u 0.9180 (0.9359) acc_u 9.3750 (8.7946) lr 4.9344e-07 eta 0:00:01
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_sym_0.625/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 4,949
* accuracy: 61.5%
* error: 38.5%
* macro_f1: 60.1%
Elapsed: 5:13:46
