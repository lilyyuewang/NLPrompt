***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.25', 'DATASET.NOISE_TYPE', 'asym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.25/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.25
  NOISE_TYPE: asym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.25/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.25/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.1445, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 453
clean true:429
clean false:24
clean_rate:0.9470198675496688
noisy true:136
noisy false:2547
after delete: len(clean_dataset) 453
after delete: len(noisy_dataset) 2683
epoch [1/200] batch [5/14] time 0.563 (0.524) data 0.434 (0.374) loss_x loss_x 2.8438 (2.9738) acc_x 31.2500 (35.0000) lr 1.0000e-05 eta 0:00:04
epoch [1/200] batch [10/14] time 0.496 (0.524) data 0.366 (0.384) loss_x loss_x 2.2910 (2.8160) acc_x 56.2500 (41.5625) lr 1.0000e-05 eta 0:00:02
epoch [1/200] batch [5/83] time 0.360 (0.502) data 0.229 (0.365) loss_u loss_u 0.8906 (0.9272) acc_u 9.3750 (10.6250) lr 1.0000e-05 eta 0:00:39
epoch [1/200] batch [10/83] time 0.566 (0.494) data 0.435 (0.359) loss_u loss_u 0.9282 (0.9199) acc_u 18.7500 (12.8125) lr 1.0000e-05 eta 0:00:36
epoch [1/200] batch [15/83] time 0.394 (0.482) data 0.264 (0.348) loss_u loss_u 0.8955 (0.9195) acc_u 15.6250 (12.9167) lr 1.0000e-05 eta 0:00:32
epoch [1/200] batch [20/83] time 0.536 (0.475) data 0.406 (0.341) loss_u loss_u 0.9097 (0.9182) acc_u 25.0000 (14.2188) lr 1.0000e-05 eta 0:00:29
epoch [1/200] batch [25/83] time 0.572 (0.469) data 0.441 (0.336) loss_u loss_u 0.8677 (0.9129) acc_u 31.2500 (15.2500) lr 1.0000e-05 eta 0:00:27
epoch [1/200] batch [30/83] time 0.445 (0.469) data 0.314 (0.336) loss_u loss_u 0.9141 (0.9120) acc_u 12.5000 (15.6250) lr 1.0000e-05 eta 0:00:24
epoch [1/200] batch [35/83] time 0.411 (0.466) data 0.280 (0.333) loss_u loss_u 0.8477 (0.9083) acc_u 21.8750 (16.4286) lr 1.0000e-05 eta 0:00:22
epoch [1/200] batch [40/83] time 0.432 (0.466) data 0.302 (0.334) loss_u loss_u 0.9053 (0.9075) acc_u 15.6250 (16.2500) lr 1.0000e-05 eta 0:00:20
epoch [1/200] batch [45/83] time 0.400 (0.464) data 0.269 (0.331) loss_u loss_u 0.9053 (0.9072) acc_u 12.5000 (16.3889) lr 1.0000e-05 eta 0:00:17
epoch [1/200] batch [50/83] time 0.457 (0.464) data 0.327 (0.331) loss_u loss_u 0.8960 (0.9066) acc_u 18.7500 (16.5625) lr 1.0000e-05 eta 0:00:15
epoch [1/200] batch [55/83] time 0.466 (0.469) data 0.335 (0.337) loss_u loss_u 0.9067 (0.9039) acc_u 12.5000 (16.4773) lr 1.0000e-05 eta 0:00:13
epoch [1/200] batch [60/83] time 0.510 (0.470) data 0.379 (0.338) loss_u loss_u 0.9297 (0.9050) acc_u 9.3750 (16.2500) lr 1.0000e-05 eta 0:00:10
epoch [1/200] batch [65/83] time 0.389 (0.468) data 0.259 (0.336) loss_u loss_u 0.8965 (0.9055) acc_u 12.5000 (16.0096) lr 1.0000e-05 eta 0:00:08
epoch [1/200] batch [70/83] time 0.522 (0.470) data 0.390 (0.339) loss_u loss_u 0.8467 (0.9047) acc_u 21.8750 (16.2500) lr 1.0000e-05 eta 0:00:06
epoch [1/200] batch [75/83] time 0.375 (0.467) data 0.244 (0.335) loss_u loss_u 0.9023 (0.9058) acc_u 25.0000 (16.1667) lr 1.0000e-05 eta 0:00:03
epoch [1/200] batch [80/83] time 0.606 (0.469) data 0.473 (0.337) loss_u loss_u 0.8862 (0.9049) acc_u 15.6250 (16.0938) lr 1.0000e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2139
confident_label rate tensor(0.2468, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 774
clean true:757
clean false:17
clean_rate:0.9780361757105943
noisy true:240
noisy false:2122
after delete: len(clean_dataset) 774
after delete: len(noisy_dataset) 2362
epoch [2/200] batch [5/24] time 0.615 (0.497) data 0.485 (0.364) loss_x loss_x 1.7930 (1.9736) acc_x 50.0000 (50.6250) lr 2.0000e-03 eta 0:00:09
epoch [2/200] batch [10/24] time 0.435 (0.466) data 0.304 (0.334) loss_x loss_x 1.9980 (1.9690) acc_x 50.0000 (49.3750) lr 2.0000e-03 eta 0:00:06
epoch [2/200] batch [15/24] time 0.443 (0.465) data 0.312 (0.334) loss_x loss_x 1.9473 (1.9543) acc_x 46.8750 (50.2083) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [20/24] time 0.348 (0.459) data 0.217 (0.328) loss_x loss_x 2.0156 (1.8872) acc_x 43.7500 (50.9375) lr 2.0000e-03 eta 0:00:01
epoch [2/200] batch [5/73] time 0.560 (0.466) data 0.429 (0.335) loss_u loss_u 0.7271 (0.8405) acc_u 31.2500 (21.2500) lr 2.0000e-03 eta 0:00:31
epoch [2/200] batch [10/73] time 0.418 (0.465) data 0.287 (0.334) loss_u loss_u 0.8647 (0.8436) acc_u 18.7500 (21.8750) lr 2.0000e-03 eta 0:00:29
epoch [2/200] batch [15/73] time 0.365 (0.457) data 0.234 (0.326) loss_u loss_u 0.8613 (0.8473) acc_u 18.7500 (21.8750) lr 2.0000e-03 eta 0:00:26
epoch [2/200] batch [20/73] time 0.644 (0.467) data 0.513 (0.336) loss_u loss_u 0.8862 (0.8420) acc_u 9.3750 (21.4062) lr 2.0000e-03 eta 0:00:24
epoch [2/200] batch [25/73] time 0.417 (0.466) data 0.286 (0.335) loss_u loss_u 0.8735 (0.8432) acc_u 18.7500 (20.5000) lr 2.0000e-03 eta 0:00:22
epoch [2/200] batch [30/73] time 0.510 (0.466) data 0.378 (0.335) loss_u loss_u 0.8086 (0.8427) acc_u 21.8750 (20.8333) lr 2.0000e-03 eta 0:00:20
epoch [2/200] batch [35/73] time 0.538 (0.464) data 0.406 (0.333) loss_u loss_u 0.7935 (0.8416) acc_u 21.8750 (20.8036) lr 2.0000e-03 eta 0:00:17
epoch [2/200] batch [40/73] time 0.445 (0.463) data 0.315 (0.332) loss_u loss_u 0.8271 (0.8407) acc_u 21.8750 (20.9375) lr 2.0000e-03 eta 0:00:15
epoch [2/200] batch [45/73] time 0.375 (0.459) data 0.243 (0.328) loss_u loss_u 0.8315 (0.8422) acc_u 25.0000 (20.8333) lr 2.0000e-03 eta 0:00:12
epoch [2/200] batch [50/73] time 0.428 (0.459) data 0.297 (0.328) loss_u loss_u 0.8843 (0.8411) acc_u 15.6250 (20.8750) lr 2.0000e-03 eta 0:00:10
epoch [2/200] batch [55/73] time 0.443 (0.460) data 0.311 (0.330) loss_u loss_u 0.8218 (0.8427) acc_u 15.6250 (20.5114) lr 2.0000e-03 eta 0:00:08
epoch [2/200] batch [60/73] time 0.405 (0.460) data 0.275 (0.329) loss_u loss_u 0.8540 (0.8462) acc_u 15.6250 (19.8438) lr 2.0000e-03 eta 0:00:05
epoch [2/200] batch [65/73] time 0.580 (0.457) data 0.448 (0.326) loss_u loss_u 0.8286 (0.8457) acc_u 25.0000 (19.8558) lr 2.0000e-03 eta 0:00:03
epoch [2/200] batch [70/73] time 0.432 (0.459) data 0.301 (0.328) loss_u loss_u 0.8955 (0.8458) acc_u 12.5000 (19.7321) lr 2.0000e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1874
confident_label rate tensor(0.3221, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1010
clean true:977
clean false:33
clean_rate:0.9673267326732673
noisy true:285
noisy false:1841
after delete: len(clean_dataset) 1010
after delete: len(noisy_dataset) 2126
epoch [3/200] batch [5/31] time 0.438 (0.471) data 0.308 (0.341) loss_x loss_x 1.5479 (1.6002) acc_x 59.3750 (58.7500) lr 1.9999e-03 eta 0:00:12
epoch [3/200] batch [10/31] time 0.528 (0.479) data 0.398 (0.348) loss_x loss_x 1.7188 (1.7007) acc_x 56.2500 (55.3125) lr 1.9999e-03 eta 0:00:10
epoch [3/200] batch [15/31] time 0.504 (0.473) data 0.374 (0.342) loss_x loss_x 1.2793 (1.7192) acc_x 62.5000 (55.6250) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [20/31] time 0.368 (0.465) data 0.236 (0.334) loss_x loss_x 2.0938 (1.7235) acc_x 46.8750 (56.2500) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [25/31] time 0.417 (0.469) data 0.286 (0.339) loss_x loss_x 1.4258 (1.7171) acc_x 68.7500 (57.0000) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [30/31] time 0.412 (0.478) data 0.281 (0.347) loss_x loss_x 1.7793 (1.7596) acc_x 53.1250 (56.0417) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/66] time 0.468 (0.474) data 0.337 (0.343) loss_u loss_u 0.8677 (0.8494) acc_u 12.5000 (21.2500) lr 1.9999e-03 eta 0:00:28
epoch [3/200] batch [10/66] time 0.367 (0.475) data 0.237 (0.344) loss_u loss_u 0.8608 (0.8492) acc_u 18.7500 (19.6875) lr 1.9999e-03 eta 0:00:26
epoch [3/200] batch [15/66] time 0.404 (0.470) data 0.274 (0.339) loss_u loss_u 0.8569 (0.8650) acc_u 12.5000 (16.8750) lr 1.9999e-03 eta 0:00:23
epoch [3/200] batch [20/66] time 0.413 (0.467) data 0.282 (0.336) loss_u loss_u 0.8374 (0.8593) acc_u 18.7500 (17.6562) lr 1.9999e-03 eta 0:00:21
epoch [3/200] batch [25/66] time 0.375 (0.459) data 0.243 (0.328) loss_u loss_u 0.8994 (0.8594) acc_u 6.2500 (17.2500) lr 1.9999e-03 eta 0:00:18
epoch [3/200] batch [30/66] time 0.471 (0.460) data 0.341 (0.329) loss_u loss_u 0.7822 (0.8566) acc_u 25.0000 (17.5000) lr 1.9999e-03 eta 0:00:16
epoch [3/200] batch [35/66] time 0.477 (0.457) data 0.345 (0.326) loss_u loss_u 0.8755 (0.8570) acc_u 18.7500 (17.5000) lr 1.9999e-03 eta 0:00:14
epoch [3/200] batch [40/66] time 0.335 (0.455) data 0.205 (0.325) loss_u loss_u 0.8647 (0.8576) acc_u 12.5000 (17.1094) lr 1.9999e-03 eta 0:00:11
epoch [3/200] batch [45/66] time 0.396 (0.454) data 0.265 (0.323) loss_u loss_u 0.8511 (0.8570) acc_u 15.6250 (17.3611) lr 1.9999e-03 eta 0:00:09
epoch [3/200] batch [50/66] time 0.355 (0.454) data 0.224 (0.323) loss_u loss_u 0.8647 (0.8549) acc_u 9.3750 (17.6875) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [55/66] time 0.372 (0.451) data 0.240 (0.320) loss_u loss_u 0.8384 (0.8543) acc_u 21.8750 (17.7273) lr 1.9999e-03 eta 0:00:04
epoch [3/200] batch [60/66] time 0.501 (0.451) data 0.371 (0.320) loss_u loss_u 0.8486 (0.8552) acc_u 12.5000 (17.3438) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [65/66] time 0.442 (0.451) data 0.312 (0.321) loss_u loss_u 0.8955 (0.8566) acc_u 15.6250 (17.4038) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1887
confident_label rate tensor(0.3144, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 986
clean true:945
clean false:41
clean_rate:0.9584178498985801
noisy true:304
noisy false:1846
after delete: len(clean_dataset) 986
after delete: len(noisy_dataset) 2150
epoch [4/200] batch [5/30] time 0.446 (0.483) data 0.315 (0.352) loss_x loss_x 1.7266 (1.6266) acc_x 53.1250 (58.1250) lr 1.9995e-03 eta 0:00:12
epoch [4/200] batch [10/30] time 0.427 (0.488) data 0.297 (0.358) loss_x loss_x 1.9062 (1.6873) acc_x 46.8750 (56.8750) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [15/30] time 0.394 (0.477) data 0.265 (0.347) loss_x loss_x 1.1875 (1.7380) acc_x 56.2500 (53.5417) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [20/30] time 0.532 (0.475) data 0.402 (0.345) loss_x loss_x 1.8770 (1.7210) acc_x 40.6250 (52.9688) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [25/30] time 0.453 (0.465) data 0.322 (0.335) loss_x loss_x 1.7588 (1.7092) acc_x 59.3750 (53.1250) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [30/30] time 0.385 (0.463) data 0.256 (0.333) loss_x loss_x 2.3184 (1.7177) acc_x 43.7500 (53.8542) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/67] time 0.329 (0.458) data 0.198 (0.328) loss_u loss_u 0.8496 (0.8458) acc_u 21.8750 (18.7500) lr 1.9995e-03 eta 0:00:28
epoch [4/200] batch [10/67] time 0.458 (0.460) data 0.328 (0.329) loss_u loss_u 0.7681 (0.8415) acc_u 34.3750 (19.3750) lr 1.9995e-03 eta 0:00:26
epoch [4/200] batch [15/67] time 0.326 (0.451) data 0.195 (0.321) loss_u loss_u 0.7847 (0.8441) acc_u 21.8750 (18.7500) lr 1.9995e-03 eta 0:00:23
epoch [4/200] batch [20/67] time 0.442 (0.453) data 0.311 (0.323) loss_u loss_u 0.8730 (0.8466) acc_u 15.6250 (18.4375) lr 1.9995e-03 eta 0:00:21
epoch [4/200] batch [25/67] time 0.468 (0.452) data 0.337 (0.322) loss_u loss_u 0.8174 (0.8416) acc_u 25.0000 (19.1250) lr 1.9995e-03 eta 0:00:18
epoch [4/200] batch [30/67] time 0.470 (0.451) data 0.338 (0.320) loss_u loss_u 0.8486 (0.8430) acc_u 25.0000 (19.4792) lr 1.9995e-03 eta 0:00:16
epoch [4/200] batch [35/67] time 0.424 (0.446) data 0.292 (0.316) loss_u loss_u 0.8594 (0.8418) acc_u 18.7500 (19.3750) lr 1.9995e-03 eta 0:00:14
epoch [4/200] batch [40/67] time 0.331 (0.444) data 0.199 (0.313) loss_u loss_u 0.8647 (0.8426) acc_u 15.6250 (19.5312) lr 1.9995e-03 eta 0:00:11
epoch [4/200] batch [45/67] time 0.336 (0.442) data 0.205 (0.312) loss_u loss_u 0.8032 (0.8409) acc_u 21.8750 (19.7917) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [50/67] time 0.396 (0.441) data 0.265 (0.311) loss_u loss_u 0.7949 (0.8387) acc_u 25.0000 (20.3750) lr 1.9995e-03 eta 0:00:07
epoch [4/200] batch [55/67] time 0.413 (0.441) data 0.283 (0.311) loss_u loss_u 0.8740 (0.8416) acc_u 9.3750 (20.0000) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [60/67] time 0.434 (0.439) data 0.304 (0.308) loss_u loss_u 0.8281 (0.8434) acc_u 21.8750 (19.8958) lr 1.9995e-03 eta 0:00:03
epoch [4/200] batch [65/67] time 0.575 (0.441) data 0.445 (0.311) loss_u loss_u 0.8047 (0.8430) acc_u 25.0000 (19.8558) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1853
confident_label rate tensor(0.3211, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1007
clean true:979
clean false:28
clean_rate:0.9721946375372393
noisy true:304
noisy false:1825
after delete: len(clean_dataset) 1007
after delete: len(noisy_dataset) 2129
epoch [5/200] batch [5/31] time 0.430 (0.445) data 0.300 (0.315) loss_x loss_x 1.8711 (1.8598) acc_x 56.2500 (56.2500) lr 1.9989e-03 eta 0:00:11
epoch [5/200] batch [10/31] time 0.482 (0.435) data 0.351 (0.305) loss_x loss_x 1.8838 (1.7220) acc_x 62.5000 (58.4375) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [15/31] time 0.347 (0.429) data 0.218 (0.299) loss_x loss_x 1.5547 (1.6315) acc_x 56.2500 (58.1250) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [20/31] time 0.460 (0.432) data 0.330 (0.302) loss_x loss_x 1.8457 (1.5926) acc_x 59.3750 (58.4375) lr 1.9989e-03 eta 0:00:04
epoch [5/200] batch [25/31] time 0.343 (0.432) data 0.212 (0.302) loss_x loss_x 1.5566 (1.5950) acc_x 56.2500 (57.6250) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [30/31] time 0.493 (0.447) data 0.362 (0.317) loss_x loss_x 1.5186 (1.5803) acc_x 62.5000 (57.9167) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/66] time 0.400 (0.447) data 0.269 (0.317) loss_u loss_u 0.8711 (0.8633) acc_u 18.7500 (18.7500) lr 1.9989e-03 eta 0:00:27
epoch [5/200] batch [10/66] time 0.376 (0.450) data 0.244 (0.319) loss_u loss_u 0.8013 (0.8529) acc_u 25.0000 (20.0000) lr 1.9989e-03 eta 0:00:25
epoch [5/200] batch [15/66] time 0.450 (0.450) data 0.319 (0.319) loss_u loss_u 0.8389 (0.8633) acc_u 21.8750 (17.7083) lr 1.9989e-03 eta 0:00:22
epoch [5/200] batch [20/66] time 0.581 (0.452) data 0.450 (0.321) loss_u loss_u 0.8716 (0.8583) acc_u 12.5000 (18.2812) lr 1.9989e-03 eta 0:00:20
epoch [5/200] batch [25/66] time 0.404 (0.448) data 0.274 (0.317) loss_u loss_u 0.9126 (0.8546) acc_u 6.2500 (18.5000) lr 1.9989e-03 eta 0:00:18
epoch [5/200] batch [30/66] time 0.466 (0.445) data 0.334 (0.315) loss_u loss_u 0.8940 (0.8550) acc_u 15.6250 (18.5417) lr 1.9989e-03 eta 0:00:16
epoch [5/200] batch [35/66] time 0.466 (0.443) data 0.335 (0.312) loss_u loss_u 0.8804 (0.8553) acc_u 9.3750 (18.5714) lr 1.9989e-03 eta 0:00:13
epoch [5/200] batch [40/66] time 0.397 (0.445) data 0.267 (0.314) loss_u loss_u 0.8574 (0.8565) acc_u 21.8750 (18.6719) lr 1.9989e-03 eta 0:00:11
epoch [5/200] batch [45/66] time 0.597 (0.448) data 0.467 (0.318) loss_u loss_u 0.8315 (0.8555) acc_u 21.8750 (18.6806) lr 1.9989e-03 eta 0:00:09
epoch [5/200] batch [50/66] time 0.522 (0.447) data 0.390 (0.316) loss_u loss_u 0.8125 (0.8540) acc_u 28.1250 (19.5000) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [55/66] time 0.485 (0.446) data 0.354 (0.315) loss_u loss_u 0.7764 (0.8511) acc_u 25.0000 (19.8295) lr 1.9989e-03 eta 0:00:04
epoch [5/200] batch [60/66] time 0.398 (0.447) data 0.268 (0.316) loss_u loss_u 0.8906 (0.8511) acc_u 9.3750 (19.6354) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [65/66] time 0.500 (0.446) data 0.370 (0.316) loss_u loss_u 0.8882 (0.8508) acc_u 18.7500 (19.7115) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1852
confident_label rate tensor(0.3294, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1033
clean true:995
clean false:38
clean_rate:0.9632139399806389
noisy true:289
noisy false:1814
after delete: len(clean_dataset) 1033
after delete: len(noisy_dataset) 2103
epoch [6/200] batch [5/32] time 0.397 (0.433) data 0.266 (0.302) loss_x loss_x 1.9043 (1.6477) acc_x 50.0000 (58.1250) lr 1.9980e-03 eta 0:00:11
epoch [6/200] batch [10/32] time 0.401 (0.444) data 0.271 (0.314) loss_x loss_x 1.8496 (1.5418) acc_x 50.0000 (59.6875) lr 1.9980e-03 eta 0:00:09
epoch [6/200] batch [15/32] time 0.550 (0.445) data 0.420 (0.314) loss_x loss_x 1.8398 (1.6024) acc_x 59.3750 (58.7500) lr 1.9980e-03 eta 0:00:07
epoch [6/200] batch [20/32] time 0.471 (0.454) data 0.341 (0.323) loss_x loss_x 1.1201 (1.5789) acc_x 75.0000 (60.1562) lr 1.9980e-03 eta 0:00:05
epoch [6/200] batch [25/32] time 0.413 (0.448) data 0.282 (0.318) loss_x loss_x 1.9062 (1.5662) acc_x 53.1250 (59.5000) lr 1.9980e-03 eta 0:00:03
epoch [6/200] batch [30/32] time 0.418 (0.441) data 0.288 (0.311) loss_x loss_x 1.3447 (1.5414) acc_x 71.8750 (59.7917) lr 1.9980e-03 eta 0:00:00
epoch [6/200] batch [5/65] time 0.472 (0.450) data 0.341 (0.320) loss_u loss_u 0.8335 (0.8602) acc_u 25.0000 (21.2500) lr 1.9980e-03 eta 0:00:27
epoch [6/200] batch [10/65] time 0.420 (0.458) data 0.289 (0.327) loss_u loss_u 0.8774 (0.8493) acc_u 15.6250 (21.8750) lr 1.9980e-03 eta 0:00:25
epoch [6/200] batch [15/65] time 0.458 (0.454) data 0.327 (0.324) loss_u loss_u 0.8774 (0.8486) acc_u 15.6250 (20.8333) lr 1.9980e-03 eta 0:00:22
epoch [6/200] batch [20/65] time 0.470 (0.454) data 0.339 (0.324) loss_u loss_u 0.9175 (0.8518) acc_u 9.3750 (20.6250) lr 1.9980e-03 eta 0:00:20
epoch [6/200] batch [25/65] time 0.326 (0.449) data 0.195 (0.319) loss_u loss_u 0.8750 (0.8516) acc_u 21.8750 (20.3750) lr 1.9980e-03 eta 0:00:17
epoch [6/200] batch [30/65] time 0.362 (0.448) data 0.230 (0.318) loss_u loss_u 0.8794 (0.8539) acc_u 15.6250 (19.7917) lr 1.9980e-03 eta 0:00:15
epoch [6/200] batch [35/65] time 0.371 (0.447) data 0.240 (0.316) loss_u loss_u 0.8984 (0.8557) acc_u 9.3750 (19.0179) lr 1.9980e-03 eta 0:00:13
epoch [6/200] batch [40/65] time 0.414 (0.446) data 0.282 (0.315) loss_u loss_u 0.8975 (0.8548) acc_u 15.6250 (19.1406) lr 1.9980e-03 eta 0:00:11
epoch [6/200] batch [45/65] time 0.355 (0.441) data 0.225 (0.311) loss_u loss_u 0.8657 (0.8545) acc_u 18.7500 (19.3056) lr 1.9980e-03 eta 0:00:08
epoch [6/200] batch [50/65] time 0.371 (0.442) data 0.240 (0.312) loss_u loss_u 0.8887 (0.8556) acc_u 18.7500 (19.2500) lr 1.9980e-03 eta 0:00:06
epoch [6/200] batch [55/65] time 0.481 (0.443) data 0.350 (0.312) loss_u loss_u 0.9160 (0.8553) acc_u 12.5000 (19.3182) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [60/65] time 0.394 (0.444) data 0.263 (0.313) loss_u loss_u 0.9136 (0.8546) acc_u 9.3750 (19.1146) lr 1.9980e-03 eta 0:00:02
epoch [6/200] batch [65/65] time 0.477 (0.445) data 0.346 (0.314) loss_u loss_u 0.8711 (0.8543) acc_u 12.5000 (19.1346) lr 1.9980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1837
confident_label rate tensor(0.3307, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1037
clean true:1000
clean false:37
clean_rate:0.9643201542912246
noisy true:299
noisy false:1800
after delete: len(clean_dataset) 1037
after delete: len(noisy_dataset) 2099
epoch [7/200] batch [5/32] time 0.428 (0.427) data 0.297 (0.297) loss_x loss_x 1.5820 (1.4961) acc_x 50.0000 (56.8750) lr 1.9969e-03 eta 0:00:11
epoch [7/200] batch [10/32] time 0.427 (0.427) data 0.297 (0.297) loss_x loss_x 1.4922 (1.5123) acc_x 53.1250 (58.1250) lr 1.9969e-03 eta 0:00:09
epoch [7/200] batch [15/32] time 0.393 (0.423) data 0.262 (0.292) loss_x loss_x 2.2090 (1.5984) acc_x 37.5000 (57.2917) lr 1.9969e-03 eta 0:00:07
epoch [7/200] batch [20/32] time 0.573 (0.440) data 0.443 (0.310) loss_x loss_x 1.2793 (1.5558) acc_x 59.3750 (59.2188) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [25/32] time 0.418 (0.444) data 0.287 (0.313) loss_x loss_x 2.0020 (1.6019) acc_x 53.1250 (58.2500) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [30/32] time 0.456 (0.442) data 0.326 (0.312) loss_x loss_x 1.8525 (1.5790) acc_x 56.2500 (59.2708) lr 1.9969e-03 eta 0:00:00
epoch [7/200] batch [5/65] time 0.353 (0.444) data 0.224 (0.314) loss_u loss_u 0.8271 (0.8578) acc_u 25.0000 (20.6250) lr 1.9969e-03 eta 0:00:26
epoch [7/200] batch [10/65] time 0.532 (0.439) data 0.400 (0.309) loss_u loss_u 0.8564 (0.8486) acc_u 12.5000 (20.3125) lr 1.9969e-03 eta 0:00:24
epoch [7/200] batch [15/65] time 0.346 (0.438) data 0.215 (0.308) loss_u loss_u 0.8237 (0.8512) acc_u 28.1250 (20.4167) lr 1.9969e-03 eta 0:00:21
epoch [7/200] batch [20/65] time 0.411 (0.439) data 0.280 (0.308) loss_u loss_u 0.8530 (0.8470) acc_u 18.7500 (20.9375) lr 1.9969e-03 eta 0:00:19
epoch [7/200] batch [25/65] time 0.355 (0.433) data 0.224 (0.303) loss_u loss_u 0.7852 (0.8420) acc_u 34.3750 (21.5000) lr 1.9969e-03 eta 0:00:17
epoch [7/200] batch [30/65] time 0.438 (0.437) data 0.307 (0.306) loss_u loss_u 0.8384 (0.8463) acc_u 15.6250 (19.8958) lr 1.9969e-03 eta 0:00:15
epoch [7/200] batch [35/65] time 0.478 (0.438) data 0.346 (0.307) loss_u loss_u 0.8423 (0.8507) acc_u 18.7500 (19.3750) lr 1.9969e-03 eta 0:00:13
epoch [7/200] batch [40/65] time 0.522 (0.439) data 0.391 (0.308) loss_u loss_u 0.8452 (0.8525) acc_u 18.7500 (19.2969) lr 1.9969e-03 eta 0:00:10
epoch [7/200] batch [45/65] time 0.419 (0.440) data 0.289 (0.309) loss_u loss_u 0.8579 (0.8553) acc_u 25.0000 (19.0972) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [50/65] time 0.465 (0.442) data 0.335 (0.311) loss_u loss_u 0.8276 (0.8541) acc_u 25.0000 (19.4375) lr 1.9969e-03 eta 0:00:06
epoch [7/200] batch [55/65] time 0.373 (0.444) data 0.241 (0.313) loss_u loss_u 0.9199 (0.8546) acc_u 12.5000 (19.4318) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [60/65] time 0.405 (0.444) data 0.274 (0.313) loss_u loss_u 0.8408 (0.8539) acc_u 15.6250 (19.3750) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [65/65] time 0.569 (0.445) data 0.439 (0.315) loss_u loss_u 0.8389 (0.8557) acc_u 18.7500 (19.1346) lr 1.9969e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1751
confident_label rate tensor(0.3498, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1097
clean true:1061
clean false:36
clean_rate:0.96718322698268
noisy true:324
noisy false:1715
after delete: len(clean_dataset) 1097
after delete: len(noisy_dataset) 2039
epoch [8/200] batch [5/34] time 0.461 (0.475) data 0.331 (0.345) loss_x loss_x 1.3027 (1.3865) acc_x 59.3750 (64.3750) lr 1.9956e-03 eta 0:00:13
epoch [8/200] batch [10/34] time 0.364 (0.442) data 0.234 (0.312) loss_x loss_x 1.7461 (1.5348) acc_x 56.2500 (61.5625) lr 1.9956e-03 eta 0:00:10
epoch [8/200] batch [15/34] time 0.412 (0.451) data 0.282 (0.321) loss_x loss_x 1.2793 (1.5562) acc_x 68.7500 (60.4167) lr 1.9956e-03 eta 0:00:08
epoch [8/200] batch [20/34] time 0.628 (0.477) data 0.498 (0.347) loss_x loss_x 1.4824 (1.5334) acc_x 62.5000 (61.7188) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [25/34] time 0.535 (0.477) data 0.405 (0.347) loss_x loss_x 1.6309 (1.5483) acc_x 59.3750 (61.8750) lr 1.9956e-03 eta 0:00:04
epoch [8/200] batch [30/34] time 0.367 (0.468) data 0.237 (0.338) loss_x loss_x 1.9502 (1.5649) acc_x 43.7500 (60.9375) lr 1.9956e-03 eta 0:00:01
epoch [8/200] batch [5/63] time 0.325 (0.456) data 0.194 (0.325) loss_u loss_u 0.8486 (0.8214) acc_u 15.6250 (21.2500) lr 1.9956e-03 eta 0:00:26
epoch [8/200] batch [10/63] time 0.432 (0.454) data 0.302 (0.324) loss_u loss_u 0.7905 (0.8329) acc_u 25.0000 (20.0000) lr 1.9956e-03 eta 0:00:24
epoch [8/200] batch [15/63] time 0.378 (0.449) data 0.248 (0.319) loss_u loss_u 0.8101 (0.8404) acc_u 21.8750 (19.1667) lr 1.9956e-03 eta 0:00:21
epoch [8/200] batch [20/63] time 0.610 (0.448) data 0.479 (0.318) loss_u loss_u 0.8843 (0.8471) acc_u 12.5000 (17.5000) lr 1.9956e-03 eta 0:00:19
epoch [8/200] batch [25/63] time 0.440 (0.445) data 0.309 (0.315) loss_u loss_u 0.8618 (0.8515) acc_u 18.7500 (17.3750) lr 1.9956e-03 eta 0:00:16
epoch [8/200] batch [30/63] time 0.426 (0.443) data 0.294 (0.313) loss_u loss_u 0.7871 (0.8521) acc_u 31.2500 (17.8125) lr 1.9956e-03 eta 0:00:14
epoch [8/200] batch [35/63] time 0.399 (0.444) data 0.268 (0.314) loss_u loss_u 0.8604 (0.8510) acc_u 21.8750 (18.8393) lr 1.9956e-03 eta 0:00:12
epoch [8/200] batch [40/63] time 0.312 (0.440) data 0.182 (0.309) loss_u loss_u 0.8911 (0.8526) acc_u 15.6250 (18.5938) lr 1.9956e-03 eta 0:00:10
epoch [8/200] batch [45/63] time 0.415 (0.441) data 0.283 (0.311) loss_u loss_u 0.8496 (0.8473) acc_u 28.1250 (19.5139) lr 1.9956e-03 eta 0:00:07
epoch [8/200] batch [50/63] time 0.452 (0.442) data 0.321 (0.311) loss_u loss_u 0.9004 (0.8462) acc_u 9.3750 (19.5000) lr 1.9956e-03 eta 0:00:05
epoch [8/200] batch [55/63] time 0.448 (0.447) data 0.317 (0.316) loss_u loss_u 0.7769 (0.8467) acc_u 25.0000 (19.0341) lr 1.9956e-03 eta 0:00:03
epoch [8/200] batch [60/63] time 0.389 (0.446) data 0.259 (0.316) loss_u loss_u 0.8926 (0.8482) acc_u 12.5000 (18.8021) lr 1.9956e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1801
confident_label rate tensor(0.3377, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1059
clean true:1026
clean false:33
clean_rate:0.9688385269121813
noisy true:309
noisy false:1768
after delete: len(clean_dataset) 1059
after delete: len(noisy_dataset) 2077
epoch [9/200] batch [5/33] time 0.453 (0.459) data 0.323 (0.329) loss_x loss_x 1.5605 (1.4488) acc_x 50.0000 (60.0000) lr 1.9940e-03 eta 0:00:12
epoch [9/200] batch [10/33] time 0.474 (0.451) data 0.344 (0.321) loss_x loss_x 1.2354 (1.3971) acc_x 62.5000 (61.2500) lr 1.9940e-03 eta 0:00:10
epoch [9/200] batch [15/33] time 0.412 (0.451) data 0.282 (0.321) loss_x loss_x 1.2471 (1.4227) acc_x 59.3750 (59.5833) lr 1.9940e-03 eta 0:00:08
epoch [9/200] batch [20/33] time 0.625 (0.464) data 0.495 (0.333) loss_x loss_x 1.5137 (1.4982) acc_x 56.2500 (58.4375) lr 1.9940e-03 eta 0:00:06
epoch [9/200] batch [25/33] time 0.447 (0.461) data 0.317 (0.330) loss_x loss_x 1.3320 (1.5172) acc_x 68.7500 (59.5000) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [30/33] time 0.389 (0.462) data 0.259 (0.331) loss_x loss_x 1.5508 (1.5263) acc_x 50.0000 (58.9583) lr 1.9940e-03 eta 0:00:01
epoch [9/200] batch [5/64] time 0.451 (0.457) data 0.321 (0.327) loss_u loss_u 0.9438 (0.8768) acc_u 9.3750 (15.6250) lr 1.9940e-03 eta 0:00:26
epoch [9/200] batch [10/64] time 0.778 (0.461) data 0.647 (0.330) loss_u loss_u 0.8853 (0.8682) acc_u 18.7500 (17.8125) lr 1.9940e-03 eta 0:00:24
epoch [9/200] batch [15/64] time 0.444 (0.454) data 0.312 (0.324) loss_u loss_u 0.8853 (0.8715) acc_u 6.2500 (16.8750) lr 1.9940e-03 eta 0:00:22
epoch [9/200] batch [20/64] time 0.477 (0.456) data 0.347 (0.325) loss_u loss_u 0.8721 (0.8666) acc_u 12.5000 (17.5000) lr 1.9940e-03 eta 0:00:20
epoch [9/200] batch [25/64] time 0.472 (0.452) data 0.341 (0.322) loss_u loss_u 0.8940 (0.8641) acc_u 12.5000 (18.3750) lr 1.9940e-03 eta 0:00:17
epoch [9/200] batch [30/64] time 0.564 (0.454) data 0.433 (0.323) loss_u loss_u 0.8032 (0.8625) acc_u 37.5000 (19.3750) lr 1.9940e-03 eta 0:00:15
epoch [9/200] batch [35/64] time 0.364 (0.453) data 0.233 (0.323) loss_u loss_u 0.7866 (0.8591) acc_u 28.1250 (19.1964) lr 1.9940e-03 eta 0:00:13
epoch [9/200] batch [40/64] time 0.404 (0.454) data 0.272 (0.324) loss_u loss_u 0.8042 (0.8573) acc_u 21.8750 (18.6719) lr 1.9940e-03 eta 0:00:10
epoch [9/200] batch [45/64] time 0.443 (0.451) data 0.313 (0.321) loss_u loss_u 0.8755 (0.8559) acc_u 18.7500 (18.4722) lr 1.9940e-03 eta 0:00:08
epoch [9/200] batch [50/64] time 0.448 (0.452) data 0.318 (0.321) loss_u loss_u 0.8462 (0.8537) acc_u 18.7500 (18.9375) lr 1.9940e-03 eta 0:00:06
epoch [9/200] batch [55/64] time 0.469 (0.451) data 0.337 (0.320) loss_u loss_u 0.8843 (0.8556) acc_u 15.6250 (18.4091) lr 1.9940e-03 eta 0:00:04
epoch [9/200] batch [60/64] time 0.404 (0.450) data 0.273 (0.320) loss_u loss_u 0.7808 (0.8524) acc_u 28.1250 (18.6979) lr 1.9940e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1738
confident_label rate tensor(0.3425, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1074
clean true:1052
clean false:22
clean_rate:0.9795158286778398
noisy true:346
noisy false:1716
after delete: len(clean_dataset) 1074
after delete: len(noisy_dataset) 2062
epoch [10/200] batch [5/33] time 0.545 (0.494) data 0.414 (0.363) loss_x loss_x 1.6064 (1.6398) acc_x 46.8750 (56.2500) lr 1.9921e-03 eta 0:00:13
epoch [10/200] batch [10/33] time 0.474 (0.462) data 0.344 (0.332) loss_x loss_x 1.6709 (1.5287) acc_x 56.2500 (57.5000) lr 1.9921e-03 eta 0:00:10
epoch [10/200] batch [15/33] time 0.397 (0.462) data 0.266 (0.332) loss_x loss_x 1.7393 (1.5368) acc_x 62.5000 (58.7500) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [20/33] time 0.359 (0.454) data 0.229 (0.323) loss_x loss_x 1.5537 (1.5860) acc_x 68.7500 (58.9062) lr 1.9921e-03 eta 0:00:05
epoch [10/200] batch [25/33] time 0.371 (0.453) data 0.241 (0.323) loss_x loss_x 1.5996 (1.5633) acc_x 56.2500 (58.8750) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [30/33] time 0.663 (0.467) data 0.534 (0.337) loss_x loss_x 1.6299 (1.5445) acc_x 59.3750 (60.1042) lr 1.9921e-03 eta 0:00:01
epoch [10/200] batch [5/64] time 0.483 (0.473) data 0.353 (0.343) loss_u loss_u 0.8975 (0.8807) acc_u 12.5000 (17.5000) lr 1.9921e-03 eta 0:00:27
epoch [10/200] batch [10/64] time 0.431 (0.468) data 0.300 (0.337) loss_u loss_u 0.8384 (0.8674) acc_u 21.8750 (17.1875) lr 1.9921e-03 eta 0:00:25
epoch [10/200] batch [15/64] time 0.570 (0.472) data 0.438 (0.341) loss_u loss_u 0.8735 (0.8616) acc_u 15.6250 (18.3333) lr 1.9921e-03 eta 0:00:23
epoch [10/200] batch [20/64] time 0.402 (0.472) data 0.270 (0.341) loss_u loss_u 0.8467 (0.8605) acc_u 18.7500 (18.4375) lr 1.9921e-03 eta 0:00:20
epoch [10/200] batch [25/64] time 0.640 (0.473) data 0.509 (0.342) loss_u loss_u 0.8838 (0.8600) acc_u 9.3750 (18.6250) lr 1.9921e-03 eta 0:00:18
epoch [10/200] batch [30/64] time 0.399 (0.466) data 0.268 (0.335) loss_u loss_u 0.8555 (0.8623) acc_u 18.7500 (17.9167) lr 1.9921e-03 eta 0:00:15
epoch [10/200] batch [35/64] time 0.389 (0.463) data 0.258 (0.332) loss_u loss_u 0.8022 (0.8594) acc_u 25.0000 (18.6607) lr 1.9921e-03 eta 0:00:13
epoch [10/200] batch [40/64] time 0.675 (0.466) data 0.545 (0.335) loss_u loss_u 0.8784 (0.8592) acc_u 15.6250 (18.6719) lr 1.9921e-03 eta 0:00:11
epoch [10/200] batch [45/64] time 0.412 (0.463) data 0.281 (0.332) loss_u loss_u 0.8101 (0.8595) acc_u 21.8750 (18.8889) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [50/64] time 0.413 (0.463) data 0.282 (0.333) loss_u loss_u 0.8325 (0.8567) acc_u 18.7500 (18.8750) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [55/64] time 0.328 (0.461) data 0.198 (0.330) loss_u loss_u 0.8086 (0.8525) acc_u 21.8750 (19.6023) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [60/64] time 0.474 (0.459) data 0.342 (0.328) loss_u loss_u 0.8013 (0.8502) acc_u 18.7500 (19.7917) lr 1.9921e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1769
confident_label rate tensor(0.3415, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1071
clean true:1039
clean false:32
clean_rate:0.9701213818860878
noisy true:328
noisy false:1737
after delete: len(clean_dataset) 1071
after delete: len(noisy_dataset) 2065
epoch [11/200] batch [5/33] time 0.327 (0.485) data 0.198 (0.355) loss_x loss_x 2.0938 (1.5600) acc_x 59.3750 (60.6250) lr 1.9900e-03 eta 0:00:13
epoch [11/200] batch [10/33] time 0.456 (0.458) data 0.327 (0.328) loss_x loss_x 1.4863 (1.4582) acc_x 62.5000 (61.2500) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [15/33] time 0.316 (0.455) data 0.186 (0.325) loss_x loss_x 1.2588 (1.4704) acc_x 71.8750 (60.8333) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [20/33] time 0.376 (0.453) data 0.246 (0.323) loss_x loss_x 1.5840 (1.4564) acc_x 53.1250 (61.4062) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [25/33] time 0.391 (0.459) data 0.261 (0.329) loss_x loss_x 1.3984 (1.4878) acc_x 68.7500 (61.0000) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [30/33] time 0.427 (0.456) data 0.297 (0.326) loss_x loss_x 1.8096 (1.4812) acc_x 71.8750 (61.7708) lr 1.9900e-03 eta 0:00:01
epoch [11/200] batch [5/64] time 0.477 (0.461) data 0.346 (0.331) loss_u loss_u 0.8267 (0.8498) acc_u 18.7500 (15.6250) lr 1.9900e-03 eta 0:00:27
epoch [11/200] batch [10/64] time 0.345 (0.457) data 0.214 (0.327) loss_u loss_u 0.8403 (0.8455) acc_u 21.8750 (17.8125) lr 1.9900e-03 eta 0:00:24
epoch [11/200] batch [15/64] time 0.376 (0.450) data 0.244 (0.320) loss_u loss_u 0.8750 (0.8509) acc_u 21.8750 (18.9583) lr 1.9900e-03 eta 0:00:22
epoch [11/200] batch [20/64] time 0.362 (0.448) data 0.232 (0.318) loss_u loss_u 0.8311 (0.8460) acc_u 31.2500 (20.0000) lr 1.9900e-03 eta 0:00:19
epoch [11/200] batch [25/64] time 0.359 (0.444) data 0.227 (0.313) loss_u loss_u 0.8604 (0.8487) acc_u 15.6250 (20.2500) lr 1.9900e-03 eta 0:00:17
epoch [11/200] batch [30/64] time 0.408 (0.445) data 0.278 (0.315) loss_u loss_u 0.8525 (0.8528) acc_u 15.6250 (18.7500) lr 1.9900e-03 eta 0:00:15
epoch [11/200] batch [35/64] time 0.407 (0.447) data 0.276 (0.316) loss_u loss_u 0.8750 (0.8480) acc_u 15.6250 (19.4643) lr 1.9900e-03 eta 0:00:12
epoch [11/200] batch [40/64] time 0.413 (0.445) data 0.282 (0.315) loss_u loss_u 0.8008 (0.8452) acc_u 31.2500 (20.0781) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [45/64] time 0.388 (0.446) data 0.258 (0.315) loss_u loss_u 0.8633 (0.8463) acc_u 12.5000 (20.0694) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [50/64] time 0.457 (0.448) data 0.327 (0.317) loss_u loss_u 0.8091 (0.8458) acc_u 28.1250 (20.0000) lr 1.9900e-03 eta 0:00:06
epoch [11/200] batch [55/64] time 0.429 (0.448) data 0.298 (0.318) loss_u loss_u 0.8037 (0.8446) acc_u 25.0000 (20.3977) lr 1.9900e-03 eta 0:00:04
epoch [11/200] batch [60/64] time 0.395 (0.446) data 0.264 (0.315) loss_u loss_u 0.8491 (0.8453) acc_u 18.7500 (20.1562) lr 1.9900e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1781
confident_label rate tensor(0.3415, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1071
clean true:1044
clean false:27
clean_rate:0.9747899159663865
noisy true:311
noisy false:1754
after delete: len(clean_dataset) 1071
after delete: len(noisy_dataset) 2065
epoch [12/200] batch [5/33] time 0.358 (0.484) data 0.228 (0.354) loss_x loss_x 1.0449 (1.4984) acc_x 75.0000 (60.6250) lr 1.9877e-03 eta 0:00:13
epoch [12/200] batch [10/33] time 0.370 (0.458) data 0.240 (0.328) loss_x loss_x 1.3770 (1.4921) acc_x 65.6250 (62.8125) lr 1.9877e-03 eta 0:00:10
epoch [12/200] batch [15/33] time 0.488 (0.464) data 0.357 (0.334) loss_x loss_x 1.6123 (1.4862) acc_x 62.5000 (61.4583) lr 1.9877e-03 eta 0:00:08
epoch [12/200] batch [20/33] time 0.389 (0.469) data 0.258 (0.339) loss_x loss_x 0.9443 (1.4920) acc_x 71.8750 (61.2500) lr 1.9877e-03 eta 0:00:06
epoch [12/200] batch [25/33] time 0.529 (0.478) data 0.398 (0.348) loss_x loss_x 1.1523 (1.4446) acc_x 68.7500 (62.6250) lr 1.9877e-03 eta 0:00:03
epoch [12/200] batch [30/33] time 0.555 (0.484) data 0.424 (0.354) loss_x loss_x 1.6445 (1.4703) acc_x 50.0000 (61.1458) lr 1.9877e-03 eta 0:00:01
epoch [12/200] batch [5/64] time 0.531 (0.485) data 0.400 (0.354) loss_u loss_u 0.8042 (0.8159) acc_u 25.0000 (21.8750) lr 1.9877e-03 eta 0:00:28
epoch [12/200] batch [10/64] time 0.443 (0.472) data 0.312 (0.341) loss_u loss_u 0.8433 (0.8230) acc_u 18.7500 (23.4375) lr 1.9877e-03 eta 0:00:25
epoch [12/200] batch [15/64] time 0.412 (0.465) data 0.281 (0.335) loss_u loss_u 0.7505 (0.8142) acc_u 25.0000 (22.9167) lr 1.9877e-03 eta 0:00:22
epoch [12/200] batch [20/64] time 0.495 (0.465) data 0.364 (0.334) loss_u loss_u 0.8730 (0.8297) acc_u 12.5000 (20.7812) lr 1.9877e-03 eta 0:00:20
epoch [12/200] batch [25/64] time 0.552 (0.459) data 0.421 (0.328) loss_u loss_u 0.8179 (0.8335) acc_u 25.0000 (20.7500) lr 1.9877e-03 eta 0:00:17
epoch [12/200] batch [30/64] time 0.480 (0.458) data 0.349 (0.327) loss_u loss_u 0.8159 (0.8380) acc_u 25.0000 (19.8958) lr 1.9877e-03 eta 0:00:15
epoch [12/200] batch [35/64] time 0.561 (0.456) data 0.429 (0.326) loss_u loss_u 0.8550 (0.8429) acc_u 15.6250 (19.4643) lr 1.9877e-03 eta 0:00:13
epoch [12/200] batch [40/64] time 0.303 (0.453) data 0.173 (0.322) loss_u loss_u 0.8872 (0.8461) acc_u 15.6250 (18.9844) lr 1.9877e-03 eta 0:00:10
epoch [12/200] batch [45/64] time 0.371 (0.450) data 0.241 (0.319) loss_u loss_u 0.8218 (0.8469) acc_u 21.8750 (18.8889) lr 1.9877e-03 eta 0:00:08
epoch [12/200] batch [50/64] time 0.408 (0.448) data 0.277 (0.317) loss_u loss_u 0.8550 (0.8460) acc_u 18.7500 (18.8750) lr 1.9877e-03 eta 0:00:06
epoch [12/200] batch [55/64] time 0.394 (0.446) data 0.263 (0.316) loss_u loss_u 0.8799 (0.8488) acc_u 12.5000 (18.5795) lr 1.9877e-03 eta 0:00:04
epoch [12/200] batch [60/64] time 0.536 (0.448) data 0.405 (0.317) loss_u loss_u 0.8867 (0.8500) acc_u 18.7500 (18.3854) lr 1.9877e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1792
confident_label rate tensor(0.3332, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1045
clean true:1014
clean false:31
clean_rate:0.970334928229665
noisy true:330
noisy false:1761
after delete: len(clean_dataset) 1045
after delete: len(noisy_dataset) 2091
epoch [13/200] batch [5/32] time 0.598 (0.480) data 0.467 (0.350) loss_x loss_x 1.3564 (1.5633) acc_x 59.3750 (61.8750) lr 1.9851e-03 eta 0:00:12
epoch [13/200] batch [10/32] time 0.461 (0.491) data 0.330 (0.360) loss_x loss_x 1.6826 (1.5127) acc_x 56.2500 (61.5625) lr 1.9851e-03 eta 0:00:10
epoch [13/200] batch [15/32] time 0.380 (0.466) data 0.249 (0.336) loss_x loss_x 1.4688 (1.4602) acc_x 56.2500 (62.7083) lr 1.9851e-03 eta 0:00:07
epoch [13/200] batch [20/32] time 0.390 (0.471) data 0.259 (0.341) loss_x loss_x 1.5117 (1.4339) acc_x 62.5000 (62.9688) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [25/32] time 0.405 (0.486) data 0.274 (0.356) loss_x loss_x 1.8740 (1.4246) acc_x 46.8750 (62.2500) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [30/32] time 0.499 (0.476) data 0.369 (0.345) loss_x loss_x 1.6982 (1.4435) acc_x 50.0000 (61.6667) lr 1.9851e-03 eta 0:00:00
epoch [13/200] batch [5/65] time 0.398 (0.472) data 0.267 (0.341) loss_u loss_u 0.7656 (0.8237) acc_u 31.2500 (25.6250) lr 1.9851e-03 eta 0:00:28
epoch [13/200] batch [10/65] time 0.517 (0.468) data 0.387 (0.338) loss_u loss_u 0.8584 (0.8406) acc_u 15.6250 (21.8750) lr 1.9851e-03 eta 0:00:25
epoch [13/200] batch [15/65] time 0.428 (0.467) data 0.297 (0.336) loss_u loss_u 0.8384 (0.8390) acc_u 12.5000 (20.4167) lr 1.9851e-03 eta 0:00:23
epoch [13/200] batch [20/65] time 0.472 (0.461) data 0.341 (0.330) loss_u loss_u 0.7983 (0.8350) acc_u 25.0000 (20.6250) lr 1.9851e-03 eta 0:00:20
epoch [13/200] batch [25/65] time 0.485 (0.469) data 0.355 (0.338) loss_u loss_u 0.9238 (0.8415) acc_u 9.3750 (19.5000) lr 1.9851e-03 eta 0:00:18
epoch [13/200] batch [30/65] time 0.460 (0.471) data 0.328 (0.340) loss_u loss_u 0.8618 (0.8437) acc_u 18.7500 (19.4792) lr 1.9851e-03 eta 0:00:16
epoch [13/200] batch [35/65] time 0.419 (0.473) data 0.287 (0.343) loss_u loss_u 0.8398 (0.8379) acc_u 18.7500 (20.7143) lr 1.9851e-03 eta 0:00:14
epoch [13/200] batch [40/65] time 0.362 (0.472) data 0.231 (0.341) loss_u loss_u 0.8848 (0.8391) acc_u 18.7500 (20.6250) lr 1.9851e-03 eta 0:00:11
epoch [13/200] batch [45/65] time 0.351 (0.469) data 0.220 (0.338) loss_u loss_u 0.8169 (0.8384) acc_u 21.8750 (20.4861) lr 1.9851e-03 eta 0:00:09
epoch [13/200] batch [50/65] time 0.349 (0.462) data 0.219 (0.331) loss_u loss_u 0.8110 (0.8367) acc_u 25.0000 (20.7500) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [55/65] time 0.496 (0.463) data 0.366 (0.332) loss_u loss_u 0.8628 (0.8365) acc_u 15.6250 (20.7386) lr 1.9851e-03 eta 0:00:04
epoch [13/200] batch [60/65] time 0.322 (0.459) data 0.191 (0.328) loss_u loss_u 0.8564 (0.8377) acc_u 12.5000 (20.4688) lr 1.9851e-03 eta 0:00:02
epoch [13/200] batch [65/65] time 0.434 (0.457) data 0.302 (0.326) loss_u loss_u 0.7930 (0.8382) acc_u 18.7500 (20.0481) lr 1.9851e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1761
confident_label rate tensor(0.3469, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1088
clean true:1061
clean false:27
clean_rate:0.9751838235294118
noisy true:314
noisy false:1734
after delete: len(clean_dataset) 1088
after delete: len(noisy_dataset) 2048
epoch [14/200] batch [5/34] time 0.396 (0.486) data 0.266 (0.356) loss_x loss_x 1.8086 (1.6609) acc_x 68.7500 (60.6250) lr 1.9823e-03 eta 0:00:14
epoch [14/200] batch [10/34] time 0.512 (0.483) data 0.381 (0.353) loss_x loss_x 1.2920 (1.6429) acc_x 62.5000 (58.4375) lr 1.9823e-03 eta 0:00:11
epoch [14/200] batch [15/34] time 0.492 (0.478) data 0.362 (0.348) loss_x loss_x 2.0410 (1.5785) acc_x 53.1250 (60.8333) lr 1.9823e-03 eta 0:00:09
epoch [14/200] batch [20/34] time 0.519 (0.480) data 0.389 (0.350) loss_x loss_x 1.4170 (1.5170) acc_x 68.7500 (62.3438) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [25/34] time 0.401 (0.471) data 0.271 (0.341) loss_x loss_x 1.1055 (1.4757) acc_x 71.8750 (62.6250) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [30/34] time 0.504 (0.470) data 0.374 (0.339) loss_x loss_x 1.1357 (1.4759) acc_x 62.5000 (62.8125) lr 1.9823e-03 eta 0:00:01
epoch [14/200] batch [5/64] time 0.766 (0.471) data 0.636 (0.341) loss_u loss_u 0.8691 (0.8600) acc_u 15.6250 (15.0000) lr 1.9823e-03 eta 0:00:27
epoch [14/200] batch [10/64] time 0.437 (0.467) data 0.306 (0.337) loss_u loss_u 0.9326 (0.8583) acc_u 6.2500 (16.2500) lr 1.9823e-03 eta 0:00:25
epoch [14/200] batch [15/64] time 0.454 (0.464) data 0.323 (0.333) loss_u loss_u 0.8135 (0.8558) acc_u 28.1250 (18.1250) lr 1.9823e-03 eta 0:00:22
epoch [14/200] batch [20/64] time 0.337 (0.457) data 0.205 (0.326) loss_u loss_u 0.7734 (0.8582) acc_u 31.2500 (17.8125) lr 1.9823e-03 eta 0:00:20
epoch [14/200] batch [25/64] time 0.516 (0.455) data 0.384 (0.325) loss_u loss_u 0.8706 (0.8586) acc_u 12.5000 (17.3750) lr 1.9823e-03 eta 0:00:17
epoch [14/200] batch [30/64] time 0.420 (0.453) data 0.289 (0.322) loss_u loss_u 0.8345 (0.8572) acc_u 18.7500 (17.5000) lr 1.9823e-03 eta 0:00:15
epoch [14/200] batch [35/64] time 0.390 (0.450) data 0.259 (0.319) loss_u loss_u 0.8247 (0.8574) acc_u 21.8750 (17.5893) lr 1.9823e-03 eta 0:00:13
epoch [14/200] batch [40/64] time 0.374 (0.449) data 0.243 (0.319) loss_u loss_u 0.8442 (0.8550) acc_u 25.0000 (17.7344) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [45/64] time 0.712 (0.452) data 0.580 (0.321) loss_u loss_u 0.8857 (0.8560) acc_u 15.6250 (17.7778) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [50/64] time 0.382 (0.448) data 0.251 (0.317) loss_u loss_u 0.8252 (0.8522) acc_u 21.8750 (18.3750) lr 1.9823e-03 eta 0:00:06
epoch [14/200] batch [55/64] time 0.533 (0.448) data 0.401 (0.318) loss_u loss_u 0.8691 (0.8526) acc_u 12.5000 (18.0114) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [60/64] time 0.408 (0.448) data 0.278 (0.317) loss_u loss_u 0.8076 (0.8517) acc_u 28.1250 (18.2812) lr 1.9823e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1719
confident_label rate tensor(0.3581, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1123
clean true:1090
clean false:33
clean_rate:0.9706144256455922
noisy true:327
noisy false:1686
after delete: len(clean_dataset) 1123
after delete: len(noisy_dataset) 2013
epoch [15/200] batch [5/35] time 0.357 (0.408) data 0.227 (0.277) loss_x loss_x 1.4434 (1.4725) acc_x 65.6250 (59.3750) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [10/35] time 0.448 (0.433) data 0.318 (0.303) loss_x loss_x 1.2588 (1.3502) acc_x 65.6250 (61.2500) lr 1.9792e-03 eta 0:00:10
epoch [15/200] batch [15/35] time 0.381 (0.427) data 0.251 (0.296) loss_x loss_x 1.3867 (1.3811) acc_x 56.2500 (61.6667) lr 1.9792e-03 eta 0:00:08
epoch [15/200] batch [20/35] time 0.463 (0.430) data 0.334 (0.300) loss_x loss_x 1.4922 (1.3915) acc_x 62.5000 (62.1875) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [25/35] time 0.475 (0.446) data 0.344 (0.316) loss_x loss_x 1.3770 (1.4082) acc_x 62.5000 (62.2500) lr 1.9792e-03 eta 0:00:04
epoch [15/200] batch [30/35] time 0.391 (0.452) data 0.262 (0.322) loss_x loss_x 1.4541 (1.4177) acc_x 59.3750 (62.3958) lr 1.9792e-03 eta 0:00:02
epoch [15/200] batch [35/35] time 0.486 (0.455) data 0.355 (0.325) loss_x loss_x 1.1035 (1.4270) acc_x 78.1250 (61.6964) lr 1.9792e-03 eta 0:00:00
epoch [15/200] batch [5/62] time 0.523 (0.465) data 0.393 (0.335) loss_u loss_u 0.8843 (0.8659) acc_u 15.6250 (16.8750) lr 1.9792e-03 eta 0:00:26
epoch [15/200] batch [10/62] time 0.390 (0.457) data 0.259 (0.327) loss_u loss_u 0.7183 (0.8454) acc_u 40.6250 (20.9375) lr 1.9792e-03 eta 0:00:23
epoch [15/200] batch [15/62] time 0.447 (0.450) data 0.315 (0.320) loss_u loss_u 0.8984 (0.8512) acc_u 12.5000 (19.7917) lr 1.9792e-03 eta 0:00:21
epoch [15/200] batch [20/62] time 0.434 (0.449) data 0.304 (0.318) loss_u loss_u 0.7554 (0.8359) acc_u 31.2500 (20.7812) lr 1.9792e-03 eta 0:00:18
epoch [15/200] batch [25/62] time 0.413 (0.452) data 0.281 (0.321) loss_u loss_u 0.8286 (0.8476) acc_u 25.0000 (19.2500) lr 1.9792e-03 eta 0:00:16
epoch [15/200] batch [30/62] time 0.506 (0.455) data 0.376 (0.324) loss_u loss_u 0.8740 (0.8508) acc_u 9.3750 (18.3333) lr 1.9792e-03 eta 0:00:14
epoch [15/200] batch [35/62] time 0.416 (0.450) data 0.284 (0.320) loss_u loss_u 0.7925 (0.8493) acc_u 28.1250 (19.0179) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [40/62] time 0.462 (0.452) data 0.330 (0.322) loss_u loss_u 0.8203 (0.8461) acc_u 21.8750 (19.4531) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [45/62] time 0.455 (0.452) data 0.324 (0.321) loss_u loss_u 0.7900 (0.8454) acc_u 28.1250 (19.8611) lr 1.9792e-03 eta 0:00:07
epoch [15/200] batch [50/62] time 0.458 (0.453) data 0.328 (0.322) loss_u loss_u 0.8223 (0.8450) acc_u 28.1250 (20.0000) lr 1.9792e-03 eta 0:00:05
epoch [15/200] batch [55/62] time 0.419 (0.454) data 0.289 (0.323) loss_u loss_u 0.8833 (0.8465) acc_u 12.5000 (19.8295) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [60/62] time 0.403 (0.452) data 0.272 (0.321) loss_u loss_u 0.9399 (0.8462) acc_u 3.1250 (19.8958) lr 1.9792e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1735
confident_label rate tensor(0.3527, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1106
clean true:1075
clean false:31
clean_rate:0.9719710669077758
noisy true:326
noisy false:1704
after delete: len(clean_dataset) 1106
after delete: len(noisy_dataset) 2030
epoch [16/200] batch [5/34] time 0.450 (0.463) data 0.319 (0.332) loss_x loss_x 1.3809 (1.5398) acc_x 68.7500 (62.5000) lr 1.9759e-03 eta 0:00:13
epoch [16/200] batch [10/34] time 0.447 (0.462) data 0.316 (0.332) loss_x loss_x 1.1807 (1.3893) acc_x 71.8750 (63.7500) lr 1.9759e-03 eta 0:00:11
epoch [16/200] batch [15/34] time 0.452 (0.456) data 0.321 (0.326) loss_x loss_x 1.4404 (1.4107) acc_x 62.5000 (63.3333) lr 1.9759e-03 eta 0:00:08
epoch [16/200] batch [20/34] time 0.474 (0.468) data 0.344 (0.338) loss_x loss_x 1.2607 (1.3815) acc_x 62.5000 (63.7500) lr 1.9759e-03 eta 0:00:06
epoch [16/200] batch [25/34] time 0.434 (0.457) data 0.305 (0.327) loss_x loss_x 2.1543 (1.4591) acc_x 46.8750 (62.1250) lr 1.9759e-03 eta 0:00:04
epoch [16/200] batch [30/34] time 0.488 (0.464) data 0.359 (0.334) loss_x loss_x 1.3652 (1.4512) acc_x 56.2500 (61.4583) lr 1.9759e-03 eta 0:00:01
epoch [16/200] batch [5/63] time 0.560 (0.461) data 0.429 (0.330) loss_u loss_u 0.8970 (0.8656) acc_u 15.6250 (16.8750) lr 1.9759e-03 eta 0:00:26
epoch [16/200] batch [10/63] time 0.421 (0.458) data 0.289 (0.327) loss_u loss_u 0.8955 (0.8629) acc_u 9.3750 (16.8750) lr 1.9759e-03 eta 0:00:24
epoch [16/200] batch [15/63] time 0.383 (0.457) data 0.253 (0.327) loss_u loss_u 0.8276 (0.8486) acc_u 21.8750 (19.7917) lr 1.9759e-03 eta 0:00:21
epoch [16/200] batch [20/63] time 0.396 (0.453) data 0.264 (0.322) loss_u loss_u 0.8774 (0.8517) acc_u 12.5000 (19.3750) lr 1.9759e-03 eta 0:00:19
epoch [16/200] batch [25/63] time 0.441 (0.449) data 0.310 (0.319) loss_u loss_u 0.8530 (0.8417) acc_u 15.6250 (20.2500) lr 1.9759e-03 eta 0:00:17
epoch [16/200] batch [30/63] time 0.391 (0.454) data 0.260 (0.323) loss_u loss_u 0.8442 (0.8408) acc_u 15.6250 (19.7917) lr 1.9759e-03 eta 0:00:14
epoch [16/200] batch [35/63] time 0.394 (0.457) data 0.262 (0.327) loss_u loss_u 0.8188 (0.8400) acc_u 28.1250 (20.5357) lr 1.9759e-03 eta 0:00:12
epoch [16/200] batch [40/63] time 0.463 (0.458) data 0.332 (0.327) loss_u loss_u 0.8726 (0.8403) acc_u 21.8750 (20.7812) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [45/63] time 0.414 (0.454) data 0.282 (0.324) loss_u loss_u 0.8394 (0.8389) acc_u 21.8750 (21.2500) lr 1.9759e-03 eta 0:00:08
epoch [16/200] batch [50/63] time 0.425 (0.452) data 0.294 (0.322) loss_u loss_u 0.8125 (0.8370) acc_u 28.1250 (21.7500) lr 1.9759e-03 eta 0:00:05
epoch [16/200] batch [55/63] time 0.332 (0.452) data 0.202 (0.321) loss_u loss_u 0.8833 (0.8393) acc_u 18.7500 (21.5909) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [60/63] time 0.379 (0.450) data 0.247 (0.320) loss_u loss_u 0.9019 (0.8393) acc_u 9.3750 (21.4583) lr 1.9759e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1716
confident_label rate tensor(0.3530, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1107
clean true:1082
clean false:25
clean_rate:0.9774164408310749
noisy true:338
noisy false:1691
after delete: len(clean_dataset) 1107
after delete: len(noisy_dataset) 2029
epoch [17/200] batch [5/34] time 0.485 (0.441) data 0.355 (0.311) loss_x loss_x 1.4648 (1.4014) acc_x 56.2500 (60.6250) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [10/34] time 0.450 (0.463) data 0.319 (0.333) loss_x loss_x 1.9795 (1.5811) acc_x 53.1250 (59.6875) lr 1.9724e-03 eta 0:00:11
epoch [17/200] batch [15/34] time 0.571 (0.462) data 0.441 (0.331) loss_x loss_x 1.2373 (1.5377) acc_x 68.7500 (60.8333) lr 1.9724e-03 eta 0:00:08
epoch [17/200] batch [20/34] time 0.384 (0.459) data 0.252 (0.329) loss_x loss_x 1.4658 (1.5667) acc_x 65.6250 (60.7812) lr 1.9724e-03 eta 0:00:06
epoch [17/200] batch [25/34] time 0.393 (0.454) data 0.262 (0.324) loss_x loss_x 1.3457 (1.5686) acc_x 71.8750 (61.7500) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [30/34] time 0.547 (0.454) data 0.416 (0.324) loss_x loss_x 1.6260 (1.5881) acc_x 53.1250 (60.3125) lr 1.9724e-03 eta 0:00:01
epoch [17/200] batch [5/63] time 0.479 (0.451) data 0.348 (0.321) loss_u loss_u 0.9165 (0.8689) acc_u 6.2500 (12.5000) lr 1.9724e-03 eta 0:00:26
epoch [17/200] batch [10/63] time 0.432 (0.449) data 0.301 (0.318) loss_u loss_u 0.8267 (0.8460) acc_u 28.1250 (19.3750) lr 1.9724e-03 eta 0:00:23
epoch [17/200] batch [15/63] time 0.460 (0.442) data 0.329 (0.312) loss_u loss_u 0.9111 (0.8442) acc_u 9.3750 (19.1667) lr 1.9724e-03 eta 0:00:21
epoch [17/200] batch [20/63] time 0.433 (0.447) data 0.301 (0.317) loss_u loss_u 0.8369 (0.8451) acc_u 21.8750 (19.0625) lr 1.9724e-03 eta 0:00:19
epoch [17/200] batch [25/63] time 0.360 (0.447) data 0.230 (0.317) loss_u loss_u 0.8706 (0.8500) acc_u 9.3750 (18.5000) lr 1.9724e-03 eta 0:00:16
epoch [17/200] batch [30/63] time 0.513 (0.444) data 0.382 (0.313) loss_u loss_u 0.7661 (0.8425) acc_u 37.5000 (20.1042) lr 1.9724e-03 eta 0:00:14
epoch [17/200] batch [35/63] time 0.417 (0.442) data 0.286 (0.311) loss_u loss_u 0.8838 (0.8481) acc_u 15.6250 (19.1964) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [40/63] time 0.432 (0.440) data 0.301 (0.310) loss_u loss_u 0.8047 (0.8482) acc_u 34.3750 (19.2969) lr 1.9724e-03 eta 0:00:10
epoch [17/200] batch [45/63] time 0.390 (0.443) data 0.260 (0.312) loss_u loss_u 0.8223 (0.8498) acc_u 25.0000 (19.1667) lr 1.9724e-03 eta 0:00:07
epoch [17/200] batch [50/63] time 0.482 (0.444) data 0.351 (0.313) loss_u loss_u 0.7793 (0.8470) acc_u 37.5000 (20.0000) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [55/63] time 0.445 (0.446) data 0.315 (0.315) loss_u loss_u 0.8096 (0.8434) acc_u 28.1250 (20.3977) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [60/63] time 0.442 (0.446) data 0.312 (0.316) loss_u loss_u 0.9004 (0.8448) acc_u 9.3750 (20.0000) lr 1.9724e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1709
confident_label rate tensor(0.3622, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1136
clean true:1099
clean false:37
clean_rate:0.9674295774647887
noisy true:328
noisy false:1672
after delete: len(clean_dataset) 1136
after delete: len(noisy_dataset) 2000
epoch [18/200] batch [5/35] time 0.388 (0.496) data 0.258 (0.365) loss_x loss_x 1.5518 (1.4465) acc_x 56.2500 (60.6250) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [10/35] time 0.484 (0.473) data 0.353 (0.342) loss_x loss_x 1.1504 (1.4167) acc_x 71.8750 (64.0625) lr 1.9686e-03 eta 0:00:11
epoch [18/200] batch [15/35] time 0.589 (0.476) data 0.458 (0.345) loss_x loss_x 1.2393 (1.4453) acc_x 68.7500 (61.6667) lr 1.9686e-03 eta 0:00:09
epoch [18/200] batch [20/35] time 0.508 (0.466) data 0.378 (0.336) loss_x loss_x 1.4141 (1.4750) acc_x 56.2500 (60.4688) lr 1.9686e-03 eta 0:00:06
epoch [18/200] batch [25/35] time 0.488 (0.461) data 0.358 (0.330) loss_x loss_x 1.7051 (1.4830) acc_x 50.0000 (59.1250) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [30/35] time 0.396 (0.448) data 0.266 (0.318) loss_x loss_x 1.1338 (1.4608) acc_x 78.1250 (60.3125) lr 1.9686e-03 eta 0:00:02
epoch [18/200] batch [35/35] time 0.458 (0.449) data 0.328 (0.319) loss_x loss_x 1.6846 (1.4654) acc_x 59.3750 (60.5357) lr 1.9686e-03 eta 0:00:00
epoch [18/200] batch [5/62] time 0.427 (0.454) data 0.295 (0.323) loss_u loss_u 0.7803 (0.8477) acc_u 31.2500 (17.5000) lr 1.9686e-03 eta 0:00:25
epoch [18/200] batch [10/62] time 0.416 (0.450) data 0.286 (0.319) loss_u loss_u 0.8423 (0.8396) acc_u 15.6250 (18.4375) lr 1.9686e-03 eta 0:00:23
epoch [18/200] batch [15/62] time 0.407 (0.456) data 0.275 (0.325) loss_u loss_u 0.7734 (0.8523) acc_u 28.1250 (17.2917) lr 1.9686e-03 eta 0:00:21
epoch [18/200] batch [20/62] time 0.443 (0.455) data 0.312 (0.325) loss_u loss_u 0.8745 (0.8501) acc_u 12.5000 (18.2812) lr 1.9686e-03 eta 0:00:19
epoch [18/200] batch [25/62] time 0.366 (0.458) data 0.235 (0.327) loss_u loss_u 0.8262 (0.8458) acc_u 25.0000 (19.0000) lr 1.9686e-03 eta 0:00:16
epoch [18/200] batch [30/62] time 0.445 (0.456) data 0.314 (0.325) loss_u loss_u 0.9062 (0.8487) acc_u 18.7500 (18.8542) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [35/62] time 0.413 (0.454) data 0.283 (0.324) loss_u loss_u 0.8721 (0.8470) acc_u 18.7500 (19.0179) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [40/62] time 0.382 (0.453) data 0.249 (0.322) loss_u loss_u 0.8613 (0.8460) acc_u 12.5000 (18.9844) lr 1.9686e-03 eta 0:00:09
epoch [18/200] batch [45/62] time 0.468 (0.456) data 0.338 (0.326) loss_u loss_u 0.8691 (0.8442) acc_u 15.6250 (19.1667) lr 1.9686e-03 eta 0:00:07
epoch [18/200] batch [50/62] time 0.382 (0.456) data 0.251 (0.325) loss_u loss_u 0.9131 (0.8459) acc_u 6.2500 (19.0625) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [55/62] time 0.360 (0.454) data 0.230 (0.324) loss_u loss_u 0.8672 (0.8451) acc_u 15.6250 (19.3750) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [60/62] time 0.461 (0.453) data 0.330 (0.322) loss_u loss_u 0.8623 (0.8466) acc_u 15.6250 (19.0625) lr 1.9686e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1711
confident_label rate tensor(0.3645, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1143
clean true:1109
clean false:34
clean_rate:0.9702537182852143
noisy true:316
noisy false:1677
after delete: len(clean_dataset) 1143
after delete: len(noisy_dataset) 1993
epoch [19/200] batch [5/35] time 0.520 (0.512) data 0.391 (0.382) loss_x loss_x 1.1846 (1.4178) acc_x 62.5000 (65.6250) lr 1.9646e-03 eta 0:00:15
epoch [19/200] batch [10/35] time 0.532 (0.501) data 0.402 (0.371) loss_x loss_x 1.0947 (1.3399) acc_x 71.8750 (65.0000) lr 1.9646e-03 eta 0:00:12
epoch [19/200] batch [15/35] time 0.331 (0.481) data 0.202 (0.351) loss_x loss_x 0.9556 (1.3889) acc_x 71.8750 (64.3750) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [20/35] time 0.496 (0.471) data 0.367 (0.341) loss_x loss_x 1.5195 (1.3606) acc_x 62.5000 (65.7812) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [25/35] time 0.447 (0.474) data 0.316 (0.344) loss_x loss_x 1.0762 (1.3977) acc_x 71.8750 (64.7500) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [30/35] time 0.340 (0.463) data 0.211 (0.333) loss_x loss_x 1.3672 (1.3903) acc_x 71.8750 (64.6875) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [35/35] time 0.444 (0.462) data 0.314 (0.332) loss_x loss_x 1.1445 (1.3925) acc_x 68.7500 (64.6429) lr 1.9646e-03 eta 0:00:00
epoch [19/200] batch [5/62] time 0.479 (0.464) data 0.347 (0.334) loss_u loss_u 0.7856 (0.8201) acc_u 37.5000 (25.0000) lr 1.9646e-03 eta 0:00:26
epoch [19/200] batch [10/62] time 0.477 (0.460) data 0.345 (0.330) loss_u loss_u 0.8276 (0.8253) acc_u 21.8750 (23.4375) lr 1.9646e-03 eta 0:00:23
epoch [19/200] batch [15/62] time 0.493 (0.455) data 0.361 (0.325) loss_u loss_u 0.8159 (0.8313) acc_u 31.2500 (23.1250) lr 1.9646e-03 eta 0:00:21
epoch [19/200] batch [20/62] time 0.403 (0.456) data 0.273 (0.325) loss_u loss_u 0.8394 (0.8375) acc_u 28.1250 (22.6562) lr 1.9646e-03 eta 0:00:19
epoch [19/200] batch [25/62] time 0.457 (0.455) data 0.327 (0.324) loss_u loss_u 0.8149 (0.8362) acc_u 25.0000 (22.6250) lr 1.9646e-03 eta 0:00:16
epoch [19/200] batch [30/62] time 0.343 (0.451) data 0.212 (0.321) loss_u loss_u 0.8174 (0.8377) acc_u 18.7500 (21.9792) lr 1.9646e-03 eta 0:00:14
epoch [19/200] batch [35/62] time 0.398 (0.449) data 0.268 (0.319) loss_u loss_u 0.8799 (0.8359) acc_u 12.5000 (21.7857) lr 1.9646e-03 eta 0:00:12
epoch [19/200] batch [40/62] time 0.435 (0.446) data 0.304 (0.315) loss_u loss_u 0.8750 (0.8336) acc_u 15.6250 (21.7188) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [45/62] time 0.445 (0.449) data 0.313 (0.318) loss_u loss_u 0.8047 (0.8316) acc_u 18.7500 (21.8056) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [50/62] time 0.393 (0.447) data 0.260 (0.316) loss_u loss_u 0.8550 (0.8311) acc_u 18.7500 (22.0625) lr 1.9646e-03 eta 0:00:05
epoch [19/200] batch [55/62] time 0.422 (0.449) data 0.291 (0.318) loss_u loss_u 0.9453 (0.8340) acc_u 3.1250 (21.4773) lr 1.9646e-03 eta 0:00:03
epoch [19/200] batch [60/62] time 0.524 (0.455) data 0.393 (0.324) loss_u loss_u 0.8281 (0.8347) acc_u 25.0000 (21.2500) lr 1.9646e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1722
confident_label rate tensor(0.3597, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1128
clean true:1086
clean false:42
clean_rate:0.9627659574468085
noisy true:328
noisy false:1680
after delete: len(clean_dataset) 1128
after delete: len(noisy_dataset) 2008
epoch [20/200] batch [5/35] time 0.442 (0.459) data 0.312 (0.328) loss_x loss_x 1.0967 (1.3734) acc_x 78.1250 (68.7500) lr 1.9603e-03 eta 0:00:13
epoch [20/200] batch [10/35] time 0.380 (0.457) data 0.250 (0.326) loss_x loss_x 1.8213 (1.5204) acc_x 53.1250 (65.0000) lr 1.9603e-03 eta 0:00:11
epoch [20/200] batch [15/35] time 0.574 (0.469) data 0.444 (0.339) loss_x loss_x 1.3350 (1.5329) acc_x 65.6250 (63.5417) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [20/35] time 0.445 (0.449) data 0.315 (0.319) loss_x loss_x 1.9189 (1.5654) acc_x 46.8750 (61.5625) lr 1.9603e-03 eta 0:00:06
epoch [20/200] batch [25/35] time 0.387 (0.461) data 0.257 (0.331) loss_x loss_x 1.2178 (1.5419) acc_x 78.1250 (61.7500) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [30/35] time 0.422 (0.464) data 0.292 (0.333) loss_x loss_x 1.0322 (1.5079) acc_x 68.7500 (62.2917) lr 1.9603e-03 eta 0:00:02
epoch [20/200] batch [35/35] time 0.428 (0.463) data 0.298 (0.333) loss_x loss_x 1.5469 (1.4879) acc_x 62.5000 (62.3214) lr 1.9603e-03 eta 0:00:00
epoch [20/200] batch [5/62] time 0.417 (0.455) data 0.285 (0.325) loss_u loss_u 0.8101 (0.8466) acc_u 25.0000 (19.3750) lr 1.9603e-03 eta 0:00:25
epoch [20/200] batch [10/62] time 0.469 (0.455) data 0.338 (0.325) loss_u loss_u 0.8345 (0.8188) acc_u 21.8750 (22.1875) lr 1.9603e-03 eta 0:00:23
epoch [20/200] batch [15/62] time 0.531 (0.451) data 0.400 (0.321) loss_u loss_u 0.7729 (0.8250) acc_u 25.0000 (21.8750) lr 1.9603e-03 eta 0:00:21
epoch [20/200] batch [20/62] time 0.470 (0.449) data 0.339 (0.319) loss_u loss_u 0.8496 (0.8245) acc_u 15.6250 (22.0312) lr 1.9603e-03 eta 0:00:18
epoch [20/200] batch [25/62] time 0.590 (0.457) data 0.459 (0.326) loss_u loss_u 0.8394 (0.8276) acc_u 31.2500 (21.7500) lr 1.9603e-03 eta 0:00:16
epoch [20/200] batch [30/62] time 0.409 (0.457) data 0.279 (0.326) loss_u loss_u 0.8750 (0.8297) acc_u 15.6250 (21.4583) lr 1.9603e-03 eta 0:00:14
epoch [20/200] batch [35/62] time 0.372 (0.455) data 0.241 (0.325) loss_u loss_u 0.8301 (0.8300) acc_u 15.6250 (20.9821) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [40/62] time 0.418 (0.454) data 0.286 (0.324) loss_u loss_u 0.7812 (0.8292) acc_u 28.1250 (21.1719) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [45/62] time 0.411 (0.454) data 0.280 (0.323) loss_u loss_u 0.8286 (0.8240) acc_u 25.0000 (22.0139) lr 1.9603e-03 eta 0:00:07
epoch [20/200] batch [50/62] time 0.497 (0.454) data 0.365 (0.323) loss_u loss_u 0.8652 (0.8305) acc_u 18.7500 (21.2500) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [55/62] time 0.508 (0.455) data 0.377 (0.324) loss_u loss_u 0.8271 (0.8343) acc_u 18.7500 (20.8523) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [60/62] time 0.419 (0.452) data 0.288 (0.322) loss_u loss_u 0.8374 (0.8338) acc_u 25.0000 (21.4583) lr 1.9603e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1664
confident_label rate tensor(0.3705, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1162
clean true:1126
clean false:36
clean_rate:0.9690189328743546
noisy true:346
noisy false:1628
after delete: len(clean_dataset) 1162
after delete: len(noisy_dataset) 1974
epoch [21/200] batch [5/36] time 0.572 (0.456) data 0.441 (0.326) loss_x loss_x 1.1592 (1.2234) acc_x 78.1250 (66.2500) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [10/36] time 0.672 (0.510) data 0.541 (0.380) loss_x loss_x 1.4639 (1.4423) acc_x 53.1250 (62.1875) lr 1.9558e-03 eta 0:00:13
epoch [21/200] batch [15/36] time 0.405 (0.490) data 0.273 (0.360) loss_x loss_x 2.4766 (1.5286) acc_x 53.1250 (60.8333) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [20/36] time 0.347 (0.473) data 0.217 (0.342) loss_x loss_x 1.8066 (1.5059) acc_x 62.5000 (61.2500) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [25/36] time 0.517 (0.470) data 0.387 (0.339) loss_x loss_x 2.0645 (1.5278) acc_x 40.6250 (59.7500) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [30/36] time 0.486 (0.466) data 0.356 (0.335) loss_x loss_x 1.9639 (1.5046) acc_x 53.1250 (60.1042) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [35/36] time 0.583 (0.474) data 0.452 (0.344) loss_x loss_x 1.7891 (1.5045) acc_x 53.1250 (60.8036) lr 1.9558e-03 eta 0:00:00
epoch [21/200] batch [5/61] time 0.470 (0.471) data 0.340 (0.340) loss_u loss_u 0.8203 (0.8265) acc_u 21.8750 (21.8750) lr 1.9558e-03 eta 0:00:26
epoch [21/200] batch [10/61] time 0.454 (0.467) data 0.322 (0.336) loss_u loss_u 0.8687 (0.8384) acc_u 15.6250 (20.9375) lr 1.9558e-03 eta 0:00:23
epoch [21/200] batch [15/61] time 0.340 (0.460) data 0.208 (0.330) loss_u loss_u 0.8647 (0.8403) acc_u 18.7500 (20.8333) lr 1.9558e-03 eta 0:00:21
epoch [21/200] batch [20/61] time 0.483 (0.462) data 0.353 (0.331) loss_u loss_u 0.8447 (0.8376) acc_u 25.0000 (20.7812) lr 1.9558e-03 eta 0:00:18
epoch [21/200] batch [25/61] time 0.484 (0.463) data 0.354 (0.332) loss_u loss_u 0.8574 (0.8382) acc_u 15.6250 (20.3750) lr 1.9558e-03 eta 0:00:16
epoch [21/200] batch [30/61] time 0.484 (0.468) data 0.352 (0.337) loss_u loss_u 0.8589 (0.8342) acc_u 18.7500 (20.9375) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [35/61] time 0.400 (0.463) data 0.268 (0.333) loss_u loss_u 0.8643 (0.8369) acc_u 15.6250 (20.2679) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [40/61] time 0.455 (0.462) data 0.324 (0.332) loss_u loss_u 0.8257 (0.8329) acc_u 21.8750 (21.0156) lr 1.9558e-03 eta 0:00:09
epoch [21/200] batch [45/61] time 0.551 (0.468) data 0.419 (0.337) loss_u loss_u 0.8311 (0.8337) acc_u 21.8750 (20.8333) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [50/61] time 0.437 (0.467) data 0.303 (0.336) loss_u loss_u 0.8477 (0.8338) acc_u 18.7500 (20.7500) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [55/61] time 0.456 (0.466) data 0.324 (0.335) loss_u loss_u 0.8110 (0.8366) acc_u 28.1250 (20.7386) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [60/61] time 0.423 (0.464) data 0.291 (0.333) loss_u loss_u 0.8691 (0.8355) acc_u 21.8750 (20.9375) lr 1.9558e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1635
confident_label rate tensor(0.3801, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1192
clean true:1159
clean false:33
clean_rate:0.9723154362416108
noisy true:342
noisy false:1602
after delete: len(clean_dataset) 1192
after delete: len(noisy_dataset) 1944
epoch [22/200] batch [5/37] time 0.450 (0.496) data 0.319 (0.365) loss_x loss_x 1.6416 (1.2064) acc_x 50.0000 (62.5000) lr 1.9511e-03 eta 0:00:15
epoch [22/200] batch [10/37] time 0.586 (0.482) data 0.455 (0.352) loss_x loss_x 1.3096 (1.2712) acc_x 62.5000 (61.8750) lr 1.9511e-03 eta 0:00:13
epoch [22/200] batch [15/37] time 0.451 (0.477) data 0.320 (0.346) loss_x loss_x 0.9272 (1.2740) acc_x 71.8750 (63.5417) lr 1.9511e-03 eta 0:00:10
epoch [22/200] batch [20/37] time 0.619 (0.484) data 0.489 (0.354) loss_x loss_x 1.6816 (1.3486) acc_x 53.1250 (62.1875) lr 1.9511e-03 eta 0:00:08
epoch [22/200] batch [25/37] time 0.416 (0.476) data 0.285 (0.346) loss_x loss_x 1.0762 (1.3877) acc_x 71.8750 (62.1250) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [30/37] time 0.428 (0.476) data 0.298 (0.346) loss_x loss_x 1.2549 (1.3898) acc_x 75.0000 (62.5000) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [35/37] time 0.467 (0.477) data 0.338 (0.346) loss_x loss_x 0.9302 (1.3975) acc_x 78.1250 (63.2143) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/60] time 0.382 (0.474) data 0.251 (0.343) loss_u loss_u 0.8901 (0.8608) acc_u 21.8750 (23.1250) lr 1.9511e-03 eta 0:00:26
epoch [22/200] batch [10/60] time 0.551 (0.474) data 0.420 (0.343) loss_u loss_u 0.9072 (0.8495) acc_u 9.3750 (24.0625) lr 1.9511e-03 eta 0:00:23
epoch [22/200] batch [15/60] time 0.465 (0.473) data 0.334 (0.343) loss_u loss_u 0.7124 (0.8486) acc_u 37.5000 (22.9167) lr 1.9511e-03 eta 0:00:21
epoch [22/200] batch [20/60] time 0.378 (0.470) data 0.247 (0.339) loss_u loss_u 0.8901 (0.8516) acc_u 12.5000 (21.7188) lr 1.9511e-03 eta 0:00:18
epoch [22/200] batch [25/60] time 0.509 (0.467) data 0.375 (0.336) loss_u loss_u 0.8008 (0.8490) acc_u 28.1250 (22.0000) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [30/60] time 0.469 (0.468) data 0.337 (0.338) loss_u loss_u 0.8242 (0.8515) acc_u 25.0000 (20.7292) lr 1.9511e-03 eta 0:00:14
epoch [22/200] batch [35/60] time 0.405 (0.465) data 0.274 (0.334) loss_u loss_u 0.8013 (0.8524) acc_u 21.8750 (20.1786) lr 1.9511e-03 eta 0:00:11
epoch [22/200] batch [40/60] time 0.328 (0.464) data 0.196 (0.333) loss_u loss_u 0.8354 (0.8538) acc_u 18.7500 (19.6875) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [45/60] time 0.426 (0.465) data 0.295 (0.334) loss_u loss_u 0.7925 (0.8523) acc_u 28.1250 (20.0694) lr 1.9511e-03 eta 0:00:06
epoch [22/200] batch [50/60] time 0.387 (0.466) data 0.256 (0.335) loss_u loss_u 0.8750 (0.8500) acc_u 15.6250 (20.5000) lr 1.9511e-03 eta 0:00:04
epoch [22/200] batch [55/60] time 0.429 (0.468) data 0.298 (0.337) loss_u loss_u 0.8691 (0.8488) acc_u 21.8750 (20.9091) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [60/60] time 0.375 (0.466) data 0.245 (0.335) loss_u loss_u 0.9141 (0.8502) acc_u 9.3750 (20.7812) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1651
confident_label rate tensor(0.3776, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1184
clean true:1149
clean false:35
clean_rate:0.9704391891891891
noisy true:336
noisy false:1616
after delete: len(clean_dataset) 1184
after delete: len(noisy_dataset) 1952
epoch [23/200] batch [5/37] time 0.429 (0.487) data 0.299 (0.357) loss_x loss_x 1.5166 (1.1891) acc_x 59.3750 (70.6250) lr 1.9461e-03 eta 0:00:15
epoch [23/200] batch [10/37] time 0.440 (0.473) data 0.309 (0.342) loss_x loss_x 1.1475 (1.3140) acc_x 65.6250 (65.9375) lr 1.9461e-03 eta 0:00:12
epoch [23/200] batch [15/37] time 0.457 (0.473) data 0.327 (0.342) loss_x loss_x 1.5576 (1.3603) acc_x 65.6250 (64.7917) lr 1.9461e-03 eta 0:00:10
epoch [23/200] batch [20/37] time 0.453 (0.469) data 0.322 (0.338) loss_x loss_x 1.1963 (1.3899) acc_x 75.0000 (66.0938) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [25/37] time 0.514 (0.471) data 0.383 (0.340) loss_x loss_x 1.6475 (1.4391) acc_x 62.5000 (65.0000) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [30/37] time 0.480 (0.468) data 0.350 (0.338) loss_x loss_x 1.2812 (1.4129) acc_x 62.5000 (65.0000) lr 1.9461e-03 eta 0:00:03
epoch [23/200] batch [35/37] time 0.439 (0.470) data 0.308 (0.339) loss_x loss_x 1.8760 (1.4435) acc_x 53.1250 (63.6607) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/61] time 0.347 (0.477) data 0.217 (0.347) loss_u loss_u 0.8198 (0.8419) acc_u 25.0000 (19.3750) lr 1.9461e-03 eta 0:00:26
epoch [23/200] batch [10/61] time 0.475 (0.479) data 0.343 (0.348) loss_u loss_u 0.8120 (0.8367) acc_u 21.8750 (20.0000) lr 1.9461e-03 eta 0:00:24
epoch [23/200] batch [15/61] time 0.414 (0.477) data 0.282 (0.346) loss_u loss_u 0.8584 (0.8411) acc_u 18.7500 (20.2083) lr 1.9461e-03 eta 0:00:21
epoch [23/200] batch [20/61] time 0.396 (0.474) data 0.264 (0.344) loss_u loss_u 0.8545 (0.8383) acc_u 15.6250 (20.1562) lr 1.9461e-03 eta 0:00:19
epoch [23/200] batch [25/61] time 0.453 (0.469) data 0.322 (0.338) loss_u loss_u 0.8618 (0.8411) acc_u 21.8750 (20.1250) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [30/61] time 0.384 (0.465) data 0.253 (0.334) loss_u loss_u 0.8354 (0.8389) acc_u 21.8750 (20.2083) lr 1.9461e-03 eta 0:00:14
epoch [23/200] batch [35/61] time 0.554 (0.464) data 0.424 (0.333) loss_u loss_u 0.8315 (0.8361) acc_u 21.8750 (20.6250) lr 1.9461e-03 eta 0:00:12
epoch [23/200] batch [40/61] time 0.371 (0.466) data 0.240 (0.335) loss_u loss_u 0.8555 (0.8413) acc_u 21.8750 (20.0000) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [45/61] time 0.416 (0.466) data 0.285 (0.335) loss_u loss_u 0.8530 (0.8398) acc_u 12.5000 (19.6528) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [50/61] time 0.440 (0.465) data 0.308 (0.334) loss_u loss_u 0.7759 (0.8387) acc_u 21.8750 (19.6875) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [55/61] time 0.400 (0.462) data 0.268 (0.331) loss_u loss_u 0.8623 (0.8417) acc_u 25.0000 (19.4318) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [60/61] time 0.428 (0.460) data 0.296 (0.329) loss_u loss_u 0.8496 (0.8411) acc_u 18.7500 (19.6354) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1682
confident_label rate tensor(0.3734, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1171
clean true:1140
clean false:31
clean_rate:0.9735269000853971
noisy true:314
noisy false:1651
after delete: len(clean_dataset) 1171
after delete: len(noisy_dataset) 1965
epoch [24/200] batch [5/36] time 0.514 (0.584) data 0.384 (0.452) loss_x loss_x 1.0410 (1.5605) acc_x 81.2500 (59.3750) lr 1.9409e-03 eta 0:00:18
epoch [24/200] batch [10/36] time 0.431 (0.533) data 0.301 (0.403) loss_x loss_x 1.2725 (1.5437) acc_x 65.6250 (58.7500) lr 1.9409e-03 eta 0:00:13
epoch [24/200] batch [15/36] time 0.556 (0.518) data 0.426 (0.388) loss_x loss_x 1.0576 (1.4417) acc_x 75.0000 (62.5000) lr 1.9409e-03 eta 0:00:10
epoch [24/200] batch [20/36] time 0.387 (0.490) data 0.256 (0.359) loss_x loss_x 1.3516 (1.4516) acc_x 65.6250 (62.8125) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [25/36] time 0.556 (0.491) data 0.426 (0.361) loss_x loss_x 1.5039 (1.4672) acc_x 53.1250 (62.5000) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [30/36] time 0.347 (0.472) data 0.216 (0.342) loss_x loss_x 1.6201 (1.4510) acc_x 59.3750 (63.2292) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [35/36] time 0.439 (0.473) data 0.308 (0.342) loss_x loss_x 1.6123 (1.4620) acc_x 68.7500 (62.7679) lr 1.9409e-03 eta 0:00:00
epoch [24/200] batch [5/61] time 0.448 (0.474) data 0.318 (0.344) loss_u loss_u 0.8574 (0.8612) acc_u 18.7500 (18.1250) lr 1.9409e-03 eta 0:00:26
epoch [24/200] batch [10/61] time 0.365 (0.471) data 0.234 (0.340) loss_u loss_u 0.8145 (0.8449) acc_u 34.3750 (21.5625) lr 1.9409e-03 eta 0:00:24
epoch [24/200] batch [15/61] time 0.469 (0.471) data 0.338 (0.340) loss_u loss_u 0.8594 (0.8345) acc_u 12.5000 (22.5000) lr 1.9409e-03 eta 0:00:21
epoch [24/200] batch [20/61] time 0.333 (0.464) data 0.202 (0.334) loss_u loss_u 0.9277 (0.8282) acc_u 6.2500 (22.6562) lr 1.9409e-03 eta 0:00:19
epoch [24/200] batch [25/61] time 0.345 (0.464) data 0.215 (0.334) loss_u loss_u 0.8301 (0.8274) acc_u 12.5000 (22.3750) lr 1.9409e-03 eta 0:00:16
epoch [24/200] batch [30/61] time 0.394 (0.462) data 0.263 (0.331) loss_u loss_u 0.8472 (0.8291) acc_u 18.7500 (21.7708) lr 1.9409e-03 eta 0:00:14
epoch [24/200] batch [35/61] time 0.332 (0.459) data 0.201 (0.329) loss_u loss_u 0.7783 (0.8312) acc_u 28.1250 (21.5179) lr 1.9409e-03 eta 0:00:11
epoch [24/200] batch [40/61] time 0.435 (0.459) data 0.305 (0.329) loss_u loss_u 0.8525 (0.8326) acc_u 21.8750 (21.4844) lr 1.9409e-03 eta 0:00:09
epoch [24/200] batch [45/61] time 0.559 (0.461) data 0.429 (0.330) loss_u loss_u 0.9009 (0.8353) acc_u 12.5000 (21.0417) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [50/61] time 0.382 (0.457) data 0.252 (0.326) loss_u loss_u 0.8979 (0.8370) acc_u 9.3750 (20.5625) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [55/61] time 0.430 (0.455) data 0.299 (0.324) loss_u loss_u 0.7783 (0.8370) acc_u 28.1250 (20.3409) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [60/61] time 0.710 (0.458) data 0.579 (0.327) loss_u loss_u 0.7705 (0.8372) acc_u 31.2500 (20.2083) lr 1.9409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1672
confident_label rate tensor(0.3769, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1182
clean true:1140
clean false:42
clean_rate:0.9644670050761421
noisy true:324
noisy false:1630
after delete: len(clean_dataset) 1182
after delete: len(noisy_dataset) 1954
epoch [25/200] batch [5/36] time 0.408 (0.404) data 0.279 (0.274) loss_x loss_x 1.2949 (1.5139) acc_x 71.8750 (65.6250) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [10/36] time 0.594 (0.429) data 0.464 (0.299) loss_x loss_x 2.2480 (1.6710) acc_x 53.1250 (60.9375) lr 1.9354e-03 eta 0:00:11
epoch [25/200] batch [15/36] time 0.471 (0.429) data 0.341 (0.299) loss_x loss_x 1.5498 (1.5211) acc_x 65.6250 (64.7917) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [20/36] time 0.464 (0.440) data 0.334 (0.310) loss_x loss_x 2.3750 (1.5809) acc_x 43.7500 (63.1250) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [25/36] time 0.467 (0.443) data 0.338 (0.313) loss_x loss_x 1.3506 (1.5383) acc_x 65.6250 (63.2500) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [30/36] time 0.394 (0.454) data 0.263 (0.324) loss_x loss_x 1.8174 (1.5506) acc_x 53.1250 (62.6042) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [35/36] time 0.567 (0.455) data 0.437 (0.325) loss_x loss_x 1.7715 (1.5734) acc_x 50.0000 (61.8750) lr 1.9354e-03 eta 0:00:00
epoch [25/200] batch [5/61] time 0.433 (0.457) data 0.302 (0.326) loss_u loss_u 0.8433 (0.8605) acc_u 18.7500 (16.2500) lr 1.9354e-03 eta 0:00:25
epoch [25/200] batch [10/61] time 0.461 (0.454) data 0.331 (0.323) loss_u loss_u 0.8643 (0.8619) acc_u 21.8750 (17.5000) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [15/61] time 0.500 (0.454) data 0.371 (0.324) loss_u loss_u 0.7969 (0.8575) acc_u 28.1250 (18.5417) lr 1.9354e-03 eta 0:00:20
epoch [25/200] batch [20/61] time 0.349 (0.446) data 0.219 (0.316) loss_u loss_u 0.7910 (0.8552) acc_u 28.1250 (19.2188) lr 1.9354e-03 eta 0:00:18
epoch [25/200] batch [25/61] time 0.431 (0.444) data 0.301 (0.314) loss_u loss_u 0.8037 (0.8485) acc_u 31.2500 (20.2500) lr 1.9354e-03 eta 0:00:15
epoch [25/200] batch [30/61] time 0.445 (0.442) data 0.315 (0.311) loss_u loss_u 0.8965 (0.8542) acc_u 12.5000 (19.1667) lr 1.9354e-03 eta 0:00:13
epoch [25/200] batch [35/61] time 0.517 (0.441) data 0.386 (0.311) loss_u loss_u 0.8042 (0.8522) acc_u 34.3750 (20.0893) lr 1.9354e-03 eta 0:00:11
epoch [25/200] batch [40/61] time 0.362 (0.441) data 0.231 (0.311) loss_u loss_u 0.8589 (0.8497) acc_u 15.6250 (20.6250) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [45/61] time 0.572 (0.445) data 0.441 (0.314) loss_u loss_u 0.8291 (0.8484) acc_u 21.8750 (20.6944) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [50/61] time 0.416 (0.442) data 0.285 (0.312) loss_u loss_u 0.8003 (0.8474) acc_u 31.2500 (20.6875) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [55/61] time 0.377 (0.442) data 0.248 (0.312) loss_u loss_u 0.9561 (0.8460) acc_u 6.2500 (20.8523) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [60/61] time 0.480 (0.444) data 0.348 (0.314) loss_u loss_u 0.6895 (0.8409) acc_u 34.3750 (21.4062) lr 1.9354e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1662
confident_label rate tensor(0.3680, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1154
clean true:1124
clean false:30
clean_rate:0.9740034662045061
noisy true:350
noisy false:1632
after delete: len(clean_dataset) 1154
after delete: len(noisy_dataset) 1982
epoch [26/200] batch [5/36] time 0.401 (0.483) data 0.270 (0.352) loss_x loss_x 1.3516 (1.2812) acc_x 68.7500 (63.7500) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [10/36] time 0.479 (0.475) data 0.349 (0.345) loss_x loss_x 1.5146 (1.4133) acc_x 62.5000 (62.5000) lr 1.9298e-03 eta 0:00:12
epoch [26/200] batch [15/36] time 0.412 (0.467) data 0.281 (0.336) loss_x loss_x 1.1152 (1.3460) acc_x 68.7500 (64.7917) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [20/36] time 0.417 (0.449) data 0.286 (0.319) loss_x loss_x 1.1719 (1.3593) acc_x 62.5000 (63.9062) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [25/36] time 0.474 (0.447) data 0.343 (0.316) loss_x loss_x 1.0098 (1.3634) acc_x 71.8750 (64.3750) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [30/36] time 0.475 (0.459) data 0.344 (0.328) loss_x loss_x 1.0352 (1.3804) acc_x 75.0000 (64.5833) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [35/36] time 0.578 (0.462) data 0.447 (0.331) loss_x loss_x 1.0869 (1.3789) acc_x 68.7500 (64.6429) lr 1.9298e-03 eta 0:00:00
epoch [26/200] batch [5/61] time 0.450 (0.454) data 0.319 (0.324) loss_u loss_u 0.8564 (0.8364) acc_u 15.6250 (20.6250) lr 1.9298e-03 eta 0:00:25
epoch [26/200] batch [10/61] time 0.431 (0.453) data 0.300 (0.322) loss_u loss_u 0.6914 (0.8337) acc_u 34.3750 (22.5000) lr 1.9298e-03 eta 0:00:23
epoch [26/200] batch [15/61] time 0.453 (0.452) data 0.322 (0.321) loss_u loss_u 0.8354 (0.8434) acc_u 21.8750 (21.0417) lr 1.9298e-03 eta 0:00:20
epoch [26/200] batch [20/61] time 0.478 (0.455) data 0.346 (0.324) loss_u loss_u 0.9165 (0.8456) acc_u 6.2500 (19.8438) lr 1.9298e-03 eta 0:00:18
epoch [26/200] batch [25/61] time 0.500 (0.453) data 0.369 (0.322) loss_u loss_u 0.8325 (0.8413) acc_u 25.0000 (21.0000) lr 1.9298e-03 eta 0:00:16
epoch [26/200] batch [30/61] time 0.537 (0.452) data 0.407 (0.321) loss_u loss_u 0.7686 (0.8381) acc_u 28.1250 (21.2500) lr 1.9298e-03 eta 0:00:13
epoch [26/200] batch [35/61] time 0.370 (0.452) data 0.239 (0.321) loss_u loss_u 0.8232 (0.8399) acc_u 18.7500 (21.0714) lr 1.9298e-03 eta 0:00:11
epoch [26/200] batch [40/61] time 0.424 (0.453) data 0.292 (0.322) loss_u loss_u 0.8701 (0.8404) acc_u 25.0000 (20.8594) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [45/61] time 0.515 (0.454) data 0.384 (0.323) loss_u loss_u 0.8877 (0.8406) acc_u 15.6250 (21.0417) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [50/61] time 0.474 (0.452) data 0.342 (0.321) loss_u loss_u 0.7563 (0.8349) acc_u 31.2500 (21.5000) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [55/61] time 0.380 (0.449) data 0.248 (0.318) loss_u loss_u 0.8179 (0.8308) acc_u 21.8750 (21.8750) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [60/61] time 0.353 (0.449) data 0.222 (0.318) loss_u loss_u 0.8521 (0.8294) acc_u 28.1250 (22.0833) lr 1.9298e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1592
confident_label rate tensor(0.3881, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1217
clean true:1187
clean false:30
clean_rate:0.9753492193919474
noisy true:357
noisy false:1562
after delete: len(clean_dataset) 1217
after delete: len(noisy_dataset) 1919
epoch [27/200] batch [5/38] time 0.387 (0.512) data 0.256 (0.381) loss_x loss_x 1.7373 (1.6291) acc_x 56.2500 (57.5000) lr 1.9239e-03 eta 0:00:16
epoch [27/200] batch [10/38] time 0.360 (0.478) data 0.229 (0.347) loss_x loss_x 1.5742 (1.4725) acc_x 53.1250 (59.3750) lr 1.9239e-03 eta 0:00:13
epoch [27/200] batch [15/38] time 0.356 (0.453) data 0.225 (0.323) loss_x loss_x 1.1982 (1.4520) acc_x 68.7500 (61.8750) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [20/38] time 0.376 (0.453) data 0.245 (0.322) loss_x loss_x 1.4912 (1.4383) acc_x 62.5000 (61.2500) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [25/38] time 0.410 (0.453) data 0.279 (0.323) loss_x loss_x 1.2041 (1.4054) acc_x 75.0000 (62.8750) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [30/38] time 0.393 (0.456) data 0.262 (0.326) loss_x loss_x 1.1123 (1.4223) acc_x 68.7500 (63.0208) lr 1.9239e-03 eta 0:00:03
epoch [27/200] batch [35/38] time 0.411 (0.459) data 0.280 (0.328) loss_x loss_x 1.1758 (1.4095) acc_x 65.6250 (63.5714) lr 1.9239e-03 eta 0:00:01
epoch [27/200] batch [5/59] time 0.462 (0.459) data 0.332 (0.329) loss_u loss_u 0.8340 (0.8294) acc_u 34.3750 (25.0000) lr 1.9239e-03 eta 0:00:24
epoch [27/200] batch [10/59] time 0.440 (0.460) data 0.309 (0.329) loss_u loss_u 0.8877 (0.8143) acc_u 12.5000 (25.3125) lr 1.9239e-03 eta 0:00:22
epoch [27/200] batch [15/59] time 0.416 (0.460) data 0.284 (0.330) loss_u loss_u 0.8110 (0.8250) acc_u 18.7500 (22.9167) lr 1.9239e-03 eta 0:00:20
epoch [27/200] batch [20/59] time 0.412 (0.464) data 0.282 (0.333) loss_u loss_u 0.7407 (0.8233) acc_u 37.5000 (21.7188) lr 1.9239e-03 eta 0:00:18
epoch [27/200] batch [25/59] time 0.405 (0.460) data 0.274 (0.329) loss_u loss_u 0.8750 (0.8289) acc_u 12.5000 (20.7500) lr 1.9239e-03 eta 0:00:15
epoch [27/200] batch [30/59] time 0.450 (0.458) data 0.320 (0.328) loss_u loss_u 0.7900 (0.8302) acc_u 25.0000 (20.7292) lr 1.9239e-03 eta 0:00:13
epoch [27/200] batch [35/59] time 0.340 (0.455) data 0.209 (0.324) loss_u loss_u 0.8789 (0.8266) acc_u 25.0000 (21.4286) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [40/59] time 0.432 (0.455) data 0.301 (0.324) loss_u loss_u 0.8267 (0.8263) acc_u 18.7500 (21.1719) lr 1.9239e-03 eta 0:00:08
epoch [27/200] batch [45/59] time 0.402 (0.453) data 0.272 (0.323) loss_u loss_u 0.8525 (0.8300) acc_u 18.7500 (20.8333) lr 1.9239e-03 eta 0:00:06
epoch [27/200] batch [50/59] time 0.477 (0.454) data 0.346 (0.323) loss_u loss_u 0.8745 (0.8316) acc_u 9.3750 (20.6250) lr 1.9239e-03 eta 0:00:04
epoch [27/200] batch [55/59] time 0.630 (0.453) data 0.500 (0.322) loss_u loss_u 0.8213 (0.8340) acc_u 21.8750 (20.2273) lr 1.9239e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1662
confident_label rate tensor(0.3740, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1173
clean true:1138
clean false:35
clean_rate:0.9701619778346121
noisy true:336
noisy false:1627
after delete: len(clean_dataset) 1173
after delete: len(noisy_dataset) 1963
epoch [28/200] batch [5/36] time 0.557 (0.502) data 0.428 (0.372) loss_x loss_x 1.3789 (1.2254) acc_x 59.3750 (66.2500) lr 1.9178e-03 eta 0:00:15
epoch [28/200] batch [10/36] time 0.380 (0.446) data 0.250 (0.317) loss_x loss_x 1.8584 (1.3518) acc_x 59.3750 (66.2500) lr 1.9178e-03 eta 0:00:11
epoch [28/200] batch [15/36] time 0.433 (0.466) data 0.302 (0.336) loss_x loss_x 1.4023 (1.3842) acc_x 71.8750 (65.6250) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [20/36] time 0.465 (0.459) data 0.335 (0.329) loss_x loss_x 1.1797 (1.3185) acc_x 78.1250 (67.6562) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [25/36] time 0.461 (0.472) data 0.331 (0.342) loss_x loss_x 1.0859 (1.2882) acc_x 78.1250 (67.6250) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [30/36] time 0.403 (0.471) data 0.271 (0.341) loss_x loss_x 1.2363 (1.3313) acc_x 65.6250 (66.7708) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [35/36] time 0.407 (0.473) data 0.276 (0.343) loss_x loss_x 1.3701 (1.3343) acc_x 65.6250 (66.4286) lr 1.9178e-03 eta 0:00:00
epoch [28/200] batch [5/61] time 0.406 (0.476) data 0.275 (0.345) loss_u loss_u 0.7988 (0.8106) acc_u 25.0000 (23.1250) lr 1.9178e-03 eta 0:00:26
epoch [28/200] batch [10/61] time 0.373 (0.467) data 0.242 (0.336) loss_u loss_u 0.8315 (0.8205) acc_u 18.7500 (22.1875) lr 1.9178e-03 eta 0:00:23
epoch [28/200] batch [15/61] time 0.435 (0.464) data 0.303 (0.334) loss_u loss_u 0.8169 (0.8369) acc_u 25.0000 (19.7917) lr 1.9178e-03 eta 0:00:21
epoch [28/200] batch [20/61] time 0.451 (0.465) data 0.321 (0.334) loss_u loss_u 0.8579 (0.8418) acc_u 15.6250 (19.6875) lr 1.9178e-03 eta 0:00:19
epoch [28/200] batch [25/61] time 0.404 (0.463) data 0.273 (0.333) loss_u loss_u 0.9072 (0.8431) acc_u 15.6250 (19.8750) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [30/61] time 0.473 (0.464) data 0.342 (0.333) loss_u loss_u 0.8193 (0.8309) acc_u 31.2500 (21.9792) lr 1.9178e-03 eta 0:00:14
epoch [28/200] batch [35/61] time 0.441 (0.461) data 0.311 (0.330) loss_u loss_u 0.8198 (0.8312) acc_u 15.6250 (21.7857) lr 1.9178e-03 eta 0:00:11
epoch [28/200] batch [40/61] time 0.369 (0.457) data 0.237 (0.326) loss_u loss_u 0.8657 (0.8297) acc_u 12.5000 (21.4062) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [45/61] time 0.352 (0.454) data 0.221 (0.323) loss_u loss_u 0.8428 (0.8330) acc_u 18.7500 (21.0417) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [50/61] time 0.468 (0.451) data 0.336 (0.321) loss_u loss_u 0.8086 (0.8325) acc_u 21.8750 (20.8750) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [55/61] time 0.425 (0.454) data 0.292 (0.323) loss_u loss_u 0.8086 (0.8356) acc_u 21.8750 (20.2273) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [60/61] time 0.524 (0.453) data 0.392 (0.322) loss_u loss_u 0.9229 (0.8339) acc_u 12.5000 (20.4167) lr 1.9178e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1660
confident_label rate tensor(0.3776, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1184
clean true:1150
clean false:34
clean_rate:0.9712837837837838
noisy true:326
noisy false:1626
after delete: len(clean_dataset) 1184
after delete: len(noisy_dataset) 1952
epoch [29/200] batch [5/37] time 0.432 (0.482) data 0.301 (0.352) loss_x loss_x 1.2441 (1.3219) acc_x 71.8750 (65.6250) lr 1.9114e-03 eta 0:00:15
epoch [29/200] batch [10/37] time 0.436 (0.473) data 0.305 (0.343) loss_x loss_x 1.3779 (1.3781) acc_x 71.8750 (66.5625) lr 1.9114e-03 eta 0:00:12
epoch [29/200] batch [15/37] time 0.625 (0.470) data 0.495 (0.339) loss_x loss_x 0.9932 (1.3320) acc_x 68.7500 (66.8750) lr 1.9114e-03 eta 0:00:10
epoch [29/200] batch [20/37] time 0.396 (0.466) data 0.265 (0.336) loss_x loss_x 1.4629 (1.3340) acc_x 56.2500 (66.8750) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [25/37] time 0.430 (0.462) data 0.299 (0.331) loss_x loss_x 1.4746 (1.3202) acc_x 62.5000 (67.7500) lr 1.9114e-03 eta 0:00:05
epoch [29/200] batch [30/37] time 0.515 (0.459) data 0.385 (0.329) loss_x loss_x 0.8926 (1.3155) acc_x 75.0000 (67.1875) lr 1.9114e-03 eta 0:00:03
epoch [29/200] batch [35/37] time 0.415 (0.468) data 0.286 (0.338) loss_x loss_x 1.4219 (1.3202) acc_x 59.3750 (66.8750) lr 1.9114e-03 eta 0:00:00
epoch [29/200] batch [5/61] time 0.425 (0.473) data 0.295 (0.343) loss_u loss_u 0.8823 (0.8289) acc_u 18.7500 (22.5000) lr 1.9114e-03 eta 0:00:26
epoch [29/200] batch [10/61] time 0.464 (0.469) data 0.333 (0.339) loss_u loss_u 0.7886 (0.8294) acc_u 25.0000 (22.8125) lr 1.9114e-03 eta 0:00:23
epoch [29/200] batch [15/61] time 0.588 (0.479) data 0.457 (0.348) loss_u loss_u 0.8438 (0.8301) acc_u 15.6250 (22.7083) lr 1.9114e-03 eta 0:00:22
epoch [29/200] batch [20/61] time 0.522 (0.476) data 0.391 (0.345) loss_u loss_u 0.8521 (0.8402) acc_u 18.7500 (20.9375) lr 1.9114e-03 eta 0:00:19
epoch [29/200] batch [25/61] time 0.400 (0.472) data 0.270 (0.341) loss_u loss_u 0.8369 (0.8406) acc_u 15.6250 (20.7500) lr 1.9114e-03 eta 0:00:16
epoch [29/200] batch [30/61] time 0.333 (0.467) data 0.201 (0.337) loss_u loss_u 0.8984 (0.8381) acc_u 15.6250 (21.1458) lr 1.9114e-03 eta 0:00:14
epoch [29/200] batch [35/61] time 0.394 (0.469) data 0.264 (0.339) loss_u loss_u 0.8174 (0.8362) acc_u 25.0000 (20.9821) lr 1.9114e-03 eta 0:00:12
epoch [29/200] batch [40/61] time 0.517 (0.467) data 0.384 (0.336) loss_u loss_u 0.7695 (0.8356) acc_u 31.2500 (21.4062) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [45/61] time 0.395 (0.463) data 0.264 (0.332) loss_u loss_u 0.8560 (0.8333) acc_u 18.7500 (21.5972) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [50/61] time 0.395 (0.462) data 0.265 (0.331) loss_u loss_u 0.8232 (0.8330) acc_u 28.1250 (22.0625) lr 1.9114e-03 eta 0:00:05
epoch [29/200] batch [55/61] time 0.552 (0.459) data 0.420 (0.328) loss_u loss_u 0.8862 (0.8351) acc_u 12.5000 (21.5341) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [60/61] time 0.378 (0.456) data 0.247 (0.325) loss_u loss_u 0.8823 (0.8364) acc_u 12.5000 (21.4583) lr 1.9114e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1653
confident_label rate tensor(0.3769, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1182
clean true:1149
clean false:33
clean_rate:0.9720812182741116
noisy true:334
noisy false:1620
after delete: len(clean_dataset) 1182
after delete: len(noisy_dataset) 1954
epoch [30/200] batch [5/36] time 0.428 (0.456) data 0.297 (0.326) loss_x loss_x 1.2754 (1.2861) acc_x 75.0000 (66.8750) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [10/36] time 0.423 (0.445) data 0.292 (0.315) loss_x loss_x 1.3965 (1.2633) acc_x 65.6250 (67.1875) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [15/36] time 0.512 (0.453) data 0.381 (0.322) loss_x loss_x 1.3320 (1.3256) acc_x 68.7500 (67.5000) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [20/36] time 0.510 (0.457) data 0.379 (0.327) loss_x loss_x 1.4990 (1.3268) acc_x 40.6250 (65.4688) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [25/36] time 0.509 (0.458) data 0.376 (0.327) loss_x loss_x 1.3213 (1.3148) acc_x 62.5000 (65.7500) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [30/36] time 0.478 (0.459) data 0.347 (0.328) loss_x loss_x 1.5449 (1.3306) acc_x 68.7500 (65.6250) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [35/36] time 0.346 (0.458) data 0.215 (0.327) loss_x loss_x 1.5918 (1.3702) acc_x 65.6250 (64.9107) lr 1.9048e-03 eta 0:00:00
epoch [30/200] batch [5/61] time 0.468 (0.458) data 0.336 (0.327) loss_u loss_u 0.8179 (0.8365) acc_u 25.0000 (18.1250) lr 1.9048e-03 eta 0:00:25
epoch [30/200] batch [10/61] time 0.375 (0.466) data 0.243 (0.335) loss_u loss_u 0.7725 (0.8269) acc_u 28.1250 (21.8750) lr 1.9048e-03 eta 0:00:23
epoch [30/200] batch [15/61] time 0.398 (0.463) data 0.267 (0.332) loss_u loss_u 0.8296 (0.8301) acc_u 21.8750 (21.0417) lr 1.9048e-03 eta 0:00:21
epoch [30/200] batch [20/61] time 0.423 (0.461) data 0.290 (0.330) loss_u loss_u 0.8628 (0.8344) acc_u 21.8750 (20.7812) lr 1.9048e-03 eta 0:00:18
epoch [30/200] batch [25/61] time 0.420 (0.455) data 0.289 (0.324) loss_u loss_u 0.9365 (0.8367) acc_u 3.1250 (20.5000) lr 1.9048e-03 eta 0:00:16
epoch [30/200] batch [30/61] time 0.330 (0.453) data 0.201 (0.322) loss_u loss_u 0.8135 (0.8344) acc_u 21.8750 (20.6250) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [35/61] time 0.515 (0.454) data 0.383 (0.323) loss_u loss_u 0.8418 (0.8353) acc_u 18.7500 (20.6250) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [40/61] time 0.471 (0.453) data 0.339 (0.322) loss_u loss_u 0.7637 (0.8307) acc_u 31.2500 (21.2500) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [45/61] time 0.432 (0.452) data 0.300 (0.321) loss_u loss_u 0.7617 (0.8283) acc_u 31.2500 (21.5972) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [50/61] time 0.361 (0.451) data 0.230 (0.320) loss_u loss_u 0.6689 (0.8253) acc_u 46.8750 (22.0625) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [55/61] time 0.441 (0.449) data 0.310 (0.318) loss_u loss_u 0.8081 (0.8234) acc_u 28.1250 (22.5000) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [60/61] time 0.469 (0.449) data 0.338 (0.317) loss_u loss_u 0.7847 (0.8225) acc_u 21.8750 (22.5000) lr 1.9048e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1609
confident_label rate tensor(0.3852, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1208
clean true:1173
clean false:35
clean_rate:0.9710264900662252
noisy true:354
noisy false:1574
after delete: len(clean_dataset) 1208
after delete: len(noisy_dataset) 1928
epoch [31/200] batch [5/37] time 0.478 (0.437) data 0.347 (0.306) loss_x loss_x 1.0537 (1.1690) acc_x 65.6250 (69.3750) lr 1.8980e-03 eta 0:00:13
epoch [31/200] batch [10/37] time 0.450 (0.472) data 0.320 (0.341) loss_x loss_x 1.7305 (1.2868) acc_x 56.2500 (66.8750) lr 1.8980e-03 eta 0:00:12
epoch [31/200] batch [15/37] time 0.490 (0.452) data 0.359 (0.321) loss_x loss_x 1.5400 (1.3418) acc_x 65.6250 (65.4167) lr 1.8980e-03 eta 0:00:09
epoch [31/200] batch [20/37] time 0.460 (0.452) data 0.330 (0.322) loss_x loss_x 1.8721 (1.3645) acc_x 53.1250 (64.6875) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [25/37] time 0.382 (0.451) data 0.251 (0.320) loss_x loss_x 1.6270 (1.3762) acc_x 65.6250 (64.8750) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [30/37] time 0.517 (0.459) data 0.387 (0.329) loss_x loss_x 1.2061 (1.3518) acc_x 68.7500 (65.4167) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [35/37] time 0.414 (0.455) data 0.283 (0.325) loss_x loss_x 1.4014 (1.3602) acc_x 62.5000 (65.5357) lr 1.8980e-03 eta 0:00:00
epoch [31/200] batch [5/60] time 0.361 (0.452) data 0.231 (0.322) loss_u loss_u 0.8628 (0.8210) acc_u 18.7500 (21.8750) lr 1.8980e-03 eta 0:00:24
epoch [31/200] batch [10/60] time 0.343 (0.443) data 0.213 (0.313) loss_u loss_u 0.8188 (0.8239) acc_u 25.0000 (21.8750) lr 1.8980e-03 eta 0:00:22
epoch [31/200] batch [15/60] time 0.321 (0.443) data 0.190 (0.313) loss_u loss_u 0.8604 (0.8208) acc_u 12.5000 (21.6667) lr 1.8980e-03 eta 0:00:19
epoch [31/200] batch [20/60] time 0.437 (0.446) data 0.307 (0.316) loss_u loss_u 0.8052 (0.8268) acc_u 25.0000 (21.2500) lr 1.8980e-03 eta 0:00:17
epoch [31/200] batch [25/60] time 0.406 (0.443) data 0.274 (0.312) loss_u loss_u 0.8652 (0.8330) acc_u 18.7500 (20.6250) lr 1.8980e-03 eta 0:00:15
epoch [31/200] batch [30/60] time 0.482 (0.446) data 0.351 (0.315) loss_u loss_u 0.8032 (0.8322) acc_u 31.2500 (20.9375) lr 1.8980e-03 eta 0:00:13
epoch [31/200] batch [35/60] time 0.398 (0.442) data 0.267 (0.312) loss_u loss_u 0.8442 (0.8366) acc_u 15.6250 (20.3571) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [40/60] time 0.402 (0.441) data 0.271 (0.310) loss_u loss_u 0.8511 (0.8356) acc_u 25.0000 (20.6250) lr 1.8980e-03 eta 0:00:08
epoch [31/200] batch [45/60] time 0.563 (0.442) data 0.430 (0.311) loss_u loss_u 0.8594 (0.8341) acc_u 21.8750 (20.6250) lr 1.8980e-03 eta 0:00:06
epoch [31/200] batch [50/60] time 0.479 (0.444) data 0.347 (0.313) loss_u loss_u 0.8672 (0.8343) acc_u 21.8750 (20.8125) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [55/60] time 0.415 (0.446) data 0.283 (0.315) loss_u loss_u 0.7920 (0.8321) acc_u 34.3750 (21.3068) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [60/60] time 0.414 (0.445) data 0.283 (0.314) loss_u loss_u 0.8887 (0.8351) acc_u 12.5000 (21.0417) lr 1.8980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1610
confident_label rate tensor(0.3814, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1196
clean true:1169
clean false:27
clean_rate:0.9774247491638796
noisy true:357
noisy false:1583
after delete: len(clean_dataset) 1196
after delete: len(noisy_dataset) 1940
epoch [32/200] batch [5/37] time 0.604 (0.475) data 0.474 (0.344) loss_x loss_x 1.3057 (1.3914) acc_x 62.5000 (66.8750) lr 1.8910e-03 eta 0:00:15
epoch [32/200] batch [10/37] time 0.510 (0.487) data 0.380 (0.356) loss_x loss_x 1.5078 (1.4342) acc_x 65.6250 (65.6250) lr 1.8910e-03 eta 0:00:13
epoch [32/200] batch [15/37] time 0.450 (0.472) data 0.319 (0.341) loss_x loss_x 1.1328 (1.3852) acc_x 59.3750 (65.8333) lr 1.8910e-03 eta 0:00:10
epoch [32/200] batch [20/37] time 0.435 (0.478) data 0.304 (0.347) loss_x loss_x 1.7617 (1.3978) acc_x 53.1250 (65.3125) lr 1.8910e-03 eta 0:00:08
epoch [32/200] batch [25/37] time 0.451 (0.469) data 0.320 (0.338) loss_x loss_x 0.9585 (1.3877) acc_x 75.0000 (66.0000) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [30/37] time 0.461 (0.467) data 0.331 (0.336) loss_x loss_x 1.5186 (1.3492) acc_x 65.6250 (66.9792) lr 1.8910e-03 eta 0:00:03
epoch [32/200] batch [35/37] time 0.408 (0.467) data 0.278 (0.337) loss_x loss_x 1.2373 (1.3524) acc_x 75.0000 (67.3214) lr 1.8910e-03 eta 0:00:00
epoch [32/200] batch [5/60] time 0.471 (0.463) data 0.339 (0.332) loss_u loss_u 0.8633 (0.8571) acc_u 18.7500 (17.5000) lr 1.8910e-03 eta 0:00:25
epoch [32/200] batch [10/60] time 0.518 (0.465) data 0.388 (0.334) loss_u loss_u 0.8594 (0.8391) acc_u 15.6250 (18.1250) lr 1.8910e-03 eta 0:00:23
epoch [32/200] batch [15/60] time 0.450 (0.460) data 0.320 (0.329) loss_u loss_u 0.8018 (0.8430) acc_u 25.0000 (17.7083) lr 1.8910e-03 eta 0:00:20
epoch [32/200] batch [20/60] time 0.500 (0.456) data 0.369 (0.325) loss_u loss_u 0.8472 (0.8382) acc_u 18.7500 (19.3750) lr 1.8910e-03 eta 0:00:18
epoch [32/200] batch [25/60] time 0.429 (0.455) data 0.298 (0.324) loss_u loss_u 0.8145 (0.8357) acc_u 21.8750 (19.8750) lr 1.8910e-03 eta 0:00:15
epoch [32/200] batch [30/60] time 0.427 (0.451) data 0.296 (0.320) loss_u loss_u 0.8125 (0.8357) acc_u 28.1250 (20.1042) lr 1.8910e-03 eta 0:00:13
epoch [32/200] batch [35/60] time 0.474 (0.450) data 0.343 (0.319) loss_u loss_u 0.8423 (0.8362) acc_u 18.7500 (19.9107) lr 1.8910e-03 eta 0:00:11
epoch [32/200] batch [40/60] time 0.576 (0.451) data 0.444 (0.320) loss_u loss_u 0.8076 (0.8363) acc_u 28.1250 (19.9219) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [45/60] time 0.398 (0.455) data 0.267 (0.324) loss_u loss_u 0.7754 (0.8364) acc_u 31.2500 (19.8611) lr 1.8910e-03 eta 0:00:06
epoch [32/200] batch [50/60] time 0.368 (0.451) data 0.236 (0.320) loss_u loss_u 0.7847 (0.8337) acc_u 25.0000 (20.3750) lr 1.8910e-03 eta 0:00:04
epoch [32/200] batch [55/60] time 0.356 (0.448) data 0.225 (0.317) loss_u loss_u 0.7734 (0.8332) acc_u 28.1250 (20.7386) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [60/60] time 0.487 (0.448) data 0.355 (0.317) loss_u loss_u 0.7891 (0.8354) acc_u 25.0000 (20.3646) lr 1.8910e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1615
confident_label rate tensor(0.3811, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1195
clean true:1163
clean false:32
clean_rate:0.9732217573221758
noisy true:358
noisy false:1583
after delete: len(clean_dataset) 1195
after delete: len(noisy_dataset) 1941
epoch [33/200] batch [5/37] time 0.539 (0.482) data 0.409 (0.351) loss_x loss_x 1.1475 (1.3014) acc_x 62.5000 (61.2500) lr 1.8838e-03 eta 0:00:15
epoch [33/200] batch [10/37] time 0.356 (0.484) data 0.225 (0.354) loss_x loss_x 1.7793 (1.3472) acc_x 56.2500 (62.8125) lr 1.8838e-03 eta 0:00:13
epoch [33/200] batch [15/37] time 0.546 (0.484) data 0.415 (0.354) loss_x loss_x 1.2285 (1.3743) acc_x 65.6250 (62.9167) lr 1.8838e-03 eta 0:00:10
epoch [33/200] batch [20/37] time 0.417 (0.471) data 0.287 (0.340) loss_x loss_x 1.7715 (1.3662) acc_x 59.3750 (63.5938) lr 1.8838e-03 eta 0:00:08
epoch [33/200] batch [25/37] time 0.480 (0.463) data 0.350 (0.332) loss_x loss_x 1.5098 (1.3517) acc_x 65.6250 (64.3750) lr 1.8838e-03 eta 0:00:05
epoch [33/200] batch [30/37] time 0.625 (0.460) data 0.494 (0.329) loss_x loss_x 1.1475 (1.3351) acc_x 62.5000 (65.1042) lr 1.8838e-03 eta 0:00:03
epoch [33/200] batch [35/37] time 0.506 (0.468) data 0.376 (0.337) loss_x loss_x 1.1875 (1.3400) acc_x 75.0000 (65.3571) lr 1.8838e-03 eta 0:00:00
epoch [33/200] batch [5/60] time 0.459 (0.458) data 0.327 (0.327) loss_u loss_u 0.8096 (0.8177) acc_u 21.8750 (23.7500) lr 1.8838e-03 eta 0:00:25
epoch [33/200] batch [10/60] time 0.365 (0.454) data 0.234 (0.324) loss_u loss_u 0.8970 (0.8213) acc_u 12.5000 (23.1250) lr 1.8838e-03 eta 0:00:22
epoch [33/200] batch [15/60] time 0.425 (0.453) data 0.294 (0.323) loss_u loss_u 0.8472 (0.8097) acc_u 21.8750 (24.7917) lr 1.8838e-03 eta 0:00:20
epoch [33/200] batch [20/60] time 0.365 (0.447) data 0.234 (0.317) loss_u loss_u 0.8350 (0.8160) acc_u 18.7500 (23.9062) lr 1.8838e-03 eta 0:00:17
epoch [33/200] batch [25/60] time 0.557 (0.448) data 0.425 (0.318) loss_u loss_u 0.7290 (0.8221) acc_u 28.1250 (22.6250) lr 1.8838e-03 eta 0:00:15
epoch [33/200] batch [30/60] time 0.530 (0.451) data 0.398 (0.320) loss_u loss_u 0.7783 (0.8210) acc_u 28.1250 (22.9167) lr 1.8838e-03 eta 0:00:13
epoch [33/200] batch [35/60] time 0.503 (0.457) data 0.372 (0.326) loss_u loss_u 0.7798 (0.8235) acc_u 18.7500 (22.4107) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [40/60] time 0.536 (0.457) data 0.405 (0.327) loss_u loss_u 0.8232 (0.8268) acc_u 21.8750 (21.9531) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [45/60] time 0.421 (0.454) data 0.290 (0.323) loss_u loss_u 0.8330 (0.8309) acc_u 18.7500 (21.5278) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [50/60] time 0.382 (0.453) data 0.252 (0.322) loss_u loss_u 0.8286 (0.8307) acc_u 34.3750 (21.8125) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [55/60] time 0.372 (0.451) data 0.241 (0.320) loss_u loss_u 0.8110 (0.8298) acc_u 25.0000 (21.9318) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [60/60] time 0.384 (0.451) data 0.253 (0.320) loss_u loss_u 0.8501 (0.8308) acc_u 18.7500 (21.5625) lr 1.8838e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1608
confident_label rate tensor(0.3874, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1215
clean true:1185
clean false:30
clean_rate:0.9753086419753086
noisy true:343
noisy false:1578
after delete: len(clean_dataset) 1215
after delete: len(noisy_dataset) 1921
epoch [34/200] batch [5/37] time 0.511 (0.497) data 0.380 (0.366) loss_x loss_x 1.3691 (1.3543) acc_x 81.2500 (70.0000) lr 1.8763e-03 eta 0:00:15
epoch [34/200] batch [10/37] time 0.462 (0.472) data 0.332 (0.341) loss_x loss_x 1.1797 (1.4015) acc_x 68.7500 (66.2500) lr 1.8763e-03 eta 0:00:12
epoch [34/200] batch [15/37] time 0.478 (0.454) data 0.347 (0.323) loss_x loss_x 1.2832 (1.4525) acc_x 53.1250 (63.9583) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [20/37] time 0.525 (0.450) data 0.394 (0.319) loss_x loss_x 1.7441 (1.4219) acc_x 53.1250 (65.3125) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [25/37] time 0.423 (0.451) data 0.293 (0.321) loss_x loss_x 1.6553 (1.4472) acc_x 65.6250 (64.6250) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [30/37] time 0.591 (0.457) data 0.461 (0.327) loss_x loss_x 1.5498 (1.4408) acc_x 59.3750 (64.4792) lr 1.8763e-03 eta 0:00:03
epoch [34/200] batch [35/37] time 0.427 (0.460) data 0.297 (0.330) loss_x loss_x 1.2275 (1.4262) acc_x 75.0000 (64.2857) lr 1.8763e-03 eta 0:00:00
epoch [34/200] batch [5/60] time 0.516 (0.459) data 0.384 (0.328) loss_u loss_u 0.8579 (0.8332) acc_u 15.6250 (20.0000) lr 1.8763e-03 eta 0:00:25
epoch [34/200] batch [10/60] time 0.402 (0.452) data 0.271 (0.321) loss_u loss_u 0.8677 (0.8353) acc_u 15.6250 (20.3125) lr 1.8763e-03 eta 0:00:22
epoch [34/200] batch [15/60] time 0.445 (0.448) data 0.314 (0.318) loss_u loss_u 0.8799 (0.8384) acc_u 18.7500 (19.7917) lr 1.8763e-03 eta 0:00:20
epoch [34/200] batch [20/60] time 0.455 (0.450) data 0.324 (0.320) loss_u loss_u 0.8618 (0.8402) acc_u 18.7500 (20.0000) lr 1.8763e-03 eta 0:00:18
epoch [34/200] batch [25/60] time 0.483 (0.452) data 0.351 (0.321) loss_u loss_u 0.7925 (0.8301) acc_u 31.2500 (21.5000) lr 1.8763e-03 eta 0:00:15
epoch [34/200] batch [30/60] time 0.404 (0.451) data 0.273 (0.321) loss_u loss_u 0.8257 (0.8266) acc_u 28.1250 (22.0833) lr 1.8763e-03 eta 0:00:13
epoch [34/200] batch [35/60] time 0.481 (0.455) data 0.350 (0.324) loss_u loss_u 0.7998 (0.8321) acc_u 25.0000 (20.9821) lr 1.8763e-03 eta 0:00:11
epoch [34/200] batch [40/60] time 0.435 (0.457) data 0.305 (0.326) loss_u loss_u 0.7441 (0.8301) acc_u 34.3750 (21.1719) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [45/60] time 0.631 (0.459) data 0.501 (0.328) loss_u loss_u 0.7568 (0.8289) acc_u 37.5000 (21.6667) lr 1.8763e-03 eta 0:00:06
epoch [34/200] batch [50/60] time 0.369 (0.454) data 0.239 (0.323) loss_u loss_u 0.8271 (0.8286) acc_u 18.7500 (21.8125) lr 1.8763e-03 eta 0:00:04
epoch [34/200] batch [55/60] time 0.474 (0.452) data 0.344 (0.321) loss_u loss_u 0.8125 (0.8278) acc_u 25.0000 (22.1023) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [60/60] time 0.555 (0.453) data 0.424 (0.322) loss_u loss_u 0.8467 (0.8320) acc_u 18.7500 (21.5104) lr 1.8763e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1557
confident_label rate tensor(0.3964, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1243
clean true:1215
clean false:28
clean_rate:0.9774738535800482
noisy true:364
noisy false:1529
after delete: len(clean_dataset) 1243
after delete: len(noisy_dataset) 1893
epoch [35/200] batch [5/38] time 0.416 (0.481) data 0.286 (0.351) loss_x loss_x 1.2412 (1.2258) acc_x 65.6250 (66.8750) lr 1.8686e-03 eta 0:00:15
epoch [35/200] batch [10/38] time 0.394 (0.441) data 0.263 (0.311) loss_x loss_x 1.8633 (1.2728) acc_x 59.3750 (66.8750) lr 1.8686e-03 eta 0:00:12
epoch [35/200] batch [15/38] time 0.399 (0.460) data 0.268 (0.330) loss_x loss_x 1.2334 (1.4129) acc_x 71.8750 (64.7917) lr 1.8686e-03 eta 0:00:10
epoch [35/200] batch [20/38] time 0.559 (0.463) data 0.428 (0.333) loss_x loss_x 1.1836 (1.4199) acc_x 71.8750 (64.5312) lr 1.8686e-03 eta 0:00:08
epoch [35/200] batch [25/38] time 0.476 (0.471) data 0.346 (0.340) loss_x loss_x 1.0977 (1.4325) acc_x 81.2500 (65.3750) lr 1.8686e-03 eta 0:00:06
epoch [35/200] batch [30/38] time 0.549 (0.482) data 0.419 (0.351) loss_x loss_x 1.5010 (1.4292) acc_x 53.1250 (65.0000) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [35/38] time 0.508 (0.487) data 0.377 (0.357) loss_x loss_x 0.8486 (1.4070) acc_x 75.0000 (64.8214) lr 1.8686e-03 eta 0:00:01
epoch [35/200] batch [5/59] time 0.400 (0.481) data 0.270 (0.350) loss_u loss_u 0.8560 (0.8496) acc_u 12.5000 (18.1250) lr 1.8686e-03 eta 0:00:25
epoch [35/200] batch [10/59] time 0.513 (0.477) data 0.383 (0.346) loss_u loss_u 0.8101 (0.8360) acc_u 25.0000 (20.3125) lr 1.8686e-03 eta 0:00:23
epoch [35/200] batch [15/59] time 0.379 (0.473) data 0.249 (0.343) loss_u loss_u 0.8047 (0.8390) acc_u 25.0000 (19.1667) lr 1.8686e-03 eta 0:00:20
epoch [35/200] batch [20/59] time 0.440 (0.467) data 0.310 (0.336) loss_u loss_u 0.7368 (0.8238) acc_u 34.3750 (22.3438) lr 1.8686e-03 eta 0:00:18
epoch [35/200] batch [25/59] time 0.469 (0.463) data 0.339 (0.332) loss_u loss_u 0.8521 (0.8281) acc_u 15.6250 (22.1250) lr 1.8686e-03 eta 0:00:15
epoch [35/200] batch [30/59] time 0.350 (0.459) data 0.220 (0.328) loss_u loss_u 0.7954 (0.8296) acc_u 31.2500 (21.4583) lr 1.8686e-03 eta 0:00:13
epoch [35/200] batch [35/59] time 0.400 (0.460) data 0.269 (0.329) loss_u loss_u 0.8921 (0.8340) acc_u 12.5000 (20.6250) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [40/59] time 0.526 (0.460) data 0.394 (0.330) loss_u loss_u 0.8271 (0.8329) acc_u 21.8750 (21.0156) lr 1.8686e-03 eta 0:00:08
epoch [35/200] batch [45/59] time 0.359 (0.461) data 0.228 (0.330) loss_u loss_u 0.7739 (0.8347) acc_u 37.5000 (20.7639) lr 1.8686e-03 eta 0:00:06
epoch [35/200] batch [50/59] time 0.386 (0.462) data 0.256 (0.332) loss_u loss_u 0.7798 (0.8332) acc_u 28.1250 (21.0000) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [55/59] time 0.622 (0.461) data 0.491 (0.330) loss_u loss_u 0.7734 (0.8345) acc_u 31.2500 (20.9659) lr 1.8686e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1629
confident_label rate tensor(0.3884, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1218
clean true:1182
clean false:36
clean_rate:0.9704433497536946
noisy true:325
noisy false:1593
after delete: len(clean_dataset) 1218
after delete: len(noisy_dataset) 1918
epoch [36/200] batch [5/38] time 0.489 (0.465) data 0.359 (0.334) loss_x loss_x 1.0215 (1.0181) acc_x 71.8750 (73.7500) lr 1.8607e-03 eta 0:00:15
epoch [36/200] batch [10/38] time 0.526 (0.447) data 0.395 (0.317) loss_x loss_x 1.1436 (1.0922) acc_x 59.3750 (70.6250) lr 1.8607e-03 eta 0:00:12
epoch [36/200] batch [15/38] time 0.399 (0.447) data 0.268 (0.317) loss_x loss_x 1.8369 (1.2179) acc_x 59.3750 (69.1667) lr 1.8607e-03 eta 0:00:10
epoch [36/200] batch [20/38] time 0.605 (0.464) data 0.475 (0.334) loss_x loss_x 1.0713 (1.2301) acc_x 71.8750 (68.4375) lr 1.8607e-03 eta 0:00:08
epoch [36/200] batch [25/38] time 0.397 (0.455) data 0.266 (0.325) loss_x loss_x 0.9888 (1.2526) acc_x 71.8750 (67.7500) lr 1.8607e-03 eta 0:00:05
epoch [36/200] batch [30/38] time 0.490 (0.459) data 0.359 (0.329) loss_x loss_x 1.6367 (1.2949) acc_x 53.1250 (66.8750) lr 1.8607e-03 eta 0:00:03
epoch [36/200] batch [35/38] time 0.452 (0.458) data 0.322 (0.328) loss_x loss_x 1.2383 (1.3027) acc_x 62.5000 (66.8750) lr 1.8607e-03 eta 0:00:01
epoch [36/200] batch [5/59] time 0.429 (0.461) data 0.299 (0.330) loss_u loss_u 0.7871 (0.8581) acc_u 31.2500 (20.0000) lr 1.8607e-03 eta 0:00:24
epoch [36/200] batch [10/59] time 0.425 (0.469) data 0.294 (0.339) loss_u loss_u 0.8530 (0.8424) acc_u 18.7500 (19.0625) lr 1.8607e-03 eta 0:00:22
epoch [36/200] batch [15/59] time 0.581 (0.467) data 0.450 (0.336) loss_u loss_u 0.7598 (0.8292) acc_u 28.1250 (20.0000) lr 1.8607e-03 eta 0:00:20
epoch [36/200] batch [20/59] time 0.594 (0.470) data 0.463 (0.340) loss_u loss_u 0.8955 (0.8319) acc_u 12.5000 (20.3125) lr 1.8607e-03 eta 0:00:18
epoch [36/200] batch [25/59] time 0.319 (0.465) data 0.189 (0.335) loss_u loss_u 0.8623 (0.8311) acc_u 21.8750 (20.8750) lr 1.8607e-03 eta 0:00:15
epoch [36/200] batch [30/59] time 0.575 (0.464) data 0.445 (0.333) loss_u loss_u 0.9180 (0.8334) acc_u 9.3750 (20.6250) lr 1.8607e-03 eta 0:00:13
epoch [36/200] batch [35/59] time 0.645 (0.466) data 0.514 (0.335) loss_u loss_u 0.8511 (0.8339) acc_u 15.6250 (20.8036) lr 1.8607e-03 eta 0:00:11
epoch [36/200] batch [40/59] time 0.430 (0.465) data 0.300 (0.334) loss_u loss_u 0.8496 (0.8325) acc_u 18.7500 (20.8594) lr 1.8607e-03 eta 0:00:08
epoch [36/200] batch [45/59] time 0.597 (0.463) data 0.466 (0.333) loss_u loss_u 0.7896 (0.8325) acc_u 28.1250 (20.5556) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [50/59] time 0.576 (0.463) data 0.445 (0.333) loss_u loss_u 0.8555 (0.8320) acc_u 15.6250 (20.4375) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [55/59] time 0.330 (0.459) data 0.199 (0.328) loss_u loss_u 0.8228 (0.8332) acc_u 21.8750 (20.4545) lr 1.8607e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1589
confident_label rate tensor(0.3945, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1237
clean true:1196
clean false:41
clean_rate:0.9668552950687146
noisy true:351
noisy false:1548
after delete: len(clean_dataset) 1237
after delete: len(noisy_dataset) 1899
epoch [37/200] batch [5/38] time 0.456 (0.459) data 0.326 (0.329) loss_x loss_x 1.3984 (1.3711) acc_x 71.8750 (68.7500) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [10/38] time 0.557 (0.463) data 0.426 (0.333) loss_x loss_x 0.9365 (1.3762) acc_x 68.7500 (65.9375) lr 1.8526e-03 eta 0:00:12
epoch [37/200] batch [15/38] time 0.610 (0.477) data 0.480 (0.346) loss_x loss_x 0.8931 (1.3524) acc_x 71.8750 (65.0000) lr 1.8526e-03 eta 0:00:10
epoch [37/200] batch [20/38] time 0.488 (0.470) data 0.358 (0.339) loss_x loss_x 1.2725 (1.3506) acc_x 65.6250 (66.0938) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [25/38] time 0.407 (0.479) data 0.277 (0.349) loss_x loss_x 1.8711 (1.3895) acc_x 50.0000 (64.8750) lr 1.8526e-03 eta 0:00:06
epoch [37/200] batch [30/38] time 0.461 (0.468) data 0.331 (0.338) loss_x loss_x 1.1426 (1.3938) acc_x 62.5000 (64.0625) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [35/38] time 0.492 (0.469) data 0.363 (0.338) loss_x loss_x 1.6973 (1.4099) acc_x 62.5000 (64.2857) lr 1.8526e-03 eta 0:00:01
epoch [37/200] batch [5/59] time 0.391 (0.475) data 0.261 (0.344) loss_u loss_u 0.8755 (0.8131) acc_u 15.6250 (25.0000) lr 1.8526e-03 eta 0:00:25
epoch [37/200] batch [10/59] time 0.356 (0.469) data 0.225 (0.339) loss_u loss_u 0.8076 (0.8082) acc_u 18.7500 (25.0000) lr 1.8526e-03 eta 0:00:22
epoch [37/200] batch [15/59] time 0.444 (0.467) data 0.313 (0.336) loss_u loss_u 0.8442 (0.8230) acc_u 18.7500 (23.1250) lr 1.8526e-03 eta 0:00:20
epoch [37/200] batch [20/59] time 0.482 (0.466) data 0.351 (0.335) loss_u loss_u 0.9028 (0.8288) acc_u 12.5000 (22.8125) lr 1.8526e-03 eta 0:00:18
epoch [37/200] batch [25/59] time 0.404 (0.467) data 0.273 (0.336) loss_u loss_u 0.8511 (0.8274) acc_u 21.8750 (23.3750) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [30/59] time 0.625 (0.468) data 0.494 (0.337) loss_u loss_u 0.8569 (0.8343) acc_u 12.5000 (22.0833) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [35/59] time 0.414 (0.467) data 0.283 (0.336) loss_u loss_u 0.8301 (0.8362) acc_u 21.8750 (21.4286) lr 1.8526e-03 eta 0:00:11
epoch [37/200] batch [40/59] time 0.782 (0.471) data 0.651 (0.340) loss_u loss_u 0.8032 (0.8364) acc_u 28.1250 (21.6406) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [45/59] time 0.370 (0.471) data 0.240 (0.341) loss_u loss_u 0.7607 (0.8353) acc_u 31.2500 (21.5972) lr 1.8526e-03 eta 0:00:06
epoch [37/200] batch [50/59] time 0.367 (0.467) data 0.235 (0.336) loss_u loss_u 0.7651 (0.8333) acc_u 25.0000 (21.5625) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [55/59] time 0.537 (0.469) data 0.406 (0.338) loss_u loss_u 0.7959 (0.8340) acc_u 25.0000 (21.3068) lr 1.8526e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1596
confident_label rate tensor(0.3878, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1216
clean true:1180
clean false:36
clean_rate:0.9703947368421053
noisy true:360
noisy false:1560
after delete: len(clean_dataset) 1216
after delete: len(noisy_dataset) 1920
epoch [38/200] batch [5/38] time 0.495 (0.461) data 0.364 (0.330) loss_x loss_x 1.4014 (1.5520) acc_x 68.7500 (62.5000) lr 1.8443e-03 eta 0:00:15
epoch [38/200] batch [10/38] time 0.615 (0.490) data 0.483 (0.358) loss_x loss_x 1.6055 (1.5213) acc_x 65.6250 (64.3750) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [15/38] time 0.454 (0.489) data 0.324 (0.358) loss_x loss_x 1.1025 (1.4494) acc_x 84.3750 (65.8333) lr 1.8443e-03 eta 0:00:11
epoch [38/200] batch [20/38] time 0.579 (0.496) data 0.448 (0.365) loss_x loss_x 1.3291 (1.4152) acc_x 65.6250 (65.7812) lr 1.8443e-03 eta 0:00:08
epoch [38/200] batch [25/38] time 0.503 (0.491) data 0.372 (0.360) loss_x loss_x 1.1699 (1.3712) acc_x 62.5000 (66.5000) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [30/38] time 0.586 (0.497) data 0.456 (0.366) loss_x loss_x 1.4150 (1.3698) acc_x 65.6250 (66.7708) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [35/38] time 0.412 (0.492) data 0.277 (0.361) loss_x loss_x 0.9580 (1.3562) acc_x 71.8750 (66.4286) lr 1.8443e-03 eta 0:00:01
epoch [38/200] batch [5/60] time 0.370 (0.481) data 0.240 (0.351) loss_u loss_u 0.7710 (0.8364) acc_u 37.5000 (22.5000) lr 1.8443e-03 eta 0:00:26
epoch [38/200] batch [10/60] time 0.564 (0.480) data 0.432 (0.350) loss_u loss_u 0.7754 (0.8088) acc_u 34.3750 (25.9375) lr 1.8443e-03 eta 0:00:24
epoch [38/200] batch [15/60] time 0.421 (0.475) data 0.290 (0.345) loss_u loss_u 0.8306 (0.8142) acc_u 31.2500 (25.4167) lr 1.8443e-03 eta 0:00:21
epoch [38/200] batch [20/60] time 0.388 (0.472) data 0.258 (0.341) loss_u loss_u 0.8213 (0.8116) acc_u 25.0000 (25.7812) lr 1.8443e-03 eta 0:00:18
epoch [38/200] batch [25/60] time 0.326 (0.466) data 0.196 (0.336) loss_u loss_u 0.8096 (0.8108) acc_u 25.0000 (25.5000) lr 1.8443e-03 eta 0:00:16
epoch [38/200] batch [30/60] time 0.362 (0.469) data 0.231 (0.338) loss_u loss_u 0.8574 (0.8170) acc_u 18.7500 (24.2708) lr 1.8443e-03 eta 0:00:14
epoch [38/200] batch [35/60] time 0.429 (0.464) data 0.298 (0.333) loss_u loss_u 0.8525 (0.8174) acc_u 15.6250 (24.0179) lr 1.8443e-03 eta 0:00:11
epoch [38/200] batch [40/60] time 0.439 (0.463) data 0.309 (0.332) loss_u loss_u 0.8477 (0.8183) acc_u 18.7500 (23.8281) lr 1.8443e-03 eta 0:00:09
epoch [38/200] batch [45/60] time 0.885 (0.467) data 0.753 (0.336) loss_u loss_u 0.8931 (0.8257) acc_u 12.5000 (22.5000) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [50/60] time 0.418 (0.466) data 0.287 (0.335) loss_u loss_u 0.8892 (0.8266) acc_u 18.7500 (22.2500) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [55/60] time 0.469 (0.463) data 0.338 (0.332) loss_u loss_u 0.7559 (0.8264) acc_u 31.2500 (22.6136) lr 1.8443e-03 eta 0:00:02
epoch [38/200] batch [60/60] time 0.369 (0.461) data 0.237 (0.330) loss_u loss_u 0.8428 (0.8245) acc_u 15.6250 (22.8125) lr 1.8443e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1562
confident_label rate tensor(0.4015, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1259
clean true:1224
clean false:35
clean_rate:0.9722001588562351
noisy true:350
noisy false:1527
after delete: len(clean_dataset) 1259
after delete: len(noisy_dataset) 1877
epoch [39/200] batch [5/39] time 0.680 (0.517) data 0.549 (0.386) loss_x loss_x 1.4863 (1.5631) acc_x 78.1250 (65.6250) lr 1.8358e-03 eta 0:00:17
epoch [39/200] batch [10/39] time 0.477 (0.507) data 0.347 (0.376) loss_x loss_x 1.5693 (1.4952) acc_x 62.5000 (63.7500) lr 1.8358e-03 eta 0:00:14
epoch [39/200] batch [15/39] time 0.391 (0.478) data 0.261 (0.348) loss_x loss_x 1.8008 (1.4549) acc_x 56.2500 (64.7917) lr 1.8358e-03 eta 0:00:11
epoch [39/200] batch [20/39] time 0.418 (0.467) data 0.287 (0.337) loss_x loss_x 1.9639 (1.4639) acc_x 50.0000 (64.0625) lr 1.8358e-03 eta 0:00:08
epoch [39/200] batch [25/39] time 0.490 (0.459) data 0.360 (0.329) loss_x loss_x 0.8086 (1.4174) acc_x 71.8750 (64.8750) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [30/39] time 0.557 (0.466) data 0.427 (0.335) loss_x loss_x 1.6973 (1.4347) acc_x 59.3750 (64.1667) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [35/39] time 0.490 (0.471) data 0.359 (0.341) loss_x loss_x 0.9585 (1.4107) acc_x 75.0000 (64.4643) lr 1.8358e-03 eta 0:00:01
epoch [39/200] batch [5/58] time 0.370 (0.461) data 0.239 (0.330) loss_u loss_u 0.8306 (0.8118) acc_u 15.6250 (23.1250) lr 1.8358e-03 eta 0:00:24
epoch [39/200] batch [10/58] time 0.556 (0.460) data 0.426 (0.329) loss_u loss_u 0.8301 (0.8169) acc_u 21.8750 (22.8125) lr 1.8358e-03 eta 0:00:22
epoch [39/200] batch [15/58] time 0.415 (0.460) data 0.284 (0.329) loss_u loss_u 0.8101 (0.8158) acc_u 31.2500 (23.3333) lr 1.8358e-03 eta 0:00:19
epoch [39/200] batch [20/58] time 0.378 (0.468) data 0.248 (0.338) loss_u loss_u 0.8623 (0.8211) acc_u 15.6250 (22.9688) lr 1.8358e-03 eta 0:00:17
epoch [39/200] batch [25/58] time 0.486 (0.464) data 0.354 (0.334) loss_u loss_u 0.8066 (0.8250) acc_u 18.7500 (22.2500) lr 1.8358e-03 eta 0:00:15
epoch [39/200] batch [30/58] time 0.397 (0.466) data 0.267 (0.335) loss_u loss_u 0.8657 (0.8320) acc_u 15.6250 (21.2500) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [35/58] time 0.494 (0.465) data 0.362 (0.335) loss_u loss_u 0.8291 (0.8327) acc_u 25.0000 (20.8929) lr 1.8358e-03 eta 0:00:10
epoch [39/200] batch [40/58] time 0.468 (0.467) data 0.337 (0.336) loss_u loss_u 0.9082 (0.8311) acc_u 15.6250 (21.2500) lr 1.8358e-03 eta 0:00:08
epoch [39/200] batch [45/58] time 0.395 (0.464) data 0.263 (0.334) loss_u loss_u 0.8574 (0.8348) acc_u 15.6250 (20.8333) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [50/58] time 0.427 (0.461) data 0.295 (0.331) loss_u loss_u 0.7822 (0.8323) acc_u 25.0000 (21.1875) lr 1.8358e-03 eta 0:00:03
epoch [39/200] batch [55/58] time 0.435 (0.461) data 0.304 (0.331) loss_u loss_u 0.8027 (0.8316) acc_u 21.8750 (21.2500) lr 1.8358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1572
confident_label rate tensor(0.4011, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1258
clean true:1223
clean false:35
clean_rate:0.9721780604133545
noisy true:341
noisy false:1537
after delete: len(clean_dataset) 1258
after delete: len(noisy_dataset) 1878
epoch [40/200] batch [5/39] time 0.619 (0.458) data 0.488 (0.328) loss_x loss_x 1.9277 (1.5705) acc_x 59.3750 (60.6250) lr 1.8271e-03 eta 0:00:15
epoch [40/200] batch [10/39] time 0.424 (0.475) data 0.292 (0.345) loss_x loss_x 1.3389 (1.5393) acc_x 68.7500 (61.8750) lr 1.8271e-03 eta 0:00:13
epoch [40/200] batch [15/39] time 0.369 (0.483) data 0.238 (0.352) loss_x loss_x 1.0068 (1.5052) acc_x 68.7500 (62.9167) lr 1.8271e-03 eta 0:00:11
epoch [40/200] batch [20/39] time 0.548 (0.478) data 0.418 (0.348) loss_x loss_x 1.3672 (1.4305) acc_x 59.3750 (63.5938) lr 1.8271e-03 eta 0:00:09
epoch [40/200] batch [25/39] time 0.376 (0.463) data 0.246 (0.333) loss_x loss_x 1.1777 (1.4360) acc_x 81.2500 (63.5000) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [30/39] time 0.735 (0.466) data 0.605 (0.335) loss_x loss_x 1.5498 (1.4247) acc_x 65.6250 (64.1667) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [35/39] time 0.408 (0.458) data 0.278 (0.328) loss_x loss_x 1.2559 (1.4011) acc_x 65.6250 (65.1786) lr 1.8271e-03 eta 0:00:01
epoch [40/200] batch [5/58] time 0.721 (0.460) data 0.589 (0.329) loss_u loss_u 0.8643 (0.8335) acc_u 12.5000 (20.6250) lr 1.8271e-03 eta 0:00:24
epoch [40/200] batch [10/58] time 0.487 (0.460) data 0.356 (0.329) loss_u loss_u 0.9219 (0.8384) acc_u 9.3750 (19.6875) lr 1.8271e-03 eta 0:00:22
epoch [40/200] batch [15/58] time 0.391 (0.458) data 0.261 (0.328) loss_u loss_u 0.9141 (0.8468) acc_u 12.5000 (18.9583) lr 1.8271e-03 eta 0:00:19
epoch [40/200] batch [20/58] time 0.600 (0.460) data 0.470 (0.330) loss_u loss_u 0.8037 (0.8438) acc_u 18.7500 (19.3750) lr 1.8271e-03 eta 0:00:17
epoch [40/200] batch [25/58] time 0.528 (0.459) data 0.397 (0.328) loss_u loss_u 0.8379 (0.8406) acc_u 21.8750 (20.5000) lr 1.8271e-03 eta 0:00:15
epoch [40/200] batch [30/58] time 0.333 (0.453) data 0.201 (0.322) loss_u loss_u 0.8247 (0.8362) acc_u 12.5000 (20.9375) lr 1.8271e-03 eta 0:00:12
epoch [40/200] batch [35/58] time 0.560 (0.451) data 0.429 (0.320) loss_u loss_u 0.8223 (0.8308) acc_u 28.1250 (21.9643) lr 1.8271e-03 eta 0:00:10
epoch [40/200] batch [40/58] time 0.399 (0.451) data 0.268 (0.321) loss_u loss_u 0.8711 (0.8304) acc_u 15.6250 (22.3438) lr 1.8271e-03 eta 0:00:08
epoch [40/200] batch [45/58] time 0.314 (0.451) data 0.182 (0.321) loss_u loss_u 0.8887 (0.8320) acc_u 18.7500 (22.3611) lr 1.8271e-03 eta 0:00:05
epoch [40/200] batch [50/58] time 0.445 (0.452) data 0.314 (0.321) loss_u loss_u 0.7476 (0.8305) acc_u 34.3750 (22.7500) lr 1.8271e-03 eta 0:00:03
epoch [40/200] batch [55/58] time 0.462 (0.451) data 0.330 (0.320) loss_u loss_u 0.8838 (0.8309) acc_u 18.7500 (22.3295) lr 1.8271e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1574
confident_label rate tensor(0.3989, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1251
clean true:1222
clean false:29
clean_rate:0.9768185451638689
noisy true:340
noisy false:1545
after delete: len(clean_dataset) 1251
after delete: len(noisy_dataset) 1885
epoch [41/200] batch [5/39] time 0.533 (0.521) data 0.403 (0.390) loss_x loss_x 1.2803 (1.3066) acc_x 59.3750 (62.5000) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [10/39] time 0.385 (0.472) data 0.254 (0.342) loss_x loss_x 1.1387 (1.3164) acc_x 65.6250 (63.1250) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [15/39] time 0.586 (0.472) data 0.456 (0.342) loss_x loss_x 1.5840 (1.3826) acc_x 71.8750 (64.3750) lr 1.8181e-03 eta 0:00:11
epoch [41/200] batch [20/39] time 0.599 (0.476) data 0.469 (0.345) loss_x loss_x 1.3584 (1.3518) acc_x 71.8750 (66.0938) lr 1.8181e-03 eta 0:00:09
epoch [41/200] batch [25/39] time 0.510 (0.481) data 0.380 (0.351) loss_x loss_x 1.0430 (1.3603) acc_x 68.7500 (65.6250) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [30/39] time 0.652 (0.483) data 0.522 (0.352) loss_x loss_x 0.7974 (1.3170) acc_x 68.7500 (66.2500) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [35/39] time 0.558 (0.484) data 0.427 (0.354) loss_x loss_x 1.2285 (1.2997) acc_x 59.3750 (66.5179) lr 1.8181e-03 eta 0:00:01
epoch [41/200] batch [5/58] time 0.383 (0.476) data 0.252 (0.346) loss_u loss_u 0.8208 (0.8463) acc_u 25.0000 (18.1250) lr 1.8181e-03 eta 0:00:25
epoch [41/200] batch [10/58] time 0.391 (0.479) data 0.260 (0.348) loss_u loss_u 0.8276 (0.8360) acc_u 21.8750 (20.3125) lr 1.8181e-03 eta 0:00:22
epoch [41/200] batch [15/58] time 0.407 (0.472) data 0.276 (0.341) loss_u loss_u 0.8271 (0.8396) acc_u 12.5000 (19.7917) lr 1.8181e-03 eta 0:00:20
epoch [41/200] batch [20/58] time 0.625 (0.473) data 0.494 (0.343) loss_u loss_u 0.8350 (0.8419) acc_u 21.8750 (19.3750) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [25/58] time 0.379 (0.467) data 0.248 (0.336) loss_u loss_u 0.8662 (0.8368) acc_u 21.8750 (21.0000) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [30/58] time 0.579 (0.469) data 0.448 (0.338) loss_u loss_u 0.8940 (0.8339) acc_u 12.5000 (21.6667) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [35/58] time 0.442 (0.469) data 0.311 (0.339) loss_u loss_u 0.8745 (0.8325) acc_u 21.8750 (21.8750) lr 1.8181e-03 eta 0:00:10
epoch [41/200] batch [40/58] time 0.450 (0.466) data 0.319 (0.336) loss_u loss_u 0.8311 (0.8340) acc_u 25.0000 (21.5625) lr 1.8181e-03 eta 0:00:08
epoch [41/200] batch [45/58] time 0.362 (0.463) data 0.230 (0.333) loss_u loss_u 0.9023 (0.8372) acc_u 6.2500 (20.9722) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [50/58] time 0.425 (0.461) data 0.294 (0.330) loss_u loss_u 0.8647 (0.8381) acc_u 12.5000 (20.8125) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [55/58] time 0.435 (0.460) data 0.304 (0.329) loss_u loss_u 0.8501 (0.8370) acc_u 18.7500 (20.6250) lr 1.8181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1550
confident_label rate tensor(0.4018, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1260
clean true:1225
clean false:35
clean_rate:0.9722222222222222
noisy true:361
noisy false:1515
after delete: len(clean_dataset) 1260
after delete: len(noisy_dataset) 1876
epoch [42/200] batch [5/39] time 0.467 (0.443) data 0.338 (0.312) loss_x loss_x 2.1289 (1.6870) acc_x 50.0000 (61.2500) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [10/39] time 0.474 (0.472) data 0.343 (0.342) loss_x loss_x 1.5332 (1.4942) acc_x 68.7500 (65.3125) lr 1.8090e-03 eta 0:00:13
epoch [42/200] batch [15/39] time 0.453 (0.467) data 0.323 (0.337) loss_x loss_x 1.1240 (1.4507) acc_x 75.0000 (64.7917) lr 1.8090e-03 eta 0:00:11
epoch [42/200] batch [20/39] time 0.500 (0.455) data 0.370 (0.325) loss_x loss_x 1.3330 (1.4536) acc_x 65.6250 (64.0625) lr 1.8090e-03 eta 0:00:08
epoch [42/200] batch [25/39] time 0.400 (0.458) data 0.270 (0.328) loss_x loss_x 1.0801 (1.4162) acc_x 68.7500 (64.3750) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [30/39] time 0.684 (0.463) data 0.553 (0.333) loss_x loss_x 1.7451 (1.4130) acc_x 53.1250 (63.8542) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [35/39] time 0.403 (0.466) data 0.273 (0.336) loss_x loss_x 1.1270 (1.3881) acc_x 68.7500 (64.4643) lr 1.8090e-03 eta 0:00:01
epoch [42/200] batch [5/58] time 0.387 (0.461) data 0.256 (0.331) loss_u loss_u 0.7817 (0.8373) acc_u 28.1250 (19.3750) lr 1.8090e-03 eta 0:00:24
epoch [42/200] batch [10/58] time 0.466 (0.459) data 0.335 (0.329) loss_u loss_u 0.8516 (0.8277) acc_u 21.8750 (20.0000) lr 1.8090e-03 eta 0:00:22
epoch [42/200] batch [15/58] time 0.453 (0.462) data 0.322 (0.332) loss_u loss_u 0.8354 (0.8387) acc_u 18.7500 (18.7500) lr 1.8090e-03 eta 0:00:19
epoch [42/200] batch [20/58] time 0.510 (0.464) data 0.380 (0.334) loss_u loss_u 0.8267 (0.8337) acc_u 21.8750 (19.2188) lr 1.8090e-03 eta 0:00:17
epoch [42/200] batch [25/58] time 0.398 (0.459) data 0.267 (0.329) loss_u loss_u 0.8022 (0.8326) acc_u 21.8750 (19.6250) lr 1.8090e-03 eta 0:00:15
epoch [42/200] batch [30/58] time 0.321 (0.458) data 0.191 (0.327) loss_u loss_u 0.8545 (0.8342) acc_u 12.5000 (19.4792) lr 1.8090e-03 eta 0:00:12
epoch [42/200] batch [35/58] time 0.817 (0.464) data 0.686 (0.334) loss_u loss_u 0.9136 (0.8383) acc_u 9.3750 (19.0179) lr 1.8090e-03 eta 0:00:10
epoch [42/200] batch [40/58] time 0.451 (0.464) data 0.319 (0.334) loss_u loss_u 0.8218 (0.8354) acc_u 25.0000 (20.0000) lr 1.8090e-03 eta 0:00:08
epoch [42/200] batch [45/58] time 0.482 (0.464) data 0.352 (0.333) loss_u loss_u 0.9019 (0.8366) acc_u 15.6250 (19.8611) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [50/58] time 0.424 (0.461) data 0.293 (0.331) loss_u loss_u 0.8428 (0.8360) acc_u 18.7500 (19.8125) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [55/58] time 0.678 (0.462) data 0.547 (0.331) loss_u loss_u 0.7646 (0.8349) acc_u 28.1250 (19.9432) lr 1.8090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1551
confident_label rate tensor(0.3999, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1254
clean true:1225
clean false:29
clean_rate:0.9768740031897927
noisy true:360
noisy false:1522
after delete: len(clean_dataset) 1254
after delete: len(noisy_dataset) 1882
epoch [43/200] batch [5/39] time 0.420 (0.516) data 0.290 (0.385) loss_x loss_x 1.3955 (1.2783) acc_x 62.5000 (66.2500) lr 1.7997e-03 eta 0:00:17
epoch [43/200] batch [10/39] time 0.417 (0.477) data 0.287 (0.347) loss_x loss_x 1.7891 (1.3247) acc_x 50.0000 (65.0000) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [15/39] time 0.521 (0.476) data 0.390 (0.346) loss_x loss_x 0.7944 (1.2690) acc_x 65.6250 (66.4583) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [20/39] time 0.402 (0.472) data 0.272 (0.342) loss_x loss_x 1.4336 (1.3340) acc_x 59.3750 (64.8438) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [25/39] time 0.424 (0.468) data 0.294 (0.338) loss_x loss_x 0.9209 (1.3488) acc_x 75.0000 (65.1250) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [30/39] time 0.534 (0.470) data 0.403 (0.340) loss_x loss_x 1.6045 (1.3433) acc_x 56.2500 (64.8958) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [35/39] time 0.714 (0.483) data 0.584 (0.353) loss_x loss_x 1.4951 (1.3314) acc_x 65.6250 (65.0893) lr 1.7997e-03 eta 0:00:01
epoch [43/200] batch [5/58] time 0.416 (0.476) data 0.286 (0.346) loss_u loss_u 0.7817 (0.7859) acc_u 31.2500 (27.5000) lr 1.7997e-03 eta 0:00:25
epoch [43/200] batch [10/58] time 0.399 (0.470) data 0.267 (0.340) loss_u loss_u 0.8506 (0.8017) acc_u 21.8750 (24.3750) lr 1.7997e-03 eta 0:00:22
epoch [43/200] batch [15/58] time 0.385 (0.465) data 0.254 (0.334) loss_u loss_u 0.8330 (0.8112) acc_u 18.7500 (23.1250) lr 1.7997e-03 eta 0:00:19
epoch [43/200] batch [20/58] time 0.500 (0.467) data 0.369 (0.336) loss_u loss_u 0.8350 (0.8159) acc_u 21.8750 (22.9688) lr 1.7997e-03 eta 0:00:17
epoch [43/200] batch [25/58] time 0.434 (0.466) data 0.303 (0.336) loss_u loss_u 0.8608 (0.8180) acc_u 21.8750 (23.2500) lr 1.7997e-03 eta 0:00:15
epoch [43/200] batch [30/58] time 0.584 (0.464) data 0.453 (0.334) loss_u loss_u 0.7935 (0.8181) acc_u 25.0000 (23.3333) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [35/58] time 0.543 (0.464) data 0.411 (0.333) loss_u loss_u 0.8721 (0.8206) acc_u 12.5000 (22.9464) lr 1.7997e-03 eta 0:00:10
epoch [43/200] batch [40/58] time 0.414 (0.460) data 0.284 (0.329) loss_u loss_u 0.8398 (0.8246) acc_u 21.8750 (22.3438) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [45/58] time 0.448 (0.460) data 0.317 (0.330) loss_u loss_u 0.7842 (0.8274) acc_u 25.0000 (21.6667) lr 1.7997e-03 eta 0:00:05
epoch [43/200] batch [50/58] time 0.334 (0.457) data 0.204 (0.326) loss_u loss_u 0.8008 (0.8259) acc_u 25.0000 (21.8125) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [55/58] time 0.411 (0.453) data 0.280 (0.322) loss_u loss_u 0.8506 (0.8261) acc_u 18.7500 (21.7614) lr 1.7997e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1592
confident_label rate tensor(0.3932, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1233
clean true:1203
clean false:30
clean_rate:0.975669099756691
noisy true:341
noisy false:1562
after delete: len(clean_dataset) 1233
after delete: len(noisy_dataset) 1903
epoch [44/200] batch [5/38] time 0.475 (0.449) data 0.345 (0.318) loss_x loss_x 1.4883 (1.4928) acc_x 59.3750 (61.2500) lr 1.7902e-03 eta 0:00:14
epoch [44/200] batch [10/38] time 0.523 (0.439) data 0.393 (0.309) loss_x loss_x 0.9155 (1.4426) acc_x 78.1250 (63.7500) lr 1.7902e-03 eta 0:00:12
epoch [44/200] batch [15/38] time 0.437 (0.444) data 0.306 (0.314) loss_x loss_x 1.1719 (1.4769) acc_x 75.0000 (64.5833) lr 1.7902e-03 eta 0:00:10
epoch [44/200] batch [20/38] time 0.627 (0.457) data 0.496 (0.327) loss_x loss_x 0.8555 (1.4223) acc_x 78.1250 (64.2188) lr 1.7902e-03 eta 0:00:08
epoch [44/200] batch [25/38] time 0.430 (0.460) data 0.300 (0.330) loss_x loss_x 1.0488 (1.3871) acc_x 65.6250 (64.2500) lr 1.7902e-03 eta 0:00:05
epoch [44/200] batch [30/38] time 0.418 (0.448) data 0.288 (0.317) loss_x loss_x 1.2617 (1.3998) acc_x 71.8750 (64.7917) lr 1.7902e-03 eta 0:00:03
epoch [44/200] batch [35/38] time 0.440 (0.448) data 0.310 (0.318) loss_x loss_x 1.5029 (1.3783) acc_x 71.8750 (65.8929) lr 1.7902e-03 eta 0:00:01
epoch [44/200] batch [5/59] time 0.416 (0.440) data 0.285 (0.309) loss_u loss_u 0.7612 (0.8036) acc_u 28.1250 (22.5000) lr 1.7902e-03 eta 0:00:23
epoch [44/200] batch [10/59] time 0.438 (0.438) data 0.307 (0.307) loss_u loss_u 0.7954 (0.8038) acc_u 25.0000 (24.3750) lr 1.7902e-03 eta 0:00:21
epoch [44/200] batch [15/59] time 0.431 (0.441) data 0.299 (0.311) loss_u loss_u 0.8638 (0.8240) acc_u 12.5000 (21.0417) lr 1.7902e-03 eta 0:00:19
epoch [44/200] batch [20/59] time 0.409 (0.441) data 0.278 (0.311) loss_u loss_u 0.7490 (0.8238) acc_u 31.2500 (21.2500) lr 1.7902e-03 eta 0:00:17
epoch [44/200] batch [25/59] time 0.361 (0.444) data 0.231 (0.314) loss_u loss_u 0.7827 (0.8218) acc_u 28.1250 (21.8750) lr 1.7902e-03 eta 0:00:15
epoch [44/200] batch [30/59] time 0.463 (0.447) data 0.331 (0.316) loss_u loss_u 0.8042 (0.8248) acc_u 21.8750 (21.5625) lr 1.7902e-03 eta 0:00:12
epoch [44/200] batch [35/59] time 0.449 (0.449) data 0.316 (0.318) loss_u loss_u 0.8999 (0.8353) acc_u 9.3750 (20.0000) lr 1.7902e-03 eta 0:00:10
epoch [44/200] batch [40/59] time 0.441 (0.449) data 0.309 (0.319) loss_u loss_u 0.9136 (0.8319) acc_u 9.3750 (20.4688) lr 1.7902e-03 eta 0:00:08
epoch [44/200] batch [45/59] time 0.349 (0.447) data 0.218 (0.316) loss_u loss_u 0.8799 (0.8358) acc_u 15.6250 (20.0694) lr 1.7902e-03 eta 0:00:06
epoch [44/200] batch [50/59] time 0.375 (0.448) data 0.245 (0.318) loss_u loss_u 0.8560 (0.8353) acc_u 15.6250 (20.3750) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [55/59] time 0.587 (0.448) data 0.455 (0.318) loss_u loss_u 0.7983 (0.8362) acc_u 25.0000 (20.4545) lr 1.7902e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1535
confident_label rate tensor(0.4094, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1284
clean true:1257
clean false:27
clean_rate:0.9789719626168224
noisy true:344
noisy false:1508
after delete: len(clean_dataset) 1284
after delete: len(noisy_dataset) 1852
epoch [45/200] batch [5/40] time 0.415 (0.453) data 0.284 (0.322) loss_x loss_x 1.4619 (1.2707) acc_x 62.5000 (64.3750) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [10/40] time 0.555 (0.474) data 0.425 (0.343) loss_x loss_x 1.2129 (1.3238) acc_x 71.8750 (65.0000) lr 1.7804e-03 eta 0:00:14
epoch [45/200] batch [15/40] time 0.439 (0.462) data 0.309 (0.332) loss_x loss_x 1.1650 (1.3239) acc_x 78.1250 (65.2083) lr 1.7804e-03 eta 0:00:11
epoch [45/200] batch [20/40] time 0.396 (0.449) data 0.265 (0.318) loss_x loss_x 1.3906 (1.3187) acc_x 68.7500 (66.4062) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [25/40] time 0.403 (0.457) data 0.272 (0.327) loss_x loss_x 2.3965 (1.3664) acc_x 43.7500 (65.2500) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [30/40] time 0.655 (0.460) data 0.525 (0.330) loss_x loss_x 1.3447 (1.3771) acc_x 65.6250 (64.7917) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [35/40] time 0.381 (0.461) data 0.251 (0.331) loss_x loss_x 0.8213 (1.3342) acc_x 87.5000 (66.4286) lr 1.7804e-03 eta 0:00:02
epoch [45/200] batch [40/40] time 0.378 (0.455) data 0.248 (0.325) loss_x loss_x 1.9971 (1.3589) acc_x 53.1250 (65.8594) lr 1.7804e-03 eta 0:00:00
epoch [45/200] batch [5/57] time 0.422 (0.452) data 0.292 (0.322) loss_u loss_u 0.6880 (0.7969) acc_u 43.7500 (26.8750) lr 1.7804e-03 eta 0:00:23
epoch [45/200] batch [10/57] time 0.475 (0.450) data 0.345 (0.319) loss_u loss_u 0.8203 (0.8055) acc_u 15.6250 (23.7500) lr 1.7804e-03 eta 0:00:21
epoch [45/200] batch [15/57] time 0.408 (0.447) data 0.278 (0.316) loss_u loss_u 0.9033 (0.8081) acc_u 9.3750 (23.7500) lr 1.7804e-03 eta 0:00:18
epoch [45/200] batch [20/57] time 0.445 (0.447) data 0.313 (0.317) loss_u loss_u 0.8140 (0.8083) acc_u 31.2500 (24.6875) lr 1.7804e-03 eta 0:00:16
epoch [45/200] batch [25/57] time 0.661 (0.453) data 0.530 (0.322) loss_u loss_u 0.8091 (0.8130) acc_u 28.1250 (24.6250) lr 1.7804e-03 eta 0:00:14
epoch [45/200] batch [30/57] time 0.487 (0.452) data 0.356 (0.321) loss_u loss_u 0.7998 (0.8150) acc_u 31.2500 (24.6875) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [35/57] time 0.389 (0.451) data 0.257 (0.320) loss_u loss_u 0.9321 (0.8206) acc_u 9.3750 (23.9286) lr 1.7804e-03 eta 0:00:09
epoch [45/200] batch [40/57] time 0.445 (0.451) data 0.314 (0.320) loss_u loss_u 0.7788 (0.8195) acc_u 25.0000 (23.9062) lr 1.7804e-03 eta 0:00:07
epoch [45/200] batch [45/57] time 0.368 (0.449) data 0.237 (0.318) loss_u loss_u 0.8672 (0.8204) acc_u 15.6250 (23.6111) lr 1.7804e-03 eta 0:00:05
epoch [45/200] batch [50/57] time 0.589 (0.450) data 0.457 (0.319) loss_u loss_u 0.8613 (0.8239) acc_u 15.6250 (23.2500) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [55/57] time 0.472 (0.451) data 0.342 (0.320) loss_u loss_u 0.8770 (0.8264) acc_u 15.6250 (22.7273) lr 1.7804e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1535
confident_label rate tensor(0.4078, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1279
clean true:1244
clean false:35
clean_rate:0.9726348709929633
noisy true:357
noisy false:1500
after delete: len(clean_dataset) 1279
after delete: len(noisy_dataset) 1857
epoch [46/200] batch [5/39] time 0.474 (0.437) data 0.344 (0.307) loss_x loss_x 1.1631 (1.2878) acc_x 65.6250 (66.8750) lr 1.7705e-03 eta 0:00:14
epoch [46/200] batch [10/39] time 0.394 (0.455) data 0.263 (0.325) loss_x loss_x 0.9673 (1.2092) acc_x 78.1250 (69.0625) lr 1.7705e-03 eta 0:00:13
epoch [46/200] batch [15/39] time 0.472 (0.443) data 0.342 (0.313) loss_x loss_x 1.7656 (1.2284) acc_x 50.0000 (68.7500) lr 1.7705e-03 eta 0:00:10
epoch [46/200] batch [20/39] time 0.435 (0.440) data 0.305 (0.309) loss_x loss_x 1.6455 (1.2659) acc_x 65.6250 (68.7500) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [25/39] time 0.492 (0.444) data 0.362 (0.314) loss_x loss_x 1.5361 (1.2933) acc_x 71.8750 (69.0000) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [30/39] time 0.495 (0.449) data 0.366 (0.318) loss_x loss_x 1.3252 (1.2755) acc_x 68.7500 (68.6458) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [35/39] time 0.463 (0.449) data 0.332 (0.319) loss_x loss_x 1.0908 (1.2555) acc_x 71.8750 (68.7500) lr 1.7705e-03 eta 0:00:01
epoch [46/200] batch [5/58] time 0.457 (0.458) data 0.327 (0.328) loss_u loss_u 0.7783 (0.8238) acc_u 28.1250 (23.1250) lr 1.7705e-03 eta 0:00:24
epoch [46/200] batch [10/58] time 0.540 (0.457) data 0.409 (0.326) loss_u loss_u 0.8501 (0.8099) acc_u 18.7500 (25.0000) lr 1.7705e-03 eta 0:00:21
epoch [46/200] batch [15/58] time 0.395 (0.458) data 0.265 (0.328) loss_u loss_u 0.8843 (0.8225) acc_u 12.5000 (22.7083) lr 1.7705e-03 eta 0:00:19
epoch [46/200] batch [20/58] time 0.421 (0.455) data 0.291 (0.324) loss_u loss_u 0.8003 (0.8245) acc_u 25.0000 (22.9688) lr 1.7705e-03 eta 0:00:17
epoch [46/200] batch [25/58] time 0.464 (0.452) data 0.333 (0.321) loss_u loss_u 0.8423 (0.8246) acc_u 18.7500 (22.8750) lr 1.7705e-03 eta 0:00:14
epoch [46/200] batch [30/58] time 0.526 (0.453) data 0.395 (0.322) loss_u loss_u 0.8662 (0.8311) acc_u 18.7500 (21.8750) lr 1.7705e-03 eta 0:00:12
epoch [46/200] batch [35/58] time 0.423 (0.455) data 0.292 (0.325) loss_u loss_u 0.8311 (0.8328) acc_u 21.8750 (21.6964) lr 1.7705e-03 eta 0:00:10
epoch [46/200] batch [40/58] time 0.403 (0.455) data 0.273 (0.324) loss_u loss_u 0.8066 (0.8299) acc_u 31.2500 (22.1094) lr 1.7705e-03 eta 0:00:08
epoch [46/200] batch [45/58] time 0.393 (0.454) data 0.263 (0.323) loss_u loss_u 0.8477 (0.8308) acc_u 18.7500 (21.8056) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [50/58] time 0.481 (0.451) data 0.349 (0.320) loss_u loss_u 0.8701 (0.8320) acc_u 15.6250 (21.4375) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [55/58] time 0.305 (0.448) data 0.173 (0.317) loss_u loss_u 0.8569 (0.8310) acc_u 18.7500 (21.5909) lr 1.7705e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1578
confident_label rate tensor(0.4031, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1264
clean true:1232
clean false:32
clean_rate:0.9746835443037974
noisy true:326
noisy false:1546
after delete: len(clean_dataset) 1264
after delete: len(noisy_dataset) 1872
epoch [47/200] batch [5/39] time 0.345 (0.412) data 0.214 (0.281) loss_x loss_x 1.3516 (1.3031) acc_x 56.2500 (71.2500) lr 1.7604e-03 eta 0:00:13
epoch [47/200] batch [10/39] time 0.379 (0.421) data 0.247 (0.290) loss_x loss_x 0.9238 (1.3418) acc_x 81.2500 (68.1250) lr 1.7604e-03 eta 0:00:12
epoch [47/200] batch [15/39] time 0.434 (0.445) data 0.305 (0.314) loss_x loss_x 1.7266 (1.3552) acc_x 53.1250 (67.7083) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [20/39] time 0.589 (0.452) data 0.458 (0.322) loss_x loss_x 1.0957 (1.3699) acc_x 75.0000 (67.6562) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [25/39] time 0.533 (0.452) data 0.402 (0.321) loss_x loss_x 1.5762 (1.3716) acc_x 56.2500 (67.0000) lr 1.7604e-03 eta 0:00:06
epoch [47/200] batch [30/39] time 0.330 (0.446) data 0.199 (0.315) loss_x loss_x 1.6904 (1.3491) acc_x 53.1250 (67.0833) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [35/39] time 0.410 (0.450) data 0.279 (0.319) loss_x loss_x 1.3252 (1.3506) acc_x 59.3750 (66.1607) lr 1.7604e-03 eta 0:00:01
epoch [47/200] batch [5/58] time 0.425 (0.444) data 0.295 (0.313) loss_u loss_u 0.8940 (0.8191) acc_u 12.5000 (23.1250) lr 1.7604e-03 eta 0:00:23
epoch [47/200] batch [10/58] time 0.425 (0.445) data 0.293 (0.314) loss_u loss_u 0.7881 (0.8138) acc_u 25.0000 (23.1250) lr 1.7604e-03 eta 0:00:21
epoch [47/200] batch [15/58] time 0.408 (0.444) data 0.277 (0.313) loss_u loss_u 0.8413 (0.8150) acc_u 25.0000 (23.7500) lr 1.7604e-03 eta 0:00:19
epoch [47/200] batch [20/58] time 0.396 (0.445) data 0.265 (0.314) loss_u loss_u 0.8281 (0.8117) acc_u 18.7500 (24.3750) lr 1.7604e-03 eta 0:00:16
epoch [47/200] batch [25/58] time 0.380 (0.441) data 0.248 (0.310) loss_u loss_u 0.8838 (0.8179) acc_u 12.5000 (23.1250) lr 1.7604e-03 eta 0:00:14
epoch [47/200] batch [30/58] time 0.420 (0.443) data 0.289 (0.312) loss_u loss_u 0.7632 (0.8150) acc_u 34.3750 (24.1667) lr 1.7604e-03 eta 0:00:12
epoch [47/200] batch [35/58] time 0.349 (0.442) data 0.218 (0.311) loss_u loss_u 0.8936 (0.8133) acc_u 15.6250 (24.2857) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [40/58] time 0.452 (0.445) data 0.322 (0.314) loss_u loss_u 0.8413 (0.8111) acc_u 21.8750 (24.8438) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [45/58] time 0.509 (0.446) data 0.377 (0.315) loss_u loss_u 0.9053 (0.8135) acc_u 6.2500 (24.5833) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [50/58] time 0.482 (0.454) data 0.350 (0.323) loss_u loss_u 0.8647 (0.8168) acc_u 21.8750 (24.0625) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [55/58] time 0.432 (0.453) data 0.300 (0.322) loss_u loss_u 0.8682 (0.8211) acc_u 15.6250 (23.2955) lr 1.7604e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1570
confident_label rate tensor(0.4015, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1259
clean true:1220
clean false:39
clean_rate:0.9690230341540905
noisy true:346
noisy false:1531
after delete: len(clean_dataset) 1259
after delete: len(noisy_dataset) 1877
epoch [48/200] batch [5/39] time 0.427 (0.449) data 0.297 (0.319) loss_x loss_x 1.4307 (1.2326) acc_x 62.5000 (68.1250) lr 1.7501e-03 eta 0:00:15
epoch [48/200] batch [10/39] time 0.508 (0.469) data 0.377 (0.339) loss_x loss_x 0.9482 (1.3516) acc_x 81.2500 (65.9375) lr 1.7501e-03 eta 0:00:13
epoch [48/200] batch [15/39] time 0.564 (0.470) data 0.432 (0.339) loss_x loss_x 1.2520 (1.3242) acc_x 65.6250 (66.8750) lr 1.7501e-03 eta 0:00:11
epoch [48/200] batch [20/39] time 0.506 (0.477) data 0.374 (0.346) loss_x loss_x 1.3428 (1.3054) acc_x 68.7500 (67.3438) lr 1.7501e-03 eta 0:00:09
epoch [48/200] batch [25/39] time 0.528 (0.476) data 0.398 (0.345) loss_x loss_x 1.3438 (1.3104) acc_x 59.3750 (66.7500) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [30/39] time 0.577 (0.485) data 0.446 (0.354) loss_x loss_x 1.1465 (1.3007) acc_x 71.8750 (67.0833) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [35/39] time 0.596 (0.486) data 0.465 (0.355) loss_x loss_x 1.6914 (1.3220) acc_x 43.7500 (65.8036) lr 1.7501e-03 eta 0:00:01
epoch [48/200] batch [5/58] time 0.477 (0.474) data 0.347 (0.343) loss_u loss_u 0.8521 (0.8669) acc_u 15.6250 (18.1250) lr 1.7501e-03 eta 0:00:25
epoch [48/200] batch [10/58] time 0.469 (0.468) data 0.337 (0.337) loss_u loss_u 0.8789 (0.8448) acc_u 12.5000 (20.3125) lr 1.7501e-03 eta 0:00:22
epoch [48/200] batch [15/58] time 0.469 (0.463) data 0.338 (0.332) loss_u loss_u 0.7744 (0.8357) acc_u 25.0000 (20.4167) lr 1.7501e-03 eta 0:00:19
epoch [48/200] batch [20/58] time 0.417 (0.466) data 0.286 (0.335) loss_u loss_u 0.7847 (0.8317) acc_u 25.0000 (20.9375) lr 1.7501e-03 eta 0:00:17
epoch [48/200] batch [25/58] time 0.442 (0.468) data 0.310 (0.337) loss_u loss_u 0.7754 (0.8353) acc_u 25.0000 (20.5000) lr 1.7501e-03 eta 0:00:15
epoch [48/200] batch [30/58] time 0.556 (0.470) data 0.425 (0.339) loss_u loss_u 0.7236 (0.8278) acc_u 40.6250 (21.6667) lr 1.7501e-03 eta 0:00:13
epoch [48/200] batch [35/58] time 0.395 (0.466) data 0.264 (0.335) loss_u loss_u 0.8335 (0.8263) acc_u 25.0000 (22.2321) lr 1.7501e-03 eta 0:00:10
epoch [48/200] batch [40/58] time 0.491 (0.466) data 0.360 (0.335) loss_u loss_u 0.8696 (0.8283) acc_u 15.6250 (21.8750) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [45/58] time 0.465 (0.466) data 0.335 (0.335) loss_u loss_u 0.7632 (0.8225) acc_u 21.8750 (22.2222) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [50/58] time 0.460 (0.465) data 0.329 (0.334) loss_u loss_u 0.8438 (0.8243) acc_u 15.6250 (22.0625) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [55/58] time 0.594 (0.465) data 0.462 (0.334) loss_u loss_u 0.8452 (0.8243) acc_u 15.6250 (21.9886) lr 1.7501e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1537
confident_label rate tensor(0.4069, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1276
clean true:1248
clean false:28
clean_rate:0.9780564263322884
noisy true:351
noisy false:1509
after delete: len(clean_dataset) 1276
after delete: len(noisy_dataset) 1860
epoch [49/200] batch [5/39] time 0.682 (0.487) data 0.551 (0.357) loss_x loss_x 1.4453 (1.5469) acc_x 68.7500 (63.1250) lr 1.7396e-03 eta 0:00:16
epoch [49/200] batch [10/39] time 0.470 (0.473) data 0.339 (0.342) loss_x loss_x 0.7764 (1.4164) acc_x 84.3750 (66.8750) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [15/39] time 0.404 (0.474) data 0.274 (0.343) loss_x loss_x 1.4072 (1.4007) acc_x 75.0000 (66.6667) lr 1.7396e-03 eta 0:00:11
epoch [49/200] batch [20/39] time 0.509 (0.483) data 0.378 (0.353) loss_x loss_x 1.0918 (1.3644) acc_x 71.8750 (67.6562) lr 1.7396e-03 eta 0:00:09
epoch [49/200] batch [25/39] time 0.453 (0.481) data 0.322 (0.350) loss_x loss_x 1.2979 (1.3704) acc_x 59.3750 (66.3750) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [30/39] time 0.462 (0.482) data 0.332 (0.351) loss_x loss_x 1.7314 (1.3472) acc_x 56.2500 (66.8750) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [35/39] time 0.424 (0.474) data 0.294 (0.344) loss_x loss_x 1.0146 (1.3226) acc_x 71.8750 (67.3214) lr 1.7396e-03 eta 0:00:01
epoch [49/200] batch [5/58] time 0.376 (0.473) data 0.244 (0.342) loss_u loss_u 0.7959 (0.8155) acc_u 31.2500 (26.2500) lr 1.7396e-03 eta 0:00:25
epoch [49/200] batch [10/58] time 0.428 (0.466) data 0.297 (0.335) loss_u loss_u 0.8081 (0.8097) acc_u 21.8750 (25.9375) lr 1.7396e-03 eta 0:00:22
epoch [49/200] batch [15/58] time 0.483 (0.463) data 0.352 (0.333) loss_u loss_u 0.8389 (0.8124) acc_u 15.6250 (24.1667) lr 1.7396e-03 eta 0:00:19
epoch [49/200] batch [20/58] time 0.396 (0.461) data 0.264 (0.330) loss_u loss_u 0.8291 (0.8228) acc_u 21.8750 (22.0312) lr 1.7396e-03 eta 0:00:17
epoch [49/200] batch [25/58] time 0.342 (0.456) data 0.211 (0.325) loss_u loss_u 0.8066 (0.8204) acc_u 18.7500 (22.1250) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [30/58] time 0.423 (0.459) data 0.293 (0.328) loss_u loss_u 0.8237 (0.8272) acc_u 21.8750 (21.1458) lr 1.7396e-03 eta 0:00:12
epoch [49/200] batch [35/58] time 0.373 (0.455) data 0.242 (0.324) loss_u loss_u 0.8467 (0.8287) acc_u 21.8750 (21.3393) lr 1.7396e-03 eta 0:00:10
epoch [49/200] batch [40/58] time 0.382 (0.454) data 0.252 (0.323) loss_u loss_u 0.8120 (0.8310) acc_u 21.8750 (20.8594) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [45/58] time 0.425 (0.452) data 0.294 (0.321) loss_u loss_u 0.7910 (0.8289) acc_u 21.8750 (21.1111) lr 1.7396e-03 eta 0:00:05
epoch [49/200] batch [50/58] time 0.428 (0.455) data 0.296 (0.324) loss_u loss_u 0.7847 (0.8288) acc_u 31.2500 (21.1250) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [55/58] time 0.309 (0.452) data 0.177 (0.321) loss_u loss_u 0.8330 (0.8263) acc_u 18.7500 (21.3068) lr 1.7396e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1530
confident_label rate tensor(0.4155, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1303
clean true:1271
clean false:32
clean_rate:0.97544128933231
noisy true:335
noisy false:1498
after delete: len(clean_dataset) 1303
after delete: len(noisy_dataset) 1833
epoch [50/200] batch [5/40] time 0.412 (0.491) data 0.282 (0.361) loss_x loss_x 1.5176 (1.2612) acc_x 68.7500 (69.3750) lr 1.7290e-03 eta 0:00:17
epoch [50/200] batch [10/40] time 0.399 (0.474) data 0.269 (0.344) loss_x loss_x 1.4033 (1.2305) acc_x 68.7500 (70.0000) lr 1.7290e-03 eta 0:00:14
epoch [50/200] batch [15/40] time 0.438 (0.464) data 0.308 (0.334) loss_x loss_x 0.8569 (1.2633) acc_x 75.0000 (70.0000) lr 1.7290e-03 eta 0:00:11
epoch [50/200] batch [20/40] time 0.519 (0.476) data 0.389 (0.346) loss_x loss_x 1.4971 (1.3091) acc_x 62.5000 (68.1250) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [25/40] time 0.477 (0.472) data 0.347 (0.342) loss_x loss_x 1.2324 (1.2523) acc_x 68.7500 (69.3750) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [30/40] time 0.451 (0.474) data 0.320 (0.343) loss_x loss_x 1.8105 (1.3013) acc_x 46.8750 (67.3958) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [35/40] time 0.392 (0.468) data 0.261 (0.338) loss_x loss_x 0.8535 (1.2875) acc_x 75.0000 (67.9464) lr 1.7290e-03 eta 0:00:02
epoch [50/200] batch [40/40] time 0.502 (0.469) data 0.373 (0.339) loss_x loss_x 1.1953 (1.2958) acc_x 68.7500 (67.8906) lr 1.7290e-03 eta 0:00:00
epoch [50/200] batch [5/57] time 0.357 (0.462) data 0.227 (0.332) loss_u loss_u 0.8555 (0.8281) acc_u 18.7500 (22.5000) lr 1.7290e-03 eta 0:00:24
epoch [50/200] batch [10/57] time 0.528 (0.462) data 0.397 (0.332) loss_u loss_u 0.8408 (0.8316) acc_u 18.7500 (21.5625) lr 1.7290e-03 eta 0:00:21
epoch [50/200] batch [15/57] time 0.487 (0.459) data 0.354 (0.328) loss_u loss_u 0.8403 (0.8382) acc_u 15.6250 (20.4167) lr 1.7290e-03 eta 0:00:19
epoch [50/200] batch [20/57] time 0.488 (0.459) data 0.356 (0.328) loss_u loss_u 0.7754 (0.8385) acc_u 25.0000 (20.0000) lr 1.7290e-03 eta 0:00:16
epoch [50/200] batch [25/57] time 0.423 (0.461) data 0.293 (0.330) loss_u loss_u 0.6279 (0.8353) acc_u 43.7500 (20.0000) lr 1.7290e-03 eta 0:00:14
epoch [50/200] batch [30/57] time 0.425 (0.456) data 0.295 (0.325) loss_u loss_u 0.7261 (0.8329) acc_u 31.2500 (20.2083) lr 1.7290e-03 eta 0:00:12
epoch [50/200] batch [35/57] time 0.380 (0.451) data 0.250 (0.320) loss_u loss_u 0.9028 (0.8343) acc_u 15.6250 (20.6250) lr 1.7290e-03 eta 0:00:09
epoch [50/200] batch [40/57] time 0.360 (0.448) data 0.229 (0.318) loss_u loss_u 0.8389 (0.8341) acc_u 18.7500 (20.7812) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [45/57] time 0.312 (0.451) data 0.182 (0.320) loss_u loss_u 0.7705 (0.8297) acc_u 40.6250 (21.3194) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [50/57] time 0.423 (0.450) data 0.292 (0.319) loss_u loss_u 0.8350 (0.8331) acc_u 18.7500 (20.8125) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [55/57] time 0.477 (0.451) data 0.345 (0.320) loss_u loss_u 0.7515 (0.8289) acc_u 31.2500 (21.4205) lr 1.7290e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1528
confident_label rate tensor(0.4206, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1319
clean true:1276
clean false:43
clean_rate:0.9673995451099318
noisy true:332
noisy false:1485
after delete: len(clean_dataset) 1319
after delete: len(noisy_dataset) 1817
epoch [51/200] batch [5/41] time 0.415 (0.441) data 0.285 (0.311) loss_x loss_x 1.3457 (1.2286) acc_x 65.6250 (70.0000) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [10/41] time 0.466 (0.467) data 0.335 (0.337) loss_x loss_x 1.3965 (1.2229) acc_x 65.6250 (69.3750) lr 1.7181e-03 eta 0:00:14
epoch [51/200] batch [15/41] time 0.501 (0.488) data 0.367 (0.358) loss_x loss_x 1.3965 (1.2808) acc_x 62.5000 (68.5417) lr 1.7181e-03 eta 0:00:12
epoch [51/200] batch [20/41] time 0.390 (0.487) data 0.260 (0.356) loss_x loss_x 1.1211 (1.2682) acc_x 71.8750 (68.9062) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [25/41] time 0.499 (0.479) data 0.369 (0.349) loss_x loss_x 1.4473 (1.2379) acc_x 71.8750 (70.0000) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [30/41] time 0.500 (0.475) data 0.369 (0.344) loss_x loss_x 1.9551 (1.2783) acc_x 65.6250 (68.8542) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [35/41] time 0.459 (0.473) data 0.328 (0.343) loss_x loss_x 1.6357 (1.3128) acc_x 68.7500 (67.3214) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [40/41] time 0.368 (0.473) data 0.237 (0.342) loss_x loss_x 1.4668 (1.3026) acc_x 68.7500 (67.8125) lr 1.7181e-03 eta 0:00:00
epoch [51/200] batch [5/56] time 0.389 (0.473) data 0.259 (0.342) loss_u loss_u 0.9204 (0.8433) acc_u 6.2500 (18.7500) lr 1.7181e-03 eta 0:00:24
epoch [51/200] batch [10/56] time 0.455 (0.469) data 0.324 (0.338) loss_u loss_u 0.8184 (0.8409) acc_u 25.0000 (20.3125) lr 1.7181e-03 eta 0:00:21
epoch [51/200] batch [15/56] time 0.351 (0.466) data 0.221 (0.335) loss_u loss_u 0.8223 (0.8277) acc_u 18.7500 (21.2500) lr 1.7181e-03 eta 0:00:19
epoch [51/200] batch [20/56] time 0.383 (0.462) data 0.251 (0.331) loss_u loss_u 0.8208 (0.8290) acc_u 25.0000 (21.8750) lr 1.7181e-03 eta 0:00:16
epoch [51/200] batch [25/56] time 0.511 (0.461) data 0.381 (0.331) loss_u loss_u 0.7573 (0.8248) acc_u 31.2500 (22.5000) lr 1.7181e-03 eta 0:00:14
epoch [51/200] batch [30/56] time 0.388 (0.462) data 0.257 (0.332) loss_u loss_u 0.8687 (0.8258) acc_u 21.8750 (22.3958) lr 1.7181e-03 eta 0:00:12
epoch [51/200] batch [35/56] time 0.522 (0.464) data 0.390 (0.333) loss_u loss_u 0.8325 (0.8270) acc_u 15.6250 (22.1429) lr 1.7181e-03 eta 0:00:09
epoch [51/200] batch [40/56] time 0.394 (0.467) data 0.263 (0.336) loss_u loss_u 0.8413 (0.8258) acc_u 21.8750 (22.5781) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [45/56] time 0.369 (0.461) data 0.239 (0.330) loss_u loss_u 0.8438 (0.8276) acc_u 18.7500 (22.2222) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [50/56] time 0.529 (0.462) data 0.398 (0.331) loss_u loss_u 0.6934 (0.8247) acc_u 37.5000 (22.4375) lr 1.7181e-03 eta 0:00:02
epoch [51/200] batch [55/56] time 0.352 (0.461) data 0.222 (0.330) loss_u loss_u 0.7861 (0.8250) acc_u 28.1250 (22.3864) lr 1.7181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1520
confident_label rate tensor(0.4136, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1297
clean true:1264
clean false:33
clean_rate:0.9745566692367
noisy true:352
noisy false:1487
after delete: len(clean_dataset) 1297
after delete: len(noisy_dataset) 1839
epoch [52/200] batch [5/40] time 0.474 (0.450) data 0.343 (0.320) loss_x loss_x 1.6982 (1.5513) acc_x 65.6250 (65.6250) lr 1.7071e-03 eta 0:00:15
epoch [52/200] batch [10/40] time 0.535 (0.487) data 0.404 (0.357) loss_x loss_x 1.2432 (1.3088) acc_x 62.5000 (69.6875) lr 1.7071e-03 eta 0:00:14
epoch [52/200] batch [15/40] time 0.462 (0.484) data 0.331 (0.354) loss_x loss_x 0.8169 (1.2833) acc_x 65.6250 (67.9167) lr 1.7071e-03 eta 0:00:12
epoch [52/200] batch [20/40] time 0.423 (0.482) data 0.292 (0.352) loss_x loss_x 1.2607 (1.3125) acc_x 65.6250 (67.3438) lr 1.7071e-03 eta 0:00:09
epoch [52/200] batch [25/40] time 0.435 (0.469) data 0.305 (0.339) loss_x loss_x 0.9595 (1.2922) acc_x 71.8750 (67.2500) lr 1.7071e-03 eta 0:00:07
epoch [52/200] batch [30/40] time 0.656 (0.469) data 0.525 (0.339) loss_x loss_x 1.1426 (1.2788) acc_x 68.7500 (67.1875) lr 1.7071e-03 eta 0:00:04
epoch [52/200] batch [35/40] time 0.399 (0.466) data 0.269 (0.336) loss_x loss_x 1.6836 (1.2868) acc_x 62.5000 (67.1429) lr 1.7071e-03 eta 0:00:02
epoch [52/200] batch [40/40] time 0.440 (0.467) data 0.309 (0.337) loss_x loss_x 1.0732 (1.2616) acc_x 75.0000 (67.5781) lr 1.7071e-03 eta 0:00:00
epoch [52/200] batch [5/57] time 0.398 (0.461) data 0.268 (0.331) loss_u loss_u 0.7964 (0.8186) acc_u 25.0000 (23.1250) lr 1.7071e-03 eta 0:00:23
epoch [52/200] batch [10/57] time 0.496 (0.458) data 0.365 (0.328) loss_u loss_u 0.8135 (0.8195) acc_u 28.1250 (24.3750) lr 1.7071e-03 eta 0:00:21
epoch [52/200] batch [15/57] time 0.623 (0.460) data 0.492 (0.329) loss_u loss_u 0.8301 (0.8227) acc_u 12.5000 (23.3333) lr 1.7071e-03 eta 0:00:19
epoch [52/200] batch [20/57] time 0.388 (0.457) data 0.256 (0.326) loss_u loss_u 0.7998 (0.8226) acc_u 21.8750 (22.9688) lr 1.7071e-03 eta 0:00:16
epoch [52/200] batch [25/57] time 0.361 (0.452) data 0.231 (0.321) loss_u loss_u 0.9180 (0.8282) acc_u 9.3750 (22.0000) lr 1.7071e-03 eta 0:00:14
epoch [52/200] batch [30/57] time 0.924 (0.457) data 0.793 (0.327) loss_u loss_u 0.7944 (0.8336) acc_u 31.2500 (21.5625) lr 1.7071e-03 eta 0:00:12
epoch [52/200] batch [35/57] time 0.429 (0.455) data 0.298 (0.324) loss_u loss_u 0.6963 (0.8236) acc_u 34.3750 (22.6786) lr 1.7071e-03 eta 0:00:10
epoch [52/200] batch [40/57] time 0.627 (0.457) data 0.497 (0.326) loss_u loss_u 0.8799 (0.8309) acc_u 21.8750 (21.7188) lr 1.7071e-03 eta 0:00:07
epoch [52/200] batch [45/57] time 0.438 (0.454) data 0.307 (0.323) loss_u loss_u 0.7998 (0.8288) acc_u 28.1250 (21.8750) lr 1.7071e-03 eta 0:00:05
epoch [52/200] batch [50/57] time 0.358 (0.453) data 0.228 (0.322) loss_u loss_u 0.7905 (0.8269) acc_u 25.0000 (22.0625) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [55/57] time 0.661 (0.453) data 0.530 (0.322) loss_u loss_u 0.8496 (0.8277) acc_u 21.8750 (21.9886) lr 1.7071e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1535
confident_label rate tensor(0.4101, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1286
clean true:1244
clean false:42
clean_rate:0.9673405909797823
noisy true:357
noisy false:1493
after delete: len(clean_dataset) 1286
after delete: len(noisy_dataset) 1850
epoch [53/200] batch [5/40] time 0.577 (0.496) data 0.445 (0.365) loss_x loss_x 0.7202 (1.1095) acc_x 81.2500 (71.8750) lr 1.6959e-03 eta 0:00:17
epoch [53/200] batch [10/40] time 0.506 (0.521) data 0.376 (0.391) loss_x loss_x 1.1543 (1.1959) acc_x 78.1250 (70.3125) lr 1.6959e-03 eta 0:00:15
epoch [53/200] batch [15/40] time 0.511 (0.494) data 0.381 (0.364) loss_x loss_x 1.0605 (1.2269) acc_x 68.7500 (69.3750) lr 1.6959e-03 eta 0:00:12
epoch [53/200] batch [20/40] time 0.643 (0.490) data 0.513 (0.360) loss_x loss_x 0.9243 (1.2251) acc_x 71.8750 (68.9062) lr 1.6959e-03 eta 0:00:09
epoch [53/200] batch [25/40] time 0.403 (0.477) data 0.273 (0.347) loss_x loss_x 1.2285 (1.2273) acc_x 65.6250 (68.5000) lr 1.6959e-03 eta 0:00:07
epoch [53/200] batch [30/40] time 0.478 (0.474) data 0.348 (0.344) loss_x loss_x 1.0000 (1.2200) acc_x 68.7500 (68.6458) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [35/40] time 0.333 (0.468) data 0.204 (0.338) loss_x loss_x 0.6768 (1.1964) acc_x 84.3750 (69.0179) lr 1.6959e-03 eta 0:00:02
epoch [53/200] batch [40/40] time 0.506 (0.467) data 0.376 (0.337) loss_x loss_x 1.0078 (1.2127) acc_x 68.7500 (68.4375) lr 1.6959e-03 eta 0:00:00
epoch [53/200] batch [5/57] time 0.486 (0.462) data 0.355 (0.332) loss_u loss_u 0.8179 (0.8115) acc_u 25.0000 (24.3750) lr 1.6959e-03 eta 0:00:24
epoch [53/200] batch [10/57] time 0.376 (0.455) data 0.245 (0.325) loss_u loss_u 0.8423 (0.8279) acc_u 25.0000 (22.1875) lr 1.6959e-03 eta 0:00:21
epoch [53/200] batch [15/57] time 0.341 (0.450) data 0.209 (0.320) loss_u loss_u 0.8530 (0.8330) acc_u 18.7500 (21.8750) lr 1.6959e-03 eta 0:00:18
epoch [53/200] batch [20/57] time 0.393 (0.456) data 0.262 (0.325) loss_u loss_u 0.8032 (0.8362) acc_u 21.8750 (21.0938) lr 1.6959e-03 eta 0:00:16
epoch [53/200] batch [25/57] time 0.433 (0.456) data 0.301 (0.325) loss_u loss_u 0.7773 (0.8269) acc_u 18.7500 (22.2500) lr 1.6959e-03 eta 0:00:14
epoch [53/200] batch [30/57] time 0.358 (0.454) data 0.228 (0.324) loss_u loss_u 0.8931 (0.8294) acc_u 12.5000 (21.5625) lr 1.6959e-03 eta 0:00:12
epoch [53/200] batch [35/57] time 0.367 (0.452) data 0.236 (0.321) loss_u loss_u 0.8335 (0.8299) acc_u 15.6250 (21.6071) lr 1.6959e-03 eta 0:00:09
epoch [53/200] batch [40/57] time 0.469 (0.449) data 0.337 (0.318) loss_u loss_u 0.8579 (0.8345) acc_u 18.7500 (21.0156) lr 1.6959e-03 eta 0:00:07
epoch [53/200] batch [45/57] time 0.590 (0.449) data 0.460 (0.319) loss_u loss_u 0.7998 (0.8345) acc_u 28.1250 (20.9028) lr 1.6959e-03 eta 0:00:05
epoch [53/200] batch [50/57] time 0.609 (0.452) data 0.478 (0.321) loss_u loss_u 0.8301 (0.8355) acc_u 21.8750 (20.6250) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [55/57] time 0.542 (0.452) data 0.412 (0.321) loss_u loss_u 0.8989 (0.8384) acc_u 12.5000 (20.3977) lr 1.6959e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1543
confident_label rate tensor(0.4117, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1291
clean true:1249
clean false:42
clean_rate:0.9674670797831139
noisy true:344
noisy false:1501
after delete: len(clean_dataset) 1291
after delete: len(noisy_dataset) 1845
epoch [54/200] batch [5/40] time 0.510 (0.477) data 0.380 (0.346) loss_x loss_x 1.3975 (1.1905) acc_x 75.0000 (70.0000) lr 1.6845e-03 eta 0:00:16
epoch [54/200] batch [10/40] time 0.352 (0.472) data 0.222 (0.342) loss_x loss_x 0.8545 (1.3026) acc_x 78.1250 (68.1250) lr 1.6845e-03 eta 0:00:14
epoch [54/200] batch [15/40] time 0.393 (0.485) data 0.262 (0.355) loss_x loss_x 1.0674 (1.2612) acc_x 68.7500 (68.7500) lr 1.6845e-03 eta 0:00:12
epoch [54/200] batch [20/40] time 0.323 (0.472) data 0.192 (0.342) loss_x loss_x 1.0664 (1.2634) acc_x 68.7500 (67.9688) lr 1.6845e-03 eta 0:00:09
epoch [54/200] batch [25/40] time 0.331 (0.457) data 0.201 (0.327) loss_x loss_x 1.1582 (1.3062) acc_x 68.7500 (67.8750) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [30/40] time 0.398 (0.458) data 0.267 (0.328) loss_x loss_x 1.4873 (1.3269) acc_x 56.2500 (67.2917) lr 1.6845e-03 eta 0:00:04
epoch [54/200] batch [35/40] time 0.456 (0.469) data 0.325 (0.339) loss_x loss_x 1.1152 (1.3062) acc_x 68.7500 (67.8571) lr 1.6845e-03 eta 0:00:02
epoch [54/200] batch [40/40] time 0.602 (0.466) data 0.471 (0.335) loss_x loss_x 1.3408 (1.3145) acc_x 62.5000 (67.5781) lr 1.6845e-03 eta 0:00:00
epoch [54/200] batch [5/57] time 0.386 (0.462) data 0.255 (0.332) loss_u loss_u 0.8066 (0.8196) acc_u 21.8750 (21.2500) lr 1.6845e-03 eta 0:00:24
epoch [54/200] batch [10/57] time 0.383 (0.459) data 0.251 (0.329) loss_u loss_u 0.8379 (0.8224) acc_u 15.6250 (20.9375) lr 1.6845e-03 eta 0:00:21
epoch [54/200] batch [15/57] time 0.473 (0.456) data 0.342 (0.325) loss_u loss_u 0.8618 (0.8231) acc_u 15.6250 (22.2917) lr 1.6845e-03 eta 0:00:19
epoch [54/200] batch [20/57] time 0.423 (0.454) data 0.292 (0.323) loss_u loss_u 0.8472 (0.8278) acc_u 15.6250 (21.7188) lr 1.6845e-03 eta 0:00:16
epoch [54/200] batch [25/57] time 0.409 (0.449) data 0.278 (0.319) loss_u loss_u 0.8208 (0.8293) acc_u 28.1250 (21.3750) lr 1.6845e-03 eta 0:00:14
epoch [54/200] batch [30/57] time 0.414 (0.446) data 0.283 (0.315) loss_u loss_u 0.8789 (0.8291) acc_u 12.5000 (21.5625) lr 1.6845e-03 eta 0:00:12
epoch [54/200] batch [35/57] time 0.429 (0.444) data 0.297 (0.313) loss_u loss_u 0.7925 (0.8277) acc_u 21.8750 (21.7857) lr 1.6845e-03 eta 0:00:09
epoch [54/200] batch [40/57] time 0.364 (0.443) data 0.233 (0.312) loss_u loss_u 0.8008 (0.8249) acc_u 28.1250 (22.5000) lr 1.6845e-03 eta 0:00:07
epoch [54/200] batch [45/57] time 0.422 (0.441) data 0.291 (0.311) loss_u loss_u 0.7065 (0.8219) acc_u 34.3750 (22.9861) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [50/57] time 0.640 (0.444) data 0.509 (0.313) loss_u loss_u 0.8643 (0.8232) acc_u 18.7500 (22.8750) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [55/57] time 0.458 (0.443) data 0.326 (0.312) loss_u loss_u 0.7798 (0.8226) acc_u 34.3750 (23.1818) lr 1.6845e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1535
confident_label rate tensor(0.4139, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1298
clean true:1261
clean false:37
clean_rate:0.9714946070878274
noisy true:340
noisy false:1498
after delete: len(clean_dataset) 1298
after delete: len(noisy_dataset) 1838
epoch [55/200] batch [5/40] time 0.357 (0.435) data 0.227 (0.304) loss_x loss_x 1.4990 (1.5607) acc_x 62.5000 (61.8750) lr 1.6730e-03 eta 0:00:15
epoch [55/200] batch [10/40] time 0.417 (0.445) data 0.287 (0.315) loss_x loss_x 1.3867 (1.4522) acc_x 75.0000 (65.9375) lr 1.6730e-03 eta 0:00:13
epoch [55/200] batch [15/40] time 0.458 (0.443) data 0.328 (0.313) loss_x loss_x 1.2959 (1.3774) acc_x 62.5000 (65.6250) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [20/40] time 0.415 (0.446) data 0.285 (0.316) loss_x loss_x 1.7344 (1.3731) acc_x 53.1250 (65.7812) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [25/40] time 0.637 (0.444) data 0.507 (0.314) loss_x loss_x 0.8657 (1.3293) acc_x 78.1250 (66.7500) lr 1.6730e-03 eta 0:00:06
epoch [55/200] batch [30/40] time 0.416 (0.444) data 0.285 (0.314) loss_x loss_x 1.1611 (1.3277) acc_x 59.3750 (66.1458) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [35/40] time 0.402 (0.440) data 0.272 (0.310) loss_x loss_x 1.3184 (1.3369) acc_x 75.0000 (66.6964) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [40/40] time 0.641 (0.445) data 0.512 (0.315) loss_x loss_x 1.3760 (1.3254) acc_x 75.0000 (66.8750) lr 1.6730e-03 eta 0:00:00
epoch [55/200] batch [5/57] time 0.408 (0.444) data 0.277 (0.314) loss_u loss_u 0.8257 (0.8604) acc_u 21.8750 (16.2500) lr 1.6730e-03 eta 0:00:23
epoch [55/200] batch [10/57] time 0.414 (0.442) data 0.283 (0.312) loss_u loss_u 0.7632 (0.8438) acc_u 28.1250 (18.1250) lr 1.6730e-03 eta 0:00:20
epoch [55/200] batch [15/57] time 0.477 (0.440) data 0.345 (0.310) loss_u loss_u 0.8003 (0.8300) acc_u 21.8750 (19.3750) lr 1.6730e-03 eta 0:00:18
epoch [55/200] batch [20/57] time 0.365 (0.437) data 0.234 (0.307) loss_u loss_u 0.8218 (0.8293) acc_u 28.1250 (20.1562) lr 1.6730e-03 eta 0:00:16
epoch [55/200] batch [25/57] time 0.465 (0.438) data 0.330 (0.308) loss_u loss_u 0.8193 (0.8223) acc_u 28.1250 (22.0000) lr 1.6730e-03 eta 0:00:14
epoch [55/200] batch [30/57] time 0.519 (0.442) data 0.388 (0.311) loss_u loss_u 0.7822 (0.8246) acc_u 21.8750 (21.4583) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [35/57] time 0.384 (0.440) data 0.253 (0.310) loss_u loss_u 0.8374 (0.8293) acc_u 21.8750 (20.8929) lr 1.6730e-03 eta 0:00:09
epoch [55/200] batch [40/57] time 0.506 (0.438) data 0.375 (0.307) loss_u loss_u 0.9229 (0.8294) acc_u 9.3750 (21.0156) lr 1.6730e-03 eta 0:00:07
epoch [55/200] batch [45/57] time 0.545 (0.439) data 0.413 (0.308) loss_u loss_u 0.8213 (0.8290) acc_u 21.8750 (21.1111) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [50/57] time 0.334 (0.436) data 0.203 (0.305) loss_u loss_u 0.7993 (0.8252) acc_u 28.1250 (21.8750) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [55/57] time 0.347 (0.440) data 0.216 (0.309) loss_u loss_u 0.7974 (0.8261) acc_u 31.2500 (22.2727) lr 1.6730e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1516
confident_label rate tensor(0.4174, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1309
clean true:1280
clean false:29
clean_rate:0.9778456837280367
noisy true:340
noisy false:1487
after delete: len(clean_dataset) 1309
after delete: len(noisy_dataset) 1827
epoch [56/200] batch [5/40] time 0.438 (0.445) data 0.307 (0.314) loss_x loss_x 1.5947 (1.3807) acc_x 59.3750 (63.7500) lr 1.6613e-03 eta 0:00:15
epoch [56/200] batch [10/40] time 0.498 (0.431) data 0.368 (0.300) loss_x loss_x 1.4395 (1.3396) acc_x 71.8750 (67.1875) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [15/40] time 0.487 (0.449) data 0.357 (0.319) loss_x loss_x 1.5137 (1.2882) acc_x 56.2500 (67.0833) lr 1.6613e-03 eta 0:00:11
epoch [56/200] batch [20/40] time 0.454 (0.461) data 0.323 (0.330) loss_x loss_x 1.1592 (1.2486) acc_x 68.7500 (67.0312) lr 1.6613e-03 eta 0:00:09
epoch [56/200] batch [25/40] time 0.482 (0.464) data 0.351 (0.333) loss_x loss_x 1.0469 (1.1948) acc_x 68.7500 (67.8750) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [30/40] time 0.399 (0.461) data 0.268 (0.330) loss_x loss_x 1.6621 (1.2557) acc_x 53.1250 (66.8750) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [35/40] time 0.457 (0.459) data 0.326 (0.328) loss_x loss_x 1.2793 (1.2688) acc_x 62.5000 (67.1429) lr 1.6613e-03 eta 0:00:02
epoch [56/200] batch [40/40] time 0.374 (0.465) data 0.243 (0.335) loss_x loss_x 1.4570 (1.2807) acc_x 62.5000 (67.5000) lr 1.6613e-03 eta 0:00:00
epoch [56/200] batch [5/57] time 0.445 (0.468) data 0.313 (0.337) loss_u loss_u 0.8560 (0.8413) acc_u 18.7500 (20.6250) lr 1.6613e-03 eta 0:00:24
epoch [56/200] batch [10/57] time 0.543 (0.468) data 0.411 (0.337) loss_u loss_u 0.8438 (0.8341) acc_u 25.0000 (21.5625) lr 1.6613e-03 eta 0:00:21
epoch [56/200] batch [15/57] time 0.409 (0.466) data 0.278 (0.335) loss_u loss_u 0.8184 (0.8354) acc_u 18.7500 (20.8333) lr 1.6613e-03 eta 0:00:19
epoch [56/200] batch [20/57] time 0.409 (0.458) data 0.276 (0.327) loss_u loss_u 0.8315 (0.8309) acc_u 21.8750 (22.3438) lr 1.6613e-03 eta 0:00:16
epoch [56/200] batch [25/57] time 0.375 (0.454) data 0.245 (0.323) loss_u loss_u 0.8013 (0.8319) acc_u 21.8750 (21.7500) lr 1.6613e-03 eta 0:00:14
epoch [56/200] batch [30/57] time 0.392 (0.453) data 0.260 (0.322) loss_u loss_u 0.7842 (0.8345) acc_u 28.1250 (21.2500) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [35/57] time 0.374 (0.453) data 0.243 (0.322) loss_u loss_u 0.7959 (0.8348) acc_u 25.0000 (21.2500) lr 1.6613e-03 eta 0:00:09
epoch [56/200] batch [40/57] time 0.435 (0.454) data 0.304 (0.323) loss_u loss_u 0.7324 (0.8350) acc_u 37.5000 (21.3281) lr 1.6613e-03 eta 0:00:07
epoch [56/200] batch [45/57] time 0.396 (0.454) data 0.264 (0.323) loss_u loss_u 0.8237 (0.8368) acc_u 25.0000 (20.9722) lr 1.6613e-03 eta 0:00:05
epoch [56/200] batch [50/57] time 0.676 (0.457) data 0.544 (0.326) loss_u loss_u 0.8149 (0.8346) acc_u 25.0000 (21.1250) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [55/57] time 0.445 (0.457) data 0.314 (0.325) loss_u loss_u 0.7109 (0.8338) acc_u 40.6250 (21.1364) lr 1.6613e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1508
confident_label rate tensor(0.4158, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1304
clean true:1268
clean false:36
clean_rate:0.9723926380368099
noisy true:360
noisy false:1472
after delete: len(clean_dataset) 1304
after delete: len(noisy_dataset) 1832
epoch [57/200] batch [5/40] time 0.554 (0.458) data 0.423 (0.327) loss_x loss_x 1.0752 (1.2563) acc_x 68.7500 (67.5000) lr 1.6494e-03 eta 0:00:16
epoch [57/200] batch [10/40] time 0.455 (0.455) data 0.325 (0.324) loss_x loss_x 0.7671 (1.1945) acc_x 75.0000 (67.8125) lr 1.6494e-03 eta 0:00:13
epoch [57/200] batch [15/40] time 0.449 (0.469) data 0.318 (0.337) loss_x loss_x 1.2842 (1.2798) acc_x 62.5000 (66.8750) lr 1.6494e-03 eta 0:00:11
epoch [57/200] batch [20/40] time 0.634 (0.493) data 0.503 (0.362) loss_x loss_x 0.7534 (1.2366) acc_x 71.8750 (68.2812) lr 1.6494e-03 eta 0:00:09
epoch [57/200] batch [25/40] time 0.599 (0.487) data 0.469 (0.356) loss_x loss_x 1.2324 (1.2278) acc_x 71.8750 (69.3750) lr 1.6494e-03 eta 0:00:07
epoch [57/200] batch [30/40] time 0.413 (0.483) data 0.282 (0.352) loss_x loss_x 1.1934 (1.2377) acc_x 75.0000 (69.7917) lr 1.6494e-03 eta 0:00:04
epoch [57/200] batch [35/40] time 0.540 (0.484) data 0.410 (0.353) loss_x loss_x 1.2051 (1.2626) acc_x 65.6250 (68.7500) lr 1.6494e-03 eta 0:00:02
epoch [57/200] batch [40/40] time 0.447 (0.480) data 0.316 (0.350) loss_x loss_x 1.9941 (1.3002) acc_x 59.3750 (67.8125) lr 1.6494e-03 eta 0:00:00
epoch [57/200] batch [5/57] time 0.399 (0.471) data 0.267 (0.340) loss_u loss_u 0.7612 (0.8029) acc_u 34.3750 (28.1250) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [10/57] time 0.359 (0.465) data 0.228 (0.334) loss_u loss_u 0.8818 (0.8137) acc_u 15.6250 (26.2500) lr 1.6494e-03 eta 0:00:21
epoch [57/200] batch [15/57] time 0.456 (0.468) data 0.325 (0.337) loss_u loss_u 0.9004 (0.8243) acc_u 15.6250 (24.5833) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [20/57] time 0.386 (0.468) data 0.254 (0.337) loss_u loss_u 0.8608 (0.8220) acc_u 12.5000 (24.0625) lr 1.6494e-03 eta 0:00:17
epoch [57/200] batch [25/57] time 0.414 (0.463) data 0.283 (0.331) loss_u loss_u 0.8467 (0.8265) acc_u 15.6250 (23.6250) lr 1.6494e-03 eta 0:00:14
epoch [57/200] batch [30/57] time 0.476 (0.461) data 0.344 (0.329) loss_u loss_u 0.8296 (0.8293) acc_u 18.7500 (23.0208) lr 1.6494e-03 eta 0:00:12
epoch [57/200] batch [35/57] time 0.502 (0.460) data 0.370 (0.328) loss_u loss_u 0.8179 (0.8289) acc_u 18.7500 (23.0357) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [40/57] time 0.366 (0.455) data 0.233 (0.324) loss_u loss_u 0.9355 (0.8305) acc_u 6.2500 (22.5781) lr 1.6494e-03 eta 0:00:07
epoch [57/200] batch [45/57] time 0.378 (0.452) data 0.247 (0.321) loss_u loss_u 0.7163 (0.8292) acc_u 34.3750 (22.7778) lr 1.6494e-03 eta 0:00:05
epoch [57/200] batch [50/57] time 0.502 (0.456) data 0.372 (0.325) loss_u loss_u 0.8555 (0.8315) acc_u 25.0000 (22.5000) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [55/57] time 0.492 (0.456) data 0.361 (0.325) loss_u loss_u 0.8843 (0.8286) acc_u 15.6250 (22.7841) lr 1.6494e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1531
confident_label rate tensor(0.4110, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1289
clean true:1251
clean false:38
clean_rate:0.9705197827773467
noisy true:354
noisy false:1493
after delete: len(clean_dataset) 1289
after delete: len(noisy_dataset) 1847
epoch [58/200] batch [5/40] time 0.453 (0.515) data 0.322 (0.385) loss_x loss_x 1.4512 (1.3180) acc_x 59.3750 (63.7500) lr 1.6374e-03 eta 0:00:18
epoch [58/200] batch [10/40] time 0.464 (0.507) data 0.334 (0.377) loss_x loss_x 1.0508 (1.3276) acc_x 71.8750 (65.9375) lr 1.6374e-03 eta 0:00:15
epoch [58/200] batch [15/40] time 0.414 (0.485) data 0.284 (0.354) loss_x loss_x 0.8862 (1.3419) acc_x 68.7500 (65.6250) lr 1.6374e-03 eta 0:00:12
epoch [58/200] batch [20/40] time 0.648 (0.484) data 0.518 (0.354) loss_x loss_x 1.3730 (1.3289) acc_x 78.1250 (67.6562) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [25/40] time 0.389 (0.475) data 0.258 (0.344) loss_x loss_x 0.7695 (1.3170) acc_x 78.1250 (67.3750) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [30/40] time 0.452 (0.471) data 0.321 (0.341) loss_x loss_x 1.2910 (1.3031) acc_x 68.7500 (68.1250) lr 1.6374e-03 eta 0:00:04
epoch [58/200] batch [35/40] time 0.435 (0.469) data 0.304 (0.338) loss_x loss_x 1.5469 (1.3448) acc_x 56.2500 (67.2321) lr 1.6374e-03 eta 0:00:02
epoch [58/200] batch [40/40] time 0.430 (0.467) data 0.299 (0.336) loss_x loss_x 1.3740 (1.3483) acc_x 65.6250 (66.6406) lr 1.6374e-03 eta 0:00:00
epoch [58/200] batch [5/57] time 0.465 (0.461) data 0.334 (0.330) loss_u loss_u 0.7510 (0.8104) acc_u 34.3750 (22.5000) lr 1.6374e-03 eta 0:00:23
epoch [58/200] batch [10/57] time 0.341 (0.460) data 0.210 (0.329) loss_u loss_u 0.8252 (0.8119) acc_u 25.0000 (22.5000) lr 1.6374e-03 eta 0:00:21
epoch [58/200] batch [15/57] time 0.380 (0.455) data 0.250 (0.324) loss_u loss_u 0.8423 (0.8241) acc_u 21.8750 (21.4583) lr 1.6374e-03 eta 0:00:19
epoch [58/200] batch [20/57] time 0.483 (0.461) data 0.352 (0.331) loss_u loss_u 0.8125 (0.8248) acc_u 25.0000 (21.4062) lr 1.6374e-03 eta 0:00:17
epoch [58/200] batch [25/57] time 0.357 (0.459) data 0.225 (0.328) loss_u loss_u 0.7681 (0.8243) acc_u 37.5000 (22.3750) lr 1.6374e-03 eta 0:00:14
epoch [58/200] batch [30/57] time 0.430 (0.455) data 0.299 (0.324) loss_u loss_u 0.7588 (0.8211) acc_u 31.2500 (22.6042) lr 1.6374e-03 eta 0:00:12
epoch [58/200] batch [35/57] time 0.414 (0.454) data 0.282 (0.323) loss_u loss_u 0.8066 (0.8182) acc_u 21.8750 (22.8571) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [40/57] time 0.366 (0.452) data 0.235 (0.321) loss_u loss_u 0.7876 (0.8154) acc_u 25.0000 (23.1250) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [45/57] time 0.531 (0.451) data 0.399 (0.320) loss_u loss_u 0.7979 (0.8166) acc_u 31.2500 (23.1944) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [50/57] time 0.457 (0.448) data 0.324 (0.317) loss_u loss_u 0.8467 (0.8203) acc_u 18.7500 (22.8750) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [55/57] time 0.333 (0.448) data 0.201 (0.317) loss_u loss_u 0.8086 (0.8197) acc_u 25.0000 (22.7841) lr 1.6374e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1497
confident_label rate tensor(0.4222, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1324
clean true:1277
clean false:47
clean_rate:0.9645015105740181
noisy true:362
noisy false:1450
after delete: len(clean_dataset) 1324
after delete: len(noisy_dataset) 1812
epoch [59/200] batch [5/41] time 0.517 (0.491) data 0.386 (0.360) loss_x loss_x 1.0928 (1.2830) acc_x 81.2500 (66.2500) lr 1.6252e-03 eta 0:00:17
epoch [59/200] batch [10/41] time 0.532 (0.481) data 0.399 (0.349) loss_x loss_x 0.8770 (1.2426) acc_x 75.0000 (64.6875) lr 1.6252e-03 eta 0:00:14
epoch [59/200] batch [15/41] time 0.420 (0.473) data 0.289 (0.342) loss_x loss_x 1.0713 (1.1973) acc_x 71.8750 (66.4583) lr 1.6252e-03 eta 0:00:12
epoch [59/200] batch [20/41] time 0.379 (0.476) data 0.249 (0.345) loss_x loss_x 1.7852 (1.2420) acc_x 59.3750 (65.3125) lr 1.6252e-03 eta 0:00:09
epoch [59/200] batch [25/41] time 0.462 (0.476) data 0.332 (0.345) loss_x loss_x 1.2100 (1.2695) acc_x 68.7500 (65.3750) lr 1.6252e-03 eta 0:00:07
epoch [59/200] batch [30/41] time 0.428 (0.470) data 0.298 (0.339) loss_x loss_x 1.7363 (1.2783) acc_x 53.1250 (65.5208) lr 1.6252e-03 eta 0:00:05
epoch [59/200] batch [35/41] time 0.442 (0.472) data 0.312 (0.341) loss_x loss_x 1.2861 (1.2772) acc_x 75.0000 (66.4286) lr 1.6252e-03 eta 0:00:02
epoch [59/200] batch [40/41] time 0.534 (0.471) data 0.403 (0.340) loss_x loss_x 1.4375 (1.2810) acc_x 62.5000 (66.5625) lr 1.6252e-03 eta 0:00:00
epoch [59/200] batch [5/56] time 0.318 (0.462) data 0.187 (0.331) loss_u loss_u 0.8701 (0.8187) acc_u 15.6250 (24.3750) lr 1.6252e-03 eta 0:00:23
epoch [59/200] batch [10/56] time 0.423 (0.457) data 0.292 (0.326) loss_u loss_u 0.8140 (0.8257) acc_u 18.7500 (20.9375) lr 1.6252e-03 eta 0:00:21
epoch [59/200] batch [15/56] time 0.340 (0.450) data 0.208 (0.319) loss_u loss_u 0.7495 (0.8162) acc_u 34.3750 (22.2917) lr 1.6252e-03 eta 0:00:18
epoch [59/200] batch [20/56] time 0.408 (0.448) data 0.278 (0.318) loss_u loss_u 0.8374 (0.8167) acc_u 18.7500 (22.5000) lr 1.6252e-03 eta 0:00:16
epoch [59/200] batch [25/56] time 0.474 (0.450) data 0.343 (0.319) loss_u loss_u 0.7822 (0.8099) acc_u 34.3750 (23.7500) lr 1.6252e-03 eta 0:00:13
epoch [59/200] batch [30/56] time 0.506 (0.449) data 0.376 (0.318) loss_u loss_u 0.8540 (0.8116) acc_u 15.6250 (23.0208) lr 1.6252e-03 eta 0:00:11
epoch [59/200] batch [35/56] time 0.487 (0.455) data 0.356 (0.324) loss_u loss_u 0.8105 (0.8110) acc_u 25.0000 (23.3036) lr 1.6252e-03 eta 0:00:09
epoch [59/200] batch [40/56] time 0.406 (0.455) data 0.275 (0.324) loss_u loss_u 0.8911 (0.8182) acc_u 15.6250 (22.4219) lr 1.6252e-03 eta 0:00:07
epoch [59/200] batch [45/56] time 0.594 (0.454) data 0.463 (0.323) loss_u loss_u 0.8750 (0.8187) acc_u 15.6250 (22.5000) lr 1.6252e-03 eta 0:00:04
epoch [59/200] batch [50/56] time 0.426 (0.455) data 0.295 (0.324) loss_u loss_u 0.8228 (0.8228) acc_u 18.7500 (21.8125) lr 1.6252e-03 eta 0:00:02
epoch [59/200] batch [55/56] time 0.390 (0.452) data 0.260 (0.321) loss_u loss_u 0.8018 (0.8243) acc_u 31.2500 (21.8750) lr 1.6252e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1510
confident_label rate tensor(0.4193, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1315
clean true:1276
clean false:39
clean_rate:0.9703422053231939
noisy true:350
noisy false:1471
after delete: len(clean_dataset) 1315
after delete: len(noisy_dataset) 1821
epoch [60/200] batch [5/41] time 0.403 (0.445) data 0.272 (0.314) loss_x loss_x 0.8394 (1.2954) acc_x 81.2500 (70.0000) lr 1.6129e-03 eta 0:00:16
epoch [60/200] batch [10/41] time 0.429 (0.431) data 0.299 (0.301) loss_x loss_x 1.3096 (1.3280) acc_x 62.5000 (65.9375) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [15/41] time 0.399 (0.448) data 0.269 (0.317) loss_x loss_x 1.6699 (1.3397) acc_x 62.5000 (66.8750) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [20/41] time 0.450 (0.450) data 0.320 (0.320) loss_x loss_x 0.8501 (1.3485) acc_x 78.1250 (67.3438) lr 1.6129e-03 eta 0:00:09
epoch [60/200] batch [25/41] time 0.522 (0.464) data 0.392 (0.333) loss_x loss_x 1.2354 (1.3764) acc_x 62.5000 (65.2500) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [30/41] time 0.486 (0.478) data 0.356 (0.348) loss_x loss_x 0.9497 (1.3719) acc_x 75.0000 (65.8333) lr 1.6129e-03 eta 0:00:05
epoch [60/200] batch [35/41] time 0.669 (0.482) data 0.538 (0.351) loss_x loss_x 1.2861 (1.3654) acc_x 68.7500 (66.3393) lr 1.6129e-03 eta 0:00:02
epoch [60/200] batch [40/41] time 0.369 (0.481) data 0.239 (0.350) loss_x loss_x 1.3105 (1.3435) acc_x 65.6250 (66.7969) lr 1.6129e-03 eta 0:00:00
epoch [60/200] batch [5/56] time 0.530 (0.477) data 0.399 (0.346) loss_u loss_u 0.7900 (0.8134) acc_u 28.1250 (25.0000) lr 1.6129e-03 eta 0:00:24
epoch [60/200] batch [10/56] time 0.408 (0.470) data 0.278 (0.339) loss_u loss_u 0.7065 (0.8072) acc_u 28.1250 (25.0000) lr 1.6129e-03 eta 0:00:21
epoch [60/200] batch [15/56] time 0.387 (0.463) data 0.256 (0.333) loss_u loss_u 0.7510 (0.8019) acc_u 31.2500 (25.2083) lr 1.6129e-03 eta 0:00:19
epoch [60/200] batch [20/56] time 0.349 (0.461) data 0.219 (0.330) loss_u loss_u 0.7646 (0.8060) acc_u 28.1250 (24.2188) lr 1.6129e-03 eta 0:00:16
epoch [60/200] batch [25/56] time 0.480 (0.462) data 0.350 (0.331) loss_u loss_u 0.8760 (0.8100) acc_u 15.6250 (23.3750) lr 1.6129e-03 eta 0:00:14
epoch [60/200] batch [30/56] time 0.348 (0.460) data 0.218 (0.329) loss_u loss_u 0.8276 (0.8151) acc_u 31.2500 (22.7083) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [35/56] time 0.419 (0.457) data 0.287 (0.327) loss_u loss_u 0.8511 (0.8187) acc_u 12.5000 (21.7857) lr 1.6129e-03 eta 0:00:09
epoch [60/200] batch [40/56] time 0.358 (0.457) data 0.227 (0.327) loss_u loss_u 0.8193 (0.8194) acc_u 25.0000 (22.1875) lr 1.6129e-03 eta 0:00:07
epoch [60/200] batch [45/56] time 0.414 (0.456) data 0.284 (0.326) loss_u loss_u 0.9292 (0.8226) acc_u 9.3750 (21.8750) lr 1.6129e-03 eta 0:00:05
epoch [60/200] batch [50/56] time 0.521 (0.454) data 0.390 (0.323) loss_u loss_u 0.8926 (0.8239) acc_u 9.3750 (21.6875) lr 1.6129e-03 eta 0:00:02
epoch [60/200] batch [55/56] time 0.431 (0.451) data 0.300 (0.320) loss_u loss_u 0.8369 (0.8258) acc_u 25.0000 (21.4773) lr 1.6129e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1524
confident_label rate tensor(0.4200, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1317
clean true:1280
clean false:37
clean_rate:0.9719058466211086
noisy true:332
noisy false:1487
after delete: len(clean_dataset) 1317
after delete: len(noisy_dataset) 1819
epoch [61/200] batch [5/41] time 0.640 (0.544) data 0.510 (0.414) loss_x loss_x 0.8945 (1.2148) acc_x 78.1250 (70.6250) lr 1.6004e-03 eta 0:00:19
epoch [61/200] batch [10/41] time 0.424 (0.492) data 0.294 (0.361) loss_x loss_x 1.6641 (1.3237) acc_x 53.1250 (67.1875) lr 1.6004e-03 eta 0:00:15
epoch [61/200] batch [15/41] time 0.353 (0.463) data 0.223 (0.332) loss_x loss_x 1.0713 (1.3005) acc_x 71.8750 (68.7500) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [20/41] time 0.506 (0.467) data 0.375 (0.336) loss_x loss_x 0.9536 (1.2184) acc_x 75.0000 (70.3125) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [25/41] time 0.409 (0.462) data 0.278 (0.332) loss_x loss_x 2.0273 (1.2634) acc_x 53.1250 (69.1250) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [30/41] time 0.568 (0.462) data 0.437 (0.331) loss_x loss_x 1.0840 (1.2371) acc_x 75.0000 (69.8958) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [35/41] time 0.471 (0.466) data 0.340 (0.336) loss_x loss_x 0.9893 (1.2137) acc_x 71.8750 (70.2679) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [40/41] time 0.417 (0.460) data 0.286 (0.329) loss_x loss_x 2.0527 (1.2316) acc_x 53.1250 (69.1406) lr 1.6004e-03 eta 0:00:00
epoch [61/200] batch [5/56] time 0.465 (0.459) data 0.333 (0.328) loss_u loss_u 0.6973 (0.7842) acc_u 37.5000 (26.2500) lr 1.6004e-03 eta 0:00:23
epoch [61/200] batch [10/56] time 0.450 (0.461) data 0.319 (0.331) loss_u loss_u 0.8481 (0.8111) acc_u 18.7500 (23.1250) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [15/56] time 0.354 (0.458) data 0.223 (0.327) loss_u loss_u 0.8530 (0.8291) acc_u 15.6250 (21.2500) lr 1.6004e-03 eta 0:00:18
epoch [61/200] batch [20/56] time 0.605 (0.461) data 0.474 (0.330) loss_u loss_u 0.8394 (0.8228) acc_u 21.8750 (22.6562) lr 1.6004e-03 eta 0:00:16
epoch [61/200] batch [25/56] time 0.374 (0.459) data 0.243 (0.328) loss_u loss_u 0.8350 (0.8230) acc_u 15.6250 (21.7500) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [30/56] time 0.672 (0.463) data 0.542 (0.332) loss_u loss_u 0.8037 (0.8181) acc_u 25.0000 (22.5000) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [35/56] time 0.417 (0.460) data 0.287 (0.329) loss_u loss_u 0.7856 (0.8182) acc_u 25.0000 (22.5893) lr 1.6004e-03 eta 0:00:09
epoch [61/200] batch [40/56] time 0.448 (0.458) data 0.317 (0.327) loss_u loss_u 0.8442 (0.8221) acc_u 15.6250 (21.9531) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [45/56] time 0.399 (0.457) data 0.267 (0.326) loss_u loss_u 0.7930 (0.8242) acc_u 28.1250 (22.0139) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [50/56] time 0.362 (0.453) data 0.229 (0.322) loss_u loss_u 0.7969 (0.8239) acc_u 25.0000 (22.2500) lr 1.6004e-03 eta 0:00:02
epoch [61/200] batch [55/56] time 0.458 (0.452) data 0.326 (0.321) loss_u loss_u 0.8545 (0.8270) acc_u 21.8750 (21.9318) lr 1.6004e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1534
confident_label rate tensor(0.4136, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1297
clean true:1260
clean false:37
clean_rate:0.9714726291441789
noisy true:342
noisy false:1497
after delete: len(clean_dataset) 1297
after delete: len(noisy_dataset) 1839
epoch [62/200] batch [5/40] time 0.393 (0.450) data 0.262 (0.319) loss_x loss_x 1.7148 (1.1854) acc_x 62.5000 (70.0000) lr 1.5878e-03 eta 0:00:15
epoch [62/200] batch [10/40] time 0.441 (0.452) data 0.311 (0.321) loss_x loss_x 1.3115 (1.1316) acc_x 65.6250 (70.9375) lr 1.5878e-03 eta 0:00:13
epoch [62/200] batch [15/40] time 0.572 (0.470) data 0.443 (0.339) loss_x loss_x 1.6650 (1.1768) acc_x 50.0000 (69.3750) lr 1.5878e-03 eta 0:00:11
epoch [62/200] batch [20/40] time 0.416 (0.469) data 0.287 (0.338) loss_x loss_x 1.5938 (1.2031) acc_x 62.5000 (68.9062) lr 1.5878e-03 eta 0:00:09
epoch [62/200] batch [25/40] time 0.443 (0.469) data 0.312 (0.338) loss_x loss_x 1.8311 (1.2331) acc_x 62.5000 (68.5000) lr 1.5878e-03 eta 0:00:07
epoch [62/200] batch [30/40] time 0.401 (0.462) data 0.271 (0.331) loss_x loss_x 0.7480 (1.2170) acc_x 75.0000 (69.3750) lr 1.5878e-03 eta 0:00:04
epoch [62/200] batch [35/40] time 0.367 (0.460) data 0.236 (0.329) loss_x loss_x 1.9795 (1.2340) acc_x 56.2500 (69.3750) lr 1.5878e-03 eta 0:00:02
epoch [62/200] batch [40/40] time 0.398 (0.457) data 0.269 (0.326) loss_x loss_x 1.4072 (1.2161) acc_x 68.7500 (69.6875) lr 1.5878e-03 eta 0:00:00
epoch [62/200] batch [5/57] time 0.444 (0.451) data 0.314 (0.320) loss_u loss_u 0.7749 (0.7998) acc_u 25.0000 (23.1250) lr 1.5878e-03 eta 0:00:23
epoch [62/200] batch [10/57] time 0.437 (0.454) data 0.305 (0.323) loss_u loss_u 0.8936 (0.8190) acc_u 12.5000 (20.9375) lr 1.5878e-03 eta 0:00:21
epoch [62/200] batch [15/57] time 0.382 (0.454) data 0.251 (0.323) loss_u loss_u 0.8350 (0.8171) acc_u 25.0000 (22.2917) lr 1.5878e-03 eta 0:00:19
epoch [62/200] batch [20/57] time 0.626 (0.459) data 0.495 (0.328) loss_u loss_u 0.8687 (0.8259) acc_u 18.7500 (21.0938) lr 1.5878e-03 eta 0:00:16
epoch [62/200] batch [25/57] time 0.344 (0.452) data 0.214 (0.321) loss_u loss_u 0.8398 (0.8295) acc_u 18.7500 (20.6250) lr 1.5878e-03 eta 0:00:14
epoch [62/200] batch [30/57] time 0.476 (0.452) data 0.345 (0.321) loss_u loss_u 0.8193 (0.8308) acc_u 25.0000 (20.6250) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [35/57] time 0.472 (0.452) data 0.341 (0.322) loss_u loss_u 0.7993 (0.8257) acc_u 21.8750 (21.3393) lr 1.5878e-03 eta 0:00:09
epoch [62/200] batch [40/57] time 0.456 (0.450) data 0.325 (0.320) loss_u loss_u 0.7915 (0.8230) acc_u 28.1250 (21.9531) lr 1.5878e-03 eta 0:00:07
epoch [62/200] batch [45/57] time 0.456 (0.455) data 0.326 (0.324) loss_u loss_u 0.8418 (0.8249) acc_u 18.7500 (21.7361) lr 1.5878e-03 eta 0:00:05
epoch [62/200] batch [50/57] time 0.376 (0.454) data 0.245 (0.323) loss_u loss_u 0.8208 (0.8223) acc_u 28.1250 (22.3750) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [55/57] time 0.360 (0.453) data 0.229 (0.322) loss_u loss_u 0.7998 (0.8207) acc_u 31.2500 (22.8409) lr 1.5878e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1522
confident_label rate tensor(0.4161, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1305
clean true:1268
clean false:37
clean_rate:0.9716475095785441
noisy true:346
noisy false:1485
after delete: len(clean_dataset) 1305
after delete: len(noisy_dataset) 1831
epoch [63/200] batch [5/40] time 0.437 (0.435) data 0.307 (0.304) loss_x loss_x 1.3369 (1.1557) acc_x 71.8750 (74.3750) lr 1.5750e-03 eta 0:00:15
epoch [63/200] batch [10/40] time 0.366 (0.433) data 0.236 (0.303) loss_x loss_x 1.5469 (1.3003) acc_x 53.1250 (68.4375) lr 1.5750e-03 eta 0:00:13
epoch [63/200] batch [15/40] time 0.504 (0.456) data 0.373 (0.326) loss_x loss_x 1.2568 (1.3390) acc_x 68.7500 (67.9167) lr 1.5750e-03 eta 0:00:11
epoch [63/200] batch [20/40] time 0.416 (0.452) data 0.286 (0.322) loss_x loss_x 1.3838 (1.2945) acc_x 56.2500 (67.9688) lr 1.5750e-03 eta 0:00:09
epoch [63/200] batch [25/40] time 0.531 (0.455) data 0.401 (0.324) loss_x loss_x 1.1875 (1.2719) acc_x 68.7500 (68.2500) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [30/40] time 0.418 (0.449) data 0.288 (0.319) loss_x loss_x 1.2363 (1.2653) acc_x 71.8750 (68.3333) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [35/40] time 0.455 (0.447) data 0.323 (0.316) loss_x loss_x 1.4668 (1.2946) acc_x 59.3750 (67.0536) lr 1.5750e-03 eta 0:00:02
epoch [63/200] batch [40/40] time 0.476 (0.453) data 0.345 (0.323) loss_x loss_x 1.2031 (1.2816) acc_x 71.8750 (67.5781) lr 1.5750e-03 eta 0:00:00
epoch [63/200] batch [5/57] time 0.465 (0.454) data 0.333 (0.324) loss_u loss_u 0.7158 (0.7969) acc_u 34.3750 (23.7500) lr 1.5750e-03 eta 0:00:23
epoch [63/200] batch [10/57] time 0.445 (0.452) data 0.313 (0.322) loss_u loss_u 0.8477 (0.8173) acc_u 15.6250 (21.8750) lr 1.5750e-03 eta 0:00:21
epoch [63/200] batch [15/57] time 0.432 (0.457) data 0.301 (0.326) loss_u loss_u 0.8350 (0.8157) acc_u 25.0000 (22.7083) lr 1.5750e-03 eta 0:00:19
epoch [63/200] batch [20/57] time 0.707 (0.463) data 0.576 (0.332) loss_u loss_u 0.8623 (0.8233) acc_u 18.7500 (22.0312) lr 1.5750e-03 eta 0:00:17
epoch [63/200] batch [25/57] time 0.503 (0.460) data 0.373 (0.329) loss_u loss_u 0.7534 (0.8195) acc_u 28.1250 (22.5000) lr 1.5750e-03 eta 0:00:14
epoch [63/200] batch [30/57] time 0.413 (0.459) data 0.282 (0.328) loss_u loss_u 0.8428 (0.8169) acc_u 15.6250 (22.6042) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [35/57] time 0.549 (0.460) data 0.418 (0.330) loss_u loss_u 0.8247 (0.8156) acc_u 25.0000 (22.8571) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [40/57] time 0.365 (0.458) data 0.234 (0.327) loss_u loss_u 0.8638 (0.8179) acc_u 12.5000 (22.1094) lr 1.5750e-03 eta 0:00:07
epoch [63/200] batch [45/57] time 0.422 (0.459) data 0.291 (0.328) loss_u loss_u 0.7808 (0.8156) acc_u 21.8750 (22.2917) lr 1.5750e-03 eta 0:00:05
epoch [63/200] batch [50/57] time 0.373 (0.455) data 0.242 (0.324) loss_u loss_u 0.8696 (0.8174) acc_u 9.3750 (22.0625) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [55/57] time 0.535 (0.454) data 0.404 (0.323) loss_u loss_u 0.8848 (0.8203) acc_u 6.2500 (21.6477) lr 1.5750e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1520
confident_label rate tensor(0.4091, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1283
clean true:1258
clean false:25
clean_rate:0.9805144193296961
noisy true:358
noisy false:1495
after delete: len(clean_dataset) 1283
after delete: len(noisy_dataset) 1853
epoch [64/200] batch [5/40] time 0.332 (0.419) data 0.201 (0.289) loss_x loss_x 0.9604 (1.1240) acc_x 68.7500 (67.5000) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [10/40] time 0.525 (0.449) data 0.395 (0.319) loss_x loss_x 0.9780 (1.0920) acc_x 84.3750 (71.5625) lr 1.5621e-03 eta 0:00:13
epoch [64/200] batch [15/40] time 0.561 (0.467) data 0.432 (0.337) loss_x loss_x 1.4014 (1.1963) acc_x 59.3750 (68.7500) lr 1.5621e-03 eta 0:00:11
epoch [64/200] batch [20/40] time 0.368 (0.474) data 0.238 (0.344) loss_x loss_x 1.0020 (1.1484) acc_x 81.2500 (70.6250) lr 1.5621e-03 eta 0:00:09
epoch [64/200] batch [25/40] time 0.414 (0.466) data 0.283 (0.335) loss_x loss_x 1.1953 (1.2067) acc_x 62.5000 (68.7500) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [30/40] time 0.487 (0.465) data 0.357 (0.335) loss_x loss_x 1.3740 (1.2088) acc_x 65.6250 (68.6458) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [35/40] time 0.366 (0.456) data 0.236 (0.326) loss_x loss_x 1.5117 (1.2344) acc_x 78.1250 (68.7500) lr 1.5621e-03 eta 0:00:02
epoch [64/200] batch [40/40] time 0.555 (0.459) data 0.424 (0.329) loss_x loss_x 1.1992 (1.2303) acc_x 65.6250 (68.6719) lr 1.5621e-03 eta 0:00:00
epoch [64/200] batch [5/57] time 0.334 (0.454) data 0.204 (0.324) loss_u loss_u 0.7427 (0.7906) acc_u 40.6250 (27.5000) lr 1.5621e-03 eta 0:00:23
epoch [64/200] batch [10/57] time 0.383 (0.451) data 0.253 (0.320) loss_u loss_u 0.7422 (0.7887) acc_u 37.5000 (26.2500) lr 1.5621e-03 eta 0:00:21
epoch [64/200] batch [15/57] time 0.526 (0.451) data 0.395 (0.321) loss_u loss_u 0.7715 (0.7941) acc_u 25.0000 (25.2083) lr 1.5621e-03 eta 0:00:18
epoch [64/200] batch [20/57] time 0.637 (0.450) data 0.504 (0.319) loss_u loss_u 0.7642 (0.7952) acc_u 31.2500 (25.6250) lr 1.5621e-03 eta 0:00:16
epoch [64/200] batch [25/57] time 0.599 (0.456) data 0.468 (0.326) loss_u loss_u 0.7993 (0.7973) acc_u 28.1250 (25.6250) lr 1.5621e-03 eta 0:00:14
epoch [64/200] batch [30/57] time 0.415 (0.455) data 0.284 (0.325) loss_u loss_u 0.9087 (0.8030) acc_u 12.5000 (25.1042) lr 1.5621e-03 eta 0:00:12
epoch [64/200] batch [35/57] time 0.584 (0.454) data 0.453 (0.324) loss_u loss_u 0.8208 (0.8080) acc_u 18.7500 (24.1964) lr 1.5621e-03 eta 0:00:09
epoch [64/200] batch [40/57] time 0.459 (0.453) data 0.327 (0.322) loss_u loss_u 0.7832 (0.8091) acc_u 28.1250 (24.1406) lr 1.5621e-03 eta 0:00:07
epoch [64/200] batch [45/57] time 0.764 (0.454) data 0.634 (0.324) loss_u loss_u 0.8169 (0.8125) acc_u 25.0000 (23.7500) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [50/57] time 0.449 (0.454) data 0.319 (0.323) loss_u loss_u 0.7808 (0.8150) acc_u 40.6250 (23.6875) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [55/57] time 0.383 (0.452) data 0.252 (0.321) loss_u loss_u 0.8193 (0.8132) acc_u 15.6250 (24.0341) lr 1.5621e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1512
confident_label rate tensor(0.4136, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1297
clean true:1255
clean false:42
clean_rate:0.9676175790285274
noisy true:369
noisy false:1470
after delete: len(clean_dataset) 1297
after delete: len(noisy_dataset) 1839
epoch [65/200] batch [5/40] time 0.546 (0.493) data 0.414 (0.362) loss_x loss_x 1.9717 (1.3836) acc_x 53.1250 (70.0000) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [10/40] time 0.609 (0.504) data 0.478 (0.373) loss_x loss_x 1.0342 (1.4099) acc_x 78.1250 (70.0000) lr 1.5490e-03 eta 0:00:15
epoch [65/200] batch [15/40] time 0.505 (0.495) data 0.375 (0.364) loss_x loss_x 1.2129 (1.3523) acc_x 68.7500 (70.0000) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [20/40] time 0.494 (0.490) data 0.363 (0.359) loss_x loss_x 1.6318 (1.3512) acc_x 56.2500 (69.0625) lr 1.5490e-03 eta 0:00:09
epoch [65/200] batch [25/40] time 0.414 (0.486) data 0.283 (0.355) loss_x loss_x 1.3818 (1.3596) acc_x 68.7500 (68.2500) lr 1.5490e-03 eta 0:00:07
epoch [65/200] batch [30/40] time 0.629 (0.486) data 0.498 (0.355) loss_x loss_x 1.8926 (1.3393) acc_x 53.1250 (68.0208) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [35/40] time 0.459 (0.483) data 0.328 (0.352) loss_x loss_x 1.0303 (1.3238) acc_x 78.1250 (68.4821) lr 1.5490e-03 eta 0:00:02
epoch [65/200] batch [40/40] time 0.601 (0.482) data 0.470 (0.351) loss_x loss_x 1.0352 (1.3012) acc_x 68.7500 (68.6719) lr 1.5490e-03 eta 0:00:00
epoch [65/200] batch [5/57] time 0.747 (0.485) data 0.615 (0.353) loss_u loss_u 0.8096 (0.8492) acc_u 21.8750 (15.6250) lr 1.5490e-03 eta 0:00:25
epoch [65/200] batch [10/57] time 0.471 (0.480) data 0.339 (0.348) loss_u loss_u 0.7744 (0.8190) acc_u 21.8750 (20.9375) lr 1.5490e-03 eta 0:00:22
epoch [65/200] batch [15/57] time 0.389 (0.479) data 0.256 (0.348) loss_u loss_u 0.8438 (0.8189) acc_u 18.7500 (21.4583) lr 1.5490e-03 eta 0:00:20
epoch [65/200] batch [20/57] time 0.429 (0.477) data 0.298 (0.346) loss_u loss_u 0.8691 (0.8155) acc_u 12.5000 (23.2812) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [25/57] time 0.537 (0.479) data 0.405 (0.348) loss_u loss_u 0.7954 (0.8197) acc_u 28.1250 (23.0000) lr 1.5490e-03 eta 0:00:15
epoch [65/200] batch [30/57] time 0.417 (0.480) data 0.285 (0.348) loss_u loss_u 0.8042 (0.8200) acc_u 21.8750 (22.7083) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [35/57] time 0.438 (0.478) data 0.306 (0.347) loss_u loss_u 0.7808 (0.8195) acc_u 28.1250 (22.7679) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [40/57] time 0.538 (0.478) data 0.406 (0.346) loss_u loss_u 0.8120 (0.8202) acc_u 21.8750 (23.0469) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [45/57] time 0.471 (0.472) data 0.339 (0.341) loss_u loss_u 0.7471 (0.8210) acc_u 31.2500 (22.8472) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [50/57] time 0.350 (0.469) data 0.218 (0.337) loss_u loss_u 0.8701 (0.8200) acc_u 21.8750 (22.8750) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [55/57] time 0.358 (0.465) data 0.226 (0.334) loss_u loss_u 0.8472 (0.8199) acc_u 12.5000 (22.8977) lr 1.5490e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1545
confident_label rate tensor(0.4139, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1298
clean true:1258
clean false:40
clean_rate:0.9691833590138675
noisy true:333
noisy false:1505
after delete: len(clean_dataset) 1298
after delete: len(noisy_dataset) 1838
epoch [66/200] batch [5/40] time 0.354 (0.474) data 0.224 (0.344) loss_x loss_x 1.5674 (1.2996) acc_x 62.5000 (67.5000) lr 1.5358e-03 eta 0:00:16
epoch [66/200] batch [10/40] time 0.379 (0.463) data 0.247 (0.333) loss_x loss_x 0.7456 (1.2362) acc_x 84.3750 (71.5625) lr 1.5358e-03 eta 0:00:13
epoch [66/200] batch [15/40] time 0.437 (0.451) data 0.307 (0.320) loss_x loss_x 0.6367 (1.2013) acc_x 84.3750 (70.6250) lr 1.5358e-03 eta 0:00:11
epoch [66/200] batch [20/40] time 0.451 (0.451) data 0.321 (0.320) loss_x loss_x 0.6689 (1.1894) acc_x 78.1250 (71.2500) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [25/40] time 0.387 (0.444) data 0.257 (0.313) loss_x loss_x 1.3525 (1.1696) acc_x 68.7500 (71.3750) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [30/40] time 0.465 (0.450) data 0.335 (0.320) loss_x loss_x 1.0986 (1.2316) acc_x 68.7500 (69.4792) lr 1.5358e-03 eta 0:00:04
epoch [66/200] batch [35/40] time 0.476 (0.453) data 0.347 (0.323) loss_x loss_x 1.3086 (1.2515) acc_x 65.6250 (68.6607) lr 1.5358e-03 eta 0:00:02
epoch [66/200] batch [40/40] time 0.640 (0.466) data 0.510 (0.335) loss_x loss_x 0.9683 (1.2601) acc_x 65.6250 (68.2031) lr 1.5358e-03 eta 0:00:00
epoch [66/200] batch [5/57] time 0.347 (0.462) data 0.216 (0.331) loss_u loss_u 0.8281 (0.8456) acc_u 25.0000 (21.2500) lr 1.5358e-03 eta 0:00:24
epoch [66/200] batch [10/57] time 0.390 (0.460) data 0.259 (0.330) loss_u loss_u 0.8110 (0.8427) acc_u 21.8750 (20.3125) lr 1.5358e-03 eta 0:00:21
epoch [66/200] batch [15/57] time 0.383 (0.456) data 0.252 (0.326) loss_u loss_u 0.9072 (0.8342) acc_u 12.5000 (22.0833) lr 1.5358e-03 eta 0:00:19
epoch [66/200] batch [20/57] time 0.379 (0.456) data 0.249 (0.325) loss_u loss_u 0.8799 (0.8370) acc_u 15.6250 (21.4062) lr 1.5358e-03 eta 0:00:16
epoch [66/200] batch [25/57] time 0.404 (0.451) data 0.272 (0.320) loss_u loss_u 0.7192 (0.8350) acc_u 40.6250 (21.5000) lr 1.5358e-03 eta 0:00:14
epoch [66/200] batch [30/57] time 0.448 (0.448) data 0.317 (0.317) loss_u loss_u 0.7637 (0.8315) acc_u 25.0000 (21.6667) lr 1.5358e-03 eta 0:00:12
epoch [66/200] batch [35/57] time 0.533 (0.449) data 0.402 (0.318) loss_u loss_u 0.8301 (0.8268) acc_u 18.7500 (21.6071) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [40/57] time 0.420 (0.446) data 0.290 (0.315) loss_u loss_u 0.8120 (0.8278) acc_u 21.8750 (21.6406) lr 1.5358e-03 eta 0:00:07
epoch [66/200] batch [45/57] time 0.506 (0.446) data 0.375 (0.316) loss_u loss_u 0.8589 (0.8242) acc_u 15.6250 (22.4306) lr 1.5358e-03 eta 0:00:05
epoch [66/200] batch [50/57] time 0.419 (0.448) data 0.286 (0.317) loss_u loss_u 0.8125 (0.8220) acc_u 28.1250 (22.5625) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [55/57] time 0.485 (0.450) data 0.354 (0.319) loss_u loss_u 0.8452 (0.8211) acc_u 15.6250 (22.4432) lr 1.5358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1487
confident_label rate tensor(0.4314, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1353
clean true:1311
clean false:42
clean_rate:0.9689578713968958
noisy true:338
noisy false:1445
after delete: len(clean_dataset) 1353
after delete: len(noisy_dataset) 1783
epoch [67/200] batch [5/42] time 0.423 (0.492) data 0.292 (0.362) loss_x loss_x 0.9722 (1.1916) acc_x 78.1250 (71.8750) lr 1.5225e-03 eta 0:00:18
epoch [67/200] batch [10/42] time 0.424 (0.461) data 0.293 (0.330) loss_x loss_x 1.3916 (1.2062) acc_x 68.7500 (68.7500) lr 1.5225e-03 eta 0:00:14
epoch [67/200] batch [15/42] time 0.395 (0.459) data 0.264 (0.328) loss_x loss_x 1.3115 (1.2292) acc_x 68.7500 (69.1667) lr 1.5225e-03 eta 0:00:12
epoch [67/200] batch [20/42] time 0.389 (0.442) data 0.259 (0.311) loss_x loss_x 1.4463 (1.2473) acc_x 56.2500 (68.7500) lr 1.5225e-03 eta 0:00:09
epoch [67/200] batch [25/42] time 0.438 (0.439) data 0.308 (0.309) loss_x loss_x 1.2129 (1.2328) acc_x 71.8750 (68.6250) lr 1.5225e-03 eta 0:00:07
epoch [67/200] batch [30/42] time 0.473 (0.446) data 0.342 (0.315) loss_x loss_x 2.1504 (1.2559) acc_x 53.1250 (68.4375) lr 1.5225e-03 eta 0:00:05
epoch [67/200] batch [35/42] time 0.400 (0.456) data 0.269 (0.325) loss_x loss_x 1.5674 (1.2466) acc_x 65.6250 (68.3036) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [40/42] time 0.493 (0.460) data 0.363 (0.329) loss_x loss_x 1.1152 (1.2413) acc_x 75.0000 (68.5938) lr 1.5225e-03 eta 0:00:00
epoch [67/200] batch [5/55] time 0.508 (0.459) data 0.377 (0.328) loss_u loss_u 0.8594 (0.8616) acc_u 18.7500 (19.3750) lr 1.5225e-03 eta 0:00:22
epoch [67/200] batch [10/55] time 0.520 (0.461) data 0.389 (0.330) loss_u loss_u 0.7827 (0.8328) acc_u 25.0000 (22.8125) lr 1.5225e-03 eta 0:00:20
epoch [67/200] batch [15/55] time 0.450 (0.466) data 0.319 (0.335) loss_u loss_u 0.7734 (0.8314) acc_u 34.3750 (22.7083) lr 1.5225e-03 eta 0:00:18
epoch [67/200] batch [20/55] time 0.404 (0.463) data 0.273 (0.332) loss_u loss_u 0.8174 (0.8306) acc_u 21.8750 (22.3438) lr 1.5225e-03 eta 0:00:16
epoch [67/200] batch [25/55] time 0.454 (0.464) data 0.323 (0.333) loss_u loss_u 0.7222 (0.8317) acc_u 28.1250 (21.6250) lr 1.5225e-03 eta 0:00:13
epoch [67/200] batch [30/55] time 0.425 (0.461) data 0.294 (0.330) loss_u loss_u 0.8037 (0.8309) acc_u 21.8750 (21.6667) lr 1.5225e-03 eta 0:00:11
epoch [67/200] batch [35/55] time 0.411 (0.462) data 0.279 (0.331) loss_u loss_u 0.9170 (0.8307) acc_u 12.5000 (21.9643) lr 1.5225e-03 eta 0:00:09
epoch [67/200] batch [40/55] time 0.415 (0.466) data 0.284 (0.335) loss_u loss_u 0.8213 (0.8284) acc_u 28.1250 (22.3438) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [45/55] time 0.463 (0.466) data 0.332 (0.335) loss_u loss_u 0.8188 (0.8285) acc_u 31.2500 (22.5694) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [50/55] time 0.527 (0.466) data 0.395 (0.335) loss_u loss_u 0.8833 (0.8260) acc_u 12.5000 (22.9375) lr 1.5225e-03 eta 0:00:02
epoch [67/200] batch [55/55] time 0.469 (0.464) data 0.338 (0.333) loss_u loss_u 0.8711 (0.8242) acc_u 18.7500 (23.1250) lr 1.5225e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1507
confident_label rate tensor(0.4184, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1312
clean true:1273
clean false:39
clean_rate:0.9702743902439024
noisy true:356
noisy false:1468
after delete: len(clean_dataset) 1312
after delete: len(noisy_dataset) 1824
epoch [68/200] batch [5/41] time 0.345 (0.450) data 0.214 (0.319) loss_x loss_x 1.5449 (1.2713) acc_x 62.5000 (66.8750) lr 1.5090e-03 eta 0:00:16
epoch [68/200] batch [10/41] time 0.463 (0.474) data 0.332 (0.343) loss_x loss_x 1.0068 (1.1628) acc_x 68.7500 (69.0625) lr 1.5090e-03 eta 0:00:14
epoch [68/200] batch [15/41] time 0.371 (0.449) data 0.241 (0.319) loss_x loss_x 1.0664 (1.1795) acc_x 78.1250 (71.0417) lr 1.5090e-03 eta 0:00:11
epoch [68/200] batch [20/41] time 0.408 (0.444) data 0.277 (0.313) loss_x loss_x 1.1699 (1.2320) acc_x 62.5000 (69.6875) lr 1.5090e-03 eta 0:00:09
epoch [68/200] batch [25/41] time 0.352 (0.447) data 0.222 (0.317) loss_x loss_x 1.2021 (1.2104) acc_x 68.7500 (69.6250) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [30/41] time 0.577 (0.459) data 0.446 (0.328) loss_x loss_x 1.1406 (1.2459) acc_x 65.6250 (67.9167) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [35/41] time 0.374 (0.456) data 0.244 (0.326) loss_x loss_x 1.2051 (1.2353) acc_x 65.6250 (68.3036) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [40/41] time 0.390 (0.450) data 0.260 (0.320) loss_x loss_x 1.2783 (1.2510) acc_x 68.7500 (67.8906) lr 1.5090e-03 eta 0:00:00
epoch [68/200] batch [5/57] time 0.375 (0.455) data 0.245 (0.325) loss_u loss_u 0.7788 (0.8410) acc_u 28.1250 (19.3750) lr 1.5090e-03 eta 0:00:23
epoch [68/200] batch [10/57] time 0.359 (0.450) data 0.229 (0.319) loss_u loss_u 0.7812 (0.8364) acc_u 25.0000 (18.7500) lr 1.5090e-03 eta 0:00:21
epoch [68/200] batch [15/57] time 0.505 (0.452) data 0.374 (0.321) loss_u loss_u 0.8125 (0.8271) acc_u 31.2500 (21.6667) lr 1.5090e-03 eta 0:00:18
epoch [68/200] batch [20/57] time 0.471 (0.457) data 0.339 (0.326) loss_u loss_u 0.8193 (0.8291) acc_u 28.1250 (22.3438) lr 1.5090e-03 eta 0:00:16
epoch [68/200] batch [25/57] time 0.649 (0.462) data 0.517 (0.331) loss_u loss_u 0.8809 (0.8379) acc_u 12.5000 (21.1250) lr 1.5090e-03 eta 0:00:14
epoch [68/200] batch [30/57] time 0.447 (0.464) data 0.316 (0.333) loss_u loss_u 0.7876 (0.8339) acc_u 31.2500 (21.9792) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [35/57] time 0.429 (0.466) data 0.298 (0.335) loss_u loss_u 0.8130 (0.8260) acc_u 28.1250 (23.0357) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [40/57] time 0.381 (0.466) data 0.249 (0.335) loss_u loss_u 0.8618 (0.8232) acc_u 18.7500 (23.6719) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [45/57] time 0.375 (0.463) data 0.243 (0.332) loss_u loss_u 0.7100 (0.8237) acc_u 37.5000 (23.4722) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [50/57] time 0.521 (0.461) data 0.389 (0.330) loss_u loss_u 0.8398 (0.8240) acc_u 18.7500 (23.2500) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [55/57] time 0.496 (0.460) data 0.365 (0.329) loss_u loss_u 0.8076 (0.8232) acc_u 25.0000 (23.1818) lr 1.5090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1493
confident_label rate tensor(0.4267, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1338
clean true:1300
clean false:38
clean_rate:0.9715994020926756
noisy true:343
noisy false:1455
after delete: len(clean_dataset) 1338
after delete: len(noisy_dataset) 1798
epoch [69/200] batch [5/41] time 0.408 (0.483) data 0.277 (0.352) loss_x loss_x 1.0410 (1.1861) acc_x 71.8750 (70.6250) lr 1.4955e-03 eta 0:00:17
epoch [69/200] batch [10/41] time 0.404 (0.459) data 0.274 (0.328) loss_x loss_x 1.3887 (1.2018) acc_x 62.5000 (68.4375) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [15/41] time 0.510 (0.469) data 0.379 (0.338) loss_x loss_x 1.4873 (1.1952) acc_x 62.5000 (69.3750) lr 1.4955e-03 eta 0:00:12
epoch [69/200] batch [20/41] time 0.519 (0.464) data 0.389 (0.334) loss_x loss_x 1.1338 (1.1655) acc_x 75.0000 (70.7812) lr 1.4955e-03 eta 0:00:09
epoch [69/200] batch [25/41] time 0.356 (0.462) data 0.225 (0.331) loss_x loss_x 1.0273 (1.1837) acc_x 71.8750 (69.8750) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [30/41] time 0.553 (0.466) data 0.423 (0.336) loss_x loss_x 1.2041 (1.2050) acc_x 65.6250 (69.5833) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [35/41] time 0.409 (0.463) data 0.279 (0.333) loss_x loss_x 1.3389 (1.2088) acc_x 59.3750 (69.4643) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [40/41] time 0.505 (0.464) data 0.375 (0.334) loss_x loss_x 1.4111 (1.2532) acc_x 68.7500 (68.9062) lr 1.4955e-03 eta 0:00:00
epoch [69/200] batch [5/56] time 0.370 (0.458) data 0.240 (0.327) loss_u loss_u 0.9277 (0.8130) acc_u 6.2500 (22.5000) lr 1.4955e-03 eta 0:00:23
epoch [69/200] batch [10/56] time 0.395 (0.463) data 0.263 (0.332) loss_u loss_u 0.8223 (0.8313) acc_u 15.6250 (20.3125) lr 1.4955e-03 eta 0:00:21
epoch [69/200] batch [15/56] time 0.439 (0.461) data 0.307 (0.330) loss_u loss_u 0.7466 (0.8204) acc_u 31.2500 (21.6667) lr 1.4955e-03 eta 0:00:18
epoch [69/200] batch [20/56] time 0.418 (0.461) data 0.286 (0.330) loss_u loss_u 0.8037 (0.8151) acc_u 25.0000 (22.5000) lr 1.4955e-03 eta 0:00:16
epoch [69/200] batch [25/56] time 0.406 (0.455) data 0.274 (0.324) loss_u loss_u 0.8066 (0.8085) acc_u 25.0000 (23.5000) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [30/56] time 0.422 (0.452) data 0.292 (0.322) loss_u loss_u 0.8271 (0.8145) acc_u 18.7500 (22.6042) lr 1.4955e-03 eta 0:00:11
epoch [69/200] batch [35/56] time 0.356 (0.453) data 0.224 (0.322) loss_u loss_u 0.7676 (0.8125) acc_u 25.0000 (22.9464) lr 1.4955e-03 eta 0:00:09
epoch [69/200] batch [40/56] time 0.541 (0.456) data 0.409 (0.325) loss_u loss_u 0.8931 (0.8183) acc_u 9.3750 (21.9531) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [45/56] time 0.462 (0.453) data 0.330 (0.322) loss_u loss_u 0.7324 (0.8191) acc_u 31.2500 (21.6667) lr 1.4955e-03 eta 0:00:04
epoch [69/200] batch [50/56] time 0.461 (0.452) data 0.329 (0.321) loss_u loss_u 0.7832 (0.8217) acc_u 28.1250 (21.5625) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [55/56] time 0.408 (0.451) data 0.278 (0.320) loss_u loss_u 0.8511 (0.8248) acc_u 18.7500 (21.1364) lr 1.4955e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1492
confident_label rate tensor(0.4286, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1344
clean true:1306
clean false:38
clean_rate:0.9717261904761905
noisy true:338
noisy false:1454
after delete: len(clean_dataset) 1344
after delete: len(noisy_dataset) 1792
epoch [70/200] batch [5/42] time 0.531 (0.448) data 0.401 (0.317) loss_x loss_x 0.7729 (1.3077) acc_x 81.2500 (66.8750) lr 1.4818e-03 eta 0:00:16
epoch [70/200] batch [10/42] time 0.425 (0.472) data 0.294 (0.341) loss_x loss_x 1.2139 (1.2476) acc_x 71.8750 (67.8125) lr 1.4818e-03 eta 0:00:15
epoch [70/200] batch [15/42] time 0.522 (0.469) data 0.392 (0.338) loss_x loss_x 1.1240 (1.2438) acc_x 65.6250 (69.1667) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [20/42] time 0.396 (0.474) data 0.265 (0.343) loss_x loss_x 1.4893 (1.2749) acc_x 59.3750 (68.1250) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [25/42] time 0.366 (0.470) data 0.236 (0.340) loss_x loss_x 1.2266 (1.2628) acc_x 65.6250 (68.0000) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [30/42] time 0.516 (0.475) data 0.385 (0.344) loss_x loss_x 1.2666 (1.2969) acc_x 56.2500 (66.7708) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [35/42] time 0.505 (0.474) data 0.374 (0.343) loss_x loss_x 1.3145 (1.2617) acc_x 65.6250 (67.0536) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [40/42] time 0.381 (0.469) data 0.251 (0.338) loss_x loss_x 1.1904 (1.2841) acc_x 75.0000 (66.4062) lr 1.4818e-03 eta 0:00:00
epoch [70/200] batch [5/56] time 0.441 (0.468) data 0.310 (0.338) loss_u loss_u 0.7993 (0.8282) acc_u 28.1250 (22.5000) lr 1.4818e-03 eta 0:00:23
epoch [70/200] batch [10/56] time 0.361 (0.466) data 0.231 (0.336) loss_u loss_u 0.8179 (0.8367) acc_u 28.1250 (21.2500) lr 1.4818e-03 eta 0:00:21
epoch [70/200] batch [15/56] time 0.406 (0.460) data 0.275 (0.329) loss_u loss_u 0.7798 (0.8274) acc_u 25.0000 (21.6667) lr 1.4818e-03 eta 0:00:18
epoch [70/200] batch [20/56] time 0.413 (0.464) data 0.281 (0.333) loss_u loss_u 0.8877 (0.8305) acc_u 15.6250 (21.0938) lr 1.4818e-03 eta 0:00:16
epoch [70/200] batch [25/56] time 0.427 (0.465) data 0.296 (0.335) loss_u loss_u 0.8428 (0.8296) acc_u 18.7500 (21.3750) lr 1.4818e-03 eta 0:00:14
epoch [70/200] batch [30/56] time 0.450 (0.464) data 0.320 (0.334) loss_u loss_u 0.8740 (0.8279) acc_u 12.5000 (21.7708) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [35/56] time 0.482 (0.462) data 0.350 (0.332) loss_u loss_u 0.9048 (0.8299) acc_u 12.5000 (21.0714) lr 1.4818e-03 eta 0:00:09
epoch [70/200] batch [40/56] time 0.369 (0.461) data 0.238 (0.330) loss_u loss_u 0.8657 (0.8322) acc_u 12.5000 (20.2344) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [45/56] time 0.439 (0.458) data 0.309 (0.327) loss_u loss_u 0.8804 (0.8271) acc_u 9.3750 (20.9722) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [50/56] time 0.433 (0.456) data 0.301 (0.325) loss_u loss_u 0.8276 (0.8266) acc_u 15.6250 (20.8750) lr 1.4818e-03 eta 0:00:02
epoch [70/200] batch [55/56] time 0.403 (0.454) data 0.271 (0.323) loss_u loss_u 0.8892 (0.8278) acc_u 9.3750 (20.7955) lr 1.4818e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1489
confident_label rate tensor(0.4235, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1328
clean true:1298
clean false:30
clean_rate:0.9774096385542169
noisy true:349
noisy false:1459
after delete: len(clean_dataset) 1328
after delete: len(noisy_dataset) 1808
epoch [71/200] batch [5/41] time 0.494 (0.505) data 0.364 (0.374) loss_x loss_x 1.4668 (1.1992) acc_x 65.6250 (71.2500) lr 1.4679e-03 eta 0:00:18
epoch [71/200] batch [10/41] time 0.538 (0.501) data 0.407 (0.371) loss_x loss_x 1.1816 (1.0930) acc_x 78.1250 (73.1250) lr 1.4679e-03 eta 0:00:15
epoch [71/200] batch [15/41] time 0.428 (0.473) data 0.298 (0.343) loss_x loss_x 1.2373 (1.2011) acc_x 65.6250 (70.4167) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [20/41] time 0.403 (0.476) data 0.272 (0.345) loss_x loss_x 1.3438 (1.2219) acc_x 59.3750 (69.5312) lr 1.4679e-03 eta 0:00:09
epoch [71/200] batch [25/41] time 0.512 (0.473) data 0.380 (0.343) loss_x loss_x 1.4961 (1.2819) acc_x 56.2500 (67.1250) lr 1.4679e-03 eta 0:00:07
epoch [71/200] batch [30/41] time 0.403 (0.461) data 0.273 (0.330) loss_x loss_x 0.8818 (1.2517) acc_x 68.7500 (67.6042) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [35/41] time 0.363 (0.457) data 0.233 (0.326) loss_x loss_x 1.6494 (1.2581) acc_x 62.5000 (68.1250) lr 1.4679e-03 eta 0:00:02
epoch [71/200] batch [40/41] time 0.494 (0.462) data 0.363 (0.331) loss_x loss_x 0.9478 (1.2355) acc_x 81.2500 (68.9844) lr 1.4679e-03 eta 0:00:00
epoch [71/200] batch [5/56] time 0.393 (0.454) data 0.261 (0.323) loss_u loss_u 0.7505 (0.8049) acc_u 25.0000 (23.1250) lr 1.4679e-03 eta 0:00:23
epoch [71/200] batch [10/56] time 0.480 (0.455) data 0.348 (0.324) loss_u loss_u 0.8564 (0.8183) acc_u 21.8750 (21.8750) lr 1.4679e-03 eta 0:00:20
epoch [71/200] batch [15/56] time 0.550 (0.455) data 0.419 (0.324) loss_u loss_u 0.7969 (0.8203) acc_u 25.0000 (22.2917) lr 1.4679e-03 eta 0:00:18
epoch [71/200] batch [20/56] time 0.420 (0.452) data 0.290 (0.322) loss_u loss_u 0.8066 (0.8259) acc_u 21.8750 (21.8750) lr 1.4679e-03 eta 0:00:16
epoch [71/200] batch [25/56] time 0.520 (0.451) data 0.389 (0.320) loss_u loss_u 0.8179 (0.8327) acc_u 21.8750 (20.6250) lr 1.4679e-03 eta 0:00:13
epoch [71/200] batch [30/56] time 0.371 (0.453) data 0.240 (0.322) loss_u loss_u 0.7949 (0.8240) acc_u 28.1250 (21.6667) lr 1.4679e-03 eta 0:00:11
epoch [71/200] batch [35/56] time 0.485 (0.453) data 0.354 (0.322) loss_u loss_u 0.8467 (0.8252) acc_u 25.0000 (21.5179) lr 1.4679e-03 eta 0:00:09
epoch [71/200] batch [40/56] time 0.369 (0.450) data 0.239 (0.320) loss_u loss_u 0.8477 (0.8279) acc_u 21.8750 (21.1719) lr 1.4679e-03 eta 0:00:07
epoch [71/200] batch [45/56] time 0.402 (0.451) data 0.271 (0.320) loss_u loss_u 0.7998 (0.8282) acc_u 25.0000 (20.8333) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [50/56] time 0.383 (0.453) data 0.251 (0.322) loss_u loss_u 0.7925 (0.8268) acc_u 28.1250 (21.5000) lr 1.4679e-03 eta 0:00:02
epoch [71/200] batch [55/56] time 0.426 (0.452) data 0.294 (0.321) loss_u loss_u 0.6953 (0.8241) acc_u 34.3750 (21.6477) lr 1.4679e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1514
confident_label rate tensor(0.4155, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1303
clean true:1271
clean false:32
clean_rate:0.97544128933231
noisy true:351
noisy false:1482
after delete: len(clean_dataset) 1303
after delete: len(noisy_dataset) 1833
epoch [72/200] batch [5/40] time 0.473 (0.422) data 0.342 (0.291) loss_x loss_x 1.3203 (1.3914) acc_x 68.7500 (63.1250) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [10/40] time 0.387 (0.435) data 0.257 (0.305) loss_x loss_x 1.2383 (1.3106) acc_x 59.3750 (65.3125) lr 1.4540e-03 eta 0:00:13
epoch [72/200] batch [15/40] time 0.368 (0.426) data 0.238 (0.296) loss_x loss_x 1.3066 (1.2570) acc_x 71.8750 (67.9167) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [20/40] time 0.461 (0.432) data 0.330 (0.302) loss_x loss_x 1.2520 (1.2546) acc_x 68.7500 (68.1250) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [25/40] time 0.571 (0.445) data 0.441 (0.315) loss_x loss_x 1.0625 (1.2235) acc_x 62.5000 (68.1250) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [30/40] time 0.332 (0.445) data 0.202 (0.314) loss_x loss_x 1.3359 (1.2359) acc_x 75.0000 (67.3958) lr 1.4540e-03 eta 0:00:04
epoch [72/200] batch [35/40] time 0.422 (0.450) data 0.291 (0.319) loss_x loss_x 1.4971 (1.2434) acc_x 50.0000 (66.6964) lr 1.4540e-03 eta 0:00:02
epoch [72/200] batch [40/40] time 0.390 (0.449) data 0.260 (0.319) loss_x loss_x 0.9277 (1.2179) acc_x 68.7500 (67.2656) lr 1.4540e-03 eta 0:00:00
epoch [72/200] batch [5/57] time 0.554 (0.455) data 0.424 (0.325) loss_u loss_u 0.7944 (0.8214) acc_u 25.0000 (23.1250) lr 1.4540e-03 eta 0:00:23
epoch [72/200] batch [10/57] time 0.380 (0.450) data 0.250 (0.319) loss_u loss_u 0.8413 (0.8300) acc_u 18.7500 (21.8750) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [15/57] time 0.363 (0.448) data 0.233 (0.317) loss_u loss_u 0.8276 (0.8287) acc_u 18.7500 (21.6667) lr 1.4540e-03 eta 0:00:18
epoch [72/200] batch [20/57] time 0.459 (0.447) data 0.327 (0.317) loss_u loss_u 0.8418 (0.8298) acc_u 25.0000 (21.2500) lr 1.4540e-03 eta 0:00:16
epoch [72/200] batch [25/57] time 0.579 (0.455) data 0.445 (0.324) loss_u loss_u 0.7920 (0.8312) acc_u 25.0000 (21.0000) lr 1.4540e-03 eta 0:00:14
epoch [72/200] batch [30/57] time 0.359 (0.452) data 0.229 (0.322) loss_u loss_u 0.7256 (0.8262) acc_u 34.3750 (21.6667) lr 1.4540e-03 eta 0:00:12
epoch [72/200] batch [35/57] time 0.688 (0.453) data 0.557 (0.322) loss_u loss_u 0.8257 (0.8271) acc_u 25.0000 (21.2500) lr 1.4540e-03 eta 0:00:09
epoch [72/200] batch [40/57] time 0.383 (0.451) data 0.251 (0.320) loss_u loss_u 0.7979 (0.8262) acc_u 21.8750 (21.1719) lr 1.4540e-03 eta 0:00:07
epoch [72/200] batch [45/57] time 0.363 (0.449) data 0.231 (0.318) loss_u loss_u 0.7832 (0.8224) acc_u 28.1250 (21.8750) lr 1.4540e-03 eta 0:00:05
epoch [72/200] batch [50/57] time 0.433 (0.452) data 0.302 (0.321) loss_u loss_u 0.8193 (0.8218) acc_u 18.7500 (22.0000) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [55/57] time 0.419 (0.452) data 0.287 (0.321) loss_u loss_u 0.8213 (0.8203) acc_u 25.0000 (22.2727) lr 1.4540e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.4200, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1317
clean true:1276
clean false:41
clean_rate:0.9688686408504176
noisy true:343
noisy false:1476
after delete: len(clean_dataset) 1317
after delete: len(noisy_dataset) 1819
epoch [73/200] batch [5/41] time 0.629 (0.540) data 0.499 (0.409) loss_x loss_x 1.3184 (1.1767) acc_x 71.8750 (70.6250) lr 1.4399e-03 eta 0:00:19
epoch [73/200] batch [10/41] time 0.453 (0.490) data 0.321 (0.360) loss_x loss_x 1.1045 (1.1430) acc_x 71.8750 (70.9375) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [15/41] time 0.506 (0.501) data 0.376 (0.370) loss_x loss_x 1.0039 (1.1801) acc_x 81.2500 (69.7917) lr 1.4399e-03 eta 0:00:13
epoch [73/200] batch [20/41] time 0.450 (0.497) data 0.319 (0.367) loss_x loss_x 1.6807 (1.2266) acc_x 53.1250 (67.8125) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [25/41] time 0.574 (0.497) data 0.444 (0.367) loss_x loss_x 1.4316 (1.2652) acc_x 62.5000 (67.1250) lr 1.4399e-03 eta 0:00:07
epoch [73/200] batch [30/41] time 0.486 (0.502) data 0.356 (0.371) loss_x loss_x 1.3545 (1.2338) acc_x 68.7500 (67.9167) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [35/41] time 0.613 (0.502) data 0.482 (0.371) loss_x loss_x 1.6611 (1.2427) acc_x 68.7500 (67.8571) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [40/41] time 0.508 (0.498) data 0.377 (0.368) loss_x loss_x 1.3701 (1.2497) acc_x 68.7500 (67.8125) lr 1.4399e-03 eta 0:00:00
epoch [73/200] batch [5/56] time 0.378 (0.487) data 0.247 (0.356) loss_u loss_u 0.8345 (0.8241) acc_u 21.8750 (21.2500) lr 1.4399e-03 eta 0:00:24
epoch [73/200] batch [10/56] time 0.413 (0.484) data 0.282 (0.353) loss_u loss_u 0.8877 (0.8420) acc_u 15.6250 (18.4375) lr 1.4399e-03 eta 0:00:22
epoch [73/200] batch [15/56] time 0.417 (0.482) data 0.287 (0.351) loss_u loss_u 0.7505 (0.8397) acc_u 34.3750 (19.3750) lr 1.4399e-03 eta 0:00:19
epoch [73/200] batch [20/56] time 0.373 (0.477) data 0.243 (0.346) loss_u loss_u 0.8145 (0.8435) acc_u 21.8750 (18.5938) lr 1.4399e-03 eta 0:00:17
epoch [73/200] batch [25/56] time 0.966 (0.481) data 0.835 (0.350) loss_u loss_u 0.8105 (0.8422) acc_u 25.0000 (19.2500) lr 1.4399e-03 eta 0:00:14
epoch [73/200] batch [30/56] time 0.385 (0.476) data 0.253 (0.346) loss_u loss_u 0.8022 (0.8366) acc_u 28.1250 (20.3125) lr 1.4399e-03 eta 0:00:12
epoch [73/200] batch [35/56] time 0.453 (0.473) data 0.323 (0.343) loss_u loss_u 0.8350 (0.8332) acc_u 21.8750 (20.8036) lr 1.4399e-03 eta 0:00:09
epoch [73/200] batch [40/56] time 0.512 (0.470) data 0.379 (0.339) loss_u loss_u 0.8848 (0.8310) acc_u 18.7500 (20.9375) lr 1.4399e-03 eta 0:00:07
epoch [73/200] batch [45/56] time 0.411 (0.471) data 0.281 (0.340) loss_u loss_u 0.7979 (0.8295) acc_u 25.0000 (21.1111) lr 1.4399e-03 eta 0:00:05
epoch [73/200] batch [50/56] time 0.448 (0.469) data 0.316 (0.338) loss_u loss_u 0.9121 (0.8283) acc_u 9.3750 (21.2500) lr 1.4399e-03 eta 0:00:02
epoch [73/200] batch [55/56] time 0.576 (0.471) data 0.444 (0.340) loss_u loss_u 0.7812 (0.8269) acc_u 28.1250 (21.4773) lr 1.4399e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1470
confident_label rate tensor(0.4289, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1345
clean true:1313
clean false:32
clean_rate:0.9762081784386617
noisy true:353
noisy false:1438
after delete: len(clean_dataset) 1345
after delete: len(noisy_dataset) 1791
epoch [74/200] batch [5/42] time 0.507 (0.485) data 0.376 (0.354) loss_x loss_x 1.2188 (1.1389) acc_x 75.0000 (70.0000) lr 1.4258e-03 eta 0:00:17
epoch [74/200] batch [10/42] time 0.450 (0.453) data 0.320 (0.323) loss_x loss_x 0.9731 (1.1420) acc_x 75.0000 (70.6250) lr 1.4258e-03 eta 0:00:14
epoch [74/200] batch [15/42] time 0.418 (0.450) data 0.286 (0.319) loss_x loss_x 1.5059 (1.2570) acc_x 59.3750 (69.7917) lr 1.4258e-03 eta 0:00:12
epoch [74/200] batch [20/42] time 0.363 (0.441) data 0.233 (0.310) loss_x loss_x 1.2178 (1.2514) acc_x 71.8750 (69.8438) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [25/42] time 0.422 (0.449) data 0.291 (0.318) loss_x loss_x 1.4609 (1.3013) acc_x 71.8750 (68.0000) lr 1.4258e-03 eta 0:00:07
epoch [74/200] batch [30/42] time 0.451 (0.445) data 0.317 (0.314) loss_x loss_x 1.7148 (1.2890) acc_x 59.3750 (68.5417) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [35/42] time 0.541 (0.450) data 0.410 (0.319) loss_x loss_x 1.3525 (1.2822) acc_x 68.7500 (68.3929) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [40/42] time 0.447 (0.449) data 0.317 (0.318) loss_x loss_x 1.2988 (1.2794) acc_x 68.7500 (68.5156) lr 1.4258e-03 eta 0:00:00
epoch [74/200] batch [5/55] time 0.440 (0.446) data 0.309 (0.315) loss_u loss_u 0.8066 (0.8021) acc_u 21.8750 (21.2500) lr 1.4258e-03 eta 0:00:22
epoch [74/200] batch [10/55] time 0.487 (0.455) data 0.356 (0.324) loss_u loss_u 0.8105 (0.7956) acc_u 25.0000 (26.2500) lr 1.4258e-03 eta 0:00:20
epoch [74/200] batch [15/55] time 0.494 (0.453) data 0.363 (0.323) loss_u loss_u 0.8467 (0.8171) acc_u 21.8750 (23.3333) lr 1.4258e-03 eta 0:00:18
epoch [74/200] batch [20/55] time 0.440 (0.451) data 0.310 (0.320) loss_u loss_u 0.8301 (0.8152) acc_u 28.1250 (23.7500) lr 1.4258e-03 eta 0:00:15
epoch [74/200] batch [25/55] time 0.609 (0.454) data 0.478 (0.323) loss_u loss_u 0.8174 (0.8128) acc_u 28.1250 (24.5000) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [30/55] time 0.367 (0.453) data 0.236 (0.323) loss_u loss_u 0.8555 (0.8147) acc_u 18.7500 (24.0625) lr 1.4258e-03 eta 0:00:11
epoch [74/200] batch [35/55] time 0.385 (0.452) data 0.254 (0.322) loss_u loss_u 0.7856 (0.8148) acc_u 25.0000 (23.9286) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [40/55] time 0.303 (0.449) data 0.173 (0.318) loss_u loss_u 0.8018 (0.8189) acc_u 21.8750 (23.4375) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [45/55] time 0.448 (0.449) data 0.317 (0.318) loss_u loss_u 0.7651 (0.8181) acc_u 28.1250 (23.1250) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [50/55] time 0.502 (0.451) data 0.370 (0.321) loss_u loss_u 0.8022 (0.8172) acc_u 18.7500 (22.6875) lr 1.4258e-03 eta 0:00:02
epoch [74/200] batch [55/55] time 0.449 (0.454) data 0.317 (0.323) loss_u loss_u 0.8750 (0.8174) acc_u 12.5000 (22.5000) lr 1.4258e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1494
confident_label rate tensor(0.4244, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1331
clean true:1291
clean false:40
clean_rate:0.9699474079639369
noisy true:351
noisy false:1454
after delete: len(clean_dataset) 1331
after delete: len(noisy_dataset) 1805
epoch [75/200] batch [5/41] time 0.412 (0.419) data 0.282 (0.288) loss_x loss_x 1.6602 (1.3076) acc_x 56.2500 (68.7500) lr 1.4115e-03 eta 0:00:15
epoch [75/200] batch [10/41] time 0.420 (0.455) data 0.290 (0.324) loss_x loss_x 1.2158 (1.2435) acc_x 59.3750 (67.5000) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [15/41] time 0.761 (0.479) data 0.630 (0.349) loss_x loss_x 1.6602 (1.2753) acc_x 53.1250 (66.8750) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [20/41] time 0.374 (0.475) data 0.244 (0.344) loss_x loss_x 1.3730 (1.3032) acc_x 68.7500 (65.6250) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [25/41] time 0.598 (0.467) data 0.468 (0.337) loss_x loss_x 1.4082 (1.2855) acc_x 65.6250 (66.1250) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [30/41] time 0.484 (0.469) data 0.353 (0.338) loss_x loss_x 1.4150 (1.2652) acc_x 56.2500 (66.5625) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [35/41] time 0.442 (0.467) data 0.311 (0.336) loss_x loss_x 1.2041 (1.2677) acc_x 71.8750 (66.7857) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [40/41] time 0.450 (0.471) data 0.319 (0.341) loss_x loss_x 0.8242 (1.2465) acc_x 65.6250 (66.5625) lr 1.4115e-03 eta 0:00:00
epoch [75/200] batch [5/56] time 0.444 (0.472) data 0.312 (0.342) loss_u loss_u 0.7407 (0.7874) acc_u 28.1250 (25.0000) lr 1.4115e-03 eta 0:00:24
epoch [75/200] batch [10/56] time 0.487 (0.467) data 0.354 (0.336) loss_u loss_u 0.8057 (0.8016) acc_u 31.2500 (25.3125) lr 1.4115e-03 eta 0:00:21
epoch [75/200] batch [15/56] time 0.329 (0.467) data 0.197 (0.336) loss_u loss_u 0.8257 (0.8071) acc_u 25.0000 (24.1667) lr 1.4115e-03 eta 0:00:19
epoch [75/200] batch [20/56] time 0.522 (0.467) data 0.391 (0.336) loss_u loss_u 0.8452 (0.8020) acc_u 18.7500 (25.1562) lr 1.4115e-03 eta 0:00:16
epoch [75/200] batch [25/56] time 0.368 (0.466) data 0.238 (0.335) loss_u loss_u 0.7661 (0.8003) acc_u 28.1250 (25.6250) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [30/56] time 0.377 (0.465) data 0.247 (0.334) loss_u loss_u 0.8325 (0.8075) acc_u 25.0000 (24.6875) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [35/56] time 0.556 (0.465) data 0.424 (0.334) loss_u loss_u 0.8662 (0.8089) acc_u 15.6250 (24.3750) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [40/56] time 0.400 (0.463) data 0.268 (0.332) loss_u loss_u 0.8267 (0.8092) acc_u 21.8750 (24.1406) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [45/56] time 0.557 (0.462) data 0.425 (0.331) loss_u loss_u 0.8550 (0.8119) acc_u 18.7500 (24.0972) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [50/56] time 0.478 (0.463) data 0.348 (0.332) loss_u loss_u 0.8037 (0.8118) acc_u 25.0000 (23.7500) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [55/56] time 0.359 (0.462) data 0.228 (0.331) loss_u loss_u 0.7710 (0.8093) acc_u 34.3750 (24.2045) lr 1.4115e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1515
confident_label rate tensor(0.4190, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1314
clean true:1276
clean false:38
clean_rate:0.9710806697108066
noisy true:345
noisy false:1477
after delete: len(clean_dataset) 1314
after delete: len(noisy_dataset) 1822
epoch [76/200] batch [5/41] time 0.560 (0.436) data 0.430 (0.306) loss_x loss_x 0.8130 (1.2718) acc_x 81.2500 (65.0000) lr 1.3971e-03 eta 0:00:15
epoch [76/200] batch [10/41] time 0.497 (0.451) data 0.367 (0.320) loss_x loss_x 0.9180 (1.1464) acc_x 71.8750 (71.2500) lr 1.3971e-03 eta 0:00:13
epoch [76/200] batch [15/41] time 0.565 (0.460) data 0.434 (0.330) loss_x loss_x 1.3018 (1.2287) acc_x 65.6250 (69.5833) lr 1.3971e-03 eta 0:00:11
epoch [76/200] batch [20/41] time 0.375 (0.457) data 0.244 (0.326) loss_x loss_x 1.5654 (1.2302) acc_x 56.2500 (68.5938) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [25/41] time 0.450 (0.458) data 0.319 (0.328) loss_x loss_x 1.4521 (1.2217) acc_x 68.7500 (69.2500) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [30/41] time 0.447 (0.467) data 0.315 (0.337) loss_x loss_x 1.3164 (1.2446) acc_x 56.2500 (69.0625) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [35/41] time 0.480 (0.465) data 0.349 (0.335) loss_x loss_x 0.8970 (1.2025) acc_x 75.0000 (69.7321) lr 1.3971e-03 eta 0:00:02
epoch [76/200] batch [40/41] time 0.581 (0.469) data 0.451 (0.339) loss_x loss_x 1.3105 (1.2301) acc_x 62.5000 (68.7500) lr 1.3971e-03 eta 0:00:00
epoch [76/200] batch [5/56] time 0.439 (0.469) data 0.308 (0.339) loss_u loss_u 0.8701 (0.7891) acc_u 15.6250 (23.1250) lr 1.3971e-03 eta 0:00:23
epoch [76/200] batch [10/56] time 0.384 (0.461) data 0.254 (0.331) loss_u loss_u 0.8569 (0.8201) acc_u 18.7500 (20.6250) lr 1.3971e-03 eta 0:00:21
epoch [76/200] batch [15/56] time 0.414 (0.459) data 0.284 (0.328) loss_u loss_u 0.7334 (0.8125) acc_u 40.6250 (22.9167) lr 1.3971e-03 eta 0:00:18
epoch [76/200] batch [20/56] time 0.356 (0.458) data 0.225 (0.327) loss_u loss_u 0.8374 (0.8189) acc_u 18.7500 (22.1875) lr 1.3971e-03 eta 0:00:16
epoch [76/200] batch [25/56] time 0.496 (0.458) data 0.365 (0.328) loss_u loss_u 0.7588 (0.8130) acc_u 34.3750 (23.2500) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [30/56] time 0.429 (0.459) data 0.298 (0.328) loss_u loss_u 0.8013 (0.8146) acc_u 28.1250 (23.1250) lr 1.3971e-03 eta 0:00:11
epoch [76/200] batch [35/56] time 0.419 (0.463) data 0.283 (0.332) loss_u loss_u 0.8193 (0.8129) acc_u 25.0000 (22.9464) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [40/56] time 0.556 (0.463) data 0.424 (0.332) loss_u loss_u 0.8169 (0.8082) acc_u 18.7500 (23.4375) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [45/56] time 0.411 (0.464) data 0.279 (0.333) loss_u loss_u 0.8423 (0.8100) acc_u 25.0000 (23.2639) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [50/56] time 0.398 (0.461) data 0.266 (0.330) loss_u loss_u 0.8711 (0.8137) acc_u 18.7500 (23.0000) lr 1.3971e-03 eta 0:00:02
epoch [76/200] batch [55/56] time 0.407 (0.460) data 0.275 (0.329) loss_u loss_u 0.7847 (0.8116) acc_u 28.1250 (23.2955) lr 1.3971e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1490
confident_label rate tensor(0.4273, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1340
clean true:1300
clean false:40
clean_rate:0.9701492537313433
noisy true:346
noisy false:1450
after delete: len(clean_dataset) 1340
after delete: len(noisy_dataset) 1796
epoch [77/200] batch [5/41] time 0.437 (0.518) data 0.306 (0.388) loss_x loss_x 0.8501 (1.1660) acc_x 71.8750 (68.7500) lr 1.3827e-03 eta 0:00:18
epoch [77/200] batch [10/41] time 0.515 (0.505) data 0.384 (0.375) loss_x loss_x 1.3975 (1.2742) acc_x 56.2500 (69.6875) lr 1.3827e-03 eta 0:00:15
epoch [77/200] batch [15/41] time 0.372 (0.503) data 0.241 (0.372) loss_x loss_x 1.0684 (1.2296) acc_x 75.0000 (70.0000) lr 1.3827e-03 eta 0:00:13
epoch [77/200] batch [20/41] time 0.521 (0.499) data 0.390 (0.368) loss_x loss_x 1.4873 (1.2314) acc_x 65.6250 (69.6875) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [25/41] time 0.488 (0.504) data 0.357 (0.373) loss_x loss_x 1.7549 (1.2723) acc_x 56.2500 (68.2500) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [30/41] time 0.486 (0.502) data 0.356 (0.371) loss_x loss_x 1.1113 (1.2886) acc_x 68.7500 (67.7083) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [35/41] time 0.460 (0.498) data 0.329 (0.367) loss_x loss_x 0.8228 (1.2640) acc_x 78.1250 (68.4821) lr 1.3827e-03 eta 0:00:02
epoch [77/200] batch [40/41] time 0.391 (0.491) data 0.260 (0.360) loss_x loss_x 0.8613 (1.2586) acc_x 78.1250 (68.4375) lr 1.3827e-03 eta 0:00:00
epoch [77/200] batch [5/56] time 0.653 (0.489) data 0.521 (0.358) loss_u loss_u 0.8315 (0.8430) acc_u 25.0000 (19.3750) lr 1.3827e-03 eta 0:00:24
epoch [77/200] batch [10/56] time 0.425 (0.484) data 0.294 (0.353) loss_u loss_u 0.6714 (0.8167) acc_u 40.6250 (21.8750) lr 1.3827e-03 eta 0:00:22
epoch [77/200] batch [15/56] time 0.435 (0.478) data 0.303 (0.347) loss_u loss_u 0.7988 (0.8164) acc_u 28.1250 (23.1250) lr 1.3827e-03 eta 0:00:19
epoch [77/200] batch [20/56] time 0.373 (0.472) data 0.241 (0.341) loss_u loss_u 0.8208 (0.8149) acc_u 18.7500 (22.8125) lr 1.3827e-03 eta 0:00:16
epoch [77/200] batch [25/56] time 0.557 (0.469) data 0.426 (0.338) loss_u loss_u 0.8140 (0.8163) acc_u 25.0000 (22.7500) lr 1.3827e-03 eta 0:00:14
epoch [77/200] batch [30/56] time 0.331 (0.465) data 0.201 (0.334) loss_u loss_u 0.8262 (0.8171) acc_u 21.8750 (22.9167) lr 1.3827e-03 eta 0:00:12
epoch [77/200] batch [35/56] time 0.485 (0.465) data 0.354 (0.334) loss_u loss_u 0.8228 (0.8118) acc_u 25.0000 (23.9286) lr 1.3827e-03 eta 0:00:09
epoch [77/200] batch [40/56] time 0.591 (0.467) data 0.459 (0.336) loss_u loss_u 0.8804 (0.8130) acc_u 12.5000 (23.7500) lr 1.3827e-03 eta 0:00:07
epoch [77/200] batch [45/56] time 0.327 (0.469) data 0.197 (0.338) loss_u loss_u 0.8960 (0.8181) acc_u 12.5000 (22.9167) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [50/56] time 0.472 (0.469) data 0.341 (0.338) loss_u loss_u 0.7563 (0.8169) acc_u 37.5000 (23.0625) lr 1.3827e-03 eta 0:00:02
epoch [77/200] batch [55/56] time 0.376 (0.465) data 0.245 (0.334) loss_u loss_u 0.7959 (0.8166) acc_u 25.0000 (22.9545) lr 1.3827e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1503
confident_label rate tensor(0.4247, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1332
clean true:1285
clean false:47
clean_rate:0.9647147147147147
noisy true:348
noisy false:1456
after delete: len(clean_dataset) 1332
after delete: len(noisy_dataset) 1804
epoch [78/200] batch [5/41] time 0.448 (0.452) data 0.318 (0.322) loss_x loss_x 0.7812 (1.1543) acc_x 81.2500 (70.0000) lr 1.3681e-03 eta 0:00:16
epoch [78/200] batch [10/41] time 0.447 (0.474) data 0.316 (0.344) loss_x loss_x 1.2969 (1.2156) acc_x 65.6250 (68.7500) lr 1.3681e-03 eta 0:00:14
epoch [78/200] batch [15/41] time 0.490 (0.476) data 0.360 (0.345) loss_x loss_x 1.8057 (1.2382) acc_x 43.7500 (66.4583) lr 1.3681e-03 eta 0:00:12
epoch [78/200] batch [20/41] time 0.423 (0.466) data 0.292 (0.336) loss_x loss_x 0.8813 (1.2170) acc_x 65.6250 (67.1875) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [25/41] time 0.432 (0.460) data 0.302 (0.330) loss_x loss_x 1.5693 (1.2646) acc_x 56.2500 (67.0000) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [30/41] time 0.434 (0.464) data 0.304 (0.334) loss_x loss_x 1.1602 (1.2556) acc_x 65.6250 (66.9792) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [35/41] time 0.521 (0.466) data 0.391 (0.335) loss_x loss_x 1.1016 (1.2435) acc_x 75.0000 (68.1250) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [40/41] time 0.596 (0.468) data 0.465 (0.337) loss_x loss_x 1.5098 (1.2485) acc_x 68.7500 (68.0469) lr 1.3681e-03 eta 0:00:00
epoch [78/200] batch [5/56] time 0.337 (0.462) data 0.205 (0.331) loss_u loss_u 0.7710 (0.7896) acc_u 25.0000 (25.6250) lr 1.3681e-03 eta 0:00:23
epoch [78/200] batch [10/56] time 0.374 (0.457) data 0.244 (0.327) loss_u loss_u 0.9033 (0.8026) acc_u 6.2500 (24.3750) lr 1.3681e-03 eta 0:00:21
epoch [78/200] batch [15/56] time 0.463 (0.457) data 0.333 (0.326) loss_u loss_u 0.8950 (0.8116) acc_u 15.6250 (23.3333) lr 1.3681e-03 eta 0:00:18
epoch [78/200] batch [20/56] time 0.411 (0.452) data 0.281 (0.322) loss_u loss_u 0.8545 (0.8105) acc_u 18.7500 (23.5938) lr 1.3681e-03 eta 0:00:16
epoch [78/200] batch [25/56] time 0.421 (0.454) data 0.290 (0.323) loss_u loss_u 0.8379 (0.8137) acc_u 21.8750 (22.7500) lr 1.3681e-03 eta 0:00:14
epoch [78/200] batch [30/56] time 0.443 (0.452) data 0.312 (0.322) loss_u loss_u 0.8472 (0.8160) acc_u 18.7500 (22.3958) lr 1.3681e-03 eta 0:00:11
epoch [78/200] batch [35/56] time 0.454 (0.450) data 0.323 (0.319) loss_u loss_u 0.8008 (0.8174) acc_u 28.1250 (22.5000) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [40/56] time 0.483 (0.452) data 0.351 (0.321) loss_u loss_u 0.7744 (0.8200) acc_u 31.2500 (22.1875) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [45/56] time 0.558 (0.454) data 0.427 (0.323) loss_u loss_u 0.7173 (0.8152) acc_u 37.5000 (22.7778) lr 1.3681e-03 eta 0:00:04
epoch [78/200] batch [50/56] time 0.345 (0.453) data 0.214 (0.322) loss_u loss_u 0.7031 (0.8118) acc_u 37.5000 (23.4375) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [55/56] time 0.414 (0.450) data 0.282 (0.319) loss_u loss_u 0.8486 (0.8121) acc_u 21.8750 (23.4659) lr 1.3681e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1502
confident_label rate tensor(0.4241, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1330
clean true:1286
clean false:44
clean_rate:0.9669172932330827
noisy true:348
noisy false:1458
after delete: len(clean_dataset) 1330
after delete: len(noisy_dataset) 1806
epoch [79/200] batch [5/41] time 0.515 (0.478) data 0.385 (0.348) loss_x loss_x 0.5381 (1.0464) acc_x 87.5000 (72.5000) lr 1.3535e-03 eta 0:00:17
epoch [79/200] batch [10/41] time 0.504 (0.464) data 0.374 (0.334) loss_x loss_x 1.4570 (1.1636) acc_x 62.5000 (70.9375) lr 1.3535e-03 eta 0:00:14
epoch [79/200] batch [15/41] time 0.366 (0.454) data 0.237 (0.324) loss_x loss_x 1.2129 (1.1989) acc_x 68.7500 (69.7917) lr 1.3535e-03 eta 0:00:11
epoch [79/200] batch [20/41] time 0.435 (0.446) data 0.304 (0.316) loss_x loss_x 0.8579 (1.1606) acc_x 75.0000 (70.7812) lr 1.3535e-03 eta 0:00:09
epoch [79/200] batch [25/41] time 0.451 (0.449) data 0.320 (0.319) loss_x loss_x 0.9707 (1.1537) acc_x 71.8750 (70.8750) lr 1.3535e-03 eta 0:00:07
epoch [79/200] batch [30/41] time 0.474 (0.454) data 0.343 (0.324) loss_x loss_x 1.2188 (1.1321) acc_x 65.6250 (70.9375) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [35/41] time 0.438 (0.456) data 0.307 (0.326) loss_x loss_x 1.2266 (1.1462) acc_x 71.8750 (70.8036) lr 1.3535e-03 eta 0:00:02
epoch [79/200] batch [40/41] time 0.322 (0.453) data 0.190 (0.323) loss_x loss_x 1.1445 (1.1457) acc_x 68.7500 (71.0938) lr 1.3535e-03 eta 0:00:00
epoch [79/200] batch [5/56] time 0.464 (0.457) data 0.333 (0.327) loss_u loss_u 0.7490 (0.8439) acc_u 28.1250 (18.7500) lr 1.3535e-03 eta 0:00:23
epoch [79/200] batch [10/56] time 0.480 (0.456) data 0.349 (0.325) loss_u loss_u 0.9136 (0.8418) acc_u 9.3750 (19.6875) lr 1.3535e-03 eta 0:00:20
epoch [79/200] batch [15/56] time 0.403 (0.451) data 0.272 (0.320) loss_u loss_u 0.8530 (0.8338) acc_u 15.6250 (20.2083) lr 1.3535e-03 eta 0:00:18
epoch [79/200] batch [20/56] time 0.388 (0.449) data 0.256 (0.318) loss_u loss_u 0.8779 (0.8330) acc_u 18.7500 (20.4688) lr 1.3535e-03 eta 0:00:16
epoch [79/200] batch [25/56] time 0.346 (0.447) data 0.214 (0.316) loss_u loss_u 0.7891 (0.8288) acc_u 25.0000 (21.3750) lr 1.3535e-03 eta 0:00:13
epoch [79/200] batch [30/56] time 0.458 (0.448) data 0.327 (0.317) loss_u loss_u 0.8135 (0.8247) acc_u 21.8750 (21.6667) lr 1.3535e-03 eta 0:00:11
epoch [79/200] batch [35/56] time 0.337 (0.449) data 0.207 (0.318) loss_u loss_u 0.6831 (0.8191) acc_u 43.7500 (22.4107) lr 1.3535e-03 eta 0:00:09
epoch [79/200] batch [40/56] time 0.413 (0.449) data 0.281 (0.318) loss_u loss_u 0.7979 (0.8191) acc_u 21.8750 (22.4219) lr 1.3535e-03 eta 0:00:07
epoch [79/200] batch [45/56] time 0.511 (0.451) data 0.381 (0.320) loss_u loss_u 0.8813 (0.8190) acc_u 15.6250 (22.5694) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [50/56] time 0.361 (0.449) data 0.229 (0.318) loss_u loss_u 0.7690 (0.8175) acc_u 31.2500 (22.6875) lr 1.3535e-03 eta 0:00:02
epoch [79/200] batch [55/56] time 0.427 (0.449) data 0.296 (0.318) loss_u loss_u 0.7085 (0.8143) acc_u 34.3750 (23.1250) lr 1.3535e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1488
confident_label rate tensor(0.4254, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1334
clean true:1297
clean false:37
clean_rate:0.972263868065967
noisy true:351
noisy false:1451
after delete: len(clean_dataset) 1334
after delete: len(noisy_dataset) 1802
epoch [80/200] batch [5/41] time 0.452 (0.504) data 0.321 (0.373) loss_x loss_x 1.2900 (1.2278) acc_x 62.5000 (67.5000) lr 1.3387e-03 eta 0:00:18
epoch [80/200] batch [10/41] time 0.520 (0.501) data 0.387 (0.371) loss_x loss_x 0.7109 (1.1527) acc_x 78.1250 (70.3125) lr 1.3387e-03 eta 0:00:15
epoch [80/200] batch [15/41] time 0.382 (0.476) data 0.253 (0.346) loss_x loss_x 1.3301 (1.2030) acc_x 62.5000 (69.1667) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [20/41] time 0.493 (0.472) data 0.363 (0.341) loss_x loss_x 0.9912 (1.1864) acc_x 75.0000 (70.0000) lr 1.3387e-03 eta 0:00:09
epoch [80/200] batch [25/41] time 0.464 (0.467) data 0.334 (0.336) loss_x loss_x 1.0508 (1.2047) acc_x 75.0000 (69.7500) lr 1.3387e-03 eta 0:00:07
epoch [80/200] batch [30/41] time 0.448 (0.465) data 0.318 (0.334) loss_x loss_x 1.3877 (1.2008) acc_x 62.5000 (70.1042) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [35/41] time 0.489 (0.462) data 0.359 (0.332) loss_x loss_x 0.7183 (1.1824) acc_x 87.5000 (70.7143) lr 1.3387e-03 eta 0:00:02
epoch [80/200] batch [40/41] time 0.456 (0.462) data 0.326 (0.332) loss_x loss_x 1.4629 (1.1876) acc_x 65.6250 (71.1719) lr 1.3387e-03 eta 0:00:00
epoch [80/200] batch [5/56] time 0.462 (0.458) data 0.332 (0.328) loss_u loss_u 0.7803 (0.8316) acc_u 25.0000 (19.3750) lr 1.3387e-03 eta 0:00:23
epoch [80/200] batch [10/56] time 0.423 (0.456) data 0.292 (0.326) loss_u loss_u 0.7734 (0.8173) acc_u 28.1250 (21.8750) lr 1.3387e-03 eta 0:00:20
epoch [80/200] batch [15/56] time 0.381 (0.456) data 0.251 (0.325) loss_u loss_u 0.8584 (0.8091) acc_u 18.7500 (23.1250) lr 1.3387e-03 eta 0:00:18
epoch [80/200] batch [20/56] time 0.403 (0.453) data 0.272 (0.322) loss_u loss_u 0.8491 (0.8147) acc_u 21.8750 (22.1875) lr 1.3387e-03 eta 0:00:16
epoch [80/200] batch [25/56] time 0.390 (0.451) data 0.260 (0.321) loss_u loss_u 0.8506 (0.8191) acc_u 15.6250 (21.8750) lr 1.3387e-03 eta 0:00:13
epoch [80/200] batch [30/56] time 0.386 (0.451) data 0.256 (0.320) loss_u loss_u 0.8755 (0.8175) acc_u 18.7500 (22.7083) lr 1.3387e-03 eta 0:00:11
epoch [80/200] batch [35/56] time 0.424 (0.450) data 0.292 (0.320) loss_u loss_u 0.7827 (0.8173) acc_u 25.0000 (22.9464) lr 1.3387e-03 eta 0:00:09
epoch [80/200] batch [40/56] time 0.495 (0.453) data 0.363 (0.323) loss_u loss_u 0.6992 (0.8143) acc_u 34.3750 (23.1250) lr 1.3387e-03 eta 0:00:07
epoch [80/200] batch [45/56] time 0.379 (0.452) data 0.248 (0.322) loss_u loss_u 0.8628 (0.8118) acc_u 25.0000 (23.8889) lr 1.3387e-03 eta 0:00:04
epoch [80/200] batch [50/56] time 0.423 (0.450) data 0.293 (0.320) loss_u loss_u 0.7378 (0.8102) acc_u 31.2500 (24.1250) lr 1.3387e-03 eta 0:00:02
epoch [80/200] batch [55/56] time 0.390 (0.449) data 0.259 (0.319) loss_u loss_u 0.8115 (0.8131) acc_u 21.8750 (23.5795) lr 1.3387e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1460
confident_label rate tensor(0.4343, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1362
clean true:1324
clean false:38
clean_rate:0.9720998531571219
noisy true:352
noisy false:1422
after delete: len(clean_dataset) 1362
after delete: len(noisy_dataset) 1774
epoch [81/200] batch [5/42] time 0.482 (0.471) data 0.351 (0.340) loss_x loss_x 0.9985 (1.0883) acc_x 78.1250 (73.7500) lr 1.3239e-03 eta 0:00:17
epoch [81/200] batch [10/42] time 0.445 (0.450) data 0.314 (0.319) loss_x loss_x 0.7920 (1.0539) acc_x 75.0000 (72.5000) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [15/42] time 0.577 (0.458) data 0.446 (0.327) loss_x loss_x 1.2930 (1.0805) acc_x 68.7500 (71.6667) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [20/42] time 0.478 (0.463) data 0.347 (0.332) loss_x loss_x 1.0801 (1.0997) acc_x 68.7500 (71.0938) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [25/42] time 0.507 (0.464) data 0.376 (0.333) loss_x loss_x 1.2158 (1.1097) acc_x 65.6250 (71.1250) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [30/42] time 0.527 (0.466) data 0.397 (0.335) loss_x loss_x 1.1523 (1.1465) acc_x 71.8750 (70.8333) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [35/42] time 0.576 (0.461) data 0.446 (0.331) loss_x loss_x 1.2979 (1.1509) acc_x 65.6250 (70.6250) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [40/42] time 0.557 (0.468) data 0.427 (0.338) loss_x loss_x 1.2109 (1.1940) acc_x 68.7500 (69.8438) lr 1.3239e-03 eta 0:00:00
epoch [81/200] batch [5/55] time 0.490 (0.465) data 0.358 (0.334) loss_u loss_u 0.8872 (0.8221) acc_u 18.7500 (21.8750) lr 1.3239e-03 eta 0:00:23
epoch [81/200] batch [10/55] time 0.394 (0.467) data 0.262 (0.336) loss_u loss_u 0.7344 (0.8151) acc_u 37.5000 (23.1250) lr 1.3239e-03 eta 0:00:21
epoch [81/200] batch [15/55] time 0.384 (0.461) data 0.253 (0.330) loss_u loss_u 0.9155 (0.8239) acc_u 12.5000 (22.5000) lr 1.3239e-03 eta 0:00:18
epoch [81/200] batch [20/55] time 0.384 (0.458) data 0.253 (0.328) loss_u loss_u 0.7729 (0.8209) acc_u 28.1250 (23.2812) lr 1.3239e-03 eta 0:00:16
epoch [81/200] batch [25/55] time 0.408 (0.458) data 0.276 (0.328) loss_u loss_u 0.8276 (0.8183) acc_u 18.7500 (23.0000) lr 1.3239e-03 eta 0:00:13
epoch [81/200] batch [30/55] time 0.486 (0.459) data 0.354 (0.328) loss_u loss_u 0.8794 (0.8216) acc_u 15.6250 (22.7083) lr 1.3239e-03 eta 0:00:11
epoch [81/200] batch [35/55] time 0.389 (0.455) data 0.258 (0.324) loss_u loss_u 0.8345 (0.8213) acc_u 18.7500 (22.7679) lr 1.3239e-03 eta 0:00:09
epoch [81/200] batch [40/55] time 0.549 (0.455) data 0.419 (0.324) loss_u loss_u 0.8892 (0.8260) acc_u 9.3750 (22.0312) lr 1.3239e-03 eta 0:00:06
epoch [81/200] batch [45/55] time 0.514 (0.457) data 0.382 (0.327) loss_u loss_u 0.8608 (0.8226) acc_u 18.7500 (22.2917) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [50/55] time 0.372 (0.453) data 0.242 (0.322) loss_u loss_u 0.8745 (0.8216) acc_u 15.6250 (22.5000) lr 1.3239e-03 eta 0:00:02
epoch [81/200] batch [55/55] time 0.418 (0.452) data 0.287 (0.321) loss_u loss_u 0.6938 (0.8214) acc_u 34.3750 (22.3864) lr 1.3239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1464
confident_label rate tensor(0.4401, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1380
clean true:1336
clean false:44
clean_rate:0.9681159420289855
noisy true:336
noisy false:1420
after delete: len(clean_dataset) 1380
after delete: len(noisy_dataset) 1756
epoch [82/200] batch [5/43] time 0.459 (0.492) data 0.329 (0.362) loss_x loss_x 0.8188 (1.1020) acc_x 81.2500 (72.5000) lr 1.3090e-03 eta 0:00:18
epoch [82/200] batch [10/43] time 0.406 (0.463) data 0.276 (0.333) loss_x loss_x 1.4502 (1.1916) acc_x 68.7500 (69.0625) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [15/43] time 0.370 (0.464) data 0.240 (0.334) loss_x loss_x 1.6797 (1.2623) acc_x 56.2500 (67.5000) lr 1.3090e-03 eta 0:00:12
epoch [82/200] batch [20/43] time 0.436 (0.456) data 0.306 (0.326) loss_x loss_x 0.8188 (1.2634) acc_x 75.0000 (67.3438) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [25/43] time 0.399 (0.457) data 0.268 (0.327) loss_x loss_x 1.3633 (1.2586) acc_x 71.8750 (68.1250) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [30/43] time 0.376 (0.460) data 0.245 (0.330) loss_x loss_x 1.5312 (1.3034) acc_x 71.8750 (67.1875) lr 1.3090e-03 eta 0:00:05
epoch [82/200] batch [35/43] time 0.404 (0.458) data 0.273 (0.328) loss_x loss_x 1.0430 (1.2892) acc_x 71.8750 (67.8571) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [40/43] time 0.529 (0.455) data 0.398 (0.324) loss_x loss_x 0.9780 (1.2979) acc_x 68.7500 (67.5000) lr 1.3090e-03 eta 0:00:01
epoch [82/200] batch [5/54] time 0.470 (0.455) data 0.338 (0.324) loss_u loss_u 0.7339 (0.8205) acc_u 31.2500 (21.8750) lr 1.3090e-03 eta 0:00:22
epoch [82/200] batch [10/54] time 0.369 (0.450) data 0.238 (0.319) loss_u loss_u 0.7710 (0.8209) acc_u 28.1250 (22.5000) lr 1.3090e-03 eta 0:00:19
epoch [82/200] batch [15/54] time 0.417 (0.446) data 0.285 (0.316) loss_u loss_u 0.9194 (0.8179) acc_u 6.2500 (22.7083) lr 1.3090e-03 eta 0:00:17
epoch [82/200] batch [20/54] time 0.522 (0.446) data 0.389 (0.316) loss_u loss_u 0.8760 (0.8197) acc_u 15.6250 (22.9688) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [25/54] time 0.477 (0.459) data 0.346 (0.328) loss_u loss_u 0.8711 (0.8257) acc_u 15.6250 (21.8750) lr 1.3090e-03 eta 0:00:13
epoch [82/200] batch [30/54] time 0.449 (0.456) data 0.317 (0.325) loss_u loss_u 0.8110 (0.8228) acc_u 21.8750 (22.3958) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [35/54] time 0.444 (0.455) data 0.313 (0.324) loss_u loss_u 0.8335 (0.8210) acc_u 21.8750 (22.3214) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [40/54] time 0.361 (0.453) data 0.229 (0.322) loss_u loss_u 0.9121 (0.8230) acc_u 9.3750 (21.8750) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [45/54] time 0.426 (0.450) data 0.295 (0.319) loss_u loss_u 0.7983 (0.8238) acc_u 25.0000 (21.8750) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [50/54] time 0.542 (0.451) data 0.410 (0.320) loss_u loss_u 0.8877 (0.8232) acc_u 9.3750 (21.9375) lr 1.3090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1500
confident_label rate tensor(0.4283, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1343
clean true:1300
clean false:43
clean_rate:0.9679821295606851
noisy true:336
noisy false:1457
after delete: len(clean_dataset) 1343
after delete: len(noisy_dataset) 1793
epoch [83/200] batch [5/41] time 0.452 (0.424) data 0.321 (0.294) loss_x loss_x 1.2402 (1.1176) acc_x 71.8750 (74.3750) lr 1.2940e-03 eta 0:00:15
epoch [83/200] batch [10/41] time 0.422 (0.451) data 0.291 (0.321) loss_x loss_x 1.0479 (1.1222) acc_x 62.5000 (71.8750) lr 1.2940e-03 eta 0:00:13
epoch [83/200] batch [15/41] time 0.394 (0.443) data 0.263 (0.312) loss_x loss_x 1.1680 (1.1333) acc_x 65.6250 (70.6250) lr 1.2940e-03 eta 0:00:11
epoch [83/200] batch [20/41] time 0.520 (0.458) data 0.389 (0.328) loss_x loss_x 0.9146 (1.1122) acc_x 78.1250 (72.6562) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [25/41] time 0.495 (0.463) data 0.365 (0.332) loss_x loss_x 1.4219 (1.1118) acc_x 65.6250 (72.8750) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [30/41] time 0.418 (0.457) data 0.287 (0.327) loss_x loss_x 1.2627 (1.1410) acc_x 68.7500 (72.3958) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [35/41] time 0.351 (0.448) data 0.221 (0.317) loss_x loss_x 1.6191 (1.1888) acc_x 56.2500 (70.4464) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [40/41] time 0.479 (0.452) data 0.348 (0.321) loss_x loss_x 1.3652 (1.1913) acc_x 59.3750 (70.3125) lr 1.2940e-03 eta 0:00:00
epoch [83/200] batch [5/56] time 0.451 (0.459) data 0.321 (0.328) loss_u loss_u 0.8979 (0.8201) acc_u 9.3750 (21.8750) lr 1.2940e-03 eta 0:00:23
epoch [83/200] batch [10/56] time 0.365 (0.459) data 0.234 (0.329) loss_u loss_u 0.7808 (0.8182) acc_u 34.3750 (22.5000) lr 1.2940e-03 eta 0:00:21
epoch [83/200] batch [15/56] time 0.462 (0.466) data 0.330 (0.335) loss_u loss_u 0.7993 (0.8140) acc_u 25.0000 (23.1250) lr 1.2940e-03 eta 0:00:19
epoch [83/200] batch [20/56] time 0.381 (0.463) data 0.250 (0.332) loss_u loss_u 0.8018 (0.8174) acc_u 21.8750 (22.9688) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [25/56] time 0.537 (0.464) data 0.406 (0.333) loss_u loss_u 0.7822 (0.8169) acc_u 28.1250 (23.2500) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [30/56] time 0.379 (0.462) data 0.247 (0.331) loss_u loss_u 0.7964 (0.8139) acc_u 21.8750 (23.2292) lr 1.2940e-03 eta 0:00:11
epoch [83/200] batch [35/56] time 0.455 (0.461) data 0.324 (0.330) loss_u loss_u 0.7954 (0.8156) acc_u 25.0000 (23.2143) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [40/56] time 0.759 (0.464) data 0.628 (0.333) loss_u loss_u 0.8291 (0.8130) acc_u 25.0000 (23.5156) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [45/56] time 0.513 (0.465) data 0.381 (0.334) loss_u loss_u 0.7993 (0.8162) acc_u 31.2500 (23.1944) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [50/56] time 0.400 (0.467) data 0.269 (0.336) loss_u loss_u 0.9302 (0.8190) acc_u 3.1250 (22.6250) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [55/56] time 0.548 (0.468) data 0.416 (0.337) loss_u loss_u 0.8770 (0.8207) acc_u 18.7500 (22.6136) lr 1.2940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1493
confident_label rate tensor(0.4216, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1322
clean true:1287
clean false:35
clean_rate:0.9735249621785174
noisy true:356
noisy false:1458
after delete: len(clean_dataset) 1322
after delete: len(noisy_dataset) 1814
epoch [84/200] batch [5/41] time 0.450 (0.453) data 0.321 (0.323) loss_x loss_x 1.4346 (1.2579) acc_x 56.2500 (66.2500) lr 1.2790e-03 eta 0:00:16
epoch [84/200] batch [10/41] time 0.383 (0.455) data 0.253 (0.325) loss_x loss_x 1.4922 (1.2165) acc_x 62.5000 (68.4375) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [15/41] time 0.435 (0.471) data 0.305 (0.341) loss_x loss_x 1.1553 (1.2184) acc_x 65.6250 (68.5417) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [20/41] time 0.429 (0.471) data 0.299 (0.341) loss_x loss_x 0.7109 (1.2150) acc_x 81.2500 (68.7500) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [25/41] time 0.597 (0.471) data 0.467 (0.340) loss_x loss_x 1.2412 (1.1925) acc_x 56.2500 (68.2500) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [30/41] time 0.423 (0.466) data 0.293 (0.336) loss_x loss_x 0.7578 (1.1886) acc_x 81.2500 (67.7083) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [35/41] time 0.387 (0.461) data 0.257 (0.331) loss_x loss_x 1.1582 (1.1918) acc_x 65.6250 (68.0357) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [40/41] time 0.453 (0.468) data 0.323 (0.337) loss_x loss_x 1.5420 (1.2131) acc_x 68.7500 (68.1250) lr 1.2790e-03 eta 0:00:00
epoch [84/200] batch [5/56] time 0.389 (0.462) data 0.258 (0.332) loss_u loss_u 0.8149 (0.8107) acc_u 31.2500 (25.6250) lr 1.2790e-03 eta 0:00:23
epoch [84/200] batch [10/56] time 0.410 (0.455) data 0.279 (0.325) loss_u loss_u 0.7441 (0.8191) acc_u 31.2500 (22.8125) lr 1.2790e-03 eta 0:00:20
epoch [84/200] batch [15/56] time 0.694 (0.460) data 0.563 (0.330) loss_u loss_u 0.8564 (0.8161) acc_u 18.7500 (23.3333) lr 1.2790e-03 eta 0:00:18
epoch [84/200] batch [20/56] time 0.482 (0.463) data 0.351 (0.333) loss_u loss_u 0.8267 (0.8146) acc_u 21.8750 (23.9062) lr 1.2790e-03 eta 0:00:16
epoch [84/200] batch [25/56] time 0.502 (0.461) data 0.370 (0.330) loss_u loss_u 0.8809 (0.8221) acc_u 12.5000 (22.6250) lr 1.2790e-03 eta 0:00:14
epoch [84/200] batch [30/56] time 0.496 (0.457) data 0.366 (0.327) loss_u loss_u 0.8730 (0.8156) acc_u 12.5000 (23.5417) lr 1.2790e-03 eta 0:00:11
epoch [84/200] batch [35/56] time 0.371 (0.453) data 0.239 (0.322) loss_u loss_u 0.7568 (0.8202) acc_u 31.2500 (23.0357) lr 1.2790e-03 eta 0:00:09
epoch [84/200] batch [40/56] time 0.422 (0.452) data 0.291 (0.322) loss_u loss_u 0.8027 (0.8178) acc_u 21.8750 (23.4375) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [45/56] time 0.461 (0.450) data 0.329 (0.319) loss_u loss_u 0.8486 (0.8211) acc_u 15.6250 (22.9167) lr 1.2790e-03 eta 0:00:04
epoch [84/200] batch [50/56] time 0.493 (0.452) data 0.363 (0.321) loss_u loss_u 0.7969 (0.8183) acc_u 25.0000 (23.1250) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [55/56] time 0.428 (0.452) data 0.296 (0.321) loss_u loss_u 0.8452 (0.8197) acc_u 12.5000 (22.7841) lr 1.2790e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1474
confident_label rate tensor(0.4334, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1359
clean true:1322
clean false:37
clean_rate:0.9727740986019132
noisy true:340
noisy false:1437
after delete: len(clean_dataset) 1359
after delete: len(noisy_dataset) 1777
epoch [85/200] batch [5/42] time 0.408 (0.489) data 0.278 (0.359) loss_x loss_x 1.0215 (1.0022) acc_x 75.0000 (75.6250) lr 1.2639e-03 eta 0:00:18
epoch [85/200] batch [10/42] time 0.471 (0.476) data 0.340 (0.346) loss_x loss_x 1.1445 (1.1539) acc_x 71.8750 (70.3125) lr 1.2639e-03 eta 0:00:15
epoch [85/200] batch [15/42] time 0.383 (0.474) data 0.253 (0.344) loss_x loss_x 1.4189 (1.1361) acc_x 59.3750 (70.4167) lr 1.2639e-03 eta 0:00:12
epoch [85/200] batch [20/42] time 0.428 (0.458) data 0.298 (0.328) loss_x loss_x 1.3545 (1.1698) acc_x 62.5000 (69.0625) lr 1.2639e-03 eta 0:00:10
epoch [85/200] batch [25/42] time 0.447 (0.461) data 0.317 (0.330) loss_x loss_x 1.0068 (1.1678) acc_x 81.2500 (69.8750) lr 1.2639e-03 eta 0:00:07
epoch [85/200] batch [30/42] time 0.434 (0.456) data 0.303 (0.326) loss_x loss_x 1.3750 (1.1875) acc_x 68.7500 (69.5833) lr 1.2639e-03 eta 0:00:05
epoch [85/200] batch [35/42] time 0.398 (0.450) data 0.267 (0.320) loss_x loss_x 1.3037 (1.1987) acc_x 68.7500 (69.1071) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [40/42] time 0.408 (0.444) data 0.278 (0.313) loss_x loss_x 1.0225 (1.1988) acc_x 81.2500 (69.0625) lr 1.2639e-03 eta 0:00:00
epoch [85/200] batch [5/55] time 0.478 (0.443) data 0.347 (0.313) loss_u loss_u 0.8745 (0.8471) acc_u 9.3750 (18.7500) lr 1.2639e-03 eta 0:00:22
epoch [85/200] batch [10/55] time 0.416 (0.445) data 0.286 (0.315) loss_u loss_u 0.7925 (0.8321) acc_u 28.1250 (22.5000) lr 1.2639e-03 eta 0:00:20
epoch [85/200] batch [15/55] time 0.465 (0.449) data 0.333 (0.319) loss_u loss_u 0.8574 (0.8254) acc_u 18.7500 (23.1250) lr 1.2639e-03 eta 0:00:17
epoch [85/200] batch [20/55] time 0.585 (0.453) data 0.451 (0.323) loss_u loss_u 0.8887 (0.8252) acc_u 15.6250 (23.2812) lr 1.2639e-03 eta 0:00:15
epoch [85/200] batch [25/55] time 0.337 (0.450) data 0.205 (0.320) loss_u loss_u 0.7720 (0.8144) acc_u 25.0000 (23.6250) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [30/55] time 0.325 (0.450) data 0.194 (0.319) loss_u loss_u 0.8408 (0.8123) acc_u 21.8750 (23.9583) lr 1.2639e-03 eta 0:00:11
epoch [85/200] batch [35/55] time 0.408 (0.449) data 0.277 (0.319) loss_u loss_u 0.8394 (0.8172) acc_u 18.7500 (23.1250) lr 1.2639e-03 eta 0:00:08
epoch [85/200] batch [40/55] time 0.478 (0.447) data 0.347 (0.317) loss_u loss_u 0.7651 (0.8140) acc_u 31.2500 (23.5156) lr 1.2639e-03 eta 0:00:06
epoch [85/200] batch [45/55] time 0.439 (0.449) data 0.308 (0.318) loss_u loss_u 0.7891 (0.8164) acc_u 25.0000 (23.0556) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [50/55] time 0.428 (0.450) data 0.297 (0.320) loss_u loss_u 0.7666 (0.8137) acc_u 28.1250 (23.4375) lr 1.2639e-03 eta 0:00:02
epoch [85/200] batch [55/55] time 0.420 (0.450) data 0.288 (0.319) loss_u loss_u 0.8022 (0.8143) acc_u 21.8750 (23.1818) lr 1.2639e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1517
confident_label rate tensor(0.4206, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1319
clean true:1275
clean false:44
clean_rate:0.9666413949962093
noisy true:344
noisy false:1473
after delete: len(clean_dataset) 1319
after delete: len(noisy_dataset) 1817
epoch [86/200] batch [5/41] time 0.492 (0.516) data 0.361 (0.385) loss_x loss_x 0.9062 (1.1253) acc_x 78.1250 (74.3750) lr 1.2487e-03 eta 0:00:18
epoch [86/200] batch [10/41] time 0.488 (0.464) data 0.358 (0.334) loss_x loss_x 1.1855 (1.1577) acc_x 71.8750 (71.5625) lr 1.2487e-03 eta 0:00:14
epoch [86/200] batch [15/41] time 0.458 (0.476) data 0.328 (0.345) loss_x loss_x 1.6113 (1.1421) acc_x 56.2500 (71.2500) lr 1.2487e-03 eta 0:00:12
epoch [86/200] batch [20/41] time 0.486 (0.472) data 0.355 (0.342) loss_x loss_x 1.0439 (1.1059) acc_x 78.1250 (71.8750) lr 1.2487e-03 eta 0:00:09
epoch [86/200] batch [25/41] time 0.373 (0.458) data 0.243 (0.327) loss_x loss_x 1.3281 (1.1110) acc_x 71.8750 (71.8750) lr 1.2487e-03 eta 0:00:07
epoch [86/200] batch [30/41] time 0.351 (0.456) data 0.222 (0.325) loss_x loss_x 1.3418 (1.1833) acc_x 59.3750 (70.2083) lr 1.2487e-03 eta 0:00:05
epoch [86/200] batch [35/41] time 0.411 (0.449) data 0.282 (0.318) loss_x loss_x 1.1094 (1.1915) acc_x 78.1250 (70.0000) lr 1.2487e-03 eta 0:00:02
epoch [86/200] batch [40/41] time 0.503 (0.449) data 0.373 (0.319) loss_x loss_x 1.1611 (1.2052) acc_x 71.8750 (69.7656) lr 1.2487e-03 eta 0:00:00
epoch [86/200] batch [5/56] time 0.384 (0.448) data 0.253 (0.318) loss_u loss_u 0.7388 (0.7977) acc_u 37.5000 (26.2500) lr 1.2487e-03 eta 0:00:22
epoch [86/200] batch [10/56] time 0.396 (0.450) data 0.265 (0.320) loss_u loss_u 0.7969 (0.8244) acc_u 18.7500 (20.0000) lr 1.2487e-03 eta 0:00:20
epoch [86/200] batch [15/56] time 0.507 (0.451) data 0.377 (0.320) loss_u loss_u 0.8198 (0.8199) acc_u 25.0000 (21.6667) lr 1.2487e-03 eta 0:00:18
epoch [86/200] batch [20/56] time 0.618 (0.459) data 0.487 (0.329) loss_u loss_u 0.8379 (0.8149) acc_u 21.8750 (22.6562) lr 1.2487e-03 eta 0:00:16
epoch [86/200] batch [25/56] time 0.478 (0.458) data 0.346 (0.328) loss_u loss_u 0.7559 (0.8054) acc_u 34.3750 (23.8750) lr 1.2487e-03 eta 0:00:14
epoch [86/200] batch [30/56] time 0.581 (0.458) data 0.449 (0.327) loss_u loss_u 0.7681 (0.8053) acc_u 28.1250 (23.8542) lr 1.2487e-03 eta 0:00:11
epoch [86/200] batch [35/56] time 0.377 (0.453) data 0.247 (0.322) loss_u loss_u 0.7764 (0.8022) acc_u 34.3750 (24.3750) lr 1.2487e-03 eta 0:00:09
epoch [86/200] batch [40/56] time 0.439 (0.452) data 0.309 (0.322) loss_u loss_u 0.8169 (0.8048) acc_u 15.6250 (23.9062) lr 1.2487e-03 eta 0:00:07
epoch [86/200] batch [45/56] time 0.556 (0.454) data 0.426 (0.323) loss_u loss_u 0.8569 (0.8077) acc_u 18.7500 (23.5417) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [50/56] time 0.431 (0.455) data 0.300 (0.324) loss_u loss_u 0.7886 (0.8067) acc_u 25.0000 (23.6250) lr 1.2487e-03 eta 0:00:02
epoch [86/200] batch [55/56] time 0.431 (0.452) data 0.300 (0.322) loss_u loss_u 0.7715 (0.8063) acc_u 28.1250 (23.5227) lr 1.2487e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1443
confident_label rate tensor(0.4432, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1390
clean true:1346
clean false:44
clean_rate:0.9683453237410072
noisy true:347
noisy false:1399
after delete: len(clean_dataset) 1390
after delete: len(noisy_dataset) 1746
epoch [87/200] batch [5/43] time 0.455 (0.439) data 0.324 (0.308) loss_x loss_x 1.1689 (0.9402) acc_x 71.8750 (79.3750) lr 1.2334e-03 eta 0:00:16
epoch [87/200] batch [10/43] time 0.455 (0.437) data 0.324 (0.306) loss_x loss_x 1.3301 (1.1492) acc_x 65.6250 (73.4375) lr 1.2334e-03 eta 0:00:14
epoch [87/200] batch [15/43] time 0.498 (0.451) data 0.368 (0.320) loss_x loss_x 0.9937 (1.0817) acc_x 81.2500 (74.3750) lr 1.2334e-03 eta 0:00:12
epoch [87/200] batch [20/43] time 0.447 (0.460) data 0.316 (0.329) loss_x loss_x 1.3721 (1.1203) acc_x 71.8750 (72.3438) lr 1.2334e-03 eta 0:00:10
epoch [87/200] batch [25/43] time 0.497 (0.472) data 0.367 (0.341) loss_x loss_x 1.0723 (1.0988) acc_x 71.8750 (73.0000) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [30/43] time 0.475 (0.470) data 0.346 (0.340) loss_x loss_x 1.2529 (1.1123) acc_x 59.3750 (71.7708) lr 1.2334e-03 eta 0:00:06
epoch [87/200] batch [35/43] time 0.501 (0.471) data 0.370 (0.340) loss_x loss_x 0.9121 (1.1064) acc_x 78.1250 (71.0714) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [40/43] time 0.378 (0.475) data 0.247 (0.344) loss_x loss_x 1.4648 (1.1322) acc_x 71.8750 (70.7031) lr 1.2334e-03 eta 0:00:01
epoch [87/200] batch [5/54] time 0.429 (0.466) data 0.297 (0.335) loss_u loss_u 0.7554 (0.7943) acc_u 31.2500 (25.0000) lr 1.2334e-03 eta 0:00:22
epoch [87/200] batch [10/54] time 0.313 (0.464) data 0.181 (0.333) loss_u loss_u 0.8691 (0.8200) acc_u 9.3750 (20.6250) lr 1.2334e-03 eta 0:00:20
epoch [87/200] batch [15/54] time 0.352 (0.463) data 0.221 (0.332) loss_u loss_u 0.6118 (0.8098) acc_u 40.6250 (21.2500) lr 1.2334e-03 eta 0:00:18
epoch [87/200] batch [20/54] time 0.533 (0.463) data 0.402 (0.332) loss_u loss_u 0.8452 (0.8086) acc_u 21.8750 (22.1875) lr 1.2334e-03 eta 0:00:15
epoch [87/200] batch [25/54] time 0.603 (0.462) data 0.471 (0.331) loss_u loss_u 0.8799 (0.8143) acc_u 15.6250 (21.5000) lr 1.2334e-03 eta 0:00:13
epoch [87/200] batch [30/54] time 0.309 (0.463) data 0.177 (0.332) loss_u loss_u 0.8013 (0.8172) acc_u 31.2500 (21.9792) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [35/54] time 0.417 (0.459) data 0.287 (0.328) loss_u loss_u 0.7798 (0.8166) acc_u 21.8750 (21.8750) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [40/54] time 0.453 (0.458) data 0.322 (0.327) loss_u loss_u 0.7974 (0.8155) acc_u 31.2500 (22.5000) lr 1.2334e-03 eta 0:00:06
epoch [87/200] batch [45/54] time 0.436 (0.458) data 0.306 (0.327) loss_u loss_u 0.7925 (0.8125) acc_u 28.1250 (23.1944) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [50/54] time 0.360 (0.455) data 0.230 (0.324) loss_u loss_u 0.7153 (0.8108) acc_u 37.5000 (23.6250) lr 1.2334e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1449
confident_label rate tensor(0.4381, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1374
clean true:1338
clean false:36
clean_rate:0.9737991266375546
noisy true:349
noisy false:1413
after delete: len(clean_dataset) 1374
after delete: len(noisy_dataset) 1762
epoch [88/200] batch [5/42] time 0.513 (0.444) data 0.383 (0.314) loss_x loss_x 0.6230 (0.9674) acc_x 84.3750 (77.5000) lr 1.2181e-03 eta 0:00:16
epoch [88/200] batch [10/42] time 0.434 (0.444) data 0.302 (0.313) loss_x loss_x 1.4121 (1.0522) acc_x 68.7500 (74.6875) lr 1.2181e-03 eta 0:00:14
epoch [88/200] batch [15/42] time 0.351 (0.440) data 0.220 (0.310) loss_x loss_x 0.9941 (1.1196) acc_x 68.7500 (71.8750) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [20/42] time 0.431 (0.435) data 0.300 (0.304) loss_x loss_x 1.7041 (1.1691) acc_x 68.7500 (71.0938) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [25/42] time 0.537 (0.437) data 0.404 (0.306) loss_x loss_x 1.1289 (1.1892) acc_x 68.7500 (70.7500) lr 1.2181e-03 eta 0:00:07
epoch [88/200] batch [30/42] time 0.478 (0.444) data 0.347 (0.313) loss_x loss_x 1.4102 (1.2015) acc_x 65.6250 (69.8958) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [35/42] time 0.518 (0.449) data 0.388 (0.318) loss_x loss_x 1.3770 (1.2153) acc_x 71.8750 (69.8214) lr 1.2181e-03 eta 0:00:03
epoch [88/200] batch [40/42] time 0.454 (0.449) data 0.324 (0.318) loss_x loss_x 1.2881 (1.2196) acc_x 56.2500 (69.6094) lr 1.2181e-03 eta 0:00:00
epoch [88/200] batch [5/55] time 0.374 (0.444) data 0.243 (0.314) loss_u loss_u 0.8076 (0.8049) acc_u 25.0000 (25.0000) lr 1.2181e-03 eta 0:00:22
epoch [88/200] batch [10/55] time 0.465 (0.445) data 0.334 (0.314) loss_u loss_u 0.7808 (0.7979) acc_u 28.1250 (23.4375) lr 1.2181e-03 eta 0:00:20
epoch [88/200] batch [15/55] time 0.519 (0.445) data 0.389 (0.314) loss_u loss_u 0.7817 (0.8096) acc_u 34.3750 (22.2917) lr 1.2181e-03 eta 0:00:17
epoch [88/200] batch [20/55] time 0.424 (0.452) data 0.293 (0.321) loss_u loss_u 0.8511 (0.8124) acc_u 25.0000 (22.0312) lr 1.2181e-03 eta 0:00:15
epoch [88/200] batch [25/55] time 0.529 (0.453) data 0.398 (0.322) loss_u loss_u 0.8169 (0.8131) acc_u 18.7500 (22.2500) lr 1.2181e-03 eta 0:00:13
epoch [88/200] batch [30/55] time 0.412 (0.450) data 0.280 (0.319) loss_u loss_u 0.7920 (0.8117) acc_u 21.8750 (22.1875) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [35/55] time 0.363 (0.450) data 0.232 (0.319) loss_u loss_u 0.8101 (0.8152) acc_u 18.7500 (21.6964) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [40/55] time 0.483 (0.448) data 0.352 (0.317) loss_u loss_u 0.8501 (0.8158) acc_u 21.8750 (21.4844) lr 1.2181e-03 eta 0:00:06
epoch [88/200] batch [45/55] time 0.459 (0.450) data 0.329 (0.319) loss_u loss_u 0.8047 (0.8144) acc_u 28.1250 (21.8750) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [50/55] time 0.402 (0.447) data 0.270 (0.316) loss_u loss_u 0.8379 (0.8143) acc_u 25.0000 (22.0625) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [55/55] time 0.426 (0.445) data 0.294 (0.314) loss_u loss_u 0.7920 (0.8164) acc_u 21.8750 (21.7045) lr 1.2181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1465
confident_label rate tensor(0.4362, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1368
clean true:1331
clean false:37
clean_rate:0.972953216374269
noisy true:340
noisy false:1428
after delete: len(clean_dataset) 1368
after delete: len(noisy_dataset) 1768
epoch [89/200] batch [5/42] time 0.353 (0.466) data 0.223 (0.335) loss_x loss_x 1.0439 (1.2146) acc_x 71.8750 (73.1250) lr 1.2028e-03 eta 0:00:17
epoch [89/200] batch [10/42] time 0.501 (0.468) data 0.371 (0.337) loss_x loss_x 1.3232 (1.3159) acc_x 56.2500 (67.5000) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [15/42] time 0.420 (0.447) data 0.290 (0.316) loss_x loss_x 1.2734 (1.2772) acc_x 75.0000 (67.0833) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [20/42] time 0.407 (0.444) data 0.277 (0.313) loss_x loss_x 1.4795 (1.2549) acc_x 68.7500 (67.5000) lr 1.2028e-03 eta 0:00:09
epoch [89/200] batch [25/42] time 0.392 (0.442) data 0.261 (0.312) loss_x loss_x 1.3418 (1.2567) acc_x 62.5000 (67.7500) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [30/42] time 0.378 (0.443) data 0.247 (0.312) loss_x loss_x 1.0869 (1.2382) acc_x 71.8750 (67.2917) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [35/42] time 0.401 (0.443) data 0.270 (0.313) loss_x loss_x 1.3809 (1.2381) acc_x 65.6250 (67.5000) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [40/42] time 0.570 (0.445) data 0.439 (0.314) loss_x loss_x 1.4756 (1.2566) acc_x 62.5000 (67.0312) lr 1.2028e-03 eta 0:00:00
epoch [89/200] batch [5/55] time 0.405 (0.447) data 0.273 (0.316) loss_u loss_u 0.8560 (0.8133) acc_u 21.8750 (21.8750) lr 1.2028e-03 eta 0:00:22
epoch [89/200] batch [10/55] time 0.496 (0.446) data 0.365 (0.315) loss_u loss_u 0.7935 (0.8078) acc_u 18.7500 (24.3750) lr 1.2028e-03 eta 0:00:20
epoch [89/200] batch [15/55] time 0.438 (0.442) data 0.307 (0.312) loss_u loss_u 0.8916 (0.8216) acc_u 15.6250 (23.9583) lr 1.2028e-03 eta 0:00:17
epoch [89/200] batch [20/55] time 0.328 (0.442) data 0.197 (0.312) loss_u loss_u 0.7065 (0.8131) acc_u 37.5000 (24.8438) lr 1.2028e-03 eta 0:00:15
epoch [89/200] batch [25/55] time 0.394 (0.439) data 0.264 (0.308) loss_u loss_u 0.8223 (0.8130) acc_u 21.8750 (24.2500) lr 1.2028e-03 eta 0:00:13
epoch [89/200] batch [30/55] time 0.433 (0.441) data 0.302 (0.310) loss_u loss_u 0.8799 (0.8157) acc_u 12.5000 (23.5417) lr 1.2028e-03 eta 0:00:11
epoch [89/200] batch [35/55] time 0.538 (0.447) data 0.406 (0.317) loss_u loss_u 0.8428 (0.8208) acc_u 18.7500 (23.1250) lr 1.2028e-03 eta 0:00:08
epoch [89/200] batch [40/55] time 0.500 (0.450) data 0.368 (0.319) loss_u loss_u 0.7969 (0.8197) acc_u 21.8750 (23.0469) lr 1.2028e-03 eta 0:00:06
epoch [89/200] batch [45/55] time 0.476 (0.449) data 0.344 (0.318) loss_u loss_u 0.7144 (0.8221) acc_u 28.1250 (22.3611) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [50/55] time 0.409 (0.449) data 0.279 (0.318) loss_u loss_u 0.8853 (0.8247) acc_u 18.7500 (22.1250) lr 1.2028e-03 eta 0:00:02
epoch [89/200] batch [55/55] time 0.623 (0.449) data 0.493 (0.318) loss_u loss_u 0.7886 (0.8236) acc_u 25.0000 (22.1023) lr 1.2028e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1462
confident_label rate tensor(0.4356, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1366
clean true:1326
clean false:40
clean_rate:0.9707174231332357
noisy true:348
noisy false:1422
after delete: len(clean_dataset) 1366
after delete: len(noisy_dataset) 1770
epoch [90/200] batch [5/42] time 0.433 (0.464) data 0.303 (0.334) loss_x loss_x 0.9409 (1.0256) acc_x 78.1250 (73.7500) lr 1.1874e-03 eta 0:00:17
epoch [90/200] batch [10/42] time 0.458 (0.487) data 0.327 (0.357) loss_x loss_x 1.0869 (1.1315) acc_x 81.2500 (71.8750) lr 1.1874e-03 eta 0:00:15
epoch [90/200] batch [15/42] time 0.408 (0.490) data 0.278 (0.360) loss_x loss_x 1.6367 (1.1735) acc_x 53.1250 (69.5833) lr 1.1874e-03 eta 0:00:13
epoch [90/200] batch [20/42] time 0.484 (0.492) data 0.353 (0.361) loss_x loss_x 1.1445 (1.1600) acc_x 75.0000 (70.7812) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [25/42] time 0.524 (0.487) data 0.393 (0.356) loss_x loss_x 1.2617 (1.1546) acc_x 62.5000 (70.3750) lr 1.1874e-03 eta 0:00:08
epoch [90/200] batch [30/42] time 0.407 (0.485) data 0.276 (0.354) loss_x loss_x 1.2744 (1.1775) acc_x 65.6250 (70.2083) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [35/42] time 0.488 (0.486) data 0.358 (0.356) loss_x loss_x 0.8623 (1.1859) acc_x 81.2500 (70.2679) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [40/42] time 0.443 (0.481) data 0.313 (0.350) loss_x loss_x 0.8999 (1.1626) acc_x 78.1250 (70.7812) lr 1.1874e-03 eta 0:00:00
epoch [90/200] batch [5/55] time 0.478 (0.475) data 0.348 (0.344) loss_u loss_u 0.8740 (0.8218) acc_u 21.8750 (22.5000) lr 1.1874e-03 eta 0:00:23
epoch [90/200] batch [10/55] time 0.428 (0.469) data 0.297 (0.338) loss_u loss_u 0.8516 (0.8181) acc_u 15.6250 (21.5625) lr 1.1874e-03 eta 0:00:21
epoch [90/200] batch [15/55] time 0.332 (0.465) data 0.202 (0.334) loss_u loss_u 0.8218 (0.8203) acc_u 21.8750 (22.0833) lr 1.1874e-03 eta 0:00:18
epoch [90/200] batch [20/55] time 0.378 (0.461) data 0.248 (0.331) loss_u loss_u 0.8608 (0.8193) acc_u 12.5000 (21.7188) lr 1.1874e-03 eta 0:00:16
epoch [90/200] batch [25/55] time 0.438 (0.459) data 0.307 (0.328) loss_u loss_u 0.9038 (0.8187) acc_u 9.3750 (21.6250) lr 1.1874e-03 eta 0:00:13
epoch [90/200] batch [30/55] time 0.533 (0.456) data 0.401 (0.326) loss_u loss_u 0.8516 (0.8224) acc_u 12.5000 (20.8333) lr 1.1874e-03 eta 0:00:11
epoch [90/200] batch [35/55] time 0.406 (0.459) data 0.275 (0.329) loss_u loss_u 0.8120 (0.8190) acc_u 21.8750 (21.5179) lr 1.1874e-03 eta 0:00:09
epoch [90/200] batch [40/55] time 0.390 (0.462) data 0.259 (0.332) loss_u loss_u 0.8740 (0.8199) acc_u 15.6250 (21.4844) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [45/55] time 0.531 (0.463) data 0.400 (0.332) loss_u loss_u 0.8242 (0.8209) acc_u 21.8750 (21.3194) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [50/55] time 0.374 (0.462) data 0.243 (0.331) loss_u loss_u 0.7656 (0.8205) acc_u 28.1250 (21.3750) lr 1.1874e-03 eta 0:00:02
epoch [90/200] batch [55/55] time 0.473 (0.460) data 0.342 (0.329) loss_u loss_u 0.8276 (0.8212) acc_u 21.8750 (21.3636) lr 1.1874e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1454
confident_label rate tensor(0.4423, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1387
clean true:1346
clean false:41
clean_rate:0.9704397981254506
noisy true:336
noisy false:1413
after delete: len(clean_dataset) 1387
after delete: len(noisy_dataset) 1749
epoch [91/200] batch [5/43] time 0.541 (0.520) data 0.410 (0.389) loss_x loss_x 1.0195 (1.0921) acc_x 71.8750 (70.6250) lr 1.1719e-03 eta 0:00:19
epoch [91/200] batch [10/43] time 0.430 (0.502) data 0.300 (0.371) loss_x loss_x 1.0977 (1.1681) acc_x 78.1250 (71.5625) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [15/43] time 0.515 (0.505) data 0.384 (0.375) loss_x loss_x 1.0449 (1.1764) acc_x 84.3750 (72.9167) lr 1.1719e-03 eta 0:00:14
epoch [91/200] batch [20/43] time 0.466 (0.498) data 0.335 (0.368) loss_x loss_x 1.1787 (1.2031) acc_x 65.6250 (72.0312) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [25/43] time 0.313 (0.488) data 0.182 (0.357) loss_x loss_x 1.1357 (1.2033) acc_x 68.7500 (71.5000) lr 1.1719e-03 eta 0:00:08
epoch [91/200] batch [30/43] time 0.510 (0.485) data 0.379 (0.354) loss_x loss_x 1.4697 (1.2278) acc_x 68.7500 (71.2500) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [35/43] time 0.437 (0.485) data 0.306 (0.354) loss_x loss_x 0.8901 (1.2150) acc_x 75.0000 (71.3393) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [40/43] time 0.439 (0.480) data 0.309 (0.350) loss_x loss_x 0.8833 (1.1997) acc_x 75.0000 (71.4844) lr 1.1719e-03 eta 0:00:01
epoch [91/200] batch [5/54] time 0.758 (0.483) data 0.626 (0.352) loss_u loss_u 0.7891 (0.8326) acc_u 25.0000 (21.8750) lr 1.1719e-03 eta 0:00:23
epoch [91/200] batch [10/54] time 0.392 (0.480) data 0.260 (0.349) loss_u loss_u 0.8330 (0.8094) acc_u 18.7500 (23.7500) lr 1.1719e-03 eta 0:00:21
epoch [91/200] batch [15/54] time 0.381 (0.475) data 0.251 (0.344) loss_u loss_u 0.8228 (0.8071) acc_u 25.0000 (24.1667) lr 1.1719e-03 eta 0:00:18
epoch [91/200] batch [20/54] time 0.380 (0.474) data 0.249 (0.343) loss_u loss_u 0.8047 (0.8119) acc_u 28.1250 (23.9062) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [25/54] time 0.503 (0.472) data 0.371 (0.342) loss_u loss_u 0.8394 (0.8168) acc_u 15.6250 (23.3750) lr 1.1719e-03 eta 0:00:13
epoch [91/200] batch [30/54] time 0.423 (0.471) data 0.292 (0.340) loss_u loss_u 0.8374 (0.8139) acc_u 21.8750 (24.0625) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [35/54] time 0.478 (0.471) data 0.346 (0.340) loss_u loss_u 0.7646 (0.8143) acc_u 28.1250 (23.9286) lr 1.1719e-03 eta 0:00:08
epoch [91/200] batch [40/54] time 0.427 (0.470) data 0.295 (0.339) loss_u loss_u 0.8027 (0.8115) acc_u 28.1250 (24.0625) lr 1.1719e-03 eta 0:00:06
epoch [91/200] batch [45/54] time 0.468 (0.469) data 0.337 (0.338) loss_u loss_u 0.8052 (0.8154) acc_u 25.0000 (23.2639) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [50/54] time 0.475 (0.468) data 0.345 (0.337) loss_u loss_u 0.8916 (0.8191) acc_u 9.3750 (22.6875) lr 1.1719e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1455
confident_label rate tensor(0.4343, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1362
clean true:1331
clean false:31
clean_rate:0.9772393538913363
noisy true:350
noisy false:1424
after delete: len(clean_dataset) 1362
after delete: len(noisy_dataset) 1774
epoch [92/200] batch [5/42] time 0.386 (0.446) data 0.256 (0.316) loss_x loss_x 1.8271 (1.0693) acc_x 50.0000 (71.2500) lr 1.1564e-03 eta 0:00:16
epoch [92/200] batch [10/42] time 0.349 (0.422) data 0.218 (0.292) loss_x loss_x 1.2568 (1.1132) acc_x 75.0000 (71.5625) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [15/42] time 0.458 (0.436) data 0.328 (0.305) loss_x loss_x 0.9570 (1.1034) acc_x 75.0000 (72.0833) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [20/42] time 0.533 (0.441) data 0.402 (0.310) loss_x loss_x 1.6650 (1.1168) acc_x 56.2500 (71.0938) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [25/42] time 0.514 (0.446) data 0.384 (0.316) loss_x loss_x 1.1299 (1.1047) acc_x 62.5000 (70.8750) lr 1.1564e-03 eta 0:00:07
epoch [92/200] batch [30/42] time 0.526 (0.449) data 0.395 (0.319) loss_x loss_x 1.0576 (1.0947) acc_x 71.8750 (71.5625) lr 1.1564e-03 eta 0:00:05
epoch [92/200] batch [35/42] time 0.429 (0.458) data 0.299 (0.327) loss_x loss_x 1.7207 (1.1136) acc_x 59.3750 (71.6071) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [40/42] time 0.439 (0.455) data 0.309 (0.325) loss_x loss_x 1.2764 (1.1369) acc_x 65.6250 (71.5625) lr 1.1564e-03 eta 0:00:00
epoch [92/200] batch [5/55] time 0.522 (0.452) data 0.390 (0.322) loss_u loss_u 0.8579 (0.8174) acc_u 18.7500 (23.7500) lr 1.1564e-03 eta 0:00:22
epoch [92/200] batch [10/55] time 0.541 (0.457) data 0.410 (0.326) loss_u loss_u 0.8325 (0.8116) acc_u 21.8750 (23.7500) lr 1.1564e-03 eta 0:00:20
epoch [92/200] batch [15/55] time 0.423 (0.453) data 0.293 (0.322) loss_u loss_u 0.8062 (0.8146) acc_u 28.1250 (24.1667) lr 1.1564e-03 eta 0:00:18
epoch [92/200] batch [20/55] time 0.368 (0.451) data 0.236 (0.321) loss_u loss_u 0.8042 (0.8105) acc_u 21.8750 (24.2188) lr 1.1564e-03 eta 0:00:15
epoch [92/200] batch [25/55] time 0.536 (0.451) data 0.406 (0.321) loss_u loss_u 0.8315 (0.8093) acc_u 21.8750 (24.1250) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [30/55] time 0.425 (0.451) data 0.294 (0.320) loss_u loss_u 0.8647 (0.8122) acc_u 18.7500 (23.9583) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [35/55] time 0.517 (0.450) data 0.386 (0.320) loss_u loss_u 0.8340 (0.8123) acc_u 21.8750 (24.0179) lr 1.1564e-03 eta 0:00:09
epoch [92/200] batch [40/55] time 0.484 (0.451) data 0.353 (0.320) loss_u loss_u 0.7959 (0.8130) acc_u 25.0000 (23.6719) lr 1.1564e-03 eta 0:00:06
epoch [92/200] batch [45/55] time 0.328 (0.455) data 0.196 (0.324) loss_u loss_u 0.8159 (0.8133) acc_u 21.8750 (23.4722) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [50/55] time 0.413 (0.451) data 0.281 (0.320) loss_u loss_u 0.8164 (0.8150) acc_u 21.8750 (23.1875) lr 1.1564e-03 eta 0:00:02
epoch [92/200] batch [55/55] time 0.391 (0.451) data 0.261 (0.320) loss_u loss_u 0.7905 (0.8178) acc_u 25.0000 (22.7841) lr 1.1564e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1476
confident_label rate tensor(0.4346, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1363
clean true:1323
clean false:40
clean_rate:0.9706529713866471
noisy true:337
noisy false:1436
after delete: len(clean_dataset) 1363
after delete: len(noisy_dataset) 1773
epoch [93/200] batch [5/42] time 0.422 (0.437) data 0.290 (0.306) loss_x loss_x 1.3945 (1.2681) acc_x 75.0000 (69.3750) lr 1.1409e-03 eta 0:00:16
epoch [93/200] batch [10/42] time 0.407 (0.436) data 0.277 (0.305) loss_x loss_x 1.5928 (1.3364) acc_x 56.2500 (67.8125) lr 1.1409e-03 eta 0:00:13
epoch [93/200] batch [15/42] time 0.416 (0.435) data 0.285 (0.304) loss_x loss_x 1.4121 (1.3157) acc_x 62.5000 (66.8750) lr 1.1409e-03 eta 0:00:11
epoch [93/200] batch [20/42] time 0.542 (0.450) data 0.412 (0.320) loss_x loss_x 1.2939 (1.3103) acc_x 62.5000 (66.7188) lr 1.1409e-03 eta 0:00:09
epoch [93/200] batch [25/42] time 0.574 (0.461) data 0.443 (0.330) loss_x loss_x 1.4854 (1.2800) acc_x 68.7500 (67.8750) lr 1.1409e-03 eta 0:00:07
epoch [93/200] batch [30/42] time 0.528 (0.457) data 0.397 (0.326) loss_x loss_x 1.3799 (1.2750) acc_x 68.7500 (68.3333) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [35/42] time 0.483 (0.457) data 0.352 (0.326) loss_x loss_x 1.2852 (1.2798) acc_x 62.5000 (68.3929) lr 1.1409e-03 eta 0:00:03
epoch [93/200] batch [40/42] time 0.441 (0.460) data 0.310 (0.329) loss_x loss_x 0.8760 (1.2497) acc_x 71.8750 (68.6719) lr 1.1409e-03 eta 0:00:00
epoch [93/200] batch [5/55] time 0.436 (0.456) data 0.305 (0.326) loss_u loss_u 0.8291 (0.7845) acc_u 18.7500 (26.2500) lr 1.1409e-03 eta 0:00:22
epoch [93/200] batch [10/55] time 0.556 (0.455) data 0.424 (0.324) loss_u loss_u 0.8013 (0.7886) acc_u 25.0000 (26.5625) lr 1.1409e-03 eta 0:00:20
epoch [93/200] batch [15/55] time 0.733 (0.455) data 0.602 (0.325) loss_u loss_u 0.8618 (0.7974) acc_u 15.6250 (25.0000) lr 1.1409e-03 eta 0:00:18
epoch [93/200] batch [20/55] time 0.407 (0.453) data 0.275 (0.322) loss_u loss_u 0.7949 (0.8037) acc_u 21.8750 (24.5312) lr 1.1409e-03 eta 0:00:15
epoch [93/200] batch [25/55] time 0.618 (0.456) data 0.487 (0.326) loss_u loss_u 0.8379 (0.8023) acc_u 21.8750 (24.7500) lr 1.1409e-03 eta 0:00:13
epoch [93/200] batch [30/55] time 0.414 (0.461) data 0.283 (0.331) loss_u loss_u 0.8701 (0.8105) acc_u 15.6250 (23.4375) lr 1.1409e-03 eta 0:00:11
epoch [93/200] batch [35/55] time 0.556 (0.461) data 0.425 (0.331) loss_u loss_u 0.7607 (0.8111) acc_u 34.3750 (23.3929) lr 1.1409e-03 eta 0:00:09
epoch [93/200] batch [40/55] time 0.514 (0.466) data 0.380 (0.335) loss_u loss_u 0.8428 (0.8115) acc_u 25.0000 (23.3594) lr 1.1409e-03 eta 0:00:06
epoch [93/200] batch [45/55] time 0.482 (0.469) data 0.349 (0.338) loss_u loss_u 0.7041 (0.8113) acc_u 34.3750 (23.4028) lr 1.1409e-03 eta 0:00:04
epoch [93/200] batch [50/55] time 0.368 (0.467) data 0.237 (0.336) loss_u loss_u 0.7900 (0.8105) acc_u 31.2500 (23.4375) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [55/55] time 0.445 (0.467) data 0.315 (0.336) loss_u loss_u 0.8540 (0.8122) acc_u 21.8750 (23.4659) lr 1.1409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1433
confident_label rate tensor(0.4471, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1402
clean true:1360
clean false:42
clean_rate:0.9700427960057061
noisy true:343
noisy false:1391
after delete: len(clean_dataset) 1402
after delete: len(noisy_dataset) 1734
epoch [94/200] batch [5/43] time 0.349 (0.467) data 0.219 (0.336) loss_x loss_x 0.8848 (1.2209) acc_x 71.8750 (70.6250) lr 1.1253e-03 eta 0:00:17
epoch [94/200] batch [10/43] time 0.556 (0.477) data 0.425 (0.347) loss_x loss_x 2.1504 (1.2914) acc_x 56.2500 (68.7500) lr 1.1253e-03 eta 0:00:15
epoch [94/200] batch [15/43] time 0.519 (0.473) data 0.388 (0.342) loss_x loss_x 1.7129 (1.3413) acc_x 65.6250 (69.5833) lr 1.1253e-03 eta 0:00:13
epoch [94/200] batch [20/43] time 0.511 (0.475) data 0.380 (0.345) loss_x loss_x 1.5752 (1.3508) acc_x 59.3750 (68.7500) lr 1.1253e-03 eta 0:00:10
epoch [94/200] batch [25/43] time 0.432 (0.465) data 0.301 (0.335) loss_x loss_x 1.1689 (1.3048) acc_x 71.8750 (69.2500) lr 1.1253e-03 eta 0:00:08
epoch [94/200] batch [30/43] time 0.450 (0.460) data 0.320 (0.330) loss_x loss_x 0.8022 (1.2530) acc_x 75.0000 (69.7917) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [35/43] time 0.376 (0.462) data 0.246 (0.332) loss_x loss_x 1.7451 (1.2743) acc_x 46.8750 (68.8393) lr 1.1253e-03 eta 0:00:03
epoch [94/200] batch [40/43] time 0.452 (0.462) data 0.322 (0.332) loss_x loss_x 0.9297 (1.2758) acc_x 81.2500 (69.0625) lr 1.1253e-03 eta 0:00:01
epoch [94/200] batch [5/54] time 0.388 (0.463) data 0.257 (0.333) loss_u loss_u 0.8701 (0.8160) acc_u 18.7500 (21.8750) lr 1.1253e-03 eta 0:00:22
epoch [94/200] batch [10/54] time 0.346 (0.458) data 0.215 (0.327) loss_u loss_u 0.8579 (0.8251) acc_u 15.6250 (21.2500) lr 1.1253e-03 eta 0:00:20
epoch [94/200] batch [15/54] time 0.427 (0.459) data 0.296 (0.328) loss_u loss_u 0.7622 (0.8145) acc_u 34.3750 (23.1250) lr 1.1253e-03 eta 0:00:17
epoch [94/200] batch [20/54] time 0.445 (0.463) data 0.314 (0.332) loss_u loss_u 0.7915 (0.8177) acc_u 28.1250 (23.5938) lr 1.1253e-03 eta 0:00:15
epoch [94/200] batch [25/54] time 0.400 (0.462) data 0.270 (0.331) loss_u loss_u 0.8423 (0.8193) acc_u 18.7500 (23.3750) lr 1.1253e-03 eta 0:00:13
epoch [94/200] batch [30/54] time 0.367 (0.459) data 0.236 (0.328) loss_u loss_u 0.8843 (0.8207) acc_u 21.8750 (23.3333) lr 1.1253e-03 eta 0:00:11
epoch [94/200] batch [35/54] time 0.386 (0.456) data 0.254 (0.325) loss_u loss_u 0.7832 (0.8217) acc_u 28.1250 (22.8571) lr 1.1253e-03 eta 0:00:08
epoch [94/200] batch [40/54] time 0.580 (0.455) data 0.449 (0.325) loss_u loss_u 0.8979 (0.8278) acc_u 12.5000 (21.9531) lr 1.1253e-03 eta 0:00:06
epoch [94/200] batch [45/54] time 0.447 (0.455) data 0.316 (0.324) loss_u loss_u 0.8062 (0.8251) acc_u 25.0000 (22.2917) lr 1.1253e-03 eta 0:00:04
epoch [94/200] batch [50/54] time 0.447 (0.454) data 0.316 (0.323) loss_u loss_u 0.7993 (0.8232) acc_u 28.1250 (22.5000) lr 1.1253e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1447
confident_label rate tensor(0.4394, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1378
clean true:1343
clean false:35
clean_rate:0.974600870827286
noisy true:346
noisy false:1412
after delete: len(clean_dataset) 1378
after delete: len(noisy_dataset) 1758
epoch [95/200] batch [5/43] time 0.405 (0.453) data 0.273 (0.322) loss_x loss_x 0.6680 (1.0577) acc_x 90.6250 (70.0000) lr 1.1097e-03 eta 0:00:17
epoch [95/200] batch [10/43] time 0.406 (0.444) data 0.275 (0.313) loss_x loss_x 1.2959 (1.1814) acc_x 71.8750 (68.7500) lr 1.1097e-03 eta 0:00:14
epoch [95/200] batch [15/43] time 0.480 (0.447) data 0.349 (0.317) loss_x loss_x 1.4443 (1.1927) acc_x 59.3750 (68.5417) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [20/43] time 0.400 (0.440) data 0.270 (0.310) loss_x loss_x 1.1104 (1.2022) acc_x 68.7500 (67.9688) lr 1.1097e-03 eta 0:00:10
epoch [95/200] batch [25/43] time 0.443 (0.443) data 0.313 (0.313) loss_x loss_x 1.5117 (1.2397) acc_x 53.1250 (67.5000) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [30/43] time 0.360 (0.439) data 0.230 (0.309) loss_x loss_x 1.7070 (1.2785) acc_x 59.3750 (66.9792) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [35/43] time 0.358 (0.443) data 0.227 (0.313) loss_x loss_x 0.7993 (1.2437) acc_x 75.0000 (67.7679) lr 1.1097e-03 eta 0:00:03
epoch [95/200] batch [40/43] time 0.519 (0.450) data 0.388 (0.319) loss_x loss_x 1.5195 (1.2490) acc_x 59.3750 (67.6562) lr 1.1097e-03 eta 0:00:01
epoch [95/200] batch [5/54] time 0.449 (0.448) data 0.317 (0.317) loss_u loss_u 0.7109 (0.8006) acc_u 40.6250 (25.0000) lr 1.1097e-03 eta 0:00:21
epoch [95/200] batch [10/54] time 0.533 (0.450) data 0.401 (0.319) loss_u loss_u 0.6929 (0.8143) acc_u 37.5000 (23.1250) lr 1.1097e-03 eta 0:00:19
epoch [95/200] batch [15/54] time 0.373 (0.450) data 0.241 (0.319) loss_u loss_u 0.8447 (0.8175) acc_u 25.0000 (23.3333) lr 1.1097e-03 eta 0:00:17
epoch [95/200] batch [20/54] time 0.443 (0.447) data 0.310 (0.316) loss_u loss_u 0.7705 (0.8145) acc_u 28.1250 (23.2812) lr 1.1097e-03 eta 0:00:15
epoch [95/200] batch [25/54] time 0.353 (0.450) data 0.221 (0.319) loss_u loss_u 0.8638 (0.8190) acc_u 21.8750 (22.8750) lr 1.1097e-03 eta 0:00:13
epoch [95/200] batch [30/54] time 0.418 (0.448) data 0.287 (0.317) loss_u loss_u 0.8794 (0.8227) acc_u 18.7500 (22.1875) lr 1.1097e-03 eta 0:00:10
epoch [95/200] batch [35/54] time 0.446 (0.451) data 0.315 (0.320) loss_u loss_u 0.7910 (0.8206) acc_u 28.1250 (22.4107) lr 1.1097e-03 eta 0:00:08
epoch [95/200] batch [40/54] time 0.435 (0.452) data 0.303 (0.321) loss_u loss_u 0.7725 (0.8187) acc_u 28.1250 (22.7344) lr 1.1097e-03 eta 0:00:06
epoch [95/200] batch [45/54] time 0.482 (0.452) data 0.350 (0.321) loss_u loss_u 0.7104 (0.8189) acc_u 34.3750 (22.3611) lr 1.1097e-03 eta 0:00:04
epoch [95/200] batch [50/54] time 0.393 (0.451) data 0.262 (0.320) loss_u loss_u 0.7993 (0.8226) acc_u 31.2500 (22.0625) lr 1.1097e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1444
confident_label rate tensor(0.4375, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1372
clean true:1331
clean false:41
clean_rate:0.9701166180758017
noisy true:361
noisy false:1403
after delete: len(clean_dataset) 1372
after delete: len(noisy_dataset) 1764
epoch [96/200] batch [5/42] time 0.525 (0.459) data 0.395 (0.328) loss_x loss_x 1.3125 (1.2174) acc_x 62.5000 (71.8750) lr 1.0941e-03 eta 0:00:16
epoch [96/200] batch [10/42] time 0.438 (0.443) data 0.308 (0.313) loss_x loss_x 1.0645 (1.2356) acc_x 75.0000 (71.8750) lr 1.0941e-03 eta 0:00:14
epoch [96/200] batch [15/42] time 0.502 (0.448) data 0.371 (0.317) loss_x loss_x 1.3350 (1.2022) acc_x 78.1250 (72.9167) lr 1.0941e-03 eta 0:00:12
epoch [96/200] batch [20/42] time 0.414 (0.445) data 0.283 (0.314) loss_x loss_x 1.0215 (1.1845) acc_x 75.0000 (72.3438) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [25/42] time 0.427 (0.453) data 0.297 (0.323) loss_x loss_x 1.1895 (1.2587) acc_x 65.6250 (70.3750) lr 1.0941e-03 eta 0:00:07
epoch [96/200] batch [30/42] time 0.426 (0.451) data 0.295 (0.320) loss_x loss_x 1.5322 (1.2892) acc_x 56.2500 (69.3750) lr 1.0941e-03 eta 0:00:05
epoch [96/200] batch [35/42] time 0.444 (0.448) data 0.313 (0.317) loss_x loss_x 1.2959 (1.2650) acc_x 71.8750 (69.2857) lr 1.0941e-03 eta 0:00:03
epoch [96/200] batch [40/42] time 0.512 (0.456) data 0.382 (0.325) loss_x loss_x 0.8848 (1.2625) acc_x 78.1250 (68.7500) lr 1.0941e-03 eta 0:00:00
epoch [96/200] batch [5/55] time 0.548 (0.456) data 0.417 (0.326) loss_u loss_u 0.7246 (0.7816) acc_u 31.2500 (24.3750) lr 1.0941e-03 eta 0:00:22
epoch [96/200] batch [10/55] time 0.467 (0.455) data 0.336 (0.324) loss_u loss_u 0.7148 (0.7731) acc_u 31.2500 (26.5625) lr 1.0941e-03 eta 0:00:20
epoch [96/200] batch [15/55] time 0.398 (0.455) data 0.265 (0.324) loss_u loss_u 0.8750 (0.7832) acc_u 15.6250 (26.0417) lr 1.0941e-03 eta 0:00:18
epoch [96/200] batch [20/55] time 0.352 (0.464) data 0.220 (0.333) loss_u loss_u 0.8203 (0.7982) acc_u 18.7500 (24.0625) lr 1.0941e-03 eta 0:00:16
epoch [96/200] batch [25/55] time 0.367 (0.461) data 0.235 (0.329) loss_u loss_u 0.8433 (0.8083) acc_u 15.6250 (23.0000) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [30/55] time 0.412 (0.457) data 0.281 (0.326) loss_u loss_u 0.8174 (0.8141) acc_u 28.1250 (22.7083) lr 1.0941e-03 eta 0:00:11
epoch [96/200] batch [35/55] time 0.408 (0.458) data 0.276 (0.327) loss_u loss_u 0.8286 (0.8181) acc_u 25.0000 (22.1429) lr 1.0941e-03 eta 0:00:09
epoch [96/200] batch [40/55] time 0.348 (0.455) data 0.216 (0.324) loss_u loss_u 0.8184 (0.8165) acc_u 18.7500 (22.4219) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [45/55] time 0.428 (0.456) data 0.296 (0.324) loss_u loss_u 0.8501 (0.8186) acc_u 21.8750 (22.3611) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [50/55] time 0.471 (0.455) data 0.339 (0.324) loss_u loss_u 0.8154 (0.8163) acc_u 25.0000 (22.8750) lr 1.0941e-03 eta 0:00:02
epoch [96/200] batch [55/55] time 0.374 (0.452) data 0.242 (0.321) loss_u loss_u 0.7734 (0.8154) acc_u 34.3750 (23.0682) lr 1.0941e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1451
confident_label rate tensor(0.4397, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1379
clean true:1334
clean false:45
clean_rate:0.9673676577229877
noisy true:351
noisy false:1406
after delete: len(clean_dataset) 1379
after delete: len(noisy_dataset) 1757
epoch [97/200] batch [5/43] time 0.524 (0.496) data 0.393 (0.366) loss_x loss_x 1.1631 (1.2177) acc_x 78.1250 (74.3750) lr 1.0785e-03 eta 0:00:18
epoch [97/200] batch [10/43] time 0.372 (0.477) data 0.242 (0.347) loss_x loss_x 1.2168 (1.1287) acc_x 68.7500 (75.0000) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [15/43] time 0.452 (0.462) data 0.322 (0.332) loss_x loss_x 1.0967 (1.0859) acc_x 75.0000 (75.4167) lr 1.0785e-03 eta 0:00:12
epoch [97/200] batch [20/43] time 0.408 (0.453) data 0.277 (0.322) loss_x loss_x 1.3594 (1.1175) acc_x 68.7500 (74.3750) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [25/43] time 0.507 (0.459) data 0.377 (0.328) loss_x loss_x 1.0967 (1.1510) acc_x 65.6250 (72.8750) lr 1.0785e-03 eta 0:00:08
epoch [97/200] batch [30/43] time 0.430 (0.451) data 0.300 (0.320) loss_x loss_x 0.9204 (1.1875) acc_x 75.0000 (71.7708) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [35/43] time 0.380 (0.447) data 0.251 (0.317) loss_x loss_x 1.6289 (1.2211) acc_x 65.6250 (71.0714) lr 1.0785e-03 eta 0:00:03
epoch [97/200] batch [40/43] time 0.420 (0.446) data 0.289 (0.316) loss_x loss_x 0.5322 (1.2083) acc_x 84.3750 (71.2500) lr 1.0785e-03 eta 0:00:01
epoch [97/200] batch [5/54] time 0.476 (0.452) data 0.345 (0.322) loss_u loss_u 0.7329 (0.7697) acc_u 34.3750 (29.3750) lr 1.0785e-03 eta 0:00:22
epoch [97/200] batch [10/54] time 0.430 (0.454) data 0.296 (0.324) loss_u loss_u 0.7905 (0.7852) acc_u 25.0000 (26.5625) lr 1.0785e-03 eta 0:00:19
epoch [97/200] batch [15/54] time 0.497 (0.454) data 0.366 (0.324) loss_u loss_u 0.7559 (0.7843) acc_u 25.0000 (26.8750) lr 1.0785e-03 eta 0:00:17
epoch [97/200] batch [20/54] time 0.364 (0.452) data 0.232 (0.321) loss_u loss_u 0.8525 (0.7952) acc_u 15.6250 (25.9375) lr 1.0785e-03 eta 0:00:15
epoch [97/200] batch [25/54] time 0.449 (0.452) data 0.317 (0.321) loss_u loss_u 0.7563 (0.7996) acc_u 21.8750 (25.3750) lr 1.0785e-03 eta 0:00:13
epoch [97/200] batch [30/54] time 0.432 (0.449) data 0.300 (0.318) loss_u loss_u 0.7930 (0.7946) acc_u 31.2500 (25.7292) lr 1.0785e-03 eta 0:00:10
epoch [97/200] batch [35/54] time 0.534 (0.451) data 0.402 (0.320) loss_u loss_u 0.8096 (0.7980) acc_u 28.1250 (25.9821) lr 1.0785e-03 eta 0:00:08
epoch [97/200] batch [40/54] time 0.590 (0.452) data 0.459 (0.321) loss_u loss_u 0.8198 (0.8011) acc_u 18.7500 (25.4688) lr 1.0785e-03 eta 0:00:06
epoch [97/200] batch [45/54] time 0.353 (0.451) data 0.221 (0.320) loss_u loss_u 0.7798 (0.8017) acc_u 25.0000 (25.2083) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [50/54] time 0.385 (0.448) data 0.253 (0.317) loss_u loss_u 0.8599 (0.8075) acc_u 15.6250 (24.1875) lr 1.0785e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1451
confident_label rate tensor(0.4420, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1386
clean true:1343
clean false:43
clean_rate:0.9689754689754689
noisy true:342
noisy false:1408
after delete: len(clean_dataset) 1386
after delete: len(noisy_dataset) 1750
epoch [98/200] batch [5/43] time 0.498 (0.411) data 0.367 (0.281) loss_x loss_x 1.1924 (1.2215) acc_x 71.8750 (70.0000) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [10/43] time 0.513 (0.423) data 0.383 (0.293) loss_x loss_x 0.9390 (1.2742) acc_x 68.7500 (68.4375) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [15/43] time 0.618 (0.450) data 0.488 (0.320) loss_x loss_x 1.2080 (1.3349) acc_x 71.8750 (67.9167) lr 1.0628e-03 eta 0:00:12
epoch [98/200] batch [20/43] time 0.482 (0.460) data 0.351 (0.330) loss_x loss_x 0.8291 (1.2837) acc_x 78.1250 (69.0625) lr 1.0628e-03 eta 0:00:10
epoch [98/200] batch [25/43] time 0.476 (0.476) data 0.345 (0.345) loss_x loss_x 0.7114 (1.2668) acc_x 84.3750 (69.0000) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [30/43] time 0.426 (0.468) data 0.295 (0.338) loss_x loss_x 1.1777 (1.2827) acc_x 71.8750 (68.6458) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [35/43] time 0.397 (0.461) data 0.267 (0.331) loss_x loss_x 0.7402 (1.2593) acc_x 84.3750 (68.7500) lr 1.0628e-03 eta 0:00:03
epoch [98/200] batch [40/43] time 0.426 (0.458) data 0.295 (0.328) loss_x loss_x 1.1816 (1.2645) acc_x 71.8750 (68.7500) lr 1.0628e-03 eta 0:00:01
epoch [98/200] batch [5/54] time 0.500 (0.466) data 0.369 (0.335) loss_u loss_u 0.7529 (0.7878) acc_u 28.1250 (28.1250) lr 1.0628e-03 eta 0:00:22
epoch [98/200] batch [10/54] time 0.473 (0.460) data 0.341 (0.330) loss_u loss_u 0.8403 (0.8118) acc_u 18.7500 (24.0625) lr 1.0628e-03 eta 0:00:20
epoch [98/200] batch [15/54] time 0.444 (0.461) data 0.313 (0.330) loss_u loss_u 0.8237 (0.8119) acc_u 21.8750 (24.3750) lr 1.0628e-03 eta 0:00:17
epoch [98/200] batch [20/54] time 0.416 (0.466) data 0.285 (0.336) loss_u loss_u 0.7915 (0.8140) acc_u 31.2500 (24.3750) lr 1.0628e-03 eta 0:00:15
epoch [98/200] batch [25/54] time 0.480 (0.466) data 0.349 (0.335) loss_u loss_u 0.7720 (0.8152) acc_u 28.1250 (24.0000) lr 1.0628e-03 eta 0:00:13
epoch [98/200] batch [30/54] time 0.469 (0.463) data 0.337 (0.332) loss_u loss_u 0.9219 (0.8191) acc_u 12.5000 (23.8542) lr 1.0628e-03 eta 0:00:11
epoch [98/200] batch [35/54] time 0.644 (0.463) data 0.512 (0.332) loss_u loss_u 0.8511 (0.8183) acc_u 18.7500 (23.9286) lr 1.0628e-03 eta 0:00:08
epoch [98/200] batch [40/54] time 0.528 (0.466) data 0.397 (0.335) loss_u loss_u 0.8677 (0.8220) acc_u 18.7500 (23.4375) lr 1.0628e-03 eta 0:00:06
epoch [98/200] batch [45/54] time 0.393 (0.468) data 0.262 (0.338) loss_u loss_u 0.8140 (0.8188) acc_u 25.0000 (23.4028) lr 1.0628e-03 eta 0:00:04
epoch [98/200] batch [50/54] time 0.527 (0.469) data 0.396 (0.338) loss_u loss_u 0.8574 (0.8198) acc_u 18.7500 (23.1250) lr 1.0628e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1479
confident_label rate tensor(0.4286, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1344
clean true:1304
clean false:40
clean_rate:0.9702380952380952
noisy true:353
noisy false:1439
after delete: len(clean_dataset) 1344
after delete: len(noisy_dataset) 1792
epoch [99/200] batch [5/42] time 0.488 (0.481) data 0.357 (0.350) loss_x loss_x 1.0059 (1.1800) acc_x 75.0000 (71.2500) lr 1.0471e-03 eta 0:00:17
epoch [99/200] batch [10/42] time 0.527 (0.475) data 0.396 (0.344) loss_x loss_x 0.9233 (1.1075) acc_x 71.8750 (71.5625) lr 1.0471e-03 eta 0:00:15
epoch [99/200] batch [15/42] time 0.458 (0.476) data 0.327 (0.345) loss_x loss_x 1.2002 (1.1688) acc_x 62.5000 (68.9583) lr 1.0471e-03 eta 0:00:12
epoch [99/200] batch [20/42] time 0.367 (0.463) data 0.236 (0.332) loss_x loss_x 0.9849 (1.1564) acc_x 71.8750 (69.3750) lr 1.0471e-03 eta 0:00:10
epoch [99/200] batch [25/42] time 0.395 (0.461) data 0.263 (0.330) loss_x loss_x 1.0088 (1.1016) acc_x 75.0000 (71.1250) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [30/42] time 0.461 (0.456) data 0.330 (0.325) loss_x loss_x 1.2314 (1.1276) acc_x 68.7500 (70.7292) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [35/42] time 0.464 (0.457) data 0.333 (0.326) loss_x loss_x 1.2236 (1.1305) acc_x 68.7500 (70.8036) lr 1.0471e-03 eta 0:00:03
epoch [99/200] batch [40/42] time 0.426 (0.455) data 0.295 (0.325) loss_x loss_x 1.2705 (1.1422) acc_x 59.3750 (70.0781) lr 1.0471e-03 eta 0:00:00
epoch [99/200] batch [5/56] time 0.408 (0.447) data 0.276 (0.316) loss_u loss_u 0.8833 (0.8036) acc_u 18.7500 (25.0000) lr 1.0471e-03 eta 0:00:22
epoch [99/200] batch [10/56] time 0.461 (0.447) data 0.330 (0.316) loss_u loss_u 0.8496 (0.7976) acc_u 18.7500 (26.2500) lr 1.0471e-03 eta 0:00:20
epoch [99/200] batch [15/56] time 0.436 (0.447) data 0.305 (0.316) loss_u loss_u 0.8813 (0.8052) acc_u 15.6250 (25.4167) lr 1.0471e-03 eta 0:00:18
epoch [99/200] batch [20/56] time 0.714 (0.451) data 0.583 (0.320) loss_u loss_u 0.8169 (0.8105) acc_u 18.7500 (24.3750) lr 1.0471e-03 eta 0:00:16
epoch [99/200] batch [25/56] time 0.370 (0.454) data 0.239 (0.323) loss_u loss_u 0.7886 (0.8081) acc_u 34.3750 (25.2500) lr 1.0471e-03 eta 0:00:14
epoch [99/200] batch [30/56] time 0.506 (0.452) data 0.374 (0.321) loss_u loss_u 0.9136 (0.8134) acc_u 3.1250 (23.6458) lr 1.0471e-03 eta 0:00:11
epoch [99/200] batch [35/56] time 0.503 (0.451) data 0.371 (0.319) loss_u loss_u 0.8120 (0.8116) acc_u 21.8750 (24.0179) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [40/56] time 0.534 (0.451) data 0.404 (0.320) loss_u loss_u 0.7998 (0.8116) acc_u 25.0000 (23.9844) lr 1.0471e-03 eta 0:00:07
epoch [99/200] batch [45/56] time 0.416 (0.452) data 0.284 (0.321) loss_u loss_u 0.8555 (0.8097) acc_u 18.7500 (24.1667) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [50/56] time 0.376 (0.452) data 0.246 (0.321) loss_u loss_u 0.7021 (0.8090) acc_u 37.5000 (24.4375) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [55/56] time 0.447 (0.451) data 0.315 (0.320) loss_u loss_u 0.8584 (0.8069) acc_u 15.6250 (24.5455) lr 1.0471e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1444
confident_label rate tensor(0.4445, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1394
clean true:1352
clean false:42
clean_rate:0.96987087517934
noisy true:340
noisy false:1402
all clean rate:  [0.9470198675496688, 0.9780361757105943, 0.9673267326732673, 0.9584178498985801, 0.9721946375372393, 0.9632139399806389, 0.9643201542912246, 0.96718322698268, 0.9688385269121813, 0.9795158286778398, 0.9701213818860878, 0.9747899159663865, 0.970334928229665, 0.9751838235294118, 0.9706144256455922, 0.9719710669077758, 0.9774164408310749, 0.9674295774647887, 0.9702537182852143, 0.9627659574468085, 0.9690189328743546, 0.9723154362416108, 0.9704391891891891, 0.9735269000853971, 0.9644670050761421, 0.9740034662045061, 0.9753492193919474, 0.9701619778346121, 0.9712837837837838, 0.9720812182741116, 0.9710264900662252, 0.9774247491638796, 0.9732217573221758, 0.9753086419753086, 0.9774738535800482, 0.9704433497536946, 0.9668552950687146, 0.9703947368421053, 0.9722001588562351, 0.9721780604133545, 0.9768185451638689, 0.9722222222222222, 0.9768740031897927, 0.975669099756691, 0.9789719626168224, 0.9726348709929633, 0.9746835443037974, 0.9690230341540905, 0.9780564263322884, 0.97544128933231, 0.9673995451099318, 0.9745566692367, 0.9673405909797823, 0.9674670797831139, 0.9714946070878274, 0.9778456837280367, 0.9723926380368099, 0.9705197827773467, 0.9645015105740181, 0.9703422053231939, 0.9719058466211086, 0.9714726291441789, 0.9716475095785441, 0.9805144193296961, 0.9676175790285274, 0.9691833590138675, 0.9689578713968958, 0.9702743902439024, 0.9715994020926756, 0.9717261904761905, 0.9774096385542169, 0.97544128933231, 0.9688686408504176, 0.9762081784386617, 0.9699474079639369, 0.9710806697108066, 0.9701492537313433, 0.9647147147147147, 0.9669172932330827, 0.972263868065967, 0.9720998531571219, 0.9681159420289855, 0.9679821295606851, 0.9735249621785174, 0.9727740986019132, 0.9666413949962093, 0.9683453237410072, 0.9737991266375546, 0.972953216374269, 0.9707174231332357, 0.9704397981254506, 0.9772393538913363, 0.9706529713866471, 0.9700427960057061, 0.974600870827286, 0.9701166180758017, 0.9673676577229877, 0.9689754689754689, 0.9702380952380952, 0.96987087517934]
after delete: len(clean_dataset) 1394
after delete: len(noisy_dataset) 1742
epoch [100/200] batch [5/43] time 0.483 (0.432) data 0.352 (0.301) loss_x loss_x 1.5596 (1.3025) acc_x 65.6250 (65.6250) lr 1.0314e-03 eta 0:00:16
epoch [100/200] batch [10/43] time 0.453 (0.449) data 0.323 (0.318) loss_x loss_x 1.1572 (1.1265) acc_x 62.5000 (68.7500) lr 1.0314e-03 eta 0:00:14
epoch [100/200] batch [15/43] time 0.559 (0.463) data 0.428 (0.333) loss_x loss_x 1.1660 (1.1037) acc_x 75.0000 (70.4167) lr 1.0314e-03 eta 0:00:12
epoch [100/200] batch [20/43] time 0.416 (0.454) data 0.286 (0.324) loss_x loss_x 0.8828 (1.1217) acc_x 71.8750 (69.3750) lr 1.0314e-03 eta 0:00:10
epoch [100/200] batch [25/43] time 0.394 (0.451) data 0.264 (0.321) loss_x loss_x 1.4727 (1.1880) acc_x 65.6250 (68.1250) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [30/43] time 0.437 (0.449) data 0.307 (0.319) loss_x loss_x 1.0166 (1.1966) acc_x 78.1250 (67.9167) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [35/43] time 0.354 (0.446) data 0.224 (0.316) loss_x loss_x 1.3516 (1.1855) acc_x 65.6250 (68.8393) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [40/43] time 0.476 (0.447) data 0.347 (0.317) loss_x loss_x 1.1211 (1.1776) acc_x 68.7500 (69.2188) lr 1.0314e-03 eta 0:00:01
epoch [100/200] batch [5/54] time 0.337 (0.443) data 0.205 (0.313) loss_u loss_u 0.7896 (0.8006) acc_u 28.1250 (24.3750) lr 1.0314e-03 eta 0:00:21
epoch [100/200] batch [10/54] time 0.420 (0.443) data 0.289 (0.312) loss_u loss_u 0.8481 (0.8271) acc_u 18.7500 (20.9375) lr 1.0314e-03 eta 0:00:19
epoch [100/200] batch [15/54] time 0.419 (0.439) data 0.287 (0.309) loss_u loss_u 0.7295 (0.8240) acc_u 34.3750 (21.2500) lr 1.0314e-03 eta 0:00:17
epoch [100/200] batch [20/54] time 0.511 (0.444) data 0.379 (0.313) loss_u loss_u 0.8618 (0.8178) acc_u 6.2500 (21.5625) lr 1.0314e-03 eta 0:00:15
epoch [100/200] batch [25/54] time 0.429 (0.447) data 0.297 (0.316) loss_u loss_u 0.8257 (0.8167) acc_u 15.6250 (21.6250) lr 1.0314e-03 eta 0:00:12
epoch [100/200] batch [30/54] time 0.548 (0.451) data 0.417 (0.320) loss_u loss_u 0.8545 (0.8196) acc_u 18.7500 (21.0417) lr 1.0314e-03 eta 0:00:10
epoch [100/200] batch [35/54] time 0.390 (0.453) data 0.258 (0.323) loss_u loss_u 0.7744 (0.8201) acc_u 25.0000 (21.0714) lr 1.0314e-03 eta 0:00:08
epoch [100/200] batch [40/54] time 0.477 (0.454) data 0.347 (0.323) loss_u loss_u 0.7495 (0.8204) acc_u 28.1250 (20.7031) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [45/54] time 0.547 (0.455) data 0.415 (0.325) loss_u loss_u 0.8506 (0.8214) acc_u 25.0000 (20.7639) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [50/54] time 0.481 (0.455) data 0.350 (0.324) loss_u loss_u 0.8013 (0.8193) acc_u 25.0000 (21.0625) lr 1.0314e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1419
confident_label rate tensor(0.4487, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1407
clean true:1367
clean false:40
clean_rate:0.9715707178393745
noisy true:350
noisy false:1379
after delete: len(clean_dataset) 1407
after delete: len(noisy_dataset) 1729
epoch [101/200] batch [5/43] time 0.437 (0.501) data 0.306 (0.371) loss_x loss_x 1.4688 (1.2871) acc_x 62.5000 (67.5000) lr 1.0157e-03 eta 0:00:19
epoch [101/200] batch [10/43] time 0.507 (0.495) data 0.377 (0.364) loss_x loss_x 1.5088 (1.2649) acc_x 71.8750 (68.1250) lr 1.0157e-03 eta 0:00:16
epoch [101/200] batch [15/43] time 0.581 (0.498) data 0.450 (0.367) loss_x loss_x 1.3232 (1.2720) acc_x 65.6250 (67.0833) lr 1.0157e-03 eta 0:00:13
epoch [101/200] batch [20/43] time 0.435 (0.498) data 0.304 (0.367) loss_x loss_x 0.6802 (1.2093) acc_x 87.5000 (69.5312) lr 1.0157e-03 eta 0:00:11
epoch [101/200] batch [25/43] time 0.557 (0.501) data 0.426 (0.370) loss_x loss_x 1.1924 (1.1702) acc_x 71.8750 (70.0000) lr 1.0157e-03 eta 0:00:09
epoch [101/200] batch [30/43] time 0.397 (0.489) data 0.266 (0.358) loss_x loss_x 1.3799 (1.1999) acc_x 65.6250 (69.3750) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [35/43] time 0.464 (0.479) data 0.334 (0.349) loss_x loss_x 1.0918 (1.1862) acc_x 84.3750 (69.8214) lr 1.0157e-03 eta 0:00:03
epoch [101/200] batch [40/43] time 0.519 (0.474) data 0.389 (0.343) loss_x loss_x 0.9248 (1.1716) acc_x 71.8750 (70.5469) lr 1.0157e-03 eta 0:00:01
epoch [101/200] batch [5/54] time 0.557 (0.473) data 0.426 (0.342) loss_u loss_u 0.7983 (0.8036) acc_u 21.8750 (24.3750) lr 1.0157e-03 eta 0:00:23
epoch [101/200] batch [10/54] time 0.576 (0.472) data 0.445 (0.342) loss_u loss_u 0.8730 (0.8485) acc_u 21.8750 (20.0000) lr 1.0157e-03 eta 0:00:20
epoch [101/200] batch [15/54] time 0.871 (0.474) data 0.739 (0.343) loss_u loss_u 0.8447 (0.8476) acc_u 15.6250 (19.1667) lr 1.0157e-03 eta 0:00:18
epoch [101/200] batch [20/54] time 0.402 (0.471) data 0.271 (0.340) loss_u loss_u 0.8267 (0.8371) acc_u 21.8750 (20.3125) lr 1.0157e-03 eta 0:00:16
epoch [101/200] batch [25/54] time 0.476 (0.468) data 0.345 (0.338) loss_u loss_u 0.9111 (0.8389) acc_u 6.2500 (20.1250) lr 1.0157e-03 eta 0:00:13
epoch [101/200] batch [30/54] time 0.432 (0.464) data 0.300 (0.334) loss_u loss_u 0.8081 (0.8406) acc_u 25.0000 (20.5208) lr 1.0157e-03 eta 0:00:11
epoch [101/200] batch [35/54] time 0.428 (0.460) data 0.297 (0.329) loss_u loss_u 0.7437 (0.8339) acc_u 25.0000 (21.2500) lr 1.0157e-03 eta 0:00:08
epoch [101/200] batch [40/54] time 0.564 (0.461) data 0.434 (0.330) loss_u loss_u 0.7798 (0.8310) acc_u 31.2500 (21.4844) lr 1.0157e-03 eta 0:00:06
epoch [101/200] batch [45/54] time 0.664 (0.461) data 0.533 (0.330) loss_u loss_u 0.8877 (0.8303) acc_u 12.5000 (21.4583) lr 1.0157e-03 eta 0:00:04
epoch [101/200] batch [50/54] time 0.406 (0.461) data 0.276 (0.330) loss_u loss_u 0.8013 (0.8300) acc_u 28.1250 (21.5625) lr 1.0157e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1465
confident_label rate tensor(0.4385, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1375
clean true:1333
clean false:42
clean_rate:0.9694545454545455
noisy true:338
noisy false:1423
after delete: len(clean_dataset) 1375
after delete: len(noisy_dataset) 1761
epoch [102/200] batch [5/42] time 0.466 (0.509) data 0.335 (0.378) loss_x loss_x 1.3984 (1.5475) acc_x 75.0000 (67.5000) lr 1.0000e-03 eta 0:00:18
epoch [102/200] batch [10/42] time 0.410 (0.478) data 0.280 (0.348) loss_x loss_x 1.1133 (1.2944) acc_x 71.8750 (70.6250) lr 1.0000e-03 eta 0:00:15
epoch [102/200] batch [15/42] time 0.511 (0.493) data 0.381 (0.363) loss_x loss_x 1.5771 (1.2502) acc_x 62.5000 (70.2083) lr 1.0000e-03 eta 0:00:13
epoch [102/200] batch [20/42] time 0.439 (0.483) data 0.308 (0.352) loss_x loss_x 0.9297 (1.2805) acc_x 75.0000 (69.3750) lr 1.0000e-03 eta 0:00:10
epoch [102/200] batch [25/42] time 0.316 (0.469) data 0.185 (0.339) loss_x loss_x 0.7095 (1.2377) acc_x 78.1250 (69.8750) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [30/42] time 0.410 (0.466) data 0.280 (0.335) loss_x loss_x 0.9507 (1.2167) acc_x 75.0000 (69.7917) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [35/42] time 0.544 (0.466) data 0.414 (0.336) loss_x loss_x 0.8701 (1.2109) acc_x 81.2500 (69.7321) lr 1.0000e-03 eta 0:00:03
epoch [102/200] batch [40/42] time 0.680 (0.467) data 0.550 (0.337) loss_x loss_x 1.4512 (1.2061) acc_x 62.5000 (70.0000) lr 1.0000e-03 eta 0:00:00
epoch [102/200] batch [5/55] time 0.412 (0.462) data 0.281 (0.331) loss_u loss_u 0.7363 (0.7891) acc_u 31.2500 (25.6250) lr 1.0000e-03 eta 0:00:23
epoch [102/200] batch [10/55] time 0.393 (0.459) data 0.262 (0.329) loss_u loss_u 0.8286 (0.8022) acc_u 18.7500 (25.6250) lr 1.0000e-03 eta 0:00:20
epoch [102/200] batch [15/55] time 0.447 (0.457) data 0.316 (0.326) loss_u loss_u 0.7798 (0.7892) acc_u 25.0000 (27.0833) lr 1.0000e-03 eta 0:00:18
epoch [102/200] batch [20/55] time 0.433 (0.456) data 0.302 (0.325) loss_u loss_u 0.7915 (0.7929) acc_u 25.0000 (26.4062) lr 1.0000e-03 eta 0:00:15
epoch [102/200] batch [25/55] time 0.342 (0.454) data 0.210 (0.323) loss_u loss_u 0.7715 (0.7970) acc_u 37.5000 (26.5000) lr 1.0000e-03 eta 0:00:13
epoch [102/200] batch [30/55] time 0.444 (0.453) data 0.312 (0.322) loss_u loss_u 0.7520 (0.7930) acc_u 25.0000 (26.6667) lr 1.0000e-03 eta 0:00:11
epoch [102/200] batch [35/55] time 0.337 (0.453) data 0.207 (0.322) loss_u loss_u 0.8628 (0.7963) acc_u 21.8750 (26.4286) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [40/55] time 0.460 (0.454) data 0.329 (0.323) loss_u loss_u 0.8550 (0.8009) acc_u 15.6250 (25.6250) lr 1.0000e-03 eta 0:00:06
epoch [102/200] batch [45/55] time 0.398 (0.454) data 0.267 (0.323) loss_u loss_u 0.8657 (0.8025) acc_u 21.8750 (25.5556) lr 1.0000e-03 eta 0:00:04
epoch [102/200] batch [50/55] time 0.528 (0.456) data 0.398 (0.326) loss_u loss_u 0.8735 (0.8035) acc_u 15.6250 (25.2500) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [55/55] time 0.397 (0.456) data 0.266 (0.325) loss_u loss_u 0.8940 (0.8097) acc_u 15.6250 (24.4318) lr 1.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1459
confident_label rate tensor(0.4353, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1365
clean true:1333
clean false:32
clean_rate:0.9765567765567765
noisy true:344
noisy false:1427
after delete: len(clean_dataset) 1365
after delete: len(noisy_dataset) 1771
epoch [103/200] batch [5/42] time 0.461 (0.426) data 0.331 (0.295) loss_x loss_x 0.5718 (1.0176) acc_x 84.3750 (71.8750) lr 9.8429e-04 eta 0:00:15
epoch [103/200] batch [10/42] time 0.439 (0.449) data 0.308 (0.318) loss_x loss_x 1.0127 (1.0375) acc_x 78.1250 (73.4375) lr 9.8429e-04 eta 0:00:14
epoch [103/200] batch [15/42] time 0.408 (0.459) data 0.277 (0.328) loss_x loss_x 1.3877 (1.0900) acc_x 65.6250 (72.5000) lr 9.8429e-04 eta 0:00:12
epoch [103/200] batch [20/42] time 0.410 (0.453) data 0.280 (0.323) loss_x loss_x 1.0908 (1.1520) acc_x 65.6250 (70.1562) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [25/42] time 0.492 (0.459) data 0.362 (0.328) loss_x loss_x 1.0977 (1.1546) acc_x 68.7500 (70.5000) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [30/42] time 0.394 (0.452) data 0.264 (0.322) loss_x loss_x 1.2305 (1.1709) acc_x 68.7500 (70.3125) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [35/42] time 0.414 (0.449) data 0.283 (0.318) loss_x loss_x 1.1152 (1.1765) acc_x 78.1250 (70.5357) lr 9.8429e-04 eta 0:00:03
epoch [103/200] batch [40/42] time 0.484 (0.455) data 0.354 (0.324) loss_x loss_x 0.9424 (1.1789) acc_x 78.1250 (70.2344) lr 9.8429e-04 eta 0:00:00
epoch [103/200] batch [5/55] time 0.588 (0.452) data 0.454 (0.321) loss_u loss_u 0.7524 (0.7909) acc_u 25.0000 (25.0000) lr 9.8429e-04 eta 0:00:22
epoch [103/200] batch [10/55] time 0.352 (0.452) data 0.221 (0.321) loss_u loss_u 0.7695 (0.7946) acc_u 31.2500 (26.2500) lr 9.8429e-04 eta 0:00:20
epoch [103/200] batch [15/55] time 0.486 (0.455) data 0.355 (0.324) loss_u loss_u 0.8188 (0.8163) acc_u 25.0000 (22.2917) lr 9.8429e-04 eta 0:00:18
epoch [103/200] batch [20/55] time 0.457 (0.456) data 0.325 (0.325) loss_u loss_u 0.8618 (0.8146) acc_u 18.7500 (22.8125) lr 9.8429e-04 eta 0:00:15
epoch [103/200] batch [25/55] time 0.534 (0.457) data 0.403 (0.326) loss_u loss_u 0.8286 (0.8221) acc_u 25.0000 (21.8750) lr 9.8429e-04 eta 0:00:13
epoch [103/200] batch [30/55] time 0.596 (0.461) data 0.464 (0.330) loss_u loss_u 0.7905 (0.8228) acc_u 31.2500 (21.9792) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [35/55] time 0.455 (0.458) data 0.324 (0.327) loss_u loss_u 0.7866 (0.8223) acc_u 28.1250 (22.0536) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [40/55] time 0.366 (0.453) data 0.235 (0.322) loss_u loss_u 0.8486 (0.8196) acc_u 18.7500 (22.5000) lr 9.8429e-04 eta 0:00:06
epoch [103/200] batch [45/55] time 0.309 (0.454) data 0.179 (0.323) loss_u loss_u 0.8804 (0.8147) acc_u 15.6250 (22.9861) lr 9.8429e-04 eta 0:00:04
epoch [103/200] batch [50/55] time 0.433 (0.454) data 0.302 (0.323) loss_u loss_u 0.8521 (0.8119) acc_u 15.6250 (23.2500) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [55/55] time 0.419 (0.453) data 0.289 (0.322) loss_u loss_u 0.8291 (0.8157) acc_u 21.8750 (22.5568) lr 9.8429e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1406
confident_label rate tensor(0.4518, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1417
clean true:1375
clean false:42
clean_rate:0.9703599153140438
noisy true:355
noisy false:1364
after delete: len(clean_dataset) 1417
after delete: len(noisy_dataset) 1719
epoch [104/200] batch [5/44] time 0.447 (0.464) data 0.316 (0.333) loss_x loss_x 0.7700 (0.9029) acc_x 84.3750 (77.5000) lr 9.6859e-04 eta 0:00:18
epoch [104/200] batch [10/44] time 0.310 (0.452) data 0.180 (0.321) loss_x loss_x 1.5352 (1.0521) acc_x 62.5000 (74.0625) lr 9.6859e-04 eta 0:00:15
epoch [104/200] batch [15/44] time 0.425 (0.457) data 0.295 (0.327) loss_x loss_x 1.1992 (1.0499) acc_x 71.8750 (74.3750) lr 9.6859e-04 eta 0:00:13
epoch [104/200] batch [20/44] time 0.520 (0.464) data 0.389 (0.333) loss_x loss_x 0.9629 (1.0689) acc_x 71.8750 (73.1250) lr 9.6859e-04 eta 0:00:11
epoch [104/200] batch [25/44] time 0.390 (0.463) data 0.260 (0.332) loss_x loss_x 0.7437 (1.0564) acc_x 81.2500 (73.2500) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [30/44] time 0.585 (0.471) data 0.455 (0.340) loss_x loss_x 1.3506 (1.0659) acc_x 75.0000 (73.2292) lr 9.6859e-04 eta 0:00:06
epoch [104/200] batch [35/44] time 0.441 (0.466) data 0.310 (0.336) loss_x loss_x 1.1543 (1.0768) acc_x 71.8750 (73.0357) lr 9.6859e-04 eta 0:00:04
epoch [104/200] batch [40/44] time 0.487 (0.464) data 0.356 (0.333) loss_x loss_x 1.0586 (1.0772) acc_x 65.6250 (72.5000) lr 9.6859e-04 eta 0:00:01
epoch [104/200] batch [5/53] time 0.354 (0.462) data 0.224 (0.331) loss_u loss_u 0.8506 (0.8400) acc_u 12.5000 (21.2500) lr 9.6859e-04 eta 0:00:22
epoch [104/200] batch [10/53] time 0.545 (0.459) data 0.413 (0.329) loss_u loss_u 0.8169 (0.8192) acc_u 28.1250 (22.5000) lr 9.6859e-04 eta 0:00:19
epoch [104/200] batch [15/53] time 0.361 (0.456) data 0.231 (0.326) loss_u loss_u 0.8569 (0.8291) acc_u 18.7500 (21.8750) lr 9.6859e-04 eta 0:00:17
epoch [104/200] batch [20/53] time 0.451 (0.459) data 0.320 (0.329) loss_u loss_u 0.8047 (0.8307) acc_u 21.8750 (21.2500) lr 9.6859e-04 eta 0:00:15
epoch [104/200] batch [25/53] time 0.543 (0.458) data 0.410 (0.327) loss_u loss_u 0.8354 (0.8293) acc_u 21.8750 (21.2500) lr 9.6859e-04 eta 0:00:12
epoch [104/200] batch [30/53] time 0.468 (0.458) data 0.337 (0.327) loss_u loss_u 0.8525 (0.8265) acc_u 18.7500 (21.6667) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [35/53] time 0.396 (0.455) data 0.266 (0.324) loss_u loss_u 0.8628 (0.8251) acc_u 18.7500 (21.5179) lr 9.6859e-04 eta 0:00:08
epoch [104/200] batch [40/53] time 0.432 (0.455) data 0.301 (0.324) loss_u loss_u 0.8003 (0.8258) acc_u 25.0000 (21.4062) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [45/53] time 0.420 (0.454) data 0.289 (0.323) loss_u loss_u 0.8931 (0.8277) acc_u 12.5000 (20.8333) lr 9.6859e-04 eta 0:00:03
epoch [104/200] batch [50/53] time 0.672 (0.454) data 0.541 (0.323) loss_u loss_u 0.8169 (0.8302) acc_u 18.7500 (20.5625) lr 9.6859e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.4327, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1357
clean true:1321
clean false:36
clean_rate:0.9734708916728076
noisy true:381
noisy false:1398
after delete: len(clean_dataset) 1357
after delete: len(noisy_dataset) 1779
epoch [105/200] batch [5/42] time 0.516 (0.421) data 0.386 (0.291) loss_x loss_x 1.6172 (1.2217) acc_x 62.5000 (65.0000) lr 9.5289e-04 eta 0:00:15
epoch [105/200] batch [10/42] time 0.522 (0.452) data 0.392 (0.322) loss_x loss_x 0.8921 (1.1595) acc_x 71.8750 (68.1250) lr 9.5289e-04 eta 0:00:14
epoch [105/200] batch [15/42] time 0.408 (0.448) data 0.278 (0.318) loss_x loss_x 0.7007 (1.1416) acc_x 81.2500 (70.4167) lr 9.5289e-04 eta 0:00:12
epoch [105/200] batch [20/42] time 0.393 (0.446) data 0.260 (0.316) loss_x loss_x 1.1475 (1.1967) acc_x 78.1250 (70.0000) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [25/42] time 0.524 (0.450) data 0.394 (0.320) loss_x loss_x 0.6738 (1.1912) acc_x 84.3750 (69.2500) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [30/42] time 0.517 (0.452) data 0.386 (0.321) loss_x loss_x 1.3125 (1.1898) acc_x 71.8750 (69.3750) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [35/42] time 0.397 (0.452) data 0.267 (0.321) loss_x loss_x 1.0732 (1.1958) acc_x 75.0000 (69.2857) lr 9.5289e-04 eta 0:00:03
epoch [105/200] batch [40/42] time 0.433 (0.451) data 0.303 (0.321) loss_x loss_x 1.6924 (1.1992) acc_x 62.5000 (69.9219) lr 9.5289e-04 eta 0:00:00
epoch [105/200] batch [5/55] time 0.810 (0.461) data 0.678 (0.330) loss_u loss_u 0.8130 (0.8141) acc_u 25.0000 (23.7500) lr 9.5289e-04 eta 0:00:23
epoch [105/200] batch [10/55] time 0.454 (0.457) data 0.323 (0.326) loss_u loss_u 0.8188 (0.8154) acc_u 18.7500 (23.7500) lr 9.5289e-04 eta 0:00:20
epoch [105/200] batch [15/55] time 0.630 (0.457) data 0.499 (0.326) loss_u loss_u 0.7573 (0.8051) acc_u 34.3750 (25.4167) lr 9.5289e-04 eta 0:00:18
epoch [105/200] batch [20/55] time 0.431 (0.461) data 0.300 (0.330) loss_u loss_u 0.7466 (0.8013) acc_u 28.1250 (26.0938) lr 9.5289e-04 eta 0:00:16
epoch [105/200] batch [25/55] time 0.446 (0.455) data 0.314 (0.324) loss_u loss_u 0.8442 (0.7991) acc_u 18.7500 (25.6250) lr 9.5289e-04 eta 0:00:13
epoch [105/200] batch [30/55] time 0.656 (0.461) data 0.524 (0.330) loss_u loss_u 0.7866 (0.7997) acc_u 25.0000 (25.8333) lr 9.5289e-04 eta 0:00:11
epoch [105/200] batch [35/55] time 0.510 (0.464) data 0.379 (0.333) loss_u loss_u 0.8008 (0.7953) acc_u 21.8750 (26.5179) lr 9.5289e-04 eta 0:00:09
epoch [105/200] batch [40/55] time 0.355 (0.462) data 0.223 (0.331) loss_u loss_u 0.7578 (0.7988) acc_u 34.3750 (26.3281) lr 9.5289e-04 eta 0:00:06
epoch [105/200] batch [45/55] time 0.364 (0.458) data 0.233 (0.327) loss_u loss_u 0.8203 (0.8030) acc_u 21.8750 (25.6250) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [50/55] time 0.471 (0.456) data 0.339 (0.325) loss_u loss_u 0.8491 (0.8051) acc_u 18.7500 (25.0000) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [55/55] time 0.438 (0.457) data 0.307 (0.326) loss_u loss_u 0.7998 (0.8081) acc_u 28.1250 (24.3750) lr 9.5289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1474
confident_label rate tensor(0.4330, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1358
clean true:1313
clean false:45
clean_rate:0.9668630338733432
noisy true:349
noisy false:1429
after delete: len(clean_dataset) 1358
after delete: len(noisy_dataset) 1778
epoch [106/200] batch [5/42] time 0.465 (0.494) data 0.334 (0.364) loss_x loss_x 1.1299 (1.3010) acc_x 68.7500 (66.8750) lr 9.3721e-04 eta 0:00:18
epoch [106/200] batch [10/42] time 0.385 (0.480) data 0.253 (0.349) loss_x loss_x 1.8105 (1.3552) acc_x 56.2500 (66.8750) lr 9.3721e-04 eta 0:00:15
epoch [106/200] batch [15/42] time 0.436 (0.479) data 0.305 (0.349) loss_x loss_x 1.1104 (1.2940) acc_x 71.8750 (67.2917) lr 9.3721e-04 eta 0:00:12
epoch [106/200] batch [20/42] time 0.493 (0.470) data 0.363 (0.339) loss_x loss_x 0.8701 (1.2159) acc_x 81.2500 (69.8438) lr 9.3721e-04 eta 0:00:10
epoch [106/200] batch [25/42] time 0.416 (0.464) data 0.286 (0.333) loss_x loss_x 0.9727 (1.1936) acc_x 75.0000 (70.3750) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [30/42] time 0.695 (0.464) data 0.565 (0.334) loss_x loss_x 1.1719 (1.1749) acc_x 62.5000 (70.6250) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [35/42] time 0.425 (0.454) data 0.294 (0.324) loss_x loss_x 1.3359 (1.1909) acc_x 71.8750 (70.1786) lr 9.3721e-04 eta 0:00:03
epoch [106/200] batch [40/42] time 0.463 (0.455) data 0.333 (0.325) loss_x loss_x 0.8188 (1.1709) acc_x 84.3750 (70.6250) lr 9.3721e-04 eta 0:00:00
epoch [106/200] batch [5/55] time 0.420 (0.457) data 0.289 (0.326) loss_u loss_u 0.8560 (0.7962) acc_u 18.7500 (25.6250) lr 9.3721e-04 eta 0:00:22
epoch [106/200] batch [10/55] time 0.707 (0.465) data 0.576 (0.334) loss_u loss_u 0.8833 (0.8107) acc_u 9.3750 (23.7500) lr 9.3721e-04 eta 0:00:20
epoch [106/200] batch [15/55] time 0.519 (0.467) data 0.387 (0.336) loss_u loss_u 0.7480 (0.8006) acc_u 34.3750 (25.6250) lr 9.3721e-04 eta 0:00:18
epoch [106/200] batch [20/55] time 0.341 (0.465) data 0.209 (0.334) loss_u loss_u 0.7812 (0.8097) acc_u 34.3750 (23.9062) lr 9.3721e-04 eta 0:00:16
epoch [106/200] batch [25/55] time 0.458 (0.463) data 0.327 (0.332) loss_u loss_u 0.7002 (0.8006) acc_u 46.8750 (25.7500) lr 9.3721e-04 eta 0:00:13
epoch [106/200] batch [30/55] time 0.380 (0.460) data 0.248 (0.329) loss_u loss_u 0.8340 (0.8031) acc_u 25.0000 (25.2083) lr 9.3721e-04 eta 0:00:11
epoch [106/200] batch [35/55] time 0.344 (0.454) data 0.213 (0.323) loss_u loss_u 0.8398 (0.8090) acc_u 21.8750 (24.1964) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [40/55] time 0.448 (0.454) data 0.316 (0.322) loss_u loss_u 0.7988 (0.8106) acc_u 21.8750 (23.8281) lr 9.3721e-04 eta 0:00:06
epoch [106/200] batch [45/55] time 0.453 (0.454) data 0.323 (0.323) loss_u loss_u 0.8286 (0.8125) acc_u 18.7500 (23.6111) lr 9.3721e-04 eta 0:00:04
epoch [106/200] batch [50/55] time 0.457 (0.454) data 0.326 (0.323) loss_u loss_u 0.8091 (0.8095) acc_u 21.8750 (23.8125) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [55/55] time 0.299 (0.449) data 0.169 (0.318) loss_u loss_u 0.7578 (0.8089) acc_u 28.1250 (23.8636) lr 9.3721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1415
confident_label rate tensor(0.4426, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1388
clean true:1354
clean false:34
clean_rate:0.9755043227665706
noisy true:367
noisy false:1381
after delete: len(clean_dataset) 1388
after delete: len(noisy_dataset) 1748
epoch [107/200] batch [5/43] time 0.462 (0.483) data 0.331 (0.353) loss_x loss_x 0.9307 (0.9284) acc_x 87.5000 (79.3750) lr 9.2154e-04 eta 0:00:18
epoch [107/200] batch [10/43] time 0.488 (0.484) data 0.358 (0.354) loss_x loss_x 1.1670 (1.1616) acc_x 68.7500 (71.2500) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [15/43] time 0.358 (0.492) data 0.227 (0.362) loss_x loss_x 1.4639 (1.1403) acc_x 68.7500 (71.2500) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [20/43] time 0.371 (0.470) data 0.241 (0.339) loss_x loss_x 0.9648 (1.1329) acc_x 71.8750 (71.5625) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [25/43] time 0.364 (0.462) data 0.233 (0.332) loss_x loss_x 1.3691 (1.1525) acc_x 56.2500 (70.3750) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [30/43] time 0.437 (0.459) data 0.306 (0.328) loss_x loss_x 0.8008 (1.1686) acc_x 75.0000 (70.5208) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [35/43] time 0.417 (0.460) data 0.287 (0.330) loss_x loss_x 0.9463 (1.1938) acc_x 71.8750 (69.9107) lr 9.2154e-04 eta 0:00:03
epoch [107/200] batch [40/43] time 0.467 (0.454) data 0.337 (0.323) loss_x loss_x 1.1406 (1.1952) acc_x 71.8750 (69.6875) lr 9.2154e-04 eta 0:00:01
epoch [107/200] batch [5/54] time 0.426 (0.454) data 0.296 (0.324) loss_u loss_u 0.8931 (0.8346) acc_u 9.3750 (20.6250) lr 9.2154e-04 eta 0:00:22
epoch [107/200] batch [10/54] time 0.668 (0.454) data 0.537 (0.324) loss_u loss_u 0.7993 (0.8402) acc_u 28.1250 (21.2500) lr 9.2154e-04 eta 0:00:19
epoch [107/200] batch [15/54] time 0.364 (0.453) data 0.233 (0.322) loss_u loss_u 0.7905 (0.8263) acc_u 21.8750 (22.2917) lr 9.2154e-04 eta 0:00:17
epoch [107/200] batch [20/54] time 0.449 (0.453) data 0.319 (0.323) loss_u loss_u 0.8091 (0.8227) acc_u 25.0000 (22.1875) lr 9.2154e-04 eta 0:00:15
epoch [107/200] batch [25/54] time 0.474 (0.456) data 0.343 (0.325) loss_u loss_u 0.8442 (0.8164) acc_u 21.8750 (22.3750) lr 9.2154e-04 eta 0:00:13
epoch [107/200] batch [30/54] time 0.506 (0.455) data 0.375 (0.324) loss_u loss_u 0.8682 (0.8173) acc_u 15.6250 (22.6042) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [35/54] time 0.398 (0.454) data 0.268 (0.323) loss_u loss_u 0.8477 (0.8122) acc_u 18.7500 (23.4821) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [40/54] time 0.356 (0.450) data 0.224 (0.320) loss_u loss_u 0.8193 (0.8147) acc_u 18.7500 (22.8906) lr 9.2154e-04 eta 0:00:06
epoch [107/200] batch [45/54] time 0.386 (0.450) data 0.255 (0.320) loss_u loss_u 0.7812 (0.8119) acc_u 25.0000 (22.8472) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [50/54] time 0.331 (0.450) data 0.199 (0.319) loss_u loss_u 0.8403 (0.8119) acc_u 18.7500 (22.6875) lr 9.2154e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1372
confident_label rate tensor(0.4589, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1439
clean true:1396
clean false:43
clean_rate:0.9701181375955524
noisy true:368
noisy false:1329
after delete: len(clean_dataset) 1439
after delete: len(noisy_dataset) 1697
epoch [108/200] batch [5/44] time 0.416 (0.404) data 0.286 (0.274) loss_x loss_x 0.8145 (1.1685) acc_x 81.2500 (73.1250) lr 9.0589e-04 eta 0:00:15
epoch [108/200] batch [10/44] time 0.695 (0.453) data 0.565 (0.323) loss_x loss_x 1.2607 (1.0764) acc_x 62.5000 (72.5000) lr 9.0589e-04 eta 0:00:15
epoch [108/200] batch [15/44] time 0.528 (0.464) data 0.397 (0.333) loss_x loss_x 0.6704 (1.0525) acc_x 75.0000 (73.5417) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [20/44] time 0.426 (0.470) data 0.295 (0.340) loss_x loss_x 1.0312 (1.0405) acc_x 75.0000 (74.2188) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [25/44] time 0.399 (0.465) data 0.268 (0.335) loss_x loss_x 0.7954 (1.0834) acc_x 78.1250 (72.8750) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [30/44] time 0.478 (0.465) data 0.347 (0.335) loss_x loss_x 1.2109 (1.1064) acc_x 71.8750 (72.1875) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [35/44] time 0.542 (0.470) data 0.412 (0.339) loss_x loss_x 1.1670 (1.1262) acc_x 71.8750 (71.4286) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [40/44] time 0.344 (0.464) data 0.214 (0.333) loss_x loss_x 0.8901 (1.1328) acc_x 81.2500 (71.4844) lr 9.0589e-04 eta 0:00:01
epoch [108/200] batch [5/53] time 0.472 (0.463) data 0.341 (0.333) loss_u loss_u 0.8169 (0.8468) acc_u 15.6250 (15.6250) lr 9.0589e-04 eta 0:00:22
epoch [108/200] batch [10/53] time 0.344 (0.461) data 0.212 (0.330) loss_u loss_u 0.8384 (0.8429) acc_u 21.8750 (18.1250) lr 9.0589e-04 eta 0:00:19
epoch [108/200] batch [15/53] time 0.370 (0.458) data 0.238 (0.327) loss_u loss_u 0.8096 (0.8450) acc_u 21.8750 (18.3333) lr 9.0589e-04 eta 0:00:17
epoch [108/200] batch [20/53] time 0.471 (0.458) data 0.339 (0.327) loss_u loss_u 0.8042 (0.8320) acc_u 25.0000 (20.3125) lr 9.0589e-04 eta 0:00:15
epoch [108/200] batch [25/53] time 0.550 (0.464) data 0.419 (0.333) loss_u loss_u 0.8042 (0.8280) acc_u 28.1250 (20.5000) lr 9.0589e-04 eta 0:00:12
epoch [108/200] batch [30/53] time 0.450 (0.461) data 0.318 (0.330) loss_u loss_u 0.8086 (0.8296) acc_u 18.7500 (20.1042) lr 9.0589e-04 eta 0:00:10
epoch [108/200] batch [35/53] time 0.608 (0.463) data 0.476 (0.332) loss_u loss_u 0.9268 (0.8303) acc_u 9.3750 (20.0893) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [40/53] time 0.475 (0.462) data 0.344 (0.331) loss_u loss_u 0.8354 (0.8257) acc_u 15.6250 (20.6250) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [45/53] time 0.412 (0.461) data 0.281 (0.330) loss_u loss_u 0.8374 (0.8264) acc_u 18.7500 (20.4861) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [50/53] time 0.576 (0.463) data 0.445 (0.332) loss_u loss_u 0.8120 (0.8248) acc_u 25.0000 (20.6250) lr 9.0589e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1416
confident_label rate tensor(0.4477, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1404
clean true:1365
clean false:39
clean_rate:0.9722222222222222
noisy true:355
noisy false:1377
after delete: len(clean_dataset) 1404
after delete: len(noisy_dataset) 1732
epoch [109/200] batch [5/43] time 0.670 (0.528) data 0.539 (0.396) loss_x loss_x 1.5762 (1.2589) acc_x 59.3750 (69.3750) lr 8.9027e-04 eta 0:00:20
epoch [109/200] batch [10/43] time 0.353 (0.480) data 0.223 (0.349) loss_x loss_x 0.7588 (1.1135) acc_x 81.2500 (72.8125) lr 8.9027e-04 eta 0:00:15
epoch [109/200] batch [15/43] time 0.403 (0.465) data 0.272 (0.334) loss_x loss_x 1.8730 (1.2135) acc_x 59.3750 (70.8333) lr 8.9027e-04 eta 0:00:13
epoch [109/200] batch [20/43] time 0.561 (0.466) data 0.430 (0.335) loss_x loss_x 1.2246 (1.2282) acc_x 62.5000 (69.2188) lr 8.9027e-04 eta 0:00:10
epoch [109/200] batch [25/43] time 0.418 (0.465) data 0.287 (0.335) loss_x loss_x 1.4277 (1.1880) acc_x 65.6250 (70.3750) lr 8.9027e-04 eta 0:00:08
epoch [109/200] batch [30/43] time 0.411 (0.468) data 0.280 (0.337) loss_x loss_x 1.1143 (1.1629) acc_x 71.8750 (71.1458) lr 8.9027e-04 eta 0:00:06
epoch [109/200] batch [35/43] time 0.431 (0.473) data 0.300 (0.343) loss_x loss_x 0.7490 (1.1633) acc_x 75.0000 (71.0714) lr 8.9027e-04 eta 0:00:03
epoch [109/200] batch [40/43] time 0.494 (0.469) data 0.363 (0.339) loss_x loss_x 1.2988 (1.1775) acc_x 68.7500 (71.0156) lr 8.9027e-04 eta 0:00:01
epoch [109/200] batch [5/54] time 0.370 (0.474) data 0.239 (0.343) loss_u loss_u 0.8276 (0.8136) acc_u 25.0000 (22.5000) lr 8.9027e-04 eta 0:00:23
epoch [109/200] batch [10/54] time 0.509 (0.473) data 0.377 (0.342) loss_u loss_u 0.8159 (0.8015) acc_u 21.8750 (22.8125) lr 8.9027e-04 eta 0:00:20
epoch [109/200] batch [15/54] time 0.412 (0.468) data 0.282 (0.337) loss_u loss_u 0.7764 (0.7992) acc_u 25.0000 (23.3333) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [20/54] time 0.583 (0.465) data 0.452 (0.334) loss_u loss_u 0.8584 (0.8057) acc_u 15.6250 (22.6562) lr 8.9027e-04 eta 0:00:15
epoch [109/200] batch [25/54] time 0.534 (0.465) data 0.402 (0.334) loss_u loss_u 0.8765 (0.8101) acc_u 15.6250 (22.0000) lr 8.9027e-04 eta 0:00:13
epoch [109/200] batch [30/54] time 0.462 (0.465) data 0.332 (0.334) loss_u loss_u 0.8145 (0.8066) acc_u 18.7500 (22.5000) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [35/54] time 0.366 (0.470) data 0.235 (0.339) loss_u loss_u 0.8472 (0.8122) acc_u 15.6250 (21.7857) lr 8.9027e-04 eta 0:00:08
epoch [109/200] batch [40/54] time 0.451 (0.468) data 0.320 (0.337) loss_u loss_u 0.8711 (0.8158) acc_u 15.6250 (21.8750) lr 8.9027e-04 eta 0:00:06
epoch [109/200] batch [45/54] time 0.391 (0.466) data 0.260 (0.335) loss_u loss_u 0.7661 (0.8175) acc_u 34.3750 (21.9444) lr 8.9027e-04 eta 0:00:04
epoch [109/200] batch [50/54] time 0.417 (0.467) data 0.286 (0.337) loss_u loss_u 0.8169 (0.8172) acc_u 21.8750 (22.1250) lr 8.9027e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1414
confident_label rate tensor(0.4525, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1419
clean true:1375
clean false:44
clean_rate:0.9689922480620154
noisy true:347
noisy false:1370
after delete: len(clean_dataset) 1419
after delete: len(noisy_dataset) 1717
epoch [110/200] batch [5/44] time 0.410 (0.452) data 0.280 (0.322) loss_x loss_x 0.7505 (1.1306) acc_x 84.3750 (69.3750) lr 8.7467e-04 eta 0:00:17
epoch [110/200] batch [10/44] time 0.364 (0.471) data 0.234 (0.341) loss_x loss_x 1.2012 (1.1867) acc_x 65.6250 (69.3750) lr 8.7467e-04 eta 0:00:16
epoch [110/200] batch [15/44] time 0.333 (0.446) data 0.202 (0.316) loss_x loss_x 0.9014 (1.1674) acc_x 78.1250 (70.2083) lr 8.7467e-04 eta 0:00:12
epoch [110/200] batch [20/44] time 0.361 (0.446) data 0.231 (0.316) loss_x loss_x 0.9810 (1.1599) acc_x 81.2500 (69.6875) lr 8.7467e-04 eta 0:00:10
epoch [110/200] batch [25/44] time 0.416 (0.445) data 0.286 (0.314) loss_x loss_x 1.1787 (1.1788) acc_x 78.1250 (69.7500) lr 8.7467e-04 eta 0:00:08
epoch [110/200] batch [30/44] time 0.417 (0.453) data 0.286 (0.323) loss_x loss_x 1.1660 (1.1974) acc_x 65.6250 (70.2083) lr 8.7467e-04 eta 0:00:06
epoch [110/200] batch [35/44] time 0.491 (0.457) data 0.361 (0.327) loss_x loss_x 1.0459 (1.1967) acc_x 68.7500 (69.3750) lr 8.7467e-04 eta 0:00:04
epoch [110/200] batch [40/44] time 0.477 (0.453) data 0.346 (0.323) loss_x loss_x 1.4785 (1.2042) acc_x 68.7500 (69.1406) lr 8.7467e-04 eta 0:00:01
epoch [110/200] batch [5/53] time 0.449 (0.445) data 0.318 (0.314) loss_u loss_u 0.8071 (0.7804) acc_u 25.0000 (25.6250) lr 8.7467e-04 eta 0:00:21
epoch [110/200] batch [10/53] time 0.532 (0.449) data 0.401 (0.319) loss_u loss_u 0.8198 (0.8071) acc_u 21.8750 (24.0625) lr 8.7467e-04 eta 0:00:19
epoch [110/200] batch [15/53] time 0.484 (0.448) data 0.354 (0.318) loss_u loss_u 0.7847 (0.8188) acc_u 28.1250 (22.0833) lr 8.7467e-04 eta 0:00:17
epoch [110/200] batch [20/53] time 0.369 (0.449) data 0.238 (0.319) loss_u loss_u 0.7202 (0.8031) acc_u 34.3750 (25.3125) lr 8.7467e-04 eta 0:00:14
epoch [110/200] batch [25/53] time 0.426 (0.446) data 0.296 (0.315) loss_u loss_u 0.9019 (0.8109) acc_u 9.3750 (24.0000) lr 8.7467e-04 eta 0:00:12
epoch [110/200] batch [30/53] time 0.400 (0.444) data 0.268 (0.314) loss_u loss_u 0.8379 (0.8174) acc_u 25.0000 (23.3333) lr 8.7467e-04 eta 0:00:10
epoch [110/200] batch [35/53] time 0.496 (0.443) data 0.366 (0.312) loss_u loss_u 0.7925 (0.8161) acc_u 28.1250 (23.7500) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [40/53] time 0.394 (0.441) data 0.264 (0.311) loss_u loss_u 0.8208 (0.8180) acc_u 25.0000 (23.0469) lr 8.7467e-04 eta 0:00:05
epoch [110/200] batch [45/53] time 0.523 (0.440) data 0.392 (0.309) loss_u loss_u 0.8188 (0.8155) acc_u 21.8750 (23.4722) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [50/53] time 0.551 (0.445) data 0.419 (0.314) loss_u loss_u 0.8823 (0.8184) acc_u 12.5000 (23.0625) lr 8.7467e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1439
confident_label rate tensor(0.4420, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1386
clean true:1346
clean false:40
clean_rate:0.9711399711399712
noisy true:351
noisy false:1399
after delete: len(clean_dataset) 1386
after delete: len(noisy_dataset) 1750
epoch [111/200] batch [5/43] time 0.475 (0.483) data 0.344 (0.353) loss_x loss_x 1.2393 (1.1353) acc_x 68.7500 (73.7500) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [10/43] time 0.412 (0.443) data 0.282 (0.313) loss_x loss_x 1.6348 (1.1273) acc_x 62.5000 (71.8750) lr 8.5910e-04 eta 0:00:14
epoch [111/200] batch [15/43] time 0.481 (0.465) data 0.350 (0.335) loss_x loss_x 1.6768 (1.1535) acc_x 56.2500 (70.2083) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [20/43] time 0.364 (0.452) data 0.234 (0.321) loss_x loss_x 1.5459 (1.1478) acc_x 46.8750 (69.5312) lr 8.5910e-04 eta 0:00:10
epoch [111/200] batch [25/43] time 0.456 (0.444) data 0.326 (0.314) loss_x loss_x 0.7686 (1.1146) acc_x 71.8750 (70.6250) lr 8.5910e-04 eta 0:00:07
epoch [111/200] batch [30/43] time 0.455 (0.460) data 0.325 (0.329) loss_x loss_x 1.6484 (1.1274) acc_x 62.5000 (70.7292) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [35/43] time 0.451 (0.457) data 0.320 (0.326) loss_x loss_x 1.1729 (1.1515) acc_x 75.0000 (70.2679) lr 8.5910e-04 eta 0:00:03
epoch [111/200] batch [40/43] time 0.458 (0.453) data 0.328 (0.323) loss_x loss_x 1.0088 (1.1258) acc_x 78.1250 (71.1719) lr 8.5910e-04 eta 0:00:01
epoch [111/200] batch [5/54] time 0.490 (0.455) data 0.359 (0.325) loss_u loss_u 0.8389 (0.8211) acc_u 18.7500 (23.1250) lr 8.5910e-04 eta 0:00:22
epoch [111/200] batch [10/54] time 0.332 (0.453) data 0.200 (0.323) loss_u loss_u 0.7397 (0.8153) acc_u 28.1250 (21.8750) lr 8.5910e-04 eta 0:00:19
epoch [111/200] batch [15/54] time 0.466 (0.452) data 0.336 (0.321) loss_u loss_u 0.8540 (0.8083) acc_u 15.6250 (22.5000) lr 8.5910e-04 eta 0:00:17
epoch [111/200] batch [20/54] time 0.482 (0.450) data 0.351 (0.319) loss_u loss_u 0.9482 (0.8171) acc_u 6.2500 (21.4062) lr 8.5910e-04 eta 0:00:15
epoch [111/200] batch [25/54] time 0.390 (0.447) data 0.258 (0.316) loss_u loss_u 0.8042 (0.8175) acc_u 28.1250 (21.7500) lr 8.5910e-04 eta 0:00:12
epoch [111/200] batch [30/54] time 0.351 (0.444) data 0.220 (0.313) loss_u loss_u 0.8315 (0.8187) acc_u 18.7500 (21.8750) lr 8.5910e-04 eta 0:00:10
epoch [111/200] batch [35/54] time 0.471 (0.445) data 0.340 (0.315) loss_u loss_u 0.7734 (0.8140) acc_u 28.1250 (22.7679) lr 8.5910e-04 eta 0:00:08
epoch [111/200] batch [40/54] time 0.752 (0.450) data 0.622 (0.319) loss_u loss_u 0.8462 (0.8106) acc_u 18.7500 (22.9688) lr 8.5910e-04 eta 0:00:06
epoch [111/200] batch [45/54] time 0.585 (0.453) data 0.454 (0.322) loss_u loss_u 0.8501 (0.8088) acc_u 18.7500 (23.1250) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [50/54] time 0.385 (0.454) data 0.254 (0.324) loss_u loss_u 0.8291 (0.8091) acc_u 18.7500 (23.1875) lr 8.5910e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1398
confident_label rate tensor(0.4522, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1418
clean true:1379
clean false:39
clean_rate:0.9724964739069112
noisy true:359
noisy false:1359
after delete: len(clean_dataset) 1418
after delete: len(noisy_dataset) 1718
epoch [112/200] batch [5/44] time 0.408 (0.456) data 0.278 (0.325) loss_x loss_x 1.1631 (1.3433) acc_x 65.6250 (66.2500) lr 8.4357e-04 eta 0:00:17
epoch [112/200] batch [10/44] time 0.463 (0.459) data 0.333 (0.328) loss_x loss_x 1.5654 (1.3605) acc_x 56.2500 (65.0000) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [15/44] time 0.426 (0.464) data 0.295 (0.334) loss_x loss_x 1.3672 (1.3028) acc_x 65.6250 (65.8333) lr 8.4357e-04 eta 0:00:13
epoch [112/200] batch [20/44] time 0.324 (0.453) data 0.194 (0.322) loss_x loss_x 1.3320 (1.2753) acc_x 53.1250 (65.7812) lr 8.4357e-04 eta 0:00:10
epoch [112/200] batch [25/44] time 0.449 (0.448) data 0.319 (0.317) loss_x loss_x 1.4854 (1.2959) acc_x 53.1250 (65.0000) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [30/44] time 0.371 (0.438) data 0.240 (0.308) loss_x loss_x 1.2002 (1.3125) acc_x 71.8750 (65.3125) lr 8.4357e-04 eta 0:00:06
epoch [112/200] batch [35/44] time 0.367 (0.443) data 0.236 (0.312) loss_x loss_x 2.1387 (1.3309) acc_x 62.5000 (65.0893) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [40/44] time 0.498 (0.446) data 0.367 (0.315) loss_x loss_x 1.0381 (1.2872) acc_x 62.5000 (66.1719) lr 8.4357e-04 eta 0:00:01
epoch [112/200] batch [5/53] time 0.565 (0.445) data 0.434 (0.314) loss_u loss_u 0.7446 (0.7844) acc_u 34.3750 (28.7500) lr 8.4357e-04 eta 0:00:21
epoch [112/200] batch [10/53] time 0.439 (0.450) data 0.307 (0.320) loss_u loss_u 0.7344 (0.8151) acc_u 31.2500 (23.4375) lr 8.4357e-04 eta 0:00:19
epoch [112/200] batch [15/53] time 0.821 (0.455) data 0.691 (0.324) loss_u loss_u 0.8052 (0.8071) acc_u 21.8750 (23.7500) lr 8.4357e-04 eta 0:00:17
epoch [112/200] batch [20/53] time 0.512 (0.455) data 0.381 (0.325) loss_u loss_u 0.8599 (0.8111) acc_u 21.8750 (23.4375) lr 8.4357e-04 eta 0:00:15
epoch [112/200] batch [25/53] time 0.478 (0.451) data 0.348 (0.321) loss_u loss_u 0.8447 (0.8096) acc_u 15.6250 (23.5000) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [30/53] time 0.486 (0.454) data 0.355 (0.324) loss_u loss_u 0.8247 (0.8030) acc_u 12.5000 (23.9583) lr 8.4357e-04 eta 0:00:10
epoch [112/200] batch [35/53] time 0.431 (0.454) data 0.299 (0.323) loss_u loss_u 0.8477 (0.8107) acc_u 21.8750 (23.5714) lr 8.4357e-04 eta 0:00:08
epoch [112/200] batch [40/53] time 0.509 (0.454) data 0.379 (0.324) loss_u loss_u 0.7920 (0.8134) acc_u 28.1250 (22.9688) lr 8.4357e-04 eta 0:00:05
epoch [112/200] batch [45/53] time 0.446 (0.454) data 0.315 (0.323) loss_u loss_u 0.8184 (0.8121) acc_u 25.0000 (23.4722) lr 8.4357e-04 eta 0:00:03
epoch [112/200] batch [50/53] time 0.470 (0.453) data 0.339 (0.322) loss_u loss_u 0.7393 (0.8119) acc_u 37.5000 (23.3750) lr 8.4357e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1405
confident_label rate tensor(0.4480, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1405
clean true:1364
clean false:41
clean_rate:0.9708185053380783
noisy true:367
noisy false:1364
after delete: len(clean_dataset) 1405
after delete: len(noisy_dataset) 1731
epoch [113/200] batch [5/43] time 0.388 (0.438) data 0.257 (0.307) loss_x loss_x 1.3623 (1.1579) acc_x 68.7500 (74.3750) lr 8.2807e-04 eta 0:00:16
epoch [113/200] batch [10/43] time 0.426 (0.455) data 0.296 (0.324) loss_x loss_x 1.0615 (1.0633) acc_x 75.0000 (75.3125) lr 8.2807e-04 eta 0:00:15
epoch [113/200] batch [15/43] time 0.467 (0.466) data 0.337 (0.336) loss_x loss_x 1.2295 (1.0810) acc_x 71.8750 (74.5833) lr 8.2807e-04 eta 0:00:13
epoch [113/200] batch [20/43] time 0.491 (0.476) data 0.361 (0.345) loss_x loss_x 1.4043 (1.1526) acc_x 68.7500 (72.5000) lr 8.2807e-04 eta 0:00:10
epoch [113/200] batch [25/43] time 0.513 (0.464) data 0.383 (0.334) loss_x loss_x 1.3135 (1.1589) acc_x 65.6250 (72.5000) lr 8.2807e-04 eta 0:00:08
epoch [113/200] batch [30/43] time 0.452 (0.463) data 0.322 (0.333) loss_x loss_x 1.5059 (1.1918) acc_x 59.3750 (71.8750) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [35/43] time 0.469 (0.464) data 0.339 (0.333) loss_x loss_x 1.0410 (1.1855) acc_x 62.5000 (71.6071) lr 8.2807e-04 eta 0:00:03
epoch [113/200] batch [40/43] time 0.517 (0.465) data 0.387 (0.335) loss_x loss_x 1.4551 (1.1903) acc_x 59.3750 (71.0938) lr 8.2807e-04 eta 0:00:01
epoch [113/200] batch [5/54] time 0.425 (0.457) data 0.293 (0.326) loss_u loss_u 0.9048 (0.8146) acc_u 9.3750 (21.2500) lr 8.2807e-04 eta 0:00:22
epoch [113/200] batch [10/54] time 0.433 (0.454) data 0.301 (0.323) loss_u loss_u 0.7427 (0.8176) acc_u 34.3750 (21.8750) lr 8.2807e-04 eta 0:00:19
epoch [113/200] batch [15/54] time 0.318 (0.452) data 0.188 (0.321) loss_u loss_u 0.7407 (0.8059) acc_u 37.5000 (23.1250) lr 8.2807e-04 eta 0:00:17
epoch [113/200] batch [20/54] time 0.517 (0.448) data 0.386 (0.318) loss_u loss_u 0.8066 (0.8151) acc_u 21.8750 (22.3438) lr 8.2807e-04 eta 0:00:15
epoch [113/200] batch [25/54] time 0.430 (0.447) data 0.299 (0.317) loss_u loss_u 0.8560 (0.8228) acc_u 18.7500 (21.2500) lr 8.2807e-04 eta 0:00:12
epoch [113/200] batch [30/54] time 0.423 (0.445) data 0.292 (0.314) loss_u loss_u 0.8267 (0.8196) acc_u 21.8750 (22.1875) lr 8.2807e-04 eta 0:00:10
epoch [113/200] batch [35/54] time 0.406 (0.444) data 0.275 (0.313) loss_u loss_u 0.8589 (0.8199) acc_u 18.7500 (22.1429) lr 8.2807e-04 eta 0:00:08
epoch [113/200] batch [40/54] time 0.433 (0.446) data 0.303 (0.316) loss_u loss_u 0.7915 (0.8187) acc_u 25.0000 (22.3438) lr 8.2807e-04 eta 0:00:06
epoch [113/200] batch [45/54] time 0.461 (0.446) data 0.329 (0.315) loss_u loss_u 0.8345 (0.8218) acc_u 25.0000 (21.8750) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [50/54] time 0.550 (0.449) data 0.419 (0.318) loss_u loss_u 0.7485 (0.8171) acc_u 25.0000 (22.3750) lr 8.2807e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1442
confident_label rate tensor(0.4426, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1388
clean true:1348
clean false:40
clean_rate:0.9711815561959655
noisy true:346
noisy false:1402
after delete: len(clean_dataset) 1388
after delete: len(noisy_dataset) 1748
epoch [114/200] batch [5/43] time 0.515 (0.493) data 0.385 (0.363) loss_x loss_x 0.9067 (1.1774) acc_x 75.0000 (70.0000) lr 8.1262e-04 eta 0:00:18
epoch [114/200] batch [10/43] time 0.449 (0.471) data 0.319 (0.341) loss_x loss_x 1.3018 (1.2838) acc_x 65.6250 (65.9375) lr 8.1262e-04 eta 0:00:15
epoch [114/200] batch [15/43] time 0.523 (0.471) data 0.392 (0.341) loss_x loss_x 1.5459 (1.2903) acc_x 68.7500 (67.0833) lr 8.1262e-04 eta 0:00:13
epoch [114/200] batch [20/43] time 0.455 (0.466) data 0.324 (0.336) loss_x loss_x 1.7764 (1.2379) acc_x 65.6250 (68.4375) lr 8.1262e-04 eta 0:00:10
epoch [114/200] batch [25/43] time 0.426 (0.469) data 0.296 (0.339) loss_x loss_x 1.0078 (1.1911) acc_x 78.1250 (69.8750) lr 8.1262e-04 eta 0:00:08
epoch [114/200] batch [30/43] time 0.408 (0.466) data 0.278 (0.335) loss_x loss_x 1.2480 (1.1897) acc_x 71.8750 (70.1042) lr 8.1262e-04 eta 0:00:06
epoch [114/200] batch [35/43] time 0.414 (0.463) data 0.283 (0.332) loss_x loss_x 1.1543 (1.1909) acc_x 71.8750 (70.0893) lr 8.1262e-04 eta 0:00:03
epoch [114/200] batch [40/43] time 0.353 (0.457) data 0.224 (0.327) loss_x loss_x 0.9619 (1.1791) acc_x 75.0000 (70.3125) lr 8.1262e-04 eta 0:00:01
epoch [114/200] batch [5/54] time 0.361 (0.454) data 0.230 (0.323) loss_u loss_u 0.7827 (0.7869) acc_u 31.2500 (30.0000) lr 8.1262e-04 eta 0:00:22
epoch [114/200] batch [10/54] time 0.357 (0.455) data 0.226 (0.325) loss_u loss_u 0.7314 (0.7756) acc_u 31.2500 (28.4375) lr 8.1262e-04 eta 0:00:20
epoch [114/200] batch [15/54] time 0.348 (0.451) data 0.216 (0.321) loss_u loss_u 0.8276 (0.7879) acc_u 21.8750 (26.2500) lr 8.1262e-04 eta 0:00:17
epoch [114/200] batch [20/54] time 0.609 (0.451) data 0.477 (0.321) loss_u loss_u 0.8096 (0.7916) acc_u 28.1250 (26.4062) lr 8.1262e-04 eta 0:00:15
epoch [114/200] batch [25/54] time 0.433 (0.451) data 0.302 (0.321) loss_u loss_u 0.7310 (0.7988) acc_u 34.3750 (25.1250) lr 8.1262e-04 eta 0:00:13
epoch [114/200] batch [30/54] time 0.524 (0.451) data 0.393 (0.321) loss_u loss_u 0.7568 (0.8053) acc_u 37.5000 (24.6875) lr 8.1262e-04 eta 0:00:10
epoch [114/200] batch [35/54] time 0.530 (0.452) data 0.399 (0.322) loss_u loss_u 0.8672 (0.8069) acc_u 21.8750 (24.7321) lr 8.1262e-04 eta 0:00:08
epoch [114/200] batch [40/54] time 0.398 (0.450) data 0.267 (0.320) loss_u loss_u 0.8472 (0.8095) acc_u 15.6250 (23.9844) lr 8.1262e-04 eta 0:00:06
epoch [114/200] batch [45/54] time 0.591 (0.452) data 0.461 (0.321) loss_u loss_u 0.7451 (0.8040) acc_u 28.1250 (24.3750) lr 8.1262e-04 eta 0:00:04
epoch [114/200] batch [50/54] time 0.500 (0.452) data 0.369 (0.321) loss_u loss_u 0.6895 (0.8014) acc_u 40.6250 (24.5625) lr 8.1262e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1416
confident_label rate tensor(0.4528, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1420
clean true:1373
clean false:47
clean_rate:0.9669014084507043
noisy true:347
noisy false:1369
after delete: len(clean_dataset) 1420
after delete: len(noisy_dataset) 1716
epoch [115/200] batch [5/44] time 0.466 (0.426) data 0.336 (0.295) loss_x loss_x 0.9380 (1.0673) acc_x 75.0000 (73.7500) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [10/44] time 0.384 (0.435) data 0.254 (0.305) loss_x loss_x 1.1533 (1.0877) acc_x 62.5000 (71.5625) lr 7.9721e-04 eta 0:00:14
epoch [115/200] batch [15/44] time 0.474 (0.441) data 0.343 (0.311) loss_x loss_x 1.4971 (1.1633) acc_x 59.3750 (69.5833) lr 7.9721e-04 eta 0:00:12
epoch [115/200] batch [20/44] time 0.552 (0.450) data 0.420 (0.320) loss_x loss_x 1.2275 (1.1874) acc_x 68.7500 (68.9062) lr 7.9721e-04 eta 0:00:10
epoch [115/200] batch [25/44] time 0.460 (0.461) data 0.330 (0.330) loss_x loss_x 1.0303 (1.1655) acc_x 68.7500 (68.8750) lr 7.9721e-04 eta 0:00:08
epoch [115/200] batch [30/44] time 0.473 (0.465) data 0.342 (0.335) loss_x loss_x 1.4570 (1.1581) acc_x 65.6250 (69.4792) lr 7.9721e-04 eta 0:00:06
epoch [115/200] batch [35/44] time 0.444 (0.469) data 0.313 (0.339) loss_x loss_x 1.0342 (1.1366) acc_x 75.0000 (70.2679) lr 7.9721e-04 eta 0:00:04
epoch [115/200] batch [40/44] time 0.413 (0.471) data 0.283 (0.340) loss_x loss_x 1.0986 (1.1312) acc_x 68.7500 (70.2344) lr 7.9721e-04 eta 0:00:01
epoch [115/200] batch [5/53] time 0.339 (0.459) data 0.208 (0.328) loss_u loss_u 0.7817 (0.8184) acc_u 25.0000 (23.1250) lr 7.9721e-04 eta 0:00:22
epoch [115/200] batch [10/53] time 0.458 (0.456) data 0.327 (0.325) loss_u loss_u 0.7798 (0.8104) acc_u 28.1250 (24.3750) lr 7.9721e-04 eta 0:00:19
epoch [115/200] batch [15/53] time 0.461 (0.456) data 0.329 (0.326) loss_u loss_u 0.8535 (0.8204) acc_u 18.7500 (22.9167) lr 7.9721e-04 eta 0:00:17
epoch [115/200] batch [20/53] time 0.571 (0.462) data 0.440 (0.332) loss_u loss_u 0.8369 (0.8223) acc_u 21.8750 (22.9688) lr 7.9721e-04 eta 0:00:15
epoch [115/200] batch [25/53] time 0.483 (0.460) data 0.352 (0.329) loss_u loss_u 0.7930 (0.8186) acc_u 25.0000 (23.2500) lr 7.9721e-04 eta 0:00:12
epoch [115/200] batch [30/53] time 0.360 (0.456) data 0.228 (0.326) loss_u loss_u 0.8193 (0.8161) acc_u 21.8750 (23.6458) lr 7.9721e-04 eta 0:00:10
epoch [115/200] batch [35/53] time 0.433 (0.458) data 0.302 (0.327) loss_u loss_u 0.7969 (0.8140) acc_u 21.8750 (23.9286) lr 7.9721e-04 eta 0:00:08
epoch [115/200] batch [40/53] time 0.488 (0.456) data 0.357 (0.325) loss_u loss_u 0.7627 (0.8113) acc_u 34.3750 (24.2188) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [45/53] time 0.567 (0.456) data 0.435 (0.325) loss_u loss_u 0.8447 (0.8125) acc_u 15.6250 (24.0278) lr 7.9721e-04 eta 0:00:03
epoch [115/200] batch [50/53] time 0.562 (0.456) data 0.432 (0.325) loss_u loss_u 0.8013 (0.8111) acc_u 34.3750 (24.3125) lr 7.9721e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1444
confident_label rate tensor(0.4381, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1374
clean true:1337
clean false:37
clean_rate:0.9730713245997089
noisy true:355
noisy false:1407
after delete: len(clean_dataset) 1374
after delete: len(noisy_dataset) 1762
epoch [116/200] batch [5/42] time 0.391 (0.426) data 0.261 (0.296) loss_x loss_x 0.7495 (1.0164) acc_x 75.0000 (75.6250) lr 7.8186e-04 eta 0:00:15
epoch [116/200] batch [10/42] time 0.455 (0.446) data 0.325 (0.316) loss_x loss_x 1.2373 (1.0939) acc_x 65.6250 (71.8750) lr 7.8186e-04 eta 0:00:14
epoch [116/200] batch [15/42] time 0.412 (0.435) data 0.281 (0.305) loss_x loss_x 1.1885 (1.1660) acc_x 68.7500 (71.0417) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [20/42] time 0.477 (0.444) data 0.347 (0.313) loss_x loss_x 1.2256 (1.1459) acc_x 75.0000 (72.6562) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [25/42] time 0.317 (0.443) data 0.187 (0.312) loss_x loss_x 1.1865 (1.1195) acc_x 75.0000 (72.7500) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [30/42] time 0.413 (0.442) data 0.282 (0.312) loss_x loss_x 1.6973 (1.1336) acc_x 56.2500 (72.2917) lr 7.8186e-04 eta 0:00:05
epoch [116/200] batch [35/42] time 0.390 (0.452) data 0.260 (0.321) loss_x loss_x 1.6162 (1.1319) acc_x 62.5000 (71.6071) lr 7.8186e-04 eta 0:00:03
epoch [116/200] batch [40/42] time 0.436 (0.450) data 0.305 (0.319) loss_x loss_x 1.5576 (1.1354) acc_x 59.3750 (71.5625) lr 7.8186e-04 eta 0:00:00
epoch [116/200] batch [5/55] time 0.494 (0.456) data 0.363 (0.325) loss_u loss_u 0.8306 (0.8248) acc_u 15.6250 (19.3750) lr 7.8186e-04 eta 0:00:22
epoch [116/200] batch [10/55] time 0.404 (0.460) data 0.272 (0.330) loss_u loss_u 0.8203 (0.8097) acc_u 21.8750 (22.1875) lr 7.8186e-04 eta 0:00:20
epoch [116/200] batch [15/55] time 0.507 (0.460) data 0.376 (0.330) loss_u loss_u 0.7661 (0.8123) acc_u 37.5000 (23.3333) lr 7.8186e-04 eta 0:00:18
epoch [116/200] batch [20/55] time 0.433 (0.460) data 0.301 (0.329) loss_u loss_u 0.8677 (0.8166) acc_u 15.6250 (22.9688) lr 7.8186e-04 eta 0:00:16
epoch [116/200] batch [25/55] time 0.605 (0.464) data 0.474 (0.333) loss_u loss_u 0.7671 (0.8146) acc_u 31.2500 (23.5000) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [30/55] time 0.451 (0.467) data 0.319 (0.336) loss_u loss_u 0.8643 (0.8181) acc_u 25.0000 (23.1250) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [35/55] time 0.362 (0.462) data 0.231 (0.331) loss_u loss_u 0.7505 (0.8137) acc_u 28.1250 (23.6607) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [40/55] time 0.545 (0.459) data 0.415 (0.328) loss_u loss_u 0.8184 (0.8163) acc_u 15.6250 (23.6719) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [45/55] time 0.443 (0.456) data 0.313 (0.325) loss_u loss_u 0.7583 (0.8130) acc_u 28.1250 (24.0972) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [50/55] time 0.388 (0.455) data 0.256 (0.324) loss_u loss_u 0.7607 (0.8118) acc_u 31.2500 (24.4375) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [55/55] time 0.398 (0.453) data 0.267 (0.322) loss_u loss_u 0.8438 (0.8147) acc_u 18.7500 (24.0341) lr 7.8186e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1438
confident_label rate tensor(0.4426, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1388
clean true:1345
clean false:43
clean_rate:0.9690201729106628
noisy true:353
noisy false:1395
after delete: len(clean_dataset) 1388
after delete: len(noisy_dataset) 1748
epoch [117/200] batch [5/43] time 0.431 (0.467) data 0.300 (0.336) loss_x loss_x 1.4580 (1.0318) acc_x 62.5000 (73.7500) lr 7.6655e-04 eta 0:00:17
epoch [117/200] batch [10/43] time 0.439 (0.455) data 0.308 (0.324) loss_x loss_x 1.0098 (1.0800) acc_x 71.8750 (72.8125) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [15/43] time 0.449 (0.438) data 0.319 (0.307) loss_x loss_x 0.9561 (1.1317) acc_x 68.7500 (70.6250) lr 7.6655e-04 eta 0:00:12
epoch [117/200] batch [20/43] time 0.521 (0.442) data 0.390 (0.312) loss_x loss_x 1.6914 (1.2033) acc_x 56.2500 (70.3125) lr 7.6655e-04 eta 0:00:10
epoch [117/200] batch [25/43] time 0.437 (0.441) data 0.306 (0.310) loss_x loss_x 1.1328 (1.1919) acc_x 68.7500 (70.7500) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [30/43] time 0.447 (0.448) data 0.317 (0.317) loss_x loss_x 1.5508 (1.1732) acc_x 65.6250 (70.7292) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [35/43] time 0.656 (0.450) data 0.524 (0.319) loss_x loss_x 0.7505 (1.1680) acc_x 81.2500 (70.9821) lr 7.6655e-04 eta 0:00:03
epoch [117/200] batch [40/43] time 0.523 (0.452) data 0.393 (0.321) loss_x loss_x 1.5225 (1.1954) acc_x 68.7500 (70.4688) lr 7.6655e-04 eta 0:00:01
epoch [117/200] batch [5/54] time 0.310 (0.457) data 0.178 (0.326) loss_u loss_u 0.7856 (0.8245) acc_u 25.0000 (23.7500) lr 7.6655e-04 eta 0:00:22
epoch [117/200] batch [10/54] time 0.491 (0.457) data 0.359 (0.326) loss_u loss_u 0.7881 (0.8193) acc_u 31.2500 (23.4375) lr 7.6655e-04 eta 0:00:20
epoch [117/200] batch [15/54] time 0.353 (0.455) data 0.222 (0.324) loss_u loss_u 0.7539 (0.8151) acc_u 34.3750 (23.5417) lr 7.6655e-04 eta 0:00:17
epoch [117/200] batch [20/54] time 0.303 (0.453) data 0.173 (0.322) loss_u loss_u 0.7832 (0.8125) acc_u 28.1250 (24.3750) lr 7.6655e-04 eta 0:00:15
epoch [117/200] batch [25/54] time 0.767 (0.459) data 0.636 (0.328) loss_u loss_u 0.8384 (0.8147) acc_u 15.6250 (23.7500) lr 7.6655e-04 eta 0:00:13
epoch [117/200] batch [30/54] time 0.365 (0.454) data 0.234 (0.323) loss_u loss_u 0.8184 (0.8117) acc_u 18.7500 (23.3333) lr 7.6655e-04 eta 0:00:10
epoch [117/200] batch [35/54] time 0.405 (0.455) data 0.274 (0.324) loss_u loss_u 0.7549 (0.8127) acc_u 28.1250 (22.9464) lr 7.6655e-04 eta 0:00:08
epoch [117/200] batch [40/54] time 0.389 (0.454) data 0.258 (0.323) loss_u loss_u 0.9131 (0.8132) acc_u 9.3750 (22.8906) lr 7.6655e-04 eta 0:00:06
epoch [117/200] batch [45/54] time 0.420 (0.454) data 0.288 (0.323) loss_u loss_u 0.8721 (0.8137) acc_u 15.6250 (22.9167) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [50/54] time 0.444 (0.452) data 0.313 (0.321) loss_u loss_u 0.8428 (0.8105) acc_u 21.8750 (23.5000) lr 7.6655e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1371
confident_label rate tensor(0.4585, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1438
clean true:1399
clean false:39
clean_rate:0.9728789986091794
noisy true:366
noisy false:1332
after delete: len(clean_dataset) 1438
after delete: len(noisy_dataset) 1698
epoch [118/200] batch [5/44] time 0.460 (0.470) data 0.330 (0.340) loss_x loss_x 1.4561 (1.2241) acc_x 65.6250 (69.3750) lr 7.5131e-04 eta 0:00:18
epoch [118/200] batch [10/44] time 0.467 (0.503) data 0.336 (0.373) loss_x loss_x 1.4336 (1.2443) acc_x 56.2500 (68.4375) lr 7.5131e-04 eta 0:00:17
epoch [118/200] batch [15/44] time 0.446 (0.479) data 0.315 (0.349) loss_x loss_x 0.7378 (1.2294) acc_x 87.5000 (69.5833) lr 7.5131e-04 eta 0:00:13
epoch [118/200] batch [20/44] time 0.432 (0.473) data 0.302 (0.342) loss_x loss_x 1.2852 (1.2604) acc_x 68.7500 (69.2188) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [25/44] time 0.389 (0.465) data 0.259 (0.335) loss_x loss_x 1.4580 (1.2771) acc_x 65.6250 (69.0000) lr 7.5131e-04 eta 0:00:08
epoch [118/200] batch [30/44] time 0.487 (0.471) data 0.357 (0.341) loss_x loss_x 0.8384 (1.2726) acc_x 78.1250 (68.4375) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [35/44] time 0.374 (0.473) data 0.244 (0.343) loss_x loss_x 0.8242 (1.2449) acc_x 81.2500 (69.4643) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [40/44] time 0.471 (0.471) data 0.339 (0.340) loss_x loss_x 1.4287 (1.2721) acc_x 65.6250 (68.1250) lr 7.5131e-04 eta 0:00:01
epoch [118/200] batch [5/53] time 0.454 (0.466) data 0.322 (0.335) loss_u loss_u 0.7539 (0.8239) acc_u 28.1250 (21.2500) lr 7.5131e-04 eta 0:00:22
epoch [118/200] batch [10/53] time 0.602 (0.465) data 0.470 (0.334) loss_u loss_u 0.8765 (0.8184) acc_u 18.7500 (23.1250) lr 7.5131e-04 eta 0:00:19
epoch [118/200] batch [15/53] time 0.425 (0.463) data 0.295 (0.333) loss_u loss_u 0.8218 (0.8280) acc_u 21.8750 (21.4583) lr 7.5131e-04 eta 0:00:17
epoch [118/200] batch [20/53] time 0.451 (0.462) data 0.319 (0.331) loss_u loss_u 0.8550 (0.8286) acc_u 21.8750 (22.0312) lr 7.5131e-04 eta 0:00:15
epoch [118/200] batch [25/53] time 0.430 (0.463) data 0.300 (0.332) loss_u loss_u 0.8203 (0.8304) acc_u 21.8750 (21.6250) lr 7.5131e-04 eta 0:00:12
epoch [118/200] batch [30/53] time 0.410 (0.458) data 0.279 (0.327) loss_u loss_u 0.8208 (0.8333) acc_u 15.6250 (21.6667) lr 7.5131e-04 eta 0:00:10
epoch [118/200] batch [35/53] time 0.418 (0.457) data 0.287 (0.327) loss_u loss_u 0.8145 (0.8315) acc_u 18.7500 (21.2500) lr 7.5131e-04 eta 0:00:08
epoch [118/200] batch [40/53] time 0.390 (0.456) data 0.260 (0.325) loss_u loss_u 0.7764 (0.8263) acc_u 28.1250 (22.0312) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [45/53] time 0.449 (0.454) data 0.318 (0.323) loss_u loss_u 0.8682 (0.8274) acc_u 15.6250 (22.0139) lr 7.5131e-04 eta 0:00:03
epoch [118/200] batch [50/53] time 0.436 (0.454) data 0.306 (0.323) loss_u loss_u 0.8809 (0.8310) acc_u 12.5000 (21.5000) lr 7.5131e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1392
confident_label rate tensor(0.4570, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1433
clean true:1382
clean false:51
clean_rate:0.9644103279832519
noisy true:362
noisy false:1341
after delete: len(clean_dataset) 1433
after delete: len(noisy_dataset) 1703
epoch [119/200] batch [5/44] time 0.353 (0.506) data 0.223 (0.376) loss_x loss_x 1.2227 (1.4438) acc_x 71.8750 (65.0000) lr 7.3613e-04 eta 0:00:19
epoch [119/200] batch [10/44] time 0.423 (0.477) data 0.292 (0.346) loss_x loss_x 1.0430 (1.3592) acc_x 75.0000 (67.1875) lr 7.3613e-04 eta 0:00:16
epoch [119/200] batch [15/44] time 0.369 (0.460) data 0.239 (0.329) loss_x loss_x 1.2920 (1.2932) acc_x 59.3750 (68.3333) lr 7.3613e-04 eta 0:00:13
epoch [119/200] batch [20/44] time 0.566 (0.458) data 0.435 (0.327) loss_x loss_x 1.0977 (1.2397) acc_x 71.8750 (69.2188) lr 7.3613e-04 eta 0:00:10
epoch [119/200] batch [25/44] time 0.389 (0.455) data 0.259 (0.324) loss_x loss_x 0.9458 (1.2046) acc_x 78.1250 (69.5000) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [30/44] time 0.393 (0.450) data 0.263 (0.319) loss_x loss_x 0.7832 (1.1875) acc_x 78.1250 (69.7917) lr 7.3613e-04 eta 0:00:06
epoch [119/200] batch [35/44] time 0.468 (0.451) data 0.338 (0.320) loss_x loss_x 0.9165 (1.1821) acc_x 81.2500 (70.0893) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [40/44] time 0.464 (0.451) data 0.334 (0.321) loss_x loss_x 1.0312 (1.1536) acc_x 71.8750 (70.4688) lr 7.3613e-04 eta 0:00:01
epoch [119/200] batch [5/53] time 0.439 (0.455) data 0.307 (0.324) loss_u loss_u 0.7290 (0.7971) acc_u 40.6250 (26.8750) lr 7.3613e-04 eta 0:00:21
epoch [119/200] batch [10/53] time 0.343 (0.455) data 0.213 (0.324) loss_u loss_u 0.8286 (0.8131) acc_u 28.1250 (25.6250) lr 7.3613e-04 eta 0:00:19
epoch [119/200] batch [15/53] time 0.550 (0.453) data 0.418 (0.322) loss_u loss_u 0.8052 (0.8018) acc_u 25.0000 (26.0417) lr 7.3613e-04 eta 0:00:17
epoch [119/200] batch [20/53] time 0.399 (0.450) data 0.267 (0.319) loss_u loss_u 0.8086 (0.8084) acc_u 25.0000 (25.3125) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [25/53] time 0.445 (0.453) data 0.315 (0.322) loss_u loss_u 0.7935 (0.8066) acc_u 25.0000 (25.1250) lr 7.3613e-04 eta 0:00:12
epoch [119/200] batch [30/53] time 0.805 (0.462) data 0.673 (0.332) loss_u loss_u 0.8545 (0.8136) acc_u 18.7500 (24.1667) lr 7.3613e-04 eta 0:00:10
epoch [119/200] batch [35/53] time 0.408 (0.461) data 0.277 (0.330) loss_u loss_u 0.7808 (0.8158) acc_u 28.1250 (23.6607) lr 7.3613e-04 eta 0:00:08
epoch [119/200] batch [40/53] time 0.362 (0.458) data 0.230 (0.327) loss_u loss_u 0.7700 (0.8157) acc_u 31.2500 (23.5156) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [45/53] time 0.387 (0.457) data 0.255 (0.326) loss_u loss_u 0.8267 (0.8125) acc_u 18.7500 (23.5417) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [50/53] time 0.420 (0.455) data 0.288 (0.324) loss_u loss_u 0.8145 (0.8121) acc_u 25.0000 (23.4375) lr 7.3613e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1468
confident_label rate tensor(0.4369, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1370
clean true:1334
clean false:36
clean_rate:0.9737226277372263
noisy true:334
noisy false:1432
after delete: len(clean_dataset) 1370
after delete: len(noisy_dataset) 1766
epoch [120/200] batch [5/42] time 0.523 (0.431) data 0.393 (0.301) loss_x loss_x 1.0479 (1.0789) acc_x 75.0000 (72.5000) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [10/42] time 0.490 (0.439) data 0.360 (0.309) loss_x loss_x 1.1855 (1.0079) acc_x 75.0000 (74.0625) lr 7.2101e-04 eta 0:00:14
epoch [120/200] batch [15/42] time 0.429 (0.435) data 0.299 (0.305) loss_x loss_x 1.3193 (1.0480) acc_x 71.8750 (75.6250) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [20/42] time 0.457 (0.451) data 0.327 (0.321) loss_x loss_x 1.4834 (1.1092) acc_x 71.8750 (74.6875) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [25/42] time 0.387 (0.449) data 0.256 (0.319) loss_x loss_x 1.5723 (1.1246) acc_x 62.5000 (73.5000) lr 7.2101e-04 eta 0:00:07
epoch [120/200] batch [30/42] time 0.495 (0.457) data 0.364 (0.327) loss_x loss_x 1.4561 (1.1055) acc_x 59.3750 (73.3333) lr 7.2101e-04 eta 0:00:05
epoch [120/200] batch [35/42] time 0.436 (0.459) data 0.304 (0.328) loss_x loss_x 1.0518 (1.1131) acc_x 78.1250 (73.4821) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [40/42] time 0.454 (0.460) data 0.324 (0.329) loss_x loss_x 1.0713 (1.1177) acc_x 75.0000 (73.5938) lr 7.2101e-04 eta 0:00:00
epoch [120/200] batch [5/55] time 0.606 (0.468) data 0.474 (0.337) loss_u loss_u 0.8384 (0.8152) acc_u 21.8750 (25.0000) lr 7.2101e-04 eta 0:00:23
epoch [120/200] batch [10/55] time 0.414 (0.464) data 0.283 (0.334) loss_u loss_u 0.6543 (0.7917) acc_u 50.0000 (26.5625) lr 7.2101e-04 eta 0:00:20
epoch [120/200] batch [15/55] time 0.381 (0.462) data 0.249 (0.332) loss_u loss_u 0.8721 (0.8062) acc_u 18.7500 (25.2083) lr 7.2101e-04 eta 0:00:18
epoch [120/200] batch [20/55] time 0.517 (0.461) data 0.387 (0.330) loss_u loss_u 0.7661 (0.7995) acc_u 25.0000 (25.7812) lr 7.2101e-04 eta 0:00:16
epoch [120/200] batch [25/55] time 0.479 (0.460) data 0.347 (0.329) loss_u loss_u 0.8228 (0.8033) acc_u 25.0000 (25.5000) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [30/55] time 0.352 (0.454) data 0.220 (0.323) loss_u loss_u 0.6689 (0.7969) acc_u 40.6250 (25.7292) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [35/55] time 0.503 (0.453) data 0.372 (0.322) loss_u loss_u 0.8101 (0.8003) acc_u 21.8750 (25.0000) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [40/55] time 0.454 (0.454) data 0.323 (0.323) loss_u loss_u 0.7046 (0.7999) acc_u 46.8750 (25.3906) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [45/55] time 0.433 (0.457) data 0.302 (0.327) loss_u loss_u 0.8657 (0.8029) acc_u 21.8750 (24.9306) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [50/55] time 0.462 (0.458) data 0.332 (0.327) loss_u loss_u 0.9136 (0.8068) acc_u 6.2500 (24.0625) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [55/55] time 0.460 (0.460) data 0.329 (0.329) loss_u loss_u 0.8882 (0.8104) acc_u 18.7500 (23.6932) lr 7.2101e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1433
confident_label rate tensor(0.4458, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1398
clean true:1357
clean false:41
clean_rate:0.9706723891273248
noisy true:346
noisy false:1392
after delete: len(clean_dataset) 1398
after delete: len(noisy_dataset) 1738
epoch [121/200] batch [5/43] time 0.496 (0.504) data 0.366 (0.373) loss_x loss_x 0.8896 (1.1008) acc_x 75.0000 (71.8750) lr 7.0596e-04 eta 0:00:19
epoch [121/200] batch [10/43] time 0.432 (0.471) data 0.301 (0.340) loss_x loss_x 0.8965 (1.1990) acc_x 84.3750 (69.0625) lr 7.0596e-04 eta 0:00:15
epoch [121/200] batch [15/43] time 0.675 (0.484) data 0.545 (0.354) loss_x loss_x 0.6772 (1.1036) acc_x 81.2500 (71.4583) lr 7.0596e-04 eta 0:00:13
epoch [121/200] batch [20/43] time 0.618 (0.491) data 0.487 (0.361) loss_x loss_x 1.0840 (1.0686) acc_x 75.0000 (71.7188) lr 7.0596e-04 eta 0:00:11
epoch [121/200] batch [25/43] time 0.434 (0.494) data 0.304 (0.364) loss_x loss_x 0.7358 (1.0591) acc_x 87.5000 (72.8750) lr 7.0596e-04 eta 0:00:08
epoch [121/200] batch [30/43] time 0.431 (0.490) data 0.301 (0.359) loss_x loss_x 0.8789 (1.0621) acc_x 78.1250 (72.8125) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [35/43] time 0.475 (0.486) data 0.344 (0.355) loss_x loss_x 1.5791 (1.0771) acc_x 56.2500 (72.0536) lr 7.0596e-04 eta 0:00:03
epoch [121/200] batch [40/43] time 0.388 (0.487) data 0.258 (0.356) loss_x loss_x 1.1885 (1.0785) acc_x 65.6250 (72.1875) lr 7.0596e-04 eta 0:00:01
epoch [121/200] batch [5/54] time 0.393 (0.481) data 0.262 (0.350) loss_u loss_u 0.7686 (0.8220) acc_u 34.3750 (23.7500) lr 7.0596e-04 eta 0:00:23
epoch [121/200] batch [10/54] time 0.505 (0.482) data 0.373 (0.351) loss_u loss_u 0.8413 (0.8323) acc_u 21.8750 (21.8750) lr 7.0596e-04 eta 0:00:21
epoch [121/200] batch [15/54] time 0.472 (0.478) data 0.342 (0.348) loss_u loss_u 0.7964 (0.8262) acc_u 21.8750 (21.6667) lr 7.0596e-04 eta 0:00:18
epoch [121/200] batch [20/54] time 0.414 (0.475) data 0.282 (0.345) loss_u loss_u 0.7905 (0.8203) acc_u 31.2500 (22.5000) lr 7.0596e-04 eta 0:00:16
epoch [121/200] batch [25/54] time 0.453 (0.472) data 0.321 (0.342) loss_u loss_u 0.7578 (0.8191) acc_u 25.0000 (22.6250) lr 7.0596e-04 eta 0:00:13
epoch [121/200] batch [30/54] time 0.364 (0.469) data 0.233 (0.338) loss_u loss_u 0.8862 (0.8239) acc_u 12.5000 (22.0833) lr 7.0596e-04 eta 0:00:11
epoch [121/200] batch [35/54] time 0.719 (0.476) data 0.589 (0.345) loss_u loss_u 0.8213 (0.8215) acc_u 18.7500 (22.2321) lr 7.0596e-04 eta 0:00:09
epoch [121/200] batch [40/54] time 0.355 (0.473) data 0.223 (0.342) loss_u loss_u 0.7451 (0.8208) acc_u 34.3750 (22.4219) lr 7.0596e-04 eta 0:00:06
epoch [121/200] batch [45/54] time 0.530 (0.475) data 0.398 (0.344) loss_u loss_u 0.7671 (0.8203) acc_u 25.0000 (22.2917) lr 7.0596e-04 eta 0:00:04
epoch [121/200] batch [50/54] time 0.346 (0.471) data 0.215 (0.340) loss_u loss_u 0.7690 (0.8138) acc_u 31.2500 (23.2500) lr 7.0596e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1424
confident_label rate tensor(0.4458, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1398
clean true:1353
clean false:45
clean_rate:0.9678111587982833
noisy true:359
noisy false:1379
after delete: len(clean_dataset) 1398
after delete: len(noisy_dataset) 1738
epoch [122/200] batch [5/43] time 0.551 (0.471) data 0.420 (0.341) loss_x loss_x 1.2334 (1.0091) acc_x 65.6250 (72.5000) lr 6.9098e-04 eta 0:00:17
epoch [122/200] batch [10/43] time 0.416 (0.448) data 0.286 (0.318) loss_x loss_x 1.0791 (0.9964) acc_x 75.0000 (75.6250) lr 6.9098e-04 eta 0:00:14
epoch [122/200] batch [15/43] time 0.481 (0.441) data 0.351 (0.310) loss_x loss_x 1.4521 (1.1032) acc_x 56.2500 (71.0417) lr 6.9098e-04 eta 0:00:12
epoch [122/200] batch [20/43] time 0.676 (0.459) data 0.546 (0.329) loss_x loss_x 0.9609 (1.0482) acc_x 81.2500 (73.5938) lr 6.9098e-04 eta 0:00:10
epoch [122/200] batch [25/43] time 0.535 (0.476) data 0.404 (0.345) loss_x loss_x 1.0088 (1.0558) acc_x 75.0000 (72.8750) lr 6.9098e-04 eta 0:00:08
epoch [122/200] batch [30/43] time 0.423 (0.468) data 0.292 (0.338) loss_x loss_x 1.1377 (1.0957) acc_x 75.0000 (72.6042) lr 6.9098e-04 eta 0:00:06
epoch [122/200] batch [35/43] time 0.357 (0.465) data 0.227 (0.335) loss_x loss_x 1.0947 (1.0985) acc_x 71.8750 (72.1429) lr 6.9098e-04 eta 0:00:03
epoch [122/200] batch [40/43] time 0.473 (0.471) data 0.341 (0.341) loss_x loss_x 1.2090 (1.1275) acc_x 68.7500 (71.7969) lr 6.9098e-04 eta 0:00:01
epoch [122/200] batch [5/54] time 0.383 (0.470) data 0.252 (0.339) loss_u loss_u 0.7808 (0.8199) acc_u 28.1250 (20.6250) lr 6.9098e-04 eta 0:00:23
epoch [122/200] batch [10/54] time 0.450 (0.475) data 0.320 (0.344) loss_u loss_u 0.7563 (0.8214) acc_u 34.3750 (22.1875) lr 6.9098e-04 eta 0:00:20
epoch [122/200] batch [15/54] time 0.411 (0.472) data 0.280 (0.342) loss_u loss_u 0.8560 (0.8225) acc_u 18.7500 (23.1250) lr 6.9098e-04 eta 0:00:18
epoch [122/200] batch [20/54] time 0.396 (0.468) data 0.264 (0.337) loss_u loss_u 0.7305 (0.8228) acc_u 34.3750 (22.5000) lr 6.9098e-04 eta 0:00:15
epoch [122/200] batch [25/54] time 0.352 (0.466) data 0.222 (0.335) loss_u loss_u 0.8286 (0.8174) acc_u 28.1250 (22.8750) lr 6.9098e-04 eta 0:00:13
epoch [122/200] batch [30/54] time 0.402 (0.467) data 0.272 (0.336) loss_u loss_u 0.8179 (0.8147) acc_u 15.6250 (22.7083) lr 6.9098e-04 eta 0:00:11
epoch [122/200] batch [35/54] time 0.408 (0.467) data 0.277 (0.337) loss_u loss_u 0.7915 (0.8153) acc_u 31.2500 (22.5000) lr 6.9098e-04 eta 0:00:08
epoch [122/200] batch [40/54] time 0.380 (0.464) data 0.249 (0.333) loss_u loss_u 0.8560 (0.8181) acc_u 18.7500 (22.1875) lr 6.9098e-04 eta 0:00:06
epoch [122/200] batch [45/54] time 0.362 (0.462) data 0.232 (0.332) loss_u loss_u 0.8516 (0.8222) acc_u 15.6250 (21.5278) lr 6.9098e-04 eta 0:00:04
epoch [122/200] batch [50/54] time 0.476 (0.462) data 0.345 (0.332) loss_u loss_u 0.7983 (0.8224) acc_u 21.8750 (21.6875) lr 6.9098e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1415
confident_label rate tensor(0.4509, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1414
clean true:1369
clean false:45
clean_rate:0.9681753889674681
noisy true:352
noisy false:1370
after delete: len(clean_dataset) 1414
after delete: len(noisy_dataset) 1722
epoch [123/200] batch [5/44] time 0.476 (0.452) data 0.345 (0.321) loss_x loss_x 1.0859 (1.0976) acc_x 71.8750 (71.8750) lr 6.7608e-04 eta 0:00:17
epoch [123/200] batch [10/44] time 0.362 (0.445) data 0.232 (0.314) loss_x loss_x 1.0693 (1.0401) acc_x 65.6250 (72.8125) lr 6.7608e-04 eta 0:00:15
epoch [123/200] batch [15/44] time 0.418 (0.461) data 0.288 (0.330) loss_x loss_x 1.1836 (1.1799) acc_x 65.6250 (69.7917) lr 6.7608e-04 eta 0:00:13
epoch [123/200] batch [20/44] time 0.443 (0.444) data 0.313 (0.313) loss_x loss_x 1.5713 (1.2407) acc_x 62.5000 (69.2188) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [25/44] time 0.559 (0.453) data 0.428 (0.323) loss_x loss_x 1.1260 (1.2010) acc_x 78.1250 (70.8750) lr 6.7608e-04 eta 0:00:08
epoch [123/200] batch [30/44] time 0.493 (0.455) data 0.363 (0.324) loss_x loss_x 1.2598 (1.2072) acc_x 59.3750 (70.3125) lr 6.7608e-04 eta 0:00:06
epoch [123/200] batch [35/44] time 0.356 (0.445) data 0.226 (0.315) loss_x loss_x 1.1357 (1.2096) acc_x 62.5000 (70.5357) lr 6.7608e-04 eta 0:00:04
epoch [123/200] batch [40/44] time 0.545 (0.447) data 0.415 (0.316) loss_x loss_x 1.0195 (1.1839) acc_x 68.7500 (71.3281) lr 6.7608e-04 eta 0:00:01
epoch [123/200] batch [5/53] time 0.607 (0.457) data 0.475 (0.326) loss_u loss_u 0.8462 (0.8473) acc_u 15.6250 (19.3750) lr 6.7608e-04 eta 0:00:21
epoch [123/200] batch [10/53] time 0.433 (0.455) data 0.301 (0.324) loss_u loss_u 0.7812 (0.8376) acc_u 28.1250 (20.6250) lr 6.7608e-04 eta 0:00:19
epoch [123/200] batch [15/53] time 0.429 (0.456) data 0.298 (0.325) loss_u loss_u 0.8555 (0.8422) acc_u 15.6250 (19.7917) lr 6.7608e-04 eta 0:00:17
epoch [123/200] batch [20/53] time 0.335 (0.452) data 0.203 (0.321) loss_u loss_u 0.8511 (0.8345) acc_u 25.0000 (21.5625) lr 6.7608e-04 eta 0:00:14
epoch [123/200] batch [25/53] time 0.461 (0.457) data 0.330 (0.326) loss_u loss_u 0.9058 (0.8288) acc_u 12.5000 (21.8750) lr 6.7608e-04 eta 0:00:12
epoch [123/200] batch [30/53] time 0.553 (0.458) data 0.421 (0.327) loss_u loss_u 0.7646 (0.8222) acc_u 34.3750 (22.8125) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [35/53] time 0.304 (0.456) data 0.173 (0.325) loss_u loss_u 0.8174 (0.8217) acc_u 25.0000 (22.8571) lr 6.7608e-04 eta 0:00:08
epoch [123/200] batch [40/53] time 0.427 (0.457) data 0.296 (0.326) loss_u loss_u 0.8130 (0.8186) acc_u 25.0000 (22.9688) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [45/53] time 0.353 (0.455) data 0.223 (0.324) loss_u loss_u 0.7373 (0.8156) acc_u 34.3750 (23.3333) lr 6.7608e-04 eta 0:00:03
epoch [123/200] batch [50/53] time 0.496 (0.457) data 0.365 (0.326) loss_u loss_u 0.8945 (0.8167) acc_u 15.6250 (23.3125) lr 6.7608e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1363
confident_label rate tensor(0.4643, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1456
clean true:1408
clean false:48
clean_rate:0.967032967032967
noisy true:365
noisy false:1315
after delete: len(clean_dataset) 1456
after delete: len(noisy_dataset) 1680
epoch [124/200] batch [5/45] time 0.458 (0.466) data 0.327 (0.335) loss_x loss_x 1.6709 (1.2957) acc_x 59.3750 (68.1250) lr 6.6126e-04 eta 0:00:18
epoch [124/200] batch [10/45] time 0.452 (0.459) data 0.321 (0.328) loss_x loss_x 0.9688 (1.1841) acc_x 71.8750 (70.6250) lr 6.6126e-04 eta 0:00:16
epoch [124/200] batch [15/45] time 0.524 (0.466) data 0.391 (0.335) loss_x loss_x 0.9634 (1.1737) acc_x 75.0000 (71.8750) lr 6.6126e-04 eta 0:00:13
epoch [124/200] batch [20/45] time 0.463 (0.477) data 0.332 (0.346) loss_x loss_x 1.0742 (1.1838) acc_x 75.0000 (71.2500) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [25/45] time 0.528 (0.471) data 0.397 (0.340) loss_x loss_x 0.8916 (1.1775) acc_x 78.1250 (71.2500) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [30/45] time 0.447 (0.469) data 0.318 (0.339) loss_x loss_x 0.8359 (1.1473) acc_x 78.1250 (71.7708) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [35/45] time 0.500 (0.462) data 0.368 (0.332) loss_x loss_x 1.9092 (1.1887) acc_x 59.3750 (70.6250) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [40/45] time 0.507 (0.464) data 0.377 (0.334) loss_x loss_x 1.3252 (1.1906) acc_x 68.7500 (70.4688) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [45/45] time 0.492 (0.462) data 0.361 (0.331) loss_x loss_x 1.2197 (1.1755) acc_x 71.8750 (70.6944) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/52] time 0.407 (0.460) data 0.275 (0.329) loss_u loss_u 0.7920 (0.8105) acc_u 25.0000 (22.5000) lr 6.6126e-04 eta 0:00:21
epoch [124/200] batch [10/52] time 0.478 (0.463) data 0.347 (0.332) loss_u loss_u 0.7739 (0.7935) acc_u 28.1250 (26.2500) lr 6.6126e-04 eta 0:00:19
epoch [124/200] batch [15/52] time 0.357 (0.458) data 0.226 (0.327) loss_u loss_u 0.8286 (0.8088) acc_u 25.0000 (24.5833) lr 6.6126e-04 eta 0:00:16
epoch [124/200] batch [20/52] time 0.637 (0.464) data 0.505 (0.333) loss_u loss_u 0.7935 (0.8150) acc_u 21.8750 (23.2812) lr 6.6126e-04 eta 0:00:14
epoch [124/200] batch [25/52] time 0.422 (0.464) data 0.291 (0.333) loss_u loss_u 0.8267 (0.8164) acc_u 25.0000 (23.5000) lr 6.6126e-04 eta 0:00:12
epoch [124/200] batch [30/52] time 0.601 (0.467) data 0.470 (0.336) loss_u loss_u 0.8340 (0.8154) acc_u 18.7500 (23.3333) lr 6.6126e-04 eta 0:00:10
epoch [124/200] batch [35/52] time 0.671 (0.468) data 0.540 (0.337) loss_u loss_u 0.7749 (0.8150) acc_u 21.8750 (23.2143) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [40/52] time 0.440 (0.467) data 0.310 (0.336) loss_u loss_u 0.7896 (0.8154) acc_u 31.2500 (23.4375) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [45/52] time 0.549 (0.466) data 0.418 (0.335) loss_u loss_u 0.7295 (0.8136) acc_u 34.3750 (23.4028) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [50/52] time 0.528 (0.468) data 0.397 (0.337) loss_u loss_u 0.8311 (0.8132) acc_u 21.8750 (23.6250) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1408
confident_label rate tensor(0.4541, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1424
clean true:1379
clean false:45
clean_rate:0.9683988764044944
noisy true:349
noisy false:1363
after delete: len(clean_dataset) 1424
after delete: len(noisy_dataset) 1712
epoch [125/200] batch [5/44] time 0.424 (0.424) data 0.293 (0.293) loss_x loss_x 1.2441 (1.1811) acc_x 65.6250 (69.3750) lr 6.4653e-04 eta 0:00:16
epoch [125/200] batch [10/44] time 0.489 (0.423) data 0.357 (0.293) loss_x loss_x 1.0664 (1.1497) acc_x 68.7500 (70.6250) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [15/44] time 0.451 (0.443) data 0.320 (0.313) loss_x loss_x 0.9995 (1.1505) acc_x 78.1250 (70.8333) lr 6.4653e-04 eta 0:00:12
epoch [125/200] batch [20/44] time 0.497 (0.444) data 0.367 (0.313) loss_x loss_x 0.5820 (1.1670) acc_x 87.5000 (70.7812) lr 6.4653e-04 eta 0:00:10
epoch [125/200] batch [25/44] time 0.447 (0.446) data 0.316 (0.316) loss_x loss_x 1.1309 (1.1637) acc_x 62.5000 (70.2500) lr 6.4653e-04 eta 0:00:08
epoch [125/200] batch [30/44] time 0.477 (0.447) data 0.347 (0.316) loss_x loss_x 1.3193 (1.1948) acc_x 65.6250 (69.5833) lr 6.4653e-04 eta 0:00:06
epoch [125/200] batch [35/44] time 0.513 (0.452) data 0.383 (0.321) loss_x loss_x 1.0703 (1.1981) acc_x 71.8750 (69.7321) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [40/44] time 0.411 (0.447) data 0.280 (0.317) loss_x loss_x 1.2246 (1.2172) acc_x 71.8750 (69.3750) lr 6.4653e-04 eta 0:00:01
epoch [125/200] batch [5/53] time 0.531 (0.452) data 0.399 (0.321) loss_u loss_u 0.8521 (0.8247) acc_u 18.7500 (22.5000) lr 6.4653e-04 eta 0:00:21
epoch [125/200] batch [10/53] time 0.433 (0.453) data 0.302 (0.322) loss_u loss_u 0.8311 (0.8047) acc_u 25.0000 (26.2500) lr 6.4653e-04 eta 0:00:19
epoch [125/200] batch [15/53] time 0.419 (0.451) data 0.287 (0.320) loss_u loss_u 0.8062 (0.8182) acc_u 25.0000 (24.3750) lr 6.4653e-04 eta 0:00:17
epoch [125/200] batch [20/53] time 0.404 (0.450) data 0.273 (0.319) loss_u loss_u 0.8115 (0.8206) acc_u 15.6250 (23.1250) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [25/53] time 0.361 (0.447) data 0.231 (0.317) loss_u loss_u 0.7676 (0.8070) acc_u 37.5000 (24.8750) lr 6.4653e-04 eta 0:00:12
epoch [125/200] batch [30/53] time 0.437 (0.451) data 0.306 (0.320) loss_u loss_u 0.7656 (0.8074) acc_u 25.0000 (24.3750) lr 6.4653e-04 eta 0:00:10
epoch [125/200] batch [35/53] time 0.561 (0.456) data 0.430 (0.325) loss_u loss_u 0.8789 (0.8111) acc_u 12.5000 (23.6607) lr 6.4653e-04 eta 0:00:08
epoch [125/200] batch [40/53] time 0.420 (0.457) data 0.290 (0.326) loss_u loss_u 0.8569 (0.8128) acc_u 12.5000 (23.3594) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [45/53] time 0.416 (0.458) data 0.284 (0.327) loss_u loss_u 0.7925 (0.8117) acc_u 28.1250 (23.5417) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [50/53] time 0.513 (0.457) data 0.382 (0.326) loss_u loss_u 0.7622 (0.8100) acc_u 34.3750 (23.4375) lr 6.4653e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1370
confident_label rate tensor(0.4598, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1442
clean true:1398
clean false:44
clean_rate:0.9694868238557559
noisy true:368
noisy false:1326
after delete: len(clean_dataset) 1442
after delete: len(noisy_dataset) 1694
epoch [126/200] batch [5/45] time 0.436 (0.486) data 0.306 (0.355) loss_x loss_x 1.1328 (1.2365) acc_x 71.8750 (68.1250) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [10/45] time 0.578 (0.460) data 0.448 (0.329) loss_x loss_x 1.7178 (1.3214) acc_x 68.7500 (67.1875) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [15/45] time 0.413 (0.464) data 0.282 (0.334) loss_x loss_x 1.2334 (1.2142) acc_x 68.7500 (68.7500) lr 6.3188e-04 eta 0:00:13
epoch [126/200] batch [20/45] time 0.573 (0.472) data 0.443 (0.341) loss_x loss_x 1.1104 (1.2041) acc_x 71.8750 (68.4375) lr 6.3188e-04 eta 0:00:11
epoch [126/200] batch [25/45] time 0.480 (0.472) data 0.349 (0.342) loss_x loss_x 0.8564 (1.1697) acc_x 78.1250 (69.2500) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [30/45] time 0.395 (0.463) data 0.265 (0.332) loss_x loss_x 0.9780 (1.1754) acc_x 81.2500 (69.7917) lr 6.3188e-04 eta 0:00:06
epoch [126/200] batch [35/45] time 0.450 (0.455) data 0.318 (0.325) loss_x loss_x 1.1406 (1.1546) acc_x 78.1250 (70.2679) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [40/45] time 0.428 (0.453) data 0.297 (0.323) loss_x loss_x 0.8467 (1.1425) acc_x 78.1250 (70.4688) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [45/45] time 0.516 (0.455) data 0.385 (0.325) loss_x loss_x 1.0430 (1.1527) acc_x 65.6250 (70.1389) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/52] time 0.372 (0.454) data 0.242 (0.323) loss_u loss_u 0.8550 (0.7806) acc_u 18.7500 (28.1250) lr 6.3188e-04 eta 0:00:21
epoch [126/200] batch [10/52] time 0.382 (0.455) data 0.250 (0.324) loss_u loss_u 0.8511 (0.8107) acc_u 18.7500 (23.1250) lr 6.3188e-04 eta 0:00:19
epoch [126/200] batch [15/52] time 0.459 (0.453) data 0.329 (0.322) loss_u loss_u 0.8389 (0.8143) acc_u 21.8750 (22.5000) lr 6.3188e-04 eta 0:00:16
epoch [126/200] batch [20/52] time 0.527 (0.450) data 0.397 (0.319) loss_u loss_u 0.8228 (0.8084) acc_u 21.8750 (23.5938) lr 6.3188e-04 eta 0:00:14
epoch [126/200] batch [25/52] time 0.692 (0.452) data 0.562 (0.321) loss_u loss_u 0.8413 (0.8192) acc_u 25.0000 (22.2500) lr 6.3188e-04 eta 0:00:12
epoch [126/200] batch [30/52] time 0.408 (0.453) data 0.277 (0.323) loss_u loss_u 0.8525 (0.8195) acc_u 18.7500 (22.2917) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [35/52] time 0.423 (0.451) data 0.292 (0.321) loss_u loss_u 0.8193 (0.8243) acc_u 25.0000 (21.6071) lr 6.3188e-04 eta 0:00:07
epoch [126/200] batch [40/52] time 0.422 (0.453) data 0.292 (0.323) loss_u loss_u 0.8647 (0.8212) acc_u 15.6250 (21.7969) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [45/52] time 0.536 (0.452) data 0.405 (0.321) loss_u loss_u 0.8174 (0.8236) acc_u 21.8750 (21.3889) lr 6.3188e-04 eta 0:00:03
epoch [126/200] batch [50/52] time 0.573 (0.452) data 0.442 (0.321) loss_u loss_u 0.8301 (0.8192) acc_u 28.1250 (22.1875) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1417
confident_label rate tensor(0.4394, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1378
clean true:1343
clean false:35
clean_rate:0.974600870827286
noisy true:376
noisy false:1382
after delete: len(clean_dataset) 1378
after delete: len(noisy_dataset) 1758
epoch [127/200] batch [5/43] time 0.432 (0.440) data 0.300 (0.309) loss_x loss_x 0.8623 (1.1130) acc_x 78.1250 (73.7500) lr 6.1732e-04 eta 0:00:16
epoch [127/200] batch [10/43] time 0.527 (0.435) data 0.396 (0.304) loss_x loss_x 0.9761 (1.0828) acc_x 71.8750 (73.7500) lr 6.1732e-04 eta 0:00:14
epoch [127/200] batch [15/43] time 0.434 (0.433) data 0.303 (0.303) loss_x loss_x 0.8721 (1.1040) acc_x 75.0000 (73.1250) lr 6.1732e-04 eta 0:00:12
epoch [127/200] batch [20/43] time 0.418 (0.443) data 0.288 (0.312) loss_x loss_x 0.8848 (1.1198) acc_x 68.7500 (72.6562) lr 6.1732e-04 eta 0:00:10
epoch [127/200] batch [25/43] time 0.443 (0.441) data 0.313 (0.311) loss_x loss_x 1.5010 (1.1292) acc_x 75.0000 (72.5000) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [30/43] time 0.457 (0.446) data 0.326 (0.315) loss_x loss_x 1.1611 (1.1195) acc_x 75.0000 (73.0208) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [35/43] time 0.505 (0.463) data 0.374 (0.332) loss_x loss_x 0.9502 (1.1067) acc_x 71.8750 (73.5714) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [40/43] time 0.372 (0.466) data 0.241 (0.335) loss_x loss_x 1.3730 (1.1277) acc_x 65.6250 (72.8906) lr 6.1732e-04 eta 0:00:01
epoch [127/200] batch [5/54] time 0.354 (0.460) data 0.224 (0.329) loss_u loss_u 0.8237 (0.7787) acc_u 21.8750 (28.7500) lr 6.1732e-04 eta 0:00:22
epoch [127/200] batch [10/54] time 0.396 (0.459) data 0.266 (0.329) loss_u loss_u 0.7861 (0.8116) acc_u 25.0000 (23.1250) lr 6.1732e-04 eta 0:00:20
epoch [127/200] batch [15/54] time 0.401 (0.453) data 0.271 (0.323) loss_u loss_u 0.8032 (0.8171) acc_u 21.8750 (21.4583) lr 6.1732e-04 eta 0:00:17
epoch [127/200] batch [20/54] time 0.522 (0.460) data 0.391 (0.329) loss_u loss_u 0.7378 (0.8127) acc_u 34.3750 (21.7188) lr 6.1732e-04 eta 0:00:15
epoch [127/200] batch [25/54] time 0.551 (0.462) data 0.421 (0.332) loss_u loss_u 0.7949 (0.8163) acc_u 21.8750 (21.6250) lr 6.1732e-04 eta 0:00:13
epoch [127/200] batch [30/54] time 0.486 (0.462) data 0.356 (0.332) loss_u loss_u 0.8252 (0.8163) acc_u 15.6250 (21.9792) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [35/54] time 0.562 (0.464) data 0.431 (0.333) loss_u loss_u 0.7661 (0.8106) acc_u 28.1250 (22.9464) lr 6.1732e-04 eta 0:00:08
epoch [127/200] batch [40/54] time 0.385 (0.461) data 0.253 (0.330) loss_u loss_u 0.8037 (0.8124) acc_u 25.0000 (22.6562) lr 6.1732e-04 eta 0:00:06
epoch [127/200] batch [45/54] time 0.417 (0.459) data 0.287 (0.328) loss_u loss_u 0.7739 (0.8064) acc_u 28.1250 (23.6111) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [50/54] time 0.344 (0.458) data 0.213 (0.328) loss_u loss_u 0.8335 (0.8078) acc_u 18.7500 (23.3750) lr 6.1732e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1388
confident_label rate tensor(0.4592, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1440
clean true:1397
clean false:43
clean_rate:0.9701388888888889
noisy true:351
noisy false:1345
after delete: len(clean_dataset) 1440
after delete: len(noisy_dataset) 1696
epoch [128/200] batch [5/45] time 0.619 (0.468) data 0.489 (0.337) loss_x loss_x 1.2246 (1.2000) acc_x 75.0000 (69.3750) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [10/45] time 0.487 (0.474) data 0.357 (0.344) loss_x loss_x 1.0537 (1.1427) acc_x 75.0000 (72.8125) lr 6.0285e-04 eta 0:00:16
epoch [128/200] batch [15/45] time 0.452 (0.484) data 0.322 (0.353) loss_x loss_x 1.1055 (1.1088) acc_x 65.6250 (72.5000) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [20/45] time 0.494 (0.480) data 0.364 (0.350) loss_x loss_x 1.0459 (1.1002) acc_x 65.6250 (72.5000) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [25/45] time 0.463 (0.468) data 0.333 (0.337) loss_x loss_x 0.8818 (1.0904) acc_x 81.2500 (72.8750) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [30/45] time 0.447 (0.473) data 0.317 (0.343) loss_x loss_x 1.0742 (1.1059) acc_x 75.0000 (72.0833) lr 6.0285e-04 eta 0:00:07
epoch [128/200] batch [35/45] time 0.391 (0.469) data 0.261 (0.338) loss_x loss_x 1.4248 (1.1345) acc_x 59.3750 (71.1607) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [40/45] time 0.478 (0.468) data 0.347 (0.338) loss_x loss_x 1.7061 (1.1250) acc_x 65.6250 (71.5625) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [45/45] time 0.340 (0.458) data 0.210 (0.328) loss_x loss_x 1.4883 (1.1419) acc_x 65.6250 (71.1111) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/53] time 0.405 (0.456) data 0.273 (0.326) loss_u loss_u 0.7324 (0.8128) acc_u 37.5000 (20.6250) lr 6.0285e-04 eta 0:00:21
epoch [128/200] batch [10/53] time 0.357 (0.457) data 0.226 (0.326) loss_u loss_u 0.9014 (0.8170) acc_u 9.3750 (20.6250) lr 6.0285e-04 eta 0:00:19
epoch [128/200] batch [15/53] time 0.408 (0.452) data 0.276 (0.322) loss_u loss_u 0.8560 (0.8187) acc_u 18.7500 (21.2500) lr 6.0285e-04 eta 0:00:17
epoch [128/200] batch [20/53] time 0.342 (0.449) data 0.212 (0.318) loss_u loss_u 0.8032 (0.8143) acc_u 25.0000 (22.0312) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [25/53] time 0.411 (0.452) data 0.279 (0.321) loss_u loss_u 0.9097 (0.8120) acc_u 9.3750 (22.3750) lr 6.0285e-04 eta 0:00:12
epoch [128/200] batch [30/53] time 0.414 (0.450) data 0.283 (0.320) loss_u loss_u 0.7910 (0.8064) acc_u 28.1250 (22.9167) lr 6.0285e-04 eta 0:00:10
epoch [128/200] batch [35/53] time 0.573 (0.451) data 0.442 (0.320) loss_u loss_u 0.8364 (0.8103) acc_u 15.6250 (22.4107) lr 6.0285e-04 eta 0:00:08
epoch [128/200] batch [40/53] time 0.464 (0.452) data 0.334 (0.322) loss_u loss_u 0.8271 (0.8135) acc_u 25.0000 (21.9531) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [45/53] time 0.393 (0.449) data 0.263 (0.319) loss_u loss_u 0.8013 (0.8163) acc_u 18.7500 (21.8750) lr 6.0285e-04 eta 0:00:03
epoch [128/200] batch [50/53] time 0.408 (0.450) data 0.276 (0.319) loss_u loss_u 0.8877 (0.8174) acc_u 12.5000 (21.6875) lr 6.0285e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1438
confident_label rate tensor(0.4401, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1380
clean true:1342
clean false:38
clean_rate:0.972463768115942
noisy true:356
noisy false:1400
after delete: len(clean_dataset) 1380
after delete: len(noisy_dataset) 1756
epoch [129/200] batch [5/43] time 0.636 (0.458) data 0.505 (0.328) loss_x loss_x 0.7607 (1.1676) acc_x 84.3750 (73.7500) lr 5.8849e-04 eta 0:00:17
epoch [129/200] batch [10/43] time 0.475 (0.460) data 0.344 (0.330) loss_x loss_x 1.5791 (1.1358) acc_x 56.2500 (71.8750) lr 5.8849e-04 eta 0:00:15
epoch [129/200] batch [15/43] time 0.546 (0.463) data 0.414 (0.333) loss_x loss_x 1.7881 (1.1901) acc_x 59.3750 (70.6250) lr 5.8849e-04 eta 0:00:12
epoch [129/200] batch [20/43] time 0.385 (0.453) data 0.254 (0.323) loss_x loss_x 1.7236 (1.1692) acc_x 62.5000 (71.4062) lr 5.8849e-04 eta 0:00:10
epoch [129/200] batch [25/43] time 0.408 (0.454) data 0.278 (0.324) loss_x loss_x 1.0039 (1.1735) acc_x 75.0000 (71.1250) lr 5.8849e-04 eta 0:00:08
epoch [129/200] batch [30/43] time 0.497 (0.452) data 0.367 (0.322) loss_x loss_x 0.9150 (1.1785) acc_x 78.1250 (71.2500) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [35/43] time 0.544 (0.449) data 0.413 (0.318) loss_x loss_x 0.9150 (1.1600) acc_x 75.0000 (71.1607) lr 5.8849e-04 eta 0:00:03
epoch [129/200] batch [40/43] time 0.429 (0.448) data 0.298 (0.317) loss_x loss_x 1.3750 (1.1565) acc_x 59.3750 (70.7812) lr 5.8849e-04 eta 0:00:01
epoch [129/200] batch [5/54] time 0.382 (0.456) data 0.251 (0.325) loss_u loss_u 0.8120 (0.8052) acc_u 25.0000 (23.7500) lr 5.8849e-04 eta 0:00:22
epoch [129/200] batch [10/54] time 0.520 (0.456) data 0.389 (0.326) loss_u loss_u 0.7974 (0.8004) acc_u 28.1250 (24.3750) lr 5.8849e-04 eta 0:00:20
epoch [129/200] batch [15/54] time 0.454 (0.457) data 0.323 (0.327) loss_u loss_u 0.7612 (0.8081) acc_u 31.2500 (23.7500) lr 5.8849e-04 eta 0:00:17
epoch [129/200] batch [20/54] time 0.497 (0.456) data 0.367 (0.325) loss_u loss_u 0.7349 (0.8010) acc_u 28.1250 (24.5312) lr 5.8849e-04 eta 0:00:15
epoch [129/200] batch [25/54] time 0.371 (0.455) data 0.241 (0.325) loss_u loss_u 0.8140 (0.8062) acc_u 21.8750 (24.0000) lr 5.8849e-04 eta 0:00:13
epoch [129/200] batch [30/54] time 0.500 (0.455) data 0.369 (0.325) loss_u loss_u 0.8071 (0.8057) acc_u 21.8750 (24.3750) lr 5.8849e-04 eta 0:00:10
epoch [129/200] batch [35/54] time 0.433 (0.455) data 0.302 (0.325) loss_u loss_u 0.8647 (0.8062) acc_u 9.3750 (23.9286) lr 5.8849e-04 eta 0:00:08
epoch [129/200] batch [40/54] time 0.411 (0.454) data 0.280 (0.323) loss_u loss_u 0.8838 (0.8101) acc_u 9.3750 (23.5938) lr 5.8849e-04 eta 0:00:06
epoch [129/200] batch [45/54] time 0.524 (0.453) data 0.392 (0.322) loss_u loss_u 0.7153 (0.8088) acc_u 31.2500 (23.8194) lr 5.8849e-04 eta 0:00:04
epoch [129/200] batch [50/54] time 0.436 (0.451) data 0.306 (0.320) loss_u loss_u 0.8345 (0.8096) acc_u 18.7500 (23.8125) lr 5.8849e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1421
confident_label rate tensor(0.4499, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1411
clean true:1368
clean false:43
clean_rate:0.969525159461375
noisy true:347
noisy false:1378
after delete: len(clean_dataset) 1411
after delete: len(noisy_dataset) 1725
epoch [130/200] batch [5/44] time 0.423 (0.464) data 0.294 (0.334) loss_x loss_x 0.6074 (1.0588) acc_x 78.1250 (70.0000) lr 5.7422e-04 eta 0:00:18
epoch [130/200] batch [10/44] time 0.486 (0.459) data 0.356 (0.329) loss_x loss_x 0.9019 (0.9820) acc_x 75.0000 (72.8125) lr 5.7422e-04 eta 0:00:15
epoch [130/200] batch [15/44] time 0.447 (0.447) data 0.317 (0.317) loss_x loss_x 1.6514 (1.1786) acc_x 56.2500 (69.3750) lr 5.7422e-04 eta 0:00:12
epoch [130/200] batch [20/44] time 0.323 (0.457) data 0.192 (0.326) loss_x loss_x 1.5791 (1.2037) acc_x 65.6250 (69.2188) lr 5.7422e-04 eta 0:00:10
epoch [130/200] batch [25/44] time 0.328 (0.449) data 0.198 (0.318) loss_x loss_x 1.3086 (1.1844) acc_x 71.8750 (70.1250) lr 5.7422e-04 eta 0:00:08
epoch [130/200] batch [30/44] time 0.516 (0.455) data 0.386 (0.324) loss_x loss_x 1.0732 (1.1942) acc_x 68.7500 (69.3750) lr 5.7422e-04 eta 0:00:06
epoch [130/200] batch [35/44] time 0.469 (0.447) data 0.337 (0.317) loss_x loss_x 1.3584 (1.1811) acc_x 71.8750 (70.0893) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [40/44] time 0.449 (0.445) data 0.318 (0.314) loss_x loss_x 1.3262 (1.1764) acc_x 68.7500 (69.9219) lr 5.7422e-04 eta 0:00:01
epoch [130/200] batch [5/53] time 0.521 (0.458) data 0.389 (0.328) loss_u loss_u 0.7881 (0.7999) acc_u 31.2500 (25.0000) lr 5.7422e-04 eta 0:00:21
epoch [130/200] batch [10/53] time 0.449 (0.458) data 0.317 (0.327) loss_u loss_u 0.8169 (0.8021) acc_u 21.8750 (24.0625) lr 5.7422e-04 eta 0:00:19
epoch [130/200] batch [15/53] time 0.465 (0.457) data 0.334 (0.327) loss_u loss_u 0.8276 (0.8207) acc_u 21.8750 (21.4583) lr 5.7422e-04 eta 0:00:17
epoch [130/200] batch [20/53] time 0.434 (0.458) data 0.303 (0.328) loss_u loss_u 0.7441 (0.8166) acc_u 28.1250 (22.0312) lr 5.7422e-04 eta 0:00:15
epoch [130/200] batch [25/53] time 0.542 (0.455) data 0.411 (0.324) loss_u loss_u 0.8398 (0.8177) acc_u 18.7500 (21.8750) lr 5.7422e-04 eta 0:00:12
epoch [130/200] batch [30/53] time 0.358 (0.449) data 0.226 (0.319) loss_u loss_u 0.8701 (0.8146) acc_u 12.5000 (22.2917) lr 5.7422e-04 eta 0:00:10
epoch [130/200] batch [35/53] time 0.428 (0.449) data 0.296 (0.318) loss_u loss_u 0.7949 (0.8102) acc_u 21.8750 (22.7679) lr 5.7422e-04 eta 0:00:08
epoch [130/200] batch [40/53] time 0.396 (0.449) data 0.265 (0.318) loss_u loss_u 0.7310 (0.8068) acc_u 28.1250 (22.8906) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [45/53] time 0.339 (0.448) data 0.208 (0.317) loss_u loss_u 0.6885 (0.8047) acc_u 40.6250 (22.9861) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [50/53] time 0.398 (0.450) data 0.267 (0.319) loss_u loss_u 0.8843 (0.8071) acc_u 12.5000 (22.8750) lr 5.7422e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1415
confident_label rate tensor(0.4471, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1402
clean true:1352
clean false:50
clean_rate:0.9643366619115549
noisy true:369
noisy false:1365
after delete: len(clean_dataset) 1402
after delete: len(noisy_dataset) 1734
epoch [131/200] batch [5/43] time 0.416 (0.433) data 0.285 (0.302) loss_x loss_x 1.0059 (1.2074) acc_x 71.8750 (68.7500) lr 5.6006e-04 eta 0:00:16
epoch [131/200] batch [10/43] time 0.400 (0.438) data 0.269 (0.307) loss_x loss_x 0.7920 (1.0556) acc_x 81.2500 (75.0000) lr 5.6006e-04 eta 0:00:14
epoch [131/200] batch [15/43] time 0.462 (0.438) data 0.332 (0.308) loss_x loss_x 1.0566 (1.0854) acc_x 71.8750 (74.1667) lr 5.6006e-04 eta 0:00:12
epoch [131/200] batch [20/43] time 0.461 (0.438) data 0.329 (0.307) loss_x loss_x 1.0947 (1.1043) acc_x 65.6250 (73.2812) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [25/43] time 0.420 (0.439) data 0.289 (0.308) loss_x loss_x 1.4902 (1.1068) acc_x 62.5000 (72.6250) lr 5.6006e-04 eta 0:00:07
epoch [131/200] batch [30/43] time 0.500 (0.435) data 0.370 (0.305) loss_x loss_x 1.0117 (1.1105) acc_x 78.1250 (73.1250) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [35/43] time 0.558 (0.445) data 0.428 (0.314) loss_x loss_x 0.9092 (1.1357) acc_x 71.8750 (72.3214) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [40/43] time 0.449 (0.442) data 0.318 (0.311) loss_x loss_x 0.9834 (1.1346) acc_x 68.7500 (72.4219) lr 5.6006e-04 eta 0:00:01
epoch [131/200] batch [5/54] time 0.516 (0.447) data 0.384 (0.316) loss_u loss_u 0.8438 (0.8085) acc_u 15.6250 (24.3750) lr 5.6006e-04 eta 0:00:21
epoch [131/200] batch [10/54] time 0.550 (0.456) data 0.418 (0.325) loss_u loss_u 0.8237 (0.8042) acc_u 18.7500 (24.3750) lr 5.6006e-04 eta 0:00:20
epoch [131/200] batch [15/54] time 0.682 (0.459) data 0.550 (0.328) loss_u loss_u 0.7837 (0.8024) acc_u 28.1250 (25.2083) lr 5.6006e-04 eta 0:00:17
epoch [131/200] batch [20/54] time 0.501 (0.464) data 0.369 (0.332) loss_u loss_u 0.7939 (0.7992) acc_u 34.3750 (26.2500) lr 5.6006e-04 eta 0:00:15
epoch [131/200] batch [25/54] time 0.452 (0.462) data 0.321 (0.330) loss_u loss_u 0.8413 (0.7955) acc_u 18.7500 (26.2500) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [30/54] time 0.368 (0.457) data 0.237 (0.326) loss_u loss_u 0.7964 (0.7984) acc_u 18.7500 (25.2083) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [35/54] time 0.402 (0.454) data 0.271 (0.323) loss_u loss_u 0.7593 (0.8030) acc_u 34.3750 (24.3750) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [40/54] time 0.380 (0.454) data 0.249 (0.322) loss_u loss_u 0.6914 (0.8003) acc_u 34.3750 (24.8438) lr 5.6006e-04 eta 0:00:06
epoch [131/200] batch [45/54] time 0.386 (0.456) data 0.255 (0.325) loss_u loss_u 0.8672 (0.8025) acc_u 18.7500 (24.3750) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [50/54] time 0.425 (0.453) data 0.293 (0.322) loss_u loss_u 0.7407 (0.7996) acc_u 34.3750 (24.9375) lr 5.6006e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1387
confident_label rate tensor(0.4649, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1458
clean true:1404
clean false:54
clean_rate:0.9629629629629629
noisy true:345
noisy false:1333
after delete: len(clean_dataset) 1458
after delete: len(noisy_dataset) 1678
epoch [132/200] batch [5/45] time 0.501 (0.492) data 0.370 (0.362) loss_x loss_x 0.6963 (1.2559) acc_x 81.2500 (66.8750) lr 5.4601e-04 eta 0:00:19
epoch [132/200] batch [10/45] time 0.498 (0.462) data 0.368 (0.331) loss_x loss_x 1.3975 (1.1890) acc_x 65.6250 (69.6875) lr 5.4601e-04 eta 0:00:16
epoch [132/200] batch [15/45] time 0.445 (0.450) data 0.314 (0.320) loss_x loss_x 1.2432 (1.1849) acc_x 75.0000 (70.0000) lr 5.4601e-04 eta 0:00:13
epoch [132/200] batch [20/45] time 0.396 (0.443) data 0.265 (0.313) loss_x loss_x 1.2363 (1.1853) acc_x 78.1250 (70.3125) lr 5.4601e-04 eta 0:00:11
epoch [132/200] batch [25/45] time 0.516 (0.452) data 0.384 (0.322) loss_x loss_x 1.3428 (1.1920) acc_x 68.7500 (69.8750) lr 5.4601e-04 eta 0:00:09
epoch [132/200] batch [30/45] time 0.409 (0.450) data 0.279 (0.319) loss_x loss_x 1.2002 (1.2144) acc_x 71.8750 (70.2083) lr 5.4601e-04 eta 0:00:06
epoch [132/200] batch [35/45] time 0.394 (0.451) data 0.263 (0.321) loss_x loss_x 1.6338 (1.2077) acc_x 78.1250 (70.8929) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [40/45] time 0.518 (0.456) data 0.388 (0.326) loss_x loss_x 1.1885 (1.2232) acc_x 65.6250 (70.1562) lr 5.4601e-04 eta 0:00:02
epoch [132/200] batch [45/45] time 0.609 (0.461) data 0.478 (0.330) loss_x loss_x 0.9824 (1.2167) acc_x 68.7500 (70.1389) lr 5.4601e-04 eta 0:00:00
epoch [132/200] batch [5/52] time 0.576 (0.460) data 0.445 (0.330) loss_u loss_u 0.7847 (0.7674) acc_u 31.2500 (31.2500) lr 5.4601e-04 eta 0:00:21
epoch [132/200] batch [10/52] time 0.429 (0.461) data 0.299 (0.331) loss_u loss_u 0.8389 (0.7957) acc_u 21.8750 (27.8125) lr 5.4601e-04 eta 0:00:19
epoch [132/200] batch [15/52] time 0.510 (0.463) data 0.378 (0.333) loss_u loss_u 0.8481 (0.8076) acc_u 12.5000 (24.3750) lr 5.4601e-04 eta 0:00:17
epoch [132/200] batch [20/52] time 0.448 (0.459) data 0.316 (0.328) loss_u loss_u 0.8198 (0.8047) acc_u 15.6250 (24.2188) lr 5.4601e-04 eta 0:00:14
epoch [132/200] batch [25/52] time 0.608 (0.460) data 0.476 (0.329) loss_u loss_u 0.8379 (0.8140) acc_u 31.2500 (23.6250) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [30/52] time 0.503 (0.463) data 0.371 (0.332) loss_u loss_u 0.7690 (0.8157) acc_u 25.0000 (23.4375) lr 5.4601e-04 eta 0:00:10
epoch [132/200] batch [35/52] time 0.417 (0.459) data 0.287 (0.328) loss_u loss_u 0.7314 (0.8142) acc_u 34.3750 (23.3036) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [40/52] time 0.567 (0.460) data 0.437 (0.330) loss_u loss_u 0.7744 (0.8168) acc_u 31.2500 (22.7344) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [45/52] time 0.416 (0.459) data 0.285 (0.328) loss_u loss_u 0.7793 (0.8160) acc_u 25.0000 (22.6389) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [50/52] time 0.431 (0.457) data 0.301 (0.327) loss_u loss_u 0.8477 (0.8194) acc_u 21.8750 (22.3750) lr 5.4601e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.4499, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1411
clean true:1366
clean false:45
clean_rate:0.9681077250177179
noisy true:357
noisy false:1368
after delete: len(clean_dataset) 1411
after delete: len(noisy_dataset) 1725
epoch [133/200] batch [5/44] time 0.409 (0.443) data 0.278 (0.313) loss_x loss_x 1.1611 (0.8641) acc_x 75.0000 (81.8750) lr 5.3207e-04 eta 0:00:17
epoch [133/200] batch [10/44] time 0.477 (0.449) data 0.347 (0.319) loss_x loss_x 1.8770 (1.0863) acc_x 65.6250 (75.6250) lr 5.3207e-04 eta 0:00:15
epoch [133/200] batch [15/44] time 0.425 (0.447) data 0.295 (0.317) loss_x loss_x 0.7153 (1.1210) acc_x 84.3750 (74.3750) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [20/44] time 0.457 (0.443) data 0.327 (0.312) loss_x loss_x 1.4971 (1.1391) acc_x 65.6250 (72.1875) lr 5.3207e-04 eta 0:00:10
epoch [133/200] batch [25/44] time 0.523 (0.453) data 0.393 (0.322) loss_x loss_x 1.0381 (1.1289) acc_x 68.7500 (71.7500) lr 5.3207e-04 eta 0:00:08
epoch [133/200] batch [30/44] time 0.465 (0.453) data 0.334 (0.322) loss_x loss_x 0.9048 (1.1327) acc_x 78.1250 (71.4583) lr 5.3207e-04 eta 0:00:06
epoch [133/200] batch [35/44] time 0.502 (0.454) data 0.371 (0.323) loss_x loss_x 1.0029 (1.1185) acc_x 81.2500 (72.2321) lr 5.3207e-04 eta 0:00:04
epoch [133/200] batch [40/44] time 0.530 (0.457) data 0.400 (0.326) loss_x loss_x 1.6689 (1.1373) acc_x 53.1250 (71.6406) lr 5.3207e-04 eta 0:00:01
epoch [133/200] batch [5/53] time 0.640 (0.471) data 0.509 (0.340) loss_u loss_u 0.8223 (0.7744) acc_u 21.8750 (29.3750) lr 5.3207e-04 eta 0:00:22
epoch [133/200] batch [10/53] time 0.739 (0.472) data 0.608 (0.342) loss_u loss_u 0.8608 (0.7903) acc_u 21.8750 (27.5000) lr 5.3207e-04 eta 0:00:20
epoch [133/200] batch [15/53] time 0.485 (0.468) data 0.355 (0.337) loss_u loss_u 0.7910 (0.7899) acc_u 25.0000 (26.8750) lr 5.3207e-04 eta 0:00:17
epoch [133/200] batch [20/53] time 0.415 (0.464) data 0.284 (0.334) loss_u loss_u 0.8101 (0.7998) acc_u 25.0000 (25.1562) lr 5.3207e-04 eta 0:00:15
epoch [133/200] batch [25/53] time 0.368 (0.470) data 0.237 (0.339) loss_u loss_u 0.7959 (0.7985) acc_u 25.0000 (25.1250) lr 5.3207e-04 eta 0:00:13
epoch [133/200] batch [30/53] time 0.485 (0.466) data 0.355 (0.335) loss_u loss_u 0.7812 (0.7967) acc_u 21.8750 (25.4167) lr 5.3207e-04 eta 0:00:10
epoch [133/200] batch [35/53] time 0.348 (0.462) data 0.218 (0.331) loss_u loss_u 0.6777 (0.7980) acc_u 43.7500 (25.3571) lr 5.3207e-04 eta 0:00:08
epoch [133/200] batch [40/53] time 0.443 (0.459) data 0.312 (0.329) loss_u loss_u 0.7915 (0.7998) acc_u 25.0000 (25.0000) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [45/53] time 0.551 (0.457) data 0.420 (0.326) loss_u loss_u 0.7886 (0.7987) acc_u 25.0000 (25.1389) lr 5.3207e-04 eta 0:00:03
epoch [133/200] batch [50/53] time 0.475 (0.455) data 0.345 (0.324) loss_u loss_u 0.7378 (0.7992) acc_u 37.5000 (24.8750) lr 5.3207e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.4538, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1423
clean true:1376
clean false:47
clean_rate:0.9669711876317639
noisy true:347
noisy false:1366
after delete: len(clean_dataset) 1423
after delete: len(noisy_dataset) 1713
epoch [134/200] batch [5/44] time 0.423 (0.494) data 0.293 (0.363) loss_x loss_x 0.8066 (1.2600) acc_x 78.1250 (73.7500) lr 5.1825e-04 eta 0:00:19
epoch [134/200] batch [10/44] time 0.401 (0.494) data 0.270 (0.364) loss_x loss_x 0.7075 (1.1344) acc_x 75.0000 (72.8125) lr 5.1825e-04 eta 0:00:16
epoch [134/200] batch [15/44] time 0.456 (0.481) data 0.326 (0.351) loss_x loss_x 1.3906 (1.1366) acc_x 62.5000 (72.9167) lr 5.1825e-04 eta 0:00:13
epoch [134/200] batch [20/44] time 0.457 (0.480) data 0.327 (0.350) loss_x loss_x 1.2021 (1.0949) acc_x 68.7500 (73.9062) lr 5.1825e-04 eta 0:00:11
epoch [134/200] batch [25/44] time 0.472 (0.477) data 0.341 (0.346) loss_x loss_x 0.7676 (1.1099) acc_x 81.2500 (73.0000) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [30/44] time 0.458 (0.478) data 0.328 (0.347) loss_x loss_x 0.6772 (1.0934) acc_x 90.6250 (73.5417) lr 5.1825e-04 eta 0:00:06
epoch [134/200] batch [35/44] time 0.470 (0.475) data 0.338 (0.344) loss_x loss_x 1.1738 (1.1023) acc_x 75.0000 (73.0357) lr 5.1825e-04 eta 0:00:04
epoch [134/200] batch [40/44] time 0.391 (0.473) data 0.261 (0.343) loss_x loss_x 1.5186 (1.0999) acc_x 62.5000 (72.8906) lr 5.1825e-04 eta 0:00:01
epoch [134/200] batch [5/53] time 0.752 (0.476) data 0.621 (0.345) loss_u loss_u 0.8818 (0.8384) acc_u 12.5000 (15.6250) lr 5.1825e-04 eta 0:00:22
epoch [134/200] batch [10/53] time 0.418 (0.482) data 0.288 (0.351) loss_u loss_u 0.7856 (0.8171) acc_u 28.1250 (20.9375) lr 5.1825e-04 eta 0:00:20
epoch [134/200] batch [15/53] time 0.432 (0.483) data 0.301 (0.352) loss_u loss_u 0.7964 (0.8195) acc_u 25.0000 (21.6667) lr 5.1825e-04 eta 0:00:18
epoch [134/200] batch [20/53] time 0.427 (0.480) data 0.296 (0.349) loss_u loss_u 0.8062 (0.8073) acc_u 18.7500 (22.9688) lr 5.1825e-04 eta 0:00:15
epoch [134/200] batch [25/53] time 0.390 (0.476) data 0.259 (0.346) loss_u loss_u 0.7954 (0.8116) acc_u 28.1250 (22.6250) lr 5.1825e-04 eta 0:00:13
epoch [134/200] batch [30/53] time 0.396 (0.476) data 0.266 (0.345) loss_u loss_u 0.8267 (0.8125) acc_u 21.8750 (22.7083) lr 5.1825e-04 eta 0:00:10
epoch [134/200] batch [35/53] time 0.403 (0.472) data 0.272 (0.341) loss_u loss_u 0.8154 (0.8085) acc_u 21.8750 (22.9464) lr 5.1825e-04 eta 0:00:08
epoch [134/200] batch [40/53] time 0.362 (0.471) data 0.231 (0.340) loss_u loss_u 0.8340 (0.8118) acc_u 25.0000 (22.4219) lr 5.1825e-04 eta 0:00:06
epoch [134/200] batch [45/53] time 0.399 (0.468) data 0.268 (0.337) loss_u loss_u 0.8125 (0.8139) acc_u 21.8750 (22.3611) lr 5.1825e-04 eta 0:00:03
epoch [134/200] batch [50/53] time 0.476 (0.465) data 0.346 (0.334) loss_u loss_u 0.7998 (0.8127) acc_u 25.0000 (22.6875) lr 5.1825e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1425
confident_label rate tensor(0.4455, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1397
clean true:1353
clean false:44
clean_rate:0.968503937007874
noisy true:358
noisy false:1381
after delete: len(clean_dataset) 1397
after delete: len(noisy_dataset) 1739
epoch [135/200] batch [5/43] time 0.399 (0.473) data 0.268 (0.342) loss_x loss_x 1.3789 (1.1147) acc_x 65.6250 (74.3750) lr 5.0454e-04 eta 0:00:17
epoch [135/200] batch [10/43] time 0.522 (0.464) data 0.392 (0.334) loss_x loss_x 0.7549 (1.0395) acc_x 81.2500 (75.6250) lr 5.0454e-04 eta 0:00:15
epoch [135/200] batch [15/43] time 0.450 (0.457) data 0.320 (0.326) loss_x loss_x 1.0859 (1.0960) acc_x 75.0000 (73.7500) lr 5.0454e-04 eta 0:00:12
epoch [135/200] batch [20/43] time 0.412 (0.458) data 0.281 (0.328) loss_x loss_x 1.3027 (1.0943) acc_x 65.6250 (73.5938) lr 5.0454e-04 eta 0:00:10
epoch [135/200] batch [25/43] time 0.539 (0.467) data 0.408 (0.337) loss_x loss_x 1.8926 (1.1572) acc_x 65.6250 (72.2500) lr 5.0454e-04 eta 0:00:08
epoch [135/200] batch [30/43] time 0.560 (0.480) data 0.430 (0.350) loss_x loss_x 0.8828 (1.1520) acc_x 75.0000 (71.8750) lr 5.0454e-04 eta 0:00:06
epoch [135/200] batch [35/43] time 0.456 (0.477) data 0.325 (0.346) loss_x loss_x 0.7139 (1.1559) acc_x 75.0000 (71.9643) lr 5.0454e-04 eta 0:00:03
epoch [135/200] batch [40/43] time 0.415 (0.470) data 0.285 (0.339) loss_x loss_x 0.6831 (1.1428) acc_x 87.5000 (71.7188) lr 5.0454e-04 eta 0:00:01
epoch [135/200] batch [5/54] time 0.408 (0.463) data 0.278 (0.332) loss_u loss_u 0.8550 (0.7943) acc_u 18.7500 (27.5000) lr 5.0454e-04 eta 0:00:22
epoch [135/200] batch [10/54] time 0.556 (0.460) data 0.424 (0.330) loss_u loss_u 0.7993 (0.7870) acc_u 21.8750 (26.5625) lr 5.0454e-04 eta 0:00:20
epoch [135/200] batch [15/54] time 0.369 (0.462) data 0.239 (0.331) loss_u loss_u 0.7925 (0.7907) acc_u 31.2500 (25.8333) lr 5.0454e-04 eta 0:00:18
epoch [135/200] batch [20/54] time 0.475 (0.460) data 0.343 (0.330) loss_u loss_u 0.8608 (0.7934) acc_u 15.6250 (25.4688) lr 5.0454e-04 eta 0:00:15
epoch [135/200] batch [25/54] time 0.425 (0.459) data 0.295 (0.328) loss_u loss_u 0.8359 (0.8027) acc_u 25.0000 (24.5000) lr 5.0454e-04 eta 0:00:13
epoch [135/200] batch [30/54] time 0.423 (0.462) data 0.292 (0.332) loss_u loss_u 0.7441 (0.7991) acc_u 25.0000 (24.4792) lr 5.0454e-04 eta 0:00:11
epoch [135/200] batch [35/54] time 0.409 (0.458) data 0.278 (0.327) loss_u loss_u 0.8086 (0.8036) acc_u 25.0000 (24.0179) lr 5.0454e-04 eta 0:00:08
epoch [135/200] batch [40/54] time 0.394 (0.460) data 0.263 (0.329) loss_u loss_u 0.7568 (0.8050) acc_u 28.1250 (23.9062) lr 5.0454e-04 eta 0:00:06
epoch [135/200] batch [45/54] time 0.464 (0.461) data 0.333 (0.330) loss_u loss_u 0.7559 (0.8052) acc_u 31.2500 (24.2361) lr 5.0454e-04 eta 0:00:04
epoch [135/200] batch [50/54] time 0.352 (0.458) data 0.221 (0.327) loss_u loss_u 0.8184 (0.8034) acc_u 21.8750 (24.5625) lr 5.0454e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1392
confident_label rate tensor(0.4576, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1435
clean true:1386
clean false:49
clean_rate:0.9658536585365853
noisy true:358
noisy false:1343
after delete: len(clean_dataset) 1435
after delete: len(noisy_dataset) 1701
epoch [136/200] batch [5/44] time 0.363 (0.425) data 0.233 (0.294) loss_x loss_x 0.8340 (0.9473) acc_x 78.1250 (77.5000) lr 4.9096e-04 eta 0:00:16
epoch [136/200] batch [10/44] time 0.444 (0.421) data 0.313 (0.290) loss_x loss_x 1.3477 (1.0196) acc_x 56.2500 (75.6250) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [15/44] time 0.374 (0.431) data 0.243 (0.301) loss_x loss_x 1.4785 (1.0524) acc_x 71.8750 (74.3750) lr 4.9096e-04 eta 0:00:12
epoch [136/200] batch [20/44] time 0.477 (0.430) data 0.346 (0.300) loss_x loss_x 0.6499 (1.0428) acc_x 81.2500 (73.5938) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [25/44] time 0.605 (0.436) data 0.474 (0.305) loss_x loss_x 1.0947 (1.0606) acc_x 71.8750 (73.0000) lr 4.9096e-04 eta 0:00:08
epoch [136/200] batch [30/44] time 0.432 (0.442) data 0.301 (0.312) loss_x loss_x 1.7090 (1.0948) acc_x 65.6250 (72.5000) lr 4.9096e-04 eta 0:00:06
epoch [136/200] batch [35/44] time 0.443 (0.446) data 0.312 (0.315) loss_x loss_x 0.8135 (1.0982) acc_x 84.3750 (72.4107) lr 4.9096e-04 eta 0:00:04
epoch [136/200] batch [40/44] time 0.341 (0.446) data 0.210 (0.316) loss_x loss_x 0.8188 (1.0961) acc_x 87.5000 (72.6562) lr 4.9096e-04 eta 0:00:01
epoch [136/200] batch [5/53] time 0.368 (0.444) data 0.236 (0.313) loss_u loss_u 0.7407 (0.7750) acc_u 34.3750 (28.1250) lr 4.9096e-04 eta 0:00:21
epoch [136/200] batch [10/53] time 0.393 (0.451) data 0.263 (0.321) loss_u loss_u 0.8296 (0.7812) acc_u 21.8750 (28.4375) lr 4.9096e-04 eta 0:00:19
epoch [136/200] batch [15/53] time 0.484 (0.453) data 0.353 (0.322) loss_u loss_u 0.8096 (0.8012) acc_u 18.7500 (25.4167) lr 4.9096e-04 eta 0:00:17
epoch [136/200] batch [20/53] time 0.518 (0.453) data 0.387 (0.322) loss_u loss_u 0.7485 (0.7969) acc_u 34.3750 (25.4688) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [25/53] time 0.354 (0.454) data 0.223 (0.324) loss_u loss_u 0.7153 (0.7962) acc_u 34.3750 (25.6250) lr 4.9096e-04 eta 0:00:12
epoch [136/200] batch [30/53] time 0.351 (0.454) data 0.220 (0.323) loss_u loss_u 0.8262 (0.8016) acc_u 21.8750 (25.2083) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [35/53] time 0.403 (0.461) data 0.273 (0.331) loss_u loss_u 0.8086 (0.8004) acc_u 18.7500 (25.0000) lr 4.9096e-04 eta 0:00:08
epoch [136/200] batch [40/53] time 0.390 (0.461) data 0.260 (0.330) loss_u loss_u 0.7803 (0.8018) acc_u 25.0000 (24.6875) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [45/53] time 0.330 (0.458) data 0.200 (0.327) loss_u loss_u 0.7842 (0.8007) acc_u 31.2500 (24.9306) lr 4.9096e-04 eta 0:00:03
epoch [136/200] batch [50/53] time 0.448 (0.456) data 0.317 (0.325) loss_u loss_u 0.8491 (0.8056) acc_u 15.6250 (24.0625) lr 4.9096e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.4490, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1408
clean true:1359
clean false:49
clean_rate:0.9651988636363636
noisy true:357
noisy false:1371
after delete: len(clean_dataset) 1408
after delete: len(noisy_dataset) 1728
epoch [137/200] batch [5/44] time 0.455 (0.441) data 0.324 (0.311) loss_x loss_x 1.1221 (1.2521) acc_x 81.2500 (73.1250) lr 4.7750e-04 eta 0:00:17
epoch [137/200] batch [10/44] time 0.449 (0.447) data 0.318 (0.316) loss_x loss_x 0.7554 (1.2757) acc_x 90.6250 (68.7500) lr 4.7750e-04 eta 0:00:15
epoch [137/200] batch [15/44] time 0.507 (0.454) data 0.377 (0.324) loss_x loss_x 1.0049 (1.2200) acc_x 84.3750 (71.2500) lr 4.7750e-04 eta 0:00:13
epoch [137/200] batch [20/44] time 0.516 (0.451) data 0.386 (0.320) loss_x loss_x 1.2588 (1.1898) acc_x 78.1250 (72.5000) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [25/44] time 0.463 (0.456) data 0.332 (0.326) loss_x loss_x 1.6504 (1.1525) acc_x 56.2500 (72.8750) lr 4.7750e-04 eta 0:00:08
epoch [137/200] batch [30/44] time 0.433 (0.460) data 0.303 (0.330) loss_x loss_x 1.2715 (1.1875) acc_x 71.8750 (71.4583) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [35/44] time 0.514 (0.461) data 0.384 (0.331) loss_x loss_x 1.5537 (1.1835) acc_x 56.2500 (70.9821) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [40/44] time 0.458 (0.457) data 0.327 (0.326) loss_x loss_x 1.0225 (1.1792) acc_x 68.7500 (70.4688) lr 4.7750e-04 eta 0:00:01
epoch [137/200] batch [5/54] time 0.599 (0.464) data 0.468 (0.334) loss_u loss_u 0.8369 (0.8032) acc_u 21.8750 (23.1250) lr 4.7750e-04 eta 0:00:22
epoch [137/200] batch [10/54] time 0.509 (0.467) data 0.378 (0.336) loss_u loss_u 0.8276 (0.8017) acc_u 21.8750 (23.7500) lr 4.7750e-04 eta 0:00:20
epoch [137/200] batch [15/54] time 0.530 (0.465) data 0.398 (0.335) loss_u loss_u 0.8247 (0.7989) acc_u 18.7500 (23.7500) lr 4.7750e-04 eta 0:00:18
epoch [137/200] batch [20/54] time 0.424 (0.466) data 0.294 (0.335) loss_u loss_u 0.8037 (0.7993) acc_u 31.2500 (24.5312) lr 4.7750e-04 eta 0:00:15
epoch [137/200] batch [25/54] time 0.543 (0.463) data 0.412 (0.332) loss_u loss_u 0.8174 (0.7975) acc_u 21.8750 (24.6250) lr 4.7750e-04 eta 0:00:13
epoch [137/200] batch [30/54] time 0.487 (0.466) data 0.356 (0.335) loss_u loss_u 0.8457 (0.8102) acc_u 15.6250 (22.7083) lr 4.7750e-04 eta 0:00:11
epoch [137/200] batch [35/54] time 0.688 (0.468) data 0.556 (0.337) loss_u loss_u 0.9097 (0.8089) acc_u 12.5000 (23.4821) lr 4.7750e-04 eta 0:00:08
epoch [137/200] batch [40/54] time 0.466 (0.468) data 0.335 (0.338) loss_u loss_u 0.9058 (0.8165) acc_u 15.6250 (22.5781) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [45/54] time 0.498 (0.468) data 0.367 (0.337) loss_u loss_u 0.7832 (0.8148) acc_u 28.1250 (22.6389) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [50/54] time 0.363 (0.466) data 0.233 (0.335) loss_u loss_u 0.7539 (0.8132) acc_u 31.2500 (22.7500) lr 4.7750e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1425
confident_label rate tensor(0.4445, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1394
clean true:1338
clean false:56
clean_rate:0.9598278335724534
noisy true:373
noisy false:1369
after delete: len(clean_dataset) 1394
after delete: len(noisy_dataset) 1742
epoch [138/200] batch [5/43] time 0.352 (0.467) data 0.222 (0.337) loss_x loss_x 1.4893 (1.2697) acc_x 68.7500 (69.3750) lr 4.6417e-04 eta 0:00:17
epoch [138/200] batch [10/43] time 0.440 (0.444) data 0.310 (0.313) loss_x loss_x 1.3613 (1.2576) acc_x 65.6250 (67.8125) lr 4.6417e-04 eta 0:00:14
epoch [138/200] batch [15/43] time 0.387 (0.451) data 0.256 (0.321) loss_x loss_x 0.8149 (1.2422) acc_x 81.2500 (67.7083) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [20/43] time 0.461 (0.447) data 0.331 (0.317) loss_x loss_x 1.0576 (1.2265) acc_x 71.8750 (69.2188) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [25/43] time 0.412 (0.451) data 0.281 (0.320) loss_x loss_x 1.5361 (1.2283) acc_x 65.6250 (69.8750) lr 4.6417e-04 eta 0:00:08
epoch [138/200] batch [30/43] time 0.410 (0.455) data 0.279 (0.324) loss_x loss_x 0.8677 (1.2200) acc_x 87.5000 (69.7917) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [35/43] time 0.520 (0.454) data 0.388 (0.324) loss_x loss_x 1.6357 (1.2275) acc_x 75.0000 (70.0893) lr 4.6417e-04 eta 0:00:03
epoch [138/200] batch [40/43] time 0.649 (0.466) data 0.518 (0.335) loss_x loss_x 1.3691 (1.2458) acc_x 53.1250 (69.7656) lr 4.6417e-04 eta 0:00:01
epoch [138/200] batch [5/54] time 0.401 (0.470) data 0.270 (0.340) loss_u loss_u 0.7554 (0.8032) acc_u 25.0000 (22.5000) lr 4.6417e-04 eta 0:00:23
epoch [138/200] batch [10/54] time 0.370 (0.465) data 0.240 (0.335) loss_u loss_u 0.7881 (0.8048) acc_u 25.0000 (23.1250) lr 4.6417e-04 eta 0:00:20
epoch [138/200] batch [15/54] time 0.456 (0.460) data 0.325 (0.330) loss_u loss_u 0.9126 (0.8067) acc_u 12.5000 (23.3333) lr 4.6417e-04 eta 0:00:17
epoch [138/200] batch [20/54] time 0.413 (0.460) data 0.283 (0.329) loss_u loss_u 0.7148 (0.8038) acc_u 40.6250 (23.7500) lr 4.6417e-04 eta 0:00:15
epoch [138/200] batch [25/54] time 0.463 (0.460) data 0.333 (0.329) loss_u loss_u 0.8516 (0.8056) acc_u 15.6250 (23.5000) lr 4.6417e-04 eta 0:00:13
epoch [138/200] batch [30/54] time 0.507 (0.463) data 0.375 (0.332) loss_u loss_u 0.7739 (0.8056) acc_u 21.8750 (23.7500) lr 4.6417e-04 eta 0:00:11
epoch [138/200] batch [35/54] time 0.332 (0.460) data 0.202 (0.329) loss_u loss_u 0.7852 (0.8015) acc_u 25.0000 (24.6429) lr 4.6417e-04 eta 0:00:08
epoch [138/200] batch [40/54] time 0.536 (0.460) data 0.405 (0.329) loss_u loss_u 0.8076 (0.8008) acc_u 25.0000 (25.1562) lr 4.6417e-04 eta 0:00:06
epoch [138/200] batch [45/54] time 0.442 (0.461) data 0.312 (0.330) loss_u loss_u 0.8716 (0.8023) acc_u 18.7500 (24.8611) lr 4.6417e-04 eta 0:00:04
epoch [138/200] batch [50/54] time 0.468 (0.458) data 0.337 (0.328) loss_u loss_u 0.7744 (0.8021) acc_u 28.1250 (25.0000) lr 4.6417e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1400
confident_label rate tensor(0.4525, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1419
clean true:1382
clean false:37
clean_rate:0.9739252995066948
noisy true:354
noisy false:1363
after delete: len(clean_dataset) 1419
after delete: len(noisy_dataset) 1717
epoch [139/200] batch [5/44] time 0.492 (0.467) data 0.362 (0.336) loss_x loss_x 0.4641 (1.0400) acc_x 81.2500 (73.7500) lr 4.5098e-04 eta 0:00:18
epoch [139/200] batch [10/44] time 0.422 (0.451) data 0.292 (0.320) loss_x loss_x 0.9766 (1.0657) acc_x 78.1250 (73.4375) lr 4.5098e-04 eta 0:00:15
epoch [139/200] batch [15/44] time 0.564 (0.450) data 0.433 (0.319) loss_x loss_x 0.9517 (1.0364) acc_x 68.7500 (72.5000) lr 4.5098e-04 eta 0:00:13
epoch [139/200] batch [20/44] time 0.382 (0.442) data 0.252 (0.312) loss_x loss_x 1.2939 (1.1333) acc_x 65.6250 (71.0938) lr 4.5098e-04 eta 0:00:10
epoch [139/200] batch [25/44] time 0.522 (0.450) data 0.392 (0.319) loss_x loss_x 1.9678 (1.1670) acc_x 59.3750 (71.1250) lr 4.5098e-04 eta 0:00:08
epoch [139/200] batch [30/44] time 0.376 (0.449) data 0.245 (0.319) loss_x loss_x 0.7715 (1.1348) acc_x 78.1250 (71.9792) lr 4.5098e-04 eta 0:00:06
epoch [139/200] batch [35/44] time 0.456 (0.449) data 0.325 (0.319) loss_x loss_x 1.4072 (1.1475) acc_x 68.7500 (71.8750) lr 4.5098e-04 eta 0:00:04
epoch [139/200] batch [40/44] time 0.500 (0.450) data 0.369 (0.319) loss_x loss_x 1.2129 (1.1662) acc_x 62.5000 (71.6406) lr 4.5098e-04 eta 0:00:01
epoch [139/200] batch [5/53] time 0.491 (0.457) data 0.360 (0.326) loss_u loss_u 0.7295 (0.7912) acc_u 31.2500 (26.2500) lr 4.5098e-04 eta 0:00:21
epoch [139/200] batch [10/53] time 0.411 (0.459) data 0.280 (0.328) loss_u loss_u 0.7896 (0.7813) acc_u 25.0000 (27.1875) lr 4.5098e-04 eta 0:00:19
epoch [139/200] batch [15/53] time 0.391 (0.459) data 0.261 (0.328) loss_u loss_u 0.6753 (0.7998) acc_u 37.5000 (23.7500) lr 4.5098e-04 eta 0:00:17
epoch [139/200] batch [20/53] time 0.394 (0.452) data 0.263 (0.321) loss_u loss_u 0.8896 (0.8000) acc_u 12.5000 (24.3750) lr 4.5098e-04 eta 0:00:14
epoch [139/200] batch [25/53] time 0.476 (0.452) data 0.344 (0.321) loss_u loss_u 0.7998 (0.8040) acc_u 31.2500 (24.1250) lr 4.5098e-04 eta 0:00:12
epoch [139/200] batch [30/53] time 0.601 (0.450) data 0.470 (0.320) loss_u loss_u 0.6333 (0.7991) acc_u 50.0000 (25.3125) lr 4.5098e-04 eta 0:00:10
epoch [139/200] batch [35/53] time 0.404 (0.449) data 0.273 (0.318) loss_u loss_u 0.8154 (0.8017) acc_u 25.0000 (24.4643) lr 4.5098e-04 eta 0:00:08
epoch [139/200] batch [40/53] time 0.489 (0.448) data 0.359 (0.317) loss_u loss_u 0.8511 (0.8031) acc_u 15.6250 (24.2188) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [45/53] time 0.448 (0.448) data 0.316 (0.317) loss_u loss_u 0.8130 (0.8036) acc_u 25.0000 (24.3056) lr 4.5098e-04 eta 0:00:03
epoch [139/200] batch [50/53] time 0.427 (0.447) data 0.297 (0.316) loss_u loss_u 0.7085 (0.8050) acc_u 34.3750 (24.1250) lr 4.5098e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1392
confident_label rate tensor(0.4550, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1427
clean true:1388
clean false:39
clean_rate:0.9726699369306236
noisy true:356
noisy false:1353
after delete: len(clean_dataset) 1427
after delete: len(noisy_dataset) 1709
epoch [140/200] batch [5/44] time 0.512 (0.494) data 0.380 (0.364) loss_x loss_x 0.7759 (1.1164) acc_x 78.1250 (73.1250) lr 4.3792e-04 eta 0:00:19
epoch [140/200] batch [10/44] time 0.391 (0.454) data 0.261 (0.324) loss_x loss_x 0.9150 (1.0939) acc_x 81.2500 (71.8750) lr 4.3792e-04 eta 0:00:15
epoch [140/200] batch [15/44] time 0.428 (0.444) data 0.298 (0.313) loss_x loss_x 1.0127 (1.0567) acc_x 71.8750 (72.5000) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [20/44] time 0.455 (0.448) data 0.325 (0.318) loss_x loss_x 0.9473 (1.0983) acc_x 68.7500 (71.2500) lr 4.3792e-04 eta 0:00:10
epoch [140/200] batch [25/44] time 0.439 (0.442) data 0.309 (0.312) loss_x loss_x 1.0713 (1.0874) acc_x 75.0000 (71.6250) lr 4.3792e-04 eta 0:00:08
epoch [140/200] batch [30/44] time 0.437 (0.441) data 0.306 (0.310) loss_x loss_x 1.2793 (1.0938) acc_x 68.7500 (71.2500) lr 4.3792e-04 eta 0:00:06
epoch [140/200] batch [35/44] time 0.477 (0.443) data 0.346 (0.312) loss_x loss_x 1.0449 (1.0938) acc_x 71.8750 (70.9821) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [40/44] time 0.507 (0.448) data 0.376 (0.317) loss_x loss_x 0.8037 (1.0865) acc_x 81.2500 (70.8594) lr 4.3792e-04 eta 0:00:01
epoch [140/200] batch [5/53] time 0.405 (0.443) data 0.273 (0.313) loss_u loss_u 0.7832 (0.7782) acc_u 25.0000 (27.5000) lr 4.3792e-04 eta 0:00:21
epoch [140/200] batch [10/53] time 0.568 (0.449) data 0.437 (0.318) loss_u loss_u 0.8325 (0.7948) acc_u 15.6250 (25.0000) lr 4.3792e-04 eta 0:00:19
epoch [140/200] batch [15/53] time 0.371 (0.448) data 0.240 (0.317) loss_u loss_u 0.7778 (0.8047) acc_u 34.3750 (24.7917) lr 4.3792e-04 eta 0:00:17
epoch [140/200] batch [20/53] time 0.540 (0.446) data 0.408 (0.315) loss_u loss_u 0.8394 (0.8079) acc_u 21.8750 (24.5312) lr 4.3792e-04 eta 0:00:14
epoch [140/200] batch [25/53] time 0.400 (0.445) data 0.268 (0.314) loss_u loss_u 0.8594 (0.8060) acc_u 12.5000 (24.8750) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [30/53] time 0.590 (0.445) data 0.458 (0.314) loss_u loss_u 0.8018 (0.8074) acc_u 28.1250 (24.3750) lr 4.3792e-04 eta 0:00:10
epoch [140/200] batch [35/53] time 0.417 (0.443) data 0.286 (0.312) loss_u loss_u 0.7817 (0.8057) acc_u 25.0000 (24.5536) lr 4.3792e-04 eta 0:00:07
epoch [140/200] batch [40/53] time 0.370 (0.443) data 0.238 (0.312) loss_u loss_u 0.8213 (0.8048) acc_u 28.1250 (24.7656) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [45/53] time 0.589 (0.447) data 0.457 (0.316) loss_u loss_u 0.7974 (0.8077) acc_u 28.1250 (24.0972) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [50/53] time 0.361 (0.447) data 0.230 (0.316) loss_u loss_u 0.7407 (0.8077) acc_u 28.1250 (23.8125) lr 4.3792e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1414
confident_label rate tensor(0.4528, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1420
clean true:1377
clean false:43
clean_rate:0.969718309859155
noisy true:345
noisy false:1371
after delete: len(clean_dataset) 1420
after delete: len(noisy_dataset) 1716
epoch [141/200] batch [5/44] time 0.463 (0.476) data 0.333 (0.346) loss_x loss_x 0.8848 (1.0853) acc_x 75.0000 (73.7500) lr 4.2499e-04 eta 0:00:18
epoch [141/200] batch [10/44] time 0.377 (0.451) data 0.246 (0.321) loss_x loss_x 1.8457 (1.0875) acc_x 62.5000 (73.4375) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [15/44] time 0.421 (0.461) data 0.291 (0.330) loss_x loss_x 1.0957 (1.1056) acc_x 68.7500 (71.0417) lr 4.2499e-04 eta 0:00:13
epoch [141/200] batch [20/44] time 0.419 (0.449) data 0.289 (0.319) loss_x loss_x 0.9653 (1.1308) acc_x 62.5000 (70.3125) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [25/44] time 0.472 (0.453) data 0.341 (0.323) loss_x loss_x 0.9619 (1.1368) acc_x 75.0000 (71.0000) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [30/44] time 0.499 (0.449) data 0.370 (0.318) loss_x loss_x 1.1104 (1.1547) acc_x 75.0000 (70.1042) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [35/44] time 0.453 (0.450) data 0.322 (0.320) loss_x loss_x 1.6465 (1.1783) acc_x 56.2500 (69.8214) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [40/44] time 0.420 (0.453) data 0.291 (0.323) loss_x loss_x 0.8032 (1.1662) acc_x 84.3750 (70.2344) lr 4.2499e-04 eta 0:00:01
epoch [141/200] batch [5/53] time 0.599 (0.457) data 0.468 (0.327) loss_u loss_u 0.8193 (0.8216) acc_u 21.8750 (21.2500) lr 4.2499e-04 eta 0:00:21
epoch [141/200] batch [10/53] time 0.486 (0.457) data 0.355 (0.327) loss_u loss_u 0.7432 (0.8247) acc_u 34.3750 (20.6250) lr 4.2499e-04 eta 0:00:19
epoch [141/200] batch [15/53] time 0.435 (0.456) data 0.304 (0.325) loss_u loss_u 0.7368 (0.8118) acc_u 37.5000 (23.3333) lr 4.2499e-04 eta 0:00:17
epoch [141/200] batch [20/53] time 0.367 (0.456) data 0.236 (0.326) loss_u loss_u 0.7681 (0.8011) acc_u 28.1250 (24.3750) lr 4.2499e-04 eta 0:00:15
epoch [141/200] batch [25/53] time 0.485 (0.454) data 0.354 (0.323) loss_u loss_u 0.8550 (0.8057) acc_u 18.7500 (24.1250) lr 4.2499e-04 eta 0:00:12
epoch [141/200] batch [30/53] time 0.484 (0.455) data 0.353 (0.324) loss_u loss_u 0.9229 (0.8114) acc_u 12.5000 (23.4375) lr 4.2499e-04 eta 0:00:10
epoch [141/200] batch [35/53] time 0.423 (0.456) data 0.292 (0.326) loss_u loss_u 0.8218 (0.8094) acc_u 25.0000 (23.4821) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [40/53] time 0.590 (0.460) data 0.457 (0.330) loss_u loss_u 0.8643 (0.8094) acc_u 18.7500 (23.5156) lr 4.2499e-04 eta 0:00:05
epoch [141/200] batch [45/53] time 0.559 (0.462) data 0.429 (0.331) loss_u loss_u 0.7192 (0.8105) acc_u 25.0000 (22.9167) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [50/53] time 0.472 (0.461) data 0.340 (0.331) loss_u loss_u 0.7217 (0.8089) acc_u 31.2500 (23.1250) lr 4.2499e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1406
confident_label rate tensor(0.4541, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1424
clean true:1376
clean false:48
clean_rate:0.9662921348314607
noisy true:354
noisy false:1358
after delete: len(clean_dataset) 1424
after delete: len(noisy_dataset) 1712
epoch [142/200] batch [5/44] time 0.535 (0.478) data 0.404 (0.347) loss_x loss_x 0.8633 (1.1214) acc_x 71.8750 (68.7500) lr 4.1221e-04 eta 0:00:18
epoch [142/200] batch [10/44] time 0.459 (0.467) data 0.330 (0.336) loss_x loss_x 1.5244 (1.1215) acc_x 56.2500 (70.3125) lr 4.1221e-04 eta 0:00:15
epoch [142/200] batch [15/44] time 0.447 (0.448) data 0.317 (0.318) loss_x loss_x 0.7524 (1.1454) acc_x 75.0000 (69.3750) lr 4.1221e-04 eta 0:00:12
epoch [142/200] batch [20/44] time 0.490 (0.453) data 0.360 (0.323) loss_x loss_x 1.0479 (1.1418) acc_x 81.2500 (70.1562) lr 4.1221e-04 eta 0:00:10
epoch [142/200] batch [25/44] time 0.508 (0.462) data 0.378 (0.331) loss_x loss_x 1.4775 (1.1350) acc_x 65.6250 (71.0000) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [30/44] time 0.393 (0.465) data 0.263 (0.334) loss_x loss_x 1.0098 (1.1265) acc_x 78.1250 (71.4583) lr 4.1221e-04 eta 0:00:06
epoch [142/200] batch [35/44] time 0.422 (0.467) data 0.290 (0.336) loss_x loss_x 1.3779 (1.1163) acc_x 71.8750 (72.0536) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [40/44] time 0.463 (0.471) data 0.332 (0.340) loss_x loss_x 1.1455 (1.1037) acc_x 71.8750 (72.3438) lr 4.1221e-04 eta 0:00:01
epoch [142/200] batch [5/53] time 0.468 (0.464) data 0.338 (0.334) loss_u loss_u 0.8311 (0.7835) acc_u 21.8750 (24.3750) lr 4.1221e-04 eta 0:00:22
epoch [142/200] batch [10/53] time 0.407 (0.459) data 0.275 (0.328) loss_u loss_u 0.7671 (0.8084) acc_u 25.0000 (21.8750) lr 4.1221e-04 eta 0:00:19
epoch [142/200] batch [15/53] time 0.535 (0.465) data 0.403 (0.335) loss_u loss_u 0.8535 (0.8010) acc_u 18.7500 (22.9167) lr 4.1221e-04 eta 0:00:17
epoch [142/200] batch [20/53] time 0.479 (0.465) data 0.348 (0.334) loss_u loss_u 0.8379 (0.8084) acc_u 21.8750 (22.5000) lr 4.1221e-04 eta 0:00:15
epoch [142/200] batch [25/53] time 0.342 (0.465) data 0.210 (0.334) loss_u loss_u 0.6704 (0.8036) acc_u 43.7500 (24.0000) lr 4.1221e-04 eta 0:00:13
epoch [142/200] batch [30/53] time 0.449 (0.462) data 0.317 (0.332) loss_u loss_u 0.7812 (0.7978) acc_u 31.2500 (24.7917) lr 4.1221e-04 eta 0:00:10
epoch [142/200] batch [35/53] time 0.372 (0.461) data 0.241 (0.330) loss_u loss_u 0.7988 (0.8025) acc_u 28.1250 (24.1071) lr 4.1221e-04 eta 0:00:08
epoch [142/200] batch [40/53] time 0.502 (0.462) data 0.371 (0.331) loss_u loss_u 0.7988 (0.8018) acc_u 25.0000 (24.2188) lr 4.1221e-04 eta 0:00:06
epoch [142/200] batch [45/53] time 0.369 (0.461) data 0.235 (0.330) loss_u loss_u 0.8696 (0.8040) acc_u 12.5000 (23.9583) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [50/53] time 0.574 (0.461) data 0.442 (0.329) loss_u loss_u 0.8130 (0.8046) acc_u 18.7500 (23.8125) lr 4.1221e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1363
confident_label rate tensor(0.4611, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1446
clean true:1409
clean false:37
clean_rate:0.9744121715076072
noisy true:364
noisy false:1326
after delete: len(clean_dataset) 1446
after delete: len(noisy_dataset) 1690
epoch [143/200] batch [5/45] time 0.455 (0.452) data 0.324 (0.321) loss_x loss_x 0.9429 (1.0261) acc_x 84.3750 (77.5000) lr 3.9958e-04 eta 0:00:18
epoch [143/200] batch [10/45] time 0.352 (0.455) data 0.221 (0.324) loss_x loss_x 0.9072 (1.1842) acc_x 75.0000 (72.5000) lr 3.9958e-04 eta 0:00:15
epoch [143/200] batch [15/45] time 0.463 (0.478) data 0.333 (0.347) loss_x loss_x 0.8472 (1.1870) acc_x 84.3750 (72.2917) lr 3.9958e-04 eta 0:00:14
epoch [143/200] batch [20/45] time 0.358 (0.464) data 0.227 (0.334) loss_x loss_x 0.9463 (1.2244) acc_x 65.6250 (70.7812) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [25/45] time 0.434 (0.456) data 0.304 (0.325) loss_x loss_x 1.2393 (1.2603) acc_x 62.5000 (69.5000) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [30/45] time 0.437 (0.451) data 0.306 (0.320) loss_x loss_x 1.2031 (1.2335) acc_x 71.8750 (69.7917) lr 3.9958e-04 eta 0:00:06
epoch [143/200] batch [35/45] time 0.518 (0.451) data 0.388 (0.320) loss_x loss_x 1.2109 (1.2091) acc_x 71.8750 (70.4464) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [40/45] time 0.441 (0.451) data 0.310 (0.321) loss_x loss_x 0.8545 (1.1907) acc_x 81.2500 (71.0938) lr 3.9958e-04 eta 0:00:02
epoch [143/200] batch [45/45] time 0.647 (0.461) data 0.516 (0.330) loss_x loss_x 1.3311 (1.1866) acc_x 65.6250 (71.1111) lr 3.9958e-04 eta 0:00:00
epoch [143/200] batch [5/52] time 0.448 (0.460) data 0.316 (0.329) loss_u loss_u 0.7979 (0.7888) acc_u 28.1250 (26.8750) lr 3.9958e-04 eta 0:00:21
epoch [143/200] batch [10/52] time 0.443 (0.459) data 0.313 (0.328) loss_u loss_u 0.8882 (0.7885) acc_u 15.6250 (27.1875) lr 3.9958e-04 eta 0:00:19
epoch [143/200] batch [15/52] time 0.413 (0.450) data 0.280 (0.320) loss_u loss_u 0.7964 (0.7970) acc_u 34.3750 (26.0417) lr 3.9958e-04 eta 0:00:16
epoch [143/200] batch [20/52] time 0.451 (0.450) data 0.320 (0.320) loss_u loss_u 0.8730 (0.8050) acc_u 12.5000 (24.8438) lr 3.9958e-04 eta 0:00:14
epoch [143/200] batch [25/52] time 0.376 (0.450) data 0.245 (0.319) loss_u loss_u 0.7778 (0.8063) acc_u 28.1250 (24.2500) lr 3.9958e-04 eta 0:00:12
epoch [143/200] batch [30/52] time 0.567 (0.452) data 0.437 (0.321) loss_u loss_u 0.7695 (0.8026) acc_u 25.0000 (24.7917) lr 3.9958e-04 eta 0:00:09
epoch [143/200] batch [35/52] time 0.422 (0.453) data 0.292 (0.322) loss_u loss_u 0.8198 (0.8063) acc_u 21.8750 (24.1964) lr 3.9958e-04 eta 0:00:07
epoch [143/200] batch [40/52] time 0.536 (0.455) data 0.405 (0.324) loss_u loss_u 0.7964 (0.8038) acc_u 21.8750 (24.5312) lr 3.9958e-04 eta 0:00:05
epoch [143/200] batch [45/52] time 0.384 (0.454) data 0.253 (0.323) loss_u loss_u 0.8193 (0.8062) acc_u 28.1250 (24.5139) lr 3.9958e-04 eta 0:00:03
epoch [143/200] batch [50/52] time 0.442 (0.453) data 0.310 (0.322) loss_u loss_u 0.8374 (0.8093) acc_u 18.7500 (24.1875) lr 3.9958e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1422
confident_label rate tensor(0.4461, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1399
clean true:1359
clean false:40
clean_rate:0.9714081486776269
noisy true:355
noisy false:1382
after delete: len(clean_dataset) 1399
after delete: len(noisy_dataset) 1737
epoch [144/200] batch [5/43] time 0.546 (0.500) data 0.415 (0.369) loss_x loss_x 0.8525 (0.9499) acc_x 78.1250 (75.0000) lr 3.8709e-04 eta 0:00:19
epoch [144/200] batch [10/43] time 0.504 (0.517) data 0.374 (0.386) loss_x loss_x 1.2627 (1.1750) acc_x 62.5000 (69.3750) lr 3.8709e-04 eta 0:00:17
epoch [144/200] batch [15/43] time 0.353 (0.481) data 0.222 (0.350) loss_x loss_x 1.5127 (1.1340) acc_x 62.5000 (71.4583) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [20/43] time 0.454 (0.479) data 0.323 (0.348) loss_x loss_x 0.9458 (1.1276) acc_x 78.1250 (71.2500) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [25/43] time 0.529 (0.473) data 0.398 (0.342) loss_x loss_x 0.8340 (1.1233) acc_x 71.8750 (71.2500) lr 3.8709e-04 eta 0:00:08
epoch [144/200] batch [30/43] time 0.469 (0.471) data 0.338 (0.340) loss_x loss_x 0.9639 (1.1446) acc_x 68.7500 (70.3125) lr 3.8709e-04 eta 0:00:06
epoch [144/200] batch [35/43] time 0.367 (0.466) data 0.237 (0.335) loss_x loss_x 1.4854 (1.1847) acc_x 65.6250 (69.6429) lr 3.8709e-04 eta 0:00:03
epoch [144/200] batch [40/43] time 0.455 (0.467) data 0.324 (0.336) loss_x loss_x 1.1182 (1.1980) acc_x 75.0000 (69.6875) lr 3.8709e-04 eta 0:00:01
epoch [144/200] batch [5/54] time 0.383 (0.468) data 0.253 (0.337) loss_u loss_u 0.7603 (0.7889) acc_u 28.1250 (25.6250) lr 3.8709e-04 eta 0:00:22
epoch [144/200] batch [10/54] time 0.385 (0.462) data 0.253 (0.331) loss_u loss_u 0.7583 (0.7865) acc_u 34.3750 (27.5000) lr 3.8709e-04 eta 0:00:20
epoch [144/200] batch [15/54] time 0.416 (0.460) data 0.286 (0.329) loss_u loss_u 0.8247 (0.7936) acc_u 21.8750 (26.6667) lr 3.8709e-04 eta 0:00:17
epoch [144/200] batch [20/54] time 0.442 (0.458) data 0.311 (0.327) loss_u loss_u 0.7812 (0.8030) acc_u 31.2500 (24.8438) lr 3.8709e-04 eta 0:00:15
epoch [144/200] batch [25/54] time 0.664 (0.457) data 0.533 (0.326) loss_u loss_u 0.7998 (0.8013) acc_u 21.8750 (25.2500) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [30/54] time 0.448 (0.462) data 0.317 (0.331) loss_u loss_u 0.7695 (0.7968) acc_u 21.8750 (25.9375) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [35/54] time 0.330 (0.461) data 0.200 (0.330) loss_u loss_u 0.7231 (0.7939) acc_u 34.3750 (26.0714) lr 3.8709e-04 eta 0:00:08
epoch [144/200] batch [40/54] time 0.419 (0.457) data 0.288 (0.326) loss_u loss_u 0.8267 (0.7933) acc_u 21.8750 (26.2500) lr 3.8709e-04 eta 0:00:06
epoch [144/200] batch [45/54] time 0.580 (0.459) data 0.448 (0.328) loss_u loss_u 0.7925 (0.7937) acc_u 28.1250 (26.1806) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [50/54] time 0.442 (0.457) data 0.311 (0.326) loss_u loss_u 0.9043 (0.7945) acc_u 15.6250 (25.9375) lr 3.8709e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1396
confident_label rate tensor(0.4547, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1426
clean true:1384
clean false:42
clean_rate:0.97054698457223
noisy true:356
noisy false:1354
after delete: len(clean_dataset) 1426
after delete: len(noisy_dataset) 1710
epoch [145/200] batch [5/44] time 0.387 (0.443) data 0.257 (0.312) loss_x loss_x 0.9790 (1.1631) acc_x 75.0000 (68.7500) lr 3.7476e-04 eta 0:00:17
epoch [145/200] batch [10/44] time 0.399 (0.446) data 0.269 (0.316) loss_x loss_x 1.2734 (1.2281) acc_x 62.5000 (66.8750) lr 3.7476e-04 eta 0:00:15
epoch [145/200] batch [15/44] time 0.473 (0.444) data 0.342 (0.314) loss_x loss_x 1.4062 (1.1391) acc_x 62.5000 (70.6250) lr 3.7476e-04 eta 0:00:12
epoch [145/200] batch [20/44] time 0.427 (0.453) data 0.296 (0.323) loss_x loss_x 1.1504 (1.1310) acc_x 68.7500 (70.6250) lr 3.7476e-04 eta 0:00:10
epoch [145/200] batch [25/44] time 0.522 (0.473) data 0.391 (0.343) loss_x loss_x 1.0420 (1.1181) acc_x 68.7500 (70.1250) lr 3.7476e-04 eta 0:00:08
epoch [145/200] batch [30/44] time 0.431 (0.476) data 0.301 (0.345) loss_x loss_x 1.1064 (1.1495) acc_x 68.7500 (69.4792) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [35/44] time 0.489 (0.477) data 0.359 (0.346) loss_x loss_x 0.9126 (1.1215) acc_x 81.2500 (70.4464) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [40/44] time 0.539 (0.479) data 0.409 (0.349) loss_x loss_x 1.4844 (1.1035) acc_x 65.6250 (71.1719) lr 3.7476e-04 eta 0:00:01
epoch [145/200] batch [5/53] time 0.446 (0.489) data 0.314 (0.358) loss_u loss_u 0.6680 (0.7600) acc_u 46.8750 (31.2500) lr 3.7476e-04 eta 0:00:23
epoch [145/200] batch [10/53] time 0.441 (0.486) data 0.309 (0.355) loss_u loss_u 0.8062 (0.7688) acc_u 25.0000 (29.3750) lr 3.7476e-04 eta 0:00:20
epoch [145/200] batch [15/53] time 0.382 (0.488) data 0.250 (0.357) loss_u loss_u 0.7720 (0.7716) acc_u 31.2500 (28.7500) lr 3.7476e-04 eta 0:00:18
epoch [145/200] batch [20/53] time 0.472 (0.484) data 0.338 (0.353) loss_u loss_u 0.7241 (0.7878) acc_u 31.2500 (25.9375) lr 3.7476e-04 eta 0:00:15
epoch [145/200] batch [25/53] time 0.396 (0.484) data 0.263 (0.353) loss_u loss_u 0.8052 (0.7960) acc_u 25.0000 (24.8750) lr 3.7476e-04 eta 0:00:13
epoch [145/200] batch [30/53] time 0.446 (0.481) data 0.314 (0.349) loss_u loss_u 0.8608 (0.8021) acc_u 12.5000 (23.9583) lr 3.7476e-04 eta 0:00:11
epoch [145/200] batch [35/53] time 0.530 (0.480) data 0.398 (0.349) loss_u loss_u 0.8979 (0.8060) acc_u 6.2500 (23.1250) lr 3.7476e-04 eta 0:00:08
epoch [145/200] batch [40/53] time 0.537 (0.479) data 0.406 (0.348) loss_u loss_u 0.7339 (0.8064) acc_u 28.1250 (22.9688) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [45/53] time 0.478 (0.480) data 0.346 (0.349) loss_u loss_u 0.7832 (0.8135) acc_u 21.8750 (22.0139) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [50/53] time 0.453 (0.480) data 0.322 (0.349) loss_u loss_u 0.7793 (0.8112) acc_u 25.0000 (22.4375) lr 3.7476e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1392
confident_label rate tensor(0.4560, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1430
clean true:1383
clean false:47
clean_rate:0.9671328671328672
noisy true:361
noisy false:1345
after delete: len(clean_dataset) 1430
after delete: len(noisy_dataset) 1706
epoch [146/200] batch [5/44] time 0.477 (0.488) data 0.346 (0.358) loss_x loss_x 1.0547 (1.0568) acc_x 75.0000 (74.3750) lr 3.6258e-04 eta 0:00:19
epoch [146/200] batch [10/44] time 0.493 (0.476) data 0.362 (0.345) loss_x loss_x 1.5205 (1.1386) acc_x 68.7500 (72.5000) lr 3.6258e-04 eta 0:00:16
epoch [146/200] batch [15/44] time 0.471 (0.461) data 0.339 (0.330) loss_x loss_x 1.8672 (1.1771) acc_x 56.2500 (71.6667) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [20/44] time 0.594 (0.477) data 0.462 (0.346) loss_x loss_x 1.2871 (1.1263) acc_x 65.6250 (72.6562) lr 3.6258e-04 eta 0:00:11
epoch [146/200] batch [25/44] time 0.415 (0.473) data 0.286 (0.342) loss_x loss_x 0.7896 (1.0858) acc_x 71.8750 (73.0000) lr 3.6258e-04 eta 0:00:08
epoch [146/200] batch [30/44] time 0.659 (0.479) data 0.528 (0.348) loss_x loss_x 1.3311 (1.1121) acc_x 65.6250 (72.5000) lr 3.6258e-04 eta 0:00:06
epoch [146/200] batch [35/44] time 0.682 (0.484) data 0.552 (0.353) loss_x loss_x 1.1045 (1.1273) acc_x 71.8750 (72.1429) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [40/44] time 0.472 (0.484) data 0.342 (0.353) loss_x loss_x 1.2559 (1.1346) acc_x 59.3750 (71.7188) lr 3.6258e-04 eta 0:00:01
epoch [146/200] batch [5/53] time 0.391 (0.483) data 0.259 (0.352) loss_u loss_u 0.8447 (0.7994) acc_u 21.8750 (25.0000) lr 3.6258e-04 eta 0:00:23
epoch [146/200] batch [10/53] time 0.419 (0.482) data 0.287 (0.351) loss_u loss_u 0.8374 (0.8155) acc_u 12.5000 (21.5625) lr 3.6258e-04 eta 0:00:20
epoch [146/200] batch [15/53] time 0.369 (0.482) data 0.239 (0.351) loss_u loss_u 0.8687 (0.8133) acc_u 15.6250 (23.1250) lr 3.6258e-04 eta 0:00:18
epoch [146/200] batch [20/53] time 0.458 (0.476) data 0.327 (0.345) loss_u loss_u 0.7070 (0.8047) acc_u 37.5000 (24.0625) lr 3.6258e-04 eta 0:00:15
epoch [146/200] batch [25/53] time 0.480 (0.477) data 0.349 (0.346) loss_u loss_u 0.8311 (0.8107) acc_u 28.1250 (23.5000) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [30/53] time 0.400 (0.475) data 0.269 (0.344) loss_u loss_u 0.8232 (0.8120) acc_u 25.0000 (23.6458) lr 3.6258e-04 eta 0:00:10
epoch [146/200] batch [35/53] time 0.412 (0.476) data 0.281 (0.345) loss_u loss_u 0.7837 (0.8083) acc_u 21.8750 (24.0179) lr 3.6258e-04 eta 0:00:08
epoch [146/200] batch [40/53] time 0.382 (0.472) data 0.252 (0.341) loss_u loss_u 0.8789 (0.8086) acc_u 15.6250 (24.0625) lr 3.6258e-04 eta 0:00:06
epoch [146/200] batch [45/53] time 0.459 (0.471) data 0.323 (0.340) loss_u loss_u 0.8232 (0.8066) acc_u 18.7500 (24.1667) lr 3.6258e-04 eta 0:00:03
epoch [146/200] batch [50/53] time 0.476 (0.475) data 0.344 (0.344) loss_u loss_u 0.8394 (0.8123) acc_u 12.5000 (23.5000) lr 3.6258e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1379
confident_label rate tensor(0.4601, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1443
clean true:1398
clean false:45
clean_rate:0.9688149688149689
noisy true:359
noisy false:1334
after delete: len(clean_dataset) 1443
after delete: len(noisy_dataset) 1693
epoch [147/200] batch [5/45] time 0.478 (0.501) data 0.347 (0.370) loss_x loss_x 1.3242 (1.0707) acc_x 62.5000 (71.2500) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [10/45] time 0.365 (0.476) data 0.234 (0.345) loss_x loss_x 1.3242 (1.0870) acc_x 62.5000 (71.5625) lr 3.5055e-04 eta 0:00:16
epoch [147/200] batch [15/45] time 0.424 (0.468) data 0.293 (0.337) loss_x loss_x 0.9033 (1.0735) acc_x 78.1250 (72.7083) lr 3.5055e-04 eta 0:00:14
epoch [147/200] batch [20/45] time 0.655 (0.484) data 0.524 (0.353) loss_x loss_x 0.8486 (1.0700) acc_x 87.5000 (73.2812) lr 3.5055e-04 eta 0:00:12
epoch [147/200] batch [25/45] time 0.388 (0.472) data 0.258 (0.341) loss_x loss_x 0.6187 (1.0777) acc_x 75.0000 (72.6250) lr 3.5055e-04 eta 0:00:09
epoch [147/200] batch [30/45] time 0.422 (0.478) data 0.291 (0.347) loss_x loss_x 0.8506 (1.0847) acc_x 81.2500 (72.6042) lr 3.5055e-04 eta 0:00:07
epoch [147/200] batch [35/45] time 0.533 (0.481) data 0.403 (0.351) loss_x loss_x 0.6587 (1.0782) acc_x 75.0000 (72.2321) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [40/45] time 0.493 (0.495) data 0.362 (0.364) loss_x loss_x 0.9199 (1.0787) acc_x 78.1250 (72.4219) lr 3.5055e-04 eta 0:00:02
epoch [147/200] batch [45/45] time 0.613 (0.500) data 0.482 (0.369) loss_x loss_x 0.8750 (1.0739) acc_x 75.0000 (72.5000) lr 3.5055e-04 eta 0:00:00
epoch [147/200] batch [5/52] time 0.654 (0.499) data 0.522 (0.368) loss_u loss_u 0.8535 (0.8066) acc_u 21.8750 (25.0000) lr 3.5055e-04 eta 0:00:23
epoch [147/200] batch [10/52] time 0.605 (0.496) data 0.474 (0.365) loss_u loss_u 0.7720 (0.7876) acc_u 31.2500 (26.8750) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [15/52] time 0.449 (0.492) data 0.318 (0.361) loss_u loss_u 0.7905 (0.7818) acc_u 31.2500 (28.7500) lr 3.5055e-04 eta 0:00:18
epoch [147/200] batch [20/52] time 0.456 (0.490) data 0.325 (0.359) loss_u loss_u 0.8940 (0.7981) acc_u 15.6250 (26.8750) lr 3.5055e-04 eta 0:00:15
epoch [147/200] batch [25/52] time 0.495 (0.488) data 0.363 (0.357) loss_u loss_u 0.8306 (0.8009) acc_u 15.6250 (26.1250) lr 3.5055e-04 eta 0:00:13
epoch [147/200] batch [30/52] time 0.583 (0.488) data 0.451 (0.357) loss_u loss_u 0.9062 (0.8044) acc_u 12.5000 (25.5208) lr 3.5055e-04 eta 0:00:10
epoch [147/200] batch [35/52] time 0.485 (0.486) data 0.354 (0.355) loss_u loss_u 0.8447 (0.8076) acc_u 15.6250 (24.8214) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [40/52] time 0.432 (0.482) data 0.301 (0.351) loss_u loss_u 0.8604 (0.8150) acc_u 18.7500 (23.9844) lr 3.5055e-04 eta 0:00:05
epoch [147/200] batch [45/52] time 0.565 (0.485) data 0.434 (0.354) loss_u loss_u 0.8579 (0.8149) acc_u 15.6250 (23.8889) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [50/52] time 0.468 (0.484) data 0.338 (0.353) loss_u loss_u 0.8340 (0.8146) acc_u 15.6250 (23.7500) lr 3.5055e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.4493, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1409
clean true:1374
clean false:35
clean_rate:0.9751596877217885
noisy true:349
noisy false:1378
after delete: len(clean_dataset) 1409
after delete: len(noisy_dataset) 1727
epoch [148/200] batch [5/44] time 0.434 (0.452) data 0.303 (0.322) loss_x loss_x 0.9502 (1.1029) acc_x 71.8750 (74.3750) lr 3.3869e-04 eta 0:00:17
epoch [148/200] batch [10/44] time 0.424 (0.463) data 0.294 (0.332) loss_x loss_x 1.2607 (1.1896) acc_x 71.8750 (71.8750) lr 3.3869e-04 eta 0:00:15
epoch [148/200] batch [15/44] time 0.533 (0.468) data 0.403 (0.338) loss_x loss_x 0.8647 (1.0972) acc_x 75.0000 (72.0833) lr 3.3869e-04 eta 0:00:13
epoch [148/200] batch [20/44] time 0.366 (0.456) data 0.235 (0.325) loss_x loss_x 1.0566 (1.0822) acc_x 75.0000 (72.6562) lr 3.3869e-04 eta 0:00:10
epoch [148/200] batch [25/44] time 0.391 (0.457) data 0.261 (0.327) loss_x loss_x 1.2422 (1.1047) acc_x 62.5000 (72.2500) lr 3.3869e-04 eta 0:00:08
epoch [148/200] batch [30/44] time 0.466 (0.458) data 0.336 (0.328) loss_x loss_x 0.7261 (1.1249) acc_x 84.3750 (71.6667) lr 3.3869e-04 eta 0:00:06
epoch [148/200] batch [35/44] time 0.404 (0.459) data 0.274 (0.328) loss_x loss_x 0.7080 (1.0873) acc_x 87.5000 (72.7679) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [40/44] time 0.386 (0.462) data 0.255 (0.331) loss_x loss_x 1.0195 (1.0931) acc_x 71.8750 (72.4219) lr 3.3869e-04 eta 0:00:01
epoch [148/200] batch [5/53] time 0.415 (0.464) data 0.284 (0.334) loss_u loss_u 0.8145 (0.8187) acc_u 25.0000 (20.0000) lr 3.3869e-04 eta 0:00:22
epoch [148/200] batch [10/53] time 0.438 (0.465) data 0.306 (0.334) loss_u loss_u 0.7588 (0.7890) acc_u 28.1250 (24.3750) lr 3.3869e-04 eta 0:00:19
epoch [148/200] batch [15/53] time 0.459 (0.466) data 0.328 (0.336) loss_u loss_u 0.8076 (0.7892) acc_u 15.6250 (24.3750) lr 3.3869e-04 eta 0:00:17
epoch [148/200] batch [20/53] time 0.443 (0.464) data 0.313 (0.333) loss_u loss_u 0.8604 (0.7994) acc_u 15.6250 (23.4375) lr 3.3869e-04 eta 0:00:15
epoch [148/200] batch [25/53] time 0.422 (0.461) data 0.292 (0.331) loss_u loss_u 0.8701 (0.7984) acc_u 15.6250 (24.2500) lr 3.3869e-04 eta 0:00:12
epoch [148/200] batch [30/53] time 0.530 (0.461) data 0.399 (0.330) loss_u loss_u 0.8115 (0.7974) acc_u 25.0000 (24.1667) lr 3.3869e-04 eta 0:00:10
epoch [148/200] batch [35/53] time 0.406 (0.456) data 0.275 (0.326) loss_u loss_u 0.8628 (0.7966) acc_u 15.6250 (23.9286) lr 3.3869e-04 eta 0:00:08
epoch [148/200] batch [40/53] time 0.504 (0.456) data 0.372 (0.325) loss_u loss_u 0.8389 (0.7961) acc_u 15.6250 (24.1406) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [45/53] time 0.611 (0.460) data 0.481 (0.329) loss_u loss_u 0.6787 (0.7981) acc_u 40.6250 (23.9583) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [50/53] time 0.429 (0.460) data 0.297 (0.329) loss_u loss_u 0.8301 (0.7979) acc_u 25.0000 (24.3125) lr 3.3869e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1410
confident_label rate tensor(0.4538, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1423
clean true:1377
clean false:46
clean_rate:0.9676739283204497
noisy true:349
noisy false:1364
after delete: len(clean_dataset) 1423
after delete: len(noisy_dataset) 1713
epoch [149/200] batch [5/44] time 0.431 (0.481) data 0.299 (0.350) loss_x loss_x 1.7725 (1.2230) acc_x 56.2500 (66.8750) lr 3.2699e-04 eta 0:00:18
epoch [149/200] batch [10/44] time 0.431 (0.468) data 0.299 (0.337) loss_x loss_x 1.6104 (1.2109) acc_x 65.6250 (69.6875) lr 3.2699e-04 eta 0:00:15
epoch [149/200] batch [15/44] time 0.452 (0.467) data 0.321 (0.336) loss_x loss_x 0.9478 (1.1525) acc_x 68.7500 (70.6250) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [20/44] time 0.477 (0.463) data 0.346 (0.332) loss_x loss_x 0.9141 (1.1022) acc_x 78.1250 (71.4062) lr 3.2699e-04 eta 0:00:11
epoch [149/200] batch [25/44] time 0.452 (0.469) data 0.322 (0.338) loss_x loss_x 0.8892 (1.0908) acc_x 81.2500 (70.8750) lr 3.2699e-04 eta 0:00:08
epoch [149/200] batch [30/44] time 0.518 (0.475) data 0.386 (0.344) loss_x loss_x 1.3066 (1.0807) acc_x 71.8750 (71.4583) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [35/44] time 0.665 (0.477) data 0.534 (0.346) loss_x loss_x 0.8467 (1.1053) acc_x 75.0000 (70.8929) lr 3.2699e-04 eta 0:00:04
epoch [149/200] batch [40/44] time 0.449 (0.474) data 0.318 (0.343) loss_x loss_x 1.5439 (1.1382) acc_x 56.2500 (70.0781) lr 3.2699e-04 eta 0:00:01
epoch [149/200] batch [5/53] time 0.432 (0.469) data 0.301 (0.338) loss_u loss_u 0.8047 (0.8176) acc_u 18.7500 (23.7500) lr 3.2699e-04 eta 0:00:22
epoch [149/200] batch [10/53] time 0.540 (0.467) data 0.409 (0.336) loss_u loss_u 0.8218 (0.8229) acc_u 21.8750 (22.8125) lr 3.2699e-04 eta 0:00:20
epoch [149/200] batch [15/53] time 0.491 (0.463) data 0.359 (0.332) loss_u loss_u 0.8203 (0.8181) acc_u 25.0000 (23.7500) lr 3.2699e-04 eta 0:00:17
epoch [149/200] batch [20/53] time 0.493 (0.461) data 0.363 (0.330) loss_u loss_u 0.8027 (0.8132) acc_u 25.0000 (23.9062) lr 3.2699e-04 eta 0:00:15
epoch [149/200] batch [25/53] time 0.417 (0.464) data 0.286 (0.332) loss_u loss_u 0.8179 (0.8130) acc_u 18.7500 (23.5000) lr 3.2699e-04 eta 0:00:12
epoch [149/200] batch [30/53] time 0.429 (0.461) data 0.296 (0.330) loss_u loss_u 0.8096 (0.8129) acc_u 18.7500 (23.1250) lr 3.2699e-04 eta 0:00:10
epoch [149/200] batch [35/53] time 0.598 (0.464) data 0.467 (0.333) loss_u loss_u 0.7407 (0.8148) acc_u 31.2500 (22.8571) lr 3.2699e-04 eta 0:00:08
epoch [149/200] batch [40/53] time 0.677 (0.468) data 0.545 (0.337) loss_u loss_u 0.7979 (0.8154) acc_u 25.0000 (22.5000) lr 3.2699e-04 eta 0:00:06
epoch [149/200] batch [45/53] time 0.415 (0.468) data 0.284 (0.337) loss_u loss_u 0.6392 (0.8095) acc_u 40.6250 (23.1250) lr 3.2699e-04 eta 0:00:03
epoch [149/200] batch [50/53] time 0.306 (0.464) data 0.174 (0.333) loss_u loss_u 0.7900 (0.8064) acc_u 25.0000 (23.5625) lr 3.2699e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1364
confident_label rate tensor(0.4656, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1460
clean true:1415
clean false:45
clean_rate:0.9691780821917808
noisy true:357
noisy false:1319
after delete: len(clean_dataset) 1460
after delete: len(noisy_dataset) 1676
epoch [150/200] batch [5/45] time 0.531 (0.498) data 0.400 (0.368) loss_x loss_x 1.3613 (1.0560) acc_x 59.3750 (73.1250) lr 3.1545e-04 eta 0:00:19
epoch [150/200] batch [10/45] time 0.466 (0.478) data 0.335 (0.348) loss_x loss_x 1.4521 (1.0891) acc_x 56.2500 (71.8750) lr 3.1545e-04 eta 0:00:16
epoch [150/200] batch [15/45] time 0.528 (0.498) data 0.396 (0.367) loss_x loss_x 1.0342 (1.1105) acc_x 75.0000 (71.8750) lr 3.1545e-04 eta 0:00:14
epoch [150/200] batch [20/45] time 0.488 (0.493) data 0.358 (0.362) loss_x loss_x 0.7949 (1.0974) acc_x 81.2500 (72.6562) lr 3.1545e-04 eta 0:00:12
epoch [150/200] batch [25/45] time 0.510 (0.482) data 0.379 (0.352) loss_x loss_x 1.0352 (1.0914) acc_x 68.7500 (72.7500) lr 3.1545e-04 eta 0:00:09
epoch [150/200] batch [30/45] time 0.431 (0.474) data 0.301 (0.343) loss_x loss_x 0.9097 (1.1180) acc_x 78.1250 (71.9792) lr 3.1545e-04 eta 0:00:07
epoch [150/200] batch [35/45] time 0.501 (0.473) data 0.371 (0.342) loss_x loss_x 1.8076 (1.1525) acc_x 56.2500 (71.3393) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [40/45] time 0.499 (0.472) data 0.368 (0.342) loss_x loss_x 1.2783 (1.1526) acc_x 62.5000 (70.9375) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [45/45] time 0.470 (0.476) data 0.339 (0.345) loss_x loss_x 1.0254 (1.1338) acc_x 81.2500 (71.3889) lr 3.1545e-04 eta 0:00:00
epoch [150/200] batch [5/52] time 0.509 (0.481) data 0.378 (0.350) loss_u loss_u 0.8516 (0.8157) acc_u 12.5000 (21.8750) lr 3.1545e-04 eta 0:00:22
epoch [150/200] batch [10/52] time 0.464 (0.479) data 0.333 (0.348) loss_u loss_u 0.8589 (0.8173) acc_u 15.6250 (21.2500) lr 3.1545e-04 eta 0:00:20
epoch [150/200] batch [15/52] time 0.412 (0.478) data 0.282 (0.347) loss_u loss_u 0.8184 (0.8342) acc_u 21.8750 (19.5833) lr 3.1545e-04 eta 0:00:17
epoch [150/200] batch [20/52] time 0.699 (0.485) data 0.568 (0.354) loss_u loss_u 0.7500 (0.8170) acc_u 34.3750 (22.0312) lr 3.1545e-04 eta 0:00:15
epoch [150/200] batch [25/52] time 0.468 (0.483) data 0.337 (0.352) loss_u loss_u 0.8848 (0.8206) acc_u 15.6250 (22.3750) lr 3.1545e-04 eta 0:00:13
epoch [150/200] batch [30/52] time 0.330 (0.480) data 0.199 (0.349) loss_u loss_u 0.7646 (0.8211) acc_u 25.0000 (22.0833) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [35/52] time 0.497 (0.477) data 0.366 (0.346) loss_u loss_u 0.8330 (0.8185) acc_u 18.7500 (22.5000) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [40/52] time 0.501 (0.474) data 0.368 (0.343) loss_u loss_u 0.8130 (0.8174) acc_u 18.7500 (22.8125) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [45/52] time 0.394 (0.475) data 0.262 (0.344) loss_u loss_u 0.8794 (0.8204) acc_u 9.3750 (22.3611) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [50/52] time 0.374 (0.475) data 0.242 (0.344) loss_u loss_u 0.8828 (0.8217) acc_u 18.7500 (22.1250) lr 3.1545e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1383
confident_label rate tensor(0.4589, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1439
clean true:1398
clean false:41
clean_rate:0.9715079916608756
noisy true:355
noisy false:1342
after delete: len(clean_dataset) 1439
after delete: len(noisy_dataset) 1697
epoch [151/200] batch [5/44] time 0.581 (0.479) data 0.451 (0.349) loss_x loss_x 0.7930 (1.1412) acc_x 87.5000 (73.1250) lr 3.0409e-04 eta 0:00:18
epoch [151/200] batch [10/44] time 0.417 (0.483) data 0.287 (0.352) loss_x loss_x 0.8325 (1.1309) acc_x 78.1250 (72.8125) lr 3.0409e-04 eta 0:00:16
epoch [151/200] batch [15/44] time 0.381 (0.451) data 0.251 (0.321) loss_x loss_x 1.1650 (1.1632) acc_x 75.0000 (72.9167) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [20/44] time 0.443 (0.448) data 0.313 (0.318) loss_x loss_x 1.5420 (1.1808) acc_x 68.7500 (72.1875) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [25/44] time 0.643 (0.457) data 0.513 (0.327) loss_x loss_x 1.4395 (1.1699) acc_x 68.7500 (72.6250) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [30/44] time 0.480 (0.458) data 0.349 (0.328) loss_x loss_x 1.1680 (1.1446) acc_x 62.5000 (72.6042) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [35/44] time 0.627 (0.473) data 0.497 (0.343) loss_x loss_x 0.9277 (1.1199) acc_x 78.1250 (72.6786) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [40/44] time 0.346 (0.469) data 0.216 (0.339) loss_x loss_x 1.0693 (1.1036) acc_x 75.0000 (72.7344) lr 3.0409e-04 eta 0:00:01
epoch [151/200] batch [5/53] time 0.391 (0.469) data 0.259 (0.339) loss_u loss_u 0.8013 (0.8017) acc_u 25.0000 (22.5000) lr 3.0409e-04 eta 0:00:22
epoch [151/200] batch [10/53] time 0.430 (0.467) data 0.298 (0.337) loss_u loss_u 0.8110 (0.7908) acc_u 21.8750 (25.3125) lr 3.0409e-04 eta 0:00:20
epoch [151/200] batch [15/53] time 0.629 (0.466) data 0.498 (0.336) loss_u loss_u 0.9238 (0.7998) acc_u 9.3750 (23.9583) lr 3.0409e-04 eta 0:00:17
epoch [151/200] batch [20/53] time 0.413 (0.471) data 0.282 (0.340) loss_u loss_u 0.7705 (0.8036) acc_u 37.5000 (24.2188) lr 3.0409e-04 eta 0:00:15
epoch [151/200] batch [25/53] time 0.399 (0.470) data 0.268 (0.339) loss_u loss_u 0.8003 (0.8109) acc_u 21.8750 (23.3750) lr 3.0409e-04 eta 0:00:13
epoch [151/200] batch [30/53] time 0.413 (0.468) data 0.282 (0.337) loss_u loss_u 0.8262 (0.8123) acc_u 25.0000 (23.7500) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [35/53] time 0.424 (0.471) data 0.293 (0.340) loss_u loss_u 0.8179 (0.8161) acc_u 28.1250 (23.2143) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [40/53] time 0.406 (0.467) data 0.275 (0.336) loss_u loss_u 0.8271 (0.8175) acc_u 15.6250 (23.1250) lr 3.0409e-04 eta 0:00:06
epoch [151/200] batch [45/53] time 0.340 (0.465) data 0.208 (0.334) loss_u loss_u 0.7979 (0.8188) acc_u 28.1250 (22.7083) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [50/53] time 0.450 (0.466) data 0.320 (0.335) loss_u loss_u 0.7012 (0.8131) acc_u 46.8750 (23.7500) lr 3.0409e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1413
confident_label rate tensor(0.4496, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1410
clean true:1366
clean false:44
clean_rate:0.9687943262411347
noisy true:357
noisy false:1369
after delete: len(clean_dataset) 1410
after delete: len(noisy_dataset) 1726
epoch [152/200] batch [5/44] time 0.472 (0.480) data 0.341 (0.348) loss_x loss_x 1.2148 (1.3488) acc_x 71.8750 (69.3750) lr 2.9289e-04 eta 0:00:18
epoch [152/200] batch [10/44] time 0.458 (0.463) data 0.328 (0.332) loss_x loss_x 1.4414 (1.1431) acc_x 71.8750 (73.1250) lr 2.9289e-04 eta 0:00:15
epoch [152/200] batch [15/44] time 0.486 (0.477) data 0.355 (0.347) loss_x loss_x 1.0977 (1.1463) acc_x 75.0000 (73.3333) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [20/44] time 0.508 (0.483) data 0.378 (0.352) loss_x loss_x 1.2686 (1.1197) acc_x 68.7500 (73.9062) lr 2.9289e-04 eta 0:00:11
epoch [152/200] batch [25/44] time 0.436 (0.467) data 0.306 (0.336) loss_x loss_x 1.6777 (1.1931) acc_x 62.5000 (72.8750) lr 2.9289e-04 eta 0:00:08
epoch [152/200] batch [30/44] time 0.500 (0.470) data 0.370 (0.340) loss_x loss_x 1.6914 (1.1999) acc_x 59.3750 (72.3958) lr 2.9289e-04 eta 0:00:06
epoch [152/200] batch [35/44] time 0.429 (0.469) data 0.298 (0.338) loss_x loss_x 1.2412 (1.1928) acc_x 65.6250 (72.3214) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [40/44] time 0.689 (0.475) data 0.557 (0.345) loss_x loss_x 1.0068 (1.1907) acc_x 78.1250 (72.5781) lr 2.9289e-04 eta 0:00:01
epoch [152/200] batch [5/53] time 0.409 (0.484) data 0.278 (0.353) loss_u loss_u 0.8359 (0.8130) acc_u 18.7500 (23.1250) lr 2.9289e-04 eta 0:00:23
epoch [152/200] batch [10/53] time 0.559 (0.480) data 0.428 (0.350) loss_u loss_u 0.8271 (0.7999) acc_u 25.0000 (25.6250) lr 2.9289e-04 eta 0:00:20
epoch [152/200] batch [15/53] time 0.424 (0.479) data 0.293 (0.349) loss_u loss_u 0.8076 (0.8089) acc_u 21.8750 (23.3333) lr 2.9289e-04 eta 0:00:18
epoch [152/200] batch [20/53] time 0.374 (0.475) data 0.242 (0.345) loss_u loss_u 0.8018 (0.8095) acc_u 18.7500 (23.5938) lr 2.9289e-04 eta 0:00:15
epoch [152/200] batch [25/53] time 0.367 (0.475) data 0.235 (0.344) loss_u loss_u 0.8501 (0.8098) acc_u 18.7500 (23.8750) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [30/53] time 0.390 (0.474) data 0.259 (0.343) loss_u loss_u 0.8809 (0.8089) acc_u 12.5000 (23.6458) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [35/53] time 0.427 (0.468) data 0.295 (0.337) loss_u loss_u 0.8271 (0.8049) acc_u 21.8750 (24.0179) lr 2.9289e-04 eta 0:00:08
epoch [152/200] batch [40/53] time 0.404 (0.465) data 0.272 (0.334) loss_u loss_u 0.7354 (0.8012) acc_u 37.5000 (24.4531) lr 2.9289e-04 eta 0:00:06
epoch [152/200] batch [45/53] time 0.521 (0.468) data 0.391 (0.338) loss_u loss_u 0.8208 (0.8040) acc_u 25.0000 (23.9583) lr 2.9289e-04 eta 0:00:03
epoch [152/200] batch [50/53] time 0.447 (0.465) data 0.317 (0.335) loss_u loss_u 0.8130 (0.8077) acc_u 18.7500 (23.1875) lr 2.9289e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1418
confident_label rate tensor(0.4477, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1404
clean true:1362
clean false:42
clean_rate:0.9700854700854701
noisy true:356
noisy false:1376
after delete: len(clean_dataset) 1404
after delete: len(noisy_dataset) 1732
epoch [153/200] batch [5/43] time 0.448 (0.427) data 0.319 (0.297) loss_x loss_x 1.5117 (1.1253) acc_x 62.5000 (69.3750) lr 2.8187e-04 eta 0:00:16
epoch [153/200] batch [10/43] time 0.412 (0.452) data 0.281 (0.321) loss_x loss_x 0.6196 (1.0511) acc_x 78.1250 (72.8125) lr 2.8187e-04 eta 0:00:14
epoch [153/200] batch [15/43] time 0.442 (0.453) data 0.312 (0.322) loss_x loss_x 0.9644 (1.0260) acc_x 78.1250 (74.1667) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [20/43] time 0.312 (0.439) data 0.182 (0.308) loss_x loss_x 1.1455 (1.0474) acc_x 68.7500 (73.7500) lr 2.8187e-04 eta 0:00:10
epoch [153/200] batch [25/43] time 0.471 (0.436) data 0.341 (0.305) loss_x loss_x 0.8223 (1.0826) acc_x 81.2500 (73.3750) lr 2.8187e-04 eta 0:00:07
epoch [153/200] batch [30/43] time 0.506 (0.446) data 0.375 (0.316) loss_x loss_x 1.4961 (1.1297) acc_x 65.6250 (71.5625) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [35/43] time 0.388 (0.446) data 0.258 (0.316) loss_x loss_x 1.2217 (1.1038) acc_x 71.8750 (72.1429) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [40/43] time 0.495 (0.455) data 0.365 (0.324) loss_x loss_x 1.0674 (1.1186) acc_x 68.7500 (71.7188) lr 2.8187e-04 eta 0:00:01
epoch [153/200] batch [5/54] time 0.599 (0.461) data 0.469 (0.331) loss_u loss_u 0.7783 (0.8188) acc_u 31.2500 (23.1250) lr 2.8187e-04 eta 0:00:22
epoch [153/200] batch [10/54] time 0.530 (0.459) data 0.399 (0.328) loss_u loss_u 0.7983 (0.7987) acc_u 31.2500 (25.3125) lr 2.8187e-04 eta 0:00:20
epoch [153/200] batch [15/54] time 0.502 (0.461) data 0.370 (0.331) loss_u loss_u 0.7959 (0.8095) acc_u 25.0000 (23.5417) lr 2.8187e-04 eta 0:00:17
epoch [153/200] batch [20/54] time 0.466 (0.459) data 0.334 (0.328) loss_u loss_u 0.7930 (0.8065) acc_u 18.7500 (24.0625) lr 2.8187e-04 eta 0:00:15
epoch [153/200] batch [25/54] time 0.431 (0.455) data 0.301 (0.324) loss_u loss_u 0.7739 (0.8055) acc_u 34.3750 (24.6250) lr 2.8187e-04 eta 0:00:13
epoch [153/200] batch [30/54] time 0.526 (0.458) data 0.396 (0.327) loss_u loss_u 0.7373 (0.8073) acc_u 34.3750 (24.7917) lr 2.8187e-04 eta 0:00:10
epoch [153/200] batch [35/54] time 0.572 (0.463) data 0.441 (0.333) loss_u loss_u 0.7915 (0.8119) acc_u 25.0000 (24.0179) lr 2.8187e-04 eta 0:00:08
epoch [153/200] batch [40/54] time 0.495 (0.464) data 0.363 (0.334) loss_u loss_u 0.7686 (0.8082) acc_u 37.5000 (24.2969) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [45/54] time 0.525 (0.463) data 0.394 (0.332) loss_u loss_u 0.8218 (0.8052) acc_u 21.8750 (24.5833) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [50/54] time 0.538 (0.464) data 0.407 (0.333) loss_u loss_u 0.7476 (0.8035) acc_u 34.3750 (24.8750) lr 2.8187e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1414
confident_label rate tensor(0.4512, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1415
clean true:1372
clean false:43
clean_rate:0.9696113074204947
noisy true:350
noisy false:1371
after delete: len(clean_dataset) 1415
after delete: len(noisy_dataset) 1721
epoch [154/200] batch [5/44] time 0.488 (0.499) data 0.358 (0.369) loss_x loss_x 1.5537 (1.1525) acc_x 59.3750 (73.7500) lr 2.7103e-04 eta 0:00:19
epoch [154/200] batch [10/44] time 0.476 (0.481) data 0.346 (0.351) loss_x loss_x 0.7949 (1.1031) acc_x 78.1250 (71.8750) lr 2.7103e-04 eta 0:00:16
epoch [154/200] batch [15/44] time 0.517 (0.489) data 0.387 (0.359) loss_x loss_x 1.2432 (1.0637) acc_x 71.8750 (73.7500) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [20/44] time 0.457 (0.484) data 0.327 (0.354) loss_x loss_x 1.0156 (1.1345) acc_x 71.8750 (72.6562) lr 2.7103e-04 eta 0:00:11
epoch [154/200] batch [25/44] time 0.484 (0.473) data 0.354 (0.343) loss_x loss_x 1.0332 (1.1363) acc_x 68.7500 (72.5000) lr 2.7103e-04 eta 0:00:08
epoch [154/200] batch [30/44] time 0.503 (0.472) data 0.373 (0.342) loss_x loss_x 1.6934 (1.1588) acc_x 46.8750 (71.6667) lr 2.7103e-04 eta 0:00:06
epoch [154/200] batch [35/44] time 0.483 (0.466) data 0.353 (0.336) loss_x loss_x 0.7285 (1.1605) acc_x 84.3750 (71.5179) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [40/44] time 0.386 (0.463) data 0.256 (0.333) loss_x loss_x 0.9531 (1.1472) acc_x 78.1250 (71.8750) lr 2.7103e-04 eta 0:00:01
epoch [154/200] batch [5/53] time 0.359 (0.456) data 0.229 (0.325) loss_u loss_u 0.7661 (0.8041) acc_u 21.8750 (21.8750) lr 2.7103e-04 eta 0:00:21
epoch [154/200] batch [10/53] time 0.432 (0.458) data 0.300 (0.328) loss_u loss_u 0.7471 (0.8050) acc_u 37.5000 (22.8125) lr 2.7103e-04 eta 0:00:19
epoch [154/200] batch [15/53] time 0.458 (0.456) data 0.328 (0.325) loss_u loss_u 0.7456 (0.7939) acc_u 34.3750 (25.2083) lr 2.7103e-04 eta 0:00:17
epoch [154/200] batch [20/53] time 0.431 (0.454) data 0.300 (0.324) loss_u loss_u 0.7998 (0.7972) acc_u 25.0000 (25.3125) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [25/53] time 0.400 (0.455) data 0.269 (0.324) loss_u loss_u 0.7847 (0.7989) acc_u 25.0000 (24.8750) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [30/53] time 0.411 (0.456) data 0.280 (0.326) loss_u loss_u 0.8486 (0.7994) acc_u 15.6250 (25.0000) lr 2.7103e-04 eta 0:00:10
epoch [154/200] batch [35/53] time 0.563 (0.458) data 0.432 (0.328) loss_u loss_u 0.7988 (0.8046) acc_u 25.0000 (23.9286) lr 2.7103e-04 eta 0:00:08
epoch [154/200] batch [40/53] time 0.443 (0.457) data 0.313 (0.326) loss_u loss_u 0.9185 (0.8117) acc_u 9.3750 (23.0469) lr 2.7103e-04 eta 0:00:05
epoch [154/200] batch [45/53] time 0.366 (0.458) data 0.237 (0.327) loss_u loss_u 0.8242 (0.8137) acc_u 18.7500 (22.7778) lr 2.7103e-04 eta 0:00:03
epoch [154/200] batch [50/53] time 0.344 (0.457) data 0.214 (0.326) loss_u loss_u 0.7861 (0.8119) acc_u 31.2500 (23.2500) lr 2.7103e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1388
confident_label rate tensor(0.4582, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1437
clean true:1394
clean false:43
clean_rate:0.9700765483646486
noisy true:354
noisy false:1345
after delete: len(clean_dataset) 1437
after delete: len(noisy_dataset) 1699
epoch [155/200] batch [5/44] time 0.496 (0.450) data 0.366 (0.320) loss_x loss_x 1.1113 (1.0372) acc_x 75.0000 (75.6250) lr 2.6037e-04 eta 0:00:17
epoch [155/200] batch [10/44] time 0.480 (0.464) data 0.350 (0.333) loss_x loss_x 0.8594 (1.0728) acc_x 78.1250 (74.3750) lr 2.6037e-04 eta 0:00:15
epoch [155/200] batch [15/44] time 0.631 (0.462) data 0.500 (0.331) loss_x loss_x 1.3271 (1.1605) acc_x 68.7500 (71.4583) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [20/44] time 0.562 (0.477) data 0.432 (0.346) loss_x loss_x 1.2500 (1.1183) acc_x 65.6250 (72.0312) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [25/44] time 0.552 (0.475) data 0.422 (0.345) loss_x loss_x 0.9185 (1.1173) acc_x 71.8750 (72.0000) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [30/44] time 0.438 (0.475) data 0.307 (0.344) loss_x loss_x 1.2207 (1.1381) acc_x 75.0000 (71.9792) lr 2.6037e-04 eta 0:00:06
epoch [155/200] batch [35/44] time 0.671 (0.477) data 0.541 (0.347) loss_x loss_x 0.6577 (1.1175) acc_x 84.3750 (72.1429) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [40/44] time 0.394 (0.475) data 0.264 (0.345) loss_x loss_x 0.7588 (1.1274) acc_x 78.1250 (71.4844) lr 2.6037e-04 eta 0:00:01
epoch [155/200] batch [5/53] time 0.505 (0.474) data 0.374 (0.343) loss_u loss_u 0.7959 (0.8269) acc_u 21.8750 (21.8750) lr 2.6037e-04 eta 0:00:22
epoch [155/200] batch [10/53] time 0.506 (0.479) data 0.376 (0.348) loss_u loss_u 0.8042 (0.8104) acc_u 25.0000 (23.7500) lr 2.6037e-04 eta 0:00:20
epoch [155/200] batch [15/53] time 0.432 (0.479) data 0.302 (0.349) loss_u loss_u 0.7915 (0.8116) acc_u 25.0000 (23.1250) lr 2.6037e-04 eta 0:00:18
epoch [155/200] batch [20/53] time 0.395 (0.478) data 0.263 (0.347) loss_u loss_u 0.7266 (0.7933) acc_u 37.5000 (25.6250) lr 2.6037e-04 eta 0:00:15
epoch [155/200] batch [25/53] time 0.539 (0.478) data 0.408 (0.347) loss_u loss_u 0.8550 (0.7992) acc_u 18.7500 (24.6250) lr 2.6037e-04 eta 0:00:13
epoch [155/200] batch [30/53] time 0.430 (0.474) data 0.299 (0.343) loss_u loss_u 0.7949 (0.7957) acc_u 28.1250 (25.0000) lr 2.6037e-04 eta 0:00:10
epoch [155/200] batch [35/53] time 0.452 (0.470) data 0.320 (0.340) loss_u loss_u 0.8408 (0.7991) acc_u 18.7500 (24.5536) lr 2.6037e-04 eta 0:00:08
epoch [155/200] batch [40/53] time 0.367 (0.467) data 0.237 (0.336) loss_u loss_u 0.8374 (0.8047) acc_u 18.7500 (24.0625) lr 2.6037e-04 eta 0:00:06
epoch [155/200] batch [45/53] time 0.524 (0.465) data 0.393 (0.334) loss_u loss_u 0.8496 (0.8072) acc_u 21.8750 (23.6111) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [50/53] time 0.449 (0.461) data 0.319 (0.330) loss_u loss_u 0.7422 (0.8082) acc_u 40.6250 (23.6250) lr 2.6037e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1362
confident_label rate tensor(0.4627, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1451
clean true:1408
clean false:43
clean_rate:0.9703652653342523
noisy true:366
noisy false:1319
after delete: len(clean_dataset) 1451
after delete: len(noisy_dataset) 1685
epoch [156/200] batch [5/45] time 0.578 (0.505) data 0.446 (0.374) loss_x loss_x 1.4326 (1.0298) acc_x 65.6250 (74.3750) lr 2.4989e-04 eta 0:00:20
epoch [156/200] batch [10/45] time 0.481 (0.490) data 0.351 (0.359) loss_x loss_x 0.7886 (1.0131) acc_x 84.3750 (76.2500) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [15/45] time 0.392 (0.489) data 0.261 (0.358) loss_x loss_x 1.0146 (1.1321) acc_x 75.0000 (74.5833) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [20/45] time 0.560 (0.477) data 0.429 (0.346) loss_x loss_x 0.8428 (1.1193) acc_x 75.0000 (73.4375) lr 2.4989e-04 eta 0:00:11
epoch [156/200] batch [25/45] time 0.426 (0.471) data 0.295 (0.340) loss_x loss_x 1.0654 (1.1430) acc_x 59.3750 (72.1250) lr 2.4989e-04 eta 0:00:09
epoch [156/200] batch [30/45] time 0.413 (0.468) data 0.284 (0.337) loss_x loss_x 1.2822 (1.0963) acc_x 65.6250 (73.1250) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [35/45] time 0.468 (0.471) data 0.337 (0.340) loss_x loss_x 0.8721 (1.1049) acc_x 68.7500 (72.1429) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [40/45] time 0.399 (0.471) data 0.268 (0.340) loss_x loss_x 1.1094 (1.1173) acc_x 78.1250 (72.2656) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [45/45] time 0.504 (0.475) data 0.373 (0.344) loss_x loss_x 1.6602 (1.1320) acc_x 62.5000 (71.5278) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/52] time 0.458 (0.475) data 0.327 (0.345) loss_u loss_u 0.8203 (0.8320) acc_u 15.6250 (18.7500) lr 2.4989e-04 eta 0:00:22
epoch [156/200] batch [10/52] time 0.566 (0.477) data 0.433 (0.346) loss_u loss_u 0.7158 (0.8094) acc_u 31.2500 (21.8750) lr 2.4989e-04 eta 0:00:20
epoch [156/200] batch [15/52] time 0.372 (0.473) data 0.241 (0.341) loss_u loss_u 0.8794 (0.8201) acc_u 15.6250 (21.4583) lr 2.4989e-04 eta 0:00:17
epoch [156/200] batch [20/52] time 0.524 (0.475) data 0.394 (0.344) loss_u loss_u 0.7886 (0.8258) acc_u 21.8750 (20.1562) lr 2.4989e-04 eta 0:00:15
epoch [156/200] batch [25/52] time 0.470 (0.473) data 0.340 (0.342) loss_u loss_u 0.7837 (0.8214) acc_u 25.0000 (21.2500) lr 2.4989e-04 eta 0:00:12
epoch [156/200] batch [30/52] time 0.468 (0.470) data 0.337 (0.339) loss_u loss_u 0.8169 (0.8204) acc_u 18.7500 (21.3542) lr 2.4989e-04 eta 0:00:10
epoch [156/200] batch [35/52] time 0.466 (0.468) data 0.334 (0.337) loss_u loss_u 0.8721 (0.8178) acc_u 15.6250 (21.5179) lr 2.4989e-04 eta 0:00:07
epoch [156/200] batch [40/52] time 0.481 (0.466) data 0.350 (0.335) loss_u loss_u 0.8008 (0.8179) acc_u 25.0000 (21.6406) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [45/52] time 0.365 (0.463) data 0.234 (0.332) loss_u loss_u 0.8130 (0.8190) acc_u 21.8750 (21.7361) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [50/52] time 0.406 (0.468) data 0.275 (0.337) loss_u loss_u 0.8540 (0.8165) acc_u 12.5000 (22.0000) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1394
confident_label rate tensor(0.4522, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1418
clean true:1372
clean false:46
clean_rate:0.9675599435825106
noisy true:370
noisy false:1348
after delete: len(clean_dataset) 1418
after delete: len(noisy_dataset) 1718
epoch [157/200] batch [5/44] time 0.445 (0.571) data 0.315 (0.440) loss_x loss_x 1.2393 (1.1280) acc_x 78.1250 (71.8750) lr 2.3959e-04 eta 0:00:22
epoch [157/200] batch [10/44] time 0.377 (0.505) data 0.247 (0.375) loss_x loss_x 1.4609 (1.1672) acc_x 65.6250 (70.0000) lr 2.3959e-04 eta 0:00:17
epoch [157/200] batch [15/44] time 0.586 (0.502) data 0.456 (0.372) loss_x loss_x 1.3057 (1.1733) acc_x 68.7500 (70.0000) lr 2.3959e-04 eta 0:00:14
epoch [157/200] batch [20/44] time 0.454 (0.492) data 0.323 (0.361) loss_x loss_x 1.1807 (1.1354) acc_x 75.0000 (70.3125) lr 2.3959e-04 eta 0:00:11
epoch [157/200] batch [25/44] time 0.539 (0.490) data 0.408 (0.359) loss_x loss_x 0.7964 (1.1411) acc_x 81.2500 (70.2500) lr 2.3959e-04 eta 0:00:09
epoch [157/200] batch [30/44] time 0.596 (0.486) data 0.466 (0.355) loss_x loss_x 1.0303 (1.1240) acc_x 71.8750 (70.7292) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [35/44] time 0.538 (0.498) data 0.408 (0.367) loss_x loss_x 1.0820 (1.1302) acc_x 78.1250 (70.1786) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [40/44] time 0.412 (0.492) data 0.282 (0.362) loss_x loss_x 1.2812 (1.1405) acc_x 75.0000 (70.1562) lr 2.3959e-04 eta 0:00:01
epoch [157/200] batch [5/53] time 0.462 (0.486) data 0.330 (0.355) loss_u loss_u 0.7061 (0.7603) acc_u 34.3750 (30.0000) lr 2.3959e-04 eta 0:00:23
epoch [157/200] batch [10/53] time 0.461 (0.483) data 0.329 (0.352) loss_u loss_u 0.8027 (0.7892) acc_u 21.8750 (25.6250) lr 2.3959e-04 eta 0:00:20
epoch [157/200] batch [15/53] time 0.493 (0.483) data 0.362 (0.352) loss_u loss_u 0.7871 (0.7865) acc_u 34.3750 (26.6667) lr 2.3959e-04 eta 0:00:18
epoch [157/200] batch [20/53] time 0.409 (0.481) data 0.278 (0.351) loss_u loss_u 0.7769 (0.7843) acc_u 28.1250 (27.6562) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [25/53] time 0.647 (0.484) data 0.516 (0.353) loss_u loss_u 0.8052 (0.7892) acc_u 21.8750 (26.2500) lr 2.3959e-04 eta 0:00:13
epoch [157/200] batch [30/53] time 0.416 (0.481) data 0.284 (0.350) loss_u loss_u 0.8257 (0.7911) acc_u 25.0000 (26.2500) lr 2.3959e-04 eta 0:00:11
epoch [157/200] batch [35/53] time 0.397 (0.479) data 0.266 (0.348) loss_u loss_u 0.7759 (0.7878) acc_u 28.1250 (26.7857) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [40/53] time 0.440 (0.481) data 0.309 (0.350) loss_u loss_u 0.8408 (0.7918) acc_u 21.8750 (25.8594) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [45/53] time 0.385 (0.480) data 0.255 (0.349) loss_u loss_u 0.8159 (0.7965) acc_u 18.7500 (25.3472) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [50/53] time 0.446 (0.481) data 0.315 (0.351) loss_u loss_u 0.7129 (0.7970) acc_u 40.6250 (25.2500) lr 2.3959e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1379
confident_label rate tensor(0.4646, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1457
clean true:1410
clean false:47
clean_rate:0.967741935483871
noisy true:347
noisy false:1332
after delete: len(clean_dataset) 1457
after delete: len(noisy_dataset) 1679
epoch [158/200] batch [5/45] time 0.541 (0.462) data 0.411 (0.332) loss_x loss_x 1.1680 (1.1569) acc_x 62.5000 (68.7500) lr 2.2949e-04 eta 0:00:18
epoch [158/200] batch [10/45] time 0.409 (0.467) data 0.278 (0.337) loss_x loss_x 0.9639 (1.0952) acc_x 71.8750 (71.8750) lr 2.2949e-04 eta 0:00:16
epoch [158/200] batch [15/45] time 0.482 (0.465) data 0.352 (0.334) loss_x loss_x 0.9121 (1.1277) acc_x 78.1250 (71.4583) lr 2.2949e-04 eta 0:00:13
epoch [158/200] batch [20/45] time 0.562 (0.470) data 0.431 (0.340) loss_x loss_x 0.6426 (1.1055) acc_x 87.5000 (72.8125) lr 2.2949e-04 eta 0:00:11
epoch [158/200] batch [25/45] time 0.484 (0.479) data 0.353 (0.349) loss_x loss_x 1.6279 (1.1294) acc_x 65.6250 (72.2500) lr 2.2949e-04 eta 0:00:09
epoch [158/200] batch [30/45] time 0.360 (0.478) data 0.230 (0.347) loss_x loss_x 1.0234 (1.1286) acc_x 75.0000 (72.6042) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [35/45] time 0.548 (0.477) data 0.417 (0.347) loss_x loss_x 0.7720 (1.1143) acc_x 78.1250 (72.2321) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [40/45] time 0.389 (0.472) data 0.259 (0.342) loss_x loss_x 1.1230 (1.1109) acc_x 75.0000 (72.4219) lr 2.2949e-04 eta 0:00:02
epoch [158/200] batch [45/45] time 0.484 (0.479) data 0.353 (0.349) loss_x loss_x 0.9648 (1.0790) acc_x 68.7500 (72.6389) lr 2.2949e-04 eta 0:00:00
epoch [158/200] batch [5/52] time 0.531 (0.476) data 0.400 (0.345) loss_u loss_u 0.8354 (0.7995) acc_u 12.5000 (24.3750) lr 2.2949e-04 eta 0:00:22
epoch [158/200] batch [10/52] time 0.419 (0.471) data 0.287 (0.341) loss_u loss_u 0.8330 (0.8043) acc_u 18.7500 (23.1250) lr 2.2949e-04 eta 0:00:19
epoch [158/200] batch [15/52] time 0.524 (0.471) data 0.392 (0.341) loss_u loss_u 0.8818 (0.8082) acc_u 12.5000 (22.7083) lr 2.2949e-04 eta 0:00:17
epoch [158/200] batch [20/52] time 0.457 (0.469) data 0.326 (0.339) loss_u loss_u 0.8086 (0.8039) acc_u 31.2500 (23.2812) lr 2.2949e-04 eta 0:00:15
epoch [158/200] batch [25/52] time 0.562 (0.468) data 0.430 (0.337) loss_u loss_u 0.8374 (0.8158) acc_u 12.5000 (21.8750) lr 2.2949e-04 eta 0:00:12
epoch [158/200] batch [30/52] time 0.380 (0.470) data 0.249 (0.339) loss_u loss_u 0.7212 (0.8134) acc_u 31.2500 (22.3958) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [35/52] time 0.454 (0.469) data 0.324 (0.338) loss_u loss_u 0.8721 (0.8160) acc_u 15.6250 (22.5000) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [40/52] time 0.478 (0.467) data 0.347 (0.336) loss_u loss_u 0.8364 (0.8166) acc_u 21.8750 (22.5781) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [45/52] time 0.497 (0.474) data 0.366 (0.343) loss_u loss_u 0.7100 (0.8163) acc_u 34.3750 (22.7778) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [50/52] time 0.412 (0.471) data 0.281 (0.340) loss_u loss_u 0.7969 (0.8170) acc_u 18.7500 (22.3125) lr 2.2949e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1397
confident_label rate tensor(0.4566, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1432
clean true:1392
clean false:40
clean_rate:0.9720670391061452
noisy true:347
noisy false:1357
after delete: len(clean_dataset) 1432
after delete: len(noisy_dataset) 1704
epoch [159/200] batch [5/44] time 0.538 (0.494) data 0.407 (0.364) loss_x loss_x 1.0059 (0.9829) acc_x 68.7500 (71.8750) lr 2.1957e-04 eta 0:00:19
epoch [159/200] batch [10/44] time 0.439 (0.478) data 0.308 (0.348) loss_x loss_x 1.2812 (1.0714) acc_x 62.5000 (70.0000) lr 2.1957e-04 eta 0:00:16
epoch [159/200] batch [15/44] time 0.675 (0.486) data 0.545 (0.355) loss_x loss_x 1.2236 (1.0686) acc_x 81.2500 (72.7083) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [20/44] time 0.507 (0.475) data 0.376 (0.344) loss_x loss_x 0.6016 (1.0706) acc_x 84.3750 (72.9688) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [25/44] time 0.411 (0.468) data 0.281 (0.337) loss_x loss_x 0.9888 (1.0870) acc_x 78.1250 (72.5000) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [30/44] time 0.478 (0.462) data 0.347 (0.331) loss_x loss_x 0.4741 (1.0617) acc_x 87.5000 (73.1250) lr 2.1957e-04 eta 0:00:06
epoch [159/200] batch [35/44] time 0.433 (0.466) data 0.303 (0.335) loss_x loss_x 1.1064 (1.0777) acc_x 71.8750 (72.4107) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [40/44] time 0.466 (0.465) data 0.335 (0.335) loss_x loss_x 0.9219 (1.0976) acc_x 68.7500 (71.4062) lr 2.1957e-04 eta 0:00:01
epoch [159/200] batch [5/53] time 0.477 (0.467) data 0.347 (0.337) loss_u loss_u 0.8652 (0.7579) acc_u 18.7500 (29.3750) lr 2.1957e-04 eta 0:00:22
epoch [159/200] batch [10/53] time 0.580 (0.466) data 0.450 (0.336) loss_u loss_u 0.7891 (0.7749) acc_u 28.1250 (27.8125) lr 2.1957e-04 eta 0:00:20
epoch [159/200] batch [15/53] time 0.565 (0.473) data 0.434 (0.342) loss_u loss_u 0.8237 (0.7800) acc_u 21.8750 (27.5000) lr 2.1957e-04 eta 0:00:17
epoch [159/200] batch [20/53] time 0.486 (0.471) data 0.354 (0.341) loss_u loss_u 0.7915 (0.7802) acc_u 28.1250 (28.9062) lr 2.1957e-04 eta 0:00:15
epoch [159/200] batch [25/53] time 0.497 (0.469) data 0.365 (0.338) loss_u loss_u 0.8555 (0.7856) acc_u 15.6250 (28.5000) lr 2.1957e-04 eta 0:00:13
epoch [159/200] batch [30/53] time 0.475 (0.468) data 0.343 (0.337) loss_u loss_u 0.8262 (0.7962) acc_u 18.7500 (26.2500) lr 2.1957e-04 eta 0:00:10
epoch [159/200] batch [35/53] time 0.500 (0.469) data 0.369 (0.339) loss_u loss_u 0.7578 (0.7986) acc_u 31.2500 (25.5357) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [40/53] time 0.622 (0.470) data 0.491 (0.340) loss_u loss_u 0.8086 (0.8003) acc_u 25.0000 (25.1562) lr 2.1957e-04 eta 0:00:06
epoch [159/200] batch [45/53] time 0.414 (0.469) data 0.282 (0.338) loss_u loss_u 0.7529 (0.7988) acc_u 31.2500 (25.4861) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [50/53] time 0.679 (0.472) data 0.548 (0.341) loss_u loss_u 0.7964 (0.8002) acc_u 18.7500 (25.0625) lr 2.1957e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1376
confident_label rate tensor(0.4617, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1448
clean true:1404
clean false:44
clean_rate:0.9696132596685083
noisy true:356
noisy false:1332
after delete: len(clean_dataset) 1448
after delete: len(noisy_dataset) 1688
epoch [160/200] batch [5/45] time 0.484 (0.433) data 0.353 (0.303) loss_x loss_x 0.6724 (0.9345) acc_x 81.2500 (75.6250) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [10/45] time 0.411 (0.441) data 0.280 (0.311) loss_x loss_x 1.0723 (0.9988) acc_x 65.6250 (72.8125) lr 2.0984e-04 eta 0:00:15
epoch [160/200] batch [15/45] time 0.381 (0.434) data 0.251 (0.303) loss_x loss_x 1.0371 (1.0566) acc_x 71.8750 (73.1250) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [20/45] time 0.525 (0.435) data 0.395 (0.305) loss_x loss_x 1.0654 (1.0681) acc_x 75.0000 (72.8125) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [25/45] time 0.399 (0.438) data 0.269 (0.307) loss_x loss_x 1.0068 (1.0864) acc_x 68.7500 (71.6250) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [30/45] time 0.435 (0.447) data 0.305 (0.316) loss_x loss_x 1.7148 (1.0931) acc_x 56.2500 (71.3542) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [35/45] time 0.518 (0.454) data 0.388 (0.323) loss_x loss_x 1.0645 (1.1237) acc_x 71.8750 (70.8929) lr 2.0984e-04 eta 0:00:04
epoch [160/200] batch [40/45] time 0.500 (0.460) data 0.369 (0.330) loss_x loss_x 1.0703 (1.1410) acc_x 81.2500 (70.8594) lr 2.0984e-04 eta 0:00:02
epoch [160/200] batch [45/45] time 0.579 (0.463) data 0.449 (0.333) loss_x loss_x 1.2842 (1.1292) acc_x 62.5000 (71.3194) lr 2.0984e-04 eta 0:00:00
epoch [160/200] batch [5/52] time 0.466 (0.461) data 0.336 (0.331) loss_u loss_u 0.7773 (0.8071) acc_u 28.1250 (25.0000) lr 2.0984e-04 eta 0:00:21
epoch [160/200] batch [10/52] time 0.446 (0.467) data 0.315 (0.337) loss_u loss_u 0.8208 (0.8246) acc_u 18.7500 (22.5000) lr 2.0984e-04 eta 0:00:19
epoch [160/200] batch [15/52] time 0.479 (0.470) data 0.349 (0.340) loss_u loss_u 0.7290 (0.8076) acc_u 28.1250 (22.9167) lr 2.0984e-04 eta 0:00:17
epoch [160/200] batch [20/52] time 0.404 (0.472) data 0.271 (0.341) loss_u loss_u 0.8506 (0.8101) acc_u 15.6250 (22.5000) lr 2.0984e-04 eta 0:00:15
epoch [160/200] batch [25/52] time 0.419 (0.471) data 0.287 (0.340) loss_u loss_u 0.8462 (0.8127) acc_u 15.6250 (22.1250) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [30/52] time 0.368 (0.473) data 0.236 (0.342) loss_u loss_u 0.8125 (0.8085) acc_u 18.7500 (22.1875) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [35/52] time 0.364 (0.471) data 0.233 (0.340) loss_u loss_u 0.8301 (0.8117) acc_u 18.7500 (21.9643) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [40/52] time 0.542 (0.472) data 0.412 (0.341) loss_u loss_u 0.7925 (0.8081) acc_u 25.0000 (22.5781) lr 2.0984e-04 eta 0:00:05
epoch [160/200] batch [45/52] time 0.405 (0.471) data 0.274 (0.341) loss_u loss_u 0.8193 (0.8067) acc_u 18.7500 (22.7778) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [50/52] time 0.434 (0.471) data 0.303 (0.340) loss_u loss_u 0.8374 (0.8066) acc_u 25.0000 (22.8125) lr 2.0984e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1384
confident_label rate tensor(0.4566, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1432
clean true:1396
clean false:36
clean_rate:0.9748603351955307
noisy true:356
noisy false:1348
after delete: len(clean_dataset) 1432
after delete: len(noisy_dataset) 1704
epoch [161/200] batch [5/44] time 0.549 (0.462) data 0.419 (0.331) loss_x loss_x 1.2412 (1.3080) acc_x 68.7500 (64.3750) lr 2.0032e-04 eta 0:00:18
epoch [161/200] batch [10/44] time 0.418 (0.480) data 0.287 (0.349) loss_x loss_x 1.7510 (1.4298) acc_x 53.1250 (63.7500) lr 2.0032e-04 eta 0:00:16
epoch [161/200] batch [15/44] time 0.483 (0.492) data 0.353 (0.361) loss_x loss_x 1.2100 (1.3054) acc_x 65.6250 (65.6250) lr 2.0032e-04 eta 0:00:14
epoch [161/200] batch [20/44] time 0.394 (0.485) data 0.264 (0.355) loss_x loss_x 1.3555 (1.2869) acc_x 59.3750 (66.2500) lr 2.0032e-04 eta 0:00:11
epoch [161/200] batch [25/44] time 0.469 (0.490) data 0.339 (0.360) loss_x loss_x 1.1816 (1.2279) acc_x 59.3750 (67.5000) lr 2.0032e-04 eta 0:00:09
epoch [161/200] batch [30/44] time 0.396 (0.493) data 0.266 (0.362) loss_x loss_x 0.8833 (1.2299) acc_x 75.0000 (67.0833) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [35/44] time 0.391 (0.487) data 0.260 (0.356) loss_x loss_x 1.4004 (1.2192) acc_x 68.7500 (67.8571) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [40/44] time 0.400 (0.480) data 0.270 (0.350) loss_x loss_x 1.0977 (1.2000) acc_x 68.7500 (68.5938) lr 2.0032e-04 eta 0:00:01
epoch [161/200] batch [5/53] time 0.396 (0.465) data 0.264 (0.335) loss_u loss_u 0.8950 (0.8051) acc_u 12.5000 (24.3750) lr 2.0032e-04 eta 0:00:22
epoch [161/200] batch [10/53] time 0.378 (0.462) data 0.246 (0.331) loss_u loss_u 0.7544 (0.7912) acc_u 34.3750 (26.2500) lr 2.0032e-04 eta 0:00:19
epoch [161/200] batch [15/53] time 0.454 (0.462) data 0.324 (0.331) loss_u loss_u 0.8125 (0.7924) acc_u 25.0000 (26.2500) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [20/53] time 0.401 (0.461) data 0.270 (0.330) loss_u loss_u 0.8818 (0.7944) acc_u 12.5000 (25.9375) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [25/53] time 0.517 (0.464) data 0.385 (0.333) loss_u loss_u 0.8135 (0.7987) acc_u 21.8750 (25.7500) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [30/53] time 0.387 (0.461) data 0.255 (0.330) loss_u loss_u 0.7646 (0.7989) acc_u 31.2500 (25.3125) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [35/53] time 0.573 (0.465) data 0.442 (0.334) loss_u loss_u 0.7842 (0.8063) acc_u 25.0000 (24.2857) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [40/53] time 0.467 (0.466) data 0.336 (0.335) loss_u loss_u 0.7622 (0.8039) acc_u 28.1250 (24.6875) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [45/53] time 0.458 (0.466) data 0.327 (0.335) loss_u loss_u 0.7817 (0.8026) acc_u 31.2500 (25.0000) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [50/53] time 0.511 (0.466) data 0.380 (0.335) loss_u loss_u 0.7964 (0.8045) acc_u 21.8750 (24.5000) lr 2.0032e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1427
confident_label rate tensor(0.4496, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1410
clean true:1359
clean false:51
clean_rate:0.9638297872340426
noisy true:350
noisy false:1376
after delete: len(clean_dataset) 1410
after delete: len(noisy_dataset) 1726
epoch [162/200] batch [5/44] time 0.573 (0.539) data 0.443 (0.408) loss_x loss_x 1.1533 (1.0389) acc_x 65.6250 (75.0000) lr 1.9098e-04 eta 0:00:21
epoch [162/200] batch [10/44] time 0.512 (0.485) data 0.383 (0.355) loss_x loss_x 1.3926 (1.0999) acc_x 65.6250 (72.8125) lr 1.9098e-04 eta 0:00:16
epoch [162/200] batch [15/44] time 0.509 (0.474) data 0.379 (0.344) loss_x loss_x 1.2832 (1.1515) acc_x 59.3750 (70.6250) lr 1.9098e-04 eta 0:00:13
epoch [162/200] batch [20/44] time 0.484 (0.471) data 0.354 (0.340) loss_x loss_x 1.2217 (1.1328) acc_x 81.2500 (71.2500) lr 1.9098e-04 eta 0:00:11
epoch [162/200] batch [25/44] time 0.493 (0.477) data 0.362 (0.346) loss_x loss_x 1.1650 (1.1378) acc_x 65.6250 (71.5000) lr 1.9098e-04 eta 0:00:09
epoch [162/200] batch [30/44] time 0.553 (0.475) data 0.422 (0.345) loss_x loss_x 1.2031 (1.1347) acc_x 75.0000 (71.6667) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [35/44] time 0.429 (0.473) data 0.299 (0.343) loss_x loss_x 0.8071 (1.1176) acc_x 81.2500 (72.2321) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [40/44] time 0.557 (0.477) data 0.427 (0.346) loss_x loss_x 0.9658 (1.1141) acc_x 71.8750 (72.4219) lr 1.9098e-04 eta 0:00:01
epoch [162/200] batch [5/53] time 0.422 (0.481) data 0.290 (0.351) loss_u loss_u 0.8823 (0.8276) acc_u 18.7500 (21.2500) lr 1.9098e-04 eta 0:00:23
epoch [162/200] batch [10/53] time 0.463 (0.479) data 0.332 (0.349) loss_u loss_u 0.7251 (0.7889) acc_u 37.5000 (24.3750) lr 1.9098e-04 eta 0:00:20
epoch [162/200] batch [15/53] time 0.415 (0.475) data 0.283 (0.344) loss_u loss_u 0.8584 (0.7869) acc_u 15.6250 (25.0000) lr 1.9098e-04 eta 0:00:18
epoch [162/200] batch [20/53] time 0.410 (0.470) data 0.279 (0.339) loss_u loss_u 0.7661 (0.7840) acc_u 34.3750 (25.4688) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [25/53] time 0.374 (0.470) data 0.242 (0.339) loss_u loss_u 0.9106 (0.7920) acc_u 9.3750 (24.3750) lr 1.9098e-04 eta 0:00:13
epoch [162/200] batch [30/53] time 0.411 (0.469) data 0.280 (0.339) loss_u loss_u 0.8433 (0.7949) acc_u 15.6250 (24.0625) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [35/53] time 0.378 (0.469) data 0.247 (0.338) loss_u loss_u 0.7324 (0.7965) acc_u 34.3750 (24.2857) lr 1.9098e-04 eta 0:00:08
epoch [162/200] batch [40/53] time 0.455 (0.466) data 0.324 (0.335) loss_u loss_u 0.7139 (0.7946) acc_u 31.2500 (24.6094) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [45/53] time 0.483 (0.466) data 0.352 (0.335) loss_u loss_u 0.7847 (0.7994) acc_u 28.1250 (24.0972) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [50/53] time 0.565 (0.469) data 0.434 (0.338) loss_u loss_u 0.8760 (0.8019) acc_u 12.5000 (24.1250) lr 1.9098e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1379
confident_label rate tensor(0.4601, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1443
clean true:1396
clean false:47
clean_rate:0.9674289674289674
noisy true:361
noisy false:1332
after delete: len(clean_dataset) 1443
after delete: len(noisy_dataset) 1693
epoch [163/200] batch [5/45] time 0.421 (0.467) data 0.290 (0.336) loss_x loss_x 0.9038 (1.0194) acc_x 75.0000 (71.2500) lr 1.8185e-04 eta 0:00:18
epoch [163/200] batch [10/45] time 0.561 (0.474) data 0.431 (0.343) loss_x loss_x 1.2227 (1.0287) acc_x 71.8750 (73.4375) lr 1.8185e-04 eta 0:00:16
epoch [163/200] batch [15/45] time 0.430 (0.477) data 0.299 (0.346) loss_x loss_x 0.8726 (1.0757) acc_x 71.8750 (72.7083) lr 1.8185e-04 eta 0:00:14
epoch [163/200] batch [20/45] time 0.636 (0.492) data 0.506 (0.362) loss_x loss_x 1.0879 (1.0334) acc_x 62.5000 (73.2812) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [25/45] time 0.514 (0.487) data 0.384 (0.357) loss_x loss_x 1.5654 (1.0554) acc_x 65.6250 (73.2500) lr 1.8185e-04 eta 0:00:09
epoch [163/200] batch [30/45] time 0.433 (0.483) data 0.303 (0.352) loss_x loss_x 1.4092 (1.0925) acc_x 59.3750 (71.8750) lr 1.8185e-04 eta 0:00:07
epoch [163/200] batch [35/45] time 0.509 (0.485) data 0.379 (0.354) loss_x loss_x 0.8408 (1.1159) acc_x 87.5000 (71.4286) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [40/45] time 0.498 (0.476) data 0.367 (0.346) loss_x loss_x 1.1465 (1.0874) acc_x 78.1250 (71.9531) lr 1.8185e-04 eta 0:00:02
epoch [163/200] batch [45/45] time 0.391 (0.473) data 0.261 (0.342) loss_x loss_x 0.7134 (1.0906) acc_x 81.2500 (72.0833) lr 1.8185e-04 eta 0:00:00
epoch [163/200] batch [5/52] time 0.416 (0.471) data 0.284 (0.340) loss_u loss_u 0.7607 (0.7763) acc_u 28.1250 (27.5000) lr 1.8185e-04 eta 0:00:22
epoch [163/200] batch [10/52] time 0.439 (0.468) data 0.309 (0.337) loss_u loss_u 0.8701 (0.7984) acc_u 18.7500 (25.3125) lr 1.8185e-04 eta 0:00:19
epoch [163/200] batch [15/52] time 0.546 (0.468) data 0.414 (0.337) loss_u loss_u 0.8286 (0.8004) acc_u 18.7500 (25.2083) lr 1.8185e-04 eta 0:00:17
epoch [163/200] batch [20/52] time 0.410 (0.462) data 0.278 (0.332) loss_u loss_u 0.8081 (0.8087) acc_u 25.0000 (23.2812) lr 1.8185e-04 eta 0:00:14
epoch [163/200] batch [25/52] time 0.364 (0.459) data 0.232 (0.328) loss_u loss_u 0.7671 (0.8113) acc_u 31.2500 (22.7500) lr 1.8185e-04 eta 0:00:12
epoch [163/200] batch [30/52] time 0.548 (0.459) data 0.417 (0.329) loss_u loss_u 0.8174 (0.8081) acc_u 21.8750 (23.4375) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [35/52] time 0.445 (0.458) data 0.313 (0.328) loss_u loss_u 0.8188 (0.8040) acc_u 25.0000 (24.3750) lr 1.8185e-04 eta 0:00:07
epoch [163/200] batch [40/52] time 0.687 (0.461) data 0.556 (0.330) loss_u loss_u 0.8076 (0.8072) acc_u 21.8750 (23.9844) lr 1.8185e-04 eta 0:00:05
epoch [163/200] batch [45/52] time 0.394 (0.461) data 0.264 (0.330) loss_u loss_u 0.8125 (0.8050) acc_u 21.8750 (24.1667) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [50/52] time 0.521 (0.462) data 0.391 (0.331) loss_u loss_u 0.8604 (0.8054) acc_u 15.6250 (24.1250) lr 1.8185e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1387
confident_label rate tensor(0.4566, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1432
clean true:1392
clean false:40
clean_rate:0.9720670391061452
noisy true:357
noisy false:1347
after delete: len(clean_dataset) 1432
after delete: len(noisy_dataset) 1704
epoch [164/200] batch [5/44] time 0.535 (0.539) data 0.405 (0.408) loss_x loss_x 1.3662 (1.2127) acc_x 68.7500 (73.1250) lr 1.7292e-04 eta 0:00:21
epoch [164/200] batch [10/44] time 0.499 (0.525) data 0.369 (0.395) loss_x loss_x 0.6333 (1.1161) acc_x 84.3750 (75.3125) lr 1.7292e-04 eta 0:00:17
epoch [164/200] batch [15/44] time 0.480 (0.522) data 0.350 (0.391) loss_x loss_x 1.2900 (1.1050) acc_x 68.7500 (75.4167) lr 1.7292e-04 eta 0:00:15
epoch [164/200] batch [20/44] time 0.475 (0.520) data 0.344 (0.389) loss_x loss_x 1.2324 (1.0990) acc_x 65.6250 (74.2188) lr 1.7292e-04 eta 0:00:12
epoch [164/200] batch [25/44] time 0.531 (0.514) data 0.401 (0.384) loss_x loss_x 1.2412 (1.0859) acc_x 68.7500 (73.8750) lr 1.7292e-04 eta 0:00:09
epoch [164/200] batch [30/44] time 0.550 (0.506) data 0.420 (0.375) loss_x loss_x 1.4883 (1.1005) acc_x 68.7500 (73.6458) lr 1.7292e-04 eta 0:00:07
epoch [164/200] batch [35/44] time 0.416 (0.494) data 0.285 (0.363) loss_x loss_x 1.2305 (1.0882) acc_x 71.8750 (73.1250) lr 1.7292e-04 eta 0:00:04
epoch [164/200] batch [40/44] time 0.398 (0.490) data 0.268 (0.360) loss_x loss_x 0.8486 (1.0896) acc_x 78.1250 (73.4375) lr 1.7292e-04 eta 0:00:01
epoch [164/200] batch [5/53] time 0.665 (0.495) data 0.535 (0.364) loss_u loss_u 0.8076 (0.8109) acc_u 21.8750 (21.2500) lr 1.7292e-04 eta 0:00:23
epoch [164/200] batch [10/53] time 0.446 (0.491) data 0.315 (0.361) loss_u loss_u 0.8564 (0.7997) acc_u 15.6250 (23.1250) lr 1.7292e-04 eta 0:00:21
epoch [164/200] batch [15/53] time 0.414 (0.490) data 0.283 (0.359) loss_u loss_u 0.8257 (0.7950) acc_u 28.1250 (23.7500) lr 1.7292e-04 eta 0:00:18
epoch [164/200] batch [20/53] time 0.329 (0.487) data 0.197 (0.356) loss_u loss_u 0.8062 (0.8060) acc_u 21.8750 (22.6562) lr 1.7292e-04 eta 0:00:16
epoch [164/200] batch [25/53] time 0.420 (0.483) data 0.290 (0.352) loss_u loss_u 0.8159 (0.8095) acc_u 21.8750 (22.5000) lr 1.7292e-04 eta 0:00:13
epoch [164/200] batch [30/53] time 0.357 (0.479) data 0.226 (0.349) loss_u loss_u 0.7959 (0.8078) acc_u 25.0000 (23.0208) lr 1.7292e-04 eta 0:00:11
epoch [164/200] batch [35/53] time 0.436 (0.477) data 0.305 (0.346) loss_u loss_u 0.8267 (0.8136) acc_u 25.0000 (22.5893) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [40/53] time 0.472 (0.477) data 0.340 (0.347) loss_u loss_u 0.8721 (0.8116) acc_u 15.6250 (23.0469) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [45/53] time 0.381 (0.474) data 0.249 (0.343) loss_u loss_u 0.8657 (0.8092) acc_u 21.8750 (23.8194) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [50/53] time 0.421 (0.476) data 0.290 (0.346) loss_u loss_u 0.7544 (0.8055) acc_u 31.2500 (24.1875) lr 1.7292e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1404
confident_label rate tensor(0.4592, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1440
clean true:1383
clean false:57
clean_rate:0.9604166666666667
noisy true:349
noisy false:1347
after delete: len(clean_dataset) 1440
after delete: len(noisy_dataset) 1696
epoch [165/200] batch [5/45] time 0.479 (0.461) data 0.348 (0.330) loss_x loss_x 0.9272 (1.0348) acc_x 75.0000 (71.8750) lr 1.6419e-04 eta 0:00:18
epoch [165/200] batch [10/45] time 0.519 (0.466) data 0.389 (0.335) loss_x loss_x 0.9688 (1.0905) acc_x 75.0000 (71.5625) lr 1.6419e-04 eta 0:00:16
epoch [165/200] batch [15/45] time 0.636 (0.471) data 0.505 (0.340) loss_x loss_x 0.8560 (1.0335) acc_x 81.2500 (72.9167) lr 1.6419e-04 eta 0:00:14
epoch [165/200] batch [20/45] time 0.505 (0.479) data 0.373 (0.348) loss_x loss_x 1.0410 (1.0824) acc_x 78.1250 (72.9688) lr 1.6419e-04 eta 0:00:11
epoch [165/200] batch [25/45] time 0.435 (0.477) data 0.304 (0.346) loss_x loss_x 0.7646 (1.0979) acc_x 81.2500 (72.7500) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [30/45] time 0.419 (0.475) data 0.288 (0.344) loss_x loss_x 1.1738 (1.1087) acc_x 65.6250 (72.0833) lr 1.6419e-04 eta 0:00:07
epoch [165/200] batch [35/45] time 0.525 (0.478) data 0.394 (0.347) loss_x loss_x 1.0635 (1.1064) acc_x 68.7500 (72.3214) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [40/45] time 0.538 (0.479) data 0.408 (0.347) loss_x loss_x 0.8940 (1.1106) acc_x 71.8750 (71.3281) lr 1.6419e-04 eta 0:00:02
epoch [165/200] batch [45/45] time 0.397 (0.479) data 0.267 (0.348) loss_x loss_x 1.3418 (1.1166) acc_x 59.3750 (71.0417) lr 1.6419e-04 eta 0:00:00
epoch [165/200] batch [5/53] time 0.390 (0.477) data 0.260 (0.346) loss_u loss_u 0.7476 (0.7910) acc_u 34.3750 (25.6250) lr 1.6419e-04 eta 0:00:22
epoch [165/200] batch [10/53] time 0.462 (0.477) data 0.332 (0.346) loss_u loss_u 0.8120 (0.7970) acc_u 25.0000 (25.9375) lr 1.6419e-04 eta 0:00:20
epoch [165/200] batch [15/53] time 0.577 (0.476) data 0.446 (0.345) loss_u loss_u 0.8481 (0.8097) acc_u 15.6250 (23.3333) lr 1.6419e-04 eta 0:00:18
epoch [165/200] batch [20/53] time 0.741 (0.479) data 0.609 (0.348) loss_u loss_u 0.8340 (0.8144) acc_u 25.0000 (23.1250) lr 1.6419e-04 eta 0:00:15
epoch [165/200] batch [25/53] time 0.366 (0.474) data 0.236 (0.343) loss_u loss_u 0.8623 (0.8166) acc_u 18.7500 (23.6250) lr 1.6419e-04 eta 0:00:13
epoch [165/200] batch [30/53] time 0.654 (0.474) data 0.522 (0.342) loss_u loss_u 0.8071 (0.8217) acc_u 25.0000 (23.1250) lr 1.6419e-04 eta 0:00:10
epoch [165/200] batch [35/53] time 0.457 (0.471) data 0.325 (0.340) loss_u loss_u 0.8560 (0.8225) acc_u 15.6250 (22.5000) lr 1.6419e-04 eta 0:00:08
epoch [165/200] batch [40/53] time 0.454 (0.470) data 0.322 (0.339) loss_u loss_u 0.7407 (0.8226) acc_u 34.3750 (22.3438) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [45/53] time 0.470 (0.470) data 0.340 (0.339) loss_u loss_u 0.7573 (0.8170) acc_u 25.0000 (23.1250) lr 1.6419e-04 eta 0:00:03
epoch [165/200] batch [50/53] time 0.445 (0.468) data 0.313 (0.337) loss_u loss_u 0.8638 (0.8145) acc_u 18.7500 (23.6250) lr 1.6419e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1420
confident_label rate tensor(0.4496, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1410
clean true:1369
clean false:41
clean_rate:0.9709219858156029
noisy true:347
noisy false:1379
after delete: len(clean_dataset) 1410
after delete: len(noisy_dataset) 1726
epoch [166/200] batch [5/44] time 0.508 (0.458) data 0.378 (0.328) loss_x loss_x 0.8960 (1.2860) acc_x 78.1250 (69.3750) lr 1.5567e-04 eta 0:00:17
epoch [166/200] batch [10/44] time 0.488 (0.461) data 0.357 (0.331) loss_x loss_x 1.1592 (1.1417) acc_x 71.8750 (73.1250) lr 1.5567e-04 eta 0:00:15
epoch [166/200] batch [15/44] time 0.442 (0.460) data 0.312 (0.329) loss_x loss_x 0.8203 (1.1447) acc_x 81.2500 (72.2917) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [20/44] time 0.368 (0.445) data 0.238 (0.314) loss_x loss_x 1.0938 (1.1370) acc_x 68.7500 (71.5625) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [25/44] time 0.338 (0.442) data 0.208 (0.312) loss_x loss_x 0.5591 (1.1554) acc_x 90.6250 (72.1250) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [30/44] time 0.571 (0.444) data 0.441 (0.314) loss_x loss_x 0.8809 (1.1201) acc_x 75.0000 (72.8125) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [35/44] time 0.437 (0.445) data 0.306 (0.315) loss_x loss_x 0.7954 (1.1191) acc_x 78.1250 (72.5893) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [40/44] time 0.487 (0.449) data 0.356 (0.319) loss_x loss_x 0.8398 (1.1083) acc_x 78.1250 (72.8125) lr 1.5567e-04 eta 0:00:01
epoch [166/200] batch [5/53] time 0.459 (0.449) data 0.327 (0.319) loss_u loss_u 0.7764 (0.8168) acc_u 25.0000 (24.3750) lr 1.5567e-04 eta 0:00:21
epoch [166/200] batch [10/53] time 0.456 (0.450) data 0.324 (0.320) loss_u loss_u 0.6753 (0.8133) acc_u 37.5000 (24.3750) lr 1.5567e-04 eta 0:00:19
epoch [166/200] batch [15/53] time 0.514 (0.457) data 0.381 (0.326) loss_u loss_u 0.8325 (0.8223) acc_u 18.7500 (22.7083) lr 1.5567e-04 eta 0:00:17
epoch [166/200] batch [20/53] time 0.618 (0.467) data 0.486 (0.337) loss_u loss_u 0.7539 (0.8117) acc_u 31.2500 (23.7500) lr 1.5567e-04 eta 0:00:15
epoch [166/200] batch [25/53] time 0.487 (0.465) data 0.355 (0.334) loss_u loss_u 0.7949 (0.8093) acc_u 21.8750 (23.3750) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [30/53] time 0.396 (0.462) data 0.264 (0.331) loss_u loss_u 0.8613 (0.8064) acc_u 15.6250 (23.9583) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [35/53] time 0.355 (0.460) data 0.223 (0.329) loss_u loss_u 0.9043 (0.8055) acc_u 12.5000 (24.1964) lr 1.5567e-04 eta 0:00:08
epoch [166/200] batch [40/53] time 0.431 (0.458) data 0.300 (0.326) loss_u loss_u 0.8135 (0.8073) acc_u 18.7500 (23.9844) lr 1.5567e-04 eta 0:00:05
epoch [166/200] batch [45/53] time 0.390 (0.456) data 0.259 (0.325) loss_u loss_u 0.7627 (0.8068) acc_u 28.1250 (23.6806) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [50/53] time 0.411 (0.454) data 0.280 (0.323) loss_u loss_u 0.7856 (0.8069) acc_u 28.1250 (23.6250) lr 1.5567e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1401
confident_label rate tensor(0.4560, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1430
clean true:1385
clean false:45
clean_rate:0.9685314685314685
noisy true:350
noisy false:1356
after delete: len(clean_dataset) 1430
after delete: len(noisy_dataset) 1706
epoch [167/200] batch [5/44] time 0.444 (0.472) data 0.314 (0.342) loss_x loss_x 0.6875 (1.1240) acc_x 87.5000 (74.3750) lr 1.4736e-04 eta 0:00:18
epoch [167/200] batch [10/44] time 0.542 (0.469) data 0.412 (0.338) loss_x loss_x 1.4785 (1.1475) acc_x 59.3750 (71.5625) lr 1.4736e-04 eta 0:00:15
epoch [167/200] batch [15/44] time 0.437 (0.466) data 0.306 (0.336) loss_x loss_x 0.6553 (1.1174) acc_x 81.2500 (72.5000) lr 1.4736e-04 eta 0:00:13
epoch [167/200] batch [20/44] time 0.402 (0.462) data 0.272 (0.332) loss_x loss_x 0.9751 (1.1646) acc_x 78.1250 (71.7188) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [25/44] time 0.457 (0.458) data 0.327 (0.328) loss_x loss_x 0.9692 (1.1349) acc_x 78.1250 (72.5000) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [30/44] time 0.501 (0.463) data 0.371 (0.333) loss_x loss_x 1.0156 (1.0807) acc_x 71.8750 (73.8542) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [35/44] time 0.408 (0.458) data 0.277 (0.328) loss_x loss_x 0.8989 (1.0849) acc_x 75.0000 (73.7500) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [40/44] time 0.532 (0.452) data 0.402 (0.322) loss_x loss_x 0.9614 (1.0842) acc_x 68.7500 (73.9844) lr 1.4736e-04 eta 0:00:01
epoch [167/200] batch [5/53] time 0.386 (0.454) data 0.256 (0.324) loss_u loss_u 0.8784 (0.8047) acc_u 18.7500 (24.3750) lr 1.4736e-04 eta 0:00:21
epoch [167/200] batch [10/53] time 0.571 (0.460) data 0.439 (0.330) loss_u loss_u 0.8306 (0.7965) acc_u 18.7500 (25.0000) lr 1.4736e-04 eta 0:00:19
epoch [167/200] batch [15/53] time 0.512 (0.461) data 0.380 (0.331) loss_u loss_u 0.8848 (0.8176) acc_u 12.5000 (21.6667) lr 1.4736e-04 eta 0:00:17
epoch [167/200] batch [20/53] time 0.385 (0.458) data 0.255 (0.327) loss_u loss_u 0.7607 (0.8115) acc_u 28.1250 (22.5000) lr 1.4736e-04 eta 0:00:15
epoch [167/200] batch [25/53] time 0.423 (0.457) data 0.292 (0.326) loss_u loss_u 0.6836 (0.8046) acc_u 34.3750 (23.0000) lr 1.4736e-04 eta 0:00:12
epoch [167/200] batch [30/53] time 0.356 (0.457) data 0.225 (0.326) loss_u loss_u 0.7139 (0.7969) acc_u 40.6250 (23.9583) lr 1.4736e-04 eta 0:00:10
epoch [167/200] batch [35/53] time 0.414 (0.453) data 0.284 (0.322) loss_u loss_u 0.8345 (0.7977) acc_u 18.7500 (24.1964) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [40/53] time 0.474 (0.454) data 0.343 (0.324) loss_u loss_u 0.7925 (0.7965) acc_u 18.7500 (24.3750) lr 1.4736e-04 eta 0:00:05
epoch [167/200] batch [45/53] time 0.487 (0.456) data 0.355 (0.326) loss_u loss_u 0.8232 (0.7991) acc_u 18.7500 (24.1667) lr 1.4736e-04 eta 0:00:03
epoch [167/200] batch [50/53] time 0.378 (0.452) data 0.247 (0.322) loss_u loss_u 0.8042 (0.7995) acc_u 25.0000 (24.0625) lr 1.4736e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1437
confident_label rate tensor(0.4467, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1401
clean true:1351
clean false:50
clean_rate:0.9643112062812277
noisy true:348
noisy false:1387
after delete: len(clean_dataset) 1401
after delete: len(noisy_dataset) 1735
epoch [168/200] batch [5/43] time 0.424 (0.477) data 0.294 (0.346) loss_x loss_x 0.9058 (1.0251) acc_x 75.0000 (75.0000) lr 1.3926e-04 eta 0:00:18
epoch [168/200] batch [10/43] time 0.557 (0.471) data 0.427 (0.341) loss_x loss_x 1.1758 (1.1018) acc_x 71.8750 (74.0625) lr 1.3926e-04 eta 0:00:15
epoch [168/200] batch [15/43] time 0.456 (0.462) data 0.325 (0.331) loss_x loss_x 0.8091 (1.0480) acc_x 81.2500 (74.7917) lr 1.3926e-04 eta 0:00:12
epoch [168/200] batch [20/43] time 0.397 (0.456) data 0.267 (0.325) loss_x loss_x 1.0684 (1.0469) acc_x 71.8750 (75.3125) lr 1.3926e-04 eta 0:00:10
epoch [168/200] batch [25/43] time 0.466 (0.464) data 0.335 (0.333) loss_x loss_x 1.2988 (1.0385) acc_x 71.8750 (75.2500) lr 1.3926e-04 eta 0:00:08
epoch [168/200] batch [30/43] time 0.538 (0.458) data 0.407 (0.328) loss_x loss_x 0.9126 (1.0373) acc_x 78.1250 (74.5833) lr 1.3926e-04 eta 0:00:05
epoch [168/200] batch [35/43] time 0.445 (0.456) data 0.314 (0.325) loss_x loss_x 1.0654 (1.0355) acc_x 75.0000 (75.1786) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [40/43] time 0.460 (0.458) data 0.330 (0.327) loss_x loss_x 1.9062 (1.0845) acc_x 46.8750 (73.2812) lr 1.3926e-04 eta 0:00:01
epoch [168/200] batch [5/54] time 0.415 (0.456) data 0.283 (0.325) loss_u loss_u 0.7939 (0.7917) acc_u 31.2500 (27.5000) lr 1.3926e-04 eta 0:00:22
epoch [168/200] batch [10/54] time 0.428 (0.463) data 0.297 (0.333) loss_u loss_u 0.8340 (0.7827) acc_u 15.6250 (27.5000) lr 1.3926e-04 eta 0:00:20
epoch [168/200] batch [15/54] time 0.379 (0.460) data 0.247 (0.330) loss_u loss_u 0.7866 (0.7886) acc_u 34.3750 (27.2917) lr 1.3926e-04 eta 0:00:17
epoch [168/200] batch [20/54] time 0.485 (0.457) data 0.354 (0.327) loss_u loss_u 0.7549 (0.7924) acc_u 34.3750 (27.1875) lr 1.3926e-04 eta 0:00:15
epoch [168/200] batch [25/54] time 0.429 (0.454) data 0.298 (0.324) loss_u loss_u 0.8271 (0.7960) acc_u 25.0000 (26.2500) lr 1.3926e-04 eta 0:00:13
epoch [168/200] batch [30/54] time 0.498 (0.456) data 0.368 (0.326) loss_u loss_u 0.7285 (0.7943) acc_u 28.1250 (26.1458) lr 1.3926e-04 eta 0:00:10
epoch [168/200] batch [35/54] time 0.437 (0.457) data 0.305 (0.326) loss_u loss_u 0.8418 (0.7951) acc_u 21.8750 (26.0714) lr 1.3926e-04 eta 0:00:08
epoch [168/200] batch [40/54] time 0.471 (0.462) data 0.341 (0.332) loss_u loss_u 0.8354 (0.7963) acc_u 15.6250 (25.8594) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [45/54] time 0.487 (0.464) data 0.355 (0.333) loss_u loss_u 0.7891 (0.7979) acc_u 25.0000 (25.6250) lr 1.3926e-04 eta 0:00:04
epoch [168/200] batch [50/54] time 0.445 (0.462) data 0.314 (0.331) loss_u loss_u 0.8696 (0.8007) acc_u 21.8750 (25.5625) lr 1.3926e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1387
confident_label rate tensor(0.4560, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1430
clean true:1392
clean false:38
clean_rate:0.9734265734265735
noisy true:357
noisy false:1349
after delete: len(clean_dataset) 1430
after delete: len(noisy_dataset) 1706
epoch [169/200] batch [5/44] time 0.472 (0.457) data 0.341 (0.326) loss_x loss_x 1.0635 (1.0702) acc_x 75.0000 (71.2500) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [10/44] time 0.460 (0.484) data 0.330 (0.353) loss_x loss_x 0.7080 (1.0781) acc_x 84.3750 (73.4375) lr 1.3137e-04 eta 0:00:16
epoch [169/200] batch [15/44] time 0.391 (0.486) data 0.260 (0.355) loss_x loss_x 1.4736 (1.1177) acc_x 62.5000 (71.8750) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [20/44] time 0.352 (0.470) data 0.222 (0.340) loss_x loss_x 0.5708 (1.0723) acc_x 81.2500 (72.5000) lr 1.3137e-04 eta 0:00:11
epoch [169/200] batch [25/44] time 0.363 (0.466) data 0.233 (0.335) loss_x loss_x 1.0186 (1.0871) acc_x 75.0000 (71.5000) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [30/44] time 0.443 (0.461) data 0.313 (0.330) loss_x loss_x 0.6626 (1.0738) acc_x 90.6250 (72.1875) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [35/44] time 0.562 (0.468) data 0.433 (0.337) loss_x loss_x 1.6025 (1.1183) acc_x 71.8750 (71.4286) lr 1.3137e-04 eta 0:00:04
epoch [169/200] batch [40/44] time 0.445 (0.467) data 0.315 (0.337) loss_x loss_x 0.9473 (1.1004) acc_x 75.0000 (71.6406) lr 1.3137e-04 eta 0:00:01
epoch [169/200] batch [5/53] time 0.430 (0.462) data 0.298 (0.331) loss_u loss_u 0.7944 (0.7992) acc_u 25.0000 (23.1250) lr 1.3137e-04 eta 0:00:22
epoch [169/200] batch [10/53] time 0.558 (0.466) data 0.427 (0.335) loss_u loss_u 0.7725 (0.8013) acc_u 21.8750 (23.1250) lr 1.3137e-04 eta 0:00:20
epoch [169/200] batch [15/53] time 0.401 (0.464) data 0.269 (0.333) loss_u loss_u 0.7622 (0.8045) acc_u 21.8750 (22.0833) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [20/53] time 0.435 (0.466) data 0.305 (0.335) loss_u loss_u 0.8652 (0.8069) acc_u 15.6250 (22.1875) lr 1.3137e-04 eta 0:00:15
epoch [169/200] batch [25/53] time 0.606 (0.464) data 0.476 (0.333) loss_u loss_u 0.8130 (0.8075) acc_u 18.7500 (22.1250) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [30/53] time 0.293 (0.460) data 0.162 (0.329) loss_u loss_u 0.8301 (0.8037) acc_u 21.8750 (22.8125) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [35/53] time 0.364 (0.458) data 0.233 (0.327) loss_u loss_u 0.7134 (0.8007) acc_u 34.3750 (23.1250) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [40/53] time 0.427 (0.459) data 0.295 (0.328) loss_u loss_u 0.7227 (0.7979) acc_u 31.2500 (23.5938) lr 1.3137e-04 eta 0:00:05
epoch [169/200] batch [45/53] time 0.578 (0.458) data 0.447 (0.327) loss_u loss_u 0.7974 (0.7980) acc_u 18.7500 (23.6806) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [50/53] time 0.463 (0.459) data 0.333 (0.328) loss_u loss_u 0.8804 (0.8000) acc_u 15.6250 (23.4375) lr 1.3137e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1395
confident_label rate tensor(0.4550, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1427
clean true:1377
clean false:50
clean_rate:0.9649614576033637
noisy true:364
noisy false:1345
after delete: len(clean_dataset) 1427
after delete: len(noisy_dataset) 1709
epoch [170/200] batch [5/44] time 0.459 (0.503) data 0.329 (0.373) loss_x loss_x 1.0479 (1.1626) acc_x 71.8750 (70.6250) lr 1.2369e-04 eta 0:00:19
epoch [170/200] batch [10/44] time 0.543 (0.472) data 0.414 (0.342) loss_x loss_x 1.3174 (1.2340) acc_x 65.6250 (68.4375) lr 1.2369e-04 eta 0:00:16
epoch [170/200] batch [15/44] time 0.420 (0.487) data 0.290 (0.356) loss_x loss_x 1.3770 (1.1700) acc_x 71.8750 (71.0417) lr 1.2369e-04 eta 0:00:14
epoch [170/200] batch [20/44] time 0.648 (0.497) data 0.517 (0.366) loss_x loss_x 1.3174 (1.2314) acc_x 62.5000 (70.3125) lr 1.2369e-04 eta 0:00:11
epoch [170/200] batch [25/44] time 0.476 (0.494) data 0.346 (0.364) loss_x loss_x 0.7363 (1.1611) acc_x 81.2500 (72.1250) lr 1.2369e-04 eta 0:00:09
epoch [170/200] batch [30/44] time 0.409 (0.485) data 0.279 (0.354) loss_x loss_x 1.1035 (1.1715) acc_x 71.8750 (71.2500) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [35/44] time 0.465 (0.481) data 0.335 (0.351) loss_x loss_x 1.0557 (1.1783) acc_x 71.8750 (70.8036) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [40/44] time 0.815 (0.490) data 0.684 (0.359) loss_x loss_x 0.8623 (1.1482) acc_x 78.1250 (71.5625) lr 1.2369e-04 eta 0:00:01
epoch [170/200] batch [5/53] time 0.604 (0.482) data 0.473 (0.352) loss_u loss_u 0.8560 (0.8300) acc_u 18.7500 (21.2500) lr 1.2369e-04 eta 0:00:23
epoch [170/200] batch [10/53] time 0.518 (0.490) data 0.386 (0.360) loss_u loss_u 0.8701 (0.8172) acc_u 18.7500 (21.8750) lr 1.2369e-04 eta 0:00:21
epoch [170/200] batch [15/53] time 0.423 (0.487) data 0.291 (0.356) loss_u loss_u 0.6895 (0.8100) acc_u 43.7500 (22.7083) lr 1.2369e-04 eta 0:00:18
epoch [170/200] batch [20/53] time 0.495 (0.485) data 0.364 (0.354) loss_u loss_u 0.7139 (0.7994) acc_u 40.6250 (24.3750) lr 1.2369e-04 eta 0:00:15
epoch [170/200] batch [25/53] time 0.440 (0.483) data 0.309 (0.352) loss_u loss_u 0.8179 (0.8058) acc_u 25.0000 (23.6250) lr 1.2369e-04 eta 0:00:13
epoch [170/200] batch [30/53] time 0.538 (0.485) data 0.408 (0.354) loss_u loss_u 0.8252 (0.8110) acc_u 28.1250 (23.8542) lr 1.2369e-04 eta 0:00:11
epoch [170/200] batch [35/53] time 0.745 (0.486) data 0.613 (0.355) loss_u loss_u 0.8306 (0.8147) acc_u 25.0000 (23.2143) lr 1.2369e-04 eta 0:00:08
epoch [170/200] batch [40/53] time 0.421 (0.483) data 0.290 (0.352) loss_u loss_u 0.7549 (0.8099) acc_u 28.1250 (23.6719) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [45/53] time 0.530 (0.481) data 0.399 (0.350) loss_u loss_u 0.8311 (0.8084) acc_u 18.7500 (23.8194) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [50/53] time 0.450 (0.480) data 0.319 (0.349) loss_u loss_u 0.8013 (0.8108) acc_u 21.8750 (23.5625) lr 1.2369e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1409
confident_label rate tensor(0.4525, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1419
clean true:1371
clean false:48
clean_rate:0.9661733615221987
noisy true:356
noisy false:1361
after delete: len(clean_dataset) 1419
after delete: len(noisy_dataset) 1717
epoch [171/200] batch [5/44] time 0.490 (0.465) data 0.360 (0.334) loss_x loss_x 1.0791 (0.9835) acc_x 75.0000 (74.3750) lr 1.1623e-04 eta 0:00:18
epoch [171/200] batch [10/44] time 0.454 (0.466) data 0.324 (0.336) loss_x loss_x 0.9097 (1.0956) acc_x 71.8750 (73.7500) lr 1.1623e-04 eta 0:00:15
epoch [171/200] batch [15/44] time 0.527 (0.476) data 0.396 (0.345) loss_x loss_x 1.0957 (1.1393) acc_x 71.8750 (73.1250) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [20/44] time 0.404 (0.465) data 0.274 (0.335) loss_x loss_x 0.9048 (1.0905) acc_x 75.0000 (74.2188) lr 1.1623e-04 eta 0:00:11
epoch [171/200] batch [25/44] time 0.500 (0.475) data 0.370 (0.344) loss_x loss_x 1.2852 (1.0756) acc_x 68.7500 (73.6250) lr 1.1623e-04 eta 0:00:09
epoch [171/200] batch [30/44] time 0.378 (0.473) data 0.247 (0.343) loss_x loss_x 0.9790 (1.0675) acc_x 78.1250 (73.8542) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [35/44] time 0.629 (0.489) data 0.498 (0.358) loss_x loss_x 1.6680 (1.0580) acc_x 59.3750 (74.0179) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [40/44] time 0.429 (0.489) data 0.299 (0.359) loss_x loss_x 1.0078 (1.0614) acc_x 71.8750 (73.2812) lr 1.1623e-04 eta 0:00:01
epoch [171/200] batch [5/53] time 0.445 (0.492) data 0.313 (0.361) loss_u loss_u 0.7378 (0.7870) acc_u 34.3750 (26.2500) lr 1.1623e-04 eta 0:00:23
epoch [171/200] batch [10/53] time 0.442 (0.491) data 0.311 (0.360) loss_u loss_u 0.7563 (0.7783) acc_u 28.1250 (26.8750) lr 1.1623e-04 eta 0:00:21
epoch [171/200] batch [15/53] time 0.396 (0.488) data 0.264 (0.358) loss_u loss_u 0.8027 (0.7901) acc_u 21.8750 (25.2083) lr 1.1623e-04 eta 0:00:18
epoch [171/200] batch [20/53] time 0.484 (0.490) data 0.353 (0.359) loss_u loss_u 0.8018 (0.7864) acc_u 25.0000 (26.2500) lr 1.1623e-04 eta 0:00:16
epoch [171/200] batch [25/53] time 0.358 (0.484) data 0.227 (0.354) loss_u loss_u 0.8291 (0.7879) acc_u 18.7500 (25.6250) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [30/53] time 0.508 (0.482) data 0.377 (0.351) loss_u loss_u 0.7632 (0.7891) acc_u 25.0000 (25.2083) lr 1.1623e-04 eta 0:00:11
epoch [171/200] batch [35/53] time 0.438 (0.485) data 0.307 (0.354) loss_u loss_u 0.8335 (0.7885) acc_u 18.7500 (25.5357) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [40/53] time 0.453 (0.481) data 0.322 (0.351) loss_u loss_u 0.8408 (0.7934) acc_u 15.6250 (24.7656) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [45/53] time 0.448 (0.480) data 0.316 (0.349) loss_u loss_u 0.8154 (0.7948) acc_u 18.7500 (24.7222) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [50/53] time 0.387 (0.479) data 0.256 (0.348) loss_u loss_u 0.7432 (0.7946) acc_u 31.2500 (24.9375) lr 1.1623e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1375
confident_label rate tensor(0.4601, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1443
clean true:1403
clean false:40
clean_rate:0.9722799722799723
noisy true:358
noisy false:1335
after delete: len(clean_dataset) 1443
after delete: len(noisy_dataset) 1693
epoch [172/200] batch [5/45] time 0.522 (0.503) data 0.392 (0.373) loss_x loss_x 0.8862 (0.9948) acc_x 78.1250 (76.2500) lr 1.0899e-04 eta 0:00:20
epoch [172/200] batch [10/45] time 0.459 (0.471) data 0.328 (0.341) loss_x loss_x 1.0059 (0.9531) acc_x 81.2500 (76.5625) lr 1.0899e-04 eta 0:00:16
epoch [172/200] batch [15/45] time 0.541 (0.462) data 0.411 (0.332) loss_x loss_x 0.7583 (0.9988) acc_x 81.2500 (76.4583) lr 1.0899e-04 eta 0:00:13
epoch [172/200] batch [20/45] time 0.486 (0.454) data 0.355 (0.323) loss_x loss_x 1.3818 (1.0239) acc_x 62.5000 (75.4688) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [25/45] time 0.362 (0.450) data 0.232 (0.319) loss_x loss_x 0.8394 (1.0229) acc_x 75.0000 (75.5000) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [30/45] time 0.495 (0.453) data 0.364 (0.322) loss_x loss_x 0.9468 (1.0904) acc_x 71.8750 (74.0625) lr 1.0899e-04 eta 0:00:06
epoch [172/200] batch [35/45] time 0.369 (0.450) data 0.238 (0.319) loss_x loss_x 0.9922 (1.0834) acc_x 81.2500 (73.9286) lr 1.0899e-04 eta 0:00:04
epoch [172/200] batch [40/45] time 0.458 (0.454) data 0.328 (0.324) loss_x loss_x 1.0732 (1.0797) acc_x 75.0000 (73.8281) lr 1.0899e-04 eta 0:00:02
epoch [172/200] batch [45/45] time 0.359 (0.457) data 0.229 (0.326) loss_x loss_x 1.0850 (1.0715) acc_x 75.0000 (73.5417) lr 1.0899e-04 eta 0:00:00
epoch [172/200] batch [5/52] time 0.560 (0.456) data 0.428 (0.325) loss_u loss_u 0.7373 (0.8211) acc_u 28.1250 (20.0000) lr 1.0899e-04 eta 0:00:21
epoch [172/200] batch [10/52] time 0.450 (0.452) data 0.318 (0.321) loss_u loss_u 0.8545 (0.8007) acc_u 15.6250 (22.8125) lr 1.0899e-04 eta 0:00:18
epoch [172/200] batch [15/52] time 0.626 (0.452) data 0.495 (0.321) loss_u loss_u 0.7676 (0.8112) acc_u 28.1250 (23.1250) lr 1.0899e-04 eta 0:00:16
epoch [172/200] batch [20/52] time 0.412 (0.448) data 0.282 (0.317) loss_u loss_u 0.7603 (0.8112) acc_u 28.1250 (22.6562) lr 1.0899e-04 eta 0:00:14
epoch [172/200] batch [25/52] time 0.439 (0.444) data 0.307 (0.313) loss_u loss_u 0.8789 (0.8111) acc_u 12.5000 (22.8750) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [30/52] time 0.440 (0.447) data 0.310 (0.316) loss_u loss_u 0.8066 (0.8097) acc_u 18.7500 (23.6458) lr 1.0899e-04 eta 0:00:09
epoch [172/200] batch [35/52] time 0.484 (0.445) data 0.353 (0.314) loss_u loss_u 0.7974 (0.8127) acc_u 21.8750 (23.0357) lr 1.0899e-04 eta 0:00:07
epoch [172/200] batch [40/52] time 0.562 (0.445) data 0.431 (0.314) loss_u loss_u 0.7681 (0.8140) acc_u 28.1250 (23.0469) lr 1.0899e-04 eta 0:00:05
epoch [172/200] batch [45/52] time 0.439 (0.447) data 0.309 (0.316) loss_u loss_u 0.7529 (0.8139) acc_u 21.8750 (22.8472) lr 1.0899e-04 eta 0:00:03
epoch [172/200] batch [50/52] time 0.681 (0.450) data 0.551 (0.319) loss_u loss_u 0.7363 (0.8098) acc_u 31.2500 (23.3750) lr 1.0899e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1402
confident_label rate tensor(0.4582, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1437
clean true:1395
clean false:42
clean_rate:0.9707724425887265
noisy true:339
noisy false:1360
after delete: len(clean_dataset) 1437
after delete: len(noisy_dataset) 1699
epoch [173/200] batch [5/44] time 0.440 (0.470) data 0.310 (0.339) loss_x loss_x 1.3125 (1.0542) acc_x 71.8750 (76.8750) lr 1.0197e-04 eta 0:00:18
epoch [173/200] batch [10/44] time 0.389 (0.472) data 0.259 (0.341) loss_x loss_x 1.3262 (1.1256) acc_x 65.6250 (72.8125) lr 1.0197e-04 eta 0:00:16
epoch [173/200] batch [15/44] time 0.449 (0.454) data 0.319 (0.323) loss_x loss_x 2.3281 (1.1588) acc_x 53.1250 (73.1250) lr 1.0197e-04 eta 0:00:13
epoch [173/200] batch [20/44] time 0.488 (0.459) data 0.357 (0.328) loss_x loss_x 1.0469 (1.1408) acc_x 71.8750 (72.5000) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [25/44] time 0.439 (0.455) data 0.309 (0.325) loss_x loss_x 1.0479 (1.1391) acc_x 78.1250 (72.5000) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [30/44] time 0.396 (0.462) data 0.265 (0.332) loss_x loss_x 1.0996 (1.1303) acc_x 65.6250 (72.6042) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [35/44] time 0.397 (0.462) data 0.266 (0.332) loss_x loss_x 0.9868 (1.1078) acc_x 75.0000 (72.5893) lr 1.0197e-04 eta 0:00:04
epoch [173/200] batch [40/44] time 0.442 (0.461) data 0.311 (0.330) loss_x loss_x 1.2031 (1.1253) acc_x 59.3750 (71.4844) lr 1.0197e-04 eta 0:00:01
epoch [173/200] batch [5/53] time 0.588 (0.455) data 0.457 (0.324) loss_u loss_u 0.7949 (0.8007) acc_u 18.7500 (23.7500) lr 1.0197e-04 eta 0:00:21
epoch [173/200] batch [10/53] time 0.411 (0.452) data 0.280 (0.322) loss_u loss_u 0.6953 (0.8101) acc_u 37.5000 (22.1875) lr 1.0197e-04 eta 0:00:19
epoch [173/200] batch [15/53] time 0.484 (0.456) data 0.352 (0.326) loss_u loss_u 0.8257 (0.8173) acc_u 21.8750 (22.0833) lr 1.0197e-04 eta 0:00:17
epoch [173/200] batch [20/53] time 0.465 (0.459) data 0.334 (0.328) loss_u loss_u 0.7900 (0.8166) acc_u 25.0000 (22.0312) lr 1.0197e-04 eta 0:00:15
epoch [173/200] batch [25/53] time 0.472 (0.460) data 0.341 (0.329) loss_u loss_u 0.7617 (0.8183) acc_u 34.3750 (21.3750) lr 1.0197e-04 eta 0:00:12
epoch [173/200] batch [30/53] time 0.434 (0.461) data 0.302 (0.330) loss_u loss_u 0.7861 (0.8162) acc_u 25.0000 (21.5625) lr 1.0197e-04 eta 0:00:10
epoch [173/200] batch [35/53] time 0.366 (0.460) data 0.234 (0.329) loss_u loss_u 0.9492 (0.8131) acc_u 6.2500 (22.0536) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [40/53] time 0.360 (0.458) data 0.229 (0.327) loss_u loss_u 0.7026 (0.8105) acc_u 37.5000 (22.5000) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [45/53] time 0.585 (0.460) data 0.454 (0.329) loss_u loss_u 0.7676 (0.8079) acc_u 34.3750 (22.9861) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [50/53] time 0.463 (0.458) data 0.331 (0.327) loss_u loss_u 0.8799 (0.8123) acc_u 12.5000 (22.5625) lr 1.0197e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1382
confident_label rate tensor(0.4633, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1453
clean true:1406
clean false:47
clean_rate:0.9676531314521679
noisy true:348
noisy false:1335
after delete: len(clean_dataset) 1453
after delete: len(noisy_dataset) 1683
epoch [174/200] batch [5/45] time 0.410 (0.470) data 0.280 (0.339) loss_x loss_x 1.2051 (1.0242) acc_x 65.6250 (71.8750) lr 9.5173e-05 eta 0:00:18
epoch [174/200] batch [10/45] time 0.604 (0.440) data 0.473 (0.310) loss_x loss_x 1.5332 (1.1262) acc_x 68.7500 (69.0625) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [15/45] time 0.473 (0.473) data 0.342 (0.342) loss_x loss_x 1.3955 (1.0867) acc_x 59.3750 (70.6250) lr 9.5173e-05 eta 0:00:14
epoch [174/200] batch [20/45] time 0.365 (0.465) data 0.235 (0.334) loss_x loss_x 1.6328 (1.1521) acc_x 59.3750 (69.2188) lr 9.5173e-05 eta 0:00:11
epoch [174/200] batch [25/45] time 0.378 (0.483) data 0.248 (0.353) loss_x loss_x 0.9121 (1.1155) acc_x 71.8750 (70.0000) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [30/45] time 0.411 (0.479) data 0.280 (0.348) loss_x loss_x 1.1660 (1.1123) acc_x 71.8750 (70.7292) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [35/45] time 0.616 (0.478) data 0.485 (0.347) loss_x loss_x 1.0166 (1.1125) acc_x 87.5000 (71.2500) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [40/45] time 0.502 (0.471) data 0.371 (0.340) loss_x loss_x 1.4160 (1.1042) acc_x 68.7500 (71.5625) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [45/45] time 0.652 (0.472) data 0.522 (0.342) loss_x loss_x 1.1777 (1.0758) acc_x 71.8750 (72.4306) lr 9.5173e-05 eta 0:00:00
epoch [174/200] batch [5/52] time 0.416 (0.469) data 0.284 (0.338) loss_u loss_u 0.6875 (0.7951) acc_u 50.0000 (26.8750) lr 9.5173e-05 eta 0:00:22
epoch [174/200] batch [10/52] time 0.395 (0.472) data 0.263 (0.341) loss_u loss_u 0.7573 (0.7978) acc_u 31.2500 (24.6875) lr 9.5173e-05 eta 0:00:19
epoch [174/200] batch [15/52] time 0.423 (0.473) data 0.293 (0.343) loss_u loss_u 0.8730 (0.8041) acc_u 21.8750 (24.5833) lr 9.5173e-05 eta 0:00:17
epoch [174/200] batch [20/52] time 0.454 (0.473) data 0.323 (0.342) loss_u loss_u 0.8423 (0.8091) acc_u 21.8750 (23.7500) lr 9.5173e-05 eta 0:00:15
epoch [174/200] batch [25/52] time 0.351 (0.471) data 0.221 (0.341) loss_u loss_u 0.8188 (0.8048) acc_u 28.1250 (25.1250) lr 9.5173e-05 eta 0:00:12
epoch [174/200] batch [30/52] time 0.472 (0.468) data 0.341 (0.337) loss_u loss_u 0.7666 (0.8031) acc_u 31.2500 (25.2083) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [35/52] time 0.406 (0.464) data 0.276 (0.333) loss_u loss_u 0.8486 (0.8073) acc_u 21.8750 (24.5536) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [40/52] time 0.585 (0.467) data 0.454 (0.336) loss_u loss_u 0.8677 (0.8065) acc_u 21.8750 (24.8438) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [45/52] time 0.471 (0.466) data 0.340 (0.336) loss_u loss_u 0.7451 (0.8033) acc_u 34.3750 (25.2083) lr 9.5173e-05 eta 0:00:03
epoch [174/200] batch [50/52] time 0.437 (0.470) data 0.306 (0.339) loss_u loss_u 0.8721 (0.8052) acc_u 12.5000 (24.4375) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1396
confident_label rate tensor(0.4525, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1419
clean true:1382
clean false:37
clean_rate:0.9739252995066948
noisy true:358
noisy false:1359
after delete: len(clean_dataset) 1419
after delete: len(noisy_dataset) 1717
epoch [175/200] batch [5/44] time 0.419 (0.473) data 0.288 (0.341) loss_x loss_x 0.7104 (1.1271) acc_x 75.0000 (66.2500) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [10/44] time 0.475 (0.457) data 0.344 (0.326) loss_x loss_x 0.9019 (1.1736) acc_x 81.2500 (66.8750) lr 8.8597e-05 eta 0:00:15
epoch [175/200] batch [15/44] time 0.427 (0.485) data 0.297 (0.354) loss_x loss_x 1.4316 (1.1064) acc_x 65.6250 (70.2083) lr 8.8597e-05 eta 0:00:14
epoch [175/200] batch [20/44] time 0.692 (0.495) data 0.561 (0.365) loss_x loss_x 1.2969 (1.0952) acc_x 65.6250 (70.9375) lr 8.8597e-05 eta 0:00:11
epoch [175/200] batch [25/44] time 0.428 (0.498) data 0.297 (0.368) loss_x loss_x 1.2246 (1.1005) acc_x 71.8750 (71.5000) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [30/44] time 0.505 (0.495) data 0.375 (0.365) loss_x loss_x 1.1562 (1.0642) acc_x 75.0000 (72.6042) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [35/44] time 0.530 (0.496) data 0.399 (0.365) loss_x loss_x 1.1484 (1.0894) acc_x 68.7500 (72.4107) lr 8.8597e-05 eta 0:00:04
epoch [175/200] batch [40/44] time 0.390 (0.491) data 0.259 (0.361) loss_x loss_x 1.3496 (1.0795) acc_x 65.6250 (72.4219) lr 8.8597e-05 eta 0:00:01
epoch [175/200] batch [5/53] time 0.602 (0.495) data 0.471 (0.364) loss_u loss_u 0.6562 (0.7849) acc_u 43.7500 (26.2500) lr 8.8597e-05 eta 0:00:23
epoch [175/200] batch [10/53] time 0.489 (0.497) data 0.358 (0.367) loss_u loss_u 0.8237 (0.7848) acc_u 21.8750 (26.2500) lr 8.8597e-05 eta 0:00:21
epoch [175/200] batch [15/53] time 0.738 (0.498) data 0.608 (0.367) loss_u loss_u 0.8423 (0.7880) acc_u 15.6250 (25.6250) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [20/53] time 0.457 (0.492) data 0.325 (0.361) loss_u loss_u 0.8657 (0.8017) acc_u 18.7500 (24.0625) lr 8.8597e-05 eta 0:00:16
epoch [175/200] batch [25/53] time 0.453 (0.486) data 0.322 (0.355) loss_u loss_u 0.7930 (0.8031) acc_u 25.0000 (24.5000) lr 8.8597e-05 eta 0:00:13
epoch [175/200] batch [30/53] time 0.618 (0.491) data 0.488 (0.360) loss_u loss_u 0.7617 (0.8051) acc_u 28.1250 (24.3750) lr 8.8597e-05 eta 0:00:11
epoch [175/200] batch [35/53] time 0.374 (0.485) data 0.242 (0.354) loss_u loss_u 0.7188 (0.7995) acc_u 37.5000 (25.1786) lr 8.8597e-05 eta 0:00:08
epoch [175/200] batch [40/53] time 0.370 (0.481) data 0.239 (0.350) loss_u loss_u 0.9023 (0.8055) acc_u 15.6250 (24.5312) lr 8.8597e-05 eta 0:00:06
epoch [175/200] batch [45/53] time 0.401 (0.480) data 0.268 (0.349) loss_u loss_u 0.7983 (0.8031) acc_u 37.5000 (24.9306) lr 8.8597e-05 eta 0:00:03
epoch [175/200] batch [50/53] time 0.527 (0.478) data 0.396 (0.347) loss_u loss_u 0.7773 (0.8007) acc_u 28.1250 (25.2500) lr 8.8597e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1392
confident_label rate tensor(0.4611, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1446
clean true:1394
clean false:52
clean_rate:0.9640387275242047
noisy true:350
noisy false:1340
after delete: len(clean_dataset) 1446
after delete: len(noisy_dataset) 1690
epoch [176/200] batch [5/45] time 0.565 (0.466) data 0.434 (0.336) loss_x loss_x 0.8701 (0.8835) acc_x 75.0000 (76.2500) lr 8.2245e-05 eta 0:00:18
epoch [176/200] batch [10/45] time 0.443 (0.452) data 0.312 (0.321) loss_x loss_x 1.3057 (1.0438) acc_x 65.6250 (70.9375) lr 8.2245e-05 eta 0:00:15
epoch [176/200] batch [15/45] time 0.549 (0.453) data 0.419 (0.323) loss_x loss_x 0.8667 (1.0594) acc_x 71.8750 (71.0417) lr 8.2245e-05 eta 0:00:13
epoch [176/200] batch [20/45] time 0.465 (0.465) data 0.335 (0.334) loss_x loss_x 1.3359 (1.0950) acc_x 65.6250 (70.6250) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [25/45] time 0.545 (0.466) data 0.414 (0.335) loss_x loss_x 1.5244 (1.0721) acc_x 68.7500 (71.8750) lr 8.2245e-05 eta 0:00:09
epoch [176/200] batch [30/45] time 0.507 (0.469) data 0.377 (0.339) loss_x loss_x 0.7461 (1.0877) acc_x 81.2500 (71.0417) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [35/45] time 0.351 (0.459) data 0.221 (0.329) loss_x loss_x 1.3125 (1.1289) acc_x 75.0000 (70.7143) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [40/45] time 0.460 (0.456) data 0.330 (0.325) loss_x loss_x 0.6079 (1.1293) acc_x 81.2500 (70.2344) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [45/45] time 0.403 (0.452) data 0.273 (0.322) loss_x loss_x 0.8418 (1.1266) acc_x 68.7500 (70.4861) lr 8.2245e-05 eta 0:00:00
epoch [176/200] batch [5/52] time 0.467 (0.451) data 0.337 (0.320) loss_u loss_u 0.8076 (0.8008) acc_u 25.0000 (25.0000) lr 8.2245e-05 eta 0:00:21
epoch [176/200] batch [10/52] time 0.374 (0.447) data 0.242 (0.316) loss_u loss_u 0.8081 (0.8189) acc_u 31.2500 (23.4375) lr 8.2245e-05 eta 0:00:18
epoch [176/200] batch [15/52] time 0.635 (0.450) data 0.503 (0.320) loss_u loss_u 0.8003 (0.8143) acc_u 28.1250 (23.3333) lr 8.2245e-05 eta 0:00:16
epoch [176/200] batch [20/52] time 0.395 (0.453) data 0.264 (0.323) loss_u loss_u 0.8188 (0.8002) acc_u 31.2500 (25.3125) lr 8.2245e-05 eta 0:00:14
epoch [176/200] batch [25/52] time 0.537 (0.460) data 0.406 (0.330) loss_u loss_u 0.7168 (0.7935) acc_u 37.5000 (25.7500) lr 8.2245e-05 eta 0:00:12
epoch [176/200] batch [30/52] time 0.400 (0.459) data 0.269 (0.328) loss_u loss_u 0.8413 (0.7969) acc_u 15.6250 (24.8958) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [35/52] time 0.343 (0.457) data 0.212 (0.326) loss_u loss_u 0.8301 (0.8003) acc_u 25.0000 (24.8214) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [40/52] time 0.397 (0.457) data 0.266 (0.327) loss_u loss_u 0.7773 (0.7989) acc_u 31.2500 (25.0781) lr 8.2245e-05 eta 0:00:05
epoch [176/200] batch [45/52] time 0.508 (0.457) data 0.377 (0.326) loss_u loss_u 0.8130 (0.8042) acc_u 18.7500 (24.3056) lr 8.2245e-05 eta 0:00:03
epoch [176/200] batch [50/52] time 0.381 (0.455) data 0.250 (0.324) loss_u loss_u 0.7861 (0.8062) acc_u 25.0000 (24.2500) lr 8.2245e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1367
confident_label rate tensor(0.4617, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1448
clean true:1402
clean false:46
clean_rate:0.9682320441988951
noisy true:367
noisy false:1321
after delete: len(clean_dataset) 1448
after delete: len(noisy_dataset) 1688
epoch [177/200] batch [5/45] time 0.590 (0.500) data 0.458 (0.369) loss_x loss_x 1.0459 (1.0656) acc_x 65.6250 (71.2500) lr 7.6120e-05 eta 0:00:19
epoch [177/200] batch [10/45] time 0.420 (0.476) data 0.289 (0.345) loss_x loss_x 1.9463 (1.2603) acc_x 62.5000 (69.3750) lr 7.6120e-05 eta 0:00:16
epoch [177/200] batch [15/45] time 0.468 (0.465) data 0.338 (0.334) loss_x loss_x 1.1387 (1.1982) acc_x 75.0000 (70.2083) lr 7.6120e-05 eta 0:00:13
epoch [177/200] batch [20/45] time 0.372 (0.460) data 0.241 (0.329) loss_x loss_x 1.8838 (1.1809) acc_x 53.1250 (71.2500) lr 7.6120e-05 eta 0:00:11
epoch [177/200] batch [25/45] time 0.466 (0.457) data 0.335 (0.326) loss_x loss_x 1.2217 (1.1632) acc_x 59.3750 (71.1250) lr 7.6120e-05 eta 0:00:09
epoch [177/200] batch [30/45] time 0.460 (0.452) data 0.328 (0.321) loss_x loss_x 1.2861 (1.1613) acc_x 71.8750 (71.6667) lr 7.6120e-05 eta 0:00:06
epoch [177/200] batch [35/45] time 0.498 (0.455) data 0.367 (0.324) loss_x loss_x 1.1133 (1.1515) acc_x 75.0000 (72.0536) lr 7.6120e-05 eta 0:00:04
epoch [177/200] batch [40/45] time 0.517 (0.458) data 0.386 (0.327) loss_x loss_x 1.3477 (1.1462) acc_x 59.3750 (72.1875) lr 7.6120e-05 eta 0:00:02
epoch [177/200] batch [45/45] time 0.437 (0.456) data 0.307 (0.325) loss_x loss_x 0.4741 (1.1229) acc_x 81.2500 (72.5000) lr 7.6120e-05 eta 0:00:00
epoch [177/200] batch [5/52] time 0.479 (0.460) data 0.347 (0.329) loss_u loss_u 0.7588 (0.8036) acc_u 25.0000 (23.7500) lr 7.6120e-05 eta 0:00:21
epoch [177/200] batch [10/52] time 0.428 (0.455) data 0.297 (0.324) loss_u loss_u 0.6646 (0.7762) acc_u 37.5000 (27.1875) lr 7.6120e-05 eta 0:00:19
epoch [177/200] batch [15/52] time 0.523 (0.458) data 0.390 (0.327) loss_u loss_u 0.8657 (0.7971) acc_u 12.5000 (24.3750) lr 7.6120e-05 eta 0:00:16
epoch [177/200] batch [20/52] time 0.363 (0.456) data 0.232 (0.325) loss_u loss_u 0.8760 (0.8051) acc_u 12.5000 (23.1250) lr 7.6120e-05 eta 0:00:14
epoch [177/200] batch [25/52] time 0.433 (0.455) data 0.301 (0.324) loss_u loss_u 0.7407 (0.8025) acc_u 37.5000 (24.1250) lr 7.6120e-05 eta 0:00:12
epoch [177/200] batch [30/52] time 0.415 (0.453) data 0.282 (0.322) loss_u loss_u 0.8101 (0.8071) acc_u 28.1250 (23.9583) lr 7.6120e-05 eta 0:00:09
epoch [177/200] batch [35/52] time 0.568 (0.452) data 0.436 (0.321) loss_u loss_u 0.8447 (0.8048) acc_u 12.5000 (23.8393) lr 7.6120e-05 eta 0:00:07
epoch [177/200] batch [40/52] time 0.373 (0.449) data 0.243 (0.317) loss_u loss_u 0.7476 (0.8031) acc_u 34.3750 (23.9844) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [45/52] time 0.469 (0.449) data 0.337 (0.318) loss_u loss_u 0.7646 (0.8057) acc_u 28.1250 (23.6111) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [50/52] time 0.669 (0.452) data 0.538 (0.321) loss_u loss_u 0.8232 (0.8083) acc_u 18.7500 (23.2500) lr 7.6120e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1370
confident_label rate tensor(0.4659, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1461
clean true:1415
clean false:46
clean_rate:0.9685147159479809
noisy true:351
noisy false:1324
after delete: len(clean_dataset) 1461
after delete: len(noisy_dataset) 1675
epoch [178/200] batch [5/45] time 0.469 (0.485) data 0.339 (0.355) loss_x loss_x 0.8101 (1.1044) acc_x 78.1250 (72.5000) lr 7.0224e-05 eta 0:00:19
epoch [178/200] batch [10/45] time 0.495 (0.480) data 0.364 (0.350) loss_x loss_x 1.1943 (1.0965) acc_x 68.7500 (71.5625) lr 7.0224e-05 eta 0:00:16
epoch [178/200] batch [15/45] time 0.512 (0.481) data 0.382 (0.351) loss_x loss_x 1.0791 (1.0605) acc_x 71.8750 (73.5417) lr 7.0224e-05 eta 0:00:14
epoch [178/200] batch [20/45] time 0.515 (0.478) data 0.384 (0.348) loss_x loss_x 1.0938 (1.0634) acc_x 59.3750 (72.3438) lr 7.0224e-05 eta 0:00:11
epoch [178/200] batch [25/45] time 0.451 (0.499) data 0.320 (0.369) loss_x loss_x 1.1572 (1.0931) acc_x 71.8750 (71.7500) lr 7.0224e-05 eta 0:00:09
epoch [178/200] batch [30/45] time 0.552 (0.494) data 0.421 (0.363) loss_x loss_x 1.5146 (1.1218) acc_x 65.6250 (71.3542) lr 7.0224e-05 eta 0:00:07
epoch [178/200] batch [35/45] time 0.483 (0.492) data 0.352 (0.362) loss_x loss_x 1.5645 (1.1573) acc_x 71.8750 (71.6071) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [40/45] time 0.387 (0.484) data 0.257 (0.354) loss_x loss_x 0.6836 (1.1536) acc_x 81.2500 (71.4062) lr 7.0224e-05 eta 0:00:02
epoch [178/200] batch [45/45] time 0.395 (0.477) data 0.265 (0.346) loss_x loss_x 1.1191 (1.1585) acc_x 71.8750 (71.5278) lr 7.0224e-05 eta 0:00:00
epoch [178/200] batch [5/52] time 0.488 (0.471) data 0.357 (0.341) loss_u loss_u 0.8213 (0.8159) acc_u 25.0000 (21.2500) lr 7.0224e-05 eta 0:00:22
epoch [178/200] batch [10/52] time 0.468 (0.473) data 0.336 (0.342) loss_u loss_u 0.8291 (0.7973) acc_u 15.6250 (22.5000) lr 7.0224e-05 eta 0:00:19
epoch [178/200] batch [15/52] time 0.609 (0.472) data 0.477 (0.342) loss_u loss_u 0.6694 (0.7921) acc_u 40.6250 (24.1667) lr 7.0224e-05 eta 0:00:17
epoch [178/200] batch [20/52] time 0.442 (0.471) data 0.311 (0.340) loss_u loss_u 0.7905 (0.7949) acc_u 28.1250 (24.2188) lr 7.0224e-05 eta 0:00:15
epoch [178/200] batch [25/52] time 0.494 (0.473) data 0.363 (0.342) loss_u loss_u 0.8604 (0.8029) acc_u 15.6250 (23.0000) lr 7.0224e-05 eta 0:00:12
epoch [178/200] batch [30/52] time 0.530 (0.473) data 0.399 (0.343) loss_u loss_u 0.8896 (0.8089) acc_u 12.5000 (21.9792) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [35/52] time 0.349 (0.468) data 0.219 (0.337) loss_u loss_u 0.8296 (0.8085) acc_u 18.7500 (22.2321) lr 7.0224e-05 eta 0:00:07
epoch [178/200] batch [40/52] time 0.414 (0.464) data 0.282 (0.333) loss_u loss_u 0.8916 (0.8073) acc_u 9.3750 (22.5000) lr 7.0224e-05 eta 0:00:05
epoch [178/200] batch [45/52] time 0.450 (0.467) data 0.319 (0.336) loss_u loss_u 0.8193 (0.8076) acc_u 21.8750 (22.6389) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [50/52] time 0.516 (0.467) data 0.383 (0.336) loss_u loss_u 0.8579 (0.8086) acc_u 25.0000 (23.0000) lr 7.0224e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1373
confident_label rate tensor(0.4589, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1439
clean true:1401
clean false:38
clean_rate:0.9735927727588604
noisy true:362
noisy false:1335
after delete: len(clean_dataset) 1439
after delete: len(noisy_dataset) 1697
epoch [179/200] batch [5/44] time 0.482 (0.437) data 0.351 (0.306) loss_x loss_x 0.6450 (1.1058) acc_x 84.3750 (71.2500) lr 6.4556e-05 eta 0:00:17
epoch [179/200] batch [10/44] time 0.485 (0.434) data 0.355 (0.304) loss_x loss_x 1.4336 (1.1529) acc_x 71.8750 (72.1875) lr 6.4556e-05 eta 0:00:14
epoch [179/200] batch [15/44] time 0.487 (0.441) data 0.357 (0.311) loss_x loss_x 0.8984 (1.1021) acc_x 75.0000 (73.7500) lr 6.4556e-05 eta 0:00:12
epoch [179/200] batch [20/44] time 0.538 (0.458) data 0.407 (0.328) loss_x loss_x 0.7271 (1.0743) acc_x 84.3750 (74.0625) lr 6.4556e-05 eta 0:00:11
epoch [179/200] batch [25/44] time 0.501 (0.460) data 0.371 (0.330) loss_x loss_x 0.9272 (1.0951) acc_x 68.7500 (73.6250) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [30/44] time 0.561 (0.462) data 0.430 (0.332) loss_x loss_x 1.0391 (1.0896) acc_x 71.8750 (73.6458) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [35/44] time 0.436 (0.464) data 0.305 (0.333) loss_x loss_x 1.1758 (1.0936) acc_x 59.3750 (72.9464) lr 6.4556e-05 eta 0:00:04
epoch [179/200] batch [40/44] time 0.552 (0.467) data 0.422 (0.337) loss_x loss_x 0.8901 (1.0667) acc_x 68.7500 (73.2031) lr 6.4556e-05 eta 0:00:01
epoch [179/200] batch [5/53] time 0.411 (0.476) data 0.280 (0.345) loss_u loss_u 0.8594 (0.8269) acc_u 18.7500 (22.5000) lr 6.4556e-05 eta 0:00:22
epoch [179/200] batch [10/53] time 0.391 (0.473) data 0.260 (0.343) loss_u loss_u 0.7407 (0.8100) acc_u 31.2500 (23.1250) lr 6.4556e-05 eta 0:00:20
epoch [179/200] batch [15/53] time 0.583 (0.478) data 0.450 (0.348) loss_u loss_u 0.7617 (0.8088) acc_u 34.3750 (23.7500) lr 6.4556e-05 eta 0:00:18
epoch [179/200] batch [20/53] time 0.494 (0.478) data 0.362 (0.347) loss_u loss_u 0.7925 (0.8152) acc_u 28.1250 (22.9688) lr 6.4556e-05 eta 0:00:15
epoch [179/200] batch [25/53] time 0.412 (0.477) data 0.280 (0.347) loss_u loss_u 0.8066 (0.8148) acc_u 25.0000 (23.1250) lr 6.4556e-05 eta 0:00:13
epoch [179/200] batch [30/53] time 0.438 (0.476) data 0.306 (0.345) loss_u loss_u 0.7671 (0.8163) acc_u 28.1250 (22.6042) lr 6.4556e-05 eta 0:00:10
epoch [179/200] batch [35/53] time 0.556 (0.474) data 0.425 (0.343) loss_u loss_u 0.8501 (0.8169) acc_u 18.7500 (22.5893) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [40/53] time 0.545 (0.473) data 0.414 (0.342) loss_u loss_u 0.8755 (0.8148) acc_u 15.6250 (22.8125) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [45/53] time 0.408 (0.476) data 0.277 (0.345) loss_u loss_u 0.7471 (0.8112) acc_u 34.3750 (23.4028) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [50/53] time 0.386 (0.475) data 0.255 (0.344) loss_u loss_u 0.9302 (0.8121) acc_u 6.2500 (23.2500) lr 6.4556e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1367
confident_label rate tensor(0.4662, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1462
clean true:1417
clean false:45
clean_rate:0.96922024623803
noisy true:352
noisy false:1322
after delete: len(clean_dataset) 1462
after delete: len(noisy_dataset) 1674
epoch [180/200] batch [5/45] time 0.372 (0.463) data 0.241 (0.332) loss_x loss_x 0.9624 (0.9870) acc_x 78.1250 (71.8750) lr 5.9119e-05 eta 0:00:18
epoch [180/200] batch [10/45] time 0.386 (0.467) data 0.256 (0.337) loss_x loss_x 0.8306 (1.0337) acc_x 75.0000 (72.5000) lr 5.9119e-05 eta 0:00:16
epoch [180/200] batch [15/45] time 0.512 (0.469) data 0.381 (0.338) loss_x loss_x 1.0410 (1.0126) acc_x 62.5000 (72.2917) lr 5.9119e-05 eta 0:00:14
epoch [180/200] batch [20/45] time 0.461 (0.472) data 0.331 (0.342) loss_x loss_x 1.4141 (1.0730) acc_x 75.0000 (72.3438) lr 5.9119e-05 eta 0:00:11
epoch [180/200] batch [25/45] time 0.461 (0.476) data 0.331 (0.345) loss_x loss_x 0.7354 (1.0633) acc_x 84.3750 (73.1250) lr 5.9119e-05 eta 0:00:09
epoch [180/200] batch [30/45] time 0.469 (0.476) data 0.339 (0.346) loss_x loss_x 1.1729 (1.0719) acc_x 71.8750 (72.8125) lr 5.9119e-05 eta 0:00:07
epoch [180/200] batch [35/45] time 0.513 (0.473) data 0.383 (0.342) loss_x loss_x 0.7256 (1.0863) acc_x 81.2500 (72.9464) lr 5.9119e-05 eta 0:00:04
epoch [180/200] batch [40/45] time 0.506 (0.475) data 0.376 (0.345) loss_x loss_x 1.3145 (1.0641) acc_x 71.8750 (73.8281) lr 5.9119e-05 eta 0:00:02
epoch [180/200] batch [45/45] time 0.354 (0.464) data 0.225 (0.333) loss_x loss_x 1.4766 (1.0801) acc_x 65.6250 (73.5417) lr 5.9119e-05 eta 0:00:00
epoch [180/200] batch [5/52] time 0.318 (0.461) data 0.187 (0.330) loss_u loss_u 0.8477 (0.8132) acc_u 15.6250 (24.3750) lr 5.9119e-05 eta 0:00:21
epoch [180/200] batch [10/52] time 0.336 (0.459) data 0.204 (0.329) loss_u loss_u 0.8779 (0.8139) acc_u 15.6250 (23.4375) lr 5.9119e-05 eta 0:00:19
epoch [180/200] batch [15/52] time 0.502 (0.458) data 0.370 (0.327) loss_u loss_u 0.7217 (0.7927) acc_u 37.5000 (25.8333) lr 5.9119e-05 eta 0:00:16
epoch [180/200] batch [20/52] time 0.462 (0.455) data 0.330 (0.325) loss_u loss_u 0.8257 (0.8031) acc_u 18.7500 (23.7500) lr 5.9119e-05 eta 0:00:14
epoch [180/200] batch [25/52] time 0.432 (0.455) data 0.301 (0.324) loss_u loss_u 0.7876 (0.8021) acc_u 31.2500 (24.1250) lr 5.9119e-05 eta 0:00:12
epoch [180/200] batch [30/52] time 0.695 (0.457) data 0.563 (0.327) loss_u loss_u 0.8247 (0.8056) acc_u 21.8750 (23.7500) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [35/52] time 0.369 (0.455) data 0.237 (0.324) loss_u loss_u 0.8501 (0.8049) acc_u 12.5000 (23.8393) lr 5.9119e-05 eta 0:00:07
epoch [180/200] batch [40/52] time 0.467 (0.453) data 0.335 (0.322) loss_u loss_u 0.9058 (0.8103) acc_u 3.1250 (22.8906) lr 5.9119e-05 eta 0:00:05
epoch [180/200] batch [45/52] time 0.394 (0.451) data 0.262 (0.321) loss_u loss_u 0.7520 (0.8088) acc_u 34.3750 (23.0556) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [50/52] time 0.390 (0.454) data 0.260 (0.323) loss_u loss_u 0.8833 (0.8134) acc_u 15.6250 (22.5625) lr 5.9119e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1410
confident_label rate tensor(0.4518, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1417
clean true:1369
clean false:48
clean_rate:0.9661256175017643
noisy true:357
noisy false:1362
after delete: len(clean_dataset) 1417
after delete: len(noisy_dataset) 1719
epoch [181/200] batch [5/44] time 0.436 (0.431) data 0.306 (0.301) loss_x loss_x 0.9233 (1.0133) acc_x 87.5000 (79.3750) lr 5.3915e-05 eta 0:00:16
epoch [181/200] batch [10/44] time 0.451 (0.436) data 0.321 (0.305) loss_x loss_x 1.6396 (1.1511) acc_x 59.3750 (73.4375) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [15/44] time 0.436 (0.430) data 0.306 (0.300) loss_x loss_x 1.2783 (1.0961) acc_x 75.0000 (73.3333) lr 5.3915e-05 eta 0:00:12
epoch [181/200] batch [20/44] time 0.445 (0.433) data 0.314 (0.302) loss_x loss_x 0.9028 (1.0664) acc_x 75.0000 (74.5312) lr 5.3915e-05 eta 0:00:10
epoch [181/200] batch [25/44] time 0.426 (0.437) data 0.296 (0.307) loss_x loss_x 1.5176 (1.1003) acc_x 65.6250 (73.0000) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [30/44] time 0.407 (0.437) data 0.277 (0.306) loss_x loss_x 1.4873 (1.1296) acc_x 75.0000 (72.7083) lr 5.3915e-05 eta 0:00:06
epoch [181/200] batch [35/44] time 0.540 (0.442) data 0.409 (0.311) loss_x loss_x 0.9204 (1.1141) acc_x 78.1250 (72.9464) lr 5.3915e-05 eta 0:00:03
epoch [181/200] batch [40/44] time 0.393 (0.443) data 0.263 (0.312) loss_x loss_x 1.1240 (1.1120) acc_x 75.0000 (72.8125) lr 5.3915e-05 eta 0:00:01
epoch [181/200] batch [5/53] time 0.415 (0.447) data 0.284 (0.317) loss_u loss_u 0.8081 (0.8183) acc_u 21.8750 (21.8750) lr 5.3915e-05 eta 0:00:21
epoch [181/200] batch [10/53] time 0.426 (0.444) data 0.294 (0.313) loss_u loss_u 0.7729 (0.8047) acc_u 28.1250 (24.6875) lr 5.3915e-05 eta 0:00:19
epoch [181/200] batch [15/53] time 0.601 (0.445) data 0.469 (0.314) loss_u loss_u 0.8237 (0.8106) acc_u 28.1250 (23.9583) lr 5.3915e-05 eta 0:00:16
epoch [181/200] batch [20/53] time 0.428 (0.447) data 0.297 (0.316) loss_u loss_u 0.8281 (0.8079) acc_u 18.7500 (23.5938) lr 5.3915e-05 eta 0:00:14
epoch [181/200] batch [25/53] time 0.563 (0.453) data 0.432 (0.322) loss_u loss_u 0.8135 (0.8053) acc_u 21.8750 (23.7500) lr 5.3915e-05 eta 0:00:12
epoch [181/200] batch [30/53] time 0.423 (0.451) data 0.291 (0.321) loss_u loss_u 0.7822 (0.8007) acc_u 25.0000 (24.1667) lr 5.3915e-05 eta 0:00:10
epoch [181/200] batch [35/53] time 0.457 (0.453) data 0.327 (0.322) loss_u loss_u 0.7051 (0.7942) acc_u 37.5000 (25.2679) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [40/53] time 0.464 (0.455) data 0.332 (0.324) loss_u loss_u 0.8296 (0.7983) acc_u 21.8750 (24.6094) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [45/53] time 0.468 (0.455) data 0.337 (0.324) loss_u loss_u 0.8267 (0.7949) acc_u 15.6250 (25.0694) lr 5.3915e-05 eta 0:00:03
epoch [181/200] batch [50/53] time 0.441 (0.456) data 0.309 (0.325) loss_u loss_u 0.8179 (0.7973) acc_u 21.8750 (24.6250) lr 5.3915e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1390
confident_label rate tensor(0.4601, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1443
clean true:1400
clean false:43
clean_rate:0.9702009702009702
noisy true:346
noisy false:1347
after delete: len(clean_dataset) 1443
after delete: len(noisy_dataset) 1693
epoch [182/200] batch [5/45] time 0.373 (0.471) data 0.242 (0.340) loss_x loss_x 1.6826 (1.1928) acc_x 62.5000 (71.8750) lr 4.8943e-05 eta 0:00:18
epoch [182/200] batch [10/45] time 0.542 (0.478) data 0.411 (0.348) loss_x loss_x 1.4775 (1.1752) acc_x 65.6250 (72.1875) lr 4.8943e-05 eta 0:00:16
epoch [182/200] batch [15/45] time 0.360 (0.467) data 0.229 (0.337) loss_x loss_x 0.7808 (1.1165) acc_x 81.2500 (73.5417) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [20/45] time 0.410 (0.459) data 0.279 (0.328) loss_x loss_x 1.6924 (1.1467) acc_x 62.5000 (72.8125) lr 4.8943e-05 eta 0:00:11
epoch [182/200] batch [25/45] time 0.398 (0.450) data 0.268 (0.320) loss_x loss_x 0.8154 (1.1384) acc_x 68.7500 (72.5000) lr 4.8943e-05 eta 0:00:09
epoch [182/200] batch [30/45] time 0.461 (0.453) data 0.331 (0.323) loss_x loss_x 1.0430 (1.1443) acc_x 71.8750 (72.5000) lr 4.8943e-05 eta 0:00:06
epoch [182/200] batch [35/45] time 0.376 (0.452) data 0.245 (0.321) loss_x loss_x 0.7959 (1.1296) acc_x 81.2500 (72.8571) lr 4.8943e-05 eta 0:00:04
epoch [182/200] batch [40/45] time 0.675 (0.455) data 0.544 (0.324) loss_x loss_x 1.3213 (1.1248) acc_x 59.3750 (72.9688) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [45/45] time 0.619 (0.461) data 0.488 (0.331) loss_x loss_x 0.9893 (1.1262) acc_x 71.8750 (72.7083) lr 4.8943e-05 eta 0:00:00
epoch [182/200] batch [5/52] time 0.461 (0.461) data 0.329 (0.331) loss_u loss_u 0.8247 (0.8254) acc_u 31.2500 (26.2500) lr 4.8943e-05 eta 0:00:21
epoch [182/200] batch [10/52] time 0.473 (0.465) data 0.342 (0.335) loss_u loss_u 0.7939 (0.8117) acc_u 21.8750 (23.1250) lr 4.8943e-05 eta 0:00:19
epoch [182/200] batch [15/52] time 0.406 (0.465) data 0.274 (0.334) loss_u loss_u 0.7529 (0.8111) acc_u 28.1250 (22.9167) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [20/52] time 0.416 (0.463) data 0.285 (0.332) loss_u loss_u 0.7642 (0.8148) acc_u 31.2500 (22.6562) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [25/52] time 0.444 (0.458) data 0.313 (0.327) loss_u loss_u 0.7588 (0.8161) acc_u 28.1250 (22.8750) lr 4.8943e-05 eta 0:00:12
epoch [182/200] batch [30/52] time 0.595 (0.466) data 0.465 (0.335) loss_u loss_u 0.7876 (0.8128) acc_u 28.1250 (23.3333) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [35/52] time 0.436 (0.462) data 0.305 (0.331) loss_u loss_u 0.7861 (0.8074) acc_u 28.1250 (24.0179) lr 4.8943e-05 eta 0:00:07
epoch [182/200] batch [40/52] time 0.567 (0.463) data 0.436 (0.332) loss_u loss_u 0.7612 (0.8061) acc_u 28.1250 (23.9062) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [45/52] time 0.427 (0.462) data 0.296 (0.331) loss_u loss_u 0.8564 (0.8083) acc_u 18.7500 (23.6806) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [50/52] time 0.608 (0.462) data 0.478 (0.331) loss_u loss_u 0.7568 (0.8089) acc_u 34.3750 (23.8750) lr 4.8943e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1367
confident_label rate tensor(0.4643, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1456
clean true:1417
clean false:39
clean_rate:0.9732142857142857
noisy true:352
noisy false:1328
after delete: len(clean_dataset) 1456
after delete: len(noisy_dataset) 1680
epoch [183/200] batch [5/45] time 0.434 (0.444) data 0.304 (0.313) loss_x loss_x 1.1377 (1.0336) acc_x 68.7500 (76.2500) lr 4.4207e-05 eta 0:00:17
epoch [183/200] batch [10/45] time 0.369 (0.429) data 0.239 (0.298) loss_x loss_x 0.8311 (1.0783) acc_x 81.2500 (74.6875) lr 4.4207e-05 eta 0:00:14
epoch [183/200] batch [15/45] time 0.415 (0.442) data 0.284 (0.311) loss_x loss_x 1.3311 (1.0960) acc_x 68.7500 (73.5417) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [20/45] time 0.386 (0.443) data 0.256 (0.313) loss_x loss_x 0.8765 (1.1051) acc_x 78.1250 (72.6562) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [25/45] time 0.504 (0.453) data 0.374 (0.323) loss_x loss_x 1.2568 (1.1033) acc_x 78.1250 (73.3750) lr 4.4207e-05 eta 0:00:09
epoch [183/200] batch [30/45] time 0.387 (0.447) data 0.257 (0.316) loss_x loss_x 0.9658 (1.0994) acc_x 59.3750 (72.7083) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [35/45] time 0.553 (0.457) data 0.423 (0.326) loss_x loss_x 0.3174 (1.0796) acc_x 93.7500 (72.9464) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [40/45] time 0.433 (0.456) data 0.303 (0.326) loss_x loss_x 0.9858 (1.0867) acc_x 81.2500 (72.7344) lr 4.4207e-05 eta 0:00:02
epoch [183/200] batch [45/45] time 0.553 (0.463) data 0.423 (0.333) loss_x loss_x 1.2109 (1.0831) acc_x 75.0000 (72.9167) lr 4.4207e-05 eta 0:00:00
epoch [183/200] batch [5/52] time 0.412 (0.461) data 0.280 (0.330) loss_u loss_u 0.7339 (0.8018) acc_u 34.3750 (24.3750) lr 4.4207e-05 eta 0:00:21
epoch [183/200] batch [10/52] time 0.527 (0.462) data 0.396 (0.331) loss_u loss_u 0.8052 (0.8006) acc_u 21.8750 (23.1250) lr 4.4207e-05 eta 0:00:19
epoch [183/200] batch [15/52] time 0.436 (0.459) data 0.305 (0.328) loss_u loss_u 0.7837 (0.8100) acc_u 28.1250 (22.2917) lr 4.4207e-05 eta 0:00:16
epoch [183/200] batch [20/52] time 0.444 (0.457) data 0.313 (0.326) loss_u loss_u 0.7964 (0.8059) acc_u 28.1250 (22.8125) lr 4.4207e-05 eta 0:00:14
epoch [183/200] batch [25/52] time 0.396 (0.455) data 0.265 (0.324) loss_u loss_u 0.7754 (0.8027) acc_u 31.2500 (23.3750) lr 4.4207e-05 eta 0:00:12
epoch [183/200] batch [30/52] time 0.327 (0.455) data 0.196 (0.325) loss_u loss_u 0.8345 (0.8010) acc_u 31.2500 (23.9583) lr 4.4207e-05 eta 0:00:10
epoch [183/200] batch [35/52] time 0.506 (0.455) data 0.374 (0.324) loss_u loss_u 0.7959 (0.8001) acc_u 21.8750 (24.3750) lr 4.4207e-05 eta 0:00:07
epoch [183/200] batch [40/52] time 0.481 (0.457) data 0.349 (0.327) loss_u loss_u 0.7974 (0.8035) acc_u 28.1250 (24.1406) lr 4.4207e-05 eta 0:00:05
epoch [183/200] batch [45/52] time 0.428 (0.455) data 0.298 (0.325) loss_u loss_u 0.8101 (0.8032) acc_u 25.0000 (24.0972) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [50/52] time 0.485 (0.459) data 0.354 (0.328) loss_u loss_u 0.8242 (0.8040) acc_u 25.0000 (24.0625) lr 4.4207e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1365
confident_label rate tensor(0.4652, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1459
clean true:1416
clean false:43
clean_rate:0.9705277587388622
noisy true:355
noisy false:1322
after delete: len(clean_dataset) 1459
after delete: len(noisy_dataset) 1677
epoch [184/200] batch [5/45] time 0.382 (0.467) data 0.251 (0.337) loss_x loss_x 0.8022 (1.2107) acc_x 81.2500 (70.6250) lr 3.9706e-05 eta 0:00:18
epoch [184/200] batch [10/45] time 0.519 (0.474) data 0.388 (0.344) loss_x loss_x 0.6162 (1.0730) acc_x 81.2500 (73.4375) lr 3.9706e-05 eta 0:00:16
epoch [184/200] batch [15/45] time 0.635 (0.490) data 0.505 (0.359) loss_x loss_x 1.1768 (1.0307) acc_x 71.8750 (72.7083) lr 3.9706e-05 eta 0:00:14
epoch [184/200] batch [20/45] time 0.429 (0.504) data 0.299 (0.373) loss_x loss_x 1.1523 (1.0539) acc_x 71.8750 (72.8125) lr 3.9706e-05 eta 0:00:12
epoch [184/200] batch [25/45] time 0.414 (0.499) data 0.283 (0.368) loss_x loss_x 1.1963 (1.0706) acc_x 68.7500 (73.2500) lr 3.9706e-05 eta 0:00:09
epoch [184/200] batch [30/45] time 0.356 (0.477) data 0.225 (0.346) loss_x loss_x 0.7471 (1.0900) acc_x 78.1250 (72.2917) lr 3.9706e-05 eta 0:00:07
epoch [184/200] batch [35/45] time 0.453 (0.479) data 0.323 (0.348) loss_x loss_x 0.7754 (1.1170) acc_x 87.5000 (72.2321) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [40/45] time 0.493 (0.483) data 0.363 (0.352) loss_x loss_x 1.2578 (1.1101) acc_x 62.5000 (71.7188) lr 3.9706e-05 eta 0:00:02
epoch [184/200] batch [45/45] time 0.462 (0.479) data 0.331 (0.349) loss_x loss_x 0.9238 (1.1064) acc_x 75.0000 (71.8750) lr 3.9706e-05 eta 0:00:00
epoch [184/200] batch [5/52] time 0.367 (0.478) data 0.235 (0.348) loss_u loss_u 0.7700 (0.8248) acc_u 28.1250 (20.6250) lr 3.9706e-05 eta 0:00:22
epoch [184/200] batch [10/52] time 0.386 (0.477) data 0.255 (0.346) loss_u loss_u 0.8652 (0.8451) acc_u 15.6250 (18.1250) lr 3.9706e-05 eta 0:00:20
epoch [184/200] batch [15/52] time 0.475 (0.477) data 0.343 (0.346) loss_u loss_u 0.6777 (0.8214) acc_u 37.5000 (21.8750) lr 3.9706e-05 eta 0:00:17
epoch [184/200] batch [20/52] time 0.395 (0.470) data 0.264 (0.339) loss_u loss_u 0.8286 (0.8159) acc_u 15.6250 (22.1875) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [25/52] time 0.482 (0.469) data 0.351 (0.338) loss_u loss_u 0.8311 (0.8189) acc_u 21.8750 (22.1250) lr 3.9706e-05 eta 0:00:12
epoch [184/200] batch [30/52] time 0.500 (0.468) data 0.368 (0.337) loss_u loss_u 0.8154 (0.8147) acc_u 18.7500 (22.9167) lr 3.9706e-05 eta 0:00:10
epoch [184/200] batch [35/52] time 0.419 (0.468) data 0.289 (0.337) loss_u loss_u 0.7852 (0.8118) acc_u 25.0000 (23.0357) lr 3.9706e-05 eta 0:00:07
epoch [184/200] batch [40/52] time 0.352 (0.468) data 0.222 (0.337) loss_u loss_u 0.8218 (0.8141) acc_u 31.2500 (22.9688) lr 3.9706e-05 eta 0:00:05
epoch [184/200] batch [45/52] time 0.441 (0.467) data 0.308 (0.336) loss_u loss_u 0.7686 (0.8133) acc_u 31.2500 (23.1250) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [50/52] time 0.449 (0.474) data 0.317 (0.343) loss_u loss_u 0.8564 (0.8165) acc_u 15.6250 (22.7500) lr 3.9706e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1346
confident_label rate tensor(0.4732, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1484
clean true:1439
clean false:45
clean_rate:0.9696765498652291
noisy true:351
noisy false:1301
after delete: len(clean_dataset) 1484
after delete: len(noisy_dataset) 1652
epoch [185/200] batch [5/46] time 0.458 (0.482) data 0.327 (0.351) loss_x loss_x 0.9019 (0.8577) acc_x 71.8750 (75.0000) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [10/46] time 0.466 (0.510) data 0.335 (0.379) loss_x loss_x 1.0645 (0.9520) acc_x 71.8750 (75.0000) lr 3.5443e-05 eta 0:00:18
epoch [185/200] batch [15/46] time 0.347 (0.478) data 0.216 (0.347) loss_x loss_x 1.2402 (1.0466) acc_x 71.8750 (73.1250) lr 3.5443e-05 eta 0:00:14
epoch [185/200] batch [20/46] time 0.450 (0.474) data 0.320 (0.344) loss_x loss_x 1.2158 (1.0845) acc_x 68.7500 (71.5625) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [25/46] time 0.398 (0.476) data 0.268 (0.346) loss_x loss_x 1.5781 (1.0955) acc_x 65.6250 (71.7500) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [30/46] time 0.453 (0.483) data 0.322 (0.352) loss_x loss_x 0.8857 (1.0911) acc_x 71.8750 (71.3542) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [35/46] time 0.398 (0.477) data 0.268 (0.346) loss_x loss_x 0.8188 (1.0819) acc_x 71.8750 (71.6964) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [40/46] time 0.669 (0.480) data 0.536 (0.349) loss_x loss_x 0.6221 (1.0735) acc_x 84.3750 (71.9531) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [45/46] time 0.397 (0.484) data 0.266 (0.353) loss_x loss_x 1.2637 (1.0772) acc_x 71.8750 (72.2222) lr 3.5443e-05 eta 0:00:00
epoch [185/200] batch [5/51] time 0.390 (0.474) data 0.257 (0.343) loss_u loss_u 0.8286 (0.8569) acc_u 18.7500 (16.2500) lr 3.5443e-05 eta 0:00:21
epoch [185/200] batch [10/51] time 0.347 (0.469) data 0.215 (0.338) loss_u loss_u 0.8394 (0.8497) acc_u 25.0000 (17.5000) lr 3.5443e-05 eta 0:00:19
epoch [185/200] batch [15/51] time 0.366 (0.467) data 0.236 (0.336) loss_u loss_u 0.7148 (0.8362) acc_u 37.5000 (20.0000) lr 3.5443e-05 eta 0:00:16
epoch [185/200] batch [20/51] time 0.400 (0.468) data 0.269 (0.337) loss_u loss_u 0.8613 (0.8355) acc_u 15.6250 (20.4688) lr 3.5443e-05 eta 0:00:14
epoch [185/200] batch [25/51] time 0.358 (0.467) data 0.228 (0.336) loss_u loss_u 0.8125 (0.8362) acc_u 21.8750 (20.3750) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [30/51] time 0.434 (0.467) data 0.304 (0.336) loss_u loss_u 0.7891 (0.8358) acc_u 31.2500 (20.3125) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [35/51] time 0.359 (0.475) data 0.228 (0.344) loss_u loss_u 0.8613 (0.8310) acc_u 15.6250 (20.4464) lr 3.5443e-05 eta 0:00:07
epoch [185/200] batch [40/51] time 0.402 (0.472) data 0.269 (0.341) loss_u loss_u 0.8438 (0.8286) acc_u 21.8750 (20.7812) lr 3.5443e-05 eta 0:00:05
epoch [185/200] batch [45/51] time 0.524 (0.471) data 0.394 (0.340) loss_u loss_u 0.7637 (0.8248) acc_u 28.1250 (21.4583) lr 3.5443e-05 eta 0:00:02
epoch [185/200] batch [50/51] time 0.459 (0.470) data 0.327 (0.338) loss_u loss_u 0.7432 (0.8213) acc_u 34.3750 (22.3750) lr 3.5443e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1392
confident_label rate tensor(0.4541, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1424
clean true:1382
clean false:42
clean_rate:0.9705056179775281
noisy true:362
noisy false:1350
after delete: len(clean_dataset) 1424
after delete: len(noisy_dataset) 1712
epoch [186/200] batch [5/44] time 0.468 (0.451) data 0.337 (0.320) loss_x loss_x 1.0322 (0.9271) acc_x 75.0000 (75.6250) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [10/44] time 0.460 (0.491) data 0.330 (0.359) loss_x loss_x 1.8936 (1.0177) acc_x 53.1250 (73.4375) lr 3.1417e-05 eta 0:00:16
epoch [186/200] batch [15/44] time 0.629 (0.511) data 0.498 (0.380) loss_x loss_x 1.0771 (1.0506) acc_x 71.8750 (72.5000) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [20/44] time 0.356 (0.502) data 0.226 (0.371) loss_x loss_x 0.7959 (1.0371) acc_x 75.0000 (72.1875) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [25/44] time 0.477 (0.484) data 0.347 (0.353) loss_x loss_x 1.7305 (1.0983) acc_x 59.3750 (71.5000) lr 3.1417e-05 eta 0:00:09
epoch [186/200] batch [30/44] time 0.506 (0.482) data 0.374 (0.351) loss_x loss_x 1.0811 (1.0926) acc_x 75.0000 (71.6667) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [35/44] time 0.448 (0.481) data 0.317 (0.350) loss_x loss_x 1.1113 (1.0731) acc_x 75.0000 (72.0536) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [40/44] time 0.429 (0.478) data 0.298 (0.347) loss_x loss_x 0.9478 (1.0604) acc_x 81.2500 (72.5781) lr 3.1417e-05 eta 0:00:01
epoch [186/200] batch [5/53] time 0.478 (0.480) data 0.347 (0.349) loss_u loss_u 0.7891 (0.8256) acc_u 25.0000 (20.0000) lr 3.1417e-05 eta 0:00:23
epoch [186/200] batch [10/53] time 0.457 (0.481) data 0.326 (0.350) loss_u loss_u 0.8579 (0.8177) acc_u 18.7500 (21.8750) lr 3.1417e-05 eta 0:00:20
epoch [186/200] batch [15/53] time 0.430 (0.477) data 0.299 (0.346) loss_u loss_u 0.8379 (0.8085) acc_u 12.5000 (22.2917) lr 3.1417e-05 eta 0:00:18
epoch [186/200] batch [20/53] time 0.426 (0.476) data 0.294 (0.345) loss_u loss_u 0.7930 (0.8090) acc_u 25.0000 (22.5000) lr 3.1417e-05 eta 0:00:15
epoch [186/200] batch [25/53] time 0.528 (0.480) data 0.397 (0.349) loss_u loss_u 0.7715 (0.8125) acc_u 28.1250 (22.5000) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [30/53] time 0.392 (0.477) data 0.261 (0.346) loss_u loss_u 0.7900 (0.8096) acc_u 28.1250 (22.8125) lr 3.1417e-05 eta 0:00:10
epoch [186/200] batch [35/53] time 0.519 (0.477) data 0.389 (0.346) loss_u loss_u 0.8506 (0.8140) acc_u 18.7500 (22.4107) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [40/53] time 0.373 (0.472) data 0.241 (0.341) loss_u loss_u 0.8447 (0.8145) acc_u 18.7500 (22.5000) lr 3.1417e-05 eta 0:00:06
epoch [186/200] batch [45/53] time 0.495 (0.473) data 0.364 (0.342) loss_u loss_u 0.7363 (0.8110) acc_u 28.1250 (23.1250) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [50/53] time 0.382 (0.470) data 0.251 (0.339) loss_u loss_u 0.8765 (0.8091) acc_u 12.5000 (23.3125) lr 3.1417e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1355
confident_label rate tensor(0.4694, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1472
clean true:1424
clean false:48
clean_rate:0.967391304347826
noisy true:357
noisy false:1307
after delete: len(clean_dataset) 1472
after delete: len(noisy_dataset) 1664
epoch [187/200] batch [5/46] time 0.470 (0.421) data 0.340 (0.291) loss_x loss_x 1.3105 (1.1424) acc_x 71.8750 (70.0000) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [10/46] time 0.579 (0.473) data 0.447 (0.342) loss_x loss_x 0.6660 (1.1175) acc_x 84.3750 (72.8125) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [15/46] time 0.516 (0.477) data 0.385 (0.346) loss_x loss_x 0.9404 (1.1056) acc_x 78.1250 (73.3333) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [20/46] time 0.486 (0.473) data 0.355 (0.342) loss_x loss_x 0.6914 (1.0752) acc_x 84.3750 (73.2812) lr 2.7630e-05 eta 0:00:12
epoch [187/200] batch [25/46] time 0.440 (0.467) data 0.309 (0.336) loss_x loss_x 1.2305 (1.1289) acc_x 68.7500 (72.3750) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [30/46] time 0.440 (0.465) data 0.309 (0.334) loss_x loss_x 1.3711 (1.1340) acc_x 75.0000 (72.1875) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [35/46] time 0.446 (0.466) data 0.317 (0.336) loss_x loss_x 0.9600 (1.1230) acc_x 78.1250 (73.0357) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [40/46] time 0.348 (0.460) data 0.217 (0.330) loss_x loss_x 0.6260 (1.1215) acc_x 84.3750 (72.8125) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [45/46] time 0.391 (0.461) data 0.261 (0.331) loss_x loss_x 1.1377 (1.1190) acc_x 75.0000 (72.9861) lr 2.7630e-05 eta 0:00:00
epoch [187/200] batch [5/52] time 0.530 (0.461) data 0.399 (0.330) loss_u loss_u 0.8901 (0.8371) acc_u 9.3750 (19.3750) lr 2.7630e-05 eta 0:00:21
epoch [187/200] batch [10/52] time 0.429 (0.463) data 0.298 (0.333) loss_u loss_u 0.8838 (0.8379) acc_u 15.6250 (19.0625) lr 2.7630e-05 eta 0:00:19
epoch [187/200] batch [15/52] time 0.402 (0.462) data 0.271 (0.331) loss_u loss_u 0.8223 (0.8192) acc_u 21.8750 (22.2917) lr 2.7630e-05 eta 0:00:17
epoch [187/200] batch [20/52] time 0.509 (0.461) data 0.377 (0.330) loss_u loss_u 0.7646 (0.8130) acc_u 31.2500 (23.4375) lr 2.7630e-05 eta 0:00:14
epoch [187/200] batch [25/52] time 0.421 (0.464) data 0.290 (0.333) loss_u loss_u 0.8491 (0.8136) acc_u 18.7500 (23.3750) lr 2.7630e-05 eta 0:00:12
epoch [187/200] batch [30/52] time 0.439 (0.467) data 0.308 (0.336) loss_u loss_u 0.8818 (0.8175) acc_u 12.5000 (22.5000) lr 2.7630e-05 eta 0:00:10
epoch [187/200] batch [35/52] time 0.471 (0.464) data 0.340 (0.333) loss_u loss_u 0.7417 (0.8169) acc_u 31.2500 (22.4107) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [40/52] time 0.535 (0.466) data 0.404 (0.335) loss_u loss_u 0.8516 (0.8192) acc_u 21.8750 (22.0312) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [45/52] time 0.669 (0.469) data 0.539 (0.339) loss_u loss_u 0.7803 (0.8135) acc_u 21.8750 (23.0556) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [50/52] time 0.406 (0.468) data 0.274 (0.338) loss_u loss_u 0.7295 (0.8097) acc_u 28.1250 (23.4375) lr 2.7630e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1388
confident_label rate tensor(0.4541, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1424
clean true:1382
clean false:42
clean_rate:0.9705056179775281
noisy true:366
noisy false:1346
after delete: len(clean_dataset) 1424
after delete: len(noisy_dataset) 1712
epoch [188/200] batch [5/44] time 0.548 (0.492) data 0.417 (0.362) loss_x loss_x 0.8384 (1.0479) acc_x 75.0000 (75.0000) lr 2.4083e-05 eta 0:00:19
epoch [188/200] batch [10/44] time 0.399 (0.485) data 0.269 (0.354) loss_x loss_x 1.1221 (1.0489) acc_x 71.8750 (74.3750) lr 2.4083e-05 eta 0:00:16
epoch [188/200] batch [15/44] time 0.499 (0.492) data 0.369 (0.362) loss_x loss_x 1.0146 (1.0294) acc_x 78.1250 (74.3750) lr 2.4083e-05 eta 0:00:14
epoch [188/200] batch [20/44] time 0.549 (0.506) data 0.419 (0.375) loss_x loss_x 0.9595 (1.0160) acc_x 71.8750 (73.9062) lr 2.4083e-05 eta 0:00:12
epoch [188/200] batch [25/44] time 0.446 (0.494) data 0.315 (0.363) loss_x loss_x 0.5522 (0.9682) acc_x 87.5000 (75.5000) lr 2.4083e-05 eta 0:00:09
epoch [188/200] batch [30/44] time 0.508 (0.488) data 0.377 (0.357) loss_x loss_x 0.9155 (0.9770) acc_x 81.2500 (75.5208) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [35/44] time 0.600 (0.484) data 0.469 (0.353) loss_x loss_x 1.0879 (0.9948) acc_x 75.0000 (75.3571) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [40/44] time 0.451 (0.478) data 0.320 (0.348) loss_x loss_x 0.8818 (1.0089) acc_x 78.1250 (74.7656) lr 2.4083e-05 eta 0:00:01
epoch [188/200] batch [5/53] time 0.383 (0.469) data 0.252 (0.339) loss_u loss_u 0.8408 (0.8081) acc_u 18.7500 (23.7500) lr 2.4083e-05 eta 0:00:22
epoch [188/200] batch [10/53] time 0.285 (0.468) data 0.154 (0.337) loss_u loss_u 0.8110 (0.8081) acc_u 21.8750 (22.5000) lr 2.4083e-05 eta 0:00:20
epoch [188/200] batch [15/53] time 0.432 (0.466) data 0.302 (0.335) loss_u loss_u 0.7349 (0.8007) acc_u 34.3750 (24.1667) lr 2.4083e-05 eta 0:00:17
epoch [188/200] batch [20/53] time 0.378 (0.463) data 0.248 (0.333) loss_u loss_u 0.7334 (0.7941) acc_u 37.5000 (25.4688) lr 2.4083e-05 eta 0:00:15
epoch [188/200] batch [25/53] time 0.647 (0.464) data 0.517 (0.334) loss_u loss_u 0.8418 (0.7960) acc_u 18.7500 (25.1250) lr 2.4083e-05 eta 0:00:12
epoch [188/200] batch [30/53] time 0.362 (0.460) data 0.231 (0.329) loss_u loss_u 0.7695 (0.7970) acc_u 25.0000 (25.0000) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [35/53] time 0.409 (0.457) data 0.278 (0.326) loss_u loss_u 0.8530 (0.7994) acc_u 18.7500 (24.7321) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [40/53] time 0.416 (0.456) data 0.285 (0.325) loss_u loss_u 0.6987 (0.7973) acc_u 37.5000 (25.0781) lr 2.4083e-05 eta 0:00:05
epoch [188/200] batch [45/53] time 0.637 (0.460) data 0.505 (0.329) loss_u loss_u 0.7822 (0.8001) acc_u 28.1250 (24.6528) lr 2.4083e-05 eta 0:00:03
epoch [188/200] batch [50/53] time 0.460 (0.461) data 0.328 (0.330) loss_u loss_u 0.7983 (0.8013) acc_u 18.7500 (24.1250) lr 2.4083e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1393
confident_label rate tensor(0.4560, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1430
clean true:1387
clean false:43
clean_rate:0.9699300699300699
noisy true:356
noisy false:1350
after delete: len(clean_dataset) 1430
after delete: len(noisy_dataset) 1706
epoch [189/200] batch [5/44] time 0.518 (0.454) data 0.387 (0.324) loss_x loss_x 1.0156 (0.9246) acc_x 71.8750 (75.0000) lr 2.0777e-05 eta 0:00:17
epoch [189/200] batch [10/44] time 0.385 (0.431) data 0.255 (0.300) loss_x loss_x 1.1143 (1.0176) acc_x 71.8750 (74.6875) lr 2.0777e-05 eta 0:00:14
epoch [189/200] batch [15/44] time 0.523 (0.445) data 0.393 (0.315) loss_x loss_x 1.9072 (1.1508) acc_x 53.1250 (71.4583) lr 2.0777e-05 eta 0:00:12
epoch [189/200] batch [20/44] time 0.439 (0.460) data 0.309 (0.330) loss_x loss_x 0.6201 (1.1245) acc_x 84.3750 (72.1875) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [25/44] time 0.473 (0.454) data 0.342 (0.324) loss_x loss_x 0.7051 (1.1416) acc_x 84.3750 (71.7500) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [30/44] time 0.390 (0.450) data 0.260 (0.320) loss_x loss_x 1.5293 (1.1273) acc_x 59.3750 (71.5625) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [35/44] time 0.434 (0.461) data 0.303 (0.331) loss_x loss_x 0.6753 (1.1198) acc_x 81.2500 (72.0536) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [40/44] time 0.435 (0.460) data 0.304 (0.329) loss_x loss_x 1.4004 (1.1407) acc_x 75.0000 (72.0312) lr 2.0777e-05 eta 0:00:01
epoch [189/200] batch [5/53] time 0.536 (0.462) data 0.405 (0.331) loss_u loss_u 0.7524 (0.7949) acc_u 31.2500 (25.0000) lr 2.0777e-05 eta 0:00:22
epoch [189/200] batch [10/53] time 0.381 (0.463) data 0.250 (0.333) loss_u loss_u 0.6152 (0.7754) acc_u 46.8750 (28.1250) lr 2.0777e-05 eta 0:00:19
epoch [189/200] batch [15/53] time 0.539 (0.461) data 0.408 (0.331) loss_u loss_u 0.8237 (0.7700) acc_u 25.0000 (28.9583) lr 2.0777e-05 eta 0:00:17
epoch [189/200] batch [20/53] time 0.581 (0.463) data 0.451 (0.333) loss_u loss_u 0.7861 (0.7854) acc_u 28.1250 (26.5625) lr 2.0777e-05 eta 0:00:15
epoch [189/200] batch [25/53] time 0.422 (0.467) data 0.290 (0.336) loss_u loss_u 0.8066 (0.7921) acc_u 18.7500 (25.6250) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [30/53] time 0.420 (0.461) data 0.289 (0.331) loss_u loss_u 0.7671 (0.7902) acc_u 25.0000 (25.8333) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [35/53] time 0.394 (0.458) data 0.264 (0.327) loss_u loss_u 0.8159 (0.7913) acc_u 21.8750 (25.6250) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [40/53] time 0.520 (0.461) data 0.388 (0.330) loss_u loss_u 0.7744 (0.7912) acc_u 28.1250 (25.7031) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [45/53] time 0.363 (0.464) data 0.232 (0.333) loss_u loss_u 0.7402 (0.7920) acc_u 31.2500 (25.4167) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [50/53] time 0.486 (0.464) data 0.355 (0.334) loss_u loss_u 0.8926 (0.7962) acc_u 12.5000 (25.0625) lr 2.0777e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1353
confident_label rate tensor(0.4652, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1459
clean true:1415
clean false:44
clean_rate:0.969842357779301
noisy true:368
noisy false:1309
after delete: len(clean_dataset) 1459
after delete: len(noisy_dataset) 1677
epoch [190/200] batch [5/45] time 0.625 (0.516) data 0.494 (0.386) loss_x loss_x 1.3418 (0.8634) acc_x 62.5000 (73.1250) lr 1.7713e-05 eta 0:00:20
epoch [190/200] batch [10/45] time 0.395 (0.491) data 0.264 (0.360) loss_x loss_x 1.0068 (0.9962) acc_x 75.0000 (70.6250) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [15/45] time 0.443 (0.479) data 0.313 (0.349) loss_x loss_x 0.7144 (1.0352) acc_x 87.5000 (71.6667) lr 1.7713e-05 eta 0:00:14
epoch [190/200] batch [20/45] time 0.338 (0.466) data 0.207 (0.336) loss_x loss_x 0.8730 (1.0536) acc_x 84.3750 (71.8750) lr 1.7713e-05 eta 0:00:11
epoch [190/200] batch [25/45] time 0.552 (0.474) data 0.422 (0.343) loss_x loss_x 1.0430 (1.0816) acc_x 78.1250 (71.6250) lr 1.7713e-05 eta 0:00:09
epoch [190/200] batch [30/45] time 0.510 (0.478) data 0.379 (0.347) loss_x loss_x 1.2021 (1.1000) acc_x 78.1250 (71.8750) lr 1.7713e-05 eta 0:00:07
epoch [190/200] batch [35/45] time 0.416 (0.473) data 0.286 (0.343) loss_x loss_x 1.7178 (1.0927) acc_x 59.3750 (71.9643) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [40/45] time 0.554 (0.475) data 0.422 (0.344) loss_x loss_x 0.7603 (1.0636) acc_x 75.0000 (73.3594) lr 1.7713e-05 eta 0:00:02
epoch [190/200] batch [45/45] time 0.456 (0.476) data 0.325 (0.345) loss_x loss_x 0.6152 (1.0414) acc_x 78.1250 (73.9583) lr 1.7713e-05 eta 0:00:00
epoch [190/200] batch [5/52] time 0.922 (0.480) data 0.790 (0.349) loss_u loss_u 0.8105 (0.7937) acc_u 28.1250 (26.8750) lr 1.7713e-05 eta 0:00:22
epoch [190/200] batch [10/52] time 0.440 (0.476) data 0.308 (0.345) loss_u loss_u 0.8491 (0.7817) acc_u 15.6250 (27.1875) lr 1.7713e-05 eta 0:00:19
epoch [190/200] batch [15/52] time 0.490 (0.475) data 0.359 (0.344) loss_u loss_u 0.8677 (0.7994) acc_u 18.7500 (25.2083) lr 1.7713e-05 eta 0:00:17
epoch [190/200] batch [20/52] time 0.610 (0.477) data 0.479 (0.346) loss_u loss_u 0.7891 (0.8075) acc_u 25.0000 (23.5938) lr 1.7713e-05 eta 0:00:15
epoch [190/200] batch [25/52] time 0.589 (0.479) data 0.458 (0.348) loss_u loss_u 0.8906 (0.8072) acc_u 12.5000 (23.5000) lr 1.7713e-05 eta 0:00:12
epoch [190/200] batch [30/52] time 0.396 (0.473) data 0.264 (0.343) loss_u loss_u 0.9326 (0.8090) acc_u 3.1250 (23.1250) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [35/52] time 0.583 (0.472) data 0.451 (0.341) loss_u loss_u 0.8555 (0.8137) acc_u 18.7500 (22.4107) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [40/52] time 0.568 (0.473) data 0.436 (0.341) loss_u loss_u 0.8833 (0.8181) acc_u 15.6250 (22.1875) lr 1.7713e-05 eta 0:00:05
epoch [190/200] batch [45/52] time 0.544 (0.470) data 0.412 (0.339) loss_u loss_u 0.8467 (0.8143) acc_u 15.6250 (22.6389) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [50/52] time 0.388 (0.468) data 0.257 (0.337) loss_u loss_u 0.7959 (0.8094) acc_u 21.8750 (23.3750) lr 1.7713e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1401
confident_label rate tensor(0.4557, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1429
clean true:1381
clean false:48
clean_rate:0.966410076976907
noisy true:354
noisy false:1353
after delete: len(clean_dataset) 1429
after delete: len(noisy_dataset) 1707
epoch [191/200] batch [5/44] time 0.516 (0.465) data 0.385 (0.334) loss_x loss_x 1.2422 (1.0886) acc_x 71.8750 (71.8750) lr 1.4891e-05 eta 0:00:18
epoch [191/200] batch [10/44] time 0.352 (0.484) data 0.222 (0.353) loss_x loss_x 1.0498 (1.1216) acc_x 71.8750 (70.3125) lr 1.4891e-05 eta 0:00:16
epoch [191/200] batch [15/44] time 0.511 (0.488) data 0.380 (0.357) loss_x loss_x 1.1475 (1.0710) acc_x 68.7500 (72.2917) lr 1.4891e-05 eta 0:00:14
epoch [191/200] batch [20/44] time 0.553 (0.487) data 0.422 (0.356) loss_x loss_x 1.1465 (1.0875) acc_x 71.8750 (71.2500) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [25/44] time 0.492 (0.482) data 0.361 (0.351) loss_x loss_x 1.5068 (1.0649) acc_x 65.6250 (72.2500) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [30/44] time 0.479 (0.479) data 0.348 (0.349) loss_x loss_x 0.9287 (1.0942) acc_x 78.1250 (71.8750) lr 1.4891e-05 eta 0:00:06
epoch [191/200] batch [35/44] time 0.391 (0.474) data 0.260 (0.343) loss_x loss_x 0.9976 (1.0946) acc_x 75.0000 (72.1429) lr 1.4891e-05 eta 0:00:04
epoch [191/200] batch [40/44] time 0.516 (0.469) data 0.386 (0.338) loss_x loss_x 1.3135 (1.1100) acc_x 68.7500 (71.8750) lr 1.4891e-05 eta 0:00:01
epoch [191/200] batch [5/53] time 0.648 (0.478) data 0.516 (0.347) loss_u loss_u 0.8066 (0.8137) acc_u 21.8750 (20.0000) lr 1.4891e-05 eta 0:00:22
epoch [191/200] batch [10/53] time 0.468 (0.478) data 0.336 (0.348) loss_u loss_u 0.6938 (0.8150) acc_u 40.6250 (21.2500) lr 1.4891e-05 eta 0:00:20
epoch [191/200] batch [15/53] time 0.392 (0.481) data 0.261 (0.350) loss_u loss_u 0.8096 (0.8127) acc_u 31.2500 (22.5000) lr 1.4891e-05 eta 0:00:18
epoch [191/200] batch [20/53] time 0.541 (0.480) data 0.409 (0.349) loss_u loss_u 0.6816 (0.8069) acc_u 34.3750 (23.2812) lr 1.4891e-05 eta 0:00:15
epoch [191/200] batch [25/53] time 0.384 (0.478) data 0.254 (0.347) loss_u loss_u 0.7515 (0.8062) acc_u 34.3750 (23.8750) lr 1.4891e-05 eta 0:00:13
epoch [191/200] batch [30/53] time 0.366 (0.473) data 0.235 (0.342) loss_u loss_u 0.7974 (0.8063) acc_u 25.0000 (24.0625) lr 1.4891e-05 eta 0:00:10
epoch [191/200] batch [35/53] time 0.533 (0.476) data 0.401 (0.345) loss_u loss_u 0.7207 (0.7999) acc_u 34.3750 (24.8214) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [40/53] time 0.468 (0.472) data 0.337 (0.342) loss_u loss_u 0.7837 (0.7992) acc_u 25.0000 (24.8438) lr 1.4891e-05 eta 0:00:06
epoch [191/200] batch [45/53] time 0.621 (0.474) data 0.488 (0.343) loss_u loss_u 0.8501 (0.7998) acc_u 15.6250 (25.0000) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [50/53] time 0.439 (0.473) data 0.308 (0.342) loss_u loss_u 0.7603 (0.8000) acc_u 31.2500 (24.8750) lr 1.4891e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1357
confident_label rate tensor(0.4707, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1476
clean true:1427
clean false:49
clean_rate:0.9668021680216802
noisy true:352
noisy false:1308
after delete: len(clean_dataset) 1476
after delete: len(noisy_dataset) 1660
epoch [192/200] batch [5/46] time 0.520 (0.494) data 0.389 (0.363) loss_x loss_x 1.2480 (1.1887) acc_x 65.6250 (69.3750) lr 1.2312e-05 eta 0:00:20
epoch [192/200] batch [10/46] time 0.443 (0.502) data 0.312 (0.371) loss_x loss_x 1.1924 (1.1746) acc_x 62.5000 (68.7500) lr 1.2312e-05 eta 0:00:18
epoch [192/200] batch [15/46] time 0.527 (0.487) data 0.395 (0.357) loss_x loss_x 1.1221 (1.1428) acc_x 68.7500 (70.2083) lr 1.2312e-05 eta 0:00:15
epoch [192/200] batch [20/46] time 0.517 (0.477) data 0.386 (0.346) loss_x loss_x 0.7607 (1.1208) acc_x 71.8750 (70.6250) lr 1.2312e-05 eta 0:00:12
epoch [192/200] batch [25/46] time 0.435 (0.473) data 0.304 (0.342) loss_x loss_x 1.0566 (1.1268) acc_x 75.0000 (70.6250) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [30/46] time 0.462 (0.465) data 0.332 (0.334) loss_x loss_x 1.2070 (1.1175) acc_x 65.6250 (70.3125) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [35/46] time 0.416 (0.465) data 0.285 (0.334) loss_x loss_x 1.1582 (1.1324) acc_x 62.5000 (70.3571) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [40/46] time 0.448 (0.462) data 0.318 (0.331) loss_x loss_x 1.6621 (1.1388) acc_x 65.6250 (70.3906) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [45/46] time 0.377 (0.459) data 0.247 (0.328) loss_x loss_x 0.5493 (1.1254) acc_x 81.2500 (70.5556) lr 1.2312e-05 eta 0:00:00
epoch [192/200] batch [5/51] time 0.346 (0.459) data 0.216 (0.328) loss_u loss_u 0.7026 (0.7595) acc_u 34.3750 (30.0000) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [10/51] time 0.469 (0.460) data 0.338 (0.329) loss_u loss_u 0.8745 (0.8075) acc_u 12.5000 (25.0000) lr 1.2312e-05 eta 0:00:18
epoch [192/200] batch [15/51] time 0.474 (0.462) data 0.343 (0.331) loss_u loss_u 0.8208 (0.8082) acc_u 18.7500 (24.5833) lr 1.2312e-05 eta 0:00:16
epoch [192/200] batch [20/51] time 0.560 (0.462) data 0.428 (0.331) loss_u loss_u 0.8257 (0.8141) acc_u 25.0000 (22.9688) lr 1.2312e-05 eta 0:00:14
epoch [192/200] batch [25/51] time 0.493 (0.466) data 0.362 (0.335) loss_u loss_u 0.8325 (0.8130) acc_u 21.8750 (23.3750) lr 1.2312e-05 eta 0:00:12
epoch [192/200] batch [30/51] time 0.431 (0.464) data 0.300 (0.333) loss_u loss_u 0.7568 (0.8068) acc_u 31.2500 (24.4792) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [35/51] time 0.436 (0.468) data 0.304 (0.337) loss_u loss_u 0.7656 (0.8058) acc_u 31.2500 (24.5536) lr 1.2312e-05 eta 0:00:07
epoch [192/200] batch [40/51] time 0.621 (0.467) data 0.490 (0.336) loss_u loss_u 0.8057 (0.8079) acc_u 21.8750 (24.2188) lr 1.2312e-05 eta 0:00:05
epoch [192/200] batch [45/51] time 0.457 (0.468) data 0.327 (0.338) loss_u loss_u 0.9028 (0.8083) acc_u 6.2500 (24.0972) lr 1.2312e-05 eta 0:00:02
epoch [192/200] batch [50/51] time 0.421 (0.467) data 0.290 (0.336) loss_u loss_u 0.8818 (0.8086) acc_u 15.6250 (23.8750) lr 1.2312e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1383
confident_label rate tensor(0.4547, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1426
clean true:1383
clean false:43
clean_rate:0.9698457223001402
noisy true:370
noisy false:1340
after delete: len(clean_dataset) 1426
after delete: len(noisy_dataset) 1710
epoch [193/200] batch [5/44] time 0.395 (0.442) data 0.265 (0.311) loss_x loss_x 1.0713 (1.0356) acc_x 78.1250 (73.7500) lr 9.9763e-06 eta 0:00:17
epoch [193/200] batch [10/44] time 0.436 (0.444) data 0.305 (0.313) loss_x loss_x 0.8125 (1.0897) acc_x 78.1250 (73.1250) lr 9.9763e-06 eta 0:00:15
epoch [193/200] batch [15/44] time 0.615 (0.451) data 0.483 (0.320) loss_x loss_x 1.1025 (1.0160) acc_x 71.8750 (75.4167) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [20/44] time 0.674 (0.470) data 0.543 (0.339) loss_x loss_x 1.3477 (1.0456) acc_x 71.8750 (75.4688) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [25/44] time 0.412 (0.468) data 0.281 (0.337) loss_x loss_x 1.8291 (1.0979) acc_x 56.2500 (74.5000) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [30/44] time 0.434 (0.467) data 0.303 (0.336) loss_x loss_x 1.2236 (1.1416) acc_x 68.7500 (72.9167) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [35/44] time 0.450 (0.466) data 0.319 (0.335) loss_x loss_x 0.8882 (1.1133) acc_x 68.7500 (73.3929) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [40/44] time 0.488 (0.470) data 0.356 (0.339) loss_x loss_x 0.7759 (1.0929) acc_x 87.5000 (73.9844) lr 9.9763e-06 eta 0:00:01
epoch [193/200] batch [5/53] time 0.443 (0.463) data 0.311 (0.332) loss_u loss_u 0.8315 (0.8336) acc_u 25.0000 (21.2500) lr 9.9763e-06 eta 0:00:22
epoch [193/200] batch [10/53] time 0.486 (0.466) data 0.354 (0.335) loss_u loss_u 0.8135 (0.8220) acc_u 21.8750 (21.8750) lr 9.9763e-06 eta 0:00:20
epoch [193/200] batch [15/53] time 0.437 (0.465) data 0.305 (0.333) loss_u loss_u 0.7124 (0.8010) acc_u 31.2500 (24.7917) lr 9.9763e-06 eta 0:00:17
epoch [193/200] batch [20/53] time 0.364 (0.460) data 0.232 (0.329) loss_u loss_u 0.7324 (0.8021) acc_u 37.5000 (24.6875) lr 9.9763e-06 eta 0:00:15
epoch [193/200] batch [25/53] time 0.438 (0.461) data 0.306 (0.330) loss_u loss_u 0.7485 (0.8020) acc_u 34.3750 (25.0000) lr 9.9763e-06 eta 0:00:12
epoch [193/200] batch [30/53] time 0.396 (0.456) data 0.264 (0.325) loss_u loss_u 0.7505 (0.7989) acc_u 28.1250 (25.2083) lr 9.9763e-06 eta 0:00:10
epoch [193/200] batch [35/53] time 0.393 (0.455) data 0.261 (0.323) loss_u loss_u 0.6377 (0.7976) acc_u 40.6250 (25.2679) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [40/53] time 0.524 (0.455) data 0.394 (0.324) loss_u loss_u 0.8140 (0.7968) acc_u 21.8750 (25.5469) lr 9.9763e-06 eta 0:00:05
epoch [193/200] batch [45/53] time 0.427 (0.454) data 0.297 (0.323) loss_u loss_u 0.8584 (0.8005) acc_u 12.5000 (24.7917) lr 9.9763e-06 eta 0:00:03
epoch [193/200] batch [50/53] time 0.488 (0.457) data 0.356 (0.325) loss_u loss_u 0.7344 (0.8011) acc_u 34.3750 (24.8125) lr 9.9763e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1392
confident_label rate tensor(0.4585, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1438
clean true:1392
clean false:46
clean_rate:0.9680111265646731
noisy true:352
noisy false:1346
after delete: len(clean_dataset) 1438
after delete: len(noisy_dataset) 1698
epoch [194/200] batch [5/44] time 0.417 (0.451) data 0.286 (0.320) loss_x loss_x 0.9194 (1.1021) acc_x 78.1250 (70.6250) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [10/44] time 0.441 (0.460) data 0.310 (0.329) loss_x loss_x 1.2500 (1.1025) acc_x 71.8750 (71.2500) lr 7.8853e-06 eta 0:00:15
epoch [194/200] batch [15/44] time 0.361 (0.454) data 0.230 (0.323) loss_x loss_x 0.8486 (1.0846) acc_x 81.2500 (71.2500) lr 7.8853e-06 eta 0:00:13
epoch [194/200] batch [20/44] time 0.466 (0.456) data 0.335 (0.325) loss_x loss_x 1.4863 (1.0755) acc_x 65.6250 (72.5000) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [25/44] time 0.478 (0.443) data 0.347 (0.312) loss_x loss_x 0.7339 (1.0487) acc_x 81.2500 (73.6250) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [30/44] time 0.403 (0.448) data 0.272 (0.317) loss_x loss_x 1.0830 (1.0521) acc_x 65.6250 (73.5417) lr 7.8853e-06 eta 0:00:06
epoch [194/200] batch [35/44] time 0.540 (0.451) data 0.410 (0.320) loss_x loss_x 0.8389 (1.0306) acc_x 75.0000 (73.9286) lr 7.8853e-06 eta 0:00:04
epoch [194/200] batch [40/44] time 0.730 (0.460) data 0.599 (0.329) loss_x loss_x 1.1582 (1.0606) acc_x 81.2500 (73.5938) lr 7.8853e-06 eta 0:00:01
epoch [194/200] batch [5/53] time 0.407 (0.460) data 0.275 (0.329) loss_u loss_u 0.7695 (0.8224) acc_u 31.2500 (20.6250) lr 7.8853e-06 eta 0:00:22
epoch [194/200] batch [10/53] time 0.369 (0.455) data 0.237 (0.323) loss_u loss_u 0.7568 (0.8083) acc_u 34.3750 (22.8125) lr 7.8853e-06 eta 0:00:19
epoch [194/200] batch [15/53] time 0.337 (0.455) data 0.205 (0.324) loss_u loss_u 0.8550 (0.8175) acc_u 21.8750 (21.4583) lr 7.8853e-06 eta 0:00:17
epoch [194/200] batch [20/53] time 0.461 (0.450) data 0.329 (0.319) loss_u loss_u 0.8735 (0.8150) acc_u 12.5000 (22.1875) lr 7.8853e-06 eta 0:00:14
epoch [194/200] batch [25/53] time 0.396 (0.449) data 0.264 (0.318) loss_u loss_u 0.7993 (0.8166) acc_u 21.8750 (21.7500) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [30/53] time 0.431 (0.452) data 0.299 (0.321) loss_u loss_u 0.8213 (0.8156) acc_u 18.7500 (22.2917) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [35/53] time 0.386 (0.450) data 0.255 (0.319) loss_u loss_u 0.8564 (0.8154) acc_u 15.6250 (22.3214) lr 7.8853e-06 eta 0:00:08
epoch [194/200] batch [40/53] time 0.396 (0.446) data 0.266 (0.315) loss_u loss_u 0.7598 (0.8117) acc_u 34.3750 (23.0469) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [45/53] time 0.508 (0.447) data 0.377 (0.315) loss_u loss_u 0.7900 (0.8114) acc_u 28.1250 (23.4028) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [50/53] time 0.431 (0.445) data 0.301 (0.313) loss_u loss_u 0.8408 (0.8124) acc_u 21.8750 (23.2500) lr 7.8853e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1385
confident_label rate tensor(0.4538, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1423
clean true:1389
clean false:34
clean_rate:0.9761068165846802
noisy true:362
noisy false:1351
after delete: len(clean_dataset) 1423
after delete: len(noisy_dataset) 1713
epoch [195/200] batch [5/44] time 0.512 (0.459) data 0.381 (0.329) loss_x loss_x 1.2139 (1.1183) acc_x 62.5000 (71.2500) lr 6.0390e-06 eta 0:00:17
epoch [195/200] batch [10/44] time 0.444 (0.441) data 0.314 (0.311) loss_x loss_x 1.1768 (1.1175) acc_x 59.3750 (70.0000) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [15/44] time 0.477 (0.445) data 0.347 (0.315) loss_x loss_x 0.8506 (0.9994) acc_x 78.1250 (73.5417) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [20/44] time 0.439 (0.432) data 0.309 (0.302) loss_x loss_x 1.0117 (1.0007) acc_x 78.1250 (73.7500) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [25/44] time 0.593 (0.442) data 0.462 (0.311) loss_x loss_x 0.9727 (0.9803) acc_x 65.6250 (74.0000) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [30/44] time 0.369 (0.446) data 0.239 (0.315) loss_x loss_x 0.7217 (1.0080) acc_x 84.3750 (73.6458) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [35/44] time 0.506 (0.445) data 0.376 (0.315) loss_x loss_x 1.1367 (1.0272) acc_x 68.7500 (73.5714) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [40/44] time 0.426 (0.439) data 0.296 (0.309) loss_x loss_x 1.1855 (1.0471) acc_x 62.5000 (72.8125) lr 6.0390e-06 eta 0:00:01
epoch [195/200] batch [5/53] time 0.393 (0.433) data 0.262 (0.302) loss_u loss_u 0.7617 (0.7900) acc_u 25.0000 (21.8750) lr 6.0390e-06 eta 0:00:20
epoch [195/200] batch [10/53] time 0.395 (0.435) data 0.264 (0.304) loss_u loss_u 0.8833 (0.8084) acc_u 15.6250 (20.6250) lr 6.0390e-06 eta 0:00:18
epoch [195/200] batch [15/53] time 0.496 (0.436) data 0.366 (0.306) loss_u loss_u 0.7480 (0.7994) acc_u 31.2500 (22.5000) lr 6.0390e-06 eta 0:00:16
epoch [195/200] batch [20/53] time 0.385 (0.438) data 0.254 (0.308) loss_u loss_u 0.8438 (0.8035) acc_u 18.7500 (22.5000) lr 6.0390e-06 eta 0:00:14
epoch [195/200] batch [25/53] time 0.344 (0.438) data 0.212 (0.307) loss_u loss_u 0.7998 (0.8043) acc_u 28.1250 (22.8750) lr 6.0390e-06 eta 0:00:12
epoch [195/200] batch [30/53] time 0.463 (0.440) data 0.330 (0.309) loss_u loss_u 0.7192 (0.7983) acc_u 28.1250 (23.3333) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [35/53] time 0.380 (0.437) data 0.249 (0.306) loss_u loss_u 0.7617 (0.7988) acc_u 31.2500 (23.6607) lr 6.0390e-06 eta 0:00:07
epoch [195/200] batch [40/53] time 0.503 (0.437) data 0.372 (0.306) loss_u loss_u 0.8140 (0.7992) acc_u 21.8750 (23.9062) lr 6.0390e-06 eta 0:00:05
epoch [195/200] batch [45/53] time 0.543 (0.439) data 0.411 (0.308) loss_u loss_u 0.7622 (0.7980) acc_u 28.1250 (24.0972) lr 6.0390e-06 eta 0:00:03
epoch [195/200] batch [50/53] time 0.449 (0.439) data 0.317 (0.308) loss_u loss_u 0.7661 (0.7999) acc_u 31.2500 (24.0000) lr 6.0390e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1363
confident_label rate tensor(0.4649, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1458
clean true:1415
clean false:43
clean_rate:0.9705075445816187
noisy true:358
noisy false:1320
after delete: len(clean_dataset) 1458
after delete: len(noisy_dataset) 1678
epoch [196/200] batch [5/45] time 0.418 (0.441) data 0.289 (0.311) loss_x loss_x 0.8325 (1.3294) acc_x 71.8750 (64.3750) lr 4.4380e-06 eta 0:00:17
epoch [196/200] batch [10/45] time 0.373 (0.447) data 0.243 (0.317) loss_x loss_x 0.6616 (1.1016) acc_x 78.1250 (70.0000) lr 4.4380e-06 eta 0:00:15
epoch [196/200] batch [15/45] time 0.547 (0.466) data 0.417 (0.337) loss_x loss_x 0.9141 (1.0942) acc_x 75.0000 (68.9583) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [20/45] time 0.442 (0.448) data 0.312 (0.318) loss_x loss_x 0.7837 (1.0780) acc_x 81.2500 (69.2188) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [25/45] time 0.436 (0.440) data 0.306 (0.310) loss_x loss_x 1.1836 (1.0766) acc_x 71.8750 (69.6250) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [30/45] time 0.404 (0.441) data 0.273 (0.311) loss_x loss_x 0.8838 (1.0835) acc_x 78.1250 (70.2083) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [35/45] time 0.518 (0.444) data 0.388 (0.314) loss_x loss_x 1.3340 (1.0855) acc_x 68.7500 (70.7143) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [40/45] time 0.478 (0.445) data 0.348 (0.315) loss_x loss_x 0.7412 (1.0912) acc_x 87.5000 (71.0938) lr 4.4380e-06 eta 0:00:02
epoch [196/200] batch [45/45] time 0.442 (0.446) data 0.312 (0.315) loss_x loss_x 0.5464 (1.0747) acc_x 87.5000 (71.3194) lr 4.4380e-06 eta 0:00:00
epoch [196/200] batch [5/52] time 0.530 (0.449) data 0.399 (0.319) loss_u loss_u 0.8550 (0.8142) acc_u 21.8750 (23.7500) lr 4.4380e-06 eta 0:00:21
epoch [196/200] batch [10/52] time 0.469 (0.451) data 0.338 (0.321) loss_u loss_u 0.8198 (0.8181) acc_u 18.7500 (21.5625) lr 4.4380e-06 eta 0:00:18
epoch [196/200] batch [15/52] time 0.378 (0.446) data 0.247 (0.316) loss_u loss_u 0.9062 (0.8220) acc_u 12.5000 (22.0833) lr 4.4380e-06 eta 0:00:16
epoch [196/200] batch [20/52] time 0.637 (0.447) data 0.506 (0.317) loss_u loss_u 0.8154 (0.8207) acc_u 18.7500 (21.8750) lr 4.4380e-06 eta 0:00:14
epoch [196/200] batch [25/52] time 0.375 (0.445) data 0.244 (0.314) loss_u loss_u 0.8301 (0.8216) acc_u 15.6250 (21.6250) lr 4.4380e-06 eta 0:00:12
epoch [196/200] batch [30/52] time 0.433 (0.441) data 0.301 (0.311) loss_u loss_u 0.8706 (0.8204) acc_u 12.5000 (21.6667) lr 4.4380e-06 eta 0:00:09
epoch [196/200] batch [35/52] time 0.389 (0.443) data 0.258 (0.313) loss_u loss_u 0.7544 (0.8181) acc_u 31.2500 (22.2321) lr 4.4380e-06 eta 0:00:07
epoch [196/200] batch [40/52] time 0.401 (0.442) data 0.270 (0.311) loss_u loss_u 0.8198 (0.8136) acc_u 21.8750 (22.7344) lr 4.4380e-06 eta 0:00:05
epoch [196/200] batch [45/52] time 0.418 (0.438) data 0.288 (0.308) loss_u loss_u 0.7339 (0.8099) acc_u 37.5000 (23.4028) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [50/52] time 0.486 (0.439) data 0.355 (0.309) loss_u loss_u 0.9058 (0.8090) acc_u 12.5000 (23.5000) lr 4.4380e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1412
confident_label rate tensor(0.4471, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1402
clean true:1357
clean false:45
clean_rate:0.9679029957203994
noisy true:367
noisy false:1367
after delete: len(clean_dataset) 1402
after delete: len(noisy_dataset) 1734
epoch [197/200] batch [5/43] time 0.479 (0.433) data 0.348 (0.302) loss_x loss_x 1.5205 (1.0503) acc_x 71.8750 (70.6250) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [10/43] time 0.369 (0.426) data 0.239 (0.295) loss_x loss_x 0.9536 (0.9914) acc_x 75.0000 (73.1250) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [15/43] time 0.420 (0.453) data 0.290 (0.323) loss_x loss_x 0.7554 (1.0025) acc_x 84.3750 (73.9583) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [20/43] time 0.589 (0.453) data 0.459 (0.322) loss_x loss_x 0.9751 (1.0427) acc_x 78.1250 (73.7500) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [25/43] time 0.342 (0.452) data 0.212 (0.321) loss_x loss_x 0.8970 (1.0453) acc_x 84.3750 (73.7500) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [30/43] time 0.419 (0.443) data 0.289 (0.313) loss_x loss_x 1.3643 (1.0650) acc_x 71.8750 (73.4375) lr 3.0827e-06 eta 0:00:05
epoch [197/200] batch [35/43] time 0.417 (0.442) data 0.286 (0.312) loss_x loss_x 0.7349 (1.0505) acc_x 84.3750 (74.2857) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [40/43] time 0.419 (0.440) data 0.288 (0.309) loss_x loss_x 1.0508 (1.0471) acc_x 68.7500 (73.6719) lr 3.0827e-06 eta 0:00:01
epoch [197/200] batch [5/54] time 0.347 (0.434) data 0.216 (0.304) loss_u loss_u 0.8027 (0.7809) acc_u 21.8750 (26.8750) lr 3.0827e-06 eta 0:00:21
epoch [197/200] batch [10/54] time 0.367 (0.432) data 0.236 (0.302) loss_u loss_u 0.7837 (0.7815) acc_u 18.7500 (26.2500) lr 3.0827e-06 eta 0:00:19
epoch [197/200] batch [15/54] time 0.523 (0.435) data 0.392 (0.305) loss_u loss_u 0.8008 (0.7785) acc_u 34.3750 (27.2917) lr 3.0827e-06 eta 0:00:16
epoch [197/200] batch [20/54] time 0.368 (0.440) data 0.237 (0.309) loss_u loss_u 0.7817 (0.7775) acc_u 25.0000 (27.6562) lr 3.0827e-06 eta 0:00:14
epoch [197/200] batch [25/54] time 0.362 (0.437) data 0.231 (0.306) loss_u loss_u 0.8325 (0.7801) acc_u 15.6250 (27.6250) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [30/54] time 0.369 (0.436) data 0.238 (0.305) loss_u loss_u 0.7642 (0.7850) acc_u 25.0000 (26.4583) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [35/54] time 0.358 (0.432) data 0.227 (0.302) loss_u loss_u 0.8740 (0.7898) acc_u 21.8750 (25.9821) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [40/54] time 0.388 (0.436) data 0.257 (0.305) loss_u loss_u 0.7979 (0.7955) acc_u 25.0000 (25.0781) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [45/54] time 0.357 (0.434) data 0.226 (0.304) loss_u loss_u 0.8262 (0.7982) acc_u 25.0000 (25.0000) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [50/54] time 0.360 (0.434) data 0.229 (0.303) loss_u loss_u 0.8140 (0.7969) acc_u 25.0000 (25.1250) lr 3.0827e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1379
confident_label rate tensor(0.4534, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1422
clean true:1383
clean false:39
clean_rate:0.9725738396624473
noisy true:374
noisy false:1340
after delete: len(clean_dataset) 1422
after delete: len(noisy_dataset) 1714
epoch [198/200] batch [5/44] time 0.449 (0.450) data 0.319 (0.319) loss_x loss_x 1.1973 (1.0051) acc_x 68.7500 (75.0000) lr 1.9733e-06 eta 0:00:17
epoch [198/200] batch [10/44] time 0.434 (0.443) data 0.303 (0.312) loss_x loss_x 1.1660 (0.9924) acc_x 71.8750 (75.9375) lr 1.9733e-06 eta 0:00:15
epoch [198/200] batch [15/44] time 0.405 (0.442) data 0.274 (0.311) loss_x loss_x 1.0469 (1.0631) acc_x 78.1250 (74.1667) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [20/44] time 0.402 (0.441) data 0.272 (0.310) loss_x loss_x 0.7646 (1.0644) acc_x 75.0000 (73.2812) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [25/44] time 0.389 (0.434) data 0.259 (0.304) loss_x loss_x 0.8594 (1.0800) acc_x 78.1250 (73.2500) lr 1.9733e-06 eta 0:00:08
epoch [198/200] batch [30/44] time 0.379 (0.441) data 0.249 (0.311) loss_x loss_x 1.0557 (1.0674) acc_x 75.0000 (73.2292) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [35/44] time 0.414 (0.443) data 0.284 (0.312) loss_x loss_x 1.1377 (1.0551) acc_x 75.0000 (73.9286) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [40/44] time 0.497 (0.443) data 0.367 (0.313) loss_x loss_x 1.9238 (1.0776) acc_x 59.3750 (73.2812) lr 1.9733e-06 eta 0:00:01
epoch [198/200] batch [5/53] time 0.480 (0.439) data 0.349 (0.309) loss_u loss_u 0.8506 (0.8043) acc_u 15.6250 (26.8750) lr 1.9733e-06 eta 0:00:21
epoch [198/200] batch [10/53] time 0.480 (0.437) data 0.349 (0.307) loss_u loss_u 0.8174 (0.8143) acc_u 21.8750 (24.3750) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [15/53] time 0.516 (0.436) data 0.385 (0.305) loss_u loss_u 0.7686 (0.8125) acc_u 25.0000 (23.7500) lr 1.9733e-06 eta 0:00:16
epoch [198/200] batch [20/53] time 0.398 (0.435) data 0.267 (0.305) loss_u loss_u 0.7944 (0.8082) acc_u 28.1250 (24.5312) lr 1.9733e-06 eta 0:00:14
epoch [198/200] batch [25/53] time 0.389 (0.435) data 0.258 (0.304) loss_u loss_u 0.7812 (0.8058) acc_u 28.1250 (25.0000) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [30/53] time 0.587 (0.445) data 0.456 (0.314) loss_u loss_u 0.8066 (0.8085) acc_u 28.1250 (24.8958) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [35/53] time 0.380 (0.442) data 0.249 (0.312) loss_u loss_u 0.9038 (0.8110) acc_u 9.3750 (23.9286) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [40/53] time 0.393 (0.441) data 0.261 (0.310) loss_u loss_u 0.8208 (0.8091) acc_u 25.0000 (23.9062) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [45/53] time 0.498 (0.441) data 0.367 (0.310) loss_u loss_u 0.8423 (0.8085) acc_u 18.7500 (24.0278) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [50/53] time 0.393 (0.437) data 0.262 (0.306) loss_u loss_u 0.8042 (0.8081) acc_u 25.0000 (23.9375) lr 1.9733e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1377
confident_label rate tensor(0.4605, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1444
clean true:1405
clean false:39
clean_rate:0.9729916897506925
noisy true:354
noisy false:1338
after delete: len(clean_dataset) 1444
after delete: len(noisy_dataset) 1692
epoch [199/200] batch [5/45] time 0.419 (0.424) data 0.288 (0.294) loss_x loss_x 0.8843 (1.2544) acc_x 78.1250 (70.6250) lr 1.1101e-06 eta 0:00:16
epoch [199/200] batch [10/45] time 0.460 (0.432) data 0.329 (0.302) loss_x loss_x 1.0889 (1.1931) acc_x 68.7500 (71.5625) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [15/45] time 0.452 (0.423) data 0.322 (0.293) loss_x loss_x 1.1797 (1.1417) acc_x 78.1250 (72.2917) lr 1.1101e-06 eta 0:00:12
epoch [199/200] batch [20/45] time 0.369 (0.420) data 0.239 (0.289) loss_x loss_x 1.0273 (1.1729) acc_x 71.8750 (71.5625) lr 1.1101e-06 eta 0:00:10
epoch [199/200] batch [25/45] time 0.458 (0.429) data 0.327 (0.298) loss_x loss_x 1.0527 (1.1386) acc_x 71.8750 (71.8750) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [30/45] time 0.436 (0.435) data 0.305 (0.305) loss_x loss_x 0.9434 (1.1309) acc_x 75.0000 (72.6042) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [35/45] time 0.445 (0.432) data 0.315 (0.301) loss_x loss_x 1.8213 (1.1675) acc_x 62.5000 (71.5179) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [40/45] time 0.384 (0.433) data 0.253 (0.302) loss_x loss_x 1.5479 (1.1417) acc_x 68.7500 (72.2656) lr 1.1101e-06 eta 0:00:02
epoch [199/200] batch [45/45] time 0.508 (0.435) data 0.377 (0.304) loss_x loss_x 0.7949 (1.1528) acc_x 78.1250 (71.7361) lr 1.1101e-06 eta 0:00:00
epoch [199/200] batch [5/52] time 0.511 (0.434) data 0.380 (0.303) loss_u loss_u 0.9028 (0.7948) acc_u 6.2500 (25.0000) lr 1.1101e-06 eta 0:00:20
epoch [199/200] batch [10/52] time 0.424 (0.429) data 0.293 (0.298) loss_u loss_u 0.8892 (0.8244) acc_u 12.5000 (21.5625) lr 1.1101e-06 eta 0:00:18
epoch [199/200] batch [15/52] time 0.486 (0.426) data 0.355 (0.295) loss_u loss_u 0.8325 (0.8265) acc_u 21.8750 (21.2500) lr 1.1101e-06 eta 0:00:15
epoch [199/200] batch [20/52] time 0.433 (0.429) data 0.302 (0.299) loss_u loss_u 0.8223 (0.8213) acc_u 18.7500 (21.7188) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [25/52] time 0.518 (0.434) data 0.387 (0.303) loss_u loss_u 0.7476 (0.8091) acc_u 31.2500 (23.8750) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [30/52] time 0.428 (0.433) data 0.297 (0.302) loss_u loss_u 0.7256 (0.8091) acc_u 37.5000 (24.1667) lr 1.1101e-06 eta 0:00:09
epoch [199/200] batch [35/52] time 0.389 (0.437) data 0.258 (0.306) loss_u loss_u 0.8340 (0.8055) acc_u 18.7500 (24.0179) lr 1.1101e-06 eta 0:00:07
epoch [199/200] batch [40/52] time 0.434 (0.437) data 0.303 (0.307) loss_u loss_u 0.7959 (0.8017) acc_u 18.7500 (24.5312) lr 1.1101e-06 eta 0:00:05
epoch [199/200] batch [45/52] time 0.383 (0.435) data 0.252 (0.304) loss_u loss_u 0.8027 (0.8027) acc_u 28.1250 (24.2361) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [50/52] time 0.513 (0.436) data 0.382 (0.305) loss_u loss_u 0.9014 (0.8063) acc_u 9.3750 (23.8125) lr 1.1101e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1434
confident_label rate tensor(0.4522, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1418
clean true:1375
clean false:43
clean_rate:0.9696755994358252
noisy true:327
noisy false:1391
after delete: len(clean_dataset) 1418
after delete: len(noisy_dataset) 1718
epoch [200/200] batch [5/44] time 0.399 (0.422) data 0.269 (0.292) loss_x loss_x 0.8149 (1.0586) acc_x 81.2500 (73.1250) lr 4.9344e-07 eta 0:00:16
epoch [200/200] batch [10/44] time 0.470 (0.427) data 0.338 (0.297) loss_x loss_x 0.7915 (1.0129) acc_x 81.2500 (74.3750) lr 4.9344e-07 eta 0:00:14
epoch [200/200] batch [15/44] time 0.413 (0.444) data 0.280 (0.313) loss_x loss_x 0.7974 (1.0583) acc_x 78.1250 (72.0833) lr 4.9344e-07 eta 0:00:12
epoch [200/200] batch [20/44] time 0.496 (0.447) data 0.365 (0.315) loss_x loss_x 0.9307 (1.0755) acc_x 71.8750 (71.8750) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [25/44] time 0.415 (0.453) data 0.282 (0.322) loss_x loss_x 1.3965 (1.0706) acc_x 62.5000 (72.6250) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [30/44] time 0.593 (0.460) data 0.461 (0.328) loss_x loss_x 1.4297 (1.0927) acc_x 68.7500 (72.0833) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [35/44] time 0.487 (0.456) data 0.356 (0.325) loss_x loss_x 0.9814 (1.0989) acc_x 71.8750 (72.0536) lr 4.9344e-07 eta 0:00:04
epoch [200/200] batch [40/44] time 0.410 (0.454) data 0.280 (0.323) loss_x loss_x 1.4854 (1.0809) acc_x 65.6250 (72.8906) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [5/53] time 0.414 (0.456) data 0.283 (0.325) loss_u loss_u 0.8506 (0.8285) acc_u 15.6250 (21.2500) lr 4.9344e-07 eta 0:00:21
epoch [200/200] batch [10/53] time 0.561 (0.456) data 0.431 (0.325) loss_u loss_u 0.7710 (0.8277) acc_u 31.2500 (21.5625) lr 4.9344e-07 eta 0:00:19
epoch [200/200] batch [15/53] time 0.514 (0.451) data 0.382 (0.320) loss_u loss_u 0.8447 (0.8220) acc_u 12.5000 (21.2500) lr 4.9344e-07 eta 0:00:17
epoch [200/200] batch [20/53] time 0.497 (0.450) data 0.364 (0.318) loss_u loss_u 0.8750 (0.8276) acc_u 18.7500 (20.3125) lr 4.9344e-07 eta 0:00:14
epoch [200/200] batch [25/53] time 0.390 (0.449) data 0.259 (0.318) loss_u loss_u 0.8081 (0.8267) acc_u 21.8750 (20.5000) lr 4.9344e-07 eta 0:00:12
epoch [200/200] batch [30/53] time 0.475 (0.448) data 0.343 (0.317) loss_u loss_u 0.8765 (0.8206) acc_u 15.6250 (21.3542) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [35/53] time 0.428 (0.447) data 0.297 (0.316) loss_u loss_u 0.8247 (0.8150) acc_u 18.7500 (22.0536) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [40/53] time 0.425 (0.448) data 0.294 (0.316) loss_u loss_u 0.7070 (0.8106) acc_u 34.3750 (22.5000) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [45/53] time 0.396 (0.443) data 0.264 (0.312) loss_u loss_u 0.7905 (0.8080) acc_u 25.0000 (22.9167) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [50/53] time 0.487 (0.442) data 0.356 (0.311) loss_u loss_u 0.6499 (0.8073) acc_u 43.7500 (23.0625) lr 4.9344e-07 eta 0:00:01
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.25/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 5,345
* accuracy: 66.5%
* error: 33.5%
* macro_f1: 64.8%
Elapsed: 5:17:27
