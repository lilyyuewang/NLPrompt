***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NLPrompt/rn50.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.NOISE_RATE', '0.5', 'DATASET.NOISE_TYPE', 'asym', 'DATASET.num_class', '196']
output_dir: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.50/seed1
resume: 
root: ~/datasets/nlprompt
seed: 1
source_domains: None
target_domains: None
trainer: NLPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 0
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: StanfordCars
  NOISE_LABEL: True
  NOISE_RATE: 0.5
  NOISE_TYPE: asym
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: ~/datasets/nlprompt
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  USE_OT: True
  VAL_PERCENT: 0.1
  num_class: 196
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.50/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NLPrompt
  NLPROMPT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.4.0
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 24.04.2 LTS (x86_64)
GCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.39

Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A40
GPU 1: NVIDIA A40
GPU 2: NVIDIA A40
GPU 3: NVIDIA A40

Nvidia driver version: 575.64.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                            x86_64
CPU op-mode(s):                          32-bit, 64-bit
Address sizes:                           46 bits physical, 57 bits virtual
Byte Order:                              Little Endian
CPU(s):                                  64
On-line CPU(s) list:                     0-63
Vendor ID:                               GenuineIntel
Model name:                              Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz
CPU family:                              6
Model:                                   106
Thread(s) per core:                      2
Core(s) per socket:                      16
Socket(s):                               2
Stepping:                                6
BogoMIPS:                                4800.00
Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
Virtualization:                          VT-x
L1d cache:                               1.5 MiB (32 instances)
L1i cache:                               1 MiB (32 instances)
L2 cache:                                40 MiB (32 instances)
L3 cache:                                48 MiB (2 instances)
NUMA node(s):                            2
NUMA node0 CPU(s):                       0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):                       1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
Vulnerability Gather data sampling:      Vulnerable
Vulnerability Ghostwrite:                Not affected
Vulnerability Indirect target selection: Mitigation; Aligned branch/return thunks
Vulnerability Itlb multihit:             Not affected
Vulnerability L1tf:                      Not affected
Vulnerability Mds:                       Not affected
Vulnerability Meltdown:                  Not affected
Vulnerability Mmio stale data:           Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Reg file data sampling:    Not affected
Vulnerability Retbleed:                  Not affected
Vulnerability Spec rstack overflow:      Not affected
Vulnerability Spec store bypass:         Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:                Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:                Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                     Not affected
Vulnerability Tsx async abort:           Not affected

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.4.0
[pip3] torchaudio==2.4.0
[pip3] torchvision==0.19.0
[pip3] triton==3.0.0
[conda] blas                       1.0              mkl
[conda] libjpeg-turbo              2.0.0            h9bf148f_0                   pytorch
[conda] mkl                        2023.1.0         h213fc3f_46344
[conda] mkl-service                2.4.0            py38h5eee18b_1
[conda] mkl_fft                    1.3.8            py38h5eee18b_0
[conda] mkl_random                 1.2.4            py38hdb19cb5_0
[conda] numpy                      1.24.3           py38hf6e8229_1
[conda] numpy-base                 1.24.3           py38h060ed82_1
[conda] pytorch                    2.4.0            py3.8_cuda12.1_cudnn9.1.0_0  pytorch
[conda] pytorch-cuda               12.1             ha16c6d3_6                   pytorch
[conda] pytorch-mutex              1.0              cuda                         pytorch
[conda] torchaudio                 2.4.0            py38_cu121                   pytorch
[conda] torchtriton                3.0.0            py38                         pytorch
[conda] torchvision                0.19.0           py38_cu121                   pytorch
        Pillow (10.4.0)

Loading trainer: NLPrompt
Loading dataset: StanfordCars
Reading split from /home/convex/datasets/nlprompt/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /home/convex/datasets/nlprompt/stanford_cars/split_fewshot/shot_16-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
add noise 
Data loader size: 98
Data loader size: 8
Data loader size: 81
---------  ------------
Dataset    StanfordCars
# classes  196
# train_x  3,136
# val      784
# test     8,041
---------  ------------
Loading CLIP (backbone: RN50)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.50/seed1/tensorboard)
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2571
confident_label rate tensor(0.1110, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 348
clean true:297
clean false:51
clean_rate:0.853448275862069
noisy true:268
noisy false:2520
after delete: len(clean_dataset) 348
after delete: len(noisy_dataset) 2788
epoch [1/200] batch [5/10] time 0.498 (0.534) data 0.367 (0.382) loss_x loss_x 3.2305 (2.9805) acc_x 28.1250 (41.2500) lr 1.0000e-05 eta 0:00:02
epoch [1/200] batch [10/10] time 0.623 (0.529) data 0.494 (0.387) loss_x loss_x 2.4121 (2.7643) acc_x 53.1250 (46.5625) lr 1.0000e-05 eta 0:00:00
epoch [1/200] batch [5/87] time 0.582 (0.526) data 0.451 (0.387) loss_u loss_u 0.9712 (0.9649) acc_u 3.1250 (6.2500) lr 1.0000e-05 eta 0:00:43
epoch [1/200] batch [10/87] time 0.471 (0.512) data 0.339 (0.375) loss_u loss_u 0.9438 (0.9622) acc_u 12.5000 (7.8125) lr 1.0000e-05 eta 0:00:39
epoch [1/200] batch [15/87] time 0.402 (0.497) data 0.273 (0.362) loss_u loss_u 0.9683 (0.9633) acc_u 3.1250 (7.2917) lr 1.0000e-05 eta 0:00:35
epoch [1/200] batch [20/87] time 0.405 (0.484) data 0.275 (0.349) loss_u loss_u 0.9375 (0.9610) acc_u 3.1250 (6.5625) lr 1.0000e-05 eta 0:00:32
epoch [1/200] batch [25/87] time 0.375 (0.481) data 0.244 (0.347) loss_u loss_u 0.9595 (0.9595) acc_u 6.2500 (6.6250) lr 1.0000e-05 eta 0:00:29
epoch [1/200] batch [30/87] time 0.444 (0.477) data 0.313 (0.343) loss_u loss_u 0.9663 (0.9589) acc_u 3.1250 (6.7708) lr 1.0000e-05 eta 0:00:27
epoch [1/200] batch [35/87] time 0.356 (0.474) data 0.225 (0.341) loss_u loss_u 0.9492 (0.9566) acc_u 9.3750 (7.3214) lr 1.0000e-05 eta 0:00:24
epoch [1/200] batch [40/87] time 0.413 (0.472) data 0.282 (0.338) loss_u loss_u 0.9395 (0.9535) acc_u 9.3750 (7.8125) lr 1.0000e-05 eta 0:00:22
epoch [1/200] batch [45/87] time 0.397 (0.470) data 0.266 (0.337) loss_u loss_u 0.9224 (0.9523) acc_u 12.5000 (7.9167) lr 1.0000e-05 eta 0:00:19
epoch [1/200] batch [50/87] time 0.751 (0.473) data 0.621 (0.340) loss_u loss_u 0.9556 (0.9509) acc_u 3.1250 (8.1250) lr 1.0000e-05 eta 0:00:17
epoch [1/200] batch [55/87] time 0.591 (0.481) data 0.460 (0.349) loss_u loss_u 0.9463 (0.9502) acc_u 9.3750 (8.4091) lr 1.0000e-05 eta 0:00:15
epoch [1/200] batch [60/87] time 0.483 (0.482) data 0.352 (0.349) loss_u loss_u 0.9316 (0.9495) acc_u 9.3750 (8.4375) lr 1.0000e-05 eta 0:00:13
epoch [1/200] batch [65/87] time 0.373 (0.476) data 0.243 (0.343) loss_u loss_u 0.9404 (0.9475) acc_u 15.6250 (8.8942) lr 1.0000e-05 eta 0:00:10
epoch [1/200] batch [70/87] time 0.410 (0.475) data 0.281 (0.343) loss_u loss_u 0.9380 (0.9480) acc_u 6.2500 (8.7500) lr 1.0000e-05 eta 0:00:08
epoch [1/200] batch [75/87] time 0.356 (0.474) data 0.223 (0.341) loss_u loss_u 0.9150 (0.9474) acc_u 18.7500 (8.8333) lr 1.0000e-05 eta 0:00:05
epoch [1/200] batch [80/87] time 0.353 (0.475) data 0.223 (0.343) loss_u loss_u 0.9248 (0.9458) acc_u 15.6250 (9.1016) lr 1.0000e-05 eta 0:00:03
epoch [1/200] batch [85/87] time 0.731 (0.479) data 0.600 (0.347) loss_u loss_u 0.9585 (0.9457) acc_u 6.2500 (9.1544) lr 1.0000e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 2236
confident_label rate tensor(0.1623, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 509
clean true:466
clean false:43
clean_rate:0.9155206286836935
noisy true:434
noisy false:2193
after delete: len(clean_dataset) 509
after delete: len(noisy_dataset) 2627
epoch [2/200] batch [5/15] time 0.458 (0.490) data 0.328 (0.359) loss_x loss_x 1.5049 (1.6746) acc_x 65.6250 (61.8750) lr 2.0000e-03 eta 0:00:04
epoch [2/200] batch [10/15] time 0.421 (0.466) data 0.291 (0.336) loss_x loss_x 1.6504 (1.7207) acc_x 56.2500 (56.2500) lr 2.0000e-03 eta 0:00:02
epoch [2/200] batch [15/15] time 0.558 (0.484) data 0.426 (0.353) loss_x loss_x 1.5654 (1.7381) acc_x 43.7500 (53.9583) lr 2.0000e-03 eta 0:00:00
epoch [2/200] batch [5/82] time 0.475 (0.474) data 0.344 (0.343) loss_u loss_u 0.8784 (0.8880) acc_u 9.3750 (14.3750) lr 2.0000e-03 eta 0:00:36
epoch [2/200] batch [10/82] time 0.371 (0.457) data 0.239 (0.326) loss_u loss_u 0.9243 (0.9023) acc_u 6.2500 (10.6250) lr 2.0000e-03 eta 0:00:32
epoch [2/200] batch [15/82] time 0.375 (0.451) data 0.244 (0.320) loss_u loss_u 0.8726 (0.8898) acc_u 15.6250 (13.5417) lr 2.0000e-03 eta 0:00:30
epoch [2/200] batch [20/82] time 0.485 (0.451) data 0.352 (0.320) loss_u loss_u 0.8857 (0.8877) acc_u 18.7500 (14.0625) lr 2.0000e-03 eta 0:00:27
epoch [2/200] batch [25/82] time 0.366 (0.449) data 0.234 (0.318) loss_u loss_u 0.8276 (0.8834) acc_u 25.0000 (14.8750) lr 2.0000e-03 eta 0:00:25
epoch [2/200] batch [30/82] time 0.485 (0.451) data 0.353 (0.320) loss_u loss_u 0.8428 (0.8824) acc_u 18.7500 (15.3125) lr 2.0000e-03 eta 0:00:23
epoch [2/200] batch [35/82] time 0.412 (0.453) data 0.280 (0.322) loss_u loss_u 0.9155 (0.8852) acc_u 9.3750 (14.6429) lr 2.0000e-03 eta 0:00:21
epoch [2/200] batch [40/82] time 0.368 (0.450) data 0.237 (0.319) loss_u loss_u 0.8770 (0.8880) acc_u 12.5000 (14.1406) lr 2.0000e-03 eta 0:00:18
epoch [2/200] batch [45/82] time 0.878 (0.456) data 0.748 (0.325) loss_u loss_u 0.9004 (0.8859) acc_u 9.3750 (14.4444) lr 2.0000e-03 eta 0:00:16
epoch [2/200] batch [50/82] time 0.512 (0.456) data 0.383 (0.325) loss_u loss_u 0.8901 (0.8864) acc_u 12.5000 (14.3750) lr 2.0000e-03 eta 0:00:14
epoch [2/200] batch [55/82] time 0.457 (0.455) data 0.326 (0.323) loss_u loss_u 0.8535 (0.8846) acc_u 18.7500 (14.9432) lr 2.0000e-03 eta 0:00:12
epoch [2/200] batch [60/82] time 0.541 (0.451) data 0.410 (0.320) loss_u loss_u 0.8955 (0.8835) acc_u 18.7500 (15.0521) lr 2.0000e-03 eta 0:00:09
epoch [2/200] batch [65/82] time 0.429 (0.454) data 0.298 (0.323) loss_u loss_u 0.9204 (0.8828) acc_u 9.3750 (15.0962) lr 2.0000e-03 eta 0:00:07
epoch [2/200] batch [70/82] time 0.444 (0.450) data 0.314 (0.319) loss_u loss_u 0.9199 (0.8827) acc_u 6.2500 (14.8661) lr 2.0000e-03 eta 0:00:05
epoch [2/200] batch [75/82] time 0.395 (0.450) data 0.264 (0.319) loss_u loss_u 0.8535 (0.8833) acc_u 15.6250 (14.7500) lr 2.0000e-03 eta 0:00:03
epoch [2/200] batch [80/82] time 0.464 (0.447) data 0.334 (0.316) loss_u loss_u 0.7603 (0.8832) acc_u 34.3750 (14.7266) lr 2.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1877
confident_label rate tensor(0.2239, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 702
clean true:635
clean false:67
clean_rate:0.9045584045584045
noisy true:624
noisy false:1810
after delete: len(clean_dataset) 702
after delete: len(noisy_dataset) 2434
epoch [3/200] batch [5/21] time 0.584 (0.530) data 0.451 (0.399) loss_x loss_x 1.7910 (1.4693) acc_x 53.1250 (58.7500) lr 1.9999e-03 eta 0:00:08
epoch [3/200] batch [10/21] time 0.403 (0.474) data 0.272 (0.343) loss_x loss_x 1.8340 (1.6886) acc_x 43.7500 (54.0625) lr 1.9999e-03 eta 0:00:05
epoch [3/200] batch [15/21] time 0.444 (0.484) data 0.314 (0.353) loss_x loss_x 1.7979 (1.6473) acc_x 56.2500 (54.7917) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [20/21] time 0.614 (0.485) data 0.483 (0.354) loss_x loss_x 1.5820 (1.6273) acc_x 56.2500 (55.3125) lr 1.9999e-03 eta 0:00:00
epoch [3/200] batch [5/76] time 0.342 (0.476) data 0.210 (0.344) loss_u loss_u 0.8848 (0.8904) acc_u 12.5000 (15.6250) lr 1.9999e-03 eta 0:00:33
epoch [3/200] batch [10/76] time 0.416 (0.472) data 0.285 (0.340) loss_u loss_u 0.9429 (0.9036) acc_u 9.3750 (12.8125) lr 1.9999e-03 eta 0:00:31
epoch [3/200] batch [15/76] time 0.461 (0.467) data 0.329 (0.336) loss_u loss_u 0.9219 (0.9002) acc_u 9.3750 (12.9167) lr 1.9999e-03 eta 0:00:28
epoch [3/200] batch [20/76] time 0.430 (0.471) data 0.299 (0.339) loss_u loss_u 0.9272 (0.9010) acc_u 9.3750 (12.5000) lr 1.9999e-03 eta 0:00:26
epoch [3/200] batch [25/76] time 0.348 (0.469) data 0.215 (0.338) loss_u loss_u 0.9370 (0.9030) acc_u 6.2500 (12.1250) lr 1.9999e-03 eta 0:00:23
epoch [3/200] batch [30/76] time 0.483 (0.470) data 0.352 (0.339) loss_u loss_u 0.9180 (0.9063) acc_u 9.3750 (11.6667) lr 1.9999e-03 eta 0:00:21
epoch [3/200] batch [35/76] time 0.336 (0.465) data 0.206 (0.333) loss_u loss_u 0.7998 (0.9032) acc_u 21.8750 (12.0536) lr 1.9999e-03 eta 0:00:19
epoch [3/200] batch [40/76] time 0.406 (0.462) data 0.273 (0.331) loss_u loss_u 0.9224 (0.9033) acc_u 3.1250 (11.7188) lr 1.9999e-03 eta 0:00:16
epoch [3/200] batch [45/76] time 0.492 (0.458) data 0.361 (0.326) loss_u loss_u 0.9507 (0.9029) acc_u 3.1250 (11.8750) lr 1.9999e-03 eta 0:00:14
epoch [3/200] batch [50/76] time 0.582 (0.456) data 0.450 (0.325) loss_u loss_u 0.8940 (0.9018) acc_u 18.7500 (12.1875) lr 1.9999e-03 eta 0:00:11
epoch [3/200] batch [55/76] time 0.408 (0.454) data 0.277 (0.323) loss_u loss_u 0.8921 (0.9017) acc_u 9.3750 (12.0455) lr 1.9999e-03 eta 0:00:09
epoch [3/200] batch [60/76] time 0.481 (0.453) data 0.350 (0.322) loss_u loss_u 0.9023 (0.9017) acc_u 12.5000 (12.1875) lr 1.9999e-03 eta 0:00:07
epoch [3/200] batch [65/76] time 0.356 (0.449) data 0.224 (0.318) loss_u loss_u 0.9106 (0.9023) acc_u 6.2500 (12.1635) lr 1.9999e-03 eta 0:00:04
epoch [3/200] batch [70/76] time 0.383 (0.452) data 0.251 (0.321) loss_u loss_u 0.8721 (0.9020) acc_u 15.6250 (12.0536) lr 1.9999e-03 eta 0:00:02
epoch [3/200] batch [75/76] time 0.369 (0.450) data 0.237 (0.319) loss_u loss_u 0.9199 (0.9012) acc_u 12.5000 (12.0833) lr 1.9999e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1840
confident_label rate tensor(0.2321, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 728
clean true:669
clean false:59
clean_rate:0.9189560439560439
noisy true:627
noisy false:1781
after delete: len(clean_dataset) 728
after delete: len(noisy_dataset) 2408
epoch [4/200] batch [5/22] time 0.463 (0.498) data 0.332 (0.367) loss_x loss_x 1.8818 (1.6211) acc_x 65.6250 (57.5000) lr 1.9995e-03 eta 0:00:08
epoch [4/200] batch [10/22] time 0.495 (0.465) data 0.364 (0.334) loss_x loss_x 1.6328 (1.6413) acc_x 53.1250 (55.6250) lr 1.9995e-03 eta 0:00:05
epoch [4/200] batch [15/22] time 0.548 (0.460) data 0.417 (0.329) loss_x loss_x 1.1338 (1.6243) acc_x 65.6250 (56.6667) lr 1.9995e-03 eta 0:00:03
epoch [4/200] batch [20/22] time 0.517 (0.467) data 0.387 (0.336) loss_x loss_x 1.4297 (1.6699) acc_x 78.1250 (57.0312) lr 1.9995e-03 eta 0:00:00
epoch [4/200] batch [5/75] time 0.434 (0.457) data 0.301 (0.326) loss_u loss_u 0.9321 (0.8928) acc_u 12.5000 (17.5000) lr 1.9995e-03 eta 0:00:31
epoch [4/200] batch [10/75] time 0.484 (0.460) data 0.352 (0.329) loss_u loss_u 0.9038 (0.8947) acc_u 9.3750 (14.6875) lr 1.9995e-03 eta 0:00:29
epoch [4/200] batch [15/75] time 0.477 (0.461) data 0.345 (0.330) loss_u loss_u 0.8809 (0.8972) acc_u 15.6250 (13.5417) lr 1.9995e-03 eta 0:00:27
epoch [4/200] batch [20/75] time 0.693 (0.467) data 0.563 (0.335) loss_u loss_u 0.8750 (0.8911) acc_u 18.7500 (14.5312) lr 1.9995e-03 eta 0:00:25
epoch [4/200] batch [25/75] time 0.416 (0.457) data 0.284 (0.326) loss_u loss_u 0.9102 (0.8931) acc_u 12.5000 (13.7500) lr 1.9995e-03 eta 0:00:22
epoch [4/200] batch [30/75] time 0.392 (0.462) data 0.261 (0.330) loss_u loss_u 0.9478 (0.8954) acc_u 3.1250 (13.2292) lr 1.9995e-03 eta 0:00:20
epoch [4/200] batch [35/75] time 0.591 (0.461) data 0.460 (0.329) loss_u loss_u 0.8691 (0.8985) acc_u 15.6250 (12.6786) lr 1.9995e-03 eta 0:00:18
epoch [4/200] batch [40/75] time 0.459 (0.456) data 0.327 (0.324) loss_u loss_u 0.8950 (0.8970) acc_u 12.5000 (12.7344) lr 1.9995e-03 eta 0:00:15
epoch [4/200] batch [45/75] time 0.430 (0.453) data 0.298 (0.322) loss_u loss_u 0.9087 (0.8983) acc_u 9.3750 (12.7778) lr 1.9995e-03 eta 0:00:13
epoch [4/200] batch [50/75] time 0.484 (0.456) data 0.351 (0.324) loss_u loss_u 0.9165 (0.8991) acc_u 12.5000 (12.6875) lr 1.9995e-03 eta 0:00:11
epoch [4/200] batch [55/75] time 0.357 (0.452) data 0.226 (0.321) loss_u loss_u 0.9487 (0.9006) acc_u 3.1250 (12.5000) lr 1.9995e-03 eta 0:00:09
epoch [4/200] batch [60/75] time 0.443 (0.452) data 0.311 (0.320) loss_u loss_u 0.9053 (0.9005) acc_u 9.3750 (12.3958) lr 1.9995e-03 eta 0:00:06
epoch [4/200] batch [65/75] time 0.408 (0.449) data 0.276 (0.318) loss_u loss_u 0.9492 (0.9019) acc_u 6.2500 (12.1154) lr 1.9995e-03 eta 0:00:04
epoch [4/200] batch [70/75] time 0.328 (0.446) data 0.196 (0.315) loss_u loss_u 0.9116 (0.9018) acc_u 12.5000 (12.1429) lr 1.9995e-03 eta 0:00:02
epoch [4/200] batch [75/75] time 0.447 (0.448) data 0.317 (0.317) loss_u loss_u 0.8628 (0.9010) acc_u 12.5000 (12.0000) lr 1.9995e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1875
confident_label rate tensor(0.2321, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 728
clean true:657
clean false:71
clean_rate:0.9024725274725275
noisy true:604
noisy false:1804
after delete: len(clean_dataset) 728
after delete: len(noisy_dataset) 2408
epoch [5/200] batch [5/22] time 0.449 (0.463) data 0.318 (0.332) loss_x loss_x 1.7168 (1.6162) acc_x 46.8750 (57.5000) lr 1.9989e-03 eta 0:00:07
epoch [5/200] batch [10/22] time 0.452 (0.468) data 0.321 (0.338) loss_x loss_x 1.7139 (1.6393) acc_x 62.5000 (58.1250) lr 1.9989e-03 eta 0:00:05
epoch [5/200] batch [15/22] time 0.478 (0.469) data 0.347 (0.338) loss_x loss_x 1.3066 (1.5673) acc_x 65.6250 (58.9583) lr 1.9989e-03 eta 0:00:03
epoch [5/200] batch [20/22] time 0.557 (0.482) data 0.426 (0.351) loss_x loss_x 1.8828 (1.6057) acc_x 56.2500 (58.5938) lr 1.9989e-03 eta 0:00:00
epoch [5/200] batch [5/75] time 0.460 (0.470) data 0.328 (0.339) loss_u loss_u 0.9263 (0.8873) acc_u 6.2500 (13.1250) lr 1.9989e-03 eta 0:00:32
epoch [5/200] batch [10/75] time 0.486 (0.463) data 0.354 (0.332) loss_u loss_u 0.9121 (0.8912) acc_u 12.5000 (12.5000) lr 1.9989e-03 eta 0:00:30
epoch [5/200] batch [15/75] time 0.359 (0.454) data 0.228 (0.323) loss_u loss_u 0.8438 (0.8882) acc_u 15.6250 (13.7500) lr 1.9989e-03 eta 0:00:27
epoch [5/200] batch [20/75] time 0.405 (0.456) data 0.274 (0.325) loss_u loss_u 0.9351 (0.8938) acc_u 12.5000 (12.9688) lr 1.9989e-03 eta 0:00:25
epoch [5/200] batch [25/75] time 0.435 (0.461) data 0.304 (0.330) loss_u loss_u 0.9355 (0.8966) acc_u 9.3750 (12.6250) lr 1.9989e-03 eta 0:00:23
epoch [5/200] batch [30/75] time 0.321 (0.455) data 0.190 (0.324) loss_u loss_u 0.8975 (0.9010) acc_u 15.6250 (11.8750) lr 1.9989e-03 eta 0:00:20
epoch [5/200] batch [35/75] time 0.326 (0.453) data 0.195 (0.322) loss_u loss_u 0.8843 (0.8991) acc_u 12.5000 (12.0536) lr 1.9989e-03 eta 0:00:18
epoch [5/200] batch [40/75] time 0.519 (0.455) data 0.387 (0.323) loss_u loss_u 0.9229 (0.8995) acc_u 6.2500 (11.9531) lr 1.9989e-03 eta 0:00:15
epoch [5/200] batch [45/75] time 0.450 (0.455) data 0.319 (0.324) loss_u loss_u 0.8882 (0.8984) acc_u 15.6250 (12.2917) lr 1.9989e-03 eta 0:00:13
epoch [5/200] batch [50/75] time 0.374 (0.451) data 0.243 (0.320) loss_u loss_u 0.9155 (0.8957) acc_u 12.5000 (12.8750) lr 1.9989e-03 eta 0:00:11
epoch [5/200] batch [55/75] time 0.376 (0.446) data 0.245 (0.315) loss_u loss_u 0.8906 (0.8950) acc_u 6.2500 (12.8977) lr 1.9989e-03 eta 0:00:08
epoch [5/200] batch [60/75] time 0.384 (0.444) data 0.253 (0.313) loss_u loss_u 0.9312 (0.8960) acc_u 3.1250 (12.7604) lr 1.9989e-03 eta 0:00:06
epoch [5/200] batch [65/75] time 0.432 (0.445) data 0.302 (0.314) loss_u loss_u 0.9316 (0.8948) acc_u 6.2500 (12.7885) lr 1.9989e-03 eta 0:00:04
epoch [5/200] batch [70/75] time 0.407 (0.448) data 0.276 (0.317) loss_u loss_u 0.9595 (0.8960) acc_u 6.2500 (12.5446) lr 1.9989e-03 eta 0:00:02
epoch [5/200] batch [75/75] time 0.462 (0.447) data 0.331 (0.316) loss_u loss_u 0.9473 (0.8971) acc_u 6.2500 (12.3750) lr 1.9989e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1826
confident_label rate tensor(0.2420, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 759
clean true:698
clean false:61
clean_rate:0.919631093544137
noisy true:612
noisy false:1765
after delete: len(clean_dataset) 759
after delete: len(noisy_dataset) 2377
epoch [6/200] batch [5/23] time 0.514 (0.510) data 0.384 (0.379) loss_x loss_x 1.7217 (1.5766) acc_x 53.1250 (58.1250) lr 1.9980e-03 eta 0:00:09
epoch [6/200] batch [10/23] time 0.505 (0.495) data 0.374 (0.364) loss_x loss_x 2.1504 (1.6674) acc_x 37.5000 (55.9375) lr 1.9980e-03 eta 0:00:06
epoch [6/200] batch [15/23] time 0.403 (0.489) data 0.273 (0.358) loss_x loss_x 1.9492 (1.6426) acc_x 40.6250 (56.2500) lr 1.9980e-03 eta 0:00:03
epoch [6/200] batch [20/23] time 0.467 (0.478) data 0.336 (0.348) loss_x loss_x 1.5498 (1.6593) acc_x 59.3750 (55.0000) lr 1.9980e-03 eta 0:00:01
epoch [6/200] batch [5/74] time 0.399 (0.466) data 0.268 (0.335) loss_u loss_u 0.8662 (0.8863) acc_u 18.7500 (13.7500) lr 1.9980e-03 eta 0:00:32
epoch [6/200] batch [10/74] time 0.489 (0.464) data 0.357 (0.333) loss_u loss_u 0.9478 (0.9046) acc_u 3.1250 (10.6250) lr 1.9980e-03 eta 0:00:29
epoch [6/200] batch [15/74] time 0.459 (0.457) data 0.327 (0.326) loss_u loss_u 0.8965 (0.9076) acc_u 15.6250 (10.8333) lr 1.9980e-03 eta 0:00:26
epoch [6/200] batch [20/74] time 0.383 (0.451) data 0.251 (0.320) loss_u loss_u 0.8809 (0.9026) acc_u 12.5000 (11.7188) lr 1.9980e-03 eta 0:00:24
epoch [6/200] batch [25/74] time 0.335 (0.444) data 0.203 (0.313) loss_u loss_u 0.8530 (0.8993) acc_u 25.0000 (12.3750) lr 1.9980e-03 eta 0:00:21
epoch [6/200] batch [30/74] time 0.519 (0.448) data 0.386 (0.317) loss_u loss_u 0.8872 (0.8985) acc_u 12.5000 (12.3958) lr 1.9980e-03 eta 0:00:19
epoch [6/200] batch [35/74] time 0.555 (0.455) data 0.423 (0.324) loss_u loss_u 0.8447 (0.9004) acc_u 21.8750 (12.0536) lr 1.9980e-03 eta 0:00:17
epoch [6/200] batch [40/74] time 0.420 (0.450) data 0.290 (0.319) loss_u loss_u 0.9082 (0.8969) acc_u 12.5000 (12.6562) lr 1.9980e-03 eta 0:00:15
epoch [6/200] batch [45/74] time 0.468 (0.453) data 0.337 (0.322) loss_u loss_u 0.8784 (0.8975) acc_u 18.7500 (12.4306) lr 1.9980e-03 eta 0:00:13
epoch [6/200] batch [50/74] time 0.414 (0.454) data 0.282 (0.323) loss_u loss_u 0.8965 (0.8992) acc_u 12.5000 (12.3125) lr 1.9980e-03 eta 0:00:10
epoch [6/200] batch [55/74] time 0.487 (0.450) data 0.356 (0.319) loss_u loss_u 0.9053 (0.8983) acc_u 6.2500 (12.3864) lr 1.9980e-03 eta 0:00:08
epoch [6/200] batch [60/74] time 0.410 (0.453) data 0.278 (0.322) loss_u loss_u 0.9048 (0.8986) acc_u 9.3750 (12.2917) lr 1.9980e-03 eta 0:00:06
epoch [6/200] batch [65/74] time 0.444 (0.453) data 0.312 (0.322) loss_u loss_u 0.8799 (0.8980) acc_u 12.5000 (12.3558) lr 1.9980e-03 eta 0:00:04
epoch [6/200] batch [70/74] time 0.430 (0.453) data 0.299 (0.321) loss_u loss_u 0.9414 (0.8975) acc_u 6.2500 (12.3661) lr 1.9980e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1831
confident_label rate tensor(0.2315, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 726
clean true:661
clean false:65
clean_rate:0.9104683195592287
noisy true:644
noisy false:1766
after delete: len(clean_dataset) 726
after delete: len(noisy_dataset) 2410
epoch [7/200] batch [5/22] time 0.413 (0.460) data 0.283 (0.329) loss_x loss_x 2.2910 (1.5869) acc_x 56.2500 (60.6250) lr 1.9969e-03 eta 0:00:07
epoch [7/200] batch [10/22] time 0.455 (0.451) data 0.325 (0.320) loss_x loss_x 1.3291 (1.5393) acc_x 75.0000 (60.3125) lr 1.9969e-03 eta 0:00:05
epoch [7/200] batch [15/22] time 0.525 (0.438) data 0.395 (0.308) loss_x loss_x 1.4258 (1.5766) acc_x 53.1250 (58.9583) lr 1.9969e-03 eta 0:00:03
epoch [7/200] batch [20/22] time 0.409 (0.445) data 0.279 (0.314) loss_x loss_x 1.5703 (1.5827) acc_x 53.1250 (57.6562) lr 1.9969e-03 eta 0:00:00
epoch [7/200] batch [5/75] time 0.512 (0.440) data 0.382 (0.310) loss_u loss_u 0.8979 (0.8966) acc_u 9.3750 (12.5000) lr 1.9969e-03 eta 0:00:30
epoch [7/200] batch [10/75] time 0.358 (0.438) data 0.226 (0.307) loss_u loss_u 0.8711 (0.8977) acc_u 15.6250 (12.5000) lr 1.9969e-03 eta 0:00:28
epoch [7/200] batch [15/75] time 0.439 (0.436) data 0.307 (0.305) loss_u loss_u 0.8911 (0.8842) acc_u 9.3750 (14.1667) lr 1.9969e-03 eta 0:00:26
epoch [7/200] batch [20/75] time 0.388 (0.434) data 0.257 (0.303) loss_u loss_u 0.9136 (0.8886) acc_u 6.2500 (13.1250) lr 1.9969e-03 eta 0:00:23
epoch [7/200] batch [25/75] time 0.516 (0.434) data 0.384 (0.302) loss_u loss_u 0.9263 (0.8944) acc_u 12.5000 (13.1250) lr 1.9969e-03 eta 0:00:21
epoch [7/200] batch [30/75] time 0.389 (0.435) data 0.258 (0.303) loss_u loss_u 0.9360 (0.8964) acc_u 9.3750 (12.3958) lr 1.9969e-03 eta 0:00:19
epoch [7/200] batch [35/75] time 0.439 (0.432) data 0.307 (0.301) loss_u loss_u 0.8906 (0.8929) acc_u 12.5000 (12.9464) lr 1.9969e-03 eta 0:00:17
epoch [7/200] batch [40/75] time 0.483 (0.431) data 0.352 (0.300) loss_u loss_u 0.8794 (0.8936) acc_u 15.6250 (12.6562) lr 1.9969e-03 eta 0:00:15
epoch [7/200] batch [45/75] time 0.447 (0.435) data 0.316 (0.303) loss_u loss_u 0.8687 (0.8921) acc_u 15.6250 (12.9167) lr 1.9969e-03 eta 0:00:13
epoch [7/200] batch [50/75] time 0.485 (0.437) data 0.353 (0.305) loss_u loss_u 0.9609 (0.8953) acc_u 6.2500 (12.5625) lr 1.9969e-03 eta 0:00:10
epoch [7/200] batch [55/75] time 0.555 (0.444) data 0.421 (0.312) loss_u loss_u 0.9302 (0.8962) acc_u 9.3750 (12.5000) lr 1.9969e-03 eta 0:00:08
epoch [7/200] batch [60/75] time 0.351 (0.442) data 0.219 (0.310) loss_u loss_u 0.8975 (0.8950) acc_u 9.3750 (12.7083) lr 1.9969e-03 eta 0:00:06
epoch [7/200] batch [65/75] time 0.536 (0.444) data 0.404 (0.312) loss_u loss_u 0.8306 (0.8940) acc_u 15.6250 (12.6923) lr 1.9969e-03 eta 0:00:04
epoch [7/200] batch [70/75] time 0.546 (0.445) data 0.415 (0.314) loss_u loss_u 0.8701 (0.8931) acc_u 18.7500 (12.9464) lr 1.9969e-03 eta 0:00:02
epoch [7/200] batch [75/75] time 0.609 (0.448) data 0.474 (0.316) loss_u loss_u 0.8730 (0.8952) acc_u 18.7500 (12.7083) lr 1.9969e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1808
confident_label rate tensor(0.2363, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 741
clean true:678
clean false:63
clean_rate:0.9149797570850202
noisy true:650
noisy false:1745
after delete: len(clean_dataset) 741
after delete: len(noisy_dataset) 2395
epoch [8/200] batch [5/23] time 0.449 (0.536) data 0.319 (0.405) loss_x loss_x 1.6699 (1.6461) acc_x 53.1250 (56.8750) lr 1.9956e-03 eta 0:00:09
epoch [8/200] batch [10/23] time 0.407 (0.487) data 0.277 (0.357) loss_x loss_x 1.4131 (1.7046) acc_x 71.8750 (55.9375) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [15/23] time 0.487 (0.481) data 0.357 (0.350) loss_x loss_x 1.4414 (1.6803) acc_x 59.3750 (56.8750) lr 1.9956e-03 eta 0:00:03
epoch [8/200] batch [20/23] time 0.464 (0.482) data 0.332 (0.351) loss_x loss_x 1.2783 (1.6182) acc_x 59.3750 (58.2812) lr 1.9956e-03 eta 0:00:01
epoch [8/200] batch [5/74] time 0.453 (0.480) data 0.321 (0.349) loss_u loss_u 0.8418 (0.8778) acc_u 21.8750 (13.7500) lr 1.9956e-03 eta 0:00:33
epoch [8/200] batch [10/74] time 0.550 (0.474) data 0.418 (0.343) loss_u loss_u 0.9390 (0.8908) acc_u 6.2500 (12.5000) lr 1.9956e-03 eta 0:00:30
epoch [8/200] batch [15/74] time 0.406 (0.464) data 0.274 (0.333) loss_u loss_u 0.8555 (0.8961) acc_u 15.6250 (12.2917) lr 1.9956e-03 eta 0:00:27
epoch [8/200] batch [20/74] time 0.525 (0.462) data 0.393 (0.331) loss_u loss_u 0.9209 (0.8971) acc_u 15.6250 (12.3438) lr 1.9956e-03 eta 0:00:24
epoch [8/200] batch [25/74] time 0.333 (0.461) data 0.202 (0.330) loss_u loss_u 0.8921 (0.8962) acc_u 15.6250 (12.6250) lr 1.9956e-03 eta 0:00:22
epoch [8/200] batch [30/74] time 0.357 (0.460) data 0.225 (0.329) loss_u loss_u 0.9160 (0.8983) acc_u 6.2500 (12.0833) lr 1.9956e-03 eta 0:00:20
epoch [8/200] batch [35/74] time 0.379 (0.458) data 0.247 (0.327) loss_u loss_u 0.8774 (0.8981) acc_u 15.6250 (12.4107) lr 1.9956e-03 eta 0:00:17
epoch [8/200] batch [40/74] time 0.439 (0.455) data 0.308 (0.324) loss_u loss_u 0.8882 (0.9003) acc_u 12.5000 (11.7188) lr 1.9956e-03 eta 0:00:15
epoch [8/200] batch [45/74] time 0.334 (0.449) data 0.204 (0.318) loss_u loss_u 0.9277 (0.9014) acc_u 9.3750 (11.8750) lr 1.9956e-03 eta 0:00:13
epoch [8/200] batch [50/74] time 0.458 (0.447) data 0.326 (0.316) loss_u loss_u 0.9253 (0.9031) acc_u 0.0000 (11.7500) lr 1.9956e-03 eta 0:00:10
epoch [8/200] batch [55/74] time 0.442 (0.449) data 0.311 (0.318) loss_u loss_u 0.9146 (0.9029) acc_u 9.3750 (11.8750) lr 1.9956e-03 eta 0:00:08
epoch [8/200] batch [60/74] time 0.425 (0.447) data 0.293 (0.316) loss_u loss_u 0.8389 (0.9016) acc_u 25.0000 (12.0833) lr 1.9956e-03 eta 0:00:06
epoch [8/200] batch [65/74] time 0.408 (0.448) data 0.277 (0.316) loss_u loss_u 0.8965 (0.9007) acc_u 12.5000 (12.1635) lr 1.9956e-03 eta 0:00:04
epoch [8/200] batch [70/74] time 0.580 (0.448) data 0.449 (0.316) loss_u loss_u 0.9595 (0.9010) acc_u 3.1250 (11.9196) lr 1.9956e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1808
confident_label rate tensor(0.2404, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 754
clean true:685
clean false:69
clean_rate:0.9084880636604774
noisy true:643
noisy false:1739
after delete: len(clean_dataset) 754
after delete: len(noisy_dataset) 2382
epoch [9/200] batch [5/23] time 0.359 (0.439) data 0.229 (0.309) loss_x loss_x 1.6143 (1.5420) acc_x 56.2500 (56.8750) lr 1.9940e-03 eta 0:00:07
epoch [9/200] batch [10/23] time 0.506 (0.473) data 0.374 (0.342) loss_x loss_x 1.3076 (1.5229) acc_x 59.3750 (57.5000) lr 1.9940e-03 eta 0:00:06
epoch [9/200] batch [15/23] time 0.502 (0.462) data 0.371 (0.331) loss_x loss_x 1.5459 (1.5341) acc_x 53.1250 (58.1250) lr 1.9940e-03 eta 0:00:03
epoch [9/200] batch [20/23] time 0.427 (0.461) data 0.297 (0.330) loss_x loss_x 1.8750 (1.5393) acc_x 40.6250 (57.5000) lr 1.9940e-03 eta 0:00:01
epoch [9/200] batch [5/74] time 0.427 (0.456) data 0.295 (0.325) loss_u loss_u 0.8740 (0.8891) acc_u 15.6250 (14.3750) lr 1.9940e-03 eta 0:00:31
epoch [9/200] batch [10/74] time 0.542 (0.466) data 0.410 (0.335) loss_u loss_u 0.8354 (0.8927) acc_u 21.8750 (14.0625) lr 1.9940e-03 eta 0:00:29
epoch [9/200] batch [15/74] time 0.376 (0.463) data 0.245 (0.332) loss_u loss_u 0.8721 (0.8916) acc_u 15.6250 (14.3750) lr 1.9940e-03 eta 0:00:27
epoch [9/200] batch [20/74] time 0.414 (0.459) data 0.283 (0.328) loss_u loss_u 0.9297 (0.8924) acc_u 6.2500 (13.7500) lr 1.9940e-03 eta 0:00:24
epoch [9/200] batch [25/74] time 0.418 (0.463) data 0.287 (0.332) loss_u loss_u 0.9111 (0.8964) acc_u 9.3750 (13.1250) lr 1.9940e-03 eta 0:00:22
epoch [9/200] batch [30/74] time 0.446 (0.458) data 0.315 (0.327) loss_u loss_u 0.8281 (0.8951) acc_u 25.0000 (13.0208) lr 1.9940e-03 eta 0:00:20
epoch [9/200] batch [35/74] time 0.448 (0.461) data 0.316 (0.330) loss_u loss_u 0.9189 (0.8952) acc_u 9.3750 (13.0357) lr 1.9940e-03 eta 0:00:17
epoch [9/200] batch [40/74] time 0.403 (0.457) data 0.271 (0.326) loss_u loss_u 0.9072 (0.8964) acc_u 15.6250 (12.9688) lr 1.9940e-03 eta 0:00:15
epoch [9/200] batch [45/74] time 0.429 (0.458) data 0.296 (0.326) loss_u loss_u 0.8911 (0.8952) acc_u 15.6250 (13.2639) lr 1.9940e-03 eta 0:00:13
epoch [9/200] batch [50/74] time 0.463 (0.456) data 0.332 (0.325) loss_u loss_u 0.9141 (0.8952) acc_u 9.3750 (13.3750) lr 1.9940e-03 eta 0:00:10
epoch [9/200] batch [55/74] time 0.335 (0.455) data 0.204 (0.324) loss_u loss_u 0.8765 (0.8938) acc_u 18.7500 (13.4659) lr 1.9940e-03 eta 0:00:08
epoch [9/200] batch [60/74] time 0.800 (0.459) data 0.668 (0.327) loss_u loss_u 0.8960 (0.8948) acc_u 9.3750 (13.2292) lr 1.9940e-03 eta 0:00:06
epoch [9/200] batch [65/74] time 0.453 (0.460) data 0.321 (0.328) loss_u loss_u 0.8066 (0.8923) acc_u 18.7500 (13.4135) lr 1.9940e-03 eta 0:00:04
epoch [9/200] batch [70/74] time 0.416 (0.458) data 0.283 (0.327) loss_u loss_u 0.8867 (0.8931) acc_u 6.2500 (13.1250) lr 1.9940e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1775
confident_label rate tensor(0.2439, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 765
clean true:713
clean false:52
clean_rate:0.9320261437908497
noisy true:648
noisy false:1723
after delete: len(clean_dataset) 765
after delete: len(noisy_dataset) 2371
epoch [10/200] batch [5/23] time 0.545 (0.471) data 0.415 (0.340) loss_x loss_x 1.3574 (1.4773) acc_x 62.5000 (58.7500) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [10/23] time 0.440 (0.464) data 0.309 (0.334) loss_x loss_x 1.4746 (1.4243) acc_x 59.3750 (61.8750) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [15/23] time 0.571 (0.467) data 0.441 (0.336) loss_x loss_x 1.5410 (1.5324) acc_x 59.3750 (60.2083) lr 1.9921e-03 eta 0:00:03
epoch [10/200] batch [20/23] time 0.424 (0.467) data 0.294 (0.337) loss_x loss_x 1.2275 (1.5208) acc_x 71.8750 (60.1562) lr 1.9921e-03 eta 0:00:01
epoch [10/200] batch [5/74] time 0.415 (0.460) data 0.284 (0.329) loss_u loss_u 0.8574 (0.9076) acc_u 18.7500 (11.8750) lr 1.9921e-03 eta 0:00:31
epoch [10/200] batch [10/74] time 0.508 (0.454) data 0.376 (0.324) loss_u loss_u 0.8960 (0.9017) acc_u 12.5000 (11.8750) lr 1.9921e-03 eta 0:00:29
epoch [10/200] batch [15/74] time 0.377 (0.452) data 0.246 (0.321) loss_u loss_u 0.9727 (0.8992) acc_u 3.1250 (12.0833) lr 1.9921e-03 eta 0:00:26
epoch [10/200] batch [20/74] time 0.400 (0.447) data 0.269 (0.316) loss_u loss_u 0.8784 (0.8959) acc_u 12.5000 (12.0312) lr 1.9921e-03 eta 0:00:24
epoch [10/200] batch [25/74] time 0.402 (0.446) data 0.272 (0.315) loss_u loss_u 0.9170 (0.8929) acc_u 12.5000 (12.1250) lr 1.9921e-03 eta 0:00:21
epoch [10/200] batch [30/74] time 0.388 (0.444) data 0.258 (0.313) loss_u loss_u 0.8794 (0.8934) acc_u 12.5000 (11.9792) lr 1.9921e-03 eta 0:00:19
epoch [10/200] batch [35/74] time 0.394 (0.441) data 0.263 (0.310) loss_u loss_u 0.8760 (0.8938) acc_u 15.6250 (11.8750) lr 1.9921e-03 eta 0:00:17
epoch [10/200] batch [40/74] time 0.429 (0.438) data 0.297 (0.307) loss_u loss_u 0.8994 (0.8941) acc_u 18.7500 (12.0312) lr 1.9921e-03 eta 0:00:14
epoch [10/200] batch [45/74] time 0.419 (0.437) data 0.289 (0.306) loss_u loss_u 0.8521 (0.8922) acc_u 25.0000 (12.5694) lr 1.9921e-03 eta 0:00:12
epoch [10/200] batch [50/74] time 0.426 (0.438) data 0.295 (0.307) loss_u loss_u 0.9062 (0.8938) acc_u 12.5000 (12.3750) lr 1.9921e-03 eta 0:00:10
epoch [10/200] batch [55/74] time 0.417 (0.439) data 0.286 (0.308) loss_u loss_u 0.9360 (0.8947) acc_u 9.3750 (12.5000) lr 1.9921e-03 eta 0:00:08
epoch [10/200] batch [60/74] time 0.682 (0.443) data 0.551 (0.312) loss_u loss_u 0.8608 (0.8919) acc_u 21.8750 (12.8125) lr 1.9921e-03 eta 0:00:06
epoch [10/200] batch [65/74] time 0.570 (0.446) data 0.438 (0.315) loss_u loss_u 0.8999 (0.8917) acc_u 15.6250 (13.1731) lr 1.9921e-03 eta 0:00:04
epoch [10/200] batch [70/74] time 0.459 (0.446) data 0.328 (0.315) loss_u loss_u 0.9165 (0.8929) acc_u 12.5000 (12.9464) lr 1.9921e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1808
confident_label rate tensor(0.2462, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 772
clean true:699
clean false:73
clean_rate:0.905440414507772
noisy true:629
noisy false:1735
after delete: len(clean_dataset) 772
after delete: len(noisy_dataset) 2364
epoch [11/200] batch [5/24] time 0.342 (0.466) data 0.212 (0.335) loss_x loss_x 1.5352 (1.6207) acc_x 56.2500 (57.5000) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [10/24] time 0.435 (0.474) data 0.305 (0.344) loss_x loss_x 1.5049 (1.6235) acc_x 53.1250 (57.1875) lr 1.9900e-03 eta 0:00:06
epoch [11/200] batch [15/24] time 0.484 (0.461) data 0.353 (0.330) loss_x loss_x 1.0898 (1.6044) acc_x 68.7500 (58.7500) lr 1.9900e-03 eta 0:00:04
epoch [11/200] batch [20/24] time 0.502 (0.459) data 0.371 (0.329) loss_x loss_x 1.8428 (1.5987) acc_x 53.1250 (58.7500) lr 1.9900e-03 eta 0:00:01
epoch [11/200] batch [5/73] time 0.415 (0.453) data 0.284 (0.323) loss_u loss_u 0.8936 (0.9060) acc_u 18.7500 (11.2500) lr 1.9900e-03 eta 0:00:30
epoch [11/200] batch [10/73] time 0.378 (0.448) data 0.246 (0.318) loss_u loss_u 0.8862 (0.8999) acc_u 18.7500 (12.1875) lr 1.9900e-03 eta 0:00:28
epoch [11/200] batch [15/73] time 0.513 (0.447) data 0.381 (0.317) loss_u loss_u 0.9326 (0.9032) acc_u 12.5000 (11.6667) lr 1.9900e-03 eta 0:00:25
epoch [11/200] batch [20/73] time 0.431 (0.453) data 0.301 (0.322) loss_u loss_u 0.9038 (0.8961) acc_u 9.3750 (12.1875) lr 1.9900e-03 eta 0:00:23
epoch [11/200] batch [25/73] time 0.467 (0.450) data 0.335 (0.320) loss_u loss_u 0.8804 (0.8962) acc_u 15.6250 (11.8750) lr 1.9900e-03 eta 0:00:21
epoch [11/200] batch [30/73] time 0.405 (0.449) data 0.275 (0.318) loss_u loss_u 0.8638 (0.8931) acc_u 15.6250 (12.6042) lr 1.9900e-03 eta 0:00:19
epoch [11/200] batch [35/73] time 0.561 (0.449) data 0.430 (0.318) loss_u loss_u 0.9106 (0.8921) acc_u 9.3750 (12.8571) lr 1.9900e-03 eta 0:00:17
epoch [11/200] batch [40/73] time 0.413 (0.445) data 0.282 (0.314) loss_u loss_u 0.9146 (0.8912) acc_u 12.5000 (13.4375) lr 1.9900e-03 eta 0:00:14
epoch [11/200] batch [45/73] time 0.516 (0.446) data 0.385 (0.316) loss_u loss_u 0.8848 (0.8924) acc_u 12.5000 (13.0556) lr 1.9900e-03 eta 0:00:12
epoch [11/200] batch [50/73] time 0.402 (0.457) data 0.270 (0.326) loss_u loss_u 0.8711 (0.8930) acc_u 18.7500 (12.8750) lr 1.9900e-03 eta 0:00:10
epoch [11/200] batch [55/73] time 0.462 (0.456) data 0.331 (0.325) loss_u loss_u 0.8940 (0.8946) acc_u 6.2500 (12.6705) lr 1.9900e-03 eta 0:00:08
epoch [11/200] batch [60/73] time 0.385 (0.453) data 0.254 (0.322) loss_u loss_u 0.9087 (0.8972) acc_u 18.7500 (12.3958) lr 1.9900e-03 eta 0:00:05
epoch [11/200] batch [65/73] time 0.389 (0.453) data 0.258 (0.322) loss_u loss_u 0.9146 (0.8958) acc_u 3.1250 (12.6442) lr 1.9900e-03 eta 0:00:03
epoch [11/200] batch [70/73] time 0.382 (0.450) data 0.251 (0.319) loss_u loss_u 0.8975 (0.8945) acc_u 12.5000 (12.9464) lr 1.9900e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1816
confident_label rate tensor(0.2459, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 771
clean true:692
clean false:79
clean_rate:0.8975356679636836
noisy true:628
noisy false:1737
after delete: len(clean_dataset) 771
after delete: len(noisy_dataset) 2365
epoch [12/200] batch [5/24] time 0.428 (0.447) data 0.297 (0.315) loss_x loss_x 2.2949 (1.6492) acc_x 43.7500 (55.6250) lr 1.9877e-03 eta 0:00:08
epoch [12/200] batch [10/24] time 0.547 (0.478) data 0.417 (0.347) loss_x loss_x 1.4688 (1.5978) acc_x 68.7500 (56.2500) lr 1.9877e-03 eta 0:00:06
epoch [12/200] batch [15/24] time 0.518 (0.478) data 0.387 (0.347) loss_x loss_x 1.7578 (1.5962) acc_x 56.2500 (56.8750) lr 1.9877e-03 eta 0:00:04
epoch [12/200] batch [20/24] time 0.506 (0.474) data 0.375 (0.343) loss_x loss_x 1.5117 (1.5907) acc_x 59.3750 (57.1875) lr 1.9877e-03 eta 0:00:01
epoch [12/200] batch [5/73] time 0.479 (0.471) data 0.349 (0.340) loss_u loss_u 0.8599 (0.8787) acc_u 25.0000 (14.3750) lr 1.9877e-03 eta 0:00:32
epoch [12/200] batch [10/73] time 0.490 (0.467) data 0.358 (0.336) loss_u loss_u 0.8745 (0.8902) acc_u 15.6250 (13.1250) lr 1.9877e-03 eta 0:00:29
epoch [12/200] batch [15/73] time 0.550 (0.460) data 0.418 (0.329) loss_u loss_u 0.8979 (0.8961) acc_u 12.5000 (13.3333) lr 1.9877e-03 eta 0:00:26
epoch [12/200] batch [20/73] time 0.428 (0.460) data 0.298 (0.329) loss_u loss_u 0.8804 (0.8897) acc_u 12.5000 (14.3750) lr 1.9877e-03 eta 0:00:24
epoch [12/200] batch [25/73] time 0.478 (0.460) data 0.347 (0.329) loss_u loss_u 0.8315 (0.8855) acc_u 25.0000 (14.7500) lr 1.9877e-03 eta 0:00:22
epoch [12/200] batch [30/73] time 0.367 (0.457) data 0.236 (0.326) loss_u loss_u 0.8784 (0.8872) acc_u 18.7500 (14.4792) lr 1.9877e-03 eta 0:00:19
epoch [12/200] batch [35/73] time 0.402 (0.449) data 0.269 (0.318) loss_u loss_u 0.8472 (0.8882) acc_u 21.8750 (14.1071) lr 1.9877e-03 eta 0:00:17
epoch [12/200] batch [40/73] time 0.408 (0.446) data 0.276 (0.315) loss_u loss_u 0.9341 (0.8888) acc_u 3.1250 (14.0625) lr 1.9877e-03 eta 0:00:14
epoch [12/200] batch [45/73] time 0.451 (0.445) data 0.319 (0.314) loss_u loss_u 0.9160 (0.8878) acc_u 12.5000 (14.3056) lr 1.9877e-03 eta 0:00:12
epoch [12/200] batch [50/73] time 0.564 (0.454) data 0.433 (0.322) loss_u loss_u 0.9048 (0.8894) acc_u 9.3750 (14.1875) lr 1.9877e-03 eta 0:00:10
epoch [12/200] batch [55/73] time 0.360 (0.451) data 0.228 (0.320) loss_u loss_u 0.9180 (0.8895) acc_u 6.2500 (14.0909) lr 1.9877e-03 eta 0:00:08
epoch [12/200] batch [60/73] time 0.431 (0.451) data 0.299 (0.320) loss_u loss_u 0.8721 (0.8916) acc_u 9.3750 (13.5417) lr 1.9877e-03 eta 0:00:05
epoch [12/200] batch [65/73] time 0.584 (0.451) data 0.452 (0.320) loss_u loss_u 0.8911 (0.8919) acc_u 9.3750 (13.3173) lr 1.9877e-03 eta 0:00:03
epoch [12/200] batch [70/73] time 0.347 (0.449) data 0.216 (0.318) loss_u loss_u 0.8457 (0.8910) acc_u 18.7500 (13.7054) lr 1.9877e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1825
confident_label rate tensor(0.2465, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 773
clean true:701
clean false:72
clean_rate:0.906856403622251
noisy true:610
noisy false:1753
after delete: len(clean_dataset) 773
after delete: len(noisy_dataset) 2363
epoch [13/200] batch [5/24] time 0.416 (0.470) data 0.286 (0.339) loss_x loss_x 1.5527 (1.3629) acc_x 56.2500 (64.3750) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [10/24] time 0.589 (0.483) data 0.457 (0.352) loss_x loss_x 1.5947 (1.4913) acc_x 62.5000 (63.7500) lr 1.9851e-03 eta 0:00:06
epoch [13/200] batch [15/24] time 0.457 (0.470) data 0.326 (0.339) loss_x loss_x 1.8154 (1.5322) acc_x 53.1250 (62.0833) lr 1.9851e-03 eta 0:00:04
epoch [13/200] batch [20/24] time 0.491 (0.464) data 0.360 (0.333) loss_x loss_x 1.6104 (1.5173) acc_x 53.1250 (61.7188) lr 1.9851e-03 eta 0:00:01
epoch [13/200] batch [5/73] time 0.309 (0.453) data 0.178 (0.322) loss_u loss_u 0.9648 (0.9195) acc_u 0.0000 (11.2500) lr 1.9851e-03 eta 0:00:30
epoch [13/200] batch [10/73] time 0.479 (0.454) data 0.346 (0.323) loss_u loss_u 0.8984 (0.9084) acc_u 15.6250 (12.8125) lr 1.9851e-03 eta 0:00:28
epoch [13/200] batch [15/73] time 0.296 (0.454) data 0.164 (0.324) loss_u loss_u 0.9292 (0.9015) acc_u 12.5000 (13.5417) lr 1.9851e-03 eta 0:00:26
epoch [13/200] batch [20/73] time 0.450 (0.455) data 0.317 (0.324) loss_u loss_u 0.8667 (0.8961) acc_u 12.5000 (13.5938) lr 1.9851e-03 eta 0:00:24
epoch [13/200] batch [25/73] time 0.435 (0.453) data 0.305 (0.322) loss_u loss_u 0.9116 (0.8942) acc_u 9.3750 (13.5000) lr 1.9851e-03 eta 0:00:21
epoch [13/200] batch [30/73] time 0.383 (0.448) data 0.252 (0.317) loss_u loss_u 0.8726 (0.8955) acc_u 12.5000 (12.9167) lr 1.9851e-03 eta 0:00:19
epoch [13/200] batch [35/73] time 0.422 (0.445) data 0.292 (0.314) loss_u loss_u 0.8799 (0.8943) acc_u 9.3750 (12.9464) lr 1.9851e-03 eta 0:00:16
epoch [13/200] batch [40/73] time 0.423 (0.445) data 0.292 (0.314) loss_u loss_u 0.9385 (0.8952) acc_u 6.2500 (12.8906) lr 1.9851e-03 eta 0:00:14
epoch [13/200] batch [45/73] time 0.670 (0.448) data 0.536 (0.317) loss_u loss_u 0.9058 (0.8953) acc_u 15.6250 (12.9167) lr 1.9851e-03 eta 0:00:12
epoch [13/200] batch [50/73] time 0.416 (0.449) data 0.285 (0.318) loss_u loss_u 0.9170 (0.8940) acc_u 18.7500 (13.3750) lr 1.9851e-03 eta 0:00:10
epoch [13/200] batch [55/73] time 0.483 (0.451) data 0.351 (0.320) loss_u loss_u 0.8975 (0.8918) acc_u 9.3750 (13.5795) lr 1.9851e-03 eta 0:00:08
epoch [13/200] batch [60/73] time 0.643 (0.455) data 0.512 (0.324) loss_u loss_u 0.9214 (0.8942) acc_u 9.3750 (13.2812) lr 1.9851e-03 eta 0:00:05
epoch [13/200] batch [65/73] time 0.485 (0.456) data 0.355 (0.325) loss_u loss_u 0.9292 (0.8951) acc_u 3.1250 (12.8846) lr 1.9851e-03 eta 0:00:03
epoch [13/200] batch [70/73] time 0.394 (0.457) data 0.261 (0.326) loss_u loss_u 0.8975 (0.8933) acc_u 12.5000 (12.9911) lr 1.9851e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1778
confident_label rate tensor(0.2500, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 784
clean true:712
clean false:72
clean_rate:0.9081632653061225
noisy true:646
noisy false:1706
after delete: len(clean_dataset) 784
after delete: len(noisy_dataset) 2352
epoch [14/200] batch [5/24] time 0.668 (0.520) data 0.538 (0.389) loss_x loss_x 1.2324 (1.8150) acc_x 75.0000 (55.6250) lr 1.9823e-03 eta 0:00:09
epoch [14/200] batch [10/24] time 0.425 (0.506) data 0.295 (0.375) loss_x loss_x 1.4961 (1.6555) acc_x 68.7500 (57.5000) lr 1.9823e-03 eta 0:00:07
epoch [14/200] batch [15/24] time 0.582 (0.496) data 0.452 (0.366) loss_x loss_x 1.5615 (1.6250) acc_x 68.7500 (58.7500) lr 1.9823e-03 eta 0:00:04
epoch [14/200] batch [20/24] time 0.387 (0.469) data 0.257 (0.339) loss_x loss_x 1.7217 (1.6078) acc_x 62.5000 (58.9062) lr 1.9823e-03 eta 0:00:01
epoch [14/200] batch [5/73] time 0.404 (0.462) data 0.272 (0.332) loss_u loss_u 0.8633 (0.8811) acc_u 15.6250 (15.0000) lr 1.9823e-03 eta 0:00:31
epoch [14/200] batch [10/73] time 0.590 (0.456) data 0.458 (0.326) loss_u loss_u 0.9004 (0.8983) acc_u 12.5000 (11.5625) lr 1.9823e-03 eta 0:00:28
epoch [14/200] batch [15/73] time 0.354 (0.452) data 0.223 (0.321) loss_u loss_u 0.9048 (0.8987) acc_u 6.2500 (11.2500) lr 1.9823e-03 eta 0:00:26
epoch [14/200] batch [20/73] time 0.492 (0.460) data 0.361 (0.329) loss_u loss_u 0.9111 (0.9047) acc_u 12.5000 (10.7812) lr 1.9823e-03 eta 0:00:24
epoch [14/200] batch [25/73] time 0.438 (0.457) data 0.306 (0.326) loss_u loss_u 0.9526 (0.9049) acc_u 6.2500 (11.1250) lr 1.9823e-03 eta 0:00:21
epoch [14/200] batch [30/73] time 0.361 (0.464) data 0.230 (0.333) loss_u loss_u 0.8818 (0.9000) acc_u 15.6250 (11.7708) lr 1.9823e-03 eta 0:00:19
epoch [14/200] batch [35/73] time 0.472 (0.466) data 0.342 (0.335) loss_u loss_u 0.7915 (0.8975) acc_u 37.5000 (12.6786) lr 1.9823e-03 eta 0:00:17
epoch [14/200] batch [40/73] time 0.377 (0.460) data 0.245 (0.329) loss_u loss_u 0.8452 (0.8930) acc_u 15.6250 (13.2031) lr 1.9823e-03 eta 0:00:15
epoch [14/200] batch [45/73] time 0.352 (0.459) data 0.220 (0.328) loss_u loss_u 0.8779 (0.8957) acc_u 12.5000 (12.7778) lr 1.9823e-03 eta 0:00:12
epoch [14/200] batch [50/73] time 0.502 (0.460) data 0.370 (0.329) loss_u loss_u 0.7744 (0.8899) acc_u 25.0000 (13.1875) lr 1.9823e-03 eta 0:00:10
epoch [14/200] batch [55/73] time 0.421 (0.457) data 0.291 (0.326) loss_u loss_u 0.8975 (0.8896) acc_u 12.5000 (13.2386) lr 1.9823e-03 eta 0:00:08
epoch [14/200] batch [60/73] time 0.395 (0.454) data 0.265 (0.323) loss_u loss_u 0.8936 (0.8893) acc_u 12.5000 (13.4375) lr 1.9823e-03 eta 0:00:05
epoch [14/200] batch [65/73] time 0.406 (0.452) data 0.276 (0.322) loss_u loss_u 0.8936 (0.8899) acc_u 9.3750 (13.2212) lr 1.9823e-03 eta 0:00:03
epoch [14/200] batch [70/73] time 0.464 (0.449) data 0.331 (0.319) loss_u loss_u 0.9116 (0.8901) acc_u 9.3750 (13.1696) lr 1.9823e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1765
confident_label rate tensor(0.2516, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 789
clean true:719
clean false:70
clean_rate:0.9112801013941698
noisy true:652
noisy false:1695
after delete: len(clean_dataset) 789
after delete: len(noisy_dataset) 2347
epoch [15/200] batch [5/24] time 0.406 (0.512) data 0.275 (0.381) loss_x loss_x 1.4062 (1.3248) acc_x 56.2500 (67.5000) lr 1.9792e-03 eta 0:00:09
epoch [15/200] batch [10/24] time 0.474 (0.490) data 0.344 (0.359) loss_x loss_x 1.4580 (1.4235) acc_x 56.2500 (63.4375) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [15/24] time 0.467 (0.489) data 0.337 (0.359) loss_x loss_x 1.0605 (1.3816) acc_x 65.6250 (63.5417) lr 1.9792e-03 eta 0:00:04
epoch [15/200] batch [20/24] time 0.370 (0.467) data 0.240 (0.337) loss_x loss_x 1.6465 (1.4797) acc_x 62.5000 (61.4062) lr 1.9792e-03 eta 0:00:01
epoch [15/200] batch [5/73] time 0.518 (0.465) data 0.387 (0.334) loss_u loss_u 0.8276 (0.8842) acc_u 15.6250 (14.3750) lr 1.9792e-03 eta 0:00:31
epoch [15/200] batch [10/73] time 0.631 (0.479) data 0.500 (0.348) loss_u loss_u 0.8271 (0.8757) acc_u 21.8750 (16.2500) lr 1.9792e-03 eta 0:00:30
epoch [15/200] batch [15/73] time 0.405 (0.474) data 0.274 (0.343) loss_u loss_u 0.8828 (0.8875) acc_u 15.6250 (14.3750) lr 1.9792e-03 eta 0:00:27
epoch [15/200] batch [20/73] time 0.404 (0.469) data 0.273 (0.338) loss_u loss_u 0.8750 (0.8895) acc_u 21.8750 (14.0625) lr 1.9792e-03 eta 0:00:24
epoch [15/200] batch [25/73] time 0.351 (0.460) data 0.220 (0.328) loss_u loss_u 0.9419 (0.8922) acc_u 6.2500 (13.5000) lr 1.9792e-03 eta 0:00:22
epoch [15/200] batch [30/73] time 0.758 (0.464) data 0.628 (0.333) loss_u loss_u 0.8818 (0.8921) acc_u 9.3750 (13.3333) lr 1.9792e-03 eta 0:00:19
epoch [15/200] batch [35/73] time 0.408 (0.462) data 0.277 (0.331) loss_u loss_u 0.8857 (0.8873) acc_u 12.5000 (13.7500) lr 1.9792e-03 eta 0:00:17
epoch [15/200] batch [40/73] time 0.426 (0.458) data 0.295 (0.327) loss_u loss_u 0.9043 (0.8880) acc_u 12.5000 (13.9844) lr 1.9792e-03 eta 0:00:15
epoch [15/200] batch [45/73] time 0.432 (0.458) data 0.302 (0.327) loss_u loss_u 0.9229 (0.8893) acc_u 9.3750 (13.8194) lr 1.9792e-03 eta 0:00:12
epoch [15/200] batch [50/73] time 0.463 (0.461) data 0.333 (0.330) loss_u loss_u 0.9150 (0.8892) acc_u 9.3750 (13.6875) lr 1.9792e-03 eta 0:00:10
epoch [15/200] batch [55/73] time 0.470 (0.461) data 0.339 (0.330) loss_u loss_u 0.7988 (0.8874) acc_u 25.0000 (14.0341) lr 1.9792e-03 eta 0:00:08
epoch [15/200] batch [60/73] time 0.377 (0.462) data 0.247 (0.331) loss_u loss_u 0.9282 (0.8907) acc_u 9.3750 (13.5938) lr 1.9792e-03 eta 0:00:06
epoch [15/200] batch [65/73] time 0.387 (0.467) data 0.256 (0.336) loss_u loss_u 0.9487 (0.8902) acc_u 6.2500 (13.7981) lr 1.9792e-03 eta 0:00:03
epoch [15/200] batch [70/73] time 0.388 (0.465) data 0.257 (0.334) loss_u loss_u 0.8203 (0.8889) acc_u 18.7500 (13.9732) lr 1.9792e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1755
confident_label rate tensor(0.2580, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 809
clean true:733
clean false:76
clean_rate:0.9060568603213844
noisy true:648
noisy false:1679
after delete: len(clean_dataset) 809
after delete: len(noisy_dataset) 2327
epoch [16/200] batch [5/25] time 0.574 (0.565) data 0.444 (0.434) loss_x loss_x 1.3916 (1.5219) acc_x 62.5000 (61.2500) lr 1.9759e-03 eta 0:00:11
epoch [16/200] batch [10/25] time 0.399 (0.506) data 0.269 (0.376) loss_x loss_x 1.0596 (1.4884) acc_x 71.8750 (60.3125) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [15/25] time 0.406 (0.500) data 0.275 (0.370) loss_x loss_x 1.2266 (1.4400) acc_x 59.3750 (60.6250) lr 1.9759e-03 eta 0:00:05
epoch [16/200] batch [20/25] time 0.495 (0.497) data 0.366 (0.366) loss_x loss_x 1.5635 (1.4103) acc_x 59.3750 (62.5000) lr 1.9759e-03 eta 0:00:02
epoch [16/200] batch [25/25] time 0.464 (0.494) data 0.333 (0.363) loss_x loss_x 1.2295 (1.3831) acc_x 65.6250 (62.7500) lr 1.9759e-03 eta 0:00:00
epoch [16/200] batch [5/72] time 0.384 (0.492) data 0.252 (0.361) loss_u loss_u 0.9287 (0.8917) acc_u 9.3750 (15.6250) lr 1.9759e-03 eta 0:00:32
epoch [16/200] batch [10/72] time 0.468 (0.493) data 0.335 (0.362) loss_u loss_u 0.9067 (0.8847) acc_u 12.5000 (15.0000) lr 1.9759e-03 eta 0:00:30
epoch [16/200] batch [15/72] time 0.620 (0.492) data 0.487 (0.361) loss_u loss_u 0.9399 (0.8871) acc_u 3.1250 (14.1667) lr 1.9759e-03 eta 0:00:28
epoch [16/200] batch [20/72] time 0.384 (0.486) data 0.253 (0.355) loss_u loss_u 0.8472 (0.8841) acc_u 21.8750 (14.6875) lr 1.9759e-03 eta 0:00:25
epoch [16/200] batch [25/72] time 0.410 (0.479) data 0.280 (0.348) loss_u loss_u 0.8687 (0.8880) acc_u 15.6250 (13.7500) lr 1.9759e-03 eta 0:00:22
epoch [16/200] batch [30/72] time 0.398 (0.474) data 0.268 (0.343) loss_u loss_u 0.9404 (0.8914) acc_u 9.3750 (13.1250) lr 1.9759e-03 eta 0:00:19
epoch [16/200] batch [35/72] time 0.395 (0.471) data 0.265 (0.340) loss_u loss_u 0.9214 (0.8911) acc_u 9.3750 (12.9464) lr 1.9759e-03 eta 0:00:17
epoch [16/200] batch [40/72] time 0.563 (0.470) data 0.433 (0.339) loss_u loss_u 0.8794 (0.8917) acc_u 9.3750 (12.8906) lr 1.9759e-03 eta 0:00:15
epoch [16/200] batch [45/72] time 0.444 (0.467) data 0.311 (0.336) loss_u loss_u 0.8979 (0.8904) acc_u 9.3750 (12.9167) lr 1.9759e-03 eta 0:00:12
epoch [16/200] batch [50/72] time 0.508 (0.467) data 0.376 (0.336) loss_u loss_u 0.7632 (0.8894) acc_u 21.8750 (12.7500) lr 1.9759e-03 eta 0:00:10
epoch [16/200] batch [55/72] time 0.453 (0.464) data 0.321 (0.333) loss_u loss_u 0.9053 (0.8907) acc_u 6.2500 (12.5568) lr 1.9759e-03 eta 0:00:07
epoch [16/200] batch [60/72] time 0.408 (0.461) data 0.275 (0.330) loss_u loss_u 0.8730 (0.8898) acc_u 6.2500 (12.6562) lr 1.9759e-03 eta 0:00:05
epoch [16/200] batch [65/72] time 0.466 (0.459) data 0.333 (0.328) loss_u loss_u 0.9194 (0.8892) acc_u 12.5000 (12.6923) lr 1.9759e-03 eta 0:00:03
epoch [16/200] batch [70/72] time 0.440 (0.463) data 0.308 (0.332) loss_u loss_u 0.8950 (0.8895) acc_u 12.5000 (12.6339) lr 1.9759e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1771
confident_label rate tensor(0.2503, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 785
clean true:706
clean false:79
clean_rate:0.8993630573248408
noisy true:659
noisy false:1692
after delete: len(clean_dataset) 785
after delete: len(noisy_dataset) 2351
epoch [17/200] batch [5/24] time 0.467 (0.459) data 0.336 (0.328) loss_x loss_x 1.6436 (1.3429) acc_x 56.2500 (68.1250) lr 1.9724e-03 eta 0:00:08
epoch [17/200] batch [10/24] time 0.357 (0.464) data 0.227 (0.333) loss_x loss_x 1.5801 (1.5258) acc_x 68.7500 (64.3750) lr 1.9724e-03 eta 0:00:06
epoch [17/200] batch [15/24] time 0.577 (0.493) data 0.446 (0.363) loss_x loss_x 1.2168 (1.6077) acc_x 62.5000 (60.2083) lr 1.9724e-03 eta 0:00:04
epoch [17/200] batch [20/24] time 0.372 (0.474) data 0.241 (0.343) loss_x loss_x 1.7832 (1.5496) acc_x 53.1250 (61.2500) lr 1.9724e-03 eta 0:00:01
epoch [17/200] batch [5/73] time 0.436 (0.466) data 0.305 (0.335) loss_u loss_u 0.9019 (0.9091) acc_u 12.5000 (11.2500) lr 1.9724e-03 eta 0:00:31
epoch [17/200] batch [10/73] time 0.403 (0.465) data 0.271 (0.334) loss_u loss_u 0.8906 (0.8985) acc_u 9.3750 (11.5625) lr 1.9724e-03 eta 0:00:29
epoch [17/200] batch [15/73] time 0.411 (0.466) data 0.278 (0.335) loss_u loss_u 0.8442 (0.8916) acc_u 18.7500 (12.0833) lr 1.9724e-03 eta 0:00:27
epoch [17/200] batch [20/73] time 0.542 (0.468) data 0.411 (0.337) loss_u loss_u 0.8843 (0.8891) acc_u 18.7500 (13.4375) lr 1.9724e-03 eta 0:00:24
epoch [17/200] batch [25/73] time 0.559 (0.467) data 0.427 (0.336) loss_u loss_u 0.8408 (0.8875) acc_u 18.7500 (13.6250) lr 1.9724e-03 eta 0:00:22
epoch [17/200] batch [30/73] time 0.382 (0.466) data 0.251 (0.335) loss_u loss_u 0.8926 (0.8898) acc_u 15.6250 (13.6458) lr 1.9724e-03 eta 0:00:20
epoch [17/200] batch [35/73] time 0.466 (0.467) data 0.335 (0.336) loss_u loss_u 0.9106 (0.8888) acc_u 9.3750 (13.7500) lr 1.9724e-03 eta 0:00:17
epoch [17/200] batch [40/73] time 0.406 (0.461) data 0.273 (0.329) loss_u loss_u 0.8760 (0.8913) acc_u 15.6250 (13.2812) lr 1.9724e-03 eta 0:00:15
epoch [17/200] batch [45/73] time 0.436 (0.463) data 0.305 (0.332) loss_u loss_u 0.8721 (0.8894) acc_u 12.5000 (13.5417) lr 1.9724e-03 eta 0:00:12
epoch [17/200] batch [50/73] time 0.478 (0.459) data 0.347 (0.328) loss_u loss_u 0.8975 (0.8885) acc_u 15.6250 (13.6250) lr 1.9724e-03 eta 0:00:10
epoch [17/200] batch [55/73] time 0.373 (0.454) data 0.242 (0.323) loss_u loss_u 0.7930 (0.8851) acc_u 21.8750 (13.8636) lr 1.9724e-03 eta 0:00:08
epoch [17/200] batch [60/73] time 0.388 (0.452) data 0.257 (0.321) loss_u loss_u 0.8945 (0.8835) acc_u 15.6250 (14.1667) lr 1.9724e-03 eta 0:00:05
epoch [17/200] batch [65/73] time 0.591 (0.453) data 0.461 (0.322) loss_u loss_u 0.8389 (0.8840) acc_u 18.7500 (14.0385) lr 1.9724e-03 eta 0:00:03
epoch [17/200] batch [70/73] time 0.396 (0.452) data 0.265 (0.321) loss_u loss_u 0.9839 (0.8842) acc_u 0.0000 (14.0625) lr 1.9724e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1771
confident_label rate tensor(0.2462, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 772
clean true:705
clean false:67
clean_rate:0.9132124352331606
noisy true:660
noisy false:1704
after delete: len(clean_dataset) 772
after delete: len(noisy_dataset) 2364
epoch [18/200] batch [5/24] time 0.430 (0.449) data 0.300 (0.318) loss_x loss_x 1.6221 (1.3760) acc_x 62.5000 (66.2500) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [10/24] time 0.450 (0.460) data 0.320 (0.330) loss_x loss_x 1.7861 (1.4470) acc_x 53.1250 (64.3750) lr 1.9686e-03 eta 0:00:06
epoch [18/200] batch [15/24] time 0.621 (0.487) data 0.490 (0.357) loss_x loss_x 1.6211 (1.4116) acc_x 56.2500 (64.1667) lr 1.9686e-03 eta 0:00:04
epoch [18/200] batch [20/24] time 0.493 (0.480) data 0.362 (0.350) loss_x loss_x 1.5586 (1.3958) acc_x 53.1250 (62.3438) lr 1.9686e-03 eta 0:00:01
epoch [18/200] batch [5/73] time 0.446 (0.486) data 0.316 (0.355) loss_u loss_u 0.8740 (0.8821) acc_u 12.5000 (15.0000) lr 1.9686e-03 eta 0:00:33
epoch [18/200] batch [10/73] time 0.391 (0.470) data 0.260 (0.339) loss_u loss_u 0.9072 (0.8680) acc_u 15.6250 (17.5000) lr 1.9686e-03 eta 0:00:29
epoch [18/200] batch [15/73] time 0.398 (0.464) data 0.267 (0.333) loss_u loss_u 0.8320 (0.8745) acc_u 25.0000 (16.8750) lr 1.9686e-03 eta 0:00:26
epoch [18/200] batch [20/73] time 0.420 (0.458) data 0.289 (0.328) loss_u loss_u 0.9185 (0.8826) acc_u 9.3750 (15.0000) lr 1.9686e-03 eta 0:00:24
epoch [18/200] batch [25/73] time 0.419 (0.453) data 0.288 (0.322) loss_u loss_u 0.9004 (0.8890) acc_u 12.5000 (14.1250) lr 1.9686e-03 eta 0:00:21
epoch [18/200] batch [30/73] time 0.393 (0.451) data 0.261 (0.320) loss_u loss_u 0.8262 (0.8872) acc_u 28.1250 (14.6875) lr 1.9686e-03 eta 0:00:19
epoch [18/200] batch [35/73] time 0.501 (0.455) data 0.371 (0.324) loss_u loss_u 0.8691 (0.8902) acc_u 18.7500 (14.3750) lr 1.9686e-03 eta 0:00:17
epoch [18/200] batch [40/73] time 0.389 (0.452) data 0.258 (0.322) loss_u loss_u 0.9272 (0.8893) acc_u 9.3750 (14.4531) lr 1.9686e-03 eta 0:00:14
epoch [18/200] batch [45/73] time 0.408 (0.452) data 0.277 (0.321) loss_u loss_u 0.9038 (0.8892) acc_u 15.6250 (14.7222) lr 1.9686e-03 eta 0:00:12
epoch [18/200] batch [50/73] time 0.513 (0.456) data 0.380 (0.325) loss_u loss_u 0.8828 (0.8857) acc_u 18.7500 (15.0625) lr 1.9686e-03 eta 0:00:10
epoch [18/200] batch [55/73] time 0.463 (0.459) data 0.330 (0.328) loss_u loss_u 0.8408 (0.8852) acc_u 12.5000 (14.8864) lr 1.9686e-03 eta 0:00:08
epoch [18/200] batch [60/73] time 0.402 (0.456) data 0.271 (0.325) loss_u loss_u 0.8813 (0.8838) acc_u 15.6250 (15.1562) lr 1.9686e-03 eta 0:00:05
epoch [18/200] batch [65/73] time 0.321 (0.452) data 0.191 (0.321) loss_u loss_u 0.8462 (0.8842) acc_u 18.7500 (15.0962) lr 1.9686e-03 eta 0:00:03
epoch [18/200] batch [70/73] time 0.362 (0.449) data 0.230 (0.318) loss_u loss_u 0.8823 (0.8830) acc_u 15.6250 (15.2679) lr 1.9686e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1741
confident_label rate tensor(0.2656, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 833
clean true:755
clean false:78
clean_rate:0.9063625450180072
noisy true:640
noisy false:1663
after delete: len(clean_dataset) 833
after delete: len(noisy_dataset) 2303
epoch [19/200] batch [5/26] time 0.419 (0.447) data 0.287 (0.316) loss_x loss_x 1.5586 (1.4869) acc_x 65.6250 (63.1250) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [10/26] time 0.432 (0.467) data 0.300 (0.336) loss_x loss_x 1.9795 (1.5997) acc_x 50.0000 (60.0000) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [15/26] time 0.501 (0.466) data 0.370 (0.334) loss_x loss_x 1.7383 (1.4757) acc_x 50.0000 (62.0833) lr 1.9646e-03 eta 0:00:05
epoch [19/200] batch [20/26] time 0.355 (0.464) data 0.225 (0.333) loss_x loss_x 1.6211 (1.4796) acc_x 62.5000 (61.5625) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [25/26] time 0.435 (0.457) data 0.303 (0.326) loss_x loss_x 1.2783 (1.4776) acc_x 68.7500 (61.5000) lr 1.9646e-03 eta 0:00:00
epoch [19/200] batch [5/71] time 0.466 (0.446) data 0.335 (0.315) loss_u loss_u 0.9170 (0.8918) acc_u 12.5000 (13.7500) lr 1.9646e-03 eta 0:00:29
epoch [19/200] batch [10/71] time 0.407 (0.448) data 0.276 (0.317) loss_u loss_u 0.8926 (0.8822) acc_u 15.6250 (15.3125) lr 1.9646e-03 eta 0:00:27
epoch [19/200] batch [15/71] time 0.419 (0.446) data 0.289 (0.315) loss_u loss_u 0.8931 (0.8882) acc_u 15.6250 (14.1667) lr 1.9646e-03 eta 0:00:24
epoch [19/200] batch [20/71] time 0.424 (0.444) data 0.292 (0.312) loss_u loss_u 0.8145 (0.8904) acc_u 25.0000 (13.9062) lr 1.9646e-03 eta 0:00:22
epoch [19/200] batch [25/71] time 0.368 (0.442) data 0.236 (0.311) loss_u loss_u 0.8628 (0.8900) acc_u 15.6250 (13.3750) lr 1.9646e-03 eta 0:00:20
epoch [19/200] batch [30/71] time 0.401 (0.439) data 0.271 (0.308) loss_u loss_u 0.9219 (0.8949) acc_u 6.2500 (12.6042) lr 1.9646e-03 eta 0:00:18
epoch [19/200] batch [35/71] time 0.454 (0.449) data 0.322 (0.318) loss_u loss_u 0.7935 (0.8893) acc_u 28.1250 (13.6607) lr 1.9646e-03 eta 0:00:16
epoch [19/200] batch [40/71] time 0.367 (0.448) data 0.234 (0.317) loss_u loss_u 0.9043 (0.8928) acc_u 18.7500 (13.7500) lr 1.9646e-03 eta 0:00:13
epoch [19/200] batch [45/71] time 0.666 (0.452) data 0.534 (0.320) loss_u loss_u 0.8809 (0.8919) acc_u 12.5000 (13.6111) lr 1.9646e-03 eta 0:00:11
epoch [19/200] batch [50/71] time 0.385 (0.448) data 0.254 (0.317) loss_u loss_u 0.8770 (0.8891) acc_u 21.8750 (14.3125) lr 1.9646e-03 eta 0:00:09
epoch [19/200] batch [55/71] time 0.395 (0.447) data 0.263 (0.315) loss_u loss_u 0.8652 (0.8899) acc_u 9.3750 (14.0341) lr 1.9646e-03 eta 0:00:07
epoch [19/200] batch [60/71] time 0.532 (0.450) data 0.401 (0.318) loss_u loss_u 0.9033 (0.8920) acc_u 9.3750 (13.7500) lr 1.9646e-03 eta 0:00:04
epoch [19/200] batch [65/71] time 0.525 (0.453) data 0.393 (0.321) loss_u loss_u 0.8887 (0.8912) acc_u 15.6250 (13.7981) lr 1.9646e-03 eta 0:00:02
epoch [19/200] batch [70/71] time 0.532 (0.453) data 0.400 (0.322) loss_u loss_u 0.9233 (0.8892) acc_u 12.5000 (14.1071) lr 1.9646e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1788
confident_label rate tensor(0.2529, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 793
clean true:709
clean false:84
clean_rate:0.8940731399747793
noisy true:639
noisy false:1704
after delete: len(clean_dataset) 793
after delete: len(noisy_dataset) 2343
epoch [20/200] batch [5/24] time 0.459 (0.479) data 0.328 (0.348) loss_x loss_x 1.4092 (1.5754) acc_x 68.7500 (62.5000) lr 1.9603e-03 eta 0:00:09
epoch [20/200] batch [10/24] time 0.540 (0.487) data 0.408 (0.357) loss_x loss_x 1.0801 (1.4115) acc_x 65.6250 (64.0625) lr 1.9603e-03 eta 0:00:06
epoch [20/200] batch [15/24] time 0.612 (0.494) data 0.481 (0.363) loss_x loss_x 1.8369 (1.5035) acc_x 46.8750 (61.4583) lr 1.9603e-03 eta 0:00:04
epoch [20/200] batch [20/24] time 0.499 (0.469) data 0.368 (0.338) loss_x loss_x 1.4551 (1.5308) acc_x 59.3750 (62.1875) lr 1.9603e-03 eta 0:00:01
epoch [20/200] batch [5/73] time 0.435 (0.458) data 0.303 (0.327) loss_u loss_u 0.8862 (0.8642) acc_u 12.5000 (17.5000) lr 1.9603e-03 eta 0:00:31
epoch [20/200] batch [10/73] time 0.392 (0.457) data 0.262 (0.326) loss_u loss_u 0.9116 (0.8723) acc_u 18.7500 (15.9375) lr 1.9603e-03 eta 0:00:28
epoch [20/200] batch [15/73] time 0.593 (0.456) data 0.461 (0.325) loss_u loss_u 0.8193 (0.8789) acc_u 25.0000 (15.8333) lr 1.9603e-03 eta 0:00:26
epoch [20/200] batch [20/73] time 0.382 (0.454) data 0.252 (0.323) loss_u loss_u 0.8833 (0.8746) acc_u 12.5000 (16.2500) lr 1.9603e-03 eta 0:00:24
epoch [20/200] batch [25/73] time 0.337 (0.451) data 0.206 (0.320) loss_u loss_u 0.8574 (0.8694) acc_u 18.7500 (17.0000) lr 1.9603e-03 eta 0:00:21
epoch [20/200] batch [30/73] time 0.408 (0.447) data 0.276 (0.316) loss_u loss_u 0.8276 (0.8666) acc_u 18.7500 (17.0833) lr 1.9603e-03 eta 0:00:19
epoch [20/200] batch [35/73] time 0.385 (0.447) data 0.253 (0.316) loss_u loss_u 0.8423 (0.8678) acc_u 18.7500 (16.8750) lr 1.9603e-03 eta 0:00:16
epoch [20/200] batch [40/73] time 0.383 (0.449) data 0.251 (0.318) loss_u loss_u 0.9404 (0.8730) acc_u 12.5000 (16.3281) lr 1.9603e-03 eta 0:00:14
epoch [20/200] batch [45/73] time 0.416 (0.450) data 0.285 (0.318) loss_u loss_u 0.8857 (0.8742) acc_u 15.6250 (16.1806) lr 1.9603e-03 eta 0:00:12
epoch [20/200] batch [50/73] time 0.448 (0.446) data 0.316 (0.315) loss_u loss_u 0.8467 (0.8731) acc_u 21.8750 (16.1250) lr 1.9603e-03 eta 0:00:10
epoch [20/200] batch [55/73] time 0.383 (0.445) data 0.251 (0.314) loss_u loss_u 0.9351 (0.8748) acc_u 9.3750 (15.7955) lr 1.9603e-03 eta 0:00:08
epoch [20/200] batch [60/73] time 0.370 (0.442) data 0.239 (0.311) loss_u loss_u 0.9126 (0.8768) acc_u 9.3750 (15.4688) lr 1.9603e-03 eta 0:00:05
epoch [20/200] batch [65/73] time 0.396 (0.442) data 0.265 (0.311) loss_u loss_u 0.8950 (0.8771) acc_u 9.3750 (15.3365) lr 1.9603e-03 eta 0:00:03
epoch [20/200] batch [70/73] time 0.555 (0.443) data 0.423 (0.311) loss_u loss_u 0.9204 (0.8772) acc_u 9.3750 (15.1786) lr 1.9603e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1785
confident_label rate tensor(0.2608, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 818
clean true:731
clean false:87
clean_rate:0.8936430317848411
noisy true:620
noisy false:1698
after delete: len(clean_dataset) 818
after delete: len(noisy_dataset) 2318
epoch [21/200] batch [5/25] time 0.398 (0.427) data 0.267 (0.297) loss_x loss_x 0.8101 (1.5058) acc_x 78.1250 (63.7500) lr 1.9558e-03 eta 0:00:08
epoch [21/200] batch [10/25] time 0.384 (0.439) data 0.254 (0.308) loss_x loss_x 1.9971 (1.5169) acc_x 62.5000 (65.9375) lr 1.9558e-03 eta 0:00:06
epoch [21/200] batch [15/25] time 0.390 (0.439) data 0.259 (0.308) loss_x loss_x 1.2842 (1.4856) acc_x 75.0000 (65.2083) lr 1.9558e-03 eta 0:00:04
epoch [21/200] batch [20/25] time 0.459 (0.461) data 0.329 (0.330) loss_x loss_x 1.4072 (1.4885) acc_x 56.2500 (62.9688) lr 1.9558e-03 eta 0:00:02
epoch [21/200] batch [25/25] time 0.401 (0.468) data 0.271 (0.338) loss_x loss_x 1.1592 (1.4419) acc_x 65.6250 (64.2500) lr 1.9558e-03 eta 0:00:00
epoch [21/200] batch [5/72] time 0.462 (0.470) data 0.330 (0.339) loss_u loss_u 0.9136 (0.9192) acc_u 12.5000 (8.7500) lr 1.9558e-03 eta 0:00:31
epoch [21/200] batch [10/72] time 0.452 (0.460) data 0.322 (0.329) loss_u loss_u 0.8740 (0.8940) acc_u 18.7500 (13.1250) lr 1.9558e-03 eta 0:00:28
epoch [21/200] batch [15/72] time 0.399 (0.453) data 0.269 (0.322) loss_u loss_u 0.8989 (0.8975) acc_u 18.7500 (13.5417) lr 1.9558e-03 eta 0:00:25
epoch [21/200] batch [20/72] time 0.457 (0.452) data 0.326 (0.321) loss_u loss_u 0.9380 (0.9028) acc_u 6.2500 (12.6562) lr 1.9558e-03 eta 0:00:23
epoch [21/200] batch [25/72] time 0.673 (0.453) data 0.542 (0.322) loss_u loss_u 0.8706 (0.8942) acc_u 18.7500 (13.7500) lr 1.9558e-03 eta 0:00:21
epoch [21/200] batch [30/72] time 0.350 (0.456) data 0.219 (0.325) loss_u loss_u 0.8877 (0.8929) acc_u 18.7500 (13.7500) lr 1.9558e-03 eta 0:00:19
epoch [21/200] batch [35/72] time 0.369 (0.453) data 0.238 (0.322) loss_u loss_u 0.8511 (0.8936) acc_u 15.6250 (13.3929) lr 1.9558e-03 eta 0:00:16
epoch [21/200] batch [40/72] time 0.497 (0.458) data 0.365 (0.327) loss_u loss_u 0.9404 (0.8942) acc_u 6.2500 (13.3594) lr 1.9558e-03 eta 0:00:14
epoch [21/200] batch [45/72] time 0.444 (0.461) data 0.312 (0.330) loss_u loss_u 0.8652 (0.8925) acc_u 18.7500 (13.8194) lr 1.9558e-03 eta 0:00:12
epoch [21/200] batch [50/72] time 0.331 (0.458) data 0.200 (0.327) loss_u loss_u 0.9204 (0.8960) acc_u 9.3750 (13.1250) lr 1.9558e-03 eta 0:00:10
epoch [21/200] batch [55/72] time 0.404 (0.454) data 0.272 (0.323) loss_u loss_u 0.9126 (0.8960) acc_u 9.3750 (13.2386) lr 1.9558e-03 eta 0:00:07
epoch [21/200] batch [60/72] time 0.504 (0.452) data 0.372 (0.321) loss_u loss_u 0.9175 (0.8964) acc_u 6.2500 (12.9688) lr 1.9558e-03 eta 0:00:05
epoch [21/200] batch [65/72] time 0.512 (0.450) data 0.381 (0.319) loss_u loss_u 0.9277 (0.8953) acc_u 9.3750 (13.2692) lr 1.9558e-03 eta 0:00:03
epoch [21/200] batch [70/72] time 0.456 (0.450) data 0.324 (0.319) loss_u loss_u 0.8955 (0.8935) acc_u 12.5000 (13.3036) lr 1.9558e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1729
confident_label rate tensor(0.2621, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 822
clean true:747
clean false:75
clean_rate:0.9087591240875912
noisy true:660
noisy false:1654
after delete: len(clean_dataset) 822
after delete: len(noisy_dataset) 2314
epoch [22/200] batch [5/25] time 0.377 (0.475) data 0.247 (0.345) loss_x loss_x 1.8672 (1.5314) acc_x 46.8750 (59.3750) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [10/25] time 0.397 (0.454) data 0.267 (0.323) loss_x loss_x 1.0400 (1.2916) acc_x 78.1250 (66.5625) lr 1.9511e-03 eta 0:00:06
epoch [22/200] batch [15/25] time 0.406 (0.453) data 0.276 (0.322) loss_x loss_x 1.3447 (1.3726) acc_x 50.0000 (64.1667) lr 1.9511e-03 eta 0:00:04
epoch [22/200] batch [20/25] time 0.407 (0.460) data 0.276 (0.330) loss_x loss_x 1.4326 (1.3635) acc_x 56.2500 (63.9062) lr 1.9511e-03 eta 0:00:02
epoch [22/200] batch [25/25] time 0.453 (0.463) data 0.323 (0.333) loss_x loss_x 1.9678 (1.3862) acc_x 59.3750 (63.6250) lr 1.9511e-03 eta 0:00:00
epoch [22/200] batch [5/72] time 0.400 (0.459) data 0.269 (0.328) loss_u loss_u 0.8872 (0.8857) acc_u 12.5000 (13.7500) lr 1.9511e-03 eta 0:00:30
epoch [22/200] batch [10/72] time 0.463 (0.455) data 0.331 (0.325) loss_u loss_u 0.8691 (0.8866) acc_u 15.6250 (13.4375) lr 1.9511e-03 eta 0:00:28
epoch [22/200] batch [15/72] time 0.472 (0.461) data 0.340 (0.330) loss_u loss_u 0.9341 (0.9000) acc_u 9.3750 (12.0833) lr 1.9511e-03 eta 0:00:26
epoch [22/200] batch [20/72] time 0.406 (0.454) data 0.276 (0.323) loss_u loss_u 0.9053 (0.8974) acc_u 15.6250 (12.0312) lr 1.9511e-03 eta 0:00:23
epoch [22/200] batch [25/72] time 0.397 (0.454) data 0.267 (0.323) loss_u loss_u 0.9404 (0.8977) acc_u 6.2500 (11.8750) lr 1.9511e-03 eta 0:00:21
epoch [22/200] batch [30/72] time 0.389 (0.446) data 0.259 (0.316) loss_u loss_u 0.9272 (0.8972) acc_u 6.2500 (12.0833) lr 1.9511e-03 eta 0:00:18
epoch [22/200] batch [35/72] time 0.375 (0.445) data 0.244 (0.315) loss_u loss_u 0.8848 (0.8930) acc_u 18.7500 (12.7679) lr 1.9511e-03 eta 0:00:16
epoch [22/200] batch [40/72] time 0.412 (0.441) data 0.282 (0.310) loss_u loss_u 0.8550 (0.8896) acc_u 15.6250 (13.6719) lr 1.9511e-03 eta 0:00:14
epoch [22/200] batch [45/72] time 0.368 (0.442) data 0.238 (0.312) loss_u loss_u 0.8740 (0.8911) acc_u 21.8750 (13.4722) lr 1.9511e-03 eta 0:00:11
epoch [22/200] batch [50/72] time 0.375 (0.438) data 0.244 (0.307) loss_u loss_u 0.9067 (0.8892) acc_u 15.6250 (14.0000) lr 1.9511e-03 eta 0:00:09
epoch [22/200] batch [55/72] time 0.556 (0.441) data 0.425 (0.310) loss_u loss_u 0.9189 (0.8909) acc_u 9.3750 (13.8636) lr 1.9511e-03 eta 0:00:07
epoch [22/200] batch [60/72] time 0.380 (0.441) data 0.248 (0.311) loss_u loss_u 0.9229 (0.8935) acc_u 3.1250 (13.2292) lr 1.9511e-03 eta 0:00:05
epoch [22/200] batch [65/72] time 0.448 (0.440) data 0.316 (0.310) loss_u loss_u 0.8618 (0.8929) acc_u 25.0000 (13.3173) lr 1.9511e-03 eta 0:00:03
epoch [22/200] batch [70/72] time 0.533 (0.444) data 0.403 (0.313) loss_u loss_u 0.8989 (0.8917) acc_u 9.3750 (13.4821) lr 1.9511e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1740
confident_label rate tensor(0.2707, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 849
clean true:761
clean false:88
clean_rate:0.8963486454652533
noisy true:635
noisy false:1652
after delete: len(clean_dataset) 849
after delete: len(noisy_dataset) 2287
epoch [23/200] batch [5/26] time 0.459 (0.504) data 0.328 (0.374) loss_x loss_x 0.9883 (1.4135) acc_x 68.7500 (60.6250) lr 1.9461e-03 eta 0:00:10
epoch [23/200] batch [10/26] time 0.376 (0.470) data 0.245 (0.340) loss_x loss_x 1.8945 (1.5009) acc_x 56.2500 (60.9375) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [15/26] time 0.498 (0.471) data 0.367 (0.341) loss_x loss_x 1.4941 (1.4822) acc_x 50.0000 (60.2083) lr 1.9461e-03 eta 0:00:05
epoch [23/200] batch [20/26] time 0.524 (0.471) data 0.393 (0.341) loss_x loss_x 1.3125 (1.4792) acc_x 71.8750 (61.0938) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [25/26] time 0.390 (0.466) data 0.260 (0.335) loss_x loss_x 1.7891 (1.4837) acc_x 59.3750 (61.6250) lr 1.9461e-03 eta 0:00:00
epoch [23/200] batch [5/71] time 0.360 (0.450) data 0.230 (0.319) loss_u loss_u 0.8257 (0.8796) acc_u 18.7500 (13.1250) lr 1.9461e-03 eta 0:00:29
epoch [23/200] batch [10/71] time 0.430 (0.444) data 0.299 (0.314) loss_u loss_u 0.9312 (0.8831) acc_u 9.3750 (13.1250) lr 1.9461e-03 eta 0:00:27
epoch [23/200] batch [15/71] time 0.458 (0.451) data 0.328 (0.320) loss_u loss_u 0.9170 (0.8846) acc_u 9.3750 (13.3333) lr 1.9461e-03 eta 0:00:25
epoch [23/200] batch [20/71] time 0.353 (0.444) data 0.223 (0.314) loss_u loss_u 0.9688 (0.8934) acc_u 3.1250 (12.5000) lr 1.9461e-03 eta 0:00:22
epoch [23/200] batch [25/71] time 0.503 (0.450) data 0.373 (0.319) loss_u loss_u 0.8774 (0.8930) acc_u 15.6250 (12.7500) lr 1.9461e-03 eta 0:00:20
epoch [23/200] batch [30/71] time 0.404 (0.451) data 0.270 (0.320) loss_u loss_u 0.8530 (0.8930) acc_u 21.8750 (13.3333) lr 1.9461e-03 eta 0:00:18
epoch [23/200] batch [35/71] time 0.420 (0.446) data 0.288 (0.315) loss_u loss_u 0.8960 (0.8946) acc_u 12.5000 (13.0357) lr 1.9461e-03 eta 0:00:16
epoch [23/200] batch [40/71] time 0.387 (0.448) data 0.256 (0.317) loss_u loss_u 0.8452 (0.8947) acc_u 21.8750 (13.1250) lr 1.9461e-03 eta 0:00:13
epoch [23/200] batch [45/71] time 0.474 (0.449) data 0.343 (0.318) loss_u loss_u 0.8960 (0.8932) acc_u 9.3750 (13.2639) lr 1.9461e-03 eta 0:00:11
epoch [23/200] batch [50/71] time 0.426 (0.449) data 0.295 (0.317) loss_u loss_u 0.8892 (0.8940) acc_u 12.5000 (13.0625) lr 1.9461e-03 eta 0:00:09
epoch [23/200] batch [55/71] time 0.452 (0.446) data 0.320 (0.315) loss_u loss_u 0.8994 (0.8955) acc_u 18.7500 (12.7841) lr 1.9461e-03 eta 0:00:07
epoch [23/200] batch [60/71] time 0.421 (0.445) data 0.290 (0.314) loss_u loss_u 0.9175 (0.8955) acc_u 12.5000 (12.8125) lr 1.9461e-03 eta 0:00:04
epoch [23/200] batch [65/71] time 0.399 (0.443) data 0.268 (0.312) loss_u loss_u 0.8730 (0.8953) acc_u 12.5000 (12.5962) lr 1.9461e-03 eta 0:00:02
epoch [23/200] batch [70/71] time 0.587 (0.446) data 0.456 (0.315) loss_u loss_u 0.8804 (0.8953) acc_u 15.6250 (12.5446) lr 1.9461e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1745
confident_label rate tensor(0.2564, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 804
clean true:728
clean false:76
clean_rate:0.9054726368159204
noisy true:663
noisy false:1669
after delete: len(clean_dataset) 804
after delete: len(noisy_dataset) 2332
epoch [24/200] batch [5/25] time 0.623 (0.471) data 0.493 (0.340) loss_x loss_x 1.1650 (1.6051) acc_x 59.3750 (61.2500) lr 1.9409e-03 eta 0:00:09
epoch [24/200] batch [10/25] time 0.454 (0.489) data 0.323 (0.359) loss_x loss_x 1.1943 (1.4701) acc_x 59.3750 (63.7500) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [15/25] time 0.353 (0.463) data 0.223 (0.332) loss_x loss_x 1.0928 (1.4255) acc_x 62.5000 (63.9583) lr 1.9409e-03 eta 0:00:04
epoch [24/200] batch [20/25] time 0.495 (0.456) data 0.365 (0.325) loss_x loss_x 1.3066 (1.4369) acc_x 68.7500 (64.0625) lr 1.9409e-03 eta 0:00:02
epoch [24/200] batch [25/25] time 0.542 (0.452) data 0.412 (0.322) loss_x loss_x 1.6494 (1.4380) acc_x 43.7500 (63.3750) lr 1.9409e-03 eta 0:00:00
epoch [24/200] batch [5/72] time 0.386 (0.453) data 0.255 (0.322) loss_u loss_u 0.8569 (0.8815) acc_u 21.8750 (14.3750) lr 1.9409e-03 eta 0:00:30
epoch [24/200] batch [10/72] time 0.484 (0.454) data 0.353 (0.324) loss_u loss_u 0.8286 (0.8872) acc_u 21.8750 (13.4375) lr 1.9409e-03 eta 0:00:28
epoch [24/200] batch [15/72] time 0.344 (0.446) data 0.213 (0.315) loss_u loss_u 0.8516 (0.8770) acc_u 15.6250 (14.7917) lr 1.9409e-03 eta 0:00:25
epoch [24/200] batch [20/72] time 0.385 (0.438) data 0.255 (0.308) loss_u loss_u 0.8823 (0.8806) acc_u 18.7500 (14.2188) lr 1.9409e-03 eta 0:00:22
epoch [24/200] batch [25/72] time 0.393 (0.439) data 0.262 (0.309) loss_u loss_u 0.8936 (0.8809) acc_u 12.5000 (14.2500) lr 1.9409e-03 eta 0:00:20
epoch [24/200] batch [30/72] time 0.423 (0.440) data 0.293 (0.309) loss_u loss_u 0.9028 (0.8846) acc_u 9.3750 (13.4375) lr 1.9409e-03 eta 0:00:18
epoch [24/200] batch [35/72] time 0.377 (0.440) data 0.246 (0.310) loss_u loss_u 0.9126 (0.8883) acc_u 6.2500 (12.7679) lr 1.9409e-03 eta 0:00:16
epoch [24/200] batch [40/72] time 0.469 (0.442) data 0.337 (0.312) loss_u loss_u 0.8896 (0.8875) acc_u 15.6250 (13.1250) lr 1.9409e-03 eta 0:00:14
epoch [24/200] batch [45/72] time 0.488 (0.440) data 0.357 (0.309) loss_u loss_u 0.9097 (0.8890) acc_u 12.5000 (13.1250) lr 1.9409e-03 eta 0:00:11
epoch [24/200] batch [50/72] time 0.452 (0.442) data 0.320 (0.311) loss_u loss_u 0.9370 (0.8908) acc_u 9.3750 (13.0625) lr 1.9409e-03 eta 0:00:09
epoch [24/200] batch [55/72] time 0.415 (0.441) data 0.284 (0.310) loss_u loss_u 0.8169 (0.8868) acc_u 21.8750 (13.5227) lr 1.9409e-03 eta 0:00:07
epoch [24/200] batch [60/72] time 0.420 (0.440) data 0.289 (0.309) loss_u loss_u 0.9229 (0.8846) acc_u 9.3750 (13.9062) lr 1.9409e-03 eta 0:00:05
epoch [24/200] batch [65/72] time 0.509 (0.439) data 0.377 (0.308) loss_u loss_u 0.8770 (0.8838) acc_u 12.5000 (13.9904) lr 1.9409e-03 eta 0:00:03
epoch [24/200] batch [70/72] time 0.485 (0.439) data 0.354 (0.308) loss_u loss_u 0.7866 (0.8830) acc_u 25.0000 (14.1071) lr 1.9409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1747
confident_label rate tensor(0.2643, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 829
clean true:743
clean false:86
clean_rate:0.8962605548854041
noisy true:646
noisy false:1661
after delete: len(clean_dataset) 829
after delete: len(noisy_dataset) 2307
epoch [25/200] batch [5/25] time 0.452 (0.473) data 0.321 (0.342) loss_x loss_x 1.7061 (1.6266) acc_x 65.6250 (61.2500) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [10/25] time 0.506 (0.468) data 0.375 (0.337) loss_x loss_x 1.5078 (1.5538) acc_x 62.5000 (61.2500) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [15/25] time 0.676 (0.474) data 0.546 (0.343) loss_x loss_x 1.3320 (1.4861) acc_x 53.1250 (61.8750) lr 1.9354e-03 eta 0:00:04
epoch [25/200] batch [20/25] time 0.525 (0.472) data 0.396 (0.341) loss_x loss_x 1.4697 (1.5030) acc_x 59.3750 (61.8750) lr 1.9354e-03 eta 0:00:02
epoch [25/200] batch [25/25] time 0.481 (0.469) data 0.351 (0.339) loss_x loss_x 1.4180 (1.5154) acc_x 56.2500 (61.1250) lr 1.9354e-03 eta 0:00:00
epoch [25/200] batch [5/72] time 0.532 (0.458) data 0.400 (0.327) loss_u loss_u 0.8711 (0.8961) acc_u 18.7500 (14.3750) lr 1.9354e-03 eta 0:00:30
epoch [25/200] batch [10/72] time 0.641 (0.458) data 0.509 (0.327) loss_u loss_u 0.9312 (0.9010) acc_u 9.3750 (13.7500) lr 1.9354e-03 eta 0:00:28
epoch [25/200] batch [15/72] time 0.548 (0.461) data 0.416 (0.330) loss_u loss_u 0.8926 (0.9008) acc_u 9.3750 (13.3333) lr 1.9354e-03 eta 0:00:26
epoch [25/200] batch [20/72] time 0.492 (0.458) data 0.361 (0.327) loss_u loss_u 0.8745 (0.8929) acc_u 18.7500 (14.0625) lr 1.9354e-03 eta 0:00:23
epoch [25/200] batch [25/72] time 0.507 (0.457) data 0.376 (0.326) loss_u loss_u 0.8384 (0.8909) acc_u 15.6250 (13.7500) lr 1.9354e-03 eta 0:00:21
epoch [25/200] batch [30/72] time 0.328 (0.452) data 0.198 (0.321) loss_u loss_u 0.7979 (0.8835) acc_u 28.1250 (15.2083) lr 1.9354e-03 eta 0:00:18
epoch [25/200] batch [35/72] time 0.314 (0.449) data 0.184 (0.318) loss_u loss_u 0.8828 (0.8822) acc_u 12.5000 (14.9107) lr 1.9354e-03 eta 0:00:16
epoch [25/200] batch [40/72] time 0.354 (0.444) data 0.223 (0.313) loss_u loss_u 0.8970 (0.8839) acc_u 9.3750 (14.3750) lr 1.9354e-03 eta 0:00:14
epoch [25/200] batch [45/72] time 0.539 (0.448) data 0.404 (0.317) loss_u loss_u 0.8857 (0.8855) acc_u 18.7500 (14.2361) lr 1.9354e-03 eta 0:00:12
epoch [25/200] batch [50/72] time 0.461 (0.450) data 0.330 (0.319) loss_u loss_u 0.8281 (0.8842) acc_u 21.8750 (14.3750) lr 1.9354e-03 eta 0:00:09
epoch [25/200] batch [55/72] time 0.462 (0.454) data 0.330 (0.323) loss_u loss_u 0.9038 (0.8848) acc_u 9.3750 (14.4886) lr 1.9354e-03 eta 0:00:07
epoch [25/200] batch [60/72] time 0.443 (0.456) data 0.311 (0.325) loss_u loss_u 0.8560 (0.8845) acc_u 18.7500 (14.5833) lr 1.9354e-03 eta 0:00:05
epoch [25/200] batch [65/72] time 0.326 (0.453) data 0.195 (0.322) loss_u loss_u 0.9531 (0.8846) acc_u 3.1250 (14.5192) lr 1.9354e-03 eta 0:00:03
epoch [25/200] batch [70/72] time 0.395 (0.454) data 0.263 (0.323) loss_u loss_u 0.9116 (0.8833) acc_u 15.6250 (14.5982) lr 1.9354e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1775
confident_label rate tensor(0.2628, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 824
clean true:732
clean false:92
clean_rate:0.8883495145631068
noisy true:629
noisy false:1683
after delete: len(clean_dataset) 824
after delete: len(noisy_dataset) 2312
epoch [26/200] batch [5/25] time 0.456 (0.493) data 0.326 (0.363) loss_x loss_x 1.6738 (1.6730) acc_x 50.0000 (60.0000) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [10/25] time 0.405 (0.482) data 0.275 (0.352) loss_x loss_x 1.8086 (1.5886) acc_x 50.0000 (61.5625) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [15/25] time 0.492 (0.490) data 0.362 (0.360) loss_x loss_x 2.0137 (1.5830) acc_x 46.8750 (61.2500) lr 1.9298e-03 eta 0:00:04
epoch [26/200] batch [20/25] time 0.397 (0.469) data 0.265 (0.338) loss_x loss_x 1.0352 (1.4977) acc_x 68.7500 (62.3438) lr 1.9298e-03 eta 0:00:02
epoch [26/200] batch [25/25] time 0.497 (0.468) data 0.366 (0.337) loss_x loss_x 1.3008 (1.4619) acc_x 65.6250 (62.6250) lr 1.9298e-03 eta 0:00:00
epoch [26/200] batch [5/72] time 0.574 (0.461) data 0.442 (0.330) loss_u loss_u 0.8276 (0.8585) acc_u 25.0000 (21.2500) lr 1.9298e-03 eta 0:00:30
epoch [26/200] batch [10/72] time 0.407 (0.454) data 0.275 (0.323) loss_u loss_u 0.8848 (0.8697) acc_u 15.6250 (18.4375) lr 1.9298e-03 eta 0:00:28
epoch [26/200] batch [15/72] time 0.336 (0.456) data 0.204 (0.325) loss_u loss_u 0.9180 (0.8762) acc_u 9.3750 (17.2917) lr 1.9298e-03 eta 0:00:25
epoch [26/200] batch [20/72] time 0.371 (0.450) data 0.241 (0.319) loss_u loss_u 0.8555 (0.8706) acc_u 15.6250 (17.1875) lr 1.9298e-03 eta 0:00:23
epoch [26/200] batch [25/72] time 0.406 (0.452) data 0.274 (0.322) loss_u loss_u 0.9238 (0.8781) acc_u 6.2500 (16.2500) lr 1.9298e-03 eta 0:00:21
epoch [26/200] batch [30/72] time 0.416 (0.451) data 0.285 (0.320) loss_u loss_u 0.9277 (0.8791) acc_u 9.3750 (16.3542) lr 1.9298e-03 eta 0:00:18
epoch [26/200] batch [35/72] time 0.429 (0.451) data 0.298 (0.320) loss_u loss_u 0.9077 (0.8842) acc_u 6.2500 (15.2679) lr 1.9298e-03 eta 0:00:16
epoch [26/200] batch [40/72] time 0.355 (0.447) data 0.224 (0.316) loss_u loss_u 0.8750 (0.8844) acc_u 15.6250 (15.4688) lr 1.9298e-03 eta 0:00:14
epoch [26/200] batch [45/72] time 0.475 (0.449) data 0.345 (0.318) loss_u loss_u 0.8525 (0.8838) acc_u 21.8750 (15.5556) lr 1.9298e-03 eta 0:00:12
epoch [26/200] batch [50/72] time 0.457 (0.448) data 0.327 (0.317) loss_u loss_u 0.9131 (0.8853) acc_u 12.5000 (15.0000) lr 1.9298e-03 eta 0:00:09
epoch [26/200] batch [55/72] time 0.384 (0.446) data 0.253 (0.315) loss_u loss_u 0.9062 (0.8863) acc_u 6.2500 (14.6591) lr 1.9298e-03 eta 0:00:07
epoch [26/200] batch [60/72] time 0.528 (0.446) data 0.396 (0.315) loss_u loss_u 0.8877 (0.8867) acc_u 9.3750 (14.4792) lr 1.9298e-03 eta 0:00:05
epoch [26/200] batch [65/72] time 0.440 (0.444) data 0.308 (0.313) loss_u loss_u 0.9111 (0.8869) acc_u 12.5000 (14.5673) lr 1.9298e-03 eta 0:00:03
epoch [26/200] batch [70/72] time 0.356 (0.444) data 0.224 (0.313) loss_u loss_u 0.9458 (0.8866) acc_u 6.2500 (14.5982) lr 1.9298e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1749
confident_label rate tensor(0.2669, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 837
clean true:747
clean false:90
clean_rate:0.8924731182795699
noisy true:640
noisy false:1659
after delete: len(clean_dataset) 837
after delete: len(noisy_dataset) 2299
epoch [27/200] batch [5/26] time 0.447 (0.522) data 0.316 (0.391) loss_x loss_x 1.2578 (1.2756) acc_x 65.6250 (66.2500) lr 1.9239e-03 eta 0:00:10
epoch [27/200] batch [10/26] time 0.437 (0.474) data 0.307 (0.344) loss_x loss_x 2.0801 (1.5751) acc_x 50.0000 (60.6250) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [15/26] time 0.358 (0.460) data 0.227 (0.329) loss_x loss_x 1.6992 (1.5255) acc_x 53.1250 (61.2500) lr 1.9239e-03 eta 0:00:05
epoch [27/200] batch [20/26] time 0.398 (0.456) data 0.268 (0.326) loss_x loss_x 1.9717 (1.5577) acc_x 50.0000 (60.0000) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [25/26] time 0.518 (0.469) data 0.388 (0.339) loss_x loss_x 2.1992 (1.5472) acc_x 65.6250 (61.0000) lr 1.9239e-03 eta 0:00:00
epoch [27/200] batch [5/71] time 0.517 (0.463) data 0.386 (0.332) loss_u loss_u 0.9116 (0.9051) acc_u 6.2500 (10.6250) lr 1.9239e-03 eta 0:00:30
epoch [27/200] batch [10/71] time 0.474 (0.460) data 0.344 (0.329) loss_u loss_u 0.8569 (0.9110) acc_u 21.8750 (9.6875) lr 1.9239e-03 eta 0:00:28
epoch [27/200] batch [15/71] time 0.726 (0.460) data 0.595 (0.329) loss_u loss_u 0.9053 (0.9070) acc_u 6.2500 (10.4167) lr 1.9239e-03 eta 0:00:25
epoch [27/200] batch [20/71] time 0.384 (0.451) data 0.252 (0.320) loss_u loss_u 0.9072 (0.9061) acc_u 9.3750 (10.6250) lr 1.9239e-03 eta 0:00:22
epoch [27/200] batch [25/71] time 0.418 (0.453) data 0.287 (0.322) loss_u loss_u 0.8462 (0.8966) acc_u 18.7500 (12.0000) lr 1.9239e-03 eta 0:00:20
epoch [27/200] batch [30/71] time 0.406 (0.450) data 0.275 (0.319) loss_u loss_u 0.8765 (0.8891) acc_u 15.6250 (13.3333) lr 1.9239e-03 eta 0:00:18
epoch [27/200] batch [35/71] time 0.417 (0.450) data 0.285 (0.320) loss_u loss_u 0.8662 (0.8878) acc_u 12.5000 (13.3036) lr 1.9239e-03 eta 0:00:16
epoch [27/200] batch [40/71] time 0.401 (0.452) data 0.270 (0.321) loss_u loss_u 0.8599 (0.8859) acc_u 18.7500 (13.5156) lr 1.9239e-03 eta 0:00:14
epoch [27/200] batch [45/71] time 0.575 (0.453) data 0.443 (0.322) loss_u loss_u 0.7925 (0.8837) acc_u 31.2500 (14.3056) lr 1.9239e-03 eta 0:00:11
epoch [27/200] batch [50/71] time 0.372 (0.450) data 0.241 (0.319) loss_u loss_u 0.9321 (0.8856) acc_u 3.1250 (14.1250) lr 1.9239e-03 eta 0:00:09
epoch [27/200] batch [55/71] time 0.475 (0.449) data 0.343 (0.318) loss_u loss_u 0.8613 (0.8852) acc_u 18.7500 (14.2045) lr 1.9239e-03 eta 0:00:07
epoch [27/200] batch [60/71] time 0.586 (0.452) data 0.454 (0.320) loss_u loss_u 0.8589 (0.8840) acc_u 12.5000 (14.2708) lr 1.9239e-03 eta 0:00:04
epoch [27/200] batch [65/71] time 0.457 (0.448) data 0.326 (0.317) loss_u loss_u 0.8462 (0.8839) acc_u 21.8750 (14.4231) lr 1.9239e-03 eta 0:00:02
epoch [27/200] batch [70/71] time 0.406 (0.448) data 0.274 (0.317) loss_u loss_u 0.9102 (0.8846) acc_u 6.2500 (14.1071) lr 1.9239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1739
confident_label rate tensor(0.2656, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 833
clean true:743
clean false:90
clean_rate:0.8919567827130852
noisy true:654
noisy false:1649
after delete: len(clean_dataset) 833
after delete: len(noisy_dataset) 2303
epoch [28/200] batch [5/26] time 0.551 (0.514) data 0.421 (0.383) loss_x loss_x 1.4189 (1.4602) acc_x 53.1250 (56.8750) lr 1.9178e-03 eta 0:00:10
epoch [28/200] batch [10/26] time 0.400 (0.483) data 0.270 (0.352) loss_x loss_x 1.6670 (1.4242) acc_x 62.5000 (60.0000) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [15/26] time 0.389 (0.485) data 0.259 (0.355) loss_x loss_x 1.4727 (1.4659) acc_x 65.6250 (59.7917) lr 1.9178e-03 eta 0:00:05
epoch [28/200] batch [20/26] time 0.592 (0.482) data 0.461 (0.352) loss_x loss_x 1.6680 (1.5147) acc_x 37.5000 (58.2812) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [25/26] time 0.424 (0.484) data 0.294 (0.353) loss_x loss_x 0.9824 (1.4807) acc_x 75.0000 (59.8750) lr 1.9178e-03 eta 0:00:00
epoch [28/200] batch [5/71] time 0.460 (0.483) data 0.328 (0.352) loss_u loss_u 0.8398 (0.8809) acc_u 18.7500 (15.0000) lr 1.9178e-03 eta 0:00:31
epoch [28/200] batch [10/71] time 0.516 (0.477) data 0.385 (0.347) loss_u loss_u 0.8506 (0.8729) acc_u 21.8750 (16.5625) lr 1.9178e-03 eta 0:00:29
epoch [28/200] batch [15/71] time 0.403 (0.472) data 0.271 (0.341) loss_u loss_u 0.9204 (0.8733) acc_u 9.3750 (16.8750) lr 1.9178e-03 eta 0:00:26
epoch [28/200] batch [20/71] time 0.480 (0.464) data 0.349 (0.333) loss_u loss_u 0.9082 (0.8788) acc_u 9.3750 (15.9375) lr 1.9178e-03 eta 0:00:23
epoch [28/200] batch [25/71] time 0.396 (0.460) data 0.265 (0.329) loss_u loss_u 0.8384 (0.8737) acc_u 18.7500 (16.2500) lr 1.9178e-03 eta 0:00:21
epoch [28/200] batch [30/71] time 0.423 (0.454) data 0.292 (0.323) loss_u loss_u 0.8901 (0.8766) acc_u 12.5000 (15.6250) lr 1.9178e-03 eta 0:00:18
epoch [28/200] batch [35/71] time 0.380 (0.454) data 0.248 (0.323) loss_u loss_u 0.8433 (0.8790) acc_u 25.0000 (15.3571) lr 1.9178e-03 eta 0:00:16
epoch [28/200] batch [40/71] time 0.470 (0.453) data 0.338 (0.322) loss_u loss_u 0.8569 (0.8792) acc_u 12.5000 (15.0781) lr 1.9178e-03 eta 0:00:14
epoch [28/200] batch [45/71] time 0.453 (0.454) data 0.322 (0.322) loss_u loss_u 0.8247 (0.8794) acc_u 28.1250 (15.1389) lr 1.9178e-03 eta 0:00:11
epoch [28/200] batch [50/71] time 0.427 (0.453) data 0.296 (0.322) loss_u loss_u 0.9165 (0.8820) acc_u 6.2500 (14.5000) lr 1.9178e-03 eta 0:00:09
epoch [28/200] batch [55/71] time 0.391 (0.450) data 0.260 (0.319) loss_u loss_u 0.9321 (0.8847) acc_u 9.3750 (14.2614) lr 1.9178e-03 eta 0:00:07
epoch [28/200] batch [60/71] time 0.439 (0.451) data 0.307 (0.320) loss_u loss_u 0.8525 (0.8854) acc_u 25.0000 (14.2708) lr 1.9178e-03 eta 0:00:04
epoch [28/200] batch [65/71] time 0.498 (0.451) data 0.366 (0.320) loss_u loss_u 0.8809 (0.8845) acc_u 18.7500 (14.4712) lr 1.9178e-03 eta 0:00:02
epoch [28/200] batch [70/71] time 0.438 (0.452) data 0.306 (0.321) loss_u loss_u 0.8892 (0.8833) acc_u 12.5000 (14.7321) lr 1.9178e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1702
confident_label rate tensor(0.2698, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 846
clean true:764
clean false:82
clean_rate:0.9030732860520094
noisy true:670
noisy false:1620
after delete: len(clean_dataset) 846
after delete: len(noisy_dataset) 2290
epoch [29/200] batch [5/26] time 0.405 (0.432) data 0.275 (0.302) loss_x loss_x 1.4404 (1.4414) acc_x 59.3750 (60.0000) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [10/26] time 0.442 (0.421) data 0.312 (0.290) loss_x loss_x 0.8877 (1.3548) acc_x 68.7500 (64.6875) lr 1.9114e-03 eta 0:00:06
epoch [29/200] batch [15/26] time 0.402 (0.429) data 0.270 (0.298) loss_x loss_x 1.5576 (1.4813) acc_x 68.7500 (61.8750) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [20/26] time 0.401 (0.443) data 0.270 (0.313) loss_x loss_x 1.9805 (1.4986) acc_x 53.1250 (61.0938) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [25/26] time 0.515 (0.443) data 0.384 (0.312) loss_x loss_x 1.3008 (1.5068) acc_x 65.6250 (61.0000) lr 1.9114e-03 eta 0:00:00
epoch [29/200] batch [5/71] time 0.435 (0.442) data 0.305 (0.311) loss_u loss_u 0.9248 (0.8995) acc_u 6.2500 (12.5000) lr 1.9114e-03 eta 0:00:29
epoch [29/200] batch [10/71] time 0.462 (0.437) data 0.331 (0.307) loss_u loss_u 0.8691 (0.8939) acc_u 15.6250 (13.1250) lr 1.9114e-03 eta 0:00:26
epoch [29/200] batch [15/71] time 0.467 (0.443) data 0.336 (0.312) loss_u loss_u 0.8672 (0.9025) acc_u 12.5000 (11.8750) lr 1.9114e-03 eta 0:00:24
epoch [29/200] batch [20/71] time 0.470 (0.440) data 0.339 (0.310) loss_u loss_u 0.8623 (0.8998) acc_u 15.6250 (11.8750) lr 1.9114e-03 eta 0:00:22
epoch [29/200] batch [25/71] time 0.413 (0.440) data 0.283 (0.309) loss_u loss_u 0.9160 (0.8988) acc_u 6.2500 (11.6250) lr 1.9114e-03 eta 0:00:20
epoch [29/200] batch [30/71] time 0.344 (0.436) data 0.214 (0.305) loss_u loss_u 0.9009 (0.9015) acc_u 12.5000 (11.4583) lr 1.9114e-03 eta 0:00:17
epoch [29/200] batch [35/71] time 0.537 (0.437) data 0.406 (0.306) loss_u loss_u 0.9058 (0.8945) acc_u 18.7500 (12.5893) lr 1.9114e-03 eta 0:00:15
epoch [29/200] batch [40/71] time 0.460 (0.440) data 0.329 (0.310) loss_u loss_u 0.9316 (0.8941) acc_u 9.3750 (12.7344) lr 1.9114e-03 eta 0:00:13
epoch [29/200] batch [45/71] time 0.711 (0.449) data 0.580 (0.319) loss_u loss_u 0.8511 (0.8933) acc_u 18.7500 (12.5694) lr 1.9114e-03 eta 0:00:11
epoch [29/200] batch [50/71] time 0.370 (0.445) data 0.239 (0.315) loss_u loss_u 0.9272 (0.8938) acc_u 6.2500 (12.3125) lr 1.9114e-03 eta 0:00:09
epoch [29/200] batch [55/71] time 0.475 (0.448) data 0.345 (0.318) loss_u loss_u 0.9189 (0.8938) acc_u 9.3750 (12.3295) lr 1.9114e-03 eta 0:00:07
epoch [29/200] batch [60/71] time 0.377 (0.446) data 0.246 (0.315) loss_u loss_u 0.8828 (0.8969) acc_u 18.7500 (12.0312) lr 1.9114e-03 eta 0:00:04
epoch [29/200] batch [65/71] time 0.386 (0.444) data 0.254 (0.313) loss_u loss_u 0.9243 (0.8966) acc_u 9.3750 (12.1635) lr 1.9114e-03 eta 0:00:02
epoch [29/200] batch [70/71] time 0.367 (0.442) data 0.236 (0.311) loss_u loss_u 0.8516 (0.8949) acc_u 18.7500 (12.4554) lr 1.9114e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1710
confident_label rate tensor(0.2723, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 854
clean true:768
clean false:86
clean_rate:0.8992974238875878
noisy true:658
noisy false:1624
after delete: len(clean_dataset) 854
after delete: len(noisy_dataset) 2282
epoch [30/200] batch [5/26] time 0.612 (0.468) data 0.482 (0.336) loss_x loss_x 1.8037 (1.5938) acc_x 46.8750 (54.3750) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [10/26] time 0.383 (0.439) data 0.253 (0.308) loss_x loss_x 1.2988 (1.4442) acc_x 53.1250 (60.0000) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [15/26] time 0.404 (0.443) data 0.274 (0.312) loss_x loss_x 2.1562 (1.4260) acc_x 59.3750 (62.0833) lr 1.9048e-03 eta 0:00:04
epoch [30/200] batch [20/26] time 0.589 (0.446) data 0.458 (0.315) loss_x loss_x 1.4785 (1.4514) acc_x 65.6250 (62.5000) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [25/26] time 0.573 (0.456) data 0.442 (0.325) loss_x loss_x 1.7363 (1.4213) acc_x 59.3750 (63.6250) lr 1.9048e-03 eta 0:00:00
epoch [30/200] batch [5/71] time 0.430 (0.469) data 0.298 (0.338) loss_u loss_u 0.9648 (0.9038) acc_u 3.1250 (11.2500) lr 1.9048e-03 eta 0:00:30
epoch [30/200] batch [10/71] time 0.411 (0.462) data 0.278 (0.331) loss_u loss_u 0.9351 (0.9047) acc_u 6.2500 (10.6250) lr 1.9048e-03 eta 0:00:28
epoch [30/200] batch [15/71] time 0.454 (0.470) data 0.322 (0.339) loss_u loss_u 0.8350 (0.8886) acc_u 25.0000 (13.5417) lr 1.9048e-03 eta 0:00:26
epoch [30/200] batch [20/71] time 0.514 (0.477) data 0.384 (0.346) loss_u loss_u 0.8774 (0.8870) acc_u 18.7500 (14.2188) lr 1.9048e-03 eta 0:00:24
epoch [30/200] batch [25/71] time 0.390 (0.473) data 0.259 (0.341) loss_u loss_u 0.8369 (0.8862) acc_u 18.7500 (14.3750) lr 1.9048e-03 eta 0:00:21
epoch [30/200] batch [30/71] time 0.412 (0.466) data 0.282 (0.335) loss_u loss_u 0.8521 (0.8816) acc_u 18.7500 (15.2083) lr 1.9048e-03 eta 0:00:19
epoch [30/200] batch [35/71] time 0.421 (0.461) data 0.290 (0.330) loss_u loss_u 0.9619 (0.8843) acc_u 3.1250 (14.7321) lr 1.9048e-03 eta 0:00:16
epoch [30/200] batch [40/71] time 0.406 (0.458) data 0.276 (0.326) loss_u loss_u 0.8794 (0.8866) acc_u 18.7500 (14.3750) lr 1.9048e-03 eta 0:00:14
epoch [30/200] batch [45/71] time 0.480 (0.458) data 0.349 (0.327) loss_u loss_u 0.8682 (0.8848) acc_u 18.7500 (14.5139) lr 1.9048e-03 eta 0:00:11
epoch [30/200] batch [50/71] time 0.501 (0.458) data 0.369 (0.327) loss_u loss_u 0.8843 (0.8861) acc_u 9.3750 (14.1250) lr 1.9048e-03 eta 0:00:09
epoch [30/200] batch [55/71] time 0.472 (0.457) data 0.337 (0.326) loss_u loss_u 0.9355 (0.8882) acc_u 3.1250 (13.6364) lr 1.9048e-03 eta 0:00:07
epoch [30/200] batch [60/71] time 0.489 (0.456) data 0.358 (0.324) loss_u loss_u 0.8447 (0.8872) acc_u 15.6250 (13.8021) lr 1.9048e-03 eta 0:00:05
epoch [30/200] batch [65/71] time 0.414 (0.454) data 0.283 (0.322) loss_u loss_u 0.9326 (0.8879) acc_u 9.3750 (13.7981) lr 1.9048e-03 eta 0:00:02
epoch [30/200] batch [70/71] time 0.562 (0.455) data 0.431 (0.324) loss_u loss_u 0.8975 (0.8857) acc_u 9.3750 (14.0625) lr 1.9048e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1699
confident_label rate tensor(0.2771, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 869
clean true:781
clean false:88
clean_rate:0.8987341772151899
noisy true:656
noisy false:1611
after delete: len(clean_dataset) 869
after delete: len(noisy_dataset) 2267
epoch [31/200] batch [5/27] time 0.708 (0.517) data 0.575 (0.385) loss_x loss_x 1.3594 (1.3467) acc_x 56.2500 (61.2500) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [10/27] time 0.427 (0.469) data 0.296 (0.338) loss_x loss_x 1.3311 (1.3404) acc_x 68.7500 (65.6250) lr 1.8980e-03 eta 0:00:07
epoch [31/200] batch [15/27] time 0.548 (0.479) data 0.418 (0.348) loss_x loss_x 1.2021 (1.3134) acc_x 68.7500 (66.2500) lr 1.8980e-03 eta 0:00:05
epoch [31/200] batch [20/27] time 0.412 (0.466) data 0.282 (0.335) loss_x loss_x 1.0391 (1.3543) acc_x 75.0000 (64.8438) lr 1.8980e-03 eta 0:00:03
epoch [31/200] batch [25/27] time 0.576 (0.471) data 0.446 (0.340) loss_x loss_x 1.5703 (1.3511) acc_x 53.1250 (64.3750) lr 1.8980e-03 eta 0:00:00
epoch [31/200] batch [5/70] time 0.584 (0.471) data 0.452 (0.340) loss_u loss_u 0.9111 (0.9057) acc_u 9.3750 (10.0000) lr 1.8980e-03 eta 0:00:30
epoch [31/200] batch [10/70] time 0.534 (0.462) data 0.403 (0.331) loss_u loss_u 0.8999 (0.8841) acc_u 12.5000 (12.8125) lr 1.8980e-03 eta 0:00:27
epoch [31/200] batch [15/70] time 0.373 (0.459) data 0.241 (0.328) loss_u loss_u 0.8765 (0.8913) acc_u 15.6250 (12.9167) lr 1.8980e-03 eta 0:00:25
epoch [31/200] batch [20/70] time 0.404 (0.462) data 0.274 (0.331) loss_u loss_u 0.8623 (0.8888) acc_u 15.6250 (13.4375) lr 1.8980e-03 eta 0:00:23
epoch [31/200] batch [25/70] time 0.452 (0.461) data 0.320 (0.330) loss_u loss_u 0.8965 (0.8899) acc_u 9.3750 (13.6250) lr 1.8980e-03 eta 0:00:20
epoch [31/200] batch [30/70] time 0.370 (0.458) data 0.240 (0.327) loss_u loss_u 0.8711 (0.8928) acc_u 25.0000 (13.4375) lr 1.8980e-03 eta 0:00:18
epoch [31/200] batch [35/70] time 0.424 (0.454) data 0.293 (0.323) loss_u loss_u 0.9517 (0.8960) acc_u 9.3750 (13.0357) lr 1.8980e-03 eta 0:00:15
epoch [31/200] batch [40/70] time 0.363 (0.450) data 0.233 (0.319) loss_u loss_u 0.8687 (0.8954) acc_u 15.6250 (12.8125) lr 1.8980e-03 eta 0:00:13
epoch [31/200] batch [45/70] time 0.562 (0.449) data 0.431 (0.318) loss_u loss_u 0.9141 (0.8960) acc_u 9.3750 (12.7083) lr 1.8980e-03 eta 0:00:11
epoch [31/200] batch [50/70] time 0.389 (0.448) data 0.257 (0.317) loss_u loss_u 0.8921 (0.8932) acc_u 15.6250 (13.0000) lr 1.8980e-03 eta 0:00:08
epoch [31/200] batch [55/70] time 0.402 (0.444) data 0.271 (0.313) loss_u loss_u 0.9385 (0.8932) acc_u 6.2500 (12.8977) lr 1.8980e-03 eta 0:00:06
epoch [31/200] batch [60/70] time 0.329 (0.446) data 0.199 (0.315) loss_u loss_u 0.8105 (0.8940) acc_u 15.6250 (12.7083) lr 1.8980e-03 eta 0:00:04
epoch [31/200] batch [65/70] time 0.453 (0.446) data 0.321 (0.315) loss_u loss_u 0.8838 (0.8935) acc_u 18.7500 (12.8846) lr 1.8980e-03 eta 0:00:02
epoch [31/200] batch [70/70] time 0.456 (0.446) data 0.326 (0.315) loss_u loss_u 0.8613 (0.8917) acc_u 12.5000 (12.9911) lr 1.8980e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1729
confident_label rate tensor(0.2730, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 856
clean true:771
clean false:85
clean_rate:0.9007009345794392
noisy true:636
noisy false:1644
after delete: len(clean_dataset) 856
after delete: len(noisy_dataset) 2280
epoch [32/200] batch [5/26] time 0.551 (0.590) data 0.419 (0.459) loss_x loss_x 0.6870 (1.3938) acc_x 78.1250 (66.2500) lr 1.8910e-03 eta 0:00:12
epoch [32/200] batch [10/26] time 0.578 (0.514) data 0.447 (0.383) loss_x loss_x 1.6943 (1.4198) acc_x 62.5000 (63.1250) lr 1.8910e-03 eta 0:00:08
epoch [32/200] batch [15/26] time 0.574 (0.496) data 0.442 (0.365) loss_x loss_x 1.3730 (1.4476) acc_x 62.5000 (62.7083) lr 1.8910e-03 eta 0:00:05
epoch [32/200] batch [20/26] time 0.476 (0.484) data 0.345 (0.353) loss_x loss_x 1.6982 (1.4638) acc_x 59.3750 (62.6562) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [25/26] time 0.466 (0.480) data 0.335 (0.349) loss_x loss_x 1.7344 (1.4950) acc_x 50.0000 (62.2500) lr 1.8910e-03 eta 0:00:00
epoch [32/200] batch [5/71] time 0.369 (0.469) data 0.239 (0.338) loss_u loss_u 0.8408 (0.8705) acc_u 21.8750 (16.8750) lr 1.8910e-03 eta 0:00:30
epoch [32/200] batch [10/71] time 0.459 (0.460) data 0.327 (0.329) loss_u loss_u 0.8838 (0.8750) acc_u 12.5000 (15.0000) lr 1.8910e-03 eta 0:00:28
epoch [32/200] batch [15/71] time 0.569 (0.460) data 0.437 (0.329) loss_u loss_u 0.9355 (0.8844) acc_u 6.2500 (13.7500) lr 1.8910e-03 eta 0:00:25
epoch [32/200] batch [20/71] time 0.392 (0.452) data 0.260 (0.321) loss_u loss_u 0.7988 (0.8840) acc_u 34.3750 (15.0000) lr 1.8910e-03 eta 0:00:23
epoch [32/200] batch [25/71] time 0.306 (0.449) data 0.174 (0.318) loss_u loss_u 0.9194 (0.8861) acc_u 12.5000 (14.6250) lr 1.8910e-03 eta 0:00:20
epoch [32/200] batch [30/71] time 0.465 (0.455) data 0.334 (0.324) loss_u loss_u 0.8525 (0.8864) acc_u 21.8750 (15.0000) lr 1.8910e-03 eta 0:00:18
epoch [32/200] batch [35/71] time 0.446 (0.457) data 0.314 (0.325) loss_u loss_u 0.7979 (0.8863) acc_u 28.1250 (14.9107) lr 1.8910e-03 eta 0:00:16
epoch [32/200] batch [40/71] time 0.665 (0.460) data 0.533 (0.328) loss_u loss_u 0.8452 (0.8860) acc_u 18.7500 (14.6094) lr 1.8910e-03 eta 0:00:14
epoch [32/200] batch [45/71] time 0.478 (0.456) data 0.347 (0.325) loss_u loss_u 0.8394 (0.8854) acc_u 18.7500 (14.5139) lr 1.8910e-03 eta 0:00:11
epoch [32/200] batch [50/71] time 0.545 (0.456) data 0.414 (0.324) loss_u loss_u 0.9395 (0.8868) acc_u 6.2500 (14.1250) lr 1.8910e-03 eta 0:00:09
epoch [32/200] batch [55/71] time 0.331 (0.450) data 0.200 (0.319) loss_u loss_u 0.9126 (0.8885) acc_u 12.5000 (14.0909) lr 1.8910e-03 eta 0:00:07
epoch [32/200] batch [60/71] time 0.447 (0.450) data 0.316 (0.318) loss_u loss_u 0.8784 (0.8871) acc_u 18.7500 (14.1667) lr 1.8910e-03 eta 0:00:04
epoch [32/200] batch [65/71] time 0.357 (0.447) data 0.226 (0.316) loss_u loss_u 0.9492 (0.8873) acc_u 12.5000 (14.3269) lr 1.8910e-03 eta 0:00:02
epoch [32/200] batch [70/71] time 0.406 (0.445) data 0.274 (0.314) loss_u loss_u 0.8501 (0.8863) acc_u 18.7500 (14.5089) lr 1.8910e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1700
confident_label rate tensor(0.2774, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 870
clean true:776
clean false:94
clean_rate:0.8919540229885058
noisy true:660
noisy false:1606
after delete: len(clean_dataset) 870
after delete: len(noisy_dataset) 2266
epoch [33/200] batch [5/27] time 0.449 (0.426) data 0.317 (0.294) loss_x loss_x 1.1455 (1.2373) acc_x 71.8750 (66.8750) lr 1.8838e-03 eta 0:00:09
epoch [33/200] batch [10/27] time 0.478 (0.421) data 0.346 (0.289) loss_x loss_x 1.1641 (1.3750) acc_x 78.1250 (63.1250) lr 1.8838e-03 eta 0:00:07
epoch [33/200] batch [15/27] time 0.409 (0.432) data 0.279 (0.301) loss_x loss_x 1.0244 (1.3824) acc_x 65.6250 (62.7083) lr 1.8838e-03 eta 0:00:05
epoch [33/200] batch [20/27] time 0.453 (0.433) data 0.322 (0.302) loss_x loss_x 1.8623 (1.3973) acc_x 59.3750 (62.3438) lr 1.8838e-03 eta 0:00:03
epoch [33/200] batch [25/27] time 0.392 (0.423) data 0.262 (0.292) loss_x loss_x 1.4844 (1.3916) acc_x 62.5000 (62.6250) lr 1.8838e-03 eta 0:00:00
epoch [33/200] batch [5/70] time 0.605 (0.431) data 0.473 (0.300) loss_u loss_u 0.9092 (0.9111) acc_u 9.3750 (9.3750) lr 1.8838e-03 eta 0:00:28
epoch [33/200] batch [10/70] time 0.572 (0.439) data 0.441 (0.309) loss_u loss_u 0.8804 (0.8971) acc_u 15.6250 (12.1875) lr 1.8838e-03 eta 0:00:26
epoch [33/200] batch [15/70] time 0.506 (0.438) data 0.374 (0.307) loss_u loss_u 0.9541 (0.8994) acc_u 6.2500 (11.8750) lr 1.8838e-03 eta 0:00:24
epoch [33/200] batch [20/70] time 0.435 (0.443) data 0.305 (0.312) loss_u loss_u 0.8311 (0.8942) acc_u 18.7500 (12.6562) lr 1.8838e-03 eta 0:00:22
epoch [33/200] batch [25/70] time 0.424 (0.446) data 0.293 (0.315) loss_u loss_u 0.8979 (0.8952) acc_u 9.3750 (12.0000) lr 1.8838e-03 eta 0:00:20
epoch [33/200] batch [30/70] time 0.358 (0.447) data 0.226 (0.316) loss_u loss_u 0.8257 (0.8900) acc_u 18.7500 (13.0208) lr 1.8838e-03 eta 0:00:17
epoch [33/200] batch [35/70] time 0.393 (0.449) data 0.259 (0.318) loss_u loss_u 0.9038 (0.8907) acc_u 12.5000 (13.1250) lr 1.8838e-03 eta 0:00:15
epoch [33/200] batch [40/70] time 0.367 (0.445) data 0.233 (0.314) loss_u loss_u 0.9272 (0.8920) acc_u 9.3750 (12.6562) lr 1.8838e-03 eta 0:00:13
epoch [33/200] batch [45/70] time 0.381 (0.449) data 0.247 (0.318) loss_u loss_u 0.9048 (0.8895) acc_u 9.3750 (13.0556) lr 1.8838e-03 eta 0:00:11
epoch [33/200] batch [50/70] time 0.500 (0.448) data 0.367 (0.317) loss_u loss_u 0.8916 (0.8914) acc_u 12.5000 (13.0000) lr 1.8838e-03 eta 0:00:08
epoch [33/200] batch [55/70] time 0.390 (0.446) data 0.256 (0.314) loss_u loss_u 0.9370 (0.8936) acc_u 9.3750 (12.7841) lr 1.8838e-03 eta 0:00:06
epoch [33/200] batch [60/70] time 0.421 (0.445) data 0.288 (0.314) loss_u loss_u 0.8213 (0.8903) acc_u 18.7500 (13.2812) lr 1.8838e-03 eta 0:00:04
epoch [33/200] batch [65/70] time 0.366 (0.444) data 0.233 (0.312) loss_u loss_u 0.9360 (0.8910) acc_u 12.5000 (13.1731) lr 1.8838e-03 eta 0:00:02
epoch [33/200] batch [70/70] time 0.398 (0.442) data 0.265 (0.310) loss_u loss_u 0.8926 (0.8909) acc_u 12.5000 (13.2589) lr 1.8838e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1708
confident_label rate tensor(0.2698, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 846
clean true:764
clean false:82
clean_rate:0.9030732860520094
noisy true:664
noisy false:1626
after delete: len(clean_dataset) 846
after delete: len(noisy_dataset) 2290
epoch [34/200] batch [5/26] time 0.453 (0.488) data 0.323 (0.357) loss_x loss_x 1.4424 (1.2158) acc_x 65.6250 (67.5000) lr 1.8763e-03 eta 0:00:10
epoch [34/200] batch [10/26] time 0.579 (0.475) data 0.448 (0.344) loss_x loss_x 1.4883 (1.2814) acc_x 56.2500 (65.6250) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [15/26] time 0.488 (0.485) data 0.358 (0.354) loss_x loss_x 1.2227 (1.2993) acc_x 65.6250 (66.2500) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [20/26] time 0.540 (0.493) data 0.408 (0.362) loss_x loss_x 1.5117 (1.3658) acc_x 62.5000 (65.0000) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [25/26] time 0.480 (0.491) data 0.349 (0.360) loss_x loss_x 1.8379 (1.3438) acc_x 62.5000 (65.8750) lr 1.8763e-03 eta 0:00:00
epoch [34/200] batch [5/71] time 0.386 (0.474) data 0.255 (0.343) loss_u loss_u 0.8633 (0.9044) acc_u 12.5000 (12.5000) lr 1.8763e-03 eta 0:00:31
epoch [34/200] batch [10/71] time 0.423 (0.466) data 0.292 (0.335) loss_u loss_u 0.8745 (0.8978) acc_u 9.3750 (11.2500) lr 1.8763e-03 eta 0:00:28
epoch [34/200] batch [15/71] time 0.438 (0.459) data 0.306 (0.328) loss_u loss_u 0.9390 (0.8872) acc_u 6.2500 (12.0833) lr 1.8763e-03 eta 0:00:25
epoch [34/200] batch [20/71] time 0.396 (0.456) data 0.263 (0.325) loss_u loss_u 0.8643 (0.8862) acc_u 15.6250 (12.6562) lr 1.8763e-03 eta 0:00:23
epoch [34/200] batch [25/71] time 0.325 (0.463) data 0.193 (0.332) loss_u loss_u 0.8799 (0.8926) acc_u 18.7500 (11.8750) lr 1.8763e-03 eta 0:00:21
epoch [34/200] batch [30/71] time 0.351 (0.463) data 0.220 (0.332) loss_u loss_u 0.9482 (0.8918) acc_u 9.3750 (12.3958) lr 1.8763e-03 eta 0:00:18
epoch [34/200] batch [35/71] time 0.424 (0.457) data 0.294 (0.326) loss_u loss_u 0.9131 (0.8897) acc_u 6.2500 (12.2321) lr 1.8763e-03 eta 0:00:16
epoch [34/200] batch [40/71] time 0.602 (0.465) data 0.471 (0.333) loss_u loss_u 0.9199 (0.8922) acc_u 12.5000 (12.1875) lr 1.8763e-03 eta 0:00:14
epoch [34/200] batch [45/71] time 0.488 (0.466) data 0.357 (0.335) loss_u loss_u 0.9209 (0.8941) acc_u 12.5000 (12.3611) lr 1.8763e-03 eta 0:00:12
epoch [34/200] batch [50/71] time 0.706 (0.469) data 0.574 (0.338) loss_u loss_u 0.8350 (0.8911) acc_u 15.6250 (12.8750) lr 1.8763e-03 eta 0:00:09
epoch [34/200] batch [55/71] time 0.477 (0.466) data 0.345 (0.335) loss_u loss_u 0.9331 (0.8912) acc_u 3.1250 (12.6136) lr 1.8763e-03 eta 0:00:07
epoch [34/200] batch [60/71] time 0.425 (0.463) data 0.293 (0.331) loss_u loss_u 0.9390 (0.8897) acc_u 6.2500 (12.6562) lr 1.8763e-03 eta 0:00:05
epoch [34/200] batch [65/71] time 0.514 (0.462) data 0.383 (0.331) loss_u loss_u 0.9126 (0.8893) acc_u 6.2500 (12.5962) lr 1.8763e-03 eta 0:00:02
epoch [34/200] batch [70/71] time 0.586 (0.465) data 0.454 (0.333) loss_u loss_u 0.9136 (0.8876) acc_u 3.1250 (12.7232) lr 1.8763e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1640
confident_label rate tensor(0.2905, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 911
clean true:824
clean false:87
clean_rate:0.9045005488474204
noisy true:672
noisy false:1553
after delete: len(clean_dataset) 911
after delete: len(noisy_dataset) 2225
epoch [35/200] batch [5/28] time 0.366 (0.440) data 0.235 (0.309) loss_x loss_x 1.5117 (1.3344) acc_x 62.5000 (71.2500) lr 1.8686e-03 eta 0:00:10
epoch [35/200] batch [10/28] time 0.608 (0.454) data 0.477 (0.324) loss_x loss_x 1.1699 (1.3385) acc_x 62.5000 (66.2500) lr 1.8686e-03 eta 0:00:08
epoch [35/200] batch [15/28] time 0.430 (0.452) data 0.299 (0.321) loss_x loss_x 1.4268 (1.3420) acc_x 65.6250 (65.8333) lr 1.8686e-03 eta 0:00:05
epoch [35/200] batch [20/28] time 0.494 (0.461) data 0.364 (0.330) loss_x loss_x 2.1328 (1.3559) acc_x 50.0000 (65.4688) lr 1.8686e-03 eta 0:00:03
epoch [35/200] batch [25/28] time 0.525 (0.463) data 0.394 (0.332) loss_x loss_x 1.9277 (1.4194) acc_x 50.0000 (64.6250) lr 1.8686e-03 eta 0:00:01
epoch [35/200] batch [5/69] time 0.397 (0.462) data 0.266 (0.331) loss_u loss_u 0.9541 (0.9147) acc_u 6.2500 (11.8750) lr 1.8686e-03 eta 0:00:29
epoch [35/200] batch [10/69] time 0.464 (0.458) data 0.333 (0.327) loss_u loss_u 0.8481 (0.8972) acc_u 15.6250 (12.8125) lr 1.8686e-03 eta 0:00:26
epoch [35/200] batch [15/69] time 0.548 (0.456) data 0.417 (0.325) loss_u loss_u 0.8750 (0.8929) acc_u 15.6250 (13.9583) lr 1.8686e-03 eta 0:00:24
epoch [35/200] batch [20/69] time 0.436 (0.457) data 0.305 (0.326) loss_u loss_u 0.8311 (0.8861) acc_u 25.0000 (14.6875) lr 1.8686e-03 eta 0:00:22
epoch [35/200] batch [25/69] time 0.525 (0.457) data 0.392 (0.326) loss_u loss_u 0.8984 (0.8852) acc_u 9.3750 (14.6250) lr 1.8686e-03 eta 0:00:20
epoch [35/200] batch [30/69] time 0.428 (0.455) data 0.295 (0.324) loss_u loss_u 0.9092 (0.8872) acc_u 12.5000 (14.3750) lr 1.8686e-03 eta 0:00:17
epoch [35/200] batch [35/69] time 0.425 (0.456) data 0.294 (0.325) loss_u loss_u 0.8496 (0.8859) acc_u 15.6250 (14.5536) lr 1.8686e-03 eta 0:00:15
epoch [35/200] batch [40/69] time 0.453 (0.457) data 0.322 (0.325) loss_u loss_u 0.9014 (0.8899) acc_u 6.2500 (13.5938) lr 1.8686e-03 eta 0:00:13
epoch [35/200] batch [45/69] time 0.526 (0.460) data 0.394 (0.329) loss_u loss_u 0.9116 (0.8922) acc_u 15.6250 (13.5417) lr 1.8686e-03 eta 0:00:11
epoch [35/200] batch [50/69] time 0.450 (0.461) data 0.319 (0.330) loss_u loss_u 0.8667 (0.8911) acc_u 15.6250 (13.7500) lr 1.8686e-03 eta 0:00:08
epoch [35/200] batch [55/69] time 0.524 (0.467) data 0.393 (0.336) loss_u loss_u 0.9341 (0.8923) acc_u 6.2500 (13.5227) lr 1.8686e-03 eta 0:00:06
epoch [35/200] batch [60/69] time 0.425 (0.465) data 0.293 (0.334) loss_u loss_u 0.9209 (0.8943) acc_u 12.5000 (13.3333) lr 1.8686e-03 eta 0:00:04
epoch [35/200] batch [65/69] time 0.456 (0.462) data 0.324 (0.330) loss_u loss_u 0.8726 (0.8951) acc_u 18.7500 (13.1731) lr 1.8686e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1689
confident_label rate tensor(0.2886, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 905
clean true:815
clean false:90
clean_rate:0.9005524861878453
noisy true:632
noisy false:1599
after delete: len(clean_dataset) 905
after delete: len(noisy_dataset) 2231
epoch [36/200] batch [5/28] time 0.618 (0.562) data 0.487 (0.431) loss_x loss_x 1.4688 (1.2958) acc_x 75.0000 (69.3750) lr 1.8607e-03 eta 0:00:12
epoch [36/200] batch [10/28] time 0.380 (0.486) data 0.249 (0.355) loss_x loss_x 1.4219 (1.3485) acc_x 62.5000 (67.5000) lr 1.8607e-03 eta 0:00:08
epoch [36/200] batch [15/28] time 0.392 (0.467) data 0.260 (0.336) loss_x loss_x 1.5000 (1.4481) acc_x 68.7500 (63.5417) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [20/28] time 0.413 (0.467) data 0.282 (0.336) loss_x loss_x 1.6191 (1.4572) acc_x 53.1250 (62.8125) lr 1.8607e-03 eta 0:00:03
epoch [36/200] batch [25/28] time 0.465 (0.462) data 0.334 (0.332) loss_x loss_x 1.5469 (1.4150) acc_x 59.3750 (63.7500) lr 1.8607e-03 eta 0:00:01
epoch [36/200] batch [5/69] time 0.396 (0.460) data 0.265 (0.330) loss_u loss_u 0.8921 (0.8635) acc_u 12.5000 (15.6250) lr 1.8607e-03 eta 0:00:29
epoch [36/200] batch [10/69] time 0.384 (0.455) data 0.253 (0.324) loss_u loss_u 0.9160 (0.8893) acc_u 6.2500 (12.5000) lr 1.8607e-03 eta 0:00:26
epoch [36/200] batch [15/69] time 0.416 (0.451) data 0.286 (0.320) loss_u loss_u 0.9067 (0.8911) acc_u 12.5000 (12.2917) lr 1.8607e-03 eta 0:00:24
epoch [36/200] batch [20/69] time 0.523 (0.450) data 0.392 (0.319) loss_u loss_u 0.8711 (0.8906) acc_u 18.7500 (12.8125) lr 1.8607e-03 eta 0:00:22
epoch [36/200] batch [25/69] time 0.421 (0.450) data 0.289 (0.319) loss_u loss_u 0.9194 (0.8852) acc_u 6.2500 (13.8750) lr 1.8607e-03 eta 0:00:19
epoch [36/200] batch [30/69] time 0.393 (0.446) data 0.262 (0.315) loss_u loss_u 0.9341 (0.8876) acc_u 6.2500 (13.5417) lr 1.8607e-03 eta 0:00:17
epoch [36/200] batch [35/69] time 0.561 (0.446) data 0.429 (0.316) loss_u loss_u 0.8721 (0.8907) acc_u 15.6250 (13.2143) lr 1.8607e-03 eta 0:00:15
epoch [36/200] batch [40/69] time 0.464 (0.446) data 0.334 (0.315) loss_u loss_u 0.9111 (0.8882) acc_u 15.6250 (13.7500) lr 1.8607e-03 eta 0:00:12
epoch [36/200] batch [45/69] time 0.380 (0.445) data 0.249 (0.314) loss_u loss_u 0.8262 (0.8857) acc_u 21.8750 (14.0278) lr 1.8607e-03 eta 0:00:10
epoch [36/200] batch [50/69] time 0.688 (0.447) data 0.555 (0.317) loss_u loss_u 0.9209 (0.8856) acc_u 9.3750 (14.0625) lr 1.8607e-03 eta 0:00:08
epoch [36/200] batch [55/69] time 0.505 (0.452) data 0.373 (0.321) loss_u loss_u 0.9131 (0.8879) acc_u 12.5000 (13.8068) lr 1.8607e-03 eta 0:00:06
epoch [36/200] batch [60/69] time 0.465 (0.454) data 0.334 (0.323) loss_u loss_u 0.8169 (0.8879) acc_u 18.7500 (13.6979) lr 1.8607e-03 eta 0:00:04
epoch [36/200] batch [65/69] time 0.393 (0.455) data 0.263 (0.324) loss_u loss_u 0.9297 (0.8901) acc_u 15.6250 (13.4615) lr 1.8607e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1719
confident_label rate tensor(0.2809, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 881
clean true:776
clean false:105
clean_rate:0.8808172531214529
noisy true:641
noisy false:1614
after delete: len(clean_dataset) 881
after delete: len(noisy_dataset) 2255
epoch [37/200] batch [5/27] time 0.332 (0.448) data 0.201 (0.318) loss_x loss_x 1.7031 (1.4059) acc_x 59.3750 (63.7500) lr 1.8526e-03 eta 0:00:09
epoch [37/200] batch [10/27] time 0.491 (0.467) data 0.360 (0.336) loss_x loss_x 1.6016 (1.3911) acc_x 53.1250 (64.0625) lr 1.8526e-03 eta 0:00:07
epoch [37/200] batch [15/27] time 0.485 (0.476) data 0.354 (0.346) loss_x loss_x 2.0078 (1.4106) acc_x 37.5000 (62.0833) lr 1.8526e-03 eta 0:00:05
epoch [37/200] batch [20/27] time 0.471 (0.471) data 0.340 (0.341) loss_x loss_x 1.3564 (1.4188) acc_x 59.3750 (60.7812) lr 1.8526e-03 eta 0:00:03
epoch [37/200] batch [25/27] time 0.538 (0.474) data 0.408 (0.343) loss_x loss_x 0.9912 (1.4361) acc_x 78.1250 (59.8750) lr 1.8526e-03 eta 0:00:00
epoch [37/200] batch [5/70] time 0.337 (0.458) data 0.206 (0.327) loss_u loss_u 0.8779 (0.8844) acc_u 21.8750 (16.2500) lr 1.8526e-03 eta 0:00:29
epoch [37/200] batch [10/70] time 0.460 (0.449) data 0.328 (0.318) loss_u loss_u 0.8750 (0.8897) acc_u 15.6250 (15.0000) lr 1.8526e-03 eta 0:00:26
epoch [37/200] batch [15/70] time 0.331 (0.443) data 0.199 (0.312) loss_u loss_u 0.8901 (0.8889) acc_u 6.2500 (14.5833) lr 1.8526e-03 eta 0:00:24
epoch [37/200] batch [20/70] time 0.485 (0.441) data 0.353 (0.310) loss_u loss_u 0.8486 (0.8908) acc_u 21.8750 (14.3750) lr 1.8526e-03 eta 0:00:22
epoch [37/200] batch [25/70] time 0.368 (0.439) data 0.236 (0.308) loss_u loss_u 0.8711 (0.8899) acc_u 12.5000 (14.3750) lr 1.8526e-03 eta 0:00:19
epoch [37/200] batch [30/70] time 0.415 (0.440) data 0.284 (0.309) loss_u loss_u 0.9404 (0.8896) acc_u 6.2500 (14.8958) lr 1.8526e-03 eta 0:00:17
epoch [37/200] batch [35/70] time 0.440 (0.441) data 0.310 (0.310) loss_u loss_u 0.8467 (0.8905) acc_u 15.6250 (14.4643) lr 1.8526e-03 eta 0:00:15
epoch [37/200] batch [40/70] time 0.450 (0.441) data 0.320 (0.310) loss_u loss_u 0.8799 (0.8929) acc_u 15.6250 (13.9062) lr 1.8526e-03 eta 0:00:13
epoch [37/200] batch [45/70] time 0.445 (0.442) data 0.315 (0.311) loss_u loss_u 0.9028 (0.8887) acc_u 12.5000 (14.5139) lr 1.8526e-03 eta 0:00:11
epoch [37/200] batch [50/70] time 0.599 (0.445) data 0.468 (0.314) loss_u loss_u 0.8608 (0.8887) acc_u 21.8750 (14.5625) lr 1.8526e-03 eta 0:00:08
epoch [37/200] batch [55/70] time 0.475 (0.442) data 0.344 (0.311) loss_u loss_u 0.8569 (0.8862) acc_u 18.7500 (14.7727) lr 1.8526e-03 eta 0:00:06
epoch [37/200] batch [60/70] time 0.343 (0.440) data 0.212 (0.309) loss_u loss_u 0.9023 (0.8844) acc_u 12.5000 (14.7917) lr 1.8526e-03 eta 0:00:04
epoch [37/200] batch [65/70] time 0.392 (0.440) data 0.260 (0.309) loss_u loss_u 0.8848 (0.8845) acc_u 12.5000 (14.9038) lr 1.8526e-03 eta 0:00:02
epoch [37/200] batch [70/70] time 0.344 (0.439) data 0.214 (0.308) loss_u loss_u 0.9180 (0.8854) acc_u 15.6250 (14.9107) lr 1.8526e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1737
confident_label rate tensor(0.2803, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 879
clean true:770
clean false:109
clean_rate:0.875995449374289
noisy true:629
noisy false:1628
after delete: len(clean_dataset) 879
after delete: len(noisy_dataset) 2257
epoch [38/200] batch [5/27] time 0.379 (0.470) data 0.248 (0.339) loss_x loss_x 1.1143 (1.3889) acc_x 68.7500 (61.8750) lr 1.8443e-03 eta 0:00:10
epoch [38/200] batch [10/27] time 0.483 (0.469) data 0.352 (0.338) loss_x loss_x 1.4531 (1.3189) acc_x 62.5000 (65.3125) lr 1.8443e-03 eta 0:00:07
epoch [38/200] batch [15/27] time 0.471 (0.459) data 0.341 (0.328) loss_x loss_x 1.9521 (1.3790) acc_x 59.3750 (65.2083) lr 1.8443e-03 eta 0:00:05
epoch [38/200] batch [20/27] time 0.584 (0.463) data 0.453 (0.332) loss_x loss_x 1.6396 (1.4466) acc_x 59.3750 (64.0625) lr 1.8443e-03 eta 0:00:03
epoch [38/200] batch [25/27] time 0.490 (0.459) data 0.360 (0.328) loss_x loss_x 0.9141 (1.3849) acc_x 65.6250 (65.0000) lr 1.8443e-03 eta 0:00:00
epoch [38/200] batch [5/70] time 0.463 (0.466) data 0.331 (0.335) loss_u loss_u 0.9238 (0.9038) acc_u 9.3750 (12.5000) lr 1.8443e-03 eta 0:00:30
epoch [38/200] batch [10/70] time 0.453 (0.462) data 0.322 (0.331) loss_u loss_u 0.9038 (0.9063) acc_u 12.5000 (11.2500) lr 1.8443e-03 eta 0:00:27
epoch [38/200] batch [15/70] time 0.349 (0.454) data 0.218 (0.323) loss_u loss_u 0.8833 (0.8938) acc_u 12.5000 (13.1250) lr 1.8443e-03 eta 0:00:24
epoch [38/200] batch [20/70] time 0.387 (0.455) data 0.256 (0.324) loss_u loss_u 0.9146 (0.8913) acc_u 6.2500 (13.9062) lr 1.8443e-03 eta 0:00:22
epoch [38/200] batch [25/70] time 0.336 (0.449) data 0.204 (0.318) loss_u loss_u 0.8926 (0.8900) acc_u 15.6250 (14.2500) lr 1.8443e-03 eta 0:00:20
epoch [38/200] batch [30/70] time 0.360 (0.446) data 0.228 (0.315) loss_u loss_u 0.9082 (0.8912) acc_u 9.3750 (14.0625) lr 1.8443e-03 eta 0:00:17
epoch [38/200] batch [35/70] time 0.496 (0.447) data 0.364 (0.316) loss_u loss_u 0.8418 (0.8902) acc_u 21.8750 (13.7500) lr 1.8443e-03 eta 0:00:15
epoch [38/200] batch [40/70] time 0.499 (0.446) data 0.367 (0.315) loss_u loss_u 0.8857 (0.8904) acc_u 12.5000 (13.6719) lr 1.8443e-03 eta 0:00:13
epoch [38/200] batch [45/70] time 0.500 (0.452) data 0.368 (0.321) loss_u loss_u 0.8184 (0.8871) acc_u 25.0000 (14.0972) lr 1.8443e-03 eta 0:00:11
epoch [38/200] batch [50/70] time 0.467 (0.452) data 0.335 (0.321) loss_u loss_u 0.9028 (0.8882) acc_u 15.6250 (13.8750) lr 1.8443e-03 eta 0:00:09
epoch [38/200] batch [55/70] time 0.426 (0.450) data 0.295 (0.319) loss_u loss_u 0.8750 (0.8881) acc_u 18.7500 (13.8636) lr 1.8443e-03 eta 0:00:06
epoch [38/200] batch [60/70] time 0.387 (0.447) data 0.255 (0.316) loss_u loss_u 0.9194 (0.8868) acc_u 9.3750 (13.9583) lr 1.8443e-03 eta 0:00:04
epoch [38/200] batch [65/70] time 0.433 (0.447) data 0.301 (0.316) loss_u loss_u 0.9082 (0.8864) acc_u 15.6250 (14.0385) lr 1.8443e-03 eta 0:00:02
epoch [38/200] batch [70/70] time 0.445 (0.448) data 0.314 (0.317) loss_u loss_u 0.8555 (0.8862) acc_u 15.6250 (14.0179) lr 1.8443e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1694
confident_label rate tensor(0.2812, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 882
clean true:786
clean false:96
clean_rate:0.891156462585034
noisy true:656
noisy false:1598
after delete: len(clean_dataset) 882
after delete: len(noisy_dataset) 2254
epoch [39/200] batch [5/27] time 0.451 (0.431) data 0.321 (0.300) loss_x loss_x 1.4609 (1.5111) acc_x 59.3750 (61.8750) lr 1.8358e-03 eta 0:00:09
epoch [39/200] batch [10/27] time 0.456 (0.467) data 0.325 (0.336) loss_x loss_x 1.9854 (1.5044) acc_x 53.1250 (63.1250) lr 1.8358e-03 eta 0:00:07
epoch [39/200] batch [15/27] time 0.410 (0.453) data 0.280 (0.323) loss_x loss_x 2.6836 (1.5375) acc_x 34.3750 (62.2917) lr 1.8358e-03 eta 0:00:05
epoch [39/200] batch [20/27] time 0.436 (0.449) data 0.306 (0.318) loss_x loss_x 0.8608 (1.4554) acc_x 75.0000 (63.2812) lr 1.8358e-03 eta 0:00:03
epoch [39/200] batch [25/27] time 0.501 (0.456) data 0.370 (0.325) loss_x loss_x 1.7520 (1.4352) acc_x 59.3750 (63.0000) lr 1.8358e-03 eta 0:00:00
epoch [39/200] batch [5/70] time 0.429 (0.450) data 0.299 (0.319) loss_u loss_u 0.9131 (0.8789) acc_u 9.3750 (14.3750) lr 1.8358e-03 eta 0:00:29
epoch [39/200] batch [10/70] time 0.519 (0.446) data 0.387 (0.315) loss_u loss_u 0.8999 (0.8886) acc_u 15.6250 (13.7500) lr 1.8358e-03 eta 0:00:26
epoch [39/200] batch [15/70] time 0.475 (0.444) data 0.345 (0.314) loss_u loss_u 0.8511 (0.8789) acc_u 15.6250 (14.5833) lr 1.8358e-03 eta 0:00:24
epoch [39/200] batch [20/70] time 0.513 (0.444) data 0.381 (0.313) loss_u loss_u 0.8945 (0.8869) acc_u 18.7500 (14.0625) lr 1.8358e-03 eta 0:00:22
epoch [39/200] batch [25/70] time 0.401 (0.442) data 0.270 (0.311) loss_u loss_u 0.9082 (0.8861) acc_u 12.5000 (14.5000) lr 1.8358e-03 eta 0:00:19
epoch [39/200] batch [30/70] time 0.495 (0.440) data 0.364 (0.309) loss_u loss_u 0.7983 (0.8825) acc_u 25.0000 (14.8958) lr 1.8358e-03 eta 0:00:17
epoch [39/200] batch [35/70] time 0.389 (0.437) data 0.259 (0.306) loss_u loss_u 0.8008 (0.8817) acc_u 18.7500 (14.8214) lr 1.8358e-03 eta 0:00:15
epoch [39/200] batch [40/70] time 0.338 (0.434) data 0.207 (0.303) loss_u loss_u 0.8989 (0.8813) acc_u 15.6250 (14.7656) lr 1.8358e-03 eta 0:00:13
epoch [39/200] batch [45/70] time 0.408 (0.435) data 0.277 (0.304) loss_u loss_u 0.8750 (0.8795) acc_u 18.7500 (15.2778) lr 1.8358e-03 eta 0:00:10
epoch [39/200] batch [50/70] time 0.483 (0.445) data 0.351 (0.314) loss_u loss_u 0.8872 (0.8805) acc_u 6.2500 (15.0625) lr 1.8358e-03 eta 0:00:08
epoch [39/200] batch [55/70] time 0.376 (0.442) data 0.245 (0.312) loss_u loss_u 0.8213 (0.8793) acc_u 21.8750 (15.3409) lr 1.8358e-03 eta 0:00:06
epoch [39/200] batch [60/70] time 0.506 (0.442) data 0.375 (0.311) loss_u loss_u 0.9209 (0.8808) acc_u 9.3750 (15.2083) lr 1.8358e-03 eta 0:00:04
epoch [39/200] batch [65/70] time 0.454 (0.443) data 0.322 (0.312) loss_u loss_u 0.8384 (0.8809) acc_u 18.7500 (15.1442) lr 1.8358e-03 eta 0:00:02
epoch [39/200] batch [70/70] time 0.497 (0.445) data 0.367 (0.314) loss_u loss_u 0.8838 (0.8800) acc_u 15.6250 (15.3571) lr 1.8358e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1688
confident_label rate tensor(0.2946, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 924
clean true:825
clean false:99
clean_rate:0.8928571428571429
noisy true:623
noisy false:1589
after delete: len(clean_dataset) 924
after delete: len(noisy_dataset) 2212
epoch [40/200] batch [5/28] time 0.389 (0.433) data 0.259 (0.303) loss_x loss_x 1.2080 (1.3949) acc_x 59.3750 (62.5000) lr 1.8271e-03 eta 0:00:09
epoch [40/200] batch [10/28] time 0.421 (0.444) data 0.290 (0.314) loss_x loss_x 0.7109 (1.3452) acc_x 81.2500 (65.9375) lr 1.8271e-03 eta 0:00:07
epoch [40/200] batch [15/28] time 0.660 (0.467) data 0.529 (0.336) loss_x loss_x 1.2451 (1.3389) acc_x 71.8750 (65.8333) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [20/28] time 0.516 (0.476) data 0.385 (0.346) loss_x loss_x 0.9795 (1.3799) acc_x 65.6250 (64.3750) lr 1.8271e-03 eta 0:00:03
epoch [40/200] batch [25/28] time 0.491 (0.473) data 0.360 (0.342) loss_x loss_x 1.1123 (1.3580) acc_x 78.1250 (64.8750) lr 1.8271e-03 eta 0:00:01
epoch [40/200] batch [5/69] time 0.515 (0.480) data 0.384 (0.349) loss_u loss_u 0.9243 (0.9133) acc_u 9.3750 (10.6250) lr 1.8271e-03 eta 0:00:30
epoch [40/200] batch [10/69] time 0.430 (0.475) data 0.299 (0.344) loss_u loss_u 0.8965 (0.9062) acc_u 18.7500 (11.5625) lr 1.8271e-03 eta 0:00:28
epoch [40/200] batch [15/69] time 0.701 (0.475) data 0.569 (0.344) loss_u loss_u 0.9604 (0.8985) acc_u 3.1250 (12.0833) lr 1.8271e-03 eta 0:00:25
epoch [40/200] batch [20/69] time 0.466 (0.472) data 0.334 (0.341) loss_u loss_u 0.9453 (0.8962) acc_u 6.2500 (12.1875) lr 1.8271e-03 eta 0:00:23
epoch [40/200] batch [25/69] time 0.361 (0.466) data 0.230 (0.335) loss_u loss_u 0.9258 (0.8972) acc_u 9.3750 (12.0000) lr 1.8271e-03 eta 0:00:20
epoch [40/200] batch [30/69] time 0.458 (0.467) data 0.327 (0.336) loss_u loss_u 0.9404 (0.8993) acc_u 6.2500 (11.5625) lr 1.8271e-03 eta 0:00:18
epoch [40/200] batch [35/69] time 0.353 (0.463) data 0.221 (0.332) loss_u loss_u 0.7788 (0.8975) acc_u 34.3750 (12.0536) lr 1.8271e-03 eta 0:00:15
epoch [40/200] batch [40/69] time 0.628 (0.463) data 0.497 (0.332) loss_u loss_u 0.9634 (0.8979) acc_u 3.1250 (12.0312) lr 1.8271e-03 eta 0:00:13
epoch [40/200] batch [45/69] time 0.465 (0.463) data 0.333 (0.332) loss_u loss_u 0.8394 (0.8973) acc_u 21.8750 (12.3611) lr 1.8271e-03 eta 0:00:11
epoch [40/200] batch [50/69] time 0.385 (0.463) data 0.253 (0.331) loss_u loss_u 0.8960 (0.8961) acc_u 9.3750 (12.5625) lr 1.8271e-03 eta 0:00:08
epoch [40/200] batch [55/69] time 0.430 (0.464) data 0.300 (0.333) loss_u loss_u 0.8765 (0.8953) acc_u 15.6250 (12.5568) lr 1.8271e-03 eta 0:00:06
epoch [40/200] batch [60/69] time 0.540 (0.465) data 0.408 (0.334) loss_u loss_u 0.8706 (0.8953) acc_u 18.7500 (12.6562) lr 1.8271e-03 eta 0:00:04
epoch [40/200] batch [65/69] time 0.487 (0.464) data 0.356 (0.333) loss_u loss_u 0.8784 (0.8940) acc_u 9.3750 (12.8365) lr 1.8271e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1702
confident_label rate tensor(0.2812, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 882
clean true:780
clean false:102
clean_rate:0.8843537414965986
noisy true:654
noisy false:1600
after delete: len(clean_dataset) 882
after delete: len(noisy_dataset) 2254
epoch [41/200] batch [5/27] time 0.435 (0.434) data 0.304 (0.303) loss_x loss_x 1.5312 (1.2285) acc_x 68.7500 (72.5000) lr 1.8181e-03 eta 0:00:09
epoch [41/200] batch [10/27] time 0.448 (0.439) data 0.317 (0.308) loss_x loss_x 1.3838 (1.2073) acc_x 50.0000 (66.5625) lr 1.8181e-03 eta 0:00:07
epoch [41/200] batch [15/27] time 0.411 (0.446) data 0.281 (0.315) loss_x loss_x 1.7100 (1.2381) acc_x 59.3750 (68.5417) lr 1.8181e-03 eta 0:00:05
epoch [41/200] batch [20/27] time 0.553 (0.462) data 0.422 (0.332) loss_x loss_x 1.4316 (1.2531) acc_x 65.6250 (68.2812) lr 1.8181e-03 eta 0:00:03
epoch [41/200] batch [25/27] time 0.488 (0.461) data 0.357 (0.331) loss_x loss_x 1.3203 (1.2744) acc_x 65.6250 (68.0000) lr 1.8181e-03 eta 0:00:00
epoch [41/200] batch [5/70] time 0.433 (0.455) data 0.303 (0.324) loss_u loss_u 0.8647 (0.8667) acc_u 15.6250 (17.5000) lr 1.8181e-03 eta 0:00:29
epoch [41/200] batch [10/70] time 0.438 (0.447) data 0.307 (0.316) loss_u loss_u 0.9258 (0.8744) acc_u 12.5000 (17.1875) lr 1.8181e-03 eta 0:00:26
epoch [41/200] batch [15/70] time 0.456 (0.450) data 0.325 (0.319) loss_u loss_u 0.8062 (0.8798) acc_u 18.7500 (16.4583) lr 1.8181e-03 eta 0:00:24
epoch [41/200] batch [20/70] time 0.465 (0.449) data 0.334 (0.318) loss_u loss_u 0.8369 (0.8826) acc_u 25.0000 (16.4062) lr 1.8181e-03 eta 0:00:22
epoch [41/200] batch [25/70] time 0.473 (0.446) data 0.342 (0.315) loss_u loss_u 0.8574 (0.8823) acc_u 15.6250 (16.3750) lr 1.8181e-03 eta 0:00:20
epoch [41/200] batch [30/70] time 0.516 (0.447) data 0.385 (0.316) loss_u loss_u 0.9146 (0.8831) acc_u 15.6250 (16.2500) lr 1.8181e-03 eta 0:00:17
epoch [41/200] batch [35/70] time 0.394 (0.447) data 0.263 (0.316) loss_u loss_u 0.8696 (0.8842) acc_u 18.7500 (15.8036) lr 1.8181e-03 eta 0:00:15
epoch [41/200] batch [40/70] time 0.357 (0.451) data 0.225 (0.320) loss_u loss_u 0.7964 (0.8812) acc_u 15.6250 (15.8594) lr 1.8181e-03 eta 0:00:13
epoch [41/200] batch [45/70] time 0.405 (0.450) data 0.274 (0.320) loss_u loss_u 0.9204 (0.8829) acc_u 9.3750 (15.5556) lr 1.8181e-03 eta 0:00:11
epoch [41/200] batch [50/70] time 0.506 (0.452) data 0.374 (0.321) loss_u loss_u 0.8740 (0.8846) acc_u 9.3750 (15.0625) lr 1.8181e-03 eta 0:00:09
epoch [41/200] batch [55/70] time 0.473 (0.454) data 0.341 (0.323) loss_u loss_u 0.9141 (0.8843) acc_u 9.3750 (15.1705) lr 1.8181e-03 eta 0:00:06
epoch [41/200] batch [60/70] time 0.396 (0.453) data 0.264 (0.322) loss_u loss_u 0.8906 (0.8857) acc_u 15.6250 (14.8958) lr 1.8181e-03 eta 0:00:04
epoch [41/200] batch [65/70] time 0.397 (0.454) data 0.267 (0.323) loss_u loss_u 0.8257 (0.8848) acc_u 21.8750 (14.8558) lr 1.8181e-03 eta 0:00:02
epoch [41/200] batch [70/70] time 0.351 (0.451) data 0.221 (0.320) loss_u loss_u 0.8770 (0.8829) acc_u 9.3750 (15.0446) lr 1.8181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1651
confident_label rate tensor(0.2883, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 904
clean true:811
clean false:93
clean_rate:0.8971238938053098
noisy true:674
noisy false:1558
after delete: len(clean_dataset) 904
after delete: len(noisy_dataset) 2232
epoch [42/200] batch [5/28] time 0.609 (0.498) data 0.478 (0.367) loss_x loss_x 1.2451 (1.3990) acc_x 65.6250 (65.6250) lr 1.8090e-03 eta 0:00:11
epoch [42/200] batch [10/28] time 0.461 (0.506) data 0.331 (0.375) loss_x loss_x 1.2695 (1.3877) acc_x 65.6250 (65.3125) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [15/28] time 0.447 (0.486) data 0.317 (0.356) loss_x loss_x 1.4512 (1.3810) acc_x 65.6250 (65.6250) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [20/28] time 0.579 (0.498) data 0.447 (0.367) loss_x loss_x 1.4561 (1.3941) acc_x 53.1250 (65.0000) lr 1.8090e-03 eta 0:00:03
epoch [42/200] batch [25/28] time 0.728 (0.505) data 0.595 (0.374) loss_x loss_x 1.5508 (1.3946) acc_x 65.6250 (65.2500) lr 1.8090e-03 eta 0:00:01
epoch [42/200] batch [5/69] time 0.561 (0.506) data 0.429 (0.375) loss_u loss_u 0.9014 (0.8849) acc_u 12.5000 (15.0000) lr 1.8090e-03 eta 0:00:32
epoch [42/200] batch [10/69] time 0.443 (0.508) data 0.310 (0.377) loss_u loss_u 0.8096 (0.8823) acc_u 21.8750 (14.6875) lr 1.8090e-03 eta 0:00:29
epoch [42/200] batch [15/69] time 0.435 (0.509) data 0.304 (0.378) loss_u loss_u 0.8154 (0.8762) acc_u 21.8750 (15.6250) lr 1.8090e-03 eta 0:00:27
epoch [42/200] batch [20/69] time 0.450 (0.504) data 0.318 (0.373) loss_u loss_u 0.8701 (0.8774) acc_u 9.3750 (14.6875) lr 1.8090e-03 eta 0:00:24
epoch [42/200] batch [25/69] time 0.419 (0.496) data 0.286 (0.365) loss_u loss_u 0.9160 (0.8810) acc_u 9.3750 (14.0000) lr 1.8090e-03 eta 0:00:21
epoch [42/200] batch [30/69] time 0.443 (0.492) data 0.311 (0.360) loss_u loss_u 0.9175 (0.8788) acc_u 6.2500 (14.2708) lr 1.8090e-03 eta 0:00:19
epoch [42/200] batch [35/69] time 0.370 (0.489) data 0.238 (0.357) loss_u loss_u 0.9180 (0.8813) acc_u 9.3750 (13.7500) lr 1.8090e-03 eta 0:00:16
epoch [42/200] batch [40/69] time 0.365 (0.487) data 0.233 (0.355) loss_u loss_u 0.8721 (0.8819) acc_u 15.6250 (13.8281) lr 1.8090e-03 eta 0:00:14
epoch [42/200] batch [45/69] time 0.506 (0.482) data 0.375 (0.351) loss_u loss_u 0.8408 (0.8790) acc_u 21.8750 (14.4444) lr 1.8090e-03 eta 0:00:11
epoch [42/200] batch [50/69] time 0.370 (0.478) data 0.238 (0.346) loss_u loss_u 0.8428 (0.8786) acc_u 21.8750 (14.5000) lr 1.8090e-03 eta 0:00:09
epoch [42/200] batch [55/69] time 0.734 (0.477) data 0.603 (0.346) loss_u loss_u 0.8496 (0.8796) acc_u 28.1250 (14.6591) lr 1.8090e-03 eta 0:00:06
epoch [42/200] batch [60/69] time 0.347 (0.475) data 0.215 (0.343) loss_u loss_u 0.8657 (0.8782) acc_u 15.6250 (14.9479) lr 1.8090e-03 eta 0:00:04
epoch [42/200] batch [65/69] time 0.518 (0.475) data 0.386 (0.344) loss_u loss_u 0.9219 (0.8817) acc_u 6.2500 (14.5192) lr 1.8090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1685
confident_label rate tensor(0.2857, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 896
clean true:797
clean false:99
clean_rate:0.8895089285714286
noisy true:654
noisy false:1586
after delete: len(clean_dataset) 896
after delete: len(noisy_dataset) 2240
epoch [43/200] batch [5/28] time 0.570 (0.497) data 0.440 (0.366) loss_x loss_x 1.2988 (1.2828) acc_x 68.7500 (66.8750) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [10/28] time 0.365 (0.468) data 0.234 (0.338) loss_x loss_x 1.0010 (1.3016) acc_x 68.7500 (66.8750) lr 1.7997e-03 eta 0:00:08
epoch [43/200] batch [15/28] time 0.559 (0.476) data 0.429 (0.345) loss_x loss_x 1.6240 (1.2815) acc_x 56.2500 (67.5000) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [20/28] time 0.484 (0.473) data 0.354 (0.342) loss_x loss_x 1.5498 (1.3868) acc_x 68.7500 (64.5312) lr 1.7997e-03 eta 0:00:03
epoch [43/200] batch [25/28] time 0.431 (0.463) data 0.300 (0.332) loss_x loss_x 1.6582 (1.3840) acc_x 62.5000 (64.6250) lr 1.7997e-03 eta 0:00:01
epoch [43/200] batch [5/70] time 0.469 (0.462) data 0.337 (0.331) loss_u loss_u 0.9268 (0.8988) acc_u 9.3750 (12.5000) lr 1.7997e-03 eta 0:00:30
epoch [43/200] batch [10/70] time 0.609 (0.469) data 0.476 (0.338) loss_u loss_u 0.8921 (0.9051) acc_u 18.7500 (13.1250) lr 1.7997e-03 eta 0:00:28
epoch [43/200] batch [15/70] time 0.454 (0.467) data 0.322 (0.336) loss_u loss_u 0.8691 (0.8978) acc_u 18.7500 (13.7500) lr 1.7997e-03 eta 0:00:25
epoch [43/200] batch [20/70] time 0.486 (0.470) data 0.354 (0.338) loss_u loss_u 0.9316 (0.8895) acc_u 9.3750 (14.2188) lr 1.7997e-03 eta 0:00:23
epoch [43/200] batch [25/70] time 0.429 (0.464) data 0.298 (0.333) loss_u loss_u 0.9468 (0.8959) acc_u 6.2500 (13.0000) lr 1.7997e-03 eta 0:00:20
epoch [43/200] batch [30/70] time 0.390 (0.466) data 0.259 (0.334) loss_u loss_u 0.8320 (0.8953) acc_u 18.7500 (13.0208) lr 1.7997e-03 eta 0:00:18
epoch [43/200] batch [35/70] time 0.547 (0.465) data 0.415 (0.334) loss_u loss_u 0.9229 (0.8917) acc_u 6.2500 (13.3036) lr 1.7997e-03 eta 0:00:16
epoch [43/200] batch [40/70] time 0.489 (0.463) data 0.357 (0.332) loss_u loss_u 0.9136 (0.8923) acc_u 9.3750 (13.2031) lr 1.7997e-03 eta 0:00:13
epoch [43/200] batch [45/70] time 0.411 (0.462) data 0.280 (0.331) loss_u loss_u 0.8799 (0.8913) acc_u 15.6250 (13.2639) lr 1.7997e-03 eta 0:00:11
epoch [43/200] batch [50/70] time 0.421 (0.459) data 0.290 (0.328) loss_u loss_u 0.8872 (0.8901) acc_u 12.5000 (13.5000) lr 1.7997e-03 eta 0:00:09
epoch [43/200] batch [55/70] time 0.351 (0.456) data 0.221 (0.324) loss_u loss_u 0.8530 (0.8872) acc_u 18.7500 (13.6364) lr 1.7997e-03 eta 0:00:06
epoch [43/200] batch [60/70] time 0.399 (0.455) data 0.269 (0.324) loss_u loss_u 0.8818 (0.8869) acc_u 12.5000 (13.6979) lr 1.7997e-03 eta 0:00:04
epoch [43/200] batch [65/70] time 0.315 (0.454) data 0.183 (0.323) loss_u loss_u 0.8682 (0.8863) acc_u 18.7500 (13.8462) lr 1.7997e-03 eta 0:00:02
epoch [43/200] batch [70/70] time 0.403 (0.452) data 0.271 (0.320) loss_u loss_u 0.8965 (0.8885) acc_u 9.3750 (13.3482) lr 1.7997e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1693
confident_label rate tensor(0.2841, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 891
clean true:796
clean false:95
clean_rate:0.8933782267115601
noisy true:647
noisy false:1598
after delete: len(clean_dataset) 891
after delete: len(noisy_dataset) 2245
epoch [44/200] batch [5/27] time 0.426 (0.464) data 0.296 (0.333) loss_x loss_x 1.1787 (1.4072) acc_x 71.8750 (65.0000) lr 1.7902e-03 eta 0:00:10
epoch [44/200] batch [10/27] time 0.439 (0.451) data 0.308 (0.321) loss_x loss_x 2.0977 (1.4915) acc_x 53.1250 (63.7500) lr 1.7902e-03 eta 0:00:07
epoch [44/200] batch [15/27] time 0.399 (0.447) data 0.269 (0.317) loss_x loss_x 1.2236 (1.4434) acc_x 62.5000 (63.7500) lr 1.7902e-03 eta 0:00:05
epoch [44/200] batch [20/27] time 0.495 (0.458) data 0.363 (0.327) loss_x loss_x 1.5312 (1.3917) acc_x 62.5000 (64.0625) lr 1.7902e-03 eta 0:00:03
epoch [44/200] batch [25/27] time 0.464 (0.466) data 0.334 (0.336) loss_x loss_x 1.4326 (1.4178) acc_x 62.5000 (63.3750) lr 1.7902e-03 eta 0:00:00
epoch [44/200] batch [5/70] time 0.619 (0.473) data 0.488 (0.342) loss_u loss_u 0.8628 (0.8531) acc_u 18.7500 (15.6250) lr 1.7902e-03 eta 0:00:30
epoch [44/200] batch [10/70] time 0.535 (0.471) data 0.404 (0.340) loss_u loss_u 0.8511 (0.8727) acc_u 25.0000 (14.0625) lr 1.7902e-03 eta 0:00:28
epoch [44/200] batch [15/70] time 0.409 (0.467) data 0.277 (0.336) loss_u loss_u 0.8657 (0.8755) acc_u 12.5000 (13.5417) lr 1.7902e-03 eta 0:00:25
epoch [44/200] batch [20/70] time 0.509 (0.471) data 0.377 (0.340) loss_u loss_u 0.8555 (0.8678) acc_u 18.7500 (15.0000) lr 1.7902e-03 eta 0:00:23
epoch [44/200] batch [25/70] time 0.536 (0.472) data 0.405 (0.341) loss_u loss_u 0.7808 (0.8664) acc_u 28.1250 (15.5000) lr 1.7902e-03 eta 0:00:21
epoch [44/200] batch [30/70] time 0.528 (0.477) data 0.396 (0.346) loss_u loss_u 0.8862 (0.8669) acc_u 18.7500 (15.5208) lr 1.7902e-03 eta 0:00:19
epoch [44/200] batch [35/70] time 0.380 (0.477) data 0.247 (0.346) loss_u loss_u 0.9277 (0.8673) acc_u 6.2500 (15.2679) lr 1.7902e-03 eta 0:00:16
epoch [44/200] batch [40/70] time 0.552 (0.474) data 0.420 (0.342) loss_u loss_u 0.9087 (0.8698) acc_u 9.3750 (15.1562) lr 1.7902e-03 eta 0:00:14
epoch [44/200] batch [45/70] time 0.349 (0.470) data 0.218 (0.339) loss_u loss_u 0.8745 (0.8734) acc_u 12.5000 (14.7222) lr 1.7902e-03 eta 0:00:11
epoch [44/200] batch [50/70] time 0.406 (0.466) data 0.276 (0.335) loss_u loss_u 0.9487 (0.8774) acc_u 6.2500 (14.3750) lr 1.7902e-03 eta 0:00:09
epoch [44/200] batch [55/70] time 0.417 (0.468) data 0.287 (0.337) loss_u loss_u 0.9307 (0.8771) acc_u 9.3750 (14.3750) lr 1.7902e-03 eta 0:00:07
epoch [44/200] batch [60/70] time 0.425 (0.467) data 0.294 (0.336) loss_u loss_u 0.8096 (0.8760) acc_u 28.1250 (14.5833) lr 1.7902e-03 eta 0:00:04
epoch [44/200] batch [65/70] time 0.508 (0.466) data 0.373 (0.335) loss_u loss_u 0.8301 (0.8753) acc_u 15.6250 (14.6635) lr 1.7902e-03 eta 0:00:02
epoch [44/200] batch [70/70] time 0.435 (0.463) data 0.303 (0.331) loss_u loss_u 0.9199 (0.8768) acc_u 9.3750 (14.6429) lr 1.7902e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1668
confident_label rate tensor(0.2940, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 922
clean true:810
clean false:112
clean_rate:0.8785249457700651
noisy true:658
noisy false:1556
after delete: len(clean_dataset) 922
after delete: len(noisy_dataset) 2214
epoch [45/200] batch [5/28] time 0.569 (0.467) data 0.438 (0.337) loss_x loss_x 1.1074 (1.2354) acc_x 65.6250 (65.0000) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [10/28] time 0.396 (0.449) data 0.265 (0.318) loss_x loss_x 1.2158 (1.3624) acc_x 65.6250 (64.6875) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [15/28] time 0.593 (0.465) data 0.461 (0.334) loss_x loss_x 1.2422 (1.3191) acc_x 71.8750 (66.4583) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [20/28] time 0.381 (0.451) data 0.250 (0.320) loss_x loss_x 1.3770 (1.3238) acc_x 59.3750 (65.6250) lr 1.7804e-03 eta 0:00:03
epoch [45/200] batch [25/28] time 0.518 (0.446) data 0.388 (0.315) loss_x loss_x 1.2686 (1.3572) acc_x 65.6250 (64.2500) lr 1.7804e-03 eta 0:00:01
epoch [45/200] batch [5/69] time 0.486 (0.443) data 0.356 (0.312) loss_u loss_u 0.8931 (0.9065) acc_u 15.6250 (13.1250) lr 1.7804e-03 eta 0:00:28
epoch [45/200] batch [10/69] time 0.505 (0.451) data 0.374 (0.320) loss_u loss_u 0.8901 (0.8957) acc_u 15.6250 (14.6875) lr 1.7804e-03 eta 0:00:26
epoch [45/200] batch [15/69] time 0.439 (0.448) data 0.308 (0.317) loss_u loss_u 0.8335 (0.8938) acc_u 25.0000 (14.3750) lr 1.7804e-03 eta 0:00:24
epoch [45/200] batch [20/69] time 0.396 (0.450) data 0.266 (0.320) loss_u loss_u 0.9287 (0.8988) acc_u 9.3750 (13.2812) lr 1.7804e-03 eta 0:00:22
epoch [45/200] batch [25/69] time 0.562 (0.448) data 0.431 (0.318) loss_u loss_u 0.8667 (0.8973) acc_u 21.8750 (13.1250) lr 1.7804e-03 eta 0:00:19
epoch [45/200] batch [30/69] time 0.338 (0.445) data 0.207 (0.314) loss_u loss_u 0.8105 (0.8904) acc_u 25.0000 (14.0625) lr 1.7804e-03 eta 0:00:17
epoch [45/200] batch [35/69] time 0.333 (0.442) data 0.202 (0.311) loss_u loss_u 0.8867 (0.8883) acc_u 15.6250 (14.5536) lr 1.7804e-03 eta 0:00:15
epoch [45/200] batch [40/69] time 0.362 (0.445) data 0.230 (0.314) loss_u loss_u 0.8579 (0.8875) acc_u 15.6250 (14.5312) lr 1.7804e-03 eta 0:00:12
epoch [45/200] batch [45/69] time 0.386 (0.443) data 0.254 (0.312) loss_u loss_u 0.8145 (0.8830) acc_u 25.0000 (15.2778) lr 1.7804e-03 eta 0:00:10
epoch [45/200] batch [50/69] time 0.406 (0.445) data 0.275 (0.314) loss_u loss_u 0.9116 (0.8839) acc_u 9.3750 (15.2500) lr 1.7804e-03 eta 0:00:08
epoch [45/200] batch [55/69] time 0.360 (0.446) data 0.230 (0.315) loss_u loss_u 0.8789 (0.8842) acc_u 18.7500 (15.1705) lr 1.7804e-03 eta 0:00:06
epoch [45/200] batch [60/69] time 0.461 (0.445) data 0.331 (0.314) loss_u loss_u 0.8569 (0.8849) acc_u 21.8750 (15.0521) lr 1.7804e-03 eta 0:00:04
epoch [45/200] batch [65/69] time 0.421 (0.444) data 0.291 (0.313) loss_u loss_u 0.9160 (0.8857) acc_u 3.1250 (14.7596) lr 1.7804e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1669
confident_label rate tensor(0.2793, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 876
clean true:793
clean false:83
clean_rate:0.9052511415525114
noisy true:674
noisy false:1586
after delete: len(clean_dataset) 876
after delete: len(noisy_dataset) 2260
epoch [46/200] batch [5/27] time 0.441 (0.448) data 0.311 (0.317) loss_x loss_x 1.3086 (1.1214) acc_x 71.8750 (70.0000) lr 1.7705e-03 eta 0:00:09
epoch [46/200] batch [10/27] time 0.433 (0.445) data 0.302 (0.314) loss_x loss_x 1.6055 (1.2157) acc_x 53.1250 (67.8125) lr 1.7705e-03 eta 0:00:07
epoch [46/200] batch [15/27] time 0.469 (0.440) data 0.338 (0.309) loss_x loss_x 0.8770 (1.2142) acc_x 71.8750 (66.6667) lr 1.7705e-03 eta 0:00:05
epoch [46/200] batch [20/27] time 0.393 (0.439) data 0.262 (0.309) loss_x loss_x 1.4795 (1.2379) acc_x 53.1250 (66.7188) lr 1.7705e-03 eta 0:00:03
epoch [46/200] batch [25/27] time 0.490 (0.446) data 0.360 (0.316) loss_x loss_x 0.9507 (1.2386) acc_x 71.8750 (66.6250) lr 1.7705e-03 eta 0:00:00
epoch [46/200] batch [5/70] time 0.344 (0.444) data 0.214 (0.313) loss_u loss_u 0.8545 (0.8564) acc_u 18.7500 (18.7500) lr 1.7705e-03 eta 0:00:28
epoch [46/200] batch [10/70] time 0.370 (0.453) data 0.239 (0.323) loss_u loss_u 0.8706 (0.8616) acc_u 15.6250 (18.1250) lr 1.7705e-03 eta 0:00:27
epoch [46/200] batch [15/70] time 0.447 (0.458) data 0.317 (0.328) loss_u loss_u 0.8071 (0.8697) acc_u 25.0000 (16.6667) lr 1.7705e-03 eta 0:00:25
epoch [46/200] batch [20/70] time 0.387 (0.454) data 0.256 (0.323) loss_u loss_u 0.8892 (0.8750) acc_u 9.3750 (15.6250) lr 1.7705e-03 eta 0:00:22
epoch [46/200] batch [25/70] time 0.476 (0.458) data 0.346 (0.328) loss_u loss_u 0.8560 (0.8775) acc_u 15.6250 (15.1250) lr 1.7705e-03 eta 0:00:20
epoch [46/200] batch [30/70] time 0.371 (0.454) data 0.240 (0.324) loss_u loss_u 0.8755 (0.8792) acc_u 15.6250 (15.1042) lr 1.7705e-03 eta 0:00:18
epoch [46/200] batch [35/70] time 0.428 (0.452) data 0.297 (0.321) loss_u loss_u 0.9126 (0.8811) acc_u 9.3750 (14.5536) lr 1.7705e-03 eta 0:00:15
epoch [46/200] batch [40/70] time 0.471 (0.452) data 0.339 (0.321) loss_u loss_u 0.9482 (0.8853) acc_u 6.2500 (13.9844) lr 1.7705e-03 eta 0:00:13
epoch [46/200] batch [45/70] time 0.463 (0.455) data 0.333 (0.324) loss_u loss_u 0.8311 (0.8802) acc_u 21.8750 (14.8611) lr 1.7705e-03 eta 0:00:11
epoch [46/200] batch [50/70] time 0.503 (0.453) data 0.372 (0.322) loss_u loss_u 0.8525 (0.8797) acc_u 21.8750 (14.7500) lr 1.7705e-03 eta 0:00:09
epoch [46/200] batch [55/70] time 0.469 (0.453) data 0.337 (0.322) loss_u loss_u 0.8359 (0.8802) acc_u 18.7500 (14.4886) lr 1.7705e-03 eta 0:00:06
epoch [46/200] batch [60/70] time 0.325 (0.448) data 0.194 (0.317) loss_u loss_u 0.9082 (0.8800) acc_u 9.3750 (14.6875) lr 1.7705e-03 eta 0:00:04
epoch [46/200] batch [65/70] time 0.335 (0.445) data 0.204 (0.314) loss_u loss_u 0.8691 (0.8801) acc_u 21.8750 (14.8077) lr 1.7705e-03 eta 0:00:02
epoch [46/200] batch [70/70] time 0.539 (0.443) data 0.407 (0.313) loss_u loss_u 0.8623 (0.8801) acc_u 18.7500 (14.8214) lr 1.7705e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1661
confident_label rate tensor(0.2915, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 914
clean true:804
clean false:110
clean_rate:0.8796498905908097
noisy true:671
noisy false:1551
after delete: len(clean_dataset) 914
after delete: len(noisy_dataset) 2222
epoch [47/200] batch [5/28] time 0.467 (0.447) data 0.336 (0.316) loss_x loss_x 1.7441 (1.3596) acc_x 46.8750 (64.3750) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [10/28] time 0.406 (0.441) data 0.275 (0.310) loss_x loss_x 1.3818 (1.4874) acc_x 71.8750 (63.1250) lr 1.7604e-03 eta 0:00:07
epoch [47/200] batch [15/28] time 0.418 (0.441) data 0.288 (0.311) loss_x loss_x 1.7646 (1.4991) acc_x 62.5000 (61.4583) lr 1.7604e-03 eta 0:00:05
epoch [47/200] batch [20/28] time 0.382 (0.446) data 0.251 (0.315) loss_x loss_x 1.1816 (1.4509) acc_x 68.7500 (62.9688) lr 1.7604e-03 eta 0:00:03
epoch [47/200] batch [25/28] time 0.433 (0.447) data 0.302 (0.316) loss_x loss_x 1.4443 (1.4057) acc_x 56.2500 (63.7500) lr 1.7604e-03 eta 0:00:01
epoch [47/200] batch [5/69] time 0.510 (0.453) data 0.379 (0.322) loss_u loss_u 0.8589 (0.8604) acc_u 21.8750 (18.7500) lr 1.7604e-03 eta 0:00:28
epoch [47/200] batch [10/69] time 0.395 (0.447) data 0.265 (0.316) loss_u loss_u 0.8999 (0.8770) acc_u 15.6250 (15.9375) lr 1.7604e-03 eta 0:00:26
epoch [47/200] batch [15/69] time 0.507 (0.451) data 0.375 (0.320) loss_u loss_u 0.9302 (0.8840) acc_u 3.1250 (14.7917) lr 1.7604e-03 eta 0:00:24
epoch [47/200] batch [20/69] time 0.366 (0.445) data 0.236 (0.314) loss_u loss_u 0.8735 (0.8812) acc_u 18.7500 (15.3125) lr 1.7604e-03 eta 0:00:21
epoch [47/200] batch [25/69] time 0.437 (0.440) data 0.306 (0.309) loss_u loss_u 0.9224 (0.8812) acc_u 9.3750 (16.0000) lr 1.7604e-03 eta 0:00:19
epoch [47/200] batch [30/69] time 0.536 (0.438) data 0.404 (0.307) loss_u loss_u 0.9370 (0.8853) acc_u 9.3750 (15.5208) lr 1.7604e-03 eta 0:00:17
epoch [47/200] batch [35/69] time 0.456 (0.438) data 0.324 (0.307) loss_u loss_u 0.7915 (0.8830) acc_u 21.8750 (15.5357) lr 1.7604e-03 eta 0:00:14
epoch [47/200] batch [40/69] time 0.411 (0.439) data 0.279 (0.307) loss_u loss_u 0.8989 (0.8821) acc_u 6.2500 (15.5469) lr 1.7604e-03 eta 0:00:12
epoch [47/200] batch [45/69] time 0.416 (0.441) data 0.284 (0.310) loss_u loss_u 0.8848 (0.8813) acc_u 18.7500 (15.7639) lr 1.7604e-03 eta 0:00:10
epoch [47/200] batch [50/69] time 0.389 (0.442) data 0.256 (0.310) loss_u loss_u 0.9282 (0.8831) acc_u 6.2500 (15.3750) lr 1.7604e-03 eta 0:00:08
epoch [47/200] batch [55/69] time 0.453 (0.442) data 0.321 (0.311) loss_u loss_u 0.9146 (0.8847) acc_u 9.3750 (15.1136) lr 1.7604e-03 eta 0:00:06
epoch [47/200] batch [60/69] time 0.486 (0.447) data 0.354 (0.316) loss_u loss_u 0.8750 (0.8849) acc_u 12.5000 (14.8958) lr 1.7604e-03 eta 0:00:04
epoch [47/200] batch [65/69] time 0.458 (0.447) data 0.327 (0.316) loss_u loss_u 0.8315 (0.8822) acc_u 21.8750 (15.3846) lr 1.7604e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1676
confident_label rate tensor(0.2918, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 915
clean true:805
clean false:110
clean_rate:0.8797814207650273
noisy true:655
noisy false:1566
after delete: len(clean_dataset) 915
after delete: len(noisy_dataset) 2221
epoch [48/200] batch [5/28] time 0.617 (0.530) data 0.485 (0.398) loss_x loss_x 1.2432 (1.4422) acc_x 68.7500 (65.6250) lr 1.7501e-03 eta 0:00:12
epoch [48/200] batch [10/28] time 0.481 (0.523) data 0.351 (0.392) loss_x loss_x 1.1299 (1.3845) acc_x 78.1250 (66.5625) lr 1.7501e-03 eta 0:00:09
epoch [48/200] batch [15/28] time 0.443 (0.503) data 0.313 (0.372) loss_x loss_x 1.3740 (1.4320) acc_x 65.6250 (63.7500) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [20/28] time 0.554 (0.499) data 0.423 (0.368) loss_x loss_x 1.1670 (1.3584) acc_x 81.2500 (66.5625) lr 1.7501e-03 eta 0:00:03
epoch [48/200] batch [25/28] time 0.407 (0.492) data 0.276 (0.361) loss_x loss_x 1.4873 (1.4096) acc_x 65.6250 (66.2500) lr 1.7501e-03 eta 0:00:01
epoch [48/200] batch [5/69] time 0.474 (0.475) data 0.342 (0.344) loss_u loss_u 0.8955 (0.8833) acc_u 9.3750 (13.7500) lr 1.7501e-03 eta 0:00:30
epoch [48/200] batch [10/69] time 0.416 (0.472) data 0.284 (0.341) loss_u loss_u 0.9487 (0.8905) acc_u 6.2500 (13.7500) lr 1.7501e-03 eta 0:00:27
epoch [48/200] batch [15/69] time 0.571 (0.470) data 0.440 (0.339) loss_u loss_u 0.9067 (0.8868) acc_u 6.2500 (13.7500) lr 1.7501e-03 eta 0:00:25
epoch [48/200] batch [20/69] time 0.398 (0.466) data 0.267 (0.335) loss_u loss_u 0.8486 (0.8873) acc_u 15.6250 (13.7500) lr 1.7501e-03 eta 0:00:22
epoch [48/200] batch [25/69] time 0.528 (0.463) data 0.396 (0.332) loss_u loss_u 0.8237 (0.8841) acc_u 18.7500 (14.3750) lr 1.7501e-03 eta 0:00:20
epoch [48/200] batch [30/69] time 0.365 (0.457) data 0.233 (0.325) loss_u loss_u 0.8901 (0.8822) acc_u 15.6250 (14.7917) lr 1.7501e-03 eta 0:00:17
epoch [48/200] batch [35/69] time 0.439 (0.454) data 0.307 (0.323) loss_u loss_u 0.9253 (0.8841) acc_u 6.2500 (14.3750) lr 1.7501e-03 eta 0:00:15
epoch [48/200] batch [40/69] time 0.352 (0.458) data 0.219 (0.327) loss_u loss_u 0.9102 (0.8851) acc_u 12.5000 (13.9062) lr 1.7501e-03 eta 0:00:13
epoch [48/200] batch [45/69] time 0.526 (0.458) data 0.394 (0.327) loss_u loss_u 0.8960 (0.8863) acc_u 12.5000 (13.8194) lr 1.7501e-03 eta 0:00:11
epoch [48/200] batch [50/69] time 0.404 (0.457) data 0.272 (0.326) loss_u loss_u 0.8784 (0.8847) acc_u 12.5000 (14.0625) lr 1.7501e-03 eta 0:00:08
epoch [48/200] batch [55/69] time 0.442 (0.453) data 0.310 (0.322) loss_u loss_u 0.8970 (0.8823) acc_u 12.5000 (14.4318) lr 1.7501e-03 eta 0:00:06
epoch [48/200] batch [60/69] time 0.379 (0.454) data 0.246 (0.323) loss_u loss_u 0.8599 (0.8833) acc_u 18.7500 (14.2708) lr 1.7501e-03 eta 0:00:04
epoch [48/200] batch [65/69] time 0.430 (0.455) data 0.299 (0.323) loss_u loss_u 0.8657 (0.8847) acc_u 15.6250 (14.1346) lr 1.7501e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1639
confident_label rate tensor(0.2864, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 898
clean true:807
clean false:91
clean_rate:0.8986636971046771
noisy true:690
noisy false:1548
after delete: len(clean_dataset) 898
after delete: len(noisy_dataset) 2238
epoch [49/200] batch [5/28] time 0.425 (0.426) data 0.295 (0.296) loss_x loss_x 1.4375 (1.2629) acc_x 59.3750 (65.6250) lr 1.7396e-03 eta 0:00:09
epoch [49/200] batch [10/28] time 0.485 (0.449) data 0.354 (0.318) loss_x loss_x 1.7002 (1.4191) acc_x 62.5000 (63.1250) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [15/28] time 0.454 (0.449) data 0.324 (0.319) loss_x loss_x 1.2197 (1.3383) acc_x 71.8750 (65.2083) lr 1.7396e-03 eta 0:00:05
epoch [49/200] batch [20/28] time 0.412 (0.437) data 0.281 (0.307) loss_x loss_x 1.6377 (1.3649) acc_x 62.5000 (65.1562) lr 1.7396e-03 eta 0:00:03
epoch [49/200] batch [25/28] time 0.477 (0.443) data 0.346 (0.312) loss_x loss_x 1.8896 (1.4058) acc_x 59.3750 (64.3750) lr 1.7396e-03 eta 0:00:01
epoch [49/200] batch [5/69] time 0.403 (0.445) data 0.272 (0.314) loss_u loss_u 0.8833 (0.8855) acc_u 18.7500 (15.6250) lr 1.7396e-03 eta 0:00:28
epoch [49/200] batch [10/69] time 0.511 (0.454) data 0.380 (0.323) loss_u loss_u 0.8809 (0.8800) acc_u 21.8750 (16.2500) lr 1.7396e-03 eta 0:00:26
epoch [49/200] batch [15/69] time 0.528 (0.461) data 0.397 (0.329) loss_u loss_u 0.9146 (0.8798) acc_u 9.3750 (15.6250) lr 1.7396e-03 eta 0:00:24
epoch [49/200] batch [20/69] time 0.439 (0.460) data 0.308 (0.329) loss_u loss_u 0.8926 (0.8860) acc_u 15.6250 (14.6875) lr 1.7396e-03 eta 0:00:22
epoch [49/200] batch [25/69] time 0.435 (0.458) data 0.305 (0.327) loss_u loss_u 0.8579 (0.8846) acc_u 15.6250 (14.5000) lr 1.7396e-03 eta 0:00:20
epoch [49/200] batch [30/69] time 0.506 (0.457) data 0.376 (0.326) loss_u loss_u 0.9062 (0.8874) acc_u 15.6250 (14.5833) lr 1.7396e-03 eta 0:00:17
epoch [49/200] batch [35/69] time 0.415 (0.452) data 0.283 (0.321) loss_u loss_u 0.8540 (0.8901) acc_u 18.7500 (14.2857) lr 1.7396e-03 eta 0:00:15
epoch [49/200] batch [40/69] time 0.454 (0.455) data 0.322 (0.324) loss_u loss_u 0.9395 (0.8927) acc_u 3.1250 (13.8281) lr 1.7396e-03 eta 0:00:13
epoch [49/200] batch [45/69] time 0.332 (0.456) data 0.201 (0.325) loss_u loss_u 0.8335 (0.8909) acc_u 15.6250 (14.0972) lr 1.7396e-03 eta 0:00:10
epoch [49/200] batch [50/69] time 0.447 (0.454) data 0.316 (0.323) loss_u loss_u 0.9561 (0.8873) acc_u 6.2500 (14.5000) lr 1.7396e-03 eta 0:00:08
epoch [49/200] batch [55/69] time 0.528 (0.455) data 0.397 (0.323) loss_u loss_u 0.8589 (0.8843) acc_u 12.5000 (14.6591) lr 1.7396e-03 eta 0:00:06
epoch [49/200] batch [60/69] time 0.370 (0.450) data 0.239 (0.319) loss_u loss_u 0.8721 (0.8853) acc_u 18.7500 (14.5833) lr 1.7396e-03 eta 0:00:04
epoch [49/200] batch [65/69] time 0.465 (0.453) data 0.334 (0.321) loss_u loss_u 0.9185 (0.8872) acc_u 6.2500 (14.3750) lr 1.7396e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1740
confident_label rate tensor(0.2777, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 871
clean true:773
clean false:98
clean_rate:0.8874856486796785
noisy true:623
noisy false:1642
after delete: len(clean_dataset) 871
after delete: len(noisy_dataset) 2265
epoch [50/200] batch [5/27] time 0.462 (0.404) data 0.331 (0.273) loss_x loss_x 1.3613 (1.2949) acc_x 50.0000 (63.1250) lr 1.7290e-03 eta 0:00:08
epoch [50/200] batch [10/27] time 0.429 (0.426) data 0.298 (0.295) loss_x loss_x 1.0244 (1.2604) acc_x 65.6250 (64.0625) lr 1.7290e-03 eta 0:00:07
epoch [50/200] batch [15/27] time 0.402 (0.426) data 0.272 (0.295) loss_x loss_x 1.1855 (1.3283) acc_x 65.6250 (64.3750) lr 1.7290e-03 eta 0:00:05
epoch [50/200] batch [20/27] time 0.398 (0.432) data 0.268 (0.302) loss_x loss_x 1.4658 (1.3221) acc_x 59.3750 (65.0000) lr 1.7290e-03 eta 0:00:03
epoch [50/200] batch [25/27] time 0.406 (0.435) data 0.275 (0.305) loss_x loss_x 1.6377 (1.3091) acc_x 56.2500 (65.6250) lr 1.7290e-03 eta 0:00:00
epoch [50/200] batch [5/70] time 0.372 (0.446) data 0.241 (0.315) loss_u loss_u 0.9092 (0.8696) acc_u 6.2500 (15.0000) lr 1.7290e-03 eta 0:00:28
epoch [50/200] batch [10/70] time 0.397 (0.443) data 0.266 (0.312) loss_u loss_u 0.7847 (0.8500) acc_u 28.1250 (18.1250) lr 1.7290e-03 eta 0:00:26
epoch [50/200] batch [15/70] time 0.418 (0.441) data 0.287 (0.310) loss_u loss_u 0.8745 (0.8680) acc_u 15.6250 (16.2500) lr 1.7290e-03 eta 0:00:24
epoch [50/200] batch [20/70] time 0.626 (0.448) data 0.495 (0.317) loss_u loss_u 0.8525 (0.8706) acc_u 15.6250 (15.9375) lr 1.7290e-03 eta 0:00:22
epoch [50/200] batch [25/70] time 0.453 (0.448) data 0.322 (0.317) loss_u loss_u 0.9121 (0.8724) acc_u 9.3750 (15.7500) lr 1.7290e-03 eta 0:00:20
epoch [50/200] batch [30/70] time 0.572 (0.453) data 0.440 (0.322) loss_u loss_u 0.8262 (0.8714) acc_u 21.8750 (16.3542) lr 1.7290e-03 eta 0:00:18
epoch [50/200] batch [35/70] time 0.439 (0.453) data 0.307 (0.322) loss_u loss_u 0.9419 (0.8722) acc_u 9.3750 (16.3393) lr 1.7290e-03 eta 0:00:15
epoch [50/200] batch [40/70] time 0.379 (0.450) data 0.249 (0.319) loss_u loss_u 0.9375 (0.8771) acc_u 6.2500 (15.7812) lr 1.7290e-03 eta 0:00:13
epoch [50/200] batch [45/70] time 0.411 (0.446) data 0.281 (0.315) loss_u loss_u 0.9160 (0.8777) acc_u 12.5000 (15.7639) lr 1.7290e-03 eta 0:00:11
epoch [50/200] batch [50/70] time 0.432 (0.447) data 0.302 (0.316) loss_u loss_u 0.8481 (0.8777) acc_u 18.7500 (15.7500) lr 1.7290e-03 eta 0:00:08
epoch [50/200] batch [55/70] time 0.418 (0.447) data 0.288 (0.315) loss_u loss_u 0.9077 (0.8778) acc_u 18.7500 (15.9091) lr 1.7290e-03 eta 0:00:06
epoch [50/200] batch [60/70] time 0.533 (0.448) data 0.402 (0.317) loss_u loss_u 0.8013 (0.8784) acc_u 25.0000 (15.9375) lr 1.7290e-03 eta 0:00:04
epoch [50/200] batch [65/70] time 0.468 (0.449) data 0.338 (0.317) loss_u loss_u 0.8525 (0.8771) acc_u 21.8750 (16.1058) lr 1.7290e-03 eta 0:00:02
epoch [50/200] batch [70/70] time 0.564 (0.448) data 0.433 (0.317) loss_u loss_u 0.8569 (0.8739) acc_u 21.8750 (16.4286) lr 1.7290e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1677
confident_label rate tensor(0.2927, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 918
clean true:814
clean false:104
clean_rate:0.8867102396514162
noisy true:645
noisy false:1573
after delete: len(clean_dataset) 918
after delete: len(noisy_dataset) 2218
epoch [51/200] batch [5/28] time 0.382 (0.408) data 0.251 (0.277) loss_x loss_x 1.8994 (1.4465) acc_x 56.2500 (66.2500) lr 1.7181e-03 eta 0:00:09
epoch [51/200] batch [10/28] time 0.571 (0.435) data 0.441 (0.305) loss_x loss_x 0.9976 (1.3570) acc_x 68.7500 (66.5625) lr 1.7181e-03 eta 0:00:07
epoch [51/200] batch [15/28] time 0.502 (0.443) data 0.371 (0.312) loss_x loss_x 0.9077 (1.2693) acc_x 65.6250 (67.2917) lr 1.7181e-03 eta 0:00:05
epoch [51/200] batch [20/28] time 0.420 (0.436) data 0.290 (0.305) loss_x loss_x 1.5967 (1.2954) acc_x 65.6250 (67.3438) lr 1.7181e-03 eta 0:00:03
epoch [51/200] batch [25/28] time 0.389 (0.444) data 0.258 (0.314) loss_x loss_x 1.5225 (1.2769) acc_x 59.3750 (67.7500) lr 1.7181e-03 eta 0:00:01
epoch [51/200] batch [5/69] time 0.452 (0.449) data 0.322 (0.318) loss_u loss_u 0.8867 (0.8827) acc_u 9.3750 (13.7500) lr 1.7181e-03 eta 0:00:28
epoch [51/200] batch [10/69] time 0.651 (0.462) data 0.519 (0.332) loss_u loss_u 0.8911 (0.8788) acc_u 15.6250 (15.9375) lr 1.7181e-03 eta 0:00:27
epoch [51/200] batch [15/69] time 0.564 (0.460) data 0.434 (0.329) loss_u loss_u 0.9453 (0.8861) acc_u 9.3750 (14.1667) lr 1.7181e-03 eta 0:00:24
epoch [51/200] batch [20/69] time 0.532 (0.457) data 0.401 (0.327) loss_u loss_u 0.7993 (0.8804) acc_u 28.1250 (14.8438) lr 1.7181e-03 eta 0:00:22
epoch [51/200] batch [25/69] time 0.538 (0.460) data 0.408 (0.329) loss_u loss_u 0.9448 (0.8852) acc_u 3.1250 (14.0000) lr 1.7181e-03 eta 0:00:20
epoch [51/200] batch [30/69] time 0.585 (0.460) data 0.453 (0.329) loss_u loss_u 0.9390 (0.8811) acc_u 6.2500 (14.7917) lr 1.7181e-03 eta 0:00:17
epoch [51/200] batch [35/69] time 0.390 (0.454) data 0.258 (0.323) loss_u loss_u 0.9185 (0.8828) acc_u 6.2500 (14.5536) lr 1.7181e-03 eta 0:00:15
epoch [51/200] batch [40/69] time 0.351 (0.453) data 0.219 (0.322) loss_u loss_u 0.7573 (0.8769) acc_u 28.1250 (15.0000) lr 1.7181e-03 eta 0:00:13
epoch [51/200] batch [45/69] time 0.334 (0.451) data 0.203 (0.320) loss_u loss_u 0.8599 (0.8774) acc_u 12.5000 (14.9306) lr 1.7181e-03 eta 0:00:10
epoch [51/200] batch [50/69] time 0.422 (0.450) data 0.290 (0.319) loss_u loss_u 0.8267 (0.8778) acc_u 15.6250 (14.6875) lr 1.7181e-03 eta 0:00:08
epoch [51/200] batch [55/69] time 0.460 (0.448) data 0.328 (0.317) loss_u loss_u 0.8750 (0.8784) acc_u 15.6250 (14.3750) lr 1.7181e-03 eta 0:00:06
epoch [51/200] batch [60/69] time 0.558 (0.446) data 0.426 (0.315) loss_u loss_u 0.8877 (0.8779) acc_u 15.6250 (14.6354) lr 1.7181e-03 eta 0:00:04
epoch [51/200] batch [65/69] time 0.460 (0.443) data 0.329 (0.312) loss_u loss_u 0.9277 (0.8811) acc_u 12.5000 (14.3750) lr 1.7181e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1682
confident_label rate tensor(0.2921, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 916
clean true:799
clean false:117
clean_rate:0.8722707423580786
noisy true:655
noisy false:1565
after delete: len(clean_dataset) 916
after delete: len(noisy_dataset) 2220
epoch [52/200] batch [5/28] time 0.403 (0.451) data 0.273 (0.321) loss_x loss_x 1.4258 (1.3179) acc_x 59.3750 (65.0000) lr 1.7071e-03 eta 0:00:10
epoch [52/200] batch [10/28] time 0.461 (0.473) data 0.331 (0.342) loss_x loss_x 1.7578 (1.4105) acc_x 56.2500 (63.1250) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [15/28] time 0.415 (0.459) data 0.284 (0.329) loss_x loss_x 0.7959 (1.3285) acc_x 81.2500 (65.0000) lr 1.7071e-03 eta 0:00:05
epoch [52/200] batch [20/28] time 0.395 (0.463) data 0.265 (0.333) loss_x loss_x 1.9561 (1.3492) acc_x 56.2500 (64.6875) lr 1.7071e-03 eta 0:00:03
epoch [52/200] batch [25/28] time 0.391 (0.458) data 0.261 (0.327) loss_x loss_x 1.4658 (1.3379) acc_x 65.6250 (65.7500) lr 1.7071e-03 eta 0:00:01
epoch [52/200] batch [5/69] time 0.350 (0.446) data 0.219 (0.315) loss_u loss_u 0.8042 (0.8855) acc_u 28.1250 (15.0000) lr 1.7071e-03 eta 0:00:28
epoch [52/200] batch [10/69] time 0.491 (0.451) data 0.360 (0.320) loss_u loss_u 0.9004 (0.8973) acc_u 9.3750 (10.9375) lr 1.7071e-03 eta 0:00:26
epoch [52/200] batch [15/69] time 0.422 (0.448) data 0.290 (0.317) loss_u loss_u 0.8647 (0.8886) acc_u 18.7500 (12.9167) lr 1.7071e-03 eta 0:00:24
epoch [52/200] batch [20/69] time 0.500 (0.460) data 0.367 (0.329) loss_u loss_u 0.9185 (0.8873) acc_u 9.3750 (13.1250) lr 1.7071e-03 eta 0:00:22
epoch [52/200] batch [25/69] time 0.402 (0.460) data 0.271 (0.329) loss_u loss_u 0.8823 (0.8827) acc_u 18.7500 (14.3750) lr 1.7071e-03 eta 0:00:20
epoch [52/200] batch [30/69] time 0.727 (0.461) data 0.596 (0.330) loss_u loss_u 0.8745 (0.8831) acc_u 12.5000 (14.4792) lr 1.7071e-03 eta 0:00:17
epoch [52/200] batch [35/69] time 0.377 (0.459) data 0.245 (0.328) loss_u loss_u 0.8721 (0.8813) acc_u 21.8750 (14.7321) lr 1.7071e-03 eta 0:00:15
epoch [52/200] batch [40/69] time 0.381 (0.461) data 0.249 (0.330) loss_u loss_u 0.8438 (0.8805) acc_u 18.7500 (14.6875) lr 1.7071e-03 eta 0:00:13
epoch [52/200] batch [45/69] time 0.401 (0.459) data 0.270 (0.328) loss_u loss_u 0.9165 (0.8801) acc_u 9.3750 (14.7917) lr 1.7071e-03 eta 0:00:11
epoch [52/200] batch [50/69] time 0.488 (0.458) data 0.357 (0.327) loss_u loss_u 0.9009 (0.8774) acc_u 15.6250 (15.1250) lr 1.7071e-03 eta 0:00:08
epoch [52/200] batch [55/69] time 0.373 (0.454) data 0.242 (0.323) loss_u loss_u 0.9321 (0.8767) acc_u 9.3750 (15.3409) lr 1.7071e-03 eta 0:00:06
epoch [52/200] batch [60/69] time 0.422 (0.453) data 0.290 (0.321) loss_u loss_u 0.8452 (0.8766) acc_u 21.8750 (15.4688) lr 1.7071e-03 eta 0:00:04
epoch [52/200] batch [65/69] time 0.431 (0.452) data 0.299 (0.320) loss_u loss_u 0.8516 (0.8784) acc_u 21.8750 (15.4327) lr 1.7071e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1642
confident_label rate tensor(0.2930, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 919
clean true:815
clean false:104
clean_rate:0.8868335146898803
noisy true:679
noisy false:1538
after delete: len(clean_dataset) 919
after delete: len(noisy_dataset) 2217
epoch [53/200] batch [5/28] time 0.456 (0.429) data 0.324 (0.298) loss_x loss_x 0.9917 (1.0884) acc_x 75.0000 (73.1250) lr 1.6959e-03 eta 0:00:09
epoch [53/200] batch [10/28] time 0.512 (0.454) data 0.381 (0.323) loss_x loss_x 1.0557 (1.2397) acc_x 75.0000 (66.8750) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [15/28] time 0.522 (0.469) data 0.392 (0.338) loss_x loss_x 0.8369 (1.2743) acc_x 68.7500 (66.2500) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [20/28] time 0.432 (0.462) data 0.302 (0.332) loss_x loss_x 1.2900 (1.2354) acc_x 68.7500 (67.3438) lr 1.6959e-03 eta 0:00:03
epoch [53/200] batch [25/28] time 0.411 (0.461) data 0.281 (0.330) loss_x loss_x 1.4668 (1.2379) acc_x 65.6250 (68.0000) lr 1.6959e-03 eta 0:00:01
epoch [53/200] batch [5/69] time 0.682 (0.471) data 0.551 (0.340) loss_u loss_u 0.9087 (0.9237) acc_u 12.5000 (9.3750) lr 1.6959e-03 eta 0:00:30
epoch [53/200] batch [10/69] time 0.509 (0.473) data 0.377 (0.342) loss_u loss_u 0.8716 (0.8708) acc_u 12.5000 (15.3125) lr 1.6959e-03 eta 0:00:27
epoch [53/200] batch [15/69] time 0.434 (0.474) data 0.303 (0.343) loss_u loss_u 0.9395 (0.8731) acc_u 9.3750 (15.8333) lr 1.6959e-03 eta 0:00:25
epoch [53/200] batch [20/69] time 0.374 (0.469) data 0.243 (0.338) loss_u loss_u 0.8667 (0.8785) acc_u 15.6250 (14.8438) lr 1.6959e-03 eta 0:00:22
epoch [53/200] batch [25/69] time 0.538 (0.465) data 0.407 (0.334) loss_u loss_u 0.8950 (0.8799) acc_u 18.7500 (15.1250) lr 1.6959e-03 eta 0:00:20
epoch [53/200] batch [30/69] time 0.377 (0.457) data 0.246 (0.326) loss_u loss_u 0.9380 (0.8832) acc_u 6.2500 (14.6875) lr 1.6959e-03 eta 0:00:17
epoch [53/200] batch [35/69] time 0.421 (0.454) data 0.290 (0.323) loss_u loss_u 0.9170 (0.8832) acc_u 6.2500 (14.4643) lr 1.6959e-03 eta 0:00:15
epoch [53/200] batch [40/69] time 0.443 (0.452) data 0.311 (0.321) loss_u loss_u 0.8584 (0.8838) acc_u 15.6250 (14.1406) lr 1.6959e-03 eta 0:00:13
epoch [53/200] batch [45/69] time 0.438 (0.456) data 0.307 (0.325) loss_u loss_u 0.9019 (0.8845) acc_u 12.5000 (14.2361) lr 1.6959e-03 eta 0:00:10
epoch [53/200] batch [50/69] time 0.403 (0.455) data 0.271 (0.323) loss_u loss_u 0.8135 (0.8817) acc_u 21.8750 (14.6875) lr 1.6959e-03 eta 0:00:08
epoch [53/200] batch [55/69] time 0.576 (0.456) data 0.445 (0.325) loss_u loss_u 0.8457 (0.8815) acc_u 15.6250 (14.7159) lr 1.6959e-03 eta 0:00:06
epoch [53/200] batch [60/69] time 0.495 (0.457) data 0.364 (0.326) loss_u loss_u 0.8721 (0.8822) acc_u 15.6250 (14.6875) lr 1.6959e-03 eta 0:00:04
epoch [53/200] batch [65/69] time 0.368 (0.454) data 0.238 (0.323) loss_u loss_u 0.9624 (0.8826) acc_u 3.1250 (14.6635) lr 1.6959e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1669
confident_label rate tensor(0.2905, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 911
clean true:813
clean false:98
clean_rate:0.8924259055982436
noisy true:654
noisy false:1571
after delete: len(clean_dataset) 911
after delete: len(noisy_dataset) 2225
epoch [54/200] batch [5/28] time 0.445 (0.463) data 0.314 (0.332) loss_x loss_x 1.4229 (1.1960) acc_x 68.7500 (68.7500) lr 1.6845e-03 eta 0:00:10
epoch [54/200] batch [10/28] time 0.365 (0.465) data 0.235 (0.334) loss_x loss_x 1.4922 (1.2420) acc_x 56.2500 (65.3125) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [15/28] time 0.470 (0.461) data 0.340 (0.331) loss_x loss_x 0.9937 (1.3082) acc_x 71.8750 (65.4167) lr 1.6845e-03 eta 0:00:05
epoch [54/200] batch [20/28] time 0.406 (0.456) data 0.276 (0.325) loss_x loss_x 1.1553 (1.3662) acc_x 68.7500 (64.0625) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [25/28] time 0.493 (0.461) data 0.364 (0.331) loss_x loss_x 1.2959 (1.3917) acc_x 62.5000 (64.0000) lr 1.6845e-03 eta 0:00:01
epoch [54/200] batch [5/69] time 0.400 (0.453) data 0.269 (0.323) loss_u loss_u 0.8853 (0.8962) acc_u 15.6250 (12.5000) lr 1.6845e-03 eta 0:00:29
epoch [54/200] batch [10/69] time 0.387 (0.449) data 0.257 (0.318) loss_u loss_u 0.9307 (0.8994) acc_u 9.3750 (12.5000) lr 1.6845e-03 eta 0:00:26
epoch [54/200] batch [15/69] time 0.300 (0.444) data 0.169 (0.313) loss_u loss_u 0.9307 (0.9030) acc_u 9.3750 (12.0833) lr 1.6845e-03 eta 0:00:23
epoch [54/200] batch [20/69] time 0.403 (0.436) data 0.273 (0.305) loss_u loss_u 0.9238 (0.9012) acc_u 12.5000 (12.3438) lr 1.6845e-03 eta 0:00:21
epoch [54/200] batch [25/69] time 0.396 (0.433) data 0.265 (0.302) loss_u loss_u 0.8442 (0.8973) acc_u 18.7500 (13.0000) lr 1.6845e-03 eta 0:00:19
epoch [54/200] batch [30/69] time 0.592 (0.436) data 0.462 (0.306) loss_u loss_u 0.8516 (0.8939) acc_u 18.7500 (13.0208) lr 1.6845e-03 eta 0:00:17
epoch [54/200] batch [35/69] time 0.526 (0.435) data 0.395 (0.305) loss_u loss_u 0.8101 (0.8918) acc_u 25.0000 (13.4821) lr 1.6845e-03 eta 0:00:14
epoch [54/200] batch [40/69] time 0.384 (0.435) data 0.252 (0.304) loss_u loss_u 0.9043 (0.8914) acc_u 9.3750 (13.7500) lr 1.6845e-03 eta 0:00:12
epoch [54/200] batch [45/69] time 0.603 (0.437) data 0.472 (0.306) loss_u loss_u 0.8301 (0.8873) acc_u 25.0000 (14.5139) lr 1.6845e-03 eta 0:00:10
epoch [54/200] batch [50/69] time 0.368 (0.436) data 0.237 (0.305) loss_u loss_u 0.9268 (0.8879) acc_u 9.3750 (14.4375) lr 1.6845e-03 eta 0:00:08
epoch [54/200] batch [55/69] time 0.489 (0.440) data 0.359 (0.309) loss_u loss_u 0.7896 (0.8837) acc_u 31.2500 (15.1136) lr 1.6845e-03 eta 0:00:06
epoch [54/200] batch [60/69] time 0.368 (0.437) data 0.237 (0.306) loss_u loss_u 0.8721 (0.8805) acc_u 15.6250 (15.2604) lr 1.6845e-03 eta 0:00:03
epoch [54/200] batch [65/69] time 0.395 (0.438) data 0.264 (0.307) loss_u loss_u 0.8633 (0.8834) acc_u 12.5000 (14.6635) lr 1.6845e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1689
confident_label rate tensor(0.2851, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 894
clean true:792
clean false:102
clean_rate:0.8859060402684564
noisy true:655
noisy false:1587
after delete: len(clean_dataset) 894
after delete: len(noisy_dataset) 2242
epoch [55/200] batch [5/27] time 0.445 (0.479) data 0.314 (0.349) loss_x loss_x 1.0234 (1.1387) acc_x 75.0000 (66.8750) lr 1.6730e-03 eta 0:00:10
epoch [55/200] batch [10/27] time 0.467 (0.484) data 0.335 (0.353) loss_x loss_x 1.3096 (1.1547) acc_x 65.6250 (68.1250) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [15/27] time 0.481 (0.462) data 0.350 (0.331) loss_x loss_x 1.1445 (1.2287) acc_x 75.0000 (66.4583) lr 1.6730e-03 eta 0:00:05
epoch [55/200] batch [20/27] time 0.452 (0.455) data 0.322 (0.324) loss_x loss_x 0.7910 (1.2096) acc_x 71.8750 (66.7188) lr 1.6730e-03 eta 0:00:03
epoch [55/200] batch [25/27] time 0.530 (0.461) data 0.399 (0.330) loss_x loss_x 1.1680 (1.2142) acc_x 62.5000 (65.8750) lr 1.6730e-03 eta 0:00:00
epoch [55/200] batch [5/70] time 0.324 (0.459) data 0.193 (0.329) loss_u loss_u 0.8765 (0.8577) acc_u 18.7500 (16.8750) lr 1.6730e-03 eta 0:00:29
epoch [55/200] batch [10/70] time 0.466 (0.458) data 0.334 (0.327) loss_u loss_u 0.8569 (0.8646) acc_u 18.7500 (16.8750) lr 1.6730e-03 eta 0:00:27
epoch [55/200] batch [15/70] time 0.453 (0.458) data 0.321 (0.327) loss_u loss_u 0.8862 (0.8677) acc_u 9.3750 (16.4583) lr 1.6730e-03 eta 0:00:25
epoch [55/200] batch [20/70] time 0.324 (0.448) data 0.194 (0.317) loss_u loss_u 0.8340 (0.8696) acc_u 28.1250 (17.0312) lr 1.6730e-03 eta 0:00:22
epoch [55/200] batch [25/70] time 0.319 (0.445) data 0.188 (0.314) loss_u loss_u 0.9214 (0.8760) acc_u 12.5000 (16.3750) lr 1.6730e-03 eta 0:00:20
epoch [55/200] batch [30/70] time 0.385 (0.444) data 0.254 (0.313) loss_u loss_u 0.8936 (0.8798) acc_u 9.3750 (15.9375) lr 1.6730e-03 eta 0:00:17
epoch [55/200] batch [35/70] time 0.441 (0.445) data 0.311 (0.314) loss_u loss_u 0.8691 (0.8807) acc_u 12.5000 (15.6250) lr 1.6730e-03 eta 0:00:15
epoch [55/200] batch [40/70] time 0.400 (0.444) data 0.269 (0.313) loss_u loss_u 0.9199 (0.8806) acc_u 12.5000 (15.6250) lr 1.6730e-03 eta 0:00:13
epoch [55/200] batch [45/70] time 0.379 (0.442) data 0.249 (0.311) loss_u loss_u 0.8867 (0.8809) acc_u 12.5000 (15.4167) lr 1.6730e-03 eta 0:00:11
epoch [55/200] batch [50/70] time 0.412 (0.440) data 0.281 (0.309) loss_u loss_u 0.8496 (0.8793) acc_u 21.8750 (15.5000) lr 1.6730e-03 eta 0:00:08
epoch [55/200] batch [55/70] time 0.507 (0.441) data 0.375 (0.310) loss_u loss_u 0.8188 (0.8802) acc_u 18.7500 (15.1705) lr 1.6730e-03 eta 0:00:06
epoch [55/200] batch [60/70] time 0.489 (0.440) data 0.358 (0.309) loss_u loss_u 0.8521 (0.8815) acc_u 25.0000 (15.0521) lr 1.6730e-03 eta 0:00:04
epoch [55/200] batch [65/70] time 0.489 (0.439) data 0.358 (0.307) loss_u loss_u 0.8843 (0.8797) acc_u 15.6250 (15.3365) lr 1.6730e-03 eta 0:00:02
epoch [55/200] batch [70/70] time 0.369 (0.440) data 0.238 (0.309) loss_u loss_u 0.8877 (0.8814) acc_u 18.7500 (15.0893) lr 1.6730e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1642
confident_label rate tensor(0.2959, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 928
clean true:821
clean false:107
clean_rate:0.884698275862069
noisy true:673
noisy false:1535
after delete: len(clean_dataset) 928
after delete: len(noisy_dataset) 2208
epoch [56/200] batch [5/29] time 0.470 (0.447) data 0.339 (0.317) loss_x loss_x 1.3350 (1.3143) acc_x 56.2500 (63.1250) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [10/29] time 0.401 (0.467) data 0.271 (0.337) loss_x loss_x 0.7520 (1.1611) acc_x 71.8750 (65.6250) lr 1.6613e-03 eta 0:00:08
epoch [56/200] batch [15/29] time 0.430 (0.465) data 0.300 (0.334) loss_x loss_x 0.9268 (1.1213) acc_x 78.1250 (68.5417) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [20/29] time 0.390 (0.457) data 0.259 (0.327) loss_x loss_x 1.1455 (1.1945) acc_x 71.8750 (67.6562) lr 1.6613e-03 eta 0:00:04
epoch [56/200] batch [25/29] time 0.481 (0.450) data 0.351 (0.320) loss_x loss_x 1.1855 (1.1970) acc_x 71.8750 (68.1250) lr 1.6613e-03 eta 0:00:01
epoch [56/200] batch [5/69] time 0.529 (0.454) data 0.397 (0.323) loss_u loss_u 0.9229 (0.9062) acc_u 9.3750 (11.2500) lr 1.6613e-03 eta 0:00:29
epoch [56/200] batch [10/69] time 0.412 (0.451) data 0.280 (0.320) loss_u loss_u 0.9082 (0.8996) acc_u 9.3750 (12.5000) lr 1.6613e-03 eta 0:00:26
epoch [56/200] batch [15/69] time 0.338 (0.452) data 0.206 (0.321) loss_u loss_u 0.9360 (0.8944) acc_u 6.2500 (12.7083) lr 1.6613e-03 eta 0:00:24
epoch [56/200] batch [20/69] time 0.367 (0.446) data 0.235 (0.315) loss_u loss_u 0.9219 (0.8925) acc_u 6.2500 (12.6562) lr 1.6613e-03 eta 0:00:21
epoch [56/200] batch [25/69] time 0.454 (0.450) data 0.324 (0.319) loss_u loss_u 0.8730 (0.8884) acc_u 18.7500 (12.8750) lr 1.6613e-03 eta 0:00:19
epoch [56/200] batch [30/69] time 0.485 (0.449) data 0.355 (0.318) loss_u loss_u 0.8931 (0.8863) acc_u 12.5000 (13.3333) lr 1.6613e-03 eta 0:00:17
epoch [56/200] batch [35/69] time 0.419 (0.444) data 0.288 (0.313) loss_u loss_u 0.8843 (0.8896) acc_u 9.3750 (12.7679) lr 1.6613e-03 eta 0:00:15
epoch [56/200] batch [40/69] time 0.452 (0.443) data 0.320 (0.312) loss_u loss_u 0.9277 (0.8947) acc_u 12.5000 (12.2656) lr 1.6613e-03 eta 0:00:12
epoch [56/200] batch [45/69] time 0.553 (0.440) data 0.423 (0.309) loss_u loss_u 0.8247 (0.8917) acc_u 28.1250 (13.1250) lr 1.6613e-03 eta 0:00:10
epoch [56/200] batch [50/69] time 0.548 (0.440) data 0.417 (0.309) loss_u loss_u 0.9155 (0.8881) acc_u 12.5000 (13.8750) lr 1.6613e-03 eta 0:00:08
epoch [56/200] batch [55/69] time 0.332 (0.441) data 0.200 (0.310) loss_u loss_u 0.8818 (0.8867) acc_u 12.5000 (14.2614) lr 1.6613e-03 eta 0:00:06
epoch [56/200] batch [60/69] time 0.336 (0.444) data 0.204 (0.313) loss_u loss_u 0.8970 (0.8854) acc_u 9.3750 (14.4792) lr 1.6613e-03 eta 0:00:03
epoch [56/200] batch [65/69] time 0.382 (0.442) data 0.252 (0.311) loss_u loss_u 0.8008 (0.8844) acc_u 28.1250 (14.6154) lr 1.6613e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1689
confident_label rate tensor(0.2915, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 914
clean true:803
clean false:111
clean_rate:0.8785557986870897
noisy true:644
noisy false:1578
after delete: len(clean_dataset) 914
after delete: len(noisy_dataset) 2222
epoch [57/200] batch [5/28] time 0.343 (0.460) data 0.213 (0.330) loss_x loss_x 1.1045 (1.3496) acc_x 71.8750 (63.7500) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [10/28] time 0.428 (0.454) data 0.298 (0.323) loss_x loss_x 1.0752 (1.3438) acc_x 62.5000 (62.8125) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [15/28] time 0.475 (0.447) data 0.345 (0.317) loss_x loss_x 1.7041 (1.3462) acc_x 59.3750 (63.7500) lr 1.6494e-03 eta 0:00:05
epoch [57/200] batch [20/28] time 0.436 (0.459) data 0.307 (0.328) loss_x loss_x 1.0908 (1.3199) acc_x 71.8750 (63.7500) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [25/28] time 0.539 (0.465) data 0.408 (0.335) loss_x loss_x 1.9697 (1.3500) acc_x 59.3750 (64.0000) lr 1.6494e-03 eta 0:00:01
epoch [57/200] batch [5/69] time 0.590 (0.467) data 0.458 (0.336) loss_u loss_u 0.8813 (0.9104) acc_u 12.5000 (6.8750) lr 1.6494e-03 eta 0:00:29
epoch [57/200] batch [10/69] time 0.517 (0.462) data 0.386 (0.331) loss_u loss_u 0.8330 (0.8822) acc_u 15.6250 (12.5000) lr 1.6494e-03 eta 0:00:27
epoch [57/200] batch [15/69] time 0.400 (0.460) data 0.270 (0.329) loss_u loss_u 0.9038 (0.8855) acc_u 9.3750 (12.5000) lr 1.6494e-03 eta 0:00:24
epoch [57/200] batch [20/69] time 0.446 (0.451) data 0.314 (0.320) loss_u loss_u 0.9097 (0.8864) acc_u 12.5000 (12.5000) lr 1.6494e-03 eta 0:00:22
epoch [57/200] batch [25/69] time 0.496 (0.447) data 0.365 (0.316) loss_u loss_u 0.9785 (0.8860) acc_u 0.0000 (12.6250) lr 1.6494e-03 eta 0:00:19
epoch [57/200] batch [30/69] time 0.545 (0.449) data 0.414 (0.318) loss_u loss_u 0.8862 (0.8894) acc_u 18.7500 (12.6042) lr 1.6494e-03 eta 0:00:17
epoch [57/200] batch [35/69] time 0.377 (0.448) data 0.246 (0.317) loss_u loss_u 0.9155 (0.8881) acc_u 9.3750 (13.0357) lr 1.6494e-03 eta 0:00:15
epoch [57/200] batch [40/69] time 0.423 (0.446) data 0.293 (0.315) loss_u loss_u 0.8843 (0.8863) acc_u 18.7500 (13.6719) lr 1.6494e-03 eta 0:00:12
epoch [57/200] batch [45/69] time 0.607 (0.447) data 0.475 (0.317) loss_u loss_u 0.7935 (0.8856) acc_u 31.2500 (14.1667) lr 1.6494e-03 eta 0:00:10
epoch [57/200] batch [50/69] time 0.451 (0.447) data 0.321 (0.316) loss_u loss_u 0.8853 (0.8842) acc_u 15.6250 (14.5000) lr 1.6494e-03 eta 0:00:08
epoch [57/200] batch [55/69] time 0.366 (0.446) data 0.236 (0.315) loss_u loss_u 0.8750 (0.8837) acc_u 18.7500 (14.6591) lr 1.6494e-03 eta 0:00:06
epoch [57/200] batch [60/69] time 0.424 (0.443) data 0.293 (0.312) loss_u loss_u 0.8594 (0.8850) acc_u 15.6250 (14.3229) lr 1.6494e-03 eta 0:00:03
epoch [57/200] batch [65/69] time 0.380 (0.444) data 0.250 (0.314) loss_u loss_u 0.8760 (0.8861) acc_u 12.5000 (14.1346) lr 1.6494e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1665
confident_label rate tensor(0.2908, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 912
clean true:806
clean false:106
clean_rate:0.8837719298245614
noisy true:665
noisy false:1559
after delete: len(clean_dataset) 912
after delete: len(noisy_dataset) 2224
epoch [58/200] batch [5/28] time 0.547 (0.418) data 0.416 (0.287) loss_x loss_x 1.2949 (1.2324) acc_x 75.0000 (74.3750) lr 1.6374e-03 eta 0:00:09
epoch [58/200] batch [10/28] time 0.417 (0.414) data 0.287 (0.284) loss_x loss_x 1.4570 (1.2867) acc_x 59.3750 (67.5000) lr 1.6374e-03 eta 0:00:07
epoch [58/200] batch [15/28] time 0.537 (0.449) data 0.407 (0.319) loss_x loss_x 1.5312 (1.3453) acc_x 65.6250 (66.6667) lr 1.6374e-03 eta 0:00:05
epoch [58/200] batch [20/28] time 0.556 (0.457) data 0.426 (0.327) loss_x loss_x 1.1123 (1.3386) acc_x 65.6250 (67.0312) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [25/28] time 0.610 (0.457) data 0.480 (0.327) loss_x loss_x 1.9229 (1.3552) acc_x 56.2500 (65.7500) lr 1.6374e-03 eta 0:00:01
epoch [58/200] batch [5/69] time 0.453 (0.447) data 0.322 (0.316) loss_u loss_u 0.9009 (0.8816) acc_u 9.3750 (13.1250) lr 1.6374e-03 eta 0:00:28
epoch [58/200] batch [10/69] time 0.543 (0.453) data 0.412 (0.323) loss_u loss_u 0.8252 (0.8624) acc_u 21.8750 (17.5000) lr 1.6374e-03 eta 0:00:26
epoch [58/200] batch [15/69] time 0.716 (0.458) data 0.585 (0.328) loss_u loss_u 0.8457 (0.8588) acc_u 15.6250 (18.7500) lr 1.6374e-03 eta 0:00:24
epoch [58/200] batch [20/69] time 0.336 (0.456) data 0.206 (0.326) loss_u loss_u 0.8838 (0.8629) acc_u 9.3750 (17.6562) lr 1.6374e-03 eta 0:00:22
epoch [58/200] batch [25/69] time 0.381 (0.451) data 0.250 (0.320) loss_u loss_u 0.8735 (0.8700) acc_u 15.6250 (16.1250) lr 1.6374e-03 eta 0:00:19
epoch [58/200] batch [30/69] time 0.437 (0.446) data 0.306 (0.315) loss_u loss_u 0.8809 (0.8722) acc_u 9.3750 (15.5208) lr 1.6374e-03 eta 0:00:17
epoch [58/200] batch [35/69] time 0.399 (0.445) data 0.268 (0.314) loss_u loss_u 0.8291 (0.8723) acc_u 21.8750 (15.4464) lr 1.6374e-03 eta 0:00:15
epoch [58/200] batch [40/69] time 0.545 (0.445) data 0.413 (0.315) loss_u loss_u 0.8438 (0.8739) acc_u 18.7500 (15.0000) lr 1.6374e-03 eta 0:00:12
epoch [58/200] batch [45/69] time 0.434 (0.442) data 0.302 (0.312) loss_u loss_u 0.8193 (0.8732) acc_u 21.8750 (15.3472) lr 1.6374e-03 eta 0:00:10
epoch [58/200] batch [50/69] time 0.393 (0.443) data 0.261 (0.312) loss_u loss_u 0.8965 (0.8719) acc_u 12.5000 (15.5625) lr 1.6374e-03 eta 0:00:08
epoch [58/200] batch [55/69] time 0.393 (0.442) data 0.261 (0.311) loss_u loss_u 0.8364 (0.8726) acc_u 21.8750 (15.3977) lr 1.6374e-03 eta 0:00:06
epoch [58/200] batch [60/69] time 0.509 (0.440) data 0.379 (0.310) loss_u loss_u 0.9204 (0.8749) acc_u 12.5000 (15.2604) lr 1.6374e-03 eta 0:00:03
epoch [58/200] batch [65/69] time 0.405 (0.439) data 0.273 (0.308) loss_u loss_u 0.8857 (0.8745) acc_u 12.5000 (15.3365) lr 1.6374e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1630
confident_label rate tensor(0.2978, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 934
clean true:831
clean false:103
clean_rate:0.8897216274089935
noisy true:675
noisy false:1527
after delete: len(clean_dataset) 934
after delete: len(noisy_dataset) 2202
epoch [59/200] batch [5/29] time 0.454 (0.473) data 0.323 (0.343) loss_x loss_x 0.7896 (1.0782) acc_x 81.2500 (73.7500) lr 1.6252e-03 eta 0:00:11
epoch [59/200] batch [10/29] time 0.448 (0.470) data 0.318 (0.339) loss_x loss_x 1.3213 (1.1627) acc_x 75.0000 (70.9375) lr 1.6252e-03 eta 0:00:08
epoch [59/200] batch [15/29] time 0.392 (0.470) data 0.262 (0.340) loss_x loss_x 1.4189 (1.2493) acc_x 68.7500 (69.1667) lr 1.6252e-03 eta 0:00:06
epoch [59/200] batch [20/29] time 0.415 (0.464) data 0.284 (0.333) loss_x loss_x 1.0068 (1.2646) acc_x 75.0000 (68.9062) lr 1.6252e-03 eta 0:00:04
epoch [59/200] batch [25/29] time 0.355 (0.450) data 0.224 (0.319) loss_x loss_x 1.7812 (1.2915) acc_x 53.1250 (67.6250) lr 1.6252e-03 eta 0:00:01
epoch [59/200] batch [5/68] time 0.516 (0.455) data 0.385 (0.324) loss_u loss_u 0.8979 (0.8760) acc_u 12.5000 (15.6250) lr 1.6252e-03 eta 0:00:28
epoch [59/200] batch [10/68] time 0.450 (0.449) data 0.319 (0.318) loss_u loss_u 0.9111 (0.8799) acc_u 15.6250 (15.0000) lr 1.6252e-03 eta 0:00:26
epoch [59/200] batch [15/68] time 0.438 (0.446) data 0.307 (0.316) loss_u loss_u 0.8945 (0.8817) acc_u 9.3750 (15.4167) lr 1.6252e-03 eta 0:00:23
epoch [59/200] batch [20/68] time 0.490 (0.451) data 0.360 (0.320) loss_u loss_u 0.8203 (0.8844) acc_u 21.8750 (14.5312) lr 1.6252e-03 eta 0:00:21
epoch [59/200] batch [25/68] time 0.450 (0.448) data 0.320 (0.317) loss_u loss_u 0.8877 (0.8798) acc_u 15.6250 (15.0000) lr 1.6252e-03 eta 0:00:19
epoch [59/200] batch [30/68] time 0.402 (0.445) data 0.271 (0.315) loss_u loss_u 0.8208 (0.8785) acc_u 28.1250 (15.5208) lr 1.6252e-03 eta 0:00:16
epoch [59/200] batch [35/68] time 0.356 (0.444) data 0.226 (0.313) loss_u loss_u 0.8755 (0.8826) acc_u 15.6250 (15.1786) lr 1.6252e-03 eta 0:00:14
epoch [59/200] batch [40/68] time 0.332 (0.439) data 0.202 (0.309) loss_u loss_u 0.9253 (0.8843) acc_u 9.3750 (14.6875) lr 1.6252e-03 eta 0:00:12
epoch [59/200] batch [45/68] time 0.358 (0.437) data 0.226 (0.307) loss_u loss_u 0.8452 (0.8817) acc_u 18.7500 (15.0694) lr 1.6252e-03 eta 0:00:10
epoch [59/200] batch [50/68] time 0.503 (0.437) data 0.372 (0.307) loss_u loss_u 0.8394 (0.8798) acc_u 15.6250 (15.3750) lr 1.6252e-03 eta 0:00:07
epoch [59/200] batch [55/68] time 0.382 (0.438) data 0.251 (0.307) loss_u loss_u 0.8926 (0.8796) acc_u 15.6250 (15.3409) lr 1.6252e-03 eta 0:00:05
epoch [59/200] batch [60/68] time 0.579 (0.441) data 0.448 (0.310) loss_u loss_u 0.9097 (0.8824) acc_u 12.5000 (14.8958) lr 1.6252e-03 eta 0:00:03
epoch [59/200] batch [65/68] time 0.395 (0.440) data 0.264 (0.310) loss_u loss_u 0.9170 (0.8840) acc_u 9.3750 (14.8077) lr 1.6252e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1638
confident_label rate tensor(0.3026, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 949
clean true:837
clean false:112
clean_rate:0.8819810326659642
noisy true:661
noisy false:1526
after delete: len(clean_dataset) 949
after delete: len(noisy_dataset) 2187
epoch [60/200] batch [5/29] time 0.562 (0.478) data 0.431 (0.347) loss_x loss_x 1.8271 (1.4627) acc_x 62.5000 (63.7500) lr 1.6129e-03 eta 0:00:11
epoch [60/200] batch [10/29] time 0.383 (0.454) data 0.253 (0.323) loss_x loss_x 1.5781 (1.4014) acc_x 68.7500 (65.9375) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [15/29] time 0.349 (0.441) data 0.217 (0.310) loss_x loss_x 1.4453 (1.3842) acc_x 59.3750 (65.4167) lr 1.6129e-03 eta 0:00:06
epoch [60/200] batch [20/29] time 0.425 (0.453) data 0.295 (0.322) loss_x loss_x 1.1719 (1.3169) acc_x 81.2500 (67.5000) lr 1.6129e-03 eta 0:00:04
epoch [60/200] batch [25/29] time 0.443 (0.450) data 0.311 (0.319) loss_x loss_x 1.0947 (1.3180) acc_x 68.7500 (67.3750) lr 1.6129e-03 eta 0:00:01
epoch [60/200] batch [5/68] time 0.366 (0.462) data 0.234 (0.331) loss_u loss_u 0.9131 (0.8982) acc_u 9.3750 (14.3750) lr 1.6129e-03 eta 0:00:29
epoch [60/200] batch [10/68] time 0.422 (0.468) data 0.291 (0.337) loss_u loss_u 0.9155 (0.8936) acc_u 9.3750 (14.3750) lr 1.6129e-03 eta 0:00:27
epoch [60/200] batch [15/68] time 0.394 (0.460) data 0.262 (0.329) loss_u loss_u 0.8784 (0.8910) acc_u 18.7500 (14.5833) lr 1.6129e-03 eta 0:00:24
epoch [60/200] batch [20/68] time 0.510 (0.462) data 0.379 (0.330) loss_u loss_u 0.9634 (0.8995) acc_u 0.0000 (13.4375) lr 1.6129e-03 eta 0:00:22
epoch [60/200] batch [25/68] time 0.651 (0.471) data 0.520 (0.339) loss_u loss_u 0.8921 (0.9003) acc_u 12.5000 (12.7500) lr 1.6129e-03 eta 0:00:20
epoch [60/200] batch [30/68] time 0.442 (0.472) data 0.307 (0.341) loss_u loss_u 0.8438 (0.8984) acc_u 21.8750 (13.0208) lr 1.6129e-03 eta 0:00:17
epoch [60/200] batch [35/68] time 0.512 (0.468) data 0.380 (0.337) loss_u loss_u 0.8867 (0.8984) acc_u 15.6250 (13.3036) lr 1.6129e-03 eta 0:00:15
epoch [60/200] batch [40/68] time 0.410 (0.467) data 0.278 (0.335) loss_u loss_u 0.8696 (0.8965) acc_u 18.7500 (13.5156) lr 1.6129e-03 eta 0:00:13
epoch [60/200] batch [45/68] time 0.335 (0.466) data 0.203 (0.335) loss_u loss_u 0.8979 (0.8942) acc_u 12.5000 (13.6806) lr 1.6129e-03 eta 0:00:10
epoch [60/200] batch [50/68] time 0.474 (0.463) data 0.342 (0.331) loss_u loss_u 0.8960 (0.8934) acc_u 12.5000 (13.8750) lr 1.6129e-03 eta 0:00:08
epoch [60/200] batch [55/68] time 0.407 (0.461) data 0.276 (0.329) loss_u loss_u 0.8589 (0.8931) acc_u 15.6250 (13.5795) lr 1.6129e-03 eta 0:00:05
epoch [60/200] batch [60/68] time 0.531 (0.458) data 0.400 (0.327) loss_u loss_u 0.8711 (0.8931) acc_u 9.3750 (13.3854) lr 1.6129e-03 eta 0:00:03
epoch [60/200] batch [65/68] time 0.621 (0.460) data 0.490 (0.329) loss_u loss_u 0.8521 (0.8927) acc_u 21.8750 (13.5577) lr 1.6129e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1665
confident_label rate tensor(0.2966, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 930
clean true:820
clean false:110
clean_rate:0.8817204301075269
noisy true:651
noisy false:1555
after delete: len(clean_dataset) 930
after delete: len(noisy_dataset) 2206
epoch [61/200] batch [5/29] time 0.416 (0.468) data 0.286 (0.338) loss_x loss_x 1.1670 (1.3596) acc_x 78.1250 (69.3750) lr 1.6004e-03 eta 0:00:11
epoch [61/200] batch [10/29] time 0.409 (0.462) data 0.279 (0.331) loss_x loss_x 1.1387 (1.2844) acc_x 71.8750 (69.0625) lr 1.6004e-03 eta 0:00:08
epoch [61/200] batch [15/29] time 0.576 (0.467) data 0.445 (0.336) loss_x loss_x 1.0371 (1.2437) acc_x 78.1250 (69.7917) lr 1.6004e-03 eta 0:00:06
epoch [61/200] batch [20/29] time 0.479 (0.467) data 0.348 (0.336) loss_x loss_x 1.8066 (1.3836) acc_x 46.8750 (66.4062) lr 1.6004e-03 eta 0:00:04
epoch [61/200] batch [25/29] time 0.414 (0.458) data 0.284 (0.328) loss_x loss_x 1.4727 (1.3588) acc_x 65.6250 (66.2500) lr 1.6004e-03 eta 0:00:01
epoch [61/200] batch [5/68] time 0.422 (0.450) data 0.291 (0.319) loss_u loss_u 0.8403 (0.8548) acc_u 25.0000 (20.6250) lr 1.6004e-03 eta 0:00:28
epoch [61/200] batch [10/68] time 0.425 (0.446) data 0.294 (0.315) loss_u loss_u 0.8770 (0.8604) acc_u 18.7500 (20.3125) lr 1.6004e-03 eta 0:00:25
epoch [61/200] batch [15/68] time 0.420 (0.445) data 0.289 (0.314) loss_u loss_u 0.8613 (0.8642) acc_u 18.7500 (18.5417) lr 1.6004e-03 eta 0:00:23
epoch [61/200] batch [20/68] time 0.421 (0.443) data 0.289 (0.312) loss_u loss_u 0.8032 (0.8703) acc_u 15.6250 (16.8750) lr 1.6004e-03 eta 0:00:21
epoch [61/200] batch [25/68] time 0.373 (0.444) data 0.242 (0.313) loss_u loss_u 0.8931 (0.8750) acc_u 12.5000 (15.7500) lr 1.6004e-03 eta 0:00:19
epoch [61/200] batch [30/68] time 0.381 (0.442) data 0.249 (0.311) loss_u loss_u 0.8540 (0.8795) acc_u 18.7500 (15.4167) lr 1.6004e-03 eta 0:00:16
epoch [61/200] batch [35/68] time 0.556 (0.440) data 0.425 (0.309) loss_u loss_u 0.8633 (0.8807) acc_u 18.7500 (15.2679) lr 1.6004e-03 eta 0:00:14
epoch [61/200] batch [40/68] time 0.727 (0.441) data 0.595 (0.310) loss_u loss_u 0.8696 (0.8807) acc_u 15.6250 (15.2344) lr 1.6004e-03 eta 0:00:12
epoch [61/200] batch [45/68] time 0.419 (0.440) data 0.287 (0.309) loss_u loss_u 0.9009 (0.8816) acc_u 12.5000 (15.3472) lr 1.6004e-03 eta 0:00:10
epoch [61/200] batch [50/68] time 0.477 (0.442) data 0.346 (0.311) loss_u loss_u 0.8013 (0.8790) acc_u 31.2500 (15.6875) lr 1.6004e-03 eta 0:00:07
epoch [61/200] batch [55/68] time 0.461 (0.442) data 0.329 (0.311) loss_u loss_u 0.8574 (0.8777) acc_u 15.6250 (15.5682) lr 1.6004e-03 eta 0:00:05
epoch [61/200] batch [60/68] time 0.381 (0.441) data 0.249 (0.309) loss_u loss_u 0.8477 (0.8771) acc_u 18.7500 (15.6250) lr 1.6004e-03 eta 0:00:03
epoch [61/200] batch [65/68] time 0.466 (0.443) data 0.334 (0.311) loss_u loss_u 0.9043 (0.8789) acc_u 9.3750 (15.3846) lr 1.6004e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1645
confident_label rate tensor(0.2940, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 922
clean true:820
clean false:102
clean_rate:0.8893709327548807
noisy true:671
noisy false:1543
after delete: len(clean_dataset) 922
after delete: len(noisy_dataset) 2214
epoch [62/200] batch [5/28] time 0.493 (0.484) data 0.361 (0.353) loss_x loss_x 1.3389 (1.2765) acc_x 59.3750 (66.8750) lr 1.5878e-03 eta 0:00:11
epoch [62/200] batch [10/28] time 0.427 (0.458) data 0.296 (0.327) loss_x loss_x 0.6470 (1.1363) acc_x 90.6250 (69.6875) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [15/28] time 0.388 (0.450) data 0.257 (0.319) loss_x loss_x 1.2334 (1.2185) acc_x 65.6250 (67.5000) lr 1.5878e-03 eta 0:00:05
epoch [62/200] batch [20/28] time 0.442 (0.449) data 0.312 (0.318) loss_x loss_x 1.6729 (1.2311) acc_x 50.0000 (66.2500) lr 1.5878e-03 eta 0:00:03
epoch [62/200] batch [25/28] time 0.393 (0.451) data 0.262 (0.320) loss_x loss_x 1.7822 (1.2532) acc_x 59.3750 (65.6250) lr 1.5878e-03 eta 0:00:01
epoch [62/200] batch [5/69] time 0.432 (0.439) data 0.302 (0.308) loss_u loss_u 0.9106 (0.8889) acc_u 9.3750 (11.2500) lr 1.5878e-03 eta 0:00:28
epoch [62/200] batch [10/69] time 0.544 (0.447) data 0.412 (0.317) loss_u loss_u 0.8145 (0.8743) acc_u 21.8750 (14.0625) lr 1.5878e-03 eta 0:00:26
epoch [62/200] batch [15/69] time 0.363 (0.441) data 0.232 (0.311) loss_u loss_u 0.9155 (0.8818) acc_u 12.5000 (13.9583) lr 1.5878e-03 eta 0:00:23
epoch [62/200] batch [20/69] time 0.472 (0.441) data 0.341 (0.310) loss_u loss_u 0.9004 (0.8824) acc_u 9.3750 (14.2188) lr 1.5878e-03 eta 0:00:21
epoch [62/200] batch [25/69] time 0.392 (0.441) data 0.262 (0.311) loss_u loss_u 0.8555 (0.8786) acc_u 18.7500 (14.8750) lr 1.5878e-03 eta 0:00:19
epoch [62/200] batch [30/69] time 0.465 (0.445) data 0.334 (0.314) loss_u loss_u 0.9180 (0.8776) acc_u 9.3750 (15.2083) lr 1.5878e-03 eta 0:00:17
epoch [62/200] batch [35/69] time 0.490 (0.440) data 0.358 (0.309) loss_u loss_u 0.8960 (0.8775) acc_u 9.3750 (15.4464) lr 1.5878e-03 eta 0:00:14
epoch [62/200] batch [40/69] time 0.480 (0.440) data 0.348 (0.309) loss_u loss_u 0.8384 (0.8789) acc_u 18.7500 (15.3906) lr 1.5878e-03 eta 0:00:12
epoch [62/200] batch [45/69] time 0.561 (0.444) data 0.430 (0.313) loss_u loss_u 0.8633 (0.8797) acc_u 18.7500 (15.3472) lr 1.5878e-03 eta 0:00:10
epoch [62/200] batch [50/69] time 0.417 (0.443) data 0.285 (0.312) loss_u loss_u 0.8130 (0.8792) acc_u 18.7500 (15.3125) lr 1.5878e-03 eta 0:00:08
epoch [62/200] batch [55/69] time 0.418 (0.443) data 0.286 (0.312) loss_u loss_u 0.8774 (0.8807) acc_u 12.5000 (14.9432) lr 1.5878e-03 eta 0:00:06
epoch [62/200] batch [60/69] time 0.567 (0.445) data 0.434 (0.313) loss_u loss_u 0.8330 (0.8791) acc_u 21.8750 (15.3125) lr 1.5878e-03 eta 0:00:04
epoch [62/200] batch [65/69] time 0.402 (0.449) data 0.270 (0.317) loss_u loss_u 0.8086 (0.8776) acc_u 28.1250 (15.8173) lr 1.5878e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1631
confident_label rate tensor(0.3017, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 946
clean true:852
clean false:94
clean_rate:0.9006342494714588
noisy true:653
noisy false:1537
after delete: len(clean_dataset) 946
after delete: len(noisy_dataset) 2190
epoch [63/200] batch [5/29] time 0.523 (0.472) data 0.393 (0.342) loss_x loss_x 1.3779 (1.3119) acc_x 65.6250 (68.1250) lr 1.5750e-03 eta 0:00:11
epoch [63/200] batch [10/29] time 0.414 (0.464) data 0.283 (0.333) loss_x loss_x 1.1436 (1.2887) acc_x 65.6250 (70.6250) lr 1.5750e-03 eta 0:00:08
epoch [63/200] batch [15/29] time 0.591 (0.463) data 0.459 (0.332) loss_x loss_x 1.1436 (1.2357) acc_x 62.5000 (70.4167) lr 1.5750e-03 eta 0:00:06
epoch [63/200] batch [20/29] time 0.393 (0.463) data 0.261 (0.332) loss_x loss_x 1.1914 (1.2625) acc_x 65.6250 (68.7500) lr 1.5750e-03 eta 0:00:04
epoch [63/200] batch [25/29] time 0.549 (0.471) data 0.418 (0.340) loss_x loss_x 1.9365 (1.2903) acc_x 56.2500 (67.8750) lr 1.5750e-03 eta 0:00:01
epoch [63/200] batch [5/68] time 0.478 (0.462) data 0.347 (0.331) loss_u loss_u 0.7783 (0.8498) acc_u 21.8750 (18.1250) lr 1.5750e-03 eta 0:00:29
epoch [63/200] batch [10/68] time 0.415 (0.463) data 0.284 (0.333) loss_u loss_u 0.8950 (0.8792) acc_u 12.5000 (13.1250) lr 1.5750e-03 eta 0:00:26
epoch [63/200] batch [15/68] time 0.398 (0.461) data 0.266 (0.330) loss_u loss_u 0.8242 (0.8723) acc_u 18.7500 (15.2083) lr 1.5750e-03 eta 0:00:24
epoch [63/200] batch [20/68] time 0.432 (0.455) data 0.300 (0.325) loss_u loss_u 0.9028 (0.8722) acc_u 3.1250 (15.3125) lr 1.5750e-03 eta 0:00:21
epoch [63/200] batch [25/68] time 0.387 (0.459) data 0.256 (0.328) loss_u loss_u 0.9004 (0.8735) acc_u 12.5000 (15.6250) lr 1.5750e-03 eta 0:00:19
epoch [63/200] batch [30/68] time 0.399 (0.458) data 0.268 (0.327) loss_u loss_u 0.8325 (0.8730) acc_u 21.8750 (15.8333) lr 1.5750e-03 eta 0:00:17
epoch [63/200] batch [35/68] time 0.400 (0.457) data 0.269 (0.326) loss_u loss_u 0.9346 (0.8741) acc_u 6.2500 (15.4464) lr 1.5750e-03 eta 0:00:15
epoch [63/200] batch [40/68] time 0.368 (0.454) data 0.237 (0.323) loss_u loss_u 0.8936 (0.8719) acc_u 12.5000 (15.7812) lr 1.5750e-03 eta 0:00:12
epoch [63/200] batch [45/68] time 0.456 (0.454) data 0.325 (0.323) loss_u loss_u 0.9072 (0.8734) acc_u 12.5000 (15.8333) lr 1.5750e-03 eta 0:00:10
epoch [63/200] batch [50/68] time 0.393 (0.453) data 0.260 (0.322) loss_u loss_u 0.8838 (0.8760) acc_u 9.3750 (15.3750) lr 1.5750e-03 eta 0:00:08
epoch [63/200] batch [55/68] time 0.400 (0.451) data 0.267 (0.319) loss_u loss_u 0.9287 (0.8766) acc_u 6.2500 (15.1136) lr 1.5750e-03 eta 0:00:05
epoch [63/200] batch [60/68] time 0.475 (0.452) data 0.343 (0.321) loss_u loss_u 0.8228 (0.8750) acc_u 28.1250 (15.5208) lr 1.5750e-03 eta 0:00:03
epoch [63/200] batch [65/68] time 0.445 (0.457) data 0.314 (0.325) loss_u loss_u 0.8906 (0.8754) acc_u 12.5000 (15.4327) lr 1.5750e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1661
confident_label rate tensor(0.2937, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 921
clean true:816
clean false:105
clean_rate:0.8859934853420195
noisy true:659
noisy false:1556
after delete: len(clean_dataset) 921
after delete: len(noisy_dataset) 2215
epoch [64/200] batch [5/28] time 0.429 (0.401) data 0.298 (0.270) loss_x loss_x 1.0693 (1.4348) acc_x 65.6250 (64.3750) lr 1.5621e-03 eta 0:00:09
epoch [64/200] batch [10/28] time 0.448 (0.451) data 0.318 (0.320) loss_x loss_x 1.7900 (1.4234) acc_x 56.2500 (65.3125) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [15/28] time 0.412 (0.458) data 0.281 (0.327) loss_x loss_x 1.3867 (1.3798) acc_x 62.5000 (65.8333) lr 1.5621e-03 eta 0:00:05
epoch [64/200] batch [20/28] time 0.442 (0.457) data 0.311 (0.325) loss_x loss_x 1.1016 (1.3030) acc_x 78.1250 (67.3438) lr 1.5621e-03 eta 0:00:03
epoch [64/200] batch [25/28] time 0.520 (0.457) data 0.389 (0.325) loss_x loss_x 0.9580 (1.2825) acc_x 78.1250 (67.6250) lr 1.5621e-03 eta 0:00:01
epoch [64/200] batch [5/69] time 0.524 (0.457) data 0.392 (0.326) loss_u loss_u 0.8608 (0.8689) acc_u 12.5000 (15.0000) lr 1.5621e-03 eta 0:00:29
epoch [64/200] batch [10/69] time 0.355 (0.451) data 0.223 (0.319) loss_u loss_u 0.9272 (0.8773) acc_u 6.2500 (14.6875) lr 1.5621e-03 eta 0:00:26
epoch [64/200] batch [15/69] time 0.433 (0.451) data 0.301 (0.320) loss_u loss_u 0.9233 (0.8782) acc_u 9.3750 (13.7500) lr 1.5621e-03 eta 0:00:24
epoch [64/200] batch [20/69] time 0.420 (0.447) data 0.289 (0.315) loss_u loss_u 0.8867 (0.8817) acc_u 15.6250 (14.0625) lr 1.5621e-03 eta 0:00:21
epoch [64/200] batch [25/69] time 0.464 (0.449) data 0.331 (0.318) loss_u loss_u 0.8970 (0.8730) acc_u 12.5000 (15.1250) lr 1.5621e-03 eta 0:00:19
epoch [64/200] batch [30/69] time 0.348 (0.450) data 0.215 (0.319) loss_u loss_u 0.8394 (0.8728) acc_u 18.7500 (15.2083) lr 1.5621e-03 eta 0:00:17
epoch [64/200] batch [35/69] time 0.470 (0.450) data 0.339 (0.318) loss_u loss_u 0.9199 (0.8738) acc_u 12.5000 (15.4464) lr 1.5621e-03 eta 0:00:15
epoch [64/200] batch [40/69] time 0.815 (0.455) data 0.682 (0.324) loss_u loss_u 0.8091 (0.8743) acc_u 25.0000 (15.3906) lr 1.5621e-03 eta 0:00:13
epoch [64/200] batch [45/69] time 0.517 (0.457) data 0.384 (0.325) loss_u loss_u 0.9668 (0.8763) acc_u 3.1250 (15.3472) lr 1.5621e-03 eta 0:00:10
epoch [64/200] batch [50/69] time 0.541 (0.458) data 0.410 (0.326) loss_u loss_u 0.8335 (0.8746) acc_u 15.6250 (15.6250) lr 1.5621e-03 eta 0:00:08
epoch [64/200] batch [55/69] time 0.646 (0.457) data 0.516 (0.326) loss_u loss_u 0.8672 (0.8769) acc_u 15.6250 (15.4545) lr 1.5621e-03 eta 0:00:06
epoch [64/200] batch [60/69] time 0.496 (0.457) data 0.365 (0.326) loss_u loss_u 0.9062 (0.8779) acc_u 9.3750 (15.2604) lr 1.5621e-03 eta 0:00:04
epoch [64/200] batch [65/69] time 0.459 (0.456) data 0.329 (0.325) loss_u loss_u 0.8105 (0.8791) acc_u 28.1250 (15.2404) lr 1.5621e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1680
confident_label rate tensor(0.2966, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 930
clean true:822
clean false:108
clean_rate:0.8838709677419355
noisy true:634
noisy false:1572
after delete: len(clean_dataset) 930
after delete: len(noisy_dataset) 2206
epoch [65/200] batch [5/29] time 0.458 (0.558) data 0.327 (0.427) loss_x loss_x 1.6885 (1.2734) acc_x 59.3750 (69.3750) lr 1.5490e-03 eta 0:00:13
epoch [65/200] batch [10/29] time 0.438 (0.491) data 0.307 (0.360) loss_x loss_x 1.2539 (1.3186) acc_x 65.6250 (65.0000) lr 1.5490e-03 eta 0:00:09
epoch [65/200] batch [15/29] time 0.455 (0.471) data 0.324 (0.340) loss_x loss_x 1.5381 (1.3676) acc_x 68.7500 (65.4167) lr 1.5490e-03 eta 0:00:06
epoch [65/200] batch [20/29] time 0.384 (0.471) data 0.253 (0.340) loss_x loss_x 2.0605 (1.3951) acc_x 56.2500 (64.6875) lr 1.5490e-03 eta 0:00:04
epoch [65/200] batch [25/29] time 0.487 (0.478) data 0.356 (0.348) loss_x loss_x 1.4951 (1.3662) acc_x 62.5000 (64.5000) lr 1.5490e-03 eta 0:00:01
epoch [65/200] batch [5/68] time 0.424 (0.467) data 0.292 (0.336) loss_u loss_u 0.8911 (0.8849) acc_u 9.3750 (13.7500) lr 1.5490e-03 eta 0:00:29
epoch [65/200] batch [10/68] time 0.451 (0.463) data 0.320 (0.331) loss_u loss_u 0.8750 (0.8811) acc_u 15.6250 (14.3750) lr 1.5490e-03 eta 0:00:26
epoch [65/200] batch [15/68] time 0.464 (0.462) data 0.333 (0.331) loss_u loss_u 0.8594 (0.8806) acc_u 18.7500 (14.7917) lr 1.5490e-03 eta 0:00:24
epoch [65/200] batch [20/68] time 0.512 (0.462) data 0.380 (0.331) loss_u loss_u 0.8647 (0.8691) acc_u 18.7500 (17.0312) lr 1.5490e-03 eta 0:00:22
epoch [65/200] batch [25/68] time 0.387 (0.463) data 0.255 (0.331) loss_u loss_u 0.8687 (0.8726) acc_u 12.5000 (16.6250) lr 1.5490e-03 eta 0:00:19
epoch [65/200] batch [30/68] time 0.381 (0.458) data 0.248 (0.327) loss_u loss_u 0.8618 (0.8734) acc_u 18.7500 (16.6667) lr 1.5490e-03 eta 0:00:17
epoch [65/200] batch [35/68] time 0.435 (0.456) data 0.304 (0.325) loss_u loss_u 0.8564 (0.8732) acc_u 18.7500 (16.6071) lr 1.5490e-03 eta 0:00:15
epoch [65/200] batch [40/68] time 0.340 (0.452) data 0.209 (0.320) loss_u loss_u 0.8184 (0.8710) acc_u 21.8750 (16.7188) lr 1.5490e-03 eta 0:00:12
epoch [65/200] batch [45/68] time 0.530 (0.452) data 0.398 (0.320) loss_u loss_u 0.8867 (0.8728) acc_u 12.5000 (16.3889) lr 1.5490e-03 eta 0:00:10
epoch [65/200] batch [50/68] time 0.366 (0.453) data 0.235 (0.321) loss_u loss_u 0.8550 (0.8740) acc_u 18.7500 (16.1250) lr 1.5490e-03 eta 0:00:08
epoch [65/200] batch [55/68] time 0.420 (0.451) data 0.289 (0.320) loss_u loss_u 0.8701 (0.8742) acc_u 18.7500 (15.9091) lr 1.5490e-03 eta 0:00:05
epoch [65/200] batch [60/68] time 0.457 (0.450) data 0.326 (0.319) loss_u loss_u 0.8755 (0.8744) acc_u 18.7500 (15.8854) lr 1.5490e-03 eta 0:00:03
epoch [65/200] batch [65/68] time 0.374 (0.450) data 0.244 (0.319) loss_u loss_u 0.8760 (0.8749) acc_u 18.7500 (15.8654) lr 1.5490e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1656
confident_label rate tensor(0.2994, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 939
clean true:826
clean false:113
clean_rate:0.8796592119275826
noisy true:654
noisy false:1543
after delete: len(clean_dataset) 939
after delete: len(noisy_dataset) 2197
epoch [66/200] batch [5/29] time 0.653 (0.472) data 0.523 (0.341) loss_x loss_x 1.3613 (1.3182) acc_x 65.6250 (65.0000) lr 1.5358e-03 eta 0:00:11
epoch [66/200] batch [10/29] time 0.557 (0.518) data 0.426 (0.387) loss_x loss_x 1.1074 (1.2287) acc_x 65.6250 (67.8125) lr 1.5358e-03 eta 0:00:09
epoch [66/200] batch [15/29] time 0.471 (0.504) data 0.340 (0.373) loss_x loss_x 1.1631 (1.2350) acc_x 71.8750 (67.9167) lr 1.5358e-03 eta 0:00:07
epoch [66/200] batch [20/29] time 0.438 (0.483) data 0.307 (0.353) loss_x loss_x 1.4355 (1.2634) acc_x 65.6250 (67.5000) lr 1.5358e-03 eta 0:00:04
epoch [66/200] batch [25/29] time 0.583 (0.479) data 0.452 (0.348) loss_x loss_x 1.3486 (1.2834) acc_x 65.6250 (67.1250) lr 1.5358e-03 eta 0:00:01
epoch [66/200] batch [5/68] time 0.533 (0.471) data 0.401 (0.340) loss_u loss_u 0.9141 (0.8908) acc_u 12.5000 (15.0000) lr 1.5358e-03 eta 0:00:29
epoch [66/200] batch [10/68] time 0.418 (0.462) data 0.288 (0.331) loss_u loss_u 0.8594 (0.8878) acc_u 15.6250 (14.3750) lr 1.5358e-03 eta 0:00:26
epoch [66/200] batch [15/68] time 0.294 (0.457) data 0.164 (0.326) loss_u loss_u 0.9048 (0.8868) acc_u 12.5000 (15.0000) lr 1.5358e-03 eta 0:00:24
epoch [66/200] batch [20/68] time 0.414 (0.454) data 0.282 (0.323) loss_u loss_u 0.8711 (0.8846) acc_u 12.5000 (15.0000) lr 1.5358e-03 eta 0:00:21
epoch [66/200] batch [25/68] time 0.337 (0.455) data 0.206 (0.324) loss_u loss_u 0.8442 (0.8777) acc_u 15.6250 (16.0000) lr 1.5358e-03 eta 0:00:19
epoch [66/200] batch [30/68] time 0.594 (0.460) data 0.461 (0.329) loss_u loss_u 0.8828 (0.8773) acc_u 12.5000 (15.4167) lr 1.5358e-03 eta 0:00:17
epoch [66/200] batch [35/68] time 0.660 (0.462) data 0.528 (0.331) loss_u loss_u 0.8853 (0.8761) acc_u 15.6250 (15.6250) lr 1.5358e-03 eta 0:00:15
epoch [66/200] batch [40/68] time 0.505 (0.469) data 0.373 (0.338) loss_u loss_u 0.8784 (0.8767) acc_u 15.6250 (15.6250) lr 1.5358e-03 eta 0:00:13
epoch [66/200] batch [45/68] time 0.505 (0.466) data 0.373 (0.335) loss_u loss_u 0.8301 (0.8776) acc_u 12.5000 (15.4167) lr 1.5358e-03 eta 0:00:10
epoch [66/200] batch [50/68] time 0.381 (0.464) data 0.250 (0.333) loss_u loss_u 0.8228 (0.8780) acc_u 21.8750 (15.3750) lr 1.5358e-03 eta 0:00:08
epoch [66/200] batch [55/68] time 0.501 (0.464) data 0.370 (0.332) loss_u loss_u 0.8149 (0.8772) acc_u 25.0000 (15.5682) lr 1.5358e-03 eta 0:00:06
epoch [66/200] batch [60/68] time 0.450 (0.463) data 0.320 (0.332) loss_u loss_u 0.8760 (0.8782) acc_u 21.8750 (15.5729) lr 1.5358e-03 eta 0:00:03
epoch [66/200] batch [65/68] time 0.536 (0.467) data 0.404 (0.335) loss_u loss_u 0.8608 (0.8785) acc_u 15.6250 (15.6731) lr 1.5358e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1671
confident_label rate tensor(0.2985, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 936
clean true:815
clean false:121
clean_rate:0.8707264957264957
noisy true:650
noisy false:1550
after delete: len(clean_dataset) 936
after delete: len(noisy_dataset) 2200
epoch [67/200] batch [5/29] time 0.431 (0.463) data 0.300 (0.332) loss_x loss_x 1.4736 (1.4572) acc_x 65.6250 (65.0000) lr 1.5225e-03 eta 0:00:11
epoch [67/200] batch [10/29] time 0.372 (0.448) data 0.241 (0.317) loss_x loss_x 1.4736 (1.4055) acc_x 65.6250 (66.8750) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [15/29] time 0.577 (0.471) data 0.445 (0.339) loss_x loss_x 1.3896 (1.3677) acc_x 65.6250 (67.5000) lr 1.5225e-03 eta 0:00:06
epoch [67/200] batch [20/29] time 0.558 (0.481) data 0.426 (0.349) loss_x loss_x 1.5635 (1.4092) acc_x 68.7500 (66.0938) lr 1.5225e-03 eta 0:00:04
epoch [67/200] batch [25/29] time 0.381 (0.480) data 0.250 (0.349) loss_x loss_x 1.5986 (1.4295) acc_x 68.7500 (65.5000) lr 1.5225e-03 eta 0:00:01
epoch [67/200] batch [5/68] time 0.356 (0.466) data 0.224 (0.335) loss_u loss_u 0.9087 (0.8738) acc_u 9.3750 (15.6250) lr 1.5225e-03 eta 0:00:29
epoch [67/200] batch [10/68] time 0.417 (0.457) data 0.284 (0.325) loss_u loss_u 0.9141 (0.8795) acc_u 18.7500 (15.9375) lr 1.5225e-03 eta 0:00:26
epoch [67/200] batch [15/68] time 0.418 (0.457) data 0.286 (0.326) loss_u loss_u 0.8828 (0.8815) acc_u 12.5000 (15.0000) lr 1.5225e-03 eta 0:00:24
epoch [67/200] batch [20/68] time 0.342 (0.452) data 0.210 (0.321) loss_u loss_u 0.8584 (0.8830) acc_u 15.6250 (14.5312) lr 1.5225e-03 eta 0:00:21
epoch [67/200] batch [25/68] time 0.474 (0.451) data 0.342 (0.319) loss_u loss_u 0.8457 (0.8813) acc_u 25.0000 (15.0000) lr 1.5225e-03 eta 0:00:19
epoch [67/200] batch [30/68] time 0.528 (0.452) data 0.397 (0.321) loss_u loss_u 0.8989 (0.8869) acc_u 6.2500 (13.8542) lr 1.5225e-03 eta 0:00:17
epoch [67/200] batch [35/68] time 0.479 (0.454) data 0.347 (0.323) loss_u loss_u 0.8140 (0.8849) acc_u 28.1250 (13.9286) lr 1.5225e-03 eta 0:00:14
epoch [67/200] batch [40/68] time 0.373 (0.452) data 0.241 (0.321) loss_u loss_u 0.9434 (0.8844) acc_u 6.2500 (13.9844) lr 1.5225e-03 eta 0:00:12
epoch [67/200] batch [45/68] time 0.478 (0.451) data 0.347 (0.320) loss_u loss_u 0.8882 (0.8819) acc_u 12.5000 (14.1667) lr 1.5225e-03 eta 0:00:10
epoch [67/200] batch [50/68] time 0.472 (0.456) data 0.340 (0.325) loss_u loss_u 0.8989 (0.8842) acc_u 9.3750 (13.8125) lr 1.5225e-03 eta 0:00:08
epoch [67/200] batch [55/68] time 0.411 (0.454) data 0.280 (0.322) loss_u loss_u 0.9033 (0.8843) acc_u 12.5000 (13.8636) lr 1.5225e-03 eta 0:00:05
epoch [67/200] batch [60/68] time 0.612 (0.454) data 0.481 (0.323) loss_u loss_u 0.9097 (0.8851) acc_u 9.3750 (13.8021) lr 1.5225e-03 eta 0:00:03
epoch [67/200] batch [65/68] time 0.393 (0.456) data 0.262 (0.325) loss_u loss_u 0.8691 (0.8846) acc_u 15.6250 (13.9423) lr 1.5225e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1648
confident_label rate tensor(0.3074, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 964
clean true:842
clean false:122
clean_rate:0.8734439834024896
noisy true:646
noisy false:1526
after delete: len(clean_dataset) 964
after delete: len(noisy_dataset) 2172
epoch [68/200] batch [5/30] time 0.489 (0.473) data 0.358 (0.342) loss_x loss_x 1.3779 (1.0790) acc_x 62.5000 (72.5000) lr 1.5090e-03 eta 0:00:11
epoch [68/200] batch [10/30] time 0.463 (0.461) data 0.333 (0.330) loss_x loss_x 1.1904 (1.2012) acc_x 78.1250 (69.0625) lr 1.5090e-03 eta 0:00:09
epoch [68/200] batch [15/30] time 0.428 (0.455) data 0.298 (0.324) loss_x loss_x 1.1572 (1.2275) acc_x 71.8750 (68.5417) lr 1.5090e-03 eta 0:00:06
epoch [68/200] batch [20/30] time 0.605 (0.460) data 0.474 (0.330) loss_x loss_x 1.4277 (1.2159) acc_x 59.3750 (68.1250) lr 1.5090e-03 eta 0:00:04
epoch [68/200] batch [25/30] time 0.372 (0.468) data 0.241 (0.337) loss_x loss_x 1.6553 (1.2325) acc_x 53.1250 (67.7500) lr 1.5090e-03 eta 0:00:02
epoch [68/200] batch [30/30] time 0.565 (0.467) data 0.434 (0.336) loss_x loss_x 1.0449 (1.2362) acc_x 71.8750 (68.0208) lr 1.5090e-03 eta 0:00:00
epoch [68/200] batch [5/67] time 0.431 (0.469) data 0.300 (0.338) loss_u loss_u 0.8818 (0.8824) acc_u 12.5000 (13.1250) lr 1.5090e-03 eta 0:00:29
epoch [68/200] batch [10/67] time 0.365 (0.472) data 0.233 (0.341) loss_u loss_u 0.8818 (0.8747) acc_u 12.5000 (14.6875) lr 1.5090e-03 eta 0:00:26
epoch [68/200] batch [15/67] time 0.528 (0.475) data 0.397 (0.344) loss_u loss_u 0.8159 (0.8710) acc_u 28.1250 (15.4167) lr 1.5090e-03 eta 0:00:24
epoch [68/200] batch [20/67] time 0.461 (0.476) data 0.329 (0.345) loss_u loss_u 0.8525 (0.8686) acc_u 21.8750 (16.0938) lr 1.5090e-03 eta 0:00:22
epoch [68/200] batch [25/67] time 0.501 (0.474) data 0.369 (0.343) loss_u loss_u 0.8237 (0.8676) acc_u 21.8750 (16.3750) lr 1.5090e-03 eta 0:00:19
epoch [68/200] batch [30/67] time 0.490 (0.469) data 0.358 (0.337) loss_u loss_u 0.7515 (0.8664) acc_u 34.3750 (16.6667) lr 1.5090e-03 eta 0:00:17
epoch [68/200] batch [35/67] time 0.453 (0.468) data 0.323 (0.337) loss_u loss_u 0.8184 (0.8683) acc_u 21.8750 (16.4286) lr 1.5090e-03 eta 0:00:14
epoch [68/200] batch [40/67] time 0.429 (0.466) data 0.298 (0.335) loss_u loss_u 0.9189 (0.8727) acc_u 6.2500 (15.7031) lr 1.5090e-03 eta 0:00:12
epoch [68/200] batch [45/67] time 0.352 (0.464) data 0.220 (0.332) loss_u loss_u 0.8062 (0.8710) acc_u 18.7500 (15.9028) lr 1.5090e-03 eta 0:00:10
epoch [68/200] batch [50/67] time 0.422 (0.462) data 0.291 (0.330) loss_u loss_u 0.8560 (0.8737) acc_u 18.7500 (15.7500) lr 1.5090e-03 eta 0:00:07
epoch [68/200] batch [55/67] time 0.465 (0.459) data 0.335 (0.327) loss_u loss_u 0.8657 (0.8740) acc_u 15.6250 (15.7386) lr 1.5090e-03 eta 0:00:05
epoch [68/200] batch [60/67] time 0.462 (0.458) data 0.330 (0.327) loss_u loss_u 0.9014 (0.8746) acc_u 12.5000 (15.7812) lr 1.5090e-03 eta 0:00:03
epoch [68/200] batch [65/67] time 0.357 (0.456) data 0.225 (0.324) loss_u loss_u 0.9126 (0.8773) acc_u 12.5000 (15.4808) lr 1.5090e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1641
confident_label rate tensor(0.3202, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1004
clean true:876
clean false:128
clean_rate:0.8725099601593626
noisy true:619
noisy false:1513
after delete: len(clean_dataset) 1004
after delete: len(noisy_dataset) 2132
epoch [69/200] batch [5/31] time 0.522 (0.507) data 0.391 (0.375) loss_x loss_x 1.2646 (1.2160) acc_x 68.7500 (68.7500) lr 1.4955e-03 eta 0:00:13
epoch [69/200] batch [10/31] time 0.484 (0.506) data 0.354 (0.375) loss_x loss_x 1.0312 (1.2092) acc_x 68.7500 (69.6875) lr 1.4955e-03 eta 0:00:10
epoch [69/200] batch [15/31] time 0.447 (0.505) data 0.317 (0.374) loss_x loss_x 1.0713 (1.2145) acc_x 65.6250 (68.1250) lr 1.4955e-03 eta 0:00:08
epoch [69/200] batch [20/31] time 0.483 (0.486) data 0.352 (0.355) loss_x loss_x 1.3604 (1.2598) acc_x 56.2500 (66.5625) lr 1.4955e-03 eta 0:00:05
epoch [69/200] batch [25/31] time 0.428 (0.477) data 0.297 (0.346) loss_x loss_x 1.6377 (1.3013) acc_x 59.3750 (65.6250) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [30/31] time 0.377 (0.475) data 0.246 (0.344) loss_x loss_x 1.1064 (1.3054) acc_x 65.6250 (65.2083) lr 1.4955e-03 eta 0:00:00
epoch [69/200] batch [5/66] time 0.392 (0.465) data 0.261 (0.334) loss_u loss_u 0.8594 (0.8754) acc_u 21.8750 (14.3750) lr 1.4955e-03 eta 0:00:28
epoch [69/200] batch [10/66] time 0.515 (0.470) data 0.384 (0.339) loss_u loss_u 0.8843 (0.8807) acc_u 15.6250 (15.3125) lr 1.4955e-03 eta 0:00:26
epoch [69/200] batch [15/66] time 0.400 (0.470) data 0.269 (0.339) loss_u loss_u 0.9575 (0.8910) acc_u 9.3750 (14.3750) lr 1.4955e-03 eta 0:00:23
epoch [69/200] batch [20/66] time 0.418 (0.467) data 0.286 (0.336) loss_u loss_u 0.8765 (0.8812) acc_u 9.3750 (14.6875) lr 1.4955e-03 eta 0:00:21
epoch [69/200] batch [25/66] time 0.396 (0.461) data 0.265 (0.330) loss_u loss_u 0.9019 (0.8873) acc_u 9.3750 (13.7500) lr 1.4955e-03 eta 0:00:18
epoch [69/200] batch [30/66] time 0.342 (0.459) data 0.210 (0.328) loss_u loss_u 0.8896 (0.8792) acc_u 9.3750 (14.7917) lr 1.4955e-03 eta 0:00:16
epoch [69/200] batch [35/66] time 0.517 (0.459) data 0.385 (0.328) loss_u loss_u 0.8550 (0.8786) acc_u 25.0000 (15.2679) lr 1.4955e-03 eta 0:00:14
epoch [69/200] batch [40/66] time 0.438 (0.456) data 0.308 (0.325) loss_u loss_u 0.9243 (0.8804) acc_u 9.3750 (15.0781) lr 1.4955e-03 eta 0:00:11
epoch [69/200] batch [45/66] time 0.349 (0.454) data 0.218 (0.323) loss_u loss_u 0.7583 (0.8779) acc_u 31.2500 (15.3472) lr 1.4955e-03 eta 0:00:09
epoch [69/200] batch [50/66] time 0.495 (0.453) data 0.363 (0.321) loss_u loss_u 0.9370 (0.8789) acc_u 6.2500 (15.2500) lr 1.4955e-03 eta 0:00:07
epoch [69/200] batch [55/66] time 0.468 (0.453) data 0.336 (0.322) loss_u loss_u 0.8950 (0.8809) acc_u 15.6250 (15.1136) lr 1.4955e-03 eta 0:00:04
epoch [69/200] batch [60/66] time 0.418 (0.452) data 0.286 (0.321) loss_u loss_u 0.7544 (0.8796) acc_u 25.0000 (15.2083) lr 1.4955e-03 eta 0:00:02
epoch [69/200] batch [65/66] time 0.382 (0.450) data 0.251 (0.319) loss_u loss_u 0.8833 (0.8809) acc_u 18.7500 (15.2404) lr 1.4955e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1602
confident_label rate tensor(0.3080, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 966
clean true:856
clean false:110
clean_rate:0.8861283643892339
noisy true:678
noisy false:1492
after delete: len(clean_dataset) 966
after delete: len(noisy_dataset) 2170
epoch [70/200] batch [5/30] time 0.479 (0.439) data 0.348 (0.308) loss_x loss_x 0.9565 (1.0029) acc_x 68.7500 (68.7500) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [10/30] time 0.590 (0.461) data 0.459 (0.331) loss_x loss_x 2.4160 (1.3521) acc_x 50.0000 (63.1250) lr 1.4818e-03 eta 0:00:09
epoch [70/200] batch [15/30] time 0.518 (0.482) data 0.387 (0.352) loss_x loss_x 1.1758 (1.3758) acc_x 68.7500 (63.3333) lr 1.4818e-03 eta 0:00:07
epoch [70/200] batch [20/30] time 0.506 (0.481) data 0.375 (0.351) loss_x loss_x 1.1191 (1.3424) acc_x 71.8750 (65.0000) lr 1.4818e-03 eta 0:00:04
epoch [70/200] batch [25/30] time 0.561 (0.491) data 0.431 (0.360) loss_x loss_x 1.3213 (1.3187) acc_x 68.7500 (66.6250) lr 1.4818e-03 eta 0:00:02
epoch [70/200] batch [30/30] time 0.395 (0.482) data 0.264 (0.351) loss_x loss_x 0.7568 (1.3181) acc_x 78.1250 (66.6667) lr 1.4818e-03 eta 0:00:00
epoch [70/200] batch [5/67] time 0.527 (0.485) data 0.397 (0.354) loss_u loss_u 0.7788 (0.8902) acc_u 21.8750 (14.3750) lr 1.4818e-03 eta 0:00:30
epoch [70/200] batch [10/67] time 0.409 (0.478) data 0.278 (0.347) loss_u loss_u 0.9238 (0.8965) acc_u 9.3750 (14.0625) lr 1.4818e-03 eta 0:00:27
epoch [70/200] batch [15/67] time 0.456 (0.480) data 0.325 (0.349) loss_u loss_u 0.8882 (0.8930) acc_u 12.5000 (13.7500) lr 1.4818e-03 eta 0:00:24
epoch [70/200] batch [20/67] time 0.433 (0.479) data 0.301 (0.348) loss_u loss_u 0.8716 (0.8849) acc_u 15.6250 (14.0625) lr 1.4818e-03 eta 0:00:22
epoch [70/200] batch [25/67] time 0.444 (0.476) data 0.313 (0.345) loss_u loss_u 0.8828 (0.8809) acc_u 12.5000 (14.3750) lr 1.4818e-03 eta 0:00:19
epoch [70/200] batch [30/67] time 0.443 (0.470) data 0.312 (0.339) loss_u loss_u 0.8394 (0.8793) acc_u 18.7500 (14.7917) lr 1.4818e-03 eta 0:00:17
epoch [70/200] batch [35/67] time 0.469 (0.474) data 0.339 (0.343) loss_u loss_u 0.9224 (0.8839) acc_u 6.2500 (14.1071) lr 1.4818e-03 eta 0:00:15
epoch [70/200] batch [40/67] time 0.457 (0.474) data 0.328 (0.343) loss_u loss_u 0.8574 (0.8832) acc_u 18.7500 (14.2969) lr 1.4818e-03 eta 0:00:12
epoch [70/200] batch [45/67] time 0.426 (0.474) data 0.294 (0.343) loss_u loss_u 0.8857 (0.8810) acc_u 18.7500 (14.5139) lr 1.4818e-03 eta 0:00:10
epoch [70/200] batch [50/67] time 0.472 (0.472) data 0.342 (0.341) loss_u loss_u 0.9507 (0.8835) acc_u 6.2500 (14.0625) lr 1.4818e-03 eta 0:00:08
epoch [70/200] batch [55/67] time 0.560 (0.474) data 0.428 (0.343) loss_u loss_u 0.8813 (0.8853) acc_u 12.5000 (13.7500) lr 1.4818e-03 eta 0:00:05
epoch [70/200] batch [60/67] time 0.498 (0.473) data 0.368 (0.342) loss_u loss_u 0.8843 (0.8849) acc_u 12.5000 (13.5938) lr 1.4818e-03 eta 0:00:03
epoch [70/200] batch [65/67] time 0.422 (0.471) data 0.291 (0.340) loss_u loss_u 0.8306 (0.8849) acc_u 21.8750 (13.6538) lr 1.4818e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1652
confident_label rate tensor(0.2985, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 936
clean true:840
clean false:96
clean_rate:0.8974358974358975
noisy true:644
noisy false:1556
after delete: len(clean_dataset) 936
after delete: len(noisy_dataset) 2200
epoch [71/200] batch [5/29] time 0.531 (0.534) data 0.401 (0.404) loss_x loss_x 1.6279 (1.2434) acc_x 65.6250 (72.5000) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [10/29] time 0.392 (0.537) data 0.262 (0.406) loss_x loss_x 1.3613 (1.3134) acc_x 68.7500 (69.3750) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [15/29] time 0.543 (0.512) data 0.412 (0.381) loss_x loss_x 1.5508 (1.3652) acc_x 53.1250 (67.0833) lr 1.4679e-03 eta 0:00:07
epoch [71/200] batch [20/29] time 0.367 (0.501) data 0.237 (0.370) loss_x loss_x 1.0391 (1.3243) acc_x 65.6250 (68.2812) lr 1.4679e-03 eta 0:00:04
epoch [71/200] batch [25/29] time 0.359 (0.481) data 0.230 (0.350) loss_x loss_x 1.0020 (1.2897) acc_x 68.7500 (68.1250) lr 1.4679e-03 eta 0:00:01
epoch [71/200] batch [5/68] time 0.470 (0.478) data 0.339 (0.347) loss_u loss_u 0.8784 (0.8842) acc_u 12.5000 (16.2500) lr 1.4679e-03 eta 0:00:30
epoch [71/200] batch [10/68] time 0.498 (0.478) data 0.365 (0.347) loss_u loss_u 0.9067 (0.8753) acc_u 12.5000 (16.5625) lr 1.4679e-03 eta 0:00:27
epoch [71/200] batch [15/68] time 0.440 (0.473) data 0.308 (0.342) loss_u loss_u 0.9155 (0.8800) acc_u 9.3750 (15.0000) lr 1.4679e-03 eta 0:00:25
epoch [71/200] batch [20/68] time 0.482 (0.470) data 0.350 (0.340) loss_u loss_u 0.8857 (0.8833) acc_u 12.5000 (14.3750) lr 1.4679e-03 eta 0:00:22
epoch [71/200] batch [25/68] time 0.437 (0.463) data 0.305 (0.332) loss_u loss_u 0.9512 (0.8824) acc_u 6.2500 (14.2500) lr 1.4679e-03 eta 0:00:19
epoch [71/200] batch [30/68] time 0.529 (0.468) data 0.397 (0.337) loss_u loss_u 0.8750 (0.8855) acc_u 12.5000 (13.6458) lr 1.4679e-03 eta 0:00:17
epoch [71/200] batch [35/68] time 0.366 (0.463) data 0.235 (0.332) loss_u loss_u 0.8682 (0.8863) acc_u 15.6250 (13.6607) lr 1.4679e-03 eta 0:00:15
epoch [71/200] batch [40/68] time 0.493 (0.463) data 0.363 (0.332) loss_u loss_u 0.8662 (0.8824) acc_u 15.6250 (14.2188) lr 1.4679e-03 eta 0:00:12
epoch [71/200] batch [45/68] time 0.428 (0.463) data 0.297 (0.332) loss_u loss_u 0.9517 (0.8831) acc_u 6.2500 (14.2361) lr 1.4679e-03 eta 0:00:10
epoch [71/200] batch [50/68] time 0.481 (0.462) data 0.350 (0.331) loss_u loss_u 0.8735 (0.8808) acc_u 18.7500 (14.6875) lr 1.4679e-03 eta 0:00:08
epoch [71/200] batch [55/68] time 0.435 (0.461) data 0.303 (0.330) loss_u loss_u 0.8735 (0.8816) acc_u 15.6250 (14.4886) lr 1.4679e-03 eta 0:00:05
epoch [71/200] batch [60/68] time 0.433 (0.460) data 0.302 (0.329) loss_u loss_u 0.9307 (0.8816) acc_u 9.3750 (14.8438) lr 1.4679e-03 eta 0:00:03
epoch [71/200] batch [65/68] time 0.394 (0.457) data 0.263 (0.326) loss_u loss_u 0.8882 (0.8805) acc_u 12.5000 (15.0000) lr 1.4679e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1691
confident_label rate tensor(0.2997, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 940
clean true:818
clean false:122
clean_rate:0.8702127659574468
noisy true:627
noisy false:1569
after delete: len(clean_dataset) 940
after delete: len(noisy_dataset) 2196
epoch [72/200] batch [5/29] time 0.591 (0.473) data 0.460 (0.342) loss_x loss_x 1.0596 (1.2616) acc_x 78.1250 (66.8750) lr 1.4540e-03 eta 0:00:11
epoch [72/200] batch [10/29] time 0.567 (0.489) data 0.436 (0.358) loss_x loss_x 1.0605 (1.3193) acc_x 75.0000 (65.9375) lr 1.4540e-03 eta 0:00:09
epoch [72/200] batch [15/29] time 0.408 (0.475) data 0.278 (0.344) loss_x loss_x 1.1387 (1.3128) acc_x 68.7500 (66.0417) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [20/29] time 0.446 (0.478) data 0.316 (0.347) loss_x loss_x 1.2607 (1.3199) acc_x 71.8750 (65.9375) lr 1.4540e-03 eta 0:00:04
epoch [72/200] batch [25/29] time 0.389 (0.467) data 0.259 (0.337) loss_x loss_x 1.3994 (1.4002) acc_x 62.5000 (64.2500) lr 1.4540e-03 eta 0:00:01
epoch [72/200] batch [5/68] time 0.370 (0.459) data 0.240 (0.328) loss_u loss_u 0.8413 (0.8727) acc_u 18.7500 (16.2500) lr 1.4540e-03 eta 0:00:28
epoch [72/200] batch [10/68] time 0.421 (0.464) data 0.290 (0.333) loss_u loss_u 0.8716 (0.8831) acc_u 12.5000 (14.3750) lr 1.4540e-03 eta 0:00:26
epoch [72/200] batch [15/68] time 0.543 (0.461) data 0.413 (0.330) loss_u loss_u 0.8672 (0.8788) acc_u 9.3750 (14.1667) lr 1.4540e-03 eta 0:00:24
epoch [72/200] batch [20/68] time 0.432 (0.457) data 0.302 (0.327) loss_u loss_u 0.9497 (0.8817) acc_u 9.3750 (14.2188) lr 1.4540e-03 eta 0:00:21
epoch [72/200] batch [25/68] time 0.462 (0.462) data 0.331 (0.331) loss_u loss_u 0.8384 (0.8791) acc_u 15.6250 (14.3750) lr 1.4540e-03 eta 0:00:19
epoch [72/200] batch [30/68] time 0.361 (0.460) data 0.229 (0.329) loss_u loss_u 0.8901 (0.8780) acc_u 12.5000 (14.8958) lr 1.4540e-03 eta 0:00:17
epoch [72/200] batch [35/68] time 0.541 (0.467) data 0.407 (0.336) loss_u loss_u 0.9111 (0.8760) acc_u 12.5000 (15.4464) lr 1.4540e-03 eta 0:00:15
epoch [72/200] batch [40/68] time 0.397 (0.468) data 0.266 (0.337) loss_u loss_u 0.8667 (0.8734) acc_u 18.7500 (15.8594) lr 1.4540e-03 eta 0:00:13
epoch [72/200] batch [45/68] time 0.537 (0.468) data 0.405 (0.337) loss_u loss_u 0.8428 (0.8728) acc_u 18.7500 (15.7639) lr 1.4540e-03 eta 0:00:10
epoch [72/200] batch [50/68] time 0.390 (0.466) data 0.260 (0.335) loss_u loss_u 0.8267 (0.8721) acc_u 21.8750 (15.7500) lr 1.4540e-03 eta 0:00:08
epoch [72/200] batch [55/68] time 0.429 (0.467) data 0.297 (0.336) loss_u loss_u 0.8872 (0.8734) acc_u 15.6250 (15.3977) lr 1.4540e-03 eta 0:00:06
epoch [72/200] batch [60/68] time 0.401 (0.464) data 0.269 (0.333) loss_u loss_u 0.8652 (0.8736) acc_u 18.7500 (15.4688) lr 1.4540e-03 eta 0:00:03
epoch [72/200] batch [65/68] time 0.378 (0.463) data 0.247 (0.332) loss_u loss_u 0.9160 (0.8747) acc_u 9.3750 (15.3846) lr 1.4540e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1644
confident_label rate tensor(0.2937, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 921
clean true:822
clean false:99
clean_rate:0.8925081433224755
noisy true:670
noisy false:1545
after delete: len(clean_dataset) 921
after delete: len(noisy_dataset) 2215
epoch [73/200] batch [5/28] time 0.420 (0.458) data 0.290 (0.328) loss_x loss_x 1.4092 (1.4053) acc_x 75.0000 (66.2500) lr 1.4399e-03 eta 0:00:10
epoch [73/200] batch [10/28] time 0.592 (0.466) data 0.461 (0.335) loss_x loss_x 1.0361 (1.3312) acc_x 71.8750 (64.0625) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [15/28] time 0.529 (0.476) data 0.398 (0.345) loss_x loss_x 0.7324 (1.3607) acc_x 75.0000 (65.2083) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [20/28] time 0.493 (0.478) data 0.361 (0.347) loss_x loss_x 2.0195 (1.3805) acc_x 46.8750 (64.8438) lr 1.4399e-03 eta 0:00:03
epoch [73/200] batch [25/28] time 0.506 (0.474) data 0.376 (0.343) loss_x loss_x 1.4395 (1.3856) acc_x 71.8750 (65.3750) lr 1.4399e-03 eta 0:00:01
epoch [73/200] batch [5/69] time 0.471 (0.475) data 0.340 (0.344) loss_u loss_u 0.8682 (0.8700) acc_u 15.6250 (16.8750) lr 1.4399e-03 eta 0:00:30
epoch [73/200] batch [10/69] time 0.366 (0.463) data 0.235 (0.332) loss_u loss_u 0.8975 (0.8815) acc_u 12.5000 (15.9375) lr 1.4399e-03 eta 0:00:27
epoch [73/200] batch [15/69] time 0.473 (0.463) data 0.343 (0.332) loss_u loss_u 0.8955 (0.8964) acc_u 6.2500 (13.3333) lr 1.4399e-03 eta 0:00:24
epoch [73/200] batch [20/69] time 0.486 (0.458) data 0.354 (0.327) loss_u loss_u 0.8970 (0.8960) acc_u 9.3750 (12.9688) lr 1.4399e-03 eta 0:00:22
epoch [73/200] batch [25/69] time 0.565 (0.463) data 0.432 (0.332) loss_u loss_u 0.8799 (0.8948) acc_u 12.5000 (13.0000) lr 1.4399e-03 eta 0:00:20
epoch [73/200] batch [30/69] time 0.482 (0.462) data 0.350 (0.331) loss_u loss_u 0.8940 (0.8897) acc_u 9.3750 (13.3333) lr 1.4399e-03 eta 0:00:18
epoch [73/200] batch [35/69] time 0.422 (0.460) data 0.290 (0.329) loss_u loss_u 0.8169 (0.8865) acc_u 21.8750 (13.6607) lr 1.4399e-03 eta 0:00:15
epoch [73/200] batch [40/69] time 0.437 (0.459) data 0.304 (0.328) loss_u loss_u 0.8926 (0.8841) acc_u 15.6250 (14.2188) lr 1.4399e-03 eta 0:00:13
epoch [73/200] batch [45/69] time 0.580 (0.463) data 0.451 (0.332) loss_u loss_u 0.8887 (0.8836) acc_u 12.5000 (14.3056) lr 1.4399e-03 eta 0:00:11
epoch [73/200] batch [50/69] time 0.606 (0.463) data 0.476 (0.332) loss_u loss_u 0.8916 (0.8832) acc_u 15.6250 (14.2500) lr 1.4399e-03 eta 0:00:08
epoch [73/200] batch [55/69] time 0.524 (0.460) data 0.394 (0.330) loss_u loss_u 0.9014 (0.8826) acc_u 9.3750 (14.1477) lr 1.4399e-03 eta 0:00:06
epoch [73/200] batch [60/69] time 0.392 (0.461) data 0.262 (0.330) loss_u loss_u 0.9180 (0.8821) acc_u 6.2500 (14.0625) lr 1.4399e-03 eta 0:00:04
epoch [73/200] batch [65/69] time 0.394 (0.461) data 0.263 (0.330) loss_u loss_u 0.8716 (0.8806) acc_u 12.5000 (14.5673) lr 1.4399e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1612
confident_label rate tensor(0.3052, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 957
clean true:854
clean false:103
clean_rate:0.8923719958202717
noisy true:670
noisy false:1509
after delete: len(clean_dataset) 957
after delete: len(noisy_dataset) 2179
epoch [74/200] batch [5/29] time 0.615 (0.544) data 0.484 (0.413) loss_x loss_x 1.6807 (1.2411) acc_x 56.2500 (67.5000) lr 1.4258e-03 eta 0:00:13
epoch [74/200] batch [10/29] time 0.443 (0.501) data 0.312 (0.370) loss_x loss_x 1.6553 (1.4452) acc_x 65.6250 (63.4375) lr 1.4258e-03 eta 0:00:09
epoch [74/200] batch [15/29] time 0.444 (0.485) data 0.314 (0.354) loss_x loss_x 1.4688 (1.3968) acc_x 65.6250 (65.4167) lr 1.4258e-03 eta 0:00:06
epoch [74/200] batch [20/29] time 0.436 (0.471) data 0.305 (0.340) loss_x loss_x 1.8574 (1.4232) acc_x 59.3750 (64.6875) lr 1.4258e-03 eta 0:00:04
epoch [74/200] batch [25/29] time 0.564 (0.473) data 0.434 (0.342) loss_x loss_x 1.3418 (1.4124) acc_x 68.7500 (64.8750) lr 1.4258e-03 eta 0:00:01
epoch [74/200] batch [5/68] time 0.630 (0.470) data 0.500 (0.340) loss_u loss_u 0.8999 (0.8689) acc_u 15.6250 (18.1250) lr 1.4258e-03 eta 0:00:29
epoch [74/200] batch [10/68] time 0.495 (0.473) data 0.363 (0.342) loss_u loss_u 0.8921 (0.8858) acc_u 9.3750 (14.0625) lr 1.4258e-03 eta 0:00:27
epoch [74/200] batch [15/68] time 0.390 (0.470) data 0.259 (0.339) loss_u loss_u 0.9209 (0.8809) acc_u 12.5000 (15.2083) lr 1.4258e-03 eta 0:00:24
epoch [74/200] batch [20/68] time 0.457 (0.473) data 0.324 (0.342) loss_u loss_u 0.9121 (0.8856) acc_u 6.2500 (14.3750) lr 1.4258e-03 eta 0:00:22
epoch [74/200] batch [25/68] time 0.404 (0.465) data 0.272 (0.334) loss_u loss_u 0.8062 (0.8829) acc_u 25.0000 (14.3750) lr 1.4258e-03 eta 0:00:19
epoch [74/200] batch [30/68] time 0.438 (0.464) data 0.307 (0.333) loss_u loss_u 0.8647 (0.8811) acc_u 18.7500 (14.5833) lr 1.4258e-03 eta 0:00:17
epoch [74/200] batch [35/68] time 0.439 (0.461) data 0.309 (0.330) loss_u loss_u 0.8877 (0.8826) acc_u 12.5000 (14.4643) lr 1.4258e-03 eta 0:00:15
epoch [74/200] batch [40/68] time 0.422 (0.455) data 0.292 (0.324) loss_u loss_u 0.8726 (0.8837) acc_u 18.7500 (14.5312) lr 1.4258e-03 eta 0:00:12
epoch [74/200] batch [45/68] time 0.516 (0.457) data 0.386 (0.326) loss_u loss_u 0.9233 (0.8828) acc_u 9.3750 (14.6528) lr 1.4258e-03 eta 0:00:10
epoch [74/200] batch [50/68] time 0.466 (0.456) data 0.335 (0.325) loss_u loss_u 0.8618 (0.8791) acc_u 12.5000 (14.9375) lr 1.4258e-03 eta 0:00:08
epoch [74/200] batch [55/68] time 0.452 (0.456) data 0.320 (0.325) loss_u loss_u 0.8062 (0.8756) acc_u 28.1250 (15.4545) lr 1.4258e-03 eta 0:00:05
epoch [74/200] batch [60/68] time 0.506 (0.453) data 0.374 (0.322) loss_u loss_u 0.9316 (0.8748) acc_u 3.1250 (15.5729) lr 1.4258e-03 eta 0:00:03
epoch [74/200] batch [65/68] time 0.362 (0.451) data 0.232 (0.320) loss_u loss_u 0.8901 (0.8749) acc_u 9.3750 (15.6250) lr 1.4258e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1627
confident_label rate tensor(0.3125, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 980
clean true:863
clean false:117
clean_rate:0.8806122448979592
noisy true:646
noisy false:1510
after delete: len(clean_dataset) 980
after delete: len(noisy_dataset) 2156
epoch [75/200] batch [5/30] time 0.382 (0.416) data 0.252 (0.285) loss_x loss_x 1.2998 (1.3096) acc_x 71.8750 (66.8750) lr 1.4115e-03 eta 0:00:10
epoch [75/200] batch [10/30] time 0.585 (0.459) data 0.454 (0.328) loss_x loss_x 1.0312 (1.3682) acc_x 75.0000 (65.9375) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [15/30] time 0.440 (0.448) data 0.310 (0.317) loss_x loss_x 1.3955 (1.3650) acc_x 62.5000 (65.4167) lr 1.4115e-03 eta 0:00:06
epoch [75/200] batch [20/30] time 0.415 (0.445) data 0.284 (0.314) loss_x loss_x 1.0020 (1.3612) acc_x 78.1250 (65.1562) lr 1.4115e-03 eta 0:00:04
epoch [75/200] batch [25/30] time 0.441 (0.440) data 0.310 (0.309) loss_x loss_x 1.8174 (1.3789) acc_x 53.1250 (63.7500) lr 1.4115e-03 eta 0:00:02
epoch [75/200] batch [30/30] time 0.550 (0.446) data 0.419 (0.315) loss_x loss_x 1.7666 (1.3495) acc_x 62.5000 (64.4792) lr 1.4115e-03 eta 0:00:00
epoch [75/200] batch [5/67] time 0.434 (0.448) data 0.303 (0.317) loss_u loss_u 0.9380 (0.8822) acc_u 9.3750 (15.6250) lr 1.4115e-03 eta 0:00:27
epoch [75/200] batch [10/67] time 0.347 (0.438) data 0.216 (0.308) loss_u loss_u 0.8682 (0.8854) acc_u 15.6250 (15.3125) lr 1.4115e-03 eta 0:00:24
epoch [75/200] batch [15/67] time 0.354 (0.444) data 0.222 (0.313) loss_u loss_u 0.8809 (0.8866) acc_u 12.5000 (15.2083) lr 1.4115e-03 eta 0:00:23
epoch [75/200] batch [20/67] time 0.437 (0.441) data 0.306 (0.310) loss_u loss_u 0.8608 (0.8905) acc_u 18.7500 (14.5312) lr 1.4115e-03 eta 0:00:20
epoch [75/200] batch [25/67] time 0.383 (0.446) data 0.251 (0.316) loss_u loss_u 0.8569 (0.8886) acc_u 25.0000 (14.6250) lr 1.4115e-03 eta 0:00:18
epoch [75/200] batch [30/67] time 0.370 (0.447) data 0.238 (0.316) loss_u loss_u 0.8438 (0.8850) acc_u 18.7500 (14.7917) lr 1.4115e-03 eta 0:00:16
epoch [75/200] batch [35/67] time 0.363 (0.447) data 0.231 (0.316) loss_u loss_u 0.7837 (0.8789) acc_u 21.8750 (15.6250) lr 1.4115e-03 eta 0:00:14
epoch [75/200] batch [40/67] time 0.405 (0.447) data 0.274 (0.316) loss_u loss_u 0.8638 (0.8792) acc_u 15.6250 (15.4688) lr 1.4115e-03 eta 0:00:12
epoch [75/200] batch [45/67] time 0.636 (0.447) data 0.505 (0.316) loss_u loss_u 0.8955 (0.8793) acc_u 15.6250 (15.6250) lr 1.4115e-03 eta 0:00:09
epoch [75/200] batch [50/67] time 0.591 (0.451) data 0.460 (0.320) loss_u loss_u 0.8672 (0.8776) acc_u 12.5000 (15.6875) lr 1.4115e-03 eta 0:00:07
epoch [75/200] batch [55/67] time 0.464 (0.450) data 0.333 (0.319) loss_u loss_u 0.8633 (0.8789) acc_u 15.6250 (15.3977) lr 1.4115e-03 eta 0:00:05
epoch [75/200] batch [60/67] time 0.508 (0.450) data 0.378 (0.319) loss_u loss_u 0.8594 (0.8771) acc_u 21.8750 (15.6771) lr 1.4115e-03 eta 0:00:03
epoch [75/200] batch [65/67] time 0.410 (0.448) data 0.279 (0.317) loss_u loss_u 0.9048 (0.8774) acc_u 12.5000 (15.5288) lr 1.4115e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1596
confident_label rate tensor(0.3103, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 973
clean true:857
clean false:116
clean_rate:0.880781089414183
noisy true:683
noisy false:1480
after delete: len(clean_dataset) 973
after delete: len(noisy_dataset) 2163
epoch [76/200] batch [5/30] time 0.505 (0.453) data 0.373 (0.322) loss_x loss_x 1.2197 (1.2152) acc_x 68.7500 (69.3750) lr 1.3971e-03 eta 0:00:11
epoch [76/200] batch [10/30] time 0.537 (0.493) data 0.405 (0.362) loss_x loss_x 1.1396 (1.1778) acc_x 75.0000 (68.1250) lr 1.3971e-03 eta 0:00:09
epoch [76/200] batch [15/30] time 0.500 (0.499) data 0.370 (0.368) loss_x loss_x 1.1016 (1.1992) acc_x 62.5000 (67.2917) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [20/30] time 0.545 (0.502) data 0.413 (0.372) loss_x loss_x 1.8838 (1.2095) acc_x 50.0000 (66.8750) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [25/30] time 0.527 (0.504) data 0.396 (0.373) loss_x loss_x 1.0781 (1.1895) acc_x 59.3750 (67.7500) lr 1.3971e-03 eta 0:00:02
epoch [76/200] batch [30/30] time 0.437 (0.496) data 0.306 (0.365) loss_x loss_x 1.1377 (1.2141) acc_x 68.7500 (67.5000) lr 1.3971e-03 eta 0:00:00
epoch [76/200] batch [5/67] time 0.422 (0.488) data 0.292 (0.357) loss_u loss_u 0.8789 (0.8575) acc_u 15.6250 (22.5000) lr 1.3971e-03 eta 0:00:30
epoch [76/200] batch [10/67] time 0.420 (0.481) data 0.290 (0.351) loss_u loss_u 0.8208 (0.8675) acc_u 25.0000 (19.6875) lr 1.3971e-03 eta 0:00:27
epoch [76/200] batch [15/67] time 0.393 (0.477) data 0.261 (0.346) loss_u loss_u 0.7979 (0.8640) acc_u 21.8750 (18.7500) lr 1.3971e-03 eta 0:00:24
epoch [76/200] batch [20/67] time 0.471 (0.474) data 0.340 (0.343) loss_u loss_u 0.8496 (0.8687) acc_u 15.6250 (17.3438) lr 1.3971e-03 eta 0:00:22
epoch [76/200] batch [25/67] time 0.534 (0.470) data 0.402 (0.339) loss_u loss_u 0.8955 (0.8716) acc_u 12.5000 (16.7500) lr 1.3971e-03 eta 0:00:19
epoch [76/200] batch [30/67] time 0.451 (0.465) data 0.320 (0.334) loss_u loss_u 0.8804 (0.8746) acc_u 21.8750 (16.1458) lr 1.3971e-03 eta 0:00:17
epoch [76/200] batch [35/67] time 0.532 (0.463) data 0.401 (0.332) loss_u loss_u 0.8477 (0.8740) acc_u 25.0000 (16.3393) lr 1.3971e-03 eta 0:00:14
epoch [76/200] batch [40/67] time 0.442 (0.461) data 0.311 (0.330) loss_u loss_u 0.9209 (0.8786) acc_u 12.5000 (15.7812) lr 1.3971e-03 eta 0:00:12
epoch [76/200] batch [45/67] time 0.422 (0.460) data 0.290 (0.328) loss_u loss_u 0.9111 (0.8789) acc_u 9.3750 (15.5556) lr 1.3971e-03 eta 0:00:10
epoch [76/200] batch [50/67] time 0.560 (0.462) data 0.428 (0.331) loss_u loss_u 0.8994 (0.8818) acc_u 12.5000 (15.1875) lr 1.3971e-03 eta 0:00:07
epoch [76/200] batch [55/67] time 0.692 (0.462) data 0.561 (0.331) loss_u loss_u 0.8506 (0.8806) acc_u 18.7500 (15.3409) lr 1.3971e-03 eta 0:00:05
epoch [76/200] batch [60/67] time 0.530 (0.465) data 0.397 (0.334) loss_u loss_u 0.9038 (0.8821) acc_u 12.5000 (15.2083) lr 1.3971e-03 eta 0:00:03
epoch [76/200] batch [65/67] time 0.417 (0.463) data 0.286 (0.332) loss_u loss_u 0.9004 (0.8825) acc_u 15.6250 (15.0962) lr 1.3971e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1647
confident_label rate tensor(0.3042, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 954
clean true:836
clean false:118
clean_rate:0.8763102725366876
noisy true:653
noisy false:1529
after delete: len(clean_dataset) 954
after delete: len(noisy_dataset) 2182
epoch [77/200] batch [5/29] time 0.445 (0.462) data 0.314 (0.331) loss_x loss_x 1.1709 (1.3457) acc_x 65.6250 (64.3750) lr 1.3827e-03 eta 0:00:11
epoch [77/200] batch [10/29] time 0.598 (0.461) data 0.467 (0.330) loss_x loss_x 0.9409 (1.1922) acc_x 78.1250 (69.3750) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [15/29] time 0.435 (0.460) data 0.304 (0.329) loss_x loss_x 1.3018 (1.2179) acc_x 68.7500 (69.7917) lr 1.3827e-03 eta 0:00:06
epoch [77/200] batch [20/29] time 0.567 (0.473) data 0.436 (0.342) loss_x loss_x 1.5361 (1.2689) acc_x 59.3750 (68.5938) lr 1.3827e-03 eta 0:00:04
epoch [77/200] batch [25/29] time 0.497 (0.474) data 0.365 (0.343) loss_x loss_x 1.4648 (1.3049) acc_x 65.6250 (67.5000) lr 1.3827e-03 eta 0:00:01
epoch [77/200] batch [5/68] time 0.384 (0.472) data 0.252 (0.342) loss_u loss_u 0.8950 (0.8400) acc_u 12.5000 (22.5000) lr 1.3827e-03 eta 0:00:29
epoch [77/200] batch [10/68] time 0.446 (0.466) data 0.315 (0.336) loss_u loss_u 0.8438 (0.8595) acc_u 25.0000 (18.7500) lr 1.3827e-03 eta 0:00:27
epoch [77/200] batch [15/68] time 0.372 (0.470) data 0.241 (0.339) loss_u loss_u 0.9307 (0.8757) acc_u 9.3750 (16.6667) lr 1.3827e-03 eta 0:00:24
epoch [77/200] batch [20/68] time 0.416 (0.466) data 0.284 (0.335) loss_u loss_u 0.9214 (0.8757) acc_u 12.5000 (16.8750) lr 1.3827e-03 eta 0:00:22
epoch [77/200] batch [25/68] time 0.475 (0.465) data 0.343 (0.334) loss_u loss_u 0.8159 (0.8682) acc_u 21.8750 (17.6250) lr 1.3827e-03 eta 0:00:20
epoch [77/200] batch [30/68] time 0.485 (0.461) data 0.353 (0.330) loss_u loss_u 0.8530 (0.8652) acc_u 21.8750 (18.0208) lr 1.3827e-03 eta 0:00:17
epoch [77/200] batch [35/68] time 0.527 (0.458) data 0.396 (0.327) loss_u loss_u 0.9111 (0.8698) acc_u 12.5000 (17.5000) lr 1.3827e-03 eta 0:00:15
epoch [77/200] batch [40/68] time 0.419 (0.453) data 0.289 (0.322) loss_u loss_u 0.9146 (0.8744) acc_u 18.7500 (16.8750) lr 1.3827e-03 eta 0:00:12
epoch [77/200] batch [45/68] time 0.418 (0.452) data 0.287 (0.321) loss_u loss_u 0.9106 (0.8788) acc_u 18.7500 (16.3889) lr 1.3827e-03 eta 0:00:10
epoch [77/200] batch [50/68] time 0.359 (0.451) data 0.228 (0.320) loss_u loss_u 0.8506 (0.8788) acc_u 18.7500 (16.2500) lr 1.3827e-03 eta 0:00:08
epoch [77/200] batch [55/68] time 0.516 (0.452) data 0.386 (0.321) loss_u loss_u 0.8506 (0.8790) acc_u 21.8750 (15.9091) lr 1.3827e-03 eta 0:00:05
epoch [77/200] batch [60/68] time 0.378 (0.454) data 0.246 (0.323) loss_u loss_u 0.8813 (0.8775) acc_u 15.6250 (15.8333) lr 1.3827e-03 eta 0:00:03
epoch [77/200] batch [65/68] time 0.409 (0.453) data 0.277 (0.322) loss_u loss_u 0.9146 (0.8777) acc_u 6.2500 (15.9135) lr 1.3827e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1608
confident_label rate tensor(0.3109, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 975
clean true:847
clean false:128
clean_rate:0.8687179487179487
noisy true:681
noisy false:1480
after delete: len(clean_dataset) 975
after delete: len(noisy_dataset) 2161
epoch [78/200] batch [5/30] time 0.391 (0.498) data 0.261 (0.368) loss_x loss_x 0.7603 (1.2622) acc_x 81.2500 (70.6250) lr 1.3681e-03 eta 0:00:12
epoch [78/200] batch [10/30] time 0.446 (0.477) data 0.316 (0.346) loss_x loss_x 1.2773 (1.3089) acc_x 62.5000 (66.8750) lr 1.3681e-03 eta 0:00:09
epoch [78/200] batch [15/30] time 0.652 (0.488) data 0.522 (0.357) loss_x loss_x 2.0859 (1.3751) acc_x 56.2500 (65.4167) lr 1.3681e-03 eta 0:00:07
epoch [78/200] batch [20/30] time 0.505 (0.489) data 0.375 (0.358) loss_x loss_x 1.5889 (1.3719) acc_x 62.5000 (64.6875) lr 1.3681e-03 eta 0:00:04
epoch [78/200] batch [25/30] time 0.472 (0.489) data 0.341 (0.359) loss_x loss_x 1.1270 (1.3235) acc_x 65.6250 (65.6250) lr 1.3681e-03 eta 0:00:02
epoch [78/200] batch [30/30] time 0.749 (0.492) data 0.618 (0.361) loss_x loss_x 0.9951 (1.3409) acc_x 68.7500 (65.5208) lr 1.3681e-03 eta 0:00:00
epoch [78/200] batch [5/67] time 0.671 (0.494) data 0.540 (0.363) loss_u loss_u 0.8579 (0.8686) acc_u 18.7500 (15.0000) lr 1.3681e-03 eta 0:00:30
epoch [78/200] batch [10/67] time 0.444 (0.490) data 0.312 (0.359) loss_u loss_u 0.8794 (0.8723) acc_u 18.7500 (16.2500) lr 1.3681e-03 eta 0:00:27
epoch [78/200] batch [15/67] time 0.471 (0.488) data 0.340 (0.358) loss_u loss_u 0.8872 (0.8676) acc_u 15.6250 (17.0833) lr 1.3681e-03 eta 0:00:25
epoch [78/200] batch [20/67] time 0.491 (0.490) data 0.359 (0.360) loss_u loss_u 0.9370 (0.8712) acc_u 9.3750 (16.7188) lr 1.3681e-03 eta 0:00:23
epoch [78/200] batch [25/67] time 0.505 (0.486) data 0.374 (0.356) loss_u loss_u 0.9023 (0.8723) acc_u 15.6250 (16.5000) lr 1.3681e-03 eta 0:00:20
epoch [78/200] batch [30/67] time 0.535 (0.485) data 0.403 (0.354) loss_u loss_u 0.8354 (0.8726) acc_u 15.6250 (16.0417) lr 1.3681e-03 eta 0:00:17
epoch [78/200] batch [35/67] time 0.424 (0.480) data 0.292 (0.349) loss_u loss_u 0.8691 (0.8759) acc_u 18.7500 (15.8929) lr 1.3681e-03 eta 0:00:15
epoch [78/200] batch [40/67] time 0.394 (0.475) data 0.262 (0.344) loss_u loss_u 0.8862 (0.8760) acc_u 15.6250 (16.0156) lr 1.3681e-03 eta 0:00:12
epoch [78/200] batch [45/67] time 0.384 (0.473) data 0.254 (0.342) loss_u loss_u 0.9180 (0.8796) acc_u 12.5000 (15.3472) lr 1.3681e-03 eta 0:00:10
epoch [78/200] batch [50/67] time 0.506 (0.471) data 0.376 (0.340) loss_u loss_u 0.8057 (0.8737) acc_u 21.8750 (16.0000) lr 1.3681e-03 eta 0:00:08
epoch [78/200] batch [55/67] time 0.493 (0.470) data 0.360 (0.339) loss_u loss_u 0.9438 (0.8775) acc_u 3.1250 (15.3409) lr 1.3681e-03 eta 0:00:05
epoch [78/200] batch [60/67] time 0.490 (0.468) data 0.359 (0.337) loss_u loss_u 0.9316 (0.8768) acc_u 6.2500 (15.4688) lr 1.3681e-03 eta 0:00:03
epoch [78/200] batch [65/67] time 0.371 (0.465) data 0.241 (0.334) loss_u loss_u 0.8545 (0.8783) acc_u 21.8750 (15.3365) lr 1.3681e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1624
confident_label rate tensor(0.3029, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 950
clean true:838
clean false:112
clean_rate:0.8821052631578947
noisy true:674
noisy false:1512
after delete: len(clean_dataset) 950
after delete: len(noisy_dataset) 2186
epoch [79/200] batch [5/29] time 0.379 (0.455) data 0.248 (0.324) loss_x loss_x 1.4834 (1.2358) acc_x 62.5000 (68.1250) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [10/29] time 0.429 (0.459) data 0.298 (0.328) loss_x loss_x 1.4238 (1.3333) acc_x 62.5000 (66.2500) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [15/29] time 0.453 (0.464) data 0.323 (0.333) loss_x loss_x 1.1582 (1.2988) acc_x 71.8750 (66.4583) lr 1.3535e-03 eta 0:00:06
epoch [79/200] batch [20/29] time 0.496 (0.465) data 0.366 (0.335) loss_x loss_x 1.2598 (1.3091) acc_x 71.8750 (67.5000) lr 1.3535e-03 eta 0:00:04
epoch [79/200] batch [25/29] time 0.475 (0.465) data 0.344 (0.334) loss_x loss_x 1.6270 (1.3056) acc_x 53.1250 (67.1250) lr 1.3535e-03 eta 0:00:01
epoch [79/200] batch [5/68] time 0.416 (0.461) data 0.286 (0.330) loss_u loss_u 0.8257 (0.8729) acc_u 21.8750 (15.6250) lr 1.3535e-03 eta 0:00:29
epoch [79/200] batch [10/68] time 0.679 (0.465) data 0.548 (0.334) loss_u loss_u 0.9023 (0.8793) acc_u 12.5000 (14.6875) lr 1.3535e-03 eta 0:00:26
epoch [79/200] batch [15/68] time 0.542 (0.468) data 0.411 (0.337) loss_u loss_u 0.8657 (0.8821) acc_u 21.8750 (14.1667) lr 1.3535e-03 eta 0:00:24
epoch [79/200] batch [20/68] time 0.539 (0.466) data 0.407 (0.335) loss_u loss_u 0.8892 (0.8788) acc_u 9.3750 (14.5312) lr 1.3535e-03 eta 0:00:22
epoch [79/200] batch [25/68] time 0.535 (0.465) data 0.403 (0.335) loss_u loss_u 0.8574 (0.8809) acc_u 12.5000 (14.0000) lr 1.3535e-03 eta 0:00:20
epoch [79/200] batch [30/68] time 0.519 (0.465) data 0.389 (0.334) loss_u loss_u 0.8193 (0.8806) acc_u 15.6250 (14.4792) lr 1.3535e-03 eta 0:00:17
epoch [79/200] batch [35/68] time 0.439 (0.462) data 0.308 (0.331) loss_u loss_u 0.9253 (0.8819) acc_u 9.3750 (14.3750) lr 1.3535e-03 eta 0:00:15
epoch [79/200] batch [40/68] time 0.546 (0.463) data 0.415 (0.332) loss_u loss_u 0.9741 (0.8841) acc_u 0.0000 (14.2188) lr 1.3535e-03 eta 0:00:12
epoch [79/200] batch [45/68] time 0.402 (0.460) data 0.271 (0.329) loss_u loss_u 0.9429 (0.8855) acc_u 3.1250 (13.7500) lr 1.3535e-03 eta 0:00:10
epoch [79/200] batch [50/68] time 0.400 (0.459) data 0.269 (0.328) loss_u loss_u 0.8984 (0.8837) acc_u 12.5000 (14.0625) lr 1.3535e-03 eta 0:00:08
epoch [79/200] batch [55/68] time 0.469 (0.461) data 0.337 (0.330) loss_u loss_u 0.8530 (0.8831) acc_u 18.7500 (14.2045) lr 1.3535e-03 eta 0:00:05
epoch [79/200] batch [60/68] time 0.436 (0.457) data 0.305 (0.326) loss_u loss_u 0.8584 (0.8817) acc_u 18.7500 (14.4271) lr 1.3535e-03 eta 0:00:03
epoch [79/200] batch [65/68] time 0.384 (0.456) data 0.253 (0.325) loss_u loss_u 0.9551 (0.8831) acc_u 6.2500 (14.2788) lr 1.3535e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1659
confident_label rate tensor(0.3045, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 955
clean true:834
clean false:121
clean_rate:0.8732984293193717
noisy true:643
noisy false:1538
after delete: len(clean_dataset) 955
after delete: len(noisy_dataset) 2181
epoch [80/200] batch [5/29] time 0.452 (0.524) data 0.321 (0.393) loss_x loss_x 0.9165 (1.2425) acc_x 75.0000 (65.0000) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [10/29] time 0.422 (0.461) data 0.291 (0.330) loss_x loss_x 1.2041 (1.2298) acc_x 78.1250 (67.8125) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [15/29] time 0.457 (0.467) data 0.326 (0.337) loss_x loss_x 0.8770 (1.2053) acc_x 75.0000 (67.5000) lr 1.3387e-03 eta 0:00:06
epoch [80/200] batch [20/29] time 0.443 (0.471) data 0.312 (0.340) loss_x loss_x 1.7969 (1.2047) acc_x 59.3750 (69.3750) lr 1.3387e-03 eta 0:00:04
epoch [80/200] batch [25/29] time 0.354 (0.473) data 0.223 (0.342) loss_x loss_x 1.4648 (1.2435) acc_x 53.1250 (68.6250) lr 1.3387e-03 eta 0:00:01
epoch [80/200] batch [5/68] time 0.461 (0.459) data 0.330 (0.328) loss_u loss_u 0.8716 (0.8798) acc_u 15.6250 (13.1250) lr 1.3387e-03 eta 0:00:28
epoch [80/200] batch [10/68] time 0.390 (0.455) data 0.258 (0.324) loss_u loss_u 0.9585 (0.8849) acc_u 0.0000 (12.8125) lr 1.3387e-03 eta 0:00:26
epoch [80/200] batch [15/68] time 0.373 (0.456) data 0.241 (0.325) loss_u loss_u 0.9194 (0.8896) acc_u 9.3750 (13.1250) lr 1.3387e-03 eta 0:00:24
epoch [80/200] batch [20/68] time 0.672 (0.463) data 0.541 (0.332) loss_u loss_u 0.7983 (0.8855) acc_u 25.0000 (13.2812) lr 1.3387e-03 eta 0:00:22
epoch [80/200] batch [25/68] time 0.499 (0.459) data 0.368 (0.328) loss_u loss_u 0.8862 (0.8824) acc_u 15.6250 (14.1250) lr 1.3387e-03 eta 0:00:19
epoch [80/200] batch [30/68] time 0.557 (0.458) data 0.427 (0.327) loss_u loss_u 0.9697 (0.8820) acc_u 3.1250 (14.1667) lr 1.3387e-03 eta 0:00:17
epoch [80/200] batch [35/68] time 0.404 (0.455) data 0.274 (0.324) loss_u loss_u 0.9243 (0.8837) acc_u 3.1250 (13.7500) lr 1.3387e-03 eta 0:00:15
epoch [80/200] batch [40/68] time 0.409 (0.455) data 0.278 (0.324) loss_u loss_u 0.9097 (0.8773) acc_u 9.3750 (14.1406) lr 1.3387e-03 eta 0:00:12
epoch [80/200] batch [45/68] time 0.462 (0.455) data 0.330 (0.324) loss_u loss_u 0.9243 (0.8769) acc_u 9.3750 (14.5139) lr 1.3387e-03 eta 0:00:10
epoch [80/200] batch [50/68] time 0.467 (0.456) data 0.335 (0.325) loss_u loss_u 0.9023 (0.8753) acc_u 15.6250 (14.5625) lr 1.3387e-03 eta 0:00:08
epoch [80/200] batch [55/68] time 0.548 (0.458) data 0.417 (0.327) loss_u loss_u 0.8330 (0.8729) acc_u 15.6250 (14.9432) lr 1.3387e-03 eta 0:00:05
epoch [80/200] batch [60/68] time 0.408 (0.456) data 0.276 (0.325) loss_u loss_u 0.8604 (0.8729) acc_u 15.6250 (15.1042) lr 1.3387e-03 eta 0:00:03
epoch [80/200] batch [65/68] time 0.365 (0.454) data 0.233 (0.323) loss_u loss_u 0.7427 (0.8686) acc_u 34.3750 (15.7692) lr 1.3387e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1643
confident_label rate tensor(0.3128, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 981
clean true:857
clean false:124
clean_rate:0.873598369011213
noisy true:636
noisy false:1519
after delete: len(clean_dataset) 981
after delete: len(noisy_dataset) 2155
epoch [81/200] batch [5/30] time 0.426 (0.451) data 0.295 (0.320) loss_x loss_x 0.8560 (1.1446) acc_x 71.8750 (68.1250) lr 1.3239e-03 eta 0:00:11
epoch [81/200] batch [10/30] time 0.530 (0.481) data 0.400 (0.350) loss_x loss_x 1.2998 (1.1835) acc_x 59.3750 (69.3750) lr 1.3239e-03 eta 0:00:09
epoch [81/200] batch [15/30] time 0.381 (0.469) data 0.252 (0.338) loss_x loss_x 1.8203 (1.2540) acc_x 53.1250 (67.9167) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [20/30] time 0.486 (0.474) data 0.356 (0.343) loss_x loss_x 1.2676 (1.2586) acc_x 68.7500 (67.5000) lr 1.3239e-03 eta 0:00:04
epoch [81/200] batch [25/30] time 0.457 (0.478) data 0.326 (0.348) loss_x loss_x 0.8130 (1.2327) acc_x 81.2500 (68.7500) lr 1.3239e-03 eta 0:00:02
epoch [81/200] batch [30/30] time 0.388 (0.476) data 0.257 (0.345) loss_x loss_x 1.1465 (1.2436) acc_x 75.0000 (68.4375) lr 1.3239e-03 eta 0:00:00
epoch [81/200] batch [5/67] time 0.394 (0.470) data 0.262 (0.340) loss_u loss_u 0.8496 (0.8941) acc_u 21.8750 (13.1250) lr 1.3239e-03 eta 0:00:29
epoch [81/200] batch [10/67] time 0.302 (0.461) data 0.171 (0.331) loss_u loss_u 0.8940 (0.8916) acc_u 12.5000 (12.8125) lr 1.3239e-03 eta 0:00:26
epoch [81/200] batch [15/67] time 0.370 (0.453) data 0.239 (0.323) loss_u loss_u 0.8008 (0.8782) acc_u 31.2500 (14.5833) lr 1.3239e-03 eta 0:00:23
epoch [81/200] batch [20/67] time 0.409 (0.450) data 0.278 (0.319) loss_u loss_u 0.9097 (0.8811) acc_u 12.5000 (14.2188) lr 1.3239e-03 eta 0:00:21
epoch [81/200] batch [25/67] time 0.385 (0.454) data 0.255 (0.323) loss_u loss_u 0.9194 (0.8832) acc_u 9.3750 (14.5000) lr 1.3239e-03 eta 0:00:19
epoch [81/200] batch [30/67] time 0.648 (0.459) data 0.516 (0.328) loss_u loss_u 0.8589 (0.8809) acc_u 18.7500 (14.7917) lr 1.3239e-03 eta 0:00:16
epoch [81/200] batch [35/67] time 0.385 (0.453) data 0.254 (0.322) loss_u loss_u 0.9033 (0.8820) acc_u 9.3750 (14.3750) lr 1.3239e-03 eta 0:00:14
epoch [81/200] batch [40/67] time 0.421 (0.457) data 0.289 (0.326) loss_u loss_u 0.8843 (0.8848) acc_u 12.5000 (13.8281) lr 1.3239e-03 eta 0:00:12
epoch [81/200] batch [45/67] time 0.415 (0.456) data 0.282 (0.325) loss_u loss_u 0.8740 (0.8848) acc_u 9.3750 (13.7500) lr 1.3239e-03 eta 0:00:10
epoch [81/200] batch [50/67] time 0.547 (0.462) data 0.415 (0.331) loss_u loss_u 0.9424 (0.8839) acc_u 6.2500 (13.8750) lr 1.3239e-03 eta 0:00:07
epoch [81/200] batch [55/67] time 0.575 (0.462) data 0.444 (0.331) loss_u loss_u 0.8633 (0.8820) acc_u 12.5000 (13.9773) lr 1.3239e-03 eta 0:00:05
epoch [81/200] batch [60/67] time 0.430 (0.460) data 0.298 (0.329) loss_u loss_u 0.8853 (0.8825) acc_u 18.7500 (14.1146) lr 1.3239e-03 eta 0:00:03
epoch [81/200] batch [65/67] time 0.457 (0.458) data 0.327 (0.326) loss_u loss_u 0.9531 (0.8835) acc_u 3.1250 (14.0385) lr 1.3239e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1636
confident_label rate tensor(0.3042, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 954
clean true:842
clean false:112
clean_rate:0.8825995807127882
noisy true:658
noisy false:1524
after delete: len(clean_dataset) 954
after delete: len(noisy_dataset) 2182
epoch [82/200] batch [5/29] time 0.461 (0.482) data 0.330 (0.351) loss_x loss_x 1.2158 (1.3396) acc_x 71.8750 (73.7500) lr 1.3090e-03 eta 0:00:11
epoch [82/200] batch [10/29] time 0.698 (0.525) data 0.568 (0.394) loss_x loss_x 0.8286 (1.2534) acc_x 81.2500 (72.5000) lr 1.3090e-03 eta 0:00:09
epoch [82/200] batch [15/29] time 0.565 (0.503) data 0.435 (0.372) loss_x loss_x 1.3008 (1.2771) acc_x 75.0000 (71.0417) lr 1.3090e-03 eta 0:00:07
epoch [82/200] batch [20/29] time 0.506 (0.489) data 0.376 (0.359) loss_x loss_x 0.9917 (1.2874) acc_x 71.8750 (69.5312) lr 1.3090e-03 eta 0:00:04
epoch [82/200] batch [25/29] time 0.445 (0.488) data 0.315 (0.358) loss_x loss_x 1.2500 (1.2724) acc_x 65.6250 (69.8750) lr 1.3090e-03 eta 0:00:01
epoch [82/200] batch [5/68] time 0.427 (0.484) data 0.295 (0.353) loss_u loss_u 0.8760 (0.8635) acc_u 15.6250 (17.5000) lr 1.3090e-03 eta 0:00:30
epoch [82/200] batch [10/68] time 0.383 (0.474) data 0.253 (0.343) loss_u loss_u 0.8530 (0.8696) acc_u 12.5000 (15.6250) lr 1.3090e-03 eta 0:00:27
epoch [82/200] batch [15/68] time 0.525 (0.472) data 0.394 (0.341) loss_u loss_u 0.8462 (0.8663) acc_u 25.0000 (16.8750) lr 1.3090e-03 eta 0:00:24
epoch [82/200] batch [20/68] time 0.449 (0.470) data 0.318 (0.339) loss_u loss_u 0.8833 (0.8710) acc_u 12.5000 (15.9375) lr 1.3090e-03 eta 0:00:22
epoch [82/200] batch [25/68] time 0.386 (0.462) data 0.256 (0.331) loss_u loss_u 0.8989 (0.8733) acc_u 12.5000 (15.7500) lr 1.3090e-03 eta 0:00:19
epoch [82/200] batch [30/68] time 0.334 (0.465) data 0.203 (0.334) loss_u loss_u 0.9360 (0.8760) acc_u 6.2500 (15.6250) lr 1.3090e-03 eta 0:00:17
epoch [82/200] batch [35/68] time 0.583 (0.471) data 0.452 (0.340) loss_u loss_u 0.9136 (0.8751) acc_u 12.5000 (15.7143) lr 1.3090e-03 eta 0:00:15
epoch [82/200] batch [40/68] time 0.370 (0.469) data 0.239 (0.338) loss_u loss_u 0.8848 (0.8756) acc_u 15.6250 (15.9375) lr 1.3090e-03 eta 0:00:13
epoch [82/200] batch [45/68] time 0.427 (0.471) data 0.295 (0.340) loss_u loss_u 0.8652 (0.8774) acc_u 15.6250 (15.7639) lr 1.3090e-03 eta 0:00:10
epoch [82/200] batch [50/68] time 0.408 (0.471) data 0.277 (0.340) loss_u loss_u 0.8916 (0.8781) acc_u 9.3750 (15.5000) lr 1.3090e-03 eta 0:00:08
epoch [82/200] batch [55/68] time 0.445 (0.469) data 0.313 (0.338) loss_u loss_u 0.9082 (0.8777) acc_u 12.5000 (15.4545) lr 1.3090e-03 eta 0:00:06
epoch [82/200] batch [60/68] time 0.439 (0.467) data 0.307 (0.336) loss_u loss_u 0.8872 (0.8763) acc_u 9.3750 (15.5729) lr 1.3090e-03 eta 0:00:03
epoch [82/200] batch [65/68] time 0.452 (0.464) data 0.320 (0.333) loss_u loss_u 0.8955 (0.8770) acc_u 12.5000 (15.4808) lr 1.3090e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1621
confident_label rate tensor(0.3119, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 978
clean true:855
clean false:123
clean_rate:0.8742331288343558
noisy true:660
noisy false:1498
after delete: len(clean_dataset) 978
after delete: len(noisy_dataset) 2158
epoch [83/200] batch [5/30] time 0.619 (0.508) data 0.488 (0.377) loss_x loss_x 0.9268 (1.2736) acc_x 81.2500 (70.0000) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [10/30] time 0.468 (0.460) data 0.337 (0.329) loss_x loss_x 1.1289 (1.3351) acc_x 65.6250 (66.5625) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [15/30] time 0.446 (0.444) data 0.316 (0.313) loss_x loss_x 1.9453 (1.3289) acc_x 46.8750 (66.8750) lr 1.2940e-03 eta 0:00:06
epoch [83/200] batch [20/30] time 0.365 (0.440) data 0.233 (0.310) loss_x loss_x 1.5713 (1.3320) acc_x 53.1250 (66.7188) lr 1.2940e-03 eta 0:00:04
epoch [83/200] batch [25/30] time 0.513 (0.443) data 0.382 (0.313) loss_x loss_x 1.8076 (1.3228) acc_x 59.3750 (67.2500) lr 1.2940e-03 eta 0:00:02
epoch [83/200] batch [30/30] time 0.453 (0.455) data 0.322 (0.325) loss_x loss_x 0.9653 (1.3320) acc_x 71.8750 (67.3958) lr 1.2940e-03 eta 0:00:00
epoch [83/200] batch [5/67] time 0.362 (0.453) data 0.230 (0.322) loss_u loss_u 0.8379 (0.8695) acc_u 18.7500 (15.0000) lr 1.2940e-03 eta 0:00:28
epoch [83/200] batch [10/67] time 0.431 (0.451) data 0.299 (0.320) loss_u loss_u 0.9023 (0.8636) acc_u 9.3750 (15.9375) lr 1.2940e-03 eta 0:00:25
epoch [83/200] batch [15/67] time 0.398 (0.450) data 0.266 (0.319) loss_u loss_u 0.8838 (0.8731) acc_u 12.5000 (15.0000) lr 1.2940e-03 eta 0:00:23
epoch [83/200] batch [20/67] time 0.433 (0.451) data 0.302 (0.320) loss_u loss_u 0.8438 (0.8749) acc_u 15.6250 (14.6875) lr 1.2940e-03 eta 0:00:21
epoch [83/200] batch [25/67] time 0.295 (0.449) data 0.164 (0.318) loss_u loss_u 0.8926 (0.8733) acc_u 9.3750 (15.0000) lr 1.2940e-03 eta 0:00:18
epoch [83/200] batch [30/67] time 0.483 (0.449) data 0.352 (0.318) loss_u loss_u 0.8555 (0.8758) acc_u 15.6250 (14.6875) lr 1.2940e-03 eta 0:00:16
epoch [83/200] batch [35/67] time 0.394 (0.448) data 0.263 (0.316) loss_u loss_u 0.8931 (0.8748) acc_u 9.3750 (14.8214) lr 1.2940e-03 eta 0:00:14
epoch [83/200] batch [40/67] time 0.388 (0.446) data 0.257 (0.315) loss_u loss_u 0.8525 (0.8734) acc_u 21.8750 (15.2344) lr 1.2940e-03 eta 0:00:12
epoch [83/200] batch [45/67] time 0.411 (0.446) data 0.280 (0.314) loss_u loss_u 0.9585 (0.8779) acc_u 9.3750 (14.7917) lr 1.2940e-03 eta 0:00:09
epoch [83/200] batch [50/67] time 0.664 (0.450) data 0.532 (0.319) loss_u loss_u 0.8955 (0.8779) acc_u 15.6250 (14.8125) lr 1.2940e-03 eta 0:00:07
epoch [83/200] batch [55/67] time 0.415 (0.449) data 0.284 (0.317) loss_u loss_u 0.8921 (0.8787) acc_u 12.5000 (14.7727) lr 1.2940e-03 eta 0:00:05
epoch [83/200] batch [60/67] time 0.520 (0.455) data 0.389 (0.324) loss_u loss_u 0.8955 (0.8792) acc_u 18.7500 (14.7917) lr 1.2940e-03 eta 0:00:03
epoch [83/200] batch [65/67] time 0.443 (0.453) data 0.312 (0.322) loss_u loss_u 0.8564 (0.8805) acc_u 18.7500 (14.7596) lr 1.2940e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1657
confident_label rate tensor(0.3084, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 967
clean true:851
clean false:116
clean_rate:0.8800413650465356
noisy true:628
noisy false:1541
after delete: len(clean_dataset) 967
after delete: len(noisy_dataset) 2169
epoch [84/200] batch [5/30] time 0.581 (0.531) data 0.450 (0.400) loss_x loss_x 0.8223 (1.3150) acc_x 78.1250 (70.0000) lr 1.2790e-03 eta 0:00:13
epoch [84/200] batch [10/30] time 0.366 (0.517) data 0.236 (0.386) loss_x loss_x 1.6885 (1.3564) acc_x 59.3750 (67.8125) lr 1.2790e-03 eta 0:00:10
epoch [84/200] batch [15/30] time 0.414 (0.498) data 0.284 (0.367) loss_x loss_x 1.4238 (1.3333) acc_x 62.5000 (68.9583) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [20/30] time 0.687 (0.503) data 0.556 (0.372) loss_x loss_x 1.4385 (1.3249) acc_x 53.1250 (67.9688) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [25/30] time 0.491 (0.509) data 0.361 (0.378) loss_x loss_x 1.2227 (1.2522) acc_x 65.6250 (69.2500) lr 1.2790e-03 eta 0:00:02
epoch [84/200] batch [30/30] time 0.432 (0.503) data 0.301 (0.372) loss_x loss_x 1.2520 (1.2726) acc_x 65.6250 (68.6458) lr 1.2790e-03 eta 0:00:00
epoch [84/200] batch [5/67] time 0.428 (0.493) data 0.297 (0.362) loss_u loss_u 0.8516 (0.8347) acc_u 21.8750 (22.5000) lr 1.2790e-03 eta 0:00:30
epoch [84/200] batch [10/67] time 0.446 (0.488) data 0.315 (0.357) loss_u loss_u 0.9561 (0.8528) acc_u 6.2500 (20.9375) lr 1.2790e-03 eta 0:00:27
epoch [84/200] batch [15/67] time 0.489 (0.482) data 0.357 (0.351) loss_u loss_u 0.9009 (0.8652) acc_u 9.3750 (17.9167) lr 1.2790e-03 eta 0:00:25
epoch [84/200] batch [20/67] time 0.482 (0.475) data 0.350 (0.344) loss_u loss_u 0.9204 (0.8702) acc_u 9.3750 (16.7188) lr 1.2790e-03 eta 0:00:22
epoch [84/200] batch [25/67] time 0.424 (0.478) data 0.293 (0.347) loss_u loss_u 0.8457 (0.8731) acc_u 18.7500 (16.2500) lr 1.2790e-03 eta 0:00:20
epoch [84/200] batch [30/67] time 0.526 (0.477) data 0.396 (0.346) loss_u loss_u 0.8662 (0.8734) acc_u 15.6250 (16.0417) lr 1.2790e-03 eta 0:00:17
epoch [84/200] batch [35/67] time 0.395 (0.470) data 0.263 (0.339) loss_u loss_u 0.8770 (0.8772) acc_u 12.5000 (15.4464) lr 1.2790e-03 eta 0:00:15
epoch [84/200] batch [40/67] time 0.385 (0.469) data 0.254 (0.338) loss_u loss_u 0.8823 (0.8813) acc_u 15.6250 (15.0781) lr 1.2790e-03 eta 0:00:12
epoch [84/200] batch [45/67] time 0.404 (0.467) data 0.272 (0.336) loss_u loss_u 0.8721 (0.8838) acc_u 18.7500 (14.9306) lr 1.2790e-03 eta 0:00:10
epoch [84/200] batch [50/67] time 0.446 (0.467) data 0.315 (0.336) loss_u loss_u 0.8809 (0.8855) acc_u 15.6250 (14.6875) lr 1.2790e-03 eta 0:00:07
epoch [84/200] batch [55/67] time 0.571 (0.468) data 0.440 (0.337) loss_u loss_u 0.8984 (0.8842) acc_u 12.5000 (14.8295) lr 1.2790e-03 eta 0:00:05
epoch [84/200] batch [60/67] time 0.492 (0.469) data 0.362 (0.338) loss_u loss_u 0.9214 (0.8844) acc_u 6.2500 (14.6875) lr 1.2790e-03 eta 0:00:03
epoch [84/200] batch [65/67] time 0.589 (0.468) data 0.458 (0.337) loss_u loss_u 0.8970 (0.8843) acc_u 12.5000 (14.6154) lr 1.2790e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1613
confident_label rate tensor(0.3084, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 967
clean true:854
clean false:113
clean_rate:0.8831437435367114
noisy true:669
noisy false:1500
after delete: len(clean_dataset) 967
after delete: len(noisy_dataset) 2169
epoch [85/200] batch [5/30] time 0.467 (0.552) data 0.335 (0.421) loss_x loss_x 1.2334 (1.1516) acc_x 62.5000 (66.2500) lr 1.2639e-03 eta 0:00:13
epoch [85/200] batch [10/30] time 0.576 (0.523) data 0.446 (0.392) loss_x loss_x 1.4111 (1.1084) acc_x 59.3750 (70.0000) lr 1.2639e-03 eta 0:00:10
epoch [85/200] batch [15/30] time 0.442 (0.497) data 0.311 (0.367) loss_x loss_x 1.8125 (1.2370) acc_x 56.2500 (67.9167) lr 1.2639e-03 eta 0:00:07
epoch [85/200] batch [20/30] time 0.559 (0.488) data 0.429 (0.357) loss_x loss_x 1.3643 (1.2638) acc_x 59.3750 (66.8750) lr 1.2639e-03 eta 0:00:04
epoch [85/200] batch [25/30] time 0.456 (0.468) data 0.326 (0.338) loss_x loss_x 1.0791 (1.2498) acc_x 71.8750 (68.0000) lr 1.2639e-03 eta 0:00:02
epoch [85/200] batch [30/30] time 0.377 (0.464) data 0.247 (0.334) loss_x loss_x 0.9585 (1.2531) acc_x 75.0000 (67.0833) lr 1.2639e-03 eta 0:00:00
epoch [85/200] batch [5/67] time 0.409 (0.462) data 0.275 (0.332) loss_u loss_u 0.8691 (0.8705) acc_u 15.6250 (14.3750) lr 1.2639e-03 eta 0:00:28
epoch [85/200] batch [10/67] time 0.472 (0.467) data 0.340 (0.336) loss_u loss_u 0.9229 (0.8832) acc_u 9.3750 (13.4375) lr 1.2639e-03 eta 0:00:26
epoch [85/200] batch [15/67] time 0.394 (0.468) data 0.263 (0.337) loss_u loss_u 0.8281 (0.8735) acc_u 18.7500 (15.4167) lr 1.2639e-03 eta 0:00:24
epoch [85/200] batch [20/67] time 0.472 (0.463) data 0.340 (0.333) loss_u loss_u 0.8618 (0.8811) acc_u 18.7500 (14.8438) lr 1.2639e-03 eta 0:00:21
epoch [85/200] batch [25/67] time 0.386 (0.458) data 0.255 (0.327) loss_u loss_u 0.8979 (0.8883) acc_u 12.5000 (13.8750) lr 1.2639e-03 eta 0:00:19
epoch [85/200] batch [30/67] time 0.476 (0.463) data 0.346 (0.332) loss_u loss_u 0.8271 (0.8821) acc_u 21.8750 (14.6875) lr 1.2639e-03 eta 0:00:17
epoch [85/200] batch [35/67] time 0.547 (0.462) data 0.416 (0.331) loss_u loss_u 0.8232 (0.8802) acc_u 18.7500 (14.8214) lr 1.2639e-03 eta 0:00:14
epoch [85/200] batch [40/67] time 0.441 (0.461) data 0.308 (0.330) loss_u loss_u 0.9004 (0.8812) acc_u 9.3750 (14.5312) lr 1.2639e-03 eta 0:00:12
epoch [85/200] batch [45/67] time 0.465 (0.463) data 0.334 (0.332) loss_u loss_u 0.8325 (0.8812) acc_u 15.6250 (14.4444) lr 1.2639e-03 eta 0:00:10
epoch [85/200] batch [50/67] time 0.388 (0.461) data 0.256 (0.330) loss_u loss_u 0.8545 (0.8830) acc_u 15.6250 (14.2500) lr 1.2639e-03 eta 0:00:07
epoch [85/200] batch [55/67] time 0.510 (0.460) data 0.378 (0.329) loss_u loss_u 0.8584 (0.8810) acc_u 21.8750 (14.8295) lr 1.2639e-03 eta 0:00:05
epoch [85/200] batch [60/67] time 0.419 (0.458) data 0.287 (0.327) loss_u loss_u 0.8887 (0.8798) acc_u 12.5000 (15.0521) lr 1.2639e-03 eta 0:00:03
epoch [85/200] batch [65/67] time 0.462 (0.456) data 0.330 (0.325) loss_u loss_u 0.8784 (0.8805) acc_u 18.7500 (14.9519) lr 1.2639e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1610
confident_label rate tensor(0.3087, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 968
clean true:856
clean false:112
clean_rate:0.8842975206611571
noisy true:670
noisy false:1498
after delete: len(clean_dataset) 968
after delete: len(noisy_dataset) 2168
epoch [86/200] batch [5/30] time 0.528 (0.466) data 0.397 (0.335) loss_x loss_x 0.8198 (1.3302) acc_x 81.2500 (66.8750) lr 1.2487e-03 eta 0:00:11
epoch [86/200] batch [10/30] time 0.467 (0.432) data 0.336 (0.301) loss_x loss_x 0.9434 (1.3368) acc_x 81.2500 (69.3750) lr 1.2487e-03 eta 0:00:08
epoch [86/200] batch [15/30] time 0.341 (0.421) data 0.210 (0.290) loss_x loss_x 1.4062 (1.3714) acc_x 56.2500 (66.4583) lr 1.2487e-03 eta 0:00:06
epoch [86/200] batch [20/30] time 0.437 (0.437) data 0.306 (0.307) loss_x loss_x 1.3809 (1.3330) acc_x 59.3750 (66.5625) lr 1.2487e-03 eta 0:00:04
epoch [86/200] batch [25/30] time 0.394 (0.447) data 0.263 (0.316) loss_x loss_x 1.2939 (1.3232) acc_x 56.2500 (65.8750) lr 1.2487e-03 eta 0:00:02
epoch [86/200] batch [30/30] time 0.433 (0.446) data 0.302 (0.316) loss_x loss_x 1.4551 (1.3030) acc_x 65.6250 (66.6667) lr 1.2487e-03 eta 0:00:00
epoch [86/200] batch [5/67] time 0.461 (0.447) data 0.330 (0.316) loss_u loss_u 0.9487 (0.8765) acc_u 6.2500 (15.0000) lr 1.2487e-03 eta 0:00:27
epoch [86/200] batch [10/67] time 0.438 (0.449) data 0.307 (0.318) loss_u loss_u 0.8647 (0.8820) acc_u 18.7500 (14.3750) lr 1.2487e-03 eta 0:00:25
epoch [86/200] batch [15/67] time 0.391 (0.444) data 0.259 (0.313) loss_u loss_u 0.9336 (0.8799) acc_u 9.3750 (14.3750) lr 1.2487e-03 eta 0:00:23
epoch [86/200] batch [20/67] time 0.408 (0.445) data 0.276 (0.314) loss_u loss_u 0.8701 (0.8722) acc_u 18.7500 (15.1562) lr 1.2487e-03 eta 0:00:20
epoch [86/200] batch [25/67] time 0.386 (0.446) data 0.255 (0.315) loss_u loss_u 0.8672 (0.8647) acc_u 21.8750 (16.6250) lr 1.2487e-03 eta 0:00:18
epoch [86/200] batch [30/67] time 0.503 (0.447) data 0.371 (0.316) loss_u loss_u 0.9116 (0.8700) acc_u 6.2500 (15.4167) lr 1.2487e-03 eta 0:00:16
epoch [86/200] batch [35/67] time 0.468 (0.450) data 0.337 (0.319) loss_u loss_u 0.8535 (0.8664) acc_u 21.8750 (16.0714) lr 1.2487e-03 eta 0:00:14
epoch [86/200] batch [40/67] time 0.621 (0.454) data 0.490 (0.322) loss_u loss_u 0.9033 (0.8659) acc_u 9.3750 (16.0938) lr 1.2487e-03 eta 0:00:12
epoch [86/200] batch [45/67] time 0.479 (0.459) data 0.349 (0.328) loss_u loss_u 0.8906 (0.8685) acc_u 15.6250 (15.7639) lr 1.2487e-03 eta 0:00:10
epoch [86/200] batch [50/67] time 0.430 (0.460) data 0.298 (0.329) loss_u loss_u 0.8486 (0.8700) acc_u 18.7500 (15.8125) lr 1.2487e-03 eta 0:00:07
epoch [86/200] batch [55/67] time 0.477 (0.462) data 0.345 (0.331) loss_u loss_u 0.9302 (0.8706) acc_u 3.1250 (15.7386) lr 1.2487e-03 eta 0:00:05
epoch [86/200] batch [60/67] time 0.547 (0.464) data 0.416 (0.332) loss_u loss_u 0.9160 (0.8723) acc_u 15.6250 (15.6250) lr 1.2487e-03 eta 0:00:03
epoch [86/200] batch [65/67] time 0.557 (0.464) data 0.426 (0.333) loss_u loss_u 0.7925 (0.8699) acc_u 28.1250 (16.2500) lr 1.2487e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1638
confident_label rate tensor(0.2994, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 939
clean true:841
clean false:98
clean_rate:0.8956336528221512
noisy true:657
noisy false:1540
after delete: len(clean_dataset) 939
after delete: len(noisy_dataset) 2197
epoch [87/200] batch [5/29] time 0.406 (0.480) data 0.277 (0.350) loss_x loss_x 1.0244 (1.1862) acc_x 75.0000 (70.0000) lr 1.2334e-03 eta 0:00:11
epoch [87/200] batch [10/29] time 0.383 (0.452) data 0.253 (0.321) loss_x loss_x 1.3154 (1.2872) acc_x 75.0000 (69.0625) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [15/29] time 0.372 (0.445) data 0.242 (0.315) loss_x loss_x 1.1748 (1.2843) acc_x 56.2500 (67.0833) lr 1.2334e-03 eta 0:00:06
epoch [87/200] batch [20/29] time 0.437 (0.456) data 0.305 (0.325) loss_x loss_x 0.8530 (1.2517) acc_x 78.1250 (68.1250) lr 1.2334e-03 eta 0:00:04
epoch [87/200] batch [25/29] time 0.425 (0.460) data 0.295 (0.329) loss_x loss_x 1.2783 (1.2424) acc_x 75.0000 (68.2500) lr 1.2334e-03 eta 0:00:01
epoch [87/200] batch [5/68] time 0.457 (0.480) data 0.327 (0.349) loss_u loss_u 0.8677 (0.8809) acc_u 12.5000 (11.8750) lr 1.2334e-03 eta 0:00:30
epoch [87/200] batch [10/68] time 0.537 (0.481) data 0.406 (0.350) loss_u loss_u 0.9219 (0.8892) acc_u 9.3750 (12.5000) lr 1.2334e-03 eta 0:00:27
epoch [87/200] batch [15/68] time 0.391 (0.480) data 0.259 (0.349) loss_u loss_u 0.8604 (0.8753) acc_u 12.5000 (14.7917) lr 1.2334e-03 eta 0:00:25
epoch [87/200] batch [20/68] time 0.470 (0.481) data 0.338 (0.350) loss_u loss_u 0.9263 (0.8765) acc_u 6.2500 (15.1562) lr 1.2334e-03 eta 0:00:23
epoch [87/200] batch [25/68] time 0.448 (0.475) data 0.317 (0.344) loss_u loss_u 0.9287 (0.8804) acc_u 9.3750 (14.5000) lr 1.2334e-03 eta 0:00:20
epoch [87/200] batch [30/68] time 0.529 (0.473) data 0.396 (0.342) loss_u loss_u 0.7559 (0.8727) acc_u 31.2500 (15.4167) lr 1.2334e-03 eta 0:00:17
epoch [87/200] batch [35/68] time 0.434 (0.477) data 0.303 (0.346) loss_u loss_u 0.9575 (0.8746) acc_u 6.2500 (15.0000) lr 1.2334e-03 eta 0:00:15
epoch [87/200] batch [40/68] time 0.453 (0.476) data 0.322 (0.345) loss_u loss_u 0.8091 (0.8756) acc_u 25.0000 (15.0000) lr 1.2334e-03 eta 0:00:13
epoch [87/200] batch [45/68] time 0.626 (0.474) data 0.495 (0.343) loss_u loss_u 0.8765 (0.8742) acc_u 12.5000 (15.1389) lr 1.2334e-03 eta 0:00:10
epoch [87/200] batch [50/68] time 0.444 (0.474) data 0.313 (0.343) loss_u loss_u 0.8896 (0.8711) acc_u 9.3750 (15.3750) lr 1.2334e-03 eta 0:00:08
epoch [87/200] batch [55/68] time 0.423 (0.470) data 0.292 (0.339) loss_u loss_u 0.9258 (0.8734) acc_u 9.3750 (15.0000) lr 1.2334e-03 eta 0:00:06
epoch [87/200] batch [60/68] time 0.460 (0.470) data 0.329 (0.339) loss_u loss_u 0.9380 (0.8705) acc_u 6.2500 (15.3125) lr 1.2334e-03 eta 0:00:03
epoch [87/200] batch [65/68] time 0.419 (0.468) data 0.288 (0.337) loss_u loss_u 0.8774 (0.8697) acc_u 12.5000 (15.4808) lr 1.2334e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1582
confident_label rate tensor(0.3176, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 996
clean true:880
clean false:116
clean_rate:0.8835341365461847
noisy true:674
noisy false:1466
after delete: len(clean_dataset) 996
after delete: len(noisy_dataset) 2140
epoch [88/200] batch [5/31] time 0.475 (0.456) data 0.344 (0.325) loss_x loss_x 1.4717 (1.2141) acc_x 65.6250 (69.3750) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [10/31] time 0.372 (0.448) data 0.242 (0.318) loss_x loss_x 1.6084 (1.3120) acc_x 59.3750 (66.5625) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [15/31] time 0.551 (0.446) data 0.421 (0.315) loss_x loss_x 1.1709 (1.2522) acc_x 65.6250 (67.2917) lr 1.2181e-03 eta 0:00:07
epoch [88/200] batch [20/31] time 0.472 (0.454) data 0.342 (0.324) loss_x loss_x 1.2773 (1.3004) acc_x 65.6250 (67.0312) lr 1.2181e-03 eta 0:00:04
epoch [88/200] batch [25/31] time 0.444 (0.453) data 0.313 (0.322) loss_x loss_x 1.6602 (1.3397) acc_x 53.1250 (65.6250) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [30/31] time 0.435 (0.461) data 0.304 (0.330) loss_x loss_x 1.0381 (1.3316) acc_x 68.7500 (65.7292) lr 1.2181e-03 eta 0:00:00
epoch [88/200] batch [5/66] time 0.421 (0.453) data 0.290 (0.322) loss_u loss_u 0.8784 (0.8935) acc_u 15.6250 (15.6250) lr 1.2181e-03 eta 0:00:27
epoch [88/200] batch [10/66] time 0.380 (0.450) data 0.248 (0.319) loss_u loss_u 0.8965 (0.9010) acc_u 12.5000 (14.3750) lr 1.2181e-03 eta 0:00:25
epoch [88/200] batch [15/66] time 0.382 (0.451) data 0.249 (0.320) loss_u loss_u 0.7847 (0.8942) acc_u 31.2500 (14.7917) lr 1.2181e-03 eta 0:00:23
epoch [88/200] batch [20/66] time 0.396 (0.453) data 0.264 (0.322) loss_u loss_u 0.8730 (0.8913) acc_u 12.5000 (14.8438) lr 1.2181e-03 eta 0:00:20
epoch [88/200] batch [25/66] time 0.524 (0.454) data 0.392 (0.323) loss_u loss_u 0.7993 (0.8829) acc_u 25.0000 (16.0000) lr 1.2181e-03 eta 0:00:18
epoch [88/200] batch [30/66] time 0.465 (0.458) data 0.331 (0.327) loss_u loss_u 0.8569 (0.8805) acc_u 18.7500 (16.1458) lr 1.2181e-03 eta 0:00:16
epoch [88/200] batch [35/66] time 0.460 (0.458) data 0.328 (0.327) loss_u loss_u 0.8755 (0.8816) acc_u 18.7500 (15.7143) lr 1.2181e-03 eta 0:00:14
epoch [88/200] batch [40/66] time 0.480 (0.461) data 0.348 (0.330) loss_u loss_u 0.8916 (0.8791) acc_u 9.3750 (16.1719) lr 1.2181e-03 eta 0:00:11
epoch [88/200] batch [45/66] time 0.368 (0.458) data 0.236 (0.327) loss_u loss_u 0.8701 (0.8786) acc_u 25.0000 (16.6667) lr 1.2181e-03 eta 0:00:09
epoch [88/200] batch [50/66] time 0.389 (0.458) data 0.259 (0.327) loss_u loss_u 0.8989 (0.8799) acc_u 12.5000 (16.4375) lr 1.2181e-03 eta 0:00:07
epoch [88/200] batch [55/66] time 0.463 (0.459) data 0.332 (0.327) loss_u loss_u 0.8599 (0.8800) acc_u 18.7500 (16.1932) lr 1.2181e-03 eta 0:00:05
epoch [88/200] batch [60/66] time 0.442 (0.456) data 0.311 (0.324) loss_u loss_u 0.9219 (0.8810) acc_u 9.3750 (15.8333) lr 1.2181e-03 eta 0:00:02
epoch [88/200] batch [65/66] time 0.426 (0.454) data 0.295 (0.323) loss_u loss_u 0.9551 (0.8824) acc_u 3.1250 (15.4327) lr 1.2181e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1623
confident_label rate tensor(0.3103, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 973
clean true:848
clean false:125
clean_rate:0.8715313463514902
noisy true:665
noisy false:1498
after delete: len(clean_dataset) 973
after delete: len(noisy_dataset) 2163
epoch [89/200] batch [5/30] time 0.458 (0.435) data 0.327 (0.305) loss_x loss_x 1.5820 (1.2618) acc_x 56.2500 (65.0000) lr 1.2028e-03 eta 0:00:10
epoch [89/200] batch [10/30] time 0.552 (0.463) data 0.421 (0.332) loss_x loss_x 1.1328 (1.4108) acc_x 75.0000 (62.1875) lr 1.2028e-03 eta 0:00:09
epoch [89/200] batch [15/30] time 0.446 (0.471) data 0.315 (0.341) loss_x loss_x 0.5938 (1.2807) acc_x 87.5000 (65.6250) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [20/30] time 0.413 (0.473) data 0.282 (0.342) loss_x loss_x 1.3506 (1.2618) acc_x 65.6250 (66.2500) lr 1.2028e-03 eta 0:00:04
epoch [89/200] batch [25/30] time 0.525 (0.465) data 0.394 (0.334) loss_x loss_x 0.7417 (1.2767) acc_x 84.3750 (66.3750) lr 1.2028e-03 eta 0:00:02
epoch [89/200] batch [30/30] time 0.377 (0.465) data 0.246 (0.335) loss_x loss_x 1.0254 (1.3182) acc_x 68.7500 (65.6250) lr 1.2028e-03 eta 0:00:00
epoch [89/200] batch [5/67] time 0.356 (0.454) data 0.224 (0.323) loss_u loss_u 0.8540 (0.8883) acc_u 15.6250 (12.5000) lr 1.2028e-03 eta 0:00:28
epoch [89/200] batch [10/67] time 0.515 (0.455) data 0.384 (0.324) loss_u loss_u 0.8076 (0.8859) acc_u 18.7500 (13.4375) lr 1.2028e-03 eta 0:00:25
epoch [89/200] batch [15/67] time 0.441 (0.455) data 0.310 (0.324) loss_u loss_u 0.8604 (0.8826) acc_u 15.6250 (13.5417) lr 1.2028e-03 eta 0:00:23
epoch [89/200] batch [20/67] time 0.507 (0.453) data 0.377 (0.322) loss_u loss_u 0.8867 (0.8815) acc_u 12.5000 (13.7500) lr 1.2028e-03 eta 0:00:21
epoch [89/200] batch [25/67] time 0.470 (0.454) data 0.340 (0.323) loss_u loss_u 0.8213 (0.8775) acc_u 28.1250 (14.1250) lr 1.2028e-03 eta 0:00:19
epoch [89/200] batch [30/67] time 0.379 (0.455) data 0.248 (0.324) loss_u loss_u 0.9131 (0.8720) acc_u 12.5000 (15.5208) lr 1.2028e-03 eta 0:00:16
epoch [89/200] batch [35/67] time 0.585 (0.457) data 0.452 (0.326) loss_u loss_u 0.9028 (0.8732) acc_u 12.5000 (15.8036) lr 1.2028e-03 eta 0:00:14
epoch [89/200] batch [40/67] time 0.408 (0.458) data 0.277 (0.327) loss_u loss_u 0.8911 (0.8728) acc_u 15.6250 (15.7031) lr 1.2028e-03 eta 0:00:12
epoch [89/200] batch [45/67] time 0.446 (0.454) data 0.315 (0.323) loss_u loss_u 0.9009 (0.8762) acc_u 12.5000 (15.1389) lr 1.2028e-03 eta 0:00:09
epoch [89/200] batch [50/67] time 0.365 (0.454) data 0.233 (0.323) loss_u loss_u 0.8447 (0.8743) acc_u 21.8750 (15.7500) lr 1.2028e-03 eta 0:00:07
epoch [89/200] batch [55/67] time 0.455 (0.454) data 0.324 (0.323) loss_u loss_u 0.9204 (0.8767) acc_u 3.1250 (15.4545) lr 1.2028e-03 eta 0:00:05
epoch [89/200] batch [60/67] time 0.452 (0.455) data 0.321 (0.324) loss_u loss_u 0.8896 (0.8766) acc_u 18.7500 (15.3646) lr 1.2028e-03 eta 0:00:03
epoch [89/200] batch [65/67] time 0.353 (0.460) data 0.221 (0.329) loss_u loss_u 0.9243 (0.8761) acc_u 9.3750 (15.2404) lr 1.2028e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1651
confident_label rate tensor(0.3099, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 972
clean true:850
clean false:122
clean_rate:0.8744855967078189
noisy true:635
noisy false:1529
after delete: len(clean_dataset) 972
after delete: len(noisy_dataset) 2164
epoch [90/200] batch [5/30] time 0.516 (0.478) data 0.386 (0.347) loss_x loss_x 0.8550 (0.8878) acc_x 75.0000 (76.8750) lr 1.1874e-03 eta 0:00:11
epoch [90/200] batch [10/30] time 0.490 (0.466) data 0.360 (0.336) loss_x loss_x 1.3350 (1.0601) acc_x 68.7500 (73.1250) lr 1.1874e-03 eta 0:00:09
epoch [90/200] batch [15/30] time 0.531 (0.462) data 0.400 (0.332) loss_x loss_x 1.1689 (1.1580) acc_x 68.7500 (70.8333) lr 1.1874e-03 eta 0:00:06
epoch [90/200] batch [20/30] time 0.454 (0.455) data 0.323 (0.325) loss_x loss_x 1.0801 (1.1524) acc_x 71.8750 (70.1562) lr 1.1874e-03 eta 0:00:04
epoch [90/200] batch [25/30] time 0.500 (0.467) data 0.369 (0.336) loss_x loss_x 0.8799 (1.1718) acc_x 75.0000 (69.2500) lr 1.1874e-03 eta 0:00:02
epoch [90/200] batch [30/30] time 0.429 (0.474) data 0.299 (0.343) loss_x loss_x 1.1387 (1.2364) acc_x 65.6250 (67.1875) lr 1.1874e-03 eta 0:00:00
epoch [90/200] batch [5/67] time 0.388 (0.468) data 0.257 (0.338) loss_u loss_u 0.8496 (0.8956) acc_u 15.6250 (12.5000) lr 1.1874e-03 eta 0:00:29
epoch [90/200] batch [10/67] time 0.420 (0.464) data 0.289 (0.334) loss_u loss_u 0.8672 (0.8657) acc_u 15.6250 (17.1875) lr 1.1874e-03 eta 0:00:26
epoch [90/200] batch [15/67] time 0.488 (0.460) data 0.356 (0.329) loss_u loss_u 0.8906 (0.8720) acc_u 9.3750 (15.8333) lr 1.1874e-03 eta 0:00:23
epoch [90/200] batch [20/67] time 0.462 (0.461) data 0.329 (0.331) loss_u loss_u 0.9224 (0.8695) acc_u 12.5000 (15.9375) lr 1.1874e-03 eta 0:00:21
epoch [90/200] batch [25/67] time 0.718 (0.463) data 0.586 (0.332) loss_u loss_u 0.8853 (0.8715) acc_u 15.6250 (15.6250) lr 1.1874e-03 eta 0:00:19
epoch [90/200] batch [30/67] time 0.487 (0.465) data 0.356 (0.334) loss_u loss_u 0.8887 (0.8742) acc_u 15.6250 (15.3125) lr 1.1874e-03 eta 0:00:17
epoch [90/200] batch [35/67] time 0.354 (0.464) data 0.223 (0.333) loss_u loss_u 0.7661 (0.8741) acc_u 31.2500 (15.2679) lr 1.1874e-03 eta 0:00:14
epoch [90/200] batch [40/67] time 0.492 (0.463) data 0.362 (0.332) loss_u loss_u 0.8320 (0.8713) acc_u 18.7500 (15.3906) lr 1.1874e-03 eta 0:00:12
epoch [90/200] batch [45/67] time 0.446 (0.463) data 0.315 (0.332) loss_u loss_u 0.9248 (0.8723) acc_u 6.2500 (15.3472) lr 1.1874e-03 eta 0:00:10
epoch [90/200] batch [50/67] time 0.487 (0.466) data 0.355 (0.335) loss_u loss_u 0.8638 (0.8732) acc_u 12.5000 (15.1875) lr 1.1874e-03 eta 0:00:07
epoch [90/200] batch [55/67] time 0.504 (0.465) data 0.372 (0.334) loss_u loss_u 0.8276 (0.8732) acc_u 25.0000 (15.1705) lr 1.1874e-03 eta 0:00:05
epoch [90/200] batch [60/67] time 0.490 (0.465) data 0.359 (0.334) loss_u loss_u 0.9077 (0.8767) acc_u 9.3750 (14.7396) lr 1.1874e-03 eta 0:00:03
epoch [90/200] batch [65/67] time 0.454 (0.470) data 0.325 (0.339) loss_u loss_u 0.8262 (0.8762) acc_u 25.0000 (14.9038) lr 1.1874e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1603
confident_label rate tensor(0.3154, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 989
clean true:869
clean false:120
clean_rate:0.8786653185035389
noisy true:664
noisy false:1483
after delete: len(clean_dataset) 989
after delete: len(noisy_dataset) 2147
epoch [91/200] batch [5/30] time 0.534 (0.455) data 0.404 (0.325) loss_x loss_x 1.1807 (1.2035) acc_x 65.6250 (69.3750) lr 1.1719e-03 eta 0:00:11
epoch [91/200] batch [10/30] time 0.409 (0.478) data 0.279 (0.348) loss_x loss_x 1.7080 (1.2628) acc_x 59.3750 (68.1250) lr 1.1719e-03 eta 0:00:09
epoch [91/200] batch [15/30] time 0.663 (0.479) data 0.533 (0.349) loss_x loss_x 0.5928 (1.2539) acc_x 84.3750 (68.3333) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [20/30] time 0.525 (0.479) data 0.395 (0.349) loss_x loss_x 1.7061 (1.2820) acc_x 56.2500 (67.3438) lr 1.1719e-03 eta 0:00:04
epoch [91/200] batch [25/30] time 0.428 (0.480) data 0.298 (0.349) loss_x loss_x 1.0967 (1.3239) acc_x 78.1250 (67.0000) lr 1.1719e-03 eta 0:00:02
epoch [91/200] batch [30/30] time 0.426 (0.478) data 0.295 (0.347) loss_x loss_x 1.2676 (1.3282) acc_x 65.6250 (66.5625) lr 1.1719e-03 eta 0:00:00
epoch [91/200] batch [5/67] time 0.457 (0.478) data 0.325 (0.348) loss_u loss_u 0.9424 (0.9055) acc_u 6.2500 (10.6250) lr 1.1719e-03 eta 0:00:29
epoch [91/200] batch [10/67] time 0.387 (0.476) data 0.256 (0.345) loss_u loss_u 0.9263 (0.9087) acc_u 9.3750 (11.2500) lr 1.1719e-03 eta 0:00:27
epoch [91/200] batch [15/67] time 0.474 (0.473) data 0.342 (0.343) loss_u loss_u 0.8916 (0.9020) acc_u 15.6250 (13.7500) lr 1.1719e-03 eta 0:00:24
epoch [91/200] batch [20/67] time 0.426 (0.468) data 0.295 (0.338) loss_u loss_u 0.8735 (0.8995) acc_u 18.7500 (13.5938) lr 1.1719e-03 eta 0:00:22
epoch [91/200] batch [25/67] time 0.367 (0.459) data 0.236 (0.329) loss_u loss_u 0.8965 (0.8956) acc_u 18.7500 (14.0000) lr 1.1719e-03 eta 0:00:19
epoch [91/200] batch [30/67] time 0.386 (0.457) data 0.254 (0.326) loss_u loss_u 0.8794 (0.8891) acc_u 15.6250 (14.5833) lr 1.1719e-03 eta 0:00:16
epoch [91/200] batch [35/67] time 0.474 (0.457) data 0.342 (0.326) loss_u loss_u 0.9434 (0.8889) acc_u 6.2500 (14.1964) lr 1.1719e-03 eta 0:00:14
epoch [91/200] batch [40/67] time 0.547 (0.461) data 0.416 (0.330) loss_u loss_u 0.8262 (0.8867) acc_u 28.1250 (14.6094) lr 1.1719e-03 eta 0:00:12
epoch [91/200] batch [45/67] time 0.395 (0.460) data 0.263 (0.330) loss_u loss_u 0.8677 (0.8856) acc_u 15.6250 (14.7222) lr 1.1719e-03 eta 0:00:10
epoch [91/200] batch [50/67] time 0.506 (0.460) data 0.373 (0.329) loss_u loss_u 0.9053 (0.8853) acc_u 12.5000 (14.6250) lr 1.1719e-03 eta 0:00:07
epoch [91/200] batch [55/67] time 0.491 (0.460) data 0.359 (0.329) loss_u loss_u 0.9106 (0.8869) acc_u 9.3750 (14.3182) lr 1.1719e-03 eta 0:00:05
epoch [91/200] batch [60/67] time 0.436 (0.461) data 0.305 (0.330) loss_u loss_u 0.9009 (0.8872) acc_u 12.5000 (14.3229) lr 1.1719e-03 eta 0:00:03
epoch [91/200] batch [65/67] time 0.375 (0.459) data 0.243 (0.327) loss_u loss_u 0.8716 (0.8863) acc_u 21.8750 (14.2788) lr 1.1719e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1605
confident_label rate tensor(0.3058, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 959
clean true:839
clean false:120
clean_rate:0.8748696558915537
noisy true:692
noisy false:1485
after delete: len(clean_dataset) 959
after delete: len(noisy_dataset) 2177
epoch [92/200] batch [5/29] time 0.473 (0.453) data 0.342 (0.323) loss_x loss_x 1.2617 (1.2458) acc_x 75.0000 (70.6250) lr 1.1564e-03 eta 0:00:10
epoch [92/200] batch [10/29] time 0.578 (0.448) data 0.447 (0.318) loss_x loss_x 0.9863 (1.1499) acc_x 84.3750 (72.5000) lr 1.1564e-03 eta 0:00:08
epoch [92/200] batch [15/29] time 0.529 (0.450) data 0.397 (0.319) loss_x loss_x 1.1992 (1.1858) acc_x 78.1250 (72.0833) lr 1.1564e-03 eta 0:00:06
epoch [92/200] batch [20/29] time 0.702 (0.463) data 0.571 (0.332) loss_x loss_x 1.3623 (1.1421) acc_x 56.2500 (72.1875) lr 1.1564e-03 eta 0:00:04
epoch [92/200] batch [25/29] time 0.469 (0.477) data 0.339 (0.347) loss_x loss_x 0.5908 (1.1381) acc_x 81.2500 (72.6250) lr 1.1564e-03 eta 0:00:01
epoch [92/200] batch [5/68] time 0.567 (0.488) data 0.435 (0.357) loss_u loss_u 0.8936 (0.8703) acc_u 18.7500 (18.1250) lr 1.1564e-03 eta 0:00:30
epoch [92/200] batch [10/68] time 0.469 (0.489) data 0.338 (0.358) loss_u loss_u 0.8760 (0.8656) acc_u 15.6250 (18.1250) lr 1.1564e-03 eta 0:00:28
epoch [92/200] batch [15/68] time 0.444 (0.483) data 0.314 (0.352) loss_u loss_u 0.8774 (0.8661) acc_u 25.0000 (17.9167) lr 1.1564e-03 eta 0:00:25
epoch [92/200] batch [20/68] time 0.472 (0.478) data 0.341 (0.347) loss_u loss_u 0.9238 (0.8725) acc_u 12.5000 (16.8750) lr 1.1564e-03 eta 0:00:22
epoch [92/200] batch [25/68] time 0.459 (0.482) data 0.328 (0.351) loss_u loss_u 0.9316 (0.8652) acc_u 9.3750 (17.8750) lr 1.1564e-03 eta 0:00:20
epoch [92/200] batch [30/68] time 0.475 (0.479) data 0.344 (0.348) loss_u loss_u 0.8257 (0.8656) acc_u 21.8750 (17.8125) lr 1.1564e-03 eta 0:00:18
epoch [92/200] batch [35/68] time 0.519 (0.479) data 0.387 (0.348) loss_u loss_u 0.8115 (0.8637) acc_u 28.1250 (18.2143) lr 1.1564e-03 eta 0:00:15
epoch [92/200] batch [40/68] time 0.358 (0.482) data 0.227 (0.351) loss_u loss_u 0.8892 (0.8662) acc_u 9.3750 (17.7344) lr 1.1564e-03 eta 0:00:13
epoch [92/200] batch [45/68] time 0.427 (0.481) data 0.296 (0.350) loss_u loss_u 0.8599 (0.8664) acc_u 18.7500 (17.7083) lr 1.1564e-03 eta 0:00:11
epoch [92/200] batch [50/68] time 0.465 (0.480) data 0.335 (0.349) loss_u loss_u 0.8623 (0.8708) acc_u 18.7500 (17.2500) lr 1.1564e-03 eta 0:00:08
epoch [92/200] batch [55/68] time 0.494 (0.478) data 0.363 (0.347) loss_u loss_u 0.8955 (0.8714) acc_u 15.6250 (17.0455) lr 1.1564e-03 eta 0:00:06
epoch [92/200] batch [60/68] time 0.486 (0.478) data 0.355 (0.347) loss_u loss_u 0.8867 (0.8714) acc_u 18.7500 (17.0312) lr 1.1564e-03 eta 0:00:03
epoch [92/200] batch [65/68] time 0.352 (0.473) data 0.220 (0.342) loss_u loss_u 0.9360 (0.8737) acc_u 6.2500 (16.6827) lr 1.1564e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1615
confident_label rate tensor(0.3202, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1004
clean true:876
clean false:128
clean_rate:0.8725099601593626
noisy true:645
noisy false:1487
after delete: len(clean_dataset) 1004
after delete: len(noisy_dataset) 2132
epoch [93/200] batch [5/31] time 0.416 (0.479) data 0.285 (0.348) loss_x loss_x 1.8955 (1.5039) acc_x 43.7500 (61.2500) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [10/31] time 0.512 (0.488) data 0.381 (0.358) loss_x loss_x 0.8833 (1.2832) acc_x 75.0000 (68.7500) lr 1.1409e-03 eta 0:00:10
epoch [93/200] batch [15/31] time 0.418 (0.478) data 0.288 (0.348) loss_x loss_x 0.9902 (1.2274) acc_x 68.7500 (68.7500) lr 1.1409e-03 eta 0:00:07
epoch [93/200] batch [20/31] time 0.447 (0.472) data 0.316 (0.342) loss_x loss_x 1.5488 (1.2475) acc_x 65.6250 (68.4375) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [25/31] time 0.499 (0.467) data 0.369 (0.336) loss_x loss_x 1.4111 (1.2429) acc_x 62.5000 (69.0000) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [30/31] time 0.615 (0.470) data 0.483 (0.339) loss_x loss_x 1.0371 (1.2597) acc_x 65.6250 (68.1250) lr 1.1409e-03 eta 0:00:00
epoch [93/200] batch [5/66] time 0.432 (0.468) data 0.298 (0.337) loss_u loss_u 0.8794 (0.8591) acc_u 12.5000 (17.5000) lr 1.1409e-03 eta 0:00:28
epoch [93/200] batch [10/66] time 0.447 (0.472) data 0.317 (0.341) loss_u loss_u 0.8936 (0.8720) acc_u 9.3750 (15.0000) lr 1.1409e-03 eta 0:00:26
epoch [93/200] batch [15/66] time 0.490 (0.472) data 0.360 (0.341) loss_u loss_u 0.9150 (0.8768) acc_u 9.3750 (14.5833) lr 1.1409e-03 eta 0:00:24
epoch [93/200] batch [20/66] time 0.526 (0.469) data 0.395 (0.338) loss_u loss_u 0.9287 (0.8743) acc_u 9.3750 (14.8438) lr 1.1409e-03 eta 0:00:21
epoch [93/200] batch [25/66] time 0.386 (0.470) data 0.254 (0.339) loss_u loss_u 0.8369 (0.8717) acc_u 18.7500 (15.6250) lr 1.1409e-03 eta 0:00:19
epoch [93/200] batch [30/66] time 0.431 (0.468) data 0.300 (0.337) loss_u loss_u 0.9419 (0.8743) acc_u 12.5000 (15.5208) lr 1.1409e-03 eta 0:00:16
epoch [93/200] batch [35/66] time 0.555 (0.467) data 0.425 (0.336) loss_u loss_u 0.8604 (0.8702) acc_u 21.8750 (16.5179) lr 1.1409e-03 eta 0:00:14
epoch [93/200] batch [40/66] time 0.369 (0.462) data 0.238 (0.332) loss_u loss_u 0.9399 (0.8783) acc_u 9.3750 (15.1562) lr 1.1409e-03 eta 0:00:12
epoch [93/200] batch [45/66] time 0.333 (0.460) data 0.201 (0.329) loss_u loss_u 0.8867 (0.8800) acc_u 12.5000 (14.8611) lr 1.1409e-03 eta 0:00:09
epoch [93/200] batch [50/66] time 0.471 (0.460) data 0.339 (0.329) loss_u loss_u 0.8164 (0.8791) acc_u 21.8750 (15.0000) lr 1.1409e-03 eta 0:00:07
epoch [93/200] batch [55/66] time 0.516 (0.469) data 0.384 (0.338) loss_u loss_u 0.9014 (0.8798) acc_u 12.5000 (15.1705) lr 1.1409e-03 eta 0:00:05
epoch [93/200] batch [60/66] time 0.329 (0.468) data 0.199 (0.337) loss_u loss_u 0.9116 (0.8825) acc_u 12.5000 (14.7917) lr 1.1409e-03 eta 0:00:02
epoch [93/200] batch [65/66] time 0.435 (0.471) data 0.303 (0.340) loss_u loss_u 0.9160 (0.8831) acc_u 9.3750 (14.6154) lr 1.1409e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1577
confident_label rate tensor(0.3179, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 997
clean true:885
clean false:112
clean_rate:0.8876629889669007
noisy true:674
noisy false:1465
after delete: len(clean_dataset) 997
after delete: len(noisy_dataset) 2139
epoch [94/200] batch [5/31] time 0.447 (0.472) data 0.317 (0.342) loss_x loss_x 1.6328 (1.3516) acc_x 53.1250 (66.2500) lr 1.1253e-03 eta 0:00:12
epoch [94/200] batch [10/31] time 0.776 (0.506) data 0.646 (0.375) loss_x loss_x 0.7393 (1.2724) acc_x 68.7500 (67.8125) lr 1.1253e-03 eta 0:00:10
epoch [94/200] batch [15/31] time 0.473 (0.489) data 0.343 (0.358) loss_x loss_x 1.4834 (1.3613) acc_x 59.3750 (65.6250) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [20/31] time 0.378 (0.486) data 0.247 (0.355) loss_x loss_x 1.2783 (1.3383) acc_x 78.1250 (66.7188) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [25/31] time 0.589 (0.489) data 0.457 (0.358) loss_x loss_x 1.0273 (1.2855) acc_x 71.8750 (67.6250) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [30/31] time 0.436 (0.484) data 0.306 (0.354) loss_x loss_x 1.7646 (1.2972) acc_x 53.1250 (66.4583) lr 1.1253e-03 eta 0:00:00
epoch [94/200] batch [5/66] time 0.489 (0.484) data 0.359 (0.353) loss_u loss_u 0.9434 (0.9042) acc_u 6.2500 (10.6250) lr 1.1253e-03 eta 0:00:29
epoch [94/200] batch [10/66] time 0.490 (0.485) data 0.359 (0.354) loss_u loss_u 0.8774 (0.8974) acc_u 15.6250 (11.5625) lr 1.1253e-03 eta 0:00:27
epoch [94/200] batch [15/66] time 0.421 (0.479) data 0.288 (0.348) loss_u loss_u 0.8579 (0.8896) acc_u 25.0000 (13.9583) lr 1.1253e-03 eta 0:00:24
epoch [94/200] batch [20/66] time 0.610 (0.484) data 0.478 (0.353) loss_u loss_u 0.9199 (0.8860) acc_u 12.5000 (14.3750) lr 1.1253e-03 eta 0:00:22
epoch [94/200] batch [25/66] time 0.314 (0.479) data 0.183 (0.348) loss_u loss_u 0.9189 (0.8872) acc_u 9.3750 (14.2500) lr 1.1253e-03 eta 0:00:19
epoch [94/200] batch [30/66] time 0.443 (0.475) data 0.312 (0.343) loss_u loss_u 0.8516 (0.8885) acc_u 18.7500 (13.9583) lr 1.1253e-03 eta 0:00:17
epoch [94/200] batch [35/66] time 0.377 (0.475) data 0.244 (0.344) loss_u loss_u 0.8677 (0.8876) acc_u 18.7500 (14.0179) lr 1.1253e-03 eta 0:00:14
epoch [94/200] batch [40/66] time 0.431 (0.474) data 0.299 (0.343) loss_u loss_u 0.8594 (0.8877) acc_u 15.6250 (13.6719) lr 1.1253e-03 eta 0:00:12
epoch [94/200] batch [45/66] time 0.667 (0.474) data 0.535 (0.342) loss_u loss_u 0.8076 (0.8861) acc_u 18.7500 (13.6806) lr 1.1253e-03 eta 0:00:09
epoch [94/200] batch [50/66] time 0.484 (0.471) data 0.353 (0.340) loss_u loss_u 0.8608 (0.8844) acc_u 21.8750 (14.0625) lr 1.1253e-03 eta 0:00:07
epoch [94/200] batch [55/66] time 0.520 (0.470) data 0.389 (0.339) loss_u loss_u 0.9414 (0.8869) acc_u 6.2500 (13.5227) lr 1.1253e-03 eta 0:00:05
epoch [94/200] batch [60/66] time 0.415 (0.467) data 0.283 (0.336) loss_u loss_u 0.8569 (0.8849) acc_u 15.6250 (13.8021) lr 1.1253e-03 eta 0:00:02
epoch [94/200] batch [65/66] time 0.533 (0.467) data 0.402 (0.336) loss_u loss_u 0.8486 (0.8826) acc_u 18.7500 (13.9904) lr 1.1253e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1635
confident_label rate tensor(0.3135, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 983
clean true:862
clean false:121
clean_rate:0.8769074262461851
noisy true:639
noisy false:1514
after delete: len(clean_dataset) 983
after delete: len(noisy_dataset) 2153
epoch [95/200] batch [5/30] time 0.462 (0.511) data 0.332 (0.381) loss_x loss_x 1.1904 (1.1856) acc_x 65.6250 (69.3750) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [10/30] time 0.448 (0.494) data 0.318 (0.363) loss_x loss_x 1.5293 (1.1878) acc_x 56.2500 (70.0000) lr 1.1097e-03 eta 0:00:09
epoch [95/200] batch [15/30] time 0.561 (0.497) data 0.431 (0.366) loss_x loss_x 1.4902 (1.2144) acc_x 62.5000 (68.9583) lr 1.1097e-03 eta 0:00:07
epoch [95/200] batch [20/30] time 0.655 (0.500) data 0.523 (0.369) loss_x loss_x 1.2285 (1.2343) acc_x 68.7500 (69.2188) lr 1.1097e-03 eta 0:00:04
epoch [95/200] batch [25/30] time 0.440 (0.504) data 0.309 (0.373) loss_x loss_x 1.4668 (1.2275) acc_x 68.7500 (68.8750) lr 1.1097e-03 eta 0:00:02
epoch [95/200] batch [30/30] time 0.441 (0.489) data 0.310 (0.358) loss_x loss_x 1.0078 (1.2226) acc_x 71.8750 (69.5833) lr 1.1097e-03 eta 0:00:00
epoch [95/200] batch [5/67] time 0.531 (0.487) data 0.399 (0.356) loss_u loss_u 0.8936 (0.8725) acc_u 15.6250 (13.7500) lr 1.1097e-03 eta 0:00:30
epoch [95/200] batch [10/67] time 0.551 (0.487) data 0.421 (0.356) loss_u loss_u 0.8750 (0.8805) acc_u 18.7500 (14.3750) lr 1.1097e-03 eta 0:00:27
epoch [95/200] batch [15/67] time 0.474 (0.481) data 0.343 (0.351) loss_u loss_u 0.8276 (0.8847) acc_u 18.7500 (13.7500) lr 1.1097e-03 eta 0:00:25
epoch [95/200] batch [20/67] time 0.500 (0.483) data 0.370 (0.353) loss_u loss_u 0.9272 (0.8844) acc_u 12.5000 (14.5312) lr 1.1097e-03 eta 0:00:22
epoch [95/200] batch [25/67] time 0.389 (0.479) data 0.258 (0.349) loss_u loss_u 0.9121 (0.8859) acc_u 9.3750 (14.7500) lr 1.1097e-03 eta 0:00:20
epoch [95/200] batch [30/67] time 0.633 (0.483) data 0.503 (0.352) loss_u loss_u 0.8032 (0.8771) acc_u 21.8750 (15.4167) lr 1.1097e-03 eta 0:00:17
epoch [95/200] batch [35/67] time 0.382 (0.477) data 0.251 (0.346) loss_u loss_u 0.9062 (0.8769) acc_u 9.3750 (15.3571) lr 1.1097e-03 eta 0:00:15
epoch [95/200] batch [40/67] time 0.729 (0.476) data 0.599 (0.345) loss_u loss_u 0.8394 (0.8770) acc_u 18.7500 (15.2344) lr 1.1097e-03 eta 0:00:12
epoch [95/200] batch [45/67] time 0.397 (0.474) data 0.266 (0.343) loss_u loss_u 0.8032 (0.8741) acc_u 28.1250 (15.8333) lr 1.1097e-03 eta 0:00:10
epoch [95/200] batch [50/67] time 0.350 (0.476) data 0.217 (0.345) loss_u loss_u 0.8584 (0.8768) acc_u 12.5000 (15.1875) lr 1.1097e-03 eta 0:00:08
epoch [95/200] batch [55/67] time 0.431 (0.473) data 0.299 (0.342) loss_u loss_u 0.8516 (0.8752) acc_u 21.8750 (15.3409) lr 1.1097e-03 eta 0:00:05
epoch [95/200] batch [60/67] time 0.388 (0.471) data 0.258 (0.340) loss_u loss_u 0.8921 (0.8752) acc_u 12.5000 (15.4688) lr 1.1097e-03 eta 0:00:03
epoch [95/200] batch [65/67] time 0.551 (0.470) data 0.418 (0.339) loss_u loss_u 0.8638 (0.8736) acc_u 18.7500 (15.8173) lr 1.1097e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1635
confident_label rate tensor(0.3036, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 952
clean true:833
clean false:119
clean_rate:0.875
noisy true:668
noisy false:1516
after delete: len(clean_dataset) 952
after delete: len(noisy_dataset) 2184
epoch [96/200] batch [5/29] time 0.445 (0.450) data 0.315 (0.319) loss_x loss_x 0.8540 (1.1946) acc_x 78.1250 (68.7500) lr 1.0941e-03 eta 0:00:10
epoch [96/200] batch [10/29] time 0.468 (0.439) data 0.338 (0.309) loss_x loss_x 0.8203 (1.2651) acc_x 78.1250 (67.8125) lr 1.0941e-03 eta 0:00:08
epoch [96/200] batch [15/29] time 0.455 (0.450) data 0.325 (0.319) loss_x loss_x 1.8926 (1.3030) acc_x 56.2500 (67.0833) lr 1.0941e-03 eta 0:00:06
epoch [96/200] batch [20/29] time 0.434 (0.475) data 0.303 (0.344) loss_x loss_x 1.2900 (1.3442) acc_x 68.7500 (66.8750) lr 1.0941e-03 eta 0:00:04
epoch [96/200] batch [25/29] time 0.487 (0.472) data 0.356 (0.342) loss_x loss_x 1.3818 (1.3321) acc_x 59.3750 (66.2500) lr 1.0941e-03 eta 0:00:01
epoch [96/200] batch [5/68] time 0.493 (0.478) data 0.360 (0.348) loss_u loss_u 0.8774 (0.8613) acc_u 12.5000 (18.7500) lr 1.0941e-03 eta 0:00:30
epoch [96/200] batch [10/68] time 0.457 (0.476) data 0.326 (0.345) loss_u loss_u 0.8809 (0.8588) acc_u 12.5000 (17.1875) lr 1.0941e-03 eta 0:00:27
epoch [96/200] batch [15/68] time 0.428 (0.473) data 0.297 (0.342) loss_u loss_u 0.9111 (0.8549) acc_u 15.6250 (18.1250) lr 1.0941e-03 eta 0:00:25
epoch [96/200] batch [20/68] time 0.343 (0.464) data 0.211 (0.333) loss_u loss_u 0.8618 (0.8510) acc_u 15.6250 (18.5938) lr 1.0941e-03 eta 0:00:22
epoch [96/200] batch [25/68] time 0.408 (0.463) data 0.273 (0.332) loss_u loss_u 0.9038 (0.8606) acc_u 12.5000 (17.5000) lr 1.0941e-03 eta 0:00:19
epoch [96/200] batch [30/68] time 0.576 (0.466) data 0.445 (0.334) loss_u loss_u 0.8877 (0.8641) acc_u 12.5000 (16.9792) lr 1.0941e-03 eta 0:00:17
epoch [96/200] batch [35/68] time 0.432 (0.463) data 0.301 (0.332) loss_u loss_u 0.9048 (0.8657) acc_u 9.3750 (16.8750) lr 1.0941e-03 eta 0:00:15
epoch [96/200] batch [40/68] time 0.489 (0.466) data 0.358 (0.335) loss_u loss_u 0.8687 (0.8706) acc_u 15.6250 (16.0938) lr 1.0941e-03 eta 0:00:13
epoch [96/200] batch [45/68] time 0.405 (0.465) data 0.275 (0.334) loss_u loss_u 0.8433 (0.8646) acc_u 15.6250 (16.8056) lr 1.0941e-03 eta 0:00:10
epoch [96/200] batch [50/68] time 0.333 (0.461) data 0.202 (0.330) loss_u loss_u 0.9229 (0.8669) acc_u 9.3750 (16.5625) lr 1.0941e-03 eta 0:00:08
epoch [96/200] batch [55/68] time 0.393 (0.459) data 0.262 (0.328) loss_u loss_u 0.9160 (0.8664) acc_u 9.3750 (16.7614) lr 1.0941e-03 eta 0:00:05
epoch [96/200] batch [60/68] time 0.402 (0.461) data 0.270 (0.330) loss_u loss_u 0.8145 (0.8655) acc_u 25.0000 (16.7708) lr 1.0941e-03 eta 0:00:03
epoch [96/200] batch [65/68] time 0.451 (0.461) data 0.321 (0.330) loss_u loss_u 0.9023 (0.8664) acc_u 9.3750 (16.6827) lr 1.0941e-03 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1606
confident_label rate tensor(0.3208, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1006
clean true:884
clean false:122
clean_rate:0.878727634194831
noisy true:646
noisy false:1484
after delete: len(clean_dataset) 1006
after delete: len(noisy_dataset) 2130
epoch [97/200] batch [5/31] time 0.505 (0.447) data 0.374 (0.316) loss_x loss_x 1.3750 (1.2713) acc_x 59.3750 (67.5000) lr 1.0785e-03 eta 0:00:11
epoch [97/200] batch [10/31] time 0.524 (0.447) data 0.393 (0.316) loss_x loss_x 1.3359 (1.2682) acc_x 65.6250 (66.2500) lr 1.0785e-03 eta 0:00:09
epoch [97/200] batch [15/31] time 0.473 (0.446) data 0.342 (0.316) loss_x loss_x 1.7686 (1.2986) acc_x 56.2500 (66.2500) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [20/31] time 0.481 (0.450) data 0.349 (0.319) loss_x loss_x 1.6387 (1.2654) acc_x 59.3750 (67.5000) lr 1.0785e-03 eta 0:00:04
epoch [97/200] batch [25/31] time 0.410 (0.449) data 0.279 (0.319) loss_x loss_x 0.8013 (1.2649) acc_x 78.1250 (67.2500) lr 1.0785e-03 eta 0:00:02
epoch [97/200] batch [30/31] time 0.597 (0.453) data 0.465 (0.322) loss_x loss_x 1.6328 (1.2958) acc_x 62.5000 (67.1875) lr 1.0785e-03 eta 0:00:00
epoch [97/200] batch [5/66] time 0.496 (0.457) data 0.361 (0.326) loss_u loss_u 0.8853 (0.8711) acc_u 9.3750 (13.7500) lr 1.0785e-03 eta 0:00:27
epoch [97/200] batch [10/66] time 0.442 (0.455) data 0.310 (0.323) loss_u loss_u 0.8555 (0.8718) acc_u 21.8750 (15.0000) lr 1.0785e-03 eta 0:00:25
epoch [97/200] batch [15/66] time 0.528 (0.454) data 0.396 (0.322) loss_u loss_u 0.8799 (0.8817) acc_u 12.5000 (13.5417) lr 1.0785e-03 eta 0:00:23
epoch [97/200] batch [20/66] time 0.417 (0.452) data 0.285 (0.320) loss_u loss_u 0.8638 (0.8817) acc_u 18.7500 (13.9062) lr 1.0785e-03 eta 0:00:20
epoch [97/200] batch [25/66] time 0.745 (0.456) data 0.612 (0.325) loss_u loss_u 0.9224 (0.8843) acc_u 12.5000 (13.7500) lr 1.0785e-03 eta 0:00:18
epoch [97/200] batch [30/66] time 0.462 (0.459) data 0.330 (0.328) loss_u loss_u 0.8047 (0.8759) acc_u 28.1250 (14.7917) lr 1.0785e-03 eta 0:00:16
epoch [97/200] batch [35/66] time 0.620 (0.461) data 0.489 (0.330) loss_u loss_u 0.8398 (0.8736) acc_u 18.7500 (15.1786) lr 1.0785e-03 eta 0:00:14
epoch [97/200] batch [40/66] time 0.495 (0.465) data 0.363 (0.334) loss_u loss_u 0.8364 (0.8758) acc_u 18.7500 (14.7656) lr 1.0785e-03 eta 0:00:12
epoch [97/200] batch [45/66] time 0.382 (0.464) data 0.250 (0.332) loss_u loss_u 0.7925 (0.8744) acc_u 25.0000 (14.7917) lr 1.0785e-03 eta 0:00:09
epoch [97/200] batch [50/66] time 0.513 (0.465) data 0.381 (0.333) loss_u loss_u 0.8945 (0.8748) acc_u 15.6250 (15.1250) lr 1.0785e-03 eta 0:00:07
epoch [97/200] batch [55/66] time 0.510 (0.468) data 0.379 (0.336) loss_u loss_u 0.8438 (0.8720) acc_u 15.6250 (15.5114) lr 1.0785e-03 eta 0:00:05
epoch [97/200] batch [60/66] time 0.670 (0.468) data 0.539 (0.336) loss_u loss_u 0.8799 (0.8735) acc_u 15.6250 (15.3646) lr 1.0785e-03 eta 0:00:02
epoch [97/200] batch [65/66] time 0.424 (0.471) data 0.293 (0.339) loss_u loss_u 0.9565 (0.8772) acc_u 3.1250 (14.8558) lr 1.0785e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1575
confident_label rate tensor(0.3182, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 998
clean true:882
clean false:116
clean_rate:0.8837675350701403
noisy true:679
noisy false:1459
after delete: len(clean_dataset) 998
after delete: len(noisy_dataset) 2138
epoch [98/200] batch [5/31] time 0.575 (0.464) data 0.444 (0.333) loss_x loss_x 1.2373 (1.3268) acc_x 62.5000 (68.1250) lr 1.0628e-03 eta 0:00:12
epoch [98/200] batch [10/31] time 0.553 (0.468) data 0.422 (0.337) loss_x loss_x 1.3350 (1.2940) acc_x 65.6250 (68.4375) lr 1.0628e-03 eta 0:00:09
epoch [98/200] batch [15/31] time 0.527 (0.480) data 0.396 (0.349) loss_x loss_x 0.7896 (1.3189) acc_x 81.2500 (67.7083) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [20/31] time 0.537 (0.479) data 0.407 (0.348) loss_x loss_x 1.3633 (1.3283) acc_x 68.7500 (67.5000) lr 1.0628e-03 eta 0:00:05
epoch [98/200] batch [25/31] time 0.526 (0.498) data 0.394 (0.366) loss_x loss_x 1.1357 (1.2983) acc_x 68.7500 (68.1250) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [30/31] time 0.481 (0.487) data 0.350 (0.356) loss_x loss_x 1.0195 (1.2998) acc_x 62.5000 (66.9792) lr 1.0628e-03 eta 0:00:00
epoch [98/200] batch [5/66] time 0.471 (0.482) data 0.339 (0.351) loss_u loss_u 0.8838 (0.8876) acc_u 15.6250 (13.1250) lr 1.0628e-03 eta 0:00:29
epoch [98/200] batch [10/66] time 0.493 (0.478) data 0.362 (0.347) loss_u loss_u 0.8286 (0.8777) acc_u 18.7500 (14.0625) lr 1.0628e-03 eta 0:00:26
epoch [98/200] batch [15/66] time 0.413 (0.471) data 0.281 (0.340) loss_u loss_u 0.9224 (0.8871) acc_u 9.3750 (12.7083) lr 1.0628e-03 eta 0:00:24
epoch [98/200] batch [20/66] time 0.391 (0.475) data 0.259 (0.344) loss_u loss_u 0.8799 (0.8838) acc_u 21.8750 (14.2188) lr 1.0628e-03 eta 0:00:21
epoch [98/200] batch [25/66] time 0.637 (0.475) data 0.507 (0.344) loss_u loss_u 0.8857 (0.8831) acc_u 12.5000 (14.2500) lr 1.0628e-03 eta 0:00:19
epoch [98/200] batch [30/66] time 0.402 (0.472) data 0.270 (0.341) loss_u loss_u 0.8853 (0.8840) acc_u 12.5000 (14.3750) lr 1.0628e-03 eta 0:00:16
epoch [98/200] batch [35/66] time 0.448 (0.472) data 0.317 (0.340) loss_u loss_u 0.8760 (0.8825) acc_u 15.6250 (14.5536) lr 1.0628e-03 eta 0:00:14
epoch [98/200] batch [40/66] time 0.406 (0.471) data 0.275 (0.340) loss_u loss_u 0.8574 (0.8853) acc_u 21.8750 (14.3750) lr 1.0628e-03 eta 0:00:12
epoch [98/200] batch [45/66] time 0.447 (0.468) data 0.317 (0.336) loss_u loss_u 0.9004 (0.8862) acc_u 12.5000 (14.3056) lr 1.0628e-03 eta 0:00:09
epoch [98/200] batch [50/66] time 0.417 (0.465) data 0.286 (0.334) loss_u loss_u 0.8770 (0.8835) acc_u 12.5000 (14.6875) lr 1.0628e-03 eta 0:00:07
epoch [98/200] batch [55/66] time 0.474 (0.464) data 0.343 (0.332) loss_u loss_u 0.8384 (0.8815) acc_u 21.8750 (14.8864) lr 1.0628e-03 eta 0:00:05
epoch [98/200] batch [60/66] time 0.493 (0.463) data 0.361 (0.331) loss_u loss_u 0.8643 (0.8801) acc_u 21.8750 (15.1562) lr 1.0628e-03 eta 0:00:02
epoch [98/200] batch [65/66] time 0.446 (0.462) data 0.313 (0.330) loss_u loss_u 0.8413 (0.8816) acc_u 15.6250 (14.8077) lr 1.0628e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1640
confident_label rate tensor(0.3084, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 967
clean true:851
clean false:116
clean_rate:0.8800413650465356
noisy true:645
noisy false:1524
after delete: len(clean_dataset) 967
after delete: len(noisy_dataset) 2169
epoch [99/200] batch [5/30] time 0.523 (0.493) data 0.393 (0.362) loss_x loss_x 1.1855 (1.2327) acc_x 65.6250 (67.5000) lr 1.0471e-03 eta 0:00:12
epoch [99/200] batch [10/30] time 0.422 (0.460) data 0.291 (0.329) loss_x loss_x 1.2773 (1.2027) acc_x 68.7500 (69.3750) lr 1.0471e-03 eta 0:00:09
epoch [99/200] batch [15/30] time 0.519 (0.460) data 0.388 (0.329) loss_x loss_x 1.8223 (1.2361) acc_x 59.3750 (67.7083) lr 1.0471e-03 eta 0:00:06
epoch [99/200] batch [20/30] time 0.559 (0.482) data 0.428 (0.351) loss_x loss_x 1.0801 (1.2217) acc_x 75.0000 (69.5312) lr 1.0471e-03 eta 0:00:04
epoch [99/200] batch [25/30] time 0.735 (0.503) data 0.604 (0.372) loss_x loss_x 1.3721 (1.2684) acc_x 65.6250 (68.3750) lr 1.0471e-03 eta 0:00:02
epoch [99/200] batch [30/30] time 0.561 (0.507) data 0.430 (0.376) loss_x loss_x 1.0469 (1.2603) acc_x 71.8750 (68.5417) lr 1.0471e-03 eta 0:00:00
epoch [99/200] batch [5/67] time 0.618 (0.517) data 0.484 (0.386) loss_u loss_u 0.8545 (0.8640) acc_u 12.5000 (14.3750) lr 1.0471e-03 eta 0:00:32
epoch [99/200] batch [10/67] time 0.565 (0.525) data 0.433 (0.394) loss_u loss_u 0.8228 (0.8789) acc_u 18.7500 (13.1250) lr 1.0471e-03 eta 0:00:29
epoch [99/200] batch [15/67] time 0.384 (0.518) data 0.252 (0.386) loss_u loss_u 0.8374 (0.8716) acc_u 21.8750 (14.7917) lr 1.0471e-03 eta 0:00:26
epoch [99/200] batch [20/67] time 0.427 (0.510) data 0.296 (0.379) loss_u loss_u 0.8516 (0.8684) acc_u 25.0000 (15.4688) lr 1.0471e-03 eta 0:00:23
epoch [99/200] batch [25/67] time 0.357 (0.502) data 0.225 (0.370) loss_u loss_u 0.8818 (0.8777) acc_u 15.6250 (14.2500) lr 1.0471e-03 eta 0:00:21
epoch [99/200] batch [30/67] time 0.447 (0.496) data 0.316 (0.365) loss_u loss_u 0.8936 (0.8785) acc_u 9.3750 (14.3750) lr 1.0471e-03 eta 0:00:18
epoch [99/200] batch [35/67] time 0.468 (0.498) data 0.338 (0.367) loss_u loss_u 0.8511 (0.8775) acc_u 18.7500 (14.6429) lr 1.0471e-03 eta 0:00:15
epoch [99/200] batch [40/67] time 0.390 (0.492) data 0.260 (0.361) loss_u loss_u 0.8608 (0.8769) acc_u 12.5000 (14.6875) lr 1.0471e-03 eta 0:00:13
epoch [99/200] batch [45/67] time 0.621 (0.495) data 0.486 (0.364) loss_u loss_u 0.8828 (0.8772) acc_u 15.6250 (14.7917) lr 1.0471e-03 eta 0:00:10
epoch [99/200] batch [50/67] time 0.430 (0.494) data 0.299 (0.362) loss_u loss_u 0.9229 (0.8773) acc_u 9.3750 (14.8750) lr 1.0471e-03 eta 0:00:08
epoch [99/200] batch [55/67] time 0.550 (0.492) data 0.419 (0.361) loss_u loss_u 0.9341 (0.8760) acc_u 6.2500 (15.0000) lr 1.0471e-03 eta 0:00:05
epoch [99/200] batch [60/67] time 0.407 (0.487) data 0.276 (0.355) loss_u loss_u 0.9072 (0.8775) acc_u 9.3750 (14.8958) lr 1.0471e-03 eta 0:00:03
epoch [99/200] batch [65/67] time 0.392 (0.486) data 0.260 (0.354) loss_u loss_u 0.9058 (0.8767) acc_u 9.3750 (15.0000) lr 1.0471e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1608
confident_label rate tensor(0.3112, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 976
clean true:864
clean false:112
clean_rate:0.8852459016393442
noisy true:664
noisy false:1496
all clean rate:  [0.853448275862069, 0.9155206286836935, 0.9045584045584045, 0.9189560439560439, 0.9024725274725275, 0.919631093544137, 0.9104683195592287, 0.9149797570850202, 0.9084880636604774, 0.9320261437908497, 0.905440414507772, 0.8975356679636836, 0.906856403622251, 0.9081632653061225, 0.9112801013941698, 0.9060568603213844, 0.8993630573248408, 0.9132124352331606, 0.9063625450180072, 0.8940731399747793, 0.8936430317848411, 0.9087591240875912, 0.8963486454652533, 0.9054726368159204, 0.8962605548854041, 0.8883495145631068, 0.8924731182795699, 0.8919567827130852, 0.9030732860520094, 0.8992974238875878, 0.8987341772151899, 0.9007009345794392, 0.8919540229885058, 0.9030732860520094, 0.9045005488474204, 0.9005524861878453, 0.8808172531214529, 0.875995449374289, 0.891156462585034, 0.8928571428571429, 0.8843537414965986, 0.8971238938053098, 0.8895089285714286, 0.8933782267115601, 0.8785249457700651, 0.9052511415525114, 0.8796498905908097, 0.8797814207650273, 0.8986636971046771, 0.8874856486796785, 0.8867102396514162, 0.8722707423580786, 0.8868335146898803, 0.8924259055982436, 0.8859060402684564, 0.884698275862069, 0.8785557986870897, 0.8837719298245614, 0.8897216274089935, 0.8819810326659642, 0.8817204301075269, 0.8893709327548807, 0.9006342494714588, 0.8859934853420195, 0.8838709677419355, 0.8796592119275826, 0.8707264957264957, 0.8734439834024896, 0.8725099601593626, 0.8861283643892339, 0.8974358974358975, 0.8702127659574468, 0.8925081433224755, 0.8923719958202717, 0.8806122448979592, 0.880781089414183, 0.8763102725366876, 0.8687179487179487, 0.8821052631578947, 0.8732984293193717, 0.873598369011213, 0.8825995807127882, 0.8742331288343558, 0.8800413650465356, 0.8831437435367114, 0.8842975206611571, 0.8956336528221512, 0.8835341365461847, 0.8715313463514902, 0.8744855967078189, 0.8786653185035389, 0.8748696558915537, 0.8725099601593626, 0.8876629889669007, 0.8769074262461851, 0.875, 0.878727634194831, 0.8837675350701403, 0.8800413650465356, 0.8852459016393442]
after delete: len(clean_dataset) 976
after delete: len(noisy_dataset) 2160
epoch [100/200] batch [5/30] time 0.404 (0.457) data 0.273 (0.326) loss_x loss_x 1.5283 (1.2645) acc_x 53.1250 (66.2500) lr 1.0314e-03 eta 0:00:11
epoch [100/200] batch [10/30] time 0.373 (0.461) data 0.243 (0.330) loss_x loss_x 1.1084 (1.2396) acc_x 68.7500 (66.8750) lr 1.0314e-03 eta 0:00:09
epoch [100/200] batch [15/30] time 0.494 (0.459) data 0.362 (0.328) loss_x loss_x 0.9502 (1.2314) acc_x 75.0000 (67.2917) lr 1.0314e-03 eta 0:00:06
epoch [100/200] batch [20/30] time 0.484 (0.466) data 0.354 (0.335) loss_x loss_x 0.8579 (1.2065) acc_x 75.0000 (67.9688) lr 1.0314e-03 eta 0:00:04
epoch [100/200] batch [25/30] time 0.502 (0.472) data 0.370 (0.341) loss_x loss_x 0.9707 (1.2213) acc_x 75.0000 (68.1250) lr 1.0314e-03 eta 0:00:02
epoch [100/200] batch [30/30] time 0.481 (0.484) data 0.350 (0.353) loss_x loss_x 1.6055 (1.2457) acc_x 62.5000 (66.9792) lr 1.0314e-03 eta 0:00:00
epoch [100/200] batch [5/67] time 0.578 (0.485) data 0.446 (0.354) loss_u loss_u 0.8682 (0.8571) acc_u 15.6250 (16.2500) lr 1.0314e-03 eta 0:00:30
epoch [100/200] batch [10/67] time 0.380 (0.473) data 0.248 (0.342) loss_u loss_u 0.8628 (0.8678) acc_u 21.8750 (15.9375) lr 1.0314e-03 eta 0:00:26
epoch [100/200] batch [15/67] time 0.453 (0.467) data 0.321 (0.336) loss_u loss_u 0.8496 (0.8678) acc_u 18.7500 (16.4583) lr 1.0314e-03 eta 0:00:24
epoch [100/200] batch [20/67] time 0.433 (0.463) data 0.302 (0.332) loss_u loss_u 0.8594 (0.8662) acc_u 18.7500 (16.5625) lr 1.0314e-03 eta 0:00:21
epoch [100/200] batch [25/67] time 0.499 (0.465) data 0.369 (0.334) loss_u loss_u 0.8701 (0.8700) acc_u 15.6250 (16.2500) lr 1.0314e-03 eta 0:00:19
epoch [100/200] batch [30/67] time 0.458 (0.468) data 0.327 (0.336) loss_u loss_u 0.8682 (0.8711) acc_u 18.7500 (16.3542) lr 1.0314e-03 eta 0:00:17
epoch [100/200] batch [35/67] time 0.352 (0.464) data 0.220 (0.333) loss_u loss_u 0.8491 (0.8704) acc_u 15.6250 (16.2500) lr 1.0314e-03 eta 0:00:14
epoch [100/200] batch [40/67] time 0.516 (0.466) data 0.385 (0.335) loss_u loss_u 0.8496 (0.8681) acc_u 15.6250 (16.6406) lr 1.0314e-03 eta 0:00:12
epoch [100/200] batch [45/67] time 0.449 (0.466) data 0.317 (0.335) loss_u loss_u 0.9375 (0.8700) acc_u 9.3750 (16.3889) lr 1.0314e-03 eta 0:00:10
epoch [100/200] batch [50/67] time 0.514 (0.464) data 0.383 (0.333) loss_u loss_u 0.8579 (0.8716) acc_u 15.6250 (16.0000) lr 1.0314e-03 eta 0:00:07
epoch [100/200] batch [55/67] time 0.431 (0.465) data 0.299 (0.334) loss_u loss_u 0.9014 (0.8739) acc_u 9.3750 (15.3977) lr 1.0314e-03 eta 0:00:05
epoch [100/200] batch [60/67] time 0.425 (0.465) data 0.294 (0.334) loss_u loss_u 0.8623 (0.8723) acc_u 21.8750 (15.8333) lr 1.0314e-03 eta 0:00:03
epoch [100/200] batch [65/67] time 0.485 (0.466) data 0.354 (0.335) loss_u loss_u 0.8682 (0.8717) acc_u 18.7500 (16.0096) lr 1.0314e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1579
confident_label rate tensor(0.3202, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1004
clean true:884
clean false:120
clean_rate:0.8804780876494024
noisy true:673
noisy false:1459
after delete: len(clean_dataset) 1004
after delete: len(noisy_dataset) 2132
epoch [101/200] batch [5/31] time 0.722 (0.546) data 0.591 (0.414) loss_x loss_x 1.5615 (1.3876) acc_x 59.3750 (68.7500) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [10/31] time 0.348 (0.496) data 0.218 (0.365) loss_x loss_x 0.8398 (1.2707) acc_x 78.1250 (69.3750) lr 1.0157e-03 eta 0:00:10
epoch [101/200] batch [15/31] time 0.515 (0.494) data 0.385 (0.363) loss_x loss_x 1.7188 (1.3153) acc_x 53.1250 (67.2917) lr 1.0157e-03 eta 0:00:07
epoch [101/200] batch [20/31] time 0.442 (0.485) data 0.311 (0.355) loss_x loss_x 0.9233 (1.2597) acc_x 75.0000 (67.6562) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [25/31] time 0.517 (0.486) data 0.385 (0.355) loss_x loss_x 1.2559 (1.2621) acc_x 75.0000 (68.2500) lr 1.0157e-03 eta 0:00:02
epoch [101/200] batch [30/31] time 0.554 (0.495) data 0.423 (0.364) loss_x loss_x 1.5742 (1.3067) acc_x 59.3750 (67.2917) lr 1.0157e-03 eta 0:00:00
epoch [101/200] batch [5/66] time 0.405 (0.494) data 0.273 (0.363) loss_u loss_u 0.8550 (0.8386) acc_u 21.8750 (21.2500) lr 1.0157e-03 eta 0:00:30
epoch [101/200] batch [10/66] time 0.501 (0.486) data 0.370 (0.355) loss_u loss_u 0.8716 (0.8629) acc_u 18.7500 (17.8125) lr 1.0157e-03 eta 0:00:27
epoch [101/200] batch [15/66] time 0.403 (0.480) data 0.272 (0.349) loss_u loss_u 0.9004 (0.8676) acc_u 12.5000 (17.0833) lr 1.0157e-03 eta 0:00:24
epoch [101/200] batch [20/66] time 0.459 (0.479) data 0.329 (0.348) loss_u loss_u 0.9375 (0.8798) acc_u 6.2500 (15.0000) lr 1.0157e-03 eta 0:00:22
epoch [101/200] batch [25/66] time 0.523 (0.483) data 0.392 (0.352) loss_u loss_u 0.8916 (0.8763) acc_u 15.6250 (15.3750) lr 1.0157e-03 eta 0:00:19
epoch [101/200] batch [30/66] time 0.441 (0.479) data 0.309 (0.348) loss_u loss_u 0.8501 (0.8771) acc_u 18.7500 (14.8958) lr 1.0157e-03 eta 0:00:17
epoch [101/200] batch [35/66] time 0.327 (0.472) data 0.195 (0.341) loss_u loss_u 0.8809 (0.8797) acc_u 18.7500 (14.7321) lr 1.0157e-03 eta 0:00:14
epoch [101/200] batch [40/66] time 0.384 (0.472) data 0.252 (0.341) loss_u loss_u 0.8291 (0.8801) acc_u 18.7500 (14.6875) lr 1.0157e-03 eta 0:00:12
epoch [101/200] batch [45/66] time 0.454 (0.473) data 0.322 (0.342) loss_u loss_u 0.8950 (0.8768) acc_u 12.5000 (15.1389) lr 1.0157e-03 eta 0:00:09
epoch [101/200] batch [50/66] time 0.812 (0.477) data 0.680 (0.346) loss_u loss_u 0.8613 (0.8763) acc_u 18.7500 (15.1875) lr 1.0157e-03 eta 0:00:07
epoch [101/200] batch [55/66] time 0.456 (0.475) data 0.325 (0.344) loss_u loss_u 0.9336 (0.8787) acc_u 6.2500 (14.6591) lr 1.0157e-03 eta 0:00:05
epoch [101/200] batch [60/66] time 0.359 (0.477) data 0.229 (0.346) loss_u loss_u 0.8394 (0.8785) acc_u 18.7500 (14.6875) lr 1.0157e-03 eta 0:00:02
epoch [101/200] batch [65/66] time 0.431 (0.474) data 0.300 (0.343) loss_u loss_u 0.8809 (0.8799) acc_u 15.6250 (14.4231) lr 1.0157e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1614
confident_label rate tensor(0.3208, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1006
clean true:876
clean false:130
clean_rate:0.8707753479125249
noisy true:646
noisy false:1484
after delete: len(clean_dataset) 1006
after delete: len(noisy_dataset) 2130
epoch [102/200] batch [5/31] time 0.437 (0.473) data 0.306 (0.343) loss_x loss_x 1.4814 (1.3199) acc_x 59.3750 (68.1250) lr 1.0000e-03 eta 0:00:12
epoch [102/200] batch [10/31] time 0.524 (0.475) data 0.392 (0.344) loss_x loss_x 1.4971 (1.3906) acc_x 65.6250 (66.2500) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [15/31] time 0.416 (0.466) data 0.286 (0.335) loss_x loss_x 1.5771 (1.3400) acc_x 65.6250 (66.6667) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [20/31] time 0.528 (0.473) data 0.397 (0.342) loss_x loss_x 1.1650 (1.2790) acc_x 75.0000 (68.2812) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [25/31] time 0.509 (0.472) data 0.378 (0.341) loss_x loss_x 0.9067 (1.2549) acc_x 81.2500 (69.0000) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [30/31] time 0.535 (0.484) data 0.404 (0.353) loss_x loss_x 1.4570 (1.2822) acc_x 59.3750 (68.3333) lr 1.0000e-03 eta 0:00:00
epoch [102/200] batch [5/66] time 0.417 (0.478) data 0.287 (0.347) loss_u loss_u 0.9131 (0.8726) acc_u 9.3750 (16.8750) lr 1.0000e-03 eta 0:00:29
epoch [102/200] batch [10/66] time 0.517 (0.475) data 0.386 (0.345) loss_u loss_u 0.8760 (0.8869) acc_u 12.5000 (14.6875) lr 1.0000e-03 eta 0:00:26
epoch [102/200] batch [15/66] time 0.393 (0.470) data 0.263 (0.339) loss_u loss_u 0.9312 (0.8891) acc_u 9.3750 (13.7500) lr 1.0000e-03 eta 0:00:23
epoch [102/200] batch [20/66] time 0.419 (0.465) data 0.289 (0.334) loss_u loss_u 0.9229 (0.8903) acc_u 6.2500 (13.9062) lr 1.0000e-03 eta 0:00:21
epoch [102/200] batch [25/66] time 0.435 (0.461) data 0.305 (0.331) loss_u loss_u 0.8320 (0.8863) acc_u 18.7500 (14.3750) lr 1.0000e-03 eta 0:00:18
epoch [102/200] batch [30/66] time 0.384 (0.457) data 0.254 (0.327) loss_u loss_u 0.9082 (0.8890) acc_u 6.2500 (13.6458) lr 1.0000e-03 eta 0:00:16
epoch [102/200] batch [35/66] time 0.654 (0.463) data 0.524 (0.332) loss_u loss_u 0.9077 (0.8840) acc_u 12.5000 (14.6429) lr 1.0000e-03 eta 0:00:14
epoch [102/200] batch [40/66] time 0.405 (0.460) data 0.274 (0.330) loss_u loss_u 0.8521 (0.8826) acc_u 21.8750 (14.7656) lr 1.0000e-03 eta 0:00:11
epoch [102/200] batch [45/66] time 0.517 (0.462) data 0.386 (0.331) loss_u loss_u 0.8916 (0.8822) acc_u 15.6250 (14.8611) lr 1.0000e-03 eta 0:00:09
epoch [102/200] batch [50/66] time 0.363 (0.460) data 0.232 (0.330) loss_u loss_u 0.9199 (0.8852) acc_u 6.2500 (14.2500) lr 1.0000e-03 eta 0:00:07
epoch [102/200] batch [55/66] time 0.419 (0.463) data 0.287 (0.332) loss_u loss_u 0.9092 (0.8877) acc_u 18.7500 (14.2614) lr 1.0000e-03 eta 0:00:05
epoch [102/200] batch [60/66] time 0.426 (0.465) data 0.293 (0.334) loss_u loss_u 0.8379 (0.8870) acc_u 21.8750 (14.3229) lr 1.0000e-03 eta 0:00:02
epoch [102/200] batch [65/66] time 0.349 (0.463) data 0.216 (0.332) loss_u loss_u 0.9067 (0.8834) acc_u 9.3750 (14.8077) lr 1.0000e-03 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1594
confident_label rate tensor(0.3262, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1023
clean true:881
clean false:142
clean_rate:0.8611925708699902
noisy true:661
noisy false:1452
after delete: len(clean_dataset) 1023
after delete: len(noisy_dataset) 2113
epoch [103/200] batch [5/31] time 0.531 (0.474) data 0.399 (0.342) loss_x loss_x 0.9756 (1.2674) acc_x 78.1250 (71.2500) lr 9.8429e-04 eta 0:00:12
epoch [103/200] batch [10/31] time 0.417 (0.464) data 0.287 (0.332) loss_x loss_x 1.1201 (1.2938) acc_x 62.5000 (68.4375) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [15/31] time 0.590 (0.482) data 0.459 (0.351) loss_x loss_x 1.2246 (1.3624) acc_x 68.7500 (66.6667) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [20/31] time 0.437 (0.470) data 0.306 (0.339) loss_x loss_x 0.9155 (1.3131) acc_x 84.3750 (67.8125) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [25/31] time 0.548 (0.475) data 0.418 (0.343) loss_x loss_x 1.1611 (1.3415) acc_x 71.8750 (67.3750) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [30/31] time 0.326 (0.472) data 0.195 (0.341) loss_x loss_x 0.9341 (1.3111) acc_x 81.2500 (68.4375) lr 9.8429e-04 eta 0:00:00
epoch [103/200] batch [5/66] time 0.453 (0.468) data 0.321 (0.336) loss_u loss_u 0.8862 (0.8940) acc_u 9.3750 (14.3750) lr 9.8429e-04 eta 0:00:28
epoch [103/200] batch [10/66] time 0.417 (0.464) data 0.285 (0.333) loss_u loss_u 0.8438 (0.8858) acc_u 21.8750 (15.3125) lr 9.8429e-04 eta 0:00:26
epoch [103/200] batch [15/66] time 0.409 (0.465) data 0.278 (0.334) loss_u loss_u 0.9302 (0.8830) acc_u 6.2500 (15.2083) lr 9.8429e-04 eta 0:00:23
epoch [103/200] batch [20/66] time 0.410 (0.462) data 0.279 (0.331) loss_u loss_u 0.9058 (0.8820) acc_u 15.6250 (15.3125) lr 9.8429e-04 eta 0:00:21
epoch [103/200] batch [25/66] time 0.420 (0.464) data 0.290 (0.333) loss_u loss_u 0.8384 (0.8819) acc_u 18.7500 (14.8750) lr 9.8429e-04 eta 0:00:19
epoch [103/200] batch [30/66] time 0.430 (0.461) data 0.299 (0.330) loss_u loss_u 0.8057 (0.8754) acc_u 28.1250 (15.6250) lr 9.8429e-04 eta 0:00:16
epoch [103/200] batch [35/66] time 0.371 (0.461) data 0.240 (0.329) loss_u loss_u 0.8823 (0.8798) acc_u 15.6250 (15.0000) lr 9.8429e-04 eta 0:00:14
epoch [103/200] batch [40/66] time 0.434 (0.458) data 0.304 (0.327) loss_u loss_u 0.8442 (0.8821) acc_u 18.7500 (14.7656) lr 9.8429e-04 eta 0:00:11
epoch [103/200] batch [45/66] time 0.487 (0.456) data 0.356 (0.325) loss_u loss_u 0.8384 (0.8819) acc_u 15.6250 (14.5833) lr 9.8429e-04 eta 0:00:09
epoch [103/200] batch [50/66] time 0.503 (0.461) data 0.373 (0.329) loss_u loss_u 0.9126 (0.8797) acc_u 15.6250 (14.8750) lr 9.8429e-04 eta 0:00:07
epoch [103/200] batch [55/66] time 0.709 (0.467) data 0.577 (0.336) loss_u loss_u 0.8237 (0.8777) acc_u 25.0000 (15.1705) lr 9.8429e-04 eta 0:00:05
epoch [103/200] batch [60/66] time 0.448 (0.466) data 0.317 (0.334) loss_u loss_u 0.9438 (0.8793) acc_u 9.3750 (15.0000) lr 9.8429e-04 eta 0:00:02
epoch [103/200] batch [65/66] time 0.431 (0.463) data 0.300 (0.331) loss_u loss_u 0.9253 (0.8813) acc_u 12.5000 (14.9038) lr 9.8429e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1620
confident_label rate tensor(0.3211, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1007
clean true:867
clean false:140
clean_rate:0.8609731876861966
noisy true:649
noisy false:1480
after delete: len(clean_dataset) 1007
after delete: len(noisy_dataset) 2129
epoch [104/200] batch [5/31] time 0.456 (0.442) data 0.324 (0.309) loss_x loss_x 0.9731 (1.2841) acc_x 68.7500 (66.8750) lr 9.6859e-04 eta 0:00:11
epoch [104/200] batch [10/31] time 0.501 (0.490) data 0.370 (0.357) loss_x loss_x 0.9702 (1.1433) acc_x 68.7500 (70.3125) lr 9.6859e-04 eta 0:00:10
epoch [104/200] batch [15/31] time 0.514 (0.491) data 0.382 (0.359) loss_x loss_x 1.2637 (1.2121) acc_x 71.8750 (68.1250) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [20/31] time 0.504 (0.502) data 0.373 (0.370) loss_x loss_x 1.2891 (1.2168) acc_x 62.5000 (67.5000) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [25/31] time 0.403 (0.488) data 0.272 (0.357) loss_x loss_x 0.7720 (1.1842) acc_x 75.0000 (68.5000) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [30/31] time 0.492 (0.484) data 0.362 (0.353) loss_x loss_x 1.0078 (1.1719) acc_x 81.2500 (69.0625) lr 9.6859e-04 eta 0:00:00
epoch [104/200] batch [5/66] time 0.379 (0.487) data 0.248 (0.355) loss_u loss_u 0.8999 (0.8805) acc_u 12.5000 (15.0000) lr 9.6859e-04 eta 0:00:29
epoch [104/200] batch [10/66] time 0.465 (0.483) data 0.334 (0.352) loss_u loss_u 0.8608 (0.8874) acc_u 15.6250 (15.0000) lr 9.6859e-04 eta 0:00:27
epoch [104/200] batch [15/66] time 0.592 (0.479) data 0.460 (0.348) loss_u loss_u 0.8691 (0.8882) acc_u 9.3750 (13.9583) lr 9.6859e-04 eta 0:00:24
epoch [104/200] batch [20/66] time 0.570 (0.477) data 0.437 (0.346) loss_u loss_u 0.8916 (0.8815) acc_u 15.6250 (15.6250) lr 9.6859e-04 eta 0:00:21
epoch [104/200] batch [25/66] time 0.370 (0.470) data 0.239 (0.339) loss_u loss_u 0.8623 (0.8820) acc_u 12.5000 (15.1250) lr 9.6859e-04 eta 0:00:19
epoch [104/200] batch [30/66] time 0.440 (0.469) data 0.310 (0.337) loss_u loss_u 0.8511 (0.8765) acc_u 15.6250 (15.6250) lr 9.6859e-04 eta 0:00:16
epoch [104/200] batch [35/66] time 0.327 (0.464) data 0.197 (0.333) loss_u loss_u 0.9390 (0.8795) acc_u 9.3750 (15.2679) lr 9.6859e-04 eta 0:00:14
epoch [104/200] batch [40/66] time 0.408 (0.462) data 0.276 (0.330) loss_u loss_u 0.8672 (0.8779) acc_u 15.6250 (15.3906) lr 9.6859e-04 eta 0:00:12
epoch [104/200] batch [45/66] time 0.380 (0.460) data 0.249 (0.328) loss_u loss_u 0.8647 (0.8791) acc_u 9.3750 (15.2083) lr 9.6859e-04 eta 0:00:09
epoch [104/200] batch [50/66] time 0.553 (0.463) data 0.422 (0.332) loss_u loss_u 0.8608 (0.8758) acc_u 21.8750 (15.5000) lr 9.6859e-04 eta 0:00:07
epoch [104/200] batch [55/66] time 0.345 (0.464) data 0.214 (0.333) loss_u loss_u 0.8335 (0.8761) acc_u 12.5000 (15.3977) lr 9.6859e-04 eta 0:00:05
epoch [104/200] batch [60/66] time 0.436 (0.462) data 0.304 (0.330) loss_u loss_u 0.8472 (0.8766) acc_u 18.7500 (15.2604) lr 9.6859e-04 eta 0:00:02
epoch [104/200] batch [65/66] time 0.425 (0.462) data 0.293 (0.331) loss_u loss_u 0.8091 (0.8752) acc_u 25.0000 (15.2885) lr 9.6859e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1627
confident_label rate tensor(0.3135, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 983
clean true:857
clean false:126
clean_rate:0.8718209562563581
noisy true:652
noisy false:1501
after delete: len(clean_dataset) 983
after delete: len(noisy_dataset) 2153
epoch [105/200] batch [5/30] time 0.481 (0.540) data 0.351 (0.410) loss_x loss_x 1.2764 (1.1129) acc_x 65.6250 (74.3750) lr 9.5289e-04 eta 0:00:13
epoch [105/200] batch [10/30] time 0.463 (0.511) data 0.332 (0.381) loss_x loss_x 1.1895 (1.1600) acc_x 65.6250 (73.4375) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [15/30] time 0.395 (0.473) data 0.265 (0.342) loss_x loss_x 0.9917 (1.2105) acc_x 75.0000 (71.8750) lr 9.5289e-04 eta 0:00:07
epoch [105/200] batch [20/30] time 0.400 (0.455) data 0.270 (0.324) loss_x loss_x 0.9951 (1.1858) acc_x 71.8750 (71.0938) lr 9.5289e-04 eta 0:00:04
epoch [105/200] batch [25/30] time 0.534 (0.469) data 0.404 (0.339) loss_x loss_x 1.7705 (1.1589) acc_x 50.0000 (70.8750) lr 9.5289e-04 eta 0:00:02
epoch [105/200] batch [30/30] time 0.515 (0.479) data 0.385 (0.348) loss_x loss_x 1.2139 (1.1868) acc_x 65.6250 (69.8958) lr 9.5289e-04 eta 0:00:00
epoch [105/200] batch [5/67] time 0.496 (0.483) data 0.365 (0.352) loss_u loss_u 0.8965 (0.8784) acc_u 12.5000 (14.3750) lr 9.5289e-04 eta 0:00:29
epoch [105/200] batch [10/67] time 0.367 (0.483) data 0.236 (0.352) loss_u loss_u 0.8506 (0.8810) acc_u 21.8750 (15.0000) lr 9.5289e-04 eta 0:00:27
epoch [105/200] batch [15/67] time 0.330 (0.481) data 0.197 (0.350) loss_u loss_u 0.8540 (0.8742) acc_u 15.6250 (15.6250) lr 9.5289e-04 eta 0:00:25
epoch [105/200] batch [20/67] time 0.419 (0.484) data 0.288 (0.353) loss_u loss_u 0.9395 (0.8802) acc_u 12.5000 (15.0000) lr 9.5289e-04 eta 0:00:22
epoch [105/200] batch [25/67] time 0.507 (0.481) data 0.376 (0.350) loss_u loss_u 0.8892 (0.8805) acc_u 12.5000 (14.7500) lr 9.5289e-04 eta 0:00:20
epoch [105/200] batch [30/67] time 0.422 (0.483) data 0.291 (0.352) loss_u loss_u 0.8926 (0.8830) acc_u 12.5000 (14.5833) lr 9.5289e-04 eta 0:00:17
epoch [105/200] batch [35/67] time 0.423 (0.480) data 0.291 (0.349) loss_u loss_u 0.9019 (0.8820) acc_u 9.3750 (14.8214) lr 9.5289e-04 eta 0:00:15
epoch [105/200] batch [40/67] time 0.401 (0.479) data 0.270 (0.348) loss_u loss_u 0.8862 (0.8801) acc_u 18.7500 (15.3906) lr 9.5289e-04 eta 0:00:12
epoch [105/200] batch [45/67] time 0.368 (0.472) data 0.237 (0.342) loss_u loss_u 0.9438 (0.8801) acc_u 6.2500 (15.3472) lr 9.5289e-04 eta 0:00:10
epoch [105/200] batch [50/67] time 0.421 (0.471) data 0.289 (0.340) loss_u loss_u 0.8760 (0.8781) acc_u 12.5000 (15.5000) lr 9.5289e-04 eta 0:00:08
epoch [105/200] batch [55/67] time 0.491 (0.474) data 0.359 (0.343) loss_u loss_u 0.8687 (0.8783) acc_u 21.8750 (15.6818) lr 9.5289e-04 eta 0:00:05
epoch [105/200] batch [60/67] time 0.545 (0.475) data 0.413 (0.344) loss_u loss_u 0.8604 (0.8746) acc_u 18.7500 (16.0417) lr 9.5289e-04 eta 0:00:03
epoch [105/200] batch [65/67] time 0.470 (0.475) data 0.339 (0.344) loss_u loss_u 0.9185 (0.8748) acc_u 6.2500 (15.9615) lr 9.5289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1583
confident_label rate tensor(0.3224, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1011
clean true:887
clean false:124
clean_rate:0.8773491592482691
noisy true:666
noisy false:1459
after delete: len(clean_dataset) 1011
after delete: len(noisy_dataset) 2125
epoch [106/200] batch [5/31] time 0.428 (0.439) data 0.297 (0.308) loss_x loss_x 1.2061 (1.2006) acc_x 75.0000 (70.0000) lr 9.3721e-04 eta 0:00:11
epoch [106/200] batch [10/31] time 0.534 (0.470) data 0.403 (0.339) loss_x loss_x 1.5518 (1.2651) acc_x 65.6250 (68.7500) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [15/31] time 0.397 (0.479) data 0.267 (0.348) loss_x loss_x 1.0469 (1.3307) acc_x 81.2500 (66.4583) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [20/31] time 0.456 (0.482) data 0.326 (0.351) loss_x loss_x 0.8921 (1.2644) acc_x 78.1250 (68.1250) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [25/31] time 0.506 (0.478) data 0.376 (0.348) loss_x loss_x 1.2803 (1.2818) acc_x 68.7500 (67.5000) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [30/31] time 0.394 (0.479) data 0.264 (0.348) loss_x loss_x 1.2363 (1.2904) acc_x 53.1250 (66.8750) lr 9.3721e-04 eta 0:00:00
epoch [106/200] batch [5/66] time 0.423 (0.483) data 0.292 (0.352) loss_u loss_u 0.8579 (0.8720) acc_u 18.7500 (15.6250) lr 9.3721e-04 eta 0:00:29
epoch [106/200] batch [10/66] time 0.480 (0.478) data 0.348 (0.347) loss_u loss_u 0.8447 (0.8511) acc_u 15.6250 (18.7500) lr 9.3721e-04 eta 0:00:26
epoch [106/200] batch [15/66] time 0.443 (0.474) data 0.312 (0.343) loss_u loss_u 0.8838 (0.8641) acc_u 15.6250 (17.2917) lr 9.3721e-04 eta 0:00:24
epoch [106/200] batch [20/66] time 0.495 (0.473) data 0.363 (0.342) loss_u loss_u 0.8726 (0.8671) acc_u 15.6250 (17.3438) lr 9.3721e-04 eta 0:00:21
epoch [106/200] batch [25/66] time 0.366 (0.471) data 0.235 (0.340) loss_u loss_u 0.8223 (0.8674) acc_u 21.8750 (17.2500) lr 9.3721e-04 eta 0:00:19
epoch [106/200] batch [30/66] time 0.416 (0.468) data 0.286 (0.337) loss_u loss_u 0.8145 (0.8681) acc_u 25.0000 (17.3958) lr 9.3721e-04 eta 0:00:16
epoch [106/200] batch [35/66] time 0.494 (0.468) data 0.364 (0.337) loss_u loss_u 0.9126 (0.8717) acc_u 12.5000 (16.9643) lr 9.3721e-04 eta 0:00:14
epoch [106/200] batch [40/66] time 0.393 (0.467) data 0.263 (0.336) loss_u loss_u 0.8521 (0.8726) acc_u 15.6250 (16.5625) lr 9.3721e-04 eta 0:00:12
epoch [106/200] batch [45/66] time 0.372 (0.464) data 0.240 (0.333) loss_u loss_u 0.8252 (0.8712) acc_u 21.8750 (16.8750) lr 9.3721e-04 eta 0:00:09
epoch [106/200] batch [50/66] time 0.615 (0.466) data 0.484 (0.335) loss_u loss_u 0.9170 (0.8745) acc_u 9.3750 (16.4375) lr 9.3721e-04 eta 0:00:07
epoch [106/200] batch [55/66] time 0.393 (0.463) data 0.261 (0.332) loss_u loss_u 0.8721 (0.8746) acc_u 15.6250 (16.4773) lr 9.3721e-04 eta 0:00:05
epoch [106/200] batch [60/66] time 0.337 (0.466) data 0.205 (0.335) loss_u loss_u 0.9395 (0.8773) acc_u 3.1250 (16.0417) lr 9.3721e-04 eta 0:00:02
epoch [106/200] batch [65/66] time 0.432 (0.465) data 0.300 (0.334) loss_u loss_u 0.9419 (0.8795) acc_u 9.3750 (15.6731) lr 9.3721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1654
confident_label rate tensor(0.3115, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 977
clean true:860
clean false:117
clean_rate:0.8802456499488229
noisy true:622
noisy false:1537
after delete: len(clean_dataset) 977
after delete: len(noisy_dataset) 2159
epoch [107/200] batch [5/30] time 0.395 (0.447) data 0.264 (0.316) loss_x loss_x 0.8389 (1.1973) acc_x 75.0000 (65.6250) lr 9.2154e-04 eta 0:00:11
epoch [107/200] batch [10/30] time 0.376 (0.441) data 0.245 (0.310) loss_x loss_x 1.2109 (1.1657) acc_x 68.7500 (67.1875) lr 9.2154e-04 eta 0:00:08
epoch [107/200] batch [15/30] time 0.430 (0.434) data 0.299 (0.303) loss_x loss_x 1.1816 (1.2277) acc_x 62.5000 (67.2917) lr 9.2154e-04 eta 0:00:06
epoch [107/200] batch [20/30] time 0.466 (0.451) data 0.335 (0.320) loss_x loss_x 1.6406 (1.2955) acc_x 59.3750 (66.4062) lr 9.2154e-04 eta 0:00:04
epoch [107/200] batch [25/30] time 0.560 (0.459) data 0.430 (0.328) loss_x loss_x 1.2002 (1.2764) acc_x 68.7500 (66.5000) lr 9.2154e-04 eta 0:00:02
epoch [107/200] batch [30/30] time 0.408 (0.463) data 0.278 (0.332) loss_x loss_x 1.2529 (1.2826) acc_x 68.7500 (66.6667) lr 9.2154e-04 eta 0:00:00
epoch [107/200] batch [5/67] time 0.426 (0.464) data 0.293 (0.333) loss_u loss_u 0.9414 (0.8983) acc_u 6.2500 (10.6250) lr 9.2154e-04 eta 0:00:28
epoch [107/200] batch [10/67] time 0.419 (0.459) data 0.288 (0.328) loss_u loss_u 0.8872 (0.8915) acc_u 15.6250 (12.8125) lr 9.2154e-04 eta 0:00:26
epoch [107/200] batch [15/67] time 0.351 (0.455) data 0.219 (0.324) loss_u loss_u 0.8892 (0.8898) acc_u 12.5000 (13.3333) lr 9.2154e-04 eta 0:00:23
epoch [107/200] batch [20/67] time 0.458 (0.457) data 0.326 (0.326) loss_u loss_u 0.8433 (0.8811) acc_u 15.6250 (14.5312) lr 9.2154e-04 eta 0:00:21
epoch [107/200] batch [25/67] time 0.463 (0.458) data 0.330 (0.327) loss_u loss_u 0.9209 (0.8748) acc_u 12.5000 (15.1250) lr 9.2154e-04 eta 0:00:19
epoch [107/200] batch [30/67] time 0.407 (0.452) data 0.275 (0.321) loss_u loss_u 0.8901 (0.8776) acc_u 15.6250 (14.6875) lr 9.2154e-04 eta 0:00:16
epoch [107/200] batch [35/67] time 0.419 (0.454) data 0.288 (0.323) loss_u loss_u 0.8613 (0.8763) acc_u 18.7500 (14.9107) lr 9.2154e-04 eta 0:00:14
epoch [107/200] batch [40/67] time 0.500 (0.459) data 0.366 (0.328) loss_u loss_u 0.7656 (0.8728) acc_u 31.2500 (15.3906) lr 9.2154e-04 eta 0:00:12
epoch [107/200] batch [45/67] time 0.433 (0.461) data 0.300 (0.330) loss_u loss_u 0.8843 (0.8734) acc_u 12.5000 (15.0694) lr 9.2154e-04 eta 0:00:10
epoch [107/200] batch [50/67] time 0.477 (0.462) data 0.345 (0.330) loss_u loss_u 0.8320 (0.8714) acc_u 21.8750 (15.3125) lr 9.2154e-04 eta 0:00:07
epoch [107/200] batch [55/67] time 0.410 (0.462) data 0.278 (0.330) loss_u loss_u 0.8203 (0.8715) acc_u 21.8750 (15.4545) lr 9.2154e-04 eta 0:00:05
epoch [107/200] batch [60/67] time 0.550 (0.463) data 0.418 (0.331) loss_u loss_u 0.8267 (0.8688) acc_u 15.6250 (15.4688) lr 9.2154e-04 eta 0:00:03
epoch [107/200] batch [65/67] time 0.598 (0.462) data 0.465 (0.330) loss_u loss_u 0.8247 (0.8682) acc_u 18.7500 (15.7212) lr 9.2154e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1557
confident_label rate tensor(0.3278, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1028
clean true:898
clean false:130
clean_rate:0.8735408560311284
noisy true:681
noisy false:1427
after delete: len(clean_dataset) 1028
after delete: len(noisy_dataset) 2108
epoch [108/200] batch [5/32] time 0.331 (0.496) data 0.201 (0.365) loss_x loss_x 1.2266 (1.1248) acc_x 71.8750 (71.2500) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [10/32] time 0.543 (0.477) data 0.411 (0.346) loss_x loss_x 1.2920 (1.2656) acc_x 75.0000 (67.5000) lr 9.0589e-04 eta 0:00:10
epoch [108/200] batch [15/32] time 0.491 (0.484) data 0.360 (0.353) loss_x loss_x 1.2520 (1.3031) acc_x 78.1250 (66.8750) lr 9.0589e-04 eta 0:00:08
epoch [108/200] batch [20/32] time 0.547 (0.493) data 0.416 (0.362) loss_x loss_x 1.5420 (1.3339) acc_x 62.5000 (66.4062) lr 9.0589e-04 eta 0:00:05
epoch [108/200] batch [25/32] time 0.550 (0.481) data 0.421 (0.351) loss_x loss_x 0.9443 (1.3316) acc_x 78.1250 (67.2500) lr 9.0589e-04 eta 0:00:03
epoch [108/200] batch [30/32] time 0.450 (0.484) data 0.320 (0.353) loss_x loss_x 1.3643 (1.3022) acc_x 62.5000 (67.6042) lr 9.0589e-04 eta 0:00:00
epoch [108/200] batch [5/65] time 0.370 (0.476) data 0.238 (0.345) loss_u loss_u 0.9668 (0.9193) acc_u 0.0000 (7.5000) lr 9.0589e-04 eta 0:00:28
epoch [108/200] batch [10/65] time 0.395 (0.476) data 0.264 (0.345) loss_u loss_u 0.9204 (0.8760) acc_u 6.2500 (12.8125) lr 9.0589e-04 eta 0:00:26
epoch [108/200] batch [15/65] time 0.547 (0.470) data 0.416 (0.339) loss_u loss_u 0.9116 (0.8856) acc_u 9.3750 (13.1250) lr 9.0589e-04 eta 0:00:23
epoch [108/200] batch [20/65] time 0.457 (0.468) data 0.327 (0.337) loss_u loss_u 0.8765 (0.8859) acc_u 18.7500 (13.4375) lr 9.0589e-04 eta 0:00:21
epoch [108/200] batch [25/65] time 0.376 (0.466) data 0.243 (0.335) loss_u loss_u 0.7896 (0.8760) acc_u 21.8750 (15.0000) lr 9.0589e-04 eta 0:00:18
epoch [108/200] batch [30/65] time 0.327 (0.459) data 0.197 (0.328) loss_u loss_u 0.9102 (0.8733) acc_u 9.3750 (15.1042) lr 9.0589e-04 eta 0:00:16
epoch [108/200] batch [35/65] time 0.405 (0.462) data 0.274 (0.331) loss_u loss_u 0.8921 (0.8747) acc_u 18.7500 (15.2679) lr 9.0589e-04 eta 0:00:13
epoch [108/200] batch [40/65] time 0.430 (0.461) data 0.298 (0.330) loss_u loss_u 0.8760 (0.8756) acc_u 15.6250 (15.1562) lr 9.0589e-04 eta 0:00:11
epoch [108/200] batch [45/65] time 0.474 (0.461) data 0.342 (0.330) loss_u loss_u 0.8940 (0.8766) acc_u 12.5000 (15.2778) lr 9.0589e-04 eta 0:00:09
epoch [108/200] batch [50/65] time 0.375 (0.456) data 0.243 (0.325) loss_u loss_u 0.9434 (0.8801) acc_u 6.2500 (14.7500) lr 9.0589e-04 eta 0:00:06
epoch [108/200] batch [55/65] time 0.638 (0.459) data 0.507 (0.328) loss_u loss_u 0.9038 (0.8815) acc_u 15.6250 (14.5455) lr 9.0589e-04 eta 0:00:04
epoch [108/200] batch [60/65] time 0.413 (0.458) data 0.281 (0.327) loss_u loss_u 0.8101 (0.8788) acc_u 18.7500 (14.6875) lr 9.0589e-04 eta 0:00:02
epoch [108/200] batch [65/65] time 0.620 (0.464) data 0.489 (0.333) loss_u loss_u 0.8467 (0.8795) acc_u 21.8750 (14.7115) lr 9.0589e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1591
confident_label rate tensor(0.3246, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1018
clean true:891
clean false:127
clean_rate:0.8752455795677799
noisy true:654
noisy false:1464
after delete: len(clean_dataset) 1018
after delete: len(noisy_dataset) 2118
epoch [109/200] batch [5/31] time 0.430 (0.473) data 0.299 (0.343) loss_x loss_x 1.1602 (1.1854) acc_x 71.8750 (68.7500) lr 8.9027e-04 eta 0:00:12
epoch [109/200] batch [10/31] time 0.548 (0.468) data 0.418 (0.338) loss_x loss_x 1.9404 (1.2425) acc_x 62.5000 (69.0625) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [15/31] time 0.532 (0.486) data 0.402 (0.355) loss_x loss_x 1.5059 (1.1832) acc_x 65.6250 (70.0000) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [20/31] time 0.482 (0.493) data 0.352 (0.362) loss_x loss_x 1.6113 (1.1749) acc_x 50.0000 (69.2188) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [25/31] time 0.385 (0.479) data 0.254 (0.349) loss_x loss_x 1.6299 (1.1849) acc_x 59.3750 (70.1250) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [30/31] time 0.446 (0.484) data 0.316 (0.354) loss_x loss_x 1.2529 (1.1972) acc_x 68.7500 (69.8958) lr 8.9027e-04 eta 0:00:00
epoch [109/200] batch [5/66] time 0.444 (0.482) data 0.313 (0.352) loss_u loss_u 0.8765 (0.8717) acc_u 18.7500 (16.8750) lr 8.9027e-04 eta 0:00:29
epoch [109/200] batch [10/66] time 0.422 (0.476) data 0.291 (0.345) loss_u loss_u 0.8403 (0.8833) acc_u 25.0000 (15.0000) lr 8.9027e-04 eta 0:00:26
epoch [109/200] batch [15/66] time 0.357 (0.467) data 0.226 (0.336) loss_u loss_u 0.8457 (0.8799) acc_u 21.8750 (15.8333) lr 8.9027e-04 eta 0:00:23
epoch [109/200] batch [20/66] time 0.538 (0.465) data 0.406 (0.334) loss_u loss_u 0.9414 (0.8794) acc_u 9.3750 (16.4062) lr 8.9027e-04 eta 0:00:21
epoch [109/200] batch [25/66] time 0.420 (0.463) data 0.289 (0.332) loss_u loss_u 0.9448 (0.8810) acc_u 6.2500 (15.6250) lr 8.9027e-04 eta 0:00:18
epoch [109/200] batch [30/66] time 0.475 (0.462) data 0.343 (0.331) loss_u loss_u 0.8516 (0.8833) acc_u 18.7500 (15.1042) lr 8.9027e-04 eta 0:00:16
epoch [109/200] batch [35/66] time 0.408 (0.461) data 0.277 (0.330) loss_u loss_u 0.9116 (0.8843) acc_u 12.5000 (14.6429) lr 8.9027e-04 eta 0:00:14
epoch [109/200] batch [40/66] time 0.445 (0.460) data 0.315 (0.329) loss_u loss_u 0.9248 (0.8833) acc_u 9.3750 (14.9219) lr 8.9027e-04 eta 0:00:11
epoch [109/200] batch [45/66] time 0.376 (0.459) data 0.245 (0.328) loss_u loss_u 0.9346 (0.8869) acc_u 6.2500 (14.4444) lr 8.9027e-04 eta 0:00:09
epoch [109/200] batch [50/66] time 0.518 (0.460) data 0.386 (0.329) loss_u loss_u 0.8438 (0.8884) acc_u 18.7500 (14.1875) lr 8.9027e-04 eta 0:00:07
epoch [109/200] batch [55/66] time 0.486 (0.459) data 0.354 (0.328) loss_u loss_u 0.8535 (0.8860) acc_u 21.8750 (14.5455) lr 8.9027e-04 eta 0:00:05
epoch [109/200] batch [60/66] time 0.378 (0.461) data 0.246 (0.330) loss_u loss_u 0.8730 (0.8864) acc_u 15.6250 (14.4271) lr 8.9027e-04 eta 0:00:02
epoch [109/200] batch [65/66] time 0.564 (0.462) data 0.432 (0.331) loss_u loss_u 0.9639 (0.8854) acc_u 0.0000 (14.3269) lr 8.9027e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1589
confident_label rate tensor(0.3265, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1024
clean true:898
clean false:126
clean_rate:0.876953125
noisy true:649
noisy false:1463
after delete: len(clean_dataset) 1024
after delete: len(noisy_dataset) 2112
epoch [110/200] batch [5/32] time 0.542 (0.518) data 0.411 (0.387) loss_x loss_x 1.4404 (1.4850) acc_x 68.7500 (64.3750) lr 8.7467e-04 eta 0:00:13
epoch [110/200] batch [10/32] time 0.398 (0.481) data 0.268 (0.350) loss_x loss_x 1.3115 (1.4044) acc_x 68.7500 (65.0000) lr 8.7467e-04 eta 0:00:10
epoch [110/200] batch [15/32] time 0.777 (0.495) data 0.647 (0.364) loss_x loss_x 0.8696 (1.3055) acc_x 75.0000 (66.2500) lr 8.7467e-04 eta 0:00:08
epoch [110/200] batch [20/32] time 0.417 (0.494) data 0.286 (0.364) loss_x loss_x 0.9644 (1.2687) acc_x 78.1250 (67.3438) lr 8.7467e-04 eta 0:00:05
epoch [110/200] batch [25/32] time 0.415 (0.487) data 0.284 (0.356) loss_x loss_x 1.3115 (1.2485) acc_x 68.7500 (67.5000) lr 8.7467e-04 eta 0:00:03
epoch [110/200] batch [30/32] time 0.411 (0.475) data 0.281 (0.345) loss_x loss_x 1.0449 (1.2440) acc_x 78.1250 (67.8125) lr 8.7467e-04 eta 0:00:00
epoch [110/200] batch [5/66] time 0.390 (0.473) data 0.258 (0.342) loss_u loss_u 0.8813 (0.9002) acc_u 15.6250 (13.1250) lr 8.7467e-04 eta 0:00:28
epoch [110/200] batch [10/66] time 0.337 (0.465) data 0.205 (0.334) loss_u loss_u 0.8076 (0.8873) acc_u 21.8750 (13.7500) lr 8.7467e-04 eta 0:00:26
epoch [110/200] batch [15/66] time 0.525 (0.463) data 0.394 (0.332) loss_u loss_u 0.9199 (0.8787) acc_u 6.2500 (14.5833) lr 8.7467e-04 eta 0:00:23
epoch [110/200] batch [20/66] time 0.430 (0.461) data 0.297 (0.330) loss_u loss_u 0.9087 (0.8802) acc_u 12.5000 (14.5312) lr 8.7467e-04 eta 0:00:21
epoch [110/200] batch [25/66] time 0.405 (0.459) data 0.274 (0.328) loss_u loss_u 0.8833 (0.8812) acc_u 12.5000 (14.6250) lr 8.7467e-04 eta 0:00:18
epoch [110/200] batch [30/66] time 0.461 (0.454) data 0.329 (0.323) loss_u loss_u 0.9160 (0.8814) acc_u 9.3750 (14.8958) lr 8.7467e-04 eta 0:00:16
epoch [110/200] batch [35/66] time 0.356 (0.453) data 0.224 (0.321) loss_u loss_u 0.8916 (0.8806) acc_u 15.6250 (15.0000) lr 8.7467e-04 eta 0:00:14
epoch [110/200] batch [40/66] time 0.687 (0.458) data 0.553 (0.327) loss_u loss_u 0.9141 (0.8815) acc_u 9.3750 (15.0781) lr 8.7467e-04 eta 0:00:11
epoch [110/200] batch [45/66] time 0.367 (0.459) data 0.235 (0.328) loss_u loss_u 0.8711 (0.8796) acc_u 18.7500 (15.4861) lr 8.7467e-04 eta 0:00:09
epoch [110/200] batch [50/66] time 0.345 (0.457) data 0.214 (0.325) loss_u loss_u 0.8408 (0.8775) acc_u 21.8750 (16.0000) lr 8.7467e-04 eta 0:00:07
epoch [110/200] batch [55/66] time 0.422 (0.457) data 0.290 (0.325) loss_u loss_u 0.8042 (0.8759) acc_u 21.8750 (16.0795) lr 8.7467e-04 eta 0:00:05
epoch [110/200] batch [60/66] time 0.393 (0.462) data 0.261 (0.331) loss_u loss_u 0.8794 (0.8756) acc_u 18.7500 (16.0938) lr 8.7467e-04 eta 0:00:02
epoch [110/200] batch [65/66] time 0.497 (0.464) data 0.366 (0.332) loss_u loss_u 0.8550 (0.8760) acc_u 21.8750 (16.0577) lr 8.7467e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1603
confident_label rate tensor(0.3294, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1033
clean true:886
clean false:147
clean_rate:0.8576960309777347
noisy true:647
noisy false:1456
after delete: len(clean_dataset) 1033
after delete: len(noisy_dataset) 2103
epoch [111/200] batch [5/32] time 0.434 (0.481) data 0.303 (0.350) loss_x loss_x 0.6543 (1.1615) acc_x 84.3750 (72.5000) lr 8.5910e-04 eta 0:00:12
epoch [111/200] batch [10/32] time 0.467 (0.497) data 0.335 (0.366) loss_x loss_x 1.2305 (1.1496) acc_x 65.6250 (72.1875) lr 8.5910e-04 eta 0:00:10
epoch [111/200] batch [15/32] time 0.481 (0.481) data 0.349 (0.350) loss_x loss_x 1.1230 (1.2433) acc_x 81.2500 (70.6250) lr 8.5910e-04 eta 0:00:08
epoch [111/200] batch [20/32] time 0.618 (0.486) data 0.487 (0.355) loss_x loss_x 1.3594 (1.2369) acc_x 56.2500 (70.6250) lr 8.5910e-04 eta 0:00:05
epoch [111/200] batch [25/32] time 0.419 (0.475) data 0.288 (0.345) loss_x loss_x 1.7383 (1.2785) acc_x 53.1250 (69.3750) lr 8.5910e-04 eta 0:00:03
epoch [111/200] batch [30/32] time 0.435 (0.474) data 0.305 (0.343) loss_x loss_x 0.9121 (1.2907) acc_x 71.8750 (68.4375) lr 8.5910e-04 eta 0:00:00
epoch [111/200] batch [5/65] time 0.385 (0.466) data 0.254 (0.335) loss_u loss_u 0.9224 (0.8720) acc_u 9.3750 (15.6250) lr 8.5910e-04 eta 0:00:27
epoch [111/200] batch [10/65] time 0.467 (0.467) data 0.335 (0.336) loss_u loss_u 0.8818 (0.8689) acc_u 12.5000 (15.3125) lr 8.5910e-04 eta 0:00:25
epoch [111/200] batch [15/65] time 0.388 (0.461) data 0.257 (0.330) loss_u loss_u 0.8994 (0.8727) acc_u 12.5000 (14.5833) lr 8.5910e-04 eta 0:00:23
epoch [111/200] batch [20/65] time 0.346 (0.461) data 0.216 (0.330) loss_u loss_u 0.8740 (0.8634) acc_u 18.7500 (16.8750) lr 8.5910e-04 eta 0:00:20
epoch [111/200] batch [25/65] time 0.644 (0.462) data 0.512 (0.330) loss_u loss_u 0.7866 (0.8635) acc_u 25.0000 (17.2500) lr 8.5910e-04 eta 0:00:18
epoch [111/200] batch [30/65] time 0.467 (0.463) data 0.335 (0.332) loss_u loss_u 0.8135 (0.8674) acc_u 18.7500 (16.2500) lr 8.5910e-04 eta 0:00:16
epoch [111/200] batch [35/65] time 0.385 (0.459) data 0.253 (0.328) loss_u loss_u 0.8989 (0.8692) acc_u 15.6250 (16.2500) lr 8.5910e-04 eta 0:00:13
epoch [111/200] batch [40/65] time 0.507 (0.462) data 0.375 (0.331) loss_u loss_u 0.9019 (0.8680) acc_u 15.6250 (16.4844) lr 8.5910e-04 eta 0:00:11
epoch [111/200] batch [45/65] time 0.597 (0.461) data 0.465 (0.330) loss_u loss_u 0.8965 (0.8696) acc_u 9.3750 (16.3194) lr 8.5910e-04 eta 0:00:09
epoch [111/200] batch [50/65] time 0.395 (0.460) data 0.264 (0.328) loss_u loss_u 0.9209 (0.8707) acc_u 6.2500 (16.1875) lr 8.5910e-04 eta 0:00:06
epoch [111/200] batch [55/65] time 0.346 (0.458) data 0.215 (0.327) loss_u loss_u 0.8960 (0.8731) acc_u 15.6250 (16.0227) lr 8.5910e-04 eta 0:00:04
epoch [111/200] batch [60/65] time 0.401 (0.457) data 0.270 (0.326) loss_u loss_u 0.9326 (0.8733) acc_u 9.3750 (15.9896) lr 8.5910e-04 eta 0:00:02
epoch [111/200] batch [65/65] time 0.446 (0.458) data 0.314 (0.326) loss_u loss_u 0.9062 (0.8751) acc_u 9.3750 (15.6731) lr 8.5910e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1589
confident_label rate tensor(0.3214, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1008
clean true:889
clean false:119
clean_rate:0.8819444444444444
noisy true:658
noisy false:1470
after delete: len(clean_dataset) 1008
after delete: len(noisy_dataset) 2128
epoch [112/200] batch [5/31] time 0.463 (0.498) data 0.332 (0.367) loss_x loss_x 1.4834 (1.2962) acc_x 59.3750 (71.2500) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [10/31] time 0.397 (0.476) data 0.267 (0.345) loss_x loss_x 1.7744 (1.3110) acc_x 62.5000 (70.9375) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [15/31] time 0.508 (0.476) data 0.377 (0.345) loss_x loss_x 0.6792 (1.2196) acc_x 87.5000 (71.4583) lr 8.4357e-04 eta 0:00:07
epoch [112/200] batch [20/31] time 0.497 (0.471) data 0.367 (0.340) loss_x loss_x 1.2793 (1.2407) acc_x 65.6250 (70.0000) lr 8.4357e-04 eta 0:00:05
epoch [112/200] batch [25/31] time 0.484 (0.461) data 0.353 (0.330) loss_x loss_x 1.1035 (1.2254) acc_x 75.0000 (70.5000) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [30/31] time 0.461 (0.474) data 0.330 (0.343) loss_x loss_x 1.3047 (1.2109) acc_x 62.5000 (70.2083) lr 8.4357e-04 eta 0:00:00
epoch [112/200] batch [5/66] time 0.397 (0.468) data 0.267 (0.338) loss_u loss_u 0.8066 (0.8417) acc_u 31.2500 (20.6250) lr 8.4357e-04 eta 0:00:28
epoch [112/200] batch [10/66] time 0.419 (0.460) data 0.289 (0.330) loss_u loss_u 0.8638 (0.8550) acc_u 18.7500 (17.5000) lr 8.4357e-04 eta 0:00:25
epoch [112/200] batch [15/66] time 0.405 (0.464) data 0.275 (0.334) loss_u loss_u 0.8862 (0.8601) acc_u 12.5000 (16.6667) lr 8.4357e-04 eta 0:00:23
epoch [112/200] batch [20/66] time 0.575 (0.471) data 0.444 (0.340) loss_u loss_u 0.8887 (0.8649) acc_u 12.5000 (16.4062) lr 8.4357e-04 eta 0:00:21
epoch [112/200] batch [25/66] time 0.595 (0.470) data 0.463 (0.339) loss_u loss_u 0.8950 (0.8656) acc_u 12.5000 (16.5000) lr 8.4357e-04 eta 0:00:19
epoch [112/200] batch [30/66] time 0.439 (0.469) data 0.307 (0.338) loss_u loss_u 0.9097 (0.8691) acc_u 12.5000 (16.4583) lr 8.4357e-04 eta 0:00:16
epoch [112/200] batch [35/66] time 0.549 (0.467) data 0.418 (0.336) loss_u loss_u 0.7725 (0.8711) acc_u 25.0000 (15.8929) lr 8.4357e-04 eta 0:00:14
epoch [112/200] batch [40/66] time 0.368 (0.463) data 0.234 (0.332) loss_u loss_u 0.9009 (0.8732) acc_u 12.5000 (15.7031) lr 8.4357e-04 eta 0:00:12
epoch [112/200] batch [45/66] time 0.584 (0.460) data 0.453 (0.329) loss_u loss_u 0.9014 (0.8762) acc_u 12.5000 (15.4861) lr 8.4357e-04 eta 0:00:09
epoch [112/200] batch [50/66] time 0.517 (0.459) data 0.384 (0.328) loss_u loss_u 0.8999 (0.8781) acc_u 15.6250 (15.1875) lr 8.4357e-04 eta 0:00:07
epoch [112/200] batch [55/66] time 0.530 (0.462) data 0.399 (0.331) loss_u loss_u 0.8569 (0.8758) acc_u 21.8750 (15.6250) lr 8.4357e-04 eta 0:00:05
epoch [112/200] batch [60/66] time 0.698 (0.465) data 0.567 (0.333) loss_u loss_u 0.8384 (0.8747) acc_u 18.7500 (15.8333) lr 8.4357e-04 eta 0:00:02
epoch [112/200] batch [65/66] time 0.388 (0.465) data 0.257 (0.334) loss_u loss_u 0.9028 (0.8773) acc_u 15.6250 (15.5288) lr 8.4357e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1608
confident_label rate tensor(0.3253, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1020
clean true:889
clean false:131
clean_rate:0.8715686274509804
noisy true:639
noisy false:1477
after delete: len(clean_dataset) 1020
after delete: len(noisy_dataset) 2116
epoch [113/200] batch [5/31] time 0.387 (0.476) data 0.256 (0.345) loss_x loss_x 1.1289 (1.2605) acc_x 62.5000 (63.7500) lr 8.2807e-04 eta 0:00:12
epoch [113/200] batch [10/31] time 0.369 (0.471) data 0.239 (0.340) loss_x loss_x 1.2188 (1.2439) acc_x 78.1250 (67.1875) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [15/31] time 0.408 (0.460) data 0.277 (0.329) loss_x loss_x 1.1299 (1.2375) acc_x 65.6250 (66.8750) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [20/31] time 0.499 (0.454) data 0.369 (0.323) loss_x loss_x 0.9917 (1.2042) acc_x 75.0000 (68.1250) lr 8.2807e-04 eta 0:00:04
epoch [113/200] batch [25/31] time 0.725 (0.477) data 0.594 (0.346) loss_x loss_x 1.3906 (1.1981) acc_x 62.5000 (68.3750) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [30/31] time 0.575 (0.486) data 0.444 (0.355) loss_x loss_x 1.1611 (1.1878) acc_x 78.1250 (70.3125) lr 8.2807e-04 eta 0:00:00
epoch [113/200] batch [5/66] time 0.399 (0.480) data 0.267 (0.349) loss_u loss_u 0.9082 (0.8652) acc_u 6.2500 (16.8750) lr 8.2807e-04 eta 0:00:29
epoch [113/200] batch [10/66] time 0.378 (0.470) data 0.248 (0.340) loss_u loss_u 0.9014 (0.8659) acc_u 12.5000 (16.2500) lr 8.2807e-04 eta 0:00:26
epoch [113/200] batch [15/66] time 0.318 (0.464) data 0.187 (0.333) loss_u loss_u 0.8735 (0.8700) acc_u 18.7500 (15.6250) lr 8.2807e-04 eta 0:00:23
epoch [113/200] batch [20/66] time 0.408 (0.457) data 0.276 (0.327) loss_u loss_u 0.8779 (0.8718) acc_u 18.7500 (15.9375) lr 8.2807e-04 eta 0:00:21
epoch [113/200] batch [25/66] time 0.516 (0.460) data 0.384 (0.329) loss_u loss_u 0.8018 (0.8673) acc_u 25.0000 (16.7500) lr 8.2807e-04 eta 0:00:18
epoch [113/200] batch [30/66] time 0.574 (0.463) data 0.443 (0.332) loss_u loss_u 0.9185 (0.8712) acc_u 6.2500 (16.0417) lr 8.2807e-04 eta 0:00:16
epoch [113/200] batch [35/66] time 0.367 (0.459) data 0.236 (0.328) loss_u loss_u 0.8242 (0.8675) acc_u 18.7500 (16.6071) lr 8.2807e-04 eta 0:00:14
epoch [113/200] batch [40/66] time 0.396 (0.459) data 0.264 (0.328) loss_u loss_u 0.8896 (0.8680) acc_u 12.5000 (16.7188) lr 8.2807e-04 eta 0:00:11
epoch [113/200] batch [45/66] time 0.378 (0.460) data 0.247 (0.329) loss_u loss_u 0.8926 (0.8670) acc_u 12.5000 (16.7361) lr 8.2807e-04 eta 0:00:09
epoch [113/200] batch [50/66] time 0.412 (0.458) data 0.282 (0.327) loss_u loss_u 0.9194 (0.8667) acc_u 15.6250 (16.9375) lr 8.2807e-04 eta 0:00:07
epoch [113/200] batch [55/66] time 0.414 (0.459) data 0.283 (0.328) loss_u loss_u 0.8945 (0.8685) acc_u 12.5000 (16.8182) lr 8.2807e-04 eta 0:00:05
epoch [113/200] batch [60/66] time 0.665 (0.460) data 0.535 (0.329) loss_u loss_u 0.8799 (0.8695) acc_u 12.5000 (16.6146) lr 8.2807e-04 eta 0:00:02
epoch [113/200] batch [65/66] time 0.410 (0.464) data 0.279 (0.333) loss_u loss_u 0.8970 (0.8699) acc_u 15.6250 (16.5865) lr 8.2807e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1576
confident_label rate tensor(0.3186, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 999
clean true:863
clean false:136
clean_rate:0.8638638638638638
noisy true:697
noisy false:1440
after delete: len(clean_dataset) 999
after delete: len(noisy_dataset) 2137
epoch [114/200] batch [5/31] time 0.502 (0.540) data 0.371 (0.409) loss_x loss_x 1.4443 (1.3353) acc_x 56.2500 (67.5000) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [10/31] time 0.521 (0.513) data 0.391 (0.382) loss_x loss_x 1.2041 (1.3423) acc_x 78.1250 (67.5000) lr 8.1262e-04 eta 0:00:10
epoch [114/200] batch [15/31] time 0.443 (0.496) data 0.312 (0.365) loss_x loss_x 1.1494 (1.2536) acc_x 75.0000 (69.7917) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [20/31] time 0.432 (0.497) data 0.302 (0.366) loss_x loss_x 1.0537 (1.2174) acc_x 75.0000 (69.5312) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [25/31] time 0.394 (0.488) data 0.264 (0.358) loss_x loss_x 1.3906 (1.2119) acc_x 62.5000 (68.7500) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [30/31] time 0.465 (0.489) data 0.334 (0.358) loss_x loss_x 1.3809 (1.1971) acc_x 71.8750 (69.1667) lr 8.1262e-04 eta 0:00:00
epoch [114/200] batch [5/66] time 0.479 (0.480) data 0.348 (0.349) loss_u loss_u 0.8652 (0.8705) acc_u 21.8750 (17.5000) lr 8.1262e-04 eta 0:00:29
epoch [114/200] batch [10/66] time 0.436 (0.477) data 0.305 (0.346) loss_u loss_u 0.8540 (0.8576) acc_u 15.6250 (17.8125) lr 8.1262e-04 eta 0:00:26
epoch [114/200] batch [15/66] time 0.430 (0.472) data 0.298 (0.342) loss_u loss_u 0.8179 (0.8579) acc_u 25.0000 (17.9167) lr 8.1262e-04 eta 0:00:24
epoch [114/200] batch [20/66] time 0.502 (0.473) data 0.370 (0.342) loss_u loss_u 0.8887 (0.8638) acc_u 9.3750 (16.8750) lr 8.1262e-04 eta 0:00:21
epoch [114/200] batch [25/66] time 0.469 (0.471) data 0.338 (0.340) loss_u loss_u 0.9443 (0.8738) acc_u 12.5000 (15.3750) lr 8.1262e-04 eta 0:00:19
epoch [114/200] batch [30/66] time 0.481 (0.473) data 0.350 (0.342) loss_u loss_u 0.8984 (0.8790) acc_u 12.5000 (14.6875) lr 8.1262e-04 eta 0:00:17
epoch [114/200] batch [35/66] time 0.436 (0.472) data 0.306 (0.341) loss_u loss_u 0.9375 (0.8792) acc_u 9.3750 (14.4643) lr 8.1262e-04 eta 0:00:14
epoch [114/200] batch [40/66] time 0.543 (0.470) data 0.412 (0.340) loss_u loss_u 0.8745 (0.8782) acc_u 12.5000 (14.5312) lr 8.1262e-04 eta 0:00:12
epoch [114/200] batch [45/66] time 0.360 (0.469) data 0.230 (0.338) loss_u loss_u 0.8984 (0.8777) acc_u 15.6250 (14.6528) lr 8.1262e-04 eta 0:00:09
epoch [114/200] batch [50/66] time 0.398 (0.465) data 0.266 (0.334) loss_u loss_u 0.9102 (0.8764) acc_u 12.5000 (15.0625) lr 8.1262e-04 eta 0:00:07
epoch [114/200] batch [55/66] time 0.325 (0.463) data 0.193 (0.332) loss_u loss_u 0.8701 (0.8763) acc_u 15.6250 (15.0568) lr 8.1262e-04 eta 0:00:05
epoch [114/200] batch [60/66] time 0.350 (0.462) data 0.218 (0.331) loss_u loss_u 0.8711 (0.8761) acc_u 12.5000 (14.8958) lr 8.1262e-04 eta 0:00:02
epoch [114/200] batch [65/66] time 0.527 (0.466) data 0.396 (0.335) loss_u loss_u 0.8945 (0.8758) acc_u 12.5000 (15.0481) lr 8.1262e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1595
confident_label rate tensor(0.3253, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1020
clean true:896
clean false:124
clean_rate:0.8784313725490196
noisy true:645
noisy false:1471
after delete: len(clean_dataset) 1020
after delete: len(noisy_dataset) 2116
epoch [115/200] batch [5/31] time 0.572 (0.535) data 0.440 (0.404) loss_x loss_x 0.5083 (0.8953) acc_x 87.5000 (78.7500) lr 7.9721e-04 eta 0:00:13
epoch [115/200] batch [10/31] time 0.512 (0.550) data 0.381 (0.420) loss_x loss_x 1.0400 (1.0878) acc_x 78.1250 (74.3750) lr 7.9721e-04 eta 0:00:11
epoch [115/200] batch [15/31] time 0.435 (0.510) data 0.304 (0.379) loss_x loss_x 1.5039 (1.1407) acc_x 56.2500 (72.5000) lr 7.9721e-04 eta 0:00:08
epoch [115/200] batch [20/31] time 0.569 (0.512) data 0.437 (0.381) loss_x loss_x 1.6113 (1.1844) acc_x 68.7500 (72.0312) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [25/31] time 0.510 (0.509) data 0.380 (0.378) loss_x loss_x 1.3477 (1.1666) acc_x 56.2500 (71.3750) lr 7.9721e-04 eta 0:00:03
epoch [115/200] batch [30/31] time 0.503 (0.500) data 0.373 (0.370) loss_x loss_x 0.9878 (1.1659) acc_x 78.1250 (72.0833) lr 7.9721e-04 eta 0:00:00
epoch [115/200] batch [5/66] time 0.426 (0.494) data 0.295 (0.364) loss_u loss_u 0.8545 (0.8603) acc_u 21.8750 (18.7500) lr 7.9721e-04 eta 0:00:30
epoch [115/200] batch [10/66] time 0.323 (0.488) data 0.191 (0.357) loss_u loss_u 0.8926 (0.8702) acc_u 6.2500 (15.9375) lr 7.9721e-04 eta 0:00:27
epoch [115/200] batch [15/66] time 0.464 (0.486) data 0.333 (0.355) loss_u loss_u 0.8101 (0.8754) acc_u 18.7500 (14.5833) lr 7.9721e-04 eta 0:00:24
epoch [115/200] batch [20/66] time 0.440 (0.477) data 0.309 (0.346) loss_u loss_u 0.8564 (0.8799) acc_u 18.7500 (14.3750) lr 7.9721e-04 eta 0:00:21
epoch [115/200] batch [25/66] time 0.377 (0.470) data 0.245 (0.339) loss_u loss_u 0.8691 (0.8810) acc_u 15.6250 (14.1250) lr 7.9721e-04 eta 0:00:19
epoch [115/200] batch [30/66] time 0.500 (0.471) data 0.368 (0.340) loss_u loss_u 0.8867 (0.8743) acc_u 15.6250 (15.0000) lr 7.9721e-04 eta 0:00:16
epoch [115/200] batch [35/66] time 0.468 (0.474) data 0.336 (0.342) loss_u loss_u 0.8169 (0.8765) acc_u 25.0000 (14.5536) lr 7.9721e-04 eta 0:00:14
epoch [115/200] batch [40/66] time 0.423 (0.471) data 0.291 (0.340) loss_u loss_u 0.7808 (0.8771) acc_u 34.3750 (14.6094) lr 7.9721e-04 eta 0:00:12
epoch [115/200] batch [45/66] time 0.421 (0.470) data 0.287 (0.339) loss_u loss_u 0.8418 (0.8787) acc_u 18.7500 (14.3750) lr 7.9721e-04 eta 0:00:09
epoch [115/200] batch [50/66] time 0.365 (0.471) data 0.235 (0.340) loss_u loss_u 0.8970 (0.8796) acc_u 18.7500 (14.6250) lr 7.9721e-04 eta 0:00:07
epoch [115/200] batch [55/66] time 0.507 (0.469) data 0.376 (0.338) loss_u loss_u 0.9058 (0.8748) acc_u 9.3750 (14.9432) lr 7.9721e-04 eta 0:00:05
epoch [115/200] batch [60/66] time 0.480 (0.467) data 0.348 (0.336) loss_u loss_u 0.7959 (0.8726) acc_u 21.8750 (15.1562) lr 7.9721e-04 eta 0:00:02
epoch [115/200] batch [65/66] time 0.500 (0.464) data 0.369 (0.333) loss_u loss_u 0.9307 (0.8744) acc_u 9.3750 (15.0000) lr 7.9721e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1547
confident_label rate tensor(0.3316, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1040
clean true:909
clean false:131
clean_rate:0.8740384615384615
noisy true:680
noisy false:1416
after delete: len(clean_dataset) 1040
after delete: len(noisy_dataset) 2096
epoch [116/200] batch [5/32] time 0.398 (0.487) data 0.268 (0.356) loss_x loss_x 0.9277 (0.8993) acc_x 71.8750 (73.1250) lr 7.8186e-04 eta 0:00:13
epoch [116/200] batch [10/32] time 0.476 (0.474) data 0.345 (0.342) loss_x loss_x 1.0410 (1.1708) acc_x 68.7500 (70.0000) lr 7.8186e-04 eta 0:00:10
epoch [116/200] batch [15/32] time 0.423 (0.465) data 0.293 (0.334) loss_x loss_x 0.8145 (1.1193) acc_x 78.1250 (71.0417) lr 7.8186e-04 eta 0:00:07
epoch [116/200] batch [20/32] time 0.433 (0.463) data 0.302 (0.332) loss_x loss_x 1.4307 (1.1028) acc_x 65.6250 (71.8750) lr 7.8186e-04 eta 0:00:05
epoch [116/200] batch [25/32] time 0.432 (0.470) data 0.301 (0.339) loss_x loss_x 0.7207 (1.0643) acc_x 81.2500 (72.8750) lr 7.8186e-04 eta 0:00:03
epoch [116/200] batch [30/32] time 0.454 (0.484) data 0.324 (0.354) loss_x loss_x 1.1963 (1.0862) acc_x 71.8750 (71.8750) lr 7.8186e-04 eta 0:00:00
epoch [116/200] batch [5/65] time 0.349 (0.487) data 0.219 (0.356) loss_u loss_u 0.8223 (0.8709) acc_u 18.7500 (15.6250) lr 7.8186e-04 eta 0:00:29
epoch [116/200] batch [10/65] time 0.506 (0.481) data 0.375 (0.350) loss_u loss_u 0.8765 (0.8613) acc_u 18.7500 (17.1875) lr 7.8186e-04 eta 0:00:26
epoch [116/200] batch [15/65] time 0.402 (0.473) data 0.271 (0.343) loss_u loss_u 0.8672 (0.8653) acc_u 12.5000 (16.6667) lr 7.8186e-04 eta 0:00:23
epoch [116/200] batch [20/65] time 0.496 (0.481) data 0.365 (0.350) loss_u loss_u 0.9341 (0.8709) acc_u 6.2500 (16.2500) lr 7.8186e-04 eta 0:00:21
epoch [116/200] batch [25/65] time 0.403 (0.477) data 0.270 (0.346) loss_u loss_u 0.9097 (0.8723) acc_u 12.5000 (15.7500) lr 7.8186e-04 eta 0:00:19
epoch [116/200] batch [30/65] time 0.450 (0.474) data 0.318 (0.343) loss_u loss_u 0.8760 (0.8758) acc_u 12.5000 (15.1042) lr 7.8186e-04 eta 0:00:16
epoch [116/200] batch [35/65] time 0.511 (0.472) data 0.381 (0.341) loss_u loss_u 0.8135 (0.8780) acc_u 25.0000 (14.8214) lr 7.8186e-04 eta 0:00:14
epoch [116/200] batch [40/65] time 0.405 (0.468) data 0.274 (0.337) loss_u loss_u 0.9551 (0.8794) acc_u 3.1250 (14.8438) lr 7.8186e-04 eta 0:00:11
epoch [116/200] batch [45/65] time 0.487 (0.468) data 0.355 (0.337) loss_u loss_u 0.7964 (0.8762) acc_u 25.0000 (15.3472) lr 7.8186e-04 eta 0:00:09
epoch [116/200] batch [50/65] time 0.433 (0.466) data 0.302 (0.335) loss_u loss_u 0.8379 (0.8747) acc_u 21.8750 (15.5625) lr 7.8186e-04 eta 0:00:06
epoch [116/200] batch [55/65] time 0.435 (0.464) data 0.304 (0.333) loss_u loss_u 0.9014 (0.8769) acc_u 12.5000 (15.2273) lr 7.8186e-04 eta 0:00:04
epoch [116/200] batch [60/65] time 0.420 (0.464) data 0.289 (0.333) loss_u loss_u 0.9307 (0.8786) acc_u 9.3750 (15.0521) lr 7.8186e-04 eta 0:00:02
epoch [116/200] batch [65/65] time 0.468 (0.461) data 0.336 (0.330) loss_u loss_u 0.8838 (0.8774) acc_u 12.5000 (15.1442) lr 7.8186e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1598
confident_label rate tensor(0.3221, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1010
clean true:889
clean false:121
clean_rate:0.8801980198019802
noisy true:649
noisy false:1477
after delete: len(clean_dataset) 1010
after delete: len(noisy_dataset) 2126
epoch [117/200] batch [5/31] time 0.431 (0.416) data 0.301 (0.285) loss_x loss_x 1.1562 (1.3656) acc_x 65.6250 (64.3750) lr 7.6655e-04 eta 0:00:10
epoch [117/200] batch [10/31] time 0.503 (0.449) data 0.373 (0.319) loss_x loss_x 1.0771 (1.1990) acc_x 75.0000 (68.7500) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [15/31] time 0.449 (0.459) data 0.319 (0.328) loss_x loss_x 1.0166 (1.2172) acc_x 75.0000 (70.2083) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [20/31] time 0.435 (0.459) data 0.305 (0.328) loss_x loss_x 1.6963 (1.2010) acc_x 68.7500 (70.6250) lr 7.6655e-04 eta 0:00:05
epoch [117/200] batch [25/31] time 0.378 (0.455) data 0.248 (0.325) loss_x loss_x 0.9683 (1.1823) acc_x 75.0000 (70.5000) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [30/31] time 0.423 (0.457) data 0.292 (0.326) loss_x loss_x 1.3350 (1.1858) acc_x 56.2500 (69.8958) lr 7.6655e-04 eta 0:00:00
epoch [117/200] batch [5/66] time 0.442 (0.453) data 0.311 (0.322) loss_u loss_u 0.8862 (0.8702) acc_u 9.3750 (16.8750) lr 7.6655e-04 eta 0:00:27
epoch [117/200] batch [10/66] time 0.542 (0.455) data 0.411 (0.324) loss_u loss_u 0.8481 (0.8649) acc_u 18.7500 (16.8750) lr 7.6655e-04 eta 0:00:25
epoch [117/200] batch [15/66] time 0.407 (0.452) data 0.277 (0.322) loss_u loss_u 0.9419 (0.8806) acc_u 6.2500 (14.3750) lr 7.6655e-04 eta 0:00:23
epoch [117/200] batch [20/66] time 0.457 (0.456) data 0.325 (0.325) loss_u loss_u 0.9292 (0.8819) acc_u 6.2500 (13.5938) lr 7.6655e-04 eta 0:00:20
epoch [117/200] batch [25/66] time 0.544 (0.456) data 0.414 (0.325) loss_u loss_u 0.9189 (0.8781) acc_u 9.3750 (14.5000) lr 7.6655e-04 eta 0:00:18
epoch [117/200] batch [30/66] time 0.346 (0.455) data 0.215 (0.324) loss_u loss_u 0.9077 (0.8831) acc_u 12.5000 (14.1667) lr 7.6655e-04 eta 0:00:16
epoch [117/200] batch [35/66] time 0.482 (0.457) data 0.351 (0.326) loss_u loss_u 0.8906 (0.8778) acc_u 15.6250 (14.9107) lr 7.6655e-04 eta 0:00:14
epoch [117/200] batch [40/66] time 0.510 (0.457) data 0.378 (0.326) loss_u loss_u 0.9438 (0.8795) acc_u 6.2500 (14.7656) lr 7.6655e-04 eta 0:00:11
epoch [117/200] batch [45/66] time 0.477 (0.456) data 0.345 (0.325) loss_u loss_u 0.8354 (0.8778) acc_u 21.8750 (15.0694) lr 7.6655e-04 eta 0:00:09
epoch [117/200] batch [50/66] time 0.440 (0.456) data 0.309 (0.325) loss_u loss_u 0.8223 (0.8781) acc_u 18.7500 (14.9375) lr 7.6655e-04 eta 0:00:07
epoch [117/200] batch [55/66] time 0.447 (0.453) data 0.315 (0.322) loss_u loss_u 0.8232 (0.8766) acc_u 25.0000 (15.2841) lr 7.6655e-04 eta 0:00:04
epoch [117/200] batch [60/66] time 0.342 (0.455) data 0.211 (0.324) loss_u loss_u 0.9141 (0.8778) acc_u 12.5000 (14.8958) lr 7.6655e-04 eta 0:00:02
epoch [117/200] batch [65/66] time 0.509 (0.456) data 0.377 (0.325) loss_u loss_u 0.7949 (0.8747) acc_u 18.7500 (15.2885) lr 7.6655e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1610
confident_label rate tensor(0.3262, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1023
clean true:896
clean false:127
clean_rate:0.8758553274682307
noisy true:630
noisy false:1483
after delete: len(clean_dataset) 1023
after delete: len(noisy_dataset) 2113
epoch [118/200] batch [5/31] time 0.441 (0.436) data 0.310 (0.305) loss_x loss_x 1.5674 (1.2876) acc_x 65.6250 (66.2500) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [10/31] time 0.424 (0.451) data 0.293 (0.320) loss_x loss_x 1.3145 (1.2675) acc_x 68.7500 (68.1250) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [15/31] time 0.383 (0.432) data 0.252 (0.302) loss_x loss_x 1.0967 (1.2702) acc_x 81.2500 (68.7500) lr 7.5131e-04 eta 0:00:06
epoch [118/200] batch [20/31] time 0.483 (0.437) data 0.353 (0.306) loss_x loss_x 1.1172 (1.2476) acc_x 78.1250 (70.1562) lr 7.5131e-04 eta 0:00:04
epoch [118/200] batch [25/31] time 0.405 (0.444) data 0.275 (0.314) loss_x loss_x 1.3438 (1.2233) acc_x 59.3750 (70.3750) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [30/31] time 0.518 (0.449) data 0.389 (0.319) loss_x loss_x 1.4814 (1.2534) acc_x 68.7500 (69.8958) lr 7.5131e-04 eta 0:00:00
epoch [118/200] batch [5/66] time 0.440 (0.445) data 0.309 (0.314) loss_u loss_u 0.8271 (0.8641) acc_u 21.8750 (18.7500) lr 7.5131e-04 eta 0:00:27
epoch [118/200] batch [10/66] time 0.412 (0.438) data 0.280 (0.308) loss_u loss_u 0.8628 (0.8681) acc_u 12.5000 (16.5625) lr 7.5131e-04 eta 0:00:24
epoch [118/200] batch [15/66] time 0.350 (0.442) data 0.219 (0.311) loss_u loss_u 0.8789 (0.8729) acc_u 15.6250 (14.7917) lr 7.5131e-04 eta 0:00:22
epoch [118/200] batch [20/66] time 0.446 (0.446) data 0.315 (0.315) loss_u loss_u 0.8608 (0.8834) acc_u 21.8750 (13.9062) lr 7.5131e-04 eta 0:00:20
epoch [118/200] batch [25/66] time 0.428 (0.445) data 0.296 (0.314) loss_u loss_u 0.9541 (0.8847) acc_u 6.2500 (13.8750) lr 7.5131e-04 eta 0:00:18
epoch [118/200] batch [30/66] time 0.532 (0.448) data 0.401 (0.317) loss_u loss_u 0.9131 (0.8888) acc_u 9.3750 (13.0208) lr 7.5131e-04 eta 0:00:16
epoch [118/200] batch [35/66] time 0.427 (0.453) data 0.295 (0.322) loss_u loss_u 0.8315 (0.8859) acc_u 18.7500 (13.5714) lr 7.5131e-04 eta 0:00:14
epoch [118/200] batch [40/66] time 0.424 (0.454) data 0.293 (0.323) loss_u loss_u 0.8525 (0.8858) acc_u 15.6250 (13.5156) lr 7.5131e-04 eta 0:00:11
epoch [118/200] batch [45/66] time 0.537 (0.454) data 0.406 (0.323) loss_u loss_u 0.8555 (0.8832) acc_u 15.6250 (13.6806) lr 7.5131e-04 eta 0:00:09
epoch [118/200] batch [50/66] time 0.528 (0.452) data 0.397 (0.321) loss_u loss_u 0.8818 (0.8807) acc_u 25.0000 (14.6875) lr 7.5131e-04 eta 0:00:07
epoch [118/200] batch [55/66] time 0.470 (0.455) data 0.339 (0.324) loss_u loss_u 0.8750 (0.8798) acc_u 12.5000 (14.7727) lr 7.5131e-04 eta 0:00:05
epoch [118/200] batch [60/66] time 0.509 (0.457) data 0.377 (0.326) loss_u loss_u 0.8306 (0.8801) acc_u 21.8750 (14.7917) lr 7.5131e-04 eta 0:00:02
epoch [118/200] batch [65/66] time 0.625 (0.458) data 0.495 (0.327) loss_u loss_u 0.8687 (0.8805) acc_u 21.8750 (14.7596) lr 7.5131e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1589
confident_label rate tensor(0.3361, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1054
clean true:907
clean false:147
clean_rate:0.8605313092979127
noisy true:640
noisy false:1442
after delete: len(clean_dataset) 1054
after delete: len(noisy_dataset) 2082
epoch [119/200] batch [5/32] time 0.534 (0.495) data 0.404 (0.364) loss_x loss_x 1.0254 (1.0457) acc_x 78.1250 (75.0000) lr 7.3613e-04 eta 0:00:13
epoch [119/200] batch [10/32] time 0.435 (0.473) data 0.304 (0.342) loss_x loss_x 1.5898 (1.1545) acc_x 62.5000 (74.0625) lr 7.3613e-04 eta 0:00:10
epoch [119/200] batch [15/32] time 0.537 (0.462) data 0.405 (0.332) loss_x loss_x 1.0537 (1.1869) acc_x 68.7500 (71.8750) lr 7.3613e-04 eta 0:00:07
epoch [119/200] batch [20/32] time 0.503 (0.477) data 0.373 (0.347) loss_x loss_x 1.1904 (1.1380) acc_x 65.6250 (72.8125) lr 7.3613e-04 eta 0:00:05
epoch [119/200] batch [25/32] time 0.533 (0.473) data 0.402 (0.342) loss_x loss_x 1.8584 (1.1756) acc_x 59.3750 (71.5000) lr 7.3613e-04 eta 0:00:03
epoch [119/200] batch [30/32] time 0.528 (0.478) data 0.395 (0.347) loss_x loss_x 0.7803 (1.1517) acc_x 71.8750 (71.7708) lr 7.3613e-04 eta 0:00:00
epoch [119/200] batch [5/65] time 0.500 (0.481) data 0.369 (0.350) loss_u loss_u 0.8770 (0.8894) acc_u 15.6250 (16.8750) lr 7.3613e-04 eta 0:00:28
epoch [119/200] batch [10/65] time 0.553 (0.480) data 0.423 (0.349) loss_u loss_u 0.8403 (0.8811) acc_u 18.7500 (15.0000) lr 7.3613e-04 eta 0:00:26
epoch [119/200] batch [15/65] time 0.493 (0.486) data 0.363 (0.355) loss_u loss_u 0.8970 (0.8816) acc_u 12.5000 (15.8333) lr 7.3613e-04 eta 0:00:24
epoch [119/200] batch [20/65] time 0.436 (0.480) data 0.305 (0.349) loss_u loss_u 0.8555 (0.8746) acc_u 18.7500 (16.5625) lr 7.3613e-04 eta 0:00:21
epoch [119/200] batch [25/65] time 0.499 (0.474) data 0.369 (0.343) loss_u loss_u 0.8862 (0.8766) acc_u 18.7500 (16.6250) lr 7.3613e-04 eta 0:00:18
epoch [119/200] batch [30/65] time 0.455 (0.477) data 0.323 (0.346) loss_u loss_u 0.9175 (0.8764) acc_u 12.5000 (16.5625) lr 7.3613e-04 eta 0:00:16
epoch [119/200] batch [35/65] time 0.422 (0.477) data 0.291 (0.347) loss_u loss_u 0.8750 (0.8772) acc_u 18.7500 (16.4286) lr 7.3613e-04 eta 0:00:14
epoch [119/200] batch [40/65] time 0.423 (0.475) data 0.291 (0.344) loss_u loss_u 0.8794 (0.8798) acc_u 15.6250 (15.9375) lr 7.3613e-04 eta 0:00:11
epoch [119/200] batch [45/65] time 0.543 (0.473) data 0.411 (0.342) loss_u loss_u 0.8213 (0.8775) acc_u 21.8750 (16.1111) lr 7.3613e-04 eta 0:00:09
epoch [119/200] batch [50/65] time 0.440 (0.472) data 0.310 (0.341) loss_u loss_u 0.8525 (0.8780) acc_u 21.8750 (15.9375) lr 7.3613e-04 eta 0:00:07
epoch [119/200] batch [55/65] time 0.534 (0.474) data 0.403 (0.343) loss_u loss_u 0.9077 (0.8775) acc_u 18.7500 (15.9659) lr 7.3613e-04 eta 0:00:04
epoch [119/200] batch [60/65] time 0.489 (0.473) data 0.356 (0.342) loss_u loss_u 0.9150 (0.8778) acc_u 12.5000 (15.8854) lr 7.3613e-04 eta 0:00:02
epoch [119/200] batch [65/65] time 0.403 (0.472) data 0.271 (0.341) loss_u loss_u 0.8457 (0.8775) acc_u 18.7500 (15.9615) lr 7.3613e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1599
confident_label rate tensor(0.3275, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1027
clean true:889
clean false:138
clean_rate:0.8656280428432327
noisy true:648
noisy false:1461
after delete: len(clean_dataset) 1027
after delete: len(noisy_dataset) 2109
epoch [120/200] batch [5/32] time 0.398 (0.448) data 0.267 (0.318) loss_x loss_x 1.1816 (1.1377) acc_x 71.8750 (70.6250) lr 7.2101e-04 eta 0:00:12
epoch [120/200] batch [10/32] time 0.573 (0.473) data 0.443 (0.343) loss_x loss_x 1.2363 (1.2083) acc_x 71.8750 (69.3750) lr 7.2101e-04 eta 0:00:10
epoch [120/200] batch [15/32] time 0.400 (0.458) data 0.269 (0.328) loss_x loss_x 0.9663 (1.1481) acc_x 65.6250 (70.0000) lr 7.2101e-04 eta 0:00:07
epoch [120/200] batch [20/32] time 0.591 (0.462) data 0.459 (0.331) loss_x loss_x 0.8760 (1.2298) acc_x 81.2500 (68.2812) lr 7.2101e-04 eta 0:00:05
epoch [120/200] batch [25/32] time 0.362 (0.460) data 0.231 (0.329) loss_x loss_x 1.3965 (1.2097) acc_x 59.3750 (68.8750) lr 7.2101e-04 eta 0:00:03
epoch [120/200] batch [30/32] time 0.435 (0.474) data 0.304 (0.343) loss_x loss_x 0.9849 (1.2281) acc_x 81.2500 (68.8542) lr 7.2101e-04 eta 0:00:00
epoch [120/200] batch [5/65] time 0.504 (0.465) data 0.374 (0.334) loss_u loss_u 0.8125 (0.8556) acc_u 25.0000 (20.0000) lr 7.2101e-04 eta 0:00:27
epoch [120/200] batch [10/65] time 0.392 (0.459) data 0.261 (0.327) loss_u loss_u 0.8623 (0.8711) acc_u 15.6250 (16.2500) lr 7.2101e-04 eta 0:00:25
epoch [120/200] batch [15/65] time 0.438 (0.458) data 0.307 (0.327) loss_u loss_u 0.8584 (0.8751) acc_u 18.7500 (15.8333) lr 7.2101e-04 eta 0:00:22
epoch [120/200] batch [20/65] time 0.475 (0.455) data 0.341 (0.324) loss_u loss_u 0.9019 (0.8705) acc_u 12.5000 (16.2500) lr 7.2101e-04 eta 0:00:20
epoch [120/200] batch [25/65] time 0.342 (0.453) data 0.210 (0.322) loss_u loss_u 0.9038 (0.8764) acc_u 18.7500 (15.6250) lr 7.2101e-04 eta 0:00:18
epoch [120/200] batch [30/65] time 0.461 (0.450) data 0.330 (0.318) loss_u loss_u 0.9165 (0.8753) acc_u 12.5000 (15.5208) lr 7.2101e-04 eta 0:00:15
epoch [120/200] batch [35/65] time 0.402 (0.449) data 0.270 (0.317) loss_u loss_u 0.9336 (0.8756) acc_u 6.2500 (15.3571) lr 7.2101e-04 eta 0:00:13
epoch [120/200] batch [40/65] time 0.347 (0.448) data 0.215 (0.317) loss_u loss_u 0.8110 (0.8748) acc_u 28.1250 (15.4688) lr 7.2101e-04 eta 0:00:11
epoch [120/200] batch [45/65] time 0.424 (0.454) data 0.293 (0.323) loss_u loss_u 0.9272 (0.8749) acc_u 12.5000 (15.6944) lr 7.2101e-04 eta 0:00:09
epoch [120/200] batch [50/65] time 0.336 (0.453) data 0.203 (0.322) loss_u loss_u 0.8853 (0.8743) acc_u 12.5000 (15.8125) lr 7.2101e-04 eta 0:00:06
epoch [120/200] batch [55/65] time 0.509 (0.457) data 0.377 (0.325) loss_u loss_u 0.9067 (0.8746) acc_u 12.5000 (15.9659) lr 7.2101e-04 eta 0:00:04
epoch [120/200] batch [60/65] time 0.472 (0.456) data 0.340 (0.325) loss_u loss_u 0.8579 (0.8743) acc_u 21.8750 (15.8333) lr 7.2101e-04 eta 0:00:02
epoch [120/200] batch [65/65] time 0.365 (0.456) data 0.232 (0.324) loss_u loss_u 0.9419 (0.8757) acc_u 6.2500 (15.6250) lr 7.2101e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1599
confident_label rate tensor(0.3227, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1012
clean true:884
clean false:128
clean_rate:0.8735177865612648
noisy true:653
noisy false:1471
after delete: len(clean_dataset) 1012
after delete: len(noisy_dataset) 2124
epoch [121/200] batch [5/31] time 0.461 (0.459) data 0.329 (0.328) loss_x loss_x 1.7539 (1.2581) acc_x 59.3750 (70.0000) lr 7.0596e-04 eta 0:00:11
epoch [121/200] batch [10/31] time 0.490 (0.475) data 0.360 (0.344) loss_x loss_x 1.3262 (1.2604) acc_x 75.0000 (69.6875) lr 7.0596e-04 eta 0:00:09
epoch [121/200] batch [15/31] time 0.374 (0.466) data 0.242 (0.335) loss_x loss_x 1.1201 (1.2394) acc_x 71.8750 (70.6250) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [20/31] time 0.520 (0.482) data 0.389 (0.351) loss_x loss_x 1.1689 (1.1938) acc_x 65.6250 (71.2500) lr 7.0596e-04 eta 0:00:05
epoch [121/200] batch [25/31] time 0.505 (0.480) data 0.373 (0.349) loss_x loss_x 1.7764 (1.2061) acc_x 62.5000 (70.5000) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [30/31] time 0.420 (0.475) data 0.289 (0.344) loss_x loss_x 1.2139 (1.2443) acc_x 78.1250 (69.6875) lr 7.0596e-04 eta 0:00:00
epoch [121/200] batch [5/66] time 0.455 (0.475) data 0.324 (0.344) loss_u loss_u 0.8569 (0.8456) acc_u 18.7500 (16.8750) lr 7.0596e-04 eta 0:00:28
epoch [121/200] batch [10/66] time 0.526 (0.481) data 0.396 (0.350) loss_u loss_u 0.9419 (0.8635) acc_u 6.2500 (16.2500) lr 7.0596e-04 eta 0:00:26
epoch [121/200] batch [15/66] time 0.433 (0.483) data 0.301 (0.352) loss_u loss_u 0.8774 (0.8675) acc_u 18.7500 (15.8333) lr 7.0596e-04 eta 0:00:24
epoch [121/200] batch [20/66] time 0.466 (0.483) data 0.335 (0.352) loss_u loss_u 0.8149 (0.8657) acc_u 28.1250 (15.9375) lr 7.0596e-04 eta 0:00:22
epoch [121/200] batch [25/66] time 0.434 (0.478) data 0.303 (0.346) loss_u loss_u 0.8218 (0.8662) acc_u 25.0000 (16.0000) lr 7.0596e-04 eta 0:00:19
epoch [121/200] batch [30/66] time 0.490 (0.478) data 0.360 (0.347) loss_u loss_u 0.9258 (0.8704) acc_u 9.3750 (15.3125) lr 7.0596e-04 eta 0:00:17
epoch [121/200] batch [35/66] time 0.480 (0.474) data 0.348 (0.343) loss_u loss_u 0.8286 (0.8676) acc_u 21.8750 (15.6250) lr 7.0596e-04 eta 0:00:14
epoch [121/200] batch [40/66] time 0.610 (0.473) data 0.479 (0.342) loss_u loss_u 0.8823 (0.8701) acc_u 12.5000 (15.3906) lr 7.0596e-04 eta 0:00:12
epoch [121/200] batch [45/66] time 0.600 (0.476) data 0.468 (0.345) loss_u loss_u 0.9131 (0.8742) acc_u 9.3750 (14.9306) lr 7.0596e-04 eta 0:00:09
epoch [121/200] batch [50/66] time 0.745 (0.476) data 0.614 (0.345) loss_u loss_u 0.8887 (0.8767) acc_u 12.5000 (14.5625) lr 7.0596e-04 eta 0:00:07
epoch [121/200] batch [55/66] time 0.531 (0.474) data 0.401 (0.343) loss_u loss_u 0.9092 (0.8779) acc_u 12.5000 (14.4318) lr 7.0596e-04 eta 0:00:05
epoch [121/200] batch [60/66] time 0.354 (0.471) data 0.221 (0.340) loss_u loss_u 0.8892 (0.8773) acc_u 21.8750 (14.6354) lr 7.0596e-04 eta 0:00:02
epoch [121/200] batch [65/66] time 0.555 (0.470) data 0.424 (0.338) loss_u loss_u 0.8413 (0.8771) acc_u 18.7500 (14.6635) lr 7.0596e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1601
confident_label rate tensor(0.3265, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1024
clean true:885
clean false:139
clean_rate:0.8642578125
noisy true:650
noisy false:1462
after delete: len(clean_dataset) 1024
after delete: len(noisy_dataset) 2112
epoch [122/200] batch [5/32] time 0.453 (0.483) data 0.324 (0.352) loss_x loss_x 1.9463 (1.4300) acc_x 59.3750 (66.2500) lr 6.9098e-04 eta 0:00:13
epoch [122/200] batch [10/32] time 0.481 (0.456) data 0.350 (0.326) loss_x loss_x 1.0879 (1.2587) acc_x 68.7500 (68.1250) lr 6.9098e-04 eta 0:00:10
epoch [122/200] batch [15/32] time 0.421 (0.457) data 0.290 (0.326) loss_x loss_x 1.0127 (1.2011) acc_x 75.0000 (68.9583) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [20/32] time 0.409 (0.470) data 0.277 (0.339) loss_x loss_x 0.9702 (1.2387) acc_x 68.7500 (68.2812) lr 6.9098e-04 eta 0:00:05
epoch [122/200] batch [25/32] time 0.480 (0.464) data 0.349 (0.333) loss_x loss_x 1.3555 (1.2346) acc_x 65.6250 (68.6250) lr 6.9098e-04 eta 0:00:03
epoch [122/200] batch [30/32] time 0.605 (0.471) data 0.475 (0.340) loss_x loss_x 0.9946 (1.2417) acc_x 78.1250 (68.6458) lr 6.9098e-04 eta 0:00:00
epoch [122/200] batch [5/66] time 0.429 (0.470) data 0.299 (0.340) loss_u loss_u 0.8516 (0.8838) acc_u 12.5000 (12.5000) lr 6.9098e-04 eta 0:00:28
epoch [122/200] batch [10/66] time 0.400 (0.473) data 0.270 (0.342) loss_u loss_u 0.8560 (0.8808) acc_u 18.7500 (13.4375) lr 6.9098e-04 eta 0:00:26
epoch [122/200] batch [15/66] time 0.534 (0.472) data 0.404 (0.341) loss_u loss_u 0.8711 (0.8822) acc_u 18.7500 (14.3750) lr 6.9098e-04 eta 0:00:24
epoch [122/200] batch [20/66] time 0.410 (0.468) data 0.279 (0.337) loss_u loss_u 0.9141 (0.8829) acc_u 6.2500 (13.9062) lr 6.9098e-04 eta 0:00:21
epoch [122/200] batch [25/66] time 0.386 (0.468) data 0.255 (0.337) loss_u loss_u 0.8975 (0.8760) acc_u 12.5000 (14.6250) lr 6.9098e-04 eta 0:00:19
epoch [122/200] batch [30/66] time 0.507 (0.473) data 0.377 (0.342) loss_u loss_u 0.8857 (0.8714) acc_u 12.5000 (15.1042) lr 6.9098e-04 eta 0:00:17
epoch [122/200] batch [35/66] time 0.503 (0.472) data 0.369 (0.341) loss_u loss_u 0.8638 (0.8718) acc_u 12.5000 (15.0893) lr 6.9098e-04 eta 0:00:14
epoch [122/200] batch [40/66] time 0.369 (0.473) data 0.238 (0.342) loss_u loss_u 0.8804 (0.8712) acc_u 12.5000 (15.0781) lr 6.9098e-04 eta 0:00:12
epoch [122/200] batch [45/66] time 0.404 (0.470) data 0.274 (0.339) loss_u loss_u 0.9150 (0.8753) acc_u 9.3750 (14.7222) lr 6.9098e-04 eta 0:00:09
epoch [122/200] batch [50/66] time 0.481 (0.468) data 0.350 (0.337) loss_u loss_u 0.8555 (0.8739) acc_u 18.7500 (14.8750) lr 6.9098e-04 eta 0:00:07
epoch [122/200] batch [55/66] time 0.498 (0.468) data 0.367 (0.337) loss_u loss_u 0.8633 (0.8757) acc_u 21.8750 (14.9432) lr 6.9098e-04 eta 0:00:05
epoch [122/200] batch [60/66] time 0.428 (0.468) data 0.297 (0.337) loss_u loss_u 0.8892 (0.8761) acc_u 18.7500 (15.0521) lr 6.9098e-04 eta 0:00:02
epoch [122/200] batch [65/66] time 0.398 (0.469) data 0.266 (0.338) loss_u loss_u 0.8853 (0.8746) acc_u 15.6250 (15.3365) lr 6.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1620
confident_label rate tensor(0.3262, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1023
clean true:884
clean false:139
clean_rate:0.8641251221896383
noisy true:632
noisy false:1481
after delete: len(clean_dataset) 1023
after delete: len(noisy_dataset) 2113
epoch [123/200] batch [5/31] time 0.453 (0.499) data 0.323 (0.369) loss_x loss_x 0.9551 (1.2992) acc_x 81.2500 (68.1250) lr 6.7608e-04 eta 0:00:12
epoch [123/200] batch [10/31] time 0.468 (0.486) data 0.337 (0.355) loss_x loss_x 1.3223 (1.2673) acc_x 59.3750 (67.1875) lr 6.7608e-04 eta 0:00:10
epoch [123/200] batch [15/31] time 0.443 (0.477) data 0.312 (0.346) loss_x loss_x 0.9912 (1.2157) acc_x 75.0000 (69.7917) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [20/31] time 0.403 (0.466) data 0.272 (0.336) loss_x loss_x 1.3496 (1.2223) acc_x 65.6250 (69.3750) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [25/31] time 0.448 (0.480) data 0.318 (0.349) loss_x loss_x 0.9634 (1.2077) acc_x 65.6250 (69.2500) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [30/31] time 0.538 (0.485) data 0.408 (0.354) loss_x loss_x 1.1621 (1.1904) acc_x 68.7500 (69.8958) lr 6.7608e-04 eta 0:00:00
epoch [123/200] batch [5/66] time 0.362 (0.477) data 0.231 (0.346) loss_u loss_u 0.8540 (0.8996) acc_u 21.8750 (13.1250) lr 6.7608e-04 eta 0:00:29
epoch [123/200] batch [10/66] time 0.450 (0.468) data 0.318 (0.337) loss_u loss_u 0.8418 (0.8729) acc_u 21.8750 (16.8750) lr 6.7608e-04 eta 0:00:26
epoch [123/200] batch [15/66] time 0.542 (0.471) data 0.411 (0.340) loss_u loss_u 0.8794 (0.8702) acc_u 15.6250 (16.8750) lr 6.7608e-04 eta 0:00:24
epoch [123/200] batch [20/66] time 0.520 (0.471) data 0.388 (0.340) loss_u loss_u 0.8735 (0.8772) acc_u 15.6250 (15.7812) lr 6.7608e-04 eta 0:00:21
epoch [123/200] batch [25/66] time 0.387 (0.467) data 0.256 (0.336) loss_u loss_u 0.9136 (0.8721) acc_u 9.3750 (16.6250) lr 6.7608e-04 eta 0:00:19
epoch [123/200] batch [30/66] time 0.451 (0.465) data 0.319 (0.334) loss_u loss_u 0.9351 (0.8750) acc_u 6.2500 (16.0417) lr 6.7608e-04 eta 0:00:16
epoch [123/200] batch [35/66] time 0.496 (0.469) data 0.363 (0.338) loss_u loss_u 0.8652 (0.8740) acc_u 25.0000 (16.4286) lr 6.7608e-04 eta 0:00:14
epoch [123/200] batch [40/66] time 0.441 (0.470) data 0.311 (0.338) loss_u loss_u 0.9365 (0.8789) acc_u 3.1250 (15.7031) lr 6.7608e-04 eta 0:00:12
epoch [123/200] batch [45/66] time 0.513 (0.470) data 0.381 (0.339) loss_u loss_u 0.8921 (0.8778) acc_u 15.6250 (15.6944) lr 6.7608e-04 eta 0:00:09
epoch [123/200] batch [50/66] time 0.376 (0.467) data 0.245 (0.336) loss_u loss_u 0.8452 (0.8765) acc_u 15.6250 (15.8125) lr 6.7608e-04 eta 0:00:07
epoch [123/200] batch [55/66] time 0.555 (0.468) data 0.425 (0.337) loss_u loss_u 0.8784 (0.8755) acc_u 15.6250 (16.0227) lr 6.7608e-04 eta 0:00:05
epoch [123/200] batch [60/66] time 0.379 (0.471) data 0.247 (0.339) loss_u loss_u 0.9248 (0.8745) acc_u 15.6250 (16.1979) lr 6.7608e-04 eta 0:00:02
epoch [123/200] batch [65/66] time 0.406 (0.471) data 0.273 (0.340) loss_u loss_u 0.8540 (0.8732) acc_u 18.7500 (16.2019) lr 6.7608e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1566
confident_label rate tensor(0.3304, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1036
clean true:916
clean false:120
clean_rate:0.8841698841698842
noisy true:654
noisy false:1446
after delete: len(clean_dataset) 1036
after delete: len(noisy_dataset) 2100
epoch [124/200] batch [5/32] time 0.545 (0.534) data 0.414 (0.402) loss_x loss_x 0.8901 (1.1206) acc_x 78.1250 (75.0000) lr 6.6126e-04 eta 0:00:14
epoch [124/200] batch [10/32] time 0.470 (0.511) data 0.339 (0.380) loss_x loss_x 1.3301 (1.0082) acc_x 68.7500 (78.1250) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [15/32] time 0.538 (0.487) data 0.407 (0.355) loss_x loss_x 1.0811 (1.0391) acc_x 68.7500 (75.8333) lr 6.6126e-04 eta 0:00:08
epoch [124/200] batch [20/32] time 0.558 (0.485) data 0.428 (0.354) loss_x loss_x 1.1455 (1.0512) acc_x 68.7500 (74.3750) lr 6.6126e-04 eta 0:00:05
epoch [124/200] batch [25/32] time 0.471 (0.478) data 0.341 (0.347) loss_x loss_x 1.4541 (1.1134) acc_x 68.7500 (72.7500) lr 6.6126e-04 eta 0:00:03
epoch [124/200] batch [30/32] time 0.473 (0.475) data 0.343 (0.344) loss_x loss_x 1.2529 (1.1154) acc_x 68.7500 (72.3958) lr 6.6126e-04 eta 0:00:00
epoch [124/200] batch [5/65] time 0.575 (0.470) data 0.443 (0.339) loss_u loss_u 0.8750 (0.8763) acc_u 18.7500 (15.6250) lr 6.6126e-04 eta 0:00:28
epoch [124/200] batch [10/65] time 0.430 (0.465) data 0.299 (0.334) loss_u loss_u 0.8950 (0.8714) acc_u 12.5000 (16.2500) lr 6.6126e-04 eta 0:00:25
epoch [124/200] batch [15/65] time 0.444 (0.464) data 0.314 (0.333) loss_u loss_u 0.8101 (0.8730) acc_u 25.0000 (16.2500) lr 6.6126e-04 eta 0:00:23
epoch [124/200] batch [20/65] time 0.515 (0.467) data 0.382 (0.336) loss_u loss_u 0.8335 (0.8775) acc_u 18.7500 (15.4688) lr 6.6126e-04 eta 0:00:21
epoch [124/200] batch [25/65] time 0.550 (0.470) data 0.418 (0.339) loss_u loss_u 0.8564 (0.8767) acc_u 15.6250 (15.6250) lr 6.6126e-04 eta 0:00:18
epoch [124/200] batch [30/65] time 0.399 (0.473) data 0.268 (0.342) loss_u loss_u 0.7690 (0.8718) acc_u 28.1250 (16.4583) lr 6.6126e-04 eta 0:00:16
epoch [124/200] batch [35/65] time 0.513 (0.476) data 0.382 (0.345) loss_u loss_u 0.8579 (0.8702) acc_u 15.6250 (16.1607) lr 6.6126e-04 eta 0:00:14
epoch [124/200] batch [40/65] time 0.508 (0.477) data 0.377 (0.346) loss_u loss_u 0.8823 (0.8705) acc_u 12.5000 (15.7031) lr 6.6126e-04 eta 0:00:11
epoch [124/200] batch [45/65] time 0.541 (0.478) data 0.410 (0.347) loss_u loss_u 0.9653 (0.8752) acc_u 3.1250 (15.0694) lr 6.6126e-04 eta 0:00:09
epoch [124/200] batch [50/65] time 0.487 (0.478) data 0.357 (0.347) loss_u loss_u 0.8047 (0.8734) acc_u 31.2500 (15.5000) lr 6.6126e-04 eta 0:00:07
epoch [124/200] batch [55/65] time 0.399 (0.475) data 0.268 (0.344) loss_u loss_u 0.9043 (0.8724) acc_u 12.5000 (15.7386) lr 6.6126e-04 eta 0:00:04
epoch [124/200] batch [60/65] time 0.633 (0.477) data 0.502 (0.346) loss_u loss_u 0.9253 (0.8721) acc_u 9.3750 (15.8333) lr 6.6126e-04 eta 0:00:02
epoch [124/200] batch [65/65] time 0.488 (0.475) data 0.356 (0.344) loss_u loss_u 0.8818 (0.8725) acc_u 18.7500 (15.9615) lr 6.6126e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1607
confident_label rate tensor(0.3268, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1025
clean true:888
clean false:137
clean_rate:0.8663414634146341
noisy true:641
noisy false:1470
after delete: len(clean_dataset) 1025
after delete: len(noisy_dataset) 2111
epoch [125/200] batch [5/32] time 0.530 (0.578) data 0.399 (0.446) loss_x loss_x 0.9209 (1.2713) acc_x 75.0000 (65.6250) lr 6.4653e-04 eta 0:00:15
epoch [125/200] batch [10/32] time 0.587 (0.521) data 0.456 (0.390) loss_x loss_x 1.5156 (1.3419) acc_x 56.2500 (64.0625) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [15/32] time 0.618 (0.522) data 0.488 (0.391) loss_x loss_x 1.1611 (1.2581) acc_x 75.0000 (66.8750) lr 6.4653e-04 eta 0:00:08
epoch [125/200] batch [20/32] time 0.454 (0.492) data 0.323 (0.362) loss_x loss_x 1.8057 (1.2813) acc_x 65.6250 (67.6562) lr 6.4653e-04 eta 0:00:05
epoch [125/200] batch [25/32] time 0.439 (0.484) data 0.309 (0.353) loss_x loss_x 1.1094 (1.2750) acc_x 75.0000 (67.5000) lr 6.4653e-04 eta 0:00:03
epoch [125/200] batch [30/32] time 0.613 (0.482) data 0.483 (0.351) loss_x loss_x 1.4121 (1.2842) acc_x 65.6250 (67.7083) lr 6.4653e-04 eta 0:00:00
epoch [125/200] batch [5/65] time 0.502 (0.475) data 0.371 (0.345) loss_u loss_u 0.9014 (0.8758) acc_u 12.5000 (13.7500) lr 6.4653e-04 eta 0:00:28
epoch [125/200] batch [10/65] time 0.710 (0.481) data 0.579 (0.350) loss_u loss_u 0.9224 (0.8846) acc_u 12.5000 (13.1250) lr 6.4653e-04 eta 0:00:26
epoch [125/200] batch [15/65] time 0.366 (0.479) data 0.236 (0.349) loss_u loss_u 0.8804 (0.8908) acc_u 12.5000 (12.7083) lr 6.4653e-04 eta 0:00:23
epoch [125/200] batch [20/65] time 0.341 (0.475) data 0.210 (0.344) loss_u loss_u 0.8643 (0.8876) acc_u 15.6250 (12.9688) lr 6.4653e-04 eta 0:00:21
epoch [125/200] batch [25/65] time 0.448 (0.470) data 0.318 (0.340) loss_u loss_u 0.8438 (0.8809) acc_u 18.7500 (13.7500) lr 6.4653e-04 eta 0:00:18
epoch [125/200] batch [30/65] time 0.421 (0.467) data 0.291 (0.337) loss_u loss_u 0.8623 (0.8774) acc_u 18.7500 (14.7917) lr 6.4653e-04 eta 0:00:16
epoch [125/200] batch [35/65] time 0.364 (0.467) data 0.234 (0.336) loss_u loss_u 0.8145 (0.8760) acc_u 25.0000 (15.0893) lr 6.4653e-04 eta 0:00:14
epoch [125/200] batch [40/65] time 0.340 (0.462) data 0.210 (0.331) loss_u loss_u 0.9150 (0.8781) acc_u 12.5000 (15.0781) lr 6.4653e-04 eta 0:00:11
epoch [125/200] batch [45/65] time 0.449 (0.462) data 0.318 (0.331) loss_u loss_u 0.8271 (0.8759) acc_u 15.6250 (15.4167) lr 6.4653e-04 eta 0:00:09
epoch [125/200] batch [50/65] time 0.592 (0.461) data 0.461 (0.331) loss_u loss_u 0.8535 (0.8747) acc_u 15.6250 (15.6875) lr 6.4653e-04 eta 0:00:06
epoch [125/200] batch [55/65] time 0.517 (0.460) data 0.385 (0.329) loss_u loss_u 0.8755 (0.8739) acc_u 15.6250 (15.8523) lr 6.4653e-04 eta 0:00:04
epoch [125/200] batch [60/65] time 0.350 (0.460) data 0.219 (0.329) loss_u loss_u 0.8457 (0.8754) acc_u 21.8750 (15.8854) lr 6.4653e-04 eta 0:00:02
epoch [125/200] batch [65/65] time 0.350 (0.460) data 0.218 (0.329) loss_u loss_u 0.8975 (0.8756) acc_u 15.6250 (15.8654) lr 6.4653e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1587
confident_label rate tensor(0.3294, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1033
clean true:891
clean false:142
clean_rate:0.8625363020329139
noisy true:658
noisy false:1445
after delete: len(clean_dataset) 1033
after delete: len(noisy_dataset) 2103
epoch [126/200] batch [5/32] time 0.454 (0.498) data 0.323 (0.367) loss_x loss_x 0.5669 (1.0727) acc_x 87.5000 (73.7500) lr 6.3188e-04 eta 0:00:13
epoch [126/200] batch [10/32] time 0.407 (0.500) data 0.277 (0.369) loss_x loss_x 0.8452 (1.0944) acc_x 81.2500 (71.5625) lr 6.3188e-04 eta 0:00:10
epoch [126/200] batch [15/32] time 0.400 (0.493) data 0.270 (0.363) loss_x loss_x 1.2119 (1.1785) acc_x 68.7500 (70.0000) lr 6.3188e-04 eta 0:00:08
epoch [126/200] batch [20/32] time 0.437 (0.476) data 0.307 (0.345) loss_x loss_x 0.7700 (1.1723) acc_x 81.2500 (70.1562) lr 6.3188e-04 eta 0:00:05
epoch [126/200] batch [25/32] time 0.507 (0.473) data 0.377 (0.342) loss_x loss_x 1.1240 (1.1429) acc_x 68.7500 (70.3750) lr 6.3188e-04 eta 0:00:03
epoch [126/200] batch [30/32] time 0.558 (0.473) data 0.427 (0.343) loss_x loss_x 1.3467 (1.1427) acc_x 59.3750 (69.8958) lr 6.3188e-04 eta 0:00:00
epoch [126/200] batch [5/65] time 0.398 (0.464) data 0.268 (0.334) loss_u loss_u 0.8608 (0.8796) acc_u 18.7500 (15.6250) lr 6.3188e-04 eta 0:00:27
epoch [126/200] batch [10/65] time 0.364 (0.456) data 0.234 (0.325) loss_u loss_u 0.8457 (0.8843) acc_u 21.8750 (14.3750) lr 6.3188e-04 eta 0:00:25
epoch [126/200] batch [15/65] time 0.598 (0.466) data 0.467 (0.336) loss_u loss_u 0.9189 (0.8826) acc_u 12.5000 (15.2083) lr 6.3188e-04 eta 0:00:23
epoch [126/200] batch [20/65] time 0.431 (0.464) data 0.301 (0.334) loss_u loss_u 0.8735 (0.8854) acc_u 15.6250 (15.1562) lr 6.3188e-04 eta 0:00:20
epoch [126/200] batch [25/65] time 0.413 (0.458) data 0.283 (0.327) loss_u loss_u 0.9087 (0.8796) acc_u 9.3750 (16.0000) lr 6.3188e-04 eta 0:00:18
epoch [126/200] batch [30/65] time 0.336 (0.456) data 0.205 (0.325) loss_u loss_u 0.8813 (0.8801) acc_u 12.5000 (15.7292) lr 6.3188e-04 eta 0:00:15
epoch [126/200] batch [35/65] time 0.553 (0.460) data 0.422 (0.330) loss_u loss_u 0.9185 (0.8802) acc_u 12.5000 (15.8036) lr 6.3188e-04 eta 0:00:13
epoch [126/200] batch [40/65] time 0.453 (0.463) data 0.321 (0.332) loss_u loss_u 0.8882 (0.8807) acc_u 12.5000 (15.6250) lr 6.3188e-04 eta 0:00:11
epoch [126/200] batch [45/65] time 0.480 (0.467) data 0.350 (0.336) loss_u loss_u 0.8286 (0.8802) acc_u 21.8750 (15.5556) lr 6.3188e-04 eta 0:00:09
epoch [126/200] batch [50/65] time 0.389 (0.466) data 0.257 (0.336) loss_u loss_u 0.8955 (0.8793) acc_u 15.6250 (15.6250) lr 6.3188e-04 eta 0:00:06
epoch [126/200] batch [55/65] time 0.364 (0.461) data 0.233 (0.330) loss_u loss_u 0.8149 (0.8769) acc_u 25.0000 (15.9091) lr 6.3188e-04 eta 0:00:04
epoch [126/200] batch [60/65] time 0.412 (0.468) data 0.278 (0.337) loss_u loss_u 0.8745 (0.8775) acc_u 18.7500 (15.7812) lr 6.3188e-04 eta 0:00:02
epoch [126/200] batch [65/65] time 0.576 (0.470) data 0.439 (0.339) loss_u loss_u 0.8882 (0.8780) acc_u 15.6250 (15.6731) lr 6.3188e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1597
confident_label rate tensor(0.3278, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1028
clean true:894
clean false:134
clean_rate:0.8696498054474708
noisy true:645
noisy false:1463
after delete: len(clean_dataset) 1028
after delete: len(noisy_dataset) 2108
epoch [127/200] batch [5/32] time 0.537 (0.454) data 0.407 (0.324) loss_x loss_x 1.4717 (1.1229) acc_x 59.3750 (68.7500) lr 6.1732e-04 eta 0:00:12
epoch [127/200] batch [10/32] time 0.454 (0.475) data 0.324 (0.345) loss_x loss_x 1.3896 (1.1860) acc_x 59.3750 (67.5000) lr 6.1732e-04 eta 0:00:10
epoch [127/200] batch [15/32] time 0.442 (0.459) data 0.312 (0.329) loss_x loss_x 1.2568 (1.2127) acc_x 75.0000 (68.5417) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [20/32] time 0.610 (0.491) data 0.479 (0.360) loss_x loss_x 1.2441 (1.1683) acc_x 71.8750 (69.8438) lr 6.1732e-04 eta 0:00:05
epoch [127/200] batch [25/32] time 0.451 (0.477) data 0.320 (0.346) loss_x loss_x 1.4297 (1.2092) acc_x 65.6250 (69.6250) lr 6.1732e-04 eta 0:00:03
epoch [127/200] batch [30/32] time 0.536 (0.486) data 0.406 (0.355) loss_x loss_x 0.8486 (1.2085) acc_x 75.0000 (69.1667) lr 6.1732e-04 eta 0:00:00
epoch [127/200] batch [5/65] time 0.453 (0.480) data 0.322 (0.350) loss_u loss_u 0.8804 (0.8836) acc_u 12.5000 (11.2500) lr 6.1732e-04 eta 0:00:28
epoch [127/200] batch [10/65] time 0.418 (0.482) data 0.287 (0.352) loss_u loss_u 0.8022 (0.8804) acc_u 21.8750 (12.8125) lr 6.1732e-04 eta 0:00:26
epoch [127/200] batch [15/65] time 0.496 (0.479) data 0.365 (0.349) loss_u loss_u 0.8848 (0.8897) acc_u 18.7500 (12.2917) lr 6.1732e-04 eta 0:00:23
epoch [127/200] batch [20/65] time 0.391 (0.474) data 0.261 (0.343) loss_u loss_u 0.9287 (0.8878) acc_u 6.2500 (12.8125) lr 6.1732e-04 eta 0:00:21
epoch [127/200] batch [25/65] time 0.484 (0.473) data 0.353 (0.342) loss_u loss_u 0.8589 (0.8896) acc_u 15.6250 (12.7500) lr 6.1732e-04 eta 0:00:18
epoch [127/200] batch [30/65] time 0.569 (0.477) data 0.437 (0.346) loss_u loss_u 0.8447 (0.8851) acc_u 25.0000 (13.6458) lr 6.1732e-04 eta 0:00:16
epoch [127/200] batch [35/65] time 0.602 (0.476) data 0.471 (0.345) loss_u loss_u 0.9185 (0.8847) acc_u 9.3750 (13.5714) lr 6.1732e-04 eta 0:00:14
epoch [127/200] batch [40/65] time 0.428 (0.474) data 0.297 (0.343) loss_u loss_u 0.8589 (0.8790) acc_u 15.6250 (14.1406) lr 6.1732e-04 eta 0:00:11
epoch [127/200] batch [45/65] time 0.612 (0.476) data 0.481 (0.345) loss_u loss_u 0.8633 (0.8772) acc_u 25.0000 (14.6528) lr 6.1732e-04 eta 0:00:09
epoch [127/200] batch [50/65] time 0.566 (0.475) data 0.435 (0.344) loss_u loss_u 0.8013 (0.8746) acc_u 25.0000 (15.1875) lr 6.1732e-04 eta 0:00:07
epoch [127/200] batch [55/65] time 0.346 (0.474) data 0.215 (0.343) loss_u loss_u 0.8589 (0.8768) acc_u 18.7500 (15.0000) lr 6.1732e-04 eta 0:00:04
epoch [127/200] batch [60/65] time 0.427 (0.473) data 0.295 (0.342) loss_u loss_u 0.9111 (0.8788) acc_u 9.3750 (14.6875) lr 6.1732e-04 eta 0:00:02
epoch [127/200] batch [65/65] time 0.476 (0.473) data 0.345 (0.342) loss_u loss_u 0.9180 (0.8789) acc_u 9.3750 (14.7115) lr 6.1732e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1591
confident_label rate tensor(0.3310, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1038
clean true:893
clean false:145
clean_rate:0.8603082851637764
noisy true:652
noisy false:1446
after delete: len(clean_dataset) 1038
after delete: len(noisy_dataset) 2098
epoch [128/200] batch [5/32] time 0.458 (0.540) data 0.327 (0.408) loss_x loss_x 0.9839 (1.1484) acc_x 75.0000 (68.1250) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [10/32] time 0.538 (0.513) data 0.407 (0.382) loss_x loss_x 1.7773 (1.2999) acc_x 50.0000 (64.3750) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [15/32] time 0.512 (0.507) data 0.381 (0.376) loss_x loss_x 1.5830 (1.2805) acc_x 71.8750 (65.6250) lr 6.0285e-04 eta 0:00:08
epoch [128/200] batch [20/32] time 0.508 (0.493) data 0.376 (0.362) loss_x loss_x 1.7920 (1.3204) acc_x 56.2500 (65.0000) lr 6.0285e-04 eta 0:00:05
epoch [128/200] batch [25/32] time 0.477 (0.495) data 0.346 (0.364) loss_x loss_x 1.5332 (1.2828) acc_x 65.6250 (66.5000) lr 6.0285e-04 eta 0:00:03
epoch [128/200] batch [30/32] time 0.390 (0.482) data 0.259 (0.351) loss_x loss_x 1.3916 (1.2893) acc_x 65.6250 (66.7708) lr 6.0285e-04 eta 0:00:00
epoch [128/200] batch [5/65] time 0.348 (0.481) data 0.218 (0.350) loss_u loss_u 0.9082 (0.8974) acc_u 9.3750 (11.2500) lr 6.0285e-04 eta 0:00:28
epoch [128/200] batch [10/65] time 0.357 (0.481) data 0.225 (0.350) loss_u loss_u 0.8901 (0.8967) acc_u 15.6250 (10.9375) lr 6.0285e-04 eta 0:00:26
epoch [128/200] batch [15/65] time 0.429 (0.481) data 0.299 (0.350) loss_u loss_u 0.8921 (0.8881) acc_u 9.3750 (12.5000) lr 6.0285e-04 eta 0:00:24
epoch [128/200] batch [20/65] time 0.461 (0.476) data 0.329 (0.345) loss_u loss_u 0.8203 (0.8764) acc_u 28.1250 (14.2188) lr 6.0285e-04 eta 0:00:21
epoch [128/200] batch [25/65] time 0.341 (0.470) data 0.210 (0.339) loss_u loss_u 0.8423 (0.8735) acc_u 18.7500 (14.5000) lr 6.0285e-04 eta 0:00:18
epoch [128/200] batch [30/65] time 0.340 (0.465) data 0.210 (0.334) loss_u loss_u 0.8345 (0.8789) acc_u 18.7500 (13.7500) lr 6.0285e-04 eta 0:00:16
epoch [128/200] batch [35/65] time 0.511 (0.473) data 0.380 (0.342) loss_u loss_u 0.8950 (0.8744) acc_u 15.6250 (14.4643) lr 6.0285e-04 eta 0:00:14
epoch [128/200] batch [40/65] time 0.526 (0.475) data 0.394 (0.344) loss_u loss_u 0.8760 (0.8696) acc_u 18.7500 (15.2344) lr 6.0285e-04 eta 0:00:11
epoch [128/200] batch [45/65] time 0.344 (0.469) data 0.214 (0.338) loss_u loss_u 0.8574 (0.8671) acc_u 21.8750 (15.7639) lr 6.0285e-04 eta 0:00:09
epoch [128/200] batch [50/65] time 0.407 (0.466) data 0.276 (0.334) loss_u loss_u 0.8833 (0.8680) acc_u 9.3750 (15.6250) lr 6.0285e-04 eta 0:00:06
epoch [128/200] batch [55/65] time 0.389 (0.463) data 0.257 (0.332) loss_u loss_u 0.9497 (0.8692) acc_u 3.1250 (15.4545) lr 6.0285e-04 eta 0:00:04
epoch [128/200] batch [60/65] time 0.594 (0.466) data 0.461 (0.335) loss_u loss_u 0.9058 (0.8686) acc_u 12.5000 (15.5729) lr 6.0285e-04 eta 0:00:02
epoch [128/200] batch [65/65] time 0.454 (0.463) data 0.322 (0.332) loss_u loss_u 0.8809 (0.8701) acc_u 15.6250 (15.5288) lr 6.0285e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1567
confident_label rate tensor(0.3243, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1017
clean true:888
clean false:129
clean_rate:0.8731563421828908
noisy true:681
noisy false:1438
after delete: len(clean_dataset) 1017
after delete: len(noisy_dataset) 2119
epoch [129/200] batch [5/31] time 0.471 (0.485) data 0.340 (0.353) loss_x loss_x 0.8979 (1.1013) acc_x 81.2500 (73.7500) lr 5.8849e-04 eta 0:00:12
epoch [129/200] batch [10/31] time 0.643 (0.480) data 0.513 (0.349) loss_x loss_x 1.9258 (1.1706) acc_x 56.2500 (71.2500) lr 5.8849e-04 eta 0:00:10
epoch [129/200] batch [15/31] time 0.449 (0.483) data 0.318 (0.352) loss_x loss_x 1.0537 (1.1690) acc_x 75.0000 (70.2083) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [20/31] time 0.517 (0.485) data 0.386 (0.355) loss_x loss_x 0.9683 (1.1417) acc_x 75.0000 (72.1875) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [25/31] time 0.468 (0.490) data 0.338 (0.360) loss_x loss_x 1.3584 (1.1576) acc_x 68.7500 (72.0000) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [30/31] time 0.434 (0.489) data 0.303 (0.358) loss_x loss_x 1.5635 (1.1805) acc_x 56.2500 (71.2500) lr 5.8849e-04 eta 0:00:00
epoch [129/200] batch [5/66] time 0.410 (0.486) data 0.277 (0.355) loss_u loss_u 0.8535 (0.8837) acc_u 15.6250 (14.3750) lr 5.8849e-04 eta 0:00:29
epoch [129/200] batch [10/66] time 0.414 (0.482) data 0.283 (0.352) loss_u loss_u 0.9292 (0.8901) acc_u 12.5000 (14.0625) lr 5.8849e-04 eta 0:00:27
epoch [129/200] batch [15/66] time 0.482 (0.479) data 0.352 (0.348) loss_u loss_u 0.8525 (0.8812) acc_u 15.6250 (14.5833) lr 5.8849e-04 eta 0:00:24
epoch [129/200] batch [20/66] time 0.427 (0.479) data 0.295 (0.348) loss_u loss_u 0.8813 (0.8756) acc_u 12.5000 (15.0000) lr 5.8849e-04 eta 0:00:22
epoch [129/200] batch [25/66] time 0.438 (0.477) data 0.307 (0.346) loss_u loss_u 0.9136 (0.8749) acc_u 9.3750 (15.5000) lr 5.8849e-04 eta 0:00:19
epoch [129/200] batch [30/66] time 0.420 (0.474) data 0.289 (0.343) loss_u loss_u 0.8862 (0.8752) acc_u 15.6250 (15.6250) lr 5.8849e-04 eta 0:00:17
epoch [129/200] batch [35/66] time 0.361 (0.478) data 0.231 (0.347) loss_u loss_u 0.8423 (0.8744) acc_u 21.8750 (15.6250) lr 5.8849e-04 eta 0:00:14
epoch [129/200] batch [40/66] time 0.464 (0.474) data 0.334 (0.343) loss_u loss_u 0.8389 (0.8711) acc_u 21.8750 (16.0156) lr 5.8849e-04 eta 0:00:12
epoch [129/200] batch [45/66] time 0.501 (0.472) data 0.369 (0.341) loss_u loss_u 0.9243 (0.8712) acc_u 9.3750 (16.1806) lr 5.8849e-04 eta 0:00:09
epoch [129/200] batch [50/66] time 0.469 (0.468) data 0.338 (0.337) loss_u loss_u 0.9160 (0.8741) acc_u 9.3750 (15.7500) lr 5.8849e-04 eta 0:00:07
epoch [129/200] batch [55/66] time 0.680 (0.468) data 0.550 (0.337) loss_u loss_u 0.8379 (0.8730) acc_u 21.8750 (15.7955) lr 5.8849e-04 eta 0:00:05
epoch [129/200] batch [60/66] time 0.475 (0.468) data 0.343 (0.337) loss_u loss_u 0.9009 (0.8732) acc_u 9.3750 (15.7292) lr 5.8849e-04 eta 0:00:02
epoch [129/200] batch [65/66] time 0.452 (0.466) data 0.321 (0.335) loss_u loss_u 0.8887 (0.8744) acc_u 15.6250 (15.5288) lr 5.8849e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1606
confident_label rate tensor(0.3275, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1027
clean true:886
clean false:141
clean_rate:0.8627069133398247
noisy true:644
noisy false:1465
after delete: len(clean_dataset) 1027
after delete: len(noisy_dataset) 2109
epoch [130/200] batch [5/32] time 0.503 (0.439) data 0.373 (0.308) loss_x loss_x 1.3398 (1.3738) acc_x 68.7500 (68.1250) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [10/32] time 0.457 (0.445) data 0.324 (0.314) loss_x loss_x 1.3682 (1.3256) acc_x 62.5000 (65.6250) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [15/32] time 0.398 (0.459) data 0.267 (0.328) loss_x loss_x 1.8906 (1.3241) acc_x 50.0000 (66.6667) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [20/32] time 0.428 (0.476) data 0.297 (0.345) loss_x loss_x 1.4590 (1.2513) acc_x 62.5000 (68.4375) lr 5.7422e-04 eta 0:00:05
epoch [130/200] batch [25/32] time 0.463 (0.481) data 0.332 (0.350) loss_x loss_x 0.9380 (1.2303) acc_x 71.8750 (69.3750) lr 5.7422e-04 eta 0:00:03
epoch [130/200] batch [30/32] time 0.511 (0.480) data 0.380 (0.349) loss_x loss_x 1.4199 (1.2233) acc_x 65.6250 (69.8958) lr 5.7422e-04 eta 0:00:00
epoch [130/200] batch [5/65] time 0.398 (0.485) data 0.266 (0.354) loss_u loss_u 0.9038 (0.8398) acc_u 12.5000 (19.3750) lr 5.7422e-04 eta 0:00:29
epoch [130/200] batch [10/65] time 0.462 (0.486) data 0.330 (0.354) loss_u loss_u 0.8291 (0.8523) acc_u 18.7500 (18.1250) lr 5.7422e-04 eta 0:00:26
epoch [130/200] batch [15/65] time 0.447 (0.490) data 0.315 (0.359) loss_u loss_u 0.9062 (0.8623) acc_u 12.5000 (16.8750) lr 5.7422e-04 eta 0:00:24
epoch [130/200] batch [20/65] time 0.358 (0.484) data 0.226 (0.353) loss_u loss_u 0.8857 (0.8730) acc_u 12.5000 (15.4688) lr 5.7422e-04 eta 0:00:21
epoch [130/200] batch [25/65] time 0.414 (0.477) data 0.283 (0.346) loss_u loss_u 0.8687 (0.8768) acc_u 15.6250 (14.6250) lr 5.7422e-04 eta 0:00:19
epoch [130/200] batch [30/65] time 0.398 (0.473) data 0.267 (0.342) loss_u loss_u 0.8130 (0.8761) acc_u 28.1250 (14.8958) lr 5.7422e-04 eta 0:00:16
epoch [130/200] batch [35/65] time 0.394 (0.467) data 0.263 (0.336) loss_u loss_u 0.8857 (0.8723) acc_u 15.6250 (15.0893) lr 5.7422e-04 eta 0:00:14
epoch [130/200] batch [40/65] time 0.450 (0.466) data 0.318 (0.335) loss_u loss_u 0.8296 (0.8720) acc_u 18.7500 (15.0781) lr 5.7422e-04 eta 0:00:11
epoch [130/200] batch [45/65] time 0.611 (0.469) data 0.480 (0.338) loss_u loss_u 0.8491 (0.8718) acc_u 18.7500 (15.4167) lr 5.7422e-04 eta 0:00:09
epoch [130/200] batch [50/65] time 0.698 (0.472) data 0.566 (0.341) loss_u loss_u 0.9380 (0.8746) acc_u 6.2500 (15.2500) lr 5.7422e-04 eta 0:00:07
epoch [130/200] batch [55/65] time 0.363 (0.473) data 0.231 (0.342) loss_u loss_u 0.8760 (0.8741) acc_u 15.6250 (15.2273) lr 5.7422e-04 eta 0:00:04
epoch [130/200] batch [60/65] time 0.536 (0.474) data 0.404 (0.343) loss_u loss_u 0.8691 (0.8733) acc_u 18.7500 (15.4167) lr 5.7422e-04 eta 0:00:02
epoch [130/200] batch [65/65] time 0.452 (0.475) data 0.320 (0.344) loss_u loss_u 0.9175 (0.8750) acc_u 9.3750 (15.1442) lr 5.7422e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1576
confident_label rate tensor(0.3300, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1035
clean true:903
clean false:132
clean_rate:0.8724637681159421
noisy true:657
noisy false:1444
after delete: len(clean_dataset) 1035
after delete: len(noisy_dataset) 2101
epoch [131/200] batch [5/32] time 0.588 (0.511) data 0.457 (0.380) loss_x loss_x 1.6094 (1.2367) acc_x 53.1250 (69.3750) lr 5.6006e-04 eta 0:00:13
epoch [131/200] batch [10/32] time 0.480 (0.495) data 0.350 (0.364) loss_x loss_x 1.3672 (1.2276) acc_x 50.0000 (67.5000) lr 5.6006e-04 eta 0:00:10
epoch [131/200] batch [15/32] time 0.516 (0.486) data 0.385 (0.355) loss_x loss_x 1.3984 (1.2725) acc_x 62.5000 (64.7917) lr 5.6006e-04 eta 0:00:08
epoch [131/200] batch [20/32] time 0.503 (0.484) data 0.372 (0.354) loss_x loss_x 1.0723 (1.2292) acc_x 68.7500 (67.0312) lr 5.6006e-04 eta 0:00:05
epoch [131/200] batch [25/32] time 0.456 (0.479) data 0.325 (0.348) loss_x loss_x 0.9268 (1.1847) acc_x 71.8750 (68.5000) lr 5.6006e-04 eta 0:00:03
epoch [131/200] batch [30/32] time 0.398 (0.484) data 0.267 (0.354) loss_x loss_x 1.1133 (1.1735) acc_x 65.6250 (68.6458) lr 5.6006e-04 eta 0:00:00
epoch [131/200] batch [5/65] time 0.429 (0.486) data 0.297 (0.355) loss_u loss_u 0.8774 (0.8938) acc_u 9.3750 (13.7500) lr 5.6006e-04 eta 0:00:29
epoch [131/200] batch [10/65] time 0.577 (0.482) data 0.445 (0.351) loss_u loss_u 0.8438 (0.8771) acc_u 12.5000 (14.3750) lr 5.6006e-04 eta 0:00:26
epoch [131/200] batch [15/65] time 0.417 (0.483) data 0.287 (0.352) loss_u loss_u 0.8076 (0.8732) acc_u 21.8750 (15.0000) lr 5.6006e-04 eta 0:00:24
epoch [131/200] batch [20/65] time 0.463 (0.482) data 0.331 (0.351) loss_u loss_u 0.8823 (0.8789) acc_u 18.7500 (14.6875) lr 5.6006e-04 eta 0:00:21
epoch [131/200] batch [25/65] time 0.533 (0.480) data 0.401 (0.349) loss_u loss_u 0.9292 (0.8819) acc_u 6.2500 (14.1250) lr 5.6006e-04 eta 0:00:19
epoch [131/200] batch [30/65] time 0.376 (0.481) data 0.244 (0.350) loss_u loss_u 0.8740 (0.8781) acc_u 15.6250 (14.6875) lr 5.6006e-04 eta 0:00:16
epoch [131/200] batch [35/65] time 0.458 (0.483) data 0.325 (0.351) loss_u loss_u 0.8530 (0.8798) acc_u 18.7500 (14.1964) lr 5.6006e-04 eta 0:00:14
epoch [131/200] batch [40/65] time 0.501 (0.487) data 0.369 (0.356) loss_u loss_u 0.8521 (0.8783) acc_u 18.7500 (14.6875) lr 5.6006e-04 eta 0:00:12
epoch [131/200] batch [45/65] time 0.365 (0.484) data 0.234 (0.353) loss_u loss_u 0.9199 (0.8791) acc_u 9.3750 (14.4444) lr 5.6006e-04 eta 0:00:09
epoch [131/200] batch [50/65] time 0.427 (0.481) data 0.297 (0.350) loss_u loss_u 0.8799 (0.8777) acc_u 12.5000 (14.5625) lr 5.6006e-04 eta 0:00:07
epoch [131/200] batch [55/65] time 0.598 (0.485) data 0.467 (0.354) loss_u loss_u 0.8716 (0.8787) acc_u 18.7500 (14.4886) lr 5.6006e-04 eta 0:00:04
epoch [131/200] batch [60/65] time 0.428 (0.482) data 0.297 (0.351) loss_u loss_u 0.7793 (0.8760) acc_u 25.0000 (14.8958) lr 5.6006e-04 eta 0:00:02
epoch [131/200] batch [65/65] time 0.509 (0.484) data 0.377 (0.353) loss_u loss_u 0.8774 (0.8779) acc_u 15.6250 (14.7115) lr 5.6006e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1565
confident_label rate tensor(0.3288, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1031
clean true:905
clean false:126
clean_rate:0.8777885548011639
noisy true:666
noisy false:1439
after delete: len(clean_dataset) 1031
after delete: len(noisy_dataset) 2105
epoch [132/200] batch [5/32] time 0.464 (0.469) data 0.334 (0.339) loss_x loss_x 0.8438 (1.0707) acc_x 78.1250 (73.1250) lr 5.4601e-04 eta 0:00:12
epoch [132/200] batch [10/32] time 0.379 (0.444) data 0.249 (0.314) loss_x loss_x 1.3076 (1.1060) acc_x 65.6250 (70.6250) lr 5.4601e-04 eta 0:00:09
epoch [132/200] batch [15/32] time 0.581 (0.484) data 0.451 (0.353) loss_x loss_x 1.0186 (1.0821) acc_x 65.6250 (71.8750) lr 5.4601e-04 eta 0:00:08
epoch [132/200] batch [20/32] time 0.544 (0.489) data 0.413 (0.359) loss_x loss_x 1.3877 (1.1525) acc_x 68.7500 (71.0938) lr 5.4601e-04 eta 0:00:05
epoch [132/200] batch [25/32] time 0.485 (0.482) data 0.355 (0.352) loss_x loss_x 1.4385 (1.2341) acc_x 62.5000 (68.5000) lr 5.4601e-04 eta 0:00:03
epoch [132/200] batch [30/32] time 0.565 (0.480) data 0.435 (0.350) loss_x loss_x 1.1504 (1.2561) acc_x 71.8750 (68.0208) lr 5.4601e-04 eta 0:00:00
epoch [132/200] batch [5/65] time 0.601 (0.486) data 0.470 (0.355) loss_u loss_u 0.9028 (0.8754) acc_u 9.3750 (14.3750) lr 5.4601e-04 eta 0:00:29
epoch [132/200] batch [10/65] time 0.495 (0.488) data 0.365 (0.357) loss_u loss_u 0.8804 (0.8854) acc_u 12.5000 (13.4375) lr 5.4601e-04 eta 0:00:26
epoch [132/200] batch [15/65] time 0.388 (0.484) data 0.258 (0.354) loss_u loss_u 0.9097 (0.8770) acc_u 12.5000 (14.7917) lr 5.4601e-04 eta 0:00:24
epoch [132/200] batch [20/65] time 0.609 (0.481) data 0.478 (0.351) loss_u loss_u 0.9116 (0.8723) acc_u 12.5000 (15.3125) lr 5.4601e-04 eta 0:00:21
epoch [132/200] batch [25/65] time 0.462 (0.481) data 0.331 (0.351) loss_u loss_u 0.9854 (0.8743) acc_u 0.0000 (15.1250) lr 5.4601e-04 eta 0:00:19
epoch [132/200] batch [30/65] time 0.372 (0.477) data 0.241 (0.347) loss_u loss_u 0.8931 (0.8744) acc_u 9.3750 (15.8333) lr 5.4601e-04 eta 0:00:16
epoch [132/200] batch [35/65] time 0.671 (0.475) data 0.540 (0.345) loss_u loss_u 0.9189 (0.8779) acc_u 9.3750 (15.4464) lr 5.4601e-04 eta 0:00:14
epoch [132/200] batch [40/65] time 0.407 (0.472) data 0.275 (0.341) loss_u loss_u 0.8574 (0.8756) acc_u 15.6250 (15.7812) lr 5.4601e-04 eta 0:00:11
epoch [132/200] batch [45/65] time 0.544 (0.474) data 0.413 (0.343) loss_u loss_u 0.9087 (0.8754) acc_u 18.7500 (16.0417) lr 5.4601e-04 eta 0:00:09
epoch [132/200] batch [50/65] time 0.440 (0.473) data 0.310 (0.342) loss_u loss_u 0.8442 (0.8764) acc_u 18.7500 (15.8125) lr 5.4601e-04 eta 0:00:07
epoch [132/200] batch [55/65] time 0.433 (0.474) data 0.301 (0.343) loss_u loss_u 0.9160 (0.8775) acc_u 9.3750 (15.5682) lr 5.4601e-04 eta 0:00:04
epoch [132/200] batch [60/65] time 0.408 (0.471) data 0.277 (0.340) loss_u loss_u 0.8315 (0.8764) acc_u 18.7500 (15.5208) lr 5.4601e-04 eta 0:00:02
epoch [132/200] batch [65/65] time 0.451 (0.468) data 0.320 (0.337) loss_u loss_u 0.8833 (0.8776) acc_u 12.5000 (15.3365) lr 5.4601e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1602
confident_label rate tensor(0.3262, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1023
clean true:889
clean false:134
clean_rate:0.8690127077223851
noisy true:645
noisy false:1468
after delete: len(clean_dataset) 1023
after delete: len(noisy_dataset) 2113
epoch [133/200] batch [5/31] time 0.440 (0.430) data 0.309 (0.300) loss_x loss_x 1.3320 (1.2680) acc_x 62.5000 (70.0000) lr 5.3207e-04 eta 0:00:11
epoch [133/200] batch [10/31] time 0.449 (0.445) data 0.318 (0.315) loss_x loss_x 1.2139 (1.2694) acc_x 78.1250 (70.3125) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [15/31] time 0.421 (0.440) data 0.290 (0.309) loss_x loss_x 1.5107 (1.2125) acc_x 68.7500 (71.6667) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [20/31] time 0.395 (0.442) data 0.265 (0.311) loss_x loss_x 0.8877 (1.1734) acc_x 75.0000 (70.7812) lr 5.3207e-04 eta 0:00:04
epoch [133/200] batch [25/31] time 0.508 (0.456) data 0.378 (0.325) loss_x loss_x 1.5547 (1.1766) acc_x 59.3750 (70.2500) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [30/31] time 0.473 (0.459) data 0.343 (0.329) loss_x loss_x 1.3115 (1.1831) acc_x 71.8750 (70.4167) lr 5.3207e-04 eta 0:00:00
epoch [133/200] batch [5/66] time 0.416 (0.452) data 0.285 (0.321) loss_u loss_u 0.7944 (0.8724) acc_u 21.8750 (15.6250) lr 5.3207e-04 eta 0:00:27
epoch [133/200] batch [10/66] time 0.507 (0.456) data 0.374 (0.325) loss_u loss_u 0.8330 (0.8739) acc_u 25.0000 (15.6250) lr 5.3207e-04 eta 0:00:25
epoch [133/200] batch [15/66] time 0.414 (0.459) data 0.284 (0.328) loss_u loss_u 0.8740 (0.8772) acc_u 12.5000 (14.3750) lr 5.3207e-04 eta 0:00:23
epoch [133/200] batch [20/66] time 0.439 (0.459) data 0.308 (0.328) loss_u loss_u 0.8623 (0.8833) acc_u 12.5000 (13.2812) lr 5.3207e-04 eta 0:00:21
epoch [133/200] batch [25/66] time 0.575 (0.463) data 0.443 (0.333) loss_u loss_u 0.8770 (0.8819) acc_u 12.5000 (13.6250) lr 5.3207e-04 eta 0:00:19
epoch [133/200] batch [30/66] time 0.396 (0.461) data 0.264 (0.330) loss_u loss_u 0.9126 (0.8862) acc_u 9.3750 (12.9167) lr 5.3207e-04 eta 0:00:16
epoch [133/200] batch [35/66] time 0.554 (0.463) data 0.423 (0.332) loss_u loss_u 0.9243 (0.8861) acc_u 9.3750 (13.1250) lr 5.3207e-04 eta 0:00:14
epoch [133/200] batch [40/66] time 0.430 (0.469) data 0.298 (0.338) loss_u loss_u 0.8398 (0.8856) acc_u 21.8750 (13.2812) lr 5.3207e-04 eta 0:00:12
epoch [133/200] batch [45/66] time 0.467 (0.466) data 0.335 (0.335) loss_u loss_u 0.8140 (0.8815) acc_u 18.7500 (13.5417) lr 5.3207e-04 eta 0:00:09
epoch [133/200] batch [50/66] time 0.535 (0.465) data 0.404 (0.334) loss_u loss_u 0.9253 (0.8821) acc_u 6.2500 (13.3750) lr 5.3207e-04 eta 0:00:07
epoch [133/200] batch [55/66] time 0.376 (0.464) data 0.245 (0.333) loss_u loss_u 0.8037 (0.8814) acc_u 25.0000 (13.6364) lr 5.3207e-04 eta 0:00:05
epoch [133/200] batch [60/66] time 0.468 (0.467) data 0.334 (0.336) loss_u loss_u 0.9014 (0.8809) acc_u 9.3750 (13.6979) lr 5.3207e-04 eta 0:00:02
epoch [133/200] batch [65/66] time 0.429 (0.464) data 0.299 (0.333) loss_u loss_u 0.8525 (0.8788) acc_u 21.8750 (14.0865) lr 5.3207e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1581
confident_label rate tensor(0.3262, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1023
clean true:897
clean false:126
clean_rate:0.8768328445747801
noisy true:658
noisy false:1455
after delete: len(clean_dataset) 1023
after delete: len(noisy_dataset) 2113
epoch [134/200] batch [5/31] time 0.458 (0.482) data 0.327 (0.351) loss_x loss_x 1.4297 (1.2116) acc_x 59.3750 (69.3750) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [10/31] time 0.484 (0.496) data 0.355 (0.366) loss_x loss_x 1.0977 (1.1683) acc_x 59.3750 (68.4375) lr 5.1825e-04 eta 0:00:10
epoch [134/200] batch [15/31] time 0.576 (0.490) data 0.444 (0.360) loss_x loss_x 1.3984 (1.2410) acc_x 65.6250 (67.5000) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [20/31] time 0.406 (0.477) data 0.276 (0.346) loss_x loss_x 1.2158 (1.2111) acc_x 65.6250 (67.9688) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [25/31] time 0.529 (0.477) data 0.399 (0.347) loss_x loss_x 1.2266 (1.2428) acc_x 71.8750 (67.3750) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [30/31] time 0.589 (0.480) data 0.459 (0.349) loss_x loss_x 0.9224 (1.2123) acc_x 71.8750 (67.8125) lr 5.1825e-04 eta 0:00:00
epoch [134/200] batch [5/66] time 0.563 (0.486) data 0.431 (0.356) loss_u loss_u 0.9199 (0.8793) acc_u 9.3750 (14.3750) lr 5.1825e-04 eta 0:00:29
epoch [134/200] batch [10/66] time 0.610 (0.487) data 0.479 (0.356) loss_u loss_u 0.8467 (0.8812) acc_u 18.7500 (13.7500) lr 5.1825e-04 eta 0:00:27
epoch [134/200] batch [15/66] time 0.409 (0.480) data 0.277 (0.349) loss_u loss_u 0.8354 (0.8820) acc_u 25.0000 (14.5833) lr 5.1825e-04 eta 0:00:24
epoch [134/200] batch [20/66] time 0.380 (0.482) data 0.248 (0.351) loss_u loss_u 0.9062 (0.8805) acc_u 12.5000 (14.6875) lr 5.1825e-04 eta 0:00:22
epoch [134/200] batch [25/66] time 0.523 (0.480) data 0.392 (0.349) loss_u loss_u 0.8608 (0.8870) acc_u 15.6250 (14.1250) lr 5.1825e-04 eta 0:00:19
epoch [134/200] batch [30/66] time 0.477 (0.480) data 0.346 (0.349) loss_u loss_u 0.8506 (0.8816) acc_u 25.0000 (15.0000) lr 5.1825e-04 eta 0:00:17
epoch [134/200] batch [35/66] time 0.547 (0.480) data 0.417 (0.349) loss_u loss_u 0.9106 (0.8815) acc_u 12.5000 (15.3571) lr 5.1825e-04 eta 0:00:14
epoch [134/200] batch [40/66] time 0.447 (0.476) data 0.314 (0.345) loss_u loss_u 0.9028 (0.8818) acc_u 12.5000 (15.2344) lr 5.1825e-04 eta 0:00:12
epoch [134/200] batch [45/66] time 0.451 (0.475) data 0.321 (0.344) loss_u loss_u 0.8804 (0.8830) acc_u 12.5000 (14.8611) lr 5.1825e-04 eta 0:00:09
epoch [134/200] batch [50/66] time 0.446 (0.472) data 0.316 (0.341) loss_u loss_u 0.8584 (0.8825) acc_u 15.6250 (14.8750) lr 5.1825e-04 eta 0:00:07
epoch [134/200] batch [55/66] time 0.450 (0.469) data 0.318 (0.338) loss_u loss_u 0.8901 (0.8802) acc_u 15.6250 (15.3409) lr 5.1825e-04 eta 0:00:05
epoch [134/200] batch [60/66] time 0.515 (0.469) data 0.383 (0.338) loss_u loss_u 0.8901 (0.8792) acc_u 18.7500 (15.3125) lr 5.1825e-04 eta 0:00:02
epoch [134/200] batch [65/66] time 0.392 (0.469) data 0.261 (0.338) loss_u loss_u 0.9302 (0.8779) acc_u 3.1250 (15.4327) lr 5.1825e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1645
confident_label rate tensor(0.3246, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1018
clean true:877
clean false:141
clean_rate:0.8614931237721022
noisy true:614
noisy false:1504
after delete: len(clean_dataset) 1018
after delete: len(noisy_dataset) 2118
epoch [135/200] batch [5/31] time 0.476 (0.467) data 0.344 (0.336) loss_x loss_x 1.4932 (1.0449) acc_x 68.7500 (73.1250) lr 5.0454e-04 eta 0:00:12
epoch [135/200] batch [10/31] time 0.452 (0.470) data 0.322 (0.340) loss_x loss_x 0.9795 (1.1275) acc_x 75.0000 (71.5625) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [15/31] time 0.392 (0.464) data 0.262 (0.333) loss_x loss_x 1.0947 (1.1580) acc_x 71.8750 (71.2500) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [20/31] time 0.369 (0.461) data 0.238 (0.330) loss_x loss_x 0.8647 (1.1231) acc_x 71.8750 (71.8750) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [25/31] time 0.439 (0.472) data 0.309 (0.341) loss_x loss_x 1.6562 (1.1579) acc_x 59.3750 (70.3750) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [30/31] time 0.398 (0.475) data 0.268 (0.345) loss_x loss_x 0.7817 (1.1353) acc_x 78.1250 (70.7292) lr 5.0454e-04 eta 0:00:00
epoch [135/200] batch [5/66] time 0.316 (0.464) data 0.184 (0.334) loss_u loss_u 0.8896 (0.8823) acc_u 15.6250 (14.3750) lr 5.0454e-04 eta 0:00:28
epoch [135/200] batch [10/66] time 0.371 (0.458) data 0.240 (0.327) loss_u loss_u 0.7461 (0.8592) acc_u 31.2500 (17.8125) lr 5.0454e-04 eta 0:00:25
epoch [135/200] batch [15/66] time 0.428 (0.461) data 0.296 (0.330) loss_u loss_u 0.9336 (0.8675) acc_u 9.3750 (16.6667) lr 5.0454e-04 eta 0:00:23
epoch [135/200] batch [20/66] time 0.372 (0.457) data 0.240 (0.327) loss_u loss_u 0.8667 (0.8706) acc_u 18.7500 (15.4688) lr 5.0454e-04 eta 0:00:21
epoch [135/200] batch [25/66] time 0.401 (0.463) data 0.271 (0.332) loss_u loss_u 0.8501 (0.8736) acc_u 25.0000 (15.5000) lr 5.0454e-04 eta 0:00:18
epoch [135/200] batch [30/66] time 0.505 (0.465) data 0.373 (0.334) loss_u loss_u 0.9194 (0.8771) acc_u 12.5000 (15.1042) lr 5.0454e-04 eta 0:00:16
epoch [135/200] batch [35/66] time 0.427 (0.465) data 0.296 (0.334) loss_u loss_u 0.9487 (0.8712) acc_u 6.2500 (15.9821) lr 5.0454e-04 eta 0:00:14
epoch [135/200] batch [40/66] time 0.455 (0.463) data 0.324 (0.332) loss_u loss_u 0.8330 (0.8709) acc_u 25.0000 (16.0156) lr 5.0454e-04 eta 0:00:12
epoch [135/200] batch [45/66] time 0.578 (0.464) data 0.448 (0.333) loss_u loss_u 0.8794 (0.8696) acc_u 15.6250 (15.9722) lr 5.0454e-04 eta 0:00:09
epoch [135/200] batch [50/66] time 0.421 (0.462) data 0.290 (0.331) loss_u loss_u 0.8867 (0.8675) acc_u 12.5000 (16.2500) lr 5.0454e-04 eta 0:00:07
epoch [135/200] batch [55/66] time 0.429 (0.461) data 0.298 (0.330) loss_u loss_u 0.7671 (0.8626) acc_u 25.0000 (16.7045) lr 5.0454e-04 eta 0:00:05
epoch [135/200] batch [60/66] time 0.441 (0.458) data 0.309 (0.327) loss_u loss_u 0.8770 (0.8641) acc_u 15.6250 (16.6146) lr 5.0454e-04 eta 0:00:02
epoch [135/200] batch [65/66] time 0.476 (0.457) data 0.344 (0.326) loss_u loss_u 0.8887 (0.8641) acc_u 18.7500 (16.7308) lr 5.0454e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1591
confident_label rate tensor(0.3237, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1015
clean true:888
clean false:127
clean_rate:0.8748768472906404
noisy true:657
noisy false:1464
after delete: len(clean_dataset) 1015
after delete: len(noisy_dataset) 2121
epoch [136/200] batch [5/31] time 0.410 (0.511) data 0.279 (0.380) loss_x loss_x 0.9326 (0.9030) acc_x 78.1250 (78.7500) lr 4.9096e-04 eta 0:00:13
epoch [136/200] batch [10/31] time 0.556 (0.478) data 0.426 (0.347) loss_x loss_x 0.6079 (1.0311) acc_x 87.5000 (74.3750) lr 4.9096e-04 eta 0:00:10
epoch [136/200] batch [15/31] time 0.519 (0.473) data 0.388 (0.342) loss_x loss_x 0.9033 (1.0958) acc_x 78.1250 (71.6667) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [20/31] time 0.505 (0.472) data 0.374 (0.341) loss_x loss_x 0.7856 (1.1222) acc_x 75.0000 (70.4688) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [25/31] time 0.537 (0.473) data 0.406 (0.342) loss_x loss_x 1.2188 (1.1359) acc_x 65.6250 (70.7500) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [30/31] time 0.640 (0.479) data 0.508 (0.348) loss_x loss_x 1.6426 (1.1392) acc_x 62.5000 (70.7292) lr 4.9096e-04 eta 0:00:00
epoch [136/200] batch [5/66] time 0.402 (0.468) data 0.271 (0.337) loss_u loss_u 0.8760 (0.8515) acc_u 15.6250 (16.8750) lr 4.9096e-04 eta 0:00:28
epoch [136/200] batch [10/66] time 0.455 (0.472) data 0.324 (0.341) loss_u loss_u 0.7544 (0.8544) acc_u 28.1250 (17.1875) lr 4.9096e-04 eta 0:00:26
epoch [136/200] batch [15/66] time 0.452 (0.474) data 0.319 (0.342) loss_u loss_u 0.9268 (0.8663) acc_u 3.1250 (15.2083) lr 4.9096e-04 eta 0:00:24
epoch [136/200] batch [20/66] time 0.425 (0.467) data 0.293 (0.335) loss_u loss_u 0.9092 (0.8710) acc_u 12.5000 (14.8438) lr 4.9096e-04 eta 0:00:21
epoch [136/200] batch [25/66] time 0.363 (0.461) data 0.233 (0.329) loss_u loss_u 0.8760 (0.8679) acc_u 18.7500 (15.7500) lr 4.9096e-04 eta 0:00:18
epoch [136/200] batch [30/66] time 0.418 (0.460) data 0.286 (0.329) loss_u loss_u 0.8979 (0.8688) acc_u 9.3750 (15.6250) lr 4.9096e-04 eta 0:00:16
epoch [136/200] batch [35/66] time 0.415 (0.462) data 0.283 (0.330) loss_u loss_u 0.8833 (0.8710) acc_u 18.7500 (15.5357) lr 4.9096e-04 eta 0:00:14
epoch [136/200] batch [40/66] time 0.430 (0.464) data 0.300 (0.333) loss_u loss_u 0.9102 (0.8705) acc_u 12.5000 (15.7812) lr 4.9096e-04 eta 0:00:12
epoch [136/200] batch [45/66] time 0.482 (0.464) data 0.351 (0.333) loss_u loss_u 0.8862 (0.8717) acc_u 9.3750 (15.6250) lr 4.9096e-04 eta 0:00:09
epoch [136/200] batch [50/66] time 0.439 (0.464) data 0.308 (0.332) loss_u loss_u 0.9263 (0.8697) acc_u 9.3750 (16.1875) lr 4.9096e-04 eta 0:00:07
epoch [136/200] batch [55/66] time 0.405 (0.465) data 0.273 (0.333) loss_u loss_u 0.8613 (0.8694) acc_u 15.6250 (16.2500) lr 4.9096e-04 eta 0:00:05
epoch [136/200] batch [60/66] time 0.394 (0.466) data 0.263 (0.334) loss_u loss_u 0.8413 (0.8692) acc_u 18.7500 (16.5104) lr 4.9096e-04 eta 0:00:02
epoch [136/200] batch [65/66] time 0.498 (0.468) data 0.368 (0.337) loss_u loss_u 0.8643 (0.8691) acc_u 18.7500 (16.4423) lr 4.9096e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1598
confident_label rate tensor(0.3294, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1033
clean true:887
clean false:146
clean_rate:0.8586640851887706
noisy true:651
noisy false:1452
after delete: len(clean_dataset) 1033
after delete: len(noisy_dataset) 2103
epoch [137/200] batch [5/32] time 0.539 (0.494) data 0.408 (0.363) loss_x loss_x 1.2881 (1.0897) acc_x 68.7500 (70.0000) lr 4.7750e-04 eta 0:00:13
epoch [137/200] batch [10/32] time 0.480 (0.457) data 0.350 (0.327) loss_x loss_x 0.7280 (1.1512) acc_x 81.2500 (70.9375) lr 4.7750e-04 eta 0:00:10
epoch [137/200] batch [15/32] time 0.429 (0.467) data 0.299 (0.336) loss_x loss_x 1.3164 (1.1445) acc_x 62.5000 (71.4583) lr 4.7750e-04 eta 0:00:07
epoch [137/200] batch [20/32] time 0.408 (0.476) data 0.277 (0.345) loss_x loss_x 1.0938 (1.1759) acc_x 71.8750 (70.4688) lr 4.7750e-04 eta 0:00:05
epoch [137/200] batch [25/32] time 0.480 (0.473) data 0.350 (0.342) loss_x loss_x 1.0283 (1.1837) acc_x 71.8750 (70.5000) lr 4.7750e-04 eta 0:00:03
epoch [137/200] batch [30/32] time 0.407 (0.476) data 0.277 (0.345) loss_x loss_x 0.8774 (1.1440) acc_x 75.0000 (71.4583) lr 4.7750e-04 eta 0:00:00
epoch [137/200] batch [5/65] time 0.525 (0.488) data 0.394 (0.357) loss_u loss_u 0.8813 (0.8621) acc_u 15.6250 (15.6250) lr 4.7750e-04 eta 0:00:29
epoch [137/200] batch [10/65] time 0.458 (0.482) data 0.326 (0.351) loss_u loss_u 0.8232 (0.8656) acc_u 25.0000 (16.8750) lr 4.7750e-04 eta 0:00:26
epoch [137/200] batch [15/65] time 0.400 (0.477) data 0.268 (0.346) loss_u loss_u 0.8428 (0.8589) acc_u 15.6250 (16.8750) lr 4.7750e-04 eta 0:00:23
epoch [137/200] batch [20/65] time 0.344 (0.469) data 0.213 (0.338) loss_u loss_u 0.8965 (0.8610) acc_u 12.5000 (16.4062) lr 4.7750e-04 eta 0:00:21
epoch [137/200] batch [25/65] time 0.521 (0.468) data 0.390 (0.337) loss_u loss_u 0.9053 (0.8626) acc_u 15.6250 (16.1250) lr 4.7750e-04 eta 0:00:18
epoch [137/200] batch [30/65] time 0.435 (0.465) data 0.305 (0.334) loss_u loss_u 0.8784 (0.8688) acc_u 12.5000 (15.5208) lr 4.7750e-04 eta 0:00:16
epoch [137/200] batch [35/65] time 0.439 (0.463) data 0.308 (0.331) loss_u loss_u 0.9038 (0.8740) acc_u 9.3750 (14.9107) lr 4.7750e-04 eta 0:00:13
epoch [137/200] batch [40/65] time 0.487 (0.465) data 0.355 (0.334) loss_u loss_u 0.9102 (0.8733) acc_u 9.3750 (14.8438) lr 4.7750e-04 eta 0:00:11
epoch [137/200] batch [45/65] time 0.504 (0.463) data 0.373 (0.332) loss_u loss_u 0.8682 (0.8742) acc_u 18.7500 (15.2778) lr 4.7750e-04 eta 0:00:09
epoch [137/200] batch [50/65] time 0.415 (0.459) data 0.283 (0.328) loss_u loss_u 0.9512 (0.8713) acc_u 9.3750 (15.9375) lr 4.7750e-04 eta 0:00:06
epoch [137/200] batch [55/65] time 0.340 (0.460) data 0.208 (0.329) loss_u loss_u 0.8662 (0.8724) acc_u 18.7500 (15.9659) lr 4.7750e-04 eta 0:00:04
epoch [137/200] batch [60/65] time 0.471 (0.458) data 0.341 (0.327) loss_u loss_u 0.8735 (0.8744) acc_u 15.6250 (15.7812) lr 4.7750e-04 eta 0:00:02
epoch [137/200] batch [65/65] time 0.469 (0.462) data 0.338 (0.330) loss_u loss_u 0.9160 (0.8758) acc_u 6.2500 (15.5769) lr 4.7750e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1596
confident_label rate tensor(0.3246, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1018
clean true:882
clean false:136
clean_rate:0.8664047151277013
noisy true:658
noisy false:1460
after delete: len(clean_dataset) 1018
after delete: len(noisy_dataset) 2118
epoch [138/200] batch [5/31] time 0.410 (0.472) data 0.279 (0.341) loss_x loss_x 1.1084 (1.1258) acc_x 75.0000 (71.2500) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [10/31] time 0.485 (0.487) data 0.354 (0.357) loss_x loss_x 0.9873 (1.1044) acc_x 71.8750 (72.8125) lr 4.6417e-04 eta 0:00:10
epoch [138/200] batch [15/31] time 0.471 (0.483) data 0.341 (0.353) loss_x loss_x 1.4844 (1.1551) acc_x 50.0000 (70.4167) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [20/31] time 0.414 (0.481) data 0.284 (0.350) loss_x loss_x 0.4497 (1.0836) acc_x 90.6250 (71.5625) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [25/31] time 0.465 (0.481) data 0.334 (0.351) loss_x loss_x 0.8984 (1.0778) acc_x 78.1250 (72.2500) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [30/31] time 0.461 (0.480) data 0.331 (0.349) loss_x loss_x 1.6387 (1.0965) acc_x 53.1250 (70.7292) lr 4.6417e-04 eta 0:00:00
epoch [138/200] batch [5/66] time 0.351 (0.473) data 0.219 (0.342) loss_u loss_u 0.8960 (0.8711) acc_u 15.6250 (18.1250) lr 4.6417e-04 eta 0:00:28
epoch [138/200] batch [10/66] time 0.503 (0.465) data 0.370 (0.335) loss_u loss_u 0.9282 (0.8665) acc_u 6.2500 (16.2500) lr 4.6417e-04 eta 0:00:26
epoch [138/200] batch [15/66] time 0.511 (0.472) data 0.378 (0.341) loss_u loss_u 0.8853 (0.8789) acc_u 18.7500 (15.0000) lr 4.6417e-04 eta 0:00:24
epoch [138/200] batch [20/66] time 0.449 (0.467) data 0.318 (0.336) loss_u loss_u 0.8057 (0.8697) acc_u 21.8750 (16.0938) lr 4.6417e-04 eta 0:00:21
epoch [138/200] batch [25/66] time 0.433 (0.468) data 0.301 (0.337) loss_u loss_u 0.8799 (0.8701) acc_u 15.6250 (16.0000) lr 4.6417e-04 eta 0:00:19
epoch [138/200] batch [30/66] time 0.427 (0.468) data 0.297 (0.337) loss_u loss_u 0.9209 (0.8731) acc_u 9.3750 (15.8333) lr 4.6417e-04 eta 0:00:16
epoch [138/200] batch [35/66] time 0.563 (0.467) data 0.432 (0.336) loss_u loss_u 0.9248 (0.8734) acc_u 9.3750 (15.6250) lr 4.6417e-04 eta 0:00:14
epoch [138/200] batch [40/66] time 0.461 (0.466) data 0.329 (0.335) loss_u loss_u 0.9180 (0.8791) acc_u 9.3750 (14.6875) lr 4.6417e-04 eta 0:00:12
epoch [138/200] batch [45/66] time 0.432 (0.462) data 0.301 (0.331) loss_u loss_u 0.8721 (0.8780) acc_u 15.6250 (15.0694) lr 4.6417e-04 eta 0:00:09
epoch [138/200] batch [50/66] time 0.464 (0.457) data 0.332 (0.326) loss_u loss_u 0.8169 (0.8747) acc_u 25.0000 (15.5000) lr 4.6417e-04 eta 0:00:07
epoch [138/200] batch [55/66] time 0.603 (0.459) data 0.471 (0.327) loss_u loss_u 0.8384 (0.8737) acc_u 21.8750 (15.7955) lr 4.6417e-04 eta 0:00:05
epoch [138/200] batch [60/66] time 0.546 (0.460) data 0.414 (0.329) loss_u loss_u 0.8774 (0.8729) acc_u 15.6250 (15.8854) lr 4.6417e-04 eta 0:00:02
epoch [138/200] batch [65/66] time 0.568 (0.464) data 0.436 (0.332) loss_u loss_u 0.9111 (0.8741) acc_u 12.5000 (15.5769) lr 4.6417e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1600
confident_label rate tensor(0.3253, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1020
clean true:887
clean false:133
clean_rate:0.8696078431372549
noisy true:649
noisy false:1467
after delete: len(clean_dataset) 1020
after delete: len(noisy_dataset) 2116
epoch [139/200] batch [5/31] time 0.472 (0.464) data 0.341 (0.333) loss_x loss_x 0.9487 (1.1958) acc_x 65.6250 (68.7500) lr 4.5098e-04 eta 0:00:12
epoch [139/200] batch [10/31] time 0.412 (0.469) data 0.280 (0.338) loss_x loss_x 1.5107 (1.2339) acc_x 71.8750 (68.4375) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [15/31] time 0.385 (0.468) data 0.255 (0.337) loss_x loss_x 0.9204 (1.1748) acc_x 78.1250 (69.1667) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [20/31] time 0.510 (0.459) data 0.380 (0.329) loss_x loss_x 0.7134 (1.1569) acc_x 90.6250 (70.4688) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [25/31] time 0.384 (0.458) data 0.254 (0.327) loss_x loss_x 1.0039 (1.1349) acc_x 71.8750 (71.1250) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [30/31] time 0.609 (0.464) data 0.479 (0.333) loss_x loss_x 0.9106 (1.1653) acc_x 81.2500 (70.6250) lr 4.5098e-04 eta 0:00:00
epoch [139/200] batch [5/66] time 0.507 (0.458) data 0.373 (0.327) loss_u loss_u 0.8877 (0.8766) acc_u 9.3750 (14.3750) lr 4.5098e-04 eta 0:00:27
epoch [139/200] batch [10/66] time 0.398 (0.452) data 0.266 (0.321) loss_u loss_u 0.8037 (0.8690) acc_u 25.0000 (16.2500) lr 4.5098e-04 eta 0:00:25
epoch [139/200] batch [15/66] time 0.496 (0.457) data 0.364 (0.326) loss_u loss_u 0.8706 (0.8664) acc_u 12.5000 (16.2500) lr 4.5098e-04 eta 0:00:23
epoch [139/200] batch [20/66] time 0.378 (0.452) data 0.246 (0.321) loss_u loss_u 0.8345 (0.8649) acc_u 21.8750 (16.5625) lr 4.5098e-04 eta 0:00:20
epoch [139/200] batch [25/66] time 0.567 (0.455) data 0.436 (0.323) loss_u loss_u 0.8325 (0.8690) acc_u 21.8750 (16.1250) lr 4.5098e-04 eta 0:00:18
epoch [139/200] batch [30/66] time 0.483 (0.456) data 0.351 (0.325) loss_u loss_u 0.9473 (0.8762) acc_u 9.3750 (15.1042) lr 4.5098e-04 eta 0:00:16
epoch [139/200] batch [35/66] time 0.420 (0.459) data 0.288 (0.327) loss_u loss_u 0.8931 (0.8727) acc_u 12.5000 (15.5357) lr 4.5098e-04 eta 0:00:14
epoch [139/200] batch [40/66] time 0.457 (0.461) data 0.326 (0.330) loss_u loss_u 0.8638 (0.8733) acc_u 12.5000 (15.3906) lr 4.5098e-04 eta 0:00:11
epoch [139/200] batch [45/66] time 0.316 (0.458) data 0.184 (0.327) loss_u loss_u 0.9473 (0.8784) acc_u 6.2500 (14.5139) lr 4.5098e-04 eta 0:00:09
epoch [139/200] batch [50/66] time 0.380 (0.456) data 0.249 (0.325) loss_u loss_u 0.8340 (0.8783) acc_u 21.8750 (14.5000) lr 4.5098e-04 eta 0:00:07
epoch [139/200] batch [55/66] time 0.389 (0.458) data 0.258 (0.326) loss_u loss_u 0.8325 (0.8767) acc_u 18.7500 (14.6023) lr 4.5098e-04 eta 0:00:05
epoch [139/200] batch [60/66] time 0.399 (0.459) data 0.267 (0.327) loss_u loss_u 0.8545 (0.8758) acc_u 18.7500 (14.7917) lr 4.5098e-04 eta 0:00:02
epoch [139/200] batch [65/66] time 0.388 (0.456) data 0.256 (0.324) loss_u loss_u 0.9434 (0.8745) acc_u 6.2500 (15.1923) lr 4.5098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1598
confident_label rate tensor(0.3326, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1043
clean true:894
clean false:149
clean_rate:0.8571428571428571
noisy true:644
noisy false:1449
after delete: len(clean_dataset) 1043
after delete: len(noisy_dataset) 2093
epoch [140/200] batch [5/32] time 0.495 (0.456) data 0.365 (0.325) loss_x loss_x 1.4590 (1.1999) acc_x 62.5000 (68.7500) lr 4.3792e-04 eta 0:00:12
epoch [140/200] batch [10/32] time 0.362 (0.449) data 0.231 (0.319) loss_x loss_x 1.6816 (1.2457) acc_x 59.3750 (68.1250) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [15/32] time 0.533 (0.471) data 0.402 (0.340) loss_x loss_x 1.6602 (1.2271) acc_x 62.5000 (69.5833) lr 4.3792e-04 eta 0:00:08
epoch [140/200] batch [20/32] time 0.525 (0.468) data 0.394 (0.338) loss_x loss_x 1.2744 (1.1784) acc_x 68.7500 (71.2500) lr 4.3792e-04 eta 0:00:05
epoch [140/200] batch [25/32] time 0.407 (0.469) data 0.276 (0.338) loss_x loss_x 1.2959 (1.1447) acc_x 65.6250 (72.0000) lr 4.3792e-04 eta 0:00:03
epoch [140/200] batch [30/32] time 0.521 (0.475) data 0.391 (0.344) loss_x loss_x 1.0615 (1.1403) acc_x 65.6250 (71.6667) lr 4.3792e-04 eta 0:00:00
epoch [140/200] batch [5/65] time 0.368 (0.465) data 0.236 (0.334) loss_u loss_u 0.9150 (0.8969) acc_u 6.2500 (11.2500) lr 4.3792e-04 eta 0:00:27
epoch [140/200] batch [10/65] time 0.479 (0.472) data 0.348 (0.341) loss_u loss_u 0.8257 (0.8896) acc_u 18.7500 (12.8125) lr 4.3792e-04 eta 0:00:25
epoch [140/200] batch [15/65] time 0.461 (0.467) data 0.330 (0.336) loss_u loss_u 0.9277 (0.8836) acc_u 6.2500 (13.5417) lr 4.3792e-04 eta 0:00:23
epoch [140/200] batch [20/65] time 0.339 (0.457) data 0.208 (0.327) loss_u loss_u 0.7900 (0.8750) acc_u 28.1250 (14.6875) lr 4.3792e-04 eta 0:00:20
epoch [140/200] batch [25/65] time 0.467 (0.460) data 0.336 (0.329) loss_u loss_u 0.9272 (0.8796) acc_u 9.3750 (14.6250) lr 4.3792e-04 eta 0:00:18
epoch [140/200] batch [30/65] time 0.462 (0.460) data 0.330 (0.329) loss_u loss_u 0.8188 (0.8766) acc_u 21.8750 (15.0000) lr 4.3792e-04 eta 0:00:16
epoch [140/200] batch [35/65] time 0.400 (0.461) data 0.269 (0.331) loss_u loss_u 0.8750 (0.8748) acc_u 12.5000 (15.0893) lr 4.3792e-04 eta 0:00:13
epoch [140/200] batch [40/65] time 0.417 (0.461) data 0.285 (0.330) loss_u loss_u 0.8477 (0.8725) acc_u 18.7500 (15.2344) lr 4.3792e-04 eta 0:00:11
epoch [140/200] batch [45/65] time 0.457 (0.462) data 0.326 (0.331) loss_u loss_u 0.7524 (0.8685) acc_u 28.1250 (15.8333) lr 4.3792e-04 eta 0:00:09
epoch [140/200] batch [50/65] time 0.496 (0.466) data 0.365 (0.335) loss_u loss_u 0.8545 (0.8719) acc_u 21.8750 (15.4375) lr 4.3792e-04 eta 0:00:06
epoch [140/200] batch [55/65] time 0.367 (0.463) data 0.235 (0.332) loss_u loss_u 0.8735 (0.8722) acc_u 15.6250 (15.5682) lr 4.3792e-04 eta 0:00:04
epoch [140/200] batch [60/65] time 0.415 (0.465) data 0.284 (0.334) loss_u loss_u 0.8833 (0.8736) acc_u 12.5000 (15.5208) lr 4.3792e-04 eta 0:00:02
epoch [140/200] batch [65/65] time 0.425 (0.466) data 0.293 (0.335) loss_u loss_u 0.9043 (0.8752) acc_u 12.5000 (15.3846) lr 4.3792e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1570
confident_label rate tensor(0.3304, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1036
clean true:907
clean false:129
clean_rate:0.8754826254826255
noisy true:659
noisy false:1441
after delete: len(clean_dataset) 1036
after delete: len(noisy_dataset) 2100
epoch [141/200] batch [5/32] time 0.716 (0.496) data 0.585 (0.365) loss_x loss_x 0.6543 (1.0856) acc_x 84.3750 (73.1250) lr 4.2499e-04 eta 0:00:13
epoch [141/200] batch [10/32] time 0.554 (0.519) data 0.424 (0.388) loss_x loss_x 1.1875 (1.1037) acc_x 75.0000 (70.3125) lr 4.2499e-04 eta 0:00:11
epoch [141/200] batch [15/32] time 0.584 (0.519) data 0.453 (0.388) loss_x loss_x 1.4932 (1.1644) acc_x 56.2500 (69.7917) lr 4.2499e-04 eta 0:00:08
epoch [141/200] batch [20/32] time 0.387 (0.503) data 0.257 (0.372) loss_x loss_x 1.4482 (1.2410) acc_x 68.7500 (68.1250) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [25/32] time 0.757 (0.495) data 0.625 (0.364) loss_x loss_x 1.1787 (1.1873) acc_x 81.2500 (70.0000) lr 4.2499e-04 eta 0:00:03
epoch [141/200] batch [30/32] time 0.521 (0.497) data 0.391 (0.366) loss_x loss_x 0.6611 (1.1393) acc_x 84.3750 (71.2500) lr 4.2499e-04 eta 0:00:00
epoch [141/200] batch [5/65] time 0.380 (0.484) data 0.248 (0.353) loss_u loss_u 0.9053 (0.8678) acc_u 12.5000 (16.8750) lr 4.2499e-04 eta 0:00:29
epoch [141/200] batch [10/65] time 0.420 (0.475) data 0.288 (0.344) loss_u loss_u 0.8906 (0.8654) acc_u 15.6250 (19.3750) lr 4.2499e-04 eta 0:00:26
epoch [141/200] batch [15/65] time 0.427 (0.469) data 0.295 (0.338) loss_u loss_u 0.9136 (0.8673) acc_u 6.2500 (18.3333) lr 4.2499e-04 eta 0:00:23
epoch [141/200] batch [20/65] time 0.419 (0.468) data 0.287 (0.337) loss_u loss_u 0.7900 (0.8662) acc_u 25.0000 (17.5000) lr 4.2499e-04 eta 0:00:21
epoch [141/200] batch [25/65] time 0.380 (0.465) data 0.249 (0.334) loss_u loss_u 0.8618 (0.8719) acc_u 18.7500 (16.6250) lr 4.2499e-04 eta 0:00:18
epoch [141/200] batch [30/65] time 0.551 (0.465) data 0.420 (0.334) loss_u loss_u 0.9097 (0.8704) acc_u 12.5000 (16.7708) lr 4.2499e-04 eta 0:00:16
epoch [141/200] batch [35/65] time 0.383 (0.467) data 0.251 (0.336) loss_u loss_u 0.8701 (0.8690) acc_u 15.6250 (16.8750) lr 4.2499e-04 eta 0:00:14
epoch [141/200] batch [40/65] time 0.406 (0.468) data 0.275 (0.336) loss_u loss_u 0.8921 (0.8718) acc_u 12.5000 (16.5625) lr 4.2499e-04 eta 0:00:11
epoch [141/200] batch [45/65] time 0.466 (0.468) data 0.335 (0.336) loss_u loss_u 0.9448 (0.8733) acc_u 6.2500 (16.5278) lr 4.2499e-04 eta 0:00:09
epoch [141/200] batch [50/65] time 0.341 (0.463) data 0.209 (0.332) loss_u loss_u 0.8799 (0.8730) acc_u 12.5000 (16.3750) lr 4.2499e-04 eta 0:00:06
epoch [141/200] batch [55/65] time 0.418 (0.462) data 0.286 (0.331) loss_u loss_u 0.9224 (0.8719) acc_u 6.2500 (16.4205) lr 4.2499e-04 eta 0:00:04
epoch [141/200] batch [60/65] time 0.363 (0.459) data 0.231 (0.328) loss_u loss_u 0.8677 (0.8694) acc_u 15.6250 (16.8229) lr 4.2499e-04 eta 0:00:02
epoch [141/200] batch [65/65] time 0.378 (0.456) data 0.248 (0.325) loss_u loss_u 0.8843 (0.8737) acc_u 6.2500 (16.1538) lr 4.2499e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1604
confident_label rate tensor(0.3284, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1030
clean true:890
clean false:140
clean_rate:0.8640776699029126
noisy true:642
noisy false:1464
after delete: len(clean_dataset) 1030
after delete: len(noisy_dataset) 2106
epoch [142/200] batch [5/32] time 0.524 (0.521) data 0.394 (0.390) loss_x loss_x 1.1309 (0.9824) acc_x 71.8750 (77.5000) lr 4.1221e-04 eta 0:00:14
epoch [142/200] batch [10/32] time 0.400 (0.474) data 0.270 (0.344) loss_x loss_x 1.5068 (1.1027) acc_x 68.7500 (75.9375) lr 4.1221e-04 eta 0:00:10
epoch [142/200] batch [15/32] time 0.442 (0.461) data 0.310 (0.331) loss_x loss_x 1.5127 (1.1023) acc_x 65.6250 (74.7917) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [20/32] time 0.403 (0.454) data 0.273 (0.323) loss_x loss_x 0.9463 (1.1027) acc_x 81.2500 (74.2188) lr 4.1221e-04 eta 0:00:05
epoch [142/200] batch [25/32] time 0.435 (0.465) data 0.304 (0.335) loss_x loss_x 1.1172 (1.1344) acc_x 81.2500 (73.6250) lr 4.1221e-04 eta 0:00:03
epoch [142/200] batch [30/32] time 0.415 (0.477) data 0.284 (0.346) loss_x loss_x 1.8584 (1.1624) acc_x 59.3750 (72.3958) lr 4.1221e-04 eta 0:00:00
epoch [142/200] batch [5/65] time 0.570 (0.480) data 0.439 (0.349) loss_u loss_u 0.9043 (0.9181) acc_u 12.5000 (9.3750) lr 4.1221e-04 eta 0:00:28
epoch [142/200] batch [10/65] time 0.676 (0.484) data 0.546 (0.353) loss_u loss_u 0.8252 (0.8963) acc_u 18.7500 (12.1875) lr 4.1221e-04 eta 0:00:26
epoch [142/200] batch [15/65] time 0.529 (0.481) data 0.398 (0.350) loss_u loss_u 0.8745 (0.8808) acc_u 15.6250 (14.5833) lr 4.1221e-04 eta 0:00:24
epoch [142/200] batch [20/65] time 0.526 (0.481) data 0.395 (0.350) loss_u loss_u 0.8389 (0.8650) acc_u 18.7500 (16.7188) lr 4.1221e-04 eta 0:00:21
epoch [142/200] batch [25/65] time 0.416 (0.481) data 0.285 (0.350) loss_u loss_u 0.8521 (0.8683) acc_u 18.7500 (16.2500) lr 4.1221e-04 eta 0:00:19
epoch [142/200] batch [30/65] time 0.513 (0.482) data 0.381 (0.351) loss_u loss_u 0.8442 (0.8643) acc_u 15.6250 (16.3542) lr 4.1221e-04 eta 0:00:16
epoch [142/200] batch [35/65] time 0.484 (0.478) data 0.353 (0.347) loss_u loss_u 0.9028 (0.8637) acc_u 9.3750 (16.3393) lr 4.1221e-04 eta 0:00:14
epoch [142/200] batch [40/65] time 0.509 (0.477) data 0.375 (0.346) loss_u loss_u 0.9189 (0.8633) acc_u 12.5000 (16.4062) lr 4.1221e-04 eta 0:00:11
epoch [142/200] batch [45/65] time 0.353 (0.474) data 0.222 (0.343) loss_u loss_u 0.7959 (0.8658) acc_u 28.1250 (16.1806) lr 4.1221e-04 eta 0:00:09
epoch [142/200] batch [50/65] time 0.360 (0.475) data 0.228 (0.344) loss_u loss_u 0.8232 (0.8673) acc_u 21.8750 (15.8750) lr 4.1221e-04 eta 0:00:07
epoch [142/200] batch [55/65] time 0.331 (0.469) data 0.201 (0.338) loss_u loss_u 0.8667 (0.8677) acc_u 12.5000 (15.6818) lr 4.1221e-04 eta 0:00:04
epoch [142/200] batch [60/65] time 0.635 (0.467) data 0.503 (0.336) loss_u loss_u 0.8921 (0.8695) acc_u 15.6250 (15.6250) lr 4.1221e-04 eta 0:00:02
epoch [142/200] batch [65/65] time 0.425 (0.471) data 0.294 (0.339) loss_u loss_u 0.9600 (0.8724) acc_u 3.1250 (15.1923) lr 4.1221e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1571
confident_label rate tensor(0.3425, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1074
clean true:920
clean false:154
clean_rate:0.8566108007448789
noisy true:645
noisy false:1417
after delete: len(clean_dataset) 1074
after delete: len(noisy_dataset) 2062
epoch [143/200] batch [5/33] time 0.483 (0.534) data 0.354 (0.403) loss_x loss_x 1.3633 (1.3291) acc_x 68.7500 (66.8750) lr 3.9958e-04 eta 0:00:14
epoch [143/200] batch [10/33] time 0.476 (0.505) data 0.346 (0.374) loss_x loss_x 0.8550 (1.2155) acc_x 78.1250 (69.3750) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [15/33] time 0.452 (0.490) data 0.321 (0.359) loss_x loss_x 0.7749 (1.1600) acc_x 81.2500 (70.8333) lr 3.9958e-04 eta 0:00:08
epoch [143/200] batch [20/33] time 0.510 (0.482) data 0.379 (0.351) loss_x loss_x 0.9468 (1.1309) acc_x 75.0000 (70.3125) lr 3.9958e-04 eta 0:00:06
epoch [143/200] batch [25/33] time 0.456 (0.481) data 0.327 (0.350) loss_x loss_x 1.0830 (1.1424) acc_x 71.8750 (69.5000) lr 3.9958e-04 eta 0:00:03
epoch [143/200] batch [30/33] time 0.504 (0.474) data 0.374 (0.344) loss_x loss_x 0.7476 (1.1203) acc_x 81.2500 (70.6250) lr 3.9958e-04 eta 0:00:01
epoch [143/200] batch [5/64] time 0.404 (0.463) data 0.274 (0.332) loss_u loss_u 0.8979 (0.8833) acc_u 9.3750 (13.1250) lr 3.9958e-04 eta 0:00:27
epoch [143/200] batch [10/64] time 0.415 (0.457) data 0.284 (0.327) loss_u loss_u 0.8545 (0.8751) acc_u 21.8750 (14.6875) lr 3.9958e-04 eta 0:00:24
epoch [143/200] batch [15/64] time 0.420 (0.454) data 0.290 (0.324) loss_u loss_u 0.8931 (0.8701) acc_u 15.6250 (15.8333) lr 3.9958e-04 eta 0:00:22
epoch [143/200] batch [20/64] time 0.409 (0.461) data 0.279 (0.331) loss_u loss_u 0.9341 (0.8796) acc_u 9.3750 (14.3750) lr 3.9958e-04 eta 0:00:20
epoch [143/200] batch [25/64] time 0.442 (0.460) data 0.310 (0.329) loss_u loss_u 0.8789 (0.8777) acc_u 21.8750 (15.0000) lr 3.9958e-04 eta 0:00:17
epoch [143/200] batch [30/64] time 0.449 (0.458) data 0.318 (0.327) loss_u loss_u 0.8789 (0.8791) acc_u 15.6250 (14.7917) lr 3.9958e-04 eta 0:00:15
epoch [143/200] batch [35/64] time 0.638 (0.466) data 0.508 (0.335) loss_u loss_u 0.8452 (0.8807) acc_u 15.6250 (14.2857) lr 3.9958e-04 eta 0:00:13
epoch [143/200] batch [40/64] time 0.497 (0.464) data 0.365 (0.333) loss_u loss_u 0.8740 (0.8809) acc_u 18.7500 (14.1406) lr 3.9958e-04 eta 0:00:11
epoch [143/200] batch [45/64] time 0.400 (0.463) data 0.269 (0.332) loss_u loss_u 0.8779 (0.8792) acc_u 15.6250 (14.3056) lr 3.9958e-04 eta 0:00:08
epoch [143/200] batch [50/64] time 0.352 (0.460) data 0.221 (0.329) loss_u loss_u 0.8643 (0.8789) acc_u 15.6250 (14.5000) lr 3.9958e-04 eta 0:00:06
epoch [143/200] batch [55/64] time 0.369 (0.462) data 0.237 (0.331) loss_u loss_u 0.8745 (0.8798) acc_u 15.6250 (14.3750) lr 3.9958e-04 eta 0:00:04
epoch [143/200] batch [60/64] time 0.531 (0.462) data 0.399 (0.331) loss_u loss_u 0.8086 (0.8799) acc_u 21.8750 (14.3750) lr 3.9958e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1579
confident_label rate tensor(0.3361, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1054
clean true:906
clean false:148
clean_rate:0.8595825426944972
noisy true:651
noisy false:1431
after delete: len(clean_dataset) 1054
after delete: len(noisy_dataset) 2082
epoch [144/200] batch [5/32] time 0.466 (0.440) data 0.335 (0.309) loss_x loss_x 1.2314 (1.1391) acc_x 78.1250 (70.0000) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [10/32] time 0.421 (0.470) data 0.291 (0.339) loss_x loss_x 1.0410 (1.1547) acc_x 75.0000 (70.3125) lr 3.8709e-04 eta 0:00:10
epoch [144/200] batch [15/32] time 0.463 (0.486) data 0.332 (0.355) loss_x loss_x 1.2891 (1.1764) acc_x 65.6250 (68.5417) lr 3.8709e-04 eta 0:00:08
epoch [144/200] batch [20/32] time 0.440 (0.467) data 0.310 (0.337) loss_x loss_x 1.2803 (1.1723) acc_x 62.5000 (69.0625) lr 3.8709e-04 eta 0:00:05
epoch [144/200] batch [25/32] time 0.473 (0.466) data 0.343 (0.336) loss_x loss_x 1.2227 (1.1953) acc_x 65.6250 (68.2500) lr 3.8709e-04 eta 0:00:03
epoch [144/200] batch [30/32] time 0.421 (0.464) data 0.291 (0.333) loss_x loss_x 1.0859 (1.1883) acc_x 75.0000 (68.1250) lr 3.8709e-04 eta 0:00:00
epoch [144/200] batch [5/65] time 0.334 (0.454) data 0.202 (0.324) loss_u loss_u 0.8999 (0.9067) acc_u 9.3750 (10.6250) lr 3.8709e-04 eta 0:00:27
epoch [144/200] batch [10/65] time 0.437 (0.460) data 0.307 (0.330) loss_u loss_u 0.8350 (0.8912) acc_u 18.7500 (12.5000) lr 3.8709e-04 eta 0:00:25
epoch [144/200] batch [15/65] time 0.372 (0.459) data 0.241 (0.328) loss_u loss_u 0.9658 (0.8927) acc_u 3.1250 (13.3333) lr 3.8709e-04 eta 0:00:22
epoch [144/200] batch [20/65] time 0.503 (0.459) data 0.371 (0.328) loss_u loss_u 0.8999 (0.8835) acc_u 15.6250 (14.5312) lr 3.8709e-04 eta 0:00:20
epoch [144/200] batch [25/65] time 0.411 (0.462) data 0.279 (0.331) loss_u loss_u 0.8823 (0.8797) acc_u 9.3750 (14.8750) lr 3.8709e-04 eta 0:00:18
epoch [144/200] batch [30/65] time 0.569 (0.467) data 0.437 (0.336) loss_u loss_u 0.8506 (0.8780) acc_u 18.7500 (14.7917) lr 3.8709e-04 eta 0:00:16
epoch [144/200] batch [35/65] time 0.572 (0.463) data 0.441 (0.332) loss_u loss_u 0.8530 (0.8777) acc_u 15.6250 (14.8214) lr 3.8709e-04 eta 0:00:13
epoch [144/200] batch [40/65] time 0.431 (0.463) data 0.299 (0.333) loss_u loss_u 0.8086 (0.8755) acc_u 25.0000 (15.1562) lr 3.8709e-04 eta 0:00:11
epoch [144/200] batch [45/65] time 0.434 (0.462) data 0.303 (0.331) loss_u loss_u 0.8906 (0.8725) acc_u 18.7500 (15.6250) lr 3.8709e-04 eta 0:00:09
epoch [144/200] batch [50/65] time 0.453 (0.461) data 0.323 (0.330) loss_u loss_u 0.9062 (0.8727) acc_u 12.5000 (15.6250) lr 3.8709e-04 eta 0:00:06
epoch [144/200] batch [55/65] time 0.466 (0.461) data 0.334 (0.330) loss_u loss_u 0.8877 (0.8742) acc_u 12.5000 (15.3977) lr 3.8709e-04 eta 0:00:04
epoch [144/200] batch [60/65] time 0.510 (0.461) data 0.380 (0.330) loss_u loss_u 0.8691 (0.8735) acc_u 12.5000 (15.3646) lr 3.8709e-04 eta 0:00:02
epoch [144/200] batch [65/65] time 0.552 (0.463) data 0.419 (0.332) loss_u loss_u 0.9590 (0.8774) acc_u 3.1250 (14.7596) lr 3.8709e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1597
confident_label rate tensor(0.3275, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1027
clean true:894
clean false:133
clean_rate:0.8704965920155794
noisy true:645
noisy false:1464
after delete: len(clean_dataset) 1027
after delete: len(noisy_dataset) 2109
epoch [145/200] batch [5/32] time 0.410 (0.459) data 0.280 (0.329) loss_x loss_x 0.9897 (1.1925) acc_x 71.8750 (68.7500) lr 3.7476e-04 eta 0:00:12
epoch [145/200] batch [10/32] time 0.542 (0.484) data 0.411 (0.353) loss_x loss_x 0.7544 (1.0880) acc_x 84.3750 (72.8125) lr 3.7476e-04 eta 0:00:10
epoch [145/200] batch [15/32] time 0.605 (0.481) data 0.475 (0.350) loss_x loss_x 1.1523 (1.2032) acc_x 71.8750 (71.0417) lr 3.7476e-04 eta 0:00:08
epoch [145/200] batch [20/32] time 0.403 (0.475) data 0.274 (0.344) loss_x loss_x 1.3643 (1.2198) acc_x 65.6250 (70.0000) lr 3.7476e-04 eta 0:00:05
epoch [145/200] batch [25/32] time 0.404 (0.471) data 0.273 (0.340) loss_x loss_x 1.6299 (1.1969) acc_x 62.5000 (69.5000) lr 3.7476e-04 eta 0:00:03
epoch [145/200] batch [30/32] time 0.496 (0.470) data 0.365 (0.339) loss_x loss_x 1.5547 (1.2020) acc_x 56.2500 (69.1667) lr 3.7476e-04 eta 0:00:00
epoch [145/200] batch [5/65] time 0.415 (0.479) data 0.283 (0.348) loss_u loss_u 0.8970 (0.8358) acc_u 9.3750 (20.0000) lr 3.7476e-04 eta 0:00:28
epoch [145/200] batch [10/65] time 0.499 (0.479) data 0.367 (0.348) loss_u loss_u 0.8354 (0.8604) acc_u 31.2500 (18.1250) lr 3.7476e-04 eta 0:00:26
epoch [145/200] batch [15/65] time 0.410 (0.475) data 0.278 (0.344) loss_u loss_u 0.8604 (0.8703) acc_u 18.7500 (16.6667) lr 3.7476e-04 eta 0:00:23
epoch [145/200] batch [20/65] time 0.436 (0.473) data 0.304 (0.343) loss_u loss_u 0.8896 (0.8732) acc_u 12.5000 (15.9375) lr 3.7476e-04 eta 0:00:21
epoch [145/200] batch [25/65] time 0.522 (0.472) data 0.389 (0.341) loss_u loss_u 0.9331 (0.8723) acc_u 9.3750 (16.3750) lr 3.7476e-04 eta 0:00:18
epoch [145/200] batch [30/65] time 0.348 (0.470) data 0.217 (0.339) loss_u loss_u 0.9189 (0.8745) acc_u 12.5000 (16.2500) lr 3.7476e-04 eta 0:00:16
epoch [145/200] batch [35/65] time 0.510 (0.468) data 0.380 (0.337) loss_u loss_u 0.7603 (0.8737) acc_u 25.0000 (16.1607) lr 3.7476e-04 eta 0:00:14
epoch [145/200] batch [40/65] time 0.531 (0.467) data 0.400 (0.336) loss_u loss_u 0.8193 (0.8706) acc_u 21.8750 (16.4844) lr 3.7476e-04 eta 0:00:11
epoch [145/200] batch [45/65] time 0.463 (0.466) data 0.332 (0.335) loss_u loss_u 0.8486 (0.8729) acc_u 15.6250 (15.9722) lr 3.7476e-04 eta 0:00:09
epoch [145/200] batch [50/65] time 0.403 (0.466) data 0.272 (0.336) loss_u loss_u 0.8687 (0.8714) acc_u 12.5000 (15.9375) lr 3.7476e-04 eta 0:00:06
epoch [145/200] batch [55/65] time 0.493 (0.467) data 0.362 (0.336) loss_u loss_u 0.7837 (0.8704) acc_u 37.5000 (16.3636) lr 3.7476e-04 eta 0:00:04
epoch [145/200] batch [60/65] time 0.502 (0.468) data 0.371 (0.337) loss_u loss_u 0.8921 (0.8710) acc_u 15.6250 (16.1979) lr 3.7476e-04 eta 0:00:02
epoch [145/200] batch [65/65] time 0.384 (0.467) data 0.253 (0.336) loss_u loss_u 0.9390 (0.8720) acc_u 3.1250 (15.8654) lr 3.7476e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1585
confident_label rate tensor(0.3294, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1033
clean true:888
clean false:145
clean_rate:0.8596321393998064
noisy true:663
noisy false:1440
after delete: len(clean_dataset) 1033
after delete: len(noisy_dataset) 2103
epoch [146/200] batch [5/32] time 0.424 (0.484) data 0.293 (0.353) loss_x loss_x 0.9648 (1.1670) acc_x 78.1250 (70.6250) lr 3.6258e-04 eta 0:00:13
epoch [146/200] batch [10/32] time 0.464 (0.496) data 0.333 (0.365) loss_x loss_x 0.6523 (1.0556) acc_x 84.3750 (73.4375) lr 3.6258e-04 eta 0:00:10
epoch [146/200] batch [15/32] time 0.420 (0.478) data 0.289 (0.347) loss_x loss_x 2.0039 (1.1820) acc_x 50.0000 (71.2500) lr 3.6258e-04 eta 0:00:08
epoch [146/200] batch [20/32] time 0.509 (0.476) data 0.379 (0.345) loss_x loss_x 1.2842 (1.1448) acc_x 65.6250 (71.7188) lr 3.6258e-04 eta 0:00:05
epoch [146/200] batch [25/32] time 0.482 (0.481) data 0.350 (0.350) loss_x loss_x 0.7134 (1.1309) acc_x 81.2500 (72.3750) lr 3.6258e-04 eta 0:00:03
epoch [146/200] batch [30/32] time 0.408 (0.474) data 0.277 (0.343) loss_x loss_x 0.8066 (1.1383) acc_x 71.8750 (71.5625) lr 3.6258e-04 eta 0:00:00
epoch [146/200] batch [5/65] time 0.581 (0.479) data 0.447 (0.348) loss_u loss_u 0.8950 (0.8878) acc_u 12.5000 (13.1250) lr 3.6258e-04 eta 0:00:28
epoch [146/200] batch [10/65] time 0.479 (0.479) data 0.347 (0.348) loss_u loss_u 0.8916 (0.8819) acc_u 12.5000 (15.0000) lr 3.6258e-04 eta 0:00:26
epoch [146/200] batch [15/65] time 0.417 (0.482) data 0.286 (0.350) loss_u loss_u 0.8599 (0.8776) acc_u 12.5000 (14.7917) lr 3.6258e-04 eta 0:00:24
epoch [146/200] batch [20/65] time 0.580 (0.478) data 0.448 (0.347) loss_u loss_u 0.8823 (0.8751) acc_u 15.6250 (15.0000) lr 3.6258e-04 eta 0:00:21
epoch [146/200] batch [25/65] time 0.495 (0.478) data 0.364 (0.346) loss_u loss_u 0.9072 (0.8788) acc_u 12.5000 (14.6250) lr 3.6258e-04 eta 0:00:19
epoch [146/200] batch [30/65] time 0.538 (0.479) data 0.407 (0.348) loss_u loss_u 0.8555 (0.8820) acc_u 25.0000 (14.4792) lr 3.6258e-04 eta 0:00:16
epoch [146/200] batch [35/65] time 0.453 (0.483) data 0.322 (0.352) loss_u loss_u 0.8643 (0.8737) acc_u 21.8750 (15.4464) lr 3.6258e-04 eta 0:00:14
epoch [146/200] batch [40/65] time 0.518 (0.482) data 0.388 (0.351) loss_u loss_u 0.9204 (0.8704) acc_u 9.3750 (15.8594) lr 3.6258e-04 eta 0:00:12
epoch [146/200] batch [45/65] time 0.460 (0.480) data 0.329 (0.349) loss_u loss_u 0.8491 (0.8688) acc_u 15.6250 (16.2500) lr 3.6258e-04 eta 0:00:09
epoch [146/200] batch [50/65] time 0.376 (0.478) data 0.244 (0.347) loss_u loss_u 0.8384 (0.8712) acc_u 15.6250 (15.8750) lr 3.6258e-04 eta 0:00:07
epoch [146/200] batch [55/65] time 0.366 (0.476) data 0.236 (0.345) loss_u loss_u 0.8262 (0.8695) acc_u 18.7500 (16.0795) lr 3.6258e-04 eta 0:00:04
epoch [146/200] batch [60/65] time 0.434 (0.471) data 0.303 (0.340) loss_u loss_u 0.8335 (0.8707) acc_u 25.0000 (16.0417) lr 3.6258e-04 eta 0:00:02
epoch [146/200] batch [65/65] time 0.465 (0.471) data 0.333 (0.340) loss_u loss_u 0.8872 (0.8708) acc_u 9.3750 (15.8654) lr 3.6258e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1556
confident_label rate tensor(0.3377, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1059
clean true:904
clean false:155
clean_rate:0.8536355051935789
noisy true:676
noisy false:1401
after delete: len(clean_dataset) 1059
after delete: len(noisy_dataset) 2077
epoch [147/200] batch [5/33] time 0.384 (0.417) data 0.255 (0.286) loss_x loss_x 1.2402 (1.3043) acc_x 65.6250 (66.8750) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [10/33] time 0.498 (0.462) data 0.367 (0.332) loss_x loss_x 1.4443 (1.3103) acc_x 62.5000 (67.1875) lr 3.5055e-04 eta 0:00:10
epoch [147/200] batch [15/33] time 0.360 (0.463) data 0.230 (0.333) loss_x loss_x 1.5742 (1.2595) acc_x 65.6250 (68.5417) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [20/33] time 0.473 (0.475) data 0.343 (0.345) loss_x loss_x 1.0068 (1.2248) acc_x 71.8750 (69.6875) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [25/33] time 0.505 (0.478) data 0.373 (0.347) loss_x loss_x 0.5830 (1.1842) acc_x 87.5000 (71.1250) lr 3.5055e-04 eta 0:00:03
epoch [147/200] batch [30/33] time 0.463 (0.484) data 0.332 (0.353) loss_x loss_x 0.5508 (1.1729) acc_x 81.2500 (71.1458) lr 3.5055e-04 eta 0:00:01
epoch [147/200] batch [5/64] time 0.435 (0.479) data 0.305 (0.348) loss_u loss_u 0.8828 (0.8833) acc_u 15.6250 (13.1250) lr 3.5055e-04 eta 0:00:28
epoch [147/200] batch [10/64] time 0.364 (0.472) data 0.234 (0.342) loss_u loss_u 0.8643 (0.8805) acc_u 18.7500 (14.3750) lr 3.5055e-04 eta 0:00:25
epoch [147/200] batch [15/64] time 0.453 (0.466) data 0.322 (0.335) loss_u loss_u 0.9072 (0.8828) acc_u 9.3750 (13.5417) lr 3.5055e-04 eta 0:00:22
epoch [147/200] batch [20/64] time 0.353 (0.464) data 0.222 (0.333) loss_u loss_u 0.8691 (0.8793) acc_u 12.5000 (13.7500) lr 3.5055e-04 eta 0:00:20
epoch [147/200] batch [25/64] time 0.370 (0.467) data 0.239 (0.337) loss_u loss_u 0.8452 (0.8789) acc_u 18.7500 (14.0000) lr 3.5055e-04 eta 0:00:18
epoch [147/200] batch [30/64] time 0.555 (0.465) data 0.423 (0.334) loss_u loss_u 0.9277 (0.8854) acc_u 12.5000 (13.4375) lr 3.5055e-04 eta 0:00:15
epoch [147/200] batch [35/64] time 0.486 (0.464) data 0.353 (0.333) loss_u loss_u 0.8506 (0.8827) acc_u 21.8750 (14.1071) lr 3.5055e-04 eta 0:00:13
epoch [147/200] batch [40/64] time 0.433 (0.465) data 0.301 (0.334) loss_u loss_u 0.9131 (0.8817) acc_u 9.3750 (14.2969) lr 3.5055e-04 eta 0:00:11
epoch [147/200] batch [45/64] time 0.550 (0.464) data 0.418 (0.333) loss_u loss_u 0.8340 (0.8778) acc_u 21.8750 (14.7222) lr 3.5055e-04 eta 0:00:08
epoch [147/200] batch [50/64] time 0.583 (0.462) data 0.450 (0.331) loss_u loss_u 0.9077 (0.8758) acc_u 9.3750 (15.0625) lr 3.5055e-04 eta 0:00:06
epoch [147/200] batch [55/64] time 0.444 (0.462) data 0.313 (0.330) loss_u loss_u 0.9312 (0.8755) acc_u 6.2500 (15.0000) lr 3.5055e-04 eta 0:00:04
epoch [147/200] batch [60/64] time 0.458 (0.460) data 0.327 (0.329) loss_u loss_u 0.8594 (0.8747) acc_u 21.8750 (15.3125) lr 3.5055e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1613
confident_label rate tensor(0.3281, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1029
clean true:888
clean false:141
clean_rate:0.8629737609329446
noisy true:635
noisy false:1472
after delete: len(clean_dataset) 1029
after delete: len(noisy_dataset) 2107
epoch [148/200] batch [5/32] time 0.771 (0.559) data 0.640 (0.428) loss_x loss_x 0.8364 (0.9247) acc_x 81.2500 (79.3750) lr 3.3869e-04 eta 0:00:15
epoch [148/200] batch [10/32] time 0.459 (0.504) data 0.329 (0.373) loss_x loss_x 0.9805 (0.9300) acc_x 84.3750 (78.4375) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [15/32] time 0.450 (0.514) data 0.319 (0.384) loss_x loss_x 1.2334 (1.0025) acc_x 65.6250 (76.2500) lr 3.3869e-04 eta 0:00:08
epoch [148/200] batch [20/32] time 0.479 (0.498) data 0.349 (0.368) loss_x loss_x 0.9922 (0.9935) acc_x 75.0000 (77.1875) lr 3.3869e-04 eta 0:00:05
epoch [148/200] batch [25/32] time 0.466 (0.496) data 0.333 (0.365) loss_x loss_x 0.9277 (0.9937) acc_x 65.6250 (76.6250) lr 3.3869e-04 eta 0:00:03
epoch [148/200] batch [30/32] time 0.402 (0.497) data 0.272 (0.367) loss_x loss_x 1.1006 (1.0184) acc_x 75.0000 (75.7292) lr 3.3869e-04 eta 0:00:00
epoch [148/200] batch [5/65] time 0.310 (0.489) data 0.179 (0.358) loss_u loss_u 0.8584 (0.8893) acc_u 15.6250 (13.7500) lr 3.3869e-04 eta 0:00:29
epoch [148/200] batch [10/65] time 0.325 (0.483) data 0.195 (0.352) loss_u loss_u 0.9185 (0.8643) acc_u 9.3750 (16.8750) lr 3.3869e-04 eta 0:00:26
epoch [148/200] batch [15/65] time 0.521 (0.482) data 0.390 (0.351) loss_u loss_u 0.8921 (0.8741) acc_u 12.5000 (15.4167) lr 3.3869e-04 eta 0:00:24
epoch [148/200] batch [20/65] time 0.347 (0.476) data 0.216 (0.346) loss_u loss_u 0.8711 (0.8820) acc_u 21.8750 (14.6875) lr 3.3869e-04 eta 0:00:21
epoch [148/200] batch [25/65] time 0.407 (0.471) data 0.276 (0.340) loss_u loss_u 0.9497 (0.8800) acc_u 9.3750 (14.8750) lr 3.3869e-04 eta 0:00:18
epoch [148/200] batch [30/65] time 0.335 (0.470) data 0.205 (0.339) loss_u loss_u 0.9014 (0.8838) acc_u 15.6250 (14.5833) lr 3.3869e-04 eta 0:00:16
epoch [148/200] batch [35/65] time 0.345 (0.468) data 0.214 (0.337) loss_u loss_u 0.9546 (0.8837) acc_u 3.1250 (14.3750) lr 3.3869e-04 eta 0:00:14
epoch [148/200] batch [40/65] time 0.420 (0.464) data 0.289 (0.333) loss_u loss_u 0.9048 (0.8807) acc_u 9.3750 (14.7656) lr 3.3869e-04 eta 0:00:11
epoch [148/200] batch [45/65] time 0.390 (0.462) data 0.258 (0.331) loss_u loss_u 0.8589 (0.8774) acc_u 15.6250 (15.1389) lr 3.3869e-04 eta 0:00:09
epoch [148/200] batch [50/65] time 0.405 (0.463) data 0.274 (0.332) loss_u loss_u 0.9136 (0.8759) acc_u 12.5000 (15.4375) lr 3.3869e-04 eta 0:00:06
epoch [148/200] batch [55/65] time 0.408 (0.461) data 0.276 (0.330) loss_u loss_u 0.7949 (0.8713) acc_u 25.0000 (16.1932) lr 3.3869e-04 eta 0:00:04
epoch [148/200] batch [60/65] time 0.411 (0.463) data 0.280 (0.332) loss_u loss_u 0.8613 (0.8688) acc_u 18.7500 (16.4062) lr 3.3869e-04 eta 0:00:02
epoch [148/200] batch [65/65] time 0.438 (0.461) data 0.307 (0.330) loss_u loss_u 0.8286 (0.8699) acc_u 21.8750 (16.2500) lr 3.3869e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1600
confident_label rate tensor(0.3240, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1016
clean true:880
clean false:136
clean_rate:0.8661417322834646
noisy true:656
noisy false:1464
after delete: len(clean_dataset) 1016
after delete: len(noisy_dataset) 2120
epoch [149/200] batch [5/31] time 0.624 (0.509) data 0.494 (0.378) loss_x loss_x 1.4277 (1.2902) acc_x 62.5000 (70.6250) lr 3.2699e-04 eta 0:00:13
epoch [149/200] batch [10/31] time 0.413 (0.487) data 0.283 (0.357) loss_x loss_x 0.7847 (1.2042) acc_x 78.1250 (69.6875) lr 3.2699e-04 eta 0:00:10
epoch [149/200] batch [15/31] time 0.419 (0.486) data 0.288 (0.356) loss_x loss_x 1.3643 (1.2353) acc_x 71.8750 (69.7917) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [20/31] time 0.624 (0.502) data 0.493 (0.371) loss_x loss_x 0.9521 (1.2334) acc_x 75.0000 (69.5312) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [25/31] time 0.367 (0.484) data 0.236 (0.353) loss_x loss_x 1.7422 (1.2894) acc_x 65.6250 (68.2500) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [30/31] time 0.483 (0.479) data 0.353 (0.348) loss_x loss_x 1.0391 (1.2814) acc_x 75.0000 (69.0625) lr 3.2699e-04 eta 0:00:00
epoch [149/200] batch [5/66] time 0.523 (0.475) data 0.393 (0.344) loss_u loss_u 0.8345 (0.8590) acc_u 21.8750 (15.6250) lr 3.2699e-04 eta 0:00:28
epoch [149/200] batch [10/66] time 0.460 (0.475) data 0.329 (0.345) loss_u loss_u 0.9023 (0.8599) acc_u 9.3750 (15.6250) lr 3.2699e-04 eta 0:00:26
epoch [149/200] batch [15/66] time 0.458 (0.479) data 0.326 (0.348) loss_u loss_u 0.9331 (0.8689) acc_u 6.2500 (15.0000) lr 3.2699e-04 eta 0:00:24
epoch [149/200] batch [20/66] time 0.782 (0.479) data 0.652 (0.348) loss_u loss_u 0.8872 (0.8721) acc_u 15.6250 (15.1562) lr 3.2699e-04 eta 0:00:22
epoch [149/200] batch [25/66] time 0.515 (0.478) data 0.382 (0.347) loss_u loss_u 0.9126 (0.8749) acc_u 9.3750 (15.0000) lr 3.2699e-04 eta 0:00:19
epoch [149/200] batch [30/66] time 0.507 (0.478) data 0.375 (0.347) loss_u loss_u 0.8550 (0.8713) acc_u 18.7500 (15.7292) lr 3.2699e-04 eta 0:00:17
epoch [149/200] batch [35/66] time 0.538 (0.477) data 0.406 (0.346) loss_u loss_u 0.8721 (0.8712) acc_u 12.5000 (15.5357) lr 3.2699e-04 eta 0:00:14
epoch [149/200] batch [40/66] time 0.419 (0.476) data 0.288 (0.345) loss_u loss_u 0.9414 (0.8690) acc_u 9.3750 (16.0156) lr 3.2699e-04 eta 0:00:12
epoch [149/200] batch [45/66] time 0.347 (0.473) data 0.216 (0.342) loss_u loss_u 0.8413 (0.8682) acc_u 21.8750 (16.2500) lr 3.2699e-04 eta 0:00:09
epoch [149/200] batch [50/66] time 0.390 (0.472) data 0.259 (0.341) loss_u loss_u 0.7832 (0.8669) acc_u 25.0000 (16.5625) lr 3.2699e-04 eta 0:00:07
epoch [149/200] batch [55/66] time 0.463 (0.473) data 0.332 (0.341) loss_u loss_u 0.8779 (0.8671) acc_u 15.6250 (16.3636) lr 3.2699e-04 eta 0:00:05
epoch [149/200] batch [60/66] time 0.444 (0.470) data 0.313 (0.339) loss_u loss_u 0.8540 (0.8661) acc_u 21.8750 (16.5104) lr 3.2699e-04 eta 0:00:02
epoch [149/200] batch [65/66] time 0.349 (0.467) data 0.217 (0.336) loss_u loss_u 0.8921 (0.8685) acc_u 15.6250 (16.4423) lr 3.2699e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1579
confident_label rate tensor(0.3345, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1049
clean true:908
clean false:141
clean_rate:0.86558627264061
noisy true:649
noisy false:1438
after delete: len(clean_dataset) 1049
after delete: len(noisy_dataset) 2087
epoch [150/200] batch [5/32] time 0.389 (0.494) data 0.259 (0.363) loss_x loss_x 1.7422 (1.2935) acc_x 65.6250 (71.8750) lr 3.1545e-04 eta 0:00:13
epoch [150/200] batch [10/32] time 0.570 (0.479) data 0.439 (0.348) loss_x loss_x 0.6494 (1.1332) acc_x 87.5000 (74.6875) lr 3.1545e-04 eta 0:00:10
epoch [150/200] batch [15/32] time 0.527 (0.481) data 0.396 (0.350) loss_x loss_x 1.0039 (1.0902) acc_x 75.0000 (74.7917) lr 3.1545e-04 eta 0:00:08
epoch [150/200] batch [20/32] time 0.392 (0.479) data 0.261 (0.348) loss_x loss_x 1.2939 (1.1403) acc_x 68.7500 (73.2812) lr 3.1545e-04 eta 0:00:05
epoch [150/200] batch [25/32] time 0.398 (0.470) data 0.267 (0.339) loss_x loss_x 1.7100 (1.1621) acc_x 56.2500 (72.2500) lr 3.1545e-04 eta 0:00:03
epoch [150/200] batch [30/32] time 0.536 (0.469) data 0.406 (0.338) loss_x loss_x 0.6509 (1.1431) acc_x 81.2500 (72.2917) lr 3.1545e-04 eta 0:00:00
epoch [150/200] batch [5/65] time 0.379 (0.458) data 0.248 (0.327) loss_u loss_u 0.9321 (0.8792) acc_u 6.2500 (15.0000) lr 3.1545e-04 eta 0:00:27
epoch [150/200] batch [10/65] time 0.440 (0.461) data 0.308 (0.330) loss_u loss_u 0.8530 (0.8893) acc_u 25.0000 (14.6875) lr 3.1545e-04 eta 0:00:25
epoch [150/200] batch [15/65] time 0.398 (0.451) data 0.267 (0.320) loss_u loss_u 0.8174 (0.8928) acc_u 21.8750 (13.9583) lr 3.1545e-04 eta 0:00:22
epoch [150/200] batch [20/65] time 0.385 (0.454) data 0.254 (0.323) loss_u loss_u 0.8599 (0.8900) acc_u 15.6250 (14.0625) lr 3.1545e-04 eta 0:00:20
epoch [150/200] batch [25/65] time 0.392 (0.456) data 0.262 (0.325) loss_u loss_u 0.8916 (0.8885) acc_u 12.5000 (14.2500) lr 3.1545e-04 eta 0:00:18
epoch [150/200] batch [30/65] time 0.388 (0.455) data 0.257 (0.323) loss_u loss_u 0.9277 (0.8903) acc_u 6.2500 (13.9583) lr 3.1545e-04 eta 0:00:15
epoch [150/200] batch [35/65] time 0.415 (0.456) data 0.285 (0.325) loss_u loss_u 0.8848 (0.8835) acc_u 12.5000 (14.9107) lr 3.1545e-04 eta 0:00:13
epoch [150/200] batch [40/65] time 0.380 (0.457) data 0.249 (0.326) loss_u loss_u 0.8252 (0.8822) acc_u 18.7500 (15.0781) lr 3.1545e-04 eta 0:00:11
epoch [150/200] batch [45/65] time 0.396 (0.458) data 0.265 (0.327) loss_u loss_u 0.9263 (0.8834) acc_u 9.3750 (14.9306) lr 3.1545e-04 eta 0:00:09
epoch [150/200] batch [50/65] time 0.627 (0.457) data 0.495 (0.326) loss_u loss_u 0.8091 (0.8833) acc_u 18.7500 (14.6875) lr 3.1545e-04 eta 0:00:06
epoch [150/200] batch [55/65] time 0.450 (0.459) data 0.319 (0.328) loss_u loss_u 0.8413 (0.8792) acc_u 21.8750 (15.0000) lr 3.1545e-04 eta 0:00:04
epoch [150/200] batch [60/65] time 0.464 (0.458) data 0.333 (0.327) loss_u loss_u 0.8950 (0.8764) acc_u 12.5000 (15.1562) lr 3.1545e-04 eta 0:00:02
epoch [150/200] batch [65/65] time 0.482 (0.460) data 0.349 (0.328) loss_u loss_u 0.9033 (0.8761) acc_u 15.6250 (15.2885) lr 3.1545e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1583
confident_label rate tensor(0.3297, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1034
clean true:902
clean false:132
clean_rate:0.8723404255319149
noisy true:651
noisy false:1451
after delete: len(clean_dataset) 1034
after delete: len(noisy_dataset) 2102
epoch [151/200] batch [5/32] time 0.531 (0.439) data 0.401 (0.308) loss_x loss_x 0.7700 (1.0281) acc_x 71.8750 (71.8750) lr 3.0409e-04 eta 0:00:11
epoch [151/200] batch [10/32] time 0.500 (0.457) data 0.369 (0.326) loss_x loss_x 1.5029 (0.9915) acc_x 59.3750 (73.7500) lr 3.0409e-04 eta 0:00:10
epoch [151/200] batch [15/32] time 0.518 (0.474) data 0.386 (0.344) loss_x loss_x 1.0195 (1.0352) acc_x 65.6250 (72.2917) lr 3.0409e-04 eta 0:00:08
epoch [151/200] batch [20/32] time 0.446 (0.482) data 0.315 (0.352) loss_x loss_x 0.8618 (1.0547) acc_x 81.2500 (72.0312) lr 3.0409e-04 eta 0:00:05
epoch [151/200] batch [25/32] time 0.557 (0.486) data 0.427 (0.356) loss_x loss_x 1.5039 (1.0926) acc_x 62.5000 (72.3750) lr 3.0409e-04 eta 0:00:03
epoch [151/200] batch [30/32] time 0.628 (0.493) data 0.497 (0.363) loss_x loss_x 1.0020 (1.1056) acc_x 81.2500 (71.6667) lr 3.0409e-04 eta 0:00:00
epoch [151/200] batch [5/65] time 0.361 (0.480) data 0.231 (0.349) loss_u loss_u 0.8892 (0.8732) acc_u 12.5000 (19.3750) lr 3.0409e-04 eta 0:00:28
epoch [151/200] batch [10/65] time 0.381 (0.473) data 0.249 (0.342) loss_u loss_u 0.8472 (0.8801) acc_u 18.7500 (16.8750) lr 3.0409e-04 eta 0:00:26
epoch [151/200] batch [15/65] time 0.508 (0.480) data 0.377 (0.349) loss_u loss_u 0.9492 (0.8821) acc_u 6.2500 (15.8333) lr 3.0409e-04 eta 0:00:23
epoch [151/200] batch [20/65] time 0.446 (0.475) data 0.316 (0.345) loss_u loss_u 0.8472 (0.8760) acc_u 15.6250 (15.9375) lr 3.0409e-04 eta 0:00:21
epoch [151/200] batch [25/65] time 0.449 (0.476) data 0.314 (0.345) loss_u loss_u 0.7910 (0.8645) acc_u 21.8750 (16.8750) lr 3.0409e-04 eta 0:00:19
epoch [151/200] batch [30/65] time 0.379 (0.473) data 0.248 (0.342) loss_u loss_u 0.9102 (0.8628) acc_u 6.2500 (16.4583) lr 3.0409e-04 eta 0:00:16
epoch [151/200] batch [35/65] time 0.630 (0.472) data 0.500 (0.341) loss_u loss_u 0.9199 (0.8653) acc_u 6.2500 (16.0714) lr 3.0409e-04 eta 0:00:14
epoch [151/200] batch [40/65] time 0.632 (0.473) data 0.501 (0.342) loss_u loss_u 0.8916 (0.8652) acc_u 9.3750 (15.9375) lr 3.0409e-04 eta 0:00:11
epoch [151/200] batch [45/65] time 0.393 (0.472) data 0.260 (0.341) loss_u loss_u 0.9194 (0.8675) acc_u 9.3750 (15.5556) lr 3.0409e-04 eta 0:00:09
epoch [151/200] batch [50/65] time 0.480 (0.471) data 0.349 (0.340) loss_u loss_u 0.8354 (0.8672) acc_u 18.7500 (15.6250) lr 3.0409e-04 eta 0:00:07
epoch [151/200] batch [55/65] time 0.447 (0.471) data 0.316 (0.340) loss_u loss_u 0.8335 (0.8678) acc_u 25.0000 (15.5114) lr 3.0409e-04 eta 0:00:04
epoch [151/200] batch [60/65] time 0.557 (0.472) data 0.425 (0.341) loss_u loss_u 0.9102 (0.8674) acc_u 12.5000 (15.7812) lr 3.0409e-04 eta 0:00:02
epoch [151/200] batch [65/65] time 0.524 (0.471) data 0.391 (0.340) loss_u loss_u 0.9331 (0.8694) acc_u 6.2500 (15.3846) lr 3.0409e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1597
confident_label rate tensor(0.3313, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1039
clean true:898
clean false:141
clean_rate:0.8642925890279115
noisy true:641
noisy false:1456
after delete: len(clean_dataset) 1039
after delete: len(noisy_dataset) 2097
epoch [152/200] batch [5/32] time 0.561 (0.489) data 0.430 (0.358) loss_x loss_x 1.7021 (1.2302) acc_x 53.1250 (65.6250) lr 2.9289e-04 eta 0:00:13
epoch [152/200] batch [10/32] time 0.522 (0.459) data 0.390 (0.329) loss_x loss_x 1.2090 (1.1669) acc_x 65.6250 (68.7500) lr 2.9289e-04 eta 0:00:10
epoch [152/200] batch [15/32] time 0.495 (0.472) data 0.364 (0.341) loss_x loss_x 0.9751 (1.1659) acc_x 62.5000 (68.3333) lr 2.9289e-04 eta 0:00:08
epoch [152/200] batch [20/32] time 0.517 (0.470) data 0.386 (0.339) loss_x loss_x 0.8315 (1.1349) acc_x 71.8750 (69.3750) lr 2.9289e-04 eta 0:00:05
epoch [152/200] batch [25/32] time 0.514 (0.479) data 0.383 (0.348) loss_x loss_x 1.3301 (1.1863) acc_x 65.6250 (68.3750) lr 2.9289e-04 eta 0:00:03
epoch [152/200] batch [30/32] time 0.418 (0.481) data 0.288 (0.350) loss_x loss_x 0.6001 (1.1526) acc_x 84.3750 (70.1042) lr 2.9289e-04 eta 0:00:00
epoch [152/200] batch [5/65] time 0.539 (0.480) data 0.409 (0.349) loss_u loss_u 0.7817 (0.8447) acc_u 34.3750 (20.0000) lr 2.9289e-04 eta 0:00:28
epoch [152/200] batch [10/65] time 0.463 (0.476) data 0.333 (0.346) loss_u loss_u 0.8721 (0.8670) acc_u 15.6250 (16.2500) lr 2.9289e-04 eta 0:00:26
epoch [152/200] batch [15/65] time 0.377 (0.478) data 0.246 (0.347) loss_u loss_u 0.8896 (0.8795) acc_u 15.6250 (15.4167) lr 2.9289e-04 eta 0:00:23
epoch [152/200] batch [20/65] time 0.438 (0.484) data 0.308 (0.353) loss_u loss_u 0.9556 (0.8856) acc_u 0.0000 (14.6875) lr 2.9289e-04 eta 0:00:21
epoch [152/200] batch [25/65] time 0.466 (0.486) data 0.335 (0.355) loss_u loss_u 0.8306 (0.8805) acc_u 18.7500 (15.0000) lr 2.9289e-04 eta 0:00:19
epoch [152/200] batch [30/65] time 0.465 (0.486) data 0.332 (0.355) loss_u loss_u 0.8481 (0.8714) acc_u 18.7500 (16.0417) lr 2.9289e-04 eta 0:00:17
epoch [152/200] batch [35/65] time 0.438 (0.480) data 0.308 (0.349) loss_u loss_u 0.8423 (0.8736) acc_u 21.8750 (15.5357) lr 2.9289e-04 eta 0:00:14
epoch [152/200] batch [40/65] time 0.572 (0.478) data 0.440 (0.347) loss_u loss_u 0.8882 (0.8738) acc_u 15.6250 (15.3906) lr 2.9289e-04 eta 0:00:11
epoch [152/200] batch [45/65] time 0.407 (0.474) data 0.274 (0.344) loss_u loss_u 0.7485 (0.8724) acc_u 28.1250 (15.4167) lr 2.9289e-04 eta 0:00:09
epoch [152/200] batch [50/65] time 0.518 (0.474) data 0.387 (0.343) loss_u loss_u 0.8638 (0.8741) acc_u 15.6250 (15.0000) lr 2.9289e-04 eta 0:00:07
epoch [152/200] batch [55/65] time 0.393 (0.472) data 0.263 (0.341) loss_u loss_u 0.7744 (0.8718) acc_u 25.0000 (15.2273) lr 2.9289e-04 eta 0:00:04
epoch [152/200] batch [60/65] time 0.490 (0.471) data 0.358 (0.340) loss_u loss_u 0.8936 (0.8726) acc_u 12.5000 (15.1042) lr 2.9289e-04 eta 0:00:02
epoch [152/200] batch [65/65] time 0.491 (0.470) data 0.359 (0.339) loss_u loss_u 0.9116 (0.8736) acc_u 15.6250 (15.1923) lr 2.9289e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1551
confident_label rate tensor(0.3329, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1044
clean true:913
clean false:131
clean_rate:0.8745210727969349
noisy true:672
noisy false:1420
after delete: len(clean_dataset) 1044
after delete: len(noisy_dataset) 2092
epoch [153/200] batch [5/32] time 0.458 (0.452) data 0.326 (0.322) loss_x loss_x 0.7344 (1.0428) acc_x 87.5000 (74.3750) lr 2.8187e-04 eta 0:00:12
epoch [153/200] batch [10/32] time 0.569 (0.475) data 0.439 (0.344) loss_x loss_x 1.6670 (1.0620) acc_x 62.5000 (74.6875) lr 2.8187e-04 eta 0:00:10
epoch [153/200] batch [15/32] time 0.435 (0.478) data 0.304 (0.348) loss_x loss_x 1.1543 (1.0710) acc_x 62.5000 (73.1250) lr 2.8187e-04 eta 0:00:08
epoch [153/200] batch [20/32] time 0.615 (0.481) data 0.485 (0.351) loss_x loss_x 1.0742 (1.1320) acc_x 78.1250 (72.6562) lr 2.8187e-04 eta 0:00:05
epoch [153/200] batch [25/32] time 0.459 (0.479) data 0.328 (0.348) loss_x loss_x 0.7593 (1.1215) acc_x 81.2500 (72.7500) lr 2.8187e-04 eta 0:00:03
epoch [153/200] batch [30/32] time 0.428 (0.477) data 0.298 (0.347) loss_x loss_x 1.0576 (1.1430) acc_x 68.7500 (72.1875) lr 2.8187e-04 eta 0:00:00
epoch [153/200] batch [5/65] time 0.416 (0.473) data 0.285 (0.343) loss_u loss_u 0.9023 (0.8784) acc_u 12.5000 (15.0000) lr 2.8187e-04 eta 0:00:28
epoch [153/200] batch [10/65] time 0.518 (0.476) data 0.388 (0.346) loss_u loss_u 0.8281 (0.8612) acc_u 21.8750 (15.9375) lr 2.8187e-04 eta 0:00:26
epoch [153/200] batch [15/65] time 0.411 (0.473) data 0.280 (0.343) loss_u loss_u 0.9033 (0.8606) acc_u 9.3750 (16.2500) lr 2.8187e-04 eta 0:00:23
epoch [153/200] batch [20/65] time 0.474 (0.470) data 0.343 (0.339) loss_u loss_u 0.9067 (0.8684) acc_u 12.5000 (16.2500) lr 2.8187e-04 eta 0:00:21
epoch [153/200] batch [25/65] time 0.449 (0.467) data 0.318 (0.336) loss_u loss_u 0.8325 (0.8614) acc_u 21.8750 (17.1250) lr 2.8187e-04 eta 0:00:18
epoch [153/200] batch [30/65] time 0.416 (0.465) data 0.285 (0.334) loss_u loss_u 0.9482 (0.8626) acc_u 6.2500 (16.9792) lr 2.8187e-04 eta 0:00:16
epoch [153/200] batch [35/65] time 0.389 (0.464) data 0.258 (0.333) loss_u loss_u 0.8901 (0.8628) acc_u 12.5000 (17.0536) lr 2.8187e-04 eta 0:00:13
epoch [153/200] batch [40/65] time 0.418 (0.459) data 0.286 (0.329) loss_u loss_u 0.8369 (0.8631) acc_u 21.8750 (16.9531) lr 2.8187e-04 eta 0:00:11
epoch [153/200] batch [45/65] time 0.491 (0.465) data 0.360 (0.334) loss_u loss_u 0.9009 (0.8664) acc_u 9.3750 (16.5278) lr 2.8187e-04 eta 0:00:09
epoch [153/200] batch [50/65] time 0.475 (0.463) data 0.344 (0.332) loss_u loss_u 0.8496 (0.8686) acc_u 12.5000 (16.0625) lr 2.8187e-04 eta 0:00:06
epoch [153/200] batch [55/65] time 0.403 (0.463) data 0.273 (0.332) loss_u loss_u 0.8604 (0.8684) acc_u 21.8750 (16.0227) lr 2.8187e-04 eta 0:00:04
epoch [153/200] batch [60/65] time 0.361 (0.462) data 0.230 (0.331) loss_u loss_u 0.9106 (0.8701) acc_u 12.5000 (15.9375) lr 2.8187e-04 eta 0:00:02
epoch [153/200] batch [65/65] time 0.381 (0.463) data 0.250 (0.332) loss_u loss_u 0.8145 (0.8721) acc_u 28.1250 (15.8173) lr 2.8187e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1542
confident_label rate tensor(0.3291, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1032
clean true:897
clean false:135
clean_rate:0.8691860465116279
noisy true:697
noisy false:1407
after delete: len(clean_dataset) 1032
after delete: len(noisy_dataset) 2104
epoch [154/200] batch [5/32] time 0.669 (0.530) data 0.538 (0.399) loss_x loss_x 0.8325 (1.1315) acc_x 87.5000 (75.6250) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [10/32] time 0.576 (0.521) data 0.445 (0.391) loss_x loss_x 0.9019 (1.1669) acc_x 78.1250 (74.0625) lr 2.7103e-04 eta 0:00:11
epoch [154/200] batch [15/32] time 0.463 (0.515) data 0.333 (0.385) loss_x loss_x 1.1885 (1.1733) acc_x 71.8750 (72.7083) lr 2.7103e-04 eta 0:00:08
epoch [154/200] batch [20/32] time 0.394 (0.512) data 0.264 (0.381) loss_x loss_x 1.4619 (1.2011) acc_x 59.3750 (70.6250) lr 2.7103e-04 eta 0:00:06
epoch [154/200] batch [25/32] time 0.483 (0.509) data 0.353 (0.378) loss_x loss_x 1.0332 (1.2099) acc_x 71.8750 (70.1250) lr 2.7103e-04 eta 0:00:03
epoch [154/200] batch [30/32] time 0.477 (0.507) data 0.346 (0.377) loss_x loss_x 1.6787 (1.1926) acc_x 65.6250 (70.9375) lr 2.7103e-04 eta 0:00:01
epoch [154/200] batch [5/65] time 0.570 (0.505) data 0.439 (0.374) loss_u loss_u 0.9160 (0.8640) acc_u 12.5000 (18.1250) lr 2.7103e-04 eta 0:00:30
epoch [154/200] batch [10/65] time 0.371 (0.496) data 0.240 (0.365) loss_u loss_u 0.8804 (0.8793) acc_u 12.5000 (15.9375) lr 2.7103e-04 eta 0:00:27
epoch [154/200] batch [15/65] time 0.469 (0.496) data 0.337 (0.366) loss_u loss_u 0.8921 (0.8788) acc_u 12.5000 (15.8333) lr 2.7103e-04 eta 0:00:24
epoch [154/200] batch [20/65] time 0.426 (0.493) data 0.296 (0.362) loss_u loss_u 0.9229 (0.8817) acc_u 6.2500 (15.6250) lr 2.7103e-04 eta 0:00:22
epoch [154/200] batch [25/65] time 0.436 (0.486) data 0.305 (0.355) loss_u loss_u 0.9180 (0.8731) acc_u 12.5000 (16.2500) lr 2.7103e-04 eta 0:00:19
epoch [154/200] batch [30/65] time 0.395 (0.485) data 0.264 (0.354) loss_u loss_u 0.9058 (0.8766) acc_u 9.3750 (15.4167) lr 2.7103e-04 eta 0:00:16
epoch [154/200] batch [35/65] time 0.457 (0.482) data 0.325 (0.351) loss_u loss_u 0.7969 (0.8741) acc_u 28.1250 (15.6250) lr 2.7103e-04 eta 0:00:14
epoch [154/200] batch [40/65] time 0.394 (0.481) data 0.262 (0.350) loss_u loss_u 0.8853 (0.8714) acc_u 15.6250 (15.9375) lr 2.7103e-04 eta 0:00:12
epoch [154/200] batch [45/65] time 0.516 (0.478) data 0.385 (0.347) loss_u loss_u 0.9243 (0.8725) acc_u 9.3750 (16.0417) lr 2.7103e-04 eta 0:00:09
epoch [154/200] batch [50/65] time 0.490 (0.475) data 0.358 (0.344) loss_u loss_u 0.8779 (0.8718) acc_u 15.6250 (16.1875) lr 2.7103e-04 eta 0:00:07
epoch [154/200] batch [55/65] time 0.408 (0.474) data 0.277 (0.343) loss_u loss_u 0.9072 (0.8740) acc_u 12.5000 (15.9091) lr 2.7103e-04 eta 0:00:04
epoch [154/200] batch [60/65] time 0.545 (0.474) data 0.413 (0.343) loss_u loss_u 0.8735 (0.8751) acc_u 12.5000 (15.7292) lr 2.7103e-04 eta 0:00:02
epoch [154/200] batch [65/65] time 0.425 (0.471) data 0.293 (0.340) loss_u loss_u 0.9155 (0.8747) acc_u 9.3750 (15.7212) lr 2.7103e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1569
confident_label rate tensor(0.3355, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1052
clean true:905
clean false:147
clean_rate:0.8602661596958175
noisy true:662
noisy false:1422
after delete: len(clean_dataset) 1052
after delete: len(noisy_dataset) 2084
epoch [155/200] batch [5/32] time 0.417 (0.444) data 0.287 (0.313) loss_x loss_x 0.9814 (1.0397) acc_x 78.1250 (75.0000) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [10/32] time 0.728 (0.496) data 0.596 (0.366) loss_x loss_x 0.9487 (1.0695) acc_x 75.0000 (70.6250) lr 2.6037e-04 eta 0:00:10
epoch [155/200] batch [15/32] time 0.408 (0.486) data 0.278 (0.355) loss_x loss_x 1.2998 (1.1061) acc_x 62.5000 (71.6667) lr 2.6037e-04 eta 0:00:08
epoch [155/200] batch [20/32] time 0.549 (0.488) data 0.418 (0.357) loss_x loss_x 0.9829 (1.0865) acc_x 71.8750 (72.5000) lr 2.6037e-04 eta 0:00:05
epoch [155/200] batch [25/32] time 0.432 (0.485) data 0.302 (0.355) loss_x loss_x 0.6226 (1.0958) acc_x 84.3750 (71.7500) lr 2.6037e-04 eta 0:00:03
epoch [155/200] batch [30/32] time 0.496 (0.483) data 0.365 (0.353) loss_x loss_x 1.4551 (1.1340) acc_x 62.5000 (70.1042) lr 2.6037e-04 eta 0:00:00
epoch [155/200] batch [5/65] time 0.613 (0.482) data 0.481 (0.352) loss_u loss_u 0.8667 (0.8774) acc_u 18.7500 (15.0000) lr 2.6037e-04 eta 0:00:28
epoch [155/200] batch [10/65] time 0.423 (0.477) data 0.293 (0.346) loss_u loss_u 0.8823 (0.8696) acc_u 15.6250 (17.5000) lr 2.6037e-04 eta 0:00:26
epoch [155/200] batch [15/65] time 0.456 (0.476) data 0.326 (0.345) loss_u loss_u 0.8511 (0.8716) acc_u 18.7500 (17.0833) lr 2.6037e-04 eta 0:00:23
epoch [155/200] batch [20/65] time 0.461 (0.471) data 0.331 (0.340) loss_u loss_u 0.8408 (0.8701) acc_u 18.7500 (17.0312) lr 2.6037e-04 eta 0:00:21
epoch [155/200] batch [25/65] time 0.427 (0.469) data 0.296 (0.338) loss_u loss_u 0.7690 (0.8682) acc_u 28.1250 (16.6250) lr 2.6037e-04 eta 0:00:18
epoch [155/200] batch [30/65] time 0.649 (0.472) data 0.518 (0.341) loss_u loss_u 0.9238 (0.8729) acc_u 9.3750 (15.6250) lr 2.6037e-04 eta 0:00:16
epoch [155/200] batch [35/65] time 0.461 (0.471) data 0.328 (0.340) loss_u loss_u 0.8477 (0.8709) acc_u 18.7500 (15.8036) lr 2.6037e-04 eta 0:00:14
epoch [155/200] batch [40/65] time 0.613 (0.473) data 0.481 (0.343) loss_u loss_u 0.8882 (0.8670) acc_u 18.7500 (16.4062) lr 2.6037e-04 eta 0:00:11
epoch [155/200] batch [45/65] time 0.485 (0.473) data 0.353 (0.343) loss_u loss_u 0.9106 (0.8686) acc_u 9.3750 (16.1111) lr 2.6037e-04 eta 0:00:09
epoch [155/200] batch [50/65] time 0.462 (0.470) data 0.331 (0.339) loss_u loss_u 0.9072 (0.8701) acc_u 9.3750 (15.9375) lr 2.6037e-04 eta 0:00:07
epoch [155/200] batch [55/65] time 0.361 (0.467) data 0.230 (0.336) loss_u loss_u 0.9097 (0.8697) acc_u 12.5000 (15.9091) lr 2.6037e-04 eta 0:00:04
epoch [155/200] batch [60/65] time 0.324 (0.464) data 0.192 (0.333) loss_u loss_u 0.8955 (0.8707) acc_u 12.5000 (15.6250) lr 2.6037e-04 eta 0:00:02
epoch [155/200] batch [65/65] time 0.450 (0.463) data 0.317 (0.332) loss_u loss_u 0.9526 (0.8728) acc_u 6.2500 (15.4327) lr 2.6037e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1588
confident_label rate tensor(0.3316, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1040
clean true:899
clean false:141
clean_rate:0.864423076923077
noisy true:649
noisy false:1447
after delete: len(clean_dataset) 1040
after delete: len(noisy_dataset) 2096
epoch [156/200] batch [5/32] time 0.539 (0.529) data 0.408 (0.398) loss_x loss_x 2.3516 (1.5574) acc_x 46.8750 (63.7500) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [10/32] time 0.436 (0.538) data 0.306 (0.407) loss_x loss_x 1.2510 (1.3074) acc_x 68.7500 (67.5000) lr 2.4989e-04 eta 0:00:11
epoch [156/200] batch [15/32] time 0.433 (0.500) data 0.304 (0.369) loss_x loss_x 0.7061 (1.2621) acc_x 87.5000 (68.3333) lr 2.4989e-04 eta 0:00:08
epoch [156/200] batch [20/32] time 0.530 (0.494) data 0.399 (0.363) loss_x loss_x 1.2900 (1.2322) acc_x 65.6250 (69.0625) lr 2.4989e-04 eta 0:00:05
epoch [156/200] batch [25/32] time 0.328 (0.482) data 0.197 (0.352) loss_x loss_x 1.0742 (1.1813) acc_x 75.0000 (70.6250) lr 2.4989e-04 eta 0:00:03
epoch [156/200] batch [30/32] time 0.473 (0.477) data 0.342 (0.347) loss_x loss_x 1.4746 (1.1649) acc_x 68.7500 (70.8333) lr 2.4989e-04 eta 0:00:00
epoch [156/200] batch [5/65] time 0.422 (0.472) data 0.291 (0.342) loss_u loss_u 0.9360 (0.8829) acc_u 6.2500 (12.5000) lr 2.4989e-04 eta 0:00:28
epoch [156/200] batch [10/65] time 0.470 (0.464) data 0.338 (0.334) loss_u loss_u 0.8853 (0.8759) acc_u 12.5000 (14.0625) lr 2.4989e-04 eta 0:00:25
epoch [156/200] batch [15/65] time 0.415 (0.467) data 0.284 (0.337) loss_u loss_u 0.8979 (0.8715) acc_u 12.5000 (14.7917) lr 2.4989e-04 eta 0:00:23
epoch [156/200] batch [20/65] time 0.496 (0.466) data 0.365 (0.335) loss_u loss_u 0.9414 (0.8712) acc_u 6.2500 (15.6250) lr 2.4989e-04 eta 0:00:20
epoch [156/200] batch [25/65] time 0.410 (0.470) data 0.279 (0.339) loss_u loss_u 0.8159 (0.8717) acc_u 28.1250 (15.7500) lr 2.4989e-04 eta 0:00:18
epoch [156/200] batch [30/65] time 0.529 (0.468) data 0.398 (0.337) loss_u loss_u 0.8696 (0.8702) acc_u 15.6250 (15.9375) lr 2.4989e-04 eta 0:00:16
epoch [156/200] batch [35/65] time 0.371 (0.469) data 0.240 (0.338) loss_u loss_u 0.8501 (0.8668) acc_u 21.8750 (16.7857) lr 2.4989e-04 eta 0:00:14
epoch [156/200] batch [40/65] time 0.459 (0.466) data 0.329 (0.335) loss_u loss_u 0.8369 (0.8664) acc_u 15.6250 (16.7188) lr 2.4989e-04 eta 0:00:11
epoch [156/200] batch [45/65] time 0.345 (0.463) data 0.215 (0.332) loss_u loss_u 0.8491 (0.8632) acc_u 15.6250 (17.0139) lr 2.4989e-04 eta 0:00:09
epoch [156/200] batch [50/65] time 0.444 (0.462) data 0.312 (0.331) loss_u loss_u 0.8926 (0.8650) acc_u 9.3750 (16.4375) lr 2.4989e-04 eta 0:00:06
epoch [156/200] batch [55/65] time 0.426 (0.465) data 0.295 (0.335) loss_u loss_u 0.8467 (0.8671) acc_u 15.6250 (16.1364) lr 2.4989e-04 eta 0:00:04
epoch [156/200] batch [60/65] time 0.482 (0.464) data 0.350 (0.333) loss_u loss_u 0.9082 (0.8656) acc_u 9.3750 (16.3542) lr 2.4989e-04 eta 0:00:02
epoch [156/200] batch [65/65] time 0.398 (0.461) data 0.266 (0.330) loss_u loss_u 0.8818 (0.8672) acc_u 12.5000 (16.1538) lr 2.4989e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1554
confident_label rate tensor(0.3393, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1064
clean true:918
clean false:146
clean_rate:0.8627819548872181
noisy true:664
noisy false:1408
after delete: len(clean_dataset) 1064
after delete: len(noisy_dataset) 2072
epoch [157/200] batch [5/33] time 0.651 (0.547) data 0.520 (0.416) loss_x loss_x 1.5566 (1.1146) acc_x 59.3750 (71.2500) lr 2.3959e-04 eta 0:00:15
epoch [157/200] batch [10/33] time 0.425 (0.516) data 0.295 (0.385) loss_x loss_x 1.6182 (1.1646) acc_x 65.6250 (69.6875) lr 2.3959e-04 eta 0:00:11
epoch [157/200] batch [15/33] time 0.420 (0.484) data 0.289 (0.353) loss_x loss_x 0.9194 (1.1758) acc_x 84.3750 (70.4167) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [20/33] time 0.503 (0.491) data 0.372 (0.360) loss_x loss_x 1.1953 (1.2060) acc_x 71.8750 (70.4688) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [25/33] time 0.513 (0.493) data 0.383 (0.362) loss_x loss_x 1.1436 (1.2149) acc_x 78.1250 (70.5000) lr 2.3959e-04 eta 0:00:03
epoch [157/200] batch [30/33] time 0.386 (0.490) data 0.255 (0.360) loss_x loss_x 0.6128 (1.1678) acc_x 81.2500 (71.6667) lr 2.3959e-04 eta 0:00:01
epoch [157/200] batch [5/64] time 0.410 (0.479) data 0.280 (0.348) loss_u loss_u 0.8408 (0.8675) acc_u 18.7500 (16.2500) lr 2.3959e-04 eta 0:00:28
epoch [157/200] batch [10/64] time 0.318 (0.469) data 0.186 (0.338) loss_u loss_u 0.9082 (0.8776) acc_u 9.3750 (14.0625) lr 2.3959e-04 eta 0:00:25
epoch [157/200] batch [15/64] time 0.463 (0.468) data 0.333 (0.338) loss_u loss_u 0.7793 (0.8713) acc_u 34.3750 (14.7917) lr 2.3959e-04 eta 0:00:22
epoch [157/200] batch [20/64] time 0.373 (0.474) data 0.242 (0.343) loss_u loss_u 0.8535 (0.8720) acc_u 15.6250 (14.5312) lr 2.3959e-04 eta 0:00:20
epoch [157/200] batch [25/64] time 0.439 (0.474) data 0.306 (0.344) loss_u loss_u 0.8965 (0.8745) acc_u 18.7500 (14.8750) lr 2.3959e-04 eta 0:00:18
epoch [157/200] batch [30/64] time 0.402 (0.472) data 0.270 (0.341) loss_u loss_u 0.8892 (0.8742) acc_u 12.5000 (14.8958) lr 2.3959e-04 eta 0:00:16
epoch [157/200] batch [35/64] time 0.419 (0.471) data 0.287 (0.340) loss_u loss_u 0.8857 (0.8723) acc_u 12.5000 (15.3571) lr 2.3959e-04 eta 0:00:13
epoch [157/200] batch [40/64] time 0.610 (0.468) data 0.478 (0.337) loss_u loss_u 0.8022 (0.8683) acc_u 18.7500 (15.7812) lr 2.3959e-04 eta 0:00:11
epoch [157/200] batch [45/64] time 0.371 (0.465) data 0.240 (0.334) loss_u loss_u 0.8740 (0.8632) acc_u 12.5000 (16.5278) lr 2.3959e-04 eta 0:00:08
epoch [157/200] batch [50/64] time 0.548 (0.466) data 0.418 (0.335) loss_u loss_u 0.9092 (0.8660) acc_u 9.3750 (16.2500) lr 2.3959e-04 eta 0:00:06
epoch [157/200] batch [55/64] time 0.459 (0.466) data 0.328 (0.335) loss_u loss_u 0.8242 (0.8646) acc_u 25.0000 (16.3636) lr 2.3959e-04 eta 0:00:04
epoch [157/200] batch [60/64] time 0.419 (0.466) data 0.287 (0.335) loss_u loss_u 0.9009 (0.8667) acc_u 12.5000 (16.1458) lr 2.3959e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1607
confident_label rate tensor(0.3323, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1042
clean true:887
clean false:155
clean_rate:0.8512476007677543
noisy true:642
noisy false:1452
after delete: len(clean_dataset) 1042
after delete: len(noisy_dataset) 2094
epoch [158/200] batch [5/32] time 0.579 (0.485) data 0.449 (0.354) loss_x loss_x 1.3291 (1.0370) acc_x 75.0000 (76.2500) lr 2.2949e-04 eta 0:00:13
epoch [158/200] batch [10/32] time 0.572 (0.493) data 0.440 (0.362) loss_x loss_x 0.9258 (1.0231) acc_x 75.0000 (75.6250) lr 2.2949e-04 eta 0:00:10
epoch [158/200] batch [15/32] time 0.607 (0.493) data 0.476 (0.362) loss_x loss_x 0.8013 (0.9932) acc_x 71.8750 (73.9583) lr 2.2949e-04 eta 0:00:08
epoch [158/200] batch [20/32] time 0.461 (0.484) data 0.330 (0.353) loss_x loss_x 1.4365 (1.0763) acc_x 59.3750 (71.7188) lr 2.2949e-04 eta 0:00:05
epoch [158/200] batch [25/32] time 0.579 (0.484) data 0.448 (0.353) loss_x loss_x 1.1240 (1.0552) acc_x 62.5000 (72.8750) lr 2.2949e-04 eta 0:00:03
epoch [158/200] batch [30/32] time 0.457 (0.484) data 0.326 (0.353) loss_x loss_x 1.2578 (1.0955) acc_x 65.6250 (71.9792) lr 2.2949e-04 eta 0:00:00
epoch [158/200] batch [5/65] time 0.526 (0.481) data 0.394 (0.350) loss_u loss_u 0.9229 (0.8471) acc_u 3.1250 (17.5000) lr 2.2949e-04 eta 0:00:28
epoch [158/200] batch [10/65] time 0.457 (0.477) data 0.326 (0.346) loss_u loss_u 0.9248 (0.8625) acc_u 6.2500 (16.5625) lr 2.2949e-04 eta 0:00:26
epoch [158/200] batch [15/65] time 0.358 (0.469) data 0.227 (0.338) loss_u loss_u 0.9053 (0.8760) acc_u 9.3750 (14.5833) lr 2.2949e-04 eta 0:00:23
epoch [158/200] batch [20/65] time 0.578 (0.468) data 0.447 (0.337) loss_u loss_u 0.8452 (0.8724) acc_u 15.6250 (14.6875) lr 2.2949e-04 eta 0:00:21
epoch [158/200] batch [25/65] time 0.497 (0.468) data 0.364 (0.337) loss_u loss_u 0.9209 (0.8733) acc_u 12.5000 (14.6250) lr 2.2949e-04 eta 0:00:18
epoch [158/200] batch [30/65] time 0.437 (0.466) data 0.306 (0.335) loss_u loss_u 0.9424 (0.8760) acc_u 3.1250 (14.4792) lr 2.2949e-04 eta 0:00:16
epoch [158/200] batch [35/65] time 0.386 (0.465) data 0.255 (0.334) loss_u loss_u 0.8384 (0.8765) acc_u 18.7500 (14.2857) lr 2.2949e-04 eta 0:00:13
epoch [158/200] batch [40/65] time 0.542 (0.463) data 0.411 (0.332) loss_u loss_u 0.8486 (0.8753) acc_u 15.6250 (14.3750) lr 2.2949e-04 eta 0:00:11
epoch [158/200] batch [45/65] time 0.610 (0.467) data 0.479 (0.335) loss_u loss_u 0.8564 (0.8750) acc_u 12.5000 (14.4444) lr 2.2949e-04 eta 0:00:09
epoch [158/200] batch [50/65] time 0.560 (0.469) data 0.429 (0.338) loss_u loss_u 0.8960 (0.8742) acc_u 15.6250 (14.9375) lr 2.2949e-04 eta 0:00:07
epoch [158/200] batch [55/65] time 0.414 (0.468) data 0.283 (0.337) loss_u loss_u 0.8823 (0.8734) acc_u 18.7500 (15.1136) lr 2.2949e-04 eta 0:00:04
epoch [158/200] batch [60/65] time 0.419 (0.466) data 0.287 (0.335) loss_u loss_u 0.8584 (0.8740) acc_u 15.6250 (15.0521) lr 2.2949e-04 eta 0:00:02
epoch [158/200] batch [65/65] time 0.429 (0.464) data 0.299 (0.333) loss_u loss_u 0.8809 (0.8751) acc_u 12.5000 (14.9038) lr 2.2949e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1600
confident_label rate tensor(0.3297, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1034
clean true:892
clean false:142
clean_rate:0.8626692456479691
noisy true:644
noisy false:1458
after delete: len(clean_dataset) 1034
after delete: len(noisy_dataset) 2102
epoch [159/200] batch [5/32] time 0.490 (0.508) data 0.359 (0.378) loss_x loss_x 0.8237 (1.0967) acc_x 68.7500 (70.6250) lr 2.1957e-04 eta 0:00:13
epoch [159/200] batch [10/32] time 0.454 (0.505) data 0.323 (0.375) loss_x loss_x 1.0723 (1.0581) acc_x 75.0000 (72.5000) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [15/32] time 0.504 (0.497) data 0.373 (0.367) loss_x loss_x 1.2949 (1.0714) acc_x 62.5000 (71.0417) lr 2.1957e-04 eta 0:00:08
epoch [159/200] batch [20/32] time 0.537 (0.504) data 0.406 (0.373) loss_x loss_x 1.1582 (1.1112) acc_x 68.7500 (70.1562) lr 2.1957e-04 eta 0:00:06
epoch [159/200] batch [25/32] time 0.410 (0.497) data 0.279 (0.367) loss_x loss_x 1.1172 (1.1123) acc_x 71.8750 (70.1250) lr 2.1957e-04 eta 0:00:03
epoch [159/200] batch [30/32] time 0.593 (0.499) data 0.462 (0.369) loss_x loss_x 1.1660 (1.1078) acc_x 65.6250 (70.6250) lr 2.1957e-04 eta 0:00:00
epoch [159/200] batch [5/65] time 0.606 (0.503) data 0.474 (0.373) loss_u loss_u 0.8506 (0.8450) acc_u 18.7500 (20.0000) lr 2.1957e-04 eta 0:00:30
epoch [159/200] batch [10/65] time 0.445 (0.498) data 0.315 (0.368) loss_u loss_u 0.8921 (0.8517) acc_u 15.6250 (19.3750) lr 2.1957e-04 eta 0:00:27
epoch [159/200] batch [15/65] time 0.444 (0.491) data 0.311 (0.361) loss_u loss_u 0.8857 (0.8568) acc_u 18.7500 (19.5833) lr 2.1957e-04 eta 0:00:24
epoch [159/200] batch [20/65] time 0.494 (0.494) data 0.363 (0.363) loss_u loss_u 0.8184 (0.8605) acc_u 18.7500 (18.7500) lr 2.1957e-04 eta 0:00:22
epoch [159/200] batch [25/65] time 0.400 (0.483) data 0.268 (0.352) loss_u loss_u 0.8638 (0.8621) acc_u 15.6250 (18.5000) lr 2.1957e-04 eta 0:00:19
epoch [159/200] batch [30/65] time 0.390 (0.476) data 0.259 (0.345) loss_u loss_u 0.8540 (0.8596) acc_u 18.7500 (18.9583) lr 2.1957e-04 eta 0:00:16
epoch [159/200] batch [35/65] time 0.351 (0.473) data 0.219 (0.342) loss_u loss_u 0.9033 (0.8637) acc_u 9.3750 (18.0357) lr 2.1957e-04 eta 0:00:14
epoch [159/200] batch [40/65] time 0.529 (0.473) data 0.397 (0.342) loss_u loss_u 0.8447 (0.8640) acc_u 15.6250 (17.8906) lr 2.1957e-04 eta 0:00:11
epoch [159/200] batch [45/65] time 0.373 (0.473) data 0.241 (0.342) loss_u loss_u 0.8535 (0.8627) acc_u 18.7500 (17.9861) lr 2.1957e-04 eta 0:00:09
epoch [159/200] batch [50/65] time 0.480 (0.471) data 0.349 (0.340) loss_u loss_u 0.8823 (0.8631) acc_u 12.5000 (17.8125) lr 2.1957e-04 eta 0:00:07
epoch [159/200] batch [55/65] time 0.405 (0.470) data 0.273 (0.338) loss_u loss_u 0.8301 (0.8605) acc_u 25.0000 (18.1250) lr 2.1957e-04 eta 0:00:04
epoch [159/200] batch [60/65] time 0.535 (0.469) data 0.403 (0.338) loss_u loss_u 0.9297 (0.8640) acc_u 9.3750 (17.6042) lr 2.1957e-04 eta 0:00:02
epoch [159/200] batch [65/65] time 0.487 (0.468) data 0.355 (0.337) loss_u loss_u 0.8887 (0.8630) acc_u 15.6250 (17.8365) lr 2.1957e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1541
confident_label rate tensor(0.3438, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1078
clean true:941
clean false:137
clean_rate:0.87291280148423
noisy true:654
noisy false:1404
after delete: len(clean_dataset) 1078
after delete: len(noisy_dataset) 2058
epoch [160/200] batch [5/33] time 0.396 (0.441) data 0.266 (0.310) loss_x loss_x 1.0303 (1.2684) acc_x 71.8750 (66.2500) lr 2.0984e-04 eta 0:00:12
epoch [160/200] batch [10/33] time 0.465 (0.462) data 0.335 (0.331) loss_x loss_x 0.9888 (1.1047) acc_x 68.7500 (70.3125) lr 2.0984e-04 eta 0:00:10
epoch [160/200] batch [15/33] time 0.604 (0.487) data 0.474 (0.357) loss_x loss_x 1.3525 (1.1253) acc_x 68.7500 (71.0417) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [20/33] time 0.493 (0.482) data 0.362 (0.351) loss_x loss_x 0.8086 (1.0842) acc_x 71.8750 (72.5000) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [25/33] time 0.462 (0.477) data 0.331 (0.346) loss_x loss_x 0.8662 (1.0826) acc_x 68.7500 (72.1250) lr 2.0984e-04 eta 0:00:03
epoch [160/200] batch [30/33] time 0.562 (0.484) data 0.431 (0.353) loss_x loss_x 0.7598 (1.0767) acc_x 87.5000 (72.5000) lr 2.0984e-04 eta 0:00:01
epoch [160/200] batch [5/64] time 0.340 (0.468) data 0.208 (0.337) loss_u loss_u 0.8828 (0.8733) acc_u 15.6250 (15.0000) lr 2.0984e-04 eta 0:00:27
epoch [160/200] batch [10/64] time 0.450 (0.470) data 0.317 (0.339) loss_u loss_u 0.9141 (0.8750) acc_u 9.3750 (16.2500) lr 2.0984e-04 eta 0:00:25
epoch [160/200] batch [15/64] time 0.453 (0.468) data 0.322 (0.337) loss_u loss_u 0.7920 (0.8653) acc_u 21.8750 (16.6667) lr 2.0984e-04 eta 0:00:22
epoch [160/200] batch [20/64] time 0.523 (0.472) data 0.392 (0.341) loss_u loss_u 0.9204 (0.8624) acc_u 9.3750 (16.8750) lr 2.0984e-04 eta 0:00:20
epoch [160/200] batch [25/64] time 0.383 (0.468) data 0.252 (0.337) loss_u loss_u 0.9229 (0.8641) acc_u 9.3750 (16.7500) lr 2.0984e-04 eta 0:00:18
epoch [160/200] batch [30/64] time 0.438 (0.471) data 0.306 (0.339) loss_u loss_u 0.9268 (0.8647) acc_u 9.3750 (16.5625) lr 2.0984e-04 eta 0:00:16
epoch [160/200] batch [35/64] time 0.488 (0.467) data 0.356 (0.336) loss_u loss_u 0.8398 (0.8632) acc_u 21.8750 (16.7857) lr 2.0984e-04 eta 0:00:13
epoch [160/200] batch [40/64] time 0.494 (0.471) data 0.362 (0.339) loss_u loss_u 0.8555 (0.8626) acc_u 21.8750 (16.9531) lr 2.0984e-04 eta 0:00:11
epoch [160/200] batch [45/64] time 0.554 (0.468) data 0.422 (0.337) loss_u loss_u 0.8350 (0.8648) acc_u 28.1250 (16.8056) lr 2.0984e-04 eta 0:00:08
epoch [160/200] batch [50/64] time 0.465 (0.466) data 0.333 (0.334) loss_u loss_u 0.8623 (0.8662) acc_u 15.6250 (16.5000) lr 2.0984e-04 eta 0:00:06
epoch [160/200] batch [55/64] time 0.549 (0.465) data 0.417 (0.334) loss_u loss_u 0.9194 (0.8704) acc_u 9.3750 (16.0795) lr 2.0984e-04 eta 0:00:04
epoch [160/200] batch [60/64] time 0.695 (0.466) data 0.562 (0.334) loss_u loss_u 0.8735 (0.8691) acc_u 12.5000 (16.2500) lr 2.0984e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1527
confident_label rate tensor(0.3508, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1100
clean true:959
clean false:141
clean_rate:0.8718181818181818
noisy true:650
noisy false:1386
after delete: len(clean_dataset) 1100
after delete: len(noisy_dataset) 2036
epoch [161/200] batch [5/34] time 0.435 (0.445) data 0.304 (0.315) loss_x loss_x 1.0459 (1.0706) acc_x 75.0000 (73.7500) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [10/34] time 0.470 (0.466) data 0.340 (0.335) loss_x loss_x 0.7432 (1.0545) acc_x 84.3750 (74.0625) lr 2.0032e-04 eta 0:00:11
epoch [161/200] batch [15/34] time 0.480 (0.474) data 0.349 (0.344) loss_x loss_x 1.5635 (1.1228) acc_x 59.3750 (72.5000) lr 2.0032e-04 eta 0:00:09
epoch [161/200] batch [20/34] time 0.400 (0.463) data 0.269 (0.332) loss_x loss_x 1.6562 (1.1501) acc_x 59.3750 (72.6562) lr 2.0032e-04 eta 0:00:06
epoch [161/200] batch [25/34] time 0.618 (0.476) data 0.487 (0.345) loss_x loss_x 0.7061 (1.0912) acc_x 87.5000 (74.0000) lr 2.0032e-04 eta 0:00:04
epoch [161/200] batch [30/34] time 0.572 (0.474) data 0.441 (0.343) loss_x loss_x 1.3906 (1.0898) acc_x 59.3750 (73.3333) lr 2.0032e-04 eta 0:00:01
epoch [161/200] batch [5/63] time 0.322 (0.458) data 0.190 (0.327) loss_u loss_u 0.8589 (0.8651) acc_u 15.6250 (15.6250) lr 2.0032e-04 eta 0:00:26
epoch [161/200] batch [10/63] time 0.391 (0.453) data 0.260 (0.322) loss_u loss_u 0.8477 (0.8654) acc_u 15.6250 (15.0000) lr 2.0032e-04 eta 0:00:24
epoch [161/200] batch [15/63] time 0.694 (0.459) data 0.563 (0.328) loss_u loss_u 0.8999 (0.8763) acc_u 15.6250 (14.3750) lr 2.0032e-04 eta 0:00:22
epoch [161/200] batch [20/63] time 0.418 (0.456) data 0.288 (0.325) loss_u loss_u 0.8833 (0.8704) acc_u 15.6250 (15.6250) lr 2.0032e-04 eta 0:00:19
epoch [161/200] batch [25/63] time 0.441 (0.457) data 0.309 (0.326) loss_u loss_u 0.8833 (0.8671) acc_u 12.5000 (16.2500) lr 2.0032e-04 eta 0:00:17
epoch [161/200] batch [30/63] time 0.383 (0.456) data 0.251 (0.325) loss_u loss_u 0.8945 (0.8688) acc_u 15.6250 (15.9375) lr 2.0032e-04 eta 0:00:15
epoch [161/200] batch [35/63] time 0.509 (0.456) data 0.378 (0.325) loss_u loss_u 0.9062 (0.8700) acc_u 12.5000 (15.7143) lr 2.0032e-04 eta 0:00:12
epoch [161/200] batch [40/63] time 0.494 (0.453) data 0.362 (0.322) loss_u loss_u 0.9004 (0.8702) acc_u 9.3750 (15.8594) lr 2.0032e-04 eta 0:00:10
epoch [161/200] batch [45/63] time 0.355 (0.453) data 0.224 (0.322) loss_u loss_u 0.9253 (0.8748) acc_u 12.5000 (15.4167) lr 2.0032e-04 eta 0:00:08
epoch [161/200] batch [50/63] time 0.507 (0.453) data 0.376 (0.322) loss_u loss_u 0.9102 (0.8750) acc_u 9.3750 (15.5000) lr 2.0032e-04 eta 0:00:05
epoch [161/200] batch [55/63] time 0.475 (0.453) data 0.342 (0.322) loss_u loss_u 0.9404 (0.8775) acc_u 9.3750 (15.1705) lr 2.0032e-04 eta 0:00:03
epoch [161/200] batch [60/63] time 0.412 (0.452) data 0.281 (0.321) loss_u loss_u 0.8457 (0.8751) acc_u 18.7500 (15.4688) lr 2.0032e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1576
confident_label rate tensor(0.3316, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1040
clean true:905
clean false:135
clean_rate:0.8701923076923077
noisy true:655
noisy false:1441
after delete: len(clean_dataset) 1040
after delete: len(noisy_dataset) 2096
epoch [162/200] batch [5/32] time 0.522 (0.510) data 0.392 (0.380) loss_x loss_x 1.2100 (1.1413) acc_x 78.1250 (74.3750) lr 1.9098e-04 eta 0:00:13
epoch [162/200] batch [10/32] time 0.522 (0.482) data 0.391 (0.351) loss_x loss_x 0.4460 (1.1857) acc_x 84.3750 (72.8125) lr 1.9098e-04 eta 0:00:10
epoch [162/200] batch [15/32] time 0.364 (0.453) data 0.234 (0.322) loss_x loss_x 1.2686 (1.1811) acc_x 59.3750 (70.6250) lr 1.9098e-04 eta 0:00:07
epoch [162/200] batch [20/32] time 0.423 (0.457) data 0.293 (0.326) loss_x loss_x 1.5342 (1.2054) acc_x 68.7500 (70.6250) lr 1.9098e-04 eta 0:00:05
epoch [162/200] batch [25/32] time 0.601 (0.462) data 0.470 (0.331) loss_x loss_x 1.3779 (1.1696) acc_x 71.8750 (71.3750) lr 1.9098e-04 eta 0:00:03
epoch [162/200] batch [30/32] time 0.448 (0.460) data 0.317 (0.330) loss_x loss_x 1.3066 (1.1940) acc_x 68.7500 (70.6250) lr 1.9098e-04 eta 0:00:00
epoch [162/200] batch [5/65] time 0.385 (0.464) data 0.253 (0.333) loss_u loss_u 0.9297 (0.8790) acc_u 9.3750 (14.3750) lr 1.9098e-04 eta 0:00:27
epoch [162/200] batch [10/65] time 0.374 (0.457) data 0.243 (0.327) loss_u loss_u 0.8574 (0.8752) acc_u 18.7500 (15.0000) lr 1.9098e-04 eta 0:00:25
epoch [162/200] batch [15/65] time 0.423 (0.453) data 0.291 (0.322) loss_u loss_u 0.8369 (0.8576) acc_u 25.0000 (17.5000) lr 1.9098e-04 eta 0:00:22
epoch [162/200] batch [20/65] time 0.587 (0.451) data 0.455 (0.320) loss_u loss_u 0.8257 (0.8629) acc_u 15.6250 (17.0312) lr 1.9098e-04 eta 0:00:20
epoch [162/200] batch [25/65] time 0.418 (0.446) data 0.287 (0.315) loss_u loss_u 0.8525 (0.8633) acc_u 28.1250 (17.2500) lr 1.9098e-04 eta 0:00:17
epoch [162/200] batch [30/65] time 0.480 (0.443) data 0.348 (0.312) loss_u loss_u 0.8833 (0.8655) acc_u 12.5000 (16.7708) lr 1.9098e-04 eta 0:00:15
epoch [162/200] batch [35/65] time 0.507 (0.446) data 0.376 (0.315) loss_u loss_u 0.8877 (0.8619) acc_u 15.6250 (17.5893) lr 1.9098e-04 eta 0:00:13
epoch [162/200] batch [40/65] time 0.559 (0.444) data 0.427 (0.313) loss_u loss_u 0.8818 (0.8645) acc_u 9.3750 (17.0312) lr 1.9098e-04 eta 0:00:11
epoch [162/200] batch [45/65] time 0.341 (0.444) data 0.209 (0.313) loss_u loss_u 0.8438 (0.8659) acc_u 28.1250 (16.9444) lr 1.9098e-04 eta 0:00:08
epoch [162/200] batch [50/65] time 0.554 (0.445) data 0.420 (0.314) loss_u loss_u 0.9170 (0.8678) acc_u 9.3750 (16.8125) lr 1.9098e-04 eta 0:00:06
epoch [162/200] batch [55/65] time 0.357 (0.445) data 0.225 (0.314) loss_u loss_u 0.8872 (0.8661) acc_u 12.5000 (17.0455) lr 1.9098e-04 eta 0:00:04
epoch [162/200] batch [60/65] time 0.607 (0.446) data 0.475 (0.315) loss_u loss_u 0.8823 (0.8687) acc_u 18.7500 (16.7188) lr 1.9098e-04 eta 0:00:02
epoch [162/200] batch [65/65] time 0.439 (0.448) data 0.309 (0.317) loss_u loss_u 0.8994 (0.8679) acc_u 12.5000 (16.7308) lr 1.9098e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1571
confident_label rate tensor(0.3294, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1033
clean true:890
clean false:143
clean_rate:0.861568247821878
noisy true:675
noisy false:1428
after delete: len(clean_dataset) 1033
after delete: len(noisy_dataset) 2103
epoch [163/200] batch [5/32] time 0.546 (0.513) data 0.416 (0.383) loss_x loss_x 1.6602 (1.3757) acc_x 59.3750 (62.5000) lr 1.8185e-04 eta 0:00:13
epoch [163/200] batch [10/32] time 0.520 (0.479) data 0.389 (0.348) loss_x loss_x 1.0059 (1.1868) acc_x 71.8750 (68.4375) lr 1.8185e-04 eta 0:00:10
epoch [163/200] batch [15/32] time 0.427 (0.491) data 0.297 (0.360) loss_x loss_x 1.4443 (1.1732) acc_x 68.7500 (70.0000) lr 1.8185e-04 eta 0:00:08
epoch [163/200] batch [20/32] time 0.393 (0.483) data 0.262 (0.352) loss_x loss_x 1.0684 (1.1164) acc_x 68.7500 (71.2500) lr 1.8185e-04 eta 0:00:05
epoch [163/200] batch [25/32] time 0.488 (0.499) data 0.357 (0.368) loss_x loss_x 1.0615 (1.1156) acc_x 75.0000 (71.5000) lr 1.8185e-04 eta 0:00:03
epoch [163/200] batch [30/32] time 0.394 (0.492) data 0.260 (0.361) loss_x loss_x 1.2510 (1.0823) acc_x 68.7500 (72.6042) lr 1.8185e-04 eta 0:00:00
epoch [163/200] batch [5/65] time 0.384 (0.487) data 0.253 (0.356) loss_u loss_u 0.9102 (0.8827) acc_u 15.6250 (16.8750) lr 1.8185e-04 eta 0:00:29
epoch [163/200] batch [10/65] time 0.333 (0.478) data 0.203 (0.347) loss_u loss_u 0.8032 (0.8724) acc_u 21.8750 (17.1875) lr 1.8185e-04 eta 0:00:26
epoch [163/200] batch [15/65] time 0.750 (0.482) data 0.620 (0.351) loss_u loss_u 0.7783 (0.8685) acc_u 28.1250 (17.0833) lr 1.8185e-04 eta 0:00:24
epoch [163/200] batch [20/65] time 0.430 (0.474) data 0.299 (0.343) loss_u loss_u 0.8755 (0.8740) acc_u 15.6250 (16.2500) lr 1.8185e-04 eta 0:00:21
epoch [163/200] batch [25/65] time 0.432 (0.469) data 0.301 (0.338) loss_u loss_u 0.8813 (0.8736) acc_u 15.6250 (16.3750) lr 1.8185e-04 eta 0:00:18
epoch [163/200] batch [30/65] time 0.557 (0.472) data 0.426 (0.341) loss_u loss_u 0.8682 (0.8760) acc_u 18.7500 (16.2500) lr 1.8185e-04 eta 0:00:16
epoch [163/200] batch [35/65] time 0.403 (0.467) data 0.270 (0.337) loss_u loss_u 0.7715 (0.8737) acc_u 28.1250 (16.1607) lr 1.8185e-04 eta 0:00:14
epoch [163/200] batch [40/65] time 0.425 (0.465) data 0.295 (0.334) loss_u loss_u 0.9019 (0.8687) acc_u 12.5000 (16.6406) lr 1.8185e-04 eta 0:00:11
epoch [163/200] batch [45/65] time 0.521 (0.467) data 0.390 (0.336) loss_u loss_u 0.8550 (0.8742) acc_u 18.7500 (15.9722) lr 1.8185e-04 eta 0:00:09
epoch [163/200] batch [50/65] time 0.406 (0.464) data 0.276 (0.333) loss_u loss_u 0.8350 (0.8722) acc_u 25.0000 (16.1875) lr 1.8185e-04 eta 0:00:06
epoch [163/200] batch [55/65] time 0.458 (0.461) data 0.326 (0.330) loss_u loss_u 0.8203 (0.8717) acc_u 18.7500 (16.1364) lr 1.8185e-04 eta 0:00:04
epoch [163/200] batch [60/65] time 0.575 (0.463) data 0.443 (0.332) loss_u loss_u 0.9346 (0.8750) acc_u 3.1250 (15.6250) lr 1.8185e-04 eta 0:00:02
epoch [163/200] batch [65/65] time 0.533 (0.465) data 0.402 (0.334) loss_u loss_u 0.9307 (0.8745) acc_u 3.1250 (15.4808) lr 1.8185e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1579
confident_label rate tensor(0.3351, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1051
clean true:916
clean false:135
clean_rate:0.8715509039010466
noisy true:641
noisy false:1444
after delete: len(clean_dataset) 1051
after delete: len(noisy_dataset) 2085
epoch [164/200] batch [5/32] time 0.451 (0.501) data 0.321 (0.371) loss_x loss_x 1.2734 (1.2117) acc_x 65.6250 (70.6250) lr 1.7292e-04 eta 0:00:13
epoch [164/200] batch [10/32] time 0.563 (0.511) data 0.431 (0.381) loss_x loss_x 1.1289 (1.1505) acc_x 71.8750 (70.9375) lr 1.7292e-04 eta 0:00:11
epoch [164/200] batch [15/32] time 0.451 (0.488) data 0.320 (0.357) loss_x loss_x 0.8389 (1.0931) acc_x 75.0000 (73.1250) lr 1.7292e-04 eta 0:00:08
epoch [164/200] batch [20/32] time 0.535 (0.482) data 0.404 (0.351) loss_x loss_x 1.2959 (1.1197) acc_x 68.7500 (71.7188) lr 1.7292e-04 eta 0:00:05
epoch [164/200] batch [25/32] time 0.516 (0.485) data 0.385 (0.355) loss_x loss_x 1.0508 (1.1057) acc_x 78.1250 (72.1250) lr 1.7292e-04 eta 0:00:03
epoch [164/200] batch [30/32] time 0.434 (0.478) data 0.303 (0.347) loss_x loss_x 1.0107 (1.0880) acc_x 78.1250 (72.6042) lr 1.7292e-04 eta 0:00:00
epoch [164/200] batch [5/65] time 0.574 (0.473) data 0.442 (0.342) loss_u loss_u 0.8032 (0.8545) acc_u 21.8750 (16.8750) lr 1.7292e-04 eta 0:00:28
epoch [164/200] batch [10/65] time 0.585 (0.474) data 0.454 (0.343) loss_u loss_u 0.8540 (0.8557) acc_u 25.0000 (18.1250) lr 1.7292e-04 eta 0:00:26
epoch [164/200] batch [15/65] time 0.421 (0.471) data 0.290 (0.340) loss_u loss_u 0.8418 (0.8610) acc_u 25.0000 (17.9167) lr 1.7292e-04 eta 0:00:23
epoch [164/200] batch [20/65] time 0.499 (0.467) data 0.367 (0.336) loss_u loss_u 0.9087 (0.8734) acc_u 21.8750 (16.7188) lr 1.7292e-04 eta 0:00:21
epoch [164/200] batch [25/65] time 0.470 (0.468) data 0.340 (0.337) loss_u loss_u 0.9097 (0.8739) acc_u 12.5000 (16.2500) lr 1.7292e-04 eta 0:00:18
epoch [164/200] batch [30/65] time 0.574 (0.471) data 0.444 (0.340) loss_u loss_u 0.8462 (0.8694) acc_u 18.7500 (16.4583) lr 1.7292e-04 eta 0:00:16
epoch [164/200] batch [35/65] time 0.662 (0.472) data 0.532 (0.341) loss_u loss_u 0.8301 (0.8693) acc_u 21.8750 (16.4286) lr 1.7292e-04 eta 0:00:14
epoch [164/200] batch [40/65] time 0.468 (0.471) data 0.337 (0.340) loss_u loss_u 0.8096 (0.8706) acc_u 25.0000 (16.4844) lr 1.7292e-04 eta 0:00:11
epoch [164/200] batch [45/65] time 0.395 (0.466) data 0.264 (0.335) loss_u loss_u 0.8745 (0.8732) acc_u 15.6250 (16.1806) lr 1.7292e-04 eta 0:00:09
epoch [164/200] batch [50/65] time 0.499 (0.465) data 0.368 (0.334) loss_u loss_u 0.8428 (0.8707) acc_u 18.7500 (16.1250) lr 1.7292e-04 eta 0:00:06
epoch [164/200] batch [55/65] time 0.498 (0.468) data 0.366 (0.337) loss_u loss_u 0.8867 (0.8716) acc_u 18.7500 (15.8523) lr 1.7292e-04 eta 0:00:04
epoch [164/200] batch [60/65] time 0.401 (0.465) data 0.270 (0.334) loss_u loss_u 0.8745 (0.8733) acc_u 15.6250 (15.7292) lr 1.7292e-04 eta 0:00:02
epoch [164/200] batch [65/65] time 0.404 (0.465) data 0.272 (0.334) loss_u loss_u 0.8218 (0.8723) acc_u 25.0000 (15.8654) lr 1.7292e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1564
confident_label rate tensor(0.3374, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1058
clean true:914
clean false:144
clean_rate:0.8638941398865785
noisy true:658
noisy false:1420
after delete: len(clean_dataset) 1058
after delete: len(noisy_dataset) 2078
epoch [165/200] batch [5/33] time 0.427 (0.513) data 0.296 (0.382) loss_x loss_x 1.0371 (1.3240) acc_x 62.5000 (70.6250) lr 1.6419e-04 eta 0:00:14
epoch [165/200] batch [10/33] time 0.425 (0.511) data 0.295 (0.380) loss_x loss_x 1.2373 (1.2604) acc_x 59.3750 (69.0625) lr 1.6419e-04 eta 0:00:11
epoch [165/200] batch [15/33] time 0.508 (0.513) data 0.377 (0.382) loss_x loss_x 1.6768 (1.2240) acc_x 62.5000 (69.7917) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [20/33] time 0.480 (0.519) data 0.349 (0.388) loss_x loss_x 1.0957 (1.2443) acc_x 65.6250 (69.3750) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [25/33] time 0.633 (0.506) data 0.502 (0.375) loss_x loss_x 1.4727 (1.2188) acc_x 59.3750 (70.0000) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [30/33] time 0.443 (0.505) data 0.313 (0.374) loss_x loss_x 0.9849 (1.2134) acc_x 78.1250 (69.5833) lr 1.6419e-04 eta 0:00:01
epoch [165/200] batch [5/64] time 0.360 (0.500) data 0.229 (0.369) loss_u loss_u 0.9150 (0.9058) acc_u 9.3750 (11.8750) lr 1.6419e-04 eta 0:00:29
epoch [165/200] batch [10/64] time 0.424 (0.497) data 0.294 (0.366) loss_u loss_u 0.8496 (0.8715) acc_u 15.6250 (15.9375) lr 1.6419e-04 eta 0:00:26
epoch [165/200] batch [15/64] time 0.419 (0.489) data 0.289 (0.358) loss_u loss_u 0.9385 (0.8739) acc_u 9.3750 (16.0417) lr 1.6419e-04 eta 0:00:23
epoch [165/200] batch [20/64] time 0.410 (0.483) data 0.279 (0.352) loss_u loss_u 0.8926 (0.8704) acc_u 12.5000 (16.7188) lr 1.6419e-04 eta 0:00:21
epoch [165/200] batch [25/64] time 0.422 (0.477) data 0.291 (0.346) loss_u loss_u 0.8369 (0.8687) acc_u 18.7500 (17.1250) lr 1.6419e-04 eta 0:00:18
epoch [165/200] batch [30/64] time 0.407 (0.476) data 0.275 (0.345) loss_u loss_u 0.7598 (0.8637) acc_u 31.2500 (17.7083) lr 1.6419e-04 eta 0:00:16
epoch [165/200] batch [35/64] time 0.385 (0.474) data 0.253 (0.343) loss_u loss_u 0.8198 (0.8577) acc_u 21.8750 (18.1250) lr 1.6419e-04 eta 0:00:13
epoch [165/200] batch [40/64] time 0.437 (0.475) data 0.306 (0.344) loss_u loss_u 0.8135 (0.8599) acc_u 28.1250 (17.8125) lr 1.6419e-04 eta 0:00:11
epoch [165/200] batch [45/64] time 0.424 (0.475) data 0.293 (0.344) loss_u loss_u 0.9253 (0.8658) acc_u 9.3750 (17.1528) lr 1.6419e-04 eta 0:00:09
epoch [165/200] batch [50/64] time 0.468 (0.473) data 0.336 (0.342) loss_u loss_u 0.9434 (0.8688) acc_u 6.2500 (16.5625) lr 1.6419e-04 eta 0:00:06
epoch [165/200] batch [55/64] time 0.540 (0.475) data 0.409 (0.344) loss_u loss_u 0.9121 (0.8685) acc_u 12.5000 (16.4773) lr 1.6419e-04 eta 0:00:04
epoch [165/200] batch [60/64] time 0.545 (0.477) data 0.414 (0.346) loss_u loss_u 0.9253 (0.8710) acc_u 6.2500 (15.9375) lr 1.6419e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1568
confident_label rate tensor(0.3291, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1032
clean true:886
clean false:146
clean_rate:0.8585271317829457
noisy true:682
noisy false:1422
after delete: len(clean_dataset) 1032
after delete: len(noisy_dataset) 2104
epoch [166/200] batch [5/32] time 0.384 (0.459) data 0.253 (0.328) loss_x loss_x 0.9741 (1.0468) acc_x 75.0000 (74.3750) lr 1.5567e-04 eta 0:00:12
epoch [166/200] batch [10/32] time 0.481 (0.466) data 0.350 (0.334) loss_x loss_x 0.7954 (1.0691) acc_x 81.2500 (74.3750) lr 1.5567e-04 eta 0:00:10
epoch [166/200] batch [15/32] time 0.533 (0.465) data 0.401 (0.333) loss_x loss_x 0.9204 (1.0742) acc_x 78.1250 (75.2083) lr 1.5567e-04 eta 0:00:07
epoch [166/200] batch [20/32] time 0.393 (0.460) data 0.263 (0.329) loss_x loss_x 0.7690 (1.0692) acc_x 81.2500 (74.6875) lr 1.5567e-04 eta 0:00:05
epoch [166/200] batch [25/32] time 0.501 (0.455) data 0.369 (0.324) loss_x loss_x 1.2793 (1.0787) acc_x 53.1250 (72.8750) lr 1.5567e-04 eta 0:00:03
epoch [166/200] batch [30/32] time 0.538 (0.459) data 0.408 (0.328) loss_x loss_x 1.4287 (1.0739) acc_x 71.8750 (73.4375) lr 1.5567e-04 eta 0:00:00
epoch [166/200] batch [5/65] time 0.394 (0.454) data 0.263 (0.323) loss_u loss_u 0.9209 (0.8980) acc_u 12.5000 (12.5000) lr 1.5567e-04 eta 0:00:27
epoch [166/200] batch [10/65] time 0.375 (0.452) data 0.243 (0.321) loss_u loss_u 0.8633 (0.8940) acc_u 15.6250 (11.8750) lr 1.5567e-04 eta 0:00:24
epoch [166/200] batch [15/65] time 0.566 (0.461) data 0.434 (0.330) loss_u loss_u 0.9043 (0.8899) acc_u 12.5000 (13.1250) lr 1.5567e-04 eta 0:00:23
epoch [166/200] batch [20/65] time 0.390 (0.454) data 0.258 (0.323) loss_u loss_u 0.8818 (0.8862) acc_u 12.5000 (13.4375) lr 1.5567e-04 eta 0:00:20
epoch [166/200] batch [25/65] time 0.499 (0.453) data 0.367 (0.322) loss_u loss_u 0.7725 (0.8824) acc_u 28.1250 (13.7500) lr 1.5567e-04 eta 0:00:18
epoch [166/200] batch [30/65] time 0.459 (0.453) data 0.327 (0.322) loss_u loss_u 0.8462 (0.8786) acc_u 18.7500 (14.2708) lr 1.5567e-04 eta 0:00:15
epoch [166/200] batch [35/65] time 0.377 (0.454) data 0.246 (0.323) loss_u loss_u 0.8325 (0.8715) acc_u 25.0000 (15.0000) lr 1.5567e-04 eta 0:00:13
epoch [166/200] batch [40/65] time 0.546 (0.457) data 0.416 (0.326) loss_u loss_u 0.9072 (0.8702) acc_u 9.3750 (15.3906) lr 1.5567e-04 eta 0:00:11
epoch [166/200] batch [45/65] time 0.578 (0.461) data 0.446 (0.330) loss_u loss_u 0.8892 (0.8690) acc_u 21.8750 (15.6944) lr 1.5567e-04 eta 0:00:09
epoch [166/200] batch [50/65] time 0.520 (0.465) data 0.388 (0.333) loss_u loss_u 0.8950 (0.8703) acc_u 15.6250 (15.7500) lr 1.5567e-04 eta 0:00:06
epoch [166/200] batch [55/65] time 0.486 (0.468) data 0.354 (0.337) loss_u loss_u 0.9067 (0.8739) acc_u 18.7500 (15.4545) lr 1.5567e-04 eta 0:00:04
epoch [166/200] batch [60/65] time 0.464 (0.468) data 0.334 (0.337) loss_u loss_u 0.8271 (0.8702) acc_u 18.7500 (15.7812) lr 1.5567e-04 eta 0:00:02
epoch [166/200] batch [65/65] time 0.400 (0.468) data 0.269 (0.336) loss_u loss_u 0.8335 (0.8705) acc_u 21.8750 (15.7212) lr 1.5567e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1553
confident_label rate tensor(0.3444, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1080
clean true:930
clean false:150
clean_rate:0.8611111111111112
noisy true:653
noisy false:1403
after delete: len(clean_dataset) 1080
after delete: len(noisy_dataset) 2056
epoch [167/200] batch [5/33] time 0.415 (0.425) data 0.285 (0.295) loss_x loss_x 2.0566 (1.2233) acc_x 65.6250 (71.2500) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [10/33] time 0.447 (0.435) data 0.316 (0.304) loss_x loss_x 1.1475 (1.1735) acc_x 71.8750 (70.0000) lr 1.4736e-04 eta 0:00:10
epoch [167/200] batch [15/33] time 0.708 (0.464) data 0.577 (0.333) loss_x loss_x 1.3613 (1.2314) acc_x 65.6250 (69.1667) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [20/33] time 0.558 (0.478) data 0.427 (0.347) loss_x loss_x 1.8223 (1.2524) acc_x 56.2500 (68.9062) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [25/33] time 0.514 (0.487) data 0.384 (0.356) loss_x loss_x 0.8760 (1.1906) acc_x 78.1250 (70.0000) lr 1.4736e-04 eta 0:00:03
epoch [167/200] batch [30/33] time 0.557 (0.484) data 0.426 (0.353) loss_x loss_x 1.1123 (1.1696) acc_x 56.2500 (69.7917) lr 1.4736e-04 eta 0:00:01
epoch [167/200] batch [5/64] time 0.500 (0.476) data 0.370 (0.346) loss_u loss_u 0.9233 (0.8932) acc_u 9.3750 (13.7500) lr 1.4736e-04 eta 0:00:28
epoch [167/200] batch [10/64] time 0.500 (0.473) data 0.368 (0.343) loss_u loss_u 0.8740 (0.8957) acc_u 15.6250 (12.1875) lr 1.4736e-04 eta 0:00:25
epoch [167/200] batch [15/64] time 0.680 (0.476) data 0.549 (0.345) loss_u loss_u 0.8643 (0.8941) acc_u 18.7500 (12.5000) lr 1.4736e-04 eta 0:00:23
epoch [167/200] batch [20/64] time 0.445 (0.475) data 0.313 (0.344) loss_u loss_u 0.8179 (0.8820) acc_u 21.8750 (13.7500) lr 1.4736e-04 eta 0:00:20
epoch [167/200] batch [25/64] time 0.419 (0.474) data 0.287 (0.343) loss_u loss_u 0.8501 (0.8792) acc_u 15.6250 (13.8750) lr 1.4736e-04 eta 0:00:18
epoch [167/200] batch [30/64] time 0.393 (0.475) data 0.261 (0.343) loss_u loss_u 0.8564 (0.8779) acc_u 18.7500 (13.8542) lr 1.4736e-04 eta 0:00:16
epoch [167/200] batch [35/64] time 0.379 (0.473) data 0.249 (0.342) loss_u loss_u 0.7651 (0.8717) acc_u 31.2500 (14.6429) lr 1.4736e-04 eta 0:00:13
epoch [167/200] batch [40/64] time 0.363 (0.471) data 0.231 (0.340) loss_u loss_u 0.8604 (0.8754) acc_u 21.8750 (14.5312) lr 1.4736e-04 eta 0:00:11
epoch [167/200] batch [45/64] time 0.337 (0.470) data 0.205 (0.339) loss_u loss_u 0.8477 (0.8771) acc_u 21.8750 (14.2361) lr 1.4736e-04 eta 0:00:08
epoch [167/200] batch [50/64] time 0.427 (0.473) data 0.295 (0.341) loss_u loss_u 0.8975 (0.8764) acc_u 12.5000 (14.3750) lr 1.4736e-04 eta 0:00:06
epoch [167/200] batch [55/64] time 0.399 (0.472) data 0.268 (0.341) loss_u loss_u 0.8325 (0.8736) acc_u 25.0000 (14.9432) lr 1.4736e-04 eta 0:00:04
epoch [167/200] batch [60/64] time 0.447 (0.471) data 0.315 (0.339) loss_u loss_u 0.8135 (0.8731) acc_u 25.0000 (15.3125) lr 1.4736e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1562
confident_label rate tensor(0.3355, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1052
clean true:911
clean false:141
clean_rate:0.8659695817490495
noisy true:663
noisy false:1421
after delete: len(clean_dataset) 1052
after delete: len(noisy_dataset) 2084
epoch [168/200] batch [5/32] time 0.466 (0.525) data 0.335 (0.395) loss_x loss_x 0.8027 (1.1261) acc_x 78.1250 (73.7500) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [10/32] time 0.479 (0.535) data 0.349 (0.404) loss_x loss_x 0.9971 (1.0989) acc_x 75.0000 (73.7500) lr 1.3926e-04 eta 0:00:11
epoch [168/200] batch [15/32] time 0.447 (0.518) data 0.316 (0.387) loss_x loss_x 1.4336 (1.0893) acc_x 59.3750 (73.5417) lr 1.3926e-04 eta 0:00:08
epoch [168/200] batch [20/32] time 0.487 (0.504) data 0.356 (0.373) loss_x loss_x 1.7197 (1.1311) acc_x 53.1250 (72.1875) lr 1.3926e-04 eta 0:00:06
epoch [168/200] batch [25/32] time 0.551 (0.494) data 0.420 (0.363) loss_x loss_x 1.2402 (1.1393) acc_x 71.8750 (71.8750) lr 1.3926e-04 eta 0:00:03
epoch [168/200] batch [30/32] time 0.517 (0.484) data 0.386 (0.353) loss_x loss_x 0.8867 (1.1127) acc_x 78.1250 (72.8125) lr 1.3926e-04 eta 0:00:00
epoch [168/200] batch [5/65] time 0.415 (0.479) data 0.284 (0.349) loss_u loss_u 0.8086 (0.8710) acc_u 18.7500 (14.3750) lr 1.3926e-04 eta 0:00:28
epoch [168/200] batch [10/65] time 0.447 (0.476) data 0.316 (0.345) loss_u loss_u 0.8672 (0.8675) acc_u 21.8750 (15.9375) lr 1.3926e-04 eta 0:00:26
epoch [168/200] batch [15/65] time 0.513 (0.471) data 0.383 (0.340) loss_u loss_u 0.8940 (0.8704) acc_u 18.7500 (16.0417) lr 1.3926e-04 eta 0:00:23
epoch [168/200] batch [20/65] time 0.672 (0.475) data 0.542 (0.344) loss_u loss_u 0.8579 (0.8774) acc_u 9.3750 (14.6875) lr 1.3926e-04 eta 0:00:21
epoch [168/200] batch [25/65] time 0.408 (0.477) data 0.276 (0.346) loss_u loss_u 0.9189 (0.8789) acc_u 6.2500 (14.3750) lr 1.3926e-04 eta 0:00:19
epoch [168/200] batch [30/65] time 0.464 (0.476) data 0.333 (0.345) loss_u loss_u 0.7456 (0.8778) acc_u 31.2500 (14.5833) lr 1.3926e-04 eta 0:00:16
epoch [168/200] batch [35/65] time 0.522 (0.476) data 0.389 (0.345) loss_u loss_u 0.8491 (0.8785) acc_u 25.0000 (14.6429) lr 1.3926e-04 eta 0:00:14
epoch [168/200] batch [40/65] time 0.529 (0.474) data 0.396 (0.343) loss_u loss_u 0.8770 (0.8788) acc_u 18.7500 (14.8438) lr 1.3926e-04 eta 0:00:11
epoch [168/200] batch [45/65] time 0.479 (0.478) data 0.346 (0.347) loss_u loss_u 0.8857 (0.8827) acc_u 15.6250 (14.3750) lr 1.3926e-04 eta 0:00:09
epoch [168/200] batch [50/65] time 0.382 (0.478) data 0.251 (0.347) loss_u loss_u 0.8145 (0.8814) acc_u 18.7500 (14.4375) lr 1.3926e-04 eta 0:00:07
epoch [168/200] batch [55/65] time 0.417 (0.478) data 0.285 (0.347) loss_u loss_u 0.8662 (0.8766) acc_u 15.6250 (15.0568) lr 1.3926e-04 eta 0:00:04
epoch [168/200] batch [60/65] time 0.387 (0.476) data 0.254 (0.345) loss_u loss_u 0.9502 (0.8757) acc_u 3.1250 (15.1562) lr 1.3926e-04 eta 0:00:02
epoch [168/200] batch [65/65] time 0.398 (0.476) data 0.266 (0.345) loss_u loss_u 0.8423 (0.8754) acc_u 18.7500 (15.1442) lr 1.3926e-04 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1532
confident_label rate tensor(0.3482, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1092
clean true:933
clean false:159
clean_rate:0.8543956043956044
noisy true:671
noisy false:1373
after delete: len(clean_dataset) 1092
after delete: len(noisy_dataset) 2044
epoch [169/200] batch [5/34] time 0.524 (0.498) data 0.393 (0.367) loss_x loss_x 1.8447 (1.2953) acc_x 62.5000 (65.0000) lr 1.3137e-04 eta 0:00:14
epoch [169/200] batch [10/34] time 0.535 (0.480) data 0.406 (0.350) loss_x loss_x 1.4639 (1.1644) acc_x 65.6250 (70.0000) lr 1.3137e-04 eta 0:00:11
epoch [169/200] batch [15/34] time 0.486 (0.475) data 0.356 (0.345) loss_x loss_x 2.0371 (1.2141) acc_x 62.5000 (70.6250) lr 1.3137e-04 eta 0:00:09
epoch [169/200] batch [20/34] time 0.521 (0.482) data 0.391 (0.352) loss_x loss_x 1.2441 (1.2338) acc_x 68.7500 (70.9375) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [25/34] time 0.536 (0.476) data 0.405 (0.346) loss_x loss_x 1.0332 (1.2226) acc_x 78.1250 (70.7500) lr 1.3137e-04 eta 0:00:04
epoch [169/200] batch [30/34] time 0.605 (0.480) data 0.475 (0.349) loss_x loss_x 1.4492 (1.2345) acc_x 62.5000 (70.2083) lr 1.3137e-04 eta 0:00:01
epoch [169/200] batch [5/63] time 0.419 (0.467) data 0.289 (0.336) loss_u loss_u 0.8877 (0.9155) acc_u 12.5000 (10.6250) lr 1.3137e-04 eta 0:00:27
epoch [169/200] batch [10/63] time 0.465 (0.467) data 0.335 (0.337) loss_u loss_u 0.8398 (0.8889) acc_u 15.6250 (14.0625) lr 1.3137e-04 eta 0:00:24
epoch [169/200] batch [15/63] time 0.359 (0.466) data 0.229 (0.336) loss_u loss_u 0.9121 (0.8862) acc_u 6.2500 (14.3750) lr 1.3137e-04 eta 0:00:22
epoch [169/200] batch [20/63] time 0.381 (0.466) data 0.251 (0.335) loss_u loss_u 0.9297 (0.8794) acc_u 6.2500 (15.1562) lr 1.3137e-04 eta 0:00:20
epoch [169/200] batch [25/63] time 0.426 (0.468) data 0.295 (0.337) loss_u loss_u 0.9189 (0.8776) acc_u 9.3750 (15.1250) lr 1.3137e-04 eta 0:00:17
epoch [169/200] batch [30/63] time 0.539 (0.469) data 0.409 (0.339) loss_u loss_u 0.8325 (0.8732) acc_u 18.7500 (15.8333) lr 1.3137e-04 eta 0:00:15
epoch [169/200] batch [35/63] time 0.357 (0.464) data 0.226 (0.333) loss_u loss_u 0.8506 (0.8708) acc_u 18.7500 (15.9821) lr 1.3137e-04 eta 0:00:12
epoch [169/200] batch [40/63] time 0.467 (0.462) data 0.335 (0.331) loss_u loss_u 0.9712 (0.8734) acc_u 3.1250 (15.6250) lr 1.3137e-04 eta 0:00:10
epoch [169/200] batch [45/63] time 0.744 (0.468) data 0.612 (0.337) loss_u loss_u 0.9160 (0.8760) acc_u 9.3750 (15.2778) lr 1.3137e-04 eta 0:00:08
epoch [169/200] batch [50/63] time 0.494 (0.468) data 0.363 (0.338) loss_u loss_u 0.9009 (0.8752) acc_u 15.6250 (15.5000) lr 1.3137e-04 eta 0:00:06
epoch [169/200] batch [55/63] time 0.449 (0.468) data 0.319 (0.337) loss_u loss_u 0.8765 (0.8789) acc_u 15.6250 (15.0568) lr 1.3137e-04 eta 0:00:03
epoch [169/200] batch [60/63] time 0.415 (0.468) data 0.283 (0.337) loss_u loss_u 0.8604 (0.8784) acc_u 12.5000 (15.1042) lr 1.3137e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1567
confident_label rate tensor(0.3380, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1060
clean true:923
clean false:137
clean_rate:0.870754716981132
noisy true:646
noisy false:1430
after delete: len(clean_dataset) 1060
after delete: len(noisy_dataset) 2076
epoch [170/200] batch [5/33] time 0.458 (0.430) data 0.326 (0.299) loss_x loss_x 0.9395 (0.8843) acc_x 78.1250 (76.8750) lr 1.2369e-04 eta 0:00:12
epoch [170/200] batch [10/33] time 0.472 (0.466) data 0.342 (0.336) loss_x loss_x 1.1621 (0.9409) acc_x 62.5000 (75.6250) lr 1.2369e-04 eta 0:00:10
epoch [170/200] batch [15/33] time 0.500 (0.462) data 0.370 (0.331) loss_x loss_x 1.1143 (0.9432) acc_x 78.1250 (76.2500) lr 1.2369e-04 eta 0:00:08
epoch [170/200] batch [20/33] time 0.631 (0.467) data 0.501 (0.337) loss_x loss_x 1.4463 (0.9598) acc_x 68.7500 (75.7812) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [25/33] time 0.539 (0.458) data 0.409 (0.328) loss_x loss_x 1.6084 (0.9876) acc_x 68.7500 (75.0000) lr 1.2369e-04 eta 0:00:03
epoch [170/200] batch [30/33] time 0.591 (0.467) data 0.460 (0.336) loss_x loss_x 0.9473 (1.0340) acc_x 81.2500 (75.4167) lr 1.2369e-04 eta 0:00:01
epoch [170/200] batch [5/64] time 0.711 (0.465) data 0.578 (0.334) loss_u loss_u 0.8071 (0.8485) acc_u 25.0000 (20.0000) lr 1.2369e-04 eta 0:00:27
epoch [170/200] batch [10/64] time 0.409 (0.461) data 0.277 (0.330) loss_u loss_u 0.9453 (0.8653) acc_u 9.3750 (17.5000) lr 1.2369e-04 eta 0:00:24
epoch [170/200] batch [15/64] time 0.427 (0.466) data 0.295 (0.335) loss_u loss_u 0.8657 (0.8672) acc_u 15.6250 (17.5000) lr 1.2369e-04 eta 0:00:22
epoch [170/200] batch [20/64] time 0.439 (0.462) data 0.308 (0.331) loss_u loss_u 0.9062 (0.8714) acc_u 15.6250 (16.4062) lr 1.2369e-04 eta 0:00:20
epoch [170/200] batch [25/64] time 0.465 (0.459) data 0.334 (0.328) loss_u loss_u 0.8994 (0.8765) acc_u 12.5000 (15.6250) lr 1.2369e-04 eta 0:00:17
epoch [170/200] batch [30/64] time 0.385 (0.460) data 0.255 (0.329) loss_u loss_u 0.8208 (0.8776) acc_u 18.7500 (15.4167) lr 1.2369e-04 eta 0:00:15
epoch [170/200] batch [35/64] time 0.367 (0.456) data 0.235 (0.325) loss_u loss_u 0.8667 (0.8788) acc_u 15.6250 (15.0893) lr 1.2369e-04 eta 0:00:13
epoch [170/200] batch [40/64] time 0.417 (0.462) data 0.285 (0.331) loss_u loss_u 0.8555 (0.8728) acc_u 18.7500 (15.8594) lr 1.2369e-04 eta 0:00:11
epoch [170/200] batch [45/64] time 0.402 (0.465) data 0.271 (0.334) loss_u loss_u 0.9390 (0.8723) acc_u 6.2500 (15.9722) lr 1.2369e-04 eta 0:00:08
epoch [170/200] batch [50/64] time 0.368 (0.465) data 0.237 (0.334) loss_u loss_u 0.8623 (0.8737) acc_u 15.6250 (15.5625) lr 1.2369e-04 eta 0:00:06
epoch [170/200] batch [55/64] time 0.478 (0.470) data 0.347 (0.339) loss_u loss_u 0.9097 (0.8737) acc_u 9.3750 (15.6818) lr 1.2369e-04 eta 0:00:04
epoch [170/200] batch [60/64] time 0.450 (0.470) data 0.319 (0.339) loss_u loss_u 0.8911 (0.8726) acc_u 9.3750 (15.6250) lr 1.2369e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1550
confident_label rate tensor(0.3399, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1066
clean true:925
clean false:141
clean_rate:0.8677298311444653
noisy true:661
noisy false:1409
after delete: len(clean_dataset) 1066
after delete: len(noisy_dataset) 2070
epoch [171/200] batch [5/33] time 0.405 (0.493) data 0.275 (0.362) loss_x loss_x 0.6079 (1.0513) acc_x 90.6250 (73.7500) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [10/33] time 0.402 (0.461) data 0.272 (0.330) loss_x loss_x 1.5361 (1.1971) acc_x 68.7500 (71.2500) lr 1.1623e-04 eta 0:00:10
epoch [171/200] batch [15/33] time 0.484 (0.473) data 0.354 (0.342) loss_x loss_x 0.7349 (1.0949) acc_x 75.0000 (74.1667) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [20/33] time 0.642 (0.487) data 0.511 (0.356) loss_x loss_x 1.4922 (1.1180) acc_x 65.6250 (72.5000) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [25/33] time 0.472 (0.481) data 0.341 (0.351) loss_x loss_x 1.4736 (1.1349) acc_x 71.8750 (72.5000) lr 1.1623e-04 eta 0:00:03
epoch [171/200] batch [30/33] time 0.520 (0.479) data 0.389 (0.349) loss_x loss_x 1.3955 (1.1381) acc_x 65.6250 (72.1875) lr 1.1623e-04 eta 0:00:01
epoch [171/200] batch [5/64] time 0.469 (0.479) data 0.338 (0.348) loss_u loss_u 0.8315 (0.8445) acc_u 18.7500 (16.8750) lr 1.1623e-04 eta 0:00:28
epoch [171/200] batch [10/64] time 0.506 (0.474) data 0.376 (0.343) loss_u loss_u 0.8838 (0.8413) acc_u 12.5000 (19.0625) lr 1.1623e-04 eta 0:00:25
epoch [171/200] batch [15/64] time 0.404 (0.470) data 0.273 (0.339) loss_u loss_u 0.9575 (0.8540) acc_u 6.2500 (18.7500) lr 1.1623e-04 eta 0:00:23
epoch [171/200] batch [20/64] time 0.736 (0.476) data 0.605 (0.346) loss_u loss_u 0.8916 (0.8586) acc_u 12.5000 (17.8125) lr 1.1623e-04 eta 0:00:20
epoch [171/200] batch [25/64] time 0.409 (0.471) data 0.277 (0.340) loss_u loss_u 0.8364 (0.8620) acc_u 21.8750 (17.3750) lr 1.1623e-04 eta 0:00:18
epoch [171/200] batch [30/64] time 0.426 (0.468) data 0.294 (0.337) loss_u loss_u 0.8877 (0.8695) acc_u 18.7500 (16.5625) lr 1.1623e-04 eta 0:00:15
epoch [171/200] batch [35/64] time 0.536 (0.465) data 0.402 (0.334) loss_u loss_u 0.8984 (0.8670) acc_u 15.6250 (17.2321) lr 1.1623e-04 eta 0:00:13
epoch [171/200] batch [40/64] time 0.565 (0.470) data 0.432 (0.338) loss_u loss_u 0.8813 (0.8697) acc_u 15.6250 (16.7969) lr 1.1623e-04 eta 0:00:11
epoch [171/200] batch [45/64] time 0.508 (0.473) data 0.376 (0.342) loss_u loss_u 0.8091 (0.8675) acc_u 28.1250 (17.0139) lr 1.1623e-04 eta 0:00:08
epoch [171/200] batch [50/64] time 0.382 (0.470) data 0.250 (0.339) loss_u loss_u 0.8799 (0.8655) acc_u 12.5000 (17.3125) lr 1.1623e-04 eta 0:00:06
epoch [171/200] batch [55/64] time 0.374 (0.471) data 0.243 (0.340) loss_u loss_u 0.8965 (0.8672) acc_u 12.5000 (17.0455) lr 1.1623e-04 eta 0:00:04
epoch [171/200] batch [60/64] time 0.722 (0.474) data 0.591 (0.342) loss_u loss_u 0.9395 (0.8662) acc_u 3.1250 (17.0833) lr 1.1623e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1579
confident_label rate tensor(0.3418, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1072
clean true:920
clean false:152
clean_rate:0.8582089552238806
noisy true:637
noisy false:1427
after delete: len(clean_dataset) 1072
after delete: len(noisy_dataset) 2064
epoch [172/200] batch [5/33] time 0.469 (0.424) data 0.340 (0.294) loss_x loss_x 1.7910 (1.4928) acc_x 56.2500 (63.1250) lr 1.0899e-04 eta 0:00:11
epoch [172/200] batch [10/33] time 0.550 (0.440) data 0.419 (0.310) loss_x loss_x 1.8135 (1.4385) acc_x 59.3750 (65.0000) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [15/33] time 0.486 (0.471) data 0.356 (0.341) loss_x loss_x 1.2520 (1.3257) acc_x 62.5000 (67.2917) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [20/33] time 0.464 (0.473) data 0.333 (0.342) loss_x loss_x 1.4316 (1.2862) acc_x 56.2500 (67.9688) lr 1.0899e-04 eta 0:00:06
epoch [172/200] batch [25/33] time 0.489 (0.467) data 0.359 (0.336) loss_x loss_x 1.7412 (1.3012) acc_x 59.3750 (67.1250) lr 1.0899e-04 eta 0:00:03
epoch [172/200] batch [30/33] time 0.467 (0.461) data 0.337 (0.330) loss_x loss_x 1.1963 (1.2400) acc_x 68.7500 (68.6458) lr 1.0899e-04 eta 0:00:01
epoch [172/200] batch [5/64] time 0.434 (0.454) data 0.302 (0.324) loss_u loss_u 0.7974 (0.8535) acc_u 21.8750 (17.5000) lr 1.0899e-04 eta 0:00:26
epoch [172/200] batch [10/64] time 0.402 (0.447) data 0.270 (0.317) loss_u loss_u 0.9067 (0.8690) acc_u 12.5000 (15.6250) lr 1.0899e-04 eta 0:00:24
epoch [172/200] batch [15/64] time 0.388 (0.445) data 0.257 (0.315) loss_u loss_u 0.9370 (0.8779) acc_u 9.3750 (15.4167) lr 1.0899e-04 eta 0:00:21
epoch [172/200] batch [20/64] time 0.459 (0.446) data 0.328 (0.315) loss_u loss_u 0.8291 (0.8698) acc_u 25.0000 (16.4062) lr 1.0899e-04 eta 0:00:19
epoch [172/200] batch [25/64] time 0.409 (0.452) data 0.278 (0.321) loss_u loss_u 0.8716 (0.8653) acc_u 15.6250 (16.5000) lr 1.0899e-04 eta 0:00:17
epoch [172/200] batch [30/64] time 0.452 (0.455) data 0.320 (0.324) loss_u loss_u 0.8066 (0.8653) acc_u 21.8750 (16.2500) lr 1.0899e-04 eta 0:00:15
epoch [172/200] batch [35/64] time 0.381 (0.454) data 0.249 (0.323) loss_u loss_u 0.8848 (0.8673) acc_u 12.5000 (15.9821) lr 1.0899e-04 eta 0:00:13
epoch [172/200] batch [40/64] time 0.448 (0.451) data 0.316 (0.320) loss_u loss_u 0.9233 (0.8696) acc_u 9.3750 (15.9375) lr 1.0899e-04 eta 0:00:10
epoch [172/200] batch [45/64] time 0.538 (0.456) data 0.406 (0.325) loss_u loss_u 0.8062 (0.8675) acc_u 25.0000 (16.1806) lr 1.0899e-04 eta 0:00:08
epoch [172/200] batch [50/64] time 0.412 (0.461) data 0.280 (0.330) loss_u loss_u 0.8770 (0.8679) acc_u 18.7500 (16.1875) lr 1.0899e-04 eta 0:00:06
epoch [172/200] batch [55/64] time 0.444 (0.462) data 0.312 (0.331) loss_u loss_u 0.9185 (0.8660) acc_u 9.3750 (16.4205) lr 1.0899e-04 eta 0:00:04
epoch [172/200] batch [60/64] time 0.411 (0.462) data 0.279 (0.331) loss_u loss_u 0.8677 (0.8675) acc_u 18.7500 (16.1979) lr 1.0899e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1499
confident_label rate tensor(0.3514, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1102
clean true:962
clean false:140
clean_rate:0.8729582577132486
noisy true:675
noisy false:1359
after delete: len(clean_dataset) 1102
after delete: len(noisy_dataset) 2034
epoch [173/200] batch [5/34] time 0.431 (0.546) data 0.301 (0.415) loss_x loss_x 1.7207 (1.4521) acc_x 56.2500 (66.2500) lr 1.0197e-04 eta 0:00:15
epoch [173/200] batch [10/34] time 0.478 (0.494) data 0.346 (0.363) loss_x loss_x 1.2383 (1.3936) acc_x 75.0000 (67.8125) lr 1.0197e-04 eta 0:00:11
epoch [173/200] batch [15/34] time 0.388 (0.477) data 0.257 (0.346) loss_x loss_x 1.1592 (1.3279) acc_x 78.1250 (68.9583) lr 1.0197e-04 eta 0:00:09
epoch [173/200] batch [20/34] time 0.470 (0.474) data 0.340 (0.344) loss_x loss_x 1.2725 (1.2773) acc_x 75.0000 (70.1562) lr 1.0197e-04 eta 0:00:06
epoch [173/200] batch [25/34] time 0.393 (0.465) data 0.261 (0.334) loss_x loss_x 1.2627 (1.2919) acc_x 68.7500 (70.3750) lr 1.0197e-04 eta 0:00:04
epoch [173/200] batch [30/34] time 0.340 (0.457) data 0.210 (0.326) loss_x loss_x 1.1133 (1.2665) acc_x 68.7500 (70.7292) lr 1.0197e-04 eta 0:00:01
epoch [173/200] batch [5/63] time 0.472 (0.452) data 0.340 (0.322) loss_u loss_u 0.8213 (0.8725) acc_u 25.0000 (14.3750) lr 1.0197e-04 eta 0:00:26
epoch [173/200] batch [10/63] time 0.434 (0.451) data 0.302 (0.320) loss_u loss_u 0.8906 (0.8600) acc_u 15.6250 (15.9375) lr 1.0197e-04 eta 0:00:23
epoch [173/200] batch [15/63] time 0.467 (0.452) data 0.337 (0.321) loss_u loss_u 0.8560 (0.8658) acc_u 21.8750 (15.4167) lr 1.0197e-04 eta 0:00:21
epoch [173/200] batch [20/63] time 0.478 (0.453) data 0.347 (0.322) loss_u loss_u 0.8848 (0.8691) acc_u 12.5000 (15.1562) lr 1.0197e-04 eta 0:00:19
epoch [173/200] batch [25/63] time 0.406 (0.449) data 0.275 (0.319) loss_u loss_u 0.9341 (0.8684) acc_u 6.2500 (15.1250) lr 1.0197e-04 eta 0:00:17
epoch [173/200] batch [30/63] time 0.445 (0.449) data 0.312 (0.318) loss_u loss_u 0.7695 (0.8713) acc_u 28.1250 (15.2083) lr 1.0197e-04 eta 0:00:14
epoch [173/200] batch [35/63] time 0.413 (0.456) data 0.281 (0.325) loss_u loss_u 0.8701 (0.8738) acc_u 15.6250 (15.0893) lr 1.0197e-04 eta 0:00:12
epoch [173/200] batch [40/63] time 0.410 (0.454) data 0.278 (0.323) loss_u loss_u 0.9243 (0.8772) acc_u 9.3750 (14.7656) lr 1.0197e-04 eta 0:00:10
epoch [173/200] batch [45/63] time 0.368 (0.453) data 0.237 (0.322) loss_u loss_u 0.9551 (0.8776) acc_u 6.2500 (14.7917) lr 1.0197e-04 eta 0:00:08
epoch [173/200] batch [50/63] time 0.379 (0.455) data 0.247 (0.324) loss_u loss_u 0.8301 (0.8770) acc_u 18.7500 (14.6875) lr 1.0197e-04 eta 0:00:05
epoch [173/200] batch [55/63] time 0.375 (0.456) data 0.243 (0.325) loss_u loss_u 0.8135 (0.8736) acc_u 18.7500 (14.9432) lr 1.0197e-04 eta 0:00:03
epoch [173/200] batch [60/63] time 0.380 (0.457) data 0.248 (0.325) loss_u loss_u 0.8901 (0.8764) acc_u 9.3750 (14.6875) lr 1.0197e-04 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1565
confident_label rate tensor(0.3358, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1053
clean true:910
clean false:143
clean_rate:0.8641975308641975
noisy true:661
noisy false:1422
after delete: len(clean_dataset) 1053
after delete: len(noisy_dataset) 2083
epoch [174/200] batch [5/32] time 0.426 (0.526) data 0.296 (0.395) loss_x loss_x 1.1709 (1.0280) acc_x 75.0000 (74.3750) lr 9.5173e-05 eta 0:00:14
epoch [174/200] batch [10/32] time 0.417 (0.479) data 0.287 (0.349) loss_x loss_x 1.5859 (1.0848) acc_x 59.3750 (71.5625) lr 9.5173e-05 eta 0:00:10
epoch [174/200] batch [15/32] time 0.505 (0.503) data 0.375 (0.372) loss_x loss_x 1.0371 (1.0822) acc_x 68.7500 (72.0833) lr 9.5173e-05 eta 0:00:08
epoch [174/200] batch [20/32] time 0.402 (0.495) data 0.271 (0.365) loss_x loss_x 1.4961 (1.0678) acc_x 62.5000 (73.1250) lr 9.5173e-05 eta 0:00:05
epoch [174/200] batch [25/32] time 0.475 (0.484) data 0.344 (0.354) loss_x loss_x 1.8994 (1.1070) acc_x 62.5000 (72.7500) lr 9.5173e-05 eta 0:00:03
epoch [174/200] batch [30/32] time 0.521 (0.482) data 0.390 (0.351) loss_x loss_x 1.1445 (1.1217) acc_x 68.7500 (72.1875) lr 9.5173e-05 eta 0:00:00
epoch [174/200] batch [5/65] time 0.457 (0.480) data 0.327 (0.349) loss_u loss_u 0.8267 (0.8823) acc_u 25.0000 (13.7500) lr 9.5173e-05 eta 0:00:28
epoch [174/200] batch [10/65] time 0.456 (0.479) data 0.325 (0.348) loss_u loss_u 0.8853 (0.8715) acc_u 15.6250 (16.5625) lr 9.5173e-05 eta 0:00:26
epoch [174/200] batch [15/65] time 0.433 (0.474) data 0.302 (0.343) loss_u loss_u 0.9116 (0.8787) acc_u 12.5000 (15.6250) lr 9.5173e-05 eta 0:00:23
epoch [174/200] batch [20/65] time 0.437 (0.472) data 0.306 (0.342) loss_u loss_u 0.8921 (0.8773) acc_u 15.6250 (15.6250) lr 9.5173e-05 eta 0:00:21
epoch [174/200] batch [25/65] time 0.487 (0.472) data 0.356 (0.341) loss_u loss_u 0.9165 (0.8787) acc_u 12.5000 (15.6250) lr 9.5173e-05 eta 0:00:18
epoch [174/200] batch [30/65] time 0.370 (0.474) data 0.239 (0.343) loss_u loss_u 0.9062 (0.8793) acc_u 6.2500 (15.0000) lr 9.5173e-05 eta 0:00:16
epoch [174/200] batch [35/65] time 0.644 (0.472) data 0.512 (0.341) loss_u loss_u 0.9194 (0.8810) acc_u 9.3750 (14.8214) lr 9.5173e-05 eta 0:00:14
epoch [174/200] batch [40/65] time 0.457 (0.475) data 0.325 (0.344) loss_u loss_u 0.8228 (0.8760) acc_u 25.0000 (15.6250) lr 9.5173e-05 eta 0:00:11
epoch [174/200] batch [45/65] time 0.462 (0.475) data 0.331 (0.344) loss_u loss_u 0.9536 (0.8763) acc_u 6.2500 (15.6944) lr 9.5173e-05 eta 0:00:09
epoch [174/200] batch [50/65] time 0.625 (0.475) data 0.493 (0.345) loss_u loss_u 0.8232 (0.8757) acc_u 15.6250 (15.5625) lr 9.5173e-05 eta 0:00:07
epoch [174/200] batch [55/65] time 0.466 (0.474) data 0.335 (0.343) loss_u loss_u 0.9272 (0.8774) acc_u 9.3750 (15.1705) lr 9.5173e-05 eta 0:00:04
epoch [174/200] batch [60/65] time 0.474 (0.474) data 0.344 (0.343) loss_u loss_u 0.7598 (0.8751) acc_u 40.6250 (15.4167) lr 9.5173e-05 eta 0:00:02
epoch [174/200] batch [65/65] time 0.387 (0.472) data 0.257 (0.341) loss_u loss_u 0.8892 (0.8762) acc_u 12.5000 (15.1442) lr 9.5173e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1579
confident_label rate tensor(0.3259, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1022
clean true:891
clean false:131
clean_rate:0.8718199608610567
noisy true:666
noisy false:1448
after delete: len(clean_dataset) 1022
after delete: len(noisy_dataset) 2114
epoch [175/200] batch [5/31] time 0.542 (0.472) data 0.411 (0.341) loss_x loss_x 1.2549 (0.9149) acc_x 71.8750 (77.5000) lr 8.8597e-05 eta 0:00:12
epoch [175/200] batch [10/31] time 0.421 (0.445) data 0.290 (0.314) loss_x loss_x 1.1436 (0.9015) acc_x 65.6250 (77.5000) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [15/31] time 0.407 (0.460) data 0.276 (0.329) loss_x loss_x 1.3262 (1.0170) acc_x 62.5000 (73.3333) lr 8.8597e-05 eta 0:00:07
epoch [175/200] batch [20/31] time 0.430 (0.458) data 0.300 (0.328) loss_x loss_x 0.9106 (1.0417) acc_x 75.0000 (72.6562) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [25/31] time 0.446 (0.448) data 0.315 (0.317) loss_x loss_x 1.2090 (1.0516) acc_x 65.6250 (73.0000) lr 8.8597e-05 eta 0:00:02
epoch [175/200] batch [30/31] time 0.540 (0.457) data 0.410 (0.327) loss_x loss_x 0.6870 (1.0265) acc_x 81.2500 (73.5417) lr 8.8597e-05 eta 0:00:00
epoch [175/200] batch [5/66] time 0.473 (0.453) data 0.343 (0.322) loss_u loss_u 0.8350 (0.8892) acc_u 18.7500 (13.1250) lr 8.8597e-05 eta 0:00:27
epoch [175/200] batch [10/66] time 0.516 (0.454) data 0.385 (0.324) loss_u loss_u 0.8633 (0.8659) acc_u 15.6250 (15.6250) lr 8.8597e-05 eta 0:00:25
epoch [175/200] batch [15/66] time 0.588 (0.457) data 0.457 (0.327) loss_u loss_u 0.9209 (0.8654) acc_u 9.3750 (16.0417) lr 8.8597e-05 eta 0:00:23
epoch [175/200] batch [20/66] time 0.590 (0.459) data 0.459 (0.328) loss_u loss_u 0.8657 (0.8667) acc_u 18.7500 (16.2500) lr 8.8597e-05 eta 0:00:21
epoch [175/200] batch [25/66] time 0.522 (0.457) data 0.392 (0.327) loss_u loss_u 0.8784 (0.8692) acc_u 15.6250 (16.1250) lr 8.8597e-05 eta 0:00:18
epoch [175/200] batch [30/66] time 0.420 (0.458) data 0.288 (0.327) loss_u loss_u 0.9365 (0.8712) acc_u 12.5000 (16.0417) lr 8.8597e-05 eta 0:00:16
epoch [175/200] batch [35/66] time 0.361 (0.456) data 0.231 (0.325) loss_u loss_u 0.8501 (0.8698) acc_u 15.6250 (15.9821) lr 8.8597e-05 eta 0:00:14
epoch [175/200] batch [40/66] time 0.354 (0.456) data 0.222 (0.325) loss_u loss_u 0.7031 (0.8625) acc_u 34.3750 (16.7969) lr 8.8597e-05 eta 0:00:11
epoch [175/200] batch [45/66] time 0.395 (0.456) data 0.265 (0.326) loss_u loss_u 0.7793 (0.8620) acc_u 25.0000 (16.8750) lr 8.8597e-05 eta 0:00:09
epoch [175/200] batch [50/66] time 0.380 (0.455) data 0.249 (0.325) loss_u loss_u 0.8018 (0.8582) acc_u 28.1250 (17.5000) lr 8.8597e-05 eta 0:00:07
epoch [175/200] batch [55/66] time 0.482 (0.455) data 0.351 (0.324) loss_u loss_u 0.9199 (0.8627) acc_u 9.3750 (17.2159) lr 8.8597e-05 eta 0:00:05
epoch [175/200] batch [60/66] time 0.328 (0.452) data 0.198 (0.322) loss_u loss_u 0.9028 (0.8626) acc_u 12.5000 (17.2396) lr 8.8597e-05 eta 0:00:02
epoch [175/200] batch [65/66] time 0.449 (0.453) data 0.317 (0.322) loss_u loss_u 0.7480 (0.8591) acc_u 28.1250 (17.5962) lr 8.8597e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1581
confident_label rate tensor(0.3367, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1056
clean true:911
clean false:145
clean_rate:0.8626893939393939
noisy true:644
noisy false:1436
after delete: len(clean_dataset) 1056
after delete: len(noisy_dataset) 2080
epoch [176/200] batch [5/33] time 0.404 (0.453) data 0.273 (0.323) loss_x loss_x 1.1211 (1.0271) acc_x 68.7500 (71.8750) lr 8.2245e-05 eta 0:00:12
epoch [176/200] batch [10/33] time 0.429 (0.464) data 0.299 (0.334) loss_x loss_x 0.7891 (1.0079) acc_x 75.0000 (72.5000) lr 8.2245e-05 eta 0:00:10
epoch [176/200] batch [15/33] time 0.606 (0.460) data 0.474 (0.330) loss_x loss_x 0.9058 (1.0440) acc_x 71.8750 (71.8750) lr 8.2245e-05 eta 0:00:08
epoch [176/200] batch [20/33] time 0.409 (0.472) data 0.280 (0.341) loss_x loss_x 1.5254 (1.0936) acc_x 65.6250 (71.7188) lr 8.2245e-05 eta 0:00:06
epoch [176/200] batch [25/33] time 0.465 (0.474) data 0.334 (0.344) loss_x loss_x 1.4033 (1.1229) acc_x 68.7500 (71.8750) lr 8.2245e-05 eta 0:00:03
epoch [176/200] batch [30/33] time 0.422 (0.489) data 0.291 (0.359) loss_x loss_x 0.9805 (1.1262) acc_x 68.7500 (71.5625) lr 8.2245e-05 eta 0:00:01
epoch [176/200] batch [5/65] time 0.439 (0.485) data 0.308 (0.354) loss_u loss_u 0.8037 (0.8765) acc_u 21.8750 (13.7500) lr 8.2245e-05 eta 0:00:29
epoch [176/200] batch [10/65] time 0.460 (0.483) data 0.329 (0.352) loss_u loss_u 0.7983 (0.8756) acc_u 34.3750 (15.0000) lr 8.2245e-05 eta 0:00:26
epoch [176/200] batch [15/65] time 0.576 (0.477) data 0.446 (0.347) loss_u loss_u 0.8071 (0.8674) acc_u 21.8750 (16.2500) lr 8.2245e-05 eta 0:00:23
epoch [176/200] batch [20/65] time 0.408 (0.471) data 0.276 (0.340) loss_u loss_u 0.8643 (0.8682) acc_u 25.0000 (16.0938) lr 8.2245e-05 eta 0:00:21
epoch [176/200] batch [25/65] time 0.518 (0.468) data 0.386 (0.337) loss_u loss_u 0.8340 (0.8665) acc_u 18.7500 (16.2500) lr 8.2245e-05 eta 0:00:18
epoch [176/200] batch [30/65] time 0.416 (0.464) data 0.286 (0.333) loss_u loss_u 0.8828 (0.8711) acc_u 15.6250 (15.6250) lr 8.2245e-05 eta 0:00:16
epoch [176/200] batch [35/65] time 0.406 (0.466) data 0.274 (0.335) loss_u loss_u 0.9170 (0.8715) acc_u 12.5000 (15.6250) lr 8.2245e-05 eta 0:00:13
epoch [176/200] batch [40/65] time 0.474 (0.466) data 0.342 (0.335) loss_u loss_u 0.9380 (0.8690) acc_u 6.2500 (15.8594) lr 8.2245e-05 eta 0:00:11
epoch [176/200] batch [45/65] time 0.413 (0.464) data 0.282 (0.333) loss_u loss_u 0.7607 (0.8682) acc_u 31.2500 (16.2500) lr 8.2245e-05 eta 0:00:09
epoch [176/200] batch [50/65] time 0.821 (0.467) data 0.689 (0.336) loss_u loss_u 0.8857 (0.8696) acc_u 9.3750 (16.1250) lr 8.2245e-05 eta 0:00:07
epoch [176/200] batch [55/65] time 0.471 (0.469) data 0.340 (0.338) loss_u loss_u 0.9009 (0.8711) acc_u 12.5000 (16.0227) lr 8.2245e-05 eta 0:00:04
epoch [176/200] batch [60/65] time 0.489 (0.466) data 0.359 (0.335) loss_u loss_u 0.7915 (0.8708) acc_u 21.8750 (15.8854) lr 8.2245e-05 eta 0:00:02
epoch [176/200] batch [65/65] time 0.584 (0.466) data 0.452 (0.335) loss_u loss_u 0.8877 (0.8706) acc_u 15.6250 (15.9135) lr 8.2245e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1568
confident_label rate tensor(0.3371, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1057
clean true:912
clean false:145
clean_rate:0.8628192999053926
noisy true:656
noisy false:1423
after delete: len(clean_dataset) 1057
after delete: len(noisy_dataset) 2079
epoch [177/200] batch [5/33] time 0.546 (0.457) data 0.416 (0.326) loss_x loss_x 0.9561 (0.9884) acc_x 75.0000 (75.6250) lr 7.6120e-05 eta 0:00:12
epoch [177/200] batch [10/33] time 0.431 (0.461) data 0.301 (0.330) loss_x loss_x 1.1035 (1.0180) acc_x 65.6250 (73.7500) lr 7.6120e-05 eta 0:00:10
epoch [177/200] batch [15/33] time 0.354 (0.464) data 0.225 (0.334) loss_x loss_x 0.8730 (1.0660) acc_x 75.0000 (73.3333) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [20/33] time 0.476 (0.460) data 0.346 (0.330) loss_x loss_x 1.0332 (1.0926) acc_x 84.3750 (73.7500) lr 7.6120e-05 eta 0:00:05
epoch [177/200] batch [25/33] time 0.355 (0.456) data 0.225 (0.325) loss_x loss_x 1.0742 (1.1036) acc_x 65.6250 (72.6250) lr 7.6120e-05 eta 0:00:03
epoch [177/200] batch [30/33] time 0.442 (0.459) data 0.311 (0.328) loss_x loss_x 0.7339 (1.1210) acc_x 81.2500 (72.5000) lr 7.6120e-05 eta 0:00:01
epoch [177/200] batch [5/64] time 0.393 (0.464) data 0.262 (0.333) loss_u loss_u 0.7695 (0.8618) acc_u 28.1250 (16.2500) lr 7.6120e-05 eta 0:00:27
epoch [177/200] batch [10/64] time 0.453 (0.460) data 0.322 (0.329) loss_u loss_u 0.8701 (0.8591) acc_u 15.6250 (17.1875) lr 7.6120e-05 eta 0:00:24
epoch [177/200] batch [15/64] time 0.460 (0.457) data 0.328 (0.326) loss_u loss_u 0.9468 (0.8699) acc_u 3.1250 (16.0417) lr 7.6120e-05 eta 0:00:22
epoch [177/200] batch [20/64] time 0.443 (0.458) data 0.311 (0.327) loss_u loss_u 0.8042 (0.8639) acc_u 28.1250 (16.5625) lr 7.6120e-05 eta 0:00:20
epoch [177/200] batch [25/64] time 0.504 (0.461) data 0.372 (0.330) loss_u loss_u 0.9277 (0.8648) acc_u 6.2500 (16.6250) lr 7.6120e-05 eta 0:00:17
epoch [177/200] batch [30/64] time 0.442 (0.463) data 0.311 (0.332) loss_u loss_u 0.8975 (0.8631) acc_u 12.5000 (17.3958) lr 7.6120e-05 eta 0:00:15
epoch [177/200] batch [35/64] time 0.443 (0.462) data 0.311 (0.331) loss_u loss_u 0.8623 (0.8679) acc_u 15.6250 (16.6071) lr 7.6120e-05 eta 0:00:13
epoch [177/200] batch [40/64] time 0.599 (0.461) data 0.467 (0.330) loss_u loss_u 0.9565 (0.8718) acc_u 3.1250 (16.0156) lr 7.6120e-05 eta 0:00:11
epoch [177/200] batch [45/64] time 0.450 (0.459) data 0.320 (0.328) loss_u loss_u 0.8975 (0.8737) acc_u 18.7500 (15.8333) lr 7.6120e-05 eta 0:00:08
epoch [177/200] batch [50/64] time 0.508 (0.461) data 0.376 (0.330) loss_u loss_u 0.8872 (0.8751) acc_u 12.5000 (15.6250) lr 7.6120e-05 eta 0:00:06
epoch [177/200] batch [55/64] time 0.443 (0.461) data 0.312 (0.330) loss_u loss_u 0.8779 (0.8753) acc_u 12.5000 (15.6250) lr 7.6120e-05 eta 0:00:04
epoch [177/200] batch [60/64] time 0.650 (0.463) data 0.519 (0.332) loss_u loss_u 0.8423 (0.8724) acc_u 18.7500 (15.9375) lr 7.6120e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1534
confident_label rate tensor(0.3415, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1071
clean true:933
clean false:138
clean_rate:0.8711484593837535
noisy true:669
noisy false:1396
after delete: len(clean_dataset) 1071
after delete: len(noisy_dataset) 2065
epoch [178/200] batch [5/33] time 0.632 (0.503) data 0.501 (0.372) loss_x loss_x 1.1475 (1.0741) acc_x 71.8750 (71.2500) lr 7.0224e-05 eta 0:00:14
epoch [178/200] batch [10/33] time 0.506 (0.467) data 0.376 (0.336) loss_x loss_x 0.7773 (1.1096) acc_x 84.3750 (71.2500) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [15/33] time 0.604 (0.472) data 0.473 (0.342) loss_x loss_x 1.0557 (1.1035) acc_x 78.1250 (71.8750) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [20/33] time 0.487 (0.467) data 0.357 (0.337) loss_x loss_x 1.1074 (1.1518) acc_x 75.0000 (70.9375) lr 7.0224e-05 eta 0:00:06
epoch [178/200] batch [25/33] time 0.364 (0.457) data 0.233 (0.326) loss_x loss_x 1.2354 (1.1405) acc_x 65.6250 (70.6250) lr 7.0224e-05 eta 0:00:03
epoch [178/200] batch [30/33] time 0.457 (0.462) data 0.327 (0.331) loss_x loss_x 1.1934 (1.1396) acc_x 68.7500 (70.9375) lr 7.0224e-05 eta 0:00:01
epoch [178/200] batch [5/64] time 0.406 (0.465) data 0.274 (0.335) loss_u loss_u 0.9189 (0.8794) acc_u 6.2500 (12.5000) lr 7.0224e-05 eta 0:00:27
epoch [178/200] batch [10/64] time 0.356 (0.464) data 0.225 (0.334) loss_u loss_u 0.8936 (0.8831) acc_u 9.3750 (11.5625) lr 7.0224e-05 eta 0:00:25
epoch [178/200] batch [15/64] time 0.417 (0.464) data 0.285 (0.333) loss_u loss_u 0.8364 (0.8814) acc_u 21.8750 (13.1250) lr 7.0224e-05 eta 0:00:22
epoch [178/200] batch [20/64] time 0.491 (0.464) data 0.359 (0.334) loss_u loss_u 0.8198 (0.8787) acc_u 28.1250 (14.5312) lr 7.0224e-05 eta 0:00:20
epoch [178/200] batch [25/64] time 0.406 (0.459) data 0.275 (0.328) loss_u loss_u 0.9468 (0.8829) acc_u 6.2500 (14.0000) lr 7.0224e-05 eta 0:00:17
epoch [178/200] batch [30/64] time 0.458 (0.458) data 0.327 (0.327) loss_u loss_u 0.9487 (0.8782) acc_u 3.1250 (14.4792) lr 7.0224e-05 eta 0:00:15
epoch [178/200] batch [35/64] time 0.405 (0.455) data 0.273 (0.324) loss_u loss_u 0.7764 (0.8732) acc_u 25.0000 (15.0000) lr 7.0224e-05 eta 0:00:13
epoch [178/200] batch [40/64] time 0.444 (0.455) data 0.312 (0.324) loss_u loss_u 0.8955 (0.8758) acc_u 9.3750 (14.3750) lr 7.0224e-05 eta 0:00:10
epoch [178/200] batch [45/64] time 0.417 (0.455) data 0.286 (0.324) loss_u loss_u 0.9102 (0.8750) acc_u 12.5000 (14.3750) lr 7.0224e-05 eta 0:00:08
epoch [178/200] batch [50/64] time 0.447 (0.454) data 0.315 (0.323) loss_u loss_u 0.9458 (0.8782) acc_u 9.3750 (14.3125) lr 7.0224e-05 eta 0:00:06
epoch [178/200] batch [55/64] time 0.420 (0.458) data 0.289 (0.326) loss_u loss_u 0.8672 (0.8780) acc_u 18.7500 (14.4886) lr 7.0224e-05 eta 0:00:04
epoch [178/200] batch [60/64] time 0.399 (0.460) data 0.268 (0.329) loss_u loss_u 0.8535 (0.8768) acc_u 21.8750 (14.6354) lr 7.0224e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1547
confident_label rate tensor(0.3409, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1069
clean true:921
clean false:148
clean_rate:0.8615528531337698
noisy true:668
noisy false:1399
after delete: len(clean_dataset) 1069
after delete: len(noisy_dataset) 2067
epoch [179/200] batch [5/33] time 0.543 (0.499) data 0.412 (0.368) loss_x loss_x 1.1943 (1.1680) acc_x 68.7500 (68.7500) lr 6.4556e-05 eta 0:00:13
epoch [179/200] batch [10/33] time 0.489 (0.490) data 0.359 (0.360) loss_x loss_x 0.7793 (1.1197) acc_x 87.5000 (71.5625) lr 6.4556e-05 eta 0:00:11
epoch [179/200] batch [15/33] time 0.402 (0.483) data 0.272 (0.352) loss_x loss_x 1.2109 (1.1353) acc_x 62.5000 (70.2083) lr 6.4556e-05 eta 0:00:08
epoch [179/200] batch [20/33] time 0.466 (0.490) data 0.335 (0.360) loss_x loss_x 0.7271 (1.1185) acc_x 75.0000 (71.0938) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [25/33] time 0.446 (0.482) data 0.316 (0.352) loss_x loss_x 1.7988 (1.1719) acc_x 62.5000 (70.6250) lr 6.4556e-05 eta 0:00:03
epoch [179/200] batch [30/33] time 0.450 (0.487) data 0.320 (0.356) loss_x loss_x 0.8716 (1.1746) acc_x 75.0000 (70.5208) lr 6.4556e-05 eta 0:00:01
epoch [179/200] batch [5/64] time 0.371 (0.481) data 0.240 (0.351) loss_u loss_u 0.8516 (0.8714) acc_u 18.7500 (15.0000) lr 6.4556e-05 eta 0:00:28
epoch [179/200] batch [10/64] time 0.501 (0.482) data 0.370 (0.351) loss_u loss_u 0.8491 (0.8722) acc_u 21.8750 (16.2500) lr 6.4556e-05 eta 0:00:26
epoch [179/200] batch [15/64] time 0.435 (0.480) data 0.304 (0.349) loss_u loss_u 0.8359 (0.8508) acc_u 15.6250 (18.5417) lr 6.4556e-05 eta 0:00:23
epoch [179/200] batch [20/64] time 0.477 (0.484) data 0.345 (0.352) loss_u loss_u 0.9229 (0.8668) acc_u 12.5000 (16.5625) lr 6.4556e-05 eta 0:00:21
epoch [179/200] batch [25/64] time 0.348 (0.490) data 0.217 (0.358) loss_u loss_u 0.9082 (0.8672) acc_u 12.5000 (16.6250) lr 6.4556e-05 eta 0:00:19
epoch [179/200] batch [30/64] time 0.452 (0.489) data 0.320 (0.358) loss_u loss_u 0.8809 (0.8686) acc_u 18.7500 (16.3542) lr 6.4556e-05 eta 0:00:16
epoch [179/200] batch [35/64] time 0.393 (0.482) data 0.261 (0.351) loss_u loss_u 0.8389 (0.8705) acc_u 18.7500 (15.9821) lr 6.4556e-05 eta 0:00:13
epoch [179/200] batch [40/64] time 0.453 (0.484) data 0.321 (0.352) loss_u loss_u 0.9028 (0.8738) acc_u 9.3750 (15.3906) lr 6.4556e-05 eta 0:00:11
epoch [179/200] batch [45/64] time 0.413 (0.481) data 0.282 (0.350) loss_u loss_u 0.9116 (0.8759) acc_u 9.3750 (15.0000) lr 6.4556e-05 eta 0:00:09
epoch [179/200] batch [50/64] time 0.417 (0.477) data 0.285 (0.345) loss_u loss_u 0.9048 (0.8747) acc_u 12.5000 (15.3750) lr 6.4556e-05 eta 0:00:06
epoch [179/200] batch [55/64] time 0.393 (0.477) data 0.261 (0.345) loss_u loss_u 0.9229 (0.8775) acc_u 15.6250 (15.1136) lr 6.4556e-05 eta 0:00:04
epoch [179/200] batch [60/64] time 0.441 (0.474) data 0.309 (0.342) loss_u loss_u 0.9536 (0.8778) acc_u 6.2500 (15.0521) lr 6.4556e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1565
confident_label rate tensor(0.3380, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1060
clean true:914
clean false:146
clean_rate:0.8622641509433963
noisy true:657
noisy false:1419
after delete: len(clean_dataset) 1060
after delete: len(noisy_dataset) 2076
epoch [180/200] batch [5/33] time 0.389 (0.465) data 0.259 (0.335) loss_x loss_x 1.1768 (0.9605) acc_x 78.1250 (77.5000) lr 5.9119e-05 eta 0:00:13
epoch [180/200] batch [10/33] time 0.412 (0.435) data 0.281 (0.305) loss_x loss_x 1.2373 (1.0400) acc_x 71.8750 (75.9375) lr 5.9119e-05 eta 0:00:10
epoch [180/200] batch [15/33] time 0.611 (0.469) data 0.480 (0.339) loss_x loss_x 0.8066 (1.0491) acc_x 81.2500 (75.0000) lr 5.9119e-05 eta 0:00:08
epoch [180/200] batch [20/33] time 0.477 (0.469) data 0.347 (0.338) loss_x loss_x 0.8916 (1.0764) acc_x 65.6250 (72.5000) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [25/33] time 0.560 (0.480) data 0.430 (0.349) loss_x loss_x 0.8267 (1.0963) acc_x 71.8750 (71.6250) lr 5.9119e-05 eta 0:00:03
epoch [180/200] batch [30/33] time 0.476 (0.489) data 0.345 (0.358) loss_x loss_x 1.0146 (1.1176) acc_x 68.7500 (71.1458) lr 5.9119e-05 eta 0:00:01
epoch [180/200] batch [5/64] time 0.468 (0.487) data 0.338 (0.356) loss_u loss_u 0.8218 (0.8330) acc_u 21.8750 (18.7500) lr 5.9119e-05 eta 0:00:28
epoch [180/200] batch [10/64] time 0.459 (0.483) data 0.327 (0.353) loss_u loss_u 0.8560 (0.8490) acc_u 18.7500 (17.5000) lr 5.9119e-05 eta 0:00:26
epoch [180/200] batch [15/64] time 0.512 (0.482) data 0.380 (0.351) loss_u loss_u 0.9077 (0.8724) acc_u 9.3750 (14.7917) lr 5.9119e-05 eta 0:00:23
epoch [180/200] batch [20/64] time 0.468 (0.479) data 0.336 (0.348) loss_u loss_u 0.9521 (0.8737) acc_u 9.3750 (14.5312) lr 5.9119e-05 eta 0:00:21
epoch [180/200] batch [25/64] time 0.413 (0.476) data 0.282 (0.345) loss_u loss_u 0.9141 (0.8706) acc_u 9.3750 (14.5000) lr 5.9119e-05 eta 0:00:18
epoch [180/200] batch [30/64] time 0.450 (0.479) data 0.319 (0.347) loss_u loss_u 0.8950 (0.8687) acc_u 15.6250 (14.5833) lr 5.9119e-05 eta 0:00:16
epoch [180/200] batch [35/64] time 0.440 (0.475) data 0.309 (0.344) loss_u loss_u 0.8291 (0.8694) acc_u 21.8750 (14.6429) lr 5.9119e-05 eta 0:00:13
epoch [180/200] batch [40/64] time 0.463 (0.474) data 0.332 (0.343) loss_u loss_u 0.8062 (0.8680) acc_u 21.8750 (14.9219) lr 5.9119e-05 eta 0:00:11
epoch [180/200] batch [45/64] time 0.563 (0.475) data 0.433 (0.344) loss_u loss_u 0.8145 (0.8688) acc_u 21.8750 (14.7222) lr 5.9119e-05 eta 0:00:09
epoch [180/200] batch [50/64] time 0.366 (0.474) data 0.234 (0.343) loss_u loss_u 0.8784 (0.8660) acc_u 15.6250 (15.2500) lr 5.9119e-05 eta 0:00:06
epoch [180/200] batch [55/64] time 0.453 (0.474) data 0.321 (0.342) loss_u loss_u 0.9111 (0.8676) acc_u 12.5000 (15.0000) lr 5.9119e-05 eta 0:00:04
epoch [180/200] batch [60/64] time 0.493 (0.475) data 0.361 (0.344) loss_u loss_u 0.8809 (0.8691) acc_u 15.6250 (15.0000) lr 5.9119e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1549
confident_label rate tensor(0.3438, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1078
clean true:940
clean false:138
clean_rate:0.8719851576994434
noisy true:647
noisy false:1411
after delete: len(clean_dataset) 1078
after delete: len(noisy_dataset) 2058
epoch [181/200] batch [5/33] time 0.343 (0.423) data 0.213 (0.292) loss_x loss_x 1.5273 (1.2234) acc_x 62.5000 (70.6250) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [10/33] time 0.391 (0.437) data 0.261 (0.306) loss_x loss_x 1.5332 (1.2424) acc_x 65.6250 (68.7500) lr 5.3915e-05 eta 0:00:10
epoch [181/200] batch [15/33] time 0.490 (0.450) data 0.359 (0.320) loss_x loss_x 0.6992 (1.1156) acc_x 75.0000 (71.4583) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [20/33] time 0.468 (0.446) data 0.335 (0.316) loss_x loss_x 0.9819 (1.1592) acc_x 75.0000 (70.1562) lr 5.3915e-05 eta 0:00:05
epoch [181/200] batch [25/33] time 0.539 (0.456) data 0.409 (0.326) loss_x loss_x 1.1084 (1.1533) acc_x 78.1250 (71.2500) lr 5.3915e-05 eta 0:00:03
epoch [181/200] batch [30/33] time 0.461 (0.461) data 0.331 (0.330) loss_x loss_x 0.9531 (1.1339) acc_x 75.0000 (71.4583) lr 5.3915e-05 eta 0:00:01
epoch [181/200] batch [5/64] time 0.557 (0.462) data 0.425 (0.331) loss_u loss_u 0.9307 (0.8957) acc_u 9.3750 (12.5000) lr 5.3915e-05 eta 0:00:27
epoch [181/200] batch [10/64] time 0.526 (0.460) data 0.395 (0.329) loss_u loss_u 0.8589 (0.8966) acc_u 18.7500 (12.8125) lr 5.3915e-05 eta 0:00:24
epoch [181/200] batch [15/64] time 0.368 (0.463) data 0.237 (0.332) loss_u loss_u 0.8569 (0.8843) acc_u 15.6250 (13.7500) lr 5.3915e-05 eta 0:00:22
epoch [181/200] batch [20/64] time 0.366 (0.461) data 0.235 (0.330) loss_u loss_u 0.8408 (0.8725) acc_u 18.7500 (15.7812) lr 5.3915e-05 eta 0:00:20
epoch [181/200] batch [25/64] time 0.473 (0.461) data 0.341 (0.330) loss_u loss_u 0.8647 (0.8688) acc_u 18.7500 (16.3750) lr 5.3915e-05 eta 0:00:17
epoch [181/200] batch [30/64] time 0.535 (0.460) data 0.404 (0.330) loss_u loss_u 0.8813 (0.8714) acc_u 9.3750 (15.8333) lr 5.3915e-05 eta 0:00:15
epoch [181/200] batch [35/64] time 0.574 (0.464) data 0.443 (0.333) loss_u loss_u 0.8564 (0.8705) acc_u 15.6250 (15.9821) lr 5.3915e-05 eta 0:00:13
epoch [181/200] batch [40/64] time 0.377 (0.464) data 0.247 (0.333) loss_u loss_u 0.8706 (0.8713) acc_u 21.8750 (16.1719) lr 5.3915e-05 eta 0:00:11
epoch [181/200] batch [45/64] time 0.433 (0.462) data 0.302 (0.331) loss_u loss_u 0.8525 (0.8722) acc_u 18.7500 (16.0417) lr 5.3915e-05 eta 0:00:08
epoch [181/200] batch [50/64] time 0.389 (0.460) data 0.257 (0.329) loss_u loss_u 0.8320 (0.8721) acc_u 18.7500 (15.8750) lr 5.3915e-05 eta 0:00:06
epoch [181/200] batch [55/64] time 0.463 (0.459) data 0.332 (0.328) loss_u loss_u 0.8267 (0.8711) acc_u 28.1250 (16.0795) lr 5.3915e-05 eta 0:00:04
epoch [181/200] batch [60/64] time 0.485 (0.458) data 0.354 (0.327) loss_u loss_u 0.9087 (0.8733) acc_u 15.6250 (15.9375) lr 5.3915e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1592
confident_label rate tensor(0.3358, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1053
clean true:895
clean false:158
clean_rate:0.8499525166191833
noisy true:649
noisy false:1434
after delete: len(clean_dataset) 1053
after delete: len(noisy_dataset) 2083
epoch [182/200] batch [5/32] time 0.594 (0.504) data 0.463 (0.373) loss_x loss_x 1.0059 (1.0671) acc_x 75.0000 (70.6250) lr 4.8943e-05 eta 0:00:13
epoch [182/200] batch [10/32] time 0.568 (0.491) data 0.437 (0.361) loss_x loss_x 1.2256 (1.2266) acc_x 78.1250 (70.6250) lr 4.8943e-05 eta 0:00:10
epoch [182/200] batch [15/32] time 0.521 (0.479) data 0.389 (0.348) loss_x loss_x 1.4062 (1.1776) acc_x 56.2500 (72.0833) lr 4.8943e-05 eta 0:00:08
epoch [182/200] batch [20/32] time 0.572 (0.493) data 0.441 (0.362) loss_x loss_x 0.8701 (1.1079) acc_x 81.2500 (73.2812) lr 4.8943e-05 eta 0:00:05
epoch [182/200] batch [25/32] time 0.475 (0.494) data 0.344 (0.364) loss_x loss_x 1.4775 (1.1693) acc_x 62.5000 (71.7500) lr 4.8943e-05 eta 0:00:03
epoch [182/200] batch [30/32] time 0.515 (0.490) data 0.384 (0.360) loss_x loss_x 0.9248 (1.1289) acc_x 75.0000 (72.7083) lr 4.8943e-05 eta 0:00:00
epoch [182/200] batch [5/65] time 0.464 (0.492) data 0.333 (0.362) loss_u loss_u 0.8501 (0.8909) acc_u 18.7500 (14.3750) lr 4.8943e-05 eta 0:00:29
epoch [182/200] batch [10/65] time 0.411 (0.489) data 0.279 (0.359) loss_u loss_u 0.8667 (0.8634) acc_u 21.8750 (18.4375) lr 4.8943e-05 eta 0:00:26
epoch [182/200] batch [15/65] time 0.408 (0.487) data 0.278 (0.356) loss_u loss_u 0.8335 (0.8664) acc_u 21.8750 (17.7083) lr 4.8943e-05 eta 0:00:24
epoch [182/200] batch [20/65] time 0.401 (0.491) data 0.269 (0.360) loss_u loss_u 0.8506 (0.8575) acc_u 21.8750 (18.5938) lr 4.8943e-05 eta 0:00:22
epoch [182/200] batch [25/65] time 0.459 (0.489) data 0.327 (0.358) loss_u loss_u 0.9155 (0.8609) acc_u 9.3750 (17.8750) lr 4.8943e-05 eta 0:00:19
epoch [182/200] batch [30/65] time 0.471 (0.486) data 0.338 (0.355) loss_u loss_u 0.7939 (0.8632) acc_u 18.7500 (17.1875) lr 4.8943e-05 eta 0:00:17
epoch [182/200] batch [35/65] time 0.412 (0.480) data 0.281 (0.349) loss_u loss_u 0.7866 (0.8599) acc_u 21.8750 (16.9643) lr 4.8943e-05 eta 0:00:14
epoch [182/200] batch [40/65] time 0.480 (0.476) data 0.348 (0.346) loss_u loss_u 0.9688 (0.8651) acc_u 3.1250 (16.2500) lr 4.8943e-05 eta 0:00:11
epoch [182/200] batch [45/65] time 0.523 (0.479) data 0.393 (0.348) loss_u loss_u 0.8809 (0.8650) acc_u 12.5000 (16.1111) lr 4.8943e-05 eta 0:00:09
epoch [182/200] batch [50/65] time 0.461 (0.478) data 0.330 (0.347) loss_u loss_u 0.8242 (0.8643) acc_u 21.8750 (16.1875) lr 4.8943e-05 eta 0:00:07
epoch [182/200] batch [55/65] time 0.339 (0.474) data 0.209 (0.343) loss_u loss_u 0.8184 (0.8643) acc_u 25.0000 (16.1932) lr 4.8943e-05 eta 0:00:04
epoch [182/200] batch [60/65] time 0.559 (0.474) data 0.428 (0.343) loss_u loss_u 0.9346 (0.8669) acc_u 6.2500 (15.8854) lr 4.8943e-05 eta 0:00:02
epoch [182/200] batch [65/65] time 0.497 (0.474) data 0.366 (0.343) loss_u loss_u 0.9512 (0.8673) acc_u 6.2500 (15.7212) lr 4.8943e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1532
confident_label rate tensor(0.3399, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1066
clean true:931
clean false:135
clean_rate:0.873358348968105
noisy true:673
noisy false:1397
after delete: len(clean_dataset) 1066
after delete: len(noisy_dataset) 2070
epoch [183/200] batch [5/33] time 0.381 (0.488) data 0.250 (0.357) loss_x loss_x 1.3408 (1.1713) acc_x 71.8750 (73.1250) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [10/33] time 0.641 (0.507) data 0.511 (0.376) loss_x loss_x 1.1650 (1.1728) acc_x 75.0000 (71.8750) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [15/33] time 0.507 (0.510) data 0.375 (0.379) loss_x loss_x 1.1631 (1.0886) acc_x 68.7500 (73.3333) lr 4.4207e-05 eta 0:00:09
epoch [183/200] batch [20/33] time 0.479 (0.498) data 0.348 (0.367) loss_x loss_x 0.7686 (1.0786) acc_x 81.2500 (72.9688) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [25/33] time 0.444 (0.485) data 0.313 (0.354) loss_x loss_x 1.2520 (1.1029) acc_x 71.8750 (72.8750) lr 4.4207e-05 eta 0:00:03
epoch [183/200] batch [30/33] time 0.451 (0.487) data 0.320 (0.356) loss_x loss_x 0.8057 (1.1033) acc_x 81.2500 (72.9167) lr 4.4207e-05 eta 0:00:01
epoch [183/200] batch [5/64] time 0.446 (0.481) data 0.314 (0.350) loss_u loss_u 0.7134 (0.8447) acc_u 40.6250 (18.1250) lr 4.4207e-05 eta 0:00:28
epoch [183/200] batch [10/64] time 0.502 (0.477) data 0.371 (0.346) loss_u loss_u 0.9102 (0.8508) acc_u 9.3750 (18.1250) lr 4.4207e-05 eta 0:00:25
epoch [183/200] batch [15/64] time 0.384 (0.474) data 0.253 (0.343) loss_u loss_u 0.8916 (0.8545) acc_u 12.5000 (17.2917) lr 4.4207e-05 eta 0:00:23
epoch [183/200] batch [20/64] time 0.516 (0.474) data 0.385 (0.343) loss_u loss_u 0.8921 (0.8640) acc_u 12.5000 (15.7812) lr 4.4207e-05 eta 0:00:20
epoch [183/200] batch [25/64] time 0.381 (0.469) data 0.250 (0.338) loss_u loss_u 0.8647 (0.8666) acc_u 18.7500 (15.8750) lr 4.4207e-05 eta 0:00:18
epoch [183/200] batch [30/64] time 0.396 (0.463) data 0.265 (0.333) loss_u loss_u 0.8779 (0.8697) acc_u 15.6250 (15.4167) lr 4.4207e-05 eta 0:00:15
epoch [183/200] batch [35/64] time 0.511 (0.466) data 0.380 (0.335) loss_u loss_u 0.8955 (0.8712) acc_u 15.6250 (15.6250) lr 4.4207e-05 eta 0:00:13
epoch [183/200] batch [40/64] time 0.889 (0.470) data 0.758 (0.339) loss_u loss_u 0.9521 (0.8726) acc_u 0.0000 (15.3906) lr 4.4207e-05 eta 0:00:11
epoch [183/200] batch [45/64] time 0.408 (0.469) data 0.277 (0.338) loss_u loss_u 0.7812 (0.8698) acc_u 28.1250 (15.9028) lr 4.4207e-05 eta 0:00:08
epoch [183/200] batch [50/64] time 0.396 (0.467) data 0.265 (0.336) loss_u loss_u 0.8750 (0.8665) acc_u 18.7500 (16.4375) lr 4.4207e-05 eta 0:00:06
epoch [183/200] batch [55/64] time 0.440 (0.463) data 0.307 (0.332) loss_u loss_u 0.8926 (0.8673) acc_u 15.6250 (16.4205) lr 4.4207e-05 eta 0:00:04
epoch [183/200] batch [60/64] time 0.472 (0.465) data 0.340 (0.334) loss_u loss_u 0.9131 (0.8682) acc_u 9.3750 (16.2500) lr 4.4207e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1528
confident_label rate tensor(0.3402, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1067
clean true:923
clean false:144
clean_rate:0.8650421743205249
noisy true:685
noisy false:1384
after delete: len(clean_dataset) 1067
after delete: len(noisy_dataset) 2069
epoch [184/200] batch [5/33] time 0.508 (0.504) data 0.375 (0.373) loss_x loss_x 0.7817 (1.1080) acc_x 81.2500 (77.5000) lr 3.9706e-05 eta 0:00:14
epoch [184/200] batch [10/33] time 0.592 (0.488) data 0.461 (0.357) loss_x loss_x 1.2910 (1.0863) acc_x 65.6250 (74.3750) lr 3.9706e-05 eta 0:00:11
epoch [184/200] batch [15/33] time 0.599 (0.477) data 0.468 (0.346) loss_x loss_x 0.9937 (1.0839) acc_x 75.0000 (73.9583) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [20/33] time 0.478 (0.474) data 0.347 (0.342) loss_x loss_x 0.7471 (1.1046) acc_x 78.1250 (73.5938) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [25/33] time 0.444 (0.468) data 0.313 (0.337) loss_x loss_x 1.0957 (1.1047) acc_x 71.8750 (73.6250) lr 3.9706e-05 eta 0:00:03
epoch [184/200] batch [30/33] time 0.391 (0.465) data 0.260 (0.334) loss_x loss_x 1.4941 (1.1034) acc_x 62.5000 (73.0208) lr 3.9706e-05 eta 0:00:01
epoch [184/200] batch [5/64] time 0.643 (0.466) data 0.511 (0.335) loss_u loss_u 0.9277 (0.8734) acc_u 9.3750 (16.8750) lr 3.9706e-05 eta 0:00:27
epoch [184/200] batch [10/64] time 0.408 (0.460) data 0.277 (0.329) loss_u loss_u 0.9229 (0.8794) acc_u 6.2500 (15.6250) lr 3.9706e-05 eta 0:00:24
epoch [184/200] batch [15/64] time 0.442 (0.464) data 0.311 (0.333) loss_u loss_u 0.7510 (0.8705) acc_u 28.1250 (16.0417) lr 3.9706e-05 eta 0:00:22
epoch [184/200] batch [20/64] time 0.360 (0.465) data 0.229 (0.334) loss_u loss_u 0.9058 (0.8785) acc_u 12.5000 (14.6875) lr 3.9706e-05 eta 0:00:20
epoch [184/200] batch [25/64] time 0.461 (0.464) data 0.330 (0.333) loss_u loss_u 0.9077 (0.8811) acc_u 15.6250 (14.3750) lr 3.9706e-05 eta 0:00:18
epoch [184/200] batch [30/64] time 0.554 (0.467) data 0.421 (0.336) loss_u loss_u 0.9043 (0.8806) acc_u 9.3750 (14.1667) lr 3.9706e-05 eta 0:00:15
epoch [184/200] batch [35/64] time 0.384 (0.462) data 0.252 (0.331) loss_u loss_u 0.8687 (0.8773) acc_u 12.5000 (14.5536) lr 3.9706e-05 eta 0:00:13
epoch [184/200] batch [40/64] time 0.392 (0.461) data 0.261 (0.329) loss_u loss_u 0.8867 (0.8756) acc_u 12.5000 (14.8438) lr 3.9706e-05 eta 0:00:11
epoch [184/200] batch [45/64] time 0.428 (0.460) data 0.296 (0.329) loss_u loss_u 0.8521 (0.8729) acc_u 15.6250 (15.3472) lr 3.9706e-05 eta 0:00:08
epoch [184/200] batch [50/64] time 0.616 (0.461) data 0.484 (0.330) loss_u loss_u 0.8247 (0.8733) acc_u 25.0000 (15.5000) lr 3.9706e-05 eta 0:00:06
epoch [184/200] batch [55/64] time 0.528 (0.464) data 0.397 (0.333) loss_u loss_u 0.7925 (0.8702) acc_u 25.0000 (15.9091) lr 3.9706e-05 eta 0:00:04
epoch [184/200] batch [60/64] time 0.406 (0.463) data 0.274 (0.332) loss_u loss_u 0.8652 (0.8710) acc_u 15.6250 (15.8854) lr 3.9706e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1532
confident_label rate tensor(0.3386, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1062
clean true:926
clean false:136
clean_rate:0.871939736346516
noisy true:678
noisy false:1396
after delete: len(clean_dataset) 1062
after delete: len(noisy_dataset) 2074
epoch [185/200] batch [5/33] time 0.588 (0.544) data 0.458 (0.413) loss_x loss_x 1.5010 (1.1593) acc_x 62.5000 (71.2500) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [10/33] time 0.482 (0.534) data 0.352 (0.403) loss_x loss_x 1.2461 (1.1748) acc_x 56.2500 (69.3750) lr 3.5443e-05 eta 0:00:12
epoch [185/200] batch [15/33] time 0.427 (0.522) data 0.296 (0.391) loss_x loss_x 0.7485 (1.1633) acc_x 87.5000 (70.2083) lr 3.5443e-05 eta 0:00:09
epoch [185/200] batch [20/33] time 0.439 (0.499) data 0.309 (0.368) loss_x loss_x 1.3369 (1.1780) acc_x 65.6250 (70.3125) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [25/33] time 0.405 (0.498) data 0.275 (0.367) loss_x loss_x 1.1367 (1.1938) acc_x 75.0000 (70.2500) lr 3.5443e-05 eta 0:00:03
epoch [185/200] batch [30/33] time 0.454 (0.490) data 0.323 (0.359) loss_x loss_x 0.9629 (1.1883) acc_x 81.2500 (70.8333) lr 3.5443e-05 eta 0:00:01
epoch [185/200] batch [5/64] time 0.457 (0.476) data 0.326 (0.346) loss_u loss_u 0.9224 (0.8702) acc_u 9.3750 (15.0000) lr 3.5443e-05 eta 0:00:28
epoch [185/200] batch [10/64] time 0.582 (0.477) data 0.450 (0.346) loss_u loss_u 0.8335 (0.8730) acc_u 21.8750 (15.3125) lr 3.5443e-05 eta 0:00:25
epoch [185/200] batch [15/64] time 0.451 (0.476) data 0.319 (0.345) loss_u loss_u 0.8687 (0.8700) acc_u 12.5000 (15.6250) lr 3.5443e-05 eta 0:00:23
epoch [185/200] batch [20/64] time 0.383 (0.474) data 0.251 (0.343) loss_u loss_u 0.8184 (0.8734) acc_u 21.8750 (15.3125) lr 3.5443e-05 eta 0:00:20
epoch [185/200] batch [25/64] time 0.418 (0.470) data 0.287 (0.339) loss_u loss_u 0.8633 (0.8691) acc_u 15.6250 (15.8750) lr 3.5443e-05 eta 0:00:18
epoch [185/200] batch [30/64] time 0.503 (0.467) data 0.371 (0.336) loss_u loss_u 0.8857 (0.8725) acc_u 15.6250 (15.3125) lr 3.5443e-05 eta 0:00:15
epoch [185/200] batch [35/64] time 0.356 (0.462) data 0.224 (0.331) loss_u loss_u 0.9229 (0.8737) acc_u 6.2500 (15.1786) lr 3.5443e-05 eta 0:00:13
epoch [185/200] batch [40/64] time 0.412 (0.458) data 0.280 (0.327) loss_u loss_u 0.8306 (0.8739) acc_u 18.7500 (15.0781) lr 3.5443e-05 eta 0:00:10
epoch [185/200] batch [45/64] time 0.443 (0.459) data 0.311 (0.327) loss_u loss_u 0.8701 (0.8711) acc_u 25.0000 (15.6250) lr 3.5443e-05 eta 0:00:08
epoch [185/200] batch [50/64] time 0.617 (0.463) data 0.485 (0.331) loss_u loss_u 0.8555 (0.8672) acc_u 15.6250 (16.0000) lr 3.5443e-05 eta 0:00:06
epoch [185/200] batch [55/64] time 0.447 (0.468) data 0.317 (0.336) loss_u loss_u 0.9268 (0.8688) acc_u 9.3750 (15.7386) lr 3.5443e-05 eta 0:00:04
epoch [185/200] batch [60/64] time 0.406 (0.469) data 0.275 (0.338) loss_u loss_u 0.8105 (0.8696) acc_u 25.0000 (15.7292) lr 3.5443e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1546
confident_label rate tensor(0.3348, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1050
clean true:913
clean false:137
clean_rate:0.8695238095238095
noisy true:677
noisy false:1409
after delete: len(clean_dataset) 1050
after delete: len(noisy_dataset) 2086
epoch [186/200] batch [5/32] time 0.424 (0.503) data 0.294 (0.372) loss_x loss_x 1.1846 (1.0916) acc_x 62.5000 (70.0000) lr 3.1417e-05 eta 0:00:13
epoch [186/200] batch [10/32] time 0.431 (0.506) data 0.300 (0.376) loss_x loss_x 1.0020 (1.0357) acc_x 81.2500 (73.1250) lr 3.1417e-05 eta 0:00:11
epoch [186/200] batch [15/32] time 0.500 (0.490) data 0.369 (0.360) loss_x loss_x 1.4707 (1.0645) acc_x 65.6250 (72.7083) lr 3.1417e-05 eta 0:00:08
epoch [186/200] batch [20/32] time 0.455 (0.496) data 0.322 (0.365) loss_x loss_x 1.2266 (1.0656) acc_x 68.7500 (72.5000) lr 3.1417e-05 eta 0:00:05
epoch [186/200] batch [25/32] time 0.383 (0.480) data 0.252 (0.349) loss_x loss_x 0.9277 (1.1096) acc_x 75.0000 (71.7500) lr 3.1417e-05 eta 0:00:03
epoch [186/200] batch [30/32] time 0.630 (0.481) data 0.499 (0.351) loss_x loss_x 0.8862 (1.1050) acc_x 75.0000 (71.5625) lr 3.1417e-05 eta 0:00:00
epoch [186/200] batch [5/65] time 0.525 (0.483) data 0.394 (0.352) loss_u loss_u 0.9448 (0.8823) acc_u 3.1250 (11.8750) lr 3.1417e-05 eta 0:00:28
epoch [186/200] batch [10/65] time 0.472 (0.494) data 0.341 (0.363) loss_u loss_u 0.8999 (0.8836) acc_u 9.3750 (12.5000) lr 3.1417e-05 eta 0:00:27
epoch [186/200] batch [15/65] time 0.576 (0.495) data 0.445 (0.364) loss_u loss_u 0.9414 (0.8846) acc_u 9.3750 (13.3333) lr 3.1417e-05 eta 0:00:24
epoch [186/200] batch [20/65] time 0.397 (0.496) data 0.267 (0.365) loss_u loss_u 0.8384 (0.8813) acc_u 21.8750 (14.0625) lr 3.1417e-05 eta 0:00:22
epoch [186/200] batch [25/65] time 0.431 (0.496) data 0.299 (0.364) loss_u loss_u 0.8462 (0.8743) acc_u 18.7500 (15.5000) lr 3.1417e-05 eta 0:00:19
epoch [186/200] batch [30/65] time 0.461 (0.495) data 0.331 (0.363) loss_u loss_u 0.9302 (0.8758) acc_u 15.6250 (15.5208) lr 3.1417e-05 eta 0:00:17
epoch [186/200] batch [35/65] time 0.471 (0.492) data 0.341 (0.361) loss_u loss_u 0.8960 (0.8765) acc_u 9.3750 (15.3571) lr 3.1417e-05 eta 0:00:14
epoch [186/200] batch [40/65] time 0.497 (0.490) data 0.366 (0.358) loss_u loss_u 0.8228 (0.8744) acc_u 21.8750 (15.4688) lr 3.1417e-05 eta 0:00:12
epoch [186/200] batch [45/65] time 0.435 (0.486) data 0.304 (0.355) loss_u loss_u 0.8550 (0.8738) acc_u 21.8750 (15.6250) lr 3.1417e-05 eta 0:00:09
epoch [186/200] batch [50/65] time 0.452 (0.485) data 0.320 (0.354) loss_u loss_u 0.9019 (0.8755) acc_u 12.5000 (15.3750) lr 3.1417e-05 eta 0:00:07
epoch [186/200] batch [55/65] time 0.346 (0.479) data 0.215 (0.348) loss_u loss_u 0.8179 (0.8729) acc_u 25.0000 (15.6818) lr 3.1417e-05 eta 0:00:04
epoch [186/200] batch [60/65] time 0.377 (0.477) data 0.247 (0.346) loss_u loss_u 0.7861 (0.8719) acc_u 28.1250 (15.8333) lr 3.1417e-05 eta 0:00:02
epoch [186/200] batch [65/65] time 0.557 (0.478) data 0.426 (0.347) loss_u loss_u 0.8940 (0.8711) acc_u 12.5000 (15.8654) lr 3.1417e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1588
confident_label rate tensor(0.3320, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1041
clean true:906
clean false:135
clean_rate:0.8703170028818443
noisy true:642
noisy false:1453
after delete: len(clean_dataset) 1041
after delete: len(noisy_dataset) 2095
epoch [187/200] batch [5/32] time 0.332 (0.428) data 0.202 (0.297) loss_x loss_x 0.7866 (1.3013) acc_x 81.2500 (68.1250) lr 2.7630e-05 eta 0:00:11
epoch [187/200] batch [10/32] time 0.420 (0.444) data 0.289 (0.313) loss_x loss_x 0.7754 (1.1357) acc_x 84.3750 (71.8750) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [15/32] time 0.478 (0.451) data 0.348 (0.320) loss_x loss_x 0.9580 (1.1073) acc_x 71.8750 (71.2500) lr 2.7630e-05 eta 0:00:07
epoch [187/200] batch [20/32] time 0.520 (0.451) data 0.390 (0.321) loss_x loss_x 1.0029 (1.1706) acc_x 75.0000 (70.3125) lr 2.7630e-05 eta 0:00:05
epoch [187/200] batch [25/32] time 0.512 (0.460) data 0.381 (0.329) loss_x loss_x 0.5728 (1.1106) acc_x 90.6250 (72.3750) lr 2.7630e-05 eta 0:00:03
epoch [187/200] batch [30/32] time 0.574 (0.466) data 0.444 (0.336) loss_x loss_x 1.2305 (1.1160) acc_x 71.8750 (72.1875) lr 2.7630e-05 eta 0:00:00
epoch [187/200] batch [5/65] time 0.470 (0.462) data 0.339 (0.331) loss_u loss_u 0.9053 (0.9022) acc_u 6.2500 (12.5000) lr 2.7630e-05 eta 0:00:27
epoch [187/200] batch [10/65] time 0.657 (0.460) data 0.526 (0.329) loss_u loss_u 0.9214 (0.8912) acc_u 6.2500 (13.7500) lr 2.7630e-05 eta 0:00:25
epoch [187/200] batch [15/65] time 0.403 (0.462) data 0.273 (0.331) loss_u loss_u 0.8613 (0.8915) acc_u 18.7500 (13.3333) lr 2.7630e-05 eta 0:00:23
epoch [187/200] batch [20/65] time 0.452 (0.464) data 0.322 (0.333) loss_u loss_u 0.8594 (0.8891) acc_u 15.6250 (13.2812) lr 2.7630e-05 eta 0:00:20
epoch [187/200] batch [25/65] time 0.331 (0.462) data 0.199 (0.331) loss_u loss_u 0.8286 (0.8838) acc_u 18.7500 (14.0000) lr 2.7630e-05 eta 0:00:18
epoch [187/200] batch [30/65] time 0.503 (0.464) data 0.372 (0.333) loss_u loss_u 0.8413 (0.8786) acc_u 18.7500 (14.5833) lr 2.7630e-05 eta 0:00:16
epoch [187/200] batch [35/65] time 0.576 (0.464) data 0.444 (0.333) loss_u loss_u 0.8262 (0.8730) acc_u 18.7500 (15.2679) lr 2.7630e-05 eta 0:00:13
epoch [187/200] batch [40/65] time 0.554 (0.466) data 0.422 (0.335) loss_u loss_u 0.8975 (0.8745) acc_u 15.6250 (15.0000) lr 2.7630e-05 eta 0:00:11
epoch [187/200] batch [45/65] time 0.442 (0.465) data 0.311 (0.334) loss_u loss_u 0.8638 (0.8711) acc_u 18.7500 (15.2778) lr 2.7630e-05 eta 0:00:09
epoch [187/200] batch [50/65] time 0.398 (0.461) data 0.267 (0.331) loss_u loss_u 0.8853 (0.8743) acc_u 12.5000 (14.9375) lr 2.7630e-05 eta 0:00:06
epoch [187/200] batch [55/65] time 0.338 (0.461) data 0.206 (0.330) loss_u loss_u 0.8291 (0.8735) acc_u 28.1250 (15.1705) lr 2.7630e-05 eta 0:00:04
epoch [187/200] batch [60/65] time 0.399 (0.459) data 0.267 (0.328) loss_u loss_u 0.8560 (0.8754) acc_u 15.6250 (14.8438) lr 2.7630e-05 eta 0:00:02
epoch [187/200] batch [65/65] time 0.445 (0.460) data 0.315 (0.329) loss_u loss_u 0.8462 (0.8750) acc_u 18.7500 (14.8558) lr 2.7630e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1554
confident_label rate tensor(0.3450, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1082
clean true:939
clean false:143
clean_rate:0.8678373382624769
noisy true:643
noisy false:1411
after delete: len(clean_dataset) 1082
after delete: len(noisy_dataset) 2054
epoch [188/200] batch [5/33] time 0.387 (0.472) data 0.256 (0.341) loss_x loss_x 0.9043 (1.1224) acc_x 78.1250 (70.6250) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [10/33] time 0.464 (0.478) data 0.334 (0.347) loss_x loss_x 1.0381 (1.1326) acc_x 65.6250 (70.6250) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [15/33] time 0.491 (0.475) data 0.361 (0.344) loss_x loss_x 1.1104 (1.1230) acc_x 75.0000 (71.2500) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [20/33] time 0.432 (0.471) data 0.301 (0.340) loss_x loss_x 0.7798 (1.0776) acc_x 81.2500 (73.2812) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [25/33] time 0.444 (0.467) data 0.314 (0.337) loss_x loss_x 0.7676 (1.0818) acc_x 84.3750 (73.3750) lr 2.4083e-05 eta 0:00:03
epoch [188/200] batch [30/33] time 0.526 (0.459) data 0.395 (0.329) loss_x loss_x 0.8916 (1.0837) acc_x 75.0000 (73.0208) lr 2.4083e-05 eta 0:00:01
epoch [188/200] batch [5/64] time 0.423 (0.468) data 0.292 (0.337) loss_u loss_u 0.8818 (0.8598) acc_u 18.7500 (18.7500) lr 2.4083e-05 eta 0:00:27
epoch [188/200] batch [10/64] time 0.506 (0.470) data 0.374 (0.339) loss_u loss_u 0.9253 (0.8697) acc_u 6.2500 (16.2500) lr 2.4083e-05 eta 0:00:25
epoch [188/200] batch [15/64] time 0.438 (0.461) data 0.306 (0.330) loss_u loss_u 0.8940 (0.8695) acc_u 9.3750 (16.2500) lr 2.4083e-05 eta 0:00:22
epoch [188/200] batch [20/64] time 0.423 (0.456) data 0.292 (0.325) loss_u loss_u 0.8594 (0.8711) acc_u 18.7500 (16.0938) lr 2.4083e-05 eta 0:00:20
epoch [188/200] batch [25/64] time 0.397 (0.454) data 0.264 (0.323) loss_u loss_u 0.9712 (0.8716) acc_u 0.0000 (15.6250) lr 2.4083e-05 eta 0:00:17
epoch [188/200] batch [30/64] time 0.468 (0.454) data 0.336 (0.323) loss_u loss_u 0.8252 (0.8716) acc_u 21.8750 (15.5208) lr 2.4083e-05 eta 0:00:15
epoch [188/200] batch [35/64] time 0.372 (0.451) data 0.239 (0.320) loss_u loss_u 0.7979 (0.8731) acc_u 21.8750 (15.4464) lr 2.4083e-05 eta 0:00:13
epoch [188/200] batch [40/64] time 0.463 (0.454) data 0.331 (0.323) loss_u loss_u 0.8232 (0.8693) acc_u 28.1250 (16.3281) lr 2.4083e-05 eta 0:00:10
epoch [188/200] batch [45/64] time 0.492 (0.453) data 0.361 (0.322) loss_u loss_u 0.8398 (0.8721) acc_u 25.0000 (16.1806) lr 2.4083e-05 eta 0:00:08
epoch [188/200] batch [50/64] time 0.464 (0.452) data 0.333 (0.320) loss_u loss_u 0.8594 (0.8709) acc_u 12.5000 (16.1875) lr 2.4083e-05 eta 0:00:06
epoch [188/200] batch [55/64] time 0.476 (0.455) data 0.345 (0.323) loss_u loss_u 0.8149 (0.8708) acc_u 28.1250 (16.2500) lr 2.4083e-05 eta 0:00:04
epoch [188/200] batch [60/64] time 0.373 (0.456) data 0.242 (0.325) loss_u loss_u 0.9229 (0.8716) acc_u 12.5000 (16.4062) lr 2.4083e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1584
confident_label rate tensor(0.3422, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1073
clean true:921
clean false:152
clean_rate:0.8583410997204101
noisy true:631
noisy false:1432
after delete: len(clean_dataset) 1073
after delete: len(noisy_dataset) 2063
epoch [189/200] batch [5/33] time 0.444 (0.460) data 0.314 (0.329) loss_x loss_x 1.1699 (0.8966) acc_x 62.5000 (73.1250) lr 2.0777e-05 eta 0:00:12
epoch [189/200] batch [10/33] time 0.327 (0.478) data 0.197 (0.347) loss_x loss_x 1.2344 (0.9942) acc_x 59.3750 (71.8750) lr 2.0777e-05 eta 0:00:10
epoch [189/200] batch [15/33] time 0.347 (0.453) data 0.217 (0.322) loss_x loss_x 0.9434 (1.1050) acc_x 75.0000 (71.4583) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [20/33] time 0.414 (0.439) data 0.284 (0.309) loss_x loss_x 1.1846 (1.1172) acc_x 68.7500 (70.4688) lr 2.0777e-05 eta 0:00:05
epoch [189/200] batch [25/33] time 0.438 (0.455) data 0.307 (0.325) loss_x loss_x 1.1367 (1.0833) acc_x 71.8750 (71.3750) lr 2.0777e-05 eta 0:00:03
epoch [189/200] batch [30/33] time 0.491 (0.455) data 0.360 (0.324) loss_x loss_x 1.0195 (1.0667) acc_x 78.1250 (72.1875) lr 2.0777e-05 eta 0:00:01
epoch [189/200] batch [5/64] time 0.397 (0.452) data 0.267 (0.321) loss_u loss_u 0.8550 (0.8657) acc_u 25.0000 (18.7500) lr 2.0777e-05 eta 0:00:26
epoch [189/200] batch [10/64] time 0.463 (0.454) data 0.331 (0.323) loss_u loss_u 0.8774 (0.8680) acc_u 9.3750 (16.8750) lr 2.0777e-05 eta 0:00:24
epoch [189/200] batch [15/64] time 0.420 (0.458) data 0.290 (0.327) loss_u loss_u 0.9082 (0.8662) acc_u 18.7500 (17.2917) lr 2.0777e-05 eta 0:00:22
epoch [189/200] batch [20/64] time 0.518 (0.459) data 0.386 (0.329) loss_u loss_u 0.8940 (0.8702) acc_u 9.3750 (16.0938) lr 2.0777e-05 eta 0:00:20
epoch [189/200] batch [25/64] time 0.502 (0.463) data 0.371 (0.332) loss_u loss_u 0.8994 (0.8756) acc_u 12.5000 (15.3750) lr 2.0777e-05 eta 0:00:18
epoch [189/200] batch [30/64] time 0.513 (0.464) data 0.381 (0.334) loss_u loss_u 0.8936 (0.8787) acc_u 9.3750 (14.4792) lr 2.0777e-05 eta 0:00:15
epoch [189/200] batch [35/64] time 0.547 (0.462) data 0.414 (0.332) loss_u loss_u 0.9409 (0.8794) acc_u 3.1250 (14.4643) lr 2.0777e-05 eta 0:00:13
epoch [189/200] batch [40/64] time 0.480 (0.459) data 0.347 (0.328) loss_u loss_u 0.8477 (0.8774) acc_u 21.8750 (14.8438) lr 2.0777e-05 eta 0:00:11
epoch [189/200] batch [45/64] time 0.511 (0.460) data 0.381 (0.329) loss_u loss_u 0.9131 (0.8798) acc_u 12.5000 (14.6528) lr 2.0777e-05 eta 0:00:08
epoch [189/200] batch [50/64] time 0.468 (0.462) data 0.337 (0.331) loss_u loss_u 0.8486 (0.8773) acc_u 18.7500 (14.9375) lr 2.0777e-05 eta 0:00:06
epoch [189/200] batch [55/64] time 0.591 (0.462) data 0.460 (0.331) loss_u loss_u 0.8906 (0.8780) acc_u 18.7500 (15.1136) lr 2.0777e-05 eta 0:00:04
epoch [189/200] batch [60/64] time 0.499 (0.462) data 0.367 (0.331) loss_u loss_u 0.8506 (0.8768) acc_u 15.6250 (15.2604) lr 2.0777e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1550
confident_label rate tensor(0.3399, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1066
clean true:914
clean false:152
clean_rate:0.8574108818011257
noisy true:672
noisy false:1398
after delete: len(clean_dataset) 1066
after delete: len(noisy_dataset) 2070
epoch [190/200] batch [5/33] time 0.445 (0.478) data 0.313 (0.347) loss_x loss_x 0.9414 (1.1465) acc_x 71.8750 (73.1250) lr 1.7713e-05 eta 0:00:13
epoch [190/200] batch [10/33] time 0.556 (0.471) data 0.425 (0.340) loss_x loss_x 1.0039 (1.1846) acc_x 68.7500 (69.3750) lr 1.7713e-05 eta 0:00:10
epoch [190/200] batch [15/33] time 0.454 (0.483) data 0.324 (0.352) loss_x loss_x 0.9512 (1.2017) acc_x 75.0000 (70.0000) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [20/33] time 0.443 (0.483) data 0.311 (0.352) loss_x loss_x 0.7402 (1.1514) acc_x 78.1250 (72.0312) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [25/33] time 0.461 (0.481) data 0.329 (0.350) loss_x loss_x 0.9414 (1.1657) acc_x 75.0000 (71.8750) lr 1.7713e-05 eta 0:00:03
epoch [190/200] batch [30/33] time 0.493 (0.474) data 0.361 (0.343) loss_x loss_x 1.6055 (1.1804) acc_x 65.6250 (71.1458) lr 1.7713e-05 eta 0:00:01
epoch [190/200] batch [5/64] time 0.446 (0.472) data 0.315 (0.341) loss_u loss_u 0.9663 (0.8983) acc_u 3.1250 (11.2500) lr 1.7713e-05 eta 0:00:27
epoch [190/200] batch [10/64] time 0.477 (0.465) data 0.345 (0.334) loss_u loss_u 0.9380 (0.8946) acc_u 9.3750 (11.5625) lr 1.7713e-05 eta 0:00:25
epoch [190/200] batch [15/64] time 0.396 (0.463) data 0.265 (0.332) loss_u loss_u 0.9111 (0.8906) acc_u 12.5000 (12.2917) lr 1.7713e-05 eta 0:00:22
epoch [190/200] batch [20/64] time 0.393 (0.462) data 0.261 (0.331) loss_u loss_u 0.8447 (0.8769) acc_u 15.6250 (14.0625) lr 1.7713e-05 eta 0:00:20
epoch [190/200] batch [25/64] time 0.428 (0.467) data 0.298 (0.336) loss_u loss_u 0.9033 (0.8776) acc_u 9.3750 (14.2500) lr 1.7713e-05 eta 0:00:18
epoch [190/200] batch [30/64] time 0.505 (0.467) data 0.374 (0.335) loss_u loss_u 0.8384 (0.8781) acc_u 21.8750 (14.0625) lr 1.7713e-05 eta 0:00:15
epoch [190/200] batch [35/64] time 0.592 (0.468) data 0.460 (0.337) loss_u loss_u 0.8711 (0.8793) acc_u 15.6250 (14.0179) lr 1.7713e-05 eta 0:00:13
epoch [190/200] batch [40/64] time 0.400 (0.465) data 0.269 (0.333) loss_u loss_u 0.9111 (0.8784) acc_u 15.6250 (14.5312) lr 1.7713e-05 eta 0:00:11
epoch [190/200] batch [45/64] time 0.527 (0.465) data 0.396 (0.334) loss_u loss_u 0.8965 (0.8790) acc_u 15.6250 (14.4444) lr 1.7713e-05 eta 0:00:08
epoch [190/200] batch [50/64] time 0.445 (0.462) data 0.314 (0.331) loss_u loss_u 0.8188 (0.8732) acc_u 28.1250 (15.3125) lr 1.7713e-05 eta 0:00:06
epoch [190/200] batch [55/64] time 0.339 (0.461) data 0.209 (0.330) loss_u loss_u 0.8247 (0.8725) acc_u 21.8750 (15.2841) lr 1.7713e-05 eta 0:00:04
epoch [190/200] batch [60/64] time 0.451 (0.463) data 0.319 (0.332) loss_u loss_u 0.8130 (0.8733) acc_u 21.8750 (15.2083) lr 1.7713e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1561
confident_label rate tensor(0.3313, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1039
clean true:902
clean false:137
clean_rate:0.8681424446583254
noisy true:673
noisy false:1424
after delete: len(clean_dataset) 1039
after delete: len(noisy_dataset) 2097
epoch [191/200] batch [5/32] time 0.521 (0.504) data 0.390 (0.373) loss_x loss_x 1.1436 (1.1179) acc_x 65.6250 (70.0000) lr 1.4891e-05 eta 0:00:13
epoch [191/200] batch [10/32] time 0.471 (0.514) data 0.340 (0.383) loss_x loss_x 0.8008 (1.0594) acc_x 84.3750 (71.8750) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [15/32] time 0.465 (0.500) data 0.335 (0.369) loss_x loss_x 0.8130 (1.0845) acc_x 81.2500 (70.6250) lr 1.4891e-05 eta 0:00:08
epoch [191/200] batch [20/32] time 0.516 (0.492) data 0.386 (0.362) loss_x loss_x 0.9683 (1.1332) acc_x 71.8750 (71.2500) lr 1.4891e-05 eta 0:00:05
epoch [191/200] batch [25/32] time 0.527 (0.489) data 0.395 (0.359) loss_x loss_x 1.3662 (1.1876) acc_x 78.1250 (70.8750) lr 1.4891e-05 eta 0:00:03
epoch [191/200] batch [30/32] time 0.475 (0.483) data 0.345 (0.353) loss_x loss_x 1.2998 (1.1807) acc_x 65.6250 (71.4583) lr 1.4891e-05 eta 0:00:00
epoch [191/200] batch [5/65] time 0.464 (0.480) data 0.333 (0.350) loss_u loss_u 0.9365 (0.8313) acc_u 9.3750 (22.5000) lr 1.4891e-05 eta 0:00:28
epoch [191/200] batch [10/65] time 0.380 (0.474) data 0.249 (0.343) loss_u loss_u 0.9258 (0.8599) acc_u 9.3750 (18.4375) lr 1.4891e-05 eta 0:00:26
epoch [191/200] batch [15/65] time 0.578 (0.473) data 0.446 (0.342) loss_u loss_u 0.9092 (0.8581) acc_u 12.5000 (18.5417) lr 1.4891e-05 eta 0:00:23
epoch [191/200] batch [20/65] time 0.491 (0.471) data 0.360 (0.341) loss_u loss_u 0.8965 (0.8679) acc_u 12.5000 (17.0312) lr 1.4891e-05 eta 0:00:21
epoch [191/200] batch [25/65] time 0.574 (0.472) data 0.443 (0.341) loss_u loss_u 0.8525 (0.8612) acc_u 15.6250 (17.1250) lr 1.4891e-05 eta 0:00:18
epoch [191/200] batch [30/65] time 0.603 (0.470) data 0.472 (0.339) loss_u loss_u 0.8159 (0.8606) acc_u 18.7500 (16.8750) lr 1.4891e-05 eta 0:00:16
epoch [191/200] batch [35/65] time 0.398 (0.471) data 0.266 (0.340) loss_u loss_u 0.9116 (0.8651) acc_u 9.3750 (16.4286) lr 1.4891e-05 eta 0:00:14
epoch [191/200] batch [40/65] time 0.454 (0.468) data 0.323 (0.337) loss_u loss_u 0.8496 (0.8670) acc_u 15.6250 (16.0938) lr 1.4891e-05 eta 0:00:11
epoch [191/200] batch [45/65] time 0.412 (0.469) data 0.281 (0.338) loss_u loss_u 0.9004 (0.8670) acc_u 12.5000 (16.0417) lr 1.4891e-05 eta 0:00:09
epoch [191/200] batch [50/65] time 0.523 (0.471) data 0.392 (0.340) loss_u loss_u 0.8945 (0.8695) acc_u 12.5000 (15.6250) lr 1.4891e-05 eta 0:00:07
epoch [191/200] batch [55/65] time 0.544 (0.472) data 0.413 (0.341) loss_u loss_u 0.8604 (0.8703) acc_u 15.6250 (15.3977) lr 1.4891e-05 eta 0:00:04
epoch [191/200] batch [60/65] time 0.546 (0.474) data 0.415 (0.342) loss_u loss_u 0.8652 (0.8682) acc_u 18.7500 (15.7812) lr 1.4891e-05 eta 0:00:02
epoch [191/200] batch [65/65] time 0.485 (0.474) data 0.354 (0.343) loss_u loss_u 0.8994 (0.8698) acc_u 12.5000 (15.7212) lr 1.4891e-05 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1543
confident_label rate tensor(0.3418, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1072
clean true:925
clean false:147
clean_rate:0.8628731343283582
noisy true:668
noisy false:1396
after delete: len(clean_dataset) 1072
after delete: len(noisy_dataset) 2064
epoch [192/200] batch [5/33] time 0.475 (0.487) data 0.344 (0.357) loss_x loss_x 1.0449 (1.0813) acc_x 71.8750 (73.7500) lr 1.2312e-05 eta 0:00:13
epoch [192/200] batch [10/33] time 0.506 (0.474) data 0.375 (0.344) loss_x loss_x 0.8105 (1.0992) acc_x 71.8750 (73.7500) lr 1.2312e-05 eta 0:00:10
epoch [192/200] batch [15/33] time 0.485 (0.478) data 0.354 (0.348) loss_x loss_x 1.2256 (1.1439) acc_x 68.7500 (72.2917) lr 1.2312e-05 eta 0:00:08
epoch [192/200] batch [20/33] time 0.439 (0.482) data 0.308 (0.352) loss_x loss_x 1.0234 (1.1458) acc_x 81.2500 (72.3438) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [25/33] time 0.636 (0.488) data 0.506 (0.357) loss_x loss_x 1.1982 (1.1027) acc_x 81.2500 (73.8750) lr 1.2312e-05 eta 0:00:03
epoch [192/200] batch [30/33] time 0.467 (0.492) data 0.335 (0.361) loss_x loss_x 1.4150 (1.1222) acc_x 68.7500 (74.0625) lr 1.2312e-05 eta 0:00:01
epoch [192/200] batch [5/64] time 0.369 (0.483) data 0.236 (0.352) loss_u loss_u 0.8438 (0.8630) acc_u 25.0000 (16.2500) lr 1.2312e-05 eta 0:00:28
epoch [192/200] batch [10/64] time 0.424 (0.476) data 0.292 (0.345) loss_u loss_u 0.9087 (0.8754) acc_u 12.5000 (15.0000) lr 1.2312e-05 eta 0:00:25
epoch [192/200] batch [15/64] time 0.493 (0.481) data 0.361 (0.350) loss_u loss_u 0.8115 (0.8679) acc_u 25.0000 (16.4583) lr 1.2312e-05 eta 0:00:23
epoch [192/200] batch [20/64] time 0.488 (0.477) data 0.358 (0.346) loss_u loss_u 0.9331 (0.8702) acc_u 9.3750 (16.2500) lr 1.2312e-05 eta 0:00:21
epoch [192/200] batch [25/64] time 0.397 (0.485) data 0.266 (0.354) loss_u loss_u 0.8711 (0.8698) acc_u 15.6250 (15.7500) lr 1.2312e-05 eta 0:00:18
epoch [192/200] batch [30/64] time 0.432 (0.480) data 0.301 (0.349) loss_u loss_u 0.8740 (0.8694) acc_u 15.6250 (15.7292) lr 1.2312e-05 eta 0:00:16
epoch [192/200] batch [35/64] time 0.520 (0.477) data 0.387 (0.346) loss_u loss_u 0.7915 (0.8699) acc_u 18.7500 (15.7143) lr 1.2312e-05 eta 0:00:13
epoch [192/200] batch [40/64] time 0.411 (0.480) data 0.280 (0.349) loss_u loss_u 0.8857 (0.8703) acc_u 12.5000 (15.4688) lr 1.2312e-05 eta 0:00:11
epoch [192/200] batch [45/64] time 0.336 (0.481) data 0.204 (0.350) loss_u loss_u 0.8516 (0.8707) acc_u 18.7500 (15.3472) lr 1.2312e-05 eta 0:00:09
epoch [192/200] batch [50/64] time 0.615 (0.479) data 0.483 (0.348) loss_u loss_u 0.8711 (0.8715) acc_u 9.3750 (15.0625) lr 1.2312e-05 eta 0:00:06
epoch [192/200] batch [55/64] time 0.383 (0.477) data 0.252 (0.346) loss_u loss_u 0.8652 (0.8712) acc_u 15.6250 (15.2273) lr 1.2312e-05 eta 0:00:04
epoch [192/200] batch [60/64] time 0.440 (0.473) data 0.308 (0.342) loss_u loss_u 0.8472 (0.8723) acc_u 18.7500 (15.1042) lr 1.2312e-05 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1549
confident_label rate tensor(0.3422, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1073
clean true:920
clean false:153
clean_rate:0.8574091332712023
noisy true:667
noisy false:1396
after delete: len(clean_dataset) 1073
after delete: len(noisy_dataset) 2063
epoch [193/200] batch [5/33] time 0.358 (0.477) data 0.227 (0.346) loss_x loss_x 1.0898 (1.4137) acc_x 68.7500 (65.0000) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [10/33] time 0.403 (0.483) data 0.273 (0.352) loss_x loss_x 0.7134 (1.2636) acc_x 87.5000 (70.0000) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [15/33] time 0.407 (0.468) data 0.277 (0.338) loss_x loss_x 0.9302 (1.2176) acc_x 68.7500 (70.4167) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [20/33] time 0.604 (0.466) data 0.473 (0.335) loss_x loss_x 1.1045 (1.1445) acc_x 68.7500 (71.5625) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [25/33] time 0.495 (0.465) data 0.364 (0.334) loss_x loss_x 0.9795 (1.1342) acc_x 71.8750 (71.6250) lr 9.9763e-06 eta 0:00:03
epoch [193/200] batch [30/33] time 0.425 (0.477) data 0.294 (0.346) loss_x loss_x 1.5020 (1.1384) acc_x 59.3750 (71.4583) lr 9.9763e-06 eta 0:00:01
epoch [193/200] batch [5/64] time 0.416 (0.471) data 0.284 (0.340) loss_u loss_u 0.8115 (0.8784) acc_u 25.0000 (16.8750) lr 9.9763e-06 eta 0:00:27
epoch [193/200] batch [10/64] time 0.425 (0.466) data 0.293 (0.336) loss_u loss_u 0.8828 (0.8874) acc_u 15.6250 (15.0000) lr 9.9763e-06 eta 0:00:25
epoch [193/200] batch [15/64] time 0.415 (0.461) data 0.283 (0.330) loss_u loss_u 0.8398 (0.8831) acc_u 15.6250 (14.7917) lr 9.9763e-06 eta 0:00:22
epoch [193/200] batch [20/64] time 0.547 (0.464) data 0.414 (0.333) loss_u loss_u 0.8672 (0.8814) acc_u 15.6250 (14.3750) lr 9.9763e-06 eta 0:00:20
epoch [193/200] batch [25/64] time 0.744 (0.466) data 0.612 (0.335) loss_u loss_u 0.8184 (0.8809) acc_u 21.8750 (14.5000) lr 9.9763e-06 eta 0:00:18
epoch [193/200] batch [30/64] time 0.443 (0.466) data 0.312 (0.335) loss_u loss_u 0.8291 (0.8792) acc_u 21.8750 (14.5833) lr 9.9763e-06 eta 0:00:15
epoch [193/200] batch [35/64] time 0.383 (0.465) data 0.251 (0.334) loss_u loss_u 0.8477 (0.8741) acc_u 18.7500 (15.3571) lr 9.9763e-06 eta 0:00:13
epoch [193/200] batch [40/64] time 0.370 (0.461) data 0.238 (0.330) loss_u loss_u 0.8721 (0.8684) acc_u 12.5000 (16.0938) lr 9.9763e-06 eta 0:00:11
epoch [193/200] batch [45/64] time 0.415 (0.462) data 0.283 (0.331) loss_u loss_u 0.8325 (0.8680) acc_u 18.7500 (16.3194) lr 9.9763e-06 eta 0:00:08
epoch [193/200] batch [50/64] time 0.466 (0.467) data 0.335 (0.335) loss_u loss_u 0.9204 (0.8699) acc_u 3.1250 (15.9375) lr 9.9763e-06 eta 0:00:06
epoch [193/200] batch [55/64] time 0.433 (0.466) data 0.301 (0.335) loss_u loss_u 0.8740 (0.8711) acc_u 15.6250 (15.9659) lr 9.9763e-06 eta 0:00:04
epoch [193/200] batch [60/64] time 0.409 (0.462) data 0.279 (0.331) loss_u loss_u 0.8931 (0.8691) acc_u 9.3750 (16.1979) lr 9.9763e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1563
confident_label rate tensor(0.3278, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1028
clean true:894
clean false:134
clean_rate:0.8696498054474708
noisy true:679
noisy false:1429
after delete: len(clean_dataset) 1028
after delete: len(noisy_dataset) 2108
epoch [194/200] batch [5/32] time 0.387 (0.475) data 0.256 (0.344) loss_x loss_x 0.9165 (1.0820) acc_x 81.2500 (74.3750) lr 7.8853e-06 eta 0:00:12
epoch [194/200] batch [10/32] time 0.451 (0.466) data 0.321 (0.335) loss_x loss_x 1.1797 (1.0755) acc_x 71.8750 (73.7500) lr 7.8853e-06 eta 0:00:10
epoch [194/200] batch [15/32] time 0.399 (0.465) data 0.269 (0.334) loss_x loss_x 1.3975 (1.1071) acc_x 62.5000 (71.8750) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [20/32] time 0.404 (0.466) data 0.273 (0.335) loss_x loss_x 0.9619 (1.1185) acc_x 71.8750 (70.9375) lr 7.8853e-06 eta 0:00:05
epoch [194/200] batch [25/32] time 0.564 (0.466) data 0.433 (0.336) loss_x loss_x 1.0732 (1.0994) acc_x 71.8750 (70.5000) lr 7.8853e-06 eta 0:00:03
epoch [194/200] batch [30/32] time 0.460 (0.469) data 0.330 (0.338) loss_x loss_x 0.7402 (1.1259) acc_x 81.2500 (70.6250) lr 7.8853e-06 eta 0:00:00
epoch [194/200] batch [5/65] time 0.354 (0.467) data 0.224 (0.337) loss_u loss_u 0.8125 (0.8456) acc_u 21.8750 (19.3750) lr 7.8853e-06 eta 0:00:28
epoch [194/200] batch [10/65] time 0.467 (0.468) data 0.335 (0.337) loss_u loss_u 0.8857 (0.8559) acc_u 15.6250 (17.8125) lr 7.8853e-06 eta 0:00:25
epoch [194/200] batch [15/65] time 0.407 (0.466) data 0.276 (0.335) loss_u loss_u 0.8521 (0.8672) acc_u 21.8750 (16.4583) lr 7.8853e-06 eta 0:00:23
epoch [194/200] batch [20/65] time 0.389 (0.463) data 0.257 (0.333) loss_u loss_u 0.8853 (0.8701) acc_u 6.2500 (15.6250) lr 7.8853e-06 eta 0:00:20
epoch [194/200] batch [25/65] time 0.398 (0.463) data 0.267 (0.333) loss_u loss_u 0.8022 (0.8646) acc_u 25.0000 (16.6250) lr 7.8853e-06 eta 0:00:18
epoch [194/200] batch [30/65] time 0.462 (0.461) data 0.330 (0.330) loss_u loss_u 0.8955 (0.8676) acc_u 15.6250 (16.2500) lr 7.8853e-06 eta 0:00:16
epoch [194/200] batch [35/65] time 0.445 (0.464) data 0.313 (0.333) loss_u loss_u 0.8989 (0.8701) acc_u 6.2500 (15.4464) lr 7.8853e-06 eta 0:00:13
epoch [194/200] batch [40/65] time 0.605 (0.469) data 0.470 (0.338) loss_u loss_u 0.7900 (0.8654) acc_u 21.8750 (15.7812) lr 7.8853e-06 eta 0:00:11
epoch [194/200] batch [45/65] time 0.386 (0.471) data 0.256 (0.339) loss_u loss_u 0.8789 (0.8658) acc_u 12.5000 (15.5556) lr 7.8853e-06 eta 0:00:09
epoch [194/200] batch [50/65] time 0.458 (0.467) data 0.327 (0.336) loss_u loss_u 0.8740 (0.8677) acc_u 15.6250 (15.5625) lr 7.8853e-06 eta 0:00:07
epoch [194/200] batch [55/65] time 0.414 (0.466) data 0.283 (0.335) loss_u loss_u 0.9058 (0.8699) acc_u 12.5000 (15.3409) lr 7.8853e-06 eta 0:00:04
epoch [194/200] batch [60/65] time 0.402 (0.462) data 0.270 (0.331) loss_u loss_u 0.9199 (0.8697) acc_u 9.3750 (15.4688) lr 7.8853e-06 eta 0:00:02
epoch [194/200] batch [65/65] time 0.516 (0.464) data 0.385 (0.332) loss_u loss_u 0.7920 (0.8668) acc_u 31.2500 (15.9615) lr 7.8853e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1530
confident_label rate tensor(0.3399, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1066
clean true:943
clean false:123
clean_rate:0.8846153846153846
noisy true:663
noisy false:1407
after delete: len(clean_dataset) 1066
after delete: len(noisy_dataset) 2070
epoch [195/200] batch [5/33] time 0.554 (0.473) data 0.424 (0.343) loss_x loss_x 0.9263 (0.9357) acc_x 62.5000 (74.3750) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [10/33] time 0.414 (0.477) data 0.283 (0.347) loss_x loss_x 1.4375 (1.0849) acc_x 68.7500 (73.4375) lr 6.0390e-06 eta 0:00:10
epoch [195/200] batch [15/33] time 0.493 (0.480) data 0.362 (0.350) loss_x loss_x 1.8408 (1.1209) acc_x 65.6250 (73.7500) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [20/33] time 0.640 (0.500) data 0.510 (0.369) loss_x loss_x 0.9053 (1.0931) acc_x 75.0000 (74.5312) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [25/33] time 0.466 (0.501) data 0.335 (0.370) loss_x loss_x 1.5215 (1.1018) acc_x 65.6250 (74.0000) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [30/33] time 0.427 (0.490) data 0.297 (0.359) loss_x loss_x 0.9980 (1.0868) acc_x 65.6250 (74.1667) lr 6.0390e-06 eta 0:00:01
epoch [195/200] batch [5/64] time 0.602 (0.478) data 0.472 (0.347) loss_u loss_u 0.8921 (0.8962) acc_u 12.5000 (11.8750) lr 6.0390e-06 eta 0:00:28
epoch [195/200] batch [10/64] time 0.391 (0.474) data 0.259 (0.343) loss_u loss_u 0.8774 (0.8847) acc_u 15.6250 (13.4375) lr 6.0390e-06 eta 0:00:25
epoch [195/200] batch [15/64] time 0.495 (0.471) data 0.363 (0.341) loss_u loss_u 0.8584 (0.8819) acc_u 15.6250 (13.9583) lr 6.0390e-06 eta 0:00:23
epoch [195/200] batch [20/64] time 0.469 (0.464) data 0.337 (0.333) loss_u loss_u 0.9297 (0.8912) acc_u 12.5000 (12.9688) lr 6.0390e-06 eta 0:00:20
epoch [195/200] batch [25/64] time 0.389 (0.462) data 0.255 (0.331) loss_u loss_u 0.7837 (0.8849) acc_u 28.1250 (13.5000) lr 6.0390e-06 eta 0:00:18
epoch [195/200] batch [30/64] time 0.413 (0.465) data 0.282 (0.334) loss_u loss_u 0.8218 (0.8756) acc_u 28.1250 (15.3125) lr 6.0390e-06 eta 0:00:15
epoch [195/200] batch [35/64] time 0.428 (0.469) data 0.296 (0.338) loss_u loss_u 0.8711 (0.8755) acc_u 15.6250 (15.3571) lr 6.0390e-06 eta 0:00:13
epoch [195/200] batch [40/64] time 0.394 (0.469) data 0.262 (0.338) loss_u loss_u 0.8691 (0.8742) acc_u 21.8750 (15.5469) lr 6.0390e-06 eta 0:00:11
epoch [195/200] batch [45/64] time 0.418 (0.469) data 0.287 (0.338) loss_u loss_u 0.8550 (0.8730) acc_u 15.6250 (15.7639) lr 6.0390e-06 eta 0:00:08
epoch [195/200] batch [50/64] time 0.630 (0.470) data 0.498 (0.339) loss_u loss_u 0.9097 (0.8748) acc_u 12.5000 (15.7500) lr 6.0390e-06 eta 0:00:06
epoch [195/200] batch [55/64] time 0.408 (0.470) data 0.277 (0.339) loss_u loss_u 0.8525 (0.8748) acc_u 21.8750 (15.8523) lr 6.0390e-06 eta 0:00:04
epoch [195/200] batch [60/64] time 0.445 (0.468) data 0.313 (0.337) loss_u loss_u 0.8916 (0.8737) acc_u 15.6250 (16.0938) lr 6.0390e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1528
confident_label rate tensor(0.3450, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1082
clean true:933
clean false:149
clean_rate:0.8622920517560074
noisy true:675
noisy false:1379
after delete: len(clean_dataset) 1082
after delete: len(noisy_dataset) 2054
epoch [196/200] batch [5/33] time 0.436 (0.421) data 0.305 (0.291) loss_x loss_x 1.2764 (1.2369) acc_x 71.8750 (68.1250) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [10/33] time 0.411 (0.438) data 0.279 (0.307) loss_x loss_x 1.1143 (1.1942) acc_x 78.1250 (69.3750) lr 4.4380e-06 eta 0:00:10
epoch [196/200] batch [15/33] time 0.467 (0.473) data 0.336 (0.342) loss_x loss_x 1.0156 (1.1433) acc_x 78.1250 (72.5000) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [20/33] time 0.632 (0.482) data 0.501 (0.351) loss_x loss_x 1.0234 (1.1693) acc_x 71.8750 (72.0312) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [25/33] time 0.585 (0.492) data 0.454 (0.361) loss_x loss_x 1.2314 (1.2197) acc_x 65.6250 (70.6250) lr 4.4380e-06 eta 0:00:03
epoch [196/200] batch [30/33] time 0.391 (0.491) data 0.260 (0.360) loss_x loss_x 0.8989 (1.2130) acc_x 71.8750 (70.3125) lr 4.4380e-06 eta 0:00:01
epoch [196/200] batch [5/64] time 0.554 (0.482) data 0.422 (0.352) loss_u loss_u 0.9482 (0.9041) acc_u 6.2500 (10.6250) lr 4.4380e-06 eta 0:00:28
epoch [196/200] batch [10/64] time 0.476 (0.476) data 0.344 (0.345) loss_u loss_u 0.8657 (0.8955) acc_u 12.5000 (11.5625) lr 4.4380e-06 eta 0:00:25
epoch [196/200] batch [15/64] time 0.577 (0.480) data 0.444 (0.349) loss_u loss_u 0.8799 (0.8931) acc_u 12.5000 (12.0833) lr 4.4380e-06 eta 0:00:23
epoch [196/200] batch [20/64] time 0.339 (0.475) data 0.207 (0.344) loss_u loss_u 0.8364 (0.8852) acc_u 15.6250 (13.2812) lr 4.4380e-06 eta 0:00:20
epoch [196/200] batch [25/64] time 0.491 (0.472) data 0.361 (0.341) loss_u loss_u 0.9272 (0.8902) acc_u 9.3750 (12.7500) lr 4.4380e-06 eta 0:00:18
epoch [196/200] batch [30/64] time 0.436 (0.473) data 0.305 (0.342) loss_u loss_u 0.9146 (0.8893) acc_u 12.5000 (13.1250) lr 4.4380e-06 eta 0:00:16
epoch [196/200] batch [35/64] time 0.500 (0.473) data 0.368 (0.341) loss_u loss_u 0.8823 (0.8874) acc_u 15.6250 (13.2143) lr 4.4380e-06 eta 0:00:13
epoch [196/200] batch [40/64] time 0.347 (0.469) data 0.214 (0.338) loss_u loss_u 0.8760 (0.8851) acc_u 12.5000 (13.5156) lr 4.4380e-06 eta 0:00:11
epoch [196/200] batch [45/64] time 0.380 (0.468) data 0.248 (0.336) loss_u loss_u 0.8877 (0.8849) acc_u 12.5000 (13.4028) lr 4.4380e-06 eta 0:00:08
epoch [196/200] batch [50/64] time 0.406 (0.466) data 0.275 (0.334) loss_u loss_u 0.8857 (0.8804) acc_u 12.5000 (13.9375) lr 4.4380e-06 eta 0:00:06
epoch [196/200] batch [55/64] time 0.557 (0.469) data 0.426 (0.338) loss_u loss_u 0.7759 (0.8795) acc_u 25.0000 (14.2045) lr 4.4380e-06 eta 0:00:04
epoch [196/200] batch [60/64] time 0.448 (0.467) data 0.316 (0.336) loss_u loss_u 0.8511 (0.8797) acc_u 21.8750 (14.2188) lr 4.4380e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1525
confident_label rate tensor(0.3438, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1078
clean true:943
clean false:135
clean_rate:0.8747680890538033
noisy true:668
noisy false:1390
after delete: len(clean_dataset) 1078
after delete: len(noisy_dataset) 2058
epoch [197/200] batch [5/33] time 0.483 (0.447) data 0.353 (0.317) loss_x loss_x 1.1221 (0.9834) acc_x 81.2500 (76.8750) lr 3.0827e-06 eta 0:00:12
epoch [197/200] batch [10/33] time 0.500 (0.473) data 0.369 (0.343) loss_x loss_x 1.0293 (1.0053) acc_x 75.0000 (74.3750) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [15/33] time 0.553 (0.463) data 0.422 (0.333) loss_x loss_x 1.1631 (1.0753) acc_x 68.7500 (72.9167) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [20/33] time 0.433 (0.467) data 0.303 (0.336) loss_x loss_x 0.9155 (1.0633) acc_x 68.7500 (73.2812) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [25/33] time 0.475 (0.466) data 0.344 (0.335) loss_x loss_x 1.0449 (1.0541) acc_x 62.5000 (72.5000) lr 3.0827e-06 eta 0:00:03
epoch [197/200] batch [30/33] time 0.417 (0.463) data 0.286 (0.332) loss_x loss_x 1.2285 (1.0935) acc_x 65.6250 (71.2500) lr 3.0827e-06 eta 0:00:01
epoch [197/200] batch [5/64] time 0.407 (0.454) data 0.276 (0.324) loss_u loss_u 0.8652 (0.8735) acc_u 15.6250 (16.8750) lr 3.0827e-06 eta 0:00:26
epoch [197/200] batch [10/64] time 0.380 (0.454) data 0.248 (0.323) loss_u loss_u 0.8252 (0.8605) acc_u 18.7500 (17.5000) lr 3.0827e-06 eta 0:00:24
epoch [197/200] batch [15/64] time 0.365 (0.455) data 0.233 (0.324) loss_u loss_u 0.9136 (0.8680) acc_u 9.3750 (15.8333) lr 3.0827e-06 eta 0:00:22
epoch [197/200] batch [20/64] time 0.654 (0.459) data 0.522 (0.328) loss_u loss_u 0.8032 (0.8599) acc_u 28.1250 (17.1875) lr 3.0827e-06 eta 0:00:20
epoch [197/200] batch [25/64] time 0.365 (0.457) data 0.234 (0.326) loss_u loss_u 0.9048 (0.8636) acc_u 9.3750 (16.7500) lr 3.0827e-06 eta 0:00:17
epoch [197/200] batch [30/64] time 0.438 (0.452) data 0.307 (0.321) loss_u loss_u 0.8086 (0.8646) acc_u 25.0000 (16.7708) lr 3.0827e-06 eta 0:00:15
epoch [197/200] batch [35/64] time 0.367 (0.452) data 0.236 (0.321) loss_u loss_u 0.8511 (0.8677) acc_u 18.7500 (16.1607) lr 3.0827e-06 eta 0:00:13
epoch [197/200] batch [40/64] time 0.415 (0.453) data 0.282 (0.322) loss_u loss_u 0.9263 (0.8708) acc_u 9.3750 (15.7031) lr 3.0827e-06 eta 0:00:10
epoch [197/200] batch [45/64] time 0.476 (0.454) data 0.345 (0.323) loss_u loss_u 0.8701 (0.8725) acc_u 18.7500 (15.6944) lr 3.0827e-06 eta 0:00:08
epoch [197/200] batch [50/64] time 0.442 (0.451) data 0.312 (0.320) loss_u loss_u 0.9497 (0.8749) acc_u 6.2500 (15.3750) lr 3.0827e-06 eta 0:00:06
epoch [197/200] batch [55/64] time 0.373 (0.453) data 0.242 (0.322) loss_u loss_u 0.9033 (0.8763) acc_u 12.5000 (15.1705) lr 3.0827e-06 eta 0:00:04
epoch [197/200] batch [60/64] time 0.437 (0.452) data 0.305 (0.321) loss_u loss_u 0.9126 (0.8778) acc_u 9.3750 (15.0000) lr 3.0827e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1573
confident_label rate tensor(0.3323, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1042
clean true:900
clean false:142
clean_rate:0.8637236084452975
noisy true:663
noisy false:1431
after delete: len(clean_dataset) 1042
after delete: len(noisy_dataset) 2094
epoch [198/200] batch [5/32] time 0.430 (0.455) data 0.300 (0.324) loss_x loss_x 1.0283 (1.1514) acc_x 65.6250 (69.3750) lr 1.9733e-06 eta 0:00:12
epoch [198/200] batch [10/32] time 0.469 (0.470) data 0.339 (0.339) loss_x loss_x 1.1953 (1.1195) acc_x 71.8750 (70.9375) lr 1.9733e-06 eta 0:00:10
epoch [198/200] batch [15/32] time 0.425 (0.466) data 0.293 (0.336) loss_x loss_x 0.7573 (1.0989) acc_x 75.0000 (72.2917) lr 1.9733e-06 eta 0:00:07
epoch [198/200] batch [20/32] time 0.409 (0.465) data 0.279 (0.334) loss_x loss_x 0.8828 (1.0714) acc_x 75.0000 (72.9688) lr 1.9733e-06 eta 0:00:05
epoch [198/200] batch [25/32] time 0.392 (0.460) data 0.261 (0.330) loss_x loss_x 1.0342 (1.0934) acc_x 62.5000 (71.6250) lr 1.9733e-06 eta 0:00:03
epoch [198/200] batch [30/32] time 0.443 (0.465) data 0.312 (0.335) loss_x loss_x 1.5811 (1.1341) acc_x 59.3750 (70.9375) lr 1.9733e-06 eta 0:00:00
epoch [198/200] batch [5/65] time 0.367 (0.460) data 0.236 (0.330) loss_u loss_u 0.8818 (0.8610) acc_u 12.5000 (16.8750) lr 1.9733e-06 eta 0:00:27
epoch [198/200] batch [10/65] time 0.388 (0.467) data 0.257 (0.336) loss_u loss_u 0.8921 (0.8693) acc_u 18.7500 (15.6250) lr 1.9733e-06 eta 0:00:25
epoch [198/200] batch [15/65] time 0.362 (0.459) data 0.232 (0.328) loss_u loss_u 0.8975 (0.8610) acc_u 6.2500 (16.2500) lr 1.9733e-06 eta 0:00:22
epoch [198/200] batch [20/65] time 0.396 (0.454) data 0.264 (0.324) loss_u loss_u 0.8765 (0.8607) acc_u 15.6250 (17.1875) lr 1.9733e-06 eta 0:00:20
epoch [198/200] batch [25/65] time 0.390 (0.461) data 0.259 (0.330) loss_u loss_u 0.8706 (0.8621) acc_u 18.7500 (17.5000) lr 1.9733e-06 eta 0:00:18
epoch [198/200] batch [30/65] time 0.438 (0.460) data 0.307 (0.329) loss_u loss_u 0.9082 (0.8674) acc_u 12.5000 (16.6667) lr 1.9733e-06 eta 0:00:16
epoch [198/200] batch [35/65] time 0.474 (0.458) data 0.344 (0.328) loss_u loss_u 0.9131 (0.8695) acc_u 12.5000 (16.0714) lr 1.9733e-06 eta 0:00:13
epoch [198/200] batch [40/65] time 0.476 (0.455) data 0.342 (0.324) loss_u loss_u 0.8599 (0.8651) acc_u 25.0000 (16.7188) lr 1.9733e-06 eta 0:00:11
epoch [198/200] batch [45/65] time 0.524 (0.456) data 0.393 (0.325) loss_u loss_u 0.8564 (0.8669) acc_u 12.5000 (16.3889) lr 1.9733e-06 eta 0:00:09
epoch [198/200] batch [50/65] time 0.534 (0.459) data 0.403 (0.328) loss_u loss_u 0.9087 (0.8695) acc_u 12.5000 (16.3125) lr 1.9733e-06 eta 0:00:06
epoch [198/200] batch [55/65] time 0.365 (0.456) data 0.234 (0.325) loss_u loss_u 0.9282 (0.8707) acc_u 6.2500 (16.0795) lr 1.9733e-06 eta 0:00:04
epoch [198/200] batch [60/65] time 0.371 (0.455) data 0.236 (0.324) loss_u loss_u 0.9204 (0.8684) acc_u 12.5000 (16.4062) lr 1.9733e-06 eta 0:00:02
epoch [198/200] batch [65/65] time 0.517 (0.454) data 0.385 (0.323) loss_u loss_u 0.8257 (0.8694) acc_u 21.8750 (16.3942) lr 1.9733e-06 eta 0:00:00
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1567
confident_label rate tensor(0.3409, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1069
clean true:916
clean false:153
clean_rate:0.8568755846585594
noisy true:653
noisy false:1414
after delete: len(clean_dataset) 1069
after delete: len(noisy_dataset) 2067
epoch [199/200] batch [5/33] time 0.532 (0.518) data 0.401 (0.387) loss_x loss_x 1.2588 (1.1977) acc_x 75.0000 (70.6250) lr 1.1101e-06 eta 0:00:14
epoch [199/200] batch [10/33] time 0.468 (0.514) data 0.338 (0.383) loss_x loss_x 1.0264 (1.1338) acc_x 68.7500 (71.2500) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [15/33] time 0.416 (0.503) data 0.285 (0.372) loss_x loss_x 1.0557 (1.1005) acc_x 75.0000 (72.9167) lr 1.1101e-06 eta 0:00:09
epoch [199/200] batch [20/33] time 0.468 (0.482) data 0.338 (0.352) loss_x loss_x 1.2500 (1.1216) acc_x 71.8750 (73.7500) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [25/33] time 0.524 (0.484) data 0.392 (0.354) loss_x loss_x 1.4043 (1.0996) acc_x 65.6250 (73.5000) lr 1.1101e-06 eta 0:00:03
epoch [199/200] batch [30/33] time 0.407 (0.482) data 0.277 (0.351) loss_x loss_x 0.8691 (1.0862) acc_x 84.3750 (73.6458) lr 1.1101e-06 eta 0:00:01
epoch [199/200] batch [5/64] time 0.446 (0.478) data 0.314 (0.347) loss_u loss_u 0.9189 (0.8752) acc_u 6.2500 (15.0000) lr 1.1101e-06 eta 0:00:28
epoch [199/200] batch [10/64] time 0.497 (0.481) data 0.366 (0.351) loss_u loss_u 0.8115 (0.8712) acc_u 25.0000 (14.6875) lr 1.1101e-06 eta 0:00:25
epoch [199/200] batch [15/64] time 0.335 (0.484) data 0.205 (0.353) loss_u loss_u 0.9272 (0.8791) acc_u 6.2500 (13.9583) lr 1.1101e-06 eta 0:00:23
epoch [199/200] batch [20/64] time 0.482 (0.481) data 0.349 (0.350) loss_u loss_u 0.9048 (0.8783) acc_u 12.5000 (14.0625) lr 1.1101e-06 eta 0:00:21
epoch [199/200] batch [25/64] time 0.491 (0.479) data 0.360 (0.348) loss_u loss_u 0.8706 (0.8758) acc_u 15.6250 (14.3750) lr 1.1101e-06 eta 0:00:18
epoch [199/200] batch [30/64] time 0.478 (0.475) data 0.348 (0.344) loss_u loss_u 0.9043 (0.8779) acc_u 9.3750 (14.2708) lr 1.1101e-06 eta 0:00:16
epoch [199/200] batch [35/64] time 0.424 (0.474) data 0.294 (0.343) loss_u loss_u 0.8130 (0.8718) acc_u 25.0000 (15.2679) lr 1.1101e-06 eta 0:00:13
epoch [199/200] batch [40/64] time 0.388 (0.473) data 0.258 (0.342) loss_u loss_u 0.8433 (0.8703) acc_u 21.8750 (15.4688) lr 1.1101e-06 eta 0:00:11
epoch [199/200] batch [45/64] time 0.379 (0.470) data 0.248 (0.339) loss_u loss_u 0.9570 (0.8728) acc_u 6.2500 (15.3472) lr 1.1101e-06 eta 0:00:08
epoch [199/200] batch [50/64] time 0.441 (0.470) data 0.310 (0.339) loss_u loss_u 0.8320 (0.8712) acc_u 21.8750 (15.5625) lr 1.1101e-06 eta 0:00:06
epoch [199/200] batch [55/64] time 0.380 (0.469) data 0.248 (0.338) loss_u loss_u 0.8638 (0.8711) acc_u 15.6250 (15.5114) lr 1.1101e-06 eta 0:00:04
epoch [199/200] batch [60/64] time 0.506 (0.468) data 0.375 (0.337) loss_u loss_u 0.9224 (0.8718) acc_u 9.3750 (15.6250) lr 1.1101e-06 eta 0:00:01
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
[OT_PL] Loading all features and outputs... Total samples: 3136
[OT_PL] Features shape: torch.Size([3136, 1024]), Outputs shape: torch.Size([3136, 196])
[OT_PL] Starting Sinkhorn computation... P shape: torch.Size([3136, 196]), budget: 1.0
[Sinkhorn] Starting: a.shape=torch.Size([3136]), b.shape=torch.Size([196]), P.shape=torch.Size([3136, 196]), reg=0.01, numItermax=1000
[Sinkhorn] Completed! coupling.shape=torch.Size([3136, 196])
[OT_PL] Sinkhorn computation completed!
before epoch:data num: 3136
before epoch:different number: 1553
confident_label rate tensor(0.3428, device='cuda:0')
before: len(self.train) 3136
before: len of confident samples 1075
clean true:927
clean false:148
clean_rate:0.8623255813953489
noisy true:656
noisy false:1405
after delete: len(clean_dataset) 1075
after delete: len(noisy_dataset) 2061
epoch [200/200] batch [5/33] time 0.390 (0.468) data 0.260 (0.337) loss_x loss_x 0.7480 (0.9570) acc_x 78.1250 (75.0000) lr 4.9344e-07 eta 0:00:13
epoch [200/200] batch [10/33] time 0.397 (0.456) data 0.266 (0.325) loss_x loss_x 1.3467 (1.0889) acc_x 65.6250 (69.3750) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [15/33] time 0.414 (0.456) data 0.284 (0.325) loss_x loss_x 1.0146 (1.0618) acc_x 75.0000 (70.4167) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [20/33] time 0.405 (0.445) data 0.275 (0.314) loss_x loss_x 1.2998 (1.1288) acc_x 68.7500 (70.4688) lr 4.9344e-07 eta 0:00:05
epoch [200/200] batch [25/33] time 0.463 (0.441) data 0.333 (0.310) loss_x loss_x 1.3271 (1.1350) acc_x 78.1250 (71.1250) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [30/33] time 0.432 (0.442) data 0.302 (0.311) loss_x loss_x 1.2949 (1.1253) acc_x 68.7500 (71.4583) lr 4.9344e-07 eta 0:00:01
epoch [200/200] batch [5/64] time 0.397 (0.443) data 0.266 (0.313) loss_u loss_u 0.7954 (0.8740) acc_u 25.0000 (17.5000) lr 4.9344e-07 eta 0:00:26
epoch [200/200] batch [10/64] time 0.422 (0.438) data 0.292 (0.308) loss_u loss_u 0.8984 (0.8774) acc_u 15.6250 (17.8125) lr 4.9344e-07 eta 0:00:23
epoch [200/200] batch [15/64] time 0.453 (0.440) data 0.323 (0.309) loss_u loss_u 0.8286 (0.8805) acc_u 28.1250 (16.4583) lr 4.9344e-07 eta 0:00:21
epoch [200/200] batch [20/64] time 0.659 (0.440) data 0.529 (0.310) loss_u loss_u 0.8535 (0.8723) acc_u 18.7500 (17.1875) lr 4.9344e-07 eta 0:00:19
epoch [200/200] batch [25/64] time 0.375 (0.439) data 0.246 (0.308) loss_u loss_u 0.8613 (0.8703) acc_u 12.5000 (16.8750) lr 4.9344e-07 eta 0:00:17
epoch [200/200] batch [30/64] time 0.334 (0.437) data 0.203 (0.306) loss_u loss_u 0.8594 (0.8714) acc_u 15.6250 (16.5625) lr 4.9344e-07 eta 0:00:14
epoch [200/200] batch [35/64] time 0.493 (0.434) data 0.362 (0.304) loss_u loss_u 0.8462 (0.8658) acc_u 25.0000 (17.2321) lr 4.9344e-07 eta 0:00:12
epoch [200/200] batch [40/64] time 0.447 (0.435) data 0.315 (0.305) loss_u loss_u 0.8276 (0.8684) acc_u 18.7500 (16.9531) lr 4.9344e-07 eta 0:00:10
epoch [200/200] batch [45/64] time 0.526 (0.437) data 0.395 (0.307) loss_u loss_u 0.9170 (0.8708) acc_u 15.6250 (16.6667) lr 4.9344e-07 eta 0:00:08
epoch [200/200] batch [50/64] time 0.421 (0.436) data 0.290 (0.306) loss_u loss_u 0.8799 (0.8719) acc_u 9.3750 (16.4375) lr 4.9344e-07 eta 0:00:06
epoch [200/200] batch [55/64] time 0.492 (0.434) data 0.361 (0.304) loss_u loss_u 0.9268 (0.8716) acc_u 9.3750 (16.2500) lr 4.9344e-07 eta 0:00:03
epoch [200/200] batch [60/64] time 0.403 (0.435) data 0.272 (0.304) loss_u loss_u 0.8438 (0.8715) acc_u 21.8750 (16.3542) lr 4.9344e-07 eta 0:00:01
Checkpoint saved to output/stanford_cars/NLPrompt/rn50_16shots/noise_asym_0.50/seed1/prompt_learner/model.pth.tar-200
after epoch: len(clean dataset) 3136
after epoch: len(noisy dataset) 3136
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 8,041
* correct: 4,829
* accuracy: 60.1%
* error: 39.9%
* macro_f1: 57.9%
Elapsed: 5:04:50
